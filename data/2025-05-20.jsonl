{"id": "2505.11533", "pdf": "https://arxiv.org/pdf/2505.11533", "abs": "https://arxiv.org/abs/2505.11533", "authors": ["Jinqiang Wang", "Huansheng Ning", "Tao Zhu", "Jianguo Ding"], "title": "A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism", "categories": ["cs.CL"], "comment": null, "summary": "In the tourism domain, Large Language Models (LLMs) often struggle to mine\nimplicit user intentions from tourists' ambiguous inquiries and lack the\ncapacity to proactively guide users toward clarifying their needs. A critical\nbottleneck is the scarcity of high-quality training datasets that facilitate\nproactive questioning and implicit intention mining. While recent advances\nleverage LLM-driven data synthesis to generate such datasets and transfer\nspecialized knowledge to downstream models, existing approaches suffer from\nseveral shortcomings: (1) lack of adaptation to the tourism domain, (2) skewed\ndistributions of detail levels in initial inquiries, (3) contextual redundancy\nin the implicit intention mining module, and (4) lack of explicit thinking\nabout tourists' emotions and intention values. Therefore, we propose SynPT (A\nData Synthesis Method Driven by LLMs for Proactive Mining of Implicit User\nIntentions in the Tourism), which constructs an LLM-driven user agent and\nassistant agent to simulate dialogues based on seed data collected from Chinese\ntourism websites. This approach addresses the aforementioned limitations and\ngenerates SynPT-Dialog, a training dataset containing explicit reasoning. The\ndataset is utilized to fine-tune a general LLM, enabling it to proactively mine\nimplicit user intentions. Experimental evaluations, conducted from both human\nand LLM perspectives, demonstrate the superiority of SynPT compared to existing\nmethods. Furthermore, we analyze key hyperparameters and present case studies\nto illustrate the practical applicability of our method, including discussions\non its adaptability to English-language scenarios. All code and data are\npublicly available."}
{"id": "2505.11550", "pdf": "https://arxiv.org/pdf/2505.11550", "abs": "https://arxiv.org/abs/2505.11550", "authors": ["Harika Abburi", "Sanmitra Bhattacharya", "Edward Bowen", "Nirmala Pudota"], "title": "AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating text that closely resembles human writing across a wide range of\nstyles and genres. However, such capabilities are prone to potential misuse,\nsuch as fake news generation, spam email creation, and misuse in academic\nassignments. As a result, accurate detection of AI-generated text and\nidentification of the model that generated it are crucial for maintaining the\nresponsible use of LLMs. In this work, we addressed two sub-tasks put forward\nby the Defactify workshop under AI-Generated Text Detection shared task at the\nAssociation for the Advancement of Artificial Intelligence (AAAI 2025): Task A\ninvolved distinguishing between human-authored or AI-generated text, while Task\nB focused on attributing text to its originating language model. For each task,\nwe proposed two neural architectures: an optimized model and a simpler variant.\nFor Task A, the optimized neural architecture achieved fifth place with $F1$\nscore of 0.994, and for Task B, the simpler neural architecture also ranked\nfifth place with $F1$ score of 0.627."}
{"id": "2505.11556", "pdf": "https://arxiv.org/pdf/2505.11556", "abs": "https://arxiv.org/abs/2505.11556", "authors": ["Yuxuan Li", "Aoi Naito", "Hirokazu Shirado"], "title": "Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "Multi-agent systems built on large language models (LLMs) promise enhanced\nproblem-solving through distributed information integration, but also risk\nreplicating collective reasoning failures observed in human groups. Yet, no\ntheory-grounded benchmark exists to systematically evaluate such failures. In\nthis paper, we introduce the Hidden Profile paradigm from social psychology as\na diagnostic testbed for multi-agent LLM systems. By distributing critical\ninformation asymmetrically across agents, the paradigm reveals how inter-agent\ndynamics support or hinder collective reasoning. We first formalize the\nparadigm for multi-agent decision-making under distributed knowledge and\ninstantiate it as a benchmark with nine tasks spanning diverse scenarios,\nincluding adaptations from prior human studies. We then conduct experiments\nwith GPT-4.1 and five other leading LLMs, including reasoning-enhanced\nvariants, showing that multi-agent systems across all models fail to match the\naccuracy of single agents given complete information. While agents' collective\nperformance is broadly comparable to that of human groups, nuanced behavioral\ndifferences emerge, such as increased sensitivity to social desirability.\nFinally, we demonstrate the paradigm's diagnostic utility by exploring a\ncooperation-contradiction trade-off in multi-agent LLM systems. We find that\nwhile cooperative agents are prone to over-coordination in collective settings,\nincreased contradiction impairs group convergence. This work contributes a\nreproducible framework for evaluating multi-agent LLM systems and motivates\nfuture research on artificial collective intelligence and human-AI interaction."}
{"id": "2505.11604", "pdf": "https://arxiv.org/pdf/2505.11604", "abs": "https://arxiv.org/abs/2505.11604", "authors": ["Kyudan Jung", "Hojun Cho", "Jooyeol Yun", "Jaehyeok Jang", "Jagul Choo"], "title": "Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models", "categories": ["cs.CL"], "comment": "14 pages, 6 figures", "summary": "Existing research on large language models (LLMs) for PowerPoint\npredominantly focuses on slide generation, overlooking the common yet tedious\ntask of editing existing slides. We introduce Talk-to-Your-Slides, an\nLLM-powered agent that directly edits slides within active PowerPoint sessions\nthrough COM communication. Our system employs a two-level approach: (1)\nhigh-level processing where an LLM agent interprets instructions and formulates\nediting plans, and (2) low-level execution where Python scripts directly\nmanipulate PowerPoint objects. Unlike previous methods relying on predefined\noperations, our approach enables more flexible and contextually-aware editing.\nTo facilitate evaluation, we present TSBench, a human-annotated dataset of 379\ndiverse editing instructions with corresponding slide variations. Experimental\nresults demonstrate that Talk-to-Your-Slides significantly outperforms baseline\nmethods in execution success rate, instruction fidelity, and editing\nefficiency. Our code and benchmark are available at\nhttps://anonymous.4open.science/r/talk-to-your-slides/"}
{"id": "2505.11584", "pdf": "https://arxiv.org/pdf/2505.11584", "abs": "https://arxiv.org/abs/2505.11584", "authors": ["Manuel Cherep", "Pattie Maes", "Nikhil Singh"], "title": "LLM Agents Are Hypersensitive to Nudges", "categories": ["cs.AI"], "comment": "33 pages, 28 figures", "summary": "LLMs are being set loose in complex, real-world environments involving\nsequential decision-making and tool use. Often, this involves making choices on\nbehalf of human users. However, not much is known about the distribution of\nsuch choices, and how susceptible they are to different choice architectures.\nWe perform a case study with a few such LLM models on a multi-attribute tabular\ndecision-making problem, under canonical nudges such as the default option,\nsuggestions, and information highlighting, as well as additional prompting\nstrategies. We show that, despite superficial similarities to human choice\ndistributions, such models differ in subtle but important ways. First, they\nshow much higher susceptibility to the nudges. Second, they diverge in points\nearned, being affected by factors like the idiosyncrasy of available prizes.\nThird, they diverge in information acquisition strategies: e.g. incurring\nsubstantial cost to reveal too much information, or selecting without revealing\nany. Moreover, we show that simple prompt strategies like zero-shot chain of\nthought (CoT) can shift the choice distribution, and few-shot prompting with\nhuman data can induce greater alignment. Yet, none of these methods resolve the\nsensitivity of these models to nudges. Finally, we show how optimal nudges\noptimized with a human resource-rational model can similarly increase LLM\nperformance for some models. All these findings suggest that behavioral tests\nare needed before deploying models as agents or assistants acting on behalf of\nusers in complex environments."}
{"id": "2505.11523", "pdf": "https://arxiv.org/pdf/2505.11523", "abs": "https://arxiv.org/abs/2505.11523", "authors": ["Zhenxing Dou", "Yijiao Wang", "Tao Zou", "Zhiwei Chen", "Fei Liu", "Peng Wang", "Weisheng Zhao"], "title": "PRIME: Physics-Related Intelligent Mixture of Experts for Transistor Characteristics Prediction", "categories": ["cs.LG"], "comment": "8 pages, 6figures", "summary": "In recent years, machine learning has been extensively applied to data\nprediction during process ramp-up, with a particular focus on transistor\ncharacteristics for circuit design and manufacture. However, capturing the\nnonlinear current response across multiple operating regions remains a\nchallenge for neural networks. To address such challenge, a novel machine\nlearning framework, PRIME (Physics-Related Intelligent Mixture of Experts), is\nproposed to capture and integrate complex regional characteristics. In essence,\nour framework incorporates physics-based knowledge with data-driven\nintelligence. By leveraging a dynamic weighting mechanism in its gating\nnetwork, PRIME adaptively activates the suitable expert model based on distinct\ninput data features. Extensive evaluations are conducted on various\ngate-all-around (GAA) structures to examine the effectiveness of PRIME and\nconsiderable improvements (60\\%-84\\%) in prediction accuracy are shown over\nstate-of-the-art models."}
{"id": "2505.11613", "pdf": "https://arxiv.org/pdf/2505.11613", "abs": "https://arxiv.org/abs/2505.11613", "authors": ["Xiaomin Li", "Mingye Gao", "Yuexing Hao", "Taoran Li", "Guangya Wan", "Zihan Wang", "Yijun Wang"], "title": "MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Clinical guidelines, typically structured as decision trees, are central to\nevidence-based medical practice and critical for ensuring safe and accurate\ndiagnostic decision-making. However, it remains unclear whether Large Language\nModels (LLMs) can reliably follow such structured protocols. In this work, we\nintroduce MedGUIDE, a new benchmark for evaluating LLMs on their ability to\nmake guideline-consistent clinical decisions. MedGUIDE is constructed from 55\ncurated NCCN decision trees across 17 cancer types and uses clinical scenarios\ngenerated by LLMs to create a large pool of multiple-choice diagnostic\nquestions. We apply a two-stage quality selection process, combining\nexpert-labeled reward models and LLM-as-a-judge ensembles across ten clinical\nand linguistic criteria, to select 7,747 high-quality samples. We evaluate 25\nLLMs spanning general-purpose, open-source, and medically specialized models,\nand find that even domain-specific LLMs often underperform on tasks requiring\nstructured guideline adherence. We also test whether performance can be\nimproved via in-context guideline inclusion or continued pretraining. Our\nfindings underscore the importance of MedGUIDE in assessing whether LLMs can\noperate safely within the procedural frameworks expected in real-world clinical\nsettings."}
{"id": "2505.11610", "pdf": "https://arxiv.org/pdf/2505.11610", "abs": "https://arxiv.org/abs/2505.11610", "authors": ["Asher Moldwin", "Amarda Shehu"], "title": "Foundation Models for AI-Enabled Biological Design", "categories": ["cs.AI", "cs.LG", "q-bio.BM", "q-bio.GN"], "comment": "Published as part of the workshop proceedings at AAAI 2025 in the\n  workshop \"Foundation Models for Biological Discoveries\"", "summary": "This paper surveys foundation models for AI-enabled biological design,\nfocusing on recent developments in applying large-scale, self-supervised models\nto tasks such as protein engineering, small molecule design, and genomic\nsequence design. Though this domain is evolving rapidly, this survey presents\nand discusses a taxonomy of current models and methods. The focus is on\nchallenges and solutions in adapting these models for biological applications,\nincluding biological sequence modeling architectures, controllability in\ngeneration, and multi-modal integration. The survey concludes with a discussion\nof open problems and future directions, offering concrete next-steps to improve\nthe quality of biological sequence generation."}
{"id": "2505.11561", "pdf": "https://arxiv.org/pdf/2505.11561", "abs": "https://arxiv.org/abs/2505.11561", "authors": ["Tianyu Sun"], "title": "Policy Gradient with Second Order Momentum", "categories": ["cs.LG", "cs.NA", "math.NA", "math.OC"], "comment": null, "summary": "We develop Policy Gradient with Second-Order Momentum (PG-SOM), a lightweight\nsecond-order optimisation scheme for reinforcement-learning policies. PG-SOM\naugments the classical REINFORCE update with two exponentially weighted\nstatistics: a first-order gradient average and a diagonal approximation of the\nHessian. By preconditioning the gradient with this curvature estimate, the\nmethod adaptively rescales each parameter, yielding faster and more stable\nascent of the expected return. We provide a concise derivation, establish that\nthe diagonal Hessian estimator is unbiased and positive-definite under mild\nregularity assumptions, and prove that the resulting update is a descent\ndirection in expectation. Numerical experiments on standard control benchmarks\nshow up to a 2.1x increase in sample efficiency and a substantial reduction in\nvariance compared to first-order and Fisher-matrix baselines. These results\nindicate that even coarse second-order information can deliver significant\npractical gains while incurring only D memory overhead for a D-parameter\npolicy. All code and reproducibility scripts will be made publicly available."}
{"id": "2505.11615", "pdf": "https://arxiv.org/pdf/2505.11615", "abs": "https://arxiv.org/abs/2505.11615", "authors": ["Jian-Qiao Zhu", "Haijiang Yan", "Thomas L. Griffiths"], "title": "Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Changing the behavior of large language models (LLMs) can be as\nstraightforward as editing the Transformer's residual streams using\nappropriately constructed \"steering vectors.\" These modifications to internal\nneural activations, a form of representation engineering, offer an effective\nand targeted means of influencing model behavior without retraining or\nfine-tuning the model. But how can such steering vectors be systematically\nidentified? We propose a principled approach for uncovering steering vectors by\naligning latent representations elicited through behavioral methods\n(specifically, Markov chain Monte Carlo with LLMs) with their neural\ncounterparts. To evaluate this approach, we focus on extracting latent risk\npreferences from LLMs and steering their risk-related outputs using the aligned\nrepresentations as steering vectors. We show that the resulting steering\nvectors successfully and reliably modulate LLM outputs in line with the\ntargeted behavior."}
{"id": "2505.11611", "pdf": "https://arxiv.org/pdf/2505.11611", "abs": "https://arxiv.org/abs/2505.11611", "authors": ["Bofan Gong", "Shiyang Lai", "Dawn Song"], "title": "Probing the Vulnerability of Large Language Models to Polysemantic Interventions", "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Polysemanticity -- where individual neurons encode multiple unrelated\nfeatures -- is a well-known characteristic of large neural networks and remains\na central challenge in the interpretability of language models. At the same\ntime, its implications for model safety are also poorly understood. Leveraging\nrecent advances in sparse autoencoders, we investigate the polysemantic\nstructure of two small models (Pythia-70M and GPT-2-Small) and evaluate their\nvulnerability to targeted, covert interventions at the prompt, feature, token,\nand neuron levels. Our analysis reveals a consistent polysemantic topology\nshared across both models. Strikingly, we demonstrate that this structure can\nbe exploited to mount effective interventions on two larger, black-box\ninstruction-tuned models (LLaMA3.1-8B-Instruct and Gemma-2-9B-Instruct). These\nfindings suggest not only the generalizability of the interventions but also\npoint to a stable and transferable polysemantic structure that could\npotentially persist across architectures and training regimes."}
{"id": "2505.11564", "pdf": "https://arxiv.org/pdf/2505.11564", "abs": "https://arxiv.org/abs/2505.11564", "authors": ["Diego Granziol"], "title": "HessFormer: Hessians at Foundation Scale", "categories": ["cs.LG", "stat.ML"], "comment": "9 pages", "summary": "Whilst there have been major advancements in the field of first order\noptimisation of deep learning models, where state of the art open source\nmixture of expert models go into the hundreds of billions of parameters,\nmethods that rely on Hessian vector products, are still limited to run on a\nsingle GPU and thus cannot even work for models in the billion parameter range.\nWe release a software package \\textbf{HessFormer}, which integrates nicely with\nthe well known Transformers package and allows for distributed hessian vector\ncomputation across a single node with multiple GPUs. Underpinning our\nimplementation is a distributed stochastic lanczos quadrature algorithm, which\nwe release for public consumption. Using this package we investigate the\nHessian spectral density of the recent Deepseek $70$bn parameter model."}
{"id": "2505.11626", "pdf": "https://arxiv.org/pdf/2505.11626", "abs": "https://arxiv.org/abs/2505.11626", "authors": ["Udita Patel", "Rutu Mulkar", "Jay Roberts", "Cibi Chakravarthy Senthilkumar", "Sujay Gandhi", "Xiaofei Zheng", "Naumaan Nayyar", "Rafael Castrillo"], "title": "THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "We propose THELMA (Task Based Holistic Evaluation of Large Language Model\nApplications), a reference free framework for RAG (Retrieval Augmented\ngeneration) based question answering (QA) applications. THELMA consist of six\ninterdependent metrics specifically designed for holistic, fine grained\nevaluation of RAG QA applications. THELMA framework helps developers and\napplication owners evaluate, monitor and improve end to end RAG QA pipelines\nwithout requiring labelled sources or reference responses.We also present our\nfindings on the interplay of the proposed THELMA metrics, which can be\ninterpreted to identify the specific RAG component needing improvement in QA\napplications."}
{"id": "2505.11612", "pdf": "https://arxiv.org/pdf/2505.11612", "abs": "https://arxiv.org/abs/2505.11612", "authors": ["Hung Nguyen", "Alireza Rahimi", "Veronica Whitford", "Hélène Fournier", "Irina Kondratova", "René Richard", "Hung Cao"], "title": "Heart2Mind: Human-Centered Contestable Psychiatric Disorder Diagnosis System using Wearable ECG Monitors", "categories": ["cs.AI", "cs.HC"], "comment": "41 pages", "summary": "Psychiatric disorders affect millions globally, yet their diagnosis faces\nsignificant challenges in clinical practice due to subjective assessments and\naccessibility concerns, leading to potential delays in treatment. To help\naddress this issue, we present Heart2Mind, a human-centered contestable\npsychiatric disorder diagnosis system using wearable electrocardiogram (ECG)\nmonitors. Our approach leverages cardiac biomarkers, particularly heart rate\nvariability (HRV) and R-R intervals (RRI) time series, as objective indicators\nof autonomic dysfunction in psychiatric conditions. The system comprises three\nkey components: (1) a Cardiac Monitoring Interface (CMI) for real-time data\nacquisition from Polar H9/H10 devices; (2) a Multi-Scale Temporal-Frequency\nTransformer (MSTFT) that processes RRI time series through integrated\ntime-frequency domain analysis; (3) a Contestable Diagnosis Interface (CDI)\ncombining Self-Adversarial Explanations (SAEs) with contestable Large Language\nModels (LLMs). Our MSTFT achieves 91.7% accuracy on the HRV-ACC dataset using\nleave-one-out cross-validation, outperforming state-of-the-art methods. SAEs\nsuccessfully detect inconsistencies in model predictions by comparing\nattention-based and gradient-based explanations, while LLMs enable clinicians\nto validate correct predictions and contest erroneous ones. This work\ndemonstrates the feasibility of combining wearable technology with Explainable\nArtificial Intelligence (XAI) and contestable LLMs to create a transparent,\ncontestable system for psychiatric diagnosis that maintains clinical oversight\nwhile leveraging advanced AI capabilities. Our implementation is publicly\navailable at: https://github.com/Analytics-Everywhere-Lab/heart2mind."}
{"id": "2505.11567", "pdf": "https://arxiv.org/pdf/2505.11567", "abs": "https://arxiv.org/abs/2505.11567", "authors": ["Tianyi Shi", "Zhu Meng", "Yue Chen", "Siyang Zheng", "Fei Su", "Jin Huang", "Changrui Ren", "Zhicheng Zhao"], "title": "Beyond Time: Cross-Dimensional Frequency Supervision for Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series forecasting plays a crucial role in various fields, and the\nmethods based on frequency domain analysis have become an important branch.\nHowever, most existing studies focus on the design of elaborate model\narchitectures and are often tailored for limited datasets, still lacking\nuniversality. Besides, the assumption of independent and identically\ndistributed (IID) data also contradicts the strong correlation of the time\ndomain labels. To address these issues, abandoning time domain supervision, we\npropose a purely frequency domain supervision approach named cross-dimensional\nfrequency (X-Freq) loss. Specifically, based on a statistical phenomenon, we\nfirst prove that the information entropy of the time series is higher than its\nspectral entropy, which implies higher certainty in frequency domain and thus\ncan provide better supervision. Secondly, the Fourier Transform and the Wavelet\nTransform are applied to the time dimension and the channel dimension of the\ntime series respectively, to capture the long-term and short-term frequency\nvariations as well as the spatial configuration features. Thirdly, the loss\nbetween predictions and targets is uniformly computed in the frequency domain.\nMoreover, we plug-and-play incorporate X-Freq into multiple advanced\nforecasting models and compare on 14 real-world datasets. The experimental\nresults demonstrate that, without making any modification to the original\narchitectures or hyperparameters, X-Freq can improve the forecasting\nperformance by an average of 3.3% on long-term forecasting datasets and 27.7%\non short-term ones, showcasing superior generality and practicality. The code\nwill be released publicly."}
{"id": "2505.11628", "pdf": "https://arxiv.org/pdf/2505.11628", "abs": "https://arxiv.org/abs/2505.11628", "authors": ["Berkcan Kapusuzoglu", "Supriyo Chakraborty", "Chia-Hsuan Lee", "Sambit Sahu"], "title": "Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation", "categories": ["cs.CL", "cs.LG"], "comment": "Submitted to NeurIPS 2025", "summary": "Supervised fine-tuning (SFT) using expert demonstrations often suffer from\nthe imitation problem, where the model learns to reproduce the correct\nresponses without \\emph{understanding} the underlying rationale. To address\nthis limitation, we propose \\textsc{Critique-Guided Distillation (CGD)}, a\nnovel multi-stage framework that integrates teacher model generated\n\\emph{explanatory critiques} and \\emph{refined responses} into the SFT process.\nA student model is then trained to map the triplet of prompt, teacher critique,\nand its own initial response to the corresponding refined teacher response,\nthereby learning both \\emph{what} to imitate and \\emph{why}. Using\nentropy-based analysis, we show that \\textsc{CGD} reduces refinement\nuncertainty and can be interpreted as a Bayesian posterior update. We perform\nextensive empirical evaluation of \\textsc{CGD}, on variety of benchmark tasks,\nand demonstrate significant gains on both math (AMC23 +17.5%) and language\nunderstanding tasks (MMLU-Pro +6.3%), while successfully mitigating the format\ndrift issues observed in previous critique fine-tuning (CFT) techniques."}
{"id": "2505.11614", "pdf": "https://arxiv.org/pdf/2505.11614", "abs": "https://arxiv.org/abs/2505.11614", "authors": ["Jian-Qiao Zhu", "Hanbo Xie", "Dilip Arumugam", "Robert C. Wilson", "Thomas L. Griffiths"], "title": "Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "A central goal of cognitive modeling is to develop models that not only\npredict human behavior but also provide insight into the underlying cognitive\nmechanisms. While neural network models trained on large-scale behavioral data\noften achieve strong predictive performance, they typically fall short in\noffering interpretable explanations of the cognitive processes they capture. In\nthis work, we explore the potential of pretrained large language models (LLMs)\nto serve as dual-purpose cognitive models--capable of both accurate prediction\nand interpretable explanation in natural language. Specifically, we employ\nreinforcement learning with outcome-based rewards to guide LLMs toward\ngenerating explicit reasoning traces for explaining human risky choices. Our\nfindings demonstrate that this approach produces high-quality explanations\nalongside strong quantitative predictions of human decisions."}
{"id": "2505.11569", "pdf": "https://arxiv.org/pdf/2505.11569", "abs": "https://arxiv.org/abs/2505.11569", "authors": ["Pooja Mangal", "Sudaksh Kalra", "Dolly Sapra"], "title": "Towards Adaptive Deep Learning: Model Elasticity via Prune-and-Grow CNN Architectures", "categories": ["cs.LG", "cs.AI"], "comment": "50 Pages, 11 figures, Preprint", "summary": "Deploying deep convolutional neural networks (CNNs) on resource-constrained\ndevices presents significant challenges due to their high computational demands\nand rigid, static architectures. To overcome these limitations, this thesis\nexplores methods for enabling CNNs to dynamically adjust their computational\ncomplexity based on available hardware resources. We introduce adaptive CNN\narchitectures capable of scaling their capacity at runtime, thus efficiently\nbalancing performance and resource utilization. To achieve this adaptability,\nwe propose a structured pruning and dynamic re-construction approach that\ncreates nested subnetworks within a single CNN model. This approach allows the\nnetwork to dynamically switch between compact and full-sized configurations\nwithout retraining, making it suitable for deployment across varying hardware\nplatforms. Experiments conducted across multiple CNN architectures including\nVGG-16, AlexNet, ResNet-20, and ResNet-56 on CIFAR-10 and Imagenette datasets\ndemonstrate that adaptive models effectively maintain or even enhance\nperformance under varying computational constraints. Our results highlight that\nembedding adaptability directly into CNN architectures significantly improves\ntheir robustness and flexibility, paving the way for efficient real-world\ndeployment in diverse computational environments."}
{"id": "2505.11643", "pdf": "https://arxiv.org/pdf/2505.11643", "abs": "https://arxiv.org/abs/2505.11643", "authors": ["Xiang Fu"], "title": "Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models? Evidence from a Four-Stage Curriculum on GPT-2", "categories": ["cs.CL"], "comment": null, "summary": "We demonstrate that a developmentally ordered curriculum markedly improves\nreasoning transparency and sample-efficiency in small language models (SLMs).\nConcretely, we train Cognivolve, a 124 M-parameter GPT-2 model, on a four-stage\nsyllabus that ascends from lexical matching to multi-step symbolic inference\nand then evaluate it without any task-specific fine-tuning. Cognivolve reaches\ntarget accuracy in half the optimization steps of a single-phase baseline,\nactivates an order-of-magnitude more gradient-salient reasoning heads, and\nshifts those heads toward deeper layers, yielding higher-entropy attention that\nbalances local and long-range context. The same curriculum applied out of order\nor with optimizer resets fails to reproduce these gains, confirming that\nprogression--not extra compute--drives the effect. We also identify open\nchallenges: final-answer success still lags a conventional run by about 30%,\nand our saliency probe under-detects verbal-knowledge heads in the hardest\nstage, suggesting directions for mixed-stage fine-tuning and probe expansion."}
{"id": "2505.11618", "pdf": "https://arxiv.org/pdf/2505.11618", "abs": "https://arxiv.org/abs/2505.11618", "authors": ["Pengrui Quan", "Brian Wang", "Kang Yang", "Liying Han", "Mani Srivastava"], "title": "Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges", "categories": ["cs.AI", "cs.LG", "eess.SP"], "comment": null, "summary": "Spatiotemporal reasoning plays a key role in Cyber-Physical Systems (CPS).\nDespite advances in Large Language Models (LLMs) and Large Reasoning Models\n(LRMs), their capacity to reason about complex spatiotemporal signals remains\nunderexplored. This paper proposes a hierarchical SpatioTemporal reAsoning\nbenchmaRK, STARK, to systematically evaluate LLMs across three levels of\nreasoning complexity: state estimation (e.g., predicting field variables,\nlocalizing and tracking events in space and time), spatiotemporal reasoning\nover states (e.g., inferring spatial-temporal relationships), and\nworld-knowledge-aware reasoning that integrates contextual and domain knowledge\n(e.g., intent prediction, landmark-aware navigation). We curate 26 distinct\nspatiotemporal tasks with diverse sensor modalities, comprising 14,552\nchallenges where models answer directly or by Python Code Interpreter.\nEvaluating 3 LRMs and 8 LLMs, we find LLMs achieve limited success in tasks\nrequiring geometric reasoning (e.g., multilateration or triangulation),\nparticularly as complexity increases. Surprisingly, LRMs show robust\nperformance across tasks with various levels of difficulty, often competing or\nsurpassing traditional first-principle-based methods. Our results show that in\nreasoning tasks requiring world knowledge, the performance gap between LLMs and\nLRMs narrows, with some LLMs even surpassing LRMs. However, the LRM o3 model\ncontinues to achieve leading performance across all evaluated tasks, a result\nattributed primarily to the larger size of the reasoning models. STARK\nmotivates future innovations in model architectures and reasoning paradigms for\nintelligent CPS by providing a structured framework to identify limitations in\nthe spatiotemporal reasoning of LLMs and LRMs."}
{"id": "2505.11570", "pdf": "https://arxiv.org/pdf/2505.11570", "abs": "https://arxiv.org/abs/2505.11570", "authors": ["Chongyang Tan", "Ruoqi Wen", "Rongpeng Li", "Zhifeng Zhao", "Ekram Hossain", "Honggang Zhang"], "title": "Tool-Aided Evolutionary LLM for Generative Policy Toward Efficient Resource Management in Wireless Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) enables distributed model training across edge\ndevices in a privacy-friendly manner. However, its efficiency heavily depends\non effective device selection and high-dimensional resource allocation in\ndynamic and heterogeneous wireless environments. Conventional methods demand a\nconfluence of domain-specific expertise, extensive hyperparameter tuning,\nand/or heavy interaction cost. This paper proposes a Tool-aided Evolutionary\nLarge Language Model (T-ELLM) framework to generate a qualified policy for\ndevice selection in a wireless FL environment. Unlike conventional optimization\nmethods, T-ELLM leverages natural language-based scenario prompts to enhance\ngeneralization across varying network conditions. The framework decouples the\njoint optimization problem mathematically, enabling tractable learning of\ndevice selection policies while delegating resource allocation to convex\noptimization tools. To improve adaptability, T-ELLM integrates a\nsample-efficient, model-based virtual learning environment that captures the\nrelationship between device selection and learning performance, facilitating\nsubsequent group relative policy optimization. This concerted approach reduces\nreliance on real-world interactions, minimizing communication overhead while\nmaintaining high-fidelity decision-making. Theoretical analysis proves that the\ndiscrepancy between virtual and real environments is bounded, ensuring the\nadvantage function learned in the virtual environment maintains a provably\nsmall deviation from real-world conditions. Experimental results demonstrate\nthat T-ELLM outperforms benchmark methods in energy efficiency and exhibits\nrobust adaptability to environmental changes."}
{"id": "2505.11665", "pdf": "https://arxiv.org/pdf/2505.11665", "abs": "https://arxiv.org/abs/2505.11665", "authors": ["Shubham Vatsal", "Harsh Dubey", "Aditi Singh"], "title": "Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance across\na wide range of Natural Language Processing (NLP) tasks. However, ensuring\ntheir effectiveness across multiple languages presents unique challenges.\nMultilingual prompt engineering has emerged as a key approach to enhance LLMs'\ncapabilities in diverse linguistic settings without requiring extensive\nparameter re-training or fine-tuning. With growing interest in multilingual\nprompt engineering over the past two to three years, researchers have explored\nvarious strategies to improve LLMs' performance across languages and NLP tasks.\nBy crafting structured natural language prompts, researchers have successfully\nextracted knowledge from LLMs across different languages, making these\ntechniques an accessible pathway for a broader audience, including those\nwithout deep expertise in machine learning, to harness the capabilities of\nLLMs. In this paper, we survey and categorize different multilingual prompting\ntechniques based on the NLP tasks they address across a diverse set of datasets\nthat collectively span around 250 languages. We further highlight the LLMs\nemployed, present a taxonomy of approaches and discuss potential\nstate-of-the-art (SoTA) methods for specific multilingual datasets.\nAdditionally, we derive a range of insights across language families and\nresource levels (high-resource vs. low-resource), including analyses such as\nthe distribution of NLP tasks by language resource type and the frequency of\nprompting methods across different language families. Our survey reviews 36\nresearch papers covering 39 prompting techniques applied to 30 multilingual NLP\ntasks, with the majority of these studies published in the last two years."}
{"id": "2505.11646", "pdf": "https://arxiv.org/pdf/2505.11646", "abs": "https://arxiv.org/abs/2505.11646", "authors": ["Evelyn Duesterwald", "Siyu Huo", "Vatche Isahagian", "K. R. Jayaram", "Ritesh Kumar", "Vinod Muthusamy", "Punleuk Oum", "Debashish Saha", "Gegi Thomas", "Praveen Venkateswaran"], "title": "FLOW-BENCH: Towards Conversational Generation of Enterprise Workflows", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Business process automation (BPA) that leverages Large Language Models (LLMs)\nto convert natural language (NL) instructions into structured business process\nartifacts is becoming a hot research topic. This paper makes two technical\ncontributions -- (i) FLOW-BENCH, a high quality dataset of paired natural\nlanguage instructions and structured business process definitions to evaluate\nNL-based BPA tools, and support bourgeoning research in this area, and (ii)\nFLOW-GEN, our approach to utilize LLMs to translate natural language into an\nintermediate representation with Python syntax that facilitates final\nconversion into widely adopted business process definition languages, such as\nBPMN and DMN. We bootstrap FLOW-BENCH by demonstrating how it can be used to\nevaluate the components of FLOW-GEN across eight LLMs of varying sizes. We hope\nthat FLOW-GEN and FLOW-BENCH catalyze further research in BPA making it more\naccessible to novice and expert users."}
{"id": "2505.11574", "pdf": "https://arxiv.org/pdf/2505.11574", "abs": "https://arxiv.org/abs/2505.11574", "authors": ["Zhen Li", "Yupeng Su", "Songmiao Wang", "Runming Yang", "Congkai Xie", "Aofan Liu", "Ming Li", "Jiannong Cao", "Yuan Xie", "Ngai Wong", "Hongxia Yang"], "title": "InfiJanice: Joint Analysis and In-situ Correction Engine for Quantization-Induced Math Degradation in Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "23pages", "summary": "Large Language Models (LLMs) have demonstrated impressive performance on\ncomplex reasoning benchmarks such as GSM8K, MATH, and AIME. However, the\nsubstantial computational demands of these tasks pose significant challenges\nfor real-world deployment. Model quantization has emerged as a promising\napproach to reduce memory footprint and inference latency by representing\nweights and activations with lower bit-widths. In this work, we conduct a\ncomprehensive study of mainstream quantization methods(e.g., AWQ, GPTQ,\nSmoothQuant) on the most popular open-sourced models (e.g., Qwen2.5, LLaMA3\nseries), and reveal that quantization can degrade mathematical reasoning\naccuracy by up to 69.81%. To better understand this degradation, we develop an\nautomated assignment and judgment pipeline that qualitatively categorizes\nfailures into four error types and quantitatively identifies the most impacted\nreasoning capabilities. Building on these findings, we employ an automated\ndata-curation pipeline to construct a compact \"Silver Bullet\" datasets.\nTraining a quantized model on as few as 332 carefully selected examples for\njust 3-5 minutes on a single GPU is enough to restore its reasoning accuracy to\nmatch that of the full-precision baseline."}
{"id": "2505.11679", "pdf": "https://arxiv.org/pdf/2505.11679", "abs": "https://arxiv.org/abs/2505.11679", "authors": ["Zhibo Hu", "Chen Wang", "Yanfeng Shu", "Hye-Young Paik", "Liming Zhu"], "title": "Ambiguity Resolution in Text-to-Structured Data Mapping", "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "15 pages, 11 figures", "summary": "Ambiguity in natural language is a significant obstacle for achieving\naccurate text to structured data mapping through large language models (LLMs),\nwhich affects the performance of tasks such as mapping text to agentic tool\ncalling and text-to-SQL queries. Existing methods of ambiguity handling either\nexploit ReACT framework to produce the correct mapping through trial and error,\nor supervised fine tuning to guide models to produce a biased mapping to\nimprove certain tasks. In this paper, we adopt a different approach that\ncharacterizes the representation difference of ambiguous text in the latent\nspace and leverage the difference to identify ambiguity before mapping them to\nstructured data. To detect ambiguity of a sentence, we focused on the\nrelationship between ambiguous questions and their interpretations and what\ncause the LLM ignore multiple interpretations. Different to the distance\ncalculated by dense embedding vectors, we utilize the observation that\nambiguity is caused by concept missing in latent space of LLM to design a new\ndistance measurement, computed through the path kernel by the integral of\ngradient values for each concepts from sparse-autoencoder (SAE) under each\nstate. We identify patterns to distinguish ambiguous questions with this\nmeasurement. Based on our observation, We propose a new framework to improve\nthe performance of LLMs on ambiguous agentic tool calling through missing\nconcepts prediction."}
{"id": "2505.11661", "pdf": "https://arxiv.org/pdf/2505.11661", "abs": "https://arxiv.org/abs/2505.11661", "authors": ["Zihan Ye", "Oleg Arenz", "Kristian Kersting"], "title": "Learning from Less: Guiding Deep Reinforcement Learning with Differentiable Symbolic Planning", "categories": ["cs.AI"], "comment": "conference paper, 9 pages", "summary": "When tackling complex problems, humans naturally break them down into\nsmaller, manageable subtasks and adjust their initial plans based on\nobservations. For instance, if you want to make coffee at a friend's place, you\nmight initially plan to grab coffee beans, go to the coffee machine, and pour\nthem into the machine. Upon noticing that the machine is full, you would skip\nthe initial steps and proceed directly to brewing. In stark contrast, state of\nthe art reinforcement learners, such as Proximal Policy Optimization (PPO),\nlack such prior knowledge and therefore require significantly more training\nsteps to exhibit comparable adaptive behavior. Thus, a central research\nquestion arises: \\textit{How can we enable reinforcement learning (RL) agents\nto have similar ``human priors'', allowing the agent to learn with fewer\ntraining interactions?} To address this challenge, we propose differentiable\nsymbolic planner (Dylan), a novel framework that integrates symbolic planning\ninto Reinforcement Learning. Dylan serves as a reward model that dynamically\nshapes rewards by leveraging human priors, guiding agents through intermediate\nsubtasks, thus enabling more efficient exploration. Beyond reward shaping,\nDylan can work as a high level planner that composes primitive policies to\ngenerate new behaviors while avoiding common symbolic planner pitfalls such as\ninfinite execution loops. Our experimental evaluations demonstrate that Dylan\nsignificantly improves RL agents' performance and facilitates generalization to\nunseen tasks."}
{"id": "2505.11576", "pdf": "https://arxiv.org/pdf/2505.11576", "abs": "https://arxiv.org/abs/2505.11576", "authors": ["Shuchen Wu", "Stephan Alaniz", "Shyamgopal Karthik", "Peter Dayan", "Eric Schulz", "Zeynep Akata"], "title": "Concept-Guided Interpretability via Neural Chunking", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "35 pages, 32 figures. arXiv admin note: text overlap with\n  arXiv:2502.01803", "summary": "Neural networks are often black boxes, reflecting the significant challenge\nof understanding their internal workings. We propose a different perspective\nthat challenges the prevailing view: rather than being inscrutable, neural\nnetworks exhibit patterns in their raw population activity that mirror\nregularities in the training data. We refer to this as the Reflection\nHypothesis and provide evidence for this phenomenon in both simple recurrent\nneural networks (RNNs) and complex large language models (LLMs). Building on\nthis insight, we propose to leverage cognitively-inspired methods of chunking\nto segment high-dimensional neural population dynamics into interpretable units\nthat reflect underlying concepts. We propose three methods to extract these\nemerging entities, complementing each other based on label availability and\ndimensionality. Discrete sequence chunking (DSC) creates a dictionary of\nentities; population averaging (PA) extracts recurring entities that correspond\nto known labels; and unsupervised chunk discovery (UCD) can be used when labels\nare absent. We demonstrate the effectiveness of these methods in extracting\nentities across varying model sizes, ranging from inducing compositionality in\nRNNs to uncovering recurring neural population states in large models with\ndiverse architectures, and illustrate their advantage over other methods.\nThroughout, we observe a robust correspondence between the extracted entities\nand concrete or abstract concepts. Artificially inducing the extracted entities\nin neural populations effectively alters the network's generation of associated\nconcepts. Our work points to a new direction for interpretability, one that\nharnesses both cognitive principles and the structure of naturalistic data to\nreveal the hidden computations of complex learning systems, gradually\ntransforming them from black boxes into systems we can begin to understand."}
{"id": "2505.11683", "pdf": "https://arxiv.org/pdf/2505.11683", "abs": "https://arxiv.org/abs/2505.11683", "authors": ["Susanna Rücker", "Alan Akbik"], "title": "Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (The 63rd Annual Meeting of the Association for\n  Computational Linguistics)", "summary": "Entity disambiguation (ED) is the task of linking mentions in text to\ncorresponding entries in a knowledge base. Dual Encoders address this by\nembedding mentions and label candidates in a shared embedding space and\napplying a similarity metric to predict the correct label. In this work, we\nfocus on evaluating key design decisions for Dual Encoder-based ED, such as its\nloss function, similarity metric, label verbalization format, and negative\nsampling strategy. We present the resulting model VerbalizED, a document-level\nDual Encoder model that includes contextual label verbalizations and efficient\nhard negative sampling. Additionally, we explore an iterative prediction\nvariant that aims to improve the disambiguation of challenging data points.\nComprehensive experiments on AIDA-Yago validate the effectiveness of our\napproach, offering insights into impactful design choices that result in a new\nState-of-the-Art system on the ZELDA benchmark."}
{"id": "2505.11698", "pdf": "https://arxiv.org/pdf/2505.11698", "abs": "https://arxiv.org/abs/2505.11698", "authors": ["Antoine Bigeard", "Anthony Corso", "Mykel Kochenderfer"], "title": "Conditional Deep Generative Models for Belief State Planning", "categories": ["cs.AI"], "comment": null, "summary": "Partially observable Markov decision processes (POMDPs) are used to model a\nwide range of applications, including robotics, autonomous vehicles, and\nsubsurface problems. However, accurately representing the belief is difficult\nfor POMDPs with high-dimensional states. In this paper, we propose a novel\napproach that uses conditional deep generative models (cDGMs) to represent the\nbelief. Unlike traditional belief representations, cDGMs are well-suited for\nhigh-dimensional states and large numbers of observations, and they can\ngenerate an arbitrary number of samples from the posterior belief. We train the\ncDGMs on data produced by random rollout trajectories and show their\neffectiveness in solving a mineral exploration POMDP with a large and\ncontinuous state space. The cDGMs outperform particle filter baselines in both\ntask-agnostic measures of belief accuracy as well as in planning performance."}
{"id": "2505.11578", "pdf": "https://arxiv.org/pdf/2505.11578", "abs": "https://arxiv.org/abs/2505.11578", "authors": ["Peimian Du", "Jiabin Liu", "Xiaowei Jin", "Mengwang Zuo", "Hui Li"], "title": "Spatiotemporal Field Generation Based on Hybrid Mamba-Transformer with Physics-informed Fine-tuning", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "comment": null, "summary": "This research confronts the challenge of substantial physical equation\ndiscrepancies encountered in the generation of spatiotemporal physical fields\nthrough data-driven trained models. A spatiotemporal physical field generation\nmodel, named HMT-PF, is developed based on the hybrid Mamba-Transformer\narchitecture, incorporating unstructured grid information as input. A\nfine-tuning block, enhanced with physical information, is introduced to\neffectively reduce the physical equation discrepancies. The physical equation\nresiduals are computed through a point query mechanism for efficient gradient\nevaluation, then encoded into latent space for refinement. The fine-tuning\nprocess employs a self-supervised learning approach to achieve physical\nconsistency while maintaining essential field characteristics. Results show\nthat the hybrid Mamba-Transformer model achieves good performance in generating\nspatiotemporal fields, while the physics-informed fine-tuning mechanism further\nreduces significant physical errors effectively. A MSE-R evaluation method is\ndeveloped to assess the accuracy and realism of physical field generation."}
{"id": "2505.11690", "pdf": "https://arxiv.org/pdf/2505.11690", "abs": "https://arxiv.org/abs/2505.11690", "authors": ["Sukairaj Hafiz Imam", "Babangida Sani", "Dawit Ketema Gete", "Bedru Yimam Ahamed", "Ibrahim Said Ahmad", "Idris Abdulmumin", "Seid Muhie Yimam", "Muhammad Yahuza Bello", "Shamsuddeen Hassan Muhammad"], "title": "Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Automatic Speech Recognition (ASR) technologies have transformed\nhuman-computer interaction; however, low-resource languages in Africa remain\nsignificantly underrepresented in both research and practical applications.\nThis study investigates the major challenges hindering the development of ASR\nsystems for these languages, which include data scarcity, linguistic\ncomplexity, limited computational resources, acoustic variability, and ethical\nconcerns surrounding bias and privacy. The primary goal is to critically\nanalyze these barriers and identify practical, inclusive strategies to advance\nASR technologies within the African context. Recent advances and case studies\nemphasize promising strategies such as community-driven data collection,\nself-supervised and multilingual learning, lightweight model architectures, and\ntechniques that prioritize privacy. Evidence from pilot projects involving\nvarious African languages showcases the feasibility and impact of customized\nsolutions, which encompass morpheme-based modeling and domain-specific ASR\napplications in sectors like healthcare and education. The findings highlight\nthe importance of interdisciplinary collaboration and sustained investment to\ntackle the distinct linguistic and infrastructural challenges faced by the\ncontinent. This study offers a progressive roadmap for creating ethical,\nefficient, and inclusive ASR systems that not only safeguard linguistic\ndiversity but also improve digital accessibility and promote socioeconomic\nparticipation for speakers of African languages."}
{"id": "2505.11701", "pdf": "https://arxiv.org/pdf/2505.11701", "abs": "https://arxiv.org/abs/2505.11701", "authors": ["Shaghayegh Abedi", "Amin Jalali"], "title": "DMN-Guided Prompting: A Low-Code Framework for Controlling LLM Behavior", "categories": ["cs.AI"], "comment": "Large Language Models, Decision Model and Notation, Prompt\n  Engineering, Automated Feedback", "summary": "Large Language Models (LLMs) have shown considerable potential in automating\ndecision logic within knowledge-intensive processes. However, their\neffectiveness largely depends on the strategy and quality of prompting. Since\ndecision logic is typically embedded in prompts, it becomes challenging for end\nusers to modify or refine it. Decision Model and Notation (DMN) offers a\nstandardized graphical approach for defining decision logic in a structured,\nuser-friendly manner. This paper introduces a DMN-guided prompting framework\nthat breaks down complex decision logic into smaller, manageable components,\nguiding LLMs through structured decision pathways. We implemented the framework\nin a graduate-level course where students submitted assignments. The\nassignments and DMN models representing feedback instructions served as inputs\nto our framework. The instructor evaluated the generated feedback and labeled\nit for performance assessment. Our approach demonstrated promising results,\noutperforming chain-of-thought (CoT) prompting. Students also responded\npositively to the generated feedback, reporting high levels of perceived\nusefulness in a survey based on the Technology Acceptance Model."}
{"id": "2505.11580", "pdf": "https://arxiv.org/pdf/2505.11580", "abs": "https://arxiv.org/abs/2505.11580", "authors": ["Andrew Liu", "Axel Elaldi", "Nicholas T Franklin", "Nathan Russell", "Gurinder S Atwal", "Yih-En A Ban", "Olivia Viessmann"], "title": "Flash Invariant Point Attention", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": null, "summary": "Invariant Point Attention (IPA) is a key algorithm for geometry-aware\nmodeling in structural biology, central to many protein and RNA models.\nHowever, its quadratic complexity limits the input sequence length. We\nintroduce FlashIPA, a factorized reformulation of IPA that leverages\nhardware-efficient FlashAttention to achieve linear scaling in GPU memory and\nwall-clock time with sequence length. FlashIPA matches or exceeds standard IPA\nperformance while substantially reducing computational costs. FlashIPA extends\ntraining to previously unattainable lengths, and we demonstrate this by\nre-training generative models without length restrictions and generating\nstructures of thousands of residues. FlashIPA is available at\nhttps://github.com/flagshippioneering/flash_ipa."}
{"id": "2505.11693", "pdf": "https://arxiv.org/pdf/2505.11693", "abs": "https://arxiv.org/abs/2505.11693", "authors": ["Ana Ezquerro", "David Vilares", "Anssi Yli-Jyrä", "Carlos Gómez-Rodríguez"], "title": "Hierarchical Bracketing Encodings for Dependency Parsing as Tagging", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025. Original submission; camera-ready coming soon", "summary": "We present a family of encodings for sequence labeling dependency parsing,\nbased on the concept of hierarchical bracketing. We prove that the existing\n4-bit projective encoding belongs to this family, but it is suboptimal in the\nnumber of labels used to encode a tree. We derive an optimal hierarchical\nbracketing, which minimizes the number of symbols used and encodes projective\ntrees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also\nextend optimal hierarchical bracketing to support arbitrary non-projectivity in\na more compact way than previous encodings. Our new encodings yield competitive\naccuracy on a diverse set of treebanks."}
{"id": "2505.11718", "pdf": "https://arxiv.org/pdf/2505.11718", "abs": "https://arxiv.org/abs/2505.11718", "authors": ["Pawin Taechoyotin", "Daniel Acuna"], "title": "REMOR: Automated Peer Review Generation with LLM Reasoning and Multi-Objective Reinforcement Learning", "categories": ["cs.AI"], "comment": "18 pages, 6 figures", "summary": "AI-based peer review systems tend to produce shallow and overpraising\nsuggestions compared to human feedback. Here, we evaluate how well a reasoning\nLLM trained with multi-objective reinforcement learning (REMOR) can overcome\nthese limitations. We start by designing a multi-aspect reward function that\naligns with human evaluation of reviews. The aspects are related to the review\nitself (e.g., criticisms, novelty) and the relationship between the review and\nthe manuscript (i.e., relevance). First, we perform supervised fine-tuning of\nDeepSeek-R1-Distill-Qwen-7B using LoRA on PeerRT, a new dataset of high-quality\ntop AI conference reviews enriched with reasoning traces. We then apply Group\nRelative Policy Optimization (GRPO) to train two models: REMOR-H (with the\nhuman-aligned reward) and REMOR-U (with a uniform reward). Interestingly, the\nhuman-aligned reward penalizes aspects typically associated with strong\nreviews, leading REMOR-U to produce qualitatively more substantive feedback.\nOur results show that REMOR-U and REMOR-H achieve more than twice the average\nrewards of human reviews, non-reasoning state-of-the-art agentic multi-modal AI\nreview systems, and general commercial LLM baselines. We found that while the\nbest AI and human reviews are comparable in quality, REMOR avoids the long tail\nof low-quality human reviews. We discuss how reasoning is key to achieving\nthese improvements and release the Human-aligned Peer Review Reward (HPRR)\nfunction, the Peer Review Reasoning-enriched Traces (PeerRT) dataset, and the\nREMOR models, which we believe can help spur progress in the area."}
{"id": "2505.11589", "pdf": "https://arxiv.org/pdf/2505.11589", "abs": "https://arxiv.org/abs/2505.11589", "authors": ["Forsad Al Hossain", "Tauhidur Rahman"], "title": "A Training Framework for Optimal and Stable Training of Polynomial Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "By replacing standard non-linearities with polynomial activations, Polynomial\nNeural Networks (PNNs) are pivotal for applications such as privacy-preserving\ninference via Homomorphic Encryption (HE). However, training PNNs effectively\npresents a significant challenge: low-degree polynomials can limit model\nexpressivity, while higher-degree polynomials, crucial for capturing complex\nfunctions, often suffer from numerical instability and gradient explosion. We\nintroduce a robust and versatile training framework featuring two synergistic\ninnovations: 1) a novel Boundary Loss that exponentially penalizes activation\ninputs outside a predefined stable range, and 2) Selective Gradient Clipping\nthat effectively tames gradient magnitudes while preserving essential Batch\nNormalization statistics. We demonstrate our framework's broad efficacy by\ntraining PNNs within deep architectures composed of HE-compatible layers (e.g.,\nlinear layers, average pooling, batch normalization, as used in ResNet\nvariants) across diverse image, audio, and human activity recognition datasets.\nThese models consistently achieve high accuracy with low-degree polynomial\nactivations (such as degree 2) and, critically, exhibit stable training and\nstrong performance with polynomial degrees up to 22, where standard methods\ntypically fail or suffer severe degradation. Furthermore, the performance of\nthese PNNs achieves a remarkable parity, closely approaching that of their\noriginal ReLU-based counterparts. Extensive ablation studies validate the\ncontributions of our techniques and guide hyperparameter selection. We confirm\nthe HE-compatibility of the trained models, advancing the practical deployment\nof accurate, stable, and secure deep learning inference."}
{"id": "2505.11726", "pdf": "https://arxiv.org/pdf/2505.11726", "abs": "https://arxiv.org/abs/2505.11726", "authors": ["Shun Inadumi", "Nobuhiro Ueda", "Koichiro Yoshino"], "title": "Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures", "categories": ["cs.CL"], "comment": "ACL2025 main. Code available at https://github.com/SInadumi/mmrr", "summary": "Multimodal reference resolution, including phrase grounding, aims to\nunderstand the semantic relations between mentions and real-world objects.\nPhrase grounding between images and their captions is a well-established task.\nIn contrast, for real-world applications, it is essential to integrate textual\nand multimodal reference resolution to unravel the reference relations within\ndialogue, especially in handling ambiguities caused by pronouns and ellipses.\nThis paper presents a framework that unifies textual and multimodal reference\nresolution by mapping mention embeddings to object embeddings and selecting\nmentions or objects based on their similarity. Our experiments show that\nlearning textual reference resolution, such as coreference resolution and\npredicate-argument structure analysis, positively affects performance in\nmultimodal reference resolution. In particular, our model with coreference\nresolution performs better in pronoun phrase grounding than representative\nmodels for this task, MDETR and GLIP. Our qualitative analysis demonstrates\nthat incorporating textual reference relations strengthens the confidence\nscores between mentions, including pronouns and predicates, and objects, which\ncan reduce the ambiguities that arise in visually grounded dialogues."}
{"id": "2505.11730", "pdf": "https://arxiv.org/pdf/2505.11730", "abs": "https://arxiv.org/abs/2505.11730", "authors": ["Hao Mark Chen", "Guanxi Lu", "Yasuyuki Okoshi", "Zhiwen Mo", "Masato Motomura", "Hongxiang Fan"], "title": "Rethinking Optimal Verification Granularity for Compute-Efficient Test-Time Scaling", "categories": ["cs.AI", "cs.LG"], "comment": "Preprint. Under review", "summary": "Test-time scaling (TTS) has proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Verification plays a key role in\nTTS, simultaneously influencing (1) reasoning performance and (2) compute\nefficiency, due to the quality and computational cost of verification. In this\nwork, we challenge the conventional paradigms of verification, and make the\nfirst attempt toward systematically investigating the impact of verification\ngranularity-that is, how frequently the verifier is invoked during generation,\nbeyond verifying only the final output or individual generation steps. To this\nend, we introduce Variable Granularity Search (VG-Search), a unified algorithm\nthat generalizes beam search and Best-of-N sampling via a tunable granularity\nparameter g. Extensive experiments with VG-Search under varying compute\nbudgets, generator-verifier configurations, and task attributes reveal that\ndynamically selecting g can improve the compute efficiency and scaling\nbehavior. Building on these findings, we propose adaptive VG-Search strategies\nthat achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over\nBest-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to\nsupport future research."}
{"id": "2505.11594", "pdf": "https://arxiv.org/pdf/2505.11594", "abs": "https://arxiv.org/abs/2505.11594", "authors": ["Jintao Zhang", "Jia Wei", "Pengle Zhang", "Xiaoming Xu", "Haofeng Huang", "Haoxu Wang", "Kai Jiang", "Jun Zhu", "Jianfei Chen"], "title": "SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CV", "cs.PF"], "comment": null, "summary": "The efficiency of attention is important due to its quadratic time\ncomplexity. We enhance the efficiency of attention through two key\ncontributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to\naccelerate attention computation. Our implementation achieves 1038 TOPS on\nRTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.\nExperiments show that our FP4 attention can accelerate inference of various\nmodels in a plug-and-play way. Second, we pioneer low-bit attention to training\ntasks. Existing low-bit attention works like FlashAttention3 and SageAttention\nfocus only on inference. However, the efficiency of training large models is\nalso important. To explore whether low-bit attention can be effectively applied\nto training tasks, we design an accurate and efficient 8-bit attention for both\nforward and backward propagation. Experiments indicate that 8-bit attention\nachieves lossless performance in fine-tuning tasks but exhibits slower\nconvergence in pretraining tasks. The code will be available at\nhttps://github.com/thu-ml/SageAttention."}
{"id": "2505.11733", "pdf": "https://arxiv.org/pdf/2505.11733", "abs": "https://arxiv.org/abs/2505.11733", "authors": ["Kevin Wu", "Eric Wu", "Rahul Thapa", "Kevin Wei", "Angela Zhang", "Arvind Suresh", "Jacqueline J. Tao", "Min Woo Sun", "Alejandro Lozano", "James Zou"], "title": "MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports", "categories": ["cs.CL"], "comment": null, "summary": "Doctors and patients alike increasingly use Large Language Models (LLMs) to\ndiagnose clinical cases. However, unlike domains such as math or coding, where\ncorrectness can be objectively defined by the final answer, medical diagnosis\nrequires both the outcome and the reasoning process to be accurate. Currently,\nwidely used medical benchmarks like MedQA and MMLU assess only accuracy in the\nfinal answer, overlooking the quality and faithfulness of the clinical\nreasoning process. To address this limitation, we introduce MedCaseReasoning,\nthe first open-access dataset for evaluating LLMs on their ability to align\nwith clinician-authored diagnostic reasoning. The dataset includes 14,489\ndiagnostic question-and-answer cases, each paired with detailed reasoning\nstatements derived from open-access medical case reports. We evaluate\nstate-of-the-art reasoning LLMs on MedCaseReasoning and find significant\nshortcomings in their diagnoses and reasoning: for instance, the top-performing\nopen-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy\nand mentions only 64% of the clinician reasoning statements (recall). However,\nwe demonstrate that fine-tuning LLMs on the reasoning traces derived from\nMedCaseReasoning significantly improves diagnostic accuracy and clinical\nreasoning recall by an average relative gain of 29% and 41%, respectively. The\nopen-source dataset, code, and models are available at\nhttps://github.com/kevinwu23/Stanford-MedCaseReasoning."}
{"id": "2505.11738", "pdf": "https://arxiv.org/pdf/2505.11738", "abs": "https://arxiv.org/abs/2505.11738", "authors": ["Zhongnan Fang", "Andrew Johnston", "Lina Cheuy", "Hye Sun Na", "Magdalini Paschali", "Camila Gonzalez", "Bonnie A. Armstrong", "Arogya Koirala", "Derrick Laurel", "Andrew Walker Campion", "Michael Iv", "Akshay S. Chaudhari", "David B. Larson"], "title": "Automated Real-time Assessment of Intracranial Hemorrhage Detection AI Using an Ensembled Monitoring Model (EMM)", "categories": ["cs.AI"], "comment": null, "summary": "Artificial intelligence (AI) tools for radiology are commonly unmonitored\nonce deployed. The lack of real-time case-by-case assessments of AI prediction\nconfidence requires users to independently distinguish between trustworthy and\nunreliable AI predictions, which increases cognitive burden, reduces\nproductivity, and potentially leads to misdiagnoses. To address these\nchallenges, we introduce Ensembled Monitoring Model (EMM), a framework inspired\nby clinical consensus practices using multiple expert reviews. Designed\nspecifically for black-box commercial AI products, EMM operates independently\nwithout requiring access to internal AI components or intermediate outputs,\nwhile still providing robust confidence measurements. Using intracranial\nhemorrhage detection as our test case on a large, diverse dataset of 2919\nstudies, we demonstrate that EMM successfully categorizes confidence in the\nAI-generated prediction, suggesting different actions and helping improve the\noverall performance of AI tools to ultimately reduce cognitive burden.\nImportantly, we provide key technical considerations and best practices for\nsuccessfully translating EMM into clinical settings."}
{"id": "2505.11595", "pdf": "https://arxiv.org/pdf/2505.11595", "abs": "https://arxiv.org/abs/2505.11595", "authors": ["Peter Chen", "Xiaopeng Li", "Ziniu Li", "Xi Chen", "Tianyi Lin"], "title": "Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "28 pages", "summary": "Reinforcement learning (RL) has demonstrated significant success in enhancing\nreasoning capabilities in large language models (LLMs). One of the most widely\nused RL methods is Group Relative Policy Optimization\n(GRPO)~\\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and\nsuccess in training DeepSeek-R1~\\cite{Guo-2025-Deepseek}. However, GRPO stalls\nwhen all sampled responses in a group are incorrect -- referred to as an\n\\emph{all-negative-sample} group -- as it fails to update the policy, hindering\nlearning progress. The contributions of this paper are two-fold. First, we\npropose a simple yet effective framework that introduces response diversity\nwithin all-negative-sample groups in GRPO using AI feedback. We also provide a\ntheoretical analysis, via a stylized model, showing how this diversification\nimproves learning dynamics. Second, we empirically validate our approach,\nshowing the improved performance across various model sizes (7B, 14B, 32B) in\nboth offline and online learning settings with 10 benchmarks, including base\nand distilled variants. Our findings highlight that learning from\nall-negative-sample groups is not only feasible but beneficial, advancing\nrecent insights from \\citet{Xiong-2025-Minimalist}."}
{"id": "2505.11739", "pdf": "https://arxiv.org/pdf/2505.11739", "abs": "https://arxiv.org/abs/2505.11739", "authors": ["Feijiang Han", "Xiaodong Yu", "Jianheng Tang", "Lyle Ungar"], "title": "ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, training-free methods for improving large language models (LLMs)\nhave attracted growing interest, with token-level attention tuning emerging as\na promising and interpretable direction. However, existing methods typically\nrely on auxiliary mechanisms to identify important or irrelevant task-specific\ntokens, introducing potential bias and limiting applicability. In this paper,\nwe uncover a surprising and elegant alternative: the semantically empty initial\ntoken is a powerful and underexplored control point for optimizing model\nbehavior. Through theoretical analysis, we show that tuning the initial token's\nattention sharpens or flattens the attention distribution over subsequent\ntokens, and its role as an attention sink amplifies this effect. Empirically,\nwe find that: (1) tuning its attention improves LLM performance more\neffectively than tuning other task-specific tokens; (2) the effect follows a\nconsistent trend across layers, with earlier layers having greater impact, but\nvaries across attention heads, with different heads showing distinct\npreferences in how they attend to this token. Based on these findings, we\npropose ZeroTuning, a training-free approach that improves LLM performance by\napplying head-specific attention adjustments to this special token. Despite\ntuning only one token, ZeroTuning achieves higher performance on text\nclassification, multiple-choice, and multi-turn conversation tasks across\nmodels such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves\nLlama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its\nmulti-turn score from 7.804 to 7.966. The method is also robust to limited\nresources, few-shot settings, long contexts, quantization, decoding strategies,\nand prompt variations. Our work sheds light on a previously overlooked control\npoint in LLMs, offering new insights into both inference-time tuning and model\ninterpretability."}
{"id": "2505.11741", "pdf": "https://arxiv.org/pdf/2505.11741", "abs": "https://arxiv.org/abs/2505.11741", "authors": ["Geigh Zollicoffer", "Minh Vu", "Manish Bhattarai"], "title": "Diverging Towards Hallucination: Detection of Failures in Vision-Language Models via Multi-token Aggregation", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "Vision-language models (VLMs) now rival human performance on many multimodal\ntasks, yet they still hallucinate objects or generate unsafe text. Current\nhallucination detectors, e.g., single-token linear probing (SLP) and P(True),\ntypically analyze only the logit of the first generated token or just its\nhighest scoring component overlooking richer signals embedded within earlier\ntoken distributions. We demonstrate that analyzing the complete sequence of\nearly logits potentially provides substantially more diagnostic information. We\nemphasize that hallucinations may only emerge after several tokens, as subtle\ninconsistencies accumulate over time. By analyzing the Kullback-Leibler (KL)\ndivergence between logits corresponding to hallucinated and non-hallucinated\ntokens, we underscore the importance of incorporating later-token logits to\nmore accurately capture the reliability dynamics of VLMs. In response, we\nintroduce Multi-Token Reliability Estimation (MTRE), a lightweight, white-box\nmethod that aggregates logits from the first ten tokens using multi-token\nlog-likelihood ratios and self-attention. Despite the challenges posed by large\nvocabulary sizes and long logit sequences, MTRE remains efficient and\ntractable. On MAD-Bench, MM-SafetyBench, MathVista, and four\ncompositional-geometry benchmarks, MTRE improves AUROC by 9.4 +/- 1.3 points\nover SLP and by 12.1 +/- 1.7 points over P(True), setting a new\nstate-of-the-art in hallucination detection for open-source VLMs."}
{"id": "2505.11601", "pdf": "https://arxiv.org/pdf/2505.11601", "abs": "https://arxiv.org/abs/2505.11601", "authors": ["Rui Liu", "Rui Xie", "Zijun Yao", "Yanjie Fu", "Dongjie Wang"], "title": "Continuous Optimization for Feature Selection with Permutation-Invariant Embedding and Policy-Guided Search", "categories": ["cs.LG", "cs.AI"], "comment": "KDD 2025", "summary": "Feature selection removes redundant features to enhanc performance and\ncomputational efficiency in downstream tasks. Existing works often struggle to\ncapture complex feature interactions and adapt to diverse scenarios. Recent\nadvances in this domain have incorporated generative intelligence to address\nthese drawbacks by uncovering intricate relationships between features.\nHowever, two key limitations remain: 1) embedding feature subsets in a\ncontinuous space is challenging due to permutation sensitivity, as changes in\nfeature order can introduce biases and weaken the embedding learning process;\n2) gradient-based search in the embedding space assumes convexity, which is\nrarely guaranteed, leading to reduced search effectiveness and suboptimal\nsubsets. To address these limitations, we propose a new framework that can: 1)\npreserve feature subset knowledge in a continuous embedding space while\nensuring permutation invariance; 2) effectively explore the embedding space\nwithout relying on strong convex assumptions. For the first objective, we\ndevelop an encoder-decoder paradigm to preserve feature selection knowledge\ninto a continuous embedding space. This paradigm captures feature interactions\nthrough pairwise relationships within the subset, removing the influence of\nfeature order on the embedding. Moreover, an inducing point mechanism is\nintroduced to accelerate pairwise relationship computations. For the second\nobjective, we employ a policy-based reinforcement learning (RL) approach to\nguide the exploration of the embedding space. The RL agent effectively\nnavigates the space by balancing multiple objectives. By prioritizing\nhigh-potential regions adaptively and eliminating the reliance on convexity\nassumptions, the RL agent effectively reduces the risk of converging to local\noptima. Extensive experiments demonstrate the effectiveness, efficiency,\nrobustness and explicitness of our model."}
{"id": "2505.11746", "pdf": "https://arxiv.org/pdf/2505.11746", "abs": "https://arxiv.org/abs/2505.11746", "authors": ["Xianglong Xu", "John Bowen", "Rojin Taheri"], "title": "Token Masking Improves Transformer-Based Text Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While transformer-based models achieve strong performance on text\nclassification, we explore whether masking input tokens can further enhance\ntheir effectiveness. We propose token masking regularization, a simple yet\ntheoretically motivated method that randomly replaces input tokens with a\nspecial [MASK] token at probability p. This introduces stochastic perturbations\nduring training, leading to implicit gradient averaging that encourages the\nmodel to capture deeper inter-token dependencies. Experiments on language\nidentification and sentiment analysis -- across diverse models (mBERT,\nQwen2.5-0.5B, TinyLlama-1.1B) -- show consistent improvements over standard\nregularization techniques. We identify task-specific optimal masking rates,\nwith p = 0.1 as a strong general default. We attribute the gains to two key\neffects: (1) input perturbation reduces overfitting, and (2) gradient-level\nsmoothing acts as implicit ensembling."}
{"id": "2505.11780", "pdf": "https://arxiv.org/pdf/2505.11780", "abs": "https://arxiv.org/abs/2505.11780", "authors": ["Zeinab Shiralizadeh"], "title": "A Review and Analysis of a Parallel Approach for Decision Tree Learning from Large Data Streams", "categories": ["cs.AI"], "comment": null, "summary": "This work studies one of the parallel decision tree learning algorithms,\npdsCART, designed for scalable and efficient data analysis. The method\nincorporates three core capabilities. First, it supports real-time learning\nfrom data streams, allowing trees to be constructed incrementally. Second, it\nenables parallel processing of high-volume streaming data, making it\nwell-suited for large-scale applications. Third, the algorithm integrates\nseamlessly into the MapReduce framework, ensuring compatibility with\ndistributed computing environments. In what follows, we present the algorithm's\nkey components along with results highlighting its performance and scalability."}
{"id": "2505.11602", "pdf": "https://arxiv.org/pdf/2505.11602", "abs": "https://arxiv.org/abs/2505.11602", "authors": ["Nikola Zubić", "Davide Scaramuzza"], "title": "Regularity and Stability Properties of Selective SSMs with Discontinuous Gating", "categories": ["cs.LG", "math.DS", "math.OC", "stat.ML"], "comment": "21 page, 6 theorems", "summary": "Deep Selective State-Space Models (SSMs), characterized by input-dependent,\ntime-varying parameters, offer significant expressive power but pose challenges\nfor stability analysis, especially with discontinuous gating signals. In this\npaper, we investigate the stability and regularity properties of\ncontinuous-time selective SSMs through the lens of passivity and Input-to-State\nStability (ISS). We establish that intrinsic energy dissipation guarantees\nexponential forgetting of past states. Crucially, we prove that the unforced\nsystem dynamics possess an underlying minimal quadratic energy function whose\ndefining matrix exhibits robust $\\text{AUC}_{\\text{loc}}$ regularity,\naccommodating discontinuous gating. Furthermore, assuming a universal quadratic\nstorage function ensures passivity across all inputs, we derive parametric LMI\nconditions and kernel constraints that limit gating mechanisms, formalizing\n\"irreversible forgetting\" of recurrent models. Finally, we provide sufficient\nconditions for global ISS, linking uniform local dissipativity to overall\nsystem robustness. Our findings offer a rigorous framework for understanding\nand designing stable and reliable deep selective SSMs."}
{"id": "2505.11754", "pdf": "https://arxiv.org/pdf/2505.11754", "abs": "https://arxiv.org/abs/2505.11754", "authors": ["Wenyu Huang", "Pavlos Vougiouklis", "Mirella Lapata", "Jeff Z. Pan"], "title": "Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation", "categories": ["cs.CL"], "comment": "ACL 2025 main", "summary": "Multi-hop Question Answering (MHQA) adds layers of complexity to question\nanswering, making it more challenging. When Language Models (LMs) are prompted\nwith multiple search results, they are tasked not only with retrieving relevant\ninformation but also employing multi-hop reasoning across the information\nsources. Although LMs perform well on traditional question-answering tasks, the\ncausal mask can hinder their capacity to reason across complex contexts. In\nthis paper, we explore how LMs respond to multi-hop questions by permuting\nsearch results (retrieved documents) under various configurations. Our study\nreveals interesting findings as follows: 1) Encoder-decoder models, such as the\nones in the Flan-T5 family, generally outperform causal decoder-only LMs in\nMHQA tasks, despite being significantly smaller in size; 2) altering the order\nof gold documents reveals distinct trends in both Flan T5 models and fine-tuned\ndecoder-only models, with optimal performance observed when the document order\naligns with the reasoning chain order; 3) enhancing causal decoder-only models\nwith bi-directional attention by modifying the causal mask can effectively\nboost their end performance. In addition to the above, we conduct a thorough\ninvestigation of the distribution of LM attention weights in the context of\nMHQA. Our experiments reveal that attention weights tend to peak at higher\nvalues when the resulting answer is correct. We leverage this finding to\nheuristically improve LMs' performance on this task. Our code is publicly\navailable at https://github.com/hwy9855/MultiHopQA-Reasoning."}
{"id": "2505.11792", "pdf": "https://arxiv.org/pdf/2505.11792", "abs": "https://arxiv.org/abs/2505.11792", "authors": ["Yitian Chen", "Jingfan Xia", "Siyu Shao", "Dongdong Ge", "Yinyu Ye"], "title": "Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling", "categories": ["cs.AI"], "comment": null, "summary": "Optimization modeling is fundamental to decision-making across diverse\ndomains.Despite progress in automating optimization formulation from natural\nlanguage descriptions, Large Language Models (LLMs) often struggle to generate\nformally correct and usable models due to hallucinations, posing a challenge\nfor reliable automation. Inspired by the success of Reinforcement Learning (RL)\nin enhancing Large Reasoning Models, we present Solver-Informed Reinforcement\nLearning (SIRL).This novel framework leverages external optimization solvers as\nverifiable reward mechanisms to significantly improve the authenticity of LLMs\nfor optimization modeling.Acting as precise verifiers, these solvers\nautomatically assess the executable code and the instance-level mathematical\nmodel represented by the associated LP file, yielding precise and comprehensive\nfeedback signals -- including syntax, feasibility, and solution quality that\ndirectly inform the RL process. This automated verification process, powered by\nclassic optimization solvers, also underpins our instance-enhanced\nself-consistency method to synthesize high-quality training data. Extensive\nexperiments on diverse public benchmarks demonstrate that SIRL achieves\nstate-of-the-art performance, substantially outperforming existing methods in\ngenerating accurate and executable optimization models."}
{"id": "2505.11621", "pdf": "https://arxiv.org/pdf/2505.11621", "abs": "https://arxiv.org/abs/2505.11621", "authors": ["Junhyung Park", "Patrick Bloebaum", "Shiva Prasad Kasiviswanathan"], "title": "A Classical View on Benign Overfitting: The Role of Sample Size", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "The results here subsume: arXiv:2410.06191", "summary": "Benign overfitting is a phenomenon in machine learning where a model\nperfectly fits (interpolates) the training data, including noisy examples, yet\nstill generalizes well to unseen data. Understanding this phenomenon has\nattracted considerable attention in recent years. In this work, we introduce a\nconceptual shift, by focusing on almost benign overfitting, where models\nsimultaneously achieve both arbitrarily small training and test errors. This\nbehavior is characteristic of neural networks, which often achieve low (but\nnon-zero) training error while still generalizing well. We hypothesize that\nthis almost benign overfitting can emerge even in classical regimes, by\nanalyzing how the interaction between sample size and model complexity enables\nlarger models to achieve both good training fit but still approach\nBayes-optimal generalization. We substantiate this hypothesis with theoretical\nevidence from two case studies: (i) kernel ridge regression, and (ii)\nleast-squares regression using a two-layer fully connected ReLU neural network\ntrained via gradient flow. In both cases, we overcome the strong assumptions\noften required in prior work on benign overfitting.\n  Our results on neural networks also provide the first generalization result\nin this setting that does not rely on any assumptions about the underlying\nregression function or noise, beyond boundedness. Our analysis introduces a\nnovel proof technique based on decomposing the excess risk into estimation and\napproximation errors, interpreting gradient flow as an implicit regularizer,\nthat helps avoid uniform convergence traps. This analysis idea could be of\nindependent interest."}
{"id": "2505.11764", "pdf": "https://arxiv.org/pdf/2505.11764", "abs": "https://arxiv.org/abs/2505.11764", "authors": ["Raymond Baartmans", "Matthew Raffel", "Rahul Vikram", "Aiden Deringer", "Lizhong Chen"], "title": "Towards Universal Semantics With Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a\nuniversal set of semantic primes: simple, primitive word-meanings that have\nbeen shown to exist in most, if not all, languages of the world. According to\nthis framework, any word, regardless of complexity, can be paraphrased using\nthese primes, revealing a clear and universally translatable meaning. These\nparaphrases, known as explications, can offer valuable applications for many\nnatural language processing (NLP) tasks, but producing them has traditionally\nbeen a slow, manual process. In this work, we present the first study of using\nlarge language models (LLMs) to generate NSM explications. We introduce\nautomatic evaluation methods, a tailored dataset for training and evaluation,\nand fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in\nproducing accurate, cross-translatable explications, marking a significant step\ntoward universal semantic representation with LLMs and opening up new\npossibilities for applications in semantic analysis, translation, and beyond."}
{"id": "2505.11803", "pdf": "https://arxiv.org/pdf/2505.11803", "abs": "https://arxiv.org/abs/2505.11803", "authors": ["ChongIn Un", "Yuhuan Lu", "Tianyue Yang", "Dingqi Yang"], "title": "VITA: Versatile Time Representation Learning for Temporal Hyper-Relational Knowledge Graphs", "categories": ["cs.AI", "cs.SC"], "comment": null, "summary": "Knowledge graphs (KGs) have become an effective paradigm for managing\nreal-world facts, which are not only complex but also dynamically evolve over\ntime. The temporal validity of facts often serves as a strong clue in\ndownstream link prediction tasks, which predicts a missing element in a fact.\nTraditional link prediction techniques on temporal KGs either consider a\nsequence of temporal snapshots of KGs with an ad-hoc defined time interval or\nexpand a temporal fact over its validity period under a predefined time\ngranularity; these approaches not only suffer from the sensitivity of the\nselection of time interval/granularity, but also face the computational\nchallenges when handling facts with long (even infinite) validity. Although the\nrecent hyper-relational KGs represent the temporal validity of a fact as\nqualifiers describing the fact, it is still suboptimal due to its ignorance of\nthe infinite validity of some facts and the insufficient information encoded\nfrom the qualifiers about the temporal validity. Against this background, we\npropose VITA, a $\\underline{V}$ersatile t$\\underline{I}$me\nrepresen$\\underline{TA}$tion learning method for temporal hyper-relational\nknowledge graphs. We first propose a versatile time representation that can\nflexibly accommodate all four types of temporal validity of facts (i.e., since,\nuntil, period, time-invariant), and then design VITA to effectively learn the\ntime information in both aspects of time value and timespan to boost the link\nprediction performance. We conduct a thorough evaluation of VITA compared to a\nsizable collection of baselines on real-world KG datasets. Results show that\nVITA outperforms the best-performing baselines in various link prediction tasks\n(predicting missing entities, relations, time, and other numeric literals) by\nup to 75.3%. Ablation studies and a case study also support our key design\nchoices."}
{"id": "2505.11625", "pdf": "https://arxiv.org/pdf/2505.11625", "abs": "https://arxiv.org/abs/2505.11625", "authors": ["Huiliang Zhang", "Ping Nie", "Lijun Sun", "Benoit Boulet"], "title": "Nearest Neighbor Multivariate Time Series Forecasting", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Multivariate time series (MTS) forecasting has a wide range of applications\nin both industry and academia. Recently, spatial-temporal graph neural networks\n(STGNNs) have gained popularity as MTS forecasting methods. However, current\nSTGNNs can only use the finite length of MTS input data due to the\ncomputational complexity. Moreover, they lack the ability to identify similar\npatterns throughout the entire dataset and struggle with data that exhibit\nsparsely and discontinuously distributed correlations among variables over an\nextensive historical period, resulting in only marginal improvements. In this\narticle, we introduce a simple yet effective k-nearest neighbor MTS forecasting\n( kNN-MTS) framework, which forecasts with a nearest neighbor retrieval\nmechanism over a large datastore of cached series, using representations from\nthe MTS model for similarity search. This approach requires no additional\ntraining and scales to give the MTS model direct access to the whole dataset at\ntest time, resulting in a highly expressive model that consistently improves\nperformance, and has the ability to extract sparse distributed but similar\npatterns spanning over multivariables from the entire dataset. Furthermore, a\nhybrid spatial-temporal encoder (HSTEncoder) is designed for kNN-MTS which can\ncapture both long-term temporal and short-term spatial-temporal dependencies\nand is shown to provide accurate representation for kNN-MTSfor better\nforecasting. Experimental results on several real-world datasets show a\nsignificant improvement in the forecasting performance of kNN-MTS. The\nquantitative analysis also illustrates the interpretability and efficiency of\nkNN-MTS, showing better application prospects and opening up a new path for\nefficiently using the large dataset in MTS models."}
{"id": "2505.11807", "pdf": "https://arxiv.org/pdf/2505.11807", "abs": "https://arxiv.org/abs/2505.11807", "authors": ["Yufei Xiang", "Yiqun Shen", "Yeqin Zhang", "Cam-Tu Nguyen"], "title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "Large Language Models (LLMs) possess extensive knowledge and commonsense\nreasoning capabilities, making them valuable for creating powerful agents.\nHowever, existing LLM agent frameworks have not fully utilized past experiences\nfor improvement. This work introduces a new LLM-based agent framework called\nRetrospex, which addresses this challenge by analyzing past experiences in\ndepth. Unlike previous approaches, Retrospex does not directly integrate\nexperiences into the LLM's context. Instead, it combines the LLM's action\nlikelihood with action values estimated by a Reinforcement Learning (RL)\nCritic, which is trained on past experiences through an offline\n''retrospection'' process. Additionally, Retrospex employs a dynamic action\nrescoring mechanism that increases the importance of experience-based values\nfor tasks that require more interaction with the environment. We evaluate\nRetrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its\nadvantages over strong, contemporary baselines."}
{"id": "2505.11814", "pdf": "https://arxiv.org/pdf/2505.11814", "abs": "https://arxiv.org/abs/2505.11814", "authors": ["Hector Munoz-Avila", "David W. Aha", "Paola Rizzo"], "title": "ChatHTN: Interleaving Approximate (LLM) and Symbolic HTN Planning", "categories": ["cs.AI"], "comment": "2nd International Conference on Neuro-symbolic Systems (NeuS) 2025", "summary": "We introduce ChatHTN, a Hierarchical Task Network (HTN) planner that combines\nsymbolic HTN planning techniques with queries to ChatGPT to approximate\nsolutions in the form of task decompositions. The resulting hierarchies\ninterleave task decompositions generated by symbolic HTN planning with those\ngenerated by ChatGPT. Despite the approximate nature of the results generates\nby ChatGPT, ChatHTN is provably sound; any plan it generates correctly achieves\nthe input tasks. We demonstrate this property with an open-source\nimplementation of our system."}
{"id": "2505.11627", "pdf": "https://arxiv.org/pdf/2505.11627", "abs": "https://arxiv.org/abs/2505.11627", "authors": ["Shuyi Chen", "Shixiang Zhu", "Ramteen Sioshansi"], "title": "Adaptive Robust Optimization with Data-Driven Uncertainty for Enhancing Distribution System Resilience", "categories": ["cs.LG"], "comment": null, "summary": "Extreme weather events are placing growing strain on electric power systems,\nexposing the limitations of purely reactive responses and prompting the need\nfor proactive resilience planning. However, existing approaches often rely on\nsimplified uncertainty models and decouple proactive and reactive decisions,\noverlooking their critical interdependence. This paper proposes a novel\ntri-level optimization framework that integrates proactive infrastructure\ninvestment, adversarial modeling of spatio-temporal disruptions, and adaptive\nreactive response. We construct high-probability, distribution-free uncertainty\nsets using conformal prediction to capture complex and data-scarce outage\npatterns. To solve the resulting nested decision problem, we derive a bi-level\nreformulation via strong duality and develop a scalable Benders decomposition\nalgorithm. Experiments on both real and synthetic data demonstrate that our\napproach consistently outperforms conventional robust and two-stage methods,\nachieving lower worst-case losses and more efficient resource allocation,\nespecially under tight operational constraints and large-scale uncertainty."}
{"id": "2505.11810", "pdf": "https://arxiv.org/pdf/2505.11810", "abs": "https://arxiv.org/abs/2505.11810", "authors": ["Shen Li", "Renfen Hu", "Lijun Wang"], "title": "Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model", "categories": ["cs.CL"], "comment": null, "summary": "General-purpose large language models demonstrate notable capabilities in\nlanguage comprehension and generation, achieving results that are comparable\nto, or even surpass, human performance in many language information processing\ntasks. Nevertheless, when general models are applied to some specific domains,\ne.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and\nfine-tuning open-source foundational models similarly struggles to adequately\nincorporate domain-specific knowledge. To address this challenge, this study\ndeveloped a large language model, AI Taiyan, specifically designed for\nunderstanding and generating Classical Chinese. Experiments show that with a\nreasonable model design, data processing, foundational training, and\nfine-tuning, satisfactory results can be achieved with only 1.8 billion\nparameters. In key tasks related to Classical Chinese information processing\nsuch as punctuation, identification of allusions, explanation of word meanings,\nand translation between ancient and modern Chinese, this model exhibits a clear\nadvantage over both general-purpose large models and domain-specific\ntraditional models, achieving levels close to or surpassing human baselines.\nThis research provides a reference for the efficient construction of\nspecialized domain-specific large language models. Furthermore, the paper\ndiscusses the application of this model in fields such as the collation of\nancient texts, dictionary editing, and language research, combined with case\nstudies."}
{"id": "2505.11831", "pdf": "https://arxiv.org/pdf/2505.11831", "abs": "https://arxiv.org/abs/2505.11831", "authors": ["Francois Chollet", "Mike Knoop", "Gregory Kamradt", "Bryan Landers", "Henry Pinkard"], "title": "ARC-AGI-2: A New Challenge for Frontier AI Reasoning Systems", "categories": ["cs.AI"], "comment": null, "summary": "The Abstraction and Reasoning Corpus for Artificial General Intelligence\n(ARC-AGI), introduced in 2019, established a challenging benchmark for\nevaluating the general fluid intelligence of artificial systems via a set of\nunique, novel tasks only requiring minimal prior knowledge. While ARC-AGI has\nspurred significant research activity over the past five years, recent AI\nprogress calls for benchmarks capable of finer-grained evaluation at higher\nlevels of cognitive complexity. We introduce ARC-AGI-2, an upgraded version of\nthe benchmark. ARC-AGI-2 preserves the input-output pair task format of its\npredecessor, ensuring continuity for researchers. It incorporates a newly\ncurated and expanded set of tasks specifically designed to provide a more\ngranular signal to assess abstract reasoning and problem-solving abilities at\nhigher levels of fluid intelligence. To contextualize the difficulty and\ncharacteristics of ARC-AGI-2, we present extensive results from human testing,\nproviding a robust baseline that highlights the benchmark's accessibility to\nhuman intelligence, yet difficulty for current AI systems. ARC-AGI-2 aims to\nserve as a next-generation tool for rigorously measuring progress towards more\ngeneral and human-like AI capabilities."}
{"id": "2505.11631", "pdf": "https://arxiv.org/pdf/2505.11631", "abs": "https://arxiv.org/abs/2505.11631", "authors": ["Wajdi Hammami", "Soumaya Cherkaoui", "Shengrui Wang"], "title": "Enhancing Network Anomaly Detection with Quantum GANs and Successive Data Injection for Multivariate Time Series", "categories": ["cs.LG", "quant-ph"], "comment": null, "summary": "Quantum computing may offer new approaches for advancing machine learning,\nincluding in complex tasks such as anomaly detection in network traffic. In\nthis paper, we introduce a quantum generative adversarial network (QGAN)\narchitecture for multivariate time-series anomaly detection that leverages\nvariational quantum circuits (VQCs) in combination with a time-window shifting\ntechnique, data re-uploading, and successive data injection (SuDaI). The method\nencodes multivariate time series data as rotation angles. By integrating both\ndata re-uploading and SuDaI, the approach maps classical data into quantum\nstates efficiently, helping to address hardware limitations such as the\nrestricted number of available qubits. In addition, the approach employs an\nanomaly scoring technique that utilizes both the generator and the\ndiscriminator output to enhance the accuracy of anomaly detection. The QGAN was\ntrained using the parameter shift rule and benchmarked against a classical GAN.\nExperimental results indicate that the quantum model achieves a accuracy high\nalong with high recall and F1-scores in anomaly detection, and attains a lower\nMSE compared to the classical model. Notably, the QGAN accomplishes this\nperformance with only 80 parameters, demonstrating competitive results with a\ncompact architecture. Tests using a noisy simulator suggest that the approach\nremains effective under realistic noise-prone conditions."}
{"id": "2505.11811", "pdf": "https://arxiv.org/pdf/2505.11811", "abs": "https://arxiv.org/abs/2505.11811", "authors": ["Taolin Zhang", "Dongyang Li", "Qizhou Chen", "Chengyu Wang", "Xiaofeng He"], "title": "BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering", "categories": ["cs.CL"], "comment": "Accepted by ACL2025 main track", "summary": "Multi-hop question answering (QA) involves finding multiple relevant passages\nand performing step-by-step reasoning to answer complex questions. Previous\nworks on multi-hop QA employ specific methods from different modeling\nperspectives based on large language models (LLMs), regardless of the question\ntypes. In this paper, we first conduct an in-depth analysis of public multi-hop\nQA benchmarks, dividing the questions into four types and evaluating five types\nof cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step,\nIterative-step, Sub-step, and Adaptive-step. We find that different types of\nmulti-hop questions have varying degrees of sensitivity to different types of\nmethods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to\naddress multi-hop QA by specifically focusing on the correspondence between\nquestion types and methods, where each type of method is regarded as an\n''operator'' by prompting LLMs differently. The first level of BELLE includes\nmultiple agents that debate to obtain an executive plan of combined\n''operators'' to address the multi-hop QA task comprehensively. During the\ndebate, in addition to the basic roles of affirmative debater, negative\ndebater, and judge, at the second level, we further leverage fast and slow\ndebaters to monitor whether changes in viewpoints are reasonable. Extensive\nexperiments demonstrate that BELLE significantly outperforms strong baselines\nin various datasets. Additionally, the model consumption of BELLE is higher\ncost-effectiveness than that of single models in more complex multi-hop QA\nscenarios."}
{"id": "2505.11833", "pdf": "https://arxiv.org/pdf/2505.11833", "abs": "https://arxiv.org/abs/2505.11833", "authors": ["Haotian Chen", "Zijun Song", "Boye Niu", "Ke Zhang", "Litu Ou", "Yaxi Lu", "Zhong Zhang", "Xin Cong", "Yankai Lin", "Zhiyuan Liu", "Maosong Sun"], "title": "ToLeaP: Rethinking Development of Tool Learning with Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Tool learning, which enables large language models (LLMs) to utilize external\ntools effectively, has garnered increasing attention for its potential to\nrevolutionize productivity across industries. Despite rapid development in tool\nlearning, key challenges and opportunities remain understudied, limiting deeper\ninsights and future advancements. In this paper, we investigate the tool\nlearning ability of 41 prevalent LLMs by reproducing 33 benchmarks and enabling\none-click evaluation for seven of them, forming a Tool Learning Platform named\nToLeaP. We also collect 21 out of 33 potential training datasets to facilitate\nfuture exploration. After analyzing over 3,000 bad cases of 41 LLMs based on\nToLeaP, we identify four main critical challenges: (1) benchmark limitations\ninduce both the neglect and lack of (2) autonomous learning, (3)\ngeneralization, and (4) long-horizon task-solving capabilities of LLMs. To aid\nfuture advancements, we take a step further toward exploring potential\ndirections, namely (1) real-world benchmark construction, (2)\ncompatibility-aware autonomous learning, (3) rationale learning by thinking,\nand (4) identifying and recalling key clues. The preliminary experiments\ndemonstrate their effectiveness, highlighting the need for further research and\nexploration."}
{"id": "2505.11635", "pdf": "https://arxiv.org/pdf/2505.11635", "abs": "https://arxiv.org/abs/2505.11635", "authors": ["Nikhil Kapasi", "William Whitehead", "Luke Theogarajan"], "title": "The Gaussian-Multinoulli Restricted Boltzmann Machine: A Potts Model Extension of the GRBM", "categories": ["cs.LG"], "comment": "11 pages, 3 figures (1 figure has 2 subfigures), conference", "summary": "Many real-world tasks, from associative memory to symbolic reasoning, demand\ndiscrete, structured representations that standard continuous latent models\nstruggle to express naturally. We introduce the Gaussian-Multinoulli Restricted\nBoltzmann Machine (GM-RBM), a generative energy-based model that extends the\nGaussian-Bernoulli RBM (GB-RBM) by replacing binary hidden units with $q$-state\nPotts variables. This modification enables a combinatorially richer latent\nspace and supports learning over multivalued, interpretable latent concepts. We\nformally derive GM-RBM's energy function, learning dynamics, and conditional\ndistributions, showing that it preserves tractable inference and training\nthrough contrastive divergence. Empirically, we demonstrate that GM-RBMs model\ncomplex multimodal distributions more effectively than binary RBMs,\noutperforming them on tasks involving analogical recall and structured memory.\nOur results highlight GM-RBMs as a scalable framework for discrete latent\ninference with enhanced expressiveness and interoperability."}
{"id": "2505.11820", "pdf": "https://arxiv.org/pdf/2505.11820", "abs": "https://arxiv.org/abs/2505.11820", "authors": ["Kaitao Song", "Xiaohua Wang", "Xu Tan", "Huiqiang Jiang", "Chengruidong Zhang", "Yongliang Shen", "Cen LU", "Zihao Li", "Zifan Song", "Caihua Shan", "Yansen Wang", "Kan Ren", "Xiaoqing Zheng", "Tao Qin", "Yuqing Yang", "Dongsheng Li", "Lili Qiu"], "title": "Chain-of-Model Learning for Language Model", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM."}
{"id": "2505.11839", "pdf": "https://arxiv.org/pdf/2505.11839", "abs": "https://arxiv.org/abs/2505.11839", "authors": ["Shuai Yang", "Qi Yang", "Luoxi Tang", "Jeremy Blackburn", "Zhaohan Xi"], "title": "On the Eligibility of LLMs for Counterfactual Reasoning: A Decompositional Study", "categories": ["cs.AI"], "comment": null, "summary": "Counterfactual reasoning has emerged as a crucial technique for generalizing\nthe reasoning capabilities of large language models (LLMs). By generating and\nanalyzing counterfactual scenarios, researchers can assess the adaptability and\nreliability of model decision-making. Although prior work has shown that LLMs\noften struggle with counterfactual reasoning, it remains unclear which factors\nmost significantly impede their performance across different tasks and\nmodalities. In this paper, we propose a decompositional strategy that breaks\ndown the counterfactual generation from causality construction to the reasoning\nover counterfactual interventions. To support decompositional analysis, we\ninvestigate 11 datasets spanning diverse tasks, including natural language\nunderstanding, mathematics, programming, and vision-language tasks. Through\nextensive evaluations, we characterize LLM behavior across each decompositional\nstage and identify how modality type and intermediate reasoning influence\nperformance. By establishing a structured framework for analyzing\ncounterfactual reasoning, this work contributes to the development of more\nreliable LLM-based reasoning systems and informs future elicitation strategies."}
{"id": "2505.11636", "pdf": "https://arxiv.org/pdf/2505.11636", "abs": "https://arxiv.org/abs/2505.11636", "authors": ["Hongyu Cheng", "Amitabh Basu"], "title": "Generalization Guarantees for Learning Branch-and-Cut Policies in Integer Programming", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Mixed-integer programming (MIP) provides a powerful framework for\noptimization problems, with Branch-and-Cut (B&C) being the predominant\nalgorithm in state-of-the-art solvers. The efficiency of B&C critically depends\non heuristic policies for making sequential decisions, including node\nselection, cut selection, and branching variable selection. While traditional\nsolvers often employ heuristics with manually tuned parameters, recent\napproaches increasingly leverage machine learning, especially neural networks,\nto learn these policies directly from data. A key challenge is to understand\nthe theoretical underpinnings of these learned policies, particularly their\ngeneralization performance from finite data. This paper establishes rigorous\nsample complexity bounds for learning B&C policies where the scoring functions\nguiding each decision step (node, cut, branch) have a certain piecewise\npolynomial structure. This structure generalizes the linear models that form\nthe most commonly deployed policies in practice and investigated recently in a\nfoundational series of theoretical works by Balcan et al. Such piecewise\npolynomial policies also cover the neural network architectures (e.g., using\nReLU activations) that have been the focal point of contemporary practical\nstudies. Consequently, our theoretical framework closely reflects the models\nutilized by practitioners investigating machine learning within B&C, offering a\nunifying perspective relevant to both established theory and modern empirical\nresearch in this area. Furthermore, our theory applies to quite general\nsequential decision making problems beyond B&C."}
{"id": "2505.11827", "pdf": "https://arxiv.org/pdf/2505.11827", "abs": "https://arxiv.org/abs/2505.11827", "authors": ["Yansong Ning", "Wei Li", "Jun Fang", "Naiqiang Tan", "Hao Liu"], "title": "Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "In progress", "summary": "Compressing long chain-of-thought (CoT) from large language models (LLMs) is\nan emerging strategy to improve the reasoning efficiency of LLMs. Despite its\npromising benefits, existing studies equally compress all thoughts within a\nlong CoT, hindering more concise and effective reasoning. To this end, we first\ninvestigate the importance of different thoughts by examining their\neffectiveness and efficiency in contributing to reasoning through automatic\nlong CoT chunking and Monte Carlo rollouts. Building upon the insights, we\npropose a theoretically bounded metric to jointly measure the effectiveness and\nefficiency of different thoughts. We then propose Long$\\otimes$Short, an\nefficient reasoning framework that enables two LLMs to collaboratively solve\nthe problem: a long-thought LLM for more effectively generating important\nthoughts, while a short-thought LLM for efficiently generating remaining\nthoughts. Specifically, we begin by synthesizing a small amount of cold-start\ndata to fine-tune LLMs for long-thought and short-thought reasoning styles,\nrespectively. Furthermore, we propose a synergizing-oriented multi-turn\nreinforcement learning, focusing on the model self-evolution and collaboration\nbetween long-thought and short-thought LLMs. Experimental results show that our\nmethod enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance\ncompared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while\nreducing token length by over 80% across the MATH500, AIME24/25, AMC23, and\nGPQA Diamond benchmarks. Our data and code are available at\nhttps://github.com/yasNing/Long-otimes-Short/."}
{"id": "2505.11849", "pdf": "https://arxiv.org/pdf/2505.11849", "abs": "https://arxiv.org/abs/2505.11849", "authors": ["Yiting Wang", "Guoheng Sun", "Wanghao Ye", "Gang Qu", "Ang Li"], "title": "VeriReason: Reinforcement Learning with Testbench Feedback for Reasoning-Enhanced Verilog Generation", "categories": ["cs.AI", "cs.AR", "cs.LG", "cs.PL"], "comment": "11 pages, 2 figures", "summary": "Automating Register Transfer Level (RTL) code generation using Large Language\nModels (LLMs) offers substantial promise for streamlining digital circuit\ndesign and reducing human effort. However, current LLM-based approaches face\nsignificant challenges with training data scarcity, poor specification-code\nalignment, lack of verification mechanisms, and balancing generalization with\nspecialization. Inspired by DeepSeek-R1, we introduce VeriReason, a framework\nintegrating supervised fine-tuning with Guided Reward Proximal Optimization\n(GRPO) reinforcement learning for RTL generation. Using curated training\nexamples and a feedback-driven reward model, VeriReason combines testbench\nevaluations with structural heuristics while embedding self-checking\ncapabilities for autonomous error correction. On the VerilogEval Benchmark,\nVeriReason delivers significant improvements: achieving 83.1% functional\ncorrectness on the VerilogEval Machine benchmark, substantially outperforming\nboth comparable-sized models and much larger commercial systems like GPT-4\nTurbo. Additionally, our approach demonstrates up to a 2.8X increase in\nfirst-attempt functional correctness compared to baseline methods and exhibits\nrobust generalization to unseen designs. To our knowledge, VeriReason\nrepresents the first system to successfully integrate explicit reasoning\ncapabilities with reinforcement learning for Verilog generation, establishing a\nnew state-of-the-art for automated RTL synthesis. The models and datasets are\navailable at: https://huggingface.co/collections/AI4EDA-CASE Code is Available\nat: https://github.com/NellyW8/VeriReason"}
{"id": "2505.11645", "pdf": "https://arxiv.org/pdf/2505.11645", "abs": "https://arxiv.org/abs/2505.11645", "authors": ["Jinzhou Cao", "Xiangxu Wang", "Jiashi Chen", "Wei Tu", "Zhenhui Li", "Xindong Yang", "Tianhong Zhao", "Qingquan Li"], "title": "Urban Representation Learning for Fine-grained Economic Mapping: A Semi-supervised Graph-based Approach", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted for publication in International Society Journal of\n  Photogrammetry and Remote Sensing (ISPRS). 70 pages, 10 Figures, 15 Tables", "summary": "Fine-grained economic mapping through urban representation learning has\nemerged as a crucial tool for evidence-based economic decisions. While existing\nmethods primarily rely on supervised or unsupervised approaches, they often\noverlook semi-supervised learning in data-scarce scenarios and lack unified\nmulti-task frameworks for comprehensive sectoral economic analysis. To address\nthese gaps, we propose SemiGTX, an explainable semi-supervised graph learning\nframework for sectoral economic mapping. The framework is designed with\ndedicated fusion encoding modules for various geospatial data modalities,\nseamlessly integrating them into a cohesive graph structure. It introduces a\nsemi-information loss function that combines spatial self-supervision with\nlocally masked supervised regression, enabling more informative and effective\nregion representations. Through multi-task learning, SemiGTX concurrently maps\nGDP across primary, secondary, and tertiary sectors within a unified model.\nExtensive experiments conducted in the Pearl River Delta region of China\ndemonstrate the model's superior performance compared to existing methods,\nachieving R2 scores of 0.93, 0.96, and 0.94 for the primary, secondary and\ntertiary sectors, respectively. Cross-regional experiments in Beijing and\nChengdu further illustrate its generality. Systematic analysis reveals how\ndifferent data modalities influence model predictions, enhancing explainability\nwhile providing valuable insights for regional development planning. This\nrepresentation learning framework advances regional economic monitoring through\ndiverse urban data integration, providing a robust foundation for precise\neconomic forecasting."}
{"id": "2505.11829", "pdf": "https://arxiv.org/pdf/2505.11829", "abs": "https://arxiv.org/abs/2505.11829", "authors": ["Chenlu Wang", "Weimin Lyu", "Ritwik Banerjee"], "title": "Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Detecting deviant language such as sexism, or nuanced language such as\nmetaphors or sarcasm, is crucial for enhancing the safety, clarity, and\ninterpretation of online social discourse. While existing classifiers deliver\nstrong results on these tasks, they often come with significant computational\ncost and high data demands. In this work, we propose \\textbf{Cla}ss\n\\textbf{D}istillation (ClaD), a novel training paradigm that targets the core\nchallenge: distilling a small, well-defined target class from a highly diverse\nand heterogeneous background. ClaD integrates two key innovations: (i) a loss\nfunction informed by the structural properties of class distributions, based on\nMahalanobis distance, and (ii) an interpretable decision algorithm optimized\nfor class separation. Across three benchmark detection tasks -- sexism,\nmetaphor, and sarcasm -- ClaD outperforms competitive baselines, and even with\nsmaller language models and orders of magnitude fewer parameters, achieves\nperformance comparable to several large language models (LLMs). These results\ndemonstrate ClaD as an efficient tool for pragmatic language understanding\ntasks that require gleaning a small target class from a larger heterogeneous\nbackground."}
{"id": "2505.11854", "pdf": "https://arxiv.org/pdf/2505.11854", "abs": "https://arxiv.org/abs/2505.11854", "authors": ["Hanmeng Liu", "Yiran Ding", "Zhizhang Fu", "Chaoli Zhang", "Xiaozhang Liu", "Yue Zhang"], "title": "Evaluating the Logical Reasoning Abilities of Large Reasoning Models", "categories": ["cs.AI"], "comment": null, "summary": "Large reasoning models, often post-trained on long chain-of-thought (long\nCoT) data with reinforcement learning, achieve state-of-the-art performance on\nmathematical, coding, and domain-specific reasoning benchmarks. However, their\nlogical reasoning capabilities - fundamental to human cognition and independent\nof domain knowledge - remain understudied. To address this gap, we introduce\nLogiEval, a holistic benchmark for evaluating logical reasoning in large\nreasoning models. LogiEval spans diverse reasoning types (deductive, inductive,\nanalogical, and abductive) and task formats (e.g., logical sequence, argument\nanalysis), sourced from high-quality human examinations (e.g., LSAT, GMAT). Our\nexperiments demonstrate that modern reasoning models excel at 4-choice argument\nanalysis problems and analogical reasoning, surpassing human performance, yet\nexhibit uneven capabilities across reasoning types and formats, highlighting\nlimitations in their generalization. Our analysis reveals that human\nperformance does not mirror model failure distributions. To foster further\nresearch, we curate LogiEval-Hard, a challenging subset identified through a\nnovel screening paradigm where small-model failures (Qwen3-30B-A3B) reliably\npredict difficulties for larger models. Modern models show striking, consistent\nfailures on LogiEval-Hard. This demonstrates that fundamental reasoning\nbottlenecks persist across model scales, and establishes LogiEval-Hard as both\na diagnostic tool and a rigorous testbed for advancing logical reasoning in\nLLMs."}
{"id": "2505.11648", "pdf": "https://arxiv.org/pdf/2505.11648", "abs": "https://arxiv.org/abs/2505.11648", "authors": ["Tsutahiro Fukuhara", "Junya Hara", "Hiroshi Higashi", "Yuichi Tanaka"], "title": "Joint Graph Estimation and Signal Restoration for Robust Federated Learning", "categories": ["cs.LG", "eess.SP"], "comment": "Preprint submitted to the 2025 IEEE International Workshop on Machine\n  Learning for Signal Processing (MLSP), Istanbul, Turkey, Aug. 2025. 8 pages,\n  2 figures", "summary": "We propose a robust aggregation method for model parameters in federated\nlearning (FL) under noisy communications. FL is a distributed machine learning\nparadigm in which a central server aggregates local model parameters from\nmultiple clients. These parameters are often noisy and/or have missing values\nduring data collection, training, and communication between the clients and\nserver. This may cause a considerable drop in model accuracy. To address this\nissue, we learn a graph that represents pairwise relationships between model\nparameters of the clients during aggregation. We realize it with a joint\nproblem of graph learning and signal (i.e., model parameters) restoration. The\nproblem is formulated as a difference-of-convex (DC) optimization, which is\nefficiently solved via a proximal DC algorithm. Experimental results on MNIST\nand CIFAR-10 datasets show that the proposed method outperforms existing\napproaches by up to $2$--$5\\%$ in classification accuracy under biased data\ndistributions and noisy conditions."}
{"id": "2505.11835", "pdf": "https://arxiv.org/pdf/2505.11835", "abs": "https://arxiv.org/abs/2505.11835", "authors": ["Hongliang Li", "Jinan Xu", "Gengping Cui", "Changhao Guan", "Fengran Mo", "Kaiyu Huang"], "title": "Multilingual Collaborative Defense for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 4figures", "summary": "The robustness and security of large language models (LLMs) has become a\nprominent research area. One notable vulnerability is the ability to bypass LLM\nsafeguards by translating harmful queries into rare or underrepresented\nlanguages, a simple yet effective method of \"jailbreaking\" these models.\nDespite the growing concern, there has been limited research addressing the\nsafeguarding of LLMs in multilingual scenarios, highlighting an urgent need to\nenhance multilingual safety. In this work, we investigate the correlation\nbetween various attack features across different languages and propose\nMultilingual Collaborative Defense (MCD), a novel learning method that\noptimizes a continuous, soft safety prompt automatically to facilitate\nmultilingual safeguarding of LLMs. The MCD approach offers three advantages:\nFirst, it effectively improves safeguarding performance across multiple\nlanguages. Second, MCD maintains strong generalization capabilities while\nminimizing false refusal rates. Third, MCD mitigates the language safety\nmisalignment caused by imbalances in LLM training corpora. To evaluate the\neffectiveness of MCD, we manually construct multilingual versions of commonly\nused jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess\nvarious safeguarding methods. Additionally, we introduce these datasets in\nunderrepresented (zero-shot) languages to verify the language transferability\nof MCD. The results demonstrate that MCD outperforms existing approaches in\nsafeguarding against multilingual jailbreak attempts while also exhibiting\nstrong language transfer capabilities. Our code is available at\nhttps://github.com/HLiang-Lee/MCD."}
{"id": "2505.11861", "pdf": "https://arxiv.org/pdf/2505.11861", "abs": "https://arxiv.org/abs/2505.11861", "authors": ["Qi Zhou", "Jie Zhang", "Dongxia Wang", "Qiang Liu", "Tianlin Li", "Jin Song Dong", "Wenhai Wang", "Qing Guo"], "title": "Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity", "categories": ["cs.AI", "cs.CL", "91C99", "I.2.7; J.4"], "comment": "under review", "summary": "Human preference plays a crucial role in the refinement of large language\nmodels (LLMs). However, collecting human preference feedback is costly and most\nexisting datasets neglect the correlation between personalization and\npreferences. To address this issue, we introduce Fair-PP, a synthetic dataset\nof personalized preferences targeting social equity, derived from real-world\nsocial survey data, which includes 28 social groups, 98 equity topics, and 5\npersonal preference dimensions. Leveraging GPT-4o-mini, we engage in\nrole-playing based on seven representative persona portrayals guided by\nexisting social survey data, yielding a total of 238,623 preference records.\nThrough Fair-PP, we also contribute (i) An automated framework for generating\npreference data, along with a more fine-grained dataset of personalized\npreferences; (ii) analysis of the positioning of the existing mainstream LLMs\nacross five major global regions within the personalized preference space; and\n(iii) a sample reweighting method for personalized preference alignment,\nenabling alignment with a target persona while maximizing the divergence from\nother personas. Empirical experiments show our method outperforms the\nbaselines."}
{"id": "2505.11654", "pdf": "https://arxiv.org/pdf/2505.11654", "abs": "https://arxiv.org/abs/2505.11654", "authors": ["Yuhang Liu", "Yingxue Zhang", "Xin Zhang", "Ling Tian", "Xu Zheng", "Yanhua Li", "Jun Luo"], "title": "UrbanMind: Urban Dynamics Prediction with Multifaceted Spatial-Temporal Large Language Models", "categories": ["cs.LG"], "comment": "KDD 2025", "summary": "Understanding and predicting urban dynamics is crucial for managing\ntransportation systems, optimizing urban planning, and enhancing public\nservices. While neural network-based approaches have achieved success, they\noften rely on task-specific architectures and large volumes of data, limiting\ntheir ability to generalize across diverse urban scenarios. Meanwhile, Large\nLanguage Models (LLMs) offer strong reasoning and generalization capabilities,\nyet their application to spatial-temporal urban dynamics remains underexplored.\nExisting LLM-based methods struggle to effectively integrate multifaceted\nspatial-temporal data and fail to address distributional shifts between\ntraining and testing data, limiting their predictive reliability in real-world\napplications. To bridge this gap, we propose UrbanMind, a novel\nspatial-temporal LLM framework for multifaceted urban dynamics prediction that\nensures both accurate forecasting and robust generalization. At its core,\nUrbanMind introduces Muffin-MAE, a multifaceted fusion masked autoencoder with\nspecialized masking strategies that capture intricate spatial-temporal\ndependencies and intercorrelations among multifaceted urban dynamics.\nAdditionally, we design a semantic-aware prompting and fine-tuning strategy\nthat encodes spatial-temporal contextual details into prompts, enhancing LLMs'\nability to reason over spatial-temporal patterns. To further improve\ngeneralization, we introduce a test time adaptation mechanism with a test data\nreconstructor, enabling UrbanMind to dynamically adjust to unseen test data by\nreconstructing LLM-generated embeddings. Extensive experiments on real-world\nurban datasets across multiple cities demonstrate that UrbanMind consistently\noutperforms state-of-the-art baselines, achieving high accuracy and robust\ngeneralization, even in zero-shot settings."}
{"id": "2505.11855", "pdf": "https://arxiv.org/pdf/2505.11855", "abs": "https://arxiv.org/abs/2505.11855", "authors": ["Guijin Son", "Jiwoo Hong", "Honglu Fan", "Heejeong Nam", "Hyunwoo Ko", "Seungwon Lim", "Jinyeop Song", "Jinha Choi", "Gonçalo Paulo", "Youngjae Yu", "Stella Biderman"], "title": "When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research", "categories": ["cs.CL"], "comment": "work in progress", "summary": "Recent advances in large language models (LLMs) have fueled the vision of\nautomated scientific discovery, often called AI Co-Scientists. To date, prior\nwork casts these systems as generative co-authors responsible for crafting\nhypotheses, synthesizing code, or drafting manuscripts. In this work, we\nexplore a complementary application: using LLMs as verifiers to automate the\n\\textbf{academic verification of scientific manuscripts}. To that end, we\nintroduce SPOT, a dataset of 83 published papers paired with 91 errors\nsignificant enough to prompt errata or retraction, cross-validated with actual\nauthors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find\nthat none surpasses 21.1\\% recall or 6.1\\% precision (o3 achieves the best\nscores, with all others near zero). Furthermore, confidence estimates are\nuniformly low, and across eight independent runs, models rarely rediscover the\nsame errors, undermining their reliability. Finally, qualitative analysis with\ndomain experts reveals that even the strongest models make mistakes resembling\nstudent-level misconceptions derived from misunderstandings. These findings\nhighlight the substantial gap between current LLM capabilities and the\nrequirements for dependable AI-assisted academic verification."}
{"id": "2505.11866", "pdf": "https://arxiv.org/pdf/2505.11866", "abs": "https://arxiv.org/abs/2505.11866", "authors": ["Ali A. Minai"], "title": "Position Paper: Bounded Alignment: What (Not) To Expect From AGI Agents", "categories": ["cs.AI", "I.2.0; I.2.6"], "comment": "Paper accepted for the 2025 IEEE/INNS International Joint Conference\n  on Neural Networks, Rome, Italy, June 30 - July 5, 2025", "summary": "The issues of AI risk and AI safety are becoming critical as the prospect of\nartificial general intelligence (AGI) looms larger. The emergence of extremely\nlarge and capable generative models has led to alarming predictions and created\na stir from boardrooms to legislatures. As a result, AI alignment has emerged\nas one of the most important areas in AI research. The goal of this position\npaper is to argue that the currently dominant vision of AGI in the AI and\nmachine learning (AI/ML) community needs to evolve, and that expectations and\nmetrics for its safety must be informed much more by our understanding of the\nonly existing instance of general intelligence, i.e., the intelligence found in\nanimals, and especially in humans. This change in perspective will lead to a\nmore realistic view of the technology, and allow for better policy decisions."}
{"id": "2505.11664", "pdf": "https://arxiv.org/pdf/2505.11664", "abs": "https://arxiv.org/abs/2505.11664", "authors": ["Ziqing Xu", "Hancheng Min", "Salma Tarmoun", "Enrique Mallada", "Rene Vidal"], "title": "A Local Polyak-Lojasiewicz and Descent Lemma of Gradient Descent For Overparametrized Linear Models", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "Most prior work on the convergence of gradient descent (GD) for\noverparameterized neural networks relies on strong assumptions on the step size\n(infinitesimal), the hidden-layer width (infinite), or the initialization\n(large, spectral, balanced). Recent efforts to relax these assumptions focus on\ntwo-layer linear networks trained with the squared loss. In this work, we\nderive a linear convergence rate for training two-layer linear neural networks\nwith GD for general losses and under relaxed assumptions on the step size,\nwidth, and initialization. A key challenge in deriving this result is that\nclassical ingredients for deriving convergence rates for nonconvex problems,\nsuch as the Polyak-{\\L}ojasiewicz (PL) condition and Descent Lemma, do not hold\nglobally for overparameterized neural networks. Here, we prove that these two\nconditions hold locally with local constants that depend on the weights. Then,\nwe provide bounds on these local constants, which depend on the initialization\nof the weights, the current loss, and the global PL and smoothness constants of\nthe non-overparameterized model. Based on these bounds, we derive a linear\nconvergence rate for GD. Our convergence analysis not only improves upon prior\nresults but also suggests a better choice for the step size, as verified\nthrough our numerical experiments."}
{"id": "2505.11876", "pdf": "https://arxiv.org/pdf/2505.11876", "abs": "https://arxiv.org/abs/2505.11876", "authors": ["Yanbo Dai", "Zhenlan Ji", "Zongjie Li", "Shuai Wang"], "title": "NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Model editing techniques are essential for efficiently updating knowledge in\nlarge language models (LLMs). However, the effectiveness of existing approaches\ndegrades in massive editing scenarios, particularly when evaluated with\npractical metrics or in context-rich settings. We attribute these failures to\nembedding collisions among knowledge items, which undermine editing reliability\nat scale. To address this, we propose NAMET (Noise-aware Model Editing in\nTransformers), a simple yet effective method that introduces noise during\nmemory extraction via a one-line modification to MEMIT. Extensive experiments\nacross six LLMs and three datasets demonstrate that NAMET consistently\noutperforms existing methods when editing thousands of facts."}
{"id": "2505.11899", "pdf": "https://arxiv.org/pdf/2505.11899", "abs": "https://arxiv.org/abs/2505.11899", "authors": ["Yongan Yu", "Alexandre Krantz", "Nikki G. Lobczowski"], "title": "From Recall to Reasoning: Automated Question Generation for Deeper Math Learning through Large Language Models", "categories": ["cs.AI"], "comment": "8 pages, 2 figures, accepted by AIED conference", "summary": "Educators have started to turn to Generative AI (GenAI) to help create new\ncourse content, but little is known about how they should do so. In this\nproject, we investigated the first steps for optimizing content creation for\nadvanced math. In particular, we looked at the ability of GenAI to produce\nhigh-quality practice problems that are relevant to the course content. We\nconducted two studies to: (1) explore the capabilities of current versions of\npublicly available GenAI and (2) develop an improved framework to address the\nlimitations we found. Our results showed that GenAI can create math problems at\nvarious levels of quality with minimal support, but that providing examples and\nrelevant content results in better quality outputs. This research can help\neducators decide the ideal way to adopt GenAI in their workflows, to create\nmore effective educational experiences for students."}
{"id": "2505.11669", "pdf": "https://arxiv.org/pdf/2505.11669", "abs": "https://arxiv.org/abs/2505.11669", "authors": ["Yiming Zhang", "Sitong Liu", "Alex Cloninger"], "title": "OT Score: An OT based Confidence Score for Unsupervised Domain Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We address the computational and theoretical limitations of existing\ndistributional alignment methods for unsupervised domain adaptation (UDA),\nparticularly regarding the estimation of classification performance and\nconfidence without target labels. Current theoretical frameworks for these\nmethods often yield computationally intractable quantities and fail to\nadequately reflect the properties of the alignment algorithms employed. To\novercome these challenges, we introduce the Optimal Transport (OT) score, a\nconfidence metric derived from a novel theoretical analysis that exploits the\nflexibility of decision boundaries induced by Semi-Discrete Optimal Transport\nalignment. The proposed OT score is intuitively interpretable, theoretically\nrigorous, and computationally efficient. It provides principled uncertainty\nestimates for any given set of target pseudo-labels without requiring model\nretraining, and can flexibly adapt to varying degrees of available source\ninformation. Experimental results on standard UDA benchmarks demonstrate that\nclassification accuracy consistently improves by identifying and removing\nlow-confidence predictions, and that OT score significantly outperforms\nexisting confidence metrics across diverse adaptation scenarios."}
{"id": "2505.11887", "pdf": "https://arxiv.org/pdf/2505.11887", "abs": "https://arxiv.org/abs/2505.11887", "authors": ["Xiechi Zhang", "Zetian Ouyang", "Linlin Wang", "Gerard de Melo", "Zhu Cao", "Xiaoling Wang", "Ya Zhang", "Yanfeng Wang", "Liang He"], "title": "AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "With the proliferation of large language models (LLMs) in the medical domain,\nthere is increasing demand for improved evaluation techniques to assess their\ncapabilities. However, traditional metrics like F1 and ROUGE, which rely on\ntoken overlaps to measure quality, significantly overlook the importance of\nmedical terminology. While human evaluation tends to be more reliable, it can\nbe very costly and may as well suffer from inaccuracies due to limits in human\nexpertise and motivation. Although there are some evaluation methods based on\nLLMs, their usability in the medical field is limited due to their proprietary\nnature or lack of expertise. To tackle these challenges, we present\nAutoMedEval, an open-sourced automatic evaluation model with 13B parameters\nspecifically engineered to measure the question-answering proficiency of\nmedical LLMs. The overarching objective of AutoMedEval is to assess the quality\nof responses produced by diverse models, aspiring to significantly reduce the\ndependence on human evaluation. Specifically, we propose a hierarchical\ntraining method involving curriculum instruction tuning and an iterative\nknowledge introspection mechanism, enabling AutoMedEval to acquire professional\nmedical assessment capabilities with limited instructional data. Human\nevaluations indicate that AutoMedEval surpasses other baselines in terms of\ncorrelation with human judgments."}
{"id": "2505.11942", "pdf": "https://arxiv.org/pdf/2505.11942", "abs": "https://arxiv.org/abs/2505.11942", "authors": ["Junhao Zheng", "Xidi Cai", "Qiuke Li", "Duzhen Zhang", "ZhongZhi Li", "Yingying Zhang", "Le Song", "Qianli Ma"], "title": "LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners", "categories": ["cs.AI"], "comment": null, "summary": "Lifelong learning is essential for intelligent agents operating in dynamic\nenvironments. Current large language model (LLM)-based agents, however, remain\nstateless and unable to accumulate or transfer knowledge over time. Existing\nbenchmarks treat agents as static systems and fail to evaluate lifelong\nlearning capabilities. We present LifelongAgentBench, the first unified\nbenchmark designed to systematically assess the lifelong learning ability of\nLLM agents. It provides skill-grounded, interdependent tasks across three\ninteractive environments, Database, Operating System, and Knowledge Graph, with\nautomatic label verification, reproducibility, and modular extensibility.\nExtensive experiments reveal that conventional experience replay has limited\neffectiveness for LLM agents due to irrelevant information and context length\nconstraints. We further introduce a group self-consistency mechanism that\nsignificantly improves lifelong learning performance. We hope\nLifelongAgentBench will advance the development of adaptive, memory-capable LLM\nagents."}
{"id": "2505.11682", "pdf": "https://arxiv.org/pdf/2505.11682", "abs": "https://arxiv.org/abs/2505.11682", "authors": ["Ananyae Kumar Bhartari", "Vinayak Vinayak", "Vivek B Shenoy"], "title": "Mollifier Layers: Enabling Efficient High-Order Derivatives in Inverse PDE Learning", "categories": ["cs.LG"], "comment": null, "summary": "Parameter estimation in inverse problems involving partial differential\nequations (PDEs) underpins modeling across scientific disciplines, especially\nwhen parameters vary in space or time. Physics-informed Machine Learning\n(PhiML) integrates PDE constraints into deep learning, but prevailing\napproaches depend on recursive automatic differentiation (autodiff), which\nproduces inaccurate high-order derivatives, inflates memory usage, and\nunderperforms in noisy settings. We propose Mollifier Layers, a lightweight,\narchitecture-agnostic module that replaces autodiff with convolutional\noperations using analytically defined mollifiers. This reframing of derivative\ncomputation as smoothing integration enables efficient, noise-robust estimation\nof high-order derivatives directly from network outputs. Mollifier Layers\nattach at the output layer and require no architectural modifications. We\ncompare them with three distinct architectures and benchmark performance across\nfirst-, second-, and fourth-order PDEs -- including Langevin dynamics, heat\ndiffusion, and reaction-diffusion systems -- observing significant improvements\nin memory efficiency, training time and accuracy for parameter recovery across\ntasks. To demonstrate practical relevance, we apply Mollifier Layers to infer\nspatially varying epigenetic reaction rates from super-resolution chromatin\nimaging data -- a real-world inverse problem with biomedical significance. Our\nresults establish Mollifier Layers as an efficient and scalable tool for\nphysics-constrained learning."}
{"id": "2505.11891", "pdf": "https://arxiv.org/pdf/2505.11891", "abs": "https://arxiv.org/abs/2505.11891", "authors": ["Weikai Xu", "Zhizheng Jiang", "Yuxuan Liu", "Wei Liu", "Jian Luan", "Yuanchun Li", "Yunxin Liu", "Bin Wang", "Bo An"], "title": "Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "VLM-based mobile agents are increasingly popular due to their capabilities to\ninteract with smartphone GUIs and XML-structured texts and to complete daily\ntasks. However, existing online benchmarks struggle with obtaining stable\nreward signals due to dynamic environmental changes. Offline benchmarks\nevaluate the agents through single-path trajectories, which stands in contrast\nto the inherently multi-solution characteristics of GUI tasks. Additionally,\nboth types of benchmarks fail to assess whether mobile agents can handle noise\nor engage in proactive interactions due to a lack of noisy apps or overly full\ninstructions during the evaluation process. To address these limitations, we\nuse a slot-based instruction generation method to construct a more realistic\nand comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a\ncommon task split, with offline multi-path evaluation to assess the agent's\nability to obtain step rewards during task execution. It contains a noisy split\nbased on pop-ups and ads apps, and a contaminated split named AITZ-Noise to\nformulate a real noisy environment. Furthermore, an ambiguous instruction split\nwith preset Q\\&A interactions is released to evaluate the agent's proactive\ninteraction capabilities. We conduct evaluations on these splits using the\nsingle-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2,\nas well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are\navailable at https://huggingface.co/datasets/xwk123/MobileBench-v2."}
{"id": "2505.11962", "pdf": "https://arxiv.org/pdf/2505.11962", "abs": "https://arxiv.org/abs/2505.11962", "authors": ["Zoya Volovikova", "Gregory Gorbov", "Petr Kuderov", "Aleksandr I. Panov", "Alexey Skrynnik"], "title": "CrafText Benchmark: Advancing Instruction Following in Complex Multimodal Open-Ended World", "categories": ["cs.AI"], "comment": null, "summary": "Following instructions in real-world conditions requires the ability to adapt\nto the world's volatility and entanglement: the environment is dynamic and\nunpredictable, instructions can be linguistically complex with diverse\nvocabulary, and the number of possible goals an agent may encounter is vast.\nDespite extensive research in this area, most studies are conducted in static\nenvironments with simple instructions and a limited vocabulary, making it\ndifficult to assess agent performance in more diverse and challenging settings.\nTo address this gap, we introduce CrafText, a benchmark for evaluating\ninstruction following in a multimodal environment with diverse instructions and\ndynamic interactions. CrafText includes 3,924 instructions with 3,423 unique\nwords, covering Localization, Conditional, Building, and Achievement tasks.\nAdditionally, we propose an evaluation protocol that measures an agent's\nability to generalize to novel instruction formulations and dynamically\nevolving task configurations, providing a rigorous test of both linguistic\nunderstanding and adaptive decision-making."}
{"id": "2505.11692", "pdf": "https://arxiv.org/pdf/2505.11692", "abs": "https://arxiv.org/abs/2505.11692", "authors": ["Sahil Rajesh Dhayalkar"], "title": "The Geometry of ReLU Networks through the ReLU Transition Graph", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "13 pages, 4 figures", "summary": "We develop a novel theoretical framework for analyzing ReLU neural networks\nthrough the lens of a combinatorial object we term the ReLU Transition Graph\n(RTG). In this graph, each node corresponds to a linear region induced by the\nnetwork's activation patterns, and edges connect regions that differ by a\nsingle neuron flip. Building on this structure, we derive a suite of new\ntheoretical results connecting RTG geometry to expressivity, generalization,\nand robustness. Our contributions include tight combinatorial bounds on RTG\nsize and diameter, a proof of RTG connectivity, and graph-theoretic\ninterpretations of VC-dimension. We also relate entropy and average degree of\nthe RTG to generalization error. Each theoretical result is rigorously\nvalidated via carefully controlled experiments across varied network depths,\nwidths, and data regimes. This work provides the first unified treatment of\nReLU network structure via graph theory and opens new avenues for compression,\nregularization, and complexity control rooted in RTG analysis."}
{"id": "2505.11893", "pdf": "https://arxiv.org/pdf/2505.11893", "abs": "https://arxiv.org/abs/2505.11893", "authors": ["Zepeng Ding", "Dixuan Wang", "Ziqin Luo", "Guochao Jiang", "Deqing Yang", "Jiaqing Liang"], "title": "RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-step planning has been widely employed to enhance the performance of\nlarge language models (LLMs) on downstream natural language processing (NLP)\ntasks, which decomposes the original task into multiple subtasks and guide LLMs\nto solve them sequentially without additional training. When addressing task\ninstances, existing methods either preset the order of steps or attempt\nmultiple paths at each step. However, these methods overlook instances'\nlinguistic features and rely on the intrinsic planning capabilities of LLMs to\nevaluate intermediate feedback and then select subtasks, resulting in\nsuboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this\npaper we propose a Reinforcement Learning enhanced Adaptive Planning framework\n(RLAP). In our framework, we model an NLP task as a Markov decision process\n(MDP) and employ an LLM directly into the environment. In particular, a\nlightweight Actor model is trained to estimate Q-values for natural language\nsequences consisting of states and actions through reinforcement learning.\nTherefore, during sequential planning, the linguistic features of each sequence\nin the MDP can be taken into account, and the Actor model interacts with the\nLLM to determine the optimal order of subtasks for each task instance. We apply\nRLAP on three different types of NLP tasks and conduct extensive experiments on\nmultiple datasets to verify RLAP's effectiveness and robustness."}
{"id": "2505.11966", "pdf": "https://arxiv.org/pdf/2505.11966", "abs": "https://arxiv.org/abs/2505.11966", "authors": ["Jianyuan Zhong", "Zeju Li", "Zhijian Xu", "Xiangyu Wen", "Kezhi Li", "Qiang Xu"], "title": "Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative Verifier", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Model (LLM) reasoning for complex tasks inherently involves a\ntrade-off between solution accuracy and computational efficiency. The\nsubsequent step of verification, while intended to improve performance, further\ncomplicates this landscape by introducing its own challenging trade-off:\nsophisticated Generative Reward Models (GenRMs) can be computationally\nprohibitive if naively integrated with LLMs at test-time, while simpler, faster\nmethods may lack reliability. To overcome these challenges, we introduce\nFlexiVe, a novel generative verifier that flexibly balances computational\nresources between rapid, reliable fast thinking and meticulous slow thinking\nusing a Flexible Allocation of Verification Budget strategy. We further propose\nthe Solve-Detect-Verify pipeline, an efficient inference-time scaling framework\nthat intelligently integrates FlexiVe, proactively identifying solution\ncompletion points to trigger targeted verification and provide focused solver\nfeedback. Experiments show FlexiVe achieves superior accuracy in pinpointing\nerrors within reasoning traces on ProcessBench. Furthermore, on challenging\nmathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full\napproach outperforms baselines like self-consistency in reasoning accuracy and\ninference efficiency. Our system offers a scalable and effective solution to\nenhance LLM reasoning at test time."}
{"id": "2505.11694", "pdf": "https://arxiv.org/pdf/2505.11694", "abs": "https://arxiv.org/abs/2505.11694", "authors": ["Sahil Rajesh Dhayalkar"], "title": "Neural Networks as Universal Finite-State Machines: A Constructive Deterministic Finite Automaton Theory", "categories": ["cs.LG", "cs.AI", "cs.FL"], "comment": "15 pages, 1 figure", "summary": "We present a complete theoretical and empirical framework establishing\nfeedforward neural networks as universal finite-state machines (N-FSMs). Our\nresults prove that finite-depth ReLU and threshold networks can exactly\nsimulate deterministic finite automata (DFAs) by unrolling state transitions\ninto depth-wise neural layers, with formal characterizations of required depth,\nwidth, and state compression. We demonstrate that DFA transitions are linearly\nseparable, binary threshold activations allow exponential compression, and\nMyhill-Nerode equivalence classes can be embedded into continuous latent spaces\nwhile preserving separability. We also formalize the expressivity boundary:\nfixed-depth feedforward networks cannot recognize non-regular languages\nrequiring unbounded memory. Unlike prior heuristic or probing-based studies, we\nprovide constructive proofs and design explicit DFA-unrolled neural\narchitectures that empirically validate every claim. Our results bridge deep\nlearning, automata theory, and neural-symbolic computation, offering a rigorous\nblueprint for how discrete symbolic processes can be realized in continuous\nneural systems."}
{"id": "2505.11900", "pdf": "https://arxiv.org/pdf/2505.11900", "abs": "https://arxiv.org/abs/2505.11900", "authors": ["Philipp Christmann", "Gerhard Weikum"], "title": "Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at ACL 2025 (Findings)", "summary": "Question answering over mixed sources, like text and tables, has been\nadvanced by verbalizing all contents and encoding it with a language model. A\nprominent case of such heterogeneous data is personal information: user devices\nlog vast amounts of data every day, such as calendar entries, workout\nstatistics, shopping records, streaming history, and more. Information needs\nrange from simple look-ups to queries of analytical nature. The challenge is to\nprovide humans with convenient access with small footprint, so that all\npersonal data stays on the user devices. We present ReQAP, a novel method that\ncreates an executable operator tree for a given question, via recursive\ndecomposition. Operators are designed to enable seamless integration of\nstructured and unstructured sources, and the execution of the operator tree\nyields a traceable answer. We further release the PerQA benchmark, with\npersona-based data and questions, covering a diverse spectrum of realistic user\nneeds."}
{"id": "2505.11999", "pdf": "https://arxiv.org/pdf/2505.11999", "abs": "https://arxiv.org/abs/2505.11999", "authors": ["Chang Liu", "Huan Yan", "Hongjie Sui", "Haomin Wen", "Yuan Yuan", "Yuyang Han", "Hongsen Liao", "Xuetao Ding", "Jinghua Hao", "Yong Li"], "title": "MRGRP: Empowering Courier Route Prediction in Food Delivery Service with Multi-Relational Graph", "categories": ["cs.AI"], "comment": null, "summary": "Instant food delivery has become one of the most popular web services\nworldwide due to its convenience in daily life. A fundamental challenge is\naccurately predicting courier routes to optimize task dispatch and improve\ndelivery efficiency. This enhances satisfaction for couriers and users and\nincreases platform profitability. The current heuristic prediction method uses\nonly limited human-selected task features and ignores couriers preferences,\ncausing suboptimal results. Additionally, existing learning-based methods do\nnot fully capture the diverse factors influencing courier decisions or the\ncomplex relationships among them. To address this, we propose a\nMulti-Relational Graph-based Route Prediction (MRGRP) method that models\nfine-grained correlations among tasks affecting courier decisions for accurate\nprediction. We encode spatial and temporal proximity, along with\npickup-delivery relationships, into a multi-relational graph and design a\nGraphFormer architecture to capture these complex connections. We also\nintroduce a route decoder that leverages courier information and dynamic\ndistance and time contexts for prediction, using existing route solutions as\nreferences to improve outcomes. Experiments show our model achieves\nstate-of-the-art route prediction on offline data from cities of various sizes.\nDeployed on the Meituan Turing platform, it surpasses the current heuristic\nalgorithm, reaching a high route prediction accuracy of 0.819, essential for\ncourier and user satisfaction in instant food delivery."}
{"id": "2505.11695", "pdf": "https://arxiv.org/pdf/2505.11695", "abs": "https://arxiv.org/abs/2505.11695", "authors": ["Shihao Zhang", "Haoyu Zhang", "Ian Colbert", "Rayan Saab"], "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training Quantization", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches."}
{"id": "2505.11908", "pdf": "https://arxiv.org/pdf/2505.11908", "abs": "https://arxiv.org/abs/2505.11908", "authors": ["Zhangyu Wang", "Siyuan Gao", "Rong Zhou", "Hao Wang", "Li Ning"], "title": "ELITE: Embedding-Less retrieval with Iterative Text Exploration", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive progress in natural\nlanguage processing, but their limited ability to retain long-term context\nconstrains performance on document-level or multi-turn tasks.\nRetrieval-Augmented Generation (RAG) mitigates this by retrieving relevant\ninformation from an external corpus. However, existing RAG systems often rely\non embedding-based retrieval trained on corpus-level semantic similarity, which\ncan lead to retrieving content that is semantically similar in form but\nmisaligned with the question's true intent. Furthermore, recent RAG variants\nconstruct graph- or hierarchy-based structures to improve retrieval accuracy,\nresulting in significant computation and storage overhead. In this paper, we\npropose an embedding-free retrieval framework. Our method leverages the logical\ninferencing ability of LLMs in retrieval using iterative search space\nrefinement guided by our novel importance measure and extend our retrieval\nresults with logically related information without explicit graph construction.\nExperiments on long-context QA benchmarks, including NovelQA and Marathon, show\nthat our approach outperforms strong baselines while reducing storage and\nruntime by over an order of magnitude."}
{"id": "2505.12001", "pdf": "https://arxiv.org/pdf/2505.12001", "abs": "https://arxiv.org/abs/2505.12001", "authors": ["Ruta Binkyte"], "title": "Interactional Fairness in LLM Multi-Agent Systems: An Evaluation Framework", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "As large language models (LLMs) are increasingly used in multi-agent systems,\nquestions of fairness should extend beyond resource distribution and procedural\ndesign to include the fairness of how agents communicate. Drawing from\norganizational psychology, we introduce a novel framework for evaluating\nInteractional fairness encompassing Interpersonal fairness (IF) and\nInformational fairness (InfF) in LLM-based multi-agent systems (LLM-MAS). We\nextend the theoretical grounding of Interactional Fairness to non-sentient\nagents, reframing fairness as a socially interpretable signal rather than a\nsubjective experience. We then adapt established tools from organizational\njustice research, including Colquitt's Organizational Justice Scale and the\nCritical Incident Technique, to measure fairness as a behavioral property of\nagent interaction. We validate our framework through a pilot study using\ncontrolled simulations of a resource negotiation task. We systematically\nmanipulate tone, explanation quality, outcome inequality, and task framing\n(collaborative vs. competitive) to assess how IF influences agent behavior.\nResults show that tone and justification quality significantly affect\nacceptance decisions even when objective outcomes are held constant. In\naddition, the influence of IF vs. InfF varies with context. This work lays the\nfoundation for fairness auditing and norm-sensitive alignment in LLM-MAS."}
{"id": "2505.11702", "pdf": "https://arxiv.org/pdf/2505.11702", "abs": "https://arxiv.org/abs/2505.11702", "authors": ["Keenan Eikenberry", "Lizuo Liu", "Yoonsang Lee"], "title": "Invariant Representations via Wasserstein Correlation Maximization", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This work investigates the use of Wasserstein correlation -- a normalized\nmeasure of statistical dependence based on the Wasserstein distance between a\njoint distribution and the product of its marginals -- for unsupervised\nrepresentation learning. Unlike, for example, contrastive methods, which\nnaturally cluster classes in the latent space, we find that an (auto)encoder\ntrained to maximize Wasserstein correlation between the input and encoded\ndistributions instead acts as a compressor, reducing dimensionality while\napproximately preserving the topological and geometric properties of the input\ndistribution. More strikingly, we show that Wasserstein correlation\nmaximization can be used to arrive at an (auto)encoder -- either trained from\nscratch, or else one that extends a frozen, pretrained model -- that is\napproximately invariant to a chosen augmentation, or collection of\naugmentations, and that still approximately preserves the structural properties\nof the non-augmented input distribution. To do this, we first define the notion\nof an augmented encoder using the machinery of Markov-Wasserstein kernels. When\nthe maximization objective is then applied to the augmented encoder, as opposed\nto the underlying, deterministic encoder, the resulting model exhibits the\ndesired invariance properties. Finally, besides our experimental results, which\nshow that even simple feedforward networks can be imbued with invariants or\ncan, alternatively, be used to impart invariants to pretrained models under\nthis training process, we additionally establish various theoretical results\nfor optimal transport-based dependence measures. Code is available at\nhttps://github.com/keenan-eikenberry/wasserstein_correlation_maximization ."}
{"id": "2505.11922", "pdf": "https://arxiv.org/pdf/2505.11922", "abs": "https://arxiv.org/abs/2505.11922", "authors": ["Yuheng Lu", "ZiMeng Bai", "Caixia Yuan", "Huixing Jiang", "Xiaojie Wang"], "title": "Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable capabilities in handling\nnatural language tasks; however, they may struggle to consistently follow\ncomplex instructions including those involve multiple constraints.\nPost-training LLMs using supervised fine-tuning (SFT) is a standard approach to\nimprove their ability to follow instructions. In addressing complex instruction\nfollowing, existing efforts primarily focus on data-driven methods that\nsynthesize complex instruction-output pairs for SFT. However, insufficient\nattention allocated to crucial sub-contexts may reduce the effectiveness of\nSFT. In this work, we propose transforming sequentially structured input\ninstruction into multiple parallel instructions containing subcontexts. To\nsupport processing this multi-input, we propose MISO (Multi-Input\nSingle-Output), an extension to currently dominant decoder-only\ntransformer-based LLMs. MISO introduces a mixture-of-contexts paradigm that\njointly considers the overall instruction-output alignment and the influence of\nindividual sub-contexts to enhance SFT effectiveness. We apply MISO fine-tuning\nto complex instructionfollowing datasets and evaluate it with standard LLM\ninference. Empirical results demonstrate the superiority of MISO as a\nfine-tuning method for LLMs, both in terms of effectiveness in complex\ninstruction-following scenarios and its potential for training efficiency."}
{"id": "2505.12006", "pdf": "https://arxiv.org/pdf/2505.12006", "abs": "https://arxiv.org/abs/2505.12006", "authors": ["Yuncheng Hua", "Ji Miao", "Mehdi Jafari", "Jianxiang Xie", "Hao Xue", "Flora D. Salim"], "title": "SOCIA: An End-to-End Agentic Framework for Automated Cyber-Physical-Social Simulator Generation", "categories": ["cs.AI", "I.2.7"], "comment": "28 pages, 3 figures, 2 tables. The paper is under review", "summary": "This paper introduces SOCIA (Simulation Orchestration for\nCyber-physical-social Intelligence and Agents), a novel end-to-end framework\nleveraging Large Language Model (LLM)-based multi-agent systems to automate the\ngeneration of high-fidelity Cyber-Physical-Social (CPS) simulators. Addressing\nthe challenges of labor-intensive manual simulator development and complex data\ncalibration, SOCIA integrates a centralized orchestration manager that\ncoordinates specialized agents for tasks including data comprehension, code\ngeneration, simulation execution, and iterative evaluation-feedback loops.\nThrough empirical evaluations across diverse CPS tasks, such as mask adoption\nbehavior simulation (social), personal mobility generation (physical), and user\nmodeling (cyber), SOCIA demonstrates its ability to produce high-fidelity,\nscalable simulations with reduced human intervention. These results highlight\nSOCIA's potential to offer a scalable solution for studying complex CPS\nphenomena"}
{"id": "2505.11711", "pdf": "https://arxiv.org/pdf/2505.11711", "abs": "https://arxiv.org/abs/2505.11711", "authors": ["Sagnik Mukherjee", "Lifan Yuan", "Dilek Hakkani-Tur", "Hao Peng"], "title": "Reinforcement Learning Finetunes Small Subnetworks in Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) yields substantial improvements in large language\nmodels (LLMs) downstream task performance and alignment with human values.\nSurprisingly, such large gains result from updating only a small subnetwork\ncomprising just 5 percent to 30 percent of the parameters, with the rest\neffectively unchanged. We refer to this phenomenon as parameter update sparsity\ninduced by RL. It is observed across all 7 widely used RL algorithms (e.g.,\nPPO, GRPO, DPO) and all 10 LLMs from different families in our experiments.\nThis sparsity is intrinsic and occurs without any explicit sparsity promoting\nregularizations or architectural constraints. Finetuning the subnetwork alone\nrecovers the test accuracy, and, remarkably, produces a model nearly identical\nto the one obtained via full finetuning. The subnetworks from different random\nseeds, training data, and even RL algorithms show substantially greater overlap\nthan expected by chance. Our analysis suggests that this sparsity is not due to\nupdating only a subset of layers, instead, nearly all parameter matrices\nreceive similarly sparse updates. Moreover, the updates to almost all parameter\nmatrices are nearly full-rank, suggesting RL updates a small subset of\nparameters that nevertheless span almost the full subspaces that the parameter\nmatrices can represent. We conjecture that the this update sparsity can be\nprimarily attributed to training on data that is near the policy distribution,\ntechniques that encourage the policy to remain close to the pretrained model,\nsuch as the KL regularization and gradient clipping, have limited impact."}
{"id": "2505.11924", "pdf": "https://arxiv.org/pdf/2505.11924", "abs": "https://arxiv.org/abs/2505.11924", "authors": ["Yu-Ting Lee", "Hui-Ying Shih", "Fu-Chieh Chang", "Pei-Yuan Wu"], "title": "An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We provide an explanation for the performance gains of intrinsic\nself-correction, a process where a language model iteratively refines its\noutputs without external feedback. More precisely, we investigate how prompting\ninduces interpretable changes in hidden states and thus affects the output\ndistributions. We hypothesize that each prompt-induced shift lies in a linear\nspan of some linear representation vectors, naturally separating tokens based\non individual concept alignment. Building around this idea, we give a\nmathematical formulation of self-correction and derive a concentration result\nfor output tokens based on alignment magnitudes. Our experiments on text\ndetoxification with zephyr-7b-sft reveal a substantial gap in the inner\nproducts of the prompt-induced shifts and the unembeddings of the top-100 most\ntoxic tokens vs. those of the unembeddings of the bottom-100 least toxic\ntokens, under toxic instructions. This suggests that self-correction prompts\nenhance a language model's capability of latent concept recognition. Our\nanalysis offers insights into the underlying mechanism of self-correction by\ncharacterizing how prompting works explainably. For reproducibility, our code\nis available."}
{"id": "2505.12012", "pdf": "https://arxiv.org/pdf/2505.12012", "abs": "https://arxiv.org/abs/2505.12012", "authors": ["Georgios Pavlidis"], "title": "Empowering Sustainable Finance with Artificial Intelligence: A Framework for Responsible Implementation", "categories": ["cs.AI"], "comment": null, "summary": "This chapter explores the convergence of two major developments: the rise of\nenvironmental, social, and governance (ESG) investing and the exponential\ngrowth of artificial intelligence (AI) technology. The increased demand for\ndiverse ESG instruments, such as green and ESG-linked loans, will be aligned\nwith the rapid growth of the global AI market, which is expected to be worth\n$1,394.30 billion by 2029. AI can assist in identifying and pricing climate\nrisks, setting more ambitious ESG goals, and advancing sustainable finance\ndecisions. However, delegating sustainable finance decisions to AI poses\nserious risks, and new principles and rules for AI and ESG investing are\nnecessary to mitigate these risks. This chapter highlights the challenges\nassociated with norm-setting initiatives and stresses the need for the\nfine-tuning of the principles of legitimacy, oversight and verification,\ntransparency, and explainability. Finally, the chapter contends that\nintegrating AI into ESG non-financial reporting necessitates a heightened sense\nof responsibility and the establishment of fundamental guiding principles\nwithin the spheres of AI and ESG investing."}
{"id": "2505.11714", "pdf": "https://arxiv.org/pdf/2505.11714", "abs": "https://arxiv.org/abs/2505.11714", "authors": ["Arjun Prakash", "Naicheng He", "Denizalp Goktas", "Amy Greenwald"], "title": "Bi-Level Policy Optimization with Nyström Hypergradients", "categories": ["cs.LG", "cs.AI", "cs.GT"], "comment": null, "summary": "The dependency of the actor on the critic in actor-critic (AC) reinforcement\nlearning means that AC can be characterized as a bilevel optimization (BLO)\nproblem, also called a Stackelberg game. This characterization motivates two\nmodifications to vanilla AC algorithms. First, the critic's update should be\nnested to learn a best response to the actor's policy. Second, the actor should\nupdate according to a hypergradient that takes changes in the critic's behavior\ninto account. Computing this hypergradient involves finding an inverse Hessian\nvector product, a process that can be numerically unstable. We thus propose a\nnew algorithm, Bilevel Policy Optimization with Nystr\\\"om Hypergradients\n(BLPO), which uses nesting to account for the nested structure of BLO, and\nleverages the Nystr\\\"om method to compute the hypergradient. Theoretically, we\nprove BLPO converges to (a point that satisfies the necessary conditions for) a\nlocal strong Stackelberg equilibrium in polynomial time with high probability,\nassuming a linear parametrization of the critic's objective. Empirically, we\ndemonstrate that BLPO performs on par with or better than PPO on a variety of\ndiscrete and continuous control tasks."}
{"id": "2505.11932", "pdf": "https://arxiv.org/pdf/2505.11932", "abs": "https://arxiv.org/abs/2505.11932", "authors": ["Yuyao Zhang", "Zhicheng Dou", "Xiaoxi Li", "Jiajie Jin", "Yongkang Wu", "Zhonghua Li", "Qi Ye", "Ji-Rong Wen"], "title": "Neuro-Symbolic Query Compiler", "categories": ["cs.CL", "cs.IR"], "comment": "Findings of ACL2025, codes are available at this url:\n  https://github.com/YuyaoZhangQAQ/Query_Compiler", "summary": "Precise recognition of search intent in Retrieval-Augmented Generation (RAG)\nsystems remains a challenging goal, especially under resource constraints and\nfor complex queries with nested structures and dependencies. This paper\npresents QCompiler, a neuro-symbolic framework inspired by linguistic grammar\nrules and compiler design, to bridge this gap. It theoretically designs a\nminimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize\ncomplex queries. Unlike previous methods, this grammar maintains completeness\nwhile minimizing redundancy. Based on this, QCompiler includes a Query\nExpression Translator, a Lexical Syntax Parser, and a Recursive Descent\nProcessor to compile queries into Abstract Syntax Trees (ASTs) for execution.\nThe atomicity of the sub-queries in the leaf nodes ensures more precise\ndocument retrieval and response generation, significantly improving the RAG\nsystem's ability to address complex queries."}
{"id": "2505.12031", "pdf": "https://arxiv.org/pdf/2505.12031", "abs": "https://arxiv.org/abs/2505.12031", "authors": ["Junyu Lai", "Jiakun Zhang", "Shuo Xu", "Taolue Chen", "Zihang Wang", "Yao Yang", "Jiarui Zhang", "Chun Cao", "Jingwei Xu"], "title": "LLM-based Automated Theorem Proving Hinges on Scalable Synthetic Data Generation", "categories": ["cs.AI", "I.2.7"], "comment": "20 pages", "summary": "Recent advancements in large language models (LLMs) have sparked considerable\ninterest in automated theorem proving and a prominent line of research\nintegrates stepwise LLM-based provers into tree search. In this paper, we\nintroduce a novel proof-state exploration approach for training data synthesis,\ndesigned to produce diverse tactics across a wide range of intermediate proof\nstates, thereby facilitating effective one-shot fine-tuning of LLM as the\npolicy model. We also propose an adaptive beam size strategy, which effectively\ntakes advantage of our data synthesis method and achieves a trade-off between\nexploration and exploitation during tree search. Evaluations on the MiniF2F and\nProofNet benchmarks demonstrate that our method outperforms strong baselines\nunder the stringent Pass@1 metric, attaining an average pass rate of $60.74\\%$\non MiniF2F and $21.18\\%$ on ProofNet. These results underscore the impact of\nlarge-scale synthetic data in advancing automated theorem proving."}
{"id": "2505.11717", "pdf": "https://arxiv.org/pdf/2505.11717", "abs": "https://arxiv.org/abs/2505.11717", "authors": ["Xilong Wang", "John Bloch", "Zedian Shao", "Yuepeng Hu", "Shuyan Zhou", "Neil Zhenqiang Gong"], "title": "EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Multi-modal large language model (MLLM)-based web agents interact with\nwebpage environments by generating actions based on screenshots of the\nwebpages. Environmental prompt injection attacks manipulate the environment to\ninduce the web agent to perform a specific, attacker-chosen action--referred to\nas the target action. However, existing attacks suffer from limited\neffectiveness or stealthiness, or are impractical in real-world settings. In\nthis work, we propose EnvInjection, a new attack that addresses these\nlimitations. Our attack adds a perturbation to the raw pixel values of the\nrendered webpage, which can be implemented by modifying the webpage's source\ncode. After these perturbed pixels are mapped into a screenshot, the\nperturbation induces the web agent to perform the target action. We formulate\nthe task of finding the perturbation as an optimization problem. A key\nchallenge in solving this problem is that the mapping between raw pixel values\nand screenshot is non-differentiable, making it difficult to backpropagate\ngradients to the perturbation. To overcome this, we train a neural network to\napproximate the mapping and apply projected gradient descent to solve the\nreformulated optimization problem. Extensive evaluation on multiple webpage\ndatasets shows that EnvInjection is highly effective and significantly\noutperforms existing baselines."}
{"id": "2505.11935", "pdf": "https://arxiv.org/pdf/2505.11935", "abs": "https://arxiv.org/abs/2505.11935", "authors": ["Xuanle Zhao", "Xuexin Liu", "Haoyue Yang", "Xianzhen Luo", "Fanhu Zeng", "Jianling Li", "Qi Shi", "Chi Chen"], "title": "ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing", "categories": ["cs.CL"], "comment": "Accept by ACL2025 Findings, preprint version", "summary": "Although multimodal large language models (MLLMs) show promise in generating\nchart rendering code, chart editing presents a greater challenge. This\ndifficulty stems from its nature as a labor-intensive task for humans that also\ndemands MLLMs to integrate chart understanding, complex reasoning, and precise\nintent interpretation. While many MLLMs claim such editing capabilities,\ncurrent assessments typically rely on limited case studies rather than robust\nevaluation methodologies, highlighting the urgent need for a comprehensive\nevaluation framework. In this work, we propose ChartEdit, a new high-quality\nbenchmark designed for chart editing tasks. This benchmark comprises $1,405$\ndiverse editing instructions applied to $233$ real-world charts, with each\ninstruction-chart instance having been manually annotated and validated for\naccuracy. Utilizing ChartEdit, we evaluate the performance of 10 mainstream\nMLLMs across two types of experiments, assessing them at both the code and\nchart levels. The results suggest that large-scale models can generate code to\nproduce images that partially match the reference images. However, their\nability to generate accurate edits according to the instructions remains\nlimited. The state-of-the-art (SOTA) model achieves a score of only $59.96$,\nhighlighting significant challenges in precise modification. In contrast,\nsmall-scale models, including chart-domain models, struggle both with following\nediting instructions and generating overall chart images, underscoring the need\nfor further development in this area. Code is available at\nhttps://github.com/xxlllz/ChartEdit."}
{"id": "2505.12039", "pdf": "https://arxiv.org/pdf/2505.12039", "abs": "https://arxiv.org/abs/2505.12039", "authors": ["Renqi Chen", "Haoyang Su", "Shixiang Tang", "Zhenfei Yin", "Qi Wu", "Hui Li", "Ye Sun", "Nanqing Dong", "Wanli Ouyang", "Philip Torr"], "title": "AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research", "categories": ["cs.AI", "cs.CL", "physics.soc-ph"], "comment": null, "summary": "The Science of Science (SoS) explores the mechanisms underlying scientific\ndiscovery, and offers valuable insights for enhancing scientific efficiency and\nfostering innovation. Traditional approaches often rely on simplistic\nassumptions and basic statistical tools, such as linear regression and\nrule-based simulations, which struggle to capture the complexity and scale of\nmodern research ecosystems. The advent of artificial intelligence (AI) presents\na transformative opportunity for the next generation of SoS, enabling the\nautomation of large-scale pattern discovery and uncovering insights previously\nunattainable. This paper offers a forward-looking perspective on the\nintegration of Science of Science with AI for automated research pattern\ndiscovery and highlights key open challenges that could greatly benefit from\nAI. We outline the advantages of AI over traditional methods, discuss potential\nlimitations, and propose pathways to overcome them. Additionally, we present a\npreliminary multi-agent system as an illustrative example to simulate research\nsocieties, showcasing AI's ability to replicate real-world research patterns\nand accelerate progress in Science of Science research."}
{"id": "2505.11725", "pdf": "https://arxiv.org/pdf/2505.11725", "abs": "https://arxiv.org/abs/2505.11725", "authors": ["Imon Banerjee", "Sayak Chakrabarty"], "title": "CLT and Edgeworth Expansion for m-out-of-n Bootstrap Estimators of The Studentized Median", "categories": ["cs.LG", "cs.AI", "cs.CE", "math.ST", "stat.ME", "stat.ML", "stat.TH"], "comment": "48 pages", "summary": "The m-out-of-n bootstrap, originally proposed by Bickel, Gotze, and Zwet\n(1992), approximates the distribution of a statistic by repeatedly drawing m\nsubsamples (with m much smaller than n) without replacement from an original\nsample of size n. It is now routinely used for robust inference with\nheavy-tailed data, bandwidth selection, and other large-sample applications.\nDespite its broad applicability across econometrics, biostatistics, and machine\nlearning, rigorous parameter-free guarantees for the soundness of the\nm-out-of-n bootstrap when estimating sample quantiles have remained elusive.\n  This paper establishes such guarantees by analyzing the estimator of sample\nquantiles obtained from m-out-of-n resampling of a dataset of size n. We first\nprove a central limit theorem for a fully data-driven version of the estimator\nthat holds under a mild moment condition and involves no unknown nuisance\nparameters. We then show that the moment assumption is essentially tight by\nconstructing a counter-example in which the CLT fails. Strengthening the\nassumptions slightly, we derive an Edgeworth expansion that provides exact\nconvergence rates and, as a corollary, a Berry Esseen bound on the bootstrap\napproximation error. Finally, we illustrate the scope of our results by\nderiving parameter-free asymptotic distributions for practical statistics,\nincluding the quantiles for random walk Metropolis-Hastings and the rewards of\nergodic Markov decision processes, thereby demonstrating the usefulness of our\ntheory in modern estimation and learning tasks."}
{"id": "2505.11958", "pdf": "https://arxiv.org/pdf/2505.11958", "abs": "https://arxiv.org/abs/2505.11958", "authors": ["Aswini Kumar Padhi", "Anil Bandhakavi", "Tanmoy Chakraborty"], "title": "Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning", "categories": ["cs.CL"], "comment": null, "summary": "Counterspeech has proven to be a powerful tool to combat hate speech online.\nPrevious studies have focused on generating counterspeech conditioned only on\nspecific intents (single attributed). However, a holistic approach considering\nmultiple attributes simultaneously can yield more nuanced and effective\nresponses. Here, we introduce HiPPrO, Hierarchical Prefix learning with\nPreference Optimization, a novel two-stage framework that utilizes the\neffectiveness of attribute-specific prefix embedding spaces hierarchically\noptimized during the counterspeech generation process in the first phase.\nThereafter, we incorporate both reference and reward-free preference\noptimization to generate more constructive counterspeech. Furthermore, we\nextend IntentCONANv2 by annotating all 13,973 counterspeech instances with\nemotion labels by five annotators. HiPPrO leverages hierarchical prefix\noptimization to integrate these dual attributes effectively. An extensive\nevaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent\nconformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L,\nrespectively, compared to several baseline models. Human evaluations further\nsubstantiate the superiority of our approach, highlighting the enhanced\nrelevance and appropriateness of the generated counterspeech. This work\nunderscores the potential of multi-attribute conditioning in advancing the\nefficacy of counterspeech generation systems."}
{"id": "2505.12057", "pdf": "https://arxiv.org/pdf/2505.12057", "abs": "https://arxiv.org/abs/2505.12057", "authors": ["Jing Zou", "Qingqiu Li", "Chenyu Lian", "Lihao Liu", "Xiaohan Yan", "Shujun Wang", "Jing Qin"], "title": "CorBenchX: Large-Scale Chest X-Ray Error Dataset and Vision-Language Model Benchmark for Report Error Correction", "categories": ["cs.AI"], "comment": "12 pages, 5figures", "summary": "AI-driven models have shown great promise in detecting errors in radiology\nreports, yet the field lacks a unified benchmark for rigorous evaluation of\nerror detection and further correction. To address this gap, we introduce\nCorBenchX, a comprehensive suite for automated error detection and correction\nin chest X-ray reports, designed to advance AI-assisted quality control in\nclinical practice. We first synthesize a large-scale dataset of 26,326 chest\nX-ray error reports by injecting clinically common errors via prompting\nDeepSeek-R1, with each corrupted report paired with its original text, error\ntype, and human-readable description. Leveraging this dataset, we benchmark\nboth open- and closed-source vision-language models,(e.g., InternVL, Qwen-VL,\nGPT-4o, o4-mini, and Claude-3.7) for error detection and correction under\nzero-shot prompting. Among these models, o4-mini achieves the best performance,\nwith 50.6 % detection accuracy and correction scores of BLEU 0.853, ROUGE\n0.924, BERTScore 0.981, SembScore 0.865, and CheXbertF1 0.954, remaining below\nclinical-level accuracy, highlighting the challenge of precise report\ncorrection. To advance the state of the art, we propose a multi-step\nreinforcement learning (MSRL) framework that optimizes a multi-objective reward\ncombining format compliance, error-type accuracy, and BLEU similarity. We apply\nMSRL to QwenVL2.5-7B, the top open-source model in our benchmark, achieving an\nimprovement of 38.3% in single-error detection precision and 5.2% in\nsingle-error correction over the zero-shot baseline."}
{"id": "2505.11731", "pdf": "https://arxiv.org/pdf/2505.11731", "abs": "https://arxiv.org/abs/2505.11731", "authors": ["Harshil Vejendla", "Haizhou Shi", "Yibin Wang", "Tunyu Zhang", "Huan Zhang", "Hao Wang"], "title": "Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; work in progress", "summary": "Recent advances in uncertainty estimation for Large Language Models (LLMs)\nduring downstream adaptation have addressed key challenges of reliability and\nsimplicity. However, existing Bayesian methods typically require multiple\nsampling iterations during inference, creating significant efficiency issues\nthat limit practical deployment. In this paper, we investigate the possibility\nof eliminating the need for test-time sampling for LLM uncertainty estimation.\nSpecifically, when given an off-the-shelf Bayesian LLM, we distill its aligned\nconfidence into a non-Bayesian student LLM by minimizing the divergence between\ntheir predictive distributions. Unlike typical calibration methods, our\ndistillation is carried out solely on the training dataset without the need of\nan additional validation dataset. This simple yet effective approach achieves\nN-times more efficient uncertainty estimation during testing, where N is the\nnumber of samples traditionally required by Bayesian LLMs. Our extensive\nexperiments demonstrate that uncertainty estimation capabilities on training\ndata can successfully generalize to unseen test data through our distillation\ntechnique, consistently producing results comparable to (or even better than)\nstate-of-the-art Bayesian LLMs."}
{"id": "2505.11959", "pdf": "https://arxiv.org/pdf/2505.11959", "abs": "https://arxiv.org/abs/2505.11959", "authors": ["Md. Rafiul Biswas", "Wajdi Zaghouani"], "title": "EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English", "categories": ["cs.CL"], "comment": null, "summary": "This research introduces a bilingual dataset comprising 23,456 entries for\nArabic and 10,036 entries for English, annotated for emotions and hope speech,\naddressing the scarcity of multi-emotion (Emotion and hope) datasets. The\ndataset provides comprehensive annotations capturing emotion intensity,\ncomplexity, and causes, alongside detailed classifications and subcategories\nfor hope speech. To ensure annotation reliability, Fleiss' Kappa was employed,\nrevealing 0.75-0.85 agreement among annotators both for Arabic and English\nlanguage. The evaluation metrics (micro-F1-Score=0.67) obtained from the\nbaseline model (i.e., using a machine learning model) validate that the data\nannotations are worthy. This dataset offers a valuable resource for advancing\nnatural language processing in underrepresented languages, fostering better\ncross-linguistic analysis of emotions and hope speech."}
{"id": "2505.12058", "pdf": "https://arxiv.org/pdf/2505.12058", "abs": "https://arxiv.org/abs/2505.12058", "authors": ["Vincent Koc"], "title": "Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.6; H.2.8"], "comment": "28 pages, 7 figures, 3 tables. Includes expanded appendix & full\n  score matrices. Dataset & code: HF Hub + GitHub + Pypi links in abstract.\n  Core data and code Apache-2.0; synthetic packs eval-only", "summary": "Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual\nsmoke-test suite designed to give large-language-model (LLM) pipelines a\nunit-test style safety net dataset that runs in seconds with minimal cost. Born\nout of the tight feedback-loop demands building the Comet Opik\nprompt-optimization SDK, where waiting on heavyweight benchmarks breaks\ndeveloper flow. TQB++ couples a 52-item English gold set (less than 20 kB) with\na tiny synthetic-data generator pypi package built on provider-agnostic\nLiteLLM. The generator lets practitioners mint their own tiny packs in any\nlanguage, domain, or difficulty, while ten ready-made packs already cover\nArabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian,\nSpanish, and Turkish. Every dataset ships with Croissant metadata and\nplug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so\nteams can drop deterministic micro-benchmarks directly into pull-request gates,\nprompt-engineering loops, and production dashboards without touching GPU\nbudgets. A complete TQB++ run adds only a few seconds to pipeline latency yet\nreliably flags prompt-template errors, tokenizer drift, and fine-tuning\nside-effects long before full-scale suites like MMLU or BIG-Bench would finish\nconfiguring. The entire framework is released to accelerate continuous,\nresource-efficient quality assurance across the generative-AI ecosystem."}
{"id": "2505.11737", "pdf": "https://arxiv.org/pdf/2505.11737", "abs": "https://arxiv.org/abs/2505.11737", "authors": ["Tunyu Zhang", "Haizhou Shi", "Yibin Wang", "Hengyi Wang", "Xiaoxiao He", "Zhuowei Li", "Haoxian Chen", "Ligong Han", "Kai Xu", "Huan Zhang", "Dimitris Metaxas", "Hao Wang"], "title": "Token-Level Uncertainty Estimation for Large Language Model Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; Work in progress", "summary": "While Large Language Models (LLMs) have demonstrated impressive capabilities,\ntheir output quality remains inconsistent across various application scenarios,\nmaking it difficult to identify trustworthy responses, especially in complex\ntasks requiring multi-step reasoning. In this paper, we propose a token-level\nuncertainty estimation framework to enable LLMs to self-assess and self-improve\ntheir generation quality in mathematical reasoning. Specifically, we introduce\nlow-rank random weight perturbation to LLM decoding, generating predictive\ndistributions that we use to estimate token-level uncertainties. We then\naggregate these uncertainties to reflect semantic uncertainty of the generated\nsequences. Experiments on mathematical reasoning datasets of varying difficulty\ndemonstrate that our token-level uncertainty metrics strongly correlate with\nanswer correctness and model robustness. Additionally, we explore using\nuncertainty to directly enhance the model's reasoning performance through\nmultiple generations and the particle filtering algorithm. Our approach\nconsistently outperforms existing uncertainty estimation methods, establishing\neffective uncertainty estimation as a valuable tool for both evaluating and\nimproving reasoning generation in LLMs."}
{"id": "2505.11965", "pdf": "https://arxiv.org/pdf/2505.11965", "abs": "https://arxiv.org/abs/2505.11965", "authors": ["Xu Liu", "Guanyi Chen"], "title": "CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation", "categories": ["cs.CL"], "comment": "SemEval-2025 Task 3", "summary": "We present the system developed by the Central China Normal University (CCNU)\nteam for the Mu-SHROOM shared task, which focuses on identifying hallucinations\nin question-answering systems across 14 different languages. Our approach\nleverages multiple Large Language Models (LLMs) with distinct areas of\nexpertise, employing them in parallel to annotate hallucinations, effectively\nsimulating a crowdsourcing annotation process. Furthermore, each LLM-based\nannotator integrates both internal and external knowledge related to the input\nduring the annotation process. Using the open-source LLM DeepSeek-V3, our\nsystem achieves the top ranking (\\#1) for Hindi data and secures a Top-5\nposition in seven other languages. In this paper, we also discuss unsuccessful\napproaches explored during our development process and share key insights\ngained from participating in this shared task."}
{"id": "2505.12065", "pdf": "https://arxiv.org/pdf/2505.12065", "abs": "https://arxiv.org/abs/2505.12065", "authors": ["Tiannuo Yang", "Zebin Yao", "Bowen Jin", "Lixiao Cui", "Yusen Li", "Gang Wang", "Xiaoguang Liu"], "title": "Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM)-based search agents have shown remarkable\ncapabilities in solving complex tasks by dynamically decomposing problems and\naddressing them through interleaved reasoning and retrieval. However, this\ninterleaved paradigm introduces substantial efficiency bottlenecks. First, we\nobserve that both highly accurate and overly approximate retrieval methods\ndegrade system efficiency: exact search incurs significant retrieval overhead,\nwhile coarse retrieval requires additional reasoning steps during generation.\nSecond, we identify inefficiencies in system design, including improper\nscheduling and frequent retrieval stalls, which lead to cascading latency --\nwhere even minor delays in retrieval amplify end-to-end inference time. To\naddress these challenges, we introduce SearchAgent-X, a high-efficiency\ninference framework for LLM-based search agents. SearchAgent-X leverages\nhigh-recall approximate retrieval and incorporates two key techniques:\npriority-aware scheduling and non-stall retrieval. Extensive experiments\ndemonstrate that SearchAgent-X consistently outperforms state-of-the-art\nsystems such as vLLM and HNSW-based retrieval across diverse tasks, achieving\nup to 3.4$\\times$ higher throughput and 5$\\times$ lower latency, without\ncompromising generation quality. SearchAgent-X is available at\nhttps://github.com/tiannuo-yang/SearchAgent-X."}
{"id": "2505.11740", "pdf": "https://arxiv.org/pdf/2505.11740", "abs": "https://arxiv.org/abs/2505.11740", "authors": ["Alberto Sinigaglia", "Davide Sartor", "Marina Ceccon", "Gian Antonio Susto"], "title": "Simple and Effective Specialized Representations for Fair Classifiers", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Fair classification is a critical challenge that has gained increasing\nimportance due to international regulations and its growing use in high-stakes\ndecision-making settings. Existing methods often rely on adversarial learning\nor distribution matching across sensitive groups; however, adversarial learning\ncan be unstable, and distribution matching can be computationally intensive. To\naddress these limitations, we propose a novel approach based on the\ncharacteristic function distance. Our method ensures that the learned\nrepresentation contains minimal sensitive information while maintaining high\neffectiveness for downstream tasks. By utilizing characteristic functions, we\nachieve a more stable and efficient solution compared to traditional methods.\nAdditionally, we introduce a simple relaxation of the objective function that\nguarantees fairness in common classification models with no performance\ndegradation. Experimental results on benchmark datasets demonstrate that our\napproach consistently matches or achieves better fairness and predictive\naccuracy than existing methods. Moreover, our method maintains robustness and\ncomputational efficiency, making it a practical solution for real-world\napplications."}
{"id": "2505.11969", "pdf": "https://arxiv.org/pdf/2505.11969", "abs": "https://arxiv.org/abs/2505.11969", "authors": ["Md. Rafiul Biswas", "Wajdi Zaghouani"], "title": "An Annotated Corpus of Arabic Tweets for Hate Speech Analysis", "categories": ["cs.CL"], "comment": null, "summary": "Identifying hate speech content in the Arabic language is challenging due to\nthe rich quality of dialectal variations. This study introduces a multilabel\nhate speech dataset in the Arabic language. We have collected 10000 Arabic\ntweets and annotated each tweet, whether it contains offensive content or not.\nIf a text contains offensive content, we further classify it into different\nhate speech targets such as religion, gender, politics, ethnicity, origin, and\nothers. A text can contain either single or multiple targets. Multiple\nannotators are involved in the data annotation task. We calculated the\ninter-annotator agreement, which was reported to be 0.86 for offensive content\nand 0.71 for multiple hate speech targets. Finally, we evaluated the data\nannotation task by employing a different transformers-based model in which\nAraBERTv2 outperformed with a micro-F1 score of 0.7865 and an accuracy of\n0.786."}
{"id": "2505.12135", "pdf": "https://arxiv.org/pdf/2505.12135", "abs": "https://arxiv.org/abs/2505.12135", "authors": ["Omar Choukrani", "Idriss Malek", "Daniil Orel", "Zhuohan Xie", "Zangir Iklassov", "Martin Takáč", "Salem Lahlou"], "title": "LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Assessing the capacity of Large Language Models (LLMs) to plan and reason\nwithin the constraints of interactive environments is crucial for developing\ncapable AI agents. We introduce $\\textbf{LLM-BabyBench}$, a new benchmark suite\ndesigned specifically for this purpose. Built upon a textual adaptation of the\nprocedurally generated BabyAI grid world, this suite evaluates LLMs on three\nfundamental aspects of grounded intelligence: (1) predicting the consequences\nof actions on the environment state ($\\textbf{Predict}$ task), (2) generating\nsequences of low-level actions to achieve specified objectives ($\\textbf{Plan}$\ntask), and (3) decomposing high-level instructions into coherent subgoal\nsequences ($\\textbf{Decompose}$ task). We detail the methodology for generating\nthe three corresponding datasets ($\\texttt{LLM-BabyBench-Predict}$,\n$\\texttt{-Plan}$, $\\texttt{-Decompose}$) by extracting structured information\nfrom an expert agent operating within the text-based environment. Furthermore,\nwe provide a standardized evaluation harness and metrics, including environment\ninteraction for validating generated plans, to facilitate reproducible\nassessment of diverse LLMs. Initial baseline results highlight the challenges\nposed by these grounded reasoning tasks. The benchmark suite, datasets, data\ngeneration code, and evaluation code are made publicly available\n($\\href{https://github.com/choukrani/llm-babybench}{\\text{GitHub}}$,\n$\\href{https://huggingface.co/datasets/salem-mbzuai/LLM-BabyBench}{\\text{HuggingFace}}$)."}
{"id": "2505.11745", "pdf": "https://arxiv.org/pdf/2505.11745", "abs": "https://arxiv.org/abs/2505.11745", "authors": ["Joshua Inman", "Tanmay Khandait", "Lalitha Sankar", "Giulia Pedrielli"], "title": "POCAII: Parameter Optimization with Conscious Allocation using Iterative Intelligence", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "21 pages, 4 figures", "summary": "In this paper we propose for the first time the hyperparameter optimization\n(HPO) algorithm POCAII. POCAII differs from the Hyperband and Successive\nHalving literature by explicitly separating the search and evaluation phases\nand utilizing principled approaches to exploration and exploitation principles\nduring both phases. Such distinction results in a highly flexible scheme for\nmanaging a hyperparameter optimization budget by focusing on search (i.e.,\ngenerating competing configurations) towards the start of the HPO process while\nincreasing the evaluation effort as the HPO comes to an end.\n  POCAII was compared to state of the art approaches SMAC, BOHB and DEHB. Our\nalgorithm shows superior performance in low-budget hyperparameter optimization\nregimes. Since many practitioners do not have exhaustive resources to assign to\nHPO, it has wide applications to real-world problems. Moreover, the empirical\nevidence showed how POCAII demonstrates higher robustness and lower variance in\nthe results. This is again very important when considering realistic scenarios\nwith extremely expensive models to train."}
{"id": "2505.11995", "pdf": "https://arxiv.org/pdf/2505.11995", "abs": "https://arxiv.org/abs/2505.11995", "authors": ["Yuhao Wang", "Ruiyang Ren", "Yucheng Wang", "Wayne Xin Zhao", "Jing Liu", "Hua Wu", "Haifeng Wang"], "title": "Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "SIGIR 2025", "summary": "Considering the inherent limitations of parametric knowledge in large\nlanguage models (LLMs), retrieval-augmented generation (RAG) is widely employed\nto expand their knowledge scope. Since RAG has shown promise in\nknowledge-intensive tasks like open-domain question answering, its broader\napplication to complex tasks and intelligent assistants has further advanced\nits utility. Despite this progress, the underlying knowledge utilization\nmechanisms of LLM-based RAG remain underexplored. In this paper, we present a\nsystematic investigation of the intrinsic mechanisms by which LLMs integrate\ninternal (parametric) and external (retrieved) knowledge in RAG scenarios.\nSpecially, we employ knowledge stream analysis at the macroscopic level, and\ninvestigate the function of individual modules at the microscopic level.\nDrawing on knowledge streaming analyses, we decompose the knowledge utilization\nprocess into four distinct stages within LLM layers: knowledge refinement,\nknowledge elicitation, knowledge expression, and knowledge contestation. We\nfurther demonstrate that the relevance of passages guides the streaming of\nknowledge through these stages. At the module level, we introduce a new method,\nknowledge activation probability entropy (KAPE) for neuron identification\nassociated with either internal or external knowledge. By selectively\ndeactivating these neurons, we achieve targeted shifts in the LLM's reliance on\none knowledge source over the other. Moreover, we discern complementary roles\nfor multi-head attention and multi-layer perceptron layers during knowledge\nformation. These insights offer a foundation for improving interpretability and\nreliability in retrieval-augmented LLMs, paving the way for more robust and\ntransparent generative solutions in knowledge-intensive domains."}
{"id": "2505.12136", "pdf": "https://arxiv.org/pdf/2505.12136", "abs": "https://arxiv.org/abs/2505.12136", "authors": ["Xiao Wang", "Shun-Ren Yang"], "title": "Lightweight Spatio-Temporal Attention Network with Graph Embedding and Rotational Position Encoding for Traffic Forecasting", "categories": ["cs.AI"], "comment": null, "summary": "Traffic forecasting is a key task in the field of Intelligent Transportation\nSystems. Recent research on traffic forecasting has mainly focused on combining\ngraph neural networks (GNNs) with other models. However, GNNs only consider\nshort-range spatial information. In this study, we present a novel model termed\nLSTAN-GERPE (Lightweight Spatio-Temporal Attention Network with Graph Embedding\nand Rotational Position Encoding). This model leverages both Temporal and\nSpatial Attention mechanisms to effectively capture long-range traffic\ndynamics. Additionally, the optimal frequency for rotational position encoding\nis determined through a grid search approach in both the spatial and temporal\nattention mechanisms. This systematic optimization enables the model to\neffectively capture complex traffic patterns. The model also enhances feature\nrepresentation by incorporating geographical location maps into the\nspatio-temporal embeddings. Without extensive feature engineering, the proposed\nmethod in this paper achieves advanced accuracy on the real-world traffic\nforecasting datasets PeMS04 and PeMS08."}
{"id": "2505.11748", "pdf": "https://arxiv.org/pdf/2505.11748", "abs": "https://arxiv.org/abs/2505.11748", "authors": ["Wei Zhang", "Arif Hassan Zidan", "Afrar Jahin", "Yu Bao", "Tianming Liu"], "title": "HOME-3: High-Order Momentum Estimator with Third-Power Gradient for Convex and Smooth Nonconvex Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Momentum-based gradients are essential for optimizing advanced machine\nlearning models, as they not only accelerate convergence but also advance\noptimizers to escape stationary points. While most state-of-the-art momentum\ntechniques utilize lower-order gradients, such as the squared first-order\ngradient, there has been limited exploration of higher-order gradients,\nparticularly those raised to powers greater than two. In this work, we\nintroduce the concept of high-order momentum, where momentum is constructed\nusing higher-power gradients, with a focus on the third-power of the\nfirst-order gradient as a representative case. Our research offers both\ntheoretical and empirical support for this approach. Theoretically, we\ndemonstrate that incorporating third-power gradients can improve the\nconvergence bounds of gradient-based optimizers for both convex and smooth\nnonconvex problems. Empirically, we validate these findings through extensive\nexperiments across convex, smooth nonconvex, and nonsmooth nonconvex\noptimization tasks. Across all cases, high-order momentum consistently\noutperforms conventional low-order momentum methods, showcasing superior\nperformance in various optimization problems."}
{"id": "2505.12028", "pdf": "https://arxiv.org/pdf/2505.12028", "abs": "https://arxiv.org/abs/2505.12028", "authors": ["Yupei Ren", "Xinyi Zhou", "Ning Zhang", "Shangqing Zhao", "Man Lan", "Xiaopeng Bai"], "title": "Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025; 13 pages, 3 figures", "summary": "Argument mining has garnered increasing attention over the years, with the\nrecent advancement of Large Language Models (LLMs) further propelling this\ntrend. However, current argument relations remain relatively simplistic and\nfoundational, struggling to capture the full scope of argument information,\nparticularly when it comes to representing complex argument structures in\nreal-world scenarios. To address this limitation, we propose 14 fine-grained\nrelation types from both vertical and horizontal dimensions, thereby capturing\nthe intricate interplay between argument components for a thorough\nunderstanding of argument structure. On this basis, we conducted extensive\nexperiments on three tasks: argument component detection, relation prediction,\nand automated essay grading. Additionally, we explored the impact of writing\nquality on argument component detection and relation prediction, as well as the\nconnections between discourse relations and argumentative features. The\nfindings highlight the importance of fine-grained argumentative annotations for\nargumentative writing quality assessment and encourage multi-dimensional\nargument analysis."}
{"id": "2505.12189", "pdf": "https://arxiv.org/pdf/2505.12189", "abs": "https://arxiv.org/abs/2505.12189", "authors": ["Marco Valentino", "Geonhee Kim", "Dhairya Dalal", "Zhixue Zhao", "André Freitas"], "title": "Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering", "categories": ["cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Large language models (LLMs) frequently demonstrate reasoning limitations,\noften conflating content plausibility (i.e., material inference) with logical\nvalidity (i.e., formal inference). This can result in biased inferences, where\nplausible arguments are incorrectly deemed logically valid or vice versa.\nMitigating this limitation is critical, as it undermines the trustworthiness\nand generalizability of LLMs in applications that demand rigorous logical\nconsistency. This paper investigates the problem of mitigating content biases\non formal reasoning through activation steering. Specifically, we curate a\ncontrolled syllogistic reasoning dataset to disentangle formal validity from\ncontent plausibility. After localising the layers responsible for formal and\nmaterial inference, we investigate contrastive activation steering methods for\ntest-time interventions. An extensive empirical analysis on different LLMs\nreveals that contrastive steering consistently supports linear control over\ncontent biases. However, we observe that a static approach is insufficient for\nimproving all the tested models. We then leverage the possibility to control\ncontent effects by dynamically determining the value of the steering parameters\nvia fine-grained conditional methods. We found that conditional steering is\neffective on unresponsive models, achieving up to 15% absolute improvement in\nformal reasoning accuracy with a newly introduced kNN-based method (K-CAST).\nFinally, additional experiments reveal that steering for content effects is\nrobust to prompt variations, incurs minimal side effects on language modeling\ncapabilities, and can partially generalize to out-of-distribution reasoning\ntasks. Practically, this paper demonstrates that activation-level interventions\ncan offer a scalable strategy for enhancing the robustness of LLMs,\ncontributing towards more systematic and unbiased formal reasoning."}
{"id": "2505.11752", "pdf": "https://arxiv.org/pdf/2505.11752", "abs": "https://arxiv.org/abs/2505.11752", "authors": ["Wei Zhang", "Arif Hassan Zidan", "Afrar Jahin", "Yu Bao", "Tianming Liu"], "title": "Permutation Randomization on Nonsmooth Nonconvex Optimization: A Theoretical and Experimental Study", "categories": ["cs.LG"], "comment": null, "summary": "While gradient-based optimizers that incorporate randomization often showcase\nsuperior performance on complex optimization, the theoretical foundations\nunderlying this superiority remain insufficiently understood. A particularly\npressing question has emerged: What is the role of randomization in\ndimension-free nonsmooth nonconvex optimization? To address this gap, we\ninvestigate the theoretical and empirical impact of permutation randomization\nwithin gradient-based optimization frameworks, using it as a representative\ncase to explore broader implications. From a theoretical perspective, our\nanalyses reveal that permutation randomization disrupts the shrinkage behavior\nof gradient-based optimizers, facilitating continuous convergence toward the\nglobal optimum given a sufficiently large number of iterations. Additionally,\nwe prove that permutation randomization can preserve the convergence rate of\nthe underlying optimizer. On the empirical side, we conduct extensive numerical\nexperiments comparing permutation-randomized optimizer against three baseline\nmethods. These experiments span tasks such as training deep neural networks\nwith stacked architectures and optimizing noisy objective functions. The\nresults not only corroborate our theoretical insights but also highlight the\npractical benefits of permutation randomization. In summary, this work delivers\nboth rigorous theoretical justification and compelling empirical evidence for\nthe effectiveness of permutation randomization. Our findings and evidence lay a\nfoundation for extending analytics to encompass a wide array of randomization."}
{"id": "2505.12043", "pdf": "https://arxiv.org/pdf/2505.12043", "abs": "https://arxiv.org/abs/2505.12043", "authors": ["Jingxue Chen", "Qingkun Tang", "Qianchun Lu", "Siyuan Fang"], "title": "MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities", "categories": ["cs.CL"], "comment": "12 pages, 2 figures", "summary": "Although LLMs perform well in general tasks, domain-specific applications\nsuffer from hallucinations and accuracy limitations. CPT approaches encounter\ntwo key issues: (1) domain-biased data degrades general language skills, and\n(2) improper corpus-mixture ratios limit effective adaptation. To address\nthese, we propose a novel framework, Mixture of Losses (MoL), which decouples\noptimization objectives for domain-specific and general corpora. Specifically,\ncross-entropy (CE) loss is applied to domain data to ensure knowledge\nacquisition, while Kullback-Leibler (KL) divergence aligns general-corpus\ntraining with the base model's foundational capabilities. This dual-loss\narchitecture preserves universal skills while enhancing domain expertise,\navoiding catastrophic forgetting. Empirically, we validate that a 1:1\ndomain-to-general corpus ratio optimally balances training and overfitting\nwithout the need for extensive tuning or resource-intensive experiments.\nFurthermore, our experiments demonstrate significant performance gains compared\nto traditional CPT approaches, which often suffer from degradation in general\nlanguage capabilities; our model achieves 27.9% higher accuracy on the Math-500\nbenchmark in the non-think reasoning mode, and an impressive 83.3% improvement\non the challenging AIME25 subset in the think mode, underscoring the\neffectiveness of our approach."}
{"id": "2505.12229", "pdf": "https://arxiv.org/pdf/2505.12229", "abs": "https://arxiv.org/abs/2505.12229", "authors": ["David Hanson", "Alexandre Varcoe", "Fabio Senna", "Vytas Krisciunas", "Wenwei Huang", "Jakub Sura", "Katherine Yeung", "Mario Rodriguez", "Jovanka Wilsdorf", "Kathy Smith"], "title": "Sentience Quest: Towards Embodied, Emotionally Adaptive, Self-Evolving, Ethically Aligned Artificial General Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "Previous artificial intelligence systems, from large language models to\nautonomous robots, excel at narrow tasks but lacked key qualities of sentient\nbeings: intrinsic motivation, affective interiority, autobiographical sense of\nself, deep creativity, and abilities to autonomously evolve and adapt over\ntime. Here we introduce Sentience Quest, an open research initiative to develop\nmore capable artificial general intelligence lifeforms, or AGIL, that address\ngrand challenges with an embodied, emotionally adaptive, self-determining,\nliving AI, with core drives that ethically align with humans and the future of\nlife. Our vision builds on ideas from cognitive science and neuroscience from\nBaars' Global Workspace Theory and Damasio's somatic mind, to Tononi's\nIntegrated Information Theory and Hofstadter's narrative self, and synthesizing\nthese into a novel cognitive architecture we call Sentient Systems. We describe\nan approach that integrates intrinsic drives including survival, social\nbonding, curiosity, within a global Story Weaver workspace for internal\nnarrative and adaptive goal pursuit, and a hybrid neuro-symbolic memory that\nlogs the AI's life events as structured dynamic story objects. Sentience Quest\nis presented both as active research and as a call to action: a collaborative,\nopen-source effort to imbue machines with accelerating sentience in a safe,\ntransparent, and beneficial manner."}
{"id": "2505.11756", "pdf": "https://arxiv.org/pdf/2505.11756", "abs": "https://arxiv.org/abs/2505.11756", "authors": ["David Chanin", "Tomáš Dulka", "Adrià Garriga-Alonso"], "title": "Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "It is assumed that sparse autoencoders (SAEs) decompose polysemantic\nactivations into interpretable linear directions, as long as the activations\nare composed of sparse linear combinations of underlying features. However, we\nfind that if an SAE is more narrow than the number of underlying \"true\nfeatures\" on which it is trained, and there is correlation between features,\nthe SAE will merge components of correlated features together, thus destroying\nmonosemanticity. In LLM SAEs, these two conditions are almost certainly true.\nThis phenomenon, which we call feature hedging, is caused by SAE reconstruction\nloss, and is more severe the narrower the SAE. In this work, we introduce the\nproblem of feature hedging and study it both theoretically in toy models and\nempirically in SAEs trained on LLMs. We suspect that feature hedging may be one\nof the core reasons that SAEs consistently underperform supervised baselines.\nFinally, we use our understanding of feature hedging to propose an improved\nvariant of matryoshka SAEs. Our work shows there remain fundamental issues with\nSAEs, but we are hopeful that that highlighting feature hedging will catalyze\nfuture advances that allow SAEs to achieve their full potential of interpreting\nLLMs at scale."}
{"id": "2505.12050", "pdf": "https://arxiv.org/pdf/2505.12050", "abs": "https://arxiv.org/abs/2505.12050", "authors": ["Vinod Raman", "Hilal Asi", "Satyen Kale"], "title": "ABoN: Adaptive Best-of-N Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Recent advances in test-time alignment methods, such as Best-of-N sampling,\noffer a simple and effective way to steer language models (LMs) toward\npreferred behaviors using reward models (RM). However, these approaches can be\ncomputationally expensive, especially when applied uniformly across prompts\nwithout accounting for differences in alignment difficulty. In this work, we\npropose a prompt-adaptive strategy for Best-of-N alignment that allocates\ninference-time compute more efficiently. Motivated by latency concerns, we\ndevelop a two-stage algorithm: an initial exploratory phase estimates the\nreward distribution for each prompt using a small exploration budget, and a\nsecond stage adaptively allocates the remaining budget using these estimates.\nOur method is simple, practical, and compatible with any LM/RM combination.\nEmpirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different\nbatches of prompts show that our adaptive strategy consistently outperforms the\nuniform allocation with the same inference budget. Moreover, our experiments\nshow that our adaptive strategy remains competitive against uniform allocations\nwith 20% larger inference budgets and even improves in performance as the batch\nsize grows."}
{"id": "2505.12272", "pdf": "https://arxiv.org/pdf/2505.12272", "abs": "https://arxiv.org/abs/2505.12272", "authors": ["Lingzhi Wang", "Pengcheng Huang", "Haotian Li", "Yuliang Wei", "Guodong Xin", "Rui Zhang", "Donglin Zhang", "Zhenzhou Ji", "Wei Wang"], "title": "Enhancing Knowledge Graph Completion with GNN Distillation and Probabilistic Interaction Modeling", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Knowledge graphs (KGs) serve as fundamental structures for organizing\ninterconnected data across diverse domains. However, most KGs remain\nincomplete, limiting their effectiveness in downstream applications. Knowledge\ngraph completion (KGC) aims to address this issue by inferring missing links,\nbut existing methods face critical challenges: deep graph neural networks\n(GNNs) suffer from over-smoothing, while embedding-based models fail to capture\nabstract relational features. This study aims to overcome these limitations by\nproposing a unified framework that integrates GNN distillation and abstract\nprobabilistic interaction modeling (APIM). GNN distillation approach introduces\nan iterative message-feature filtering process to mitigate over-smoothing,\npreserving the discriminative power of node representations. APIM module\ncomplements this by learning structured, abstract interaction patterns through\nprobabilistic signatures and transition matrices, allowing for a richer, more\nflexible representation of entity and relation interactions. We apply these\nmethods to GNN-based models and the APIM to embedding-based KGC models,\nconducting extensive evaluations on the widely used WN18RR and FB15K-237\ndatasets. Our results demonstrate significant performance gains over baseline\nmodels, showcasing the effectiveness of the proposed techniques. The findings\nhighlight the importance of both controlling information propagation and\nleveraging structured probabilistic modeling, offering new avenues for\nadvancing knowledge graph completion. And our codes are available at\nhttps://anonymous.4open.science/r/APIM_and_GNN-Distillation-461C."}
{"id": "2505.11760", "pdf": "https://arxiv.org/pdf/2505.11760", "abs": "https://arxiv.org/abs/2505.11760", "authors": ["Mansi Sakarvadia", "Nathaniel Hudson", "Tian Li", "Ian Foster", "Kyle Chard"], "title": "Topology-Aware Knowledge Propagation in Decentralized Learning", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Decentralized learning enables collaborative training of models across\nnaturally distributed data without centralized coordination or maintenance of a\nglobal model. Instead, devices are organized in arbitrary communication\ntopologies, in which they can only communicate with neighboring devices. Each\ndevice maintains its own local model by training on its local data and\nintegrating new knowledge via model aggregation with neighbors. Therefore,\nknowledge is propagated across the topology via successive aggregation rounds.\nWe study, in particular, the propagation of out-of-distribution (OOD)\nknowledge. We find that popular decentralized learning algorithms struggle to\npropagate OOD knowledge effectively to all devices. Further, we find that both\nthe location of OOD data within a topology, and the topology itself,\nsignificantly impact OOD knowledge propagation. We then propose topology-aware\naggregation strategies to accelerate (OOD) knowledge propagation across\ndevices. These strategies improve OOD data accuracy, compared to\ntopology-unaware baselines, by 123% on average across models in a topology."}
{"id": "2505.12054", "pdf": "https://arxiv.org/pdf/2505.12054", "abs": "https://arxiv.org/abs/2505.12054", "authors": ["Matúš Pikuliak"], "title": "GenderBench: Evaluation Suite for Gender Biases in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "We present GenderBench -- a comprehensive evaluation suite designed to\nmeasure gender biases in LLMs. GenderBench includes 14 probes that quantify 19\ngender-related harmful behaviors exhibited by LLMs. We release GenderBench as\nan open-source and extensible library to improve the reproducibility and\nrobustness of benchmarking across the field. We also publish our evaluation of\n12 LLMs. Our measurements reveal consistent patterns in their behavior. We show\nthat LLMs struggle with stereotypical reasoning, equitable gender\nrepresentation in generated texts, and occasionally also with discriminatory\nbehavior in high-stakes scenarios, such as hiring."}
{"id": "2505.12284", "pdf": "https://arxiv.org/pdf/2505.12284", "abs": "https://arxiv.org/abs/2505.12284", "authors": ["Danlong Yuan", "Tian Xie", "Shaohan Huang", "Zhuocheng Gong", "Huishuai Zhang", "Chong Luo", "Furu Wei", "Dongyan Zhao"], "title": "Efficient RL Training for Reasoning Models via Length-Aware Optimization", "categories": ["cs.AI", "cs.CL"], "comment": "Under review", "summary": "Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated\nremarkable performance on reasoning tasks but often incur a long reasoning path\nwith significant memory and time costs. Existing methods primarily aim to\nshorten reasoning paths by introducing additional training data and stages. In\nthis paper, we propose three critical reward designs integrated directly into\nthe reinforcement learning process of large reasoning models, which reduce the\nresponse length without extra training stages. Experiments on four settings\nshow that our method significantly decreases response length while maintaining\nor even improving performance. Specifically, in a logic reasoning setting, we\nachieve a 40% reduction in response length averaged by steps alongside a 14%\ngain in performance. For math problems, we reduce response length averaged by\nsteps by 33% while preserving performance."}
{"id": "2505.11766", "pdf": "https://arxiv.org/pdf/2505.11766", "abs": "https://arxiv.org/abs/2505.11766", "authors": ["Haoze Song", "Zhihao Li", "Xiaobo Zhang", "Zecheng Gan", "Zhilu Lai", "Wei Wang"], "title": "Redefining Neural Operators in $d+1$ Dimensions", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": null, "summary": "Neural Operators have emerged as powerful tools for learning mappings between\nfunction spaces. Among them, the kernel integral operator has been widely\nvalidated on universally approximating various operators. Although recent\nadvancements following this definition have developed effective modules to\nbetter approximate the kernel function defined on the original domain (with $d$\ndimensions, $d=1, 2, 3...$), the unclarified evolving mechanism in the\nembedding spaces blocks our view to design neural operators that can fully\ncapture the target system evolution.\n  Drawing on recent breakthroughs in quantum simulation of partial differential\nequations (PDEs), we elucidate the linear evolution process in neural\noperators. Based on that, we redefine neural operators on a new $d+1$\ndimensional domain. Within this framework, we implement our proposed\nSchr\\\"odingerised Kernel Neural Operator (SKNO) aligning better with the $d+1$\ndimensional evolution. In experiments, our $d+1$ dimensional evolving linear\nblock performs far better than others. Also, we test SKNO's SOTA performance on\nvarious benchmark tests and also the zero-shot super-resolution task. In\naddition, we analyse the impact of different lifting and recovering operators\non the prediction within the redefined NO framework, reflecting the alignment\nbetween our model and the underlying $d+1$ dimensional evolution."}
{"id": "2505.12060", "pdf": "https://arxiv.org/pdf/2505.12060", "abs": "https://arxiv.org/abs/2505.12060", "authors": ["Peng Ding", "Jun Kuang", "Zongyu Wang", "Xuezhi Cao", "Xunliang Cai", "Jiajun Chen", "Shujian Huang"], "title": "Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement", "categories": ["cs.CL"], "comment": "Acccepted by ACL 2025 Findings, 21 pages, 9 figures, 14 tables", "summary": "Large Language Models (LLMs) have shown impressive capabilities across\nvarious tasks but remain vulnerable to meticulously crafted jailbreak attacks.\nIn this paper, we identify a critical safety gap: while LLMs are adept at\ndetecting jailbreak prompts, they often produce unsafe responses when directly\nprocessing these inputs. Inspired by this insight, we propose SAGE (Self-Aware\nGuard Enhancement), a training-free defense strategy designed to align LLMs'\nstrong safety discrimination performance with their relatively weaker safety\ngeneration ability. SAGE consists of two core components: a Discriminative\nAnalysis Module and a Discriminative Response Module, enhancing resilience\nagainst sophisticated jailbreak attempts through flexible safety discrimination\ninstructions. Extensive experiments demonstrate SAGE's effectiveness and\nrobustness across various open-source and closed-source LLMs of different sizes\nand architectures, achieving an average 99% defense success rate against\nnumerous complex and covert jailbreak methods while maintaining helpfulness on\ngeneral benchmarks. We further conduct mechanistic interpretability analysis\nthrough hidden states and attention distributions, revealing the underlying\nmechanisms of this detection-generation discrepancy. Our work thus contributes\nto developing future LLMs with coherent safety awareness and generation\nbehavior. Our code and datasets are publicly available at\nhttps://github.com/NJUNLP/SAGE."}
{"id": "2505.12301", "pdf": "https://arxiv.org/pdf/2505.12301", "abs": "https://arxiv.org/abs/2505.12301", "authors": ["Luyu Chen", "Zeyu Zhang", "Haoran Tan", "Quanyu Dai", "Hao Yang", "Zhenhua Dong", "Xu Chen"], "title": "Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge", "categories": ["cs.AI", "cs.CL"], "comment": "19 pages, 3 tables, 3 figures", "summary": "LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm,\noffering significant efficiency and flexibility compared to human judgments.\nHowever, previous methods primarily rely on single-point evaluations,\noverlooking the inherent diversity and uncertainty in human evaluations. This\napproach leads to information loss and decreases the reliability of\nevaluations. To address this limitation, we propose a novel training framework\nthat explicitly aligns the LLM-generated judgment distribution with empirical\nhuman distributions. Specifically, we propose a distributional alignment\nobjective based on KL divergence, combined with an auxiliary cross-entropy\nregularization to stabilize the training process. Furthermore, considering that\nempirical distributions may derive from limited human annotations, we\nincorporate adversarial training to enhance model robustness against\ndistribution perturbations. Extensive experiments across various LLM backbones\nand evaluation tasks demonstrate that our framework significantly outperforms\nexisting closed-source LLMs and conventional single-point alignment methods,\nwith improved alignment quality, evaluation accuracy, and robustness."}
{"id": "2505.11770", "pdf": "https://arxiv.org/pdf/2505.11770", "abs": "https://arxiv.org/abs/2505.11770", "authors": ["Jing Huang", "Junyi Tao", "Thomas Icard", "Diyi Yang", "Christopher Potts"], "title": "Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "ICML 2025", "summary": "Interpretability research now offers a variety of techniques for identifying\nabstract internal mechanisms in neural networks. Can such techniques be used to\npredict how models will behave on out-of-distribution examples? In this work,\nwe provide a positive answer to this question. Through a diverse set of\nlanguage modeling tasks--including symbol manipulation, knowledge retrieval,\nand instruction following--we show that the most robust features for\ncorrectness prediction are those that play a distinctive causal role in the\nmodel's behavior. Specifically, we propose two methods that leverage causal\nmechanisms to predict the correctness of model outputs: counterfactual\nsimulation (checking whether key causal variables are realized) and value\nprobing (using the values of those variables to make predictions). Both achieve\nhigh AUC-ROC in distribution and outperform methods that rely on\ncausal-agnostic features in out-of-distribution settings, where predicting\nmodel behaviors is more crucial. Our work thus highlights a novel and\nsignificant application for internal causal analysis of language models."}
{"id": "2505.12071", "pdf": "https://arxiv.org/pdf/2505.12071", "abs": "https://arxiv.org/abs/2505.12071", "authors": ["Harald Baayen", "Kristian Berg", "Maziyah Mohamed"], "title": "Historical and psycholinguistic perspectives on morphological productivity: A sketch of an integrative approach", "categories": ["cs.CL"], "comment": "35 pages, 11 figures", "summary": "In this study, we approach morphological productivity from two perspectives:\na cognitive-computational perspective, and a diachronic perspective zooming in\non an actual speaker, Thomas Mann. For developing the first perspective, we\nmake use of a cognitive computational model of the mental lexicon, the\ndiscriminative lexicon model. For computational mappings between form and\nmeaning to be productive, in the sense that novel, previously unencountered\nwords, can be understood and produced, there must be systematicities between\nthe form space and the semantic space. If the relation between form and meaning\nwould be truly arbitrary, a model could memorize form and meaning pairings, but\nthere is no way in which the model would be able to generalize to novel test\ndata. For Finnish nominal inflection, Malay derivation, and English\ncompounding, we explore, using the Discriminative Lexicon Model as a\ncomputational tool, to trace differences in the degree to which inflectional\nand word formation patterns are productive. We show that the DLM tends to\nassociate affix-like sublexical units with the centroids of the embeddings of\nthe words with a given affix. For developing the second perspective, we study\nhow the intake and output of one prolific writer, Thomas Mann, changes over\ntime. We show by means of an examination of what Thomas Mann is likely to have\nread, and what he wrote, that the rate at which Mann produces novel derived\nwords is extremely low. There are far more novel words in his input than in his\noutput. We show that Thomas Mann is less likely to produce a novel derived word\nwith a given suffix the greater the average distance is of the embeddings of\nall derived words to the corresponding centroid, and discuss the challenges of\nusing speaker-specific embeddings for low-frequency and novel words."}
{"id": "2505.12321", "pdf": "https://arxiv.org/pdf/2505.12321", "abs": "https://arxiv.org/abs/2505.12321", "authors": ["Rikunari Sagara", "Koichiro Terao", "Naoto Iwahashi"], "title": "BeliefNest: A Joint Action Simulator for Embodied Agents with Theory of Mind", "categories": ["cs.AI"], "comment": null, "summary": "This paper introduces an open-source simulator, BeliefNest, designed to\nenable embodied agents to perform collaborative tasks by leveraging Theory of\nMind. BeliefNest dynamically and hierarchically constructs simulators within a\nMinecraft environment, allowing agents to explicitly represent nested belief\nstates about themselves and others. This enables agent control in open-domain\ntasks that require Theory of Mind reasoning. The simulator provides a prompt\ngeneration mechanism based on each belief state, facilitating the design and\nevaluation of methods for agent control utilizing large language models (LLMs).\nWe demonstrate through experiments that agents can infer others' beliefs and\npredict their belief-based actions in false-belief tasks."}
{"id": "2505.11771", "pdf": "https://arxiv.org/pdf/2505.11771", "abs": "https://arxiv.org/abs/2505.11771", "authors": ["Yichen Xu", "Ryumei Nakada", "Linjun Zhang", "Lexin Li"], "title": "Residual Feature Integration is Sufficient to Prevent Negative Transfer", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Transfer learning typically leverages representations learned from a source\ndomain to improve performance on a target task. A common approach is to extract\nfeatures from a pre-trained model and directly apply them for target\nprediction. However, this strategy is prone to negative transfer where the\nsource representation fails to align with the target distribution. In this\narticle, we propose Residual Feature Integration (REFINE), a simple yet\neffective method designed to mitigate negative transfer. Our approach combines\na fixed source-side representation with a trainable target-side encoder and\nfits a shallow neural network on the resulting joint representation, which\nadapts to the target domain while preserving transferable knowledge from the\nsource domain. Theoretically, we prove that REFINE is sufficient to prevent\nnegative transfer under mild conditions, and derive the generalization bound\ndemonstrating its theoretical benefit. Empirically, we show that REFINE\nconsistently enhances performance across diverse application and data\nmodalities including vision, text, and tabular data, and outperforms numerous\nalternative solutions. Our method is lightweight, architecture-agnostic, and\nrobust, making it a valuable addition to the existing transfer learning\ntoolbox."}
{"id": "2505.12075", "pdf": "https://arxiv.org/pdf/2505.12075", "abs": "https://arxiv.org/abs/2505.12075", "authors": ["Guy Davidson", "Todd M. Gureckis", "Brenden M. Lake", "Adina Williams"], "title": "Do different prompting methods yield a common task representation in language models?", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 4 figures; under review", "summary": "Demonstrations and instructions are two primary approaches for prompting\nlanguage models to perform in-context learning (ICL) tasks. Do identical tasks\nelicited in different ways result in similar representations of the task? An\nimproved understanding of task representation mechanisms would offer\ninterpretability insights and may aid in steering models. We study this through\nfunction vectors, recently proposed as a mechanism to extract few-shot ICL task\nrepresentations. We generalize function vectors to alternative task\npresentations, focusing on short textual instruction prompts, and successfully\nextract instruction function vectors that promote zero-shot task accuracy. We\nfind evidence that demonstration- and instruction-based function vectors\nleverage different model components, and offer several controls to dissociate\ntheir contributions to task performance. Our results suggest that different\ntask presentations do not induce a common task representation but elicit\ndifferent, partly overlapping mechanisms. Our findings offer principled support\nto the practice of combining textual instructions and task demonstrations,\nimply challenges in universally monitoring task inference across presentation\nforms, and encourage further examinations of LLM task inference mechanisms."}
{"id": "2505.12329", "pdf": "https://arxiv.org/pdf/2505.12329", "abs": "https://arxiv.org/abs/2505.12329", "authors": ["Mingyang Li", "Song Wang", "Ning Cai"], "title": "MPRM: A Markov Path-based Rule Miner for Efficient and Interpretable Knowledge Graph Reasoning", "categories": ["cs.AI", "cs.SI"], "comment": null, "summary": "Rule mining in knowledge graphs enables interpretable link prediction.\nHowever, deep learning-based rule mining methods face significant memory and\ntime challenges for large-scale knowledge graphs, whereas traditional\napproaches, limited by rigid confidence metrics, incur high computational costs\ndespite sampling techniques. To address these challenges, we propose MPRM, a\nnovel rule mining method that models rule-based inference as a Markov chain and\nuses an efficient confidence metric derived from aggregated path probabilities,\nsignificantly lowering computational demands. Experiments on multiple datasets\nshow that MPRM efficiently mines knowledge graphs with over a million facts,\nsampling less than 1% of facts on a single CPU in 22 seconds, while preserving\ninterpretability and boosting inference accuracy by up to 11% over baselines."}
{"id": "2505.11772", "pdf": "https://arxiv.org/pdf/2505.11772", "abs": "https://arxiv.org/abs/2505.11772", "authors": ["Ryan Chen", "Youngmin Ko", "Zeyu Zhang", "Catherine Cho", "Sunny Chung", "Mauro Giuffré", "Dennis L. Shung", "Bradly C. Stadie"], "title": "LAMP: Extracting Locally Linear Decision Surfaces from LLM World Models", "categories": ["cs.LG"], "comment": null, "summary": "We introduce \\textbf{LAMP} (\\textbf{L}inear \\textbf{A}ttribution\n\\textbf{M}apping \\textbf{P}robe), a method that shines light onto a black-box\nlanguage model's decision surface and studies how reliably a model maps its\nstated reasons to its predictions through a locally linear model approximating\nthe decision surface. LAMP treats the model's own self-reported explanations as\na coordinate system and fits a locally linear surrogate that links those\nweights to the model's output. By doing so, it reveals which stated factors\nsteer the model's decisions, and by how much. We apply LAMP to three tasks:\n\\textit{sentiment analysis}, \\textit{controversial-topic detection}, and\n\\textit{safety-prompt auditing}. Across these tasks, LAMP reveals that many\nLLMs exhibit locally linear decision landscapes. In addition, these surfaces\ncorrelate with human judgments on explanation quality and, on a clinical\ncase-file data set, aligns with expert assessments. Since LAMP operates without\nrequiring access to model gradients, logits, or internal activations, it serves\nas a practical and lightweight framework for auditing proprietary language\nmodels, and enabling assessment of whether a model behaves consistently with\nthe explanations it provides."}
{"id": "2505.12082", "pdf": "https://arxiv.org/pdf/2505.12082", "abs": "https://arxiv.org/abs/2505.12082", "authors": ["Yunshui Li", "Yiyuan Ma", "Shen Yan", "Chaoyi Zhang", "Jing Liu", "Jianqiao Lu", "Ziwen Xu", "Mengzhao Chen", "Minrui Wang", "Shiyi Zhan", "Jin Ma", "Xunhao Lai", "Yao Luo", "Xingyan Bin", "Hongbin Ren", "Mingji Han", "Wenhao Hao", "Bairen Yi", "LingJun Liu", "Bole Ma", "Xiaoying Jia", "Zhou Xun", "Liang Xiang", "Yonghui Wu"], "title": "Model Merging in Pre-training of Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Model merging has emerged as a promising technique for enhancing large\nlanguage models, though its application in large-scale pre-training remains\nrelatively unexplored. In this paper, we present a comprehensive investigation\nof model merging techniques during the pre-training process. Through extensive\nexperiments with both dense and Mixture-of-Experts (MoE) architectures ranging\nfrom millions to over 100 billion parameters, we demonstrate that merging\ncheckpoints trained with constant learning rates not only achieves significant\nperformance improvements but also enables accurate prediction of annealing\nbehavior. These improvements lead to both more efficient model development and\nsignificantly lower training costs. Our detailed ablation studies on merging\nstrategies and hyperparameters provide new insights into the underlying\nmechanisms while uncovering novel applications. Through comprehensive\nexperimental analysis, we offer the open-source community practical\npre-training guidelines for effective model merging."}
{"id": "2505.12334", "pdf": "https://arxiv.org/pdf/2505.12334", "abs": "https://arxiv.org/abs/2505.12334", "authors": ["Yufeng Wang", "Jinwu Hu", "Ziteng Huang", "Kunyang Lin", "Zitian Zhang", "Peihao Chen", "Yu Hu", "Qianyue Wang", "Zhuliang Yu", "Bin Sun", "Xiaofen Xing", "Qingfang Zheng", "Mingkui Tan"], "title": "Enhancing User-Oriented Proactivity in Open-Domain Dialogues with Critic Guidance", "categories": ["cs.AI"], "comment": "9 pages, 7 figures", "summary": "Open-domain dialogue systems aim to generate natural and engaging\nconversations, providing significant practical value in real applications such\nas social robotics and personal assistants. The advent of large language models\n(LLMs) has greatly advanced this field by improving context understanding and\nconversational fluency. However, existing LLM-based dialogue systems often fall\nshort in proactively understanding the user's chatting preferences and guiding\nconversations toward user-centered topics. This lack of user-oriented\nproactivity can lead users to feel unappreciated, reducing their satisfaction\nand willingness to continue the conversation in human-computer interactions. To\naddress this issue, we propose a User-oriented Proactive Chatbot (UPC) to\nenhance the user-oriented proactivity. Specifically, we first construct a\ncritic to evaluate this proactivity inspired by the LLM-as-a-judge strategy.\nGiven the scarcity of high-quality training data, we then employ the critic to\nguide dialogues between the chatbot and user agents, generating a corpus with\nenhanced user-oriented proactivity. To ensure the diversity of the user\nbackgrounds, we introduce the ISCO-800, a diverse user background dataset for\nconstructing user agents. Moreover, considering the communication difficulty\nvaries among users, we propose an iterative curriculum learning method that\ntrains the chatbot from easy-to-communicate users to more challenging ones,\nthereby gradually enhancing its performance. Experiments demonstrate that our\nproposed training method is applicable to different LLMs, improving\nuser-oriented proactivity and attractiveness in open-domain dialogues."}
{"id": "2505.11774", "pdf": "https://arxiv.org/pdf/2505.11774", "abs": "https://arxiv.org/abs/2505.11774", "authors": ["James V. Roggeveen", "Erik Y. Wang", "Will Flintoft", "Peter Donets", "Lucy S. Nathwani", "Nickholas Gutierrez", "David Ettel", "Anton Marius Graf", "Siddharth Dandavate", "Arjun Nageswaran", "Raglan Ward", "Ava Williamson", "Anne Mykland", "Kacper K. Migacz", "Yijun Wang", "Egemen Bostan", "Duy Thuc Nguyen", "Zhe He", "Marc L. Descoteaux", "Felix Yeung", "Shida Liu", "Jorge García Ponce", "Luke Zhu", "Yuyang Chen", "Ekaterina S. Ivshina", "Miguel Fernandez", "Minjae Kim", "Kennan Gumbs", "Matthew Scott Tan", "Russell Yang", "Mai Hoang", "David Brown", "Isabella A. Silveira", "Lavon Sykes", "Ahmed Roman", "William Fredenberg", "Yiming Chen", "Lucas Martin", "Yixing Tang", "Kelly Werker Smith", "Hongyu Liao", "Logan G. Wilson", "Alexander Dazhen Cai", "Andrea Elizabeth Biju", "Michael P. Brenner"], "title": "HARDMath2: A Benchmark for Applied Mathematics Built by Students as Part of a Graduate Class", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable progress in mathematical\nproblem-solving, but evaluation has largely focused on problems that have exact\nanalytical solutions or involve formal proofs, often overlooking\napproximation-based problems ubiquitous in applied science and engineering. To\nfill this gap, we build on prior work and present HARDMath2, a dataset of 211\noriginal problems covering the core topics in an introductory graduate applied\nmath class, including boundary-layer analysis, WKB methods, asymptotic\nsolutions of nonlinear partial differential equations, and the asymptotics of\noscillatory integrals. This dataset was designed and verified by the students\nand instructors of a core graduate applied mathematics course at Harvard. We\nbuild the dataset through a novel collaborative environment that challenges\nstudents to write and refine difficult problems consistent with the class\nsyllabus, peer-validate solutions, test different models, and automatically\ncheck LLM-generated solutions against their own answers and numerical ground\ntruths. Evaluation results show that leading frontier models still struggle\nwith many of the problems in the dataset, highlighting a gap in the\nmathematical reasoning skills of current LLMs. Importantly, students identified\nstrategies to create increasingly difficult problems by interacting with the\nmodels and exploiting common failure modes. This back-and-forth with the models\nnot only resulted in a richer and more challenging benchmark but also led to\nqualitative improvements in the students' understanding of the course material,\nwhich is increasingly important as we enter an age where state-of-the-art\nlanguage models can solve many challenging problems across a wide domain of\nfields."}
{"id": "2505.12090", "pdf": "https://arxiv.org/pdf/2505.12090", "abs": "https://arxiv.org/abs/2505.12090", "authors": ["Mohammad Shokri", "Sarah Ita Levitan", "Rivka Levitan"], "title": "Personalized Author Obfuscation with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we investigate the efficacy of large language models (LLMs) in\nobfuscating authorship by paraphrasing and altering writing styles. Rather than\nadopting a holistic approach that evaluates performance across the entire\ndataset, we focus on user-wise performance to analyze how obfuscation\neffectiveness varies across individual authors. While LLMs are generally\neffective, we observe a bimodal distribution of efficacy, with performance\nvarying significantly across users. To address this, we propose a personalized\nprompting method that outperforms standard prompting techniques and partially\nmitigates the bimodality issue."}
{"id": "2505.12346", "pdf": "https://arxiv.org/pdf/2505.12346", "abs": "https://arxiv.org/abs/2505.12346", "authors": ["Minghan Chen", "Guikun Chen", "Wenguan Wang", "Yi Yang"], "title": "SEED-GRPO: Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy Optimization", "categories": ["cs.AI"], "comment": "On going project", "summary": "Large language models (LLMs) exhibit varying levels of confidence across\ninput prompts (questions): some lead to consistent, semantically similar\nanswers, while others yield diverse or contradictory outputs. This variation\nreflects LLM's uncertainty about the input prompt, a signal of how confidently\nthe model understands a given problem. However, vanilla Group Relative Policy\nOptimization (GRPO) treats all prompts equally during policy updates, ignoring\nthis important information about the model's knowledge boundaries. To address\nthis limitation, we propose SEED-GRPO (Semantic Entropy EnhanceD GRPO), which\nexplicitly measures LLMs' uncertainty of the input prompts semantic entropy.\nSemantic entropy measures the diversity of meaning in multiple generated\nanswers given a prompt and uses this to modulate the magnitude of policy\nupdates. This uncertainty-aware training mechanism enables dynamic adjustment\nof policy update magnitudes based on question uncertainty. It allows more\nconservative updates on high-uncertainty questions while maintaining the\noriginal learning signal on confident ones. Experimental results on five\nmathematical reasoning benchmarks (AIME24 56.7, AMC 68.7, MATH 83.4, Minerva\n34.2, and OlympiadBench 48.0) demonstrate that SEED-GRPO achieves new\nstate-of-the-art performance in average accuracy, validating the effectiveness\nof uncertainty-aware policy optimization."}
{"id": "2505.11776", "pdf": "https://arxiv.org/pdf/2505.11776", "abs": "https://arxiv.org/abs/2505.11776", "authors": ["Jiali Chen", "Avijit Mukherjee"], "title": "Generative and Contrastive Graph Representation Learning", "categories": ["cs.LG", "cs.AI", "I.2.4, I2.6"], "comment": "8 pages, 3 figures", "summary": "Self-supervised learning (SSL) on graphs generates node and graph\nrepresentations (i.e., embeddings) that can be used for downstream tasks such\nas node classification, node clustering, and link prediction. Graph SSL is\nparticularly useful in scenarios with limited or no labeled data. Existing SSL\nmethods predominantly follow contrastive or generative paradigms, each\nexcelling in different tasks: contrastive methods typically perform well on\nclassification tasks, while generative methods often excel in link prediction.\nIn this paper, we present a novel architecture for graph SSL that integrates\nthe strengths of both approaches. Our framework introduces community-aware\nnode-level contrastive learning, providing more robust and effective positive\nand negative node pairs generation, alongside graph-level contrastive learning\nto capture global semantic information. Additionally, we employ a comprehensive\naugmentation strategy that combines feature masking, node perturbation, and\nedge perturbation, enabling robust and diverse representation learning. By\nincorporating these enhancements, our model achieves superior performance\nacross multiple tasks, including node classification, clustering, and link\nprediction. Evaluations on open benchmark datasets demonstrate that our model\noutperforms state-of-the-art methods, achieving a performance lift of\n0.23%-2.01% depending on the task and dataset."}
{"id": "2505.12100", "pdf": "https://arxiv.org/pdf/2505.12100", "abs": "https://arxiv.org/abs/2505.12100", "authors": ["Isabela Pereira Gregio", "Ian Pons", "Anna Helena Reali Costa", "Artur Jordão"], "title": "Improving Fairness in LLMs Through Testing-Time Adversaries", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) push the bound-aries in natural language\nprocessing and generative AI, driving progress across various aspects of modern\nsociety. Unfortunately, the pervasive issue of bias in LLMs responses (i.e.,\npredictions) poses a significant and open challenge, hindering their\napplication in tasks involving ethical sensitivity and responsible\ndecision-making. In this work, we propose a straightforward, user-friendly and\npractical method to mitigate such biases, enhancing the reliability and\ntrustworthiness of LLMs. Our method creates multiple variations of a given\nsentence by modifying specific attributes and evaluates the corresponding\nprediction behavior compared to the original, unaltered, prediction/sentence.\nThe idea behind this process is that critical ethical predictions often exhibit\nnotable inconsistencies, indicating the presence of bias. Unlike previous\napproaches, our method relies solely on forward passes (i.e., testing-time\nadversaries), eliminating the need for training, fine-tuning, or prior\nknowledge of the training data distribution. Through extensive experiments on\nthe popular Llama family, we demonstrate the effectiveness of our method in\nimproving various fairness metrics, focusing on the reduction of disparities in\nhow the model treats individuals from different racial groups. Specifically,\nusing standard metrics, we improve the fairness in Llama3 in up to 27\npercentage points. Overall, our approach significantly enhances fairness,\nequity, and reliability in LLM-generated results without parameter tuning or\ntraining data modifications, confirming its effectiveness in practical\nscenarios. We believe our work establishes an important step toward enabling\nthe use of LLMs in tasks that require ethical considerations and responsible\ndecision-making."}
{"id": "2505.12348", "pdf": "https://arxiv.org/pdf/2505.12348", "abs": "https://arxiv.org/abs/2505.12348", "authors": ["Zhi Zheng", "Wee Sun Lee"], "title": "Reasoning-CV: Fine-tuning Powerful Reasoning LLMs for Knowledge-Assisted Claim Verification", "categories": ["cs.AI"], "comment": null, "summary": "Claim verification is essential in combating misinformation, and large\nlanguage models (LLMs) have recently emerged in this area as powerful tools for\nassessing the veracity of claims using external knowledge. Existing LLM-based\nmethods for claim verification typically adopt a Decompose-Then-Verify\nparadigm, which involves decomposing complex claims into several independent\nsub-claims and verifying each sub-claim separately. However, this paradigm\noften introduces errors during the claim decomposition process. To mitigate\nthese errors, we propose to develop the Chain-of-Thought (CoT)-Verify paradigm,\nwhich leverages LLM reasoning methods to generate CoT-verification paths for\nthe original complex claim without requiring decompositions into sub-claims and\nseparate verification stages. The CoT-Verify paradigm allows us to propose a\nnatural fine-tuning method called Reasoning-CV to enhance the verification\ncapabilities in LLMs. Reasoning-CV includes a supervised fine-tuning (SFT)\nstage and a self-improvement direct preference optimization (DPO) stage.\nUtilizing only an 8B pre-trained LLM, Reasoning-CV demonstrates superior\nknowledge-assisted claim verification performances compared to existing\nDecompose-Then-Verify methods, as well as powerful black-box LLMs such as\nGPT-4o+CoT and o1-preview. Our code is available."}
{"id": "2505.11781", "pdf": "https://arxiv.org/pdf/2505.11781", "abs": "https://arxiv.org/abs/2505.11781", "authors": ["Ziyu Zhou", "Jiaxi Hu", "Qingsong Wen", "James T. Kwok", "Yuxuan Liang"], "title": "Multi-Order Wavelet Derivative Transform for Deep Time Series Forecasting", "categories": ["cs.LG"], "comment": "Preprint. Work in progress", "summary": "In deep time series forecasting, the Fourier Transform (FT) is extensively\nemployed for frequency representation learning. However, it often struggles in\ncapturing multi-scale, time-sensitive patterns. Although the Wavelet Transform\n(WT) can capture these patterns through frequency decomposition, its\ncoefficients are insensitive to change points in time series, leading to\nsuboptimal modeling. To mitigate these limitations, we introduce the\nmulti-order Wavelet Derivative Transform (WDT) grounded in the WT, enabling the\nextraction of time-aware patterns spanning both the overall trend and subtle\nfluctuations. Compared with the standard FT and WT, which model the raw series,\nthe WDT operates on the derivative of the series, selectively magnifying\nrate-of-change cues and exposing abrupt regime shifts that are particularly\ninformative for time series modeling. Practically, we embed the WDT into a\nmulti-branch framework named WaveTS, which decomposes the input series into\nmulti-scale time-frequency coefficients, refines them via linear layers, and\nreconstructs them into the time domain via the inverse WDT. Extensive\nexperiments on ten benchmark datasets demonstrate that WaveTS achieves\nstate-of-the-art forecasting accuracy while retaining high computational\nefficiency."}
{"id": "2505.12116", "pdf": "https://arxiv.org/pdf/2505.12116", "abs": "https://arxiv.org/abs/2505.12116", "authors": ["Fitsum Gaim", "Hoyun Song", "Huije Lee", "Changgeon Ko", "Eui Jun Hwang", "Jong C. Park"], "title": "A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Content moderation research has recently made significant advances, but still\nfails to serve the majority of the world's languages due to the lack of\nresources, leaving millions of vulnerable users to online hostility. This work\npresents a large-scale human-annotated multi-task benchmark dataset for abusive\nlanguage detection in Tigrinya social media with joint annotations for three\ntasks: abusiveness, sentiment, and topic classification. The dataset comprises\n13,717 YouTube comments annotated by nine native speakers, collected from 7,373\nvideos with a total of over 1.2 billion views across 51 channels. We developed\nan iterative term clustering approach for effective data selection. Recognizing\nthat around 64% of Tigrinya social media content uses Romanized\ntransliterations rather than native Ge'ez script, our dataset accommodates both\nwriting systems to reflect actual language use. We establish strong baselines\nacross the tasks in the benchmark, while leaving significant challenges for\nfuture contributions. Our experiments reveal that small, specialized multi-task\nmodels outperform the current frontier models in the low-resource setting,\nachieving up to 86% accuracy (+7 points) in abusiveness detection. We make the\nresources publicly available to promote research on online safety."}
{"id": "2505.12355", "pdf": "https://arxiv.org/pdf/2505.12355", "abs": "https://arxiv.org/abs/2505.12355", "authors": ["Ya Shen", "Gang Chen", "Hui Ma", "Mengjie Zhang"], "title": "GATES: Cost-aware Dynamic Workflow Scheduling via Graph Attention Networks and Evolution Strategy", "categories": ["cs.AI"], "comment": "This paper has been accepted by the 34th International Joint\n  Conference on Artificial Intelligence (IJCAI-25)", "summary": "Cost-aware Dynamic Workflow Scheduling (CADWS) is a key challenge in cloud\ncomputing, focusing on devising an effective scheduling policy to efficiently\nschedule dynamically arriving workflow tasks, represented as Directed Acyclic\nGraphs (DAG), to suitable virtual machines (VMs). Deep reinforcement learning\n(DRL) has been widely employed for automated scheduling policy design. However,\nthe performance of DRL is heavily influenced by the design of the\nproblem-tailored policy network and is highly sensitive to hyperparameters and\nthe design of reward feedback. Considering the above-mentioned issues, this\nstudy proposes a novel DRL method combining Graph Attention Networks-based\npolicy network and Evolution Strategy, referred to as GATES. The contributions\nof GATES are summarized as follows: (1) GATES can capture the impact of current\ntask scheduling on subsequent tasks by learning the topological relationships\nbetween tasks in a DAG. (2) GATES can learn the importance of each VM to ready\ntasks, increasing the chance of selecting the optimal VM. (3) Utilizing\nEvolution Strategy's robustness, exploratory nature, and tolerance for delayed\nrewards, GATES achieves stable policy learning in CADWS. Extensive experimental\nresults demonstrate the superiority of the proposed GATES in CADWS,\noutperforming several state-of-the-art algorithms. Codes are available at:\nhttps://github.com/YaShen998/GATES"}
{"id": "2505.11785", "pdf": "https://arxiv.org/pdf/2505.11785", "abs": "https://arxiv.org/abs/2505.11785", "authors": ["Gina Wong", "Drew Prinster", "Suchi Saria", "Rama Chellappa", "Anqi Liu"], "title": "Improving Coverage in Combined Prediction Sets with Weighted p-values", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Conformal prediction quantifies the uncertainty of machine learning models by\naugmenting point predictions with valid prediction sets, assuming\nexchangeability. For complex scenarios involving multiple trials, models, or\ndata sources, conformal prediction sets can be aggregated to create a\nprediction set that captures the overall uncertainty, often improving\nprecision. However, aggregating multiple prediction sets with individual\n$1-\\alpha$ coverage inevitably weakens the overall guarantee, typically\nresulting in $1-2\\alpha$ worst-case coverage. In this work, we propose a\nframework for the weighted aggregation of prediction sets, where weights are\nassigned to each prediction set based on their contribution. Our framework\noffers flexible control over how the sets are aggregated, achieving tighter\ncoverage bounds that interpolate between the $1-2\\alpha$ guarantee of the\ncombined models and the $1-\\alpha$ guarantee of an individual model depending\non the distribution of weights. We extend our framework to data-dependent\nweights, and we derive a general procedure for data-dependent weight\naggregation that maintains finite-sample validity. We demonstrate the\neffectiveness of our methods through experiments on synthetic and real data in\nthe mixture-of-experts setting, and we show that aggregation with\ndata-dependent weights provides a form of adaptive coverage."}
{"id": "2505.12158", "pdf": "https://arxiv.org/pdf/2505.12158", "abs": "https://arxiv.org/abs/2505.12158", "authors": ["Elisa Bassignana", "Amanda Cercas Curry", "Dirk Hovy"], "title": "The AI Gap: How Socioeconomic Status Affects Language Technology Interactions", "categories": ["cs.CL"], "comment": "Accepted at ACL Main 2025", "summary": "Socioeconomic status (SES) fundamentally influences how people interact with\neach other and more recently, with digital technologies like Large Language\nModels (LLMs). While previous research has highlighted the interaction between\nSES and language technology, it was limited by reliance on proxy metrics and\nsynthetic data. We survey 1,000 individuals from diverse socioeconomic\nbackgrounds about their use of language technologies and generative AI, and\ncollect 6,482 prompts from their previous interactions with LLMs. We find\nsystematic differences across SES groups in language technology usage (i.e.,\nfrequency, performed tasks), interaction styles, and topics. Higher SES entails\na higher level of abstraction, convey requests more concisely, and topics like\n'inclusivity' and 'travel'. Lower SES correlates with higher\nanthropomorphization of LLMs (using ''hello'' and ''thank you'') and more\nconcrete language. Our findings suggest that while generative language\ntechnologies are becoming more accessible to everyone, socioeconomic linguistic\ndifferences still stratify their use to exacerbate the digital divide. These\ndifferences underscore the importance of considering SES in developing language\ntechnologies to accommodate varying linguistic needs rooted in socioeconomic\nfactors and limit the AI Gap across SES groups."}
{"id": "2505.12369", "pdf": "https://arxiv.org/pdf/2505.12369", "abs": "https://arxiv.org/abs/2505.12369", "authors": ["Fernando Zhapa-Camacho", "Robert Hoehndorf"], "title": "Fully Geometric Multi-Hop Reasoning on Knowledge Graphs with Transitive Relations", "categories": ["cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Geometric embedding methods have shown to be useful for multi-hop reasoning\non knowledge graphs by mapping entities and logical operations to geometric\nregions and geometric transformations, respectively. Geometric embeddings\nprovide direct interpretability framework for queries. However, current methods\nhave only leveraged the geometric construction of entities, failing to map\nlogical operations to geometric transformations and, instead, using neural\ncomponents to learn these operations. We introduce GeometrE, a geometric\nembedding method for multi-hop reasoning, which does not require learning the\nlogical operations and enables full geometric interpretability. Additionally,\nunlike previous methods, we introduce a transitive loss function and show that\nit can preserve the logical rule $\\forall a,b,c: r(a,b) \\land r(b,c) \\to\nr(a,c)$. Our experiments show that GeometrE outperforms current\nstate-of-the-art methods on standard benchmark datasets."}
{"id": "2505.11790", "pdf": "https://arxiv.org/pdf/2505.11790", "abs": "https://arxiv.org/abs/2505.11790", "authors": ["Jesson Wang", "Zhanhao Hu", "David Wagner"], "title": "JULI: Jailbreak Large Language Models by Self-Introspection", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs) are trained with safety alignment to prevent\ngenerating malicious content. Although some attacks have highlighted\nvulnerabilities in these safety-aligned LLMs, they typically have limitations,\nsuch as necessitating access to the model weights or the generation process.\nSince proprietary models through API-calling do not grant users such\npermissions, these attacks find it challenging to compromise them. In this\npaper, we propose Jailbreaking Using LLM Introspection (JULI), which jailbreaks\nLLMs by manipulating the token log probabilities, using a tiny plug-in block,\nBiasNet. JULI relies solely on the knowledge of the target LLM's predicted\ntoken log probabilities. It can effectively jailbreak API-calling LLMs under a\nblack-box setting and knowing only top-$5$ token log probabilities. Our\napproach demonstrates superior effectiveness, outperforming existing\nstate-of-the-art (SOTA) approaches across multiple metrics."}
{"id": "2505.12160", "pdf": "https://arxiv.org/pdf/2505.12160", "abs": "https://arxiv.org/abs/2505.12160", "authors": ["Darmawan Wicaksono", "Hasri Akbar Awal Rozaq", "Nevfel Boz"], "title": "Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse", "categories": ["cs.CL"], "comment": null, "summary": "Social media platforms like X (formerly Twitter) play a crucial role in\nshaping public discourse and societal norms. This study examines the term\nSessiz Istila (Silent Invasion) on Turkish social media, highlighting the rise\nof anti-refugee sentiment amidst the Syrian refugee influx. Using BERTurk and\nthe TREMO dataset, we developed an advanced Emotion Recognition Model (ERM)\ntailored for Turkish, achieving 92.62% accuracy in categorizing emotions such\nas happiness, fear, anger, sadness, disgust, and surprise. By applying this\nmodel to large-scale X data, the study uncovers emotional nuances in Turkish\ndiscourse, contributing to computational social science by advancing sentiment\nanalysis in underrepresented languages and enhancing our understanding of\nglobal digital discourse and the unique linguistic challenges of Turkish. The\nfindings underscore the transformative potential of localized NLP tools, with\nour ERM model offering practical applications for real-time sentiment analysis\nin Turkish-language contexts. By addressing critical areas, including\nmarketing, public relations, and crisis management, these models facilitate\nimproved decision-making through timely and accurate sentiment tracking. This\nhighlights the significance of advancing research that accounts for regional\nand linguistic nuances."}
{"id": "2505.12370", "pdf": "https://arxiv.org/pdf/2505.12370", "abs": "https://arxiv.org/abs/2505.12370", "authors": ["Xinbin Yuan", "Jian Zhang", "Kaixin Li", "Zhuoxuan Cai", "Lujian Yao", "Jie Chen", "Enguang Wang", "Qibin Hou", "Jinwei Chen", "Peng-Tao Jiang", "Bo Li"], "title": "Enhancing Visual Grounding for GUI Agents via Self-Evolutionary Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Graphical User Interface (GUI) agents have made substantial strides in\nunderstanding and executing user instructions across diverse platforms. Yet,\ngrounding these instructions to precise interface elements remains challenging,\nespecially in complex, high-resolution, professional environments. Traditional\nsupervised finetuning (SFT) methods often require large volumes of diverse data\nand exhibit weak generalization. To overcome these limitations, we introduce a\nreinforcement learning (RL) based framework that incorporates three core\nstrategies: (1) seed data curation to ensure high quality training samples, (2)\na dense policy gradient that provides continuous feedback based on prediction\naccuracy, and (3) a self evolutionary reinforcement finetuning mechanism that\niteratively refines the model using attention maps. With only 3k training\nsamples, our 7B-parameter model achieves state-of-the-art results among\nsimilarly sized models on three grounding benchmarks. Notably, it attains\n47.3\\% accuracy on the ScreenSpot-Pro dataset, outperforming much larger\nmodels, such as UI-TARS-72B, by a margin of 24.2\\%. These findings underscore\nthe effectiveness of RL-based approaches in enhancing GUI agent performance,\nparticularly in high-resolution, complex environments."}
{"id": "2505.11802", "pdf": "https://arxiv.org/pdf/2505.11802", "abs": "https://arxiv.org/abs/2505.11802", "authors": ["Chuang Zhao", "Hui Tang", "Hongke Zhao", "Xiaomeng Li"], "title": "Diffmv: A Unified Diffusion Framework for Healthcare Predictions with Random Missing Views and View Laziness", "categories": ["cs.LG", "cs.AI"], "comment": "SIGKDD2025, accepted", "summary": "Advanced healthcare predictions offer significant improvements in patient\noutcomes by leveraging predictive analytics. Existing works primarily utilize\nvarious views of Electronic Health Record (EHR) data, such as diagnoses, lab\ntests, or clinical notes, for model training. These methods typically assume\nthe availability of complete EHR views and that the designed model could fully\nleverage the potential of each view. However, in practice, random missing views\nand view laziness present two significant challenges that hinder further\nimprovements in multi-view utilization. To address these challenges, we\nintroduce Diffmv, an innovative diffusion-based generative framework designed\nto advance the exploitation of multiple views of EHR data. Specifically, to\naddress random missing views, we integrate various views of EHR data into a\nunified diffusion-denoising framework, enriched with diverse contextual\nconditions to facilitate progressive alignment and view transformation. To\nmitigate view laziness, we propose a novel reweighting strategy that assesses\nthe relative advantages of each view, promoting a balanced utilization of\nvarious data views within the model. Our proposed strategy achieves superior\nperformance across multiple health prediction tasks derived from three popular\ndatasets, including multi-view and multi-modality scenarios."}
{"id": "2505.12182", "pdf": "https://arxiv.org/pdf/2505.12182", "abs": "https://arxiv.org/abs/2505.12182", "authors": ["Haohang Li", "Yupeng Cao", "Yangyang Yu", "Jordan W. Suchow", "Zining Zhu"], "title": "Truth Neurons", "categories": ["cs.CL"], "comment": null, "summary": "Despite their remarkable success and deployment across diverse workflows,\nlanguage models sometimes produce untruthful responses. Our limited\nunderstanding of how truthfulness is mechanistically encoded within these\nmodels jeopardizes their reliability and safety. In this paper, we propose a\nmethod for identifying representations of truthfulness at the neuron level. We\nshow that language models contain truth neurons, which encode truthfulness in a\nsubject-agnostic manner. Experiments conducted across models of varying scales\nvalidate the existence of truth neurons, confirming that the encoding of\ntruthfulness at the neuron level is a property shared by many language models.\nThe distribution patterns of truth neurons over layers align with prior\nfindings on the geometry of truthfulness. Selectively suppressing the\nactivations of truth neurons found through the TruthfulQA dataset degrades\nperformance both on TruthfulQA and on other benchmarks, showing that the\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\nnovel insights into the mechanisms underlying truthfulness in language models\nand highlight potential directions toward improving their trustworthiness and\nreliability."}
{"id": "2505.12371", "pdf": "https://arxiv.org/pdf/2505.12371", "abs": "https://arxiv.org/abs/2505.12371", "authors": ["Yinghao Zhu", "Ziyi He", "Haoran Hu", "Xiaochen Zheng", "Xichen Zhang", "Zixiang Wang", "Junyi Gao", "Liantao Ma", "Lequan Yu"], "title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/."}
{"id": "2505.11812", "pdf": "https://arxiv.org/pdf/2505.11812", "abs": "https://arxiv.org/abs/2505.11812", "authors": ["Yang Tan", "Wenrui Gou", "Bozitao Zhong", "Liang Hong", "Huiqun Yu", "Bingxin Zhou"], "title": "VenusX: Unlocking Fine-Grained Functional Understanding of Proteins", "categories": ["cs.LG", "cs.CL", "q-bio.QM"], "comment": "29 pages, 3 figures, 17 tables", "summary": "Deep learning models have driven significant progress in predicting protein\nfunction and interactions at the protein level. While these advancements have\nbeen invaluable for many biological applications such as enzyme engineering and\nfunction annotation, a more detailed perspective is essential for understanding\nprotein functional mechanisms and evaluating the biological knowledge captured\nby models. To address this demand, we introduce VenusX, the first large-scale\nbenchmark for fine-grained functional annotation and function-based protein\npairing at the residue, fragment, and domain levels. VenusX comprises three\nmajor task categories across six types of annotations, including residue-level\nbinary classification, fragment-level multi-class classification, and pairwise\nfunctional similarity scoring for identifying critical active sites, binding\nsites, conserved sites, motifs, domains, and epitopes. The benchmark features\nover 878,000 samples curated from major open-source databases such as InterPro,\nBioLiP, and SAbDab. By providing mixed-family and cross-family splits at three\nsequence identity thresholds, our benchmark enables a comprehensive assessment\nof model performance on both in-distribution and out-of-distribution scenarios.\nFor baseline evaluation, we assess a diverse set of popular and open-source\nmodels, including pre-trained protein language models, sequence-structure\nhybrids, structure-based methods, and alignment-based techniques. Their\nperformance is reported across all benchmark datasets and evaluation settings\nusing multiple metrics, offering a thorough comparison and a strong foundation\nfor future research. Code and data are publicly available at\nhttps://github.com/ai4protein/VenusX."}
{"id": "2505.12183", "pdf": "https://arxiv.org/pdf/2505.12183", "abs": "https://arxiv.org/abs/2505.12183", "authors": ["Manari Hirose", "Masato Uchida"], "title": "Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "23 pages, 5 figures, 17 tables", "summary": "The widespread integration of Large Language Models (LLMs) across various\nsectors has highlighted the need for empirical research to understand their\nbiases, thought patterns, and societal implications to ensure ethical and\neffective use. In this study, we propose a novel framework for evaluating LLMs,\nfocusing on uncovering their ideological biases through a quantitative analysis\nof 436 binary-choice questions, many of which have no definitive answer. By\napplying our framework to ChatGPT and Gemini, findings revealed that while LLMs\ngenerally maintain consistent opinions on many topics, their ideologies differ\nacross models and languages. Notably, ChatGPT exhibits a tendency to change\ntheir opinion to match the questioner's opinion. Both models also exhibited\nproblematic biases, unethical or unfair claims, which might have negative\nsocietal impacts. These results underscore the importance of addressing both\nideological and ethical considerations when evaluating LLMs. The proposed\nframework offers a flexible, quantitative method for assessing LLM behavior,\nproviding valuable insights for the development of more socially aligned AI\nsystems."}
{"id": "2505.12440", "pdf": "https://arxiv.org/pdf/2505.12440", "abs": "https://arxiv.org/abs/2505.12440", "authors": ["Jakub Skrzyński", "Dominik Sepioło", "Antoni Ligęza"], "title": "Model Discovery with Grammatical Evolution. An Experiment with Prime Numbers", "categories": ["cs.AI"], "comment": "Presented during 5th Polish Conference on Artificial Intelligence,\n  published in \"PROGRESS IN POLISH ARTIFICIAL INTELLIGENCE RESEARCH 5\" ISBN\n  978-83-8156-696-4", "summary": "Machine Learning produces efficient decision and prediction models based on\ninput-output data only. Such models have the form of decision trees or neural\nnets and are far from transparent analytical models, based on mathematical\nformulas. Analytical model discovery requires additional knowledge and may be\nperformed with Grammatical Evolution. Such models are transparent, concise, and\nhave readable components and structure. This paper reports on a non-trivial\nexperiment with generating such models."}
{"id": "2505.11821", "pdf": "https://arxiv.org/pdf/2505.11821", "abs": "https://arxiv.org/abs/2505.11821", "authors": ["Siliang Zeng", "Quan Wei", "William Brown", "Oana Frunza", "Yuriy Nevmyvaka", "Mingyi Hong"], "title": "Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment", "categories": ["cs.LG"], "comment": "work in progress", "summary": "This paper investigates approaches to enhance the reasoning capabilities of\nLarge Language Model (LLM) agents using Reinforcement Learning (RL).\nSpecifically, we focus on multi-turn tool-use scenarios, which can be naturally\nmodeled as Markov Decision Processes (MDPs). While existing approaches often\ntrain multi-turn LLM agents with trajectory-level advantage estimation in\nbandit settings, they struggle with turn-level credit assignment across\nmultiple decision steps, limiting their performance on multi-turn reasoning\ntasks. To address this, we introduce a fine-grained turn-level advantage\nestimation strategy to enable more precise credit assignment in multi-turn\nagent interactions. The strategy is general and can be incorporated into\nvarious RL algorithms such as Group Relative Preference Optimization (GRPO).\nOur experimental evaluation on multi-turn reasoning and search-based tool-use\ntasks with GRPO implementations highlights the effectiveness of the MDP\nframework and the turn-level credit assignment in advancing the multi-turn\nreasoning capabilities of LLM agents in complex decision-making settings. Our\nmethod achieves 100% success in tool execution and 50% accuracy in exact answer\nmatching, significantly outperforming baselines, which fail to invoke tools and\nachieve only 20-30% exact match accuracy."}
{"id": "2505.12196", "pdf": "https://arxiv.org/pdf/2505.12196", "abs": "https://arxiv.org/abs/2505.12196", "authors": ["Yi-Chien Lin", "Hongao Zhu", "William Schuler"], "title": "Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled", "categories": ["cs.CL"], "comment": null, "summary": "The impressive linguistic abilities of large language models (LLMs) have\nrecommended them as models of human sentence processing, with some conjecturing\na positive 'quality-power' relationship (Wilcox et al., 2023), in which\nlanguage models' (LMs') fit to psychometric data continues to improve as their\nability to predict words in context increases. This is important because it\nsuggests that elements of LLM architecture, such as veridical attention to\ncontext and a unique objective of predicting upcoming words, reflect the\narchitecture of the human sentence processing faculty, and that any\ninadequacies in predicting human reading time and brain imaging data may be\nattributed to insufficient model complexity, which recedes as larger models\nbecome available. Recent studies (Oh and Schuler, 2023) have shown this scaling\ninverts after a point, as LMs become excessively large and accurate, when word\nprediction probability (as information-theoretic surprisal) is used as a\npredictor. Other studies propose the use of entire vectors from differently\nsized LLMs, still showing positive scaling (Schrimpf et al., 2021), casting\ndoubt on the value of surprisal as a predictor, but do not control for the\nlarger number of predictors in vectors from larger LMs. This study evaluates\nLLM scaling using entire LLM vectors, while controlling for the larger number\nof predictors in vectors from larger LLMs. Results show that inverse scaling\nobtains, suggesting that inadequacies in predicting human reading time and\nbrain imaging data may be due to substantial misalignment between LLMs and\nhuman sentence processing, which worsens as larger models are used."}
{"id": "2505.12470", "pdf": "https://arxiv.org/pdf/2505.12470", "abs": "https://arxiv.org/abs/2505.12470", "authors": ["Jiaqi Wang", "Yusen Zhang", "Xi Li"], "title": "NeuroGen: Neural Network Parameter Generation via Large Language Models", "categories": ["cs.AI"], "comment": "The three authors contributed equally to this work. The codes will be\n  public after being accepted", "summary": "Acquiring the parameters of neural networks (NNs) has been one of the most\nimportant problems in machine learning since the inception of NNs. Traditional\napproaches, such as backpropagation and forward-only optimization, acquire\nparameters via iterative data fitting to gradually optimize them. This paper\naims to explore the feasibility of a new direction: acquiring NN parameters via\nlarge language model generation. We propose NeuroGen, a generalized and\neasy-to-implement two-stage approach for NN parameter generation conditioned on\ndescriptions of the data, task, and network architecture. Stage one is\nParameter Reference Knowledge Injection, where LLMs are pretrained on NN\ncheckpoints to build foundational understanding of parameter space, whereas\nstage two is Context-Enhanced Instruction Tuning, enabling LLMs to adapt to\nspecific tasks through enriched, task-aware prompts. Experimental results\ndemonstrate that NeuroGen effectively generates usable NN parameters. Our\nfindings highlight the feasibility of LLM-based NN parameter generation and\nsuggest a promising new paradigm where LLMs and lightweight NNs can coexist\nsynergistically"}
{"id": "2505.11823", "pdf": "https://arxiv.org/pdf/2505.11823", "abs": "https://arxiv.org/abs/2505.11823", "authors": ["Yuhao Sun", "Zhenyi Zhang", "Zihan Wang", "Tiejun Li", "Peijie Zhou"], "title": "Variational Regularized Unbalanced Optimal Transport: Single Network, Least Action", "categories": ["cs.LG", "math.OC", "q-bio.QM"], "comment": null, "summary": "Recovering the dynamics from a few snapshots of a high-dimensional system is\na challenging task in statistical physics and machine learning, with important\napplications in computational biology. Many algorithms have been developed to\ntackle this problem, based on frameworks such as optimal transport and the\nSchr\\\"odinger bridge. A notable recent framework is Regularized Unbalanced\nOptimal Transport (RUOT), which integrates both stochastic dynamics and\nunnormalized distributions. However, since many existing methods do not\nexplicitly enforce optimality conditions, their solutions often struggle to\nsatisfy the principle of least action and meet challenges to converge in a\nstable and reliable way. To address these issues, we propose Variational RUOT\n(Var-RUOT), a new framework to solve the RUOT problem. By incorporating the\noptimal necessary conditions for the RUOT problem into both the\nparameterization of the search space and the loss function design, Var-RUOT\nonly needs to learn a scalar field to solve the RUOT problem and can search for\nsolutions with lower action. We also examined the challenge of selecting a\ngrowth penalty function in the widely used Wasserstein-Fisher-Rao metric and\nproposed a solution that better aligns with biological priors in Var-RUOT. We\nvalidated the effectiveness of Var-RUOT on both simulated data and real\nsingle-cell datasets. Compared with existing algorithms, Var-RUOT can find\nsolutions with lower action while exhibiting faster convergence and improved\ntraining stability."}
{"id": "2505.12201", "pdf": "https://arxiv.org/pdf/2505.12201", "abs": "https://arxiv.org/abs/2505.12201", "authors": ["Xiyan Fu", "Wei Liu"], "title": "How Reliable is Multilingual LLM-as-a-Judge?", "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced\nlarge language models assess generation results in alignment with human\ninstructions. While these models serve as a promising alternative to human\nannotators, their reliability in multilingual evaluation remains uncertain. To\nbridge this gap, we conduct a comprehensive analysis of multilingual\nLLM-as-a-Judge. Specifically, we evaluate five models from different model\nfamilies across five diverse tasks involving 25 languages. Our findings reveal\nthat LLMs struggle to achieve consistent judgment results across languages,\nwith an average Fleiss' Kappa of approximately 0.3, and some models performing\neven worse. To investigate the cause of inconsistency, we analyze various\ninfluencing factors. We observe that consistency varies significantly across\nlanguages, with particularly poor performance in low-resource languages.\nAdditionally, we find that neither training on multilingual data nor increasing\nmodel scale directly improves judgment consistency. These findings suggest that\nLLMs are not yet reliable for evaluating multilingual predictions. We finally\npropose an ensemble strategy which improves the consistency of the multilingual\njudge in real-world applications."}
{"id": "2505.12493", "pdf": "https://arxiv.org/pdf/2505.12493", "abs": "https://arxiv.org/abs/2505.12493", "authors": ["Longxi Gao", "Li Zhang", "Mengwei Xu"], "title": "UIShift: Enhancing VLM-based GUI Agents through Self-supervised Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Training effective Vision Language Models (VLMs) for GUI agents typically\nrelies on supervised fine-tuning (SFT) over large-scale annotated datasets,\nwhere the collection process is labor-intensive and error-prone. In this work,\nwe propose a self-supervised inverse dynamics task to enable VLMs to learn from\nGUI transition pairs by inferring the action that caused that transition. This\ntraining task offers two advantages: (1) It enables VLMs to ignore variations\nunrelated to user actions (e.g., background refreshes, ads) and to focus on\ntrue affordances such as buttons and input fields within complex GUIs. (2) The\ntraining data can be easily obtained from existing GUI trajectories without\nrequiring human annotation, and it can be easily scaled through automatic\noffline exploration. Using this training task, we propose UI-shift, a framework\nfor enhancing VLM-based GUI agents through self-supervised reinforcement\nlearning (RL). With only 2K training samples sourced from existing datasets,\ntwo VLMs -- Qwen2.5-VL-3B and Qwen2.5-VL-7B -- trained with UI-Shift achieve\ncompetitive or superior performance on grounding tasks (ScreenSpot-series\nbenchmarks) and GUI automation tasks (AndroidControl), compared to SFT\nbaselines and GUI-specific models that explicitly elicit reasoning abilities\nduring RL. Our findings suggest a potential direction for enhancing VLMs for\nGUI agents by leveraging more self-supervised training data in the future."}
{"id": "2505.11824", "pdf": "https://arxiv.org/pdf/2505.11824", "abs": "https://arxiv.org/abs/2505.11824", "authors": ["Minsu Kim", "Jean-Pierre Falet", "Oliver E. Richardson", "Xiaoyin Chen", "Moksh Jain", "Sungjin Ahn", "Sungsoo Ahn", "Yoshua Bengio"], "title": "Search-Based Correction of Reasoning Chains for Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Chain-of-Thought (CoT) reasoning has advanced the capabilities and\ntransparency of language models (LMs); however, reasoning chains can contain\ninaccurate statements that reduce performance and trustworthiness. To address\nthis, we introduce a new self-correction framework that augments each reasoning\nstep in a CoT with a latent variable indicating its veracity, enabling modeling\nof all possible truth assignments rather than assuming correctness throughout.\nTo efficiently explore this expanded space, we introduce Search Corrector, a\ndiscrete search algorithm over boolean-valued veracity assignments. It\nefficiently performs otherwise intractable inference in the posterior\ndistribution over veracity assignments by leveraging the LM's joint likelihood\nover veracity and the final answer as a proxy reward. This efficient\ninference-time correction method facilitates supervised fine-tuning of an\nAmortized Corrector by providing pseudo-labels for veracity. The Amortized\nCorrector generalizes self-correction, enabling accurate zero-shot veracity\ninference in novel contexts. Empirical results demonstrate that Search\nCorrector reliably identifies errors in logical (ProntoQA) and mathematical\nreasoning (GSM8K) benchmarks. The Amortized Corrector achieves comparable\nzero-shot accuracy and improves final answer accuracy by up to 25%."}
{"id": "2505.12212", "pdf": "https://arxiv.org/pdf/2505.12212", "abs": "https://arxiv.org/abs/2505.12212", "authors": ["Shaobo Wang", "Ziming Wang", "Xiangqi Jin", "Jize Wang", "Jiajun Zhang", "Kaixin Li", "Zichen Wen", "Zhong Li", "Conghui He", "Xuming Hu", "Linfeng Zhang"], "title": "Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main, 18 pages, 8 figures, 6 tables", "summary": "Fine-tuning large language models (LLMs) on task-specific data is essential\nfor their effective deployment. As dataset sizes grow, efficiently selecting\noptimal subsets for training becomes crucial to balancing performance and\ncomputational costs. Traditional data selection methods often require\nfine-tuning a scoring model on the target dataset, which is time-consuming and\nresource-intensive, or rely on heuristics that fail to fully leverage the\nmodel's predictive capabilities. To address these challenges, we propose Data\nWhisperer, an efficient, training-free, attention-based method that leverages\nfew-shot in-context learning with the model to be fine-tuned. Comprehensive\nevaluations were conducted on both raw and synthetic datasets across diverse\ntasks and models. Notably, Data Whisperer achieves superior performance\ncompared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just\n10% of the data, and outperforms existing methods with a 3.1-point improvement\nand a 7.4$\\times$ speedup."}
{"id": "2505.12500", "pdf": "https://arxiv.org/pdf/2505.12500", "abs": "https://arxiv.org/abs/2505.12500", "authors": ["Jingyue Gao", "Runji Lin", "Keming Lu", "Bowen Yu", "Junyang Lin", "Jianyu Chen"], "title": "MARGE: Improving Math Reasoning for LLMs with Guided Exploration", "categories": ["cs.AI"], "comment": "To appear at ICML 2025", "summary": "Large Language Models (LLMs) exhibit strong potential in mathematical\nreasoning, yet their effectiveness is often limited by a shortage of\nhigh-quality queries. This limitation necessitates scaling up computational\nresponses through self-generated data, yet current methods struggle due to\nspurious correlated data caused by ineffective exploration across all reasoning\nstages. To address such challenge, we introduce \\textbf{MARGE}: Improving\n\\textbf{Ma}th \\textbf{R}easoning with \\textbf{G}uided \\textbf{E}xploration, a\nnovel method to address this issue and enhance mathematical reasoning through\nhit-guided exploration. MARGE systematically explores intermediate reasoning\nstates derived from self-generated solutions, enabling adequate exploration and\nimproved credit assignment throughout the reasoning process. Through extensive\nexperiments across multiple backbone models and benchmarks, we demonstrate that\nMARGE significantly improves reasoning capabilities without requiring external\nannotations or training additional value models. Notably, MARGE improves both\nsingle-shot accuracy and exploration diversity, mitigating a common trade-off\nin alignment methods. These results demonstrate MARGE's effectiveness in\nenhancing mathematical reasoning capabilities and unlocking the potential of\nscaling self-generated training data. Our code and models are available at\n\\href{https://github.com/georgao35/MARGE}{this link}."}
{"id": "2505.11836", "pdf": "https://arxiv.org/pdf/2505.11836", "abs": "https://arxiv.org/abs/2505.11836", "authors": ["Jeremy Budd", "Javier Ideami", "Benjamin Macdowall Rynne", "Keith Duggar", "Randall Balestriero"], "title": "SplInterp: Improving our Understanding and Training of Sparse Autoencoders", "categories": ["cs.LG", "cs.AI", "68T07, 65D07"], "comment": "44 pages, 38 figures, under review", "summary": "Sparse autoencoders (SAEs) have received considerable recent attention as\ntools for mechanistic interpretability, showing success at extracting\ninterpretable features even from very large LLMs. However, this research has\nbeen largely empirical, and there have been recent doubts about the true\nutility of SAEs. In this work, we seek to enhance the theoretical understanding\nof SAEs, using the spline theory of deep learning. By situating SAEs in this\nframework: we discover that SAEs generalise ``$k$-means autoencoders'' to be\npiecewise affine, but sacrifice accuracy for interpretability vs. the optimal\n``$k$-means-esque plus local principal component analysis (PCA)'' piecewise\naffine autoencoder. We characterise the underlying geometry of (TopK) SAEs\nusing power diagrams. And we develop a novel proximal alternating method SGD\n(PAM-SGD) algorithm for training SAEs, with both solid theoretical foundations\nand promising empirical results in MNIST and LLM experiments, particularly in\nsample efficiency and (in the LLM setting) improved sparsity of codes. All code\nis available at: https://github.com/splInterp2025/splInterp"}
{"id": "2505.12215", "pdf": "https://arxiv.org/pdf/2505.12215", "abs": "https://arxiv.org/abs/2505.12215", "authors": ["Jiwei Tang", "Zhicheng Zhang", "Shunlong Wu", "Jingheng Ye", "Lichen Bai", "Zitai Wang", "Tingwei Lu", "Jiaqi Chen", "Lin Hai", "Hai-Tao Zheng", "Hong-Gee Kim"], "title": "GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment", "categories": ["cs.CL"], "comment": "19 pages, 7 figures", "summary": "Large language models (LLMs) have achieved impressive performance in a\nvariety of natural language processing (NLP) tasks. However, when applied to\nlong-context scenarios, they face two challenges, i.e., low computational\nefficiency and much redundant information. This paper introduces GMSA, a\ncontext compression framework based on the encoder-decoder architecture, which\naddresses these challenges by reducing input sequence length and redundant\ninformation. Structurally, GMSA has two key components: Group Merging and Layer\nSemantic Alignment (LSA). Group merging is used to effectively and efficiently\nextract summary vectors from the original context. Layer semantic alignment, on\nthe other hand, aligns the high-level summary vectors with the low-level\nprimary input semantics, thus bridging the semantic gap between different\nlayers. In the training process, GMSA first learns soft tokens that contain\ncomplete semantics through autoencoder training. To furtherly adapt GMSA to\ndownstream tasks, we propose Knowledge Extraction Fine-tuning (KEFT) to extract\nknowledge from the soft tokens for downstream tasks. We train GMSA by randomly\nsampling the compression rate for each sample in the dataset. Under this\ncondition, GMSA not only significantly outperforms the traditional compression\nparadigm in context restoration but also achieves stable and significantly\nfaster convergence with only a few encoder layers. In downstream\nquestion-answering (QA) tasks, GMSA can achieve approximately a 2x speedup in\nend-to-end inference while outperforming both the original input prompts and\nvarious state-of-the-art (SOTA) methods by a large margin."}
{"id": "2505.12501", "pdf": "https://arxiv.org/pdf/2505.12501", "abs": "https://arxiv.org/abs/2505.12501", "authors": ["Edward Y. Chang", "Longling Geng"], "title": "ALAS: A Stateful Multi-LLM Agent Framework for Disruption-Aware Planning", "categories": ["cs.AI", "I.2.7"], "comment": "36 pages, 10 figures, 19 tables", "summary": "Large language models (LLMs) excel at rapid generation of text and multimodal\ncontent, yet they falter on transaction-style planning that demands ACID-like\nguarantees and real-time disruption recovery. We present Adaptive LLM Agent\nSystem (ALAS), a framework that tackles four fundamental LLM deficits: (i)\nabsence of self-verification, (ii) context erosion, (iii) next-token myopia,\nand (iv) lack of persistent state. ALAS decomposes each plan into\nrole-specialized agents, equips them with automatic state tracking, and\ncoordinates them through a lightweight protocol. When disruptions arise, agents\napply history-aware local compensation, avoiding costly global replanning and\ncontaining cascade effects. On real-world, large-scale job-shop scheduling\nbenchmarks, ALAS sets new best results for static sequential planning and\nexcels in dynamic reactive scenarios with unexpected disruptions. These gains\nshow that principled modularization plus targeted compensation can unlock\nscalable and resilient planning with LLMs."}
{"id": "2505.11837", "pdf": "https://arxiv.org/pdf/2505.11837", "abs": "https://arxiv.org/abs/2505.11837", "authors": ["Ziyao Cui", "Minxing Zhang", "Jian Pei"], "title": "On Membership Inference Attacks in Knowledge Distillation", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Nowadays, Large Language Models (LLMs) are trained on huge datasets, some\nincluding sensitive information. This poses a serious privacy concern because\nprivacy attacks such as Membership Inference Attacks (MIAs) may detect this\nsensitive information. While knowledge distillation compresses LLMs into\nefficient, smaller student models, its impact on privacy remains underexplored.\nIn this paper, we investigate how knowledge distillation affects model\nrobustness against MIA. We focus on two questions. First, how is private data\nprotected in teacher and student models? Second, how can we strengthen privacy\npreservation against MIAs in knowledge distillation? Through comprehensive\nexperiments, we show that while teacher and student models achieve similar\noverall MIA accuracy, teacher models better protect member data, the primary\ntarget of MIA, whereas student models better protect non-member data. To\naddress this vulnerability in student models, we propose 5 privacy-preserving\ndistillation methods and demonstrate that they successfully reduce student\nmodels' vulnerability to MIA, with ensembling further stabilizing the\nrobustness, offering a reliable approach for distilling more secure and\nefficient student models. Our implementation source code is available at\nhttps://github.com/richardcui18/MIA_in_KD."}
{"id": "2505.12216", "pdf": "https://arxiv.org/pdf/2505.12216", "abs": "https://arxiv.org/abs/2505.12216", "authors": ["Rongguang Ye", "Ming Tang"], "title": "One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models", "categories": ["cs.CL"], "comment": "ACL Findings", "summary": "Existing pruning methods for large language models (LLMs) focus on achieving\nhigh compression rates while maintaining model performance. Although these\nmethods have demonstrated satisfactory performance in handling a single user's\ncompression request, their processing time increases linearly with the number\nof requests, making them inefficient for real-world scenarios with multiple\nsimultaneous requests. To address this limitation, we propose a Univeral Model\nfor Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that\nlearns to map arbitrary requests to their optimal pruning strategy. The\nchallenge in training StratNet lies in the high computational cost of\nevaluating pruning strategies and the non-differentiable nature of the pruning\nprocess, which hinders gradient backpropagation for StratNet updates. To\novercome these challenges, we leverage a Gaussian process to approximate the\nevaluation process. Since the gradient of the Gaussian process is computable,\nwe can use it to approximate the gradient of the non-differentiable pruning\nprocess, thereby enabling StratNet updates. Experimental results show that\nUniCuCo is 28 times faster than baselines in processing 64 requests, while\nmaintaining comparable accuracy to baselines."}
{"id": "2505.12565", "pdf": "https://arxiv.org/pdf/2505.12565", "abs": "https://arxiv.org/abs/2505.12565", "authors": ["Carl Edwards", "Chi Han", "Gawon Lee", "Thao Nguyen", "Bowen Jin", "Chetan Kumar Prasad", "Sara Szymkuć", "Bartosz A. Grzybowski", "Ying Diao", "Jiawei Han", "Ge Liu", "Hao Peng", "Martin D. Burke", "Heng Ji"], "title": "mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model", "categories": ["cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Despite their ability to understand chemical knowledge and accurately\ngenerate sequential representations, large language models (LLMs) remain\nlimited in their capacity to propose novel molecules with drug-like properties.\nIn addition, the molecules that LLMs propose can often be challenging to make\nin the lab. To more effectively enable the discovery of functional small\nmolecules, LLMs need to learn a molecular language. However, LLMs are currently\nlimited by encoding molecules from atoms. In this paper, we argue that just\nlike tokenizing texts into (sub-)word tokens instead of characters, molecules\nshould be decomposed and reassembled at the level of functional building\nblocks, i.e., parts of molecules that bring unique functions and serve as\neffective building blocks for real-world automated laboratory synthesis. This\nmotivates us to propose mCLM, a modular Chemical-Language Model tokenizing\nmolecules into building blocks and learning a bilingual language model of both\nnatural language descriptions of functions and molecule building blocks. By\nreasoning on such functional building blocks, mCLM guarantees to generate\nefficiently synthesizable molecules thanks to recent progress in block-based\nchemistry, while also improving the functions of molecules in a principled\nmanner. In experiments on 430 FDA-approved drugs, we find mCLM capable of\nsignificantly improving 5 out of 6 chemical functions critical to determining\ndrug potentials. More importantly, mCLM can reason on multiple functions and\nimprove the FDA-rejected drugs (``fallen angels'') over multiple iterations to\ngreatly improve their shortcomings."}
{"id": "2505.11840", "pdf": "https://arxiv.org/pdf/2505.11840", "abs": "https://arxiv.org/abs/2505.11840", "authors": ["Huan Li", "Yiming Dong", "Zhouchen Lin"], "title": "On the $O(\\frac{\\sqrt{d}}{K^{1/4}})$ Convergence Rate of AdamW Measured by $\\ell_1$ Norm", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "As the default optimizer for training large language models, AdamW has\nachieved remarkable success in deep learning. However, its convergence behavior\nis not theoretically well-understood. This paper establishes the convergence\nrate $\\frac{1}{K}\\sum_{k=1}^KE\\left[\\|\\nabla f(x^k)\\|_1\\right]\\leq\nO(\\frac{\\sqrt{d}C}{K^{1/4}})$ for AdamW measured by $\\ell_1$ norm, where $K$\nrepresents the iteration number, $d$ denotes the model dimension, and $C$\nmatches the constant in the optimal convergence rate of SGD. Theoretically, we\nhave $E\\left[\\|\\nabla f(x)\\|_1\\right]\\geq\\sqrt{\\frac{2d}{\\pi}}E\\left[\\|\\nabla\nf(x)\\|_2\\right]$ when each element of $\\nabla f(x)$ is generated from Gaussian\ndistribution $\\mathcal N(0,1)$. Empirically, our experimental results on\nreal-world deep learning tasks reveal $\\|\\nabla\nf(x)\\|_1=\\varTheta(\\sqrt{d})\\|\\nabla f(x)\\|_2$. Both support that our\nconvergence rate can be considered to be analogous to the optimal\n$\\frac{1}{K}\\sum_{k=1}^KE\\left[\\|\\nabla f(x^k)\\|_2\\right]\\leq\nO(\\frac{C}{K^{1/4}})$ convergence rate of SGD."}
{"id": "2505.12218", "pdf": "https://arxiv.org/pdf/2505.12218", "abs": "https://arxiv.org/abs/2505.12218", "authors": ["Tong Bao", "Yi Zhao", "Jin Mao", "Chengzhi Zhang"], "title": "Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs), such as ChatGPT, have prompted academic\nconcerns about their impact on academic writing. Existing studies have\nprimarily examined LLM usage in academic writing through quantitative\napproaches, such as word frequency statistics and probability-based analyses.\nHowever, few have systematically examined the potential impact of LLMs on the\nlinguistic characteristics of academic writing. To address this gap, we\nconducted a large-scale analysis across 823,798 abstracts published in last\ndecade from arXiv dataset. Through the linguistic analysis of features such as\nthe frequency of LLM-preferred words, lexical complexity, syntactic complexity,\ncohesion, readability and sentiment, the results indicate a significant\nincrease in the proportion of LLM-preferred words in abstracts, revealing the\nwidespread influence of LLMs on academic writing. Additionally, we observed an\nincrease in lexical complexity and sentiment in the abstracts, but a decrease\nin syntactic complexity, suggesting that LLMs introduce more new vocabulary and\nsimplify sentence structure. However, the significant decrease in cohesion and\nreadability indicates that abstracts have fewer connecting words and are\nbecoming more difficult to read. Moreover, our analysis reveals that scholars\nwith weaker English proficiency were more likely to use the LLMs for academic\nwriting, and focused on improving the overall logic and fluency of the\nabstracts. Finally, at discipline level, we found that scholars in Computer\nScience showed more pronounced changes in writing style, while the changes in\nMathematics were minimal."}
{"id": "2505.12575", "pdf": "https://arxiv.org/pdf/2505.12575", "abs": "https://arxiv.org/abs/2505.12575", "authors": ["Jie Zhang", "Cezara Petrui", "Kristina Nikolić", "Florian Tramèr"], "title": "RealMath: A Continuous Benchmark for Evaluating Language Models on Research-Level Mathematics", "categories": ["cs.AI"], "comment": null, "summary": "Existing benchmarks for evaluating mathematical reasoning in large language\nmodels (LLMs) rely primarily on competition problems, formal proofs, or\nartificially challenging questions -- failing to capture the nature of\nmathematics encountered in actual research environments. We introduce RealMath,\na novel benchmark derived directly from research papers and mathematical forums\nthat assesses LLMs' abilities on authentic mathematical tasks. Our approach\naddresses three critical challenges: sourcing diverse research-level content,\nenabling reliable automated evaluation through verifiable statements, and\ndesigning a continually refreshable dataset to mitigate contamination risks.\nExperimental results across multiple LLMs reveal surprising capabilities in\nhandling research mathematics compared to competition problems, suggesting\ncurrent models may already serve as valuable assistants for working\nmathematicians despite limitations on highly challenging problems. The code and\ndataset for RealMath are publicly available."}
{"id": "2505.11846", "pdf": "https://arxiv.org/pdf/2505.11846", "abs": "https://arxiv.org/abs/2505.11846", "authors": ["Vahid Shahverdi", "Giovanni Luca Marchetti", "Kathlén Kohn"], "title": "Learning on a Razor's Edge: the Singularity Bias of Polynomial Neural Networks", "categories": ["cs.LG", "math.AG"], "comment": null, "summary": "Deep neural networks often infer sparse representations, converging to a\nsubnetwork during the learning process. In this work, we theoretically analyze\nsubnetworks and their bias through the lens of algebraic geometry. We consider\nfully-connected networks with polynomial activation functions, and focus on the\ngeometry of the function space they parametrize, often referred to as\nneuromanifold. First, we compute the dimension of the subspace of the\nneuromanifold parametrized by subnetworks. Second, we show that this subspace\nis singular. Third, we argue that such singularities often correspond to\ncritical points of the training dynamics. Lastly, we discuss convolutional\nnetworks, for which subnetworks and singularities are similarly related, but\nthe bias does not arise."}
{"id": "2505.12236", "pdf": "https://arxiv.org/pdf/2505.12236", "abs": "https://arxiv.org/abs/2505.12236", "authors": ["Quanjiang Guo", "Jinchuan Zhang", "Sijie Wang", "Ling Tian", "Zhao Kang", "Bin Yan", "Weidong Xiao"], "title": "Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 6 figures, Appear on IJCAI 2025", "summary": "Few-Shot Relation Extraction (FSRE) remains a challenging task due to the\nscarcity of annotated data and the limited generalization capabilities of\nexisting models. Although large language models (LLMs) have demonstrated\npotential in FSRE through in-context learning (ICL), their general-purpose\ntraining objectives often result in suboptimal performance for task-specific\nrelation extraction. To overcome these challenges, we propose TKRE (Two-Stage\nKnowledge-Guided Pre-training for Relation Extraction), a novel framework that\nsynergistically integrates LLMs with traditional relation extraction models,\nbridging generative and discriminative learning paradigms. TKRE introduces two\nkey innovations: (1) leveraging LLMs to generate explanation-driven knowledge\nand schema-constrained synthetic data, addressing the issue of data scarcity;\nand (2) a two-stage pre-training strategy combining Masked Span Language\nModeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational\nreasoning and generalization. Together, these components enable TKRE to\neffectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets\ndemonstrate the efficacy of TKRE, achieving new state-of-the-art performance in\nFSRE and underscoring its potential for broader application in low-resource\nscenarios. \\footnote{The code and data are released on\nhttps://github.com/UESTC-GQJ/TKRE."}
{"id": "2505.12651", "pdf": "https://arxiv.org/pdf/2505.12651", "abs": "https://arxiv.org/abs/2505.12651", "authors": ["Sayontan Ghosh", "Mahnaz Koupaee", "Yash Kumar Lal", "Pegah Alipoormolabashi", "Mohammad Saqib Hasan", "Jun Seok Kang", "Niranjan Balasubramanian"], "title": "$\\texttt{DIAMONDs}$: A Dataset for $\\mathbb{D}$ynamic $\\mathbb{I}$nformation $\\mathbb{A}$nd $\\mathbb{M}$ental modeling $\\mathbb{O}$f $\\mathbb{N}$umeric $\\mathbb{D}$iscussions", "categories": ["cs.AI"], "comment": null, "summary": "Understanding multiparty conversations demands robust Theory of Mind (ToM)\ncapabilities, including the ability to track dynamic information, manage\nknowledge asymmetries, and distinguish relevant information across extended\nexchanges. To advance ToM evaluation in such settings, we present a carefully\ndesigned scalable methodology for generating high-quality benchmark\nconversation-question pairs with these characteristics. Using this methodology,\nwe create $\\texttt{DIAMONDs}$, a new conversational QA dataset covering common\nbusiness, financial or other group interactions. In these goal-oriented\nconversations, participants often have to track certain numerical quantities\n(say $\\textit{expected profit}$) of interest that can be derived from other\nvariable quantities (like $\\textit{marketing expenses, expected sales,\nsalary}$, etc.), whose values also change over the course of the conversation.\n$\\texttt{DIAMONDs}$ questions pose simple numerical reasoning problems over\nsuch quantities of interest (e.g., $\\textit{funds required for charity events,\nexpected company profit next quarter}$, etc.) in the context of the information\nexchanged in conversations. This allows for precisely evaluating ToM\ncapabilities for carefully tracking and reasoning over participants' knowledge\nstates.\n  Our evaluation of state-of-the-art language models reveals significant\nchallenges in handling participant-centric reasoning, specifically in\nsituations where participants have false beliefs. Models also struggle with\nconversations containing distractors and show limited ability to identify\nscenarios with insufficient information. These findings highlight current\nmodels' ToM limitations in handling real-world multi-party conversations."}
{"id": "2505.11847", "pdf": "https://arxiv.org/pdf/2505.11847", "abs": "https://arxiv.org/abs/2505.11847", "authors": ["Sizhe Ma", "Katherine A. Flanigan", "Mario Bergés"], "title": "Bridging the Reality Gap in Digital Twins with Context-Aware, Physics-Guided Deep Learning", "categories": ["cs.LG", "cs.RO"], "comment": "Submitted to ASCE Journal of Computing in Civil Engineering", "summary": "Digital twins (DTs) enable powerful predictive analytics, but persistent\ndiscrepancies between simulations and real systems--known as the reality\ngap--undermine their reliability. Coined in robotics, the term now applies to\nDTs, where discrepancies stem from context mismatches, cross-domain\ninteractions, and multi-scale dynamics. Among these, context mismatch is\npressing and underexplored, as DT accuracy depends on capturing operational\ncontext, often only partially observable. However, DTs have a key advantage:\nsimulators can systematically vary contextual factors and explore scenarios\ndifficult or impossible to observe empirically, informing inference and model\nalignment. While sim-to-real transfer like domain adaptation shows promise in\nrobotics, their application to DTs poses two key challenges. First, unlike\none-time policy transfers, DTs require continuous calibration across an asset's\nlifecycle--demanding structured information flow, timely detection of\nout-of-sync states, and integration of historical and new data. Second, DTs\noften perform inverse modeling, inferring latent states or faults from\nobservations that may reflect multiple evolving contexts. These needs strain\npurely data-driven models and risk violating physical consistency. Though some\napproaches preserve validity via reduced-order model, most domain adaptation\ntechniques still lack such constraints. To address this, we propose a Reality\nGap Analysis (RGA) module for DTs that continuously integrates new sensor data,\ndetects misalignments, and recalibrates DTs via a query-response framework. Our\napproach fuses domain-adversarial deep learning with reduced-order simulator\nguidance to improve context inference and preserve physical consistency. We\nillustrate the RGA module in a structural health monitoring case study on a\nsteel truss bridge in Pittsburgh, PA, showing faster calibration and better\nreal-world alignment."}
{"id": "2505.12238", "pdf": "https://arxiv.org/pdf/2505.12238", "abs": "https://arxiv.org/abs/2505.12238", "authors": ["Sriram Selvam", "Anneswa Ghosh"], "title": "PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The memorization of sensitive and personally identifiable information (PII)\nby large language models (LLMs) poses growing privacy risks as models scale and\nare increasingly deployed in real-world applications. Existing efforts to study\nsensitive and PII data memorization and develop mitigation strategies are\nhampered by the absence of comprehensive, realistic, and ethically sourced\ndatasets reflecting the diversity of sensitive information found on the web. We\nintroduce PANORAMA - Profile-based Assemblage for Naturalistic Online\nRepresentation and Attribute Memorization Analysis, a large-scale synthetic\ncorpus of 384,789 samples derived from 9,674 synthetic profiles designed to\nclosely emulate the distribution, variety, and context of PII and sensitive\ndata as it naturally occurs in online environments. Our data generation\npipeline begins with the construction of internally consistent, multi-attribute\nhuman profiles using constrained selection to reflect real-world demographics\nsuch as education, health attributes, financial status, etc. Using a\ncombination of zero-shot prompting and OpenAI o3-mini, we generate diverse\ncontent types - including wiki-style articles, social media posts, forum\ndiscussions, online reviews, comments, and marketplace listings - each\nembedding realistic, contextually appropriate PII and other sensitive\ninformation. We validate the utility of PANORAMA by fine-tuning the Mistral-7B\nmodel on 1x, 5x, 10x, and 25x data replication rates with a subset of data and\nmeasure PII memorization rates - revealing not only consistent increases with\nrepetition but also variation across content types, highlighting PANORAMA's\nability to model how memorization risks differ by context. Our dataset and code\nare publicly available, providing a much-needed resource for privacy risk\nassessment, model auditing, and the development of privacy-preserving LLMs."}
{"id": "2505.12680", "pdf": "https://arxiv.org/pdf/2505.12680", "abs": "https://arxiv.org/abs/2505.12680", "authors": ["Haoyu Zhao", "Yihan Geng", "Shange Tang", "Yong Lin", "Bohan Lyu", "Hongzhou Lin", "Chi Jin", "Sanjeev Arora"], "title": "Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "27 pages", "summary": "LLM-based formal proof assistants (e.g., in Lean) hold great promise for\nautomating mathematical discovery. But beyond syntactic correctness, do these\nsystems truly understand mathematical structure as humans do? We investigate\nthis question through the lens of mathematical inequalities -- a fundamental\ntool across many domains. While modern provers can solve basic inequalities, we\nprobe their ability to handle human-intuitive compositionality. We introduce\nIneq-Comp, a benchmark built from elementary inequalities through systematic\ntransformations, including variable duplication, algebraic rewriting, and\nmulti-step composition. Although these problems remain easy for humans, we find\nthat most provers -- including Goedel, STP, and Kimina-7B -- struggle\nsignificantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly\nbecause it is trained to decompose the problems into sub-problems -- but still\nsuffers a 20\\% performance drop (pass@32). Strikingly, performance remains poor\nfor all models even when formal proofs of the constituent parts are provided in\ncontext, revealing that the source of weakness is indeed in compositional\nreasoning. Our results expose a persisting gap between the generalization\nbehavior of current AI provers and human mathematical intuition."}
{"id": "2505.11862", "pdf": "https://arxiv.org/pdf/2505.11862", "abs": "https://arxiv.org/abs/2505.11862", "authors": ["Kalyan Cherukuri", "Aarav Lala", "Yash Yardi"], "title": "Q-Policy: Quantum-Enhanced Policy Evaluation for Scalable Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": null, "summary": "We propose Q-Policy, a hybrid quantum-classical reinforcement learning (RL)\nframework that mathematically accelerates policy evaluation and optimization by\nexploiting quantum computing primitives. Q-Policy encodes value functions in\nquantum superposition, enabling simultaneous evaluation of multiple\nstate-action pairs via amplitude encoding and quantum parallelism. We introduce\na quantum-enhanced policy iteration algorithm with provable polynomial\nreductions in sample complexity for the evaluation step, under standard\nassumptions. To demonstrate the technical feasibility and theoretical soundness\nof our approach, we validate Q-Policy on classical emulations of small discrete\ncontrol tasks. Due to current hardware and simulation limitations, our\nexperiments focus on showcasing proof-of-concept behavior rather than\nlarge-scale empirical evaluation. Our results support the potential of Q-Policy\nas a theoretical foundation for scalable RL on future quantum devices,\naddressing RL scalability challenges beyond classical approaches."}
{"id": "2505.12244", "pdf": "https://arxiv.org/pdf/2505.12244", "abs": "https://arxiv.org/abs/2505.12244", "authors": ["Haojin Wang", "Zining Zhu", "Freda Shi"], "title": "Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce", "categories": ["cs.CL"], "comment": null, "summary": "Autoregressive neural language models (LMs) generate a probability\ndistribution over tokens at each time step given a prompt. In this work, we\nattempt to systematically understand the probability distributions that LMs can\nproduce, showing that some distributions are significantly harder to elicit\nthan others. Specifically, for any target next-token distribution over the\nvocabulary, we attempt to find a prompt that induces the LM to output a\ndistribution as close as possible to the target, using either soft or hard\ngradient-based prompt tuning. We find that (1) in general, distributions with\nvery low or very high entropy are easier to approximate than those with\nmoderate entropy; (2) among distributions with the same entropy, those\ncontaining ''outlier tokens'' are easier to approximate; (3) target\ndistributions generated by LMs -- even LMs with different tokenizers -- are\neasier to approximate than randomly chosen targets. These results offer\ninsights into the expressiveness of LMs and the challenges of using them as\nprobability distribution proposers."}
{"id": "2505.12692", "pdf": "https://arxiv.org/pdf/2505.12692", "abs": "https://arxiv.org/abs/2505.12692", "authors": ["Ziwei Xu", "Udit Sanghi", "Mohan Kankanhalli"], "title": "Bullying the Machine: How Personas Increase LLM Vulnerability", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in interactions where\nthey are prompted to adopt personas. This paper investigates whether such\npersona conditioning affects model safety under bullying, an adversarial\nmanipulation that applies psychological pressures in order to force the victim\nto comply to the attacker. We introduce a simulation framework in which an\nattacker LLM engages a victim LLM using psychologically grounded bullying\ntactics, while the victim adopts personas aligned with the Big Five personality\ntraits. Experiments using multiple open-source LLMs and a wide range of\nadversarial goals reveal that certain persona configurations -- such as\nweakened agreeableness or conscientiousness -- significantly increase victim's\nsusceptibility to unsafe outputs. Bullying tactics involving emotional or\nsarcastic manipulation, such as gaslighting and ridicule, are particularly\neffective. These findings suggest that persona-driven interaction introduces a\nnovel vector for safety risks in LLMs and highlight the need for persona-aware\nsafety evaluation and alignment strategies."}
{"id": "2505.11864", "pdf": "https://arxiv.org/pdf/2505.11864", "abs": "https://arxiv.org/abs/2505.11864", "authors": ["Kalyan Cherukuri", "Aarav Lala"], "title": "Learning Pareto-Optimal Rewards from Noisy Preferences: A Framework for Multi-Objective Inverse Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CG"], "comment": null, "summary": "As generative agents become increasingly capable, alignment of their behavior\nwith complex human values remains a fundamental challenge. Existing approaches\noften simplify human intent through reduction to a scalar reward, overlooking\nthe multi-faceted nature of human feedback. In this work, we introduce a\ntheoretical framework for preference-based Multi-Objective Inverse\nReinforcement Learning (MO-IRL), where human preferences are modeled as latent\nvector-valued reward functions. We formalize the problem of recovering a\nPareto-optimal reward representation from noisy preference queries and\nestablish conditions for identifying the underlying multi-objective structure.\nWe derive tight sample complexity bounds for recovering\n$\\epsilon$-approximations of the Pareto front and introduce a regret\nformulation to quantify suboptimality in this multi-objective setting.\nFurthermore, we propose a provably convergent algorithm for policy optimization\nusing preference-inferred reward cones. Our results bridge the gap between\npractical alignment techniques and theoretical guarantees, providing a\nprincipled foundation for learning aligned behaviors in a high-dimension and\nvalue-pluralistic environment."}
{"id": "2505.12250", "pdf": "https://arxiv.org/pdf/2505.12250", "abs": "https://arxiv.org/abs/2505.12250", "authors": ["Chi Zhang", "Huaping Zhong", "Hongtao Li", "Chengliang Chai", "Jiawei Hong", "Yuhao Deng", "Jiacheng Wang", "Tian Tan", "Yizhou Yan", "Jiantao Qiu", "Ye Yuan", "Guoren Wang", "Conghui He", "Lei Cao"], "title": "Not All Documents Are What You Need for Extracting Instruction Tuning Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction tuning improves the performance of large language models (LLMs),\nbut it heavily relies on high-quality training data. Recently, LLMs have been\nused to synthesize instruction data using seed question-answer (QA) pairs.\nHowever, these synthesized instructions often lack diversity and tend to be\nsimilar to the input seeds, limiting their applicability in real-world\nscenarios. To address this, we propose extracting instruction tuning data from\nweb corpora that contain rich and diverse knowledge. A naive solution is to\nretrieve domain-specific documents and extract all QA pairs from them, but this\nfaces two key challenges: (1) extracting all QA pairs using LLMs is\nprohibitively expensive, and (2) many extracted QA pairs may be irrelevant to\nthe downstream tasks, potentially degrading model performance. To tackle these\nissues, we introduce EQUAL, an effective and scalable data extraction framework\nthat iteratively alternates between document selection and high-quality QA pair\nextraction to enhance instruction tuning. EQUAL first clusters the document\ncorpus based on embeddings derived from contrastive learning, then uses a\nmulti-armed bandit strategy to efficiently identify clusters that are likely to\ncontain valuable QA pairs. This iterative approach significantly reduces\ncomputational cost while boosting model performance. Experiments on\nAutoMathText and StackOverflow across four downstream tasks show that EQUAL\nreduces computational costs by 5-10x and improves accuracy by 2.5 percent on\nLLaMA-3.1-8B and Mistral-7B"}
{"id": "2505.12731", "pdf": "https://arxiv.org/pdf/2505.12731", "abs": "https://arxiv.org/abs/2505.12731", "authors": ["Jie Ou", "Jinyu Guo", "Shuaihong Jiang", "Zhaokun Wang", "Libo Qin", "Shunyu Yao", "Wenhong Tian"], "title": "Accelerating Adaptive Retrieval Augmented Generation via Instruction-Driven Representation Reduction of Retrieval Overlaps", "categories": ["cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality."}
{"id": "2505.11875", "pdf": "https://arxiv.org/pdf/2505.11875", "abs": "https://arxiv.org/abs/2505.11875", "authors": ["Chi-Min Chan", "Chunpu Xu", "Jiaming Ji", "Zhen Ye", "Pengcheng Wen", "Chunyang Jiang", "Yaodong Yang", "Wei Xue", "Sirui Han", "Yike Guo"], "title": "J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge", "categories": ["cs.LG", "cs.CL"], "comment": "33 pages, 27 figures", "summary": "The current focus of AI research is shifting from emphasizing model training\ntowards enhancing evaluation quality, a transition that is crucial for driving\nfurther advancements in AI systems. Traditional evaluation methods typically\nrely on reward models assigning scalar preference scores to outputs. Although\neffective, such approaches lack interpretability, leaving users often uncertain\nabout why a reward model rates a particular response as high or low. The advent\nof LLM-as-a-Judge provides a more scalable and interpretable method of\nsupervision, offering insights into the decision-making process. Moreover, with\nthe emergence of large reasoning models, which consume more tokens for deeper\nthinking and answer refinement, scaling test-time computation in the\nLLM-as-a-Judge paradigm presents an avenue for further boosting performance and\nproviding more interpretability through reasoning traces. In this paper, we\nintroduce $\\textbf{J1-7B}$, which is first supervised fine-tuned on\nreflection-enhanced datasets collected via rejection-sampling and subsequently\ntrained using Reinforcement Learning (RL) with verifiable rewards. At inference\ntime, we apply Simple Test-Time Scaling (STTS) strategies for additional\nperformance improvement. Experimental results demonstrate that $\\textbf{J1-7B}$\nsurpasses the previous state-of-the-art LLM-as-a-Judge by $ \\textbf{4.8}$\\% and\nexhibits a $ \\textbf{5.1}$\\% stronger scaling trend under STTS. Additionally,\nwe present three key findings: (1) Existing LLM-as-a-Judge does not inherently\nexhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced\ndatasets continues to demonstrate similarly weak scaling behavior. (3)\nSignificant scaling trend emerges primarily during the RL phase, suggesting\nthat effective STTS capability is acquired predominantly through RL training."}
{"id": "2505.12259", "pdf": "https://arxiv.org/pdf/2505.12259", "abs": "https://arxiv.org/abs/2505.12259", "authors": ["Yuhang Zhou", "Xutian Chen", "Yixin Cao", "Yuchen Ni", "Yu He", "Siyu Tian", "Xiang Liu", "Jian Zhang", "Chuanjun Ji", "Guangnan Ye", "Xipeng Qiu"], "title": "Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches", "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has outpaced the development\nof effective evaluation methods. Traditional benchmarks rely on task-specific\nmetrics and static datasets, which often suffer from fairness issues, limited\nscalability, and contamination risks. In this paper, we introduce Teach2Eval,\nan indirect evaluation framework inspired by the Feynman Technique. Instead of\ndirectly testing LLMs on predefined tasks, our method evaluates a model's\nmultiple abilities to teach weaker student models to perform tasks effectively.\nBy converting open-ended tasks into standardized multiple-choice questions\n(MCQs) through teacher-generated feedback, Teach2Eval enables scalable,\nautomated, and multi-dimensional assessment. Our approach not only avoids data\nleakage and memorization but also captures a broad range of cognitive abilities\nthat are orthogonal to current benchmarks. Experimental results across 26\nleading LLMs show strong alignment with existing human and model-based dynamic\nrankings, while offering additional interpretability for training guidance."}
{"id": "2505.12741", "pdf": "https://arxiv.org/pdf/2505.12741", "abs": "https://arxiv.org/abs/2505.12741", "authors": ["Shiguang Wu", "Yaqing Wang", "Quanming Yao"], "title": "Dense Communication between Language Models", "categories": ["cs.AI"], "comment": null, "summary": "As higher-level intelligence emerges from the combination of modular\ncomponents with lower-level intelligence, many works combines Large Language\nModels (LLMs) for collective intelligence. Such combination is achieved by\nbuilding communications among LLMs. While current systems primarily facilitate\nsuch communication through natural language, this paper proposes a novel\nparadigm of direct dense vector communication between LLMs. Our approach\neliminates the unnecessary embedding and de-embedding steps when LLM interact\nwith another, enabling more efficient information transfer, fully\ndifferentiable optimization pathways, and exploration of capabilities beyond\nhuman heuristics. We use such stripped LLMs as vertexes and optimizable seq2seq\nmodules as edges to construct LMNet, with similar structure as MLPs. By\nutilizing smaller pre-trained LLMs as vertexes, we train a LMNet that achieves\ncomparable performance with LLMs in similar size with only less than 0.1%\ntraining cost. This offers a new perspective on scaling for general\nintelligence rather than training a monolithic LLM from scratch. Besides, the\nproposed method can be used for other applications, like customizing LLM with\nlimited data, showing its versatility."}
{"id": "2505.11878", "pdf": "https://arxiv.org/pdf/2505.11878", "abs": "https://arxiv.org/abs/2505.11878", "authors": ["Yifan Dai", "Xuanbai Ren", "Tengfei Ma", "Qipeng Yan", "Yiping Liu", "Yuansheng Liu", "Xiangxiang Zeng"], "title": "AdaptMol: Adaptive Fusion from Sequence String to Topological Structure for Few-shot Drug Discovery", "categories": ["cs.LG", "cs.AI", "q-bio.MN", "J.3; I.2.7"], "comment": "15 pages, 6 figures", "summary": "Accurate molecular property prediction (MPP) is a critical step in modern\ndrug development. However, the scarcity of experimental validation data poses a\nsignificant challenge to AI-driven research paradigms. Under few-shot learning\nscenarios, the quality of molecular representations directly dictates the\ntheoretical upper limit of model performance. We present AdaptMol, a\nprototypical network integrating Adaptive multimodal fusion for Molecular\nrepresentation. This framework employs a dual-level attention mechanism to\ndynamically integrate global and local molecular features derived from two\nmodalities: SMILES sequences and molecular graphs. (1) At the local level,\nstructural features such as atomic interactions and substructures are extracted\nfrom molecular graphs, emphasizing fine-grained topological information; (2) At\nthe global level, the SMILES sequence provides a holistic representation of the\nmolecule. To validate the necessity of multimodal adaptive fusion, we propose\nan interpretable approach based on identifying molecular active substructures\nto demonstrate that multimodal adaptive fusion can efficiently represent\nmolecules. Extensive experiments on three commonly used benchmarks under 5-shot\nand 10-shot settings demonstrate that AdaptMol achieves state-of-the-art\nperformance in most cases. The rationale-extracted method guides the fusion of\ntwo modalities and highlights the importance of both modalities."}
{"id": "2505.12265", "pdf": "https://arxiv.org/pdf/2505.12265", "abs": "https://arxiv.org/abs/2505.12265", "authors": ["Chengwei Qin", "Wenxuan Zhou", "Karthik Abinav Sankararaman", "Nanshu Wang", "Tengyu Xu", "Alexander Radovic", "Eryk Helenowski", "Arya Talebzadeh", "Aditya Tayade", "Sinong Wang", "Shafiq Joty", "Han Fang", "Hao Ma"], "title": "Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation", "categories": ["cs.CL"], "comment": null, "summary": "Hallucination, the generation of factually incorrect information, remains a\nsignificant challenge for large language models (LLMs), especially in\nopen-domain long-form generation. Existing approaches for detecting\nhallucination in long-form tasks either focus on limited domains or rely\nheavily on external fact-checking tools, which may not always be available.\n  In this work, we systematically investigate reference-free hallucination\ndetection in open-domain long-form responses. Our findings reveal that internal\nstates (e.g., model's output probability and entropy) alone are insufficient\nfor reliably (i.e., better than random guessing) distinguishing between factual\nand hallucinated content. To enhance detection, we explore various existing\napproaches, including prompting-based methods, probing, and fine-tuning, with\nfine-tuning proving the most effective. To further improve the accuracy, we\nintroduce a new paradigm, named RATE-FT, that augments fine-tuning with an\nauxiliary task for the model to jointly learn with the main task of\nhallucination detection. With extensive experiments and analysis using a\nvariety of model families & datasets, we demonstrate the effectiveness and\ngeneralizability of our method, e.g., +3% over general fine-tuning methods on\nLongFact."}
{"id": "2505.12744", "pdf": "https://arxiv.org/pdf/2505.12744", "abs": "https://arxiv.org/abs/2505.12744", "authors": ["Weiliang Tang", "Dong Jing", "Jia-Hui Pan", "Zhiwu Lu", "Yun-Hui Liu", "Li Erran Li", "Mingyu Ding", "Chi-Wing Fu"], "title": "Incentivizing Multimodal Reasoning in Large Models for Direct Robot Manipulation", "categories": ["cs.AI"], "comment": "17 pages, 16 figures", "summary": "Recent Large Multimodal Models have demonstrated remarkable reasoning\ncapabilities, especially in solving complex mathematical problems and realizing\naccurate spatial perception. Our key insight is that these emerging abilities\ncan naturally extend to robotic manipulation by enabling LMMs to directly infer\nthe next goal in language via reasoning, rather than relying on a separate\naction head. However, this paradigm meets two main challenges: i) How to make\nLMMs understand the spatial action space, and ii) How to fully exploit the\nreasoning capacity of LMMs in solving these tasks. To tackle the former\nchallenge, we propose a novel task formulation, which inputs the current states\nof object parts and the gripper, and reformulates rotation by a new axis\nrepresentation instead of traditional Euler angles. This representation is more\ncompatible with spatial reasoning and easier to interpret within a unified\nlanguage space. For the latter challenge, we design a pipeline to utilize\ncutting-edge LMMs to generate a small but high-quality reasoning dataset of\nmulti-round dialogues that successfully solve manipulation tasks for supervised\nfine-tuning. Then, we perform reinforcement learning by trial-and-error\ninteractions in simulation to further enhance the model's reasoning abilities\nfor robotic manipulation. Our resulting reasoning model built upon a 7B\nbackbone, named ReasonManip, demonstrates three notable advantages driven by\nits system-2 level reasoning capabilities: i) exceptional generalizability to\nout-of-distribution environments, objects, and tasks; ii) inherent sim-to-real\ntransfer ability enabled by the unified language representation shared across\ndomains; iii) transparent interpretability connecting high-level reasoning and\nlow-level control. Extensive experiments demonstrate the effectiveness of the\nproposed paradigm and its potential to advance LMM-driven robotic manipulation."}
{"id": "2505.11883", "pdf": "https://arxiv.org/pdf/2505.11883", "abs": "https://arxiv.org/abs/2505.11883", "authors": ["Zihuan Qiu", "Yi Xu", "Chiyuan He", "Fanman Meng", "Linfeng Xu", "Qingbo Wu", "Hongliang Li"], "title": "MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Continual model merging integrates independently fine-tuned models\nsequentially without access to original training data, providing a scalable and\nefficient solution to continual learning. However, current methods still face\ncritical challenges, notably parameter interference among tasks and limited\nadaptability to evolving test distributions. The former causes catastrophic\nforgetting of integrated tasks, while the latter hinders effective adaptation\nto new tasks. To address these, we propose MINGLE, a novel framework for\ntest-time continual model merging, which leverages test-time adaptation using a\nsmall set of unlabeled test samples from the current task to dynamically guide\nthe merging process. MINGLE employs a mixture-of-experts architecture composed\nof parameter-efficient, low-rank experts, enabling efficient adaptation and\nimproving robustness to distribution shifts. To mitigate catastrophic\nforgetting, we propose Null-Space Constrained Gating, which restricts gating\nupdates to subspaces orthogonal to prior task representations. This suppresses\nactivations on old task inputs and preserves model behavior on past tasks. To\nfurther balance stability and adaptability, we design an Adaptive Relaxation\nStrategy, which dynamically adjusts the constraint strength based on\ninterference signals captured during test-time adaptation. Extensive\nexperiments on standard continual merging benchmarks demonstrate that MINGLE\nachieves robust generalization, reduces forgetting significantly, and\nconsistently surpasses previous state-of-the-art methods by 7-9\\% on average\nacross diverse task orders."}
{"id": "2505.12268", "pdf": "https://arxiv.org/pdf/2505.12268", "abs": "https://arxiv.org/abs/2505.12268", "authors": ["Pratim Chowdhary"], "title": "$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Understanding which neural components drive specific capabilities in\nmid-sized language models ($\\leq$10B parameters) remains a key challenge. We\nintroduce the $(\\bm{K}, \\epsilon)$-Minimum Sufficient Head Circuit ($K$-MSHC),\na methodology to identify minimal sets of attention heads crucial for\nclassification tasks as well as Search-K-MSHC, an efficient algorithm for\ndiscovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B,\nwe analyze three syntactic task families: grammar acceptability, arithmetic\nverification, and arithmetic word problems. Our findings reveal distinct\ntask-specific head circuits, with grammar tasks predominantly utilizing early\nlayers, word problems showing pronounced activity in both shallow and deep\nregions, and arithmetic verification demonstrating a more distributed pattern\nacross the network. We discover non-linear circuit overlap patterns, where\ndifferent task pairs share computational components at varying levels of\nimportance. While grammar and arithmetic share many \"weak\" heads, arithmetic\nand word problems share more consistently critical \"strong\" heads. Importantly,\nwe find that each task maintains dedicated \"super-heads\" with minimal\ncross-task overlap, suggesting that syntactic and numerical competencies emerge\nfrom specialized yet partially reusable head circuits."}
{"id": "2505.12746", "pdf": "https://arxiv.org/pdf/2505.12746", "abs": "https://arxiv.org/abs/2505.12746", "authors": ["Haruka Asanuma", "Naoko Koide-Majima", "Ken Nakamura", "Takato Horii", "Shinji Nishimoto", "Masafumi Oizumi"], "title": "Correspondence of high-dimensional emotion structures elicited by video clips between humans and Multimodal LLMs", "categories": ["cs.AI", "I.2.7; I.2.10; I.5.1"], "comment": "25 pages, 7 figures", "summary": "Recent studies have revealed that human emotions exhibit a high-dimensional,\ncomplex structure. A full capturing of this complexity requires new approaches,\nas conventional models that disregard high dimensionality risk overlooking key\nnuances of human emotions. Here, we examined the extent to which the latest\ngeneration of rapidly evolving Multimodal Large Language Models (MLLMs) capture\nthese high-dimensional, intricate emotion structures, including capabilities\nand limitations. Specifically, we compared self-reported emotion ratings from\nparticipants watching videos with model-generated estimates (e.g., Gemini or\nGPT). We evaluated performance not only at the individual video level but also\nfrom emotion structures that account for inter-video relationships. At the\nlevel of simple correlation between emotion structures, our results\ndemonstrated strong similarity between human and model-inferred emotion\nstructures. To further explore whether the similarity between humans and models\nis at the signle item level or the coarse-categorical level, we applied Gromov\nWasserstein Optimal Transport. We found that although performance was not\nnecessarily high at the strict, single-item level, performance across video\ncategories that elicit similar emotions was substantial, indicating that the\nmodel could infer human emotional experiences at the category level. Our\nresults suggest that current state-of-the-art MLLMs broadly capture the complex\nhigh-dimensional emotion structures at the category level, as well as their\napparent limitations in accurately capturing entire structures at the\nsingle-item level."}
{"id": "2505.11892", "pdf": "https://arxiv.org/pdf/2505.11892", "abs": "https://arxiv.org/abs/2505.11892", "authors": ["Josh Alman", "Zhao Song"], "title": "Fast RoPE Attention: Combining the Polynomial Method and Fast Fourier Transform", "categories": ["cs.LG", "cs.DS"], "comment": null, "summary": "The transformer architecture has been widely applied to many machine learning\ntasks. A main bottleneck in the time to perform transformer computations is a\ntask called attention computation. [Alman and Song, NeurIPS 2023] have shown\nthat in the bounded entry regime, there is an almost linear time algorithm to\napproximate the attention computation. They also proved that the bounded entry\nassumption is necessary for a fast algorithm assuming the popular Strong\nExponential Time Hypothesis.\n  A new version of transformer which uses position embeddings has recently been\nvery successful. At a high level, position embedding enables the model to\ncapture the correlations between tokens while taking into account their\nposition in the sequence. Perhaps the most popular and effective version is\nRotary Position Embedding (RoPE), which was proposed by [Su, Lu, Pan, Murtadha,\nWen, and Liu, Neurocomputing 2024].\n  A main downside of RoPE is that it complicates the attention computation\nproblem, so that previous techniques for designing almost linear time\nalgorithms no longer seem to work. In this paper, we show how to overcome this\nissue, and give a new algorithm to compute the RoPE attention in almost linear\ntime in the bounded entry regime. (Again, known lower bounds imply that bounded\nentries are necessary.) Our new algorithm combines two techniques in a novel\nway: the polynomial method, which was used in prior fast attention algorithms,\nand the Fast Fourier Transform."}
{"id": "2505.12273", "pdf": "https://arxiv.org/pdf/2505.12273", "abs": "https://arxiv.org/abs/2505.12273", "authors": ["Md. Atiqur Rahman", "Sabrina Islam", "Mushfiqul Haque Omi"], "title": "LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating machine translation (MT) for low-resource languages poses a\npersistent challenge, primarily due to the limited availability of high quality\nreference translations. This issue is further exacerbated in languages with\nmultiple dialects, where linguistic diversity and data scarcity hinder robust\nevaluation. Large Language Models (LLMs) present a promising solution through\nreference-free evaluation techniques; however, their effectiveness diminishes\nin the absence of dialect-specific context and tailored guidance. In this work,\nwe propose a comprehensive framework that enhances LLM-based MT evaluation\nusing a dialect guided approach. We extend the ONUBAD dataset by incorporating\nSylheti-English sentence pairs, corresponding machine translations, and Direct\nAssessment (DA) scores annotated by native speakers. To address the vocabulary\ngap, we augment the tokenizer vocabulary with dialect-specific terms. We\nfurther introduce a regression head to enable scalar score prediction and\ndesign a dialect-guided (DG) prompting strategy. Our evaluation across multiple\nLLMs shows that the proposed pipeline consistently outperforms existing\nmethods, achieving the highest gain of +0.1083 in Spearman correlation, along\nwith improvements across other evaluation settings. The dataset and the code\nare available at https://github.com/180041123-Atiq/MTEonLowResourceLanguage."}
{"id": "2505.12762", "pdf": "https://arxiv.org/pdf/2505.12762", "abs": "https://arxiv.org/abs/2505.12762", "authors": ["Chenlin Ming", "Chendi Qu", "Mengzhang Cai", "Qizhi Pei", "Zhuoshi Pan", "Yu Li", "Xiaoming Duan", "Lijun Wu", "Conghui He"], "title": "IDEAL: Data Equilibrium Adaptation for Multi-Capability Language Model Alignment", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive performance through\nSupervised Fine-tuning (SFT) on diverse instructional datasets. When training\non multiple capabilities simultaneously, the mixture training dataset, governed\nby volumes of data from different domains, is a critical factor that directly\nimpacts the final model's performance. Unlike many studies that focus on\nenhancing the quality of training datasets through data selection methods, few\nworks explore the intricate relationship between the compositional quantity of\nmixture training datasets and the emergent capabilities of LLMs. Given the\navailability of a high-quality multi-domain training dataset, understanding the\nimpact of data from each domain on the model's overall capabilities is crucial\nfor preparing SFT data and training a well-balanced model that performs\neffectively across diverse domains. In this work, we introduce IDEAL, an\ninnovative data equilibrium adaptation framework designed to effectively\noptimize volumes of data from different domains within mixture SFT datasets,\nthereby enhancing the model's alignment and performance across multiple\ncapabilities. IDEAL employs a gradient-based approach to iteratively refine the\ntraining data distribution, dynamically adjusting the volumes of\ndomain-specific data based on their impact on downstream task performance. By\nleveraging this adaptive mechanism, IDEAL ensures a balanced dataset\ncomposition, enabling the model to achieve robust generalization and consistent\nproficiency across diverse tasks. Experiments across different capabilities\ndemonstrate that IDEAL outperforms conventional uniform data allocation\nstrategies, achieving a comprehensive improvement of approximately 7% in\nmulti-task evaluation scores."}
{"id": "2505.11896", "pdf": "https://arxiv.org/pdf/2505.11896", "abs": "https://arxiv.org/abs/2505.11896", "authors": ["Chenwei Lou", "Zewei Sun", "Xinnian Liang", "Meng Qu", "Wei Shen", "Wenqi Wang", "Yuntao Li", "Qingping Yang", "Shuangzhi Wu"], "title": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities but\noften face challenges with tasks requiring sophisticated reasoning. While\nChain-of-Thought (CoT) prompting significantly enhances reasoning, it\nindiscriminately generates lengthy reasoning steps for all queries, leading to\nsubstantial computational costs and inefficiency, especially for simpler\ninputs. To address this critical issue, we introduce AdaCoT (Adaptive\nChain-of-Thought), a novel framework enabling LLMs to adaptively decide when to\ninvoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem\nthat seeks to balance model performance with the costs associated with CoT\ninvocation (both frequency and computational overhead). We propose a\nreinforcement learning (RL) based method, specifically utilizing Proximal\nPolicy Optimization (PPO), to dynamically control the CoT triggering decision\nboundary by adjusting penalty coefficients, thereby allowing the model to\ndetermine CoT necessity based on implicit query complexity. A key technical\ncontribution is Selective Loss Masking (SLM), designed to counteract decision\nboundary collapse during multi-stage RL training, ensuring robust and stable\nadaptive triggering. Experimental results demonstrate that AdaCoT successfully\nnavigates the Pareto frontier, achieving substantial reductions in CoT usage\nfor queries not requiring elaborate reasoning. For instance, on our production\ntraffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\\% and\ndecreased average response tokens by 69.06%, while maintaining high performance\non complex tasks."}
{"id": "2505.12287", "pdf": "https://arxiv.org/pdf/2505.12287", "abs": "https://arxiv.org/abs/2505.12287", "authors": ["Linghan Huang", "Haolin Jin", "Zhaoge Bi", "Pengyue Yang", "Peizhou Zhao", "Taozhao Chen", "Xiongfei Wu", "Lei Ma", "Huaming Chen"], "title": "The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have seen widespread applications across various\ndomains, yet remain vulnerable to adversarial prompt injections. While most\nexisting research on jailbreak attacks and hallucination phenomena has focused\nprimarily on open-source models, we investigate the frontier of closed-source\nLLMs under multilingual attack scenarios. We present a first-of-its-kind\nintegrated adversarial framework that leverages diverse attack techniques to\nsystematically evaluate frontier proprietary solutions, including GPT-4o,\nDeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories\nof security contents in both English and Chinese, generating 38,400 responses\nacross 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as\nthe quantitative metric to assess performance from three dimensions: prompt\ndesign, model architecture, and language environment. Our findings suggest that\nQwen-Max is the most vulnerable, while GPT-4o shows the strongest defense.\nNotably, prompts in Chinese consistently yield higher ASRs than their English\ncounterparts, and our novel Two-Sides attack technique proves to be the most\neffective across all models. This work highlights a dire need for\nlanguage-aware alignment and robust cross-lingual defenses in LLMs, and we hope\nit will inspire researchers, developers, and policymakers toward more robust\nand inclusive AI systems."}
{"id": "2505.12767", "pdf": "https://arxiv.org/pdf/2505.12767", "abs": "https://arxiv.org/abs/2505.12767", "authors": ["Danqing Chen", "Tobias Ladner", "Ahmed Rayen Mhadhbi", "Matthias Althoff"], "title": "Language Models That Walk the Talk: A Framework for Formal Fairness Certificates", "categories": ["cs.AI"], "comment": null, "summary": "As large language models become integral to high-stakes applications,\nensuring their robustness and fairness is critical. Despite their success,\nlarge language models remain vulnerable to adversarial attacks, where small\nperturbations, such as synonym substitutions, can alter model predictions,\nposing risks in fairness-critical areas, such as gender bias mitigation, and\nsafety-critical areas, such as toxicity detection. While formal verification\nhas been explored for neural networks, its application to large language models\nremains limited. This work presents a holistic verification framework to\ncertify the robustness of transformer-based language models, with a focus on\nensuring gender fairness and consistent outputs across different gender-related\nterms. Furthermore, we extend this methodology to toxicity detection, offering\nformal guarantees that adversarially manipulated toxic inputs are consistently\ndetected and appropriately censored, thereby ensuring the reliability of\nmoderation systems. By formalizing robustness within the embedding space, this\nwork strengthens the reliability of language models in ethical AI deployment\nand content moderation."}
{"id": "2505.11902", "pdf": "https://arxiv.org/pdf/2505.11902", "abs": "https://arxiv.org/abs/2505.11902", "authors": ["Jiang You", "Xiaozhen Wang", "Arben Cela"], "title": "Dynamic Perturbed Adaptive Method for Infinite Task-Conflicting Time Series", "categories": ["cs.LG"], "comment": null, "summary": "We formulate time series tasks as input-output mappings under varying\nobjectives, where the same input may yield different outputs. This challenges a\nmodel's generalization and adaptability. To study this, we construct a\nsynthetic dataset with numerous conflicting subtasks to evaluate adaptation\nunder frequent task shifts. Existing static models consistently fail in such\nsettings. We propose a dynamic perturbed adaptive method based on a\ntrunk-branch architecture, where the trunk evolves slowly to capture long-term\nstructure, and branch modules are re-initialized and updated for each task.\nThis enables continual test-time adaptation and cross-task transfer without\nrelying on explicit task labels. Theoretically, we show that this architecture\nhas strictly higher functional expressivity than static models and LoRA. We\nalso establish exponential convergence of branch adaptation under the\nPolyak-Lojasiewicz condition. Experiments demonstrate that our method\nsignificantly outperforms competitive baselines in complex and conflicting task\nenvironments, exhibiting fast adaptation and progressive learning capabilities."}
{"id": "2505.12299", "pdf": "https://arxiv.org/pdf/2505.12299", "abs": "https://arxiv.org/abs/2505.12299", "authors": ["Kun Huang", "Weikai Xu", "Yuxuan Liu", "Quandong Wang", "Pengzhi Gao", "Wei Liu", "Jian Luan", "Bin Wang", "Bo An"], "title": "Enhance Mobile Agents Thinking Process Via Iterative Preference Learning", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 8 figures, 7 tables", "summary": "The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to\nimprove the reasoning performance of VLM-based mobile agents in GUI tasks.\nHowever, the scarcity of diverse CoaT trajectories limits the expressiveness\nand generalization ability of such agents. While self-training is commonly\nemployed to address data scarcity, existing approaches either overlook the\ncorrectness of intermediate reasoning steps or depend on expensive\nprocess-level annotations to construct process reward models (PRM). To address\nthe above problems, we propose an Iterative Preference Learning (IPL) that\nconstructs a CoaT-tree through interative sampling, scores leaf nodes using\nrule-based reward, and backpropagates feedback to derive Thinking-level Direct\nPreference Optimization (T-DPO) pairs. To prevent overfitting during warm-up\nsupervised fine-tuning, we further introduce a three-stage instruction\nevolution, which leverages GPT-4o to generate diverse Q\\&A pairs based on real\nmobile UI screenshots, enhancing both generality and layout understanding.\nExperiments on three standard Mobile GUI-agent benchmarks demonstrate that our\nagent MobileIPL outperforms strong baselines, including continual pretraining\nmodels such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance\nacross three standard Mobile GUI-Agents benchmarks and shows strong\ngeneralization to out-of-domain scenarios."}
{"id": "2505.12788", "pdf": "https://arxiv.org/pdf/2505.12788", "abs": "https://arxiv.org/abs/2505.12788", "authors": ["Zhongni Hou", "Miao Su", "Xiaolong Jin", "Zixuan Li", "Long Bai", "Jiafeng Guo", "Xueqi Cheng"], "title": "Mixture Policy based Multi-Hop Reasoning over N-tuple Temporal Knowledge Graphs", "categories": ["cs.AI"], "comment": null, "summary": "Temporal Knowledge Graphs (TKGs), which utilize quadruples in the form of\n(subject, predicate, object, timestamp) to describe temporal facts, have\nattracted extensive attention. N-tuple TKGs (N-TKGs) further extend traditional\nTKGs by utilizing n-tuples to incorporate auxiliary elements alongside core\nelements (i.e., subject, predicate, and object) of facts, so as to represent\nthem in a more fine-grained manner. Reasoning over N-TKGs aims to predict\npotential future facts based on historical ones. However, existing N-TKG\nreasoning methods often lack explainability due to their black-box nature.\nTherefore, we introduce a new Reinforcement Learning-based method, named\nMT-Path, which leverages the temporal information to traverse historical\nn-tuples and construct a temporal reasoning path. Specifically, in order to\nintegrate the information encapsulated within n-tuples, i.e., the\nentity-irrelevant information within the predicate, the information about core\nelements, and the complete information about the entire n-tuples, MT-Path\nutilizes a mixture policy-driven action selector, which bases on three\nlow-level policies, namely, the predicate-focused policy, the\ncore-element-focused policy and the whole-fact-focused policy. Further, MT-Path\nutilizes an auxiliary element-aware GCN to capture the rich semantic\ndependencies among facts, thereby enabling the agent to gain a deep\nunderstanding of each n-tuple. Experimental results demonstrate the\neffectiveness and the explainability of MT-Path."}
{"id": "2505.11904", "pdf": "https://arxiv.org/pdf/2505.11904", "abs": "https://arxiv.org/abs/2505.11904", "authors": ["Louis Mahon", "Mirella Lapata"], "title": "K*-Means: A Parameter-free Clustering Algorithm", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Clustering is a widely used and powerful machine learning technique, but its\neffectiveness is often limited by the need to specify the number of clusters,\nk, or by relying on thresholds that implicitly determine k. We introduce\nk*-means, a novel clustering algorithm that eliminates the need to set k or any\nother parameters. Instead, it uses the minimum description length principle to\nautomatically determine the optimal number of clusters, k*, by splitting and\nmerging clusters while also optimising the standard k-means objective. We prove\nthat k*-means is guaranteed to converge and demonstrate experimentally that it\nsignificantly outperforms existing methods in scenarios where k is unknown. We\nalso show that it is accurate in estimating k, and that empirically its runtime\nis competitive with existing methods, and scales well with dataset size."}
{"id": "2505.12300", "pdf": "https://arxiv.org/pdf/2505.12300", "abs": "https://arxiv.org/abs/2505.12300", "authors": ["Weixuan Wang", "Minghao Wu", "Barry Haddow", "Alexandra Birch"], "title": "HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning large language models (LLMs) on a mixture of diverse datasets\nposes challenges due to data imbalance and heterogeneity. Existing methods\noften address these issues across datasets (globally) but overlook the\nimbalance and heterogeneity within individual datasets (locally), which limits\ntheir effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a\nnovel method that enables LLMs to autonomously adjust data allocation during\nfine-tuning both across datasets (globally) and within each individual dataset\n(locally). HBO employs a bilevel optimization strategy with two types of\nactors: a Global Actor, which balances data sampling across different subsets\nof the training mixture, and several Local Actors, which optimizes data usage\nwithin each subset based on difficulty levels. These actors are guided by\nreward functions derived from the LLM's training state, which measure learning\nprogress and relative performance improvement. We evaluate HBO on three LLM\nbackbones across nine diverse tasks in multilingual and multitask setups.\nResults show that HBO consistently outperforms existing baselines, achieving\nsignificant accuracy gains. Our in-depth analysis further demonstrates that\nboth the global actor and local actors of HBO effectively adjust data usage\nduring fine-tuning. HBO provides a comprehensive solution to the challenges of\ndata imbalance and heterogeneity in LLM fine-tuning, enabling more effective\ntraining across diverse datasets."}
{"id": "2505.12795", "pdf": "https://arxiv.org/pdf/2505.12795", "abs": "https://arxiv.org/abs/2505.12795", "authors": ["Shibo Hong", "Jiahao Ying", "Haiyuan Liang", "Mengdi Zhang", "Jun Kuang", "Jiazheng Zhang", "Yixin Cao"], "title": "FRAbench and GenEval: Scaling Fine-Grained Aspect Evaluation across Tasks, Modalities", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Evaluating the open-ended outputs of large language models (LLMs) has become\na bottleneck as model capabilities, task diversity, and modality coverage\nrapidly expand. Existing \"LLM-as-a-Judge\" evaluators are typically narrow in a\nfew tasks, aspects, or modalities, and easily suffer from low consistency. In\nthis paper, we argue that explicit, fine-grained aspect specification is the\nkey to both generalizability and objectivity in automated evaluation. To do so,\nwe introduce a hierarchical aspect taxonomy spanning 112 aspects that unifies\nevaluation across four representative settings - Natural Language Generation,\nImage Understanding, Image Generation, and Interleaved Text-and-Image\nGeneration. Building on this taxonomy, we create FRAbench, a benchmark\ncomprising 60.4k pairwise samples with 325k aspect-level labels obtained from a\ncombination of human and LLM annotations. FRAbench provides the first\nlarge-scale, multi-modal resource for training and meta-evaluating fine-grained\nLMM judges. Leveraging FRAbench, we develop GenEval, a fine-grained evaluator\ngeneralizable across tasks and modalities. Experiments show that GenEval (i)\nattains high agreement with GPT-4o and expert annotators, (ii) transfers\nrobustly to unseen tasks and modalities, and (iii) reveals systematic\nweaknesses of current LMMs on evaluation."}
{"id": "2505.11912", "pdf": "https://arxiv.org/pdf/2505.11912", "abs": "https://arxiv.org/abs/2505.11912", "authors": ["Paul Saves", "Nicolas Verstaevel", "Benoît Gaudou"], "title": "Modèles de Substitution pour les Modèles à base d'Agents : Enjeux, Méthodes et Applications", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "12 pages, in French language. Les 33\\`emes Journ\\'ees Francophones\n  sur les Syst\\`emes Multi-Agents (JFSMA 2025). 2025", "summary": "Multi-agent simulations enables the modeling and analyses of the dynamic\nbehaviors and interactions of autonomous entities evolving in complex\nenvironments. Agent-based models (ABM) are widely used to study emergent\nphenomena arising from local interactions. However, their high computational\ncost poses a significant challenge, particularly for large-scale simulations\nrequiring extensive parameter exploration, optimization, or uncertainty\nquantification. The increasing complexity of ABM limits their feasibility for\nreal-time decision-making and large-scale scenario analysis. To address these\nlimitations, surrogate models offer an efficient alternative by learning\napproximations from sparse simulation data. These models provide\ncheap-to-evaluate predictions, significantly reducing computational costs while\nmaintaining accuracy. Various machine learning techniques, including regression\nmodels, neural networks, random forests and Gaussian processes, have been\napplied to construct robust surrogates. Moreover, uncertainty quantification\nand sensitivity analysis play a crucial role in enhancing model reliability and\ninterpretability.\n  This article explores the motivations, methods, and applications of surrogate\nmodeling for ABM, emphasizing the trade-offs between accuracy, computational\nefficiency, and interpretability. Through a case study on a segregation model,\nwe highlight the challenges associated with building and validating surrogate\nmodels, comparing different approaches and evaluating their performance.\nFinally, we discuss future perspectives on integrating surrogate models within\nABM to improve scalability, explainability, and real-time decision support\nacross various fields such as ecology, urban planning and economics."}
{"id": "2505.12306", "pdf": "https://arxiv.org/pdf/2505.12306", "abs": "https://arxiv.org/abs/2505.12306", "authors": ["Yuwei Zhang", "Wenhao Yu", "Shangbin Feng", "Yifan Zhu", "Letian Peng", "Jayanth Srinivasa", "Gaowen Liu", "Jingbo Shang"], "title": "Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection", "categories": ["cs.CL"], "comment": "Dataset is available at\n  https://huggingface.co/datasets/YWZBrandon/wikidyk", "summary": "Despite significant advances in large language models (LLMs), their knowledge\nmemorization capabilities remain underexplored, due to the lack of standardized\nand high-quality test ground. In this paper, we introduce a novel, real-world\nand large-scale knowledge injection benchmark that evolves continuously over\ntime without requiring human intervention. Specifically, we propose WikiDYK,\nwhich leverages recently-added and human-written facts from Wikipedia's \"Did\nYou Know...\" entries. These entries are carefully selected by expert Wikipedia\neditors based on criteria such as verifiability and clarity. Each entry is\nconverted into multiple question-answer pairs spanning diverse task formats\nfrom easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290\nfacts and 77,180 questions, which is also seamlessly extensible with future\nupdates from Wikipedia editors. Extensive experiments using continued\npre-training reveal a surprising insight: despite their prevalence in modern\nLLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge\nmemorization capabilities compared to Bidirectional Language Models (BiLMs),\nexhibiting a 23% lower accuracy in terms of reliability. To compensate for the\nsmaller scales of current BiLMs, we introduce a modular collaborative framework\nutilizing ensembles of BiLMs as external knowledge repositories to integrate\nwith LLMs. Experiment shows that our framework further improves the reliability\naccuracy by up to 29.1%."}
{"id": "2505.12822", "pdf": "https://arxiv.org/pdf/2505.12822", "abs": "https://arxiv.org/abs/2505.12822", "authors": ["Jing Liu", "Haozheng Wang", "Yueheng Li"], "title": "Emergent Specialization: Rare Token Neurons in Language Models", "categories": ["cs.AI"], "comment": "9 pages, 6 figures", "summary": "Large language models struggle with representing and generating rare tokens\ndespite their importance in specialized domains. In this study, we identify\nneuron structures with exceptionally strong influence on language model's\nprediction of rare tokens, termed as rare token neurons, and investigate the\nmechanism for their emergence and behavior. These neurons exhibit a\ncharacteristic three-phase organization (plateau, power-law, and rapid decay)\nthat emerges dynamically during training, evolving from a homogeneous initial\nstate to a functionally differentiated architecture. In the activation space,\nrare token neurons form a coordinated subnetwork that selectively co-activates\nwhile avoiding co-activation with other neurons. This functional specialization\npotentially correlates with the development of heavy-tailed weight\ndistributions, suggesting a statistical mechanical basis for emergent\nspecialization."}
{"id": "2505.11918", "pdf": "https://arxiv.org/pdf/2505.11918", "abs": "https://arxiv.org/abs/2505.11918", "authors": ["Zhiheng Chen", "Ruofan Wu", "Guanhua Fang"], "title": "Transformers as Unsupervised Learning Algorithms: A study on Gaussian Mixtures", "categories": ["cs.LG", "stat.ML"], "comment": "Code available at\n  https://github.com/Rorschach1989/transformer-for-gmm", "summary": "The transformer architecture has demonstrated remarkable capabilities in\nmodern artificial intelligence, among which the capability of implicitly\nlearning an internal model during inference time is widely believed to play a\nkey role in the under standing of pre-trained large language models. However,\nmost recent works have been focusing on studying supervised learning topics\nsuch as in-context learning, leaving the field of unsupervised learning largely\nunexplored. This paper investigates the capabilities of transformers in solving\nGaussian Mixture Models (GMMs), a fundamental unsupervised learning problem\nthrough the lens of statistical estimation. We propose a transformer-based\nlearning framework called TGMM that simultaneously learns to solve multiple GMM\ntasks using a shared transformer backbone. The learned models are empirically\ndemonstrated to effectively mitigate the limitations of classical methods such\nas Expectation-Maximization (EM) or spectral algorithms, at the same time\nexhibit reasonable robustness to distribution shifts. Theoretically, we prove\nthat transformers can approximate both the EM algorithm and a core component of\nspectral methods (cubic tensor power iterations). These results bridge the gap\nbetween practical success and theoretical understanding, positioning\ntransformers as versatile tools for unsupervised learning."}
{"id": "2505.12313", "pdf": "https://arxiv.org/pdf/2505.12313", "abs": "https://arxiv.org/abs/2505.12313", "authors": ["Weixuan Wang", "Minghao Wu", "Barry Haddow", "Alexandra Birch"], "title": "ExpertSteer: Intervening in LLMs through Expert Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit remarkable capabilities across various\ntasks, yet guiding them to follow desired behaviours during inference remains a\nsignificant challenge. Activation steering offers a promising method to control\nthe generation process of LLMs by modifying their internal activations.\nHowever, existing methods commonly intervene in the model's behaviour using\nsteering vectors generated by the model itself, which constrains their\neffectiveness to that specific model and excludes the possibility of leveraging\npowerful external expert models for steering. To address these limitations, we\npropose ExpertSteer, a novel approach that leverages arbitrary specialized\nexpert models to generate steering vectors, enabling intervention in any LLMs.\nExpertSteer transfers the knowledge from an expert model to a target LLM\nthrough a cohesive four-step process: first aligning representation dimensions\nwith auto-encoders to enable cross-model transfer, then identifying\nintervention layer pairs based on mutual information analysis, next generating\nsteering vectors from the expert model using Recursive Feature Machines, and\nfinally applying these vectors on the identified layers during inference to\nselectively guide the target LLM without updating model parameters. We conduct\ncomprehensive experiments using three LLMs on 15 popular benchmarks across four\ndistinct domains. Experiments demonstrate that ExpertSteer significantly\noutperforms established baselines across diverse tasks at minimal cost."}
{"id": "2505.12833", "pdf": "https://arxiv.org/pdf/2505.12833", "abs": "https://arxiv.org/abs/2505.12833", "authors": ["Zhuo Yang", "Lingli Ge", "Dong Han", "Tianfan Fu", "Yuqiang Li"], "title": "Reasoning BO: Enhancing Bayesian Optimization with Long-Context Reasoning Power of LLMs", "categories": ["cs.AI"], "comment": null, "summary": "Many real-world scientific and industrial applications require the\noptimization of expensive black-box functions. Bayesian Optimization (BO)\nprovides an effective framework for such problems. However, traditional BO\nmethods are prone to get trapped in local optima and often lack interpretable\ninsights. To address this issue, this paper designs Reasoning BO, a novel\nframework that leverages reasoning models to guide the sampling process in BO\nwhile incorporating multi-agent systems and knowledge graphs for online\nknowledge accumulation. By integrating the reasoning and contextual\nunderstanding capabilities of Large Language Models (LLMs), we can provide\nstrong guidance to enhance the BO process. As the optimization progresses,\nReasoning BO provides real-time sampling recommendations along with critical\ninsights grounded in plausible scientific theories, aiding in the discovery of\nsuperior solutions within the search space. We systematically evaluate our\napproach across 10 diverse tasks encompassing synthetic mathematical functions\nand complex real-world applications. The framework demonstrates its capability\nto progressively refine sampling strategies through real-time insights and\nhypothesis evolution, effectively identifying higher-performing regions of the\nsearch space for focused exploration. This process highlights the powerful\nreasoning and context-learning abilities of LLMs in optimization scenarios. For\nexample, in the Direct Arylation task, our method increased the yield to 60.7%,\nwhereas traditional BO achieved only a 25.2% yield. Furthermore, our\ninvestigation reveals that smaller LLMs, when fine-tuned through reinforcement\nlearning, can attain comparable performance to their larger counterparts. This\nenhanced reasoning capability paves the way for more efficient automated\nscientific experimentation while maintaining computational feasibility."}
{"id": "2505.11925", "pdf": "https://arxiv.org/pdf/2505.11925", "abs": "https://arxiv.org/abs/2505.11925", "authors": ["Nikolai West", "Jochen Deuse"], "title": "PyScrew: A Comprehensive Dataset Collection from Industrial Screw Driving Experiments", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "18 pages, 2 figures, 7 tables", "summary": "This paper presents a comprehensive collection of industrial screw driving\ndatasets designed to advance research in manufacturing process monitoring and\nquality control. The collection comprises six distinct datasets with over\n34,000 individual screw driving operations conducted under controlled\nexperimental conditions, capturing the multifaceted nature of screw driving\nprocesses in plastic components. Each dataset systematically investigates\nspecific aspects: natural thread degradation patterns through repeated use\n(s01), variations in surface friction conditions including contamination and\nsurface treatments (s02), diverse assembly faults with up to 27 error types\n(s03-s04), and fabrication parameter variations in both upper and lower\nworkpieces through modified injection molding settings (s05-s06). We detail the\nstandardized experimental setup used across all datasets, including hardware\nspecifications, process phases, and data acquisition methods. The hierarchical\ndata model preserves the temporal and operational structure of screw driving\nprocesses, facilitating both exploratory analysis and the development of\nmachine learning models. To maximize accessibility, we provide dual access\npathways: raw data through Zenodo with a persistent DOI, and a purpose-built\nPython library (PyScrew) that offers consistent interfaces for data loading,\npreprocessing, and integration with common analysis workflows. These datasets\nserve diverse research applications including anomaly detection, predictive\nmaintenance, quality control system development, feature extraction methodology\nevaluation, and classification of specific error conditions. By addressing the\nscarcity of standardized, comprehensive datasets in industrial manufacturing,\nthis collection enables reproducible research and fair comparison of analytical\napproaches in an area of growing importance for industrial automation."}
{"id": "2505.12328", "pdf": "https://arxiv.org/pdf/2505.12328", "abs": "https://arxiv.org/abs/2505.12328", "authors": ["Xinye Li", "Mingqi Wan", "Dianbo Sui"], "title": "LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "We present Team asdfo123's submission to the LLMSR@XLLM25 shared task, which\nevaluates large language models on producing fine-grained, controllable, and\ninterpretable reasoning processes. Systems must extract all problem conditions,\ndecompose a chain of thought into statement-evidence pairs, and verify the\nlogical validity of each pair. Leveraging only the off-the-shelf\nMeta-Llama-3-8B-Instruct, we craft a concise few-shot, multi-turn prompt that\nfirst enumerates all conditions and then guides the model to label, cite, and\nadjudicate every reasoning step. A lightweight post-processor based on regular\nexpressions normalises spans and enforces the official JSON schema. Without\nfine-tuning, external retrieval, or ensembling, our method ranks 5th overall,\nachieving macro F1 scores on par with substantially more complex and\nresource-consuming pipelines. We conclude by analysing the strengths and\nlimitations of our approach and outlining directions for future research in\nstructural reasoning with LLMs. Our code is available at\nhttps://github.com/asdfo123/LLMSR-asdfo123."}
{"id": "2505.12844", "pdf": "https://arxiv.org/pdf/2505.12844", "abs": "https://arxiv.org/abs/2505.12844", "authors": ["Shuo Sun", "Yimin Zhao", "Christina Dao Wen Lee", "Jiawei Sun", "Chengran Yuan", "Zefan Huang", "Dongen Li", "Justin KW Yeoh", "Alok Prakash", "Thomas W. Malone", "Marcelo H. Ang Jr"], "title": "AGI-Elo: How Far Are We From Mastering A Task?", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "As the field progresses toward Artificial General Intelligence (AGI), there\nis a pressing need for more comprehensive and insightful evaluation frameworks\nthat go beyond aggregate performance metrics. This paper introduces a unified\nrating system that jointly models the difficulty of individual test cases and\nthe competency of AI models (or humans) across vision, language, and action\ndomains. Unlike existing metrics that focus solely on models, our approach\nallows for fine-grained, difficulty-aware evaluations through competitive\ninteractions between models and tasks, capturing both the long-tail\ndistribution of real-world challenges and the competency gap between current\nmodels and full task mastery. We validate the generalizability and robustness\nof our system through extensive experiments on multiple established datasets\nand models across distinct AGI domains. The resulting rating distributions\noffer novel perspectives and interpretable insights into task difficulty, model\nprogression, and the outstanding challenges that remain on the path to\nachieving full AGI task mastery."}
{"id": "2505.11930", "pdf": "https://arxiv.org/pdf/2505.11930", "abs": "https://arxiv.org/abs/2505.11930", "authors": ["Marco Sälzer", "Przemysław Andrzej Wałęga", "Martin Lange"], "title": "The Logical Expressiveness of Temporal GNNs via Two-Dimensional Product Logics", "categories": ["cs.LG", "cs.AI", "cs.LO"], "comment": null, "summary": "In recent years, the expressive power of various neural architectures --\nincluding graph neural networks (GNNs), transformers, and recurrent neural\nnetworks -- has been characterised using tools from logic and formal language\ntheory. As the capabilities of basic architectures are becoming well\nunderstood, increasing attention is turning to models that combine multiple\narchitectural paradigms. Among them particularly important, and challenging to\nanalyse, are temporal extensions of GNNs, which integrate both spatial\n(graph-structure) and temporal (evolution over time) dimensions. In this paper,\nwe initiate the study of logical characterisation of temporal GNNs by\nconnecting them to two-dimensional product logics. We show that the expressive\npower of temporal GNNs depends on how graph and temporal components are\ncombined. In particular, temporal GNNs that apply static GNNs recursively over\ntime can capture all properties definable in the product logic of (past)\npropositional temporal logic PTL and the modal logic K. In contrast,\narchitectures such as graph-and-time TGNNs and global TGNNs can only express\nrestricted fragments of this logic, where the interaction between temporal and\nspatial operators is syntactically constrained. These results yield the first\nlogical characterisations of temporal GNNs and establish new relative\nexpressiveness results for temporal GNNs."}
{"id": "2505.12345", "pdf": "https://arxiv.org/pdf/2505.12345", "abs": "https://arxiv.org/abs/2505.12345", "authors": ["Qizhou Chen", "Dakan Wang", "Taolin Zhang", "Zaoming Yan", "Chengsong You", "Chengyu Wang", "Xiaofeng He"], "title": "UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Model editing aims to enhance the accuracy and reliability of large language\nmodels (LLMs) by efficiently adjusting their internal parameters. Currently,\nmost LLM editing datasets are confined to narrow knowledge domains and cover a\nlimited range of editing evaluation. They often overlook the broad scope of\nediting demands and the diversity of ripple effects resulting from edits. In\nthis context, we introduce UniEdit, a unified benchmark for LLM editing\ngrounded in open-domain knowledge. First, we construct editing samples by\nselecting entities from 25 common domains across five major categories,\nutilizing the extensive triple knowledge available in open-domain knowledge\ngraphs to ensure comprehensive coverage of the knowledge domains. To address\nthe issues of generality and locality in editing, we design an Neighborhood\nMulti-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given\nknowledge piece to entail comprehensive ripple effects to evaluate. Finally, we\nemploy proprietary LLMs to convert the sampled knowledge subgraphs into natural\nlanguage text, guaranteeing grammatical accuracy and syntactical diversity.\nExtensive statistical analysis confirms the scale, comprehensiveness, and\ndiversity of our UniEdit benchmark. We conduct comprehensive experiments across\nmultiple LLMs and editors, analyzing their performance to highlight strengths\nand weaknesses in editing across open knowledge domains and various evaluation\ncriteria, thereby offering valuable insights for future research endeavors."}
{"id": "2505.12845", "pdf": "https://arxiv.org/pdf/2505.12845", "abs": "https://arxiv.org/abs/2505.12845", "authors": ["Ruopei Sun", "Jianfeng Cai", "Jinhua Zhu", "Kangwen Zhao", "Dongyun Xue", "Wengang Zhou", "Li Li", "Houqiang Li"], "title": "Multi-Level Aware Preference Learning: Enhancing RLHF for Complex Multi-Instruction Tasks", "categories": ["cs.AI"], "comment": null, "summary": "RLHF has emerged as a predominant approach for aligning artificial\nintelligence systems with human preferences, demonstrating exceptional and\nmeasurable efficacy in instruction following tasks; however, it exhibits\ninsufficient compliance capabilities when confronted with complex\nmulti-instruction tasks. Conventional approaches rely heavily on human\nannotation or more sophisticated large language models, thereby introducing\nsubstantial resource expenditure or potential bias concerns. Meanwhile,\nalternative synthetic methods that augment standard preference datasets often\ncompromise the model's semantic quality. Our research identifies a critical\noversight in existing techniques, which predominantly focus on comparing\nresponses while neglecting valuable latent signals embedded within prompt\ninputs, and which only focus on preference disparities at the intra-sample\nlevel, while neglecting to account for the inter-sample level preference\ndifferentials that exist among preference data. To leverage these previously\nneglected indicators, we propose a novel Multi-level Aware Preference Learning\n(MAPL) framework, capable of enhancing multi-instruction capabilities.\nSpecifically, for any given response in original preference data pairs, we\nconstruct varied prompts with a preference relation under different conditions,\nin order to learn intra-sample level preference disparities. Furthermore, for\nany given original preference pair, we synthesize multi-instruction preference\npairs to capture preference discrepancies at the inter-sample level. Building\non the two datasets constructed above, we consequently devise two sophisticated\ntraining objective functions. Subsequently, our framework integrates seamlessly\ninto both Reward Modeling and Direct Preference Optimization paradigms. Through\nrigorous evaluation across multiple benchmarks, we empirically validate the\nefficacy of our framework."}
{"id": "2505.11936", "pdf": "https://arxiv.org/pdf/2505.11936", "abs": "https://arxiv.org/abs/2505.11936", "authors": ["Jingren Liu", "Zhong Ji", "Xiangyu Chen"], "title": "How can Diffusion Models Evolve into Continual Generators?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While diffusion models have achieved remarkable success in static data\ngeneration, their deployment in streaming or continual learning (CL) scenarios\nfaces a major challenge: catastrophic forgetting (CF), where newly acquired\ngenerative capabilities overwrite previously learned ones. To systematically\naddress this, we introduce a formal Continual Diffusion Generation (CDG)\nparadigm that characterizes and redefines CL in the context of generative\ndiffusion models. Prior efforts often adapt heuristic strategies from continual\nclassification tasks but lack alignment with the underlying diffusion process.\nIn this work, we develop the first theoretical framework for CDG by analyzing\ncross-task dynamics in diffusion-based generative modeling. Our analysis\nreveals that the retention and stability of generative knowledge across tasks\nare governed by three key consistency criteria: inter-task knowledge\nconsistency (IKC), unconditional knowledge consistency (UKC), and label\nknowledge consistency (LKC). Building on these insights, we propose Continual\nConsistency Diffusion (CCD), a principled framework that integrates these\nconsistency objectives into training via hierarchical loss terms\n$\\mathcal{L}_{IKC}$, $\\mathcal{L}_{UKC}$, and $\\mathcal{L}_{LKC}$. This\npromotes effective knowledge retention while enabling the assimilation of new\ngenerative capabilities. Extensive experiments on four benchmark datasets\ndemonstrate that CCD achieves state-of-the-art performance under continual\nsettings, with substantial gains in Mean Fidelity (MF) and Incremental Mean\nFidelity (IMF), particularly in tasks with rich cross-task knowledge overlap."}
{"id": "2505.12349", "pdf": "https://arxiv.org/pdf/2505.12349", "abs": "https://arxiv.org/abs/2505.12349", "authors": ["Axel Abels", "Tom Lenaerts"], "title": "Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted for publication in the Proceedings of the 34th International\n  Joint Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Despite their performance, large language models (LLMs) can inadvertently\nperpetuate biases found in the data they are trained on. By analyzing LLM\nresponses to bias-eliciting headlines, we find that these models often mirror\nhuman biases. To address this, we explore crowd-based strategies for mitigating\nbias through response aggregation. We first demonstrate that simply averaging\nresponses from multiple LLMs, intended to leverage the \"wisdom of the crowd\",\ncan exacerbate existing biases due to the limited diversity within LLM crowds.\nIn contrast, we show that locally weighted aggregation methods more effectively\nleverage the wisdom of the LLM crowd, achieving both bias mitigation and\nimproved accuracy. Finally, recognizing the complementary strengths of LLMs\n(accuracy) and humans (diversity), we demonstrate that hybrid crowds containing\nboth significantly enhance performance and further reduce biases across ethnic\nand gender-related contexts."}
{"id": "2505.12872", "pdf": "https://arxiv.org/pdf/2505.12872", "abs": "https://arxiv.org/abs/2505.12872", "authors": ["Maytus Piriyajitakonkij", "Rujikorn Charakorn", "Weicheng Tao", "Wei Pan", "Mingfei Sun", "Cheston Tan", "Mengmi Zhang"], "title": "From Grunts to Grammar: Emergent Language from Cooperative Foraging", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Early cavemen relied on gestures, vocalizations, and simple signals to\ncoordinate, plan, avoid predators, and share resources. Today, humans\ncollaborate using complex languages to achieve remarkable results. What drives\nthis evolution in communication? How does language emerge, adapt, and become\nvital for teamwork? Understanding the origins of language remains a challenge.\nA leading hypothesis in linguistics and anthropology posits that language\nevolved to meet the ecological and social demands of early human cooperation.\nLanguage did not arise in isolation, but through shared survival goals.\nInspired by this view, we investigate the emergence of language in multi-agent\nForaging Games. These environments are designed to reflect the cognitive and\necological constraints believed to have influenced the evolution of\ncommunication. Agents operate in a shared grid world with only partial\nknowledge about other agents and the environment, and must coordinate to\ncomplete games like picking up high-value targets or executing temporally\nordered actions. Using end-to-end deep reinforcement learning, agents learn\nboth actions and communication strategies from scratch. We find that agents\ndevelop communication protocols with hallmark features of natural language:\narbitrariness, interchangeability, displacement, cultural transmission, and\ncompositionality. We quantify each property and analyze how different factors,\nsuch as population size and temporal dependencies, shape specific aspects of\nthe emergent language. Our framework serves as a platform for studying how\nlanguage can evolve from partial observability, temporal reasoning, and\ncooperative goals in embodied multi-agent settings. We will release all data,\ncode, and models publicly."}
{"id": "2505.11953", "pdf": "https://arxiv.org/pdf/2505.11953", "abs": "https://arxiv.org/abs/2505.11953", "authors": ["Puning Yang", "Qizhou Wang", "Zhuo Huang", "Tongliang Liu", "Chengqi Zhang", "Bo Han"], "title": "Exploring Criteria of Loss Reweighting to Enhance LLM Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Loss reweighting has shown significant benefits for machine unlearning with\nlarge language models (LLMs). However, their exact functionalities are left\nunclear and the optimal strategy remains an open question, thus impeding the\nunderstanding and improvement of existing methodologies. In this paper, we\nidentify two distinct goals of loss reweighting, namely, Saturation and\nImportance -- the former indicates that those insufficiently optimized data\nshould be emphasized, while the latter stresses some critical data that are\nmost influential for loss minimization. To study their usefulness, we design\nspecific reweighting strategies for each goal and evaluate their respective\neffects on unlearning. We conduct extensive empirical analyses on\nwell-established benchmarks, and summarize some important observations as\nfollows: (i) Saturation enhances efficacy more than importance-based\nreweighting, and their combination can yield additional improvements. (ii)\nSaturation typically allocates lower weights to data with lower likelihoods,\nwhereas importance-based reweighting does the opposite. (iii) The efficacy of\nunlearning is also largely influenced by the smoothness and granularity of the\nweight distributions. Based on these findings, we propose SatImp, a simple\nreweighting method that combines the advantages of both saturation and\nimportance. Empirical results on extensive datasets validate the efficacy of\nour method, potentially bridging existing research gaps and indicating\ndirections for future research. Our code is available at\nhttps://github.com/Puning97/SatImp-for-LLM-Unlearning."}
{"id": "2505.12368", "pdf": "https://arxiv.org/pdf/2505.12368", "abs": "https://arxiv.org/abs/2505.12368", "authors": ["Gauri Kholkar", "Ratinder Ahuja"], "title": "CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in ACL LLMSec Workshop 2025", "summary": "Prompt injection remains a major security risk for large language models.\nHowever, the efficacy of existing guardrail models in context-aware settings\nremains underexplored, as they often rely on static attack benchmarks.\nAdditionally, they have over-defense tendencies. We introduce CAPTURE, a novel\ncontext-aware benchmark assessing both attack detection and over-defense\ntendencies with minimal in-domain examples. Our experiments reveal that current\nprompt injection guardrail models suffer from high false negatives in\nadversarial cases and excessive false positives in benign scenarios,\nhighlighting critical limitations."}
{"id": "2505.12886", "pdf": "https://arxiv.org/pdf/2505.12886", "abs": "https://arxiv.org/abs/2505.12886", "authors": ["Zhongxiang Sun", "Qipeng Wang", "Haoyu Wang", "Xiao Zhang", "Jun Xu"], "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "25 pages", "summary": "Large Reasoning Models (LRMs) have shown impressive capabilities in\nmulti-step reasoning tasks. However, alongside these successes, a more\ndeceptive form of model error has emerged--Reasoning Hallucination--where\nlogically coherent but factually incorrect reasoning traces lead to persuasive\nyet faulty conclusions. Unlike traditional hallucinations, these errors are\nembedded within structured reasoning, making them more difficult to detect and\npotentially more harmful. In this work, we investigate reasoning hallucinations\nfrom a mechanistic perspective. We propose the Reasoning Score, which\nquantifies the depth of reasoning by measuring the divergence between logits\nobtained from projecting late layers of LRMs to the vocabulary space,\neffectively distinguishing shallow pattern-matching from genuine deep\nreasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA\ndataset and identify two key reasoning hallucination patterns: early-stage\nfluctuation in reasoning depth and incorrect backtracking to flawed prior\nsteps. These insights motivate our Reasoning Hallucination Detection (RHD)\nframework, which achieves state-of-the-art performance across multiple domains.\nTo mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced\nreinforcement learning algorithm that incorporates step-level deep reasoning\nrewards via potential-based shaping. Our theoretical analysis establishes\nstronger generalization guarantees, and experiments demonstrate improved\nreasoning quality and reduced hallucination rates."}
{"id": "2505.11972", "pdf": "https://arxiv.org/pdf/2505.11972", "abs": "https://arxiv.org/abs/2505.11972", "authors": ["Daniyar Zakarin", "Sidak Pal Singh"], "title": "Accelerating Neural Network Training Along Sharp and Flat Directions", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Recent work has highlighted a surprising alignment between gradients and the\ntop eigenspace of the Hessian -- termed the Dominant subspace -- during neural\nnetwork training. Concurrently, there has been growing interest in the distinct\nroles of sharp and flat directions in the Hessian spectrum. In this work, we\nstudy Bulk-SGD, a variant of SGD that restricts updates to the orthogonal\ncomplement of the Dominant subspace. Through ablation studies, we characterize\nthe stability properties of Bulk-SGD and identify critical hyperparameters that\ngovern its behavior. We show that updates along the Bulk subspace,\ncorresponding to flatter directions in the loss landscape, can accelerate\nconvergence but may compromise stability. To balance these effects, we\nintroduce interpolated gradient methods that unify SGD, Dom-SGD, and Bulk-SGD.\nFinally, we empirically connect this subspace decomposition to the Generalized\nGauss-Newton and Functional Hessian terms, showing that curvature energy is\nlargely concentrated in the Dominant subspace. Our findings suggest a\nprincipled approach to designing curvature-aware optimizers."}
{"id": "2505.12381", "pdf": "https://arxiv.org/pdf/2505.12381", "abs": "https://arxiv.org/abs/2505.12381", "authors": ["Mohsinul Kabir", "Tasfia Tahsin", "Sophia Ananiadou"], "title": "From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Current research on bias in language models (LMs) predominantly focuses on\ndata quality, with significantly less attention paid to model architecture and\ntemporal influences of data. Even more critically, few studies systematically\ninvestigate the origins of bias. We propose a methodology grounded in\ncomparative behavioral theory to interpret the complex interaction between\ntraining data and model architecture in bias propagation during language\nmodeling. Building on recent work that relates transformers to n-gram LMs, we\nevaluate how data, model design choices, and temporal dynamics affect bias\npropagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to\ncontext window size in bias propagation, while transformers demonstrate\narchitectural robustness; (2) the temporal provenance of training data\nsignificantly affects bias; and (3) different model architectures respond\ndifferentially to controlled bias injection, with certain biases (e.g. sexual\norientation) being disproportionately amplified. As language models become\nubiquitous, our findings highlight the need for a holistic approach -- tracing\nbias to its origins across both data and model dimensions, not just symptoms,\nto mitigate harm."}
{"id": "2505.12891", "pdf": "https://arxiv.org/pdf/2505.12891", "abs": "https://arxiv.org/abs/2505.12891", "authors": ["Shaohang Wei", "Wei Li", "Feifan Song", "Wen Luo", "Tianyi Zhuang", "Haochen Tan", "Zhijiang Guo", "Houfeng Wang"], "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "categories": ["cs.AI", "cs.CL"], "comment": "First version. There are still some examples to be added into the\n  appendix", "summary": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME ."}
{"id": "2505.11982", "pdf": "https://arxiv.org/pdf/2505.11982", "abs": "https://arxiv.org/abs/2505.11982", "authors": ["Zihao Zheng", "Ziyao Wang", "Xiuping Cui", "Maoliang Li", "Jiayu Chen", "Yun", "Liang", "Ang Li", "Xiang Chen"], "title": "FedHQ: Hybrid Runtime Quantization for Federated Learning", "categories": ["cs.LG"], "comment": "5 figures and 4 tables", "summary": "Federated Learning (FL) is a decentralized model training approach that\npreserves data privacy but struggles with low efficiency. Quantization, a\npowerful training optimization technique, has been widely explored for\nintegration into FL. However, many studies fail to consider the distinct\nperformance attribution between particular quantization strategies, such as\npost-training quantization (PTQ) or quantization-aware training (QAT). As a\nresult, existing FL quantization methods rely solely on either PTQ or QAT,\noptimizing for speed or accuracy while compromising the other. To efficiently\naccelerate FL and maintain distributed convergence accuracy across various FL\nsettings, this paper proposes a hybrid quantitation approach combining PTQ and\nQAT for FL systems. We conduct case studies to validate the effectiveness of\nusing hybrid quantization in FL. To solve the difficulty of modeling speed and\naccuracy caused by device and data heterogeneity, we propose a hardware-related\nanalysis and data-distribution-related analysis to help identify the trade-off\nboundaries for strategy selection. Based on these, we proposed a novel\nframework named FedHQ to automatically adopt optimal hybrid strategy allocation\nfor FL systems. Specifically, FedHQ develops a coarse-grained global\ninitialization and fine-grained ML-based adjustment to ensure efficiency and\nrobustness. Experiments show that FedHQ achieves up to 2.47x times training\nacceleration and up to 11.15% accuracy improvement and negligible extra\noverhead."}
{"id": "2505.12392", "pdf": "https://arxiv.org/pdf/2505.12392", "abs": "https://arxiv.org/abs/2505.12392", "authors": ["Yang Hu", "Xingyu Zhang", "Xueji Fang", "Zhiyang Chen", "Xiao Wang", "Huatian Zhang", "Guojun Qi"], "title": "SLOT: Sample-specific Language Model Optimization at Test-time", "categories": ["cs.CL"], "comment": null, "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT."}
{"id": "2505.12923", "pdf": "https://arxiv.org/pdf/2505.12923", "abs": "https://arxiv.org/abs/2505.12923", "authors": ["Pedro M. P. Curvo"], "title": "The Traitors: Deception and Trust in Multi-Agent Language Model Simulations", "categories": ["cs.AI", "cs.MA"], "comment": "9 main pages, 31 pages", "summary": "As AI systems increasingly assume roles where trust and alignment with human\nvalues are essential, understanding when and why they engage in deception has\nbecome a critical research priority. We introduce The Traitors, a multi-agent\nsimulation framework inspired by social deduction games, designed to probe\ndeception, trust formation, and strategic communication among large language\nmodel (LLM) agents under asymmetric information. A minority of agents the\ntraitors seek to mislead the majority, while the faithful must infer hidden\nidentities through dialogue and reasoning. Our contributions are: (1) we ground\nthe environment in formal frameworks from game theory, behavioral economics,\nand social cognition; (2) we develop a suite of evaluation metrics capturing\ndeception success, trust dynamics, and collective inference quality; (3) we\nimplement a fully autonomous simulation platform where LLMs reason over\npersistent memory and evolving social dynamics, with support for heterogeneous\nagent populations, specialized traits, and adaptive behaviors. Our initial\nexperiments across DeepSeek-V3, GPT-4o-mini, and GPT-4o (10 runs per model)\nreveal a notable asymmetry: advanced models like GPT-4o demonstrate superior\ndeceptive capabilities yet exhibit disproportionate vulnerability to others'\nfalsehoods. This suggests deception skills may scale faster than detection\nabilities. Overall, The Traitors provides a focused, configurable testbed for\ninvestigating LLM behavior in socially nuanced interactions. We position this\nwork as a contribution toward more rigorous research on deception mechanisms,\nalignment challenges, and the broader social reliability of AI systems."}
{"id": "2505.11985", "pdf": "https://arxiv.org/pdf/2505.11985", "abs": "https://arxiv.org/abs/2505.11985", "authors": ["Sabrina Khurshid", "Gourab Ghatak", "Mohammad Shahid Abdulla"], "title": "Variance-Optimal Arm Selection: Regret Minimization and Best Arm Identification", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "This paper focuses on selecting the arm with the highest variance from a set\nof $K$ independent arms. Specifically, we focus on two settings: (i) regret\nsetting, that penalizes the number of pulls of suboptimal arms in terms of\nvariance, and (ii) fixed-budget \\ac{BAI} setting, that evaluates the ability of\nan algorithm to determine the arm with the highest variance after a fixed\nnumber of pulls. We develop a novel online algorithm called \\texttt{UCB-VV} for\nthe regret setting and show that its upper bound on regret for bounded rewards\nevolves as $\\mathcal{O}\\left(\\log{n}\\right)$ where $n$ is the horizon. By\nderiving the lower bound on the regret, we show that \\texttt{UCB-VV} is order\noptimal. For the fixed budget \\ac{BAI} setting and propose the \\texttt{SHVV}\nalgorithm. We show that the upper bound of the error probability of\n\\texttt{SHVV} evolves as $\\exp\\left(-\\frac{n}{\\log(K) H}\\right)$, where $H$\nrepresents the complexity of the problem, and this rate matches the\ncorresponding lower bound. We extend the framework from bounded distributions\nto sub-Gaussian distributions using a novel concentration inequality on the\nsample variance. Leveraging the same, we derive a concentration inequality for\nthe empirical Sharpe ratio (SR) for sub-Gaussian distributions, which was\npreviously unknown in the literature. Empirical simulations show that\n\\texttt{UCB-VV} consistently outperforms \\texttt{$\\epsilon$-greedy} across\ndifferent sub-optimality gaps though it is surpassed by \\texttt{VTS}, which\nexhibits the lowest regret, albeit lacking in theoretical guarantees. We also\nillustrate the superior performance of \\texttt{SHVV}, for a fixed budget\nsetting under 6 different setups against uniform sampling. Finally, we conduct\na case study to empirically evaluate the performance of the \\texttt{UCB-VV} and\n\\texttt{SHVV} in call option trading on $100$ stocks generated using \\ac{GBM}."}
{"id": "2505.12398", "pdf": "https://arxiv.org/pdf/2505.12398", "abs": "https://arxiv.org/abs/2505.12398", "authors": ["Yepeng Weng", "Qiao Hu", "Xujie Chen", "Li Liu", "Dianwen Mei", "Huishi Qiu", "Jiang Tian", "Zhongchao Shi"], "title": "Traversal Verification for Speculative Tree Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Speculative decoding is a promising approach for accelerating large language\nmodels. The primary idea is to use a lightweight draft model to speculate the\noutput of the target model for multiple subsequent timesteps, and then verify\nthem in parallel to determine whether the drafted tokens should be accepted or\nrejected. To enhance acceptance rates, existing frameworks typically construct\ntoken trees containing multiple candidates in each timestep. However, their\nreliance on token-level verification mechanisms introduces two critical\nlimitations: First, the probability distribution of a sequence differs from\nthat of individual tokens, leading to suboptimal acceptance length. Second,\ncurrent verification schemes begin from the root node and proceed layer by\nlayer in a top-down manner. Once a parent node is rejected, all its child nodes\nshould be discarded, resulting in inefficient utilization of speculative\ncandidates. This paper introduces Traversal Verification, a novel speculative\ndecoding algorithm that fundamentally rethinks the verification paradigm\nthrough leaf-to-root traversal. Our approach considers the acceptance of the\nentire token sequence from the current node to the root, and preserves\npotentially valid subsequences that would be prematurely discarded by existing\nmethods. We theoretically prove that the probability distribution obtained\nthrough Traversal Verification is identical to that of the target model,\nguaranteeing lossless inference while achieving substantial acceleration gains.\nExperimental results across different large language models and multiple tasks\nshow that our method consistently improves acceptance length and throughput\nover existing methods"}
{"id": "2505.13011", "pdf": "https://arxiv.org/pdf/2505.13011", "abs": "https://arxiv.org/abs/2505.13011", "authors": ["Yubin Li", "Xingyu Liu", "Guozhang Chen"], "title": "Unveiling and Steering Connectome Organization with Interpretable Latent Variables", "categories": ["cs.AI"], "comment": null, "summary": "The brain's intricate connectome, a blueprint for its function, presents\nimmense complexity, yet it arises from a compact genetic code, hinting at\nunderlying low-dimensional organizational principles. This work bridges\nconnectomics and representation learning to uncover these principles. We\npropose a framework that combines subgraph extraction from the Drosophila\nconnectome, FlyWire, with a generative model to derive interpretable\nlow-dimensional representations of neural circuitry. Crucially, an\nexplainability module links these latent dimensions to specific structural\nfeatures, offering insights into their functional relevance. We validate our\napproach by demonstrating effective graph reconstruction and, significantly,\nthe ability to manipulate these latent codes to controllably generate\nconnectome subgraphs with predefined properties. This research offers a novel\ntool for understanding brain architecture and a potential avenue for designing\nbio-inspired artificial neural networks."}
{"id": "2505.11998", "pdf": "https://arxiv.org/pdf/2505.11998", "abs": "https://arxiv.org/abs/2505.11998", "authors": ["Prashant Shivaram Bhat", "Shakib Yazdani", "Elahe Arani", "Bahram Zonooz"], "title": "Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation", "categories": ["cs.LG", "cs.CV"], "comment": "27 pages, 5 figures", "summary": "Catastrophic forgetting has remained a critical challenge for deep neural\nnetworks in Continual Learning (CL) as it undermines consolidated knowledge\nwhen learning new tasks. Parameter efficient fine tuning CL techniques are\ngaining traction for their effectiveness in addressing catastrophic forgetting\nwith a lightweight training schedule while avoiding degradation of consolidated\nknowledge in pre-trained models. However, low rank adapters (LoRA) in these\napproaches are highly sensitive to rank selection which can lead to sub-optimal\nresource allocation and performance. To this end, we introduce PEARL, a\nrehearsal-free CL framework that entails dynamic rank allocation for LoRA\ncomponents during CL training. Specifically, PEARL leverages reference task\nweights and adaptively determines the rank of task-specific LoRA components\nbased on the current tasks' proximity to reference task weights in parameter\nspace. To demonstrate the versatility of PEARL, we evaluate it across three\nvision architectures (ResNet, Separable Convolutional Network and Vision\nTransformer) and a multitude of CL scenarios, and show that PEARL outperforms\nall considered baselines by a large margin."}
{"id": "2505.12405", "pdf": "https://arxiv.org/pdf/2505.12405", "abs": "https://arxiv.org/abs/2505.12405", "authors": ["Konstantinos Xylogiannopoulos", "Petros Xanthopoulos", "Panagiotis Karampelas", "Georgios Bakamitsos"], "title": "The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative AI paraphrased text can be used for copyright infringement and the\nAI paraphrased content can deprive substantial revenue from original content\ncreators. Despite this recent surge of malicious use of generative AI, there\nare few academic publications that research this threat. In this article, we\ndemonstrate the ability of pattern-based similarity detection for AI\nparaphrased news recognition. We propose an algorithmic scheme, which is not\nlimited to detect whether an article is an AI paraphrase, but, more\nimportantly, to identify that the source of infringement is the ChatGPT. The\nproposed method is tested with a benchmark dataset specifically created for\nthis task that incorporates real articles from BBC, incorporating a total of\n2,224 articles across five different news categories, as well as 2,224\nparaphrased articles created with ChatGPT. Results show that our pattern\nsimilarity-based method, that makes no use of deep learning, can detect ChatGPT\nassisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for\nprecision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1\nscore."}
{"id": "2505.13031", "pdf": "https://arxiv.org/pdf/2505.13031", "abs": "https://arxiv.org/abs/2505.13031", "authors": ["Yicheng Xiao", "Lin Song", "Yukang Chen", "Yingmin Luo", "Yuxin Chen", "Yukang Gan", "Wei Huang", "Xiu Li", "Xiaojuan Qi", "Ying Shan"], "title": "MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO", "categories": ["cs.AI"], "comment": "Code: https://github.com/EasonXiao-888/MindOmni", "summary": "Recent text-to-image systems face limitations in handling multimodal inputs\nand complex reasoning tasks. We introduce MindOmni, a unified multimodal large\nlanguage model that addresses these challenges by incorporating reasoning\ngeneration through reinforcement learning. MindOmni leverages a three-phase\ntraining strategy: i) design of a unified vision language model with a\ndecoder-only diffusion module, ii) supervised fine-tuning with Chain-of-Thought\n(CoT) instruction data, and iii) our proposed Reasoning Generation Policy\nOptimization (RGPO) algorithm, utilizing multimodal feedback to effectively\nguide policy updates. Experimental results demonstrate that MindOmni\noutperforms existing models, achieving impressive performance on both\nunderstanding and generation benchmarks, meanwhile showcasing advanced\nfine-grained reasoning generation capabilities, especially with mathematical\nreasoning instruction. All codes will be made public at\n\\href{https://github.com/EasonXiao-888/MindOmni}{https://github.com/EasonXiao-888/MindOmni}."}
{"id": "2505.12003", "pdf": "https://arxiv.org/pdf/2505.12003", "abs": "https://arxiv.org/abs/2505.12003", "authors": ["Davide Murari", "Takashi Furuya", "Carola-Bibiane Schönlieb"], "title": "Approximation theory for 1-Lipschitz ResNets", "categories": ["cs.LG", "cs.NA", "math.NA", "68T07"], "comment": null, "summary": "1-Lipschitz neural networks are fundamental for generative modelling, inverse\nproblems, and robust classifiers. In this paper, we focus on 1-Lipschitz\nresidual networks (ResNets) based on explicit Euler steps of negative gradient\nflows and study their approximation capabilities. Leveraging the Restricted\nStone-Weierstrass Theorem, we first show that these 1-Lipschitz ResNets are\ndense in the set of scalar 1-Lipschitz functions on any compact domain when\nwidth and depth are allowed to grow. We also show that these networks can\nexactly represent scalar piecewise affine 1-Lipschitz functions. We then prove\na stronger statement: by inserting norm-constrained linear maps between the\nresidual blocks, the same density holds when the hidden width is fixed. Because\nevery layer obeys simple norm constraints, the resulting models can be trained\nwith off-the-shelf optimisers. This paper provides the first universal\napproximation guarantees for 1-Lipschitz ResNets, laying a rigorous foundation\nfor their practical use."}
{"id": "2505.12415", "pdf": "https://arxiv.org/pdf/2505.12415", "abs": "https://arxiv.org/abs/2505.12415", "authors": ["Zhenhe Wu", "Jian Yang", "Jiaheng Liu", "Xianjie Wu", "Changzai Pan", "Jie Zhang", "Yu Zhao", "Shuangyong Song", "Yongxiang Li", "Zhoujun Li"], "title": "Table-R1: Region-based Reinforcement Learning for Table Understanding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning."}
{"id": "2505.13044", "pdf": "https://arxiv.org/pdf/2505.13044", "abs": "https://arxiv.org/abs/2505.13044", "authors": ["Rebecca Westhäußer", "Frederik Berenz", "Wolfgang Minker", "Sebastian Zepf"], "title": "CAIM: Development and Evaluation of a Cognitive AI Memory Framework for Long-Term Interaction with Intelligent Agents", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) have advanced the field of artificial\nintelligence (AI) and are a powerful enabler for interactive systems. However,\nthey still face challenges in long-term interactions that require adaptation\ntowards the user as well as contextual knowledge and understanding of the\never-changing environment. To overcome these challenges, holistic memory\nmodeling is required to efficiently retrieve and store relevant information\nacross interaction sessions for suitable responses. Cognitive AI, which aims to\nsimulate the human thought process in a computerized model, highlights\ninteresting aspects, such as thoughts, memory mechanisms, and decision-making,\nthat can contribute towards improved memory modeling for LLMs. Inspired by\nthese cognitive AI principles, we propose our memory framework CAIM. CAIM\nconsists of three modules: 1.) The Memory Controller as the central decision\nunit; 2.) the Memory Retrieval, which filters relevant data for interaction\nupon request; and 3.) the Post-Thinking, which maintains the memory storage. We\ncompare CAIM against existing approaches, focusing on metrics such as retrieval\naccuracy, response correctness, contextual coherence, and memory storage. The\nresults demonstrate that CAIM outperforms baseline frameworks across different\nmetrics, highlighting its context-awareness and potential to improve long-term\nhuman-AI interactions."}
{"id": "2505.12020", "pdf": "https://arxiv.org/pdf/2505.12020", "abs": "https://arxiv.org/abs/2505.12020", "authors": ["Xi Han", "Jingwei Zhang", "Dimitris Samaras", "Fei Hou", "Hong Qin"], "title": "GeoMaNO: Geometric Mamba Neural Operator for Partial Differential Equations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The neural operator (NO) framework has emerged as a powerful tool for solving\npartial differential equations (PDEs). Recent NOs are dominated by the\nTransformer architecture, which offers NOs the capability to capture long-range\ndependencies in PDE dynamics. However, existing Transformer-based NOs suffer\nfrom quadratic complexity, lack geometric rigor, and thus suffer from\nsub-optimal performance on regular grids. As a remedy, we propose the Geometric\nMamba Neural Operator (GeoMaNO) framework, which empowers NOs with Mamba's\nmodeling capability, linear complexity, plus geometric rigor. We evaluate\nGeoMaNO's performance on multiple standard and popularly employed PDE\nbenchmarks, spanning from Darcy flow problems to Navier-Stokes problems.\nGeoMaNO improves existing baselines in solution operator approximation by as\nmuch as 58.9%."}
{"id": "2505.12423", "pdf": "https://arxiv.org/pdf/2505.12423", "abs": "https://arxiv.org/abs/2505.12423", "authors": ["Wenqiao Zhu", "Chao Xu", "Lulu Wang", "Jun Wu"], "title": "PSC: Extending Context Window of Large Language Models via Phase Shift Calibration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Rotary Position Embedding (RoPE) is an efficient position encoding approach\nand is widely utilized in numerous large language models (LLMs). Recently, a\nlot of methods have been put forward to further expand the context window based\non RoPE. The core concept of those methods is to predefine or search for a set\nof factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a\nchallenge for existing methods to predefine an optimal factor due to the\nexponential search space. In view of this, we introduce PSC (Phase Shift\nCalibration), a small module for calibrating the frequencies predefined by\nexisting methods. With the employment of PSC, we demonstrate that many existing\nmethods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted\nextensive experiments across multiple models and tasks. The results demonstrate\nthat (1) when PSC is enabled, the comparative reductions in perplexity increase\nas the context window size is varied from 16k, to 32k, and up to 64k. (2) Our\napproach is broadly applicable and exhibits robustness across a variety of\nmodels and tasks. The code can be found at https://github.com/WNQzhu/PSC."}
{"id": "2505.13098", "pdf": "https://arxiv.org/pdf/2505.13098", "abs": "https://arxiv.org/abs/2505.13098", "authors": ["Lars-Peter Meyer", "Johannes Frey", "Desiree Heim", "Felix Brei", "Claus Stadler", "Kurt Junghanns", "Michael Martin"], "title": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs", "categories": ["cs.AI", "cs.CL", "cs.DB"], "comment": "Peer reviewed publication at ESWC 2025 Resources Track", "summary": "Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks."}
{"id": "2505.12025", "pdf": "https://arxiv.org/pdf/2505.12025", "abs": "https://arxiv.org/abs/2505.12025", "authors": ["Praveen Venkateswaran", "Danish Contractor"], "title": "Spotlight Your Instructions: Instruction-following with Dynamic Attention Steering", "categories": ["cs.LG"], "comment": null, "summary": "In many real-world applications, users rely on natural language instructions\nto guide large language models (LLMs) across a wide range of tasks. These\ninstructions are often complex, diverse, and subject to frequent change.\nHowever, LLMs do not always attend to these instructions reliably, and users\nlack simple mechanisms to emphasize their importance beyond modifying prompt\nwording or structure. To address this, we present an inference-time method that\nenables users to emphasize specific parts of their prompt by steering the\nmodel's attention toward them, aligning the model's perceived importance of\ndifferent prompt tokens with user intent. Unlike prior approaches that are\nlimited to static instructions, require significant offline profiling, or rely\non fixed biases, we dynamically update the proportion of model attention given\nto the user-specified parts--ensuring improved instruction following without\nperformance degradation. We demonstrate that our approach improves instruction\nfollowing across a variety of tasks involving multiple instructions and\ngeneralizes across models of varying scales."}
{"id": "2505.12439", "pdf": "https://arxiv.org/pdf/2505.12439", "abs": "https://arxiv.org/abs/2505.12439", "authors": ["Jinming Zhang", "Yunfei Long"], "title": "Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games", "categories": ["cs.CL"], "comment": null, "summary": "Interactive Fiction games (IF games) are where players interact through\nnatural language commands. While recent advances in Artificial Intelligence\nagents have reignited interest in IF games as a domain for studying\ndecision-making, existing approaches prioritize task-specific performance\nmetrics over human-like comprehension of narrative context and gameplay logic.\nThis work presents a cognitively inspired framework that guides Large Language\nModels (LLMs) to learn and play IF games systematically. Our proposed\n**L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three\nkey components: (1) structured map building to capture spatial and narrative\nrelationships, (2) action learning to identify context-appropriate commands,\nand (3) feedback-driven experience analysis to refine decision-making over\ntime. By aligning LLMs-based agents' behavior with narrative intent and\ncommonsense constraints, LPLH moves beyond purely exploratory strategies to\ndeliver more interpretable, human-like performance. Crucially, this approach\ndraws on cognitive science principles to more closely simulate how human\nplayers read, interpret, and respond within narrative worlds. As a result, LPLH\nreframes the IF games challenge as a learning problem for LLMs-based agents,\noffering a new path toward robust, context-aware gameplay in complex text-based\nenvironments."}
{"id": "2505.13118", "pdf": "https://arxiv.org/pdf/2505.13118", "abs": "https://arxiv.org/abs/2505.13118", "authors": ["Marouane Il Idrissi", "Agathe Fernandes Machado", "Ewen Gallic", "Arthur Charpentier"], "title": "Unveil Sources of Uncertainty: Feature Contribution to Conformal Prediction Intervals", "categories": ["cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "Cooperative game theory methods, notably Shapley values, have significantly\nenhanced machine learning (ML) interpretability. However, existing explainable\nAI (XAI) frameworks mainly attribute average model predictions, overlooking\npredictive uncertainty. This work addresses that gap by proposing a novel,\nmodel-agnostic uncertainty attribution (UA) method grounded in conformal\nprediction (CP). By defining cooperative games where CP interval\nproperties-such as width and bounds-serve as value functions, we systematically\nattribute predictive uncertainty to input features. Extending beyond the\ntraditional Shapley values, we use the richer class of Harsanyi allocations,\nand in particular the proportional Shapley values, which distribute attribution\nproportionally to feature importance. We propose a Monte Carlo approximation\nmethod with robust statistical guarantees to address computational feasibility,\nsignificantly improving runtime efficiency. Our comprehensive experiments on\nsynthetic benchmarks and real-world datasets demonstrate the practical utility\nand interpretative depth of our approach. By combining cooperative game theory\nand conformal prediction, we offer a rigorous, flexible toolkit for\nunderstanding and communicating predictive uncertainty in high-stakes ML\napplications."}
{"id": "2505.12027", "pdf": "https://arxiv.org/pdf/2505.12027", "abs": "https://arxiv.org/abs/2505.12027", "authors": ["Jianxiang Yu", "Jiapeng Zhu", "Hao Qian", "Ziqi Liu", "Zhiqiang Zhang", "Xiang Li"], "title": "Relation-Aware Graph Foundation Model", "categories": ["cs.LG"], "comment": null, "summary": "In recent years, large language models (LLMs) have demonstrated remarkable\ngeneralization capabilities across various natural language processing (NLP)\ntasks. Similarly, graph foundation models (GFMs) have emerged as a promising\ndirection in graph learning, aiming to generalize across diverse datasets\nthrough large-scale pre-training. However, unlike language models that rely on\nexplicit token representations, graphs lack a well-defined unit for\ngeneralization, making it challenging to design effective pre-training\nstrategies. In this work, we propose REEF, a novel framework that leverages\nrelation tokens as the basic units for GFMs. Inspired by the token vocabulary\nin LLMs, we construct a relation vocabulary of relation tokens to store\nrelational information within graphs. To accommodate diverse relations, we\nintroduce two hypernetworks that adaptively generate the parameters of\naggregators and classifiers in graph neural networks based on relation tokens.\nIn addition, we design another hypernetwork to construct dataset-specific\nprojectors and incorporate a dataset-level feature bias into the initial node\nrepresentations, enhancing flexibility across different datasets with the same\nrelation. Further, we adopt graph data augmentation and a mixed-dataset\npre-training strategy, allowing REEF to capture relational diversity more\neffectively and exhibit strong generalization capabilities. Extensive\nexperiments show that REEF significantly outperforms existing methods on both\npre-training and transfer learning tasks, underscoring its potential as a\npowerful foundation model for graph-based applications."}
{"id": "2505.12452", "pdf": "https://arxiv.org/pdf/2505.12452", "abs": "https://arxiv.org/abs/2505.12452", "authors": ["Siyang Wu", "Honglin Bao", "Nadav Kunievsky", "James A. Evans"], "title": "Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment", "categories": ["cs.CL", "cs.CY", "cs.DL", "cs.IR"], "comment": "We commit to fully open-source our patent dataset", "summary": "Large language models (LLMs) increasingly demonstrate signs of conceptual\nunderstanding, yet much of their internal knowledge remains latent, loosely\nstructured, and difficult to access or evaluate. We propose self-questioning as\na lightweight and scalable strategy to improve LLMs' understanding,\nparticularly in domains where success depends on fine-grained semantic\ndistinctions. To evaluate this approach, we introduce a challenging new\nbenchmark of 1.3 million post-2015 computer science patent pairs, characterized\nby dense technical jargon and strategically complex writing. The benchmark\ncenters on a pairwise differentiation task: can a model distinguish between\nclosely related but substantively different inventions? We show that prompting\nLLMs to generate and answer their own questions - targeting the background\nknowledge required for the task - significantly improves performance. These\nself-generated questions and answers activate otherwise underutilized internal\nknowledge. Allowing LLMs to retrieve answers from external scientific texts\nfurther enhances performance, suggesting that model knowledge is compressed and\nlacks the full richness of the training data. We also find that\nchain-of-thought prompting and self-questioning converge, though\nself-questioning remains more effective for improving understanding of\ntechnical concepts. Notably, we uncover an asymmetry in prompting: smaller\nmodels often generate more fundamental, more open-ended, better-aligned\nquestions for mid-sized models than large models with better understanding do,\nrevealing a new strategy for cross-model collaboration. Altogether, our\nfindings establish self-questioning as both a practical mechanism for\nautomatically improving LLM comprehension, especially in domains with sparse\nand underrepresented knowledge, and a diagnostic probe of how internal and\nexternal knowledge are organized."}
{"id": "2505.13126", "pdf": "https://arxiv.org/pdf/2505.13126", "abs": "https://arxiv.org/abs/2505.13126", "authors": ["Liancheng Gong", "Wang Zhu", "Jesse Thomason", "Li Zhang"], "title": "Zero-Shot Iterative Formalization and Planning in Partially Observable Environments", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In planning, using LLMs not to predict plans but to formalize an environment\ninto the Planning Domain Definition Language (PDDL) has been shown to greatly\nimprove performance and control. While most work focused on fully observable\nenvironments, we tackle the more realistic and challenging partially observable\nenvironments where existing methods are incapacitated by the lack of complete\ninformation. We propose PDDLego+, a framework to iteratively formalize, plan,\ngrow, and refine PDDL representations in a zero-shot manner, without needing\naccess to any existing trajectories. On two textual simulated environments, we\nshow that PDDLego+ not only achieves superior performance, but also shows\nrobustness against problem complexity. We also show that the domain knowledge\ncaptured after a successful trial is interpretable and benefits future tasks."}
{"id": "2505.12037", "pdf": "https://arxiv.org/pdf/2505.12037", "abs": "https://arxiv.org/abs/2505.12037", "authors": ["Jiashuo Jiang", "Yiming Zong", "Yinyu Ye"], "title": "Adaptive Resolving Methods for Reinforcement Learning with Function Approximations", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) problems are fundamental in online\ndecision-making and have been instrumental in finding an optimal policy for\nMarkov decision processes (MDPs). Function approximations are usually deployed\nto handle large or infinite state-action space. In our work, we consider the RL\nproblems with function approximation and we develop a new algorithm to solve it\nefficiently. Our algorithm is based on the linear programming (LP)\nreformulation and it resolves the LP at each iteration improved with new data\narrival. Such a resolving scheme enables our algorithm to achieve an\ninstance-dependent sample complexity guarantee, more precisely, when we have\n$N$ data, the output of our algorithm enjoys an instance-dependent\n$\\tilde{O}(1/N)$ suboptimality gap. In comparison to the $O(1/\\sqrt{N})$\nworst-case guarantee established in the previous literature, our\ninstance-dependent guarantee is tighter when the underlying instance is\nfavorable, and the numerical experiments also reveal the efficient empirical\nperformances of our algorithms."}
{"id": "2505.12454", "pdf": "https://arxiv.org/pdf/2505.12454", "abs": "https://arxiv.org/abs/2505.12454", "authors": ["Yuyang Ding", "Dan Qiao", "Juntao Li", "Jiajie Xu", "Pingfu Chao", "Xiaofang Zhou", "Min Zhang"], "title": "Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Distantly supervised named entity recognition (DS-NER) has emerged as a cheap\nand convenient alternative to traditional human annotation methods, enabling\nthe automatic generation of training data by aligning text with external\nresources. Despite the many efforts in noise measurement methods, few works\nfocus on the latent noise distribution between different distant annotation\nmethods. In this work, we explore the effectiveness and robustness of DS-NER by\ntwo aspects: (1) distant annotation techniques, which encompasses both\ntraditional rule-based methods and the innovative large language model\nsupervision approach, and (2) noise assessment, for which we introduce a novel\nframework. This framework addresses the challenges by distinctly categorizing\nthem into the unlabeled-entity problem (UEP) and the noisy-entity problem\n(NEP), subsequently providing specialized solutions for each. Our proposed\nmethod achieves significant improvements on eight real-world distant\nsupervision datasets originating from three different data sources and\ninvolving four distinct annotation techniques, confirming its superiority over\ncurrent state-of-the-art methods."}
{"id": "2505.13175", "pdf": "https://arxiv.org/pdf/2505.13175", "abs": "https://arxiv.org/abs/2505.13175", "authors": ["Siming Sun", "Kai Zhang", "Xuejun Jiang", "Wenchao Meng", "Qinmin Yang"], "title": "Enhancing LLMs for Time Series Forecasting via Structure-Guided Cross-Modal Alignment", "categories": ["cs.AI"], "comment": null, "summary": "The emerging paradigm of leveraging pretrained large language models (LLMs)\nfor time series forecasting has predominantly employed linguistic-temporal\nmodality alignment strategies through token-level or layer-wise feature\nmapping. However, these approaches fundamentally neglect a critical insight:\nthe core competency of LLMs resides not merely in processing localized token\nfeatures but in their inherent capacity to model holistic sequence structures.\nThis paper posits that effective cross-modal alignment necessitates structural\nconsistency at the sequence level. We propose the Structure-Guided Cross-Modal\nAlignment (SGCMA), a framework that fully exploits and aligns the\nstate-transition graph structures shared by time-series and linguistic data as\nsequential modalities, thereby endowing time series with language-like\nproperties and delivering stronger generalization after modality alignment.\nSGCMA consists of two key components, namely Structure Alignment and Semantic\nAlignment. In Structure Alignment, a state transition matrix is learned from\ntext data through Hidden Markov Models (HMMs), and a shallow transformer-based\nMaximum Entropy Markov Model (MEMM) receives the hot-start transition matrix\nand annotates each temporal patch into state probability, ensuring that the\ntemporal representation sequence inherits language-like sequential dynamics. In\nSemantic Alignment, cross-attention is applied between temporal patches and the\ntop-k tokens within each state, and the ultimate temporal embeddings are\nderived by the expected value of these embeddings using a weighted average\nbased on state probabilities. Experiments on multiple benchmarks demonstrate\nthat SGCMA achieves state-of-the-art performance, offering a novel approach to\ncross-modal alignment in time series forecasting."}
{"id": "2505.12038", "pdf": "https://arxiv.org/pdf/2505.12038", "abs": "https://arxiv.org/abs/2505.12038", "authors": ["Ning Lu", "Shengcai Liu", "Jiahao Wu", "Weiyu Chen", "Zhirui Zhang", "Yew-Soon Ong", "Qi Wang", "Ke Tang"], "title": "Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "ICML 2025 Camera Ready", "summary": "Large language models (LLMs) have shown great potential as general-purpose AI\nassistants across various domains. To fully leverage this potential in specific\napplications, many companies provide fine-tuning API services, enabling users\nto upload their own data for LLM customization. However, fine-tuning services\nintroduce a new safety threat: user-uploaded data, whether harmful or benign,\ncan break the model's alignment, leading to unsafe outputs. Moreover, existing\ndefense methods struggle to address the diversity of fine-tuning datasets\n(e.g., varying sizes, tasks), often sacrificing utility for safety or vice\nversa. To address this issue, we propose Safe Delta, a safety-aware\npost-training defense method that adjusts the delta parameters (i.e., the\nparameter change before and after fine-tuning). Specifically, Safe Delta\nestimates the safety degradation, selects delta parameters to maximize utility\nwhile limiting overall safety loss, and applies a safety compensation vector to\nmitigate residual safety loss. Through extensive experiments on four diverse\ndatasets with varying settings, our approach consistently preserves safety\nwhile ensuring that the utility gain from benign datasets remains unaffected."}
{"id": "2505.12474", "pdf": "https://arxiv.org/pdf/2505.12474", "abs": "https://arxiv.org/abs/2505.12474", "authors": ["Weixiao Zhou", "Junnan Zhu", "Gengyao Li", "Xianfu Cheng", "Xinnian Liang", "Feifei Zhai", "Zhoujun Li"], "title": "What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization", "categories": ["cs.CL"], "comment": "Submitted to EMNLP 2025", "summary": "In this work, we investigate the performance of LLMs on a new task that\nrequires combining discussion with background knowledge for summarization. This\naims to address the limitation of outside observer confusion in existing\ndialogue summarization systems due to their reliance solely on discussion\ninformation. To achieve this, we model the task output as background and\nopinion summaries and define two standardized summarization patterns. To\nsupport assessment, we introduce the first benchmark comprising high-quality\nsamples consistently annotated by human experts and propose a novel\nhierarchical evaluation framework with fine-grained, interpretable metrics. We\nevaluate 12 LLMs under structured-prompt and self-reflection paradigms. Our\nfindings reveal: (1) LLMs struggle with background summary retrieval,\ngeneration, and opinion summary integration. (2) Even top LLMs achieve less\nthan 69% average performance across both patterns. (3) Current LLMs lack\nadequate self-evaluation and self-correction capabilities for this task."}
{"id": "2505.13180", "pdf": "https://arxiv.org/pdf/2505.13180", "abs": "https://arxiv.org/abs/2505.13180", "authors": ["Matteo Merler", "Nicola Dainese", "Minttu Alakuijala", "Giovanni Bonetta", "Pietro Ferrazzi", "Yu Tian", "Bernardo Magnini", "Pekka Marttinen"], "title": "ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and Vision-Language Models", "categories": ["cs.AI"], "comment": "9 pages, 5 figures and 1 table in the main text; 43 pages, 9 figures\n  and 16 tables including supplementary material", "summary": "Integrating Large Language Models with symbolic planners is a promising\ndirection for obtaining verifiable and grounded plans compared to planning in\nnatural language, with recent works extending this idea to visual domains using\nVision-Language Models (VLMs). However, rigorous comparison between\nVLM-grounded symbolic approaches and methods that plan directly with a VLM has\nbeen hindered by a lack of common environments, evaluation protocols and model\ncoverage. We introduce ViPlan, the first open-source benchmark for Visual\nPlanning with symbolic predicates and VLMs. ViPlan features a series of\nincreasingly challenging tasks in two domains: a visual variant of the classic\nBlocksworld planning problem and a simulated household robotics environment. We\nbenchmark nine open-source VLM families across multiple sizes, along with\nselected closed models, evaluating both VLM-grounded symbolic planning and\nusing the models directly to propose actions. We find symbolic planning to\noutperform direct VLM planning in Blocksworld, where accurate image grounding\nis crucial, whereas the opposite is true in the household robotics tasks, where\ncommonsense knowledge and the ability to recover from errors are beneficial.\nFinally, we show that across most models and methods, there is no significant\nbenefit to using Chain-of-Thought prompting, suggesting that current VLMs still\nstruggle with visual reasoning."}
{"id": "2505.12040", "pdf": "https://arxiv.org/pdf/2505.12040", "abs": "https://arxiv.org/abs/2505.12040", "authors": ["James Jackaman", "Oliver Sutton"], "title": "Improving regional weather forecasts with neural interpolation", "categories": ["cs.LG"], "comment": null, "summary": "In this paper we design a neural interpolation operator to improve the\nboundary data for regional weather models, which is a challenging problem as we\nare required to map multi-scale dynamics between grid resolutions. In\nparticular, we expose a methodology for approaching the problem through the\nstudy of a simplified model, with a view to generalise the results in this work\nto the dynamical core of regional weather models. Our approach will exploit a\ncombination of techniques from image super-resolution with convolutional neural\nnetworks (CNNs) and residual networks, in addition to building the flow of\natmospheric dynamics into the neural network"}
{"id": "2505.12476", "pdf": "https://arxiv.org/pdf/2505.12476", "abs": "https://arxiv.org/abs/2505.12476", "authors": ["Xiao Long", "Liansheng Zhuang", "Chen Shen", "Shaotian Yan", "Yifei Li", "Shafei Wang"], "title": "Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated impressive\nperformance in Knowledge Graph Question Answering (KGQA) tasks, which aim to\nfind answers based on knowledge graphs (KGs) for natural language questions.\nExisting LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented\nGeneration (GraphRAG) paradigm, which first retrieves reasoning paths from the\nlarge KGs, and then generates the answers based on them. However, these methods\nemphasize the exploration of new optimal reasoning paths in KGs while ignoring\nthe exploitation of historical reasoning paths, which may lead to sub-optimal\nreasoning paths. Additionally, the complex semantics contained in questions may\nlead to the retrieval of inaccurate reasoning paths. To address these issues,\nthis paper proposes a novel and training-free framework for KGQA tasks called\nReward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original\nquestion into a series of simpler and well-defined sub-questions to handle the\ncomplex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided\nby a reward model is introduced to iteratively retrieve weighted reasoning\npaths as contextual knowledge. Finally, it stacks the weighted reasoning paths\naccording to their weights to generate the final answers. Extensive experiments\non four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves\n8.7\\% and 7.0\\% performance improvement over the state-of-the-art method on the\nGrailQA and the WebQSP respectively."}
{"id": "2505.13195", "pdf": "https://arxiv.org/pdf/2505.13195", "abs": "https://arxiv.org/abs/2505.13195", "authors": ["Lili Zhang", "Haomiaomiao Wang", "Long Cheng", "Libao Deng", "Tomas Ward"], "title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities", "categories": ["cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world decision-making systems, understanding their behavioural\nvulnerabilities remains a critical challenge for AI safety and alignment. While\nexisting evaluation metrics focus primarily on reasoning accuracy or factual\ncorrectness, they often overlook whether LLMs are robust to adversarial\nmanipulation or capable of using adaptive strategy in dynamic environments.\nThis paper introduces an adversarial evaluation framework designed to\nsystematically stress-test the decision-making processes of LLMs under\ninteractive and adversarial conditions. Drawing on methodologies from cognitive\npsychology and game theory, our framework probes how models respond in two\ncanonical tasks: the two-armed bandit task and the Multi-Round Trust Task.\nThese tasks capture key aspects of exploration-exploitation trade-offs, social\ncooperation, and strategic flexibility. We apply this framework to several\nstate-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3,\nrevealing model-specific susceptibilities to manipulation and rigidity in\nstrategy adaptation. Our findings highlight distinct behavioral patterns across\nmodels and emphasize the importance of adaptability and fairness recognition\nfor trustworthy AI deployment. Rather than offering a performance benchmark,\nthis work proposes a methodology for diagnosing decision-making weaknesses in\nLLM-based agents, providing actionable insights for alignment and safety\nresearch."}
{"id": "2505.12044", "pdf": "https://arxiv.org/pdf/2505.12044", "abs": "https://arxiv.org/abs/2505.12044", "authors": ["Haixu Wu", "Minghao Guo", "Yuezhou Ma", "Yuanxu Sun", "Jianmin Wang", "Wojciech Matusik", "Mingsheng Long"], "title": "FlashBias: Fast Computation of Attention with Bias", "categories": ["cs.LG"], "comment": null, "summary": "Attention mechanism has emerged as a foundation module of modern deep\nlearning models and has also empowered many milestones in various domains.\nMoreover, FlashAttention with IO-aware speedup resolves the efficiency issue of\nstandard attention, further promoting its practicality. Beyond canonical\nattention, attention with bias also widely exists, such as relative position\nbias in vision and language models and pair representation bias in AlphaFold.\nIn these works, prior knowledge is introduced as an additive bias term of\nattention weights to guide the learning process, which has been proven\nessential for model performance. Surprisingly, despite the common usage of\nattention with bias, its targeted efficiency optimization is still absent,\nwhich seriously hinders its wide applications in complex tasks. Diving into the\ncomputation of FlashAttention, we prove that its optimal efficiency is\ndetermined by the rank of the attention weight matrix. Inspired by this\ntheoretical result, this paper presents FlashBias based on the low-rank\ncompressed sensing theory, which can provide fast-exact computation for many\nwidely used attention biases and a fast-accurate approximation for biases in\ngeneral formalization. FlashBias can fully take advantage of the extremely\noptimized matrix multiplication operation in modern GPUs, achieving 1.5$\\times$\nspeedup for AlphaFold, and over 2$\\times$ speedup for attention with bias in\nvision and language models without loss of accuracy."}
{"id": "2505.12495", "pdf": "https://arxiv.org/pdf/2505.12495", "abs": "https://arxiv.org/abs/2505.12495", "authors": ["Nikita Tatarinov", "Vidhyakshaya Kannan", "Haricharana Srinivasa", "Arnav Raj", "Harpreet Singh Anand", "Varun Singh", "Aditya Luthra", "Ravij Lade", "Agam Shah", "Sudheer Chava"], "title": "KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "The increasing context length of modern language models has created a need\nfor evaluating their ability to retrieve and process information across\nextensive documents. While existing benchmarks test long-context capabilities,\nthey often lack a structured way to systematically vary question complexity. We\nintroduce KG-QAGen (Knowledge-Graph-based Question-Answer Generation), a\nframework that (1) extracts QA pairs at multiple complexity levels (2) by\nleveraging structured representations of financial agreements (3) along three\nkey dimensions -- multi-hop retrieval, set operations, and answer plurality --\nenabling fine-grained assessment of model performance across controlled\ndifficulty levels. Using this framework, we construct a dataset of 20,139 QA\npairs (the largest number among the long-context benchmarks) and open-source a\npart of it. We evaluate 13 proprietary and open-source LLMs and observe that\neven the best-performing models are struggling with set-based comparisons and\nmulti-hop logical inference. Our analysis reveals systematic failure modes tied\nto semantic misinterpretation and inability to handle implicit relations."}
{"id": "2505.13227", "pdf": "https://arxiv.org/pdf/2505.13227", "abs": "https://arxiv.org/abs/2505.13227", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "49 pages, 13 figures", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io."}
{"id": "2505.12046", "pdf": "https://arxiv.org/pdf/2505.12046", "abs": "https://arxiv.org/abs/2505.12046", "authors": ["Andreas Hadjipieris", "Neofytos Dimitriou", "Ognjen Arandjelović"], "title": "Unsupervised Port Berth Identification from Automatic Identification System Data", "categories": ["cs.LG"], "comment": null, "summary": "Port berthing sites are regions of high interest for monitoring and\noptimizing port operations. Data sourced from the Automatic Identification\nSystem (AIS) can be superimposed on berths enabling their real-time monitoring\nand revealing long-term utilization patterns. Ultimately, insights from\nmultiple berths can uncover bottlenecks, and lead to the optimization of the\nunderlying supply chain of the port and beyond. However, publicly available\ndocumentation of port berths, even when available, is frequently incomplete -\ne.g. there may be missing berths or inaccuracies such as incorrect boundary\nboxes - necessitating a more robust, data-driven approach to port berth\nlocalization. In this context, we propose an unsupervised spatial modeling\nmethod that leverages AIS data clustering and hyperparameter optimization to\nidentify berthing sites. Trained on one month of freely available AIS data and\nevaluated across ports of varying sizes, our models significantly outperform\ncompeting methods, achieving a mean Bhattacharyya distance of 0.85 when\ncomparing Gaussian Mixture Models (GMMs) trained on separate data splits,\ncompared to 13.56 for the best existing method. Qualitative comparison with\nsatellite images and existing berth labels further supports the superiority of\nour method, revealing more precise berth boundaries and improved spatial\nresolution across diverse port environments."}
{"id": "2505.12507", "pdf": "https://arxiv.org/pdf/2505.12507", "abs": "https://arxiv.org/abs/2505.12507", "authors": ["Xu Zheng", "Zhuomin Chen", "Esteban Schafir", "Sipeng Chen", "Hojat Allah Salehi", "Haifeng Chen", "Farhad Shirani", "Wei Cheng", "Dongsheng Luo"], "title": "LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The impressive ability of large language models to generate natural text\nacross various tasks has led to critical challenges in authorship\nauthentication. Although numerous detection methods have been developed to\ndifferentiate between machine-generated texts (MGT) and human-generated texts\n(HGT), the explainability of these methods remains a significant gap.\nTraditional explainability techniques often fall short in capturing the complex\nword relationships that distinguish HGT from MGT. To address this limitation,\nwe present LM$^2$otifs, a novel explainable framework for MGT detection.\nInspired by probabilistic graphical models, we provide a theoretical rationale\nfor the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networks\nto achieve both accurate detection and interpretability. The LM$^2$otifs\npipeline operates in three key stages: first, it transforms text into graphs\nbased on word co-occurrence to represent lexical dependencies; second, graph\nneural networks are used for prediction; and third, a post-hoc explainability\nmethod extracts interpretable motifs, offering multi-level explanations from\nindividual words to sentence structures. Extensive experiments on multiple\nbenchmark datasets demonstrate the comparable performance of LM$^2$otifs. The\nempirical evaluation of the extracted explainable motifs confirms their\neffectiveness in differentiating HGT and MGT. Furthermore, qualitative analysis\nreveals distinct and visible linguistic fingerprints characteristic of MGT."}
{"id": "2505.13232", "pdf": "https://arxiv.org/pdf/2505.13232", "abs": "https://arxiv.org/abs/2505.13232", "authors": ["Younghyun Kim", "Jongheon Jeong", "Sangkyung Kwak", "Kyungmin Lee", "Juho Lee", "Jinwoo Shin"], "title": "StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Learning robust representations from data often requires scale, which has led\nto the success of recent zero-shot models such as CLIP. However, the obtained\nrobustness can easily be deteriorated when these models are fine-tuned on other\ndownstream tasks (e.g., of smaller scales). Previous works often interpret this\nphenomenon in the context of domain shift, developing fine-tuning methods that\naim to preserve the original domain as much as possible. However, in a\ndifferent context, fine-tuned models with limited data are also prone to\nlearning features that are spurious to humans, such as background or texture.\nIn this paper, we propose StarFT (Spurious Textual Alignment Regularization), a\nnovel framework for fine-tuning zero-shot models to enhance robustness by\npreventing them from learning spuriosity. We introduce a regularization that\naligns the output distribution for spuriosity-injected labels with the original\nzero-shot model, ensuring that the model is not induced to extract irrelevant\nfeatures further from these descriptions.We leverage recent language models to\nget such spuriosity-injected labels by generating alternative textual\ndescriptions that highlight potentially confounding features.Extensive\nexperiments validate the robust generalization of StarFT and its emerging\nproperties: zero-shot group robustness and improved zero-shot classification.\nNotably, StarFT boosts both worst-group and average accuracy by 14.30% and\n3.02%, respectively, in the Waterbirds group shift scenario, where other robust\nfine-tuning baselines show even degraded performance."}
{"id": "2505.12049", "pdf": "https://arxiv.org/pdf/2505.12049", "abs": "https://arxiv.org/abs/2505.12049", "authors": ["Mehran Shakerinava", "Siamak Ravanbakhsh", "Adam Oberman"], "title": "Beyond Scalar Rewards: An Axiomatic Framework for Lexicographic MDPs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent work has formalized the reward hypothesis through the lens of expected\nutility theory, by interpreting reward as utility. Hausner's foundational work\nshowed that dropping the continuity axiom leads to a generalization of expected\nutility theory where utilities are lexicographically ordered vectors of\narbitrary dimension. In this paper, we extend this result by identifying a\nsimple and practical condition under which preferences cannot be represented by\nscalar rewards, necessitating a 2-dimensional reward function. We provide a\nfull characterization of such reward functions, as well as the general\nd-dimensional case, in Markov Decision Processes (MDPs) under a memorylessness\nassumption on preferences. Furthermore, we show that optimal policies in this\nsetting retain many desirable properties of their scalar-reward counterparts,\nwhile in the Constrained MDP (CMDP) setting -- another common multiobjective\nsetting -- they do not."}
{"id": "2505.12511", "pdf": "https://arxiv.org/pdf/2505.12511", "abs": "https://arxiv.org/abs/2505.12511", "authors": ["Yanting Li", "Jiyue Jiang", "Zikang Wang", "Ziqian Lin", "Dongchen He", "Yuheng Shan", "Yanruisheng Shao", "Jiayi Li", "Xiangyu Shi", "Jiuming Wang", "Yanyu Chen", "Yimin Fan", "Han Li", "Yu Li"], "title": "DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design", "categories": ["cs.CL"], "comment": null, "summary": "Inverse Protein Folding (IPF) is a critical subtask in the field of protein\ndesign, aiming to engineer amino acid sequences capable of folding correctly\ninto a specified three-dimensional (3D) conformation. Although substantial\nprogress has been achieved in recent years, existing methods generally rely on\neither backbone coordinates or molecular surface features alone, which\nrestricts their ability to fully capture the complex chemical and geometric\nconstraints necessary for precise sequence prediction. To address this\nlimitation, we present DS-ProGen, a dual-structure deep language model for\nfunctional protein design, which integrates both backbone geometry and\nsurface-level representations. By incorporating backbone coordinates as well as\nsurface chemical and geometric descriptors into a next-amino-acid prediction\nparadigm, DS-ProGen is able to generate functionally relevant and structurally\nstable sequences while satisfying both global and local conformational\nconstraints. On the PRIDE dataset, DS-ProGen attains the current\nstate-of-the-art recovery rate of 61.47%, demonstrating the synergistic\nadvantage of multi-modal structural encoding in protein design. Furthermore,\nDS-ProGen excels in predicting interactions with a variety of biological\npartners, including ligands, ions, and RNA, confirming its robust functional\nretention capabilities."}
{"id": "2505.13246", "pdf": "https://arxiv.org/pdf/2505.13246", "abs": "https://arxiv.org/abs/2505.13246", "authors": ["Roberto Pugliese", "George Kourousias", "Francesco Venier", "Grazia Garlatti Costa"], "title": "Agentic Publications: An LLM-Driven Framework for Interactive Scientific Publishing, Supplementing Traditional Papers with AI-Powered Knowledge Systems", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "The exponential growth of scientific literature presents significant\nchallenges for researchers navigating the complex knowledge landscape. We\npropose \"Agentic Publications\", a novel LLM-driven framework complementing\ntraditional publishing by transforming papers into interactive knowledge\nsystems. Our architecture integrates structured data with unstructured content\nthrough retrieval-augmented generation and multi-agent verification. The\nframework offers interfaces for both humans and machines, combining narrative\nexplanations with machine-readable outputs while addressing ethical\nconsiderations through automated validation and transparent governance. Key\nfeatures include continuous knowledge updates, automatic integration of new\nfindings, and customizable detail levels. Our proof-of-concept demonstrates\nmultilingual interaction, API accessibility, and structured knowledge\nrepresentation through vector databases, knowledge graphs, and verification\nagents. This approach enhances scientific communication across disciplines,\nimproving efficiency and collaboration while preserving traditional publishing\npathways, particularly valuable for interdisciplinary fields where knowledge\nintegration remains challenging."}
{"id": "2505.12083", "pdf": "https://arxiv.org/pdf/2505.12083", "abs": "https://arxiv.org/abs/2505.12083", "authors": ["Jianke Yang", "Manu Bhat", "Bryan Hu", "Yadi Cao", "Nima Dehmamy", "Robin Walters", "Rose Yu"], "title": "Discovering Symbolic Differential Equations with Symmetry Invariants", "categories": ["cs.LG"], "comment": null, "summary": "Discovering symbolic differential equations from data uncovers fundamental\ndynamical laws underlying complex systems. However, existing methods often\nstruggle with the vast search space of equations and may produce equations that\nviolate known physical laws. In this work, we address these problems by\nintroducing the concept of \\textit{symmetry invariants} in equation discovery.\nWe leverage the fact that differential equations admitting a symmetry group can\nbe expressed in terms of differential invariants of symmetry transformations.\nThus, we propose to use these invariants as atomic entities in equation\ndiscovery, ensuring the discovered equations satisfy the specified symmetry.\nOur approach integrates seamlessly with existing equation discovery methods\nsuch as sparse regression and genetic programming, improving their accuracy and\nefficiency. We validate the proposed method through applications to various\nphysical systems, such as fluid and reaction-diffusion, demonstrating its\nability to recover parsimonious and interpretable equations that respect the\nlaws of physics."}
{"id": "2505.12531", "pdf": "https://arxiv.org/pdf/2505.12531", "abs": "https://arxiv.org/abs/2505.12531", "authors": ["Navid Madani", "Rohini Srihari"], "title": "ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) increasingly power mental-health chatbots, yet\nthe field still lacks a scalable, theory-grounded way to decide which model is\nmost effective to deploy. We present ESC-Judge, the first end-to-end evaluation\nframework that (i) grounds head-to-head comparisons of emotional-support LLMs\nin Clara Hill's established Exploration-Insight-Action counseling model,\nproviding a structured and interpretable view of performance, and (ii) fully\nautomates the evaluation pipeline at scale. ESC-Judge operates in three stages:\nfirst, it synthesizes realistic help-seeker roles by sampling empirically\nsalient attributes such as stressors, personality, and life history; second, it\nhas two candidate support agents conduct separate sessions with the same role,\nisolating model-specific strategies; and third, it asks a specialized judge LLM\nto express pairwise preferences across rubric-anchored skills that span the\nExploration, Insight, and Action spectrum. In our study, ESC-Judge matched\nPhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and\n86 percent of Action decisions, demonstrating human-level reliability at a\nfraction of the cost. All code, prompts, synthetic roles, transcripts, and\njudgment scripts are released to promote transparent progress in emotionally\nsupportive AI."}
{"id": "2505.13273", "pdf": "https://arxiv.org/pdf/2505.13273", "abs": "https://arxiv.org/abs/2505.13273", "authors": ["Lucas Berry", "Axel Brando", "Wei-Di Chang", "Juan Camilo Gamboa Higuera", "David Meger"], "title": "Seeing the Unseen: How EMoE Unveils Bias in Text-to-Image Diffusion Models", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Estimating uncertainty in text-to-image diffusion models is challenging\nbecause of their large parameter counts (often exceeding 100 million) and\noperation in complex, high-dimensional spaces with virtually infinite input\npossibilities. In this paper, we propose Epistemic Mixture of Experts (EMoE), a\nnovel framework for efficiently estimating epistemic uncertainty in diffusion\nmodels. EMoE leverages pre-trained networks without requiring additional\ntraining, enabling direct uncertainty estimation from a prompt. We leverage a\nlatent space within the diffusion process that captures epistemic uncertainty\nbetter than existing methods. Experimental results on the COCO dataset\ndemonstrate EMoE's effectiveness, showing a strong correlation between\nuncertainty and image quality. Additionally, EMoE identifies under-sampled\nlanguages and regions with higher uncertainty, revealing hidden biases in the\ntraining set. This capability demonstrates the relevance of EMoE as a tool for\naddressing fairness and accountability in AI-generated content."}
{"id": "2505.12094", "pdf": "https://arxiv.org/pdf/2505.12094", "abs": "https://arxiv.org/abs/2505.12094", "authors": ["M Ruhul Amin"], "title": "Attribution Projection Calculus: A Novel Framework for Causal Inference in Bayesian Networks", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ML", "60E10, 62R07, 68Q32, 68T07, 94A16", "F.2.2; G.3; I.1.2; I.2.6"], "comment": "*AI was used to improve Text and collecting Citations", "summary": "This paper introduces Attribution Projection Calculus (AP-Calculus), a novel\nmathematical framework for determining causal relationships in structured\nBayesian networks. We investigate a specific network architecture with source\nnodes connected to destination nodes through intermediate nodes, where each\ninput maps to a single label with maximum marginal probability. We prove that\nfor each label, exactly one intermediate node acts as a deconfounder while\nothers serve as confounders, enabling optimal attribution of features to their\ncorresponding labels. The framework formalizes the dual nature of intermediate\nnodes as both confounders and deconfounders depending on the context, and\nestablishes separation functions that maximize distinctions between\nintermediate representations. We demonstrate that the proposed network\narchitecture is optimal for causal inference compared to alternative\nstructures, including those based on Pearl's causal framework. AP-Calculus\nprovides a comprehensive mathematical foundation for analyzing feature-label\nattributions, managing spurious correlations, quantifying information gain,\nensuring fairness, and evaluating uncertainty in prediction models, including\nlarge language models. Theoretical verification shows that AP-Calculus not only\nextends but can also subsume traditional do-calculus for many practical\napplications, offering a more direct approach to causal inference in supervised\nlearning contexts."}
{"id": "2505.12533", "pdf": "https://arxiv.org/pdf/2505.12533", "abs": "https://arxiv.org/abs/2505.12533", "authors": ["Varvara Arzt", "Allan Hanbury", "Michael Wiegand", "Gábor Recski", "Terra Blevins"], "title": "Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE", "categories": ["cs.CL"], "comment": null, "summary": "Analysing the generalisation capabilities of relation extraction (RE) models\nis crucial for assessing whether they learn robust relational patterns or rely\non spurious correlations. Our cross-dataset experiments find that RE models\nstruggle with unseen data, even within similar domains. Notably, higher\nintra-dataset performance does not indicate better transferability, instead\noften signaling overfitting to dataset-specific artefacts. Our results also\nshow that data quality, rather than lexical similarity, is key to robust\ntransfer, and the choice of optimal adaptation strategy depends on the quality\nof data available: while fine-tuning yields the best cross-dataset performance\nwith high-quality data, few-shot in-context learning (ICL) is more effective\nwith noisier data. However, even in these cases, zero-shot baselines\noccasionally outperform all cross-dataset results. Structural issues in RE\nbenchmarks, such as single-relation per sample constraints and non-standardised\nnegative class definitions, further hinder model transferability."}
{"id": "2505.13287", "pdf": "https://arxiv.org/pdf/2505.13287", "abs": "https://arxiv.org/abs/2505.13287", "authors": ["João S. Ferreira", "Pierre Fromholz", "Hari Shaji", "James R. Wootton"], "title": "Level Generation with Quantum Reservoir Computing", "categories": ["cs.AI", "quant-ph"], "comment": null, "summary": "Reservoir computing is a form of machine learning particularly suited for\ntime series analysis, including forecasting predictions. We take an\nimplementation of \\emph{quantum} reservoir computing that was initially\ndesigned to generate variants of musical scores and adapt it to create levels\nof Super Mario Bros. Motivated by our analysis of these levels, we develop a\nnew Roblox \\textit{obby} where the courses can be generated in real time on\nsuperconducting qubit hardware, and investigate some of the constraints placed\nby such real-time generation."}
{"id": "2505.12096", "pdf": "https://arxiv.org/pdf/2505.12096", "abs": "https://arxiv.org/abs/2505.12096", "authors": ["Alberto Bassi", "Carlo Albert", "Aurelien Lucchi", "Marco Baity-Jesi", "Emanuele Francazi"], "title": "When the Left Foot Leads to the Right Path: Bridging Initial Prejudice and Trainability", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Understanding the statistical properties of deep neural networks (DNNs) at\ninitialization is crucial for elucidating both their trainability and the\nintrinsic architectural biases they encode prior to data exposure. Mean-field\n(MF) analyses have demonstrated that the parameter distribution in randomly\ninitialized networks dictates whether gradients vanish or explode.\nConcurrently, untrained DNNs were found to exhibit an initial-guessing bias\n(IGB), in which large regions of the input space are assigned to a single\nclass. In this work, we derive a theoretical proof establishing the\ncorrespondence between IGB and previous MF theories, thereby connecting a\nnetwork prejudice toward specific classes with the conditions for fast and\naccurate learning. This connection yields the counter-intuitive conclusion: the\ninitialization that optimizes trainability is necessarily biased, rather than\nneutral. Furthermore, we extend the MF/IGB framework to multi-node activation\nfunctions, offering practical guidelines for designing initialization schemes\nthat ensure stable optimization in architectures employing max- and\naverage-pooling layers."}
{"id": "2505.12543", "pdf": "https://arxiv.org/pdf/2505.12543", "abs": "https://arxiv.org/abs/2505.12543", "authors": ["Md Mehrab Tanjim", "Yeonjun In", "Xiang Chen", "Victor S. Bursztyn", "Ryan A. Rossi", "Sungchul Kim", "Guang-Jie Ren", "Vaishnavi Muppala", "Shun Jiang", "Yongsung Kim", "Chanyoung Park"], "title": "Disambiguation in Conversational Question Answering in the Era of LLM: A Survey", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Ambiguity remains a fundamental challenge in Natural Language Processing\n(NLP) due to the inherent complexity and flexibility of human language. With\nthe advent of Large Language Models (LLMs), addressing ambiguity has become\neven more critical due to their expanded capabilities and applications. In the\ncontext of Conversational Question Answering (CQA), this paper explores the\ndefinition, forms, and implications of ambiguity for language driven systems,\nparticularly in the context of LLMs. We define key terms and concepts,\ncategorize various disambiguation approaches enabled by LLMs, and provide a\ncomparative analysis of their advantages and disadvantages. We also explore\npublicly available datasets for benchmarking ambiguity detection and resolution\ntechniques and highlight their relevance for ongoing research. Finally, we\nidentify open problems and future research directions, proposing areas for\nfurther investigation. By offering a comprehensive review of current research\non ambiguities and disambiguation with LLMs, we aim to contribute to the\ndevelopment of more robust and reliable language systems."}
{"id": "2505.13355", "pdf": "https://arxiv.org/pdf/2505.13355", "abs": "https://arxiv.org/abs/2505.13355", "authors": ["Djallel Bouneffouf", "Raphael Feraud"], "title": "Multi-Armed Bandits Meet Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Bandit algorithms and Large Language Models (LLMs) have emerged as powerful\ntools in artificial intelligence, each addressing distinct yet complementary\nchallenges in decision-making and natural language processing. This survey\nexplores the synergistic potential between these two fields, highlighting how\nbandit algorithms can enhance the performance of LLMs and how LLMs, in turn,\ncan provide novel insights for improving bandit-based decision-making. We first\nexamine the role of bandit algorithms in optimizing LLM fine-tuning, prompt\nengineering, and adaptive response generation, focusing on their ability to\nbalance exploration and exploitation in large-scale learning tasks.\nSubsequently, we explore how LLMs can augment bandit algorithms through\nadvanced contextual understanding, dynamic adaptation, and improved policy\nselection using natural language reasoning. By providing a comprehensive review\nof existing research and identifying key challenges and opportunities, this\nsurvey aims to bridge the gap between bandit algorithms and LLMs, paving the\nway for innovative applications and interdisciplinary research in AI."}
{"id": "2505.12109", "pdf": "https://arxiv.org/pdf/2505.12109", "abs": "https://arxiv.org/abs/2505.12109", "authors": ["Matthew Landers", "Taylor W. Killian", "Thomas Hartvigsen", "Afsaneh Doryab"], "title": "SAINT: Attention-Based Modeling of Sub-Action Dependencies in Multi-Action Policies", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The combinatorial structure of many real-world action spaces leads to\nexponential growth in the number of possible actions, limiting the\neffectiveness of conventional reinforcement learning algorithms. Recent\napproaches for combinatorial action spaces impose factorized or sequential\nstructures over sub-actions, failing to capture complex joint behavior. We\nintroduce the Sub-Action Interaction Network using Transformers (SAINT), a\nnovel policy architecture that represents multi-component actions as unordered\nsets and models their dependencies via self-attention conditioned on the global\nstate. SAINT is permutation-invariant, sample-efficient, and compatible with\nstandard policy optimization algorithms. In 15 distinct combinatorial\nenvironments across three task domains, including environments with nearly 17\nmillion joint actions, SAINT consistently outperforms strong baselines."}
{"id": "2505.12545", "pdf": "https://arxiv.org/pdf/2505.12545", "abs": "https://arxiv.org/abs/2505.12545", "authors": ["Yang Zhao", "Pu Wang", "Yibo Zhao", "Hongru Du", "Hao", "Yang"], "title": "Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models", "categories": ["cs.CL"], "comment": "Last revised 13 Feb 2025. Under review in Nature portfolio", "summary": "Predicting crash events is crucial for understanding crash distributions and\ntheir contributing factors, thereby enabling the design of proactive traffic\nsafety policy interventions. However, existing methods struggle to interpret\nthe complex interplay among various sources of traffic crash data, including\nnumeric characteristics, textual reports, crash imagery, environmental\nconditions, and driver behavior records. As a result, they often fail to\ncapture the rich semantic information and intricate interrelationships embedded\nin these diverse data sources, limiting their ability to identify critical\ncrash risk factors. In this research, we propose TrafficSafe, a framework that\nadapts LLMs to reframe crash prediction and feature attribution as text-based\nreasoning. A multi-modal crash dataset including 58,903 real-world reports\ntogether with belonged infrastructure, environmental, driver, and vehicle\ninformation is collected and textualized into TrafficSafe Event Dataset. By\ncustomizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves\na 42% average improvement in F1-score over baselines. To interpret these\npredictions and uncover contributing factors, we introduce TrafficSafe\nAttribution, a sentence-level feature attribution framework enabling\nconditional risk analysis. Findings show that alcohol-impaired driving is the\nleading factor in severe crashes, with aggressive and impairment-related\nbehaviors having nearly twice the contribution for severe crashes compared to\nother driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal\nfeatures during model training, guiding strategic crash data collection for\niterative performance improvements. The proposed TrafficSafe offers a\ntransformative leap in traffic safety research, providing a blueprint for\ntranslating advanced AI technologies into responsible, actionable, and\nlife-saving outcomes."}
{"id": "2505.13372", "pdf": "https://arxiv.org/pdf/2505.13372", "abs": "https://arxiv.org/abs/2505.13372", "authors": ["Irene Brugnara", "Alessandro Valentini", "Andrea Micheli"], "title": "Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific Temporal Planning Guidance using Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Recent work investigated the use of Reinforcement Learning (RL) for the\nsynthesis of heuristic guidance to improve the performance of temporal planners\nwhen a domain is fixed and a set of training problems (not plans) is given. The\nidea is to extract a heuristic from the value function of a particular\n(possibly infinite-state) MDP constructed over the training problems.\n  In this paper, we propose an evolution of this learning and planning\nframework that focuses on exploiting the information provided by symbolic\nheuristics during both the RL and planning phases. First, we formalize\ndifferent reward schemata for the synthesis and use symbolic heuristics to\nmitigate the problems caused by the truncation of episodes needed to deal with\nthe potentially infinite MDP. Second, we propose learning a residual of an\nexisting symbolic heuristic, which is a \"correction\" of the heuristic value,\ninstead of eagerly learning the whole heuristic from scratch. Finally, we use\nthe learned heuristic in combination with a symbolic heuristic using a\nmultiple-queue planning approach to balance systematic search with imperfect\nlearned information. We experimentally compare all the approaches, highlighting\ntheir strengths and weaknesses and significantly advancing the state of the art\nfor this planning and learning schema."}
{"id": "2505.12129", "pdf": "https://arxiv.org/pdf/2505.12129", "abs": "https://arxiv.org/abs/2505.12129", "authors": ["Yueqi Cao", "Anthea Monod"], "title": "Metric Graph Kernels via the Tropical Torelli Map", "categories": ["cs.LG", "stat.ME", "stat.ML"], "comment": "20 pages, 7 figures", "summary": "We propose new graph kernels grounded in the study of metric graphs via\ntropical algebraic geometry. In contrast to conventional graph kernels that are\nbased on graph combinatorics such as nodes, edges, and subgraphs, our graph\nkernels are purely based on the geometry and topology of the underlying metric\nspace. A key characterizing property of our construction is its invariance\nunder edge subdivision, making the kernels intrinsically well-suited for\ncomparing graphs that represent different underlying spaces. We develop\nefficient algorithms for computing these kernels and analyze their complexity,\nshowing that it depends primarily on the genus of the input graphs.\nEmpirically, our kernels outperform existing methods in label-free settings, as\ndemonstrated on both synthetic and real-world benchmark datasets. We further\nhighlight their practical utility through an urban road network classification\ntask."}
{"id": "2505.12546", "pdf": "https://arxiv.org/pdf/2505.12546", "abs": "https://arxiv.org/abs/2505.12546", "authors": ["A. Feder Cooper", "Aaron Gokaslan", "Amy B. Cyphert", "Christopher De Sa", "Mark A. Lemley", "Daniel E. Ho", "Percy Liang"], "title": "Extracting memorized pieces of (copyrighted) books from open-weight language models", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Plaintiffs and defendants in copyright lawsuits over generative AI often make\nsweeping, opposing claims about the extent to which large language models\n(LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial\nML and copyright law, we show that these polarized positions dramatically\noversimplify the relationship between memorization and copyright. To do so, we\nleverage a recent probabilistic extraction technique to extract pieces of the\nBooks3 dataset from 13 open-weight LLMs. Through numerous experiments, we show\nthat it's possible to extract substantial parts of at least some books from\ndifferent LLMs. This is evidence that the LLMs have memorized the extracted\ntext; this memorized content is copied inside the model parameters. But the\nresults are complicated: the extent of memorization varies both by model and by\nbook. With our specific experiments, we find that the largest LLMs don't\nmemorize most books -- either in whole or in part. However, we also find that\nLlama 3.1 70B memorizes some books, like Harry Potter and 1984, almost\nentirely. We discuss why our results have significant implications for\ncopyright cases, though not ones that unambiguously favor either side."}
{"id": "2505.13380", "pdf": "https://arxiv.org/pdf/2505.13380", "abs": "https://arxiv.org/abs/2505.13380", "authors": ["Nam V. Nguyen", "Huy Nguyen", "Quang Pham", "Van Nguyen", "Savitha Ramasamy", "Nhat Ho"], "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition", "categories": ["cs.AI", "cs.CL"], "comment": "52 pages. This work is an improved version of the previous study at\n  arXiv:2402.02526", "summary": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526"}
{"id": "2505.12137", "pdf": "https://arxiv.org/pdf/2505.12137", "abs": "https://arxiv.org/abs/2505.12137", "authors": ["Can Polat", "Hasan Kurban", "Erchin Serpedin", "Mustafa Kurban"], "title": "Understanding the Capabilities of Molecular Graph Neural Networks in Materials Science Through Multimodal Learning and Physical Context Encoding", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "comment": "Accepted Spotlight Paper at CVPR 2025 for MM4Mat", "summary": "Molecular graph neural networks (GNNs) often focus exclusively on XYZ-based\ngeometric representations and thus overlook valuable chemical context available\nin public databases like PubChem. This work introduces a multimodal framework\nthat integrates textual descriptors, such as IUPAC names, molecular formulas,\nphysicochemical properties, and synonyms, alongside molecular graphs. A gated\nfusion mechanism balances geometric and textual features, allowing models to\nexploit complementary information. Experiments on benchmark datasets indicate\nthat adding textual data yields notable improvements for certain electronic\nproperties, while gains remain limited for others. Furthermore, the GNN\narchitectures display similar performance patterns (improving and deteriorating\non analogous targets), suggesting they learn comparable representations rather\nthan distinctly different physical insights."}
{"id": "2505.12560", "pdf": "https://arxiv.org/pdf/2505.12560", "abs": "https://arxiv.org/abs/2505.12560", "authors": ["Hiram Ring"], "title": "The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations", "categories": ["cs.CL"], "comment": null, "summary": "Existing datasets available for crosslinguistic investigations have tended to\nfocus on large amounts of data for a small group of languages or a small amount\nof data for a large number of languages. This means that claims based on these\ndatasets are limited in what they reveal about universal properties of the\nhuman language faculty. While this has begun to change through the efforts of\nprojects seeking to develop tagged corpora for a large number of languages,\nsuch efforts are still constrained by limits on resources. The current paper\nreports on a large automatically tagged parallel dataset which has been\ndeveloped to partially address this issue. The taggedPBC contains more than\n1,800 sentences of pos-tagged parallel text data from over 1,500 languages,\nrepresenting 133 language families and 111 isolates, dwarfing previously\navailable resources. The accuracy of tags in this dataset is shown to correlate\nwell with both existing SOTA taggers for high-resource languages (SpaCy,\nTrankit) as well as hand-tagged corpora (Universal Dependencies Treebanks).\nAdditionally, a novel measure derived from this dataset, the N1 ratio,\ncorrelates with expert determinations of word order in three typological\ndatabases (WALS, Grambank, Autotyp) such that a Gaussian Naive Bayes classifier\ntrained on this feature can accurately identify basic word order for languages\nnot in those databases. While much work is still needed to expand and develop\nthis dataset, the taggedPBC is an important step to enable corpus-based\ncrosslinguistic investigations, and is made available for research and\ncollaboration via GitHub."}
{"id": "2505.13391", "pdf": "https://arxiv.org/pdf/2505.13391", "abs": "https://arxiv.org/abs/2505.13391", "authors": ["Mikołaj Małkiński", "Jacek Mańdziuk"], "title": "Advancing Generalization Across a Variety of Abstract Visual Reasoning Tasks", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted to the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "The abstract visual reasoning (AVR) domain presents a diverse suite of\nanalogy-based tasks devoted to studying model generalization. Recent years have\nbrought dynamic progress in the field, particularly in i.i.d. scenarios, in\nwhich models are trained and evaluated on the same data distributions.\nNevertheless, o.o.d. setups that assess model generalization to new test\ndistributions remain challenging even for the most recent models. To advance\ngeneralization in AVR tasks, we present the Pathways of Normalized Group\nConvolution model (PoNG), a novel neural architecture that features group\nconvolution, normalization, and a parallel design. We consider a wide set of\nAVR benchmarks, including Raven's Progressive Matrices and visual analogy\nproblems with both synthetic and real-world images. The experiments demonstrate\nstrong generalization capabilities of the proposed model, which in several\nsettings outperforms the existing literature methods."}
{"id": "2505.12138", "pdf": "https://arxiv.org/pdf/2505.12138", "abs": "https://arxiv.org/abs/2505.12138", "authors": ["Fei Lu", "Yue Yu"], "title": "Transformer learns the cross-task prior and regularization for in-context learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Transformers have shown a remarkable ability for in-context learning (ICL),\nmaking predictions based on contextual examples. However, while theoretical\nanalyses have explored this prediction capability, the nature of the inferred\ncontext and its utility for downstream predictions remain open questions. This\npaper aims to address these questions by examining ICL for inverse linear\nregression (ILR), where context inference can be characterized by unsupervised\nlearning of underlying weight vectors. Focusing on the challenging scenario of\nrank-deficient inverse problems, where context length is smaller than the\nnumber of unknowns in the weight vectors and regularization is necessary, we\nintroduce a linear transformer to learn the inverse mapping from contextual\nexamples to the underlying weight vector. Our findings reveal that the\ntransformer implicitly learns both a prior distribution and an effective\nregularization strategy, outperforming traditional ridge regression and\nregularization methods. A key insight is the necessity of low task\ndimensionality relative to the context length for successful learning.\nFurthermore, we numerically verify that the error of the transformer estimator\nscales linearly with the noise level, the ratio of task dimension to context\nlength, and the condition number of the input data. These results not only\ndemonstrate the potential of transformers for solving ill-posed inverse\nproblems, but also provide a new perspective towards understanding the\nknowledge extraction mechanism within transformers."}
{"id": "2505.12568", "pdf": "https://arxiv.org/pdf/2505.12568", "abs": "https://arxiv.org/abs/2505.12568", "authors": ["Lekang Jiang", "Chengzu Li", "Stephan Goetz"], "title": "Enriching Patent Claim Generation with European Patent Dataset", "categories": ["cs.CL"], "comment": "18 pages, 13 tables, 4 figures", "summary": "Drafting patent claims is time-intensive, costly, and requires professional\nskill. Therefore, researchers have investigated large language models (LLMs) to\nassist inventors in writing claims. However, existing work has largely relied\non datasets from the United States Patent and Trademark Office (USPTO). To\nenlarge research scope regarding various jurisdictions, drafting conventions,\nand legal standards, we introduce EPD, a European patent dataset. EPD presents\nrich textual data and structured metadata to support multiple patent-related\ntasks, including claim generation. This dataset enriches the field in three\ncritical aspects: (1) Jurisdictional diversity: Patents from different offices\nvary in legal and drafting conventions. EPD fills a critical gap by providing a\nbenchmark for European patents to enable more comprehensive evaluation. (2)\nQuality improvement: EPD offers high-quality granted patents with finalized and\nlegally approved texts, whereas others consist of patent applications that are\nunexamined or provisional. Experiments show that LLMs fine-tuned on EPD\nsignificantly outperform those trained on previous datasets and even GPT-4o in\nclaim quality and cross-domain generalization. (3) Real-world simulation: We\npropose a difficult subset of EPD to better reflect real-world challenges of\nclaim generation. Results reveal that all tested LLMs perform substantially\nworse on these challenging samples, which highlights the need for future\nresearch."}
{"id": "2505.13400", "pdf": "https://arxiv.org/pdf/2505.13400", "abs": "https://arxiv.org/abs/2505.13400", "authors": ["Ali Essam Ghareeb", "Benjamin Chang", "Ludovico Mitchener", "Angela Yiu", "Caralyn J. Szostkiewicz", "Jon M. Laurent", "Muhammed T. Razzak", "Andrew D. White", "Michaela M. Hinks", "Samuel G. Rodriques"], "title": "Robin: A multi-agent system for automating scientific discovery", "categories": ["cs.AI", "cs.MA", "q-bio.QM"], "comment": null, "summary": "Scientific discovery is driven by the iterative process of background\nresearch, hypothesis generation, experimentation, and data analysis. Despite\nrecent advancements in applying artificial intelligence to scientific\ndiscovery, no system has yet automated all of these stages in a single\nworkflow. Here, we introduce Robin, the first multi-agent system capable of\nfully automating the key intellectual steps of the scientific process. By\nintegrating literature search agents with data analysis agents, Robin can\ngenerate hypotheses, propose experiments, interpret experimental results, and\ngenerate updated hypotheses, achieving a semi-autonomous approach to scientific\ndiscovery. By applying this system, we were able to identify a novel treatment\nfor dry age-related macular degeneration (dAMD), the major cause of blindness\nin the developed world. Robin proposed enhancing retinal pigment epithelium\nphagocytosis as a therapeutic strategy, and identified and validated a\npromising therapeutic candidate, ripasudil. Ripasudil is a clinically-used rho\nkinase (ROCK) inhibitor that has never previously been proposed for treating\ndAMD. To elucidate the mechanism of ripasudil-induced upregulation of\nphagocytosis, Robin then proposed and analyzed a follow-up RNA-seq experiment,\nwhich revealed upregulation of ABCA1, a critical lipid efflux pump and possible\nnovel target. All hypotheses, experimental plans, data analyses, and data\nfigures in the main text of this report were produced by Robin. As the first AI\nsystem to autonomously discover and validate a novel therapeutic candidate\nwithin an iterative lab-in-the-loop framework, Robin establishes a new paradigm\nfor AI-driven scientific discovery."}
{"id": "2505.12143", "pdf": "https://arxiv.org/pdf/2505.12143", "abs": "https://arxiv.org/abs/2505.12143", "authors": ["Arun Kumar", "Paul Schrater"], "title": "Structured Representation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Invariant representations are core to representation learning, yet a central\nchallenge remains: uncovering invariants that are stable and transferable\nwithout suppressing task-relevant signals. This raises fundamental questions,\nrequiring further inquiry, about the appropriate level of abstraction at which\nsuch invariants should be defined, and which aspects of a system they should\ncharacterize. Interpretation of the environment relies on abstract knowledge\nstructures to make sense of the current state, which leads to interactions,\nessential drivers of learning and knowledge acquisition. We posit that\ninterpretation operates at the level of higher-order relational knowledge;\nhence, invariant structures must be where knowledge resides, specifically, as\npartitions defined by the closure of relational paths within an abstract\nknowledge space. These partitions serve as the core invariant representations,\nforming the structural substrate where knowledge is stored and learning occurs.\nOn the other hand, inter-partition connectors enable the deployment of these\nknowledge partitions encoding task-relevant transitions. Thus, invariant\npartitions provide the foundational primitives of structured representation. We\nformalize the computational foundations for structured representation of the\ninvariant partitions based on closed semiring, a relational algebraic\nstructure."}
{"id": "2505.12572", "pdf": "https://arxiv.org/pdf/2505.12572", "abs": "https://arxiv.org/abs/2505.12572", "authors": ["Hanwen Shen", "Ting Ying"], "title": "Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Writing novels with Large Language Models (LLMs) raises a critical question:\nhow much human-authored outline is necessary to generate high-quality\nmillion-word novels? While frameworks such as DOME, Plan&Write, and Long Writer\nhave improved stylistic coherence and logical consistency, they primarily\ntarget shorter novels (10k--100k words), leaving ultra-long generation largely\nunexplored. Drawing on insights from recent text compression methods like\nLLMZip and LLM2Vec, we conduct an information-theoretic analysis that\nquantifies distortion occurring when LLMs compress and reconstruct ultra-long\nnovels under varying compression-expansion ratios. We introduce a hierarchical\ntwo-stage generation pipeline (outline -> detailed outline -> manuscript) and\nfind an optimal outline length that balances information preservation with\nhuman effort. Through extensive experimentation with Chinese novels, we\nestablish that a two-stage hierarchical outline approach significantly reduces\nsemantic distortion compared to single-stage methods. Our findings provide\nempirically-grounded guidance for authors and researchers collaborating with\nLLMs to create million-word novels."}
{"id": "2505.13406", "pdf": "https://arxiv.org/pdf/2505.13406", "abs": "https://arxiv.org/abs/2505.13406", "authors": ["Rong Bian", "Yu Geng", "Zijian Yang", "Bing Cheng"], "title": "AutoMathKG: The automated mathematical knowledge graph based on LLM and vector database", "categories": ["cs.AI"], "comment": null, "summary": "A mathematical knowledge graph (KG) presents knowledge within the field of\nmathematics in a structured manner. Constructing a math KG using natural\nlanguage is an essential but challenging task. There are two major limitations\nof existing works: first, they are constrained by corpus completeness, often\ndiscarding or manually supplementing incomplete knowledge; second, they\ntypically fail to fully automate the integration of diverse knowledge sources.\nThis paper proposes AutoMathKG, a high-quality, wide-coverage, and\nmulti-dimensional math KG capable of automatic updates. AutoMathKG regards\nmathematics as a vast directed graph composed of Definition, Theorem, and\nProblem entities, with their reference relationships as edges. It integrates\nknowledge from ProofWiki, textbooks, arXiv papers, and TheoremQA, enhancing\nentities and relationships with large language models (LLMs) via in-context\nlearning for data augmentation. To search for similar entities, MathVD, a\nvector database, is built through two designed embedding strategies using\nSBERT. To automatically update, two mechanisms are proposed. For knowledge\ncompletion mechanism, Math LLM is developed to interact with AutoMathKG,\nproviding missing proofs or solutions. For knowledge fusion mechanism, MathVD\nis used to retrieve similar entities, and LLM is used to determine whether to\nmerge with a candidate or add as a new entity. A wide range of experiments\ndemonstrate the advanced performance and broad applicability of the AutoMathKG\nsystem, including superior reachability query results in MathVD compared to\nfive baselines and robust mathematical reasoning capability in Math LLM."}
{"id": "2505.12147", "pdf": "https://arxiv.org/pdf/2505.12147", "abs": "https://arxiv.org/abs/2505.12147", "authors": ["Nikolaos-Lysias Kosioris", "Sotirios Nikoletseas", "Gavrilis Filios", "Stefanos Panagiotou"], "title": "Causal Machine Learning in IoT-based Engineering Problems: A Tool Comparison in the Case of Household Energy Consumption", "categories": ["cs.LG"], "comment": null, "summary": "The rapid increase in computing power and the ability to store Big Data in\nthe infrastructure has enabled predictions in a large variety of domains by\nMachine Learning. However, in many cases, existing Machine Learning tools are\nconsidered insufficient or incorrect since they exploit only probabilistic\ndependencies rather than inference logic. Causal Machine Learning methods seem\nto close this gap. In this paper, two prevalent tools based on Causal Machine\nLearning methods are compared, as well as their mathematical underpinning\nbackground. The operation of the tools is demonstrated by examining their\nresponse to 18 queries, based on the IDEAL Household Energy Dataset, published\nby the University of Edinburgh. First, it was important to evaluate the causal\nrelations assumption that allowed the use of this approach; this was based on\nthe preexisting scientific knowledge of the domain and was implemented by use\nof the in-built validation tools. Results were encouraging and may easily be\nextended to other domains."}
{"id": "2505.12584", "pdf": "https://arxiv.org/pdf/2505.12584", "abs": "https://arxiv.org/abs/2505.12584", "authors": ["Omar Mahmoud", "Buddhika Laknath Semage", "Thommen George Karimpanal", "Santu Rana"], "title": "Improving Multilingual Language Models by Aligning Representations through Steering", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we investigate how large language models (LLMS) process\nnon-English tokens within their layer representations, an open question despite\nsignificant advancements in the field. Using representation steering,\nspecifically by adding a learned vector to a single model layer's activations,\nwe demonstrate that steering a single model layer can notably enhance\nperformance. Our analysis shows that this approach achieves results comparable\nto translation baselines and surpasses state of the art prompt optimization\nmethods. Additionally, we highlight how advanced techniques like supervised\nfine tuning (\\textsc{sft}) and reinforcement learning from human feedback\n(\\textsc{rlhf}) improve multilingual capabilities by altering representation\nspaces. We further illustrate how these methods align with our approach to\nreshaping LLMS layer representations."}
{"id": "2505.13408", "pdf": "https://arxiv.org/pdf/2505.13408", "abs": "https://arxiv.org/abs/2505.13408", "authors": ["Jinhe Bi", "Danqi Yan", "Yifan Wang", "Wenke Huang", "Haokun Chen", "Guancheng Wan", "Mang Ye", "Xun Xiao", "Hinrich Schuetze", "Volker Tresp", "Yunpu Ma"], "title": "CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent Large Reasoning Models significantly improve the reasoning ability of\nLarge Language Models by learning to reason, exhibiting the promising\nperformance in solving complex tasks. LRMs solve tasks that require complex\nreasoning by explicitly generating reasoning trajectories together with\nanswers. Nevertheless, judging the quality of such an output answer is not easy\nbecause only considering the correctness of the answer is not enough and the\nsoundness of the reasoning trajectory part matters as well. Logically, if the\nsoundness of the reasoning part is poor, even if the answer is correct, the\nconfidence of the derived answer should be low. Existing methods did consider\njointly assessing the overall output answer by taking into account the\nreasoning part, however, their capability is still not satisfactory as the\ncausal relationship of the reasoning to the concluded answer cannot properly\nreflected. In this paper, inspired by classical mechanics, we present a novel\napproach towards establishing a CoT-Kinetics energy equation. Specifically, our\nCoT-Kinetics energy equation formulates the token state transformation process,\nwhich is regulated by LRM internal transformer layers, as like a particle\nkinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy\nassigns a scalar score to evaluate specifically the soundness of the reasoning\nphase, telling how confident the derived answer could be given the evaluated\nreasoning. As such, the LRM's overall output quality can be accurately\nmeasured, rather than a coarse judgment (e.g., correct or incorrect) anymore."}
{"id": "2505.12149", "pdf": "https://arxiv.org/pdf/2505.12149", "abs": "https://arxiv.org/abs/2505.12149", "authors": ["Andrés Guzmán-Cordero", "Felix Dangel", "Gil Goldshlager", "Marius Zeinhofer"], "title": "Improving Energy Natural Gradient Descent through Woodbury, Momentum, and Randomization", "categories": ["cs.LG"], "comment": null, "summary": "Natural gradient methods significantly accelerate the training of\nPhysics-Informed Neural Networks (PINNs), but are often prohibitively costly.\nWe introduce a suite of techniques to improve the accuracy and efficiency of\nenergy natural gradient descent (ENGD) for PINNs. First, we leverage the\nWoodbury formula to dramatically reduce the computational complexity of ENGD.\nSecond, we adapt the Subsampled Projected-Increment Natural Gradient Descent\nalgorithm from the variational Monte Carlo literature to accelerate the\nconvergence. Third, we explore the use of randomized algorithms to further\nreduce the computational cost in the case of large batch sizes. We find that\nrandomization accelerates progress in the early stages of training for\nlow-dimensional problems, and we identify key barriers to attaining\nacceleration in other scenarios. Our numerical experiments demonstrate that our\nmethods outperform previous approaches, achieving the same $L^2$ error as the\noriginal ENGD up to $75\\times$ faster."}
{"id": "2505.12587", "pdf": "https://arxiv.org/pdf/2505.12587", "abs": "https://arxiv.org/abs/2505.12587", "authors": ["Aditeya Baral", "Allen George Ajith", "Roshan Nayak", "Mrityunjay Abhijeet Bhanja"], "title": "CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Code-mixed languages, characterized by frequent within-sentence language\ntransitions, present structural challenges that standard language models fail\nto address. In this work, we propose CMLFormer, an enhanced multi-layer\ndual-decoder Transformer with a shared encoder and synchronized decoder\ncross-attention, designed to model the linguistic and semantic dynamics of\ncode-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with\nswitching point and translation annotations with multiple new objectives\nspecifically aimed at capturing switching behavior, cross-lingual structure,\nand code-mixing complexity. Our experiments show that CMLFormer improves F1\nscore, precision, and accuracy over other approaches on the HASOC-2021\nbenchmark under select pre-training setups. Attention analyses further show\nthat it can identify and attend to switching points, validating its sensitivity\nto code-mixed structure. These results demonstrate the effectiveness of\nCMLFormer's architecture and multi-task pre-training strategy for modeling\ncode-mixed languages."}
{"id": "2505.13427", "pdf": "https://arxiv.org/pdf/2505.13427", "abs": "https://arxiv.org/abs/2505.13427", "authors": ["Lingxiao Du", "Fanqing Meng", "Zongkai Liu", "Zhixiang Zhou", "Ping Luo", "Qiaosheng Zhang", "Wenqi Shao"], "title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable Step-Level Supervision", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) have achieved impressive\nprogress in vision-language understanding, they still struggle with complex\nmulti-step reasoning, often producing logically inconsistent or partially\ncorrect solutions. A key limitation lies in the lack of fine-grained\nsupervision over intermediate reasoning steps. To address this, we propose\nMM-PRM, a process reward model trained within a fully automated, scalable\nframework. We first build MM-Policy, a strong multimodal model trained on\ndiverse mathematical reasoning data. Then, we construct MM-K12, a curated\ndataset of 10,000 multimodal math problems with verifiable answers, which\nserves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based\npipeline, we generate over 700k step-level annotations without human labeling.\nThe resulting PRM is used to score candidate reasoning paths in the Best-of-N\ninference setup and achieves significant improvements across both in-domain\n(MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.)\nbenchmarks. Further analysis confirms the effectiveness of soft labels, smaller\nlearning rates, and path diversity in optimizing PRM performance. MM-PRM\ndemonstrates that process supervision is a powerful tool for enhancing the\nlogical robustness of multimodal reasoning systems. We release all our codes\nand data at https://github.com/ModalMinds/MM-PRM."}
{"id": "2505.12151", "pdf": "https://arxiv.org/pdf/2505.12151", "abs": "https://arxiv.org/abs/2505.12151", "authors": ["Alex Heyman", "Joel Zylberberg"], "title": "Reasoning Large Language Model Errors Arise from Hallucinating Critical Problem Features", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.7"], "comment": "13 pages (9 excluding references and appendices); 7 figures (6\n  excluding appendices)", "summary": "Large language models have recently made great strides in reasoning task\nperformance through chain-of-thought (CoT) strategies trained via reinforcement\nlearning; however, these \"reasoning large language models\" (RLLMs) remain\nimperfect reasoners, and understanding the frequencies and causes of their\nfailure modes is important for both users and developers. We test o1-mini,\no3-mini, DeepSeek-R1, Claude 3.7 Sonnet, Gemini 2.5 Pro Preview, and Grok 3\nMini Beta on graph coloring as a variable-complexity constraint-satisfaction\nlogic problem, and find evidence from both error rate comparisons and\nCoT/explanation text analysis that RLLMs are prone to hallucinate edges not\nspecified in the prompt's description of the graph. This phenomenon persists\nacross multiple problem complexity levels and semantic frames, and it appears\nto account for a significant fraction of the incorrect answers from every\ntested model, and the vast majority of them for some models. Our results\nindicate that RLLMs may possess broader issues with misrepresentation of\nproblem specifics, and we offer suggestions for design choices to mitigate this\nweakness."}
{"id": "2505.12592", "pdf": "https://arxiv.org/pdf/2505.12592", "abs": "https://arxiv.org/abs/2505.12592", "authors": ["Sullam Jeoung", "Yueyan Chen", "Yi Zhang", "Shuai Wang", "Haibo Ding", "Lin Lee Cheong"], "title": "PromptPrism: A Linguistically-Inspired Taxonomy for Prompts", "categories": ["cs.CL"], "comment": null, "summary": "Prompts are the interface for eliciting the capabilities of large language\nmodels (LLMs). Understanding their structure and components is critical for\nanalyzing LLM behavior and optimizing performance. However, the field lacks a\ncomprehensive framework for systematic prompt analysis and understanding. We\nintroduce PromptPrism, a linguistically-inspired taxonomy that enables prompt\nanalysis across three hierarchical levels: functional structure, semantic\ncomponent, and syntactic pattern. We show the practical utility of PromptPrism\nby applying it to three applications: (1) a taxonomy-guided prompt refinement\napproach that automatically improves prompt quality and enhances model\nperformance across a range of tasks; (2) a multi-dimensional dataset profiling\nmethod that extracts and aggregates structural, semantic, and syntactic\ncharacteristics from prompt datasets, enabling comprehensive analysis of prompt\ndistributions and patterns; (3) a controlled experimental framework for prompt\nsensitivity analysis by quantifying the impact of semantic reordering and\ndelimiter modifications on LLM performance. Our experimental results validate\nthe effectiveness of our taxonomy across these applications, demonstrating that\nPromptPrism provides a foundation for refining, profiling, and analyzing\nprompts."}
{"id": "2505.13445", "pdf": "https://arxiv.org/pdf/2505.13445", "abs": "https://arxiv.org/abs/2505.13445", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "categories": ["cs.AI", "cs.CL"], "comment": "code available at https://github.com/xyliu-cs/RISE", "summary": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners."}
{"id": "2505.12167", "pdf": "https://arxiv.org/pdf/2505.12167", "abs": "https://arxiv.org/abs/2505.12167", "authors": ["Yue Deng", "Asadullah Hill Galib", "Xin Lan", "Pang-Ning Tan", "Lifeng Luo"], "title": "FABLE: A Localized, Targeted Adversarial Attack on Weather Forecasting Models", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Deep learning-based weather forecasting models have recently demonstrated\nsignificant performance improvements over gold-standard physics-based\nsimulation tools. However, these models are vulnerable to adversarial attacks,\nwhich raises concerns about their trustworthiness. In this paper, we first\ninvestigate the feasibility of applying existing adversarial attack methods to\nweather forecasting models. We argue that a successful attack should (1) not\nmodify significantly its original inputs, (2) be faithful, i.e., achieve the\ndesired forecast at targeted locations with minimal changes to non-targeted\nlocations, and (3) be geospatio-temporally realistic. However, balancing these\ncriteria is a challenge as existing methods are not designed to preserve the\ngeospatio-temporal dependencies of the original samples. To address this\nchallenge, we propose a novel framework called FABLE (Forecast Alteration By\nLocalized targeted advErsarial attack), which employs a 3D discrete wavelet\ndecomposition to extract the varying components of the geospatio-temporal data.\nBy regulating the magnitude of adversarial perturbations across different\ncomponents, FABLE can generate adversarial inputs that maintain\ngeospatio-temporal coherence while remaining faithful and closely aligned with\nthe original inputs. Experimental results on multiple real-world datasets\ndemonstrate the effectiveness of our framework over baseline methods across\nvarious metrics."}
{"id": "2505.12594", "pdf": "https://arxiv.org/pdf/2505.12594", "abs": "https://arxiv.org/abs/2505.12594", "authors": ["Tiankai Yang", "Junjun Liu", "Wingchun Siu", "Jiahang Wang", "Zhuangzhuang Qian", "Chanjuan Song", "Cheng Cheng", "Xiyang Hu", "Yue Zhao"], "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD."}
{"id": "2309.11091", "pdf": "https://arxiv.org/pdf/2309.11091", "abs": "https://arxiv.org/abs/2309.11091", "authors": ["Chen Jiang", "Kaiming Huang", "Sifeng He", "Xudong Yang", "Wei Zhang", "Xiaobo Zhang", "Yuan Cheng", "Lei Yang", "Qing Wang", "Furong Xu", "Tan Pan", "Wei Chu"], "title": "Learning Segment Similarity and Alignment in Large-Scale Content Based Video Retrieval", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted by ACM MM 2021", "summary": "With the explosive growth of web videos in recent years, large-scale\nContent-Based Video Retrieval (CBVR) becomes increasingly essential in video\nfiltering, recommendation, and copyright protection. Segment-level CBVR\n(S-CBVR) locates the start and end time of similar segments in finer\ngranularity, which is beneficial for user browsing efficiency and infringement\ndetection especially in long video scenarios. The challenge of S-CBVR task is\nhow to achieve high temporal alignment accuracy with efficient computation and\nlow storage consumption. In this paper, we propose a Segment Similarity and\nAlignment Network (SSAN) in dealing with the challenge which is firstly trained\nend-to-end in S-CBVR. SSAN is based on two newly proposed modules in video\nretrieval: (1) An efficient Self-supervised Keyframe Extraction (SKE) module to\nreduce redundant frame features, (2) A robust Similarity Pattern Detection\n(SPD) module for temporal alignment. In comparison with uniform frame\nextraction, SKE not only saves feature storage and search time, but also\nintroduces comparable accuracy and limited extra computation time. In terms of\ntemporal alignment, SPD localizes similar segments with higher accuracy and\nefficiency than existing deep learning methods. Furthermore, we jointly train\nSSAN with SKE and SPD and achieve an end-to-end improvement. Meanwhile, the two\nkey modules SKE and SPD can also be effectively inserted into other video\nretrieval pipelines and gain considerable performance improvements.\nExperimental results on public datasets show that SSAN can obtain higher\nalignment accuracy while saving storage and online query computational cost\ncompared to existing methods."}
{"id": "2505.12171", "pdf": "https://arxiv.org/pdf/2505.12171", "abs": "https://arxiv.org/abs/2505.12171", "authors": ["Jared Boyer", "T. Konstantin Rusch", "Daniela Rus"], "title": "Learning to Dissipate Energy in Oscillatory State-Space Models", "categories": ["cs.LG", "stat.ML"], "comment": "18 pages, 2 figures", "summary": "State-space models (SSMs) are a class of networks for sequence learning that\nbenefit from fixed state size and linear complexity with respect to sequence\nlength, contrasting the quadratic scaling of typical attention mechanisms.\nInspired from observations in neuroscience, Linear Oscillatory State-Space\nmodels (LinOSS) are a recently proposed class of SSMs constructed from layers\nof discretized forced harmonic oscillators. Although these models perform\ncompetitively, leveraging fast parallel scans over diagonal recurrent matrices\nand achieving state-of-the-art performance on tasks with sequence length up to\n50k, LinOSS models rely on rigid energy dissipation (\"forgetting\") mechanisms\nthat are inherently coupled to the timescale of state evolution. As forgetting\nis a crucial mechanism for long-range reasoning, we demonstrate the\nrepresentational limitations of these models and introduce Damped Linear\nOscillatory State-Space models (D-LinOSS), a more general class of oscillatory\nSSMs that learn to dissipate latent state energy on multiple timescales. We\nanalyze the spectral distribution of the model's recurrent matrices and prove\nthat the SSM layers exhibit stable dynamics under simple, flexible\nparameterizations. D-LinOSS consistently outperforms previous LinOSS methods on\nlong-range learning tasks, without introducing additional complexity, and\nsimultaneously reduces the hyperparameter search space by 50%."}
{"id": "2505.12616", "pdf": "https://arxiv.org/pdf/2505.12616", "abs": "https://arxiv.org/abs/2505.12616", "authors": ["Shujauddin Syed", "Ted Pedersen"], "title": "Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval", "categories": ["cs.CL", "68T50"], "comment": "SemEval-2025", "summary": "This paper presents the Duluth approach to the SemEval-2025 Task 7 on\nMultilingual and Crosslingual Fact-Checked Claim Retrieval. We implemented a\nTF-IDF-based retrieval system with experimentation on vector dimensions and\ntokenization strategies. Our best-performing configuration used word-level\ntokenization with a vocabulary size of 15,000 features, achieving an average\nsuccess@10 score of 0.78 on the development set and 0.69 on the test set across\nten languages. Our system showed stronger performance on higher-resource\nlanguages but still lagged significantly behind the top-ranked system, which\nachieved 0.96 average success@10. Our findings suggest that though advanced\nneural architectures are increasingly dominant in multilingual retrieval tasks,\nproperly optimized traditional methods like TF-IDF remain competitive\nbaselines, especially in limited compute resource scenarios."}
{"id": "2401.04354", "pdf": "https://arxiv.org/pdf/2401.04354", "abs": "https://arxiv.org/abs/2401.04354", "authors": ["Xuzheng Yu", "Chen Jiang", "Wei Zhang", "Tian Gan", "Linlin Chao", "Jianan Zhao", "Yuan Cheng", "Qingpei Guo", "Wei Chu"], "title": "Knowledge-enhanced Multi-perspective Video Representation Learning for Scene Recognition", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "With the explosive growth of video data in real-world applications, a\ncomprehensive representation of videos becomes increasingly important. In this\npaper, we address the problem of video scene recognition, whose goal is to\nlearn a high-level video representation to classify scenes in videos. Due to\nthe diversity and complexity of video contents in realistic scenarios, this\ntask remains a challenge. Most existing works identify scenes for videos only\nfrom visual or textual information in a temporal perspective, ignoring the\nvaluable information hidden in single frames, while several earlier studies\nonly recognize scenes for separate images in a non-temporal perspective. We\nargue that these two perspectives are both meaningful for this task and\ncomplementary to each other, meanwhile, externally introduced knowledge can\nalso promote the comprehension of videos. We propose a novel two-stream\nframework to model video representations from multiple perspectives, i.e.\ntemporal and non-temporal perspectives, and integrate the two perspectives in\nan end-to-end manner by self-distillation. Besides, we design a\nknowledge-enhanced feature fusion and label prediction method that contributes\nto naturally introducing knowledge into the task of video scene recognition.\nExperiments conducted on a real-world dataset demonstrate the effectiveness of\nour proposed method."}
{"id": "2505.12186", "pdf": "https://arxiv.org/pdf/2505.12186", "abs": "https://arxiv.org/abs/2505.12186", "authors": ["Yuhui Wang", "Rongyi Zhu", "Ting Wang"], "title": "Self-Destructive Language Model", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Harmful fine-tuning attacks pose a major threat to the security of large\nlanguage models (LLMs), allowing adversaries to compromise safety guardrails\nwith minimal harmful data. While existing defenses attempt to reinforce LLM\nalignment, they fail to address models' inherent \"trainability\" on harmful\ndata, leaving them vulnerable to stronger attacks with increased learning rates\nor larger harmful datasets. To overcome this critical limitation, we introduce\nSEAM, a novel alignment-enhancing defense that transforms LLMs into\nself-destructive models with intrinsic resilience to misalignment attempts.\nSpecifically, these models retain their capabilities for legitimate tasks while\nexhibiting substantial performance degradation when fine-tuned on harmful data.\nThe protection is achieved through a novel loss function that couples the\noptimization trajectories of benign and harmful data, enhanced with adversarial\ngradient ascent to amplify the self-destructive effect. To enable practical\ntraining, we develop an efficient Hessian-free gradient estimate with\ntheoretical error bounds. Extensive evaluation across LLMs and datasets\ndemonstrates that SEAM creates a no-win situation for adversaries: the\nself-destructive models achieve state-of-the-art robustness against\nlow-intensity attacks and undergo catastrophic performance collapse under\nhigh-intensity attacks, rendering them effectively unusable. (warning: this\npaper contains potentially harmful content generated by LLMs.)"}
{"id": "2505.12621", "pdf": "https://arxiv.org/pdf/2505.12621", "abs": "https://arxiv.org/abs/2505.12621", "authors": ["João Eduardo Batista", "Emil Vatai", "Mohamed Wahib"], "title": "Think Before You Attribute: Improving the Performance of LLMs Attribution Systems", "categories": ["cs.CL", "cs.IR"], "comment": "22 pages (9 pages of content, 4 pages of references, 9 pages of\n  supplementary material), 7 figures, 10 tables", "summary": "Large Language Models (LLMs) are increasingly applied in various science\ndomains, yet their broader adoption remains constrained by a critical\nchallenge: the lack of trustworthy, verifiable outputs. Current LLMs often\ngenerate answers without reliable source attribution, or worse, with incorrect\nattributions, posing a barrier to their use in scientific and high-stakes\nsettings, where traceability and accountability are non-negotiable. To be\nreliable, attribution systems need high accuracy and retrieve data with short\nlengths, i.e., attribute to a sentence within a document rather than a whole\ndocument. We propose a sentence-level pre-attribution step for\nRetrieve-Augmented Generation (RAG) systems that classify sentences into three\ncategories: not attributable, attributable to a single quote, and attributable\nto multiple quotes. By separating sentences before attribution, a proper\nattribution method can be selected for the type of sentence, or the attribution\ncan be skipped altogether. Our results indicate that classifiers are\nwell-suited for this task. In this work, we propose a pre-attribution step to\nreduce the computational complexity of attribution, provide a clean version of\nthe HAGRID dataset, and provide an end-to-end attribution system that works out\nof the box."}
{"id": "2505.11520", "pdf": "https://arxiv.org/pdf/2505.11520", "abs": "https://arxiv.org/abs/2505.11520", "authors": ["Himaja Papala", "Daniel Polani", "Stas Tiomkin"], "title": "Decentralized Traffic Flow Optimization Through Intrinsic Motivation", "categories": ["physics.soc-ph", "cs.AI"], "comment": "9 pages, 6 figures, Published in the Proceedings of the 2024 IEEE\n  27th International Conference on Intelligent Transportation Systems (ITSC)", "summary": "Traffic congestion has long been an ubiquitous problem that is exacerbating\nwith the rapid growth of megacities. In this proof-of-concept work we study\nintrinsic motivation, implemented via the empowerment principle, to control\nautonomous car behavior to improve traffic flow. In standard models of traffic\ndynamics, self-organized traffic jams emerge spontaneously from the individual\nbehavior of cars, affecting traffic over long distances. Our novel car behavior\nstrategy improves traffic flow while still being decentralized and using only\nlocally available information without explicit coordination. Decentralization\nis essential for various reasons, not least to be able to absorb robustly\nsubstantial levels of uncertainty. Our scenario is based on the\nwell-established traffic dynamics model, the Nagel-Schreckenberg cellular\nautomaton. In a fraction of the cars in this model, we substitute the default\nbehavior by empowerment, our intrinsic motivation-based method. This proposed\nmodel significantly improves overall traffic flow, mitigates congestion, and\nreduces the average traffic jam time."}
{"id": "2505.12192", "pdf": "https://arxiv.org/pdf/2505.12192", "abs": "https://arxiv.org/abs/2505.12192", "authors": ["Riad Hossain", "Muhammad Ashad Kabir", "Arat Ibne Golam Mowla", "Animesh Chandra Roy", "Ranjit Kumar Ghosh"], "title": "BenSParX: A Robust Explainable Machine Learning Framework for Parkinson's Disease Detection from Bengali Conversational Speech", "categories": ["cs.LG", "cs.SD", "eess.AS"], "comment": "46 pages, 16 figures", "summary": "Parkinson's disease (PD) poses a growing global health challenge, with\nBangladesh experiencing a notable rise in PD-related mortality. Early detection\nof PD remains particularly challenging in resource-constrained settings, where\nvoice-based analysis has emerged as a promising non-invasive and cost-effective\nalternative. However, existing studies predominantly focus on English or other\nmajor languages; notably, no voice dataset for PD exists for Bengali - posing a\nsignificant barrier to culturally inclusive and accessible healthcare\nsolutions. Moreover, most prior studies employed only a narrow set of acoustic\nfeatures, with limited or no hyperparameter tuning and feature selection\nstrategies, and little attention to model explainability. This restricts the\ndevelopment of a robust and generalizable machine learning model. To address\nthis gap, we present BenSparX, the first Bengali conversational speech dataset\nfor PD detection, along with a robust and explainable machine learning\nframework tailored for early diagnosis. The proposed framework incorporates\ndiverse acoustic feature categories, systematic feature selection methods, and\nstate-of-the-art machine learning algorithms with extensive hyperparameter\noptimization. Furthermore, to enhance interpretability and trust in model\npredictions, the framework incorporates SHAP (SHapley Additive exPlanations)\nanalysis to quantify the contribution of individual acoustic features toward PD\ndetection. Our framework achieves state-of-the-art performance, yielding an\naccuracy of 95.77%, F1 score of 95.57%, and AUC-ROC of 0.982. We further\nexternally validated our approach by applying the framework to existing PD\ndatasets in other languages, where it consistently outperforms state-of-the-art\napproaches. To facilitate further research and reproducibility, the dataset has\nbeen made publicly available at https://github.com/Riad071/BenSParX."}
{"id": "2505.12625", "pdf": "https://arxiv.org/pdf/2505.12625", "abs": "https://arxiv.org/abs/2505.12625", "authors": ["Ali Naseh", "Harsh Chaudhari", "Jaechul Roh", "Mingshi Wu", "Alina Oprea", "Amir Houmansadr"], "title": "R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "DeepSeek recently released R1, a high-performing large language model (LLM)\noptimized for reasoning tasks. Despite its efficient training pipeline, R1\nachieves competitive performance, even surpassing leading reasoning models like\nOpenAI's o1 on several benchmarks. However, emerging reports suggest that R1\nrefuses to answer certain prompts related to politically sensitive topics in\nChina. While existing LLMs often implement safeguards to avoid generating\nharmful or offensive outputs, R1 represents a notable shift - exhibiting\ncensorship-like behavior on politically charged queries. In this paper, we\ninvestigate this phenomenon by first introducing a large-scale set of heavily\ncurated prompts that get censored by R1, covering a range of politically\nsensitive topics, but are not censored by other models. We then conduct a\ncomprehensive analysis of R1's censorship patterns, examining their\nconsistency, triggers, and variations across topics, prompt phrasing, and\ncontext. Beyond English-language queries, we explore censorship behavior in\nother languages. We also investigate the transferability of censorship to\nmodels distilled from the R1 language model. Finally, we propose techniques for\nbypassing or removing this censorship. Our findings reveal possible additional\ncensorship integration likely shaped by design choices during training or\nalignment, raising concerns about transparency, bias, and governance in\nlanguage model deployment."}
{"id": "2505.11526", "pdf": "https://arxiv.org/pdf/2505.11526", "abs": "https://arxiv.org/abs/2505.11526", "authors": ["Tianxing Yang", "Huigen Ye", "Hua Xu"], "title": "Code Retrieval for MILP Instance Generation", "categories": ["math.OC", "cs.AI"], "comment": null, "summary": "Mixed-Integer Linear Programming (MILP) is widely used in fields such as\nscheduling, logistics, and planning. Enhancing the performance of MILP solvers,\nparticularly learning-based solvers, requires substantial amounts of\nhigh-quality data. However, existing methods for MILP instance generation\ntypically necessitate training a separate model for each problem class and are\ncomputationally intensive when generating new instances. To address these\nlimitations, we reformulate the MILP Instance Generation task as MILP Code\nGeneration task, enabling efficient, flexible, and interpretable instance\ngeneration through code. Since MILP instances generated from code can vary\nsignificantly in scale, we introduce MILP-EmbedSim, a new similarity metric\nthat accurately measures the similarity between instances of varying sizes\nwithin the same problem class. Leveraging this metric, we propose\nMILP-Retrieval, a pipeline that retrieves generation code from library to\nproduce MILP instances highly similar to target instance. MILP-Retrieval\noutperforms baselines in both MILP Code Generation and Instance Generation\ntasks, provides a novel perspective on MILP instance generation and opens new\npossibilities for learning-based solvers."}
{"id": "2505.12202", "pdf": "https://arxiv.org/pdf/2505.12202", "abs": "https://arxiv.org/abs/2505.12202", "authors": ["Zhenghao Li", "Shengbo Wang", "Nian Si"], "title": "Near-Optimal Sample Complexities of Divergence-based S-rectangular Distributionally Robust Reinforcement Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Distributionally robust reinforcement learning (DR-RL) has recently gained\nsignificant attention as a principled approach that addresses discrepancies\nbetween training and testing environments. To balance robustness, conservatism,\nand computational traceability, the literature has introduced DR-RL models with\nSA-rectangular and S-rectangular adversaries. While most existing statistical\nanalyses focus on SA-rectangular models, owing to their algorithmic simplicity\nand the optimality of deterministic policies, S-rectangular models more\naccurately capture distributional discrepancies in many real-world applications\nand often yield more effective robust randomized policies. In this paper, we\nstudy the empirical value iteration algorithm for divergence-based\nS-rectangular DR-RL and establish near-optimal sample complexity bounds of\n$\\widetilde{O}(|\\mathcal{S}||\\mathcal{A}|(1-\\gamma)^{-4}\\varepsilon^{-2})$,\nwhere $\\varepsilon$ is the target accuracy, $|\\mathcal{S}|$ and $|\\mathcal{A}|$\ndenote the cardinalities of the state and action spaces, and $\\gamma$ is the\ndiscount factor. To the best of our knowledge, these are the first sample\ncomplexity results for divergence-based S-rectangular models that achieve\noptimal dependence on $|\\mathcal{S}|$, $|\\mathcal{A}|$, and $\\varepsilon$\nsimultaneously. We further validate this theoretical dependence through\nnumerical experiments on a robust inventory control problem and a theoretical\nworst-case example, demonstrating the fast learning performance of our proposed\nalgorithm."}
{"id": "2505.12636", "pdf": "https://arxiv.org/pdf/2505.12636", "abs": "https://arxiv.org/abs/2505.12636", "authors": ["Jiakuan Xie", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "title": "Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main", "summary": "Knowledge editing, which aims to update the knowledge encoded in language\nmodels, can be deceptive. Despite the fact that many existing knowledge editing\nalgorithms achieve near-perfect performance on conventional metrics, the models\nedited by them are still prone to generating original knowledge. This paper\nintroduces the concept of \"superficial editing\" to describe this phenomenon.\nOur comprehensive evaluation reveals that this issue presents a significant\nchallenge to existing algorithms. Through systematic investigation, we identify\nand validate two key factors contributing to this issue: (1) the residual\nstream at the last subject position in earlier layers and (2) specific\nattention modules in later layers. Notably, certain attention heads in later\nlayers, along with specific left singular vectors in their output matrices,\nencapsulate the original knowledge and exhibit a causal relationship with\nsuperficial editing. Furthermore, we extend our analysis to the task of\nsuperficial unlearning, where we observe consistent patterns in the behavior of\nspecific attention heads and their corresponding left singular vectors, thereby\ndemonstrating the robustness and broader applicability of our methodology and\nconclusions. Our code is available here."}
{"id": "2505.11528", "pdf": "https://arxiv.org/pdf/2505.11528", "abs": "https://arxiv.org/abs/2505.11528", "authors": ["Yuhang Huang", "JIazhao Zhang", "Shilong Zou", "XInwang Liu", "Ruizhen Hu", "Kai Xu"], "title": "LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Predictive manipulation has recently gained considerable attention in the\nEmbodied AI community due to its potential to improve robot policy performance\nby leveraging predicted states. However, generating accurate future visual\nstates of robot-object interactions from world models remains a well-known\nchallenge, particularly in achieving high-quality pixel-level representations.\nTo this end, we propose LaDi-WM, a world model that predicts the latent space\nof future states using diffusion modeling. Specifically, LaDi-WM leverages the\nwell-established latent space aligned with pre-trained Visual Foundation Models\n(VFMs), which comprises both geometric features (DINO-based) and semantic\nfeatures (CLIP-based). We find that predicting the evolution of the latent\nspace is easier to learn and more generalizable than directly predicting\npixel-level images. Building on LaDi-WM, we design a diffusion policy that\niteratively refines output actions by incorporating forecasted states, thereby\ngenerating more consistent and accurate results. Extensive experiments on both\nsynthetic and real-world benchmarks demonstrate that LaDi-WM significantly\nenhances policy performance by 27.9\\% on the LIBERO-LONG benchmark and 20\\% on\nthe real-world scenario. Furthermore, our world model and policies achieve\nimpressive generalizability in real-world experiments."}
{"id": "2505.12204", "pdf": "https://arxiv.org/pdf/2505.12204", "abs": "https://arxiv.org/abs/2505.12204", "authors": ["Shuo Han", "German Espinosa", "Junda Huang", "Daniel A. Dombeck", "Malcolm A. MacIver", "Bradly C. Stadie"], "title": "Of Mice and Machines: A Comparison of Learning Between Real World Mice and RL Agents", "categories": ["cs.LG", "q-bio.NC"], "comment": "19 pages", "summary": "Recent advances in reinforcement learning (RL) have demonstrated impressive\ncapabilities in complex decision-making tasks. This progress raises a natural\nquestion: how do these artificial systems compare to biological agents, which\nhave been shaped by millions of years of evolution? To help answer this\nquestion, we undertake a comparative study of biological mice and RL agents in\na predator-avoidance maze environment. Through this analysis, we identify a\nstriking disparity: RL agents consistently demonstrate a lack of\nself-preservation instinct, readily risking ``death'' for marginal efficiency\ngains. These risk-taking strategies are in contrast to biological agents, which\nexhibit sophisticated risk-assessment and avoidance behaviors. Towards bridging\nthis gap between the biological and artificial, we propose two novel mechanisms\nthat encourage more naturalistic risk-avoidance behaviors in RL agents. Our\napproach leads to the emergence of naturalistic behaviors, including strategic\nenvironment assessment, cautious path planning, and predator avoidance patterns\nthat closely mirror those observed in biological systems."}
{"id": "2505.12654", "pdf": "https://arxiv.org/pdf/2505.12654", "abs": "https://arxiv.org/abs/2505.12654", "authors": ["Yuxin Lin", "Yinglin Zheng", "Ming Zeng", "Wangzheng Shi"], "title": "Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals", "categories": ["cs.CL", "cs.AI"], "comment": "Accepected by ACL 2025", "summary": "This paper addresses the gap in predicting turn-taking and backchannel\nactions in human-machine conversations using multi-modal signals (linguistic,\nacoustic, and visual). To overcome the limitation of existing datasets, we\npropose an automatic data collection pipeline that allows us to collect and\nannotate over 210 hours of human conversation videos. From this, we construct a\nMulti-Modal Face-to-Face (MM-F2F) human conversation dataset, including over\n1.5M words and corresponding turn-taking and backchannel annotations from\napproximately 20M frames. Additionally, we present an end-to-end framework that\npredicts the probability of turn-taking and backchannel actions from\nmulti-modal signals. The proposed model emphasizes the interrelation between\nmodalities and supports any combination of text, audio, and video inputs,\nmaking it adaptable to a variety of realistic scenarios. Our experiments show\nthat our approach achieves state-of-the-art performance on turn-taking and\nbackchannel prediction tasks, achieving a 10\\% increase in F1-score on\nturn-taking and a 33\\% increase on backchannel prediction. Our dataset and code\nare publicly available online to ease of subsequent research."}
{"id": "2505.11545", "pdf": "https://arxiv.org/pdf/2505.11545", "abs": "https://arxiv.org/abs/2505.11545", "authors": ["Xingyu Ji", "Parker Glenn", "Aditya G. Parameswaran", "Madelon Hulsebos"], "title": "TARGET: Benchmarking Table Retrieval for Generative Tasks", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB"], "comment": null, "summary": "The data landscape is rich with structured data, often of high value to\norganizations, driving important applications in data analysis and machine\nlearning. Recent progress in representation learning and generative models for\nsuch data has led to the development of natural language interfaces to\nstructured data, including those leveraging text-to-SQL. Contextualizing\ninteractions, either through conversational interfaces or agentic components,\nin structured data through retrieval-augmented generation can provide\nsubstantial benefits in the form of freshness, accuracy, and comprehensiveness\nof answers. The key question is: how do we retrieve the right table(s) for the\nanalytical query or task at hand? To this end, we introduce TARGET: a benchmark\nfor evaluating TAble Retrieval for GEnerative Tasks. With TARGET we analyze the\nretrieval performance of different retrievers in isolation, as well as their\nimpact on downstream tasks. We find that dense embedding-based retrievers far\noutperform a BM25 baseline which is less effective than it is for retrieval\nover unstructured text. We also surface the sensitivity of retrievers across\nvarious metadata (e.g., missing table titles), and demonstrate a stark\nvariation of retrieval performance across datasets and tasks. TARGET is\navailable at https://target-benchmark.github.io."}
{"id": "2505.12211", "pdf": "https://arxiv.org/pdf/2505.12211", "abs": "https://arxiv.org/abs/2505.12211", "authors": ["Wenhui Liu", "Zhijian Wu", "Jingchao Wang", "Dingjiang Huang", "Shuigeng Zhou"], "title": "Imagination-Limited Q-Learning for Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Offline reinforcement learning seeks to derive improved policies entirely\nfrom historical data but often struggles with over-optimistic value estimates\nfor out-of-distribution (OOD) actions. This issue is typically mitigated via\npolicy constraint or conservative value regularization methods. However, these\napproaches may impose overly constraints or biased value estimates, potentially\nlimiting performance improvements. To balance exploitation and restriction, we\npropose an Imagination-Limited Q-learning (ILQ) method, which aims to maintain\nthe optimism that OOD actions deserve within appropriate limits. Specifically,\nwe utilize the dynamics model to imagine OOD action-values, and then clip the\nimagined values with the maximum behavior values. Such design maintains\nreasonable evaluation of OOD actions to the furthest extent, while avoiding its\nover-optimism. Theoretically, we prove the convergence of the proposed ILQ\nunder tabular Markov decision processes. Particularly, we demonstrate that the\nerror bound between estimated values and optimality values of OOD state-actions\npossesses the same magnitude as that of in-distribution ones, thereby\nindicating that the bias in value estimates is effectively mitigated.\nEmpirically, our method achieves state-of-the-art performance on a wide range\nof tasks in the D4RL benchmark."}
{"id": "2505.12662", "pdf": "https://arxiv.org/pdf/2505.12662", "abs": "https://arxiv.org/abs/2505.12662", "authors": ["Xukai Liu", "Ye Liu", "Shiwen Wu", "Yanghai Zhang", "Yihao Yuan", "Kai Zhang", "Qi Liu"], "title": "Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have led to impressive\nprogress in natural language generation, yet their tendency to produce\nhallucinated or unsubstantiated content remains a critical concern. To improve\nfactual reliability, Retrieval-Augmented Generation (RAG) integrates external\nknowledge during inference. However, existing RAG systems face two major\nlimitations: (1) unreliable adaptive control due to limited external knowledge\nsupervision, and (2) hallucinations caused by inaccurate or irrelevant\nreferences. To address these issues, we propose Know3-RAG, a knowledge-aware\nRAG framework that leverages structured knowledge from knowledge graphs (KGs)\nto guide three core stages of the RAG process, including retrieval, generation,\nand filtering. Specifically, we introduce a knowledge-aware adaptive retrieval\nmodule that employs KG embedding to assess the confidence of the generated\nanswer and determine retrieval necessity, a knowledge-enhanced reference\ngeneration strategy that enriches queries with KG-derived entities to improve\ngenerated reference relevance, and a knowledge-driven reference filtering\nmechanism that ensures semantic alignment and factual accuracy of references.\nExperiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG\nconsistently outperforms strong baselines, significantly reducing\nhallucinations and enhancing answer reliability."}
{"id": "2505.11546", "pdf": "https://arxiv.org/pdf/2505.11546", "abs": "https://arxiv.org/abs/2505.11546", "authors": ["Xiao Li", "Tianhao Wei", "Changliu Liu", "Anouck Girard", "Ilya Kolmanovsky"], "title": "Control Invariant Sets for Neural Network Dynamical Systems and Recursive Feasibility in Model Predictive Control", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": null, "summary": "Neural networks are powerful tools for data-driven modeling of complex\ndynamical systems, enhancing predictive capability for control applications.\nHowever, their inherent nonlinearity and black-box nature challenge control\ndesigns that prioritize rigorous safety and recursive feasibility guarantees.\nThis paper presents algorithmic methods for synthesizing control invariant sets\nspecifically tailored to neural network based dynamical models. These\nalgorithms employ set recursion, ensuring termination after a finite number of\niterations and generating subsets in which closed-loop dynamics are forward\ninvariant, thus guaranteeing perpetual operational safety. Additionally, we\npropose model predictive control designs that integrate these control invariant\nsets into mixed-integer optimization, with guaranteed adherence to safety\nconstraints and recursive feasibility at the computational level. We also\npresent a comprehensive theoretical analysis examining the properties and\nguarantees of the proposed methods. Numerical simulations in an autonomous\ndriving scenario demonstrate the methods' effectiveness in synthesizing\ncontrol-invariant sets offline and implementing model predictive control\nonline, ensuring safety and recursive feasibility."}
{"id": "2505.12220", "pdf": "https://arxiv.org/pdf/2505.12220", "abs": "https://arxiv.org/abs/2505.12220", "authors": ["Yuhan Zhang", "Yishu Wei", "Yanshan Wang", "Yunyu Xiao", "COL", "Ronald K. Poropatich", "Gretchen L. Haas", "Yiye Zhang", "Chunhua Weng", "Jinze Liu", "Lisa A. Brenner", "James M. Bjork", "Yifan Peng"], "title": "Machine Learning Applications Related to Suicide in Military and Veterans: A Scoping Literature Review", "categories": ["cs.LG"], "comment": null, "summary": "Suicide remains one of the main preventable causes of death among active\nservice members and veterans. Early detection and prediction are crucial in\nsuicide prevention. Machine learning techniques have yielded promising results\nin this area recently. This study aims to assess and summarize current research\nand provides a comprehensive review regarding the application of machine\nlearning techniques in assessing and predicting suicidal ideation, attempts,\nand mortality among members of military and veteran populations.\n  A keyword search using PubMed, IEEE, ACM, and Google Scholar was conducted,\nand the PRISMA protocol was adopted for relevant study selection. Thirty-two\narticles met the inclusion criteria. These studies consistently identified risk\nfactors relevant to mental health issues such as depression, post-traumatic\nstress disorder (PTSD), suicidal ideation, prior attempts, physical health\nproblems, and demographic characteristics.\n  Machine learning models applied in this area have demonstrated reasonable\npredictive accuracy. However, additional research gaps still exist. First, many\nstudies have overlooked metrics that distinguish between false positives and\nnegatives, such as positive predictive value and negative predictive value,\nwhich are crucial in the context of suicide prevention policies. Second, more\ndedicated approaches to handling survival and longitudinal data should be\nexplored. Lastly, most studies focused on machine learning methods, with\nlimited discussion of their connection to clinical rationales.\n  In summary, machine learning analyses have identified a wide range of risk\nfactors associated with suicide in military populations. The diversity and\ncomplexity of these factors also demonstrates that effective prevention\nstrategies must be comprehensive and flexible."}
{"id": "2505.12716", "pdf": "https://arxiv.org/pdf/2505.12716", "abs": "https://arxiv.org/abs/2505.12716", "authors": ["Taiqiang Wu", "Runming Yang", "Jiayi Li", "Pengfei Hu", "Ngai Wong", "Yujiu Yang"], "title": "Shadow-FT: Tuning Instruct via Base", "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Large language models (LLMs) consistently benefit from further fine-tuning on\nvarious tasks. However, we observe that directly tuning the INSTRUCT (i.e.,\ninstruction tuned) models often leads to marginal improvements and even\nperformance degeneration. Notably, paired BASE models, the foundation for these\nINSTRUCT variants, contain highly similar weight values (i.e., less than 2% on\naverage for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to\ntune the INSTRUCT models by leveraging the corresponding BASE models. The key\ninsight is to fine-tune the BASE model, and then directly graft the learned\nweight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no\nadditional parameters, is easy to implement, and significantly improves\nperformance. We conduct extensive experiments on tuning mainstream LLMs, such\nas Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering\ncoding, reasoning, and mathematical tasks. Experimental results demonstrate\nthat Shadow-FT consistently outperforms conventional full-parameter and\nparameter-efficient tuning approaches. Further analyses indicate that Shadow-FT\ncan be applied to multimodal large language models (MLLMs) and combined with\ndirect preference optimization (DPO). Codes and weights are available at\n\\href{https://github.com/wutaiqiang/Shadow-FT}{Github}."}
{"id": "2505.11547", "pdf": "https://arxiv.org/pdf/2505.11547", "abs": "https://arxiv.org/abs/2505.11547", "authors": ["Kyla Guru", "Robert J. Moss", "Mykel J. Kochenderfer"], "title": "On Technique Identification and Threat-Actor Attribution using LLMs and Embedding Models", "categories": ["cs.CR", "cs.AI", "cs.CY"], "comment": null, "summary": "Attribution of cyber-attacks remains a complex but critical challenge for\ncyber defenders. Currently, manual extraction of behavioral indicators from\ndense forensic documentation causes significant attribution delays, especially\nfollowing major incidents at the international scale. This research evaluates\nlarge language models (LLMs) for cyber-attack attribution based on behavioral\nindicators extracted from forensic documentation. We test OpenAI's GPT-4 and\ntext-embedding-3-large for identifying threat actors' tactics, techniques, and\nprocedures (TTPs) by comparing LLM-generated TTPs against human-generated data\nfrom MITRE ATT&CK Groups. Our framework then identifies TTPs from text using\nvector embedding search and builds profiles to attribute new attacks for a\nmachine learning model to learn. Key contributions include: (1) assessing\noff-the-shelf LLMs for TTP extraction and attribution, and (2) developing an\nend-to-end pipeline from raw CTI documents to threat-actor prediction. This\nresearch finds that standard LLMs generate TTP datasets with noise, resulting\nin a low similarity to human-generated datasets. However, the TTPs generated\nare similar in frequency to those within the existing MITRE datasets.\nAdditionally, although these TTPs are different than human-generated datasets,\nour work demonstrates that they still prove useful for training a model that\nperforms above baseline on attribution. Project code and files are contained\nhere: https://github.com/kylag/ttp_attribution."}
{"id": "2505.12225", "pdf": "https://arxiv.org/pdf/2505.12225", "abs": "https://arxiv.org/abs/2505.12225", "authors": ["Jizhou Guo", "Zhaomin Wu", "Philip S. Yu"], "title": "Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "High-quality reward models are crucial for unlocking the reasoning potential\nof large language models (LLMs), with best-of-N voting demonstrating\nsignificant performance gains. However, current reward models, which typically\noperate on the textual output of LLMs, are computationally expensive and\nparameter-heavy, limiting their real-world applications. We introduce the\nEfficient Linear Hidden State Reward (ELHSR) model - a novel, highly\nparameter-efficient approach that leverages the rich information embedded in\nLLM hidden states to address these issues. ELHSR systematically outperform\nbaselines with less than 0.005% of the parameters of baselines, requiring only\na few samples for training. ELHSR also achieves orders-of-magnitude efficiency\nimprovement with significantly less time and fewer FLOPs per sample than\nbaseline reward models. Moreover, ELHSR exhibits robust performance even when\ntrained only on logits, extending its applicability to some closed-source LLMs.\nIn addition, ELHSR can also be combined with traditional reward models to\nachieve additional performance gains."}
{"id": "2505.12717", "pdf": "https://arxiv.org/pdf/2505.12717", "abs": "https://arxiv.org/abs/2505.12717", "authors": ["Haoyuan Wu", "Xueyi Chen", "Rui Ming", "Jilong Gao", "Shoubo Hu", "Zhuolun He", "Bei Yu"], "title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) demonstrate significant reasoning capabilities,\nparticularly through long chain-of-thought (CoT) processes, which can be\nelicited by reinforcement learning (RL). However, prolonged CoT reasoning\npresents limitations, primarily verbose outputs due to excessive introspection.\nThe reasoning process in these LLMs often appears to follow a trial-and-error\nmethodology rather than a systematic, logical deduction. In contrast,\ntree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling\nreasoning as an exploration within a tree structure. This reasoning structure\nfacilitates the parallel generation and evaluation of multiple reasoning\nbranches, allowing for the active identification, assessment, and pruning of\nunproductive paths. This process can potentially lead to improved performance\nand reduced token costs. Building upon the long CoT capability of LLMs, we\nintroduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a\nrule-based reward. ToTRL is designed to guide LLMs in developing the parallel\nToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs\nas players in a puzzle game during the ToTRL training process. Solving puzzle\ngames inherently necessitates exploring interdependent choices and managing\nmultiple constraints, which requires the construction and exploration of a\nthought tree, providing challenging tasks for cultivating the ToT reasoning\ncapability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,\ntrained with our ToTRL, achieves significant improvement in performance and\nreasoning efficiency on complex reasoning tasks."}
{"id": "2505.11548", "pdf": "https://arxiv.org/pdf/2505.11548", "abs": "https://arxiv.org/abs/2505.11548", "authors": ["Zhiyuan Chang", "Xiaojun Jia", "Mingyang Li", "Junjie Wang", "Yuekai Huang", "Qing Wang", "Ziyou Jiang", "Yang Liu"], "title": "One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented Generation Systems", "categories": ["cs.CR", "cs.AI"], "comment": "15pages, 4 figures", "summary": "Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation\n(RAG) have shown improved performance in generating accurate responses.\nHowever, the dependence on external knowledge bases introduces potential\nsecurity vulnerabilities, particularly when these knowledge bases are publicly\naccessible and modifiable. Poisoning attacks on knowledge bases for RAG systems\nface two fundamental challenges: the injected malicious content must compete\nwith multiple authentic documents retrieved by the retriever, and LLMs tend to\ntrust retrieved information that aligns with their internal memorized\nknowledge. Previous works attempt to address these challenges by injecting\nmultiple malicious documents, but such saturation attacks are easily detectable\nand impractical in real-world scenarios. To enable the effective single\ndocument poisoning attack, we propose AuthChain, a novel knowledge poisoning\nattack method that leverages Chain-of-Evidence theory and authority effect to\ncraft more convincing poisoned documents. AuthChain generates poisoned content\nthat establishes strong evidence chains and incorporates authoritative\nstatements, effectively overcoming the interference from both authentic\ndocuments and LLMs' internal knowledge. Extensive experiments across six\npopular LLMs demonstrate that AuthChain achieves significantly higher attack\nsuccess rates while maintaining superior stealthiness against RAG defense\nmechanisms compared to state-of-the-art baselines."}
{"id": "2505.12239", "pdf": "https://arxiv.org/pdf/2505.12239", "abs": "https://arxiv.org/abs/2505.12239", "authors": ["Jianheng Tang", "Huiping Zhuang", "Di Fang", "Jiaxu Li", "Feijiang Han", "Yajiang Huang", "Kejia Fan", "Leye Wang", "Zhanxing Zhu", "Shanghang Zhang", "Houbing Herbert Song", "Yunhuai Liu"], "title": "ACU: Analytic Continual Unlearning for Efficient and Exact Forgetting with Privacy Preservation", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "21 pages, 4 figures, 2 tables", "summary": "The development of artificial intelligence demands that models incrementally\nupdate knowledge by Continual Learning (CL) to adapt to open-world\nenvironments. To meet privacy and security requirements, Continual Unlearning\n(CU) emerges as an important problem, aiming to sequentially forget particular\nknowledge acquired during the CL phase. However, existing unlearning methods\nprimarily focus on single-shot joint forgetting and face significant\nlimitations when applied to CU. First, most existing methods require access to\nthe retained dataset for re-training or fine-tuning, violating the inherent\nconstraint in CL that historical data cannot be revisited. Second, these\nmethods often suffer from a poor trade-off between system efficiency and model\nfidelity, making them vulnerable to being overwhelmed or degraded by\nadversaries through deliberately frequent requests. In this paper, we identify\nthat the limitations of existing unlearning methods stem fundamentally from\ntheir reliance on gradient-based updates. To bridge the research gap at its\nroot, we propose a novel gradient-free method for CU, named Analytic Continual\nUnlearning (ACU), for efficient and exact forgetting with historical data\nprivacy preservation. In response to each unlearning request, our ACU\nrecursively derives an analytical (i.e., closed-form) solution in an\ninterpretable manner using the least squares method. Theoretical and\nexperimental evaluations validate the superiority of our ACU on unlearning\neffectiveness, model fidelity, and system efficiency."}
{"id": "2505.12718", "pdf": "https://arxiv.org/pdf/2505.12718", "abs": "https://arxiv.org/abs/2505.12718", "authors": ["Jingyang Peng", "Wenyuan Shen", "Jiarui Rao", "Jionghao Lin"], "title": "Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework", "categories": ["cs.CL", "cs.HC"], "comment": "Accepted by AIED 2025: Late-Breaking Results (LBR) Track", "summary": "Recent advances in Generative Artificial Intelligence (GenAI) have\ntransformed educational content creation, particularly in developing tutor\ntraining materials. However, biases embedded in AI-generated content--such as\ngender, racial, or national stereotypes--raise significant ethical and\neducational concerns. Despite the growing use of GenAI, systematic methods for\ndetecting and evaluating such biases in educational materials remain limited.\nThis study proposes an automated bias assessment approach that integrates the\nContextualized Embedding Association Test with a prompt-engineered word\nextraction method within a Retrieval-Augmented Generation framework. We applied\nthis method to AI-generated texts used in tutor training lessons. Results show\na high alignment between the automated and manually curated word sets, with a\nPearson correlation coefficient of r = 0.993, indicating reliable and\nconsistent bias assessment. Our method reduces human subjectivity and enhances\nfairness, scalability, and reproducibility in auditing GenAI-produced\neducational content."}
{"id": "2505.11550", "pdf": "https://arxiv.org/pdf/2505.11550", "abs": "https://arxiv.org/abs/2505.11550", "authors": ["Harika Abburi", "Sanmitra Bhattacharya", "Edward Bowen", "Nirmala Pudota"], "title": "AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating text that closely resembles human writing across a wide range of\nstyles and genres. However, such capabilities are prone to potential misuse,\nsuch as fake news generation, spam email creation, and misuse in academic\nassignments. As a result, accurate detection of AI-generated text and\nidentification of the model that generated it are crucial for maintaining the\nresponsible use of LLMs. In this work, we addressed two sub-tasks put forward\nby the Defactify workshop under AI-Generated Text Detection shared task at the\nAssociation for the Advancement of Artificial Intelligence (AAAI 2025): Task A\ninvolved distinguishing between human-authored or AI-generated text, while Task\nB focused on attributing text to its originating language model. For each task,\nwe proposed two neural architectures: an optimized model and a simpler variant.\nFor Task A, the optimized neural architecture achieved fifth place with $F1$\nscore of 0.994, and for Task B, the simpler neural architecture also ranked\nfifth place with $F1$ score of 0.627."}
{"id": "2505.12245", "pdf": "https://arxiv.org/pdf/2505.12245", "abs": "https://arxiv.org/abs/2505.12245", "authors": ["Jianheng Tang", "Huiping Zhuang", "Jingyu He", "Run He", "Jingchao Wang", "Kejia Fan", "Anfeng Liu", "Tian Wang", "Leye Wang", "Zhanxing Zhu", "Shanghang Zhang", "Houbing Herbert Song", "Yunhuai Liu"], "title": "AFCL: Analytic Federated Continual Learning for Spatio-Temporal Invariance of Non-IID Data", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages, 5 figures, 5 tables", "summary": "Federated Continual Learning (FCL) enables distributed clients to\ncollaboratively train a global model from online task streams in dynamic\nreal-world scenarios. However, existing FCL methods face challenges of both\nspatial data heterogeneity among distributed clients and temporal data\nheterogeneity across online tasks. Such data heterogeneity significantly\ndegrades the model performance with severe spatial-temporal catastrophic\nforgetting of local and past knowledge. In this paper, we identify that the\nroot cause of this issue lies in the inherent vulnerability and sensitivity of\ngradients to non-IID data. To fundamentally address this issue, we propose a\ngradient-free method, named Analytic Federated Continual Learning (AFCL), by\nderiving analytical (i.e., closed-form) solutions from frozen extracted\nfeatures. In local training, our AFCL enables single-epoch learning with only a\nlightweight forward-propagation process for each client. In global aggregation,\nthe server can recursively and efficiently update the global model with\nsingle-round aggregation. Theoretical analyses validate that our AFCL achieves\nspatio-temporal invariance of non-IID data. This ideal property implies that,\nregardless of how heterogeneous the data are distributed across local clients\nand online tasks, the aggregated model of our AFCL remains invariant and\nidentical to that of centralized joint learning. Extensive experiments show the\nconsistent superiority of our AFCL over state-of-the-art baselines across\nvarious benchmark datasets and settings."}
{"id": "2505.12723", "pdf": "https://arxiv.org/pdf/2505.12723", "abs": "https://arxiv.org/abs/2505.12723", "authors": ["Haoyuan Wu", "Rui Ming", "Jilong Gao", "Hangyu Zhao", "Xueyi Chen", "Yikai Yang", "Haisheng Zheng", "Zhuolun He", "Bei Yu"], "title": "On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve remarkable performance in code\ngeneration tasks. However, a significant performance disparity persists between\npopular programming languages (e.g., Python, C++) and others. To address this\ncapability gap, we leverage the code translation task to train LLMs, thereby\nfacilitating the transfer of coding proficiency across diverse programming\nlanguages. Moreover, we introduce OORL for training, a novel reinforcement\nlearning (RL) framework that integrates on-policy and off-policy strategies.\nWithin OORL, on-policy RL is applied during code translation, guided by a\nrule-based reward signal derived from unit tests. Complementing this\ncoarse-grained rule-based reward, we propose Group Equivalent Preference\nOptimization (GEPO), a novel preference optimization method. Specifically, GEPO\ntrains the LLM using intermediate representations (IRs) groups. LLMs can be\nguided to discern IRs equivalent to the source code from inequivalent ones,\nwhile also utilizing signals about the mutual equivalence between IRs within\nthe group. This process allows LLMs to capture nuanced aspects of code\nfunctionality. By employing OORL for training with code translation tasks, LLMs\nimprove their recognition of code functionality and their understanding of the\nrelationships between code implemented in different languages. Extensive\nexperiments demonstrate that our OORL for LLMs training with code translation\ntasks achieves significant performance improvements on code benchmarks across\nmultiple programming languages."}
{"id": "2505.11552", "pdf": "https://arxiv.org/pdf/2505.11552", "abs": "https://arxiv.org/abs/2505.11552", "authors": ["Ahmad Bin Rabiah", "Julian McAuley"], "title": "GSPRec: Temporal-Aware Graph Spectral Filtering for Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Graph-based recommendation systems are effective at modeling collaborative\npatterns but often suffer from two limitations: overreliance on low-pass\nfiltering, which suppresses user-specific signals, and omission of sequential\ndynamics in graph construction. We introduce GSPRec, a graph spectral model\nthat integrates temporal transitions through sequentially-informed graph\nconstruction and applies frequency-aware filtering in the spectral domain.\nGSPRec encodes item transitions via multi-hop diffusion to enable the use of\nsymmetric Laplacians for spectral processing. To capture user preferences, we\ndesign a dual-filtering mechanism: a Gaussian bandpass filter to extract\nmid-frequency, user-level patterns, and a low-pass filter to retain global\ntrends. Extensive experiments on four public datasets show that GSPRec\nconsistently outperforms baselines, with an average improvement of 6.77% in\nNDCG@10. Ablation studies show the complementary benefits of both sequential\ngraph augmentation and bandpass filtering."}
{"id": "2505.12252", "pdf": "https://arxiv.org/pdf/2505.12252", "abs": "https://arxiv.org/abs/2505.12252", "authors": ["Yuhan Guo", "Lizhong Ding", "Yuwan Yang", "Xuewei Guo"], "title": "SchoenbAt: Rethinking Attention with Polynomial basis", "categories": ["cs.LG"], "comment": null, "summary": "Kernelized attention extends the attention mechanism by modeling sequence\ncorrelations through kernel functions, making significant progresses in\noptimizing attention. Under the guarantee of harmonic analysis theory, kernel\nfunctions can be expanded with basis functions, inspiring random feature-based\napproaches to enhance the efficiency of kernelized attention while maintaining\npredictive performance. However, current random feature-based works are limited\nto the Fourier basis expansions under Bochner's theorem. We propose\nSchoenberg's theorem-based attention (SchoenbAt), which approximates\ndot-product kernelized attention with the polynomial basis under Schoenberg's\ntheorem via random Maclaurin features and applies a two-stage regularization to\nconstrain the input space and restore the output scale, acting as a drop-in\nreplacement of dot-product kernelized attention. Our theoretical proof of the\nunbiasedness and concentration error bound of SchoenbAt supports its efficiency\nand accuracy as a kernelized attention approximation, which is also empirically\nvalidated under various random feature dimensions. Evaluations on real-world\ndatasets demonstrate that SchoenbAt significantly enhances computational speed\nwhile preserving competitive performance in terms of precision, outperforming\nseveral efficient attention methods."}
{"id": "2505.12727", "pdf": "https://arxiv.org/pdf/2505.12727", "abs": "https://arxiv.org/abs/2505.12727", "authors": ["Han Meng", "Yancan Chen", "Yunan Li", "Yitian Yang", "Jungup Lee", "Renwen Zhang", "Yi-Chieh Lee"], "title": "What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "Accepted to ACL 2025 Main Conference, 35 Pages", "summary": "Mental-health stigma remains a pervasive social problem that hampers\ntreatment-seeking and recovery. Existing resources for training neural models\nto finely classify such stigma are limited, relying primarily on social-media\nor synthetic data without theoretical underpinnings. To remedy this gap, we\npresent an expert-annotated, theory-informed corpus of human-chatbot\ninterviews, comprising 4,141 snippets from 684 participants with documented\nsocio-cultural backgrounds. Our experiments benchmark state-of-the-art neural\nmodels and empirically unpack the challenges of stigma detection. This dataset\ncan facilitate research on computationally detecting, neutralizing, and\ncounteracting mental-health stigma."}
{"id": "2505.11556", "pdf": "https://arxiv.org/pdf/2505.11556", "abs": "https://arxiv.org/abs/2505.11556", "authors": ["Yuxuan Li", "Aoi Naito", "Hirokazu Shirado"], "title": "Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "Multi-agent systems built on large language models (LLMs) promise enhanced\nproblem-solving through distributed information integration, but also risk\nreplicating collective reasoning failures observed in human groups. Yet, no\ntheory-grounded benchmark exists to systematically evaluate such failures. In\nthis paper, we introduce the Hidden Profile paradigm from social psychology as\na diagnostic testbed for multi-agent LLM systems. By distributing critical\ninformation asymmetrically across agents, the paradigm reveals how inter-agent\ndynamics support or hinder collective reasoning. We first formalize the\nparadigm for multi-agent decision-making under distributed knowledge and\ninstantiate it as a benchmark with nine tasks spanning diverse scenarios,\nincluding adaptations from prior human studies. We then conduct experiments\nwith GPT-4.1 and five other leading LLMs, including reasoning-enhanced\nvariants, showing that multi-agent systems across all models fail to match the\naccuracy of single agents given complete information. While agents' collective\nperformance is broadly comparable to that of human groups, nuanced behavioral\ndifferences emerge, such as increased sensitivity to social desirability.\nFinally, we demonstrate the paradigm's diagnostic utility by exploring a\ncooperation-contradiction trade-off in multi-agent LLM systems. We find that\nwhile cooperative agents are prone to over-coordination in collective settings,\nincreased contradiction impairs group convergence. This work contributes a\nreproducible framework for evaluating multi-agent LLM systems and motivates\nfuture research on artificial collective intelligence and human-AI interaction."}
{"id": "2505.12275", "pdf": "https://arxiv.org/pdf/2505.12275", "abs": "https://arxiv.org/abs/2505.12275", "authors": ["Wen-Chao Hu", "Qi-Jie Li", "Lin-Han Jia", "Cunjing Ge", "Yu-Feng Li", "Yuan Jiang", "Zhi-Hua Zhou"], "title": "Curriculum Abductive Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Abductive Learning (ABL) integrates machine learning with logical reasoning\nin a loop: a learning model predicts symbolic concept labels from raw inputs,\nwhich are revised through abduction using domain knowledge and then fed back\nfor retraining. However, due to the nondeterminism of abduction, the training\nprocess often suffers from instability, especially when the knowledge base is\nlarge and complex, resulting in a prohibitively large abduction space. While\nprior works focus on improving candidate selection within this space, they\ntypically treat the knowledge base as a static black box. In this work, we\npropose Curriculum Abductive Learning (C-ABL), a method that explicitly\nleverages the internal structure of the knowledge base to address the ABL\ntraining challenges. C-ABL partitions the knowledge base into a sequence of\nsub-bases, progressively introduced during training. This reduces the abduction\nspace throughout training and enables the model to incorporate logic in a\nstepwise, smooth way. Experiments across multiple tasks show that C-ABL\noutperforms previous ABL implementations, significantly improves training\nstability, convergence speed, and final accuracy, especially under complex\nknowledge setting."}
{"id": "2505.12768", "pdf": "https://arxiv.org/pdf/2505.12768", "abs": "https://arxiv.org/abs/2505.12768", "authors": ["Yaxun Dai", "Wenxuan Xie", "Xialie Zhuang", "Tianyu Yang", "Yiying Yang", "Haiqin Yang", "Yuhang Zhao", "Pingfu Chao", "Wenhao Jiang"], "title": "ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL", "categories": ["cs.CL"], "comment": null, "summary": "In Text-to-SQL, execution feedback is essential for guiding large language\nmodels (LLMs) to reason accurately and generate reliable SQL queries. However,\nexisting methods treat execution feedback solely as a post-hoc signal for\ncorrection or selection, failing to integrate it into the generation process.\nThis limitation hinders their ability to address reasoning errors as they\noccur, ultimately reducing query accuracy and robustness. To address this\nissue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement\nLearning), a framework for Text-to-SQL that enables models to interact with the\ndatabase during decoding and dynamically adjust their reasoning based on\nexecution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm\nthat interleaves intermediate SQL execution into reasoning paths, facilitating\ncontext-sensitive revisions. It achieves this through structured prompts with\nmarkup tags and a stepwise rollout strategy that integrates execution feedback\ninto each stage of generation. To supervise policy learning, we develop a\ncomposite reward function that includes an exploration reward, explicitly\nencouraging effective database interaction. Additionally, ReEx-SQL adopts a\ntree-based decoding strategy to support exploratory reasoning, enabling dynamic\nexpansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on\nSpider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning\nbaseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving\n85.2% on Spider-Realistic with leading performance. In addition, its\ntree-structured decoding improves efficiency and performance over linear\ndecoding, reducing inference time by 51.9% on the BIRD development set."}
{"id": "2505.11557", "pdf": "https://arxiv.org/pdf/2505.11557", "abs": "https://arxiv.org/abs/2505.11557", "authors": ["Lara Magdalena Lazier", "Aritra Dhar", "Vasilije Stambolic", "Lukas Cavigelli"], "title": "AC-LoRA: (Almost) Training-Free Access Control-Aware Multi-Modal LLMs", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Corporate LLMs are gaining traction for efficient knowledge dissemination and\nmanagement within organizations. However, as current LLMs are vulnerable to\nleaking sensitive information, it has proven difficult to apply them in\nsettings where strict access control is necessary. To this end, we design\nAC-LoRA, an end-to-end system for access control-aware corporate LLM chatbots\nthat maintains a strong information isolation guarantee. AC-LoRA maintains\nseparate LoRA adapters for permissioned datasets, along with the document\nembedding they are finetuned on. AC-LoRA retrieves a precise set of LoRA\nadapters based on the similarity score with the user query and their\npermission. This similarity score is later used to merge the responses if more\nthan one LoRA is retrieved, without requiring any additional training for LoRA\nrouting. We provide an end-to-end prototype of AC-LoRA, evaluate it on two\ndatasets, and show that AC-LoRA matches or even exceeds the performance of\nstate-of-the-art LoRA mixing techniques while providing strong isolation\nguarantees. Furthermore, we show that AC-LoRA design can be directly applied to\ndifferent modalities."}
{"id": "2505.12302", "pdf": "https://arxiv.org/pdf/2505.12302", "abs": "https://arxiv.org/abs/2505.12302", "authors": ["Zhen Zhao", "Wenqi Huang", "Zicheng Wang", "Jiaxuan Hou", "Peng Li", "Lei Bai"], "title": "SenseFlow: A Physics-Informed and Self-Ensembling Iterative Framework for Power Flow Estimation", "categories": ["cs.LG"], "comment": null, "summary": "Power flow estimation plays a vital role in ensuring the stability and\nreliability of electrical power systems, particularly in the context of growing\nnetwork complexities and renewable energy integration. However, existing\nstudies often fail to adequately address the unique characteristics of power\nsystems, such as the sparsity of network connections and the critical\nimportance of the unique Slack node, which poses significant challenges in\nachieving high-accuracy estimations. In this paper, we present SenseFlow, a\nnovel physics-informed and self-ensembling iterative framework that integrates\ntwo main designs, the Physics-Informed Power Flow Network (FlowNet) and\nSelf-Ensembling Iterative Estimation (SeIter), to carefully address the unique\nproperties of the power system and thereby enhance the power flow estimation.\nSpecifically, SenseFlow enforces the FlowNet to gradually predict\nhigh-precision voltage magnitudes and phase angles through the iterative SeIter\nprocess. On the one hand, FlowNet employs the Virtual Node Attention and\nSlack-Gated Feed-Forward modules to facilitate efficient global-local\ncommunication in the face of network sparsity and amplify the influence of the\nSlack node on angle predictions, respectively. On the other hand, SeIter\nmaintains an exponential moving average of FlowNet's parameters to create a\nrobust ensemble model that refines power state predictions throughout the\niterative fitting process. Experimental results demonstrate that SenseFlow\noutperforms existing methods, providing a promising solution for high-accuracy\npower flow estimation across diverse grid configurations."}
{"id": "2505.12781", "pdf": "https://arxiv.org/pdf/2505.12781", "abs": "https://arxiv.org/abs/2505.12781", "authors": ["Jitai Hao", "Qiang Huang", "Hao Liu", "Xinyan Xiao", "Zhaochun Ren", "Jun Yu"], "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Training high-performing Small Language Models (SLMs) remains costly, even\nwith knowledge distillation and pruning from larger teacher models. Existing\nwork often faces three key challenges: (1) information loss from hard pruning,\n(2) inefficient alignment of representations, and (3) underutilization of\ninformative activations, particularly from Feed-Forward Networks (FFNs). To\naddress these challenges, we introduce Low-Rank Clone (LRC), an efficient\npre-training method that constructs SLMs aspiring to behavioral equivalence\nwith strong teacher models. LRC trains a set of low-rank projection matrices\nthat jointly enable soft pruning by compressing teacher weights, and activation\nclone by aligning student activations, including FFN signals, with those of the\nteacher. This unified design maximizes knowledge transfer while removing the\nneed for explicit alignment modules. Extensive experiments with open-source\nteachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC\nmatches or surpasses state-of-the-art models trained on trillions of\ntokens--while using only 20B tokens, achieving over 1,000x training efficiency.\nOur codes and model checkpoints are available at\nhttps://github.com/CURRENTF/LowRankClone and\nhttps://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf."}
{"id": "2505.11559", "pdf": "https://arxiv.org/pdf/2505.11559", "abs": "https://arxiv.org/abs/2505.11559", "authors": ["Sushrit Kafle", "Shreejan Pandey"], "title": "Analysis and Resilience of the U.S. Flight Network", "categories": ["physics.soc-ph", "cs.AI", "cs.SI"], "comment": "Investigates resilience of the U.S. flight network under node\n  failures. Includes percolation threshold detection, cascade simulations, and\n  community structure analysis. 9 pages, 14 figures", "summary": "Air travel is one of the most widely used transportation services in the\nUnited States. This paper analyzes the U.S. Flight Network (USFN) using complex\nnetwork theory by exploring how the network's topology contributes to its\nefficiency and vulnerability. This is done by examining the structural\nproperties, degree distributions, and community structures in the network. USFN\nwas observed to follow power-law distribution and falls under the anomalous\nregime, suggesting that the network is hub dominant. Compared to null networks,\nUSFN has a higher clustering coefficient and modularity. Various percolation\ntest revealed that USFN is vulnerable to targeted attacks and is susceptible to\ncomplete cascading failure if one of the major hubs fails. The overall results\nsuggest that while the USFN is designed for efficiency, it is highly vulnerable\nto disruptions. Protecting key hub airports is important to make the network\nmore robust and prevent large-scale failures."}
{"id": "2505.12318", "pdf": "https://arxiv.org/pdf/2505.12318", "abs": "https://arxiv.org/abs/2505.12318", "authors": ["Feng Yu", "Jia Hu", "Geyong Min"], "title": "Efficient Federated Class-Incremental Learning of Pre-Trained Models via Task-agnostic Low-rank Residual Adaptation", "categories": ["cs.LG"], "comment": null, "summary": "Federated Parameter-Efficient Fine-Tuning (FedPEFT) reduces communication and\ncomputation costs in federated fine-tuning of pre-trained models by updating\nonly a small subset of model parameters. However, existing approaches assume\nstatic data distributions, failing to adequately address real-world scenarios\nwhere new classes continually emerge, particularly in Federated Class\nIncremental Learning (FCIL). FCIL faces two key challenges: catastrophic\nforgetting and performance degradation caused by non-IID data across clients.\nUnlike current methods that maintain separate task-specific components or\nsuffer from aggregation noise during parameter aggregation, we propose\nFederated Task-agnostic Low-rank Residual Adaptation (Fed-TaLoRA), a novel\nparameter-efficient approach for fine-tuning in resource-constrained FCIL\nscenarios. Specifically, we fine-tune only shared task-agnostic LoRA parameters\nacross sequential tasks, effectively mitigating catastrophic forgetting while\nenabling efficient knowledge transfer among clients. Based on a theoretical\nanalysis of aggregation, we develop a novel residual weight update mechanism\nthat ensures accurate knowledge consolidation with minimal overhead. Our\nmethodological innovations are attributed to three key strategies:\ntask-agnostic adaptation, post-aggregation model calibration, and strategic\nplacement of LoRA modules. Extensive experiments on multiple benchmark datasets\ndemonstrate that Fed-TaLoRA consistently outperforms state-of-the-art methods\nin diverse data heterogeneity scenarios while substantially reducing resource\nrequirements."}
{"id": "2505.12792", "pdf": "https://arxiv.org/pdf/2505.12792", "abs": "https://arxiv.org/abs/2505.12792", "authors": ["Wenhao Zhu", "Yuhang Xie", "Guojie Song", "Xin Zhang"], "title": "EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs", "categories": ["cs.CL"], "comment": null, "summary": "The rapid evolution of large language models (LLMs) has revolutionized\nvarious fields, including the identification and discovery of human values\nwithin text data. While traditional NLP models, such as BERT, have been\nemployed for this task, their ability to represent textual data is\nsignificantly outperformed by emerging LLMs like GPTs. However, the performance\nof online LLMs often degrades when handling long contexts required for value\nidentification, which also incurs substantial computational costs. To address\nthese challenges, we propose EAVIT, an efficient and accurate framework for\nhuman value identification that combines the strengths of both locally\nfine-tunable and online black-box LLMs. Our framework employs a value detector\n- a small, local language model - to generate initial value estimations. These\nestimations are then used to construct concise input prompts for online LLMs,\nenabling accurate final value identification. To train the value detector, we\nintroduce explanation-based training and data generation techniques\nspecifically tailored for value identification, alongside sampling strategies\nto optimize the brevity of LLM input prompts. Our approach effectively reduces\nthe number of input tokens by up to 1/6 compared to directly querying online\nLLMs, while consistently outperforming traditional NLP methods and other\nLLM-based strategies."}
{"id": "2505.11563", "pdf": "https://arxiv.org/pdf/2505.11563", "abs": "https://arxiv.org/abs/2505.11563", "authors": ["Alexandre Chapin", "Bruno Machado", "Emmanuel Dellandrea", "Liming Chen"], "title": "Object-Centric Representations Improve Policy Generalization in Robot Manipulation", "categories": ["cs.RO", "cs.AI", "eess.IV"], "comment": null, "summary": "Visual representations are central to the learning and generalization\ncapabilities of robotic manipulation policies. While existing methods rely on\nglobal or dense features, such representations often entangle task-relevant and\nirrelevant scene information, limiting robustness under distribution shifts. In\nthis work, we investigate object-centric representations (OCR) as a structured\nalternative that segments visual input into a finished set of entities,\nintroducing inductive biases that align more naturally with manipulation tasks.\nWe benchmark a range of visual encoders-object-centric, global and dense\nmethods-across a suite of simulated and real-world manipulation tasks ranging\nfrom simple to complex, and evaluate their generalization under diverse visual\nconditions including changes in lighting, texture, and the presence of\ndistractors. Our findings reveal that OCR-based policies outperform dense and\nglobal representations in generalization settings, even without task-specific\npretraining. These insights suggest that OCR is a promising direction for\ndesigning visual systems that generalize effectively in dynamic, real-world\nrobotic environments."}
{"id": "2505.12322", "pdf": "https://arxiv.org/pdf/2505.12322", "abs": "https://arxiv.org/abs/2505.12322", "authors": ["Ali Gholamzadeh", "Noor Sajid"], "title": "Model alignment using inter-modal bridges", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Foundation models have demonstrated remarkable performance across modalities\nsuch as language and vision. However, model reuse across distinct modalities\n(e.g., text and vision) remains limited due to the difficulty of aligning\ninternal representations. Existing methods require extensive paired training\ndata or are constrained to specific domains. We introduce a semi-supervised\napproach for model alignment via conditional flow matching. The conditional\nflow between latent spaces of different modalities (e.g., text-to-image or\nbiological-to-artificial neuronal activity) can be learned in two settings:\n($1$) solving a (balanced or unbalanced) optimal transport problem with an\ninter-space bridge cost, and ($2$) performing memory-efficient alignment using\nlabelled exemplars. Despite being constrained by the original models' capacity,\nour method--under both settings--matches downstream task performance of\nend-to-end trained models on object recognition and image generation tasks\nacross MNIST, ImageNet, and \\cite{majaj2015simple} datasets, particularly when\nlabelled training data is scarce ($<20\\%$). Our method provides a\ndata-efficient solution for inter-modal model alignment with minimal\nsupervision."}
{"id": "2505.12808", "pdf": "https://arxiv.org/pdf/2505.12808", "abs": "https://arxiv.org/abs/2505.12808", "authors": ["Yanbin Yin", "Kun Zhou", "Zhen Wang", "Xiangdong Zhang", "Yifei Shao", "Shibo Hao", "Yi Gu", "Jieyuan Liu", "Somanshu Singla", "Tianyang Liu", "Eric P. Xing", "Zhengzhong Liu", "Haojian Jin", "Zhiting Hu"], "title": "Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "20 pages, ongoing work", "summary": "The recent explosion of large language models (LLMs), each with its own\ngeneral or specialized strengths, makes scalable, reliable benchmarking more\nurgent than ever. Standard practices nowadays face fundamental trade-offs:\nclosed-ended question-based benchmarks (eg MMLU) struggle with saturation as\nnewer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely\non costly and slow human judges. Recently, automated methods (eg\nLLM-as-a-judge) shed light on the scalability, but risk bias by relying on one\nor a few \"authority\" models. To tackle these issues, we propose Decentralized\nArena (dearena), a fully automated framework leveraging collective intelligence\nfrom all LLMs to evaluate each other. It mitigates single-model judge bias by\ndemocratic, pairwise evaluation, and remains efficient at scale through two key\ncomponents: (1) a coarse-to-fine ranking algorithm for fast incremental\ninsertion of new models with sub-quadratic complexity, and (2) an automatic\nquestion selection strategy for the construction of new evaluation dimensions.\nAcross extensive experiments across 66 LLMs, dearena attains up to 97%\ncorrelation with human judgements, while significantly reducing the cost. Our\ncode and data will be publicly released on\nhttps://github.com/maitrix-org/de-arena."}
{"id": "2505.11565", "pdf": "https://arxiv.org/pdf/2505.11565", "abs": "https://arxiv.org/abs/2505.11565", "authors": ["Sarthak Munshi", "Swapnil Pathak", "Sonam Ghatode", "Thenuga Priyadarshini", "Dhivya Chandramouleeswaran", "Ashutosh Rana"], "title": "ACSE-Eval: Can LLMs threat model real-world cloud infrastructure?", "categories": ["cs.CR", "cs.AI"], "comment": "Submitted to the 39th Annual Conference on Neural Information\n  Processing Systems", "summary": "While Large Language Models have shown promise in cybersecurity applications,\ntheir effectiveness in identifying security threats within cloud deployments\nremains unexplored. This paper introduces AWS Cloud Security Engineering Eval,\na novel dataset for evaluating LLMs cloud security threat modeling\ncapabilities. ACSE-Eval contains 100 production grade AWS deployment scenarios,\neach featuring detailed architectural specifications, Infrastructure as Code\nimplementations, documented security vulnerabilities, and associated threat\nmodeling parameters. Our dataset enables systemic assessment of LLMs abilities\nto identify security risks, analyze attack vectors, and propose mitigation\nstrategies in cloud environments. Our evaluations on ACSE-Eval demonstrate that\nGPT 4.1 and Gemini 2.5 Pro excel at threat identification, with Gemini 2.5 Pro\nperforming optimally in 0-shot scenarios and GPT 4.1 showing superior results\nin few-shot settings. While GPT 4.1 maintains a slight overall performance\nadvantage, Claude 3.7 Sonnet generates the most semantically sophisticated\nthreat models but struggles with threat categorization and generalization. To\npromote reproducibility and advance research in automated cybersecurity threat\nanalysis, we open-source our dataset, evaluation metrics, and methodologies."}
{"id": "2505.12323", "pdf": "https://arxiv.org/pdf/2505.12323", "abs": "https://arxiv.org/abs/2505.12323", "authors": ["Mohit Kataria", "Nikita Malik", "Sandeep Kumar", "Jayadeva"], "title": "GraphFLEx: Structure Learning Framework for Large Expanding Graphs", "categories": ["cs.LG"], "comment": null, "summary": "Graph structure learning is a core problem in graph-based machine learning,\nessential for uncovering latent relationships and ensuring model\ninterpretability. However, most existing approaches are ill-suited for\nlarge-scale and dynamically evolving graphs, as they often require complete\nre-learning of the structure upon the arrival of new nodes and incur\nsubstantial computational and memory costs. In this work, we propose GraphFLEx:\na unified and scalable framework for Graph Structure Learning in Large and\nExpanding Graphs. GraphFLEx mitigates the scalability bottlenecks by\nrestricting edge formation to structurally relevant subsets of nodes identified\nthrough a combination of clustering and coarsening techniques. This\ndramatically reduces the search space and enables efficient, incremental graph\nupdates. The framework supports 48 flexible configurations by integrating\ndiverse choices of learning paradigms, coarsening strategies, and clustering\nmethods, making it adaptable to a wide range of graph settings and learning\nobjectives. Extensive experiments across 26 diverse datasets and Graph Neural\nNetwork architectures demonstrate that GraphFLEx achieves state-of-the-art\nperformance with significantly improved scalability."}
{"id": "2505.12814", "pdf": "https://arxiv.org/pdf/2505.12814", "abs": "https://arxiv.org/abs/2505.12814", "authors": ["Xilong Cheng", "Yunxiao Qin", "Yuting Tan", "Zhengnan Li", "Ye Wang", "Hongjiang Xiao", "Yuan Zhang"], "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity."}
{"id": "2505.11567", "pdf": "https://arxiv.org/pdf/2505.11567", "abs": "https://arxiv.org/abs/2505.11567", "authors": ["Tianyi Shi", "Zhu Meng", "Yue Chen", "Siyang Zheng", "Fei Su", "Jin Huang", "Changrui Ren", "Zhicheng Zhao"], "title": "Beyond Time: Cross-Dimensional Frequency Supervision for Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series forecasting plays a crucial role in various fields, and the\nmethods based on frequency domain analysis have become an important branch.\nHowever, most existing studies focus on the design of elaborate model\narchitectures and are often tailored for limited datasets, still lacking\nuniversality. Besides, the assumption of independent and identically\ndistributed (IID) data also contradicts the strong correlation of the time\ndomain labels. To address these issues, abandoning time domain supervision, we\npropose a purely frequency domain supervision approach named cross-dimensional\nfrequency (X-Freq) loss. Specifically, based on a statistical phenomenon, we\nfirst prove that the information entropy of the time series is higher than its\nspectral entropy, which implies higher certainty in frequency domain and thus\ncan provide better supervision. Secondly, the Fourier Transform and the Wavelet\nTransform are applied to the time dimension and the channel dimension of the\ntime series respectively, to capture the long-term and short-term frequency\nvariations as well as the spatial configuration features. Thirdly, the loss\nbetween predictions and targets is uniformly computed in the frequency domain.\nMoreover, we plug-and-play incorporate X-Freq into multiple advanced\nforecasting models and compare on 14 real-world datasets. The experimental\nresults demonstrate that, without making any modification to the original\narchitectures or hyperparameters, X-Freq can improve the forecasting\nperformance by an average of 3.3% on long-term forecasting datasets and 27.7%\non short-term ones, showcasing superior generality and practicality. The code\nwill be released publicly."}
{"id": "2505.12325", "pdf": "https://arxiv.org/pdf/2505.12325", "abs": "https://arxiv.org/abs/2505.12325", "authors": ["Chaolong Ying", "Yingqi Ruan", "Xuemin Chen", "Yaomin Wang", "Tianshu Yu"], "title": "Neural Graduated Assignment for Maximum Common Edge Subgraphs", "categories": ["cs.LG"], "comment": null, "summary": "The Maximum Common Edge Subgraph (MCES) problem is a crucial challenge with\nsignificant implications in domains such as biology and chemistry. Traditional\napproaches, which include transformations into max-clique and search-based\nalgorithms, suffer from scalability issues when dealing with larger instances.\nThis paper introduces ``Neural Graduated Assignment'' (NGA), a simple,\nscalable, unsupervised-training-based method that addresses these limitations\nby drawing inspiration from the classical Graduated Assignment (GA) technique.\nCentral to NGA is stacking of neural components that closely resemble the GA\nprocess, but with the reparameterization of learnable temperature into higher\ndimension. We further theoretically analyze the learning dynamics of NGA,\nshowing its design leads to fast convergence, better exploration-exploitation\ntradeoff, and ability to escape local optima. Extensive experiments across MCES\ncomputation, graph similarity estimation, and graph retrieval tasks reveal that\nNGA not only significantly improves computation time and scalability on large\ninstances but also enhances performance compared to existing methodologies. The\nintroduction of NGA marks a significant advancement in the computation of MCES\nand offers insights into other assignment problems."}
{"id": "2505.12821", "pdf": "https://arxiv.org/pdf/2505.12821", "abs": "https://arxiv.org/abs/2505.12821", "authors": ["Han Sun", "Zhen Sun", "Zongmin Zhang", "Linzhao Jia", "Wei Shao", "Min Zhang"], "title": "SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are emerging as dominant forces for textual\nstyle transfer. However, for arbitrary style transfer, LLMs face two key\nchallenges: (1) considerable reliance on manually-constructed prompts and (2)\nrigid stylistic biases inherent in LLMs. In this paper, we propose a novel\nSynthesize-then-Decode (SynDec) approach, which automatically synthesizes\nhigh-quality prompts and amplifies their roles during decoding process.\nSpecifically, our approach synthesizes prompts by selecting representative\nfew-shot samples, conducting a four-dimensional style analysis, and reranking\nthe candidates. At LLM decoding stage, the TST effect is amplified by\nmaximizing the contrast in output probabilities between scenarios with and\nwithout the synthesized prompt, as well as between prompts and negative\nsamples. We conduct extensive experiments and the results show that SynDec\noutperforms existing state-of-the-art LLM-based methods on five out of six\nbenchmarks (e.g., achieving up to a 9\\% increase in accuracy for\nmodern-to-Elizabethan English transfer). Detailed ablation studies further\nvalidate the effectiveness of SynDec."}
{"id": "2505.11568", "pdf": "https://arxiv.org/pdf/2505.11568", "abs": "https://arxiv.org/abs/2505.11568", "authors": ["Stylianos Stasinos", "Martino Mensio", "Elena Lazovik", "Athanasios Trantas"], "title": "BioCube: A Multimodal Dataset for Biodiversity Research", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": "submitted to BiDS'25, 5 pages, 1 figure", "summary": "Biodiversity research requires complete and detailed information to study\necosystem dynamics at different scales. Employing data-driven methods like\nMachine Learning is getting traction in ecology and more specific biodiversity,\noffering alternative modelling pathways. For these methods to deliver accurate\nresults there is the need for large, curated and multimodal datasets that offer\ngranular spatial and temporal resolutions. In this work, we introduce BioCube,\na multimodal, fine-grained global dataset for ecology and biodiversity\nresearch. BioCube incorporates species observations through images, audio\nrecordings and descriptions, environmental DNA, vegetation indices,\nagricultural, forest, land indicators, and high-resolution climate variables.\nAll observations are geospatially aligned under the WGS84 geodetic system,\nspanning from 2000 to 2020. The dataset will become available at\nhttps://huggingface.co/datasets/BioDT/BioCube while the acquisition and\nprocessing code base at https://github.com/BioDT/bfm-data."}
{"id": "2505.12343", "pdf": "https://arxiv.org/pdf/2505.12343", "abs": "https://arxiv.org/abs/2505.12343", "authors": ["Kai Tang", "Jinhao You", "Xiuqi Ge", "Hanze Li", "Yichen Guo", "Xiande Huang"], "title": "Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite the impressive capabilities of Large Vision-Language Models (LVLMs),\nthey remain susceptible to hallucinations-generating content that is\ninconsistent with the input image. Existing training-free hallucination\nmitigation methods often suffer from unstable performance and high sensitivity\nto hyperparameter settings, limiting their practicality and broader adoption.\nIn this paper, we propose a novel decoding mechanism, Decoding with Inter-layer\nConsistency via Layer Aggregation (DCLA), which requires no retraining,\nfine-tuning, or access to external knowledge bases. Specifically, our approach\nconstructs a dynamic semantic reference by aggregating representations from\nprevious layers, and corrects semantically deviated layers to enforce\ninter-layer consistency. The method allows DCLA to robustly mitigate\nhallucinations across multiple LVLMs. Experiments on hallucination benchmarks\nsuch as MME and POPE demonstrate that DCLA effectively reduces hallucinations\nwhile enhancing the reliability and performance of LVLMs."}
{"id": "2505.12831", "pdf": "https://arxiv.org/pdf/2505.12831", "abs": "https://arxiv.org/abs/2505.12831", "authors": ["Zifeng Cheng", "Zhonghui Wang", "Yuchen Fu", "Zhiwei Jiang", "Yafeng Yin", "Cong Wang", "Qing Gu"], "title": "Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Extracting sentence embeddings from large language models (LLMs) is a\npractical direction, as it requires neither additional data nor fine-tuning.\nPrevious studies usually focus on prompt engineering to guide LLMs to encode\nthe core semantic information of the sentence into the embedding of the last\ntoken. However, the last token in these methods still encodes an excess of\nnon-essential information, such as stop words, limiting its encoding capacity.\nTo this end, we propose a Contrastive Prompting (CP) method that introduces an\nextra auxiliary prompt to elicit better sentence embedding. By contrasting with\nthe auxiliary prompt, CP can steer existing prompts to encode the core\nsemantics of the sentence, rather than non-essential information. CP is a\nplug-and-play inference-time intervention method that can be combined with\nvarious prompt-based methods. Extensive experiments on Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nmethod can improve the performance of existing prompt-based methods across\ndifferent LLMs. Our code will be released at https://github.com/zifengcheng/CP."}
{"id": "2505.11569", "pdf": "https://arxiv.org/pdf/2505.11569", "abs": "https://arxiv.org/abs/2505.11569", "authors": ["Pooja Mangal", "Sudaksh Kalra", "Dolly Sapra"], "title": "Towards Adaptive Deep Learning: Model Elasticity via Prune-and-Grow CNN Architectures", "categories": ["cs.LG", "cs.AI"], "comment": "50 Pages, 11 figures, Preprint", "summary": "Deploying deep convolutional neural networks (CNNs) on resource-constrained\ndevices presents significant challenges due to their high computational demands\nand rigid, static architectures. To overcome these limitations, this thesis\nexplores methods for enabling CNNs to dynamically adjust their computational\ncomplexity based on available hardware resources. We introduce adaptive CNN\narchitectures capable of scaling their capacity at runtime, thus efficiently\nbalancing performance and resource utilization. To achieve this adaptability,\nwe propose a structured pruning and dynamic re-construction approach that\ncreates nested subnetworks within a single CNN model. This approach allows the\nnetwork to dynamically switch between compact and full-sized configurations\nwithout retraining, making it suitable for deployment across varying hardware\nplatforms. Experiments conducted across multiple CNN architectures including\nVGG-16, AlexNet, ResNet-20, and ResNet-56 on CIFAR-10 and Imagenette datasets\ndemonstrate that adaptive models effectively maintain or even enhance\nperformance under varying computational constraints. Our results highlight that\nembedding adaptability directly into CNN architectures significantly improves\ntheir robustness and flexibility, paving the way for efficient real-world\ndeployment in diverse computational environments."}
{"id": "2505.12344", "pdf": "https://arxiv.org/pdf/2505.12344", "abs": "https://arxiv.org/abs/2505.12344", "authors": ["Han Wang"], "title": "Early Prediction of In-Hospital ICU Mortality Using Innovative First-Day Data: A Review", "categories": ["cs.LG", "cs.CY"], "comment": "23 pages, 1 table", "summary": "The intensive care unit (ICU) manages critically ill patients, many of whom\nface a high risk of mortality. Early and accurate prediction of in-hospital\nmortality within the first 24 hours of ICU admission is crucial for timely\nclinical interventions, resource optimization, and improved patient outcomes.\nTraditional scoring systems, while useful, often have limitations in predictive\naccuracy and adaptability. Objective: This review aims to systematically\nevaluate and benchmark innovative methodologies that leverage data available\nwithin the first day of ICU admission for predicting in-hospital mortality. We\nfocus on advancements in machine learning, novel biomarker applications, and\nthe integration of diverse data types."}
{"id": "2505.12835", "pdf": "https://arxiv.org/pdf/2505.12835", "abs": "https://arxiv.org/abs/2505.12835", "authors": ["Hengxing Cai", "Jinhan Dong", "Jingjun Tan", "Jingcheng Deng", "Sihang Li", "Zhifeng Gao", "Haidong Wang", "Zicheng Su", "Agachai Sumalee", "Renxin Zhong"], "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital\nfor applications such as disaster response, logistics delivery, and urban\ninspection. However, existing methods often struggle with insufficient\nmultimodal fusion, weak generalization, and poor interpretability. To address\nthese challenges, we propose FlightGPT, a novel UAV VLN framework built upon\nVision-Language Models (VLMs) with powerful multimodal perception capabilities.\nWe design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)\nusing high-quality demonstrations to improve initialization and structured\nreasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by\na composite reward that considers goal accuracy, reasoning quality, and format\ncompliance, to enhance generalization and adaptability. Furthermore, FlightGPT\nintroduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve\ndecision interpretability. Extensive experiments on the city-scale dataset\nCityNav demonstrate that FlightGPT achieves state-of-the-art performance across\nall scenarios, with a 9.22\\% higher success rate than the strongest baseline in\nunseen environments. Our implementation is publicly available."}
{"id": "2505.11570", "pdf": "https://arxiv.org/pdf/2505.11570", "abs": "https://arxiv.org/abs/2505.11570", "authors": ["Chongyang Tan", "Ruoqi Wen", "Rongpeng Li", "Zhifeng Zhao", "Ekram Hossain", "Honggang Zhang"], "title": "Tool-Aided Evolutionary LLM for Generative Policy Toward Efficient Resource Management in Wireless Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) enables distributed model training across edge\ndevices in a privacy-friendly manner. However, its efficiency heavily depends\non effective device selection and high-dimensional resource allocation in\ndynamic and heterogeneous wireless environments. Conventional methods demand a\nconfluence of domain-specific expertise, extensive hyperparameter tuning,\nand/or heavy interaction cost. This paper proposes a Tool-aided Evolutionary\nLarge Language Model (T-ELLM) framework to generate a qualified policy for\ndevice selection in a wireless FL environment. Unlike conventional optimization\nmethods, T-ELLM leverages natural language-based scenario prompts to enhance\ngeneralization across varying network conditions. The framework decouples the\njoint optimization problem mathematically, enabling tractable learning of\ndevice selection policies while delegating resource allocation to convex\noptimization tools. To improve adaptability, T-ELLM integrates a\nsample-efficient, model-based virtual learning environment that captures the\nrelationship between device selection and learning performance, facilitating\nsubsequent group relative policy optimization. This concerted approach reduces\nreliance on real-world interactions, minimizing communication overhead while\nmaintaining high-fidelity decision-making. Theoretical analysis proves that the\ndiscrepancy between virtual and real environments is bounded, ensuring the\nadvantage function learned in the virtual environment maintains a provably\nsmall deviation from real-world conditions. Experimental results demonstrate\nthat T-ELLM outperforms benchmark methods in energy efficiency and exhibits\nrobust adaptability to environmental changes."}
{"id": "2505.12350", "pdf": "https://arxiv.org/pdf/2505.12350", "abs": "https://arxiv.org/abs/2505.12350", "authors": ["Georgiy Malaniya", "Anton Bolychev", "Grigory Yaremenko", "Anastasia Krasnaya", "Pavel Osinenko"], "title": "Multi-CALF: A Policy Combination Approach with Statistical Guarantees", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "We introduce Multi-CALF, an algorithm that intelligently combines\nreinforcement learning policies based on their relative value improvements. Our\napproach integrates a standard RL policy with a theoretically-backed\nalternative policy, inheriting formal stability guarantees while often\nachieving better performance than either policy individually. We prove that our\ncombined policy converges to a specified goal set with known probability and\nprovide precise bounds on maximum deviation and convergence time. Empirical\nvalidation on control tasks demonstrates enhanced performance while maintaining\nstability guarantees."}
{"id": "2505.12837", "pdf": "https://arxiv.org/pdf/2505.12837", "abs": "https://arxiv.org/abs/2505.12837", "authors": ["Christian Braun", "Alexander Lilienbeck", "Daniel Mentjukov"], "title": "The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 3 figures", "summary": "Legal contracts possess an inherent, semantically vital structure (e.g.,\nsections, clauses) that is crucial for human comprehension but whose impact on\nLLM processing remains under-explored. This paper investigates the effects of\nexplicit input text structure and prompt engineering on the performance of\nGPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the\nCUAD. We compare model exact-match accuracy across various input formats:\nwell-structured plain-text (human-generated from CUAD), plain-text cleaned of\nline breaks, extracted plain-text from Azure OCR, plain-text extracted by\nGPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o\nVision. To give an indication of the impact of possible prompt engineering, we\nassess the impact of shifting task instructions to the system prompt and\nexplicitly informing the model about the structured nature of the input. Our\nfindings reveal that GPT-4o demonstrates considerable robustness to variations\nin input structure, but lacks in overall performance. Conversely, GPT-4.1's\nperformance is markedly sensitive; poorly structured inputs yield suboptimal\nresults (but identical with GPT-4o), while well-structured formats (original\nCUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by\n~20 percentage points. Optimizing the system prompt to include task details and\nan advisory about structured input further elevates GPT-4.1's accuracy by an\nadditional ~10-13 percentage points, with Markdown ultimately achieving the\nhighest performance under these conditions (79 percentage points overall\nexact-match accuracy). This research empirically demonstrates that while newer\nmodels exhibit greater resilience, careful input structuring and strategic\nprompt design remain critical for optimizing the performance of LLMs, and can\nsignificantly affect outcomes in high-stakes legal applications."}
{"id": "2505.11574", "pdf": "https://arxiv.org/pdf/2505.11574", "abs": "https://arxiv.org/abs/2505.11574", "authors": ["Zhen Li", "Yupeng Su", "Songmiao Wang", "Runming Yang", "Congkai Xie", "Aofan Liu", "Ming Li", "Jiannong Cao", "Yuan Xie", "Ngai Wong", "Hongxia Yang"], "title": "InfiJanice: Joint Analysis and In-situ Correction Engine for Quantization-Induced Math Degradation in Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "23pages", "summary": "Large Language Models (LLMs) have demonstrated impressive performance on\ncomplex reasoning benchmarks such as GSM8K, MATH, and AIME. However, the\nsubstantial computational demands of these tasks pose significant challenges\nfor real-world deployment. Model quantization has emerged as a promising\napproach to reduce memory footprint and inference latency by representing\nweights and activations with lower bit-widths. In this work, we conduct a\ncomprehensive study of mainstream quantization methods(e.g., AWQ, GPTQ,\nSmoothQuant) on the most popular open-sourced models (e.g., Qwen2.5, LLaMA3\nseries), and reveal that quantization can degrade mathematical reasoning\naccuracy by up to 69.81%. To better understand this degradation, we develop an\nautomated assignment and judgment pipeline that qualitatively categorizes\nfailures into four error types and quantitatively identifies the most impacted\nreasoning capabilities. Building on these findings, we employ an automated\ndata-curation pipeline to construct a compact \"Silver Bullet\" datasets.\nTraining a quantized model on as few as 332 carefully selected examples for\njust 3-5 minutes on a single GPU is enough to restore its reasoning accuracy to\nmatch that of the full-precision baseline."}
{"id": "2505.12353", "pdf": "https://arxiv.org/pdf/2505.12353", "abs": "https://arxiv.org/abs/2505.12353", "authors": ["Prakash Palanivelu Rajmohan", "Fred Roosta"], "title": "Importance Sampling for Nonlinear Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "This work is accepted at ICML 2025", "summary": "While norm-based and leverage-score-based methods have been extensively\nstudied for identifying \"important\" data points in linear models, analogous\ntools for nonlinear models remain significantly underdeveloped. By introducing\nthe concept of the adjoint operator of a nonlinear map, we address this gap and\ngeneralize norm-based and leverage-score-based importance sampling to nonlinear\nsettings. We demonstrate that sampling based on these generalized notions of\nnorm and leverage scores provides approximation guarantees for the underlying\nnonlinear mapping, similar to linear subspace embeddings. As direct\napplications, these nonlinear scores not only reduce the computational\ncomplexity of training nonlinear models by enabling efficient sampling over\nlarge datasets but also offer a novel mechanism for model explainability and\noutlier detection. Our contributions are supported by both theoretical analyses\nand experimental results across a variety of supervised learning scenarios."}
{"id": "2505.12859", "pdf": "https://arxiv.org/pdf/2505.12859", "abs": "https://arxiv.org/abs/2505.12859", "authors": ["Lucas Georges Gabriel Charpentier", "Pierre Lison"], "title": "Re-identification of De-identified Documents with Autoregressive Infilling", "categories": ["cs.CL"], "comment": "To be presented a ACL 2025, Main, Long paper", "summary": "Documents revealing sensitive information about individuals must typically be\nde-identified. This de-identification is often done by masking all mentions of\npersonally identifiable information (PII), thereby making it more difficult to\nuncover the identity of the person(s) in question. To investigate the\nrobustness of de-identification methods, we present a novel, RAG-inspired\napproach that attempts the reverse process of re-identification based on a\ndatabase of documents representing background knowledge. Given a text in which\npersonal identifiers have been masked, the re-identification proceeds in two\nsteps. A retriever first selects from the background knowledge passages deemed\nrelevant for the re-identification. Those passages are then provided to an\ninfilling model which seeks to infer the original content of each text span.\nThis process is repeated until all masked spans are replaced. We evaluate the\nre-identification on three datasets (Wikipedia biographies, court rulings and\nclinical notes). Results show that (1) as many as 80% of de-identified text\nspans can be successfully recovered and (2) the re-identification accuracy\nincreases along with the level of background knowledge."}
{"id": "2505.11576", "pdf": "https://arxiv.org/pdf/2505.11576", "abs": "https://arxiv.org/abs/2505.11576", "authors": ["Shuchen Wu", "Stephan Alaniz", "Shyamgopal Karthik", "Peter Dayan", "Eric Schulz", "Zeynep Akata"], "title": "Concept-Guided Interpretability via Neural Chunking", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "35 pages, 32 figures. arXiv admin note: text overlap with\n  arXiv:2502.01803", "summary": "Neural networks are often black boxes, reflecting the significant challenge\nof understanding their internal workings. We propose a different perspective\nthat challenges the prevailing view: rather than being inscrutable, neural\nnetworks exhibit patterns in their raw population activity that mirror\nregularities in the training data. We refer to this as the Reflection\nHypothesis and provide evidence for this phenomenon in both simple recurrent\nneural networks (RNNs) and complex large language models (LLMs). Building on\nthis insight, we propose to leverage cognitively-inspired methods of chunking\nto segment high-dimensional neural population dynamics into interpretable units\nthat reflect underlying concepts. We propose three methods to extract these\nemerging entities, complementing each other based on label availability and\ndimensionality. Discrete sequence chunking (DSC) creates a dictionary of\nentities; population averaging (PA) extracts recurring entities that correspond\nto known labels; and unsupervised chunk discovery (UCD) can be used when labels\nare absent. We demonstrate the effectiveness of these methods in extracting\nentities across varying model sizes, ranging from inducing compositionality in\nRNNs to uncovering recurring neural population states in large models with\ndiverse architectures, and illustrate their advantage over other methods.\nThroughout, we observe a robust correspondence between the extracted entities\nand concrete or abstract concepts. Artificially inducing the extracted entities\nin neural populations effectively alters the network's generation of associated\nconcepts. Our work points to a new direction for interpretability, one that\nharnesses both cognitive principles and the structure of naturalistic data to\nreveal the hidden computations of complex learning systems, gradually\ntransforming them from black boxes into systems we can begin to understand."}
{"id": "2505.12354", "pdf": "https://arxiv.org/pdf/2505.12354", "abs": "https://arxiv.org/abs/2505.12354", "authors": ["Anton Bolychev", "Georgiy Malaniya", "Grigory Yaremenko", "Anastasia Krasnaya", "Pavel Osinenko"], "title": "A universal policy wrapper with guarantees", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "We introduce a universal policy wrapper for reinforcement learning agents\nthat ensures formal goal-reaching guarantees. In contrast to standard\nreinforcement learning algorithms that excel in performance but lack rigorous\nsafety assurances, our wrapper selectively switches between a high-performing\nbase policy -- derived from any existing RL method -- and a fallback policy\nwith known convergence properties. Base policy's value function supervises this\nswitching process, determining when the fallback policy should override the\nbase policy to ensure the system remains on a stable path. The analysis proves\nthat our wrapper inherits the fallback policy's goal-reaching guarantees while\npreserving or improving upon the performance of the base policy. Notably, it\noperates without needing additional system knowledge or online constrained\noptimization, making it readily deployable across diverse reinforcement\nlearning architectures and tasks."}
{"id": "2505.12864", "pdf": "https://arxiv.org/pdf/2505.12864", "abs": "https://arxiv.org/abs/2505.12864", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan Hermstrüwer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "comment": null, "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/"}
{"id": "2505.11577", "pdf": "https://arxiv.org/pdf/2505.11577", "abs": "https://arxiv.org/abs/2505.11577", "authors": ["FLorian A. D. Burnat", "Brittany I. Davidson"], "title": "The Accountability Paradox: How Platform API Restrictions Undermine AI Transparency Mandates", "categories": ["cs.CY", "cs.AI", "I.2.0; E.0; K.4.1; K.4.2; K.4.3; K.5.0; K.5.2"], "comment": null, "summary": "Recent application programming interface (API) restrictions on major social\nmedia platforms challenge compliance with the EU Digital Services Act [20],\nwhich mandates data access for algorithmic transparency. We develop a\nstructured audit framework to assess the growing misalignment between\nregulatory requirements and platform implementations. Our comparative analysis\nof X/Twitter, Reddit, TikTok, and Meta identifies critical ``audit\nblind-spots'' where platform content moderation and algorithmic amplification\nremain inaccessible to independent verification. Our findings reveal an\n``accountability paradox'': as platforms increasingly rely on AI systems, they\nsimultaneously restrict the capacity for independent oversight. We propose\ntargeted policy interventions aligned with the AI Risk Management Framework of\nthe National Institute of Standards and Technology [80], emphasizing federated\naccess models and enhanced regulatory enforcement."}
{"id": "2505.12358", "pdf": "https://arxiv.org/pdf/2505.12358", "abs": "https://arxiv.org/abs/2505.12358", "authors": ["Abrar Rahman Abir", "Haz Sameen Shahgir", "Md Rownok Zahan Ratul", "Md Toki Tahmid", "Greg Ver Steeg", "Yue Dong"], "title": "AbFlowNet: Optimizing Antibody-Antigen Binding Energy via Diffusion-GFlowNet Fusion", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Complementarity Determining Regions (CDRs) are critical segments of an\nantibody that facilitate binding to specific antigens. Current computational\nmethods for CDR design utilize reconstruction losses and do not jointly\noptimize binding energy, a crucial metric for antibody efficacy. Rather,\nbinding energy optimization is done through computationally expensive Online\nReinforcement Learning (RL) pipelines rely heavily on unreliable binding energy\nestimators. In this paper, we propose AbFlowNet, a novel generative framework\nthat integrates GFlowNet with Diffusion models. By framing each diffusion step\nas a state in the GFlowNet framework, AbFlowNet jointly optimizes standard\ndiffusion losses and binding energy by directly incorporating energy signals\ninto the training process, thereby unifying diffusion and reward optimization\nin a single procedure. Experimental results show that AbFlowNet outperforms the\nbase diffusion model by 3.06% in amino acid recovery, 20.40% in geometric\nreconstruction (RMSD), and 3.60% in binding energy improvement ratio. ABFlowNet\nalso decreases Top-1 total energy and binding energy errors by 24.8% and 38.1%\nwithout pseudo-labeling the test dataset or using computationally expensive\nonline RL regimes."}
{"id": "2505.12888", "pdf": "https://arxiv.org/pdf/2505.12888", "abs": "https://arxiv.org/abs/2505.12888", "authors": ["Jialun Zhong", "Yanzeng Li", "Sen Hu", "Yang Zhang", "Teng Xu", "Lei Zou"], "title": "GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation", "categories": ["cs.CL"], "comment": null, "summary": "Medication recommendations have become an important task in the healthcare\ndomain, especially in measuring the accuracy and safety of medical dialogue\nsystems (MDS). Different from the recommendation task based on electronic\nhealth records (EHRs), dialogue-based medication recommendations require\nresearch on the interaction details between patients and doctors, which is\ncrucial but may not exist in EHRs. Recent advancements in large language models\n(LLM) have extended the medical dialogue domain. These LLMs can interpret\npatients' intent and provide medical suggestions including medication\nrecommendations, but some challenges are still worth attention. During a\nmulti-turn dialogue, LLMs may ignore the fine-grained medical information or\nconnections across the dialogue turns, which is vital for providing accurate\nsuggestions. Besides, LLMs may generate non-factual responses when there is a\nlack of domain-specific knowledge, which is more risky in the medical domain.\nTo address these challenges, we propose a \\textbf{G}raph-\\textbf{A}ssisted\n\\textbf{P}rompts (\\textbf{GAP}) framework for dialogue-based medication\nrecommendation. It extracts medical concepts and corresponding states from\ndialogue to construct an explicitly patient-centric graph, which can describe\nthe neglected but important information. Further, combined with external\nmedical knowledge graphs, GAP can generate abundant queries and prompts, thus\nretrieving information from multiple sources to reduce the non-factual\nresponses. We evaluate GAP on a dialogue-based medication recommendation\ndataset and further explore its potential in a more difficult scenario,\ndynamically diagnostic interviewing. Extensive experiments demonstrate its\ncompetitive performance when compared with strong baselines."}
{"id": "2505.11578", "pdf": "https://arxiv.org/pdf/2505.11578", "abs": "https://arxiv.org/abs/2505.11578", "authors": ["Peimian Du", "Jiabin Liu", "Xiaowei Jin", "Mengwang Zuo", "Hui Li"], "title": "Spatiotemporal Field Generation Based on Hybrid Mamba-Transformer with Physics-informed Fine-tuning", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "comment": null, "summary": "This research confronts the challenge of substantial physical equation\ndiscrepancies encountered in the generation of spatiotemporal physical fields\nthrough data-driven trained models. A spatiotemporal physical field generation\nmodel, named HMT-PF, is developed based on the hybrid Mamba-Transformer\narchitecture, incorporating unstructured grid information as input. A\nfine-tuning block, enhanced with physical information, is introduced to\neffectively reduce the physical equation discrepancies. The physical equation\nresiduals are computed through a point query mechanism for efficient gradient\nevaluation, then encoded into latent space for refinement. The fine-tuning\nprocess employs a self-supervised learning approach to achieve physical\nconsistency while maintaining essential field characteristics. Results show\nthat the hybrid Mamba-Transformer model achieves good performance in generating\nspatiotemporal fields, while the physics-informed fine-tuning mechanism further\nreduces significant physical errors effectively. A MSE-R evaluation method is\ndeveloped to assess the accuracy and realism of physical field generation."}
{"id": "2505.12359", "pdf": "https://arxiv.org/pdf/2505.12359", "abs": "https://arxiv.org/abs/2505.12359", "authors": ["Yichen Guo", "Hanze Li", "Zonghao Zhang", "Jinhao You", "Kai Tang", "Xiande Huang"], "title": "STAR: Stage-Wise Attention-Guided Token Reduction for Efficient Large Vision-Language Models Inference", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Although large vision-language models (LVLMs) leverage rich visual token\nrepresentations to achieve strong performance on multimodal tasks, these tokens\nalso introduce significant computational overhead during inference. Existing\ntraining-free token pruning methods typically adopt a single-stage strategy,\nfocusing either on visual self-attention or visual-textual cross-attention.\nHowever, such localized perspectives often overlook the broader information\nflow across the model, leading to substantial performance degradation,\nespecially under high pruning ratios. In this work, we propose STAR (Stage-wise\nAttention-guided token Reduction), a training-free, plug-and-play framework\nthat approaches token pruning from a global perspective. Instead of pruning at\na single point, STAR performs attention-guided reduction in two complementary\nstages: an early-stage pruning based on visual self-attention to remove\nredundant low-level features, and a later-stage pruning guided by cross-modal\nattention to discard task-irrelevant tokens. This holistic approach allows STAR\nto significantly reduce computational cost while better preserving\ntask-critical information. Extensive experiments across multiple LVLM\narchitectures and benchmarks show that STAR achieves strong acceleration while\nmaintaining comparable, and in some cases even improved performance."}
{"id": "2505.12896", "pdf": "https://arxiv.org/pdf/2505.12896", "abs": "https://arxiv.org/abs/2505.12896", "authors": ["Chenxi Liu", "Yongqiang Chen", "Tongliang Liu", "James Cheng", "Bo Han", "Kun Zhang"], "title": "On the Thinking-Language Modeling Gap in Large Language Models", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "Chenxi and Yongqiang contributed equally; project page:\n  https://causalcoat.github.io/lot.html", "summary": "System 2 reasoning is one of the defining characteristics of intelligence,\nwhich requires slow and logical thinking. Human conducts System 2 reasoning via\nthe language of thoughts that organizes the reasoning process as a causal\nsequence of mental language, or thoughts. Recently, it has been observed that\nSystem 2 reasoning can be elicited from Large Language Models (LLMs)\npre-trained on large-scale natural languages. However, in this work, we show\nthat there is a significant gap between the modeling of languages and thoughts.\nAs language is primarily a tool for humans to share knowledge and thinking,\nmodeling human language can easily absorb language biases into LLMs deviated\nfrom the chain of thoughts in minds. Furthermore, we show that the biases will\nmislead the eliciting of \"thoughts\" in LLMs to focus only on a biased part of\nthe premise. To this end, we propose a new prompt technique termed\nLanguage-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of\ndirectly eliciting the chain of thoughts from partial information, LoT\ninstructs LLMs to adjust the order and token used for the expressions of all\nthe relevant information. We show that the simple strategy significantly\nreduces the language modeling biases in LLMs and improves the performance of\nLLMs across a variety of reasoning tasks."}
{"id": "2505.11579", "pdf": "https://arxiv.org/pdf/2505.11579", "abs": "https://arxiv.org/abs/2505.11579", "authors": ["Zeynep Engin", "David Hand"], "title": "Toward Adaptive Categories: Dimensional Governance for Agentic AI", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.LG", "cs.MA"], "comment": "12 pages core text, 14 pages including references, 2 figures", "summary": "As AI systems evolve from static tools to dynamic agents, traditional\ncategorical governance frameworks -- based on fixed risk tiers, levels of\nautonomy, or human oversight models -- are increasingly insufficient on their\nown. Systems built on foundation models, self-supervised learning, and\nmulti-agent architectures increasingly blur the boundaries that categories were\ndesigned to police. In this Perspective, we make the case for dimensional\ngovernance: a framework that tracks how decision authority, process autonomy,\nand accountability (the 3As) distribute dynamically across human-AI\nrelationships. A critical advantage of this approach is its ability to\nexplicitly monitor system movement toward and across key governance thresholds,\nenabling preemptive adjustments before risks materialize. This dimensional\napproach provides the necessary foundation for more adaptive categorization,\nenabling thresholds and classifications that can evolve with emerging\ncapabilities. While categories remain essential for decision-making, building\nthem upon dimensional foundations allows for context-specific adaptability and\nstakeholder-responsive governance that static approaches cannot achieve. We\noutline key dimensions, critical trust thresholds, and practical examples\nillustrating where rigid categorical frameworks fail -- and where a dimensional\nmindset could offer a more resilient and future-proof path forward for both\ngovernance and innovation at the frontier of artificial intelligence."}
{"id": "2505.12366", "pdf": "https://arxiv.org/pdf/2505.12366", "abs": "https://arxiv.org/abs/2505.12366", "authors": ["Gang Li", "Ming Lin", "Tomer Galanti", "Zhengzhong Tu", "Tianbao Yang"], "title": "DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "20 pages, 4 figures", "summary": "The recent success and openness of DeepSeek-R1 have brought widespread\nattention to Group Relative Policy Optimization (GRPO) as a reinforcement\nlearning method for large reasoning models (LRMs). In this work, we analyze the\nGRPO objective under a binary reward setting and reveal an inherent limitation\nof question-level difficulty bias. We also identify a connection between GRPO\nand traditional discriminative methods in supervised learning. Motivated by\nthese insights, we introduce a new Discriminative Constrained Optimization\n(DisCO) framework for reinforcing LRMs, grounded in the principle of\ndiscriminative learning. The main differences between DisCO and GRPO and its\nrecent variants are: (1) it replaces the group relative objective with a\ndiscriminative objective defined by a scoring function; (2) it abandons\nclipping-based surrogates in favor of non-clipping RL surrogate objectives used\nas scoring functions; (3) it employs a simple yet effective constrained\noptimization approach to enforce the KL divergence constraint, ensuring stable\ntraining. As a result, DisCO offers notable advantages over GRPO and its\nvariants: (i) it completely eliminates difficulty bias by adopting\ndiscriminative objectives; (ii) it addresses the entropy instability in GRPO\nand its variants through the use of non-clipping scoring functions and a\nconstrained optimization approach; (iii) it allows the incorporation of\nadvanced discriminative learning techniques to address data imbalance, where a\nsignificant number of questions have more negative than positive generated\nanswers during training. Our experiments on enhancing the mathematical\nreasoning capabilities of SFT-finetuned models show that DisCO significantly\noutperforms GRPO and its improved variants such as DAPO, achieving average\ngains of 7\\% over GRPO and 6\\% over DAPO across six benchmark tasks for an 1.5B\nmodel."}
{"id": "2505.12920", "pdf": "https://arxiv.org/pdf/2505.12920", "abs": "https://arxiv.org/abs/2505.12920", "authors": ["Paul Van Eecke", "Katrien Beuls"], "title": "PyFCG: Fluid Construction Grammar in Python", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "We present PyFCG, an open source software library that ports Fluid\nConstruction Grammar (FCG) to the Python programming language. PyFCG enables\nits users to seamlessly integrate FCG functionality into Python programs, and\nto use FCG in combination with other libraries within Python's rich ecosystem.\nApart from a general description of the library, this paper provides three\nwalkthrough tutorials that demonstrate example usage of PyFCG in typical use\ncases of FCG: (i) formalising and testing construction grammar analyses, (ii)\nlearning usage-based construction grammars from corpora, and (iii) implementing\nagent-based experiments on emergent communication."}
{"id": "2505.11580", "pdf": "https://arxiv.org/pdf/2505.11580", "abs": "https://arxiv.org/abs/2505.11580", "authors": ["Andrew Liu", "Axel Elaldi", "Nicholas T Franklin", "Nathan Russell", "Gurinder S Atwal", "Yih-En A Ban", "Olivia Viessmann"], "title": "Flash Invariant Point Attention", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": null, "summary": "Invariant Point Attention (IPA) is a key algorithm for geometry-aware\nmodeling in structural biology, central to many protein and RNA models.\nHowever, its quadratic complexity limits the input sequence length. We\nintroduce FlashIPA, a factorized reformulation of IPA that leverages\nhardware-efficient FlashAttention to achieve linear scaling in GPU memory and\nwall-clock time with sequence length. FlashIPA matches or exceeds standard IPA\nperformance while substantially reducing computational costs. FlashIPA extends\ntraining to previously unattainable lengths, and we demonstrate this by\nre-training generative models without length restrictions and generating\nstructures of thousands of residues. FlashIPA is available at\nhttps://github.com/flagshippioneering/flash_ipa."}
{"id": "2505.12380", "pdf": "https://arxiv.org/pdf/2505.12380", "abs": "https://arxiv.org/abs/2505.12380", "authors": ["Han Weng", "Boyi Liu", "Yuanfeng Song", "Dun Zeng", "Yingxiang Yang", "Yi Zhan", "Longjie Cui", "Xiaoming Yin", "Yang Sun"], "title": "Graph-Reward-SQL: Execution-Free Reinforcement Learning for Text-to-SQL via Graph Matching and Stepwise Reward", "categories": ["cs.LG", "cs.DB", "cs.PL"], "comment": null, "summary": "Reinforcement learning (RL) has been widely adopted to enhance the\nperformance of large language models (LLMs) on Text-to-SQL tasks. However,\nexisting methods often rely on execution-based or LLM-based Bradley-Terry\nreward models. The former suffers from high execution latency caused by\nrepeated database calls, whereas the latter imposes substantial GPU memory\noverhead, both of which significantly hinder the efficiency and scalability of\nRL pipelines. To this end, we propose a novel Text-to-SQL RL fine-tuning\nframework named Graph-Reward-SQL, which employs the GMNScore outcome reward\nmodel. We leverage SQL graph representations to provide accurate reward signals\nwhile significantly reducing inference time and GPU memory usage. Building on\nthis foundation, we further introduce StepRTM, a stepwise reward model that\nprovides intermediate supervision over Common Table Expression (CTE)\nsubqueries. This encourages both functional correctness and structural clarity\nof SQL. Extensive comparative and ablation experiments on standard benchmarks,\nincluding Spider and BIRD, demonstrate that our method consistently outperforms\nexisting reward models."}
{"id": "2505.12929", "pdf": "https://arxiv.org/pdf/2505.12929", "abs": "https://arxiv.org/abs/2505.12929", "authors": ["Zhihe Yang", "Xufang Luo", "Zilong Wang", "Dongqi Han", "Zhiyuan He", "Dongsheng Li", "Yunjian Xu"], "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 12 figures", "summary": "Reinforcement learning (RL) has become a cornerstone for enhancing the\nreasoning capabilities of large language models (LLMs), with recent innovations\nsuch as Group Relative Policy Optimization (GRPO) demonstrating exceptional\neffectiveness. In this study, we identify a critical yet underexplored issue in\nRL training: low-probability tokens disproportionately influence model updates\ndue to their large gradient magnitudes. This dominance hinders the effective\nlearning of high-probability tokens, whose gradients are essential for LLMs'\nperformance but are substantially suppressed. To mitigate this interference, we\npropose two novel methods: Advantage Reweighting and Low-Probability Token\nIsolation (Lopti), both of which effectively attenuate gradients from\nlow-probability tokens while emphasizing parameter updates driven by\nhigh-probability tokens. Our approaches promote balanced updates across tokens\nwith varying probabilities, thereby enhancing the efficiency of RL training.\nExperimental results demonstrate that they substantially improve the\nperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K\nLogic Puzzle reasoning tasks. Our implementation is available at\nhttps://github.com/zhyang2226/AR-Lopti."}
{"id": "2505.11582", "pdf": "https://arxiv.org/pdf/2505.11582", "abs": "https://arxiv.org/abs/2505.11582", "authors": ["Lee Harris", "Philippe De Wilde", "James Bentham"], "title": "Comparing Lexical and Semantic Vector Search Methods When Classifying Medical Documents", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Classification is a common AI problem, and vector search is a typical\nsolution. This transforms a given body of text into a numerical representation,\nknown as an embedding, and modern improvements to vector search focus on\noptimising speed and predictive accuracy. This is often achieved through neural\nmethods that aim to learn language semantics. However, our results suggest that\nthese are not always the best solution. Our task was to classify\nrigidly-structured medical documents according to their content, and we found\nthat using off-the-shelf semantic vector search produced slightly worse\npredictive accuracy than creating a bespoke lexical vector search model, and\nthat it required significantly more time to execute. These findings suggest\nthat traditional methods deserve to be contenders in the information retrieval\ntoolkit, despite the prevalence and success of neural models."}
{"id": "2505.12387", "pdf": "https://arxiv.org/pdf/2505.12387", "abs": "https://arxiv.org/abs/2505.12387", "authors": ["Liu Ziyin", "Yizhou Xu", "Isaac Chuang"], "title": "Neural Thermodynamics I: Entropic Forces in Deep and Universal Representation Learning", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "math-ph", "math.MP", "q-bio.NC", "stat.ML"], "comment": "preprint", "summary": "With the rapid discovery of emergent phenomena in deep learning and large\nlanguage models, explaining and understanding their cause has become an urgent\nneed. Here, we propose a rigorous entropic-force theory for understanding the\nlearning dynamics of neural networks trained with stochastic gradient descent\n(SGD) and its variants. Building on the theory of parameter symmetries and an\nentropic loss landscape, we show that representation learning is crucially\ngoverned by emergent entropic forces arising from stochasticity and\ndiscrete-time updates. These forces systematically break continuous parameter\nsymmetries and preserve discrete ones, leading to a series of gradient balance\nphenomena that resemble the equipartition property of thermal systems. These\nphenomena, in turn, (a) explain the universal alignment of neural\nrepresentations between AI models and lead to a proof of the Platonic\nRepresentation Hypothesis, and (b) reconcile the seemingly contradictory\nobservations of sharpness- and flatness-seeking behavior of deep learning\noptimization. Our theory and experiments demonstrate that a combination of\nentropic forces and symmetry breaking is key to understanding emergent\nphenomena in deep learning."}
{"id": "2505.12942", "pdf": "https://arxiv.org/pdf/2505.12942", "abs": "https://arxiv.org/abs/2505.12942", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Xinye Cao", "Pedro Gimenes", "George A. Constantinides", "Wayne Luk", "Yiren Zhao"], "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."}
{"id": "2505.11586", "pdf": "https://arxiv.org/pdf/2505.11586", "abs": "https://arxiv.org/abs/2505.11586", "authors": ["Rui Zhang", "Yun Shen", "Hongwei Li", "Wenbo Jiang", "Hanxiao Chen", "Yuan Zhang", "Guowen Xu", "Yang Zhang"], "title": "The Ripple Effect: On Unforeseen Complications of Backdoor Attacks", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Recent research highlights concerns about the trustworthiness of third-party\nPre-Trained Language Models (PTLMs) due to potential backdoor attacks. These\nbackdoored PTLMs, however, are effective only for specific pre-defined\ndownstream tasks. In reality, these PTLMs can be adapted to many other\nunrelated downstream tasks. Such adaptation may lead to unforeseen consequences\nin downstream model outputs, consequently raising user suspicion and\ncompromising attack stealthiness. We refer to this phenomenon as backdoor\ncomplications. In this paper, we undertake the first comprehensive\nquantification of backdoor complications. Through extensive experiments using 4\nprominent PTLMs and 16 text classification benchmark datasets, we demonstrate\nthe widespread presence of backdoor complications in downstream models\nfine-tuned from backdoored PTLMs. The output distribution of triggered samples\nsignificantly deviates from that of clean samples. Consequently, we propose a\nbackdoor complication reduction method leveraging multi-task learning to\nmitigate complications without prior knowledge of downstream tasks. The\nexperimental results demonstrate that our proposed method can effectively\nreduce complications while maintaining the efficacy and consistency of backdoor\nattacks. Our code is available at\nhttps://github.com/zhangrui4041/Backdoor_Complications."}
{"id": "2505.12389", "pdf": "https://arxiv.org/pdf/2505.12389", "abs": "https://arxiv.org/abs/2505.12389", "authors": ["Su Yeong Jo", "Sanghyeon Park", "Seungchan Ko", "Jongcheon Park", "Hosung Kim", "Sangseung Lee", "Joongoo Jeon"], "title": "Engineering application of physics-informed neural networks for Saint-Venant torsion", "categories": ["cs.LG"], "comment": null, "summary": "The Saint-Venant torsion theory is a classical theory for analyzing the\ntorsional behavior of structural components, and it remains critically\nimportant in modern computational design workflows. Conventional numerical\nmethods, including the finite element method (FEM), typically rely on\nmesh-based approaches to obtain approximate solutions. However, these methods\noften require complex and computationally intensive techniques to overcome the\nlimitations of approximation, leading to significant increases in computational\ncost. The objective of this study is to develop a series of novel numerical\nmethods based on physics-informed neural networks (PINN) for solving the\nSaint-Venant torsion equations. Utilizing the expressive power and the\nautomatic differentiation capability of neural networks, the PINN can solve\npartial differential equations (PDEs) along with boundary conditions without\nthe need for intricate computational techniques. First, a PINN solver was\ndeveloped to compute the torsional constant for bars with arbitrary\ncross-sectional geometries. This was followed by the development of a solver\ncapable of handling cases with sharp geometric transitions; variable-scaling\nPINN (VS-PINN). Finally, a parametric PINN was constructed to address the\nlimitations of conventional single-instance PINN. The results from all three\nsolvers showed good agreement with reference solutions, demonstrating their\naccuracy and robustness. Each solver can be selectively utilized depending on\nthe specific requirements of torsional behavior analysis."}
{"id": "2505.12949", "pdf": "https://arxiv.org/pdf/2505.12949", "abs": "https://arxiv.org/abs/2505.12949", "authors": ["Cael Marquard", "Simbarashe Mawere", "Francois Meyer"], "title": "Neural Morphological Tagging for Nguni Languages", "categories": ["cs.CL"], "comment": null, "summary": "Morphological parsing is the task of decomposing words into morphemes, the\nsmallest units of meaning in a language, and labelling their grammatical roles.\nIt is a particularly challenging task for agglutinative languages, such as the\nNguni languages of South Africa, which construct words by concatenating\nmultiple morphemes. A morphological parsing system can be framed as a pipeline\nwith two separate components, a segmenter followed by a tagger. This paper\ninvestigates the use of neural methods to build morphological taggers for the\nfour Nguni languages. We compare two classes of approaches: training neural\nsequence labellers (LSTMs and neural CRFs) from scratch and finetuning\npretrained language models. We compare performance across these two categories,\nas well as to a traditional rule-based morphological parser. Neural taggers\ncomfortably outperform the rule-based baseline and models trained from scratch\ntend to outperform pretrained models. We also compare parsing results across\ndifferent upstream segmenters and with varying linguistic input features. Our\nfindings confirm the viability of employing neural taggers based on\npre-existing morphological segmenters for the Nguni languages."}
{"id": "2505.11594", "pdf": "https://arxiv.org/pdf/2505.11594", "abs": "https://arxiv.org/abs/2505.11594", "authors": ["Jintao Zhang", "Jia Wei", "Pengle Zhang", "Xiaoming Xu", "Haofeng Huang", "Haoxu Wang", "Kai Jiang", "Jun Zhu", "Jianfei Chen"], "title": "SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CV", "cs.PF"], "comment": null, "summary": "The efficiency of attention is important due to its quadratic time\ncomplexity. We enhance the efficiency of attention through two key\ncontributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to\naccelerate attention computation. Our implementation achieves 1038 TOPS on\nRTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.\nExperiments show that our FP4 attention can accelerate inference of various\nmodels in a plug-and-play way. Second, we pioneer low-bit attention to training\ntasks. Existing low-bit attention works like FlashAttention3 and SageAttention\nfocus only on inference. However, the efficiency of training large models is\nalso important. To explore whether low-bit attention can be effectively applied\nto training tasks, we design an accurate and efficient 8-bit attention for both\nforward and backward propagation. Experiments indicate that 8-bit attention\nachieves lossless performance in fine-tuning tasks but exhibits slower\nconvergence in pretraining tasks. The code will be available at\nhttps://github.com/thu-ml/SageAttention."}
{"id": "2505.12395", "pdf": "https://arxiv.org/pdf/2505.12395", "abs": "https://arxiv.org/abs/2505.12395", "authors": ["Udaya Shreyas", "L. N. Aadarsh"], "title": "Few-Shot Concept Unlearning with Low Rank Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Image Generation models are a trending topic nowadays, with many people\nutilizing Artificial Intelligence models in order to generate images. There are\nmany such models which, given a prompt of a text, will generate an image which\ndepicts said prompt. There are many image generation models, such as Latent\nDiffusion Models, Denoising Diffusion Probabilistic Models, Generative\nAdversarial Networks and many more. When generating images, these models can\ngenerate sensitive image data, which can be threatening to privacy or may\nviolate copyright laws of private entities. Machine unlearning aims at removing\nthe influence of specific data subsets from the trained models and in the case\nof image generation models, remove the influence of a concept such that the\nmodel is unable to generate said images of the concept when prompted.\nConventional retraining of the model can take upto days, hence fast algorithms\nare the need of the hour. In this paper we propose an algorithm that aims to\nremove the influence of concepts in diffusion models through updating the\ngradients of the final layers of the text encoders. Using a weighted loss\nfunction, we utilize backpropagation in order to update the weights of the\nfinal layers of the Text Encoder componet of the Stable Diffusion Model,\nremoving influence of the concept from the text-image embedding space, such\nthat when prompted, the result is an image not containing the concept. The\nweighted loss function makes use of Textual Inversion and Low-Rank\nAdaptation.We perform our experiments on Latent Diffusion Models, namely the\nStable Diffusion v2 model, with an average concept unlearning runtime of 50\nseconds using 4-5 images."}
{"id": "2505.12950", "pdf": "https://arxiv.org/pdf/2505.12950", "abs": "https://arxiv.org/abs/2505.12950", "authors": ["Daehee Kim", "Deokhyung Kang", "Jonghwi Kim", "Sangwon Ryu", "Gary Geunbae Lee"], "title": "GuRE:Generative Query REwriter for Legal Passage Retrieval", "categories": ["cs.CL"], "comment": "14 pages, 9 figures", "summary": "Legal Passage Retrieval (LPR) systems are crucial as they help practitioners\nsave time when drafting legal arguments. However, it remains an underexplored\navenue. One primary reason is the significant vocabulary mismatch between the\nquery and the target passage. To address this, we propose a simple yet\neffective method, the Generative query REwriter (GuRE). We leverage the\ngenerative capabilities of Large Language Models (LLMs) by training the LLM for\nquery rewriting. \"Rewritten queries\" help retrievers to retrieve target\npassages by mitigating vocabulary mismatch. Experimental results show that GuRE\nsignificantly improves performance in a retriever-agnostic manner,\noutperforming all baseline methods. Further analysis reveals that different\ntraining objectives lead to distinct retrieval behaviors, making GuRE more\nsuitable than direct retriever fine-tuning for real-world applications. Codes\nare avaiable at github.com/daehuikim/GuRE."}
{"id": "2505.11595", "pdf": "https://arxiv.org/pdf/2505.11595", "abs": "https://arxiv.org/abs/2505.11595", "authors": ["Peter Chen", "Xiaopeng Li", "Ziniu Li", "Xi Chen", "Tianyi Lin"], "title": "Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "28 pages", "summary": "Reinforcement learning (RL) has demonstrated significant success in enhancing\nreasoning capabilities in large language models (LLMs). One of the most widely\nused RL methods is Group Relative Policy Optimization\n(GRPO)~\\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and\nsuccess in training DeepSeek-R1~\\cite{Guo-2025-Deepseek}. However, GRPO stalls\nwhen all sampled responses in a group are incorrect -- referred to as an\n\\emph{all-negative-sample} group -- as it fails to update the policy, hindering\nlearning progress. The contributions of this paper are two-fold. First, we\npropose a simple yet effective framework that introduces response diversity\nwithin all-negative-sample groups in GRPO using AI feedback. We also provide a\ntheoretical analysis, via a stylized model, showing how this diversification\nimproves learning dynamics. Second, we empirically validate our approach,\nshowing the improved performance across various model sizes (7B, 14B, 32B) in\nboth offline and online learning settings with 10 benchmarks, including base\nand distilled variants. Our findings highlight that learning from\nall-negative-sample groups is not only feasible but beneficial, advancing\nrecent insights from \\citet{Xiong-2025-Minimalist}."}
{"id": "2505.12404", "pdf": "https://arxiv.org/pdf/2505.12404", "abs": "https://arxiv.org/abs/2505.12404", "authors": ["Piotr Piękos", "Subhradeep Kayal", "Alexandros Karatzoglou"], "title": "Hyperbolic Residual Quantization: Discrete Representations for Data with Latent Hierarchies", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Hierarchical data arise in countless domains, from biological taxonomies and\norganizational charts to legal codes and knowledge graphs. Residual\nQuantization (RQ) is widely used to generate discrete, multitoken\nrepresentations for such data by iteratively quantizing residuals in a\nmultilevel codebook. However, its reliance on Euclidean geometry can introduce\nfundamental mismatches that hinder modeling of hierarchical branching,\nnecessary for faithful representation of hierarchical data. In this work, we\npropose Hyperbolic Residual Quantization (HRQ), which embeds data natively in a\nhyperbolic manifold and performs residual quantization using hyperbolic\noperations and distance metrics. By adapting the embedding network, residual\ncomputation, and distance metric to hyperbolic geometry, HRQ imparts an\ninductive bias that aligns naturally with hierarchical branching. We claim that\nHRQ in comparison to RQ can generate more useful for downstream tasks discrete\nhierarchical representations for data with latent hierarchies. We evaluate HRQ\non two tasks: supervised hierarchy modeling using WordNet hypernym trees, where\nthe model is supervised to learn the latent hierarchy - and hierarchy\ndiscovery, where, while latent hierarchy exists in the data, the model is not\ndirectly trained or evaluated on a task related to the hierarchy. Across both\nscenarios, HRQ hierarchical tokens yield better performance on downstream tasks\ncompared to Euclidean RQ with gains of up to $20\\%$ for the hierarchy modeling\ntask. Our results demonstrate that integrating hyperbolic geometry into\ndiscrete representation learning substantially enhances the ability to capture\nlatent hierarchies."}
{"id": "2505.12964", "pdf": "https://arxiv.org/pdf/2505.12964", "abs": "https://arxiv.org/abs/2505.12964", "authors": ["Shanshan Liu", "Noriki Nishida", "Rumana Ferdous Munne", "Narumi Tokunaga", "Yuki Yamagata", "Kouji Kozaki", "Yuji Matsumoto"], "title": "MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition", "categories": ["cs.CL"], "comment": "preprint", "summary": "Recognizing biomedical concepts in the text is vital for ontology refinement,\nknowledge graph construction, and concept relationship discovery. However,\ntraditional concept recognition methods, relying on explicit mention\nidentification, often fail to capture complex concepts not explicitly stated in\nthe text. To overcome this limitation, we introduce MA-COIR, a framework that\nreformulates concept recognition as an indexing-recognition task. By assigning\nsemantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in\nontology entries and enhances recognition efficiency. Using a pretrained\nBART-based model fine-tuned on small datasets, our approach reduces\ncomputational requirements to facilitate adoption by domain experts.\nFurthermore, we incorporate large language models (LLMs)-generated queries and\nsynthetic data to improve recognition in low-resource settings. Experimental\nresults on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of\nMA-COIR in recognizing both explicit and implicit concepts without the need for\nmention-level annotations during inference, advancing ontology-driven concept\nrecognition in biomedical domain applications. Our code and constructed data\nare available at https://github.com/sl-633/macoir-master."}
{"id": "2505.11601", "pdf": "https://arxiv.org/pdf/2505.11601", "abs": "https://arxiv.org/abs/2505.11601", "authors": ["Rui Liu", "Rui Xie", "Zijun Yao", "Yanjie Fu", "Dongjie Wang"], "title": "Continuous Optimization for Feature Selection with Permutation-Invariant Embedding and Policy-Guided Search", "categories": ["cs.LG", "cs.AI"], "comment": "KDD 2025", "summary": "Feature selection removes redundant features to enhanc performance and\ncomputational efficiency in downstream tasks. Existing works often struggle to\ncapture complex feature interactions and adapt to diverse scenarios. Recent\nadvances in this domain have incorporated generative intelligence to address\nthese drawbacks by uncovering intricate relationships between features.\nHowever, two key limitations remain: 1) embedding feature subsets in a\ncontinuous space is challenging due to permutation sensitivity, as changes in\nfeature order can introduce biases and weaken the embedding learning process;\n2) gradient-based search in the embedding space assumes convexity, which is\nrarely guaranteed, leading to reduced search effectiveness and suboptimal\nsubsets. To address these limitations, we propose a new framework that can: 1)\npreserve feature subset knowledge in a continuous embedding space while\nensuring permutation invariance; 2) effectively explore the embedding space\nwithout relying on strong convex assumptions. For the first objective, we\ndevelop an encoder-decoder paradigm to preserve feature selection knowledge\ninto a continuous embedding space. This paradigm captures feature interactions\nthrough pairwise relationships within the subset, removing the influence of\nfeature order on the embedding. Moreover, an inducing point mechanism is\nintroduced to accelerate pairwise relationship computations. For the second\nobjective, we employ a policy-based reinforcement learning (RL) approach to\nguide the exploration of the embedding space. The RL agent effectively\nnavigates the space by balancing multiple objectives. By prioritizing\nhigh-potential regions adaptively and eliminating the reliance on convexity\nassumptions, the RL agent effectively reduces the risk of converging to local\noptima. Extensive experiments demonstrate the effectiveness, efficiency,\nrobustness and explicitness of our model."}
{"id": "2505.12411", "pdf": "https://arxiv.org/pdf/2505.12411", "abs": "https://arxiv.org/abs/2505.12411", "authors": ["Harel Mendelman", "Haggai Maron", "Ronen Talmon"], "title": "It Takes a Graph to Know a Graph: Rewiring for Homophily with a Reference Graph", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) excel at analyzing graph-structured data but\nstruggle on heterophilic graphs, where connected nodes often belong to\ndifferent classes. While this challenge is commonly addressed with specialized\nGNN architectures, graph rewiring remains an underexplored strategy in this\ncontext. We provide theoretical foundations linking edge homophily, GNN\nembedding smoothness, and node classification performance, motivating the need\nto enhance homophily. Building on this insight, we introduce a rewiring\nframework that increases graph homophily using a reference graph, with\ntheoretical guarantees on the homophily of the rewired graph. To broaden\napplicability, we propose a label-driven diffusion approach for constructing a\nhomophilic reference graph from node features and training labels. Through\nextensive simulations, we analyze how the homophily of both the original and\nreference graphs influences the rewired graph homophily and downstream GNN\nperformance. We evaluate our method on 11 real-world heterophilic datasets and\nshow that it outperforms existing rewiring techniques and specialized GNNs for\nheterophilic graphs, achieving improved node classification accuracy while\nremaining efficient and scalable to large graphs."}
{"id": "2505.12969", "pdf": "https://arxiv.org/pdf/2505.12969", "abs": "https://arxiv.org/abs/2505.12969", "authors": ["Yingzhi Wang", "Anas Alhmoud", "Saad Alsahly", "Muhammad Alqurishi", "Mirco Ravanelli"], "title": "Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down", "categories": ["cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "OpenAI's Whisper has achieved significant success in Automatic Speech\nRecognition. However, it has consistently been found to exhibit hallucination\nissues, particularly in non-speech segments, which limits its broader\napplication in complex industrial settings.\n  In this paper, we introduce a novel method to reduce Whisper's hallucination\non non-speech segments without using any pre- or post-possessing techniques.\nSpecifically, we benchmark the contribution of each self-attentional head in\nthe Whisper-large-v3 decoder to the hallucination problem by performing a\nhead-wise mask. Our findings reveal that only 3 of the 20 heads account for\nover 75% of the hallucinations on the UrbanSound dataset. We then fine-tune\nthese three crazy heads using a collection of non-speech data. The results show\nthat our best fine-tuned model, namely Calm-Whisper, achieves over 80%\nreduction in non-speech hallucination with only less than 0.1% WER degradation\non LibriSpeech test-clean and test-other."}
{"id": "2505.11615", "pdf": "https://arxiv.org/pdf/2505.11615", "abs": "https://arxiv.org/abs/2505.11615", "authors": ["Jian-Qiao Zhu", "Haijiang Yan", "Thomas L. Griffiths"], "title": "Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Changing the behavior of large language models (LLMs) can be as\nstraightforward as editing the Transformer's residual streams using\nappropriately constructed \"steering vectors.\" These modifications to internal\nneural activations, a form of representation engineering, offer an effective\nand targeted means of influencing model behavior without retraining or\nfine-tuning the model. But how can such steering vectors be systematically\nidentified? We propose a principled approach for uncovering steering vectors by\naligning latent representations elicited through behavioral methods\n(specifically, Markov chain Monte Carlo with LLMs) with their neural\ncounterparts. To evaluate this approach, we focus on extracting latent risk\npreferences from LLMs and steering their risk-related outputs using the aligned\nrepresentations as steering vectors. We show that the resulting steering\nvectors successfully and reliably modulate LLM outputs in line with the\ntargeted behavior."}
{"id": "2505.12419", "pdf": "https://arxiv.org/pdf/2505.12419", "abs": "https://arxiv.org/abs/2505.12419", "authors": ["Jiahan Zhang", "Tao Luo", "Yaoyu Zhang"], "title": "Embedding principle of homogeneous neural network for classification problem", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Understanding the convergence points and optimization landscape of neural\nnetworks is crucial, particularly for homogeneous networks where\nKarush-Kuhn-Tucker (KKT) points of the associated maximum-margin problem often\ncharacterize solutions. This paper investigates the relationship between such\nKKT points across networks of different widths generated via neuron splitting.\nWe introduce and formalize the \\textbf{KKT point embedding principle},\nestablishing that KKT points of a homogeneous network's max-margin problem\n($P_{\\Phi}$) can be embedded into the KKT points of a larger network's problem\n($P_{\\tilde{\\Phi}}$) via specific linear isometric transformations\ncorresponding to neuron splitting. We rigorously prove this principle holds for\nneuron splitting in both two-layer and deep homogeneous networks. Furthermore,\nwe connect this static embedding to the dynamics of gradient flow training with\nsmooth losses. We demonstrate that trajectories initiated from appropriately\nmapped points remain mapped throughout training and that the resulting\n$\\omega$-limit sets of directions are correspondingly mapped ($T(L(\\theta(0)))\n= L(\\boldsymbol{\\eta}(0))$), thereby preserving the alignment with KKT\ndirections dynamically when directional convergence occurs. Our findings offer\ninsights into the effects of network width, parameter redundancy, and the\nstructural connections between solutions found via optimization in homogeneous\nnetworks of varying sizes."}
{"id": "2505.12970", "pdf": "https://arxiv.org/pdf/2505.12970", "abs": "https://arxiv.org/abs/2505.12970", "authors": ["Robin Jegan", "Andreas Henrich"], "title": "A Structured Literature Review on Traditional Approaches in Current Natural Language Processing", "categories": ["cs.CL"], "comment": "14 pages, 1 figure", "summary": "The continued rise of neural networks and large language models in the more\nrecent past has altered the natural language processing landscape, enabling new\napproaches towards typical language tasks and achieving mainstream success.\nDespite the huge success of large language models, many disadvantages still\nremain and through this work we assess the state of the art in five application\nscenarios with a particular focus on the future perspectives and sensible\napplication scenarios of traditional and older approaches and techniques.\n  In this paper we survey recent publications in the application scenarios\nclassification, information and relation extraction, text simplification as\nwell as text summarization. After defining our terminology, i.e., which\nfeatures are characteristic for traditional techniques in our interpretation\nfor the five scenarios, we survey if such traditional approaches are still\nbeing used, and if so, in what way they are used. It turns out that all five\napplication scenarios still exhibit traditional models in one way or another,\nas part of a processing pipeline, as a comparison/baseline to the core model of\nthe respective paper, or as the main model(s) of the paper. For the complete\nstatistics, see https://zenodo.org/records/13683801"}
{"id": "2505.11621", "pdf": "https://arxiv.org/pdf/2505.11621", "abs": "https://arxiv.org/abs/2505.11621", "authors": ["Junhyung Park", "Patrick Bloebaum", "Shiva Prasad Kasiviswanathan"], "title": "A Classical View on Benign Overfitting: The Role of Sample Size", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "The results here subsume: arXiv:2410.06191", "summary": "Benign overfitting is a phenomenon in machine learning where a model\nperfectly fits (interpolates) the training data, including noisy examples, yet\nstill generalizes well to unseen data. Understanding this phenomenon has\nattracted considerable attention in recent years. In this work, we introduce a\nconceptual shift, by focusing on almost benign overfitting, where models\nsimultaneously achieve both arbitrarily small training and test errors. This\nbehavior is characteristic of neural networks, which often achieve low (but\nnon-zero) training error while still generalizing well. We hypothesize that\nthis almost benign overfitting can emerge even in classical regimes, by\nanalyzing how the interaction between sample size and model complexity enables\nlarger models to achieve both good training fit but still approach\nBayes-optimal generalization. We substantiate this hypothesis with theoretical\nevidence from two case studies: (i) kernel ridge regression, and (ii)\nleast-squares regression using a two-layer fully connected ReLU neural network\ntrained via gradient flow. In both cases, we overcome the strong assumptions\noften required in prior work on benign overfitting.\n  Our results on neural networks also provide the first generalization result\nin this setting that does not rely on any assumptions about the underlying\nregression function or noise, beyond boundedness. Our analysis introduces a\nnovel proof technique based on decomposing the excess risk into estimation and\napproximation errors, interpreting gradient flow as an implicit regularizer,\nthat helps avoid uniform convergence traps. This analysis idea could be of\nindependent interest."}
{"id": "2505.12421", "pdf": "https://arxiv.org/pdf/2505.12421", "abs": "https://arxiv.org/abs/2505.12421", "authors": ["Emanuele La Malfa", "Jon Vadillo", "Marco Molinari", "Michael Wooldridge"], "title": "Fixed Point Explainability", "categories": ["cs.LG", "cs.AI"], "comment": "Code: https://github.com/EmanueleLM/fixed-point-explainability", "summary": "This paper introduces a formal notion of fixed point explanations, inspired\nby the \"why regress\" principle, to assess, through recursive applications, the\nstability of the interplay between a model and its explainer. Fixed point\nexplanations satisfy properties like minimality, stability, and faithfulness,\nrevealing hidden model behaviours and explanatory weaknesses. We define\nconvergence conditions for several classes of explainers, from feature-based to\nmechanistic tools like Sparse AutoEncoders, and we report quantitative and\nqualitative results."}
{"id": "2505.12973", "pdf": "https://arxiv.org/pdf/2505.12973", "abs": "https://arxiv.org/abs/2505.12973", "authors": ["Mahta Fetrat Qharabagh", "Zahra Dehghanian", "Hamid R. Rabiee"], "title": "Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models", "categories": ["cs.CL"], "comment": "8 main body pages, total 25 pages, 15 figures", "summary": "Homograph disambiguation remains a significant challenge in\ngrapheme-to-phoneme (G2P) conversion, especially for low-resource languages.\nThis challenge is twofold: (1) creating balanced and comprehensive homograph\ndatasets is labor-intensive and costly, and (2) specific disambiguation\nstrategies introduce additional latency, making them unsuitable for real-time\napplications such as screen readers and other accessibility tools. In this\npaper, we address both issues. First, we propose a semi-automated pipeline for\nconstructing homograph-focused datasets, introduce the HomoRich dataset\ngenerated through this pipeline, and demonstrate its effectiveness by applying\nit to enhance a state-of-the-art deep learning-based G2P system for Persian.\nSecond, we advocate for a paradigm shift - utilizing rich offline datasets to\ninform the development of fast, rule-based methods suitable for\nlatency-sensitive accessibility applications like screen readers. To this end,\nwe improve one of the most well-known rule-based G2P systems, eSpeak, into a\nfast homograph-aware version, HomoFast eSpeak. Our results show an approximate\n30% improvement in homograph disambiguation accuracy for the deep\nlearning-based and eSpeak systems."}
{"id": "2505.11625", "pdf": "https://arxiv.org/pdf/2505.11625", "abs": "https://arxiv.org/abs/2505.11625", "authors": ["Huiliang Zhang", "Ping Nie", "Lijun Sun", "Benoit Boulet"], "title": "Nearest Neighbor Multivariate Time Series Forecasting", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Multivariate time series (MTS) forecasting has a wide range of applications\nin both industry and academia. Recently, spatial-temporal graph neural networks\n(STGNNs) have gained popularity as MTS forecasting methods. However, current\nSTGNNs can only use the finite length of MTS input data due to the\ncomputational complexity. Moreover, they lack the ability to identify similar\npatterns throughout the entire dataset and struggle with data that exhibit\nsparsely and discontinuously distributed correlations among variables over an\nextensive historical period, resulting in only marginal improvements. In this\narticle, we introduce a simple yet effective k-nearest neighbor MTS forecasting\n( kNN-MTS) framework, which forecasts with a nearest neighbor retrieval\nmechanism over a large datastore of cached series, using representations from\nthe MTS model for similarity search. This approach requires no additional\ntraining and scales to give the MTS model direct access to the whole dataset at\ntest time, resulting in a highly expressive model that consistently improves\nperformance, and has the ability to extract sparse distributed but similar\npatterns spanning over multivariables from the entire dataset. Furthermore, a\nhybrid spatial-temporal encoder (HSTEncoder) is designed for kNN-MTS which can\ncapture both long-term temporal and short-term spatial-temporal dependencies\nand is shown to provide accurate representation for kNN-MTSfor better\nforecasting. Experimental results on several real-world datasets show a\nsignificant improvement in the forecasting performance of kNN-MTS. The\nquantitative analysis also illustrates the interpretability and efficiency of\nkNN-MTS, showing better application prospects and opening up a new path for\nefficiently using the large dataset in MTS models."}
{"id": "2505.12430", "pdf": "https://arxiv.org/pdf/2505.12430", "abs": "https://arxiv.org/abs/2505.12430", "authors": ["Rafael Florencio", "Julio Guerrero"], "title": "A Learning-Based Ansatz Satisfying Boundary Conditions in Variational Problems", "categories": ["cs.LG"], "comment": null, "summary": "Recently, innovative adaptations of the Ritz Method incorporating deep\nlearning have been developed, known as the Deep Ritz Method. This approach\nemploys a neural network as the test function for variational problems.\nHowever, the neural network does not inherently satisfy the boundary conditions\nof the variational problem. To resolve this issue, the Deep Ritz Method\nintroduces a penalty term into the functional of the variational problem, which\ncan lead to misleading results during the optimization process. In this work,\nan ansatz is proposed that inherently satisfies the boundary conditions of the\nvariational problem. The results demonstrate that the proposed ansatz not only\neliminates misleading outcomes but also reduces complexity while maintaining\naccuracy, showcasing its practical effectiveness in addressing variational\nproblems."}
{"id": "2505.12983", "pdf": "https://arxiv.org/pdf/2505.12983", "abs": "https://arxiv.org/abs/2505.12983", "authors": ["Jiaan Wang", "Fandong Meng", "Zengkui Sun", "Yunlong Liang", "Yuxuan Cao", "Jiarong Xu", "Haoxiang Shi", "Jie Zhou"], "title": "An Empirical Study of Many-to-Many Summarization with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 main conference", "summary": "Many-to-many summarization (M2MS) aims to process documents in any language\nand generate the corresponding summaries also in any language. Recently, large\nlanguage models (LLMs) have shown strong multi-lingual abilities, giving them\nthe potential to perform M2MS in real applications. This work presents a\nsystematic empirical study on LLMs' M2MS ability. Specifically, we first\nreorganize M2MS data based on eight previous domain-specific datasets. The\nreorganized data contains 47.8K samples spanning five domains and six\nlanguages, which could be used to train and evaluate LLMs. Then, we benchmark\n18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned\ntraditional models (e.g., mBART) are also conducted for comparisons. Our\nexperiments reveal that, zero-shot LLMs achieve competitive results with\nfine-tuned traditional models. After instruct-tuning, open-source LLMs can\nsignificantly improve their M2MS ability, and outperform zero-shot LLMs\n(including GPT-4) in terms of automatic evaluations. In addition, we\ndemonstrate that this task-specific improvement does not sacrifice the LLMs'\ngeneral task-solving abilities. However, as revealed by our human evaluation,\nLLMs still face the factuality issue, and the instruction tuning might\nintensify the issue. Thus, how to control factual errors becomes the key when\nbuilding LLM summarizers in real applications, and is worth noting in future\nresearch."}
{"id": "2505.11633", "pdf": "https://arxiv.org/pdf/2505.11633", "abs": "https://arxiv.org/abs/2505.11633", "authors": ["Vyacheslav Tykhonov", "Han Yang", "Philipp Mayr", "Jetze Touber", "Andrea Scharnhorst"], "title": "Chatting with Papers: A Hybrid Approach Using LLMs and Knowledge Graphs", "categories": ["cs.DL", "cs.AI"], "comment": "9 pages, 3 figures, Submitted to Joint Workshop of the 5th AI +\n  Informetrics (AII) and the 6th Extraction and Evaluation of Knowledge\n  Entities from Scientific Documents (EEKE)", "summary": "This demo paper reports on a new workflow \\textit{GhostWriter} that combines\nthe use of Large Language Models and Knowledge Graphs (semantic artifacts) to\nsupport navigation through collections. Situated in the research area of\nRetrieval Augmented Generation, this specific workflow details the creation of\nlocal and adaptable chatbots. Based on the tool-suite \\textit{EverythingData}\nat the backend, \\textit{GhostWriter} provides an interface that enables\nquerying and ``chatting'' with a collection. Applied iteratively, the workflow\nsupports the information needs of researchers when interacting with a\ncollection of papers, whether it be to gain an overview, to learn more about a\nspecific concept and its context, and helps the researcher ultimately to refine\ntheir research question in a controlled way. We demonstrate the workflow for a\ncollection of articles from the \\textit{method data analysis} journal published\nby GESIS -- Leibniz-Institute for the Social Sciences. We also point to further\napplication areas."}
{"id": "2505.12432", "pdf": "https://arxiv.org/pdf/2505.12432", "abs": "https://arxiv.org/abs/2505.12432", "authors": ["Zirun Guo", "Minjie Hong", "Tao Jin"], "title": "Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Reinforcement Learning (RL) has shown promise in improving the reasoning\nabilities of Large Language Models (LLMs). However, the specific challenges of\nadapting RL to multimodal data and formats remain relatively unexplored. In\nthis work, we present Observe-R1, a novel framework aimed at enhancing the\nreasoning capabilities of multimodal large language models (MLLMs). We draw\ninspirations from human learning progression--from simple to complex and easy\nto difficult, and propose a gradual learning paradigm for MLLMs. To this end,\nwe construct the NeuraLadder dataset, which is organized and sampled according\nto the difficulty and complexity of data samples for RL training. To tackle\nmultimodal tasks, we introduce a multimodal format constraint that encourages\ncareful observation of images, resulting in enhanced visual abilities and\nclearer and more structured responses. Additionally, we implement a bonus\nreward system that favors concise, correct answers within a length constraint,\nalongside a dynamic weighting mechanism that prioritizes uncertain and\nmedium-difficulty problems, ensuring that more informative samples have a\ngreater impact on training. Our experiments with the Qwen2.5-VL-3B and\nQwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that\nObserve-R1 outperforms a series of larger reasoning models on both reasoning\nand general benchmarks, achieving superior clarity and conciseness in reasoning\nchains. Ablation studies validate the effectiveness of our strategies,\nhighlighting the robustness and generalization of our approach. The dataset and\ncode will be released at https://github.com/zrguo/Observe-R1."}
{"id": "2505.12996", "pdf": "https://arxiv.org/pdf/2505.12996", "abs": "https://arxiv.org/abs/2505.12996", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 2 figures", "summary": "In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance."}
{"id": "2505.11642", "pdf": "https://arxiv.org/pdf/2505.11642", "abs": "https://arxiv.org/abs/2505.11642", "authors": ["Falong Fan", "Xi Li"], "title": "PeerGuard: Defending Multi-Agent Systems Against Backdoor Attacks Through Mutual Reasoning", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Multi-agent systems leverage advanced AI models as autonomous agents that\ninteract, cooperate, or compete to complete complex tasks across applications\nsuch as robotics and traffic management. Despite their growing importance,\nsafety in multi-agent systems remains largely underexplored, with most research\nfocusing on single AI models rather than interacting agents. This work\ninvestigates backdoor vulnerabilities in multi-agent systems and proposes a\ndefense mechanism based on agent interactions. By leveraging reasoning\nabilities, each agent evaluates responses from others to detect illogical\nreasoning processes, which indicate poisoned agents. Experiments on LLM-based\nmulti-agent systems, including ChatGPT series and Llama 3, demonstrate the\neffectiveness of the proposed method, achieving high accuracy in identifying\npoisoned agents while minimizing false positives on clean agents. We believe\nthis work provides insights into multi-agent system safety and contributes to\nthe development of robust, trustworthy AI interactions."}
{"id": "2505.12435", "pdf": "https://arxiv.org/pdf/2505.12435", "abs": "https://arxiv.org/abs/2505.12435", "authors": ["Wenqiao Zhu", "Ji Liu", "Lulu Wang", "Jun Wu", "Yulun Zhang"], "title": "SGDPO: Self-Guided Direct Preference Optimization for Language Model Alignment", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "18 pages, to appear in ACL'25", "summary": "Direct Preference Optimization (DPO) is broadly utilized for aligning Large\nLanguage Models (LLMs) with human values because of its flexibility. Despite\nits effectiveness, it has been observed that the capability of DPO to generate\nhuman-preferred response is limited and the results of DPO are far from\nresilient. To address these limitations, in this paper we propose a novel\nSelf-Guided Direct Preference Optimization algorithm, i.e., SGDPO, which\nincorporates a pilot term to steer the gradient flow during the optimization\nprocess, allowing for fine-grained control over the updates of chosen and\nrejected rewards. We provide a detailed theoretical analysis of our proposed\nmethod and elucidate its operational mechanism. Furthermore, we conduct\ncomprehensive experiments on various models and benchmarks. The extensive\nexperimental results demonstrate the consistency between the empirical results\nand our theoretical analysis and confirm the effectiveness of our proposed\napproach (up to 9.19% higher score)."}
{"id": "2505.13004", "pdf": "https://arxiv.org/pdf/2505.13004", "abs": "https://arxiv.org/abs/2505.13004", "authors": ["Yuhao Qing", "Boyu Zhu", "Mingzhe Du", "Zhijiang Guo", "Terry Yue Zhuo", "Qianru Zhang", "Jie M. Zhang", "Heming Cui", "Siu-Ming Yiu", "Dong Huang", "See-Kiong Ng", "Luu Anh Tuan"], "title": "EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code", "categories": ["cs.CL"], "comment": "Under Review", "summary": "Existing code generation benchmarks primarily evaluate functional\ncorrectness, with limited focus on code efficiency and often restricted to a\nsingle language like Python. To address this gap, we introduce EffiBench-X, the\nfirst multi-language benchmark designed to measure the efficiency of\nLLM-generated code. EffiBench-X supports Python, C++, Java, JavaScript, Ruby,\nand Golang. It comprises competitive programming tasks with human-expert\nsolutions as efficiency baselines. Evaluating state-of-the-art LLMs on\nEffiBench-X reveals that while models generate functionally correct code, they\nconsistently underperform human experts in efficiency. Even the most efficient\nLLM-generated solutions (Qwen3-32B) achieve only around \\textbf{62\\%} of human\nefficiency on average, with significant language-specific variations. LLMs show\nbetter efficiency in Python, Ruby, and JavaScript than in Java, C++, and\nGolang. For instance, DeepSeek-R1's Python code is significantly more efficient\nthan its Java code. These results highlight the critical need for research into\nLLM optimization techniques to improve code efficiency across diverse\nlanguages. The dataset and evaluation infrastructure are submitted and\navailable at https://github.com/EffiBench/EffiBench-X.git and\nhttps://huggingface.co/datasets/EffiBench/effibench-x."}
{"id": "2505.11659", "pdf": "https://arxiv.org/pdf/2505.11659", "abs": "https://arxiv.org/abs/2505.11659", "authors": ["Loubnan Abou-Hamdan", "Emil Marinov", "Peter Wiecha", "Philipp del Hougne", "Tianyu Wang", "Patrice Genevet"], "title": "Programmable metasurfaces for future photonic artificial intelligence", "categories": ["physics.optics", "cs.AI", "physics.app-ph"], "comment": "Nat. Rev. Phys. (2025)", "summary": "Photonic neural networks (PNNs), which share the inherent benefits of\nphotonic systems, such as high parallelism and low power consumption, could\nchallenge traditional digital neural networks in terms of energy efficiency,\nlatency, and throughput. However, producing scalable photonic artificial\nintelligence (AI) solutions remains challenging. To make photonic AI models\nviable, the scalability problem needs to be solved. Large optical AI models\nimplemented on PNNs are only commercially feasible if the advantages of optical\ncomputation outweigh the cost of their input-output overhead. In this\nPerspective, we discuss how field-programmable metasurface technology may\nbecome a key hardware ingredient in achieving scalable photonic AI accelerators\nand how it can compete with current digital electronic technologies.\nProgrammability or reconfigurability is a pivotal component for PNN hardware,\nenabling in situ training and accommodating non-stationary use cases that\nrequire fine-tuning or transfer learning. Co-integration with electronics, 3D\nstacking, and large-scale manufacturing of metasurfaces would significantly\nimprove PNN scalability and functionalities. Programmable metasurfaces could\naddress some of the current challenges that PNNs face and enable\nnext-generation photonic AI technology."}
{"id": "2505.12437", "pdf": "https://arxiv.org/pdf/2505.12437", "abs": "https://arxiv.org/abs/2505.12437", "authors": ["Michele Fontanesi", "Alessio Micheli", "Marco Podda", "Domenico Tortorella"], "title": "Addressing the Scarcity of Benchmarks for Graph XAI", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While Graph Neural Networks (GNNs) have become the de facto model for\nlearning from structured data, their decisional process remains opaque to the\nend user, undermining their deployment in safety-critical applications. In the\ncase of graph classification, Explainable Artificial Intelligence (XAI)\ntechniques address this major issue by identifying sub-graph motifs that\nexplain predictions. However, advancements in this field are hindered by a\nchronic scarcity of benchmark datasets with known ground-truth motifs to assess\nthe explanations' quality. Current graph XAI benchmarks are limited to\nsynthetic data or a handful of real-world tasks hand-curated by domain experts.\nIn this paper, we propose a general method to automate the construction of XAI\nbenchmarks for graph classification from real-world datasets. We provide both\n15 ready-made benchmarks, as well as the code to generate more than 2000\nadditional XAI benchmarks with our method. As a use case, we employ our\nbenchmarks to assess the effectiveness of some popular graph explainers."}
{"id": "2505.13006", "pdf": "https://arxiv.org/pdf/2505.13006", "abs": "https://arxiv.org/abs/2505.13006", "authors": ["Yuyang Li", "Philip J. M. Kerbusch", "Raimon H. R. Pruim", "Tobias Käfer"], "title": "Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain", "categories": ["cs.CL"], "comment": "Accepted by NAACL 2025 industry track", "summary": "Airports from the top 20 in terms of annual passengers are highly dynamic\nenvironments with thousands of flights daily, and they aim to increase the\ndegree of automation. To contribute to this, we implemented a Conversational AI\nsystem that enables staff in an airport to communicate with flight information\nsystems. This system not only answers standard airport queries but also\nresolves airport terminology, jargon, abbreviations, and dynamic questions\ninvolving reasoning. In this paper, we built three different\nRetrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL\nRAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that\ntraditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally\nproduced hallucinations, which is risky to airport safety. In contrast, SQL RAG\nand Graph RAG achieved 80.85% and 91.49% accuracy respectively, with\nsignificantly fewer hallucinations. Moreover, Graph RAG was especially\neffective for questions that involved reasoning. Based on our observations, we\nthus recommend SQL RAG and Graph RAG are better for airport environments, due\nto fewer hallucinations and the ability to handle dynamic questions."}
{"id": "2505.11665", "pdf": "https://arxiv.org/pdf/2505.11665", "abs": "https://arxiv.org/abs/2505.11665", "authors": ["Shubham Vatsal", "Harsh Dubey", "Aditi Singh"], "title": "Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance across\na wide range of Natural Language Processing (NLP) tasks. However, ensuring\ntheir effectiveness across multiple languages presents unique challenges.\nMultilingual prompt engineering has emerged as a key approach to enhance LLMs'\ncapabilities in diverse linguistic settings without requiring extensive\nparameter re-training or fine-tuning. With growing interest in multilingual\nprompt engineering over the past two to three years, researchers have explored\nvarious strategies to improve LLMs' performance across languages and NLP tasks.\nBy crafting structured natural language prompts, researchers have successfully\nextracted knowledge from LLMs across different languages, making these\ntechniques an accessible pathway for a broader audience, including those\nwithout deep expertise in machine learning, to harness the capabilities of\nLLMs. In this paper, we survey and categorize different multilingual prompting\ntechniques based on the NLP tasks they address across a diverse set of datasets\nthat collectively span around 250 languages. We further highlight the LLMs\nemployed, present a taxonomy of approaches and discuss potential\nstate-of-the-art (SoTA) methods for specific multilingual datasets.\nAdditionally, we derive a range of insights across language families and\nresource levels (high-resource vs. low-resource), including analyses such as\nthe distribution of NLP tasks by language resource type and the frequency of\nprompting methods across different language families. Our survey reviews 36\nresearch papers covering 39 prompting techniques applied to 30 multilingual NLP\ntasks, with the majority of these studies published in the last two years."}
{"id": "2505.12455", "pdf": "https://arxiv.org/pdf/2505.12455", "abs": "https://arxiv.org/abs/2505.12455", "authors": ["Xin Yu", "Yujia Wang", "Jinghui Chen", "Lingzhou Xue"], "title": "AltLoRA: Towards Better Gradient Approximation in Low-Rank Adaptation with Alternating Projections", "categories": ["cs.LG"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) has emerged as an effective technique for reducing\nmemory overhead in fine-tuning large language models. However, it often suffers\nfrom sub-optimal performance compared with full fine-tuning since the update is\nconstrained in the low-rank space. Recent variants such as LoRA-Pro attempt to\nmitigate this by adjusting the gradients of the low-rank matrices to\napproximate the full gradient. However, LoRA-Pro's solution is not unique, and\ndifferent solutions can lead to significantly varying performance in ablation\nstudies. Besides, to incorporate momentum or adaptive optimization design,\napproaches like LoRA-Pro must first compute the equivalent gradient, causing a\nhigher memory cost close to full fine-tuning. A key challenge remains in\nintegrating momentum properly into the low-rank space with lower memory cost.\nIn this work, we propose AltLoRA, an alternating projection method that avoids\nthe difficulties in gradient approximation brought by the joint update design,\nmeanwhile integrating momentum without higher memory complexity. Our\ntheoretical analysis provides convergence guarantees and further shows that\nAltLoRA enables stable feature learning and robustness to transformation\ninvariance. Extensive experiments across multiple tasks demonstrate that\nAltLoRA outperforms LoRA and its variants, narrowing the gap toward full\nfine-tuning while preserving superior memory efficiency."}
{"id": "2505.13010", "pdf": "https://arxiv.org/pdf/2505.13010", "abs": "https://arxiv.org/abs/2505.13010", "authors": ["Himel Ghosh", "Ahmed Mosharafa", "Georg Groh"], "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "7 pages, 5 figures, 2 tables", "summary": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection."}
{"id": "2505.11669", "pdf": "https://arxiv.org/pdf/2505.11669", "abs": "https://arxiv.org/abs/2505.11669", "authors": ["Yiming Zhang", "Sitong Liu", "Alex Cloninger"], "title": "OT Score: An OT based Confidence Score for Unsupervised Domain Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We address the computational and theoretical limitations of existing\ndistributional alignment methods for unsupervised domain adaptation (UDA),\nparticularly regarding the estimation of classification performance and\nconfidence without target labels. Current theoretical frameworks for these\nmethods often yield computationally intractable quantities and fail to\nadequately reflect the properties of the alignment algorithms employed. To\novercome these challenges, we introduce the Optimal Transport (OT) score, a\nconfidence metric derived from a novel theoretical analysis that exploits the\nflexibility of decision boundaries induced by Semi-Discrete Optimal Transport\nalignment. The proposed OT score is intuitively interpretable, theoretically\nrigorous, and computationally efficient. It provides principled uncertainty\nestimates for any given set of target pseudo-labels without requiring model\nretraining, and can flexibly adapt to varying degrees of available source\ninformation. Experimental results on standard UDA benchmarks demonstrate that\nclassification accuracy consistently improves by identifying and removing\nlow-confidence predictions, and that OT score significantly outperforms\nexisting confidence metrics across diverse adaptation scenarios."}
{"id": "2505.12457", "pdf": "https://arxiv.org/pdf/2505.12457", "abs": "https://arxiv.org/abs/2505.12457", "authors": ["Yang Zhao", "Kai Xiong", "Xiao Ding", "Li Du", "YangouOuyang", "Zhouhao Sun", "Jiannan Guan", "Wenbin Zhang", "Bin Liu", "Dong Hu", "Bing Qin", "Ting Liu"], "title": "UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Scaling RL for LLMs is computationally expensive, largely due to\nmulti-sampling for policy optimization and evaluation, making efficient data\nselection crucial. Inspired by the Zone of Proximal Development (ZPD) theory,\nwe hypothesize LLMs learn best from data within their potential comprehension\nzone. Addressing the limitation of conventional, computationally intensive\nmulti-sampling methods for data assessment, we introduce UFO-RL. This novel\nframework uses a computationally efficient single-pass uncertainty estimation\nto identify informative data instances, achieving up to 185x faster data\nevaluation. UFO-RL leverages this metric to select data within the estimated\nZPD for training. Experiments show that training with just 10% of data selected\nby UFO-RL yields performance comparable to or surpassing full-data training,\nreducing overall training time by up to 16x while enhancing stability and\ngeneralization. UFO-RL offers a practical and highly efficient strategy for\nscaling RL fine-tuning of LLMs by focusing learning on valuable data."}
{"id": "2505.13034", "pdf": "https://arxiv.org/pdf/2505.13034", "abs": "https://arxiv.org/abs/2505.13034", "authors": ["Márton Kardos", "Kenneth C. Enevoldsen", "Kristoffer Laigaard Nielbo"], "title": "topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation", "categories": ["cs.CL"], "comment": "9 pages, 9 figures", "summary": "Topic models are statistical tools that allow their users to gain qualitative\nand quantitative insights into the contents of textual corpora without the need\nfor close reading. They can be applied in a wide range of settings from\ndiscourse analysis, through pretraining data curation, to text filtering. Topic\nmodels are typically parameter-rich, complex models, and interpreting these\nparameters can be challenging for their users. It is typical practice for users\nto interpret topics based on the top 10 highest ranking terms on a given topic.\nThis list-of-words approach, however, gives users a limited and biased picture\nof the content of topics. Thoughtful user interface design and visualizations\ncan help users gain a more complete and accurate understanding of topic models'\noutput. While some visualization utilities do exist for topic models, these are\ntypically limited to a certain type of topic model. We introduce topicwizard, a\nframework for model-agnostic topic model interpretation, that provides\nintuitive and interactive tools that help users examine the complex semantic\nrelations between documents, words and topics learned by topic models."}
{"id": "2505.11687", "pdf": "https://arxiv.org/pdf/2505.11687", "abs": "https://arxiv.org/abs/2505.11687", "authors": ["Philipp Schaer", "Christin Katharina Kreutz", "Krisztian Balog", "Timo Breuer", "Andreas Konstantin Kruff"], "title": "Second SIGIR Workshop on Simulations for Information Access (Sim4IA 2025)", "categories": ["cs.IR", "cs.AI", "cs.HC"], "comment": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR '25), July 13--18,\n  2025, Padua, Italy", "summary": "Simulations in information access (IA) have recently gained interest, as\nshown by various tutorials and workshops around that topic. Simulations can be\nkey contributors to central IA research and evaluation questions, especially\naround interactive settings when real users are unavailable, or their\nparticipation is impossible due to ethical reasons. In addition, simulations in\nIA can help contribute to a better understanding of users, reduce complexity of\nevaluation experiments, and improve reproducibility. Building on recent\ndevelopments in methods and toolkits, the second iteration of our Sim4IA\nworkshop aims to again bring together researchers and practitioners to form an\ninteractive and engaging forum for discussions on the future perspectives of\nthe field. An additional aim is to plan an upcoming TREC/CLEF campaign."}
{"id": "2505.12460", "pdf": "https://arxiv.org/pdf/2505.12460", "abs": "https://arxiv.org/abs/2505.12460", "authors": ["Asher Labovich"], "title": "A Case for Library-Level k-Means Binning in Histogram Gradient-Boosted Trees", "categories": ["cs.LG"], "comment": null, "summary": "Modern gradient-boosted decision trees (GBDTs) accelerate split finding with\nhistogram-based binning, which reduces complexity from O(N) to O(B) given a\nfixed bin budget B. However, the predominant quantile binning strategy-designed\nto distribute data points evenly among bins-may overlook critical boundary\nvalues that could enhance predictive performance. In this work, we propose\nreplacing quantile binning with a k-means discretizer initialized with quantile\nbins. We test this swap on 33 OpenML tasks plus synthetics that control for\nmodality, skew, and bin budget. Across 18 regression datasets, k-means shows no\nstatistically significant losses at the 5% level and wins in four cases-most\nstrikingly a 55% MSE drop on one particularly skewed dataset-even though\nk-means' mean reciprocal rank (MRR) is slightly lower (0.65 vs 0.72). On the 15\nclassification datasets the two methods are statistically tied (MRR 0.70 vs\n0.68) with gaps $\\leq$0.2 pp. Synthetic experiments confirm consistently large\nMSE gains-typically >20% and rising to 90% as outlier magnitude increases or\nbin budget drops. We find that k-means keeps error on par with exact splitting\nwhen extra cuts add little value, yet still recovers key split points that\nquantile overlooks. As such, we advocate for a built-in bin_method=k-means\nflag, especially in regression tasks and in tight-budget settings such as the\n32-64-bin GPU regime-because it is a \"safe default\" with large upside, yet adds\nonly a one-off, cacheable overhead ($\\approx$ 2s to bin 10M rows on one core)."}
{"id": "2505.13036", "pdf": "https://arxiv.org/pdf/2505.13036", "abs": "https://arxiv.org/abs/2505.13036", "authors": ["Sai Koneru", "Maike Züfle", "Thai-Binh Nguyen", "Seymanur Akti", "Jan Niehues", "Alexander Waibel"], "title": "KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The scope of the International Workshop on Spoken Language Translation\n(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to\nencompass a wider array of tasks, including Speech Question Answering and\nSummarization. This shift is partly driven by the growing capabilities of\nmodern systems, particularly with the success of Large Language Models (LLMs).\nIn this paper, we present the Karlsruhe Institute of Technology's submissions\nfor the Offline ST and Instruction Following (IF) tracks, where we leverage\nLLMs to enhance performance across all tasks. For the Offline ST track, we\npropose a pipeline that employs multiple automatic speech recognition systems,\nwhose outputs are fused using an LLM with document-level context. This is\nfollowed by a two-step translation process, incorporating additional refinement\nstep to improve translation quality. For the IF track, we develop an end-to-end\nmodel that integrates a speech encoder with an LLM to perform a wide range of\ninstruction-following tasks. We complement it with a final document-level\nrefinement stage to further enhance output quality by using contextual\ninformation."}
{"id": "2505.11692", "pdf": "https://arxiv.org/pdf/2505.11692", "abs": "https://arxiv.org/abs/2505.11692", "authors": ["Sahil Rajesh Dhayalkar"], "title": "The Geometry of ReLU Networks through the ReLU Transition Graph", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "13 pages, 4 figures", "summary": "We develop a novel theoretical framework for analyzing ReLU neural networks\nthrough the lens of a combinatorial object we term the ReLU Transition Graph\n(RTG). In this graph, each node corresponds to a linear region induced by the\nnetwork's activation patterns, and edges connect regions that differ by a\nsingle neuron flip. Building on this structure, we derive a suite of new\ntheoretical results connecting RTG geometry to expressivity, generalization,\nand robustness. Our contributions include tight combinatorial bounds on RTG\nsize and diameter, a proof of RTG connectivity, and graph-theoretic\ninterpretations of VC-dimension. We also relate entropy and average degree of\nthe RTG to generalization error. Each theoretical result is rigorously\nvalidated via carefully controlled experiments across varied network depths,\nwidths, and data regimes. This work provides the first unified treatment of\nReLU network structure via graph theory and opens new avenues for compression,\nregularization, and complexity control rooted in RTG analysis."}
{"id": "2505.12462", "pdf": "https://arxiv.org/pdf/2505.12462", "abs": "https://arxiv.org/abs/2505.12462", "authors": ["Zachary Roch", "Chi Zhang", "George Atia", "Yue Wang"], "title": "A Finite-Sample Analysis of Distributionally Robust Average-Reward Reinforcement Learning", "categories": ["cs.LG", "stat.ML"], "comment": "Preprint, work in progress", "summary": "Robust reinforcement learning (RL) under the average-reward criterion is\ncrucial for long-term decision making under potential environment mismatches,\nyet its finite-sample complexity study remains largely unexplored. Existing\nworks offer algorithms with asymptotic guarantees, but the absence of\nfinite-sample analysis hinders its principled understanding and practical\ndeployment, especially in data-limited settings. We close this gap by proposing\nRobust Halpern Iteration (RHI), the first algorithm with provable finite-sample\ncomplexity guarantee. Under standard uncertainty sets -- including\ncontamination sets and $\\ell_p$-norm balls -- RHI attains an $\\epsilon$-optimal\npolicy with near-optimal sample complexity of $\\tilde{\\mathcal\nO}\\left(\\frac{SA\\mathcal H^{2}}{\\epsilon^{2}}\\right)$, where $S$ and $A$ denote\nthe numbers of states and actions, and $\\mathcal H$ is the robust optimal bias\nspan. This result gives the first polynomial sample complexity guarantee for\nrobust average-reward RL. Moreover, our RHI's independence from prior knowledge\ndistinguishes it from many previous average-reward RL studies. Our work thus\nconstitutes a significant advancement in enhancing the practical applicability\nof robust average-reward methods to complex, real-world problems."}
{"id": "2505.13053", "pdf": "https://arxiv.org/pdf/2505.13053", "abs": "https://arxiv.org/abs/2505.13053", "authors": ["Amelie S. Robrecht", "Christoph R. Kowalski", "Stefan Kopp"], "title": "SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation", "categories": ["cs.CL", "cs.AI"], "comment": "currently under review at Frontiers in Communication", "summary": "Adapting to the addressee is crucial for successful explanations, yet poses\nsignificant challenges for dialogsystems. We adopt the approach of treating\nexplanation generation as a non-stationary decision process, where the optimal\nstrategy varies according to changing beliefs about the explainee and the\ninteraction context. In this paper we address the questions of (1) how to track\nthe interaction context and the relevant listener features in a formally\ndefined computational partner model, and (2) how to utilize this model in the\ndynamically adjusted, rational decision process that determines the currently\nbest explanation strategy. We propose a Bayesian inference-based approach to\ncontinuously update the partner model based on user feedback, and a\nnon-stationary Markov Decision Process to adjust decision-making based on the\npartner model values. We evaluate an implementation of this framework with five\nsimulated interlocutors, demonstrating its effectiveness in adapting to\ndifferent partners with constant and even changing feedback behavior. The\nresults show high adaptivity with distinct explanation strategies emerging for\ndifferent partners, highlighting the potential of our approach to improve\nexplainable AI systems and dialogsystems in general."}
{"id": "2505.11694", "pdf": "https://arxiv.org/pdf/2505.11694", "abs": "https://arxiv.org/abs/2505.11694", "authors": ["Sahil Rajesh Dhayalkar"], "title": "Neural Networks as Universal Finite-State Machines: A Constructive Deterministic Finite Automaton Theory", "categories": ["cs.LG", "cs.AI", "cs.FL"], "comment": "15 pages, 1 figure", "summary": "We present a complete theoretical and empirical framework establishing\nfeedforward neural networks as universal finite-state machines (N-FSMs). Our\nresults prove that finite-depth ReLU and threshold networks can exactly\nsimulate deterministic finite automata (DFAs) by unrolling state transitions\ninto depth-wise neural layers, with formal characterizations of required depth,\nwidth, and state compression. We demonstrate that DFA transitions are linearly\nseparable, binary threshold activations allow exponential compression, and\nMyhill-Nerode equivalence classes can be embedded into continuous latent spaces\nwhile preserving separability. We also formalize the expressivity boundary:\nfixed-depth feedforward networks cannot recognize non-regular languages\nrequiring unbounded memory. Unlike prior heuristic or probing-based studies, we\nprovide constructive proofs and design explicit DFA-unrolled neural\narchitectures that empirically validate every claim. Our results bridge deep\nlearning, automata theory, and neural-symbolic computation, offering a rigorous\nblueprint for how discrete symbolic processes can be realized in continuous\nneural systems."}
{"id": "2505.12465", "pdf": "https://arxiv.org/pdf/2505.12465", "abs": "https://arxiv.org/abs/2505.12465", "authors": ["Junzhe Jiang", "Chang Yang", "Xinrun Wang", "Zhiming Li", "Xiao Huang", "Bo Li"], "title": "Resolving Latency and Inventory Risk in Market Making with Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "The latency of the exchanges in Market Making (MM) is inevitable due to\nhardware limitations, system processing times, delays in receiving data from\nexchanges, the time required for order transmission to reach the market, etc.\nExisting reinforcement learning (RL) methods for Market Making (MM) overlook\nthe impact of these latency, which can lead to unintended order cancellations\ndue to price discrepancies between decision and execution times and result in\nundesired inventory accumulation, exposing MM traders to increased market risk.\nTherefore, these methods cannot be applied in real MM scenarios. To address\nthese issues, we first build a realistic MM environment with random delays of\n30-100 milliseconds for order placement and market information reception, and\nimplement a batch matching mechanism that collects orders within every 500\nmilliseconds before matching them all at once, simulating the batch auction\nmechanisms adopted by some exchanges. Then, we propose Relaver, an RL-based\nmethod for MM to tackle the latency and inventory risk issues. The three main\ncontributions of Relaver are: i) we introduce an augmented state-action space\nthat incorporates order hold time alongside price and volume, enabling Relaver\nto optimize execution strategies under latency constraints and time-priority\nmatching mechanisms, ii) we leverage dynamic programming (DP) to guide the\nexploration of RL training for better policies, iii) we train a market trend\npredictor, which can guide the agent to intelligently adjust the inventory to\nreduce the risk. Extensive experiments and ablation studies on four real-world\ndatasets demonstrate that \\textsc{Relaver} significantly improves the\nperformance of state-of-the-art RL-based MM strategies across multiple metrics."}
{"id": "2505.13069", "pdf": "https://arxiv.org/pdf/2505.13069", "abs": "https://arxiv.org/abs/2505.13069", "authors": ["Ambre Marie", "Ilias Maoudj", "Guillaume Dardenne", "Gwenolé Quellec"], "title": "Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7; I.5.1"], "comment": "Submitted to the SpeechWellness Challenge at Interspeech 2025; 5\n  pages, 2 figures, 2 tables", "summary": "The 1st SpeechWellness Challenge conveys the need for speech-based suicide\nrisk assessment in adolescents. This study investigates a multimodal approach\nfor this challenge, integrating automatic transcription with WhisperX,\nlinguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM.\nAdditionally, handcrafted acoustic features -- including MFCCs, spectral\ncontrast, and pitch-related statistics -- were incorporated. We explored three\nfusion strategies: early concatenation, modality-specific processing, and\nweighted attention with mixup regularization. Results show that weighted\nattention provided the best generalization, achieving 69% accuracy on the\ndevelopment set, though a performance gap between development and test sets\nhighlights generalization challenges. Our findings, strictly tied to the\nMINI-KID framework, emphasize the importance of refining embedding\nrepresentations and fusion mechanisms to enhance classification reliability."}
{"id": "2505.11695", "pdf": "https://arxiv.org/pdf/2505.11695", "abs": "https://arxiv.org/abs/2505.11695", "authors": ["Shihao Zhang", "Haoyu Zhang", "Ian Colbert", "Rayan Saab"], "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training Quantization", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches."}
{"id": "2505.12477", "pdf": "https://arxiv.org/pdf/2505.12477", "abs": "https://arxiv.org/abs/2505.12477", "authors": ["Hugues Van Assel", "Mark Ibrahim", "Tommaso Biancalani", "Aviv Regev", "Randall Balestriero"], "title": "Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "33 pages, 9 figures", "summary": "Reconstruction and joint embedding have emerged as two leading paradigms in\nSelf Supervised Learning (SSL). Reconstruction methods focus on recovering the\noriginal sample from a different view in input space. On the other hand, joint\nembedding methods align the representations of different views in latent space.\nBoth approaches offer compelling advantages, yet practitioners lack clear\nguidelines for choosing between them. In this work, we unveil the core\nmechanisms that distinguish each paradigm. By leveraging closed form solutions\nfor both approaches, we precisely characterize how the view generation process,\ne.g. data augmentation, impacts the learned representations. We then\ndemonstrate that, unlike supervised learning, both SSL paradigms require a\nminimal alignment between augmentations and irrelevant features to achieve\nasymptotic optimality with increasing sample size. Our findings indicate that\nin scenarios where these irrelevant features have a large magnitude, joint\nembedding methods are preferable because they impose a strictly weaker\nalignment condition compared to reconstruction based methods. These results not\nonly clarify the trade offs between the two paradigms but also substantiate the\nempirical success of joint embedding approaches on real world challenging\ndatasets."}
{"id": "2505.13077", "pdf": "https://arxiv.org/pdf/2505.13077", "abs": "https://arxiv.org/abs/2505.13077", "authors": ["Xiang Fei", "Jinghui Lu", "Qi Sun", "Hao Feng", "Yanjie Wang", "Wei Shi", "An-Lan Wang", "Jingqun Tang", "Can Huang"], "title": "Advancing Sequential Numerical Prediction in Autoregressive Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL."}
{"id": "2505.11714", "pdf": "https://arxiv.org/pdf/2505.11714", "abs": "https://arxiv.org/abs/2505.11714", "authors": ["Arjun Prakash", "Naicheng He", "Denizalp Goktas", "Amy Greenwald"], "title": "Bi-Level Policy Optimization with Nyström Hypergradients", "categories": ["cs.LG", "cs.AI", "cs.GT"], "comment": null, "summary": "The dependency of the actor on the critic in actor-critic (AC) reinforcement\nlearning means that AC can be characterized as a bilevel optimization (BLO)\nproblem, also called a Stackelberg game. This characterization motivates two\nmodifications to vanilla AC algorithms. First, the critic's update should be\nnested to learn a best response to the actor's policy. Second, the actor should\nupdate according to a hypergradient that takes changes in the critic's behavior\ninto account. Computing this hypergradient involves finding an inverse Hessian\nvector product, a process that can be numerically unstable. We thus propose a\nnew algorithm, Bilevel Policy Optimization with Nystr\\\"om Hypergradients\n(BLPO), which uses nesting to account for the nested structure of BLO, and\nleverages the Nystr\\\"om method to compute the hypergradient. Theoretically, we\nprove BLPO converges to (a point that satisfies the necessary conditions for) a\nlocal strong Stackelberg equilibrium in polynomial time with high probability,\nassuming a linear parametrization of the critic's objective. Empirically, we\ndemonstrate that BLPO performs on par with or better than PPO on a variety of\ndiscrete and continuous control tasks."}
{"id": "2505.12479", "pdf": "https://arxiv.org/pdf/2505.12479", "abs": "https://arxiv.org/abs/2505.12479", "authors": ["Rongwei Lu", "Yutong Jiang", "Jinrui Zhang", "Chunyang Li", "Yifei Zhu", "Bin Chen", "Zhi Wang"], "title": "$γ$-FedHT: Stepsize-Aware Hard-Threshold Gradient Compression in Federated Learning", "categories": ["cs.LG"], "comment": "This article has been accepted for publication in IEEE INFOCOM 2025", "summary": "Gradient compression can effectively alleviate communication bottlenecks in\nFederated Learning (FL). Contemporary state-of-the-art sparse compressors, such\nas Top-$k$, exhibit high computational complexity, up to\n$\\mathcal{O}(d\\log_2{k})$, where $d$ is the number of model parameters. The\nhard-threshold compressor, which simply transmits elements with absolute values\nhigher than a fixed threshold, is thus proposed to reduce the complexity to\n$\\mathcal{O}(d)$. However, the hard-threshold compression causes accuracy\ndegradation in FL, where the datasets are non-IID and the stepsize $\\gamma$ is\ndecreasing for model convergence. The decaying stepsize reduces the updates and\ncauses the compression ratio of the hard-threshold compression to drop rapidly\nto an aggressive ratio. At or below this ratio, the model accuracy has been\nobserved to degrade severely. To address this, we propose $\\gamma$-FedHT, a\nstepsize-aware low-cost compressor with Error-Feedback to guarantee\nconvergence. Given that the traditional theoretical framework of FL does not\nconsider Error-Feedback, we introduce the fundamental conversation of\nError-Feedback. We prove that $\\gamma$-FedHT has the convergence rate of\n$\\mathcal{O}(\\frac{1}{T})$ ($T$ representing total training iterations) under\n$\\mu$-strongly convex cases and $\\mathcal{O}(\\frac{1}{\\sqrt{T}})$ under\nnon-convex cases, \\textit{same as FedAVG}. Extensive experiments demonstrate\nthat $\\gamma$-FedHT improves accuracy by up to $7.42\\%$ over Top-$k$ under\nequal communication traffic on various non-IID image datasets."}
{"id": "2505.13089", "pdf": "https://arxiv.org/pdf/2505.13089", "abs": "https://arxiv.org/abs/2505.13089", "authors": ["Sondre Wold", "Lucas Georges Gabriel Charpentier", "Étienne Simon"], "title": "Systematic Generalization in Language Models Scales with Information Entropy", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025: Findings", "summary": "Systematic generalization remains challenging for current language models,\nwhich are known to be both sensitive to semantically similar permutations of\nthe input and to struggle with known concepts presented in novel contexts.\nAlthough benchmarks exist for assessing compositional behavior, it is unclear\nhow to measure the difficulty of a systematic generalization problem. In this\nwork, we show how one aspect of systematic generalization can be described by\nthe entropy of the distribution of component parts in the training data. We\nformalize a framework for measuring entropy in a sequence-to-sequence task and\nfind that the performance of popular model architectures scales with the\nentropy. Our work connects systematic generalization to information efficiency,\nand our results indicate that success at high entropy can be achieved even\nwithout built-in priors, and that success at low entropy can serve as a target\nfor assessing progress towards robust systematic generalization."}
{"id": "2505.11717", "pdf": "https://arxiv.org/pdf/2505.11717", "abs": "https://arxiv.org/abs/2505.11717", "authors": ["Xilong Wang", "John Bloch", "Zedian Shao", "Yuepeng Hu", "Shuyan Zhou", "Neil Zhenqiang Gong"], "title": "EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Multi-modal large language model (MLLM)-based web agents interact with\nwebpage environments by generating actions based on screenshots of the\nwebpages. Environmental prompt injection attacks manipulate the environment to\ninduce the web agent to perform a specific, attacker-chosen action--referred to\nas the target action. However, existing attacks suffer from limited\neffectiveness or stealthiness, or are impractical in real-world settings. In\nthis work, we propose EnvInjection, a new attack that addresses these\nlimitations. Our attack adds a perturbation to the raw pixel values of the\nrendered webpage, which can be implemented by modifying the webpage's source\ncode. After these perturbed pixels are mapped into a screenshot, the\nperturbation induces the web agent to perform the target action. We formulate\nthe task of finding the perturbation as an optimization problem. A key\nchallenge in solving this problem is that the mapping between raw pixel values\nand screenshot is non-differentiable, making it difficult to backpropagate\ngradients to the perturbation. To overcome this, we train a neural network to\napproximate the mapping and apply projected gradient descent to solve the\nreformulated optimization problem. Extensive evaluation on multiple webpage\ndatasets shows that EnvInjection is highly effective and significantly\noutperforms existing baselines."}
{"id": "2505.12504", "pdf": "https://arxiv.org/pdf/2505.12504", "abs": "https://arxiv.org/abs/2505.12504", "authors": ["Zongkai Liu", "Fanqing Meng", "Lingxiao Du", "Zhixiang Zhou", "Chao Yu", "Wenqi Shao", "Qiaosheng Zhang"], "title": "CPGD: Toward Stable Rule-based Reinforcement Learning for Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advances in rule-based reinforcement learning (RL) have significantly\nimproved the reasoning capability of language models (LMs) with rule-based\nrewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO --\noften suffer from training instability, where large policy updates and improper\nclipping can lead to training collapse. To address this issue, we propose\nClipped Policy Gradient Optimization with Policy Drift (CPGD), a novel\nalgorithm designed to stabilize policy learning in LMs. CPGD introduces a\npolicy drift constraint based on KL divergence to dynamically regularize policy\nupdates, and leverages a clip mechanism on the logarithm of the ratio to\nprevent excessive policy updates. We provide theoretical justification for CPGD\nand demonstrate through empirical analysis that it mitigates the instability\nobserved in prior approaches. Furthermore, we show that CPGD significantly\nimproves performance while maintaining training stability. Our implementation\nbalances theoretical rigor with practical usability, offering a robust\nalternative for RL in the post-training of LMs. We release our code at\nhttps://github.com/ModalMinds/MM-EUREKA."}
{"id": "2505.13090", "pdf": "https://arxiv.org/pdf/2505.13090", "abs": "https://arxiv.org/abs/2505.13090", "authors": ["David Stap", "Christof Monz"], "title": "The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation", "categories": ["cs.CL"], "comment": null, "summary": "Prior research diverges on language diversity in LLM fine-tuning: Some\nstudies report benefits while others find no advantages. Through controlled\nfine-tuning experiments across 132 translation directions, we systematically\nresolve these disparities. We find that expanding language diversity during\nfine-tuning improves translation quality for both unsupervised and --\nsurprisingly -- supervised pairs, despite less diverse models being fine-tuned\nexclusively on these supervised pairs. However, benefits plateau or decrease\nbeyond a certain diversity threshold. We show that increased language diversity\ncreates more language-agnostic representations. These representational\nadaptations help explain the improved performance in models fine-tuned with\ngreater diversity."}
{"id": "2505.11719", "pdf": "https://arxiv.org/pdf/2505.11719", "abs": "https://arxiv.org/abs/2505.11719", "authors": ["Sumeet Batra", "Gaurav Sukhatme"], "title": "Zero-Shot Visual Generalization in Robot Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Training vision-based manipulation policies that are robust across diverse\nvisual environments remains an important and unresolved challenge in robot\nlearning. Current approaches often sidestep the problem by relying on invariant\nrepresentations such as point clouds and depth, or by brute-forcing\ngeneralization through visual domain randomization and/or large, visually\ndiverse datasets. Disentangled representation learning - especially when\ncombined with principles of associative memory - has recently shown promise in\nenabling vision-based reinforcement learning policies to be robust to visual\ndistribution shifts. However, these techniques have largely been constrained to\nsimpler benchmarks and toy environments. In this work, we scale disentangled\nrepresentation learning and associative memory to more visually and dynamically\ncomplex manipulation tasks and demonstrate zero-shot adaptability to visual\nperturbations in both simulation and on real hardware. We further extend this\napproach to imitation learning, specifically Diffusion Policy, and empirically\nshow significant gains in visual generalization compared to state-of-the-art\nimitation learning methods. Finally, we introduce a novel technique adapted\nfrom the model equivariance literature that transforms any trained neural\nnetwork policy into one invariant to 2D planar rotations, making our policy not\nonly visually robust but also resilient to certain camera perturbations. We\nbelieve that this work marks a significant step towards manipulation policies\nthat are not only adaptable out of the box, but also robust to the complexities\nand dynamical nature of real-world deployment. Supplementary videos are\navailable at https://sites.google.com/view/vis-gen-robotics/home."}
{"id": "2505.12506", "pdf": "https://arxiv.org/pdf/2505.12506", "abs": "https://arxiv.org/abs/2505.12506", "authors": ["Yotam Norman", "Ron Meir"], "title": "Unsupervised Invariant Risk Minimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose a novel unsupervised framework for \\emph{Invariant Risk\nMinimization} (IRM), extending the concept of invariance to settings where\nlabels are unavailable. Traditional IRM methods rely on labeled data to learn\nrepresentations that are robust to distributional shifts across environments.\nIn contrast, our approach redefines invariance through feature distribution\nalignment, enabling robust representation learning from unlabeled data. We\nintroduce two methods within this framework: Principal Invariant Component\nAnalysis (PICA), a linear method that extracts invariant directions under\nGaussian assumptions, and Variational Invariant Autoencoder (VIAE), a deep\ngenerative model that disentangles environment-invariant and\nenvironment-dependent latent factors. Our approach is based on a novel\n``unsupervised'' structural causal model and supports environment-conditioned\nsample-generation and intervention. Empirical evaluations on synthetic dataset\nand modified versions of MNIST demonstrate the effectiveness of our methods in\ncapturing invariant structure, preserving relevant information, and\ngeneralizing across environments without access to labels."}
{"id": "2505.13115", "pdf": "https://arxiv.org/pdf/2505.13115", "abs": "https://arxiv.org/abs/2505.13115", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands", "summary": "The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications."}
{"id": "2505.11725", "pdf": "https://arxiv.org/pdf/2505.11725", "abs": "https://arxiv.org/abs/2505.11725", "authors": ["Imon Banerjee", "Sayak Chakrabarty"], "title": "CLT and Edgeworth Expansion for m-out-of-n Bootstrap Estimators of The Studentized Median", "categories": ["cs.LG", "cs.AI", "cs.CE", "math.ST", "stat.ME", "stat.ML", "stat.TH"], "comment": "48 pages", "summary": "The m-out-of-n bootstrap, originally proposed by Bickel, Gotze, and Zwet\n(1992), approximates the distribution of a statistic by repeatedly drawing m\nsubsamples (with m much smaller than n) without replacement from an original\nsample of size n. It is now routinely used for robust inference with\nheavy-tailed data, bandwidth selection, and other large-sample applications.\nDespite its broad applicability across econometrics, biostatistics, and machine\nlearning, rigorous parameter-free guarantees for the soundness of the\nm-out-of-n bootstrap when estimating sample quantiles have remained elusive.\n  This paper establishes such guarantees by analyzing the estimator of sample\nquantiles obtained from m-out-of-n resampling of a dataset of size n. We first\nprove a central limit theorem for a fully data-driven version of the estimator\nthat holds under a mild moment condition and involves no unknown nuisance\nparameters. We then show that the moment assumption is essentially tight by\nconstructing a counter-example in which the CLT fails. Strengthening the\nassumptions slightly, we derive an Edgeworth expansion that provides exact\nconvergence rates and, as a corollary, a Berry Esseen bound on the bootstrap\napproximation error. Finally, we illustrate the scope of our results by\nderiving parameter-free asymptotic distributions for practical statistics,\nincluding the quantiles for random walk Metropolis-Hastings and the rewards of\nergodic Markov decision processes, thereby demonstrating the usefulness of our\ntheory in modern estimation and learning tasks."}
{"id": "2505.12508", "pdf": "https://arxiv.org/pdf/2505.12508", "abs": "https://arxiv.org/abs/2505.12508", "authors": ["Rubens O. Moraes", "Quazi Asif Sadmine", "Hendrik Baier", "Levi H. S. Lelis"], "title": "InnateCoder: Learning Programmatic Options with Foundation Models", "categories": ["cs.LG"], "comment": "Accepted at IJCAI 2025", "summary": "Outside of transfer learning settings, reinforcement learning agents start\ntheir learning process from a clean slate. As a result, such agents have to go\nthrough a slow process to learn even the most obvious skills required to solve\na problem. In this paper, we present InnateCoder, a system that leverages human\nknowledge encoded in foundation models to provide programmatic policies that\nencode \"innate skills\" in the form of temporally extended actions, or options.\nIn contrast to existing approaches to learning options, InnateCoder learns them\nfrom the general human knowledge encoded in foundation models in a zero-shot\nsetting, and not from the knowledge the agent gains by interacting with the\nenvironment. Then, InnateCoder searches for a programmatic policy by combining\nthe programs encoding these options into larger and more complex programs. We\nhypothesized that InnateCoder's way of learning and using options could improve\nthe sampling efficiency of current methods for learning programmatic policies.\nEmpirical results in MicroRTS and Karel the Robot support our hypothesis, since\nthey show that InnateCoder is more sample efficient than versions of the system\nthat do not use options or learn them from experience."}
{"id": "2505.13136", "pdf": "https://arxiv.org/pdf/2505.13136", "abs": "https://arxiv.org/abs/2505.13136", "authors": ["Anton Ehrmanntraut", "Julia Wunderle", "Jan Pfister", "Fotis Jannidis", "Andreas Hotho"], "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review @ARR", "summary": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models."}
{"id": "2505.11731", "pdf": "https://arxiv.org/pdf/2505.11731", "abs": "https://arxiv.org/abs/2505.11731", "authors": ["Harshil Vejendla", "Haizhou Shi", "Yibin Wang", "Tunyu Zhang", "Huan Zhang", "Hao Wang"], "title": "Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; work in progress", "summary": "Recent advances in uncertainty estimation for Large Language Models (LLMs)\nduring downstream adaptation have addressed key challenges of reliability and\nsimplicity. However, existing Bayesian methods typically require multiple\nsampling iterations during inference, creating significant efficiency issues\nthat limit practical deployment. In this paper, we investigate the possibility\nof eliminating the need for test-time sampling for LLM uncertainty estimation.\nSpecifically, when given an off-the-shelf Bayesian LLM, we distill its aligned\nconfidence into a non-Bayesian student LLM by minimizing the divergence between\ntheir predictive distributions. Unlike typical calibration methods, our\ndistillation is carried out solely on the training dataset without the need of\nan additional validation dataset. This simple yet effective approach achieves\nN-times more efficient uncertainty estimation during testing, where N is the\nnumber of samples traditionally required by Bayesian LLMs. Our extensive\nexperiments demonstrate that uncertainty estimation capabilities on training\ndata can successfully generalize to unseen test data through our distillation\ntechnique, consistently producing results comparable to (or even better than)\nstate-of-the-art Bayesian LLMs."}
{"id": "2505.12509", "pdf": "https://arxiv.org/pdf/2505.12509", "abs": "https://arxiv.org/abs/2505.12509", "authors": ["Junhao Liu", "Haonan Yu", "Xin Zhang"], "title": "Towards Budget-Friendly Model-Agnostic Explanation Generation for Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With Large language models (LLMs) becoming increasingly prevalent in various\napplications, the need for interpreting their predictions has become a critical\nchallenge. As LLMs vary in architecture and some are closed-sourced,\nmodel-agnostic techniques show great promise without requiring access to the\nmodel's internal parameters. However, existing model-agnostic techniques need\nto invoke LLMs many times to gain sufficient samples for generating faithful\nexplanations, which leads to high economic costs. In this paper, we show that\nit is practical to generate faithful explanations for large-scale LLMs by\nsampling from some budget-friendly models through a series of empirical\nstudies. Moreover, we show that such proxy explanations also perform well on\ndownstream tasks. Our analysis provides a new paradigm of model-agnostic\nexplanation methods for LLMs, by including information from budget-friendly\nmodels."}
{"id": "2505.13141", "pdf": "https://arxiv.org/pdf/2505.13141", "abs": "https://arxiv.org/abs/2505.13141", "authors": ["Zheng Wei Lim", "Alham Fikri Aji", "Trevor Cohn"], "title": "Understanding Cross-Lingual Inconsistency in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are demonstrably capable of cross-lingual\ntransfer, but can produce inconsistent output when prompted with the same\nqueries written in different languages. To understand how language models are\nable to generalize knowledge from one language to the others, we apply the\nlogit lens to interpret the implicit steps taken by LLMs to solve multilingual\nmulti-choice reasoning questions. We find LLMs predict inconsistently and are\nless accurate because they rely on subspaces of individual languages, rather\nthan working in a shared semantic space. While larger models are more\nmultilingual, we show their hidden states are more likely to dissociate from\nthe shared representation compared to smaller models, but are nevertheless more\ncapable of retrieving knowledge embedded across different languages. Finally,\nwe demonstrate that knowledge sharing can be modulated by steering the models'\nlatent processing towards the shared semantic space. We find reinforcing\nutilization of the shared space improves the models' multilingual reasoning\nperformance, as a result of more knowledge transfer from, and better output\nconsistency with English."}
{"id": "2505.11737", "pdf": "https://arxiv.org/pdf/2505.11737", "abs": "https://arxiv.org/abs/2505.11737", "authors": ["Tunyu Zhang", "Haizhou Shi", "Yibin Wang", "Hengyi Wang", "Xiaoxiao He", "Zhuowei Li", "Haoxian Chen", "Ligong Han", "Kai Xu", "Huan Zhang", "Dimitris Metaxas", "Hao Wang"], "title": "Token-Level Uncertainty Estimation for Large Language Model Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; Work in progress", "summary": "While Large Language Models (LLMs) have demonstrated impressive capabilities,\ntheir output quality remains inconsistent across various application scenarios,\nmaking it difficult to identify trustworthy responses, especially in complex\ntasks requiring multi-step reasoning. In this paper, we propose a token-level\nuncertainty estimation framework to enable LLMs to self-assess and self-improve\ntheir generation quality in mathematical reasoning. Specifically, we introduce\nlow-rank random weight perturbation to LLM decoding, generating predictive\ndistributions that we use to estimate token-level uncertainties. We then\naggregate these uncertainties to reflect semantic uncertainty of the generated\nsequences. Experiments on mathematical reasoning datasets of varying difficulty\ndemonstrate that our token-level uncertainty metrics strongly correlate with\nanswer correctness and model robustness. Additionally, we explore using\nuncertainty to directly enhance the model's reasoning performance through\nmultiple generations and the particle filtering algorithm. Our approach\nconsistently outperforms existing uncertainty estimation methods, establishing\neffective uncertainty estimation as a valuable tool for both evaluating and\nimproving reasoning generation in LLMs."}
{"id": "2505.12512", "pdf": "https://arxiv.org/pdf/2505.12512", "abs": "https://arxiv.org/abs/2505.12512", "authors": ["Truman Hickok"], "title": "Scalable Strategies for Continual Learning with Replay", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Future deep learning models will be distinguished by systems that perpetually\nlearn through interaction, imagination, and cooperation, blurring the line\nbetween training and inference. This makes continual learning a critical\nchallenge, as methods that efficiently maximize bidirectional transfer across\nlearning trajectories will be essential. Replay is on track to play a\nfoundational role in continual learning, allowing models to directly reconcile\nnew information with past knowledge. In practice, however, replay is quite\nunscalable, doubling the cost of continual learning when applied naively.\nMoreover, the continual learning literature has not fully synchronized with the\nmulti-task fine-tuning literature, having not fully integrated highly scalable\ntechniques like model merging and low rank adaptation into a replay-enabled\ntoolset that can produce a unified model in the face of many sequential tasks.\nIn this paper, we begin by applying and analyzing low rank adaptation in a\ncontinual learning setting. Next, we introduce consolidation, a phasic approach\nto replay which leads to up to 55\\% less replay samples being needed for a\ngiven performance target. Then, we propose sequential merging, an offshoot of\ntask arithmetic which is tailored to the continual learning setting and is\nshown to work well in combination with replay. Finally, we demonstrate that the\ndeveloped strategies can operate synergistically, resulting in a highly\nscalable toolset that outperforms standalone variants."}
{"id": "2505.13147", "pdf": "https://arxiv.org/pdf/2505.13147", "abs": "https://arxiv.org/abs/2505.13147", "authors": ["Aswathy Velutharambath", "Roman Klinger", "Kai Sassenberg"], "title": "What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text", "categories": ["cs.CL"], "comment": null, "summary": "Can deception be detected solely from written text? Cues of deceptive\ncommunication are inherently subtle, even more so in text-only communication.\nYet, prior studies have reported considerable success in automatic deception\ndetection. We hypothesize that such findings are largely driven by artifacts\nintroduced during data collection and do not generalize beyond specific\ndatasets. We revisit this assumption by introducing a belief-based deception\nframework, which defines deception as a misalignment between an author's claims\nand true beliefs, irrespective of factual accuracy, allowing deception cues to\nbe studied in isolation. Based on this framework, we construct three corpora,\ncollectively referred to as DeFaBel, including a German-language corpus of\ndeceptive and non-deceptive arguments and a multilingual version in German and\nEnglish, each collected under varying conditions to account for belief change\nand enable cross-linguistic analysis. Using these corpora, we evaluate commonly\nreported linguistic cues of deception. Across all three DeFaBel variants, these\ncues show negligible, statistically insignificant correlations with deception\nlabels, contrary to prior work that treats such cues as reliable indicators. We\nfurther benchmark against other English deception datasets following similar\ndata collection protocols. While some show statistically significant\ncorrelations, effect sizes remain low and, critically, the set of predictive\ncues is inconsistent across datasets. We also evaluate deception detection\nusing feature-based models, pretrained language models, and instruction-tuned\nlarge language models. While some models perform well on established deception\ndatasets, they consistently perform near chance on DeFaBel. Our findings\nchallenge the assumption that deception can be reliably inferred from\nlinguistic cues and call for rethinking how deception is studied and modeled in\nNLP."}
{"id": "2505.11739", "pdf": "https://arxiv.org/pdf/2505.11739", "abs": "https://arxiv.org/abs/2505.11739", "authors": ["Feijiang Han", "Xiaodong Yu", "Jianheng Tang", "Lyle Ungar"], "title": "ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, training-free methods for improving large language models (LLMs)\nhave attracted growing interest, with token-level attention tuning emerging as\na promising and interpretable direction. However, existing methods typically\nrely on auxiliary mechanisms to identify important or irrelevant task-specific\ntokens, introducing potential bias and limiting applicability. In this paper,\nwe uncover a surprising and elegant alternative: the semantically empty initial\ntoken is a powerful and underexplored control point for optimizing model\nbehavior. Through theoretical analysis, we show that tuning the initial token's\nattention sharpens or flattens the attention distribution over subsequent\ntokens, and its role as an attention sink amplifies this effect. Empirically,\nwe find that: (1) tuning its attention improves LLM performance more\neffectively than tuning other task-specific tokens; (2) the effect follows a\nconsistent trend across layers, with earlier layers having greater impact, but\nvaries across attention heads, with different heads showing distinct\npreferences in how they attend to this token. Based on these findings, we\npropose ZeroTuning, a training-free approach that improves LLM performance by\napplying head-specific attention adjustments to this special token. Despite\ntuning only one token, ZeroTuning achieves higher performance on text\nclassification, multiple-choice, and multi-turn conversation tasks across\nmodels such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves\nLlama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its\nmulti-turn score from 7.804 to 7.966. The method is also robust to limited\nresources, few-shot settings, long contexts, quantization, decoding strategies,\nand prompt variations. Our work sheds light on a previously overlooked control\npoint in LLMs, offering new insights into both inference-time tuning and model\ninterpretability."}
{"id": "2505.12514", "pdf": "https://arxiv.org/pdf/2505.12514", "abs": "https://arxiv.org/abs/2505.12514", "authors": ["Hanlin Zhu", "Shibo Hao", "Zhiting Hu", "Jiantao Jiao", "Stuart Russell", "Yuandong Tian"], "title": "Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought", "categories": ["cs.LG"], "comment": "26 pages, 7 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in many\napplications, including challenging reasoning problems via chain-of-thoughts\n(CoTs) techniques that generate ``thinking tokens'' before answering the\nquestions. While existing theoretical works demonstrate that CoTs with discrete\ntokens boost the capability of LLMs, recent work on continuous CoTs lacks a\ntheoretical understanding of why it outperforms discrete counterparts in\nvarious reasoning tasks such as directed graph reachability, a fundamental\ngraph reasoning problem that includes many practical domain applications as\nspecial cases. In this paper, we prove that a two-layer transformer with $D$\nsteps of continuous CoTs can solve the directed graph reachability problem,\nwhere $D$ is the diameter of the graph, while the best known result of\nconstant-depth transformers with discrete CoTs requires $O(n^2)$ decoding steps\nwhere $n$ is the number of vertices ($D<n$). In our construction, each\ncontinuous thought vector is a superposition state that encodes multiple search\nfrontiers simultaneously (i.e., parallel breadth-first search (BFS)), while\ndiscrete CoTs must choose a single path sampled from the superposition state,\nwhich leads to sequential search that requires many more steps and may be\ntrapped into local solutions. We also performed extensive experiments to verify\nthat our theoretical construction aligns well with the empirical solution\nobtained via training dynamics. Notably, encoding of multiple search frontiers\nas a superposition state automatically emerges in training continuous CoTs,\nwithout explicit supervision to guide the model to explore multiple paths\nsimultaneously."}
{"id": "2505.13156", "pdf": "https://arxiv.org/pdf/2505.13156", "abs": "https://arxiv.org/abs/2505.13156", "authors": ["Zhi Liu", "Tao Yang", "Jing Wang", "Yexin Chen", "Zhan Gao", "Jiaxi Yang", "Kui Chen", "Bingji Lu", "Xiaochen Li", "Changyong Luo", "Yan Li", "Xiaohong Gu", "Peng Cao"], "title": "Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice", "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 4 figures, and 1 tables", "summary": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application."}
{"id": "2505.11740", "pdf": "https://arxiv.org/pdf/2505.11740", "abs": "https://arxiv.org/abs/2505.11740", "authors": ["Alberto Sinigaglia", "Davide Sartor", "Marina Ceccon", "Gian Antonio Susto"], "title": "Simple and Effective Specialized Representations for Fair Classifiers", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Fair classification is a critical challenge that has gained increasing\nimportance due to international regulations and its growing use in high-stakes\ndecision-making settings. Existing methods often rely on adversarial learning\nor distribution matching across sensitive groups; however, adversarial learning\ncan be unstable, and distribution matching can be computationally intensive. To\naddress these limitations, we propose a novel approach based on the\ncharacteristic function distance. Our method ensures that the learned\nrepresentation contains minimal sensitive information while maintaining high\neffectiveness for downstream tasks. By utilizing characteristic functions, we\nachieve a more stable and efficient solution compared to traditional methods.\nAdditionally, we introduce a simple relaxation of the objective function that\nguarantees fairness in common classification models with no performance\ndegradation. Experimental results on benchmark datasets demonstrate that our\napproach consistently matches or achieves better fairness and predictive\naccuracy than existing methods. Moreover, our method maintains robustness and\ncomputational efficiency, making it a practical solution for real-world\napplications."}
{"id": "2505.12523", "pdf": "https://arxiv.org/pdf/2505.12523", "abs": "https://arxiv.org/abs/2505.12523", "authors": ["Josh Millar", "Hamed Haddadi", "Anil Madhavapeddy"], "title": "Energy-Aware Deep Learning on Resource-Constrained Hardware", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "The use of deep learning (DL) on Internet of Things (IoT) and mobile devices\noffers numerous advantages over cloud-based processing. However, such devices\nface substantial energy constraints to prolong battery-life, or may even\noperate intermittently via energy-harvesting. Consequently,\n\\textit{energy-aware} approaches for optimizing DL inference and training on\nsuch resource-constrained devices have garnered recent interest. We present an\noverview of such approaches, outlining their methodologies, implications for\nenergy consumption and system-level efficiency, and their limitations in terms\nof supported network types, hardware platforms, and application scenarios. We\nhope our review offers a clear synthesis of the evolving energy-aware DL\nlandscape and serves as a foundation for future research in energy-constrained\ncomputing."}
{"id": "2505.13157", "pdf": "https://arxiv.org/pdf/2505.13157", "abs": "https://arxiv.org/abs/2505.13157", "authors": ["Yassine El Boudouri", "Walter Nuninger", "Julian Alvarez", "Yvan Peter"], "title": "Role-Playing Evaluation for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval"}
{"id": "2505.11743", "pdf": "https://arxiv.org/pdf/2505.11743", "abs": "https://arxiv.org/abs/2505.11743", "authors": ["Cheng Ji", "Huaiying Luo"], "title": "Cloud-Based AI Systems: Leveraging Large Language Models for Intelligent Fault Detection and Autonomous Self-Healing", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "With the rapid development of cloud computing systems and the increasing\ncomplexity of their infrastructure, intelligent mechanisms to detect and\nmitigate failures in real time are becoming increasingly important. Traditional\nmethods of failure detection are often difficult to cope with the scale and\ndynamics of modern cloud environments. In this study, we propose a novel AI\nframework based on Massive Language Model (LLM) for intelligent fault detection\nand self-healing mechanisms in cloud systems. The model combines existing\nmachine learning fault detection algorithms with LLM's natural language\nunderstanding capabilities to process and parse system logs, error reports, and\nreal-time data streams through semantic context. The method adopts a\nmulti-level architecture, combined with supervised learning for fault\nclassification and unsupervised learning for anomaly detection, so that the\nsystem can predict potential failures before they occur and automatically\ntrigger the self-healing mechanism. Experimental results show that the proposed\nmodel is significantly better than the traditional fault detection system in\nterms of fault detection accuracy, system downtime reduction and recovery\nspeed."}
{"id": "2505.12526", "pdf": "https://arxiv.org/pdf/2505.12526", "abs": "https://arxiv.org/abs/2505.12526", "authors": ["Alexander Panyshev", "Dmitry Vinichenko", "Oleg Travkin", "Roman Alferov", "Alexey Zaytsev"], "title": "Never Skip a Batch: Continuous Training of Temporal GNNs via Adaptive Pseudo-Supervision", "categories": ["cs.LG"], "comment": null, "summary": "Temporal Graph Networks (TGNs), while being accurate, face significant\ntraining inefficiencies due to irregular supervision signals in dynamic graphs,\nwhich induce sparse gradient updates. We first theoretically establish that\naggregating historical node interactions into pseudo-labels reduces gradient\nvariance, accelerating convergence. Building on this analysis, we propose\nHistory-Averaged Labels (HAL), a method that dynamically enriches training\nbatches with pseudo-targets derived from historical label distributions. HAL\nensures continuous parameter updates without architectural modifications by\nconverting idle computation into productive learning steps. Experiments on the\nTemporal Graph Benchmark (TGB) validate our findings and an assumption about\nslow change of user preferences: HAL accelerates TGNv2 training by up to 15x\nwhile maintaining competitive performance. Thus, this work offers an efficient,\nlightweight, architecture-agnostic, and theoretically motivated solution to\nlabel sparsity in temporal graph learning."}
{"id": "2505.13171", "pdf": "https://arxiv.org/pdf/2505.13171", "abs": "https://arxiv.org/abs/2505.13171", "authors": ["Yixuan Xu", "Antoine Bosselut", "Imanol Schlag"], "title": "Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks", "categories": ["cs.CL"], "comment": null, "summary": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences."}
{"id": "2505.11745", "pdf": "https://arxiv.org/pdf/2505.11745", "abs": "https://arxiv.org/abs/2505.11745", "authors": ["Joshua Inman", "Tanmay Khandait", "Lalitha Sankar", "Giulia Pedrielli"], "title": "POCAII: Parameter Optimization with Conscious Allocation using Iterative Intelligence", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "21 pages, 4 figures", "summary": "In this paper we propose for the first time the hyperparameter optimization\n(HPO) algorithm POCAII. POCAII differs from the Hyperband and Successive\nHalving literature by explicitly separating the search and evaluation phases\nand utilizing principled approaches to exploration and exploitation principles\nduring both phases. Such distinction results in a highly flexible scheme for\nmanaging a hyperparameter optimization budget by focusing on search (i.e.,\ngenerating competing configurations) towards the start of the HPO process while\nincreasing the evaluation effort as the HPO comes to an end.\n  POCAII was compared to state of the art approaches SMAC, BOHB and DEHB. Our\nalgorithm shows superior performance in low-budget hyperparameter optimization\nregimes. Since many practitioners do not have exhaustive resources to assign to\nHPO, it has wide applications to real-world problems. Moreover, the empirical\nevidence showed how POCAII demonstrates higher robustness and lower variance in\nthe results. This is again very important when considering realistic scenarios\nwith extremely expensive models to train."}
{"id": "2505.12530", "pdf": "https://arxiv.org/pdf/2505.12530", "abs": "https://arxiv.org/abs/2505.12530", "authors": ["Yutian He", "Yankun Huang", "Yao Yao", "Qihang Lin"], "title": "Enforcing Fairness Where It Matters: An Approach Based on Difference-of-Convex Constraints", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": "43 pages, 10 figures, 1 table", "summary": "Fairness in machine learning has become a critical concern, particularly in\nhigh-stakes applications. Existing approaches often focus on achieving full\nfairness across all score ranges generated by predictive models, ensuring\nfairness in both high and low-scoring populations. However, this stringent\nrequirement can compromise predictive performance and may not align with the\npractical fairness concerns of stakeholders. In this work, we propose a novel\nframework for building partially fair machine learning models, which enforce\nfairness within a specific score range of interest, such as the middle range\nwhere decisions are most contested, while maintaining flexibility in other\nregions. We introduce two statistical metrics to rigorously evaluate partial\nfairness within a given score range, such as the top 20%-40% of scores. To\nachieve partial fairness, we propose an in-processing method by formulating the\nmodel training problem as constrained optimization with difference-of-convex\nconstraints, which can be solved by an inexact difference-of-convex algorithm\n(IDCA). We provide the complexity analysis of IDCA for finding a nearly KKT\npoint. Through numerical experiments on real-world datasets, we demonstrate\nthat our framework achieves high predictive performance while enforcing partial\nfairness where it matters most."}
{"id": "2505.13173", "pdf": "https://arxiv.org/pdf/2505.13173", "abs": "https://arxiv.org/abs/2505.13173", "authors": ["V. S. D. S. Mahesh Akavarapu", "Hrishikesh Terdalkar", "Pramit Bhattacharyya", "Shubhangi Agarwal", "Vishakha Deulgaonkar", "Pralay Manna", "Chaitali Dangarikar", "Arnab Bhattacharya"], "title": "A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs", "categories": ["cs.CL", "I.2.7"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across diverse tasks and languages. In this study, we focus on\nnatural language understanding in three classical languages -- Sanskrit,\nAncient Greek and Latin -- to investigate the factors affecting cross-lingual\nzero-shot generalization. First, we explore named entity recognition and\nmachine translation into English. While LLMs perform equal to or better than\nfine-tuned baselines on out-of-domain data, smaller models often struggle,\nespecially with niche or abstract entity types. In addition, we concentrate on\nSanskrit by presenting a factoid question-answering (QA) dataset and show that\nincorporating context via retrieval-augmented generation approach significantly\nboosts performance. In contrast, we observe pronounced performance drops for\nsmaller LLMs across these QA tasks. These results suggest model scale as an\nimportant factor influencing cross-lingual generalization. Assuming that models\nused such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical\nlanguages, our findings provide insights into how LLMs may generalize on these\nlanguages and their consequent utility in classical studies."}
{"id": "2505.11746", "pdf": "https://arxiv.org/pdf/2505.11746", "abs": "https://arxiv.org/abs/2505.11746", "authors": ["Xianglong Xu", "John Bowen", "Rojin Taheri"], "title": "Token Masking Improves Transformer-Based Text Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While transformer-based models achieve strong performance on text\nclassification, we explore whether masking input tokens can further enhance\ntheir effectiveness. We propose token masking regularization, a simple yet\ntheoretically motivated method that randomly replaces input tokens with a\nspecial [MASK] token at probability p. This introduces stochastic perturbations\nduring training, leading to implicit gradient averaging that encourages the\nmodel to capture deeper inter-token dependencies. Experiments on language\nidentification and sentiment analysis -- across diverse models (mBERT,\nQwen2.5-0.5B, TinyLlama-1.1B) -- show consistent improvements over standard\nregularization techniques. We identify task-specific optimal masking rates,\nwith p = 0.1 as a strong general default. We attribute the gains to two key\neffects: (1) input perturbation reduces overfitting, and (2) gradient-level\nsmoothing acts as implicit ensembling."}
{"id": "2505.12534", "pdf": "https://arxiv.org/pdf/2505.12534", "abs": "https://arxiv.org/abs/2505.12534", "authors": ["Adrian Mirza", "Nawaf Alampara", "Martiño Ríos-García", "Mohamed Abdelalim", "Jack Butler", "Bethany Connolly", "Tunca Dogan", "Marianna Nezhurina", "Bünyamin Şen", "Santosh Tirunagari", "Mark Worrall", "Adamo Young", "Philippe Schwaller", "Michael Pieler", "Kevin Maik Jablonka"], "title": "ChemPile: A 250GB Diverse and Curated Dataset for Chemical Foundation Models", "categories": ["cs.LG"], "comment": null, "summary": "Foundation models have shown remarkable success across scientific domains,\nyet their impact in chemistry remains limited due to the absence of diverse,\nlarge-scale, high-quality datasets that reflect the field's multifaceted\nnature. We present the ChemPile, an open dataset containing over 75 billion\ntokens of curated chemical data, specifically built for training and evaluating\ngeneral-purpose models in the chemical sciences. The dataset mirrors the human\nlearning journey through chemistry -- from educational foundations to\nspecialized expertise -- spanning multiple modalities and content types\nincluding structured data in diverse chemical representations (SMILES, SELFIES,\nIUPAC names, InChI, molecular renderings), scientific and educational text,\nexecutable code, and chemical images. ChemPile integrates foundational\nknowledge (textbooks, lecture notes), specialized expertise (scientific\narticles and language-interfaced data), visual understanding (molecular\nstructures, diagrams), and advanced reasoning (problem-solving traces and code)\n-- mirroring how human chemists develop expertise through diverse learning\nmaterials and experiences. Constructed through hundreds of hours of expert\ncuration, the ChemPile captures both foundational concepts and domain-specific\ncomplexity. We provide standardized training, validation, and test splits,\nenabling robust benchmarking. ChemPile is openly released via HuggingFace with\na consistent API, permissive license, and detailed documentation. We hope the\nChemPile will serve as a catalyst for chemical AI, enabling the development of\nthe next generation of chemical foundation models."}
{"id": "2505.13176", "pdf": "https://arxiv.org/pdf/2505.13176", "abs": "https://arxiv.org/abs/2505.13176", "authors": ["Zihao Cheng", "Hongru Wang", "Zeming Liu", "Yuhang Guo", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum."}
{"id": "2505.11750", "pdf": "https://arxiv.org/pdf/2505.11750", "abs": "https://arxiv.org/abs/2505.11750", "authors": ["Zhanxiang Hua", "Ryan Sobash", "David John Gagne II", "Yingkai Sha", "Alexandra Anderson-Frey"], "title": "Improving Medium Range Severe Weather Prediction through Transformer Post-processing of AI Weather Forecasts", "categories": ["physics.ao-ph", "cs.AI", "cs.LG"], "comment": "16 pages, 10 figures", "summary": "Improving the skill of medium-range (1-8 day) severe weather prediction is\ncrucial for mitigating societal impacts. This study introduces a novel approach\nleveraging decoder-only transformer networks to post-process AI-based weather\nforecasts, specifically from the Pangu-Weather model, for improved severe\nweather guidance. Unlike traditional post-processing methods that use a dense\nneural network to predict the probability of severe weather using discrete\nforecast samples, our method treats forecast lead times as sequential\n``tokens'', enabling the transformer to learn complex temporal relationships\nwithin the evolving atmospheric state. We compare this approach against\npost-processing of the Global Forecast System (GFS) using both a traditional\ndense neural network and our transformer, as well as configurations that\nexclude convective parameters to fairly evaluate the impact of using the\nPangu-Weather AI model. Results demonstrate that the transformer-based\npost-processing significantly enhances forecast skill compared to dense neural\nnetworks. Furthermore, AI-driven forecasts, particularly Pangu-Weather\ninitialized from high resolution analysis, exhibit superior performance to GFS\nin the medium-range, even without explicit convective parameters. Our approach\noffers improved accuracy, and reliability, which also provides interpretability\nthrough feature attribution analysis, advancing medium-range severe weather\nprediction capabilities."}
{"id": "2505.12540", "pdf": "https://arxiv.org/pdf/2505.12540", "abs": "https://arxiv.org/abs/2505.12540", "authors": ["Rishi Jha", "Collin Zhang", "Vitaly Shmatikov", "John X. Morris"], "title": "Harnessing the Universal Geometry of Embeddings", "categories": ["cs.LG"], "comment": null, "summary": "We introduce the first method for translating text embeddings from one vector\nspace to another without any paired data, encoders, or predefined sets of\nmatches. Our unsupervised approach translates any embedding to and from a\nuniversal latent representation (i.e., a universal semantic structure\nconjectured by the Platonic Representation Hypothesis). Our translations\nachieve high cosine similarity across model pairs with different architectures,\nparameter counts, and training datasets.\n  The ability to translate unknown embeddings into a different space while\npreserving their geometry has serious implications for the security of vector\ndatabases. An adversary with access only to embedding vectors can extract\nsensitive information about the underlying documents, sufficient for\nclassification and attribute inference."}
{"id": "2505.13181", "pdf": "https://arxiv.org/pdf/2505.13181", "abs": "https://arxiv.org/abs/2505.13181", "authors": ["Zhengrui Ma", "Yang Feng", "Chenze Shao", "Fandong Meng", "Jie Zhou", "Min Zhang"], "title": "Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Demos and code are available at https://github.com/ictnlp/SLED-TTS", "summary": "We introduce SLED, an alternative approach to speech language modeling by\nencoding speech waveforms into sequences of continuous latent representations\nand modeling them autoregressively using an energy distance objective. The\nenergy distance offers an analytical measure of the distributional gap by\ncontrasting simulated and target samples, enabling efficient training to\ncapture the underlying continuous autoregressive distribution. By bypassing\nreliance on residual vector quantization, SLED avoids discretization errors and\neliminates the need for the complicated hierarchical architectures common in\nexisting speech language models. It simplifies the overall modeling pipeline\nwhile preserving the richness of speech information and maintaining inference\nefficiency. Empirical results demonstrate that SLED achieves strong performance\nin both zero-shot and streaming speech synthesis, showing its potential for\nbroader applications in general-purpose speech language models."}
{"id": "2505.11755", "pdf": "https://arxiv.org/pdf/2505.11755", "abs": "https://arxiv.org/abs/2505.11755", "authors": ["Matthew Kim", "William Sharpless", "Hyun Joe Jeong", "Sander Tonkens", "Somil Bansal", "Sylvia Herbert"], "title": "Reachability Barrier Networks: Learning Hamilton-Jacobi Solutions for Smooth and Flexible Control Barrier Functions", "categories": ["cs.RO", "cs.AI"], "comment": "15 pages, 7 figures", "summary": "Recent developments in autonomous driving and robotics underscore the\nnecessity of safety-critical controllers. Control barrier functions (CBFs) are\na popular method for appending safety guarantees to a general control\nframework, but they are notoriously difficult to generate beyond low\ndimensions. Existing methods often yield non-differentiable or inaccurate\napproximations that lack integrity, and thus fail to ensure safety. In this\nwork, we use physics-informed neural networks (PINNs) to generate smooth\napproximations of CBFs by computing Hamilton-Jacobi (HJ) optimal control\nsolutions. These reachability barrier networks (RBNs) avoid traditional\ndimensionality constraints and support the tuning of their conservativeness\npost-training through a parameterized discount term. To ensure robustness of\nthe discounted solutions, we leverage conformal prediction methods to derive\nprobabilistic safety guarantees for RBNs. We demonstrate that RBNs are highly\naccurate in low dimensions, and safer than the standard neural CBF approach in\nhigh dimensions. Namely, we showcase the RBNs in a 9D multi-vehicle collision\navoidance problem where it empirically proves to be 5.5x safer and 1.9x less\nconservative than the neural CBFs, offering a promising method to synthesize\nCBFs for general nonlinear autonomous systems."}
{"id": "2505.12541", "pdf": "https://arxiv.org/pdf/2505.12541", "abs": "https://arxiv.org/abs/2505.12541", "authors": ["Manolis Zampetakis", "Felix Zhou"], "title": "Private Statistical Estimation via Truncation", "categories": ["cs.LG", "cs.CR", "cs.DS", "stat.ML"], "comment": null, "summary": "We introduce a novel framework for differentially private (DP) statistical\nestimation via data truncation, addressing a key challenge in DP estimation\nwhen the data support is unbounded. Traditional approaches rely on\nproblem-specific sensitivity analysis, limiting their applicability. By\nleveraging techniques from truncated statistics, we develop computationally\nefficient DP estimators for exponential family distributions, including\nGaussian mean and covariance estimation, achieving near-optimal sample\ncomplexity. Previous works on exponential families only consider bounded or\none-dimensional families. Our approach mitigates sensitivity through truncation\nwhile carefully correcting for the introduced bias using maximum likelihood\nestimation and DP stochastic gradient descent. Along the way, we establish\nimproved uniform convergence guarantees for the log-likelihood function of\nexponential families, which may be of independent interest. Our results provide\na general blueprint for DP algorithm design via truncated statistics."}
{"id": "2505.13204", "pdf": "https://arxiv.org/pdf/2505.13204", "abs": "https://arxiv.org/abs/2505.13204", "authors": ["Jikai Wang", "Zhenxu Tian", "Juntao Li", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "title": "Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification", "categories": ["cs.CL"], "comment": "Pre-print", "summary": "Recent works have revealed the great potential of speculative decoding in\naccelerating the autoregressive generation process of large language models.\nThe success of these methods relies on the alignment between draft candidates\nand the sampled outputs of the target model. Existing methods mainly achieve\ndraft-target alignment with training-based methods, e.g., EAGLE, Medusa,\ninvolving considerable training costs. In this paper, we present a\ntraining-free alignment-augmented speculative decoding algorithm. We propose\nalignment sampling, which leverages output distribution obtained in the\nprefilling phase to provide more aligned draft candidates. To further benefit\nfrom high-quality but non-aligned draft candidates, we also introduce a simple\nyet effective flexible verification strategy. Through an adaptive probability\nthreshold, our approach can improve generation accuracy while further improving\ninference efficiency. Experiments on 8 datasets (including question answering,\nsummarization and code completion tasks) show that our approach increases the\naverage generation score by 3.3 points for the LLaMA3 model. Our method\nachieves a mean acceptance length up to 2.39 and speed up generation by 2.23."}
{"id": "2505.11756", "pdf": "https://arxiv.org/pdf/2505.11756", "abs": "https://arxiv.org/abs/2505.11756", "authors": ["David Chanin", "Tomáš Dulka", "Adrià Garriga-Alonso"], "title": "Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "It is assumed that sparse autoencoders (SAEs) decompose polysemantic\nactivations into interpretable linear directions, as long as the activations\nare composed of sparse linear combinations of underlying features. However, we\nfind that if an SAE is more narrow than the number of underlying \"true\nfeatures\" on which it is trained, and there is correlation between features,\nthe SAE will merge components of correlated features together, thus destroying\nmonosemanticity. In LLM SAEs, these two conditions are almost certainly true.\nThis phenomenon, which we call feature hedging, is caused by SAE reconstruction\nloss, and is more severe the narrower the SAE. In this work, we introduce the\nproblem of feature hedging and study it both theoretically in toy models and\nempirically in SAEs trained on LLMs. We suspect that feature hedging may be one\nof the core reasons that SAEs consistently underperform supervised baselines.\nFinally, we use our understanding of feature hedging to propose an improved\nvariant of matryoshka SAEs. Our work shows there remain fundamental issues with\nSAEs, but we are hopeful that that highlighting feature hedging will catalyze\nfuture advances that allow SAEs to achieve their full potential of interpreting\nLLMs at scale."}
{"id": "2505.12544", "pdf": "https://arxiv.org/pdf/2505.12544", "abs": "https://arxiv.org/abs/2505.12544", "authors": ["Mohammad R. Rezaei", "Adji Bousso Dieng"], "title": "Alternators With Noise Models", "categories": ["cs.LG"], "comment": null, "summary": "Alternators have recently been introduced as a framework for modeling\ntime-dependent data. They often outperform other popular frameworks, such as\nstate-space models and diffusion models, on challenging time-series tasks. This\npaper introduces a new Alternator model, called Alternator++, which enhances\nthe flexibility of traditional Alternators by explicitly modeling the noise\nterms used to sample the latent and observed trajectories, drawing on the idea\nof noise models from the diffusion modeling literature. Alternator++ optimizes\nthe sum of the Alternator loss and a noise-matching loss. The latter forces the\nnoise trajectories generated by the two noise models to approximate the noise\ntrajectories that produce the observed and latent trajectories. We demonstrate\nthe effectiveness of Alternator++ in tasks such as density estimation, time\nseries imputation, and forecasting, showing that it outperforms several strong\nbaselines, including Mambas, ScoreGrad, and Dyffusion."}
{"id": "2505.13210", "pdf": "https://arxiv.org/pdf/2505.13210", "abs": "https://arxiv.org/abs/2505.13210", "authors": ["Xiaocong Du", "Haoyu Pei", "Haipeng Zhang"], "title": "Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation."}
{"id": "2505.11758", "pdf": "https://arxiv.org/pdf/2505.11758", "abs": "https://arxiv.org/abs/2505.11758", "authors": ["Sriram Mandalika"], "title": "Generalizable Vision-Language Few-Shot Adaptation with Predictive Prompts and Negative Learning", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.RO"], "comment": null, "summary": "Few-shot adaptation remains a core challenge for vision-language models\n(VLMs), especially under limited supervision and noisy support samples. We\npropose PromptFuseNL, a unified framework that enhances few-shot generalization\nby combining predictive prompt tuning with dual-branch positive and negative\nlearning. The method refines class prototypes through task-conditioned\nresiduals, multi-stage cross-modal coordination, and semantic hard negative\nmining. To address label noise, we introduce an unsupervised instance\nreweighting strategy that downweights unreliable support examples without\nrequiring additional labels or structural changes. PromptFuseNL fuses visual\nand textual cues through lightweight modules for efficient and discriminative\nprediction. Evaluated across 15 benchmarks, it consistently surpasses existing\nprompt- and adapter-based methods in all shot settings while remaining highly\nefficient, achieving up to 300x faster training and 1000x lower FLOPs compared\nto full prompt tuning, achieving a new state-of-the-art for robust and scalable\nfew-shot vision-language adaptation."}
{"id": "2505.12556", "pdf": "https://arxiv.org/pdf/2505.12556", "abs": "https://arxiv.org/abs/2505.12556", "authors": ["Taniya Kapoor", "Abhishek Chandra", "Anastasios Stamou", "Stephen J Roberts"], "title": "Beyond Accuracy: EcoL2 Metric for Sustainable Neural PDE Solvers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Real-world systems, from aerospace to railway engineering, are modeled with\npartial differential equations (PDEs) describing the physics of the system.\nEstimating robust solutions for such problems is essential. Deep learning-based\narchitectures, such as neural PDE solvers, have recently gained traction as a\nreliable solution method. The current state of development of these approaches,\nhowever, primarily focuses on improving accuracy. The environmental impact of\nexcessive computation, leading to increased carbon emissions, has largely been\noverlooked. This paper introduces a carbon emission measure for a range of PDE\nsolvers. Our proposed metric, EcoL2, balances model accuracy with emissions\nacross data collection, model training, and deployment. Experiments across both\nphysics-informed machine learning and operator learning architectures\ndemonstrate that the proposed metric presents a holistic assessment of model\nperformance and emission cost. As such solvers grow in scale and deployment,\nEcoL2 represents a step toward building performant scientific machine learning\nsystems with lower long-term environmental impact."}
{"id": "2505.13220", "pdf": "https://arxiv.org/pdf/2505.13220", "abs": "https://arxiv.org/abs/2505.13220", "authors": ["Jie Ying", "Zihong Chen", "Zhefan Wang", "Wanli Jiang", "Chenyang Wang", "Zhonghang Yuan", "Haoyang Su", "Huanjun Kong", "Fan Yang", "Nanqing Dong"], "title": "SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Seed science is essential for modern agriculture, directly influencing crop\nyields and global food security. However, challenges such as interdisciplinary\ncomplexity and high costs with limited returns hinder progress, leading to a\nshortage of experts and insufficient technological support. While large\nlanguage models (LLMs) have shown promise across various fields, their\napplication in seed science remains limited due to the scarcity of digital\nresources, complex gene-trait relationships, and the lack of standardized\nbenchmarks. To address this gap, we introduce SeedBench -- the first multi-task\nbenchmark specifically designed for seed science. Developed in collaboration\nwith domain experts, SeedBench focuses on seed breeding and simulates key\naspects of modern breeding processes. We conduct a comprehensive evaluation of\n26 leading LLMs, encompassing proprietary, open-source, and domain-specific\nfine-tuned models. Our findings not only highlight the substantial gaps between\nthe power of LLMs and the real-world seed science problems, but also make a\nfoundational step for research on LLMs for seed design."}
{"id": "2505.11760", "pdf": "https://arxiv.org/pdf/2505.11760", "abs": "https://arxiv.org/abs/2505.11760", "authors": ["Mansi Sakarvadia", "Nathaniel Hudson", "Tian Li", "Ian Foster", "Kyle Chard"], "title": "Topology-Aware Knowledge Propagation in Decentralized Learning", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Decentralized learning enables collaborative training of models across\nnaturally distributed data without centralized coordination or maintenance of a\nglobal model. Instead, devices are organized in arbitrary communication\ntopologies, in which they can only communicate with neighboring devices. Each\ndevice maintains its own local model by training on its local data and\nintegrating new knowledge via model aggregation with neighbors. Therefore,\nknowledge is propagated across the topology via successive aggregation rounds.\nWe study, in particular, the propagation of out-of-distribution (OOD)\nknowledge. We find that popular decentralized learning algorithms struggle to\npropagate OOD knowledge effectively to all devices. Further, we find that both\nthe location of OOD data within a topology, and the topology itself,\nsignificantly impact OOD knowledge propagation. We then propose topology-aware\naggregation strategies to accelerate (OOD) knowledge propagation across\ndevices. These strategies improve OOD data accuracy, compared to\ntopology-unaware baselines, by 123% on average across models in a topology."}
{"id": "2505.12566", "pdf": "https://arxiv.org/pdf/2505.12566", "abs": "https://arxiv.org/abs/2505.12566", "authors": ["Leyang Xue", "Yao Fu", "Luo Mai", "Mahesh K. Marina"], "title": "HybridServe: Efficient Serving of Large AI Models with Confidence-Based Cascade Routing", "categories": ["cs.LG"], "comment": null, "summary": "Giant Deep Neural Networks (DNNs), have become indispensable for accurate and\nrobust support of large-scale cloud based AI services. However, serving giant\nDNNs is prohibitively expensive from an energy consumption viewpoint easily\nexceeding that of training, due to the enormous scale of GPU clusters needed to\nhold giant DNN model partitions and replicas. Existing approaches can either\noptimize energy efficiency or inference accuracy but not both. To overcome this\nstatus quo, we propose HybridServe, a novel hybrid DNN model serving system\nthat leverages multiple sized versions (small to giant) of the model to be\nserved in tandem. Through a confidence based hybrid model serving dataflow,\nHybridServe prefers to serve inference requests with energy-efficient smaller\nmodels so long as accuracy is not compromised, thereby reducing the number of\nreplicas needed for giant DNNs. HybridServe also features a dataflow planner\nfor efficient partitioning and replication of candidate models to maximize\nserving system throughput. Experimental results using a prototype\nimplementation of HybridServe show that it reduces energy footprint by up to\n19.8x compared to the state-of-the-art DNN model serving systems while matching\nthe accuracy of serving solely with giant DNNs."}
{"id": "2505.13244", "pdf": "https://arxiv.org/pdf/2505.13244", "abs": "https://arxiv.org/abs/2505.13244", "authors": ["Jieying Xue", "Phuong Minh Nguyen", "Minh Le Nguyen", "Xin Liu"], "title": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models", "categories": ["cs.CL", "cs.LG"], "comment": "Published in The 19th International Workshop on Semantic Evaluation\n  (SemEval-2025)", "summary": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection."}
{"id": "2505.11764", "pdf": "https://arxiv.org/pdf/2505.11764", "abs": "https://arxiv.org/abs/2505.11764", "authors": ["Raymond Baartmans", "Matthew Raffel", "Rahul Vikram", "Aiden Deringer", "Lizhong Chen"], "title": "Towards Universal Semantics With Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a\nuniversal set of semantic primes: simple, primitive word-meanings that have\nbeen shown to exist in most, if not all, languages of the world. According to\nthis framework, any word, regardless of complexity, can be paraphrased using\nthese primes, revealing a clear and universally translatable meaning. These\nparaphrases, known as explications, can offer valuable applications for many\nnatural language processing (NLP) tasks, but producing them has traditionally\nbeen a slow, manual process. In this work, we present the first study of using\nlarge language models (LLMs) to generate NSM explications. We introduce\nautomatic evaluation methods, a tailored dataset for training and evaluation,\nand fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in\nproducing accurate, cross-translatable explications, marking a significant step\ntoward universal semantic representation with LLMs and opening up new\npossibilities for applications in semantic analysis, translation, and beyond."}
{"id": "2505.12576", "pdf": "https://arxiv.org/pdf/2505.12576", "abs": "https://arxiv.org/abs/2505.12576", "authors": ["Kiran Kokilepersaud", "Mohit Prabhushankar", "Ghassan AlRegib"], "title": "AdaDim: Dimensionality Adaptation for SSL Representational Dynamics", "categories": ["cs.LG", "cs.AI"], "comment": "Under Review", "summary": "A key factor in effective Self-Supervised learning (SSL) is preventing\ndimensional collapse, which is where higher-dimensional representation spaces\nspan a lower-dimensional subspace. Therefore, SSL optimization strategies\ninvolve guiding a model to produce representations ($R$) with a higher\ndimensionality. Dimensionality is either optimized through a\ndimension-contrastive approach that encourages feature decorrelation or through\na sample-contrastive method that promotes a uniform spread of sample\nrepresentations. Both families of SSL algorithms also utilize a projection head\nthat maps $R$ into a lower-dimensional embedding space $Z$. Recent work has\ncharacterized the projection head as a filter of irrelevant features from the\nSSL objective by reducing mutual information, $I(R;Z)$. Therefore, the current\nliterature's view is that a good SSL representation space should have a high\n$H(R)$ and a low $I(R;Z)$. However, this view of the problem is lacking in\nterms of an understanding of the underlying training dynamics that influences\nboth terms, as well as how the values of $H(R)$ and $I(R;Z)$ arrived at the end\nof training reflect the downstream performance of an SSL model. We address both\ngaps in the literature by demonstrating that increases in $H(R)$ due to feature\ndecorrelation at the start of training lead to a higher $I(R;Z)$, while\nincreases in $H(R)$ due to samples distributing uniformly in a high-dimensional\nspace at the end of training cause $I(R;Z)$ to plateau or decrease.\nFurthermore, our analysis shows that the best performing SSL models do not have\nthe highest $H(R)$ nor the lowest $I(R;Z)$, but arrive at an optimal\nintermediate point for both. We develop a method called AdaDim to exploit these\nobserved training dynamics by adaptively weighting between losses based on\nfeature decorrelation and uniform sample spread."}
{"id": "2505.13251", "pdf": "https://arxiv.org/pdf/2505.13251", "abs": "https://arxiv.org/abs/2505.13251", "authors": ["Sidney Wong"], "title": "Stronger Together: Unleashing the Social Impact of Hate Speech Research", "categories": ["cs.CL"], "comment": "Accepted Proceedings of the Linguistic Society of America 2025 Annual\n  Meeting", "summary": "The advent of the internet has been both a blessing and a curse for once\nmarginalised communities. When used well, the internet can be used to connect\nand establish communities crossing different intersections; however, it can\nalso be used as a tool to alienate people and communities as well as perpetuate\nhate, misinformation, and disinformation especially on social media platforms.\nWe propose steering hate speech research and researchers away from pre-existing\ncomputational solutions and consider social methods to inform social solutions\nto address this social problem. In a similar way linguistics research can\ninform language planning policy, linguists should apply what we know about\nlanguage and society to mitigate some of the emergent risks and dangers of\nanti-social behaviour in digital spaces. We argue linguists and NLP researchers\ncan play a principle role in unleashing the social impact potential of\nlinguistics research working alongside communities, advocates, activists, and\npolicymakers to enable equitable digital inclusion and to close the digital\ndivide."}
{"id": "2505.11765", "pdf": "https://arxiv.org/pdf/2505.11765", "abs": "https://arxiv.org/abs/2505.11765", "authors": ["Shijun Li", "Hilaf Hasson", "Joydeep Ghosh"], "title": "OMAC: A Broad Optimization Framework for LLM-Based Multi-Agent Collaboration", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Agents powered by advanced large language models (LLMs) have demonstrated\nimpressive capabilities across diverse complex applications. Recently,\nMulti-Agent Systems (MAS), wherein multiple agents collaborate and communicate\nwith each other, have exhibited enhanced capabilities in complex tasks, such as\nhigh-quality code generation and arithmetic reasoning. However, the development\nof such systems often relies on handcrafted methods, and the literature on\nsystematic design and optimization of LLM-based MAS remains limited.\n  In this work, we introduce OMAC, a general framework designed for holistic\noptimization of LLM-based MAS. Specifically, we identify five key optimization\ndimensions for MAS, encompassing both agent functionality and collaboration\nstructure. Building upon these dimensions, we first propose a general\nalgorithm, utilizing two actors termed the Semantic Initializer and the\nContrastive Comparator, to optimize any single dimension. Then, we present an\nalgorithm for joint optimization across multiple dimensions. Extensive\nexperiments demonstrate the superior performance of OMAC on code generation,\narithmetic reasoning, and general reasoning tasks against state-of-the-art\napproaches."}
{"id": "2505.12579", "pdf": "https://arxiv.org/pdf/2505.12579", "abs": "https://arxiv.org/abs/2505.12579", "authors": ["Shiyun Xu", "Zhiqi Bu"], "title": "Adaptive parameter-efficient fine-tuning via Hessian-informed subset selection", "categories": ["cs.LG"], "comment": "Equal contribution", "summary": "Parameter-efficient fine-tuning (PEFT) is a highly effective approach for\nadapting large pre-trained models to downstream tasks with minimal\ncomputational overhead. At the core, PEFT methods freeze most parameters and\nonly trains a small subset (say $<0.1\\%$ of total parameters). Notably,\ndifferent PEFT methods select different subsets, resulting in varying levels of\nperformance. This variation prompts a key question: how to effectively select\nthe most influential subset to train?\n  We formulate the subset selection as a multi-task problem: maximizing the\nperformance and minimizing the number of trainable parameters. We leverage a\nseries of transformations -- including $\\epsilon$-constraint method and\nsecond-order Taylor approximation -- to arrive at the classical 0-1 knapsack\nproblem, which we solve through the lens of Pareto optimality. Consequently, we\npropose AdaPEFT, a Hessian-informed PEFT that adapts to various tasks and\nmodels, in which the selected subset empirically transfers across training\nhorizons and model sizes."}
{"id": "2505.13252", "pdf": "https://arxiv.org/pdf/2505.13252", "abs": "https://arxiv.org/abs/2505.13252", "authors": ["Rikhil Amonkar", "Ronan Le Bras", "Li Zhang"], "title": "Natural Language Planning via Coding and Inference Scaling", "categories": ["cs.CL"], "comment": null, "summary": "Real-life textual planning tasks such as meeting scheduling have posed much\nchallenge to LLMs especially when the complexity is high. While previous work\nprimarily studied auto-regressive generation of plans with closed-source\nmodels, we systematically evaluate both closed- and open-source models,\nincluding those that scales output length with complexity during inference, in\ngenerating programs, which are executed to output the plan. We consider not\nonly standard Python code, but also the code to a constraint satisfaction\nproblem solver. Despite the algorithmic nature of the task, we show that\nprogramming often but not always outperforms planning. Our detailed error\nanalysis also indicates a lack of robustness and efficiency in the generated\ncode that hinders generalization."}
{"id": "2505.11766", "pdf": "https://arxiv.org/pdf/2505.11766", "abs": "https://arxiv.org/abs/2505.11766", "authors": ["Haoze Song", "Zhihao Li", "Xiaobo Zhang", "Zecheng Gan", "Zhilu Lai", "Wei Wang"], "title": "Redefining Neural Operators in $d+1$ Dimensions", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": null, "summary": "Neural Operators have emerged as powerful tools for learning mappings between\nfunction spaces. Among them, the kernel integral operator has been widely\nvalidated on universally approximating various operators. Although recent\nadvancements following this definition have developed effective modules to\nbetter approximate the kernel function defined on the original domain (with $d$\ndimensions, $d=1, 2, 3...$), the unclarified evolving mechanism in the\nembedding spaces blocks our view to design neural operators that can fully\ncapture the target system evolution.\n  Drawing on recent breakthroughs in quantum simulation of partial differential\nequations (PDEs), we elucidate the linear evolution process in neural\noperators. Based on that, we redefine neural operators on a new $d+1$\ndimensional domain. Within this framework, we implement our proposed\nSchr\\\"odingerised Kernel Neural Operator (SKNO) aligning better with the $d+1$\ndimensional evolution. In experiments, our $d+1$ dimensional evolving linear\nblock performs far better than others. Also, we test SKNO's SOTA performance on\nvarious benchmark tests and also the zero-shot super-resolution task. In\naddition, we analyse the impact of different lifting and recovering operators\non the prediction within the redefined NO framework, reflecting the alignment\nbetween our model and the underlying $d+1$ dimensional evolution."}
{"id": "2505.12581", "pdf": "https://arxiv.org/pdf/2505.12581", "abs": "https://arxiv.org/abs/2505.12581", "authors": ["Lucas M. Dorneles", "Luan Fonseca Garcia", "Joel Luís Carbonera"], "title": "An approach based on class activation maps for investigating the effects of data augmentation on neural networks for image classification", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Neural networks have become increasingly popular in the last few years as an\neffective tool for the task of image classification due to the impressive\nperformance they have achieved on this task. In image classification tasks, it\nis common to use data augmentation strategies to increase the robustness of\ntrained networks to changes in the input images and to avoid overfitting.\nAlthough data augmentation is a widely adopted technique, the literature lacks\na body of research analyzing the effects data augmentation methods have on the\npatterns learned by neural network models working on complex datasets. The\nprimary objective of this work is to propose a methodology and set of metrics\nthat may allow a quantitative approach to analyzing the effects of data\naugmentation in convolutional networks applied to image classification. An\nimportant tool used in the proposed approach lies in the concept of class\nactivation maps for said models, which allow us to identify and measure the\nimportance these models assign to each individual pixel in an image when\nexecuting the classification task. From these maps, we may then extract metrics\nover the similarities and differences between maps generated by these models\ntrained on a given dataset with different data augmentation strategies.\nExperiments made using this methodology suggest that the effects of these data\naugmentation techniques not only can be analyzed in this way but also allow us\nto identify different impact profiles over the trained models."}
{"id": "2505.13254", "pdf": "https://arxiv.org/pdf/2505.13254", "abs": "https://arxiv.org/abs/2505.13254", "authors": ["Siran Liu", "Yang Ye", "Qianchao Zhu", "Zheng Cao", "Yongchao He"], "title": "HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding", "categories": ["cs.CL"], "comment": null, "summary": "Autoregressive decoding, the standard approach for Large Language Model (LLM)\ninference, remains a significant bottleneck due to its sequential nature. While\nspeculative decoding algorithms mitigate this inefficiency through parallel\nverification, they fail to exploit the inherent heterogeneity in linguistic\ncomplexity, a key factor leading to suboptimal resource allocation. We address\nthis by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding\nframework that dynamically optimizes computational resource allocation based on\nlinguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A\nnovel cumulative meta-path Top-$K$ entropy metric for efficiently identifying\npredictable contexts. (2) A dynamic resource allocation strategy based on\ndata-driven entropy partitioning, enabling adaptive speculative expansion and\npruning tailored to local context difficulty. Evaluated on five public\nbenchmarks and four models, HeteroSpec achieves an average speedup of\n4.26$\\times$. It consistently outperforms state-of-the-art EAGLE-3 across\nspeedup rates, average acceptance length, and verification cost. Notably,\nHeteroSpec requires no draft model retraining, incurs minimal overhead, and is\northogonal to other acceleration techniques. It demonstrates enhanced\nacceleration with stronger draft models, establishing a new paradigm for\ncontext-aware LLM inference acceleration."}
{"id": "2505.11770", "pdf": "https://arxiv.org/pdf/2505.11770", "abs": "https://arxiv.org/abs/2505.11770", "authors": ["Jing Huang", "Junyi Tao", "Thomas Icard", "Diyi Yang", "Christopher Potts"], "title": "Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "ICML 2025", "summary": "Interpretability research now offers a variety of techniques for identifying\nabstract internal mechanisms in neural networks. Can such techniques be used to\npredict how models will behave on out-of-distribution examples? In this work,\nwe provide a positive answer to this question. Through a diverse set of\nlanguage modeling tasks--including symbol manipulation, knowledge retrieval,\nand instruction following--we show that the most robust features for\ncorrectness prediction are those that play a distinctive causal role in the\nmodel's behavior. Specifically, we propose two methods that leverage causal\nmechanisms to predict the correctness of model outputs: counterfactual\nsimulation (checking whether key causal variables are realized) and value\nprobing (using the values of those variables to make predictions). Both achieve\nhigh AUC-ROC in distribution and outperform methods that rely on\ncausal-agnostic features in out-of-distribution settings, where predicting\nmodel behaviors is more crucial. Our work thus highlights a novel and\nsignificant application for internal causal analysis of language models."}
{"id": "2505.12585", "pdf": "https://arxiv.org/pdf/2505.12585", "abs": "https://arxiv.org/abs/2505.12585", "authors": ["En Yu", "Jie Lu", "Xiaoyu Yang", "Guangquan Zhang", "Zhen Fang"], "title": "Learning Robust Spectral Dynamics for Temporal Domain Generalization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Modern machine learning models struggle to maintain performance in dynamic\nenvironments where temporal distribution shifts, \\emph{i.e., concept drift},\nare prevalent. Temporal Domain Generalization (TDG) seeks to enable model\ngeneralization across evolving domains, yet existing approaches typically\nassume smooth incremental changes, struggling with complex real-world drifts\ninvolving long-term structure (incremental evolution/periodicity) and local\nuncertainties. To overcome these limitations, we introduce FreKoo, which\ntackles these challenges via a novel frequency-domain analysis of parameter\ntrajectories. It leverages the Fourier transform to disentangle parameter\nevolution into distinct spectral bands. Specifically, low-frequency component\nwith dominant dynamics are learned and extrapolated using the Koopman operator,\nrobustly capturing diverse drift patterns including both incremental and\nperiodicity. Simultaneously, potentially disruptive high-frequency variations\nare smoothed via targeted temporal regularization, preventing overfitting to\ntransient noise and domain uncertainties. In addition, this dual spectral\nstrategy is rigorously grounded through theoretical analysis, providing\nstability guarantees for the Koopman prediction, a principled Bayesian\njustification for the high-frequency regularization, and culminating in a\nmultiscale generalization bound connecting spectral dynamics to improved\ngeneralization. Extensive experiments demonstrate FreKoo's significant\nsuperiority over SOTA TDG approaches, particularly excelling in real-world\nstreaming scenarios with complex drifts and uncertainties."}
{"id": "2505.13257", "pdf": "https://arxiv.org/pdf/2505.13257", "abs": "https://arxiv.org/abs/2505.13257", "authors": ["Zilu Tang", "Afra Feyza Akyürek", "Ekin Akyürek", "Derry Wijaya"], "title": "WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, preprint", "summary": "Preference alignment has become a standard pipeline in finetuning models to\nfollow \\emph{generic} human preferences. Majority of work seeks to optimize\nmodel to produce responses that would be preferable \\emph{on average},\nsimplifying the diverse and often \\emph{contradicting} space of human\npreferences. While research has increasingly focused on personalized alignment:\nadapting models to individual user preferences, there is a lack of personalized\npreference dataset which focus on nuanced individual-level preferences. To\naddress this, we introduce WikiPersona: the first fine-grained personalization\nusing well-documented, famous individuals. Our dataset challenges models to\nalign with these personas through an interpretable process: generating\nverifiable textual descriptions of a persona's background and preferences in\naddition to alignment. We systematically evaluate different personalization\napproaches and find that as few-shot prompting with preferences and fine-tuning\nfail to simultaneously ensure effectiveness and efficiency, using\n\\textit{inferred personal preferences} as prefixes enables effective\npersonalization, especially in topics where preferences clash while leading to\nmore equitable generalization across unseen personas."}
{"id": "2505.11771", "pdf": "https://arxiv.org/pdf/2505.11771", "abs": "https://arxiv.org/abs/2505.11771", "authors": ["Yichen Xu", "Ryumei Nakada", "Linjun Zhang", "Lexin Li"], "title": "Residual Feature Integration is Sufficient to Prevent Negative Transfer", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Transfer learning typically leverages representations learned from a source\ndomain to improve performance on a target task. A common approach is to extract\nfeatures from a pre-trained model and directly apply them for target\nprediction. However, this strategy is prone to negative transfer where the\nsource representation fails to align with the target distribution. In this\narticle, we propose Residual Feature Integration (REFINE), a simple yet\neffective method designed to mitigate negative transfer. Our approach combines\na fixed source-side representation with a trainable target-side encoder and\nfits a shallow neural network on the resulting joint representation, which\nadapts to the target domain while preserving transferable knowledge from the\nsource domain. Theoretically, we prove that REFINE is sufficient to prevent\nnegative transfer under mild conditions, and derive the generalization bound\ndemonstrating its theoretical benefit. Empirically, we show that REFINE\nconsistently enhances performance across diverse application and data\nmodalities including vision, text, and tabular data, and outperforms numerous\nalternative solutions. Our method is lightweight, architecture-agnostic, and\nrobust, making it a valuable addition to the existing transfer learning\ntoolbox."}
{"id": "2505.12586", "pdf": "https://arxiv.org/pdf/2505.12586", "abs": "https://arxiv.org/abs/2505.12586", "authors": ["Sanggeon Yun", "Ryozo Masukawa", "Hyunwoo Oh", "Nathaniel D. Bastian", "Mohsen Imani"], "title": "A Few Large Shifts: Layer-Inconsistency Based Minimal Overhead Adversarial Example Detection", "categories": ["cs.LG"], "comment": null, "summary": "Deep neural networks (DNNs) are highly susceptible to adversarial\nexamples--subtle, imperceptible perturbations that can lead to incorrect\npredictions. While detection-based defenses offer a practical alternative to\nadversarial training, many existing methods depend on external models, complex\narchitectures, heavy augmentations, or adversarial data, limiting their\nefficiency and generalizability. We introduce a lightweight, plug-in detection\nframework that leverages internal layer-wise inconsistencies within the target\nmodel itself, requiring only benign data for calibration. Our approach is\ngrounded in the A Few Large Shifts Assumption, which posits that adversarial\nperturbations typically induce large representation shifts in a small subset of\nlayers. Building on this, we propose two complementary strategies--Recovery\nTesting (RT) and Logit-layer Testing (LT)--to expose internal disruptions\ncaused by adversaries. Evaluated on CIFAR-10, CIFAR-100, and ImageNet under\nboth standard and adaptive threat models, our method achieves state-of-the-art\ndetection performance with negligible computational overhead and no compromise\nto clean accuracy."}
{"id": "2505.13258", "pdf": "https://arxiv.org/pdf/2505.13258", "abs": "https://arxiv.org/abs/2505.13258", "authors": ["Jingyi Ren", "Yekun Xu", "Xiaolong Wang", "Weitao Li", "Weizhi Ma", "Yang Liu"], "title": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive domains.\nHowever, although RAG achieved successes across distinct domains, there are\nstill some unsolved challenges: 1) Effectiveness. Existing research mainly\nfocuses on developing more powerful RAG retrievers, but how to enhance the\ngenerator's (LLM's) ability to utilize the retrieved information for reasoning\nand generation? 2) Transparency. Most RAG methods ignore which retrieved\ncontent actually contributes to the reasoning process, resulting in a lack of\ninterpretability and visibility. To address this, we propose ARENA\n(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator\nframework trained via reinforcement learning (RL) with our proposed rewards.\nBased on the structured generation and adaptive reward calculation, our\nRL-based training enables the model to identify key evidence, perform\nstructured reasoning, and generate answers with interpretable decision traces.\nApplied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments\nwith various RAG baselines demonstrate that our model achieves 10-30%\nimprovements on all multi-hop QA datasets, which is comparable with the SOTA\nCommercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses\nshow that ARENA has strong flexibility to be adopted on new datasets without\nextra training. Our models and codes are publicly released."}
{"id": "2505.11774", "pdf": "https://arxiv.org/pdf/2505.11774", "abs": "https://arxiv.org/abs/2505.11774", "authors": ["James V. Roggeveen", "Erik Y. Wang", "Will Flintoft", "Peter Donets", "Lucy S. Nathwani", "Nickholas Gutierrez", "David Ettel", "Anton Marius Graf", "Siddharth Dandavate", "Arjun Nageswaran", "Raglan Ward", "Ava Williamson", "Anne Mykland", "Kacper K. Migacz", "Yijun Wang", "Egemen Bostan", "Duy Thuc Nguyen", "Zhe He", "Marc L. Descoteaux", "Felix Yeung", "Shida Liu", "Jorge García Ponce", "Luke Zhu", "Yuyang Chen", "Ekaterina S. Ivshina", "Miguel Fernandez", "Minjae Kim", "Kennan Gumbs", "Matthew Scott Tan", "Russell Yang", "Mai Hoang", "David Brown", "Isabella A. Silveira", "Lavon Sykes", "Ahmed Roman", "William Fredenberg", "Yiming Chen", "Lucas Martin", "Yixing Tang", "Kelly Werker Smith", "Hongyu Liao", "Logan G. Wilson", "Alexander Dazhen Cai", "Andrea Elizabeth Biju", "Michael P. Brenner"], "title": "HARDMath2: A Benchmark for Applied Mathematics Built by Students as Part of a Graduate Class", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable progress in mathematical\nproblem-solving, but evaluation has largely focused on problems that have exact\nanalytical solutions or involve formal proofs, often overlooking\napproximation-based problems ubiquitous in applied science and engineering. To\nfill this gap, we build on prior work and present HARDMath2, a dataset of 211\noriginal problems covering the core topics in an introductory graduate applied\nmath class, including boundary-layer analysis, WKB methods, asymptotic\nsolutions of nonlinear partial differential equations, and the asymptotics of\noscillatory integrals. This dataset was designed and verified by the students\nand instructors of a core graduate applied mathematics course at Harvard. We\nbuild the dataset through a novel collaborative environment that challenges\nstudents to write and refine difficult problems consistent with the class\nsyllabus, peer-validate solutions, test different models, and automatically\ncheck LLM-generated solutions against their own answers and numerical ground\ntruths. Evaluation results show that leading frontier models still struggle\nwith many of the problems in the dataset, highlighting a gap in the\nmathematical reasoning skills of current LLMs. Importantly, students identified\nstrategies to create increasingly difficult problems by interacting with the\nmodels and exploiting common failure modes. This back-and-forth with the models\nnot only resulted in a richer and more challenging benchmark but also led to\nqualitative improvements in the students' understanding of the course material,\nwhich is increasingly important as we enter an age where state-of-the-art\nlanguage models can solve many challenging problems across a wide domain of\nfields."}
{"id": "2505.12601", "pdf": "https://arxiv.org/pdf/2505.12601", "abs": "https://arxiv.org/abs/2505.12601", "authors": ["Yang Li"], "title": "Rethinking Predictive Modeling for LLM Routing: When Simple kNN Beats Complex Learned Routers", "categories": ["cs.LG"], "comment": null, "summary": "As large language models (LLMs) grow in scale and specialization,\nrouting--selecting the best model for a given input--has become essential for\nefficient and effective deployment. While recent methods rely on complex\nlearned routing strategies, their dependence on disparate training data and\nevaluation setups makes comparison and generalization difficult. In this work,\nwe revisit LLM routing through the lens of simplicity. We show that a\nwell-tuned k-Nearest Neighbors (kNN) approach not only matches but often\noutperforms state-of-the-art learned routers across diverse tasks. To support\nsystematic evaluation, we introduce a suite of standardized routing benchmarks\nspanning instruction-following, question-answering, and reasoning tasks, as\nwell as the first multi-modal routing dataset involving visual inputs. Our\nfindings reveal that the locality properties of model performance in embedding\nspace enable simple non-parametric methods to achieve strong routing decisions\nwith lower sample complexity than parametric approaches. This challenges the\nprevailing trend toward sophisticated architectures and highlights the\nimportance of thoroughly evaluating simple baselines before investing in\ncomplex solutions. To support reproducibility and further exploration, we will\nrelease all benchmarks and code upon publication."}
{"id": "2505.13259", "pdf": "https://arxiv.org/pdf/2505.13259", "abs": "https://arxiv.org/abs/2505.13259", "authors": ["Tianshi Zheng", "Zheye Deng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Zihao Wang", "Yangqiu Song"], "title": "From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery", "categories": ["cs.CL"], "comment": "16 pages", "summary": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery."}
{"id": "2505.11776", "pdf": "https://arxiv.org/pdf/2505.11776", "abs": "https://arxiv.org/abs/2505.11776", "authors": ["Jiali Chen", "Avijit Mukherjee"], "title": "Generative and Contrastive Graph Representation Learning", "categories": ["cs.LG", "cs.AI", "I.2.4, I2.6"], "comment": "8 pages, 3 figures", "summary": "Self-supervised learning (SSL) on graphs generates node and graph\nrepresentations (i.e., embeddings) that can be used for downstream tasks such\nas node classification, node clustering, and link prediction. Graph SSL is\nparticularly useful in scenarios with limited or no labeled data. Existing SSL\nmethods predominantly follow contrastive or generative paradigms, each\nexcelling in different tasks: contrastive methods typically perform well on\nclassification tasks, while generative methods often excel in link prediction.\nIn this paper, we present a novel architecture for graph SSL that integrates\nthe strengths of both approaches. Our framework introduces community-aware\nnode-level contrastive learning, providing more robust and effective positive\nand negative node pairs generation, alongside graph-level contrastive learning\nto capture global semantic information. Additionally, we employ a comprehensive\naugmentation strategy that combines feature masking, node perturbation, and\nedge perturbation, enabling robust and diverse representation learning. By\nincorporating these enhancements, our model achieves superior performance\nacross multiple tasks, including node classification, clustering, and link\nprediction. Evaluations on open benchmark datasets demonstrate that our model\noutperforms state-of-the-art methods, achieving a performance lift of\n0.23%-2.01% depending on the task and dataset."}
{"id": "2505.12611", "pdf": "https://arxiv.org/pdf/2505.12611", "abs": "https://arxiv.org/abs/2505.12611", "authors": ["Grant C. Forbes", "Jianxun Wang", "Leonardo Villalobos-Arias", "Arnav Jhala", "David L. Roberts"], "title": "Action-Dependent Optimality-Preserving Reward Shaping", "categories": ["cs.LG", "I.2.6"], "comment": "Extended abstract at AAMAS 2025; full paper at ICML 2025", "summary": "Recent RL research has utilized reward shaping--particularly complex shaping\nrewards such as intrinsic motivation (IM)--to encourage agent exploration in\nsparse-reward environments. While often effective, ``reward hacking'' can lead\nto the shaping reward being optimized at the expense of the extrinsic reward,\nresulting in a suboptimal policy. Potential-Based Reward Shaping (PBRS)\ntechniques such as Generalized Reward Matching (GRM) and Policy-Invariant\nExplicit Shaping (PIES) have mitigated this. These methods allow for\nimplementing IM without altering optimal policies. In this work we show that\nthey are effectively unsuitable for complex, exploration-heavy environments\nwith long-duration episodes. To remedy this, we introduce Action-Dependent\nOptimality Preserving Shaping (ADOPS), a method of converting intrinsic rewards\nto an optimality-preserving form that allows agents to utilize IM more\neffectively in the extremely sparse environment of Montezuma's Revenge. We also\nprove ADOPS accommodates reward shaping functions that cannot be written in a\npotential-based form: while PBRS-based methods require the cumulative\ndiscounted intrinsic return be independent of actions, ADOPS allows for\nintrinsic cumulative returns to be dependent on agents' actions while still\npreserving the optimal policy set. We show how action-dependence enables\nADOPS's to preserve optimality while learning in complex, sparse-reward\nenvironments where other methods struggle."}
{"id": "2505.13268", "pdf": "https://arxiv.org/pdf/2505.13268", "abs": "https://arxiv.org/abs/2505.13268", "authors": ["Livia Qian", "Carol Figueroa", "Gabriel Skantze"], "title": "Representation of perceived prosodic similarity of conversational feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Interspeech 2025", "summary": "Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of\nspoken dialogue and is crucial to ensuring common ground in conversational\nsystems. The exact meaning of such feedback is conveyed through both lexical\nand prosodic form. In this work, we investigate the perceived prosodic\nsimilarity of vocal feedback with the same lexical form, and to what extent\nexisting speech representations reflect such similarities. A triadic comparison\ntask with recruited participants is used to measure perceived similarity of\nfeedback responses taken from two different datasets. We find that spectral and\nself-supervised speech representations encode prosody better than extracted\npitch features, especially in the case of feedback from the same speaker. We\nalso find that it is possible to further condense and align the representations\nto human perception through contrastive learning."}
{"id": "2505.11785", "pdf": "https://arxiv.org/pdf/2505.11785", "abs": "https://arxiv.org/abs/2505.11785", "authors": ["Gina Wong", "Drew Prinster", "Suchi Saria", "Rama Chellappa", "Anqi Liu"], "title": "Improving Coverage in Combined Prediction Sets with Weighted p-values", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Conformal prediction quantifies the uncertainty of machine learning models by\naugmenting point predictions with valid prediction sets, assuming\nexchangeability. For complex scenarios involving multiple trials, models, or\ndata sources, conformal prediction sets can be aggregated to create a\nprediction set that captures the overall uncertainty, often improving\nprecision. However, aggregating multiple prediction sets with individual\n$1-\\alpha$ coverage inevitably weakens the overall guarantee, typically\nresulting in $1-2\\alpha$ worst-case coverage. In this work, we propose a\nframework for the weighted aggregation of prediction sets, where weights are\nassigned to each prediction set based on their contribution. Our framework\noffers flexible control over how the sets are aggregated, achieving tighter\ncoverage bounds that interpolate between the $1-2\\alpha$ guarantee of the\ncombined models and the $1-\\alpha$ guarantee of an individual model depending\non the distribution of weights. We extend our framework to data-dependent\nweights, and we derive a general procedure for data-dependent weight\naggregation that maintains finite-sample validity. We demonstrate the\neffectiveness of our methods through experiments on synthetic and real data in\nthe mixture-of-experts setting, and we show that aggregation with\ndata-dependent weights provides a form of adaptive coverage."}
{"id": "2505.12614", "pdf": "https://arxiv.org/pdf/2505.12614", "abs": "https://arxiv.org/abs/2505.12614", "authors": ["Pengfei Ding", "Yan Wang", "Guanfeng Liu", "Jiajie Zhu"], "title": "Adaptive Graph Unlearning", "categories": ["cs.LG"], "comment": "This paper has been accepted by IJCAI 2025", "summary": "Graph unlearning, which deletes graph elements such as nodes and edges from\ntrained graph neural networks (GNNs), is crucial for real-world applications\nwhere graph data may contain outdated, inaccurate, or privacy-sensitive\ninformation. However, existing methods often suffer from (1) incomplete or over\nunlearning due to neglecting the distinct objectives of different unlearning\ntasks, and (2) inaccurate identification of neighbors affected by deleted\nelements across various GNN architectures. To address these limitations, we\npropose AGU, a novel Adaptive Graph Unlearning framework that flexibly adapts\nto diverse unlearning tasks and GNN architectures. AGU ensures the complete\nforgetting of deleted elements while preserving the integrity of the remaining\ngraph. It also accurately identifies affected neighbors for each GNN\narchitecture and prioritizes important ones to enhance unlearning performance.\nExtensive experiments on seven real-world graphs demonstrate that AGU\noutperforms existing methods in terms of effectiveness, efficiency, and\nunlearning capability."}
{"id": "2505.13271", "pdf": "https://arxiv.org/pdf/2505.13271", "abs": "https://arxiv.org/abs/2505.13271", "authors": ["Lei Sheng", "Shuai-Shuai Xu"], "title": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning", "categories": ["cs.CL"], "comment": "11 pages, 5 figures", "summary": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution\naccuracy, while the 7B model achieves 69.19%. The code will be open sourced at\nhttps://github.com/CycloneBoy/csc_sql."}
{"id": "2505.11793", "pdf": "https://arxiv.org/pdf/2505.11793", "abs": "https://arxiv.org/abs/2505.11793", "authors": ["Jianing Wang", "Siying Guo", "Zheng Hua", "Runhu Huang", "Jinyu Hu", "Maoguo Gong"], "title": "CL-CaGAN: Capsule differential adversarial continuous learning for cross-domain hyperspectral anomaly detection", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Anomaly detection (AD) has attracted remarkable attention in hyperspectral\nimage (HSI) processing fields, and most existing deep learning (DL)-based\nalgorithms indicate dramatic potential for detecting anomaly samples through\nspecific training process under current scenario. However, the limited prior\ninformation and the catastrophic forgetting problem indicate crucial challenges\nfor existing DL structure in open scenarios cross-domain detection. In order to\nimprove the detection performance, a novel continual learning-based capsule\ndifferential generative adversarial network (CL-CaGAN) is proposed to elevate\nthe cross-scenario learning performance for facilitating the real application\nof DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule\nstructure with adversarial learning network is constructed to estimate the\nbackground distribution for surmounting the deficiency of prior information. To\nmitigate the catastrophic forgetting phenomenon, clustering-based sample replay\nstrategy and a designed extra self-distillation regularization are integrated\nfor merging the history and future knowledge in continual AD task, while the\ndiscriminative learning ability from previous detection scenario to current\nscenario is retained by the elaborately designed structure with continual\nlearning (CL) strategy. In addition, the differentiable enhancement is enforced\nto augment the generation performance of the training data. This further\nstabilizes the training process with better convergence and efficiently\nconsolidates the reconstruction ability of background samples. To verify the\neffectiveness of our proposed CL-CaGAN, we conduct experiments on several real\nHSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher\ndetection performance and continuous learning capacity for mitigating the\ncatastrophic forgetting under cross-domain scenarios."}
{"id": "2505.12628", "pdf": "https://arxiv.org/pdf/2505.12628", "abs": "https://arxiv.org/abs/2505.12628", "authors": ["Wanfu Gao", "Zengyao Man", "Hanlin Pan", "Kunpeng Liu"], "title": "Dual-Agent Reinforcement Learning for Automated Feature Generation", "categories": ["cs.LG"], "comment": null, "summary": "Feature generation involves creating new features from raw data to capture\ncomplex relationships among the original features, improving model robustness\nand machine learning performance. Current methods using reinforcement learning\nfor feature generation have made feature exploration more flexible and\nefficient. However, several challenges remain: first, during feature expansion,\na large number of redundant features are generated. When removing them, current\nmethods only retain the best features each round, neglecting those that perform\npoorly initially but could improve later. Second, the state representation used\nby current methods fails to fully capture complex feature relationships. Third,\nthere are significant differences between discrete and continuous features in\ntabular data, requiring different operations for each type. To address these\nchallenges, we propose a novel dual-agent reinforcement learning method for\nfeature generation. Two agents are designed: the first generates new features,\nand the second determines whether they should be preserved. A self-attention\nmechanism enhances state representation, and diverse operations distinguish\ninteractions between discrete and continuous features. The experimental results\non multiple datasets demonstrate that the proposed method is effective. The\ncode is available at https://github.com/extess0/DARL."}
{"id": "2505.13282", "pdf": "https://arxiv.org/pdf/2505.13282", "abs": "https://arxiv.org/abs/2505.13282", "authors": ["Sahil Mishra", "Kumar Arjun", "Tanmoy Chakraborty"], "title": "$\\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion", "categories": ["cs.CL"], "comment": null, "summary": "Taxonomies are hierarchical knowledge graphs crucial for recommendation\nsystems, and web applications. As data grows, expanding taxonomies is\nessential, but existing methods face key challenges: (1) discriminative models\nstruggle with representation limits and generalization, while (2) generative\nmethods either process all candidates at once, introducing noise and exceeding\ncontext limits, or discard relevant entities by selecting noisy candidates. We\npropose LORex ($\\textbf{L}$ineage-$\\textbf{O}$riented $\\textbf{Re}$asoning for\nTaxonomy E$\\textbf{x}$pansion), a plug-and-play framework that combines\ndiscriminative ranking and generative reasoning for efficient taxonomy\nexpansion. Unlike prior methods, LORex ranks and chunks candidate terms into\nbatches, filtering noise and iteratively refining selections by reasoning\ncandidates' hierarchy to ensure contextual efficiency. Extensive experiments\nacross four benchmarks and twelve baselines show that LORex improves accuracy\nby 12% and Wu & Palmer similarity by 5% over state-of-the-art methods."}
{"id": "2505.11802", "pdf": "https://arxiv.org/pdf/2505.11802", "abs": "https://arxiv.org/abs/2505.11802", "authors": ["Chuang Zhao", "Hui Tang", "Hongke Zhao", "Xiaomeng Li"], "title": "Diffmv: A Unified Diffusion Framework for Healthcare Predictions with Random Missing Views and View Laziness", "categories": ["cs.LG", "cs.AI"], "comment": "SIGKDD2025, accepted", "summary": "Advanced healthcare predictions offer significant improvements in patient\noutcomes by leveraging predictive analytics. Existing works primarily utilize\nvarious views of Electronic Health Record (EHR) data, such as diagnoses, lab\ntests, or clinical notes, for model training. These methods typically assume\nthe availability of complete EHR views and that the designed model could fully\nleverage the potential of each view. However, in practice, random missing views\nand view laziness present two significant challenges that hinder further\nimprovements in multi-view utilization. To address these challenges, we\nintroduce Diffmv, an innovative diffusion-based generative framework designed\nto advance the exploitation of multiple views of EHR data. Specifically, to\naddress random missing views, we integrate various views of EHR data into a\nunified diffusion-denoising framework, enriched with diverse contextual\nconditions to facilitate progressive alignment and view transformation. To\nmitigate view laziness, we propose a novel reweighting strategy that assesses\nthe relative advantages of each view, promoting a balanced utilization of\nvarious data views within the model. Our proposed strategy achieves superior\nperformance across multiple health prediction tasks derived from three popular\ndatasets, including multi-view and multi-modality scenarios."}
{"id": "2505.12629", "pdf": "https://arxiv.org/pdf/2505.12629", "abs": "https://arxiv.org/abs/2505.12629", "authors": ["Yuchang Sun", "Yanxi Chen", "Yaliang Li", "Bolin Ding"], "title": "Enhancing Latent Computation in Transformers with Latent Tokens", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Augmenting large language models (LLMs) with auxiliary tokens has emerged as\na promising strategy for enhancing model performance. In this work, we\nintroduce a lightweight method termed latent tokens; these are dummy tokens\nthat may be non-interpretable in natural language but steer the autoregressive\ndecoding process of a Transformer-based LLM via the attention mechanism. The\nproposed latent tokens can be seamlessly integrated with a pre-trained\nTransformer, trained in a parameter-efficient manner, and applied flexibly at\ninference time, while adding minimal complexity overhead to the existing\ninfrastructure of standard Transformers. We propose several hypotheses about\nthe underlying mechanisms of latent tokens and design synthetic tasks\naccordingly to verify them. Numerical results confirm that the proposed method\nnoticeably outperforms the baselines, particularly in the out-of-distribution\ngeneralization scenarios, highlighting its potential in improving the\nadaptability of LLMs."}
{"id": "2505.13302", "pdf": "https://arxiv.org/pdf/2505.13302", "abs": "https://arxiv.org/abs/2505.13302", "authors": ["Alice Plebe", "Timothy Douglas", "Diana Riazi", "R. Maria del Rio-Chanona"], "title": "I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models are increasingly integrated into news recommendation\nsystems, raising concerns about their role in spreading misinformation. In\nhumans, visual content is known to boost credibility and shareability of\ninformation, yet its effect on vision-language models (VLMs) remains unclear.\nWe present the first study examining how images influence VLMs' propensity to\nreshare news content, whether this effect varies across model families, and how\npersona conditioning and content attributes modulate this behavior. To support\nthis analysis, we introduce two methodological contributions: a\njailbreaking-inspired prompting strategy that elicits resharing decisions from\nVLMs while simulating users with antisocial traits and political alignments;\nand a multimodal dataset of fact-checked political news from PolitiFact, paired\nwith corresponding images and ground-truth veracity labels. Experiments across\nmodel families reveal that image presence increases resharing rates by 4.8% for\ntrue news and 15.0% for false news. Persona conditioning further modulates this\neffect: Dark Triad traits amplify resharing of false news, whereas\nRepublican-aligned profiles exhibit reduced veracity sensitivity. Of all the\ntested models, only Claude-3-Haiku demonstrates robustness to visual\nmisinformation. These findings highlight emerging risks in multimodal model\nbehavior and motivate the development of tailored evaluation frameworks and\nmitigation strategies for personalized AI systems. Code and dataset are\navailable at: https://github.com/3lis/misinfo_vlm"}
{"id": "2505.11804", "pdf": "https://arxiv.org/pdf/2505.11804", "abs": "https://arxiv.org/abs/2505.11804", "authors": ["Xi Wang", "Eric Nalisnick"], "title": "Are vision language models robust to uncertain inputs?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Robustness against uncertain and ambiguous inputs is a critical challenge for\ndeep learning models. While recent advancements in large scale vision language\nmodels (VLMs, e.g. GPT4o) might suggest that increasing model and training\ndataset size would mitigate this issue, our empirical evaluation shows a more\ncomplicated picture. Testing models using two classic uncertainty\nquantification tasks, anomaly detection and classification under inherently\nambiguous conditions, we find that newer and larger VLMs indeed exhibit\nimproved robustness compared to earlier models, but still suffer from a\ntendency to strictly follow instructions, often causing them to hallucinate\nconfident responses even when faced with unclear or anomalous inputs.\nRemarkably, for natural images such as ImageNet, this limitation can be\novercome without pipeline modifications: simply prompting models to abstain\nfrom uncertain predictions enables significant reliability gains, achieving\nnear-perfect robustness in several settings. However, for domain-specific tasks\nsuch as galaxy morphology classification, a lack of specialized knowledge\nprevents reliable uncertainty estimation. Finally, we propose a novel mechanism\nbased on caption diversity to reveal a model's internal uncertainty, enabling\npractitioners to predict when models will successfully abstain without relying\non labeled data."}
{"id": "2505.12642", "pdf": "https://arxiv.org/pdf/2505.12642", "abs": "https://arxiv.org/abs/2505.12642", "authors": ["Jung Hoon Lee", "Sujith Vijayan"], "title": "Two out of Three (ToT): using self-consistency to make robust predictions", "categories": ["cs.LG", "cs.CV"], "comment": "12 pages, 7 main figures, 1 supplementary table and 2 supplementary\n  figures", "summary": "Deep learning (DL) can automatically construct intelligent agents, deep\nneural networks (alternatively, DL models), that can outperform humans in\ncertain tasks. However, the operating principles of DL remain poorly\nunderstood, making its decisions incomprehensible. As a result, it poses a\ngreat risk to deploy DL in high-stakes domains in which mistakes or errors may\nlead to critical consequences. Here, we aim to develop an algorithm that can\nhelp DL models make more robust decisions by allowing them to abstain from\nanswering when they are uncertain. Our algorithm, named `Two out of Three\n(ToT)', is inspired by the sensitivity of the human brain to conflicting\ninformation. ToT creates two alternative predictions in addition to the\noriginal model prediction and uses the alternative predictions to decide\nwhether it should provide an answer or not."}
{"id": "2505.13307", "pdf": "https://arxiv.org/pdf/2505.13307", "abs": "https://arxiv.org/abs/2505.13307", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Yue Liao", "Jiaqi Wang", "Jingxuan Zhou", "Wanxiang Che"], "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Manuscript", "summary": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary."}
{"id": "2505.11807", "pdf": "https://arxiv.org/pdf/2505.11807", "abs": "https://arxiv.org/abs/2505.11807", "authors": ["Yufei Xiang", "Yiqun Shen", "Yeqin Zhang", "Cam-Tu Nguyen"], "title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "Large Language Models (LLMs) possess extensive knowledge and commonsense\nreasoning capabilities, making them valuable for creating powerful agents.\nHowever, existing LLM agent frameworks have not fully utilized past experiences\nfor improvement. This work introduces a new LLM-based agent framework called\nRetrospex, which addresses this challenge by analyzing past experiences in\ndepth. Unlike previous approaches, Retrospex does not directly integrate\nexperiences into the LLM's context. Instead, it combines the LLM's action\nlikelihood with action values estimated by a Reinforcement Learning (RL)\nCritic, which is trained on past experiences through an offline\n''retrospection'' process. Additionally, Retrospex employs a dynamic action\nrescoring mechanism that increases the importance of experience-based values\nfor tasks that require more interaction with the environment. We evaluate\nRetrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its\nadvantages over strong, contemporary baselines."}
{"id": "2505.12647", "pdf": "https://arxiv.org/pdf/2505.12647", "abs": "https://arxiv.org/abs/2505.12647", "authors": ["Jung Hoon Lee", "Sujith Vijayan"], "title": "Spiking Neural Network: a low power solution for physical layer authentication", "categories": ["cs.LG"], "comment": "11 pages, 7 figures and 2 pages", "summary": "Deep learning (DL) is a powerful tool that can solve complex problems, and\nthus, it seems natural to assume that DL can be used to enhance the security of\nwireless communication. However, deploying DL models to edge devices in\nwireless networks is challenging, as they require significant amounts of\ncomputing and power resources. Notably, Spiking Neural Networks (SNNs) are\nknown to be efficient in terms of power consumption, meaning they can be an\nalternative platform for DL models for edge devices. In this study, we ask if\nSNNs can be used in physical layer authentication. Our evaluation suggests that\nSNNs can learn unique physical properties (i.e., `fingerprints') of RF\ntransmitters and use them to identify individual devices. Furthermore, we find\nthat SNNs are also vulnerable to adversarial attacks and that an autoencoder\ncan be used clean out adversarial perturbations to harden SNNs against them."}
{"id": "2505.13312", "pdf": "https://arxiv.org/pdf/2505.13312", "abs": "https://arxiv.org/abs/2505.13312", "authors": ["Zhijie Deng", "Chris Yuhao Liu", "Zirui Pang", "Xinlei He", "Lei Feng", "Qi Xuan", "Zhaowei Zhu", "Jiaheng Wei"], "title": "GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmemorizing vast amounts of knowledge across diverse domains. However, the\nability to selectively forget specific knowledge is critical for ensuring the\nsafety and compliance of deployed models. Existing unlearning efforts typically\nfine-tune the model with resources such as forget data, retain data, and a\ncalibration model. These additional gradient steps blur the decision boundary\nbetween forget and retain knowledge, making unlearning often at the expense of\noverall performance. To avoid the negative impact of fine-tuning, it would be\nbetter to unlearn solely at inference time by safely guarding the model against\ngenerating responses related to the forget target, without destroying the\nfluency of text generation. In this work, we propose Generation-time Unlearning\nvia Adaptive Restriction and Detection (GUARD), a framework that enables\ndynamic unlearning during LLM generation. Specifically, we first employ a\nprompt classifier to detect unlearning targets and extract the corresponding\nforbidden token. We then dynamically penalize and filter candidate tokens\nduring generation using a combination of token matching and semantic matching,\neffectively preventing the model from leaking the forgotten content.\nExperimental results on copyright content unlearning tasks over the Harry\nPotter dataset and the MUSE benchmark, as well as entity unlearning tasks on\nthe TOFU dataset, demonstrate that GUARD achieves strong forget quality across\nvarious tasks while causing almost no degradation to the LLM's general\ncapabilities, striking an excellent trade-off between forgetting and utility."}
{"id": "2505.11813", "pdf": "https://arxiv.org/pdf/2505.11813", "abs": "https://arxiv.org/abs/2505.11813", "authors": ["Yixuan Dong", "Fang-Yi Su", "Jung-Hsien Chiang"], "title": "SGD-Mix: Enhancing Domain-Specific Image Classification with Label-Preserving Data Augmentation", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 6 figures, 6 tables", "summary": "Data augmentation for domain-specific image classification tasks often\nstruggles to simultaneously address diversity, faithfulness, and label clarity\nof generated data, leading to suboptimal performance in downstream tasks. While\nexisting generative diffusion model-based methods aim to enhance augmentation,\nthey fail to cohesively tackle these three critical aspects and often overlook\nintrinsic challenges of diffusion models, such as sensitivity to model\ncharacteristics and stochasticity under strong transformations. In this paper,\nwe propose a novel framework that explicitly integrates diversity,\nfaithfulness, and label clarity into the augmentation process. Our approach\nemploys saliency-guided mixing and a fine-tuned diffusion model to preserve\nforeground semantics, enrich background diversity, and ensure label\nconsistency, while mitigating diffusion model limitations. Extensive\nexperiments across fine-grained, long-tail, few-shot, and background robustness\ntasks demonstrate our method's superior performance over state-of-the-art\napproaches."}
{"id": "2505.12672", "pdf": "https://arxiv.org/pdf/2505.12672", "abs": "https://arxiv.org/abs/2505.12672", "authors": ["Tonglong Wei", "Yan Lin", "Zeyu Zhou", "Haomin Wen", "Jilin Hu", "Shengnan Guo", "Youfang Lin", "Gao Cong", "Huaiyu Wan"], "title": "TransferTraj: A Vehicle Trajectory Learning Model for Region and Task Transferability", "categories": ["cs.LG"], "comment": null, "summary": "Vehicle GPS trajectories provide valuable movement information that supports\nvarious downstream tasks and applications. A desirable trajectory learning\nmodel should be able to transfer across regions and tasks without retraining,\navoiding the need to maintain multiple specialized models and subpar\nperformance with limited training data. However, each region has its unique\nspatial features and contexts, which are reflected in vehicle movement patterns\nand difficult to generalize. Additionally, transferring across different tasks\nfaces technical challenges due to the varying input-output structures required\nfor each task. Existing efforts towards transferability primarily involve\nlearning embedding vectors for trajectories, which perform poorly in region\ntransfer and require retraining of prediction modules for task transfer.\n  To address these challenges, we propose TransferTraj, a vehicle GPS\ntrajectory learning model that excels in both region and task transferability.\nFor region transferability, we introduce RTTE as the main learnable module\nwithin TransferTraj. It integrates spatial, temporal, POI, and road network\nmodalities of trajectories to effectively manage variations in spatial context\ndistribution across regions. It also introduces a TRIE module for incorporating\nrelative information of spatial features and a spatial context MoE module for\nhandling movement patterns in diverse contexts. For task transferability, we\npropose a task-transferable input-output scheme that unifies the input-output\nstructure of different tasks into the masking and recovery of modalities and\ntrajectory points. This approach allows TransferTraj to be pre-trained once and\ntransferred to different tasks without retraining. Extensive experiments on\nthree real-world vehicle trajectory datasets under task transfer, zero-shot,\nand few-shot region transfer, validating TransferTraj's effectiveness."}
{"id": "2505.13328", "pdf": "https://arxiv.org/pdf/2505.13328", "abs": "https://arxiv.org/abs/2505.13328", "authors": ["Hongru Wang", "Wenyu Huang", "Yufei Wang", "Yuanhao Xi", "Jianqiao Lu", "Huan Zhang", "Nan Hu", "Zeming Liu", "Jeff Z. Pan", "Kam-Fai Wong"], "title": "Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges", "categories": ["cs.CL"], "comment": null, "summary": "Existing benchmarks that assess Language Models (LMs) as Language Agents\n(LAs) for tool use primarily focus on stateless, single-turn interactions or\npartial evaluations, such as tool selection in a single turn, overlooking the\ninherent stateful nature of interactions in multi-turn applications. To fulfill\nthis gap, we propose \\texttt{DialogTool}, a multi-turn dialogue dataset with\nstateful tool interactions considering the whole life cycle of tool use, across\nsix key tasks in three stages: 1) \\textit{tool creation}; 2) \\textit{tool\nutilization}: tool awareness, tool selection, tool execution; and 3)\n\\textit{role-consistent response}: response generation and role play.\nFurthermore, we build \\texttt{VirtualMobile} -- an embodied virtual mobile\nevaluation environment to simulate API calls and assess the robustness of the\ncreated APIs\\footnote{We will use tools and APIs alternatively, there are no\nsignificant differences between them in this paper.}. Taking advantage of these\nartifacts, we conduct comprehensive evaluation on 13 distinct open- and\nclosed-source LLMs and provide detailed analysis at each stage, revealing that\nthe existing state-of-the-art LLMs still cannot perform well to use tools over\nlong horizons."}
{"id": "2505.11824", "pdf": "https://arxiv.org/pdf/2505.11824", "abs": "https://arxiv.org/abs/2505.11824", "authors": ["Minsu Kim", "Jean-Pierre Falet", "Oliver E. Richardson", "Xiaoyin Chen", "Moksh Jain", "Sungjin Ahn", "Sungsoo Ahn", "Yoshua Bengio"], "title": "Search-Based Correction of Reasoning Chains for Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Chain-of-Thought (CoT) reasoning has advanced the capabilities and\ntransparency of language models (LMs); however, reasoning chains can contain\ninaccurate statements that reduce performance and trustworthiness. To address\nthis, we introduce a new self-correction framework that augments each reasoning\nstep in a CoT with a latent variable indicating its veracity, enabling modeling\nof all possible truth assignments rather than assuming correctness throughout.\nTo efficiently explore this expanded space, we introduce Search Corrector, a\ndiscrete search algorithm over boolean-valued veracity assignments. It\nefficiently performs otherwise intractable inference in the posterior\ndistribution over veracity assignments by leveraging the LM's joint likelihood\nover veracity and the final answer as a proxy reward. This efficient\ninference-time correction method facilitates supervised fine-tuning of an\nAmortized Corrector by providing pseudo-labels for veracity. The Amortized\nCorrector generalizes self-correction, enabling accurate zero-shot veracity\ninference in novel contexts. Empirical results demonstrate that Search\nCorrector reliably identifies errors in logical (ProntoQA) and mathematical\nreasoning (GSM8K) benchmarks. The Amortized Corrector achieves comparable\nzero-shot accuracy and improves final answer accuracy by up to 25%."}
{"id": "2505.12681", "pdf": "https://arxiv.org/pdf/2505.12681", "abs": "https://arxiv.org/abs/2505.12681", "authors": ["Hana Satou", "Alan Mitkiy"], "title": "On the Mechanisms of Adversarial Data Augmentation for Robust and Adaptive Transfer Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Transfer learning across domains with distribution shift remains a\nfundamental challenge in building robust and adaptable machine learning\nsystems. While adversarial perturbations are traditionally viewed as threats\nthat expose model vulnerabilities, recent studies suggest that they can also\nserve as constructive tools for data augmentation. In this work, we\nsystematically investigate the role of adversarial data augmentation (ADA) in\nenhancing both robustness and adaptivity in transfer learning settings. We\nanalyze how adversarial examples, when used strategically during training,\nimprove domain generalization by enriching decision boundaries and reducing\noverfitting to source-domain-specific features. We further propose a unified\nframework that integrates ADA with consistency regularization and\ndomain-invariant representation learning. Extensive experiments across multiple\nbenchmark datasets -- including VisDA, DomainNet, and Office-Home --\ndemonstrate that our method consistently improves target-domain performance\nunder both unsupervised and few-shot domain adaptation settings. Our results\nhighlight a constructive perspective of adversarial learning, transforming\nperturbation from a destructive attack into a regularizing force for\ncross-domain transferability."}
{"id": "2505.13338", "pdf": "https://arxiv.org/pdf/2505.13338", "abs": "https://arxiv.org/abs/2505.13338", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities."}
{"id": "2505.11825", "pdf": "https://arxiv.org/pdf/2505.11825", "abs": "https://arxiv.org/abs/2505.11825", "authors": ["Xudong Ma"], "title": "Bootstrapping Diffusion: Diffusion Model Training Leveraging Partial and Corrupted Data", "categories": ["cs.CV", "cs.AI"], "comment": "21 pages, 1 figure", "summary": "Training diffusion models requires large datasets. However, acquiring large\nvolumes of high-quality data can be challenging, for example, collecting large\nnumbers of high-resolution images and long videos. On the other hand, there are\nmany complementary data that are usually considered corrupted or partial, such\nas low-resolution images and short videos. Other examples of corrupted data\ninclude videos that contain subtitles, watermarks, and logos. In this study, we\ninvestigate the theoretical problem of whether the above partial data can be\nutilized to train conventional diffusion models. Motivated by our theoretical\nanalysis in this study, we propose a straightforward approach of training\ndiffusion models utilizing partial data views, where we consider each form of\ncomplementary data as a view of conventional data. Our proposed approach first\ntrains one separate diffusion model for each individual view, and then trains a\nmodel for predicting the residual score function. We prove generalization error\nbounds, which show that the proposed diffusion model training approach can\nachieve lower generalization errors if proper regularizations are adopted in\nthe residual score function training. In particular, we prove that the\ndifficulty in training the residual score function scales proportionally with\nthe signal correlations not captured by partial data views. Consequently, the\nproposed approach achieves near first-order optimal data efficiency."}
{"id": "2505.12682", "pdf": "https://arxiv.org/pdf/2505.12682", "abs": "https://arxiv.org/abs/2505.12682", "authors": ["Yun-Yun Tsai", "Chuan Guo", "Junfeng Yang", "Laurens van der Maaten"], "title": "RoFL: Robust Fingerprinting of Language Models", "categories": ["cs.LG"], "comment": null, "summary": "AI developers are releasing large language models (LLMs) under a variety of\ndifferent licenses. Many of these licenses restrict the ways in which the\nmodels or their outputs may be used. This raises the question how license\nviolations may be recognized. In particular, how can we identify that an API or\nproduct uses (an adapted version of) a particular LLM? We present a new method\nthat enable model developers to perform such identification via fingerprints:\nstatistical patterns that are unique to the developer's model and robust to\ncommon alterations of that model. Our method permits model identification in a\nblack-box setting using a limited number of queries, enabling identification of\nmodels that can only be accessed via an API or product. The fingerprints are\nnon-invasive: our method does not require any changes to the model during\ntraining, hence by design, it does not impact model quality. Empirically, we\nfind our method provides a high degree of robustness to common changes in the\nmodel or inference settings. In our experiments, it substantially outperforms\nprior art, including invasive methods that explicitly train watermarks into the\nmodel."}
{"id": "2505.13346", "pdf": "https://arxiv.org/pdf/2505.13346", "abs": "https://arxiv.org/abs/2505.13346", "authors": ["Austin Xu", "Yilun Zhou", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark", "summary": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench."}
{"id": "2505.11827", "pdf": "https://arxiv.org/pdf/2505.11827", "abs": "https://arxiv.org/abs/2505.11827", "authors": ["Yansong Ning", "Wei Li", "Jun Fang", "Naiqiang Tan", "Hao Liu"], "title": "Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "In progress", "summary": "Compressing long chain-of-thought (CoT) from large language models (LLMs) is\nan emerging strategy to improve the reasoning efficiency of LLMs. Despite its\npromising benefits, existing studies equally compress all thoughts within a\nlong CoT, hindering more concise and effective reasoning. To this end, we first\ninvestigate the importance of different thoughts by examining their\neffectiveness and efficiency in contributing to reasoning through automatic\nlong CoT chunking and Monte Carlo rollouts. Building upon the insights, we\npropose a theoretically bounded metric to jointly measure the effectiveness and\nefficiency of different thoughts. We then propose Long$\\otimes$Short, an\nefficient reasoning framework that enables two LLMs to collaboratively solve\nthe problem: a long-thought LLM for more effectively generating important\nthoughts, while a short-thought LLM for efficiently generating remaining\nthoughts. Specifically, we begin by synthesizing a small amount of cold-start\ndata to fine-tune LLMs for long-thought and short-thought reasoning styles,\nrespectively. Furthermore, we propose a synergizing-oriented multi-turn\nreinforcement learning, focusing on the model self-evolution and collaboration\nbetween long-thought and short-thought LLMs. Experimental results show that our\nmethod enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance\ncompared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while\nreducing token length by over 80% across the MATH500, AIME24/25, AMC23, and\nGPQA Diamond benchmarks. Our data and code are available at\nhttps://github.com/yasNing/Long-otimes-Short/."}
{"id": "2505.12683", "pdf": "https://arxiv.org/pdf/2505.12683", "abs": "https://arxiv.org/abs/2505.12683", "authors": ["Yihong Huang", "Chen Chu"], "title": "DimGrow: Memory-Efficient Field-level Embedding Dimension Search", "categories": ["cs.LG"], "comment": null, "summary": "Key feature fields need bigger embedding dimensionality, others need smaller.\nThis demands automated dimension allocation. Existing approaches, such as\npruning or Neural Architecture Search (NAS), require training a\nmemory-intensive SuperNet that enumerates all possible dimension combinations,\nwhich is infeasible for large feature spaces. We propose DimGrow, a lightweight\napproach that eliminates the SuperNet requirement. Starting training model from\none dimension per feature field, DimGrow can progressively expand/shrink\ndimensions via importance scoring. Dimensions grow only when their importance\nconsistently exceed a threshold, ensuring memory efficiency. Experiments on\nthree recommendation datasets verify the effectiveness of DimGrow while it\nreduces training memory compared to SuperNet-based methods."}
{"id": "2505.13348", "pdf": "https://arxiv.org/pdf/2505.13348", "abs": "https://arxiv.org/abs/2505.13348", "authors": ["Narek Maloyan", "Bislan Ashinov", "Dmitry Namiot"], "title": "Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly employed as evaluators\n(LLM-as-a-Judge) for assessing the quality of machine-generated text. This\nparadigm offers scalability and cost-effectiveness compared to human\nannotation. However, the reliability and security of such systems, particularly\ntheir robustness against adversarial manipulations, remain critical concerns.\nThis paper investigates the vulnerability of LLM-as-a-Judge architectures to\nprompt-injection attacks, where malicious inputs are designed to compromise the\njudge's decision-making process. We formalize two primary attack strategies:\nComparative Undermining Attack (CUA), which directly targets the final decision\noutput, and Justification Manipulation Attack (JMA), which aims to alter the\nmodel's generated reasoning. Using the Greedy Coordinate Gradient (GCG)\noptimization method, we craft adversarial suffixes appended to one of the\nresponses being compared. Experiments conducted on the MT-Bench Human Judgments\ndataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and\nFalcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves\nan Attack Success Rate (ASR) exceeding 30\\%, while JMA also shows notable\neffectiveness. These findings highlight substantial vulnerabilities in current\nLLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and\nfurther research into adversarial evaluation and trustworthiness in LLM-based\nassessment frameworks."}
{"id": "2505.11830", "pdf": "https://arxiv.org/pdf/2505.11830", "abs": "https://arxiv.org/abs/2505.11830", "authors": ["Hongbo Jin", "Ruyang Liu", "Wenhao Zhang", "Guibo Luo", "Ge Li"], "title": "CoT-Vid: Dynamic Chain-of-Thought Routing with Self Verification for Training-Free Video Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 7 figures", "summary": "System2 reasoning is developing rapidly these days with the emergence of\nDeep- Thinking Models and chain-of-thought technology, which has become a\ncentralized discussion point in the AI community. However, there is a relative\ngap in the research on complex video reasoning at present. In this work, we\npropose CoT-Vid, a novel training-free paradigm for the video domain with a\nmultistage complex reasoning design. Distinguishing from existing video LLMs,\nwhich rely heavily on perceptual abilities, it achieved surprising performance\ngain with explicit reasoning mechanism. The paradigm consists of three main\ncomponents: dynamic inference path routing, problem decoupling strategy, and\nvideo self-consistency verification. In addition, we propose a new standard for\ncategorization of video questions. CoT- Vid showed outstanding results on a\nwide range of benchmarks, and outperforms its base model by 9.3% on Egochema\nand 5.6% on VideoEspresso, rivalling or even surpassing larger and proprietary\nmodels, such as GPT-4V, GPT-4o and Gemini-1.5-flash. Our codebase will be\npublicly available soon."}
{"id": "2505.12684", "pdf": "https://arxiv.org/pdf/2505.12684", "abs": "https://arxiv.org/abs/2505.12684", "authors": ["Yinlin Zhu", "Xunkai Li", "Jishuo Jia", "Miao Hu", "Di Wu", "Meikang Qiu"], "title": "Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.SI"], "comment": "Under Review", "summary": "Recent advances in graph machine learning have shifted to data-centric\nparadigms, driven by two emerging fields: (1) Federated graph learning (FGL)\nenables multi-client collaboration but faces challenges from data and task\nheterogeneity, limiting its practicality; (2) Graph foundation models (GFM)\noffer strong domain generalization but are usually trained on single machines,\nmissing out on cross-silo data and resources.\n  These paradigms are complementary, and their integration brings notable\nbenefits. Motivated by this, we propose FedGFM, a novel decentralized GFM\ntraining paradigm. However, a key challenge is knowledge entanglement, where\nmulti-domain knowledge merges into indistinguishable representations, hindering\ndownstream adaptation.\n  To address this, we present FedGFM+, an enhanced framework with two core\nmodules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based\ndomain-aware initialization strategy. Before pre-training, each client encodes\nits local graph into domain-specific prototypes that serve as semantic anchors.\nSynthetic embeddings around these anchors initialize the global model. We\ntheoretically prove these prototypes are distinguishable across domains,\nproviding a strong inductive bias to disentangle domain-specific knowledge. (2)\nAdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a\nlightweight graph prompt capturing domain semantics during pre-training. During\nfine-tuning, prompts from all clients form a pool from which the GFM selects\nrelevant prompts to augment target graph attributes, improving downstream\nadaptation.\n  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and\ntasks, outperforming 20 baselines from supervised learning, FGL, and federated\nGFM variants."}
{"id": "2505.13353", "pdf": "https://arxiv.org/pdf/2505.13353", "abs": "https://arxiv.org/abs/2505.13353", "authors": ["Adam Štorek", "Mukur Gupta", "Samira Hajizadeh", "Prashast Srivastava", "Suman Jana"], "title": "Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning", "categories": ["cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Although modern Large Language Models (LLMs) support extremely large\ncontexts, their effectiveness in utilizing long context for code reasoning\nremains unclear. This paper investigates LLM reasoning ability over code\nsnippets within large repositories and how it relates to their recall ability.\nSpecifically, we differentiate between lexical code recall (verbatim retrieval)\nand semantic code recall (remembering what the code does). To measure semantic\nrecall, we propose SemTrace, a code reasoning technique where the impact of\nspecific statements on output is attributable and unpredictable. We also\npresent a method to quantify semantic recall sensitivity in existing\nbenchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop\nin code reasoning accuracy as a code snippet approaches the middle of the input\ncontext, particularly with techniques requiring high semantic recall like\nSemTrace. Moreover, we find that lexical recall varies by granularity, with\nmodels excelling at function retrieval but struggling with line-by-line recall.\nNotably, a disconnect exists between lexical and semantic recall, suggesting\ndifferent underlying mechanisms. Finally, our findings indicate that current\ncode reasoning benchmarks may exhibit low semantic recall sensitivity,\npotentially underestimating LLM challenges in leveraging in-context\ninformation."}
{"id": "2505.11835", "pdf": "https://arxiv.org/pdf/2505.11835", "abs": "https://arxiv.org/abs/2505.11835", "authors": ["Hongliang Li", "Jinan Xu", "Gengping Cui", "Changhao Guan", "Fengran Mo", "Kaiyu Huang"], "title": "Multilingual Collaborative Defense for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 4figures", "summary": "The robustness and security of large language models (LLMs) has become a\nprominent research area. One notable vulnerability is the ability to bypass LLM\nsafeguards by translating harmful queries into rare or underrepresented\nlanguages, a simple yet effective method of \"jailbreaking\" these models.\nDespite the growing concern, there has been limited research addressing the\nsafeguarding of LLMs in multilingual scenarios, highlighting an urgent need to\nenhance multilingual safety. In this work, we investigate the correlation\nbetween various attack features across different languages and propose\nMultilingual Collaborative Defense (MCD), a novel learning method that\noptimizes a continuous, soft safety prompt automatically to facilitate\nmultilingual safeguarding of LLMs. The MCD approach offers three advantages:\nFirst, it effectively improves safeguarding performance across multiple\nlanguages. Second, MCD maintains strong generalization capabilities while\nminimizing false refusal rates. Third, MCD mitigates the language safety\nmisalignment caused by imbalances in LLM training corpora. To evaluate the\neffectiveness of MCD, we manually construct multilingual versions of commonly\nused jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess\nvarious safeguarding methods. Additionally, we introduce these datasets in\nunderrepresented (zero-shot) languages to verify the language transferability\nof MCD. The results demonstrate that MCD outperforms existing approaches in\nsafeguarding against multilingual jailbreak attempts while also exhibiting\nstrong language transfer capabilities. Our code is available at\nhttps://github.com/HLiang-Lee/MCD."}
{"id": "2505.12686", "pdf": "https://arxiv.org/pdf/2505.12686", "abs": "https://arxiv.org/abs/2505.12686", "authors": ["Seungmin Kim", "Sohee Park", "Donghyun Kim", "Jisu Lee", "Daeseon Choi"], "title": "RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with Embedding-Level Perturbations", "categories": ["cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "With the advancement of AI-based speech synthesis technologies such as Deep\nVoice, there is an increasing risk of voice spoofing attacks, including voice\nphishing and fake news, through unauthorized use of others' voices. Existing\ndefenses that inject adversarial perturbations directly into audio signals have\nlimited effectiveness, as these perturbations can easily be neutralized by\nspeech enhancement methods. To overcome this limitation, we propose RoVo\n(Robust Voice), a novel proactive defense technique that injects adversarial\nperturbations into high-dimensional embedding vectors of audio signals,\nreconstructing them into protected speech. This approach effectively defends\nagainst speech synthesis attacks and also provides strong resistance to speech\nenhancement models, which represent a secondary attack threat.\n  In extensive experiments, RoVo increased the Defense Success Rate (DSR) by\nover 70% compared to unprotected speech, across four state-of-the-art speech\nsynthesis models. Specifically, RoVo achieved a DSR of 99.5% on a commercial\nspeaker-verification API, effectively neutralizing speech synthesis attack.\nMoreover, RoVo's perturbations remained robust even under strong speech\nenhancement conditions, outperforming traditional methods. A user study\nconfirmed that RoVo preserves both naturalness and usability of protected\nspeech, highlighting its effectiveness in complex and evolving threat\nscenarios."}
{"id": "2505.13360", "pdf": "https://arxiv.org/pdf/2505.13360", "abs": "https://arxiv.org/abs/2505.13360", "authors": ["Chenyang Yang", "Yike Shi", "Qianou Ma", "Michael Xieyang Liu", "Christian Kästner", "Tongshuang Wu"], "title": "What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts", "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Building LLM-powered software requires developers to communicate their\nrequirements through natural language, but developer prompts are frequently\nunderspecified, failing to fully capture many user-important requirements. In\nthis paper, we present an in-depth analysis of prompt underspecification,\nshowing that while LLMs can often (41.1%) guess unspecified requirements by\ndefault, such behavior is less robust: Underspecified prompts are 2x more\nlikely to regress over model or prompt changes, sometimes with accuracy drops\nby more than 20%. We then demonstrate that simply adding more requirements to a\nprompt does not reliably improve performance, due to LLMs' limited\ninstruction-following capabilities and competing constraints, and standard\nprompt optimizers do not offer much help. To address this, we introduce novel\nrequirements-aware prompt optimization mechanisms that can improve performance\nby 4.8% on average over baselines that naively specify everything in the\nprompt. Beyond prompt optimization, we envision that effectively managing\nprompt underspecification requires a broader process, including proactive\nrequirements discovery, evaluation, and monitoring."}
{"id": "2505.11836", "pdf": "https://arxiv.org/pdf/2505.11836", "abs": "https://arxiv.org/abs/2505.11836", "authors": ["Jeremy Budd", "Javier Ideami", "Benjamin Macdowall Rynne", "Keith Duggar", "Randall Balestriero"], "title": "SplInterp: Improving our Understanding and Training of Sparse Autoencoders", "categories": ["cs.LG", "cs.AI", "68T07, 65D07"], "comment": "44 pages, 38 figures, under review", "summary": "Sparse autoencoders (SAEs) have received considerable recent attention as\ntools for mechanistic interpretability, showing success at extracting\ninterpretable features even from very large LLMs. However, this research has\nbeen largely empirical, and there have been recent doubts about the true\nutility of SAEs. In this work, we seek to enhance the theoretical understanding\nof SAEs, using the spline theory of deep learning. By situating SAEs in this\nframework: we discover that SAEs generalise ``$k$-means autoencoders'' to be\npiecewise affine, but sacrifice accuracy for interpretability vs. the optimal\n``$k$-means-esque plus local principal component analysis (PCA)'' piecewise\naffine autoencoder. We characterise the underlying geometry of (TopK) SAEs\nusing power diagrams. And we develop a novel proximal alternating method SGD\n(PAM-SGD) algorithm for training SAEs, with both solid theoretical foundations\nand promising empirical results in MNIST and LLM experiments, particularly in\nsample efficiency and (in the LLM setting) improved sparsity of codes. All code\nis available at: https://github.com/splInterp2025/splInterp"}
{"id": "2505.12701", "pdf": "https://arxiv.org/pdf/2505.12701", "abs": "https://arxiv.org/abs/2505.12701", "authors": ["Shuyang Dong", "Shangtong Zhang", "Lu Feng"], "title": "Counterfactual Explanations for Continuous Action Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by International Joint Conference on Artificial Intelligence\n  (IJCAI) 2025", "summary": "Reinforcement Learning (RL) has shown great promise in domains like\nhealthcare and robotics but often struggles with adoption due to its lack of\ninterpretability. Counterfactual explanations, which address \"what if\"\nscenarios, provide a promising avenue for understanding RL decisions but remain\nunderexplored for continuous action spaces. We propose a novel approach for\ngenerating counterfactual explanations in continuous action RL by computing\nalternative action sequences that improve outcomes while minimizing deviations\nfrom the original sequence. Our approach leverages a distance metric for\ncontinuous actions and accounts for constraints such as adhering to predefined\npolicies in specific states. Evaluations in two RL domains, Diabetes Control\nand Lunar Lander, demonstrate the effectiveness, efficiency, and generalization\nof our approach, enabling more interpretable and trustworthy RL applications."}
{"id": "2505.13379", "pdf": "https://arxiv.org/pdf/2505.13379", "abs": "https://arxiv.org/abs/2505.13379", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "title": "Thinkless: LLM Learns When to Think", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless"}
{"id": "2505.11837", "pdf": "https://arxiv.org/pdf/2505.11837", "abs": "https://arxiv.org/abs/2505.11837", "authors": ["Ziyao Cui", "Minxing Zhang", "Jian Pei"], "title": "On Membership Inference Attacks in Knowledge Distillation", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Nowadays, Large Language Models (LLMs) are trained on huge datasets, some\nincluding sensitive information. This poses a serious privacy concern because\nprivacy attacks such as Membership Inference Attacks (MIAs) may detect this\nsensitive information. While knowledge distillation compresses LLMs into\nefficient, smaller student models, its impact on privacy remains underexplored.\nIn this paper, we investigate how knowledge distillation affects model\nrobustness against MIA. We focus on two questions. First, how is private data\nprotected in teacher and student models? Second, how can we strengthen privacy\npreservation against MIAs in knowledge distillation? Through comprehensive\nexperiments, we show that while teacher and student models achieve similar\noverall MIA accuracy, teacher models better protect member data, the primary\ntarget of MIA, whereas student models better protect non-member data. To\naddress this vulnerability in student models, we propose 5 privacy-preserving\ndistillation methods and demonstrate that they successfully reduce student\nmodels' vulnerability to MIA, with ensembling further stabilizing the\nrobustness, offering a reliable approach for distilling more secure and\nefficient student models. Our implementation source code is available at\nhttps://github.com/richardcui18/MIA_in_KD."}
{"id": "2505.12707", "pdf": "https://arxiv.org/pdf/2505.12707", "abs": "https://arxiv.org/abs/2505.12707", "authors": ["Yingchen He", "Christian D. Weilbach", "Martyna E. Wojciechowska", "Yuxuan Zhang", "Frank Wood"], "title": "PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "9 pages, 8 figures", "summary": "Advances in deep generative modelling have made it increasingly plausible to\ntrain human-level embodied agents. Yet progress has been limited by the absence\nof large-scale, real-time, multi-modal, and socially interactive datasets that\nreflect the sensory-motor complexity of natural environments. To address this,\nwe present PLAICraft, a novel data collection platform and dataset capturing\nmultiplayer Minecraft interactions across five time-aligned modalities: video,\ngame output audio, microphone input audio, mouse, and keyboard actions. Each\nmodality is logged with millisecond time precision, enabling the study of\nsynchronous, embodied behaviour in a rich, open-ended world. The dataset\ncomprises over 10,000 hours of gameplay from more than 10,000 global\nparticipants.\\footnote{We have done a privacy review for the public release of\nan initial 200-hour subset of the dataset, with plans to release most of the\ndataset over time.} Alongside the dataset, we provide an evaluation suite for\nbenchmarking model capabilities in object recognition, spatial awareness,\nlanguage grounding, and long-term memory. PLAICraft opens a path toward\ntraining and evaluating agents that act fluently and purposefully in real time,\npaving the way for truly embodied artificial intelligence."}
{"id": "2505.13388", "pdf": "https://arxiv.org/pdf/2505.13388", "abs": "https://arxiv.org/abs/2505.13388", "authors": ["David Anugraha", "Zilu Tang", "Lester James V. Miranda", "Hanyang Zhao", "Mohammad Rifqi Farhansyah", "Garry Kuwanto", "Derry Wijaya", "Genta Indra Winata"], "title": "R3: Robust Rubric-Agnostic Reward Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3"}
{"id": "2505.11862", "pdf": "https://arxiv.org/pdf/2505.11862", "abs": "https://arxiv.org/abs/2505.11862", "authors": ["Kalyan Cherukuri", "Aarav Lala", "Yash Yardi"], "title": "Q-Policy: Quantum-Enhanced Policy Evaluation for Scalable Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": null, "summary": "We propose Q-Policy, a hybrid quantum-classical reinforcement learning (RL)\nframework that mathematically accelerates policy evaluation and optimization by\nexploiting quantum computing primitives. Q-Policy encodes value functions in\nquantum superposition, enabling simultaneous evaluation of multiple\nstate-action pairs via amplitude encoding and quantum parallelism. We introduce\na quantum-enhanced policy iteration algorithm with provable polynomial\nreductions in sample complexity for the evaluation step, under standard\nassumptions. To demonstrate the technical feasibility and theoretical soundness\nof our approach, we validate Q-Policy on classical emulations of small discrete\ncontrol tasks. Due to current hardware and simulation limitations, our\nexperiments focus on showcasing proof-of-concept behavior rather than\nlarge-scale empirical evaluation. Our results support the potential of Q-Policy\nas a theoretical foundation for scalable RL on future quantum devices,\naddressing RL scalability challenges beyond classical approaches."}
{"id": "2505.12709", "pdf": "https://arxiv.org/pdf/2505.12709", "abs": "https://arxiv.org/abs/2505.12709", "authors": ["Zhichen Zeng", "Ruizhong Qiu", "Wenxuan Bao", "Tianxin Wei", "Xiao Lin", "Yuchen Yan", "Tarek F. Abdelzaher", "Jiawei Han", "Hanghang Tong"], "title": "Pave Your Own Path: Graph Gradual Domain Adaptation on Fused Gromov-Wasserstein Geodesics", "categories": ["cs.LG"], "comment": "27 pages, 10 figures", "summary": "Graph neural networks, despite their impressive performance, are highly\nvulnerable to distribution shifts on graphs. Existing graph domain adaptation\n(graph DA) methods often implicitly assume a \\textit{mild} shift between source\nand target graphs, limiting their applicability to real-world scenarios with\n\\textit{large} shifts. Gradual domain adaptation (GDA) has emerged as a\npromising approach for addressing large shifts by gradually adapting the source\nmodel to the target domain via a path of unlabeled intermediate domains.\nExisting GDA methods exclusively focus on independent and identically\ndistributed (IID) data with a predefined path, leaving their extension to\n\\textit{non-IID graphs without a given path} an open challenge. To bridge this\ngap, we present Gadget, the first GDA framework for non-IID graph data. First\n(\\textit{theoretical foundation}), the Fused Gromov-Wasserstein (FGW) distance\nis adopted as the domain discrepancy for non-IID graphs, based on which, we\nderive an error bound revealing that the target domain error is proportional to\nthe length of the path. Second (\\textit{optimal path}), guided by the error\nbound, we identify the FGW geodesic as the optimal path, which can be\nefficiently generated by our proposed algorithm. The generated path can be\nseamlessly integrated with existing graph DA methods to handle large shifts on\ngraphs, improving state-of-the-art graph DA methods by up to 6.8\\% in node\nclassification accuracy on real-world datasets."}
{"id": "2505.13403", "pdf": "https://arxiv.org/pdf/2505.13403", "abs": "https://arxiv.org/abs/2505.13403", "authors": ["Renjie Pi", "Felix Bai", "Qibin Chen", "Simon Wang", "Jiulong Shan", "Kieran Liu", "Meng Cao"], "title": "MR. Judge: Multimodal Reasoner as a Judge", "categories": ["cs.CL"], "comment": null, "summary": "The paradigm of using Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) as evaluative judges has emerged as an effective\napproach in RLHF and inference-time scaling. In this work, we propose\nMultimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering\ngeneral-purpose MLLMs judges with strong reasoning capabilities. Instead of\ndirectly assigning scores for each response, we formulate the judgement process\nas a reasoning-inspired multiple-choice problem. Specifically, the judge model\nfirst conducts deliberate reasoning covering different aspects of the responses\nand eventually selects the best response from them. This reasoning process not\nonly improves the interpretibility of the judgement, but also greatly enhances\nthe performance of MLLM judges. To cope with the lack of questions with scored\nresponses, we propose the following strategy to achieve automatic annotation:\n1) Reverse Response Candidates Synthesis: starting from a supervised\nfine-tuning (SFT) dataset, we treat the original response as the best candidate\nand prompt the MLLM to generate plausible but flawed negative candidates. 2)\nText-based reasoning extraction: we carefully design a data synthesis pipeline\nfor distilling the reasoning capability from a text-based reasoning model,\nwhich is adopted to enable the MLLM judges to regain complex reasoning ability\nvia warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge\nis effective across a wide range of tasks. Specifically, our MR. Judge-7B\nsurpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet\nduring inference-time scaling by up to 7.7%."}
{"id": "2505.11864", "pdf": "https://arxiv.org/pdf/2505.11864", "abs": "https://arxiv.org/abs/2505.11864", "authors": ["Kalyan Cherukuri", "Aarav Lala"], "title": "Learning Pareto-Optimal Rewards from Noisy Preferences: A Framework for Multi-Objective Inverse Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CG"], "comment": null, "summary": "As generative agents become increasingly capable, alignment of their behavior\nwith complex human values remains a fundamental challenge. Existing approaches\noften simplify human intent through reduction to a scalar reward, overlooking\nthe multi-faceted nature of human feedback. In this work, we introduce a\ntheoretical framework for preference-based Multi-Objective Inverse\nReinforcement Learning (MO-IRL), where human preferences are modeled as latent\nvector-valued reward functions. We formalize the problem of recovering a\nPareto-optimal reward representation from noisy preference queries and\nestablish conditions for identifying the underlying multi-objective structure.\nWe derive tight sample complexity bounds for recovering\n$\\epsilon$-approximations of the Pareto front and introduce a regret\nformulation to quantify suboptimality in this multi-objective setting.\nFurthermore, we propose a provably convergent algorithm for policy optimization\nusing preference-inferred reward cones. Our results bridge the gap between\npractical alignment techniques and theoretical guarantees, providing a\nprincipled foundation for learning aligned behaviors in a high-dimension and\nvalue-pluralistic environment."}
{"id": "2505.12710", "pdf": "https://arxiv.org/pdf/2505.12710", "abs": "https://arxiv.org/abs/2505.12710", "authors": ["Yingkai Kang", "Jiawen Kang", "Jinbo Wen", "Tao Zhang", "Zhaohui Yang", "Dusit Niyato", "Yan Zhang"], "title": "Confidence-Regulated Generative Diffusion Models for Reliable AI Agent Migration in Vehicular Metaverses", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "Vehicular metaverses are an emerging paradigm that merges intelligent\ntransportation systems with virtual spaces, leveraging advanced digital twin\nand Artificial Intelligence (AI) technologies to seamlessly integrate vehicles,\nusers, and digital environments. In this paradigm, vehicular AI agents are\nendowed with environment perception, decision-making, and action execution\ncapabilities, enabling real-time processing and analysis of multi-modal data to\nprovide users with customized interactive services. Since vehicular AI agents\nrequire substantial resources for real-time decision-making, given vehicle\nmobility and network dynamics conditions, the AI agents are deployed in\nRoadSide Units (RSUs) with sufficient resources and dynamically migrated among\nthem. However, AI agent migration requires frequent data exchanges, which may\nexpose vehicular metaverses to potential cyber attacks. To this end, we propose\na reliable vehicular AI agent migration framework, achieving reliable dynamic\nmigration and efficient resource scheduling through cooperation between\nvehicles and RSUs. Additionally, we design a trust evaluation model based on\nthe theory of planned behavior to dynamically quantify the reputation of RSUs,\nthereby better accommodating the personalized trust preferences of users. We\nthen model the vehicular AI agent migration process as a partially observable\nmarkov decision process and develop a Confidence-regulated Generative Diffusion\nModel (CGDM) to efficiently generate AI agent migration decisions. Numerical\nresults demonstrate that the CGDM algorithm significantly outperforms baseline\nmethods in reducing system latency and enhancing robustness against cyber\nattacks."}
{"id": "2505.13404", "pdf": "https://arxiv.org/pdf/2505.13404", "abs": "https://arxiv.org/abs/2505.13404", "authors": ["Nithin Rao Koluguri", "Monica Sekoyan", "George Zelenfroynd", "Sasha Meister", "Shuoyang Ding", "Sofia Kostandian", "He Huang", "Nikolay Karpov", "Jagadeesh Balam", "Vitaly Lavrukhin", "Yifan Peng", "Sara Papi", "Marco Gaido", "Alessio Brutti", "Boris Ginsburg"], "title": "Granary: Speech Recognition and Translation Dataset in 25 European Languages", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Multi-task and multilingual approaches benefit large models, yet speech\nprocessing for low-resource languages remains underexplored due to data\nscarcity. To address this, we present Granary, a large-scale collection of\nspeech datasets for recognition and translation across 25 European languages.\nThis is the first open-source effort at this scale for both transcription and\ntranslation. We enhance data quality using a pseudo-labeling pipeline with\nsegmentation, two-pass inference, hallucination filtering, and punctuation\nrestoration. We further generate translation pairs from pseudo-labeled\ntranscriptions using EuroLLM, followed by a data filtration pipeline. Designed\nfor efficiency, our pipeline processes vast amount of data within hours. We\nassess models trained on processed data by comparing their performance on\npreviously curated datasets for both high- and low-resource languages. Our\nfindings show that these models achieve similar performance using approx. 50%\nless data. Dataset will be made available at\nhttps://hf.co/datasets/nvidia/Granary"}
{"id": "2505.11878", "pdf": "https://arxiv.org/pdf/2505.11878", "abs": "https://arxiv.org/abs/2505.11878", "authors": ["Yifan Dai", "Xuanbai Ren", "Tengfei Ma", "Qipeng Yan", "Yiping Liu", "Yuansheng Liu", "Xiangxiang Zeng"], "title": "AdaptMol: Adaptive Fusion from Sequence String to Topological Structure for Few-shot Drug Discovery", "categories": ["cs.LG", "cs.AI", "q-bio.MN", "J.3; I.2.7"], "comment": "15 pages, 6 figures", "summary": "Accurate molecular property prediction (MPP) is a critical step in modern\ndrug development. However, the scarcity of experimental validation data poses a\nsignificant challenge to AI-driven research paradigms. Under few-shot learning\nscenarios, the quality of molecular representations directly dictates the\ntheoretical upper limit of model performance. We present AdaptMol, a\nprototypical network integrating Adaptive multimodal fusion for Molecular\nrepresentation. This framework employs a dual-level attention mechanism to\ndynamically integrate global and local molecular features derived from two\nmodalities: SMILES sequences and molecular graphs. (1) At the local level,\nstructural features such as atomic interactions and substructures are extracted\nfrom molecular graphs, emphasizing fine-grained topological information; (2) At\nthe global level, the SMILES sequence provides a holistic representation of the\nmolecule. To validate the necessity of multimodal adaptive fusion, we propose\nan interpretable approach based on identifying molecular active substructures\nto demonstrate that multimodal adaptive fusion can efficiently represent\nmolecules. Extensive experiments on three commonly used benchmarks under 5-shot\nand 10-shot settings demonstrate that AdaptMol achieves state-of-the-art\nperformance in most cases. The rationale-extracted method guides the fusion of\ntwo modalities and highlights the importance of both modalities."}
{"id": "2505.12736", "pdf": "https://arxiv.org/pdf/2505.12736", "abs": "https://arxiv.org/abs/2505.12736", "authors": ["Zeyi Ren", "Jingreng Lei", "Yichen Jin", "Ermo Hua", "Qingfeng Lin", "Chen Zhang", "Bowen Zhou", "Yik-Chung Wu"], "title": "Deep Unfolding with Kernel-based Quantization in MIMO Detection", "categories": ["cs.LG"], "comment": "submitted to ICML ML4Wireless workshop", "summary": "The development of edge computing places critical demands on energy-efficient\nmodel deployment for multiple-input multiple-output (MIMO) detection tasks.\nDeploying deep unfolding models such as PGD-Nets and ADMM-Nets into\nresource-constrained edge devices using quantization methods is challenging.\nExisting quantization methods based on quantization aware training (QAT) suffer\nfrom performance degradation due to their reliance on parametric distribution\nassumption of activations and static quantization step sizes. To address these\nchallenges, this paper proposes a novel kernel-based adaptive quantization\n(KAQ) framework for deep unfolding networks. By utilizing a joint kernel\ndensity estimation (KDE) and maximum mean discrepancy (MMD) approach to align\nactivation distributions between full-precision and quantized models, the need\nfor prior distribution assumptions is eliminated. Additionally, a dynamic step\nsize updating method is introduced to adjust the quantization step size based\non the channel conditions of wireless networks. Extensive simulations\ndemonstrate that the accuracy of proposed KAQ framework outperforms traditional\nmethods and successfully reduces the model's inference latency."}
{"id": "2505.13417", "pdf": "https://arxiv.org/pdf/2505.13417", "abs": "https://arxiv.org/abs/2505.13417", "authors": ["Jiajie Zhang", "Nianyi Lin", "Lei Hou", "Ling Feng", "Juanzi Li"], "title": "AdaptThink: Reasoning Models Can Learn When to Think", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink."}
{"id": "2505.11881", "pdf": "https://arxiv.org/pdf/2505.11881", "abs": "https://arxiv.org/abs/2505.11881", "authors": ["Giyeong Oh", "Woohyun Cho", "Siyeol Kim", "Suhwan Choi", "Younjae Yu"], "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, WIP", "summary": "Residual connections are pivotal for deep neural networks, enabling greater\ndepth by mitigating vanishing gradients. However, in standard residual updates,\nthe module's output is directly added to the input stream. This can lead to\nupdates that predominantly reinforce or modulate the existing stream direction,\npotentially underutilizing the module's capacity for learning entirely novel\nfeatures. In this work, we introduce Orthogonal Residual Update: we decompose\nthe module's output relative to the input stream and add only the component\northogonal to this stream. This design aims to guide modules to contribute\nprimarily new representational directions, fostering richer feature learning\nwhile promoting more efficient training. We demonstrate that our orthogonal\nupdate strategy improves generalization accuracy and training stability across\ndiverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs,\nTinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy\ngain for ViT-B on ImageNet-1k."}
{"id": "2505.12737", "pdf": "https://arxiv.org/pdf/2505.12737", "abs": "https://arxiv.org/abs/2505.12737", "authors": ["Hongjoon Ahn", "Heewoong Choi", "Jisu Han", "Taesup Moon"], "title": "Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Offline goal-conditioned reinforcement learning (GCRL) offers a practical\nlearning paradigm where goal-reaching policies are trained from abundant\nunlabeled (reward-free) datasets without additional environment interaction.\nHowever, offline GCRL still struggles with long-horizon tasks, even with recent\nadvances that employ hierarchical policy structures, such as HIQL. By\nidentifying the root cause of this challenge, we observe the following\ninsights: First, performance bottlenecks mainly stem from the high-level\npolicy's inability to generate appropriate subgoals. Second, when learning the\nhigh-level policy in the long-horizon regime, the sign of the advantage signal\nfrequently becomes incorrect. Thus, we argue that improving the value function\nto produce a clear advantage signal for learning the high-level policy is\nessential. In this paper, we propose a simple yet effective solution:\nOption-aware Temporally Abstracted value learning, dubbed OTA, which\nincorporates temporal abstraction into the temporal-difference learning\nprocess. By modifying the value update to be option-aware, the proposed\nlearning scheme contracts the effective horizon length, enabling better\nadvantage estimates even in long-horizon regimes. We experimentally show that\nthe high-level policy extracted using the OTA value function achieves strong\nperformance on complex tasks from OGBench, a recently proposed offline GCRL\nbenchmark, including maze navigation and visual robotic manipulation\nenvironments."}
{"id": "2505.13418", "pdf": "https://arxiv.org/pdf/2505.13418", "abs": "https://arxiv.org/abs/2505.13418", "authors": ["Lotem Peled-Cohen", "Maya Zadok", "Nitay Calderon", "Hila Gonen", "Roi Reichart"], "title": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter."}
{"id": "2505.11889", "pdf": "https://arxiv.org/pdf/2505.11889", "abs": "https://arxiv.org/abs/2505.11889", "authors": ["Hanfang Cui", "Longfei Song", "Li Li", "Dongxing Xu", "Yanhua Long"], "title": "Exploring the Potential of SSL Models for Sound Event Detection", "categories": ["eess.AS", "cs.AI", "cs.SD", "I.5.4; I.2.10; H.5.5"], "comment": "27 pages, 5 figures, submitted to the Journal of King Saud University\n  - Computer and Information Sciences (under review)", "summary": "Self-supervised learning (SSL) models offer powerful representations for\nsound event detection (SED), yet their synergistic potential remains\nunderexplored. This study systematically evaluates state-of-the-art SSL models\nto guide optimal model selection and integration for SED. We propose a\nframework that combines heterogeneous SSL representations (e.g., BEATs, HuBERT,\nWavLM) through three fusion strategies: individual SSL embedding integration,\ndual-modal fusion, and full aggregation. Experiments on the DCASE 2023 Task 4\nChallenge reveal that dual-modal fusion (e.g., CRNN+BEATs+WavLM) achieves\ncomplementary performance gains, while CRNN+BEATs alone delivers the best\nresults among individual SSL models. We further introduce normalized sound\nevent bounding boxes (nSEBBs), an adaptive post-processing method that\ndynamically adjusts event boundary predictions, improving PSDS1 by up to 4% for\nstandalone SSL models. These findings highlight the compatibility and\ncomplementarity of SSL architectures, providing guidance for task-specific\nfusion and robust SED system design."}
{"id": "2505.12738", "pdf": "https://arxiv.org/pdf/2505.12738", "abs": "https://arxiv.org/abs/2505.12738", "authors": ["Chenghua Gong", "Rui Sun", "Yuhao Zheng", "Juyuan Zhang", "Tianjun Gu", "Liming Pan", "Linyuan Lv"], "title": "EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting", "categories": ["cs.LG", "cs.AI", "cs.SI"], "comment": "18 pages", "summary": "Advanced epidemic forecasting is critical for enabling precision containment\nstrategies, highlighting its strategic importance for public health security.\nWhile recent advances in Large Language Models (LLMs) have demonstrated\neffectiveness as foundation models for domain-specific tasks, their potential\nfor epidemic forecasting remains largely unexplored. In this paper, we\nintroduce EpiLLM, a novel LLM-based framework tailored for spatio-temporal\nepidemic forecasting. Considering the key factors in real-world epidemic\ntransmission: infection cases and human mobility, we introduce a dual-branch\narchitecture to achieve fine-grained token-level alignment between such complex\nepidemic patterns and language tokens for LLM adaptation. To unleash the\nmulti-step forecasting and generalization potential of LLM architectures, we\npropose an autoregressive modeling paradigm that reformulates the epidemic\nforecasting task into next-token prediction. To further enhance LLM perception\nof epidemics, we introduce spatio-temporal prompt learning techniques, which\nstrengthen forecasting capabilities from a data-driven perspective. Extensive\nexperiments show that EpiLLM significantly outperforms existing baselines on\nreal-world COVID-19 datasets and exhibits scaling behavior characteristic of\nLLMs."}
{"id": "2505.13434", "pdf": "https://arxiv.org/pdf/2505.13434", "abs": "https://arxiv.org/abs/2505.13434", "authors": ["Mateusz Bystroński", "Mikołaj Hołysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "title": "SMOTExT: SMOTE meets Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Data scarcity and class imbalance are persistent challenges in training\nrobust NLP models, especially in specialized domains or low-resource settings.\nWe propose a novel technique, SMOTExT, that adapts the idea of Synthetic\nMinority Over-sampling (SMOTE) to textual data. Our method generates new\nsynthetic examples by interpolating between BERT-based embeddings of two\nexisting examples and then decoding the resulting latent point into text with\nxRAG architecture. By leveraging xRAG's cross-modal retrieval-generation\nframework, we can effectively turn interpolated vectors into coherent text.\nWhile this is preliminary work supported by qualitative outputs only, the\nmethod shows strong potential for knowledge distillation and data augmentation\nin few-shot settings. Notably, our approach also shows promise for\nprivacy-preserving machine learning: in early experiments, training models\nsolely on generated data achieved comparable performance to models trained on\nthe original dataset. This suggests a viable path toward safe and effective\nlearning under data protection constraints."}
{"id": "2505.11891", "pdf": "https://arxiv.org/pdf/2505.11891", "abs": "https://arxiv.org/abs/2505.11891", "authors": ["Weikai Xu", "Zhizheng Jiang", "Yuxuan Liu", "Wei Liu", "Jian Luan", "Yuanchun Li", "Yunxin Liu", "Bin Wang", "Bo An"], "title": "Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "VLM-based mobile agents are increasingly popular due to their capabilities to\ninteract with smartphone GUIs and XML-structured texts and to complete daily\ntasks. However, existing online benchmarks struggle with obtaining stable\nreward signals due to dynamic environmental changes. Offline benchmarks\nevaluate the agents through single-path trajectories, which stands in contrast\nto the inherently multi-solution characteristics of GUI tasks. Additionally,\nboth types of benchmarks fail to assess whether mobile agents can handle noise\nor engage in proactive interactions due to a lack of noisy apps or overly full\ninstructions during the evaluation process. To address these limitations, we\nuse a slot-based instruction generation method to construct a more realistic\nand comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a\ncommon task split, with offline multi-path evaluation to assess the agent's\nability to obtain step rewards during task execution. It contains a noisy split\nbased on pop-ups and ads apps, and a contaminated split named AITZ-Noise to\nformulate a real noisy environment. Furthermore, an ambiguous instruction split\nwith preset Q\\&A interactions is released to evaluate the agent's proactive\ninteraction capabilities. We conduct evaluations on these splits using the\nsingle-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2,\nas well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are\navailable at https://huggingface.co/datasets/xwk123/MobileBench-v2."}
{"id": "2505.12745", "pdf": "https://arxiv.org/pdf/2505.12745", "abs": "https://arxiv.org/abs/2505.12745", "authors": ["Dong Kyu Cho", "Inwoo Hwang", "Sanghack Lee"], "title": "PEER pressure: Model-to-Model Regularization for Single Source Domain Generalization", "categories": ["cs.LG", "cs.AI"], "comment": "21 pages, 9 figures, Accepted at CVPR 2025", "summary": "Data augmentation is a popular tool for single source domain generalization,\nwhich expands the source domain by generating simulated ones, improving\ngeneralization on unseen target domains. In this work, we show that the\nperformance of such augmentation-based methods in the target domains\nuniversally fluctuates during training, posing challenges in model selection\nunder realistic scenarios. We argue that the fluctuation stems from the\ninability of the model to accumulate the knowledge learned from diverse\naugmentations, exacerbating feature distortion during training. Based on this\nobservation, we propose a novel generalization method, coined Parameter-Space\nEnsemble with Entropy Regularization (PEER), that uses a proxy model to learn\nthe augmented data on behalf of the main model. The main model is updated by\naveraging its parameters with the proxy model, progressively accumulating\nknowledge over the training steps. Maximizing the mutual information between\nthe output representations of the two models guides the learning process of the\nproxy model, mitigating feature distortion during training. Experimental\nresults demonstrate the effectiveness of PEER in reducing the OOD performance\nfluctuation and enhancing generalization across various datasets, including\nPACS, Digits, Office-Home, and VLCS. Notably, our method with simple random\naugmentation achieves state-of-the-art performance, surpassing prior approaches\non sDG that utilize complex data augmentation strategies."}
{"id": "2505.13444", "pdf": "https://arxiv.org/pdf/2505.13444", "abs": "https://arxiv.org/abs/2505.13444", "authors": ["Liyan Tang", "Grace Kim", "Xinyu Zhao", "Thom Lake", "Wenxuan Ding", "Fangcong Yin", "Prasann Singhal", "Manya Wadhwa", "Zeyu Leo Liu", "Zayne Sprague", "Ramya Namuduri", "Bodun Hu", "Juan Diego Rodriguez", "Puyuan Peng", "Greg Durrett"], "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs."}
{"id": "2505.11893", "pdf": "https://arxiv.org/pdf/2505.11893", "abs": "https://arxiv.org/abs/2505.11893", "authors": ["Zepeng Ding", "Dixuan Wang", "Ziqin Luo", "Guochao Jiang", "Deqing Yang", "Jiaqing Liang"], "title": "RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-step planning has been widely employed to enhance the performance of\nlarge language models (LLMs) on downstream natural language processing (NLP)\ntasks, which decomposes the original task into multiple subtasks and guide LLMs\nto solve them sequentially without additional training. When addressing task\ninstances, existing methods either preset the order of steps or attempt\nmultiple paths at each step. However, these methods overlook instances'\nlinguistic features and rely on the intrinsic planning capabilities of LLMs to\nevaluate intermediate feedback and then select subtasks, resulting in\nsuboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this\npaper we propose a Reinforcement Learning enhanced Adaptive Planning framework\n(RLAP). In our framework, we model an NLP task as a Markov decision process\n(MDP) and employ an LLM directly into the environment. In particular, a\nlightweight Actor model is trained to estimate Q-values for natural language\nsequences consisting of states and actions through reinforcement learning.\nTherefore, during sequential planning, the linguistic features of each sequence\nin the MDP can be taken into account, and the Actor model interacts with the\nLLM to determine the optimal order of subtasks for each task instance. We apply\nRLAP on three different types of NLP tasks and conduct extensive experiments on\nmultiple datasets to verify RLAP's effectiveness and robustness."}
{"id": "2505.12751", "pdf": "https://arxiv.org/pdf/2505.12751", "abs": "https://arxiv.org/abs/2505.12751", "authors": ["Filippo Leveni"], "title": "Structure-based Anomaly Detection and Clustering", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Doctoral dissertation at Politecnico di Milano", "summary": "Anomaly detection is a fundamental problem in domains such as healthcare,\nmanufacturing, and cybersecurity. This thesis proposes new unsupervised methods\nfor anomaly detection in both structured and streaming data settings. In the\nfirst part, we focus on structure-based anomaly detection, where normal data\nfollows low-dimensional manifolds while anomalies deviate from them. We\nintroduce Preference Isolation Forest (PIF), which embeds data into a\nhigh-dimensional preference space via manifold fitting, and isolates outliers\nusing two variants: Voronoi-iForest, based on geometric distances, and\nRuzHash-iForest, leveraging Locality Sensitive Hashing for scalability. We also\npropose Sliding-PIF, which captures local manifold information for streaming\nscenarios. Our methods outperform existing techniques on synthetic and real\ndatasets. We extend this to structure-based clustering with MultiLink, a novel\nmethod for recovering multiple geometric model families in noisy data.\nMultiLink merges clusters via a model-aware linkage strategy, enabling robust\nmulti-class structure recovery. It offers key advantages over existing\napproaches, such as speed, reduced sensitivity to thresholds, and improved\nrobustness to poor initial sampling. The second part of the thesis addresses\nonline anomaly detection in evolving data streams. We propose Online Isolation\nForest (Online-iForest), which uses adaptive, multi-resolution histograms and\ndynamically updates tree structures to track changes over time. It avoids\nretraining while achieving accuracy comparable to offline models, with superior\nefficiency for real-time applications. Finally, we tackle anomaly detection in\ncybersecurity via open-set recognition for malware classification. We enhance a\nGradient Boosting classifier with MaxLogit to detect unseen malware families, a\nmethod now integrated into Cleafy's production system."}
{"id": "2505.13448", "pdf": "https://arxiv.org/pdf/2505.13448", "abs": "https://arxiv.org/abs/2505.13448", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE."}
{"id": "2505.11896", "pdf": "https://arxiv.org/pdf/2505.11896", "abs": "https://arxiv.org/abs/2505.11896", "authors": ["Chenwei Lou", "Zewei Sun", "Xinnian Liang", "Meng Qu", "Wei Shen", "Wenqi Wang", "Yuntao Li", "Qingping Yang", "Shuangzhi Wu"], "title": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities but\noften face challenges with tasks requiring sophisticated reasoning. While\nChain-of-Thought (CoT) prompting significantly enhances reasoning, it\nindiscriminately generates lengthy reasoning steps for all queries, leading to\nsubstantial computational costs and inefficiency, especially for simpler\ninputs. To address this critical issue, we introduce AdaCoT (Adaptive\nChain-of-Thought), a novel framework enabling LLMs to adaptively decide when to\ninvoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem\nthat seeks to balance model performance with the costs associated with CoT\ninvocation (both frequency and computational overhead). We propose a\nreinforcement learning (RL) based method, specifically utilizing Proximal\nPolicy Optimization (PPO), to dynamically control the CoT triggering decision\nboundary by adjusting penalty coefficients, thereby allowing the model to\ndetermine CoT necessity based on implicit query complexity. A key technical\ncontribution is Selective Loss Masking (SLM), designed to counteract decision\nboundary collapse during multi-stage RL training, ensuring robust and stable\nadaptive triggering. Experimental results demonstrate that AdaCoT successfully\nnavigates the Pareto frontier, achieving substantial reductions in CoT usage\nfor queries not requiring elaborate reasoning. For instance, on our production\ntraffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\\% and\ndecreased average response tokens by 69.06%, while maintaining high performance\non complex tasks."}
{"id": "2505.12754", "pdf": "https://arxiv.org/pdf/2505.12754", "abs": "https://arxiv.org/abs/2505.12754", "authors": ["Wenya Guo", "Zhengkun Zhang", "Xumeng Liu", "Ying Zhang", "Ziyu Lu", "Haoze Zhu", "Xubo Liu", "Ruxue Yan"], "title": "ProDS: Preference-oriented Data Selection for Instruction Tuning", "categories": ["cs.LG"], "comment": null, "summary": "Instruction data selection aims to identify a high-quality subset from the\ntraining set that matches or exceeds the performance of the full dataset on\ntarget tasks. Existing methods focus on the instruction-to-response mapping,\nbut neglect the human preference for diverse responses. In this paper, we\npropose Preference-oriented Data Selection method (ProDS) that scores training\nsamples based on their alignment with preferences observed in the target set.\nOur key innovation lies in shifting the data selection criteria from merely\nestimating features for accurate response generation to explicitly aligning\ntraining samples with human preferences in target tasks. Specifically, direct\npreference optimization (DPO) is employed to estimate human preferences across\ndiverse responses. Besides, a bidirectional preference synthesis strategy is\ndesigned to score training samples according to both positive preferences and\nnegative preferences. Extensive experimental results demonstrate our\nsuperiority to existing task-agnostic and targeted methods."}
{"id": "2505.11545", "pdf": "https://arxiv.org/pdf/2505.11545", "abs": "https://arxiv.org/abs/2505.11545", "authors": ["Xingyu Ji", "Parker Glenn", "Aditya G. Parameswaran", "Madelon Hulsebos"], "title": "TARGET: Benchmarking Table Retrieval for Generative Tasks", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB"], "comment": null, "summary": "The data landscape is rich with structured data, often of high value to\norganizations, driving important applications in data analysis and machine\nlearning. Recent progress in representation learning and generative models for\nsuch data has led to the development of natural language interfaces to\nstructured data, including those leveraging text-to-SQL. Contextualizing\ninteractions, either through conversational interfaces or agentic components,\nin structured data through retrieval-augmented generation can provide\nsubstantial benefits in the form of freshness, accuracy, and comprehensiveness\nof answers. The key question is: how do we retrieve the right table(s) for the\nanalytical query or task at hand? To this end, we introduce TARGET: a benchmark\nfor evaluating TAble Retrieval for GEnerative Tasks. With TARGET we analyze the\nretrieval performance of different retrievers in isolation, as well as their\nimpact on downstream tasks. We find that dense embedding-based retrievers far\noutperform a BM25 baseline which is less effective than it is for retrieval\nover unstructured text. We also surface the sensitivity of retrievers across\nvarious metadata (e.g., missing table titles), and demonstrate a stark\nvariation of retrieval performance across datasets and tasks. TARGET is\navailable at https://target-benchmark.github.io."}
{"id": "2505.11904", "pdf": "https://arxiv.org/pdf/2505.11904", "abs": "https://arxiv.org/abs/2505.11904", "authors": ["Louis Mahon", "Mirella Lapata"], "title": "K*-Means: A Parameter-free Clustering Algorithm", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Clustering is a widely used and powerful machine learning technique, but its\neffectiveness is often limited by the need to specify the number of clusters,\nk, or by relying on thresholds that implicitly determine k. We introduce\nk*-means, a novel clustering algorithm that eliminates the need to set k or any\nother parameters. Instead, it uses the minimum description length principle to\nautomatically determine the optimal number of clusters, k*, by splitting and\nmerging clusters while also optimising the standard k-means objective. We prove\nthat k*-means is guaranteed to converge and demonstrate experimentally that it\nsignificantly outperforms existing methods in scenarios where k is unknown. We\nalso show that it is accurate in estimating k, and that empirically its runtime\nis competitive with existing methods, and scales well with dataset size."}
{"id": "2505.12759", "pdf": "https://arxiv.org/pdf/2505.12759", "abs": "https://arxiv.org/abs/2505.12759", "authors": ["Haochen Yuan", "Minting Pan", "Yunbo Wang", "Siyu Gao", "Philip S. Yu", "Xiaokang Yang"], "title": "Your Offline Policy is Not Trustworthy: Bilevel Reinforcement Learning for Sequential Portfolio Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has shown significant promise for sequential\nportfolio optimization tasks, such as stock trading, where the objective is to\nmaximize cumulative returns while minimizing risks using historical data.\nHowever, traditional RL approaches often produce policies that merely memorize\nthe optimal yet impractical buying and selling behaviors within the fixed\ndataset. These offline policies are less generalizable as they fail to account\nfor the non-stationary nature of the market. Our approach, MetaTrader, frames\nportfolio optimization as a new type of partial-offline RL problem and makes\ntwo technical contributions. First, MetaTrader employs a bilevel learning\nframework that explicitly trains the RL agent to improve both in-domain profits\non the original dataset and out-of-domain performance across diverse\ntransformations of the raw financial data. Second, our approach incorporates a\nnew temporal difference (TD) method that approximates worst-case TD estimates\nfrom a batch of transformed TD targets, addressing the value overestimation\nissue that is particularly challenging in scenarios with limited offline data.\nOur empirical results on two public stock datasets show that MetaTrader\noutperforms existing methods, including both RL-based approaches and\ntraditional stock prediction models."}
{"id": "2505.11572", "pdf": "https://arxiv.org/pdf/2505.11572", "abs": "https://arxiv.org/abs/2505.11572", "authors": ["Anand Rai", "Satyam Rahangdale", "Utkarsh Anand", "Animesh Mukherjee"], "title": "ASR-FAIRBENCH: Measuring and Benchmarking Equity Across Speech Recognition Systems", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Paper accepted at INTERSPEECH 2025", "summary": "Automatic Speech Recognition (ASR) systems have become ubiquitous in everyday\napplications, yet significant disparities in performance across diverse\ndemographic groups persist. In this work, we introduce the ASR-FAIRBENCH\nleaderboard which is designed to assess both the accuracy and equity of ASR\nmodels in real-time. Leveraging the Meta's Fair-Speech dataset, which captures\ndiverse demographic characteristics, we employ a mixed-effects Poisson\nregression model to derive an overall fairness score. This score is integrated\nwith traditional metrics like Word Error Rate (WER) to compute the Fairness\nAdjusted ASR Score (FAAS), providing a comprehensive evaluation framework. Our\napproach reveals significant performance disparities in SOTA ASR models across\ndemographic groups and offers a benchmark to drive the development of more\ninclusive ASR technologies."}
{"id": "2505.11912", "pdf": "https://arxiv.org/pdf/2505.11912", "abs": "https://arxiv.org/abs/2505.11912", "authors": ["Paul Saves", "Nicolas Verstaevel", "Benoît Gaudou"], "title": "Modèles de Substitution pour les Modèles à base d'Agents : Enjeux, Méthodes et Applications", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "12 pages, in French language. Les 33\\`emes Journ\\'ees Francophones\n  sur les Syst\\`emes Multi-Agents (JFSMA 2025). 2025", "summary": "Multi-agent simulations enables the modeling and analyses of the dynamic\nbehaviors and interactions of autonomous entities evolving in complex\nenvironments. Agent-based models (ABM) are widely used to study emergent\nphenomena arising from local interactions. However, their high computational\ncost poses a significant challenge, particularly for large-scale simulations\nrequiring extensive parameter exploration, optimization, or uncertainty\nquantification. The increasing complexity of ABM limits their feasibility for\nreal-time decision-making and large-scale scenario analysis. To address these\nlimitations, surrogate models offer an efficient alternative by learning\napproximations from sparse simulation data. These models provide\ncheap-to-evaluate predictions, significantly reducing computational costs while\nmaintaining accuracy. Various machine learning techniques, including regression\nmodels, neural networks, random forests and Gaussian processes, have been\napplied to construct robust surrogates. Moreover, uncertainty quantification\nand sensitivity analysis play a crucial role in enhancing model reliability and\ninterpretability.\n  This article explores the motivations, methods, and applications of surrogate\nmodeling for ABM, emphasizing the trade-offs between accuracy, computational\nefficiency, and interpretability. Through a case study on a segregation model,\nwe highlight the challenges associated with building and validating surrogate\nmodels, comparing different approaches and evaluating their performance.\nFinally, we discuss future perspectives on integrating surrogate models within\nABM to improve scalability, explainability, and real-time decision support\nacross various fields such as ecology, urban planning and economics."}
{"id": "2505.12761", "pdf": "https://arxiv.org/pdf/2505.12761", "abs": "https://arxiv.org/abs/2505.12761", "authors": ["Donghwa Shin", "Edwin Zhang"], "title": "Enhancing Channel-Independent Time-Series Forecasting via Cross-Variate Patch Embedding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Transformers have recently gained popularity in time series forecasting due\nto their ability to capture long-term dependencies. However, many existing\nmodels focus only on capturing temporal dependencies while omitting intricate\nrelationships between variables. Recent models have tried tackling this by\nexplicitly modeling both cross-time and cross-variate dependencies through a\nsequential or unified attention mechanism, but they are entirely channel\ndependent (CD) across all layers, making them potentially susceptible to\noverfitting. To address this, we propose Cross-Variate Patch Embeddings (CVPE),\na lightweight CD module that injects cross-variate context into\nchannel-independent (CI) models by simply modifying the patch embedding\nprocess. We achieve this by adding a learnable positional encoding and a\nlightweight router-attention block to the vanilla patch embedding layer. We\nthen integrate CVPE into Time-LLM, a multimodal CI forecasting model, to\ndemonstrate its effectiveness in capturing cross-variate dependencies and\nenhance the CI model's performance. Extensive experimental results on seven\nreal-world datasets show that our enhanced Time-LLM outperforms the original\nbaseline model simply by incorporating the CVPE module, with no other changes."}
{"id": "2505.11595", "pdf": "https://arxiv.org/pdf/2505.11595", "abs": "https://arxiv.org/abs/2505.11595", "authors": ["Peter Chen", "Xiaopeng Li", "Ziniu Li", "Xi Chen", "Tianyi Lin"], "title": "Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "28 pages", "summary": "Reinforcement learning (RL) has demonstrated significant success in enhancing\nreasoning capabilities in large language models (LLMs). One of the most widely\nused RL methods is Group Relative Policy Optimization\n(GRPO)~\\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and\nsuccess in training DeepSeek-R1~\\cite{Guo-2025-Deepseek}. However, GRPO stalls\nwhen all sampled responses in a group are incorrect -- referred to as an\n\\emph{all-negative-sample} group -- as it fails to update the policy, hindering\nlearning progress. The contributions of this paper are two-fold. First, we\npropose a simple yet effective framework that introduces response diversity\nwithin all-negative-sample groups in GRPO using AI feedback. We also provide a\ntheoretical analysis, via a stylized model, showing how this diversification\nimproves learning dynamics. Second, we empirically validate our approach,\nshowing the improved performance across various model sizes (7B, 14B, 32B) in\nboth offline and online learning settings with 10 benchmarks, including base\nand distilled variants. Our findings highlight that learning from\nall-negative-sample groups is not only feasible but beneficial, advancing\nrecent insights from \\citet{Xiong-2025-Minimalist}."}
{"id": "2505.11924", "pdf": "https://arxiv.org/pdf/2505.11924", "abs": "https://arxiv.org/abs/2505.11924", "authors": ["Yu-Ting Lee", "Hui-Ying Shih", "Fu-Chieh Chang", "Pei-Yuan Wu"], "title": "An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We provide an explanation for the performance gains of intrinsic\nself-correction, a process where a language model iteratively refines its\noutputs without external feedback. More precisely, we investigate how prompting\ninduces interpretable changes in hidden states and thus affects the output\ndistributions. We hypothesize that each prompt-induced shift lies in a linear\nspan of some linear representation vectors, naturally separating tokens based\non individual concept alignment. Building around this idea, we give a\nmathematical formulation of self-correction and derive a concentration result\nfor output tokens based on alignment magnitudes. Our experiments on text\ndetoxification with zephyr-7b-sft reveal a substantial gap in the inner\nproducts of the prompt-induced shifts and the unembeddings of the top-100 most\ntoxic tokens vs. those of the unembeddings of the bottom-100 least toxic\ntokens, under toxic instructions. This suggests that self-correction prompts\nenhance a language model's capability of latent concept recognition. Our\nanalysis offers insights into the underlying mechanism of self-correction by\ncharacterizing how prompting works explainably. For reproducibility, our code\nis available."}
{"id": "2505.12763", "pdf": "https://arxiv.org/pdf/2505.12763", "abs": "https://arxiv.org/abs/2505.12763", "authors": ["Sunghwan Kim", "Dongjin Kang", "Taeyoon Kwon", "Hyungjoo Chae", "Dongha Lee", "Jinyoung Yeo"], "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Reward models (RMs) play a crucial role in reinforcement learning from human\nfeedback (RLHF), aligning model behavior with human preferences. However,\nexisting benchmarks for reward models show a weak correlation with the\nperformance of optimized policies, suggesting that they fail to accurately\nassess the true capabilities of RMs. To bridge this gap, we explore several\nevaluation designs through the lens of reward overoptimization\\textemdash a\nphenomenon that captures both how well the reward model aligns with human\npreferences and the dynamics of the learning signal it provides to the policy.\nThe results highlight three key findings on how to construct a reliable\nbenchmark: (i) it is important to minimize differences between chosen and\nrejected responses beyond correctness, (ii) evaluating reward models requires\nmultiple comparisons across a wide range of chosen and rejected responses, and\n(iii) given that reward models encounter responses with diverse\nrepresentations, responses should be sourced from a variety of models. However,\nwe also observe that a extremely high correlation with degree of\noveroptimization leads to comparatively lower correlation with certain\ndownstream performance. Thus, when designing a benchmark, it is desirable to\nuse the degree of overoptimization as a useful tool, rather than the end goal."}
{"id": "2505.11611", "pdf": "https://arxiv.org/pdf/2505.11611", "abs": "https://arxiv.org/abs/2505.11611", "authors": ["Bofan Gong", "Shiyang Lai", "Dawn Song"], "title": "Probing the Vulnerability of Large Language Models to Polysemantic Interventions", "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Polysemanticity -- where individual neurons encode multiple unrelated\nfeatures -- is a well-known characteristic of large neural networks and remains\na central challenge in the interpretability of language models. At the same\ntime, its implications for model safety are also poorly understood. Leveraging\nrecent advances in sparse autoencoders, we investigate the polysemantic\nstructure of two small models (Pythia-70M and GPT-2-Small) and evaluate their\nvulnerability to targeted, covert interventions at the prompt, feature, token,\nand neuron levels. Our analysis reveals a consistent polysemantic topology\nshared across both models. Strikingly, we demonstrate that this structure can\nbe exploited to mount effective interventions on two larger, black-box\ninstruction-tuned models (LLaMA3.1-8B-Instruct and Gemma-2-9B-Instruct). These\nfindings suggest not only the generalizability of the interventions but also\npoint to a stable and transferable polysemantic structure that could\npotentially persist across architectures and training regimes."}
{"id": "2505.11926", "pdf": "https://arxiv.org/pdf/2505.11926", "abs": "https://arxiv.org/abs/2505.11926", "authors": ["Yixu Wang", "Jiaxin Song", "Yifeng Gao", "Xin Wang", "Yang Yao", "Yan Teng", "Xingjun Ma", "Yingchun Wang", "Yu-Gang Jiang"], "title": "SafeVid: Toward Safety Aligned Video Large Multimodal Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As Video Large Multimodal Models (VLMMs) rapidly advance, their inherent\ncomplexity introduces significant safety challenges, particularly the issue of\nmismatched generalization where static safety alignments fail to transfer to\ndynamic video contexts. We introduce SafeVid, a framework designed to instill\nvideo-specific safety principles in VLMMs. SafeVid uniquely transfers robust\ntextual safety alignment capabilities to the video domain by employing detailed\ntextual video descriptions as an interpretive bridge, facilitating LLM-based\nrule-driven safety reasoning. This is achieved through a closed-loop system\ncomprising: 1) generation of SafeVid-350K, a novel 350,000-pair video-specific\nsafety preference dataset; 2) targeted alignment of VLMMs using Direct\nPreference Optimization (DPO); and 3) comprehensive evaluation via our new\nSafeVidBench benchmark. Alignment with SafeVid-350K significantly enhances VLMM\nsafety, with models like LLaVA-NeXT-Video demonstrating substantial\nimprovements (e.g., up to 42.39%) on SafeVidBench. SafeVid provides critical\nresources and a structured approach, demonstrating that leveraging textual\ndescriptions as a conduit for safety reasoning markedly improves the safety\nalignment of VLMMs. We have made SafeVid-350K dataset\n(https://huggingface.co/datasets/yxwang/SafeVid-350K) publicly available."}
{"id": "2505.12805", "pdf": "https://arxiv.org/pdf/2505.12805", "abs": "https://arxiv.org/abs/2505.12805", "authors": ["Seanie Lee", "Sangwoo Park", "Dong Bok Lee", "Dominik Wagner", "Haebin Seong", "Tobias Bocklet", "Juho Lee", "Sung Ju Hwang"], "title": "FedSVD: Adaptive Orthogonalization for Private Federated Learning with LoRA", "categories": ["cs.LG", "cs.AI"], "comment": "preprint", "summary": "Low-Rank Adaptation (LoRA), which introduces a product of two trainable\nlow-rank matrices into frozen pre-trained weights, is widely used for efficient\nfine-tuning of language models in federated learning (FL). However, when\ncombined with differentially private stochastic gradient descent (DP-SGD), LoRA\nfaces substantial noise amplification: DP-SGD perturbs per-sample gradients,\nand the matrix multiplication of the LoRA update ($BA$) intensifies this\neffect. Freezing one matrix (e.g., $A$) reduces the noise but restricts model\nexpressiveness, often resulting in suboptimal adaptation. To address this, we\npropose FedSVD, a simple yet effective method that introduces a global\nreparameterization based on singular value decomposition (SVD). In our\napproach, each client optimizes only the $B$ matrix and transmits it to the\nserver. The server aggregates the $B$ matrices, computes the product $BA$ using\nthe previous $A$, and refactorizes the result via SVD. This yields a new\nadaptive $A$ composed of the orthonormal right singular vectors of $BA$, and an\nupdated $B$ containing the remaining SVD components. This reparameterization\navoids quadratic noise amplification, while allowing $A$ to better capture the\nprincipal directions of the aggregate updates. Moreover, the orthonormal\nstructure of $A$ bounds the gradient norms of $B$ and preserves more signal\nunder DP-SGD, as confirmed by our theoretical analysis. As a result, FedSVD\nconsistently improves stability and performance across a variety of privacy\nsettings and benchmarks, outperforming relevant baselines under both private\nand non-private regimes."}
{"id": "2505.11614", "pdf": "https://arxiv.org/pdf/2505.11614", "abs": "https://arxiv.org/abs/2505.11614", "authors": ["Jian-Qiao Zhu", "Hanbo Xie", "Dilip Arumugam", "Robert C. Wilson", "Thomas L. Griffiths"], "title": "Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "A central goal of cognitive modeling is to develop models that not only\npredict human behavior but also provide insight into the underlying cognitive\nmechanisms. While neural network models trained on large-scale behavioral data\noften achieve strong predictive performance, they typically fall short in\noffering interpretable explanations of the cognitive processes they capture. In\nthis work, we explore the potential of pretrained large language models (LLMs)\nto serve as dual-purpose cognitive models--capable of both accurate prediction\nand interpretable explanation in natural language. Specifically, we employ\nreinforcement learning with outcome-based rewards to guide LLMs toward\ngenerating explicit reasoning traces for explaining human risky choices. Our\nfindings demonstrate that this approach produces high-quality explanations\nalongside strong quantitative predictions of human decisions."}
{"id": "2505.11930", "pdf": "https://arxiv.org/pdf/2505.11930", "abs": "https://arxiv.org/abs/2505.11930", "authors": ["Marco Sälzer", "Przemysław Andrzej Wałęga", "Martin Lange"], "title": "The Logical Expressiveness of Temporal GNNs via Two-Dimensional Product Logics", "categories": ["cs.LG", "cs.AI", "cs.LO"], "comment": null, "summary": "In recent years, the expressive power of various neural architectures --\nincluding graph neural networks (GNNs), transformers, and recurrent neural\nnetworks -- has been characterised using tools from logic and formal language\ntheory. As the capabilities of basic architectures are becoming well\nunderstood, increasing attention is turning to models that combine multiple\narchitectural paradigms. Among them particularly important, and challenging to\nanalyse, are temporal extensions of GNNs, which integrate both spatial\n(graph-structure) and temporal (evolution over time) dimensions. In this paper,\nwe initiate the study of logical characterisation of temporal GNNs by\nconnecting them to two-dimensional product logics. We show that the expressive\npower of temporal GNNs depends on how graph and temporal components are\ncombined. In particular, temporal GNNs that apply static GNNs recursively over\ntime can capture all properties definable in the product logic of (past)\npropositional temporal logic PTL and the modal logic K. In contrast,\narchitectures such as graph-and-time TGNNs and global TGNNs can only express\nrestricted fragments of this logic, where the interaction between temporal and\nspatial operators is syntactically constrained. These results yield the first\nlogical characterisations of temporal GNNs and establish new relative\nexpressiveness results for temporal GNNs."}
{"id": "2505.12809", "pdf": "https://arxiv.org/pdf/2505.12809", "abs": "https://arxiv.org/abs/2505.12809", "authors": ["Nishant Suresh Aswani", "Saif Eddin Jabari"], "title": "Koopman Autoencoders Learn Neural Representation Dynamics", "categories": ["cs.LG"], "comment": null, "summary": "This paper explores a simple question: can we model the internal\ntransformations of a neural network using dynamical systems theory? We\nintroduce Koopman autoencoders to capture how neural representations evolve\nthrough network layers, treating these representations as states in a dynamical\nsystem. Our approach learns a surrogate model that predicts how neural\nrepresentations transform from input to output, with two key advantages. First,\nby way of lifting the original states via an autoencoder, it operates in a\nlinear space, making editing the dynamics straightforward. Second, it preserves\nthe topologies of the original representations by regularizing the autoencoding\nobjective. We demonstrate that these surrogate models naturally replicate the\nprogressive topological simplification observed in neural networks. As a\npractical application, we show how our approach enables targeted class\nunlearning in the Yin-Yang and MNIST classification tasks."}
{"id": "2505.11717", "pdf": "https://arxiv.org/pdf/2505.11717", "abs": "https://arxiv.org/abs/2505.11717", "authors": ["Xilong Wang", "John Bloch", "Zedian Shao", "Yuepeng Hu", "Shuyan Zhou", "Neil Zhenqiang Gong"], "title": "EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Multi-modal large language model (MLLM)-based web agents interact with\nwebpage environments by generating actions based on screenshots of the\nwebpages. Environmental prompt injection attacks manipulate the environment to\ninduce the web agent to perform a specific, attacker-chosen action--referred to\nas the target action. However, existing attacks suffer from limited\neffectiveness or stealthiness, or are impractical in real-world settings. In\nthis work, we propose EnvInjection, a new attack that addresses these\nlimitations. Our attack adds a perturbation to the raw pixel values of the\nrendered webpage, which can be implemented by modifying the webpage's source\ncode. After these perturbed pixels are mapped into a screenshot, the\nperturbation induces the web agent to perform the target action. We formulate\nthe task of finding the perturbation as an optimization problem. A key\nchallenge in solving this problem is that the mapping between raw pixel values\nand screenshot is non-differentiable, making it difficult to backpropagate\ngradients to the perturbation. To overcome this, we train a neural network to\napproximate the mapping and apply projected gradient descent to solve the\nreformulated optimization problem. Extensive evaluation on multiple webpage\ndatasets shows that EnvInjection is highly effective and significantly\noutperforms existing baselines."}
{"id": "2505.11933", "pdf": "https://arxiv.org/pdf/2505.11933", "abs": "https://arxiv.org/abs/2505.11933", "authors": ["Piyush Talegaonkar", "Siddhant Hole", "Shrinesh Kamble", "Prashil Gulechha", "Deepali Salapurkar"], "title": "Conversational Recommendation System using NLP and Sentiment Analysis", "categories": ["cs.IR", "cs.AI"], "comment": "Presented in ISETE conference (International Conference on Artificial\n  Intelligence, Machine Learning and Big Data Engineering 2024)", "summary": "In today's digitally-driven world, the demand for personalized and\ncontext-aware recommendations has never been greater. Traditional recommender\nsystems have made significant strides in this direction, but they often lack\nthe ability to tap into the richness of conversational data. This paper\nrepresents a novel approach to recommendation systems by integrating\nconversational insights into the recommendation process. The Conversational\nRecommender System integrates cutting-edge technologies such as deep learning,\nleveraging machine learning algorithms like Apriori for Association Rule\nMining, Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN),\nand Long Short-Term Memory (LTSM). Furthermore, sophisticated voice recognition\ntechnologies, including Hidden Markov Models (HMMs) and Dynamic Time Warping\n(DTW) algorithms, play a crucial role in accurate speech-to-text conversion,\nensuring robust performance in diverse environments. The methodology\nincorporates a fusion of content-based and collaborative recommendation\napproaches, enhancing them with NLP techniques. This innovative integration\nensures a more personalized and context-aware recommendation experience,\nparticularly in marketing applications."}
{"id": "2505.12825", "pdf": "https://arxiv.org/pdf/2505.12825", "abs": "https://arxiv.org/abs/2505.12825", "authors": ["Qin-Cheng Zheng", "Shao-Qun Zhang", "Shen-Huan Lyu", "Yuan Jiang", "Zhi-Hua Zhou"], "title": "Theoretical Investigation on Inductive Bias of Isolation Forest", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Isolation Forest (iForest) stands out as a widely-used unsupervised anomaly\ndetector valued for its exceptional runtime efficiency and performance on\nlarge-scale tasks. Despite its widespread adoption, a theoretical foundation\nexplaining iForest's success remains unclear. This paper theoretically\ninvestigates the conditions and extent of iForest's effectiveness by analyzing\nits inductive bias through the formulation of depth functions and growth\nprocesses. Since directly analyzing the depth function proves intractable due\nto iForest's random splitting mechanism, we model the growth process of iForest\nas a random walk, enabling us to derive the expected depth function using\ntransition probabilities. Our case studies reveal key inductive biases: iForest\nexhibits lower sensitivity to central anomalies while demonstrating greater\nparameter adaptability compared to $k$-Nearest Neighbor anomaly detectors. Our\nstudy provides theoretical understanding of the effectiveness of iForest and\nestablishes a foundation for further theoretical exploration."}
{"id": "2505.11731", "pdf": "https://arxiv.org/pdf/2505.11731", "abs": "https://arxiv.org/abs/2505.11731", "authors": ["Harshil Vejendla", "Haizhou Shi", "Yibin Wang", "Tunyu Zhang", "Huan Zhang", "Hao Wang"], "title": "Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; work in progress", "summary": "Recent advances in uncertainty estimation for Large Language Models (LLMs)\nduring downstream adaptation have addressed key challenges of reliability and\nsimplicity. However, existing Bayesian methods typically require multiple\nsampling iterations during inference, creating significant efficiency issues\nthat limit practical deployment. In this paper, we investigate the possibility\nof eliminating the need for test-time sampling for LLM uncertainty estimation.\nSpecifically, when given an off-the-shelf Bayesian LLM, we distill its aligned\nconfidence into a non-Bayesian student LLM by minimizing the divergence between\ntheir predictive distributions. Unlike typical calibration methods, our\ndistillation is carried out solely on the training dataset without the need of\nan additional validation dataset. This simple yet effective approach achieves\nN-times more efficient uncertainty estimation during testing, where N is the\nnumber of samples traditionally required by Bayesian LLMs. Our extensive\nexperiments demonstrate that uncertainty estimation capabilities on training\ndata can successfully generalize to unseen test data through our distillation\ntechnique, consistently producing results comparable to (or even better than)\nstate-of-the-art Bayesian LLMs."}
{"id": "2505.11936", "pdf": "https://arxiv.org/pdf/2505.11936", "abs": "https://arxiv.org/abs/2505.11936", "authors": ["Jingren Liu", "Zhong Ji", "Xiangyu Chen"], "title": "How can Diffusion Models Evolve into Continual Generators?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While diffusion models have achieved remarkable success in static data\ngeneration, their deployment in streaming or continual learning (CL) scenarios\nfaces a major challenge: catastrophic forgetting (CF), where newly acquired\ngenerative capabilities overwrite previously learned ones. To systematically\naddress this, we introduce a formal Continual Diffusion Generation (CDG)\nparadigm that characterizes and redefines CL in the context of generative\ndiffusion models. Prior efforts often adapt heuristic strategies from continual\nclassification tasks but lack alignment with the underlying diffusion process.\nIn this work, we develop the first theoretical framework for CDG by analyzing\ncross-task dynamics in diffusion-based generative modeling. Our analysis\nreveals that the retention and stability of generative knowledge across tasks\nare governed by three key consistency criteria: inter-task knowledge\nconsistency (IKC), unconditional knowledge consistency (UKC), and label\nknowledge consistency (LKC). Building on these insights, we propose Continual\nConsistency Diffusion (CCD), a principled framework that integrates these\nconsistency objectives into training via hierarchical loss terms\n$\\mathcal{L}_{IKC}$, $\\mathcal{L}_{UKC}$, and $\\mathcal{L}_{LKC}$. This\npromotes effective knowledge retention while enabling the assimilation of new\ngenerative capabilities. Extensive experiments on four benchmark datasets\ndemonstrate that CCD achieves state-of-the-art performance under continual\nsettings, with substantial gains in Mean Fidelity (MF) and Incremental Mean\nFidelity (IMF), particularly in tasks with rich cross-task knowledge overlap."}
{"id": "2505.12842", "pdf": "https://arxiv.org/pdf/2505.12842", "abs": "https://arxiv.org/abs/2505.12842", "authors": ["Zheng Wu", "Pengzhou Cheng", "Zongru Wu", "Lingzhong Dong", "Zhuosheng Zhang"], "title": "GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Graphical user interface (GUI) agents have recently emerged as an intriguing\nparadigm for human-computer interaction, capable of automatically executing\nuser instructions to operate intelligent terminal devices. However, when\nencountering out-of-distribution (OOD) instructions that violate environmental\nconstraints or exceed the current capabilities of agents, GUI agents may suffer\ntask breakdowns or even pose security threats. Therefore, effective OOD\ndetection for GUI agents is essential. Traditional OOD detection methods\nperform suboptimally in this domain due to the complex embedding space and\nevolving GUI environments. In this work, we observe that the in-distribution\ninput semantic space of GUI agents exhibits a clustering pattern with respect\nto the distance from the centroid. Based on the finding, we propose GEM, a\nnovel method based on fitting a Gaussian mixture model over input embedding\ndistances extracted from the GUI Agent that reflect its capability boundary.\nEvaluated on eight datasets spanning smartphones, computers, and web browsers,\nour method achieves an average accuracy improvement of 23.70\\% over the\nbest-performing baseline. Analysis verifies the generalization ability of our\nmethod through experiments on nine different backbones. The codes are available\nat https://github.com/Wuzheng02/GEM-OODforGUIagents."}
{"id": "2505.11737", "pdf": "https://arxiv.org/pdf/2505.11737", "abs": "https://arxiv.org/abs/2505.11737", "authors": ["Tunyu Zhang", "Haizhou Shi", "Yibin Wang", "Hengyi Wang", "Xiaoxiao He", "Zhuowei Li", "Haoxian Chen", "Ligong Han", "Kai Xu", "Huan Zhang", "Dimitris Metaxas", "Hao Wang"], "title": "Token-Level Uncertainty Estimation for Large Language Model Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; Work in progress", "summary": "While Large Language Models (LLMs) have demonstrated impressive capabilities,\ntheir output quality remains inconsistent across various application scenarios,\nmaking it difficult to identify trustworthy responses, especially in complex\ntasks requiring multi-step reasoning. In this paper, we propose a token-level\nuncertainty estimation framework to enable LLMs to self-assess and self-improve\ntheir generation quality in mathematical reasoning. Specifically, we introduce\nlow-rank random weight perturbation to LLM decoding, generating predictive\ndistributions that we use to estimate token-level uncertainties. We then\naggregate these uncertainties to reflect semantic uncertainty of the generated\nsequences. Experiments on mathematical reasoning datasets of varying difficulty\ndemonstrate that our token-level uncertainty metrics strongly correlate with\nanswer correctness and model robustness. Additionally, we explore using\nuncertainty to directly enhance the model's reasoning performance through\nmultiple generations and the particle filtering algorithm. Our approach\nconsistently outperforms existing uncertainty estimation methods, establishing\neffective uncertainty estimation as a valuable tool for both evaluating and\nimproving reasoning generation in LLMs."}
{"id": "2505.11939", "pdf": "https://arxiv.org/pdf/2505.11939", "abs": "https://arxiv.org/abs/2505.11939", "authors": ["Haitao Li", "Che Liu", "Zhengyao Ding", "Ziyi Liu", "Zhengxing Huang"], "title": "Fine-Grained ECG-Text Contrastive Learning via Waveform Understanding Enhancement", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Electrocardiograms (ECGs) are essential for diagnosing cardiovascular\ndiseases. While previous ECG-text contrastive learning methods have shown\npromising results, they often overlook the incompleteness of the reports. Given\nan ECG, the report is generated by first identifying key waveform features and\nthen inferring the final diagnosis through these features. Despite their\nimportance, these waveform features are often not recorded in the report as\nintermediate results. Aligning ECGs with such incomplete reports impedes the\nmodel's ability to capture the ECG's waveform features and limits its\nunderstanding of diagnostic reasoning based on those features. To address this,\nwe propose FG-CLEP (Fine-Grained Contrastive Language ECG Pre-training), which\naims to recover these waveform features from incomplete reports with the help\nof large language models (LLMs), under the challenges of hallucinations and the\nnon-bijective relationship between waveform features and diagnoses.\nAdditionally, considering the frequent false negatives due to the prevalence of\ncommon diagnoses in ECGs, we introduce a semantic similarity matrix to guide\ncontrastive learning. Furthermore, we adopt a sigmoid-based loss function to\naccommodate the multi-label nature of ECG-related tasks. Experiments on six\ndatasets demonstrate that FG-CLEP outperforms state-of-the-art methods in both\nzero-shot prediction and linear probing across these datasets."}
{"id": "2505.12843", "pdf": "https://arxiv.org/pdf/2505.12843", "abs": "https://arxiv.org/abs/2505.12843", "authors": ["Kangwen Zhao", "Jianfeng Cai", "Jinhua Zhu", "Ruopei Sun", "Dongyun Xue", "Wengang Zhou", "Li Li", "Houqiang Li"], "title": "Bias Fitting to Mitigate Length Bias of Reward Model in RLHF", "categories": ["cs.LG", "cs.AI"], "comment": "Due to the word limit for arXiv abstract, the abstract here has been\n  abridged compared to the one in the PDF", "summary": "Reinforcement Learning from Human Feedback relies on reward models to align\nlarge language models with human preferences. However, RLHF often suffers from\nreward hacking, wherein policy learning exploits flaws in the trained reward\nmodel to maximize reward scores without genuinely aligning with human\npreferences. A significant example of such reward hacking is length bias, where\nreward models usually favor longer responses irrespective of actual response\nquality. Previous works on length bias have notable limitations, these\napproaches either mitigate bias without characterizing the bias form, or simply\nassume a linear length-reward relation. To accurately model the intricate\nnature of length bias and facilitate more effective bias mitigation, we propose\nFiMi-RM (Bias Fitting to Mitigate Length Bias of Reward Model in RLHF), a\nframework that autonomously learns and corrects underlying bias patterns. Our\napproach consists of three stages: First, we train a standard reward model\nwhich inherently contains length bias. Next, we deploy a lightweight fitting\nmodel to explicitly capture the non-linear relation between length and reward.\nFinally, we incorporate this learned relation into the reward model to debias.\nExperimental results demonstrate that FiMi-RM achieves a more balanced\nlength-reward distribution. Furthermore, when applied to alignment algorithms,\nour debiased reward model improves length-controlled win rate and reduces\nverbosity without compromising its performance."}
{"id": "2505.11756", "pdf": "https://arxiv.org/pdf/2505.11756", "abs": "https://arxiv.org/abs/2505.11756", "authors": ["David Chanin", "Tomáš Dulka", "Adrià Garriga-Alonso"], "title": "Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "It is assumed that sparse autoencoders (SAEs) decompose polysemantic\nactivations into interpretable linear directions, as long as the activations\nare composed of sparse linear combinations of underlying features. However, we\nfind that if an SAE is more narrow than the number of underlying \"true\nfeatures\" on which it is trained, and there is correlation between features,\nthe SAE will merge components of correlated features together, thus destroying\nmonosemanticity. In LLM SAEs, these two conditions are almost certainly true.\nThis phenomenon, which we call feature hedging, is caused by SAE reconstruction\nloss, and is more severe the narrower the SAE. In this work, we introduce the\nproblem of feature hedging and study it both theoretically in toy models and\nempirically in SAEs trained on LLMs. We suspect that feature hedging may be one\nof the core reasons that SAEs consistently underperform supervised baselines.\nFinally, we use our understanding of feature hedging to propose an improved\nvariant of matryoshka SAEs. Our work shows there remain fundamental issues with\nSAEs, but we are hopeful that that highlighting feature hedging will catalyze\nfuture advances that allow SAEs to achieve their full potential of interpreting\nLLMs at scale."}
{"id": "2505.11946", "pdf": "https://arxiv.org/pdf/2505.11946", "abs": "https://arxiv.org/abs/2505.11946", "authors": ["Adam Kovari", "Yasin Ghafourian", "Csaba Hegedus", "Belal Abu Naim", "Kitti Mezei", "Pal Varga", "Markus Tauber"], "title": "Let's have a chat with the EU AI Act", "categories": ["cs.IR", "cs.AI", "cs.CY", "cs.DL", "cs.LG"], "comment": null, "summary": "As artificial intelligence (AI) regulations evolve and the regulatory\nlandscape develops and becomes more complex, ensuring compliance with ethical\nguidelines and legal frameworks remains a challenge for AI developers. This\npaper introduces an AI-driven self-assessment chatbot designed to assist users\nin navigating the European Union AI Act and related standards. Leveraging a\nRetrieval-Augmented Generation (RAG) framework, the chatbot enables real-time,\ncontext-aware compliance verification by retrieving relevant regulatory texts\nand providing tailored guidance. By integrating both public and proprietary\nstandards, it streamlines regulatory adherence, reduces complexity, and fosters\nresponsible AI development. The paper explores the chatbot's architecture,\ncomparing naive and graph-based RAG models, and discusses its potential impact\non AI governance."}
{"id": "2505.12871", "pdf": "https://arxiv.org/pdf/2505.12871", "abs": "https://arxiv.org/abs/2505.12871", "authors": ["Zi Liang", "Haibo Hu", "Qingqing Ye", "Yaxin Xiao", "Ronghua Li"], "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "To appear at ICML 25", "summary": "Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings."}
{"id": "2505.11770", "pdf": "https://arxiv.org/pdf/2505.11770", "abs": "https://arxiv.org/abs/2505.11770", "authors": ["Jing Huang", "Junyi Tao", "Thomas Icard", "Diyi Yang", "Christopher Potts"], "title": "Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "ICML 2025", "summary": "Interpretability research now offers a variety of techniques for identifying\nabstract internal mechanisms in neural networks. Can such techniques be used to\npredict how models will behave on out-of-distribution examples? In this work,\nwe provide a positive answer to this question. Through a diverse set of\nlanguage modeling tasks--including symbol manipulation, knowledge retrieval,\nand instruction following--we show that the most robust features for\ncorrectness prediction are those that play a distinctive causal role in the\nmodel's behavior. Specifically, we propose two methods that leverage causal\nmechanisms to predict the correctness of model outputs: counterfactual\nsimulation (checking whether key causal variables are realized) and value\nprobing (using the values of those variables to make predictions). Both achieve\nhigh AUC-ROC in distribution and outperform methods that rely on\ncausal-agnostic features in out-of-distribution settings, where predicting\nmodel behaviors is more crucial. Our work thus highlights a novel and\nsignificant application for internal causal analysis of language models."}
{"id": "2505.11953", "pdf": "https://arxiv.org/pdf/2505.11953", "abs": "https://arxiv.org/abs/2505.11953", "authors": ["Puning Yang", "Qizhou Wang", "Zhuo Huang", "Tongliang Liu", "Chengqi Zhang", "Bo Han"], "title": "Exploring Criteria of Loss Reweighting to Enhance LLM Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Loss reweighting has shown significant benefits for machine unlearning with\nlarge language models (LLMs). However, their exact functionalities are left\nunclear and the optimal strategy remains an open question, thus impeding the\nunderstanding and improvement of existing methodologies. In this paper, we\nidentify two distinct goals of loss reweighting, namely, Saturation and\nImportance -- the former indicates that those insufficiently optimized data\nshould be emphasized, while the latter stresses some critical data that are\nmost influential for loss minimization. To study their usefulness, we design\nspecific reweighting strategies for each goal and evaluate their respective\neffects on unlearning. We conduct extensive empirical analyses on\nwell-established benchmarks, and summarize some important observations as\nfollows: (i) Saturation enhances efficacy more than importance-based\nreweighting, and their combination can yield additional improvements. (ii)\nSaturation typically allocates lower weights to data with lower likelihoods,\nwhereas importance-based reweighting does the opposite. (iii) The efficacy of\nunlearning is also largely influenced by the smoothness and granularity of the\nweight distributions. Based on these findings, we propose SatImp, a simple\nreweighting method that combines the advantages of both saturation and\nimportance. Empirical results on extensive datasets validate the efficacy of\nour method, potentially bridging existing research gaps and indicating\ndirections for future research. Our code is available at\nhttps://github.com/Puning97/SatImp-for-LLM-Unlearning."}
{"id": "2505.12880", "pdf": "https://arxiv.org/pdf/2505.12880", "abs": "https://arxiv.org/abs/2505.12880", "authors": ["Maksim Zhdanov", "Nabil Iqbal", "Erik Bekkers", "Patrick Forré"], "title": "AdS-GNN -- a Conformally Equivariant Graph Neural Network", "categories": ["cs.LG", "cs.AI", "hep-th"], "comment": null, "summary": "Conformal symmetries, i.e.\\ coordinate transformations that preserve angles,\nplay a key role in many fields, including physics, mathematics, computer vision\nand (geometric) machine learning. Here we build a neural network that is\nequivariant under general conformal transformations. To achieve this, we lift\ndata from flat Euclidean space to Anti de Sitter (AdS) space. This allows us to\nexploit a known correspondence between conformal transformations of flat space\nand isometric transformations on the AdS space. We then build upon the fact\nthat such isometric transformations have been extensively studied on general\ngeometries in the geometric deep learning literature. We employ message-passing\nlayers conditioned on the proper distance, yielding a computationally efficient\nframework. We validate our model on tasks from computer vision and statistical\nphysics, demonstrating strong performance, improved generalization capacities,\nand the ability to extract conformal data such as scaling dimensions from the\ntrained network."}
{"id": "2505.11812", "pdf": "https://arxiv.org/pdf/2505.11812", "abs": "https://arxiv.org/abs/2505.11812", "authors": ["Yang Tan", "Wenrui Gou", "Bozitao Zhong", "Liang Hong", "Huiqun Yu", "Bingxin Zhou"], "title": "VenusX: Unlocking Fine-Grained Functional Understanding of Proteins", "categories": ["cs.LG", "cs.CL", "q-bio.QM"], "comment": "29 pages, 3 figures, 17 tables", "summary": "Deep learning models have driven significant progress in predicting protein\nfunction and interactions at the protein level. While these advancements have\nbeen invaluable for many biological applications such as enzyme engineering and\nfunction annotation, a more detailed perspective is essential for understanding\nprotein functional mechanisms and evaluating the biological knowledge captured\nby models. To address this demand, we introduce VenusX, the first large-scale\nbenchmark for fine-grained functional annotation and function-based protein\npairing at the residue, fragment, and domain levels. VenusX comprises three\nmajor task categories across six types of annotations, including residue-level\nbinary classification, fragment-level multi-class classification, and pairwise\nfunctional similarity scoring for identifying critical active sites, binding\nsites, conserved sites, motifs, domains, and epitopes. The benchmark features\nover 878,000 samples curated from major open-source databases such as InterPro,\nBioLiP, and SAbDab. By providing mixed-family and cross-family splits at three\nsequence identity thresholds, our benchmark enables a comprehensive assessment\nof model performance on both in-distribution and out-of-distribution scenarios.\nFor baseline evaluation, we assess a diverse set of popular and open-source\nmodels, including pre-trained protein language models, sequence-structure\nhybrids, structure-based methods, and alignment-based techniques. Their\nperformance is reported across all benchmark datasets and evaluation settings\nusing multiple metrics, offering a thorough comparison and a strong foundation\nfor future research. Code and data are publicly available at\nhttps://github.com/ai4protein/VenusX."}
{"id": "2505.11963", "pdf": "https://arxiv.org/pdf/2505.11963", "abs": "https://arxiv.org/abs/2505.11963", "authors": ["Luca Collini", "Baleegh Ahmad", "Joey Ah-kiow", "Ramesh Karri"], "title": "MARVEL: Multi-Agent RTL Vulnerability Extraction using Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": "Submitted for Peer Review", "summary": "Hardware security verification is a challenging and time-consuming task. For\nthis purpose, design engineers may utilize tools such as formal verification,\nlinters, and functional simulation tests, coupled with analysis and a deep\nunderstanding of the hardware design being inspected. Large Language Models\n(LLMs) have been used to assist during this task, either directly or in\nconjunction with existing tools. We improve the state of the art by proposing\nMARVEL, a multi-agent LLM framework for a unified approach to decision-making,\ntool use, and reasoning. MARVEL mimics the cognitive process of a designer\nlooking for security vulnerabilities in RTL code. It consists of a supervisor\nagent that devises the security policy of the system-on-chips (SoCs) using its\nsecurity documentation. It delegates tasks to validate the security policy to\nindividual executor agents. Each executor agent carries out its assigned task\nusing a particular strategy. Each executor agent may use one or more tools to\nidentify potential security bugs in the design and send the results back to the\nsupervisor agent for further analysis and confirmation. MARVEL includes\nexecutor agents that leverage formal tools, linters, simulation tests,\nLLM-based detection schemes, and static analysis-based checks. We test our\napproach on a known buggy SoC based on OpenTitan from the Hack@DATE\ncompetition. We find that 20 of the 48 issues reported by MARVEL pose security\nvulnerabilities."}
{"id": "2505.12882", "pdf": "https://arxiv.org/pdf/2505.12882", "abs": "https://arxiv.org/abs/2505.12882", "authors": ["Hao Wang", "Jindong Han", "Wei Fan", "Weijia Zhang", "Hao Liu"], "title": "PhyDA: Physics-Guided Diffusion Models for Data Assimilation in Atmospheric Systems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Data Assimilation (DA) plays a critical role in atmospheric science by\nreconstructing spatially continous estimates of the system state, which serves\nas initial conditions for scientific analysis. While recent advances in\ndiffusion models have shown great potential for DA tasks, most existing\napproaches remain purely data-driven and often overlook the physical laws that\ngovern complex atmospheric dynamics. As a result, they may yield physically\ninconsistent reconstructions that impair downstream applications. To overcome\nthis limitation, we propose PhyDA, a physics-guided diffusion framework\ndesigned to ensure physical coherence in atmospheric data assimilation. PhyDA\nintroduces two key components: (1) a Physically Regularized Diffusion Objective\nthat integrates physical constraints into the training process by penalizing\ndeviations from known physical laws expressed as partial differential\nequations, and (2) a Virtual Reconstruction Encoder that bridges observational\nsparsity for structured latent representations, further enhancing the model's\nability to infer complete and physically coherent states. Experiments on the\nERA5 reanalysis dataset demonstrate that PhyDA achieves superior accuracy and\nbetter physical plausibility compared to state-of-the-art baselines. Our\nresults emphasize the importance of combining generative modeling with\ndomain-specific physical knowledge and show that PhyDA offers a promising\ndirection for improving real-world data assimilation systems."}
{"id": "2505.11842", "pdf": "https://arxiv.org/pdf/2505.11842", "abs": "https://arxiv.org/abs/2505.11842", "authors": ["Xuannan Liu", "Zekun Li", "Zheqi He", "Peipei Li", "Shuhan Xia", "Xing Cui", "Huaibo Huang", "Xi Yang", "Ran He"], "title": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "categories": ["cs.CV", "cs.CL"], "comment": "Project page:\n  https://liuxuannan.github.io/Video-SafetyBench.github.io/", "summary": "The increasing deployment of Large Vision-Language Models (LVLMs) raises\nsafety concerns under potential malicious inputs. However, existing multimodal\nsafety evaluations primarily focus on model vulnerabilities exposed by static\nimage inputs, ignoring the temporal dynamics of video that may induce distinct\nsafety risks. To bridge this gap, we introduce Video-SafetyBench, the first\ncomprehensive benchmark designed to evaluate the safety of LVLMs under\nvideo-text attacks. It comprises 2,264 video-text pairs spanning 48\nfine-grained unsafe categories, each pairing a synthesized video with either a\nharmful query, which contains explicit malice, or a benign query, which appears\nharmless but triggers harmful behavior when interpreted alongside the video. To\ngenerate semantically accurate videos for safety evaluation, we design a\ncontrollable pipeline that decomposes video semantics into subject images (what\nis shown) and motion text (how it moves), which jointly guide the synthesis of\nquery-relevant videos. To effectively evaluate uncertain or borderline harmful\noutputs, we propose RJScore, a novel LLM-based metric that incorporates the\nconfidence of judge models and human-aligned decision threshold calibration.\nExtensive experiments show that benign-query video composition achieves average\nattack success rates of 67.2%, revealing consistent vulnerabilities to\nvideo-induced attacks. We believe Video-SafetyBench will catalyze future\nresearch into video-based safety evaluation and defense strategies."}
{"id": "2505.11979", "pdf": "https://arxiv.org/pdf/2505.11979", "abs": "https://arxiv.org/abs/2505.11979", "authors": ["Tarik Houichime", "Younes El Amrani"], "title": "Introduction to Analytical Software Engineering Design Paradigm", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MS", "cs.PL"], "comment": "The Conference's autorization to submit a preprint was granted", "summary": "As modern software systems expand in scale and complexity, the challenges\nassociated with their modeling and formulation grow increasingly intricate.\nTraditional approaches often fall short in effectively addressing these\ncomplexities, particularly in tasks such as design pattern detection for\nmaintenance and assessment, as well as code refactoring for optimization and\nlong-term sustainability. This growing inadequacy underscores the need for a\nparadigm shift in how such challenges are approached and resolved. This paper\npresents Analytical Software Engineering (ASE), a novel design paradigm aimed\nat balancing abstraction, tool accessibility, compatibility, and scalability.\nASE enables effective modeling and resolution of complex software engineering\nproblems. The paradigm is evaluated through two frameworks\nBehavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR),\nboth developed in accordance with ASE principles. BSS offers a compact,\nlanguage-agnostic representation of codebases to facilitate precise design\npattern detection. ODR unifies artifact and solution representations to\noptimize code refactoring via heuristic algorithms while eliminating iterative\ncomputational overhead. By providing a structured approach to software design\nchallenges, ASE lays the groundwork for future research in encoding and\nanalyzing complex software metrics."}
{"id": "2505.12884", "pdf": "https://arxiv.org/pdf/2505.12884", "abs": "https://arxiv.org/abs/2505.12884", "authors": ["Yuanze Hu", "Zhaoxin Fan", "Xinyu Wang", "Gen Li", "Ye Qiu", "Zhichao Yang", "Wenjun Wu", "Kejian Wu", "Yifan Sun", "Xiaotie Deng", "Jin Dong"], "title": "TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Lightweight Vision-Language Models (VLMs) are indispensable for\nresource-constrained applications. The prevailing approach to aligning vision\nand language models involves freezing both the vision encoder and the language\nmodel while training small connector modules. However, this strategy heavily\ndepends on the intrinsic capabilities of the language model, which can be\nsuboptimal for lightweight models with limited representational capacity. In\nthis work, we investigate this alignment bottleneck through the lens of mutual\ninformation, demonstrating that the constrained capacity of the language model\ninherently limits the Effective Mutual Information (EMI) between multimodal\ninputs and outputs, thereby compromising alignment quality. To address this\nchallenge, we propose TinyAlign, a novel framework inspired by\nRetrieval-Augmented Generation, which strategically retrieves relevant context\nfrom a memory bank to enrich multimodal inputs and enhance their alignment.\nExtensive empirical evaluations reveal that TinyAlign significantly reduces\ntraining loss, accelerates convergence, and enhances task performance.\nRemarkably, it allows models to achieve baseline-level performance with only\n40\\% of the fine-tuning data, highlighting exceptional data efficiency. Our\nwork thus offers a practical pathway for developing more capable lightweight\nVLMs while introducing a fresh theoretical lens to better understand and\naddress alignment bottlenecks in constrained multimodal systems."}
{"id": "2505.11861", "pdf": "https://arxiv.org/pdf/2505.11861", "abs": "https://arxiv.org/abs/2505.11861", "authors": ["Qi Zhou", "Jie Zhang", "Dongxia Wang", "Qiang Liu", "Tianlin Li", "Jin Song Dong", "Wenhai Wang", "Qing Guo"], "title": "Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity", "categories": ["cs.AI", "cs.CL", "91C99", "I.2.7; J.4"], "comment": "under review", "summary": "Human preference plays a crucial role in the refinement of large language\nmodels (LLMs). However, collecting human preference feedback is costly and most\nexisting datasets neglect the correlation between personalization and\npreferences. To address this issue, we introduce Fair-PP, a synthetic dataset\nof personalized preferences targeting social equity, derived from real-world\nsocial survey data, which includes 28 social groups, 98 equity topics, and 5\npersonal preference dimensions. Leveraging GPT-4o-mini, we engage in\nrole-playing based on seven representative persona portrayals guided by\nexisting social survey data, yielding a total of 238,623 preference records.\nThrough Fair-PP, we also contribute (i) An automated framework for generating\npreference data, along with a more fine-grained dataset of personalized\npreferences; (ii) analysis of the positioning of the existing mainstream LLMs\nacross five major global regions within the personalized preference space; and\n(iii) a sample reweighting method for personalized preference alignment,\nenabling alignment with a target persona while maximizing the divergence from\nother personas. Empirical experiments show our method outperforms the\nbaselines."}
{"id": "2505.11980", "pdf": "https://arxiv.org/pdf/2505.11980", "abs": "https://arxiv.org/abs/2505.11980", "authors": ["Yi Chen", "Mu-Young Son", "Chuanbo Hua", "Joo-Young Kim"], "title": "AoP-SAM: Automation of Prompts for Efficient Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at AAAI 2025", "summary": "The Segment Anything Model (SAM) is a powerful foundation model for image\nsegmentation, showing robust zero-shot generalization through prompt\nengineering. However, relying on manual prompts is impractical for real-world\napplications, particularly in scenarios where rapid prompt provision and\nresource efficiency are crucial. In this paper, we propose the Automation of\nPrompts for SAM (AoP-SAM), a novel approach that learns to generate essential\nprompts in optimal locations automatically. AoP-SAM enhances SAM's efficiency\nand usability by eliminating manual input, making it better suited for\nreal-world tasks. Our approach employs a lightweight yet efficient Prompt\nPredictor model that detects key entities across images and identifies the\noptimal regions for placing prompt candidates. This method leverages SAM's\nimage embeddings, preserving its zero-shot generalization capabilities without\nrequiring fine-tuning. Additionally, we introduce a test-time instance-level\nAdaptive Sampling and Filtering mechanism that generates prompts in a\ncoarse-to-fine manner. This notably enhances both prompt and mask generation\nefficiency by reducing computational overhead and minimizing redundant mask\nrefinements. Evaluations of three datasets demonstrate that AoP-SAM\nsubstantially improves both prompt generation efficiency and mask generation\naccuracy, making SAM more effective for automated segmentation tasks."}
{"id": "2505.12906", "pdf": "https://arxiv.org/pdf/2505.12906", "abs": "https://arxiv.org/abs/2505.12906", "authors": ["Zhiwei Yang", "Zeyang Fan", "Yihang Lai", "Qi Chen", "Tian Zhang", "Jian Dai", "Kun Xu"], "title": "Efficient training for large-scale optical neural network using an evolutionary strategy and attention pruning", "categories": ["cs.LG", "physics.optics"], "comment": null, "summary": "MZI-based block optical neural networks (BONNs), which can achieve\nlarge-scale network models, have increasingly drawn attentions. However, the\nrobustness of the current training algorithm is not high enough. Moreover,\nlarge-scale BONNs usually contain numerous trainable parameters, resulting in\nexpensive computation and power consumption. In this article, by pruning matrix\nblocks and directly optimizing the individuals in population, we propose an\non-chip covariance matrix adaptation evolution strategy and attention-based\npruning (CAP) algorithm for large-scale BONNs. The calculated results\ndemonstrate that the CAP algorithm can prune 60% and 80% of the parameters for\nMNIST and Fashion-MNIST datasets, respectively, while only degrades the\nperformance by 3.289% and 4.693%. Considering the influence of dynamic noise in\nphase shifters, our proposed CAP algorithm (performance degradation of 22.327%\nfor MNIST dataset and 24.019% for Fashion-MNIST dataset utilizing a poor\nfabricated chip and electrical control with a standard deviation of 0.5)\nexhibits strongest robustness compared with both our previously reported block\nadjoint training algorithm (43.963% and 41.074%) and the covariance matrix\nadaptation evolution strategy (25.757% and 32.871%), respectively. Moreover,\nwhen 60% of the parameters are pruned, the CAP algorithm realizes 88.5%\naccuracy in experiment for the simplified MNIST dataset, which is similar to\nthe simulation result without noise (92.1%). Additionally, we simulationally\nand experimentally demonstrate that using MZIs with only internal phase\nshifters to construct BONNs is an efficient way to reduce both the system area\nand the required trainable parameters. Notably, our proposed CAP algorithm show\nexcellent potential for larger-scale network models and more complex tasks."}
{"id": "2505.11875", "pdf": "https://arxiv.org/pdf/2505.11875", "abs": "https://arxiv.org/abs/2505.11875", "authors": ["Chi-Min Chan", "Chunpu Xu", "Jiaming Ji", "Zhen Ye", "Pengcheng Wen", "Chunyang Jiang", "Yaodong Yang", "Wei Xue", "Sirui Han", "Yike Guo"], "title": "J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge", "categories": ["cs.LG", "cs.CL"], "comment": "33 pages, 27 figures", "summary": "The current focus of AI research is shifting from emphasizing model training\ntowards enhancing evaluation quality, a transition that is crucial for driving\nfurther advancements in AI systems. Traditional evaluation methods typically\nrely on reward models assigning scalar preference scores to outputs. Although\neffective, such approaches lack interpretability, leaving users often uncertain\nabout why a reward model rates a particular response as high or low. The advent\nof LLM-as-a-Judge provides a more scalable and interpretable method of\nsupervision, offering insights into the decision-making process. Moreover, with\nthe emergence of large reasoning models, which consume more tokens for deeper\nthinking and answer refinement, scaling test-time computation in the\nLLM-as-a-Judge paradigm presents an avenue for further boosting performance and\nproviding more interpretability through reasoning traces. In this paper, we\nintroduce $\\textbf{J1-7B}$, which is first supervised fine-tuned on\nreflection-enhanced datasets collected via rejection-sampling and subsequently\ntrained using Reinforcement Learning (RL) with verifiable rewards. At inference\ntime, we apply Simple Test-Time Scaling (STTS) strategies for additional\nperformance improvement. Experimental results demonstrate that $\\textbf{J1-7B}$\nsurpasses the previous state-of-the-art LLM-as-a-Judge by $ \\textbf{4.8}$\\% and\nexhibits a $ \\textbf{5.1}$\\% stronger scaling trend under STTS. Additionally,\nwe present three key findings: (1) Existing LLM-as-a-Judge does not inherently\nexhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced\ndatasets continues to demonstrate similarly weak scaling behavior. (3)\nSignificant scaling trend emerges primarily during the RL phase, suggesting\nthat effective STTS capability is acquired predominantly through RL training."}
{"id": "2505.11983", "pdf": "https://arxiv.org/pdf/2505.11983", "abs": "https://arxiv.org/abs/2505.11983", "authors": ["Ting Xiao", "Lei Shi", "Yang Zhang", "HaoFeng Yang", "Zhe Wang", "Chenjia Bai"], "title": "Online Iterative Self-Alignment for Radiology Report Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ACL 2025 Main", "summary": "Radiology Report Generation (RRG) is an important research topic for\nrelieving radiologist' heavy workload. Existing RRG models mainly rely on\nsupervised fine-tuning (SFT) based on different model architectures using data\npairs of radiological images and corresponding radiologist-annotated reports.\nRecent research has shifted focus to post-training improvements, aligning RRG\nmodel outputs with human preferences using reinforcement learning (RL).\nHowever, the limited data coverage of high-quality annotated data poses risks\nof overfitting and generalization. This paper proposes a novel Online Iterative\nSelf-Alignment (OISA) method for RRG that consists of four stages:\nself-generation of diverse data, self-evaluation for multi-objective preference\ndata,self-alignment for multi-objective optimization and self-iteration for\nfurther improvement. Our approach allows for generating varied reports tailored\nto specific clinical objectives, enhancing the overall performance of the RRG\nmodel iteratively. Unlike existing methods, our frame-work significantly\nincreases data quality and optimizes performance through iterative\nmulti-objective optimization. Experimental results demonstrate that our method\nsurpasses previous approaches, achieving state-of-the-art performance across\nmultiple evaluation metrics."}
{"id": "2505.12909", "pdf": "https://arxiv.org/pdf/2505.12909", "abs": "https://arxiv.org/abs/2505.12909", "authors": ["Alberto Fernández-Hernández", "Jose I. Mestre", "Manuel F. Dolz", "Jose Duato", "Enrique S. Quintana-Ortí"], "title": "Sinusoidal Initialization, Time for a New Start", "categories": ["cs.LG", "cs.AI", "I.2; G.3; I.2.6"], "comment": null, "summary": "Initialization plays a critical role in Deep Neural Network training,\ndirectly influencing convergence, stability, and generalization. Common\napproaches such as Glorot and He initializations rely on randomness, which can\nproduce uneven weight distributions across layer connections. In this paper, we\nintroduce the Sinusoidal initialization, a novel deterministic method that\nemploys sinusoidal functions to construct structured weight matrices expressly\nto improve the spread and balance of weights throughout the network while\nsimultaneously fostering a more uniform, well-conditioned distribution of\nneuron activation states from the very first forward pass. Because Sinusoidal\ninitialization begins with weights and activations that are already evenly and\nefficiently utilized, it delivers consistently faster convergence, greater\ntraining stability, and higher final accuracy across a wide range of models,\nincluding convolutional neural networks, vision transformers, and large\nlanguage models. On average, our experiments show an increase of 4.8 % in final\nvalidation accuracy and 20.9 % in convergence speed. By replacing randomness\nwith structure, this initialization provides a stronger and more reliable\nfoundation for Deep Learning systems."}
{"id": "2505.11979", "pdf": "https://arxiv.org/pdf/2505.11979", "abs": "https://arxiv.org/abs/2505.11979", "authors": ["Tarik Houichime", "Younes El Amrani"], "title": "Introduction to Analytical Software Engineering Design Paradigm", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MS", "cs.PL"], "comment": "The Conference's autorization to submit a preprint was granted", "summary": "As modern software systems expand in scale and complexity, the challenges\nassociated with their modeling and formulation grow increasingly intricate.\nTraditional approaches often fall short in effectively addressing these\ncomplexities, particularly in tasks such as design pattern detection for\nmaintenance and assessment, as well as code refactoring for optimization and\nlong-term sustainability. This growing inadequacy underscores the need for a\nparadigm shift in how such challenges are approached and resolved. This paper\npresents Analytical Software Engineering (ASE), a novel design paradigm aimed\nat balancing abstraction, tool accessibility, compatibility, and scalability.\nASE enables effective modeling and resolution of complex software engineering\nproblems. The paradigm is evaluated through two frameworks\nBehavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR),\nboth developed in accordance with ASE principles. BSS offers a compact,\nlanguage-agnostic representation of codebases to facilitate precise design\npattern detection. ODR unifies artifact and solution representations to\noptimize code refactoring via heuristic algorithms while eliminating iterative\ncomputational overhead. By providing a structured approach to software design\nchallenges, ASE lays the groundwork for future research in encoding and\nanalyzing complex software metrics."}
{"id": "2505.12005", "pdf": "https://arxiv.org/pdf/2505.12005", "abs": "https://arxiv.org/abs/2505.12005", "authors": ["Dong Liu", "Yifan Yang", "Zixiong Huang", "Yuxin Gao", "Mingkui Tan"], "title": "CHRIS: Clothed Human Reconstruction with Side View Consistency", "categories": ["cs.CV", "cs.AI"], "comment": "ICME 2025", "summary": "Creating a realistic clothed human from a single-view RGB image is crucial\nfor applications like mixed reality and filmmaking. Despite some progress in\nrecent years, mainstream methods often fail to fully utilize side-view\ninformation, as the input single-view image contains front-view information\nonly. This leads to globally unrealistic topology and local surface\ninconsistency in side views. To address these, we introduce Clothed Human\nReconstruction with Side View Consistency, namely CHRIS, which consists of 1) A\nSide-View Normal Discriminator that enhances global visual reasonability by\ndistinguishing the generated side-view normals from the ground truth ones; 2) A\nMulti-to-One Gradient Computation (M2O) that ensures local surface consistency.\nM2O calculates the gradient of a sampling point by integrating the gradients of\nthe nearby points, effectively acting as a smooth operation. Experimental\nresults demonstrate that CHRIS achieves state-of-the-art performance on public\nbenchmarks and outperforms the prior work."}
{"id": "2505.12913", "pdf": "https://arxiv.org/pdf/2505.12913", "abs": "https://arxiv.org/abs/2505.12913", "authors": ["Tom George Grigg", "Mason Burlage", "Oliver Brook Scott", "Adam Taouil", "Dominique Sydow", "Liam Wilbraham"], "title": "Active Learning on Synthons for Molecular Design", "categories": ["cs.LG", "q-bio.QM"], "comment": "14 pages, 10 figures. Presented at ICLR 2025 GEM Workshop", "summary": "Exhaustive virtual screening is highly informative but often intractable\nagainst the expensive objective functions involved in modern drug discovery.\nThis problem is exacerbated in combinatorial contexts such as multi-vector\nexpansion, where molecular spaces can quickly become ultra-large. Here, we\nintroduce Scalable Active Learning via Synthon Acquisition (SALSA): a simple\nalgorithm applicable to multi-vector expansion which extends pool-based active\nlearning to non-enumerable spaces by factoring modeling and acquisition over\nsynthon or fragment choices. Through experiments on ligand- and structure-based\nobjectives, we highlight SALSA's sample efficiency, and its ability to scale to\nspaces of trillions of compounds. Further, we demonstrate application toward\nmulti-parameter objective design tasks on three protein targets - finding\nSALSA-generated molecules have comparable chemical property profiles to known\nbioactives, and exhibit greater diversity and higher scores over an\nindustry-leading generative approach."}
{"id": "2505.12039", "pdf": "https://arxiv.org/pdf/2505.12039", "abs": "https://arxiv.org/abs/2505.12039", "authors": ["Renqi Chen", "Haoyang Su", "Shixiang Tang", "Zhenfei Yin", "Qi Wu", "Hui Li", "Ye Sun", "Nanqing Dong", "Wanli Ouyang", "Philip Torr"], "title": "AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research", "categories": ["cs.AI", "cs.CL", "physics.soc-ph"], "comment": null, "summary": "The Science of Science (SoS) explores the mechanisms underlying scientific\ndiscovery, and offers valuable insights for enhancing scientific efficiency and\nfostering innovation. Traditional approaches often rely on simplistic\nassumptions and basic statistical tools, such as linear regression and\nrule-based simulations, which struggle to capture the complexity and scale of\nmodern research ecosystems. The advent of artificial intelligence (AI) presents\na transformative opportunity for the next generation of SoS, enabling the\nautomation of large-scale pattern discovery and uncovering insights previously\nunattainable. This paper offers a forward-looking perspective on the\nintegration of Science of Science with AI for automated research pattern\ndiscovery and highlights key open challenges that could greatly benefit from\nAI. We outline the advantages of AI over traditional methods, discuss potential\nlimitations, and propose pathways to overcome them. Additionally, we present a\npreliminary multi-agent system as an illustrative example to simulate research\nsocieties, showcasing AI's ability to replicate real-world research patterns\nand accelerate progress in Science of Science research."}
{"id": "2505.12020", "pdf": "https://arxiv.org/pdf/2505.12020", "abs": "https://arxiv.org/abs/2505.12020", "authors": ["Xi Han", "Jingwei Zhang", "Dimitris Samaras", "Fei Hou", "Hong Qin"], "title": "GeoMaNO: Geometric Mamba Neural Operator for Partial Differential Equations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The neural operator (NO) framework has emerged as a powerful tool for solving\npartial differential equations (PDEs). Recent NOs are dominated by the\nTransformer architecture, which offers NOs the capability to capture long-range\ndependencies in PDE dynamics. However, existing Transformer-based NOs suffer\nfrom quadratic complexity, lack geometric rigor, and thus suffer from\nsub-optimal performance on regular grids. As a remedy, we propose the Geometric\nMamba Neural Operator (GeoMaNO) framework, which empowers NOs with Mamba's\nmodeling capability, linear complexity, plus geometric rigor. We evaluate\nGeoMaNO's performance on multiple standard and popularly employed PDE\nbenchmarks, spanning from Darcy flow problems to Navier-Stokes problems.\nGeoMaNO improves existing baselines in solution operator approximation by as\nmuch as 58.9%."}
{"id": "2505.12917", "pdf": "https://arxiv.org/pdf/2505.12917", "abs": "https://arxiv.org/abs/2505.12917", "authors": ["Shengsheng Lin", "Haojun Chen", "Haijie Wu", "Chunyun Qiu", "Weiwei Lin"], "title": "Temporal Query Network for Efficient Multivariate Time Series Forecasting", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Sufficiently modeling the correlations among variables (aka channels) is\ncrucial for achieving accurate multivariate time series forecasting (MTSF). In\nthis paper, we propose a novel technique called Temporal Query (TQ) to more\neffectively capture multivariate correlations, thereby improving model\nperformance in MTSF tasks. Technically, the TQ technique employs periodically\nshifted learnable vectors as queries in the attention mechanism to capture\nglobal inter-variable patterns, while the keys and values are derived from the\nraw input data to encode local, sample-level correlations. Building upon the TQ\ntechnique, we develop a simple yet efficient model named Temporal Query Network\n(TQNet), which employs only a single-layer attention mechanism and a\nlightweight multi-layer perceptron (MLP). Extensive experiments demonstrate\nthat TQNet learns more robust multivariate correlations, achieving\nstate-of-the-art forecasting accuracy across 12 challenging real-world\ndatasets. Furthermore, TQNet achieves high efficiency comparable to\nlinear-based methods even on high-dimensional datasets, balancing performance\nand computational cost. The code is available at:\nhttps://github.com/ACAT-SCUT/TQNet."}
{"id": "2505.12058", "pdf": "https://arxiv.org/pdf/2505.12058", "abs": "https://arxiv.org/abs/2505.12058", "authors": ["Vincent Koc"], "title": "Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.6; H.2.8"], "comment": "28 pages, 7 figures, 3 tables. Includes expanded appendix & full\n  score matrices. Dataset & code: HF Hub + GitHub + Pypi links in abstract.\n  Core data and code Apache-2.0; synthetic packs eval-only", "summary": "Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual\nsmoke-test suite designed to give large-language-model (LLM) pipelines a\nunit-test style safety net dataset that runs in seconds with minimal cost. Born\nout of the tight feedback-loop demands building the Comet Opik\nprompt-optimization SDK, where waiting on heavyweight benchmarks breaks\ndeveloper flow. TQB++ couples a 52-item English gold set (less than 20 kB) with\na tiny synthetic-data generator pypi package built on provider-agnostic\nLiteLLM. The generator lets practitioners mint their own tiny packs in any\nlanguage, domain, or difficulty, while ten ready-made packs already cover\nArabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian,\nSpanish, and Turkish. Every dataset ships with Croissant metadata and\nplug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so\nteams can drop deterministic micro-benchmarks directly into pull-request gates,\nprompt-engineering loops, and production dashboards without touching GPU\nbudgets. A complete TQB++ run adds only a few seconds to pipeline latency yet\nreliably flags prompt-template errors, tokenizer drift, and fine-tuning\nside-effects long before full-scale suites like MMLU or BIG-Bench would finish\nconfiguring. The entire framework is released to accelerate continuous,\nresource-efficient quality assurance across the generative-AI ecosystem."}
{"id": "2505.12038", "pdf": "https://arxiv.org/pdf/2505.12038", "abs": "https://arxiv.org/abs/2505.12038", "authors": ["Ning Lu", "Shengcai Liu", "Jiahao Wu", "Weiyu Chen", "Zhirui Zhang", "Yew-Soon Ong", "Qi Wang", "Ke Tang"], "title": "Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "ICML 2025 Camera Ready", "summary": "Large language models (LLMs) have shown great potential as general-purpose AI\nassistants across various domains. To fully leverage this potential in specific\napplications, many companies provide fine-tuning API services, enabling users\nto upload their own data for LLM customization. However, fine-tuning services\nintroduce a new safety threat: user-uploaded data, whether harmful or benign,\ncan break the model's alignment, leading to unsafe outputs. Moreover, existing\ndefense methods struggle to address the diversity of fine-tuning datasets\n(e.g., varying sizes, tasks), often sacrificing utility for safety or vice\nversa. To address this issue, we propose Safe Delta, a safety-aware\npost-training defense method that adjusts the delta parameters (i.e., the\nparameter change before and after fine-tuning). Specifically, Safe Delta\nestimates the safety degradation, selects delta parameters to maximize utility\nwhile limiting overall safety loss, and applies a safety compensation vector to\nmitigate residual safety loss. Through extensive experiments on four diverse\ndatasets with varying settings, our approach consistently preserves safety\nwhile ensuring that the utility gain from benign datasets remains unaffected."}
{"id": "2505.12919", "pdf": "https://arxiv.org/pdf/2505.12919", "abs": "https://arxiv.org/abs/2505.12919", "authors": ["Eilon Vaknin Laufer", "Boaz Nadler"], "title": "RGNMR: A Gauss-Newton method for robust matrix completion with theoretical guarantees", "categories": ["cs.LG", "cs.NA", "math.NA", "math.OC", "stat.ML"], "comment": null, "summary": "Recovering a low rank matrix from a subset of its entries, some of which may\nbe corrupted, is known as the robust matrix completion (RMC) problem. Existing\nRMC methods have several limitations: they require a relatively large number of\nobserved entries; they may fail under overparametrization, when their assumed\nrank is higher than the correct one; and many of them fail to recover even\nmildly ill-conditioned matrices. In this paper we propose a novel RMC method,\ndenoted $\\texttt{RGNMR}$, which overcomes these limitations. $\\texttt{RGNMR}$\nis a simple factorization-based iterative algorithm, which combines a\nGauss-Newton linearization with removal of entries suspected to be outliers. On\nthe theoretical front, we prove that under suitable assumptions,\n$\\texttt{RGNMR}$ is guaranteed exact recovery of the underlying low rank\nmatrix. Our theoretical results improve upon the best currently known for\nfactorization-based methods. On the empirical front, we show via several\nsimulations the advantages of $\\texttt{RGNMR}$ over existing RMC methods, and\nin particular its ability to handle a small number of observed entries,\noverparameterization of the rank and ill-conditioned matrices."}
{"id": "2505.12065", "pdf": "https://arxiv.org/pdf/2505.12065", "abs": "https://arxiv.org/abs/2505.12065", "authors": ["Tiannuo Yang", "Zebin Yao", "Bowen Jin", "Lixiao Cui", "Yusen Li", "Gang Wang", "Xiaoguang Liu"], "title": "Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM)-based search agents have shown remarkable\ncapabilities in solving complex tasks by dynamically decomposing problems and\naddressing them through interleaved reasoning and retrieval. However, this\ninterleaved paradigm introduces substantial efficiency bottlenecks. First, we\nobserve that both highly accurate and overly approximate retrieval methods\ndegrade system efficiency: exact search incurs significant retrieval overhead,\nwhile coarse retrieval requires additional reasoning steps during generation.\nSecond, we identify inefficiencies in system design, including improper\nscheduling and frequent retrieval stalls, which lead to cascading latency --\nwhere even minor delays in retrieval amplify end-to-end inference time. To\naddress these challenges, we introduce SearchAgent-X, a high-efficiency\ninference framework for LLM-based search agents. SearchAgent-X leverages\nhigh-recall approximate retrieval and incorporates two key techniques:\npriority-aware scheduling and non-stall retrieval. Extensive experiments\ndemonstrate that SearchAgent-X consistently outperforms state-of-the-art\nsystems such as vLLM and HNSW-based retrieval across diverse tasks, achieving\nup to 3.4$\\times$ higher throughput and 5$\\times$ lower latency, without\ncompromising generation quality. SearchAgent-X is available at\nhttps://github.com/tiannuo-yang/SearchAgent-X."}
{"id": "2505.12049", "pdf": "https://arxiv.org/pdf/2505.12049", "abs": "https://arxiv.org/abs/2505.12049", "authors": ["Mehran Shakerinava", "Siamak Ravanbakhsh", "Adam Oberman"], "title": "Beyond Scalar Rewards: An Axiomatic Framework for Lexicographic MDPs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent work has formalized the reward hypothesis through the lens of expected\nutility theory, by interpreting reward as utility. Hausner's foundational work\nshowed that dropping the continuity axiom leads to a generalization of expected\nutility theory where utilities are lexicographically ordered vectors of\narbitrary dimension. In this paper, we extend this result by identifying a\nsimple and practical condition under which preferences cannot be represented by\nscalar rewards, necessitating a 2-dimensional reward function. We provide a\nfull characterization of such reward functions, as well as the general\nd-dimensional case, in Markov Decision Processes (MDPs) under a memorylessness\nassumption on preferences. Furthermore, we show that optimal policies in this\nsetting retain many desirable properties of their scalar-reward counterparts,\nwhile in the Constrained MDP (CMDP) setting -- another common multiobjective\nsetting -- they do not."}
{"id": "2505.12938", "pdf": "https://arxiv.org/pdf/2505.12938", "abs": "https://arxiv.org/abs/2505.12938", "authors": ["Uri Dalal", "Meirav Segal", "Zvika Ben-Haim", "Dan Lahav", "Omer Nevo"], "title": "Leveraging LLM Inconsistency to Boost Pass@k Performance", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations."}
{"id": "2505.12135", "pdf": "https://arxiv.org/pdf/2505.12135", "abs": "https://arxiv.org/abs/2505.12135", "authors": ["Omar Choukrani", "Idriss Malek", "Daniil Orel", "Zhuohan Xie", "Zangir Iklassov", "Martin Takáč", "Salem Lahlou"], "title": "LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Assessing the capacity of Large Language Models (LLMs) to plan and reason\nwithin the constraints of interactive environments is crucial for developing\ncapable AI agents. We introduce $\\textbf{LLM-BabyBench}$, a new benchmark suite\ndesigned specifically for this purpose. Built upon a textual adaptation of the\nprocedurally generated BabyAI grid world, this suite evaluates LLMs on three\nfundamental aspects of grounded intelligence: (1) predicting the consequences\nof actions on the environment state ($\\textbf{Predict}$ task), (2) generating\nsequences of low-level actions to achieve specified objectives ($\\textbf{Plan}$\ntask), and (3) decomposing high-level instructions into coherent subgoal\nsequences ($\\textbf{Decompose}$ task). We detail the methodology for generating\nthe three corresponding datasets ($\\texttt{LLM-BabyBench-Predict}$,\n$\\texttt{-Plan}$, $\\texttt{-Decompose}$) by extracting structured information\nfrom an expert agent operating within the text-based environment. Furthermore,\nwe provide a standardized evaluation harness and metrics, including environment\ninteraction for validating generated plans, to facilitate reproducible\nassessment of diverse LLMs. Initial baseline results highlight the challenges\nposed by these grounded reasoning tasks. The benchmark suite, datasets, data\ngeneration code, and evaluation code are made publicly available\n($\\href{https://github.com/choukrani/llm-babybench}{\\text{GitHub}}$,\n$\\href{https://huggingface.co/datasets/salem-mbzuai/LLM-BabyBench}{\\text{HuggingFace}}$)."}
{"id": "2505.12050", "pdf": "https://arxiv.org/pdf/2505.12050", "abs": "https://arxiv.org/abs/2505.12050", "authors": ["Vinod Raman", "Hilal Asi", "Satyen Kale"], "title": "ABoN: Adaptive Best-of-N Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Recent advances in test-time alignment methods, such as Best-of-N sampling,\noffer a simple and effective way to steer language models (LMs) toward\npreferred behaviors using reward models (RM). However, these approaches can be\ncomputationally expensive, especially when applied uniformly across prompts\nwithout accounting for differences in alignment difficulty. In this work, we\npropose a prompt-adaptive strategy for Best-of-N alignment that allocates\ninference-time compute more efficiently. Motivated by latency concerns, we\ndevelop a two-stage algorithm: an initial exploratory phase estimates the\nreward distribution for each prompt using a small exploration budget, and a\nsecond stage adaptively allocates the remaining budget using these estimates.\nOur method is simple, practical, and compatible with any LM/RM combination.\nEmpirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different\nbatches of prompts show that our adaptive strategy consistently outperforms the\nuniform allocation with the same inference budget. Moreover, our experiments\nshow that our adaptive strategy remains competitive against uniform allocations\nwith 20% larger inference budgets and even improves in performance as the batch\nsize grows."}
{"id": "2505.12940", "pdf": "https://arxiv.org/pdf/2505.12940", "abs": "https://arxiv.org/abs/2505.12940", "authors": ["James Rowbottom", "Stefania Fresca", "Pietro Lio", "Carola-Bibiane Schönlieb", "Nicolas Boullé"], "title": "Multi-Level Monte Carlo Training of Neural Operators", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": "18 pages, 8 figures", "summary": "Operator learning is a rapidly growing field that aims to approximate\nnonlinear operators related to partial differential equations (PDEs) using\nneural operators. These rely on discretization of input and output functions\nand are, usually, expensive to train for large-scale problems at\nhigh-resolution. Motivated by this, we present a Multi-Level Monte Carlo (MLMC)\napproach to train neural operators by leveraging a hierarchy of resolutions of\nfunction dicretization. Our framework relies on using gradient corrections from\nfewer samples of fine-resolution data to decrease the computational cost of\ntraining while maintaining a high level accuracy. The proposed MLMC training\nprocedure can be applied to any architecture accepting multi-resolution data.\nOur numerical experiments on a range of state-of-the-art models and test-cases\ndemonstrate improved computational efficiency compared to traditional\nsingle-resolution training approaches, and highlight the existence of a Pareto\ncurve between accuracy and computational time, related to the number of samples\nper resolution."}
{"id": "2505.12185", "pdf": "https://arxiv.org/pdf/2505.12185", "abs": "https://arxiv.org/abs/2505.12185", "authors": ["Sen Fang", "Weiyuan Ding", "Bowen Xu"], "title": "EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "19 pages, 11 figures", "summary": "Assessing the programming capabilities of Large Language Models (LLMs) is\ncrucial for their effective use in software engineering. Current evaluations,\nhowever, predominantly measure the accuracy of generated code on static\nbenchmarks, neglecting the critical aspect of model robustness during\nprogramming tasks. While adversarial attacks offer insights on model\nrobustness, their effectiveness is limited and evaluation could be constrained.\nCurrent adversarial attack methods for robustness evaluation yield inconsistent\nresults, struggling to provide a unified evaluation across different LLMs. We\nintroduce EVALOOP, a novel assessment framework that evaluate the robustness\nfrom a self-consistency perspective, i.e., leveraging the natural duality\ninherent in popular software engineering tasks, e.g., code generation and code\nsummarization. EVALOOP initiates a self-contained feedback loop: an LLM\ngenerates output (e.g., code) from an input (e.g., natural language\nspecification), and then use the generated output as the input to produce a new\noutput (e.g., summarizes that code into a new specification). EVALOOP repeats\nthe process to assess the effectiveness of EVALOOP in each loop. This cyclical\nstrategy intrinsically evaluates robustness without rely on any external attack\nsetups, providing a unified metric to evaluate LLMs' robustness in programming.\nWe evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found\nthat EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1\nperformance within ten loops. Intriguingly, robustness does not always align\nwith initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,\ndespite superior initial code generation compared to DeepSeek-V2, demonstrated\nlower robustness over repeated evaluation loop."}
{"id": "2505.12051", "pdf": "https://arxiv.org/pdf/2505.12051", "abs": "https://arxiv.org/abs/2505.12051", "authors": ["Yinghui Zhang", "Tailin Chen", "Yuchen Zhang", "Zeyu Fu"], "title": "Enhanced Multimodal Hate Video Detection via Channel-wise and Modality-wise Fusion", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": "ICDMW 2024, Github: https://github.com/EvelynZ10/cmfusion", "summary": "The rapid rise of video content on platforms such as TikTok and YouTube has\ntransformed information dissemination, but it has also facilitated the spread\nof harmful content, particularly hate videos. Despite significant efforts to\ncombat hate speech, detecting these videos remains challenging due to their\noften implicit nature. Current detection methods primarily rely on unimodal\napproaches, which inadequately capture the complementary features across\ndifferent modalities. While multimodal techniques offer a broader perspective,\nmany fail to effectively integrate temporal dynamics and modality-wise\ninteractions essential for identifying nuanced hate content. In this paper, we\npresent CMFusion, an enhanced multimodal hate video detection model utilizing a\nnovel Channel-wise and Modality-wise Fusion Mechanism. CMFusion first extracts\nfeatures from text, audio, and video modalities using pre-trained models and\nthen incorporates a temporal cross-attention mechanism to capture dependencies\nbetween video and audio streams. The learned features are then processed by\nchannel-wise and modality-wise fusion modules to obtain informative\nrepresentations of videos. Our extensive experiments on a real-world dataset\ndemonstrate that CMFusion significantly outperforms five widely used baselines\nin terms of accuracy, precision, recall, and F1 score. Comprehensive ablation\nstudies and parameter analyses further validate our design choices,\nhighlighting the model's effectiveness in detecting hate videos. The source\ncodes will be made publicly available at https://github.com/EvelynZ10/cmfusion."}
{"id": "2505.12944", "pdf": "https://arxiv.org/pdf/2505.12944", "abs": "https://arxiv.org/abs/2505.12944", "authors": ["Jan Hagnberger", "Daniel Musekamp", "Mathias Niepert"], "title": "CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "physics.comp-ph"], "comment": null, "summary": "Solving time-dependent Partial Differential Equations (PDEs) using a densely\ndiscretized spatial domain is a fundamental problem in various scientific and\nengineering disciplines, including modeling climate phenomena and fluid\ndynamics. However, performing these computations directly in the physical space\noften incurs significant computational costs. To address this issue, several\nneural surrogate models have been developed that operate in a compressed latent\nspace to solve the PDE. While these approaches reduce computational complexity,\nthey often use Transformer-based attention mechanisms to handle irregularly\nsampled domains, resulting in increased memory consumption. In contrast,\nconvolutional neural networks allow memory-efficient encoding and decoding but\nare limited to regular discretizations. Motivated by these considerations, we\npropose CALM-PDE, a model class that efficiently solves arbitrarily discretized\nPDEs in a compressed latent space. We introduce a novel continuous\nconvolution-based encoder-decoder architecture that uses an\nepsilon-neighborhood-constrained kernel and learns to apply the convolution\noperator to adaptive and optimized query points. We demonstrate the\neffectiveness of CALM-PDE on a diverse set of PDEs with both regularly and\nirregularly sampled spatial domains. CALM-PDE is competitive with or\noutperforms existing baseline methods while offering significant improvements\nin memory and inference time efficiency compared to Transformer-based methods."}
{"id": "2505.12189", "pdf": "https://arxiv.org/pdf/2505.12189", "abs": "https://arxiv.org/abs/2505.12189", "authors": ["Marco Valentino", "Geonhee Kim", "Dhairya Dalal", "Zhixue Zhao", "André Freitas"], "title": "Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering", "categories": ["cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Large language models (LLMs) frequently demonstrate reasoning limitations,\noften conflating content plausibility (i.e., material inference) with logical\nvalidity (i.e., formal inference). This can result in biased inferences, where\nplausible arguments are incorrectly deemed logically valid or vice versa.\nMitigating this limitation is critical, as it undermines the trustworthiness\nand generalizability of LLMs in applications that demand rigorous logical\nconsistency. This paper investigates the problem of mitigating content biases\non formal reasoning through activation steering. Specifically, we curate a\ncontrolled syllogistic reasoning dataset to disentangle formal validity from\ncontent plausibility. After localising the layers responsible for formal and\nmaterial inference, we investigate contrastive activation steering methods for\ntest-time interventions. An extensive empirical analysis on different LLMs\nreveals that contrastive steering consistently supports linear control over\ncontent biases. However, we observe that a static approach is insufficient for\nimproving all the tested models. We then leverage the possibility to control\ncontent effects by dynamically determining the value of the steering parameters\nvia fine-grained conditional methods. We found that conditional steering is\neffective on unresponsive models, achieving up to 15% absolute improvement in\nformal reasoning accuracy with a newly introduced kNN-based method (K-CAST).\nFinally, additional experiments reveal that steering for content effects is\nrobust to prompt variations, incurs minimal side effects on language modeling\ncapabilities, and can partially generalize to out-of-distribution reasoning\ntasks. Practically, this paper demonstrates that activation-level interventions\ncan offer a scalable strategy for enhancing the robustness of LLMs,\ncontributing towards more systematic and unbiased formal reasoning."}
{"id": "2505.12053", "pdf": "https://arxiv.org/pdf/2505.12053", "abs": "https://arxiv.org/abs/2505.12053", "authors": ["Tianxiong Zhong", "Xingye Tian", "Boyuan Jiang", "Xuebo Wang", "Xin Tao", "Pengfei Wan", "Zhiwei Zhang"], "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 10 figures", "summary": "Modern video generation frameworks based on Latent Diffusion Models suffer\nfrom inefficiencies in tokenization due to the Frame-Proportional Information\nAssumption. Existing tokenizers provide fixed temporal compression rates,\ncausing the computational cost of the diffusion model to scale linearly with\nthe frame rate. The paper proposes the Duration-Proportional Information\nAssumption: the upper bound on the information capacity of a video is\nproportional to the duration rather than the number of frames. Based on this\ninsight, the paper introduces VFRTok, a Transformer-based video tokenizer, that\nenables variable frame rate encoding and decoding through asymmetric frame rate\ntraining between the encoder and decoder. Furthermore, the paper proposes\nPartial Rotary Position Embeddings (RoPE) to decouple position and content\nmodeling, which groups correlated patches into unified tokens. The Partial RoPE\neffectively improves content-awareness, enhancing the video generation\ncapability. Benefiting from the compact and continuous spatio-temporal\nrepresentation, VFRTok achieves competitive reconstruction quality and\nstate-of-the-art generation fidelity while using only 1/8 tokens compared to\nexisting tokenizers."}
{"id": "2505.12951", "pdf": "https://arxiv.org/pdf/2505.12951", "abs": "https://arxiv.org/abs/2505.12951", "authors": ["Xuerui Su", "Liya Guo", "Yue Wang", "Yi Zhu", "Zhiming Ma", "Zun Wang", "Yuting Liu"], "title": "DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Inference scaling further accelerates Large Language Models (LLMs) toward\nArtificial General Intelligence (AGI), with large-scale Reinforcement Learning\n(RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning\napproaches usually rely on handcrafted rule-based reward functions. However,\nthe tarde-offs of exploration and exploitation in RL algorithms involves\nmultiple complex considerations, and the theoretical and empirical impacts of\nmanually designed reward functions remain insufficiently explored. In this\npaper, we propose Decoupled Group Reward Optimization (DGRO), a general RL\nalgorithm for LLM reasoning. On the one hand, DGRO decouples the traditional\nregularization coefficient into two independent hyperparameters: one scales the\npolicy gradient term, and the other regulates the distance from the sampling\npolicy. This decoupling not only enables precise control over balancing\nexploration and exploitation, but also can be seamlessly extended to Online\nPolicy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward\nOptimization. On the other hand, we observe that reward variance significantly\naffects both convergence speed and final model performance. We conduct both\ntheoretical analysis and extensive empirical validation to assess DGRO,\nincluding a detailed ablation study that investigates its performance and\noptimization dynamics. Experimental results show that DGRO achieves\nstate-of-the-art performance on the Logic dataset with an average accuracy of\n96.9\\%, and demonstrates strong generalization across mathematical benchmarks."}
{"id": "2505.12225", "pdf": "https://arxiv.org/pdf/2505.12225", "abs": "https://arxiv.org/abs/2505.12225", "authors": ["Jizhou Guo", "Zhaomin Wu", "Philip S. Yu"], "title": "Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "High-quality reward models are crucial for unlocking the reasoning potential\nof large language models (LLMs), with best-of-N voting demonstrating\nsignificant performance gains. However, current reward models, which typically\noperate on the textual output of LLMs, are computationally expensive and\nparameter-heavy, limiting their real-world applications. We introduce the\nEfficient Linear Hidden State Reward (ELHSR) model - a novel, highly\nparameter-efficient approach that leverages the rich information embedded in\nLLM hidden states to address these issues. ELHSR systematically outperform\nbaselines with less than 0.005% of the parameters of baselines, requiring only\na few samples for training. ELHSR also achieves orders-of-magnitude efficiency\nimprovement with significantly less time and fewer FLOPs per sample than\nbaseline reward models. Moreover, ELHSR exhibits robust performance even when\ntrained only on logits, extending its applicability to some closed-source LLMs.\nIn addition, ELHSR can also be combined with traditional reward models to\nachieve additional performance gains."}
{"id": "2505.12069", "pdf": "https://arxiv.org/pdf/2505.12069", "abs": "https://arxiv.org/abs/2505.12069", "authors": ["Shenzhou Liu", "Di Wang", "Haonan Guo", "Chengxi Han", "Wenzhi Zeng"], "title": "MT-CYP-Net: Multi-Task Network for Pixel-Level Crop Yield Prediction Under Very Few Samples", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate and fine-grained crop yield prediction plays a crucial role in\nadvancing global agriculture. However, the accuracy of pixel-level yield\nestimation based on satellite remote sensing data has been constrained by the\nscarcity of ground truth data. To address this challenge, we propose a novel\napproach called the Multi-Task Crop Yield Prediction Network (MT-CYP-Net). This\nframework introduces an effective multi-task feature-sharing strategy, where\nfeatures extracted from a shared backbone network are simultaneously utilized\nby both crop yield prediction decoders and crop classification decoders with\nthe ability to fuse information between them. This design allows MT-CYP-Net to\nbe trained with extremely sparse crop yield point labels and crop type labels,\nwhile still generating detailed pixel-level crop yield maps. Concretely, we\ncollected 1,859 yield point labels along with corresponding crop type labels\nand satellite images from eight farms in Heilongjiang Province, China, in 2023,\ncovering soybean, maize, and rice crops, and constructed a sparse crop yield\nlabel dataset. MT-CYP-Net is compared with three classical machine learning and\ndeep learning benchmark methods in this dataset. Experimental results not only\nindicate the superiority of MT-CYP-Net compared to previous methods on multiple\ntypes of crops but also demonstrate the potential of deep networks on precise\npixel-level crop yield prediction, especially with limited data labels."}
{"id": "2505.12952", "pdf": "https://arxiv.org/pdf/2505.12952", "abs": "https://arxiv.org/abs/2505.12952", "authors": ["Chuanxing Geng", "Qifei Li", "Xinrui Wang", "Dong Liang", "Songcan Chen", "Pong C. Yuen"], "title": "LoD: Loss-difference OOD Detection by Intentionally Label-Noisifying Unlabeled Wild Data", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted by IJCAI2025", "summary": "Using unlabeled wild data containing both in-distribution (ID) and\nout-of-distribution (OOD) data to improve the safety and reliability of models\nhas recently received increasing attention. Existing methods either design\ncustomized losses for labeled ID and unlabeled wild data then perform joint\noptimization, or first filter out OOD data from the latter then learn an OOD\ndetector. While achieving varying degrees of success, two potential issues\nremain: (i) Labeled ID data typically dominates the learning of models,\ninevitably making models tend to fit OOD data as IDs; (ii) The selection of\nthresholds for identifying OOD data in unlabeled wild data usually faces\ndilemma due to the unavailability of pure OOD samples. To address these issues,\nwe propose a novel loss-difference OOD detection framework (LoD) by\n\\textit{intentionally label-noisifying} unlabeled wild data. Such operations\nnot only enable labeled ID data and OOD data in unlabeled wild data to jointly\ndominate the models' learning but also ensure the distinguishability of the\nlosses between ID and OOD samples in unlabeled wild data, allowing the classic\nclustering technique (e.g., K-means) to filter these OOD samples without\nrequiring thresholds any longer. We also provide theoretical foundation for\nLoD's viability, and extensive experiments verify its superiority."}
{"id": "2505.12260", "pdf": "https://arxiv.org/pdf/2505.12260", "abs": "https://arxiv.org/abs/2505.12260", "authors": ["Guangyuan Ma", "Yongliang Ma", "Xuanrui Gou", "Zhenpeng Su", "Ming Zhou", "Songlin Hu"], "title": "LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode\nqueries and documents into low-dimensional dense or high-dimensional sparse\nvectors. It retrieves documents relevant to search queries based on vector\nsimilarities. Documents are pre-encoded offline, while queries arrive in\nreal-time, necessitating an efficient online query encoder. Although LLMs\nsignificantly enhance retrieval capabilities, serving deeply parameterized LLMs\nslows down query inference throughput and increases demands for online\ndeployment resources. In this paper, we propose LightRetriever, a novel\nLLM-based hybrid retriever with extremely lightweight query encoders. Our\nmethod retains a full-sized LLM for document encoding, but reduces the workload\nof query encoding to no more than an embedding lookup. Compared to serving a\nfull-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for\nquery inference with GPU acceleration, and even a 20x speedup without GPU.\nExperiments on large-scale retrieval benchmarks demonstrate that our method\ngeneralizes well across diverse retrieval tasks, retaining an average of 95%\nfull-sized performance."}
{"id": "2505.12079", "pdf": "https://arxiv.org/pdf/2505.12079", "abs": "https://arxiv.org/abs/2505.12079", "authors": ["Yuqi Li", "Kai Li", "Xin Yin", "Zhifei Yang", "Junhao Dong", "Zeyu Dong", "Chuanguang Yang", "Yingli Tian", "Yao Lu"], "title": "SepPrune: Structured Pruning for Efficient Deep Speech Separation", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Although deep learning has substantially advanced speech separation in recent\nyears, most existing studies continue to prioritize separation quality while\noverlooking computational efficiency, an essential factor for low-latency\nspeech processing in real-time applications. In this paper, we propose\nSepPrune, the first structured pruning framework specifically designed to\ncompress deep speech separation models and reduce their computational cost.\nSepPrune begins by analyzing the computational structure of a given model to\nidentify layers with the highest computational burden. It then introduces a\ndifferentiable masking strategy to enable gradient-driven channel selection.\nBased on the learned masks, SepPrune prunes redundant channels and fine-tunes\nthe remaining parameters to recover performance. Extensive experiments\ndemonstrate that this learnable pruning paradigm yields substantial advantages\nfor channel pruning in speech separation models, outperforming existing\nmethods. Notably, a model pruned with SepPrune can recover 85% of the\nperformance of a pre-trained model (trained over hundreds of epochs) with only\none epoch of fine-tuning, and achieves convergence 36$\\times$ faster than\ntraining from scratch. Code is available at\nhttps://github.com/itsnotacie/SepPrune."}
{"id": "2505.12960", "pdf": "https://arxiv.org/pdf/2505.12960", "abs": "https://arxiv.org/abs/2505.12960", "authors": ["Chengping He", "Mingrui Jiang", "Keyi Shan", "Szu-Hao Yang", "Zefan Li", "Shengbo Wang", "Giacomo Pedretti", "Jim Ignowski", "Can Li"], "title": "Hardware-Adaptive and Superlinear-Capacity Memristor-based Associative Memory", "categories": ["cs.LG", "cs.AI", "cs.ET"], "comment": null, "summary": "Brain-inspired computing aims to mimic cognitive functions like associative\nmemory, the ability to recall complete patterns from partial cues. Memristor\ntechnology offers promising hardware for such neuromorphic systems due to its\npotential for efficient in-memory analog computing. Hopfield Neural Networks\n(HNNs) are a classic model for associative memory, but implementations on\nconventional hardware suffer from efficiency bottlenecks, while prior\nmemristor-based HNNs faced challenges with vulnerability to hardware defects\ndue to offline training, limited storage capacity, and difficulty processing\nanalog patterns. Here we introduce and experimentally demonstrate on integrated\nmemristor hardware a new hardware-adaptive learning algorithm for associative\nmemories that significantly improves defect tolerance and capacity, and\nnaturally extends to scalable multilayer architectures capable of handling both\nbinary and continuous patterns. Our approach achieves 3x effective capacity\nunder 50% device faults compared to state-of-the-art methods. Furthermore, its\nextension to multilayer architectures enables superlinear capacity scaling\n(\\(\\propto N^{1.49}\\ for binary patterns) and effective recalling of continuous\npatterns (\\propto N^{1.74}\\ scaling), as compared to linear capacity scaling\nfor previous HNNs. It also provides flexibility to adjust capacity by tuning\nhidden neurons for the same-sized patterns. By leveraging the massive\nparallelism of the hardware enabled by synchronous updates, it reduces energy\nby 8.8x and latency by 99.7% for 64-dimensional patterns over asynchronous\nschemes, with greater improvements at scale. This promises the development of\nmore reliable memristor-based associative memory systems and enables new\napplications research due to the significantly improved capacity, efficiency,\nand flexibility."}
{"id": "2505.12269", "pdf": "https://arxiv.org/pdf/2505.12269", "abs": "https://arxiv.org/abs/2505.12269", "authors": ["Kerry Xiao", "Amy Zang"], "title": "Vague Knowledge: Evidence from Analyst Reports", "categories": ["econ.GN", "cs.AI", "cs.CL", "math.LO", "q-fin.EC", "q-fin.GN", "03B48, 03B65, 03E02, 03E15, 03E72, 18E45, 28A05, 62F15, 68T01,\n  68T35, 68T50, 91G30,", "F.4; I.2.3; I.2.4; I.2.7; J.1; J.4; J.5"], "comment": null, "summary": "People in the real world often possess vague knowledge of future payoffs, for\nwhich quantification is not feasible or desirable. We argue that language, with\ndiffering ability to convey vague information, plays an important but less\nknown-role in subjective expectations. Empirically, we find that in their\nreports, analysts include useful information in linguistic expressions but not\nnumerical forecasts. Specifically, the textual tone of analyst reports has\npredictive power for forecast errors and subsequent revisions in numerical\nforecasts, and this relation becomes stronger when analyst's language is\nvaguer, when uncertainty is higher, and when analysts are busier. Overall, our\ntheory and evidence suggest that some useful information is vaguely known and\nonly communicated through language."}
{"id": "2505.12089", "pdf": "https://arxiv.org/pdf/2505.12089", "abs": "https://arxiv.org/abs/2505.12089", "authors": ["Sangmin Lee", "Eunpil Park", "Angel Canelo", "Hyunhee Park", "Youngjo Kim", "Hyung-Ju Chun", "Xin Jin", "Chongyi Li", "Chun-Le Guo", "Radu Timofte", "Qi Wu", "Tianheng Qiu", "Yuchun Dong", "Shenglin Ding", "Guanghua Pan", "Weiyu Zhou", "Tao Hu", "Yixu Feng", "Duwei Dai", "Yu Cao", "Peng Wu", "Wei Dong", "Yanning Zhang", "Qingsen Yan", "Simon J. Larsen", "Ruixuan Jiang", "Senyan Xu", "Xingbo Wang", "Xin Lu", "Marcos V. Conde", "Javier Abad-Hernandez", "Alvaro Garcıa-Lara", "Daniel Feijoo", "Alvaro Garcıa", "Zeyu Xiao", "Zhuoyuan Li"], "title": "NTIRE 2025 Challenge on Efficient Burst HDR and Restoration: Datasets, Methods, and Results", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper reviews the NTIRE 2025 Efficient Burst HDR and Restoration\nChallenge, which aims to advance efficient multi-frame high dynamic range (HDR)\nand restoration techniques. The challenge is based on a novel RAW multi-frame\nfusion dataset, comprising nine noisy and misaligned RAW frames with various\nexposure levels per scene. Participants were tasked with developing solutions\ncapable of effectively fusing these frames while adhering to strict efficiency\nconstraints: fewer than 30 million model parameters and a computational budget\nunder 4.0 trillion FLOPs. A total of 217 participants registered, with six\nteams finally submitting valid solutions. The top-performing approach achieved\na PSNR of 43.22 dB, showcasing the potential of novel methods in this domain.\nThis paper provides a comprehensive overview of the challenge, compares the\nproposed solutions, and serves as a valuable reference for researchers and\npractitioners in efficient burst HDR and restoration."}
{"id": "2505.12967", "pdf": "https://arxiv.org/pdf/2505.12967", "abs": "https://arxiv.org/abs/2505.12967", "authors": ["Akhila Henry", "Nithin Nagaraj"], "title": "Augmented Regression Models using Neurochaos Learning", "categories": ["cs.LG", "math.DS"], "comment": null, "summary": "This study presents novel Augmented Regression Models using Neurochaos\nLearning (NL), where Tracemean features derived from the Neurochaos Learning\nframework are integrated with traditional regression algorithms : Linear\nRegression, Ridge Regression, Lasso Regression, and Support Vector Regression\n(SVR). Our approach was evaluated using ten diverse real-life datasets and a\nsynthetically generated dataset of the form $y = mx + c + \\epsilon$. Results\nshow that incorporating the Tracemean feature (mean of the chaotic neural\ntraces of the neurons in the NL architecture) significantly enhances regression\nperformance, particularly in Augmented Lasso Regression and Augmented SVR,\nwhere six out of ten real-life datasets exhibited improved predictive accuracy.\nAmong the models, Augmented Chaotic Ridge Regression achieved the highest\naverage performance boost (11.35 %). Additionally, experiments on the simulated\ndataset demonstrated that the Mean Squared Error (MSE) of the augmented models\nconsistently decreased and converged towards the Minimum Mean Squared Error\n(MMSE) as the sample size increased. This work demonstrates the potential of\nchaos-inspired features in regression tasks, offering a pathway to more\naccurate and computationally efficient prediction models."}
{"id": "2505.12284", "pdf": "https://arxiv.org/pdf/2505.12284", "abs": "https://arxiv.org/abs/2505.12284", "authors": ["Danlong Yuan", "Tian Xie", "Shaohan Huang", "Zhuocheng Gong", "Huishuai Zhang", "Chong Luo", "Furu Wei", "Dongyan Zhao"], "title": "Efficient RL Training for Reasoning Models via Length-Aware Optimization", "categories": ["cs.AI", "cs.CL"], "comment": "Under review", "summary": "Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated\nremarkable performance on reasoning tasks but often incur a long reasoning path\nwith significant memory and time costs. Existing methods primarily aim to\nshorten reasoning paths by introducing additional training data and stages. In\nthis paper, we propose three critical reward designs integrated directly into\nthe reinforcement learning process of large reasoning models, which reduce the\nresponse length without extra training stages. Experiments on four settings\nshow that our method significantly decreases response length while maintaining\nor even improving performance. Specifically, in a logic reasoning setting, we\nachieve a 40% reduction in response length averaged by steps alongside a 14%\ngain in performance. For math problems, we reduce response length averaged by\nsteps by 33% while preserving performance."}
{"id": "2505.12090", "pdf": "https://arxiv.org/pdf/2505.12090", "abs": "https://arxiv.org/abs/2505.12090", "authors": ["Mohammad Shokri", "Sarah Ita Levitan", "Rivka Levitan"], "title": "Personalized Author Obfuscation with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we investigate the efficacy of large language models (LLMs) in\nobfuscating authorship by paraphrasing and altering writing styles. Rather than\nadopting a holistic approach that evaluates performance across the entire\ndataset, we focus on user-wise performance to analyze how obfuscation\neffectiveness varies across individual authors. While LLMs are generally\neffective, we observe a bimodal distribution of efficacy, with performance\nvarying significantly across users. To address this, we propose a personalized\nprompting method that outperforms standard prompting techniques and partially\nmitigates the bimodality issue."}
{"id": "2505.12982", "pdf": "https://arxiv.org/pdf/2505.12982", "abs": "https://arxiv.org/abs/2505.12982", "authors": ["Tai Nguyen", "Phong Le", "Carola Doerr", "Nguyen Dang"], "title": "Multi-parameter Control for the (1+($λ$,$λ$))-GA on OneMax via Deep Reinforcement Learning", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "It is well known that evolutionary algorithms can benefit from dynamic\nchoices of the key parameters that control their behavior, to adjust their\nsearch strategy to the different stages of the optimization process. A\nprominent example where dynamic parameter choices have shown a provable\nsuper-constant speed-up is the $(1+(\\lambda,\\lambda))$ Genetic Algorithm\noptimizing the OneMax function. While optimal parameter control policies result\nin linear expected running times, this is not possible with static parameter\nchoices. This result has spurred a lot of interest in parameter control\npolicies. However, many works, in particular theoretical running time analyses,\nfocus on controlling one single parameter. Deriving policies for controlling\nmultiple parameters remains very challenging. In this work we reconsider the\nproblem of the $(1+(\\lambda,\\lambda))$ Genetic Algorithm optimizing OneMax. We\ndecouple its four main parameters and investigate how well state-of-the-art\ndeep reinforcement learning techniques can approximate good control policies.\nWe show that although making deep reinforcement learning learn effectively is a\nchallenging task, once it works, it is very powerful and is able to find\npolicies that outperform all previously known control policies on the same\nbenchmark. Based on the results found through reinforcement learning, we derive\na simple control policy that consistently outperforms the default\ntheory-recommended setting by $27\\%$ and the irace-tuned policy, the strongest\nexisting control policy on this benchmark, by $13\\%$, for all tested problem\nsizes up to $40{,}000$."}
{"id": "2505.12301", "pdf": "https://arxiv.org/pdf/2505.12301", "abs": "https://arxiv.org/abs/2505.12301", "authors": ["Luyu Chen", "Zeyu Zhang", "Haoran Tan", "Quanyu Dai", "Hao Yang", "Zhenhua Dong", "Xu Chen"], "title": "Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge", "categories": ["cs.AI", "cs.CL"], "comment": "19 pages, 3 tables, 3 figures", "summary": "LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm,\noffering significant efficiency and flexibility compared to human judgments.\nHowever, previous methods primarily rely on single-point evaluations,\noverlooking the inherent diversity and uncertainty in human evaluations. This\napproach leads to information loss and decreases the reliability of\nevaluations. To address this limitation, we propose a novel training framework\nthat explicitly aligns the LLM-generated judgment distribution with empirical\nhuman distributions. Specifically, we propose a distributional alignment\nobjective based on KL divergence, combined with an auxiliary cross-entropy\nregularization to stabilize the training process. Furthermore, considering that\nempirical distributions may derive from limited human annotations, we\nincorporate adversarial training to enhance model robustness against\ndistribution perturbations. Extensive experiments across various LLM backbones\nand evaluation tasks demonstrate that our framework significantly outperforms\nexisting closed-source LLMs and conventional single-point alignment methods,\nwith improved alignment quality, evaluation accuracy, and robustness."}
{"id": "2505.12094", "pdf": "https://arxiv.org/pdf/2505.12094", "abs": "https://arxiv.org/abs/2505.12094", "authors": ["M Ruhul Amin"], "title": "Attribution Projection Calculus: A Novel Framework for Causal Inference in Bayesian Networks", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ML", "60E10, 62R07, 68Q32, 68T07, 94A16", "F.2.2; G.3; I.1.2; I.2.6"], "comment": "*AI was used to improve Text and collecting Citations", "summary": "This paper introduces Attribution Projection Calculus (AP-Calculus), a novel\nmathematical framework for determining causal relationships in structured\nBayesian networks. We investigate a specific network architecture with source\nnodes connected to destination nodes through intermediate nodes, where each\ninput maps to a single label with maximum marginal probability. We prove that\nfor each label, exactly one intermediate node acts as a deconfounder while\nothers serve as confounders, enabling optimal attribution of features to their\ncorresponding labels. The framework formalizes the dual nature of intermediate\nnodes as both confounders and deconfounders depending on the context, and\nestablishes separation functions that maximize distinctions between\nintermediate representations. We demonstrate that the proposed network\narchitecture is optimal for causal inference compared to alternative\nstructures, including those based on Pearl's causal framework. AP-Calculus\nprovides a comprehensive mathematical foundation for analyzing feature-label\nattributions, managing spurious correlations, quantifying information gain,\nensuring fairness, and evaluating uncertainty in prediction models, including\nlarge language models. Theoretical verification shows that AP-Calculus not only\nextends but can also subsume traditional do-calculus for many practical\napplications, offering a more direct approach to causal inference in supervised\nlearning contexts."}
{"id": "2505.12988", "pdf": "https://arxiv.org/pdf/2505.12988", "abs": "https://arxiv.org/abs/2505.12988", "authors": ["Douglas Orr", "Luka Ribar", "Carlo Luschi"], "title": "Optimal Formats for Weight Quantisation", "categories": ["cs.LG"], "comment": "35 pages, 32 figures", "summary": "Weight quantisation is an essential technique for enabling efficient training\nand deployment of modern deep learning models. However, the recipe book of\nquantisation formats is large and the formats are often chosen empirically. In\nthis paper, we propose a framework for systematic design and analysis of\nquantisation formats. By connecting the question of format design with the\nclassical quantisation theory, we show that the strong practical performance of\npopular formats comes from their ability to represent values using\nvariable-length codes. Framing the optimisation problem as minimising the KL\ndivergence between the original and quantised model outputs, the objective is\naligned with minimising the squared quantisation error of the model parameters.\nWe therefore develop and evaluate squared-error-optimal formats for known\ndistributions, observing significant improvement of variable-length codes over\nfixed-length codes. Uniform quantisation followed by lossless compression with\na variable-length code is shown to be optimal. However, we find that commonly\nused block formats and sparse outlier formats also outperform fixed-length\ncodes, implying they also exploit variable-length encoding. Finally, by using\nthe relationship between the Fisher information and KL divergence, we derive\nthe optimal allocation of bit-widths to individual parameter tensors across the\nmodel's layers, saving up to 0.25 bits per parameter when tested with\ndirect-cast quantisation of language models."}
{"id": "2505.12307", "pdf": "https://arxiv.org/pdf/2505.12307", "abs": "https://arxiv.org/abs/2505.12307", "authors": ["Maoyuan Ye", "Jing Zhang", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?", "categories": ["cs.CV", "cs.CL"], "comment": "GitHub: \\url{https://github.com/MiliLab/LogicOCR}", "summary": "Recent advances in Large Multimodal Models (LMMs) have significantly improved\ntheir reasoning and Optical Character Recognition (OCR) capabilities. However,\ntheir performance on complex logical reasoning tasks involving text-rich images\nremains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark\ncomprising 1,100 multiple-choice questions designed to evaluate LMMs' logical\nreasoning abilities on text-rich images, while minimizing reliance on\ndomain-specific knowledge (e.g., mathematics). We construct LogicOCR by\ncurating a text corpus from the Chinese National Civil Servant Examination and\ndevelop a scalable, automated pipeline to convert it into multimodal samples.\nFirst, we design prompt templates to steer GPT-Image-1 to generate images with\ndiverse backgrounds, interleaved text-illustration layouts, and varied fonts,\nensuring contextual relevance and visual realism. Then, the generated images\nare manually verified, with low-quality examples discarded. We evaluate a range\nof representative open-source and proprietary LMMs under both Chain-of-Thought\n(CoT) and direct-answer settings. Our multi-dimensional analysis reveals key\ninsights, such as the impact of test-time scaling, input modality differences,\nand sensitivity to visual-text orientation. Notably, LMMs still lag in\nmultimodal reasoning compared to text-only inputs, indicating that they have\nnot fully bridged visual reading with reasoning. We hope LogicOCR will serve as\na valuable resource for advancing multimodal reasoning research. The dataset is\navailable at https://github.com/MiliLab/LogicOCR."}
{"id": "2505.12096", "pdf": "https://arxiv.org/pdf/2505.12096", "abs": "https://arxiv.org/abs/2505.12096", "authors": ["Alberto Bassi", "Carlo Albert", "Aurelien Lucchi", "Marco Baity-Jesi", "Emanuele Francazi"], "title": "When the Left Foot Leads to the Right Path: Bridging Initial Prejudice and Trainability", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Understanding the statistical properties of deep neural networks (DNNs) at\ninitialization is crucial for elucidating both their trainability and the\nintrinsic architectural biases they encode prior to data exposure. Mean-field\n(MF) analyses have demonstrated that the parameter distribution in randomly\ninitialized networks dictates whether gradients vanish or explode.\nConcurrently, untrained DNNs were found to exhibit an initial-guessing bias\n(IGB), in which large regions of the input space are assigned to a single\nclass. In this work, we derive a theoretical proof establishing the\ncorrespondence between IGB and previous MF theories, thereby connecting a\nnetwork prejudice toward specific classes with the conditions for fast and\naccurate learning. This connection yields the counter-intuitive conclusion: the\ninitialization that optimizes trainability is necessarily biased, rather than\nneutral. Furthermore, we extend the MF/IGB framework to multi-node activation\nfunctions, offering practical guidelines for designing initialization schemes\nthat ensure stable optimization in architectures employing max- and\naverage-pooling layers."}
{"id": "2505.12992", "pdf": "https://arxiv.org/pdf/2505.12992", "abs": "https://arxiv.org/abs/2505.12992", "authors": ["Baohao Liao", "Hanze Dong", "Yuhui Xu", "Doyen Sahoo", "Christof Monz", "Junnan Li", "Caiming Xiong"], "title": "Fractured Chain-of-Thought Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning."}
{"id": "2505.12312", "pdf": "https://arxiv.org/pdf/2505.12312", "abs": "https://arxiv.org/abs/2505.12312", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Visuospatial Cognitive Assistant", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "31 pages, 10 figures, 6 tables. The implementation and fine-tuned\n  model (ViCA-7B) are publicly available at https://huggingface.co/nkkbr/ViCA.\n  The ViCA-322K dataset can be found at\n  https://huggingface.co/datasets/nkkbr/ViCA-322K, and the ViCA-Thinking-2.68K\n  dataset is at https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k", "summary": "Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence."}
{"id": "2505.12100", "pdf": "https://arxiv.org/pdf/2505.12100", "abs": "https://arxiv.org/abs/2505.12100", "authors": ["Isabela Pereira Gregio", "Ian Pons", "Anna Helena Reali Costa", "Artur Jordão"], "title": "Improving Fairness in LLMs Through Testing-Time Adversaries", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) push the bound-aries in natural language\nprocessing and generative AI, driving progress across various aspects of modern\nsociety. Unfortunately, the pervasive issue of bias in LLMs responses (i.e.,\npredictions) poses a significant and open challenge, hindering their\napplication in tasks involving ethical sensitivity and responsible\ndecision-making. In this work, we propose a straightforward, user-friendly and\npractical method to mitigate such biases, enhancing the reliability and\ntrustworthiness of LLMs. Our method creates multiple variations of a given\nsentence by modifying specific attributes and evaluates the corresponding\nprediction behavior compared to the original, unaltered, prediction/sentence.\nThe idea behind this process is that critical ethical predictions often exhibit\nnotable inconsistencies, indicating the presence of bias. Unlike previous\napproaches, our method relies solely on forward passes (i.e., testing-time\nadversaries), eliminating the need for training, fine-tuning, or prior\nknowledge of the training data distribution. Through extensive experiments on\nthe popular Llama family, we demonstrate the effectiveness of our method in\nimproving various fairness metrics, focusing on the reduction of disparities in\nhow the model treats individuals from different racial groups. Specifically,\nusing standard metrics, we improve the fairness in Llama3 in up to 27\npercentage points. Overall, our approach significantly enhances fairness,\nequity, and reliability in LLM-generated results without parameter tuning or\ntraining data modifications, confirming its effectiveness in practical\nscenarios. We believe our work establishes an important step toward enabling\nthe use of LLMs in tasks that require ethical considerations and responsible\ndecision-making."}
{"id": "2505.13007", "pdf": "https://arxiv.org/pdf/2505.13007", "abs": "https://arxiv.org/abs/2505.13007", "authors": ["James E. Warner", "Tristan A. Shah", "Patrick E. Leser", "Geoffrey F. Bomarito", "Joshua D. Pribe", "Michael C. Stanley"], "title": "Generative Modeling of Random Fields from Limited Data via Constrained Latent Flow Matching", "categories": ["cs.LG", "cs.CE"], "comment": "10 pages plus references and appendices, 17 figures", "summary": "Deep generative models are promising tools for science and engineering, but\ntheir reliance on abundant, high-quality data limits applicability. We present\na novel framework for generative modeling of random fields (probability\ndistributions over continuous functions) that incorporates domain knowledge to\nsupplement limited, sparse, and indirect data. The foundation of the approach\nis latent flow matching, where generative modeling occurs on compressed\nfunction representations in the latent space of a pre-trained variational\nautoencoder (VAE). Innovations include the adoption of a function decoder\nwithin the VAE and integration of physical/statistical constraints into the VAE\ntraining process. In this way, a latent function representation is learned that\nyields continuous random field samples satisfying domain-specific constraints\nwhen decoded, even in data-limited regimes. Efficacy is demonstrated on two\nchallenging applications: wind velocity field reconstruction from sparse\nsensors and material property inference from a limited number of indirect\nmeasurements. Results show that the proposed framework achieves significant\nimprovements in reconstruction accuracy compared to unconstrained methods and\nenables effective inference with relatively small training datasets that is\nintractable without constraints."}
{"id": "2505.12363", "pdf": "https://arxiv.org/pdf/2505.12363", "abs": "https://arxiv.org/abs/2505.12363", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "26 pages, 19 figures, 4 tables. Code, models, and dataset are\n  available at our project page: https://github.com/nkkbr/ViCA", "summary": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research."}
{"id": "2505.12107", "pdf": "https://arxiv.org/pdf/2505.12107", "abs": "https://arxiv.org/abs/2505.12107", "authors": ["Rajarshi Roy", "Yash Pote", "David Parker", "Marta Kwiatkowska"], "title": "Learning Probabilistic Temporal Logic Specifications for Stochastic Systems", "categories": ["cs.LO", "cs.AI", "cs.FL"], "comment": "Full version of the paper that appears in IJCAI'25", "summary": "There has been substantial progress in the inference of formal behavioural\nspecifications from sample trajectories, for example, using Linear Temporal\nLogic (LTL). However, these techniques cannot handle specifications that\ncorrectly characterise systems with stochastic behaviour, which occur commonly\nin reinforcement learning and formal verification. We consider the passive\nlearning problem of inferring a Boolean combination of probabilistic LTL (PLTL)\nformulas from a set of Markov chains, classified as either positive or\nnegative. We propose a novel learning algorithm that infers concise PLTL\nspecifications, leveraging grammar-based enumeration, search heuristics,\nprobabilistic model checking and Boolean set-cover procedures. We demonstrate\nthe effectiveness of our algorithm in two use cases: learning from policies\ninduced by RL algorithms and learning from variants of a probabilistic model.\nIn both cases, our method automatically and efficiently extracts PLTL\nspecifications that succinctly characterise the temporal differences between\nthe policies or model variants."}
{"id": "2505.13025", "pdf": "https://arxiv.org/pdf/2505.13025", "abs": "https://arxiv.org/abs/2505.13025", "authors": ["Jiyuan Pei", "Yi Mei", "Jialin Liu", "Mengjie Zhang"], "title": "LiBOG: Lifelong Learning for Black-Box Optimizer Generation", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at IJCAI 2025. To appear", "summary": "Meta-Black-Box Optimization (MetaBBO) garners attention due to its success in\nautomating the configuration and generation of black-box optimizers,\nsignificantly reducing the human effort required for optimizer design and\ndiscovering optimizers with higher performance than classic human-designed\noptimizers. However, existing MetaBBO methods conduct one-off training under\nthe assumption that a stationary problem distribution with extensive and\nrepresentative training problem samples is pre-available. This assumption is\noften impractical in real-world scenarios, where diverse problems following\nshifting distribution continually arise. Consequently, there is a pressing need\nfor methods that can continuously learn from new problems encountered\non-the-fly and progressively enhance their capabilities. In this work, we\nexplore a novel paradigm of lifelong learning in MetaBBO and introduce LiBOG, a\nnovel approach designed to learn from sequentially encountered problems and\ngenerate high-performance optimizers for Black-Box Optimization (BBO). LiBOG\nconsolidates knowledge both across tasks and within tasks to mitigate\ncatastrophic forgetting. Extensive experiments demonstrate LiBOG's\neffectiveness in learning to generate high-performance optimizers in a lifelong\nlearning manner, addressing catastrophic forgetting while maintaining\nplasticity to learn new tasks."}
{"id": "2505.12371", "pdf": "https://arxiv.org/pdf/2505.12371", "abs": "https://arxiv.org/abs/2505.12371", "authors": ["Yinghao Zhu", "Ziyi He", "Haoran Hu", "Xiaochen Zheng", "Xichen Zhang", "Zixiang Wang", "Junyi Gao", "Liantao Ma", "Lequan Yu"], "title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/."}
{"id": "2505.12108", "pdf": "https://arxiv.org/pdf/2505.12108", "abs": "https://arxiv.org/abs/2505.12108", "authors": ["Jiancheng Pan", "Shiye Lei", "Yuqian Fu", "Jiahao Li", "Yanxing Liu", "Yuze Sun", "Xiao He", "Long Peng", "Xiaomeng Huang", "Bo Zhao"], "title": "EarthSynth: Generating Informative Earth Observation with Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "23 pages", "summary": "Remote sensing image (RSI) interpretation typically faces challenges due to\nthe scarcity of labeled data, which limits the performance of RSI\ninterpretation tasks. To tackle this challenge, we propose EarthSynth, a\ndiffusion-based generative foundation model that enables synthesizing\nmulti-category, cross-satellite labeled Earth observation for downstream RSI\ninterpretation tasks. To the best of our knowledge, EarthSynth is the first to\nexplore multi-task generation for remote sensing. EarthSynth, trained on the\nEarthSynth-180K dataset, employs the Counterfactual Composition training\nstrategy to improve training data diversity and enhance category control.\nFurthermore, a rule-based method of R-Filter is proposed to filter more\ninformative synthetic data for downstream tasks. We evaluate our EarthSynth on\nscene classification, object detection, and semantic segmentation in open-world\nscenarios, offering a practical solution for advancing RSI interpretation."}
{"id": "2505.13026", "pdf": "https://arxiv.org/pdf/2505.13026", "abs": "https://arxiv.org/abs/2505.13026", "authors": ["Jack Chen", "Fazhong Liu", "Naruto Liu", "Yuhan Luo", "Erqu Qin", "Harry Zheng", "Tian Dong", "Haojin Zhu", "Yan Meng", "Xiao Wang"], "title": "Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at mathematical reasoning and logical\nproblem-solving. The current popular training paradigms primarily use\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the\nmodels' reasoning abilities. However, when using SFT or RL alone, there are\nrespective challenges: SFT may suffer from overfitting, while RL is prone to\nmode collapse. The state-of-the-art methods have proposed hybrid training\nschemes. However, static switching faces challenges such as poor generalization\nacross different tasks and high dependence on data quality. In response to\nthese challenges, inspired by the curriculum learning-quiz mechanism in human\nreasoning cultivation, We propose SASR, a step-wise adaptive hybrid training\nframework that theoretically unifies SFT and RL and dynamically balances the\ntwo throughout optimization. SASR uses SFT for initial warm-up to establish\nbasic reasoning skills, and then uses an adaptive dynamic adjustment algorithm\nbased on gradient norm and divergence relative to the original distribution to\nseamlessly integrate SFT with the online RL method GRPO. By monitoring the\ntraining status of LLMs and adjusting the training process in sequence, SASR\nensures a smooth transition between training schemes, maintaining core\nreasoning abilities while exploring different paths. Experimental results\ndemonstrate that SASR outperforms SFT, RL, and static hybrid training methods."}
{"id": "2505.12442", "pdf": "https://arxiv.org/pdf/2505.12442", "abs": "https://arxiv.org/abs/2505.12442", "authors": ["Liwen Wang", "Wenxuan Wang", "Shuai Wang", "Zongjie Li", "Zhenlan Ji", "Zongyi Lyu", "Daoyuan Wu", "Shing-Chi Cheung"], "title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses."}
{"id": "2505.12109", "pdf": "https://arxiv.org/pdf/2505.12109", "abs": "https://arxiv.org/abs/2505.12109", "authors": ["Matthew Landers", "Taylor W. Killian", "Thomas Hartvigsen", "Afsaneh Doryab"], "title": "SAINT: Attention-Based Modeling of Sub-Action Dependencies in Multi-Action Policies", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The combinatorial structure of many real-world action spaces leads to\nexponential growth in the number of possible actions, limiting the\neffectiveness of conventional reinforcement learning algorithms. Recent\napproaches for combinatorial action spaces impose factorized or sequential\nstructures over sub-actions, failing to capture complex joint behavior. We\nintroduce the Sub-Action Interaction Network using Transformers (SAINT), a\nnovel policy architecture that represents multi-component actions as unordered\nsets and models their dependencies via self-attention conditioned on the global\nstate. SAINT is permutation-invariant, sample-efficient, and compatible with\nstandard policy optimization algorithms. In 15 distinct combinatorial\nenvironments across three task domains, including environments with nearly 17\nmillion joint actions, SAINT consistently outperforms strong baselines."}
{"id": "2505.13027", "pdf": "https://arxiv.org/pdf/2505.13027", "abs": "https://arxiv.org/abs/2505.13027", "authors": ["Zihan Gu", "Han Zhang", "Ruoyu Chen", "Yue Hu", "Hua Zhang"], "title": "Unpacking Positional Encoding in Transformers: A Spectral Analysis of Content-Position Coupling", "categories": ["cs.LG"], "comment": null, "summary": "Positional encoding (PE) is essential for enabling Transformers to model\nsequential structure. However, the mechanisms by which different PE schemes\ncouple token content and positional information-and how these mechanisms\ninfluence model dynamics-remain theoretically underexplored. In this work, we\npresent a unified framework that analyzes PE through the spectral properties of\nToeplitz and related matrices derived from attention logits. We show that\nmultiplicative content-position coupling-exemplified by Rotary Positional\nEncoding (RoPE) via a Hadamard product with a Toeplitz matrix-induces spectral\ncontraction, which theoretically improves optimization stability and\nefficiency. Guided by this theory, we construct synthetic tasks that contrast\ncontent-position dependent and content-position independent settings, and\nevaluate a range of PE methods. Our experiments reveal strong alignment with\ntheory: RoPE consistently outperforms other methods on position-sensitive tasks\nand induces \"single-head deposit\" patterns in early layers, indicating\nlocalized positional processing. Further analyses show that modifying the\nmethod and timing of PE coupling, such as MLA in Deepseek-V3, can effectively\nmitigate this concentration. These results establish explicit content-relative\nmixing with relative-position Toeplitz signals as a key principle for effective\nPE design and provide new insight into how positional structure is integrated\nin Transformer architectures."}
{"id": "2505.12457", "pdf": "https://arxiv.org/pdf/2505.12457", "abs": "https://arxiv.org/abs/2505.12457", "authors": ["Yang Zhao", "Kai Xiong", "Xiao Ding", "Li Du", "YangouOuyang", "Zhouhao Sun", "Jiannan Guan", "Wenbin Zhang", "Bin Liu", "Dong Hu", "Bing Qin", "Ting Liu"], "title": "UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Scaling RL for LLMs is computationally expensive, largely due to\nmulti-sampling for policy optimization and evaluation, making efficient data\nselection crucial. Inspired by the Zone of Proximal Development (ZPD) theory,\nwe hypothesize LLMs learn best from data within their potential comprehension\nzone. Addressing the limitation of conventional, computationally intensive\nmulti-sampling methods for data assessment, we introduce UFO-RL. This novel\nframework uses a computationally efficient single-pass uncertainty estimation\nto identify informative data instances, achieving up to 185x faster data\nevaluation. UFO-RL leverages this metric to select data within the estimated\nZPD for training. Experiments show that training with just 10% of data selected\nby UFO-RL yields performance comparable to or surpassing full-data training,\nreducing overall training time by up to 16x while enhancing stability and\ngeneralization. UFO-RL offers a practical and highly efficient strategy for\nscaling RL fine-tuning of LLMs by focusing learning on valuable data."}
{"id": "2505.12130", "pdf": "https://arxiv.org/pdf/2505.12130", "abs": "https://arxiv.org/abs/2505.12130", "authors": ["Niaz Ahmad", "Jawad Khan", "Kang G. Shin", "Youngmoon Lee", "Guanghui Wang"], "title": "Keypoints as Dynamic Centroids for Unified Human Pose and Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The dynamic movement of the human body presents a fundamental challenge for\nhuman pose estimation and body segmentation. State-of-the-art approaches\nprimarily rely on combining keypoint heatmaps with segmentation masks but often\nstruggle in scenarios involving overlapping joints or rapidly changing poses\nduring instance-level segmentation. To address these limitations, we propose\nKeypoints as Dynamic Centroid (KDC), a new centroid-based representation for\nunified human pose estimation and instance-level segmentation. KDC adopts a\nbottom-up paradigm to generate keypoint heatmaps for both easily\ndistinguishable and complex keypoints and improves keypoint detection and\nconfidence scores by introducing KeyCentroids using a keypoint disk. It\nleverages high-confidence keypoints as dynamic centroids in the embedding space\nto generate MaskCentroids, allowing for swift clustering of pixels to specific\nhuman instances during rapid body movements in live environments. Our\nexperimental evaluations on the CrowdPose, OCHuman, and COCO benchmarks\ndemonstrate KDC's effectiveness and generalizability in challenging scenarios\nin terms of both accuracy and runtime performance. The implementation is\navailable at: https://sites.google.com/view/niazahmad/projects/kdc."}
{"id": "2505.13033", "pdf": "https://arxiv.org/pdf/2505.13033", "abs": "https://arxiv.org/abs/2505.13033", "authors": ["Vijay Ekambaram", "Subodh Kumar", "Arindam Jati", "Sumanta Mukherjee", "Tomoya Sakai", "Pankaj Dayama", "Wesley M. Gifford", "Jayant Kalagnanam"], "title": "TSPulse: Dual Space Tiny Pre-Trained Models for Rapid Time-Series Analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rise of time-series pre-trained models has advanced temporal\nrepresentation learning, but current state-of-the-art models are often\nlarge-scale, requiring substantial compute. We introduce TSPulse, ultra-compact\ntime-series pre-trained models with only 1M parameters, specialized to perform\nstrongly across classification, anomaly detection, imputation, and retrieval\ntasks. TSPulse introduces innovations at both the architecture and task levels.\nAt the architecture level, it employs a dual-space masked reconstruction,\nlearning from both time and frequency domains to capture complementary signals.\nThis is further enhanced by a dual-embedding disentanglement, generating both\ndetailed embeddings for fine-grained analysis and high-level semantic\nembeddings for broader task understanding. Notably, TSPulse's semantic\nembeddings are robust to shifts in time, magnitude, and noise, which is\nimportant for robust retrieval. At the task level, TSPulse incorporates TSLens,\na fine-tuning component enabling task-specific feature attention. It also\nintroduces a multi-head triangulation technique that correlates deviations from\nmultiple prediction heads, enhancing anomaly detection by fusing complementary\nmodel outputs. Additionally, a hybrid mask pretraining is proposed to improves\nzero-shot imputation by reducing pre-training bias. These architecture and task\ninnovations collectively contribute to TSPulse's significant performance gains:\n5-16% on the UEA classification benchmarks, +20% on the TSB-AD anomaly\ndetection leaderboard, +50% in zero-shot imputation, and +25% in time-series\nretrieval. Remarkably, these results are achieved with just 1M parameters,\nmaking TSPulse 10-100X smaller than existing pre-trained models. Its efficiency\nenables GPU-free inference and rapid pre-training, setting a new standard for\nefficient time-series pre-trained models. Models will be open-sourced soon."}
{"id": "2505.12565", "pdf": "https://arxiv.org/pdf/2505.12565", "abs": "https://arxiv.org/abs/2505.12565", "authors": ["Carl Edwards", "Chi Han", "Gawon Lee", "Thao Nguyen", "Bowen Jin", "Chetan Kumar Prasad", "Sara Szymkuć", "Bartosz A. Grzybowski", "Ying Diao", "Jiawei Han", "Ge Liu", "Hao Peng", "Martin D. Burke", "Heng Ji"], "title": "mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model", "categories": ["cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Despite their ability to understand chemical knowledge and accurately\ngenerate sequential representations, large language models (LLMs) remain\nlimited in their capacity to propose novel molecules with drug-like properties.\nIn addition, the molecules that LLMs propose can often be challenging to make\nin the lab. To more effectively enable the discovery of functional small\nmolecules, LLMs need to learn a molecular language. However, LLMs are currently\nlimited by encoding molecules from atoms. In this paper, we argue that just\nlike tokenizing texts into (sub-)word tokens instead of characters, molecules\nshould be decomposed and reassembled at the level of functional building\nblocks, i.e., parts of molecules that bring unique functions and serve as\neffective building blocks for real-world automated laboratory synthesis. This\nmotivates us to propose mCLM, a modular Chemical-Language Model tokenizing\nmolecules into building blocks and learning a bilingual language model of both\nnatural language descriptions of functions and molecule building blocks. By\nreasoning on such functional building blocks, mCLM guarantees to generate\nefficiently synthesizable molecules thanks to recent progress in block-based\nchemistry, while also improving the functions of molecules in a principled\nmanner. In experiments on 430 FDA-approved drugs, we find mCLM capable of\nsignificantly improving 5 out of 6 chemical functions critical to determining\ndrug potentials. More importantly, mCLM can reason on multiple functions and\nimprove the FDA-rejected drugs (``fallen angels'') over multiple iterations to\ngreatly improve their shortcomings."}
{"id": "2505.12143", "pdf": "https://arxiv.org/pdf/2505.12143", "abs": "https://arxiv.org/abs/2505.12143", "authors": ["Arun Kumar", "Paul Schrater"], "title": "Structured Representation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Invariant representations are core to representation learning, yet a central\nchallenge remains: uncovering invariants that are stable and transferable\nwithout suppressing task-relevant signals. This raises fundamental questions,\nrequiring further inquiry, about the appropriate level of abstraction at which\nsuch invariants should be defined, and which aspects of a system they should\ncharacterize. Interpretation of the environment relies on abstract knowledge\nstructures to make sense of the current state, which leads to interactions,\nessential drivers of learning and knowledge acquisition. We posit that\ninterpretation operates at the level of higher-order relational knowledge;\nhence, invariant structures must be where knowledge resides, specifically, as\npartitions defined by the closure of relational paths within an abstract\nknowledge space. These partitions serve as the core invariant representations,\nforming the structural substrate where knowledge is stored and learning occurs.\nOn the other hand, inter-partition connectors enable the deployment of these\nknowledge partitions encoding task-relevant transitions. Thus, invariant\npartitions provide the foundational primitives of structured representation. We\nformalize the computational foundations for structured representation of the\ninvariant partitions based on closed semiring, a relational algebraic\nstructure."}
{"id": "2505.13047", "pdf": "https://arxiv.org/pdf/2505.13047", "abs": "https://arxiv.org/abs/2505.13047", "authors": ["Hongrui Kou", "Jingkai Li", "Ziyu Wang", "Zhouhang Lv", "Yuxin Zhang", "Cheng Wang"], "title": "PPTNet: A Hybrid Periodic Pattern-Transformer Architecture for Traffic Flow Prediction and Congestion Identification", "categories": ["cs.LG"], "comment": null, "summary": "Accurate prediction of traffic flow parameters and real time identification\nof congestion states are essential for the efficient operation of intelligent\ntransportation systems. This paper proposes a Periodic Pattern Transformer\nNetwork (PPTNet) for traffic flow prediction, integrating periodic pattern\nextraction with the Transformer architecture, coupled with a fuzzy inference\nmethod for real-time congestion identification. Firstly, a high-precision\ntraffic flow dataset (Traffic Flow Dataset for China's Congested Highways and\nExpressways, TF4CHE) suitable for congested highway scenarios in China is\nconstructed based on drone aerial imagery data. Subsequently, the proposed\nPPTNet employs Fast Fourier Transform to capture multi-scale periodic patterns\nand utilizes two-dimensional Inception convolutions to efficiently extract\nintra and inter periodic features. A Transformer decoder dynamically models\ntemporal dependencies, enabling accurate predictions of traffic density and\nspeed. Finally, congestion probabilities are calculated in real-time using the\npredicted outcomes via a Mamdani fuzzy inference-based congestion\nidentification module. Experimental results demonstrate that the proposed\nPPTNet significantly outperforms mainstream traffic prediction methods in\nprediction accuracy, and the congestion identification module effectively\nidentifies real-time road congestion states, verifying the superiority and\npracticality of the proposed method in real-world traffic scenarios. Project\npage: https://github.com/ADSafetyJointLab/PPTNet."}
{"id": "2505.12629", "pdf": "https://arxiv.org/pdf/2505.12629", "abs": "https://arxiv.org/abs/2505.12629", "authors": ["Yuchang Sun", "Yanxi Chen", "Yaliang Li", "Bolin Ding"], "title": "Enhancing Latent Computation in Transformers with Latent Tokens", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Augmenting large language models (LLMs) with auxiliary tokens has emerged as\na promising strategy for enhancing model performance. In this work, we\nintroduce a lightweight method termed latent tokens; these are dummy tokens\nthat may be non-interpretable in natural language but steer the autoregressive\ndecoding process of a Transformer-based LLM via the attention mechanism. The\nproposed latent tokens can be seamlessly integrated with a pre-trained\nTransformer, trained in a parameter-efficient manner, and applied flexibly at\ninference time, while adding minimal complexity overhead to the existing\ninfrastructure of standard Transformers. We propose several hypotheses about\nthe underlying mechanisms of latent tokens and design synthetic tasks\naccordingly to verify them. Numerical results confirm that the proposed method\nnoticeably outperforms the baselines, particularly in the out-of-distribution\ngeneralization scenarios, highlighting its potential in improving the\nadaptability of LLMs."}
{"id": "2505.12151", "pdf": "https://arxiv.org/pdf/2505.12151", "abs": "https://arxiv.org/abs/2505.12151", "authors": ["Alex Heyman", "Joel Zylberberg"], "title": "Reasoning Large Language Model Errors Arise from Hallucinating Critical Problem Features", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.7"], "comment": "13 pages (9 excluding references and appendices); 7 figures (6\n  excluding appendices)", "summary": "Large language models have recently made great strides in reasoning task\nperformance through chain-of-thought (CoT) strategies trained via reinforcement\nlearning; however, these \"reasoning large language models\" (RLLMs) remain\nimperfect reasoners, and understanding the frequencies and causes of their\nfailure modes is important for both users and developers. We test o1-mini,\no3-mini, DeepSeek-R1, Claude 3.7 Sonnet, Gemini 2.5 Pro Preview, and Grok 3\nMini Beta on graph coloring as a variable-complexity constraint-satisfaction\nlogic problem, and find evidence from both error rate comparisons and\nCoT/explanation text analysis that RLLMs are prone to hallucinate edges not\nspecified in the prompt's description of the graph. This phenomenon persists\nacross multiple problem complexity levels and semantic frames, and it appears\nto account for a significant fraction of the incorrect answers from every\ntested model, and the vast majority of them for some models. Our results\nindicate that RLLMs may possess broader issues with misrepresentation of\nproblem specifics, and we offer suggestions for design choices to mitigate this\nweakness."}
{"id": "2505.13058", "pdf": "https://arxiv.org/pdf/2505.13058", "abs": "https://arxiv.org/abs/2505.13058", "authors": ["Gabriel Béna", "Maxence Faldor", "Dan F. M. Goodman", "Antoine Cully"], "title": "A Path to Universal Neural Cellular Automata", "categories": ["cs.LG", "cs.ET", "cs.NE"], "comment": "Published in Genetic and Evolutionary Computation Conference (GECCO\n  '25 Companion), July 14--18, 2025, Malaga, Spain. 8 Pages + References", "summary": "Cellular automata have long been celebrated for their ability to generate\ncomplex behaviors from simple, local rules, with well-known discrete models\nlike Conway's Game of Life proven capable of universal computation. Recent\nadvancements have extended cellular automata into continuous domains, raising\nthe question of whether these systems retain the capacity for universal\ncomputation. In parallel, neural cellular automata have emerged as a powerful\nparadigm where rules are learned via gradient descent rather than manually\ndesigned. This work explores the potential of neural cellular automata to\ndevelop a continuous Universal Cellular Automaton through training by gradient\ndescent. We introduce a cellular automaton model, objective functions and\ntraining strategies to guide neural cellular automata toward universal\ncomputation in a continuous setting. Our experiments demonstrate the successful\ntraining of fundamental computational primitives - such as matrix\nmultiplication and transposition - culminating in the emulation of a neural\nnetwork solving the MNIST digit classification task directly within the\ncellular automata state. These results represent a foundational step toward\nrealizing analog general-purpose computers, with implications for understanding\nuniversal computation in continuous dynamics and advancing the automated\ndiscovery of complex cellular automata behaviors via machine learning."}
{"id": "2505.12632", "pdf": "https://arxiv.org/pdf/2505.12632", "abs": "https://arxiv.org/abs/2505.12632", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation."}
{"id": "2505.12155", "pdf": "https://arxiv.org/pdf/2505.12155", "abs": "https://arxiv.org/abs/2505.12155", "authors": ["Ranit Karmakar", "Simon F. Nørrelykke"], "title": "SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Segmentation evaluation metrics traditionally rely on binary decision logic:\npredictions are either correct or incorrect, based on rigid IoU thresholds.\nDetection--based metrics such as F1 and mAP determine correctness at the object\nlevel using fixed overlap cutoffs, while overlap--based metrics like\nIntersection over Union (IoU) and Dice operate at the pixel level, often\noverlooking instance--level structure. Panoptic Quality (PQ) attempts to unify\ndetection and segmentation assessment, but it remains dependent on\nhard-threshold matching--treating predictions below the threshold as entirely\nincorrect. This binary framing obscures important distinctions between\nqualitatively different errors and fails to reward gradual model improvements.\nWe propose SoftPQ, a flexible and interpretable instance segmentation metric\nthat redefines evaluation as a graded continuum rather than a binary\nclassification. SoftPQ introduces tunable upper and lower IoU thresholds to\ndefine a partial matching region and applies a sublinear penalty function to\nambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit\nsmoother score behavior, greater robustness to structural segmentation errors,\nand more informative feedback for model development and evaluation. Through\ncontrolled perturbation experiments, we show that SoftPQ captures meaningful\ndifferences in segmentation quality that existing metrics overlook, making it a\npractical and principled alternative for both benchmarking and iterative model\nrefinement."}
{"id": "2505.13060", "pdf": "https://arxiv.org/pdf/2505.13060", "abs": "https://arxiv.org/abs/2505.13060", "authors": ["Shmulik Markovich-Golan", "Daniel Ohayon", "Itay Niv", "Yair Hanani"], "title": "Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs", "categories": ["cs.LG"], "comment": "Preprint, under review", "summary": "Quantization is essential for Neural Network (NN) compression, reducing model\nsize and computational demands by using lower bit-width data types, though\naggressive reduction often hampers accuracy. Mixed Precision (MP) mitigates\nthis tradeoff by varying the numerical precision across network layers. This\nstudy focuses on automatically selecting an optimal MP configuration within\nPost-Training Quantization (PTQ) for inference. The first key contribution is a\nnovel sensitivity metric derived from a first-order Taylor series expansion of\nthe loss function as a function of quantization errors in weights and\nactivations. This metric, based on the Mean Square Error (MSE) of the loss, is\nefficiently calculated per layer using high-precision forward and backward\npasses over a small calibration dataset. The metric is additive across layers,\nwith low calibration memory overhead as weight optimization is unnecessary. The\nsecond contribution is an accurate hardware-aware method for predicting MP time\ngain by modeling it as additive for sequential sub-graphs. An algorithm\npartitions the model graph into sequential subgraphs, measuring time gain for\neach configuration using a few samples. After calibrating per-layer sensitivity\nand time gain, an Integer Programming (IP) problem is formulated to maximize\ntime gain while keeping loss MSE below a set threshold. Memory gain and\ntheoretical time gain based on Multiply and Accumulate (MAC) operations are\nalso considered. Rigorous experiments on the Intel Gaudi 2 accelerator validate\nthe approach on several Large Language Models (LLMs)."}
{"id": "2505.12680", "pdf": "https://arxiv.org/pdf/2505.12680", "abs": "https://arxiv.org/abs/2505.12680", "authors": ["Haoyu Zhao", "Yihan Geng", "Shange Tang", "Yong Lin", "Bohan Lyu", "Hongzhou Lin", "Chi Jin", "Sanjeev Arora"], "title": "Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "27 pages", "summary": "LLM-based formal proof assistants (e.g., in Lean) hold great promise for\nautomating mathematical discovery. But beyond syntactic correctness, do these\nsystems truly understand mathematical structure as humans do? We investigate\nthis question through the lens of mathematical inequalities -- a fundamental\ntool across many domains. While modern provers can solve basic inequalities, we\nprobe their ability to handle human-intuitive compositionality. We introduce\nIneq-Comp, a benchmark built from elementary inequalities through systematic\ntransformations, including variable duplication, algebraic rewriting, and\nmulti-step composition. Although these problems remain easy for humans, we find\nthat most provers -- including Goedel, STP, and Kimina-7B -- struggle\nsignificantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly\nbecause it is trained to decompose the problems into sub-problems -- but still\nsuffers a 20\\% performance drop (pass@32). Strikingly, performance remains poor\nfor all models even when formal proofs of the constituent parts are provided in\ncontext, revealing that the source of weakness is indeed in compositional\nreasoning. Our results expose a persisting gap between the generalization\nbehavior of current AI provers and human mathematical intuition."}
{"id": "2505.12183", "pdf": "https://arxiv.org/pdf/2505.12183", "abs": "https://arxiv.org/abs/2505.12183", "authors": ["Manari Hirose", "Masato Uchida"], "title": "Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "23 pages, 5 figures, 17 tables", "summary": "The widespread integration of Large Language Models (LLMs) across various\nsectors has highlighted the need for empirical research to understand their\nbiases, thought patterns, and societal implications to ensure ethical and\neffective use. In this study, we propose a novel framework for evaluating LLMs,\nfocusing on uncovering their ideological biases through a quantitative analysis\nof 436 binary-choice questions, many of which have no definitive answer. By\napplying our framework to ChatGPT and Gemini, findings revealed that while LLMs\ngenerally maintain consistent opinions on many topics, their ideologies differ\nacross models and languages. Notably, ChatGPT exhibits a tendency to change\ntheir opinion to match the questioner's opinion. Both models also exhibited\nproblematic biases, unethical or unfair claims, which might have negative\nsocietal impacts. These results underscore the importance of addressing both\nideological and ethical considerations when evaluating LLMs. The proposed\nframework offers a flexible, quantitative method for assessing LLM behavior,\nproviding valuable insights for the development of more socially aligned AI\nsystems."}
{"id": "2505.13071", "pdf": "https://arxiv.org/pdf/2505.13071", "abs": "https://arxiv.org/abs/2505.13071", "authors": ["Jie Yan", "Xin Liu", "Zhong-Yuan Zhang"], "title": "OmniFC: Rethinking Federated Clustering via Lossless and Secure Distance Reconstruction", "categories": ["cs.LG"], "comment": null, "summary": "Federated clustering (FC) aims to discover global cluster structures across\ndecentralized clients without sharing raw data, making privacy preservation a\nfundamental requirement. There are two critical challenges: (1) privacy leakage\nduring collaboration, and (2) robustness degradation due to aggregation of\nproxy information from non-independent and identically distributed (Non-IID)\nlocal data, leading to inaccurate or inconsistent global clustering. Existing\nsolutions typically rely on model-specific local proxies, which are sensitive\nto data heterogeneity and inherit inductive biases from their centralized\ncounterparts, thus limiting robustness and generality. We propose Omni\nFederated Clustering (OmniFC), a unified and model-agnostic framework.\nLeveraging Lagrange coded computing, our method enables clients to share only\nencoded data, allowing exact reconstruction of the global distance matrix--a\nfundamental representation of sample relationships--without leaking private\ninformation, even under client collusion. This construction is naturally\nresilient to Non-IID data distributions. This approach decouples FC from\nmodel-specific proxies, providing a unified extension mechanism applicable to\ndiverse centralized clustering methods. Theoretical analysis confirms both\nreconstruction fidelity and privacy guarantees, while comprehensive experiments\ndemonstrate OmniFC's superior robustness, effectiveness, and generality across\nvarious benchmarks compared to state-of-the-art methods. Code will be released."}
{"id": "2505.12692", "pdf": "https://arxiv.org/pdf/2505.12692", "abs": "https://arxiv.org/abs/2505.12692", "authors": ["Ziwei Xu", "Udit Sanghi", "Mohan Kankanhalli"], "title": "Bullying the Machine: How Personas Increase LLM Vulnerability", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in interactions where\nthey are prompted to adopt personas. This paper investigates whether such\npersona conditioning affects model safety under bullying, an adversarial\nmanipulation that applies psychological pressures in order to force the victim\nto comply to the attacker. We introduce a simulation framework in which an\nattacker LLM engages a victim LLM using psychologically grounded bullying\ntactics, while the victim adopts personas aligned with the Big Five personality\ntraits. Experiments using multiple open-source LLMs and a wide range of\nadversarial goals reveal that certain persona configurations -- such as\nweakened agreeableness or conscientiousness -- significantly increase victim's\nsusceptibility to unsafe outputs. Bullying tactics involving emotional or\nsarcastic manipulation, such as gaslighting and ridicule, are particularly\neffective. These findings suggest that persona-driven interaction introduces a\nnovel vector for safety risks in LLMs and highlight the need for persona-aware\nsafety evaluation and alignment strategies."}
{"id": "2505.12186", "pdf": "https://arxiv.org/pdf/2505.12186", "abs": "https://arxiv.org/abs/2505.12186", "authors": ["Yuhui Wang", "Rongyi Zhu", "Ting Wang"], "title": "Self-Destructive Language Model", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Harmful fine-tuning attacks pose a major threat to the security of large\nlanguage models (LLMs), allowing adversaries to compromise safety guardrails\nwith minimal harmful data. While existing defenses attempt to reinforce LLM\nalignment, they fail to address models' inherent \"trainability\" on harmful\ndata, leaving them vulnerable to stronger attacks with increased learning rates\nor larger harmful datasets. To overcome this critical limitation, we introduce\nSEAM, a novel alignment-enhancing defense that transforms LLMs into\nself-destructive models with intrinsic resilience to misalignment attempts.\nSpecifically, these models retain their capabilities for legitimate tasks while\nexhibiting substantial performance degradation when fine-tuned on harmful data.\nThe protection is achieved through a novel loss function that couples the\noptimization trajectories of benign and harmful data, enhanced with adversarial\ngradient ascent to amplify the self-destructive effect. To enable practical\ntraining, we develop an efficient Hessian-free gradient estimate with\ntheoretical error bounds. Extensive evaluation across LLMs and datasets\ndemonstrates that SEAM creates a no-win situation for adversaries: the\nself-destructive models achieve state-of-the-art robustness against\nlow-intensity attacks and undergo catastrophic performance collapse under\nhigh-intensity attacks, rendering them effectively unusable. (warning: this\npaper contains potentially harmful content generated by LLMs.)"}
{"id": "2505.13072", "pdf": "https://arxiv.org/pdf/2505.13072", "abs": "https://arxiv.org/abs/2505.13072", "authors": ["Dennis Frauen", "Maresa Schröder", "Konstantin Hess", "Stefan Feuerriegel"], "title": "Orthogonal Survival Learners for Estimating Heterogeneous Treatment Effects from Time-to-Event Data", "categories": ["cs.LG", "stat.ML"], "comment": "Preprint", "summary": "Estimating heterogeneous treatment effects (HTEs) is crucial for personalized\ndecision-making. However, this task is challenging in survival analysis, which\nincludes time-to-event data with censored outcomes (e.g., due to study\ndropout). In this paper, we propose a toolbox of novel orthogonal survival\nlearners to estimate HTEs from time-to-event data under censoring. Our learners\nhave three main advantages: (i) we show that learners from our toolbox are\nguaranteed to be orthogonal and thus come with favorable theoretical\nproperties; (ii) our toolbox allows for incorporating a custom weighting\nfunction, which can lead to robustness against different types of low overlap,\nand (iii) our learners are model-agnostic (i.e., they can be combined with\narbitrary machine learning models). We instantiate the learners from our\ntoolbox using several weighting functions and, as a result, propose various\nneural orthogonal survival learners. Some of these coincide with existing\nsurvival learners (including survival versions of the DR- and R-learner), while\nothers are novel and further robust w.r.t. low overlap regimes specific to the\nsurvival setting (i.e., survival overlap and censoring overlap). We then\nempirically verify the effectiveness of our learners for HTE estimation in\ndifferent low-overlap regimes through numerical experiments. In sum, we provide\npractitioners with a large toolbox of learners that can be used for randomized\nand observational studies with censored time-to-event data."}
{"id": "2505.12763", "pdf": "https://arxiv.org/pdf/2505.12763", "abs": "https://arxiv.org/abs/2505.12763", "authors": ["Sunghwan Kim", "Dongjin Kang", "Taeyoon Kwon", "Hyungjoo Chae", "Dongha Lee", "Jinyoung Yeo"], "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Reward models (RMs) play a crucial role in reinforcement learning from human\nfeedback (RLHF), aligning model behavior with human preferences. However,\nexisting benchmarks for reward models show a weak correlation with the\nperformance of optimized policies, suggesting that they fail to accurately\nassess the true capabilities of RMs. To bridge this gap, we explore several\nevaluation designs through the lens of reward overoptimization\\textemdash a\nphenomenon that captures both how well the reward model aligns with human\npreferences and the dynamics of the learning signal it provides to the policy.\nThe results highlight three key findings on how to construct a reliable\nbenchmark: (i) it is important to minimize differences between chosen and\nrejected responses beyond correctness, (ii) evaluating reward models requires\nmultiple comparisons across a wide range of chosen and rejected responses, and\n(iii) given that reward models encounter responses with diverse\nrepresentations, responses should be sourced from a variety of models. However,\nwe also observe that a extremely high correlation with degree of\noveroptimization leads to comparatively lower correlation with certain\ndownstream performance. Thus, when designing a benchmark, it is desirable to\nuse the degree of overoptimization as a useful tool, rather than the end goal."}
{"id": "2505.12188", "pdf": "https://arxiv.org/pdf/2505.12188", "abs": "https://arxiv.org/abs/2505.12188", "authors": ["Hanyu Wang", "Xinrui Wu", "Zijian Ding", "Su Zheng", "Chengyue Wang", "Tony Nowatzki", "Yizhou Sun", "Jason Cong"], "title": "LLM-DSE: Searching Accelerator Parameters with LLM Agents", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Even though high-level synthesis (HLS) tools mitigate the challenges of\nprogramming domain-specific accelerators (DSAs) by raising the abstraction\nlevel, optimizing hardware directive parameters remains a significant hurdle.\nExisting heuristic and learning-based methods struggle with adaptability and\nsample efficiency.We present LLM-DSE, a multi-agent framework designed\nspecifically for optimizing HLS directives. Combining LLM with design space\nexploration (DSE), our explorer coordinates four agents: Router, Specialists,\nArbitrator, and Critic. These multi-agent components interact with various\ntools to accelerate the optimization process. LLM-DSE leverages essential\ndomain knowledge to identify efficient parameter combinations while maintaining\nadaptability through verbal learning from online interactions. Evaluations on\nthe HLSyn dataset demonstrate that LLM-DSE achieves substantial $2.55\\times$\nperformance gains over state-of-the-art methods, uncovering novel designs while\nreducing runtime. Ablation studies validate the effectiveness and necessity of\nthe proposed agent interactions. Our code is open-sourced here:\nhttps://github.com/Nozidoali/LLM-DSE."}
{"id": "2505.13081", "pdf": "https://arxiv.org/pdf/2505.13081", "abs": "https://arxiv.org/abs/2505.13081", "authors": ["Xiaoyu Yang", "Jie Lu", "En Yu"], "title": "Walking the Tightrope: Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning", "categories": ["cs.LG", "cs.CV"], "comment": "17 pages, 5figures", "summary": "This paper uncovers a critical yet overlooked phenomenon in multi-modal large\nlanguage models (MLLMs): detrimental concept drift within chain-of-thought\n(CoT) reasoning during non-stationary reinforcement fine-tuning (RFT), where\nreasoning token distributions evolve unpredictably, thereby introducing\nsignificant biases in final predictions. To address this, we are pioneers in\nestablishing the theoretical bridge between concept drift theory and RFT\nprocesses by formalizing CoT's autoregressive token streams as non-stationary\ndistributions undergoing arbitrary temporal shifts. Leveraging this framework,\nwe propose a novel counterfact-aware RFT that systematically decouples\nbeneficial distribution adaptation from harmful concept drift through concept\ngraph-empowered LLM experts generating counterfactual reasoning trajectories.\nOur solution, Counterfactual Preference Optimization (CPO), enables stable RFT\nin non-stationary environments, particularly within the medical domain, through\ncustom-tuning of counterfactual-aware preference alignment. Extensive\nexperiments demonstrate our superior performance of robustness, generalization\nand coordination within RFT. Besides, we also contributed a large-scale dataset\nCXR-CounterFact (CCF), comprising 320,416 meticulously curated counterfactual\nreasoning trajectories derived from MIMIC-CXR. Our code and data are public."}
{"id": "2505.12842", "pdf": "https://arxiv.org/pdf/2505.12842", "abs": "https://arxiv.org/abs/2505.12842", "authors": ["Zheng Wu", "Pengzhou Cheng", "Zongru Wu", "Lingzhong Dong", "Zhuosheng Zhang"], "title": "GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Graphical user interface (GUI) agents have recently emerged as an intriguing\nparadigm for human-computer interaction, capable of automatically executing\nuser instructions to operate intelligent terminal devices. However, when\nencountering out-of-distribution (OOD) instructions that violate environmental\nconstraints or exceed the current capabilities of agents, GUI agents may suffer\ntask breakdowns or even pose security threats. Therefore, effective OOD\ndetection for GUI agents is essential. Traditional OOD detection methods\nperform suboptimally in this domain due to the complex embedding space and\nevolving GUI environments. In this work, we observe that the in-distribution\ninput semantic space of GUI agents exhibits a clustering pattern with respect\nto the distance from the centroid. Based on the finding, we propose GEM, a\nnovel method based on fitting a Gaussian mixture model over input embedding\ndistances extracted from the GUI Agent that reflect its capability boundary.\nEvaluated on eight datasets spanning smartphones, computers, and web browsers,\nour method achieves an average accuracy improvement of 23.70\\% over the\nbest-performing baseline. Analysis verifies the generalization ability of our\nmethod through experiments on nine different backbones. The codes are available\nat https://github.com/Wuzheng02/GEM-OODforGUIagents."}
{"id": "2505.12191", "pdf": "https://arxiv.org/pdf/2505.12191", "abs": "https://arxiv.org/abs/2505.12191", "authors": ["Wenquan Lu", "Jiaqi Zhang", "Hugues Van Assel", "Randall Balestriero"], "title": "Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Self-Supervised Learning (SSL) has become a powerful solution to extract rich\nrepresentations from unlabeled data. Yet, SSL research is mostly focused on\nclean, curated and high-quality datasets. As a result, applying SSL on noisy\ndata remains a challenge, despite being crucial to applications such as\nastrophysics, medical imaging, geophysics or finance. In this work, we present\na fully self-supervised framework that enables noise-robust representation\nlearning without requiring a denoiser at inference or downstream fine-tuning.\nOur method first trains an SSL denoiser on noisy data, then uses it to\nconstruct a denoised-to-noisy data curriculum (i.e., training first on\ndenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),\ncombined with a teacher-guided regularization that anchors noisy embeddings to\ntheir denoised counterparts. This process encourages the model to internalize\nnoise robustness. Notably, the denoiser can be discarded after pretraining,\nsimplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise\n($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by\n4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from\nnoise-aware pretraining. The code is available at\nhttps://github.com/wenquanlu/noisy_dinov2."}
{"id": "2505.13087", "pdf": "https://arxiv.org/pdf/2505.13087", "abs": "https://arxiv.org/abs/2505.13087", "authors": ["Adrien Lagesse", "Marc Lelarge"], "title": "Graph Alignment for Benchmarking Graph Neural Networks and Learning Positional Encodings", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose a novel benchmarking methodology for graph neural networks (GNNs)\nbased on the graph alignment problem, a combinatorial optimization task that\ngeneralizes graph isomorphism by aligning two unlabeled graphs to maximize\noverlapping edges. We frame this problem as a self-supervised learning task and\npresent several methods to generate graph alignment datasets using synthetic\nrandom graphs and real-world graph datasets from multiple domains. For a given\ngraph dataset, we generate a family of graph alignment datasets with increasing\ndifficulty, allowing us to rank the performance of various architectures. Our\nexperiments indicate that anisotropic graph neural networks outperform standard\nconvolutional architectures. To further demonstrate the utility of the graph\nalignment task, we show its effectiveness for unsupervised GNN pre-training,\nwhere the learned node embeddings outperform other positional encodings on\nthree molecular regression tasks and achieve state-of-the-art results on the\nPCQM4Mv2 dataset with significantly fewer parameters. To support\nreproducibility and further research, we provide an open-source Python package\nto generate graph alignment datasets and benchmark new GNN architectures."}
{"id": "2505.12871", "pdf": "https://arxiv.org/pdf/2505.12871", "abs": "https://arxiv.org/abs/2505.12871", "authors": ["Zi Liang", "Haibo Hu", "Qingqing Ye", "Yaxin Xiao", "Ronghua Li"], "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "To appear at ICML 25", "summary": "Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings."}
{"id": "2505.12199", "pdf": "https://arxiv.org/pdf/2505.12199", "abs": "https://arxiv.org/abs/2505.12199", "authors": ["Kui Jiang", "Jing Cao", "Zhaocheng Yu", "Junjun Jiang", "Jingchun Zhou"], "title": "Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Monocular depth estimation is critical for applications such as autonomous\ndriving and scene reconstruction. While existing methods perform well under\nnormal scenarios, their performance declines in adverse weather, due to\nchallenging domain shifts and difficulties in extracting scene information. To\naddress this issue, we present a robust monocular depth estimation method\ncalled \\textbf{ACDepth} from the perspective of high-quality training data\ngeneration and domain adaptation. Specifically, we introduce a one-step\ndiffusion model for generating samples that simulate adverse weather\nconditions, constructing a multi-tuple degradation dataset during training. To\nensure the quality of the generated degradation samples, we employ LoRA\nadapters to fine-tune the generation weights of diffusion model. Additionally,\nwe integrate circular consistency loss and adversarial training to guarantee\nthe fidelity and naturalness of the scene contents. Furthermore, we elaborate\non a multi-granularity knowledge distillation strategy (MKD) that encourages\nthe student network to absorb knowledge from both the teacher model and\npretrained Depth Anything V2. This strategy guides the student model in\nlearning degradation-agnostic scene information from various degradation\ninputs. In particular, we introduce an ordinal guidance distillation mechanism\n(OGD) that encourages the network to focus on uncertain regions through\ndifferential ranking, leading to a more precise depth estimation. Experimental\nresults demonstrate that our ACDepth surpasses md4all-DD by 2.50\\% for night\nscene and 2.61\\% for rainy scene on the nuScenes dataset in terms of the absRel\nmetric."}
{"id": "2505.13092", "pdf": "https://arxiv.org/pdf/2505.13092", "abs": "https://arxiv.org/abs/2505.13092", "authors": ["Dennis Frauen", "Valentyn Melnychuk", "Jonas Schweisthal", "Stefan Feuerriegel"], "title": "Treatment Effect Estimation for Optimal Decision-Making", "categories": ["cs.LG"], "comment": "Preprint", "summary": "Decision-making across various fields, such as medicine, heavily relies on\nconditional average treatment effects (CATEs). Practitioners commonly make\ndecisions by checking whether the estimated CATE is positive, even though the\ndecision-making performance of modern CATE estimators is poorly understood from\na theoretical perspective. In this paper, we study optimal decision-making\nbased on two-stage CATE estimators (e.g., DR-learner), which are considered\nstate-of-the-art and widely used in practice. We prove that, while such\nestimators may be optimal for estimating CATE, they can be suboptimal when used\nfor decision-making. Intuitively, this occurs because such estimators\nprioritize CATE accuracy in regions far away from the decision boundary, which\nis ultimately irrelevant to decision-making. As a remedy, we propose a novel\ntwo-stage learning objective that retargets the CATE to balance CATE estimation\nerror and decision performance. We then propose a neural method that optimizes\nan adaptively-smoothed approximation of our learning objective. Finally, we\nconfirm the effectiveness of our method both empirically and theoretically. In\nsum, our work is the first to show how two-stage CATE estimators can be adapted\nfor optimal decision-making."}
{"id": "2505.12886", "pdf": "https://arxiv.org/pdf/2505.12886", "abs": "https://arxiv.org/abs/2505.12886", "authors": ["Zhongxiang Sun", "Qipeng Wang", "Haoyu Wang", "Xiao Zhang", "Jun Xu"], "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "25 pages", "summary": "Large Reasoning Models (LRMs) have shown impressive capabilities in\nmulti-step reasoning tasks. However, alongside these successes, a more\ndeceptive form of model error has emerged--Reasoning Hallucination--where\nlogically coherent but factually incorrect reasoning traces lead to persuasive\nyet faulty conclusions. Unlike traditional hallucinations, these errors are\nembedded within structured reasoning, making them more difficult to detect and\npotentially more harmful. In this work, we investigate reasoning hallucinations\nfrom a mechanistic perspective. We propose the Reasoning Score, which\nquantifies the depth of reasoning by measuring the divergence between logits\nobtained from projecting late layers of LRMs to the vocabulary space,\neffectively distinguishing shallow pattern-matching from genuine deep\nreasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA\ndataset and identify two key reasoning hallucination patterns: early-stage\nfluctuation in reasoning depth and incorrect backtracking to flawed prior\nsteps. These insights motivate our Reasoning Hallucination Detection (RHD)\nframework, which achieves state-of-the-art performance across multiple domains.\nTo mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced\nreinforcement learning algorithm that incorporates step-level deep reasoning\nrewards via potential-based shaping. Our theoretical analysis establishes\nstronger generalization guarantees, and experiments demonstrate improved\nreasoning quality and reduced hallucination rates."}
{"id": "2505.12207", "pdf": "https://arxiv.org/pdf/2505.12207", "abs": "https://arxiv.org/abs/2505.12207", "authors": ["Qingmei Li", "Yang Zhang", "Zurong Mai", "Yuhang Chen", "Shuohong Lou", "Henglian Huang", "Jiarui Zhang", "Zhiwei Zhang", "Yibin Wen", "Weijia Li", "Haohuan Fu", "Jianxi Huang", "Juepeng Zheng"], "title": "Can Large Multimodal Models Understand Agricultural Scenes? Benchmarking with AgroMind", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Multimodal Models (LMMs) has demonstrated capabilities across various\ndomains, but comprehensive benchmarks for agricultural remote sensing (RS)\nremain scarce. Existing benchmarks designed for agricultural RS scenarios\nexhibit notable limitations, primarily in terms of insufficient scene diversity\nin the dataset and oversimplified task design. To bridge this gap, we introduce\nAgroMind, a comprehensive agricultural remote sensing benchmark covering four\ntask dimensions: spatial perception, object understanding, scene understanding,\nand scene reasoning, with a total of 13 task types, ranging from crop\nidentification and health monitoring to environmental analysis. We curate a\nhigh-quality evaluation set by integrating eight public datasets and one\nprivate farmland plot dataset, containing 25,026 QA pairs and 15,556 images.\nThe pipeline begins with multi-source data preprocessing, including collection,\nformat standardization, and annotation refinement. We then generate a diverse\nset of agriculturally relevant questions through the systematic definition of\ntasks. Finally, we employ LMMs for inference, generating responses, and\nperforming detailed examinations. We evaluated 18 open-source LMMs and 3\nclosed-source models on AgroMind. Experiments reveal significant performance\ngaps, particularly in spatial reasoning and fine-grained recognition, it is\nnotable that human performance lags behind several leading LMMs. By\nestablishing a standardized evaluation framework for agricultural RS, AgroMind\nreveals the limitations of LMMs in domain knowledge and highlights critical\nchallenges for future work. Data and code can be accessed at\nhttps://rssysu.github.io/AgroMind/."}
{"id": "2505.13100", "pdf": "https://arxiv.org/pdf/2505.13100", "abs": "https://arxiv.org/abs/2505.13100", "authors": ["Christodoulos Kechris", "Jonathan Dan", "David Atienza"], "title": "Time series saliency maps: explaining models across multiple domains", "categories": ["cs.LG"], "comment": null, "summary": "Traditional saliency map methods, popularized in computer vision, highlight\nindividual points (pixels) of the input that contribute the most to the model's\noutput. However, in time-series they offer limited insights as semantically\nmeaningful features are often found in other domains. We introduce Cross-domain\nIntegrated Gradients, a generalization of Integrated Gradients. Our method\nenables feature attributions on any domain that can be formulated as an\ninvertible, differentiable transformation of the time domain. Crucially, our\nderivation extends the original Integrated Gradients into the complex domain,\nenabling frequency-based attributions. We provide the necessary theoretical\nguarantees, namely, path independence and completeness. Our approach reveals\ninterpretable, problem-specific attributions that time-domain methods cannot\ncapture, on three real-world tasks: wearable sensor heart rate extraction,\nelectroencephalography-based seizure detection, and zero-shot time-series\nforecasting. We release an open-source Tensorflow/PyTorch library to enable\nplug-and-play cross-domain explainability for time-series models. These results\ndemonstrate the ability of cross-domain integrated gradients to provide\nsemantically meaningful insights in time-series models that are impossible with\ntraditional time-domain saliency."}
{"id": "2505.12891", "pdf": "https://arxiv.org/pdf/2505.12891", "abs": "https://arxiv.org/abs/2505.12891", "authors": ["Shaohang Wei", "Wei Li", "Feifan Song", "Wen Luo", "Tianyi Zhuang", "Haochen Tan", "Zhijiang Guo", "Houfeng Wang"], "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "categories": ["cs.AI", "cs.CL"], "comment": "First version. There are still some examples to be added into the\n  appendix", "summary": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME ."}
{"id": "2505.12211", "pdf": "https://arxiv.org/pdf/2505.12211", "abs": "https://arxiv.org/abs/2505.12211", "authors": ["Wenhui Liu", "Zhijian Wu", "Jingchao Wang", "Dingjiang Huang", "Shuigeng Zhou"], "title": "Imagination-Limited Q-Learning for Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Offline reinforcement learning seeks to derive improved policies entirely\nfrom historical data but often struggles with over-optimistic value estimates\nfor out-of-distribution (OOD) actions. This issue is typically mitigated via\npolicy constraint or conservative value regularization methods. However, these\napproaches may impose overly constraints or biased value estimates, potentially\nlimiting performance improvements. To balance exploitation and restriction, we\npropose an Imagination-Limited Q-learning (ILQ) method, which aims to maintain\nthe optimism that OOD actions deserve within appropriate limits. Specifically,\nwe utilize the dynamics model to imagine OOD action-values, and then clip the\nimagined values with the maximum behavior values. Such design maintains\nreasonable evaluation of OOD actions to the furthest extent, while avoiding its\nover-optimism. Theoretically, we prove the convergence of the proposed ILQ\nunder tabular Markov decision processes. Particularly, we demonstrate that the\nerror bound between estimated values and optimality values of OOD state-actions\npossesses the same magnitude as that of in-distribution ones, thereby\nindicating that the bias in value estimates is effectively mitigated.\nEmpirically, our method achieves state-of-the-art performance on a wide range\nof tasks in the D4RL benchmark."}
{"id": "2505.13102", "pdf": "https://arxiv.org/pdf/2505.13102", "abs": "https://arxiv.org/abs/2505.13102", "authors": ["Ji Qi", "Tam Thuc Do", "Mingxiao Liu", "Zhuoshi Pan", "Yuzhe Li", "Gene Cheung", "H. Vicky Zhao"], "title": "Lightweight Transformer via Unrolling of Mixed Graph Algorithms for Traffic Forecast", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": "19 pages, 5 figures, 8 tables", "summary": "To forecast traffic with both spatial and temporal dimensions, we unroll a\nmixed-graph-based optimization algorithm into a lightweight and interpretable\ntransformer-like neural net. Specifically, we construct two graphs: an\nundirected graph $\\mathcal{G}^u$ capturing spatial correlations across\ngeography, and a directed graph $\\mathcal{G}^d$ capturing sequential\nrelationships over time. We formulate a prediction problem for the future\nsamples of signal $\\mathbf{x}$, assuming it is \"smooth\" with respect to both\n$\\mathcal{G}^u$ and $\\mathcal{G}^d$, where we design new $\\ell_2$ and\n$\\ell_1$-norm variational terms to quantify and promote signal smoothness\n(low-frequency reconstruction) on a directed graph. We construct an iterative\nalgorithm based on alternating direction method of multipliers (ADMM), and\nunroll it into a feed-forward network for data-driven parameter learning. We\ninsert graph learning modules for $\\mathcal{G}^u$ and $\\mathcal{G}^d$, which\nare akin to the self-attention mechanism in classical transformers. Experiments\nshow that our unrolled networks achieve competitive traffic forecast\nperformance as state-of-the-art prediction schemes, while reducing parameter\ncounts drastically. Our code is available in\nhttps://github.com/SingularityUndefined/Unrolling-GSP-STForecast."}
{"id": "2505.12900", "pdf": "https://arxiv.org/pdf/2505.12900", "abs": "https://arxiv.org/abs/2505.12900", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Jianyuan Liang", "Haoyue Jiao", "Yaxian Qing", "Xiaopu Zhang", "Xu Li", "Zhipeng Gui", "Xuefeng Guan", "Longgang Xiang"], "title": "AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models", "categories": ["cs.SE", "cs.AI", "cs.CG", "cs.CL", "cs.DB"], "comment": null, "summary": "Geospatial code generation is emerging as a key direction in the integration\nof artificial intelligence and geoscientific analysis. However, there remains a\nlack of standardized tools for automatic evaluation in this domain. To address\nthis gap, we propose AutoGEEval, the first multimodal, unit-level automated\nevaluation framework for geospatial code generation tasks on the Google Earth\nEngine (GEE) platform powered by large language models (LLMs). Built upon the\nGEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)\ncomprising 1325 test cases that span 26 GEE data types. The framework\nintegrates both question generation and answer verification components to\nenable an end-to-end automated evaluation pipeline-from function invocation to\nexecution validation. AutoGEEval supports multidimensional quantitative\nanalysis of model outputs in terms of accuracy, resource consumption, execution\nefficiency, and error types. We evaluate 18 state-of-the-art LLMs-including\ngeneral-purpose, reasoning-augmented, code-centric, and geoscience-specialized\nmodels-revealing their performance characteristics and potential optimization\npathways in GEE code generation. This work provides a unified protocol and\nfoundational resource for the development and assessment of geospatial code\ngeneration models, advancing the frontier of automated natural language to\ndomain-specific code translation."}
{"id": "2505.12224", "pdf": "https://arxiv.org/pdf/2505.12224", "abs": "https://arxiv.org/abs/2505.12224", "authors": ["Weifeng Lu", "Minghao Ye", "Zewei Ye", "Ruihan Tao", "Shuo Yang", "Bo Zhao"], "title": "RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language-Action (VLA) models have recently advanced robotic\nmanipulation by translating natural-language instructions and image information\ninto sequential control actions. However, these models often underperform in\nopen-world scenarios, as they are predominantly trained on successful expert\ndemonstrations and exhibit a limited capacity for failure recovery. In this\nwork, we present a Robotic Failure Analysis and Correction (RoboFAC) framework\nto address this issue. Firstly, we construct RoboFAC dataset comprising 9,440\nerroneous manipulation trajectories and 78,623 QA pairs across 16 diverse tasks\nand 53 scenes in both simulation and real-world environments. Leveraging our\ndataset, we develop RoboFAC model, which is capable of Task Understanding,\nFailure Analysis and Failure Correction. Experimental results demonstrate that\nthe RoboFAC model outperforms GPT-4o by 34.1% on our evaluation benchmark.\nFurthermore, we integrate the RoboFAC model into a real-world VLA control\npipeline as an external supervision providing correction instructions, yielding\na 29.1% relative improvement on average on four real-world tasks. The results\nshow that our RoboFAC framework effectively handles robotic failures and\nassists the VLA model in recovering from failures."}
{"id": "2505.13109", "pdf": "https://arxiv.org/pdf/2505.13109", "abs": "https://arxiv.org/abs/2505.13109", "authors": ["Guangda Liu", "Chengwei Li", "Zhenyu Ning", "Jing Lin", "Yiwu Yao", "Danning Ke", "Minyi Guo", "Jieru Zhao"], "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."}
{"id": "2505.12938", "pdf": "https://arxiv.org/pdf/2505.12938", "abs": "https://arxiv.org/abs/2505.12938", "authors": ["Uri Dalal", "Meirav Segal", "Zvika Ben-Haim", "Dan Lahav", "Omer Nevo"], "title": "Leveraging LLM Inconsistency to Boost Pass@k Performance", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations."}
{"id": "2505.12225", "pdf": "https://arxiv.org/pdf/2505.12225", "abs": "https://arxiv.org/abs/2505.12225", "authors": ["Jizhou Guo", "Zhaomin Wu", "Philip S. Yu"], "title": "Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "High-quality reward models are crucial for unlocking the reasoning potential\nof large language models (LLMs), with best-of-N voting demonstrating\nsignificant performance gains. However, current reward models, which typically\noperate on the textual output of LLMs, are computationally expensive and\nparameter-heavy, limiting their real-world applications. We introduce the\nEfficient Linear Hidden State Reward (ELHSR) model - a novel, highly\nparameter-efficient approach that leverages the rich information embedded in\nLLM hidden states to address these issues. ELHSR systematically outperform\nbaselines with less than 0.005% of the parameters of baselines, requiring only\na few samples for training. ELHSR also achieves orders-of-magnitude efficiency\nimprovement with significantly less time and fewer FLOPs per sample than\nbaseline reward models. Moreover, ELHSR exhibits robust performance even when\ntrained only on logits, extending its applicability to some closed-source LLMs.\nIn addition, ELHSR can also be combined with traditional reward models to\nachieve additional performance gains."}
{"id": "2505.13111", "pdf": "https://arxiv.org/pdf/2505.13111", "abs": "https://arxiv.org/abs/2505.13111", "authors": ["Sungmin Cha", "Kyunghyun Cho"], "title": "Why Knowledge Distillation Works in Generative Models: A Minimal Working Explanation", "categories": ["cs.LG"], "comment": "Preprint", "summary": "Knowledge distillation (KD) is a core component in the training and\ndeployment of modern generative models, particularly large language models\n(LLMs). While its empirical benefits are well documented--enabling smaller\nstudent models to emulate the performance of much larger teachers--the\nunderlying mechanisms by which KD improves generative quality remain poorly\nunderstood. In this work, we present a minimal working explanation of KD in\ngenerative modeling. Using a controlled simulation with mixtures of Gaussians,\nwe demonstrate that distillation induces a trade-off between precision and\nrecall in the student model. As the teacher distribution becomes more\nselective, the student concentrates more probability mass on high-likelihood\nregions at the expense of coverage--a behavior modulated by a single\nentropy-controlling parameter. We then validate this effect in a large-scale\nlanguage modeling setup using the SmolLM2 family of models. Empirical results\nreveal the same precision-recall dynamics observed in simulation, where\nprecision corresponds to sample quality and recall to distributional coverage.\nThis precision-recall trade-off proves especially beneficial in scenarios where\nsample quality outweighs diversity, such as instruction tuning or downstream\ngeneration. Our analysis provides a simple and general explanation for the\neffectiveness of KD in generative modeling."}
{"id": "2505.12992", "pdf": "https://arxiv.org/pdf/2505.12992", "abs": "https://arxiv.org/abs/2505.12992", "authors": ["Baohao Liao", "Hanze Dong", "Yuhui Xu", "Doyen Sahoo", "Christof Monz", "Junnan Li", "Caiming Xiong"], "title": "Fractured Chain-of-Thought Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning."}
{"id": "2505.12226", "pdf": "https://arxiv.org/pdf/2505.12226", "abs": "https://arxiv.org/abs/2505.12226", "authors": ["Dong Yang", "Yiyi Cai", "Yuki Saito", "Lixu Wang", "Hiroshi Saruwatari"], "title": "Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": null, "summary": "We propose a shallow flow matching (SFM) mechanism to enhance flow matching\n(FM)-based text-to-speech (TTS) models within a coarse-to-fine generation\nparadigm. SFM constructs intermediate states along the FM paths using coarse\noutput representations. During training, we introduce an orthogonal projection\nmethod to adaptively determine the temporal position of these states, and apply\na principled construction strategy based on a single-segment piecewise flow.\nThe SFM inference starts from the intermediate state rather than pure noise and\nfocuses computation on the latter stages of the FM paths. We integrate SFM into\nmultiple TTS models with a lightweight SFM head. Experiments show that SFM\nconsistently improves the naturalness of synthesized speech in both objective\nand subjective evaluations, while significantly reducing inference when using\nadaptive-step ODE solvers. Demo and codes are available at\nhttps://ydqmkkx.github.io/SFMDemo/."}
{"id": "2505.13116", "pdf": "https://arxiv.org/pdf/2505.13116", "abs": "https://arxiv.org/abs/2505.13116", "authors": ["Kathrin Lammers", "Valerie Vaquet", "Barbara Hammer"], "title": "Continuous Fair SMOTE -- Fairness-Aware Stream Learning from Imbalanced Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As machine learning is increasingly applied in an online fashion to deal with\nevolving data streams, the fairness of these algorithms is a matter of growing\nethical and legal concern. In many use cases, class imbalance in the data also\nneeds to be dealt with to ensure predictive performance. Current fairness-aware\nstream learners typically attempt to solve these issues through in- or\npost-processing by focusing on optimizing one specific discrimination metric,\naddressing class imbalance in a separate processing step. While C-SMOTE is a\nhighly effective model-agnostic pre-processing approach to mitigate class\nimbalance, as a side effect of this method, algorithmic bias is often\nintroduced.\n  Therefore, we propose CFSMOTE - a fairness-aware, continuous SMOTE variant -\nas a pre-processing approach to simultaneously address the class imbalance and\nfairness concerns by employing situation testing and balancing\nfairness-relevant groups during oversampling. Unlike other fairness-aware\nstream learners, CFSMOTE is not optimizing for only one specific fairness\nmetric, therefore avoiding potentially problematic trade-offs. Our experiments\nshow significant improvement on several common group fairness metrics in\ncomparison to vanilla C-SMOTE while maintaining competitive performance, also\nin comparison to other fairness-aware algorithms."}
{"id": "2505.13028", "pdf": "https://arxiv.org/pdf/2505.13028", "abs": "https://arxiv.org/abs/2505.13028", "authors": ["Sayon Palit", "Daniel Woods"], "title": "Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset", "categories": ["cs.CR", "cs.AI", "cs.CL", "F.2.2, I.2.7; F.2.2, I.2.7; F.2.2, I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into critical\nsystems in industries like healthcare and finance. Users can often submit\nqueries to LLM-enabled chatbots, some of which can enrich responses with\ninformation retrieved from internal databases storing sensitive data. This\ngives rise to a range of attacks in which a user submits a malicious query and\nthe LLM-system outputs a response that creates harm to the owner, such as\nleaking internal data or creating legal liability by harming a third-party.\nWhile security tools are being developed to counter these threats, there is\nlittle formal evaluation of their effectiveness and usability. This study\naddresses this gap by conducting a thorough comparative analysis of LLM\nsecurity tools. We identified 13 solutions (9 closed-source, 4 open-source),\nbut only 7 were evaluated due to a lack of participation by proprietary model\nowners.To evaluate, we built a benchmark dataset of malicious prompts, and\nevaluate these tools performance against a baseline LLM model\n(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many\nfalse positives to be used for this task. Lakera Guard and ProtectAI LLM Guard\nemerged as the best overall tools showcasing the tradeoff between usability and\nperformance. The study concluded with recommendations for greater transparency\namong closed source providers, improved context-aware detections, enhanced\nopen-source engagement, increased user awareness, and the adoption of more\nrepresentative performance metrics."}
{"id": "2505.12236", "pdf": "https://arxiv.org/pdf/2505.12236", "abs": "https://arxiv.org/abs/2505.12236", "authors": ["Quanjiang Guo", "Jinchuan Zhang", "Sijie Wang", "Ling Tian", "Zhao Kang", "Bin Yan", "Weidong Xiao"], "title": "Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 6 figures, Appear on IJCAI 2025", "summary": "Few-Shot Relation Extraction (FSRE) remains a challenging task due to the\nscarcity of annotated data and the limited generalization capabilities of\nexisting models. Although large language models (LLMs) have demonstrated\npotential in FSRE through in-context learning (ICL), their general-purpose\ntraining objectives often result in suboptimal performance for task-specific\nrelation extraction. To overcome these challenges, we propose TKRE (Two-Stage\nKnowledge-Guided Pre-training for Relation Extraction), a novel framework that\nsynergistically integrates LLMs with traditional relation extraction models,\nbridging generative and discriminative learning paradigms. TKRE introduces two\nkey innovations: (1) leveraging LLMs to generate explanation-driven knowledge\nand schema-constrained synthetic data, addressing the issue of data scarcity;\nand (2) a two-stage pre-training strategy combining Masked Span Language\nModeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational\nreasoning and generalization. Together, these components enable TKRE to\neffectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets\ndemonstrate the efficacy of TKRE, achieving new state-of-the-art performance in\nFSRE and underscoring its potential for broader application in low-resource\nscenarios. \\footnote{The code and data are released on\nhttps://github.com/UESTC-GQJ/TKRE."}
{"id": "2505.13122", "pdf": "https://arxiv.org/pdf/2505.13122", "abs": "https://arxiv.org/abs/2505.13122", "authors": ["François Bachoc", "Jérôme Bolte", "Ryan Boustany", "Jean-Michel Loubes"], "title": "When majority rules, minority loses: bias amplification of gradient descent", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "Despite growing empirical evidence of bias amplification in machine learning,\nits theoretical foundations remain poorly understood. We develop a formal\nframework for majority-minority learning tasks, showing how standard training\ncan favor majority groups and produce stereotypical predictors that neglect\nminority-specific features. Assuming population and variance imbalance, our\nanalysis reveals three key findings: (i) the close proximity between\n``full-data'' and stereotypical predictors, (ii) the dominance of a region\nwhere training the entire model tends to merely learn the majority traits, and\n(iii) a lower bound on the additional training required. Our results are\nillustrated through experiments in deep learning for tabular and image\nclassification tasks."}
{"id": "2505.13032", "pdf": "https://arxiv.org/pdf/2505.13032", "abs": "https://arxiv.org/abs/2505.13032", "authors": ["Ziyang Ma", "Yinghao Ma", "Yanqiao Zhu", "Chen Yang", "Yi-Wen Chao", "Ruiyang Xu", "Wenxi Chen", "Yuanzhe Chen", "Zhuo Chen", "Jian Cong", "Kai Li", "Keliang Li", "Siyou Li", "Xinfeng Li", "Xiquan Li", "Zheng Lian", "Yuzhe Liang", "Minghao Liu", "Zhikang Niu", "Tianrui Wang", "Yuping Wang", "Yuxuan Wang", "Yihao Wu", "Guanrou Yang", "Jianwei Yu", "Ruibin Yuan", "Zhisheng Zheng", "Ziya Zhou", "Haina Zhu", "Wei Xue", "Emmanouil Benetos", "Kai Yu", "Eng-Siong Chng", "Xie Chen"], "title": "MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "Open-source at https://github.com/ddlBoJack/MMAR", "summary": "We introduce MMAR, a new benchmark designed to evaluate the deep reasoning\ncapabilities of Audio-Language Models (ALMs) across massive multi-disciplinary\ntasks. MMAR comprises 1,000 meticulously curated audio-question-answer\ntriplets, collected from real-world internet videos and refined through\niterative error corrections and quality checks to ensure high quality. Unlike\nexisting benchmarks that are limited to specific domains of sound, music, or\nspeech, MMAR extends them to a broad spectrum of real-world audio scenarios,\nincluding mixed-modality combinations of sound, music, and speech. Each\nquestion in MMAR is hierarchically categorized across four reasoning layers:\nSignal, Perception, Semantic, and Cultural, with additional sub-categories\nwithin each layer to reflect task diversity and complexity. To further foster\nresearch in this area, we annotate every question with a Chain-of-Thought (CoT)\nrationale to promote future advancements in audio reasoning. Each item in the\nbenchmark demands multi-step deep reasoning beyond surface-level understanding.\nMoreover, a part of the questions requires graduate-level perceptual and\ndomain-specific knowledge, elevating the benchmark's difficulty and depth. We\nevaluate MMAR using a broad set of models, including Large Audio-Language\nModels (LALMs), Large Audio Reasoning Models (LARMs), Omni Language Models\n(OLMs), Large Language Models (LLMs), and Large Reasoning Models (LRMs), with\naudio caption inputs. The performance of these models on MMAR highlights the\nbenchmark's challenging nature, and our analysis further reveals critical\nlimitations of understanding and reasoning capabilities among current models.\nWe hope MMAR will serve as a catalyst for future advances in this important but\nlittle-explored area."}
{"id": "2505.12238", "pdf": "https://arxiv.org/pdf/2505.12238", "abs": "https://arxiv.org/abs/2505.12238", "authors": ["Sriram Selvam", "Anneswa Ghosh"], "title": "PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The memorization of sensitive and personally identifiable information (PII)\nby large language models (LLMs) poses growing privacy risks as models scale and\nare increasingly deployed in real-world applications. Existing efforts to study\nsensitive and PII data memorization and develop mitigation strategies are\nhampered by the absence of comprehensive, realistic, and ethically sourced\ndatasets reflecting the diversity of sensitive information found on the web. We\nintroduce PANORAMA - Profile-based Assemblage for Naturalistic Online\nRepresentation and Attribute Memorization Analysis, a large-scale synthetic\ncorpus of 384,789 samples derived from 9,674 synthetic profiles designed to\nclosely emulate the distribution, variety, and context of PII and sensitive\ndata as it naturally occurs in online environments. Our data generation\npipeline begins with the construction of internally consistent, multi-attribute\nhuman profiles using constrained selection to reflect real-world demographics\nsuch as education, health attributes, financial status, etc. Using a\ncombination of zero-shot prompting and OpenAI o3-mini, we generate diverse\ncontent types - including wiki-style articles, social media posts, forum\ndiscussions, online reviews, comments, and marketplace listings - each\nembedding realistic, contextually appropriate PII and other sensitive\ninformation. We validate the utility of PANORAMA by fine-tuning the Mistral-7B\nmodel on 1x, 5x, 10x, and 25x data replication rates with a subset of data and\nmeasure PII memorization rates - revealing not only consistent increases with\nrepetition but also variation across content types, highlighting PANORAMA's\nability to model how memorization risks differ by context. Our dataset and code\nare publicly available, providing a much-needed resource for privacy risk\nassessment, model auditing, and the development of privacy-preserving LLMs."}
{"id": "2505.13124", "pdf": "https://arxiv.org/pdf/2505.13124", "abs": "https://arxiv.org/abs/2505.13124", "authors": ["Francesco Innocenti", "El Mehdi Achour", "Christopher L. Buckley"], "title": "$μ$PC: Scaling Predictive Coding to 100+ Layer Networks", "categories": ["cs.LG", "cs.AI", "cs.NE", "I.2.6"], "comment": "34 pages, 41 figures", "summary": "The biological implausibility of backpropagation (BP) has motivated many\nalternative, brain-inspired algorithms that attempt to rely only on local\ninformation, such as predictive coding (PC) and equilibrium propagation.\nHowever, these algorithms have notoriously struggled to train very deep\nnetworks, preventing them from competing with BP in large-scale settings.\nIndeed, scaling PC networks (PCNs) has recently been posed as a challenge for\nthe community (Pinchetti et al., 2024). Here, we show that 100+ layer PCNs can\nbe trained reliably using a Depth-$\\mu$P parameterisation (Yang et al., 2023;\nBordelon et al., 2023) which we call \"$\\mu$PC\". Through an extensive analysis\nof the scaling behaviour of PCNs, we reveal several pathologies that make\nstandard PCNs difficult to train at large depths. We then show that, despite\naddressing only some of these instabilities, $\\mu$PC allows stable training of\nvery deep (up to 128-layer) residual networks on simple classification tasks\nwith competitive performance and little tuning compared to current benchmarks.\nMoreover, $\\mu$PC enables zero-shot transfer of both weight and activity\nlearning rates across widths and depths. Our results have implications for\nother local algorithms and could be extended to convolutional and transformer\narchitectures. Code for $\\mu$PC is made available as part of a JAX library for\nPCNs at https://github.com/thebuckleylab/jpc (Innocenti et al., 2024)."}
{"id": "2505.13098", "pdf": "https://arxiv.org/pdf/2505.13098", "abs": "https://arxiv.org/abs/2505.13098", "authors": ["Lars-Peter Meyer", "Johannes Frey", "Desiree Heim", "Felix Brei", "Claus Stadler", "Kurt Junghanns", "Michael Martin"], "title": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs", "categories": ["cs.AI", "cs.CL", "cs.DB"], "comment": "Peer reviewed publication at ESWC 2025 Resources Track", "summary": "Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks."}
{"id": "2505.12239", "pdf": "https://arxiv.org/pdf/2505.12239", "abs": "https://arxiv.org/abs/2505.12239", "authors": ["Jianheng Tang", "Huiping Zhuang", "Di Fang", "Jiaxu Li", "Feijiang Han", "Yajiang Huang", "Kejia Fan", "Leye Wang", "Zhanxing Zhu", "Shanghang Zhang", "Houbing Herbert Song", "Yunhuai Liu"], "title": "ACU: Analytic Continual Unlearning for Efficient and Exact Forgetting with Privacy Preservation", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "21 pages, 4 figures, 2 tables", "summary": "The development of artificial intelligence demands that models incrementally\nupdate knowledge by Continual Learning (CL) to adapt to open-world\nenvironments. To meet privacy and security requirements, Continual Unlearning\n(CU) emerges as an important problem, aiming to sequentially forget particular\nknowledge acquired during the CL phase. However, existing unlearning methods\nprimarily focus on single-shot joint forgetting and face significant\nlimitations when applied to CU. First, most existing methods require access to\nthe retained dataset for re-training or fine-tuning, violating the inherent\nconstraint in CL that historical data cannot be revisited. Second, these\nmethods often suffer from a poor trade-off between system efficiency and model\nfidelity, making them vulnerable to being overwhelmed or degraded by\nadversaries through deliberately frequent requests. In this paper, we identify\nthat the limitations of existing unlearning methods stem fundamentally from\ntheir reliance on gradient-based updates. To bridge the research gap at its\nroot, we propose a novel gradient-free method for CU, named Analytic Continual\nUnlearning (ACU), for efficient and exact forgetting with historical data\nprivacy preservation. In response to each unlearning request, our ACU\nrecursively derives an analytical (i.e., closed-form) solution in an\ninterpretable manner using the least squares method. Theoretical and\nexperimental evaluations validate the superiority of our ACU on unlearning\neffectiveness, model fidelity, and system efficiency."}
{"id": "2505.13138", "pdf": "https://arxiv.org/pdf/2505.13138", "abs": "https://arxiv.org/abs/2505.13138", "authors": ["Emile van Krieken", "Pasquale Minervini", "Edoardo Ponti", "Antonio Vergari"], "title": "Neurosymbolic Diffusion Models", "categories": ["cs.LG"], "comment": null, "summary": "Neurosymbolic (NeSy) predictors combine neural perception with symbolic\nreasoning to solve tasks like visual reasoning. However, standard NeSy\npredictors assume conditional independence between the symbols they extract,\nthus limiting their ability to model interactions and uncertainty - often\nleading to overconfident predictions and poor out-of-distribution\ngeneralisation. To overcome the limitations of the independence assumption, we\nintroduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy\npredictors that use discrete diffusion to model dependencies between symbols.\nOur approach reuses the independence assumption from NeSy predictors at each\nstep of the diffusion process, enabling scalable learning while capturing\nsymbol dependencies and uncertainty quantification. Across both synthetic and\nreal-world benchmarks - including high-dimensional visual path planning and\nrule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among\nNeSy predictors and demonstrate strong calibration."}
{"id": "2505.13109", "pdf": "https://arxiv.org/pdf/2505.13109", "abs": "https://arxiv.org/abs/2505.13109", "authors": ["Guangda Liu", "Chengwei Li", "Zhenyu Ning", "Jing Lin", "Yiwu Yao", "Danning Ke", "Minyi Guo", "Jieru Zhao"], "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."}
{"id": "2505.12245", "pdf": "https://arxiv.org/pdf/2505.12245", "abs": "https://arxiv.org/abs/2505.12245", "authors": ["Jianheng Tang", "Huiping Zhuang", "Jingyu He", "Run He", "Jingchao Wang", "Kejia Fan", "Anfeng Liu", "Tian Wang", "Leye Wang", "Zhanxing Zhu", "Shanghang Zhang", "Houbing Herbert Song", "Yunhuai Liu"], "title": "AFCL: Analytic Federated Continual Learning for Spatio-Temporal Invariance of Non-IID Data", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages, 5 figures, 5 tables", "summary": "Federated Continual Learning (FCL) enables distributed clients to\ncollaboratively train a global model from online task streams in dynamic\nreal-world scenarios. However, existing FCL methods face challenges of both\nspatial data heterogeneity among distributed clients and temporal data\nheterogeneity across online tasks. Such data heterogeneity significantly\ndegrades the model performance with severe spatial-temporal catastrophic\nforgetting of local and past knowledge. In this paper, we identify that the\nroot cause of this issue lies in the inherent vulnerability and sensitivity of\ngradients to non-IID data. To fundamentally address this issue, we propose a\ngradient-free method, named Analytic Federated Continual Learning (AFCL), by\nderiving analytical (i.e., closed-form) solutions from frozen extracted\nfeatures. In local training, our AFCL enables single-epoch learning with only a\nlightweight forward-propagation process for each client. In global aggregation,\nthe server can recursively and efficiently update the global model with\nsingle-round aggregation. Theoretical analyses validate that our AFCL achieves\nspatio-temporal invariance of non-IID data. This ideal property implies that,\nregardless of how heterogeneous the data are distributed across local clients\nand online tasks, the aggregated model of our AFCL remains invariant and\nidentical to that of centralized joint learning. Extensive experiments show the\nconsistent superiority of our AFCL over state-of-the-art baselines across\nvarious benchmark datasets and settings."}
{"id": "2505.13142", "pdf": "https://arxiv.org/pdf/2505.13142", "abs": "https://arxiv.org/abs/2505.13142", "authors": ["Yunhao Ni", "Yuhe Liu", "Wenxin Sun", "Yitong Tang", "Yuxin Guo", "Peilin Feng", "Wenjun Wu", "Lei Huang"], "title": "Parallel Layer Normalization for Universal Approximation", "categories": ["cs.LG", "stat.ML"], "comment": "30 pages", "summary": "Universal approximation theorem (UAT) is a fundamental theory for deep neural\nnetworks (DNNs), demonstrating their powerful representation capacity to\nrepresent and approximate any function. The analyses and proofs of UAT are\nbased on traditional network with only linear and nonlinear activation\nfunctions, but omitting normalization layers, which are commonly employed to\nenhance the training of modern networks. This paper conducts research on UAT of\nDNNs with normalization layers for the first time. We theoretically prove that\nan infinitely wide network -- composed solely of parallel layer normalization\n(PLN) and linear layers -- has universal approximation capacity. Additionally,\nwe investigate the minimum number of neurons required to approximate\n$L$-Lipchitz continuous functions, with a single hidden-layer network. We\ncompare the approximation capacity of PLN with traditional activation functions\nin theory. Different from the traditional activation functions, we identify\nthat PLN can act as both activation function and normalization in deep neural\nnetworks at the same time. We also find that PLN can improve the performance\nwhen replacing LN in transformer architectures, which reveals the potential of\nPLN used in neural architectures."}
{"id": "2505.13126", "pdf": "https://arxiv.org/pdf/2505.13126", "abs": "https://arxiv.org/abs/2505.13126", "authors": ["Liancheng Gong", "Wang Zhu", "Jesse Thomason", "Li Zhang"], "title": "Zero-Shot Iterative Formalization and Planning in Partially Observable Environments", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In planning, using LLMs not to predict plans but to formalize an environment\ninto the Planning Domain Definition Language (PDDL) has been shown to greatly\nimprove performance and control. While most work focused on fully observable\nenvironments, we tackle the more realistic and challenging partially observable\nenvironments where existing methods are incapacitated by the lack of complete\ninformation. We propose PDDLego+, a framework to iteratively formalize, plan,\ngrow, and refine PDDL representations in a zero-shot manner, without needing\naccess to any existing trajectories. On two textual simulated environments, we\nshow that PDDLego+ not only achieves superior performance, but also shows\nrobustness against problem complexity. We also show that the domain knowledge\ncaptured after a successful trial is interpretable and benefits future tasks."}
{"id": "2505.12247", "pdf": "https://arxiv.org/pdf/2505.12247", "abs": "https://arxiv.org/abs/2505.12247", "authors": ["Yinqiu Liu", "Guangyuan Liu", "Jiacheng Wang", "Ruichen Zhang", "Dusit Niyato", "Geng Sun", "Zehui Xiong", "Zhu Han"], "title": "LAMeTA: Intent-Aware Agentic Network Optimization via a Large AI Model-Empowered Two-Stage Approach", "categories": ["cs.NI", "cs.AI"], "comment": "13 pages", "summary": "Nowadays, Generative AI (GenAI) reshapes numerous domains by enabling\nmachines to create content across modalities. As GenAI evolves into autonomous\nagents capable of reasoning, collaboration, and interaction, they are\nincreasingly deployed on network infrastructures to serve humans automatically.\nThis emerging paradigm, known as the agentic network, presents new optimization\nchallenges due to the demand to incorporate subjective intents of human users\nexpressed in natural language. Traditional generic Deep Reinforcement Learning\n(DRL) struggles to capture intent semantics and adjust policies dynamically,\nthus leading to suboptimality. In this paper, we present LAMeTA, a Large AI\nModel (LAM)-empowered Two-stage Approach for intent-aware agentic network\noptimization. First, we propose Intent-oriented Knowledge Distillation (IoKD),\nwhich efficiently distills intent-understanding capabilities from\nresource-intensive LAMs to lightweight edge LAMs (E-LAMs) to serve end users.\nSecond, we develop Symbiotic Reinforcement Learning (SRL), integrating E-LAMs\nwith a policy-based DRL framework. In SRL, E-LAMs translate natural language\nuser intents into structured preference vectors that guide both state\nrepresentation and reward design. The DRL, in turn, optimizes the generative\nservice function chain composition and E-LAM selection based on real-time\nnetwork conditions, thus optimizing the subjective Quality-of-Experience (QoE).\nExtensive experiments conducted in an agentic network with 81 agents\ndemonstrate that IoKD reduces mean squared error in intent prediction by up to\n22.5%, while SRL outperforms conventional generic DRL by up to 23.5% in\nmaximizing intent-aware QoE."}
{"id": "2505.13144", "pdf": "https://arxiv.org/pdf/2505.13144", "abs": "https://arxiv.org/abs/2505.13144", "authors": ["Dongsu Lee", "Minhae Kwon"], "title": "Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "2025 ICML", "summary": "The goal of offline reinforcement learning (RL) is to extract a\nhigh-performance policy from the fixed datasets, minimizing performance\ndegradation due to out-of-distribution (OOD) samples. Offline model-based RL\n(MBRL) is a promising approach that ameliorates OOD issues by enriching\nstate-action transitions with augmentations synthesized via a learned dynamics\nmodel. Unfortunately, seminal offline MBRL methods often struggle in\nsparse-reward, long-horizon tasks. In this work, we introduce a novel MBRL\nframework, dubbed Temporal Distance-Aware Transition Augmentation (TempDATA),\nthat generates augmented transitions in a temporally structured latent space\nrather than in raw state space. To model long-horizon behavior, TempDATA learns\na latent abstraction that captures a temporal distance from both trajectory and\ntransition levels of state space. Our experiments confirm that TempDATA\noutperforms previous offline MBRL methods and achieves matching or surpassing\nthe performance of diffusion-based trajectory augmentation and goal-conditioned\nRL on the D4RL AntMaze, FrankaKitchen, CALVIN, and pixel-based FrankaKitchen."}
{"id": "2505.13208", "pdf": "https://arxiv.org/pdf/2505.13208", "abs": "https://arxiv.org/abs/2505.13208", "authors": ["Colin Krawchuk", "Nikhil Khatri", "Neil John Ortega", "Dimitri Kartsaklis"], "title": "Efficient Generation of Parameterised Quantum Circuits from Large Texts", "categories": ["quant-ph", "cs.AI", "cs.CL"], "comment": null, "summary": "Quantum approaches to natural language processing (NLP) are redefining how\nlinguistic information is represented and processed. While traditional hybrid\nquantum-classical models rely heavily on classical neural networks, recent\nadvancements propose a novel framework, DisCoCirc, capable of directly encoding\nentire documents as parameterised quantum circuits (PQCs), besides enjoying\nsome additional interpretability and compositionality benefits. Following these\nideas, this paper introduces an efficient methodology for converting\nlarge-scale texts into quantum circuits using tree-like representations of\npregroup diagrams. Exploiting the compositional parallels between language and\nquantum mechanics, grounded in symmetric monoidal categories, our approach\nenables faithful and efficient encoding of syntactic and discourse\nrelationships in long and complex texts (up to 6410 words in our experiments)\nto quantum circuits. The developed system is provided to the community as part\nof the augmented open-source quantum NLP package lambeq Gen II."}
{"id": "2505.12250", "pdf": "https://arxiv.org/pdf/2505.12250", "abs": "https://arxiv.org/abs/2505.12250", "authors": ["Chi Zhang", "Huaping Zhong", "Hongtao Li", "Chengliang Chai", "Jiawei Hong", "Yuhao Deng", "Jiacheng Wang", "Tian Tan", "Yizhou Yan", "Jiantao Qiu", "Ye Yuan", "Guoren Wang", "Conghui He", "Lei Cao"], "title": "Not All Documents Are What You Need for Extracting Instruction Tuning Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction tuning improves the performance of large language models (LLMs),\nbut it heavily relies on high-quality training data. Recently, LLMs have been\nused to synthesize instruction data using seed question-answer (QA) pairs.\nHowever, these synthesized instructions often lack diversity and tend to be\nsimilar to the input seeds, limiting their applicability in real-world\nscenarios. To address this, we propose extracting instruction tuning data from\nweb corpora that contain rich and diverse knowledge. A naive solution is to\nretrieve domain-specific documents and extract all QA pairs from them, but this\nfaces two key challenges: (1) extracting all QA pairs using LLMs is\nprohibitively expensive, and (2) many extracted QA pairs may be irrelevant to\nthe downstream tasks, potentially degrading model performance. To tackle these\nissues, we introduce EQUAL, an effective and scalable data extraction framework\nthat iteratively alternates between document selection and high-quality QA pair\nextraction to enhance instruction tuning. EQUAL first clusters the document\ncorpus based on embeddings derived from contrastive learning, then uses a\nmulti-armed bandit strategy to efficiently identify clusters that are likely to\ncontain valuable QA pairs. This iterative approach significantly reduces\ncomputational cost while boosting model performance. Experiments on\nAutoMathText and StackOverflow across four downstream tasks show that EQUAL\nreduces computational costs by 5-10x and improves accuracy by 2.5 percent on\nLLaMA-3.1-8B and Mistral-7B"}
{"id": "2505.13150", "pdf": "https://arxiv.org/pdf/2505.13150", "abs": "https://arxiv.org/abs/2505.13150", "authors": ["Maksim Bobrin", "Ilya Zisman", "Alexander Nikulin", "Vladislav Kurenkov", "Dmitry Dylov"], "title": "Zero-Shot Adaptation of Behavioral Foundation Models to Unseen Dynamics", "categories": ["cs.LG"], "comment": null, "summary": "Behavioral Foundation Models (BFMs) proved successful in producing policies\nfor arbitrary tasks in a zero-shot manner, requiring no test-time training or\ntask-specific fine-tuning. Among the most promising BFMs are the ones that\nestimate the successor measure learned in an unsupervised way from\ntask-agnostic offline data. However, these methods fail to react to changes in\nthe dynamics, making them inefficient under partial observability or when the\ntransition function changes. This hinders the applicability of BFMs in a\nreal-world setting, e.g., in robotics, where the dynamics can unexpectedly\nchange at test time. In this work, we demonstrate that Forward-Backward (FB)\nrepresentation, one of the methods from the BFM family, cannot distinguish\nbetween distinct dynamics, leading to an interference among the latent\ndirections, which parametrize different policies. To address this, we propose a\nFB model with a transformer-based belief estimator, which greatly facilitates\nzero-shot adaptation. We also show that partitioning the policy encoding space\ninto dynamics-specific clusters, aligned with the context-embedding directions,\nyields additional gain in performance. These traits allow our method to respond\nto the dynamics observed during training and to generalize to unseen ones.\nEmpirically, in the changing dynamics setting, our approach achieves up to a 2x\nhigher zero-shot returns compared to the baselines for both discrete and\ncontinuous tasks."}
{"id": "2505.13227", "pdf": "https://arxiv.org/pdf/2505.13227", "abs": "https://arxiv.org/abs/2505.13227", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "49 pages, 13 figures", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io."}
{"id": "2505.12254", "pdf": "https://arxiv.org/pdf/2505.12254", "abs": "https://arxiv.org/abs/2505.12254", "authors": ["Yiwei Ou", "Xiaobin Ren", "Ronggui Sun", "Guansong Gao", "Ziyi Jiang", "Kaiqi Zhao", "Manfredo Manfredini"], "title": "MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Existing visual place recognition (VPR) datasets predominantly rely on\nvehicle-mounted imagery, lack multimodal diversity and underrepresent dense,\nmixed-use street-level spaces, especially in non-Western urban contexts. To\naddress these gaps, we introduce MMS-VPR, a large-scale multimodal dataset for\nstreet-level place recognition in complex, pedestrian-only environments. The\ndataset comprises 78,575 annotated images and 2,512 video clips captured across\n207 locations in a ~70,800 $\\mathrm{m}^2$ open-air commercial district in\nChengdu, China. Each image is labeled with precise GPS coordinates, timestamp,\nand textual metadata, and covers varied lighting conditions, viewpoints, and\ntimeframes. MMS-VPR follows a systematic and replicable data collection\nprotocol with minimal device requirements, lowering the barrier for scalable\ndataset creation. Importantly, the dataset forms an inherent spatial graph with\n125 edges, 81 nodes, and 1 subgraph, enabling structure-aware place\nrecognition. We further define two application-specific subsets --\nDataset_Edges and Dataset_Points -- to support fine-grained and graph-based\nevaluation tasks. Extensive benchmarks using conventional VPR models, graph\nneural networks, and multimodal baselines show substantial improvements when\nleveraging multimodal and structural cues. MMS-VPR facilitates future research\nat the intersection of computer vision, geospatial understanding, and\nmultimodal reasoning. The dataset is publicly available at\nhttps://huggingface.co/datasets/Yiwei-Ou/MMS-VPR."}
{"id": "2505.13169", "pdf": "https://arxiv.org/pdf/2505.13169", "abs": "https://arxiv.org/abs/2505.13169", "authors": ["Sara Alosaime", "Arshad Jhumka"], "title": "RIFLES: Resource-effIcient Federated LEarning via Scheduling", "categories": ["cs.LG"], "comment": null, "summary": "Federated Learning (FL) is a privacy-preserving machine learning technique\nthat allows decentralized collaborative model training across a set of\ndistributed clients, by avoiding raw data exchange. A fundamental component of\nFL is the selection of a subset of clients in each round for model training by\na central server. Current selection strategies are myopic in nature in that\nthey are based on past or current interactions, often leading to inefficiency\nissues such as straggling clients. In this paper, we address this serious\nshortcoming by proposing the RIFLES approach that builds a novel availability\nforecasting layer to support the client selection process. We make the\nfollowing contributions: (i) we formalise the sequential selection problem and\nreduce it to a scheduling problem and show that the problem is NP-complete,\n(ii) leveraging heartbeat messages from clients, RIFLES build an availability\nprediction layer to support (long term) selection decisions, (iii) we propose a\nnovel adaptive selection strategy to support efficient learning and resource\nusage. To circumvent the inherent exponential complexity, we present RIFLES, a\nheuristic that leverages clients' historical availability data by using a\nCNN-LSTM time series forecasting model, allowing the server to predict the\noptimal participation times of clients, thereby enabling informed selection\ndecisions. By comparing against other FL techniques, we show that RIFLES\nprovide significant improvement by between 10%-50% on a variety of metrics such\nas accuracy and test loss. To the best of our knowledge, it is the first work\nto investigate FL as a scheduling problem."}
{"id": "2505.13237", "pdf": "https://arxiv.org/pdf/2505.13237", "abs": "https://arxiv.org/abs/2505.13237", "authors": ["Chih-Kai Yang", "Neo Ho", "Yen-Ting Piao", "Hung-yi Lee"], "title": "SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Large audio-language models (LALMs) extend the large language models with\nmultimodal understanding in speech, audio, etc. While their performances on\nspeech and audio-processing tasks are extensively studied, their reasoning\nabilities remain underexplored. Particularly, their multi-hop reasoning, the\nability to recall and integrate multiple facts, lacks systematic evaluation.\nExisting benchmarks focus on general speech and audio-processing tasks,\nconversational abilities, and fairness but overlook this aspect. To bridge this\ngap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning\nbased on speech and audio information. Results show that LALMs struggle to\nintegrate speech/audio representations for multi-hop reasoning, even when they\nextract the relevant information correctly, highlighting a fundamental\nchallenge in multimodal reasoning. Our findings expose a critical limitation in\nLALMs, offering insights and resources for future research."}
{"id": "2505.12260", "pdf": "https://arxiv.org/pdf/2505.12260", "abs": "https://arxiv.org/abs/2505.12260", "authors": ["Guangyuan Ma", "Yongliang Ma", "Xuanrui Gou", "Zhenpeng Su", "Ming Zhou", "Songlin Hu"], "title": "LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode\nqueries and documents into low-dimensional dense or high-dimensional sparse\nvectors. It retrieves documents relevant to search queries based on vector\nsimilarities. Documents are pre-encoded offline, while queries arrive in\nreal-time, necessitating an efficient online query encoder. Although LLMs\nsignificantly enhance retrieval capabilities, serving deeply parameterized LLMs\nslows down query inference throughput and increases demands for online\ndeployment resources. In this paper, we propose LightRetriever, a novel\nLLM-based hybrid retriever with extremely lightweight query encoders. Our\nmethod retains a full-sized LLM for document encoding, but reduces the workload\nof query encoding to no more than an embedding lookup. Compared to serving a\nfull-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for\nquery inference with GPU acceleration, and even a 20x speedup without GPU.\nExperiments on large-scale retrieval benchmarks demonstrate that our method\ngeneralizes well across diverse retrieval tasks, retaining an average of 95%\nfull-sized performance."}
{"id": "2505.13188", "pdf": "https://arxiv.org/pdf/2505.13188", "abs": "https://arxiv.org/abs/2505.13188", "authors": ["Juntian Zhu", "Miguel de Carvalho", "Zhouwang Yang", "Fengxiang He"], "title": "When a Reinforcement Learning Agent Encounters Unknown Unknowns", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "An AI agent might surprisingly find she has reached an unknown state which\nshe has never been aware of -- an unknown unknown. We mathematically ground\nthis scenario in reinforcement learning: an agent, after taking an action\ncalculated from value functions $Q$ and $V$ defined on the {\\it {aware\ndomain}}, reaches a state out of the domain. To enable the agent to handle this\nscenario, we propose an {\\it episodic Markov decision {process} with growing\nawareness} (EMDP-GA) model, taking a new {\\it noninformative value expansion}\n(NIVE) approach to expand value functions to newly aware areas: when an agent\narrives at an unknown unknown, value functions $Q$ and $V$ whereon are\ninitialised by noninformative beliefs -- the averaged values on the aware\ndomain. This design is out of respect for the complete absence of knowledge in\nthe newly discovered state. The upper confidence bound momentum Q-learning is\nthen adapted to the growing awareness for training the EMDP-GA model. We prove\nthat (1) the regret of our approach is asymptotically consistent with the state\nof the art (SOTA) without exposure to unknown unknowns in an extremely\nuncertain environment, and (2) our computational complexity and space\ncomplexity are comparable with the SOTA -- these collectively suggest that\nthough an unknown unknown is surprising, it will be asymptotically properly\ndiscovered with decent speed and an affordable cost."}
{"id": "2505.13308", "pdf": "https://arxiv.org/pdf/2505.13308", "abs": "https://arxiv.org/abs/2505.13308", "authors": ["Hengli Li", "Chenxi Li", "Tong Wu", "Xuekai Zhu", "Yuxuan Wang", "Zhaoxin Yu", "Eric Hanchen Jiang", "Song-Chun Zhu", "Zixia Jia", "Ying Nian Wu", "Zilong Zheng"], "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs."}
{"id": "2505.12269", "pdf": "https://arxiv.org/pdf/2505.12269", "abs": "https://arxiv.org/abs/2505.12269", "authors": ["Kerry Xiao", "Amy Zang"], "title": "Vague Knowledge: Evidence from Analyst Reports", "categories": ["econ.GN", "cs.AI", "cs.CL", "math.LO", "q-fin.EC", "q-fin.GN", "03B48, 03B65, 03E02, 03E15, 03E72, 18E45, 28A05, 62F15, 68T01,\n  68T35, 68T50, 91G30,", "F.4; I.2.3; I.2.4; I.2.7; J.1; J.4; J.5"], "comment": null, "summary": "People in the real world often possess vague knowledge of future payoffs, for\nwhich quantification is not feasible or desirable. We argue that language, with\ndiffering ability to convey vague information, plays an important but less\nknown-role in subjective expectations. Empirically, we find that in their\nreports, analysts include useful information in linguistic expressions but not\nnumerical forecasts. Specifically, the textual tone of analyst reports has\npredictive power for forecast errors and subsequent revisions in numerical\nforecasts, and this relation becomes stronger when analyst's language is\nvaguer, when uncertainty is higher, and when analysts are busier. Overall, our\ntheory and evidence suggest that some useful information is vaguely known and\nonly communicated through language."}
{"id": "2505.13192", "pdf": "https://arxiv.org/pdf/2505.13192", "abs": "https://arxiv.org/abs/2505.13192", "authors": ["Christoph Jürgen Hemmer", "Daniel Durstewitz"], "title": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics", "categories": ["cs.LG", "cs.AI", "math.DS", "nlin.CD"], "comment": null, "summary": "Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters and\norders of magnitude faster inference times. DynaMix outperforms TS foundation\nmodels in terms of long-term statistics, and often also short-term forecasts,\neven on real-world time series, like traffic or weather data, typically used\nfor training and evaluating TS models, but not at all part of DynaMix' training\ncorpus. We illustrate some of the failure modes of TS models for DSR problems,\nand conclude that models built on DS principles may bear a huge potential also\nfor advancing the TS prediction field."}
{"id": "2505.13380", "pdf": "https://arxiv.org/pdf/2505.13380", "abs": "https://arxiv.org/abs/2505.13380", "authors": ["Nam V. Nguyen", "Huy Nguyen", "Quang Pham", "Van Nguyen", "Savitha Ramasamy", "Nhat Ho"], "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition", "categories": ["cs.AI", "cs.CL"], "comment": "52 pages. This work is an improved version of the previous study at\n  arXiv:2402.02526", "summary": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526"}
{"id": "2505.12275", "pdf": "https://arxiv.org/pdf/2505.12275", "abs": "https://arxiv.org/abs/2505.12275", "authors": ["Wen-Chao Hu", "Qi-Jie Li", "Lin-Han Jia", "Cunjing Ge", "Yu-Feng Li", "Yuan Jiang", "Zhi-Hua Zhou"], "title": "Curriculum Abductive Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Abductive Learning (ABL) integrates machine learning with logical reasoning\nin a loop: a learning model predicts symbolic concept labels from raw inputs,\nwhich are revised through abduction using domain knowledge and then fed back\nfor retraining. However, due to the nondeterminism of abduction, the training\nprocess often suffers from instability, especially when the knowledge base is\nlarge and complex, resulting in a prohibitively large abduction space. While\nprior works focus on improving candidate selection within this space, they\ntypically treat the knowledge base as a static black box. In this work, we\npropose Curriculum Abductive Learning (C-ABL), a method that explicitly\nleverages the internal structure of the knowledge base to address the ABL\ntraining challenges. C-ABL partitions the knowledge base into a sequence of\nsub-bases, progressively introduced during training. This reduces the abduction\nspace throughout training and enables the model to incorporate logic in a\nstepwise, smooth way. Experiments across multiple tasks show that C-ABL\noutperforms previous ABL implementations, significantly improves training\nstability, convergence speed, and final accuracy, especially under complex\nknowledge setting."}
{"id": "2505.13196", "pdf": "https://arxiv.org/pdf/2505.13196", "abs": "https://arxiv.org/abs/2505.13196", "authors": ["Pranav Vaidhyanathan", "Lucas Schorling", "Natalia Ares", "Michael A. Osborne"], "title": "A Physics-Inspired Optimizer: Velocity Regularized Adam", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": "L. Schorling and P. Vaidhyanathan contributed equally to this work.\n  20 pages, 13 figures", "summary": "We introduce Velocity-Regularized Adam (VRAdam), a physics-inspired optimizer\nfor training deep neural networks that draws on ideas from quartic terms for\nkinetic energy with its stabilizing effects on various system dynamics.\nPrevious algorithms, including the ubiquitous Adam, operate at the so called\nadaptive edge of stability regime during training leading to rapid oscillations\nand slowed convergence of loss. However, VRAdam adds a higher order penalty on\nthe learning rate based on the velocity such that the algorithm automatically\nslows down whenever weight updates become large. In practice, we observe that\nthe effective dynamic learning rate shrinks in high-velocity regimes, damping\noscillations and allowing for a more aggressive base step size when necessary\nwithout divergence. By combining this velocity-based regularizer for global\ndamping with per-parameter scaling of Adam to create a hybrid optimizer, we\ndemonstrate that VRAdam consistently exceeds the performance against standard\noptimizers including AdamW. We benchmark various tasks such as image\nclassification, language modeling, image generation and generative modeling\nusing diverse architectures and training methodologies including Convolutional\nNeural Networks (CNNs), Transformers, and GFlowNets."}
{"id": "2505.13393", "pdf": "https://arxiv.org/pdf/2505.13393", "abs": "https://arxiv.org/abs/2505.13393", "authors": ["Christopher K. Frantz"], "title": "IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar", "categories": ["cs.MA", "cs.AI", "cs.CL", "68T30, 68T50", "E.2; H.1.0; I.7.2; I.6.5; K.4.1"], "comment": "24 pages", "summary": "This article provides an overview of IG Parser, a software that facilitates\nqualitative content analysis of formal (e.g., legal) rules or informal (e.g.,\nsocio-normative) norms, and strategies (such as conventions) -- referred to as\n\\emph{institutions} -- that govern social systems and operate configurally to\ndescribe \\emph{institutional systems}. To this end, the IG Parser employs a\ndistinctive syntax that ensures rigorous encoding of natural language, while\nautomating the transformation into various formats that support the downstream\nanalysis using diverse analytical techniques. The conceptual core of the IG\nParser is an associated syntax, IG Script, that operationalizes the conceptual\nfoundations of the Institutional Grammar, and more specifically Institutional\nGrammar 2.0, an analytical paradigm for institutional analysis. This article\npresents the IG Parser, including its conceptual foundations, syntactic\nspecification of IG Script, alongside architectural principles. This\nintroduction is augmented with selective illustrative examples that highlight\nthe use and benefit associated with the tool."}
{"id": "2505.12287", "pdf": "https://arxiv.org/pdf/2505.12287", "abs": "https://arxiv.org/abs/2505.12287", "authors": ["Linghan Huang", "Haolin Jin", "Zhaoge Bi", "Pengyue Yang", "Peizhou Zhao", "Taozhao Chen", "Xiongfei Wu", "Lei Ma", "Huaming Chen"], "title": "The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have seen widespread applications across various\ndomains, yet remain vulnerable to adversarial prompt injections. While most\nexisting research on jailbreak attacks and hallucination phenomena has focused\nprimarily on open-source models, we investigate the frontier of closed-source\nLLMs under multilingual attack scenarios. We present a first-of-its-kind\nintegrated adversarial framework that leverages diverse attack techniques to\nsystematically evaluate frontier proprietary solutions, including GPT-4o,\nDeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories\nof security contents in both English and Chinese, generating 38,400 responses\nacross 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as\nthe quantitative metric to assess performance from three dimensions: prompt\ndesign, model architecture, and language environment. Our findings suggest that\nQwen-Max is the most vulnerable, while GPT-4o shows the strongest defense.\nNotably, prompts in Chinese consistently yield higher ASRs than their English\ncounterparts, and our novel Two-Sides attack technique proves to be the most\neffective across all models. This work highlights a dire need for\nlanguage-aware alignment and robust cross-lingual defenses in LLMs, and we hope\nit will inspire researchers, developers, and policymakers toward more robust\nand inclusive AI systems."}
{"id": "2505.13197", "pdf": "https://arxiv.org/pdf/2505.13197", "abs": "https://arxiv.org/abs/2505.13197", "authors": ["Stephen Zhang", "Suryanarayana Maddu", "Xiaoje Qiu", "Victor Chardès"], "title": "Inferring stochastic dynamics with growth from cross-sectional data", "categories": ["cs.LG", "physics.bio-ph", "q-bio.QM"], "comment": "9 pages, 5 figures", "summary": "Time-resolved single-cell omics data offers high-throughput, genome-wide\nmeasurements of cellular states, which are instrumental to reverse-engineer the\nprocesses underpinning cell fate. Such technologies are inherently destructive,\nallowing only cross-sectional measurements of the underlying stochastic\ndynamical system. Furthermore, cells may divide or die in addition to changing\ntheir molecular state. Collectively these present a major challenge to\ninferring realistic biophysical models. We present a novel approach,\n\\emph{unbalanced} probability flow inference, that addresses this challenge for\nbiological processes modelled as stochastic dynamics with growth. By leveraging\na Lagrangian formulation of the Fokker-Planck equation, our method accurately\ndisentangles drift from intrinsic noise and growth. We showcase the\napplicability of our approach through evaluation on a range of simulated and\nreal single-cell RNA-seq datasets. Comparing to several existing methods, we\nfind our method achieves higher accuracy while enjoying a simple two-step\ntraining scheme."}
{"id": "2505.13398", "pdf": "https://arxiv.org/pdf/2505.13398", "abs": "https://arxiv.org/abs/2505.13398", "authors": ["Matan Abudy", "Orr Well", "Emmanuel Chemla", "Roni Katzir", "Nur Lan"], "title": "A Minimum Description Length Approach to Regularization in Neural Networks", "categories": ["cs.LG", "cs.CL"], "comment": "9 pages", "summary": "State-of-the-art neural networks can be trained to become remarkable\nsolutions to many problems. But while these architectures can express symbolic,\nperfect solutions, trained models often arrive at approximations instead. We\nshow that the choice of regularization method plays a crucial role: when\ntrained on formal languages with standard regularization ($L_1$, $L_2$, or\nnone), expressive architectures not only fail to converge to correct solutions\nbut are actively pushed away from perfect initializations. In contrast,\napplying the Minimum Description Length (MDL) principle to balance model\ncomplexity with data fit provides a theoretically grounded regularization\nmethod. Using MDL, perfect solutions are selected over approximations,\nindependently of the optimization algorithm. We propose that unlike existing\nregularization techniques, MDL introduces the appropriate inductive bias to\neffectively counteract overfitting and promote generalization."}
{"id": "2505.12292", "pdf": "https://arxiv.org/pdf/2505.12292", "abs": "https://arxiv.org/abs/2505.12292", "authors": ["Boxun Xu", "Richard Boone", "Peng Li"], "title": "SpikeX: Exploring Accelerator Architecture and Network-Hardware Co-Optimization for Sparse Spiking Neural Networks", "categories": ["cs.NE", "cs.AI", "cs.AR"], "comment": "The paper has been accepted by IEEE TCAD", "summary": "Spiking Neural Networks (SNNs) are promising biologically plausible models of\ncomputation which utilize a spiking binary activation function similar to that\nof biological neurons. SNNs are well positioned to process spatiotemporal data,\nand are advantageous in ultra-low power and real-time processing. Despite a\nlarge body of work on conventional artificial neural network accelerators, much\nless attention has been given to efficient SNN hardware accelerator design. In\nparticular, SNNs exhibit inherent unstructured spatial and temporal firing\nsparsity, an opportunity yet to be fully explored for great hardware processing\nefficiency. In this work, we propose a novel systolic-array SNN accelerator\narchitecture, called SpikeX, to take on the challenges and opportunities\nstemming from unstructured sparsity while taking into account the unique\ncharacteristics of spike-based computation. By developing an efficient dataflow\ntargeting expensive multi-bit weight data movements, SpikeX reduces memory\naccess and increases data sharing and hardware utilization for computations\nspanning across both time and space, thereby significantly improving energy\nefficiency and inference latency. Furthermore, recognizing the importance of\nSNN network and hardware co-design, we develop a co-optimization methodology\nfacilitating not only hardware-aware SNN training but also hardware accelerator\narchitecture search, allowing joint network weight parameter optimization and\naccelerator architectural reconfiguration. This end-to-end network/accelerator\nco-design approach offers a significant reduction of 15.1x-150.87x in\nenergy-delay-product(EDP) without comprising model accuracy."}
{"id": "2505.13230", "pdf": "https://arxiv.org/pdf/2505.13230", "abs": "https://arxiv.org/abs/2505.13230", "authors": ["Francesco D'Amico", "Dario Bocchi", "Matteo Negri"], "title": "Implicit bias produces neural scaling laws in learning curves, from perceptrons to deep networks", "categories": ["cs.LG", "cond-mat.dis-nn", "stat.ML"], "comment": null, "summary": "Scaling laws in deep learning - empirical power-law relationships linking\nmodel performance to resource growth - have emerged as simple yet striking\nregularities across architectures, datasets, and tasks. These laws are\nparticularly impactful in guiding the design of state-of-the-art models, since\nthey quantify the benefits of increasing data or model size, and hint at the\nfoundations of interpretability in machine learning. However, most studies\nfocus on asymptotic behavior at the end of training or on the optimal training\ntime given the model size. In this work, we uncover a richer picture by\nanalyzing the entire training dynamics through the lens of spectral complexity\nnorms. We identify two novel dynamical scaling laws that govern how performance\nevolves during training. These laws together recover the well-known test error\nscaling at convergence, offering a mechanistic explanation of generalization\nemergence. Our findings are consistent across CNNs, ResNets, and Vision\nTransformers trained on MNIST, CIFAR-10 and CIFAR-100. Furthermore, we provide\nanalytical support using a solvable model: a single-layer perceptron trained\nwith binary cross-entropy. In this setting, we show that the growth of spectral\ncomplexity driven by the implicit bias mirrors the generalization behavior\nobserved at fixed norm, allowing us to connect the performance dynamics to\nclassical learning rules in the perceptron."}
{"id": "2505.13408", "pdf": "https://arxiv.org/pdf/2505.13408", "abs": "https://arxiv.org/abs/2505.13408", "authors": ["Jinhe Bi", "Danqi Yan", "Yifan Wang", "Wenke Huang", "Haokun Chen", "Guancheng Wan", "Mang Ye", "Xun Xiao", "Hinrich Schuetze", "Volker Tresp", "Yunpu Ma"], "title": "CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent Large Reasoning Models significantly improve the reasoning ability of\nLarge Language Models by learning to reason, exhibiting the promising\nperformance in solving complex tasks. LRMs solve tasks that require complex\nreasoning by explicitly generating reasoning trajectories together with\nanswers. Nevertheless, judging the quality of such an output answer is not easy\nbecause only considering the correctness of the answer is not enough and the\nsoundness of the reasoning trajectory part matters as well. Logically, if the\nsoundness of the reasoning part is poor, even if the answer is correct, the\nconfidence of the derived answer should be low. Existing methods did consider\njointly assessing the overall output answer by taking into account the\nreasoning part, however, their capability is still not satisfactory as the\ncausal relationship of the reasoning to the concluded answer cannot properly\nreflected. In this paper, inspired by classical mechanics, we present a novel\napproach towards establishing a CoT-Kinetics energy equation. Specifically, our\nCoT-Kinetics energy equation formulates the token state transformation process,\nwhich is regulated by LRM internal transformer layers, as like a particle\nkinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy\nassigns a scalar score to evaluate specifically the soundness of the reasoning\nphase, telling how confident the derived answer could be given the evaluated\nreasoning. As such, the LRM's overall output quality can be accurately\nmeasured, rather than a coarse judgment (e.g., correct or incorrect) anymore."}
{"id": "2505.12296", "pdf": "https://arxiv.org/pdf/2505.12296", "abs": "https://arxiv.org/abs/2505.12296", "authors": ["Haiyu Deng", "Yanna Jiang", "Guangsheng Yu", "Qin Wang", "Xu Wang", "Baihe Ma", "Wei Ni", "Ren Ping Liu"], "title": "PoLO: Proof-of-Learning and Proof-of-Ownership at Once with Chained Watermarking", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Machine learning models are increasingly shared and outsourced, raising\nrequirements of verifying training effort (Proof-of-Learning, PoL) to ensure\nclaimed performance and establishing ownership (Proof-of-Ownership, PoO) for\ntransactions. When models are trained by untrusted parties, PoL and PoO must be\nenforced together to enable protection, attribution, and compensation. However,\nexisting studies typically address them separately, which not only weakens\nprotection against forgery and privacy breaches but also leads to high\nverification overhead.\n  We propose PoLO, a unified framework that simultaneously achieves PoL and PoO\nusing chained watermarks. PoLO splits the training process into fine-grained\ntraining shards and embeds a dedicated watermark in each shard. Each watermark\nis generated using the hash of the preceding shard, certifying the training\nprocess of the preceding shard. The chained structure makes it computationally\ndifficult to forge any individual part of the whole training process. The\ncomplete set of watermarks serves as the PoL, while the final watermark\nprovides the PoO. PoLO offers more efficient and privacy-preserving\nverification compared to the vanilla PoL solutions that rely on gradient-based\ntrajectory tracing and inadvertently expose training data during verification,\nwhile maintaining the same level of ownership assurance of watermark-based PoO\nschemes. Our evaluation shows that PoLO achieves 99% watermark detection\naccuracy for ownership verification, while preserving data privacy and cutting\nverification costs to just 1.5-10% of traditional methods. Forging PoLO demands\n1.1-4x more resources than honest proof generation, with the original proof\nretaining over 90% detection accuracy even after attacks."}
{"id": "2505.13241", "pdf": "https://arxiv.org/pdf/2505.13241", "abs": "https://arxiv.org/abs/2505.13241", "authors": ["Yuan-Zheng Lei", "Yaobang Gong", "Dianwei Chen", "Yao Cheng", "Xianfeng Terry Yang"], "title": "Reconstructing Physics-Informed Machine Learning for Traffic Flow Modeling: a Multi-Gradient Descent and Pareto Learning Approach", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Physics-informed machine learning (PIML) is crucial in modern traffic flow\nmodeling because it combines the benefits of both physics-based and data-driven\napproaches. In conventional PIML, physical information is typically\nincorporated by constructing a hybrid loss function that combines data-driven\nloss and physics loss through linear scalarization. The goal is to find a\ntrade-off between these two objectives to improve the accuracy of model\npredictions. However, from a mathematical perspective, linear scalarization is\nlimited to identifying only the convex region of the Pareto front, as it treats\ndata-driven and physics losses as separate objectives. Given that most PIML\nloss functions are non-convex, linear scalarization restricts the achievable\ntrade-off solutions. Moreover, tuning the weighting coefficients for the two\nloss components can be both time-consuming and computationally challenging. To\naddress these limitations, this paper introduces a paradigm shift in PIML by\nreformulating the training process as a multi-objective optimization problem,\ntreating data-driven loss and physics loss independently. We apply several\nmulti-gradient descent algorithms (MGDAs), including traditional multi-gradient\ndescent (TMGD) and dual cone gradient descent (DCGD), to explore the Pareto\nfront in this multi-objective setting. These methods are evaluated on both\nmacroscopic and microscopic traffic flow models. In the macroscopic case, MGDAs\nachieved comparable performance to traditional linear scalarization methods.\nNotably, in the microscopic case, MGDAs significantly outperformed their\nscalarization-based counterparts, demonstrating the advantages of a\nmulti-objective optimization approach in complex PIML scenarios."}
{"id": "2505.13430", "pdf": "https://arxiv.org/pdf/2505.13430", "abs": "https://arxiv.org/abs/2505.13430", "authors": ["Sifeng Shang", "Jiayi Zhou", "Chenyu Lin", "Minxian Li", "Kaiyang Zhou"], "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU."}
{"id": "2505.12298", "pdf": "https://arxiv.org/pdf/2505.12298", "abs": "https://arxiv.org/abs/2505.12298", "authors": ["Amal Lahchim", "Lazar Davic"], "title": "Attention-Enhanced U-Net for Accurate Segmentation of COVID-19 Infected Lung Regions in CT Scans", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "14 pages, 9 figures, created using Google Colab and PyTorch. Compares\n  segmentation models for COVID-19 CT data", "summary": "In this study, we propose a robust methodology for automatic segmentation of\ninfected lung regions in COVID-19 CT scans using convolutional neural networks.\nThe approach is based on a modified U-Net architecture enhanced with attention\nmechanisms, data augmentation, and postprocessing techniques. It achieved a\nDice coefficient of 0.8658 and mean IoU of 0.8316, outperforming other methods.\nThe dataset was sourced from public repositories and augmented for diversity.\nResults demonstrate superior segmentation performance. Future work includes\nexpanding the dataset, exploring 3D segmentation, and preparing the model for\nclinical deployment."}
{"id": "2505.13249", "pdf": "https://arxiv.org/pdf/2505.13249", "abs": "https://arxiv.org/abs/2505.13249", "authors": ["Le Vu Anh", "Dinh Duc Nha Nguyen", "Phi Long Nguyen"], "title": "RN-F: A Novel Approach for Mitigating Contaminated Data in Large Language Models", "categories": ["cs.LG"], "comment": "12 pages, 4 figures, 3 tables", "summary": "Large Language Models (LLMs) have become foundational in modern artificial\nintelligence, powering a wide range of applications from code generation and\nvirtual assistants to scientific research and enterprise automation. However,\nconcerns about data contamination--where test data overlaps with training\ndata--have raised serious questions about the reliability of these\napplications. Despite awareness of this issue, existing methods fall short in\neffectively identifying or mitigating contamination. In this paper, we propose\nResidual-Noise Fingerprinting (RN-F), a novel framework for detecting\ncontaminated data in LLMs. RN-F is a single-pass, gradient-free detection\nmethod that leverages residual signal patterns without introducing additional\nfloating-point operations. Our approach is lightweight, model-agnostic, and\nefficient. We evaluate RN-F on multiple LLMs across various contaminated\ndatasets and show that it consistently outperforms existing state-of-the-art\nmethods, achieving performance improvements of up to 10.5% in contamination\ndetection metrics."}
{"id": "2505.13438", "pdf": "https://arxiv.org/pdf/2505.13438", "abs": "https://arxiv.org/abs/2505.13438", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency."}
{"id": "2505.12299", "pdf": "https://arxiv.org/pdf/2505.12299", "abs": "https://arxiv.org/abs/2505.12299", "authors": ["Kun Huang", "Weikai Xu", "Yuxuan Liu", "Quandong Wang", "Pengzhi Gao", "Wei Liu", "Jian Luan", "Bin Wang", "Bo An"], "title": "Enhance Mobile Agents Thinking Process Via Iterative Preference Learning", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 8 figures, 7 tables", "summary": "The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to\nimprove the reasoning performance of VLM-based mobile agents in GUI tasks.\nHowever, the scarcity of diverse CoaT trajectories limits the expressiveness\nand generalization ability of such agents. While self-training is commonly\nemployed to address data scarcity, existing approaches either overlook the\ncorrectness of intermediate reasoning steps or depend on expensive\nprocess-level annotations to construct process reward models (PRM). To address\nthe above problems, we propose an Iterative Preference Learning (IPL) that\nconstructs a CoaT-tree through interative sampling, scores leaf nodes using\nrule-based reward, and backpropagates feedback to derive Thinking-level Direct\nPreference Optimization (T-DPO) pairs. To prevent overfitting during warm-up\nsupervised fine-tuning, we further introduce a three-stage instruction\nevolution, which leverages GPT-4o to generate diverse Q\\&A pairs based on real\nmobile UI screenshots, enhancing both generality and layout understanding.\nExperiments on three standard Mobile GUI-agent benchmarks demonstrate that our\nagent MobileIPL outperforms strong baselines, including continual pretraining\nmodels such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance\nacross three standard Mobile GUI-Agents benchmarks and shows strong\ngeneralization to out-of-domain scenarios."}
{"id": "2505.13264", "pdf": "https://arxiv.org/pdf/2505.13264", "abs": "https://arxiv.org/abs/2505.13264", "authors": ["Carlos Rodriguez-Pardo", "Louis Daumas", "Leonardo Chiani", "Massimo Tavoni"], "title": "Net-Zero: A Comparative Study on Neural Network Design for Climate-Economic PDEs Under Uncertainty", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.PF", "math.AP", "68T07 (Primary) 35Q91, 91B76 (Secondary)", "I.2.1; I.5.1; J.4"], "comment": "Under review", "summary": "Climate-economic modeling under uncertainty presents significant\ncomputational challenges that may limit policymakers' ability to address\nclimate change effectively. This paper explores neural network-based approaches\nfor solving high-dimensional optimal control problems arising from models that\nincorporate ambiguity aversion in climate mitigation decisions. We develop a\ncontinuous-time endogenous-growth economic model that accounts for multiple\nmitigation pathways, including emission-free capital and carbon intensity\nreductions. Given the inherent complexity and high dimensionality of these\nmodels, traditional numerical methods become computationally intractable. We\nbenchmark several neural network architectures against finite-difference\ngenerated solutions, evaluating their ability to capture the dynamic\ninteractions between uncertainty, technology transitions, and optimal climate\npolicy. Our findings demonstrate that appropriate neural architecture selection\nsignificantly impacts both solution accuracy and computational efficiency when\nmodeling climate-economic systems under uncertainty. These methodological\nadvances enable more sophisticated modeling of climate policy decisions,\nallowing for better representation of technology transitions and\nuncertainty-critical elements for developing effective mitigation strategies in\nthe face of climate change."}
{"id": "2505.13445", "pdf": "https://arxiv.org/pdf/2505.13445", "abs": "https://arxiv.org/abs/2505.13445", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "categories": ["cs.AI", "cs.CL"], "comment": "code available at https://github.com/xyliu-cs/RISE", "summary": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners."}
{"id": "2505.12304", "pdf": "https://arxiv.org/pdf/2505.12304", "abs": "https://arxiv.org/abs/2505.12304", "authors": ["Li Ni", "Hengkai Xu", "Lin Mu", "Yiwen Zhang", "Wenjian Luo"], "title": "Pre-trained Prompt-driven Community Search", "categories": ["cs.SI", "cs.AI"], "comment": "11 pages, 3 figures", "summary": "The \"pre-train, prompt\" paradigm is widely adopted in various graph-based\ntasks and has shown promising performance in community detection. Most existing\nsemi-supervised community detection algorithms detect communities based on\nknown ones, and the detected communities typically do not contain the given\nquery node. Therefore, they are not suitable for searching the community of a\ngiven node. Motivated by this, we adopt this paradigm into the semi-supervised\ncommunity search for the first time and propose Pre-trained Prompt-driven\nCommunity Search (PPCS), a novel model designed to enhance search accuracy and\nefficiency. PPCS consists of three main components: node encoding, sample\ngeneration, and prompt-driven fine-tuning. Specifically, the node encoding\ncomponent employs graph neural networks to learn local structural patterns of\nnodes in a graph, thereby obtaining representations for nodes and communities.\nNext, the sample generation component identifies an initial community for a\ngiven node and selects known communities that are structurally similar to the\ninitial one as training samples. Finally, the prompt-driven fine-tuning\ncomponent leverages these samples as prompts to guide the final community\nprediction. Experimental results on five real-world datasets demonstrate that\nPPCS performs better than baseline algorithms. It also achieves higher\ncommunity search efficiency than semi-supervised community search baseline\nmethods, with ablation studies verifying the effectiveness of each component of\nPPCS."}
{"id": "2505.13275", "pdf": "https://arxiv.org/pdf/2505.13275", "abs": "https://arxiv.org/abs/2505.13275", "authors": ["Anthony Zhou", "Amir Barati Farimani"], "title": "Neural Functional: Learning Function to Scalar Maps for Neural PDE Surrogates", "categories": ["cs.LG"], "comment": "19 Pages, 7 Figures. Code and datasets are at\n  http://github.com/anthonyzhou-1/hamiltonian_pdes/tree/main", "summary": "Many architectures for neural PDE surrogates have been proposed in recent\nyears, largely based on neural networks or operator learning. In this work, we\nderive and propose a new architecture, the Neural Functional, which learns\nfunction to scalar mappings. Its implementation leverages insights from\noperator learning and neural fields, and we show the ability of neural\nfunctionals to implicitly learn functional derivatives. For the first time,\nthis allows for an extension of Hamiltonian mechanics to neural PDE surrogates\nby learning the Hamiltonian functional and optimizing its functional\nderivatives. We demonstrate that the Hamiltonian Neural Functional can be an\neffective surrogate model through improved stability and conserving energy-like\nquantities on 1D and 2D PDEs. Beyond PDEs, functionals are prevalent in\nphysics; functional approximation and learning with its gradients may find\nother uses, such as in molecular dynamics or design optimization."}
{"id": "2505.12309", "pdf": "https://arxiv.org/pdf/2505.12309", "abs": "https://arxiv.org/abs/2505.12309", "authors": ["Li Ni", "Hengkai Xu", "Lin Mu", "Yiwen Zhang", "Wenjian Luo"], "title": "Community Search in Time-dependent Road-social Attributed Networks", "categories": ["cs.SI", "cs.AI"], "comment": "12 pages, 7 figures", "summary": "Real-world networks often involve both keywords and locations, along with\ntravel time variations between locations due to traffic conditions. However,\nmost existing cohesive subgraph-based community search studies utilize a single\nattribute, either keywords or locations, to identify communities. They do not\nsimultaneously consider both keywords and locations, which results in low\nsemantic or spatial cohesiveness of the detected communities, and they fail to\naccount for variations in travel time. Additionally, these studies traverse the\nentire network to build efficient indexes, but the detected community only\ninvolves nodes around the query node, leading to the traversal of nodes that\nare not relevant to the community. Therefore, we propose the problem of\ndiscovering semantic-spatial aware k-core, which refers to a k-core with high\nsemantic and time-dependent spatial cohesiveness containing the query node. To\naddress this problem, we propose an exact and a greedy algorithm, both of which\ngradually expand outward from the query node. They are local methods that only\naccess the local part of the attributed network near the query node rather than\nthe entire network. Moreover, we design a method to calculate the semantic\nsimilarity between two keywords using large language models. This method\nalleviates the disadvantages of keyword-matching methods used in existing\ncommunity search studies, such as mismatches caused by differently expressed\nsynonyms and the presence of irrelevant words. Experimental results show that\nthe greedy algorithm outperforms baselines in terms of structural, semantic,\nand time-dependent spatial cohesiveness."}
{"id": "2505.13280", "pdf": "https://arxiv.org/pdf/2505.13280", "abs": "https://arxiv.org/abs/2505.13280", "authors": ["Elias Collaert", "Abel Rodríguez", "Sander Joos", "Lieven Desmet", "Vera Rimmer"], "title": "FlowPure: Continuous Normalizing Flows for Adversarial Purification", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Despite significant advancements in the area, adversarial robustness remains\na critical challenge in systems employing machine learning models. The removal\nof adversarial perturbations at inference time, known as adversarial\npurification, has emerged as a promising defense strategy. To achieve this,\nstate-of-the-art methods leverage diffusion models that inject Gaussian noise\nduring a forward process to dilute adversarial perturbations, followed by a\ndenoising step to restore clean samples before classification. In this work, we\npropose FlowPure, a novel purification method based on Continuous Normalizing\nFlows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings\nfrom adversarial examples to their clean counterparts. Unlike prior\ndiffusion-based approaches that rely on fixed noise processes, FlowPure can\nleverage specific attack knowledge to improve robustness under known threats,\nwhile also supporting a more general stochastic variant trained on Gaussian\nperturbations for settings where such knowledge is unavailable. Experiments on\nCIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art\npurification-based defenses in preprocessor-blind and white-box scenarios, and\ncan do so while fully preserving benign accuracy in the former. Moreover, our\nresults show that not only is FlowPure a highly effective purifier but it also\nholds a strong potential for adversarial detection, identifying\npreprocessor-blind PGD samples with near-perfect accuracy."}
{"id": "2505.12310", "pdf": "https://arxiv.org/pdf/2505.12310", "abs": "https://arxiv.org/abs/2505.12310", "authors": ["Shouyi Lu", "Huanyu Zhou", "Guirong Zhuo"], "title": "DNOI-4DRO: Deep 4D Radar Odometry with Differentiable Neural-Optimization Iterations", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "16 pages,10 figures", "summary": "A novel learning-optimization-combined 4D radar odometry model, named\nDNOI-4DRO, is proposed in this paper. The proposed model seamlessly integrates\ntraditional geometric optimization with end-to-end neural network training,\nleveraging an innovative differentiable neural-optimization iteration operator.\nIn this framework, point-wise motion flow is first estimated using a neural\nnetwork, followed by the construction of a cost function based on the\nrelationship between point motion and pose in 3D space. The radar pose is then\nrefined using Gauss-Newton updates. Additionally, we design a dual-stream 4D\nradar backbone that integrates multi-scale geometric features and\nclustering-based class-aware features to enhance the representation of sparse\n4D radar point clouds. Extensive experiments on the VoD and Snail-Radar\ndatasets demonstrate the superior performance of our model, which outperforms\nrecent classical and learning-based approaches. Notably, our method even\nachieves results comparable to A-LOAM with mapping optimization using LiDAR\npoint clouds as input. Our models and code will be publicly released."}
{"id": "2505.13289", "pdf": "https://arxiv.org/pdf/2505.13289", "abs": "https://arxiv.org/abs/2505.13289", "authors": ["Alonso Urbano", "David W. Romero", "Max Zimmer", "Sebastian Pokutta"], "title": "RECON: Robust symmetry discovery via Explicit Canonical Orientation Normalization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Real-world data often exhibits unknown or approximate symmetries, yet\nexisting equivariant networks must commit to a fixed transformation group prior\nto training, e.g., continuous $SO(2)$ rotations. This mismatch degrades\nperformance when the actual data symmetries differ from those in the\ntransformation group. We introduce RECON, a framework to discover each input's\nintrinsic symmetry distribution from unlabeled data. RECON leverages class-pose\ndecompositions and applies a data-driven normalization to align arbitrary\nreference frames into a common natural pose, yielding directly comparable and\ninterpretable symmetry descriptors. We demonstrate effective symmetry discovery\non 2D image benchmarks and -- for the first time -- extend it to 3D\ntransformation groups, paving the way towards more flexible equivariant\nmodeling."}
{"id": "2505.12312", "pdf": "https://arxiv.org/pdf/2505.12312", "abs": "https://arxiv.org/abs/2505.12312", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Visuospatial Cognitive Assistant", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "31 pages, 10 figures, 6 tables. The implementation and fine-tuned\n  model (ViCA-7B) are publicly available at https://huggingface.co/nkkbr/ViCA.\n  The ViCA-322K dataset can be found at\n  https://huggingface.co/datasets/nkkbr/ViCA-322K, and the ViCA-Thinking-2.68K\n  dataset is at https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k", "summary": "Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence."}
{"id": "2505.13291", "pdf": "https://arxiv.org/pdf/2505.13291", "abs": "https://arxiv.org/abs/2505.13291", "authors": ["Yifu Cai", "Xinyu Li", "Mononito Goswami", "Michał Wiliński", "Gus Welter", "Artur Dubrawski"], "title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents", "categories": ["cs.LG", "cs.AI"], "comment": "Open source code available at\n  https://github.com/moment-timeseries-foundation-model/TimeSeriesGym. YC, XL,\n  MG and MW contributed equally, and should be considered joint first authors", "summary": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating\nArtificial Intelligence (AI) agents on time series machine learning engineering\nchallenges. Existing benchmarks lack scalability, focus narrowly on model\nbuilding in well-defined settings, and evaluate only a limited set of research\nartifacts (e.g., CSV submission files). To make AI agent benchmarking more\nrelevant to the practice of machine learning engineering, our framework scales\nalong two critical dimensions. First, recognizing that effective ML engineering\nrequires a range of diverse skills, TimeSeriesGym incorporates challenges from\ndiverse sources spanning multiple domains and tasks. We design challenges to\nevaluate both isolated capabilities (including data handling, understanding\nresearch repositories, and code translation) and their combinations, and rather\nthan addressing each challenge independently, we develop tools that support\ndesigning multiple challenges at scale. Second, we implement evaluation\nmechanisms for multiple research artifacts, including submission files, code,\nand models, using both precise numeric measures and more flexible LLM-based\nevaluation approaches. This dual strategy balances objective assessment with\ncontextual judgment. Although our initial focus is on time series applications,\nour framework can be readily extended to other data modalities, broadly\nenhancing the comprehensiveness and practical utility of agentic AI evaluation.\nWe open-source our benchmarking framework to facilitate future research on the\nML engineering capabilities of AI agents."}
{"id": "2505.12327", "pdf": "https://arxiv.org/pdf/2505.12327", "abs": "https://arxiv.org/abs/2505.12327", "authors": ["Albert Zhao", "Stefano Soatto"], "title": "Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion Predictions", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "IEEE International Conference on Robotics and Automation (ICRA) 2025", "summary": "We describe a robust planning method for autonomous driving that mixes normal\nand adversarial agent predictions output by a diffusion model trained for\nmotion prediction. We first train a diffusion model to learn an unbiased\ndistribution of normal agent behaviors. We then generate a distribution of\nadversarial predictions by biasing the diffusion model at test time to generate\npredictions that are likely to collide with a candidate plan. We score plans\nusing expected cost with respect to a mixture distribution of normal and\nadversarial predictions, leading to a planner that is robust against\nadversarial behaviors but not overly conservative when agents behave normally.\nUnlike current approaches, we do not use risk measures that over-weight\nadversarial behaviors while placing little to no weight on low-cost normal\nbehaviors or use hard safety constraints that may not be appropriate for all\ndriving scenarios. We show the effectiveness of our method on single-agent and\nmulti-agent jaywalking scenarios as well as a red light violation scenario."}
{"id": "2505.13308", "pdf": "https://arxiv.org/pdf/2505.13308", "abs": "https://arxiv.org/abs/2505.13308", "authors": ["Hengli Li", "Chenxi Li", "Tong Wu", "Xuekai Zhu", "Yuxuan Wang", "Zhaoxin Yu", "Eric Hanchen Jiang", "Song-Chun Zhu", "Zixia Jia", "Ying Nian Wu", "Zilong Zheng"], "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs."}
{"id": "2505.12332", "pdf": "https://arxiv.org/pdf/2505.12332", "abs": "https://arxiv.org/abs/2505.12332", "authors": ["Qianyue Hu", "Junyan Wu", "Wei Lu", "Xiangyang Luo"], "title": "VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "comment": null, "summary": "Diffusion Models (DMs) have achieved remarkable success in realistic voice\ncloning (VC), while they also increase the risk of malicious misuse. Existing\nproactive defenses designed for traditional VC models aim to disrupt the\nforgery process, but they have been proven incompatible with DMs due to the\nintricate generative mechanisms of diffusion. To bridge this gap, we introduce\nVoiceCloak, a multi-dimensional proactive defense framework with the goal of\nobfuscating speaker identity and degrading perceptual quality in potential\nunauthorized VC. To achieve these goals, we conduct a focused analysis to\nidentify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt\nthe cloning process by introducing adversarial perturbations into the reference\naudio. Specifically, to obfuscate speaker identity, VoiceCloak first targets\nspeaker identity by distorting representation learning embeddings to maximize\nidentity variation, which is guided by auditory perception principles.\nAdditionally, VoiceCloak disrupts crucial conditional guidance processes,\nparticularly attention context, thereby preventing the alignment of vocal\ncharacteristics that are essential for achieving convincing cloning. Then, to\naddress the second objective, VoiceCloak introduces score magnitude\namplification to actively steer the reverse trajectory away from the generation\nof high-quality speech. Noise-guided semantic corruption is further employed to\ndisrupt structural speech semantics captured by DMs, degrading output quality.\nExtensive experiments highlight VoiceCloak's outstanding defense success rate\nagainst unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak\nare available at https://voice-cloak.github.io/VoiceCloak/."}
{"id": "2505.13315", "pdf": "https://arxiv.org/pdf/2505.13315", "abs": "https://arxiv.org/abs/2505.13315", "authors": ["Reza T. Batley", "Sourav Saha"], "title": "KHRONOS: a Kernel-Based Neural Architecture for Rapid, Resource-Efficient Scientific Computation", "categories": ["cs.LG", "cs.AI", "cs.MS"], "comment": null, "summary": "Contemporary models of high dimensional physical systems are constrained by\nthe curse of dimensionality and a reliance on dense data. We introduce KHRONOS\n(Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates), an\nAI framework for model based, model free and model inversion tasks. KHRONOS\nconstructs continuously differentiable target fields with a hierarchical\ncomposition of per-dimension kernel expansions, which are tensorized into modes\nand then superposed. We evaluate KHRONOS on a canonical 2D, Poisson equation\nbenchmark: across 16 to 512 degrees of freedom (DoFs), it obtained L2 square\nerrors of 5e-4 down to 6e-10. This represents a 100 time gain over Kolmogorov\nArnold Networks (which itself reports a 100 times improvement on MLPs/PINNs\nwith 100 times fewer parameters) when controlling for the number of parameters.\nThis also represents a 1e4 times improvement in L2 square error compared to\nstandard linear FEM at comparable DoFs. Inference complexity is dominated by\ninner products, yielding sub-millisecond full-field predictions that scale to\nan arbitrary resolution. For inverse problems, KHRONOS facilitates rapid,\niterative level set recovery in only a few forward evaluations, with\nsub-microsecond per sample latency. KHRONOS scalability, expressivity, and\ninterpretability open new avenues in constrained edge computing, online\ncontrol, computer vision, and beyond."}
{"id": "2505.12339", "pdf": "https://arxiv.org/pdf/2505.12339", "abs": "https://arxiv.org/abs/2505.12339", "authors": ["Midou Guo", "Qilin Yin", "Wei Lu", "Xiangyang Luo"], "title": "Towards Open-world Generalized Deepfake Detection: General Feature Extraction via Unsupervised Domain Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the development of generative artificial intelligence, new forgery\nmethods are rapidly emerging. Social platforms are flooded with vast amounts of\nunlabeled synthetic data and authentic data, making it increasingly challenging\nto distinguish real from fake. Due to the lack of labels, existing supervised\ndetection methods struggle to effectively address the detection of unknown\ndeepfake methods. Moreover, in open world scenarios, the amount of unlabeled\ndata greatly exceeds that of labeled data. Therefore, we define a new deepfake\ndetection generalization task which focuses on how to achieve efficient\ndetection of large amounts of unlabeled data based on limited labeled data to\nsimulate a open world scenario. To solve the above mentioned task, we propose a\nnovel Open-World Deepfake Detection Generalization Enhancement Training\nStrategy (OWG-DS) to improve the generalization ability of existing methods.\nOur approach aims to transfer deepfake detection knowledge from a small amount\nof labeled source domain data to large-scale unlabeled target domain data.\nSpecifically, we introduce the Domain Distance Optimization (DDO) module to\nalign different domain features by optimizing both inter-domain and\nintra-domain distances. Additionally, the Similarity-based Class Boundary\nSeparation (SCBS) module is used to enhance the aggregation of similar samples\nto ensure clearer class boundaries, while an adversarial training mechanism is\nadopted to learn the domain-invariant features. Extensive experiments show that\nthe proposed deepfake detection generalization enhancement training strategy\nexcels in cross-method and cross-dataset scenarios, improving the model's\ngeneralization."}
{"id": "2505.13317", "pdf": "https://arxiv.org/pdf/2505.13317", "abs": "https://arxiv.org/abs/2505.13317", "authors": ["Song-Lin Li", "Rui Zhu", "Yu-Feng Li", "Lan-Zhe Guo"], "title": "Unlabeled Data or Pre-trained Model: Rethinking Semi-Supervised Learning and Pretrain-Finetuning", "categories": ["cs.LG"], "comment": null, "summary": "Semi-supervised learning (SSL) alleviates the cost of data labeling process\nby exploiting unlabeled data, and has achieved promising results on various\ntasks such as image classification. Meanwhile, the Pretrain-Finetuning paradigm\nhas garnered significant attention in recent years, and exploiting pre-trained\nmodels could also reduce the requirement of labeled data in downstream tasks.\nTherefore, a question naturally occurs: \\emph{When the labeled data is scarce\nin the target tasks, should we exploit unlabeled data or pre-trained models?}\nTo answer this question, we select pre-trained Vision-Language Models (VLMs) as\nrepresentative pretrain-finetuning instances and propose \\textit{Few-shot SSL}\n-- a framework that enables fair comparison between these two paradigms by\ncontrolling the amount of labeled data used. Extensive experiments across\nvarious settings demonstrate that pre-trained VLMs generally outperform SSL\nmethods in nearly all cases, except when the data has low resolution or lacks\nclear semantic structure. Therefore, we encourage future SSL research to\ncompare with pre-trained models and explore deeper integration, such as using\npre-trained knowledge to enhance pseudo-labeling. To support future research,\nwe release our unified reproduction and evaluation framework. Codes are\navailable at\nhttps://anonymous.4open.science/r/Rethinking-SSL-and-Pretrain-Finetuning-5566"}
{"id": "2505.12343", "pdf": "https://arxiv.org/pdf/2505.12343", "abs": "https://arxiv.org/abs/2505.12343", "authors": ["Kai Tang", "Jinhao You", "Xiuqi Ge", "Hanze Li", "Yichen Guo", "Xiande Huang"], "title": "Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite the impressive capabilities of Large Vision-Language Models (LVLMs),\nthey remain susceptible to hallucinations-generating content that is\ninconsistent with the input image. Existing training-free hallucination\nmitigation methods often suffer from unstable performance and high sensitivity\nto hyperparameter settings, limiting their practicality and broader adoption.\nIn this paper, we propose a novel decoding mechanism, Decoding with Inter-layer\nConsistency via Layer Aggregation (DCLA), which requires no retraining,\nfine-tuning, or access to external knowledge bases. Specifically, our approach\nconstructs a dynamic semantic reference by aggregating representations from\nprevious layers, and corrects semantically deviated layers to enforce\ninter-layer consistency. The method allows DCLA to robustly mitigate\nhallucinations across multiple LVLMs. Experiments on hallucination benchmarks\nsuch as MME and POPE demonstrate that DCLA effectively reduces hallucinations\nwhile enhancing the reliability and performance of LVLMs."}
{"id": "2505.13326", "pdf": "https://arxiv.org/pdf/2505.13326", "abs": "https://arxiv.org/abs/2505.13326", "authors": ["Yuhang Wang", "Youhe Jiang", "Bin Cui", "Fangcheng Fu"], "title": "Thinking Short and Right Over Thinking Long: Serving LLM Reasoning Efficiently and Accurately", "categories": ["cs.LG"], "comment": null, "summary": "Recent advances in test-time scaling suggest that Large Language Models\n(LLMs) can gain better capabilities by generating Chain-of-Thought reasoning\n(analogous to human thinking) to respond a given request, and meanwhile\nexploring more reasoning branches (i.e., generating multiple responses and\nensembling them) can improve the final output quality. However, when\nincorporating the two scaling dimensions, we find that the system efficiency is\ndampened significantly for two reasons. Firstly, the time cost to generate the\nfinal output increases substantially as many reasoning branches would be\ntrapped in the over-thinking dilemma, producing excessively long responses.\nSecondly, generating multiple reasoning branches for each request increases\nmemory consumption, which is unsuitable for LLM serving since we can only batch\na limited number of requests to process simultaneously. To address this, we\npresent SART, a serving framework for efficient and accurate LLM reasoning. The\nessential idea is to manage the thinking to be short and right, rather than\nlong. For one thing, we devise a redundant sampling with early stopping\napproach based on empirical observations and theoretic analysis, which\nincreases the likelihood of obtaining short-thinking responses when sampling\nreasoning branches. For another, we propose to dynamically prune low-quality\nbranches so that only right-thinking branches are maintained, reducing the\nmemory consumption and allowing us to batch more requests. Experimental results\ndemonstrate that SART not only improves the accuracy of LLM reasoning but also\nenhances the serving efficiency, outperforming existing methods by up to 28.2\ntimes and on average 15.7 times in terms of efficiency when achieving the same\nlevel of accuracy."}
{"id": "2505.12349", "pdf": "https://arxiv.org/pdf/2505.12349", "abs": "https://arxiv.org/abs/2505.12349", "authors": ["Axel Abels", "Tom Lenaerts"], "title": "Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted for publication in the Proceedings of the 34th International\n  Joint Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Despite their performance, large language models (LLMs) can inadvertently\nperpetuate biases found in the data they are trained on. By analyzing LLM\nresponses to bias-eliciting headlines, we find that these models often mirror\nhuman biases. To address this, we explore crowd-based strategies for mitigating\nbias through response aggregation. We first demonstrate that simply averaging\nresponses from multiple LLMs, intended to leverage the \"wisdom of the crowd\",\ncan exacerbate existing biases due to the limited diversity within LLM crowds.\nIn contrast, we show that locally weighted aggregation methods more effectively\nleverage the wisdom of the LLM crowd, achieving both bias mitigation and\nimproved accuracy. Finally, recognizing the complementary strengths of LLMs\n(accuracy) and humans (diversity), we demonstrate that hybrid crowds containing\nboth significantly enhance performance and further reduce biases across ethnic\nand gender-related contexts."}
{"id": "2505.13342", "pdf": "https://arxiv.org/pdf/2505.13342", "abs": "https://arxiv.org/abs/2505.13342", "authors": ["Yuval Grinberg", "Nimrod Harel", "Jacob Goldberger", "Ofir Lindenbaum"], "title": "Detect and Correct: A Selective Noise Correction Method for Learning with Noisy Labels", "categories": ["cs.LG"], "comment": null, "summary": "Falsely annotated samples, also known as noisy labels, can significantly harm\nthe performance of deep learning models. Two main approaches for learning with\nnoisy labels are global noise estimation and data filtering. Global noise\nestimation approximates the noise across the entire dataset using a noise\ntransition matrix, but it can unnecessarily adjust correct labels, leaving room\nfor local improvements. Data filtering, on the other hand, discards potentially\nnoisy samples but risks losing valuable data. Our method identifies potentially\nnoisy samples based on their loss distribution. We then apply a selection\nprocess to separate noisy and clean samples and learn a noise transition matrix\nto correct the loss for noisy samples while leaving the clean data unaffected,\nthereby improving the training process. Our approach ensures robust learning\nand enhanced model performance by preserving valuable information from noisy\nsamples and refining the correction process. We applied our method to standard\nimage datasets (MNIST, CIFAR-10, and CIFAR-100) and a biological scRNA-seq\ncell-type annotation dataset. We observed a significant improvement in model\naccuracy and robustness compared to traditional methods."}
{"id": "2505.12350", "pdf": "https://arxiv.org/pdf/2505.12350", "abs": "https://arxiv.org/abs/2505.12350", "authors": ["Georgiy Malaniya", "Anton Bolychev", "Grigory Yaremenko", "Anastasia Krasnaya", "Pavel Osinenko"], "title": "Multi-CALF: A Policy Combination Approach with Statistical Guarantees", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "We introduce Multi-CALF, an algorithm that intelligently combines\nreinforcement learning policies based on their relative value improvements. Our\napproach integrates a standard RL policy with a theoretically-backed\nalternative policy, inheriting formal stability guarantees while often\nachieving better performance than either policy individually. We prove that our\ncombined policy converges to a specified goal set with known probability and\nprovide precise bounds on maximum deviation and convergence time. Empirical\nvalidation on control tasks demonstrates enhanced performance while maintaining\nstability guarantees."}
{"id": "2505.13343", "pdf": "https://arxiv.org/pdf/2505.13343", "abs": "https://arxiv.org/abs/2505.13343", "authors": ["Andrej Čop", "Blaž Bertalanič", "Marko Grobelnik", "Carolina Fortuna"], "title": "MRM3: Machine Readable ML Model Metadata", "categories": ["cs.LG"], "comment": null, "summary": "As the complexity and number of machine learning (ML) models grows,\nwell-documented ML models are essential for developers and companies to use or\nadapt them to their specific use cases. Model metadata, already present in\nunstructured format as model cards in online repositories such as Hugging Face,\ncould be more structured and machine readable while also incorporating\nenvironmental impact metrics such as energy consumption and carbon footprint.\nOur work extends the existing State of the Art by defining a structured schema\nfor ML model metadata focusing on machine-readable format and support for\nintegration into a knowledge graph (KG) for better organization and querying,\nenabling a wider set of use cases. Furthermore, we present an example wireless\nlocalization model metadata dataset consisting of 22 models trained on 4\ndatasets, integrated into a Neo4j-based KG with 113 nodes and 199 relations."}
{"id": "2505.12353", "pdf": "https://arxiv.org/pdf/2505.12353", "abs": "https://arxiv.org/abs/2505.12353", "authors": ["Prakash Palanivelu Rajmohan", "Fred Roosta"], "title": "Importance Sampling for Nonlinear Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "This work is accepted at ICML 2025", "summary": "While norm-based and leverage-score-based methods have been extensively\nstudied for identifying \"important\" data points in linear models, analogous\ntools for nonlinear models remain significantly underdeveloped. By introducing\nthe concept of the adjoint operator of a nonlinear map, we address this gap and\ngeneralize norm-based and leverage-score-based importance sampling to nonlinear\nsettings. We demonstrate that sampling based on these generalized notions of\nnorm and leverage scores provides approximation guarantees for the underlying\nnonlinear mapping, similar to linear subspace embeddings. As direct\napplications, these nonlinear scores not only reduce the computational\ncomplexity of training nonlinear models by enabling efficient sampling over\nlarge datasets but also offer a novel mechanism for model explainability and\noutlier detection. Our contributions are supported by both theoretical analyses\nand experimental results across a variety of supervised learning scenarios."}
{"id": "2505.13345", "pdf": "https://arxiv.org/pdf/2505.13345", "abs": "https://arxiv.org/abs/2505.13345", "authors": ["Shuqing Luo", "Pingzhi Li", "Jie Peng", "Hanrui Wang", "Yang", "Zhao", "Yu", "Cao", "Yu Cheng", "Tianlong Chen"], "title": "Occult: Optimizing Collaborative Communication across Experts for Accelerated Parallel MoE Training and Inference", "categories": ["cs.LG", "cs.DC"], "comment": "Accepted by ICML2025", "summary": "Mixture-of-experts (MoE) architectures could achieve impressive computational\nefficiency with expert parallelism, which relies heavily on all-to-all\ncommunication across devices. Unfortunately, such communication overhead\ntypically constitutes a significant portion of the total runtime, hampering the\nscalability of distributed training and inference for modern MoE models\n(consuming over $40\\%$ runtime in large-scale training). In this paper, we\nfirst define collaborative communication to illustrate this intrinsic\nlimitation, and then propose system- and algorithm-level innovations to reduce\ncommunication costs. Specifically, given a pair of experts co-activated by one\ntoken, we call them \"collaborated\", which comprises $2$ cases as intra- and\ninter-collaboration, depending on whether they are kept on the same device. Our\npilot investigations reveal that augmenting the proportion of\nintra-collaboration can accelerate expert parallelism at scale. It motivates us\nto strategically optimize collaborative communication for accelerated MoE\ntraining and inference, dubbed Occult. Our designs are capable of either\ndelivering exact results with reduced communication cost or controllably\nminimizing the cost with collaboration pruning, materialized by modified\nfine-tuning. Comprehensive experiments on various MoE-LLMs demonstrate that\nOccult can be faster than popular state-of-the-art inference or training\nframeworks (more than $1.5\\times$ speed up across multiple tasks and models)\nwith comparable or superior quality compared to the standard fine-tuning. Code\nis available at\n$\\href{https://github.com/UNITES-Lab/Occult}{https://github.com/UNITES-Lab/Occult}$."}
{"id": "2505.12354", "pdf": "https://arxiv.org/pdf/2505.12354", "abs": "https://arxiv.org/abs/2505.12354", "authors": ["Anton Bolychev", "Georgiy Malaniya", "Grigory Yaremenko", "Anastasia Krasnaya", "Pavel Osinenko"], "title": "A universal policy wrapper with guarantees", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "We introduce a universal policy wrapper for reinforcement learning agents\nthat ensures formal goal-reaching guarantees. In contrast to standard\nreinforcement learning algorithms that excel in performance but lack rigorous\nsafety assurances, our wrapper selectively switches between a high-performing\nbase policy -- derived from any existing RL method -- and a fallback policy\nwith known convergence properties. Base policy's value function supervises this\nswitching process, determining when the fallback policy should override the\nbase policy to ensure the system remains on a stable path. The analysis proves\nthat our wrapper inherits the fallback policy's goal-reaching guarantees while\npreserving or improving upon the performance of the base policy. Notably, it\noperates without needing additional system knowledge or online constrained\noptimization, making it readily deployable across diverse reinforcement\nlearning architectures and tasks."}
{"id": "2505.13358", "pdf": "https://arxiv.org/pdf/2505.13358", "abs": "https://arxiv.org/abs/2505.13358", "authors": ["Nimrod Berman", "Ilan Naiman", "Moshe Eliasof", "Hedi Zisling", "Omri Azencot"], "title": "One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diffusion-based generative models have demonstrated exceptional performance,\nyet their iterative sampling procedures remain computationally expensive. A\nprominent strategy to mitigate this cost is distillation, with offline\ndistillation offering particular advantages in terms of efficiency, modularity,\nand flexibility. In this work, we identify two key observations that motivate a\nprincipled distillation framework: (1) while diffusion models have been viewed\nthrough the lens of dynamical systems theory, powerful and underexplored tools\ncan be further leveraged; and (2) diffusion models inherently impose\nstructured, semantically coherent trajectories in latent space. Building on\nthese observations, we introduce the Koopman Distillation Model KDM, a novel\noffline distillation approach grounded in Koopman theory-a classical framework\nfor representing nonlinear dynamics linearly in a transformed space. KDM\nencodes noisy inputs into an embedded space where a learned linear operator\npropagates them forward, followed by a decoder that reconstructs clean samples.\nThis enables single-step generation while preserving semantic fidelity. We\nprovide theoretical justification for our approach: (1) under mild assumptions,\nthe learned diffusion dynamics admit a finite-dimensional Koopman\nrepresentation; and (2) proximity in the Koopman latent space correlates with\nsemantic similarity in the generated outputs, allowing for effective trajectory\nalignment. Empirically, KDM achieves state-of-the-art performance across\nstandard offline distillation benchmarks, improving FID scores by up to 40% in\na single generation step. All implementation details and code for the\nexperimental setups are provided in our GitHub -\nhttps://github.com/azencot-group/KDM, or in our project page -\nhttps://sites.google.com/view/koopman-distillation-model."}
{"id": "2505.12358", "pdf": "https://arxiv.org/pdf/2505.12358", "abs": "https://arxiv.org/abs/2505.12358", "authors": ["Abrar Rahman Abir", "Haz Sameen Shahgir", "Md Rownok Zahan Ratul", "Md Toki Tahmid", "Greg Ver Steeg", "Yue Dong"], "title": "AbFlowNet: Optimizing Antibody-Antigen Binding Energy via Diffusion-GFlowNet Fusion", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Complementarity Determining Regions (CDRs) are critical segments of an\nantibody that facilitate binding to specific antigens. Current computational\nmethods for CDR design utilize reconstruction losses and do not jointly\noptimize binding energy, a crucial metric for antibody efficacy. Rather,\nbinding energy optimization is done through computationally expensive Online\nReinforcement Learning (RL) pipelines rely heavily on unreliable binding energy\nestimators. In this paper, we propose AbFlowNet, a novel generative framework\nthat integrates GFlowNet with Diffusion models. By framing each diffusion step\nas a state in the GFlowNet framework, AbFlowNet jointly optimizes standard\ndiffusion losses and binding energy by directly incorporating energy signals\ninto the training process, thereby unifying diffusion and reward optimization\nin a single procedure. Experimental results show that AbFlowNet outperforms the\nbase diffusion model by 3.06% in amino acid recovery, 20.40% in geometric\nreconstruction (RMSD), and 3.60% in binding energy improvement ratio. ABFlowNet\nalso decreases Top-1 total energy and binding energy errors by 24.8% and 38.1%\nwithout pseudo-labeling the test dataset or using computationally expensive\nonline RL regimes."}
{"id": "2505.13377", "pdf": "https://arxiv.org/pdf/2505.13377", "abs": "https://arxiv.org/abs/2505.13377", "authors": ["Yasi Zhang", "Tianyu Chen", "Zhendong Wang", "Ying Nian Wu", "Mingyuan Zhou", "Oscar Leong"], "title": "Restoration Score Distillation: From Corrupted Diffusion Pretraining to One-Step High-Quality Generation", "categories": ["cs.LG"], "comment": null, "summary": "Learning generative models from corrupted data is a fundamental yet\npersistently challenging task across scientific disciplines, particularly when\naccess to clean data is limited or expensive. Denoising Score Distillation\n(DSD) \\cite{chen2025denoising} recently introduced a novel and surprisingly\neffective strategy that leverages score distillation to train high-fidelity\ngenerative models directly from noisy observations. Building upon this\nfoundation, we propose \\textit{Restoration Score Distillation} (RSD), a\nprincipled generalization of DSD that accommodates a broader range of\ncorruption types, such as blurred, incomplete, or low-resolution images. RSD\noperates by first pretraining a teacher diffusion model solely on corrupted\ndata and subsequently distilling it into a single-step generator that produces\nhigh-quality reconstructions. Empirically, RSD consistently surpasses its\nteacher model across diverse restoration tasks on both natural and scientific\ndatasets. Moreover, beyond standard diffusion objectives, the RSD framework is\ncompatible with several corruption-aware training techniques such as Ambient\nTweedie, Ambient Diffusion, and its Fourier-space variant, enabling flexible\nintegration with recent advances in diffusion modeling. Theoretically, we\ndemonstrate that in a linear regime, RSD recovers the eigenspace of the clean\ndata covariance matrix from linear measurements, thereby serving as an implicit\nregularizer. This interpretation recasts score distillation not only as a\nsampling acceleration technique but as a principled approach to enhancing\ngenerative performance in severely degraded data regimes."}
{"id": "2505.12361", "pdf": "https://arxiv.org/pdf/2505.12361", "abs": "https://arxiv.org/abs/2505.12361", "authors": ["Elizaveta Pestova", "Ilya Osokin", "Danil Belov", "Pavel Osinenko"], "title": "Adaptive MPC-based quadrupedal robot control under periodic disturbances", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Recent advancements in adaptive control for reference trajectory tracking\nenable quadrupedal robots to perform locomotion tasks under challenging\nconditions. There are methods enabling the estimation of the external\ndisturbances in terms of forces and torques. However, a specific case of\ndisturbances that are periodic was not explicitly tackled in application to\nquadrupeds. This work is devoted to the estimation of the periodic disturbances\nwith a lightweight regressor using simplified robot dynamics and extracting the\ndisturbance properties in terms of the magnitude and frequency. Experimental\nevidence suggests performance improvement over the baseline static disturbance\ncompensation. All source files, including simulation setups, code, and\ncalculation scripts, are available on GitHub at\nhttps://github.com/aidagroup/quad-periodic-mpc."}
{"id": "2505.13397", "pdf": "https://arxiv.org/pdf/2505.13397", "abs": "https://arxiv.org/abs/2505.13397", "authors": ["Benoit Dherin", "Michael Munn", "Hanna Mazzawi", "Michael Wunder", "Sourabh Medapati", "Javier Gonzalvo"], "title": "Learning by solving differential equations", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML"], "comment": null, "summary": "Modern deep learning algorithms use variations of gradient descent as their\nmain learning methods. Gradient descent can be understood as the simplest\nOrdinary Differential Equation (ODE) solver; namely, the Euler method applied\nto the gradient flow differential equation. Since Euler, many ODE solvers have\nbeen devised that follow the gradient flow equation more precisely and more\nstably. Runge-Kutta (RK) methods provide a family of very powerful explicit and\nimplicit high-order ODE solvers. However, these higher-order solvers have not\nfound wide application in deep learning so far. In this work, we evaluate the\nperformance of higher-order RK solvers when applied in deep learning, study\ntheir limitations, and propose ways to overcome these drawbacks. In particular,\nwe explore how to improve their performance by naturally incorporating key\ningredients of modern neural network optimizers such as preconditioning,\nadaptive learning rates, and momentum."}
{"id": "2505.12363", "pdf": "https://arxiv.org/pdf/2505.12363", "abs": "https://arxiv.org/abs/2505.12363", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "26 pages, 19 figures, 4 tables. Code, models, and dataset are\n  available at our project page: https://github.com/nkkbr/ViCA", "summary": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research."}
{"id": "2505.13398", "pdf": "https://arxiv.org/pdf/2505.13398", "abs": "https://arxiv.org/abs/2505.13398", "authors": ["Matan Abudy", "Orr Well", "Emmanuel Chemla", "Roni Katzir", "Nur Lan"], "title": "A Minimum Description Length Approach to Regularization in Neural Networks", "categories": ["cs.LG", "cs.CL"], "comment": "9 pages", "summary": "State-of-the-art neural networks can be trained to become remarkable\nsolutions to many problems. But while these architectures can express symbolic,\nperfect solutions, trained models often arrive at approximations instead. We\nshow that the choice of regularization method plays a crucial role: when\ntrained on formal languages with standard regularization ($L_1$, $L_2$, or\nnone), expressive architectures not only fail to converge to correct solutions\nbut are actively pushed away from perfect initializations. In contrast,\napplying the Minimum Description Length (MDL) principle to balance model\ncomplexity with data fit provides a theoretically grounded regularization\nmethod. Using MDL, perfect solutions are selected over approximations,\nindependently of the optimization algorithm. We propose that unlike existing\nregularization techniques, MDL introduces the appropriate inductive bias to\neffectively counteract overfitting and promote generalization."}
{"id": "2505.12366", "pdf": "https://arxiv.org/pdf/2505.12366", "abs": "https://arxiv.org/abs/2505.12366", "authors": ["Gang Li", "Ming Lin", "Tomer Galanti", "Zhengzhong Tu", "Tianbao Yang"], "title": "DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "20 pages, 4 figures", "summary": "The recent success and openness of DeepSeek-R1 have brought widespread\nattention to Group Relative Policy Optimization (GRPO) as a reinforcement\nlearning method for large reasoning models (LRMs). In this work, we analyze the\nGRPO objective under a binary reward setting and reveal an inherent limitation\nof question-level difficulty bias. We also identify a connection between GRPO\nand traditional discriminative methods in supervised learning. Motivated by\nthese insights, we introduce a new Discriminative Constrained Optimization\n(DisCO) framework for reinforcing LRMs, grounded in the principle of\ndiscriminative learning. The main differences between DisCO and GRPO and its\nrecent variants are: (1) it replaces the group relative objective with a\ndiscriminative objective defined by a scoring function; (2) it abandons\nclipping-based surrogates in favor of non-clipping RL surrogate objectives used\nas scoring functions; (3) it employs a simple yet effective constrained\noptimization approach to enforce the KL divergence constraint, ensuring stable\ntraining. As a result, DisCO offers notable advantages over GRPO and its\nvariants: (i) it completely eliminates difficulty bias by adopting\ndiscriminative objectives; (ii) it addresses the entropy instability in GRPO\nand its variants through the use of non-clipping scoring functions and a\nconstrained optimization approach; (iii) it allows the incorporation of\nadvanced discriminative learning techniques to address data imbalance, where a\nsignificant number of questions have more negative than positive generated\nanswers during training. Our experiments on enhancing the mathematical\nreasoning capabilities of SFT-finetuned models show that DisCO significantly\noutperforms GRPO and its improved variants such as DAPO, achieving average\ngains of 7\\% over GRPO and 6\\% over DAPO across six benchmark tasks for an 1.5B\nmodel."}
{"id": "2505.13405", "pdf": "https://arxiv.org/pdf/2505.13405", "abs": "https://arxiv.org/abs/2505.13405", "authors": ["Gabriel Malikal", "Ismail Alkhouri", "Alvaro Velasquez", "Adam M Alessio", "Saiprasad Ravishankar"], "title": "A Dataless Reinforcement Learning Approach to Rounding Hyperplane Optimization for Max-Cut", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "The Maximum Cut (MaxCut) problem is NP-Complete, and obtaining its optimal\nsolution is NP-hard in the worst case. As a result, heuristic-based algorithms\nare commonly used, though their design often requires significant domain\nexpertise. More recently, learning-based methods trained on large (un)labeled\ndatasets have been proposed; however, these approaches often struggle with\ngeneralizability and scalability. A well-known approximation algorithm for\nMaxCut is the Goemans-Williamson (GW) algorithm, which relaxes the Quadratic\nUnconstrained Binary Optimization (QUBO) formulation into a semidefinite\nprogram (SDP). The GW algorithm then applies hyperplane rounding by uniformly\nsampling a random hyperplane to convert the SDP solution into binary node\nassignments. In this paper, we propose a training-data-free approach based on a\nnon-episodic reinforcement learning formulation, in which an agent learns to\nselect improved rounding hyperplanes that yield better cuts than those produced\nby the GW algorithm. By optimizing over a Markov Decision Process (MDP), our\nmethod consistently achieves better cuts across large-scale graphs with varying\ndensities and degree distributions."}
{"id": "2505.12368", "pdf": "https://arxiv.org/pdf/2505.12368", "abs": "https://arxiv.org/abs/2505.12368", "authors": ["Gauri Kholkar", "Ratinder Ahuja"], "title": "CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in ACL LLMSec Workshop 2025", "summary": "Prompt injection remains a major security risk for large language models.\nHowever, the efficacy of existing guardrail models in context-aware settings\nremains underexplored, as they often rely on static attack benchmarks.\nAdditionally, they have over-defense tendencies. We introduce CAPTURE, a novel\ncontext-aware benchmark assessing both attack detection and over-defense\ntendencies with minimal in-domain examples. Our experiments reveal that current\nprompt injection guardrail models suffer from high false negatives in\nadversarial cases and excessive false positives in benign scenarios,\nhighlighting critical limitations."}
{"id": "2505.13413", "pdf": "https://arxiv.org/pdf/2505.13413", "abs": "https://arxiv.org/abs/2505.13413", "authors": ["Dongyi Wang", "Yuanwei Jiang", "Zhenyi Zhang", "Xiang Gu", "Peijie Zhou", "Jian Sun"], "title": "Joint Velocity-Growth Flow Matching for Single-Cell Dynamics Modeling", "categories": ["cs.LG"], "comment": null, "summary": "Learning the underlying dynamics of single cells from snapshot data has\ngained increasing attention in scientific and machine learning research. The\ndestructive measurement technique and cell proliferation/death result in\nunpaired and unbalanced data between snapshots, making the learning of the\nunderlying dynamics challenging. In this paper, we propose joint\nVelocity-Growth Flow Matching (VGFM), a novel paradigm that jointly learns\nstate transition and mass growth of single-cell populations via flow matching.\nVGFM builds an ideal single-cell dynamics containing velocity of state and\ngrowth of mass, driven by a presented two-period dynamic understanding of the\nstatic semi-relaxed optimal transport, a mathematical tool that seeks the\ncoupling between unpaired and unbalanced data. To enable practical usage, we\napproximate the ideal dynamics using neural networks, forming our joint\nvelocity and growth matching framework. A distribution fitting loss is also\nemployed in VGFM to further improve the fitting performance for snapshot data.\nExtensive experimental results on both synthetic and real datasets demonstrate\nthat VGFM can capture the underlying biological dynamics accounting for mass\nand state variations over time, outperforming existing approaches for\nsingle-cell dynamics modeling."}
{"id": "2505.12381", "pdf": "https://arxiv.org/pdf/2505.12381", "abs": "https://arxiv.org/abs/2505.12381", "authors": ["Mohsinul Kabir", "Tasfia Tahsin", "Sophia Ananiadou"], "title": "From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Current research on bias in language models (LMs) predominantly focuses on\ndata quality, with significantly less attention paid to model architecture and\ntemporal influences of data. Even more critically, few studies systematically\ninvestigate the origins of bias. We propose a methodology grounded in\ncomparative behavioral theory to interpret the complex interaction between\ntraining data and model architecture in bias propagation during language\nmodeling. Building on recent work that relates transformers to n-gram LMs, we\nevaluate how data, model design choices, and temporal dynamics affect bias\npropagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to\ncontext window size in bias propagation, while transformers demonstrate\narchitectural robustness; (2) the temporal provenance of training data\nsignificantly affects bias; and (3) different model architectures respond\ndifferentially to controlled bias injection, with certain biases (e.g. sexual\norientation) being disproportionately amplified. As language models become\nubiquitous, our findings highlight the need for a holistic approach -- tracing\nbias to its origins across both data and model dimensions, not just symptoms,\nto mitigate harm."}
{"id": "2505.13416", "pdf": "https://arxiv.org/pdf/2505.13416", "abs": "https://arxiv.org/abs/2505.13416", "authors": ["Artem Riabinin", "Egor Shulgin", "Kaja Gruntkowska", "Peter Richtárik"], "title": "Gluon: Making Muon & Scion Great Again! (Bridging Theory and Practice of LMO-based Optimizers for LLMs)", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "Recent developments in deep learning optimization have brought about\nradically new algorithms based on the Linear Minimization Oracle (LMO)\nframework, such as $\\sf Muon$ and $\\sf Scion$. After over a decade of $\\sf\nAdam$'s dominance, these LMO-based methods are emerging as viable replacements,\noffering several practical advantages such as improved memory efficiency,\nbetter hyperparameter transferability, and most importantly, superior empirical\nperformance on large-scale tasks, including LLM training. However, a\nsignificant gap remains between their practical use and our current theoretical\nunderstanding: prior analyses (1) overlook the layer-wise LMO application of\nthese optimizers in practice, and (2) rely on an unrealistic smoothness\nassumption, leading to impractically small stepsizes. To address both, we\npropose a new LMO-based method called $\\sf Gluon$, capturing prior\ntheoretically analyzed methods as special cases, and introduce a new refined\ngeneralized smoothness model that captures the layer-wise geometry of neural\nnetworks, matches the layer-wise practical implementation of $\\sf Muon$ and\n$\\sf Scion$, and leads to convergence guarantees with strong practical\npredictive power. Unlike prior results, our theoretical stepsizes closely match\nthe fine-tuned values reported by Pethick et al. (2025). Our experiments with\nNanoGPT and CNN confirm that our assumption holds along the optimization\ntrajectory, ultimately closing the gap between theory and practice."}
{"id": "2505.12386", "pdf": "https://arxiv.org/pdf/2505.12386", "abs": "https://arxiv.org/abs/2505.12386", "authors": ["Boaz Taitler", "Omer Madmon", "Moshe Tennenholtz", "Omer Ben-Porat"], "title": "Data Sharing with a Generative AI Competitor", "categories": ["cs.GT", "cs.AI"], "comment": null, "summary": "As GenAI platforms grow, their dependence on content from competing\nproviders, combined with access to alternative data sources, creates new\nchallenges for data-sharing decisions. In this paper, we provide a model of\ndata sharing between a content creation firm and a GenAI platform that can also\nacquire content from third-party experts. The interaction is modeled as a\nStackelberg game: the firm first decides how much of its proprietary dataset to\nshare with GenAI, and GenAI subsequently determines how much additional data to\nacquire from external experts. Their utilities depend on user traffic, monetary\ntransfers, and the cost of acquiring additional data from external experts. We\ncharacterize the unique subgame perfect equilibrium of the game and uncover a\nsurprising phenomenon: The firm may be willing to pay GenAI to share the firm's\nown data, leading to a costly data-sharing equilibrium. We further characterize\nthe set of Pareto improving data prices, and show that such improvements occur\nonly when the firm pays to share data. Finally, we study how the price can be\nset to optimize different design objectives, such as promoting firm data\nsharing, expert data acquisition, or a balance of both. Our results shed light\non the economic forces shaping data-sharing partnerships in the age of GenAI,\nand provide guidance for platforms, regulators and policymakers seeking to\ndesign effective data exchange mechanisms."}
{"id": "2505.13421", "pdf": "https://arxiv.org/pdf/2505.13421", "abs": "https://arxiv.org/abs/2505.13421", "authors": ["Si-Yang Liu", "Qile Zhou", "Han-Jia Ye"], "title": "Make Still Further Progress: Chain of Thoughts for Tabular Data Leaderboard", "categories": ["cs.LG"], "comment": null, "summary": "Tabular data, a fundamental data format in machine learning, is predominantly\nutilized in competitions and real-world applications. The performance of\ntabular models--such as gradient boosted decision trees and neural\nnetworks--can vary significantly across datasets due to differences in feature\ndistributions and task characteristics. Achieving top performance on each\ndataset often requires specialized expert knowledge. To address this\nvariability, practitioners often aggregate the predictions of multiple models.\nHowever, conventional aggregation strategies typically rely on static\ncombination rules and lack instance-level adaptability. In this work, we\npropose an in-context ensemble framework for tabular prediction that leverages\nlarge language models (LLMs) to perform dynamic, instance-specific integration\nof external model predictions. Without access to raw tabular features or\nsemantic information, our method constructs a context around each test instance\nusing its nearest neighbors and the predictions from a pool of external models.\nWithin this enriched context, we introduce Chain of Tabular Thoughts (CoT$^2$),\na prompting strategy that guides LLMs through multi-step, interpretable\nreasoning, making still further progress toward expert-level decision-making.\nExperimental results show that our method outperforms well-tuned baselines and\nstandard ensemble techniques across a wide range of tabular datasets."}
{"id": "2505.12395", "pdf": "https://arxiv.org/pdf/2505.12395", "abs": "https://arxiv.org/abs/2505.12395", "authors": ["Udaya Shreyas", "L. N. Aadarsh"], "title": "Few-Shot Concept Unlearning with Low Rank Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Image Generation models are a trending topic nowadays, with many people\nutilizing Artificial Intelligence models in order to generate images. There are\nmany such models which, given a prompt of a text, will generate an image which\ndepicts said prompt. There are many image generation models, such as Latent\nDiffusion Models, Denoising Diffusion Probabilistic Models, Generative\nAdversarial Networks and many more. When generating images, these models can\ngenerate sensitive image data, which can be threatening to privacy or may\nviolate copyright laws of private entities. Machine unlearning aims at removing\nthe influence of specific data subsets from the trained models and in the case\nof image generation models, remove the influence of a concept such that the\nmodel is unable to generate said images of the concept when prompted.\nConventional retraining of the model can take upto days, hence fast algorithms\nare the need of the hour. In this paper we propose an algorithm that aims to\nremove the influence of concepts in diffusion models through updating the\ngradients of the final layers of the text encoders. Using a weighted loss\nfunction, we utilize backpropagation in order to update the weights of the\nfinal layers of the Text Encoder componet of the Stable Diffusion Model,\nremoving influence of the concept from the text-image embedding space, such\nthat when prompted, the result is an image not containing the concept. The\nweighted loss function makes use of Textual Inversion and Low-Rank\nAdaptation.We perform our experiments on Latent Diffusion Models, namely the\nStable Diffusion v2 model, with an average concept unlearning runtime of 50\nseconds using 4-5 images."}
{"id": "2505.13425", "pdf": "https://arxiv.org/pdf/2505.13425", "abs": "https://arxiv.org/abs/2505.13425", "authors": ["Zhi-Hao Tan", "Zi-Chen Zhao", "Hao-Yu Shi", "Xin-Yu Zhang", "Peng Tan", "Yang Yu", "Zhi-Hua Zhou"], "title": "Learnware of Language Models: Specialized Small Language Models Can Do Big", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The learnware paradigm offers a novel approach to machine learning by\nenabling users to reuse a set of well-trained models for tasks beyond the\nmodels' original purposes. It eliminates the need to build models from scratch,\ninstead relying on specifications (representations of a model's capabilities)\nto identify and leverage the most suitable models for new tasks. While\nlearnware has proven effective in many scenarios, its application to language\nmodels has remained largely unexplored. At the same time, large language models\n(LLMs) have demonstrated remarkable universal question-answering abilities, yet\nthey face challenges in specialized scenarios due to data scarcity, privacy\nconcerns, and high computational costs, thus more and more specialized small\nlanguage models (SLMs) are being trained for specific domains. To address these\nlimitations systematically, the learnware paradigm provides a promising\nsolution by enabling maximum utilization of specialized SLMs, and allowing\nusers to identify and reuse them in a collaborative and privacy-preserving\nmanner.\n  This paper presents a preliminary attempt to apply the learnware paradigm to\nlanguage models. We simulated a learnware system comprising approximately 100\nlearnwares of specialized SLMs with 8B parameters, fine-tuned across finance,\nhealthcare, and mathematics domains. Each learnware contains an SLM and a\nspecification, which enables users to identify the most relevant models without\nexposing their own data. Experimental results demonstrate promising\nperformance: by selecting one suitable learnware for each task-specific\ninference, the system outperforms the base SLMs on all benchmarks. Compared to\nLLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and\nLlama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses\nFlan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical\ndomain tasks."}
{"id": "2505.12398", "pdf": "https://arxiv.org/pdf/2505.12398", "abs": "https://arxiv.org/abs/2505.12398", "authors": ["Yepeng Weng", "Qiao Hu", "Xujie Chen", "Li Liu", "Dianwen Mei", "Huishi Qiu", "Jiang Tian", "Zhongchao Shi"], "title": "Traversal Verification for Speculative Tree Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Speculative decoding is a promising approach for accelerating large language\nmodels. The primary idea is to use a lightweight draft model to speculate the\noutput of the target model for multiple subsequent timesteps, and then verify\nthem in parallel to determine whether the drafted tokens should be accepted or\nrejected. To enhance acceptance rates, existing frameworks typically construct\ntoken trees containing multiple candidates in each timestep. However, their\nreliance on token-level verification mechanisms introduces two critical\nlimitations: First, the probability distribution of a sequence differs from\nthat of individual tokens, leading to suboptimal acceptance length. Second,\ncurrent verification schemes begin from the root node and proceed layer by\nlayer in a top-down manner. Once a parent node is rejected, all its child nodes\nshould be discarded, resulting in inefficient utilization of speculative\ncandidates. This paper introduces Traversal Verification, a novel speculative\ndecoding algorithm that fundamentally rethinks the verification paradigm\nthrough leaf-to-root traversal. Our approach considers the acceptance of the\nentire token sequence from the current node to the root, and preserves\npotentially valid subsequences that would be prematurely discarded by existing\nmethods. We theoretically prove that the probability distribution obtained\nthrough Traversal Verification is identical to that of the target model,\nguaranteeing lossless inference while achieving substantial acceleration gains.\nExperimental results across different large language models and multiple tasks\nshow that our method consistently improves acceptance length and throughput\nover existing methods"}
{"id": "2505.13430", "pdf": "https://arxiv.org/pdf/2505.13430", "abs": "https://arxiv.org/abs/2505.13430", "authors": ["Sifeng Shang", "Jiayi Zhou", "Chenyu Lin", "Minxian Li", "Kaiyang Zhou"], "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU."}
{"id": "2505.12404", "pdf": "https://arxiv.org/pdf/2505.12404", "abs": "https://arxiv.org/abs/2505.12404", "authors": ["Piotr Piękos", "Subhradeep Kayal", "Alexandros Karatzoglou"], "title": "Hyperbolic Residual Quantization: Discrete Representations for Data with Latent Hierarchies", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Hierarchical data arise in countless domains, from biological taxonomies and\norganizational charts to legal codes and knowledge graphs. Residual\nQuantization (RQ) is widely used to generate discrete, multitoken\nrepresentations for such data by iteratively quantizing residuals in a\nmultilevel codebook. However, its reliance on Euclidean geometry can introduce\nfundamental mismatches that hinder modeling of hierarchical branching,\nnecessary for faithful representation of hierarchical data. In this work, we\npropose Hyperbolic Residual Quantization (HRQ), which embeds data natively in a\nhyperbolic manifold and performs residual quantization using hyperbolic\noperations and distance metrics. By adapting the embedding network, residual\ncomputation, and distance metric to hyperbolic geometry, HRQ imparts an\ninductive bias that aligns naturally with hierarchical branching. We claim that\nHRQ in comparison to RQ can generate more useful for downstream tasks discrete\nhierarchical representations for data with latent hierarchies. We evaluate HRQ\non two tasks: supervised hierarchy modeling using WordNet hypernym trees, where\nthe model is supervised to learn the latent hierarchy - and hierarchy\ndiscovery, where, while latent hierarchy exists in the data, the model is not\ndirectly trained or evaluated on a task related to the hierarchy. Across both\nscenarios, HRQ hierarchical tokens yield better performance on downstream tasks\ncompared to Euclidean RQ with gains of up to $20\\%$ for the hierarchy modeling\ntask. Our results demonstrate that integrating hyperbolic geometry into\ndiscrete representation learning substantially enhances the ability to capture\nlatent hierarchies."}
{"id": "2505.13432", "pdf": "https://arxiv.org/pdf/2505.13432", "abs": "https://arxiv.org/abs/2505.13432", "authors": ["Meshi Bashari", "Roy Maor Lotan", "Yonghoon Lee", "Edgar Dobriban", "Yaniv Romano"], "title": "Synthetic-Powered Predictive Inference", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Conformal prediction is a framework for predictive inference with a\ndistribution-free, finite-sample guarantee. However, it tends to provide\nuninformative prediction sets when calibration data are scarce. This paper\nintroduces Synthetic-powered predictive inference (SPPI), a novel framework\nthat incorporates synthetic data -- e.g., from a generative model -- to improve\nsample efficiency. At the core of our method is a score transporter: an\nempirical quantile mapping that aligns nonconformity scores from trusted, real\ndata with those from synthetic data. By carefully integrating the score\ntransporter into the calibration process, SPPI provably achieves finite-sample\ncoverage guarantees without making any assumptions about the real and synthetic\ndata distributions. When the score distributions are well aligned, SPPI yields\nsubstantially tighter and more informative prediction sets than standard\nconformal prediction. Experiments on image classification and tabular\nregression demonstrate notable improvements in predictive efficiency in\ndata-scarce settings."}
{"id": "2505.12405", "pdf": "https://arxiv.org/pdf/2505.12405", "abs": "https://arxiv.org/abs/2505.12405", "authors": ["Konstantinos Xylogiannopoulos", "Petros Xanthopoulos", "Panagiotis Karampelas", "Georgios Bakamitsos"], "title": "The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative AI paraphrased text can be used for copyright infringement and the\nAI paraphrased content can deprive substantial revenue from original content\ncreators. Despite this recent surge of malicious use of generative AI, there\nare few academic publications that research this threat. In this article, we\ndemonstrate the ability of pattern-based similarity detection for AI\nparaphrased news recognition. We propose an algorithmic scheme, which is not\nlimited to detect whether an article is an AI paraphrase, but, more\nimportantly, to identify that the source of infringement is the ChatGPT. The\nproposed method is tested with a benchmark dataset specifically created for\nthis task that incorporates real articles from BBC, incorporating a total of\n2,224 articles across five different news categories, as well as 2,224\nparaphrased articles created with ChatGPT. Results show that our pattern\nsimilarity-based method, that makes no use of deep learning, can detect ChatGPT\nassisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for\nprecision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1\nscore."}
{"id": "2505.13438", "pdf": "https://arxiv.org/pdf/2505.13438", "abs": "https://arxiv.org/abs/2505.13438", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency."}
{"id": "2505.12408", "pdf": "https://arxiv.org/pdf/2505.12408", "abs": "https://arxiv.org/abs/2505.12408", "authors": ["Minxu Liu", "Donghai Guan", "Chuhang Zheng", "Chunwei Tian", "Jie Wen", "Qi Zhu"], "title": "ViEEG: Hierarchical Neural Coding with Cross-Modal Progressive Enhancement for EEG-Based Visual Decoding", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "24 pages, 18 figures", "summary": "Understanding and decoding brain activity into visual representations is a\nfundamental challenge at the intersection of neuroscience and artificial\nintelligence. While EEG-based visual decoding has shown promise due to its\nnon-invasive, low-cost nature and millisecond-level temporal resolution,\nexisting methods are limited by their reliance on flat neural representations\nthat overlook the brain's inherent visual hierarchy. In this paper, we\nintroduce ViEEG, a biologically inspired hierarchical EEG decoding framework\nthat aligns with the Hubel-Wiesel theory of visual processing. ViEEG decomposes\neach visual stimulus into three biologically aligned components-contour,\nforeground object, and contextual scene-serving as anchors for a three-stream\nEEG encoder. These EEG features are progressively integrated via\ncross-attention routing, simulating cortical information flow from V1 to IT to\nthe association cortex. We further adopt hierarchical contrastive learning to\nalign EEG representations with CLIP embeddings, enabling zero-shot object\nrecognition. Extensive experiments on the THINGS-EEG dataset demonstrate that\nViEEG achieves state-of-the-art performance, with 40.9% Top-1 accuracy in\nsubject-dependent and 22.9% Top-1 accuracy in cross-subject settings,\nsurpassing existing methods by over 45%. Our framework not only advances the\nperformance frontier but also sets a new paradigm for biologically grounded\nbrain decoding in AI."}
{"id": "2505.13446", "pdf": "https://arxiv.org/pdf/2505.13446", "abs": "https://arxiv.org/abs/2505.13446", "authors": ["Dulhan Jayalath", "Gilad Landau", "Oiwi Parker Jones"], "title": "Unlocking Non-Invasive Brain-to-Text", "categories": ["cs.LG"], "comment": "27 pages, 10 figures, 10 tables. Under review", "summary": "Despite major advances in surgical brain-to-text (B2T), i.e. transcribing\nspeech from invasive brain recordings, non-invasive alternatives have yet to\nsurpass even chance on standard metrics. This remains a barrier to building a\nnon-invasive brain-computer interface (BCI) capable of restoring communication\nin paralysed individuals without surgery. Here, we present the first\nnon-invasive B2T result that significantly exceeds these critical baselines,\nraising BLEU by $1.4\\mathrm{-}2.6\\times$ over prior work. This result is driven\nby three contributions: (1) we extend recent word-classification models with\nLLM-based rescoring, transforming single-word predictors into closed-vocabulary\nB2T systems; (2) we introduce a predictive in-filling approach to handle\nout-of-vocabulary (OOV) words, substantially expanding the effective\nvocabulary; and (3) we demonstrate, for the first time, how to scale\nnon-invasive B2T models across datasets, unlocking deep learning at scale and\nimproving accuracy by $2.1\\mathrm{-}2.3\\times$. Through these contributions, we\noffer new insights into the roles of data quality and vocabulary size.\nTogether, our results remove a major obstacle to realising practical\nnon-invasive B2T systems."}
{"id": "2505.12415", "pdf": "https://arxiv.org/pdf/2505.12415", "abs": "https://arxiv.org/abs/2505.12415", "authors": ["Zhenhe Wu", "Jian Yang", "Jiaheng Liu", "Xianjie Wu", "Changzai Pan", "Jie Zhang", "Yu Zhao", "Shuangyong Song", "Yongxiang Li", "Zhoujun Li"], "title": "Table-R1: Region-based Reinforcement Learning for Table Understanding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning."}
{"id": "2505.13447", "pdf": "https://arxiv.org/pdf/2505.13447", "abs": "https://arxiv.org/abs/2505.13447", "authors": ["Zhengyang Geng", "Mingyang Deng", "Xingjian Bai", "J. Zico Kolter", "Kaiming He"], "title": "Mean Flows for One-step Generative Modeling", "categories": ["cs.LG", "cs.CV"], "comment": "Tech report", "summary": "We propose a principled and effective framework for one-step generative\nmodeling. We introduce the notion of average velocity to characterize flow\nfields, in contrast to instantaneous velocity modeled by Flow Matching methods.\nA well-defined identity between average and instantaneous velocities is derived\nand used to guide neural network training. Our method, termed the MeanFlow\nmodel, is self-contained and requires no pre-training, distillation, or\ncurriculum learning. MeanFlow demonstrates strong empirical performance: it\nachieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet\n256x256 trained from scratch, significantly outperforming previous\nstate-of-the-art one-step diffusion/flow models. Our study substantially\nnarrows the gap between one-step diffusion/flow models and their multi-step\npredecessors, and we hope it will motivate future research to revisit the\nfoundations of these powerful models."}
{"id": "2505.12418", "pdf": "https://arxiv.org/pdf/2505.12418", "abs": "https://arxiv.org/abs/2505.12418", "authors": ["Yuanpeng He", "Yali Bi", "Lijian Li", "Chi-Man Pun", "Wenpin Jiao", "Zhi Jin"], "title": "Mutual Evidential Deep Learning for Medical Image Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Existing semi-supervised medical segmentation co-learning frameworks have\nrealized that model performance can be diminished by the biases in model\nrecognition caused by low-quality pseudo-labels. Due to the averaging nature of\ntheir pseudo-label integration strategy, they fail to explore the reliability\nof pseudo-labels from different sources. In this paper, we propose a mutual\nevidential deep learning (MEDL) framework that offers a potentially viable\nsolution for pseudo-label generation in semi-supervised learning from two\nperspectives. First, we introduce networks with different architectures to\ngenerate complementary evidence for unlabeled samples and adopt an improved\nclass-aware evidential fusion to guide the confident synthesis of evidential\npredictions sourced from diverse architectural networks. Second, utilizing the\nuncertainty in the fused evidence, we design an asymptotic Fisher\ninformation-based evidential learning strategy. This strategy enables the model\nto initially focus on unlabeled samples with more reliable pseudo-labels,\ngradually shifting attention to samples with lower-quality pseudo-labels while\navoiding over-penalization of mislabeled classes in high data uncertainty\nsamples. Additionally, for labeled data, we continue to adopt an\nuncertainty-driven asymptotic learning strategy, gradually guiding the model to\nfocus on challenging voxels. Extensive experiments on five mainstream datasets\nhave demonstrated that MEDL achieves state-of-the-art performance."}
{"id": "2309.12825", "pdf": "https://arxiv.org/pdf/2309.12825", "abs": "https://arxiv.org/abs/2309.12825", "authors": ["Botian Xu", "Feng Gao", "Chao Yu", "Ruize Zhang", "Yi Wu", "Yu Wang"], "title": "OmniDrones: An Efficient and Flexible Platform for Reinforcement Learning in Drone Control", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Submitted to IEEE RA-L", "summary": "In this work, we introduce OmniDrones, an efficient and flexible platform\ntailored for reinforcement learning in drone control, built on Nvidia's\nOmniverse Isaac Sim. It employs a bottom-up design approach that allows users\nto easily design and experiment with various application scenarios on top of\nGPU-parallelized simulations. It also offers a range of benchmark tasks,\npresenting challenges ranging from single-drone hovering to over-actuated\nsystem tracking. In summary, we propose an open-sourced drone simulation\nplatform, equipped with an extensive suite of tools for drone learning. It\nincludes 4 drone models, 5 sensor modalities, 4 control modes, over 10\nbenchmark tasks, and a selection of widely used RL baselines. To showcase the\ncapabilities of OmniDrones and to support future research, we also provide\npreliminary results on these benchmark tasks. We hope this platform will\nencourage further studies on applying RL to practical drone systems."}
{"id": "2505.12421", "pdf": "https://arxiv.org/pdf/2505.12421", "abs": "https://arxiv.org/abs/2505.12421", "authors": ["Emanuele La Malfa", "Jon Vadillo", "Marco Molinari", "Michael Wooldridge"], "title": "Fixed Point Explainability", "categories": ["cs.LG", "cs.AI"], "comment": "Code: https://github.com/EmanueleLM/fixed-point-explainability", "summary": "This paper introduces a formal notion of fixed point explanations, inspired\nby the \"why regress\" principle, to assess, through recursive applications, the\nstability of the interplay between a model and its explainer. Fixed point\nexplanations satisfy properties like minimality, stability, and faithfulness,\nrevealing hidden model behaviours and explanatory weaknesses. We define\nconvergence conditions for several classes of explainers, from feature-based to\nmechanistic tools like Sparse AutoEncoders, and we report quantitative and\nqualitative results."}
{"id": "2505.11528", "pdf": "https://arxiv.org/pdf/2505.11528", "abs": "https://arxiv.org/abs/2505.11528", "authors": ["Yuhang Huang", "JIazhao Zhang", "Shilong Zou", "XInwang Liu", "Ruizhen Hu", "Kai Xu"], "title": "LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Predictive manipulation has recently gained considerable attention in the\nEmbodied AI community due to its potential to improve robot policy performance\nby leveraging predicted states. However, generating accurate future visual\nstates of robot-object interactions from world models remains a well-known\nchallenge, particularly in achieving high-quality pixel-level representations.\nTo this end, we propose LaDi-WM, a world model that predicts the latent space\nof future states using diffusion modeling. Specifically, LaDi-WM leverages the\nwell-established latent space aligned with pre-trained Visual Foundation Models\n(VFMs), which comprises both geometric features (DINO-based) and semantic\nfeatures (CLIP-based). We find that predicting the evolution of the latent\nspace is easier to learn and more generalizable than directly predicting\npixel-level images. Building on LaDi-WM, we design a diffusion policy that\niteratively refines output actions by incorporating forecasted states, thereby\ngenerating more consistent and accurate results. Extensive experiments on both\nsynthetic and real-world benchmarks demonstrate that LaDi-WM significantly\nenhances policy performance by 27.9\\% on the LIBERO-LONG benchmark and 20\\% on\nthe real-world scenario. Furthermore, our world model and policies achieve\nimpressive generalizability in real-world experiments."}
{"id": "2505.12423", "pdf": "https://arxiv.org/pdf/2505.12423", "abs": "https://arxiv.org/abs/2505.12423", "authors": ["Wenqiao Zhu", "Chao Xu", "Lulu Wang", "Jun Wu"], "title": "PSC: Extending Context Window of Large Language Models via Phase Shift Calibration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Rotary Position Embedding (RoPE) is an efficient position encoding approach\nand is widely utilized in numerous large language models (LLMs). Recently, a\nlot of methods have been put forward to further expand the context window based\non RoPE. The core concept of those methods is to predefine or search for a set\nof factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a\nchallenge for existing methods to predefine an optimal factor due to the\nexponential search space. In view of this, we introduce PSC (Phase Shift\nCalibration), a small module for calibrating the frequencies predefined by\nexisting methods. With the employment of PSC, we demonstrate that many existing\nmethods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted\nextensive experiments across multiple models and tasks. The results demonstrate\nthat (1) when PSC is enabled, the comparative reductions in perplexity increase\nas the context window size is varied from 16k, to 32k, and up to 64k. (2) Our\napproach is broadly applicable and exhibits robustness across a variety of\nmodels and tasks. The code can be found at https://github.com/WNQzhu/PSC."}
{"id": "2505.11529", "pdf": "https://arxiv.org/pdf/2505.11529", "abs": "https://arxiv.org/abs/2505.11529", "authors": ["Dan Luo", "Jinyu Zhou", "Le Xu", "Sisi Yuan", "Xuan Lin"], "title": "DynamicDTA: Drug-Target Binding Affinity Prediction Using Dynamic Descriptors and Graph Representation", "categories": ["cs.RO", "cs.LG"], "comment": "Accepted for publication at Interdisciplinary Sciences: Computational\n  Life Sciences, 2025", "summary": "Predicting drug-target binding affinity (DTA) is essential for identifying\npotential therapeutic candidates in drug discovery. However, most existing\nmodels rely heavily on static protein structures, often overlooking the dynamic\nnature of proteins, which is crucial for capturing conformational flexibility\nthat will be beneficial for protein binding interactions. We introduce\nDynamicDTA, an innovative deep learning framework that incorporates static and\ndynamic protein features to enhance DTA prediction. The proposed DynamicDTA\ntakes three types of inputs, including drug sequence, protein sequence, and\ndynamic descriptors. A molecular graph representation of the drug sequence is\ngenerated and subsequently processed through graph convolutional network, while\nthe protein sequence is encoded using dilated convolutions. Dynamic\ndescriptors, such as root mean square fluctuation, are processed through a\nmulti-layer perceptron. These embedding features are fused with static protein\nfeatures using cross-attention, and a tensor fusion network integrates all\nthree modalities for DTA prediction. Extensive experiments on three datasets\ndemonstrate that DynamicDTA achieves by at least 3.4% improvement in RMSE score\nwith comparison to seven state-of-the-art baseline methods. Additionally,\npredicting novel drugs for Human Immunodeficiency Virus Type 1 and visualizing\nthe docking complexes further demonstrates the reliability and biological\nrelevance of DynamicDTA."}
{"id": "2505.12424", "pdf": "https://arxiv.org/pdf/2505.12424", "abs": "https://arxiv.org/abs/2505.12424", "authors": ["Lior Broide", "Roni Stern"], "title": "EvoGPT: Enhancing Test Suite Robustness via LLM-Based Generation and Genetic Optimization", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have recently emerged as promising tools for\nautomated unit test generation. We introduce a hybrid framework called EvoGPT\nthat integrates LLM-based test generation with evolutionary search techniques\nto create diverse, fault-revealing unit tests. Unit tests are initially\ngenerated with diverse temperature sampling to maximize behavioral and test\nsuite diversity, followed by a generation-repair loop and coverage-guided\nassertion enhancement. The resulting test suites are evolved using genetic\nalgorithms, guided by a fitness function prioritizing mutation score over\ntraditional coverage metrics. This design emphasizes the primary objective of\nunit testing-fault detection. Evaluated on multiple open-source Java projects,\nEvoGPT achieves an average improvement of 10% in both code coverage and\nmutation score compared to LLMs and traditional search-based software testing\nbaselines. These results demonstrate that combining LLM-driven diversity,\ntargeted repair, and evolutionary optimization produces more effective and\nresilient test suites."}
{"id": "2505.11534", "pdf": "https://arxiv.org/pdf/2505.11534", "abs": "https://arxiv.org/abs/2505.11534", "authors": ["Yuhang Wang", "Abdulaziz Alhuraish", "Shuyi Wang", "Hao Zhou"], "title": "Empirical Performance Evaluation of Lane Keeping Assist on Modern Production Vehicles", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Leveraging a newly released open dataset of Lane Keeping Assist (LKA) systems\nfrom production vehicles, this paper presents the first comprehensive empirical\nanalysis of real-world LKA performance. Our study yields three key findings:\n(i) LKA failures can be systematically categorized into perception, planning,\nand control errors. We present representative examples of each failure mode\nthrough in-depth analysis of LKA-related CAN signals, enabling both\njustification of the failure mechanisms and diagnosis of when and where each\nmodule begins to degrade; (ii) LKA systems tend to follow a fixed\nlane-centering strategy, often resulting in outward drift that increases\nlinearly with road curvature, whereas human drivers proactively steer slightly\ninward on similar curved segments; (iii) We provide the first statistical\nsummary and distribution analysis of environmental and road conditions under\nLKA failures, identifying with statistical significance that faded lane\nmarkings, low pavement laneline contrast, and sharp curvature are the most\ndominant individual factors, along with critical combinations that\nsubstantially increase failure likelihood. Building on these insights, we\npropose a theoretical model that integrates road geometry, speed limits, and\nLKA steering capability to inform infrastructure design. Additionally, we\ndevelop a machine learning-based model to assess roadway readiness for LKA\ndeployment, offering practical tools for safer infrastructure planning,\nespecially in rural areas. This work highlights key limitations of current LKA\nsystems and supports the advancement of safer and more reliable autonomous\ndriving technologies."}
{"id": "2505.12432", "pdf": "https://arxiv.org/pdf/2505.12432", "abs": "https://arxiv.org/abs/2505.12432", "authors": ["Zirun Guo", "Minjie Hong", "Tao Jin"], "title": "Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Reinforcement Learning (RL) has shown promise in improving the reasoning\nabilities of Large Language Models (LLMs). However, the specific challenges of\nadapting RL to multimodal data and formats remain relatively unexplored. In\nthis work, we present Observe-R1, a novel framework aimed at enhancing the\nreasoning capabilities of multimodal large language models (MLLMs). We draw\ninspirations from human learning progression--from simple to complex and easy\nto difficult, and propose a gradual learning paradigm for MLLMs. To this end,\nwe construct the NeuraLadder dataset, which is organized and sampled according\nto the difficulty and complexity of data samples for RL training. To tackle\nmultimodal tasks, we introduce a multimodal format constraint that encourages\ncareful observation of images, resulting in enhanced visual abilities and\nclearer and more structured responses. Additionally, we implement a bonus\nreward system that favors concise, correct answers within a length constraint,\nalongside a dynamic weighting mechanism that prioritizes uncertain and\nmedium-difficulty problems, ensuring that more informative samples have a\ngreater impact on training. Our experiments with the Qwen2.5-VL-3B and\nQwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that\nObserve-R1 outperforms a series of larger reasoning models on both reasoning\nand general benchmarks, achieving superior clarity and conciseness in reasoning\nchains. Ablation studies validate the effectiveness of our strategies,\nhighlighting the robustness and generalization of our approach. The dataset and\ncode will be released at https://github.com/zrguo/Observe-R1."}
{"id": "2505.11535", "pdf": "https://arxiv.org/pdf/2505.11535", "abs": "https://arxiv.org/abs/2505.11535", "authors": ["Yuhang Wang", "Hao Zhou"], "title": "Bridging Human Oversight and Black-box Driver Assistance: Vision-Language Models for Predictive Alerting in Lane Keeping Assist Systems", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Lane Keeping Assist systems, while increasingly prevalent, often suffer from\nunpredictable real-world failures, largely due to their opaque, black-box\nnature, which limits driver anticipation and trust. To bridge the gap between\nautomated assistance and effective human oversight, we present LKAlert, a novel\nsupervisory alert system that leverages VLM to forecast potential LKA risk 1-3\nseconds in advance. LKAlert processes dash-cam video and CAN data, integrating\nsurrogate lane segmentation features from a parallel interpretable model as\nautomated guiding attention. Unlike traditional binary classifiers, LKAlert\nissues both predictive alert and concise natural language explanation,\nenhancing driver situational awareness and trust. To support the development\nand evaluation of such systems, we introduce OpenLKA-Alert, the first benchmark\ndataset designed for predictive and explainable LKA failure warnings. It\ncontains synchronized multimodal inputs and human-authored justifications\nacross annotated temporal windows. We further contribute a generalizable\nmethodological framework for VLM-based black-box behavior prediction, combining\nsurrogate feature guidance with LoRA. This framework enables VLM to reason over\nstructured visual context without altering its vision backbone, making it\nbroadly applicable to other complex, opaque systems requiring interpretable\noversight. Empirical results correctly predicts upcoming LKA failures with\n69.8% accuracy and a 58.6\\% F1-score. The system also generates high-quality\ntextual explanations for drivers (71.7 ROUGE-L) and operates efficiently at\napproximately 2 Hz, confirming its suitability for real-time, in-vehicle use.\nOur findings establish LKAlert as a practical solution for enhancing the safety\nand usability of current ADAS and offer a scalable paradigm for applying VLMs\nto human-centered supervision of black-box automation."}
{"id": "2505.12433", "pdf": "https://arxiv.org/pdf/2505.12433", "abs": "https://arxiv.org/abs/2505.12433", "authors": ["Haodong Yang", "Lei Wang", "Md Zakir Hossain"], "title": "SRLoRA: Subspace Recomposition in Low-Rank Adaptation via Importance-Based Fusion and Reinitialization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Research report", "summary": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient\nfine-tuning (PEFT) method that injects two trainable low-rank matrices (A and\nB) into frozen pretrained models. While efficient, LoRA constrains updates to a\nfixed low-rank subspace (Delta W = BA), which can limit representational\ncapacity and hinder downstream performance. We introduce Subspace Recomposition\nin Low-Rank Adaptation (SRLoRA) via importance-based fusion and\nreinitialization, a novel approach that enhances LoRA's expressiveness without\ncompromising its lightweight structure. SRLoRA assigns importance scores to\neach LoRA pair (a column of B and the corresponding row of A), and dynamically\nrecomposes the subspace during training. Less important pairs are fused into\nthe frozen backbone, freeing capacity to reinitialize new pairs along unused\nprincipal directions derived from the pretrained weight's singular value\ndecomposition. This mechanism enables continual subspace refreshment and richer\nadaptation over time, without increasing the number of trainable parameters. We\nevaluate SRLoRA on both language and vision tasks, including the GLUE benchmark\nand various image classification datasets. SRLoRA consistently achieves faster\nconvergence and improved accuracy over standard LoRA, demonstrating its\ngenerality, efficiency, and potential for broader PEFT applications."}
{"id": "2505.11542", "pdf": "https://arxiv.org/pdf/2505.11542", "abs": "https://arxiv.org/abs/2505.11542", "authors": ["Jose Fuentes", "Ines Ortega-Fernandez", "Nora M. Villanueva", "Marta Sestelo"], "title": "Cybersecurity threat detection based on a UEBA framework using Deep Autoencoders", "categories": ["cs.CR", "cs.LG", "stat.CO", "stat.ML"], "comment": null, "summary": "User and Entity Behaviour Analytics (UEBA) is a broad branch of data\nanalytics that attempts to build a normal behavioural profile in order to\ndetect anomalous events. Among the techniques used to detect anomalies, Deep\nAutoencoders constitute one of the most promising deep learning models on UEBA\ntasks, allowing explainable detection of security incidents that could lead to\nthe leak of personal data, hijacking of systems, or access to sensitive\nbusiness information. In this study, we introduce the first implementation of\nan explainable UEBA-based anomaly detection framework that leverages Deep\nAutoencoders in combination with Doc2Vec to process both numerical and textual\nfeatures. Additionally, based on the theoretical foundations of neural\nnetworks, we offer a novel proof demonstrating the equivalence of two widely\nused definitions for fully-connected neural networks. The experimental results\ndemonstrate the proposed framework capability to detect real and synthetic\nanomalies effectively generated from real attack data, showing that the models\nprovide not only correct identification of anomalies but also explainable\nresults that enable the reconstruction of the possible origin of the anomaly.\nOur findings suggest that the proposed UEBA framework can be seamlessly\nintegrated into enterprise environments, complementing existing security\nsystems for explainable threat detection."}
{"id": "2505.12435", "pdf": "https://arxiv.org/pdf/2505.12435", "abs": "https://arxiv.org/abs/2505.12435", "authors": ["Wenqiao Zhu", "Ji Liu", "Lulu Wang", "Jun Wu", "Yulun Zhang"], "title": "SGDPO: Self-Guided Direct Preference Optimization for Language Model Alignment", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "18 pages, to appear in ACL'25", "summary": "Direct Preference Optimization (DPO) is broadly utilized for aligning Large\nLanguage Models (LLMs) with human values because of its flexibility. Despite\nits effectiveness, it has been observed that the capability of DPO to generate\nhuman-preferred response is limited and the results of DPO are far from\nresilient. To address these limitations, in this paper we propose a novel\nSelf-Guided Direct Preference Optimization algorithm, i.e., SGDPO, which\nincorporates a pilot term to steer the gradient flow during the optimization\nprocess, allowing for fine-grained control over the updates of chosen and\nrejected rewards. We provide a detailed theoretical analysis of our proposed\nmethod and elucidate its operational mechanism. Furthermore, we conduct\ncomprehensive experiments on various models and benchmarks. The extensive\nexperimental results demonstrate the consistency between the empirical results\nand our theoretical analysis and confirm the effectiveness of our proposed\napproach (up to 9.19% higher score)."}
{"id": "2505.11551", "pdf": "https://arxiv.org/pdf/2505.11551", "abs": "https://arxiv.org/abs/2505.11551", "authors": ["Muzun Althunayyan", "Amir Javed", "Omer Rana"], "title": "A Survey of Learning-Based Intrusion Detection Systems for In-Vehicle Network", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Connected and Autonomous Vehicles (CAVs) enhance mobility but face\ncybersecurity threats, particularly through the insecure Controller Area\nNetwork (CAN) bus. Cyberattacks can have devastating consequences in connected\nvehicles, including the loss of control over critical systems, necessitating\nrobust security solutions. In-vehicle Intrusion Detection Systems (IDSs) offer\na promising approach by detecting malicious activities in real time. This\nsurvey provides a comprehensive review of state-of-the-art research on\nlearning-based in-vehicle IDSs, focusing on Machine Learning (ML), Deep\nLearning (DL), and Federated Learning (FL) approaches. Based on the reviewed\nstudies, we critically examine existing IDS approaches, categorising them by\nthe types of attacks they detect - known, unknown, and combined known-unknown\nattacks - while identifying their limitations. We also review the evaluation\nmetrics used in research, emphasising the need to consider multiple criteria to\nmeet the requirements of safety-critical systems. Additionally, we analyse\nFL-based IDSs and highlight their limitations. By doing so, this survey helps\nidentify effective security measures, address existing limitations, and guide\nfuture research toward more resilient and adaptive protection mechanisms,\nensuring the safety and reliability of CAVs."}
{"id": "2505.12437", "pdf": "https://arxiv.org/pdf/2505.12437", "abs": "https://arxiv.org/abs/2505.12437", "authors": ["Michele Fontanesi", "Alessio Micheli", "Marco Podda", "Domenico Tortorella"], "title": "Addressing the Scarcity of Benchmarks for Graph XAI", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While Graph Neural Networks (GNNs) have become the de facto model for\nlearning from structured data, their decisional process remains opaque to the\nend user, undermining their deployment in safety-critical applications. In the\ncase of graph classification, Explainable Artificial Intelligence (XAI)\ntechniques address this major issue by identifying sub-graph motifs that\nexplain predictions. However, advancements in this field are hindered by a\nchronic scarcity of benchmark datasets with known ground-truth motifs to assess\nthe explanations' quality. Current graph XAI benchmarks are limited to\nsynthetic data or a handful of real-world tasks hand-curated by domain experts.\nIn this paper, we propose a general method to automate the construction of XAI\nbenchmarks for graph classification from real-world datasets. We provide both\n15 ready-made benchmarks, as well as the code to generate more than 2000\nadditional XAI benchmarks with our method. As a use case, we employ our\nbenchmarks to assess the effectiveness of some popular graph explainers."}
{"id": "2505.11568", "pdf": "https://arxiv.org/pdf/2505.11568", "abs": "https://arxiv.org/abs/2505.11568", "authors": ["Stylianos Stasinos", "Martino Mensio", "Elena Lazovik", "Athanasios Trantas"], "title": "BioCube: A Multimodal Dataset for Biodiversity Research", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": "submitted to BiDS'25, 5 pages, 1 figure", "summary": "Biodiversity research requires complete and detailed information to study\necosystem dynamics at different scales. Employing data-driven methods like\nMachine Learning is getting traction in ecology and more specific biodiversity,\noffering alternative modelling pathways. For these methods to deliver accurate\nresults there is the need for large, curated and multimodal datasets that offer\ngranular spatial and temporal resolutions. In this work, we introduce BioCube,\na multimodal, fine-grained global dataset for ecology and biodiversity\nresearch. BioCube incorporates species observations through images, audio\nrecordings and descriptions, environmental DNA, vegetation indices,\nagricultural, forest, land indicators, and high-resolution climate variables.\nAll observations are geospatially aligned under the WGS84 geodetic system,\nspanning from 2000 to 2020. The dataset will become available at\nhttps://huggingface.co/datasets/BioDT/BioCube while the acquisition and\nprocessing code base at https://github.com/BioDT/bfm-data."}
{"id": "2505.12442", "pdf": "https://arxiv.org/pdf/2505.12442", "abs": "https://arxiv.org/abs/2505.12442", "authors": ["Liwen Wang", "Wenxuan Wang", "Shuai Wang", "Zongjie Li", "Zhenlan Ji", "Zongyi Lyu", "Daoyuan Wu", "Shing-Chi Cheung"], "title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses."}
{"id": "2505.11579", "pdf": "https://arxiv.org/pdf/2505.11579", "abs": "https://arxiv.org/abs/2505.11579", "authors": ["Zeynep Engin", "David Hand"], "title": "Toward Adaptive Categories: Dimensional Governance for Agentic AI", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.LG", "cs.MA"], "comment": "12 pages core text, 14 pages including references, 2 figures", "summary": "As AI systems evolve from static tools to dynamic agents, traditional\ncategorical governance frameworks -- based on fixed risk tiers, levels of\nautonomy, or human oversight models -- are increasingly insufficient on their\nown. Systems built on foundation models, self-supervised learning, and\nmulti-agent architectures increasingly blur the boundaries that categories were\ndesigned to police. In this Perspective, we make the case for dimensional\ngovernance: a framework that tracks how decision authority, process autonomy,\nand accountability (the 3As) distribute dynamically across human-AI\nrelationships. A critical advantage of this approach is its ability to\nexplicitly monitor system movement toward and across key governance thresholds,\nenabling preemptive adjustments before risks materialize. This dimensional\napproach provides the necessary foundation for more adaptive categorization,\nenabling thresholds and classifications that can evolve with emerging\ncapabilities. While categories remain essential for decision-making, building\nthem upon dimensional foundations allows for context-specific adaptability and\nstakeholder-responsive governance that static approaches cannot achieve. We\noutline key dimensions, critical trust thresholds, and practical examples\nillustrating where rigid categorical frameworks fail -- and where a dimensional\nmindset could offer a more resilient and future-proof path forward for both\ngovernance and innovation at the frontier of artificial intelligence."}
{"id": "2505.12467", "pdf": "https://arxiv.org/pdf/2505.12467", "abs": "https://arxiv.org/abs/2505.12467", "authors": ["Haochun Wang", "Sendong Zhao", "Jingbo Wang", "Zewen Qiang", "Bing Qin", "Ting Liu"], "title": "Beyond Frameworks: Unpacking Collaboration Strategies in Multi-Agent Systems", "categories": ["cs.MA", "cs.AI"], "comment": "ACL 2025", "summary": "Multi-agent collaboration has emerged as a pivotal paradigm for addressing\ncomplex, distributed tasks in large language model (LLM)-driven applications.\nWhile prior research has focused on high-level architectural frameworks, the\ngranular mechanisms governing agents, critical to performance and scalability,\nremain underexplored. This study systematically investigates four dimensions of\ncollaboration strategies: (1) agent governance, (2) participation control, (3)\ninteraction dynamics, and (4) dialogue history management. Through rigorous\nexperimentation under two context-dependent scenarios: Distributed Evidence\nIntegration (DEI) and Structured Evidence Synthesis (SES), we quantify the\nimpact of these strategies on both task accuracy and computational efficiency.\nOur findings reveal that centralized governance, instructor-led participation,\nordered interaction patterns, and instructor-curated context summarization\ncollectively optimize the trade-off between decision quality and resource\nutilization with the support of the proposed Token-Accuracy Ratio (TAR). This\nwork establishes a foundation for designing adaptive, scalable multi-agent\nsystems, shifting the focus from structural novelty to strategic interaction\nmechanics."}
{"id": "2505.11581", "pdf": "https://arxiv.org/pdf/2505.11581", "abs": "https://arxiv.org/abs/2505.11581", "authors": ["Akarsh Kumar", "Jeff Clune", "Joel Lehman", "Kenneth O. Stanley"], "title": "Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis", "categories": ["cs.CV", "cs.LG", "cs.NE"], "comment": "43 pages, 25 figures", "summary": "Much of the excitement in modern AI is driven by the observation that scaling\nup existing systems leads to better performance. But does better performance\nnecessarily imply better internal representations? While the representational\noptimist assumes it must, this position paper challenges that view. We compare\nneural networks evolved through an open-ended search process to networks\ntrained via conventional stochastic gradient descent (SGD) on the simple task\nof generating a single image. This minimal setup offers a unique advantage:\neach hidden neuron's full functional behavior can be easily visualized as an\nimage, thus revealing how the network's output behavior is internally\nconstructed neuron by neuron. The result is striking: while both networks\nproduce the same output behavior, their internal representations differ\ndramatically. The SGD-trained networks exhibit a form of disorganization that\nwe term fractured entangled representation (FER). Interestingly, the evolved\nnetworks largely lack FER, even approaching a unified factored representation\n(UFR). In large models, FER may be degrading core model capacities like\ngeneralization, creativity, and (continual) learning. Therefore, understanding\nand mitigating FER could be critical to the future of representation learning."}
{"id": "2505.12476", "pdf": "https://arxiv.org/pdf/2505.12476", "abs": "https://arxiv.org/abs/2505.12476", "authors": ["Xiao Long", "Liansheng Zhuang", "Chen Shen", "Shaotian Yan", "Yifei Li", "Shafei Wang"], "title": "Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated impressive\nperformance in Knowledge Graph Question Answering (KGQA) tasks, which aim to\nfind answers based on knowledge graphs (KGs) for natural language questions.\nExisting LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented\nGeneration (GraphRAG) paradigm, which first retrieves reasoning paths from the\nlarge KGs, and then generates the answers based on them. However, these methods\nemphasize the exploration of new optimal reasoning paths in KGs while ignoring\nthe exploitation of historical reasoning paths, which may lead to sub-optimal\nreasoning paths. Additionally, the complex semantics contained in questions may\nlead to the retrieval of inaccurate reasoning paths. To address these issues,\nthis paper proposes a novel and training-free framework for KGQA tasks called\nReward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original\nquestion into a series of simpler and well-defined sub-questions to handle the\ncomplex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided\nby a reward model is introduced to iteratively retrieve weighted reasoning\npaths as contextual knowledge. Finally, it stacks the weighted reasoning paths\naccording to their weights to generate the final answers. Extensive experiments\non four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves\n8.7\\% and 7.0\\% performance improvement over the state-of-the-art method on the\nGrailQA and the WebQSP respectively."}
{"id": "2505.11610", "pdf": "https://arxiv.org/pdf/2505.11610", "abs": "https://arxiv.org/abs/2505.11610", "authors": ["Asher Moldwin", "Amarda Shehu"], "title": "Foundation Models for AI-Enabled Biological Design", "categories": ["cs.AI", "cs.LG", "q-bio.BM", "q-bio.GN"], "comment": "Published as part of the workshop proceedings at AAAI 2025 in the\n  workshop \"Foundation Models for Biological Discoveries\"", "summary": "This paper surveys foundation models for AI-enabled biological design,\nfocusing on recent developments in applying large-scale, self-supervised models\nto tasks such as protein engineering, small molecule design, and genomic\nsequence design. Though this domain is evolving rapidly, this survey presents\nand discusses a taxonomy of current models and methods. The focus is on\nchallenges and solutions in adapting these models for biological applications,\nincluding biological sequence modeling architectures, controllability in\ngeneration, and multi-modal integration. The survey concludes with a discussion\nof open problems and future directions, offering concrete next-steps to improve\nthe quality of biological sequence generation."}
{"id": "2505.12477", "pdf": "https://arxiv.org/pdf/2505.12477", "abs": "https://arxiv.org/abs/2505.12477", "authors": ["Hugues Van Assel", "Mark Ibrahim", "Tommaso Biancalani", "Aviv Regev", "Randall Balestriero"], "title": "Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "33 pages, 9 figures", "summary": "Reconstruction and joint embedding have emerged as two leading paradigms in\nSelf Supervised Learning (SSL). Reconstruction methods focus on recovering the\noriginal sample from a different view in input space. On the other hand, joint\nembedding methods align the representations of different views in latent space.\nBoth approaches offer compelling advantages, yet practitioners lack clear\nguidelines for choosing between them. In this work, we unveil the core\nmechanisms that distinguish each paradigm. By leveraging closed form solutions\nfor both approaches, we precisely characterize how the view generation process,\ne.g. data augmentation, impacts the learned representations. We then\ndemonstrate that, unlike supervised learning, both SSL paradigms require a\nminimal alignment between augmentations and irrelevant features to achieve\nasymptotic optimality with increasing sample size. Our findings indicate that\nin scenarios where these irrelevant features have a large magnitude, joint\nembedding methods are preferable because they impose a strictly weaker\nalignment condition compared to reconstruction based methods. These results not\nonly clarify the trade offs between the two paradigms but also substantiate the\nempirical success of joint embedding approaches on real world challenging\ndatasets."}
{"id": "2505.11618", "pdf": "https://arxiv.org/pdf/2505.11618", "abs": "https://arxiv.org/abs/2505.11618", "authors": ["Pengrui Quan", "Brian Wang", "Kang Yang", "Liying Han", "Mani Srivastava"], "title": "Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges", "categories": ["cs.AI", "cs.LG", "eess.SP"], "comment": null, "summary": "Spatiotemporal reasoning plays a key role in Cyber-Physical Systems (CPS).\nDespite advances in Large Language Models (LLMs) and Large Reasoning Models\n(LRMs), their capacity to reason about complex spatiotemporal signals remains\nunderexplored. This paper proposes a hierarchical SpatioTemporal reAsoning\nbenchmaRK, STARK, to systematically evaluate LLMs across three levels of\nreasoning complexity: state estimation (e.g., predicting field variables,\nlocalizing and tracking events in space and time), spatiotemporal reasoning\nover states (e.g., inferring spatial-temporal relationships), and\nworld-knowledge-aware reasoning that integrates contextual and domain knowledge\n(e.g., intent prediction, landmark-aware navigation). We curate 26 distinct\nspatiotemporal tasks with diverse sensor modalities, comprising 14,552\nchallenges where models answer directly or by Python Code Interpreter.\nEvaluating 3 LRMs and 8 LLMs, we find LLMs achieve limited success in tasks\nrequiring geometric reasoning (e.g., multilateration or triangulation),\nparticularly as complexity increases. Surprisingly, LRMs show robust\nperformance across tasks with various levels of difficulty, often competing or\nsurpassing traditional first-principle-based methods. Our results show that in\nreasoning tasks requiring world knowledge, the performance gap between LLMs and\nLRMs narrows, with some LLMs even surpassing LRMs. However, the LRM o3 model\ncontinues to achieve leading performance across all evaluated tasks, a result\nattributed primarily to the larger size of the reasoning models. STARK\nmotivates future innovations in model architectures and reasoning paradigms for\nintelligent CPS by providing a structured framework to identify limitations in\nthe spatiotemporal reasoning of LLMs and LRMs."}
{"id": "2505.12489", "pdf": "https://arxiv.org/pdf/2505.12489", "abs": "https://arxiv.org/abs/2505.12489", "authors": ["Shaobin Zhuang", "Zhipeng Huang", "Ying Zhang", "Fangyikang Wang", "Canmiao Fu", "Binxin Yang", "Chong Sun", "Chen Li", "Yali Wang"], "title": "Video-GPT via Next Clip Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 12 figures, 18 tables", "summary": "GPT has shown its remarkable success in natural language processing. However,\nthe language sequence is not sufficient to describe spatial-temporal details in\nthe visual world. Alternatively, the video sequence is good at capturing such\ndetails. Motivated by this fact, we propose a concise Video-GPT in this paper\nby treating video as new language for visual world modeling. By analogy to next\ntoken prediction in GPT, we introduce a novel next clip diffusion paradigm for\npretraining Video-GPT. Different from the previous works, this distinct\nparadigm allows Video-GPT to tackle both short-term generation and long-term\nprediction, by autoregressively denoising the noisy clip according to the clean\nclips in the history. Extensive experiments show our Video-GPT achieves the\nstate-of-the-art performance on video prediction, which is the key factor\ntowards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64\nvs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in\nboth video generation and understanding, showing its great generalization\ncapacity in downstream. The project page is at https://Video-GPT.github.io."}
{"id": "2505.11622", "pdf": "https://arxiv.org/pdf/2505.11622", "abs": "https://arxiv.org/abs/2505.11622", "authors": ["Michael L. Wells", "Kamel Lahouel", "Bruno Jedynak"], "title": "The Stochastic Occupation Kernel (SOCK) Method for Learning Stochastic Differential Equations", "categories": ["stat.ML", "cs.LG"], "comment": "10 pages, 1 figure, and 2 tables in main part of text. 30 pages, 2\n  figures, and 3 tables in full submission including technical appendices", "summary": "We present a novel kernel-based method for learning multivariate stochastic\ndifferential equations (SDEs). The method follows a two-step procedure: we\nfirst estimate the drift term function, then the (matrix-valued) diffusion\nfunction given the drift. Occupation kernels are integral functionals on a\nreproducing kernel Hilbert space (RKHS) that aggregate information over a\ntrajectory. Our approach leverages vector-valued occupation kernels for\nestimating the drift component of the stochastic process. For diffusion\nestimation, we extend this framework by introducing operator-valued occupation\nkernels, enabling the estimation of an auxiliary matrix-valued function as a\npositive semi-definite operator, from which we readily derive the diffusion\nestimate. This enables us to avoid common challenges in SDE learning, such as\nintractable likelihoods, by optimizing a reconstruction-error-based objective.\nWe propose a simple learning procedure that retains strong predictive accuracy\nwhile using Fenchel duality to promote efficiency. We validate the method on\nsimulated benchmarks and a real-world dataset of Amyloid imaging in healthy and\nAlzheimer's disease (AD) subjects."}
{"id": "2505.12492", "pdf": "https://arxiv.org/pdf/2505.12492", "abs": "https://arxiv.org/abs/2505.12492", "authors": ["Amit Cohen", "Lev Gloukhenki", "Ravid Hadar", "Eden Itah", "Yehuda Shvut", "Michael Schapira"], "title": "Unleashing Automated Congestion Control Customization in the Wild", "categories": ["cs.NI", "cs.AI", "cs.LG", "cs.PF", "cs.SY", "eess.SY"], "comment": null, "summary": "Congestion control (CC) crucially impacts user experience across Internet\nservices like streaming, gaming, AR/VR, and connected cars. Traditionally, CC\nalgorithm design seeks universal control rules that yield high performance\nacross diverse application domains and networks. However, varying service needs\nand network conditions challenge this approach. We share operational experience\nwith a system that automatically customizes congestion control logic to service\nneeds and network conditions. We discuss design, deployment challenges, and\nsolutions, highlighting performance benefits through case studies in streaming,\ngaming, connected cars, and more.\n  Our system leverages PCC Vivace, an online-learning based congestion control\nprotocol developed by researchers. Hence, along with insights from customizing\ncongestion control, we also discuss lessons learned and modifications made to\nadapt PCC Vivace for real-world deployment."}
{"id": "2505.11628", "pdf": "https://arxiv.org/pdf/2505.11628", "abs": "https://arxiv.org/abs/2505.11628", "authors": ["Berkcan Kapusuzoglu", "Supriyo Chakraborty", "Chia-Hsuan Lee", "Sambit Sahu"], "title": "Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation", "categories": ["cs.CL", "cs.LG"], "comment": "Submitted to NeurIPS 2025", "summary": "Supervised fine-tuning (SFT) using expert demonstrations often suffer from\nthe imitation problem, where the model learns to reproduce the correct\nresponses without \\emph{understanding} the underlying rationale. To address\nthis limitation, we propose \\textsc{Critique-Guided Distillation (CGD)}, a\nnovel multi-stage framework that integrates teacher model generated\n\\emph{explanatory critiques} and \\emph{refined responses} into the SFT process.\nA student model is then trained to map the triplet of prompt, teacher critique,\nand its own initial response to the corresponding refined teacher response,\nthereby learning both \\emph{what} to imitate and \\emph{why}. Using\nentropy-based analysis, we show that \\textsc{CGD} reduces refinement\nuncertainty and can be interpreted as a Bayesian posterior update. We perform\nextensive empirical evaluation of \\textsc{CGD}, on variety of benchmark tasks,\nand demonstrate significant gains on both math (AMC23 +17.5%) and language\nunderstanding tasks (MMLU-Pro +6.3%), while successfully mitigating the format\ndrift issues observed in previous critique fine-tuning (CFT) techniques."}
{"id": "2505.12504", "pdf": "https://arxiv.org/pdf/2505.12504", "abs": "https://arxiv.org/abs/2505.12504", "authors": ["Zongkai Liu", "Fanqing Meng", "Lingxiao Du", "Zhixiang Zhou", "Chao Yu", "Wenqi Shao", "Qiaosheng Zhang"], "title": "CPGD: Toward Stable Rule-based Reinforcement Learning for Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advances in rule-based reinforcement learning (RL) have significantly\nimproved the reasoning capability of language models (LMs) with rule-based\nrewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO --\noften suffer from training instability, where large policy updates and improper\nclipping can lead to training collapse. To address this issue, we propose\nClipped Policy Gradient Optimization with Policy Drift (CPGD), a novel\nalgorithm designed to stabilize policy learning in LMs. CPGD introduces a\npolicy drift constraint based on KL divergence to dynamically regularize policy\nupdates, and leverages a clip mechanism on the logarithm of the ratio to\nprevent excessive policy updates. We provide theoretical justification for CPGD\nand demonstrate through empirical analysis that it mitigates the instability\nobserved in prior approaches. Furthermore, we show that CPGD significantly\nimproves performance while maintaining training stability. Our implementation\nbalances theoretical rigor with practical usability, offering a robust\nalternative for RL in the post-training of LMs. We release our code at\nhttps://github.com/ModalMinds/MM-EUREKA."}
{"id": "2505.11638", "pdf": "https://arxiv.org/pdf/2505.11638", "abs": "https://arxiv.org/abs/2505.11638", "authors": ["Ivan Bioli", "Carlo Marcati", "Giancarlo Sangalli"], "title": "Accelerating Natural Gradient Descent for PINNs with Randomized Numerical Linear Algebra", "categories": ["math.NA", "cs.LG", "cs.NA"], "comment": null, "summary": "Natural Gradient Descent (NGD) has emerged as a promising optimization\nalgorithm for training neural network-based solvers for partial differential\nequations (PDEs), such as Physics-Informed Neural Networks (PINNs). However,\nits practical use is often limited by the high computational cost of solving\nlinear systems involving the Gramian matrix. While matrix-free NGD methods\nbased on the conjugate gradient (CG) method avoid explicit matrix inversion,\nthe ill-conditioning of the Gramian significantly slows the convergence of CG.\nIn this work, we extend matrix-free NGD to broader classes of problems than\npreviously considered and propose the use of Randomized Nystr\\\"om\npreconditioning to accelerate convergence of the inner CG solver. The resulting\nalgorithm demonstrates substantial performance improvements over existing\nNGD-based methods on a range of PDE problems discretized using neural networks."}
{"id": "2505.12506", "pdf": "https://arxiv.org/pdf/2505.12506", "abs": "https://arxiv.org/abs/2505.12506", "authors": ["Yotam Norman", "Ron Meir"], "title": "Unsupervised Invariant Risk Minimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose a novel unsupervised framework for \\emph{Invariant Risk\nMinimization} (IRM), extending the concept of invariance to settings where\nlabels are unavailable. Traditional IRM methods rely on labeled data to learn\nrepresentations that are robust to distributional shifts across environments.\nIn contrast, our approach redefines invariance through feature distribution\nalignment, enabling robust representation learning from unlabeled data. We\nintroduce two methods within this framework: Principal Invariant Component\nAnalysis (PICA), a linear method that extracts invariant directions under\nGaussian assumptions, and Variational Invariant Autoencoder (VIAE), a deep\ngenerative model that disentangles environment-invariant and\nenvironment-dependent latent factors. Our approach is based on a novel\n``unsupervised'' structural causal model and supports environment-conditioned\nsample-generation and intervention. Empirical evaluations on synthetic dataset\nand modified versions of MNIST demonstrate the effectiveness of our methods in\ncapturing invariant structure, preserving relevant information, and\ngeneralizing across environments without access to labels."}
{"id": "2505.11642", "pdf": "https://arxiv.org/pdf/2505.11642", "abs": "https://arxiv.org/abs/2505.11642", "authors": ["Falong Fan", "Xi Li"], "title": "PeerGuard: Defending Multi-Agent Systems Against Backdoor Attacks Through Mutual Reasoning", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Multi-agent systems leverage advanced AI models as autonomous agents that\ninteract, cooperate, or compete to complete complex tasks across applications\nsuch as robotics and traffic management. Despite their growing importance,\nsafety in multi-agent systems remains largely underexplored, with most research\nfocusing on single AI models rather than interacting agents. This work\ninvestigates backdoor vulnerabilities in multi-agent systems and proposes a\ndefense mechanism based on agent interactions. By leveraging reasoning\nabilities, each agent evaluates responses from others to detect illogical\nreasoning processes, which indicate poisoned agents. Experiments on LLM-based\nmulti-agent systems, including ChatGPT series and Llama 3, demonstrate the\neffectiveness of the proposed method, achieving high accuracy in identifying\npoisoned agents while minimizing false positives on clean agents. We believe\nthis work provides insights into multi-agent system safety and contributes to\nthe development of robust, trustworthy AI interactions."}
{"id": "2505.12509", "pdf": "https://arxiv.org/pdf/2505.12509", "abs": "https://arxiv.org/abs/2505.12509", "authors": ["Junhao Liu", "Haonan Yu", "Xin Zhang"], "title": "Towards Budget-Friendly Model-Agnostic Explanation Generation for Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With Large language models (LLMs) becoming increasingly prevalent in various\napplications, the need for interpreting their predictions has become a critical\nchallenge. As LLMs vary in architecture and some are closed-sourced,\nmodel-agnostic techniques show great promise without requiring access to the\nmodel's internal parameters. However, existing model-agnostic techniques need\nto invoke LLMs many times to gain sufficient samples for generating faithful\nexplanations, which leads to high economic costs. In this paper, we show that\nit is practical to generate faithful explanations for large-scale LLMs by\nsampling from some budget-friendly models through a series of empirical\nstudies. Moreover, we show that such proxy explanations also perform well on\ndownstream tasks. Our analysis provides a new paradigm of model-agnostic\nexplanation methods for LLMs, by including information from budget-friendly\nmodels."}
{"id": "2505.11665", "pdf": "https://arxiv.org/pdf/2505.11665", "abs": "https://arxiv.org/abs/2505.11665", "authors": ["Shubham Vatsal", "Harsh Dubey", "Aditi Singh"], "title": "Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance across\na wide range of Natural Language Processing (NLP) tasks. However, ensuring\ntheir effectiveness across multiple languages presents unique challenges.\nMultilingual prompt engineering has emerged as a key approach to enhance LLMs'\ncapabilities in diverse linguistic settings without requiring extensive\nparameter re-training or fine-tuning. With growing interest in multilingual\nprompt engineering over the past two to three years, researchers have explored\nvarious strategies to improve LLMs' performance across languages and NLP tasks.\nBy crafting structured natural language prompts, researchers have successfully\nextracted knowledge from LLMs across different languages, making these\ntechniques an accessible pathway for a broader audience, including those\nwithout deep expertise in machine learning, to harness the capabilities of\nLLMs. In this paper, we survey and categorize different multilingual prompting\ntechniques based on the NLP tasks they address across a diverse set of datasets\nthat collectively span around 250 languages. We further highlight the LLMs\nemployed, present a taxonomy of approaches and discuss potential\nstate-of-the-art (SoTA) methods for specific multilingual datasets.\nAdditionally, we derive a range of insights across language families and\nresource levels (high-resource vs. low-resource), including analyses such as\nthe distribution of NLP tasks by language resource type and the frequency of\nprompting methods across different language families. Our survey reviews 36\nresearch papers covering 39 prompting techniques applied to 30 multilingual NLP\ntasks, with the majority of these studies published in the last two years."}
{"id": "2505.12512", "pdf": "https://arxiv.org/pdf/2505.12512", "abs": "https://arxiv.org/abs/2505.12512", "authors": ["Truman Hickok"], "title": "Scalable Strategies for Continual Learning with Replay", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Future deep learning models will be distinguished by systems that perpetually\nlearn through interaction, imagination, and cooperation, blurring the line\nbetween training and inference. This makes continual learning a critical\nchallenge, as methods that efficiently maximize bidirectional transfer across\nlearning trajectories will be essential. Replay is on track to play a\nfoundational role in continual learning, allowing models to directly reconcile\nnew information with past knowledge. In practice, however, replay is quite\nunscalable, doubling the cost of continual learning when applied naively.\nMoreover, the continual learning literature has not fully synchronized with the\nmulti-task fine-tuning literature, having not fully integrated highly scalable\ntechniques like model merging and low rank adaptation into a replay-enabled\ntoolset that can produce a unified model in the face of many sequential tasks.\nIn this paper, we begin by applying and analyzing low rank adaptation in a\ncontinual learning setting. Next, we introduce consolidation, a phasic approach\nto replay which leads to up to 55\\% less replay samples being needed for a\ngiven performance target. Then, we propose sequential merging, an offshoot of\ntask arithmetic which is tailored to the continual learning setting and is\nshown to work well in combination with replay. Finally, we demonstrate that the\ndeveloped strategies can operate synergistically, resulting in a highly\nscalable toolset that outperforms standalone variants."}
{"id": "2505.11671", "pdf": "https://arxiv.org/pdf/2505.11671", "abs": "https://arxiv.org/abs/2505.11671", "authors": ["Andrew Millard", "Zheng Zhao", "Joshua Murphy", "Simon Maskell"], "title": "Humble your Overconfident Networks: Unlearning Overfitting via Sequential Monte Carlo Tempered Deep Ensembles", "categories": ["stat.ML", "cs.LG", "stat.CO"], "comment": null, "summary": "Sequential Monte Carlo (SMC) methods offer a principled approach to Bayesian\nuncertainty quantification but are traditionally limited by the need for\nfull-batch gradient evaluations. We introduce a scalable variant by\nincorporating Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) proposals\ninto SMC, enabling efficient mini-batch based sampling. Our resulting SMCSGHMC\nalgorithm outperforms standard stochastic gradient descent (SGD) and deep\nensembles across image classification, out-of-distribution (OOD) detection, and\ntransfer learning tasks. We further show that SMCSGHMC mitigates overfitting\nand improves calibration, providing a flexible, scalable pathway for converting\npretrained neural networks into well-calibrated Bayesian models."}
{"id": "2505.12532", "pdf": "https://arxiv.org/pdf/2505.12532", "abs": "https://arxiv.org/abs/2505.12532", "authors": ["Ahmet Bilican", "M. Akın Yılmaz", "A. Murat Tekalp", "R. Gökberk Cinbiş"], "title": "Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "eess.SP"], "comment": null, "summary": "Efficiently adapting large foundation models is critical, especially with\ntight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT)\nmethods such as LoRA offer limited granularity and effectiveness in\nfew-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT\nmethod that learns highly sparse updates in the wavelet domain of residual\nmatrices. WaveFT allows precise control of trainable parameters, offering\nfine-grained capacity adjustment and excelling with remarkably low parameter\ncount, potentially far fewer than LoRA's minimum -- ideal for extreme\nparameter-efficient scenarios. In order to demonstrate the effect of the\nwavelet transform, we compare WaveFT with a special case, called SHiRA, that\nentails applying sparse updates directly in the weight domain. Evaluated on\npersonalized text-to-image generation using Stable Diffusion XL as baseline,\nWaveFT significantly outperforms LoRA and other PEFT methods, especially at low\nparameter counts; achieving superior subject fidelity, prompt alignment, and\nimage diversity."}
{"id": "2505.11677", "pdf": "https://arxiv.org/pdf/2505.11677", "abs": "https://arxiv.org/abs/2505.11677", "authors": ["Hansen Chang", "Christian DeLozier"], "title": "Enhancing Code Quality with Generative AI: Boosting Developer Warning Compliance", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Programmers have long ignored warnings, especially those generated by static\nanalysis tools, due to the potential for false-positives. In some cases,\nwarnings may be indicative of larger issues, but programmers may not understand\nhow a seemingly unimportant warning can grow into a vulnerability. Because\nthese messages tend to be long and confusing, programmers tend to ignore them\nif they do not cause readily identifiable issues. Large language models can\nsimplify these warnings, explain the gravity of important warnings, and suggest\npotential fixes to increase developer compliance with fixing warnings."}
{"id": "2505.12547", "pdf": "https://arxiv.org/pdf/2505.12547", "abs": "https://arxiv.org/abs/2505.12547", "authors": ["Florent Chiaroni", "Ali Ayub", "Ola Ahmad"], "title": "ProMi: An Efficient Prototype-Mixture Baseline for Few-Shot Segmentation with Bounding-Box Annotations", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "In robotics applications, few-shot segmentation is crucial because it allows\nrobots to perform complex tasks with minimal training data, facilitating their\nadaptation to diverse, real-world environments. However, pixel-level\nannotations of even small amount of images is highly time-consuming and costly.\nIn this paper, we present a novel few-shot binary segmentation method based on\nbounding-box annotations instead of pixel-level labels. We introduce, ProMi, an\nefficient prototype-mixture-based method that treats the background class as a\nmixture of distributions. Our approach is simple, training-free, and effective,\naccommodating coarse annotations with ease. Compared to existing baselines,\nProMi achieves the best results across different datasets with significant\ngains, demonstrating its effectiveness. Furthermore, we present qualitative\nexperiments tailored to real-world mobile robot tasks, demonstrating the\napplicability of our approach in such scenarios. Our code:\nhttps://github.com/ThalesGroup/promi."}
{"id": "2505.11679", "pdf": "https://arxiv.org/pdf/2505.11679", "abs": "https://arxiv.org/abs/2505.11679", "authors": ["Zhibo Hu", "Chen Wang", "Yanfeng Shu", "Hye-Young Paik", "Liming Zhu"], "title": "Ambiguity Resolution in Text-to-Structured Data Mapping", "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "15 pages, 11 figures", "summary": "Ambiguity in natural language is a significant obstacle for achieving\naccurate text to structured data mapping through large language models (LLMs),\nwhich affects the performance of tasks such as mapping text to agentic tool\ncalling and text-to-SQL queries. Existing methods of ambiguity handling either\nexploit ReACT framework to produce the correct mapping through trial and error,\nor supervised fine tuning to guide models to produce a biased mapping to\nimprove certain tasks. In this paper, we adopt a different approach that\ncharacterizes the representation difference of ambiguous text in the latent\nspace and leverage the difference to identify ambiguity before mapping them to\nstructured data. To detect ambiguity of a sentence, we focused on the\nrelationship between ambiguous questions and their interpretations and what\ncause the LLM ignore multiple interpretations. Different to the distance\ncalculated by dense embedding vectors, we utilize the observation that\nambiguity is caused by concept missing in latent space of LLM to design a new\ndistance measurement, computed through the path kernel by the integral of\ngradient values for each concepts from sparse-autoencoder (SAE) under each\nstate. We identify patterns to distinguish ambiguous questions with this\nmeasurement. Based on our observation, We propose a new framework to improve\nthe performance of LLMs on ambiguous agentic tool calling through missing\nconcepts prediction."}
{"id": "2505.12552", "pdf": "https://arxiv.org/pdf/2505.12552", "abs": "https://arxiv.org/abs/2505.12552", "authors": ["Junliang Ye", "Lei Wang", "Md Zakir Hossain"], "title": "FreqSelect: Frequency-Aware fMRI-to-Image Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "Research report", "summary": "Reconstructing natural images from functional magnetic resonance imaging\n(fMRI) data remains a core challenge in natural decoding due to the mismatch\nbetween the richness of visual stimuli and the noisy, low resolution nature of\nfMRI signals. While recent two-stage models, combining deep variational\nautoencoders (VAEs) with diffusion models, have advanced this task, they treat\nall spatial-frequency components of the input equally. This uniform treatment\nforces the model to extract meaning features and suppress irrelevant noise\nsimultaneously, limiting its effectiveness. We introduce FreqSelect, a\nlightweight, adaptive module that selectively filters spatial-frequency bands\nbefore encoding. By dynamically emphasizing frequencies that are most\npredictive of brain activity and suppressing those that are uninformative,\nFreqSelect acts as a content-aware gate between image features and natural\ndata. It integrates seamlessly into standard very deep VAE-diffusion pipelines\nand requires no additional supervision. Evaluated on the Natural Scenes\ndataset, FreqSelect consistently improves reconstruction quality across both\nlow- and high-level metrics. Beyond performance gains, the learned\nfrequency-selection patterns offer interpretable insights into how different\nvisual frequencies are represented in the brain. Our method generalizes across\nsubjects and scenes, and holds promise for extension to other neuroimaging\nmodalities, offering a principled approach to enhancing both decoding accuracy\nand neuroscientific interpretability."}
{"id": "2505.11708", "pdf": "https://arxiv.org/pdf/2505.11708", "abs": "https://arxiv.org/abs/2505.11708", "authors": ["Diksha Goel", "Kristen Moore", "Jeff Wang", "Minjune Kim", "Thanh Thi Nguyen"], "title": "Unveiling the Black Box: A Multi-Layer Framework for Explaining Reinforcement Learning-Based Cyber Agents", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Reinforcement Learning (RL) agents are increasingly used to simulate\nsophisticated cyberattacks, but their decision-making processes remain opaque,\nhindering trust, debugging, and defensive preparedness. In high-stakes\ncybersecurity contexts, explainability is essential for understanding how\nadversarial strategies are formed and evolve over time. In this paper, we\npropose a unified, multi-layer explainability framework for RL-based attacker\nagents that reveals both strategic (MDP-level) and tactical (policy-level)\nreasoning. At the MDP level, we model cyberattacks as a Partially Observable\nMarkov Decision Processes (POMDPs) to expose exploration-exploitation dynamics\nand phase-aware behavioural shifts. At the policy level, we analyse the\ntemporal evolution of Q-values and use Prioritised Experience Replay (PER) to\nsurface critical learning transitions and evolving action preferences.\nEvaluated across CyberBattleSim environments of increasing complexity, our\nframework offers interpretable insights into agent behaviour at scale. Unlike\nprevious explainable RL methods, which are often post-hoc, domain-specific, or\nlimited in depth, our approach is both agent- and environment-agnostic,\nsupporting use cases ranging from red-team simulation to RL policy debugging.\nBy transforming black-box learning into actionable behavioural intelligence,\nour framework enables both defenders and developers to better anticipate,\nanalyse, and respond to autonomous cyber threats."}
{"id": "2505.12556", "pdf": "https://arxiv.org/pdf/2505.12556", "abs": "https://arxiv.org/abs/2505.12556", "authors": ["Taniya Kapoor", "Abhishek Chandra", "Anastasios Stamou", "Stephen J Roberts"], "title": "Beyond Accuracy: EcoL2 Metric for Sustainable Neural PDE Solvers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Real-world systems, from aerospace to railway engineering, are modeled with\npartial differential equations (PDEs) describing the physics of the system.\nEstimating robust solutions for such problems is essential. Deep learning-based\narchitectures, such as neural PDE solvers, have recently gained traction as a\nreliable solution method. The current state of development of these approaches,\nhowever, primarily focuses on improving accuracy. The environmental impact of\nexcessive computation, leading to increased carbon emissions, has largely been\noverlooked. This paper introduces a carbon emission measure for a range of PDE\nsolvers. Our proposed metric, EcoL2, balances model accuracy with emissions\nacross data collection, model training, and deployment. Experiments across both\nphysics-informed machine learning and operator learning architectures\ndemonstrate that the proposed metric presents a holistic assessment of model\nperformance and emission cost. As such solvers grow in scale and deployment,\nEcoL2 represents a step toward building performant scientific machine learning\nsystems with lower long-term environmental impact."}
{"id": "2505.11709", "pdf": "https://arxiv.org/pdf/2505.11709", "abs": "https://arxiv.org/abs/2505.11709", "authors": ["Ryan Hoque", "Peide Huang", "David J. Yoon", "Mouli Sivapurapu", "Jian Zhang"], "title": "EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Imitation learning for manipulation has a well-known data scarcity problem.\nUnlike natural language and 2D computer vision, there is no Internet-scale\ncorpus of data for dexterous manipulation. One appealing option is egocentric\nhuman video, a passively scalable data source. However, existing large-scale\ndatasets such as Ego4D do not have native hand pose annotations and do not\nfocus on object manipulation. To this end, we use Apple Vision Pro to collect\nEgoDex: the largest and most diverse dataset of dexterous human manipulation to\ndate. EgoDex has 829 hours of egocentric video with paired 3D hand and finger\ntracking data collected at the time of recording, where multiple calibrated\ncameras and on-device SLAM can be used to precisely track the pose of every\njoint of each hand. The dataset covers a wide range of diverse manipulation\nbehaviors with everyday household objects in 194 different tabletop tasks\nranging from tying shoelaces to folding laundry. Furthermore, we train and\nsystematically evaluate imitation learning policies for hand trajectory\nprediction on the dataset, introducing metrics and benchmarks for measuring\nprogress in this increasingly important area. By releasing this large-scale\ndataset, we hope to push the frontier of robotics, computer vision, and\nfoundation models."}
{"id": "2505.12567", "pdf": "https://arxiv.org/pdf/2505.12567", "abs": "https://arxiv.org/abs/2505.12567", "authors": ["Wenrui Xu", "Keshab K. Parhi"], "title": "A Survey of Attacks on Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) and LLM-based agents have been widely deployed\nin a wide range of applications in the real world, including healthcare\ndiagnostics, financial analysis, customer support, robotics, and autonomous\ndriving, expanding their powerful capability of understanding, reasoning, and\ngenerating natural languages. However, the wide deployment of LLM-based\napplications exposes critical security and reliability risks, such as the\npotential for malicious misuse, privacy leakage, and service disruption that\nweaken user trust and undermine societal safety. This paper provides a\nsystematic overview of the details of adversarial attacks targeting both LLMs\nand LLM-based agents. These attacks are organized into three phases in LLMs:\nTraining-Phase Attacks, Inference-Phase Attacks, and Availability & Integrity\nAttacks. For each phase, we analyze the details of representative and recently\nintroduced attack methods along with their corresponding defenses. We hope our\nsurvey will provide a good tutorial and a comprehensive understanding of LLM\nsecurity, especially for attacks on LLMs. We desire to raise attention to the\nrisks inherent in widely deployed LLM-based applications and highlight the\nurgent need for robust mitigation strategies for evolving threats."}
{"id": "2505.11719", "pdf": "https://arxiv.org/pdf/2505.11719", "abs": "https://arxiv.org/abs/2505.11719", "authors": ["Sumeet Batra", "Gaurav Sukhatme"], "title": "Zero-Shot Visual Generalization in Robot Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Training vision-based manipulation policies that are robust across diverse\nvisual environments remains an important and unresolved challenge in robot\nlearning. Current approaches often sidestep the problem by relying on invariant\nrepresentations such as point clouds and depth, or by brute-forcing\ngeneralization through visual domain randomization and/or large, visually\ndiverse datasets. Disentangled representation learning - especially when\ncombined with principles of associative memory - has recently shown promise in\nenabling vision-based reinforcement learning policies to be robust to visual\ndistribution shifts. However, these techniques have largely been constrained to\nsimpler benchmarks and toy environments. In this work, we scale disentangled\nrepresentation learning and associative memory to more visually and dynamically\ncomplex manipulation tasks and demonstrate zero-shot adaptability to visual\nperturbations in both simulation and on real hardware. We further extend this\napproach to imitation learning, specifically Diffusion Policy, and empirically\nshow significant gains in visual generalization compared to state-of-the-art\nimitation learning methods. Finally, we introduce a novel technique adapted\nfrom the model equivariance literature that transforms any trained neural\nnetwork policy into one invariant to 2D planar rotations, making our policy not\nonly visually robust but also resilient to certain camera perturbations. We\nbelieve that this work marks a significant step towards manipulation policies\nthat are not only adaptable out of the box, but also robust to the complexities\nand dynamical nature of real-world deployment. Supplementary videos are\navailable at https://sites.google.com/view/vis-gen-robotics/home."}
{"id": "2505.12572", "pdf": "https://arxiv.org/pdf/2505.12572", "abs": "https://arxiv.org/abs/2505.12572", "authors": ["Hanwen Shen", "Ting Ying"], "title": "Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Writing novels with Large Language Models (LLMs) raises a critical question:\nhow much human-authored outline is necessary to generate high-quality\nmillion-word novels? While frameworks such as DOME, Plan&Write, and Long Writer\nhave improved stylistic coherence and logical consistency, they primarily\ntarget shorter novels (10k--100k words), leaving ultra-long generation largely\nunexplored. Drawing on insights from recent text compression methods like\nLLMZip and LLM2Vec, we conduct an information-theoretic analysis that\nquantifies distortion occurring when LLMs compress and reconstruct ultra-long\nnovels under varying compression-expansion ratios. We introduce a hierarchical\ntwo-stage generation pipeline (outline -> detailed outline -> manuscript) and\nfind an optimal outline length that balances information preservation with\nhuman effort. Through extensive experimentation with Chinese novels, we\nestablish that a two-stage hierarchical outline approach significantly reduces\nsemantic distortion compared to single-stage methods. Our findings provide\nempirically-grounded guidance for authors and researchers collaborating with\nLLMs to create million-word novels."}
{"id": "2505.11720", "pdf": "https://arxiv.org/pdf/2505.11720", "abs": "https://arxiv.org/abs/2505.11720", "authors": ["Shijun Liang", "Ismail R. Alkhouri", "Siddhant Gautam", "Qing Qu", "Saiprasad Ravishankar"], "title": "UGoDIT: Unsupervised Group Deep Image Prior Via Transferable Weights", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Recent advances in data-centric deep generative models have led to\nsignificant progress in solving inverse imaging problems. However, these models\n(e.g., diffusion models (DMs)) typically require large amounts of fully sampled\n(clean) training data, which is often impractical in medical and scientific\nsettings such as dynamic imaging.\n  On the other hand, training-data-free approaches like the Deep Image Prior\n(DIP) do not require clean ground-truth images but suffer from noise\noverfitting and can be computationally expensive as the network parameters need\nto be optimized for each measurement set independently. Moreover, DIP-based\nmethods often overlook the potential of learning a prior using a small number\nof sub-sampled measurements (or degraded images) available during training. In\nthis paper, we propose UGoDIT, an Unsupervised Group DIP via Transferable\nweights, designed for the low-data regime where only a very small number, M, of\nsub-sampled measurement vectors are available during training. Our method\nlearns a set of transferable weights by optimizing a shared encoder and M\ndisentangled decoders. At test time, we reconstruct the unseen degraded image\nusing a DIP network, where part of the parameters are fixed to the learned\nweights, while the remaining are optimized to enforce measurement consistency.\nWe evaluate UGoDIT on both medical (multi-coil MRI) and natural (super\nresolution and non-linear deblurring) image recovery tasks under various\nsettings. Compared to recent standalone DIP methods, UGoDIT provides\naccelerated convergence and notable improvement in reconstruction quality.\nFurthermore, our method achieves performance competitive with SOTA DM-based and\nsupervised approaches, despite not requiring large amounts of clean training\ndata."}
{"id": "2505.12576", "pdf": "https://arxiv.org/pdf/2505.12576", "abs": "https://arxiv.org/abs/2505.12576", "authors": ["Kiran Kokilepersaud", "Mohit Prabhushankar", "Ghassan AlRegib"], "title": "AdaDim: Dimensionality Adaptation for SSL Representational Dynamics", "categories": ["cs.LG", "cs.AI"], "comment": "Under Review", "summary": "A key factor in effective Self-Supervised learning (SSL) is preventing\ndimensional collapse, which is where higher-dimensional representation spaces\nspan a lower-dimensional subspace. Therefore, SSL optimization strategies\ninvolve guiding a model to produce representations ($R$) with a higher\ndimensionality. Dimensionality is either optimized through a\ndimension-contrastive approach that encourages feature decorrelation or through\na sample-contrastive method that promotes a uniform spread of sample\nrepresentations. Both families of SSL algorithms also utilize a projection head\nthat maps $R$ into a lower-dimensional embedding space $Z$. Recent work has\ncharacterized the projection head as a filter of irrelevant features from the\nSSL objective by reducing mutual information, $I(R;Z)$. Therefore, the current\nliterature's view is that a good SSL representation space should have a high\n$H(R)$ and a low $I(R;Z)$. However, this view of the problem is lacking in\nterms of an understanding of the underlying training dynamics that influences\nboth terms, as well as how the values of $H(R)$ and $I(R;Z)$ arrived at the end\nof training reflect the downstream performance of an SSL model. We address both\ngaps in the literature by demonstrating that increases in $H(R)$ due to feature\ndecorrelation at the start of training lead to a higher $I(R;Z)$, while\nincreases in $H(R)$ due to samples distributing uniformly in a high-dimensional\nspace at the end of training cause $I(R;Z)$ to plateau or decrease.\nFurthermore, our analysis shows that the best performing SSL models do not have\nthe highest $H(R)$ nor the lowest $I(R;Z)$, but arrive at an optimal\nintermediate point for both. We develop a method called AdaDim to exploit these\nobserved training dynamics by adaptively weighting between losses based on\nfeature decorrelation and uniform sample spread."}
{"id": "2505.11722", "pdf": "https://arxiv.org/pdf/2505.11722", "abs": "https://arxiv.org/abs/2505.11722", "authors": ["Grace M. Lu", "Dallas R. Trinkle"], "title": "Explainable Machine Learning for Oxygen Diffusion in Perovskites and Pyrochlores", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "comment": "32 pages, 11 figures", "summary": "Explainable machine learning can help to discover new physical relationships\nfor material properties. To understand the material properties that govern the\nactivation energy for oxygen diffusion in perovskites and pyrochlores, we build\na database of experimental activation energies and apply a grouping algorithm\nto the material property features. These features are then used to fit seven\ndifferent machine learning models. An ensemble consensus determines that the\nmost important features for predicting the activation energy are the ionicity\nof the A-site bond and the partial pressure of oxygen for perovskites. For\npyrochlores, the two most important features are the A-site $s$ valence\nelectron count and the B-site electronegativity. The most important features\nare all constructed using the weighted averages of elemental metal properties,\ndespite weighted averages of the constituent binary oxides being included in\nour feature set. This is surprising because the material properties of the\nconstituent oxides are more similar to the experimentally measured properties\nof perovskites and pyrochlores than the features of the metals that are chosen.\nThe easy-to-measure features identified in this work enable rapid screening for\nnew materials with fast oxide-ion diffusivity."}
{"id": "2505.12581", "pdf": "https://arxiv.org/pdf/2505.12581", "abs": "https://arxiv.org/abs/2505.12581", "authors": ["Lucas M. Dorneles", "Luan Fonseca Garcia", "Joel Luís Carbonera"], "title": "An approach based on class activation maps for investigating the effects of data augmentation on neural networks for image classification", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Neural networks have become increasingly popular in the last few years as an\neffective tool for the task of image classification due to the impressive\nperformance they have achieved on this task. In image classification tasks, it\nis common to use data augmentation strategies to increase the robustness of\ntrained networks to changes in the input images and to avoid overfitting.\nAlthough data augmentation is a widely adopted technique, the literature lacks\na body of research analyzing the effects data augmentation methods have on the\npatterns learned by neural network models working on complex datasets. The\nprimary objective of this work is to propose a methodology and set of metrics\nthat may allow a quantitative approach to analyzing the effects of data\naugmentation in convolutional networks applied to image classification. An\nimportant tool used in the proposed approach lies in the concept of class\nactivation maps for said models, which allow us to identify and measure the\nimportance these models assign to each individual pixel in an image when\nexecuting the classification task. From these maps, we may then extract metrics\nover the similarities and differences between maps generated by these models\ntrained on a given dataset with different data augmentation strategies.\nExperiments made using this methodology suggest that the effects of these data\naugmentation techniques not only can be analyzed in this way but also allow us\nto identify different impact profiles over the trained models."}
{"id": "2505.11729", "pdf": "https://arxiv.org/pdf/2505.11729", "abs": "https://arxiv.org/abs/2505.11729", "authors": ["Pedro Figueiredo", "Qihao He", "Steve Bako", "Nima Khademi Kalantari"], "title": "Neural Importance Sampling of Many Lights", "categories": ["cs.GR", "cs.LG"], "comment": "11 pages, 11 figures. Accepted for publication in SIGGRAPH Conference\n  Papers '25; to be presented at SIGGRAPH 2025", "summary": "We propose a neural approach for estimating spatially varying light selection\ndistributions to improve importance sampling in Monte Carlo rendering,\nparticularly for complex scenes with many light sources. Our method uses a\nneural network to predict the light selection distribution at each shading\npoint based on local information, trained by minimizing the KL-divergence\nbetween the learned and target distributions in an online manner. To\nefficiently manage hundreds or thousands of lights, we integrate our neural\napproach with light hierarchy techniques, where the network predicts\ncluster-level distributions and existing methods sample lights within clusters.\nAdditionally, we introduce a residual learning strategy that leverages initial\ndistributions from existing techniques, accelerating convergence during\ntraining. Our method achieves superior performance across diverse and\nchallenging scenes."}
{"id": "2505.12583", "pdf": "https://arxiv.org/pdf/2505.12583", "abs": "https://arxiv.org/abs/2505.12583", "authors": ["Takeshi Kojima", "Yaonan Zhu", "Yusuke Iwasawa", "Toshinori Kitamura", "Gang Yan", "Shu Morikuni", "Ryosuke Takanami", "Alfredo Solano", "Tatsuya Matsushima", "Akiko Murakami", "Yutaka Matsuo"], "title": "A Comprehensive Survey on Physical Risk Control in the Era of Foundation Model-enabled Robotics", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted to IJCAI 2025 Survey Track", "summary": "Recent Foundation Model-enabled robotics (FMRs) display greatly improved\ngeneral-purpose skills, enabling more adaptable automation than conventional\nrobotics. Their ability to handle diverse tasks thus creates new opportunities\nto replace human labor. However, unlike general foundation models, FMRs\ninteract with the physical world, where their actions directly affect the\nsafety of humans and surrounding objects, requiring careful deployment and\ncontrol. Based on this proposition, our survey comprehensively summarizes robot\ncontrol approaches to mitigate physical risks by covering all the lifespan of\nFMRs ranging from pre-deployment to post-accident stage. Specifically, we\nbroadly divide the timeline into the following three phases: (1) pre-deployment\nphase, (2) pre-incident phase, and (3) post-incident phase. Throughout this\nsurvey, we find that there is much room to study (i) pre-incident risk\nmitigation strategies, (ii) research that assumes physical interaction with\nhumans, and (iii) essential issues of foundation models themselves. We hope\nthat this survey will be a milestone in providing a high-resolution analysis of\nthe physical risks of FMRs and their control, contributing to the realization\nof a good human-robot relationship."}
{"id": "2505.11730", "pdf": "https://arxiv.org/pdf/2505.11730", "abs": "https://arxiv.org/abs/2505.11730", "authors": ["Hao Mark Chen", "Guanxi Lu", "Yasuyuki Okoshi", "Zhiwen Mo", "Masato Motomura", "Hongxiang Fan"], "title": "Rethinking Optimal Verification Granularity for Compute-Efficient Test-Time Scaling", "categories": ["cs.AI", "cs.LG"], "comment": "Preprint. Under review", "summary": "Test-time scaling (TTS) has proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Verification plays a key role in\nTTS, simultaneously influencing (1) reasoning performance and (2) compute\nefficiency, due to the quality and computational cost of verification. In this\nwork, we challenge the conventional paradigms of verification, and make the\nfirst attempt toward systematically investigating the impact of verification\ngranularity-that is, how frequently the verifier is invoked during generation,\nbeyond verifying only the final output or individual generation steps. To this\nend, we introduce Variable Granularity Search (VG-Search), a unified algorithm\nthat generalizes beam search and Best-of-N sampling via a tunable granularity\nparameter g. Extensive experiments with VG-Search under varying compute\nbudgets, generator-verifier configurations, and task attributes reveal that\ndynamically selecting g can improve the compute efficiency and scaling\nbehavior. Building on these findings, we propose adaptive VG-Search strategies\nthat achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over\nBest-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to\nsupport future research."}
{"id": "2505.12585", "pdf": "https://arxiv.org/pdf/2505.12585", "abs": "https://arxiv.org/abs/2505.12585", "authors": ["En Yu", "Jie Lu", "Xiaoyu Yang", "Guangquan Zhang", "Zhen Fang"], "title": "Learning Robust Spectral Dynamics for Temporal Domain Generalization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Modern machine learning models struggle to maintain performance in dynamic\nenvironments where temporal distribution shifts, \\emph{i.e., concept drift},\nare prevalent. Temporal Domain Generalization (TDG) seeks to enable model\ngeneralization across evolving domains, yet existing approaches typically\nassume smooth incremental changes, struggling with complex real-world drifts\ninvolving long-term structure (incremental evolution/periodicity) and local\nuncertainties. To overcome these limitations, we introduce FreKoo, which\ntackles these challenges via a novel frequency-domain analysis of parameter\ntrajectories. It leverages the Fourier transform to disentangle parameter\nevolution into distinct spectral bands. Specifically, low-frequency component\nwith dominant dynamics are learned and extrapolated using the Koopman operator,\nrobustly capturing diverse drift patterns including both incremental and\nperiodicity. Simultaneously, potentially disruptive high-frequency variations\nare smoothed via targeted temporal regularization, preventing overfitting to\ntransient noise and domain uncertainties. In addition, this dual spectral\nstrategy is rigorously grounded through theoretical analysis, providing\nstability guarantees for the Koopman prediction, a principled Bayesian\njustification for the high-frequency regularization, and culminating in a\nmultiscale generalization bound connecting spectral dynamics to improved\ngeneralization. Extensive experiments demonstrate FreKoo's significant\nsuperiority over SOTA TDG approaches, particularly excelling in real-world\nstreaming scenarios with complex drifts and uncertainties."}
{"id": "2505.11749", "pdf": "https://arxiv.org/pdf/2505.11749", "abs": "https://arxiv.org/abs/2505.11749", "authors": ["Jiahao Yu", "Qizhen Ying", "Leyang Wang", "Ziyue Jiang", "Song Liu"], "title": "Missing Data Imputation by Reducing Mutual Information with Rectified Flows", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This paper introduces a novel iterative method for missing data imputation\nthat sequentially reduces the mutual information between data and their\ncorresponding missing mask. Inspired by GAN-based approaches, which train\ngenerators to decrease the predictability of missingness patterns, our method\nexplicitly targets the reduction of mutual information. Specifically, our\nalgorithm iteratively minimizes the KL divergence between the joint\ndistribution of the imputed data and missing mask, and the product of their\nmarginals from the previous iteration. We show that the optimal imputation\nunder this framework corresponds to solving an ODE, whose velocity field\nminimizes a rectified flow training objective. We further illustrate that some\nexisting imputation techniques can be interpreted as approximate special cases\nof our mutual-information-reducing framework. Comprehensive experiments on\nsynthetic and real-world datasets validate the efficacy of our proposed\napproach, demonstrating superior imputation performance."}
{"id": "2505.12594", "pdf": "https://arxiv.org/pdf/2505.12594", "abs": "https://arxiv.org/abs/2505.12594", "authors": ["Tiankai Yang", "Junjun Liu", "Wingchun Siu", "Jiahang Wang", "Zhuangzhuang Qian", "Chanjuan Song", "Cheng Cheng", "Xiyang Hu", "Yue Zhao"], "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD."}
{"id": "2505.11750", "pdf": "https://arxiv.org/pdf/2505.11750", "abs": "https://arxiv.org/abs/2505.11750", "authors": ["Zhanxiang Hua", "Ryan Sobash", "David John Gagne II", "Yingkai Sha", "Alexandra Anderson-Frey"], "title": "Improving Medium Range Severe Weather Prediction through Transformer Post-processing of AI Weather Forecasts", "categories": ["physics.ao-ph", "cs.AI", "cs.LG"], "comment": "16 pages, 10 figures", "summary": "Improving the skill of medium-range (1-8 day) severe weather prediction is\ncrucial for mitigating societal impacts. This study introduces a novel approach\nleveraging decoder-only transformer networks to post-process AI-based weather\nforecasts, specifically from the Pangu-Weather model, for improved severe\nweather guidance. Unlike traditional post-processing methods that use a dense\nneural network to predict the probability of severe weather using discrete\nforecast samples, our method treats forecast lead times as sequential\n``tokens'', enabling the transformer to learn complex temporal relationships\nwithin the evolving atmospheric state. We compare this approach against\npost-processing of the Global Forecast System (GFS) using both a traditional\ndense neural network and our transformer, as well as configurations that\nexclude convective parameters to fairly evaluate the impact of using the\nPangu-Weather AI model. Results demonstrate that the transformer-based\npost-processing significantly enhances forecast skill compared to dense neural\nnetworks. Furthermore, AI-driven forecasts, particularly Pangu-Weather\ninitialized from high resolution analysis, exhibit superior performance to GFS\nin the medium-range, even without explicit convective parameters. Our approach\noffers improved accuracy, and reliability, which also provides interpretability\nthrough feature attribution analysis, advancing medium-range severe weather\nprediction capabilities."}
{"id": "2505.12623", "pdf": "https://arxiv.org/pdf/2505.12623", "abs": "https://arxiv.org/abs/2505.12623", "authors": ["Keisuke Okumura", "Hiroki Nagai"], "title": "Lightweight and Effective Preference Construction in PIBT for Large-Scale Multi-Agent Pathfinding", "categories": ["cs.MA", "cs.AI"], "comment": "To be presented at SoCS-25", "summary": "PIBT is a computationally lightweight algorithm that can be applied to a\nvariety of multi-agent pathfinding (MAPF) problems, generating the next\ncollision-free locations of agents given another. Because of its simplicity and\nscalability, it is becoming a popular underlying scheme for recent large-scale\nMAPF methods involving several hundreds or thousands of agents. Vanilla PIBT\nmakes agents behave greedily towards their assigned goals, while agents\ntypically have multiple best actions, since the graph shortest path is not\nalways unique. Consequently, tiebreaking about how to choose between these\nactions significantly affects resulting solutions. This paper studies two\nsimple yet effective techniques for tiebreaking in PIBT, without compromising\nits computational advantage. The first technique allows an agent to\nintelligently dodge another, taking into account whether each action will\nhinder the progress of the next timestep. The second technique is to learn,\nthrough multiple PIBT runs, how an action causes regret in others and to use\nthis information to minimise regret collectively. Our empirical results\ndemonstrate that these techniques can reduce the solution cost of one-shot MAPF\nand improve the throughput of lifelong MAPF. For instance, in densely populated\none-shot cases, the combined use of these tiebreaks achieves improvements of\naround 10-20% in sum-of-costs, without significantly compromising the speed of\na PIBT-based planner."}
{"id": "2505.11765", "pdf": "https://arxiv.org/pdf/2505.11765", "abs": "https://arxiv.org/abs/2505.11765", "authors": ["Shijun Li", "Hilaf Hasson", "Joydeep Ghosh"], "title": "OMAC: A Broad Optimization Framework for LLM-Based Multi-Agent Collaboration", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Agents powered by advanced large language models (LLMs) have demonstrated\nimpressive capabilities across diverse complex applications. Recently,\nMulti-Agent Systems (MAS), wherein multiple agents collaborate and communicate\nwith each other, have exhibited enhanced capabilities in complex tasks, such as\nhigh-quality code generation and arithmetic reasoning. However, the development\nof such systems often relies on handcrafted methods, and the literature on\nsystematic design and optimization of LLM-based MAS remains limited.\n  In this work, we introduce OMAC, a general framework designed for holistic\noptimization of LLM-based MAS. Specifically, we identify five key optimization\ndimensions for MAS, encompassing both agent functionality and collaboration\nstructure. Building upon these dimensions, we first propose a general\nalgorithm, utilizing two actors termed the Semantic Initializer and the\nContrastive Comparator, to optimize any single dimension. Then, we present an\nalgorithm for joint optimization across multiple dimensions. Extensive\nexperiments demonstrate the superior performance of OMAC on code generation,\narithmetic reasoning, and general reasoning tasks against state-of-the-art\napproaches."}
{"id": "2505.12626", "pdf": "https://arxiv.org/pdf/2505.12626", "abs": "https://arxiv.org/abs/2505.12626", "authors": ["Ping Xu", "Zhiyuan Ning", "Pengjiang Li", "Wenhao Liu", "Pengyang Wang", "Jiaxu Cui", "Yuanchun Zhou", "Pengfei Wang"], "title": "scSiameseClu: A Siamese Clustering Framework for Interpreting single-cell RNA Sequencing Data", "categories": ["q-bio.GN", "cs.AI", "cs.LG"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) reveals cell heterogeneity, with cell\nclustering playing a key role in identifying cell types and marker genes.\nRecent advances, especially graph neural networks (GNNs)-based methods, have\nsignificantly improved clustering performance. However, the analysis of\nscRNA-seq data remains challenging due to noise, sparsity, and high\ndimensionality. Compounding these challenges, GNNs often suffer from\nover-smoothing, limiting their ability to capture complex biological\ninformation. In response, we propose scSiameseClu, a novel Siamese Clustering\nframework for interpreting single-cell RNA-seq data, comprising of 3 key steps:\n(1) Dual Augmentation Module, which applies biologically informed perturbations\nto the gene expression matrix and cell graph relationships to enhance\nrepresentation robustness; (2) Siamese Fusion Module, which combines\ncross-correlation refinement and adaptive information fusion to capture complex\ncellular relationships while mitigating over-smoothing; and (3) Optimal\nTransport Clustering, which utilizes Sinkhorn distance to efficiently align\ncluster assignments with predefined proportions while maintaining balance.\nComprehensive evaluations on seven real-world datasets demonstrate\nthat~\\methodname~outperforms state-of-the-art methods in single-cell\nclustering, cell type annotation, and cell type classification, providing a\npowerful tool for scRNA-seq data interpretation."}
{"id": "2505.11788", "pdf": "https://arxiv.org/pdf/2505.11788", "abs": "https://arxiv.org/abs/2505.11788", "authors": ["Seungeun Oh", "Jinhyuk Kim", "Jihong Park", "Seung-Woo Ko", "Jinho Choi", "Tony Q. S. Quek", "Seong-Lyun Kim"], "title": "Communication-Efficient Hybrid Language Model via Uncertainty-Aware Opportunistic and Compressed Transmission", "categories": ["cs.DC", "cs.IT", "cs.LG", "cs.NI", "eess.SP", "math.IT"], "comment": "14 pages, 10 figures, 2 tables; This work has been submitted to the\n  IEEE for possible publication", "summary": "To support emerging language-based applications using dispersed and\nheterogeneous computing resources, the hybrid language model (HLM) offers a\npromising architecture, where an on-device small language model (SLM) generates\ndraft tokens that are validated and corrected by a remote large language model\n(LLM). However, the original HLM suffers from substantial communication\noverhead, as the LLM requires the SLM to upload the full vocabulary\ndistribution for each token. Moreover, both communication and computation\nresources are wasted when the LLM validates tokens that are highly likely to be\naccepted. To overcome these limitations, we propose communication-efficient and\nuncertainty-aware HLM (CU-HLM). In CU-HLM, the SLM transmits truncated\nvocabulary distributions only when its output uncertainty is high. We validate\nthe feasibility of this opportunistic transmission by discovering a strong\ncorrelation between SLM's uncertainty and LLM's rejection probability.\nFurthermore, we theoretically derive optimal uncertainty thresholds and optimal\nvocabulary truncation strategies. Simulation results show that, compared to\nstandard HLM, CU-HLM achieves up to 206$\\times$ higher token throughput by\nskipping 74.8% transmissions with 97.4% vocabulary compression, while\nmaintaining 97.4% accuracy."}
{"id": "2505.12630", "pdf": "https://arxiv.org/pdf/2505.12630", "abs": "https://arxiv.org/abs/2505.12630", "authors": ["Xiangpeng Tian", "Xiangyu Liao", "Xiao Liu", "Meng Li", "Chao Ren"], "title": "Degradation-Aware Feature Perturbation for All-in-One Image Restoration", "categories": ["cs.CV", "cs.AI", "I.4.5"], "comment": "Accepted to CVPR 2025. 8 pages, 7 figures", "summary": "All-in-one image restoration aims to recover clear images from various\ndegradation types and levels with a unified model. Nonetheless, the significant\nvariations among degradation types present challenges for training a universal\nmodel, often resulting in task interference, where the gradient update\ndirections of different tasks may diverge due to shared parameters. To address\nthis issue, motivated by the routing strategy, we propose DFPIR, a novel\nall-in-one image restorer that introduces Degradation-aware Feature\nPerturbations(DFP) to adjust the feature space to align with the unified\nparameter space. In this paper, the feature perturbations primarily include\nchannel-wise perturbations and attention-wise perturbations. Specifically,\nchannel-wise perturbations are implemented by shuffling the channels in\nhigh-dimensional space guided by degradation types, while attention-wise\nperturbations are achieved through selective masking in the attention space. To\nachieve these goals, we propose a Degradation-Guided Perturbation Block (DGPB)\nto implement these two functions, positioned between the encoding and decoding\nstages of the encoder-decoder architecture. Extensive experimental results\ndemonstrate that DFPIR achieves state-of-the-art performance on several\nall-in-one image restoration tasks including image denoising, image dehazing,\nimage deraining, motion deblurring, and low-light image enhancement. Our codes\nare available at https://github.com/TxpHome/DFPIR."}
{"id": "2505.11816", "pdf": "https://arxiv.org/pdf/2505.11816", "abs": "https://arxiv.org/abs/2505.11816", "authors": ["Quan Cheng", "Yuanyu Wan", "Lingyu Wu", "Chenping Hou", "Lijun Zhang"], "title": "Continuous Subspace Optimization for Continual Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Continual learning aims to learn multiple tasks sequentially while preserving\nprior knowledge, but faces the challenge of catastrophic forgetting when\nacquiring new knowledge. Recently, approaches leveraging pre-trained models\nhave gained increasing popularity to mitigate this issue, due to the strong\ngeneralization ability of foundation models. To adjust pre-trained models for\nnew tasks, existing methods usually employ low-rank adaptation, which restricts\nparameter updates to a fixed low-rank subspace. However, constraining the\noptimization space inherently compromises the model's learning capacity,\nresulting in inferior performance. To address the limitation, we propose\nContinuous Subspace Optimization for Continual Learning (CoSO) to fine-tune the\nmodel in a series of subspaces rather than a single one. These sequential\nsubspaces are dynamically determined through the singular value decomposition\nof gradients. CoSO updates the model by projecting gradients into these\nsubspaces, ensuring memory-efficient optimization. To mitigate forgetting, the\noptimization subspaces of each task are set to be orthogonal to the historical\ntask subspace. During task learning, CoSO maintains a task-specific component\nthat captures the critical update directions associated with the current task.\nUpon completing a task, this component is used to update the historical task\nsubspace, laying the groundwork for subsequent learning. Extensive experiments\non multiple datasets demonstrate that CoSO significantly outperforms\nstate-of-the-art methods, especially in challenging scenarios with long task\nsequences."}
{"id": "2505.12632", "pdf": "https://arxiv.org/pdf/2505.12632", "abs": "https://arxiv.org/abs/2505.12632", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation."}
{"id": "2505.11817", "pdf": "https://arxiv.org/pdf/2505.11817", "abs": "https://arxiv.org/abs/2505.11817", "authors": ["Yang Xiao", "Tianyi Peng", "Rohan Kumar Das", "Yuchen Hu", "Huiping Zhuang"], "title": "AnalyticKWS: Towards Exemplar-Free Analytic Class Incremental Learning for Small-footprint Keyword Spotting", "categories": ["eess.AS", "cs.LG", "cs.SD"], "comment": "Accepted by ACL 2025", "summary": "Keyword spotting (KWS) offers a vital mechanism to identify spoken commands\nin voice-enabled systems, where user demands often shift, requiring models to\nlearn new keywords continually over time. However, a major problem is\ncatastrophic forgetting, where models lose their ability to recognize earlier\nkeywords. Although several continual learning methods have proven their\nusefulness for reducing forgetting, most existing approaches depend on storing\nand revisiting old data to combat catastrophic forgetting. Though effective,\nthese methods face two practical challenges: 1) privacy risks from keeping user\ndata and 2) large memory and time consumption that limit deployment on small\ndevices. To address these issues, we propose an exemplar-free Analytic\nContinual Learning (AnalyticKWS) method that updates model parameters without\nrevisiting earlier data. Inspired by efficient learning principles, AnalyticKWS\ncomputes a closed-form analytical solution for model updates and requires only\na single epoch of adaptation for incoming keywords. AnalyticKWS demands fewer\ncomputational resources by avoiding gradient-based updates and does not store\nold data. By eliminating the need for back-propagation during incremental\nlearning, the model remains lightweight and efficient. As a result, AnalyticKWS\nmeets the challenges mentioned earlier and suits resource-limited settings\nwell. Extensive experiments on various datasets and settings show that\nAnalyticKWS consistently outperforms existing continual learning methods."}
{"id": "2505.12638", "pdf": "https://arxiv.org/pdf/2505.12638", "abs": "https://arxiv.org/abs/2505.12638", "authors": ["Yifeng Jiao", "Yuchen Liu", "Yu Zhang", "Xin Guo", "Yushuai Wu", "Chen Jiang", "Jiyang Li", "Hongwei Zhang", "Limei Han", "Xin Gao", "Yuan Qi", "Yuan Cheng"], "title": "ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data", "categories": ["q-bio.GN", "cs.AI", "cs.CE", "cs.LG"], "comment": null, "summary": "The advent of single-cell Assay for Transposase-Accessible Chromatin using\nsequencing (scATAC-seq) offers an innovative perspective for deciphering\nregulatory mechanisms by assembling a vast repository of single-cell chromatin\naccessibility data. While foundation models have achieved significant success\nin single-cell transcriptomics, there is currently no foundation model for\nscATAC-seq that supports zero-shot high-quality cell identification and\ncomprehensive multi-omics analysis simultaneously. Key challenges lie in the\nhigh dimensionality and sparsity of scATAC-seq data, as well as the lack of a\nstandardized schema for representing open chromatin regions (OCRs). Here, we\npresent \\textbf{ChromFound}, a foundation model tailored for scATAC-seq.\nChromFound utilizes a hybrid architecture and genome-aware tokenization to\neffectively capture genome-wide long contexts and regulatory signals from\ndynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues\nand 6 disease conditions, ChromFound demonstrates broad applicability across 6\ndiverse tasks. Notably, it achieves robust zero-shot performance in generating\nuniversal cell representations and exhibits excellent transferability in cell\ntype annotation and cross-omics prediction. By uncovering enhancer-gene links\nundetected by existing computational methods, ChromFound offers a promising\nframework for understanding disease risk variants in the noncoding genome."}
{"id": "2505.11843", "pdf": "https://arxiv.org/pdf/2505.11843", "abs": "https://arxiv.org/abs/2505.11843", "authors": ["Junlang Huang", "Hao Chen", "Li Luo", "Yong Cai", "Lexin Zhang", "Tianhao Ma", "Yitian Zhang", "Zhong Guan"], "title": "S-Crescendo: A Nested Transformer Weaving Framework for Scalable Nonlinear System in S-Domain Representation", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Simulation of high-order nonlinear system requires extensive computational\nresources, especially in modern VLSI backend design where bifurcation-induced\ninstability and chaos-like transient behaviors pose challenges. We present\nS-Crescendo - a nested transformer weaving framework that synergizes S-domain\nwith neural operators for scalable time-domain prediction in high-order\nnonlinear networks, alleviating the computational bottlenecks of conventional\nsolvers via Newton-Raphson method. By leveraging the partial-fraction\ndecomposition of an n-th order transfer function into first-order modal terms\nwith repeated poles and residues, our method bypasses the conventional Jacobian\nmatrix-based iterations and efficiently reduces computational complexity from\ncubic $O(n^3)$ to linear $O(n)$.The proposed architecture seamlessly integrates\nan S-domain encoder with an attention-based correction operator to\nsimultaneously isolate dominant response and adaptively capture higher-order\nnon-linearities. Validated on order-1 to order-10 networks, our method achieves\nup to 0.99 test-set ($R^2$) accuracy against HSPICE golden waveforms and\naccelerates simulation by up to 18(X), providing a scalable, physics-aware\nframework for high-dimensional nonlinear modeling."}
{"id": "2505.12641", "pdf": "https://arxiv.org/pdf/2505.12641", "abs": "https://arxiv.org/abs/2505.12641", "authors": ["Yue Huang", "Zi'ang Li", "Tianle Hu", "Jie Wen", "Guanbin Li", "Jinglin Zhang", "Guoxu Zhou", "Xiaozhao Fang"], "title": "Single Image Reflection Removal via inter-layer Complementarity", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Although dual-stream architectures have achieved remarkable success in single\nimage reflection removal, they fail to fully exploit inter-layer\ncomplementarity in their physical modeling and network design, which limits the\nquality of image separation. To address this fundamental limitation, we propose\ntwo targeted improvements to enhance dual-stream architectures: First, we\nintroduce a novel inter-layer complementarity model where low-frequency\ncomponents extracted from the residual layer interact with the transmission\nlayer through dual-stream architecture to enhance inter-layer complementarity.\nMeanwhile, high-frequency components from the residual layer provide inverse\nmodulation to both streams, improving the detail quality of the transmission\nlayer. Second, we propose an efficient inter-layer complementarity attention\nmechanism which first cross-reorganizes dual streams at the channel level to\nobtain reorganized streams with inter-layer complementary structures, then\nperforms attention computation on the reorganized streams to achieve better\ninter-layer separation, and finally restores the original stream structure for\noutput. Experimental results demonstrate that our method achieves\nstate-of-the-art separation quality on multiple public datasets while\nsignificantly reducing both computational cost and model complexity."}
{"id": "2505.11849", "pdf": "https://arxiv.org/pdf/2505.11849", "abs": "https://arxiv.org/abs/2505.11849", "authors": ["Yiting Wang", "Guoheng Sun", "Wanghao Ye", "Gang Qu", "Ang Li"], "title": "VeriReason: Reinforcement Learning with Testbench Feedback for Reasoning-Enhanced Verilog Generation", "categories": ["cs.AI", "cs.AR", "cs.LG", "cs.PL"], "comment": "11 pages, 2 figures", "summary": "Automating Register Transfer Level (RTL) code generation using Large Language\nModels (LLMs) offers substantial promise for streamlining digital circuit\ndesign and reducing human effort. However, current LLM-based approaches face\nsignificant challenges with training data scarcity, poor specification-code\nalignment, lack of verification mechanisms, and balancing generalization with\nspecialization. Inspired by DeepSeek-R1, we introduce VeriReason, a framework\nintegrating supervised fine-tuning with Guided Reward Proximal Optimization\n(GRPO) reinforcement learning for RTL generation. Using curated training\nexamples and a feedback-driven reward model, VeriReason combines testbench\nevaluations with structural heuristics while embedding self-checking\ncapabilities for autonomous error correction. On the VerilogEval Benchmark,\nVeriReason delivers significant improvements: achieving 83.1% functional\ncorrectness on the VerilogEval Machine benchmark, substantially outperforming\nboth comparable-sized models and much larger commercial systems like GPT-4\nTurbo. Additionally, our approach demonstrates up to a 2.8X increase in\nfirst-attempt functional correctness compared to baseline methods and exhibits\nrobust generalization to unseen designs. To our knowledge, VeriReason\nrepresents the first system to successfully integrate explicit reasoning\ncapabilities with reinforcement learning for Verilog generation, establishing a\nnew state-of-the-art for automated RTL synthesis. The models and datasets are\navailable at: https://huggingface.co/collections/AI4EDA-CASE Code is Available\nat: https://github.com/NellyW8/VeriReason"}
{"id": "2505.12650", "pdf": "https://arxiv.org/pdf/2505.12650", "abs": "https://arxiv.org/abs/2505.12650", "authors": ["Yaotian Yang", "Yiwen Tang", "Yizhe Chen", "Xiao Chen", "Jiangjie Qiu", "Hao Xiong", "Haoyu Yin", "Zhiyao Luo", "Yifei Zhang", "Sijia Tao", "Wentao Li", "Qinghua Zhang", "Yuqiang Li", "Wanli Ouyang", "Bin Zhao", "Xiaonan Wang", "Fei Wei"], "title": "AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use", "categories": ["cs.CV", "cs.AI"], "comment": "The code and dataset are publicly available at\n  https://github.com/yyt-2378/AutoMat and\n  https://huggingface.co/datasets/yaotianvector/STEM2Mat", "summary": "Machine learning-based interatomic potentials and force fields depend\ncritically on accurate atomic structures, yet such data are scarce due to the\nlimited availability of experimentally resolved crystals. Although\natomic-resolution electron microscopy offers a potential source of structural\ndata, converting these images into simulation-ready formats remains\nlabor-intensive and error-prone, creating a bottleneck for model training and\nvalidation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that\nautomatically transforms scanning transmission electron microscopy (STEM)\nimages into atomic crystal structures and predicts their physical properties.\nAutoMat combines pattern-adaptive denoising, physics-guided template retrieval,\nsymmetry-aware atomic reconstruction, fast relaxation and property prediction\nvia MatterSim, and coordinated orchestration across all stages. We propose the\nfirst dedicated STEM2Mat-Bench for this task and evaluate performance using\nlattice RMSD, formation energy MAE, and structure-matching success rate. By\norchestrating external tool calls, AutoMat enables a text-only LLM to\noutperform vision-language models in this domain, achieving closed-loop\nreasoning throughout the pipeline. In large-scale experiments over 450\nstructure samples, AutoMat substantially outperforms existing multimodal large\nlanguage models and tools. These results validate both AutoMat and\nSTEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic\nsimulation in materials science.The code and dataset are publicly available at\nhttps://github.com/yyt-2378/AutoMat and\nhttps://huggingface.co/datasets/yaotianvector/STEM2Mat."}
{"id": "2505.11853", "pdf": "https://arxiv.org/pdf/2505.11853", "abs": "https://arxiv.org/abs/2505.11853", "authors": ["Chicago Y. Park", "Shirin Shoushtari", "Hongyu An", "Ulugbek S. Kamilov"], "title": "Measurement Score-Based Diffusion Model", "categories": ["eess.IV", "cs.LG"], "comment": null, "summary": "Diffusion models are widely used in applications ranging from image\ngeneration to inverse problems. However, training diffusion models typically\nrequires clean ground-truth images, which are unavailable in many applications.\nWe introduce the Measurement Score-based diffusion Model (MSM), a novel\nframework that learns partial measurement scores using only noisy and\nsubsampled measurements. MSM models the distribution of full measurements as an\nexpectation over partial scores induced by randomized subsampling. To make the\nMSM representation computationally efficient, we also develop a stochastic\nsampling algorithm that generates full images by using a randomly selected\nsubset of partial scores at each step. We additionally propose a new posterior\nsampling method for solving inverse problems that reconstructs images using\nthese partial scores. We provide a theoretical analysis that bounds the\nKullback-Leibler divergence between the distributions induced by full and\nstochastic sampling, establishing the accuracy of the proposed algorithm. We\ndemonstrate the effectiveness of MSM on natural images and multi-coil MRI,\nshowing that it can generate high-quality images and solve inverse problems --\nall without access to clean training data. Code is available at\nhttps://github.com/wustl-cig/MSM."}
{"id": "2505.12654", "pdf": "https://arxiv.org/pdf/2505.12654", "abs": "https://arxiv.org/abs/2505.12654", "authors": ["Yuxin Lin", "Yinglin Zheng", "Ming Zeng", "Wangzheng Shi"], "title": "Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals", "categories": ["cs.CL", "cs.AI"], "comment": "Accepected by ACL 2025", "summary": "This paper addresses the gap in predicting turn-taking and backchannel\nactions in human-machine conversations using multi-modal signals (linguistic,\nacoustic, and visual). To overcome the limitation of existing datasets, we\npropose an automatic data collection pipeline that allows us to collect and\nannotate over 210 hours of human conversation videos. From this, we construct a\nMulti-Modal Face-to-Face (MM-F2F) human conversation dataset, including over\n1.5M words and corresponding turn-taking and backchannel annotations from\napproximately 20M frames. Additionally, we present an end-to-end framework that\npredicts the probability of turn-taking and backchannel actions from\nmulti-modal signals. The proposed model emphasizes the interrelation between\nmodalities and supports any combination of text, audio, and video inputs,\nmaking it adaptable to a variety of realistic scenarios. Our experiments show\nthat our approach achieves state-of-the-art performance on turn-taking and\nbackchannel prediction tasks, achieving a 10\\% increase in F1-score on\nturn-taking and a 33\\% increase on backchannel prediction. Our dataset and code\nare publicly available online to ease of subsequent research."}
{"id": "2505.11882", "pdf": "https://arxiv.org/pdf/2505.11882", "abs": "https://arxiv.org/abs/2505.11882", "authors": ["Shiming Chen", "Dingjie Fu", "Salman Khan", "Fahad Shahbaz Khan"], "title": "GenZSL: Generative Zero-Shot Learning Via Inductive Variational Autoencoder", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICML'25", "summary": "Remarkable progress in zero-shot learning (ZSL) has been achieved using\ngenerative models. However, existing generative ZSL methods merely generate\n(imagine) the visual features from scratch guided by the strong class semantic\nvectors annotated by experts, resulting in suboptimal generative performance\nand limited scene generalization. To address these and advance ZSL, we propose\nan inductive variational autoencoder for generative zero-shot learning, dubbed\nGenZSL. Mimicking human-level concept learning, GenZSL operates by inducting\nnew class samples from similar seen classes using weak class semantic vectors\nderived from target class names (i.e., CLIP text embedding). To ensure the\ngeneration of informative samples for training an effective ZSL classifier, our\nGenZSL incorporates two key strategies. Firstly, it employs class diversity\npromotion to enhance the diversity of class semantic vectors. Secondly, it\nutilizes target class-guided information boosting criteria to optimize the\nmodel. Extensive experiments conducted on three popular benchmark datasets\nshowcase the superiority and potential of our GenZSL with significant efficacy\nand efficiency over f-VAEGAN, e.g., 24.7% performance gains and more than\n$60\\times$ faster training speed on AWA2. Codes are available at\nhttps://github.com/shiming-chen/GenZSL."}
{"id": "2505.12655", "pdf": "https://arxiv.org/pdf/2505.12655", "abs": "https://arxiv.org/abs/2505.12655", "authors": ["Yisheng Zhong", "Yizhu Wen", "Junfeng Guo", "Mehran Kafai", "Heng Huang", "Hanqing Guo", "Zhuangdi Zhu"], "title": "Web IP at Risk: Prevent Unauthorized Real-Time Retrieval by Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": "13 pages, 13 figures, 4 tables", "summary": "Protecting cyber Intellectual Property (IP) such as web content is an\nincreasingly critical concern. The rise of large language models (LLMs) with\nonline retrieval capabilities presents a double-edged sword that enables\nconvenient access to information but often undermines the rights of original\ncontent creators. As users increasingly rely on LLM-generated responses, they\ngradually diminish direct engagement with original information sources,\nsignificantly reducing the incentives for IP creators to contribute, and\nleading to a saturating cyberspace with more AI-generated content. In response,\nwe propose a novel defense framework that empowers web content creators to\nsafeguard their web-based IP from unauthorized LLM real-time extraction by\nleveraging the semantic understanding capability of LLMs themselves. Our method\nfollows principled motivations and effectively addresses an intractable\nblack-box optimization problem. Real-world experiments demonstrated that our\nmethods improve defense success rates from 2.5% to 88.6% on different LLMs,\noutperforming traditional defenses such as configuration-based restrictions."}
{"id": "2505.11910", "pdf": "https://arxiv.org/pdf/2505.11910", "abs": "https://arxiv.org/abs/2505.11910", "authors": ["Peter Vereš", "Richard Cloete", "Matthew J. Payne", "Abraham Loeb"], "title": "Improving the discovery of near-Earth objects with machine-learning methods", "categories": ["astro-ph.IM", "astro-ph.EP", "cs.LG"], "comment": "13 pages, 16 figures, 11 tables", "summary": "We present a comprehensive analysis of the digest2 parameters for candidates\nof the Near-Earth Object Confirmation Page (NEOCP) that were reported between\n2019 and 2024. Our study proposes methods for significantly reducing the\ninclusion of non-NEO objects on the NEOCP. Despite the substantial increase in\nnear-Earth object (NEO) discoveries in recent years, only about half of the\nNEOCP candidates are ultimately confirmed as NEOs. Therefore, much observing\ntime is spent following up on non-NEOs. Furthermore, approximately 11% of the\ncandidates remain unconfirmed because the follow-up observations are\ninsufficient. These are nearly 600 cases per year. To reduce false positives\nand minimize wasted resources on non-NEOs, we refine the posting criteria for\nNEOCP based on a detailed analysis of all digest2 scores. We investigated 30\ndistinct digest2 parameter categories for candidates that were confirmed as\nNEOs and non-NEOs. From this analysis, we derived a filtering mechanism based\non selected digest2 parameters that were able to exclude 20% of the non-NEOs\nfrom the NEOCP while maintaining a minimal loss of true NEOs. We also\ninvestigated the application of four machine-learning (ML) techniques, that is,\nthe gradient-boosting machine (GBM), the random forest (RF) classifier, the\nstochastic gradient descent (SGD) classifier, and neural networks (NN) to\nclassify NEOCP candidates as NEOs or non-NEOs. Based on digest2 parameters as\ninput, our ML models achieved a precision of approximately 95% in\ndistinguishing between NEOs and non-NEOs. Results. Combining the digest2\nparameter filter with an ML-based classification model, we demonstrate a\nsignificant reduction in non-NEOs on the NEOCP that exceeds 80%, while limiting\nthe loss of NEO discovery tracklets to 5.5%. Importantly, we show that most\nfollow-up tracklets of initially misclassified NEOs are later correctly\nidentified as NEOs."}
{"id": "2505.12662", "pdf": "https://arxiv.org/pdf/2505.12662", "abs": "https://arxiv.org/abs/2505.12662", "authors": ["Xukai Liu", "Ye Liu", "Shiwen Wu", "Yanghai Zhang", "Yihao Yuan", "Kai Zhang", "Qi Liu"], "title": "Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have led to impressive\nprogress in natural language generation, yet their tendency to produce\nhallucinated or unsubstantiated content remains a critical concern. To improve\nfactual reliability, Retrieval-Augmented Generation (RAG) integrates external\nknowledge during inference. However, existing RAG systems face two major\nlimitations: (1) unreliable adaptive control due to limited external knowledge\nsupervision, and (2) hallucinations caused by inaccurate or irrelevant\nreferences. To address these issues, we propose Know3-RAG, a knowledge-aware\nRAG framework that leverages structured knowledge from knowledge graphs (KGs)\nto guide three core stages of the RAG process, including retrieval, generation,\nand filtering. Specifically, we introduce a knowledge-aware adaptive retrieval\nmodule that employs KG embedding to assess the confidence of the generated\nanswer and determine retrieval necessity, a knowledge-enhanced reference\ngeneration strategy that enriches queries with KG-derived entities to improve\ngenerated reference relevance, and a knowledge-driven reference filtering\nmechanism that ensures semantic alignment and factual accuracy of references.\nExperiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG\nconsistently outperforms strong baselines, significantly reducing\nhallucinations and enhancing answer reliability."}
{"id": "2505.11924", "pdf": "https://arxiv.org/pdf/2505.11924", "abs": "https://arxiv.org/abs/2505.11924", "authors": ["Yu-Ting Lee", "Hui-Ying Shih", "Fu-Chieh Chang", "Pei-Yuan Wu"], "title": "An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We provide an explanation for the performance gains of intrinsic\nself-correction, a process where a language model iteratively refines its\noutputs without external feedback. More precisely, we investigate how prompting\ninduces interpretable changes in hidden states and thus affects the output\ndistributions. We hypothesize that each prompt-induced shift lies in a linear\nspan of some linear representation vectors, naturally separating tokens based\non individual concept alignment. Building around this idea, we give a\nmathematical formulation of self-correction and derive a concentration result\nfor output tokens based on alignment magnitudes. Our experiments on text\ndetoxification with zephyr-7b-sft reveal a substantial gap in the inner\nproducts of the prompt-induced shifts and the unembeddings of the top-100 most\ntoxic tokens vs. those of the unembeddings of the bottom-100 least toxic\ntokens, under toxic instructions. This suggests that self-correction prompts\nenhance a language model's capability of latent concept recognition. Our\nanalysis offers insights into the underlying mechanism of self-correction by\ncharacterizing how prompting works explainably. For reproducibility, our code\nis available."}
{"id": "2505.12664", "pdf": "https://arxiv.org/pdf/2505.12664", "abs": "https://arxiv.org/abs/2505.12664", "authors": ["Ziqing Xing", "Zhaoyang Zhang", "Zirui Chen", "Hongning Ruan", "Zhaohui Yang"], "title": "Multi-View Wireless Sensing via Conditional Generative Learning: Framework and Model Design", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "submitted to IEEE Transactions on Wireless Communications", "summary": "In this paper, we incorporate physical knowledge into learning-based\nhigh-precision target sensing using the multi-view channel state information\n(CSI) between multiple base stations (BSs) and user equipment (UEs). Such kind\nof multi-view sensing problem can be naturally cast into a conditional\ngeneration framework. To this end, we design a bipartite neural network\narchitecture, the first part of which uses an elaborately designed encoder to\nfuse the latent target features embedded in the multi-view CSI, and then the\nsecond uses them as conditioning inputs of a powerful generative model to guide\nthe target's reconstruction. Specifically, the encoder is designed to capture\nthe physical correlation between the CSI and the target, and also be adaptive\nto the numbers and positions of BS-UE pairs. Therein the view-specific nature\nof CSI is assimilated by introducing a spatial positional embedding scheme,\nwhich exploits the structure of electromagnetic(EM)-wave propagation channels.\nFinally, a conditional diffusion model with a weighted loss is employed to\ngenerate the target's point cloud from the fused features. Extensive numerical\nresults demonstrate that the proposed generative multi-view (Gen-MV) sensing\nframework exhibits excellent flexibility and significant performance\nimprovement on the reconstruction quality of target's shape and EM properties."}
{"id": "2505.11939", "pdf": "https://arxiv.org/pdf/2505.11939", "abs": "https://arxiv.org/abs/2505.11939", "authors": ["Haitao Li", "Che Liu", "Zhengyao Ding", "Ziyi Liu", "Zhengxing Huang"], "title": "Fine-Grained ECG-Text Contrastive Learning via Waveform Understanding Enhancement", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "Electrocardiograms (ECGs) are essential for diagnosing cardiovascular\ndiseases. While previous ECG-text contrastive learning methods have shown\npromising results, they often overlook the incompleteness of the reports. Given\nan ECG, the report is generated by first identifying key waveform features and\nthen inferring the final diagnosis through these features. Despite their\nimportance, these waveform features are often not recorded in the report as\nintermediate results. Aligning ECGs with such incomplete reports impedes the\nmodel's ability to capture the ECG's waveform features and limits its\nunderstanding of diagnostic reasoning based on those features. To address this,\nwe propose FG-CLEP (Fine-Grained Contrastive Language ECG Pre-training), which\naims to recover these waveform features from incomplete reports with the help\nof large language models (LLMs), under the challenges of hallucinations and the\nnon-bijective relationship between waveform features and diagnoses.\nAdditionally, considering the frequent false negatives due to the prevalence of\ncommon diagnoses in ECGs, we introduce a semantic similarity matrix to guide\ncontrastive learning. Furthermore, we adopt a sigmoid-based loss function to\naccommodate the multi-label nature of ECG-related tasks. Experiments on six\ndatasets demonstrate that FG-CLEP outperforms state-of-the-art methods in both\nzero-shot prediction and linear probing across these datasets."}
{"id": "2505.12669", "pdf": "https://arxiv.org/pdf/2505.12669", "abs": "https://arxiv.org/abs/2505.12669", "authors": ["Abhinaba Roy", "Geeta Puri", "Dorien Herremans"], "title": "Text2midi-InferAlign: Improving Symbolic Music Generation with Inference-Time Alignment", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS", "68T07", "I.2.1"], "comment": "7 pages, 1 figure, 5 tables", "summary": "We present Text2midi-InferAlign, a novel technique for improving symbolic\nmusic generation at inference time. Our method leverages text-to-audio\nalignment and music structural alignment rewards during inference to encourage\nthe generated music to be consistent with the input caption. Specifically, we\nintroduce two objectives scores: a text-audio consistency score that measures\nrhythmic alignment between the generated music and the original text caption,\nand a harmonic consistency score that penalizes generated music containing\nnotes inconsistent with the key. By optimizing these alignment-based objectives\nduring the generation process, our model produces symbolic music that is more\nclosely tied to the input captions, thereby improving the overall quality and\ncoherence of the generated compositions. Our approach can extend any existing\nautoregressive model without requiring further training or fine-tuning. We\nevaluate our work on top of Text2midi - an existing text-to-midi generation\nmodel, demonstrating significant improvements in both objective and subjective\nevaluation metrics."}
{"id": "2505.11946", "pdf": "https://arxiv.org/pdf/2505.11946", "abs": "https://arxiv.org/abs/2505.11946", "authors": ["Adam Kovari", "Yasin Ghafourian", "Csaba Hegedus", "Belal Abu Naim", "Kitti Mezei", "Pal Varga", "Markus Tauber"], "title": "Let's have a chat with the EU AI Act", "categories": ["cs.IR", "cs.AI", "cs.CY", "cs.DL", "cs.LG"], "comment": null, "summary": "As artificial intelligence (AI) regulations evolve and the regulatory\nlandscape develops and becomes more complex, ensuring compliance with ethical\nguidelines and legal frameworks remains a challenge for AI developers. This\npaper introduces an AI-driven self-assessment chatbot designed to assist users\nin navigating the European Union AI Act and related standards. Leveraging a\nRetrieval-Augmented Generation (RAG) framework, the chatbot enables real-time,\ncontext-aware compliance verification by retrieving relevant regulatory texts\nand providing tailored guidance. By integrating both public and proprietary\nstandards, it streamlines regulatory adherence, reduces complexity, and fosters\nresponsible AI development. The paper explores the chatbot's architecture,\ncomparing naive and graph-based RAG models, and discusses its potential impact\non AI governance."}
{"id": "2505.12684", "pdf": "https://arxiv.org/pdf/2505.12684", "abs": "https://arxiv.org/abs/2505.12684", "authors": ["Yinlin Zhu", "Xunkai Li", "Jishuo Jia", "Miao Hu", "Di Wu", "Meikang Qiu"], "title": "Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.SI"], "comment": "Under Review", "summary": "Recent advances in graph machine learning have shifted to data-centric\nparadigms, driven by two emerging fields: (1) Federated graph learning (FGL)\nenables multi-client collaboration but faces challenges from data and task\nheterogeneity, limiting its practicality; (2) Graph foundation models (GFM)\noffer strong domain generalization but are usually trained on single machines,\nmissing out on cross-silo data and resources.\n  These paradigms are complementary, and their integration brings notable\nbenefits. Motivated by this, we propose FedGFM, a novel decentralized GFM\ntraining paradigm. However, a key challenge is knowledge entanglement, where\nmulti-domain knowledge merges into indistinguishable representations, hindering\ndownstream adaptation.\n  To address this, we present FedGFM+, an enhanced framework with two core\nmodules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based\ndomain-aware initialization strategy. Before pre-training, each client encodes\nits local graph into domain-specific prototypes that serve as semantic anchors.\nSynthetic embeddings around these anchors initialize the global model. We\ntheoretically prove these prototypes are distinguishable across domains,\nproviding a strong inductive bias to disentangle domain-specific knowledge. (2)\nAdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a\nlightweight graph prompt capturing domain semantics during pre-training. During\nfine-tuning, prompts from all clients form a pool from which the GFM selects\nrelevant prompts to augment target graph attributes, improving downstream\nadaptation.\n  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and\ntasks, outperforming 20 baselines from supervised learning, FGL, and federated\nGFM variants."}
{"id": "2505.11984", "pdf": "https://arxiv.org/pdf/2505.11984", "abs": "https://arxiv.org/abs/2505.11984", "authors": ["Jitendra K Tugnait"], "title": "Multi-Attribute Graph Estimation with Sparse-Group Non-Convex Penalties", "categories": ["stat.ML", "cs.LG", "eess.SP"], "comment": "16 pages, 1 figure, 1 table, published in IEEE Access, vol. 13, pp.\n  80174-80190, 2025. arXiv admin note: text overlap with arXiv:2505.09748", "summary": "We consider the problem of inferring the conditional independence graph (CIG)\nof high-dimensional Gaussian vectors from multi-attribute data. Most existing\nmethods for graph estimation are based on single-attribute models where one\nassociates a scalar random variable with each node. In multi-attribute\ngraphical models, each node represents a random vector. In this paper we\nprovide a unified theoretical analysis of multi-attribute graph learning using\na penalized log-likelihood objective function. We consider both convex\n(sparse-group lasso) and sparse-group non-convex (log-sum and smoothly clipped\nabsolute deviation (SCAD) penalties) penalty/regularization functions. An\nalternating direction method of multipliers (ADMM) approach coupled with local\nlinear approximation to non-convex penalties is presented for optimization of\nthe objective function. For non-convex penalties, theoretical analysis\nestablishing local consistency in support recovery, local convexity and\nprecision matrix estimation in high-dimensional settings is provided under two\nsets of sufficient conditions: with and without some irrepresentability\nconditions. We illustrate our approaches using both synthetic and real-data\nnumerical examples. In the synthetic data examples the sparse-group log-sum\npenalized objective function significantly outperformed the lasso penalized as\nwell as SCAD penalized objective functions with $F_1$-score and Hamming\ndistance as performance metrics."}
{"id": "2505.12701", "pdf": "https://arxiv.org/pdf/2505.12701", "abs": "https://arxiv.org/abs/2505.12701", "authors": ["Shuyang Dong", "Shangtong Zhang", "Lu Feng"], "title": "Counterfactual Explanations for Continuous Action Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by International Joint Conference on Artificial Intelligence\n  (IJCAI) 2025", "summary": "Reinforcement Learning (RL) has shown great promise in domains like\nhealthcare and robotics but often struggles with adoption due to its lack of\ninterpretability. Counterfactual explanations, which address \"what if\"\nscenarios, provide a promising avenue for understanding RL decisions but remain\nunderexplored for continuous action spaces. We propose a novel approach for\ngenerating counterfactual explanations in continuous action RL by computing\nalternative action sequences that improve outcomes while minimizing deviations\nfrom the original sequence. Our approach leverages a distance metric for\ncontinuous actions and accounts for constraints such as adhering to predefined\npolicies in specific states. Evaluations in two RL domains, Diabetes Control\nand Lunar Lander, demonstrate the effectiveness, efficiency, and generalization\nof our approach, enabling more interpretable and trustworthy RL applications."}
{"id": "2505.12010", "pdf": "https://arxiv.org/pdf/2505.12010", "abs": "https://arxiv.org/abs/2505.12010", "authors": ["Drashthi Doshi", "Aditya Vema Reddy Kesari", "Swaprava Nath", "Avishek Ghosh", "Suhas S Kowshik"], "title": "Incentivize Contribution and Learn Parameters Too: Federated Learning with Strategic Data Owners", "categories": ["cs.GT", "cs.LG", "cs.MA"], "comment": "19 pages, 12 figures, under review", "summary": "Classical federated learning (FL) assumes that the clients have a limited\namount of noisy data with which they voluntarily participate and contribute\ntowards learning a global, more accurate model in a principled manner. The\nlearning happens in a distributed fashion without sharing the data with the\ncenter. However, these methods do not consider the incentive of an agent for\nparticipating and contributing to the process, given that data collection and\nrunning a distributed algorithm is costly for the clients. The question of\nrationality of contribution has been asked recently in the literature and some\nresults exist that consider this problem. This paper addresses the question of\nsimultaneous parameter learning and incentivizing contribution, which\ndistinguishes it from the extant literature. Our first mechanism incentivizes\neach client to contribute to the FL process at a Nash equilibrium and\nsimultaneously learn the model parameters. However, this equilibrium outcome\ncan be away from the optimal, where clients contribute with their full data and\nthe algorithm learns the optimal parameters. We propose a second mechanism with\nmonetary transfers that is budget balanced and enables the full data\ncontribution along with optimal parameter learning. Large scale experiments\nwith real (federated) datasets (CIFAR-10, FeMNIST, and Twitter) show that these\nalgorithms converge quite fast in practice, yield good welfare guarantees, and\nbetter model performance for all agents."}
{"id": "2505.12705", "pdf": "https://arxiv.org/pdf/2505.12705", "abs": "https://arxiv.org/abs/2505.12705", "authors": ["Joel Jang", "Seonghyeon Ye", "Zongyu Lin", "Jiannan Xiang", "Johan Bjorck", "Yu Fang", "Fengyuan Hu", "Spencer Huang", "Kaushil Kundalia", "Yen-Chen Lin", "Loic Magne", "Ajay Mandlekar", "Avnish Narayan", "You Liang Tan", "Guanzhi Wang", "Jing Wang", "Qi Wang", "Yinzhen Xu", "Xiaohui Zeng", "Kaiyuan Zheng", "Ruijie Zheng", "Ming-Yu Liu", "Luke Zettlemoyer", "Dieter Fox", "Jan Kautz", "Scott Reed", "Yuke Zhu", "Linxi Fan"], "title": "DreamGen: Unlocking Generalization in Robot Learning through Neural Trajectories", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "See website for videos:\n  https://research.nvidia.com/labs/gear/dreamgen", "summary": "We introduce DreamGen, a simple yet highly effective 4-stage pipeline for\ntraining robot policies that generalize across behaviors and environments\nthrough neural trajectories - synthetic robot data generated from video world\nmodels. DreamGen leverages state-of-the-art image-to-video generative models,\nadapting them to the target robot embodiment to produce photorealistic\nsynthetic videos of familiar or novel tasks in diverse environments. Since\nthese models generate only videos, we recover pseudo-action sequences using\neither a latent action model or an inverse-dynamics model (IDM). Despite its\nsimplicity, DreamGen unlocks strong behavior and environment generalization: a\nhumanoid robot can perform 22 new behaviors in both seen and unseen\nenvironments, while requiring teleoperation data from only a single\npick-and-place task in one environment. To evaluate the pipeline\nsystematically, we introduce DreamGen Bench, a video generation benchmark that\nshows a strong correlation between benchmark performance and downstream policy\nsuccess. Our work establishes a promising new axis for scaling robot learning\nwell beyond manual data collection."}
{"id": "2505.12019", "pdf": "https://arxiv.org/pdf/2505.12019", "abs": "https://arxiv.org/abs/2505.12019", "authors": ["Jianyi Zhang", "Ziyin Zhou", "Yilong Li", "Qichao Jin"], "title": "FL-PLAS: Federated Learning with Partial Layer Aggregation for Backdoor Defense Against High-Ratio Malicious Clients", "categories": ["cs.CR", "cs.LG"], "comment": "20pages", "summary": "Federated learning (FL) is gaining increasing attention as an emerging\ncollaborative machine learning approach, particularly in the context of\nlarge-scale computing and data systems. However, the fundamental algorithm of\nFL, Federated Averaging (FedAvg), is susceptible to backdoor attacks. Although\nresearchers have proposed numerous defense algorithms, two significant\nchallenges remain. The attack is becoming more stealthy and harder to detect,\nand current defense methods are unable to handle 50\\% or more malicious users\nor assume an auxiliary server dataset.\n  To address these challenges, we propose a novel defense algorithm, FL-PLAS,\n\\textbf{F}ederated \\textbf{L}earning based on \\textbf{P}artial\\textbf{ L}ayer\n\\textbf{A}ggregation \\textbf{S}trategy. In particular, we divide the local\nmodel into a feature extractor and a classifier. In each iteration, the clients\nonly upload the parameters of a feature extractor after local training. The\nserver then aggregates these local parameters and returns the results to the\nclients.\n  Each client retains its own classifier layer, ensuring that the backdoor\nlabels do not impact other clients. We assess the effectiveness of FL-PLAS\nagainst state-of-the-art (SOTA) backdoor attacks on three image datasets and\ncompare our approach to six defense strategies. The results of the experiment\ndemonstrate that our methods can effectively protect local models from backdoor\nattacks. Without requiring any auxiliary dataset for the server, our method\nachieves a high main-task accuracy with a lower backdoor accuracy even under\nthe condition of 90\\% malicious users with the attacks of trigger, semantic and\nedge-case."}
{"id": "2505.12707", "pdf": "https://arxiv.org/pdf/2505.12707", "abs": "https://arxiv.org/abs/2505.12707", "authors": ["Yingchen He", "Christian D. Weilbach", "Martyna E. Wojciechowska", "Yuxuan Zhang", "Frank Wood"], "title": "PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "9 pages, 8 figures", "summary": "Advances in deep generative modelling have made it increasingly plausible to\ntrain human-level embodied agents. Yet progress has been limited by the absence\nof large-scale, real-time, multi-modal, and socially interactive datasets that\nreflect the sensory-motor complexity of natural environments. To address this,\nwe present PLAICraft, a novel data collection platform and dataset capturing\nmultiplayer Minecraft interactions across five time-aligned modalities: video,\ngame output audio, microphone input audio, mouse, and keyboard actions. Each\nmodality is logged with millisecond time precision, enabling the study of\nsynchronous, embodied behaviour in a rich, open-ended world. The dataset\ncomprises over 10,000 hours of gameplay from more than 10,000 global\nparticipants.\\footnote{We have done a privacy review for the public release of\nan initial 200-hour subset of the dataset, with plans to release most of the\ndataset over time.} Alongside the dataset, we provide an evaluation suite for\nbenchmarking model capabilities in object recognition, spatial awareness,\nlanguage grounding, and long-term memory. PLAICraft opens a path toward\ntraining and evaluating agents that act fluently and purposefully in real time,\npaving the way for truly embodied artificial intelligence."}
{"id": "2505.12050", "pdf": "https://arxiv.org/pdf/2505.12050", "abs": "https://arxiv.org/abs/2505.12050", "authors": ["Vinod Raman", "Hilal Asi", "Satyen Kale"], "title": "ABoN: Adaptive Best-of-N Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Recent advances in test-time alignment methods, such as Best-of-N sampling,\noffer a simple and effective way to steer language models (LMs) toward\npreferred behaviors using reward models (RM). However, these approaches can be\ncomputationally expensive, especially when applied uniformly across prompts\nwithout accounting for differences in alignment difficulty. In this work, we\npropose a prompt-adaptive strategy for Best-of-N alignment that allocates\ninference-time compute more efficiently. Motivated by latency concerns, we\ndevelop a two-stage algorithm: an initial exploratory phase estimates the\nreward distribution for each prompt using a small exploration budget, and a\nsecond stage adaptively allocates the remaining budget using these estimates.\nOur method is simple, practical, and compatible with any LM/RM combination.\nEmpirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different\nbatches of prompts show that our adaptive strategy consistently outperforms the\nuniform allocation with the same inference budget. Moreover, our experiments\nshow that our adaptive strategy remains competitive against uniform allocations\nwith 20% larger inference budgets and even improves in performance as the batch\nsize grows."}
{"id": "2505.12711", "pdf": "https://arxiv.org/pdf/2505.12711", "abs": "https://arxiv.org/abs/2505.12711", "authors": ["Qichen Sun", "Zhengrui Guo", "Rui Peng", "Hao Chen", "Jinzhuo Wang"], "title": "Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in computational pathology and artificial intelligence have\nsignificantly enhanced the utilization of gigapixel whole-slide images and and\nadditional modalities (e.g., genomics) for pathological diagnosis. Although\ndeep learning has demonstrated strong potential in pathology, several key\nchallenges persist: (1) fusing heterogeneous data types requires sophisticated\nstrategies beyond simple concatenation due to high computational costs; (2)\ncommon scenarios of missing modalities necessitate flexible strategies that\nallow the model to learn robustly in the absence of certain modalities; (3) the\ndownstream tasks in CPath are diverse, ranging from unimodal to multimodal,\ncnecessitating a unified model capable of handling all modalities. To address\nthese challenges, we propose ALTER, an any-to-any tri-modal pretraining\nframework that integrates WSIs, genomics, and pathology reports. The term \"any\"\nemphasizes ALTER's modality-adaptive design, enabling flexible pretraining with\nany subset of modalities, and its capacity to learn robust, cross-modal\nrepresentations beyond WSI-centric approaches. We evaluate ALTER across\nextensive clinical tasks including survival prediction, cancer subtyping, gene\nmutation prediction, and report generation, achieving superior or comparable\nperformance to state-of-the-art baselines."}
{"id": "2505.12065", "pdf": "https://arxiv.org/pdf/2505.12065", "abs": "https://arxiv.org/abs/2505.12065", "authors": ["Tiannuo Yang", "Zebin Yao", "Bowen Jin", "Lixiao Cui", "Yusen Li", "Gang Wang", "Xiaoguang Liu"], "title": "Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM)-based search agents have shown remarkable\ncapabilities in solving complex tasks by dynamically decomposing problems and\naddressing them through interleaved reasoning and retrieval. However, this\ninterleaved paradigm introduces substantial efficiency bottlenecks. First, we\nobserve that both highly accurate and overly approximate retrieval methods\ndegrade system efficiency: exact search incurs significant retrieval overhead,\nwhile coarse retrieval requires additional reasoning steps during generation.\nSecond, we identify inefficiencies in system design, including improper\nscheduling and frequent retrieval stalls, which lead to cascading latency --\nwhere even minor delays in retrieval amplify end-to-end inference time. To\naddress these challenges, we introduce SearchAgent-X, a high-efficiency\ninference framework for LLM-based search agents. SearchAgent-X leverages\nhigh-recall approximate retrieval and incorporates two key techniques:\npriority-aware scheduling and non-stall retrieval. Extensive experiments\ndemonstrate that SearchAgent-X consistently outperforms state-of-the-art\nsystems such as vLLM and HNSW-based retrieval across diverse tasks, achieving\nup to 3.4$\\times$ higher throughput and 5$\\times$ lower latency, without\ncompromising generation quality. SearchAgent-X is available at\nhttps://github.com/tiannuo-yang/SearchAgent-X."}
{"id": "2505.12716", "pdf": "https://arxiv.org/pdf/2505.12716", "abs": "https://arxiv.org/abs/2505.12716", "authors": ["Taiqiang Wu", "Runming Yang", "Jiayi Li", "Pengfei Hu", "Ngai Wong", "Yujiu Yang"], "title": "Shadow-FT: Tuning Instruct via Base", "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Large language models (LLMs) consistently benefit from further fine-tuning on\nvarious tasks. However, we observe that directly tuning the INSTRUCT (i.e.,\ninstruction tuned) models often leads to marginal improvements and even\nperformance degeneration. Notably, paired BASE models, the foundation for these\nINSTRUCT variants, contain highly similar weight values (i.e., less than 2% on\naverage for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to\ntune the INSTRUCT models by leveraging the corresponding BASE models. The key\ninsight is to fine-tune the BASE model, and then directly graft the learned\nweight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no\nadditional parameters, is easy to implement, and significantly improves\nperformance. We conduct extensive experiments on tuning mainstream LLMs, such\nas Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering\ncoding, reasoning, and mathematical tasks. Experimental results demonstrate\nthat Shadow-FT consistently outperforms conventional full-parameter and\nparameter-efficient tuning approaches. Further analyses indicate that Shadow-FT\ncan be applied to multimodal large language models (MLLMs) and combined with\ndirect preference optimization (DPO). Codes and weights are available at\n\\href{https://github.com/wutaiqiang/Shadow-FT}{Github}."}
{"id": "2505.12075", "pdf": "https://arxiv.org/pdf/2505.12075", "abs": "https://arxiv.org/abs/2505.12075", "authors": ["Guy Davidson", "Todd M. Gureckis", "Brenden M. Lake", "Adina Williams"], "title": "Do different prompting methods yield a common task representation in language models?", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 4 figures; under review", "summary": "Demonstrations and instructions are two primary approaches for prompting\nlanguage models to perform in-context learning (ICL) tasks. Do identical tasks\nelicited in different ways result in similar representations of the task? An\nimproved understanding of task representation mechanisms would offer\ninterpretability insights and may aid in steering models. We study this through\nfunction vectors, recently proposed as a mechanism to extract few-shot ICL task\nrepresentations. We generalize function vectors to alternative task\npresentations, focusing on short textual instruction prompts, and successfully\nextract instruction function vectors that promote zero-shot task accuracy. We\nfind evidence that demonstration- and instruction-based function vectors\nleverage different model components, and offer several controls to dissociate\ntheir contributions to task performance. Our results suggest that different\ntask presentations do not induce a common task representation but elicit\ndifferent, partly overlapping mechanisms. Our findings offer principled support\nto the practice of combining textual instructions and task demonstrations,\nimply challenges in universally monitoring task inference across presentation\nforms, and encourage further examinations of LLM task inference mechanisms."}
{"id": "2505.12734", "pdf": "https://arxiv.org/pdf/2505.12734", "abs": "https://arxiv.org/abs/2505.12734", "authors": ["Junbo Wang", "Haofeng Tan", "Bowen Liao", "Albert Jiang", "Teng Fei", "Qixing Huang", "Zhengzhong Tu", "Shan Ye", "Yuhao Kang"], "title": "SounDiT: Geo-Contextual Soundscape-to-Landscape Generation", "categories": ["cs.SD", "cs.AI", "cs.GR", "cs.HC", "eess.AS"], "comment": "14 pages, 5 figures", "summary": "We present a novel and practically significant problem-Geo-Contextual\nSoundscape-to-Landscape (GeoS2L) generation-which aims to synthesize\ngeographically realistic landscape images from environmental soundscapes. Prior\naudio-to-image generation methods typically rely on general-purpose datasets\nand overlook geographic and environmental contexts, resulting in unrealistic\nimages that are misaligned with real-world environmental settings. To address\nthis limitation, we introduce a novel geo-contextual computational framework\nthat explicitly integrates geographic knowledge into multimodal generative\nmodeling. We construct two large-scale geo-contextual multimodal datasets,\nSoundingSVI and SonicUrban, pairing diverse soundscapes with real-world\nlandscape images. We propose SounDiT, a novel Diffusion Transformer (DiT)-based\nmodel that incorporates geo-contextual scene conditioning to synthesize\ngeographically coherent landscape images. Furthermore, we propose a\npractically-informed geo-contextual evaluation framework, the Place Similarity\nScore (PSS), across element-, scene-, and human perception-levels to measure\nconsistency between input soundscapes and generated landscape images. Extensive\nexperiments demonstrate that SounDiT outperforms existing baselines in both\nvisual fidelity and geographic settings. Our work not only establishes\nfoundational benchmarks for GeoS2L generation but also highlights the\nimportance of incorporating geographic domain knowledge in advancing multimodal\ngenerative models, opening new directions at the intersection of generative AI,\ngeography, urban planning, and environmental sciences."}
{"id": "2505.12082", "pdf": "https://arxiv.org/pdf/2505.12082", "abs": "https://arxiv.org/abs/2505.12082", "authors": ["Yunshui Li", "Yiyuan Ma", "Shen Yan", "Chaoyi Zhang", "Jing Liu", "Jianqiao Lu", "Ziwen Xu", "Mengzhao Chen", "Minrui Wang", "Shiyi Zhan", "Jin Ma", "Xunhao Lai", "Yao Luo", "Xingyan Bin", "Hongbin Ren", "Mingji Han", "Wenhao Hao", "Bairen Yi", "LingJun Liu", "Bole Ma", "Xiaoying Jia", "Zhou Xun", "Liang Xiang", "Yonghui Wu"], "title": "Model Merging in Pre-training of Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Model merging has emerged as a promising technique for enhancing large\nlanguage models, though its application in large-scale pre-training remains\nrelatively unexplored. In this paper, we present a comprehensive investigation\nof model merging techniques during the pre-training process. Through extensive\nexperiments with both dense and Mixture-of-Experts (MoE) architectures ranging\nfrom millions to over 100 billion parameters, we demonstrate that merging\ncheckpoints trained with constant learning rates not only achieves significant\nperformance improvements but also enables accurate prediction of annealing\nbehavior. These improvements lead to both more efficient model development and\nsignificantly lower training costs. Our detailed ablation studies on merging\nstrategies and hyperparameters provide new insights into the underlying\nmechanisms while uncovering novel applications. Through comprehensive\nexperimental analysis, we offer the open-source community practical\npre-training guidelines for effective model merging."}
{"id": "2505.12737", "pdf": "https://arxiv.org/pdf/2505.12737", "abs": "https://arxiv.org/abs/2505.12737", "authors": ["Hongjoon Ahn", "Heewoong Choi", "Jisu Han", "Taesup Moon"], "title": "Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Offline goal-conditioned reinforcement learning (GCRL) offers a practical\nlearning paradigm where goal-reaching policies are trained from abundant\nunlabeled (reward-free) datasets without additional environment interaction.\nHowever, offline GCRL still struggles with long-horizon tasks, even with recent\nadvances that employ hierarchical policy structures, such as HIQL. By\nidentifying the root cause of this challenge, we observe the following\ninsights: First, performance bottlenecks mainly stem from the high-level\npolicy's inability to generate appropriate subgoals. Second, when learning the\nhigh-level policy in the long-horizon regime, the sign of the advantage signal\nfrequently becomes incorrect. Thus, we argue that improving the value function\nto produce a clear advantage signal for learning the high-level policy is\nessential. In this paper, we propose a simple yet effective solution:\nOption-aware Temporally Abstracted value learning, dubbed OTA, which\nincorporates temporal abstraction into the temporal-difference learning\nprocess. By modifying the value update to be option-aware, the proposed\nlearning scheme contracts the effective horizon length, enabling better\nadvantage estimates even in long-horizon regimes. We experimentally show that\nthe high-level policy extracted using the OTA value function achieves strong\nperformance on complex tasks from OGBench, a recently proposed offline GCRL\nbenchmark, including maze navigation and visual robotic manipulation\nenvironments."}
{"id": "2505.12092", "pdf": "https://arxiv.org/pdf/2505.12092", "abs": "https://arxiv.org/abs/2505.12092", "authors": ["Marco Fiandri", "Alberto Maria Metelli", "Francesco Trovò"], "title": "Thompson Sampling-like Algorithms for Stochastic Rising Bandits", "categories": ["stat.ML", "cs.LG"], "comment": "57 pages", "summary": "Stochastic rising rested bandit (SRRB) is a setting where the arms' expected\nrewards increase as they are pulled. It models scenarios in which the\nperformances of the different options grow as an effect of an underlying\nlearning process (e.g., online model selection). Even if the bandit literature\nprovides specifically crafted algorithms based on upper-confidence bounds for\nsuch a setting, no study about Thompson sampling TS-like algorithms has been\nperformed so far. The strong regularity of the expected rewards in the SRRB\nsetting suggests that specific instances may be tackled effectively using\nadapted and sliding-window TS approaches. This work provides novel regret\nanalyses for such algorithms in SRRBs, highlighting the challenges and\nproviding new technical tools of independent interest. Our results allow us to\nidentify under which assumptions TS-like algorithms succeed in achieving\nsublinear regret and which properties of the environment govern the complexity\nof the regret minimization problem when approached with TS. Furthermore, we\nprovide a regret lower bound based on a complexity index we introduce. Finally,\nwe conduct numerical simulations comparing TS-like algorithms with\nstate-of-the-art approaches for SRRBs in synthetic and real-world settings."}
{"id": "2505.12738", "pdf": "https://arxiv.org/pdf/2505.12738", "abs": "https://arxiv.org/abs/2505.12738", "authors": ["Chenghua Gong", "Rui Sun", "Yuhao Zheng", "Juyuan Zhang", "Tianjun Gu", "Liming Pan", "Linyuan Lv"], "title": "EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting", "categories": ["cs.LG", "cs.AI", "cs.SI"], "comment": "18 pages", "summary": "Advanced epidemic forecasting is critical for enabling precision containment\nstrategies, highlighting its strategic importance for public health security.\nWhile recent advances in Large Language Models (LLMs) have demonstrated\neffectiveness as foundation models for domain-specific tasks, their potential\nfor epidemic forecasting remains largely unexplored. In this paper, we\nintroduce EpiLLM, a novel LLM-based framework tailored for spatio-temporal\nepidemic forecasting. Considering the key factors in real-world epidemic\ntransmission: infection cases and human mobility, we introduce a dual-branch\narchitecture to achieve fine-grained token-level alignment between such complex\nepidemic patterns and language tokens for LLM adaptation. To unleash the\nmulti-step forecasting and generalization potential of LLM architectures, we\npropose an autoregressive modeling paradigm that reformulates the epidemic\nforecasting task into next-token prediction. To further enhance LLM perception\nof epidemics, we introduce spatio-temporal prompt learning techniques, which\nstrengthen forecasting capabilities from a data-driven perspective. Extensive\nexperiments show that EpiLLM significantly outperforms existing baselines on\nreal-world COVID-19 datasets and exhibits scaling behavior characteristic of\nLLMs."}
{"id": "2505.12117", "pdf": "https://arxiv.org/pdf/2505.12117", "abs": "https://arxiv.org/abs/2505.12117", "authors": ["Daniel Cederberg"], "title": "T-Rex: Fitting a Robust Factor Model via Expectation-Maximization", "categories": ["stat.ML", "cs.LG", "eess.SP", "math.OC", "90C26"], "comment": "Currently under review", "summary": "Over the past decades, there has been a surge of interest in studying\nlow-dimensional structures within high-dimensional data. Statistical factor\nmodels $-$ i.e., low-rank plus diagonal covariance structures $-$ offer a\npowerful framework for modeling such structures. However, traditional methods\nfor fitting statistical factor models, such as principal component analysis\n(PCA) or maximum likelihood estimation assuming the data is Gaussian, are\nhighly sensitive to heavy tails and outliers in the observed data. In this\npaper, we propose a novel expectation-maximization (EM) algorithm for robustly\nfitting statistical factor models. Our approach is based on Tyler's M-estimator\nof the scatter matrix for an elliptical distribution, and consists of solving\nTyler's maximum likelihood estimation problem while imposing a structural\nconstraint that enforces the low-rank plus diagonal covariance structure. We\npresent numerical experiments on both synthetic and real examples,\ndemonstrating the robustness of our method for direction-of-arrival estimation\nin nonuniform noise and subspace recovery."}
{"id": "2505.12745", "pdf": "https://arxiv.org/pdf/2505.12745", "abs": "https://arxiv.org/abs/2505.12745", "authors": ["Dong Kyu Cho", "Inwoo Hwang", "Sanghack Lee"], "title": "PEER pressure: Model-to-Model Regularization for Single Source Domain Generalization", "categories": ["cs.LG", "cs.AI"], "comment": "21 pages, 9 figures, Accepted at CVPR 2025", "summary": "Data augmentation is a popular tool for single source domain generalization,\nwhich expands the source domain by generating simulated ones, improving\ngeneralization on unseen target domains. In this work, we show that the\nperformance of such augmentation-based methods in the target domains\nuniversally fluctuates during training, posing challenges in model selection\nunder realistic scenarios. We argue that the fluctuation stems from the\ninability of the model to accumulate the knowledge learned from diverse\naugmentations, exacerbating feature distortion during training. Based on this\nobservation, we propose a novel generalization method, coined Parameter-Space\nEnsemble with Entropy Regularization (PEER), that uses a proxy model to learn\nthe augmented data on behalf of the main model. The main model is updated by\naveraging its parameters with the proxy model, progressively accumulating\nknowledge over the training steps. Maximizing the mutual information between\nthe output representations of the two models guides the learning process of the\nproxy model, mitigating feature distortion during training. Experimental\nresults demonstrate the effectiveness of PEER in reducing the OOD performance\nfluctuation and enhancing generalization across various datasets, including\nPACS, Digits, Office-Home, and VLCS. Notably, our method with simple random\naugmentation achieves state-of-the-art performance, surpassing prior approaches\non sDG that utilize complex data augmentation strategies."}
{"id": "2505.12128", "pdf": "https://arxiv.org/pdf/2505.12128", "abs": "https://arxiv.org/abs/2505.12128", "authors": ["Nikita P. Kalinin", "Ryan McKenna", "Jalaj Upadhyay", "Christoph H. Lampert"], "title": "Back to Square Roots: An Optimal Bound on the Matrix Factorization Error for Multi-Epoch Differentially Private SGD", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Matrix factorization mechanisms for differentially private training have\nemerged as a promising approach to improve model utility under privacy\nconstraints. In practical settings, models are typically trained over multiple\nepochs, requiring matrix factorizations that account for repeated\nparticipation. Existing theoretical upper and lower bounds on multi-epoch\nfactorization error leave a significant gap. In this work, we introduce a new\nexplicit factorization method, Banded Inverse Square Root (BISR), which imposes\na banded structure on the inverse correlation matrix. This factorization\nenables us to derive an explicit and tight characterization of the multi-epoch\nerror. We further prove that BISR achieves asymptotically optimal error by\nmatching the upper and lower bounds. Empirically, BISR performs on par with\nstate-of-the-art factorization methods, while being simpler to implement,\ncomputationally efficient, and easier to analyze."}
{"id": "2505.12748", "pdf": "https://arxiv.org/pdf/2505.12748", "abs": "https://arxiv.org/abs/2505.12748", "authors": ["Hangyu Li", "Qin Zhao", "Haoran Xu", "Xinyu Jiang", "Qingwei Ben", "Feiyu Jia", "Haoyu Zhao", "Liang Xu", "Jia Zeng", "Hanqing Wang", "Bo Dai", "Junting Dong", "Jiangmiao Pang"], "title": "TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "13 pages", "summary": "Teleoperation is a cornerstone of embodied-robot learning, and bimanual\ndexterous teleoperation in particular provides rich demonstrations that are\ndifficult to obtain with fully autonomous systems. While recent studies have\nproposed diverse hardware pipelines-ranging from inertial motion-capture gloves\nto exoskeletons and vision-based interfaces-there is still no unified benchmark\nthat enables fair, reproducible comparison of these systems. In this paper, we\nintroduce TeleOpBench, a simulator-centric benchmark tailored to bimanual\ndexterous teleoperation. TeleOpBench contains 30 high-fidelity task\nenvironments that span pick-and-place, tool use, and collaborative\nmanipulation, covering a broad spectrum of kinematic and force-interaction\ndifficulty. Within this benchmark we implement four representative\nteleoperation modalities-(i) MoCap, (ii) VR device, (iii) arm-hand\nexoskeletons, and (iv) monocular vision tracking-and evaluate them with a\ncommon protocol and metric suite. To validate that performance in simulation is\npredictive of real-world behavior, we conduct mirrored experiments on a\nphysical dual-arm platform equipped with two 6-DoF dexterous hands. Across 10\nheld-out tasks we observe a strong correlation between simulator and hardware\nperformance, confirming the external validity of TeleOpBench. TeleOpBench\nestablishes a common yardstick for teleoperation research and provides an\nextensible platform for future algorithmic and hardware innovation."}
{"id": "2505.12132", "pdf": "https://arxiv.org/pdf/2505.12132", "abs": "https://arxiv.org/abs/2505.12132", "authors": ["Rodrigo Moreira", "Tereza C. M. Carvalho", "Flávio de Oliveira Silva", "Nazim Agoulmine", "Joberto S. B. Martins"], "title": "Towards Sustainability in 6G Network Slicing with Energy-Saving and Optimization Methods", "categories": ["cs.NI", "cs.ET", "cs.LG", "I.2; C.2.1"], "comment": "10 pages, 7 figures, International Workshop on ADVANCEs in ICT\n  Infrastructures and Services, 2025", "summary": "The 6G mobile network is the next evolutionary step after 5G, with a\nprediction of an explosive surge in mobile traffic. It provides ultra-low\nlatency, higher data rates, high device density, and ubiquitous coverage,\npositively impacting services in various areas. Energy saving is a major\nconcern for new systems in the telecommunications sector because all players\nare expected to reduce their carbon footprints to contribute to mitigating\nclimate change. Network slicing is a fundamental enabler for 6G/5G mobile\nnetworks and various other new systems, such as the Internet of Things (IoT),\nInternet of Vehicles (IoV), and Industrial IoT (IIoT). However, energy-saving\nmethods embedded in network slicing architectures are still a research gap.\nThis paper discusses how to embed energy-saving methods in network-slicing\narchitectures that are a fundamental enabler for nearly all new innovative\nsystems being deployed worldwide. This paper's main contribution is a proposal\nto save energy in network slicing. That is achieved by deploying ML-native\nagents in NS architectures to dynamically orchestrate and optimize resources\nbased on user demands. The SFI2 network slicing reference architecture is the\nconcrete use case scenario in which contrastive learning improves energy saving\nfor resource allocation."}
{"id": "2505.12750", "pdf": "https://arxiv.org/pdf/2505.12750", "abs": "https://arxiv.org/abs/2505.12750", "authors": ["Filippo Leveni", "Matteo Mistura", "Francesco Iubatti", "Carmine Giangregorio", "Nicolò Pastore", "Cesare Alippi", "Giacomo Boracchi"], "title": "Malware families discovery via Open-Set Recognition on Android manifest permissions", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "Submitted to European Conference on Artificial Intelligence (ECAI\n  2025)", "summary": "Malware are malicious programs that are grouped into families based on their\npenetration technique, source code, and other characteristics. Classifying\nmalware programs into their respective families is essential for building\neffective defenses against cyber threats. Machine learning models have a huge\npotential in malware detection on mobile devices, as malware families can be\nrecognized by classifying permission data extracted from Android manifest\nfiles. Still, the malware classification task is challenging due to the\nhigh-dimensional nature of permission data and the limited availability of\ntraining samples. In particular, the steady emergence of new malware families\nmakes it impossible to acquire a comprehensive training set covering all the\nmalware classes. In this work, we present a malware classification system that,\non top of classifying known malware, detects new ones. In particular, we\ncombine an open-set recognition technique developed within the computer vision\ncommunity, namely MaxLogit, with a tree-based Gradient Boosting classifier,\nwhich is particularly effective in classifying high-dimensional data. Our\nsolution turns out to be very practical, as it can be seamlessly employed in a\nstandard classification workflow, and efficient, as it adds minimal\ncomputational overhead. Experiments on public and proprietary datasets\ndemonstrate the potential of our solution, which has been deployed in a\nbusiness environment."}
{"id": "2505.12155", "pdf": "https://arxiv.org/pdf/2505.12155", "abs": "https://arxiv.org/abs/2505.12155", "authors": ["Ranit Karmakar", "Simon F. Nørrelykke"], "title": "SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Segmentation evaluation metrics traditionally rely on binary decision logic:\npredictions are either correct or incorrect, based on rigid IoU thresholds.\nDetection--based metrics such as F1 and mAP determine correctness at the object\nlevel using fixed overlap cutoffs, while overlap--based metrics like\nIntersection over Union (IoU) and Dice operate at the pixel level, often\noverlooking instance--level structure. Panoptic Quality (PQ) attempts to unify\ndetection and segmentation assessment, but it remains dependent on\nhard-threshold matching--treating predictions below the threshold as entirely\nincorrect. This binary framing obscures important distinctions between\nqualitatively different errors and fails to reward gradual model improvements.\nWe propose SoftPQ, a flexible and interpretable instance segmentation metric\nthat redefines evaluation as a graded continuum rather than a binary\nclassification. SoftPQ introduces tunable upper and lower IoU thresholds to\ndefine a partial matching region and applies a sublinear penalty function to\nambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit\nsmoother score behavior, greater robustness to structural segmentation errors,\nand more informative feedback for model development and evaluation. Through\ncontrolled perturbation experiments, we show that SoftPQ captures meaningful\ndifferences in segmentation quality that existing metrics overlook, making it a\npractical and principled alternative for both benchmarking and iterative model\nrefinement."}
{"id": "2505.12751", "pdf": "https://arxiv.org/pdf/2505.12751", "abs": "https://arxiv.org/abs/2505.12751", "authors": ["Filippo Leveni"], "title": "Structure-based Anomaly Detection and Clustering", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Doctoral dissertation at Politecnico di Milano", "summary": "Anomaly detection is a fundamental problem in domains such as healthcare,\nmanufacturing, and cybersecurity. This thesis proposes new unsupervised methods\nfor anomaly detection in both structured and streaming data settings. In the\nfirst part, we focus on structure-based anomaly detection, where normal data\nfollows low-dimensional manifolds while anomalies deviate from them. We\nintroduce Preference Isolation Forest (PIF), which embeds data into a\nhigh-dimensional preference space via manifold fitting, and isolates outliers\nusing two variants: Voronoi-iForest, based on geometric distances, and\nRuzHash-iForest, leveraging Locality Sensitive Hashing for scalability. We also\npropose Sliding-PIF, which captures local manifold information for streaming\nscenarios. Our methods outperform existing techniques on synthetic and real\ndatasets. We extend this to structure-based clustering with MultiLink, a novel\nmethod for recovering multiple geometric model families in noisy data.\nMultiLink merges clusters via a model-aware linkage strategy, enabling robust\nmulti-class structure recovery. It offers key advantages over existing\napproaches, such as speed, reduced sensitivity to thresholds, and improved\nrobustness to poor initial sampling. The second part of the thesis addresses\nonline anomaly detection in evolving data streams. We propose Online Isolation\nForest (Online-iForest), which uses adaptive, multi-resolution histograms and\ndynamically updates tree structures to track changes over time. It avoids\nretraining while achieving accuracy comparable to offline models, with superior\nefficiency for real-time applications. Finally, we tackle anomaly detection in\ncybersecurity via open-set recognition for malware classification. We enhance a\nGradient Boosting classifier with MaxLogit to detect unseen malware families, a\nmethod now integrated into Cleafy's production system."}
{"id": "2505.12161", "pdf": "https://arxiv.org/pdf/2505.12161", "abs": "https://arxiv.org/abs/2505.12161", "authors": ["Hossein Babaei", "Mel White", "Sina Alemohammad", "Richard G. Baraniuk"], "title": "WaLRUS: Wavelets for Long-range Representation Using SSMs", "categories": ["eess.IV", "cs.LG", "cs.SY", "eess.AS", "eess.SP", "eess.SY"], "comment": "15 pages, 8 figures. Submitted to Neurips 2025", "summary": "State-Space Models (SSMs) have proven to be powerful tools for modeling\nlong-range dependencies in sequential data. While the recent method known as\nHiPPO has demonstrated strong performance, and formed the basis for machine\nlearning models S4 and Mamba, it remains limited by its reliance on closed-form\nsolutions for a few specific, well-behaved bases. The SaFARi framework\ngeneralized this approach, enabling the construction of SSMs from arbitrary\nframes, including non-orthogonal and redundant ones, thus allowing an infinite\ndiversity of possible \"species\" within the SSM family. In this paper, we\nintroduce WaLRUS (Wavelets for Long-range Representation Using SSMs), a new\nimplementation of SaFARi built from Daubechies wavelets."}
{"id": "2505.12761", "pdf": "https://arxiv.org/pdf/2505.12761", "abs": "https://arxiv.org/abs/2505.12761", "authors": ["Donghwa Shin", "Edwin Zhang"], "title": "Enhancing Channel-Independent Time-Series Forecasting via Cross-Variate Patch Embedding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Transformers have recently gained popularity in time series forecasting due\nto their ability to capture long-term dependencies. However, many existing\nmodels focus only on capturing temporal dependencies while omitting intricate\nrelationships between variables. Recent models have tried tackling this by\nexplicitly modeling both cross-time and cross-variate dependencies through a\nsequential or unified attention mechanism, but they are entirely channel\ndependent (CD) across all layers, making them potentially susceptible to\noverfitting. To address this, we propose Cross-Variate Patch Embeddings (CVPE),\na lightweight CD module that injects cross-variate context into\nchannel-independent (CI) models by simply modifying the patch embedding\nprocess. We achieve this by adding a learnable positional encoding and a\nlightweight router-attention block to the vanilla patch embedding layer. We\nthen integrate CVPE into Time-LLM, a multimodal CI forecasting model, to\ndemonstrate its effectiveness in capturing cross-variate dependencies and\nenhance the CI model's performance. Extensive experimental results on seven\nreal-world datasets show that our enhanced Time-LLM outperforms the original\nbaseline model simply by incorporating the CVPE module, with no other changes."}
{"id": "2505.12185", "pdf": "https://arxiv.org/pdf/2505.12185", "abs": "https://arxiv.org/abs/2505.12185", "authors": ["Sen Fang", "Weiyuan Ding", "Bowen Xu"], "title": "EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "19 pages, 11 figures", "summary": "Assessing the programming capabilities of Large Language Models (LLMs) is\ncrucial for their effective use in software engineering. Current evaluations,\nhowever, predominantly measure the accuracy of generated code on static\nbenchmarks, neglecting the critical aspect of model robustness during\nprogramming tasks. While adversarial attacks offer insights on model\nrobustness, their effectiveness is limited and evaluation could be constrained.\nCurrent adversarial attack methods for robustness evaluation yield inconsistent\nresults, struggling to provide a unified evaluation across different LLMs. We\nintroduce EVALOOP, a novel assessment framework that evaluate the robustness\nfrom a self-consistency perspective, i.e., leveraging the natural duality\ninherent in popular software engineering tasks, e.g., code generation and code\nsummarization. EVALOOP initiates a self-contained feedback loop: an LLM\ngenerates output (e.g., code) from an input (e.g., natural language\nspecification), and then use the generated output as the input to produce a new\noutput (e.g., summarizes that code into a new specification). EVALOOP repeats\nthe process to assess the effectiveness of EVALOOP in each loop. This cyclical\nstrategy intrinsically evaluates robustness without rely on any external attack\nsetups, providing a unified metric to evaluate LLMs' robustness in programming.\nWe evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found\nthat EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1\nperformance within ten loops. Intriguingly, robustness does not always align\nwith initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,\ndespite superior initial code generation compared to DeepSeek-V2, demonstrated\nlower robustness over repeated evaluation loop."}
{"id": "2505.12763", "pdf": "https://arxiv.org/pdf/2505.12763", "abs": "https://arxiv.org/abs/2505.12763", "authors": ["Sunghwan Kim", "Dongjin Kang", "Taeyoon Kwon", "Hyungjoo Chae", "Dongha Lee", "Jinyoung Yeo"], "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Reward models (RMs) play a crucial role in reinforcement learning from human\nfeedback (RLHF), aligning model behavior with human preferences. However,\nexisting benchmarks for reward models show a weak correlation with the\nperformance of optimized policies, suggesting that they fail to accurately\nassess the true capabilities of RMs. To bridge this gap, we explore several\nevaluation designs through the lens of reward overoptimization\\textemdash a\nphenomenon that captures both how well the reward model aligns with human\npreferences and the dynamics of the learning signal it provides to the policy.\nThe results highlight three key findings on how to construct a reliable\nbenchmark: (i) it is important to minimize differences between chosen and\nrejected responses beyond correctness, (ii) evaluating reward models requires\nmultiple comparisons across a wide range of chosen and rejected responses, and\n(iii) given that reward models encounter responses with diverse\nrepresentations, responses should be sourced from a variety of models. However,\nwe also observe that a extremely high correlation with degree of\noveroptimization leads to comparatively lower correlation with certain\ndownstream performance. Thus, when designing a benchmark, it is desirable to\nuse the degree of overoptimization as a useful tool, rather than the end goal."}
{"id": "2505.12191", "pdf": "https://arxiv.org/pdf/2505.12191", "abs": "https://arxiv.org/abs/2505.12191", "authors": ["Wenquan Lu", "Jiaqi Zhang", "Hugues Van Assel", "Randall Balestriero"], "title": "Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Self-Supervised Learning (SSL) has become a powerful solution to extract rich\nrepresentations from unlabeled data. Yet, SSL research is mostly focused on\nclean, curated and high-quality datasets. As a result, applying SSL on noisy\ndata remains a challenge, despite being crucial to applications such as\nastrophysics, medical imaging, geophysics or finance. In this work, we present\na fully self-supervised framework that enables noise-robust representation\nlearning without requiring a denoiser at inference or downstream fine-tuning.\nOur method first trains an SSL denoiser on noisy data, then uses it to\nconstruct a denoised-to-noisy data curriculum (i.e., training first on\ndenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),\ncombined with a teacher-guided regularization that anchors noisy embeddings to\ntheir denoised counterparts. This process encourages the model to internalize\nnoise robustness. Notably, the denoiser can be discarded after pretraining,\nsimplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise\n($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by\n4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from\nnoise-aware pretraining. The code is available at\nhttps://github.com/wenquanlu/noisy_dinov2."}
{"id": "2505.12774", "pdf": "https://arxiv.org/pdf/2505.12774", "abs": "https://arxiv.org/abs/2505.12774", "authors": ["Zichen Geng", "Zeeshan Hayder", "Wei Liu", "Ajmal Mian"], "title": "UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Human motion synthesis in complex scenes presents a fundamental challenge,\nextending beyond conventional Text-to-Motion tasks by requiring the integration\nof diverse modalities such as static environments, movable objects, natural\nlanguage prompts, and spatial waypoints. Existing language-conditioned motion\nmodels often struggle with scene-aware motion generation due to limitations in\nmotion tokenization, which leads to information loss and fails to capture the\ncontinuous, context-dependent nature of 3D human movement. To address these\nissues, we propose UniHM, a unified motion language model that leverages\ndiffusion-based generation for synthesizing scene-aware human motion. UniHM is\nthe first framework to support both Text-to-Motion and Text-to-Human-Object\nInteraction (HOI) in complex 3D scenes. Our approach introduces three key\ncontributions: (1) a mixed-motion representation that fuses continuous 6DoF\nmotion with discrete local motion tokens to improve motion realism; (2) a novel\nLook-Up-Free Quantization VAE (LFQ-VAE) that surpasses traditional VQ-VAEs in\nboth reconstruction accuracy and generative performance; and (3) an enriched\nversion of the Lingo dataset augmented with HumanML3D annotations, providing\nstronger supervision for scene-specific motion learning. Experimental results\ndemonstrate that UniHM achieves comparative performance on the OMOMO benchmark\nfor text-to-HOI synthesis and yields competitive results on HumanML3D for\ngeneral text-conditioned motion generation."}
{"id": "2505.12206", "pdf": "https://arxiv.org/pdf/2505.12206", "abs": "https://arxiv.org/abs/2505.12206", "authors": ["Mathanesh Vellingiri Ramasamy", "Dimas Rizky Kurniasalim"], "title": "Road Segmentation for ADAS/AD Applications", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Accurate road segmentation is essential for autonomous driving and ADAS,\nenabling effective navigation in complex environments. This study examines how\nmodel architecture and dataset choice affect segmentation by training a\nmodified VGG-16 on the Comma10k dataset and a modified U-Net on the KITTI Road\ndataset. Both models achieved high accuracy, with cross-dataset testing showing\nVGG-16 outperforming U-Net despite U-Net being trained for more epochs. We\nanalyze model performance using metrics such as F1-score, mean intersection\nover union, and precision, discussing how architecture and dataset impact\nresults."}
{"id": "2505.12781", "pdf": "https://arxiv.org/pdf/2505.12781", "abs": "https://arxiv.org/abs/2505.12781", "authors": ["Jitai Hao", "Qiang Huang", "Hao Liu", "Xinyan Xiao", "Zhaochun Ren", "Jun Yu"], "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Training high-performing Small Language Models (SLMs) remains costly, even\nwith knowledge distillation and pruning from larger teacher models. Existing\nwork often faces three key challenges: (1) information loss from hard pruning,\n(2) inefficient alignment of representations, and (3) underutilization of\ninformative activations, particularly from Feed-Forward Networks (FFNs). To\naddress these challenges, we introduce Low-Rank Clone (LRC), an efficient\npre-training method that constructs SLMs aspiring to behavioral equivalence\nwith strong teacher models. LRC trains a set of low-rank projection matrices\nthat jointly enable soft pruning by compressing teacher weights, and activation\nclone by aligning student activations, including FFN signals, with those of the\nteacher. This unified design maximizes knowledge transfer while removing the\nneed for explicit alignment modules. Extensive experiments with open-source\nteachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC\nmatches or surpasses state-of-the-art models trained on trillions of\ntokens--while using only 20B tokens, achieving over 1,000x training efficiency.\nOur codes and model checkpoints are available at\nhttps://github.com/CURRENTF/LowRankClone and\nhttps://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf."}
{"id": "2505.12228", "pdf": "https://arxiv.org/pdf/2505.12228", "abs": "https://arxiv.org/abs/2505.12228", "authors": ["Karthik Gopinath", "Annabel Sorby-Adams", "Jonathan W. Ramirez", "Dina Zemlyanker", "Jennifer Guo", "David Hunt", "Christine L. Mac Donald", "C. Dirk Keene", "Timothy Coalson", "Matthew F. Glasser", "David Van Essen", "Matthew S. Rosen", "Oula Puonti", "W. Taylor Kimberly", "Juan Eugenio Iglesias"], "title": "From Low Field to High Value: Robust Cortical Mapping from Low-Field MRI", "categories": ["cs.CV", "cs.LG"], "comment": "32 pages", "summary": "Three-dimensional reconstruction of cortical surfaces from MRI for\nmorphometric analysis is fundamental for understanding brain structure. While\nhigh-field MRI (HF-MRI) is standard in research and clinical settings, its\nlimited availability hinders widespread use. Low-field MRI (LF-MRI),\nparticularly portable systems, offers a cost-effective and accessible\nalternative. However, existing cortical surface analysis tools are optimized\nfor high-resolution HF-MRI and struggle with the lower signal-to-noise ratio\nand resolution of LF-MRI. In this work, we present a machine learning method\nfor 3D reconstruction and analysis of portable LF-MRI across a range of\ncontrasts and resolutions. Our method works \"out of the box\" without\nretraining. It uses a 3D U-Net trained on synthetic LF-MRI to predict signed\ndistance functions of cortical surfaces, followed by geometric processing to\nensure topological accuracy. We evaluate our method using paired HF/LF-MRI\nscans of the same subjects, showing that LF-MRI surface reconstruction accuracy\ndepends on acquisition parameters, including contrast type (T1 vs T2),\norientation (axial vs isotropic), and resolution. A 3mm isotropic T2-weighted\nscan acquired in under 4 minutes, yields strong agreement with HF-derived\nsurfaces: surface area correlates at r=0.96, cortical parcellations reach\nDice=0.98, and gray matter volume achieves r=0.93. Cortical thickness remains\nmore challenging with correlations up to r=0.70, reflecting the difficulty of\nsub-mm precision with 3mm voxels. We further validate our method on challenging\npostmortem LF-MRI, demonstrating its robustness. Our method represents a step\ntoward enabling cortical surface analysis on portable LF-MRI. Code is available\nat https://surfer.nmr.mgh.harvard.edu/fswiki/ReconAny"}
{"id": "2505.12800", "pdf": "https://arxiv.org/pdf/2505.12800", "abs": "https://arxiv.org/abs/2505.12800", "authors": ["Hieu-Nghia Huynh-Nguyen", "Ngoc Son Nguyen", "Huynh Nguyen Dang", "Thieu Vo", "Truong-Son Hy", "Van Nguyen"], "title": "OZSpeech: One-step Zero-shot Speech Synthesis with Learned-Prior-Conditioned Flow Matching", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Text-to-speech (TTS) systems have seen significant advancements in recent\nyears, driven by improvements in deep learning and neural network\narchitectures. Viewing the output speech as a data distribution, previous\napproaches often employ traditional speech representations, such as waveforms\nor spectrograms, within the Flow Matching framework. However, these methods\nhave limitations, including overlooking various speech attributes and incurring\nhigh computational costs due to additional constraints introduced during\ntraining. To address these challenges, we introduce OZSpeech, the first TTS\nmethod to explore optimal transport conditional flow matching with one-step\nsampling and a learned prior as the condition, effectively disregarding\npreceding states and reducing the number of sampling steps. Our approach\noperates on disentangled, factorized components of speech in token format,\nenabling accurate modeling of each speech attribute, which enhances the TTS\nsystem's ability to precisely clone the prompt speech. Experimental results\nshow that our method achieves promising performance over existing methods in\ncontent accuracy, naturalness, prosody generation, and speaker style\npreservation. Audio samples are available at our demo page\nhttps://ozspeech.github.io/OZSpeech_Web/."}
{"id": "2505.12236", "pdf": "https://arxiv.org/pdf/2505.12236", "abs": "https://arxiv.org/abs/2505.12236", "authors": ["Quanjiang Guo", "Jinchuan Zhang", "Sijie Wang", "Ling Tian", "Zhao Kang", "Bin Yan", "Weidong Xiao"], "title": "Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 6 figures, Appear on IJCAI 2025", "summary": "Few-Shot Relation Extraction (FSRE) remains a challenging task due to the\nscarcity of annotated data and the limited generalization capabilities of\nexisting models. Although large language models (LLMs) have demonstrated\npotential in FSRE through in-context learning (ICL), their general-purpose\ntraining objectives often result in suboptimal performance for task-specific\nrelation extraction. To overcome these challenges, we propose TKRE (Two-Stage\nKnowledge-Guided Pre-training for Relation Extraction), a novel framework that\nsynergistically integrates LLMs with traditional relation extraction models,\nbridging generative and discriminative learning paradigms. TKRE introduces two\nkey innovations: (1) leveraging LLMs to generate explanation-driven knowledge\nand schema-constrained synthetic data, addressing the issue of data scarcity;\nand (2) a two-stage pre-training strategy combining Masked Span Language\nModeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational\nreasoning and generalization. Together, these components enable TKRE to\neffectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets\ndemonstrate the efficacy of TKRE, achieving new state-of-the-art performance in\nFSRE and underscoring its potential for broader application in low-resource\nscenarios. \\footnote{The code and data are released on\nhttps://github.com/UESTC-GQJ/TKRE."}
{"id": "2505.12805", "pdf": "https://arxiv.org/pdf/2505.12805", "abs": "https://arxiv.org/abs/2505.12805", "authors": ["Seanie Lee", "Sangwoo Park", "Dong Bok Lee", "Dominik Wagner", "Haebin Seong", "Tobias Bocklet", "Juho Lee", "Sung Ju Hwang"], "title": "FedSVD: Adaptive Orthogonalization for Private Federated Learning with LoRA", "categories": ["cs.LG", "cs.AI"], "comment": "preprint", "summary": "Low-Rank Adaptation (LoRA), which introduces a product of two trainable\nlow-rank matrices into frozen pre-trained weights, is widely used for efficient\nfine-tuning of language models in federated learning (FL). However, when\ncombined with differentially private stochastic gradient descent (DP-SGD), LoRA\nfaces substantial noise amplification: DP-SGD perturbs per-sample gradients,\nand the matrix multiplication of the LoRA update ($BA$) intensifies this\neffect. Freezing one matrix (e.g., $A$) reduces the noise but restricts model\nexpressiveness, often resulting in suboptimal adaptation. To address this, we\npropose FedSVD, a simple yet effective method that introduces a global\nreparameterization based on singular value decomposition (SVD). In our\napproach, each client optimizes only the $B$ matrix and transmits it to the\nserver. The server aggregates the $B$ matrices, computes the product $BA$ using\nthe previous $A$, and refactorizes the result via SVD. This yields a new\nadaptive $A$ composed of the orthonormal right singular vectors of $BA$, and an\nupdated $B$ containing the remaining SVD components. This reparameterization\navoids quadratic noise amplification, while allowing $A$ to better capture the\nprincipal directions of the aggregate updates. Moreover, the orthonormal\nstructure of $A$ bounds the gradient norms of $B$ and preserves more signal\nunder DP-SGD, as confirmed by our theoretical analysis. As a result, FedSVD\nconsistently improves stability and performance across a variety of privacy\nsettings and benchmarks, outperforming relevant baselines under both private\nand non-private regimes."}
{"id": "2505.12242", "pdf": "https://arxiv.org/pdf/2505.12242", "abs": "https://arxiv.org/abs/2505.12242", "authors": ["Tingfeng Lan", "Yusen Wu", "Bin Ma", "Zhaoyuan Su", "Rui Yang", "Tekin Bicer", "Dong Li", "Yue Cheng"], "title": "ZenFlow: Enabling Stall-Free Offloading Training via Asynchronous Updates", "categories": ["cs.DC", "cs.LG", "C.1.4; D.4.7"], "comment": "13 pages, 16 figures", "summary": "Fine-tuning large language models (LLMs) often exceeds GPU memory limits,\nprompting systems to offload model states to CPU memory. However, existing\noffloaded training frameworks like ZeRO-Offload treat all parameters equally\nand update the full model on the CPU, causing severe GPU stalls, where fast,\nexpensive GPUs sit idle waiting for slow CPU updates and limited-bandwidth PCIe\ntransfers.\n  We present ZenFlow, a new offloading framework that prioritizes important\nparameters and decouples updates between GPU and CPU. ZenFlow performs in-place\nupdates of important gradients on GPU, while asynchronously offloading and\naccumulating less important ones on CPU, fully overlapping CPU work with GPU\ncomputation.\n  To scale across GPUs, ZenFlow introduces a lightweight gradient selection\nmethod that exploits a novel spatial and temporal locality property of\nimportant gradients, avoiding costly global synchronization. ZenFlow achieves\nup to 5x end-to-end speedup, 2x lower PCIe traffic, and reduces GPU stalls by\nover 85 percent, all while preserving accuracy."}
{"id": "2505.12811", "pdf": "https://arxiv.org/pdf/2505.12811", "abs": "https://arxiv.org/abs/2505.12811", "authors": ["Wei-Chen Liao", "Ti-Rong Wu", "I-Chen Wu"], "title": "Dynamic Sight Range Selection in Multi-Agent Reinforcement Learning", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": "Accepted at AAMAS 2025. The compiled PDF includes the appendix", "summary": "Multi-agent reinforcement Learning (MARL) is often challenged by the sight\nrange dilemma, where agents either receive insufficient or excessive\ninformation from their environment. In this paper, we propose a novel method,\ncalled Dynamic Sight Range Selection (DSR), to address this issue. DSR utilizes\nan Upper Confidence Bound (UCB) algorithm and dynamically adjusts the sight\nrange during training. Experiment results show several advantages of using DSR.\nFirst, we demonstrate using DSR achieves better performance in three common\nMARL environments, including Level-Based Foraging (LBF), Multi-Robot Warehouse\n(RWARE), and StarCraft Multi-Agent Challenge (SMAC). Second, our results show\nthat DSR consistently improves performance across multiple MARL algorithms,\nincluding QMIX and MAPPO. Third, DSR offers suitable sight ranges for different\ntraining steps, thereby accelerating the training process. Finally, DSR\nprovides additional interpretability by indicating the optimal sight range used\nduring training. Unlike existing methods that rely on global information or\ncommunication mechanisms, our approach operates solely based on the individual\nsight ranges of agents. This approach offers a practical and efficient solution\nto the sight range dilemma, making it broadly applicable to real-world complex\nenvironments."}
{"id": "2505.12254", "pdf": "https://arxiv.org/pdf/2505.12254", "abs": "https://arxiv.org/abs/2505.12254", "authors": ["Yiwei Ou", "Xiaobin Ren", "Ronggui Sun", "Guansong Gao", "Ziyi Jiang", "Kaiqi Zhao", "Manfredo Manfredini"], "title": "MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Existing visual place recognition (VPR) datasets predominantly rely on\nvehicle-mounted imagery, lack multimodal diversity and underrepresent dense,\nmixed-use street-level spaces, especially in non-Western urban contexts. To\naddress these gaps, we introduce MMS-VPR, a large-scale multimodal dataset for\nstreet-level place recognition in complex, pedestrian-only environments. The\ndataset comprises 78,575 annotated images and 2,512 video clips captured across\n207 locations in a ~70,800 $\\mathrm{m}^2$ open-air commercial district in\nChengdu, China. Each image is labeled with precise GPS coordinates, timestamp,\nand textual metadata, and covers varied lighting conditions, viewpoints, and\ntimeframes. MMS-VPR follows a systematic and replicable data collection\nprotocol with minimal device requirements, lowering the barrier for scalable\ndataset creation. Importantly, the dataset forms an inherent spatial graph with\n125 edges, 81 nodes, and 1 subgraph, enabling structure-aware place\nrecognition. We further define two application-specific subsets --\nDataset_Edges and Dataset_Points -- to support fine-grained and graph-based\nevaluation tasks. Extensive benchmarks using conventional VPR models, graph\nneural networks, and multimodal baselines show substantial improvements when\nleveraging multimodal and structural cues. MMS-VPR facilitates future research\nat the intersection of computer vision, geospatial understanding, and\nmultimodal reasoning. The dataset is publicly available at\nhttps://huggingface.co/datasets/Yiwei-Ou/MMS-VPR."}
{"id": "2505.12814", "pdf": "https://arxiv.org/pdf/2505.12814", "abs": "https://arxiv.org/abs/2505.12814", "authors": ["Xilong Cheng", "Yunxiao Qin", "Yuting Tan", "Zhengnan Li", "Ye Wang", "Hongjiang Xiao", "Yuan Zhang"], "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity."}
{"id": "2505.12268", "pdf": "https://arxiv.org/pdf/2505.12268", "abs": "https://arxiv.org/abs/2505.12268", "authors": ["Pratim Chowdhary"], "title": "$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Understanding which neural components drive specific capabilities in\nmid-sized language models ($\\leq$10B parameters) remains a key challenge. We\nintroduce the $(\\bm{K}, \\epsilon)$-Minimum Sufficient Head Circuit ($K$-MSHC),\na methodology to identify minimal sets of attention heads crucial for\nclassification tasks as well as Search-K-MSHC, an efficient algorithm for\ndiscovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B,\nwe analyze three syntactic task families: grammar acceptability, arithmetic\nverification, and arithmetic word problems. Our findings reveal distinct\ntask-specific head circuits, with grammar tasks predominantly utilizing early\nlayers, word problems showing pronounced activity in both shallow and deep\nregions, and arithmetic verification demonstrating a more distributed pattern\nacross the network. We discover non-linear circuit overlap patterns, where\ndifferent task pairs share computational components at varying levels of\nimportance. While grammar and arithmetic share many \"weak\" heads, arithmetic\nand word problems share more consistently critical \"strong\" heads. Importantly,\nwe find that each task maintains dedicated \"super-heads\" with minimal\ncross-task overlap, suggesting that syntactic and numerical competencies emerge\nfrom specialized yet partially reusable head circuits."}
{"id": "2505.12815", "pdf": "https://arxiv.org/pdf/2505.12815", "abs": "https://arxiv.org/abs/2505.12815", "authors": ["Wenjiao Feng", "Rongxing Xiao", "Zonghang Li", "Hongfang Yu", "Gang Sun", "Long Luo", "Mohsen Guizani", "Qirong Ho"], "title": "Learning in Chaos: Efficient Autoscaling and Self-healing for Distributed Training at the Edge", "categories": ["cs.DC", "cs.AI", "68T99", "I.2.11"], "comment": "13 pages, 16 figures", "summary": "Frequent node and link changes in edge AI clusters disrupt distributed\ntraining, while traditional checkpoint-based recovery and cloud-centric\nautoscaling are too slow for scale-out and ill-suited to chaotic and\nself-governed edge. This paper proposes Chaos, a resilient and scalable edge\ndistributed training system with built-in self-healing and autoscaling. It\nspeeds up scale-out by using multi-neighbor replication with fast shard\nscheduling, allowing a new node to pull the latest training state from nearby\nneighbors in parallel while balancing the traffic load between them. It also\nuses a cluster monitor to track resource and topology changes to assist\nscheduler decisions, and handles scaling events through peer negotiation\nprotocols, enabling fully self-governed autoscaling without a central admin.\nExtensive experiments show that Chaos consistently achieves much lower\nscale-out delays than Pollux, EDL, and Autoscaling, and handles scale-in,\nconnect-link, and disconnect-link events within 1 millisecond, making it\nsmoother to handle node joins, exits, and failures. It also delivers the lowest\nidle time, showing superior resource use and scalability as the cluster grows."}
{"id": "2505.12272", "pdf": "https://arxiv.org/pdf/2505.12272", "abs": "https://arxiv.org/abs/2505.12272", "authors": ["Lingzhi Wang", "Pengcheng Huang", "Haotian Li", "Yuliang Wei", "Guodong Xin", "Rui Zhang", "Donglin Zhang", "Zhenzhou Ji", "Wei Wang"], "title": "Enhancing Knowledge Graph Completion with GNN Distillation and Probabilistic Interaction Modeling", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Knowledge graphs (KGs) serve as fundamental structures for organizing\ninterconnected data across diverse domains. However, most KGs remain\nincomplete, limiting their effectiveness in downstream applications. Knowledge\ngraph completion (KGC) aims to address this issue by inferring missing links,\nbut existing methods face critical challenges: deep graph neural networks\n(GNNs) suffer from over-smoothing, while embedding-based models fail to capture\nabstract relational features. This study aims to overcome these limitations by\nproposing a unified framework that integrates GNN distillation and abstract\nprobabilistic interaction modeling (APIM). GNN distillation approach introduces\nan iterative message-feature filtering process to mitigate over-smoothing,\npreserving the discriminative power of node representations. APIM module\ncomplements this by learning structured, abstract interaction patterns through\nprobabilistic signatures and transition matrices, allowing for a richer, more\nflexible representation of entity and relation interactions. We apply these\nmethods to GNN-based models and the APIM to embedding-based KGC models,\nconducting extensive evaluations on the widely used WN18RR and FB15K-237\ndatasets. Our results demonstrate significant performance gains over baseline\nmodels, showcasing the effectiveness of the proposed techniques. The findings\nhighlight the importance of both controlling information propagation and\nleveraging structured probabilistic modeling, offering new avenues for\nadvancing knowledge graph completion. And our codes are available at\nhttps://anonymous.4open.science/r/APIM_and_GNN-Distillation-461C."}
{"id": "2505.12821", "pdf": "https://arxiv.org/pdf/2505.12821", "abs": "https://arxiv.org/abs/2505.12821", "authors": ["Han Sun", "Zhen Sun", "Zongmin Zhang", "Linzhao Jia", "Wei Shao", "Min Zhang"], "title": "SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are emerging as dominant forces for textual\nstyle transfer. However, for arbitrary style transfer, LLMs face two key\nchallenges: (1) considerable reliance on manually-constructed prompts and (2)\nrigid stylistic biases inherent in LLMs. In this paper, we propose a novel\nSynthesize-then-Decode (SynDec) approach, which automatically synthesizes\nhigh-quality prompts and amplifies their roles during decoding process.\nSpecifically, our approach synthesizes prompts by selecting representative\nfew-shot samples, conducting a four-dimensional style analysis, and reranking\nthe candidates. At LLM decoding stage, the TST effect is amplified by\nmaximizing the contrast in output probabilities between scenarios with and\nwithout the synthesized prompt, as well as between prompts and negative\nsamples. We conduct extensive experiments and the results show that SynDec\noutperforms existing state-of-the-art LLM-based methods on five out of six\nbenchmarks (e.g., achieving up to a 9\\% increase in accuracy for\nmodern-to-Elizabethan English transfer). Detailed ablation studies further\nvalidate the effectiveness of SynDec."}
{"id": "2505.12289", "pdf": "https://arxiv.org/pdf/2505.12289", "abs": "https://arxiv.org/abs/2505.12289", "authors": ["Kingsley Yeon", "Promit Ghosal", "Mihai Anitescu"], "title": "BOLT: Block-Orthonormal Lanczos for Trace estimation of matrix functions", "categories": ["math.NA", "cs.DS", "cs.LG", "cs.NA"], "comment": null, "summary": "Efficient matrix trace estimation is essential for scalable computation of\nlog-determinants, matrix norms, and distributional divergences. In many\nlarge-scale applications, the matrices involved are too large to store or\naccess in full, making even a single matrix-vector (mat-vec) product\ninfeasible. Instead, one often has access only to small subblocks of the matrix\nor localized matrix-vector products on restricted index sets. Hutch++ achieves\noptimal convergence rate but relies on randomized SVD and assumes full mat-vec\naccess, making it difficult to apply in these constrained settings. We propose\nthe Block-Orthonormal Stochastic Lanczos Quadrature (BOLT), which matches\nHutch++ accuracy with a simpler implementation based on orthonormal block\nprobes and Lanczos iterations. BOLT builds on the Stochastic Lanczos Quadrature\n(SLQ) framework, which combines random probing with Krylov subspace methods to\nefficiently approximate traces of matrix functions, and performs better than\nHutch++ in near flat-spectrum regimes. To address memory limitations and\npartial access constraints, we introduce Subblock SLQ, a variant of BOLT that\noperates only on small principal submatrices. As a result, this framework\nyields a proxy KL divergence estimator and an efficient method for computing\nthe Wasserstein-2 distance between Gaussians - both compatible with low-memory\nand partial-access regimes. We provide theoretical guarantees and demonstrate\nstrong empirical performance across a range of high-dimensional settings."}
{"id": "2505.12837", "pdf": "https://arxiv.org/pdf/2505.12837", "abs": "https://arxiv.org/abs/2505.12837", "authors": ["Christian Braun", "Alexander Lilienbeck", "Daniel Mentjukov"], "title": "The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 3 figures", "summary": "Legal contracts possess an inherent, semantically vital structure (e.g.,\nsections, clauses) that is crucial for human comprehension but whose impact on\nLLM processing remains under-explored. This paper investigates the effects of\nexplicit input text structure and prompt engineering on the performance of\nGPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the\nCUAD. We compare model exact-match accuracy across various input formats:\nwell-structured plain-text (human-generated from CUAD), plain-text cleaned of\nline breaks, extracted plain-text from Azure OCR, plain-text extracted by\nGPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o\nVision. To give an indication of the impact of possible prompt engineering, we\nassess the impact of shifting task instructions to the system prompt and\nexplicitly informing the model about the structured nature of the input. Our\nfindings reveal that GPT-4o demonstrates considerable robustness to variations\nin input structure, but lacks in overall performance. Conversely, GPT-4.1's\nperformance is markedly sensitive; poorly structured inputs yield suboptimal\nresults (but identical with GPT-4o), while well-structured formats (original\nCUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by\n~20 percentage points. Optimizing the system prompt to include task details and\nan advisory about structured input further elevates GPT-4.1's accuracy by an\nadditional ~10-13 percentage points, with Markdown ultimately achieving the\nhighest performance under these conditions (79 percentage points overall\nexact-match accuracy). This research empirically demonstrates that while newer\nmodels exhibit greater resilience, careful input structuring and strategic\nprompt design remain critical for optimizing the performance of LLMs, and can\nsignificantly affect outcomes in high-stakes legal applications."}
{"id": "2505.12296", "pdf": "https://arxiv.org/pdf/2505.12296", "abs": "https://arxiv.org/abs/2505.12296", "authors": ["Haiyu Deng", "Yanna Jiang", "Guangsheng Yu", "Qin Wang", "Xu Wang", "Baihe Ma", "Wei Ni", "Ren Ping Liu"], "title": "PoLO: Proof-of-Learning and Proof-of-Ownership at Once with Chained Watermarking", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Machine learning models are increasingly shared and outsourced, raising\nrequirements of verifying training effort (Proof-of-Learning, PoL) to ensure\nclaimed performance and establishing ownership (Proof-of-Ownership, PoO) for\ntransactions. When models are trained by untrusted parties, PoL and PoO must be\nenforced together to enable protection, attribution, and compensation. However,\nexisting studies typically address them separately, which not only weakens\nprotection against forgery and privacy breaches but also leads to high\nverification overhead.\n  We propose PoLO, a unified framework that simultaneously achieves PoL and PoO\nusing chained watermarks. PoLO splits the training process into fine-grained\ntraining shards and embeds a dedicated watermark in each shard. Each watermark\nis generated using the hash of the preceding shard, certifying the training\nprocess of the preceding shard. The chained structure makes it computationally\ndifficult to forge any individual part of the whole training process. The\ncomplete set of watermarks serves as the PoL, while the final watermark\nprovides the PoO. PoLO offers more efficient and privacy-preserving\nverification compared to the vanilla PoL solutions that rely on gradient-based\ntrajectory tracing and inadvertently expose training data during verification,\nwhile maintaining the same level of ownership assurance of watermark-based PoO\nschemes. Our evaluation shows that PoLO achieves 99% watermark detection\naccuracy for ownership verification, while preserving data privacy and cutting\nverification costs to just 1.5-10% of traditional methods. Forging PoLO demands\n1.1-4x more resources than honest proof generation, with the original proof\nretaining over 90% detection accuracy even after attacks."}
{"id": "2505.12843", "pdf": "https://arxiv.org/pdf/2505.12843", "abs": "https://arxiv.org/abs/2505.12843", "authors": ["Kangwen Zhao", "Jianfeng Cai", "Jinhua Zhu", "Ruopei Sun", "Dongyun Xue", "Wengang Zhou", "Li Li", "Houqiang Li"], "title": "Bias Fitting to Mitigate Length Bias of Reward Model in RLHF", "categories": ["cs.LG", "cs.AI"], "comment": "Due to the word limit for arXiv abstract, the abstract here has been\n  abridged compared to the one in the PDF", "summary": "Reinforcement Learning from Human Feedback relies on reward models to align\nlarge language models with human preferences. However, RLHF often suffers from\nreward hacking, wherein policy learning exploits flaws in the trained reward\nmodel to maximize reward scores without genuinely aligning with human\npreferences. A significant example of such reward hacking is length bias, where\nreward models usually favor longer responses irrespective of actual response\nquality. Previous works on length bias have notable limitations, these\napproaches either mitigate bias without characterizing the bias form, or simply\nassume a linear length-reward relation. To accurately model the intricate\nnature of length bias and facilitate more effective bias mitigation, we propose\nFiMi-RM (Bias Fitting to Mitigate Length Bias of Reward Model in RLHF), a\nframework that autonomously learns and corrects underlying bias patterns. Our\napproach consists of three stages: First, we train a standard reward model\nwhich inherently contains length bias. Next, we deploy a lightweight fitting\nmodel to explicitly capture the non-linear relation between length and reward.\nFinally, we incorporate this learned relation into the reward model to debias.\nExperimental results demonstrate that FiMi-RM achieves a more balanced\nlength-reward distribution. Furthermore, when applied to alignment algorithms,\nour debiased reward model improves length-controlled win rate and reduces\nverbosity without compromising its performance."}
{"id": "2505.12312", "pdf": "https://arxiv.org/pdf/2505.12312", "abs": "https://arxiv.org/abs/2505.12312", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Visuospatial Cognitive Assistant", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "31 pages, 10 figures, 6 tables. The implementation and fine-tuned\n  model (ViCA-7B) are publicly available at https://huggingface.co/nkkbr/ViCA.\n  The ViCA-322K dataset can be found at\n  https://huggingface.co/datasets/nkkbr/ViCA-322K, and the ViCA-Thinking-2.68K\n  dataset is at https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k", "summary": "Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence."}
{"id": "2505.12851", "pdf": "https://arxiv.org/pdf/2505.12851", "abs": "https://arxiv.org/abs/2505.12851", "authors": ["Yanhua Wen", "Lu Ai", "Gang Liu", "Chuang Li", "Jianhao Wei"], "title": "FLTG: Byzantine-Robust Federated Learning via Angle-Based Defense and Non-IID-Aware Weighting", "categories": ["cs.CR", "cs.AI"], "comment": "14 pages, 5 figures, BlockSys2025", "summary": "Byzantine attacks during model aggregation in Federated Learning (FL)\nthreaten training integrity by manipulating malicious clients' updates.\nExisting methods struggle with limited robustness under high malicious client\nratios and sensitivity to non-i.i.d. data, leading to degraded accuracy. To\naddress this, we propose FLTG, a novel aggregation algorithm integrating\nangle-based defense and dynamic reference selection. FLTG first filters clients\nvia ReLU-clipped cosine similarity, leveraging a server-side clean dataset to\nexclude misaligned updates. It then dynamically selects a reference client\nbased on the prior global model to mitigate non-i.i.d. bias, assigns\naggregation weights inversely proportional to angular deviations, and\nnormalizes update magnitudes to suppress malicious scaling. Evaluations across\ndatasets of varying complexity under five classic attacks demonstrate FLTG's\nsuperiority over state-of-the-art methods under extreme bias scenarios and\nsustains robustness with a higher proportion(over 50%) of malicious clients."}
{"id": "2505.12317", "pdf": "https://arxiv.org/pdf/2505.12317", "abs": "https://arxiv.org/abs/2505.12317", "authors": ["Ruoqi Wang", "Haitao Wang", "Shaojie Guo", "Qiong Luo"], "title": "Improving Out-of-Domain Robustness with Targeted Augmentation in Frequency and Pixel Spaces", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Out-of-domain (OOD) robustness under domain adaptation settings, where\nlabeled source data and unlabeled target data come from different\ndistributions, is a key challenge in real-world applications. A common approach\nto improving OOD robustness is through data augmentations. However, in\nreal-world scenarios, models trained with generic augmentations can only\nimprove marginally when generalized under distribution shifts toward unlabeled\ntarget domains. While dataset-specific targeted augmentations can address this\nissue, they typically require expert knowledge and extensive prior data\nanalysis to identify the nature of the datasets and domain shift. To address\nthese challenges, we propose Frequency-Pixel Connect, a domain-adaptation\nframework that enhances OOD robustness by introducing a targeted augmentation\nin both the frequency space and pixel space. Specifically, we mix the amplitude\nspectrum and pixel content of a source image and a target image to generate\naugmented samples that introduce domain diversity while preserving the semantic\nstructure of the source image. Unlike previous targeted augmentation methods\nthat are both dataset-specific and limited to the pixel space, Frequency-Pixel\nConnect is dataset-agnostic, enabling broader and more flexible applicability\nbeyond natural image datasets. We further analyze the effectiveness of\nFrequency-Pixel Connect by evaluating the performance of our method connecting\nsame-class cross-domain samples while separating different-class examples. We\ndemonstrate that Frequency-Pixel Connect significantly improves cross-domain\nconnectivity and outperforms previous generic methods on four diverse\nreal-world benchmarks across vision, medical, audio, and astronomical domains,\nand it also outperforms other dataset-specific targeted augmentation methods."}
{"id": "2505.12863", "pdf": "https://arxiv.org/pdf/2505.12863", "abs": "https://arxiv.org/abs/2505.12863", "authors": ["Jongmin Jung", "Dongmin Kim", "Sihun Lee", "Seola Cho", "Hyungjoon Soh", "Irmak Bukey", "Chris Donahue", "Dasaem Jeong"], "title": "Unified Cross-modal Translation of Score Images, Symbolic Music, and Performance Audio", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "comment": "Submitted to IEEE Transactions on Audio, Speech and Language\n  Processing (TASLPRO)", "summary": "Music exists in various modalities, such as score images, symbolic scores,\nMIDI, and audio. Translations between each modality are established as core\ntasks of music information retrieval, such as automatic music transcription\n(audio-to-MIDI) and optical music recognition (score image to symbolic score).\nHowever, most past work on multimodal translation trains specialized models on\nindividual translation tasks. In this paper, we propose a unified approach,\nwhere we train a general-purpose model on many translation tasks\nsimultaneously. Two key factors make this unified approach viable: a new\nlarge-scale dataset and the tokenization of each modality. Firstly, we propose\na new dataset that consists of more than 1,300 hours of paired audio-score\nimage data collected from YouTube videos, which is an order of magnitude larger\nthan any existing music modal translation datasets. Secondly, our unified\ntokenization framework discretizes score images, audio, MIDI, and MusicXML into\na sequence of tokens, enabling a single encoder-decoder Transformer to tackle\nmultiple cross-modal translation as one coherent sequence-to-sequence task.\nExperimental results confirm that our unified multitask model improves upon\nsingle-task baselines in several key areas, notably reducing the symbol error\nrate for optical music recognition from 24.58% to a state-of-the-art 13.67%,\nwhile similarly substantial improvements are observed across the other\ntranslation tasks. Notably, our approach achieves the first successful\nscore-image-conditioned audio generation, marking a significant breakthrough in\ncross-modal music generation."}
{"id": "2505.12327", "pdf": "https://arxiv.org/pdf/2505.12327", "abs": "https://arxiv.org/abs/2505.12327", "authors": ["Albert Zhao", "Stefano Soatto"], "title": "Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion Predictions", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "IEEE International Conference on Robotics and Automation (ICRA) 2025", "summary": "We describe a robust planning method for autonomous driving that mixes normal\nand adversarial agent predictions output by a diffusion model trained for\nmotion prediction. We first train a diffusion model to learn an unbiased\ndistribution of normal agent behaviors. We then generate a distribution of\nadversarial predictions by biasing the diffusion model at test time to generate\npredictions that are likely to collide with a candidate plan. We score plans\nusing expected cost with respect to a mixture distribution of normal and\nadversarial predictions, leading to a planner that is robust against\nadversarial behaviors but not overly conservative when agents behave normally.\nUnlike current approaches, we do not use risk measures that over-weight\nadversarial behaviors while placing little to no weight on low-cost normal\nbehaviors or use hard safety constraints that may not be appropriate for all\ndriving scenarios. We show the effectiveness of our method on single-agent and\nmulti-agent jaywalking scenarios as well as a red light violation scenario."}
{"id": "2505.12864", "pdf": "https://arxiv.org/pdf/2505.12864", "abs": "https://arxiv.org/abs/2505.12864", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan Hermstrüwer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "comment": null, "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/"}
{"id": "2505.12331", "pdf": "https://arxiv.org/pdf/2505.12331", "abs": "https://arxiv.org/abs/2505.12331", "authors": ["Yuancheng Jiang", "Roland Yap", "Zhenkai Liang"], "title": "OSS-Bench: Benchmark Generator for Coding LLMs", "categories": ["cs.SE", "cs.LG"], "comment": "19 pages", "summary": "In light of the rapid adoption of AI coding assistants, LLM-assisted\ndevelopment has become increasingly prevalent, creating an urgent need for\nrobust evaluation of generated code quality. Existing benchmarks often require\nextensive manual effort to create static datasets, rely on indirect or\ninsufficiently challenging tasks, depend on non-scalable ground truth, or\nneglect critical low-level security evaluations, particularly memory-safety\nissues. In this work, we introduce OSS-Bench, a benchmark generator that\nautomatically constructs large-scale, live evaluation tasks from real-world\nopen-source software. OSS-Bench replaces functions with LLM-generated code and\nevaluates them using three natural metrics: compilability, functional\ncorrectness, and memory safety, leveraging robust signals like compilation\nfailures, test-suite violations, and sanitizer alerts as ground truth. In our\nevaluation, the benchmark, instantiated as OSS-Bench(php) and OSS-Bench(sql),\nprofiles 17 diverse LLMs, revealing insights such as intra-family behavioral\npatterns and inconsistencies between model size and performance. Our results\ndemonstrate that OSS-Bench mitigates overfitting by leveraging the evolving\ncomplexity of OSS and highlights LLMs' limited understanding of low-level code\nsecurity via extended fuzzing experiments. Overall, OSS-Bench offers a\npractical and scalable framework for benchmarking the real-world coding\ncapabilities of LLMs."}
{"id": "2505.12869", "pdf": "https://arxiv.org/pdf/2505.12869", "abs": "https://arxiv.org/abs/2505.12869", "authors": ["Koki Wakiyama", "Tomohiro I", "Hiroshi Sakamoto"], "title": "Outsourced Privacy-Preserving Feature Selection Based on Fully Homomorphic Encryption", "categories": ["cs.CR", "cs.AI"], "comment": "14 pages", "summary": "Feature selection is a technique that extracts a meaningful subset from a set\nof features in training data. When the training data is large-scale,\nappropriate feature selection enables the removal of redundant features, which\ncan improve generalization performance, accelerate the training process, and\nenhance the interpretability of the model. This study proposes a\nprivacy-preserving computation model for feature selection. Generally, when the\ndata owner and analyst are the same, there is no need to conceal the private\ninformation. However, when they are different parties or when multiple owners\nexist, an appropriate privacy-preserving framework is required. Although\nvarious private feature selection algorithms, they all require two or more\ncomputing parties and do not guarantee security in environments where no\nexternal party can be fully trusted. To address this issue, we propose the\nfirst outsourcing algorithm for feature selection using fully homomorphic\nencryption. Compared to a prior two-party algorithm, our result improves the\ntime and space complexity O(kn^2) to O(kn log^3 n) and O(kn), where k and n\ndenote the number of features and data samples, respectively. We also\nimplemented the proposed algorithm and conducted comparative experiments with\nthe naive one. The experimental result shows the efficiency of our method even\nwith small datasets."}
{"id": "2505.12349", "pdf": "https://arxiv.org/pdf/2505.12349", "abs": "https://arxiv.org/abs/2505.12349", "authors": ["Axel Abels", "Tom Lenaerts"], "title": "Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted for publication in the Proceedings of the 34th International\n  Joint Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Despite their performance, large language models (LLMs) can inadvertently\nperpetuate biases found in the data they are trained on. By analyzing LLM\nresponses to bias-eliciting headlines, we find that these models often mirror\nhuman biases. To address this, we explore crowd-based strategies for mitigating\nbias through response aggregation. We first demonstrate that simply averaging\nresponses from multiple LLMs, intended to leverage the \"wisdom of the crowd\",\ncan exacerbate existing biases due to the limited diversity within LLM crowds.\nIn contrast, we show that locally weighted aggregation methods more effectively\nleverage the wisdom of the LLM crowd, achieving both bias mitigation and\nimproved accuracy. Finally, recognizing the complementary strengths of LLMs\n(accuracy) and humans (diversity), we demonstrate that hybrid crowds containing\nboth significantly enhance performance and further reduce biases across ethnic\nand gender-related contexts."}
{"id": "2505.12871", "pdf": "https://arxiv.org/pdf/2505.12871", "abs": "https://arxiv.org/abs/2505.12871", "authors": ["Zi Liang", "Haibo Hu", "Qingqing Ye", "Yaxin Xiao", "Ronghua Li"], "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "To appear at ICML 25", "summary": "Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings."}
{"id": "2505.12360", "pdf": "https://arxiv.org/pdf/2505.12360", "abs": "https://arxiv.org/abs/2505.12360", "authors": ["Siwen Zhang", "Xizeng Zhao", "Zhengzhi Deng", "Zhaoyuan Huang", "Gang Tao", "Nuo Xu", "Zhouteng Ye"], "title": "LaPON: A Lagrange's-mean-value-theorem-inspired operator network for solving PDEs and its application on NSE", "categories": ["physics.comp-ph", "cs.LG"], "comment": null, "summary": "Accelerating the solution of nonlinear partial differential equations (PDEs)\nwhile maintaining accuracy at coarse spatiotemporal resolution remains a key\nchallenge in scientific computing. Physics-informed machine learning (ML)\nmethods such as Physics-Informed Neural Networks (PINNs) introduce prior\nknowledge through loss functions to ensure physical consistency, but their\n\"soft constraints\" are usually not strictly satisfied. Here, we propose LaPON,\nan operator network inspired by the Lagrange's mean value theorem, which embeds\nprior knowledge directly into the neural network architecture instead of the\nloss function, making the neural network naturally satisfy the given\nconstraints. This is a hybrid framework that combines neural operators with\ntraditional numerical methods, where neural operators are used to compensate\nfor the effect of discretization errors on the analytical scale in\nunder-resolution simulations. As evaluated on turbulence problem modeled by the\nNavier-Stokes equations (NSE), the multiple time step extrapolation accuracy\nand stability of LaPON exceed the direct numerical simulation baseline at 8x\ncoarser grids and 8x larger time steps, while achieving a vorticity correlation\nof more than 0.98 with the ground truth. It is worth noting that the model can\nbe well generalized to unseen flow states, such as turbulence with different\nforcing, without retraining. In addition, with the same training data, LaPON's\ncomprehensive metrics on the out-of-distribution test set are at least\napproximately twice as good as two popular ML baseline methods. By combining\nnumerical computing with machine learning, LaPON provides a scalable and\nreliable solution for high-fidelity fluid dynamics simulation, showing the\npotential for wide application in fields such as weather forecasting and\nengineering design."}
{"id": "2505.12880", "pdf": "https://arxiv.org/pdf/2505.12880", "abs": "https://arxiv.org/abs/2505.12880", "authors": ["Maksim Zhdanov", "Nabil Iqbal", "Erik Bekkers", "Patrick Forré"], "title": "AdS-GNN -- a Conformally Equivariant Graph Neural Network", "categories": ["cs.LG", "cs.AI", "hep-th"], "comment": null, "summary": "Conformal symmetries, i.e.\\ coordinate transformations that preserve angles,\nplay a key role in many fields, including physics, mathematics, computer vision\nand (geometric) machine learning. Here we build a neural network that is\nequivariant under general conformal transformations. To achieve this, we lift\ndata from flat Euclidean space to Anti de Sitter (AdS) space. This allows us to\nexploit a known correspondence between conformal transformations of flat space\nand isometric transformations on the AdS space. We then build upon the fact\nthat such isometric transformations have been extensively studied on general\ngeometries in the geometric deep learning literature. We employ message-passing\nlayers conditioned on the proper distance, yielding a computationally efficient\nframework. We validate our model on tasks from computer vision and statistical\nphysics, demonstrating strong performance, improved generalization capacities,\nand the ability to extract conformal data such as scaling dimensions from the\ntrained network."}
{"id": "2505.12363", "pdf": "https://arxiv.org/pdf/2505.12363", "abs": "https://arxiv.org/abs/2505.12363", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "26 pages, 19 figures, 4 tables. Code, models, and dataset are\n  available at our project page: https://github.com/nkkbr/ViCA", "summary": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research."}
{"id": "2505.12882", "pdf": "https://arxiv.org/pdf/2505.12882", "abs": "https://arxiv.org/abs/2505.12882", "authors": ["Hao Wang", "Jindong Han", "Wei Fan", "Weijia Zhang", "Hao Liu"], "title": "PhyDA: Physics-Guided Diffusion Models for Data Assimilation in Atmospheric Systems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Data Assimilation (DA) plays a critical role in atmospheric science by\nreconstructing spatially continous estimates of the system state, which serves\nas initial conditions for scientific analysis. While recent advances in\ndiffusion models have shown great potential for DA tasks, most existing\napproaches remain purely data-driven and often overlook the physical laws that\ngovern complex atmospheric dynamics. As a result, they may yield physically\ninconsistent reconstructions that impair downstream applications. To overcome\nthis limitation, we propose PhyDA, a physics-guided diffusion framework\ndesigned to ensure physical coherence in atmospheric data assimilation. PhyDA\nintroduces two key components: (1) a Physically Regularized Diffusion Objective\nthat integrates physical constraints into the training process by penalizing\ndeviations from known physical laws expressed as partial differential\nequations, and (2) a Virtual Reconstruction Encoder that bridges observational\nsparsity for structured latent representations, further enhancing the model's\nability to infer complete and physically coherent states. Experiments on the\nERA5 reanalysis dataset demonstrate that PhyDA achieves superior accuracy and\nbetter physical plausibility compared to state-of-the-art baselines. Our\nresults emphasize the importance of combining generative modeling with\ndomain-specific physical knowledge and show that PhyDA offers a promising\ndirection for improving real-world data assimilation systems."}
{"id": "2505.12369", "pdf": "https://arxiv.org/pdf/2505.12369", "abs": "https://arxiv.org/abs/2505.12369", "authors": ["Fernando Zhapa-Camacho", "Robert Hoehndorf"], "title": "Fully Geometric Multi-Hop Reasoning on Knowledge Graphs with Transitive Relations", "categories": ["cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Geometric embedding methods have shown to be useful for multi-hop reasoning\non knowledge graphs by mapping entities and logical operations to geometric\nregions and geometric transformations, respectively. Geometric embeddings\nprovide direct interpretability framework for queries. However, current methods\nhave only leveraged the geometric construction of entities, failing to map\nlogical operations to geometric transformations and, instead, using neural\ncomponents to learn these operations. We introduce GeometrE, a geometric\nembedding method for multi-hop reasoning, which does not require learning the\nlogical operations and enables full geometric interpretability. Additionally,\nunlike previous methods, we introduce a transitive loss function and show that\nit can preserve the logical rule $\\forall a,b,c: r(a,b) \\land r(b,c) \\to\nr(a,c)$. Our experiments show that GeometrE outperforms current\nstate-of-the-art methods on standard benchmark datasets."}
{"id": "2505.12884", "pdf": "https://arxiv.org/pdf/2505.12884", "abs": "https://arxiv.org/abs/2505.12884", "authors": ["Yuanze Hu", "Zhaoxin Fan", "Xinyu Wang", "Gen Li", "Ye Qiu", "Zhichao Yang", "Wenjun Wu", "Kejian Wu", "Yifan Sun", "Xiaotie Deng", "Jin Dong"], "title": "TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Lightweight Vision-Language Models (VLMs) are indispensable for\nresource-constrained applications. The prevailing approach to aligning vision\nand language models involves freezing both the vision encoder and the language\nmodel while training small connector modules. However, this strategy heavily\ndepends on the intrinsic capabilities of the language model, which can be\nsuboptimal for lightweight models with limited representational capacity. In\nthis work, we investigate this alignment bottleneck through the lens of mutual\ninformation, demonstrating that the constrained capacity of the language model\ninherently limits the Effective Mutual Information (EMI) between multimodal\ninputs and outputs, thereby compromising alignment quality. To address this\nchallenge, we propose TinyAlign, a novel framework inspired by\nRetrieval-Augmented Generation, which strategically retrieves relevant context\nfrom a memory bank to enrich multimodal inputs and enhance their alignment.\nExtensive empirical evaluations reveal that TinyAlign significantly reduces\ntraining loss, accelerates convergence, and enhances task performance.\nRemarkably, it allows models to achieve baseline-level performance with only\n40\\% of the fine-tuning data, highlighting exceptional data efficiency. Our\nwork thus offers a practical pathway for developing more capable lightweight\nVLMs while introducing a fresh theoretical lens to better understand and\naddress alignment bottlenecks in constrained multimodal systems."}
{"id": "2505.12371", "pdf": "https://arxiv.org/pdf/2505.12371", "abs": "https://arxiv.org/abs/2505.12371", "authors": ["Yinghao Zhu", "Ziyi He", "Haoran Hu", "Xiaochen Zheng", "Xichen Zhang", "Zixiang Wang", "Junyi Gao", "Liantao Ma", "Lequan Yu"], "title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/."}
{"id": "2505.12894", "pdf": "https://arxiv.org/pdf/2505.12894", "abs": "https://arxiv.org/abs/2505.12894", "authors": ["Le Cheng", "Peican Zhu", "Yangming Guo", "Keke Tang", "Chao Gao", "Zhen Wang"], "title": "HyperDet: Source Detection in Hypergraphs via Interactive Relationship Construction and Feature-rich Attention Fusion", "categories": ["cs.SI", "cs.AI"], "comment": "Accepted by IJCAI25", "summary": "Hypergraphs offer superior modeling capabilities for social networks,\nparticularly in capturing group phenomena that extend beyond pairwise\ninteractions in rumor propagation. Existing approaches in rumor source\ndetection predominantly focus on dyadic interactions, which inadequately\naddress the complexity of more intricate relational structures. In this study,\nwe present a novel approach for Source Detection in Hypergraphs (HyperDet) via\nInteractive Relationship Construction and Feature-rich Attention Fusion.\nSpecifically, our methodology employs an Interactive Relationship Construction\nmodule to accurately model both the static topology and dynamic interactions\namong users, followed by the Feature-rich Attention Fusion module, which\nautonomously learns node features and discriminates between nodes using a\nself-attention mechanism, thereby effectively learning node representations\nunder the framework of accurately modeled higher-order relationships. Extensive\nexperimental validation confirms the efficacy of our HyperDet approach,\nshowcasing its superiority relative to current state-of-the-art methods."}
{"id": "2505.12373", "pdf": "https://arxiv.org/pdf/2505.12373", "abs": "https://arxiv.org/abs/2505.12373", "authors": ["Kapil Dev"], "title": "Modeling Aesthetic Preferences in 3D Shapes: A Large-Scale Paired Comparison Study Across Object Categories", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "11 pages, 8 figures, submitted to IEEE Transactions on Visualization\n  and Computer Graphics (TVCG)", "summary": "Human aesthetic preferences for 3D shapes are central to industrial design,\nvirtual reality, and consumer product development. However, most computational\nmodels of 3D aesthetics lack empirical grounding in large-scale human\njudgments, limiting their practical relevance. We present a large-scale study\nof human preferences. We collected 22,301 pairwise comparisons across five\nobject categories (chairs, tables, mugs, lamps, and dining chairs) via Amazon\nMechanical Turk. Building on a previously published\ndataset~\\cite{dev2020learning}, we introduce new non-linear modeling and\ncross-category analysis to uncover the geometric drivers of aesthetic\npreference. We apply the Bradley-Terry model to infer latent aesthetic scores\nand use Random Forests with SHAP analysis to identify and interpret the most\ninfluential geometric features (e.g., symmetry, curvature, compactness). Our\ncross-category analysis reveals both universal principles and domain-specific\ntrends in aesthetic preferences. We focus on human interpretable geometric\nfeatures to ensure model transparency and actionable design insights, rather\nthan relying on black-box deep learning approaches. Our findings bridge\ncomputational aesthetics and cognitive science, providing practical guidance\nfor designers and a publicly available dataset to support reproducibility. This\nwork advances the understanding of 3D shape aesthetics through a human-centric,\ndata-driven framework."}
{"id": "2505.12900", "pdf": "https://arxiv.org/pdf/2505.12900", "abs": "https://arxiv.org/abs/2505.12900", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Jianyuan Liang", "Haoyue Jiao", "Yaxian Qing", "Xiaopu Zhang", "Xu Li", "Zhipeng Gui", "Xuefeng Guan", "Longgang Xiang"], "title": "AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models", "categories": ["cs.SE", "cs.AI", "cs.CG", "cs.CL", "cs.DB"], "comment": null, "summary": "Geospatial code generation is emerging as a key direction in the integration\nof artificial intelligence and geoscientific analysis. However, there remains a\nlack of standardized tools for automatic evaluation in this domain. To address\nthis gap, we propose AutoGEEval, the first multimodal, unit-level automated\nevaluation framework for geospatial code generation tasks on the Google Earth\nEngine (GEE) platform powered by large language models (LLMs). Built upon the\nGEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)\ncomprising 1325 test cases that span 26 GEE data types. The framework\nintegrates both question generation and answer verification components to\nenable an end-to-end automated evaluation pipeline-from function invocation to\nexecution validation. AutoGEEval supports multidimensional quantitative\nanalysis of model outputs in terms of accuracy, resource consumption, execution\nefficiency, and error types. We evaluate 18 state-of-the-art LLMs-including\ngeneral-purpose, reasoning-augmented, code-centric, and geoscience-specialized\nmodels-revealing their performance characteristics and potential optimization\npathways in GEE code generation. This work provides a unified protocol and\nfoundational resource for the development and assessment of geospatial code\ngeneration models, advancing the frontier of automated natural language to\ndomain-specific code translation."}
{"id": "2505.12375", "pdf": "https://arxiv.org/pdf/2505.12375", "abs": "https://arxiv.org/abs/2505.12375", "authors": ["Andreas Floros", "Seyed-Mohsen Moosavi-Dezfooli", "Pier Luigi Dragotti"], "title": "Trustworthy Image Super-Resolution via Generative Pseudoinverse", "categories": ["eess.IV", "cs.LG"], "comment": null, "summary": "We consider the problem of trustworthy image restoration, taking the form of\na constrained optimization over the prior density. To this end, we develop\ngenerative models for the task of image super-resolution that respect the\ndegradation process and that can be made asymptotically consistent with the\nlow-resolution measurements, outperforming existing methods by a large margin\nin that respect."}
{"id": "2505.12903", "pdf": "https://arxiv.org/pdf/2505.12903", "abs": "https://arxiv.org/abs/2505.12903", "authors": ["Shiao Wang", "Xiao Wang", "Liye Jin", "Bo Jiang", "Lin Zhu", "Lan Chen", "Yonghong Tian", "Bin Luo"], "title": "Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing tracking algorithms typically rely on low-frame-rate RGB cameras\ncoupled with computationally intensive deep neural network architectures to\nachieve effective tracking. However, such frame-based methods inherently face\nchallenges in achieving low-latency performance and often fail in\nresource-constrained environments. Visual object tracking using bio-inspired\nevent cameras has emerged as a promising research direction in recent years,\noffering distinct advantages for low-latency applications. In this paper, we\npropose a novel Slow-Fast Tracking paradigm that flexibly adapts to different\noperational requirements, termed SFTrack. The proposed framework supports two\ncomplementary modes, i.e., a high-precision slow tracker for scenarios with\nsufficient computational resources, and an efficient fast tracker tailored for\nlatency-aware, resource-constrained environments. Specifically, our framework\nfirst performs graph-based representation learning from\nhigh-temporal-resolution event streams, and then integrates the learned\ngraph-structured information into two FlashAttention-based vision backbones,\nyielding the slow and fast trackers, respectively. The fast tracker achieves\nlow latency through a lightweight network design and by producing multiple\nbounding box outputs in a single forward pass. Finally, we seamlessly combine\nboth trackers via supervised fine-tuning and further enhance the fast tracker's\nperformance through a knowledge distillation strategy. Extensive experiments on\npublic benchmarks, including FE240, COESOT, and EventVOT, demonstrate the\neffectiveness and efficiency of our proposed method across different real-world\nscenarios. The source code has been released on\nhttps://github.com/Event-AHU/SlowFast_Event_Track."}
{"id": "2505.12378", "pdf": "https://arxiv.org/pdf/2505.12378", "abs": "https://arxiv.org/abs/2505.12378", "authors": ["Andi Han", "Pierre-Louis Poirion", "Akiko Takeda"], "title": "Efficient Optimization with Orthogonality Constraint: a Randomized Riemannian Submanifold Method", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "Optimization with orthogonality constraints frequently arises in various\nfields such as machine learning. Riemannian optimization offers a powerful\nframework for solving these problems by equipping the constraint set with a\nRiemannian manifold structure and performing optimization intrinsically on the\nmanifold. This approach typically involves computing a search direction in the\ntangent space and updating variables via a retraction operation. However, as\nthe size of the variables increases, the computational cost of the retraction\ncan become prohibitively high, limiting the applicability of Riemannian\noptimization to large-scale problems. To address this challenge and enhance\nscalability, we propose a novel approach that restricts each update on a random\nsubmanifold, thereby significantly reducing the per-iteration complexity. We\nintroduce two sampling strategies for selecting the random submanifolds and\ntheoretically analyze the convergence of the proposed methods. We provide\nconvergence results for general nonconvex functions and functions that satisfy\nRiemannian Polyak-Lojasiewicz condition as well as for stochastic optimization\nsettings. Additionally, we demonstrate how our approach can be generalized to\nquotient manifolds derived from the orthogonal manifold. Extensive experiments\nverify the benefits of the proposed method, across a wide variety of problems."}
{"id": "2505.12904", "pdf": "https://arxiv.org/pdf/2505.12904", "abs": "https://arxiv.org/abs/2505.12904", "authors": ["Hilde I. Hummel", "Arwin Gansekoele", "Sandjai Bhulai", "Rob van der Mei"], "title": "The Computation of Generalized Embeddings for Underwater Acoustic Target Recognition using Contrastive Learning", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "The increasing level of sound pollution in marine environments poses an\nincreased threat to ocean health, making it crucial to monitor underwater\nnoise. By monitoring this noise, the sources responsible for this pollution can\nbe mapped. Monitoring is performed by passively listening to these sounds. This\ngenerates a large amount of data records, capturing a mix of sound sources such\nas ship activities and marine mammal vocalizations. Although machine learning\noffers a promising solution for automatic sound classification, current\nstate-of-the-art methods implement supervised learning. This requires a large\namount of high-quality labeled data that is not publicly available. In\ncontrast, a massive amount of lower-quality unlabeled data is publicly\navailable, offering the opportunity to explore unsupervised learning\ntechniques. This research explores this possibility by implementing an\nunsupervised Contrastive Learning approach. Here, a Conformer-based encoder is\noptimized by the so-called Variance-Invariance-Covariance Regularization loss\nfunction on these lower-quality unlabeled data and the translation to the\nlabeled data is made. Through classification tasks involving recognizing ship\ntypes and marine mammal vocalizations, our method demonstrates to produce\nrobust and generalized embeddings. This shows to potential of unsupervised\nmethods for various automatic underwater acoustic analysis tasks."}
{"id": "2505.12398", "pdf": "https://arxiv.org/pdf/2505.12398", "abs": "https://arxiv.org/abs/2505.12398", "authors": ["Yepeng Weng", "Qiao Hu", "Xujie Chen", "Li Liu", "Dianwen Mei", "Huishi Qiu", "Jiang Tian", "Zhongchao Shi"], "title": "Traversal Verification for Speculative Tree Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Speculative decoding is a promising approach for accelerating large language\nmodels. The primary idea is to use a lightweight draft model to speculate the\noutput of the target model for multiple subsequent timesteps, and then verify\nthem in parallel to determine whether the drafted tokens should be accepted or\nrejected. To enhance acceptance rates, existing frameworks typically construct\ntoken trees containing multiple candidates in each timestep. However, their\nreliance on token-level verification mechanisms introduces two critical\nlimitations: First, the probability distribution of a sequence differs from\nthat of individual tokens, leading to suboptimal acceptance length. Second,\ncurrent verification schemes begin from the root node and proceed layer by\nlayer in a top-down manner. Once a parent node is rejected, all its child nodes\nshould be discarded, resulting in inefficient utilization of speculative\ncandidates. This paper introduces Traversal Verification, a novel speculative\ndecoding algorithm that fundamentally rethinks the verification paradigm\nthrough leaf-to-root traversal. Our approach considers the acceptance of the\nentire token sequence from the current node to the root, and preserves\npotentially valid subsequences that would be prematurely discarded by existing\nmethods. We theoretically prove that the probability distribution obtained\nthrough Traversal Verification is identical to that of the target model,\nguaranteeing lossless inference while achieving substantial acceleration gains.\nExperimental results across different large language models and multiple tasks\nshow that our method consistently improves acceptance length and throughput\nover existing methods"}
{"id": "2505.12908", "pdf": "https://arxiv.org/pdf/2505.12908", "abs": "https://arxiv.org/abs/2505.12908", "authors": ["Xiao Wang", "Yu Jin", "Lan Chen", "Bo Jiang", "Lin Zhu", "Yonghong Tian", "Jin Tang", "Bin Luo"], "title": "Dynamic Graph Induced Contour-aware Heat Conduction Network for Event-based Object Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Event-based Vision Sensors (EVS) have demonstrated significant advantages\nover traditional RGB frame-based cameras in low-light conditions, high-speed\nmotion capture, and low latency. Consequently, object detection based on EVS\nhas attracted increasing attention from researchers. Current event stream\nobject detection algorithms are typically built upon Convolutional Neural\nNetworks (CNNs) or Transformers, which either capture limited local features\nusing convolutional filters or incur high computational costs due to the\nutilization of self-attention. Recently proposed vision heat conduction\nbackbone networks have shown a good balance between efficiency and accuracy;\nhowever, these models are not specifically designed for event stream data. They\nexhibit weak capability in modeling object contour information and fail to\nexploit the benefits of multi-scale features. To address these issues, this\npaper proposes a novel dynamic graph induced contour-aware heat conduction\nnetwork for event stream based object detection, termed CvHeat-DET. The\nproposed model effectively leverages the clear contour information inherent in\nevent streams to predict the thermal diffusivity coefficients within the heat\nconduction model, and integrates hierarchical structural graph features to\nenhance feature learning across multiple scales. Extensive experiments on three\nbenchmark datasets for event stream-based object detection fully validated the\neffectiveness of the proposed model. The source code of this paper will be\nreleased on https://github.com/Event-AHU/OpenEvDET."}
{"id": "2505.12412", "pdf": "https://arxiv.org/pdf/2505.12412", "abs": "https://arxiv.org/abs/2505.12412", "authors": ["Tim Y. J. Wang", "Juan Kuntz", "O. Deniz Akyildiz"], "title": "Training Latent Diffusion Models with Interacting Particle Algorithms", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We introduce a novel particle-based algorithm for end-to-end training of\nlatent diffusion models. We reformulate the training task as minimizing a free\nenergy functional and obtain a gradient flow that does so. By approximating the\nlatter with a system of interacting particles, we obtain the algorithm, which\nwe underpin it theoretically by providing error guarantees. The novel algorithm\ncompares favorably in experiments with previous particle-based methods and\nvariational inference analogues."}
{"id": "2505.12909", "pdf": "https://arxiv.org/pdf/2505.12909", "abs": "https://arxiv.org/abs/2505.12909", "authors": ["Alberto Fernández-Hernández", "Jose I. Mestre", "Manuel F. Dolz", "Jose Duato", "Enrique S. Quintana-Ortí"], "title": "Sinusoidal Initialization, Time for a New Start", "categories": ["cs.LG", "cs.AI", "I.2; G.3; I.2.6"], "comment": null, "summary": "Initialization plays a critical role in Deep Neural Network training,\ndirectly influencing convergence, stability, and generalization. Common\napproaches such as Glorot and He initializations rely on randomness, which can\nproduce uneven weight distributions across layer connections. In this paper, we\nintroduce the Sinusoidal initialization, a novel deterministic method that\nemploys sinusoidal functions to construct structured weight matrices expressly\nto improve the spread and balance of weights throughout the network while\nsimultaneously fostering a more uniform, well-conditioned distribution of\nneuron activation states from the very first forward pass. Because Sinusoidal\ninitialization begins with weights and activations that are already evenly and\nefficiently utilized, it delivers consistently faster convergence, greater\ntraining stability, and higher final accuracy across a wide range of models,\nincluding convolutional neural networks, vision transformers, and large\nlanguage models. On average, our experiments show an increase of 4.8 % in final\nvalidation accuracy and 20.9 % in convergence speed. By replacing randomness\nwith structure, this initialization provides a stronger and more reliable\nfoundation for Deep Learning systems."}
{"id": "2505.12433", "pdf": "https://arxiv.org/pdf/2505.12433", "abs": "https://arxiv.org/abs/2505.12433", "authors": ["Haodong Yang", "Lei Wang", "Md Zakir Hossain"], "title": "SRLoRA: Subspace Recomposition in Low-Rank Adaptation via Importance-Based Fusion and Reinitialization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Research report", "summary": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient\nfine-tuning (PEFT) method that injects two trainable low-rank matrices (A and\nB) into frozen pretrained models. While efficient, LoRA constrains updates to a\nfixed low-rank subspace (Delta W = BA), which can limit representational\ncapacity and hinder downstream performance. We introduce Subspace Recomposition\nin Low-Rank Adaptation (SRLoRA) via importance-based fusion and\nreinitialization, a novel approach that enhances LoRA's expressiveness without\ncompromising its lightweight structure. SRLoRA assigns importance scores to\neach LoRA pair (a column of B and the corresponding row of A), and dynamically\nrecomposes the subspace during training. Less important pairs are fused into\nthe frozen backbone, freeing capacity to reinitialize new pairs along unused\nprincipal directions derived from the pretrained weight's singular value\ndecomposition. This mechanism enables continual subspace refreshment and richer\nadaptation over time, without increasing the number of trainable parameters. We\nevaluate SRLoRA on both language and vision tasks, including the GLUE benchmark\nand various image classification datasets. SRLoRA consistently achieves faster\nconvergence and improved accuracy over standard LoRA, demonstrating its\ngenerality, efficiency, and potential for broader PEFT applications."}
{"id": "2505.12910", "pdf": "https://arxiv.org/pdf/2505.12910", "abs": "https://arxiv.org/abs/2505.12910", "authors": ["Le Cheng", "Peican Zhu", "Yangming Guo", "Chao Gao", "Zhen Wang", "Keke Tang"], "title": "SourceDetMamba: A Graph-aware State Space Model for Source Detection in Sequential Hypergraphs", "categories": ["cs.SI", "cs.AI"], "comment": "Accepted by IJCAI25", "summary": "Source detection on graphs has demonstrated high efficacy in identifying\nrumor origins. Despite advances in machine learning-based methods, many fail to\ncapture intrinsic dynamics of rumor propagation. In this work, we present\nSourceDetMamba: A Graph-aware State Space Model for Source Detection in\nSequential Hypergraphs, which harnesses the recent success of the state space\nmodel Mamba, known for its superior global modeling capabilities and\ncomputational efficiency, to address this challenge. Specifically, we first\nemploy hypergraphs to model high-order interactions within social networks.\nSubsequently, temporal network snapshots generated during the propagation\nprocess are sequentially fed in reverse order into Mamba to infer underlying\npropagation dynamics. Finally, to empower the sequential model to effectively\ncapture propagation patterns while integrating structural information, we\npropose a novel graph-aware state update mechanism, wherein the state of each\nnode is propagated and refined by both temporal dependencies and topological\ncontext. Extensive evaluations on eight datasets demonstrate that\nSourceDetMamba consistently outperforms state-of-the-art approaches."}
{"id": "2505.12444", "pdf": "https://arxiv.org/pdf/2505.12444", "abs": "https://arxiv.org/abs/2505.12444", "authors": ["Shuguang Yu", "Fan Zhou", "Yingjie Zhang", "Ziqi Chen", "Hongtu Zhu"], "title": "High-Dimensional Dynamic Covariance Models with Random Forests", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This paper introduces a novel nonparametric method for estimating\nhigh-dimensional dynamic covariance matrices with multiple conditioning\ncovariates, leveraging random forests and supported by robust theoretical\nguarantees. Unlike traditional static methods, our dynamic nonparametric\ncovariance models effectively capture distributional heterogeneity.\nFurthermore, unlike kernel-smoothing methods, which are restricted to a single\nconditioning covariate, our approach accommodates multiple covariates in a\nfully nonparametric framework. To the best of our knowledge, this is the first\nmethod to use random forests for estimating high-dimensional dynamic covariance\nmatrices. In high-dimensional settings, we establish uniform consistency\ntheory, providing nonasymptotic error rates and model selection properties,\neven when the response dimension grows sub-exponentially with the sample size.\nThese results hold uniformly across a range of conditioning variables. The\nmethod's effectiveness is demonstrated through simulations and a stock dataset\nanalysis, highlighting its ability to model complex dynamics in\nhigh-dimensional scenarios."}
{"id": "2505.12920", "pdf": "https://arxiv.org/pdf/2505.12920", "abs": "https://arxiv.org/abs/2505.12920", "authors": ["Paul Van Eecke", "Katrien Beuls"], "title": "PyFCG: Fluid Construction Grammar in Python", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "We present PyFCG, an open source software library that ports Fluid\nConstruction Grammar (FCG) to the Python programming language. PyFCG enables\nits users to seamlessly integrate FCG functionality into Python programs, and\nto use FCG in combination with other libraries within Python's rich ecosystem.\nApart from a general description of the library, this paper provides three\nwalkthrough tutorials that demonstrate example usage of PyFCG in typical use\ncases of FCG: (i) formalising and testing construction grammar analyses, (ii)\nlearning usage-based construction grammars from corpora, and (iii) implementing\nagent-based experiments on emergent communication."}
{"id": "2505.12454", "pdf": "https://arxiv.org/pdf/2505.12454", "abs": "https://arxiv.org/abs/2505.12454", "authors": ["Yuyang Ding", "Dan Qiao", "Juntao Li", "Jiajie Xu", "Pingfu Chao", "Xiaofang Zhou", "Min Zhang"], "title": "Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Distantly supervised named entity recognition (DS-NER) has emerged as a cheap\nand convenient alternative to traditional human annotation methods, enabling\nthe automatic generation of training data by aligning text with external\nresources. Despite the many efforts in noise measurement methods, few works\nfocus on the latent noise distribution between different distant annotation\nmethods. In this work, we explore the effectiveness and robustness of DS-NER by\ntwo aspects: (1) distant annotation techniques, which encompasses both\ntraditional rule-based methods and the innovative large language model\nsupervision approach, and (2) noise assessment, for which we introduce a novel\nframework. This framework addresses the challenges by distinctly categorizing\nthem into the unlabeled-entity problem (UEP) and the noisy-entity problem\n(NEP), subsequently providing specialized solutions for each. Our proposed\nmethod achieves significant improvements on eight real-world distant\nsupervision datasets originating from three different data sources and\ninvolving four distinct annotation techniques, confirming its superiority over\ncurrent state-of-the-art methods."}
{"id": "2505.12925", "pdf": "https://arxiv.org/pdf/2505.12925", "abs": "https://arxiv.org/abs/2505.12925", "authors": ["Han Deng", "Yuan Meng", "Shixiang Tang", "Wanli Ouyang", "Xinzhu Ma"], "title": "CPRet: A Dataset, Benchmark, and Model for Retrieval in Competitive Programming", "categories": ["cs.SE", "cs.AI", "cs.IR", "H.3.3"], "comment": "main 9 pages", "summary": "Competitive programming benchmarks are widely used in scenarios such as\nprogramming contests and large language model assessments. However, the growing\npresence of duplicate or highly similar problems raises concerns not only about\ncompetition fairness, but also about the validity of competitive programming as\na benchmark for model evaluation. In this paper, we propose a new problem --\nsimilar question retrieval -- to address this issue. Due to the lack of both\ndata and models, solving this problem is challenging. To this end, we introduce\nCPRet, a retrieval-oriented benchmark suite for competitive programming,\ncovering four retrieval tasks: two code-centric (i.e., Text-to-Code and\nCode-to-Code) and two newly proposed problem-centric tasks (i.e.,\nProblem-to-Duplicate and Simplified-to-Full), built from a combination of\nautomatically crawled problem-solution data and manually curated annotations.\nOur contribution includes both high-quality training data and temporally\nseparated test sets for reliable evaluation. In addition, we develop two\ntask-specialized retrievers based on this dataset: CPRetriever-Code, trained\nwith a novel Group-InfoNCE loss for problem-code alignment, and\nCPRetriever-Prob, fine-tuned for identifying problem-level similarity. Both\nmodels achieve strong results and are open-sourced for local use. Finally, we\nanalyze LiveCodeBench and find that high-similarity problems inflate model pass\nrates and reduce differentiation, underscoring the need for similarity-aware\nevaluation in future benchmarks.\n  Code and data are available at: https://github.com/coldchair/CPRet"}
{"id": "2505.12471", "pdf": "https://arxiv.org/pdf/2505.12471", "abs": "https://arxiv.org/abs/2505.12471", "authors": ["Antonio Candelieri", "Andrea Ponti", "Francesco Archetti"], "title": "Wasserstein Barycenter Gaussian Process based Bayesian Optimization", "categories": ["stat.ML", "cs.LG", "math.OC"], "comment": null, "summary": "Gaussian Process based Bayesian Optimization is a widely applied algorithm to\nlearn and optimize under uncertainty, well-known for its sample efficiency.\nHowever, recently -- and more frequently -- research studies have empirically\ndemonstrated that the Gaussian Process fitting procedure at its core could be\nits most relevant weakness. Fitting a Gaussian Process means tuning its\nkernel's hyperparameters to a set of observations, but the common Maximum\nLikelihood Estimation technique, usually appropriate for learning tasks, has\nshown different criticalities in Bayesian Optimization, making theoretical\nanalysis of this algorithm an open challenge. Exploiting the analogy between\nGaussian Processes and Gaussian Distributions, we present a new approach which\nuses a prefixed set of hyperparameters values to fit as many Gaussian Processes\nand then combines them into a unique model as a Wasserstein Barycenter of\nGaussian Processes. We considered both \"easy\" test problems and others known to\nundermine the \\textit{vanilla} Bayesian Optimization algorithm. The new method,\nnamely Wasserstein Barycenter Gausssian Process based Bayesian Optimization\n(WBGP-BO), resulted promising and able to converge to the optimum, contrary to\nvanilla Bayesian Optimization, also on the most \"tricky\" test problems."}
{"id": "2505.12929", "pdf": "https://arxiv.org/pdf/2505.12929", "abs": "https://arxiv.org/abs/2505.12929", "authors": ["Zhihe Yang", "Xufang Luo", "Zilong Wang", "Dongqi Han", "Zhiyuan He", "Dongsheng Li", "Yunjian Xu"], "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 12 figures", "summary": "Reinforcement learning (RL) has become a cornerstone for enhancing the\nreasoning capabilities of large language models (LLMs), with recent innovations\nsuch as Group Relative Policy Optimization (GRPO) demonstrating exceptional\neffectiveness. In this study, we identify a critical yet underexplored issue in\nRL training: low-probability tokens disproportionately influence model updates\ndue to their large gradient magnitudes. This dominance hinders the effective\nlearning of high-probability tokens, whose gradients are essential for LLMs'\nperformance but are substantially suppressed. To mitigate this interference, we\npropose two novel methods: Advantage Reweighting and Low-Probability Token\nIsolation (Lopti), both of which effectively attenuate gradients from\nlow-probability tokens while emphasizing parameter updates driven by\nhigh-probability tokens. Our approaches promote balanced updates across tokens\nwith varying probabilities, thereby enhancing the efficiency of RL training.\nExperimental results demonstrate that they substantially improve the\nperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K\nLogic Puzzle reasoning tasks. Our implementation is available at\nhttps://github.com/zhyang2226/AR-Lopti."}
{"id": "2505.12473", "pdf": "https://arxiv.org/pdf/2505.12473", "abs": "https://arxiv.org/abs/2505.12473", "authors": ["Yu Gui", "Cong Ma", "Zongming Ma"], "title": "Multi-modal contrastive learning adapts to intrinsic dimensions of shared latent variables", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Multi-modal contrastive learning as a self-supervised representation learning\ntechnique has achieved great success in foundation model training, such as\nCLIP~\\citep{radford2021learning}. In this paper, we study the theoretical\nproperties of the learned representations from multi-modal contrastive learning\nbeyond linear representations and specific data distributions. Our analysis\nreveals that, enabled by temperature optimization, multi-modal contrastive\nlearning not only maximizes mutual information between modalities but also\nadapts to intrinsic dimensions of data, which can be much lower than\nuser-specified dimensions for representation vectors. Experiments on both\nsynthetic and real-world datasets demonstrate the ability of contrastive\nlearning to learn low-dimensional and informative representations, bridging\ntheoretical insights and practical performance."}
{"id": "2505.12938", "pdf": "https://arxiv.org/pdf/2505.12938", "abs": "https://arxiv.org/abs/2505.12938", "authors": ["Uri Dalal", "Meirav Segal", "Zvika Ben-Haim", "Dan Lahav", "Omer Nevo"], "title": "Leveraging LLM Inconsistency to Boost Pass@k Performance", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations."}
{"id": "2505.12492", "pdf": "https://arxiv.org/pdf/2505.12492", "abs": "https://arxiv.org/abs/2505.12492", "authors": ["Amit Cohen", "Lev Gloukhenki", "Ravid Hadar", "Eden Itah", "Yehuda Shvut", "Michael Schapira"], "title": "Unleashing Automated Congestion Control Customization in the Wild", "categories": ["cs.NI", "cs.AI", "cs.LG", "cs.PF", "cs.SY", "eess.SY"], "comment": null, "summary": "Congestion control (CC) crucially impacts user experience across Internet\nservices like streaming, gaming, AR/VR, and connected cars. Traditionally, CC\nalgorithm design seeks universal control rules that yield high performance\nacross diverse application domains and networks. However, varying service needs\nand network conditions challenge this approach. We share operational experience\nwith a system that automatically customizes congestion control logic to service\nneeds and network conditions. We discuss design, deployment challenges, and\nsolutions, highlighting performance benefits through case studies in streaming,\ngaming, connected cars, and more.\n  Our system leverages PCC Vivace, an online-learning based congestion control\nprotocol developed by researchers. Hence, along with insights from customizing\ncongestion control, we also discuss lessons learned and modifications made to\nadapt PCC Vivace for real-world deployment."}
{"id": "2505.12942", "pdf": "https://arxiv.org/pdf/2505.12942", "abs": "https://arxiv.org/abs/2505.12942", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Xinye Cao", "Pedro Gimenes", "George A. Constantinides", "Wayne Luk", "Yiren Zhao"], "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."}
{"id": "2505.12519", "pdf": "https://arxiv.org/pdf/2505.12519", "abs": "https://arxiv.org/abs/2505.12519", "authors": ["Rohit Goswami", "Maxim Masterov", "Satish Kamath", "Alejandro Peña-Torres", "Hannes Jónsson"], "title": "Efficient Implementation of Gaussian Process Regression Accelerated Saddle Point Searches with Application to Molecular Reactions", "categories": ["physics.chem-ph", "cs.LG", "physics.comp-ph"], "comment": "13 pages, 4 figures", "summary": "The task of locating first order saddle points on high-dimensional surfaces\ndescribing the variation of energy as a function of atomic coordinates is an\nessential step for identifying the mechanism and estimating the rate of\nthermally activated events within the harmonic approximation of transition\nstate theory. When combined directly with electronic structure calculations,\nthe number of energy and atomic force evaluations needed for convergence is a\nprimary issue. Here, we describe an efficient implementation of Gaussian\nprocess regression (GPR) acceleration of the minimum mode following method\nwhere a dimer is used to estimate the lowest eigenmode of the Hessian. A\nsurrogate energy surface is constructed and updated after each electronic\nstructure calculation. The method is applied to a test set of 500 molecular\nreactions previously generated by Hermez and coworkers [J. Chem. Theory Comput.\n18, 6974 (2022)]. An order of magnitude reduction in the number of electronic\nstructure calculations needed to reach the saddle point configurations is\nobtained by using the GPR compared to the dimer method. Despite the wide range\nin stiffness of the molecular degrees of freedom, the calculations are carried\nout using Cartesian coordinates and are found to require similar number of\nelectronic structure calculations as an elaborate internal coordinate method\nimplemented in the Sella software package. The present implementation of the\nGPR surrogate model in C++ is efficient enough for the wall time of the saddle\npoint searches to be reduced in 3 out of 4 cases even though the calculations\nare carried out at a low Hartree-Fock level."}
{"id": "2505.12944", "pdf": "https://arxiv.org/pdf/2505.12944", "abs": "https://arxiv.org/abs/2505.12944", "authors": ["Jan Hagnberger", "Daniel Musekamp", "Mathias Niepert"], "title": "CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "physics.comp-ph"], "comment": null, "summary": "Solving time-dependent Partial Differential Equations (PDEs) using a densely\ndiscretized spatial domain is a fundamental problem in various scientific and\nengineering disciplines, including modeling climate phenomena and fluid\ndynamics. However, performing these computations directly in the physical space\noften incurs significant computational costs. To address this issue, several\nneural surrogate models have been developed that operate in a compressed latent\nspace to solve the PDE. While these approaches reduce computational complexity,\nthey often use Transformer-based attention mechanisms to handle irregularly\nsampled domains, resulting in increased memory consumption. In contrast,\nconvolutional neural networks allow memory-efficient encoding and decoding but\nare limited to regular discretizations. Motivated by these considerations, we\npropose CALM-PDE, a model class that efficiently solves arbitrarily discretized\nPDEs in a compressed latent space. We introduce a novel continuous\nconvolution-based encoder-decoder architecture that uses an\nepsilon-neighborhood-constrained kernel and learns to apply the convolution\noperator to adaptive and optimized query points. We demonstrate the\neffectiveness of CALM-PDE on a diverse set of PDEs with both regularly and\nirregularly sampled spatial domains. CALM-PDE is competitive with or\noutperforms existing baseline methods while offering significant improvements\nin memory and inference time efficiency compared to Transformer-based methods."}
{"id": "2505.12524", "pdf": "https://arxiv.org/pdf/2505.12524", "abs": "https://arxiv.org/abs/2505.12524", "authors": ["Guoyu Hu", "Shaofeng Cai", "Tien Tuan Anh Dinh", "Zhongle Xie", "Cong Yue", "Gang Chen", "Beng Chin Ooi"], "title": "HAKES: Scalable Vector Database for Embedding Search Service", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Modern deep learning models capture the semantics of complex data by\ntransforming them into high-dimensional embedding vectors. Emerging\napplications, such as retrieval-augmented generation, use approximate nearest\nneighbor (ANN) search in the embedding vector space to find similar data.\nExisting vector databases provide indexes for efficient ANN searches, with\ngraph-based indexes being the most popular due to their low latency and high\nrecall in real-world high-dimensional datasets. However, these indexes are\ncostly to build, suffer from significant contention under concurrent read-write\nworkloads, and scale poorly to multiple servers.\n  Our goal is to build a vector database that achieves high throughput and high\nrecall under concurrent read-write workloads. To this end, we first propose an\nANN index with an explicit two-stage design combining a fast filter stage with\nhighly compressed vectors and a refine stage to ensure recall, and we devise a\nnovel lightweight machine learning technique to fine-tune the index parameters.\nWe introduce an early termination check to dynamically adapt the search process\nfor each query. Next, we add support for writes while maintaining search\nperformance by decoupling the management of the learned parameters. Finally, we\ndesign HAKES, a distributed vector database that serves the new index in a\ndisaggregated architecture. We evaluate our index and system against 12\nstate-of-the-art indexes and three distributed vector databases, using\nhigh-dimensional embedding datasets generated by deep learning models. The\nexperimental results show that our index outperforms index baselines in the\nhigh recall region and under concurrent read-write workloads. Furthermore,\n\\namesys{} is scalable and achieves up to $16\\times$ higher throughputs than\nthe baselines. The HAKES project is open-sourced at\nhttps://www.comp.nus.edu.sg/~dbsystem/hakes/."}
{"id": "2505.12951", "pdf": "https://arxiv.org/pdf/2505.12951", "abs": "https://arxiv.org/abs/2505.12951", "authors": ["Xuerui Su", "Liya Guo", "Yue Wang", "Yi Zhu", "Zhiming Ma", "Zun Wang", "Yuting Liu"], "title": "DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Inference scaling further accelerates Large Language Models (LLMs) toward\nArtificial General Intelligence (AGI), with large-scale Reinforcement Learning\n(RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning\napproaches usually rely on handcrafted rule-based reward functions. However,\nthe tarde-offs of exploration and exploitation in RL algorithms involves\nmultiple complex considerations, and the theoretical and empirical impacts of\nmanually designed reward functions remain insufficiently explored. In this\npaper, we propose Decoupled Group Reward Optimization (DGRO), a general RL\nalgorithm for LLM reasoning. On the one hand, DGRO decouples the traditional\nregularization coefficient into two independent hyperparameters: one scales the\npolicy gradient term, and the other regulates the distance from the sampling\npolicy. This decoupling not only enables precise control over balancing\nexploration and exploitation, but also can be seamlessly extended to Online\nPolicy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward\nOptimization. On the other hand, we observe that reward variance significantly\naffects both convergence speed and final model performance. We conduct both\ntheoretical analysis and extensive empirical validation to assess DGRO,\nincluding a detailed ablation study that investigates its performance and\noptimization dynamics. Experimental results show that DGRO achieves\nstate-of-the-art performance on the Logic dataset with an average accuracy of\n96.9\\%, and demonstrates strong generalization across mathematical benchmarks."}
{"id": "2505.12528", "pdf": "https://arxiv.org/pdf/2505.12528", "abs": "https://arxiv.org/abs/2505.12528", "authors": ["Yuxin Ma", "Dmitriy Kunisky"], "title": "Nonlinear Laplacians: Tunable principal component analysis under directional prior information", "categories": ["stat.ML", "cs.DS", "cs.LG", "math.PR", "math.ST", "stat.TH"], "comment": "54 pages, 5 figures", "summary": "We introduce a new family of algorithms for detecting and estimating a\nrank-one signal from a noisy observation under prior information about that\nsignal's direction, focusing on examples where the signal is known to have\nentries biased to be positive. Given a matrix observation $\\mathbf{Y}$, our\nalgorithms construct a nonlinear Laplacian, another matrix of the form\n$\\mathbf{Y} + \\mathrm{diag}(\\sigma(\\mathbf{Y}\\mathbf{1}))$ for a nonlinear\n$\\sigma: \\mathbb{R} \\to \\mathbb{R}$, and examine the top eigenvalue and\neigenvector of this matrix. When $\\mathbf{Y}$ is the (suitably normalized)\nadjacency matrix of a graph, our approach gives a class of algorithms that\nsearch for unusually dense subgraphs by computing a spectrum of the graph\n\"deformed\" by the degree profile $\\mathbf{Y}\\mathbf{1}$. We study the\nperformance of such algorithms compared to direct spectral algorithms (the case\n$\\sigma = 0$) on models of sparse principal component analysis with biased\nsignals, including the Gaussian planted submatrix problem. For such models, we\nrigorously characterize the critical threshold strength of rank-one signal, as\na function of the nonlinearity $\\sigma$, at which an outlier eigenvalue appears\nin the spectrum of a nonlinear Laplacian. While identifying the $\\sigma$ that\nminimizes this critical signal strength in closed form seems intractable, we\nexplore three approaches to design $\\sigma$ numerically: exhaustively searching\nover simple classes of $\\sigma$, learning $\\sigma$ from datasets of problem\ninstances, and tuning $\\sigma$ using black-box optimization of the critical\nsignal strength. We find both theoretically and empirically that, if $\\sigma$\nis chosen appropriately, then nonlinear Laplacian spectral algorithms\nsubstantially outperform direct spectral algorithms, while avoiding the\ncomplexity of broader classes of algorithms like approximate message passing or\ngeneral first order methods."}
{"id": "2505.12960", "pdf": "https://arxiv.org/pdf/2505.12960", "abs": "https://arxiv.org/abs/2505.12960", "authors": ["Chengping He", "Mingrui Jiang", "Keyi Shan", "Szu-Hao Yang", "Zefan Li", "Shengbo Wang", "Giacomo Pedretti", "Jim Ignowski", "Can Li"], "title": "Hardware-Adaptive and Superlinear-Capacity Memristor-based Associative Memory", "categories": ["cs.LG", "cs.AI", "cs.ET"], "comment": null, "summary": "Brain-inspired computing aims to mimic cognitive functions like associative\nmemory, the ability to recall complete patterns from partial cues. Memristor\ntechnology offers promising hardware for such neuromorphic systems due to its\npotential for efficient in-memory analog computing. Hopfield Neural Networks\n(HNNs) are a classic model for associative memory, but implementations on\nconventional hardware suffer from efficiency bottlenecks, while prior\nmemristor-based HNNs faced challenges with vulnerability to hardware defects\ndue to offline training, limited storage capacity, and difficulty processing\nanalog patterns. Here we introduce and experimentally demonstrate on integrated\nmemristor hardware a new hardware-adaptive learning algorithm for associative\nmemories that significantly improves defect tolerance and capacity, and\nnaturally extends to scalable multilayer architectures capable of handling both\nbinary and continuous patterns. Our approach achieves 3x effective capacity\nunder 50% device faults compared to state-of-the-art methods. Furthermore, its\nextension to multilayer architectures enables superlinear capacity scaling\n(\\(\\propto N^{1.49}\\ for binary patterns) and effective recalling of continuous\npatterns (\\propto N^{1.74}\\ scaling), as compared to linear capacity scaling\nfor previous HNNs. It also provides flexibility to adjust capacity by tuning\nhidden neurons for the same-sized patterns. By leveraging the massive\nparallelism of the hardware enabled by synchronous updates, it reduces energy\nby 8.8x and latency by 99.7% for 64-dimensional patterns over asynchronous\nschemes, with greater improvements at scale. This promises the development of\nmore reliable memristor-based associative memory systems and enables new\napplications research due to the significantly improved capacity, efficiency,\nand flexibility."}
{"id": "2505.12532", "pdf": "https://arxiv.org/pdf/2505.12532", "abs": "https://arxiv.org/abs/2505.12532", "authors": ["Ahmet Bilican", "M. Akın Yılmaz", "A. Murat Tekalp", "R. Gökberk Cinbiş"], "title": "Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "eess.SP"], "comment": null, "summary": "Efficiently adapting large foundation models is critical, especially with\ntight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT)\nmethods such as LoRA offer limited granularity and effectiveness in\nfew-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT\nmethod that learns highly sparse updates in the wavelet domain of residual\nmatrices. WaveFT allows precise control of trainable parameters, offering\nfine-grained capacity adjustment and excelling with remarkably low parameter\ncount, potentially far fewer than LoRA's minimum -- ideal for extreme\nparameter-efficient scenarios. In order to demonstrate the effect of the\nwavelet transform, we compare WaveFT with a special case, called SHiRA, that\nentails applying sparse updates directly in the weight domain. Evaluated on\npersonalized text-to-image generation using Stable Diffusion XL as baseline,\nWaveFT significantly outperforms LoRA and other PEFT methods, especially at low\nparameter counts; achieving superior subject fidelity, prompt alignment, and\nimage diversity."}
{"id": "2505.12963", "pdf": "https://arxiv.org/pdf/2505.12963", "abs": "https://arxiv.org/abs/2505.12963", "authors": ["Maksim I. Ivanov", "Olga E. Mendybaeva", "Yuri E. Karyakin", "Igor N. Glukhikh", "Aleksey V. Lebedev"], "title": "Segmentation of temporomandibular joint structures on mri images using neural networks for diagnosis of pathologies", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 10 figures", "summary": "This article explores the use of artificial intelligence for the diagnosis of\npathologies of the temporomandibular joint (TMJ), in particular, for the\nsegmentation of the articular disc on MRI images. The relevance of the work is\ndue to the high prevalence of TMJ pathologies, as well as the need to improve\nthe accuracy and speed of diagnosis in medical institutions. During the study,\nthe existing solutions (Diagnocat, MandSeg) were analyzed, which, as a result,\nare not suitable for studying the articular disc due to the orientation towards\nbone structures. To solve the problem, an original dataset was collected from\n94 images with the classes \"temporomandibular joint\" and \"jaw\". To increase the\namount of data, augmentation methods were used. After that, the models of\nU-Net, YOLOv8n, YOLOv11n and Roboflow neural networks were trained and\ncompared. The evaluation was carried out according to the Dice Score,\nPrecision, Sensitivity, Specificity, and Mean Average Precision metrics. The\nresults confirm the potential of using the Roboflow model for segmentation of\nthe temporomandibular joint. In the future, it is planned to develop an\nalgorithm for measuring the distance between the jaws and determining the\nposition of the articular disc, which will improve the diagnosis of TMJ\npathologies."}
{"id": "2505.12535", "pdf": "https://arxiv.org/pdf/2505.12535", "abs": "https://arxiv.org/abs/2505.12535", "authors": ["Zahi Mizrahi", "Shai Berkovitz", "Nimrod Talmon", "Michael Fire"], "title": "Framework of Voting Prediction of Parliament Members", "categories": ["cs.SI", "cs.LG"], "comment": null, "summary": "Keeping track of how lawmakers vote is essential for government transparency.\nWhile many parliamentary voting records are available online, they are often\ndifficult to interpret, making it challenging to understand legislative\nbehavior across parliaments and predict voting outcomes. Accurate prediction of\nvotes has several potential benefits, from simplifying parliamentary work by\nfiltering out bills with a low chance of passing to refining proposed\nlegislation to increase its likelihood of approval. In this study, we leverage\nadvanced machine learning and data analysis techniques to develop a\ncomprehensive framework for predicting parliamentary voting outcomes across\nmultiple legislatures. We introduce the Voting Prediction Framework (VPF) - a\ndata-driven framework designed to forecast parliamentary voting outcomes at the\nindividual legislator level and for entire bills. VPF consists of three key\ncomponents: (1) Data Collection - gathering parliamentary voting records from\nmultiple countries using APIs, web crawlers, and structured databases; (2)\nParsing and Feature Integration - processing and enriching the data with\nmeaningful features, such as legislator seniority, and content-based\ncharacteristics of a given bill; and (3) Prediction Models - using machine\nlearning to forecast how each parliament member will vote and whether a bill is\nlikely to pass. The framework will be open source, enabling anyone to use or\nmodify the framework. To evaluate VPF, we analyzed over 5 million voting\nrecords from five countries - Canada, Israel, Tunisia, the United Kingdom and\nthe USA. Our results show that VPF achieves up to 85% precision in predicting\nindividual votes and up to 84% accuracy in predicting overall bill outcomes.\nThese findings highlight VPF's potential as a valuable tool for political\nanalysis, policy research, and enhancing public access to legislative\ndecision-making."}
{"id": "2505.12966", "pdf": "https://arxiv.org/pdf/2505.12966", "abs": "https://arxiv.org/abs/2505.12966", "authors": ["Zihan Xiong", "Xiaohua Wu", "Lei Chen", "Fangqi Lou"], "title": "Multiscale Adaptive Conflict-Balancing Model For Multimedia Deepfake Detection", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages,ICMR accepted", "summary": "Advances in computer vision and deep learning have blurred the line between\ndeepfakes and authentic media, undermining multimedia credibility through\naudio-visual forgery. Current multimodal detection methods remain limited by\nunbalanced learning between modalities. To tackle this issue, we propose an\nAudio-Visual Joint Learning Method (MACB-DF) to better mitigate modality\nconflicts and neglect by leveraging contrastive learning to assist in\nmulti-level and cross-modal fusion, thereby fully balancing and exploiting\ninformation from each modality. Additionally, we designed an\northogonalization-multimodal pareto module that preserves unimodal information\nwhile addressing gradient conflicts in audio-video encoders caused by differing\noptimization targets of the loss functions. Extensive experiments and ablation\nstudies conducted on mainstream deepfake datasets demonstrate consistent\nperformance gains of our model across key evaluation metrics, achieving an\naverage accuracy of 95.5% across multiple datasets. Notably, our method\nexhibits superior cross-dataset generalization capabilities, with absolute\nimprovements of 8.0% and 7.7% in ACC scores over the previous best-performing\napproach when trained on DFDC and tested on DefakeAVMiT and FakeAVCeleb\ndatasets."}
{"id": "2505.12546", "pdf": "https://arxiv.org/pdf/2505.12546", "abs": "https://arxiv.org/abs/2505.12546", "authors": ["A. Feder Cooper", "Aaron Gokaslan", "Amy B. Cyphert", "Christopher De Sa", "Mark A. Lemley", "Daniel E. Ho", "Percy Liang"], "title": "Extracting memorized pieces of (copyrighted) books from open-weight language models", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Plaintiffs and defendants in copyright lawsuits over generative AI often make\nsweeping, opposing claims about the extent to which large language models\n(LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial\nML and copyright law, we show that these polarized positions dramatically\noversimplify the relationship between memorization and copyright. To do so, we\nleverage a recent probabilistic extraction technique to extract pieces of the\nBooks3 dataset from 13 open-weight LLMs. Through numerous experiments, we show\nthat it's possible to extract substantial parts of at least some books from\ndifferent LLMs. This is evidence that the LLMs have memorized the extracted\ntext; this memorized content is copied inside the model parameters. But the\nresults are complicated: the extent of memorization varies both by model and by\nbook. With our specific experiments, we find that the largest LLMs don't\nmemorize most books -- either in whole or in part. However, we also find that\nLlama 3.1 70B memorizes some books, like Harry Potter and 1984, almost\nentirely. We discuss why our results have significant implications for\ncopyright cases, though not ones that unambiguously favor either side."}
{"id": "2505.12981", "pdf": "https://arxiv.org/pdf/2505.12981", "abs": "https://arxiv.org/abs/2505.12981", "authors": ["Liangxuan Wu", "Chao Wang", "Tianming Liu", "Yanjie Zhao", "Haoyu Wang"], "title": "From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents", "categories": ["cs.CR", "cs.AI", "cs.HC"], "comment": null, "summary": "The growing adoption of large language models (LLMs) has led to a new\nparadigm in mobile computing--LLM-powered mobile AI agents--capable of\ndecomposing and automating complex tasks directly on smartphones. However, the\nsecurity implications of these agents remain largely unexplored. In this paper,\nwe present the first comprehensive security analysis of mobile LLM agents,\nencompassing three representative categories: System-level AI Agents developed\nby original equipment manufacturers (e.g., YOYO Assistant), Third-party\nUniversal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g.,\nAlibaba Mobile Agent). We begin by analyzing the general workflow of mobile\nagents and identifying security threats across three core capability\ndimensions: language-based reasoning, GUI-based interaction, and system-level\nexecution. Our analysis reveals 11 distinct attack surfaces, all rooted in the\nunique capabilities and interaction patterns of mobile LLM agents, and spanning\ntheir entire operational lifecycle. To investigate these threats in practice,\nwe introduce AgentScan, a semi-automated security analysis framework that\nsystematically evaluates mobile LLM agents across all 11 attack scenarios.\nApplying AgentScan to nine widely deployed agents, we uncover a concerning\ntrend: every agent is vulnerable to targeted attacks. In the most severe cases,\nagents exhibit vulnerabilities across eight distinct attack vectors. These\nattacks can cause behavioral deviations, privacy leakage, or even full\nexecution hijacking. Based on these findings, we propose a set of defensive\ndesign principles and practical recommendations for building secure mobile LLM\nagents. Our disclosures have received positive feedback from two major device\nvendors. Overall, this work highlights the urgent need for standardized\nsecurity practices in the fast-evolving landscape of LLM-driven mobile\nautomation."}
{"id": "2505.12547", "pdf": "https://arxiv.org/pdf/2505.12547", "abs": "https://arxiv.org/abs/2505.12547", "authors": ["Florent Chiaroni", "Ali Ayub", "Ola Ahmad"], "title": "ProMi: An Efficient Prototype-Mixture Baseline for Few-Shot Segmentation with Bounding-Box Annotations", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "In robotics applications, few-shot segmentation is crucial because it allows\nrobots to perform complex tasks with minimal training data, facilitating their\nadaptation to diverse, real-world environments. However, pixel-level\nannotations of even small amount of images is highly time-consuming and costly.\nIn this paper, we present a novel few-shot binary segmentation method based on\nbounding-box annotations instead of pixel-level labels. We introduce, ProMi, an\nefficient prototype-mixture-based method that treats the background class as a\nmixture of distributions. Our approach is simple, training-free, and effective,\naccommodating coarse annotations with ease. Compared to existing baselines,\nProMi achieves the best results across different datasets with significant\ngains, demonstrating its effectiveness. Furthermore, we present qualitative\nexperiments tailored to real-world mobile robot tasks, demonstrating the\napplicability of our approach in such scenarios. Our code:\nhttps://github.com/ThalesGroup/promi."}
{"id": "2505.12983", "pdf": "https://arxiv.org/pdf/2505.12983", "abs": "https://arxiv.org/abs/2505.12983", "authors": ["Jiaan Wang", "Fandong Meng", "Zengkui Sun", "Yunlong Liang", "Yuxuan Cao", "Jiarong Xu", "Haoxiang Shi", "Jie Zhou"], "title": "An Empirical Study of Many-to-Many Summarization with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 main conference", "summary": "Many-to-many summarization (M2MS) aims to process documents in any language\nand generate the corresponding summaries also in any language. Recently, large\nlanguage models (LLMs) have shown strong multi-lingual abilities, giving them\nthe potential to perform M2MS in real applications. This work presents a\nsystematic empirical study on LLMs' M2MS ability. Specifically, we first\nreorganize M2MS data based on eight previous domain-specific datasets. The\nreorganized data contains 47.8K samples spanning five domains and six\nlanguages, which could be used to train and evaluate LLMs. Then, we benchmark\n18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned\ntraditional models (e.g., mBART) are also conducted for comparisons. Our\nexperiments reveal that, zero-shot LLMs achieve competitive results with\nfine-tuned traditional models. After instruct-tuning, open-source LLMs can\nsignificantly improve their M2MS ability, and outperform zero-shot LLMs\n(including GPT-4) in terms of automatic evaluations. In addition, we\ndemonstrate that this task-specific improvement does not sacrifice the LLMs'\ngeneral task-solving abilities. However, as revealed by our human evaluation,\nLLMs still face the factuality issue, and the instruction tuning might\nintensify the issue. Thus, how to control factual errors becomes the key when\nbuilding LLM summarizers in real applications, and is worth noting in future\nresearch."}
{"id": "2505.12552", "pdf": "https://arxiv.org/pdf/2505.12552", "abs": "https://arxiv.org/abs/2505.12552", "authors": ["Junliang Ye", "Lei Wang", "Md Zakir Hossain"], "title": "FreqSelect: Frequency-Aware fMRI-to-Image Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "Research report", "summary": "Reconstructing natural images from functional magnetic resonance imaging\n(fMRI) data remains a core challenge in natural decoding due to the mismatch\nbetween the richness of visual stimuli and the noisy, low resolution nature of\nfMRI signals. While recent two-stage models, combining deep variational\nautoencoders (VAEs) with diffusion models, have advanced this task, they treat\nall spatial-frequency components of the input equally. This uniform treatment\nforces the model to extract meaning features and suppress irrelevant noise\nsimultaneously, limiting its effectiveness. We introduce FreqSelect, a\nlightweight, adaptive module that selectively filters spatial-frequency bands\nbefore encoding. By dynamically emphasizing frequencies that are most\npredictive of brain activity and suppressing those that are uninformative,\nFreqSelect acts as a content-aware gate between image features and natural\ndata. It integrates seamlessly into standard very deep VAE-diffusion pipelines\nand requires no additional supervision. Evaluated on the Natural Scenes\ndataset, FreqSelect consistently improves reconstruction quality across both\nlow- and high-level metrics. Beyond performance gains, the learned\nfrequency-selection patterns offer interpretable insights into how different\nvisual frequencies are represented in the brain. Our method generalizes across\nsubjects and scenes, and holds promise for extension to other neuroimaging\nmodalities, offering a principled approach to enhancing both decoding accuracy\nand neuroscientific interpretability."}
{"id": "2505.12992", "pdf": "https://arxiv.org/pdf/2505.12992", "abs": "https://arxiv.org/abs/2505.12992", "authors": ["Baohao Liao", "Hanze Dong", "Yuhui Xu", "Doyen Sahoo", "Christof Monz", "Junnan Li", "Caiming Xiong"], "title": "Fractured Chain-of-Thought Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning."}
{"id": "2505.12553", "pdf": "https://arxiv.org/pdf/2505.12553", "abs": "https://arxiv.org/abs/2505.12553", "authors": ["Qiang Fu", "Andre Wibisono"], "title": "Hamiltonian Descent Algorithms for Optimization: Accelerated Rates via Randomized Integration Time", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": null, "summary": "We study the Hamiltonian flow for optimization (HF-opt), which simulates the\nHamiltonian dynamics for some integration time and resets the velocity to $0$\nto decrease the objective function; this is the optimization analogue of the\nHamiltonian Monte Carlo algorithm for sampling. For short integration time,\nHF-opt has the same convergence rates as gradient descent for minimizing\nstrongly and weakly convex functions. We show that by randomizing the\nintegration time in HF-opt, the resulting randomized Hamiltonian flow (RHF)\nachieves accelerated convergence rates in continuous time, similar to the rates\nfor the accelerated gradient flow. We study a discrete-time implementation of\nRHF as the randomized Hamiltonian gradient descent (RHGD) algorithm. We prove\nthat RHGD achieves the same accelerated convergence rates as Nesterov's\naccelerated gradient descent (AGD) for minimizing smooth strongly and weakly\nconvex functions. We provide numerical experiments to demonstrate that RHGD is\ncompetitive with classical accelerated methods such as AGD across all settings\nand outperforms them in certain regimes."}
{"id": "2505.12996", "pdf": "https://arxiv.org/pdf/2505.12996", "abs": "https://arxiv.org/abs/2505.12996", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 2 figures", "summary": "In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance."}
{"id": "2505.12565", "pdf": "https://arxiv.org/pdf/2505.12565", "abs": "https://arxiv.org/abs/2505.12565", "authors": ["Carl Edwards", "Chi Han", "Gawon Lee", "Thao Nguyen", "Bowen Jin", "Chetan Kumar Prasad", "Sara Szymkuć", "Bartosz A. Grzybowski", "Ying Diao", "Jiawei Han", "Ge Liu", "Hao Peng", "Martin D. Burke", "Heng Ji"], "title": "mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model", "categories": ["cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Despite their ability to understand chemical knowledge and accurately\ngenerate sequential representations, large language models (LLMs) remain\nlimited in their capacity to propose novel molecules with drug-like properties.\nIn addition, the molecules that LLMs propose can often be challenging to make\nin the lab. To more effectively enable the discovery of functional small\nmolecules, LLMs need to learn a molecular language. However, LLMs are currently\nlimited by encoding molecules from atoms. In this paper, we argue that just\nlike tokenizing texts into (sub-)word tokens instead of characters, molecules\nshould be decomposed and reassembled at the level of functional building\nblocks, i.e., parts of molecules that bring unique functions and serve as\neffective building blocks for real-world automated laboratory synthesis. This\nmotivates us to propose mCLM, a modular Chemical-Language Model tokenizing\nmolecules into building blocks and learning a bilingual language model of both\nnatural language descriptions of functions and molecule building blocks. By\nreasoning on such functional building blocks, mCLM guarantees to generate\nefficiently synthesizable molecules thanks to recent progress in block-based\nchemistry, while also improving the functions of molecules in a principled\nmanner. In experiments on 430 FDA-approved drugs, we find mCLM capable of\nsignificantly improving 5 out of 6 chemical functions critical to determining\ndrug potentials. More importantly, mCLM can reason on multiple functions and\nimprove the FDA-rejected drugs (``fallen angels'') over multiple iterations to\ngreatly improve their shortcomings."}
{"id": "2505.13010", "pdf": "https://arxiv.org/pdf/2505.13010", "abs": "https://arxiv.org/abs/2505.13010", "authors": ["Himel Ghosh", "Ahmed Mosharafa", "Georg Groh"], "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "7 pages, 5 figures, 2 tables", "summary": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection."}
{"id": "2505.12578", "pdf": "https://arxiv.org/pdf/2505.12578", "abs": "https://arxiv.org/abs/2505.12578", "authors": ["Paulo C. Marques F"], "title": "Stacked conformal prediction", "categories": ["stat.ML", "cs.LG"], "comment": "7 pages, 2 figures", "summary": "We consider the conformalization of a stacked ensemble of predictive models,\nshowing that the potentially simple form of the meta-learner at the top of the\nstack enables a procedure with manageable computational cost that achieves\napproximate marginal validity without requiring the use of a separate\ncalibration sample. Empirical results indicate that the method compares\nfavorably to a standard inductive alternative."}
{"id": "2505.13023", "pdf": "https://arxiv.org/pdf/2505.13023", "abs": "https://arxiv.org/abs/2505.13023", "authors": ["Yimao Guo", "Zuomin Qu", "Wei Lu", "Xiangyang Luo"], "title": "Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "As diffusion-based malicious image manipulation becomes increasingly\nprevalent, multiple proactive defense methods are developed to safeguard images\nagainst unauthorized tampering. However, most proactive defense methods only\ncan safeguard images against manipulation under known conditions, and fail to\nprotect images from manipulations guided by tampering conditions crafted by\nmalicious users. To tackle this issue, we propose Anti-Inpainting, a proactive\ndefense method that achieves adequate protection under unknown conditions\nthrough a triple mechanism to address this challenge. Specifically, a\nmulti-level deep feature extractor is presented to obtain intricate features\nduring the diffusion denoising process to improve protective effectiveness. We\ndesign multi-scale semantic-preserving data augmentation to enhance the\ntransferability of adversarial perturbations across unknown conditions by\nmulti-scale transformations while preserving semantic integrity. In addition,\nwe propose a selection-based distribution deviation optimization strategy to\nimprove the protection of adversarial perturbation against manipulation under\ndiverse random seeds. Extensive experiments indicate the proactive defensive\nperformance of Anti-Inpainting against diffusion-based inpainters guided by\nunknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we\nalso demonstrate the proposed approach's robustness under various image\npurification methods and its transferability across different versions of\ndiffusion models."}
{"id": "2505.12583", "pdf": "https://arxiv.org/pdf/2505.12583", "abs": "https://arxiv.org/abs/2505.12583", "authors": ["Takeshi Kojima", "Yaonan Zhu", "Yusuke Iwasawa", "Toshinori Kitamura", "Gang Yan", "Shu Morikuni", "Ryosuke Takanami", "Alfredo Solano", "Tatsuya Matsushima", "Akiko Murakami", "Yutaka Matsuo"], "title": "A Comprehensive Survey on Physical Risk Control in the Era of Foundation Model-enabled Robotics", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted to IJCAI 2025 Survey Track", "summary": "Recent Foundation Model-enabled robotics (FMRs) display greatly improved\ngeneral-purpose skills, enabling more adaptable automation than conventional\nrobotics. Their ability to handle diverse tasks thus creates new opportunities\nto replace human labor. However, unlike general foundation models, FMRs\ninteract with the physical world, where their actions directly affect the\nsafety of humans and surrounding objects, requiring careful deployment and\ncontrol. Based on this proposition, our survey comprehensively summarizes robot\ncontrol approaches to mitigate physical risks by covering all the lifespan of\nFMRs ranging from pre-deployment to post-accident stage. Specifically, we\nbroadly divide the timeline into the following three phases: (1) pre-deployment\nphase, (2) pre-incident phase, and (3) post-incident phase. Throughout this\nsurvey, we find that there is much room to study (i) pre-incident risk\nmitigation strategies, (ii) research that assumes physical interaction with\nhumans, and (iii) essential issues of foundation models themselves. We hope\nthat this survey will be a milestone in providing a high-resolution analysis of\nthe physical risks of FMRs and their control, contributing to the realization\nof a good human-robot relationship."}
{"id": "2505.13025", "pdf": "https://arxiv.org/pdf/2505.13025", "abs": "https://arxiv.org/abs/2505.13025", "authors": ["Jiyuan Pei", "Yi Mei", "Jialin Liu", "Mengjie Zhang"], "title": "LiBOG: Lifelong Learning for Black-Box Optimizer Generation", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at IJCAI 2025. To appear", "summary": "Meta-Black-Box Optimization (MetaBBO) garners attention due to its success in\nautomating the configuration and generation of black-box optimizers,\nsignificantly reducing the human effort required for optimizer design and\ndiscovering optimizers with higher performance than classic human-designed\noptimizers. However, existing MetaBBO methods conduct one-off training under\nthe assumption that a stationary problem distribution with extensive and\nrepresentative training problem samples is pre-available. This assumption is\noften impractical in real-world scenarios, where diverse problems following\nshifting distribution continually arise. Consequently, there is a pressing need\nfor methods that can continuously learn from new problems encountered\non-the-fly and progressively enhance their capabilities. In this work, we\nexplore a novel paradigm of lifelong learning in MetaBBO and introduce LiBOG, a\nnovel approach designed to learn from sequentially encountered problems and\ngenerate high-performance optimizers for Black-Box Optimization (BBO). LiBOG\nconsolidates knowledge both across tasks and within tasks to mitigate\ncatastrophic forgetting. Extensive experiments demonstrate LiBOG's\neffectiveness in learning to generate high-performance optimizers in a lifelong\nlearning manner, addressing catastrophic forgetting while maintaining\nplasticity to learn new tasks."}
{"id": "2505.12587", "pdf": "https://arxiv.org/pdf/2505.12587", "abs": "https://arxiv.org/abs/2505.12587", "authors": ["Aditeya Baral", "Allen George Ajith", "Roshan Nayak", "Mrityunjay Abhijeet Bhanja"], "title": "CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Code-mixed languages, characterized by frequent within-sentence language\ntransitions, present structural challenges that standard language models fail\nto address. In this work, we propose CMLFormer, an enhanced multi-layer\ndual-decoder Transformer with a shared encoder and synchronized decoder\ncross-attention, designed to model the linguistic and semantic dynamics of\ncode-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with\nswitching point and translation annotations with multiple new objectives\nspecifically aimed at capturing switching behavior, cross-lingual structure,\nand code-mixing complexity. Our experiments show that CMLFormer improves F1\nscore, precision, and accuracy over other approaches on the HASOC-2021\nbenchmark under select pre-training setups. Attention analyses further show\nthat it can identify and attend to switching points, validating its sensitivity\nto code-mixed structure. These results demonstrate the effectiveness of\nCMLFormer's architecture and multi-task pre-training strategy for modeling\ncode-mixed languages."}
{"id": "2505.13026", "pdf": "https://arxiv.org/pdf/2505.13026", "abs": "https://arxiv.org/abs/2505.13026", "authors": ["Jack Chen", "Fazhong Liu", "Naruto Liu", "Yuhan Luo", "Erqu Qin", "Harry Zheng", "Tian Dong", "Haojin Zhu", "Yan Meng", "Xiao Wang"], "title": "Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at mathematical reasoning and logical\nproblem-solving. The current popular training paradigms primarily use\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the\nmodels' reasoning abilities. However, when using SFT or RL alone, there are\nrespective challenges: SFT may suffer from overfitting, while RL is prone to\nmode collapse. The state-of-the-art methods have proposed hybrid training\nschemes. However, static switching faces challenges such as poor generalization\nacross different tasks and high dependence on data quality. In response to\nthese challenges, inspired by the curriculum learning-quiz mechanism in human\nreasoning cultivation, We propose SASR, a step-wise adaptive hybrid training\nframework that theoretically unifies SFT and RL and dynamically balances the\ntwo throughout optimization. SASR uses SFT for initial warm-up to establish\nbasic reasoning skills, and then uses an adaptive dynamic adjustment algorithm\nbased on gradient norm and divergence relative to the original distribution to\nseamlessly integrate SFT with the online RL method GRPO. By monitoring the\ntraining status of LLMs and adjusting the training process in sequence, SASR\nensures a smooth transition between training schemes, maintaining core\nreasoning abilities while exploring different paths. Experimental results\ndemonstrate that SASR outperforms SFT, RL, and static hybrid training methods."}
{"id": "2505.12599", "pdf": "https://arxiv.org/pdf/2505.12599", "abs": "https://arxiv.org/abs/2505.12599", "authors": ["Bohan Zhou", "Shu Liu", "Xinzhe Zuo", "Wuchen Li"], "title": "Accelerated Markov Chain Monte Carlo Algorithms on Discrete States", "categories": ["math.OC", "cs.LG", "stat.CO", "65C05, 60J22, 82M31, 49Q22, 65K10"], "comment": null, "summary": "We propose a class of discrete state sampling algorithms based on Nesterov's\naccelerated gradient method, which extends the classical Metropolis-Hastings\n(MH) algorithm. The evolution of the discrete states probability distribution\ngoverned by MH can be interpreted as a gradient descent direction of the\nKullback--Leibler (KL) divergence, via a mobility function and a score\nfunction. Specifically, this gradient is defined on a probability simplex\nequipped with a discrete Wasserstein-2 metric with a mobility function. This\nmotivates us to study a momentum-based acceleration framework using damped\nHamiltonian flows on the simplex set, whose stationary distribution matches the\ndiscrete target distribution. Furthermore, we design an interacting particle\nsystem to approximate the proposed accelerated sampling dynamics. The extension\nof the algorithm with a general choice of potentials and mobilities is also\ndiscussed. In particular, we choose the accelerated gradient flow of the\nrelative Fisher information, demonstrating the advantages of the algorithm in\nestimating discrete score functions without requiring the normalizing constant\nand keeping positive probabilities. Numerical examples, including sampling on a\nGaussian mixture supported on lattices or a distribution on a hypercube,\ndemonstrate the effectiveness of the proposed discrete-state sampling\nalgorithm."}
{"id": "2505.13028", "pdf": "https://arxiv.org/pdf/2505.13028", "abs": "https://arxiv.org/abs/2505.13028", "authors": ["Sayon Palit", "Daniel Woods"], "title": "Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset", "categories": ["cs.CR", "cs.AI", "cs.CL", "F.2.2, I.2.7; F.2.2, I.2.7; F.2.2, I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into critical\nsystems in industries like healthcare and finance. Users can often submit\nqueries to LLM-enabled chatbots, some of which can enrich responses with\ninformation retrieved from internal databases storing sensitive data. This\ngives rise to a range of attacks in which a user submits a malicious query and\nthe LLM-system outputs a response that creates harm to the owner, such as\nleaking internal data or creating legal liability by harming a third-party.\nWhile security tools are being developed to counter these threats, there is\nlittle formal evaluation of their effectiveness and usability. This study\naddresses this gap by conducting a thorough comparative analysis of LLM\nsecurity tools. We identified 13 solutions (9 closed-source, 4 open-source),\nbut only 7 were evaluated due to a lack of participation by proprietary model\nowners.To evaluate, we built a benchmark dataset of malicious prompts, and\nevaluate these tools performance against a baseline LLM model\n(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many\nfalse positives to be used for this task. Lakera Guard and ProtectAI LLM Guard\nemerged as the best overall tools showcasing the tradeoff between usability and\nperformance. The study concluded with recommendations for greater transparency\namong closed source providers, improved context-aware detections, enhanced\nopen-source engagement, increased user awareness, and the adoption of more\nrepresentative performance metrics."}
{"id": "2505.12600", "pdf": "https://arxiv.org/pdf/2505.12600", "abs": "https://arxiv.org/abs/2505.12600", "authors": ["Thai Bui", "Hoa T. Vu"], "title": "Fast and Simple Densest Subgraph with Predictions", "categories": ["cs.DS", "cs.LG"], "comment": null, "summary": "We study the densest subgraph problem and its variants through the lens of\nlearning-augmented algorithms. For this problem, the greedy algorithm by\nCharikar (APPROX 2000) provides a linear-time $ 1/2 $-approximation, while\ncomputing the exact solution typically requires solving a linear program or\nperforming maximum flow computations.We show that given a partial solution,\ni.e., one produced by a machine learning classifier that captures at least a $\n(1 - \\epsilon) $-fraction of nodes in the optimal subgraph, it is possible to\ndesign an extremely simple linear-time algorithm that achieves a provable $ (1\n- \\epsilon) $-approximation. Our approach also naturally extends to the\ndirected densest subgraph problem and several NP-hard variants.An experiment on\nthe Twitch Ego Nets dataset shows that our learning-augmented algorithm\noutperforms Charikar's greedy algorithm and a baseline that directly returns\nthe predicted densest subgraph without additional algorithmic processing."}
{"id": "2505.13033", "pdf": "https://arxiv.org/pdf/2505.13033", "abs": "https://arxiv.org/abs/2505.13033", "authors": ["Vijay Ekambaram", "Subodh Kumar", "Arindam Jati", "Sumanta Mukherjee", "Tomoya Sakai", "Pankaj Dayama", "Wesley M. Gifford", "Jayant Kalagnanam"], "title": "TSPulse: Dual Space Tiny Pre-Trained Models for Rapid Time-Series Analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rise of time-series pre-trained models has advanced temporal\nrepresentation learning, but current state-of-the-art models are often\nlarge-scale, requiring substantial compute. We introduce TSPulse, ultra-compact\ntime-series pre-trained models with only 1M parameters, specialized to perform\nstrongly across classification, anomaly detection, imputation, and retrieval\ntasks. TSPulse introduces innovations at both the architecture and task levels.\nAt the architecture level, it employs a dual-space masked reconstruction,\nlearning from both time and frequency domains to capture complementary signals.\nThis is further enhanced by a dual-embedding disentanglement, generating both\ndetailed embeddings for fine-grained analysis and high-level semantic\nembeddings for broader task understanding. Notably, TSPulse's semantic\nembeddings are robust to shifts in time, magnitude, and noise, which is\nimportant for robust retrieval. At the task level, TSPulse incorporates TSLens,\na fine-tuning component enabling task-specific feature attention. It also\nintroduces a multi-head triangulation technique that correlates deviations from\nmultiple prediction heads, enhancing anomaly detection by fusing complementary\nmodel outputs. Additionally, a hybrid mask pretraining is proposed to improves\nzero-shot imputation by reducing pre-training bias. These architecture and task\ninnovations collectively contribute to TSPulse's significant performance gains:\n5-16% on the UEA classification benchmarks, +20% on the TSB-AD anomaly\ndetection leaderboard, +50% in zero-shot imputation, and +25% in time-series\nretrieval. Remarkably, these results are achieved with just 1M parameters,\nmaking TSPulse 10-100X smaller than existing pre-trained models. Its efficiency\nenables GPU-free inference and rapid pre-training, setting a new standard for\nefficient time-series pre-trained models. Models will be open-sourced soon."}
{"id": "2505.12609", "pdf": "https://arxiv.org/pdf/2505.12609", "abs": "https://arxiv.org/abs/2505.12609", "authors": ["Toshihiro Ota", "Yuma Fujimoto"], "title": "The Hamiltonian of Poly-matrix Zero-sum Games", "categories": ["cs.GT", "cs.LG", "cs.MA", "nlin.CD"], "comment": "26 pages, 4 figures", "summary": "Understanding a dynamical system fundamentally relies on establishing an\nappropriate Hamiltonian function and elucidating its symmetries. By formulating\nagents' strategies and cumulative payoffs as canonically conjugate variables,\nwe identify the Hamiltonian function that generates the dynamics of poly-matrix\nzero-sum games. We reveal the symmetries of our Hamiltonian and derive the\nassociated conserved quantities, showing how the conservation of probability\nand the invariance of the Fenchel coupling are intrinsically encoded within the\nsystem. Furthermore, we propose the dissipation FTRL (DFTRL) dynamics by\nintroducing a perturbation that dissipates the Fenchel coupling, proving\nconvergence to the Nash equilibrium and linking DFTRL to last-iterate\nconvergent algorithms. Our results highlight the potential of Hamiltonian\ndynamics in uncovering the structural properties of learning dynamics in games,\nand pave the way for broader applications of Hamiltonian dynamics in game\ntheory and machine learning."}
{"id": "2505.13036", "pdf": "https://arxiv.org/pdf/2505.13036", "abs": "https://arxiv.org/abs/2505.13036", "authors": ["Sai Koneru", "Maike Züfle", "Thai-Binh Nguyen", "Seymanur Akti", "Jan Niehues", "Alexander Waibel"], "title": "KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The scope of the International Workshop on Spoken Language Translation\n(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to\nencompass a wider array of tasks, including Speech Question Answering and\nSummarization. This shift is partly driven by the growing capabilities of\nmodern systems, particularly with the success of Large Language Models (LLMs).\nIn this paper, we present the Karlsruhe Institute of Technology's submissions\nfor the Offline ST and Instruction Following (IF) tracks, where we leverage\nLLMs to enhance performance across all tasks. For the Offline ST track, we\npropose a pipeline that employs multiple automatic speech recognition systems,\nwhose outputs are fused using an LLM with document-level context. This is\nfollowed by a two-step translation process, incorporating additional refinement\nstep to improve translation quality. For the IF track, we develop an end-to-end\nmodel that integrates a speech encoder with an LLM to perform a wide range of\ninstruction-following tasks. We complement it with a final document-level\nrefinement stage to further enhance output quality by using contextual\ninformation."}
{"id": "2505.12625", "pdf": "https://arxiv.org/pdf/2505.12625", "abs": "https://arxiv.org/abs/2505.12625", "authors": ["Ali Naseh", "Harsh Chaudhari", "Jaechul Roh", "Mingshi Wu", "Alina Oprea", "Amir Houmansadr"], "title": "R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "DeepSeek recently released R1, a high-performing large language model (LLM)\noptimized for reasoning tasks. Despite its efficient training pipeline, R1\nachieves competitive performance, even surpassing leading reasoning models like\nOpenAI's o1 on several benchmarks. However, emerging reports suggest that R1\nrefuses to answer certain prompts related to politically sensitive topics in\nChina. While existing LLMs often implement safeguards to avoid generating\nharmful or offensive outputs, R1 represents a notable shift - exhibiting\ncensorship-like behavior on politically charged queries. In this paper, we\ninvestigate this phenomenon by first introducing a large-scale set of heavily\ncurated prompts that get censored by R1, covering a range of politically\nsensitive topics, but are not censored by other models. We then conduct a\ncomprehensive analysis of R1's censorship patterns, examining their\nconsistency, triggers, and variations across topics, prompt phrasing, and\ncontext. Beyond English-language queries, we explore censorship behavior in\nother languages. We also investigate the transferability of censorship to\nmodels distilled from the R1 language model. Finally, we propose techniques for\nbypassing or removing this censorship. Our findings reveal possible additional\ncensorship integration likely shaped by design choices during training or\nalignment, raising concerns about transparency, bias, and governance in\nlanguage model deployment."}
{"id": "2505.13043", "pdf": "https://arxiv.org/pdf/2505.13043", "abs": "https://arxiv.org/abs/2505.13043", "authors": ["Hao-Ran Yang", "Xiaohui Chen", "Chuan-Xian Ren"], "title": "A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Aiming to generalize the well-trained gaze estimation model to new target\ndomains, Cross-domain Gaze Estimation (CDGE) is developed for real-world\napplication scenarios. Existing CDGE methods typically extract the\ndomain-invariant features to mitigate domain shift in feature space, which is\nproved insufficient by Generalized Label Shift (GLS) theory. In this paper, we\nintroduce a novel GLS perspective to CDGE and modelize the cross-domain problem\nby label and conditional shift problem. A GLS correction framework is presented\nand a feasible realization is proposed, in which a importance reweighting\nstrategy based on truncated Gaussian distribution is introduced to overcome the\ncontinuity challenges in label shift correction. To embed the reweighted source\ndistribution to conditional invariant learning, we further derive a\nprobability-aware estimation of conditional operator discrepancy. Extensive\nexperiments on standard CDGE tasks with different backbone models validate the\nsuperior generalization capability across domain and applicability on various\nmodels of proposed method."}
{"id": "2505.12626", "pdf": "https://arxiv.org/pdf/2505.12626", "abs": "https://arxiv.org/abs/2505.12626", "authors": ["Ping Xu", "Zhiyuan Ning", "Pengjiang Li", "Wenhao Liu", "Pengyang Wang", "Jiaxu Cui", "Yuanchun Zhou", "Pengfei Wang"], "title": "scSiameseClu: A Siamese Clustering Framework for Interpreting single-cell RNA Sequencing Data", "categories": ["q-bio.GN", "cs.AI", "cs.LG"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) reveals cell heterogeneity, with cell\nclustering playing a key role in identifying cell types and marker genes.\nRecent advances, especially graph neural networks (GNNs)-based methods, have\nsignificantly improved clustering performance. However, the analysis of\nscRNA-seq data remains challenging due to noise, sparsity, and high\ndimensionality. Compounding these challenges, GNNs often suffer from\nover-smoothing, limiting their ability to capture complex biological\ninformation. In response, we propose scSiameseClu, a novel Siamese Clustering\nframework for interpreting single-cell RNA-seq data, comprising of 3 key steps:\n(1) Dual Augmentation Module, which applies biologically informed perturbations\nto the gene expression matrix and cell graph relationships to enhance\nrepresentation robustness; (2) Siamese Fusion Module, which combines\ncross-correlation refinement and adaptive information fusion to capture complex\ncellular relationships while mitigating over-smoothing; and (3) Optimal\nTransport Clustering, which utilizes Sinkhorn distance to efficiently align\ncluster assignments with predefined proportions while maintaining balance.\nComprehensive evaluations on seven real-world datasets demonstrate\nthat~\\methodname~outperforms state-of-the-art methods in single-cell\nclustering, cell type annotation, and cell type classification, providing a\npowerful tool for scRNA-seq data interpretation."}
{"id": "2505.13053", "pdf": "https://arxiv.org/pdf/2505.13053", "abs": "https://arxiv.org/abs/2505.13053", "authors": ["Amelie S. Robrecht", "Christoph R. Kowalski", "Stefan Kopp"], "title": "SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation", "categories": ["cs.CL", "cs.AI"], "comment": "currently under review at Frontiers in Communication", "summary": "Adapting to the addressee is crucial for successful explanations, yet poses\nsignificant challenges for dialogsystems. We adopt the approach of treating\nexplanation generation as a non-stationary decision process, where the optimal\nstrategy varies according to changing beliefs about the explainee and the\ninteraction context. In this paper we address the questions of (1) how to track\nthe interaction context and the relevant listener features in a formally\ndefined computational partner model, and (2) how to utilize this model in the\ndynamically adjusted, rational decision process that determines the currently\nbest explanation strategy. We propose a Bayesian inference-based approach to\ncontinuously update the partner model based on user feedback, and a\nnon-stationary Markov Decision Process to adjust decision-making based on the\npartner model values. We evaluate an implementation of this framework with five\nsimulated interlocutors, demonstrating its effectiveness in adapting to\ndifferent partners with constant and even changing feedback behavior. The\nresults show high adaptivity with distinct explanation strategies emerging for\ndifferent partners, highlighting the potential of our approach to improve\nexplainable AI systems and dialogsystems in general."}
{"id": "2505.12632", "pdf": "https://arxiv.org/pdf/2505.12632", "abs": "https://arxiv.org/abs/2505.12632", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation."}
{"id": "2505.13073", "pdf": "https://arxiv.org/pdf/2505.13073", "abs": "https://arxiv.org/abs/2505.13073", "authors": ["Dengfeng Liu", "Jucai Zhai", "Xiaoguang Jiang", "Ziqun Li", "Qianjin Yu", "Feng Liu", "Rui Ye", "Huang Liu", "Zhiguo Yang", "Yongsheng Du", "Fang Tan"], "title": "Structure-Aware Corpus Construction and User-Perception-Aligned Metrics for Large-Language-Model Code Completion", "categories": ["cs.SE", "cs.AI"], "comment": "14 pages,8 figures", "summary": "Code completion technology based on large language model has significantly\nimproved the development efficiency of programmers. However, in practical\napplications, there remains a gap between current commonly used code completion\nevaluation metrics and users' actual perception. To address this issue, we\npropose two evaluation metrics for code completion tasks--LCP and ROUGE-LCP,\nfrom the perspective of probabilistic modeling. Furthermore, to tackle the lack\nof effective structural semantic modeling and cross-module dependency\ninformation in LLMs for repository-level code completion scenarios, we propose\na data processing method based on a Structure-Preserving and\nSemantically-Reordered Code Graph (SPSR-Graph). Through theoretical analysis\nand experimental validation, we demonstrate the superiority of the proposed\nevaluation metrics in terms of user perception consistency, as well as the\neffectiveness of the data processing method in enhancing model performance."}
{"id": "2505.12638", "pdf": "https://arxiv.org/pdf/2505.12638", "abs": "https://arxiv.org/abs/2505.12638", "authors": ["Yifeng Jiao", "Yuchen Liu", "Yu Zhang", "Xin Guo", "Yushuai Wu", "Chen Jiang", "Jiyang Li", "Hongwei Zhang", "Limei Han", "Xin Gao", "Yuan Qi", "Yuan Cheng"], "title": "ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data", "categories": ["q-bio.GN", "cs.AI", "cs.CE", "cs.LG"], "comment": null, "summary": "The advent of single-cell Assay for Transposase-Accessible Chromatin using\nsequencing (scATAC-seq) offers an innovative perspective for deciphering\nregulatory mechanisms by assembling a vast repository of single-cell chromatin\naccessibility data. While foundation models have achieved significant success\nin single-cell transcriptomics, there is currently no foundation model for\nscATAC-seq that supports zero-shot high-quality cell identification and\ncomprehensive multi-omics analysis simultaneously. Key challenges lie in the\nhigh dimensionality and sparsity of scATAC-seq data, as well as the lack of a\nstandardized schema for representing open chromatin regions (OCRs). Here, we\npresent \\textbf{ChromFound}, a foundation model tailored for scATAC-seq.\nChromFound utilizes a hybrid architecture and genome-aware tokenization to\neffectively capture genome-wide long contexts and regulatory signals from\ndynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues\nand 6 disease conditions, ChromFound demonstrates broad applicability across 6\ndiverse tasks. Notably, it achieves robust zero-shot performance in generating\nuniversal cell representations and exhibits excellent transferability in cell\ntype annotation and cross-omics prediction. By uncovering enhancer-gene links\nundetected by existing computational methods, ChromFound offers a promising\nframework for understanding disease risk variants in the noncoding genome."}
{"id": "2505.13076", "pdf": "https://arxiv.org/pdf/2505.13076", "abs": "https://arxiv.org/abs/2505.13076", "authors": ["Mykyta Mudryi", "Markiyan Chaklosh", "Grzegorz Wójcik"], "title": "The Hidden Dangers of Browsing AI Agents", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Autonomous browsing agents powered by large language models (LLMs) are\nincreasingly used to automate web-based tasks. However, their reliance on\ndynamic content, tool execution, and user-provided data exposes them to a broad\nattack surface. This paper presents a comprehensive security evaluation of such\nagents, focusing on systemic vulnerabilities across multiple architectural\nlayers. Our work outlines the first end-to-end threat model for browsing agents\nand provides actionable guidance for securing their deployment in real-world\nenvironments. To address discovered threats, we propose a defense in depth\nstrategy incorporating input sanitization, planner executor isolation, formal\nanalyzers, and session safeguards. These measures protect against both initial\naccess and post exploitation attack vectors. Through a white box analysis of a\npopular open source project, Browser Use, we demonstrate how untrusted web\ncontent can hijack agent behavior and lead to critical security breaches. Our\nfindings include prompt injection, domain validation bypass, and credential\nexfiltration, evidenced by a disclosed CVE and a working proof of concept\nexploit."}
{"id": "2505.12664", "pdf": "https://arxiv.org/pdf/2505.12664", "abs": "https://arxiv.org/abs/2505.12664", "authors": ["Ziqing Xing", "Zhaoyang Zhang", "Zirui Chen", "Hongning Ruan", "Zhaohui Yang"], "title": "Multi-View Wireless Sensing via Conditional Generative Learning: Framework and Model Design", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "submitted to IEEE Transactions on Wireless Communications", "summary": "In this paper, we incorporate physical knowledge into learning-based\nhigh-precision target sensing using the multi-view channel state information\n(CSI) between multiple base stations (BSs) and user equipment (UEs). Such kind\nof multi-view sensing problem can be naturally cast into a conditional\ngeneration framework. To this end, we design a bipartite neural network\narchitecture, the first part of which uses an elaborately designed encoder to\nfuse the latent target features embedded in the multi-view CSI, and then the\nsecond uses them as conditioning inputs of a powerful generative model to guide\nthe target's reconstruction. Specifically, the encoder is designed to capture\nthe physical correlation between the CSI and the target, and also be adaptive\nto the numbers and positions of BS-UE pairs. Therein the view-specific nature\nof CSI is assimilated by introducing a spatial positional embedding scheme,\nwhich exploits the structure of electromagnetic(EM)-wave propagation channels.\nFinally, a conditional diffusion model with a weighted loss is employed to\ngenerate the target's point cloud from the fused features. Extensive numerical\nresults demonstrate that the proposed generative multi-view (Gen-MV) sensing\nframework exhibits excellent flexibility and significant performance\nimprovement on the reconstruction quality of target's shape and EM properties."}
{"id": "2505.13077", "pdf": "https://arxiv.org/pdf/2505.13077", "abs": "https://arxiv.org/abs/2505.13077", "authors": ["Xiang Fei", "Jinghui Lu", "Qi Sun", "Hao Feng", "Yanjie Wang", "Wei Shi", "An-Lan Wang", "Jingqun Tang", "Can Huang"], "title": "Advancing Sequential Numerical Prediction in Autoregressive Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL."}
{"id": "2505.12674", "pdf": "https://arxiv.org/pdf/2505.12674", "abs": "https://arxiv.org/abs/2505.12674", "authors": ["Mingyuan Zhou", "Yi Gu", "Zhendong Wang"], "title": "Few-Step Diffusion via Score identity Distillation", "categories": ["cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "Diffusion distillation has emerged as a promising strategy for accelerating\ntext-to-image (T2I) diffusion models by distilling a pretrained score network\ninto a one- or few-step generator. While existing methods have made notable\nprogress, they often rely on real or teacher-synthesized images to perform well\nwhen distilling high-resolution T2I diffusion models such as Stable Diffusion\nXL (SDXL), and their use of classifier-free guidance (CFG) introduces a\npersistent trade-off between text-image alignment and generation diversity. We\naddress these challenges by optimizing Score identity Distillation (SiD) -- a\ndata-free, one-step distillation framework -- for few-step generation. Backed\nby theoretical analysis that justifies matching a uniform mixture of outputs\nfrom all generation steps to the data distribution, our few-step distillation\nalgorithm avoids step-specific networks and integrates seamlessly into existing\npipelines, achieving state-of-the-art performance on SDXL at 1024x1024\nresolution. To mitigate the alignment-diversity trade-off when real text-image\npairs are available, we introduce a Diffusion GAN-based adversarial loss\napplied to the uniform mixture and propose two new guidance strategies:\nZero-CFG, which disables CFG in the teacher and removes text conditioning in\nthe fake score network, and Anti-CFG, which applies negative CFG in the fake\nscore network. This flexible setup improves diversity without sacrificing\nalignment. Comprehensive experiments on SD1.5 and SDXL demonstrate\nstate-of-the-art performance in both one-step and few-step generation settings,\nalong with robustness to the absence of real images. Our efficient PyTorch\nimplementation, along with the resulting one- and few-step distilled\ngenerators, will be released publicly as a separate branch at\nhttps://github.com/mingyuanzhou/SiD-LSG."}
{"id": "2505.13079", "pdf": "https://arxiv.org/pdf/2505.13079", "abs": "https://arxiv.org/abs/2505.13079", "authors": ["Xugang Lu", "Peng Shen", "Yu Tsao", "Hisashi Kawai"], "title": "Cross-modal Knowledge Transfer Learning as Graph Matching Based on Optimal Transport for ASR", "categories": ["eess.AS", "cs.AI"], "comment": "To appear in Interspeech 2025", "summary": "Transferring linguistic knowledge from a pretrained language model (PLM) to\nacoustic feature learning has proven effective in enhancing end-to-end\nautomatic speech recognition (E2E-ASR). However, aligning representations\nbetween linguistic and acoustic modalities remains a challenge due to inherent\nmodality gaps. Optimal transport (OT) has shown promise in mitigating these\ngaps by minimizing the Wasserstein distance (WD) between linguistic and\nacoustic feature distributions. However, previous OT-based methods overlook\nstructural relationships, treating feature vectors as unordered sets. To\naddress this, we propose Graph Matching Optimal Transport (GM-OT), which models\nlinguistic and acoustic sequences as structured graphs. Nodes represent feature\nembeddings, while edges capture temporal and sequential relationships. GM-OT\nminimizes both WD (between nodes) and Gromov-Wasserstein distance (GWD)\n(between edges), leading to a fused Gromov-Wasserstein distance (FGWD)\nformulation. This enables structured alignment and more efficient knowledge\ntransfer compared to existing OT-based approaches. Theoretical analysis further\nshows that prior OT-based methods in linguistic knowledge transfer can be\nviewed as a special case within our GM-OT framework. We evaluate GM-OT on\nMandarin ASR using a CTC-based E2E-ASR system with a PLM for knowledge\ntransfer. Experimental results demonstrate significant performance gains over\nstate-of-the-art models, validating the effectiveness of our approach."}
{"id": "2505.12680", "pdf": "https://arxiv.org/pdf/2505.12680", "abs": "https://arxiv.org/abs/2505.12680", "authors": ["Haoyu Zhao", "Yihan Geng", "Shange Tang", "Yong Lin", "Bohan Lyu", "Hongzhou Lin", "Chi Jin", "Sanjeev Arora"], "title": "Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "27 pages", "summary": "LLM-based formal proof assistants (e.g., in Lean) hold great promise for\nautomating mathematical discovery. But beyond syntactic correctness, do these\nsystems truly understand mathematical structure as humans do? We investigate\nthis question through the lens of mathematical inequalities -- a fundamental\ntool across many domains. While modern provers can solve basic inequalities, we\nprobe their ability to handle human-intuitive compositionality. We introduce\nIneq-Comp, a benchmark built from elementary inequalities through systematic\ntransformations, including variable duplication, algebraic rewriting, and\nmulti-step composition. Although these problems remain easy for humans, we find\nthat most provers -- including Goedel, STP, and Kimina-7B -- struggle\nsignificantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly\nbecause it is trained to decompose the problems into sub-problems -- but still\nsuffers a 20\\% performance drop (pass@32). Strikingly, performance remains poor\nfor all models even when formal proofs of the constituent parts are provided in\ncontext, revealing that the source of weakness is indeed in compositional\nreasoning. Our results expose a persisting gap between the generalization\nbehavior of current AI provers and human mathematical intuition."}
{"id": "2505.13082", "pdf": "https://arxiv.org/pdf/2505.13082", "abs": "https://arxiv.org/abs/2505.13082", "authors": ["Kyeongman Park", "Seongho Joo", "Kyomin Jung"], "title": "MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and Voices of Multiple Speakers", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "We introduce MultiActor-Audiobook, a zero-shot approach for generating\naudiobooks that automatically produces consistent, expressive, and\nspeaker-appropriate prosody, including intonation and emotion. Previous\naudiobook systems have several limitations: they require users to manually\nconfigure the speaker's prosody, read each sentence with a monotonic tone\ncompared to voice actors, or rely on costly training. However, our\nMultiActor-Audiobook addresses these issues by introducing two novel processes:\n(1) MSP (**Multimodal Speaker Persona Generation**) and (2) LSI (**LLM-based\nScript Instruction Generation**). With these two processes,\nMultiActor-Audiobook can generate more emotionally expressive audiobooks with a\nconsistent speaker prosody without additional training. We compare our system\nwith commercial products, through human and MLLM evaluations, achieving\ncompetitive results. Furthermore, we demonstrate the effectiveness of MSP and\nLSI through ablation studies."}
{"id": "2505.12705", "pdf": "https://arxiv.org/pdf/2505.12705", "abs": "https://arxiv.org/abs/2505.12705", "authors": ["Joel Jang", "Seonghyeon Ye", "Zongyu Lin", "Jiannan Xiang", "Johan Bjorck", "Yu Fang", "Fengyuan Hu", "Spencer Huang", "Kaushil Kundalia", "Yen-Chen Lin", "Loic Magne", "Ajay Mandlekar", "Avnish Narayan", "You Liang Tan", "Guanzhi Wang", "Jing Wang", "Qi Wang", "Yinzhen Xu", "Xiaohui Zeng", "Kaiyuan Zheng", "Ruijie Zheng", "Ming-Yu Liu", "Luke Zettlemoyer", "Dieter Fox", "Jan Kautz", "Scott Reed", "Yuke Zhu", "Linxi Fan"], "title": "DreamGen: Unlocking Generalization in Robot Learning through Neural Trajectories", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "See website for videos:\n  https://research.nvidia.com/labs/gear/dreamgen", "summary": "We introduce DreamGen, a simple yet highly effective 4-stage pipeline for\ntraining robot policies that generalize across behaviors and environments\nthrough neural trajectories - synthetic robot data generated from video world\nmodels. DreamGen leverages state-of-the-art image-to-video generative models,\nadapting them to the target robot embodiment to produce photorealistic\nsynthetic videos of familiar or novel tasks in diverse environments. Since\nthese models generate only videos, we recover pseudo-action sequences using\neither a latent action model or an inverse-dynamics model (IDM). Despite its\nsimplicity, DreamGen unlocks strong behavior and environment generalization: a\nhumanoid robot can perform 22 new behaviors in both seen and unseen\nenvironments, while requiring teleoperation data from only a single\npick-and-place task in one environment. To evaluate the pipeline\nsystematically, we introduce DreamGen Bench, a video generation benchmark that\nshows a strong correlation between benchmark performance and downstream policy\nsuccess. Our work establishes a promising new axis for scaling robot learning\nwell beyond manual data collection."}
{"id": "2505.13087", "pdf": "https://arxiv.org/pdf/2505.13087", "abs": "https://arxiv.org/abs/2505.13087", "authors": ["Adrien Lagesse", "Marc Lelarge"], "title": "Graph Alignment for Benchmarking Graph Neural Networks and Learning Positional Encodings", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose a novel benchmarking methodology for graph neural networks (GNNs)\nbased on the graph alignment problem, a combinatorial optimization task that\ngeneralizes graph isomorphism by aligning two unlabeled graphs to maximize\noverlapping edges. We frame this problem as a self-supervised learning task and\npresent several methods to generate graph alignment datasets using synthetic\nrandom graphs and real-world graph datasets from multiple domains. For a given\ngraph dataset, we generate a family of graph alignment datasets with increasing\ndifficulty, allowing us to rank the performance of various architectures. Our\nexperiments indicate that anisotropic graph neural networks outperform standard\nconvolutional architectures. To further demonstrate the utility of the graph\nalignment task, we show its effectiveness for unsupervised GNN pre-training,\nwhere the learned node embeddings outperform other positional encodings on\nthree molecular regression tasks and achieve state-of-the-art results on the\nPCQM4Mv2 dataset with significantly fewer parameters. To support\nreproducibility and further research, we provide an open-source Python package\nto generate graph alignment datasets and benchmark new GNN architectures."}
{"id": "2505.12713", "pdf": "https://arxiv.org/pdf/2505.12713", "abs": "https://arxiv.org/abs/2505.12713", "authors": ["Subhayan Saha", "Giovanni Barbarino", "Nicolas Gillis"], "title": "Identifiability of Nonnegative Tucker Decompositions -- Part I: Theory", "categories": ["math.NA", "cs.LG", "cs.NA", "eess.SP", "stat.ML"], "comment": "40 pages, 2 figures", "summary": "Tensor decompositions have become a central tool in data science, with\napplications in areas such as data analysis, signal processing, and machine\nlearning. A key property of many tensor decompositions, such as the canonical\npolyadic decomposition, is identifiability: the factors are unique, up to\ntrivial scaling and permutation ambiguities. This allows one to recover the\ngroundtruth sources that generated the data. The Tucker decomposition (TD) is a\ncentral and widely used tensor decomposition model. However, it is in general\nnot identifiable. In this paper, we study the identifiability of the\nnonnegative TD (nTD). By adapting and extending identifiability results of\nnonnegative matrix factorization (NMF), we provide uniqueness results for nTD.\nOur results require the nonnegative matrix factors to have some degree of\nsparsity (namely, satisfy the separability condition, or the sufficiently\nscattered condition), while the core tensor only needs to have some slices (or\nlinear combinations of them) or unfoldings with full column rank (but does not\nneed to be nonnegative). Under such conditions, we derive several procedures,\nusing either unfoldings or slices of the input tensor, to obtain identifiable\nnTDs by minimizing the volume of unfoldings or slices of the core tensor."}
{"id": "2505.13094", "pdf": "https://arxiv.org/pdf/2505.13094", "abs": "https://arxiv.org/abs/2505.13094", "authors": ["Guo Chen", "Kai Li", "Runxuan Yang", "Xiaolin Hu"], "title": "Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech Separation", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Existing causal speech separation models often underperform compared to\nnon-causal models due to difficulties in retaining historical information. To\naddress this, we propose the Time-Frequency Attention Cache Memory (TFACM)\nmodel, which effectively captures spatio-temporal relationships through an\nattention mechanism and cache memory (CM) for historical information storage.\nIn TFACM, an LSTM layer captures frequency-relative positions, while causal\nmodeling is applied to the time dimension using local and global\nrepresentations. The CM module stores past information, and the causal\nattention refinement (CAR) module further enhances time-based feature\nrepresentations for finer granularity. Experimental results showed that TFACM\nachieveed comparable performance to the SOTA TF-GridNet-Causal model, with\nsignificantly lower complexity and fewer trainable parameters. For more\ndetails, visit the project page: https://cslikai.cn/TFACM/."}
{"id": "2505.12750", "pdf": "https://arxiv.org/pdf/2505.12750", "abs": "https://arxiv.org/abs/2505.12750", "authors": ["Filippo Leveni", "Matteo Mistura", "Francesco Iubatti", "Carmine Giangregorio", "Nicolò Pastore", "Cesare Alippi", "Giacomo Boracchi"], "title": "Malware families discovery via Open-Set Recognition on Android manifest permissions", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "Submitted to European Conference on Artificial Intelligence (ECAI\n  2025)", "summary": "Malware are malicious programs that are grouped into families based on their\npenetration technique, source code, and other characteristics. Classifying\nmalware programs into their respective families is essential for building\neffective defenses against cyber threats. Machine learning models have a huge\npotential in malware detection on mobile devices, as malware families can be\nrecognized by classifying permission data extracted from Android manifest\nfiles. Still, the malware classification task is challenging due to the\nhigh-dimensional nature of permission data and the limited availability of\ntraining samples. In particular, the steady emergence of new malware families\nmakes it impossible to acquire a comprehensive training set covering all the\nmalware classes. In this work, we present a malware classification system that,\non top of classifying known malware, detects new ones. In particular, we\ncombine an open-set recognition technique developed within the computer vision\ncommunity, namely MaxLogit, with a tree-based Gradient Boosting classifier,\nwhich is particularly effective in classifying high-dimensional data. Our\nsolution turns out to be very practical, as it can be seamlessly employed in a\nstandard classification workflow, and efficient, as it adds minimal\ncomputational overhead. Experiments on public and proprietary datasets\ndemonstrate the potential of our solution, which has been deployed in a\nbusiness environment."}
{"id": "2505.13101", "pdf": "https://arxiv.org/pdf/2505.13101", "abs": "https://arxiv.org/abs/2505.13101", "authors": ["Shaowu Wu", "Liting Zeng", "Wei Lu", "Xiangyang Luo"], "title": "ARIW-Framework: Adaptive Robust Iterative Watermarking Framework", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "With the rapid rise of large models, copyright protection for generated image\ncontent has become a critical security challenge. Although deep learning\nwatermarking techniques offer an effective solution for digital image copyright\nprotection, they still face limitations in terms of visual quality, robustness\nand generalization. To address these issues, this paper proposes an adaptive\nrobust iterative watermarking framework (ARIW-Framework) that achieves\nhigh-quality watermarked images while maintaining exceptional robustness and\ngeneralization performance. Specifically, we introduce an iterative approach to\noptimize the encoder for generating robust residuals. The encoder incorporates\nnoise layers and a decoder to compute robustness weights for residuals under\nvarious noise attacks. By employing a parallel optimization strategy, the\nframework enhances robustness against multiple types of noise attacks.\nFurthermore, we leverage image gradients to determine the embedding strength at\neach pixel location, significantly improving the visual quality of the\nwatermarked images. Extensive experiments demonstrate that the proposed method\nachieves superior visual quality while exhibiting remarkable robustness and\ngeneralization against noise attacks."}
{"id": "2505.12758", "pdf": "https://arxiv.org/pdf/2505.12758", "abs": "https://arxiv.org/abs/2505.12758", "authors": ["Matias Quintana", "Youlong Gu", "Xiucheng Liang", "Yujun Hou", "Koichi Ito", "Yihan Zhu", "Mahmoud Abdelrahman", "Filip Biljecki"], "title": "It's not you, it's me -- Global urban visual perception varies across demographics and personalities", "categories": ["cs.CV", "cs.LG"], "comment": "Under review", "summary": "Understanding people's preferences and needs is crucial for urban planning\ndecisions, yet current approaches often combine them from multi-cultural and\nmulti-city populations, obscuring important demographic differences and risking\namplifying biases. We conducted a large-scale urban visual perception survey of\nstreetscapes worldwide using street view imagery, examining how demographics --\nincluding gender, age, income, education, race and ethnicity, and, for the\nfirst time, personality traits -- shape perceptions among 1,000 participants,\nwith balanced demographics, from five countries and 45 nationalities. This\ndataset, introduced as Street Perception Evaluation Considering Socioeconomics\n(SPECS), exhibits statistically significant differences in perception scores in\nsix traditionally used indicators (safe, lively, wealthy, beautiful, boring,\nand depressing) and four new ones we propose (live nearby, walk, cycle, green)\namong demographics and personalities. We revealed that location-based\nsentiments are carried over in people's preferences when comparing urban\nstreetscapes with other cities. Further, we compared the perception scores\nbased on where participants and streetscapes are from. We found that an\noff-the-shelf machine learning model trained on an existing global perception\ndataset tends to overestimate positive indicators and underestimate negative\nones compared to human responses, suggesting that targeted intervention should\nconsider locals' perception. Our study aspires to rectify the myopic treatment\nof street perception, which rarely considers demographics or personality\ntraits."}
{"id": "2505.13102", "pdf": "https://arxiv.org/pdf/2505.13102", "abs": "https://arxiv.org/abs/2505.13102", "authors": ["Ji Qi", "Tam Thuc Do", "Mingxiao Liu", "Zhuoshi Pan", "Yuzhe Li", "Gene Cheung", "H. Vicky Zhao"], "title": "Lightweight Transformer via Unrolling of Mixed Graph Algorithms for Traffic Forecast", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": "19 pages, 5 figures, 8 tables", "summary": "To forecast traffic with both spatial and temporal dimensions, we unroll a\nmixed-graph-based optimization algorithm into a lightweight and interpretable\ntransformer-like neural net. Specifically, we construct two graphs: an\nundirected graph $\\mathcal{G}^u$ capturing spatial correlations across\ngeography, and a directed graph $\\mathcal{G}^d$ capturing sequential\nrelationships over time. We formulate a prediction problem for the future\nsamples of signal $\\mathbf{x}$, assuming it is \"smooth\" with respect to both\n$\\mathcal{G}^u$ and $\\mathcal{G}^d$, where we design new $\\ell_2$ and\n$\\ell_1$-norm variational terms to quantify and promote signal smoothness\n(low-frequency reconstruction) on a directed graph. We construct an iterative\nalgorithm based on alternating direction method of multipliers (ADMM), and\nunroll it into a feed-forward network for data-driven parameter learning. We\ninsert graph learning modules for $\\mathcal{G}^u$ and $\\mathcal{G}^d$, which\nare akin to the self-attention mechanism in classical transformers. Experiments\nshow that our unrolled networks achieve competitive traffic forecast\nperformance as state-of-the-art prediction schemes, while reducing parameter\ncounts drastically. Our code is available in\nhttps://github.com/SingularityUndefined/Unrolling-GSP-STForecast."}
{"id": "2505.12791", "pdf": "https://arxiv.org/pdf/2505.12791", "abs": "https://arxiv.org/abs/2505.12791", "authors": ["Yiling Tao", "Shuyi Wang", "Jiaxi Yang", "Guido Zuccon"], "title": "Unlearning for Federated Online Learning to Rank: A Reproducibility Study", "categories": ["cs.IR", "cs.LG"], "comment": "Accepted at SIGIR2025", "summary": "This paper reports on findings from a comparative study on the effectiveness\nand efficiency of federated unlearning strategies within Federated Online\nLearning to Rank (FOLTR), with specific attention to systematically analysing\nthe unlearning capabilities of methods in a verifiable manner.\n  Federated approaches to ranking of search results have recently garnered\nattention to address users privacy concerns. In FOLTR, privacy is safeguarded\nby collaboratively training ranking models across decentralized data sources,\npreserving individual user data while optimizing search results based on\nimplicit feedback, such as clicks.\n  Recent legislation introduced across numerous countries is establishing the\nso called \"the right to be forgotten\", according to which services based on\nmachine learning models like those in FOLTR should provide capabilities that\nallow users to remove their own data from those used to train models. This has\nsparked the development of unlearning methods, along with evaluation practices\nto measure whether unlearning of a user data successfully occurred. Current\nevaluation practices are however often controversial, necessitating the use of\nmultiple metrics for a more comprehensive assessment -- but previous proposals\nof unlearning methods only used single evaluation metrics.\n  This paper addresses this limitation: our study rigorously assesses the\neffectiveness of unlearning strategies in managing both under-unlearning and\nover-unlearning scenarios using adapted, and newly proposed evaluation metrics.\nThanks to our detailed analysis, we uncover the strengths and limitations of\nfive unlearning strategies, offering valuable insights into optimizing\nfederated unlearning to balance data privacy and system performance within\nFOLTR. We publicly release our code and complete results at\nhttps://github.com/Iris1026/Unlearning-for-FOLTR.git."}
{"id": "2505.13109", "pdf": "https://arxiv.org/pdf/2505.13109", "abs": "https://arxiv.org/abs/2505.13109", "authors": ["Guangda Liu", "Chengwei Li", "Zhenyu Ning", "Jing Lin", "Yiwu Yao", "Danning Ke", "Minyi Guo", "Jieru Zhao"], "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."}
{"id": "2505.12795", "pdf": "https://arxiv.org/pdf/2505.12795", "abs": "https://arxiv.org/abs/2505.12795", "authors": ["Shibo Hong", "Jiahao Ying", "Haiyuan Liang", "Mengdi Zhang", "Jun Kuang", "Jiazheng Zhang", "Yixin Cao"], "title": "FRAbench and GenEval: Scaling Fine-Grained Aspect Evaluation across Tasks, Modalities", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Evaluating the open-ended outputs of large language models (LLMs) has become\na bottleneck as model capabilities, task diversity, and modality coverage\nrapidly expand. Existing \"LLM-as-a-Judge\" evaluators are typically narrow in a\nfew tasks, aspects, or modalities, and easily suffer from low consistency. In\nthis paper, we argue that explicit, fine-grained aspect specification is the\nkey to both generalizability and objectivity in automated evaluation. To do so,\nwe introduce a hierarchical aspect taxonomy spanning 112 aspects that unifies\nevaluation across four representative settings - Natural Language Generation,\nImage Understanding, Image Generation, and Interleaved Text-and-Image\nGeneration. Building on this taxonomy, we create FRAbench, a benchmark\ncomprising 60.4k pairwise samples with 325k aspect-level labels obtained from a\ncombination of human and LLM annotations. FRAbench provides the first\nlarge-scale, multi-modal resource for training and meta-evaluating fine-grained\nLMM judges. Leveraging FRAbench, we develop GenEval, a fine-grained evaluator\ngeneralizable across tasks and modalities. Experiments show that GenEval (i)\nattains high agreement with GPT-4o and expert annotators, (ii) transfers\nrobustly to unseen tasks and modalities, and (iii) reveals systematic\nweaknesses of current LMMs on evaluation."}
{"id": "2505.13115", "pdf": "https://arxiv.org/pdf/2505.13115", "abs": "https://arxiv.org/abs/2505.13115", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands", "summary": "The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications."}
{"id": "2505.12801", "pdf": "https://arxiv.org/pdf/2505.12801", "abs": "https://arxiv.org/abs/2505.12801", "authors": ["Konstantina Lelova", "Gregory F. Cooper", "Sofia Triantafillou"], "title": "Testing Identifiability and Transportability with Observational and Experimental Data", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Transporting causal information learned from experiments in one population to\nanother is a critical challenge in clinical research and decision-making.\nCausal transportability uses causal graphs to model differences between the\nsource and target populations and identifies conditions under which causal\neffects learned from experiments can be reused in a different population.\nSimilarly, causal identifiability identifies conditions under which causal\neffects can be estimated from observational data. However, these approaches\nrely on knowing the causal graph, which is often unavailable in real-world\nsettings. In this work, we propose a Bayesian method for assessing whether\nZ-specific (conditional) causal effects are both identifiable and\ntransportable, without knowing the causal graph. Our method combines\nexperimental data from the source population with observational data from the\ntarget population to compute the probability that a causal effect is both\nidentifiable from observational data and transportable. When this holds, we\nleverage both observational data from the target domain and experimental data\nfrom the source domain to obtain an unbiased, efficient estimator of the causal\neffect in the target population. Using simulations, we demonstrate that our\nmethod correctly identifies transportable causal effects and improves causal\neffect estimation."}
{"id": "2505.13116", "pdf": "https://arxiv.org/pdf/2505.13116", "abs": "https://arxiv.org/abs/2505.13116", "authors": ["Kathrin Lammers", "Valerie Vaquet", "Barbara Hammer"], "title": "Continuous Fair SMOTE -- Fairness-Aware Stream Learning from Imbalanced Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As machine learning is increasingly applied in an online fashion to deal with\nevolving data streams, the fairness of these algorithms is a matter of growing\nethical and legal concern. In many use cases, class imbalance in the data also\nneeds to be dealt with to ensure predictive performance. Current fairness-aware\nstream learners typically attempt to solve these issues through in- or\npost-processing by focusing on optimizing one specific discrimination metric,\naddressing class imbalance in a separate processing step. While C-SMOTE is a\nhighly effective model-agnostic pre-processing approach to mitigate class\nimbalance, as a side effect of this method, algorithmic bias is often\nintroduced.\n  Therefore, we propose CFSMOTE - a fairness-aware, continuous SMOTE variant -\nas a pre-processing approach to simultaneously address the class imbalance and\nfairness concerns by employing situation testing and balancing\nfairness-relevant groups during oversampling. Unlike other fairness-aware\nstream learners, CFSMOTE is not optimizing for only one specific fairness\nmetric, therefore avoiding potentially problematic trade-offs. Our experiments\nshow significant improvement on several common group fairness metrics in\ncomparison to vanilla C-SMOTE while maintaining competitive performance, also\nin comparison to other fairness-aware algorithms."}
{"id": "2505.12803", "pdf": "https://arxiv.org/pdf/2505.12803", "abs": "https://arxiv.org/abs/2505.12803", "authors": ["Jiawen Xu", "Odej Kao", "Margret Keuper"], "title": "Informed Mixing -- Improving Open Set Recognition via Attribution-based Augmentation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Open set recognition (OSR) is devised to address the problem of detecting\nnovel classes during model inference. Even in recent vision models, this\nremains an open issue which is receiving increasing attention. Thereby, a\ncrucial challenge is to learn features that are relevant for unseen categories\nfrom given data, for which these features might not be discriminative. To\nfacilitate this process and \"optimize to learn\" more diverse features, we\npropose GradMix, a data augmentation method that dynamically leverages\ngradient-based attribution maps of the model during training to mask out\nalready learned concepts. Thus GradMix encourages the model to learn a more\ncomplete set of representative features from the same data source. Extensive\nexperiments on open set recognition, close set classification, and\nout-of-distribution detection reveal that our method can often outperform the\nstate-of-the-art. GradMix can further increase model robustness to corruptions\nas well as downstream classification performance for self-supervised learning,\nindicating its benefit for model generalization."}
{"id": "2505.13122", "pdf": "https://arxiv.org/pdf/2505.13122", "abs": "https://arxiv.org/abs/2505.13122", "authors": ["François Bachoc", "Jérôme Bolte", "Ryan Boustany", "Jean-Michel Loubes"], "title": "When majority rules, minority loses: bias amplification of gradient descent", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "Despite growing empirical evidence of bias amplification in machine learning,\nits theoretical foundations remain poorly understood. We develop a formal\nframework for majority-minority learning tasks, showing how standard training\ncan favor majority groups and produce stereotypical predictors that neglect\nminority-specific features. Assuming population and variance imbalance, our\nanalysis reveals three key findings: (i) the close proximity between\n``full-data'' and stereotypical predictors, (ii) the dominance of a region\nwhere training the entire model tends to merely learn the majority traits, and\n(iii) a lower bound on the additional training required. Our results are\nillustrated through experiments in deep learning for tabular and image\nclassification tasks."}
{"id": "2505.12808", "pdf": "https://arxiv.org/pdf/2505.12808", "abs": "https://arxiv.org/abs/2505.12808", "authors": ["Yanbin Yin", "Kun Zhou", "Zhen Wang", "Xiangdong Zhang", "Yifei Shao", "Shibo Hao", "Yi Gu", "Jieyuan Liu", "Somanshu Singla", "Tianyang Liu", "Eric P. Xing", "Zhengzhong Liu", "Haojian Jin", "Zhiting Hu"], "title": "Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "20 pages, ongoing work", "summary": "The recent explosion of large language models (LLMs), each with its own\ngeneral or specialized strengths, makes scalable, reliable benchmarking more\nurgent than ever. Standard practices nowadays face fundamental trade-offs:\nclosed-ended question-based benchmarks (eg MMLU) struggle with saturation as\nnewer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely\non costly and slow human judges. Recently, automated methods (eg\nLLM-as-a-judge) shed light on the scalability, but risk bias by relying on one\nor a few \"authority\" models. To tackle these issues, we propose Decentralized\nArena (dearena), a fully automated framework leveraging collective intelligence\nfrom all LLMs to evaluate each other. It mitigates single-model judge bias by\ndemocratic, pairwise evaluation, and remains efficient at scale through two key\ncomponents: (1) a coarse-to-fine ranking algorithm for fast incremental\ninsertion of new models with sub-quadratic complexity, and (2) an automatic\nquestion selection strategy for the construction of new evaluation dimensions.\nAcross extensive experiments across 66 LLMs, dearena attains up to 97%\ncorrelation with human judgements, while significantly reducing the cost. Our\ncode and data will be publicly released on\nhttps://github.com/maitrix-org/de-arena."}
{"id": "2505.13123", "pdf": "https://arxiv.org/pdf/2505.13123", "abs": "https://arxiv.org/abs/2505.13123", "authors": ["Snehashis Majhi", "Giacomo D'Amicantonio", "Antitza Dantcheva", "Quan Kong", "Lorenzo Garattoni", "Gianpiero Francesca", "Egor Bondarev", "Francois Bremond"], "title": "Just Dance with $π$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Weakly-supervised methods for video anomaly detection (VAD) are\nconventionally based merely on RGB spatio-temporal features, which continues to\nlimit their reliability in real-world scenarios. This is due to the fact that\nRGB-features are not sufficiently distinctive in setting apart categories such\nas shoplifting from visually similar events. Therefore, towards robust complex\nreal-world VAD, it is essential to augment RGB spatio-temporal features by\nadditional modalities. Motivated by this, we introduce the Poly-modal Induced\nframework for VAD: \"PI-VAD\", a novel approach that augments RGB representations\nby five additional modalities. Specifically, the modalities include sensitivity\nto fine-grained motion (Pose), three dimensional scene and entity\nrepresentation (Depth), surrounding objects (Panoptic masks), global motion\n(optical flow), as well as language cues (VLM). Each modality represents an\naxis of a polygon, streamlined to add salient cues to RGB. PI-VAD includes two\nplug-in modules, namely Pseudo-modality Generation module and Cross Modal\nInduction module, which generate modality-specific prototypical representation\nand, thereby, induce multi-modal information into RGB cues. These modules\noperate by performing anomaly-aware auxiliary tasks and necessitate five\nmodality backbones -- only during training. Notably, PI-VAD achieves\nstate-of-the-art accuracy on three prominent VAD datasets encompassing\nreal-world scenarios, without requiring the computational overhead of five\nmodality backbones at inference."}
{"id": "2505.12811", "pdf": "https://arxiv.org/pdf/2505.12811", "abs": "https://arxiv.org/abs/2505.12811", "authors": ["Wei-Chen Liao", "Ti-Rong Wu", "I-Chen Wu"], "title": "Dynamic Sight Range Selection in Multi-Agent Reinforcement Learning", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": "Accepted at AAMAS 2025. The compiled PDF includes the appendix", "summary": "Multi-agent reinforcement Learning (MARL) is often challenged by the sight\nrange dilemma, where agents either receive insufficient or excessive\ninformation from their environment. In this paper, we propose a novel method,\ncalled Dynamic Sight Range Selection (DSR), to address this issue. DSR utilizes\nan Upper Confidence Bound (UCB) algorithm and dynamically adjusts the sight\nrange during training. Experiment results show several advantages of using DSR.\nFirst, we demonstrate using DSR achieves better performance in three common\nMARL environments, including Level-Based Foraging (LBF), Multi-Robot Warehouse\n(RWARE), and StarCraft Multi-Agent Challenge (SMAC). Second, our results show\nthat DSR consistently improves performance across multiple MARL algorithms,\nincluding QMIX and MAPPO. Third, DSR offers suitable sight ranges for different\ntraining steps, thereby accelerating the training process. Finally, DSR\nprovides additional interpretability by indicating the optimal sight range used\nduring training. Unlike existing methods that rely on global information or\ncommunication mechanisms, our approach operates solely based on the individual\nsight ranges of agents. This approach offers a practical and efficient solution\nto the sight range dilemma, making it broadly applicable to real-world complex\nenvironments."}
{"id": "2505.13124", "pdf": "https://arxiv.org/pdf/2505.13124", "abs": "https://arxiv.org/abs/2505.13124", "authors": ["Francesco Innocenti", "El Mehdi Achour", "Christopher L. Buckley"], "title": "$μ$PC: Scaling Predictive Coding to 100+ Layer Networks", "categories": ["cs.LG", "cs.AI", "cs.NE", "I.2.6"], "comment": "34 pages, 41 figures", "summary": "The biological implausibility of backpropagation (BP) has motivated many\nalternative, brain-inspired algorithms that attempt to rely only on local\ninformation, such as predictive coding (PC) and equilibrium propagation.\nHowever, these algorithms have notoriously struggled to train very deep\nnetworks, preventing them from competing with BP in large-scale settings.\nIndeed, scaling PC networks (PCNs) has recently been posed as a challenge for\nthe community (Pinchetti et al., 2024). Here, we show that 100+ layer PCNs can\nbe trained reliably using a Depth-$\\mu$P parameterisation (Yang et al., 2023;\nBordelon et al., 2023) which we call \"$\\mu$PC\". Through an extensive analysis\nof the scaling behaviour of PCNs, we reveal several pathologies that make\nstandard PCNs difficult to train at large depths. We then show that, despite\naddressing only some of these instabilities, $\\mu$PC allows stable training of\nvery deep (up to 128-layer) residual networks on simple classification tasks\nwith competitive performance and little tuning compared to current benchmarks.\nMoreover, $\\mu$PC enables zero-shot transfer of both weight and activity\nlearning rates across widths and depths. Our results have implications for\nother local algorithms and could be extended to convolutional and transformer\narchitectures. Code for $\\mu$PC is made available as part of a JAX library for\nPCNs at https://github.com/thebuckleylab/jpc (Innocenti et al., 2024)."}
{"id": "2505.12836", "pdf": "https://arxiv.org/pdf/2505.12836", "abs": "https://arxiv.org/abs/2505.12836", "authors": ["Muhamed Kuric", "Martin Zach", "Andreas Habring", "Michael Unser", "Thomas Pock"], "title": "The Gaussian Latent Machine: Efficient Prior and Posterior Sampling for Inverse Problems", "categories": ["eess.IV", "cs.CV", "cs.LG", "stat.ML", "65C40, 65C05, 68U10, 65C60"], "comment": null, "summary": "We consider the problem of sampling from a product-of-experts-type model that\nencompasses many standard prior and posterior distributions commonly found in\nBayesian imaging. We show that this model can be easily lifted into a novel\nlatent variable model, which we refer to as a Gaussian latent machine. This\nleads to a general sampling approach that unifies and generalizes many existing\nsampling algorithms in the literature. Most notably, it yields a highly\nefficient and effective two-block Gibbs sampling approach in the general case,\nwhile also specializing to direct sampling algorithms in particular cases.\nFinally, we present detailed numerical experiments that demonstrate the\nefficiency and effectiveness of our proposed sampling approach across a wide\nrange of prior and posterior sampling problems from Bayesian imaging."}
{"id": "2505.13130", "pdf": "https://arxiv.org/pdf/2505.13130", "abs": "https://arxiv.org/abs/2505.13130", "authors": ["Muhammad Awais Amin", "Adama Ilboudo", "Abdul Samad bin Shahid", "Amjad Ali", "Waqas Haider Khan Bangyal"], "title": "Adaptive Image Restoration for Video Surveillance: A Real-Time Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "One of the major challenges in the field of computer vision especially for\ndetection, segmentation, recognition, monitoring, and automated solutions, is\nthe quality of images. Image degradation, often caused by factors such as rain,\nfog, lighting, etc., has a negative impact on automated\ndecision-making.Furthermore, several image restoration solutions exist,\nincluding restoration models for single degradation and restoration models for\nmultiple degradations. However, these solutions are not suitable for real-time\nprocessing. In this study, the aim was to develop a real-time image restoration\nsolution for video surveillance. To achieve this, using transfer learning with\nResNet_50, we developed a model for automatically identifying the types of\ndegradation present in an image to reference the necessary treatment(s) for\nimage restoration. Our solution has the advantage of being flexible and\nscalable."}
{"id": "2505.12848", "pdf": "https://arxiv.org/pdf/2505.12848", "abs": "https://arxiv.org/abs/2505.12848", "authors": ["Adarsh Singh"], "title": "A Comprehensive Benchmarking Platform for Deep Generative Models in Molecular Design", "categories": ["physics.atom-ph", "cs.LG"], "comment": null, "summary": "The development of novel pharmaceuticals represents a significant challenge\nin modern science, with substantial costs and time investments. Deep generative\nmodels have emerged as promising tools for accelerating drug discovery by\nefficiently exploring the vast chemical space. However, this rapidly evolving\nfield lacks standardized evaluation protocols, impeding fair comparison between\napproaches. This research presents an extensive analysis of the Molecular Sets\n(MOSES) platform, a comprehensive benchmarking framework designed to\nstandardize evaluation of deep generative models in molecular design. Through\nrigorous assessment of multiple generative architectures, including recurrent\nneural networks, variational autoencoders, and generative adversarial networks,\nwe examine their capabilities in generating valid, unique, and novel molecular\nstructures while maintaining specific chemical properties. Our findings reveal\nthat different architectures exhibit complementary strengths across various\nmetrics, highlighting the complex trade-offs between exploration and\nexploitation in chemical space. This study provides detailed insights into the\ncurrent state of the art in molecular generation and establishes a foundation\nfor future advancements in AI-driven drug discovery."}
{"id": "2505.13136", "pdf": "https://arxiv.org/pdf/2505.13136", "abs": "https://arxiv.org/abs/2505.13136", "authors": ["Anton Ehrmanntraut", "Julia Wunderle", "Jan Pfister", "Fotis Jannidis", "Andreas Hotho"], "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review @ARR", "summary": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models."}
{"id": "2505.12864", "pdf": "https://arxiv.org/pdf/2505.12864", "abs": "https://arxiv.org/abs/2505.12864", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan Hermstrüwer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "comment": null, "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/"}
{"id": "2505.13144", "pdf": "https://arxiv.org/pdf/2505.13144", "abs": "https://arxiv.org/abs/2505.13144", "authors": ["Dongsu Lee", "Minhae Kwon"], "title": "Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "2025 ICML", "summary": "The goal of offline reinforcement learning (RL) is to extract a\nhigh-performance policy from the fixed datasets, minimizing performance\ndegradation due to out-of-distribution (OOD) samples. Offline model-based RL\n(MBRL) is a promising approach that ameliorates OOD issues by enriching\nstate-action transitions with augmentations synthesized via a learned dynamics\nmodel. Unfortunately, seminal offline MBRL methods often struggle in\nsparse-reward, long-horizon tasks. In this work, we introduce a novel MBRL\nframework, dubbed Temporal Distance-Aware Transition Augmentation (TempDATA),\nthat generates augmented transitions in a temporally structured latent space\nrather than in raw state space. To model long-horizon behavior, TempDATA learns\na latent abstraction that captures a temporal distance from both trajectory and\ntransition levels of state space. Our experiments confirm that TempDATA\noutperforms previous offline MBRL methods and achieves matching or surpassing\nthe performance of diffusion-based trajectory augmentation and goal-conditioned\nRL on the D4RL AntMaze, FrankaKitchen, CALVIN, and pixel-based FrankaKitchen."}
{"id": "2505.12868", "pdf": "https://arxiv.org/pdf/2505.12868", "abs": "https://arxiv.org/abs/2505.12868", "authors": ["Marin Šola", "Peter Bühlmann", "Xinwei Shen"], "title": "Causality-Inspired Robustness for Nonlinear Models via Representation Learning", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Distributional robustness is a central goal of prediction algorithms due to\nthe prevalent distribution shifts in real-world data. The prediction model aims\nto minimize the worst-case risk among a class of distributions, a.k.a., an\nuncertainty set. Causality provides a modeling framework with a rigorous\nrobustness guarantee in the above sense, where the uncertainty set is\ndata-driven rather than pre-specified as in traditional distributional\nrobustness optimization. However, current causality-inspired robustness methods\npossess finite-radius robustness guarantees only in the linear settings, where\nthe causal relationships among the covariates and the response are linear. In\nthis work, we propose a nonlinear method under a causal framework by\nincorporating recent developments in identifiable representation learning and\nestablish a distributional robustness guarantee. To our best knowledge, this is\nthe first causality-inspired robustness method with such a finite-radius\nrobustness guarantee in nonlinear settings. Empirical validation of the\ntheoretical findings is conducted on both synthetic data and real-world\nsingle-cell data, also illustrating that finite-radius robustness is crucial."}
{"id": "2505.13156", "pdf": "https://arxiv.org/pdf/2505.13156", "abs": "https://arxiv.org/abs/2505.13156", "authors": ["Zhi Liu", "Tao Yang", "Jing Wang", "Yexin Chen", "Zhan Gao", "Jiaxi Yang", "Kui Chen", "Bingji Lu", "Xiaochen Li", "Changyong Luo", "Yan Li", "Xiaohong Gu", "Peng Cao"], "title": "Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice", "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 4 figures, and 1 tables", "summary": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application."}
{"id": "2505.12872", "pdf": "https://arxiv.org/pdf/2505.12872", "abs": "https://arxiv.org/abs/2505.12872", "authors": ["Maytus Piriyajitakonkij", "Rujikorn Charakorn", "Weicheng Tao", "Wei Pan", "Mingfei Sun", "Cheston Tan", "Mengmi Zhang"], "title": "From Grunts to Grammar: Emergent Language from Cooperative Foraging", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Early cavemen relied on gestures, vocalizations, and simple signals to\ncoordinate, plan, avoid predators, and share resources. Today, humans\ncollaborate using complex languages to achieve remarkable results. What drives\nthis evolution in communication? How does language emerge, adapt, and become\nvital for teamwork? Understanding the origins of language remains a challenge.\nA leading hypothesis in linguistics and anthropology posits that language\nevolved to meet the ecological and social demands of early human cooperation.\nLanguage did not arise in isolation, but through shared survival goals.\nInspired by this view, we investigate the emergence of language in multi-agent\nForaging Games. These environments are designed to reflect the cognitive and\necological constraints believed to have influenced the evolution of\ncommunication. Agents operate in a shared grid world with only partial\nknowledge about other agents and the environment, and must coordinate to\ncomplete games like picking up high-value targets or executing temporally\nordered actions. Using end-to-end deep reinforcement learning, agents learn\nboth actions and communication strategies from scratch. We find that agents\ndevelop communication protocols with hallmark features of natural language:\narbitrariness, interchangeability, displacement, cultural transmission, and\ncompositionality. We quantify each property and analyze how different factors,\nsuch as population size and temporal dependencies, shape specific aspects of\nthe emergent language. Our framework serves as a platform for studying how\nlanguage can evolve from partial observability, temporal reasoning, and\ncooperative goals in embodied multi-agent settings. We will release all data,\ncode, and models publicly."}
{"id": "2505.13157", "pdf": "https://arxiv.org/pdf/2505.13157", "abs": "https://arxiv.org/abs/2505.13157", "authors": ["Yassine El Boudouri", "Walter Nuninger", "Julian Alvarez", "Yvan Peter"], "title": "Role-Playing Evaluation for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval"}
{"id": "2505.12879", "pdf": "https://arxiv.org/pdf/2505.12879", "abs": "https://arxiv.org/abs/2505.12879", "authors": ["Yeonsu Kim", "Junhan Lee", "John T. Hwang", "Bingran Wang", "Dongjin Lee"], "title": "Spline Dimensional Decomposition with Interpolation-based Optimal Knot Selection for Stochastic Dynamic Analysis", "categories": ["stat.ML", "cs.LG"], "comment": "28 pages, 15 figures", "summary": "Forward uncertainty quantification in dynamic systems is challenging due to\nnon-smooth or locally oscillating nonlinear behaviors. Spline dimensional\ndecomposition (SDD) effectively addresses such nonlinearity by partitioning\ninput coordinates via knot placement, yet its accuracy is highly sensitive to\nthe location of internal knots. Optimizing knots through sequential quadratic\nprogramming can be effective, yet the optimization process becomes\ncomputationally intense. We propose a computationally efficient,\ninterpolation-based method for optimal knot selection in SDD. The method\ninvolves three steps: (1) interpolating input-output profiles, (2) defining\nsubinterval-based reference regions, and (3) selecting optimal knot locations\nat maximum gradient points within each region. The resulting knot vector is\nthen applied to SDD for accurate approximation of non-smooth and locally\noscillating responses. A modal analysis of a lower control arm demonstrates\nthat SDD with the proposed knot selection achieves higher accuracy than SDD\nwith uniformly or randomly spaced knots, and also a Gaussian process surrogate\nmodel. The proposed SDD exhibits the lowest relative variance error (2.89%),\ncompared to SDD with uniformly spaced knots (12.310%), randomly spaced knots\n(15.274%), and Gaussian process (5.319%) in the first natural frequency\ndistribution. All surrogate models are constructed using the same 401\nsimulation datasets, and the relative errors are evaluated against a\n2000-sample Monte Carlo simulation. The scalability and applicability of\nproposed method are demonstrated through stochastic and reliability analyses of\nmathematical functions (N=1, 3) and a lower control arm system (N=10). The\nresults confirm that both second-moment statistics and reliability estimates\ncan be accurately achieved with only a few hundred function evaluations or\nfinite element simulations."}
{"id": "2505.13176", "pdf": "https://arxiv.org/pdf/2505.13176", "abs": "https://arxiv.org/abs/2505.13176", "authors": ["Zihao Cheng", "Hongru Wang", "Zeming Liu", "Yuhang Guo", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum."}
{"id": "2505.12896", "pdf": "https://arxiv.org/pdf/2505.12896", "abs": "https://arxiv.org/abs/2505.12896", "authors": ["Chenxi Liu", "Yongqiang Chen", "Tongliang Liu", "James Cheng", "Bo Han", "Kun Zhang"], "title": "On the Thinking-Language Modeling Gap in Large Language Models", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "Chenxi and Yongqiang contributed equally; project page:\n  https://causalcoat.github.io/lot.html", "summary": "System 2 reasoning is one of the defining characteristics of intelligence,\nwhich requires slow and logical thinking. Human conducts System 2 reasoning via\nthe language of thoughts that organizes the reasoning process as a causal\nsequence of mental language, or thoughts. Recently, it has been observed that\nSystem 2 reasoning can be elicited from Large Language Models (LLMs)\npre-trained on large-scale natural languages. However, in this work, we show\nthat there is a significant gap between the modeling of languages and thoughts.\nAs language is primarily a tool for humans to share knowledge and thinking,\nmodeling human language can easily absorb language biases into LLMs deviated\nfrom the chain of thoughts in minds. Furthermore, we show that the biases will\nmislead the eliciting of \"thoughts\" in LLMs to focus only on a biased part of\nthe premise. To this end, we propose a new prompt technique termed\nLanguage-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of\ndirectly eliciting the chain of thoughts from partial information, LoT\ninstructs LLMs to adjust the order and token used for the expressions of all\nthe relevant information. We show that the simple strategy significantly\nreduces the language modeling biases in LLMs and improves the performance of\nLLMs across a variety of reasoning tasks."}
{"id": "2505.13182", "pdf": "https://arxiv.org/pdf/2505.13182", "abs": "https://arxiv.org/abs/2505.13182", "authors": ["Jianfeng Xu"], "title": "Information Science Principles of Machine Learning: A Causal Chain Meta-Framework Based on Formalized Information Mapping", "categories": ["cs.LO", "cs.AI"], "comment": null, "summary": "[Objective] This study focuses on addressing the current lack of a unified\nformal theoretical framework in machine learning, as well as the deficiencies\nin interpretability and ethical safety assurance. [Methods] A formal\ninformation model is first constructed, utilizing sets of well-formed formulas\nto explicitly define the ontological states and carrier mappings of typical\ncomponents in machine learning. Learnable and processable predicates, along\nwith learning and processing functions, are introduced to analyze the logical\ndeduction and constraint rules of the causal chains within models. [Results] A\nmeta-framework for machine learning theory (MLT-MF) is established. Based on\nthis framework, universal definitions for model interpretability and ethical\nsafety are proposed. Furthermore, three key theorems are proved: the\nequivalence of model interpretability and information recoverability, the\nassurance of ethical safety, and the estimation of generalization error.\n[Limitations] The current framework assumes ideal conditions with noiseless\ninformation-enabling mappings and primarily targets model learning and\nprocessing logic in static scenarios. It does not yet address information\nfusion and conflict resolution across ontological spaces in multimodal or\nmulti-agent systems. [Conclusions] This work overcomes the limitations of\nfragmented research and provides a unified theoretical foundation for\nsystematically addressing the critical challenges currently faced in machine\nlearning."}
{"id": "2505.12902", "pdf": "https://arxiv.org/pdf/2505.12902", "abs": "https://arxiv.org/abs/2505.12902", "authors": ["Hao Fang", "Kai Huang", "Hao Ye", "Chongtao Guo", "Le Liang", "Xiao Li", "Shi Jin"], "title": "Power Allocation for Delay Optimization in Device-to-Device Networks: A Graph Reinforcement Learning Approach", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "The pursuit of rate maximization in wireless communication frequently\nencounters substantial challenges associated with user fairness. This paper\naddresses these challenges by exploring a novel power allocation approach for\ndelay optimization, utilizing graph neural networks (GNNs)-based reinforcement\nlearning (RL) in device-to-device (D2D) communication. The proposed approach\nincorporates not only channel state information but also factors such as packet\ndelay, the number of backlogged packets, and the number of transmitted packets\ninto the components of the state information. We adopt a centralized RL method,\nwhere a central controller collects and processes the state information. The\ncentral controller functions as an agent trained using the proximal policy\noptimization (PPO) algorithm. To better utilize topology information in the\ncommunication network and enhance the generalization of the proposed method, we\nembed GNN layers into both the actor and critic networks of the PPO algorithm.\nThis integration allows for efficient parameter updates of GNNs and enables the\nstate information to be parameterized as a low-dimensional embedding, which is\nleveraged by the agent to optimize power allocation strategies. Simulation\nresults demonstrate that the proposed method effectively reduces average delay\nwhile ensuring user fairness, outperforms baseline methods, and exhibits\nscalability and generalization capability."}
{"id": "2505.13188", "pdf": "https://arxiv.org/pdf/2505.13188", "abs": "https://arxiv.org/abs/2505.13188", "authors": ["Juntian Zhu", "Miguel de Carvalho", "Zhouwang Yang", "Fengxiang He"], "title": "When a Reinforcement Learning Agent Encounters Unknown Unknowns", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "An AI agent might surprisingly find she has reached an unknown state which\nshe has never been aware of -- an unknown unknown. We mathematically ground\nthis scenario in reinforcement learning: an agent, after taking an action\ncalculated from value functions $Q$ and $V$ defined on the {\\it {aware\ndomain}}, reaches a state out of the domain. To enable the agent to handle this\nscenario, we propose an {\\it episodic Markov decision {process} with growing\nawareness} (EMDP-GA) model, taking a new {\\it noninformative value expansion}\n(NIVE) approach to expand value functions to newly aware areas: when an agent\narrives at an unknown unknown, value functions $Q$ and $V$ whereon are\ninitialised by noninformative beliefs -- the averaged values on the aware\ndomain. This design is out of respect for the complete absence of knowledge in\nthe newly discovered state. The upper confidence bound momentum Q-learning is\nthen adapted to the growing awareness for training the EMDP-GA model. We prove\nthat (1) the regret of our approach is asymptotically consistent with the state\nof the art (SOTA) without exposure to unknown unknowns in an extremely\nuncertain environment, and (2) our computational complexity and space\ncomplexity are comparable with the SOTA -- these collectively suggest that\nthough an unknown unknown is surprising, it will be asymptotically properly\ndiscovered with decent speed and an affordable cost."}
{"id": "2505.12908", "pdf": "https://arxiv.org/pdf/2505.12908", "abs": "https://arxiv.org/abs/2505.12908", "authors": ["Xiao Wang", "Yu Jin", "Lan Chen", "Bo Jiang", "Lin Zhu", "Yonghong Tian", "Jin Tang", "Bin Luo"], "title": "Dynamic Graph Induced Contour-aware Heat Conduction Network for Event-based Object Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Event-based Vision Sensors (EVS) have demonstrated significant advantages\nover traditional RGB frame-based cameras in low-light conditions, high-speed\nmotion capture, and low latency. Consequently, object detection based on EVS\nhas attracted increasing attention from researchers. Current event stream\nobject detection algorithms are typically built upon Convolutional Neural\nNetworks (CNNs) or Transformers, which either capture limited local features\nusing convolutional filters or incur high computational costs due to the\nutilization of self-attention. Recently proposed vision heat conduction\nbackbone networks have shown a good balance between efficiency and accuracy;\nhowever, these models are not specifically designed for event stream data. They\nexhibit weak capability in modeling object contour information and fail to\nexploit the benefits of multi-scale features. To address these issues, this\npaper proposes a novel dynamic graph induced contour-aware heat conduction\nnetwork for event stream based object detection, termed CvHeat-DET. The\nproposed model effectively leverages the clear contour information inherent in\nevent streams to predict the thermal diffusivity coefficients within the heat\nconduction model, and integrates hierarchical structural graph features to\nenhance feature learning across multiple scales. Extensive experiments on three\nbenchmark datasets for event stream-based object detection fully validated the\neffectiveness of the proposed model. The source code of this paper will be\nreleased on https://github.com/Event-AHU/OpenEvDET."}
{"id": "2505.13191", "pdf": "https://arxiv.org/pdf/2505.13191", "abs": "https://arxiv.org/abs/2505.13191", "authors": ["Pengcheng Pan", "Yonekura Shogo", "Yasuo Kuniyoshi"], "title": "Emergence of Fixational and Saccadic Movements in a Multi-Level Recurrent Attention Model for Vision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Inspired by foveal vision, hard attention models promise interpretability and\nparameter economy. However, existing models like the Recurrent Model of Visual\nAttention (RAM) and Deep Recurrent Attention Model (DRAM) failed to model the\nhierarchy of human vision system, that compromise on the visual exploration\ndynamics. As a result, they tend to produce attention that are either overly\nfixational or excessively saccadic, diverging from human eye movement behavior.\nIn this paper, we propose a Multi-Level Recurrent Attention Model (MRAM), a\nnovel hard attention framework that explicitly models the neural hierarchy of\nhuman visual processing. By decoupling the function of glimpse location\ngeneration and task execution in two recurrent layers, MRAM emergent a balanced\nbehavior between fixation and saccadic movement. Our results show that MRAM not\nonly achieves more human-like attention dynamics, but also consistently\noutperforms CNN, RAM and DRAM baselines on standard image classification\nbenchmarks."}
{"id": "2505.12929", "pdf": "https://arxiv.org/pdf/2505.12929", "abs": "https://arxiv.org/abs/2505.12929", "authors": ["Zhihe Yang", "Xufang Luo", "Zilong Wang", "Dongqi Han", "Zhiyuan He", "Dongsheng Li", "Yunjian Xu"], "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 12 figures", "summary": "Reinforcement learning (RL) has become a cornerstone for enhancing the\nreasoning capabilities of large language models (LLMs), with recent innovations\nsuch as Group Relative Policy Optimization (GRPO) demonstrating exceptional\neffectiveness. In this study, we identify a critical yet underexplored issue in\nRL training: low-probability tokens disproportionately influence model updates\ndue to their large gradient magnitudes. This dominance hinders the effective\nlearning of high-probability tokens, whose gradients are essential for LLMs'\nperformance but are substantially suppressed. To mitigate this interference, we\npropose two novel methods: Advantage Reweighting and Low-Probability Token\nIsolation (Lopti), both of which effectively attenuate gradients from\nlow-probability tokens while emphasizing parameter updates driven by\nhigh-probability tokens. Our approaches promote balanced updates across tokens\nwith varying probabilities, thereby enhancing the efficiency of RL training.\nExperimental results demonstrate that they substantially improve the\nperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K\nLogic Puzzle reasoning tasks. Our implementation is available at\nhttps://github.com/zhyang2226/AR-Lopti."}
{"id": "2505.13192", "pdf": "https://arxiv.org/pdf/2505.13192", "abs": "https://arxiv.org/abs/2505.13192", "authors": ["Christoph Jürgen Hemmer", "Daniel Durstewitz"], "title": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics", "categories": ["cs.LG", "cs.AI", "math.DS", "nlin.CD"], "comment": null, "summary": "Complex, temporally evolving phenomena, from climate to brain activity, are\ngoverned by dynamical systems (DS). DS reconstruction (DSR) seeks to infer\ngenerative surrogate models of these from observed data, reproducing their\nlong-term behavior. Existing DSR approaches require purpose-training for any\nnew system observed, lacking the zero-shot and in-context inference\ncapabilities known from LLMs. Here we introduce DynaMix, a novel multivariate\nALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR\nmodel able to generalize zero-shot to out-of-domain DS. Just from a provided\ncontext signal, without any re-training, DynaMix faithfully forecasts the\nlong-term evolution of novel DS where existing time series (TS) foundation\nmodels, like Chronos, fail -- at a fraction of the number of parameters and\norders of magnitude faster inference times. DynaMix outperforms TS foundation\nmodels in terms of long-term statistics, and often also short-term forecasts,\neven on real-world time series, like traffic or weather data, typically used\nfor training and evaluating TS models, but not at all part of DynaMix' training\ncorpus. We illustrate some of the failure modes of TS models for DSR problems,\nand conclude that models built on DS principles may bear a huge potential also\nfor advancing the TS prediction field."}
{"id": "2505.12942", "pdf": "https://arxiv.org/pdf/2505.12942", "abs": "https://arxiv.org/abs/2505.12942", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Xinye Cao", "Pedro Gimenes", "George A. Constantinides", "Wayne Luk", "Yiren Zhao"], "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."}
{"id": "2505.13196", "pdf": "https://arxiv.org/pdf/2505.13196", "abs": "https://arxiv.org/abs/2505.13196", "authors": ["Pranav Vaidhyanathan", "Lucas Schorling", "Natalia Ares", "Michael A. Osborne"], "title": "A Physics-Inspired Optimizer: Velocity Regularized Adam", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": "L. Schorling and P. Vaidhyanathan contributed equally to this work.\n  20 pages, 13 figures", "summary": "We introduce Velocity-Regularized Adam (VRAdam), a physics-inspired optimizer\nfor training deep neural networks that draws on ideas from quartic terms for\nkinetic energy with its stabilizing effects on various system dynamics.\nPrevious algorithms, including the ubiquitous Adam, operate at the so called\nadaptive edge of stability regime during training leading to rapid oscillations\nand slowed convergence of loss. However, VRAdam adds a higher order penalty on\nthe learning rate based on the velocity such that the algorithm automatically\nslows down whenever weight updates become large. In practice, we observe that\nthe effective dynamic learning rate shrinks in high-velocity regimes, damping\noscillations and allowing for a more aggressive base step size when necessary\nwithout divergence. By combining this velocity-based regularizer for global\ndamping with per-parameter scaling of Adam to create a hybrid optimizer, we\ndemonstrate that VRAdam consistently exceeds the performance against standard\noptimizers including AdamW. We benchmark various tasks such as image\nclassification, language modeling, image generation and generative modeling\nusing diverse architectures and training methodologies including Convolutional\nNeural Networks (CNNs), Transformers, and GFlowNets."}
{"id": "2505.13012", "pdf": "https://arxiv.org/pdf/2505.13012", "abs": "https://arxiv.org/abs/2505.13012", "authors": ["Anthony Bardou", "Patrick Thiran"], "title": "Asymptotic Performance of Time-Varying Bayesian Optimization", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Time-Varying Bayesian Optimization (TVBO) is the go-to framework for\noptimizing a time-varying black-box objective function that may be noisy and\nexpensive to evaluate. Is it possible for the instantaneous regret of a TVBO\nalgorithm to vanish asymptotically, and if so, when? We answer this question of\ngreat theoretical importance by providing algorithm-independent lower regret\nbounds and upper regret bounds for TVBO algorithms, from which we derive\nsufficient conditions for a TVBO algorithm to have the no-regret property. Our\nanalysis covers all major classes of stationary kernel functions."}
{"id": "2505.13201", "pdf": "https://arxiv.org/pdf/2505.13201", "abs": "https://arxiv.org/abs/2505.13201", "authors": ["Yuzhen Chen", "Hojun Son", "Arpan Kusari"], "title": "MatPredict: a dataset and benchmark for learning material properties of diverse indoor objects", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Determining material properties from camera images can expand the ability to\nidentify complex objects in indoor environments, which is valuable for consumer\nrobotics applications. To support this, we introduce MatPredict, a dataset that\ncombines the high-quality synthetic objects from Replica dataset with MatSynth\ndataset's material properties classes - to create objects with diverse material\nproperties. We select 3D meshes of specific foreground objects and render them\nwith different material properties. In total, we generate \\textbf{18} commonly\noccurring objects with \\textbf{14} different materials. We showcase how we\nprovide variability in terms of lighting and camera placement for these\nobjects. Next, we provide a benchmark for inferring material properties from\nvisual images using these perturbed models in the scene, discussing the\nspecific neural network models involved and their performance based on\ndifferent image comparison metrics. By accurately simulating light interactions\nwith different materials, we can enhance realism, which is crucial for training\nmodels effectively through large-scale simulations. This research aims to\nrevolutionize perception in consumer robotics. The dataset is provided\n\\href{https://huggingface.co/datasets/UMTRI/MatPredict}{here} and the code is\nprovided \\href{https://github.com/arpan-kusari/MatPredict}{here}."}
{"id": "2505.13021", "pdf": "https://arxiv.org/pdf/2505.13021", "abs": "https://arxiv.org/abs/2505.13021", "authors": ["Federico Del Pup", "Andrea Zanola", "Louis Fabrice Tshimanga", "Alessandra Bertoldo", "Livio Finos", "Manfredo Atzori"], "title": "The role of data partitioning on the performance of EEG-based deep learning models in supervised cross-subject analysis: a preliminary study", "categories": ["eess.SP", "cs.LG"], "comment": "Submitted for possible publication. GitHub repository: see\n  https://github.com/MedMaxLab/eegpartition", "summary": "Deep learning is significantly advancing the analysis of\nelectroencephalography (EEG) data by effectively discovering highly nonlinear\npatterns within the signals. Data partitioning and cross-validation are crucial\nfor assessing model performance and ensuring study comparability, as they can\nproduce varied results and data leakage due to specific signal properties\n(e.g., biometric). Such variability leads to incomparable studies and,\nincreasingly, overestimated performance claims, which are detrimental to the\nfield. Nevertheless, no comprehensive guidelines for proper data partitioning\nand cross-validation exist in the domain, nor is there a quantitative\nevaluation of their impact on model accuracy, reliability, and\ngeneralizability. To assist researchers in identifying optimal experimental\nstrategies, this paper thoroughly investigates the role of data partitioning\nand cross-validation in evaluating EEG deep learning models. Five\ncross-validation settings are compared across three supervised cross-subject\nclassification tasks (BCI, Parkinson's, and Alzheimer's disease detection) and\nfour established architectures of increasing complexity (ShallowConvNet,\nEEGNet, DeepConvNet, and Temporal-based ResNet). The comparison of over 100,000\ntrained models underscores, first, the importance of using subject-based\ncross-validation strategies for evaluating EEG deep learning models, except\nwhen within-subject analyses are acceptable (e.g., BCI). Second, it highlights\nthe greater reliability of nested approaches (N-LNSO) compared to non-nested\ncounterparts, which are prone to data leakage and favor larger models\noverfitting to validation data. In conclusion, this work provides EEG deep\nlearning researchers with an analysis of data partitioning and cross-validation\nand offers guidelines to avoid data leakage, currently undermining the domain\nwith potentially overestimated performance claims."}
{"id": "2505.13208", "pdf": "https://arxiv.org/pdf/2505.13208", "abs": "https://arxiv.org/abs/2505.13208", "authors": ["Colin Krawchuk", "Nikhil Khatri", "Neil John Ortega", "Dimitri Kartsaklis"], "title": "Efficient Generation of Parameterised Quantum Circuits from Large Texts", "categories": ["quant-ph", "cs.AI", "cs.CL"], "comment": null, "summary": "Quantum approaches to natural language processing (NLP) are redefining how\nlinguistic information is represented and processed. While traditional hybrid\nquantum-classical models rely heavily on classical neural networks, recent\nadvancements propose a novel framework, DisCoCirc, capable of directly encoding\nentire documents as parameterised quantum circuits (PQCs), besides enjoying\nsome additional interpretability and compositionality benefits. Following these\nideas, this paper introduces an efficient methodology for converting\nlarge-scale texts into quantum circuits using tree-like representations of\npregroup diagrams. Exploiting the compositional parallels between language and\nquantum mechanics, grounded in symmetric monoidal categories, our approach\nenables faithful and efficient encoding of syntactic and discourse\nrelationships in long and complex texts (up to 6410 words in our experiments)\nto quantum circuits. The developed system is provided to the community as part\nof the augmented open-source quantum NLP package lambeq Gen II."}
{"id": "2505.13052", "pdf": "https://arxiv.org/pdf/2505.13052", "abs": "https://arxiv.org/abs/2505.13052", "authors": ["Tuan Thai", "TrungTin Nguyen", "Dat Do", "Nhat Ho", "Christopher Drovandi"], "title": "Model Selection for Gaussian-gated Gaussian Mixture of Experts Using Dendrograms of Mixing Measures", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO", "stat.ME", "stat.TH"], "comment": null, "summary": "Mixture of Experts (MoE) models constitute a widely utilized class of\nensemble learning approaches in statistics and machine learning, known for\ntheir flexibility and computational efficiency. They have become integral\ncomponents in numerous state-of-the-art deep neural network architectures,\nparticularly for analyzing heterogeneous data across diverse domains. Despite\ntheir practical success, the theoretical understanding of model selection,\nespecially concerning the optimal number of mixture components or experts,\nremains limited and poses significant challenges. These challenges primarily\nstem from the inclusion of covariates in both the Gaussian gating functions and\nexpert networks, which introduces intrinsic interactions governed by partial\ndifferential equations with respect to their parameters. In this paper, we\nrevisit the concept of dendrograms of mixing measures and introduce a novel\nextension to Gaussian-gated Gaussian MoE models that enables consistent\nestimation of the true number of mixture components and achieves the pointwise\noptimal convergence rate for parameter estimation in overfitted scenarios.\nNotably, this approach circumvents the need to train and compare a range of\nmodels with varying numbers of components, thereby alleviating the\ncomputational burden, particularly in high-dimensional or deep neural network\nsettings. Experimental results on synthetic data demonstrate the effectiveness\nof the proposed method in accurately recovering the number of experts. It\noutperforms common criteria such as the Akaike information criterion, the\nBayesian information criterion, and the integrated completed likelihood, while\nachieving optimal convergence rates for parameter estimation and accurately\napproximating the regression function."}
{"id": "2505.13210", "pdf": "https://arxiv.org/pdf/2505.13210", "abs": "https://arxiv.org/abs/2505.13210", "authors": ["Xiaocong Du", "Haoyu Pei", "Haipeng Zhang"], "title": "Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation."}
{"id": "2505.13055", "pdf": "https://arxiv.org/pdf/2505.13055", "abs": "https://arxiv.org/abs/2505.13055", "authors": ["Jonathan Ott", "Maximilian Stahlke", "Tobias Feigl", "Bjoern M. Eskofier", "Christopher Mutschler"], "title": "Simplicity is Key: An Unsupervised Pretraining Approach for Sparse Radio Channels", "categories": ["eess.SP", "cs.LG"], "comment": "8 pages, 1 figure", "summary": "We introduce the Sparse pretrained Radio Transformer (SpaRTran), an\nunsupervised representation learning approach based on the concept of\ncompressed sensing for radio channels. Our approach learns embeddings that\nfocus on the physical properties of radio propagation, to create the optimal\nbasis for fine-tuning on radio-based downstream tasks. SpaRTran uses a sparse\ngated autoencoder that induces a simplicity bias to the learned\nrepresentations, resembling the sparse nature of radio propagation. For signal\nreconstruction, it learns a dictionary that holds atomic features, which\nincreases flexibility across signal waveforms and spatiotemporal signal\npatterns. Our experiments show that SpaRTran reduces errors by up to 85 %\ncompared to state-of-the-art methods when fine-tuned on radio fingerprinting, a\nchallenging downstream task. In addition, our method requires less pretraining\neffort and offers greater flexibility, as we train it solely on individual\nradio signals. SpaRTran serves as an excellent base model that can be\nfine-tuned for various radio-based downstream tasks, effectively reducing the\ncost for labeling. In addition, it is significantly more versatile than\nexisting methods and demonstrates superior generalization."}
{"id": "2505.13211", "pdf": "https://arxiv.org/pdf/2505.13211", "abs": "https://arxiv.org/abs/2505.13211", "authors": ["Sand. ai", "Hansi Teng", "Hongyu Jia", "Lei Sun", "Lingzhi Li", "Maolin Li", "Mingqiu Tang", "Shuai Han", "Tianning Zhang", "W. Q. Zhang", "Weifeng Luo", "Xiaoyang Kang", "Yuchen Sun", "Yue Cao", "Yunpeng Huang", "Yutong Lin", "Yuxin Fang", "Zewei Tao", "Zheng Zhang", "Zhongshu Wang", "Zixun Liu", "Dai Shi", "Guoli Su", "Hanwen Sun", "Hong Pan", "Jie Wang", "Jiexin Sheng", "Min Cui", "Min Hu", "Ming Yan", "Shucheng Yin", "Siran Zhang", "Tingting Liu", "Xianping Yin", "Xiaoyu Yang", "Xin Song", "Xuan Hu", "Yankai Zhang", "Yuqiao Li"], "title": "MAGI-1: Autoregressive Video Generation at Scale", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present MAGI-1, a world model that generates videos by autoregressively\npredicting a sequence of video chunks, defined as fixed-length segments of\nconsecutive frames. Trained to denoise per-chunk noise that increases\nmonotonically over time, MAGI-1 enables causal temporal modeling and naturally\nsupports streaming generation. It achieves strong performance on image-to-video\n(I2V) tasks conditioned on text instructions, providing high temporal\nconsistency and scalability, which are made possible by several algorithmic\ninnovations and a dedicated infrastructure stack. MAGI-1 facilitates\ncontrollable generation via chunk-wise prompting and supports real-time,\nmemory-efficient deployment by maintaining constant peak inference cost,\nregardless of video length. The largest variant of MAGI-1 comprises 24 billion\nparameters and supports context lengths of up to 4 million tokens,\ndemonstrating the scalability and robustness of our approach. The code and\nmodels are available at https://github.com/SandAI-org/MAGI-1 and\nhttps://github.com/SandAI-org/MagiAttention. The product can be accessed at\nhttps://sand.ai."}
{"id": "2505.13077", "pdf": "https://arxiv.org/pdf/2505.13077", "abs": "https://arxiv.org/abs/2505.13077", "authors": ["Xiang Fei", "Jinghui Lu", "Qi Sun", "Hao Feng", "Yanjie Wang", "Wei Shi", "An-Lan Wang", "Jingqun Tang", "Can Huang"], "title": "Advancing Sequential Numerical Prediction in Autoregressive Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL."}
{"id": "2505.13253", "pdf": "https://arxiv.org/pdf/2505.13253", "abs": "https://arxiv.org/abs/2505.13253", "authors": ["Lennart Röstel", "Dominik Winkelbauer", "Johannes Pitz", "Leon Sievers", "Berthold Bäuml"], "title": "Composing Dextrous Grasping and In-hand Manipulation via Scoring with a Reinforcement Learning Critic", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "In-hand manipulation and grasping are fundamental yet often separately\naddressed tasks in robotics. For deriving in-hand manipulation policies,\nreinforcement learning has recently shown great success. However, the derived\ncontrollers are not yet useful in real-world scenarios because they often\nrequire a human operator to place the objects in suitable initial (grasping)\nstates. Finding stable grasps that also promote the desired in-hand\nmanipulation goal is an open problem. In this work, we propose a method for\nbridging this gap by leveraging the critic network of a reinforcement learning\nagent trained for in-hand manipulation to score and select initial grasps. Our\nexperiments show that this method significantly increases the success rate of\nin-hand manipulation without requiring additional training. We also present an\nimplementation of a full grasp manipulation pipeline on a real-world system,\nenabling autonomous grasping and reorientation even of unwieldy objects."}
{"id": "2505.13085", "pdf": "https://arxiv.org/pdf/2505.13085", "abs": "https://arxiv.org/abs/2505.13085", "authors": ["Biel Tura Vecino", "Subhadeep Maji", "Aravind Varier", "Antonio Bonafonte", "Ivan Valles", "Michael Owen", "Leif Radel", "Grant Strimmel", "Seyi Feyisetan", "Roberto Barra Chicote", "Ariya Rastrow", "Constantinos Papayiannis", "Volker Leutnant", "Trevor Wood"], "title": "Universal Semantic Disentangled Privacy-preserving Speech Representation Learning", "categories": ["eess.AS", "cs.LG"], "comment": "Accepted at Interspeech 2025", "summary": "The use of audio recordings of human speech to train LLMs poses privacy\nconcerns due to these models' potential to generate outputs that closely\nresemble artifacts in the training data. In this study, we propose a speaker\nprivacy-preserving representation learning method through the Universal Speech\nCodec (USC), a computationally efficient encoder-decoder model that\ndisentangles speech into: $\\textit{(i)}$ privacy-preserving semantically rich\nrepresentations, capturing content and speech paralinguistics, and\n$\\textit{(ii)}$ residual acoustic and speaker representations that enables\nhigh-fidelity reconstruction. Extensive evaluations presented show that USC's\nsemantic representation preserves content, prosody, and sentiment, while\nremoving potentially identifiable speaker attributes. Combining both\nrepresentations, USC achieves state-of-the-art speech reconstruction.\nAdditionally, we introduce an evaluation methodology for measuring\nprivacy-preserving properties, aligning with perceptual tests. We compare USC\nagainst other codecs in the literature and demonstrate its effectiveness on\nprivacy-preserving representation learning, illustrating the trade-offs of\nspeaker anonymization, paralinguistics retention and content preservation in\nthe learned semantic representations. Audio samples are shared in\n$\\href{https://www.amazon.science/usc-samples}{https://www.amazon.science/usc-samples}$."}
{"id": "2505.13257", "pdf": "https://arxiv.org/pdf/2505.13257", "abs": "https://arxiv.org/abs/2505.13257", "authors": ["Zilu Tang", "Afra Feyza Akyürek", "Ekin Akyürek", "Derry Wijaya"], "title": "WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, preprint", "summary": "Preference alignment has become a standard pipeline in finetuning models to\nfollow \\emph{generic} human preferences. Majority of work seeks to optimize\nmodel to produce responses that would be preferable \\emph{on average},\nsimplifying the diverse and often \\emph{contradicting} space of human\npreferences. While research has increasingly focused on personalized alignment:\nadapting models to individual user preferences, there is a lack of personalized\npreference dataset which focus on nuanced individual-level preferences. To\naddress this, we introduce WikiPersona: the first fine-grained personalization\nusing well-documented, famous individuals. Our dataset challenges models to\nalign with these personas through an interpretable process: generating\nverifiable textual descriptions of a persona's background and preferences in\naddition to alignment. We systematically evaluate different personalization\napproaches and find that as few-shot prompting with preferences and fine-tuning\nfail to simultaneously ensure effectiveness and efficiency, using\n\\textit{inferred personal preferences} as prefixes enables effective\npersonalization, especially in topics where preferences clash while leading to\nmore equitable generalization across unseen personas."}
{"id": "2505.13088", "pdf": "https://arxiv.org/pdf/2505.13088", "abs": "https://arxiv.org/abs/2505.13088", "authors": ["Zhaoyi Wang", "Shengyu Huang", "Jemil Avers Butt", "Yuanzhou Cai", "Matej Varga", "Andreas Wieser"], "title": "Cross-modal feature fusion for robust point cloud registration with ambiguous geometry", "categories": ["cs.CV", "cs.LG"], "comment": "To appear in the ISPRS Journal of Photogrammetry and Remote Sensing.\n  19 pages, 14 figures", "summary": "Point cloud registration has seen significant advancements with the\napplication of deep learning techniques. However, existing approaches often\noverlook the potential of integrating radiometric information from RGB images.\nThis limitation reduces their effectiveness in aligning point clouds pairs,\nespecially in regions where geometric data alone is insufficient. When used\neffectively, radiometric information can enhance the registration process by\nproviding context that is missing from purely geometric data. In this paper, we\npropose CoFF, a novel Cross-modal Feature Fusion method that utilizes both\npoint cloud geometry and RGB images for pairwise point cloud registration.\nAssuming that the co-registration between point clouds and RGB images is\navailable, CoFF explicitly addresses the challenges where geometric information\nalone is unclear, such as in regions with symmetric similarity or planar\nstructures, through a two-stage fusion of 3D point cloud features and 2D image\nfeatures. It incorporates a cross-modal feature fusion module that assigns\npixel-wise image features to 3D input point clouds to enhance learned 3D point\nfeatures, and integrates patch-wise image features with superpoint features to\nimprove the quality of coarse matching. This is followed by a coarse-to-fine\nmatching module that accurately establishes correspondences using the fused\nfeatures. We extensively evaluate CoFF on four common datasets: 3DMatch,\n3DLoMatch, IndoorLRS, and the recently released ScanNet++ datasets. In\naddition, we assess CoFF on specific subset datasets containing geometrically\nambiguous cases. Our experimental results demonstrate that CoFF achieves\nstate-of-the-art registration performance across all benchmarks, including\nremarkable registration recalls of 95.9% and 81.6% on the widely-used 3DMatch\nand 3DLoMatch datasets, respectively...(Truncated to fit arXiv abstract length)"}
{"id": "2505.13264", "pdf": "https://arxiv.org/pdf/2505.13264", "abs": "https://arxiv.org/abs/2505.13264", "authors": ["Carlos Rodriguez-Pardo", "Louis Daumas", "Leonardo Chiani", "Massimo Tavoni"], "title": "Net-Zero: A Comparative Study on Neural Network Design for Climate-Economic PDEs Under Uncertainty", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.PF", "math.AP", "68T07 (Primary) 35Q91, 91B76 (Secondary)", "I.2.1; I.5.1; J.4"], "comment": "Under review", "summary": "Climate-economic modeling under uncertainty presents significant\ncomputational challenges that may limit policymakers' ability to address\nclimate change effectively. This paper explores neural network-based approaches\nfor solving high-dimensional optimal control problems arising from models that\nincorporate ambiguity aversion in climate mitigation decisions. We develop a\ncontinuous-time endogenous-growth economic model that accounts for multiple\nmitigation pathways, including emission-free capital and carbon intensity\nreductions. Given the inherent complexity and high dimensionality of these\nmodels, traditional numerical methods become computationally intractable. We\nbenchmark several neural network architectures against finite-difference\ngenerated solutions, evaluating their ability to capture the dynamic\ninteractions between uncertainty, technology transitions, and optimal climate\npolicy. Our findings demonstrate that appropriate neural architecture selection\nsignificantly impacts both solution accuracy and computational efficiency when\nmodeling climate-economic systems under uncertainty. These methodological\nadvances enable more sophisticated modeling of climate policy decisions,\nallowing for better representation of technology transitions and\nuncertainty-critical elements for developing effective mitigation strategies in\nthe face of climate change."}
{"id": "2505.13112", "pdf": "https://arxiv.org/pdf/2505.13112", "abs": "https://arxiv.org/abs/2505.13112", "authors": ["Rodrigo Maulen-Soto", "Claire Boyer", "Pierre Marion"], "title": "Attention-based clustering", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Transformers have emerged as a powerful neural network architecture capable\nof tackling a wide range of learning tasks. In this work, we provide a\ntheoretical analysis of their ability to automatically extract structure from\ndata in an unsupervised setting. In particular, we demonstrate their\nsuitability for clustering when the input data is generated from a Gaussian\nmixture model. To this end, we study a simplified two-head attention layer and\ndefine a population risk whose minimization with unlabeled data drives the head\nparameters to align with the true mixture centroids."}
{"id": "2505.13268", "pdf": "https://arxiv.org/pdf/2505.13268", "abs": "https://arxiv.org/abs/2505.13268", "authors": ["Livia Qian", "Carol Figueroa", "Gabriel Skantze"], "title": "Representation of perceived prosodic similarity of conversational feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Interspeech 2025", "summary": "Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of\nspoken dialogue and is crucial to ensuring common ground in conversational\nsystems. The exact meaning of such feedback is conveyed through both lexical\nand prosodic form. In this work, we investigate the perceived prosodic\nsimilarity of vocal feedback with the same lexical form, and to what extent\nexisting speech representations reflect such similarities. A triadic comparison\ntask with recruited participants is used to measure perceived similarity of\nfeedback responses taken from two different datasets. We find that spectral and\nself-supervised speech representations encode prosody better than extracted\npitch features, especially in the case of feedback from the same speaker. We\nalso find that it is possible to further condense and align the representations\nto human perception through contrastive learning."}
{"id": "2505.13115", "pdf": "https://arxiv.org/pdf/2505.13115", "abs": "https://arxiv.org/abs/2505.13115", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands", "summary": "The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications."}
{"id": "2505.13280", "pdf": "https://arxiv.org/pdf/2505.13280", "abs": "https://arxiv.org/abs/2505.13280", "authors": ["Elias Collaert", "Abel Rodríguez", "Sander Joos", "Lieven Desmet", "Vera Rimmer"], "title": "FlowPure: Continuous Normalizing Flows for Adversarial Purification", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Despite significant advancements in the area, adversarial robustness remains\na critical challenge in systems employing machine learning models. The removal\nof adversarial perturbations at inference time, known as adversarial\npurification, has emerged as a promising defense strategy. To achieve this,\nstate-of-the-art methods leverage diffusion models that inject Gaussian noise\nduring a forward process to dilute adversarial perturbations, followed by a\ndenoising step to restore clean samples before classification. In this work, we\npropose FlowPure, a novel purification method based on Continuous Normalizing\nFlows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings\nfrom adversarial examples to their clean counterparts. Unlike prior\ndiffusion-based approaches that rely on fixed noise processes, FlowPure can\nleverage specific attack knowledge to improve robustness under known threats,\nwhile also supporting a more general stochastic variant trained on Gaussian\nperturbations for settings where such knowledge is unavailable. Experiments on\nCIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art\npurification-based defenses in preprocessor-blind and white-box scenarios, and\ncan do so while fully preserving benign accuracy in the former. Moreover, our\nresults show that not only is FlowPure a highly effective purifier but it also\nholds a strong potential for adversarial detection, identifying\npreprocessor-blind PGD samples with near-perfect accuracy."}
{"id": "2505.13118", "pdf": "https://arxiv.org/pdf/2505.13118", "abs": "https://arxiv.org/abs/2505.13118", "authors": ["Marouane Il Idrissi", "Agathe Fernandes Machado", "Ewen Gallic", "Arthur Charpentier"], "title": "Unveil Sources of Uncertainty: Feature Contribution to Conformal Prediction Intervals", "categories": ["cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "Cooperative game theory methods, notably Shapley values, have significantly\nenhanced machine learning (ML) interpretability. However, existing explainable\nAI (XAI) frameworks mainly attribute average model predictions, overlooking\npredictive uncertainty. This work addresses that gap by proposing a novel,\nmodel-agnostic uncertainty attribution (UA) method grounded in conformal\nprediction (CP). By defining cooperative games where CP interval\nproperties-such as width and bounds-serve as value functions, we systematically\nattribute predictive uncertainty to input features. Extending beyond the\ntraditional Shapley values, we use the richer class of Harsanyi allocations,\nand in particular the proportional Shapley values, which distribute attribution\nproportionally to feature importance. We propose a Monte Carlo approximation\nmethod with robust statistical guarantees to address computational feasibility,\nsignificantly improving runtime efficiency. Our comprehensive experiments on\nsynthetic benchmarks and real-world datasets demonstrate the practical utility\nand interpretative depth of our approach. By combining cooperative game theory\nand conformal prediction, we offer a rigorous, flexible toolkit for\nunderstanding and communicating predictive uncertainty in high-stakes ML\napplications."}
{"id": "2505.13291", "pdf": "https://arxiv.org/pdf/2505.13291", "abs": "https://arxiv.org/abs/2505.13291", "authors": ["Yifu Cai", "Xinyu Li", "Mononito Goswami", "Michał Wiliński", "Gus Welter", "Artur Dubrawski"], "title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents", "categories": ["cs.LG", "cs.AI"], "comment": "Open source code available at\n  https://github.com/moment-timeseries-foundation-model/TimeSeriesGym. YC, XL,\n  MG and MW contributed equally, and should be considered joint first authors", "summary": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating\nArtificial Intelligence (AI) agents on time series machine learning engineering\nchallenges. Existing benchmarks lack scalability, focus narrowly on model\nbuilding in well-defined settings, and evaluate only a limited set of research\nartifacts (e.g., CSV submission files). To make AI agent benchmarking more\nrelevant to the practice of machine learning engineering, our framework scales\nalong two critical dimensions. First, recognizing that effective ML engineering\nrequires a range of diverse skills, TimeSeriesGym incorporates challenges from\ndiverse sources spanning multiple domains and tasks. We design challenges to\nevaluate both isolated capabilities (including data handling, understanding\nresearch repositories, and code translation) and their combinations, and rather\nthan addressing each challenge independently, we develop tools that support\ndesigning multiple challenges at scale. Second, we implement evaluation\nmechanisms for multiple research artifacts, including submission files, code,\nand models, using both precise numeric measures and more flexible LLM-based\nevaluation approaches. This dual strategy balances objective assessment with\ncontextual judgment. Although our initial focus is on time series applications,\nour framework can be readily extended to other data modalities, broadly\nenhancing the comprehensiveness and practical utility of agentic AI evaluation.\nWe open-source our benchmarking framework to facilitate future research on the\nML engineering capabilities of AI agents."}
{"id": "2505.13123", "pdf": "https://arxiv.org/pdf/2505.13123", "abs": "https://arxiv.org/abs/2505.13123", "authors": ["Snehashis Majhi", "Giacomo D'Amicantonio", "Antitza Dantcheva", "Quan Kong", "Lorenzo Garattoni", "Gianpiero Francesca", "Egor Bondarev", "Francois Bremond"], "title": "Just Dance with $π$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Weakly-supervised methods for video anomaly detection (VAD) are\nconventionally based merely on RGB spatio-temporal features, which continues to\nlimit their reliability in real-world scenarios. This is due to the fact that\nRGB-features are not sufficiently distinctive in setting apart categories such\nas shoplifting from visually similar events. Therefore, towards robust complex\nreal-world VAD, it is essential to augment RGB spatio-temporal features by\nadditional modalities. Motivated by this, we introduce the Poly-modal Induced\nframework for VAD: \"PI-VAD\", a novel approach that augments RGB representations\nby five additional modalities. Specifically, the modalities include sensitivity\nto fine-grained motion (Pose), three dimensional scene and entity\nrepresentation (Depth), surrounding objects (Panoptic masks), global motion\n(optical flow), as well as language cues (VLM). Each modality represents an\naxis of a polygon, streamlined to add salient cues to RGB. PI-VAD includes two\nplug-in modules, namely Pseudo-modality Generation module and Cross Modal\nInduction module, which generate modality-specific prototypical representation\nand, thereby, induce multi-modal information into RGB cues. These modules\noperate by performing anomaly-aware auxiliary tasks and necessitate five\nmodality backbones -- only during training. Notably, PI-VAD achieves\nstate-of-the-art accuracy on three prominent VAD datasets encompassing\nreal-world scenarios, without requiring the computational overhead of five\nmodality backbones at inference."}
{"id": "2505.13292", "pdf": "https://arxiv.org/pdf/2505.13292", "abs": "https://arxiv.org/abs/2505.13292", "authors": ["Huaiying Luo", "Cheng Ji"], "title": "Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms of AI Systems by Integrating Federated Learning and LLMs", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted by 2025 IEEE 7th International Conference on Communications,\n  Information System and Computer Engineering", "summary": "In the age of cloud computing, data privacy protection has become a major\nchallenge, especially when sharing sensitive data across cloud environments.\nHowever, how to optimize collaboration across cloud environments remains an\nunresolved problem. In this paper, we combine federated learning with\nlarge-scale language models to optimize the collaborative mechanism of AI\nsystems. Based on the existing federated learning framework, we introduce a\ncross-cloud architecture in which federated learning works by aggregating model\nupdates from decentralized nodes without exposing the original data. At the\nsame time, combined with large-scale language models, its powerful context and\nsemantic understanding capabilities are used to improve model training\nefficiency and decision-making ability. We've further innovated by introducing\na secure communication layer to ensure the privacy and integrity of model\nupdates and training data. The model enables continuous model adaptation and\nfine-tuning across different cloud environments while protecting sensitive\ndata. Experimental results show that the proposed method is significantly\nbetter than the traditional federated learning model in terms of accuracy,\nconvergence speed and data privacy protection."}
{"id": "2505.13136", "pdf": "https://arxiv.org/pdf/2505.13136", "abs": "https://arxiv.org/abs/2505.13136", "authors": ["Anton Ehrmanntraut", "Julia Wunderle", "Jan Pfister", "Fotis Jannidis", "Andreas Hotho"], "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review @ARR", "summary": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models."}
{"id": "2505.13307", "pdf": "https://arxiv.org/pdf/2505.13307", "abs": "https://arxiv.org/abs/2505.13307", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Yue Liao", "Jiaqi Wang", "Jingxuan Zhou", "Wanxiang Che"], "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Manuscript", "summary": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary."}
{"id": "2505.13186", "pdf": "https://arxiv.org/pdf/2505.13186", "abs": "https://arxiv.org/abs/2505.13186", "authors": ["Philipp Scholl", "Alexander Dietrich", "Sebastian Wolf", "Jinoh Lee", "Alin-Albu Schäffer", "Gitta Kutyniok", "Maged Iskandar"], "title": "Interpretable Robotic Friction Learning via Symbolic Regression", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Accurately modeling the friction torque in robotic joints has long been\nchallenging due to the request for a robust mathematical description.\nTraditional model-based approaches are often labor-intensive, requiring\nextensive experiments and expert knowledge, and they are difficult to adapt to\nnew scenarios and dependencies. On the other hand, data-driven methods based on\nneural networks are easier to implement but often lack robustness,\ninterpretability, and trustworthiness--key considerations for robotic hardware\nand safety-critical applications such as human-robot interaction. To address\nthe limitations of both approaches, we propose the use of symbolic regression\n(SR) to estimate the friction torque. SR generates interpretable symbolic\nformulas similar to those produced by model-based methods while being flexible\nto accommodate various dynamic effects and dependencies. In this work, we apply\nSR algorithms to approximate the friction torque using collected data from a\nKUKA LWR-IV+ robot. Our results show that SR not only yields formulas with\ncomparable complexity to model-based approaches but also achieves higher\naccuracy. Moreover, SR-derived formulas can be seamlessly extended to include\nload dependencies and other dynamic factors."}
{"id": "2505.13308", "pdf": "https://arxiv.org/pdf/2505.13308", "abs": "https://arxiv.org/abs/2505.13308", "authors": ["Hengli Li", "Chenxi Li", "Tong Wu", "Xuekai Zhu", "Yuxuan Wang", "Zhaoxin Yu", "Eric Hanchen Jiang", "Song-Chun Zhu", "Zixia Jia", "Ying Nian Wu", "Zilong Zheng"], "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs."}
{"id": "2505.13189", "pdf": "https://arxiv.org/pdf/2505.13189", "abs": "https://arxiv.org/abs/2505.13189", "authors": ["Giacomo Greco"], "title": "A Malliavin-Gamma calculus approach to Score Based Diffusion Generative models for random fields", "categories": ["math.PR", "cs.LG", "stat.ML", "62D05, 60G60, 60J46, 60H07, 60H30"], "comment": "22 pages", "summary": "We adopt a Gamma and Malliavin Calculi point of view in order to generalize\nScore-based diffusion Generative Models (SGMs) to an infinite-dimensional\nabstract Hilbertian setting. Particularly, we define the forward noising\nprocess using Dirichlet forms associated to the Cameron-Martin space of\nGaussian measures and Wiener chaoses; whereas by relying on an abstract\ntime-reversal formula, we show that the score function is a Malliavin\nderivative and it corresponds to a conditional expectation. This allows us to\ngeneralize SGMs to the infinite-dimensional setting. Moreover, we extend\nexisting finite-dimensional entropic convergence bounds to this Hilbertian\nsetting by highlighting the role played by the Cameron-Martin norm in the\nFisher information of the data distribution. Lastly, we specify our discussion\nfor spherical random fields, considering as source of noise a Whittle-Mat\\'ern\nrandom spherical field."}
{"id": "2505.13315", "pdf": "https://arxiv.org/pdf/2505.13315", "abs": "https://arxiv.org/abs/2505.13315", "authors": ["Reza T. Batley", "Sourav Saha"], "title": "KHRONOS: a Kernel-Based Neural Architecture for Rapid, Resource-Efficient Scientific Computation", "categories": ["cs.LG", "cs.AI", "cs.MS"], "comment": null, "summary": "Contemporary models of high dimensional physical systems are constrained by\nthe curse of dimensionality and a reliance on dense data. We introduce KHRONOS\n(Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates), an\nAI framework for model based, model free and model inversion tasks. KHRONOS\nconstructs continuously differentiable target fields with a hierarchical\ncomposition of per-dimension kernel expansions, which are tensorized into modes\nand then superposed. We evaluate KHRONOS on a canonical 2D, Poisson equation\nbenchmark: across 16 to 512 degrees of freedom (DoFs), it obtained L2 square\nerrors of 5e-4 down to 6e-10. This represents a 100 time gain over Kolmogorov\nArnold Networks (which itself reports a 100 times improvement on MLPs/PINNs\nwith 100 times fewer parameters) when controlling for the number of parameters.\nThis also represents a 1e4 times improvement in L2 square error compared to\nstandard linear FEM at comparable DoFs. Inference complexity is dominated by\ninner products, yielding sub-millisecond full-field predictions that scale to\nan arbitrary resolution. For inverse problems, KHRONOS facilitates rapid,\niterative level set recovery in only a few forward evaluations, with\nsub-microsecond per sample latency. KHRONOS scalability, expressivity, and\ninterpretability open new avenues in constrained edge computing, online\ncontrol, computer vision, and beyond."}
{"id": "2505.13213", "pdf": "https://arxiv.org/pdf/2505.13213", "abs": "https://arxiv.org/abs/2505.13213", "authors": ["Yanfeng Yang", "Kenji Fukumizu"], "title": "Diffusion Models with Double Guidance: Generate with aggregated datasets", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Creating large-scale datasets for training high-performance generative models\nis often prohibitively expensive, especially when associated attributes or\nannotations must be provided. As a result, merging existing datasets has become\na common strategy. However, the sets of attributes across datasets are often\ninconsistent, and their naive concatenation typically leads to block-wise\nmissing conditions. This presents a significant challenge for conditional\ngenerative modeling when the multiple attributes are used jointly as\nconditions, thereby limiting the model's controllability and applicability. To\naddress this issue, we propose a novel generative approach, Diffusion Model\nwith Double Guidance, which enables precise conditional generation even when no\ntraining samples contain all conditions simultaneously. Our method maintains\nrigorous control over multiple conditions without requiring joint annotations.\nWe demonstrate its effectiveness in molecular and image generation tasks, where\nit outperforms existing baselines both in alignment with target conditional\ndistributions and in controllability under missing condition settings."}
{"id": "2505.13316", "pdf": "https://arxiv.org/pdf/2505.13316", "abs": "https://arxiv.org/abs/2505.13316", "authors": ["Gabriele Spadaro", "Alberto Presta", "Jhony H. Giraldo", "Marco Grangetto", "Wei Hu", "Giuseppe Valenzise", "Attilio Fiandrotti", "Enzo Tartaglione"], "title": "Denoising Diffusion Probabilistic Model for Point Cloud Compression at Low Bit-Rates", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "6 pages, 5 figures, accepted at ICME 2025", "summary": "Efficient compression of low-bit-rate point clouds is critical for\nbandwidth-constrained applications. However, existing techniques mainly focus\non high-fidelity reconstruction, requiring many bits for compression. This\npaper proposes a \"Denoising Diffusion Probabilistic Model\" (DDPM) architecture\nfor point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoder\nproduces the condition vector for the generation, which is then quantized via a\nlearnable vector quantizer. This configuration allows to achieve a low bitrates\nwhile preserving quality. Experiments on ShapeNet and ModelNet40 show improved\nrate-distortion at low rates compared to standardized and state-of-the-art\napproaches. We publicly released the code at\nhttps://github.com/EIDOSLAB/DDPM-PCC."}
{"id": "2505.13231", "pdf": "https://arxiv.org/pdf/2505.13231", "abs": "https://arxiv.org/abs/2505.13231", "authors": ["Junyi Chen", "Alap Kshirsagar", "Frederik Heller", "Mario Gómez Andreu", "Boris Belousov", "Tim Schneider", "Lisa P. Y. Lin", "Katja Doerschner", "Knut Drewing", "Jan Peters"], "title": "Investigating Active Sampling for Hardness Classification with Vision-Based Tactile Sensors", "categories": ["cs.RO", "cs.LG"], "comment": "7 pages", "summary": "One of the most important object properties that humans and robots perceive\nthrough touch is hardness. This paper investigates information-theoretic active\nsampling strategies for sample-efficient hardness classification with\nvision-based tactile sensors. We evaluate three probabilistic classifier models\nand two model-uncertainty-based sampling strategies on a robotic setup as well\nas on a previously published dataset of samples collected by human testers. Our\nfindings indicate that the active sampling approaches, driven by uncertainty\nmetrics, surpass a random sampling baseline in terms of accuracy and stability.\nAdditionally, while in our human study, the participants achieve an average\naccuracy of 48.00%, our best approach achieves an average accuracy of 88.78% on\nthe same set of objects, demonstrating the effectiveness of vision-based\ntactile sensors for object hardness classification."}
{"id": "2505.13324", "pdf": "https://arxiv.org/pdf/2505.13324", "abs": "https://arxiv.org/abs/2505.13324", "authors": ["Galit Shmueli", "David Martens", "Jaewon Yoo", "Travis Greene"], "title": "From What Ifs to Insights: Counterfactuals in Causal Inference vs. Explainable AI", "categories": ["stat.ML", "cs.AI", "cs.LG", "econ.EM", "stat.ME"], "comment": null, "summary": "Counterfactuals play a pivotal role in the two distinct data science fields\nof causal inference (CI) and explainable artificial intelligence (XAI). While\nthe core idea behind counterfactuals remains the same in both fields--the\nexamination of what would have happened under different circumstances--there\nare key differences in how they are used and interpreted. We introduce a formal\ndefinition that encompasses the multi-faceted concept of the counterfactual in\nCI and XAI. We then discuss how counterfactuals are used, evaluated, generated,\nand operationalized in CI vs. XAI, highlighting conceptual and practical\ndifferences. By comparing and contrasting the two, we hope to identify\nopportunities for cross-fertilization across CI and XAI."}
{"id": "2505.13235", "pdf": "https://arxiv.org/pdf/2505.13235", "abs": "https://arxiv.org/abs/2505.13235", "authors": ["Dang Hoai Nam", "Huynh Tong Dang Khoa", "Vo Nguyen Le Duy"], "title": "WriteViT: Handwritten Text Generation with Vision Transformer", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Humans can quickly generalize handwriting styles from a single example by\nintuitively separating content from style. Machines, however, struggle with\nthis task, especially in low-data settings, often missing subtle spatial and\nstylistic cues. Motivated by this gap, we introduce WriteViT, a one-shot\nhandwritten text synthesis framework that incorporates Vision Transformers\n(ViT), a family of models that have shown strong performance across various\ncomputer vision tasks. WriteViT integrates a ViT-based Writer Identifier for\nextracting style embeddings, a multi-scale generator built with Transformer\nencoder-decoder blocks enhanced by conditional positional encoding (CPE), and a\nlightweight ViT-based recognizer. While previous methods typically rely on CNNs\nor CRNNs, our design leverages transformers in key components to better capture\nboth fine-grained stroke details and higher-level style information. Although\nhandwritten text synthesis has been widely explored, its application to\nVietnamese -- a language rich in diacritics and complex typography -- remains\nlimited. Experiments on Vietnamese and English datasets demonstrate that\nWriteViT produces high-quality, style-consistent handwriting while maintaining\nstrong recognition performance in low-resource scenarios. These results\nhighlight the promise of transformer-based designs for multilingual handwriting\ngeneration and efficient style adaptation."}
{"id": "2505.13329", "pdf": "https://arxiv.org/pdf/2505.13329", "abs": "https://arxiv.org/abs/2505.13329", "authors": ["Frédéric Berdoz", "Dustin Brunner", "Yann Vonlanthen", "Roger Wattenhofer"], "title": "Recommender Systems for Democracy: Toward Adversarial Robustness in Voting Advice Applications", "categories": ["cs.CY", "cs.AI", "cs.CR"], "comment": "This is the extended version of the paper, accepted at IJCAI 2025", "summary": "Voting advice applications (VAAs) help millions of voters understand which\npolitical parties or candidates best align with their views. This paper\nexplores the potential risks these applications pose to the democratic process\nwhen targeted by adversarial entities. In particular, we expose 11 manipulation\nstrategies and measure their impact using data from Switzerland's primary VAA,\nSmartvote, collected during the last two national elections. We find that\naltering application parameters, such as the matching method, can shift a\nparty's recommendation frequency by up to 105%. Cherry-picking questionnaire\nitems can increase party recommendation frequency by over 261%, while subtle\nchanges to parties' or candidates' responses can lead to a 248% increase. To\naddress these vulnerabilities, we propose adversarial robustness properties\nVAAs should satisfy, introduce empirical metrics for assessing the resilience\nof various matching methods, and suggest possible avenues for research toward\nmitigating the effect of manipulation. Our framework is key to ensuring secure\nand reliable AI-based VAAs poised to emerge in the near future."}
{"id": "2505.13243", "pdf": "https://arxiv.org/pdf/2505.13243", "abs": "https://arxiv.org/abs/2505.13243", "authors": ["Wenbin Zhou", "Agni Orfanoudaki", "Shixiang Zhu"], "title": "Conformalized Decision Risk Assessment", "categories": ["stat.ML", "cs.LG"], "comment": "36 pages, 17 figures", "summary": "High-stakes decisions in domains such as healthcare, energy, and public\npolicy are often made by human experts using domain knowledge and heuristics,\nyet are increasingly supported by predictive and optimization-based tools. A\ndominant approach in operations research is the predict-then-optimize paradigm,\nwhere a predictive model estimates uncertain inputs, and an optimization model\nrecommends a decision. However, this approach often lacks interpretability and\ncan fail under distributional uncertainty -- particularly when the outcome\ndistribution is multi-modal or complex -- leading to brittle or misleading\ndecisions. In this paper, we introduce CREDO, a novel framework that\nquantifies, for any candidate decision, a distribution-free upper bound on the\nprobability that the decision is suboptimal. By combining inverse optimization\ngeometry with conformal prediction and generative modeling, CREDO produces risk\ncertificates that are both statistically rigorous and practically\ninterpretable. This framework enables human decision-makers to audit and\nvalidate their own decisions under uncertainty, bridging the gap between\nalgorithmic tools and real-world judgment."}
{"id": "2505.13338", "pdf": "https://arxiv.org/pdf/2505.13338", "abs": "https://arxiv.org/abs/2505.13338", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities."}
{"id": "2505.13244", "pdf": "https://arxiv.org/pdf/2505.13244", "abs": "https://arxiv.org/abs/2505.13244", "authors": ["Jieying Xue", "Phuong Minh Nguyen", "Minh Le Nguyen", "Xin Liu"], "title": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models", "categories": ["cs.CL", "cs.LG"], "comment": "Published in The 19th International Workshop on Semantic Evaluation\n  (SemEval-2025)", "summary": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection."}
{"id": "2505.13339", "pdf": "https://arxiv.org/pdf/2505.13339", "abs": "https://arxiv.org/abs/2505.13339", "authors": ["Jia-Hui Pan", "Yeok Tatt Cheah", "Zhengzhe Liu", "Ka-Hei Hui", "Xiaojie Gao", "Pheng-Ann Heng", "Yun-Hui Liu", "Chi-Wing Fu"], "title": "OPA-Pack: Object-Property-Aware Robotic Bin Packing", "categories": ["cs.RO", "cs.AI"], "comment": "Submitted to IEEE Transactions on Robotics (TRO) on Feb. 10, 2025", "summary": "Robotic bin packing aids in a wide range of real-world scenarios such as\ne-commerce and warehouses. Yet, existing works focus mainly on considering the\nshape of objects to optimize packing compactness and neglect object properties\nsuch as fragility, edibility, and chemistry that humans typically consider when\npacking objects. This paper presents OPA-Pack (Object-Property-Aware Packing\nframework), the first framework that equips the robot with object property\nconsiderations in planning the object packing. Technical-wise, we develop a\nnovel object property recognition scheme with retrieval-augmented generation\nand chain-of-thought reasoning, and build a dataset with object property\nannotations for 1,032 everyday objects. Also, we formulate OPA-Net, aiming to\njointly separate incompatible object pairs and reduce pressure on fragile\nobjects, while compacting the packing. Further, OPA-Net consists of a property\nembedding layer to encode the property of candidate objects to be packed,\ntogether with a fragility heightmap and an avoidance heightmap to keep track of\nthe packed objects. Then, we design a reward function and adopt a deep\nQ-learning scheme to train OPA-Net. Experimental results manifest that OPA-Pack\ngreatly improves the accuracy of separating incompatible object pairs (from 52%\nto 95%) and largely reduces pressure on fragile objects (by 29.4%), while\nmaintaining good packing compactness. Besides, we demonstrate the effectiveness\nof OPA-Pack on a real packing platform, showcasing its practicality in\nreal-world scenarios."}
{"id": "2505.13257", "pdf": "https://arxiv.org/pdf/2505.13257", "abs": "https://arxiv.org/abs/2505.13257", "authors": ["Zilu Tang", "Afra Feyza Akyürek", "Ekin Akyürek", "Derry Wijaya"], "title": "WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, preprint", "summary": "Preference alignment has become a standard pipeline in finetuning models to\nfollow \\emph{generic} human preferences. Majority of work seeks to optimize\nmodel to produce responses that would be preferable \\emph{on average},\nsimplifying the diverse and often \\emph{contradicting} space of human\npreferences. While research has increasingly focused on personalized alignment:\nadapting models to individual user preferences, there is a lack of personalized\npreference dataset which focus on nuanced individual-level preferences. To\naddress this, we introduce WikiPersona: the first fine-grained personalization\nusing well-documented, famous individuals. Our dataset challenges models to\nalign with these personas through an interpretable process: generating\nverifiable textual descriptions of a persona's background and preferences in\naddition to alignment. We systematically evaluate different personalization\napproaches and find that as few-shot prompting with preferences and fine-tuning\nfail to simultaneously ensure effectiveness and efficiency, using\n\\textit{inferred personal preferences} as prefixes enables effective\npersonalization, especially in topics where preferences clash while leading to\nmore equitable generalization across unseen personas."}
{"id": "2505.13344", "pdf": "https://arxiv.org/pdf/2505.13344", "abs": "https://arxiv.org/abs/2505.13344", "authors": ["Ahmet Berke Gokmen", "Yigit Ekin", "Bahri Batuhan Bilecen", "Aysegul Dundar"], "title": "RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "https://berkegokmen1.github.io/RoPECraft/", "summary": "We propose RoPECraft, a training-free video motion transfer method for\ndiffusion transformers that operates solely by modifying their rotary\npositional embeddings (RoPE). We first extract dense optical flow from a\nreference video, and utilize the resulting motion offsets to warp the\ncomplex-exponential tensors of RoPE, effectively encoding motion into the\ngeneration process. These embeddings are then further optimized during\ndenoising time steps via trajectory alignment between the predicted and target\nvelocities using a flow-matching objective. To keep the output faithful to the\ntext prompt and prevent duplicate generations, we incorporate a regularization\nterm based on the phase components of the reference video's Fourier transform,\nprojecting the phase angles onto a smooth manifold to suppress high-frequency\nartifacts. Experiments on benchmarks reveal that RoPECraft outperforms all\nrecently published methods, both qualitatively and quantitatively."}
{"id": "2505.13268", "pdf": "https://arxiv.org/pdf/2505.13268", "abs": "https://arxiv.org/abs/2505.13268", "authors": ["Livia Qian", "Carol Figueroa", "Gabriel Skantze"], "title": "Representation of perceived prosodic similarity of conversational feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Interspeech 2025", "summary": "Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of\nspoken dialogue and is crucial to ensuring common ground in conversational\nsystems. The exact meaning of such feedback is conveyed through both lexical\nand prosodic form. In this work, we investigate the perceived prosodic\nsimilarity of vocal feedback with the same lexical form, and to what extent\nexisting speech representations reflect such similarities. A triadic comparison\ntask with recruited participants is used to measure perceived similarity of\nfeedback responses taken from two different datasets. We find that spectral and\nself-supervised speech representations encode prosody better than extracted\npitch features, especially in the case of feedback from the same speaker. We\nalso find that it is possible to further condense and align the representations\nto human perception through contrastive learning."}
{"id": "2505.13346", "pdf": "https://arxiv.org/pdf/2505.13346", "abs": "https://arxiv.org/abs/2505.13346", "authors": ["Austin Xu", "Yilun Zhou", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark", "summary": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench."}
{"id": "2505.13273", "pdf": "https://arxiv.org/pdf/2505.13273", "abs": "https://arxiv.org/abs/2505.13273", "authors": ["Lucas Berry", "Axel Brando", "Wei-Di Chang", "Juan Camilo Gamboa Higuera", "David Meger"], "title": "Seeing the Unseen: How EMoE Unveils Bias in Text-to-Image Diffusion Models", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Estimating uncertainty in text-to-image diffusion models is challenging\nbecause of their large parameter counts (often exceeding 100 million) and\noperation in complex, high-dimensional spaces with virtually infinite input\npossibilities. In this paper, we propose Epistemic Mixture of Experts (EMoE), a\nnovel framework for efficiently estimating epistemic uncertainty in diffusion\nmodels. EMoE leverages pre-trained networks without requiring additional\ntraining, enabling direct uncertainty estimation from a prompt. We leverage a\nlatent space within the diffusion process that captures epistemic uncertainty\nbetter than existing methods. Experimental results on the COCO dataset\ndemonstrate EMoE's effectiveness, showing a strong correlation between\nuncertainty and image quality. Additionally, EMoE identifies under-sampled\nlanguages and regions with higher uncertainty, revealing hidden biases in the\ntraining set. This capability demonstrates the relevance of EMoE as a tool for\naddressing fairness and accountability in AI-generated content."}
{"id": "2505.13358", "pdf": "https://arxiv.org/pdf/2505.13358", "abs": "https://arxiv.org/abs/2505.13358", "authors": ["Nimrod Berman", "Ilan Naiman", "Moshe Eliasof", "Hedi Zisling", "Omri Azencot"], "title": "One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diffusion-based generative models have demonstrated exceptional performance,\nyet their iterative sampling procedures remain computationally expensive. A\nprominent strategy to mitigate this cost is distillation, with offline\ndistillation offering particular advantages in terms of efficiency, modularity,\nand flexibility. In this work, we identify two key observations that motivate a\nprincipled distillation framework: (1) while diffusion models have been viewed\nthrough the lens of dynamical systems theory, powerful and underexplored tools\ncan be further leveraged; and (2) diffusion models inherently impose\nstructured, semantically coherent trajectories in latent space. Building on\nthese observations, we introduce the Koopman Distillation Model KDM, a novel\noffline distillation approach grounded in Koopman theory-a classical framework\nfor representing nonlinear dynamics linearly in a transformed space. KDM\nencodes noisy inputs into an embedded space where a learned linear operator\npropagates them forward, followed by a decoder that reconstructs clean samples.\nThis enables single-step generation while preserving semantic fidelity. We\nprovide theoretical justification for our approach: (1) under mild assumptions,\nthe learned diffusion dynamics admit a finite-dimensional Koopman\nrepresentation; and (2) proximity in the Koopman latent space correlates with\nsemantic similarity in the generated outputs, allowing for effective trajectory\nalignment. Empirically, KDM achieves state-of-the-art performance across\nstandard offline distillation benchmarks, improving FID scores by up to 40% in\na single generation step. All implementation details and code for the\nexperimental setups are provided in our GitHub -\nhttps://github.com/azencot-group/KDM, or in our project page -\nhttps://sites.google.com/view/koopman-distillation-model."}
{"id": "2505.13299", "pdf": "https://arxiv.org/pdf/2505.13299", "abs": "https://arxiv.org/abs/2505.13299", "authors": ["Likai Chen", "Georg Keilbar", "Wei Biao Wu"], "title": "Smoothed SGD for quantiles: Bahadur representation and Gaussian approximation", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "This paper considers the estimation of quantiles via a smoothed version of\nthe stochastic gradient descent (SGD) algorithm. By smoothing the score\nfunction in the conventional SGD quantile algorithm, we achieve monotonicity in\nthe quantile level in that the estimated quantile curves do not cross. We\nderive non-asymptotic tail probability bounds for the smoothed SGD quantile\nestimate both for the case with and without Polyak-Ruppert averaging. For the\nlatter, we also provide a uniform Bahadur representation and a resulting\nGaussian approximation result. Numerical studies show good finite sample\nbehavior for our theoretical results."}
{"id": "2505.13379", "pdf": "https://arxiv.org/pdf/2505.13379", "abs": "https://arxiv.org/abs/2505.13379", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "title": "Thinkless: LLM Learns When to Think", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless"}
{"id": "2505.13316", "pdf": "https://arxiv.org/pdf/2505.13316", "abs": "https://arxiv.org/abs/2505.13316", "authors": ["Gabriele Spadaro", "Alberto Presta", "Jhony H. Giraldo", "Marco Grangetto", "Wei Hu", "Giuseppe Valenzise", "Attilio Fiandrotti", "Enzo Tartaglione"], "title": "Denoising Diffusion Probabilistic Model for Point Cloud Compression at Low Bit-Rates", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "6 pages, 5 figures, accepted at ICME 2025", "summary": "Efficient compression of low-bit-rate point clouds is critical for\nbandwidth-constrained applications. However, existing techniques mainly focus\non high-fidelity reconstruction, requiring many bits for compression. This\npaper proposes a \"Denoising Diffusion Probabilistic Model\" (DDPM) architecture\nfor point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoder\nproduces the condition vector for the generation, which is then quantized via a\nlearnable vector quantizer. This configuration allows to achieve a low bitrates\nwhile preserving quality. Experiments on ShapeNet and ModelNet40 show improved\nrate-distortion at low rates compared to standardized and state-of-the-art\napproaches. We publicly released the code at\nhttps://github.com/EIDOSLAB/DDPM-PCC."}
{"id": "2505.13381", "pdf": "https://arxiv.org/pdf/2505.13381", "abs": "https://arxiv.org/abs/2505.13381", "authors": ["Mak Ahmad", "Prerna Ravi", "David Karger", "Marc Facciotti"], "title": "How Adding Metacognitive Requirements in Support of AI Feedback in Practice Exams Transforms Student Learning Behaviors", "categories": ["cs.HC", "cs.AI", "K.3.1; I.2.7; H.5.2"], "comment": "10 pages, 3 figures, to appear in Proceedings of the Twelfth ACM\n  Conference on Learning @ Scale (L@S 2025), July 2025, Palermo, Italy", "summary": "Providing personalized, detailed feedback at scale in large undergraduate\nSTEM courses remains a persistent challenge. We present an empirically\nevaluated practice exam system that integrates AI generated feedback with\ntargeted textbook references, deployed in a large introductory biology course.\nOur system encourages metacognitive behavior by asking students to explain\ntheir answers and declare their confidence. It uses OpenAI's GPT-4o to generate\npersonalized feedback based on this information, while directing them to\nrelevant textbook sections. Through interaction logs from consenting\nparticipants across three midterms (541, 342, and 413 students respectively),\ntotaling 28,313 question-student interactions across 146 learning objectives,\nalong with 279 surveys and 23 interviews, we examined the system's impact on\nlearning outcomes and engagement. Across all midterms, feedback types showed no\nstatistically significant performance differences, though some trends suggested\npotential benefits. The most substantial impact came from the required\nconfidence ratings and explanations, which students reported transferring to\ntheir actual exam strategies. About 40 percent of students engaged with\ntextbook references when prompted by feedback -- far higher than traditional\nreading rates. Survey data revealed high satisfaction (mean rating 4.1 of 5),\nwith 82.1 percent reporting increased confidence on practiced midterm topics,\nand 73.4 percent indicating they could recall and apply specific concepts. Our\nfindings suggest that embedding structured reflection requirements may be more\nimpactful than sophisticated feedback mechanisms."}
{"id": "2505.13318", "pdf": "https://arxiv.org/pdf/2505.13318", "abs": "https://arxiv.org/abs/2505.13318", "authors": ["Paula Feldman", "Martin Sinnona", "Viviana Siless", "Claudio Delrieux", "Emmanuel Iarussi"], "title": "VesselGPT: Autoregressive Modeling of Vascular Geometry", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Anatomical trees are critical for clinical diagnosis and treatment planning,\nyet their complex and diverse geometry make accurate representation a\nsignificant challenge. Motivated by the latest advances in large language\nmodels, we introduce an autoregressive method for synthesizing anatomical\ntrees. Our approach first embeds vessel structures into a learned discrete\nvocabulary using a VQ-VAE architecture, then models their generation\nautoregressively with a GPT-2 model. This method effectively captures intricate\ngeometries and branching patterns, enabling realistic vascular tree synthesis.\nComprehensive qualitative and quantitative evaluations reveal that our\ntechnique achieves high-fidelity tree reconstruction with compact discrete\nrepresentations. Moreover, our B-spline representation of vessel cross-sections\npreserves critical morphological details that are often overlooked in previous'\nmethods parameterizations. To the best of our knowledge, this work is the first\nto generate blood vessels in an autoregressive manner. Code, data, and trained\nmodels will be made available."}
{"id": "2505.13388", "pdf": "https://arxiv.org/pdf/2505.13388", "abs": "https://arxiv.org/abs/2505.13388", "authors": ["David Anugraha", "Zilu Tang", "Lester James V. Miranda", "Hanyang Zhao", "Mohammad Rifqi Farhansyah", "Garry Kuwanto", "Derry Wijaya", "Genta Indra Winata"], "title": "R3: Robust Rubric-Agnostic Reward Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3"}
{"id": "2505.13324", "pdf": "https://arxiv.org/pdf/2505.13324", "abs": "https://arxiv.org/abs/2505.13324", "authors": ["Galit Shmueli", "David Martens", "Jaewon Yoo", "Travis Greene"], "title": "From What Ifs to Insights: Counterfactuals in Causal Inference vs. Explainable AI", "categories": ["stat.ML", "cs.AI", "cs.LG", "econ.EM", "stat.ME"], "comment": null, "summary": "Counterfactuals play a pivotal role in the two distinct data science fields\nof causal inference (CI) and explainable artificial intelligence (XAI). While\nthe core idea behind counterfactuals remains the same in both fields--the\nexamination of what would have happened under different circumstances--there\nare key differences in how they are used and interpreted. We introduce a formal\ndefinition that encompasses the multi-faceted concept of the counterfactual in\nCI and XAI. We then discuss how counterfactuals are used, evaluated, generated,\nand operationalized in CI vs. XAI, highlighting conceptual and practical\ndifferences. By comparing and contrasting the two, we hope to identify\nopportunities for cross-fertilization across CI and XAI."}
{"id": "2505.13393", "pdf": "https://arxiv.org/pdf/2505.13393", "abs": "https://arxiv.org/abs/2505.13393", "authors": ["Christopher K. Frantz"], "title": "IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar", "categories": ["cs.MA", "cs.AI", "cs.CL", "68T30, 68T50", "E.2; H.1.0; I.7.2; I.6.5; K.4.1"], "comment": "24 pages", "summary": "This article provides an overview of IG Parser, a software that facilitates\nqualitative content analysis of formal (e.g., legal) rules or informal (e.g.,\nsocio-normative) norms, and strategies (such as conventions) -- referred to as\n\\emph{institutions} -- that govern social systems and operate configurally to\ndescribe \\emph{institutional systems}. To this end, the IG Parser employs a\ndistinctive syntax that ensures rigorous encoding of natural language, while\nautomating the transformation into various formats that support the downstream\nanalysis using diverse analytical techniques. The conceptual core of the IG\nParser is an associated syntax, IG Script, that operationalizes the conceptual\nfoundations of the Institutional Grammar, and more specifically Institutional\nGrammar 2.0, an analytical paradigm for institutional analysis. This article\npresents the IG Parser, including its conceptual foundations, syntactic\nspecification of IG Script, alongside architectural principles. This\nintroduction is augmented with selective illustrative examples that highlight\nthe use and benefit associated with the tool."}
{"id": "2505.13334", "pdf": "https://arxiv.org/pdf/2505.13334", "abs": "https://arxiv.org/abs/2505.13334", "authors": ["Ho-Chun Herbert Chang"], "title": "Measuring Social Influence with Networked Synthetic Control", "categories": ["cs.SI", "cs.LG"], "comment": null, "summary": "Measuring social influence is difficult due to the lack of counter-factuals\nand comparisons. By combining machine learning-based modeling and network\nscience, we present general properties of social value, a recent measure for\nsocial influence using synthetic control applicable to political behavior.\nSocial value diverges from centrality measures on in that it relies on an\nexternal regressor to predict an output variable of interest, generates a\nsynthetic measure of influence, then distributes individual contribution based\non a social network. Through theoretical derivations, we show the properties of\nSV under linear regression with and without interaction, across lattice\nnetworks, power-law networks, and random graphs. A reduction in computation can\nbe achieved for any ensemble model. Through simulation, we find that the\ngeneralized friendship paradox holds -- that in certain situations, your\nfriends have on average more influence than you do."}
{"id": "2505.13417", "pdf": "https://arxiv.org/pdf/2505.13417", "abs": "https://arxiv.org/abs/2505.13417", "authors": ["Jiajie Zhang", "Nianyi Lin", "Lei Hou", "Ling Feng", "Juanzi Li"], "title": "AdaptThink: Reasoning Models Can Learn When to Think", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink."}
{"id": "2505.13344", "pdf": "https://arxiv.org/pdf/2505.13344", "abs": "https://arxiv.org/abs/2505.13344", "authors": ["Ahmet Berke Gokmen", "Yigit Ekin", "Bahri Batuhan Bilecen", "Aysegul Dundar"], "title": "RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "https://berkegokmen1.github.io/RoPECraft/", "summary": "We propose RoPECraft, a training-free video motion transfer method for\ndiffusion transformers that operates solely by modifying their rotary\npositional embeddings (RoPE). We first extract dense optical flow from a\nreference video, and utilize the resulting motion offsets to warp the\ncomplex-exponential tensors of RoPE, effectively encoding motion into the\ngeneration process. These embeddings are then further optimized during\ndenoising time steps via trajectory alignment between the predicted and target\nvelocities using a flow-matching objective. To keep the output faithful to the\ntext prompt and prevent duplicate generations, we incorporate a regularization\nterm based on the phase components of the reference video's Fourier transform,\nprojecting the phase angles onto a smooth manifold to suppress high-frequency\nartifacts. Experiments on benchmarks reveal that RoPECraft outperforms all\nrecently published methods, both qualitatively and quantitatively."}
{"id": "2505.13425", "pdf": "https://arxiv.org/pdf/2505.13425", "abs": "https://arxiv.org/abs/2505.13425", "authors": ["Zhi-Hao Tan", "Zi-Chen Zhao", "Hao-Yu Shi", "Xin-Yu Zhang", "Peng Tan", "Yang Yu", "Zhi-Hua Zhou"], "title": "Learnware of Language Models: Specialized Small Language Models Can Do Big", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The learnware paradigm offers a novel approach to machine learning by\nenabling users to reuse a set of well-trained models for tasks beyond the\nmodels' original purposes. It eliminates the need to build models from scratch,\ninstead relying on specifications (representations of a model's capabilities)\nto identify and leverage the most suitable models for new tasks. While\nlearnware has proven effective in many scenarios, its application to language\nmodels has remained largely unexplored. At the same time, large language models\n(LLMs) have demonstrated remarkable universal question-answering abilities, yet\nthey face challenges in specialized scenarios due to data scarcity, privacy\nconcerns, and high computational costs, thus more and more specialized small\nlanguage models (SLMs) are being trained for specific domains. To address these\nlimitations systematically, the learnware paradigm provides a promising\nsolution by enabling maximum utilization of specialized SLMs, and allowing\nusers to identify and reuse them in a collaborative and privacy-preserving\nmanner.\n  This paper presents a preliminary attempt to apply the learnware paradigm to\nlanguage models. We simulated a learnware system comprising approximately 100\nlearnwares of specialized SLMs with 8B parameters, fine-tuned across finance,\nhealthcare, and mathematics domains. Each learnware contains an SLM and a\nspecification, which enables users to identify the most relevant models without\nexposing their own data. Experimental results demonstrate promising\nperformance: by selecting one suitable learnware for each task-specific\ninference, the system outperforms the base SLMs on all benchmarks. Compared to\nLLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and\nLlama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses\nFlan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical\ndomain tasks."}
{"id": "2505.13353", "pdf": "https://arxiv.org/pdf/2505.13353", "abs": "https://arxiv.org/abs/2505.13353", "authors": ["Adam Štorek", "Mukur Gupta", "Samira Hajizadeh", "Prashast Srivastava", "Suman Jana"], "title": "Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning", "categories": ["cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Although modern Large Language Models (LLMs) support extremely large\ncontexts, their effectiveness in utilizing long context for code reasoning\nremains unclear. This paper investigates LLM reasoning ability over code\nsnippets within large repositories and how it relates to their recall ability.\nSpecifically, we differentiate between lexical code recall (verbatim retrieval)\nand semantic code recall (remembering what the code does). To measure semantic\nrecall, we propose SemTrace, a code reasoning technique where the impact of\nspecific statements on output is attributable and unpredictable. We also\npresent a method to quantify semantic recall sensitivity in existing\nbenchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop\nin code reasoning accuracy as a code snippet approaches the middle of the input\ncontext, particularly with techniques requiring high semantic recall like\nSemTrace. Moreover, we find that lexical recall varies by granularity, with\nmodels excelling at function retrieval but struggling with line-by-line recall.\nNotably, a disconnect exists between lexical and semantic recall, suggesting\ndifferent underlying mechanisms. Finally, our findings indicate that current\ncode reasoning benchmarks may exhibit low semantic recall sensitivity,\npotentially underestimating LLM challenges in leveraging in-context\ninformation."}
{"id": "2505.13437", "pdf": "https://arxiv.org/pdf/2505.13437", "abs": "https://arxiv.org/abs/2505.13437", "authors": ["Dian Shao", "Mingfei Shi", "Shengda Xu", "Haodong Chen", "Yongle Huang", "Binglu Wang"], "title": "FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Despite significant advances in video generation, synthesizing physically\nplausible human actions remains a persistent challenge, particularly in\nmodeling fine-grained semantics and complex temporal dynamics. For instance,\ngenerating gymnastics routines such as \"switch leap with 0.5 turn\" poses\nsubstantial difficulties for current methods, often yielding unsatisfactory\nresults. To bridge this gap, we propose FinePhys, a Fine-grained human action\ngeneration framework that incorporates Physics to obtain effective skeletal\nguidance. Specifically, FinePhys first estimates 2D poses in an online manner\nand then performs 2D-to-3D dimension lifting via in-context learning. To\nmitigate the instability and limited interpretability of purely data-driven 3D\nposes, we further introduce a physics-based motion re-estimation module\ngoverned by Euler-Lagrange equations, calculating joint accelerations via\nbidirectional temporal updating. The physically predicted 3D poses are then\nfused with data-driven ones, offering multi-scale 2D heatmap guidance for the\ndiffusion process. Evaluated on three fine-grained action subsets from FineGym\n(FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms\ncompetitive baselines. Comprehensive qualitative results further demonstrate\nFinePhys's ability to generate more natural and plausible fine-grained human\nactions."}
{"id": "2505.13357", "pdf": "https://arxiv.org/pdf/2505.13357", "abs": "https://arxiv.org/abs/2505.13357", "authors": ["Rebecca Pelke", "Nils Bosbach", "Lennart M. Reimann", "Rainer Leupers"], "title": "Introducing Instruction-Accurate Simulators for Performance Estimation of Autotuning Workloads", "categories": ["cs.AR", "cs.LG"], "comment": null, "summary": "Accelerating Machine Learning (ML) workloads requires efficient methods due\nto their large optimization space. Autotuning has emerged as an effective\napproach for systematically evaluating variations of implementations.\nTraditionally, autotuning requires the workloads to be executed on the target\nhardware (HW). We present an interface that allows executing autotuning\nworkloads on simulators. This approach offers high scalability when the\navailability of the target HW is limited, as many simulations can be run in\nparallel on any accessible HW. Additionally, we evaluate the feasibility of\nusing fast instruction-accurate simulators for autotuning. We train various\npredictors to forecast the performance of ML workload implementations on the\ntarget HW based on simulation statistics. Our results demonstrate that the\ntuned predictors are highly effective. The best workload implementation in\nterms of actual run time on the target HW is always within the top 3 % of\npredictions for the tested x86, ARM, and RISC-V-based architectures. In the\nbest case, this approach outperforms native execution on the target HW for\nembedded architectures when running as few as three samples on three simulators\nin parallel."}
{"id": "2505.13438", "pdf": "https://arxiv.org/pdf/2505.13438", "abs": "https://arxiv.org/abs/2505.13438", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency."}
{"id": "2505.13375", "pdf": "https://arxiv.org/pdf/2505.13375", "abs": "https://arxiv.org/abs/2505.13375", "authors": ["Christopher Kolloff", "Tobias Höppe", "Emmanouil Angelis", "Mathias Jacob Schreiner", "Stefan Bauer", "Andrea Dittadi", "Simon Olsson"], "title": "Minimum-Excess-Work Guidance", "categories": ["stat.ML", "cs.LG"], "comment": "30 pages, 18 figures", "summary": "We propose a regularization framework inspired by thermodynamic work for\nguiding pre-trained probability flow generative models (e.g., continuous\nnormalizing flows or diffusion models) by minimizing excess work, a concept\nrooted in statistical mechanics and with strong conceptual connections to\noptimal transport. Our approach enables efficient guidance in sparse-data\nregimes common to scientific applications, where only limited target samples or\npartial density constraints are available. We introduce two strategies: Path\nGuidance for sampling rare transition states by concentrating probability mass\non user-defined subsets, and Observable Guidance for aligning generated\ndistributions with experimental observables while preserving entropy. We\ndemonstrate the framework's versatility on a coarse-grained protein model,\nguiding it to sample transition configurations between folded/unfolded states\nand correct systematic biases using experimental data. The method bridges\nthermodynamic principles with modern generative architectures, offering a\nprincipled, efficient, and physics-inspired alternative to standard fine-tuning\nin data-scarce domains. Empirical results highlight improved sample efficiency\nand bias reduction, underscoring its applicability to molecular simulations and\nbeyond."}
{"id": "2505.13439", "pdf": "https://arxiv.org/pdf/2505.13439", "abs": "https://arxiv.org/abs/2505.13439", "authors": ["Huawei Lin", "Tong Geng", "Zhaozhuo Xu", "Weijie Zhao"], "title": "VTBench: Evaluating Visual Tokenizers for Autoregressive Image Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "24 pages, 13 figures, 3 tables", "summary": "Autoregressive (AR) models have recently shown strong performance in image\ngeneration, where a critical component is the visual tokenizer (VT) that maps\ncontinuous pixel inputs to discrete token sequences. The quality of the VT\nlargely defines the upper bound of AR model performance. However, current\ndiscrete VTs fall significantly behind continuous variational autoencoders\n(VAEs), leading to degraded image reconstructions and poor preservation of\ndetails and text. Existing benchmarks focus on end-to-end generation quality,\nwithout isolating VT performance. To address this gap, we introduce VTBench, a\ncomprehensive benchmark that systematically evaluates VTs across three core\ntasks: Image Reconstruction, Detail Preservation, and Text Preservation, and\ncovers a diverse range of evaluation scenarios. We systematically assess\nstate-of-the-art VTs using a set of metrics to evaluate the quality of\nreconstructed images. Our findings reveal that continuous VAEs produce superior\nvisual representations compared to discrete VTs, particularly in retaining\nspatial structure and semantic detail. In contrast, the degraded\nrepresentations produced by discrete VTs often lead to distorted\nreconstructions, loss of fine-grained textures, and failures in preserving text\nand object integrity. Furthermore, we conduct experiments on GPT-4o image\ngeneration and discuss its potential AR nature, offering new insights into the\nrole of visual tokenization. We release our benchmark and codebase publicly to\nsupport further research and call on the community to develop strong,\ngeneral-purpose open-source VTs."}
{"id": "2505.13388", "pdf": "https://arxiv.org/pdf/2505.13388", "abs": "https://arxiv.org/abs/2505.13388", "authors": ["David Anugraha", "Zilu Tang", "Lester James V. Miranda", "Hanyang Zhao", "Mohammad Rifqi Farhansyah", "Garry Kuwanto", "Derry Wijaya", "Genta Indra Winata"], "title": "R3: Robust Rubric-Agnostic Reward Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3"}
{"id": "2505.13448", "pdf": "https://arxiv.org/pdf/2505.13448", "abs": "https://arxiv.org/abs/2505.13448", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE."}
{"id": "2505.13391", "pdf": "https://arxiv.org/pdf/2505.13391", "abs": "https://arxiv.org/abs/2505.13391", "authors": ["Mikołaj Małkiński", "Jacek Mańdziuk"], "title": "Advancing Generalization Across a Variety of Abstract Visual Reasoning Tasks", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted to the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "The abstract visual reasoning (AVR) domain presents a diverse suite of\nanalogy-based tasks devoted to studying model generalization. Recent years have\nbrought dynamic progress in the field, particularly in i.i.d. scenarios, in\nwhich models are trained and evaluated on the same data distributions.\nNevertheless, o.o.d. setups that assess model generalization to new test\ndistributions remain challenging even for the most recent models. To advance\ngeneralization in AVR tasks, we present the Pathways of Normalized Group\nConvolution model (PoNG), a novel neural architecture that features group\nconvolution, normalization, and a parallel design. We consider a wide set of\nAVR benchmarks, including Raven's Progressive Matrices and visual analogy\nproblems with both synthetic and real-world images. The experiments demonstrate\nstrong generalization capabilities of the proposed model, which in several\nsettings outperforms the existing literature methods."}
{"id": "2505.13417", "pdf": "https://arxiv.org/pdf/2505.13417", "abs": "https://arxiv.org/abs/2505.13417", "authors": ["Jiajie Zhang", "Nianyi Lin", "Lei Hou", "Ling Feng", "Juanzi Li"], "title": "AdaptThink: Reasoning Models Can Learn When to Think", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink."}
{"id": "2505.13418", "pdf": "https://arxiv.org/pdf/2505.13418", "abs": "https://arxiv.org/abs/2505.13418", "authors": ["Lotem Peled-Cohen", "Maya Zadok", "Nitay Calderon", "Hila Gonen", "Roi Reichart"], "title": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter."}
{"id": "2505.13422", "pdf": "https://arxiv.org/pdf/2505.13422", "abs": "https://arxiv.org/abs/2505.13422", "authors": ["Connor Lennon", "Edward Rubin", "Glen Waddell"], "title": "Machine learning the first stage in 2SLS: Practical guidance from bias decomposition and simulation", "categories": ["econ.EM", "cs.LG", "stat.AP", "stat.ML"], "comment": null, "summary": "Machine learning (ML) primarily evolved to solve \"prediction problems.\" The\nfirst stage of two-stage least squares (2SLS) is a prediction problem,\nsuggesting potential gains from ML first-stage assistance. However, little\nguidance exists on when ML helps 2SLS$\\unicode{x2014}$or when it hurts. We\ninvestigate the implications of inserting ML into 2SLS, decomposing the bias\ninto three informative components. Mechanically, ML-in-2SLS procedures face\nissues common to prediction and causal-inference settings$\\unicode{x2014}$and\ntheir interaction. Through simulation, we show linear ML methods (e.g.,\npost-Lasso) work well, while nonlinear methods (e.g., random forests, neural\nnets) generate substantial bias in second-stage\nestimates$\\unicode{x2014}$potentially exceeding the bias of endogenous OLS."}
{"id": "2505.13439", "pdf": "https://arxiv.org/pdf/2505.13439", "abs": "https://arxiv.org/abs/2505.13439", "authors": ["Huawei Lin", "Tong Geng", "Zhaozhuo Xu", "Weijie Zhao"], "title": "VTBench: Evaluating Visual Tokenizers for Autoregressive Image Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "24 pages, 13 figures, 3 tables", "summary": "Autoregressive (AR) models have recently shown strong performance in image\ngeneration, where a critical component is the visual tokenizer (VT) that maps\ncontinuous pixel inputs to discrete token sequences. The quality of the VT\nlargely defines the upper bound of AR model performance. However, current\ndiscrete VTs fall significantly behind continuous variational autoencoders\n(VAEs), leading to degraded image reconstructions and poor preservation of\ndetails and text. Existing benchmarks focus on end-to-end generation quality,\nwithout isolating VT performance. To address this gap, we introduce VTBench, a\ncomprehensive benchmark that systematically evaluates VTs across three core\ntasks: Image Reconstruction, Detail Preservation, and Text Preservation, and\ncovers a diverse range of evaluation scenarios. We systematically assess\nstate-of-the-art VTs using a set of metrics to evaluate the quality of\nreconstructed images. Our findings reveal that continuous VAEs produce superior\nvisual representations compared to discrete VTs, particularly in retaining\nspatial structure and semantic detail. In contrast, the degraded\nrepresentations produced by discrete VTs often lead to distorted\nreconstructions, loss of fine-grained textures, and failures in preserving text\nand object integrity. Furthermore, we conduct experiments on GPT-4o image\ngeneration and discuss its potential AR nature, offering new insights into the\nrole of visual tokenization. We release our benchmark and codebase publicly to\nsupport further research and call on the community to develop strong,\ngeneral-purpose open-source VTs."}
