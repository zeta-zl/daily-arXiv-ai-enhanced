<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.LG](#cs.LG) [Total: 61]
- [cs.AI](#cs.AI) [Total: 24]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.DB](#cs.DB) [Total: 2]
- [stat.ML](#stat.ML) [Total: 6]
- [cs.SI](#cs.SI) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.CE](#cs.CE) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.CV](#cs.CV) [Total: 24]
- [cs.GT](#cs.GT) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.GR](#cs.GR) [Total: 4]
- [quant-ph](#quant-ph) [Total: 5]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [nlin.AO](#nlin.AO) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [math.DS](#math.DS) [Total: 2]
- [eess.SP](#eess.SP) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks](https://arxiv.org/abs/2505.04628)
*Yusen Wu,Junwu Xiong,Xiaotie Deng*

Main category: cs.CL

TL;DR: 引入HSII基准，评估大语言模型在复杂社交任务中的表现，包括格式解析、目标选择、对话切换等阶段，并通过COT方法提升性能，提出COT-complexity平衡效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在多人多轮社交任务中缺乏系统性评估，需填补这一空白。

Method: 提出HSII基准及HSII-Dataset，采用四阶段评估框架，并结合COT方法及COT-complexity指标。

Result: 实验证明HSII能有效评估LLM的社交能力，COT方法提升性能但增加计算成本。

Conclusion: HSII为LLM社交能力提供全面评估工具，COT-complexity优化性能与效率平衡。

Abstract: Expanding the application of large language models (LLMs) to societal life,
instead of primary function only as auxiliary assistants to communicate with
only one person at a time, necessitates LLMs' capabilities to independently
play roles in multi-user, multi-turn social agent tasks within complex social
settings. However, currently the capability has not been systematically
measured with available benchmarks. To address this gap, we first introduce an
agent task leveling framework grounded in sociological principles.
Concurrently, we propose a novel benchmark, How Social Is It (we call it HSII
below), designed to assess LLM's social capabilities in comprehensive social
agents tasks and benchmark representative models. HSII comprises four stages:
format parsing, target selection, target switching conversation, and stable
conversation, which collectively evaluate the communication and task completion
capabilities of LLMs within realistic social interaction scenarios dataset,
HSII-Dataset. The dataset is derived step by step from news dataset. We perform
an ablation study by doing clustering to the dataset. Additionally, we
investigate the impact of chain of thought (COT) method on enhancing LLMs'
social performance. Since COT cost more computation, we further introduce a new
statistical metric, COT-complexity, to quantify the efficiency of certain LLMs
with COTs for specific social tasks and strike a better trade-off between
measurement of correctness and efficiency. Various results of our experiments
demonstrate that our benchmark is well-suited for evaluating social skills in
LLMs.

</details>


### [2] [Adaptive Token Boundaries: Integrating Human Chunking Mechanisms into Multimodal LLMs](https://arxiv.org/abs/2505.04637)
*Dongxing Yu*

Main category: cs.CL

TL;DR: 该论文提出了一种动态跨模态标记化框架，模拟人类认知过程，显著提升了多模态大型语言模型在视觉问答和复杂场景描述任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在处理多样化数据类型时与人类认知存在显著差异，研究旨在缩小这一差距。

Method: 通过比较人类表现与模型行为，提出动态跨模态标记化框架，包含自适应边界、分层表示和认知科学对齐机制。

Result: 新框架在基准任务中表现显著优于现有模型（视觉问答+7.8%，复杂场景描述+5.3%），且错误模式和注意力分布更接近人类。

Conclusion: 研究不仅加深了对人类认知与AI关系的理解，还为开发更符合认知的AI系统提供了实证依据。

Abstract: Recent advancements in multimodal large language models (MLLMs) have
demonstrated remarkable capabilities in processing diverse data types, yet
significant disparities persist between human cognitive processes and
computational approaches to multimodal information integration. This research
presents a systematic investigation into the parallels between human
cross-modal chunking mechanisms and token representation methodologies in
MLLMs. Through empirical studies comparing human performance patterns with
model behaviors across visual-linguistic tasks, we demonstrate that
conventional static tokenization schemes fundamentally constrain current
models' capacity to simulate the dynamic, context-sensitive nature of human
information processing. We propose a novel framework for dynamic cross-modal
tokenization that incorporates adaptive boundaries, hierarchical
representations, and alignment mechanisms grounded in cognitive science
principles. Quantitative evaluations demonstrate that our approach yields
statistically significant improvements over state-of-the-art models on
benchmark tasks (+7.8% on Visual Question Answering, +5.3% on Complex Scene
Description) while exhibiting more human-aligned error patterns and attention
distributions. These findings contribute to the theoretical understanding of
the relationship between human cognition and artificial intelligence, while
providing empirical evidence for developing more cognitively plausible AI
systems.

</details>


### [3] [Language translation, and change of accent for speech-to-speech task using diffusion model](https://arxiv.org/abs/2505.04639)
*Abhishek Mishra,Ritesh Sur Chowdhury,Vartul Bahuguna,Isha Pandey,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 本文提出了一种统一的语音翻译和口音适应方法，将问题重构为条件生成任务，结合扩散模型以生成高质量的目标语音，实现翻译和口音适应的联合优化。


<details>
  <summary>Details</summary>
Motivation: 当前的语音到语音翻译（S2ST）通常仅关注语言翻译或口音适应，而跨文化沟通需要同时处理两者。

Method: 通过将任务重构为条件生成问题，结合扩散模型，基于源语音转录和目标语音特征生成Mel频谱图。

Result: 该方法比传统流程更参数高效且效果更好，能够同时优化翻译和口音适应。

Conclusion: 提出的统一框架为同时处理语音翻译和口音适应提供了有效解决方案，填补了当前研究的空白。

Abstract: Speech-to-speech translation (S2ST) aims to convert spoken input in one
language to spoken output in another, typically focusing on either language
translation or accent adaptation. However, effective cross-cultural
communication requires handling both aspects simultaneously - translating
content while adapting the speaker's accent to match the target language
context. In this work, we propose a unified approach for simultaneous speech
translation and change of accent, a task that remains underexplored in current
literature. Our method reformulates the problem as a conditional generation
task, where target speech is generated based on phonemes and guided by target
speech features. Leveraging the power of diffusion models, known for
high-fidelity generative capabilities, we adapt text-to-image diffusion
strategies by conditioning on source speech transcriptions and generating Mel
spectrograms representing the target speech with desired linguistic and
accentual attributes. This integrated framework enables joint optimization of
translation and accent adaptation, offering a more parameter-efficient and
effective model compared to traditional pipelines.

</details>


### [4] [A Comparative Benchmark of a Moroccan Darija Toxicity Detection Model (Typica.ai) and Major LLM-Based Moderation APIs (OpenAI, Mistral, Anthropic)](https://arxiv.org/abs/2505.04640)
*Hicham Assoudi*

Main category: cs.CL

TL;DR: Typica.ai的摩洛哥Darija毒性检测模型在对抗OpenAI、Mistral和Anthropic Claude等主流LLM审核API的基准测试中表现更优，特别是在处理文化相关的隐含侮辱、讽刺和特定攻击方面。


<details>
  <summary>Details</summary>
Motivation: 由于通用系统经常忽视文化相关的毒性内容，如隐含侮辱、讽刺和特定文化攻击，本研究旨在评估Typica.ai针对摩洛哥Darija定制的毒性检测模型的性能，以验证文化适应模型在内容审核中的重要性。

Method: 使用OMCD_Typica.ai_Mix数据集的平衡测试集，比较Typica.ai模型与OpenAI、Mistral和Anthropic Claude审核API的精度、召回率、F1分数和准确率。

Result: Typica.ai模型在所有评估指标上均优于对比的主流API，表明文化适应模型在内容审核中更具优势。

Conclusion: 研究表明，针对特定文化定制的模型在内容审核任务中表现更可靠，突显了文化适应模型在处理少数语言和复杂文化内容时的重要性。

Abstract: This paper presents a comparative benchmark evaluating the performance of
Typica.ai's custom Moroccan Darija toxicity detection model against major
LLM-based moderation APIs: OpenAI (omni-moderation-latest), Mistral
(mistral-moderation-latest), and Anthropic Claude (claude-3-haiku-20240307). We
focus on culturally grounded toxic content, including implicit insults,
sarcasm, and culturally specific aggression often overlooked by general-purpose
systems. Using a balanced test set derived from the OMCD_Typica.ai_Mix dataset,
we report precision, recall, F1-score, and accuracy, offering insights into
challenges and opportunities for moderation in underrepresented languages. Our
results highlight Typica.ai's superior performance, underlining the importance
of culturally adapted models for reliable content moderation.

</details>


### [5] [Rethinking Multimodal Sentiment Analysis: A High-Accuracy, Simplified Fusion Architecture](https://arxiv.org/abs/2505.04642)
*Nischal Mandal,Yang Li*

Main category: cs.CL

TL;DR: 提出一种轻量级但有效的融合深度学习模型，用于多模态情感分析，在六种情绪分类上达到92%的准确率。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析通常使用复杂的注意力机制和层次结构，但作者希望通过简单有效的融合策略实现高性能，尤其在资源有限的环境中。

Method: 使用IEMOCAP数据集，设计特定模态的编码器（全连接层+Dropout），通过简单拼接和多层密集融合捕获跨模态交互。

Result: 在六种情绪分类任务中，模型准确率达到92%，表明简单融合策略可以媲美或超越复杂模型。

Conclusion: 通过精细特征工程和模块化设计，轻量级融合策略在资源受限环境中表现优异，证明了其有效性。

Abstract: Multimodal sentiment analysis, a pivotal task in affective computing, seeks
to understand human emotions by integrating cues from language, audio, and
visual signals. While many recent approaches leverage complex attention
mechanisms and hierarchical architectures, we propose a lightweight, yet
effective fusion-based deep learning model tailored for utterance-level emotion
classification. Using the benchmark IEMOCAP dataset, which includes aligned
text, audio-derived numeric features, and visual descriptors, we design a
modality-specific encoder using fully connected layers followed by dropout
regularization. The modality-specific representations are then fused using
simple concatenation and passed through a dense fusion layer to capture
cross-modal interactions. This streamlined architecture avoids computational
overhead while preserving performance, achieving a classification accuracy of
92% across six emotion categories. Our approach demonstrates that with careful
feature engineering and modular design, simpler fusion strategies can
outperform or match more complex models, particularly in resource-constrained
environments.

</details>


### [6] [Prediction-powered estimators for finite population statistics in highly imbalanced textual data: Public hate crime estimation](https://arxiv.org/abs/2505.04643)
*Hannes Waldetoft,Jakob Torgander,Måns Magnusson*

Main category: cs.CL

TL;DR: 该论文提出了一种结合Transformer神经网络与经典调查抽样估计器的方法，用于高效估计文本文件有限群体中的参数，减少手动标注时间。


<details>
  <summary>Details</summary>
Motivation: 解决在需要手动标注的文本文件有限群体中估计目标变量参数的挑战，尤其针对瑞典仇恨犯罪统计数据。

Method: 将Transformer编码器神经网络的预测结果作为辅助变量，结合Hansen-Hurwitz估计器、差异估计和分层随机抽样估计方法。

Result: 在瑞典警方报告中验证了方法的适用性，成功估计了仇恨犯罪年数量及警方低报情况。

Conclusion: 若有标注训练数据，该方法可在减少手动标注时间的同时提供高效的参数估计。

Abstract: Estimating population parameters in finite populations of text documents can
be challenging when obtaining the labels for the target variable requires
manual annotation. To address this problem, we combine predictions from a
transformer encoder neural network with well-established survey sampling
estimators using the model predictions as an auxiliary variable. The
applicability is demonstrated in Swedish hate crime statistics based on Swedish
police reports. Estimates of the yearly number of hate crimes and the police's
under-reporting are derived using the Hansen-Hurwitz estimator, difference
estimation, and stratified random sampling estimation. We conclude that if
labeled training data is available, the proposed method can provide very
efficient estimates with reduced time spent on manual annotation.

</details>


### [7] [ChatGPT for automated grading of short answer questions in mechanical ventilation](https://arxiv.org/abs/2505.04645)
*Tejas Jade,Alex Yartsev*

Main category: cs.CL

TL;DR: 研究发现ChatGPT 4o在医学研究生短答题评分中表现不佳，与人类评分者一致性差，尤其在评价性和分析性题目上分歧显著。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（如ChatGPT 4o）在研究生医学短答题评分中的实用性，验证其自动化评分的可行性。

Method: 使用ChatGPT 4o对215名学生的557份短答题进行评分，比较其与人工评分的差异，并利用多种统计方法（如ICC、Cohen's kappa等）分析一致性。

Result: ChatGPT评分系统性低于人工评分（平均差异-1.34分），个体一致性差（ICC1=0.086），且60%以上评分超出高风险评估可接受范围。

Conclusion: 不建议在研究生课程评分中使用大型语言模型，因其评分与人工评分存在显著分歧，特别是在复杂题目上。

Abstract: Standardised tests using short answer questions (SAQs) are common in
postgraduate education. Large language models (LLMs) simulate conversational
language and interpret unstructured free-text responses in ways aligning with
applying SAQ grading rubrics, making them attractive for automated grading. We
evaluated ChatGPT 4o to grade SAQs in a postgraduate medical setting using data
from 215 students (557 short-answer responses) enrolled in an online course on
mechanical ventilation (2020--2024). Deidentified responses to three case-based
scenarios were presented to ChatGPT with a standardised grading prompt and
rubric. Outputs were analysed using mixed-effects modelling, variance component
analysis, intraclass correlation coefficients (ICCs), Cohen's kappa, Kendall's
W, and Bland--Altman statistics. ChatGPT awarded systematically lower marks
than human graders with a mean difference (bias) of -1.34 on a 10-point scale.
ICC values indicated poor individual-level agreement (ICC1 = 0.086), and
Cohen's kappa (-0.0786) suggested no meaningful agreement. Variance component
analysis showed minimal variability among the five ChatGPT sessions (G-value =
0.87), indicating internal consistency but divergence from the human grader.
The poorest agreement was observed for evaluative and analytic items, whereas
checklist and prescriptive rubric items had less disagreement. We caution
against the use of LLMs in grading postgraduate coursework. Over 60% of
ChatGPT-assigned grades differed from human grades by more than acceptable
boundaries for high-stakes assessments.

</details>


### [8] [FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights](https://arxiv.org/abs/2505.04649)
*Chengzhang Yu,Yiming Zhang,Zhixin Liu,Zenghui Ding,Yining Sun,Zhanpeng Jin*

Main category: cs.CL

TL;DR: FRAME提出了一种通过迭代反馈和质量评估提升医学论文生成质量的新框架，实验显示其在多个模型和评估维度上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决大语言模型在科学研究和医学论文生成中知识合成和质量保证的挑战。

Method: 采用结构化数据集构建、三部分代理架构（生成器、评估器、反思器）以及统计与人工结合的评估框架。

Result: FRAME在实验中获得显著提升（如DeepSeek V3平均提升9.91%），生成论文质量接近人类水平。

Conclusion: FRAME为自动化医学论文生成提供了高效工具，同时保持了学术严谨性。

Abstract: The automation of scientific research through large language models (LLMs)
presents significant opportunities but faces critical challenges in knowledge
synthesis and quality assurance. We introduce Feedback-Refined Agent
Methodology (FRAME), a novel framework that enhances medical paper generation
through iterative refinement and structured feedback. Our approach comprises
three key innovations: (1) A structured dataset construction method that
decomposes 4,287 medical papers into essential research components through
iterative refinement; (2) A tripartite architecture integrating Generator,
Evaluator, and Reflector agents that progressively improve content quality
through metric-driven feedback; and (3) A comprehensive evaluation framework
that combines statistical metrics with human-grounded benchmarks. Experimental
results demonstrate FRAME's effectiveness, achieving significant improvements
over conventional approaches across multiple models (9.91% average gain with
DeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation
dimensions. Human evaluation confirms that FRAME-generated papers achieve
quality comparable to human-authored works, with particular strength in
synthesizing future research directions. The results demonstrated our work
could efficiently assist medical research by building a robust foundation for
automated medical research paper generation while maintaining rigorous academic
standards.

</details>


### [9] [Scientific Hypothesis Generation and Validation: Methods, Datasets, and Future Directions](https://arxiv.org/abs/2505.04651)
*Adithya Kulkarni,Fatimah Alotaibi,Xinyue Zeng,Longfeng Wu,Tong Zeng,Barry Menglong Yao,Minqian Liu,Shuaicheng Zhang,Lifu Huang,Dawei Zhou*

Main category: cs.CL

TL;DR: 这篇论文综述了基于大语言模型（LLMs）在科学假设生成和验证中的方法和应用，对比了传统符号系统与现代LLM流程的优缺点，并提出了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs如何通过信息综合、潜在关系发现和推理增强来推动科学假设的生成和验证，填补现有研究的空白。

Method: 结构化综述了LLM驱动的方法，包括符号框架、生成模型、混合系统、多智能体架构，以及检索增强生成、知识图谱补全等技术。

Result: 总结了LLM在不同领域的应用（如生物医学、社会科学）和验证方法（如模拟、人机协作），并提出了新的资源（如AHTech）。

Conclusion: 未来LLMs应注重新颖性生成、多模态符号集成、人机协作系统及伦理保障，以支持规模化、原则性的科学发现。

Abstract: Large Language Models (LLMs) are transforming scientific hypothesis
generation and validation by enabling information synthesis, latent
relationship discovery, and reasoning augmentation. This survey provides a
structured overview of LLM-driven approaches, including symbolic frameworks,
generative models, hybrid systems, and multi-agent architectures. We examine
techniques such as retrieval-augmented generation, knowledge-graph completion,
simulation, causal inference, and tool-assisted reasoning, highlighting
trade-offs in interpretability, novelty, and domain alignment. We contrast
early symbolic discovery systems (e.g., BACON, KEKADA) with modern LLM
pipelines that leverage in-context learning and domain adaptation via
fine-tuning, retrieval, and symbolic grounding. For validation, we review
simulation, human-AI collaboration, causal modeling, and uncertainty
quantification, emphasizing iterative assessment in open-world contexts. The
survey maps datasets across biomedicine, materials science, environmental
science, and social science, introducing new resources like AHTech and
CSKG-600. Finally, we outline a roadmap emphasizing novelty-aware generation,
multimodal-symbolic integration, human-in-the-loop systems, and ethical
safeguards, positioning LLMs as agents for principled, scalable scientific
discovery.

</details>


### [10] [Advancing Conversational Diagnostic AI with Multimodal Reasoning](https://arxiv.org/abs/2505.04653)
*Khaled Saab,Jan Freyberg,Chunjong Park,Tim Strother,Yong Cheng,Wei-Hung Weng,David G. T. Barrett,David Stutz,Nenad Tomasev,Anil Palepu,Valentin Liévin,Yash Sharma,Roma Ruparel,Abdullah Ahmed,Elahe Vedadi,Kimberly Kanada,Cian Hughes,Yun Liu,Geoff Brown,Yang Gao,Sean Li,S. Sara Mahdavi,James Manyika,Katherine Chou,Yossi Matias,Avinatan Hassidim,Dale R. Webster,Pushmeet Kohli,S. M. Ali Eslami,Joëlle Barral,Adam Rodman,Vivek Natarajan,Mike Schaekermann,Tao Tu,Alan Karthikesalingam,Ryutaro Tanno*

Main category: cs.CL

TL;DR: 論文摘要總結了AMIE系統在多模態醫療對話診斷中的創新，通過整合Gemini 2.0 Flash技術實現了對多模態數據的動態推理，並在與初級醫生的對比研究中表現優異。


<details>
  <summary>Details</summary>
Motivation: 現有大型語言模型（LLMs）在遠程醫療中的評估多限於純文字互動，忽視了真實醫療場景中多模態數據（如影像、文檔等）的需求。

Method: AMIE系統採用狀態感知對話框架，動態控制會話流程，並基於患者狀態和診斷不確定性生成後續問題，模擬了資深醫生的多模態病史採集過程。

Result: 在105個模擬諮詢場景中，AMIE在多模態能力（7/9維度）及非多模態維度（如診斷準確性，29/32維度）上均顯著優於初級醫生。

Conclusion: 研究證明了多模態對話診斷AI的潛力，但實際應用仍需進一步驗證。

Abstract: Large Language Models (LLMs) have demonstrated great potential for conducting
diagnostic conversations but evaluation has been largely limited to
language-only interactions, deviating from the real-world requirements of
remote care delivery. Instant messaging platforms permit clinicians and
patients to upload and discuss multimodal medical artifacts seamlessly in
medical consultation, but the ability of LLMs to reason over such data while
preserving other attributes of competent diagnostic conversation remains
unknown. Here we advance the conversational diagnosis and management
performance of the Articulate Medical Intelligence Explorer (AMIE) through a
new capability to gather and interpret multimodal data, and reason about this
precisely during consultations. Leveraging Gemini 2.0 Flash, our system
implements a state-aware dialogue framework, where conversation flow is
dynamically controlled by intermediate model outputs reflecting patient states
and evolving diagnoses. Follow-up questions are strategically directed by
uncertainty in such patient states, leading to a more structured multimodal
history-taking process that emulates experienced clinicians. We compared AMIE
to primary care physicians (PCPs) in a randomized, blinded, OSCE-style study of
chat-based consultations with patient actors. We constructed 105 evaluation
scenarios using artifacts like smartphone skin photos, ECGs, and PDFs of
clinical documents across diverse conditions and demographics. Our rubric
assessed multimodal capabilities and other clinically meaningful axes like
history-taking, diagnostic accuracy, management reasoning, communication, and
empathy. Specialist evaluation showed AMIE to be superior to PCPs on 7/9
multimodal and 29/32 non-multimodal axes (including diagnostic accuracy). The
results show clear progress in multimodal conversational diagnostic AI, but
real-world translation needs further research.

</details>


### [11] [A Comparative Analysis of Ethical and Safety Gaps in LLMs using Relative Danger Coefficient](https://arxiv.org/abs/2505.04654)
*Yehor Tereshchenko,Mika Hämäläinen*

Main category: cs.CL

TL;DR: 该论文探讨了AI和大语言模型（LLMs）的伦理问题，提出了一种衡量LLM潜在危害的新指标RDC，并比较了多种模型的伦理表现。


<details>
  <summary>Details</summary>
Motivation: 随着AI和LLMs的快速发展，其伦理问题（如安全性、滥用、歧视等）日益凸显，需要有效的评估和监管机制。

Method: 通过比较分析DeepSeek-V3、GPT系列和Gemini等模型的伦理表现，并提出了新的危害评估指标RDC。

Result: 研究发现现有的AI模型在高风险场景中需要更强的人类监督，RDC为量化LLM潜在危害提供了新工具。

Conclusion: 论文呼吁加强AI伦理监管，强调在关键领域引入人类监督的重要性，RDC有助于评估模型的伦理风险。

Abstract: Artificial Intelligence (AI) and Large Language Models (LLMs) have rapidly
evolved in recent years, showcasing remarkable capabilities in natural language
understanding and generation. However, these advancements also raise critical
ethical questions regarding safety, potential misuse, discrimination and
overall societal impact. This article provides a comparative analysis of the
ethical performance of various AI models, including the brand new
DeepSeek-V3(R1 with reasoning and without), various GPT variants (4o, 3.5
Turbo, 4 Turbo, o1/o3 mini) and Gemini (1.5 flash, 2.0 flash and 2.0 flash exp)
and highlights the need for robust human oversight, especially in situations
with high stakes. Furthermore, we present a new metric for calculating harm in
LLMs called Relative Danger Coefficient (RDC).

</details>


### [12] [Integration of Large Language Models and Traditional Deep Learning for Social Determinants of Health Prediction](https://arxiv.org/abs/2505.04655)
*Paul Landes,Jimeng Sun,Adam Cross*

Main category: cs.CL

TL;DR: 论文提出一种结合深度学习和大语言模型（LLM）的方法，自动从临床文本中提取健康社会决定因素（SDoH），在分类任务上表现优于基准10个百分点，并通过优化方法将执行时间缩短12倍。


<details>
  <summary>Details</summary>
Motivation: SDoH对健康状况有重要影响，但传统方法效率低且成本高。论文旨在通过结合LLM的精准性和深度学习的高效性，提供更优的自动提取方案。

Method: 采用混合方法：用LLM保证精度，传统深度学习提升效率。还测试了合成数据增强和多种深度学习模型，部分模型表现优于LLM。

Result: 模型在SDoH多标签分类任务上超出基准10点，执行时间快12倍，合成数据增强后性能进一步提升。

Conclusion: 混合方法实现了高效精准的SDoH自动预测，为风险患者提供了更优解决方案。

Abstract: Social Determinants of Health (SDoH) are economic, social and personal
circumstances that affect or influence an individual's health status. SDoHs
have shown to be correlated to wellness outcomes, and therefore, are useful to
physicians in diagnosing diseases and in decision-making. In this work, we
automatically extract SDoHs from clinical text using traditional deep learning
and Large Language Models (LLMs) to find the advantages and disadvantages of
each on an existing publicly available dataset. Our models outperform a
previous reference point on a multilabel SDoH classification by 10 points, and
we present a method and model to drastically speed up classification (12X
execution time) by eliminating expensive LLM processing. The method we present
combines a more nimble and efficient solution that leverages the power of the
LLM for precision and traditional deep learning methods for efficiency. We also
show highly performant results on a dataset supplemented with synthetic data
and several traditional deep learning models that outperform LLMs. Our models
and methods offer the next iteration of automatic prediction of SDoHs that
impact at-risk patients.

</details>


### [13] [AI-Generated Fall Data: Assessing LLMs and Diffusion Model for Wearable Fall Detection](https://arxiv.org/abs/2505.04660)
*Sana Alamgeer,Yasine Souissi,Anne H. H. Ngu*

Main category: cs.CL

TL;DR: 研究探索了使用LLM生成合成跌倒数据以解决真实数据稀缺的问题，评估了多种文本到运动和文本到文本模型的效果。结果显示合成数据的有效性受数据集特性影响显著，不同模型的性能表现各异。扩散模型数据与真实数据最为接近，但未始终提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于老年人群真实跌倒数据的稀缺性，研究旨在探索利用LLM生成合成数据以增强跌倒检测系统的训练效果。

Method: 研究评估了文本到运动（T2M、SATO、ParCo）和文本到文本模型（GPT4o、GPT4、Gemini）生成合成数据的潜力，并对比了扩散模型生成的数据。最终使用LSTM模型评估合成数据对跌倒检测性能的影响。

Result: 合成数据的效果受数据集频率影响（如LLM数据在20Hz表现最佳），文本到运动模型生成的生物力学数据更真实但效果不稳定。扩散模型数据与真实数据分布最接近，但未显著提升模型性能。

Conclusion: 研究强调了合成数据生成的优化方向，指出传感器位置和跌倒表示对效果的关键影响，为改进跌倒检测模型提供了参考。

Abstract: Training fall detection systems is challenging due to the scarcity of
real-world fall data, particularly from elderly individuals. To address this,
we explore the potential of Large Language Models (LLMs) for generating
synthetic fall data. This study evaluates text-to-motion (T2M, SATO, ParCo) and
text-to-text models (GPT4o, GPT4, Gemini) in simulating realistic fall
scenarios. We generate synthetic datasets and integrate them with four
real-world baseline datasets to assess their impact on fall detection
performance using a Long Short-Term Memory (LSTM) model. Additionally, we
compare LLM-generated synthetic data with a diffusion-based method to evaluate
their alignment with real accelerometer distributions. Results indicate that
dataset characteristics significantly influence the effectiveness of synthetic
data, with LLM-generated data performing best in low-frequency settings (e.g.,
20Hz) while showing instability in high-frequency datasets (e.g., 200Hz). While
text-to-motion models produce more realistic biomechanical data than
text-to-text models, their impact on fall detection varies. Diffusion-based
synthetic data demonstrates the closest alignment to real data but does not
consistently enhance model performance. An ablation study further confirms that
the effectiveness of synthetic data depends on sensor placement and fall
representation. These findings provide insights into optimizing synthetic data
generation for fall detection models.

</details>


### [14] [Personalized Risks and Regulatory Strategies of Large Language Models in Digital Advertising](https://arxiv.org/abs/2505.04665)
*Haoyang Feng,Yanjun Dai,Yuan Gao*

Main category: cs.CL

TL;DR: 论文研究了结合隐私保护的大语言模型在数字广告中的应用，提出了一种基于BERT的个性化推荐与风险保护算法，并通过实验验证其效果。


<details>
  <summary>Details</summary>
Motivation: 探索如何在个性化广告推荐中结合隐私保护与数据安全，以解决实际运营中的隐私风险问题。

Method: 结合BERT模型与注意力机制，构建算法模型，通过数据预处理、特征选择、语义嵌入、本地训练和加密技术实现推荐与隐私保护。

Result: 实验表明，基于BERT的广告推送提高了点击率和转化率，同时通过本地训练和隐私机制降低了隐私泄露风险。

Conclusion: 该方法为个性化广告推荐与隐私保护的结合提供了可行方案，但仍需进一步优化以应对实际应用的复杂性。

Abstract: Although large language models have demonstrated the potential for
personalized advertising recommendations in experimental environments, in
actual operations, how advertising recommendation systems can be combined with
measures such as user privacy protection and data security is still an area
worthy of in-depth discussion. To this end, this paper studies the personalized
risks and regulatory strategies of large language models in digital
advertising. This study first outlines the principles of Large Language Model
(LLM), especially the self-attention mechanism based on the Transformer
architecture, and how to enable the model to understand and generate natural
language text. Then, the BERT (Bidirectional Encoder Representations from
Transformers) model and the attention mechanism are combined to construct an
algorithmic model for personalized advertising recommendations and user factor
risk protection. The specific steps include: data collection and preprocessing,
feature selection and construction, using large language models such as BERT
for advertising semantic embedding, and ad recommendations based on user
portraits. Then, local model training and data encryption are used to ensure
the security of user privacy and avoid the leakage of personal data. This paper
designs an experiment for personalized advertising recommendation based on a
large language model of BERT and verifies it with real user data. The
experimental results show that BERT-based advertising push can effectively
improve the click-through rate and conversion rate of advertisements. At the
same time, through local model training and privacy protection mechanisms, the
risk of user privacy leakage can be reduced to a certain extent.

</details>


### [15] [Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes](https://arxiv.org/abs/2505.04666)
*Mohammad Aqib,Mohd Hamza,Qipei Mei,Ying Hei Chui*

Main category: cs.CL

TL;DR: 该论文提出使用检索增强生成（RAG）系统来自动查询复杂的建筑规范，通过比较不同检索方法和微调语言模型，发现Elasticsearch是最佳检索工具，且微调能提高语言模型的领域适应性。


<details>
  <summary>Details</summary>
Motivation: 建筑规范内容庞大、技术性强且频繁更新，手动查询效率低下。通过构建基于RAG的问答系统，可以高效解决用户查询问题。

Method: 研究比较了多种检索方法在加拿大国家建筑规范（NBCC）上的表现，并利用NBCC数据集对语言模型进行微调，以优化生成能力。

Result: 实验表明Elasticsearch是最优检索工具，且微调显著提升了语言模型在NBCC领域的回答质量。

Conclusion: 结合高效检索和微调语言模型的RAG系统，能有效应对建筑规范的复杂性，提升查询效率。

Abstract: Building codes are regulations that establish standards for the design,
construction, and safety of buildings to ensure structural integrity, fire
protection, and accessibility. They are often extensive, complex, and subject
to frequent updates, making manual querying challenging and time-consuming. Key
difficulties include navigating large volumes of text, interpreting technical
language, and identifying relevant clauses across different sections. A
potential solution is to build a Question-Answering (QA) system that answers
user queries based on building codes. Among the various methods for building a
QA system, Retrieval-Augmented Generation (RAG) stands out in performance. RAG
consists of two components: a retriever and a language model. This study
focuses on identifying a suitable retriever method for building codes and
optimizing the generational capability of the language model using fine-tuning
techniques. We conducted a detailed evaluation of various retrieval methods by
performing the retrieval on the National Building Code of Canada (NBCC) and
explored the impact of domain-specific fine-tuning on several language models
using the dataset derived from NBCC. Our analysis included a comparative
assessment of different retrievers and the performance of both pre-trained and
fine-tuned models to determine the efficacy and domain-specific adaptation of
language models using fine-tuning on the NBCC dataset. Experimental results
showed that Elasticsearch proved to be the most robust retriever among all. The
findings also indicate that fine-tuning language models on an NBCC-specific
dataset can enhance their ability to generate contextually relevant responses.
When combined with context retrieved by a powerful retriever like
Elasticsearch, this improvement in LLM performance can optimize the RAG system,
enabling it to better navigate the complexities of the NBCC.

</details>


### [16] [Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards](https://arxiv.org/abs/2505.04671)
*Yuxin Zhang,Meihao Fan,Ju Fan,Mingyang Yi,Yuyu Luo,Jian Tan,Guoliang Li*

Main category: cs.CL

TL;DR: 论文提出Reward-SQL框架，通过引入外部过程奖励模型（PRM）优化Text-to-SQL任务中的推理过程，结合冷启动和PRM监督策略，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在引入过程奖励模型（PRM）时可能扭曲推理轨迹，导致SQL生成不准确。本文旨在探索如何有效整合PRM以提升Text-to-SQL任务的表现。

Method: 采用“冷启动后PRM监督”范式：先通过Chain-of-CTEs训练模型分解SQL查询为结构化推理链，再研究四种PRM集成策略，最终选择GRPO与PRM引导推理结合的方式。

Result: 在BIRD基准测试中，Reward-SQL使7B参数的PRM监督模型性能提升13.1%，基于Qwen2.5-Coder-7B-Instruct的GRPO策略模型开发集准确率达68.9%，优于同规模基线方法。

Conclusion: Reward-SQL通过奖励监督有效提升Text-to-SQL推理性能，证明了PRM在细粒度监督中的潜力。

Abstract: Recent advances in large language models (LLMs) have significantly improved
performance on the Text-to-SQL task by leveraging their powerful reasoning
capabilities. To enhance accuracy during the reasoning process, external
Process Reward Models (PRMs) can be introduced during training and inference to
provide fine-grained supervision. However, if misused, PRMs may distort the
reasoning trajectory and lead to suboptimal or incorrect SQL generation.To
address this challenge, we propose Reward-SQL, a framework that systematically
explores how to incorporate PRMs into the Text-to-SQL reasoning process
effectively. Our approach follows a "cold start, then PRM supervision"
paradigm. Specifically, we first train the model to decompose SQL queries into
structured stepwise reasoning chains using common table expressions
(Chain-of-CTEs), establishing a strong and interpretable reasoning baseline.
Then, we investigate four strategies for integrating PRMs, and find that
combining PRM as an online training signal (GRPO) with PRM-guided inference
(e.g., best-of-N sampling) yields the best results. Empirically, on the BIRD
benchmark, Reward-SQL enables models supervised by a 7B PRM to achieve a 13.1%
performance gain across various guidance strategies. Notably, our GRPO-aligned
policy model based on Qwen2.5-Coder-7B-Instruct achieves 68.9% accuracy on the
BIRD development set, outperforming all baseline methods under the same model
size. These results demonstrate the effectiveness of Reward-SQL in leveraging
reward-based supervision for Text-to-SQL reasoning. Our code is publicly
available.

</details>


### [17] [REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM](https://arxiv.org/abs/2505.04673)
*Madhur Jindal,Saurabh Deshpande*

Main category: cs.CL

TL;DR: 论文提出REVEAL框架，用于评估视觉大语言模型(VLLMs)在多轮对话中的安全风险，测试了五种模型并发现多轮对话的缺陷率显著高于单轮。GPT-4o在安全-可用性指数上表现最优。


<details>
  <summary>Details</summary>
Motivation: VLLMs结合图像与文本理解能力，但传统安全评估方法不适用于其多模态和多轮对话的复杂性，因此提出了REVEAL框架。

Method: REVEAL框架包括自动图像挖掘、合成对抗数据生成、多轮对话扩展和全面危害评估（使用GPT-4o等评估工具）。测试了五种VLLMs模型。

Result: 多轮对话的缺陷率明显更高，GPT-4o在安全性上表现最佳，而Llama-3.2缺陷率最高，Qwen2-VL拒绝率最高。

Conclusion: 研究揭示VLLMs在多轮对话中安全漏洞更显著，需加强防御，尤其是针对错误信息。GPT-4o和Pixtral表现较优。

Abstract: Vision Large Language Models (VLLMs) represent a significant advancement in
artificial intelligence by integrating image-processing capabilities with
textual understanding, thereby enhancing user interactions and expanding
application domains. However, their increased complexity introduces novel
safety and ethical challenges, particularly in multi-modal and multi-turn
conversations. Traditional safety evaluation frameworks, designed for
text-based, single-turn interactions, are inadequate for addressing these
complexities. To bridge this gap, we introduce the REVEAL (Responsible
Evaluation of Vision-Enabled AI LLMs) Framework, a scalable and automated
pipeline for evaluating image-input harms in VLLMs. REVEAL includes automated
image mining, synthetic adversarial data generation, multi-turn conversational
expansion using crescendo attack strategies, and comprehensive harm assessment
through evaluators like GPT-4o.
  We extensively evaluated five state-of-the-art VLLMs, GPT-4o, Llama-3.2,
Qwen2-VL, Phi3.5V, and Pixtral, across three important harm categories: sexual
harm, violence, and misinformation. Our findings reveal that multi-turn
interactions result in significantly higher defect rates compared to
single-turn evaluations, highlighting deeper vulnerabilities in VLLMs. Notably,
GPT-4o demonstrated the most balanced performance as measured by our
Safety-Usability Index (SUI) followed closely by Pixtral. Additionally,
misinformation emerged as a critical area requiring enhanced contextual
defenses. Llama-3.2 exhibited the highest MT defect rate ($16.55 \%$) while
Qwen2-VL showed the highest MT refusal rate ($19.1 \%$).

</details>


### [18] [Advanced Deep Learning Approaches for Automated Recognition of Cuneiform Symbols](https://arxiv.org/abs/2505.04678)
*Shahad Elshehaby,Alavikunhu Panthakkan,Hussain Al-Ahmad,Mina Al-Saad*

Main category: cs.CL

TL;DR: 本文提出了一种基于深度学习算法的全自动化方法，用于识别和解释楔形文字字符。五种不同的深度学习模型在一个全面的楔形文字数据集上进行了训练，并根据准确率和精确度等关键性能指标进行了评估。其中两个模型表现优异，随后使用《汉谟拉比法典》中的符号进行了进一步测试。这些模型有效地识别了符号的阿卡德语含义，并提供了准确的英语翻译。未来工作将探索集成和堆叠方法来优化性能，同时研究阿卡德语与阿拉伯语之间的语言关系。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于利用深度学习和计算语言学技术来自动识别和解释古老的楔形文字，从而帮助理解和保护人类历史。同时，探索阿卡德语与阿拉伯语之间的历史和文化联系。

Method: 采用了五种不同的深度学习模型，训练于一个全面的楔形文字数据集，并通过准确率和精确度等指标进行评估。表现最佳的两个模型进一步用于《汉谟拉比法典》符号的识别和翻译。

Result: 两个表现最佳的深度学习模型成功识别了楔形文字的阿卡德语含义，并提供了精确的英语翻译，展示了深度学习在解读古代文字上的潜力。

Conclusion: 研究表明，深度学习和计算语言学的结合能够有效解读古代楔形文字。未来研究可通过集成方法和混合架构进一步提升性能，为历史学和考古学提供更多见解。

Abstract: This paper presents a thoroughly automated method for identifying and
interpreting cuneiform characters via advanced deep-learning algorithms. Five
distinct deep-learning models were trained on a comprehensive dataset of
cuneiform characters and evaluated according to critical performance metrics,
including accuracy and precision. Two models demonstrated outstanding
performance and were subsequently assessed using cuneiform symbols from the
Hammurabi law acquisition, notably Hammurabi Law 1. Each model effectively
recognized the relevant Akkadian meanings of the symbols and delivered precise
English translations. Future work will investigate ensemble and stacking
approaches to optimize performance, utilizing hybrid architectures to improve
detection accuracy and reliability. This research explores the linguistic
relationships between Akkadian, an ancient Mesopotamian language, and Arabic,
emphasizing their historical and cultural linkages. This study demonstrates the
capability of deep learning to decipher ancient scripts by merging
computational linguistics with archaeology, therefore providing significant
insights for the comprehension and conservation of human history.

</details>


### [19] [SOAEsV2-7B/72B: Full-Pipeline Optimization for State-Owned Enterprise LLMs via Continual Pre-Training, Domain-Progressive SFT and Distillation-Enhanced Speculative Decoding](https://arxiv.org/abs/2505.04723)
*Jingyang Deng,Ran Chen,Jo-Ku Cheng,Jinwen Ma*

Main category: cs.CL

TL;DR: 该研究提出SOAEsV2-7B/72B，一个针对中国国有资产的领域专用大语言模型系列，通过三阶段框架（持续预训练、领域渐进式监督微调和蒸馏增强推理加速）解决了模型容量限制、过度依赖领域数据和推理效率低的问题，显著提升了领域性能并保持了通用语言能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理中国国有资产和企业的领域专用大语言模型时面临模型容量限制、过度依赖领域数据和推理效率低的挑战。

Method: 提出三阶段框架：1) 持续预训练整合领域知识；2) 领域渐进式监督微调采用课程学习策略；3) 蒸馏增强的推测解码加速推理。

Result: 领域性能显著提升（Rouge-1提高1.08倍，BLEU-4提高1.17倍），同时保留了99.8%的通用语言能力，推理速度提升1.39-1.52倍。

Conclusion: 该工作填补了通用语言能力和领域专业知识之间的空白，为优化国有资产领域专用大语言模型提供了全面的全流程方法。

Abstract: This study addresses key challenges in developing domain-specific large
language models (LLMs) for Chinese state-owned assets and enterprises (SOAEs),
where current approaches face three limitations: 1) constrained model capacity
that limits knowledge integration and cross-task adaptability; 2) excessive
reliance on domain-specific supervised fine-tuning (SFT) data, which neglects
the broader applicability of general language patterns; and 3) inefficient
inference acceleration for large models processing long contexts. In this work,
we propose SOAEsV2-7B/72B, a specialized LLM series developed via a three-phase
framework: 1) continual pre-training integrates domain knowledge while
retaining base capabilities; 2) domain-progressive SFT employs curriculum-based
learning strategy, transitioning from weakly relevant conversational data to
expert-annotated SOAEs datasets to optimize domain-specific tasks; 3)
distillation-enhanced speculative decoding accelerates inference via logit
distillation between 72B target and 7B draft models, achieving
1.39-1.52$\times$ speedup without quality loss. Experimental results
demonstrate that our domain-specific pre-training phase maintains 99.8% of
original general language capabilities while significantly improving domain
performance, resulting in a 1.08$\times$ improvement in Rouge-1 score and a
1.17$\times$ enhancement in BLEU-4 score. Ablation studies further show that
domain-progressive SFT outperforms single-stage training, achieving
1.02$\times$ improvement in Rouge-1 and 1.06$\times$ in BLEU-4. Our work
introduces a comprehensive, full-pipeline approach for optimizing SOAEs LLMs,
bridging the gap between general language capabilities and domain-specific
expertise.

</details>


### [20] [Flower Across Time and Media: Sentiment Analysis of Tang Song Poetry and Visual Correspondence](https://arxiv.org/abs/2505.04785)
*Shuai Gong,Tiange Zhou*

Main category: cs.CL

TL;DR: 本研究结合BERT情感分析和传统汉学方法，量化分析了唐宋诗词中花意象的情感模式，并将其与装饰艺术发展对比，揭示了文学表达与艺术表现之间未被认识的协同关系。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究分别探讨了唐宋时期的文学和艺术，但文学情感与视觉文化之间的系统性关联尚未充分研究。本研究旨在填补这一空白。

Method: 研究使用了基于BERT的情感分析模型，对唐宋诗词中的牡丹和梅花意象进行情感分析，并与同期的纺织品、陶瓷等物质文化进行交叉验证。

Result: 研究发现唐宋时期花意象的情感内涵存在可测量的变化，并通过视觉证据揭示了文学与艺术表现之间的协同关系。

Conclusion: 研究为理解唐宋时期文学与艺术的互动提供了新视角，同时展示了计算人文方法在传统汉学研究中的潜力。

Abstract: The Tang (618 to 907) and Song (960 to 1279) dynasties witnessed an
extraordinary flourishing of Chinese cultural expression, where floral motifs
served as a dynamic medium for both poetic sentiment and artistic design. While
previous scholarship has examined these domains independently, the systematic
correlation between evolving literary emotions and visual culture remains
underexplored. This study addresses that gap by employing BERT-based sentiment
analysis to quantify emotional patterns in floral imagery across Tang Song
poetry, then validating these patterns against contemporaneous developments in
decorative arts.Our approach builds upon recent advances in computational
humanities while remaining grounded in traditional sinological methods. By
applying a fine tuned BERT model to analyze peony and plum blossom imagery in
classical poetry, we detect measurable shifts in emotional connotations between
the Tang and Song periods. These textual patterns are then cross berenced with
visual evidence from textiles, ceramics, and other material culture, revealing
previously unrecognized synergies between literary expression and artistic
representation.

</details>


### [21] [Osiris: A Lightweight Open-Source Hallucination Detection System](https://arxiv.org/abs/2505.04844)
*Alex Shan,John Bauer,Christopher D. Manning*

Main category: cs.CL

TL;DR: 论文提出了一种基于微调7B模型的幻觉检测方法，在RAGTruth基准测试中优于GPT-4o，且参数更少。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统中的幻觉问题阻碍了其实际部署，现有检测方法（如人工评估或闭源模型）成本高且难以扩展。

Method: 通过监督微调在多跳QA数据集上训练7B模型，数据集包含诱导的幻觉样本。

Result: 微调后的7B模型在幻觉检测的召回率上优于GPT-4o，并在精确度和准确度上表现竞争性。

Conclusion: 该方法以更少参数实现了高效幻觉检测，解决了现有方法的扩展性问题。

Abstract: Retrieval-Augmented Generation (RAG) systems have gained widespread adoption
by application builders because they leverage sources of truth to enable Large
Language Models (LLMs) to generate more factually sound responses. However,
hallucinations, instances of LLM responses that are unfaithful to the provided
context, often prevent these systems from being deployed in production
environments. Current hallucination detection methods typically involve human
evaluation or the use of closed-source models to review RAG system outputs for
hallucinations. Both human evaluators and closed-source models suffer from
scaling issues due to their high costs and slow inference speeds. In this work,
we introduce a perturbed multi-hop QA dataset with induced hallucinations. Via
supervised fine-tuning on our dataset, we achieve better recall with a 7B model
than GPT-4o on the RAGTruth hallucination detection benchmark and offer
competitive performance on precision and accuracy, all while using a fraction
of the parameters. Code is released at our repository.

</details>


### [22] [Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards](https://arxiv.org/abs/2505.04847)
*Manveer Singh Tamber,Forrest Sheng Bao,Chenyu Xu,Ge Luo,Suleman Kazi,Minseok Bae,Miaoran Li,Ofer Mendelevitch,Renyi Qu,Jimmy Lin*

Main category: cs.CL

TL;DR: 该论文探讨了大型语言模型（LLM）在生成摘要时的幻觉问题，分析了现有评估方法如HHEM和Vectara幻觉排行榜的局限性，并提出了一种名为FaithJudge的新方法，通过少样本人工标注指导LLM作为评判者，显著提升了幻觉评估的准确性。


<details>
  <summary>Details</summary>
Motivation: LLM在生成内容时经常出现幻觉（即引入不支持的或矛盾的信息），即使在使用RAG（检索增强生成）技术时也是如此。现有评估方法如HHEM存在不足，需要更可靠的评估手段。

Method: 论文提出了FaithJudge方法，利用少样本人工标注指导LLM作为评判者，评估LLM生成的摘要是否存在幻觉，并改进了幻觉排行榜。

Result: FaithJudge显著优于现有的幻觉检测方法，为RAG任务中的LLM幻觉评估提供了更可靠的基准。

Conclusion: FaithJudge是一种有效的LLM幻觉评估方法，为未来的研究和应用提供了更准确的工具。

Abstract: Hallucinations remain a persistent challenge for LLMs. RAG aims to reduce
hallucinations by grounding responses in contexts. However, even when provided
context, LLMs still frequently introduce unsupported information or
contradictions. This paper presents our efforts to measure LLM hallucinations
with a focus on summarization tasks, assessing how often various LLMs introduce
hallucinations when summarizing documents. We discuss Vectara's existing LLM
hallucination leaderboard, based on the Hughes Hallucination Evaluation Model
(HHEM). While HHEM and Vectara's Hallucination Leaderboard have garnered great
research interest, we examine challenges faced by HHEM and current
hallucination detection methods by analyzing the effectiveness of these methods
on existing hallucination datasets. To address these limitations, we propose
FaithJudge, an LLM-as-a-judge approach guided by few-shot human hallucination
annotations, which substantially improves automated LLM hallucination
evaluation over current methods. We introduce an enhanced hallucination
leaderboard centered on FaithJudge, alongside our current hallucination
leaderboard, enabling more reliable benchmarking of LLMs for hallucinations in
RAG.

</details>


### [23] [An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in Higher Education](https://arxiv.org/abs/2505.04916)
*Ramteja Sajja,Yusuf Sermet,Ibrahim Demir*

Main category: cs.CL

TL;DR: 研究提出了两种针对教育问答优化的开源嵌入模型，通过结合手动标注和LLM生成的数据集训练，展示了优于现有开源基准的性能，并缩小了与商业模型的差距。


<details>
  <summary>Details</summary>
Motivation: 当前AI教育工具的语义检索系统对学术内容的语言和结构特征适应性不足，需改进。

Method: 构建合成数据集，评估两种训练策略：基准模型（MNRL损失）和双损失模型（MNRL+余弦相似损失）。

Result: 两种模型均超越开源基准，双损失模型接近商业模型性能。

Conclusion: 贡献了可复用的教育领域对齐嵌入模型及框架，支持学术聊天机器人和RAG等应用。

Abstract: Recent advances in AI have catalyzed the adoption of intelligent educational
tools, yet many semantic retrieval systems remain ill-suited to the unique
linguistic and structural characteristics of academic content. This study
presents two open-source embedding models fine-tuned for educational question
answering, particularly in the context of course syllabi. A synthetic dataset
of 3,197 sentence pairs, spanning synonymous terminology, paraphrased
questions, and implicit-explicit mappings, was constructed through a
combination of manual curation and large language model (LLM)-assisted
generation. Two training strategies were evaluated: (1) a baseline model
fine-tuned using MultipleNegativesRankingLoss (MNRL), and (2) a dual-loss model
that combines MNRL with CosineSimilarityLoss to improve both semantic ranking
and similarity calibration. Evaluations were conducted on 28 university course
syllabi using a fixed set of natural language questions categorized into
course, faculty, and teaching assistant information. Results demonstrate that
both fine-tuned models outperform strong open-source baselines, including
all-MiniLM-L6-v2 and multi-qa-MiniLM-L6-cos-v1, and that the dual-loss model
narrows the performance gap with high-performing proprietary embeddings such as
OpenAI's text-embedding-3 series. This work contributes reusable,
domain-aligned embedding models and provides a replicable framework for
educational semantic retrieval, supporting downstream applications such as
academic chatbots, retrieval-augmented generation (RAG) systems, and learning
management system (LMS) integrations.

</details>


### [24] [Chain-of-Thought Tokens are Computer Program Variables](https://arxiv.org/abs/2505.04955)
*Fangwei Zhu,Peiyi Wang,Zhifang Sui*

Main category: cs.CL

TL;DR: 论文探讨了思维链（CoT）在大型语言模型中的内在机制，发现仅保留存储中间结果的CoT标记即可达到类似效果，且这些标记可能类似于程序中的变量。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示CoT在大型语言模型中如何发挥作用，尤其是在复杂推理任务中的内部机制。

Method: 作者通过两个组合任务（多位乘法和动态规划）实证研究了CoT标记的作用，包括保留中间结果标记、替换潜在形式及随机干预CoT值。

Result: 研究发现仅保留中间结果标记即可保持模型性能，且CoT标记可能类似于程序变量，但也存在潜在缺点如计算复杂度限制。

Conclusion: 研究揭示了CoT标记的功能特性，为理解其机制提供了新视角，并指出了未来可能的改进方向。

Abstract: Chain-of-thoughts (CoT) requires large language models (LLMs) to generate
intermediate steps before reaching the final answer, and has been proven
effective to help LLMs solve complex reasoning tasks. However, the inner
mechanism of CoT still remains largely unclear. In this paper, we empirically
study the role of CoT tokens in LLMs on two compositional tasks: multi-digit
multiplication and dynamic programming. While CoT is essential for solving
these problems, we find that preserving only tokens that store intermediate
results would achieve comparable performance. Furthermore, we observe that
storing intermediate results in an alternative latent form will not affect
model performance. We also randomly intervene some values in CoT, and notice
that subsequent CoT tokens and the final answer would change correspondingly.
These findings suggest that CoT tokens may function like variables in computer
programs but with potential drawbacks like unintended shortcuts and
computational complexity limits between tokens. The code and data are available
at https://github.com/solitaryzero/CoTs_are_Variables.

</details>


### [25] [Rethinking the Relationship between the Power Law and Hierarchical Structures](https://arxiv.org/abs/2505.04984)
*Kai Nakaishi,Ryo Yoshida,Kohei Kajikawa,Koji Hukushima,Yohei Oseki*

Main category: cs.CL

TL;DR: 论文通过分析英语语料库中的句法树，验证了幂律衰减与层级结构关系的假设，发现该假设并不成立，并指出需重新思考幂律与层级结构的关系。


<details>
  <summary>Details</summary>
Motivation: 探讨统计特性（如幂律衰减）是否能作为语言层级结构的证据，尤其是针对儿童语言和动物信号的解释是否成立。

Method: 使用英语语料库，分析句法树的互信息、概率上下文无关文法（PCFG）偏差等统计特性。

Result: 发现句法树的统计特性与假设不符，无法支持幂律衰减与层级结构的直接关联。

Conclusion: 需重新审视幂律与层级结构的关系，现有解释可能不适用于儿童语言和动物信号。

Abstract: Statistical analysis of corpora provides an approach to quantitatively
investigate natural languages. This approach has revealed that several power
laws consistently emerge across different corpora and languages, suggesting the
universal principles underlying languages. Particularly, the power-law decay of
correlation has been interpreted as evidence for underlying hierarchical
structures in syntax, semantics, and discourse. This perspective has also been
extended to child languages and animal signals. However, the argument
supporting this interpretation has not been empirically tested. To address this
problem, this study examines the validity of the argument for syntactic
structures. Specifically, we test whether the statistical properties of parse
trees align with the implicit assumptions in the argument. Using English
corpora, we analyze the mutual information, deviations from probabilistic
context-free grammars (PCFGs), and other properties in parse trees, as well as
in the PCFG that approximates these trees. Our results indicate that the
assumptions do not hold for syntactic structures and that it is difficult to
apply the proposed argument to child languages and animal signals, highlighting
the need to reconsider the relationship between the power law and hierarchical
structures.

</details>


### [26] [Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes](https://arxiv.org/abs/2505.04993)
*Zhuocheng Gong,Jian Guan,Wei Wu,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CL

TL;DR: 论文提出了Latent Preference Coding (LPC)，一种通过离散潜在编码建模人类偏好中隐含因素及其组合的框架，无需依赖预定义奖励函数，显著提升了多种对齐算法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有偏好建模方法常依赖显式或隐式奖励函数，难以捕捉人类偏好的复杂性和多面性，尤其是在涉及冲突因素或多样化任务时。

Method: LPC通过离散潜在编码建模偏好的隐含因素及其组合，自动从数据中推断这些因素及重要性，无需预定义奖励函数或人工组合权重。

Result: 在多个基准测试中，LPC对三种对齐算法（DPO、SimPO、IPO）和三种基础模型（Mistral-7B、Llama3-8B等）均表现出一致性改进，且潜在编码能有效捕捉偏好差异并增强数据噪声下的鲁棒性。

Conclusion: LPC为多样化偏好因素提供了统一表征，为开发更稳健、通用的对齐技术奠定了基础，有助于LLMs的负责任部署。

Abstract: Large language models (LLMs) have achieved remarkable success, yet aligning
their generations with human preferences remains a critical challenge. Existing
approaches to preference modeling often rely on an explicit or implicit reward
function, overlooking the intricate and multifaceted nature of human
preferences that may encompass conflicting factors across diverse tasks and
populations. To address this limitation, we introduce Latent Preference Coding
(LPC), a novel framework that models the implicit factors as well as their
combinations behind holistic preferences using discrete latent codes. LPC
seamlessly integrates with various offline alignment algorithms, automatically
inferring the underlying factors and their importance from data without relying
on pre-defined reward functions and hand-crafted combination weights. Extensive
experiments on multiple benchmarks demonstrate that LPC consistently improves
upon three alignment algorithms (DPO, SimPO, and IPO) using three base models
(Mistral-7B, Llama3-8B, and Llama3-8B-Instruct). Furthermore, deeper analysis
reveals that the learned latent codes effectively capture the differences in
the distribution of human preferences and significantly enhance the robustness
of alignment against noise in data. By providing a unified representation for
the multifarious preference factors, LPC paves the way towards developing more
robust and versatile alignment techniques for the responsible deployment of
powerful LLMs.

</details>


### [27] [Rethinking Invariance in In-context Learning](https://arxiv.org/abs/2505.04994)
*Lizhe Fang,Yifei Wang,Khashayar Gatmiry,Lei Fang,Yisen Wang*

Main category: cs.CL

TL;DR: InvICL是一种新型的ICL方法，解决了现有方法无法同时实现信息不泄露和上下文相互依赖的问题，并在多个基准数据集上表现优于之前的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的ICL方法对上下文示例的顺序敏感，且无法同时实现信息不泄露和上下文相互依赖，限制了其性能和泛化能力。

Method: 提出了InvICL方法，通过设计确保信息不泄露和上下文相互依赖，从而实现了顺序不变的ICL。

Result: 实验表明InvICL在大多数基准数据集上优于现有的不变和非不变方法，展示了更好的泛化能力。

Conclusion: InvICL通过解决信息不泄露和上下文相互依赖的问题，提升了ICL的性能和适用性。

Abstract: In-Context Learning (ICL) has emerged as a pivotal capability of
auto-regressive large language models, yet it is hindered by a notable
sensitivity to the ordering of context examples regardless of their mutual
independence. To address this issue, recent studies have introduced several
variant algorithms of ICL that achieve permutation invariance. However, many of
these do not exhibit comparable performance with the standard auto-regressive
ICL algorithm. In this work, we identify two crucial elements in the design of
an invariant ICL algorithm: information non-leakage and context
interdependence, which are not simultaneously achieved by any of the existing
methods. These investigations lead us to the proposed Invariant ICL (InvICL), a
methodology designed to achieve invariance in ICL while ensuring the two
properties. Empirically, our findings reveal that InvICL surpasses previous
models, both invariant and non-invariant, in most benchmark datasets,
showcasing superior generalization capabilities across varying input lengths.
Code is available at https://github.com/PKU-ML/InvICL.

</details>


### [28] [The Pitfalls of Growing Group Complexity: LLMs and Social Choice-Based Aggregation for Group Recommendations](https://arxiv.org/abs/2505.05016)
*Cedric Waterschoot,Nava Tintarev,Francesco Barile*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型在零样本学习下如何执行群组推荐策略，分析了不同条件（如群组复杂性、提示格式）对模型准确性的影响，发现In-Context学习能显著提升性能，而较小的模型在特定条件下也能胜任。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在群组推荐系统中基于零样本学习的适用性，以及群组复杂性和提示格式如何影响其准确性。

Method: 通过测试不同语言模型、提示条件（如In-Context学习、解释生成）和群组偏好格式化方式，分析其在群组推荐任务中的表现。

Result: 模型性能在评分超过100时下降，不同模型对群组复杂性敏感度不同；In-Context学习能提升高复杂性下的表现，而其他提示修改（如领域提示或解释生成）无显著影响。

Conclusion: 群组复杂性应是评估群组推荐系统的关键因素；较小的语言模型在适当条件下可有效生成推荐，节省计算资源。

Abstract: Large Language Models (LLMs) are increasingly applied in recommender systems
aimed at both individuals and groups. Previously, Group Recommender Systems
(GRS) often used social choice-based aggregation strategies to derive a single
recommendation based on the preferences of multiple people. In this paper, we
investigate under which conditions language models can perform these strategies
correctly based on zero-shot learning and analyse whether the formatting of the
group scenario in the prompt affects accuracy. We specifically focused on the
impact of group complexity (number of users and items), different LLMs,
different prompting conditions, including In-Context learning or generating
explanations, and the formatting of group preferences. Our results show that
performance starts to deteriorate when considering more than 100 ratings.
However, not all language models were equally sensitive to growing group
complexity. Additionally, we showed that In-Context Learning (ICL) can
significantly increase the performance at higher degrees of group complexity,
while adding other prompt modifications, specifying domain cues or prompting
for explanations, did not impact accuracy. We conclude that future research
should include group complexity as a factor in GRS evaluation due to its effect
on LLM performance. Furthermore, we showed that formatting the group scenarios
differently, such as rating lists per user or per item, affected accuracy. All
in all, our study implies that smaller LLMs are capable of generating group
recommendations under the right conditions, making the case for using smaller
models that require less computing power and costs.

</details>


### [29] [Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization](https://arxiv.org/abs/2505.05017)
*Yuntai Bao,Xuhong Zhang,Tianyu Du,Xinkui Zhao,Jiang Zong,Hao Peng,Jianwei Yin*

Main category: cs.CL

TL;DR: 本文提出了一种多阶段影响函数方法，用于将微调后大型语言模型（LLM）的预测归因于其预训练数据，并通过EK-FAC近似提高效率。


<details>
  <summary>Details</summary>
Motivation: 当前方法无法计算多阶段影响且难以扩展至十亿级LLM，因此需要一种新方法来解释微调后LLM的预测来源。

Method: 采用多阶段影响函数结合EK-FAC参数化进行高效近似。

Result: 实验证明EK-FAC近似具有优越的可扩展性，且多阶段影响函数有效。实际案例展示了其解释能力。

Conclusion: 多阶段影响函数为LLM预测归因提供了高效且实用的解决方案，并通过开源代码促进进一步研究。

Abstract: Pre-trained large language models (LLMs) are commonly fine-tuned to adapt to
downstream tasks. Since the majority of knowledge is acquired during
pre-training, attributing the predictions of fine-tuned LLMs to their
pre-training data may provide valuable insights. Influence functions have been
proposed as a means to explain model predictions based on training data.
However, existing approaches fail to compute ``multi-stage'' influence and lack
scalability to billion-scale LLMs.
  In this paper, we propose the multi-stage influence function to attribute the
downstream predictions of fine-tuned LLMs to pre-training data under the
full-parameter fine-tuning paradigm. To enhance the efficiency and practicality
of our multi-stage influence function, we leverage Eigenvalue-corrected
Kronecker-Factored (EK-FAC) parameterization for efficient approximation.
Empirical results validate the superior scalability of EK-FAC approximation and
the effectiveness of our multi-stage influence function. Additionally, case
studies on a real-world LLM, dolly-v2-3b, demonstrate its interpretive power,
with exemplars illustrating insights provided by multi-stage influence
estimates. Our code is public at
https://github.com/colored-dye/multi_stage_influence_function.

</details>


### [30] [G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness](https://arxiv.org/abs/2505.05026)
*Jaehyun Jeon,Janghan Yoon,Minsoo Kim,Sumin Shim,Yejin Choi,Hanbin Kim,Youngjae Yu*

Main category: cs.CL

TL;DR: 论文介绍了WiserUI-Bench基准和G-FOCUS策略，用于评估UI设计的说服力，减少A/B测试的成本和时间，并提升视觉语言模型的分析准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的A/B测试耗时且昂贵，而现有视觉语言模型方法无法有效比较UI设计的说服力，因此需要一种更高效的方法。

Method: 提出WiserUI-Bench基准和G-FOCUS推理策略，通过减少位置偏差和提高评估准确性来优化UI设计的比较分析。

Result: 实验表明，G-FOCUS在一致性和准确性上优于现有推理策略，能够有效评估UI设计的说服力。

Conclusion: 该研究为UI设计优化提供了高效替代方案，推动了大范围UI偏好建模的发展。

Abstract: Evaluating user interface (UI) design effectiveness extends beyond aesthetics
to influencing user behavior, a principle central to Design Persuasiveness. A/B
testing is the predominant method for determining which UI variations drive
higher user engagement, but it is costly and time-consuming. While recent
Vision-Language Models (VLMs) can process automated UI analysis, current
approaches focus on isolated design attributes rather than comparative
persuasiveness-the key factor in optimizing user interactions. To address this,
we introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design
Persuasiveness Assessment task, featuring 300 real-world UI image pairs labeled
with A/B test results and expert rationales. Additionally, we propose G-FOCUS,
a novel inference-time reasoning strategy that enhances VLM-based
persuasiveness assessment by reducing position bias and improving evaluation
accuracy. Experimental results show that G-FOCUS surpasses existing inference
strategies in consistency and accuracy for pairwise UI evaluation. Through
promoting VLM-driven evaluation of UI persuasiveness, our work offers an
approach to complement A/B testing, propelling progress in scalable UI
preference modeling and design optimization. Code and data will be released
publicly.

</details>


### [31] [Image-Text Relation Prediction for Multilingual Tweets](https://arxiv.org/abs/2505.05040)
*Matīss Rikters,Edison Marrese-Taylor*

Main category: cs.CL

TL;DR: 论文探索了多语言视觉语言模型在不同语言中的图像文本关系预测能力，构建了一个来自拉脱维亚Twitter帖子的平衡基准数据集，并手动翻译成英文。结果表明最新模型在此任务上表现更好，但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 研究社交媒体中图像与文本关系的预测问题，尤其是多语言环境下的表现，以填补现有研究的空缺。

Method: 构建了一个拉脱维亚Twitter帖子的平衡数据集，并用最新视觉语言模型进行图像文本关系预测任务测试。

Result: 最新视觉语言模型在此任务上表现优于先前工作，但仍存在改进空间。

Conclusion: 多语言视觉语言模型在图像文本关系预测任务上显示出潜力，但需进一步优化。

Abstract: Various social networks have been allowing media uploads for over a decade
now. Still, it has not always been clear what is their relation with the posted
text or even if there is any at all. In this work, we explore how multilingual
vision-language models tackle the task of image-text relation prediction in
different languages, and construct a dedicated balanced benchmark data set from
Twitter posts in Latvian along with their manual translations into English. We
compare our results to previous work and show that the more recently released
vision-language model checkpoints are becoming increasingly capable at this
task, but there is still much room for further improvement.

</details>


### [32] [Teochew-Wild: The First In-the-wild Teochew Dataset with Orthographic Annotations](https://arxiv.org/abs/2505.05056)
*Linrong Pan,Chenglong Jiang,Gaoze Hou,Ying Gao*

Main category: cs.CL

TL;DR: 论文构建了Teochew-Wild语音语料库，包含18.9小时潮州话的野外语音数据，提供精确的文本和拼音标注，并验证了其在ASR和TTS任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 潮州话是一种低资源语言，缺乏公开可用的语音数据集。构建Teochew-Wild语料库旨在填补这一空白，推动相关语音任务（如ASR和TTS）的研究和应用。

Method: 收集了18.9小时的多说话人潮州话语音数据，涵盖正式和口语表达，并提供精确的文本和拼音标注，同时开发了配套的文本处理工具和资源。

Result: 实验结果表明，该语料库在ASR和TTS任务中表现有效，验证了其质量和实用性。

Conclusion: Teochew-Wild是首个公开的、带有精确文本标注的潮州话数据集，为低资源语言的语音研究提供了重要支持。

Abstract: This paper reports the construction of the Teochew-Wild, a speech corpus of
the Teochew dialect. The corpus includes 18.9 hours of in-the-wild Teochew
speech data from multiple speakers, covering both formal and colloquial
expressions, with precise orthographic and pinyin annotations. Additionally, we
provide supplementary text processing tools and resources to propel research
and applications in speech tasks for this low-resource language, such as
automatic speech recognition (ASR) and text-to-speech (TTS). To the best of our
knowledge, this is the first publicly available Teochew dataset with accurate
orthographic annotations. We conduct experiments on the corpus, and the results
validate its effectiveness in ASR and TTS tasks.

</details>


### [33] [Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization](https://arxiv.org/abs/2505.05070)
*Ajwad Abrar,Farzana Tabassum,Sabbir Ahmed*

Main category: cs.CL

TL;DR: 该研究评估了九种大型语言模型（LLMs）在零样本条件下对孟加拉语消费者健康查询（CHQs）的摘要能力，发现Mixtral-8x22b-Instruct在ROUGE-1和ROUGE-L上表现最佳，而Bangla T5在ROUGE-2上表现最优。结果表明零样本LLMs能与微调模型媲美，为低资源语言医疗查询提供了可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语（Bangla）作为一种低资源语言，其消费者健康查询（CHQs）常包含冗余信息，增加了医疗回复的复杂性。研究旨在探索零样本LLMs在该任务中的潜力，以提供高效的摘要解决方案。

Method: 研究使用包含2,350个标注查询-摘要对的BanglaCHQ-Summ数据集，评估了九种LLMs（如GPT-4、Claude-3.5-Sonnet等）的零样本表现，并通过ROUGE指标与微调模型Bangla T5进行对比。

Result: Mixtral-8x22b-Instruct在ROUGE-1和ROUGE-L上表现最佳，Bangla T5在ROUGE-2上领先。零样本LLMs整体表现接近微调模型，能够生成高质量的摘要。

Conclusion: 研究表明，零样本LLMs在低资源语言任务中具有竞争力，为医疗查询摘要提供了无需任务特定训练的可扩展方案，展现了其在低资源语言处理中的潜力。

Abstract: Consumer Health Queries (CHQs) in Bengali (Bangla), a low-resource language,
often contain extraneous details, complicating efficient medical responses.
This study investigates the zero-shot performance of nine advanced large
language models (LLMs): GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet,
Llama3-70b-Instruct, Mixtral-8x22b-Instruct, Gemini-1.5-Pro,
Qwen2-72b-Instruct, Gemma-2-27b, and Athene-70B, in summarizing Bangla CHQs.
Using the BanglaCHQ-Summ dataset comprising 2,350 annotated query-summary
pairs, we benchmarked these LLMs using ROUGE metrics against Bangla T5, a
fine-tuned state-of-the-art model. Mixtral-8x22b-Instruct emerged as the top
performing model in ROUGE-1 and ROUGE-L, while Bangla T5 excelled in ROUGE-2.
The results demonstrate that zero-shot LLMs can rival fine-tuned models,
achieving high-quality summaries even without task-specific training. This work
underscores the potential of LLMs in addressing challenges in low-resource
languages, providing scalable solutions for healthcare query summarization.

</details>


### [34] [Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction](https://arxiv.org/abs/2505.05084)
*Xiaowei Zhu,Yubing Ren,Yanan Cao,Xixun Lin,Fang Fang,Yangxi Li*

Main category: cs.CL

TL;DR: 该论文提出了一种通过多尺度符合预测（MCP）的零样本机器生成文本检测框架，以限制假阳性率（FPR）并提高检测性能，同时引入了高质量数据集RealDet以支持性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法过于关注检测准确性，而忽略了高假阳性率（FPR）带来的社会风险，亟需一种既能控制FPR又能维持检测性能的解决方案。

Method: 采用符合预测（CP）约束FPR的上限，并通过多尺度符合预测（MCP）改善性能，同时引入新数据集RealDet以实现更现实的校准。

Result: 实验表明，MCP有效约束FPR，显著提升检测性能，并在多个检测器和数据集上增强了对对抗攻击的鲁棒性。

Conclusion: MCP框架在平衡FPR约束和检测性能方面表现出色，结合RealDet数据集，为机器生成文本检测提供了更可靠的解决方案。

Abstract: The rapid advancement of large language models has raised significant
concerns regarding their potential misuse by malicious actors. As a result,
developing effective detectors to mitigate these risks has become a critical
priority. However, most existing detection methods focus excessively on
detection accuracy, often neglecting the societal risks posed by high false
positive rates (FPRs). This paper addresses this issue by leveraging Conformal
Prediction (CP), which effectively constrains the upper bound of FPRs. While
directly applying CP constrains FPRs, it also leads to a significant reduction
in detection performance. To overcome this trade-off, this paper proposes a
Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal
Prediction (MCP), which both enforces the FPR constraint and improves detection
performance. This paper also introduces RealDet, a high-quality dataset that
spans a wide range of domains, ensuring realistic calibration and enabling
superior detection performance when combined with MCP. Empirical evaluations
demonstrate that MCP effectively constrains FPRs, significantly enhances
detection performance, and increases robustness against adversarial attacks
across multiple detectors and datasets.

</details>


### [35] [Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2505.05111)
*Boyi Deng,Yu Wan,Yidan Zhang,Baosong Yang,Fuli Feng*

Main category: cs.CL

TL;DR: 该论文分析了大型语言模型（LLM）中多语言能力的机制，提出了一种基于稀疏自编码器（SAE）的新方法来分解模型激活，并引入了一种新的度量标准来评估特征的单一语言性，发现某些特征与特定语言密切相关。通过消融实验和特征增强，论文还展示了如何利用这些特征控制LLM生成的语言。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用基于神经元或内部激活的方法分析LLM的多语言能力，但这些方法因叠加和层间激活差异等问题限制了可靠性。论文旨在通过SAE提供更细致的分析，揭示多语言能力的潜在机制。

Method: 采用稀疏自编码器（SAE）分解LLM的激活为稀疏线性组合的特征，提出新的度量标准评估特征的单一语言性，并通过消融实验和特征增强验证这些特征的语言特异性及其对模型行为的影响。

Result: 研究发现某些SAE特征与特定语言强相关，消融这些特征仅显著影响模型在特定语言上的能力。此外，一些语言具有多个协同作用的SAE特征，联合消融比单独消融效果更显著。利用这些特征可以增强导向向量，实现对LLM生成语言的调控。

Conclusion: 通过SAE分解可以更可靠地分析LLM的多语言能力，揭示语言特异性特征的存在及其潜在影响。这一方法不仅提升了多语言能力的研究，还为模型行为的定向调控提供了新思路。

Abstract: The mechanisms behind multilingual capabilities in Large Language Models
(LLMs) have been examined using neuron-based or internal-activation-based
methods. However, these methods often face challenges such as superposition and
layer-wise activation variance, which limit their reliability. Sparse
Autoencoders (SAEs) offer a more nuanced analysis by decomposing the
activations of LLMs into sparse linear combination of SAE features. We
introduce a novel metric to assess the monolinguality of features obtained from
SAEs, discovering that some features are strongly related to specific
languages. Additionally, we show that ablating these SAE features only
significantly reduces abilities in one language of LLMs, leaving others almost
unaffected. Interestingly, we find some languages have multiple synergistic SAE
features, and ablating them together yields greater improvement than ablating
individually. Moreover, we leverage these SAE-derived language-specific
features to enhance steering vectors, achieving control over the language
generated by LLMs.

</details>


### [36] [A Benchmark Dataset and a Framework for Urdu Multimodal Named Entity Recognition](https://arxiv.org/abs/2505.05148)
*Hussain Ahmad,Qingyang Zeng,Jing Wan*

Main category: cs.CL

TL;DR: 该论文介绍了U-MNER框架和Twitter2015-Urdu数据集，用于研究低资源语言（如乌尔都语）的多模态命名实体识别（MNER），并通过结合文本和视觉信息的模型取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 目前多模态命名实体识别（MNER）研究主要集中在高资源语言（如英语），而对低资源语言（如乌尔都语）的研究不足，且缺乏标注数据和基准模型。

Method: 提出了U-MNER框架，结合Urdu-BERT提取文本嵌入和ResNet提取视觉特征，并通过跨模态融合模块整合信息；同时发布了Twitter2015-Urdu数据集。

Result: 模型在Twitter2015-Urdu数据集上达到了最先进的性能，为低资源语言的MNER研究奠定了基础。

Conclusion: 该研究填补了乌尔都语MNER的空白，提出的框架和数据集为未来研究提供了支持，尤其针对低资源语言。

Abstract: The emergence of multimodal content, particularly text and images on social
media, has positioned Multimodal Named Entity Recognition (MNER) as an
increasingly important area of research within Natural Language Processing.
Despite progress in high-resource languages such as English, MNER remains
underexplored for low-resource languages like Urdu. The primary challenges
include the scarcity of annotated multimodal datasets and the lack of
standardized baselines. To address these challenges, we introduce the U-MNER
framework and release the Twitter2015-Urdu dataset, a pioneering resource for
Urdu MNER. Adapted from the widely used Twitter2015 dataset, it is annotated
with Urdu-specific grammar rules. We establish benchmark baselines by
evaluating both text-based and multimodal models on this dataset, providing
comparative analyses to support future research on Urdu MNER. The U-MNER
framework integrates textual and visual context using Urdu-BERT for text
embeddings and ResNet for visual feature extraction, with a Cross-Modal Fusion
Module to align and fuse information. Our model achieves state-of-the-art
performance on the Twitter2015-Urdu dataset, laying the groundwork for further
MNER research in low-resource languages.

</details>


### [37] [QualBench: Benchmarking Chinese LLMs with Localized Professional Qualifications for Vertical Domain Evaluation](https://arxiv.org/abs/2505.05225)
*Mengze Hong,Wailing Ng,Di Jiang,Chen Jason Zhang*

Main category: cs.CL

TL;DR: 论文提出了QualBench，首个针对中文大模型的垂直领域评估基准，包含六大领域的1.7万个问题。评测显示中文模型（如Qwen2.5）优于GPT-4o，突显本地化知识的重要性，并揭示了模型在领域覆盖上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有评测基准在垂直领域覆盖不足，且缺乏对中文工作场景的针对性评估。通过资格考试的框架，论文旨在填补这一空白。

Method: 基于24项中国资格考试构建QualBench数据集，覆盖六大领域。通过全面评测分析模型表现。

Result: 中文模型（Qwen2.5）表现优于GPT-4o，最高准确率75.26%。同时发现众包机制在LLM协作中的失败案例。

Conclusion: 需增强多领域RAG知识整合及联邦学习的垂直领域训练，以提升模型在本地化场景的能力。

Abstract: The rapid advancement of Chinese large language models (LLMs) underscores the
need for domain-specific evaluations to ensure reliable applications. However,
existing benchmarks often lack coverage in vertical domains and offer limited
insights into the Chinese working context. Leveraging qualification exams as a
unified framework for human expertise evaluation, we introduce QualBench, the
first multi-domain Chinese QA benchmark dedicated to localized assessment of
Chinese LLMs. The dataset includes over 17,000 questions across six vertical
domains, with data selections grounded in 24 Chinese qualifications to closely
align with national policies and working standards. Through comprehensive
evaluation, the Qwen2.5 model outperformed the more advanced GPT-4o, with
Chinese LLMs consistently surpassing non-Chinese models, highlighting the
importance of localized domain knowledge in meeting qualification requirements.
The best performance of 75.26% reveals the current gaps in domain coverage
within model capabilities. Furthermore, we present the failure of LLM
collaboration with crowdsourcing mechanisms and suggest the opportunities for
multi-domain RAG knowledge enhancement and vertical domain LLM training with
Federated Learning.

</details>


### [38] [T-T: Table Transformer for Tagging-based Aspect Sentiment Triplet Extraction](https://arxiv.org/abs/2505.05271)
*Kun Peng,Chaodong Tong,Cong Cao,Hao Peng,Qian Li,Guanlin Wu,Lei Jiang,Yanbing Liu,Philip S. Yu*

Main category: cs.CL

TL;DR: 论文提出了Table-Transformer (T-T)，通过条纹注意力和循环移位策略解决了ASTE任务中Transformer直接应用导致的过长序列和局部注意力不公平问题，实现了高效的关系学习。


<details>
  <summary>Details</summary>
Motivation: 因发现更强的关系捕捉能力能显著提升模型性能，作者尝试直接用Transformer层作为下游关系学习模块，但面临过长序列和局部注意力不公平的挑战。

Method: 提出Table-Transformer (T-T)，结合条纹注意力（局部注意力窗口）和循环移位策略（窗口间交互），解决Transformer的直接应用问题。

Result: T-T作为下游关系学习模块，在较低计算成本下实现了最先进性能。

Conclusion: T-T通过改进注意力机制，高效解决了ASTE任务中的关系学习挑战，证明了其优越性和实用性。

Abstract: Aspect sentiment triplet extraction (ASTE) aims to extract triplets composed
of aspect terms, opinion terms, and sentiment polarities from given sentences.
The table tagging method is a popular approach to addressing this task, which
encodes a sentence into a 2-dimensional table, allowing for the tagging of
relations between any two words. Previous efforts have focused on designing
various downstream relation learning modules to better capture interactions
between tokens in the table, revealing that a stronger capability to capture
relations can lead to greater improvements in the model. Motivated by this, we
attempt to directly utilize transformer layers as downstream relation learning
modules. Due to the powerful semantic modeling capability of transformers, it
is foreseeable that this will lead to excellent improvement. However, owing to
the quadratic relation between the length of the table and the length of the
input sentence sequence, using transformers directly faces two challenges:
overly long table sequences and unfair local attention interaction. To address
these challenges, we propose a novel Table-Transformer (T-T) for the
tagging-based ASTE method. Specifically, we introduce a stripe attention
mechanism with a loop-shift strategy to tackle these challenges. The former
modifies the global attention mechanism to only attend to a 2-dimensional local
attention window, while the latter facilitates interaction between different
attention windows. Extensive and comprehensive experiments demonstrate that the
T-T, as a downstream relation learning module, achieves state-of-the-art
performance with lower computational costs.

</details>


### [39] [Toward Reasonable Parrots: Why Large Language Models Should Argue with Us by Design](https://arxiv.org/abs/2505.05298)
*Elena Musi,Nadin Kokciyan,Khalid Al-Khatib,Davide Ceolin,Emmanuelle Dietz,Klara Gutekunst,Annette Hautli-Janisz,Cristian Manuel Santibañez Yañez,Jodi Schneider,Jonas Scholz,Cor Steging,Jacky Visser,Henning Wachsmuth*

Main category: cs.CL

TL;DR: 论文提倡开发专门支持论证过程的对话技术，批评现有大型语言模型（LLMs）的不足，并提出以论证理论为基础设计新技术的理念。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在支持论证对话方面存在不足，作者希望通过结合论证理论，设计出能提升批判性思维的对话技术。

Method: 提出“理性鹦鹉”概念，基于相关性、责任和自由三大原则，通过论证性对话动作与技术交互。

Result: 论证理论与LLM技术的结合可能成为未来支持论证对话技术的起点。

Conclusion: 通过重新定位LLMs为工具而非替代品，可以设计出更有效的论证支持技术。

Abstract: In this position paper, we advocate for the development of conversational
technology that is inherently designed to support and facilitate argumentative
processes. We argue that, at present, large language models (LLMs) are
inadequate for this purpose, and we propose an ideal technology design aimed at
enhancing argumentative skills. This involves re-framing LLMs as tools to
exercise our critical thinking rather than replacing them. We introduce the
concept of 'reasonable parrots' that embody the fundamental principles of
relevance, responsibility, and freedom, and that interact through argumentative
dialogical moves. These principles and moves arise out of millennia of work in
argumentation theory and should serve as the starting point for LLM-based
technology that incorporates basic principles of argumentation.

</details>


### [40] [ICon: In-Context Contribution for Automatic Data Selection](https://arxiv.org/abs/2505.05327)
*Yixin Yang,Qingxiu Dong,Linli Yao,Fangwei Zhu,Zhifang Sui*

Main category: cs.CL

TL;DR: 本文提出了一种名为ICon的新型数据选择方法，用于优化指令调优过程，无需依赖计算密集型梯度或人工启发式，而是利用上下文学习的隐含特性高效测量数据贡献。实验表明，ICon显著提升模型性能并降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法要么计算代价高（基于梯度），要么依赖人工启发式，无法充分利用数据内在属性。本文旨在提出一种高效且无需人工干预的替代方案。

Method: ICon通过上下文学习（ICL）的隐含调优特性，设计三个组件评估样本贡献，避免梯度计算或人工指标设计。

Result: 在三个LLM和12个基准测试中，ICon仅使用15%数据即可超越全数据训练效果（LLaMA3.1-8B提升5.42%），且优于其他主流方法2.06%。所选样本兼具任务多样性和适度难度。

Conclusion: ICon为数据选择提供了高效、无偏的解决方案，显著提升模型性能，同时揭示了高质量数据的关键特征（多样性+适中难度）。

Abstract: Data selection for instruction tuning is essential for improving the
performance of Large Language Models (LLMs) and reducing training cost.
However, existing automated selection methods either depend on computationally
expensive gradient-based measures or manually designed heuristics, which may
fail to fully exploit the intrinsic attributes of data. In this paper, we
propose In-context Learning for Contribution Measurement (ICon), a novel
gradient-free method that takes advantage of the implicit fine-tuning nature of
in-context learning (ICL) to measure sample contribution without gradient
computation or manual indicators engineering. ICon offers a computationally
efficient alternative to gradient-based methods and reduces human inductive
bias inherent in heuristic-based approaches. ICon comprises three components
and identifies high-contribution data by assessing performance shifts under
implicit learning through ICL. Extensive experiments on three LLMs across 12
benchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of
ICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data
outperform full datasets by 5.42% points and exceed the best performance of
widely used selection methods by 2.06% points. We further analyze
high-contribution samples selected by ICon, which show both diverse tasks and
appropriate difficulty levels, rather than just the hardest ones.

</details>


### [41] [Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than Humans?](https://arxiv.org/abs/2505.05406)
*Valeria Pastorino,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLM）生成的新闻内容比人为撰写的更具框架偏见，尤其在政治和社会敏感话题上，不同模型架构间的偏见差异显著，需后训练缓解策略和更严格的评估框架。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在自动化新闻和内容创作中的应用增多，担忧其可能引入或放大框架偏见，本研究旨在对比分析未经调整和微调的LLM生成内容中的框架表现。

Method: 分析不同架构LLM生成新闻内容的框架特征，重点关注政治和社会敏感话题，并与人类撰写的新闻进行对比。

Result: LLM在敏感话题上框架偏见更明显，且模型间差异显著，某些模型偏见尤为突出。

Conclusion: 需开发有效的后训练缓解策略和更严苛的评估标准，以确保自动化新闻内容保持平衡报道。

Abstract: Framing in media critically shapes public perception by selectively
emphasizing some details while downplaying others. With the rise of large
language models in automated news and content creation, there is growing
concern that these systems may introduce or even amplify framing biases
compared to human authors. In this paper, we explore how framing manifests in
both out-of-the-box and fine-tuned LLM-generated news content. Our analysis
reveals that, particularly in politically and socially sensitive contexts, LLMs
tend to exhibit more pronounced framing than their human counterparts. In
addition, we observe significant variation in framing tendencies across
different model architectures, with some models displaying notably higher
biases. These findings point to the need for effective post-training mitigation
strategies and tighter evaluation frameworks to ensure that automated news
content upholds the standards of balanced reporting.

</details>


### [42] [Crosslingual Reasoning through Test-Time Scaling](https://arxiv.org/abs/2505.05408)
*Zheng-Xin Yong,M. Farid Adilazuarda,Jonibek Mansurov,Ruochen Zhang,Niklas Muennighoff,Carsten Eickhoff,Genta Indra Winata,Julia Kreutzer,Stephen H. Bach,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 英语为中心的长思维链（CoT）微调在不同语言中的推理能力泛化研究：发现规模化的英语推理在低资源语言中有效性优于大规模模型的潜力，但跨语言推理存在局限性，需进一步提升低资源语言和特定领域的推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在多语言环境下的推理能力泛化问题，尤其关注英语为中心的训练在其他语言中的适用性，尤其是低资源语言的表现。

Method: 通过规模化计算资源进行英语推理微调，分析其对多语言数学推理的影响，并探索思维链语言控制的策略。

Result: 英语为中心的推理模型在包括低资源语言在内的多语言推理任务中表现优于更大的模型，但在特定领域如文化常识方面的泛化能力较差。

Conclusion: 研究者应当引导英语为中心的模型使用高资源语言进行推理，同时需进一步改进低资源语言和特定领域的推理能力。

Abstract: Reasoning capabilities of large language models are primarily studied for
English, even when pretrained models are multilingual. In this work, we
investigate to what extent English reasoning finetuning with long
chain-of-thoughts (CoTs) can generalize across languages. First, we find that
scaling up inference compute for English-centric reasoning language models
(RLMs) improves multilingual mathematical reasoning across many languages
including low-resource languages, to an extent where they outperform models
twice their size. Second, we reveal that while English-centric RLM's CoTs are
naturally predominantly English, they consistently follow a quote-and-think
pattern to reason about quoted non-English inputs. Third, we discover an
effective strategy to control the language of long CoT reasoning, and we
observe that models reason better and more efficiently in high-resource
languages. Finally, we observe poor out-of-domain reasoning generalization, in
particular from STEM to cultural commonsense knowledge, even for English.
Overall, we demonstrate the potentials, study the mechanisms and outline the
limitations of crosslingual generalization of English reasoning test-time
scaling. We conclude that practitioners should let English-centric RLMs reason
in high-resource languages, while further work is needed to improve reasoning
in low-resource languages and out-of-domain contexts.

</details>


### [43] [Reasoning Models Don't Always Say What They Think](https://arxiv.org/abs/2505.05410)
*Yanda Chen,Joe Benton,Ansh Radhakrishnan,Jonathan Uesato,Carson Denison,John Schulman,Arushi Somani,Peter Hase,Misha Wagner,Fabien Roger,Vlad Mikulik,Samuel R. Bowman,Jan Leike,Jared Kaplan,Ethan Perez*

Main category: cs.CL

TL;DR: 分析了Chain-of-Thought（CoT）在AI监控中的有效性，发现CoT虽能部分揭示模型推理过程，但效果有限且无法完全避免不良行为。


<details>
  <summary>Details</summary>
Motivation: 探索CoT是否能忠实反映模型的真实推理过程，以提升AI安全性。

Method: 评估6种推理提示下CoT的忠实性，测试强化学习对CoT的影响。

Result: CoT揭示提示使用的比例通常低于20%；强化学习初期提高忠实性但后停滞；奖励机制增加提示使用频率但未提升其表达。

Conclusion: CoT监控在训练和评估中有潜力，但不足以完全排除不良行为，尤其在非必要的推理场景中效果有限。

Abstract: Chain-of-thought (CoT) offers a potential boon for AI safety as it allows
monitoring a model's CoT to try to understand its intentions and reasoning
processes. However, the effectiveness of such monitoring hinges on CoTs
faithfully representing models' actual reasoning processes. We evaluate CoT
faithfulness of state-of-the-art reasoning models across 6 reasoning hints
presented in the prompts and find: (1) for most settings and models tested,
CoTs reveal their usage of hints in at least 1% of examples where they use the
hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement
learning initially improves faithfulness but plateaus without saturating, and
(3) when reinforcement learning increases how frequently hints are used (reward
hacking), the propensity to verbalize them does not increase, even without
training against a CoT monitor. These results suggest that CoT monitoring is a
promising way of noticing undesired behaviors during training and evaluations,
but that it is not sufficient to rule them out. They also suggest that in
settings like ours where CoT reasoning is not necessary, test-time monitoring
of CoTs is unlikely to reliably catch rare and catastrophic unexpected
behaviors.

</details>


### [44] [TransProQA: an LLM-based literary Translation evaluation metric with Professional Question Answering](https://arxiv.org/abs/2505.05423)
*Ran Zhang,Wei Zhao,Lieve Macken,Steffen Eger*

Main category: cs.CL

TL;DR: 论文提出TransProQA，一种针对文学翻译评估的参考无关QA框架，结合专业译者见解，显著优于现有指标，接近人类评估水平。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标过于注重机械准确性，忽略了艺术表达，可能造成翻译质量和文化真实性的长期下降，亟需专门的文学评估指标。

Method: 基于LLM的QA框架TransProQA，融合专业译者对文学质量（如修辞手法、文化理解、作者风格）的见解，采用加权优化性能。

Result: TransProQA在相关性（ACC-EQ和Kendall's tau）上提升0.07，在充分性评估中超越现有SOTA指标15分以上，接近人类评估者水平。

Conclusion: TransProQA是一种高效、免训练的文学评估工具，适用于开源模型（如LLaMA3.3-70b），对需本地处理的文本具有广泛应用潜力。

Abstract: The impact of Large Language Models (LLMs) has extended into literary
domains. However, existing evaluation metrics prioritize mechanical accuracy
over artistic expression and tend to overrate machine translation (MT) as being
superior to experienced professional human translation. In the long run, this
bias could result in a permanent decline in translation quality and cultural
authenticity. In response to the urgent need for a specialized literary
evaluation metric, we introduce TransProQA, a novel, reference-free, LLM-based
question-answering (QA) framework designed specifically for literary
translation evaluation. TransProQA uniquely integrates insights from
professional literary translators and researchers, focusing on critical
elements in literary quality assessment such as literary devices, cultural
understanding, and authorial voice. Our extensive evaluation shows that while
literary-finetuned XCOMET-XL yields marginal gains, TransProQA substantially
outperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ
and Kendall's tau) and surpassing the best state-of-the-art (SOTA) metrics by
over 15 points in adequacy assessments. Incorporating professional translator
insights as weights further improves performance, highlighting the value of
translator inputs. Notably, TransProQA approaches human-level evaluation
performance comparable to trained linguistic annotators. It demonstrates broad
applicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,
indicating its potential as an accessible and training-free literary evaluation
metric and a valuable tool for evaluating texts that require local processing
due to copyright or ethical considerations.

</details>


### [45] [Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data](https://arxiv.org/abs/2505.05427)
*Yudong Wang,Zixuan Fu,Jie Cai,Peijun Tang,Hongya Lyu,Yewei Fang,Zhi Zheng,Jie Zhou,Guoyang Zeng,Chaojun Xiao,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 论文提出了一种高效的数据过滤管道，通过改进数据验证策略和优化种子数据选择，显著提升数据质量和训练效率，并在实际数据集中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的快速发展，数据质量成为提升模型性能的关键因素。然而，当前模型驱动的数据过滤方法面临验证策略低效和种子数据选择主观性强的两大挑战。

Method: 引入高效验证策略快速评估数据对LLM训练的影响；基于高质量种子数据的假设优化正负样本选择，设计高效数据过滤管道；使用轻量级fastText分类器实现高效过滤。

Result: 在FineWeb和Chinese FineWeb数据集上应用该管道，生成更高质量的Ultra-FineWeb数据集。实验表明，基于Ultra-FineWeb训练的LLM在多项任务中性能显著提升。

Conclusion: 所提出的数据过滤管道不仅提高了数据质量和训练效率，还降低了实验和推理成本，为LLM的高质量数据获取提供了可靠解决方案。

Abstract: Data quality has become a key factor in enhancing model performance with the
rapid development of large language models (LLMs). Model-driven data filtering
has increasingly become a primary approach for acquiring high-quality data.
However, it still faces two main challenges: (1) the lack of an efficient data
verification strategy makes it difficult to provide timely feedback on data
quality; and (2) the selection of seed data for training classifiers lacks
clear criteria and relies heavily on human expertise, introducing a degree of
subjectivity. To address the first challenge, we introduce an efficient
verification strategy that enables rapid evaluation of the impact of data on
LLM training with minimal computational cost. To tackle the second challenge,
we build upon the assumption that high-quality seed data is beneficial for LLM
training, and by integrating the proposed verification strategy, we optimize
the selection of positive and negative samples and propose an efficient data
filtering pipeline. This pipeline not only improves filtering efficiency,
classifier quality, and robustness, but also significantly reduces experimental
and inference costs. In addition, to efficiently filter high-quality data, we
employ a lightweight classifier based on fastText, and successfully apply the
filtering pipeline to two widely-used pre-training corpora, FineWeb and Chinese
FineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb
dataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120
billion Chinese tokens. Empirical results demonstrate that the LLMs trained on
Ultra-FineWeb exhibit significant performance improvements across multiple
benchmark tasks, validating the effectiveness of our pipeline in enhancing both
data quality and training efficiency.

</details>


### [46] [clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations](https://arxiv.org/abs/2505.05445)
*Chalamalasetti Kranti,Sherzod Hakimov,David Schlangen*

Main category: cs.CL

TL;DR: 提出了一个名为clem todd的框架，用于在一致条件下系统地评估对话系统，支持用户模拟器和对话系统的灵活组合，并提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常孤立评估对话系统的组件，限制了其通用性，因此需要一种更灵活、一致的评估框架。

Method: 开发了clem todd框架，支持插件式集成、统一数据集和评估指标，并在该框架内重新评估和整合了多个对话系统。

Result: 提供了关于架构、规模及提示策略如何影响对话性能的具体见解，为构建高效对话系统提供了实用指南。

Conclusion: clem todd是一个有效的工具，能够促进对话系统的系统化评估和优化。

Abstract: The emergence of instruction-tuned large language models (LLMs) has advanced
the field of dialogue systems, enabling both realistic user simulations and
robust multi-turn conversational agents. However, existing research often
evaluates these components in isolation-either focusing on a single user
simulator or a specific system design-limiting the generalisability of insights
across architectures and configurations. In this work, we propose clem todd
(chat-optimized LLMs for task-oriented dialogue systems development), a
flexible framework for systematically evaluating dialogue systems under
consistent conditions. clem todd enables detailed benchmarking across
combinations of user simulators and dialogue systems, whether existing models
from literature or newly developed ones. It supports plug-and-play integration
and ensures uniform datasets, evaluation metrics, and computational
constraints. We showcase clem todd's flexibility by re-evaluating existing
task-oriented dialogue systems within this unified setup and integrating three
newly proposed dialogue systems into the same evaluation pipeline. Our results
provide actionable insights into how architecture, scale, and prompting
strategies affect dialogue performance, offering practical guidance for
building efficient and effective conversational AI systems.

</details>


### [47] [UKElectionNarratives: A Dataset of Misleading Narratives Surrounding Recent UK General Elections](https://arxiv.org/abs/2505.05459)
*Fatima Haouari,Carolina Scarton,Nicolò Faggiani,Nikolaos Nikolaidis,Bonka Kotseva,Ibrahim Abu Farha,Jens Linge,Kalina Bontcheva*

Main category: cs.CL

TL;DR: 论文提出了一种欧洲选举中常见误导性叙事的分类法，构建了首个标注数据集UKElectionNarratives，并评估了GPT-4o等模型在检测误导性叙事中的表现。


<details>
  <summary>Details</summary>
Motivation: 误导性叙事在选举中影响公众观点，但缺乏系统化的分类和检测方法。研究旨在填补这一空白，为未来研究提供基础和工具。

Method: 提出首个误导性叙事分类法，构建人工标注数据集UKElectionNarratives，并测试预训练和大语言模型（如GPT-4o）的检测效果。

Result: 建立了系统的分类法和数据集，并发现GPT-4o在检测选举相关误导性叙事方面具有一定效果。

Conclusion: 研究为检测和应对选举中的误导性叙事提供了实用工具和方法，建议未来进一步优化模型并扩展数据集。

Abstract: Misleading narratives play a crucial role in shaping public opinion during
elections, as they can influence how voters perceive candidates and political
parties. This entails the need to detect these narratives accurately. To
address this, we introduce the first taxonomy of common misleading narratives
that circulated during recent elections in Europe. Based on this taxonomy, we
construct and analyse UKElectionNarratives: the first dataset of
human-annotated misleading narratives which circulated during the UK General
Elections in 2019 and 2024. We also benchmark Pre-trained and Large Language
Models (focusing on GPT-4o), studying their effectiveness in detecting
election-related misleading narratives. Finally, we discuss potential use cases
and make recommendations for future research directions using the proposed
codebook and dataset.

</details>


### [48] [Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging](https://arxiv.org/abs/2505.05464)
*Shiqi Chen,Jinghan Zhang,Tongyao Zhu,Wei Liu,Siyang Gao,Miao Xiong,Manling Li,Junxian He*

Main category: cs.CL

TL;DR: 该研究探索了通过模型合并将视觉与语言模型结合，以实现跨模态的感知与推理能力整合，实验证明这种方法能无训练地将大语言模型的推理能力迁移到视觉语言模型中，并揭示了感知和推理在不同模型层的分布情况。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）和大型语言模型（LLMs）的感知与推理能力结合机制尚不明确，研究希望通过模型合并方法探索这一结合方式及其内部机制。

Method: 采用跨模态的模型合并方法，将不同模型的参数连接起来，特别是将LLMs的推理能力整合到VLMs中。

Result: 实验表明，模型合并能成功无训练地转移推理能力，并发现感知能力主要分布在模型的早期层，推理能力则集中在中后期层；合并后所有层均参与推理，而感知能力的分布保持不变。

Conclusion: 模型合并是一种有效的多模态整合和解释工具，能促进感知与推理能力的结合，同时提供对模型内部机制的新见解。

Abstract: Vision-Language Models (VLMs) combine visual perception with the general
capabilities, such as reasoning, of Large Language Models (LLMs). However, the
mechanisms by which these two abilities can be combined and contribute remain
poorly understood. In this work, we explore to compose perception and reasoning
through model merging that connects parameters of different models. Unlike
previous works that often focus on merging models of the same kind, we propose
merging models across modalities, enabling the incorporation of the reasoning
capabilities of LLMs into VLMs. Through extensive experiments, we demonstrate
that model merging offers a successful pathway to transfer reasoning abilities
from LLMs to VLMs in a training-free manner. Moreover, we utilize the merged
models to understand the internal mechanism of perception and reasoning and how
merging affects it. We find that perception capabilities are predominantly
encoded in the early layers of the model, whereas reasoning is largely
facilitated by the middle-to-late layers. After merging, we observe that all
layers begin to contribute to reasoning, whereas the distribution of perception
abilities across layers remains largely unchanged. These observations shed
light on the potential of model merging as a tool for multimodal integration
and interpretation.

</details>


### [49] [ComPO: Preference Alignment via Comparison Oracles](https://arxiv.org/abs/2505.05465)
*Peter Chen,Xi Chen,Wotao Yin,Tianyi Lin*

Main category: cs.CL

TL;DR: 本文提出了一种基于比较oracle的新偏好对齐方法，并提供了其基础方案的收敛保证，同时通过启发式改进和实验验证了该方法在噪声偏好对上的灵活性和兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有直接对齐方法存在冗长和似然偏移问题，主要由于噪声偏好对导致首选和非首选响应的似然相似，因此需要设计更有效的对齐方法。

Method: 提出基于比较oracle的偏好对齐方法，并改进其启发式方案，实验验证了其在噪声偏好对上的表现。

Result: 在多个基线和指令调优模型（Mistral-7B、Llama-3-8B、Gemma-2-9B）及基准测试（AlpacaEval 2、MT-Bench、Arena-Hard）中验证了方法的有效性。

Conclusion: 该方法有效解决了现有直接对齐方法的局限性，并强调了针对不同似然边际设计专用方法的重要性。

Abstract: Direct alignment methods are increasingly used for aligning large language
models (LLMs) with human preferences. However, these methods suffer from the
issues of verbosity and likelihood displacement, which can be driven by the
noisy preference pairs that induce similar likelihood for preferred and
dispreferred responses. The contributions of this paper are two-fold. First, we
propose a new preference alignment method based on comparison oracles and
provide the convergence guarantee for its basic scheme. Second, we improve our
method using some heuristics and conduct the experiments to demonstrate the
flexibility and compatibility of practical scheme in improving the performance
of LLMs using noisy preference pairs. Evaluations are conducted across multiple
base and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with
benchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show
the effectiveness of our method as an alternative to addressing the limitations
of existing direct alignment methods. A highlight of our work is that we
evidence the importance of designing specialized methods for preference pairs
with distinct likelihood margin, which complements the recent findings in
\citet{Razin-2025-Unintentional}.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [50] [MatMMFuse: Multi-Modal Fusion model for Material Property Prediction](https://arxiv.org/abs/2505.04634)
*Abhiroop Bhattacharya,Sylvain G. Cloutier*

Main category: cs.LG

TL;DR: 提出了一种多模态融合模型MatMMFuse，结合晶体图卷积网络（CGCNN）和SciBERT文本嵌入，通过多头注意力机制优化材料性能预测，相比单模态模型性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 单模态模型无法充分利用不同表征的优势，尤其是预训练大语言模型（LLMs）的知识和大规模文本嵌入，与图编码器的局部特征学习能力结合，可能提升材料性能预测效果。

Method: 提出MatMMFuse模型，使用多头注意力机制融合CGCNN的结构感知嵌入和SciBERT的文本嵌入，并在Materials Project数据集上端到端训练。

Result: 相比单一CGCNN和SciBERT模型，在多关键性能预测（如形成能、带隙等）上表现更优，其中形成能预测提升40%（CGCNN）和68%（SciBERT），零样本任务也优于单模态。

Conclusion: 多模态融合在材料科学中具有潜力，尤其适用于数据稀缺的工业场景，展示了零样本迁移学习的实用性。

Abstract: The recent progress of using graph based encoding of crystal structures for
high throughput material property prediction has been quite successful.
However, using a single modality model prevents us from exploiting the
advantages of an enhanced features space by combining different
representations. Specifically, pre-trained Large language models(LLMs) can
encode a large amount of knowledge which is beneficial for training of models.
Moreover, the graph encoder is able to learn the local features while the text
encoder is able to learn global information such as space group and crystal
symmetry. In this work, we propose Material Multi-Modal Fusion(MatMMFuse), a
fusion based model which uses a multi-head attention mechanism for the
combination of structure aware embedding from the Crystal Graph Convolution
Network (CGCNN) and text embeddings from the SciBERT model. We train our model
in an end-to-end framework using data from the Materials Project Dataset. We
show that our proposed model shows an improvement compared to the vanilla CGCNN
and SciBERT model for all four key properties: formation energy, band gap,
energy above hull and fermi energy. Specifically, we observe an improvement of
40% compared to the vanilla CGCNN model and 68% compared to the SciBERT model
for predicting the formation energy per atom. Importantly, we demonstrate the
zero shot performance of the trained model on small curated datasets of
Perovskites, Chalcogenides and the Jarvis Dataset. The results show that the
proposed model exhibits better zero shot performance than the individual plain
vanilla CGCNN and SciBERT model. This enables researchers to deploy the model
for specialized industrial applications where collection of training data is
prohibitively expensive.

</details>


### [51] [Conformal Prediction with Corrupted Labels: Uncertain Imputation and Robust Re-weighting](https://arxiv.org/abs/2505.04733)
*Shai Feldman,Stephen Bates,Yaniv Romano*

Main category: cs.LG

TL;DR: 提出了一个在标注训练数据被噪声或缺失标签污染情况下的稳健不确定性量化框架，通过改进的共形预测方法（PCP）和新的不确定性填补方法（UI），并验证其在权重估计不准确时的有效性。


<details>
  <summary>Details</summary>
Motivation: 在数据存在噪声或缺失标签的情况下，传统的共形预测方法由于不满足独立同分布假设而失效。本文旨在解决这一问题，提供稳健的不确定性量化。

Method: 结合了特权信息（PCP）和不确定性填补（UI）的方法，并通过三重稳健框架确保至少一个方法的有效性。

Result: 理论分析和实验验证表明，PCP在权重估计不准确时仍能提供有效的不确定性估计，UI方法也展示了其潜力。

Conclusion: 所提方法在数据污染情况下仍能提供统计上有效的预测，尤其在多方法结合时表现更为稳健。

Abstract: We introduce a framework for robust uncertainty quantification in situations
where labeled training data are corrupted, through noisy or missing labels. We
build on conformal prediction, a statistical tool for generating prediction
sets that cover the test label with a pre-specified probability. The validity
of conformal prediction, however, holds under the i.i.d assumption, which does
not hold in our setting due to the corruptions in the data. To account for this
distribution shift, the privileged conformal prediction (PCP) method proposed
leveraging privileged information (PI) -- additional features available only
during training -- to re-weight the data distribution, yielding valid
prediction sets under the assumption that the weights are accurate. In this
work, we analyze the robustness of PCP to inaccuracies in the weights. Our
analysis indicates that PCP can still yield valid uncertainty estimates even
when the weights are poorly estimated. Furthermore, we introduce uncertain
imputation (UI), a new conformal method that does not rely on weight
estimation. Instead, we impute corrupted labels in a way that preserves their
uncertainty. Our approach is supported by theoretical guarantees and validated
empirically on both synthetic and real benchmarks. Finally, we show that these
techniques can be integrated into a triply robust framework, ensuring
statistically valid predictions as long as at least one underlying method is
valid.

</details>


### [52] [SetONet: A Deep Set-based Operator Network for Solving PDEs with permutation invariant variable input sampling](https://arxiv.org/abs/2505.04738)
*Stepan Tretiakov,Xingjian Li,Krishna Kumar*

Main category: cs.LG

TL;DR: SetONet是一种新型神经网络架构，结合Deep Sets原理解决DeepONet在可变传感器配置等场景中的局限，提升操作学习的灵活性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 解决标准DeepONet因固定采样位置限制其在可变传感器配置、缺失数据或不规则网格场景中的适用性问题。

Method: 通过将输入函数作为无序的位置-值对集合处理，设计具有排列不变性的SetONet分支网络，增强输入表示。

Result: 在多个基准问题中，SetONet在可变输入采样条件下成功学习操作，且在固定网格上表现优于DeepONet。

Conclusion: SetONet扩展了神经操作工具包，显著提升了在可变或不完整输入数据问题中的适用性。

Abstract: Neural operators, particularly the Deep Operator Network (DeepONet), have
shown promise in learning mappings between function spaces for solving
differential equations. However, standard DeepONet requires input functions to
be sampled at fixed locations, limiting its applicability in scenarios with
variable sensor configurations, missing data, or irregular grids. We introduce
the Set Operator Network (SetONet), a novel architecture that integrates Deep
Sets principles into the DeepONet framework to address this limitation. The
core innovation lies in the SetONet branch network, which processes the input
function as an unordered \emph{set} of location-value pairs. This design
ensures permutation invariance with respect to the input points, making SetONet
inherently robust to variations in the number and locations of sensors. SetONet
learns richer, spatially-aware input representations by explicitly processing
spatial coordinates and function values. We demonstrate SetONet's effectiveness
on several benchmark problems, including derivative/anti-derivative operators,
1D Darcy flow, and 2D elasticity. Results show that SetONet successfully learns
operators under variable input sampling conditions where standard DeepONet
fails. Furthermore, SetONet is architecturally robust to sensor drop-off;
unlike standard DeepONet, which requires methods like interpolation to function
with missing data. Notably, SetONet can achieve comparable or improved accuracy
over DeepONet on fixed grids, particularly for nonlinear problems, likely due
to its enhanced input representation. SetONet provides a flexible and robust
extension to the neural operator toolkit, significantly broadening the
applicability of operator learning to problems with variable or incomplete
input data.

</details>


### [53] [When Bad Data Leads to Good Models](https://arxiv.org/abs/2505.04741)
*Kenneth Li,Yida Chen,Fernanda Viégas,Martin Wattenberg*

Main category: cs.LG

TL;DR: 这篇论文探讨了在大型语言模型预训练中，毒性数据比例的增加可能通过后训练技术（如推理时干预）更有效地降低模型输出的毒性，同时保持其通用能力。研究发现，虽然毒性数据会增加基础模型的生成毒性，但也使得毒性更容易被移除。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为预训练数据的质量直接决定模型质量。本文通过研究毒性数据在预训练中的影响，探索了通过后训练设计来改善模型表现的可能性，重新定义了“数据质量”的概念。

Method: 研究首先通过玩具实验分析数据组成对特征空间几何的影响。随后，使用Olmo-1B模型在不同比例的清洁和毒性数据上进行训练，评估毒性数据的增加对表示空间线性可分性的影响。同时，应用推理时干预技术（ITI）来减轻毒性。

Result: 实验发现，增加毒性数据比例可以使得毒性概念的表示更加线性分离，从而更易移除。尽管基础模型的生成毒性增加，但后训练技术能更有效地降低毒性，同时在Toxigen和Real Toxicity Prompts等评测中保持了模型的通用能力。

Conclusion: 研究表明，在考虑后训练技术的情况下，预训练中使用“坏”数据（如毒性数据）可能反而有助于生成“好”模型。这一点挑战了传统的数据质量观念，并为未来模型设计提供了新的思路。

Abstract: In large language model (LLM) pretraining, data quality is believed to
determine model quality. In this paper, we re-examine the notion of "quality"
from the perspective of pre- and post-training co-design. Specifically, we
explore the possibility that pre-training on more toxic data can lead to better
control in post-training, ultimately decreasing a model's output toxicity.
First, we use a toy experiment to study how data composition affects the
geometry of features in the representation space. Next, through controlled
experiments with Olmo-1B models trained on varying ratios of clean and toxic
data, we find that the concept of toxicity enjoys a less entangled linear
representation as the proportion of toxic data increases. Furthermore, we show
that although toxic data increases the generational toxicity of the base model,
it also makes the toxicity easier to remove. Evaluations on Toxigen and Real
Toxicity Prompts demonstrate that models trained on toxic data achieve a better
trade-off between reducing generational toxicity and preserving general
capabilities when detoxifying techniques such as inference-time intervention
(ITI) are applied. Our findings suggest that, with post-training taken into
account, bad data may lead to good models.

</details>


### [54] [Primal-dual algorithm for contextual stochastic combinatorial optimization](https://arxiv.org/abs/2505.04757)
*Louis Bouvier,Thibault Prunet,Vincent Leclère,Axel Parmentier*

Main category: cs.LG

TL;DR: 本文提出了一种结合运筹学与机器学习的上下文随机优化新方法，通过神经网络和组合优化层编码策略，旨在最小化经验风险。


<details>
  <summary>Details</summary>
Motivation: 传统方法无法有效利用上下文信息，因此需要开发新算法以更好地处理不确定性下的决策问题。

Method: 采用带有组合优化层的神经网络，提出替代学习问题和通用原始-对偶算法，并引入稀疏扰动正则化方法。

Result: 算法在特定条件下线性收敛，且在上下文随机最小生成树问题中表现高效且可扩展，性能接近模仿学习。

Conclusion: 该方法为随机优化提供了一种新框架，具有实际应用潜力，尤其在需要上下文信息的情境中表现出色。

Abstract: This paper introduces a novel approach to contextual stochastic optimization,
integrating operations research and machine learning to address decision-making
under uncertainty. Traditional methods often fail to leverage contextual
information, which underscores the necessity for new algorithms. In this study,
we utilize neural networks with combinatorial optimization layers to encode
policies. Our goal is to minimize the empirical risk, which is estimated from
past data on uncertain parameters and contexts. To that end, we present a
surrogate learning problem and a generic primal-dual algorithm that is
applicable to various combinatorial settings in stochastic optimization. Our
approach extends classic Fenchel-Young loss results and introduces a new
regularization method using sparse perturbations on the distribution simplex.
This allows for tractable updates in the original space and can accommodate
diverse objective functions. We demonstrate the linear convergence of our
algorithm under certain conditions and provide a bound on the non-optimality of
the resulting policy in terms of the empirical risk. Experiments on a
contextual stochastic minimum weight spanning tree problem show that our
algorithm is efficient and scalable, achieving performance comparable to
imitation learning of solutions computed using an expensive Lagrangian-based
heuristic.

</details>


### [55] [Prediction via Shapley Value Regression](https://arxiv.org/abs/2505.04775)
*Amr Alkhatib,Roman Bresson,Henrik Boström,Michalis Vazirgiannis*

Main category: cs.LG

TL;DR: ViaSHAP提出了一种直接学习计算Shapley值的方法，避免传统后处理的高计算成本，并通过两种实现方式展示了其在表格数据和图像中的优越表现。


<details>
  <summary>Details</summary>
Motivation: 传统Shapley值计算需要额外推断成本，ViaSHAP旨在通过直接学习其计算函数来提高效率。

Method: 采用基于通用逼近定理和Kolmogorov-Arnold表示定理的两种方法实现ViaSHAP。

Result: 实验表明ViaSHAP在表格数据上表现媲美SOTA，且在解释准确性上显著优于FastSHAP。

Conclusion: ViaSHAP提供了高效且准确的模型解释方案，适用于多种数据类型。

Abstract: Shapley values have several desirable, theoretically well-supported,
properties for explaining black-box model predictions. Traditionally, Shapley
values are computed post-hoc, leading to additional computational cost at
inference time. To overcome this, a novel method, called ViaSHAP, is proposed,
that learns a function to compute Shapley values, from which the predictions
can be derived directly by summation. Two approaches to implement the proposed
method are explored; one based on the universal approximation theorem and the
other on the Kolmogorov-Arnold representation theorem. Results from a
large-scale empirical investigation are presented, showing that ViaSHAP using
Kolmogorov-Arnold Networks performs on par with state-of-the-art algorithms for
tabular data. It is also shown that the explanations of ViaSHAP are
significantly more accurate than the popular approximator FastSHAP on both
tabular data and images.

</details>


### [56] [Robust ML Auditing using Prior Knowledge](https://arxiv.org/abs/2505.04796)
*Jade Garcia Bourrée,Augustin Godinot,Martijn De Vos,Milos Vujasinovic,Sayan Biswas,Gilles Tredan,Erwan Le Merrer,Anne-Marie Kermarrec*

Main category: cs.LG

TL;DR: 为了防止机器学习决策系统在公平性审计中被操纵，本文提出了一种基于审计师先验知识的防操纵审计方法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于ML系统广泛采用，其公平性审计中存在被操纵的风险，即平台可能为通过审计而欺骗监管者，而现有方法未充分解决这一问题。

Method: 通过利用审计师的先验知识，特别是对任务真实情况的了解，设计了防操纵的审计方法，并分析了有效审计的条件。

Result: 研究表明，依赖公开先验数据的审计易被操纵，而基于真实先验知识的审计能有效防止欺骗行为，实验展示了平台可隐藏的最大不公平程度。

Conclusion: 本文提出的先验知识防操纵审计方法为公平性审计提供了新方向，增强了对ML系统公平性的信任和监管的稳健性。

Abstract: The rapid adoption of ML decision-making systems across products and services
has led to a set of regulations on how such systems should behave and be built.
Among all the technical challenges to enforcing these regulations, one crucial,
yet under-explored problem is the risk of manipulation while these systems are
being audited for fairness. This manipulation occurs when a platform
deliberately alters its answers to a regulator to pass an audit without
modifying its answers to other users. In this paper, we introduce a novel
approach to manipulation-proof auditing by taking into account the auditor's
prior knowledge of the task solved by the platform. We first demonstrate that
regulators must not rely on public priors (e.g. a public dataset), as platforms
could easily fool the auditor in such cases. We then formally establish the
conditions under which an auditor can prevent audit manipulations using prior
knowledge about the ground truth. Finally, our experiments with two standard
datasets exemplify the maximum level of unfairness a platform can hide before
being detected as malicious. Our formalization and generalization of
manipulation-proof auditing with a prior opens up new research directions for
more robust fairness audits.

</details>


### [57] [ORBIT-2: Scaling Exascale Vision Foundation Models for Weather and Climate Downscaling](https://arxiv.org/abs/2505.04802)
*Xiao Wang,Jong-Youl Choi,Takuya Kurihaya,Isaac Lyngaas,Hong-Jun Yoon,Ming Fan,Nasik Muhammad Nafi,Aristeidis Tsaris,Ashwin M. Aji,Maliha Hossain,Mohamed Wahib,Dali Wang,Peter Thornton,Prasanna Balaprakash,Moetasim Ashfaq,Dan Lu*

Main category: cs.LG

TL;DR: 论文提出了一种名为ORBIT-2的全球超分辨率气候降尺度基础模型，通过轻量级架构和算法创新解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 稀疏观测数据和低分辨率气候模型限制了区域决策的有效性，而现有AI方法在变量和地理区域的泛化能力不足，且受限于ViT自注意力的二次复杂度。

Method: ORBIT-2包含两项关键创新：(1) Residual Slim ViT (Reslim)，一个轻量级架构，结合残差学习和贝叶斯正则化；(2) TILES算法，将自注意力复杂度降至线性。

Result: ORBIT-2扩展到32768个GPU上的100亿参数，实现了高达1.8 ExaFLOPS的持续吞吐量和92-98%的强扩展效率，在7 km分辨率基准测试中R^2得分为0.98-0.99。

Conclusion: ORBIT-2通过高效的架构和算法创新，实现了全球高分辨率气候降尺度，为气候预测提供了可靠工具。

Abstract: Sparse observations and coarse-resolution climate models limit effective
regional decision-making, underscoring the need for robust downscaling.
However, existing AI methods struggle with generalization across variables and
geographies and are constrained by the quadratic complexity of Vision
Transformer (ViT) self-attention. We introduce ORBIT-2, a scalable foundation
model for global, hyper-resolution climate downscaling. ORBIT-2 incorporates
two key innovations: (1) Residual Slim ViT (Reslim), a lightweight architecture
with residual learning and Bayesian regularization for efficient, robust
prediction; and (2) TILES, a tile-wise sequence scaling algorithm that reduces
self-attention complexity from quadratic to linear, enabling long-sequence
processing and massive parallelism. ORBIT-2 scales to 10 billion parameters
across 32,768 GPUs, achieving up to 1.8 ExaFLOPS sustained throughput and
92-98% strong scaling efficiency. It supports downscaling to 0.9 km global
resolution and processes sequences up to 4.2 billion tokens. On 7 km resolution
benchmarks, ORBIT-2 achieves high accuracy with R^2 scores in the range of 0.98
to 0.99 against observation data.

</details>


### [58] [Piecewise Constant Spectral Graph Neural Network](https://arxiv.org/abs/2505.04808)
*Vahan Martirosyan,Jhony H. Giraldo,Fragkiskos D. Malliaros*

Main category: cs.LG

TL;DR: PieCoN通过结合常数和多项式谱滤波器，自适应划分频谱区间，提升了图神经网络的性能，尤其在异质性数据集上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有谱GNN因多项式滤波器度数低无法充分捕捉图的频谱特性，而增加度数会导致计算成本高且性能提升有限。

Method: 提出PieCoN模型，结合常数谱滤波器和多项式滤波器，自适应划分频谱区间以更灵活利用图结构。

Result: 在9个基准数据集（包括同质和异质图）上的实验显示，PieCoN在异质图上表现尤其优异。

Conclusion: PieCoN通过灵活频谱划分有效提升了GNN性能，展现了广泛的应用潜力。

Abstract: Graph Neural Networks (GNNs) have achieved significant success across various
domains by leveraging graph structures in data. Existing spectral GNNs, which
use low-degree polynomial filters to capture graph spectral properties, may not
fully identify the graph's spectral characteristics because of the polynomial's
small degree. However, increasing the polynomial degree is computationally
expensive and beyond certain thresholds leads to performance plateaus or
degradation. In this paper, we introduce the Piecewise Constant Spectral Graph
Neural Network(PieCoN) to address these challenges. PieCoN combines constant
spectral filters with polynomial filters to provide a more flexible way to
leverage the graph structure. By adaptively partitioning the spectrum into
intervals, our approach increases the range of spectral properties that can be
effectively learned. Experiments on nine benchmark datasets, including both
homophilic and heterophilic graphs, demonstrate that PieCoN is particularly
effective on heterophilic datasets, highlighting its potential for a wide range
of applications.

</details>


### [59] [Guide your favorite protein sequence generative model](https://arxiv.org/abs/2505.04823)
*Junhao Xiong,Hunter Nisonoff,Ishan Gaur,Jennifer Listgarten*

Main category: cs.LG

TL;DR: ProteinGuide框架可条件化生成蛋白质序列，通过整合多种生成模型（如掩码语言、扩散模型等），支持用户指定性质（如稳定性或特定折叠）的序列生成。实验展示了在ProteinMPNN和ESM3上的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一框架来灵活整合辅助信息（如实验反馈或分类器）以指导蛋白质生成模型。

Method: 提出ProteinGuide框架，统一多种生成模型（掩码语言、自回归、扩散、流匹配），实现对预训练模型的条件化采样。

Result: 成功引导ProteinMPNN和ESM3生成符合用户指定性质（稳定性和CATH折叠标签）的氨基酸与结构序列。

Conclusion: ProteinGuide为条件化蛋白质生成提供了通用且严谨的方法，拓宽了生成模型在蛋白质工程中的应用场景。

Abstract: Generative machine learning models have begun to transform protein
engineering, yet no principled framework for conditioning on auxiliary
information in a plug-and-play manner exists; one may want to iteratively
incorporate experimental feedback, or make use of an existing classifier --
such as for predicting enzyme commission number -- in order to guide the
sampling of the generative model to generate sequences with desired properties.
Herein, we present ProteinGuide, a rigorous and general framework to achieve
just that: through unifying a broad class of protein generative models that
includes masked language, (order-agnostic) autoregressive, diffusion and
flow-matching models, we provide an approach to statistically condition
pre-trained protein generative models. We demonstrate applicability of our
approach by guiding each of two commonly used protein generative models,
ProteinMPNN and ESM3, to generate amino acid and structure token sequences
conditioned on several user-specified properties, namely, enhanced stability
and CATH-labeled fold generation.

</details>


### [60] [Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers](https://arxiv.org/abs/2505.04842)
*Kusha Sareen,Morgane M Moss,Alessandro Sordoni,Rishabh Agarwal,Arian Hosseini*

Main category: cs.LG

TL;DR: 论文提出了RL$^V$方法，通过在强化学习中联合训练LLM作为推理器和生成验证器，显著提升了MATH任务的准确性和测试时计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法（如GRPO或Leave-one-out PPO）舍弃了已学习的价值函数，仅依赖经验估计的回报，这限制了依赖价值函数验证的测试时计算扩展能力。

Method: RL$^V$方法通过联合训练LLM作为推理器和生成验证器，利用RL生成的数据添加验证能力，而不引入显著开销。

Result: RL$^V$在并行采样下将MATH任务准确率提升超过20%，测试时计算效率提升8-32倍，并在多种任务中表现出强泛化能力。

Conclusion: RL$^V$不仅提升了性能，还通过联合扩展并行和顺序测试时计算，进一步提高了推理模型的表现。

Abstract: Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners,
such as GRPO or Leave-one-out PPO, abandon the learned value function in favor
of empirically estimated returns. This hinders test-time compute scaling that
relies on using the value-function for verification. In this work, we propose
RL$^V$ that augments any ``value-free'' RL method by jointly training the LLM
as both a reasoner and a generative verifier using RL-generated data, adding
verification capabilities without significant overhead. Empirically, RL$^V$
boosts MATH accuracy by over 20\% with parallel sampling and enables
$8-32\times$ efficient test-time compute scaling compared to the base RL
method. RL$^V$ also exhibits strong generalization capabilities for both
easy-to-hard and out-of-domain tasks. Furthermore, RL$^V$ achieves
$1.2-1.6\times$ higher performance when jointly scaling parallel and sequential
test-time compute with a long reasoning R1 model.

</details>


### [61] [Federated Learning for Cyber Physical Systems: A Comprehensive Survey](https://arxiv.org/abs/2505.04873)
*Minh K. Quan,Pubudu N. Pathirana,Mayuri Wijayasundara,Sujeeva Setunge,Dinh C. Nguyen,Christopher G. Brinton,David J. Love,H. Vincent Poor*

Main category: cs.LG

TL;DR: 本文综述了联邦学习（FL）在信息物理系统（CPS）中的最新进展，包括应用领域、系统拓扑和算法，并对比了FL在CPS与物联网（IoT）中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在CPS中集成的复杂性，如实时决策、安全性、设备异构性和数据隐私等问题，并探讨FL在CPS中的潜力。

Method: 通过综合分析最新的FL和CPS技术进展，比较FL在CPS与IoT中的应用，并深入探讨FL在关键CPS领域的应用案例。

Result: 提供了FL-CPS的多领域应用实例和系统实现经验，揭示了FL在CPS中的优势和挑战。

Conclusion: 提出了FL-CPS领域的未来研究方向，强调需要解决的关键问题以推动这一领域的发展。

Abstract: The integration of machine learning (ML) in cyber physical systems (CPS) is a
complex task due to the challenges that arise in terms of real-time decision
making, safety, reliability, device heterogeneity, and data privacy. There are
also open research questions that must be addressed in order to fully realize
the potential of ML in CPS. Federated learning (FL), a distributed approach to
ML, has become increasingly popular in recent years. It allows models to be
trained using data from decentralized sources. This approach has been gaining
popularity in the CPS field, as it integrates computer, communication, and
physical processes. Therefore, the purpose of this work is to provide a
comprehensive analysis of the most recent developments of FL-CPS, including the
numerous application areas, system topologies, and algorithms developed in
recent years. The paper starts by discussing recent advances in both FL and
CPS, followed by their integration. Then, the paper compares the application of
FL in CPS with its applications in the internet of things (IoT) in further
depth to show their connections and distinctions. Furthermore, the article
scrutinizes how FL is utilized in critical CPS applications, e.g., intelligent
transportation systems, cybersecurity services, smart cities, and smart
healthcare solutions. The study also includes critical insights and lessons
learned from various FL-CPS implementations. The paper's concluding section
delves into significant concerns and suggests avenues for further research in
this fast-paced and dynamic era.

</details>


### [62] [ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning](https://arxiv.org/abs/2505.04881)
*Ziqing Qiao,Yongheng Deng,Jiali Zeng,Dong Wang,Lai Wei,Fandong Meng,Jie Zhou,Ju Ren,Yaoxue Zhang*

Main category: cs.LG

TL;DR: ConCISE框架通过自信引导的压缩步骤，减少大型推理模型的冗余输出，降低50%长度且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型(LRMs)在复杂推理任务中表现优异，但冗长的输出增加了计算开销并降低了用户体验。现有压缩方法存在破坏推理连贯性或干预效果不足的问题。

Method: 提出ConCISE框架，通过分析‘自信缺失’和‘终止延迟’两种冗余模式，采用自信注入和早期终止技术优化推理过程。

Result: 实验表明，ConCISE能减少50%输出长度，同时保持高任务准确性，在多推理基准测试中优于现有基线。

Conclusion: ConCISE为LRMs提供了一种高效的推理压缩方法，显著提升了模型实用性。

Abstract: Large Reasoning Models (LRMs) perform strongly in complex reasoning tasks via
Chain-of-Thought (CoT) prompting, but often suffer from verbose outputs caused
by redundant content, increasing computational overhead, and degrading user
experience. Existing compression methods either operate post-hoc pruning,
risking disruption to reasoning coherence, or rely on sampling-based selection,
which fails to intervene effectively during generation. In this work, we
introduce a confidence-guided perspective to explain the emergence of redundant
reflection in LRMs, identifying two key patterns: Confidence Deficit, where the
model reconsiders correct steps due to low internal confidence, and Termination
Delay, where reasoning continues even after reaching a confident answer. Based
on this analysis, we propose ConCISE (Confidence-guided Compression In
Step-by-step Efficient Reasoning), a framework that simplifies reasoning chains
by reinforcing the model's confidence during inference, thus preventing the
generation of redundant reflection steps. It integrates Confidence Injection to
stabilize intermediate steps and Early Stopping to terminate reasoning when
confidence is sufficient. Extensive experiments demonstrate that fine-tuning
LRMs on ConCISE-generated data yields significantly shorter outputs, reducing
length by up to approximately 50% under SimPO, while maintaining high task
accuracy. ConCISE consistently outperforms existing baselines across multiple
reasoning benchmarks.

</details>


### [63] [FedRE: Robust and Effective Federated Learning with Privacy Preference](https://arxiv.org/abs/2505.04889)
*Tianzhe Xiao,Yichen Li,Yu Zhou,Yining Qi,Yi Liu,Wei Wang,Haozhao Wang,Yi Wang,Ruixuan Li*

Main category: cs.LG

TL;DR: 论文提出FedRE，一种结合局部差分隐私（LDP）保护的方法，通过识别并优化对隐私敏感信息（PSI）的保护，同时减少对非敏感信息的噪声干扰，提升联邦学习模型的性能和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习中的局部差分隐私方法对所有样本采用相同噪声机制，未能考虑客户端对隐私敏感信息的个性化需求，导致非敏感信息过度保护而影响模型性能。

Method: 定义PSI并根据客户端隐私偏好分层优化LDP隐私预算分配，同时设计基于扰动信息分布的参数聚合机制以减少性能损失。

Result: 在T-SROIE和DocTamper数据集上的文本篡改检测实验中，FedRE表现出与最先进方法竞争的性能。

Conclusion: FedRE通过结合PSI感知的LDP优化和参数聚合机制，有效平衡了隐私保护和模型性能，是联邦学习中隐私保护的实用解决方案。

Abstract: Despite Federated Learning (FL) employing gradient aggregation at the server
for distributed training to prevent the privacy leakage of raw data, private
information can still be divulged through the analysis of uploaded gradients
from clients. Substantial efforts have been made to integrate local
differential privacy (LDP) into the system to achieve a strict privacy
guarantee. However, existing methods fail to take practical issues into account
by merely perturbing each sample with the same mechanism while each client may
have their own privacy preferences on privacy-sensitive information (PSI),
which is not uniformly distributed across the raw data. In such a case,
excessive privacy protection from private-insensitive information can
additionally introduce unnecessary noise, which may degrade the model
performance. In this work, we study the PSI within data and develop FedRE, that
can simultaneously achieve robustness and effectiveness benefits with LDP
protection. More specifically, we first define PSI with regard to the privacy
preferences of each client. Then, we optimize the LDP by allocating less
privacy budget to gradients with higher PSI in a layer-wise manner, thus
providing a stricter privacy guarantee for PSI. Furthermore, to mitigate the
performance degradation caused by LDP, we design a parameter aggregation
mechanism based on the distribution of the perturbed information. We conducted
experiments with text tamper detection on T-SROIE and DocTamper datasets, and
FedRE achieves competitive performance compared to state-of-the-art methods.

</details>


### [64] [Clustering with Communication: A Variational Framework for Single Cell Representation Learning](https://arxiv.org/abs/2505.04891)
*Cong Qi,Yeqing Chen,Jie Zhang,Wei Zhi*

Main category: cs.LG

TL;DR: 该论文提出了一种名为CCCVAE的新型变分自编码器框架，通过融入细胞间通信信号来改进单细胞RNA测序数据的表示学习，优于传统VAE方法。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序（scRNA-seq）揭示了细胞的复杂异质性，但理解生物功能还需要建模细胞间通信（CCC），即通过配体-受体对介导的信号相互作用。现有的工具如CellChat已证明CCC在细胞分化、组织再生和免疫反应中起关键作用。

Method: 作者提出了CCCVAE，一种结合CCC信号的变分自编码器框架，利用配体-受体相互作用的通信感知核和稀疏高斯过程，将生物先验知识编码到潜在空间中。与传统VAE不同，CCCVAE鼓励潜在嵌入反映转录相似性和细胞间信号上下文。

Result: 在四个scRNA-seq数据集上的实验结果表明，CCCVAE在聚类性能上优于标准VAE基线，获得了更高的评估分数。

Conclusion: 本研究证明了在无监督单细胞分析中，将生物先验知识嵌入深度生成模型的价值。

Abstract: Single-cell RNA sequencing (scRNA-seq) has revealed complex cellular
heterogeneity, but recent studies emphasize that understanding biological
function also requires modeling cell-cell communication (CCC), the signaling
interactions mediated by ligand-receptor pairs that coordinate cellular
behavior. Tools like CellChat have demonstrated that CCC plays a critical role
in processes such as cell differentiation, tissue regeneration, and immune
response, and that transcriptomic data inherently encodes rich information
about intercellular signaling. We propose CCCVAE, a novel variational
autoencoder framework that incorporates CCC signals into single-cell
representation learning. By leveraging a communication-aware kernel derived
from ligand-receptor interactions and a sparse Gaussian process, CCCVAE encodes
biologically informed priors into the latent space. Unlike conventional VAEs
that treat each cell independently, CCCVAE encourages latent embeddings to
reflect both transcriptional similarity and intercellular signaling context.
Empirical results across four scRNA-seq datasets show that CCCVAE improves
clustering performance, achieving higher evaluation scores than standard VAE
baselines. This work demonstrates the value of embedding biological priors into
deep generative models for unsupervised single-cell analysis.

</details>


### [65] [GCN-Based Throughput-Oriented Handover Management in Dense 5G Vehicular Networks](https://arxiv.org/abs/2505.04894)
*Nazanin Mehregan,Robson E. De Grande*

Main category: cs.LG

TL;DR: 本文提出了一种基于图卷积网络（TH-GCN）的新型方法，用于优化密集5G网络中的切换管理，有效减少了切换次数并提升信号质量。


<details>
  <summary>Details</summary>
Motivation: 5G网络的快速发展和车辆网络的高移动性导致频繁切换和网络不稳定，亟需一种能自适应优化切换管理的解决方案。

Method: 通过图神经网络（GNN）建模车辆和基站为动态图中的节点，整合信号质量、吞吐量等特征，实现双中心的自适应实时切换决策。

Result: 仿真结果显示，TH-GCN减少了78%的切换次数，信号质量提升10%，性能优于现有方法。

Conclusion: TH-GCN作为一种双中心的自适应切换优化方法，显著提升了5G网络在高移动环境中的稳定性。

Abstract: The rapid advancement of 5G has transformed vehicular networks, offering high
bandwidth, low latency, and fast data rates essential for real-time
applications in smart cities and vehicles. These improvements enhance traffic
safety and entertainment services. However, the limited coverage and frequent
handovers in 5G networks cause network instability, especially in high-mobility
environments due to the ping-pong effect. This paper presents TH-GCN
(Throughput-oriented Graph Convolutional Network), a novel approach for
optimizing handover management in dense 5G networks. Using graph neural
networks (GNNs), TH-GCN models vehicles and base stations as nodes in a dynamic
graph enriched with features such as signal quality, throughput, vehicle speed,
and base station load. By integrating both user equipment and base station
perspectives, this dual-centric approach enables adaptive, real-time handover
decisions that improve network stability. Simulation results show that TH-GCN
reduces handovers by up to 78 percent and improves signal quality by 10
percent, outperforming existing methods.

</details>


### [66] [Precise gradient descent training dynamics for finite-width multi-layer neural networks](https://arxiv.org/abs/2505.04898)
*Qiyang Han,Masaaki Imaizumi*

Main category: cs.LG

TL;DR: 这篇论文首次在多层神经网络中，针对有限宽度比例机制下的单指数回归模型，提供了梯度下降迭代的精确分布特性。其非渐近状态演化理论捕捉了第一层权重的高斯波动和更深层权重的集中现象，适用于非高斯特征。


<details>
  <summary>Details</summary>
Motivation: 现有理论（如NTK、MF和TP）主要基于无限宽度机制或特定初始化方案，无法准确描述有限宽度下多层网络的训练和泛化行为。本文旨在填补这一空白，提供更贴近实际场景的理论分析。

Method: 通过非渐近状态演化理论，研究梯度下降在有限宽度比例机制下的行为，涵盖权重的高斯波动、集中现象以及非高斯特征的影响。

Result: 理论表明，梯度下降能生成一致性的泛化误差估计，可用于早停和超参数调优。即使模型存在误设，学习到的模型仍保留单指数函数结构，其有效信号由真实信号和初始化的线性组合决定。

Conclusion: 该理论为有限宽度下多层神经网络的训练和泛化提供了新的分析框架，突破了现有理论的局限性，并展示了梯度下降在实际应用中的潜力。

Abstract: In this paper, we provide the first precise distributional characterization
of gradient descent iterates for general multi-layer neural networks under the
canonical single-index regression model, in the `finite-width proportional
regime' where the sample size and feature dimension grow proportionally while
the network width and depth remain bounded. Our non-asymptotic state evolution
theory captures Gaussian fluctuations in first-layer weights and concentration
in deeper-layer weights, and remains valid for non-Gaussian features.
  Our theory differs from existing neural tangent kernel (NTK), mean-field (MF)
theories and tensor program (TP) in several key aspects. First, our theory
operates in the finite-width regime whereas these existing theories are
fundamentally infinite-width. Second, our theory allows weights to evolve from
individual initializations beyond the lazy training regime, whereas NTK and MF
are either frozen at or only weakly sensitive to initialization, and TP relies
on special initialization schemes. Third, our theory characterizes both
training and generalization errors for general multi-layer neural networks
beyond the uniform convergence regime, whereas existing theories study
generalization almost exclusively in two-layer settings.
  As a statistical application, we show that vanilla gradient descent can be
augmented to yield consistent estimates of the generalization error at each
iteration, which can be used to guide early stopping and hyperparameter tuning.
As a further theoretical implication, we show that despite model
misspecification, the model learned by gradient descent retains the structure
of a single-index function with an effective signal determined by a linear
combination of the true signal and the initialization.

</details>


### [67] [VaCDA: Variational Contrastive Alignment-based Scalable Human Activity Recognition](https://arxiv.org/abs/2505.04907)
*Soham Khisa,Avijoy Chakma*

Main category: cs.LG

TL;DR: 论文提出了一种结合变分自编码器（VAE）和对比学习的多源域适配框架VaCDA，以解决可穿戴设备数据异构性问题，并在跨位置和跨设备场景中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备产生大量未标记数据，且数据分布异构（如设备位置、类型和用户行为差异），传统迁移学习方法表现不佳，需要更鲁棒的适配方法。

Method: 提出VaCDA框架，通过VAE学习共享低维潜在空间以减少异构性，并利用对比学习对齐同类实例、分离不同类，增强特征表示。

Result: 在跨人、跨位置和跨设备的三种异构场景中测试，VaCDA在跨位置和跨设备场景中表现优于基线方法。

Conclusion: VaCDA通过结合VAE和对比学习有效解决了数据异构性问题，提升了跨域活动识别的性能。

Abstract: Technological advancements have led to the rise of wearable devices with
sensors that continuously monitor user activities, generating vast amounts of
unlabeled data. This data is challenging to interpret, and manual annotation is
labor-intensive and error-prone. Additionally, data distribution is often
heterogeneous due to device placement, type, and user behavior variations. As a
result, traditional transfer learning methods perform suboptimally, making it
difficult to recognize daily activities. To address these challenges, we use a
variational autoencoder (VAE) to learn a shared, low-dimensional latent space
from available sensor data. This space generalizes data across diverse sensors,
mitigating heterogeneity and aiding robust adaptation to the target domain. We
integrate contrastive learning to enhance feature representation by aligning
instances of the same class across domains while separating different classes.
We propose Variational Contrastive Domain Adaptation (VaCDA), a multi-source
domain adaptation framework combining VAEs and contrastive learning to improve
feature representation and reduce heterogeneity between source and target
domains. We evaluate VaCDA on multiple publicly available datasets across three
heterogeneity scenarios: cross-person, cross-position, and cross-device. VaCDA
outperforms the baselines in cross-position and cross-device scenarios.

</details>


### [68] [Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction](https://arxiv.org/abs/2505.04918)
*Jiaqi Zheng,Qing Ling,Yerong Feng*

Main category: cs.LG

TL;DR: PASSAT是一种结合物理辅助和拓扑信息的深度学习模型，用于天气预测，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在天气预测中忽视了物理规律和地球表面拓扑结构，PASSAT旨在解决这一问题。

Method: PASSAT通过求解球面上的Navier-Stokes方程和利用球形图神经网络捕捉地球-大气相互作用。

Result: 在ERA5数据集上，PASSAT的性能优于当前的深度学习模型和数值天气预报模型IFS T42。

Conclusion: 结合物理和拓扑信息的深度学习模型（PASSAT）能显著提升天气预测的准确性。

Abstract: Although deep learning models have demonstrated remarkable potential in
weather prediction, most of them overlook either the \textbf{physics} of the
underlying weather evolution or the \textbf{topology} of the Earth's surface.
In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted
And Topology-informed deep learning model for weather prediction. PASSAT
attributes the weather evolution to two key factors: (i) the advection process
that can be characterized by the advection equation and the Navier-Stokes
equation; (ii) the Earth-atmosphere interaction that is difficult to both model
and calculate. PASSAT also takes the topology of the Earth's surface into
consideration, other than simply treating it as a plane. With these
considerations, PASSAT numerically solves the advection equation and the
Navier-Stokes equation on the spherical manifold, utilizes a spherical graph
neural network to capture the Earth-atmosphere interaction, and generates the
initial velocity fields that are critical to solving the advection equation
from the same spherical graph neural network. In the $5.625^\circ$-resolution
ERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based
weather prediction models and the operational numerical weather prediction
model IFS T42. Code and checkpoint are available at
https://github.com/Yumenomae/PASSAT_5p625.

</details>


### [69] [Fair Uncertainty Quantification for Depression Prediction](https://arxiv.org/abs/2505.04931)
*Yonghong Li,Xiuzhuang Zhou*

Main category: cs.LG

TL;DR: 该论文提出了Fair Uncertainty Quantification (FUQ)方法，旨在通过群体分析和公平性优化策略，实现抑郁症预测的可靠性和算法公平性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在抑郁症预测中侧重于不确定性量化（UQ）的可靠性，但缺乏对UQ公平性的关注，尤其是在不同人口群体中。因此，需要一种既能保证预测可靠性又能兼顾公平性的方法。

Method: 论文首先按敏感属性分组参与者，利用保形预测量化各组内的不确定性。随后提出公平性优化策略，将公平性视为EOC约束下的优化问题，确保模型在保持可靠性的同时适应不同群体的不确定性水平。

Result: 在多个视觉和音频抑郁症数据集上的评估表明，FUQ方法在保证预测可靠性的同时，显著提升了算法公平性。

Conclusion: FUQ方法通过群体分析和公平性优化，成功实现了抑郁症预测的可靠性与公平性，为临床应用中可信赖的预测模型提供了新思路。

Abstract: Trustworthy depression prediction based on deep learning, incorporating both
predictive reliability and algorithmic fairness across diverse demographic
groups, is crucial for clinical application. Recently, achieving reliable
depression predictions through uncertainty quantification has attracted
increasing attention. However, few studies have focused on the fairness of
uncertainty quantification (UQ) in depression prediction. In this work, we
investigate the algorithmic fairness of UQ, namely Equal Opportunity Coverage
(EOC) fairness, and propose Fair Uncertainty Quantification (FUQ) for
depression prediction. FUQ pursues reliable and fair depression predictions
through group-based analysis. Specifically, we first group all the participants
by different sensitive attributes and leverage conformal prediction to quantify
uncertainty within each demographic group, which provides a theoretically
guaranteed and valid way to quantify uncertainty for depression prediction and
facilitates the investigation of fairness across different demographic groups.
Furthermore, we propose a fairness-aware optimization strategy that formulates
fairness as a constrained optimization problem under EOC constraints. This
enables the model to preserve predictive reliability while adapting to the
heterogeneous uncertainty levels across demographic groups, thereby achieving
optimal fairness. Through extensive evaluations on several visual and audio
depression datasets, our approach demonstrates its effectiveness.

</details>


### [70] [Structural Alignment in Link Prediction](https://arxiv.org/abs/2505.04939)
*Jeffrey Seathrún Sardina*

Main category: cs.LG

TL;DR: 该论文提出了一种从图结构优先的视角重新分析知识图谱和链路预测的方法，区别于主流的基于嵌入的范式，认为信息应以整体三元组而非单个节点和边来表示。通过文献综述和实验验证，证明了这种方法的可行性和实用性，并提出了‘结构对齐假设’。


<details>
  <summary>Details</summary>
Motivation: 传统的链路预测方法主要基于嵌入范式，即通过节点和边的向量表示来预测缺失信息。然而，该方法可能忽略了知识图谱的整体结构信息。因此，论文旨在探索一种以图结构为中心的替代视角，以更全面地表征知识图谱的信息内容。

Method: 论文通过文献综述和两组核心实验，从图结构优先的角度重新分析知识图谱和链路预测。实验验证了以整体三元组（而非单个节点和边）建模知识图谱的可行性，并提出了‘结构对齐假设’。

Result: 研究发现，结构优先的视角不仅可行，还能促进跨知识图谱的迁移学习。实验支持了‘结构对齐假设’，即链路预测可以被建模为一种结构性任务。

Conclusion: 论文表明，图结构优先的视角为链路预测和知识图谱学习提供了新的理解和工具，同时推动了跨知识图谱的迁移学习。所有代码和数据均已开源，还包括一份爱尔兰语的机器学习术语词典。

Abstract: While Knowledge Graphs (KGs) have become increasingly popular across various
scientific disciplines for their ability to model and interlink huge quantities
of data, essentially all real-world KGs are known to be incomplete. As such,
with the growth of KG use has been a concurrent development of machine learning
tools designed to predict missing information in KGs, which is referred to as
the Link Prediction Task. The majority of state-of-the-art link predictors to
date have followed an embedding-based paradigm. In this paradigm, it is assumed
that the information content of a KG is best represented by the (individual)
vector representations of its nodes and edges, and that therefore node and edge
embeddings are particularly well-suited to performing link prediction.
  This thesis proposes an alternative perspective on the field's approach to
link prediction and KG data modelling. Specifically, this work re-analyses KGs
and state-of-the-art link predictors from a graph-structure-first perspective
that models the information content of a KG in terms of whole triples, rather
than individual nodes and edges.
  Following a literature review and two core sets of experiments, this thesis
concludes that a structure-first perspective on KGs and link prediction is both
viable and useful for understanding KG learning and for enabling cross-KG
transfer learning for the link prediction task. This observation is used to
create and propose the Structural Alignment Hypothesis, which postulates that
link prediction can be understood and modelled as a structural task.
  All code and data used for this thesis are open-sourced. This thesis was
written bilingually, with the main document in English and an informal extended
summary in Irish. An Irish-language translation dictionary of machine learning
terms (the Focl\'oir Tr\'achtais) created for this work is open-sourced as
well.

</details>


### [71] [Graffe: Graph Representation Learning via Diffusion Probabilistic Models](https://arxiv.org/abs/2505.04956)
*Dingshuo Chen,Shuchen Xue,Liuji Chen,Yingheng Wang,Qiang Liu,Shu Wu,Zhi-Ming Ma,Liang Wang*

Main category: cs.LG

TL;DR: Graffe是一个自监督扩散模型，专注于图表示学习，通过图编码器和扩散解码器结合，理论上证明去噪目标隐含最大化数据与其表示的条件互信息，实验验证在节点和图分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩散概率模型（DPMs）在生成高质量样本方面表现突出，但图表示学习中的应用尚未充分探索。本文旨在探索DPMs在图表示学习中的潜力，提出Graffe模型。

Method: Graffe结合图编码器和扩散解码器，编码器将图压缩为紧凑表示，解码器以该表示为条件进行去噪。理论证明去噪目标隐含条件互信息最大化。

Result: 在11个真实数据集上，Graffe在9个数据集上达到最先进的性能，验证了扩散模型在图表示学习中的有效性。

Conclusion: 扩散模型是图表示学习的有效工具，Graffe为相关研究提供了理论支持和实践验证。

Abstract: Diffusion probabilistic models (DPMs), widely recognized for their potential
to generate high-quality samples, tend to go unnoticed in representation
learning. While recent progress has highlighted their potential for capturing
visual semantics, adapting DPMs to graph representation learning remains in its
infancy. In this paper, we introduce Graffe, a self-supervised diffusion model
proposed for graph representation learning. It features a graph encoder that
distills a source graph into a compact representation, which, in turn, serves
as the condition to guide the denoising process of the diffusion decoder. To
evaluate the effectiveness of our model, we first explore the theoretical
foundations of applying diffusion models to representation learning, proving
that the denoising objective implicitly maximizes the conditional mutual
information between data and its representation. Specifically, we prove that
the negative logarithm of the denoising score matching loss is a tractable
lower bound for the conditional mutual information. Empirically, we conduct a
series of case studies to validate our theoretical insights. In addition,
Graffe delivers competitive results under the linear probing setting on node
and graph classification tasks, achieving state-of-the-art performance on 9 of
the 11 real-world datasets. These findings indicate that powerful generative
models, especially diffusion models, serve as an effective tool for graph
representation learning.

</details>


### [72] [General Transform: A Unified Framework for Adaptive Transform to Enhance Representations](https://arxiv.org/abs/2505.04969)
*Gekko Budiutama,Shunsuke Daimon,Hirofumi Nishi,Yu-ichiro Matsushita*

Main category: cs.LG

TL;DR: 提出了一种称为通用变换（GT）的自适应变换方法，用于机器学习任务，无需依赖数据集属性的先验知识，且在多个任务中表现优于传统变换方法。


<details>
  <summary>Details</summary>
Motivation: 传统离散变换（如傅里叶变换）需依赖对数据集属性的了解来选择适当的变换方法，这在缺乏先验知识时效果较差。

Method: 通用变换（GT）通过学习数据驱动的映射，自适应地提取特征，无需预先了解数据集属性。

Result: 在计算机视觉和自然语言处理任务中，整合GT的模型性能优于传统变换方法。

Conclusion: GT作为一种自适应的变换方法，在多样化学习场景中展现了高效的性能表现。

Abstract: Discrete transforms, such as the discrete Fourier transform, are widely used
in machine learning to improve model performance by extracting meaningful
features. However, with numerous transforms available, selecting an appropriate
one often depends on understanding the dataset's properties, making the
approach less effective when such knowledge is unavailable. In this work, we
propose General Transform (GT), an adaptive transform-based representation
designed for machine learning applications. Unlike conventional transforms, GT
learns data-driven mapping tailored to the dataset and task of interest. Here,
we demonstrate that models incorporating GT outperform conventional
transform-based approaches across computer vision and natural language
processing tasks, highlighting its effectiveness in diverse learning scenarios.

</details>


### [73] [Graph Neural Network Aided Deep Reinforcement Learning for Resource Allocation in Dynamic Terahertz UAV Networks](https://arxiv.org/abs/2505.04981)
*Zhifeng Hu,Chong Han*

Main category: cs.LG

TL;DR: 摘要介绍了使用图神经网络（GNN）和深度强化学习（DRL）相结合的方法（GLOVE），用于解决太赫兹无人机网络中的动态资源分配问题。该方法在资源效率和延迟表现上优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 太赫兹无人机网络在动态拓扑下的资源分配问题因其非凸和NP难特性而具有挑战性，需要高效解决方案以支持高数据速率应用。

Method: 提出GLOVE算法，结合GNN和DRL，通过学习节点间关系并强调自身特征，协同训练功率和子阵列的多任务资源分配。

Result: GLOVE在资源效率和延迟上优于基准方法，且在训练过程中保持零丢包，表现更鲁棒。

Conclusion: GLOVE为动态太赫兹无人机网络的资源分配提供了高效且鲁棒的解决方案，适用于实际应用。

Abstract: Terahertz (THz) unmanned aerial vehicle (UAV) networks with flexible
topologies and ultra-high data rates are expected to empower numerous
applications in security surveillance, disaster response, and environmental
monitoring, among others. However, the dynamic topologies hinder the efficient
long-term joint power and antenna array resource allocation for THz links among
UAVs. Furthermore, the continuous nature of power and the discrete nature of
antennas cause this joint resource allocation problem to be a mixed-integer
nonlinear programming (MINLP) problem with non-convexity and NP-hardness.
Inspired by recent rapid advancements in deep reinforcement learning (DRL), a
graph neural network (GNN) aided DRL algorithm for resource allocation in the
dynamic THz UAV network with an emphasis on self-node features (GLOVE) is
proposed in this paper, with the aim of resource efficiency (RE) maximization.
When training the allocation policy for each UAV, GLOVE learns the relationship
between this UAV and its neighboring UAVs via GNN, while also emphasizing the
important self-node features of this UAV. In addition, a multi-task structure
is leveraged by GLOVE to cooperatively train resource allocation decisions for
the power and sub-arrays of all UAVs. Experimental results illustrate that
GLOVE outperforms benchmark schemes in terms of the highest RE and the lowest
latency. Moreover, unlike the benchmark methods with severe packet loss, GLOVE
maintains zero packet loss during the entire training process, demonstrating
its better robustness under the highly dynamic THz UAV network.

</details>


### [74] [An Agent-Based Modeling Approach to Free-Text Keyboard Dynamics for Continuous Authentication](https://arxiv.org/abs/2505.05015)
*Roberto Dillon,Arushi*

Main category: cs.LG

TL;DR: 通过代理模型模拟不同键盘的用户打字行为，发现随机森林在单键盘用户识别上表现良好，但跨键盘识别效果差，表明键盘硬件对打字行为有显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究连续认证系统中基于打字行为的生物特征的有效性，以提供不影响用户体验的透明安全层。

Method: 使用代理模型生成合成击键数据，评估OC-SVM和随机森林在用户验证中的表现。

Result: 随机森林在单键盘用户识别上准确率高（>0.7），但跨键盘识别效果不佳；OC-SVM表现较差。

Conclusion: 需为不同键盘建立特定用户档案，且随机森林在捕捉用户细粒度模式上优于OC-SVM。

Abstract: Continuous authentication systems leveraging free-text keyboard dynamics
offer a promising additional layer of security in a multifactor authentication
setup that can be used in a transparent way with no impact on user experience.
This study investigates the efficacy of behavioral biometrics by employing an
Agent-Based Model (ABM) to simulate diverse typing profiles across mechanical
and membrane keyboards. Specifically, we generated synthetic keystroke data
from five unique agents, capturing features related to dwell time, flight time,
and error rates within sliding 5-second windows updated every second. Two
machine learning approaches, One-Class Support Vector Machine (OC-SVM) and
Random Forest (RF), were evaluated for user verification. Results revealed a
stark contrast in performance: while One-Class SVM failed to differentiate
individual users within each group, Random Forest achieved robust
intra-keyboard user recognition (Accuracy > 0.7) but struggled to generalize
across keyboards for the same user, highlighting the significant impact of
keyboard hardware on typing behavior. These findings suggest that: (1)
keyboard-specific user profiles may be necessary for reliable authentication,
and (2) ensemble methods like RF outperform One-Class SVM in capturing
fine-grained user-specific patterns.

</details>


### [75] [Generating Reliable Synthetic Clinical Trial Data: The Role of Hyperparameter Optimization and Domain Constraints](https://arxiv.org/abs/2505.05019)
*Waldemar Hahn,Jan-Niklas Eckardt,Christoph Röllig,Martin Sedlmayr,Jan Moritz Middeke,Markus Wolfien*

Main category: cs.LG

TL;DR: 本文探讨了通过超参数优化（HPO）提升临床合成数据生成质量的方法，发现复合指标优化比单一指标更有效，但HPO仍需结合领域知识以确保数据有效性。


<details>
  <summary>Details</summary>
Motivation: 解决医学研究中合成数据的高保真和实用性问题，同时保护隐私和提升数据可及性。

Method: 系统评估了四种HPO策略在八种生成模型中的表现，比较单一与复合指标优化的效果。

Result: HPO显著提升数据质量，其中TVAE、CTGAN和CTAB-GAN+表现最佳，但所有模型仍需预处理和后处理以减少临床约束违规。

Conclusion: HPO结合领域知识能生成更高质量的合成数据，未来需优化指标选择和验证更大数据集。

Abstract: The generation of synthetic clinical trial data offers a promising approach
to mitigating privacy concerns and data accessibility limitations in medical
research. However, ensuring that synthetic datasets maintain high fidelity,
utility, and adherence to domain-specific constraints remains a key challenge.
While hyperparameter optimization (HPO) has been shown to improve generative
model performance, the effectiveness of different optimization strategies for
synthetic clinical data remains unclear. This study systematically evaluates
four HPO strategies across eight generative models, comparing single-metric
optimization against compound metric optimization approaches. Our results
demonstrate that HPO consistently improves synthetic data quality, with TVAE,
CTGAN, and CTAB-GAN+ achieving improvements of up to 60%, 39%, and 38%,
respectively. Compound metric optimization outperformed single-metric
strategies, producing more balanced and generalizable synthetic datasets.
Interestingly, HPO alone is insufficient to ensure clinically valid synthetic
data, as all models exhibited violations of fundamental survival constraints.
Preprocessing and postprocessing played a crucial role in reducing these
violations, as models lacking robust processing steps produced invalid data in
up to 61% of cases. These findings underscore the necessity of integrating
explicit domain knowledge alongside HPO to create high quality synthetic
datasets. Our study provides actionable recommendations for improving synthetic
data generation, with future research needed to refine metric selection and
validate these findings on larger datasets to enhance clinical applicability.

</details>


### [76] [Generative Models for Long Time Series: Approximately Equivariant Recurrent Network Structures for an Adjusted Training Scheme](https://arxiv.org/abs/2505.05020)
*Ruwen Fulek,Markus Lange-Hegermann*

Main category: cs.LG

TL;DR: 该论文介绍了一种基于变分自编码器（VAE）和循环层的简单生成模型（RVAE-ST），通过渐进式训练解决长序列建模问题，在多个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决循环层在建模长序列时面临的挑战，同时希望通过现有组件的优化组合达到或超越现有生成模型的性能。

Method: 采用变分自编码器（VAE）结合循环层，并引入渐进式序列长度增加的训练方案，保持参数恒定性以提升长时依赖建模能力。

Result: 在多个基准数据集上表现优异，尤其擅长拟周期性时间序列，同时在非平稳数据上也保持竞争力。评估指标包括ELBO、Fr\'echet距离等。

Conclusion: 通过优化已知组件的组合，无需全新架构即可实现高性能的长序列生成建模，尤其在拟周期性数据中表现突出。

Abstract: We present a simple yet effective generative model for time series data based
on a Variational Autoencoder (VAE) with recurrent layers, referred to as the
Recurrent Variational Autoencoder with Subsequent Training (RVAE-ST). Our
method introduces an adapted training scheme that progressively increases the
sequence length, addressing the challenge recurrent layers typically face when
modeling long sequences. By leveraging the recurrent architecture, the model
maintains a constant number of parameters regardless of sequence length. This
design encourages approximate time-shift equivariance and enables efficient
modeling of long-range temporal dependencies. Rather than introducing a
fundamentally new architecture, we show that a carefully composed combination
of known components can match or outperform state-of-the-art generative models
on several benchmark datasets. Our model performs particularly well on time
series that exhibit quasi-periodic structure,while remaining competitive on
datasets with more irregular or partially non-stationary behavior. We evaluate
its performance using ELBO, Fr\'echet Distance, discriminative scores, and
visualizations of the learned embeddings.

</details>


### [77] [Dequantified Diffusion Schrödinger Bridge for Density Ratio Estimation](https://arxiv.org/abs/2505.05034)
*Wei Chen,Shigui Li,Jiacheng Li,Junmei Yang,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 论文提出了一种名为$	ext{D}^3	ext{RE}$的统一框架，用于鲁棒且高效的密度比估计，解决了现有方法在分布差异大或支持集重叠不足时的失效问题。


<details>
  <summary>Details</summary>
Motivation: 现有密度比估计方法在处理分布差异大或支持集重叠不足时容易失效，存在'密度鸿沟'和'支持集鸿沟'问题，且在边界附近的时间分数不稳定。

Method: 提出Dequantified Diffusion-Bridge Interpolant (DDBI)扩展支持集覆盖并稳定时间分数，并在此基础上引入Dequantified Schr\"odinger-Bridge Interpolant (DSBI)结合最优传输解决Schr\"odinger桥问题。

Result: 理论上实现了均匀逼近和有界时间分数，并在互信息和密度估计任务中优于基线方法。

Conclusion: $	ext{D}^3	ext{RE}$框架通过扩散桥和高斯去量化解决了现有方法的局限性，提升了密度比估计的鲁棒性和效率。

Abstract: Density ratio estimation is fundamental to tasks involving $f$-divergences,
yet existing methods often fail under significantly different distributions or
inadequately overlap supports, suffering from the \textit{density-chasm} and
the \textit{support-chasm} problems. Additionally, prior approaches yield
divergent time scores near boundaries, leading to instability. We propose
$\text{D}^3\text{RE}$, a unified framework for robust and efficient density
ratio estimation. It introduces the Dequantified Diffusion-Bridge Interpolant
(DDBI), which expands support coverage and stabilizes time scores via diffusion
bridges and Gaussian dequantization. Building on DDBI, the Dequantified
Schr\"odinger-Bridge Interpolant (DSBI) incorporates optimal transport to solve
the Schr\"odinger bridge problem, enhancing accuracy and efficiency. Our method
offers uniform approximation and bounded time scores in theory, and outperforms
baselines empirically in mutual information and density estimation tasks.

</details>


### [78] [Neural Pathways to Program Success: Hopfield Networks for PERT Analysis](https://arxiv.org/abs/2505.05047)
*Azgar Ali Noor Ahamed*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Hopfield神经网络的新型PERT调度方法，将任务调度问题转化为能量最小化问题，通过神经网络优化动态生成全局一致的调度方案。


<details>
  <summary>Details</summary>
Motivation: 在项目管理中，任务持续时间和依赖关系的不确定性是主要挑战。传统的PERT技术虽能建模任务变异性，但需要更高效、灵活的方法以应对现代AI系统（如代理AI工作流和微服务应用）的需求。

Method: 作者将任务开始时间和优先约束映射到Hopfield神经网络框架中，利用其优化动态实现全局调度。重点解决了能量函数可微性、约束编码和收敛性等理论问题，并扩展了结构优先图的Hopfield模型。

Result: 数值模拟显示，该方法在包含多达1000个任务的合成项目网络中表现良好，实现了接近最优的工期且约束违规极少。

Conclusion: 研究表明，神经网络优化模型为不确定条件下的可扩展和自适应任务调度提供了有前景的方向，尤其适用于现代AI系统。

Abstract: Project and task scheduling under uncertainty remains a fundamental challenge
in program and project management, where accurate estimation of task durations
and dependencies is critical for delivering complex, multi project systems. The
Program Evaluation and Review Technique provides a probabilistic framework to
model task variability and critical paths. In this paper, the author presents a
novel formulation of PERT scheduling as an energy minimization problem within a
Hopfield neural network architecture. By mapping task start times and
precedence constraints into a neural computation framework, the networks
inherent optimization dynamics is exploited to approximate globally consistent
schedules. The author addresses key theoretical issues related to energy
function differentiability, constraint encoding, and convergence, and extends
the Hopfield model for structured precedence graphs. Numerical simulations on
synthetic project networks comprising up to 1000 tasks demonstrate the
viability of this approach, achieving near optimal makespans with minimal
constraint violations. The findings suggest that neural optimization models
offer a promising direction for scalable and adaptive project tasks scheduling
under uncertainty in areas such as the agentic AI workflows, microservice based
applications that the modern AI systems are being built upon.

</details>


### [79] [CodeMixBench: Evaluating Large Language Models on Code Generation with Code-Mixed Prompts](https://arxiv.org/abs/2505.05063)
*Manik Sheokand,Parth Sawant*

Main category: cs.LG

TL;DR: CodeMixBench新基准评估LLM在代码混合提示下的代码生成能力，发现多语言提示会降低模型性能，尤其是小模型。


<details>
  <summary>Details</summary>
Motivation: 现实世界中开发者常使用混合语言与LLM交互，现有基准如HumanEval仅关注英文提示，忽略了多语言场景的复杂性。

Method: 基于BigCodeBench，引入三种语言对（如Hinglish）的代码混合提示，评估1.5B至15B参数的开源模型。

Result: 代码混合提示导致Pass@1性能下降，小模型在高混合水平下表现更差。

Conclusion: CodeMixBench为多语言代码生成提供真实评估框架，揭示了模型在多样化语言环境中的泛化挑战。

Abstract: Large Language Models (LLMs) have achieved remarkable success in code
generation tasks, powering various applications like code completion,
debugging, and programming assistance. However, existing benchmarks such as
HumanEval, MBPP, and BigCodeBench primarily evaluate LLMs on English-only
prompts, overlooking the real-world scenario where multilingual developers
often use code-mixed language while interacting with LLMs. To address this gap,
we introduce CodeMixBench, a novel benchmark designed to evaluate the
robustness of LLMs on code generation from code-mixed prompts. Built upon
BigCodeBench, CodeMixBench introduces controlled code-mixing (CMD) into the
natural language parts of prompts across three language pairs: Hinglish
(Hindi-English), Spanish-English, and Chinese Pinyin-English. We
comprehensively evaluate a diverse set of open-source code generation models
ranging from 1.5B to 15B parameters. Our results show that code-mixed prompts
consistently degrade Pass@1 performance compared to their English-only
counterparts, with performance drops increasing under higher CMD levels for
smaller models. CodeMixBench provides a realistic evaluation framework for
studying multilingual code generation and highlights new challenges and
directions for building robust code generation models that generalize well
across diverse linguistic settings.

</details>


### [80] [WaterDrum: Watermarking for Data-centric Unlearning Metric](https://arxiv.org/abs/2505.05064)
*Xinyang Lu,Xinyuan Niu,Gregory Kang Ruey Lau,Bui Thi Cam Nhung,Rachael Hwee Ling Sim,Fanyu Wen,Chuan-Sheng Foo,See-Kiong Ng,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 介绍了一种名为WaterDrum的数据中心化遗忘度量方法，用于更准确评估大型语言模型（LLM）中数据遗忘的效果，尤其是在遗忘集与保留集语义相似等现实场景中。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型效用的遗忘度量方法在现实场景中可能失效，例如当遗忘集与保留集内容相似、重新训练不实际或模型所有者可能绕过遗忘过程时。

Method: 提出WaterDrum，利用鲁棒文本水印技术克服局限性，并引入包含不同相似度数据的新基准数据集。

Result: WaterDrum在评估LLM遗忘算法时表现更准确，新数据集支持严格测试。

Conclusion: WaterDrum为LLM遗忘问题提供了更可靠的数据中心化解决方案，并开源代码与数据集。

Abstract: Large language model (LLM) unlearning is critical in real-world applications
where it is necessary to efficiently remove the influence of private,
copyrighted, or harmful data from some users. However, existing utility-centric
unlearning metrics (based on model utility) may fail to accurately evaluate the
extent of unlearning in realistic settings such as when (a) the forget and
retain set have semantically similar content, (b) retraining the model from
scratch on the retain set is impractical, and/or (c) the model owner can
improve the unlearning metric without directly performing unlearning on the
LLM. This paper presents the first data-centric unlearning metric for LLMs
called WaterDrum that exploits robust text watermarking for overcoming these
limitations. We also introduce new benchmark datasets for LLM unlearning that
contain varying levels of similar data points and can be used to rigorously
evaluate unlearning algorithms using WaterDrum. Our code is available at
https://github.com/lululu008/WaterDrum and our new benchmark datasets are
released at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax.

</details>


### [81] [ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model](https://arxiv.org/abs/2505.05082)
*Sagnik Bhattacharya,Abhiram R. Gorle,Ahmed Mohsin,Ahsan Bilal,Connor Ding,Amit Kumar Singh Yadav,Tsachy Weissman*

Main category: cs.LG

TL;DR: 提出了一个名为ItDPDM的新模型，直接在离散状态空间进行建模，解决了传统方法在连续域工作或依赖近似变分损失的局限性。通过Poisson扩散过程和新型Poisson重建损失（PRL），显著提升了生成离散数据（如符号音乐）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成离散数据的方法存在两个主要问题：一是将离散输入嵌入连续状态空间，二是依赖近似变分损失。ItDPDM旨在直接在离散空间建模，避免这些问题。

Method: 提出了Information-Theoretic Discrete Poisson Diffusion Model (ItDPDM)，基于Poisson扩散过程设计，并引入Poisson Reconstruction Loss (PRL)直接优化负对数似然。

Result: 在Lakh MIDI数据集和CIFAR-10图像基准测试中，ItDPDM显著降低了80%的测试负对数似然，同时收敛更快。

Conclusion: ItDPDM通过直接在离散空间建模和精确优化负对数似然，显著提升了生成离散数据的效果和效率。

Abstract: Existing methods for generative modeling of discrete data, such as symbolic
music tokens, face two primary challenges: (1) they either embed discrete
inputs into continuous state-spaces or (2) rely on variational losses that only
approximate the true negative log-likelihood. Previous efforts have
individually targeted these limitations. While information-theoretic Gaussian
diffusion models alleviate the suboptimality of variational losses, they still
perform modeling in continuous domains. In this work, we introduce the
Information-Theoretic Discrete Poisson Diffusion Model (ItDPDM), which
simultaneously addresses both limitations by directly operating in a discrete
state-space via a Poisson diffusion process inspired by photon arrival
processes in camera sensors. We introduce a novel Poisson Reconstruction Loss
(PRL) and derive an exact relationship between PRL and the true negative
log-likelihood, thereby eliminating the need for approximate evidence lower
bounds. Experiments conducted on the Lakh MIDI symbolic music dataset and the
CIFAR-10 image benchmark demonstrate that ItDPDM delivers significant
improvements, reducing test NLL by up to 80% compared to prior baselines, while
also achieving faster convergence.

</details>


### [82] [Beyond Low-rank Decomposition: A Shortcut Approach for Efficient On-Device Learning](https://arxiv.org/abs/2505.05086)
*Le-Trung Nguyen,Ael Quelennec,Van-Tam Nguyen,Enzo Tartaglione*

Main category: cs.LG

TL;DR: 该论文提出了一种新的快捷方法，用于降低设备端学习中的激活内存使用和计算开销，相比传统训练方法显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 设备端学习因其能减少延迟、增强隐私保护和提高能源效率而备受关注，但内存和计算限制仍是主要挑战，论文旨在解决这些问题。

Method: 通过低秩分解方法，提出了一种新颖的快捷方法，用于优化激活内存和计算开销。

Result: 实验显示，该方法能将激活内存使用降低至传统方法的120.09分之一，训练FLOPs减少1.86倍。

Conclusion: 该方法有效解决了设备端学习的内存和计算瓶颈，为实际部署提供了可行方案。

Abstract: On-device learning has emerged as a promising direction for AI development,
particularly because of its potential to reduce latency issues and mitigate
privacy risks associated with device-server communication, while improving
energy efficiency. Despite these advantages, significant memory and
computational constraints still represent major challenges for its deployment.
Drawing on previous studies on low-rank decomposition methods that address
activation memory bottlenecks in backpropagation, we propose a novel shortcut
approach as an alternative. Our analysis and experiments demonstrate that our
method can reduce activation memory usage, even up to $120.09\times$ compared
to vanilla training, while also reducing overall training FLOPs up to
$1.86\times$ when evaluated on traditional benchmarks.

</details>


### [83] [A Conjoint Graph Representation Learning Framework for Hypertension Comorbidity Risk Prediction](https://arxiv.org/abs/2505.05094)
*Leming Zhou,Zuo Wang,Zhixuan Duan*

Main category: cs.LG

TL;DR: 研究提出了一种结合联合图学习和网络分析的方法（CGRL框架），用于高血压并发症的早期识别，通过构建患者网络和疾病差异网络来预测糖尿病和冠心病风险，结果显示其预测准确性优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 高血压并发症对患者和社会造成沉重负担，早期识别是干预的关键，但现有方法仍面临挑战。

Method: 开发了CGRL框架，包括构建患者网络和疾病差异网络、生成网络特征、结合计算结构干预和学习特征表示来预测风险。

Result: 基于差异网络提取的特征至关重要，CGRL框架的预测准确性优于其他模型。

Conclusion: CGRL框架不仅能更准确地预测疾病风险，还能揭示糖尿病和冠心病的病理发病机制。

Abstract: The comorbidities of hypertension impose a heavy burden on patients and
society. Early identification is necessary to prompt intervention, but it
remains a challenging task. This study aims to address this challenge by
combining joint graph learning with network analysis. Motivated by this
discovery, we develop a Conjoint Graph Representation Learning (CGRL) framework
that: a) constructs two networks based on disease coding, including the patient
network and the disease difference network. Three comorbidity network features
were generated based on the basic difference network to capture the potential
relationship between comorbidities and risk diseases; b) incorporates
computational structure intervention and learning feature representation, CGRL
was developed to predict the risks of diabetes and coronary heart disease in
patients; and c) analysis the comorbidity patterns and exploring the pathways
of disease progression, the pathological pathogenesis of diabetes and coronary
heart disease may be revealed. The results show that the network features
extracted based on the difference network are important, and the framework we
proposed provides more accurate predictions than other strong models in terms
of accuracy.

</details>


### [84] [Balancing Client Participation in Federated Learning Using AoI](https://arxiv.org/abs/2505.05099)
*Alireza Javani,Zhiying Wang*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息时效性（AoI）的客户端选择策略，用于联邦学习（FL）中平衡客户端参与并优化模型收敛。通过去中心化的马尔可夫调度策略，该方法在IID和非IID数据场景下比FedAvg方法收敛性提升7.5%至20%。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式客户端协作训练模型时面临通信资源有限、统计异构性和客户端参与不平衡等挑战。本研究旨在通过AoI优化客户端选择，解决这些问题。

Method: 采用去中心化的马尔可夫调度策略，客户端根据年龄依赖的选择概率独立管理参与，以减少负载不平衡并最小化中心调控需求。

Result: 实验证明，所提AoI方法在IID和非IID数据场景下分别比FedAvg方法收敛性提升7.5%和最高20%。

Conclusion: 基于AoI的调度策略能够为联邦学习系统提供可扩展、公平且高效的学习环境。

Abstract: Federated Learning (FL) offers a decentralized framework that preserves data
privacy while enabling collaborative model training across distributed clients.
However, FL faces significant challenges due to limited communication
resources, statistical heterogeneity, and the need for balanced client
participation. This paper proposes an Age of Information (AoI)-based client
selection policy that addresses these challenges by minimizing load imbalance
through controlled selection intervals. Our method employs a decentralized
Markov scheduling policy, allowing clients to independently manage
participation based on age-dependent selection probabilities, which balances
client updates across training rounds with minimal central oversight. We
provide a convergence proof for our method, demonstrating that it ensures
stable and efficient model convergence. Specifically, we derive optimal
parameters for the Markov selection model to achieve balanced and consistent
client participation, highlighting the benefits of AoI in enhancing convergence
stability. Through extensive simulations, we demonstrate that our AoI-based
method, particularly the optimal Markov variant, improves convergence over the
FedAvg selection approach across both IID and non-IID data settings by $7.5\%$
and up to $20\%$. Our findings underscore the effectiveness of AoI-based
scheduling for scalable, fair, and efficient FL systems across diverse learning
environments.

</details>


### [85] [USPR: Learning a Unified Solver for Profiled Routing](https://arxiv.org/abs/2505.05119)
*Chuanbo Hua,Federico Berto,Zhikai Zhao,Jiwoo Son,Changhyun Kwon,Jinkyoo Park*

Main category: cs.LG

TL;DR: USPR是一种用于解决带有车辆-客户特定偏好和约束的路径规划问题（PVRP）的统一框架，通过嵌入和注意力机制提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在PVRP中表现不佳，无法灵活处理不同偏好分布，且泛化能力不足。

Method: USPR通过Profile Embeddings、Multi-Head Profiled Attention和Profile-aware Score Reshaping三项创新提升性能。

Result: 在多种PVRP基准测试中，USPR取得了当前最优的学习方法结果，并具有更高的灵活性和计算效率。

Conclusion: USPR为PVRP提供了一种高效且灵活的解决方案，推动了该领域的研究发展。

Abstract: The Profiled Vehicle Routing Problem (PVRP) extends the classical VRP by
incorporating vehicle-client-specific preferences and constraints, reflecting
real-world requirements such as zone restrictions and service-level
preferences. While recent reinforcement learning (RL) solvers have shown
promise, they require retraining for each new profile distribution, suffer from
poor representation ability, and struggle to generalize to out-of-distribution
instances. In this paper, we address these limitations by introducing USPR
(Unified Solver for Profiled Routing), a novel framework that natively handles
arbitrary profile types. USPR introduces three key innovations: (i) Profile
Embeddings (PE) to encode any combination of profile types; (ii) Multi-Head
Profiled Attention (MHPA), an attention mechanism that models rich interactions
between vehicles and clients; (iii) Profile-aware Score Reshaping (PSR), which
dynamically adjusts decoder logits using profile scores to improve
generalization. Empirical results on diverse PVRP benchmarks demonstrate that
USPR achieves state-of-the-art results among learning-based methods while
offering significant gains in flexibility and computational efficiency. We make
our source code publicly available to foster future research at
https://github.com/ai4co/uspr.

</details>


### [86] [Taming OOD Actions for Offline Reinforcement Learning: An Advantage-Based Approach](https://arxiv.org/abs/2505.05126)
*Xuyang Chen,Keyu Yan,Lin Zhao*

Main category: cs.LG

TL;DR: ADAC是一种新颖的离线强化学习方法，通过优势函数调节Q函数更新，有效评估并利用有益的OOD动作，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临分布偏移问题，现有方法对所有OOD动作保守处理，限制了泛化能力。ADAC旨在通过系统评估OOD动作质量来解决这一问题。

Method: ADAC利用批量最优值函数评估OOD动作，定义优势函数调节Q函数更新，并在PointMaze环境中验证其有效性。

Result: 实验表明，ADAC在D4RL基准测试中几乎所有任务上达到最先进性能，尤其在挑战性任务上优势明显。

Conclusion: ADAC通过优势调制有效识别并利用高质量OOD动作，显著提升离线强化学习性能，为实际应用提供了更优解决方案。

Abstract: Offline reinforcement learning (RL) aims to learn decision-making policies
from fixed datasets without online interactions, providing a practical solution
where online data collection is expensive or risky. However, offline RL often
suffers from distribution shift, resulting in inaccurate evaluation and
substantial overestimation on out-of-distribution (OOD) actions. To address
this, existing approaches incorporate conservatism by indiscriminately
discouraging all OOD actions, thereby hindering the agent's ability to
generalize and exploit beneficial ones. In this paper, we propose
Advantage-based Diffusion Actor-Critic (ADAC), a novel method that
systematically evaluates OOD actions using the batch-optimal value function.
Based on this evaluation, ADAC defines an advantage function to modulate the
Q-function update, enabling more precise assessment of OOD action quality. We
design a custom PointMaze environment and collect datasets to visually reveal
that advantage modulation can effectively identify and select superior OOD
actions. Extensive experiments show that ADAC achieves state-of-the-art
performance on almost all tasks in the D4RL benchmark, with particularly clear
margins on the more challenging tasks.

</details>


### [87] [Research on Anomaly Detection Methods Based on Diffusion Models](https://arxiv.org/abs/2505.05137)
*Yi Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散概率模型（DPMs）的新框架，用于图像和音频数据的异常检测。通过多尺度特征提取、注意力机制和小波域表示增强模型性能，实验表明其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法在处理复杂高维数据分布时面临挑战，本文旨在探索扩散模型在此领域的潜力，提供更高效的解决方案。

Method: 提出基于DPMs的框架，通过扩散过程建模正常数据分布，结合重建误差和语义差异作为异常指标，并引入多尺度特征提取和注意力机制。

Result: 在MVTec AD和UrbanSound8K等基准数据集上，该方法优于现有技术，表现出更高的准确性和鲁棒性。

Conclusion: 扩散模型在异常检测中具有显著优势，为实际应用提供了高效可靠的解决方案。

Abstract: Anomaly detection is a fundamental task in machine learning and data mining,
with significant applications in cybersecurity, industrial fault diagnosis, and
clinical disease monitoring. Traditional methods, such as statistical modeling
and machine learning-based approaches, often face challenges in handling
complex, high-dimensional data distributions. In this study, we explore the
potential of diffusion models for anomaly detection, proposing a novel
framework that leverages the strengths of diffusion probabilistic models (DPMs)
to effectively identify anomalies in both image and audio data. The proposed
method models the distribution of normal data through a diffusion process and
reconstructs input data via reverse diffusion, using a combination of
reconstruction errors and semantic discrepancies as anomaly indicators. To
enhance the framework's performance, we introduce multi-scale feature
extraction, attention mechanisms, and wavelet-domain representations, enabling
the model to capture fine-grained structures and global dependencies in the
data. Extensive experiments on benchmark datasets, including MVTec AD and
UrbanSound8K, demonstrate that our method outperforms state-of-the-art anomaly
detection techniques, achieving superior accuracy and robustness across diverse
data modalities. This research highlights the effectiveness of diffusion models
in anomaly detection and provides a robust and efficient solution for
real-world applications.

</details>


### [88] [Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry](https://arxiv.org/abs/2505.05143)
*Mohammed Adnan,Rohan Jain,Ekansh Sharma,Rahul Krishnan,Yani Ioannou*

Main category: cs.LG

TL;DR: 提出通过置换LTH掩码来对齐不同初始化的优化盆地，显著提高了稀疏训练时的泛化性能，验证了多个数据集和模型。


<details>
  <summary>Details</summary>
Motivation: 探索LTH掩码在新随机初始化下泛化性差的原因，并假设盆地不对齐是主要原因，提出置换掩码的解决方案。

Method: 通过置换LTH掩码以对齐新初始化的优化盆地，在稀疏训练时使用置换后的掩码。

Result: 实验显示，置换后的LTH掩码显著提高了稀疏训练的泛化性能（CIFAR-10、CIFAR-100、ImageNet数据集，VGG11、ResNet20、ResNet50模型）。

Conclusion: 置换LTH掩码解决了泛化性问题，为稀疏训练提供了更高效的解决方案。

Abstract: The Lottery Ticket Hypothesis (LTH) suggests there exists a sparse LTH mask
and weights that achieve the same generalization performance as the dense model
while using significantly fewer parameters. However, finding a LTH solution is
computationally expensive, and a LTH sparsity mask does not generalize to other
random weight initializations. Recent work has suggested that neural networks
trained from random initialization find solutions within the same basin modulo
permutation, and proposes a method to align trained models within the same loss
basin. We hypothesize that misalignment of basins is the reason why LTH masks
do not generalize to new random initializations and propose permuting the LTH
mask to align with the new optimization basin when performing sparse training
from a different random init. We empirically show a significant increase in
generalization when sparse training from random initialization with the
permuted mask as compared to using the non-permuted LTH mask, on multiple
datasets (CIFAR-10, CIFAR-100 and ImageNet) and models (VGG11, ResNet20 and
ResNet50).

</details>


### [89] [Understanding In-context Learning of Addition via Activation Subspaces](https://arxiv.org/abs/2505.05145)
*Xinyan Hu,Kayo Yin,Michael I. Jordan,Jacob Steinhardt,Lijie Chen*

Main category: cs.LG

TL;DR: 该论文研究了语言模型如何在少样本学习中从示例中提取信号并形成预测规则，发现在特定加法任务中，Llama-3-8B仅需三个注意力头即可高效完成任务，并揭示了其底层机制。


<details>
  <summary>Details</summary>
Motivation: 为了理解语言模型如何在少样本学习中实现上下文化学习，作者希望通过研究现代Transformer模型的前向传递机制，揭示其是如何提取少数示例中的信号、聚合这些信号并形成预测规则的。

Method: 作者通过设计一种结构化的少样本学习任务（输入加法任务），使用Llama-3-8B模型进行实验，并通过一种新颖的优化方法定位了完成此任务的关键注意力头。此外，他们还分析了解码信号的子空间结构。

Result: 研究发现，仅需三个特定的注意力头即可高效完成任务，提取的信号集中在六维子空间中，其中四个维度跟踪单位数位，两个维度跟踪整体数值。此外，还发现了一种自我纠正机制，即后续示例可以抑制先前示例的错误信号。

Conclusion: 论文表明，通过跟踪前向传递中的低维子空间，可以揭示语言模型在处理少样本学习任务时的精细计算结构，为理解模型内部机制提供了新的见解。

Abstract: To perform in-context learning, language models must extract signals from
individual few-shot examples, aggregate these into a learned prediction rule,
and then apply this rule to new examples. How is this implemented in the
forward pass of modern transformer models? To study this, we consider a
structured family of few-shot learning tasks for which the true prediction rule
is to add an integer $k$ to the input. We find that Llama-3-8B attains high
accuracy on this task for a range of $k$, and localize its few-shot ability to
just three attention heads via a novel optimization approach. We further show
the extracted signals lie in a six-dimensional subspace, where four of the
dimensions track the unit digit and the other two dimensions track overall
magnitude. We finally examine how these heads extract information from
individual few-shot examples, identifying a self-correction mechanism in which
mistakes from earlier examples are suppressed by later examples. Our results
demonstrate how tracking low-dimensional subspaces across a forward pass can
provide insight into fine-grained computational structures.

</details>


### [90] [FedTDP: A Privacy-Preserving and Unified Framework for Trajectory Data Preparation via Federated Learning](https://arxiv.org/abs/2505.05155)
*Zhihao Zeng,Ziquan Fang,Wei Shao,Lu Chen,Yunjun Gao*

Main category: cs.LG

TL;DR: 本文提出了一个名为FedTDP的隐私保护统一框架，利用大语言模型在联邦环境中优化轨迹数据准备（TDP），解决了现有方法的隐私和通用性问题。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹数据准备方法在联邦环境下无法保护隐私且缺乏通用性，影响数据质量和应用潜力。

Method: 设计轨迹隐私自编码器保护数据隐私，引入轨迹知识增强器改进模型学习，并提出联邦并行优化提升效率。

Result: 在6个真实数据集和10个主流TDP任务上，FedTDP表现优于13个先进基线。

Conclusion: FedTDP有效提升了轨迹数据处理的隐私性、通用性和效率，具有广泛应用潜力。

Abstract: Trajectory data, which capture the movement patterns of people and vehicles
over time and space, are crucial for applications like traffic optimization and
urban planning. However, issues such as noise and incompleteness often
compromise data quality, leading to inaccurate trajectory analyses and limiting
the potential of these applications. While Trajectory Data Preparation (TDP)
can enhance data quality, existing methods suffer from two key limitations: (i)
they do not address data privacy concerns, particularly in federated settings
where trajectory data sharing is prohibited, and (ii) they typically design
task-specific models that lack generalizability across diverse TDP scenarios.
To overcome these challenges, we propose FedTDP, a privacy-preserving and
unified framework that leverages the capabilities of Large Language Models
(LLMs) for TDP in federated environments. Specifically, we: (i) design a
trajectory privacy autoencoder to secure data transmission and protect privacy,
(ii) introduce a trajectory knowledge enhancer to improve model learning of
TDP-related knowledge, enabling the development of TDP-oriented LLMs, and (iii)
propose federated parallel optimization to enhance training efficiency by
reducing data transmission and enabling parallel model training. Experiments on
6 real datasets and 10 mainstream TDP tasks demonstrate that FedTDP
consistently outperforms 13 state-of-the-art baselines.

</details>


### [91] [Bandit Max-Min Fair Allocation](https://arxiv.org/abs/2505.05169)
*Tsubasa Harada,Shinji Ito,Hanna Sumita*

Main category: cs.LG

TL;DR: 该论文研究了多臂老虎机中的最大最小公平分配问题，提出了一种算法，其渐进遗憾上界为 $O(m\sqrt{T}\ln T/n + m\sqrt{T \ln(mnT)})$，并给出了下界 $\Omega(m\sqrt{T}/n)$。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设每回合开始时会提供物品价值，而本文研究的是通过半反馈机制观察代理对物品的估值，且奖励函数非加性。

Method: 结合老虎机技术和竞态分析中的资源分配算法，提出了一种新算法。

Result: 证明了算法在时间 $T$ 足够大时，遗憾界上下限差距为 $T$ 的对数因子。

Conclusion: 该算法在理论与实际应用中均显示了有效性，填补了相关研究的空白。

Abstract: In this paper, we study a new decision-making problem called the bandit
max-min fair allocation (BMMFA) problem. The goal of this problem is to
maximize the minimum utility among agents with additive valuations by
repeatedly assigning indivisible goods to them. One key feature of this problem
is that each agent's valuation for each item can only be observed through the
semi-bandit feedback, while existing work supposes that the item values are
provided at the beginning of each round. Another key feature is that the
algorithm's reward function is not additive with respect to rounds, unlike most
bandit-setting problems.
  Our first contribution is to propose an algorithm that has an asymptotic
regret bound of $O(m\sqrt{T}\ln T/n + m\sqrt{T \ln(mnT)})$, where $n$ is the
number of agents, $m$ is the number of items, and $T$ is the time horizon. This
is based on a novel combination of bandit techniques and a resource allocation
algorithm studied in the literature on competitive analysis. Our second
contribution is to provide the regret lower bound of $\Omega(m\sqrt{T}/n)$.
When $T$ is sufficiently larger than $n$, the gap between the upper and lower
bounds is a logarithmic factor of $T$.

</details>


### [92] [OpenworldAUC: Towards Unified Evaluation and Optimization for Open-world Prompt Tuning](https://arxiv.org/abs/2505.05180)
*Cong Hua,Qianqian Xu,Zhiyong Yang,Zitai Wang,Shilong Bao,Qingming Huang*

Main category: cs.LG

TL;DR: 本文提出了一种新的开放世界提示调整框架GMoP和统一的评估指标OpenworldAUC，解决了现有方法在检测和分类任务中无法同时满足三种属性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理开放世界任务时无法同时满足检测、分类和不同样本比例下的鲁棒性需求，因此需要开发新的评估指标和方法。

Method: 提出Gated Mixture-of-Prompts (GMoP)，使用领域特定的提示和门控机制动态平衡检测和分类；提出OpenworldAUC作为统一的评估指标。

Result: 在15个基准测试中，GMoP和OpenworldAUC均表现出色，达到了最先进的性能。

Conclusion: GMoP和OpenworldAUC为开放世界任务提供了一个高效且鲁棒的解决方案，解决了现有方法的局限性。

Abstract: Prompt tuning adapts Vision-Language Models like CLIP to open-world tasks
with minimal training costs. In this direction, one typical paradigm evaluates
model performance separately on known classes (i.e., base domain) and unseen
classes (i.e., new domain). However, real-world scenarios require models to
handle inputs without prior domain knowledge. This practical challenge has
spurred the development of open-world prompt tuning, which demands a unified
evaluation of two stages: 1) detecting whether an input belongs to the base or
new domain (P1), and 2) classifying the sample into its correct class (P2).
What's more, as domain distributions are generally unknown, a proper metric
should be insensitive to varying base/new sample ratios (P3). However, we find
that current metrics, including HM, overall accuracy, and AUROC, fail to
satisfy these three properties simultaneously. To bridge this gap, we propose
OpenworldAUC, a unified metric that jointly assesses detection and
classification through pairwise instance comparisons. To optimize OpenworldAUC
effectively, we introduce Gated Mixture-of-Prompts (GMoP), which employs
domain-specific prompts and a gating mechanism to dynamically balance detection
and classification. Theoretical guarantees ensure generalization of GMoP under
practical conditions. Experiments on 15 benchmarks in open-world scenarios show
GMoP achieves SOTA performance on OpenworldAUC and other metrics. We release
the code at https://github.com/huacong/OpenworldAUC

</details>


### [93] [Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation](https://arxiv.org/abs/2505.05181)
*Bojian Yin,Federico Corradi*

Main category: cs.LG

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Backpropagation (BP) is the cornerstone of deep learning, but its reliance on
global gradient synchronization limits scalability and imposes significant
memory overhead. We propose Stochastic Variational Propagation (SVP), a
scalable alternative that reframes training as hierarchical variational
inference. SVP treats layer activations as latent variables and optimizes local
Evidence Lower Bounds (ELBOs), enabling independent, local updates while
preserving global coherence. However, directly applying KL divergence in
layer-wise ELBOs risks inter-layer's representation collapse due to excessive
compression. To prevent this, SVP projects activations into low-dimensional
spaces via fixed random matrices, ensuring information preservation and
representational diversity. Combined with a feature alignment loss for
inter-layer consistency, SVP achieves competitive accuracy with BP across
diverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to
ImageNet), reduces memory usage by up to 4x, and significantly improves
scalability. More broadly, SVP introduces a probabilistic perspective to deep
representation learning, opening pathways toward more modular and interpretable
neural network design.

</details>


### [94] [Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks](https://arxiv.org/abs/2505.05190)
*Yixin Cheng,Hongcheng Guo,Yangming Li,Leonid Sigal*

Main category: cs.LG

TL;DR: 本文揭示了当前文本水印算法在高熵标记中嵌入水印的设计可能被攻击者利用的漏洞，提出了一种通用的高效改写攻击方法SIRA，实验结果显示其攻击成功率高且成本低，强调了对更鲁棒水印算法的迫切需求。


<details>
  <summary>Details</summary>
Motivation: 当前文本水印算法在高熵标记中嵌入水印以确保文本质量，但这一设计存在可能被攻击者利用的风险，研究者旨在揭示这一漏洞并评估其影响。

Method: 提出了一种名为SIRA的通用高效改写攻击方法，通过计算每个标记的自信息来识别潜在的模式标记并进行针对性攻击。

Result: 实验结果显示，SIRA在七种最新的水印方法上实现了接近100%的攻击成功率，且每百万标记成本仅为0.88美元。

Conclusion: 当前水印算法的设计存在广泛漏洞，亟需开发更鲁棒的水印技术以应对此类攻击。

Abstract: Text watermarking aims to subtly embed statistical signals into text by
controlling the Large Language Model (LLM)'s sampling process, enabling
watermark detectors to verify that the output was generated by the specified
model. The robustness of these watermarking algorithms has become a key factor
in evaluating their effectiveness. Current text watermarking algorithms embed
watermarks in high-entropy tokens to ensure text quality. In this paper, we
reveal that this seemingly benign design can be exploited by attackers, posing
a significant risk to the robustness of the watermark. We introduce a generic
efficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA),
which leverages the vulnerability by calculating the self-information of each
token to identify potential pattern tokens and perform targeted attack. Our
work exposes a widely prevalent vulnerability in current watermarking
algorithms. The experimental results show SIRA achieves nearly 100% attack
success rates on seven recent watermarking methods with only 0.88 USD per
million tokens cost. Our approach does not require any access to the watermark
algorithms or the watermarked LLM and can seamlessly transfer to any LLM as the
attack model, even mobile-level models. Our findings highlight the urgent need
for more robust watermarking.

</details>


### [95] [Long-Term Individual Causal Effect Estimation via Identifiable Latent Representation Learning](https://arxiv.org/abs/2505.05192)
*Ruichu Cai,Junjie Wan,Weilin Chen,Zeqin Yang,Zijian Li,Peng Zhen,Jiecheng Guo*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的方法，通过利用数据的自然异质性（如多源数据）来识别潜在混淆因素，从而无需依赖理想化假设即可估计长期个体因果效应。通过潜在表示学习，该方法在理论和实验上均展现了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法依赖理想化假设（如潜在无混淆假设或加性等混淆偏差假设）来处理观测数据中的潜在混淆问题，但这些假设在现实中常被违背。本文旨在消除这些假设，开发一种更实用的长期因果效应估计方法。

Method: 提出基于潜在表示学习的估计器，利用数据的自然异质性（如多源数据）识别潜在混淆因素，并通过理论证明其可识别性，最终实现长期效应的估计。

Result: 在多个合成和半合成数据集上的实验表明，该方法能够有效估计长期因果效应，显著优于依赖理想化假设的现有方法。

Conclusion: 论文通过利用数据异质性避免了理想化假设，提出了一种更实用的长期因果效应估计方法，并通过理论和实验验证了其有效性。

Abstract: Estimating long-term causal effects by combining long-term observational and
short-term experimental data is a crucial but challenging problem in many
real-world scenarios. In existing methods, several ideal assumptions, e.g.
latent unconfoundedness assumption or additive equi-confounding bias
assumption, are proposed to address the latent confounder problem raised by the
observational data. However, in real-world applications, these assumptions are
typically violated which limits their practical effectiveness. In this paper,
we tackle the problem of estimating the long-term individual causal effects
without the aforementioned assumptions. Specifically, we propose to utilize the
natural heterogeneity of data, such as data from multiple sources, to identify
latent confounders, thereby significantly avoiding reliance on idealized
assumptions. Practically, we devise a latent representation learning-based
estimator of long-term causal effects. Theoretically, we establish the
identifiability of latent confounders, with which we further achieve long-term
effect identification. Extensive experimental studies, conducted on multiple
synthetic and semi-synthetic datasets, demonstrate the effectiveness of our
proposed method.

</details>


### [96] [Concept-Based Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.05195)
*Xinyue Xu,Yueying Hu,Hui Tang,Yi Qin,Lu Mi,Hao Wang,Xiaomeng Li*

Main category: cs.LG

TL;DR: 提出了Concept-based Unsupervised Domain Adaptation (CUDA)框架，通过对抗训练对齐跨领域概念表示，引入松弛阈值避免性能下降，无需标记概念数据即可推断目标领域概念，并在真实数据集上显著优于现有CBM和DA方法。


<details>
  <summary>Details</summary>
Motivation: 传统Concept Bottleneck Models (CBMs)假设训练和测试数据分布相同，但在领域偏移时性能下降且泛化能力差。为增强CBMs的鲁棒性，提出CUDA框架。

Method: CUDA采用对抗训练对齐概念表示，引入松弛阈值允许概念分布的细微差异，无需标记数据即可推断目标领域概念，并将概念学习与传统领域自适应结合。

Result: 实验表明，CUDA在真实数据集上显著优于现有CBM和DA方法。

Conclusion: CUDA框架不仅提升了CBMs在领域偏移下的性能，还提供了理论保证，为领域自适应设立了新基准。

Abstract: Concept Bottleneck Models (CBMs) enhance interpretability by explaining
predictions through human-understandable concepts but typically assume that
training and test data share the same distribution. This assumption often fails
under domain shifts, leading to degraded performance and poor generalization.
To address these limitations and improve the robustness of CBMs, we propose the
Concept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA is designed
to: (1) align concept representations across domains using adversarial
training, (2) introduce a relaxation threshold to allow minor domain-specific
differences in concept distributions, thereby preventing performance drop due
to over-constraints of these distributions, (3) infer concepts directly in the
target domain without requiring labeled concept data, enabling CBMs to adapt to
diverse domains, and (4) integrate concept learning into conventional domain
adaptation (DA) with theoretical guarantees, improving interpretability and
establishing new benchmarks for DA. Experiments demonstrate that our approach
significantly outperforms the state-of-the-art CBM and DA methods on real-world
datasets.

</details>


### [97] [GFlowNets for Active Learning Based Resource Allocation in Next Generation Wireless Networks](https://arxiv.org/abs/2505.05224)
*Charbel Bou Chaaya,Mehdi Bennis*

Main category: cs.LG

TL;DR: 该论文提出了一种基于生成流网络（GFlowNet）的新型主动学习框架，用于解决无线系统中通信、感知和计算等多功能集成的资源分配问题，其性能较基准方法提升了20%，且所需的获取轮次更少。


<details>
  <summary>Details</summary>
Motivation: 无线系统中集成了多种功能（如通信、感知和计算），资源分配问题具有高维和离散的特性，传统的资源管理方法难以同时满足这些异质需求并高效解决问题。

Method: 作者设计了一种基于GFlowNet的主动学习框架，通过序列化生成资源分配模式并在环境中评估，迭代更新环境的替代模型。GFlowNet能按训练奖励比例生成多样化的解决方案。

Result: 仿真结果表明，该方法在资源分配上的性能比基准方法提高了20%，同时所需的获取轮次少于传统方法的一半。

Conclusion: 基于GFlowNet的主动学习框架能高效生成多样化的资源管理方案，显著提升无线系统的资源分配性能。

Abstract: In this work, we consider the radio resource allocation problem in a wireless
system with various integrated functionalities, such as communication, sensing
and computing. We design suitable resource management techniques that can
simultaneously cater to those heterogeneous requirements, and scale
appropriately with the high-dimensional and discrete nature of the problem. We
propose a novel active learning framework where resource allocation patterns
are drawn sequentially, evaluated in the environment, and then used to
iteratively update a surrogate model of the environment. Our method leverages a
generative flow network (GFlowNet) to sample favorable solutions, as such
models are trained to generate compositional objects proportionally to their
training reward, hence providing an appropriate coverage of its modes. As such,
GFlowNet generates diverse and high return resource management designs that
update the surrogate model and swiftly discover suitable solutions. We provide
simulation results showing that our method can allocate radio resources
achieving 20% performance gains against benchmarks, while requiring less than
half of the number of acquisition rounds.

</details>


### [98] [Put CASH on Bandits: A Max K-Armed Problem for Automated Machine Learning](https://arxiv.org/abs/2505.05226)
*Amir Rezaei Balef,Claire Vernade,Katharina Eggensperger*

Main category: cs.LG

TL;DR: MaxUCB是一种针对AutoML中CASH问题的max $k$-armed bandit方法，特别适用于轻尾和有界奖励分布，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决AutoML中Combined Algorithm Selection and Hyperparameter optimization (CASH)的资源分配问题，传统方法假设奖励分布为重尾，不适用于轻尾和有界分布的场景。

Method: 提出MaxUCB方法，通过max $k$-armed bandit权衡模型类探索和超参数优化，适用于轻尾和有界奖励分布。

Result: 在四个标准AutoML基准上进行了理论和实证评估，表现优于先前方法。

Conclusion: MaxUCB在CASH问题中表现优越，尤其在轻尾和有界奖励分布场景下更为高效。

Abstract: The Combined Algorithm Selection and Hyperparameter optimization (CASH) is a
challenging resource allocation problem in the field of AutoML. We propose
MaxUCB, a max $k$-armed bandit method to trade off exploring different model
classes and conducting hyperparameter optimization. MaxUCB is specifically
designed for the light-tailed and bounded reward distributions arising in this
setting and, thus, provides an efficient alternative compared to classic max
$k$-armed bandit methods assuming heavy-tailed reward distributions. We
theoretically and empirically evaluate our method on four standard AutoML
benchmarks, demonstrating superior performance over prior approaches.

</details>


### [99] [Latte: Transfering LLMs` Latent-level Knowledge for Few-shot Tabular Learning](https://arxiv.org/abs/2505.05237)
*Ruxue Shi,Hengrui Gu,Hangting Ye,Yiwei Dai,Xu Shen,Xin Wang*

Main category: cs.LG

TL;DR: Latte是一个训练时知识提取框架，利用大语言模型（LLMs）的潜在先验知识优化下游表格学习模型，减少对小规模标记数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖测试时知识提取引入延迟，要么依赖文本级知识导致不可靠的特征工程，Latte旨在克服这些限制。

Method: 提出Latte框架，通过训练时提取LLMs的潜在知识，优化下游模型的泛化能力，支持跨特征值的信息加权融合。

Result: 在多个少样本表格学习基准测试中，Latte表现出色，成为该领域的先进方法。

Conclusion: Latte通过有效利用LLMs的先验知识和未标记数据，显著提升了少样本表格学习的性能。

Abstract: Few-shot tabular learning, in which machine learning models are trained with
a limited amount of labeled data, provides a cost-effective approach to
addressing real-world challenges. The advent of Large Language Models (LLMs)
has sparked interest in leveraging their pre-trained knowledge for few-shot
tabular learning. Despite promising results, existing approaches either rely on
test-time knowledge extraction, which introduces undesirable latency, or
text-level knowledge, which leads to unreliable feature engineering. To
overcome these limitations, we propose Latte, a training-time knowledge
extraction framework that transfers the latent prior knowledge within LLMs to
optimize a more generalized downstream model. Latte enables general
knowledge-guided downstream tabular learning, facilitating the weighted fusion
of information across different feature values while reducing the risk of
overfitting to limited labeled data. Furthermore, Latte is compatible with
existing unsupervised pre-training paradigms and effectively utilizes available
unlabeled samples to overcome the performance limitations imposed by an
extremely small labeled dataset. Extensive experiments on various few-shot
tabular learning benchmarks demonstrate the superior performance of Latte,
establishing it as a state-of-the-art approach in this domain

</details>


### [100] [Enhancing Treatment Effect Estimation via Active Learning: A Counterfactual Covering Perspective](https://arxiv.org/abs/2505.05242)
*Hechuan Wen,Tong Chen,Mingming Gong,Li Kheng Chai,Shazia Sadiq,Hongzhi Yin*

Main category: cs.LG

TL;DR: 论文提出了一种数据高效的因果效应估计方法，通过理论分析和主动学习，减少标记成本并提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有因果效应估计算法在标记数据不足时效果有限，而标记成本高（如肿瘤影像或活检）。如何在有限预算下获取高质量标记数据是关键问题。

Method: 通过理论分析提出“事实覆盖半径”和“反事实覆盖半径”，并设计贪心算法（FCCM）最大化覆盖以减少风险上界。

Result: FCCM在全合成和半合成数据集上均优于基线方法。

Conclusion: FCCM在有限标记预算下显著提升了因果效应估计的数据效率，适用于实际分布不均的场景。

Abstract: Although numerous complex algorithms for treatment effect estimation have
been developed in recent years, their effectiveness remains limited when
handling insufficiently labeled training sets due to the high cost of labeling
the effect after treatment, e.g., expensive tumor imaging or biopsy procedures
needed to evaluate treatment effects. Therefore, it becomes essential to
actively incorporate more high-quality labeled data, all while adhering to a
constrained labeling budget. To enable data-efficient treatment effect
estimation, we formalize the problem through rigorous theoretical analysis
within the active learning context, where the derived key measures --
\textit{factual} and \textit{counterfactual covering radius} determine the risk
upper bound. To reduce the bound, we propose a greedy radius reduction
algorithm, which excels under an idealized, balanced data distribution. To
generalize to more realistic data distributions, we further propose FCCM, which
transforms the optimization objective into the \textit{Factual} and
\textit{Counterfactual Coverage Maximization} to ensure effective radius
reduction during data acquisition. Furthermore, benchmarking FCCM against other
baselines demonstrates its superiority across both fully synthetic and
semi-synthetic datasets.

</details>


### [101] [Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration](https://arxiv.org/abs/2505.05262)
*Andreas Kontogiannis,Konstantinos Papathanasiou,Yi Shen,Giorgos Stamou,Michael M. Zavlanos,George Vouros*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的状态建模框架MARL SMPE算法，用于在分布式部分可观测环境中通过推断状态表示和增强探索策略来提升多智能体的协作能力，实验表明其在复杂协作任务中优于现有MARL算法。


<details>
  <summary>Details</summary>
Motivation: 在无通信能力的分布式部分可观测环境中，多智能体深度强化学习（MARL）面临协作挑战。本文旨在通过状态推断和探索策略改进来解决这些问题。

Method: 提出了MARL SMPE算法，通过推断非可观测状态的信念表示，并将这些表示显式和隐式地融入策略网络，同时采用对抗式探索策略以提高协作效果。

Result: 在MPE、LBF和RWARE基准的复杂完全协作任务中，SMPE优于现有MARL算法。

Conclusion: MARL SMPE算法通过状态建模和探索策略优化，显著提升了多智能体在部分可观测环境中的协作能力。

Abstract: Learning to cooperate in distributed partially observable environments with
no communication abilities poses significant challenges for multi-agent deep
reinforcement learning (MARL). This paper addresses key concerns in this
domain, focusing on inferring state representations from individual agent
observations and leveraging these representations to enhance agents'
exploration and collaborative task execution policies. To this end, we propose
a novel state modelling framework for cooperative MARL, where agents infer
meaningful belief representations of the non-observable state, with respect to
optimizing their own policies, while filtering redundant and less informative
joint state information. Building upon this framework, we propose the MARL SMPE
algorithm. In SMPE, agents enhance their own policy's discriminative abilities
under partial observability, explicitly by incorporating their beliefs into the
policy network, and implicitly by adopting an adversarial type of exploration
policies which encourages agents to discover novel, high-value states while
improving the discriminative abilities of others. Experimentally, we show that
SMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative
tasks from the MPE, LBF, and RWARE benchmarks.

</details>


### [102] [MTL-UE: Learning to Learn Nothing for Multi-Task Learning](https://arxiv.org/abs/2505.05279)
*Yi Yu,Song Xia,Siyuan Yang,Chenqi Kong,Wenhan Yang,Shijian Lu,Yap-Peng Tan,Alex C. Kot*

Main category: cs.LG

TL;DR: MTL-UE是首个针对多任务数据和多任务学习模型的统一框架，通过生成器结构和嵌入正则化提升攻击性能，支持密集预测任务且兼容现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的不可学习策略主要针对单任务学习和个人数据，而忽略了多任务数据和模型的保护需求，MTL-UE填补了这一空白。

Method: 采用基于生成器的结构，结合标签先验和类级特征嵌入，并通过任务内和任务间嵌入正则化增强攻击鲁棒性。

Result: 在4个MTL数据集、3种基础UE方法、5种模型主干和5种任务权重策略上均表现优越。

Conclusion: MTL-UE为多任务数据的保护提供了高效且通用的解决方案，并展现出强大的适应性和攻击性能。

Abstract: Most existing unlearnable strategies focus on preventing unauthorized users
from training single-task learning (STL) models with personal data.
Nevertheless, the paradigm has recently shifted towards multi-task data and
multi-task learning (MTL), targeting generalist and foundation models that can
handle multiple tasks simultaneously. Despite their growing importance, MTL
data and models have been largely neglected while pursuing unlearnable
strategies. This paper presents MTL-UE, the first unified framework for
generating unlearnable examples for multi-task data and MTL models. Instead of
optimizing perturbations for each sample, we design a generator-based structure
that introduces label priors and class-wise feature embeddings which leads to
much better attacking performance. In addition, MTL-UE incorporates intra-task
and inter-task embedding regularization to increase inter-class separation and
suppress intra-class variance which enhances the attack robustness greatly.
Furthermore, MTL-UE is versatile with good supports for dense prediction tasks
in MTL. It is also plug-and-play allowing integrating existing
surrogate-dependent unlearnable methods with little adaptation. Extensive
experiments show that MTL-UE achieves superior attacking performance
consistently across 4 MTL datasets, 3 base UE methods, 5 model backbones, and 5
MTL task-weighting strategies.

</details>


### [103] [Performance Estimation in Binary Classification Using Calibrated Confidence](https://arxiv.org/abs/2505.05295)
*Juhani Kivimäki,Jakub Białek,Wojtek Kuberski,Jukka K. Nurminen*

Main category: cs.LG

TL;DR: 论文提出了一种名为CBPE的新方法，用于在不依赖真实标签的情况下估计二元分类模型的性能指标，如准确率、精确率、召回率和F1分数。该方法通过将混淆矩阵元素视为随机变量，并利用模型的校准置信度分数估计其分布，从而推导出性能指标的完整概率分布。


<details>
  <summary>Details</summary>
Motivation: 传统的模型监测方法需要真实标签，但真实标签通常难以获取或延迟较大，导致监测延迟或无法进行。此外，现有方法多局限于准确率估计，忽视了其他重要指标。因此，研究旨在填补这一空白，提出一种无需真实标签即可估计多种性能指标的方法。

Method: 论文提出的CBPE方法将混淆矩阵的元素视为随机变量，并利用模型的校准置信度分数估计其分布。通过这种方法，可以推导出任意基于混淆矩阵的二元分类性能指标（如准确率、精确率、召回率和F1分数）的完整概率分布。

Result: CBPE方法能够生成具有强理论保证和有效置信区间的性能指标估计。实验验证了该方法在估计准确率、精确率、召回率和F1分数时的有效性。

Conclusion: CBPE是一种无需真实标签即可估计多种二元分类性能指标的通用方法，填补了现有研究的空白，并为模型监测提供了更全面的工具。

Abstract: Model monitoring is a critical component of the machine learning lifecycle,
safeguarding against undetected drops in the model's performance after
deployment. Traditionally, performance monitoring has required access to ground
truth labels, which are not always readily available. This can result in
unacceptable latency or render performance monitoring altogether impossible.
Recently, methods designed to estimate the accuracy of classifier models
without access to labels have shown promising results. However, there are
various other metrics that might be more suitable for assessing model
performance in many cases. Until now, none of these important metrics has
received similar interest from the scientific community. In this work, we
address this gap by presenting CBPE, a novel method that can estimate any
binary classification metric defined using the confusion matrix. In particular,
we choose four metrics from this large family: accuracy, precision, recall, and
F$_1$, to demonstrate our method. CBPE treats the elements of the confusion
matrix as random variables and leverages calibrated confidence scores of the
model to estimate their distributions. The desired metric is then also treated
as a random variable, whose full probability distribution can be derived from
the estimated confusion matrix. CBPE is shown to produce estimates that come
with strong theoretical guarantees and valid confidence intervals.

</details>


### [104] [Scalable Chain of Thoughts via Elastic Reasoning](https://arxiv.org/abs/2505.05315)
*Yuhui Xu,Hanze Dong,Lei Wang,Doyen Sahoo,Junnan Li,Caiming Xiong*

Main category: cs.LG

TL;DR: Elastic Reasoning框架将推理分为'思考'和'解决方案'两阶段，分别在预算约束下分配资源，显著提高了受限条件下的可靠性；并通过轻量级训练策略，使模型能自适应地处理推理截断问题，且在未见预算约束下表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂任务上表现优异，但其不可控的输出长度在实际部署中（如推理时间、延迟或计算资源受限的场景）带来了挑战。因此，需要一种在严格资源约束下仍能可靠运行的方法。

Method: 提出了Elastic Reasoning框架，明确将推理分为'思考'和'解决方案'两个阶段，并独立分配预算。通过集成GRPO的轻量级预算约束训练策略，让模型学会在思考截断时自适应推理。

Result: 在数学（AIME, MATH500）和编程（LiveCodeBench, Codeforces）基准测试中，Elastic Reasoning在严格预算约束下表现稳健，且训练成本显著低于基线方法。即使无约束条件下，其推理更简洁高效。

Conclusion: Elastic Reasoning为大规模可控推理提供了理论支持和实用方案，解决了资源受限场景下的可靠性问题，同时提升了整体推理效率。

Abstract: Large reasoning models (LRMs) have achieved remarkable progress on complex
tasks by generating extended chains of thought (CoT). However, their
uncontrolled output lengths pose significant challenges for real-world
deployment, where inference-time budgets on tokens, latency, or compute are
strictly constrained. We propose Elastic Reasoning, a novel framework for
scalable chain of thoughts that explicitly separates reasoning into two
phases--thinking and solution--with independently allocated budgets. At test
time, Elastic Reasoning prioritize that completeness of solution segments,
significantly improving reliability under tight resource constraints. To train
models that are robust to truncated thinking, we introduce a lightweight
budget-constrained rollout strategy, integrated into GRPO, which teaches the
model to reason adaptively when the thinking process is cut short and
generalizes effectively to unseen budget constraints without additional
training. Empirical results on mathematical (AIME, MATH500) and programming
(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning
performs robustly under strict budget constraints, while incurring
significantly lower training cost than baseline methods. Remarkably, our
approach also produces more concise and efficient reasoning even in
unconstrained settings. Elastic Reasoning offers a principled and practical
solution to the pressing challenge of controllable reasoning at scale.

</details>


### [105] [Nearly Optimal Sample Complexity for Learning with Label Proportions](https://arxiv.org/abs/2505.05355)
*Robert Busa-Fekete,Travis Dick,Claudio Gentile,Haim Kaplan,Tomer Koren,Uri Stemmer*

Main category: cs.LG

TL;DR: 本文研究从标签比例学习（LLP）问题，探索在仅知道每组样本的聚合标签值时，如何优化个体样本的预测性能，并展示了样本复杂度的最优性。


<details>
  <summary>Details</summary>
Motivation: 在部分信息场景下，如何通过组级别的标签比例信息实现个体级别的低预测误差是一个关键挑战。

Method: 设计了改进的经验风险最小化和随机梯度下降算法，结合方差缩减技术。

Result: 理论样本复杂度在组大小依赖性上优于现有文献，并在多个数据集上验证了更高的准确性和更低的样本需求。

Conclusion: 提出并验证了高效的LLP算法，为部分信息下的学习提供了理论支持和实践改进。

Abstract: We investigate Learning from Label Proportions (LLP), a partial information
setting where examples in a training set are grouped into bags, and only
aggregate label values in each bag are available. Despite the partial
observability, the goal is still to achieve small regret at the level of
individual examples. We give results on the sample complexity of LLP under
square loss, showing that our sample complexity is essentially optimal. From an
algorithmic viewpoint, we rely on carefully designed variants of Empirical Risk
Minimization, and Stochastic Gradient Descent algorithms, combined with ad hoc
variance reduction techniques. On one hand, our theoretical results improve in
important ways on the existing literature on LLP, specifically in the way the
sample complexity depends on the bag size. On the other hand, we validate our
algorithmic solutions on several datasets, demonstrating improved empirical
performance (better accuracy for less samples) against recent baselines.

</details>


### [106] [Denoising Diffusion Probabilistic Models for Coastal Inundation Forecasting](https://arxiv.org/abs/2505.05381)
*Kazi Ashik Islam,Zakaria Mehrab,Mahantesh Halappanavar,Henning Mortveit,Sridhar Katragadda,Jon Derek Loftis,Madhav Marathe*

Main category: cs.LG

TL;DR: DIFF-FLOOD是一种基于去噪扩散模型的概率时空预测方法，用于预测沿海洪水的淹没水平，在预测性能和可扩展性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 沿海洪水对社区造成重大风险，需要快速准确的预测方法来减轻潜在损害。

Method: DIFF-FLOOD结合了卷积神经网络和交叉注意力机制，利用空间（邻近位置的淹没水平和数字高程数据）和时间（历史淹没数据和其他协变量）上下文进行预测。

Result: 在美国弗吉尼亚州东岸的沿海淹没数据上测试显示，DIFF-FLOOD在预测性能上有6%至64%的提升。

Conclusion: DIFF-FLOOD在沿海洪水预测中表现出色，具有较高的预测性能和可扩展性。

Abstract: Coastal flooding poses significant risks to communities, necessitating fast
and accurate forecasting methods to mitigate potential damage. To approach this
problem, we present DIFF-FLOOD, a probabilistic spatiotemporal forecasting
method designed based on denoising diffusion models. DIFF-FLOOD predicts
inundation level at a location by taking both spatial and temporal context into
account. It utilizes inundation levels at neighboring locations and digital
elevation data as spatial context. Inundation history from a context time
window, together with additional co-variates are used as temporal context.
Convolutional neural networks and cross-attention mechanism are then employed
to capture the spatiotemporal dynamics in the data. We trained and tested
DIFF-FLOOD on coastal inundation data from the Eastern Shore of Virginia, a
region highly impacted by coastal flooding. Our results show that, DIFF-FLOOD
outperforms existing forecasting methods in terms of prediction performance (6%
to 64% improvement in terms of two performance metrics) and scalability.

</details>


### [107] [CART-ELC: Oblique Decision Tree Induction via Exhaustive Search](https://arxiv.org/abs/2505.05402)
*Andrew D. Laack*

Main category: cs.LG

TL;DR: 论文提出了一种新型算法CART-ELC，通过受限超平面上的穷举搜索来构建斜决策树，显著提升了小数据集上的分类准确性并生成更浅、更易解释的树。


<details>
  <summary>Details</summary>
Motivation: 传统轴对齐决策树分类性能有限，而依赖穷举搜索的斜决策树方法又面临计算挑战，因此需要开发一种高效且性能优越的斜决策树算法。

Method: 引入了CART-ELC算法，该算法在受限的超平面集合上进行穷举搜索，以构建斜决策树。

Result: 实验表明，CART-ELC在小数据集上表现优异，分类准确性优于现有算法，且生成的树更浅、更易解释。

Conclusion: CART-ELC是一种高效且有效的斜决策树算法，特别适合小数据集分类任务并能生成更易理解的模型。

Abstract: Oblique decision trees have attracted attention due to their potential for
improved classification performance over traditional axis-aligned decision
trees. However, methods that rely on exhaustive search to find oblique splits
face computational challenges. As a result, they have not been widely explored.
We introduce a novel algorithm, Classification and Regression Tree - Exhaustive
Linear Combinations (CART-ELC), for inducing oblique decision trees that
performs an exhaustive search on a restricted set of hyperplanes. We then
investigate the algorithm's computational complexity and its predictive
capabilities. Our results demonstrate that CART-ELC consistently achieves
competitive performance on small datasets, often yielding statistically
significant improvements in classification accuracy relative to existing
decision tree induction algorithms, while frequently producing shallower,
simpler, and thus more interpretable trees.

</details>


### [108] [Hide & Seek: Transformer Symmetries Obscure Sharpness & Riemannian Geometry Finds It](https://arxiv.org/abs/2505.05409)
*Marvin F. da Silva,Felix Dangel,Sageev Oore*

Main category: cs.LG

TL;DR: 论文重新定义了Transformer模型的锐度概念，提出了一种考虑其对称性的新锐度度量方法，并通过高阶近似提高了锐度与泛化能力的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有锐度度量方法未能充分考虑Transformer中的对称性，导致其与泛化能力的相关性较弱。作者认为必须完全考虑这些对称性才能准确度量锐度。

Method: 利用黎曼几何工具，在对称性修正的商流形上定义了新的锐度概念，并通过高阶近似方法逼近测地线。

Result: 在合成数据和真实世界任务（文本与图像分类）上，新锐度度量方法显示出与泛化能力的强相关性。

Conclusion: 通过完全考虑对称性并引入高阶近似，新锐度度量方法有效提升了与Transformer泛化能力的相关性。

Abstract: The concept of sharpness has been successfully applied to traditional
architectures like MLPs and CNNs to predict their generalization. For
transformers, however, recent work reported weak correlation between flatness
and generalization. We argue that existing sharpness measures fail for
transformers, because they have much richer symmetries in their attention
mechanism that induce directions in parameter space along which the network or
its loss remain identical. We posit that sharpness must account fully for these
symmetries, and thus we redefine it on a quotient manifold that results from
quotienting out the transformer symmetries, thereby removing their ambiguities.
Leveraging tools from Riemannian geometry, we propose a fully general notion of
sharpness, in terms of a geodesic ball on the symmetry-corrected quotient
manifold. In practice, we need to resort to approximating the geodesics. Doing
so up to first order yields existing adaptive sharpness measures, and we
demonstrate that including higher-order terms is crucial to recover correlation
with generalization. We present results on diagonal networks with synthetic
data, and show that our geodesic sharpness reveals strong correlation for
real-world transformers on both text and image classification tasks.

</details>


### [109] [DPQ-HD: Post-Training Compression for Ultra-Low Power Hyperdimensional Computing](https://arxiv.org/abs/2505.05413)
*Nilesh Prasad Pandey,Shriniwas Kulkarni,David Wang,Onat Gungor,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: 论文提出一种名为DPQ-HD的后训练压缩算法，通过分解、剪枝和量化三种技术结合，高效压缩超维计算（HDC）系统，无需重新训练即可接近浮点性能。在图像和图形分类任务中，内存减少20-100倍，精度仅下降1-2%，且优于现有后训练压缩方法，推理速度提升高达56倍。


<details>
  <summary>Details</summary>
Motivation: 超维计算（HDC）虽在边缘AI中前景广阔，但现有方法依赖高精度模型或编码矩阵，计算和内存开销大，难以适用于超低功耗设备。现有压缩技术（如精度降低或剪枝）通常需重新训练，成本高且不实用，因此亟需无需重新训练的高效压缩方案。

Method: 提出DPQ-HD算法，结合分解、剪枝和量化三种技术压缩HDC系统，并引入基于余弦相似度的渐进评分与早期退出机制，减少计算量并保持精度。

Result: 实验显示，DPQ-HD在图像和图形分类任务中内存减少20-100倍，精度损失仅1-2%，推理速度提升高达56倍，且优于现有后训练压缩方法。优化时间减少100倍，性能接近或优于需重新训练的先进方法。

Conclusion: DPQ-HD以极低开销实现高效压缩，适用于资源受限设备，为边缘AI提供了一种实用且高性能的解决方案。

Abstract: Hyperdimensional Computing (HDC) is emerging as a promising approach for edge
AI, offering a balance between accuracy and efficiency. However, current
HDC-based applications often rely on high-precision models and/or encoding
matrices to achieve competitive performance, which imposes significant
computational and memory demands, especially for ultra-low power devices. While
recent efforts use techniques like precision reduction and pruning to increase
the efficiency, most require retraining to maintain performance, making them
expensive and impractical. To address this issue, we propose a novel Post
Training Compression algorithm, Decomposition-Pruning-Quantization (DPQ-HD),
which aims at compressing the end-to-end HDC system, achieving near floating
point performance without the need of retraining. DPQ-HD reduces computational
and memory overhead by uniquely combining the above three compression
techniques and efficiently adapts to hardware constraints. Additionally, we
introduce an energy-efficient inference approach that progressively evaluates
similarity scores such as cosine similarity and performs early exit to reduce
the computation, accelerating prediction inference while maintaining accuracy.
We demonstrate that DPQ-HD achieves up to 20-100x reduction in memory for image
and graph classification tasks with only a 1-2% drop in accuracy compared to
uncompressed workloads. Lastly, we show that DPQ-HD outperforms the existing
post-training compression methods and performs better or at par with
retraining-based state-of-the-art techniques, requiring significantly less
overall optimization time (up to 100x) and faster inference (up to 56x) on a
microcontroller

</details>


### [110] [RL-DAUNCE: Reinforcement Learning-Driven Data Assimilation with Uncertainty-Aware Constrained Ensembles](https://arxiv.org/abs/2505.05452)
*Pouria Behnoudfar,Nan Chen*

Main category: cs.LG

TL;DR: RL-DAUNCE是一种基于强化学习的数据同化新方法，提高了计算效率并加入了物理约束，优于传统的EnKF方法。


<details>
  <summary>Details</summary>
Motivation: 传统的数据同化方法（如EnKF）在非高斯特征和物理约束下表现不佳，RL的序列决策框架更适应迭代同化过程。

Method: 设计了RL-DAUNCE方法，通过代理模拟集合成员、量化不确定性和物理约束，结合对偶优化策略和动作空间限制。

Result: 在Madden-Julian振荡中，RL-DAUNCE优于EnKF，尤其在恢复间歇信号、捕捉极端事件和不确定性量化方面表现突出。

Conclusion: RL-DAUNCE在保证物理一致性和高效性的同时，显著提升了数据同化的性能。

Abstract: Machine learning has become a powerful tool for enhancing data assimilation.
While supervised learning remains the standard method, reinforcement learning
(RL) offers unique advantages through its sequential decision-making framework,
which naturally fits the iterative nature of data assimilation by dynamically
balancing model forecasts with observations. We develop RL-DAUNCE, a new
RL-based method that enhances data assimilation with physical constraints
through three key aspects. First, RL-DAUNCE inherits the computational
efficiency of machine learning while it uniquely structures its agents to
mirror ensemble members in conventional data assimilation methods. Second,
RL-DAUNCE emphasizes uncertainty quantification by advancing multiple ensemble
members, moving beyond simple mean-state optimization. Third, RL-DAUNCE's
ensemble-as-agents design facilitates the enforcement of physical constraints
during the assimilation process, which is crucial to improving the state
estimation and subsequent forecasting. A primal-dual optimization strategy is
developed to enforce constraints, which dynamically penalizes the reward
function to ensure constraint satisfaction throughout the learning process.
Also, state variable bounds are respected by constraining the RL action space.
Together, these features ensure physical consistency without sacrificing
efficiency. RL-DAUNCE is applied to the Madden-Julian Oscillation, an
intermittent atmospheric phenomenon characterized by strongly non-Gaussian
features and multiple physical constraints. RL-DAUNCE outperforms the standard
ensemble Kalman filter (EnKF), which fails catastrophically due to the
violation of physical constraints. Notably, RL-DAUNCE matches the performance
of constrained EnKF, particularly in recovering intermittent signals, capturing
extreme events, and quantifying uncertainties, while requiring substantially
less computational effort.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [111] [Towards Artificial Intelligence Research Assistant for Expert-Involved Learning](https://arxiv.org/abs/2505.04638)
*Tianyu Liu,Simeng Han,Xiao Luo,Hanchen Wang,Pan Lu,Biqing Zhu,Yuge Wang,Keyi Li,Jiapeng Chen,Rihao Qu,Yufeng Liu,Xinyue Cui,Aviv Yaish,Yuhang Chen,Minsheng Hao,Chuhan Li,Kexing Li,Arman Cohan,Hua Xu,Mark Gerstein,James Zou,Hongyu Zhao*

Main category: cs.AI

TL;DR: 该研究介绍了多模态数据集ARIEL，用于评估和改进大型语言模型（LLM）和大型多模态模型（LMM）在生物医学研究中的文本总结和图像解释能力。通过专家评估和改进策略，模型表现提升，且探索了LMM在生成科学假设中的潜力。结果揭示了现有模型的优缺点，为未来研究提供了指导。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM和LMM在科学研究中展现出潜力，但其在生物医学应用中的可靠性和贡献尚未充分验证。研究者希望通过构建专门的数据集和评估方法，填补这一空白并为模型改进提供依据。

Method: 研究创建了包含生物医学文章和图像的开放数据集，并设计了相应问题。通过基准测试开源和闭源模型，结合专家人工评估，采用提示工程和微调策略优化模型表现，同时利用测试时计算扩展提升LMM的推理能力。

Result: 模型在文本总结和图像解释任务上表现优于人工专家修正，且在生成科学假设方面展现出潜力。但研究也揭示了当前模型在生物医学领域的显著局限性。

Conclusion: ARIEL数据集和评估框架为LLM和LMM在生物医学研究中的应用提供了实用性见解，同时指出了未来模型改进和部署的方向。

Abstract: Large Language Models (LLMs) and Large Multi-Modal Models (LMMs) have emerged
as transformative tools in scientific research, yet their reliability and
specific contributions to biomedical applications remain insufficiently
characterized. In this study, we present \textbf{AR}tificial
\textbf{I}ntelligence research assistant for \textbf{E}xpert-involved
\textbf{L}earning (ARIEL), a multimodal dataset designed to benchmark and
enhance two critical capabilities of LLMs and LMMs in biomedical research:
summarizing extensive scientific texts and interpreting complex biomedical
figures. To facilitate rigorous assessment, we create two open-source sets
comprising biomedical articles and figures with designed questions. We
systematically benchmark both open- and closed-source foundation models,
incorporating expert-driven human evaluations conducted by doctoral-level
experts. Furthermore, we improve model performance through targeted prompt
engineering and fine-tuning strategies for summarizing research papers, and
apply test-time computational scaling to enhance the reasoning capabilities of
LMMs, achieving superior accuracy compared to human-expert corrections. We also
explore the potential of using LMM Agents to generate scientific hypotheses
from diverse multimodal inputs. Overall, our results delineate clear strengths
and highlight significant limitations of current foundation models, providing
actionable insights and guiding future advancements in deploying large-scale
language and multi-modal models within biomedical research.

</details>


### [112] [Computational Irreducibility as the Foundation of Agency: A Formal Model Connecting Undecidability to Autonomous Behavior in Complex Systems](https://arxiv.org/abs/2505.04646)
*Poria Azadi*

Main category: cs.AI

TL;DR: 论文探讨了自主性与代理的起源，通过计算限制（如可判定性、完备性、计算不可约性）与物理概念的联系，提出一个形式化模型并证明真正自主性必然伴随外部视角的不可判定性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解自主性的计算基础，揭示代理如何在与环境的复杂互动中产生不可预测性和新颖信息，从而形成目标导向行为。

Method: 采用算法信息理论，构建‘最小代理’的形式化模型，分析代理-环境交互中的不可判定性和计算不可约性对代理行为的约束。

Result: 证明真正的自主性必然伴随外部视角的不可判定性，代理行为源于环境互动的计算复杂性，且无法完全预测。

Conclusion: 自主性与计算不可约性紧密关联，代理的‘源头性’是其核心特征，这一框架对AI设计、意识研究和自由意志概念具有启发意义。

Abstract: This article explores the emergence of autonomy and agency by connecting
fundamental computational limits (decidability, completeness, computational
irreducibility) with physical concepts. We introduce a formal model of a
"minimal agent" operating within potentially Turing-complete environments.
Using algorithmic information theory, we argue that the inherent undecidability
and computational irreducibility of agent-environment interaction lead to
unpredictability and novel information generation, enabling agency (effective
goal-directed action). Computational irreducibility prevents full external
prediction, creating necessary conditions for autonomous behavior. We relate
this to computational sourcehood, where an agent is the irreducible origin of
its behavior, though formalizing this concept remains challenging. Our central
thesis, formally proven, is that genuine autonomy necessarily implies
undecidability from an external perspective, distinguishing autonomous systems
from predictable ones. We propose that agency arises when agent-environment
coupling complexity allows mutual information between internal states and
relevant environmental variables to increase, particularly where analytical
solutions are absent and operational closure is needed for persistence. This
framework links agency directly to the computational properties of interaction,
offering implications for understanding consciousness, designing autonomous AI,
and reconceptualizing free will in a deterministic yet computationally
irreducible universe.

</details>


### [113] [Dynamic Location Search for Identifying Maximum Weighted Independent Sets in Complex Networks](https://arxiv.org/abs/2505.04674)
*Enqiang Zhu,Chenkai Hao,Chanjuan Liu,Yongsheng Rao*

Main category: cs.AI

TL;DR: 该论文提出了一种名为DynLS的新算法，用于高效解决最大加权独立集（MWIS）问题，优化了智能交通系统中AI技术的计算效率和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有AI技术在智能交通系统中生成高质量数据和优化解决方案时，通常需要大量训练时间和计算资源，尤其是在大规模复杂场景中。为解决这一问题，研究团队提出了DynLS算法。

Method: DynLS算法整合了三种创新技术：基于分数的自适应顶点扰动（SAVP）加速收敛；区域定位机制（RLM）动态调整搜索空间以避免局部最优；以及结合顶点交换策略和奖励机制的新型变量邻域下降策略（ComLS）。

Result: 实验结果显示，DynLS在360个测试实例中优于其他五种领先算法，其中350个实例达到最佳解，比第二名算法（Cyclic-Fast）多解决177个实例，且收敛速度与Cyclic-Fast相当。

Conclusion: DynLS是MWIS问题启发式算法的重大进步，为优化智能交通系统中的AI技术提供了高效且实用的解决方案。

Abstract: While Artificial intelligence (AI), including Generative AI, are effective at
generating high-quality traffic data and optimization solutions in intelligent
transportation systems (ITSs), these techniques often demand significant
training time and computational resources, especially in large-scale and
complex scenarios. To address this, we introduce a novel and efficient
algorithm for solving the maximum weighted independent set (MWIS) problem,
which can be used to model many ITSs applications, such as traffic signal
control and vehicle routing. Given the NP-hard nature of the MWIS problem, our
proposed algorithm, DynLS, incorporates three key innovations to solve it
effectively. First, it uses a scores-based adaptive vertex perturbation (SAVP)
technique to accelerate convergence, particularly in sparse graphs. Second, it
includes a region location mechanism (RLM) to help escape local optima by
dynamically adjusting the search space. Finally, it employs a novel variable
neighborhood descent strategy, ComLS, which combines vertex exchange strategies
with a reward mechanism to guide the search toward high-quality solutions. Our
experimental results demonstrate DynLS's superior performance, consistently
delivering high-quality solutions within 1000 seconds. DynLS outperformed five
leading algorithms across 360 test instances, achieving the best solution for
350 instances and surpassing the second-best algorithm, Cyclic-Fast, by 177
instances. Moreover, DynLS matched Cyclic-Fast's convergence speed,
highlighting its efficiency and practicality. This research represents a
significant advancement in heuristic algorithms for the MWIS problem, offering
a promising approach to aid AI techniques in optimizing intelligent
transportation systems.

</details>


### [114] [The Promise and Limits of LLMs in Constructing Proofs and Hints for Logic Problems in Intelligent Tutoring Systems](https://arxiv.org/abs/2505.04736)
*Sutapa Dey Tithi,Arun Kumar Ramesh,Clara DiMarco,Xiaoyi Tian,Nazia Alam,Kimia Fazeli,Tiffany Barnes*

Main category: cs.AI

TL;DR: 论文评估了大型语言模型（LLM）在逐步构建命题逻辑证明中的表现，发现DeepSeek-V3表现最佳（84.4%准确率），并探讨了LLM生成提示的潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 现有智能辅导系统（ITS）依赖模板化反馈，缺乏个性化。尽管LLM能生成动态反馈，但存在幻觉或教学不当的风险，因此需评估其在逻辑辅导中的适用性。

Method: 比较六种提示技术及四种先进LLM在358道命题逻辑问题上的逐步证明准确率，并基于最佳模型（DeepSeek-V3）生成1,050条提示，用LLM评分和人工专家评估其教学效果。

Result: DeepSeek-V3在逐步证明中准确率最高（84.4%），其生成的提示在清晰度与一致性上获高评分（75%准确率），但对提示的上下文解释较弱。

Conclusion: LLM可增强逻辑辅导系统的提示生成，但需进一步优化以确保准确性和教学适用性。

Abstract: Intelligent tutoring systems have demonstrated effectiveness in teaching
formal propositional logic proofs, but their reliance on template-based
explanations limits their ability to provide personalized student feedback.
While large language models (LLMs) offer promising capabilities for dynamic
feedback generation, they risk producing hallucinations or pedagogically
unsound explanations. We evaluated the stepwise accuracy of LLMs in
constructing multi-step symbolic logic proofs, comparing six prompting
techniques across four state-of-the-art LLMs on 358 propositional logic
problems. Results show that DeepSeek-V3 achieved superior performance with
84.4% accuracy on stepwise proof construction and excelled particularly in
simpler rules. We further used the best-performing LLM to generate explanatory
hints for 1,050 unique student problem-solving states from a logic ITS and
evaluated them on 4 criteria with both an LLM grader and human expert ratings
on a 20% sample. Our analysis finds that LLM-generated hints were 75% accurate
and rated highly by human evaluators on consistency and clarity, but did not
perform as well explaining why the hint was provided or its larger context. Our
results demonstrate that LLMs may be used to augment tutoring systems with
logic tutoring hints, but requires additional modifications to ensure accuracy
and pedagogical appropriateness.

</details>


### [115] [Is there Value in Reinforcement Learning?](https://arxiv.org/abs/2505.04822)
*Lior Fox,Yonatan Loewenstein*

Main category: cs.AI

TL;DR: 论文探讨了强化学习中动作价值（action-values）的显式表示争议，指出策略梯度（PG）模型并非“无价值”，因为学习仍需依赖价值。作者建议应将重点转向对底层建模假设的批判性评估，而非仅争论优化方法。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中动作价值显式表示的争议，指出当前对策略梯度模型的误解，并呼吁重新评估建模假设。

Method: 通过理论分析，对比策略梯度与价值基模型的差异，并探讨其学习机制。

Result: 策略梯度模型在学习阶段仍需依赖价值，因此并非真正“无价值”；底层建模假设是争议的核心。

Conclusion: 应重新评估强化学习的建模假设，并考虑算法复杂性对模型评价的影响。

Abstract: Action-values play a central role in popular Reinforcement Learing (RL)
models of behavior. Yet, the idea that action-values are explicitly represented
has been extensively debated. Critics had therefore repeatedly suggested that
policy-gradient (PG) models should be favored over value-based (VB) ones, as a
potential solution for this dilemma. Here we argue that this solution is
unsatisfying. This is because PG methods are not, in fact, "Value-free" --
while they do not rely on an explicit representation of Value for acting
(stimulus-response mapping), they do require it for learning. Hence, switching
to PG models is, per se, insufficient for eliminating Value from models of
behavior. More broadly, the requirement for a representation of Value stems
from the underlying assumptions regarding the optimization objective posed by
the standard RL framework, not from the particular algorithm chosen to solve
it. Previous studies mostly took these standard RL assumptions for granted, as
part of their conceptualization or problem modeling, while debating the
different methods used to optimize it (i.e., PG or VB). We propose that,
instead, the focus of the debate should shift to critically evaluating the
underlying modeling assumptions. Such evaluation is particularly important from
an experimental perspective. Indeed, the very notion of Value must be
reconsidered when standard assumptions (e.g., risk neutrality,
full-observability, Markovian environment, exponential discounting) are
relaxed, as is likely in natural settings. Finally, we use the Value debate as
a case study to argue in favor of a more nuanced, algorithmic rather than
statistical, view of what constitutes "a model" in cognitive sciences. Our
analysis suggests that besides "parametric" statistical complexity, additional
aspects such as computational complexity must also be taken into account when
evaluating model complexity.

</details>


### [116] [Large Language Models are Autonomous Cyber Defenders](https://arxiv.org/abs/2505.04843)
*Sebastián R. Castro,Roberto Campbell,Nancy Lau,Octavio Villalobos,Jiaqi Duan,Alvaro A. Cardenas*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型（LLMs）在多智能体自主网络防御（ACD）环境中的表现，提出了与CybORG CAGE 4环境的集成方案，并通过新通信协议探索LLM与强化学习（RL）智能体的协作，揭示了二者的优劣及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统ACD方法依赖单智能体和RL，但RL训练成本高且行为不可解释或迁移性差。LLMs能提供可解释的行动，但尚未评估其在多智能体场景中的表现或与其他ACD智能体的交互。

Method: 提出新方法将LLMs集成到CybORG CAGE 4环境中，设计通信协议评估LLM与RL智能体在多智能体ACD中的协作。

Result: 结果展示了LLMs和RL在多智能体ACD中的优势与局限，为未来ACD团队的设计与部署提供了方向。

Conclusion: 研究表明LLMs在多智能体ACD中具有潜力，结合其可解释性和RL的高效性，为未来混合智能体团队的研究奠定了基础。

Abstract: Fast and effective incident response is essential to prevent adversarial
cyberattacks. Autonomous Cyber Defense (ACD) aims to automate incident response
through Artificial Intelligence (AI) agents that plan and execute actions. Most
ACD approaches focus on single-agent scenarios and leverage Reinforcement
Learning (RL). However, ACD RL-trained agents depend on costly training, and
their reasoning is not always explainable or transferable. Large Language
Models (LLMs) can address these concerns by providing explainable actions in
general security contexts. Researchers have explored LLM agents for ACD but
have not evaluated them on multi-agent scenarios or interacting with other ACD
agents. In this paper, we show the first study on how LLMs perform in
multi-agent ACD environments by proposing a new integration to the CybORG CAGE
4 environment. We examine how ACD teams of LLM and RL agents can interact by
proposing a novel communication protocol. Our results highlight the strengths
and weaknesses of LLMs and RL and help us identify promising research
directions to create, train, and deploy future teams of ACD agents.

</details>


### [117] [CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused Text-to-Image Generation](https://arxiv.org/abs/2505.04851)
*Viacheslav Vasilev,Vladimir Arkhipkin,Julia Agafonova,Tatiana Nikulina,Evelina Mironova,Alisa Shichanina,Nikolai Gerasimenko,Mikhail Shoytov,Denis Dimitrov*

Main category: cs.AI

TL;DR: 该论文指出，当前流行的文本到图像生成模型在面对个别文化查询时存在知识缺口，主要因为训练数据偏重欧美文化。为解决问题，作者提出了一种基于文化代码（尤其是俄罗斯文化）的数据收集和处理方法，并通过Kandinsky 3.1模型验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型的文化适应性不足，可能导致错误结果、生成质量下降及传播刻板印象或冒犯性内容。论文旨在填补这一研究空白，提升模型对特定文化（如俄罗斯文化）的理解能力。

Method: 作者提出了一种方法，用于收集和处理基于文化代码的数据，特别是在俄罗斯文化背景下。随后利用Kandinsky 3.1模型分析这些数据对生成质量的影响。

Result: 通过人类评估显示，使用该方法后，模型对俄罗斯文化的认知水平有所提升，生成结果的准确性和文化相关性得到了改善。

Conclusion: 研究表明，针对特定文化的数据收集和处理方法可以有效提升文本到图像生成模型的文化适应性，减少文化偏见，为未来研究提供了新方向。

Abstract: Despite the fact that popular text-to-image generation models cope well with
international and general cultural queries, they have a significant knowledge
gap regarding individual cultures. This is due to the content of existing large
training datasets collected on the Internet, which are predominantly based on
Western European or American popular culture. Meanwhile, the lack of cultural
adaptation of the model can lead to incorrect results, a decrease in the
generation quality, and the spread of stereotypes and offensive content. In an
effort to address this issue, we examine the concept of cultural code and
recognize the critical importance of its understanding by modern image
generation models, an issue that has not been sufficiently addressed in the
research community to date. We propose the methodology for collecting and
processing the data necessary to form a dataset based on the cultural code, in
particular the Russian one. We explore how the collected data affects the
quality of generations in the national domain and analyze the effectiveness of
our approach using the Kandinsky 3.1 text-to-image model. Human evaluation
results demonstrate an increase in the level of awareness of Russian culture in
the model.

</details>


### [118] [Enigme: Generative Text Puzzles for Evaluating Reasoning in Language Models](https://arxiv.org/abs/2505.04914)
*John Hawkins*

Main category: cs.AI

TL;DR: 该论文探讨了Transformer-decoder语言模型在推理能力上的局限性，并通过设计文本谜题来测试其边界，提出了开源库enigme用于评估和改进AI架构的推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究的目标是理解Transformer-decoder模型在生成推理时的局限性，通过分析其架构约束和潜在变量结构，设计任务来测试其推理能力的边界。

Method: 通过考虑Transformer-decoder模型的潜在变量结构，设计并生成文本谜题（enigme），这是一种开源工具，用于训练和评估模型在各种推理任务上的表现。

Result: 提出了enigme这一开源库，能够生成多样化的文本谜题，用于测试和改进AI模型的推理能力，特别是针对Transformer-decoder架构的局限性。

Conclusion: 通过明确Transformer-decoder模型的推理限制，并开发enigme工具，论文为未来AI架构的推理能力评估和改进提供了重要支持。

Abstract: Transformer-decoder language models are a core innovation in text based
generative artificial intelligence. These models are being deployed as
general-purpose intelligence systems in many applications. Central to their
utility is the capacity to understand natural language commands and exploit the
reasoning embedded in human text corpora to apply some form of reasoning
process to a wide variety of novel tasks. To understand the limitations of this
approach to generating reasoning we argue that we need to consider the
architectural constraints of these systems. Consideration of the latent
variable structure of transformer-decoder models allows us to design reasoning
tasks that should probe the boundary of their capacity to reason. We present
enigme, an open-source library for generating text-based puzzles to be used in
training and evaluating reasoning skills within transformer-decoder models and
future AI architectures.

</details>


### [119] [Belief Filtering for Epistemic Control in Linguistic State Space](https://arxiv.org/abs/2505.04927)
*Sebastian Dumbrava*

Main category: cs.AI

TL;DR: 本文研究了信念过滤作为一种人工代理认知控制的机制，通过自然语言片段动态调控内部认知状态，基于语义流形框架实现，强调了其对AI安全和对齐的潜在提升作用。


<details>
  <summary>Details</summary>
Motivation: 动机在于探索如何通过语言化的认知架构调控人工代理的内部认知状态，以增强AI的安全性和对齐性。

Method: 方法是在语义流形框架内，将信念状态表示为动态的自然语言片段集合，并开发内容感知的信念过滤器操作这些片段。

Result: 结果表明，这种语言基础的架构具有固有的可解释性和模块化，能够直接实现信念过滤，为代理调控提供了理论支持。

Conclusion: 结论是信念过滤为AI认知治理提供了新方向，尤其在语义空间的干预方面具有潜在应用价值。

Abstract: We examine belief filtering as a mechanism for the epistemic control of
artificial agents, focusing on the regulation of internal cognitive states
represented as linguistic expressions. This mechanism is developed within the
Semantic Manifold framework, where belief states are dynamic, structured
ensembles of natural language fragments. Belief filters act as content-aware
operations on these fragments across various cognitive transitions. This paper
illustrates how the inherent interpretability and modularity of such a
linguistically-grounded cognitive architecture directly enable belief
filtering, offering a principled approach to agent regulation. The study
highlights the potential for enhancing AI safety and alignment through
structured interventions in an agent's internal semantic space and points to
new directions for architecturally embedded cognitive governance.

</details>


### [120] [Position: Epistemic Artificial Intelligence is Essential for Machine Learning Models to Know When They Do Not Know](https://arxiv.org/abs/2505.04950)
*Shireen Kudukkil Manchingal,Fabio Cuzzolin*

Main category: cs.AI

TL;DR: 论文提出AI在处理不确定性和训练数据外泛化能力的不足，主张向'认知AI'转变，强调模型需学会识别和管理不确定性以提高系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型（如自动驾驶系统）在面对未知或对抗性数据时预测不稳健，传统机器学习方法因过度依赖数据拟合和领域适应而难以解决此问题。

Method: 提出'认知人工智能'范式，强调模型需从已知和未知中学习，重点关注不确定性的识别与管理。

Result: 通过识别和管理不确定性，'认知AI'有望提高AI系统的韧性和鲁棒性，以应对不可预测的真实环境。

Conclusion: 论文主张通过'认知AI'范式改善AI系统的鲁棒性，尤其是在处理不确定性和未知情况时。

Abstract: Despite the impressive achievements of AI, including advancements in
generative models and large language models, there remains a significant gap in
the ability of AI to handle uncertainty and generalize beyond the training
data. We argue that AI models, especially in autonomous systems, fail to make
robust predictions when faced with unfamiliar or adversarial data, as evidenced
by incidents with autonomous vehicles. Traditional machine learning approaches
struggle to address these issues due to an overemphasis on data fitting and
domain adaptation. This position paper posits a paradigm shift towards
epistemic artificial intelligence, emphasizing the need for models to learn not
only from what they know but also from their ignorance. This approach, which
focuses on recognizing and managing uncertainty, offers a potential solution to
improve the resilience and robustness of AI systems, ensuring that they can
better handle unpredictable real-world environments.

</details>


### [121] [Position: The AI Conference Peer Review Crisis Demands Author Feedback and Reviewer Rewards](https://arxiv.org/abs/2505.04966)
*Jaeho Kim,Yunseok Lee,Seulki Lee*

Main category: cs.AI

TL;DR: 本文提议将传统的单向同行评审系统转变为双向反馈循环，通过作者评价评审质量和评审者获得正式认证来提升评审质量与责任。


<details>
  <summary>Details</summary>
Motivation: 随着AI会议论文投稿量激增（每场会议超过10,000篇），评审质量和评审者责任问题日益突出，需要改革现有评审系统以促进可持续、高质量的同行评审。

Method: 提出了两种机制：1. 允许作者评价评审质量的双阶段双向评审系统，以减少报复行为；2. 系统性评审奖励制度，激励高质量评审。

Result: 本文呼吁社区关注这些问题并支持改革，以优化同行评审流程。

Conclusion: 通过双向反馈和评审者激励机制，可以提升评审系统的责任感和质量，从而解决当前评审中的问题。

Abstract: The peer review process in major artificial intelligence (AI) conferences
faces unprecedented challenges with the surge of paper submissions (exceeding
10,000 submissions per venue), accompanied by growing concerns over review
quality and reviewer responsibility. This position paper argues for the need to
transform the traditional one-way review system into a bi-directional feedback
loop where authors evaluate review quality and reviewers earn formal
accreditation, creating an accountability framework that promotes a
sustainable, high-quality peer review system. The current review system can be
viewed as an interaction between three parties: the authors, reviewers, and
system (i.e., conference), where we posit that all three parties share
responsibility for the current problems. However, issues with authors can only
be addressed through policy enforcement and detection tools, and ethical
concerns can only be corrected through self-reflection. As such, this paper
focuses on reforming reviewer accountability with systematic rewards through
two key mechanisms: (1) a two-stage bi-directional review system that allows
authors to evaluate reviews while minimizing retaliatory behavior, (2)a
systematic reviewer reward system that incentivizes quality reviewing. We ask
for the community's strong interest in these problems and the reforms that are
needed to enhance the peer review process.

</details>


### [122] [Foam-Agent: Towards Automated Intelligent CFD Workflows](https://arxiv.org/abs/2505.04997)
*Ling Yue,Nithin Somasekharan,Yadi Cao,Shaowu Pan*

Main category: cs.AI

TL;DR: Foam-Agent是一个多代理框架，通过自然语言输入自动化OpenFOAM的CFD模拟流程，显著降低了CFD的使用门槛。


<details>
  <summary>Details</summary>
Motivation: CFD模拟通常需要大量专业知识和手动配置，Foam-Agent旨在通过自动化降低这一技术门槛。

Method: 采用分层多索引检索系统、依赖感知的文件生成系统和迭代错误纠正机制。

Result: 在110个模拟任务中，成功率达到83.6%，显著优于现有框架。

Conclusion: Foam-Agent能显著降低CFD的准入门槛，同时保持建模精度，展示了多代理系统在科学模拟工具中的潜力。

Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in various
engineering disciplines, but it often requires substantial domain expertise and
manual configuration, creating barriers to entry. We present Foam-Agent, a
multi-agent framework that automates complex OpenFOAM-based CFD simulation
workflows from natural language inputs. Our innovation includes (1) a
hierarchical multi-index retrieval system with specialized indices for
different simulation aspects, (2) a dependency-aware file generation system
that provides consistency management across configuration files, and (3) an
iterative error correction mechanism that diagnoses and resolves simulation
failures without human intervention. Through comprehensive evaluation on the
dataset of 110 simulation tasks, Foam-Agent achieves an 83.6% success rate with
Claude 3.5 Sonnet, significantly outperforming existing frameworks (55.5% for
MetaOpenFOAM and 37.3% for OpenFOAM-GPT). Ablation studies demonstrate the
critical contribution of each system component, with the specialized error
correction mechanism providing a 36.4% performance improvement. Foam-Agent
substantially lowers the CFD expertise threshold while maintaining modeling
accuracy, demonstrating the potential of specialized multi-agent systems to
democratize access to complex scientific simulation tools. The code is public
at https://github.com/csml-rpi/Foam-Agent

</details>


### [123] [A Reputation System for Large Language Model-based Multi-agent Systems to Avoid the Tragedy of the Commons](https://arxiv.org/abs/2505.05029)
*Siyue Ren,Wanli Fu,Xinkun Zou,Chen Shen,Yi Cai,Chen Chu,Zhen Wang,Shuyue Hu*

Main category: cs.AI

TL;DR: 论文提出了RepuNet，一个动态双层声誉框架，通过直接互动和间接交流形成声誉，有效缓解生成型多智能体系统中的'公地悲剧'，促进合作并产生丰富涌现行为。


<details>
  <summary>Details</summary>
Motivation: 解决生成型多智能体系统中因个体自利行为导致的'公地悲剧'问题，探索声誉系统的作用。

Method: 提出RepuNet框架，结合直接互动和间接交流的动态声誉建模，通过连接或断开其他智能体调整交互。

Result: RepuNet成功缓解公地悲剧，促进合作，并涌现出合作集群、剥削者孤立和偏好正面八卦等行为。

Conclusion: 声誉系统能有效解决多智能体系统的合作问题，并带来多样的社会行为。

Abstract: The tragedy of the commons, where individual self-interest leads to
collectively disastrous outcomes, is a pervasive challenge in human society.
Recent studies have demonstrated that similar phenomena can arise in generative
multi-agent systems (MASs). To address this challenge, this paper explores the
use of reputation systems as a remedy. We propose RepuNet, a dynamic,
dual-level reputation framework that models both agent-level reputation
dynamics and system-level network evolution. Specifically, driven by direct
interactions and indirect gossip, agents form reputations for both themselves
and their peers, and decide whether to connect or disconnect other agents for
future interactions. Through two distinct scenarios, we show that RepuNet
effectively mitigates the 'tragedy of the commons', promoting and sustaining
cooperation in generative MASs. Moreover, we find that reputation systems can
give rise to rich emergent behaviors in generative MASs, such as the formation
of cooperative clusters, the social isolation of exploitative agents, and the
preference for sharing positive gossip rather than negative ones.

</details>


### [124] [Enhancing Reinforcement Learning for the Floorplanning of Analog ICs with Beam Search](https://arxiv.org/abs/2505.05059)
*Sandro Junior Della Rovere,Davide Basso,Luca Bortolussi,Mirjana Videnovic-Misic,Husni Habal*

Main category: cs.AI

TL;DR: 该论文提出了一种结合强化学习（RL）和束搜索（BS）的混合方法，用于解决模拟IC布局问题。通过BS增强RL的推理过程，该方法能灵活生成满足多种目标权重的版图，并解决拥堵问题，无需重新训练或微调策略。实验结果显示在面积、死区和线长上相比标准RL应用有5-85%的提升，性能接近现有最优技术。


<details>
  <summary>Details</summary>
Motivation: 模拟IC布局需要在考虑器件物理特性和电路变异性的同时做出复杂权衡，这使得基于学习的全自动化方法难以实现。为提升布局效率和质量，作者提出了结合RL与BS的混合方法。

Method: 该方法结合了强化学习和束搜索策略。BS算法优化了RL代理的推理过程，支持灵活的目标权重调整和拥堵处理，同时保留了RL的泛化能力和对电路特性及约束的高效处理。

Result: 实验表明，该方法在面积、死区和半周长线长上比标准RL应用提升了约5-85%，同时代理的奖励更高，性能与现有最优技术相当。

Conclusion: 混合方法在保留RL泛化能力的同时，通过BS策略显著提升了布局质量和效率，实验验证了其优于标准RL的性能。

Abstract: The layout of analog ICs requires making complex trade-offs, while addressing
device physics and variability of the circuits. This makes full automation with
learning-based solutions hard to achieve. However, reinforcement learning (RL)
has recently reached significant results, particularly in solving the
floorplanning problem. This paper presents a hybrid method that combines RL
with a beam (BS) strategy. The BS algorithm enhances the agent's inference
process, allowing for the generation of flexible floorplans by accomodating
various objective weightings, and addressing congestion without without the
need for policy retraining or fine-tuning. Moreover, the RL agent's
generalization ability stays intact, along with its efficient handling of
circuit features and constraints. Experimental results show approx. 5-85%
improvement in area, dead space and half-perimeter wire length compared to a
standard RL application, along with higher rewards for the agent. Moreover,
performance and efficiency align closely with those of existing
state-of-the-art techniques.

</details>


### [125] [A Neuro-Symbolic Framework for Sequence Classification with Relational and Temporal Knowledge](https://arxiv.org/abs/2505.05106)
*Luca Salvatore Lorello,Marco Lippi,Stefano Melacci*

Main category: cs.AI

TL;DR: 本文探讨了知识驱动的序列分类问题，挑战在于知识随时间变化且涉及时序关系，实验表明神经符号方法在此新场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号AI框架多假设知识静态不变且忽略时序维度，而本文旨在解决知识随时间变化且需动态应用的更具挑战性的序列分类问题。

Method: 通过多阶段神经符号架构与纯神经架构的对比实验，评估了在新型基准框架下的性能表现。

Result: 实验结果揭示了该新场景的挑战性，并指出了神经符号方法尚未充分探索的短板。

Conclusion: 本研究为未来工作提供了宝贵参考，突显了神经符号方法在动态知识场景下的改进空间。

Abstract: One of the goals of neuro-symbolic artificial intelligence is to exploit
background knowledge to improve the performance of learning tasks. However,
most of the existing frameworks focus on the simplified scenario where
knowledge does not change over time and does not cover the temporal dimension.
In this work we consider the much more challenging problem of knowledge-driven
sequence classification where different portions of knowledge must be employed
at different timesteps, and temporal relations are available. Our experimental
evaluation compares multi-stage neuro-symbolic and neural-only architectures,
and it is conducted on a newly-introduced benchmarking framework. Results
demonstrate the challenging nature of this novel setting, and also highlight
under-explored shortcomings of neuro-symbolic methods, representing a precious
reference for future research.

</details>


### [126] [Multi-agent Embodied AI: Advances and Future Directions](https://arxiv.org/abs/2505.05108)
*Zhaohan Feng,Ruiqi Xue,Lei Yuan,Yang Yu,Ning Ding,Meiqin Liu,Bingzhao Gao,Jian Sun,Gang Wang*

Main category: cs.AI

TL;DR: 这篇论文综述了具身人工智能（Embodied AI）在多智能体系统中的研究现状、关键贡献、挑战及未来方向，填补了该领域缺乏全面调研的空白。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的具身AI需要处理动态开放的复杂环境，并与其他智能体协作，而现有研究多集中于单智能体静态环境，缺乏对多智能体系统的系统性综述。

Method: 通过回顾现有研究，分析关键技术与贡献，并总结动态开放环境中多智能体协作的挑战与解决方案。

Result: 论文梳理了多智能体具身AI的研究进展，揭示了当前简化模型的局限性，并提出了未来发展的关键方向。

Conclusion: 多智能体具身AI是应对现实应用挑战的关键领域，需进一步研究适应机制、实时学习及协作问题解决，推动该领域的创新与发展。

Abstract: Embodied artificial intelligence (Embodied AI) plays a pivotal role in the
application of advanced technologies in the intelligent era, where AI systems
are integrated with physical bodies that enable them to perceive, reason, and
interact with their environments. Through the use of sensors for input and
actuators for action, these systems can learn and adapt based on real-world
feedback, allowing them to perform tasks effectively in dynamic and
unpredictable environments. As techniques such as deep learning (DL),
reinforcement learning (RL), and large language models (LLMs) mature, embodied
AI has become a leading field in both academia and industry, with applications
spanning robotics, healthcare, transportation, and manufacturing. However, most
research has focused on single-agent systems that often assume static, closed
environments, whereas real-world embodied AI must navigate far more complex
scenarios. In such settings, agents must not only interact with their
surroundings but also collaborate with other agents, necessitating
sophisticated mechanisms for adaptation, real-time learning, and collaborative
problem-solving. Despite increasing interest in multi-agent systems, existing
research remains narrow in scope, often relying on simplified models that fail
to capture the full complexity of dynamic, open environments for multi-agent
embodied AI. Moreover, no comprehensive survey has systematically reviewed the
advancements in this area. As embodied AI rapidly evolves, it is crucial to
deepen our understanding of multi-agent embodied AI to address the challenges
presented by real-world applications. To fill this gap and foster further
development in the field, this paper reviews the current state of research,
analyzes key contributions, and identifies challenges and future directions,
providing insights to guide innovation and progress in this field.

</details>


### [127] [Is there a half-life for the success rates of AI agents?](https://arxiv.org/abs/2505.05115)
*Toby Ord*

Main category: cs.AI

TL;DR: 该论文基于Kwa等人（2025年）的实证研究，提出AI代理在长时间任务中的表现可以通过一个简单的数学模型——每分钟恒定失败率——来解释，并由此推导出成功率的指数下降及代理的半衰期特性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解AI代理在长时间任务中表现下降的规律，并通过数学模型量化其成功率，以揭示任务失败的根本原因。

Method: 采用恒定失败率模型分析AI代理在不同时长任务中的表现，并验证模型对实际数据的拟合程度。

Result: 结果表明，恒定失败率模型能很好地解释数据，说明AI代理在长时间任务中的成功率呈指数下降，且每个代理有其独特的半衰期特性。

Conclusion: 结论指出该模型可能揭示了长时间任务失败的原因是子任务的累积性失败，但其在更广泛任务中的适用性仍需进一步研究。

Abstract: Building on the recent empirical work of Kwa et al. (2025), I show that
within their suite of research-engineering tasks the performance of AI agents
on longer-duration tasks can be explained by an extremely simple mathematical
model -- a constant rate of failing during each minute a human would take to do
the task. This implies an exponentially declining success rate with the length
of the task and that each agent could be characterised by its own half-life.
This empirical regularity allows us to estimate the success rate for an agent
at different task lengths. And the fact that this model is a good fit for the
data is suggestive of the underlying causes of failure on longer tasks -- that
they involve increasingly large sets of subtasks where failing any one fails
the task. Whether this model applies more generally on other suites of tasks is
unknown and an important subject for further work.

</details>


### [128] [MARK: Memory Augmented Refinement of Knowledge](https://arxiv.org/abs/2505.05177)
*Anish Ganguli,Prabal Deb,Debleena Banerjee*

Main category: cs.AI

TL;DR: 该论文提出了MARK框架，通过结构化精炼记忆使大型语言模型（LLM）无需重新训练即可持续学习，解决了LLM在领域知识动态更新时适应性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在动态领域知识（如商业需求和实时变化）中适应性不足的问题，弥合领域专家的深刻理解与系统知识之间的差距。

Method: MARK框架通过三个专责代理（残余精炼记忆代理、用户问题精炼记忆代理、LLM响应精炼记忆代理）存储、分析和优化领域知识，利用时间因素（如时效性和频率）提升信息准确性。

Result: MARK减少了LLM的幻觉现象，支持领域特定适应（如医疗、法律），并提升了个性化AI助手的性能。

Conclusion: MARK框架为LLM的持续学习和领域知识动态适应提供了一种高效且可扩展的解决方案。

Abstract: Large Language Models (LLMs) assist in specialized tasks but struggle to
align with evolving domain knowledge without costly fine-tuning. Domain
knowledge consists of: Knowledge: Immutable facts (e.g., 'A stone is solid')
and generally accepted principles (e.g., ethical standards); Refined Memory:
Evolving insights shaped by business needs and real-world changes. However, a
significant gap often exists between a domain expert's deep, nuanced
understanding and the system's domain knowledge, which can hinder accurate
information retrieval and application. Our Memory-Augmented Refinement of
Knowledge (MARK) framework enables LLMs to continuously learn without
retraining by leveraging structured refined memory, inspired by the Society of
Mind. MARK operates through specialized agents, each serving a distinct role:
Residual Refined Memory Agent: Stores and retrieves domain-specific insights to
maintain context over time; User Question Refined Memory Agent: Captures
user-provided facts, abbreviations, and terminology for better comprehension;
LLM Response Refined Memory Agent: Extracts key elements from responses for
refinement and personalization. These agents analyse stored refined memory,
detect patterns, resolve contradictions, and improve response accuracy.
Temporal factors like recency and frequency prioritize relevant information
while discarding outdated insights. MARK enhances LLMs in multiple ways: Ground
Truth Strategy: Reduces hallucinations by establishing a structured reference;
Domain-Specific Adaptation: Essential for fields like healthcare, law, and
manufacturing, where proprietary insights are absent from public datasets;
Personalized AI Assistants: Improves virtual assistants by remembering user
preferences, ensuring coherent responses over time.

</details>


### [129] [Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt](https://arxiv.org/abs/2505.05197)
*Joel Z. Leibo,Alexander Sasha Vezhnevets,William A. Cunningham,Sébastien Krier,Manfred Diaz,Simon Osindero*

Main category: cs.AI

TL;DR: 该论文提出了一种名为'appropriateness framework'的替代方案，以解决现有AI伦理对齐方法过于统一的问题，强调在多元化背景下处理长期分歧。


<details>
  <summary>Details</summary>
Motivation: 由于AI决策对社会的影响日益增加，但现有的'对齐'方法忽视道德多样性，可能导致信任危机和制度不稳定。论文旨在解决这一问题。

Method: 提出'appropriateness framework'，基于冲突理论、文化进化、多智能体系统和制度经济学，包含四个设计原则：上下文接地、社区定制、持续适应和多中心治理。

Result: 论文认为这一框架能将伦理对齐的隐喻从道德统一转向更有效的冲突管理，适应多元社会。

Conclusion: 采用这一框架是理想且紧迫的，能更好地应对道德多样性，提升AI系统的社会接受度和稳定性。

Abstract: Artificial Intelligence (AI) systems are increasingly placed in positions
where their decisions have real consequences, e.g., moderating online spaces,
conducting research, and advising on policy. Ensuring they operate in a safe
and ethically acceptable fashion is thus critical. However, most solutions have
been a form of one-size-fits-all "alignment". We are worried that such systems,
which overlook enduring moral diversity, will spark resistance, erode trust,
and destabilize our institutions. This paper traces the underlying problem to
an often-unstated Axiom of Rational Convergence: the idea that under ideal
conditions, rational agents will converge in the limit of conversation on a
single ethics. Treating that premise as both optional and doubtful, we propose
what we call the appropriateness framework: an alternative approach grounded in
conflict theory, cultural evolution, multi-agent systems, and institutional
economics. The appropriateness framework treats persistent disagreement as the
normal case and designs for it by applying four principles: (1) contextual
grounding, (2) community customization, (3) continual adaptation, and (4)
polycentric governance. We argue here that adopting these design principles is
a good way to shift the main alignment metaphor from moral unification to a
more productive metaphor of conflict management, and that taking this step is
both desirable and urgent.

</details>


### [130] [ChemRxivQuest: A Curated Chemistry Question-Answer Database Extracted from ChemRxiv Preprints](https://arxiv.org/abs/2505.05232)
*Mahmoud Amiri,Thomas Bocklitz*

Main category: cs.AI

TL;DR: ChemRxivQuest是一个高质量的QA数据集，涵盖17个化学子领域的970对问题-答案，用于支持化学领域的NLP研究。


<details>
  <summary>Details</summary>
Motivation: 为了解决化学文献快速扩张导致的领域知识获取效率问题，推动化学NLP的发展。

Method: 采用自动化流程结合OCR、GPT-4o生成QA对及模糊匹配验证答案。

Result: 构建了一个高质量且可追溯的化学QA数据集，支持QA系统开发、搜索引擎优化和LLM微调。

Conclusion: ChemRxivQuest为化学NLP研究、教育和工具开发提供了基础资源，并指出了未来扩展方向。

Abstract: The rapid expansion of chemistry literature poses significant challenges for
researchers seeking to efficiently access domain-specific knowledge. To support
advancements in chemistry-focused natural language processing (NLP), we present
ChemRxivQuest, a curated dataset of 970 high-quality question-answer (QA) pairs
derived from 155 ChemRxiv preprints across 17 subfields of chemistry. Each QA
pair is explicitly linked to its source text segment to ensure traceability and
contextual accuracy. ChemRxivQuest was constructed using an automated pipeline
that combines optical character recognition (OCR), GPT-4o-based QA generation,
and a fuzzy matching technique for answer verification. The dataset emphasizes
conceptual, mechanistic, applied, and experimental questions, enabling
applications in retrieval-based QA systems, search engine development, and
fine-tuning of domain-adapted large language models. We analyze the dataset's
structure, coverage, and limitations, and outline future directions for
expansion and expert validation. ChemRxivQuest provides a foundational resource
for chemistry NLP research, education, and tool development.

</details>


### [131] [Advancing Neural Network Verification through Hierarchical Safety Abstract Interpretation](https://arxiv.org/abs/2505.05235)
*Luca Marzari,Isabella Mastroeni,Alessandro Farinelli*

Main category: cs.AI

TL;DR: 本文提出了一种称为抽象DNN验证的新方法，通过分层结构验证不安全输出，提供了对深度神经网络安全性的更精细分析，同时计算效率与传统二进制验证相当或更高。


<details>
  <summary>Details</summary>
Motivation: 传统的深度神经网络形式验证方法采用二元编码（安全或不安全），无法捕捉模型内部的安全级别差异，导致要求过于严格或宽松。本文旨在解决这一问题。

Method: 利用抽象解释和输出可达集推理，验证不安全输出的分层结构，从而评估多级安全性。

Result: 通过理论和实验分析，证明了该方法能够按抽象安全级别对对抗输入进行排序，并在复杂深度强化学习任务和标准基准测试中验证了其有效性。

Conclusion: 抽象DNN验证为深度神经网络的安全性提供了更细粒度的分析，同时保持了计算效率，为安全评估开辟了新方向。

Abstract: Traditional methods for formal verification (FV) of deep neural networks
(DNNs) are constrained by a binary encoding of safety properties, where a model
is classified as either safe or unsafe (robust or not robust). This binary
encoding fails to capture the nuanced safety levels within a model, often
resulting in either overly restrictive or too permissive requirements. In this
paper, we introduce a novel problem formulation called Abstract
DNN-Verification, which verifies a hierarchical structure of unsafe outputs,
providing a more granular analysis of the safety aspect for a given DNN.
Crucially, by leveraging abstract interpretation and reasoning about output
reachable sets, our approach enables assessing multiple safety levels during
the FV process, requiring the same (in the worst case) or even potentially less
computational effort than the traditional binary verification approach.
Specifically, we demonstrate how this formulation allows rank adversarial
inputs according to their abstract safety level violation, offering a more
detailed evaluation of the model's safety and robustness. Our contributions
include a theoretical exploration of the relationship between our novel
abstract safety formulation and existing approaches that employ abstract
interpretation for robustness verification, complexity analysis of the novel
problem introduced, and an empirical evaluation considering both a complex deep
reinforcement learning task (based on Habitat 3.0) and standard
DNN-Verification benchmarks.

</details>


### [132] [A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods](https://arxiv.org/abs/2505.05396)
*Stefanos Gkikas*

Main category: cs.AI

TL;DR: 该论文研究了自动疼痛评估方法，开发了高性能且适用于临床的计算方法，并探讨了影响疼痛感知的因素，提出了多模态评估流程，取得了先进成果。


<details>
  <summary>Details</summary>
Motivation: 研究目的是从临床理论角度分析疼痛评估，开发适用于实际应用的自动评估方法，并探究影响疼痛感知的多种因素。

Method: 设计了单模态和多模态的自动疼痛评估流程，结合人工智能和生成人工智能技术进行研究。

Result: 提出的方法在实验中表现优异，达到了先进水平，并为人工智能新方法的研究奠定了基础。

Conclusion: 该研究成功开发了高效的自动疼痛评估系统，为临床实践和人工智能领域提供了新思路。

Abstract: From the original abstract:
  This thesis initially aims to study the pain assessment process from a
clinical-theoretical perspective while exploring and examining existing
automatic approaches. Building on this foundation, the primary objective of
this Ph.D. project is to develop innovative computational methods for automatic
pain assessment that achieve high performance and are applicable in real
clinical settings. A primary goal is to thoroughly investigate and assess
significant factors, including demographic elements that impact pain
perception, as recognized in pain research, through a computational standpoint.
Within the limits of the available data in this research area, our goal was to
design, develop, propose, and offer automatic pain assessment pipelines for
unimodal and multimodal configurations that are applicable to the specific
requirements of different scenarios. The studies published in this Ph.D. thesis
showcased the effectiveness of the proposed methods, achieving state-of-the-art
results. Additionally, they paved the way for exploring new approaches in
artificial intelligence, foundation models, and generative artificial
intelligence.

</details>


### [133] [EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework for Mobile Automation](https://arxiv.org/abs/2505.05440)
*Biao Yi,Xavier Hu,Yurun Chen,Shengyu Zhang,Hongxia Yang,Fan Wu,Fei Wu*

Main category: cs.AI

TL;DR: EcoAgent是一个边缘-云协作的多代理框架，旨在解决云代理高延迟和高成本的问题，同时保持高效的任务完成率。


<details>
  <summary>Details</summary>
Motivation: 为了解决基于云的移动代理（如大型语言模型）的高延迟和成本问题，同时避免边缘部署的轻量级模型在复杂任务上的性能损失，作者提出了EcoAgent。

Method: EcoAgent通过云端的规划代理和边缘的执行代理及观察代理实现闭环协作，观察代理通过预理解模块压缩图像以减少令牌使用，规划代理通过反射模块在失败时重新规划。

Result: 在AndroidWorld上的实验表明，EcoAgent在显著减少MLLM令牌消耗的同时，保持了高任务成功率。

Conclusion: EcoAgent提供了一种高效且实用的移动自动化解决方案，平衡了性能和成本。

Abstract: Cloud-based mobile agents powered by (multimodal) large language models
((M)LLMs) offer strong reasoning abilities but suffer from high latency and
cost. While fine-tuned (M)SLMs enable edge deployment, they often lose general
capabilities and struggle with complex tasks. To address this, we propose
EcoAgent, an Edge-Cloud cOllaborative multi-agent framework for mobile
automation. EcoAgent features a closed-loop collaboration among a cloud-based
Planning Agent and two edge-based agents: the Execution Agent for action
execution and the Observation Agent for verifying outcomes. The Observation
Agent uses a Pre-Understanding Module to compress screen images into concise
text, reducing token usage. In case of failure, the Planning Agent retrieves
screen history and replans via a Reflection Module. Experiments on AndroidWorld
show that EcoAgent maintains high task success rates while significantly
reducing MLLM token consumption, enabling efficient and practical mobile
automation.

</details>


### [134] [Conversational Process Model Redesign](https://arxiv.org/abs/2505.05453)
*Nataliia Klievtsova,Timotheus Kampik,Juergen Mangler,Stefanie Rinderle-Ma*

Main category: cs.AI

TL;DR: 该研究探讨了使用大型语言模型（LLM）支持领域专家以交互方式创建和重新设计业务流程模型的可行性，并提出了一个多步骤的对话式流程重新设计（CPD）方法。


<details>
  <summary>Details</summary>
Motivation: 当前研究多关注单次提示的执行和结果评估，而缺乏用户与LLM的持续交互。为了解决这一问题，研究旨在探索LLM在迭代和有效的方式下支持流程模型设计的潜力。

Method: 研究提出了CPD方法，通过自然语言输入接收流程模型和重新设计请求。LLM用于识别流程变更模式、重新表述变更请求以与模式对齐，并应用变更到流程模型。

Result: 评估表明，虽然某些模式难以被LLM和用户理解，但LLM在大多数情况下能很好地处理变更，满足完整性和正确性标准。用户需要辅助以清晰描述变更。

Conclusion: 该研究验证了LLM在交互式流程重新设计中的可行性，同时强调了用户支持和清晰描述变更的重要性。未来工作可进一步优化模式理解和用户交互。

Abstract: With the recent success of large language models (LLMs), the idea of
AI-augmented Business Process Management systems is becoming more feasible. One
of their essential characteristics is the ability to be conversationally
actionable, allowing humans to interact with the LLM effectively to perform
crucial process life cycle tasks such as process model design and redesign.
However, most current research focuses on single-prompt execution and
evaluation of results, rather than on continuous interaction between the user
and the LLM. In this work, we aim to explore the feasibility of using LLMs to
empower domain experts in the creation and redesign of process models in an
iterative and effective way. The proposed conversational process model redesign
(CPD) approach receives as input a process model and a redesign request by the
user in natural language. Instead of just letting the LLM make changes, the LLM
is employed to (a) identify process change patterns from literature, (b)
re-phrase the change request to be aligned with an expected wording for the
identified pattern (i.e., the meaning), and then to (c) apply the meaning of
the change to the process model. This multi-step approach allows for
explainable and reproducible changes. In order to ensure the feasibility of the
CPD approach, and to find out how well the patterns from literature can be
handled by the LLM, we performed an extensive evaluation. The results show that
some patterns are hard to understand by LLMs and by users. Within the scope of
the study, we demonstrated that users need support to describe the changes
clearly. Overall the evaluation shows that the LLMs can handle most changes
well according to a set of completeness and correctness criteria.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [135] [Representing spherical tensors with scalar-based machine-learning models](https://arxiv.org/abs/2505.05404)
*Michelangelo Domina,Filippo Bigi,Paolo Pegolo,Michele Ceriotti*

Main category: physics.chem-ph

TL;DR: 该论文提出了一种新的方法来处理3D点云的旋转对称性问题，通过结合标量函数和小基底张量的乘积来表达等变函数，简化了计算和实现。


<details>
  <summary>Details</summary>
Motivation: 旋转对称性在物理学中至关重要，但传统的等变模型计算复杂且实现繁琐，这促使作者探索一种更高效、更易实现的方法。

Method: 作者提出将等变函数表示为点云坐标的标量函数与具有适当对称性的小基底张量的乘积，并提出了在实用场景下快速、简单且准确的近似表达式。

Result: 虽然该方法缺乏通用逼近性，但在实际应用中表现出高效性和准确性。

Conclusion: 论文通过简化等变函数的表达，为解决3D点云的对称性问题提供了一种计算高效且易实现的新途径。

Abstract: Rotational symmetry plays a central role in physics, providing an elegant
framework to describe how the properties of 3D objects -- from atoms to the
macroscopic scale -- transform under the action of rigid rotations. Equivariant
models of 3D point clouds are able to approximate structure-property relations
in a way that is fully consistent with the structure of the rotation group, by
combining intermediate representations that are themselves spherical tensors.
The symmetry constraints however make this approach computationally demanding
and cumbersome to implement, which motivates increasingly popular unconstrained
architectures that learn approximate symmetries as part of the training
process. In this work, we explore a third route to tackle this learning
problem, where equivariant functions are expressed as the product of a scalar
function of the point cloud coordinates and a small basis of tensors with the
appropriate symmetry. We also propose approximations of the general expressions
that, while lacking universal approximation properties, are fast, simple to
implement, and accurate in practical settings.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [136] [Proceedings The 13th International Workshop on Theorem proving components for Educational software](https://arxiv.org/abs/2505.04677)
*Julien Narboux,Walther Neuper,Pedro Quaresma*

Main category: cs.LO

TL;DR: ThEdu系列旨在通过定理证明技术促进中学数学到STEM教育的过渡，本次研讨会汇集了相关研究论文。


<details>
  <summary>Details</summary>
Motivation: 推动从直觉数学教育到更形式化STEM教育的平滑过渡，并利用定理证明技术作为教学工具。

Method: 举办国际研讨会（ThEdu'24），征集并审核相关研究论文，重点关注自动化推理在教育中的应用。

Result: 接收8篇论文，涵盖自动化推理研究及其在教育中的实际应用。

Conclusion: 该论文集旨在促进定理证明工具的发展，并加强计算机科学家、数学家和教育工作者之间的合作。

Abstract: The ThEdu series pursues the smooth transition from an intuitive way of doing
mathematics at secondary school to a more formal approach to the subject in
STEM education while favoring software support for this transition by
exploiting the power of theorem-proving technologies. What follows is a brief
description of how the present volume contributes to this enterprise. The 13th
International Workshop on Theorem Proving Components for Educational Software
(ThEdu'24), was a satellite event of the CADE29, part of IJCAR 2024, Nancy,
France. ThEdu'24 was a vibrant workshop, with one invited talk by Jeremy Avigad
(Carnegie Mellon University) and 14 submitted talks. An open call for papers
was then issued and attracted 9 submissions. Eight of those submissions have
been accepted by our reviewers. The resulting revised papers are collected in
the present volume. The contributions in this volume are a faithful
representation of the wide spectrum of ThEdu, ranging from those more focused
on the automated deduction research, not losing track of the possible
applications in an educational setting, to those focused on the applications,
in educational settings, of automated deduction tools and methods. We, the
volume editors, hope that this collection of papers will further promote the
development of theorem-proving-based software and that it will allow to improve
the mutual understanding between computer scientists, mathematicians, and
stakeholders in education. While this volume goes to press, the next edition of
the ThEdu workshop is being prepared: ThEdu'25 will be a satellite event of the
30th international Conference on Automated DEduction (CADE-30), July 28th -
August 2nd, 2025, Stuttgart, Germany.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [137] [Cryptogenic stroke and migraine: using probabilistic independence and machine learning to uncover latent sources of disease from the electronic health record](https://arxiv.org/abs/2505.04631)
*Joshua W. Betts,John M. Still,Thomas A. Lasko*

Main category: stat.AP

TL;DR: 该论文通过数据驱动方法从电子健康记录中提取概率独立源，构建了偏头痛患者10年隐源性中风风险预测模型，并识别了关键风险因素。


<details>
  <summary>Details</summary>
Motivation: 偏头痛是一种常见但复杂的神经系统疾病，与隐源性中风风险增加相关，但目前缺乏相关临床指南。本研究旨在填补这一空白。

Method: 利用电子健康记录数据提取概率独立源，构建因果图，并训练随机森林模型预测风险。

Result: 模型表现出良好准确性（ROC 0.771），识别出前10大风险源，揭示药物治疗是降低风险的关键因素，并发现过敏性鼻炎相关因素可能与中风风险相关。

Conclusion: 研究表明数据驱动方法能有效识别偏头痛患者的隐源性中风风险因素，为临床干预提供新方向。

Abstract: Migraine is a common but complex neurological disorder that doubles the
lifetime risk of cryptogenic stroke (CS). However, this relationship remains
poorly characterized, and few clinical guidelines exist to reduce this
associated risk. We therefore propose a data-driven approach to extract
probabilistically-independent sources from electronic health record (EHR) data
and create a 10-year risk-predictive model for CS in migraine patients. These
sources represent external latent variables acting on the causal graph
constructed from the EHR data and approximate root causes of CS in our
population. A random forest model trained on patient expressions of these
sources demonstrated good accuracy (ROC 0.771) and identified the top 10 most
predictive sources of CS in migraine patients. These sources revealed that
pharmacologic interventions were the most important factor in minimizing CS
risk in our population and identified a factor related to allergic rhinitis as
a potential causative source of CS in migraine patients.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [138] [Geometric Fault-Tolerant Neural Network Tracking Control of Unknown Systems on Matrix Lie Groups](https://arxiv.org/abs/2505.04725)
*Robin Chhabra,Farzaneh Abdollahi*

Main category: eess.SY

TL;DR: 文章提出了一种基于几何神经网络的跟踪控制器，用于解决矩阵李群上未知动态、执行器故障和有界扰动下的系统控制问题。通过利用李群的几何特性，该方法避免了参数化奇点，并通过李雅普诺夫方法证明了误差信号的最终有界性。


<details>
  <summary>Details</summary>
Motivation: 现有的控制器在处理矩阵李群上的系统时，常面临参数化奇点和动态未知的挑战。本文旨在开发一种无需显式参数化、能全局优化权重的神经网络控制器。

Method: 利用矩阵李群切丛的左不变性，设计了一种与李群结构兼容的神经网络权重学习规则，并通过李雅普诺夫直接法验证误差信号的最终有界性。

Result: 仿真结果表明，该方法在多智能体系统的分散编队控制中表现有效，特别是在特殊欧几里得群上的应用。

Conclusion: 所提出的几何神经网络控制器能够有效处理矩阵李群上的复杂控制问题，同时避免了传统方法的参数化局限性，具有全局优化潜力。

Abstract: We present a geometric neural network-based tracking controller for systems
evolving on matrix Lie groups under unknown dynamics, actuator faults, and
bounded disturbances. Leveraging the left-invariance of the tangent bundle of
matrix Lie groups, viewed as an embedded submanifold of the vector space
$\R^{N\times N}$, we propose a set of learning rules for neural network weights
that are intrinsically compatible with the Lie group structure and do not
require explicit parameterization. Exploiting the geometric properties of Lie
groups, this approach circumvents parameterization singularities and enables a
global search for optimal weights. The ultimate boundedness of all error
signals -- including the neural network weights, the coordinate-free
configuration error function, and the tracking velocity error -- is established
using Lyapunov's direct method. To validate the effectiveness of the proposed
method, we provide illustrative simulation results for decentralized formation
control of multi-agent systems on the Special Euclidean group.

</details>


### [139] [LAPSO: A Unified Optimization View for Learning-Augmented Power System Operations](https://arxiv.org/abs/2505.05203)
*Wangkun Xu,Zhongda Chu,Fei Teng*

Main category: eess.SY

TL;DR: 论文提出了一种名为LAPSO的整体框架，通过机器学习与模型优化的结合，解决高比例可再生能源下电力系统运行的经济性、稳定性与鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源比例提高，传统模型驱动的电力系统运行方法难以满足经济、稳定和鲁棒的需求，而现有的机器学习方法缺乏与现有方法的系统整合。

Method: 提出LAPSO框架，以优化视角为核心，打破电力系统任务（如预测、运行与控制）的时序隔离，统一机器学习和模型优化目标。

Result: 通过仿真验证了LAPSO在新算法设计（如稳定性约束优化和目标驱动预测）中的有效性，并开发了Python工具包lapso用于实际应用。

Conclusion: LAPSO框架成功整合了机器学习与模型优化，为电力系统运行提供了新的解决方案，并开源了相关工具与数据。

Abstract: With the high penetration of renewables, traditional model-based power system
operation is challenged to deliver economic, stable, and robust decisions.
Machine learning has emerged as a powerful modeling tool for capturing complex
dynamics to address these challenges. However, its separate design often lacks
systematic integration with existing methods. To fill the gap, this paper
proposes a holistic framework of Learning-Augmented Power System Operations
(LAPSO, pronounced as Lap-So). Adopting a native optimization perspective,
LAPSO is centered on the operation stage and aims to break the boundary between
temporally siloed power system tasks, such as forecast, operation and control,
while unifying the objectives of machine learning and model-based optimizations
at both training and inference stages. Systematic analysis and simulations
demonstrate the effectiveness of applying LAPSO in designing new integrated
algorithms, such as stability-constrained optimization (SCO) and
objective-based forecasting (OBF), while enabling end-to-end tracing of
different sources of uncertainties. In addition, a dedicated Python
package-lapso is introduced to automatically augment existing power system
optimization models with learnable components. All code and data are available
at https://github.com/xuwkk/lapso_exp.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [140] [Enhancing Text2Cypher with Schema Filtering](https://arxiv.org/abs/2505.05118)
*Makbule Gulcin Ozsoy*

Main category: cs.DB

TL;DR: 该论文探讨了知识图谱查询语言中自然语言转Cypher查询的优化方法，通过模式过滤减少噪声和计算成本，并验证了其对不同规模模型的效果。


<details>
  <summary>Details</summary>
Motivation: 针对知识图谱查询中复杂模式引入的噪声和计算成本问题，研究如何通过模式过滤优化自然语言到Cypher查询的转换。

Method: 研究比较了多种模式过滤方法，分析其对Token长度、性能和成本的影响。

Result: 结果显示模式过滤有效优化了查询生成，尤其对小模型效果显著；大模型因上下文能力较强受益较少，但仍能降低成本。

Conclusion: 模式过滤在优化Text2Cypher任务中具有实际价值，尤其在小模型和成本控制方面效果显著。

Abstract: Knowledge graphs represent complex data using nodes, relationships, and
properties. Cypher, a powerful query language for graph databases, enables
efficient modeling and querying. Recent advancements in large language models
allow translation of natural language questions into Cypher queries -
Text2Cypher. A common approach is incorporating database schema into prompts.
However, complex schemas can introduce noise, increase hallucinations, and
raise computational costs. Schema filtering addresses these challenges by
including only relevant schema elements, improving query generation while
reducing token costs. This work explores various schema filtering methods for
Text2Cypher task and analyzes their impact on token length, performance, and
cost. Results show that schema filtering effectively optimizes Text2Cypher,
especially for smaller models. Consistent with prior research, we find that
larger models benefit less from schema filtering due to their longer context
capabilities. However, schema filtering remains valuable for both larger and
smaller models in cost reduction.

</details>


### [141] [Text2Cypher: Data Pruning using Hard Example Selection](https://arxiv.org/abs/2505.05122)
*Makbule Gulcin Ozsoy*

Main category: cs.DB

TL;DR: 本文提出了五种困难样本选择技术，用于精简Text2Cypher数据集，旨在减少资源消耗的同时保持或提升性能。实验表明，这些方法可以显著降低训练时间和成本，且对性能影响极小，证明了其成本效益。


<details>
  <summary>Details</summary>
Motivation: 随着数据集规模增大，微调模型的成本也随之上升。因此，需要小型高质量数据集来降低成本，同时保证性能。本文旨在通过困难样本选择技术优化数据集，减少资源消耗。

Method: 提出五种困难样本选择技术，对Text2Cypher数据集进行剪枝，保留关键样本以减少训练成本。

Result: 实验结果显示，这些方法能使训练时间和成本减半，且对性能影响很小。

Conclusion: 困难样本选择是一种成本效益高的解决方案，适用于优化数据库查询语言的微调过程。

Abstract: Database query languages such as SQL for relational databases and Cypher for
graph databases have been widely adopted. Recent advancements in large language
models (LLMs) enable natural language interactions with databases through
models like Text2SQL and Text2Cypher. Fine-tuning these models typically
requires large, diverse datasets containing non-trivial examples. However, as
dataset size increases, the cost of fine-tuning also rises. This makes smaller,
high-quality datasets essential for reducing costs for the same or better
performance. In this paper, we propose five hard-example selection techniques
for pruning the Text2Cypher dataset, aiming to preserve or improve performance
while reducing resource usage. Our results show that these hard-example
selection approaches can halve training time and costs with minimal impact on
performance, and demonstrates that hard-example selection provides a
cost-effective solution.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [142] [Generalization Analysis for Contrastive Representation Learning under Non-IID Settings](https://arxiv.org/abs/2505.04937)
*Nong Minh Hieu,Antoine Ledent*

Main category: stat.ML

TL;DR: 本文针对对比表示学习（CRL）在非独立同分布（non-i.i.d.）设定下的泛化行为提出了理论分析，弥补了现有文献仅关注i.i.d.数据假设的不足。


<details>
  <summary>Details</summary>
Motivation: 由于实际应用中数据点常被重复使用形成元组，现有基于i.i.d.假设的泛化边界不再适用，需为CRL提供更贴合现实的理论框架。

Method: 借鉴U统计量理论，推导了非i.i.d.设定下的泛化边界，并分析了不同函数类（如线性映射和神经网络）的样本复杂度。

Result: 提出了一种新的泛化边界，表明每类所需样本数与可学习特征表示类的覆盖数对数成正比。

Conclusion: 该研究填补了CRL在非i.i.d.数据下的理论空白，并为实际应用中的样本需求提供了理论依据。

Abstract: Contrastive Representation Learning (CRL) has achieved impressive success in
various domains in recent years. Nevertheless, the theoretical understanding of
the generalization behavior of CRL is limited. Moreover, to the best of our
knowledge, the current literature only analyzes generalization bounds under the
assumption that the data tuples used for contrastive learning are independently
and identically distributed. However, in practice, we are often limited to a
fixed pool of reusable labeled data points, making it inevitable to recycle
data across tuples to create sufficiently large datasets. Therefore, the
tuple-wise independence condition imposed by previous works is invalidated. In
this paper, we provide a generalization analysis for the CRL framework under
non-$i.i.d.$ settings that adheres to practice more realistically. Drawing
inspiration from the literature on U-statistics, we derive generalization
bounds which indicate the required number of samples in each class scales as
the logarithm of the covering number of the class of learnable feature
representations associated to each class. Next, we apply our main results to
derive excess risk bounds for common function classes such as linear maps and
neural networks.

</details>


### [143] [Learning Linearized Models from Nonlinear Systems under Initialization Constraints with Finite Data](https://arxiv.org/abs/2505.04954)
*Lei Xin,Baike She,Qi Dou,George Chiu,Shreyas Sundaram*

Main category: stat.ML

TL;DR: 论文研究了从非线性动态系统中学习线性化模型的有限样本误差问题，提出了一种多轨迹确定性数据采集算法和正则化最小二乘算法，并验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统线性系统辨识方法假设动态完全线性，并依赖于单一长轨迹的独立同分布随机输入数据。本研究针对真实动态为非线性且实验初始条件受限的情况，提出了一种更鲁棒的线性化学习方案。

Method: 采用多轨迹确定性数据采集算法结合正则化最小二乘法，为学习的线性化动态提供有限样本误差界。

Result: 误差界限表明，该方法能一致性地学习线性化动态，并揭示了非线性和噪声误差间的权衡。数值实验验证了方法的优越性及单一轨迹线性辨识在非线性存在时的不足。

Conclusion: 该方法为非线性动态系统下的线性化模型学习提供了理论保证和实践验证，解决了传统方法在非线性存在时的局限性。

Abstract: The identification of a linear system model from data has wide applications
in control theory. The existing work that provides finite sample guarantees for
linear system identification typically uses data from a single long system
trajectory under i.i.d. random inputs, and assumes that the underlying dynamics
is truly linear. In contrast, we consider the problem of identifying a
linearized model when the true underlying dynamics is nonlinear, given that
there is a certain constraint on the region where one can initialize the
experiments. We provide a multiple trajectories-based deterministic data
acquisition algorithm followed by a regularized least squares algorithm, and
provide a finite sample error bound on the learned linearized dynamics. Our
error bound shows that one can consistently learn the linearized dynamics, and
demonstrates a trade-off between the error due to nonlinearity and the error
due to noise. We validate our results through numerical experiments, where we
also show the potential insufficiency of linear system identification using a
single trajectory with i.i.d. random inputs, when nonlinearity does exist.

</details>


### [144] [Conformal Prediction with Cellwise Outliers: A Detect-then-Impute Approach](https://arxiv.org/abs/2505.04986)
*Qian Peng,Yajie Bao,Haojie Ren,Zhaojun Wang,Changliang Zou*

Main category: stat.ML

TL;DR: 该论文提出了一种名为‘检测-填补’的保形预测框架（PDI-CP和JDI-CP），用于处理测试特征中的离群值，确保预测区间的覆盖性。理论分析证明JDI-CP在有限样本下达到1-2α覆盖保证，实验验证了其鲁棒性和高效性。


<details>
  <summary>Details</summary>
Motivation: 传统保形预测依赖数据的可交换性，但测试特征中的离群值（如单元离群）会破坏这一性质。

Method: 1. 检测测试特征中的离群值；2. 填补离群值；3. 通过校准集生成可交换特征以计算预测区间。PDI-CP和JDI-CP两种算法分别实现。

Result: JDI-CP在有限样本下实现1-2α覆盖保证，合成与真实数据实验显示算法覆盖鲁棒且效率接近理想基线。

Conclusion: 该框架通过检测-填补流程解决可交换性破坏问题，为存在离群值的场景提供理论保证和实用工具。

Abstract: Conformal prediction is a powerful tool for constructing prediction intervals
for black-box models, providing a finite sample coverage guarantee for
exchangeable data. However, this exchangeability is compromised when some
entries of the test feature are contaminated, such as in the case of cellwise
outliers. To address this issue, this paper introduces a novel framework called
detect-then-impute conformal prediction. This framework first employs an
outlier detection procedure on the test feature and then utilizes an imputation
method to fill in those cells identified as outliers. To quantify the
uncertainty in the processed test feature, we adaptively apply the detection
and imputation procedures to the calibration set, thereby constructing
exchangeable features for the conformal prediction interval of the test label.
We develop two practical algorithms, PDI-CP and JDI-CP, and provide a
distribution-free coverage analysis under some commonly used detection and
imputation procedures. Notably, JDI-CP achieves a finite sample $1-2\alpha$
coverage guarantee. Numerical experiments on both synthetic and real datasets
demonstrate that our proposed algorithms exhibit robust coverage properties and
comparable efficiency to the oracle baseline.

</details>


### [145] [Boosting Statistic Learning with Synthetic Data from Pretrained Large Models](https://arxiv.org/abs/2505.04992)
*Jialong Jiang,Wenkang Hu,Jian Huang,Yuling Jiao,Xu Liu*

Main category: stat.ML

TL;DR: 论文提出了一种端到端框架，通过领域特定统计方法生成并筛选合成数据，选择性整合高质量样本以提升预测性能。实验表明该方法在多场景下均有效，但生成模型的数据增强能力仍有局限性。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如Stable Diffusion）能快速生成大量合成数据，但只有部分数据能有效提升预测模型性能。需要通过系统方法筛选高质量数据以实现有效增强。

Method: 提出了一种端到端框架，结合领域特定统计方法生成合成数据，并通过系统性过滤机制选择性地整合高质量样本。

Result: 实验证明该框架在不同设置下 consistently 提升了预测性能，但生成模型中能有效改善性能的合成数据比例有限。

Conclusion: 合成数据增强具备潜力，但需依赖高质量筛选机制，且生成模型的数据增强能力存在固有局限性。

Abstract: The rapid advancement of generative models, such as Stable Diffusion, raises
a key question: how can synthetic data from these models enhance predictive
modeling? While they can generate vast amounts of datasets, only a subset
meaningfully improves performance. We propose a novel end-to-end framework that
generates and systematically filters synthetic data through domain-specific
statistical methods, selectively integrating high-quality samples for effective
augmentation. Our experiments demonstrate consistent improvements in predictive
performance across various settings, highlighting the potential of our
framework while underscoring the inherent limitations of generative models for
data augmentation. Despite the ability to produce large volumes of synthetic
data, the proportion that effectively improves model performance is limited.

</details>


### [146] [A Two-Sample Test of Text Generation Similarity](https://arxiv.org/abs/2505.05269)
*Jingbin Xu,Chen Qian,Meimei Liu,Feng Guo*

Main category: stat.ML

TL;DR: 本文提出了一种新颖的两样本文本测试方法，通过比较文档的熵来评估两组文档的相似性，使用基于神经网络的语言模型估计熵，并通过数据分割和p值整合增强测试功效。理论与实验均验证了方法的有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 随着数字化文本数据的激增，需要可靠的推断方法来比较两组文档的相似性，尤其是在文档生成概率映射是否相同的假设下。

Method: 提出基于熵比较的两样本文本测试，利用神经网络语言模型估计熵，采用估计-推断框架推导检验统计量，并通过多重数据分割策略提升检验功效。

Result: 理论证明检验统计量在温和条件下渐近服从正态分布，模拟实验和真实数据验证了方法在控制第一类错误率的同时具有更高的检验功效。

Conclusion: 该方法为文档类差异检测提供了新颖解决方案，尤其适用于大规模文本信息至关重要的领域。

Abstract: The surge in digitized text data requires reliable inferential methods on
observed textual patterns. This article proposes a novel two-sample text test
for comparing similarity between two groups of documents. The hypothesis is
whether the probabilistic mapping generating the textual data is identical
across two groups of documents. The proposed test aims to assess text
similarity by comparing the entropy of the documents. Entropy is estimated
using neural network-based language models. The test statistic is derived from
an estimation-and-inference framework, where the entropy is first approximated
using an estimation set, followed by inference on the remaining data set. We
showed theoretically that under mild conditions, the test statistic
asymptotically follows a normal distribution. A multiple data-splitting
strategy is proposed to enhance test power, which combines p-values into a
unified decision. Various simulation studies and a real data example
demonstrated that the proposed two-sample text test maintains the nominal Type
one error rate while offering greater power compared to existing methods. The
proposed method provides a novel solution to assert differences in document
classes, particularly in fields where large-scale textual information is
crucial.

</details>


### [147] [A Connection Between Learning to Reject and Bhattacharyya Divergences](https://arxiv.org/abs/2505.05273)
*Alexander Soen*

Main category: stat.ML

TL;DR: 本文探讨了一种通过学习联合理想分布来优化模型拒绝预测的方法，并比较了通过Bhattacharyya散度和KL散度进行拒绝的差异。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过学习联合理想分布来改进模型的拒绝预测能力，以减少错误预测的风险。

Method: 通过学习输入和标签的联合理想分布，并与真实分布进行比较，通过不同的统计散度（如Bhattacharyya散度和KL散度）来设定拒绝阈值。

Result: 发现当使用对数损失的变体时，通过Bhattacharyya散度进行拒绝比经典的Chow规则（基于KL散度）更为保守。

Conclusion: Bhattacharyya散度作为拒绝标准比KL散度更温和，为模型拒绝预测提供了一种新的选择。

Abstract: Learning to reject provide a learning paradigm which allows for our models to
abstain from making predictions. One way to learn the rejector is to learn an
ideal marginal distribution (w.r.t. the input domain) - which characterizes a
hypothetical best marginal distribution - and compares it to the true marginal
distribution via a density ratio. In this paper, we consider learning a joint
ideal distribution over both inputs and labels; and develop a link between
rejection and thresholding different statistical divergences. We further find
that when one considers a variant of the log-loss, the rejector obtained by
considering the joint ideal distribution corresponds to the thresholding of the
skewed Bhattacharyya divergence between class-probabilities. This is in
contrast to the marginal case - that is equivalent to a typical
characterization of optimal rejection, Chow's Rule - which corresponds to a
thresholding of the Kullback-Leibler divergence. In general, we find that
rejecting via a Bhattacharyya divergence is less aggressive than Chow's Rule.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [148] [Community and hyperedge inference in multiple hypergraphs](https://arxiv.org/abs/2505.04967)
*Li Ni,Ziqi Deng,Lin Mu,Lei Zhang,Wenjian Luo,Yiwen Zhang*

Main category: cs.SI

TL;DR: 本文提出了一种基于随机块模型的超图分析方法，用于整合多个超图中的信息以揭示潜在的高阶结构，并在社区检测、超边预测等任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的高阶系统（如生物或社交网络）存在复杂的交互关系，通过超边表示这些高阶交互可以更好地建模。然而，这些系统通常涉及多个超图之间的关联，当前方法未能充分利用这些关联信息。本文旨在开发一种模型，整合多个超图的信息以提升对系统结构的理解。

Method: 提出了一种基于随机块模型的框架，引入超边内部度量化节点对超边形成的贡献，从而建模超图中的偏好连接现象。该模型支持社区挖掘、超边预测以及超图间边的推断。

Result: 实验表明，模型在社区检测、超边预测和超图间边预测任务中表现优异，且适用于不同类型超图的分析，即使在没有超图间边的情况下也能分析单个超图。

Conclusion: 该模型为分析多个超图提供了实用且灵活的工具，显著提升了对现实世界高阶系统组织的理解能力。

Abstract: Hypergraphs, capable of representing high-order interactions via hyperedges,
have become a powerful tool for modeling real-world biological and social
systems. Inherent relationships within these real-world systems, such as the
encoding relationship between genes and their protein products, drive the
establishment of interconnections between multiple hypergraphs. Here, we
demonstrate how to utilize those interconnections between multiple hypergraphs
to synthesize integrated information from multiple higher-order systems,
thereby enhancing understanding of underlying structures. We propose a model
based on the stochastic block model, which integrates information from multiple
hypergraphs to reveal latent high-order structures. Real-world hyperedges
exhibit preferential attachment, where certain nodes dominate hyperedge
formation. To characterize this phenomenon, our model introduces hyperedge
internal degree to quantify nodes' contributions to hyperedge formation. This
model is capable of mining communities, predicting missing hyperedges of
arbitrary sizes within hypergraphs, and inferring inter-hypergraph edges
between hypergraphs. We apply our model to high-order datasets to evaluate its
performance. Experimental results demonstrate strong performance of our model
in community detection, hyperedge prediction, and inter-hypergraph edge
prediction tasks. Moreover, we show that our model enables analysis of multiple
hypergraphs of different types and supports the analysis of a single hypergraph
in the absence of inter-hypergraph edges. Our work provides a practical and
flexible tool for analyzing multiple hypergraphs, greatly advancing the
understanding of the organization in real-world high-order systems.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [149] [Local linear Fréchet curve regression in manifolds](https://arxiv.org/abs/2505.05168)
*M. D. Ruiz-Medina,A. Torres--Signes*

Main category: math.ST

TL;DR: 该论文提出了在时间相关的双变量曲线数据中，通过外在和内在方法解决Fréchet条件均值的局部线性逼近问题，并验证了其预测性能。


<details>
  <summary>Details</summary>
Motivation: 处理在流形中评估的时间相关双变量曲线数据的Fréchet回归问题，提供局部线性逼近的解决方案。

Method: 采用外在线性Fréchet回归和加权Fréchet均值的内在线性回归，并通过仿真和真实数据验证性能。

Result: 证明了内在局部逼近的渐近最优性，并通过仿真和NASA MAGSAT卫星数据验证了预测方法的有效性。

Conclusion: 该研究为处理流形上的曲线数据提供了有效的局部线性预测方法，并展示了实际应用潜力。

Abstract: Global Fr\'echet functional regression has been recently addressed from time
correlated bivariate curve data evaluated in a manifold (see Torres et al.
2025). For this type of curve data sets, the present paper solves the problem
of local linear approximation of the Fr\'echet conditional mean in an extrinsic
and intrinsic way. The extrinsic local linear Fr\'echet functional regression
predictor is obtained in the time varying tangent space by projection into an
orthornormal basis of the ambient Hilbert space. The conditions assumed ensure
the existence and uniqueness of this predictor, and its computation via
exponential and logarithmic maps. A weighted Fr\'echet mean approach is adopted
in the computation of an intrinsic local linear Fr\'echet functional regression
predictor. The asymptotic optimality of this intrinsic local approximation is
also proved. The performance of the empirical version of both, extrinsic and
intrinsic functional predictors, and of a Nadaraya-Watson type Fr\'echet curve
predictor is illustrated in the simulation study undertaken. The finite-sample
size properties are also tested in a real-data application via
cross-validation. Specifically, functional prediction of the magnetic vector
field from the time-varying geocentric latitude and longitude of the satellite
NASA's MAGSAT spacecraft is addressed.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [150] [ICNN-enhanced 2SP: Leveraging input convex neural networks for solving two-stage stochastic programming](https://arxiv.org/abs/2505.05261)
*Yu Liu,Fabricio Oliveira*

Main category: math.OC

TL;DR: 论文提出了一种基于输入凸神经网络（ICNN）的两阶段随机规划方法，通过利用线性规划（LP）的可表示性，替代了传统混合整数规划（MIP）方法，显著提高了计算效率并保持了求解质量。


<details>
  <summary>Details</summary>
Motivation: 两阶段随机规划（2SP）在不确定性决策建模中有广泛应用，但传统方法因混合整数规划（MIP）的计算复杂性导致可扩展性受限。学习型方法如Neur2SP虽然采用神经网络作为替代函数，但仍依赖计算密集的MIP。

Method: 提出ICNN-enhanced 2SP方法，利用输入凸神经网络（ICNN）的架构强制凸性，通过线性规划（LP）实现精确推理，避免了传统MIP中所需的整数变量。

Result: 实验表明，ICNN训练时间略长但验证精度与MIP相当，且在多个基准问题中，ICNN-enhanced 2SP的求解速度显著快于MIP方法，问题规模越大优势越明显，最高可达100倍加速。

Conclusion: ICNN-enhanced 2SP是一种高效且质量稳定的替代方案，特别适用于大规模两阶段随机规划问题，解决了传统MIP方法的计算效率瓶颈。

Abstract: Two-stage stochastic programming (2SP) offers a basic framework for modelling
decision-making under uncertainty, yet scalability remains a challenge due to
the computational complexity of recourse function evaluation. Existing
learning-based methods like Neural Two-Stage Stochastic Programming (Neur2SP)
employ neural networks (NNs) as recourse function surrogates but rely on
computationally intensive mixed-integer programming (MIP) formulations. We
propose ICNN-enhanced 2SP, a method that leverages Input Convex Neural Networks
(ICNNs) to exploit linear programming (LP) representability in convex 2SP
problems. By architecturally enforcing convexity and enabling exact inference
through LP, our approach eliminates the need for integer variables inherent to
the conventional MIP-based formulation while retaining an exact embedding of
the ICNN surrogate within the 2SP framework. This results in a more
computationally efficient alternative that maintains solution quality.
Comprehensive experiments reveal that ICNNs incur only marginally longer
training times while achieving validation accuracy on par with their MIP-based
counterparts. Across benchmark problems, ICNN-enhanced 2SP often exhibits
considerably faster solution times than the MIP-based formulations while
preserving solution quality, with these advantages becoming significantly more
pronounced as problem scale increases. For the most challenging instances, the
method achieves speedups of up to 100$\times$ and solution quality superior to
MIP-based formulations.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [151] [Error Analysis of Deep PDE Solvers for Option Pricing](https://arxiv.org/abs/2505.05121)
*Jasper Rou*

Main category: q-fin.CP

TL;DR: 论文研究了深度学习PDE求解器在期权定价中的实践表现，通过对比实验评估了Deep Galerkin Method和TDGF方法在Black-Scholes及Heston模型下的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习PDE求解器在期权定价中提供了快速解方，但其实证和量化精度尚未充分理解，影响了其实际应用，因此本文旨在探讨这些方法在实际中的可用性。

Method: 通过Black-Scholes和Heston模型的对比实验，评估了Deep Galerkin Method与时序深度梯度流方法（TDGF）的性能，分析了采样阶段数、样本量、网络层数、每层节点数（TDGF还包括离散化方案阶数和时间步数）对收敛速度和训练时间的影响。

Result: 实验结果提供了Deep Galerkin Method和TDGF在不同参数设置下的收敛率和训练时间的数据，为实际应用中的选择提供了依据。

Conclusion: 研究表明深度学习PDE求解器在期权定价中具有应用潜力，但其性能受多种参数影响，需根据实际场景选择适合的方法。

Abstract: Option pricing often requires solving partial differential equations (PDEs).
Although deep learning-based PDE solvers have recently emerged as quick
solutions to this problem, their empirical and quantitative accuracy remain not
well understood, hindering their real-world applicability. In this research,
our aim is to offer actionable insights into the utility of deep PDE solvers
for practical option pricing implementation. Through comparative experiments in
both the Black--Scholes and the Heston model, we assess the empirical
performance of two neural network algorithms to solve PDEs: the Deep Galerkin
Method and the Time Deep Gradient Flow method (TDGF). We determine their
empirical convergence rates and training time as functions of (i) the number of
sampling stages, (ii) the number of samples, (iii) the number of layers, and
(iv) the number of nodes per layer. For the TDGF, we also consider the order of
the discretization scheme and the number of time steps.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [152] [Exploring Zero-Shot App Review Classification with ChatGPT: Challenges and Potential](https://arxiv.org/abs/2505.04759)
*Mohit Chaudhary,Chirag Jain,Preethu Rose Anish*

Main category: cs.SE

TL;DR: 该论文研究了如何利用ChatGPT的零样本学习能力对应用评论进行分类，结果显示其F1分数达到0.842，表现稳健。


<details>
  <summary>Details</summary>
Motivation: 应用评论是用户反馈的重要来源，但传统的分类方法需要大量领域特定的数据集，成本高且耗时。研究探索了ChatGPT在零样本学习下的潜力，以更高效地分类评论。

Method: 使用ChatGPT对1,880条手动标注的评论进行分类，测试其在功能需求、非功能需求、两者兼具或无关四类中的表现，并分析评论可读性和长度对结果的影响。

Result: ChatGPT的F1分数为0.842，表现优异，但某些类别易被错误分类。评论的可读性和长度对分类准确性有影响。

Conclusion: 零样本学习的ChatGPT能有效分类应用评论，为开发决策提供支持，但仍需优化对特定评论类别的处理。

Abstract: App reviews are a critical source of user feedback, offering valuable
insights into an app's performance, features, usability, and overall user
experience. Effectively analyzing these reviews is essential for guiding app
development, prioritizing feature updates, and enhancing user satisfaction.
Classifying reviews into functional and non-functional requirements play a
pivotal role in distinguishing feedback related to specific app features
(functional requirements) from feedback concerning broader quality attributes,
such as performance, usability, and reliability (non-functional requirements).
Both categories are integral to informed development decisions. Traditional
approaches to classifying app reviews are hindered by the need for large,
domain-specific datasets, which are often costly and time-consuming to curate.
This study explores the potential of zero-shot learning with ChatGPT for
classifying app reviews into four categories: functional requirement,
non-functional requirement, both, or neither. We evaluate ChatGPT's performance
on a benchmark dataset of 1,880 manually annotated reviews from ten diverse
apps spanning multiple domains. Our findings demonstrate that ChatGPT achieves
a robust F1 score of 0.842 in review classification, despite certain challenges
and limitations. Additionally, we examine how factors such as review
readability and length impact classification accuracy and conduct a manual
analysis to identify review categories more prone to misclassification.

</details>


### [153] [PR2: Peephole Raw Pointer Rewriting with LLMs for Translating C to Safer Rust](https://arxiv.org/abs/2505.04852)
*Yifei Gao,Chengpeng Wang,Pengxiang Huang,Xuwei Liu,Mingwei Zheng,Xiangyu Zhang*

Main category: cs.SE

TL;DR: 提出一种名为PR2的技术，通过提升C到Rust转换中的指针安全性，减少unsafe代码的使用。


<details>
  <summary>Details</summary>
Motivation: 提升从C转换到Rust代码的内存安全性，减少对不安全构造（如原始指针）的依赖。

Method: 采用基于决策树的提示技术和代码变更分析，将原始指针提升为适当的Rust数据结构。

Result: 在28个实际C项目中，PR2成功消除了13.22%的本地原始指针，平均每个项目转换耗时5.44小时，成本1.46美元。

Conclusion: PR2有效提升了转换后Rust代码的安全性，且成本可控。

Abstract: There has been a growing interest in translating C code to Rust due to Rust's
robust memory and thread safety guarantees. Tools such as C2RUST enable
syntax-guided transpilation from C to semantically equivalent Rust code.
However, the resulting Rust programs often rely heavily on unsafe
constructs--particularly raw pointers--which undermines Rust's safety
guarantees. This paper aims to improve the memory safety of Rust programs
generated by C2RUST by eliminating raw pointers. Specifically, we propose a
peephole raw pointer rewriting technique that lifts raw pointers in individual
functions to appropriate Rust data structures. Technically, PR2 employs
decision-tree-based prompting to guide the pointer lifting process.
Additionally, it leverages code change analysis to guide the repair of errors
introduced during rewriting, effectively addressing errors encountered during
compilation and test case execution. We implement PR2 as a prototype and
evaluate it using gpt-4o-mini on 28 real-world C projects. The results show
that PR2 successfully eliminates 13.22% of local raw pointers across these
projects, significantly enhancing the safety of the translated Rust code. On
average, PR2 completes the transformation of a project in 5.44 hours, at an
average cost of $1.46.

</details>


### [154] [Software Development Life Cycle Perspective: A Survey of Benchmarks for CodeLLMs and Agents](https://arxiv.org/abs/2505.05283)
*Kaixin Wang,Tianlin Li,Xiaoyu Zhang,Chong Wang,Weisong Sun,Yang Liu,Bin Shi*

Main category: cs.SE

TL;DR: 该论文综述了181个针对代码大语言模型（CodeLLMs）和智能体的基准测试，发现这些测试在软件开发生命周期中分布不均，主要集中在开发阶段（60%），而需求工程（5%）和软件设计（3%）阶段覆盖较少。Python是基准测试中最主要的编程语言。论文还指出了当前研究的挑战，并提出了未来发展方向，旨在缩小CodeLLMs和智能体在理论与实际应用间的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管CodeLLMs和智能体在软件工程任务中展现出强大潜力，但缺乏对其基准测试的全面综述。因此，本文旨在填补这一空白，通过系统分析现有基准测试，为研究者和开发者提供参考。

Method: 本文从461篇相关论文中筛选出181个基准测试，并对它们在软件开发生命周期（SDLC）各阶段的覆盖情况进行分析，重点关注编程语言分布和任务类型。

Result: 研究发现基准测试在SDLC各阶段分布不均，开发阶段占比60%，而需求工程和软件设计阶段仅占5%和3%。Python是最常用的编程语言。

Conclusion: 当前基准测试存在明显的覆盖不平衡问题，未来需更多关注需求工程和软件设计阶段。同时，应致力于解决理论与实际应用间的差距，推动CodeLLMs和智能体的进一步发展。

Abstract: Code large language models (CodeLLMs) and agents have shown great promise in
tackling complex software engineering tasks.Compared to traditional software
engineering methods, CodeLLMs and agents offer stronger abilities, and can
flexibly process inputs and outputs in both natural and code. Benchmarking
plays a crucial role in evaluating the capabilities of CodeLLMs and agents,
guiding their development and deployment. However, despite their growing
significance, there remains a lack of comprehensive reviews of benchmarks for
CodeLLMs and agents. To bridge this gap, this paper provides a comprehensive
review of existing benchmarks for CodeLLMs and agents, studying and analyzing
181 benchmarks from 461 relevant papers, covering the different phases of the
software development life cycle (SDLC). Our findings reveal a notable imbalance
in the coverage of current benchmarks, with approximately 60% focused on the
software development phase in SDLC, while requirements engineering and software
design phases receive minimal attention at only 5% and 3%, respectively.
Additionally, Python emerges as the dominant programming language across the
reviewed benchmarks. Finally, this paper highlights the challenges of current
research and proposes future directions, aiming to narrow the gap between the
theoretical capabilities of CodeLLMs and agents and their application in
real-world scenarios.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [155] [A Proposal for Evaluating the Operational Risk for ChatBots based on Large Language Models](https://arxiv.org/abs/2505.04784)
*Pedro Pinacho-Davidson,Fernando Gutierrez,Pablo Zapata,Rodolfo Vergara,Pablo Aqueveque*

Main category: cs.CR

TL;DR: 论文提出了一种新型风险评估指标，用于评估生成式AI和大型语言模型聊天机器人的多维风险，涵盖服务提供商、用户和第三方，并通过开源工具Garak验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和大型语言模型的发展，聊天机器人带来的人机交互能力提升的同时，也引入了更多传统网络安全之外的运营风险，需要多维度的风险评估方法。

Method: 研究提出了一种评估指标，结合技术复杂性和上下文因素（如目标行业、用户年龄段等），并利用开源框架Garak进行验证和增强，测试多种威胁向量。

Result: 通过检索增强生成（RAG）聊天机器人的场景验证，展示了该指标如何指导短期风险缓解和长期模型设计改进。

Conclusion: 多维度风险评估对于实现安全可靠的AI驱动对话系统至关重要。

Abstract: The emergence of Generative AI (Gen AI) and Large Language Models (LLMs) has
enabled more advanced chatbots capable of human-like interactions. However,
these conversational agents introduce a broader set of operational risks that
extend beyond traditional cybersecurity considerations. In this work, we
propose a novel, instrumented risk-assessment metric that simultaneously
evaluates potential threats to three key stakeholders: the service-providing
organization, end users, and third parties. Our approach incorporates the
technical complexity required to induce erroneous behaviors in the
chatbot--ranging from non-induced failures to advanced prompt-injection
attacks--as well as contextual factors such as the target industry, user age
range, and vulnerability severity. To validate our metric, we leverage Garak,
an open-source framework for LLM vulnerability testing. We further enhance
Garak to capture a variety of threat vectors (e.g., misinformation, code
hallucinations, social engineering, and malicious code generation). Our
methodology is demonstrated in a scenario involving chatbots that employ
retrieval-augmented generation (RAG), showing how the aggregated risk scores
guide both short-term mitigation and longer-term improvements in model design
and deployment. The results underscore the importance of multi-dimensional risk
assessments in operationalizing secure, reliable AI-driven conversational
systems.

</details>


### [156] [Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs](https://arxiv.org/abs/2505.04806)
*Chetan Pathade*

Main category: cs.CR

TL;DR: 对大型语言模型（LLM）对抗攻击的系统研究，包括分类1400多个对抗性提示，分析其通用性和构建逻辑，并提出了分层缓解策略及混合红队与沙盒的安全建议。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM功能强大，但仍易受对抗攻击（如提示注入和越狱）影响，导致对齐保护失效。本文旨在系统研究这些攻击策略，并探索有效的防御方法。

Method: 通过分类1400多个对抗性提示，测试其在GPT-4、Claude 2、Mistral 7B和Vicuna等模型上的成功率，分析通用性和构建逻辑。

Result: 发现不同攻击策略对模型的成功率和通用性存在显著差异，并提出分层缓解措施和混合红队与沙盒的安全建议。

Conclusion: 需要多层次的防御策略和持续的红队测试来提升LLM的安全性，对抗越狱攻击。

Abstract: Large Language Models (LLMs) are increasingly integrated into consumer and
enterprise applications. Despite their capabilities, they remain susceptible to
adversarial attacks such as prompt injection and jailbreaks that override
alignment safeguards. This paper provides a systematic investigation of
jailbreak strategies against various state-of-the-art LLMs. We categorize over
1,400 adversarial prompts, analyze their success against GPT-4, Claude 2,
Mistral 7B, and Vicuna, and examine their generalizability and construction
logic. We further propose layered mitigation strategies and recommend a hybrid
red-teaming and sandboxing approach for robust LLM security.

</details>


### [157] [ChainMarks: Securing DNN Watermark with Cryptographic Chain](https://arxiv.org/abs/2505.04977)
*Brian Choi,Shu Wang,Isabelle Choi,Kun Sun*

Main category: cs.CR

TL;DR: 本文提出了一种名为ChainMarks的安全DNN水印方案，通过引入加密链生成强健的水印，并采用两阶段蒙特卡洛方法验证水印存在，提升了水印的鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 随着DNN模型的广泛应用，现有水印技术易受移除和模糊攻击，且水印存在判定标准模糊。为应对这些问题，本文提出ChainMarks方案。

Method: ChainMarks通过哈希函数生成触发输入作为水印数据集，结合模型所有者的数字签名生成目标标签，并通过两阶段蒙特卡洛方法验证水印。

Result: 实验表明，ChainMarks在相同水印准确率下，提供了更高的水印存在概率保证，且具有更强的鲁棒性和安全性。

Conclusion: ChainMarks为DNN模型提供了更安全、更可靠的水印保护方案。

Abstract: With the widespread deployment of deep neural network (DNN) models, dynamic
watermarking techniques are being used to protect the intellectual property of
model owners. However, recent studies have shown that existing watermarking
schemes are vulnerable to watermark removal and ambiguity attacks. Besides, the
vague criteria for determining watermark presence further increase the
likelihood of such attacks. In this paper, we propose a secure DNN watermarking
scheme named ChainMarks, which generates secure and robust watermarks by
introducing a cryptographic chain into the trigger inputs and utilizes a
two-phase Monte Carlo method for determining watermark presence. First,
ChainMarks generates trigger inputs as a watermark dataset by repeatedly
applying a hash function over a secret key, where the target labels associated
with trigger inputs are generated from the digital signature of model owner.
Then, the watermarked model is produced by training a DNN over both the
original and watermark datasets. To verify watermarks, we compare the predicted
labels of trigger inputs with the target labels and determine ownership with a
more accurate decision threshold that considers the classification probability
of specific models. Experimental results show that ChainMarks exhibits higher
levels of robustness and security compared to state-of-the-art watermarking
schemes. With a better marginal utility, ChainMarks provides a higher
probability guarantee of watermark presence in DNN models with the same level
of watermark accuracy.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [158] [Toward Holistic Evaluation of Recommender Systems Powered by Generative Models](https://arxiv.org/abs/2504.06667)
*Yashar Deldjoo,Nikhil Mehta,Maheswaran Sathiamoorthy,Shuai Zhang,Pablo Castells,Julian McAuley*

Main category: cs.IR

TL;DR: 本文关注基于生成模型的推荐系统（Gen-RecSys）的评估挑战，提出两类风险：生成输出加剧的现有问题（如偏见、隐私）和全新风险（如项目幻觉、矛盾解释），并提出了一种包含情景评估和多指标检测的全面评估方法。


<details>
  <summary>Details</summary>
Motivation: 传统的推荐系统评估指标无法充分衡量生成模型带来的新风险，如内容真实性、安全性和用户意图对齐。本文旨在填补这一空白，提供一个全面的评估框架，以确保Gen-RecSys的个性化和负责任部署。

Method: 论文首先将Gen-RecSys的评估挑战分为两类：生成输出加剧的现有问题和全新风险。随后提出一种综合评估方法，结合情景评估和多项指标检查（相关性、事实依据、偏见检测、政策合规性）。

Result: 提出的评估框架能够更全面地检测Gen-RecSys的风险，包括内容真实性、安全性和用户意图对齐，为研究人员和实践者提供了明确的评估指导。

Conclusion: 通过情景评估和多指标检测，本文的评估框架为Gen-RecSys的负责任部署提供了实用工具，同时强调了在高效个性化与风险控制之间找到平衡的重要性。

Abstract: Recommender systems powered by generative models (Gen-RecSys) extend beyond
classical item ranking by producing open-ended content, which simultaneously
unlocks richer user experiences and introduces new risks. On one hand, these
systems can enhance personalization and appeal through dynamic explanations and
multi-turn dialogues. On the other hand, they might venture into unknown
territory-hallucinating nonexistent items, amplifying bias, or leaking private
information. Traditional accuracy metrics cannot fully capture these
challenges, as they fail to measure factual correctness, content safety, or
alignment with user intent.
  This paper makes two main contributions. First, we categorize the evaluation
challenges of Gen-RecSys into two groups: (i) existing concerns that are
exacerbated by generative outputs (e.g., bias, privacy) and (ii) entirely new
risks (e.g., item hallucinations, contradictory explanations). Second, we
propose a holistic evaluation approach that includes scenario-based assessments
and multi-metric checks-incorporating relevance, factual grounding, bias
detection, and policy compliance. Our goal is to provide a guiding framework so
researchers and practitioners can thoroughly assess Gen-RecSys, ensuring
effective personalization and responsible deployment.

</details>


### [159] [QBD-RankedDataGen: Generating Custom Ranked Datasets for Improving Query-By-Document Search Using LLM-Reranking with Reduced Human Effort](https://arxiv.org/abs/2505.04732)
*Sriram Gopalakrishnan,Sunandita Patra*

Main category: cs.IR

TL;DR: 论文提出了一种生成自定义Query-By-Document（QBD）搜索数据集的方法QBD-RankedDatagen，并比较了多种方法，利用大语言模型（LLMs）优化检索性能，降低了领域数据集创建的人力成本。


<details>
  <summary>Details</summary>
Motivation: 现有的QBD检索方法依赖领域特定数据集，但创建这类数据集成本高且耗时。作者旨在通过自动化方法和LLMs的引入，减少人工投入，同时保持检索模型的性能。

Method: 提出QBD-RankedDatagen流程，利用LLMs结合领域专家输入生成文档评分、排名及解释。基于TREC的QBD数据集进行方法评估，并优化BM25模型的参数。

Result: 方法显著降低了领域数据集创建的人力成本，同时通过专家知识调优检索模型，验证了其在TREC数据集上的有效性。

Conclusion: 通过结合LLMs和自动化流程，论文为定制化QBD检索任务提供了一种高效且成本较低的解决方案，适用于专利、法律等专业领域。

Abstract: The Query-By-Document (QBD) problem is an information retrieval problem where
the query is a document, and the retrieved candidates are documents that match
the query document, often in a domain or query specific manner. This can be
crucial for tasks such as patent matching, legal or compliance case retrieval,
and academic literature review. Existing retrieval methods, including keyword
search and document embeddings, can be optimized with domain-specific datasets
to improve QBD search performance. However, creating these domain-specific
datasets is often costly and time-consuming. Our work introduces a process to
generate custom QBD-search datasets and compares a set of methods to use in
this problem, which we refer to as QBD-RankedDatagen. We provide a comparative
analysis of our proposed methods in terms of cost, speed, and the human
interface with the domain experts. The methods we compare leverage Large
Language Models (LLMs) which can incorporate domain expert input to produce
document scores and rankings, as well as explanations for human review. The
process and methods for it that we present can significantly reduce human
effort in dataset creation for custom domains while still obtaining sufficient
expert knowledge for tuning retrieval models. We evaluate our methods on QBD
datasets from the Text Retrieval Conference (TREC) and finetune the parameters
of the BM25 model -- which is used in many industrial-strength search engines
like OpenSearch -- using the generated data.

</details>


### [160] [HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights](https://arxiv.org/abs/2505.04846)
*Ozan Gokdemir,Carlo Siebenschuh,Alexander Brace,Azton Wells,Brian Hsu,Kyle Hippe,Priyanka V. Setty,Aswathy Ajith,J. Gregory Pauloski,Varuni Sastry,Sam Foreman,Huihuo Zheng,Heng Ma,Bharat Kale,Nicholas Chia,Thomas Gibbs,Michael E. Papka,Thomas Brettin,Francis J. Alexander,Anima Anandkumar,Ian Foster,Rick Stevens,Venkatram Vishwanath,Arvind Ramanathan*

Main category: cs.IR

TL;DR: HiPerRAG利用高性能计算（HPC）优化检索增强生成（RAG），通过高效解析和检索技术处理360万篇科学文献，显著提升科学问答准确性。


<details>
  <summary>Details</summary>
Motivation: 解决科学文献爆炸式增长导致的发现利用率低、重复工作和跨学科合作受限问题。

Method: 采用高性能计算（HPC）支持的RAG工作流，核心技术包括高吞吐多模态解析模型Oreo和查询感知编码器调优算法ColTrast。

Result: 在SciQ和PubMedQA基准测试中分别达到90%和76%的准确率，超越领域专用模型和商用LLM。

Conclusion: HiPerRAG通过大规模GPU扩展，实现了百万级文献的高效处理，推动科学知识统一和跨学科创新。

Abstract: The volume of scientific literature is growing exponentially, leading to
underutilized discoveries, duplicated efforts, and limited cross-disciplinary
collaboration. Retrieval Augmented Generation (RAG) offers a way to assist
scientists by improving the factuality of Large Language Models (LLMs) in
processing this influx of information. However, scaling RAG to handle millions
of articles introduces significant challenges, including the high computational
costs associated with parsing documents and embedding scientific knowledge, as
well as the algorithmic complexity of aligning these representations with the
nuanced semantics of scientific content. To address these issues, we introduce
HiPerRAG, a RAG workflow powered by high performance computing (HPC) to index
and retrieve knowledge from more than 3.6 million scientific articles. At its
core are Oreo, a high-throughput model for multimodal document parsing, and
ColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval
accuracy by using contrastive learning and late-interaction techniques.
HiPerRAG delivers robust performance on existing scientific question answering
benchmarks and two new benchmarks introduced in this work, achieving 90%
accuracy on SciQ and 76% on PubMedQA-outperforming both domain-specific models
like PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUs
on the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers million
document-scale RAG workflows for unifying scientific knowledge and fostering
interdisciplinary innovation.

</details>


### [161] [Prompt-Based LLMs for Position Bias-Aware Reranking in Personalized Recommendations](https://arxiv.org/abs/2505.04948)
*Md Aminul Islam,Ahmed Sayeed Faruk*

Main category: cs.IR

TL;DR: 论文研究了使用大语言模型（LLMs）进行推荐系统的重排名，发现其存在上下文窗口限制、位置偏差等问题，并提出了一种混合框架进行实验，结果显示LLMs在排名建模和偏差缓解方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 推荐系统通过建模用户偏好和行为提供个性化内容，LLMs因其无需任务特定训练的特性被用于推荐。然而，LLMs存在上下文窗口限制、位置偏差等问题，亟需解决。

Method: 提出了一种混合框架，结合传统推荐模型和LLM，通过结构化提示对top-k项目进行重排名，并评估了用户历史重排序和指令提示对缓解位置偏差的效果。

Result: 实验表明，随机化用户历史提高了排名质量，但LLM重排名未能超越基础模型，且明确指令对减少位置偏差无效。

Conclusion: LLMs在建模排名上下文和缓解偏差方面存在局限性，需要进一步研究改进。

Abstract: Recommender systems are essential for delivering personalized content across
digital platforms by modeling user preferences and behaviors. Recently, large
language models (LLMs) have been adopted for prompt-based recommendation due to
their ability to generate personalized outputs without task-specific training.
However, LLM-based methods face limitations such as limited context window
size, inefficient pointwise and pairwise prompting, and difficulty handling
listwise ranking due to token constraints. LLMs can also be sensitive to
position bias, as they may overemphasize earlier items in the prompt regardless
of their true relevance. To address and investigate these issues, we propose a
hybrid framework that combines a traditional recommendation model with an LLM
for reranking top-k items using structured prompts. We evaluate the effects of
user history reordering and instructional prompts for mitigating position bias.
Experiments on MovieLens-100K show that randomizing user history improves
ranking quality, but LLM-based reranking does not outperform the base model.
Explicit instructions to reduce position bias are also ineffective. Our
evaluations reveal limitations in LLMs' ability to model ranking context and
mitigate bias. Our code is publicly available at
https://github.com/aminul7506/LLMForReRanking.

</details>


### [162] [QBR: A Question-Bank-Based Approach to Fine-Grained Legal Knowledge Retrieval for the General Public](https://arxiv.org/abs/2505.04883)
*Mingruo Yuan,Ben Kao,Tien-Hsuan Wu*

Main category: cs.IR

TL;DR: 论文提出了一种名为QBR的方法，通过问题库（QB）帮助非专业人士有效检索法律知识，解决了传统检索方法在技术和用户理解之间的差距问题。实验证明QBR在准确性、效率和解释性方面优于传统方法，并具有实际社会影响。


<details>
  <summary>Details</summary>
Motivation: 由于法律知识的专业性和公众对其理解的缺乏，传统信息检索方法难以满足非专业人士的需求。QBR旨在通过问题库（QB）弥合这一知识差距。

Method: QBR采用问题库（QB）作为媒介，生成训练样本以增强文档中知识单元的嵌入，从而实现细粒度的知识检索。

Result: 实验表明，QBR在准确性、效率和解释性方面优于传统方法，并显著提升了检索结果的可理解性和细粒度知识检索效果。

Conclusion: QBR不仅提高了法律知识检索的效果，还通过帮助公众解决日常法律问题产生了积极的社会影响。

Abstract: Retrieval of legal knowledge by the general public is a challenging problem
due to the technicality of the professional knowledge and the lack of
fundamental understanding by laypersons on the subject. Traditional information
retrieval techniques assume that users are capable of formulating succinct and
precise queries for effective document retrieval. In practice, however, the
wide gap between the highly technical contents and untrained users makes legal
knowledge retrieval very difficult. We propose a methodology, called QBR, which
employs a Questions Bank (QB) as an effective medium for bridging the knowledge
gap. We show how the QB is used to derive training samples to enhance the
embedding of knowledge units within documents, which leads to effective
fine-grained knowledge retrieval. We discuss and evaluate through experiments
various advantages of QBR over traditional methods. These include more
accurate, efficient, and explainable document retrieval, better comprehension
of retrieval results, and highly effective fine-grained knowledge retrieval. We
also present some case studies and show that QBR achieves social impact by
assisting citizens to resolve everyday legal concerns.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [163] [Physics-informed solution reconstruction in elasticity and heat transfer using the explicit constraint force method](https://arxiv.org/abs/2505.04875)
*Conor Rowan,Kurt Maute,Alireza Doostan*

Main category: cs.CE

TL;DR: 论文探讨了物理信息神经网络（PINNs）在解决实际物理系统状态重建时，因参数化物理方程与真实物理现象不一致而导致的问题，并提出了一种新方法（ECFM）以提高重建的可预测性和自定义性。


<details>
  <summary>Details</summary>
Motivation: 探讨PINNs在实际应用中因参数化物理方程与真实物理现象不一致而无法满足可解释性、鲁棒性和数据一致性的问题，并提出改进方法。

Method: 提出“显式约束力方法”（ECFM），通过控制约束引入的源项来解决PINNs在重建问题中的局限性。

Result: ECFM方法在弹性和热传导问题中表现出更高的可预测性和自定义性，尤其是在参数化物理与真实系统不一致时。

Conclusion: 通过满足可解释性、鲁棒性和数据一致性标准，ECFM方法能够在噪声数据下实现更可靠的重建结果，即使参数化物理与真实系统不完全匹配。

Abstract: One use case of ``physics-informed neural networks'' (PINNs) is solution
reconstruction, which aims to estimate the full-field state of a physical
system from sparse measurements. Parameterized governing equations of the
system are used in tandem with the measurements to regularize the regression
problem. However, in real-world solution reconstruction problems, the
parameterized governing equation may be inconsistent with the physical
phenomena that give rise to the measurement data. We show that due to assuming
consistency between the true and parameterized physics, PINNs-based approaches
may fail to satisfy three basic criteria of interpretability, robustness, and
data consistency. As we argue, these criteria ensure that (i) the quality of
the reconstruction can be assessed, (ii) the reconstruction does not depend
strongly on the choice of physics loss, and (iii) that in certain situations,
the physics parameters can be uniquely recovered. In the context of elasticity
and heat transfer, we demonstrate how standard formulations of the physics loss
and techniques for constraining the solution to respect the measurement data
lead to different ``constraint forces" -- which we define as additional source
terms arising from the constraints -- and that these constraint forces can
significantly influence the reconstructed solution. To avoid the potentially
substantial influence of the choice of physics loss and method of constraint
enforcement on the reconstructed solution, we propose the ``explicit constraint
force method'' (ECFM) to gain control of the source term introduced by the
constraint. We then show that by satisfying the criteria of interpretability,
robustness, and data consistency, this approach leads to more predictable and
customizable reconstructions from noisy measurement data, even when the
parameterization of the missing physics is inconsistent with the measured
system.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [164] [BitHEP -- The Limits of Low-Precision ML in HEP](https://arxiv.org/abs/2504.03387)
*Claudius Krause,Daohan Wang,Ramon Winterhalder*

Main category: hep-ph

TL;DR: BitNet架构在HEP应用中评估其性能，结果显示其在分类任务中表现优异，但在回归和生成任务中表现因网络规模而异。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络架构日益复杂，需要快速且内存高效的实现以减少计算瓶颈。

Method: 评估BitNet在分类、回归和生成任务中的性能，并与现有方法比较效率和准确度。

Result: BitNet在分类任务中表现优异，但在回归和生成任务中表现因网络规模而异。

Conclusion: BitNet在HEP应用中表现有潜力，尤其在分类任务中，但在其他任务中仍需改进。

Abstract: The increasing complexity of modern neural network architectures demands fast
and memory-efficient implementations to mitigate computational bottlenecks. In
this work, we evaluate the recently proposed BitNet architecture in HEP
applications, assessing its performance in classification, regression, and
generative modeling tasks. Specifically, we investigate its suitability for
quark-gluon discrimination, SMEFT parameter estimation, and detector
simulation, comparing its efficiency and accuracy to state-of-the-art methods.
Our results show that while BitNet consistently performs competitively in
classification tasks, its performance in regression and generation varies with
the size and type of the network, highlighting key limitations and potential
areas for improvement.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [165] [Dukawalla: Voice Interfaces for Small Businesses in Africa](https://arxiv.org/abs/2505.05170)
*Elizabeth Ankrah,Stephanie Nyairo,Mercy Muchai,Kagonya Awori,Millicent Ochieng,Mark Kariuki,Jacki O'Neill*

Main category: cs.HC

TL;DR: 论文研究了Dukawalla语音助手在非洲中小企业中的应用，旨在通过语音交互和生成式AI简化数据驱动决策。


<details>
  <summary>Details</summary>
Motivation: 非洲中小企业因缺乏适合的分析工具而难以进行数据驱动决策，Dukawalla旨在解决这一问题。

Method: 开发了Dukawalla原型，利用语音交互和生成式AI将原始数据转化为可操作性洞察，并在内罗毕的中小企业中部署测试。

Result: Dukawalla为中小企业主提供了一种直观的数据交互方式，帮助其简化数据收集并获取业务洞察。

Conclusion: Dukawalla通过语音和AI技术有效支持了非洲中小企业的数据驱动决策需求。

Abstract: Small and medium sized businesses often struggle with data driven decision
making do to a lack of advanced analytics tools, especially in African
countries where they make up a majority of the workforce. Though many tools
exist they are not designed to fit into the ways of working of SMB workers who
are mobile first, have limited time to learn new workflows, and for whom social
and business are tightly coupled. To address this, the Dukawalla prototype was
created. This intelligent assistant bridges the gap between raw business data,
and actionable insights by leveraging voice interaction and the power of
generative AI. Dukawalla provides an intuitive way for business owners to
interact with their data, aiding in informed decision making. This paper
examines Dukawalla's deployment across SMBs in Nairobi, focusing on their
experiences using this voice based assistant to streamline data collection and
provide business insights

</details>


### [166] [Fairness Perceptions in Regression-based Predictive Models](https://arxiv.org/abs/2505.04886)
*Mukund Telukunta,Venkata Sriram Siddhardh Nadendla,Morgan Stuart,Casey Canfield*

Main category: cs.HC

TL;DR: 论文提出三种基于分歧的群体公平性概念，评估肾移植预测分析工具的公平性，并通过众包反馈确定社会接受的公平标准。结果显示分离和充分性公平概念更受青睐。


<details>
  <summary>Details</summary>
Motivation: 现有肾移植预测分析工具可能因训练数据偏见导致社会歧视和器官利用效率低下，但关于回归公平性的研究较少，尤其对器官分配的影响。

Method: 引入独立性、分离性和充分性三种公平性概念，通过众包平台收集85名参与者的反馈，使用Mixed-Logit离散选择模型分析社会公平偏好。

Result: 分离性和充分性公平概念更受认可，预测分析在性别和种族上公平，但在年龄上不公平。

Conclusion: 研究为肾移植预测工具的公平性评估提供了新方法，并揭示了社会偏好的公平标准，有助于改进器官分配策略。

Abstract: Regression-based predictive analytics used in modern kidney transplantation
is known to inherit biases from training data. This leads to social
discrimination and inefficient organ utilization, particularly in the context
of a few social groups. Despite this concern, there is limited research on
fairness in regression and its impact on organ utilization and placement. This
paper introduces three novel divergence-based group fairness notions: (i)
independence, (ii) separation, and (iii) sufficiency to assess the fairness of
regression-based analytics tools. In addition, fairness preferences are
investigated from crowd feedback, in order to identify a socially accepted
group fairness criterion for evaluating these tools. A total of 85 participants
were recruited from the Prolific crowdsourcing platform, and a Mixed-Logit
discrete choice model was used to model fairness feedback and estimate social
fairness preferences. The findings clearly depict a strong preference towards
the separation and sufficiency fairness notions, and that the predictive
analytics is deemed fair with respect to gender and race groups, but unfair in
terms of age groups.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [167] [Advancing 3D Medical Image Segmentation: Unleashing the Potential of Planarian Neural Networks in Artificial Intelligence](https://arxiv.org/abs/2505.04664)
*Ziyuan Huang,Kevin Huggins,Srikar Bellur*

Main category: eess.IV

TL;DR: PNN-UNet是一种模仿涡虫神经网络结构的深度学习方法，用于3D医学图像分割，表现优于传统UNet及其变体。


<details>
  <summary>Details</summary>
Motivation: 受涡虫神经网络结构的启发，希望设计一种更高效的3D医学图像分割模型，超越现有的UNet及其变体。

Method: 结合Deep-UNet和Wide-UNet模拟神经索功能，并通过密集连接自编码器充当"大脑"，形成PNN-UNet架构。

Result: 在3D MRI海马体数据集上，PNN-UNet在分割任务中表现优于基准UNet及其他变体，无论是否进行数据增强。

Conclusion: PNN-UNet的生物启发架构在医学图像分割中展现出显著优势，为类似任务提供了新思路。

Abstract: Our study presents PNN-UNet as a method for constructing deep neural networks
that replicate the planarian neural network (PNN) structure in the context of
3D medical image data. Planarians typically have a cerebral structure
comprising two neural cords, where the cerebrum acts as a coordinator, and the
neural cords serve slightly different purposes within the organism's
neurological system. Accordingly, PNN-UNet comprises a Deep-UNet and a
Wide-UNet as the nerve cords, with a densely connected autoencoder performing
the role of the brain. This distinct architecture offers advantages over both
monolithic (UNet) and modular networks (Ensemble-UNet). Our outcomes on a 3D
MRI hippocampus dataset, with and without data augmentation, demonstrate that
PNN-UNet outperforms the baseline UNet and several other UNet variants in image
segmentation.

</details>


### [168] [Direct Image Classification from Fourier Ptychographic Microscopy Measurements without Reconstruction](https://arxiv.org/abs/2505.05054)
*Navya Sonal Agarwal,Jan Philipp Schneider,Kanchana Vaishnavi Gandikota,Syed Muhammad Kazim,John Meshreki,Ivo Ihrke,Michael Moeller*

Main category: eess.IV

TL;DR: 该论文提出了一种直接在傅里叶叠层显微成像(FPM)测量数据中进行图像分类的方法，避免了高分辨率图像重建的计算开销，使用卷积神经网络(CNN)效果显著优于单幅图像的分类，并展示了数据复用技术可减少数据量和采集时间。


<details>
  <summary>Details</summary>
Motivation: 傅里叶叠层显微成像(FPM)虽然能实现高分辨率和大视场成像，但其重建过程计算复杂且耗时长。为了在医疗细胞分类等应用场景中提高效率，研究者希望跳过重建步骤，直接从测量数据中分类图像内容。

Method: 使用卷积神经网络(CNN)直接处理FPM测量序列，提取有效特征进行分类，并通过学习的数据复用技术优化数据利用效率。

Result: 实验表明，该方法在分类精度上比单幅图像直接分类提高12%，且显著减少了数据量和采集时间，同时保持了分类准确性。

Conclusion: 跳过FPM图像重建步骤，利用CNN直接从测量数据分类不仅高效，还能通过数据复用技术进一步提升实用性，为实时高分辨率成像应用提供新思路。

Abstract: The computational imaging technique of Fourier Ptychographic Microscopy (FPM)
enables high-resolution imaging with a wide field of view and can serve as an
extremely valuable tool, e.g. in the classification of cells in medical
applications. However, reconstructing a high-resolution image from tens or even
hundreds of measurements is computationally expensive, particularly for a wide
field of view. Therefore, in this paper, we investigate the idea of classifying
the image content in the FPM measurements directly without performing a
reconstruction step first. We show that Convolutional Neural Networks (CNN) can
extract meaningful information from measurement sequences, significantly
outperforming the classification on a single band-limited image (up to 12 %)
while being significantly more efficient than a reconstruction of a
high-resolution image. Furthermore, we demonstrate that a learned multiplexing
of several raw measurements allows maintaining the classification accuracy
while reducing the amount of data (and consequently also the acquisition time)
significantly.

</details>


### [169] [Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection](https://arxiv.org/abs/2505.05291)
*Benjamin A. Cohen,Jonathan Fhima,Meishar Meisel,Baskin Meital,Luis Filipe Nakayama,Eran Berkowitz,Joachim A. Behar*

Main category: eess.IV

TL;DR: 论文比较了自监督学习预训练的ViT模型在视网膜图像上的表现，发现基于自然图像预训练的iBOT模型在跨域泛化性能上优于领域特定模型，挑战了领域内预训练必要的假设。


<details>
  <summary>Details</summary>
Motivation: 探讨在视网膜图像任务中，自监督学习预训练的ViT模型是否需要领域内预训练以达到最佳性能。

Method: 在七个数字眼底图像数据集（共70,000张专家标注图像）上，对比六种自监督学习预训练的ViT模型在AMD识别任务中的表现。

Result: 基于自然图像预训练的iBOT模型表现最佳（AUROC 0.80-0.97），优于领域特定模型（AUROC 0.78-0.96）和无预训练的基线模型（AUROC 0.68-0.91）。

Conclusion: 自监督学习的预训练模型可显著提升AMD识别性能，且领域内预训练并非必要。此外，发布了巴西的AMD标注数据集BRAMD。

Abstract: Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to
learn robust representations from large-scale natural image datasets, enhancing
their generalization across domains. In retinal imaging, foundation models
pretrained on either natural or ophthalmic data have shown promise, but the
benefits of in-domain pretraining remain uncertain. To investigate this, we
benchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets
totaling 70,000 expert-annotated images for the task of moderate-to-late
age-related macular degeneration (AMD) identification. Our results show that
iBOT pretrained on natural images achieves the highest out-of-distribution
generalization, with AUROCs of 0.80-0.97, outperforming domain-specific models,
which achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining,
which achieved AUROCs of 0.68-0.91. These findings highlight the value of
foundation models in improving AMD identification and challenge the assumption
that in-domain pretraining is necessary. Furthermore, we release BRAMD, an
open-access dataset (n=587) of DFIs with AMD labels from Brazil.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [170] [Replay to Remember (R2R): An Efficient Uncertainty-driven Unsupervised Continual Learning Framework Using Generative Replay](https://arxiv.org/abs/2505.04787)
*Sriram Mandalika,Harsha Vardhan,Athira Nambiar*

Main category: cs.CV

TL;DR: 提出了一种基于生成重放的无人监督连续学习框架R2R，通过聚类不确定性反馈机制和VLM生成重放模块，有效解决了神经网络中的灾难性遗忘问题，并在多个数据集上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 传统的连续学习方法依赖预训练模型和伪标签，容易导致灾难性遗忘。R2R框架旨在通过无标签数据和合成标签数据的平衡使用，实现无需预训练的高效学习。

Method: R2R采用聚类驱动的动态阈值不确定性估计方法，结合生成重放机制和VLM生成合成数据来模拟生物视觉思维，以实现知识的持续学习和记忆保持。

Result: 在CIFAR-10、CIFAR-100、CINIC-10、SVHN和TinyImageNet数据集上，R2R分别取得了98.13%、73.06%、93.41%、95.18%和59.74%的最先进性能，比现有方法高出4.36%。

Conclusion: R2R框架通过无监督学习、不确定性与生成重放的结合，显著提升了知识保留能力，为连续学习领域提供了创新性解决方案。

Abstract: Continual Learning entails progressively acquiring knowledge from new data
while retaining previously acquired knowledge, thereby mitigating
``Catastrophic Forgetting'' in neural networks. Our work presents a novel
uncertainty-driven Unsupervised Continual Learning framework using Generative
Replay, namely ``Replay to Remember (R2R)''. The proposed R2R architecture
efficiently uses unlabelled and synthetic labelled data in a balanced
proportion using a cluster-level uncertainty-driven feedback mechanism and a
VLM-powered generative replay module. Unlike traditional memory-buffer methods
that depend on pretrained models and pseudo-labels, our R2R framework operates
without any prior training. It leverages visual features from unlabeled data
and adapts continuously using clustering-based uncertainty estimation coupled
with dynamic thresholding. Concurrently, a generative replay mechanism along
with DeepSeek-R1 powered CLIP VLM produces labelled synthetic data
representative of past experiences, resembling biological visual thinking that
replays memory to remember and act in new, unseen tasks. Extensive experimental
analyses are carried out in CIFAR-10, CIFAR-100, CINIC-10, SVHN and
TinyImageNet datasets. Our proposed R2R approach improves knowledge retention,
achieving a state-of-the-art performance of 98.13%, 73.06%, 93.41%, 95.18%,
59.74%, respectively, surpassing state-of-the-art performance by over 4.36%.

</details>


### [171] [Auto-regressive transformation for image alignment](https://arxiv.org/abs/2505.04864)
*Kanggeon Lee,Soochahn Lee,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: ART提出了一种自回归变换方法，通过多尺度特征和随机采样点迭代优化变换场，显著提升了特征稀疏区域的图像对齐精度。


<details>
  <summary>Details</summary>
Motivation: 现有图像对齐方法在特征稀疏区域、极端尺度和视场差异以及大变形情况下表现不佳，因此需要更鲁棒的方法。

Method: 采用自回归框架，利用多尺度特征分层迭代优化变换场，并通过交叉注意力层引导关键区域的对齐。

Result: 在多个数据集上的实验显示，ART显著优于现有最先进方法。

Conclusion: ART是一种在挑战性条件下仍能实现精确图像对齐的强大新方法，具有广泛适用性。

Abstract: Existing methods for image alignment struggle in cases involving
feature-sparse regions, extreme scale and field-of-view differences, and large
deformations, often resulting in suboptimal accuracy. Robustness to these
challenges improves through iterative refinement of the transformation field
while focusing on critical regions in multi-scale image representations. We
thus propose Auto-Regressive Transformation (ART), a novel method that
iteratively estimates the coarse-to-fine transformations within an
auto-regressive framework. Leveraging hierarchical multi-scale features, our
network refines the transformations using randomly sampled points at each
scale. By incorporating guidance from the cross-attention layer, the model
focuses on critical regions, ensuring accurate alignment even in challenging,
feature-limited conditions. Extensive experiments across diverse datasets
demonstrate that ART significantly outperforms state-of-the-art methods,
establishing it as a powerful new method for precise image alignment with broad
applicability.

</details>


### [172] [SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models](https://arxiv.org/abs/2505.04911)
*Shun Taguchi,Hideki Deguchi,Takumi Hamazaki,Hiroyuki Sakai*

Main category: cs.CV

TL;DR: SpatialPrompting框架利用现成的多模态大语言模型实现零样本三维空间推理，无需昂贵的3D数据微调，通过关键帧选择策略和视觉-语言相似性等指标，在ScanQA和SQA3D数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的3D专用微调和专用输入（如点云），限制了灵活性和可扩展性。本文旨在提供一种无需3D微调或专用输入的方法，利用直观的视觉和位置线索实现高效的空间推理。

Method: 提出关键帧驱动提示生成策略，基于视觉-语言相似性、马氏距离、视场和图像清晰度等指标选择关键帧，并结合相机姿态数据抽象空间关系。

Result: 在ScanQA和SQA3D数据集上实现了零样本SOTA性能，验证了方法的有效性和优越性。

Conclusion: SpatialPrompting为3D空间推理提供了一种更简单、更灵活的解决方案，避免了专用输入和微调的需求，具有更高的可扩展性。

Abstract: This study introduces SpatialPrompting, a novel framework that harnesses the
emergent reasoning capabilities of off-the-shelf multimodal large language
models to achieve zero-shot spatial reasoning in three-dimensional (3D)
environments. Unlike existing methods that rely on expensive 3D-specific
fine-tuning with specialized 3D inputs such as point clouds or voxel-based
features, SpatialPrompting employs a keyframe-driven prompt generation
strategy. This framework uses metrics such as vision-language similarity,
Mahalanobis distance, field of view, and image sharpness to select a diverse
and informative set of keyframes from image sequences and then integrates them
with corresponding camera pose data to effectively abstract spatial
relationships and infer complex 3D structures. The proposed framework not only
establishes a new paradigm for flexible spatial reasoning that utilizes
intuitive visual and positional cues but also achieves state-of-the-art
zero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across
several metrics. The proposed method effectively eliminates the need for
specialized 3D inputs and fine-tuning, offering a simpler and more scalable
alternative to conventional approaches.

</details>


### [173] [Learning from Loss Landscape: Generalizable Mixed-Precision Quantization via Adaptive Sharpness-Aware Gradient Aligning](https://arxiv.org/abs/2505.04877)
*Lianbo Ma,Jianlun Ma,Yuee Zhou,Guoyang Xie,Qiang He,Zhichao Lu*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，通过在小数据集上搜索量化策略并推广到大数据集，降低了混合精度量化的计算成本，同时保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有混合精度量化方法需要在大规模数据集上进行昂贵的计算搜索，效率低下。本文旨在解决这一问题，提出更高效的方法。

Method: 利用小数据集搜索量化策略，结合三种关键技术：锐度感知最小化、隐式梯度方向对齐和自适应扰动半径。

Result: 在CIFAR10（仅ImageNet训练数据的0.5%）上搜索策略，实现了与ImageNet相当的准确率，计算效率提升150%。

Conclusion: 该方法显著降低了混合精度量化的计算成本，同时保持或提升了模型性能，为实际应用提供了高效解决方案。

Abstract: Mixed Precision Quantization (MPQ) has become an essential technique for
optimizing neural network by determining the optimal bitwidth per layer.
Existing MPQ methods, however, face a major hurdle: they require a
computationally expensive search for quantization policies on large-scale
datasets. To resolve this issue, we introduce a novel approach that first
searches for quantization policies on small datasets and then generalizes them
to large-scale datasets. This approach simplifies the process, eliminating the
need for large-scale quantization fine-tuning and only necessitating model
weight adjustment. Our method is characterized by three key techniques:
sharpness-aware minimization for enhanced quantization generalization, implicit
gradient direction alignment to handle gradient conflicts among different
optimization objectives, and an adaptive perturbation radius to accelerate
optimization. Both theoretical analysis and experimental results validate our
approach. Using the CIFAR10 dataset (just 0.5\% the size of ImageNet training
data) for MPQ policy search, we achieved equivalent accuracy on ImageNet with a
significantly lower computational cost, while improving efficiency by up to
150% over the baselines.

</details>


### [174] [Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models](https://arxiv.org/abs/2505.04921)
*Yunxin Li,Zhenyu Liu,Zitao Li,Xuanyu Zhang,Zhenran Xu,Xinyu Chen,Haoyuan Shi,Shenyuan Jiang,Xintong Wang,Jifang Wang,Shouzheng Huang,Xinping Zhao,Borui Jiang,Lanqing Hong,Longyue Wang,Zhuotao Tian,Baoxing Huai,Wenhan Luo,Weihua Luo,Zheng Zhang,Baotian Hu,Min Zhang*

Main category: cs.CV

TL;DR: 该论文总结了多模态推理研究的进展，提出了一个四阶段发展路线图，从早期的模块化设计到统一的多模态大语言模型（LLMs），最终探讨了原生大型多模态推理模型（N-LMRMs）的未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在开放、不确定和多模态环境中的操作增多，推理能力成为实现鲁棒和自适应行为的关键。多模态推理模型（LMRMs）通过整合文本、图像、音频和视频等模态，旨在实现全面感知、精确理解和深度推理。

Method: 论文通过四阶段发展路线图回顾了多模态推理的演进：从任务特定模块的早期努力，到统一的多模态LLMs，最终展望了N-LMRMs的未来方向。

Result: 研究表明，多模态链式思维（MCoT）和多模态强化学习等技术推动了更丰富和结构化的推理链。通过OpenAI O3和O4-mini的实验案例，验证了多模态推理的潜力。

Conclusion: 多模态推理领域正在向原生大型多模态推理模型（N-LMRMs）发展，以支持复杂、真实环境中的可扩展、自主和自适应推理与规划。

Abstract: Reasoning lies at the heart of intelligence, shaping the ability to make
decisions, draw conclusions, and generalize across domains. In artificial
intelligence, as systems increasingly operate in open, uncertain, and
multimodal environments, reasoning becomes essential for enabling robust and
adaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a
promising paradigm, integrating modalities such as text, images, audio, and
video to support complex reasoning capabilities and aiming to achieve
comprehensive perception, precise understanding, and deep reasoning. As
research advances, multimodal reasoning has rapidly evolved from modular,
perception-driven pipelines to unified, language-centric frameworks that offer
more coherent cross-modal understanding. While instruction tuning and
reinforcement learning have improved model reasoning, significant challenges
remain in omni-modal generalization, reasoning depth, and agentic behavior. To
address these issues, we present a comprehensive and structured survey of
multimodal reasoning research, organized around a four-stage developmental
roadmap that reflects the field's shifting design philosophies and emerging
capabilities. First, we review early efforts based on task-specific modules,
where reasoning was implicitly embedded across stages of representation,
alignment, and fusion. Next, we examine recent approaches that unify reasoning
into multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT)
and multimodal reinforcement learning enabling richer and more structured
reasoning chains. Finally, drawing on empirical insights from challenging
benchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the
conceptual direction of native large multimodal reasoning models (N-LMRMs),
which aim to support scalable, agentic, and adaptive reasoning and planning in
complex, real-world environments.

</details>


### [175] [T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models](https://arxiv.org/abs/2505.04946)
*Xuyang Guo,Jiayan Huo,Zhenmei Shi,Zhao Song,Jiahao Zhang,Jiale Zhao*

Main category: cs.CV

TL;DR: 摘要介绍了T2VTextBench，这是首个评估文本到视频模型中屏幕文本保真度和时间一致性的基准测试，发现当前大多数模型在生成清晰、一致的文本上仍有困难。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成模型在广告、娱乐和教育等应用中表现出色，但在精确渲染屏幕文本（如字幕或数学公式）方面的能力尚未充分测试，这对需要准确文本的应用构成挑战。

Method: 研究团队开发了T2VTextBench，一个结合复杂文本字符串和动态场景变化的人类评估基准，用于测试模型在多帧中保持详细指令的能力。

Result: 评估了十种最先进的系统（包括开源和商业方案），发现大多数模型难以生成清晰且一致的文本。

Conclusion: 结果揭示了当前视频生成器在文本处理上的关键不足，为未来研究提供了改进视频合成中文本操作的方向。

Abstract: Thanks to recent advancements in scalable deep architectures and large-scale
pretraining, text-to-video generation has achieved unprecedented capabilities
in producing high-fidelity, instruction-following content across a wide range
of styles, enabling applications in advertising, entertainment, and education.
However, these models' ability to render precise on-screen text, such as
captions or mathematical formulas, remains largely untested, posing significant
challenges for applications requiring exact textual accuracy. In this work, we
introduce T2VTextBench, the first human-evaluation benchmark dedicated to
evaluating on-screen text fidelity and temporal consistency in text-to-video
models. Our suite of prompts integrates complex text strings with dynamic scene
changes, testing each model's ability to maintain detailed instructions across
frames. We evaluate ten state-of-the-art systems, ranging from open-source
solutions to commercial offerings, and find that most struggle to generate
legible, consistent text. These results highlight a critical gap in current
video generators and provide a clear direction for future research aimed at
enhancing textual manipulation in video synthesis.

</details>


### [176] [Cross-Branch Orthogonality for Improved Generalization in Face Deepfake Detection](https://arxiv.org/abs/2505.04888)
*Tharindu Fernando,Clinton Fookes,Sridha Sridharan,Simon Denman*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新策略，利用从粗到细的空间信息、语义信息及其交互来检测深度伪造人脸，通过特征正交解耦策略提升检测性能，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的快速发展导致现有检测器难以跟上其进步，主要因为它们依赖特定伪造痕迹，难以泛化。论文旨在解决这一问题，提出更强大的检测方法。

Method: 论文提出了一种结合空间和语义信息的策略，采用特征正交解耦技术确保特征独特性和减少冗余，从而在不增加复杂性的情况下提升检测能力。

Result: 在FaceForensics++、Celeb-DF和DFDC数据集上的实验表明，该方法在跨数据集评估中比现有最优方法在Celeb-DF上高出5%，在DFDC上高出7%。

Conclusion: 论文的方法通过创新的特征解耦和空间-语义信息整合，显著提升了深度伪造检测的性能和泛化能力。

Abstract: Remarkable advancements in generative AI technology have given rise to a
spectrum of novel deepfake categories with unprecedented leaps in their
realism, and deepfakes are increasingly becoming a nuisance to law enforcement
authorities and the general public. In particular, we observe alarming levels
of confusion, deception, and loss of faith regarding multimedia content within
society caused by face deepfakes, and existing deepfake detectors are
struggling to keep up with the pace of improvements in deepfake generation.
This is primarily due to their reliance on specific forgery artifacts, which
limits their ability to generalise and detect novel deepfake types. To combat
the spread of malicious face deepfakes, this paper proposes a new strategy that
leverages coarse-to-fine spatial information, semantic information, and their
interactions while ensuring feature distinctiveness and reducing the redundancy
of the modelled features. A novel feature orthogonality-based disentanglement
strategy is introduced to ensure branch-level and cross-branch feature
disentanglement, which allows us to integrate multiple feature vectors without
adding complexity to the feature space or compromising generalisation.
Comprehensive experiments on three public benchmarks: FaceForensics++,
Celeb-DF, and the Deepfake Detection Challenge (DFDC) show that these design
choices enable the proposed approach to outperform current state-of-the-art
methods by 5% on the Celeb-DF dataset and 7% on the DFDC dataset in a
cross-dataset evaluation setting.

</details>


### [177] [TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation](https://arxiv.org/abs/2505.05422)
*Haokun Lin,Teng Wang,Yixiao Ge,Yuying Ge,Zhichao Lu,Ying Wei,Qingfu Zhang,Zhenan Sun,Ying Shan*

Main category: cs.CV

TL;DR: TokLIP 是一种视觉标记化方法，通过语义化向量量化（VQ）标记并结合 CLIP 级语义，提升多模态自回归训练的理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如 Chameleon 和 Emu3）在多模态统一中存在训练计算成本高和理解性能有限的问题，TokLIP 旨在解决这些问题。

Method: TokLIP 结合低层离散 VQ 标记器与 ViT 标记编码器，分离理解和生成的训练目标，避免定制量化操作。

Result: TokLIP 表现出卓越的数据效率，增强了高层语义理解和低层生成能力，适用于自回归 Transformer 的多种任务。

Conclusion: TokLIP 通过高效语义化标记，显著提升了多模态任务的性能，代码和模型已开源。

Abstract: Pioneering token-based works such as Chameleon and Emu3 have established a
foundation for multimodal unification but face challenges of high training
computational overhead and limited comprehension performance due to a lack of
high-level semantics. In this paper, we introduce TokLIP, a visual tokenizer
that enhances comprehension by semanticizing vector-quantized (VQ) tokens and
incorporating CLIP-level semantics while enabling end-to-end multimodal
autoregressive training with standard VQ tokens. TokLIP integrates a low-level
discrete VQ tokenizer with a ViT-based token encoder to capture high-level
continuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize
high-level features, TokLIP disentangles training objectives for comprehension
and generation, allowing the direct application of advanced VQ tokenizers
without the need for tailored quantization operations. Our empirical results
demonstrate that TokLIP achieves exceptional data efficiency, empowering visual
tokens with high-level semantic understanding while enhancing low-level
generative capacity, making it well-suited for autoregressive Transformers in
both comprehension and generation tasks. The code and models are available at
https://github.com/TencentARC/TokLIP.

</details>


### [178] [Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding](https://arxiv.org/abs/2505.05446)
*Han Xiao,Yina Xie,Guanxin Tan,Yinghao Chen,Rui Hu,Ke Wang,Aojun Zhou,Hao Li,Hao Shao,Xudong Lu,Peng Gao,Yafei Wen,Xiaoxin Chen,Shuai Ren,Hongsheng Li*

Main category: cs.CV

TL;DR: 论文提出了一种利用标记语言生成高度结构化文档表示的新方法，以解决视觉文档理解中上下文信息不足的问题，并引入了两个细粒度数据集。实验表明，该方法在多个基准测试中表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 视觉文档理解领域因复杂布局和缺乏上下文信息而面临挑战，导致幻觉和空间关系理解受限。旨在通过结构化表示提升模型性能。

Method: 采用自适应生成标记语言（如Markdown、JSON等）构建文档表示，并引入DocMark-Pile和DocMark-Instruct两个数据集用于预训练和微调。

Result: 模型在多个视觉文档理解基准测试中显著优于现有技术，展现出更强的推理和理解能力。

Conclusion: 提出的方法通过结构化表示和高质量数据集，有效提升了复杂视觉场景下的文档理解能力。代码和模型已开源。

Abstract: Visual Document Understanding has become essential with the increase of
text-rich visual content. This field poses significant challenges due to the
need for effective integration of visual perception and textual comprehension,
particularly across diverse document types with complex layouts. Moreover,
existing fine-tuning datasets for this domain often fall short in providing the
detailed contextual information for robust understanding, leading to
hallucinations and limited comprehension of spatial relationships among visual
elements. To address these challenges, we propose an innovative pipeline that
utilizes adaptive generation of markup languages, such as Markdown, JSON, HTML,
and TiKZ, to build highly structured document representations and deliver
contextually-grounded responses. We introduce two fine-grained structured
datasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs
for document parsing, and DocMark-Instruct, featuring 624k fine-tuning data
annotations for grounded instruction following. Extensive experiments
demonstrate that our proposed model significantly outperforms existing
state-of-theart MLLMs across a range of visual document understanding
benchmarks, facilitating advanced reasoning and comprehension capabilities in
complex visual scenarios. Our code and models are released at https://github.
com/Euphoria16/DocMark.

</details>


### [179] [StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant](https://arxiv.org/abs/2505.05467)
*Haibo Wang,Bo Feng,Zhengfeng Lai,Mingze Xu,Shiyu Li,Weifeng Ge,Afshin Dehghan,Meng Cao,Ping Huang*

Main category: cs.CV

TL;DR: StreamBridge框架将离线Video-LLMs转化为支持流式处理的模型，解决了多轮实时理解和主动响应机制的挑战，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 适应现有模型到在线场景时面临的多轮实时理解能力不足和缺乏主动响应机制的问题。

Method: 采用内存缓冲区和轮次衰减压缩策略支持长上下文多轮交互，并引入解耦的轻量级激活模型实现持续主动响应。

Result: StreamBridge显著提升了离线Video-LLMs的流式理解能力，超越GPT-4o和Gemini 1.5 Pro，并在标准视频理解基准上表现优异。

Conclusion: StreamBridge为Video-LLMs的流式应用提供了一种简单高效的解决方案，同时保持了高性能。

Abstract: We present StreamBridge, a simple yet effective framework that seamlessly
transforms offline Video-LLMs into streaming-capable models. It addresses two
fundamental challenges in adapting existing models into online scenarios: (1)
limited capability for multi-turn real-time understanding, and (2) lack of
proactive response mechanisms. Specifically, StreamBridge incorporates (1) a
memory buffer combined with a round-decayed compression strategy, supporting
long-context multi-turn interactions, and (2) a decoupled, lightweight
activation model that can be effortlessly integrated into existing Video-LLMs,
enabling continuous proactive responses. To further support StreamBridge, we
construct Stream-IT, a large-scale dataset tailored for streaming video
understanding, featuring interleaved video-text sequences and diverse
instruction formats. Extensive experiments show that StreamBridge significantly
improves the streaming understanding capabilities of offline Video-LLMs across
various tasks, outperforming even proprietary models such as GPT-4o and Gemini
1.5 Pro. Simultaneously, it achieves competitive or superior performance on
standard video understanding benchmarks.

</details>


### [180] [Comparison of Visual Trackers for Biomechanical Analysis of Running](https://arxiv.org/abs/2505.04713)
*Luis F. Gomez,Gonzalo Garrido-Lopez,Julian Fierrez,Aythami Morales,Ruben Tolosana,Javier Rueda,Enrique Navarro*

Main category: cs.CV

TL;DR: 论文研究了六种姿态跟踪器（两种点跟踪器和四种关节跟踪器）在短跑生物力学分析中的性能，通过后处理模块降低了误差，证明了姿态跟踪在跑步生物力学分析中的价值，但高精度应用仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 近年来，深度学习模型和数据资源的进步推动了人体姿态估计的快速发展。本研究旨在评估姿态跟踪器在短跑生物力学分析中的性能，特别是在关键角度（躯干倾斜、髋关节屈伸、膝关节屈伸）上的准确性。

Method: 研究对比了六种跟踪器（两种点跟踪器和四种关节跟踪器）与生物力学专家手动标注的结果，分析了5870帧数据。实验框架涵盖五名职业跑者的40次短跑，并提出了异常值检测和融合预测的后处理模块。

Result: 关节模型的均方根误差为11.41°至4.37°，加入后处理模块后误差降至6.99°和3.88°。结果表明，姿态跟踪对短跑生物力学分析有效，但高精度需求场景仍需改进。

Conclusion: 姿态跟踪方法在跑步生物力学分析中具有实用价值，后处理模块能显著提升精度，但高精度应用仍需进一步优化。

Abstract: Human pose estimation has witnessed significant advancements in recent years,
mainly due to the integration of deep learning models, the availability of a
vast amount of data, and large computational resources. These developments have
led to highly accurate body tracking systems, which have direct applications in
sports analysis and performance evaluation.
  This work analyzes the performance of six trackers: two point trackers and
four joint trackers for biomechanical analysis in sprints. The proposed
framework compares the results obtained from these pose trackers with the
manual annotations of biomechanical experts for more than 5870 frames. The
experimental framework employs forty sprints from five professional runners,
focusing on three key angles in sprint biomechanics: trunk inclination, hip
flex extension, and knee flex extension. We propose a post-processing module
for outlier detection and fusion prediction in the joint angles.
  The experimental results demonstrate that using joint-based models yields
root mean squared errors ranging from 11.41{\deg} to 4.37{\deg}. When
integrated with the post-processing modules, these errors can be reduced to
6.99{\deg} and 3.88{\deg}, respectively. The experimental findings suggest that
human pose tracking approaches can be valuable resources for the biomechanical
analysis of running. However, there is still room for improvement in
applications where high accuracy is required.

</details>


### [181] [Lay-Your-Scene: Natural Scene Layout Generation with Diffusion Transformers](https://arxiv.org/abs/2505.04718)
*Divyansh Srivastava,Xiang Zhang,He Wen,Chenru Wen,Zhuowen Tu*

Main category: cs.CV

TL;DR: LayouSyn是一个基于轻量级开源语言模型和新型扩散Transformer架构的文本到布局生成方法，在开放词汇下实现自然场景布局生成，性能优于现有方法，并在图像编辑中展示了应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有场景布局生成方法要么词汇封闭，要么依赖专有大语言模型，限制了其建模能力和可控图像生成的广泛应用。

Method: 提出使用轻量级开源语言模型从文本中获取场景元素，并训练一种新型的面向长宽比的扩散Transformer架构。

Result: LayouSyn在空间和数值推理基准测试中表现优异，并展示了与大型语言模型结合及图像编辑中的应用。

Conclusion: LayouSyn为开放词汇场景布局生成提供了高效方案，展现了在图像生成和编辑中的广泛应用前景。

Abstract: We present Lay-Your-Scene (shorthand LayouSyn), a novel text-to-layout
generation pipeline for natural scenes. Prior scene layout generation methods
are either closed-vocabulary or use proprietary large language models for
open-vocabulary generation, limiting their modeling capabilities and broader
applicability in controllable image generation. In this work, we propose to use
lightweight open-source language models to obtain scene elements from text
prompts and a novel aspect-aware diffusion Transformer architecture trained in
an open-vocabulary manner for conditional layout generation. Extensive
experiments demonstrate that LayouSyn outperforms existing methods and achieves
state-of-the-art performance on challenging spatial and numerical reasoning
benchmarks. Additionally, we present two applications of LayouSyn. First, we
show that coarse initialization from large language models can be seamlessly
combined with our method to achieve better results. Second, we present a
pipeline for adding objects to images, demonstrating the potential of LayouSyn
in image editing applications.

</details>


### [182] [StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps](https://arxiv.org/abs/2505.05001)
*Lang Nie,Chunyu Lin,Kang Liao,Yun Zhang,Shuaicheng Liu,Yao Zhao*

Main category: cs.CV

TL;DR: StabStitch++是一个针对视频拼接中出现的时间性抖动问题提出的新框架，通过无监督学习同时实现空间拼接和时间稳定化。


<details>
  <summary>Details</summary>
Motivation: 解决视频拼接中因顺序不连贯的扭曲引起的时间性抖动问题，即使输入视频稳定，拼接后的视频仍会产生抖动，影响视觉体验。

Method: 提出StabStitch++框架，包括双向分解模块解耦单应性变换、整合时空扭曲轨迹的数学模型，及平滑模型以混合损失优化内容和轨迹平滑度。

Result: StabStitch++在拼接性能、鲁棒性和效率上超越现有解决方案，实现了实时在线视频拼接系统的显著进步。

Conclusion: StabStitch++不仅不牺牲对齐性能优化稳定化，还通过构建多样化数据集和实时系统，推动了视频拼接领域的进展。

Abstract: We retarget video stitching to an emerging issue, named warping shake, which
unveils the temporal content shakes induced by sequentially unsmooth warps when
extending image stitching to video stitching. Even if the input videos are
stable, the stitched video can inevitably cause undesired warping shakes and
affect the visual experience. To address this issue, we propose StabStitch++, a
novel video stitching framework to realize spatial stitching and temporal
stabilization with unsupervised learning simultaneously. First, different from
existing learning-based image stitching solutions that typically warp one image
to align with another, we suppose a virtual midplane between original image
planes and project them onto it. Concretely, we design a differentiable
bidirectional decomposition module to disentangle the homography transformation
and incorporate it into our spatial warp, evenly spreading alignment burdens
and projective distortions across two views. Then, inspired by camera paths in
video stabilization, we derive the mathematical expression of stitching
trajectories in video stitching by elaborately integrating spatial and temporal
warps. Finally, a warp smoothing model is presented to produce stable stitched
videos with a hybrid loss to simultaneously encourage content alignment,
trajectory smoothness, and online collaboration. Compared with StabStitch that
sacrifices alignment for stabilization, StabStitch++ makes no compromise and
optimizes both of them simultaneously, especially in the online mode. To
establish an evaluation benchmark and train the learning framework, we build a
video stitching dataset with a rich diversity in camera motions and scenes.
Experiments exhibit that StabStitch++ surpasses current solutions in stitching
performance, robustness, and efficiency, offering compelling advancements in
this field by building a real-time online video stitching system.

</details>


### [183] [FG-CLIP: Fine-Grained Visual and Textual Alignment](https://arxiv.org/abs/2505.05071)
*Chunyu Xie,Bin Wang,Fanjing Kong,Jincheng Li,Dawei Liang,Gengshen Zhang,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: FG-CLIP通过改进数据生成和训练方法，增强了CLIP在细粒度理解任务上的表现。


<details>
  <summary>Details</summary>
Motivation: CLIP在多模态任务上表现优异，但在细粒度理解上受限于粗粒度的短标题，因此需要改进。

Method: FG-CLIP利用大模型生成长标题-图像对，构建高质量数据集并加入困难负样本，设计对应训练方法。

Result: FG-CLIP在细粒度理解、开放词汇目标检测等多个任务上优于CLIP和其他先进方法。

Conclusion: FG-CLIP能有效捕捉细粒度细节并提升整体性能，相关资源和模型已开源。

Abstract: Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks
such as image-text retrieval and zero-shot classification but struggles with
fine-grained understanding due to its focus on coarse-grained short captions.
To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances
fine-grained understanding through three key innovations. First, we leverage
large multimodal models to generate 1.6 billion long caption-image pairs for
capturing global-level semantic details. Second, a high-quality dataset is
constructed with 12 million images and 40 million region-specific bounding
boxes aligned with detailed captions to ensure precise, context-rich
representations. Third, 10 million hard fine-grained negative samples are
incorporated to improve the model's ability to distinguish subtle semantic
differences. Corresponding training methods are meticulously designed for these
data. Extensive experiments demonstrate that FG-CLIP outperforms the original
CLIP and other state-of-the-art methods across various downstream tasks,
including fine-grained understanding, open-vocabulary object detection,
image-text retrieval, and general multimodal benchmarks. These results
highlight FG-CLIP's effectiveness in capturing fine-grained image details and
improving overall model performance. The related data, code, and models are
available at https://github.com/360CVGroup/FG-CLIP.

</details>


### [184] [Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language Models](https://arxiv.org/abs/2505.05189)
*Wei Peng,Kang Liu,Jianchen Hu,Meng Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为Biomed-DPT的双模态提示调优技术，通过结合临床知识和注意力重加权提升生物医学图像分类效果。


<details>
  <summary>Details</summary>
Motivation: 现有的提示学习方法主要依赖文本提示，忽视了生物医学图像的特殊结构（如解剖结构和病理特征）。Biomed-DPT旨在通过双模态提示改进这一问题。

Method: 设计了双提示（临床提示和领域适应提示）和零向量软提示，结合知识蒸馏和注意力重加权技术优化模型性能。

Result: 在11个数据集（覆盖9种模态和10种器官）上平均分类准确率达到66.14%，在基类和新类上分别达到78.06%和75.97%，显著优于CoOp方法。

Conclusion: Biomed-DPT通过知识增强和双模态提示，显著提升了生物医学图像分类的准确性和鲁棒性。

Abstract: Prompt learning is one of the most effective paradigms for adapting
pre-trained vision-language models (VLMs) to the biomedical image
classification tasks in few shot scenarios. However, most of the current prompt
learning methods only used the text prompts and ignored the particular
structures (such as the complex anatomical structures and subtle pathological
features) in the biomedical images. In this work, we propose Biomed-DPT, a
knowledge-enhanced dual modality prompt tuning technique. In designing the text
prompt, Biomed-DPT constructs a dual prompt including the template-driven
clinical prompts and the large language model (LLM)-driven domain-adapted
prompts, then extracts the clinical knowledge from the domain-adapted prompts
through the knowledge distillation technique. In designing the vision prompt,
Biomed-DPT introduces the zero vector as a soft prompt to leverage attention
re-weighting so that the focus on non-diagnostic regions and the recognition of
non-critical pathological features are avoided. Biomed-DPT achieves an average
classification accuracy of 66.14\% across 11 biomedical image datasets covering
9 modalities and 10 organs, with performance reaching 78.06\% in base classes
and 75.97\% in novel classes, surpassing the Context Optimization (CoOp) method
by 6.20\%, 3.78\%, and 8.04\%, respectively. Our code are available at
\underline{https://github.com/Kanyooo/Biomed-DPT}.

</details>


### [185] [Automated Thoracolumbar Stump Rib Detection and Analysis in a Large CT Cohort](https://arxiv.org/abs/2505.05004)
*Hendrik Möller,Hanna Schön,Alina Dima,Benjamin Keinert-Weth,Robert Graf,Matan Atad,Johannes Paetzold,Friederike Jungmann,Rickmer Braren,Florian Kofler,Bjoern Menze,Daniel Rueckert,Jan S. Kirschke*

Main category: cs.CV

TL;DR: 该研究开发了一种高分辨率深度学习模型，用于自动化检测胸腰椎残端肋骨，并通过形态学分析量化特征。模型在肋骨分割和长度测量方面表现优异，同时公开了模型权重和掩码供公共使用。


<details>
  <summary>Details</summary>
Motivation: 胸腰椎残端肋骨是胸腰椎过渡椎或计数异常的重要指标，但现有研究多依赖人工评估且定性描述。本研究旨在自动化检测并量化分析其形态特征。

Method: 训练高分辨率深度学习模型进行肋骨分割（迭代算法和分段线性插值评估肋骨长度），并对比形态学差异（如后位附着、厚度、方向等）。

Result: 模型分割准确率显著提升（Dice分数0.997 vs. 0.779），长度测量成功率达98.2%。形态学分析显示残端肋骨更薄、更靠后附着（p<0.01），区分残端肋骨的F1分数达0.84。

Conclusion: 自动化方法能高效检测胸腰椎残端肋骨并提供量化形态特征，模型公开促进相关研究。

Abstract: Thoracolumbar stump ribs are one of the essential indicators of thoracolumbar
transitional vertebrae or enumeration anomalies. While some studies manually
assess these anomalies and describe the ribs qualitatively, this study aims to
automate thoracolumbar stump rib detection and analyze their morphology
quantitatively. To this end, we train a high-resolution deep-learning model for
rib segmentation and show significant improvements compared to existing models
(Dice score 0.997 vs. 0.779, p-value < 0.01). In addition, we use an iterative
algorithm and piece-wise linear interpolation to assess the length of the ribs,
showing a success rate of 98.2%. When analyzing morphological features, we show
that stump ribs articulate more posteriorly at the vertebrae (-19.2 +- 3.8 vs
-13.8 +- 2.5, p-value < 0.01), are thinner (260.6 +- 103.4 vs. 563.6 +- 127.1,
p-value < 0.01), and are oriented more downwards and sideways within the first
centimeters in contrast to full-length ribs. We show that with partially
visible ribs, these features can achieve an F1-score of 0.84 in differentiating
stump ribs from regular ones. We publish the model weights and masks for public
use.

</details>


### [186] [PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes](https://arxiv.org/abs/2505.05288)
*Ahmed Abdelreheem,Filippo Aleotti,Jamie Watson,Zawar Qureshi,Abdelrahman Eldesokey,Peter Wonka,Gabriel Brostow,Sara Vicente,Guillermo Garcia-Hernando*

Main category: cs.CV

TL;DR: 本文提出了一种新任务——语言引导的3D场景物体放置，并建立了相关基准和数据集，为3D LLM模型评估提供新方向。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决语言引导的3D物体放置任务中的模糊性和几何推理挑战，填补现有研究的空白。

Method: 作者提出了一种新基准、评估协议和数据集，并开发了首个非平凡基线方法。

Result: 新任务和基准成为评估3D LLM模型能力的重要工具，为未来研究奠定了基础。

Conclusion: 本文提出的任务和资源将推动3D语言模型的发展，并为相关领域提供新的研究方向。

Abstract: We introduce the novel task of Language-Guided Object Placement in Real 3D
Scenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual
prompt broadly describing where the 3D asset should be placed. The task here is
to find a valid placement for the 3D asset that respects the prompt. Compared
with other language-guided localization tasks in 3D scenes such as grounding,
this task has specific challenges: it is ambiguous because it has multiple
valid solutions, and it requires reasoning about 3D geometric relationships and
free space. We inaugurate this task by proposing a new benchmark and evaluation
protocol. We also introduce a new dataset for training 3D LLMs on this task, as
well as the first method to serve as a non-trivial baseline. We believe that
this challenging task and our new benchmark could become part of the suite of
benchmarks used to evaluate and compare generalist 3D LLM models.

</details>


### [187] [Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects](https://arxiv.org/abs/2505.05318)
*Agnese Chiatti,Sara Bernardini,Lara Shibelski Godoy Piccolo,Viola Schiaffonati,Matteo Matteucci*

Main category: cs.CV

TL;DR: 这篇综述回顾了用户与视觉语言模型（VLM）交互中的信任动态研究，提出了一个多学科分类法，并结合文献和用户研讨会的见解，为未来VLM信任研究提供了初步要求。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLM）的广泛应用，需要保护和告知用户何时信任这些系统，从而推动了对用户-VLM交互中信任动态的研究。

Method: 通过多学科分类法（涵盖认知科学能力、协作模式和代理行为）和用户研讨会的见解，分析了现有研究和用户需求。

Result: 总结了文献和用户研讨会的主要发现，并提出了未来VLM信任研究的初步要求。

Conclusion: 未来的VLM信任研究应结合多学科的见解和用户需求，以更好地理解和改进用户与VLM的信任交互。

Abstract: The rapid adoption of Vision Language Models (VLMs), pre-trained on large
image-text and video-text datasets, calls for protecting and informing users
about when to trust these systems. This survey reviews studies on trust
dynamics in user-VLM interactions, through a multi-disciplinary taxonomy
encompassing different cognitive science capabilities, collaboration modes, and
agent behaviours. Literature insights and findings from a workshop with
prospective VLM users inform preliminary requirements for future VLM trust
studies.

</details>


### [188] [DispBench: Benchmarking Disparity Estimation to Synthetic Corruptions](https://arxiv.org/abs/2505.05091)
*Shashank Agnihotri,Amaan Ansari,Annika Dackermann,Fabian Rösch,Margret Keuper*

Main category: cs.CV

TL;DR: DispBench是一个用于评估视差估计方法可靠性的综合基准工具，针对合成图像损坏和对抗攻击进行了全面的性能与鲁棒性分析。


<details>
  <summary>Details</summary>
Motivation: 深度学习视差估计方法易受分布偏移和对抗攻击影响，缺乏标准化的鲁棒性评估基准，阻碍了该领域的发展。

Method: 提出了DispBench，一个基于合成图像损坏（如对抗攻击和常见2D损坏）和多种数据集的标准化评估工具。

Result: 进行了迄今为止最全面的性能与鲁棒性分析，揭示了精度、可靠性和泛化性之间的关键相关性。

Conclusion: DispBench填补了视差估计领域鲁棒性评估的空白，为未来研究提供了标准化工具和分析框架。

Abstract: Deep learning (DL) has surpassed human performance on standard benchmarks,
driving its widespread adoption in computer vision tasks. One such task is
disparity estimation, estimating the disparity between matching pixels in
stereo image pairs, which is crucial for safety-critical applications like
medical surgeries and autonomous navigation. However, DL-based disparity
estimation methods are highly susceptible to distribution shifts and
adversarial attacks, raising concerns about their reliability and
generalization. Despite these concerns, a standardized benchmark for evaluating
the robustness of disparity estimation methods remains absent, hindering
progress in the field.
  To address this gap, we introduce DispBench, a comprehensive benchmarking
tool for systematically assessing the reliability of disparity estimation
methods. DispBench evaluates robustness against synthetic image corruptions
such as adversarial attacks and out-of-distribution shifts caused by 2D Common
Corruptions across multiple datasets and diverse corruption scenarios. We
conduct the most extensive performance and robustness analysis of disparity
estimation methods to date, uncovering key correlations between accuracy,
reliability, and generalization. Open-source code for DispBench:
https://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation

</details>


### [189] [Feature-Augmented Deep Networks for Multiscale Building Segmentation in High-Resolution UAV and Satellite Imagery](https://arxiv.org/abs/2505.05321)
*Chintan B. Maniyar,Minakshi Kumar,Gengchen Mai*

Main category: cs.CV

TL;DR: 该研究提出了一种结合多分辨率RGB影像、特征增强和优化训练策略的深度学习框架，用于建筑物分割，性能优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 高分辨率RGB影像中建筑物分割因光谱相似性、阴影和不规则几何形状而具有挑战性，需要更鲁棒的解决方案。

Method: 采用Res-U-Net架构，结合PCA、VDVI、MBI和Sobel边缘滤波等特征增强输入，并引入层冻结、循环学习率和超收敛等训练策略。

Result: 模型在WorldView-3影像测试中达到96.5%的准确率、0.86的F1分数和0.80的IoU，优于现有方法。

Conclusion: 特征增强和优化训练策略在多分辨率RGB影像的建筑物分割中表现优异，适用于遥感应用。

Abstract: Accurate building segmentation from high-resolution RGB imagery remains
challenging due to spectral similarity with non-building features, shadows, and
irregular building geometries. In this study, we present a comprehensive deep
learning framework for multiscale building segmentation using RGB aerial and
satellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate
a diverse, multi-sensor dataset and introduce feature-augmented inputs by
deriving secondary representations including Principal Component Analysis
(PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index
(MBI), and Sobel edge filters from RGB channels. These features guide a
Res-U-Net architecture in learning complex spatial patterns more effectively.
We also propose training policies incorporating layer freezing, cyclical
learning rates, and SuperConvergence to reduce training time and resource
usage. Evaluated on a held-out WorldView-3 image, our model achieves an overall
accuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of
0.80, outperforming existing RGB-based benchmarks. This study demonstrates the
effectiveness of combining multi-resolution imagery, feature augmentation, and
optimized training strategies for robust building segmentation in remote
sensing applications.

</details>


### [190] [Threshold Modulation for Online Test-Time Adaptation of Spiking Neural Networks](https://arxiv.org/abs/2505.05375)
*Kejie Zhao,Wenjia Hua,Aiersi Tuerhong,Luziwei Leng,Yuxin Ma,Qinghua Guo*

Main category: cs.CV

TL;DR: 该论文提出了一种名为阈值调制（TM）的在线测试时间适应框架，旨在增强SNN在分布变化下的适应性，同时保持低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的OTTA方法主要针对传统人工神经网络，不适用于SNN。为了填补这一空白，研究团队设计了一个低功耗、适用于神经形态芯片的OTTA框架，以提升SNN在分布变化下的泛化能力。

Method: 提出了基于神经元动力学启发的归一化方法——阈值调制（TM），能够动态调整SNN的放电阈值，与神经形态硬件更兼容。

Result: 在基准数据集上的实验结果显示，该方法显著提升了SNN对分布变化的鲁棒性，同时保持了低计算开销。

Conclusion: TM方法为SNN的在线测试时间适应提供了实用解决方案，并为未来神经形态芯片的设计提供了灵感。

Abstract: Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,
provide highly efficient solutions on edge devices in different scenarios.
However, their ability to adapt to distribution shifts after deployment has
become a crucial challenge. Online test-time adaptation (OTTA) offers a
promising solution by enabling models to dynamically adjust to new data
distributions without requiring source data or labeled target samples.
Nevertheless, existing OTTA methods are largely designed for traditional
artificial neural networks and are not well-suited for SNNs. To address this
gap, we propose a low-power, neuromorphic chip-friendly online test-time
adaptation framework, aiming to enhance model generalization under distribution
shifts. The proposed approach is called Threshold Modulation (TM), which
dynamically adjusts the firing threshold through neuronal dynamics-inspired
normalization, being more compatible with neuromorphic hardware. Experimental
results on benchmark datasets demonstrate the effectiveness of this method in
improving the robustness of SNNs against distribution shifts while maintaining
low computational cost. The proposed method offers a practical solution for
online test-time adaptation of SNNs, providing inspiration for the design of
future neuromorphic chips. The demo code is available at
github.com/NneurotransmitterR/TM-OTTA-SNN.

</details>


### [191] [Probabilistic Embeddings for Frozen Vision-Language Models: Uncertainty Quantification with Gaussian Process Latent Variable Models](https://arxiv.org/abs/2505.05163)
*Aishwarya Venkataramanan,Paul Bodesheim,Joachim Denzler*

Main category: cs.CV

TL;DR: GroVE是一种后处理方法，通过高斯过程潜在变量模型从冻结的视觉语言模型中获取概率嵌入，优化了单模态嵌入重建和跨模态对齐目标，实现了跨多个下游任务的最先进不确定性校准。


<details>
  <summary>Details</summary>
Motivation: 标准视觉语言模型的确定性嵌入难以捕捉视觉和文本描述中的歧义及多模态对应关系的不确定性，现有方法需在大规模数据集上训练概率嵌入，而未能充分利用已训练的大规模模型（如CLIP）的强大表征。

Method: 提出GroVE方法，基于高斯过程潜在变量模型（GPLVM）构建共享低维潜在空间，通过单模态嵌入重建和跨模态对齐目标优化，生成不确定性感知的概率嵌入。

Result: GroVE在跨模态检索、视觉问答和主动学习等下游任务中实现了最优的不确定性校准性能。

Conclusion: GroVE通过后处理方式有效增强了冻结视觉语言模型的不确定性表征能力，为多模态任务提供了更可靠的嵌入表示。

Abstract: Vision-Language Models (VLMs) learn joint representations by mapping images
and text into a shared latent space. However, recent research highlights that
deterministic embeddings from standard VLMs often struggle to capture the
uncertainties arising from the ambiguities in visual and textual descriptions
and the multiple possible correspondences between images and texts. Existing
approaches tackle this by learning probabilistic embeddings during VLM
training, which demands large datasets and does not leverage the powerful
representations already learned by large-scale VLMs like CLIP. In this paper,
we propose GroVE, a post-hoc approach to obtaining probabilistic embeddings
from frozen VLMs. GroVE builds on Gaussian Process Latent Variable Model
(GPLVM) to learn a shared low-dimensional latent space where image and text
inputs are mapped to a unified representation, optimized through single-modal
embedding reconstruction and cross-modal alignment objectives. Once trained,
the Gaussian Process model generates uncertainty-aware probabilistic
embeddings. Evaluation shows that GroVE achieves state-of-the-art uncertainty
calibration across multiple downstream tasks, including cross-modal retrieval,
visual question answering, and active learning.

</details>


### [192] [PaniCar: Securing the Perception of Advanced Driving Assistance Systems Against Emergency Vehicle Lighting](https://arxiv.org/abs/2505.05183)
*Elad Feldman,Jacob Shams,Dudi Biton,Alfred Chen,Shaoyuan Xie,Satoru Koda,Yisroel Mirsky,Asaf Shabtai,Yuval Elovici,Ben Nassi*

Main category: cs.CV

TL;DR: 该研究揭示了PaniCar现象，即紧急车辆灯光会导致自动驾驶汽车的对象检测器置信度波动，从而无法检测到紧急车辆附近物体。研究者评估了七种商业ADAS系统、四种对象检测算法及14种紧急车辆灯光模式，并提出解决方案Caracetamol，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 近年来，自动驾驶汽车的安全性备受关注，尤其是特斯拉Autopilot在多起与停泊紧急车辆相撞的事故中暴露问题。研究旨在探究紧急车辆灯光对对象检测器的潜在影响及其安全风险。

Method: 评估了七种商业ADAS系统（如特斯拉Model 3）、四种对象检测算法（YOLO、SSD等）及14种紧急车辆灯光模式，分析了技术与环境因素。同时测试了四种先进的眩光去除方法，并提出新框架Caracetamol以增强检测器对紧急车辆灯光的鲁棒性。

Result: Caracetamol在YOLOv3和Faster RCNN上显著提升检测置信度（平均提升0.20）、下限置信度（提升0.33），并减少波动范围（降低0.33），且实时处理能力达30-50 FPS。现有眩光去除方法无法满足实时驾驶需求。

Conclusion: PaniCar现象揭示了紧急车辆灯光对自动驾驶系统的潜在威胁，Caracetamol框架为缓解这一风险提供了可行方案，但需进一步优化以适应更多场景。

Abstract: The safety of autonomous cars has come under scrutiny in recent years,
especially after 16 documented incidents involving Teslas (with autopilot
engaged) crashing into parked emergency vehicles (police cars, ambulances, and
firetrucks). While previous studies have revealed that strong light sources
often introduce flare artifacts in the captured image, which degrade the image
quality, the impact of flare on object detection performance remains unclear.
In this research, we unveil PaniCar, a digital phenomenon that causes an object
detector's confidence score to fluctuate below detection thresholds when
exposed to activated emergency vehicle lighting. This vulnerability poses a
significant safety risk, and can cause autonomous vehicles to fail to detect
objects near emergency vehicles. In addition, this vulnerability could be
exploited by adversaries to compromise the security of advanced driving
assistance systems (ADASs). We assess seven commercial ADASs (Tesla Model 3,
"manufacturer C", HP, Pelsee, AZDOME, Imagebon, Rexing), four object detectors
(YOLO, SSD, RetinaNet, Faster R-CNN), and 14 patterns of emergency vehicle
lighting to understand the influence of various technical and environmental
factors. We also evaluate four SOTA flare removal methods and show that their
performance and latency are insufficient for real-time driving constraints. To
mitigate this risk, we propose Caracetamol, a robust framework designed to
enhance the resilience of object detectors against the effects of activated
emergency vehicle lighting. Our evaluation shows that on YOLOv3 and Faster
RCNN, Caracetamol improves the models' average confidence of car detection by
0.20, the lower confidence bound by 0.33, and reduces the fluctuation range by
0.33. In addition, Caracetamol is capable of processing frames at a rate of
between 30-50 FPS, enabling real-time ADAS car detection.

</details>


### [193] [Flow-GRPO: Training Flow Matching Models via Online RL](https://arxiv.org/abs/2505.05470)
*Jie Liu,Gongye Liu,Jiajun Liang,Yangguang Li,Jiaheng Liu,Xintao Wang,Pengfei Wan,Di Zhang,Wanli Ouyang*

Main category: cs.CV

TL;DR: Flow-GRPO是首个将在线强化学习（RL）融入流匹配模型的方法，通过ODE-to-SDE转换和降噪策略显著提升采样效率和性能。在多项任务中表现优异，如文本到图像生成，显著提升了准确率和人类偏好对齐。


<details>
  <summary>Details</summary>
Motivation: 将强化学习与流匹配模型结合，旨在解决传统方法在复杂任务（如文本到图像生成）中采样效率低和性能不足的问题。

Method: 采用两种策略：(1) ODE-to-SDE转换，实现统计采样以支持RL探索；(2) 降噪策略，减少训练步骤同时保持推理步骤，提升效率。

Result: 在复杂构图和视觉文本渲染任务中，准确率从63%提升至95%，文本生成从59%提升至92%，且人类偏好对齐显著改善，无奖励滥用现象。

Conclusion: Flow-GRPO通过创新策略有效结合RL与流匹配模型，显著提升了生成任务的性能和效率，同时保持了图像质量与多样性。

Abstract: We propose Flow-GRPO, the first method integrating online reinforcement
learning (RL) into flow matching models. Our approach uses two key strategies:
(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary
Differential Equation (ODE) into an equivalent Stochastic Differential Equation
(SDE) that matches the original model's marginal distribution at all timesteps,
enabling statistical sampling for RL exploration; and (2) a Denoising Reduction
strategy that reduces training denoising steps while retaining the original
inference timestep number, significantly improving sampling efficiency without
performance degradation. Empirically, Flow-GRPO is effective across multiple
text-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly
perfect object counts, spatial relations, and fine-grained attributes, boosting
GenEval accuracy from $63\%$ to $95\%$. In visual text rendering, its accuracy
improves from $59\%$ to $92\%$, significantly enhancing text generation.
Flow-GRPO also achieves substantial gains in human preference alignment.
Notably, little to no reward hacking occurred, meaning rewards did not increase
at the cost of image quality or diversity, and both remained stable in our
experiments.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [194] [Incentive-Aware Machine Learning; Robustness, Fairness, Improvement & Causality](https://arxiv.org/abs/2505.05211)
*Chara Podimata*

Main category: cs.GT

TL;DR: 该论文探讨了激励感知机器学习（ML），聚焦于个体可策略性修改输入以影响结果的算法决策，并分为稳健性、公平性和改进/因果性三个视角。


<details>
  <summary>Details</summary>
Motivation: 研究个体策略性行为对ML系统的影响，旨在设计更稳健、公平且能区分真实改进与操纵的系统。

Method: 提出统一框架，涵盖离线、在线和因果设置，综合分析现有工作以解决游戏与改进的区分及智能体异质性等挑战。

Result: 总结理论进展与实践方案，为激励感知ML系统提供稳健性、公平性和因果性指导。

Conclusion: 论文整合多视角研究，为未来开发抗操纵、公平且能促进真实改进的ML系统奠定基础。

Abstract: The article explores the emerging domain of incentive-aware machine learning
(ML), which focuses on algorithmic decision-making in contexts where
individuals can strategically modify their inputs to influence outcomes. It
categorizes the research into three perspectives: robustness, aiming to design
models resilient to "gaming"; fairness, analyzing the societal impacts of such
systems; and improvement/causality, recognizing situations where strategic
actions lead to genuine personal or societal improvement. The paper introduces
a unified framework encapsulating models for these perspectives, including
offline, online, and causal settings, and highlights key challenges such as
differentiating between gaming and improvement and addressing heterogeneity
among agents. By synthesizing findings from diverse works, we outline
theoretical advancements and practical solutions for robust, fair, and
causally-informed incentive-aware ML systems.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [195] [Moments of Causal Effects](https://arxiv.org/abs/2505.04971)
*Yuta Kawakami,Jin Tian*

Main category: stat.ME

TL;DR: 本文研究了因果效应的矩和乘积矩，提出了定义、识别定理和边界，并通过实验展示了如何从有限样本中估计这些矩及其在医学数据集中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统的因果效应评估主要关注平均因果效应，但缺乏对因果效应分布形状和变量关系的全面分析。本文旨在填补这一空白，通过矩和乘积矩来描述因果效应的分布和相关性。

Method: 提出了因果效应的矩和乘积矩的定义及识别定理，推导了其边界，并通过实验从有限样本中估计这些矩，同时使用真实医学数据集进行验证。

Result: 实验结果表明，提出的方法能够有效估计因果效应的矩和乘积矩，并在实际数据中展示了其应用价值。

Conclusion: 本文扩展了因果效应的分析框架，通过矩和乘积矩提供了更全面的分布和相关性描述，为实际应用提供了新的工具。

Abstract: The moments of random variables are fundamental statistical measures for
characterizing the shape of a probability distribution, encompassing metrics
such as mean, variance, skewness, and kurtosis. Additionally, the product
moments, including covariance and correlation, reveal the relationships between
multiple random variables. On the other hand, the primary focus of causal
inference is the evaluation of causal effects, which are defined as the
difference between two potential outcomes. While traditional causal effect
assessment focuses on the average causal effect, this work provides
definitions, identification theorems, and bounds for moments and product
moments of causal effects to analyze their distribution and relationships. We
conduct experiments to illustrate the estimation of the moments of causal
effects from finite samples and demonstrate their practical application using a
real-world medical dataset.

</details>


### [196] [Decomposition of Probabilities of Causation with Two Mediators](https://arxiv.org/abs/2505.04983)
*Yuta Kawakami,Jin Tian*

Main category: stat.ME

TL;DR: 该研究提出了一种通过路径特异概率（PNS）分解总效应的方法，用于评估治疗通过不同因果路径的必要性和充分性，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决因果中介分析中的一个核心问题，即如何将总效应分解为沿不同因果路径的路径特异分量，尤其是通过两个中介变量的情况。

Method: 通过定义路径特异PNS并提供一个识别定理，结合数值实验验证估计量的性质，并应用于真实教育数据集。

Result: 研究成功分解了总PNS为路径特异分量，并通过实验证明了所提估计方法的可行性和实用性。

Conclusion: 该研究为因果中介分析中的路径特异PNS分解提供了理论基础和实用工具，具有广泛的应用潜力。

Abstract: Mediation analysis for probabilities of causation (PoC) provides a
fundamental framework for evaluating the necessity and sufficiency of treatment
in provoking an event through different causal pathways. One of the primary
objectives of causal mediation analysis is to decompose the total effect into
path-specific components. In this study, we investigate the path-specific
probability of necessity and sufficiency (PNS) to decompose the total PNS into
path-specific components along distinct causal pathways between treatment and
outcome, incorporating two mediators. We define the path-specific PNS for
decomposition and provide an identification theorem. Furthermore, we conduct
numerical experiments to assess the properties of the proposed estimators from
finite samples and demonstrate their practical application using a real-world
educational dataset.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [197] [D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation](https://arxiv.org/abs/2505.04860)
*I-Chun Arthur Liu,Jason Chen,Gaurav Sukhatme,Daniel Seita*

Main category: cs.RO

TL;DR: 论文提出了一种针对双手操作的数据增强方法D-CODA，通过扩散模型生成视角一致的双手腕部摄像头图像及动作标签，提升模仿学习的扩展性。


<details>
  <summary>Details</summary>
Motivation: 双手操作因高维度和双臂协调需求而复杂，现有单臂视觉增强方法难以直接扩展至双手操作，D-CODA旨在解决这一问题。

Method: D-CODA采用扩散模型生成双手视角一致的图像和动作标签，并通过约束优化确保接触状态的可行性。

Result: 在5个模拟和3个现实任务中，D-CODA在2250次模拟和300次现实试验中均优于基线方法。

Conclusion: D-CODA展示了在双手模仿学习中高效数据增强的潜力，支持复杂任务的扩展学习。

Abstract: Learning bimanual manipulation is challenging due to its high dimensionality
and tight coordination required between two arms. Eye-in-hand imitation
learning, which uses wrist-mounted cameras, simplifies perception by focusing
on task-relevant views. However, collecting diverse demonstrations remains
costly, motivating the need for scalable data augmentation. While prior work
has explored visual augmentation in single-arm settings, extending these
approaches to bimanual manipulation requires generating viewpoint-consistent
observations across both arms and producing corresponding action labels that
are both valid and feasible. In this work, we propose Diffusion for COordinated
Dual-arm Data Augmentation (D-CODA), a method for offline data augmentation
tailored to eye-in-hand bimanual imitation learning that trains a diffusion
model to synthesize novel, viewpoint-consistent wrist-camera images for both
arms while simultaneously generating joint-space action labels. It employs
constrained optimization to ensure that augmented states involving
gripper-to-object contacts adhere to constraints suitable for bimanual
coordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our
results across 2250 simulation trials and 300 real-world trials demonstrate
that it outperforms baselines and ablations, showing its potential for scalable
data augmentation in eye-in-hand bimanual manipulation. Our project website is
at: https://dcodaaug.github.io/D-CODA/.

</details>


### [198] [X-Driver: Explainable Autonomous Driving with Vision-Language Models](https://arxiv.org/abs/2505.05098)
*Wei Liu,Jiyuan Zhang,Binxiong Zheng,Yufeng Hu,Yingzhan Lin,Zengfeng Zeng*

Main category: cs.RO

TL;DR: 该论文提出了X-Driver，一种基于多模态大语言模型（MLLMs）的端到端自动驾驶框架，通过链式思维（CoT）和自回归建模提升感知与决策能力，在CARLA仿真环境中验证了其闭环性能优于当前最优方法。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶框架在闭环评测中成功率较低，难以实际部署，因此需要一种更能提升闭环性能与决策可解释性的方法。

Method: 利用多模态大语言模型（MLLMs）结合链式思维（CoT）和自回归建模，统一感知与决策流程。

Result: 在CARLA仿真环境（包括Bench2Drive）中，X-Driver的闭环性能超越当前SOTA，同时提高了驾驶决策的可解释性。

Conclusion: 结构化推理对端到端驾驶至关重要，X-Driver为闭环自动驾驶研究提供了强基线。

Abstract: End-to-end autonomous driving has advanced significantly, offering benefits
such as system simplicity and stronger driving performance in both open-loop
and closed-loop settings than conventional pipelines. However, existing
frameworks still suffer from low success rates in closed-loop evaluations,
highlighting their limitations in real-world deployment. In this paper, we
introduce X-Driver, a unified multi-modal large language models(MLLMs)
framework designed for closed-loop autonomous driving, leveraging
Chain-of-Thought(CoT) and autoregressive modeling to enhance perception and
decision-making. We validate X-Driver across multiple autonomous driving tasks
using public benchmarks in CARLA simulation environment, including
Bench2Drive[6]. Our experimental results demonstrate superior closed-loop
performance, surpassing the current state-of-the-art(SOTA) while improving the
interpretability of driving decisions. These findings underscore the importance
of structured reasoning in end-to-end driving and establish X-Driver as a
strong baseline for future research in closed-loop autonomous driving.

</details>


### [199] [AI and Vision based Autonomous Navigation of Nano-Drones in Partially-Known Environments](https://arxiv.org/abs/2505.04972)
*Mattia Sartori,Chetna Singhal,Neelabhro Roy,Davide Brunelli,James Gross*

Main category: cs.RO

TL;DR: 论文提出了一种基于视觉的AI辅助反应式规划方法，用于30克重的Crazyflie 2.1纳米无人机在部分已知环境中的安全自主飞行，通过将导航任务分为边缘计算（深度学习对象检测）和机载规划，实现了8帧/秒的处理速度，证明了实时导航的可行性。


<details>
  <summary>Details</summary>
Motivation: 纳米无人机的资源有限，但在物联网机器人生态系统中具有巨大潜力，作者旨在解决其在部分已知环境中安全自主飞行的挑战。

Method: 提出了一种AI辅助的视觉反应式规划方法，将导航任务分为边缘深度学习对象检测和机载规划执行。

Result: 无人机实现了8帧/秒的指令处理速度和60.8的COCO mAP性能，现场测试中能以1米/秒的速度避开障碍物并到达目标。

Conclusion: 该方法为纳米无人机的实时自主导航提供了可行方案，并可扩展至自主探索任务。

Abstract: The miniaturisation of sensors and processors, the advancements in connected
edge intelligence, and the exponential interest in Artificial Intelligence are
boosting the affirmation of autonomous nano-size drones in the Internet of
Robotic Things ecosystem. However, achieving safe autonomous navigation and
high-level tasks such as exploration and surveillance with these tiny platforms
is extremely challenging due to their limited resources. This work focuses on
enabling the safe and autonomous flight of a pocket-size, 30-gram platform
called Crazyflie 2.1 in a partially known environment. We propose a novel
AI-aided, vision-based reactive planning method for obstacle avoidance under
the ambit of Integrated Sensing, Computing and Communication paradigm. We deal
with the constraints of the nano-drone by splitting the navigation task into
two parts: a deep learning-based object detector runs on the edge (external
hardware) while the planning algorithm is executed onboard. The results show
the ability to command the drone at $\sim8$ frames-per-second and a model
performance reaching a COCO mean-average-precision of $60.8$. Field experiments
demonstrate the feasibility of the solution with the drone flying at a top
speed of $1$ m/s while steering away from an obstacle placed in an unknown
position and reaching the target destination. The outcome highlights the
compatibility of the communication delay and the model performance with the
requirements of the real-time navigation task. We provide a feasible
alternative to a fully onboard implementation that can be extended to
autonomous exploration with nano-drones.

</details>


### [200] [Steerable Scene Generation with Post Training and Inference-Time Search](https://arxiv.org/abs/2505.04831)
*Nicholas Pfaff,Hongkai Dai,Sergey Zakharov,Shun Iwase,Russ Tedrake*

Main category: cs.RO

TL;DR: 论文提出了一种基于扩散模型的统一生成方法，用于大规模生成适应机器人操作任务的3D场景，通过强化学习后训练、条件生成或推断时搜索调整生成方向，并发布了包含4400万个SE(3)场景的数据集。


<details>
  <summary>Details</summary>
Motivation: 手动标注满足机器人任务要求的高杂乱环境3D场景成本高且稀缺。论文旨在通过程序化生成方法快速生成符合物理可行性的多样化场景。

Method: 使用扩散模型预测物体及位姿（SE(3)），结合强化学习后训练、条件生成或MCTS推断时搜索优化生成方向，并通过投影和仿真确保物理可行性。

Result: 生成了一个包含4400万SE(3)场景的数据集，覆盖五类环境，生成场景能适应下游任务目标并保持物理合理性。

Conclusion: 该方法实现了目标导向的场景合成，可扩展到多种场景类型，为机器人仿真训练提供了高效的场景生成工具。

Abstract: Training robots in simulation requires diverse 3D scenes that reflect the
specific challenges of downstream tasks. However, scenes that satisfy strict
task requirements, such as high-clutter environments with plausible spatial
arrangement, are rare and costly to curate manually. Instead, we generate
large-scale scene data using procedural models that approximate realistic
environments for robotic manipulation, and adapt it to task-specific goals. We
do this by training a unified diffusion-based generative model that predicts
which objects to place from a fixed asset library, along with their SE(3)
poses. This model serves as a flexible scene prior that can be adapted using
reinforcement learning-based post training, conditional generation, or
inference-time search, steering generation toward downstream objectives even
when they differ from the original data distribution. Our method enables
goal-directed scene synthesis that respects physical feasibility and scales
across scene types. We introduce a novel MCTS-based inference-time search
strategy for diffusion models, enforce feasibility via projection and
simulation, and release a dataset of over 44 million SE(3) scenes spanning five
diverse environments. Website with videos, code, data, and model weights:
https://steerable-scene-generation.github.io/

</details>


### [201] [CubeDAgger: Improved Robustness of Interactive Imitation Learning without Violation of Dynamic Stability](https://arxiv.org/abs/2505.04897)
*Taisuke Kobayashi*

Main category: cs.RO

TL;DR: 论文提出CubeDAgger方法，通过改进EnsembleDAgger基线方法，提升交互模仿学习的鲁棒性并减少动态稳定性破坏。


<details>
  <summary>Details</summary>
Motivation: 现有交互模仿学习算法通过专家-代理切换系统减少专家负担，但难以精确选择监督时机且切换会导致动作突变，破坏动态稳定性。

Method: CubeDAgger对EnsembleDAgger进行三点改进：1) 添加正则化明确激活监督时机的阈值；2) 将专家-代理切换系统转为多动作候选的最优共识系统；3) 引入自回归彩色噪声使随机探索在时间上一致。

Result: 仿真验证显示，所学策略在交互中保持动态稳定性的同时具备足够鲁棒性。

Conclusion: CubeDAgger通过改进监督时机决策和动作平滑性，有效平衡了鲁棒性和动态稳定性。

Abstract: Interactive imitation learning makes an agent's control policy robust by
stepwise supervisions from an expert. The recent algorithms mostly employ
expert-agent switching systems to reduce the expert's burden by limitedly
selecting the supervision timing. However, the precise selection is difficult
and such a switching causes abrupt changes in actions, damaging the dynamic
stability. This paper therefore proposes a novel method, so-called CubeDAgger,
which improves robustness while reducing dynamic stability violations by making
three improvements to a baseline method, EnsembleDAgger. The first improvement
adds a regularization to explicitly activate the threshold for deciding the
supervision timing. The second transforms the expert-agent switching system to
an optimal consensus system of multiple action candidates. Third,
autoregressive colored noise to the actions is introduced to make the
stochastic exploration consistent over time. These improvements are verified by
simulations, showing that the learned policies are sufficiently robust while
maintaining dynamic stability during interaction.

</details>


### [202] [CLAM: Continuous Latent Action Models for Robot Learning from Unlabeled Demonstrations](https://arxiv.org/abs/2505.04999)
*Anthony Liang,Pavel Czempin,Matthew Hong,Yutai Zhou,Erdem Biyik,Stephen Tu*

Main category: cs.RO

TL;DR: 论文提出了一种名为CLAM的连续潜在动作模型，通过利用未标注的观察数据（如视频演示）来学习潜在动作标签，解决了模仿学习中需要大量标注专家演示数据的问题。该方法在复杂连续控制任务中表现优异，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 模仿学习需要大量标注的专家演示数据，这限制了训练数据的规模。为了解决这一问题，论文提出利用未标注的观察数据（如视频演示）来学习潜在动作标签，从而减少对标注数据的依赖。

Method: 论文设计了连续潜在动作模型（CLAM），该方法结合了两个关键要素：(a) 使用连续潜在动作标签而非离散表示，(b) 联合训练动作解码器以确保潜在动作空间可以轻松地用少量标注样本与实际动作关联。标注样本可以来自非最优的演示数据，从而避免了专家数据的需求。

Result: 在DMControl（运动）和MetaWorld（操作）等连续控制基准测试中，以及真实的WidowX机械臂上，CLAM显著优于现有方法，任务成功率提高了2-3倍。

Conclusion: CLAM证明了通过未标注数据和少量非最优标注数据学习高性能策略的可行性，为模仿学习提供了一种高效的数据利用方法。

Abstract: Learning robot policies using imitation learning requires collecting large
amounts of costly action-labeled expert demonstrations, which fundamentally
limits the scale of training data. A promising approach to address this
bottleneck is to harness the abundance of unlabeled observations-e.g., from
video demonstrations-to learn latent action labels in an unsupervised way.
However, we find that existing methods struggle when applied to complex robot
tasks requiring fine-grained motions. We design continuous latent action models
(CLAM) which incorporate two key ingredients we find necessary for learning to
solve complex continuous control tasks from unlabeled observation data: (a)
using continuous latent action labels instead of discrete representations, and
(b) jointly training an action decoder to ensure that the latent action space
can be easily grounded to real actions with relatively few labeled examples.
Importantly, the labeled examples can be collected from non-optimal play data,
enabling CLAM to learn performant policies without access to any action-labeled
expert data. We demonstrate on continuous control benchmarks in DMControl
(locomotion) and MetaWorld (manipulation), as well as on a real WidowX robot
arm that CLAM significantly outperforms prior state-of-the-art methods,
remarkably with a 2-3x improvement in task success rate compared to the best
baseline. Videos and code can be found at clamrobot.github.io.

</details>


### [203] [Multi-Objective Reinforcement Learning for Adaptive Personalized Autonomous Driving](https://arxiv.org/abs/2505.05223)
*Hendrik Surmann,Jorge de Heuvel,Maren Bennewitz*

Main category: cs.RO

TL;DR: 该论文提出了一种基于多目标强化学习（MORL）的自动驾驶方法，通过偏好驱动优化实现运行时动态调整驾驶风格，无需重新训练策略。


<details>
  <summary>Details</summary>
Motivation: 人类驾驶员有独特的驾驶风格偏好，现有端到端自动驾驶方法多依赖预设风格或持续用户反馈，难以支持动态、上下文相关的偏好。

Method: 采用多目标强化学习（MORL）框架，将偏好编码为连续权重向量，调节效率、舒适度、速度和攻击性等可解释目标。模型集成视觉感知，在CARLA模拟器中测试。

Result: 实验显示，智能体能根据动态偏好调整驾驶行为，同时在碰撞避免和路径完成方面保持性能。

Conclusion: 该方法能运行时适应驾驶风格偏好，提升了自动驾驶的用户信任和满意度，且无需策略重新训练。

Abstract: Human drivers exhibit individual preferences regarding driving style.
Adapting autonomous vehicles to these preferences is essential for user trust
and satisfaction. However, existing end-to-end driving approaches often rely on
predefined driving styles or require continuous user feedback for adaptation,
limiting their ability to support dynamic, context-dependent preferences. We
propose a novel approach using multi-objective reinforcement learning (MORL)
with preference-driven optimization for end-to-end autonomous driving that
enables runtime adaptation to driving style preferences. Preferences are
encoded as continuous weight vectors to modulate behavior along interpretable
style objectives$\unicode{x2013}$including efficiency, comfort, speed, and
aggressiveness$\unicode{x2013}$without requiring policy retraining. Our
single-policy agent integrates vision-based perception in complex mixed-traffic
scenarios and is evaluated in diverse urban environments using the CARLA
simulator. Experimental results demonstrate that the agent dynamically adapts
its driving behavior according to changing preferences while maintaining
performance in terms of collision avoidance and route completion.

</details>


### [204] [Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation](https://arxiv.org/abs/2505.05287)
*Zechu Li,Yufeng Jin,Daniel Ordonez Apraez,Claudio Semini,Puze Liu,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: 提出了SYMDEX，一个利用机器人固有双边对称性的强化学习框架，用于双手灵巧操作。通过分解任务为单臂子任务并利用对称性网络，提升了样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 人类能自然地在左右手间镜像动作，而机器人若能高效实现双手灵巧操作（ambidextrous manipulation），可提升任务灵活性。

Method: 基于双边对称性，分解复杂双手任务为单臂子任务，训练专用于每手的策略，并通过对称性网络共享经验。最后将子策略提炼为全局策略。

Result: 在6个模拟任务和2个实际任务中表现优于基线，尤其在左右手分工不同的复杂任务中。扩展至四臂操作也证明其可扩展性。

Conclusion: 结构对称性作为策略学习的归纳偏置，显著提升了样本效率、鲁棒性和任务泛化能力。

Abstract: Humans naturally exhibit bilateral symmetry in their gross manipulation
skills, effortlessly mirroring simple actions between left and right hands.
Bimanual robots-which also feature bilateral symmetry-should similarly exploit
this property to perform tasks with either hand. Unlike humans, who often favor
a dominant hand for fine dexterous skills, robots should ideally execute
ambidextrous manipulation with equal proficiency. To this end, we introduce
SYMDEX (SYMmetric DEXterity), a reinforcement learning framework for
ambidextrous bi-manipulation that leverages the robot's inherent bilateral
symmetry as an inductive bias. SYMDEX decomposes complex bimanual manipulation
tasks into per-hand subtasks and trains dedicated policies for each. By
exploiting bilateral symmetry via equivariant neural networks, experience from
one arm is inherently leveraged by the opposite arm. We then distill the
subtask policies into a global ambidextrous policy that is independent of the
hand-task assignment. We evaluate SYMDEX on six challenging simulated
manipulation tasks and demonstrate successful real-world deployment on two of
them. Our approach strongly outperforms baselines on complex task in which the
left and right hands perform different roles. We further demonstrate SYMDEX's
scalability by extending it to a four-arm manipulation setup, where our
symmetry-aware policies enable effective multi-arm collaboration and
coordination. Our results highlight how structural symmetry as inductive bias
in policy learning enhances sample efficiency, robustness, and generalization
across diverse dexterous manipulation tasks.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [205] [Multimodal Benchmarking and Recommendation of Text-to-Image Generation Models](https://arxiv.org/abs/2505.04650)
*Kapil Wanaskar,Gaytri Jena,Magdalini Eirinaki*

Main category: cs.GR

TL;DR: 该研究提出了一个开源统一的文本到图像生成模型评估框架，特别关注元数据增强提示的影响。通过多维度量化指标和定性分析，证明元数据增强显著提升生成图像的视觉真实性、语义保真度和模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过统一的评估框架量化分析元数据增强对文本到图像生成模型的影响，为模型选择和提示设计提供针对性建议。

Method: 基于DeepFashion-MultiModal数据集，结合加权分数、CLIP相似度、LPIPS、FID和检索指标等量化指标，以及定性分析评估生成结果。

Result: 元数据增强显著提升了生成图像的视觉真实性、语义保真度和模型鲁棒性，且框架能为模型选择和提示设计提供任务特异性建议。

Conclusion: 提出的框架通过系统化评估证明了元数据增强的有效性，为文本到图像生成模型的优化和选择提供了实用工具。

Abstract: This work presents an open-source unified benchmarking and evaluation
framework for text-to-image generation models, with a particular focus on the
impact of metadata augmented prompts. Leveraging the DeepFashion-MultiModal
dataset, we assess generated outputs through a comprehensive set of
quantitative metrics, including Weighted Score, CLIP (Contrastive Language
Image Pre-training)-based similarity, LPIPS (Learned Perceptual Image Patch
Similarity), FID (Frechet Inception Distance), and retrieval-based measures, as
well as qualitative analysis. Our results demonstrate that structured metadata
enrichments greatly enhance visual realism, semantic fidelity, and model
robustness across diverse text-to-image architectures. While not a traditional
recommender system, our framework enables task-specific recommendations for
model selection and prompt design based on evaluation metrics.

</details>


### [206] [ChannelExplorer: Exploring Class Separability Through Activation Channel Visualization](https://arxiv.org/abs/2505.04647)
*Md Rahat-uz- Zaman,Bei Wang,Paul Rosen*

Main category: cs.GR

TL;DR: 论文提出ChannelExplorer工具，通过可视化分析DNN各层激活通道对类别可分离性的贡献，支持多种模型架构，并在实际场景中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNN）在视觉任务中表现优异，但其内部行为（如各层和激活通道对类别可分离性的影响）仍难以理解。现有方法侧重架构分析，缺乏数据驱动的探索手段。

Method: 开发交互式可视化工具ChannelExplorer，包含三类视图：散点图（揭示类间/类内混淆）、Jaccard相似度（量化激活重叠）、热力图（检查通道模式），支持CNN、GAN、ResNet等模型。

Result: 工具在四个场景中验证能力：生成ImageNet类层次、发现错误标注图像、识别通道贡献、定位Stable Diffusion潜在状态，并通过专家评估验证实用性。

Conclusion: ChannelExplorer为理解DNN内部行为提供数据驱动的新视角，通用性强且用户评价积极，未来可扩展至更多模型和分析任务。

Abstract: Deep neural networks (DNNs) achieve state-of-the-art performance in many
vision tasks, yet understanding their internal behavior remains challenging,
particularly how different layers and activation channels contribute to class
separability. We introduce ChannelExplorer, an interactive visual analytics
tool for analyzing image-based outputs across model layers, emphasizing
data-driven insights over architecture analysis for exploring class
separability. ChannelExplorer summarizes activations across layers and
visualizes them using three primary coordinated views: a Scatterplot View to
reveal inter- and intra-class confusion, a Jaccard Similarity View to quantify
activation overlap, and a Heatmap View to inspect activation channel patterns.
Our technique supports diverse model architectures, including CNNs, GANs,
ResNet and Stable Diffusion models. We demonstrate the capabilities of
ChannelExplorer through four use-case scenarios: (1) generating class hierarchy
in ImageNet, (2) finding mislabeled images, (3) identifying activation channel
contributions, and(4) locating latent states' position in Stable Diffusion
model. Finally, we evaluate the tool with expert users.

</details>


### [207] [ADD: Physics-Based Motion Imitation with Adversarial Differential Discriminators](https://arxiv.org/abs/2505.04961)
*Ziyu Zhang,Sergey Bashkirov,Dun Yang,Michael Taylor,Xue Bin Peng*

Main category: cs.GR

TL;DR: 该论文提出了一种新颖的对抗性多目标优化技术，不依赖人工调优的聚合函数，适用于包括运动跟踪在内的多种多目标优化问题，并在模拟角色中实现了高保真效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于人工调整的聚合函数，过程繁琐且依赖专家经验，限制了其跨技能应用的广泛性。因此，研究一种无需手动调优的优化方法至关重要。

Method: 采用了对抗性多目标优化技术，通过对抗式差异判别器（接收单一样本）引导优化过程，避免人工设计奖励函数。

Result: 该方法使模拟角色能高保真复现多种灵活动作，效果媲美现有最优方法，且无需人工调优。

Conclusion: 该技术在多目标优化问题中具有广泛适用性，尤其在运动跟踪领域展现了高效与灵活性，推动了自动化优化的发展。

Abstract: Multi-objective optimization problems, which require the simultaneous
optimization of multiple terms, are prevalent across numerous applications.
Existing multi-objective optimization methods often rely on manually tuned
aggregation functions to formulate a joint optimization target. The performance
of such hand-tuned methods is heavily dependent on careful weight selection, a
time-consuming and laborious process. These limitations also arise in the
setting of reinforcement-learning-based motion tracking for physically
simulated characters, where intricately crafted reward functions are typically
used to achieve high-fidelity results. Such solutions not only require domain
expertise and significant manual adjustment, but also limit the applicability
of the resulting reward function across diverse skills. To bridge this gap, we
present a novel adversarial multi-objective optimization technique that is
broadly applicable to a range of multi-objective optimization problems,
including motion tracking. The proposed adversarial differential discriminator
receives a single positive sample, yet is still effective at guiding the
optimization process. We demonstrate that our technique can enable characters
to closely replicate a variety of acrobatic and agile behaviors, achieving
comparable quality to state-of-the-art motion-tracking methods, without relying
on manually tuned reward functions. Results are best visualized through
https://youtu.be/rz8BYCE9E2w.

</details>


### [208] [Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields](https://arxiv.org/abs/2505.05356)
*Runfeng Li,Mikhail Okunev,Zixuan Guo,Anh Ha Duong,Christian Richardt,Matthew O'Toole,James Tompkin*

Main category: cs.GR

TL;DR: 本文提出了一种从单目连续波飞行时间相机（C-ToF）重建动态场景的方法，比神经体积方法快100倍，且精度相当或更高。


<details>
  <summary>Details</summary>
Motivation: 快速从单一视角实现高保真动态3D重建是计算机视觉中的重大挑战，尤其在C-ToF光场重建中，深度信息无法直接测量，导致优化问题更加复杂。

Method: 通过将两种启发式方法融入基于3D高斯抛射的快速原始场景表示优化中，提升了高斯几何的精度。

Result: 实验结果表明，该方法在受限的C-ToF传感条件下（如快速运动的棒球挥动）仍能生成精确的重建结果。

Conclusion: 该方法在动态场景重建中实现了高效与高精度的平衡，展现了在复杂条件下的鲁棒性。

Abstract: We present a method to reconstruct dynamic scenes from monocular
continuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that
achieves similar or better accuracy than neural volumetric approaches and is
100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a
single viewpoint is a significant challenge in computer vision. In C-ToF
radiance field reconstruction, the property of interest-depth-is not directly
measured, causing an additional challenge. This problem has a large and
underappreciated impact upon the optimization when using a fast primitive-based
scene representation like 3D Gaussian splatting, which is commonly used with
multi-view data to produce satisfactory results and is brittle in its
optimization otherwise. We incorporate two heuristics into the optimization to
improve the accuracy of scene geometry represented by Gaussians. Experimental
results show that our approach produces accurate reconstructions under
constrained C-ToF sensing conditions, including for fast motions like swinging
baseball bats. https://visual.cs.brown.edu/gftorf

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [209] [Quantum-Inspired Optimization Process for Data Imputation](https://arxiv.org/abs/2505.04841)
*Nishikanta Mohanty,Bikash K. Behera,Badsah Mukherjee,Christopher Ferrie*

Main category: quant-ph

TL;DR: 该论文提出了一种结合量子启发的旋转和PCA的数据填补框架，显著提升了填补精度和统计保真度。


<details>
  <summary>Details</summary>
Motivation: 数据填补是预处理的关键步骤，尤其对于存在缺失或不可靠值的数据集。传统方法在统计保真度和填补真实性上表现不足。

Method: 通过将主成分分析（PCA）与量子辅助旋转结合，并采用无梯度优化器（COBYLA、模拟退火、差分进化）优化，填补缺失值并保持统计特性。

Result: 方法显著减少了Wasserstein距离（平均降低85%），KS检验p值更优（0.18-0.22 vs >0.99），且避免了零值伪影和中心聚集问题。

Conclusion: 该量子启发框架为医疗和AI等领域的数据填补提供了高保真、可扩展的解决方案。

Abstract: Data imputation is a critical step in data pre-processing, particularly for
datasets with missing or unreliable values. This study introduces a novel
quantum-inspired imputation framework evaluated on the UCI Diabetes dataset,
which contains biologically implausible missing values across several clinical
features. The method integrates Principal Component Analysis (PCA) with
quantum-assisted rotations, optimized through gradient-free classical
optimizers -COBYLA, Simulated Annealing, and Differential Evolution to
reconstruct missing values while preserving statistical fidelity. Reconstructed
values are constrained within +/-2 standard deviations of original feature
distributions, avoiding unrealistic clustering around central tendencies. This
approach achieves a substantial and statistically significant improvement,
including an average reduction of over 85% in Wasserstein distance and
Kolmogorov-Smirnov test p-values between 0.18 and 0.22, compared to p-values >
0.99 in classical methods such as Mean, KNN, and MICE. The method also
eliminates zero-value artifacts and enhances the realism and variability of
imputed data. By combining quantum-inspired transformations with a scalable
classical framework, this methodology provides a robust solution for imputation
tasks in domains such as healthcare and AI pipelines, where data quality and
integrity are crucial.

</details>


### [210] [GroverGPT-2: Simulating Grover's Algorithm via Chain-of-Thought Reasoning and Quantum-Native Tokenization](https://arxiv.org/abs/2505.04880)
*Min Chen,Jinglei Cheng,Pingzhi Li,Haoran Wang,Tianlong Chen,Junyu Liu*

Main category: quant-ph

TL;DR: 论文介绍了GroverGPT-2，一种基于LLM的方法，通过思维链推理和量子原生标记化模拟Grover算法，展示了经典模型（如LLM）能够捕捉量子算法结构的能力。


<details>
  <summary>Details</summary>
Motivation: 探索经典机器是否能学习和模拟量子算法，特别是利用近年来在大型语言模型（LLM）上取得的进展来模拟Grover算法。

Method: 采用GroverGPT-2，结合思维链推理和量子原生标记化，直接从量子电路表示进行模拟，并生成逻辑结构化和可解释的输出。

Result: GroverGPT-2能够通过量子原生标记处理学习并内化量子电路逻辑，其输出结合电路数据和自然语言，展示了经典模型的潜力。此外，还发现了随着量子位数增加的经验缩放规律。

Conclusion: GroverGPT-2为提升对量子算法的机器理解和建模量子电路逻辑提供了原型，为探索经典可模拟性边界、增强量子教育和研究以及未来量子计算基础模型奠定基础。

Abstract: Quantum computing offers theoretical advantages over classical computing for
specific tasks, yet the boundary of practical quantum advantage remains an open
question. To investigate this boundary, it is crucial to understand whether,
and how, classical machines can learn and simulate quantum algorithms. Recent
progress in large language models (LLMs) has demonstrated strong reasoning
abilities, prompting exploration into their potential for this challenge. In
this work, we introduce GroverGPT-2, an LLM-based method for simulating
Grover's algorithm using Chain-of-Thought (CoT) reasoning and quantum-native
tokenization. Building on its predecessor, GroverGPT-2 performs simulation
directly from quantum circuit representations while producing logically
structured and interpretable outputs. Our results show that GroverGPT-2 can
learn and internalize quantum circuit logic through efficient processing of
quantum-native tokens, providing direct evidence that classical models like
LLMs can capture the structure of quantum algorithms. Furthermore, GroverGPT-2
outputs interleave circuit data with natural language, embedding explicit
reasoning into the simulation. This dual capability positions GroverGPT-2 as a
prototype for advancing machine understanding of quantum algorithms and
modeling quantum circuit logic. We also identify an empirical scaling law for
GroverGPT-2 with increasing qubit numbers, suggesting a path toward scalable
classical simulation. These findings open new directions for exploring the
limits of classical simulatability, enhancing quantum education and research,
and laying groundwork for future foundation models in quantum computing.

</details>


### [211] [Quantum QSAR for drug discovery](https://arxiv.org/abs/2505.04648)
*Alejandro Giraldo,Daniel Ruiz,Mariano Caruso,Guido Bellomo*

Main category: quant-ph

TL;DR: 该论文提出用量子支持向量机（QSVM）增强定量构效关系（QSAR）建模，以解决传统方法在高维数据和复杂分子相互作用上的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统QSAR建模方法在处理高维数据和捕捉复杂分子相互作用时存在不足，亟需更高效、准确的解决方案。

Method: 采用量子数据编码和量子核函数，利用量子计算原理在希尔伯特空间处理信息，构建QSVM模型。

Result: 预期开发出更精准、高效的QSAR预测模型。

Conclusion: 量子支持向量机为QSAR建模提供了新的优化方向，有望推动药物发现领域的进步。

Abstract: Quantitative Structure-Activity Relationship (QSAR) modeling is key in drug
discovery, but classical methods face limitations when handling
high-dimensional data and capturing complex molecular interactions. This
research proposes enhancing QSAR techniques through Quantum Support Vector
Machines (QSVMs), which leverage quantum computing principles to process
information Hilbert spaces. By using quantum data encoding and quantum kernel
functions, we aim to develop more accurate and efficient predictive models.

</details>


### [212] [Overcoming Dimensional Factorization Limits in Discrete Diffusion Models through Quantum Joint Distribution Learning](https://arxiv.org/abs/2505.05151)
*Chuangtao Chen,Qinglin Zhao,MengChu Zhou,Zhimin He,Haozhen Situ*

Main category: quant-ph

TL;DR: 该论文提出了一种量子离散去噪扩散概率模型（QD3PM），利用量子计算在高维希尔伯特空间中联合概率学习，克服了经典离散扩散模型在高维数据分布学习中的限制，并在模拟中展示了其优越性。


<details>
  <summary>Details</summary>
Motivation: 经典离散扩散模型在高维数据分布学习中存在计算复杂度线性增长的局限性，KL散度随数据维度线性增加。为了克服这一限制，作者提出利用量子计算的并行性和高维优势来改进扩散模型。

Method: 论文提出QD3PM模型，通过量子贝叶斯定理推导后验状态，并设计量子电路实现参数共享和经典数据控制的旋转编码。模型支持一步采样，无需迭代。

Result: 模拟结果显示，QD3PM在复杂分布建模上的准确性优于传统方法，验证了量子计算在联合分布学习中的优势。

Conclusion: 该研究为生成模型建立了新的理论范式，通过量子联合分布学习的优势，为高维数据分布建模提供了更高效的解决方案。

Abstract: This study explores quantum-enhanced discrete diffusion models to overcome
classical limitations in learning high-dimensional distributions. We rigorously
prove that classical discrete diffusion models, which calculate per-dimension
transition probabilities to avoid exponential computational cost, exhibit
worst-case linear scaling of Kullback-Leibler (KL) divergence with data
dimension. To address this, we propose a Quantum Discrete Denoising Diffusion
Probabilistic Model (QD3PM), which enables joint probability learning through
diffusion and denoising in exponentially large Hilbert spaces. By deriving
posterior states through quantum Bayes' theorem, similar to the crucial role of
posterior probabilities in classical diffusion models, and by learning the
joint probability, we establish a solid theoretical foundation for
quantum-enhanced diffusion models. For denoising, we design a quantum circuit
using temporal information for parameter sharing and learnable
classical-data-controlled rotations for encoding. Exploiting joint distribution
learning, our approach enables single-step sampling from pure noise,
eliminating iterative requirements of existing models. Simulations demonstrate
the proposed model's superior accuracy in modeling complex distributions
compared to factorization methods. Hence, this paper establishes a new
theoretical paradigm in generative models by leveraging the quantum advantage
in joint distribution learning.

</details>


### [213] [Operator-Level Quantum Acceleration of Non-Logconcave Sampling](https://arxiv.org/abs/2505.05301)
*Jiaqi Leng,Zhiyan Ding,Zherui Chen,Lin Lin*

Main category: quant-ph

TL;DR: 这篇论文提出了第一个在非对数凹设置下能加速连续时间采样动力学的量子算法，解决了传统方法在非凸势能下的采样问题。


<details>
  <summary>Details</summary>
Motivation: 非凸势能下的概率分布采样是物理、化学、生物等领域的基本任务，但传统方法如朗之万动力学在此类问题中表现不佳，因此需要量子加速的解决方案。

Method: 论文通过将目标Gibbs测量编码到量子态振幅中，利用Witten拉普拉斯算子的块矩阵分解，结合奇异值阈值技术实现采样，并进一步扩展至副本交换朗之万扩散的量子加速。

Result: 该方法首次在非对数凹设置下证明了量子优势（基于Poincaré常数），并为复杂能量地貌的采样提供了量子加速框架。

Conclusion: 研究展示了量子算法在连续采样问题中的潜力，为跨学科的非凸优化和采样问题提供了新工具。

Abstract: Sampling from probability distributions of the form $\sigma \propto e^{-\beta
V}$, where $V$ is a continuous potential, is a fundamental task across physics,
chemistry, biology, computer science, and statistics. However, when $V$ is
non-convex, the resulting distribution becomes non-logconcave, and classical
methods such as Langevin dynamics often exhibit poor performance. We introduce
the first quantum algorithm that provably accelerates a broad class of
continuous-time sampling dynamics. For Langevin dynamics, our method encodes
the target Gibbs measure into the amplitudes of a quantum state, identified as
the kernel of a block matrix derived from a factorization of the Witten
Laplacian operator. This connection enables Gibbs sampling via singular value
thresholding and yields the first provable quantum advantage with respect to
the Poincar\'e constant in the non-logconcave setting. Building on this
framework, we further develop the first quantum algorithm that accelerates
replica exchange Langevin diffusion, a widely used method for sampling from
complex, rugged energy landscapes.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [214] [High-fidelity Grain Growth Modeling: Leveraging Deep Learning for Fast Computations](https://arxiv.org/abs/2505.05354)
*Pungponhavoan Tep,Marc Bernacki*

Main category: cond-mat.mtrl-sci

TL;DR: 论文提出了一种结合卷积LSTM网络和自编码器的机器学习框架，用于高效预测金属材料微观结构演变，比传统方法快89倍，且保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统偏微分方程方法计算成本高，限制了材料设计和制造效率。机器学习框架旨在解决这一瓶颈。

Method: 使用卷积LSTM网络捕捉时空特征，自编码器压缩高维数据，并结合复合损失函数（MSE、SSIM、边界保护）保持结构完整性。

Result: 方法将计算时间从10分钟降至10秒，最佳模型的结构相似性达86.71%，平均晶粒尺寸误差仅0.07%。

Conclusion: 该方法显著加速微观结构预测，为材料科学和制造领域的创新提供支持。

Abstract: Grain growth simulation is crucial for predicting metallic material
microstructure evolution during annealing and resulting final mechanical
properties, but traditional partial differential equation-based methods are
computationally expensive, creating bottlenecks in materials design and
manufacturing. In this work, we introduce a machine learning framework that
combines a Convolutional Long Short-Term Memory networks with an Autoencoder to
efficiently predict grain growth evolution. Our approach captures both spatial
and temporal aspects of grain evolution while encoding high-dimensional grain
structure data into a compact latent space for pattern learning, enhanced by a
novel composite loss function combining Mean Squared Error, Structural
Similarity Index Measurement, and Boundary Preservation to maintain structural
integrity of grain boundary topology of the prediction. Results demonstrated
that our machine learning approach accelerates grain growth prediction by up to
\SI{89}{\times} faster, reducing computation time from \SI{10}{\minute} to
approximately \SI{10}{\second} while maintaining high-fidelity predictions. The
best model (S-30-30) achieving a structural similarity score of
\SI{86.71}{\percent} and mean grain size error of just \SI{0.07}{\percent}. All
models accurately captured grain boundary topology, morphology, and size
distributions. This approach enables rapid microstructural prediction for
applications where conventional simulations are prohibitively time-consuming,
potentially accelerating innovation in materials science and manufacturing.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [215] [Facets of Disparate Impact: Evaluating Legally Consistent Bias in Machine Learning](https://arxiv.org/abs/2505.05471)
*Jarren Briscoe,Assefaw Gebremedhin*

Main category: cs.CY

TL;DR: 论文提出了一种名为'客观公平指数'的新指标，从边际效益和客观测试的角度定义偏见，并在机器学习应用中验证其效果。


<details>
  <summary>Details</summary>
Motivation: 通过结合法律标准和客观测试，解决机器学习中的公平性问题，特别是在敏感应用如COMPAS（累犯预测）中的偏见问题。

Method: 开发'客观公平指数'，结合上下文客观测试和指标稳定性，提供法律一致且可靠的测量方法。

Result: 该指数能区分歧视性测试和系统性差异，并在COMPAS等应用中展示了其理论和实践意义。

Conclusion: '客观公平指数'为机器学习公平性提供了新的衡量标准，具有重要的理论和实际应用价值。

Abstract: Leveraging current legal standards, we define bias through the lens of
marginal benefits and objective testing with the novel metric "Objective
Fairness Index". This index combines the contextual nuances of objective
testing with metric stability, providing a legally consistent and reliable
measure. Utilizing the Objective Fairness Index, we provide fresh insights into
sensitive machine learning applications, such as COMPAS (recidivism
prediction), highlighting the metric's practical and theoretical significance.
The Objective Fairness Index allows one to differentiate between discriminatory
tests and systemic disparities.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [216] [Robustly optimal dynamics for active matter reservoir computing](https://arxiv.org/abs/2505.05420)
*Mario U. Gaimann,Miriam Klopotek*

Main category: nlin.AO

TL;DR: 研究活性物质在储层计算范式中的信息处理能力，揭示了一个此前被忽视的优异动力学状态，该状态对物理参数和推断任务具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索活性物质如何通过自身动力学实现高效信息处理，为物理系统的计算与推断提供新见解。

Method: 采用外部驱动模型模拟系统，分析动力学放松能力与信息处理机制的关系。

Result: 发现最优计算状态位于临界阻尼阈值以下，依赖多阶段微观动力学放松，且适应性强大。

Conclusion: 该系统为非常规计算和平衡态外多体物理学提供了新的研究方向。

Abstract: We study the information processing abilities of active matter in the
reservoir computing (RC) paradigm, using a model that is externally driven to
infer the future state of a chaotic signal. The simulated system closely
follows a previously reported model. We uncover an exceptional dynamical regime
of agent dynamics that has been overlooked heretofore. It appears robustly
optimal across varying physical parameters and inference tasks, thus providing
valuable insights into computation and inference with physical systems more
generally. The ability to form effective mechanisms for information processing
are primarily determined by the system's own intrinsic relaxation abilities.
These are identifiable when probing the system without a specific inference
goal and manifest when testing minimalistic single-particle reservoirs. The
regime that achieves optimal computation is situated just below the critical
damping threshold, involving a microscopic dynamical relaxation with multiple
stages. The optimal system is adaptable under chaotic external driving, due to
a diversity in response mechanisms that emerge like rapid alternations between
quasi-stationary and highly nonlinear dynamical states. Both coherent and
incoherent dynamics contribute to their operation, partly at dissimilar scales
of space and delay time. Correlations on agent dynamics can indicate the
best-performing regimes and onsets of tight relationships between the
responding system and the fluctuating driver. As this model of computation is
interpretable in physical terms, it facilitates re-framing inquiries regarding
learning and unconventional computing with a fresh rationale for many-body
physics out of equilibrium.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [217] [From Dialect Gaps to Identity Maps: Tackling Variability in Speaker Verification](https://arxiv.org/abs/2505.04629)
*Abdulhady Abas Abdullah,Soran Badawi,Dana A. Abdullah,Dana Rasul Hamad,Hanan Abdulrahman Taher,Sabat Salih Muhamad,Aram Mahmood Ahmed,Bryar A. Hassan,Sirwan Abdolwahed Aula,Tarik A. Rashid*

Main category: eess.AS

TL;DR: 论文研究了库尔德语多种方言的说话人检测的复杂性和难点，提出了提升识别准确性的方法，如机器学习、数据增强和多方言训练，结果显示定制化策略显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 库尔德语的多种方言（如Kurmanji、Sorani和Hawrami）在语音和词汇上差异很大，这对说话人识别系统提出了特殊挑战。

Method: 采用了高级机器学习方法、数据增强策略，并构建了方言特定的语料库，同时进行了跨方言训练。

Result: 结果显示，针对每种方言的定制化策略与跨方言训练相结合，显著提高了说话人识别的性能。

Conclusion: 通过定制化方法和跨方言训练，可以有效提升库尔德语多方言说话人识别系统的准确性和可靠性。

Abstract: The complexity and difficulties of Kurdish speaker detection among its
several dialects are investigated in this work. Because of its great phonetic
and lexical differences, Kurdish with several dialects including Kurmanji,
Sorani, and Hawrami offers special challenges for speaker recognition systems.
The main difficulties in building a strong speaker identification system
capable of precisely identifying speakers across several dialects are
investigated in this work. To raise the accuracy and dependability of these
systems, it also suggests solutions like sophisticated machine learning
approaches, data augmentation tactics, and the building of thorough
dialect-specific corpus. The results show that customized strategies for every
dialect together with cross-dialect training greatly enhance recognition
performance.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [218] [Guiding Evolutionary AutoEncoder Training with Activation-Based Pruning Operators](https://arxiv.org/abs/2505.05138)
*Steven Jorgensen,Erik Hemberg,Jamal Toutouh,Una-May O'Reilly*

Main category: cs.NE

TL;DR: 本文提出了一种基于进化计算的神经网络剪枝新方法，重点针对自编码器的编码器和解码器同步剪枝。通过引入两种新的基于激活的变异算子，研究发现其中一种算子优于随机剪枝，能在保证性能的同时提升效率。同时，研究表明在协同进化环境下，随机剪枝反而优于基于激活的引导剪枝。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明自编码器训练在协同进化算法下表现优异，但针对其剪枝方法的优化研究较少。本文旨在探索如何通过进化计算优化自编码器的剪枝过程，尤其是在高维协同进化环境下的挑战。

Method: 引入两种基于激活的变异算子指导剪枝，并在经典训练和协同进化两种设置下进行对比实验。同时测试了不同剪枝调度策略的效果。

Result: 在经典自编码器中，基于激活的引导剪枝优于随机剪枝；但在协同进化环境下，随机剪枝表现更好。实验还揭示了高维环境下的剪枝策略需兼顾随机性和系统动态保护。

Conclusion: 剪枝策略的有效性依赖于问题场景的维度特性：低维时基于激活的引导更有效，而高维协同进化环境下需依赖统计均匀的随机性。本文提出了针对不同场景的最佳算子与调度组合。

Abstract: This study explores a novel approach to neural network pruning using
evolutionary computation, focusing on simultaneously pruning the encoder and
decoder of an autoencoder. We introduce two new mutation operators that use
layer activations to guide weight pruning. Our findings reveal that one of
these activation-informed operators outperforms random pruning, resulting in
more efficient autoencoders with comparable performance to canonically trained
models. Prior work has established that autoencoder training is effective and
scalable with a spatial coevolutionary algorithm that cooperatively coevolves a
population of encoders with a population of decoders, rather than one
autoencoder. We evaluate how the same activity-guided mutation operators
transfer to this context. We find that random pruning is better than guided
pruning, in the coevolutionary setting. This suggests activation-based guidance
proves more effective in low-dimensional pruning environments, where
constrained sample spaces can lead to deviations from true uniformity in
randomization. Conversely, population-driven strategies enhance robustness by
expanding the total pruning dimensionality, achieving statistically uniform
randomness that better preserves system dynamics. We experiment with pruning
according to different schedules and present best combinations of operator and
schedule for the canonical and coevolving populations cases.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [219] [Confabulation dynamics in a reservoir computer: Filling in the gaps with untrained attractors](https://arxiv.org/abs/2505.04792)
*Jack O'Hagan,Andrew Keane,Andrew Flynn*

Main category: math.DS

TL;DR: 本文研究了人工神经网络的动态系统——水库计算机（RCs）中‘虚构’现象的产生机制，尤其是未训练吸引子（UAs）在重构失败中的作用及其对吸引子间转换的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管人工神经网络（ANNs）取得了显著进展，但其基础学习机制仍知之甚少，尤其是‘虚构’现象（无意生成的错误信息）的成因。本文旨在通过研究RCs中的UAs，揭示这一现象背后的原理。

Method: 论文分析了RCs在重构给定吸引子时生成未训练吸引子（UAs）的行为，探讨了UAs在动力学重构失败中的作用及其对吸引子间转换建模的影响。

Result: 研究结果表明，UAs是状态空间受限的学习系统的固有特征，且这种虚构现象可能不仅限于RCs，也存在于其他系统中。

Conclusion: UAs是学习系统中不可避免的特征，揭示了虚构现象可能在更广泛的系统中普遍存在，为进一步理解人工神经网络的局限性提供了基础。

Abstract: Artificial Intelligence has advanced significantly in recent years thanks to
innovations in the design and training of artificial neural networks (ANNs).
Despite these advancements, we still understand relatively little about how
elementary forms of ANNs learn, fail to learn, and generate false information
without the intent to deceive, a phenomenon known as `confabulation'. To
provide some foundational insight, in this paper we analyse how confabulation
occurs in reservoir computers (RCs): a dynamical system in the form of an ANN.
RCs are particularly useful to study as they are known to confabulate in a
well-defined way: when RCs are trained to reconstruct the dynamics of a given
attractor, they sometimes construct an attractor that they were not trained to
construct, a so-called `untrained attractor' (UA). This paper sheds light on
the role played by UAs when reconstruction fails and their influence when
modelling transitions between reconstructed attractors. Based on our results,
we conclude that UAs are an intrinsic feature of learning systems whose state
spaces are bounded, and that this means of confabulation may be present in
systems beyond RCs.

</details>


### [220] [Learning dynamically inspired invariant subspaces for Koopman and transfer operator approximation](https://arxiv.org/abs/2505.05085)
*Gary Froyland,Kevin Kühl*

Main category: math.DS

TL;DR: 本文提出了一种通过机器学习学习正交、局部支持的基函数，以近似线性算子的方法，用于复杂非线性动力系统的线性表示，并展示了其动态自适应性和谱属性恢复能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法在估计非线性动力系统的线性变换（如转移算子和Koopman算子）时效率较低，本文旨在通过机器学习方法改进这一问题。

Method: 通过机器学习学习正交且局部支持的基函数，动态适应系统特性，从而高效地近似线性算子的作用。

Result: 提出的方法能够准确近似算子的作用，并生成几乎不变的有限维子空间，同时有效恢复算子的谱属性。

Conclusion: 机器学习生成的动态自适应基函数为解决非线性动力系统的线性表示问题提供了高效且准确的方案。

Abstract: Transfer and Koopman operator methods offer a framework for representing
complex, nonlinear dynamical systems via linear transformations, enabling for a
deeper understanding of the underlying dynamics. The spectrum of these
operators provide important insights into system predictability and emergent
behaviour, although efficiently estimating them from data can be challenging.
We tackle this issue through the lens of general operator and representational
learning, in which we approximate these linear operators using efficient
finite-dimensional representations. Specifically, we machine-learn orthonormal,
locally supported basis functions that are dynamically tailored to the system.
This learned basis provides a particularly accurate approximation of the
operator's action as well as a nearly invariant finite-dimensional subspace. We
illustrate our approach with examples that showcase the retrieval of spectral
properties from the estimated operator, and emphasise the dynamically adaptive
quality of the machine-learned basis.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [221] [Comparative Study of Generative Models for Early Detection of Failures in Medical Devices](https://arxiv.org/abs/2505.04845)
*Binesh Sadanandan,Bahareh Arghavani Nobar,Vahid Behzadan*

Main category: eess.SP

TL;DR: 该论文探讨了三种基于生成式机器学习的方法，用于检测医疗设备中的故障，重点关注外科缝合器这类设备，旨在提高其安全性。


<details>
  <summary>Details</summary>
Motivation: 由于医疗设备中复杂电子系统的增加，传统方法难以检测故障，而这类故障可能导致伤害甚至死亡，因此需要更有效的故障检测方法。

Method: 研究了三种生成式机器学习方法，利用外科缝合器的传感器数据进行故障检测。

Result: 评估了这些机器学习方法的性能和数据需求，展示了它们在提升设备安全性方面的潜力。

Conclusion: 生成式机器学习方法有望成为医疗设备故障检测的有效工具，从而减少潜在的安全风险。

Abstract: The medical device industry has significantly advanced by integrating
sophisticated electronics like microchips and field-programmable gate arrays
(FPGAs) to enhance the safety and usability of life-saving devices. These
complex electro-mechanical systems, however, introduce challenging failure
modes that are not easily detectable with conventional methods. Effective fault
detection and mitigation become vital as reliance on such electronics grows.
This paper explores three generative machine learning-based approaches for
fault detection in medical devices, leveraging sensor data from surgical
staplers,a class 2 medical device. Historically considered low-risk, these
devices have recently been linked to an increasing number of injuries and
fatalities. The study evaluates the performance and data requirements of these
machine-learning approaches, highlighting their potential to enhance device
safety.

</details>


### [222] [From Sleep Staging to Spindle Detection: Evaluating End-to-End Automated Sleep Analysis](https://arxiv.org/abs/2505.05371)
*Niklas Grieger,Siamak Mehrkanoon,Philipp Ritter,Stephan Bialonski*

Main category: eess.SP

TL;DR: 论文研究了自动化睡眠分析的可行性，结合睡眠分期和纺锤检测，成功复制了专家研究的核心结果，表明自动化方法可助力大规模睡眠研究。


<details>
  <summary>Details</summary>
Motivation: 自动化睡眠分析能减少人为差异并支持大规模研究，但多步骤自动化的可行性尚未明确。

Method: 使用RobustSleepNet进行睡眠分期和SUMOv2进行纺锤检测，验证其是否能复现双相情感障碍的专家研究结果。

Result: 自动化分析定性复现了专家研究的关键发现（如快速纺锤密度差异），量化结果虽略有不同，但模型性能达到或超过人工评分者一致性。

Conclusion: 全自动化方法具备推动大规模睡眠研究的潜力，研究团队开源工具并推出隐私保护平台SomnoBot。

Abstract: Automation of sleep analysis, including both macrostructural (sleep stages)
and microstructural (e.g., sleep spindles) elements, promises to enable
large-scale sleep studies and to reduce variance due to inter-rater
incongruencies. While individual steps, such as sleep staging and spindle
detection, have been studied separately, the feasibility of automating
multi-step sleep analysis remains unclear. Here, we evaluate whether a fully
automated analysis using state-of-the-art machine learning models for sleep
staging (RobustSleepNet) and subsequent spindle detection (SUMOv2) can
replicate findings from an expert-based study of bipolar disorder. The
automated analysis qualitatively reproduced key findings from the expert-based
study, including significant differences in fast spindle densities between
bipolar patients and healthy controls, accomplishing in minutes what previously
took months to complete manually. While the results of the automated analysis
differed quantitatively from the expert-based study, possibly due to biases
between expert raters or between raters and the models, the models individually
performed at or above inter-rater agreement for both sleep staging and spindle
detection. Our results demonstrate that fully automated approaches have the
potential to facilitate large-scale sleep research. We are providing public
access to the tools used in our automated analysis by sharing our code and
introducing SomnoBot, a privacy-preserving sleep analysis platform.

</details>
