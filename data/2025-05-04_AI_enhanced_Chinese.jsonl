{"id": "2505.00001", "pdf": "https://arxiv.org/pdf/2505.00001", "abs": "https://arxiv.org/abs/2505.00001", "authors": ["Shaun Baek", "Shaun Esua-Mensah", "Cyrus Tsui", "Sejan Vigneswaralingam", "Abdullah Alali", "Michael Lu", "Vasu Sharma", "Kevin Zhu"], "title": "Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are primarily trained on high-resource natural\nlanguages, limiting their effectiveness in low-resource settings and in tasks\nrequiring deep logical reasoning. This research introduces Rosetta-PL, a\nbenchmark designed to evaluate LLMs' logical reasoning and generalization\ncapabilities in a controlled environment. We construct Rosetta-PL by\ntranslating a dataset of logical propositions from Lean into a custom logical\nlanguage, which is then used to fine-tune an LLM (e.g., GPT-4o). Our\nexperiments analyze the impact of the size of the dataset and the translation\nmethodology on the performance of the model. Our results indicate that\npreserving logical relationships in the translation process significantly\nboosts precision, with accuracy plateauing beyond roughly 20,000 training\nsamples. These insights provide valuable guidelines for optimizing LLM training\nin formal reasoning tasks and improving performance in various low-resource\nlanguage applications.", "AI": {"tldr": "\u6784\u5efaRosetta-PL\u8bc4\u6d4b\u57fa\u51c6\uff0c\u4f18\u5316LLM\u5728\u6b63\u5f0f\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8bad\u7ec3\uff0c\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u6df1\u5ea6\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u8f6c\u6362Lean\u7684\u903b\u8f91\u547d\u9898\u6570\u636e\u96c6\u81f3\u81ea\u5b9a\u4e49\u903b\u8f91\u8bed\u8a00\uff0c\u5e76\u7528\u4e8e\u5fae\u8c03LLM\uff08\u5982GPT-4o\uff09\uff0c\u5206\u6790\u6570\u636e\u96c6\u5927\u5c0f\u548c\u8f6c\u6362\u65b9\u6cd5\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u4fdd\u7559\u903b\u8f91\u5173\u7cfb\u7684\u8f6c\u6362\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\uff0c\u8bad\u7ec3\u6837\u672c\u8d85\u8fc720,000\u540e\u7cbe\u5ea6\u8d8b\u4e8e\u7a33\u5b9a\u3002", "conclusion": "\u4e3a\u4f18\u5316LLM\u5728\u6b63\u5f0f\u63a8\u7406\u4efb\u52a1\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u5e94\u7528\u8868\u73b0\u3002"}}
{"id": "2505.00002", "pdf": "https://arxiv.org/pdf/2505.00002", "abs": "https://arxiv.org/abs/2505.00002", "authors": ["Vincent C. M\u00fcller"], "title": "Symbol grounding in computational systems: A paradox of intentions", "categories": ["cs.CL"], "comment": null, "summary": "The paper presents a paradoxical feature of computational systems that\nsuggests that computationalism cannot explain symbol grounding. If the mind is\na digital computer, as computationalism claims, then it can be computing either\nover meaningful symbols or over meaningless symbols. If it is computing over\nmeaningful symbols its functioning presupposes the existence of meaningful\nsymbols in the system, i.e. it implies semantic nativism. If the mind is\ncomputing over meaningless symbols, no intentional cognitive processes are\navailable prior to symbol grounding. In this case, no symbol grounding could\ntake place since any grounding presupposes intentional cognitive processes. So,\nwhether computing in the mind is over meaningless or over meaningful symbols,\ncomputationalism implies semantic nativism.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u8ba1\u7b97\u4e3b\u4e49\u65e0\u6cd5\u89e3\u91ca\u7b26\u53f7\u63a5\u5730\u95ee\u9898\uff0c\u6307\u51fa\u65e0\u8bba\u5fc3\u667a\u8ba1\u7b97\u662f\u57fa\u4e8e\u6709\u610f\u4e49\u8fd8\u662f\u65e0\u610f\u4e49\u7684\u7b26\u53f7\uff0c\u90fd\u9690\u542b\u7740\u8bed\u4e49\u5148\u5929\u8bba\u3002", "motivation": "\u63a2\u8ba8\u8ba1\u7b97\u4e3b\u4e49\u5728\u89e3\u91ca\u5fc3\u667a\u7b26\u53f7\u63a5\u5730\u95ee\u9898\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u63ed\u793a\u5176\u9690\u542b\u7684\u8bed\u4e49\u5148\u5929\u8bba\u77db\u76fe\u3002", "method": "\u901a\u8fc7\u903b\u8f91\u5206\u6790\uff0c\u63d0\u51fa\u5fc3\u667a\u8ba1\u7b97\u5fc5\u987b\u57fa\u4e8e\u6709\u610f\u4e49\u6216\u65e0\u610f\u4e49\u7684\u7b26\u53f7\u7684\u4e24\u79cd\u60c5\u51b5\uff0c\u5e76\u5206\u522b\u63a8\u5bfc\u5176\u77db\u76fe\u3002", "result": "\u65e0\u8bba\u5fc3\u667a\u8ba1\u7b97\u57fa\u4e8e\u54ea\u79cd\u7b26\u53f7\uff0c\u8ba1\u7b97\u4e3b\u4e49\u90fd\u5bfc\u81f4\u8bed\u4e49\u5148\u5929\u8bba\uff0c\u65e0\u6cd5\u89e3\u91ca\u7b26\u53f7\u63a5\u5730\u3002", "conclusion": "\u8ba1\u7b97\u4e3b\u4e49\u6846\u67b6\u4e0b\u65e0\u6cd5\u89e3\u51b3\u7b26\u53f7\u63a5\u5730\u95ee\u9898\uff0c\u9700\u5bfb\u6c42\u5176\u4ed6\u7406\u8bba\u7a81\u7834\u3002"}}
{"id": "2505.00003", "pdf": "https://arxiv.org/pdf/2505.00003", "abs": "https://arxiv.org/abs/2505.00003", "authors": ["Zizhou Liu", "Ziwei Gong", "Lin Ai", "Zheng Hui", "Run Chen", "Colin Wayne Leach", "Michelle R. Greene", "Julia Hirschberg"], "title": "The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Psychological insights have long shaped pivotal NLP breakthroughs, including\nthe cognitive underpinnings of attention mechanisms, formative reinforcement\nlearning, and Theory of Mind-inspired social modeling. As Large Language Models\n(LLMs) continue to grow in scale and complexity, there is a rising consensus\nthat psychology is essential for capturing human-like cognition, behavior, and\ninteraction. This paper reviews how psychological theories can inform and\nenhance stages of LLM development, including data, pre-training, post-training,\nand evaluation\\&application. Our survey integrates insights from cognitive,\ndevelopmental, behavioral, social, personality psychology, and\npsycholinguistics. Our analysis highlights current trends and gaps in how\npsychological theories are applied. By examining both cross-domain connections\nand points of tension, we aim to bridge disciplinary divides and promote more\nthoughtful integration of psychology into future NLP research.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5fc3\u7406\u5b66\u7406\u8bba\u5982\u4f55\u5f71\u54cd\u548c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u7684\u5404\u4e2a\u9636\u6bb5\uff0c\u5305\u62ec\u6570\u636e\u3001\u9884\u8bad\u7ec3\u3001\u540e\u8bad\u7ec3\u53ca\u5e94\u7528\u8bc4\u4f30\uff0c\u6574\u5408\u4e86\u591a\u9886\u57df\u5fc3\u7406\u5b66\u7684\u89c1\u89e3\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u5e94\u7528\u7684\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u548c\u590d\u6742\u5ea6\u7684\u589e\u957f\uff0c\u5fc3\u7406\u5b66\u88ab\u8ba4\u4e3a\u5bf9\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u3001\u884c\u4e3a\u548c\u4e92\u52a8\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5fc3\u7406\u5b66\u7406\u8bba\u589e\u5f3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\uff0c\u5f25\u5408\u5b66\u79d1\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u901a\u8fc7\u6574\u5408\u8ba4\u77e5\u3001\u53d1\u5c55\u3001\u884c\u4e3a\u3001\u793e\u4f1a\u3001\u4eba\u683c\u5fc3\u7406\u5b66\u53ca\u5fc3\u7406\u8bed\u8a00\u5b66\u7684\u89c1\u89e3\uff0c\u5206\u6790\u5fc3\u7406\u5b66\u5728LLM\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u5fc3\u7406\u5b66\u7406\u8bba\u5728\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u8d8b\u52bf\u548c\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u8de8\u5b66\u79d1\u6574\u5408\u7684\u5efa\u8bae\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u5fc3\u7406\u5b66\u5728\u672a\u6765\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u547c\u5401\u66f4\u6df1\u5165\u3001\u66f4\u7cfb\u7edf\u5730\u6574\u5408\u5fc3\u7406\u5b66\u7406\u8bba\u3002"}}
{"id": "2505.00004", "pdf": "https://arxiv.org/pdf/2505.00004", "abs": "https://arxiv.org/abs/2505.00004", "authors": ["Danilo S. Carvalho", "Yingji Zhang", "Harriet Unsworth", "Andr\u00e9 Freitas"], "title": "LangVAE and LangSpace: Building and Probing for Language Model VAEs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present LangVAE, a novel framework for modular construction of variational\nautoencoders (VAEs) on top of pre-trained large language models (LLMs). Such\nlanguage model VAEs can encode the knowledge of their pre-trained components\ninto more compact and semantically disentangled representations. The\nrepresentations obtained in this way can be analysed with the LangVAE companion\nframework: LangSpace, which implements a collection of probing methods, such as\nvector traversal and interpolation, disentanglement measures, and cluster\nvisualisations. LangVAE and LangSpace offer a flexible, efficient and scalable\nway of building and analysing textual representations, with simple integration\nfor models available on the HuggingFace Hub. Additionally, we conducted a set\nof experiments with different encoder and decoder combinations, as well as\nannotated inputs, revealing a wide range of interactions across architectural\nfamilies and sizes w.r.t. generalisation and disentanglement. Our findings\ndemonstrate a promising framework for systematising the experimentation and\nunderstanding of textual representations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LangVAE\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6784\u5efa\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\uff0c\u751f\u6210\u66f4\u7d27\u51d1\u4e14\u8bed\u4e49\u89e3\u8026\u7684\u8868\u793a\uff0c\u5e76\u63d0\u4f9b\u4e86\u5206\u6790\u5de5\u5177LangSpace\u3002\u5b9e\u9a8c\u5c55\u793a\u4e86\u591a\u79cd\u67b6\u6784\u7ec4\u5408\u7684\u6cdb\u5316\u548c\u89e3\u8026\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u5176\u7cfb\u7edf\u5316\u7814\u7a76\u6587\u672c\u8868\u793a\u7684\u6f5c\u529b\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u6784\u5efa\u66f4\u9ad8\u6548\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u4ee5\u751f\u6210\u8bed\u4e49\u89e3\u8026\u7684\u6587\u672c\u8868\u793a\uff0c\u5e76\u4e3a\u5176\u63d0\u4f9b\u7cfb\u7edf\u7684\u5206\u6790\u5de5\u5177\u3002", "method": "\u63d0\u51faLangVAE\u6846\u67b6\uff0c\u57fa\u4e8e\u9884\u8bad\u7ec3LLM\u6784\u5efaVAE\uff0c\u5e76\u5f00\u53d1LangSpace\u5de5\u5177\u8fdb\u884c\u5206\u6790\uff0c\u5305\u62ec\u5411\u91cf\u904d\u5386\u3001\u63d2\u503c\u3001\u89e3\u8026\u5ea6\u91cf\u548c\u805a\u7c7b\u53ef\u89c6\u5316\u7b49\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ec4\u5408\u53ca\u6807\u6ce8\u8f93\u5165\u7684\u4ea4\u4e92\u6548\u679c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6cdb\u5316\u548c\u89e3\u8026\u65b9\u9762\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "LangVAE\u548cLangSpace\u4e3a\u6587\u672c\u8868\u793a\u7684\u7cfb\u7edf\u5316\u7814\u7a76\u548c\u7406\u89e3\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2505.00018", "pdf": "https://arxiv.org/pdf/2505.00018", "abs": "https://arxiv.org/abs/2505.00018", "authors": ["Ju Wu", "Calvin K. L. Or"], "title": "Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "This position paper critically surveys a broad spectrum of recent empirical\ndevelopments on human-AI agents collaboration, highlighting both their\ntechnical achievements and persistent gaps. We observe a lack of a unifying\ntheoretical framework that can coherently integrate these varied studies,\nespecially when tackling open-ended, complex tasks. To address this, we propose\na novel conceptual architecture: one that systematically interlinks the\ntechnical details of multi-agent coordination, knowledge management, cybernetic\nfeedback loops, and higher-level control mechanisms. By mapping existing\ncontributions, from symbolic AI techniques and connectionist LLM-based agents\nto hybrid organizational practices, onto this proposed framework (Hierarchical\nExploration-Exploitation Net), our approach facilitates revision of legacy\nmethods and inspires new work that fuses qualitative and quantitative\nparadigms. The paper's structure allows it to be read from any section, serving\nequally as a critical review of technical implementations and as a\nforward-looking reference for designing or extending human-AI symbioses.\nTogether, these insights offer a stepping stone toward deeper co-evolution of\nhuman cognition and AI capability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u5ff5\u6846\u67b6\uff08Hierarchical Exploration-Exploitation Net\uff09\uff0c\u7528\u4e8e\u6574\u5408\u4eba\u7c7b\u4e0eAI\u534f\u4f5c\u4e2d\u7684\u6280\u672f\u7ec6\u8282\u548c\u591a\u5b66\u79d1\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u5f53\u524d\u9886\u57df\u7f3a\u4e4f\u7edf\u4e00\u7406\u8bba\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5f00\u653e\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5c06\u591a\u9886\u57df\u65b9\u6cd5\u6574\u5408\u7684\u6982\u5ff5\u67b6\u6784\u3002", "method": "\u63d0\u51faHierarchical Exploration-Exploitation Net\u6846\u67b6\uff0c\u7cfb\u7edf\u6027\u5730\u6574\u5408\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u77e5\u8bc6\u7ba1\u7406\u3001\u63a7\u5236\u8bba\u53cd\u9988\u5faa\u73af\u548c\u9ad8\u5c42\u63a7\u5236\u673a\u5236\u3002", "result": "\u901a\u8fc7\u8be5\u6846\u67b6\uff0c\u53ef\u4ee5\u5c06\u73b0\u6709\u7814\u7a76\uff08\u5982\u7b26\u53f7AI\u3001\u8fde\u63a5\u4e3b\u4e49\u7684LLM\u667a\u80fd\u4f53\u7b49\uff09\u5206\u7c7b\u548c\u8bc4\u4f30\uff0c\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u65b9\u5411\uff0c\u4fc3\u8fdb\u5b9a\u91cf\u4e0e\u5b9a\u6027\u65b9\u6cd5\u7684\u878d\u5408\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u4eba\u7c7b\u8ba4\u77e5\u4e0eAI\u80fd\u529b\u7684\u6df1\u5ea6\u5171\u8fdb\u5316\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u65e2\u662f\u6280\u672f\u5b9e\u73b0\u7684\u6279\u5224\u6027\u56de\u987e\uff0c\u4e5f\u662f\u672a\u6765\u8bbe\u8ba1\u7684\u53c2\u8003\u3002"}}
{"id": "2505.00101", "pdf": "https://arxiv.org/pdf/2505.00101", "abs": "https://arxiv.org/abs/2505.00101", "authors": ["Barak Gahtan", "Sanketh Vedula", "Gil Samuelly Leichtag", "Einat Kodesh", "Alex M. Bronstein"], "title": "From Lab to Wrist: Bridging Metabolic Monitoring and Consumer Wearables for Heart Rate and Oxygen Consumption Modeling", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Understanding physiological responses during running is critical for\nperformance optimization, tailored training prescriptions, and athlete health\nmanagement. We introduce a comprehensive framework -- what we believe to be the\nfirst capable of predicting instantaneous oxygen consumption (VO$_{2}$)\ntrajectories exclusively from consumer-grade wearable data. Our approach\nemploys two complementary physiological models: (1) accurate modeling of heart\nrate (HR) dynamics via a physiologically constrained ordinary differential\nequation (ODE) and neural Kalman filter, trained on over 3 million HR\nobservations, achieving 1-second interval predictions with mean absolute errors\nas low as 2.81\\,bpm (correlation 0.87); and (2) leveraging the principles of\nprecise HR modeling, a novel VO$_{2}$ prediction architecture requiring only\nthe initial second of VO$_{2}$ data for calibration, enabling robust,\nsequence-to-sequence metabolic demand estimation. Despite relying solely on\nsmartwatch and chest-strap data, our method achieves mean absolute percentage\nerrors of approximately 13\\%, effectively capturing rapid physiological\ntransitions and steady-state conditions across diverse running intensities. Our\nsynchronized dataset, complemented by blood lactate measurements, further lays\nthe foundation for future noninvasive metabolic zone identification. By\nembedding physiological constraints within modern machine learning, this\nframework democratizes advanced metabolic monitoring, bridging laboratory-grade\naccuracy and everyday accessibility, thus empowering both elite athletes and\nrecreational fitness enthusiasts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6d88\u8d39\u8005\u7ea7\u7a7f\u6234\u8bbe\u5907\u6570\u636e\u9884\u6d4b\u77ac\u65f6\u8017\u6c27\u91cf\uff08VO$_{2}$\uff09\u8f68\u8ff9\u7684\u7efc\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u5fc3\u7387\u52a8\u6001\u5efa\u6a21\u548c\u65b0\u578bVO$_{2}$\u9884\u6d4b\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u751f\u7406\u76d1\u6d4b\u3002", "motivation": "\u8dd1\u6b65\u671f\u95f4\u7684\u751f\u7406\u54cd\u5e94\u7406\u89e3\u5bf9\u4e8e\u8fd0\u52a8\u8868\u73b0\u4f18\u5316\u3001\u4e2a\u6027\u5316\u8bad\u7ec3\u548c\u5065\u5eb7\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u7f3a\u4e4f\u57fa\u4e8e\u6d88\u8d39\u7ea7\u7a7f\u6234\u8bbe\u5907\u7684VO$_{2}$\u9884\u6d4b\u65b9\u6cd5\uff0c\u9700\u586b\u8865\u8fd9\u4e00\u6280\u672f\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u7684\u751f\u7406\u6a21\u578b\uff1a1\uff09\u901a\u8fc7\u751f\u7406\u7ea6\u675fODE\u548c\u795e\u7ecf\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5efa\u6a21\u5fc3\u7387\u52a8\u6001\uff0c\u8bad\u7ec3\u8d85\u8fc7300\u4e07\u6b21\u5fc3\u7387\u89c2\u6d4b\uff1b2\uff09\u57fa\u4e8e\u5fc3\u7387\u6a21\u578b\uff0c\u8bbe\u8ba1\u4ec5\u9700\u521d\u59cb\u79d2\u7ea7VO$_{2}$\u6570\u636e\u6821\u51c6\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u4ee3\u8c22\u9700\u6c42\u9884\u6d4b\u67b6\u6784\u3002", "result": "\u65b9\u6cd5\u5728\u4ec5\u4f7f\u7528\u667a\u80fd\u624b\u8868\u548c\u80f8\u5e26\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u7ea613%\u7684\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\uff0c\u80fd\u7cbe\u51c6\u6355\u6349\u4e0d\u540c\u8dd1\u6b65\u5f3a\u5ea6\u4e0b\u7684\u5feb\u901f\u751f\u7406\u8f6c\u53d8\u548c\u7a33\u6001\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u751f\u7406\u7ea6\u675f\u5d4c\u5165\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u5b9e\u9a8c\u5ba4\u7ea7\u7cbe\u5ea6\u7684\u4ee3\u8c22\u76d1\u6d4b\u666e\u53ca\uff0c\u4e3a\u7cbe\u82f1\u8fd0\u52a8\u5458\u548c\u5065\u8eab\u7231\u597d\u8005\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\u3002"}}
{"id": "2505.00006", "pdf": "https://arxiv.org/pdf/2505.00006", "abs": "https://arxiv.org/abs/2505.00006", "authors": ["Hayden Helm", "Tianyi Chen", "Harvey McGuinness", "Paige Lee", "Brandon Duderstadt", "Carey E. Priebe"], "title": "Toward a digital twin of U.S. Congress", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "comment": null, "summary": "In this paper we provide evidence that a virtual model of U.S.\ncongresspersons based on a collection of language models satisfies the\ndefinition of a digital twin. In particular, we introduce and provide\nhigh-level descriptions of a daily-updated dataset that contains every Tweet\nfrom every U.S. congressperson during their respective terms. We demonstrate\nthat a modern language model equipped with congressperson-specific subsets of\nthis data are capable of producing Tweets that are largely indistinguishable\nfrom actual Tweets posted by their physical counterparts. We illustrate how\ngenerated Tweets can be used to predict roll-call vote behaviors and to\nquantify the likelihood of congresspersons crossing party lines, thereby\nassisting stakeholders in allocating resources and potentially impacting\nreal-world legislative dynamics. We conclude with a discussion of the\nlimitations and important extensions of our analysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u7f8e\u56fd\u8bae\u5458\u865a\u62df\u6a21\u578b\uff0c\u80fd\u751f\u6210\u4e0e\u771f\u5b9e\u63a8\u6587\u96be\u4ee5\u533a\u5206\u7684\u63a8\u6587\uff0c\u5e76\u9884\u6d4b\u6295\u7968\u884c\u4e3a\u3002", "motivation": "\u901a\u8fc7\u6784\u5efa\u8bae\u5458\u7684\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u6765\u9884\u6d4b\u5176\u884c\u4e3a\uff0c\u4e3a\u8d44\u6e90\u5206\u914d\u548c\u7acb\u6cd5\u52a8\u6001\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u4f7f\u7528\u8bae\u5458\u63a8\u7279\u6570\u636e\u96c6\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u751f\u6210\u7c7b\u4f3c\u771f\u5b9e\u63a8\u6587\u7684\u5185\u5bb9\uff0c\u5e76\u9884\u6d4b\u6295\u7968\u884c\u4e3a\u3002", "result": "\u6a21\u578b\u751f\u6210\u7684\u63a8\u6587\u4e0e\u771f\u5b9e\u63a8\u6587\u96be\u4ee5\u533a\u5206\uff0c\u5e76\u80fd\u6709\u6548\u9884\u6d4b\u8bae\u5458\u7684\u6295\u7968\u503e\u5411\u548c\u8de8\u515a\u6d3e\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u4f46\u4e5f\u6307\u51fa\u4e86\u5c40\u9650\u6027\u548c\u672a\u6765\u6269\u5c55\u65b9\u5411\u3002"}}
{"id": "2505.00173", "pdf": "https://arxiv.org/pdf/2505.00173", "abs": "https://arxiv.org/abs/2505.00173", "authors": ["Isabelle Bloch", "Enzo Bonnot", "Pietro Gori", "Giammarco La Barbera", "Sabine Sarnacki"], "title": "First Order Logic with Fuzzy Semantics for Describing and Recognizing Nerves in Medical Images", "categories": ["cs.AI", "cs.LO", "math.LO"], "comment": "Accepted for presentation at the FUZZ-IEEE 2025 conference", "summary": "This article deals with the description and recognition of fiber bundles, in\nparticular nerves, in medical images, based on the anatomical description of\nthe fiber trajectories. To this end, we propose a logical formalization of this\nanatomical knowledge. The intrinsically imprecise description of nerves, as\nfound in anatomical textbooks, leads us to propose fuzzy semantics combined\nwith first-order logic. We define a language representing spatial entities,\nrelations between these entities and quantifiers. A formula in this language is\nthen a formalization of the natural language description. The semantics are\ngiven by fuzzy representations in a concrete domain and satisfaction degrees of\nrelations. Based on this formalization, a spatial reasoning algorithm is\nproposed for segmentation and recognition of nerves from anatomical and\ndiffusion magnetic resonance images, which is illustrated on pelvic nerves in\npediatric imaging, enabling surgeons to plan surgery.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u7cca\u8bed\u4e49\u548c\u4e00\u9636\u903b\u8f91\u7684\u533b\u5b66\u56fe\u50cf\u4e2d\u7ea4\u7ef4\u675f\uff08\u5982\u795e\u7ecf\uff09\u7684\u63cf\u8ff0\u4e0e\u8bc6\u522b\u65b9\u6cd5\uff0c\u7528\u4e8e\u624b\u672f\u89c4\u5212\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u4e2d\u795e\u7ecf\u7684\u89e3\u5256\u63cf\u8ff0\u901a\u5e38\u4e0d\u7cbe\u786e\uff0c\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u8bc6\u522b\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u7a7a\u95f4\u5b9e\u4f53\u3001\u5173\u7cfb\u548c\u91cf\u8bcd\u7684\u8bed\u8a00\uff0c\u7ed3\u5408\u6a21\u7cca\u8bed\u4e49\u548c\u4e00\u9636\u903b\u8f91\uff0c\u5f00\u53d1\u7a7a\u95f4\u63a8\u7406\u7b97\u6cd5\u7528\u4e8e\u795e\u7ecf\u5206\u5272\u4e0e\u8bc6\u522b\u3002", "result": "\u5728\u513f\u79d1\u9aa8\u76c6\u795e\u7ecf\u7684\u89e3\u5256\u548c\u6269\u6563\u78c1\u5171\u632f\u56fe\u50cf\u4e2d\u6210\u529f\u5e94\u7528\uff0c\u8f85\u52a9\u5916\u79d1\u533b\u751f\u8fdb\u884c\u624b\u672f\u89c4\u5212\u3002", "conclusion": "\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u795e\u7ecf\u8bc6\u522b\u7684\u7cbe\u786e\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u533b\u5b66\u56fe\u50cf\u7684\u89e3\u6790\u3002"}}
{"id": "2505.00131", "pdf": "https://arxiv.org/pdf/2505.00131", "abs": "https://arxiv.org/abs/2505.00131", "authors": ["Dalton Durant", "Renato Zanetti"], "title": "Kernel-Based Ensemble Gaussian Mixture Probability Hypothesis Density Filter", "categories": ["cs.LG", "stat.ME"], "comment": null, "summary": "In this work, a kernel-based Ensemble Gaussian Mixture Probability Hypothesis\nDensity (EnGM-PHD) filter is presented for multi-target filtering applications.\nThe EnGM-PHD filter combines the Gaussian-mixture-based techniques of the\nGaussian Mixture Probability Hypothesis Density (GM-PHD) filter with the\nparticle-based techniques of the Sequential Monte Carlo Probability Hypothesis\nDensity (SMC-PHD) filter. It achieves this by obtaining particles from the\nposterior intensity function, propagating them through the system dynamics, and\nthen using Kernel Density Estimation (KDE) techniques to approximate the\nGaussian mixture of the prior intensity function. This approach guarantees\nconvergence to the true intensity function in the limit of the number of\ncomponents. Moreover, in the special case of a single target with no births,\ndeaths, clutter, and perfect detection probability, the EnGM-PHD filter reduces\nto the standard Ensemble Gaussian Mixture Filter (EnGMF). In the presented\nexperiment, the results indicate that the EnGM-PHD filter achieves better\nmulti-target filtering performance than both the GM-PHD and SMC-PHD filters\nwhile using the same number of components or particles.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6838\u7684\u96c6\u6210\u9ad8\u65af\u6df7\u5408\u6982\u7387\u5047\u8bbe\u5bc6\u5ea6\uff08EnGM-PHD\uff09\u6ee4\u6ce2\u5668\uff0c\u7528\u4e8e\u591a\u76ee\u6807\u8ddf\u8e2a\u4efb\u52a1\uff0c\u7ed3\u5408\u4e86GM-PHD\u548cSMC-PHD\u6ee4\u6ce2\u5668\u7684\u4f18\u52bf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u591a\u76ee\u6807\u6ee4\u6ce2\u65b9\u6cd5\u5982GM-PHD\u548cSMC-PHD\u5b58\u5728\u4e00\u5b9a\u7684\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5f88\u597d\u5730\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u7cbe\u5ea6\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b0\u65b9\u6cd5EnGM-PHD\uff0c\u4ee5\u63d0\u5347\u591a\u76ee\u6807\u8ddf\u8e2a\u7684\u6027\u80fd\u3002", "method": "EnGM-PHD\u6ee4\u6ce2\u5668\u901a\u8fc7\u4ece\u540e\u9a8c\u5f3a\u5ea6\u51fd\u6570\u4e2d\u83b7\u53d6\u7c92\u5b50\uff0c\u901a\u8fc7\u7cfb\u7edf\u52a8\u529b\u5b66\u4f20\u64ad\u8fd9\u4e9b\u7c92\u5b50\uff0c\u7136\u540e\u4f7f\u7528\u6838\u5bc6\u5ea6\u4f30\u8ba1\uff08KDE\uff09\u6280\u672f\u8fd1\u4f3c\u9ad8\u65af\u6df7\u5408\u7684\u5148\u9a8c\u5f3a\u5ea6\u51fd\u6570\uff0c\u4ece\u800c\u7ed3\u5408\u4e86\u9ad8\u65af\u6df7\u5408\u548c\u7c92\u5b50\u6ee4\u6ce2\u7684\u7279\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u76f8\u540c\u6570\u91cf\u7684\u7ec4\u4ef6\u6216\u7c92\u5b50\u6761\u4ef6\u4e0b\uff0cEnGM-PHD\u6ee4\u6ce2\u5668\u7684\u591a\u76ee\u6807\u6ee4\u6ce2\u6027\u80fd\u4f18\u4e8eGM-PHD\u548cSMC-PHD\u6ee4\u6ce2\u5668\u3002", "conclusion": "EnGM-PHD\u6ee4\u6ce2\u5668\u5728\u591a\u76ee\u6807\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0d\u4ec5\u9ad8\u6548\u4e14\u7cbe\u5ea6\u9ad8\uff0c\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u6ee4\u6ce2\u65b9\u6cd5\u3002"}}
{"id": "2505.00008", "pdf": "https://arxiv.org/pdf/2505.00008", "abs": "https://arxiv.org/abs/2505.00008", "authors": ["Zhaoyi Sun", "Wen-Wai Yim", "Ozlem Uzuner", "Fei Xia", "Meliha Yetisgen"], "title": "A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Objective: This review aims to explore the potential and challenges of using\nNatural Language Processing (NLP) to detect, correct, and mitigate medically\ninaccurate information, including errors, misinformation, and hallucination. By\nunifying these concepts, the review emphasizes their shared methodological\nfoundations and their distinct implications for healthcare. Our goal is to\nadvance patient safety, improve public health communication, and support the\ndevelopment of more reliable and transparent NLP applications in healthcare.\n  Methods: A scoping review was conducted following PRISMA guidelines,\nanalyzing studies from 2020 to 2024 across five databases. Studies were\nselected based on their use of NLP to address medically inaccurate information\nand were categorized by topic, tasks, document types, datasets, models, and\nevaluation metrics.\n  Results: NLP has shown potential in addressing medically inaccurate\ninformation on the following tasks: (1) error detection (2) error correction\n(3) misinformation detection (4) misinformation correction (5) hallucination\ndetection (6) hallucination mitigation. However, challenges remain with data\nprivacy, context dependency, and evaluation standards.\n  Conclusion: This review highlights the advancements in applying NLP to tackle\nmedically inaccurate information while underscoring the need to address\npersistent challenges. Future efforts should focus on developing real-world\ndatasets, refining contextual methods, and improving hallucination management\nto ensure reliable and transparent healthcare applications.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u68c0\u6d4b\u3001\u7ea0\u6b63\u548c\u51cf\u5c11\u533b\u5b66\u9519\u8bef\u4fe1\u606f\u7684\u6f5c\u529b\u4e0e\u6311\u6218\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u63d0\u5347\u60a3\u8005\u5b89\u5168\u548c\u516c\u5171\u5065\u5eb7\u6c9f\u901a\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u60a3\u8005\u5b89\u5168\u3001\u6539\u5584\u516c\u5171\u5065\u5eb7\u6c9f\u901a\uff0c\u5e76\u652f\u6301\u5f00\u53d1\u66f4\u53ef\u9760\u4e14\u900f\u660e\u7684\u533b\u7597NLP\u5e94\u7528\uff0c\u6587\u7ae0\u63a2\u8ba8\u4e86NLP\u5728\u533b\u5b66\u9519\u8bef\u4fe1\u606f\u5904\u7406\u4e2d\u7684\u6f5c\u529b\u4e0e\u6311\u6218\u3002", "method": "\u91c7\u7528PRISMA\u6307\u5357\u7684\u8303\u7574\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u6790\u4e862020\u81f32024\u5e74\u95f4\u4e94\u4e2a\u6570\u636e\u5e93\u4e2d\u7684\u7814\u7a76\uff0c\u7b5b\u9009\u6807\u51c6\u4e3a\u4f7f\u7528NLP\u5904\u7406\u533b\u5b66\u9519\u8bef\u4fe1\u606f\uff0c\u5e76\u6309\u4e3b\u9898\u3001\u4efb\u52a1\u3001\u6587\u6863\u7c7b\u578b\u3001\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u8bc4\u4f30\u6307\u6807\u5206\u7c7b\u3002", "result": "NLP\u5728\u533b\u5b66\u9519\u8bef\u4fe1\u606f\u5904\u7406\uff08\u5982\u9519\u8bef\u68c0\u6d4b\u4e0e\u7ea0\u6b63\u3001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e0e\u7ea0\u6b63\u3001\u5e7b\u89c9\u68c0\u6d4b\u4e0e\u7f13\u89e3\uff09\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u6570\u636e\u9690\u79c1\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u548c\u8bc4\u4f30\u6807\u51c6\u7b49\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86NLP\u5728\u533b\u5b66\u9519\u8bef\u4fe1\u606f\u5904\u7406\u4e2d\u7684\u8fdb\u5c55\uff0c\u540c\u65f6\u6307\u51fa\u9700\u89e3\u51b3\u6570\u636e\u96c6\u3001\u4e0a\u4e0b\u6587\u65b9\u6cd5\u548c\u5e7b\u89c9\u7ba1\u7406\u7b49\u6311\u6218\uff0c\u4ee5\u786e\u4fdd\u533b\u7597\u5e94\u7528\u7684\u53ef\u9760\u6027\u4e0e\u900f\u660e\u5ea6\u3002"}}
{"id": "2505.00174", "pdf": "https://arxiv.org/pdf/2505.00174", "abs": "https://arxiv.org/abs/2505.00174", "authors": ["Ilan Strauss", "Isobel Moure", "Tim O'Reilly", "Sruly Rosenblat"], "title": "Real-World Gaps in AI Governance Research", "categories": ["cs.AI"], "comment": null, "summary": "Drawing on 1,178 safety and reliability papers from 9,439 generative AI\npapers (January 2020 - March 2025), we compare research outputs of leading AI\ncompanies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI\nuniversities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of\nWashington). We find that corporate AI research increasingly concentrates on\npre-deployment areas -- model alignment and testing & evaluation -- while\nattention to deployment-stage issues such as model bias has waned. Significant\nresearch gaps exist in high-risk deployment domains, including healthcare,\nfinance, misinformation, persuasive and addictive features, hallucinations, and\ncopyright. Without improved observability into deployed AI, growing corporate\nconcentration could deepen knowledge deficits. We recommend expanding external\nresearcher access to deployment data and systematic observability of in-market\nAI behaviors.", "AI": {"tldr": "\u5206\u6790\u4e861,178\u7bc7\u5b89\u5168\u6027\u4e0e\u53ef\u9760\u6027\u7684\u751f\u6210AI\u8bba\u6587\uff082020\u5e741\u6708\u81f32025\u5e743\u6708\uff09\uff0c\u5bf9\u6bd4\u4e86\u9886\u5148AI\u516c\u53f8\uff08\u5982Anthropic\u3001Google DeepMind\u7b49\uff09\u4e0e\u9ad8\u6821\uff08\u5982MIT\u3001\u65af\u5766\u798f\u7b49\uff09\u7684\u7814\u7a76\u4ea7\u51fa\u3002\u53d1\u73b0\u4f01\u4e1a\u7814\u7a76\u66f4\u96c6\u4e2d\u4e8e\u6a21\u578b\u5bf9\u9f50\u548c\u6d4b\u8bd5\u8bc4\u4f30\uff0c\u800c\u90e8\u7f72\u9636\u6bb5\u95ee\u9898\uff08\u5982\u6a21\u578b\u504f\u89c1\uff09\u7684\u5173\u6ce8\u51cf\u5c11\u3002\u9ad8\u98ce\u9669\u9886\u57df\uff08\u533b\u7597\u3001\u91d1\u878d\u7b49\uff09\u7814\u7a76\u5b58\u5728\u663e\u8457\u7f3a\u53e3\uff0c\u5efa\u8bae\u63d0\u9ad8\u90e8\u7f72\u6570\u636e\u7684\u53ef\u89c2\u6d4b\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5bf9\u6bd4\u4f01\u4e1a\u4e0e\u9ad8\u6821\u5728\u751f\u6210AI\u5b89\u5168\u4e0e\u53ef\u9760\u6027\u7814\u7a76\u4e0a\u7684\u5dee\u5f02\uff0c\u8bc6\u522b\u5f53\u524d\u7814\u7a76\u7684\u96c6\u4e2d\u70b9\u4e0e\u7f3a\u5931\u9886\u57df\uff0c\u4ee5\u63a8\u52a8\u66f4\u5168\u9762\u7684AI\u6cbb\u7406\u3002", "method": "\u901a\u8fc7\u5206\u67901,178\u7bc7\u8bba\u6587\uff0c\u5bf9\u6bd4\u4f01\u4e1a\uff08Anthropic\u7b495\u5bb6\uff09\u4e0e\u9ad8\u6821\uff086\u6240\uff09\u7684\u7814\u7a76\u4e3b\u9898\u5206\u5e03\uff0c\u805a\u7126\u9884\u90e8\u7f72\u4e0e\u90e8\u7f72\u9636\u6bb5\u95ee\u9898\u3002", "result": "\u4f01\u4e1a\u7814\u7a76\u504f\u5411\u9884\u90e8\u7f72\uff08\u6a21\u578b\u5bf9\u9f50\u3001\u6d4b\u8bd5\u8bc4\u4f30\uff09\uff0c\u90e8\u7f72\u9636\u6bb5\u95ee\u9898\uff08\u5982\u504f\u89c1\uff09\u5173\u6ce8\u51cf\u5c11\uff1b\u9ad8\u98ce\u9669\u9886\u57df\uff08\u533b\u7597\u3001\u91d1\u878d\u7b49\uff09\u7814\u7a76\u4e0d\u8db3\u3002", "conclusion": "\u9700\u63d0\u5347\u90e8\u7f72\u6570\u636e\u7684\u53ef\u89c2\u6d4b\u6027\uff0c\u6269\u5927\u5916\u90e8\u7814\u7a76\u8005\u5bf9\u90e8\u7f72\u6570\u636e\u7684\u8bbf\u95ee\uff0c\u4ee5\u5f25\u8865\u77e5\u8bc6\u7f3a\u53e3\u5e76\u4fc3\u8fdbAI\u5b89\u5168\u53d1\u5c55\u3002"}}
{"id": "2505.00136", "pdf": "https://arxiv.org/pdf/2505.00136", "abs": "https://arxiv.org/abs/2505.00136", "authors": ["Maksim Helmann", "Alexander Strack", "Dirk Pfl\u00fcger"], "title": "GPRat: Gaussian Process Regression with Asynchronous Tasks", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "13 pages, 7 figures", "summary": "Python is the de-facto language for software development in artificial\nintelligence (AI). Commonly used libraries, such as PyTorch and TensorFlow,\nrely on parallelization built into their BLAS backends to achieve speedup on\nCPUs. However, only applying parallelization in a low-level backend can lead to\nperformance and scaling degradation. In this work, we present a novel way of\nbinding task-based C++ code built on the asynchronous runtime model HPX to a\nhigh-level Python API using pybind11. We develop a parallel Gaussian process\n(GP) li- brary as an application. The resulting Python library GPRat combines\nthe ease of use of commonly available GP libraries with the performance and\nscalability of asynchronous runtime systems. We evaluate the per- formance on a\nmass-spring-damper system, a standard benchmark from control theory, for\nvarying numbers of regressors (features). The results show almost no binding\noverhead when binding the asynchronous HPX code using pybind11. Compared to\nGPyTorch and GPflow, GPRat shows superior scaling on up to 64 cores on an AMD\nEPYC 7742 CPU for train- ing. Furthermore, our library achieves a prediction\nspeedup of 7.63 over GPyTorch and 25.25 over GPflow. If we increase the number\nof features from eight to 128, we observe speedups of 29.62 and 21.19,\nrespectively. These results showcase the potential of using asynchronous tasks\nwithin Python-based AI applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u57fa\u4e8e\u4efb\u52a1\u7684C++\u4ee3\u7801\uff08\u4f7f\u7528HPX\u5f02\u6b65\u8fd0\u884c\u65f6\u6a21\u578b\uff09\u901a\u8fc7pybind11\u7ed1\u5b9a\u5230Python API\u7684\u65b0\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u5e76\u884c\u9ad8\u65af\u8fc7\u7a0b\u5e93GPRat\uff0c\u6027\u80fd\u4f18\u4e8eGPyTorch\u548cGPflow\u3002", "motivation": "\u73b0\u6709Python AI\u5e93\uff08\u5982PyTorch\u3001TensorFlow\uff09\u4f9d\u8d56BLAS\u540e\u7aef\u7684\u5e76\u884c\u5316\uff0c\u4f46\u4ec5\u4f4e\u5c42\u5e76\u884c\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u548c\u6269\u5c55\u6027\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u4e14\u6613\u7528\u7684\u5e76\u884c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528pybind11\u5c06\u57fa\u4e8eHPX\u5f02\u6b65\u8fd0\u884c\u65f6\u6a21\u578b\u7684C++\u4ee3\u7801\u7ed1\u5b9a\u5230Python API\uff0c\u5f00\u53d1\u5e76\u884c\u9ad8\u65af\u8fc7\u7a0b\u5e93GPRat\u3002", "result": "\u5728AMD EPYC 7742 CPU\u4e0a\uff0cGPRat\u572864\u6838\u4e0a\u8bad\u7ec3\u65f6\u6269\u5c55\u6027\u4f18\u4e8eGPyTorch\u548cGPflow\uff0c\u9884\u6d4b\u901f\u5ea6\u5206\u522b\u63d0\u53477.63\u500d\u548c25.25\u500d\uff1b\u7279\u5f81\u6570\u4ece8\u589e\u81f3128\u65f6\uff0c\u901f\u5ea6\u63d0\u534729.62\u500d\u548c21.19\u500d\u3002", "conclusion": "\u5f02\u6b65\u4efb\u52a1\u5728Python AI\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\uff0cGPRat\u7ed3\u5408\u4e86\u6613\u7528\u6027\u4e0e\u9ad8\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5f02\u6b65\u8fd0\u884c\u65f6\u6a21\u578b\u7684\u4f18\u52bf\u3002"}}
{"id": "2505.00009", "pdf": "https://arxiv.org/pdf/2505.00009", "abs": "https://arxiv.org/abs/2505.00009", "authors": ["Xiao Zhang", "Kangsheng Wang", "Tianyu Hu", "Huimin Ma"], "title": "Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation", "categories": ["cs.CL"], "comment": "Accepted by IEEE International Conference on Multimedia & Expo 2025", "summary": "Pre-trained language models (PLMs) demonstrate remarkable intelligence but\nstruggle with emerging tasks unseen during training in real-world applications.\nTraining separate models for each new task is usually impractical. Multi-task\nlearning (MTL) addresses this challenge by transferring shared knowledge from\nsource tasks to target tasks. As an dominant parameter-efficient fine-tuning\nmethod, prompt tuning (PT) enhances MTL by introducing an adaptable vector that\ncaptures task-specific knowledge, which acts as a prefix to the original prompt\nthat preserves shared knowledge, while keeping PLM parameters frozen. However,\nPT struggles to effectively capture the heterogeneity of task-specific\nknowledge due to its limited representational capacity. To address this\nchallenge, we propose Task-Adaptive Low-Rank Representation (TA-LoRA), an MTL\nmethod built on PT, employing the low-rank representation to model task\nheterogeneity and a fast-slow weights mechanism where the slow weight encodes\nshared knowledge, while the fast weight captures task-specific nuances,\navoiding the mixing of shared and task-specific knowledge, caused by training\nlow-rank representations from scratch. Moreover, a zero-initialized attention\nmechanism is introduced to minimize the disruption of immature low-rank\ncomponents on original prompts during warm-up epochs. Experiments on 16 tasks\ndemonstrate that TA-LoRA achieves state-of-the-art performance in full-data and\nfew-shot settings while maintaining superior parameter efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86TA-LoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u8868\u793a\u548c\u5feb\u6162\u6743\u91cd\u673a\u5236\u89e3\u51b3\u63d0\u793a\u8c03\u4f18\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u96be\u4ee5\u6355\u6349\u4efb\u52a1\u5f02\u8d28\u6027\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u591a\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u901a\u8fc7\u8fc1\u79fb\u5171\u4eab\u77e5\u8bc6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u63d0\u793a\u8c03\u4f18\uff08PT\uff09\u867d\u80fd\u9ad8\u6548\u5fae\u8c03\uff0c\u4f46\u65e0\u6cd5\u5145\u5206\u6355\u6349\u4efb\u52a1\u5f02\u8d28\u6027\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86TA-LoRA\u65b9\u6cd5\uff0c\u7ed3\u5408\u4f4e\u79e9\u8868\u793a\u5efa\u6a21\u4efb\u52a1\u5f02\u8d28\u6027\uff0c\u91c7\u7528\u5feb\u6162\u6743\u91cd\u673a\u5236\u5206\u79bb\u5171\u4eab\u548c\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\uff0c\u5e76\u5f15\u5165\u96f6\u521d\u59cb\u5316\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u51cf\u5c11\u9884\u70ed\u9636\u6bb5\u5bf9\u539f\u59cb\u63d0\u793a\u7684\u5e72\u6270\u3002", "result": "\u572816\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTA-LoRA\u5728\u5b8c\u6574\u6570\u636e\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u53c2\u6570\u6548\u7387\u3002", "conclusion": "TA-LoRA\u901a\u8fc7\u6539\u8fdb\u63d0\u793a\u8c03\u4f18\u7684\u8868\u793a\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u6210\u4e3a\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.00204", "pdf": "https://arxiv.org/pdf/2505.00204", "abs": "https://arxiv.org/abs/2505.00204", "authors": ["Sumit Verma", "Pritam Prasun", "Arpit Jaiswal", "Pritish Kumar"], "title": "RAIL in the Wild: Operationalizing Responsible AI Evaluation Using Anthropic's Value Dataset", "categories": ["cs.AI"], "comment": null, "summary": "As AI systems become embedded in real-world applications, ensuring they meet\nethical standards is crucial. While existing AI ethics frameworks emphasize\nfairness, transparency, and accountability, they often lack actionable\nevaluation methods. This paper introduces a systematic approach using the\nResponsible AI Labs (RAIL) framework, which includes eight measurable\ndimensions to assess the normative behavior of large language models (LLMs). We\napply this framework to Anthropic's \"Values in the Wild\" dataset, containing\nover 308,000 anonymized conversations with Claude and more than 3,000 annotated\nvalue expressions. Our study maps these values to RAIL dimensions, computes\nsynthetic scores, and provides insights into the ethical behavior of LLMs in\nreal-world use.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528RAIL\u6846\u67b6\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u901a\u8fc7\u516b\u4e2a\u53ef\u8861\u91cf\u7684\u7ef4\u5ea6\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f26\u7406\u884c\u4e3a\uff0c\u5e76\u5728Anthropic\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u5206\u6790\u3002", "motivation": "\u73b0\u6709AI\u4f26\u7406\u6846\u67b6\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u7684\u8bc4\u4ef7\u65b9\u6cd5\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u65b9\u5f0f\u6765\u8bc4\u4f30AI\u7cfb\u7edf\u7684\u4f26\u7406\u8868\u73b0\u3002", "method": "\u91c7\u7528RAIL\u6846\u67b6\uff0c\u5305\u542b\u516b\u4e2a\u53ef\u8861\u91cf\u7ef4\u5ea6\uff0c\u5e94\u7528\u4e8eAnthropic\u7684308,000\u6761\u533f\u540d\u5bf9\u8bdd\u6570\u636e\u96c6\u548c3,000\u591a\u6761\u6807\u6ce8\u7684\u4ef7\u503c\u8868\u8ff0\u3002", "result": "\u901a\u8fc7\u5c06\u4ef7\u503c\u6620\u5c04\u5230RAIL\u7ef4\u5ea6\u5e76\u8ba1\u7b97\u7efc\u5408\u5206\u6570\uff0c\u63ed\u793a\u4e86LLM\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u4f26\u7406\u884c\u4e3a\u8868\u73b0\u3002", "conclusion": "RAIL\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5de5\u5177\u6765\u8bc4\u4f30\u548c\u6539\u5584AI\u7cfb\u7edf\u7684\u4f26\u7406\u8868\u73b0\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6846\u67b6\u7684\u4e0d\u8db3\u3002"}}
{"id": "2505.00162", "pdf": "https://arxiv.org/pdf/2505.00162", "abs": "https://arxiv.org/abs/2505.00162", "authors": ["Nuojin Cheng", "Alireza Doostan", "Stephen Becker"], "title": "Stochastic Subspace Descent Accelerated via Bi-fidelity Line Search", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Efficient optimization remains a fundamental challenge across numerous\nscientific and engineering domains, especially when objective function and\ngradient evaluations are computationally expensive. While zeroth-order\noptimization methods offer effective approaches when gradients are\ninaccessible, their practical performance can be limited by the high cost\nassociated with function queries. This work introduces the bi-fidelity\nstochastic subspace descent (BF-SSD) algorithm, a novel zeroth-order\noptimization method designed to reduce this computational burden. BF-SSD\nleverages a bi-fidelity framework, constructing a surrogate model from a\ncombination of computationally inexpensive low-fidelity (LF) and accurate\nhigh-fidelity (HF) function evaluations. This surrogate model facilitates an\nefficient backtracking line search for step size selection, for which we\nprovide theoretical convergence guarantees under standard assumptions. We\nperform a comprehensive empirical evaluation of BF-SSD across four distinct\nproblems: a synthetic optimization benchmark, dual-form kernel ridge\nregression, black-box adversarial attacks on machine learning models, and\ntransformer-based black-box language model fine-tuning. Numerical results\ndemonstrate that BF-SSD consistently achieves superior optimization performance\nwhile requiring significantly fewer HF function evaluations compared to\nrelevant baseline methods. This study highlights the efficacy of integrating\nbi-fidelity strategies within zeroth-order optimization, positioning BF-SSD as\na promising and computationally efficient approach for tackling large-scale,\nhigh-dimensional problems encountered in various real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u96f6\u9636\u4f18\u5316\u65b9\u6cd5BF-SSD\uff0c\u901a\u8fc7\u7ed3\u5408\u4f4e\u7cbe\u5ea6\u548c\u9ad8\u7cbe\u5ea6\u51fd\u6570\u8bc4\u4f30\u6784\u5efa\u66ff\u4ee3\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5728\u591a\u4e2a\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u5728\u51fd\u6570\u548c\u68af\u5ea6\u8bc4\u4f30\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u79d1\u5b66\u548c\u5de5\u7a0b\u9886\u57df\u4e2d\uff0c\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u867d\u7136\u6709\u6548\uff0c\u4f46\u51fd\u6570\u67e5\u8be2\u7684\u9ad8\u6210\u672c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u6027\u80fd\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5f15\u5165BF-SSD\u7b97\u6cd5\u3002", "method": "BF-SSD\u91c7\u7528\u53cc\u7cbe\u5ea6\u6846\u67b6\uff0c\u7ed3\u5408\u4f4e\u7cbe\u5ea6\u548c\u9ad8\u7cbe\u5ea6\u51fd\u6570\u8bc4\u4f30\u6784\u5efa\u66ff\u4ee3\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u56de\u6eaf\u7ebf\u641c\u7d22\u9009\u62e9\u6b65\u957f\uff0c\u7406\u8bba\u4e0a\u4fdd\u8bc1\u6536\u655b\u6027\u80fd\u3002", "result": "\u5728\u5408\u6210\u4f18\u5316\u57fa\u51c6\u3001\u6838\u5cad\u56de\u5f52\u3001\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u548c\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u56db\u4e2a\u95ee\u9898\u4e0a\uff0cBF-SSD\u5728\u4f18\u5316\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "BF-SSD\u901a\u8fc7\u6574\u5408\u53cc\u7cbe\u5ea6\u7b56\u7565\uff0c\u4e3a\u5927\u89c4\u6a21\u9ad8\u7ef4\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2505.00010", "pdf": "https://arxiv.org/pdf/2505.00010", "abs": "https://arxiv.org/abs/2505.00010", "authors": ["Tri Nguyen", "Lohith Srikanth Pentapalli", "Magnus Sieverding", "Laurah Turner", "Seth Overla", "Weibing Zheng", "Chris Zhou", "David Furniss", "Danielle Weber", "Michael Gharib", "Matt Kelleher", "Michael Shukis", "Cameron Pawlik", "Kelly Cohen"], "title": "Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Jailbreaking in Large Language Models (LLMs) threatens their safe use in\nsensitive domains like education by allowing users to bypass ethical\nsafeguards. This study focuses on detecting jailbreaks in 2-Sigma, a clinical\neducation platform that simulates patient interactions using LLMs. We annotated\nover 2,300 prompts across 158 conversations using four linguistic variables\nshown to correlate strongly with jailbreak behavior. The extracted features\nwere used to train several predictive models, including Decision Trees, Fuzzy\nLogic-based classifiers, Boosting methods, and Logistic Regression. Results\nshow that feature-based predictive models consistently outperformed Prompt\nEngineering, with the Fuzzy Decision Tree achieving the best overall\nperformance. Our findings demonstrate that linguistic-feature-based models are\neffective and explainable alternatives for jailbreak detection. We suggest\nfuture work explore hybrid frameworks that integrate prompt-based flexibility\nwith rule-based robustness for real-time, spectrum-based jailbreak monitoring\nin educational LLMs.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5728\u6559\u80b2\u9886\u57df\u7684LLM\u4e2d\u68c0\u6d4b\u8d8a\u72f1\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u7279\u5f81\u7684\u6a21\u578b\u65b9\u6cd5\uff0c\u6548\u679c\u4f18\u4e8e\u63d0\u793a\u5de5\u7a0b\uff0c\u5e76\u5efa\u8bae\u672a\u6765\u63a2\u7d22\u6df7\u5408\u6846\u67b6\u3002", "motivation": "\u4e3a\u907f\u514dLLM\u5728\u654f\u611f\u9886\u57df\uff08\u5982\u6559\u80b2\uff09\u4e2d\u7684\u6ee5\u7528\uff0c\u5c24\u5176\u662f\u2018\u8d8a\u72f1\u2019\u884c\u4e3a\u7ed5\u8fc7\u4f26\u7406\u9650\u5236\uff0c\u7814\u7a76\u805a\u7126\u4e8e\u5f00\u53d1\u53ef\u9760\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6807\u6ce82300\u6761\u63d0\u793a\u6570\u636e\uff0c\u63d0\u53d64\u79cd\u4e0e\u8d8a\u72f1\u884c\u4e3a\u76f8\u5173\u7684\u8bed\u8a00\u7279\u5f81\uff0c\u8bad\u7ec3\u4e86\u51b3\u7b56\u6811\u3001\u6a21\u7cca\u903b\u8f91\u5206\u7c7b\u5668\u3001Boosting\u65b9\u6cd5\u548c\u903b\u8f91\u56de\u5f52\u7b49\u591a\u6a21\u578b\u3002", "result": "\u57fa\u4e8e\u7279\u5f81\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u63d0\u793a\u5de5\u7a0b\uff0c\u5176\u4e2d\u6a21\u7cca\u51b3\u7b56\u6811\u6548\u679c\u6700\u4f73\uff0c\u8bc1\u660e\u8bed\u8a00\u7279\u5f81\u6a21\u578b\u662f\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u8d8a\u72f1\u68c0\u6d4b\u65b9\u6848\u3002", "conclusion": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u8bed\u8a00\u7279\u5f81\u6a21\u578b\u7684\u5b9e\u7528\u6027\uff0c\u5efa\u8bae\u672a\u6765\u7ed3\u5408\u63d0\u793a\u7075\u6d3b\u6027\u4e0e\u89c4\u5219\u9c81\u68d2\u6027\uff0c\u5f00\u53d1\u5b9e\u65f6\u8d8a\u72f1\u76d1\u6d4b\u7684\u6df7\u5408\u6846\u67b6\u3002"}}
{"id": "2505.00278", "pdf": "https://arxiv.org/pdf/2505.00278", "abs": "https://arxiv.org/abs/2505.00278", "authors": ["Lo Pang-Yun Ting", "Yu-Hao Chiang", "Yi-Tung Tsai", "Hsu-Chao Lai", "Kun-Ta Chuang"], "title": "DeCo: Defect-Aware Modeling with Contrasting Matching for Optimizing Task Assignment in Online IC Testing", "categories": ["cs.AI"], "comment": null, "summary": "In the semiconductor industry, integrated circuit (IC) processes play a vital\nrole, as the rising complexity and market expectations necessitate improvements\nin yield. Identifying IC defects and assigning IC testing tasks to the right\nengineers improves efficiency and reduces losses. While current studies\nemphasize fault localization or defect classification, they overlook the\nintegration of defect characteristics, historical failures, and the insights\nfrom engineer expertise, which restrains their effectiveness in improving IC\nhandling. To leverage AI for these challenges, we propose DeCo, an innovative\napproach for optimizing task assignment in IC testing. DeCo constructs a novel\ndefect-aware graph from IC testing reports, capturing co-failure relationships\nto enhance defect differentiation, even with scarce defect data. Additionally,\nit formulates defect-aware representations for engineers and tasks, reinforced\nby local and global structure modeling on the defect-aware graph. Finally, a\ncontrasting-based assignment mechanism pairs testing tasks with QA engineers by\nconsidering their skill level and current workload, thus promoting an equitable\nand efficient job dispatch. Experiments on a real-world dataset demonstrate\nthat DeCo achieves the highest task-handling success rates in different\nscenarios, exceeding 80\\%, while also maintaining balanced workloads on both\nscarce or expanded defect data. Moreover, case studies reveal that DeCo can\nassign tasks to potentially capable engineers, even for their unfamiliar\ndefects, highlighting its potential as an AI-driven solution for the real-world\nIC failure analysis and task handling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDeCo\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7f3a\u9677\u611f\u77e5\u56fe\u548c\u5bf9\u6bd4\u673a\u5236\u4f18\u5316IC\u6d4b\u8bd5\u4efb\u52a1\u5206\u914d\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4efb\u52a1\u5904\u7406\u6210\u529f\u7387\u8fbe80%\u4ee5\u4e0a\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u7f3a\u9677\u7279\u5f81\u3001\u5386\u53f2\u6545\u969c\u548c\u5de5\u7a0b\u5e08\u7ecf\u9a8c\u7684\u6574\u5408\uff0c\u9650\u5236\u4e86IC\u5904\u7406\u6548\u7387\u7684\u63d0\u5347\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "DeCo\u901a\u8fc7\u6784\u5efa\u7f3a\u9677\u611f\u77e5\u56fe\u6355\u83b7\u6545\u969c\u5173\u7cfb\uff0c\u7ed3\u5408\u5de5\u7a0b\u5e08\u548c\u4efb\u52a1\u7684\u8868\u5f81\u5efa\u6a21\uff0c\u91c7\u7528\u5bf9\u6bd4\u673a\u5236\u5206\u914d\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u663e\u793aDeCo\u4efb\u52a1\u5904\u7406\u6210\u529f\u7387\u8d8580%\uff0c\u5e76\u80fd\u5747\u8861\u5de5\u4f5c\u91cf\uff0c\u5373\u4f7f\u5de5\u7a0b\u5e08\u5bf9\u67d0\u4e9b\u7f3a\u9677\u4e0d\u719f\u6089\u4e5f\u80fd\u9ad8\u6548\u5206\u914d\u3002", "conclusion": "DeCo\u662fIC\u6545\u969c\u5206\u6790\u548c\u4efb\u52a1\u5904\u7406\u7684AI\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.00169", "pdf": "https://arxiv.org/pdf/2505.00169", "abs": "https://arxiv.org/abs/2505.00169", "authors": ["Filipp Nikitin", "Ian Dunn", "David Ryan Koes", "Olexandr Isayev"], "title": "GEOM-Drugs Revisited: Toward More Chemically Accurate Benchmarks for 3D Molecule Generation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep generative models have shown significant promise in generating valid 3D\nmolecular structures, with the GEOM-Drugs dataset serving as a key benchmark.\nHowever, current evaluation protocols suffer from critical flaws, including\nincorrect valency definitions, bugs in bond order calculations, and reliance on\nforce fields inconsistent with the reference data. In this work, we revisit\nGEOM-Drugs and propose a corrected evaluation framework: we identify and fix\nissues in data preprocessing, construct chemically accurate valency tables, and\nintroduce a GFN2-xTB-based geometry and energy benchmark. We retrain and\nre-evaluate several leading models under this framework, providing updated\nperformance metrics and practical recommendations for future benchmarking. Our\nresults underscore the need for chemically rigorous evaluation practices in 3D\nmolecular generation. Our recommended evaluation methods and GEOM-Drugs\nprocessing scripts are available at\nhttps://github.com/isayevlab/geom-drugs-3dgen-evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4fee\u6b63\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u4e86GEOM-Drugs\u6570\u636e\u96c6\u4e2d\u5316\u5b66\u51c6\u786e\u6027\u95ee\u9898\uff0c\u5e76\u66f4\u65b0\u4e86\u591a\u4e2a\u6a21\u578b\u7684\u6027\u80fd\u6307\u6807\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u534f\u8bae\u5b58\u5728\u5173\u952e\u7f3a\u9677\uff0c\u5982\u9519\u8bef\u7684\u4ef7\u6001\u5b9a\u4e49\u3001\u952e\u5e8f\u8ba1\u7b97\u9519\u8bef\u4ee5\u53ca\u4f9d\u8d56\u4e0e\u53c2\u8003\u6570\u636e\u4e0d\u4e00\u81f4\u7684\u529b\u573a\uff0c\u5f71\u54cd\u4e863D\u5206\u5b50\u751f\u6210\u6a21\u578b\u7684\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "method": "\u4f5c\u8005\u91cd\u65b0\u5ba1\u89c6GEOM-Drugs\u6570\u636e\u96c6\uff0c\u4fee\u6b63\u6570\u636e\u9884\u5904\u7406\u95ee\u9898\uff0c\u6784\u5efa\u5316\u5b66\u51c6\u786e\u7684\u4ef7\u6001\u8868\uff0c\u5e76\u5f15\u5165\u57fa\u4e8eGFN2-xTB\u7684\u51e0\u4f55\u548c\u80fd\u91cf\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728\u4fee\u6b63\u6846\u67b6\u4e0b\u91cd\u65b0\u8bad\u7ec3\u548c\u8bc4\u4f30\u591a\u4e2a\u9886\u5148\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u66f4\u65b0\u7684\u6027\u80fd\u6307\u6807\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u57fa\u51c6\u6d4b\u8bd5\u7684\u5b9e\u7528\u5efa\u8bae\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e863D\u5206\u5b50\u751f\u6210\u4e2d\u5316\u5b66\u4e25\u8c28\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u5e76\u516c\u5f00\u4e86\u63a8\u8350\u7684\u8bc4\u4f30\u65b9\u6cd5\u548cGEOM-Drugs\u5904\u7406\u811a\u672c\u3002"}}
{"id": "2505.00012", "pdf": "https://arxiv.org/pdf/2505.00012", "abs": "https://arxiv.org/abs/2505.00012", "authors": ["Fabian Retkowski", "Andreas Sudmann", "Alexander Waibel"], "title": "The AI Co-Ethnographer: How Far Can Automation Take Qualitative Research?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NLP4DH 2025", "summary": "Qualitative research often involves labor-intensive processes that are\ndifficult to scale while preserving analytical depth. This paper introduces The\nAI Co-Ethnographer (AICoE), a novel end-to-end pipeline developed for\nqualitative research and designed to move beyond the limitations of simply\nautomating code assignments, offering a more integrated approach. AICoE\norganizes the entire process, encompassing open coding, code consolidation,\ncode application, and even pattern discovery, leading to a comprehensive\nanalysis of qualitative data.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86AICoE\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u5b9a\u6027\u7814\u7a76\u6d41\u7a0b\uff0c\u65e8\u5728\u8d85\u8d8a\u7b80\u5355\u7684\u7f16\u7801\u81ea\u52a8\u5316\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u5206\u6790\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5b9a\u6027\u7814\u7a76\u8fc7\u7a0b\u52b3\u52a8\u5bc6\u96c6\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6574\u5408\u7f16\u7801\u3001\u6a21\u5f0f\u53d1\u73b0\u7b49\u73af\u8282\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "AICoE\u901a\u8fc7\u7aef\u5230\u7aef\u6d41\u7a0b\u5b9e\u73b0\u5f00\u653e\u5f0f\u7f16\u7801\u3001\u4ee3\u7801\u6574\u5408\u3001\u4ee3\u7801\u5e94\u7528\u53ca\u6a21\u5f0f\u53d1\u73b0\u3002", "result": "AICoE\u80fd\u5168\u9762\u5206\u6790\u5b9a\u6027\u6570\u636e\uff0c\u63d0\u5347\u7814\u7a76\u6548\u7387\u3002", "conclusion": "AICoE\u4e3a\u5b9a\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u96c6\u6210\u7684\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2505.00325", "pdf": "https://arxiv.org/pdf/2505.00325", "abs": "https://arxiv.org/abs/2505.00325", "authors": ["Rukma Talwadker", "Surajit Chakrabarty", "Aditya Pareek", "Tridib Mukherjee", "Deepak Saini"], "title": "CognitionNet: A Collaborative Neural Network for Play Style Discovery in Online Skill Gaming Platform", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Games are one of the safest source of realizing self-esteem and relaxation at\nthe same time. An online gaming platform typically has massive data coming in,\ne.g., in-game actions, player moves, clickstreams, transactions etc. It is\nrather interesting, as something as simple as data on gaming moves can help\ncreate a psychological imprint of the user at that moment, based on her\nimpulsive reactions and response to a situation in the game. Mining this\nknowledge can: (a) immediately help better explain observed and predicted\nplayer behavior; and (b) consequently propel deeper understanding towards\nplayers' experience, growth and protection. To this effect, we focus on\ndiscovery of the \"game behaviours\" as micro-patterns formed by continuous\nsequence of games and the persistent \"play styles\" of the players' as a\nsequence of such sequences on an online skill gaming platform for Rummy. We\npropose a two stage deep neural network, CognitionNet. The first stage focuses\non mining game behaviours as cluster representations in a latent space while\nthe second aggregates over these micro patterns to discover play styles via a\nsupervised classification objective around player engagement. The dual\nobjective allows CognitionNet to reveal several player psychology inspired\ndecision making and tactics. To our knowledge, this is the first and\none-of-its-kind research to fully automate the discovery of: (i) player\npsychology and game tactics from telemetry data; and (ii) relevant diagnostic\nexplanations to players' engagement predictions. The collaborative training of\nthe two networks with differential input dimensions is enabled using a novel\nformulation of \"bridge loss\". The network plays pivotal role in obtaining\nhomogeneous and consistent play style definitions and significantly outperforms\nthe SOTA baselines wherever applicable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCognitionNet\u7684\u4e24\u9636\u6bb5\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u4ece\u5728\u7ebf\u6e38\u620f\u6570\u636e\u4e2d\u6316\u6398\u73a9\u5bb6\u884c\u4e3a\u6a21\u5f0f\u548c\u6e38\u620f\u98ce\u683c\uff0c\u4ee5\u63ed\u793a\u73a9\u5bb6\u5fc3\u7406\u548c\u6218\u672f\u51b3\u7b56\u3002", "motivation": "\u6e38\u620f\u6570\u636e\u53ef\u4ee5\u53cd\u6620\u73a9\u5bb6\u7684\u5fc3\u7406\u72b6\u6001\u548c\u884c\u4e3a\u6a21\u5f0f\uff0c\u901a\u8fc7\u6316\u6398\u8fd9\u4e9b\u6570\u636e\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u73a9\u5bb6\u7684\u884c\u4e3a\u3001\u4f53\u9a8c\u548c\u4fdd\u62a4\u9700\u6c42\u3002", "method": "CognitionNet\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u805a\u7c7b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8868\u793a\u6765\u6316\u6398\u6e38\u620f\u884c\u4e3a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u76d1\u7763\u5206\u7c7b\u76ee\u6807\u805a\u5408\u8fd9\u4e9b\u5fae\u6a21\u5f0f\u4ee5\u53d1\u73b0\u6e38\u620f\u98ce\u683c\u3002\u4f7f\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u201c\u6865\u63a5\u635f\u5931\u201d\u65b9\u6cd5\u6765\u534f\u540c\u8bad\u7ec3\u4e24\u4e2a\u7f51\u7edc\u3002", "result": "CognitionNet\u5728\u5b9a\u4e49\u4e00\u81f4\u7684\u6e38\u620f\u98ce\u683c\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4e86\u4ece\u9065\u6d4b\u6570\u636e\u4e2d\u81ea\u52a8\u5316\u53d1\u73b0\u73a9\u5bb6\u5fc3\u7406\u548c\u6218\u672f\u7684\u76ee\u6807\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4ece\u6e38\u620f\u6570\u636e\u4e2d\u6316\u6398\u73a9\u5bb6\u5fc3\u7406\u548c\u884c\u4e3a\u6a21\u5f0f\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u7406\u89e3\u548c\u9884\u6d4b\u73a9\u5bb6\u884c\u4e3a\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.00171", "pdf": "https://arxiv.org/pdf/2505.00171", "abs": "https://arxiv.org/abs/2505.00171", "authors": ["Saram Abbas", "Naeem Soomro", "Rishad Shafik", "Rakesh Heer", "Kabita Adhikari"], "title": "Attention-enabled Explainable AI for Bladder Cancer Recurrence Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "7 pages, 5 figures, Accepted to be presented at the 47th Annual\n  International Conference of the IEEE Engineering in Medicine and Biology\n  Society (EMBC 2025)", "summary": "Non-muscle-invasive bladder cancer (NMIBC) is a relentless challenge in\noncology, with recurrence rates soaring as high as 70-80%. Each recurrence\ntriggers a cascade of invasive procedures, lifelong surveillance, and\nescalating healthcare costs - affecting 460,000 individuals worldwide. However,\nexisting clinical prediction tools remain fundamentally flawed, often\noverestimating recurrence risk and failing to provide personalized insights for\npatient management. In this work, we propose an interpretable deep learning\nframework that integrates vector embeddings and attention mechanisms to improve\nNMIBC recurrence prediction performance. We incorporate vector embeddings for\ncategorical variables such as smoking status and intravesical treatments,\nallowing the model to capture complex relationships between patient attributes\nand recurrence risk. These embeddings provide a richer representation of the\ndata, enabling improved feature interactions and enhancing prediction\nperformance. Our approach not only enhances performance but also provides\nclinicians with patient-specific insights by highlighting the most influential\nfeatures contributing to recurrence risk for each patient. Our model achieves\naccuracy of 70% with tabular data, outperforming conventional statistical\nmethods while providing clinician-friendly patient-level explanations through\nfeature attention. Unlike previous studies, our approach identifies new\nimportant factors influencing recurrence, such as surgical duration and\nhospital stay, which had not been considered in existing NMIBC prediction\nmodels.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u975e\u808c\u5c42\u6d78\u6da6\u6027\u8180\u80f1\u764c(NMIBC)\u590d\u53d1\u9884\u6d4b\uff0c\u6574\u5408\u4e86\u5411\u91cf\u5d4c\u5165\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u60a3\u8005\u7279\u5f02\u6027\u7684\u89e3\u91ca\u3002", "motivation": "NMIBC\u590d\u53d1\u7387\u9ad8\u4e14\u73b0\u6709\u9884\u6d4b\u5de5\u5177\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u65e0\u6cd5\u63d0\u4f9b\u4e2a\u6027\u5316\u7ba1\u7406\u5efa\u8bae\uff0c\u4e9f\u9700\u66f4\u7cbe\u51c6\u4e14\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5411\u91cf\u5d4c\u5165\uff08\u5904\u7406\u5206\u7c7b\u53d8\u91cf\uff09\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6355\u83b7\u60a3\u8005\u5c5e\u6027\u4e0e\u590d\u53d1\u98ce\u9669\u7684\u590d\u6742\u5173\u7cfb\uff0c\u5e76\u63d0\u4f9b\u7279\u5f81\u91cd\u8981\u6027\u89e3\u91ca\u3002", "result": "\u6a21\u578b\u51c6\u786e\u7387\u8fbe70%\uff0c\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\uff1b\u53d1\u73b0\u624b\u672f\u65f6\u957f\u548c\u4f4f\u9662\u65f6\u95f4\u7b49\u65b0\u5f71\u54cd\u56e0\u7d20\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u4e34\u5e8a\u53ef\u7406\u89e3\u7684\u89e3\u91ca\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u8fd8\u589e\u5f3a\u4e34\u5e8a\u5b9e\u7528\u6027\uff0c\u4e3aNMIBC\u4e2a\u6027\u5316\u7ba1\u7406\u63d0\u4f9b\u65b0\u5de5\u5177\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4f20\u7edf\u6a21\u578b\u5ffd\u7565\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2505.00013", "pdf": "https://arxiv.org/pdf/2505.00013", "abs": "https://arxiv.org/abs/2505.00013", "authors": ["Yoichi Takenaka"], "title": "Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 3 tables, 3 appendices. Submitted to New Generation\n  Computing. Includes comparisons between fine-tuned PLMs and LLMs on Japanese\n  emotion classification. Code available at\n  https://pypi.org/project/deberta-emotion-predictor/", "summary": "Background Practical applications such as social media monitoring and\ncustomer-feedback analysis require accurate emotion detection for Japanese\ntext, yet resource scarcity and class imbalance hinder model performance.\n  Objective This study aims to build a high-accuracy model for predicting the\npresence or absence of eight Plutchik emotions in Japanese sentences.\n  Methods Using the WRIME corpus, we transform reader-averaged intensity scores\ninto binary labels and fine-tune four pre-trained language models (BERT,\nRoBERTa, DeBERTa-v3-base, DeBERTa-v3-large). For context, we also assess two\nlarge language models (TinySwallow-1.5B-Instruct and ChatGPT-4o). Accuracy and\nF1-score serve as evaluation metrics.\n  Results DeBERTa-v3-large attains the best mean accuracy (0.860) and F1-score\n(0.662), outperforming all other models. It maintains robust F1 across both\nhigh-frequency emotions (e.g., Joy, Anticipation) and low-frequency emotions\n(e.g., Anger, Trust). The LLMs lag, with ChatGPT-4o and\nTinySwallow-1.5B-Instruct scoring 0.527 and 0.292 in mean F1, respectively.\n  Conclusion The fine-tuned DeBERTa-v3-large model currently offers the most\nreliable solution for binary emotion classification in Japanese. We release\nthis model as a pip-installable package (pip install\ndeberta-emotion-predictor). Future work should augment data for rare emotions,\nreduce model size, and explore prompt engineering to improve LLM performance.\n  This manuscript is under review for possible publication in New Generation\nComputing.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5fae\u8c03DeBERTa-v3-large\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u7684\u65e5\u8bed\u6587\u672c\u60c5\u611f\u5206\u7c7b\u5de5\u5177\uff0c\u4f18\u4e8e\u5176\u4ed6\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u65e5\u8bed\u6587\u672c\u60c5\u611f\u68c0\u6d4b\u5728\u793e\u4ea4\u5a92\u4f53\u76d1\u63a7\u548c\u5ba2\u6237\u53cd\u9988\u5206\u6790\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u8d44\u6e90\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002", "method": "\u4f7f\u7528WRIME\u8bed\u6599\u5e93\uff0c\u5c06\u8bfb\u8005\u5e73\u5747\u5f3a\u5ea6\u5206\u6570\u8f6c\u6362\u4e3a\u4e8c\u5206\u7c7b\u6807\u7b7e\uff0c\u5e76\u5fae\u8c03\u56db\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08BERT\u3001RoBERTa\u3001DeBERTa-v3-base\u3001DeBERTa-v3-large\uff09\uff0c\u540c\u65f6\u8bc4\u4f30\u4e24\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08TinySwallow-1.5B-Instruct\u548cChatGPT-4o\uff09\u3002", "result": "DeBERTa-v3-large\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u5747\u51c6\u786e\u7387\uff080.860\uff09\u548cF1\u5206\u6570\uff080.662\uff09\uff0c\u5728\u9ad8\u4f4e\u9891\u60c5\u611f\u5206\u7c7b\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u5fae\u8c03\u7684DeBERTa-v3-large\u6a21\u578b\u662f\u76ee\u524d\u65e5\u8bed\u60c5\u611f\u4e8c\u5206\u7c7b\u6700\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u9700\u4f18\u5316\u6570\u636e\u589e\u5f3a\u548c\u6a21\u578b\u8f7b\u91cf\u5316\uff0c\u5e76\u63a2\u7d22\u63d0\u793a\u5de5\u7a0b\u63d0\u5347LLM\u8868\u73b0\u3002"}}
{"id": "2505.00368", "pdf": "https://arxiv.org/pdf/2505.00368", "abs": "https://arxiv.org/abs/2505.00368", "authors": ["Ahmed R. Sadik", "Muhammad Ashfaq", "Niko M\u00e4kitalo", "Tommi Mikkonen"], "title": "Urban Air Mobility as a System of Systems: An LLM-Enhanced Holonic Approach", "categories": ["cs.AI", "cs.ET", "cs.MA", "cs.RO"], "comment": null, "summary": "Urban Air Mobility (UAM) is an emerging System of System (SoS) that faces\nchallenges in system architecture, planning, task management, and execution.\nTraditional architectural approaches struggle with scalability, adaptability,\nand seamless resource integration within dynamic and complex environments. This\npaper presents an intelligent holonic architecture that incorporates Large\nLanguage Model (LLM) to manage the complexities of UAM. Holons function semi\nautonomously, allowing for real time coordination among air taxis, ground\ntransport, and vertiports. LLMs process natural language inputs, generate\nadaptive plans, and manage disruptions such as weather changes or airspace\nclosures.Through a case study of multimodal transportation with electric\nscooters and air taxis, we demonstrate how this architecture enables dynamic\nresource allocation, real time replanning, and autonomous adaptation without\ncentralized control, creating more resilient and efficient urban transportation\nnetworks. By advancing decentralized control and AI driven adaptability, this\nwork lays the groundwork for resilient, human centric UAM ecosystems, with\nfuture efforts targeting hybrid AI integration and real world validation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u6574\u4f53\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\uff08UAM\uff09\u4e2d\u7684\u590d\u6742\u6027\u548c\u52a8\u6001\u534f\u8c03\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u67b6\u6784\u5728\u5e94\u5bf9\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3001\u9002\u5e94\u6027\u548c\u8d44\u6e90\u6574\u5408\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u7075\u6d3b\u548c\u5206\u6563\u7684\u89e3\u51b3\u65b9\u6848\u6765\u7ba1\u7406\u52a8\u6001\u73af\u5883\u4e2d\u7684\u590d\u6742\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u667a\u80fd\u6574\u4f53\u67b6\u6784\u548cLLM\u6280\u672f\uff0c\u4f7f\u5404\u90e8\u5206\u534a\u81ea\u4e3b\u8fd0\u884c\uff0c\u5b9e\u73b0\u5b9e\u65f6\u534f\u8c03\uff0c\u5e76\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u751f\u6210\u81ea\u9002\u5e94\u8ba1\u5212\u548c\u5904\u7406\u7a81\u53d1\u60c5\u51b5\uff08\u5982\u5929\u6c14\u53d8\u5316\uff09\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8be5\u67b6\u6784\u652f\u6301\u52a8\u6001\u8d44\u6e90\u5206\u914d\u3001\u5b9e\u65f6\u91cd\u89c4\u5212\u548c\u81ea\u4e3b\u9002\u5e94\uff0c\u65e0\u9700\u96c6\u4e2d\u63a7\u5236\u5373\u53ef\u63d0\u5347\u57ce\u5e02\u4ea4\u901a\u7f51\u7edc\u7684\u5f39\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5206\u6563\u63a7\u5236\u548cAI\u9a71\u52a8\u7684\u9002\u5e94\u6027\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u672a\u6765\u5c06\u63a2\u7d22\u6df7\u5408AI\u96c6\u6210\u548c\u5b9e\u9645\u73af\u5883\u9a8c\u8bc1\uff0c\u4ee5\u6784\u5efa\u66f4\u5177\u97e7\u6027\u548c\u4ee5\u4eba\u4e3a\u672c\u7684UAM\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2505.00189", "pdf": "https://arxiv.org/pdf/2505.00189", "abs": "https://arxiv.org/abs/2505.00189", "authors": ["Houda Belhad", "Asmae Bourbia", "Salma Boughanja"], "title": "Chronic Diseases Prediction using Machine Learning and Deep Learning Methods", "categories": ["cs.LG"], "comment": null, "summary": "Chronic diseases, such as cardiovascular disease, diabetes, chronic kidney\ndisease, and thyroid disorders, are the leading causes of premature mortality\nworldwide. Early detection and intervention are crucial for improving patient\noutcomes, yet traditional diagnostic methods often fail due to the complex\nnature of these conditions. This study explores the application of machine\nlearning (ML) and deep learning (DL) techniques to predict chronic disease and\nthyroid disorders. We used a variety of models, including Logistic Regression\n(LR), Random Forest (RF), Gradient Boosted Trees (GBT), Neural Networks (NN),\nDecision Trees (DT) and Native Bayes (NB), to analyze and predict disease\noutcomes. Our methodology involved comprehensive data pre-processing, including\nhandling missing values, categorical encoding, and feature aggregation,\nfollowed by model training and evaluation. Performance metrics such ad\nprecision, recall, accuracy, F1-score, and Area Under the Curve (AUC) were used\nto assess the effectiveness of each model. The results demonstrated that\nensemble methods like Random Forest and Gradient Boosted Trees consistently\noutperformed. Neutral Networks also showed superior performance, particularly\nin capturing complex data patterns. The findings highlight the potential of ML\nand DL in revolutionizing chronic disease prediction, enabling early diagnosis\nand personalized treatment strategies. However, challenges such as data\nquality, model interpretability, and the need for advanced computational\ntechniques in healthcare to improve patient outcomes and reduce the burden of\nchronic diseases. This study was conducted as part of Big Data class project\nunder the supervision of our professors Mr. Abderrahmane EZ-ZAHOUT and Mr.\nAbdessamad ESSAIDI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5728\u9884\u6d4b\u6162\u6027\u75be\u75c5\u548c\u7532\u72b6\u817a\u75be\u75c5\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u96c6\u6210\u65b9\u6cd5\u5982\u968f\u673a\u68ee\u6797\u548c\u68af\u5ea6\u63d0\u5347\u6811\u8868\u73b0\u6700\u4f73\uff0c\u795e\u7ecf\u7f51\u7edc\u5728\u6355\u83b7\u590d\u6742\u6570\u636e\u6a21\u5f0f\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u6162\u6027\u75be\u75c5\u662f\u5168\u7403\u65e9\u901d\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4f20\u7edf\u8bca\u65ad\u65b9\u6cd5\u56e0\u75be\u75c5\u590d\u6742\u6027\u5e38\u5931\u8d25\uff0c\u56e0\u6b64\u7814\u7a76\u63a2\u7d22ML\u548cDL\u6280\u672f\u4ee5\u5b9e\u73b0\u65e9\u671f\u68c0\u6d4b\u548c\u5e72\u9884\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u591a\u79cd\u6a21\u578b\uff08\u5982\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001\u795e\u7ecf\u7f51\u7edc\u7b49\uff09\uff0c\u5e76\u8fdb\u884c\u4e86\u6570\u636e\u9884\u5904\u7406\uff08\u5904\u7406\u7f3a\u5931\u503c\u3001\u5206\u7c7b\u7f16\u7801\u3001\u7279\u5f81\u805a\u5408\uff09\u53ca\u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30\uff0c\u4f7f\u7528\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u7b49\u6307\u6807\u8861\u91cf\u6548\u679c\u3002", "result": "\u7ed3\u679c\u663e\u793a\u96c6\u6210\u65b9\u6cd5\uff08\u5982\u968f\u673a\u68ee\u6797\u3001\u68af\u5ea6\u63d0\u5347\u6811\uff09\u548c\u795e\u7ecf\u7f51\u7edc\u8868\u73b0\u6700\u4f73\uff0c\u80fd\u6709\u6548\u9884\u6d4b\u75be\u75c5\u3002", "conclusion": "ML\u548cDL\u6709\u6f5c\u529b\u9769\u65b0\u6162\u6027\u75be\u75c5\u9884\u6d4b\uff0c\u4f46\u9700\u89e3\u51b3\u6570\u636e\u8d28\u91cf\u3001\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7b49\u95ee\u9898\u3002\u8be5\u7814\u7a76\u4e3a\u5927\u6570\u636e\u8bfe\u7a0b\u9879\u76ee\u6210\u679c\u3002"}}
{"id": "2505.00014", "pdf": "https://arxiv.org/pdf/2505.00014", "abs": "https://arxiv.org/abs/2505.00014", "authors": ["Vinit K. Chavan"], "title": "Manifold-Constrained Sentence Embeddings via Triplet Loss: Projecting Semantics onto Spheres, Tori, and M\u00f6bius Strips", "categories": ["cs.CL"], "comment": "10 pages, 6 figures. Code available at\n  https://github.com/vinitchavan/manifold-embedding-nlp", "summary": "Recent advances in representation learning have emphasized the role of\nembedding geometry in capturing semantic structure. Traditional sentence\nembeddings typically reside in unconstrained Euclidean spaces, which may limit\ntheir ability to reflect complex relationships in language. In this work, we\nintroduce a novel framework that constrains sentence embeddings to lie on\ncontinuous manifolds -- specifically the unit sphere, torus, and M\\\"obius strip\n-- using triplet loss as the core training objective. By enforcing differential\ngeometric constraints on the output space, our approach encourages the learning\nof embeddings that are both discriminative and topologically structured.\n  We evaluate our method on benchmark datasets (AG News and MBTI) and compare\nit to classical baselines including TF-IDF, Word2Vec, and unconstrained\nKeras-derived embeddings. Our results demonstrate that manifold-constrained\nembeddings, particularly those projected onto spheres and M\\\"obius strips,\nsignificantly outperform traditional approaches in both clustering quality\n(Silhouette Score) and classification performance (Accuracy). These findings\nhighlight the value of embedding in manifold space -- where topological\nstructure complements semantic separation -- offering a new and mathematically\ngrounded direction for geometric representation learning in NLP.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u53e5\u5b50\u5d4c\u5165\u7ea6\u675f\u5728\u8fde\u7eed\u6d41\u5f62\u4e0a\uff08\u5982\u5355\u4f4d\u7403\u9762\u3001\u73af\u9762\u548c\u83ab\u6bd4\u4e4c\u65af\u5e26\uff09\uff0c\u5229\u7528\u4e09\u5143\u7ec4\u635f\u5931\u4f5c\u4e3a\u8bad\u7ec3\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5d4c\u5165\u7684\u8d28\u91cf\u548c\u7ed3\u6784\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u53e5\u5b50\u5d4c\u5165\u901a\u5e38\u5b58\u5728\u4e8e\u65e0\u7ea6\u675f\u7684\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\uff0c\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u8bed\u8a00\u7684\u590d\u6742\u5173\u7cfb\u3002\u4e3a\u4e86\u6539\u8fdb\u8fd9\u4e00\u70b9\uff0c\u4f5c\u8005\u63a2\u7d22\u4e86\u5728\u6d41\u5f62\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5d4c\u5165\u5b66\u4e60\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u8bed\u4e49\u548c\u62d3\u6251\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea6\u675f\u53e5\u5b50\u5d4c\u5165\u5728\u8fde\u7eed\u6d41\u5f62\u4e0a\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u4e09\u5143\u7ec4\u635f\u5931\u4f5c\u4e3a\u8bad\u7ec3\u76ee\u6807\uff0c\u901a\u8fc7\u5728\u5355\u4f4d\u7403\u9762\u3001\u73af\u9762\u548c\u83ab\u6bd4\u4e4c\u65af\u5e26\u7b49\u6d41\u5f62\u7a7a\u95f4\u4e0a\u8fdb\u884c\u6295\u5f71\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\uff08AG News\u548cMBTI\uff09\u4e0a\u8bc4\u4f30\u8868\u660e\uff0c\u6d41\u5f62\u7ea6\u675f\u5d4c\u5165\uff08\u5c24\u5176\u662f\u7403\u9762\u548c\u83ab\u6bd4\u4e4c\u65af\u5e26\uff09\u5728\u805a\u7c7b\u8d28\u91cf\uff08Silhouette Score\uff09\u548c\u5206\u7c7b\u6027\u80fd\uff08Accuracy\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u4e86\u6d41\u5f62\u7a7a\u95f4\u5d4c\u5165\u7684\u4ef7\u503c\uff0c\u5176\u62d3\u6251\u7ed3\u6784\u80fd\u591f\u8865\u5145\u8bed\u4e49\u5206\u79bb\uff0c\u4e3aNLP\u4e2d\u7684\u51e0\u4f55\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u5b66\u57fa\u7840\u65b9\u5411\u3002"}}
{"id": "2505.00416", "pdf": "https://arxiv.org/pdf/2505.00416", "abs": "https://arxiv.org/abs/2505.00416", "authors": ["Jing Huang", "Zhixiong Zeng", "Wenkang Han", "Yufeng Zhong", "Liming Zheng", "Shuai Fu", "Jingyuan Chen", "Lin Ma"], "title": "ScaleTrack: Scaling and back-tracking Automated GUI Agents", "categories": ["cs.AI"], "comment": null, "summary": "Automated GUI agents aims to facilitate user interaction by automatically\nperforming complex tasks in digital environments, such as web, mobile, desktop\ndevices. It receives textual task instruction and GUI description to generate\nexecutable actions (\\emph{e.g.}, click) and operation boxes step by step.\nTraining a GUI agent mainly involves grounding and planning stages, in which\nthe GUI grounding focuses on finding the execution coordinates according to the\ntask, while the planning stage aims to predict the next action based on\nhistorical actions. However, previous work suffers from the limitations of\ninsufficient training data for GUI grounding, as well as the ignorance of\nbacktracking historical behaviors for GUI planning. To handle the above\nchallenges, we propose ScaleTrack, a training framework by scaling grounding\nand backtracking planning for automated GUI agents. We carefully collected GUI\nsamples of different synthesis criterions from a wide range of sources, and\nunified them into the same template for training GUI grounding models.\nMoreover, we design a novel training strategy that predicts the next action\nfrom the current GUI image, while also backtracking the historical actions that\nled to the GUI image. In this way, ScaleTrack explains the correspondence\nbetween GUI images and actions, which effectively describes the evolution rules\nof the GUI environment. Extensive experimental results demonstrate the\neffectiveness of ScaleTrack. Data and code will be available at url.", "AI": {"tldr": "ScaleTrack\u662f\u4e00\u79cd\u81ea\u52a8\u5316GUI\u4ee3\u7406\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5c55GUI\u5750\u6807\u7ed1\u5b9a\u548c\u56de\u6eaf\u5386\u53f2\u884c\u4e3a\u7684\u89c4\u5212\u6765\u63d0\u5347\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u5728\u5750\u6807\u7ed1\u5b9a\u9636\u6bb5\u7f3a\u4e4f\u8db3\u591f\u8bad\u7ec3\u6570\u636e\uff0c\u4e14\u5728\u89c4\u5212\u9636\u6bb5\u5ffd\u7565\u56de\u6eaf\u5386\u53f2\u884c\u4e3a\uff0c\u5bfc\u81f4\u4efb\u52a1\u6267\u884c\u6548\u679c\u53d7\u9650\u3002", "method": "ScaleTrack\u901a\u8fc7\u591a\u6e90GUI\u6837\u672c\u7684\u7edf\u4e00\u6a21\u677f\u8bad\u7ec3\u5750\u6807\u7ed1\u5b9a\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u56de\u6eaf\u89c4\u5212\u7b56\u7565\uff0c\u4ece\u5f53\u524dGUI\u56fe\u50cf\u9884\u6d4b\u4e0b\u4e00\u52a8\u4f5c\u5e76\u5173\u8054\u5386\u53f2\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86ScaleTrack\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u5efa\u6a21\u4e86GUI\u73af\u5883\u4e0e\u52a8\u4f5c\u7684\u6f14\u5316\u89c4\u5219\u3002", "conclusion": "ScaleTrack\u901a\u8fc7\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027\u548c\u56de\u6eaf\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86GUI\u4ee3\u7406\u7684\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002"}}
{"id": "2505.00190", "pdf": "https://arxiv.org/pdf/2505.00190", "abs": "https://arxiv.org/abs/2505.00190", "authors": ["Hans Peter", "Anders S\u00f8gaard"], "title": "Empirical Evaluation of Progressive Coding for Sparse Autoencoders", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sparse autoencoders (SAEs)\n\\citep{bricken2023monosemanticity,gao2024scalingevaluatingsparseautoencoders}\nrely on dictionary learning to extract interpretable features from neural\nnetworks at scale in an unsupervised manner, with applications to\nrepresentation engineering and information retrieval. SAEs are, however,\ncomputationally expensive \\citep{lieberum2024gemmascopeopensparse}, especially\nwhen multiple SAEs of different sizes are needed. We show that dictionary\nimportance in vanilla SAEs follows a power law. We compare progressive coding\nbased on subset pruning of SAEs -- to jointly training nested SAEs, or\nso-called {\\em Matryoshka} SAEs\n\\citep{bussmann2024learning,nabeshima2024Matryoshka} -- on a language modeling\ntask. We show Matryoshka SAEs exhibit lower reconstruction loss and recaptured\nlanguage modeling loss, as well as higher representational similarity. Pruned\nvanilla SAEs are more interpretable, however. We discuss the origins and\nimplications of this trade-off.", "AI": {"tldr": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u901a\u8fc7\u5b57\u5178\u5b66\u4e60\u4ece\u795e\u7ecf\u7f51\u7edc\u4e2d\u63d0\u53d6\u53ef\u89e3\u91ca\u7279\u5f81\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u7814\u7a76\u53d1\u73b0\u5b57\u5178\u91cd\u8981\u6027\u9075\u5faa\u5e42\u5f8b\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u5b50\u96c6\u4fee\u526a\u7684\u6e10\u8fdb\u7f16\u7801\u4e0e\u8054\u5408\u8bad\u7ec3\u7684\u5d4c\u5957SAEs\uff08Matryoshka SAEs\uff09\u3002Matryoshka SAEs\u5728\u91cd\u5efa\u635f\u5931\u548c\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4fee\u526a\u540e\u7684\u666e\u901aSAEs\u66f4\u6613\u89e3\u91ca\u3002", "motivation": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u5728\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u4e2d\u63d0\u53d6\u53ef\u89e3\u91ca\u7279\u5f81\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u591a\u79cd\u5c3a\u5bf8SAEs\u65f6\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u57fa\u4e8e\u5b50\u96c6\u4fee\u526a\u7684\u6e10\u8fdb\u7f16\u7801\uff08progressive coding\uff09\u548c\u8054\u5408\u8bad\u7ec3\u5d4c\u5957SAEs\uff08Matryoshka SAEs\uff09\u3002\u901a\u8fc7\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e0a\u8bc4\u4f30\u8fd9\u4e24\u79cd\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u6027\u80fd\u5dee\u5f02\u3002", "result": "Matryoshka SAEs\u5728\u91cd\u5efa\u635f\u5931\u3001\u8bed\u8a00\u5efa\u6a21\u635f\u5931\u548c\u8868\u5f81\u76f8\u4f3c\u6027\u4e0a\u8868\u73b0\u4f18\u4e8e\u666e\u901aSAEs\u3002\u7136\u800c\uff0c\u4fee\u526a\u540e\u7684\u666e\u901aSAEs\u5728\u53ef\u89e3\u91ca\u6027\u4e0a\u66f4\u4f18\u3002\u7814\u7a76\u63ed\u793a\u4e86\u8fd9\u4e00\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u6743\u8861\u3002", "conclusion": "Matryoshka SAEs\u5728\u6027\u80fd\u4e0a\u66f4\u4f18\uff0c\u4f46\u666e\u901aSAEs\u5728\u4fee\u526a\u540e\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3002\u8fd9\u4e00\u53d1\u73b0\u4e3a\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u9009\u62e9SAEs\u63d0\u4f9b\u4e86\u4f9d\u636e\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u53d6\u820d\u3002"}}
{"id": "2505.00015", "pdf": "https://arxiv.org/pdf/2505.00015", "abs": "https://arxiv.org/abs/2505.00015", "authors": ["MD Thamed Bin Zaman Chowdhury", "Moazzem Hossain"], "title": "Design and Application of Multimodal Large Language Model Based System for End to End Automation of Accident Dataset Generation", "categories": ["cs.CL"], "comment": "Shortened the abstract to fit within 1920 characters. This paper is\n  currently under Review in Elsevier journal 'Accident Analysis & Prevention'", "summary": "Road traffic accidents remain a major public safety and socio-economic issue\nin developing countries like Bangladesh. Existing accident data collection is\nlargely manual, fragmented, and unreliable, resulting in underreporting and\ninconsistent records. This research proposes a fully automated system using\nLarge Language Models (LLMs) and web scraping techniques to address these\nchallenges. The pipeline consists of four components: automated web scraping\ncode generation, news collection from online sources, accident news\nclassification with structured data extraction, and duplicate removal. The\nsystem uses the multimodal generative LLM Gemini-2.0-Flash for seamless\nautomation. The code generation module classifies webpages into pagination,\ndynamic, or infinite scrolling categories and generates suitable Python scripts\nfor scraping. LLMs also classify and extract key accident information such as\ndate, time, location, fatalities, injuries, road type, vehicle types, and\npedestrian involvement. A deduplication algorithm ensures data integrity by\nremoving duplicate reports. The system scraped 14 major Bangladeshi news sites\nover 111 days (Oct 1, 2024 - Jan 20, 2025), processing over 15,000 news\narticles and identifying 705 unique accidents. The code generation module\nachieved 91.3% calibration and 80% validation accuracy. Chittagong reported the\nhighest number of accidents (80), fatalities (70), and injuries (115), followed\nby Dhaka, Faridpur, Gazipur, and Cox's Bazar. Peak accident times were morning\n(8-9 AM), noon (12-1 PM), and evening (6-7 PM). A public repository was also\ndeveloped with usage instructions. This study demonstrates the viability of an\nLLM-powered, scalable system for accurate, low-effort accident data collection,\nproviding a foundation for data-driven road safety policymaking in Bangladesh.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u4e3b\u8981\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u7f51\u9875\u6293\u53d6\u6280\u672f\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u5b5f\u52a0\u62c9\u56fd\u7b49\u53d1\u5c55\u4e2d\u56fd\u5bb6\u9053\u8def\u4ea4\u901a\u4e8b\u6545\u6570\u636e\u6536\u96c6\u7684\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u4e8b\u6545\u6570\u636e\u6536\u96c6\u65b9\u5f0f\u591a\u4e3a\u624b\u52a8\u3001\u5206\u6563\u4e14\u4e0d\u53ef\u9760\uff0c\u5bfc\u81f4\u4e86\u6570\u636e\u9057\u6f0f\u548c\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u3002", "method": "\u8be5\u7cfb\u7edf\u7531\u56db\u4e2a\u6a21\u5757\u7ec4\u6210\uff1a\u81ea\u52a8\u5316\u7f51\u9875\u6293\u53d6\u4ee3\u7801\u751f\u6210\u3001\u5728\u7ebf\u65b0\u95fb\u91c7\u96c6\u3001\u4e8b\u6545\u65b0\u95fb\u5206\u7c7b\u53ca\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\u3001\u4ee5\u53ca\u53bb\u91cd\u7b97\u6cd5\u3002\u4f7f\u7528\u4e86\u591a\u6a21\u6001\u751f\u6210LLM Gemini-2.0-Flash\u6765\u5b9e\u73b0\u65e0\u7f1d\u81ea\u52a8\u5316\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u6293\u53d6\u4e8614\u4e2a\u5b5f\u52a0\u62c9\u56fd\u65b0\u95fb\u7f51\u7ad9\uff0c\u5904\u7406\u4e86\u8d85\u8fc715,000\u7bc7\u65b0\u95fb\u6587\u7ae0\uff0c\u8bc6\u522b\u51fa705\u8d77\u72ec\u7acb\u4e8b\u6545\uff0c\u4ee3\u7801\u751f\u6210\u6a21\u5757\u7684\u6821\u51c6\u548c\u9a8c\u8bc1\u51c6\u786e\u7387\u5206\u522b\u4e3a91.3%\u548c80%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8eLLM\u7684\u53ef\u6269\u5c55\u7cfb\u7edf\u80fd\u591f\u4f4e\u6295\u5165\u3001\u9ad8\u51c6\u786e\u6027\u5730\u6536\u96c6\u4e8b\u6545\u6570\u636e\uff0c\u4e3a\u5b5f\u52a0\u62c9\u56fd\u6570\u636e\u9a71\u52a8\u7684\u9053\u8def\u5b89\u5168\u653f\u7b56\u5236\u5b9a\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.00472", "pdf": "https://arxiv.org/pdf/2505.00472", "abs": "https://arxiv.org/abs/2505.00472", "authors": ["Alaa Saleh", "Sasu Tarkoma", "Praveen Kumar Donta", "Naser Hossein Motlagh", "Schahram Dustdar", "Susanna Pirttikangas", "Lauri Lov\u00e9n"], "title": "UserCentrix: An Agentic Memory-augmented AI Framework for Smart Spaces", "categories": ["cs.AI", "cs.DC", "cs.MA", "cs.NI"], "comment": null, "summary": "Agentic AI, with its autonomous and proactive decision-making, has\ntransformed smart environments. By integrating Generative AI (GenAI) and\nmulti-agent systems, modern AI frameworks can dynamically adapt to user\npreferences, optimize data management, and improve resource allocation. This\npaper introduces UserCentrix, an agentic memory-augmented AI framework designed\nto enhance smart spaces through dynamic, context-aware decision-making. This\nframework integrates personalized Large Language Model (LLM) agents that\nleverage user preferences and LLM memory management to deliver proactive and\nadaptive assistance. Furthermore, it incorporates a hybrid hierarchical control\nsystem, balancing centralized and distributed processing to optimize real-time\nresponsiveness while maintaining global situational awareness. UserCentrix\nachieves resource-efficient AI interactions by embedding memory-augmented\nreasoning, cooperative agent negotiation, and adaptive orchestration\nstrategies. Our key contributions include (i) a self-organizing framework with\nproactive scaling based on task urgency, (ii) a Value of Information\n(VoI)-driven decision-making process, (iii) a meta-reasoning personal LLM\nagent, and (iv) an intelligent multi-agent coordination system for seamless\nenvironment adaptation. Experimental results across various models confirm the\neffectiveness of our approach in enhancing response accuracy, system\nefficiency, and computational resource management in real-world application.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86UserCentrix\uff0c\u4e00\u79cd\u589e\u5f3a\u667a\u80fd\u7a7a\u95f4\u52a8\u6001\u51b3\u7b56\u7684AI\u6846\u67b6\uff0c\u901a\u8fc7\u4e2a\u6027\u5316LLM\u4ee3\u7406\u548c\u6df7\u5408\u5206\u5c42\u63a7\u5236\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u8d44\u6e90\u7ba1\u7406\u548c\u5b9e\u65f6\u54cd\u5e94\u3002", "motivation": "\u667a\u80fd\u73af\u5883\u4e2dAI\u7684\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u52a8\u6001\u9002\u5e94\u7528\u6237\u504f\u597d\u548c\u4f18\u5316\u8d44\u6e90\u5206\u914d\u3002", "method": "\u7ed3\u5408\u751f\u6210\u5f0fAI\u548c\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u91c7\u7528\u8bb0\u5fc6\u589e\u5f3a\u63a8\u7406\u3001\u534f\u4f5c\u4ee3\u7406\u534f\u5546\u548c\u81ea\u9002\u5e94\u534f\u8c03\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u63d0\u5347\u4e86\u54cd\u5e94\u51c6\u786e\u6027\u3001\u7cfb\u7edf\u6548\u7387\u548c\u8ba1\u7b97\u8d44\u6e90\u7ba1\u7406\u3002", "conclusion": "UserCentrix\u6846\u67b6\u6709\u6548\u589e\u5f3a\u4e86\u667a\u80fd\u7a7a\u95f4\u7684\u52a8\u6001\u51b3\u7b56\u548c\u8d44\u6e90\u4f18\u5316\u80fd\u529b\u3002"}}
{"id": "2505.00196", "pdf": "https://arxiv.org/pdf/2505.00196", "abs": "https://arxiv.org/abs/2505.00196", "authors": ["Eloy Geenjaar", "Vince Calhoun"], "title": "Mapping minds not averages: a scalable subject-specific manifold learning framework for neuroimaging data", "categories": ["cs.LG", "q-bio.NC"], "comment": "20 pages, 6 figures", "summary": "Mental and cognitive representations are believed to reside on\nlow-dimensional, non-linear manifolds embedded within high-dimensional brain\nactivity. Uncovering these manifolds is key to understanding individual\ndifferences in brain function, yet most existing machine learning methods\neither rely on population-level spatial alignment or assume data that is\ntemporally structured, either because data is aligned among subjects or because\nevent timings are known. We introduce a manifold learning framework that can\ncapture subject-specific spatial variations across both structured and\ntemporally unstructured neuroimaging data. On simulated data and two\nnaturalistic fMRI datasets (Sherlock and Forrest Gump), our framework\noutperforms group-based baselines by recovering more accurate and\nindividualized representations. We further show that the framework scales\nefficiently to large datasets and generalizes well to new subjects. To test\nthis, we apply the framework to temporally unstructured resting-state fMRI data\nfrom individuals with schizophrenia and healthy controls. We further apply our\nmethod to a large resting-state fMRI dataset comprising individuals with\nschizophrenia and controls. In this setting, we demonstrate that the framework\nscales efficiently to large populations and generalizes robustly to unseen\nsubjects. The learned subject-specific spatial maps our model finds reveal\nclinically relevant patterns, including increased activation in the basal\nganglia, visual, auditory, and somatosensory regions, and decreased activation\nin the insula, inferior frontal gyrus, and angular gyrus. These findings\nsuggest that our framework can uncover clinically relevant subject-specific\nbrain activity patterns. Our approach thus provides a scalable and\nindividualized framework for modeling brain activity, with applications in\ncomputational neuroscience and clinical research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u6355\u6349\u4e2a\u4f53\u7279\u5f02\u6027\u7684\u975e\u7ebf\u6027\u4f4e\u7ef4\u6d41\u5f62\u5b66\u4e60\u6846\u67b6\uff0c\u9488\u5bf9\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u795e\u7ecf\u5f71\u50cf\u6570\u636e\uff0c\u663e\u8457\u4f18\u4e8e\u7fa4\u4f53\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u4e34\u5e8a\u7814\u7a76\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u591a\u4f9d\u8d56\u7fa4\u4f53\u7a7a\u95f4\u5bf9\u9f50\u6216\u65f6\u95f4\u7ed3\u6784\u5316\u6570\u636e\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u4e2a\u4f53\u7279\u5f02\u6027\u7684\u5927\u8111\u529f\u80fd\u5dee\u5f02\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5efa\u6a21\u4e2a\u4f53\u5316\u7684\u8111\u6d3b\u52a8\u6a21\u5f0f\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u6d41\u5f62\u5b66\u4e60\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u795e\u7ecf\u5f71\u50cf\u6570\u636e\uff0c\u901a\u8fc7\u6a21\u62df\u6570\u636e\u548c\u81ea\u7136fMRI\u6570\u636e\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u6570\u636e\u548c\u81ea\u7136fMRI\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u80fd\u591f\u9ad8\u6548\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u5728\u7cbe\u795e\u5206\u88c2\u75c7\u60a3\u8005\u7684\u9759\u606f\u6001fMRI\u6570\u636e\u4e2d\u63ed\u793a\u4e86\u4e34\u5e8a\u76f8\u5173\u8111\u533a\u6d3b\u52a8\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u548c\u4e34\u5e8a\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u4e2a\u4f53\u5316\u7684\u8111\u6d3b\u52a8\u5efa\u6a21\u5de5\u5177\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.00016", "pdf": "https://arxiv.org/pdf/2505.00016", "abs": "https://arxiv.org/abs/2505.00016", "authors": ["Josefa Lia Stoisser", "Marc Boubnovski Martell", "Julien Fauqueur"], "title": "Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This work reframes the Text-to-SQL task as a pathway for teaching large\nlanguage models (LLMs) to reason over and manipulate tabular data--moving\nbeyond the traditional focus on query generation. We propose a two-stage\nframework that leverages SQL supervision to develop transferable table\nreasoning capabilities. First, we synthesize detailed chain-of-thought (CoT)\ntraces from real-world SQL queries, providing step-by-step, clause-level\nsupervision that teaches the model how to traverse, filter, and aggregate table\nfields. Second, we introduce a Group Relative Policy Optimization (GRPO)\nreinforcement learning objective that connects SQL execution accuracy to\ngeneralizable reasoning by encouraging steps that extend beyond task-specific\nsyntax and transfer across datasets. Empirically, our approach improves\nperformance on standard Text-to-SQL benchmarks and achieves substantial gains\non reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced\ngeneralization and interpretability. Specifically, the distilled-quantized\nLLaMA model achieved a 20\\% increase in accuracy when trained on Text-to-SQL\ntasks, while Qwen achieved a 5\\% increase. These results suggest that SQL can\nserve not only as a target formalism but also as an effective scaffold for\nlearning robust, transferable reasoning over structured data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06Text-to-SQL\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u901a\u8fc7SQL\u76d1\u7763\u5b66\u4e60\u8868\u683c\u6570\u636e\u63a8\u7406\u7684\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08GRPO\uff09\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u7684\u63a8\u7406\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u65e8\u5728\u8d85\u8d8a\u4f20\u7edf\u7684\u67e5\u8be2\u751f\u6210\u4efb\u52a1\uff0c\u901a\u8fc7SQL\u76d1\u7763\u5f00\u53d1\u6a21\u578b\u5bf9\u8868\u683c\u6570\u636e\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u4ece\u771f\u5b9eSQL\u67e5\u8be2\u4e2d\u5408\u6210\u8be6\u7ec6\u7684CoT\u8f68\u8ff9\uff0c\u63d0\u4f9b\u9010\u6b65\u7684\u8bed\u6cd5\u76d1\u7763\uff1b2\uff09\u5f15\u5165GRPO\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\uff0c\u5c06SQL\u6267\u884c\u51c6\u786e\u6027\u4e0e\u901a\u7528\u63a8\u7406\u80fd\u529b\u5173\u8054\u3002", "result": "\u5728Text-to-SQL\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728BIRD\u548cCRT-QA\u7b49\u63a8\u7406\u5bc6\u96c6\u578b\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u663e\u8457\uff08\u5982LLaMA\u6a21\u578b\u51c6\u786e\u7387\u63d0\u534720%\uff0cQwen\u63d0\u53475%\uff09\u3002", "conclusion": "SQL\u4e0d\u4ec5\u80fd\u4f5c\u4e3a\u76ee\u6807\u5f62\u5f0f\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u652f\u67b6\u4fc3\u8fdb\u6a21\u578b\u5b66\u4e60\u7ed3\u6784\u5316\u6570\u636e\u7684\u9c81\u68d2\u4e14\u53ef\u8fc1\u79fb\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2505.00474", "pdf": "https://arxiv.org/pdf/2505.00474", "abs": "https://arxiv.org/abs/2505.00474", "authors": ["Cecilia Di Florio", "Huimin Dong", "Antonino Rotolo"], "title": "Rule-based Classifier Models", "categories": ["cs.AI"], "comment": "11 pages, 1 figure. Extended version of a short paper accepted to\n  ICAIL 2025. This is the authors' version of the work. It is posted here for\n  your personal use", "summary": "We extend the formal framework of classifier models used in the legal domain.\nWhile the existing classifier framework characterises cases solely through the\nfacts involved, legal reasoning fundamentally relies on both facts and rules,\nparticularly the ratio decidendi. This paper presents an initial approach to\nincorporating sets of rules within a classifier. Our work is built on the work\nof Canavotto et al. (2023), which has developed the rule-based reason model of\nprecedential constraint within a hierarchy of factors. We demonstrate how\ndecisions for new cases can be inferred using this enriched rule-based\nclassifier framework. Additionally, we provide an example of how the time\nelement and the hierarchy of courts can be used in the new classifier\nframework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6269\u5c55\u7684\u6cd5\u5f8b\u5206\u7c7b\u5668\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4e8b\u5b9e\u4e0e\u89c4\u5219\uff08\u5c24\u5176\u662f\u5224\u4f8b\u4f9d\u636e\uff09\uff0c\u57fa\u4e8eCanavotto\u7b49\u4eba\u7684\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u5728\u65b0\u6846\u67b6\u4e2d\u63a8\u65ad\u65b0\u6848\u4f8b\u7684\u5224\u51b3\u3002", "motivation": "\u73b0\u6709\u7684\u6cd5\u5f8b\u5206\u7c7b\u5668\u6846\u67b6\u4ec5\u4f9d\u8d56\u4e8b\u5b9e\u7279\u5f81\uff0c\u4f46\u6cd5\u5f8b\u63a8\u7406\u9700\u7ed3\u5408\u89c4\u5219\uff08\u5982\u5224\u4f8b\u4f9d\u636e\uff09\u3002\u672c\u6587\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6784\u5efa\u66f4\u5b8c\u6574\u7684\u6cd5\u5f8b\u63a8\u7406\u6a21\u578b\u3002", "method": "\u57fa\u4e8eCanavotto\u7b49\u4eba\uff082023\uff09\u7684\u89c4\u5219\u63a8\u7406\u6a21\u578b\uff0c\u6269\u5c55\u5206\u7c7b\u5668\u6846\u67b6\u4ee5\u7eb3\u5165\u89c4\u5219\u96c6\uff0c\u5e76\u7ed3\u5408\u65f6\u95f4\u56e0\u7d20\u548c\u6cd5\u9662\u5c42\u7ea7\u8fdb\u884c\u63a8\u65ad\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u66f4\u51c6\u786e\u5730\u63a8\u65ad\u65b0\u6848\u4f8b\u7684\u5224\u51b3\uff0c\u5e76\u8bc1\u660e\u4e86\u65f6\u95f4\u56e0\u7d20\u548c\u6cd5\u9662\u5c42\u7ea7\u5728\u6cd5\u5f8b\u5206\u7c7b\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4e8b\u5b9e\u4e0e\u89c4\u5219\u7ed3\u5408\uff0c\u65b0\u7684\u5206\u7c7b\u5668\u6846\u67b6\u63d0\u5347\u4e86\u6cd5\u5f8b\u63a8\u7406\u7684\u5168\u9762\u6027\uff0c\u4e3a\u6cd5\u5f8bAI\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6a21\u578b\u57fa\u7840\u3002"}}
{"id": "2505.00210", "pdf": "https://arxiv.org/pdf/2505.00210", "abs": "https://arxiv.org/abs/2505.00210", "authors": ["Suk Ki Lee", "Hyunwoong Ko"], "title": "Generative Machine Learning in Adaptive Control of Dynamic Manufacturing Processes: A Review", "categories": ["cs.LG", "cs.CE", "cs.SY", "eess.SY"], "comment": "12 pages, 1 figure, 1 table. This paper has been accepted for\n  publication in the proceedings of ASME IDETC-CIE 2025", "summary": "Dynamic manufacturing processes exhibit complex characteristics defined by\ntime-varying parameters, nonlinear behaviors, and uncertainties. These\ncharacteristics require sophisticated in-situ monitoring techniques utilizing\nmultimodal sensor data and adaptive control systems that can respond to\nreal-time feedback while maintaining product quality. Recently, generative\nmachine learning (ML) has emerged as a powerful tool for modeling complex\ndistributions and generating synthetic data while handling these manufacturing\nuncertainties. However, adopting these generative technologies in dynamic\nmanufacturing systems lacks a functional control-oriented perspective to\ntranslate their probabilistic understanding into actionable process controls\nwhile respecting constraints. This review presents a functional classification\nof Prediction-Based, Direct Policy, Quality Inference, and Knowledge-Integrated\napproaches, offering a perspective for understanding existing ML-enhanced\ncontrol systems and incorporating generative ML. The analysis of generative ML\narchitectures within this framework demonstrates control-relevant properties\nand potential to extend current ML-enhanced approaches where conventional\nmethods prove insufficient. We show generative ML's potential for manufacturing\ncontrol through decision-making applications, process guidance, simulation, and\ndigital twins, while identifying critical research gaps: separation between\ngeneration and control functions, insufficient physical understanding of\nmanufacturing phenomena, and challenges adapting models from other domains. To\naddress these challenges, we propose future research directions aimed at\ndeveloping integrated frameworks that combine generative ML and control\ntechnologies to address the dynamic complexities of modern manufacturing\nsystems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u52a8\u6001\u5236\u9020\u8fc7\u7a0b\u4e2d\u590d\u6742\u7279\u6027\u7684\u76d1\u63a7\u4e0e\u63a7\u5236\u95ee\u9898\uff0c\u63d0\u51fa\u5c06\u751f\u6210\u5f0f\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u4e0e\u63a7\u5236\u7cfb\u7edf\u7ed3\u5408\uff0c\u5e76\u5206\u7c7b\u73b0\u6709\u65b9\u6cd5\u3002\u6587\u4e2d\u5206\u6790\u4e86\u751f\u6210\u5f0fML\u5728\u5236\u9020\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u540c\u65f6\u6307\u51fa\u5f53\u524d\u7814\u7a76\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u52a8\u6001\u5236\u9020\u8fc7\u7a0b\u5177\u6709\u65f6\u53d8\u53c2\u6570\u3001\u975e\u7ebf\u6027\u884c\u4e3a\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002\u751f\u6210\u5f0fML\u80fd\u5efa\u6a21\u590d\u6742\u5206\u5e03\u5e76\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u4f46\u5728\u5236\u9020\u63a7\u5236\u4e2d\u7f3a\u4e4f\u529f\u80fd\u5bfc\u5411\u7684\u89c6\u89d2\uff0c\u9700\u8981\u5c06\u5176\u6982\u7387\u7406\u89e3\u8f6c\u5316\u4e3a\u53ef\u884c\u7684\u63a7\u5236\u7b56\u7565\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u529f\u80fd\u5206\u7c7b\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u9884\u6d4b\u7684\u65b9\u6cd5\u3001\u76f4\u63a5\u7b56\u7565\u3001\u8d28\u91cf\u63a8\u7406\u548c\u77e5\u8bc6\u96c6\u6210\u65b9\u6cd5\uff0c\u5e76\u5728\u8be5\u6846\u67b6\u4e0b\u5206\u6790\u751f\u6210\u5f0fML\u67b6\u6784\u7684\u63a7\u5236\u76f8\u5173\u7279\u6027\u548c\u6f5c\u529b\u3002", "result": "\u751f\u6210\u5f0fML\u5728\u51b3\u7b56\u3001\u8fc7\u7a0b\u6307\u5bfc\u3001\u6a21\u62df\u548c\u6570\u5b57\u5b6a\u751f\u7b49\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4e5f\u5b58\u5728\u751f\u6210\u4e0e\u63a7\u5236\u529f\u80fd\u5206\u79bb\u3001\u5bf9\u5236\u9020\u73b0\u8c61\u7684\u7269\u7406\u7406\u89e3\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u662f\u5f00\u53d1\u96c6\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u751f\u6210\u5f0fML\u548c\u63a7\u5236\u6280\u672f\uff0c\u4ee5\u5e94\u5bf9\u73b0\u4ee3\u5236\u9020\u7cfb\u7edf\u7684\u52a8\u6001\u590d\u6742\u6027\u3002"}}
{"id": "2505.00017", "pdf": "https://arxiv.org/pdf/2505.00017", "abs": "https://arxiv.org/abs/2505.00017", "authors": ["Dezheng Han", "Yibin Jia", "Ruxiao Chen", "Wenjie Han", "Shuaishuai Guo", "Jianbo Wang"], "title": "ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "comment": null, "summary": "To enable precise and fully automated cell type annotation with large\nlanguage models (LLMs), we developed a graph structured feature marker database\nto retrieve entities linked to differential genes for cell reconstruction. We\nfurther designed a multi task workflow to optimize the annotation process.\nCompared to general purpose LLMs, our method improves human evaluation scores\nby up to 0.21 and semantic similarity by 6.1% across 11 tissue types, while\nmore closely aligning with the cognitive logic of manual annotation.", "AI": {"tldr": "\u5229\u7528\u56fe\u7ed3\u6784\u5316\u7279\u5f81\u6807\u8bb0\u6570\u636e\u5e93\u548c\u591a\u4efb\u52a1\u5de5\u4f5c\u6d41\u7a0b\u4f18\u5316\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u7cbe\u786e\u3001\u81ea\u52a8\u5316\u7684\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\uff0c\u63d0\u5347\u4eba\u5de5\u8bc4\u4f30\u5206\u6570\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u4e2d\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u3001\u81ea\u52a8\u5316\u7684\u6ce8\u91ca\u6d41\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86\u56fe\u7ed3\u6784\u5316\u7279\u5f81\u6807\u8bb0\u6570\u636e\u5e93\u4ee5\u68c0\u7d22\u4e0e\u5dee\u5f02\u57fa\u56e0\u76f8\u5173\u7684\u5b9e\u4f53\uff0c\u5e76\u8bbe\u8ba1\u591a\u4efb\u52a1\u5de5\u4f5c\u6d41\u7a0b\u4f18\u5316\u6ce8\u91ca\u8fc7\u7a0b\u3002", "result": "\u76f8\u6bd4\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u572811\u79cd\u7ec4\u7ec7\u7c7b\u578b\u4e2d\u4eba\u5de5\u8bc4\u4f30\u5206\u6570\u63d0\u53470.21\uff0c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u63d0\u9ad86.1%\uff0c\u4e14\u66f4\u7b26\u5408\u4eba\u5de5\u6ce8\u91ca\u7684\u8ba4\u77e5\u903b\u8f91\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u7684\u7cbe\u5ea6\u548c\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2505.00603", "pdf": "https://arxiv.org/pdf/2505.00603", "abs": "https://arxiv.org/abs/2505.00603", "authors": ["Phanish Puranam", "Prothit Sen", "Maciej Workiewicz"], "title": "Can LLMs Help Improve Analogical Reasoning For Strategic Decisions? Experimental Evidence from Humans and GPT-4", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "This study investigates whether large language models, specifically GPT4, can\nmatch human capabilities in analogical reasoning within strategic decision\nmaking contexts. Using a novel experimental design involving source to target\nmatching, we find that GPT4 achieves high recall by retrieving all plausible\nanalogies but suffers from low precision, frequently applying incorrect\nanalogies based on superficial similarities. In contrast, human participants\nexhibit high precision but low recall, selecting fewer analogies yet with\nstronger causal alignment. These findings advance theory by identifying\nmatching, the evaluative phase of analogical reasoning, as a distinct step that\nrequires accurate causal mapping beyond simple retrieval. While current LLMs\nare proficient in generating candidate analogies, humans maintain a comparative\nadvantage in recognizing deep structural similarities across domains. Error\nanalysis reveals that AI errors arise from surface level matching, whereas\nhuman errors stem from misinterpretations of causal structure. Taken together,\nthe results suggest a productive division of labor in AI assisted\norganizational decision making where LLMs may serve as broad analogy\ngenerators, while humans act as critical evaluators, applying the most\ncontextually appropriate analogies to strategic problems.", "AI": {"tldr": "GPT4\u5728\u7c7b\u6bd4\u63a8\u7406\u4e2d\u9ad8\u53ec\u56de\u4f46\u4f4e\u7cbe\u5ea6\uff0c\u4eba\u7c7b\u5219\u9ad8\u7cbe\u5ea6\u4f4e\u53ec\u56de\uff1bAI\u9519\u8bef\u6e90\u4e8e\u8868\u5c42\u5339\u914d\uff0c\u4eba\u7c7b\u9519\u8bef\u6e90\u4e8e\u56e0\u679c\u8bef\u89e3\uff1bAI\u751f\u6210\u7c7b\u6bd4\uff0c\u4eba\u7c7b\u8bc4\u4f30\u5e94\u7528\u3002", "motivation": "\u63a2\u7a76GPT4\u662f\u5426\u80fd\u5728\u6218\u7565\u51b3\u7b56\u7684\u7c7b\u6bd4\u63a8\u7406\u4e2d\u5ab2\u7f8e\u4eba\u7c7b\uff0c\u6bd4\u8f83\u4e24\u8005\u5728\u6df1\u5ea6\u7ed3\u6784\u76f8\u4f3c\u6027\u8bc6\u522b\u4e0a\u7684\u5dee\u5f02\u3002", "method": "\u91c7\u7528\u6e90\u76ee\u6807\u5339\u914d\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5bf9\u6bd4GPT4\u4e0e\u4eba\u7c7b\u5728\u7c7b\u6bd4\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u3002", "result": "GPT4\u53ec\u56de\u7387\u9ad8\u4f46\u7cbe\u5ea6\u4f4e\uff0c\u4eba\u7c7b\u7cbe\u5ea6\u9ad8\u4f46\u53ec\u56de\u7387\u4f4e\uff1bAI\u9519\u8bef\u4e3b\u8981\u7531\u8868\u5c42\u76f8\u4f3c\u6027\u5f15\u8d77\uff0c\u4eba\u7c7b\u9519\u8bef\u7531\u56e0\u679c\u8bef\u89e3\u5bfc\u81f4\u3002", "conclusion": "AI\u9002\u5408\u751f\u6210\u7c7b\u6bd4\uff0c\u4eba\u7c7b\u9002\u5408\u8bc4\u4f30\u5e94\u7528\uff0c\u4e24\u8005\u7ed3\u5408\u53ef\u4f18\u5316\u6218\u7565\u51b3\u7b56\u4e2d\u7684\u7c7b\u6bd4\u63a8\u7406\u3002"}}
{"id": "2505.00216", "pdf": "https://arxiv.org/pdf/2505.00216", "abs": "https://arxiv.org/abs/2505.00216", "authors": ["Xuwei Yang", "Fatemeh Tavakoli", "David B. Emerson", "Anastasis Kratsios"], "title": "Online Federation For Mixtures of Proprietary Agents with Black-Box Encoders", "categories": ["cs.LG", "cs.AI", "cs.GT", "68T05, 68T07, 91A80", "I.2.1; I.2.11; G.1.6"], "comment": "47 pages, 16 figures, 7 tables", "summary": "Most industry-standard generative AIs and feature encoders are proprietary,\noffering only black-box access: their outputs are observable, but their\ninternal parameters and architectures remain hidden from the end-user. This\nblack-box access is especially limiting when constructing mixture-of-expert\ntype ensemble models since the user cannot optimize each proprietary AI's\ninternal parameters. Our problem naturally lends itself to a non-competitive\ngame-theoretic lens where each proprietary AI (agent) is inherently competing\nagainst the other AI agents, with this competition arising naturally due to\ntheir obliviousness of the AI's to their internal structure. In contrast, the\nuser acts as a central planner trying to synchronize the ensemble of competing\nAIs.\n  We show the existence of the unique Nash equilibrium in the online setting,\nwhich we even compute in closed-form by eliciting a feedback mechanism between\nany given time series and the sequence generated by each (proprietary) AI\nagent. Our solution is implemented as a decentralized, federated-learning\nalgorithm in which each agent optimizes their structure locally on their\nmachine without ever releasing any internal structure to the others. We obtain\nrefined expressions for pre-trained models such as transformers, random feature\nmodels, and echo-state networks. Our ``proprietary federated learning''\nalgorithm is implemented on a range of real-world and synthetic time-series\nbenchmarks. It achieves orders-of-magnitude improvements in predictive accuracy\nover natural benchmarks, of which there are surprisingly few due to this\nnatural problem still being largely unexplored.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9ed1\u7bb1\u751f\u6210AI\u548c\u7279\u5f81\u7f16\u7801\u5668\u7684\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u7eb3\u4ec0\u5747\u8861\u5728\u7ebf\u4f18\u5316\u5b9e\u73b0\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u7684\u9ad8\u6548\u534f\u540c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u884c\u4e1a\u6807\u51c6\u7684\u751f\u6210AI\u548c\u7279\u5f81\u7f16\u7801\u5668\u591a\u4e3a\u9ed1\u7bb1\uff0c\u9650\u5236\u4e86\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u7684\u5185\u90e8\u53c2\u6570\u4f18\u5316\u3002\u7814\u7a76\u901a\u8fc7\u975e\u7ade\u4e89\u535a\u5f08\u8bba\u89c6\u89d2\uff0c\u89e3\u51b3\u9ed1\u7bb1AI\u5728\u76f2\u7ade\u4e89\u73af\u5883\u4e2d\u7684\u534f\u540c\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u5229\u7528\u7eb3\u4ec0\u5747\u8861\u7684\u95ed\u5f0f\u89e3\u53cd\u9988\u673a\u5236\uff0c\u4f7f\u5404AI\u4ee3\u7406\u672c\u5730\u4f18\u5316\u53c2\u6570\u4e14\u4e0d\u6cc4\u9732\u5185\u90e8\u7ed3\u6784\uff0c\u652f\u6301\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982Transformer\u3001\u968f\u673a\u7279\u5f81\u6a21\u578b\uff09\u3002", "result": "\u5728\u771f\u5b9e\u548c\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u9884\u6d4b\u7cbe\u5ea6\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u63d0\u5347\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7814\u7a76\u7a7a\u767d\u3002", "conclusion": "\u63d0\u51fa\u7684\u201c\u79c1\u6709\u8054\u90a6\u5b66\u4e60\u201d\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9ed1\u7bb1AI\u7684\u534f\u540c\u4f18\u5316\u96be\u9898\uff0c\u4e3a\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00019", "pdf": "https://arxiv.org/pdf/2505.00019", "abs": "https://arxiv.org/abs/2505.00019", "authors": ["Zheng Zhang", "Jinyi Li", "Yihuai Lan", "Xiang Wang", "Hao Wang"], "title": "An Empirical Study on Prompt Compression for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by Building Trust Workshop at ICLR 2025", "summary": "Prompt engineering enables Large Language Models (LLMs) to perform a variety\nof tasks. However, lengthy prompts significantly increase computational\ncomplexity and economic costs. To address this issue, we study six prompt\ncompression methods for LLMs, aiming to reduce prompt length while maintaining\nLLM response quality. In this paper, we present a comprehensive analysis\ncovering aspects such as generation performance, model hallucinations, efficacy\nin multimodal tasks, word omission analysis, and more. We evaluate these\nmethods across 13 datasets, including news, scientific articles, commonsense\nQA, math QA, long-context QA, and VQA datasets. Our experiments reveal that\nprompt compression has a greater impact on LLM performance in long contexts\ncompared to short ones. In the Longbench evaluation, moderate compression even\nenhances LLM performance. Our code and data is available at\nhttps://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u516d\u79cd\u63d0\u793a\u538b\u7f29\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63d0\u793a\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u54cd\u5e94\u8d28\u91cf\u3002\u901a\u8fc713\u4e2a\u6570\u636e\u96c6\u7684\u8bc4\u4f30\uff0c\u53d1\u73b0\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u538b\u7f29\u63d0\u793a\u5bf9\u6027\u80fd\u5f71\u54cd\u66f4\u5927\uff0c\u9002\u5ea6\u538b\u7f29\u751a\u81f3\u80fd\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u957f\u63d0\u793a\u5e26\u6765\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u7ecf\u6d4e\u6210\u672c\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301LLM\u7684\u54cd\u5e94\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u5e76\u6bd4\u8f83\u4e86\u516d\u79cd\u63d0\u793a\u538b\u7f29\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u5176\u5728\u751f\u6210\u6027\u80fd\u3001\u6a21\u578b\u5e7b\u89c9\u3001\u591a\u6a21\u6001\u4efb\u52a1\u7b49\u65b9\u9762\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u63d0\u793a\u538b\u7f29\u5bf9\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u5f71\u54cd\u66f4\u5927\uff0c\u9002\u5ea6\u538b\u7f29\u5728Longbench\u8bc4\u4f30\u4e2d\u80fd\u63d0\u5347LLM\u6027\u80fd\u3002", "conclusion": "\u63d0\u793a\u538b\u7f29\u662f\u53ef\u884c\u7684\uff0c\u5c24\u5176\u662f\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\uff0c\u9002\u5ea6\u538b\u7f29\u80fd\u4f18\u5316LLM\u8868\u73b0\u3002"}}
{"id": "2505.00610", "pdf": "https://arxiv.org/pdf/2505.00610", "abs": "https://arxiv.org/abs/2505.00610", "authors": ["Ziyan An", "Xia Wang", "Hendrik Baier", "Zirong Chen", "Abhishek Dubey", "Taylor T. Johnson", "Jonathan Sprinkle", "Ayan Mukhopadhyay", "Meiyi Ma"], "title": "Combining LLMs with Logic-Based Framework to Explain MCTS", "categories": ["cs.AI"], "comment": "Accepted by AAMAS-25 as an extended abstract", "summary": "In response to the lack of trust in Artificial Intelligence (AI) for\nsequential planning, we design a Computational Tree Logic-guided large language\nmodel (LLM)-based natural language explanation framework designed for the Monte\nCarlo Tree Search (MCTS) algorithm. MCTS is often considered challenging to\ninterpret due to the complexity of its search trees, but our framework is\nflexible enough to handle a wide range of free-form post-hoc queries and\nknowledge-based inquiries centered around MCTS and the Markov Decision Process\n(MDP) of the application domain. By transforming user queries into logic and\nvariable statements, our framework ensures that the evidence obtained from the\nsearch tree remains factually consistent with the underlying environmental\ndynamics and any constraints in the actual stochastic control process. We\nevaluate the framework rigorously through quantitative assessments, where it\ndemonstrates strong performance in terms of accuracy and factual consistency.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3AI\u5728\u5e8f\u5217\u89c4\u5212\u4e2d\u7f3a\u4e4f\u4fe1\u4efb\u7684\u95ee\u9898\uff0c\u7814\u7a76\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8ba1\u7b97\u6811\u903b\u8f91\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6846\u67b6\uff0c\u4e3a\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7b97\u6cd5\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "motivation": "\u9488\u5bf9MCTS\u96be\u4ee5\u89e3\u91ca\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u7528\u6237\u5bf9\u5176\u51b3\u7b56\u8fc7\u7a0b\u7684\u7406\u89e3\u548c\u4fe1\u4efb\u3002", "method": "\u901a\u8fc7\u5c06\u7528\u6237\u67e5\u8be2\u8f6c\u5316\u4e3a\u903b\u8f91\u548c\u53d8\u91cf\u9648\u8ff0\uff0c\u5229\u7528LLM\u6846\u67b6\u5904\u7406MCTS\u548cMDP\u76f8\u5173\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u3002", "result": "\u5b9a\u91cf\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u51c6\u786e\u6027\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347MCTS\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u641c\u7d22\u6811\u548c\u968f\u673a\u63a7\u5236\u8fc7\u7a0b\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2505.00225", "pdf": "https://arxiv.org/pdf/2505.00225", "abs": "https://arxiv.org/abs/2505.00225", "authors": ["Bogireddy Sai Prasanna Teja", "Valliappan Muthukaruppan", "Carls Benjamin"], "title": "Predicting Estimated Times of Restoration for Electrical Outages Using Longitudinal Tabular Transformers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As climate variability increases, the ability of utility providers to deliver\nprecise Estimated Times of Restoration (ETR) during natural disasters has\nbecome increasingly critical. Accurate and timely ETRs are essential for\nenabling customer preparedness during extended power outages, where informed\ndecision-making can be crucial, particularly in severe weather conditions.\nNonetheless, prevailing utility practices predominantly depend on manual\nassessments or traditional statistical methods, which often fail to achieve the\nlevel of precision required for reliable and actionable predictions. To address\nthese limitations, we propose a Longitudinal Tabular Transformer (LTT) model\nthat leverages historical outage event data along with sequential updates of\nthese events to improve the accuracy of ETR predictions. The model's\nperformance was evaluated over 34,000 storm-related outage events from three\nmajor utility companies, collectively serving over 3 million customers over a\n2-year period. Results demonstrate that the LTT model improves the Customer\nSatisfaction Impact (CSI) metric by an average of 19.08% (p > 0.001) compared\nto existing methods. Additionally, we introduce customer-informed regression\nmetrics that align model evaluation with real-world satisfaction, ensuring the\noutcomes resonate with customer expectations. Furthermore, we employ\ninterpretability techniques to analyze the temporal significance of\nincorporating sequential updates in modeling outage events and to identify the\ncontributions of predictive features to a given ETR. This comprehensive\napproach not only improves predictive accuracy but also enhances transparency,\nfostering greater trust in the model's capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7eb5\u5411\u8868\u683cTransformer\uff08LTT\uff09\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u81ea\u7136\u707e\u5bb3\u671f\u95f4\u7535\u529b\u6062\u590d\u65f6\u95f4\uff08ETR\uff09\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u5386\u53f2\u505c\u7535\u4e8b\u4ef6\u6570\u636e\u548c\u987a\u5e8f\u66f4\u65b0\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5728\u5f53\u524d\u6c14\u5019\u53d8\u5316\u52a0\u5267\u7684\u80cc\u666f\u4e0b\uff0c\u7535\u529b\u516c\u53f8\u9700\u8981\u63d0\u4f9b\u51c6\u786e\u7684\u7535\u529b\u6062\u590d\u65f6\u95f4\u9884\u6d4b\uff0c\u4ee5\u4fbf\u5ba2\u6237\u80fd\u66f4\u597d\u5730\u51c6\u5907\u5e94\u5bf9\u957f\u65f6\u95f4\u7684\u65ad\u7535\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u624b\u52a8\u6216\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u7684\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7eb5\u5411\u8868\u683cTransformer\uff08LTT\uff09\u6a21\u578b\uff0c\u5229\u7528\u5386\u53f2\u505c\u7535\u6570\u636e\u53ca\u5176\u987a\u5e8f\u66f4\u65b0\u6765\u4f18\u5316ETR\u9884\u6d4b\u3002", "result": "\u5728\u4e09\u5927\u516c\u7528\u4e8b\u4e1a\u516c\u53f834000\u591a\u8d77\u98ce\u66b4\u76f8\u5173\u505c\u7535\u4e8b\u4ef6\u4e0a\u7684\u6d4b\u8bd5\u663e\u793a\uff0cLTT\u6a21\u578b\u7684\u5ba2\u6237\u6ee1\u610f\u5ea6\u5f71\u54cd\uff08CSI\uff09\u6307\u6807\u5e73\u5747\u63d0\u9ad8\u4e8619.08%\uff08p > 0.001\uff09\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5f15\u5165\u4e86\u5ba2\u6237\u5bfc\u5411\u7684\u56de\u5f52\u6307\u6807\u548c\u53ef\u89e3\u91ca\u6027\u6280\u672f\u3002", "conclusion": "LTT\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86ETR\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\uff0c\u589e\u5f3a\u4e86\u5ba2\u6237\u5bf9\u6a21\u578b\u7684\u4fe1\u4efb\u3002"}}
{"id": "2505.00020", "pdf": "https://arxiv.org/pdf/2505.00020", "abs": "https://arxiv.org/abs/2505.00020", "authors": ["Sruly Rosenblat", "Tim O'Reilly", "Ilan Strauss"], "title": "Beyond Public Access in LLM Pre-Training Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Using a legally obtained dataset of 34 copyrighted O'Reilly Media books, we\napply the DE-COP membership inference attack method to investigate whether\nOpenAI's large language models were trained on copyrighted content without\nconsent. Our AUROC scores show that GPT-4o, OpenAI's more recent and capable\nmodel, demonstrates strong recognition of paywalled O'Reilly book content\n(AUROC = 82\\%), compared to OpenAI's earlier model GPT-3.5 Turbo. In contrast,\nGPT-3.5 Turbo shows greater relative recognition of publicly accessible\nO'Reilly book samples. GPT-4o Mini, as a much smaller model, shows no knowledge\nof public or non-public O'Reilly Media content when tested (AUROC $\\approx$\n50\\%). Testing multiple models, with the same cutoff date, helps us account for\npotential language shifts over time that might bias our findings. These results\nhighlight the urgent need for increased corporate transparency regarding\npre-training data sources as a means to develop formal licensing frameworks for\nAI content training", "AI": {"tldr": "\u8bba\u6587\u4f7f\u7528\u5408\u6cd5\u83b7\u53d6\u768434\u672cO'Reilly Media\u4e66\u7c4d\u6570\u636e\u96c6\uff0c\u901a\u8fc7DE-COP\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86OpenAI\u7684\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u672a\u7ecf\u8bb8\u53ef\u4f7f\u7528\u4e86\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u5185\u5bb9\u3002\u7ed3\u679c\u8868\u660e\uff0cGPT-4o\u5bf9\u6536\u8d39\u7684O'Reilly\u4e66\u7c4d\u5185\u5bb9\u8bc6\u522b\u7387\u8f83\u9ad8\uff08AUROC=82%\uff09\uff0c\u800cGPT-3.5 Turbo\u5bf9\u516c\u5f00\u5185\u5bb9\u7684\u8bc6\u522b\u7387\u66f4\u9ad8\uff0c\u8f83\u5c0f\u7684GPT-4o Mini\u5219\u672a\u663e\u793a\u5bf9O'Reilly\u5185\u5bb9\u7684\u4e86\u89e3\u3002\u7814\u7a76\u547c\u5401\u4f01\u4e1a\u63d0\u9ad8\u9884\u8bad\u7ec3\u6570\u636e\u6e90\u7684\u900f\u660e\u5ea6\uff0c\u4ee5\u5efa\u7acb\u6b63\u5f0f\u7684AI\u5185\u5bb9\u8bad\u7ec3\u8bb8\u53ef\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u8c03\u67e5OpenAI\u7684\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u672a\u7ecf\u8bb8\u53ef\u4f7f\u7528\u4e86\u53d7\u7248\u6743\u4fdd\u62a4\u7684O'Reilly Media\u4e66\u7c4d\u5185\u5bb9\uff0c\u4ee5\u547c\u5401\u4f01\u4e1a\u5728AI\u8bad\u7ec3\u6570\u636e\u65b9\u9762\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u5408\u6cd5\u6027\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u91c7\u7528\u4e86DE-COP\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\uff0c\u5bf9OpenAI\u7684\u4e0d\u540c\u6a21\u578b\uff08GPT-4o\u3001GPT-3.5 Turbo\u548cGPT-4o Mini\uff09\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u5b83\u4eec\u5bf9\u6536\u8d39\u548c\u516c\u5f00O'Reilly\u4e66\u7c4d\u5185\u5bb9\u7684\u8bc6\u522b\u80fd\u529b\u3002", "result": "GPT-4o\u663e\u793a\u51fa\u5bf9\u6536\u8d39O'Reilly\u4e66\u7c4d\u5185\u5bb9\u7684\u8f83\u5f3a\u8bc6\u522b\u80fd\u529b\uff08AUROC=82%\uff09\uff0c\u800cGPT-3.5 Turbo\u5bf9\u516c\u5f00\u5185\u5bb9\u7684\u8bc6\u522b\u7387\u66f4\u9ad8\u3002GPT-4o Mini\u5219\u672a\u663e\u793a\u5bf9O'Reilly\u5185\u5bb9\u7684\u4e86\u89e3\uff08AUROC\u224850%\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u5f3a\u8c03\u4e86\u4f01\u4e1a\u9700\u8981\u63d0\u9ad8\u9884\u8bad\u7ec3\u6570\u636e\u6e90\u7684\u900f\u660e\u5ea6\uff0c\u4ee5\u5efa\u7acb\u6b63\u5f0f\u7684AI\u5185\u5bb9\u8bad\u7ec3\u8bb8\u53ef\u6846\u67b6\uff0c\u786e\u4fdd\u7248\u6743\u5408\u89c4\u6027\u3002"}}
{"id": "2505.00612", "pdf": "https://arxiv.org/pdf/2505.00612", "abs": "https://arxiv.org/abs/2505.00612", "authors": ["D. Sculley", "Will Cukierski", "Phil Culliton", "Sohier Dane", "Maggie Demkin", "Ryan Holbrook", "Addison Howard", "Paul Mooney", "Walter Reade", "Megan Risdal", "Nate Keating"], "title": "Position: AI Competitions Provide the Gold Standard for Empirical Rigor in GenAI Evaluation", "categories": ["cs.AI"], "comment": null, "summary": "In this position paper, we observe that empirical evaluation in Generative AI\nis at a crisis point since traditional ML evaluation and benchmarking\nstrategies are insufficient to meet the needs of evaluating modern GenAI models\nand systems. There are many reasons for this, including the fact that these\nmodels typically have nearly unbounded input and output spaces, typically do\nnot have a well defined ground truth target, and typically exhibit strong\nfeedback loops and prediction dependence based on context of previous model\noutputs. On top of these critical issues, we argue that the problems of {\\em\nleakage} and {\\em contamination} are in fact the most important and difficult\nissues to address for GenAI evaluations. Interestingly, the field of AI\nCompetitions has developed effective measures and practices to combat leakage\nfor the purpose of counteracting cheating by bad actors within a competition\nsetting. This makes AI Competitions an especially valuable (but underutilized)\nresource. Now is time for the field to view AI Competitions as the gold\nstandard for empirical rigor in GenAI evaluation, and to harness and harvest\ntheir results with according value.", "AI": {"tldr": "\u672c\u6587\u8ba4\u4e3a\u5f53\u524d\u751f\u6210\u5f0fAI\u7684\u5b9e\u8bc1\u8bc4\u4f30\u6b63\u5904\u4e8e\u5371\u673a\u70b9\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u73b0\u4ee3GenAI\u6a21\u578b\u7684\u8bc4\u4f30\u9700\u6c42\u3002\u4e3b\u8981\u539f\u56e0\u5305\u62ec\u8f93\u5165\u8f93\u51fa\u7a7a\u95f4\u65e0\u9650\u3001\u7f3a\u4e4f\u660e\u786e\u771f\u5b9e\u76ee\u6807\u53ca\u5f3a\u53cd\u9988\u5faa\u73af\u7b49\u3002\u6587\u4e2d\u5f3a\u8c03\u2018\u6cc4\u6f0f\u2019\u548c\u2018\u6c61\u67d3\u2019\u662f\u8bc4\u4f30\u4e2d\u6700\u5173\u952e\u4e14\u96be\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51faAI\u7ade\u8d5b\u7684\u6807\u51c6\u53ef\u4f5c\u4e3aGenAI\u8bc4\u4f30\u7684\u9ec4\u91d1\u6807\u51c6\u3002", "motivation": "\u4f20\u7edfML\u8bc4\u4f30\u65b9\u6cd5\u5728\u5e94\u5bf9\u73b0\u4ee3\u751f\u6210\u5f0fAI\u6a21\u578b\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u6025\u9700\u65b0\u7684\u8bc4\u4f30\u7b56\u7565\u4ee5\u89e3\u51b3\u8f93\u5165\u8f93\u51fa\u7a7a\u95f4\u65e0\u9650\u3001\u7f3a\u4e4f\u771f\u5b9e\u76ee\u6807\u53ca\u5f3a\u53cd\u9988\u5faa\u73af\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u501f\u9274AI\u7ade\u8d5b\u4e2d\u7684\u9632\u6cc4\u6f0f\u63aa\u65bd\u548c\u5b9e\u8df5\uff0c\u5c06\u8fd9\u4e9b\u6807\u51c6\u4f5c\u4e3a\u751f\u6210\u5f0fAI\u8bc4\u4f30\u7684\u4e25\u8c28\u57fa\u7840\u3002", "result": "AI\u7ade\u8d5b\u7684\u9632\u6cc4\u6f0f\u65b9\u6cd5\u4e3a\u89e3\u51b3\u751f\u6210\u5f0fAI\u8bc4\u4f30\u4e2d\u7684\u5173\u952e\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5176\u6807\u51c6\u5e94\u88ab\u89c6\u4e3a\u8bc4\u4f30\u7684\u9ec4\u91d1\u6807\u51c6\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u9886\u57df\u5e94\u5c06AI\u7ade\u8d5b\u7684\u4e25\u683c\u6807\u51c6\u4f5c\u4e3a\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u5145\u5206\u5229\u7528\u5176\u6210\u679c\u4ee5\u63d0\u5347\u8bc4\u4f30\u7684\u4e25\u8c28\u6027\u3002"}}
{"id": "2505.00232", "pdf": "https://arxiv.org/pdf/2505.00232", "abs": "https://arxiv.org/abs/2505.00232", "authors": ["Jiuqiang Tang", "Raman Sarokin", "Ekaterina Ignasheva", "Grant Jensen", "Lin Chen", "Juhyun Lee", "Andrei Kulik", "Matthias Grundmann"], "title": "Scaling On-Device GPU Inference for Large Generative Models", "categories": ["cs.LG", "cs.AI"], "comment": "to be published in CVPR 2025 Workshop on Efficient and On-Device\n  Generation (EDGE)", "summary": "Driven by the advancements in generative AI, large machine learning models\nhave revolutionized domains such as image processing, audio synthesis, and\nspeech recognition. While server-based deployments remain the locus of peak\nperformance, the imperative for on-device inference, necessitated by privacy\nand efficiency considerations, persists. Recognizing GPUs as the on-device ML\naccelerator with the widest reach, we present ML Drift--an optimized framework\nthat extends the capabilities of state-of-the-art GPU-accelerated inference\nengines. ML Drift enables on-device execution of generative AI workloads which\ncontain 10 to 100x more parameters than existing on-device generative AI\nmodels. ML Drift addresses intricate engineering challenges associated with\ncross-GPU API development, and ensures broad compatibility across mobile and\ndesktop/laptop platforms, thereby facilitating the deployment of significantly\nmore complex models on resource-constrained devices. Our GPU-accelerated ML/AI\ninference engine achieves an order-of-magnitude performance improvement\nrelative to existing open-source GPU inference engines.", "AI": {"tldr": "ML Drift \u662f\u4e00\u4e2a\u4f18\u5316\u7684\u6846\u67b6\uff0c\u80fd\u5728\u8d44\u6e90\u6709\u9650\u7684\u8bbe\u5907\u4e0a\u6267\u884c\u53c2\u6570\u89c4\u6a21\u5927 10 \u5230 100 \u500d\u7684\u751f\u6210\u5f0f AI \u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347 GPU \u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u548c\u6548\u7387\u9700\u6c42\uff0c\u9700\u8981\u5728\u8bbe\u5907\u7aef\u8fdb\u884c\u63a8\u7406\uff0c\u800c\u73b0\u6709 GPU \u52a0\u901f\u63a8\u7406\u5f15\u64ce\u65e0\u6cd5\u9ad8\u6548\u652f\u6301\u5927\u89c4\u6a21\u751f\u6210\u5f0f AI \u6a21\u578b\u3002", "method": "\u63d0\u51fa ML Drift \u6846\u67b6\uff0c\u4f18\u5316\u8de8 GPU API \u5f00\u53d1\uff0c\u786e\u4fdd\u5e7f\u6cdb\u7684\u79fb\u52a8\u548c\u684c\u9762/\u7b14\u8bb0\u672c\u7535\u8111\u517c\u5bb9\u6027\u3002", "result": "ML Drift \u5728 GPU \u52a0\u901f\u7684 AI \u63a8\u7406\u5f15\u64ce\u4e0a\u5b9e\u73b0\u4e86\u76f8\u5bf9\u4e8e\u73b0\u6709\u5f00\u6e90\u5f15\u64ce\u6570\u91cf\u7ea7\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "ML Drift \u80fd\u591f\u663e\u8457\u589e\u5f3a\u8bbe\u5907\u7aef\u751f\u6210\u5f0f AI \u7684\u90e8\u7f72\u80fd\u529b\uff0c\u4e3a\u5927\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.00021", "pdf": "https://arxiv.org/pdf/2505.00021", "abs": "https://arxiv.org/abs/2505.00021", "authors": ["Zhuoang Cai", "Zhenghao Li", "Yang Liu", "Liyuan Guo", "Yangqiu Song"], "title": "Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Classification tasks often suffer from imbal- anced data distribution, which\npresents chal- lenges in food hazard detection due to severe class imbalances,\nshort and unstructured text, and overlapping semantic categories. In this\npaper, we present our system for SemEval- 2025 Task 9: Food Hazard Detection,\nwhich ad- dresses these issues by applying data augmenta- tion techniques to\nimprove classification perfor- mance. We utilize transformer-based models, BERT\nand RoBERTa, as backbone classifiers and explore various data balancing\nstrategies, including random oversampling, Easy Data Augmentation (EDA), and\nfocal loss. Our ex- periments show that EDA effectively mitigates class\nimbalance, leading to significant improve- ments in accuracy and F1 scores.\nFurthermore, combining focal loss with oversampling and EDA further enhances\nmodel robustness, par- ticularly for hard-to-classify examples. These findings\ncontribute to the development of more effective NLP-based classification models\nfor food hazard detection.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\uff1a\u4e3a\u89e3\u51b3\u98df\u54c1\u5371\u5bb3\u68c0\u6d4b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4f5c\u8005\u91c7\u7528\u6570\u636e\u589e\u5f3a\u6280\u672f\uff08\u5982\u968f\u673a\u8fc7\u91c7\u6837\u3001EDA\u548c\u7126\u70b9\u635f\u5931\uff09\u7ed3\u5408BERT\u548cRoBERTa\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u98df\u54c1\u5371\u5bb3\u68c0\u6d4b\u9762\u4e34\u6570\u636e\u7c7b\u522b\u4e25\u91cd\u4e0d\u5e73\u8861\u3001\u6587\u672c\u77ed\u4e14\u975e\u7ed3\u6784\u5316\u3001\u8bed\u4e49\u7c7b\u522b\u91cd\u53e0\u7b49\u6311\u6218\uff0c\u9700\u6539\u8fdb\u5206\u7c7b\u6a21\u578b\u6548\u679c\u3002", "method": "\u4f7f\u7528BERT\u548cRoBERTa\u4f5c\u4e3a\u4e3b\u5e72\u5206\u7c7b\u5668\uff0c\u7ed3\u5408\u968f\u673a\u8fc7\u91c7\u6837\u3001Easy Data Augmentation (EDA)\u548c\u7126\u70b9\u635f\u5931\u7b49\u6570\u636e\u5e73\u8861\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEDA\u6709\u6548\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u548cF1\u503c\uff1b\u7ed3\u5408\u7126\u70b9\u635f\u5931\u4e0e\u8fc7\u91c7\u6837\u53caEDA\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u98df\u54c1\u5371\u5bb3\u68c0\u6d4b\u7684NLP\u5206\u7c7b\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00651", "pdf": "https://arxiv.org/pdf/2505.00651", "abs": "https://arxiv.org/abs/2505.00651", "authors": ["Yazan Otoum", "Arghavan Asad", "Ishtiaq Ahmad"], "title": "Open-Source LLM-Driven Federated Transformer for Predictive IoV Management", "categories": ["cs.AI", "cs.ET", "cs.LG"], "comment": "Preprint version; submitted for academic peer review", "summary": "The proliferation of connected vehicles within the Internet of Vehicles (IoV)\necosystem presents critical challenges in ensuring scalable, real-time, and\nprivacy-preserving traffic management. Existing centralized IoV solutions often\nsuffer from high latency, limited scalability, and reliance on proprietary\nArtificial Intelligence (AI) models, creating significant barriers to\nwidespread deployment, particularly in dynamic and privacy-sensitive\nenvironments. Meanwhile, integrating Large Language Models (LLMs) in vehicular\nsystems remains underexplored, especially concerning prompt optimization and\neffective utilization in federated contexts. To address these challenges, we\npropose the Federated Prompt-Optimized Traffic Transformer (FPoTT), a novel\nframework that leverages open-source LLMs for predictive IoV management. FPoTT\nintroduces a dynamic prompt optimization mechanism that iteratively refines\ntextual prompts to enhance trajectory prediction. The architecture employs a\ndual-layer federated learning paradigm, combining lightweight edge models for\nreal-time inference with cloud-based LLMs to retain global intelligence. A\nTransformer-driven synthetic data generator is incorporated to augment training\nwith diverse, high-fidelity traffic scenarios in the Next Generation Simulation\n(NGSIM) format. Extensive evaluations demonstrate that FPoTT, utilizing\nEleutherAI Pythia-1B, achieves 99.86% prediction accuracy on real-world data\nwhile maintaining high performance on synthetic datasets. These results\nunderscore the potential of open-source LLMs in enabling secure, adaptive, and\nscalable IoV management, offering a promising alternative to proprietary\nsolutions in smart mobility ecosystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFPoTT\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u5f00\u6e90LLMs\u8fdb\u884c\u9884\u6d4b\u6027IoV\u7ba1\u7406\uff0c\u901a\u8fc7\u52a8\u6001\u63d0\u793a\u4f18\u5316\u548c\u53cc\u5c42\u6b21\u8054\u90a6\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u7684\u8f68\u8ff9\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u96c6\u4e2d\u5f0fIoV\u65b9\u6848\u7684\u9ad8\u5ef6\u8fdf\u3001\u4f4e\u6269\u5c55\u6027\u548c\u5bf9\u4e13\u6709AI\u6a21\u578b\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u540c\u65f6\u63a2\u7d22LLMs\u5728\u8f66\u8f86\u7cfb\u7edf\u4e2d\u7684\u4f18\u5316\u548c\u5e94\u7528\u3002", "method": "\u63d0\u51faFPoTT\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u6001\u63d0\u793a\u4f18\u5316\u673a\u5236\u548c\u53cc\u5c42\u6b21\u8054\u90a6\u5b66\u4e60\uff0c\u5229\u7528\u5f00\u6e90LLMs\uff08\u5982EleutherAI Pythia-1B\uff09\u548cTransformer\u751f\u6210\u5408\u6210\u6570\u636e\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8fbe\u523099.86%\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "conclusion": "FPoTT\u5c55\u793a\u4e86\u5f00\u6e90LLMs\u5728\u5b89\u5168\u3001\u81ea\u9002\u5e94\u548c\u53ef\u6269\u5c55\u7684IoV\u7ba1\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.00234", "pdf": "https://arxiv.org/pdf/2505.00234", "abs": "https://arxiv.org/abs/2505.00234", "authors": ["Vishnu Sarukkai", "Zhiqiang Xie", "Kayvon Fatahalian"], "title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Many methods for improving Large Language Model (LLM) agents for sequential\ndecision-making tasks depend on task-specific knowledge engineering--such as\nprompt tuning, curated in-context examples, or customized observation and\naction spaces. Using these approaches, agent performance improves with the\nquality or amount of knowledge engineering invested. Instead, we investigate\nhow LLM agents can automatically improve their performance by learning\nin-context from their own successful experiences on similar tasks. Rather than\nrelying on task-specific knowledge engineering, we focus on constructing and\nrefining a database of self-generated examples. We demonstrate that even a\nnaive accumulation of successful trajectories across training tasks boosts test\nperformance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%),\nand InterCode-SQL (75% to 79%)--matching the performance the initial agent\nachieves if allowed two to three attempts per task. We then introduce two\nextensions: (1) database-level selection through population-based training to\nidentify high-performing example collections, and (2) exemplar-level selection\nthat retains individual trajectories based on their empirical utility as\nin-context examples. These extensions further enhance performance, achieving\n91% on ALFWorld--matching more complex approaches that employ task-specific\ncomponents and prompts. Our results demonstrate that automatic trajectory\ndatabase construction offers a compelling alternative to labor-intensive\nknowledge engineering.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u81ea\u6211\u751f\u6210\u7684\u6210\u529f\u7ecf\u9a8c\u6570\u636e\u5e93\u6765\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u7684\u77e5\u8bc6\u5de5\u7a0b\u3002\u901a\u8fc7\u7d2f\u79ef\u6210\u529f\u8f68\u8ff9\u548c\u4e24\u79cd\u9009\u62e9\u7b56\u7565\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4e14\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u63d0\u9ad8LLM\u4ee3\u7406\u6027\u80fd\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u7684\u77e5\u8bc6\u5de5\u7a0b\uff08\u5982\u63d0\u793a\u8c03\u6574\u6216\u5b9a\u5236\u89c2\u5bdf/\u52a8\u4f5c\u7a7a\u95f4\uff09\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6295\u5165\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u4ee3\u7406\u81ea\u6211\u5b66\u4e60\u6765\u51cf\u5c11\u8fd9\u79cd\u4f9d\u8d56\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6784\u5efa\u548c\u4f18\u5316\u81ea\u6211\u751f\u6210\u7684\u6210\u529f\u8f68\u8ff9\u6570\u636e\u5e93\u3002\u9996\u5148\u901a\u8fc7\u7d2f\u79ef\u6210\u529f\u8f68\u8ff9\u63d0\u5347\u6027\u80fd\uff1b\u968f\u540e\u5f15\u5165\u4e24\u79cd\u6269\u5c55\uff1a\u57fa\u4e8e\u79cd\u7fa4\u8bad\u7ec3\u7684\u6570\u636e\u5e93\u7ea7\u9009\u62e9\u548c\u57fa\u4e8e\u5b9e\u8bc1\u6548\u7528\u7684\u793a\u4f8b\u7ea7\u9009\u62e9\uff0c\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728ALFWorld\uff0873%\u523089%\uff09\u3001Wordcraft\uff0855%\u523064%\uff09\u548cInterCode-SQL\uff0875%\u523079%\uff09\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u6700\u9ad8\u8fbe\u523091%\uff0c\u4e0e\u590d\u6742\u4efb\u52a1\u7279\u5b9a\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u81ea\u52a8\u5316\u7684\u8f68\u8ff9\u6570\u636e\u5e93\u6784\u5efa\u662f\u66ff\u4ee3\u4eba\u5de5\u5bc6\u96c6\u578b\u77e5\u8bc6\u5de5\u7a0b\u7684\u6709\u6548\u65b9\u6848\uff0c\u5c24\u5176\u5728\u51cf\u5c11\u4efb\u52a1\u7279\u5b9a\u4f9d\u8d56\u6027\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\u3002"}}
{"id": "2505.00022", "pdf": "https://arxiv.org/pdf/2505.00022", "abs": "https://arxiv.org/abs/2505.00022", "authors": ["Thomas F Burns", "Letitia Parcalabescu", "Stephan W\u00e4ldchen", "Michael Barlow", "Gregor Ziegltrum", "Volker Stampa", "Bastian Harren", "Bj\u00f6rn Deiseroth"], "title": "Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 3 figures", "summary": "Scaling data quantity is essential for large language models (LLMs), yet\nrecent findings show that data quality can significantly boost performance and\ntraining efficiency. We introduce a German-language dataset curation pipeline\nthat combines heuristic and model-based filtering techniques with synthetic\ndata generation. We use our pipeline to create Aleph-Alpha-GermanWeb, a\nlarge-scale German pre-training dataset which draws from: (1) Common Crawl web\ndata, (2) FineWeb2, and (3) synthetically-generated data conditioned on actual,\norganic web data. We evaluate our dataset by pre-training both a 1B Llama-style\nmodel and an 8B tokenizer-free hierarchical autoregressive transformer (HAT). A\ncomparison on German-language benchmarks, including MMMLU, shows significant\nperformance gains of Aleph-Alpha-GermanWeb over FineWeb2 alone. This advantage\nholds at the 8B scale even when FineWeb2 is enriched by human-curated\nhigh-quality data sources such as Wikipedia. Our findings support the growing\nbody of evidence that model-based data curation and synthetic data generation\ncan significantly enhance LLM pre-training datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5fb7\u8bed\u6570\u636e\u96c6\u4f18\u5316\u6d41\u7a0b\uff0c\u7ed3\u5408\u542f\u53d1\u5f0f\u548c\u6a21\u578b\u8fc7\u6ee4\u6280\u672f\u53ca\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u521b\u5efa\u4e86\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6570\u636e\u96c6Aleph-Alpha-GermanWeb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fb7\u8bedLLM\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u6570\u636e\u89c4\u6a21\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u6570\u636e\u8d28\u91cf\u66f4\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u65e8\u5728\u4f18\u5316\u5fb7\u8bed\u6570\u636e\u8d28\u91cf\u4ee5\u589e\u5f3aLLM\u6548\u679c\u3002", "method": "\u91c7\u7528\u542f\u53d1\u5f0f\u4e0e\u6a21\u578b\u8fc7\u6ee4\u7ed3\u5408\u7684\u6280\u672f\uff0c\u5e76\u5f15\u5165\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u6784\u5efa\u4e86Aleph-Alpha-GermanWeb\u6570\u636e\u96c6\u3002\u6570\u636e\u6e90\u5305\u62ecCommon Crawl\u3001FineWeb2\u53ca\u57fa\u4e8e\u771f\u5b9e\u7f51\u9875\u7684\u5408\u6210\u6570\u636e\u3002\u901a\u8fc7\u8bad\u7ec31B\u53c2\u6570\u7684Llama\u98ce\u683c\u6a21\u578b\u548c8B\u7684\u65e0\u5206\u8bcd\u5668\u5206\u5c42\u81ea\u56de\u5f52Transformer\uff08HAT\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u5fb7\u8bed\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982MMMLU\uff09\u4e2d\uff0cAleph-Alpha-GermanWeb\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528FineWeb2\u7684\u6570\u636e\u96c6\uff0c\u751a\u81f3\u57288B\u89c4\u6a21\u4e0b\u4f18\u4e8e\u52a0\u5165\u4eba\u7c7b\u7cbe\u9009\u6570\u636e\uff08\u5982\u7ef4\u57fa\u767e\u79d1\uff09\u7684FineWeb2\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\uff0c\u57fa\u4e8e\u6a21\u578b\u7684\u6570\u636e\u4f18\u5316\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u80fd\u663e\u8457\u63d0\u5347LLM\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u8d28\u91cf\uff0c\u652f\u6301\u4e86\u6570\u636e\u8d28\u91cf\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.00236", "pdf": "https://arxiv.org/pdf/2505.00236", "abs": "https://arxiv.org/abs/2505.00236", "authors": ["Leifeng Zhang", "Xin Dong", "Shuaibing Jia", "Jianhua Zhang"], "title": "Node2Vec-DGI-EL: A Hierarchical Graph Representation Learning Model for Ingredient-Disease Association Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Traditional Chinese medicine, as an essential component of traditional\nmedicine, contains active ingredients that serve as a crucial source for modern\ndrug development, holding immense therapeutic potential and development value.\nA multi-layered and complex network is formed from Chinese medicine to diseases\nand used to predict the potential associations between Chinese medicine\ningredients and diseases. This study proposes an ingredient-disease association\nprediction model (Node2Vec-DGI-EL) based on hierarchical graph representation\nlearning. First, the model uses the Node2Vec algorithm to extract node\nembedding vectors from the network as the initial features of the nodes. Next,\nthe network nodes are deeply represented and learned using the DGI algorithm to\nenhance the model's expressive power. To improve prediction accuracy and\nrobustness, an ensemble learning method is incorporated to achieve more\naccurate ingredient-disease association predictions. The effectiveness of the\nmodel is then evaluated through a series of theoretical verifications. The\nresults demonstrated that the proposed model significantly outperformed\nexisting methods, achieving an AUC of 0.9987 and an AUPR of 0.9545, thereby\nindicating superior predictive capability. Ablation experiments further\nrevealed the contribution and importance of each module. Additionally, case\nstudies explored potential associations, such as triptonide with hypertensive\nretinopathy and methyl ursolate with colorectal cancer. Molecular docking\nexperiments validated these findings, showing the triptonide-PGR interaction\nand the methyl ursolate-NFE2L2 interaction can bind stable. In conclusion, the\nNode2Vec-DGI-EL model focuses on TCM datasets and effectively predicts\ningredient-disease associations, overcoming the reliance on node semantic\ninformation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5c42\u6b21\u56fe\u8868\u793a\u5b66\u4e60\u7684Node2Vec-DGI-EL\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u4e2d\u836f\u6210\u5206\u4e0e\u75be\u75c5\u7684\u5173\u8054\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9a8c\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u4e2d\u836f\u6210\u5206\u4f5c\u4e3a\u73b0\u4ee3\u836f\u7269\u5f00\u53d1\u7684\u91cd\u8981\u6765\u6e90\uff0c\u5177\u6709\u5de8\u5927\u7684\u6cbb\u7597\u6f5c\u529b\u548c\u53d1\u5c55\u4ef7\u503c\uff0c\u4f46\u5173\u8054\u9884\u6d4b\u7684\u590d\u6742\u6027\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408Node2Vec\u7b97\u6cd5\u63d0\u53d6\u8282\u70b9\u5d4c\u5165\u5411\u91cf\uff0c\u901a\u8fc7DGI\u7b97\u6cd5\u8fdb\u884c\u6df1\u5ea6\u8868\u793a\u5b66\u4e60\uff0c\u5e76\u91c7\u7528\u96c6\u6210\u5b66\u4e60\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u6a21\u578b\u5728AUC\u548cAUPR\u6307\u6807\u4e0a\u5206\u522b\u8fbe\u52300.9987\u548c0.9545\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u6848\u4f8b\u7814\u7a76\u548c\u5206\u5b50\u5bf9\u63a5\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u9884\u6d4b\u7ed3\u679c\u3002", "conclusion": "Node2Vec-DGI-EL\u6a21\u578b\u6709\u6548\u9884\u6d4b\u4e86\u4e2d\u836f\u6210\u5206\u4e0e\u75be\u75c5\u7684\u5173\u8054\uff0c\u514b\u670d\u4e86\u5bf9\u8282\u70b9\u8bed\u4e49\u4fe1\u606f\u7684\u4f9d\u8d56\uff0c\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.00023", "pdf": "https://arxiv.org/pdf/2505.00023", "abs": "https://arxiv.org/abs/2505.00023", "authors": ["Hyunji Lee", "Franck Dernoncourt", "Trung Bui", "Seunghyun Yoon"], "title": "CORG: Generating Answers from Complex, Interrelated Contexts", "categories": ["cs.CL", "cs.AI"], "comment": "published at Findings of NAACL 2025", "summary": "In a real-world corpus, knowledge frequently recurs across documents but\noften contains inconsistencies due to ambiguous naming, outdated information,\nor errors, leading to complex interrelationships between contexts. Previous\nresearch has shown that language models struggle with these complexities,\ntypically focusing on single factors in isolation. We classify these\nrelationships into four types: distracting, ambiguous, counterfactual, and\nduplicated. Our analysis reveals that no single approach effectively addresses\nall these interrelationships simultaneously. Therefore, we introduce Context\nOrganizer (CORG), a framework that organizes multiple contexts into\nindependently processed groups. This design allows the model to efficiently\nfind all relevant answers while ensuring disambiguation. CORG consists of three\nkey components: a graph constructor, a reranker, and an aggregator. Our results\ndemonstrate that CORG balances performance and efficiency effectively,\noutperforming existing grouping methods and achieving comparable results to\nmore computationally intensive, single-context approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Context Organizer (CORG)\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u5904\u7406\u6587\u6863\u4e2d\u7684\u590d\u6742\u4e0a\u4e0b\u6587\u5173\u7cfb\uff0c\u89e3\u51b3\u547d\u540d\u6a21\u7cca\u3001\u4fe1\u606f\u8fc7\u65f6\u7b49\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u8bed\u6599\u5e93\u4e2d\u77e5\u8bc6\u5e38\u56e0\u547d\u540d\u6a21\u7cca\u3001\u4fe1\u606f\u8fc7\u65f6\u6216\u9519\u8bef\u5bfc\u81f4\u4e0a\u4e0b\u6587\u5173\u7cfb\u590d\u6742\uff0c\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u540c\u65f6\u5904\u7406\u591a\u56e0\u7d20\u4ea4\u4e92\u3002", "method": "CORG\u6846\u67b6\u901a\u8fc7\u56fe\u6784\u9020\u5668\u3001\u91cd\u65b0\u6392\u5e8f\u5668\u548c\u805a\u5408\u5668\uff0c\u5c06\u591a\u4e0a\u4e0b\u6587\u5206\u7ec4\u72ec\u7acb\u5904\u7406\uff0c\u786e\u4fdd\u9ad8\u6548\u4e14\u53bb\u6b67\u4e49\u3002", "result": "CORG\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u8868\u73b0\u5747\u8861\uff0c\u4f18\u4e8e\u73b0\u6709\u5206\u7ec4\u65b9\u6cd5\uff0c\u63a5\u8fd1\u8ba1\u7b97\u5bc6\u96c6\u578b\u5355\u4e0a\u4e0b\u6587\u5904\u7406\u7684\u6027\u80fd\u3002", "conclusion": "CORG\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4e0a\u4e0b\u6587\u590d\u6742\u5173\u7cfb\u7684\u5904\u7406\u95ee\u9898\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00257", "pdf": "https://arxiv.org/pdf/2505.00257", "abs": "https://arxiv.org/abs/2505.00257", "authors": ["Zhizhong Tan", "Jiexin Zheng", "Kevin Qi Zhang", "Wenyong Wang"], "title": "Graph Privacy: A Heterogeneous Federated GNN for Trans-Border Financial Data Circulation", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "The sharing of external data has become a strong demand of financial\ninstitutions, but the privacy issue has led to the difficulty of\ninterconnecting different platforms and the low degree of data openness. To\neffectively solve the privacy problem of financial data in trans-border flow\nand sharing, to ensure that the data is available but not visible, to realize\nthe joint portrait of all kinds of heterogeneous data of business organizations\nin different industries, we propose a Heterogeneous Federated Graph Neural\nNetwork (HFGNN) approach. In this method, the distribution of heterogeneous\nbusiness data of trans-border organizations is taken as subgraphs, and the\nsharing and circulation process among subgraphs is constructed as a\nstatistically heterogeneous global graph through a central server. Each\nsubgraph learns the corresponding personalized service model through local\ntraining to select and update the relevant subset of subgraphs with aggregated\nparameters, and effectively separates and combines topological and feature\ninformation among subgraphs. Finally, our simulation experimental results show\nthat the proposed method has higher accuracy performance and faster convergence\nspeed than existing methods.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u91d1\u878d\u673a\u6784\u95f4\u6570\u636e\u5171\u4eab\u4e2d\u7684\u9690\u79c1\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u5f02\u6784\u8054\u90a6\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8de8\u7ec4\u7ec7\u6570\u636e\u89c6\u4e3a\u5b50\u56fe\u5e76\u7ed3\u5408\u5168\u5c40\u56fe\u7ed3\u6784\uff0c\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u4e0b\u7684\u6570\u636e\u5171\u4eab\u4e0e\u9ad8\u6548\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u91d1\u878d\u673a\u6784\u5bf9\u6570\u636e\u5171\u4eab\u6709\u5f3a\u70c8\u9700\u6c42\uff0c\u4f46\u9690\u79c1\u95ee\u9898\u5bfc\u81f4\u6570\u636e\u4e92\u901a\u56f0\u96be\u4e14\u5f00\u653e\u7a0b\u5ea6\u4f4e\u3002\u56e0\u6b64\uff0c\u9700\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u8bc1\u6570\u636e\u53ef\u7528\u4e0d\u53ef\u89c1\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u8de8\u884c\u4e1a\u5f02\u6784\u6570\u636e\u7684\u8054\u5408\u5206\u6790\u3002", "method": "\u63d0\u51fa\u7684\u5f02\u6784\u8054\u90a6\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HFGNN\uff09\u5c06\u8de8\u7ec4\u7ec7\u5f02\u6784\u6570\u636e\u5206\u5e03\u5efa\u6a21\u4e3a\u5b50\u56fe\uff0c\u901a\u8fc7\u4e2d\u5fc3\u670d\u52a1\u5668\u6784\u5efa\u7edf\u8ba1\u5f02\u6784\u7684\u5168\u5c40\u56fe\u3002\u5404\u5b50\u56fe\u901a\u8fc7\u672c\u5730\u8bad\u7ec3\u5b66\u4e60\u4e2a\u6027\u5316\u670d\u52a1\u6a21\u578b\uff0c\u9009\u62e9\u5e76\u66f4\u65b0\u76f8\u5173\u5b50\u56fe\u53c2\u6570\uff0c\u540c\u65f6\u6709\u6548\u5206\u79bb\u4e0e\u7ec4\u5408\u5b50\u56fe\u95f4\u7684\u62d3\u6251\u4e0e\u7279\u5f81\u4fe1\u606f\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HFGNN\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u91d1\u878d\u6570\u636e\u8de8\u5883\u5171\u4eab\u4e2d\u7684\u9690\u79c1\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2505.00024", "pdf": "https://arxiv.org/pdf/2505.00024", "abs": "https://arxiv.org/abs/2505.00024", "authors": ["Shaokun Zhang", "Yi Dong", "Jieyu Zhang", "Jan Kautz", "Bryan Catanzaro", "Andrew Tao", "Qingyun Wu", "Zhiding Yu", "Guilin Liu"], "title": "Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 4 tables, 5 figures", "summary": "Enabling large language models with external tools has become a pivotal\nstrategy for extending their functionality beyond text generation tasks. Prior\nwork typically enhances tool-use abilities by either applying supervised\nfine-tuning (SFT) to enforce tool-call correctness or distilling reasoning\ntraces from stronger models for SFT. However, both approaches fall short,\neither omitting reasoning entirely or producing imitative reasoning that limits\ngeneralization. Inspired by the success of DeepSeek-R1 in eliciting reasoning\nthrough rule-based reinforcement learning, we develop the\nNemotron-Research-Tool-N1 series of tool-using language models using a similar\ntraining paradigm. Instead of restrictively supervising intermediate reasoning\ntraces distilled from stronger models, Nemotron-Research-Tool-N1 is optimized\nwith a binary reward that evaluates only the structural validity and functional\ncorrectness of tool invocations. This lightweight supervision allows the model\nto autonomously internalize reasoning strategies, without the need for\nannotated reasoning trajectories. Experiments on the BFCL and API-Bank\nbenchmarks show that Nemotron-Research-Tool-N1-7B and\nNemotron-Research-Tool-N1-14B, built on Qwen-2.5-7B/14B-Instruct, achieve\nstate-of-the-art results, outperforming GPT-4o on both evaluations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5Nemotron-Research-Tool-N1\u7cfb\u5217\u6a21\u578b\uff0c\u901a\u8fc7\u4e8c\u5143\u5956\u52b1\u4f18\u5316\u5de5\u5177\u8c03\u7528\uff0c\u65e0\u9700\u6807\u6ce8\u63a8\u7406\u8f68\u8ff9\uff0c\u5728BFCL\u548cAPI-Bank\u57fa\u51c6\u4e0a\u8d85\u8d8aGPT-4o\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5de5\u5177\u8c03\u7528\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u5373\u8981\u4e48\u5ffd\u7565\u63a8\u7406\uff0c\u8981\u4e48\u4ea7\u751f\u9650\u5236\u6cdb\u5316\u7684\u6a21\u4eff\u63a8\u7406\u3002", "method": "\u91c7\u7528\u7c7b\u4f3cDeepSeek-R1\u7684\u89c4\u5219\u5f3a\u5316\u5b66\u4e60\uff0c\u4ec5\u8bc4\u4f30\u5de5\u5177\u8c03\u7528\u7684\u7ed3\u6784\u6709\u6548\u6027\u548c\u529f\u80fd\u6b63\u786e\u6027\u3002", "result": "Nemotron-Research-Tool-N1-7B\u548c14B\u5728BFCL\u548cAPI-Bank\u4e0a\u8fbe\u5230SOTA\uff0c\u4f18\u4e8eGPT-4o\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u76d1\u7763\u65b9\u6cd5\u80fd\u81ea\u4e3b\u5185\u5316\u63a8\u7406\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002"}}
{"id": "2505.00265", "pdf": "https://arxiv.org/pdf/2505.00265", "abs": "https://arxiv.org/abs/2505.00265", "authors": ["Yi Yu", "Patrick Filippi", "Thomas F. A. Bishop"], "title": "Field-scale soil moisture estimated from Sentinel-1 SAR data using a knowledge-guided deep learning approach", "categories": ["cs.LG", "eess.IV"], "comment": "Accepted by the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025)", "summary": "Soil moisture (SM) estimation from active microwave data remains challenging\ndue to the complex interactions between radar backscatter and surface\ncharacteristics. While the water cloud model (WCM) provides a semi-physical\napproach for understanding these interactions, its empirical component often\nlimits performance across diverse agricultural landscapes. This research\npresents preliminary efforts for developing a knowledge-guided deep learning\napproach, which integrates WCM principles into a long short-term memory (LSTM)\nmodel, to estimate field SM using Sentinel-1 Synthetic Aperture Radar (SAR)\ndata. Our proposed approach leverages LSTM's capacity to capture spatiotemporal\ndependencies while maintaining physical consistency through a modified\ndual-component loss function, including a WCM-based semi-physical component and\na boundary condition regularisation. The proposed approach is built upon the\nsoil backscatter coefficients isolated from the total backscatter, together\nwith Landsat-resolution vegetation information and surface characteristics. A\nfour-fold spatial cross-validation was performed against in-situ SM data to\nassess the model performance. Results showed the proposed approach reduced SM\nretrieval uncertainties by 0.02 m$^3$/m$^3$ and achieved correlation\ncoefficients (R) of up to 0.64 in areas with varying vegetation cover and\nsurface conditions, demonstrating the potential to address the\nover-simplification in WCM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u77e5\u8bc6\u5f15\u5bfc\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528LSTM\u6a21\u578b\u548c\u6c34\u4e91\u6a21\u578b\uff08WCM\uff09\u539f\u7406\uff0c\u901a\u8fc7Sentinel-1 SAR\u6570\u636e\u4f30\u7b97\u571f\u58e4\u6e7f\u5ea6\uff08SM\uff09\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u6c34\u4e91\u6a21\u578b\uff08WCM\uff09\u7531\u4e8e\u7ecf\u9a8c\u6027\u6210\u5206\u7684\u9650\u5236\uff0c\u96be\u4ee5\u5728\u591a\u6837\u5316\u7684\u519c\u4e1a\u666f\u89c2\u4e2d\u51c6\u786e\u4f30\u7b97\u571f\u58e4\u6e7f\u5ea6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u7ed3\u5408\u7269\u7406\u6a21\u578b\u4f18\u52bf\u4e0e\u6df1\u5ea6\u5b66\u4e60\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528LSTM\u6a21\u578b\uff0c\u878d\u5408WCM\u539f\u7406\uff0c\u8bbe\u8ba1\u53cc\u7ec4\u5206\u635f\u5931\u51fd\u6570\uff08\u5305\u62ec\u534a\u7269\u7406\u7ec4\u5206\u548c\u8fb9\u754c\u6761\u4ef6\u6b63\u5219\u5316\uff09\uff0c\u5229\u7528Sentinel-1 SAR\u6570\u636e\u548cLandsat\u690d\u88ab\u4fe1\u606f\u8fdb\u884c\u571f\u58e4\u6e7f\u5ea6\u4f30\u7b97\u3002", "result": "\u6a21\u578b\u901a\u8fc7\u56db\u6298\u7a7a\u95f4\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u571f\u58e4\u6e7f\u5ea6\u4f30\u7b97\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e\u4e860.02 m\u00b3/m\u00b3\uff0c\u76f8\u5173\u7cfb\u6570\uff08R\uff09\u6700\u9ad8\u8fbe0.64\uff0c\u8868\u660e\u5176\u5728\u591a\u6837\u5316\u690d\u88ab\u548c\u5730\u8868\u6761\u4ef6\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86WCM\u7684\u8fc7\u5ea6\u7b80\u5316\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u4e0e\u7269\u7406\u6a21\u578b\u7ed3\u5408\u5728\u571f\u58e4\u6e7f\u5ea6\u4f30\u7b97\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.00025", "pdf": "https://arxiv.org/pdf/2505.00025", "abs": "https://arxiv.org/abs/2505.00025", "authors": ["Mingda Zhang", "Jianglong Qin"], "title": "A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1", "categories": ["cs.CL", "cs.AI", "I.2.7; J.3"], "comment": "14 pages, 1 figures", "summary": "In recent years, despite foundation models like DeepSeek-R1 and ChatGPT\ndemonstrating significant capabilities in general tasks, professional knowledge\nbarriers, computational resource requirements, and deployment environment\nlimitations have severely hindered their application in actual medical\nscenarios. Addressing these challenges, this paper proposes an efficient\nlightweight medical vertical large language model architecture method,\nsystematically solving the lightweight problem of medical large models from\nthree dimensions: knowledge acquisition, model compression, and computational\noptimization. At the knowledge acquisition level, a knowledge transfer pipeline\nis designed from the fine-tuned DeepSeek-R1-Distill-70B teacher model to the\nDeepSeek-R1-Distill-7B student model, and Low-Rank Adaptation (LoRA) technology\nis adopted to precisely adjust key attention layers. At the model compression\nlevel, compression techniques including 4-bit weight quantization are\nimplemented while preserving the core representation ability for medical\nreasoning. At the computational optimization level, inference optimization\ntechniques such as Flash Attention acceleration and continuous batching are\nintegrated, and a professional prompt template system is constructed to adapt\nto different types of medical problems. Experimental results on medical\nquestion-answering datasets show that the method proposed in this paper\nmaintains professional accuracy while reducing memory consumption by 64.7\\% and\ninference latency by 12.4\\%, providing an effective solution for the\napplication of medical large models in resource-constrained environments such\nas edge computing devices.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u533b\u7597\u5782\u76f4\u5927\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u77e5\u8bc6\u83b7\u53d6\u3001\u6a21\u578b\u538b\u7f29\u548c\u8ba1\u7b97\u4f18\u5316\u4e09\u4e2a\u7ef4\u5ea6\u7cfb\u7edf\u89e3\u51b3\u533b\u7597\u5927\u6a21\u578b\u7684\u8f7b\u91cf\u5316\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4e13\u4e1a\u533b\u7597\u77e5\u8bc6\u5e94\u7528\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u9650\u5236\uff0c\u5982\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u548c\u90e8\u7f72\u73af\u5883\u8981\u6c42\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u77e5\u8bc6\u8f6c\u79fb\u6d41\u7a0b\uff0c\u91c7\u7528LoRA\u6280\u672f\u8c03\u6574\u5173\u952e\u6ce8\u610f\u529b\u5c42\uff1b\u5b9e\u65bd4\u4f4d\u6743\u91cd\u91cf\u5316\u7b49\u538b\u7f29\u6280\u672f\uff1b\u96c6\u6210\u63a8\u7406\u4f18\u5316\u6280\u672f\u5982Flash Attention\u52a0\u901f\u3002", "result": "\u5728\u533b\u7597\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5185\u5b58\u6d88\u8017\u964d\u4f4e64.7%\uff0c\u63a8\u7406\u5ef6\u8fdf\u51cf\u5c1112.4%\uff0c\u540c\u65f6\u4fdd\u6301\u4e13\u4e1a\u51c6\u786e\u6027\u3002", "conclusion": "\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff08\u5982\u8fb9\u7f18\u8ba1\u7b97\u8bbe\u5907\uff09\u4e0b\u7684\u533b\u7597\u5927\u6a21\u578b\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00279", "pdf": "https://arxiv.org/pdf/2505.00279", "abs": "https://arxiv.org/abs/2505.00279", "authors": ["Kyota Kuboki", "Tatsuyoshi Ogawa", "Chu-Hsuan Hsueh", "Shi-Jim Yen", "Kokolo Ikeda"], "title": "Policies of Multiple Skill Levels for Better Strength Estimation in Games", "categories": ["cs.LG"], "comment": "25 pages, 15 figures", "summary": "Accurately estimating human skill levels is crucial for designing effective\nhuman-AI interactions so that AI can provide appropriate challenges or\nguidance. In games where AI players have beaten top human professionals,\nstrength estimation plays a key role in adapting AI behavior to match human\nskill levels. In a previous state-of-the-art study, researchers have proposed a\nstrength estimator trained using human players' match data. Given some matches,\nthe strength estimator computes strength scores and uses them to estimate\nplayer ranks (skill levels). In this paper, we focus on the observation that\nhuman players' behavior tendency varies according to their strength and aim to\nimprove the accuracy of strength estimation by taking this into account.\nSpecifically, in addition to strength scores, we obtain policies for different\nskill levels from neural networks trained using human players' match data. We\nthen combine features based on these policies with the strength scores to\nestimate strength. We conducted experiments on Go and chess. For Go, our method\nachieved an accuracy of 80% in strength estimation when given 10 matches, which\nincreased to 92% when given 20 matches. In comparison, the previous\nstate-of-the-art method had an accuracy of 71% with 10 matches and 84% with 20\nmatches, demonstrating improvements of 8-9%. We observed similar improvements\nin chess. These results contribute to developing a more accurate strength\nestimation method and to improving human-AI interaction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u73a9\u5bb6\u884c\u4e3a\u503e\u5411\u4e0e\u5f3a\u5ea6\u5206\u6570\u7684\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e86\u4eba\u7c7b\u73a9\u5bb6\u6280\u80fd\u6c34\u5e73\u7684\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u5728\u56f4\u68cb\u548c\u56fd\u9645\u8c61\u68cb\u4e0a\u5747\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u51c6\u786e\u4f30\u8ba1\u4eba\u7c7b\u73a9\u5bb6\u7684\u6280\u80fd\u6c34\u5e73\u5bf9\u4e8e\u8bbe\u8ba1\u9ad8\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u4f7fAI\u6839\u636e\u73a9\u5bb6\u6c34\u5e73\u8c03\u6574\u884c\u4e3a\u3002\u5df2\u6709\u7814\u7a76\u901a\u8fc7\u6bd4\u8d5b\u6570\u636e\u8bad\u7ec3\u5f3a\u5ea6\u4f30\u8ba1\u5668\uff0c\u4f46\u672a\u8003\u8651\u73a9\u5bb6\u884c\u4e3a\u503e\u5411\u968f\u6280\u80fd\u6c34\u5e73\u7684\u53d8\u5316\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u653f\u7b56\u7279\u5f81\u6765\u63d0\u5347\u4f30\u8ba1\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u4ece\u4eba\u7c7b\u6bd4\u8d5b\u6570\u636e\u4e2d\u63d0\u53d6\u4e0d\u540c\u6280\u80fd\u6c34\u5e73\u7684\u653f\u7b56\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u4e0e\u5f3a\u5ea6\u5206\u6570\u7ed3\u5408\uff0c\u6784\u5efa\u66f4\u51c6\u786e\u7684\u5f3a\u5ea6\u4f30\u8ba1\u6a21\u578b\u3002\u5b9e\u9a8c\u5728\u56f4\u68cb\u548c\u56fd\u9645\u8c61\u68cb\u4e0a\u8fdb\u884c\u3002", "result": "\u6539\u8fdb\u65b9\u6cd5\u5728\u56f4\u68cb\u4e2d\uff0c10\u573a\u6bd4\u8d5b\u7684\u4f30\u8ba1\u51c6\u786e\u7387\u8fbe\u523080%\uff08\u63d0\u53479%\uff09\uff0c20\u573a\u8fbe92%\uff08\u63d0\u53478%\uff09\uff0c\u56fd\u9645\u8c61\u68cb\u4e2d\u4e5f\u89c2\u5bdf\u5230\u7c7b\u4f3c\u63d0\u5347\u3002", "conclusion": "\u7ed3\u5408\u884c\u4e3a\u503e\u5411\u548c\u5f3a\u5ea6\u5206\u6570\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6280\u80fd\u6c34\u5e73\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u4eba\u673a\u4ea4\u4e92\u8bbe\u8ba1\u3002"}}
{"id": "2505.00026", "pdf": "https://arxiv.org/pdf/2505.00026", "abs": "https://arxiv.org/abs/2505.00026", "authors": ["Ruirui Chen", "Weifeng Jiang", "Chengwei Qin", "Cheston Tan"], "title": "Theory of Mind in Large Language Models: Assessment and Enhancement", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Theory of Mind (ToM)-the ability to infer and reason about others' mental\nstates-is fundamental to human social intelligence. As Large Language Models\n(LLMs) become increasingly integrated into daily life, it is crucial to assess\nand enhance their capacity to interpret and respond to human mental states. In\nthis paper, we review LLMs' ToM capabilities by examining both evaluation\nbenchmarks and the strategies designed to improve them. We focus on widely\nadopted story-based benchmarks and provide an in-depth analysis of methods\naimed at enhancing ToM in LLMs. Furthermore, we outline promising future\nresearch directions informed by recent benchmarks and state-of-the-art\napproaches. Our survey serves as a valuable resource for researchers interested\nin advancing LLMs' ToM capabilities.", "AI": {"tldr": "\u8bba\u6587\u56de\u987e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u5fc3\u667a\u7406\u8bba(ToM)\u80fd\u529b\uff0c\u901a\u8fc7\u5206\u6790\u8bc4\u4f30\u57fa\u51c6\u548c\u6539\u8fdb\u7b56\u7565\uff0c\u63a2\u8ba8\u4e86\u6545\u4e8b\u578b\u57fa\u51c6\u7684\u5c40\u9650\u6027\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5fc3\u667a\u7406\u8bba(ToM)\u662f\u4eba\u7c7b\u793e\u4f1a\u667a\u80fd\u7684\u6838\u5fc3\uff0c\u968f\u7740LLM\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u666e\u53ca\uff0c\u8bc4\u4f30\u548c\u63d0\u5347\u5176\u7406\u89e3\u548c\u5e94\u5bf9\u4eba\u7c7b\u5fc3\u7406\u72b6\u6001\u7684\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bba\u6587\u7efc\u8ff0\u4e86LLM\u7684ToM\u8bc4\u4f30\u57fa\u51c6\u548c\u6539\u8fdb\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u6545\u4e8b\u578b\u57fa\u51c6\u53ca\u5176\u4f18\u5316\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f46\u4ecd\u80fd\u901a\u8fc7\u7279\u5b9a\u65b9\u6cd5\u4f18\u5316LLM\u7684ToM\u8868\u73b0\u3002", "conclusion": "\u8bba\u6587\u4e3a\u5e0c\u671b\u63d0\u5347LLM\u7684ToM\u80fd\u529b\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u8d44\u6e90\uff0c\u5e76\u6307\u660e\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.00290", "pdf": "https://arxiv.org/pdf/2505.00290", "abs": "https://arxiv.org/abs/2505.00290", "authors": ["Hong Xin Xie", "Jian De Sun", "Fan Fu Xue", "Zi Fei Han", "Shan Shan Feng", "Qi Chen"], "title": "Multi-Hierarchical Fine-Grained Feature Mapping Driven by Feature Contribution for Molecular Odor Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Molecular odor prediction is the process of using a molecule's structure to\npredict its smell. While accurate prediction remains challenging, AI models can\nsuggest potential odors. Existing methods, however, often rely on basic\ndescriptors or handcrafted fingerprints, which lack expressive power and hinder\neffective learning. Furthermore, these methods suffer from severe class\nimbalance, limiting the training effectiveness of AI models. To address these\nchallenges, we propose a Feature Contribution-driven Hierarchical Multi-Feature\nMapping Network (HMFNet). Specifically, we introduce a fine-grained, Local\nMulti-Hierarchy Feature Extraction module (LMFE) that performs deep feature\nextraction at the atomic level, capturing detailed features crucial for odor\nprediction. To enhance the extraction of discriminative atomic features, we\nintegrate a Harmonic Modulated Feature Mapping (HMFM). This module dynamically\nlearns feature importance and frequency modulation, improving the model's\ncapability to capture relevant patterns. Additionally, a Global Multi-Hierarchy\nFeature Extraction module (GMFE) is designed to learn global features from the\nmolecular graph topology, enabling the model to fully leverage global\ninformation and enhance its discriminative power for odor prediction. To\nfurther mitigate the issue of class imbalance, we propose a Chemically-Informed\nLoss (CIL). Experimental results demonstrate that our approach significantly\nimproves performance across various deep learning models, highlighting its\npotential to advance molecular structure representation and accelerate the\ndevelopment of AI-driven technologies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHMFNet\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u5c40\u90e8\u548c\u5168\u5c40\u591a\u5c42\u7ea7\u7279\u5f81\u63d0\u53d6\u53ca\u5316\u5b66\u611f\u77e5\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5b50\u6c14\u5473\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5b50\u6c14\u5473\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u57fa\u7840\u63cf\u8ff0\u7b26\u6216\u4eba\u5de5\u7279\u5f81\uff0c\u96be\u4ee5\u6709\u6548\u5b66\u4e60\u4e14\u9762\u4e34\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5bfc\u81f4\u9884\u6d4b\u51c6\u786e\u6027\u53d7\u9650\u3002", "method": "HMFNet\u7ed3\u5408\u4e86\u5c40\u90e8\u591a\u5c42\u7ea7\u7279\u5f81\u63d0\u53d6\uff08LMFE\uff09\u3001\u8c10\u6ce2\u8c03\u5236\u7279\u5f81\u6620\u5c04\uff08HMFM\uff09\u3001\u5168\u5c40\u591a\u5c42\u7ea7\u7279\u5f81\u63d0\u53d6\uff08GMFE\uff09\u548c\u5316\u5b66\u611f\u77e5\u635f\u5931\uff08CIL\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\uff0c\u63a8\u52a8\u4e86\u5206\u5b50\u7ed3\u6784\u8868\u5f81\u7684\u8fdb\u6b65\u3002", "conclusion": "HMFNet\u901a\u8fc7\u591a\u5c42\u7ea7\u7279\u5f81\u63d0\u53d6\u548c\u5316\u5b66\u611f\u77e5\u635f\u5931\uff0c\u4e3aAI\u9a71\u52a8\u7684\u6c14\u5473\u9884\u6d4b\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00027", "pdf": "https://arxiv.org/pdf/2505.00027", "abs": "https://arxiv.org/abs/2505.00027", "authors": ["Jian Zhou", "Jiazheng Li", "Sirui Zhuge", "Hai Zhuge"], "title": "Extracting Abstraction Dimensions by Identifying Syntax Pattern from Texts", "categories": ["cs.CL", "cs.AI", "68T50 (Primary) 91F20 (Secondary)", "I.2.7; I.2.1"], "comment": "25pages, 3 figures, 8 tables", "summary": "This paper proposed an approach to automatically discovering subject\ndimension, action dimension, object dimension and adverbial dimension from\ntexts to efficiently operate texts and support query in natural language. The\nhigh quality of trees guarantees that all subjects, actions, objects and\nadverbials and their subclass relations within texts can be represented. The\nindependency of trees ensures that there is no redundant representation between\ntrees. The expressiveness of trees ensures that the majority of sentences can\nbe accessed from each tree and the rest of sentences can be accessed from at\nleast one tree so that the tree-based search mechanism can support querying in\nnatural language. Experiments show that the average precision, recall and\nF1-score of the abstraction trees constructed by the subclass relations of\nsubject, action, object and adverbial are all greater than 80%. The application\nof the proposed approach to supporting query in natural language demonstrates\nthat different types of question patterns for querying subject or object have\nhigh coverage of texts, and searching multiple trees on subject, action, object\nand adverbial according to the question pattern can quickly reduce search space\nto locate target sentences, which can support precise operation on texts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u6587\u672c\u4e2d\u81ea\u52a8\u53d1\u73b0\u4e3b\u8bed\u3001\u52a8\u4f5c\u3001\u5bbe\u8bed\u548c\u72b6\u8bed\u7ef4\u5ea6\u7684\u65b9\u6cd5\uff0c\u4ee5\u9ad8\u6548\u64cd\u4f5c\u6587\u672c\u5e76\u652f\u6301\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u3002\u5b9e\u9a8c\u663e\u793a\u5176\u6784\u5efa\u7684\u62bd\u8c61\u6811\u5728\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u8d85\u8fc780%\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u63d0\u53d6\u6587\u672c\u4e2d\u7684\u5173\u952e\u7ef4\u5ea6\uff08\u4e3b\u8bed\u3001\u52a8\u4f5c\u3001\u5bbe\u8bed\u3001\u72b6\u8bed\uff09\uff0c\u652f\u6301\u9ad8\u6548\u6587\u672c\u64cd\u4f5c\u548c\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff0c\u51cf\u5c11\u5197\u4f59\u5e76\u63d0\u5347\u67e5\u8be2\u6548\u7387\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6811\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b50\u7c7b\u5173\u7cfb\u6784\u5efa\u72ec\u7acb\u4e14\u9ad8\u8868\u8fbe\u529b\u7684\u62bd\u8c61\u6811\uff0c\u8986\u76d6\u5927\u90e8\u5206\u53e5\u5b50\u5e76\u652f\u6301\u591a\u6811\u8054\u5408\u641c\u7d22\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u62bd\u8c61\u6811\u7684\u5e73\u5747\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5747\u8d85\u8fc780%\uff0c\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8986\u76d6\u7387\u9ad8\u4e14\u80fd\u5feb\u901f\u5b9a\u4f4d\u76ee\u6807\u53e5\u5b50\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6811\u72b6\u7ed3\u6784\u548c\u591a\u7ef4\u5ea6\u641c\u7d22\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u64cd\u4f5c\u548c\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u6548\u7387\u548c\u7cbe\u786e\u6027\u3002"}}
{"id": "2505.00291", "pdf": "https://arxiv.org/pdf/2505.00291", "abs": "https://arxiv.org/abs/2505.00291", "authors": ["Eran Rosenbluth", "Martin Grohe"], "title": "Repetition Makes Perfect: Recurrent Sum-GNNs Match Message Passing Limit", "categories": ["cs.LG", "68T05, 68T07", "I.2.6"], "comment": null, "summary": "We provide first tight bounds for the expressivity of Recurrent Graph Neural\nNetworks (recurrent GNNs) with finite-precision parameters. We prove that\nrecurrent GNNs, with sum aggregation and ReLU activation, can emulate any graph\nalgorithm that respects the natural message-passing invariance induced by the\ncolor refinement (or Weisfeiler-Leman) algorithm. While it is well known that\nthe expressive power of GNNs is limited by this invariance [Morris et al., AAAI\n2019; Xu et al., ICLR 2019], we establish that recurrent GNNs can actually\nreach this limit. This is in contrast to non-recurrent GNNs, which have the\npower of Weisfeiler-Leman only in a very weak, \"non-uniform\", sense where every\ngraph size requires a different GNN model to compute with. The emulation we\nconstruct introduces only a polynomial overhead in both time and space.\n  Furthermore, we show that by incorporating random initialization, recurrent\nGNNs can emulate all graph algorithms, implying in particular that any graph\nalgorithm with polynomial-time complexity can be emulated by a recurrent GNN\nwith random initialization, running in polynomial time.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u6709\u9650\u7cbe\u5ea6\u53c2\u6570\u7684\u5faa\u73af\u56fe\u795e\u7ecf\u7f51\u7edc\uff08recurrent GNNs\uff09\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u8fbe\u5230\u4e86\u989c\u8272\u7ec6\u5316\u7b97\u6cd5\uff08Weisfeiler-Leman\uff09\u7684\u6781\u9650\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u521d\u59cb\u5316\u5b9e\u73b0\u4e86\u5bf9\u6240\u6709\u56fe\u7b97\u6cd5\u7684\u6a21\u62df\u3002", "motivation": "\u7814\u7a76\u5faa\u73afGNNs\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5176\u5728\u989c\u8272\u7ec6\u5316\u7b97\u6cd5\uff08Weisfeiler-Leman\uff09\u6846\u67b6\u4e0b\u7684\u6781\u9650\u6027\u80fd\uff0c\u5e76\u63a2\u8ba8\u5176\u4e0e\u4f20\u7edf\u975e\u5faa\u73afGNNs\u7684\u533a\u522b\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u6bd4\u8f83\u5faa\u73afGNNs\u4e0e\u975e\u5faa\u73afGNNs\u5728Weisfeiler-Leman\u6846\u67b6\u4e0b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u968f\u673a\u521d\u59cb\u5316\u6269\u5c55\u5176\u6a21\u62df\u80fd\u529b\u3002", "result": "\u5faa\u73afGNNs\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u4e0e\u7a7a\u95f4\u5f00\u9500\u5185\u53ef\u6a21\u62df\u4efb\u4f55\u989c\u8272\u7ec6\u5316\u7b97\u6cd5\u6846\u67b6\u5185\u7684\u56fe\u7b97\u6cd5\uff0c\u4e14\u901a\u8fc7\u968f\u673a\u521d\u59cb\u5316\u53ef\u6a21\u62df\u6240\u6709\u591a\u9879\u5f0f\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u56fe\u7b97\u6cd5\u3002", "conclusion": "\u5faa\u73afGNNs\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u8fbe\u5230\u4e86\u7406\u8bba\u6781\u9650\uff0c\u4e14\u5176\u6027\u80fd\u53ef\u901a\u8fc7\u968f\u673a\u521d\u59cb\u5316\u8fdb\u4e00\u6b65\u6269\u5c55\uff0c\u4e3a\u56fe\u7b97\u6cd5\u6a21\u62df\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2505.00028", "pdf": "https://arxiv.org/pdf/2505.00028", "abs": "https://arxiv.org/abs/2505.00028", "authors": ["Pengchao Feng", "Ziyang Ma", "Wenxi Chen", "Yao Li", "Sheng Wang", "Kai Yu", "Xie Chen"], "title": "Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "In recent years, end-to-end speech-to-speech (S2S) dialogue systems have\ngarnered increasing research attention due to their advantages over traditional\ncascaded systems, including achieving lower latency and more natural\nintegration of nonverbal cues such as emotion and speaker identity. However,\nthese end-to-end systems face key challenges, particularly in incorporating\nexternal knowledge, a capability commonly addressed by Retrieval-Augmented\nGeneration (RAG) in text-based large language models (LLMs). The core\ndifficulty lies in the modality gap between input speech and retrieved textual\nknowledge, which hinders effective integration. To address this issue, we\npropose a novel end-to-end RAG framework that directly retrieves relevant\ntextual knowledge from speech queries, eliminating the need for intermediate\nspeech-to-text conversion via techniques like ASR. Experimental results\ndemonstrate that our method significantly improves the performance of\nend-to-end S2S dialogue systems while achieving higher retrieval efficiency.\nAlthough the overall performance still lags behind cascaded models, our\nframework offers a promising direction for enhancing knowledge integration in\nend-to-end S2S systems. We will release the code and dataset to support\nreproducibility and promote further research in this area.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7aef\u5230\u7aefRAG\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u8bed\u97f3\u67e5\u8be2\u4e2d\u68c0\u7d22\u76f8\u5173\u6587\u672c\u77e5\u8bc6\uff0c\u65e0\u9700\u4e2d\u95f4\u8bed\u97f3\u8f6c\u6587\u672c\u6b65\u9aa4\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u9ad8\u4e86\u68c0\u7d22\u6548\u7387\u3002", "motivation": "\u76ee\u524d\u7aef\u5230\u7aef\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u5728\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u9762\u4e34\u8bed\u97f3\u4e0e\u6587\u672c\u6a21\u6001\u95f4\u7684\u9e3f\u6c9f\u95ee\u9898\u3002\u4f20\u7edfRAG\u65b9\u6cd5\u9700\u8981\u8bed\u97f3\u8f6c\u6587\u672c\u6b65\u9aa4\uff0c\u800c\u8bba\u6587\u65e8\u5728\u6d88\u9664\u8fd9\u4e00\u4e2d\u95f4\u73af\u8282\uff0c\u76f4\u63a5\u63d0\u5347\u77e5\u8bc6\u6574\u5408\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684RAG\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u4ece\u8bed\u97f3\u67e5\u8be2\u4e2d\u68c0\u7d22\u76f8\u5173\u6587\u672c\u77e5\u8bc6\uff0c\u7ed5\u8fc7\u4e86\u4f20\u7edf\u8bed\u97f3\u8f6c\u6587\u672c\uff08ASR\uff09\u7684\u4e2d\u95f4\u6b65\u9aa4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u68c0\u7d22\u6548\u7387\u3002\u5c3d\u7ba1\u6027\u80fd\u4ecd\u4f4e\u4e8e\u7ea7\u8054\u6a21\u578b\uff0c\u4f46\u4e3a\u77e5\u8bc6\u6574\u5408\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7aef\u5230\u7aef\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u7684\u77e5\u8bc6\u6574\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u5c06\u901a\u8fc7\u516c\u5f00\u4ee3\u7801\u548c\u6570\u636e\u96c6\u63a8\u52a8\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.00302", "pdf": "https://arxiv.org/pdf/2505.00302", "abs": "https://arxiv.org/abs/2505.00302", "authors": ["Xinlong Zhao", "Liying Zhang", "Tianbo Zou", "Yan Zhang"], "title": "Temporal Attention Evolutional Graph Convolutional Network for Multivariate Time Series Forecasting", "categories": ["cs.LG", "68T09 (Primary), 68T07 (Secondary)"], "comment": "13 pages, 7 figures", "summary": "Multivariate time series forecasting enables the prediction of future states\nby leveraging historical data, thereby facilitating decision-making processes.\nEach data node in a multivariate time series encompasses a sequence of multiple\ndimensions. These nodes exhibit interdependent relationships, forming a graph\nstructure. While existing prediction methods often assume a fixed graph\nstructure, many real-world scenarios involve dynamic graph structures.\nMoreover, interactions among time series observed at different time scales vary\nsignificantly. To enhance prediction accuracy by capturing precise temporal and\nspatial features, this paper introduces the Temporal Attention Evolutional\nGraph Convolutional Network (TAEGCN). This novel method not only integrates\ncausal temporal convolution and a multi-head self-attention mechanism to learn\ntemporal features of nodes, but also construct the dynamic graph structure\nbased on these temporal features to keep the consistency of the changing in\nspatial feature with temporal series. TAEGCN adeptly captures temporal causal\nrelationships and hidden spatial dependencies within the data. Furthermore,\nTAEGCN incorporates a unified neural network that seamlessly integrates these\ncomponents to generate final predictions. Experimental results conducted on two\npublic transportation network datasets, METR-LA and PEMS-BAY, demonstrate the\nsuperior performance of the proposed model.", "AI": {"tldr": "TAEGCN\u6a21\u578b\u7ed3\u5408\u65f6\u95f4\u6ce8\u610f\u529b\u548c\u52a8\u6001\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u63d0\u5347\u4e86\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u52a8\u6001\u56fe\u7ed3\u6784\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5e38\u5448\u73b0\u52a8\u6001\u56fe\u7ed3\u6784\uff0c\u4e14\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u7684\u4ea4\u4e92\u53d8\u5316\u663e\u8457\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u5047\u8bbe\u56fa\u5b9a\u56fe\u7ed3\u6784\uff0c\u96be\u4ee5\u6355\u6349\u7cbe\u786e\u7684\u65f6\u7a7a\u7279\u5f81\u3002", "method": "\u63d0\u51faTAEGCN\uff0c\u6574\u5408\u56e0\u679c\u65f6\u95f4\u5377\u79ef\u3001\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u8282\u70b9\u65f6\u95f4\u7279\u5f81\uff0c\u5e76\u57fa\u4e8e\u65f6\u95f4\u7279\u5f81\u6784\u5efa\u52a8\u6001\u56fe\u7ed3\u6784\u4ee5\u786e\u4fdd\u65f6\u7a7a\u53d8\u5316\u4e00\u81f4\u6027\u3002", "result": "\u5728METR-LA\u548cPEMS-BAY\u6570\u636e\u96c6\u4e0a\uff0cTAEGCN\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "TAEGCN\u80fd\u6709\u6548\u6355\u6349\u65f6\u7a7a\u4f9d\u8d56\u6027\uff0c\u4e3a\u52a8\u6001\u56fe\u7ed3\u6784\u4e0b\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00029", "pdf": "https://arxiv.org/pdf/2505.00029", "abs": "https://arxiv.org/abs/2505.00029", "authors": ["Yijie Hong", "Xiaofei Yin", "Xinzhong Wang", "Yi Tu", "Ya Guo", "Sufeng Duan", "Weiqiang Wang", "Lingyong Fang", "Depeng Wang", "Huijia Zhu"], "title": "Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for Knowledge Injection without Catastrophic Forgetting", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 3 figures", "summary": "Large Vision Language Models have demonstrated impressive versatile\ncapabilities through extensive multimodal pre-training, but face significant\nlimitations when incorporating specialized knowledge domains beyond their\ntraining distribution. These models struggle with a fundamental dilemma: direct\nadaptation approaches that inject domain-specific knowledge often trigger\ncatastrophic forgetting of foundational visual-linguistic abilities. We\nintroduce Structured Dialogue Fine-Tuning (SDFT), an effective approach that\neffectively injects domain-specific knowledge while minimizing catastrophic\nforgetting. Drawing inspiration from supervised fine-tuning in LLMs and\nsubject-driven personalization in text-to-image diffusion models, our method\nemploys a three-phase dialogue structure: Foundation Preservation reinforces\npre-trained visual-linguistic alignment through caption tasks; Contrastive\nDisambiguation introduces carefully designed counterfactual examples to\nmaintain semantic boundaries; and Knowledge Specialization embeds specialized\ninformation through chain-of-thought reasoning. Experimental results across\nmultiple domains confirm SDFT's effectiveness in balancing specialized\nknowledge acquisition with general capability retention. Our key contributions\ninclude a data-centric dialogue template that balances foundational alignment\nwith targeted knowledge integration, a weighted multi-turn supervision\nframework, and comprehensive evaluation across diverse knowledge types.", "AI": {"tldr": "SDFT\u662f\u4e00\u79cd\u901a\u8fc7\u4e09\u9636\u6bb5\u5bf9\u8bdd\u7ed3\u6784\uff08\u57fa\u7840\u4fdd\u7559\u3001\u5bf9\u6bd4\u6d88\u6b67\u3001\u77e5\u8bc6\u4e13\u4e1a\u5316\uff09\u6709\u6548\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\u5e76\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u7684\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u5206\u5e03\u4e4b\u5916\u7684\u9886\u57df\u77e5\u8bc6\u6574\u5408\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u76f4\u63a5\u9002\u5e94\u65b9\u6cd5\u5bb9\u6613\u5bfc\u81f4\u57fa\u7840\u80fd\u529b\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u5bf9\u8bdd\u7ed3\u6784\uff1a\u57fa\u7840\u4fdd\u7559\uff08\u901a\u8fc7\u6807\u9898\u4efb\u52a1\u5f3a\u5316\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\uff09\u3001\u5bf9\u6bd4\u6d88\u6b67\uff08\u4f7f\u7528\u53cd\u4e8b\u5b9e\u793a\u4f8b\u7ef4\u62a4\u8bed\u4e49\u8fb9\u754c\uff09\u3001\u77e5\u8bc6\u4e13\u4e1a\u5316\uff08\u901a\u8fc7\u601d\u7ef4\u94fe\u63a8\u7406\u5d4c\u5165\u9886\u57df\u77e5\u8bc6\uff09\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eSDFT\u80fd\u5e73\u8861\u9886\u57df\u77e5\u8bc6\u5b66\u4e60\u548c\u901a\u7528\u80fd\u529b\u4fdd\u7559\uff0c\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SDFT\u901a\u8fc7\u7ed3\u6784\u5316\u5bf9\u8bdd\u6a21\u677f\u548c\u591a\u8f6e\u76d1\u7763\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u9886\u57df\u9002\u5e94\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002"}}
{"id": "2505.00307", "pdf": "https://arxiv.org/pdf/2505.00307", "abs": "https://arxiv.org/abs/2505.00307", "authors": ["Yu-Hsiang Lan", "Anton Alyakin", "Eric K. Oermann"], "title": "Gateformer: Advancing Multivariate Time Series Forecasting through Temporal and Variate-Wise Attention with Gated Representations", "categories": ["cs.LG"], "comment": null, "summary": "There has been a recent surge of interest in time series modeling using the\nTransformer architecture. However, forecasting multivariate time series with\nTransformer presents a unique challenge as it requires modeling both temporal\n(cross-time) and variate (cross-variate) dependencies. While Transformer-based\nmodels have gained popularity for their flexibility in capturing both\nsequential and cross-variate relationships, it is unclear how to best integrate\nthese two sources of information in the context of the Transformer architecture\nwhile optimizing for both performance and efficiency. We re-purpose the\nTransformer architecture to effectively model both cross-time and cross-variate\ndependencies. Our approach begins by embedding each variate independently into\na variate-wise representation that captures its cross-time dynamics, and then\nmodels cross-variate dependencies through attention mechanisms on these learned\nembeddings. Gating operations in both cross-time and cross-variate modeling\nphases regulate information flow, allowing the model to focus on the most\nrelevant features for accurate predictions. Our method achieves\nstate-of-the-art performance across 13 real-world datasets and can be\nseamlessly integrated into other Transformer-based and LLM-based forecasters,\ndelivering performance improvements up to 20.7\\% over original models. Code is\navailable at this repository: https://github.com/nyuolab/Gateformer.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684Transformer\u67b6\u6784\uff08Gateformer\uff09\uff0c\u901a\u8fc7\u72ec\u7acb\u5d4c\u5165\u53d8\u91cf\u548c\u5b66\u4e60\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u5efa\u6a21\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u8de8\u65f6\u95f4\u548c\u8de8\u53d8\u91cf\u4f9d\u8d56\uff0c\u5e76\u572813\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "Transformer\u5728\u5904\u7406\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u65f6\u9762\u4e34\u540c\u65f6\u5efa\u6a21\u8de8\u65f6\u95f4\u548c\u8de8\u53d8\u91cf\u4f9d\u8d56\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u9ad8\u6548\u6574\u5408\u8fd9\u4e24\u79cd\u4fe1\u606f\u3002", "method": "\u901a\u8fc7\u72ec\u7acb\u5d4c\u5165\u53d8\u91cf\u6355\u83b7\u8de8\u65f6\u95f4\u52a8\u6001\uff0c\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u5efa\u6a21\u8de8\u53d8\u91cf\u4f9d\u8d56\uff0c\u5e76\u5f15\u5165\u95e8\u63a7\u673a\u5236\u4f18\u5316\u4fe1\u606f\u6d41\u3002", "result": "\u572813\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe20.7%\u3002", "conclusion": "Gateformer\u901a\u8fc7\u95e8\u63a7\u548c\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u6574\u5408\u8de8\u65f6\u95f4\u548c\u8de8\u53d8\u91cf\u4fe1\u606f\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u5176\u4ed6Transformer\u6216LLM\u9884\u6d4b\u6a21\u578b\u4e2d\u3002"}}
{"id": "2505.00030", "pdf": "https://arxiv.org/pdf/2505.00030", "abs": "https://arxiv.org/abs/2505.00030", "authors": ["Ted Underwood", "Laura K. Nelson", "Matthew Wilkens"], "title": "Can Language Models Represent the Past without Anachronism?", "categories": ["cs.CL"], "comment": null, "summary": "Before researchers can use language models to simulate the past, they need to\nunderstand the risk of anachronism. We find that prompting a contemporary model\nwith examples of period prose does not produce output consistent with period\nstyle. Fine-tuning produces results that are stylistically convincing enough to\nfool an automated judge, but human evaluators can still distinguish fine-tuned\nmodel outputs from authentic historical text. We tentatively conclude that\npretraining on period prose may be required in order to reliably simulate\nhistorical perspectives for social research.", "AI": {"tldr": "\u7814\u7a76\u5c1d\u8bd5\u7528\u5f53\u4ee3\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u5386\u53f2\u6587\u672c\u98ce\u683c\uff0c\u53d1\u73b0\u4ec5\u63d0\u793a\u6216\u5fae\u8c03\u5747\u65e0\u6cd5\u5b8c\u5168\u590d\u73b0\u5386\u53f2\u98ce\u683c\uff0c\u9700\u9884\u8bad\u7ec3\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u5386\u53f2\u6587\u672c\u98ce\u683c\u7684\u53ef\u884c\u6027\uff0c\u4ee5\u907f\u514d\u65f6\u4ee3\u9519\u4f4d\u98ce\u9669\u3002", "method": "\u6bd4\u8f83\u63d0\u793a\u548c\u5fae\u8c03\u4e24\u79cd\u65b9\u6cd5\u751f\u6210\u7684\u5386\u53f2\u98ce\u683c\u6587\u672c\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u8bc4\u5224\u4e0e\u4eba\u5de5\u8bc4\u4f30\u5bf9\u6bd4\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u80fd\u6b3a\u9a97\u81ea\u52a8\u5316\u8bc4\u5224\uff0c\u4f46\u4eba\u5de5\u4ecd\u80fd\u533a\u5206\u5176\u4e0e\u771f\u5b9e\u5386\u53f2\u6587\u672c\u3002", "conclusion": "\u53ef\u9760\u6a21\u62df\u5386\u53f2\u89c6\u89d2\u53ef\u80fd\u9700\u8981\u9488\u5bf9\u5386\u53f2\u6587\u672c\u8fdb\u884c\u9884\u8bad\u7ec3\u3002"}}
{"id": "2505.00315", "pdf": "https://arxiv.org/pdf/2505.00315", "abs": "https://arxiv.org/abs/2505.00315", "authors": ["Piotr Pi\u0119kos", "R\u00f3bert Csord\u00e1s", "J\u00fcrgen Schmidhuber"], "title": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines.", "AI": {"tldr": "MoSA\u662f\u4e00\u79cd\u65b0\u578b\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9tokens\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u6027\u80fd\u4f18\u4e8e\u5bc6\u96c6\u6ce8\u610f\u529b\uff0c\u540c\u65f6\u8282\u7701\u8d44\u6e90\u3002", "motivation": "\u51cf\u5c11\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u601d\u60f3\uff0c\u52a8\u6001\u9009\u62e9token\uff0c\u5b9e\u73b0\u4efb\u610f\u7a00\u758f\u6ce8\u610f\u529b\u6a21\u5f0f\u3002", "result": "MoSA\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u8868\u73b0\u4f18\u4e8e\u5bc6\u96c6\u57fa\u7ebf\uff0c\u6700\u9ad8\u63d0\u534727%\u56f0\u60d1\u5ea6\uff0c\u5e76\u964d\u4f4e\u8d44\u6e90\u4f7f\u7528\u3002", "conclusion": "MoSA\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2505.00031", "pdf": "https://arxiv.org/pdf/2505.00031", "abs": "https://arxiv.org/abs/2505.00031", "authors": ["Jin Zhang", "Flood Sung", "Zhilin Yang", "Yang Gao", "Chongjie Zhang"], "title": "Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the field of large language model (LLM) post-training, the effectiveness\nof utilizing synthetic data generated by the LLM itself has been\nwell-presented. However, a key question remains unaddressed: what essential\ninformation should such self-generated data encapsulate? Existing approaches\nonly produce step-by-step problem solutions, and fail to capture the abstract\nmeta-knowledge necessary for generalization across similar problems. Drawing\ninsights from cognitive science, where humans employ high-level abstraction to\nsimplify complex problems before delving into specifics, we introduce a novel\nself-training algorithm: LEarning to Plan before Answering (LEPA). LEPA trains\nthe LLM to formulate anticipatory plans, which serve as abstract meta-knowledge\nfor problem-solving, before engaging with the intricacies of problems. This\napproach not only outlines the solution generation path but also shields the\nLLM from the distraction of irrelevant details. During data generation, LEPA\nfirst crafts an anticipatory plan based on the problem, and then generates a\nsolution that aligns with both the plan and the problem. LEPA refines the plan\nthrough self-reflection, aiming to acquire plans that are instrumental in\nyielding correct solutions. During model optimization, the LLM is trained to\npredict both the refined plans and the corresponding solutions. By efficiently\nextracting and utilizing the anticipatory plans, LEPA demonstrates remarkable\nsuperiority over conventional algorithms on various challenging natural\nlanguage reasoning benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u8bad\u7ec3\u7b97\u6cd5LEPA\uff0c\u901a\u8fc7\u9884\u5148\u751f\u6210\u62bd\u8c61\u5143\u77e5\u8bc6\uff08\u524d\u77bb\u6027\u8ba1\u5212\uff09\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u95ee\u9898\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u751f\u6210\u9010\u6b65\u89e3\u51b3\u65b9\u6848\uff0c\u7f3a\u4e4f\u5bf9\u62bd\u8c61\u5143\u77e5\u8bc6\u7684\u6355\u83b7\uff0c\u800c\u4eba\u7c7b\u5728\u89e3\u51b3\u95ee\u9898\u524d\u901a\u5e38\u4f1a\u5148\u8fdb\u884c\u9ad8\u7ea7\u62bd\u8c61\u3002LEPA\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u7684\u8ba4\u77e5\u8fc7\u7a0b\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "LEPA\u7b97\u6cd5\u5206\u4e24\u6b65\uff1a1) \u57fa\u4e8e\u95ee\u9898\u751f\u6210\u524d\u77bb\u6027\u8ba1\u5212\uff08\u62bd\u8c61\u5143\u77e5\u8bc6\uff09\uff1b2) \u6839\u636e\u8ba1\u5212\u751f\u6210\u5bf9\u9f50\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u4f18\u5316\u8ba1\u5212\u3002\u6a21\u578b\u8bad\u7ec3\u65f6\u540c\u65f6\u9884\u6d4b\u4f18\u5316\u540e\u7684\u8ba1\u5212\u548c\u89e3\u51b3\u65b9\u6848\u3002", "result": "LEPA\u5728\u591a\u9879\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u9a8c\u8bc1\u4e86\u62bd\u8c61\u8ba1\u5212\u5bf9\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u524d\u77bb\u6027\u8ba1\u5212\u4f5c\u4e3a\u5143\u77e5\u8bc6\u80fd\u6709\u6548\u6307\u5bfc\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u95ee\u9898\uff0c\u51cf\u5c11\u65e0\u5173\u7ec6\u8282\u5e72\u6270\u3002LEPA\u4e3a\u81ea\u8bad\u7ec3\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.00316", "pdf": "https://arxiv.org/pdf/2505.00316", "abs": "https://arxiv.org/abs/2505.00316", "authors": ["Tien Comlekoglu", "J. Quetzalc\u00f3atl Toledo-Mar\u00edn", "Tina Comlekoglu", "Douglas W. DeSimone", "Shayn M. Peirce", "Geoffrey Fox", "James A. Glazier"], "title": "Surrogate modeling of Cellular-Potts Agent-Based Models as a segmentation task using the U-Net neural network architecture", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "The Cellular-Potts model is a powerful and ubiquitous framework for\ndeveloping computational models for simulating complex multicellular biological\nsystems. Cellular-Potts models (CPMs) are often computationally expensive due\nto the explicit modeling of interactions among large numbers of individual\nmodel agents and diffusive fields described by partial differential equations\n(PDEs). In this work, we develop a convolutional neural network (CNN) surrogate\nmodel using a U-Net architecture that accounts for periodic boundary\nconditions. We use this model to accelerate the evaluation of a mechanistic CPM\npreviously used to investigate \\textit{in vitro} vasculogenesis. The surrogate\nmodel was trained to predict 100 computational steps ahead (Monte-Carlo steps,\nMCS), accelerating simulation evaluations by a factor of 590 times compared to\nCPM code execution. Over multiple recursive evaluations, our model effectively\ncaptures the emergent behaviors demonstrated by the original Cellular-Potts\nmodel of such as vessel sprouting, extension and anastomosis, and contraction\nof vascular lacunae. This approach demonstrates the potential for deep learning\nto serve as efficient surrogate models for CPM simulations, enabling faster\nevaluation of computationally expensive CPM of biological processes at greater\nspatial and temporal scales.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eU-Net\u67b6\u6784\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u66ff\u4ee3\u6a21\u578b\uff0c\u7528\u4e8e\u52a0\u901f\u6602\u8d35\u7684Cellular-Potts\u6a21\u578b\uff08CPM\uff09\u4eff\u771f\uff0c\u4eff\u771f\u901f\u5ea6\u63d0\u5347\u4e86590\u500d\u3002", "motivation": "\u7531\u4e8eCPM\u5728\u6a21\u62df\u590d\u6742\u591a\u7ec6\u80de\u751f\u7269\u7cfb\u7edf\u65f6\u9700\u8981\u663e\u5f0f\u5efa\u6a21\u5927\u91cf\u4e2a\u4f53\u548c\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528U-Net\u67b6\u6784\u8bbe\u8ba1CNN\u66ff\u4ee3\u6a21\u578b\uff0c\u8003\u8651\u5468\u671f\u6027\u8fb9\u754c\u6761\u4ef6\uff0c\u5e76\u8bad\u7ec3\u8be5\u6a21\u578b\u9884\u6d4b100\u4e2aMCS\u6b65\u9aa4\u7684\u4eff\u771f\u7ed3\u679c\u3002", "result": "\u66ff\u4ee3\u6a21\u578b\u5c06\u4eff\u771f\u901f\u5ea6\u63d0\u5347590\u500d\uff0c\u5e76\u5728\u591a\u6b21\u9012\u5f52\u8bc4\u4f30\u4e2d\u6210\u529f\u6355\u6349\u4e86\u8840\u7ba1\u751f\u6210\u7684\u5173\u952e\u884c\u4e3a\uff08\u5982\u53d1\u82bd\u3001\u5ef6\u4f38\u548c\u543b\u5408\uff09\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53ef\u9ad8\u6548\u66ff\u4ee3\u4f20\u7edfCPM\u4eff\u771f\uff0c\u4e3a\u7814\u7a76\u66f4\u5927\u65f6\u7a7a\u5c3a\u5ea6\u7684\u751f\u7269\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u66f4\u5feb\u7684\u5de5\u5177\u3002"}}
{"id": "2505.00032", "pdf": "https://arxiv.org/pdf/2505.00032", "abs": "https://arxiv.org/abs/2505.00032", "authors": ["Yuyang Sha", "Hongxin Pan", "Wei Xu", "Weiyu Meng", "Gang Luo", "Xinyu Du", "Xiaobing Zhai", "Henry H. Y. Tong", "Caijuan Shi", "Kefeng Li"], "title": "MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Major depressive disorder (MDD) impacts more than 300 million people\nworldwide, highlighting a significant public health issue. However, the uneven\ndistribution of medical resources and the complexity of diagnostic methods have\nresulted in inadequate attention to this disorder in numerous countries and\nregions. This paper introduces a high-performance MDD diagnosis tool named\nMDD-LLM, an AI-driven framework that utilizes fine-tuned large language models\n(LLMs) and extensive real-world samples to tackle challenges in MDD diagnosis.\nTherefore, we select 274,348 individual information from the UK Biobank cohort\nto train and evaluate the proposed method. Specifically, we select 274,348\nindividual records from the UK Biobank cohort and design a tabular data\ntransformation method to create a large corpus for training and evaluating the\nproposed approach. To illustrate the advantages of MDD-LLM, we perform\ncomprehensive experiments and provide several comparative analyses against\nexisting model-based solutions across multiple evaluation metrics. Experimental\nresults show that MDD-LLM (70B) achieves an accuracy of 0.8378 and an AUC of\n0.8919 (95% CI: 0.8799 - 0.9040), significantly outperforming existing machine\nlearning and deep learning frameworks for MDD diagnosis. Given the limited\nexploration of LLMs in MDD diagnosis, we examine numerous factors that may\ninfluence the performance of our proposed method, such as tabular data\ntransformation techniques and different fine-tuning strategies.", "AI": {"tldr": "\u8ad6\u6587\u63d0\u51fa\u4e86\u4e00\u7a2e\u540d\u70baMDD-LLM\u7684\u9ad8\u6027\u80fd\u6291\u9b31\u75c7\u8a3a\u65b7\u5de5\u5177\uff0c\u57fa\u65bcAI\u548c\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5229\u7528UK Biobank\u7684\u5927\u898f\u6a21\u6578\u64da\u96c6\u9032\u884c\u8a13\u7df4\uff0c\u5be6\u9a57\u7d50\u679c\u986f\u793a\u5176\u5728\u6e96\u78ba\u6027\u548cAUC\u4e0a\u986f\u8457\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\u3002", "motivation": "\u5168\u7403\u6291\u9b31\u75c7\u60a3\u8005\u773e\u591a\uff0c\u4f46\u91ab\u7642\u8cc7\u6e90\u5206\u4f48\u4e0d\u5747\u4e14\u8a3a\u65b7\u65b9\u6cd5\u8907\u96dc\uff0c\u5c0e\u81f4\u8a31\u591a\u5730\u5340\u5c0d\u6b64\u75be\u75c5\u7684\u95dc\u6ce8\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u958b\u767c\u9ad8\u6548\u8a3a\u65b7\u5de5\u5177\u8feb\u5728\u7709\u776b\u3002", "method": "\u7814\u7a76\u5f9eUK Biobank\u9078\u53d6274,348\u689d\u500b\u9ad4\u6578\u64da\uff0c\u8a2d\u8a08\u8868\u683c\u6578\u64da\u8f49\u63db\u65b9\u6cd5\u4ee5\u69cb\u5efa\u8a13\u7df4\u8a9e\u6599\u5eab\uff0c\u4e26\u5229\u7528\u5fae\u8abf\u7684\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff08LLMs\uff09\u9032\u884c\u8a13\u7df4\u548c\u8a55\u4f30\u3002", "result": "MDD-LLM\uff0870B\u7248\u672c\uff09\u9054\u52300.8378\u7684\u6e96\u78ba\u7387\u548c0.8919\u7684AUC\uff0895% CI: 0.8799 - 0.9040\uff09\uff0c\u986f\u8457\u512a\u65bc\u73fe\u6709\u7684\u6a5f\u5668\u5b78\u7fd2\u548c\u6df1\u5ea6\u5b78\u7fd2\u6846\u67b6\u3002", "conclusion": "MDD-LLM\u5c55\u793a\u4e86LLMs\u5728\u6291\u9b31\u75c7\u8a3a\u65b7\u4e2d\u7684\u6f5b\u529b\uff0c\u4e26\u63a2\u8a0e\u4e86\u6578\u64da\u8f49\u63db\u6280\u8853\u548c\u5fae\u8abf\u7b56\u7565\u5c0d\u6027\u80fd\u7684\u5f71\u97ff\uff0c\u70ba\u672a\u4f86\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.00326", "pdf": "https://arxiv.org/pdf/2505.00326", "abs": "https://arxiv.org/abs/2505.00326", "authors": ["Apratim Dey", "David Donoho"], "title": "Optimal Vector Compressed Sensing Using James Stein Shrinkage", "categories": ["cs.LG", "eess.IV", "eess.SP", "stat.CO", "stat.ME"], "comment": "69 pages", "summary": "The trend in modern science and technology is to take vector measurements\nrather than scalars, ruthlessly scaling to ever higher dimensional vectors. For\nabout two decades now, traditional scalar Compressed Sensing has been\nsynonymous with a Convex Optimization based procedure called Basis Pursuit. In\nthe vector recovery case, the natural tendency is to return to a\nstraightforward vector extension of Basis Pursuit, also based on Convex\nOptimization. However, Convex Optimization is provably suboptimal, particularly\nwhen $B$ is large. In this paper, we propose SteinSense, a lightweight\niterative algorithm, which is provably optimal when $B$ is large. It does not\nhave any tuning parameter, does not need any training data, requires zero\nknowledge of sparsity, is embarrassingly simple to implement, and all of this\nmakes it easily scalable to high vector dimensions. We conduct a massive volume\nof both real and synthetic experiments that confirm the efficacy of SteinSense,\nand also provide theoretical justification based on ideas from Approximate\nMessage Passing. Fascinatingly, we discover that SteinSense is quite robust,\ndelivering the same quality of performance on real data, and even under\nsubstantial departures from conditions under which existing theory holds.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSteinSense\u7684\u8f7b\u91cf\u7ea7\u8fed\u4ee3\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u7ef4\u5411\u91cf\u6062\u590d\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u57fa\u4e8e\u51f8\u4f18\u5316\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u4f18\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u8c03\u53c2\u6216\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u51f8\u4f18\u5316\u7684\u5411\u91cf\u538b\u7f29\u611f\u77e5\u65b9\u6cd5\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u88ab\u8bc1\u660e\u662f\u6b21\u4f18\u7684\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86SteinSense\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8fed\u4ee3\u65b9\u6cd5\uff0c\u65e0\u9700\u8c03\u53c2\u3001\u8bad\u7ec3\u6570\u636e\u6216\u7a00\u758f\u6027\u77e5\u8bc6\uff0c\u4e14\u6613\u4e8e\u5b9e\u73b0\u548c\u6269\u5c55\u3002", "result": "\u5927\u91cf\u771f\u5b9e\u548c\u5408\u6210\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SteinSense\u7684\u9ad8\u6548\u6027\uff0c\u7406\u8bba\u5206\u6790\u57fa\u4e8e\u8fd1\u4f3c\u6d88\u606f\u4f20\u9012\u601d\u60f3\uff0c\u5e76\u663e\u793a\u51fa\u7b97\u6cd5\u5728\u73b0\u5b9e\u6570\u636e\u548c\u975e\u7406\u60f3\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "SteinSense\u5728\u9ad8\u7ef4\u5411\u91cf\u6062\u590d\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u5177\u6709\u7b80\u5355\u548c\u9c81\u68d2\u7684\u7279\u70b9\u3002"}}
{"id": "2505.00033", "pdf": "https://arxiv.org/pdf/2505.00033", "abs": "https://arxiv.org/abs/2505.00033", "authors": ["Andrew Kiruluta"], "title": "From Attention to Atoms: Spectral Dictionary Learning for Fast, Interpretable Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We propose a novel spectral generative modeling framework for natural\nlanguage processing that jointly learns a global time varying Fourier\ndictionary and per token mixing coefficients, replacing the ubiquitous self\nattention mechanism in transformer architectures. By enforcing reconstruction\nlosses in both the time domain (embedding reconstruction) and the frequency\ndomain (via Short Time Fourier Transform magnitude matching) alongside a\nstandard language modeling objective, and fitting a Gaussian Mixture Model\n(GMM) prior over the learned mixing vectors, our approach achieves competitive\nperplexity and generation quality on standard benchmarks such as WikiText2 and\nPenn Treebank. In contrast to the quadratic computation complexity of self\nattention, our method operates with linear complexity, delivering substantial\nefficiency gains. We demonstrate that spectral dictionary models can achieve\ncompetitive performance compared to transformer baselines while significantly\nreducing inference latency and memory footprint, offering a compelling\nalternative for scalable language modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5149\u8c31\u751f\u6210\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u901a\u8fc7\u5b66\u4e60\u5168\u5c40\u65f6\u53d8\u5085\u91cc\u53f6\u5b57\u5178\u548c\u6bcf\u4e2a\u4ee4\u724c\u7684\u6df7\u5408\u7cfb\u6570\uff0c\u66ff\u4ee3\u4e86Transformer\u67b6\u6784\u4e2d\u666e\u904d\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002\u901a\u8fc7\u65f6\u95f4\u57df\uff08\u5d4c\u5165\u91cd\u6784\uff09\u548c\u9891\u57df\uff08\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\u5e45\u5ea6\u5339\u914d\uff09\u7684\u91cd\u6784\u635f\u5931\u4e0e\u6807\u51c6\u8bed\u8a00\u5efa\u6a21\u76ee\u6807\u76f8\u7ed3\u5408\uff0c\u5e76\u5728\u5b66\u4e60\u7684\u6df7\u5408\u5411\u91cf\u4e0a\u62df\u5408\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u5148\u9a8c\uff0c\u8be5\u6a21\u578b\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u56f0\u60d1\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u3002\u4e0e\u81ea\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u4ee5\u7ebf\u6027\u590d\u6742\u5ea6\u8fd0\u884c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002", "motivation": "\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728Transformer\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u4e14\u8d44\u6e90\u6d88\u8017\u5927\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u5149\u8c31\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548\u4e14\u6027\u80fd\u63a5\u8fd1\u7684\u8bed\u8a00\u5efa\u6a21\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u5b66\u4e60\u5168\u5c40\u65f6\u53d8\u5085\u91cc\u53f6\u5b57\u5178\u548c\u6bcf\u4ee4\u724c\u6df7\u5408\u7cfb\u6570\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u95f4\u57df\u548c\u9891\u57df\u91cd\u6784\u635f\u5931\uff0c\u5e76\u5728\u6df7\u5408\u5411\u91cf\u4e0a\u5e94\u7528GMM\u5148\u9a8c\u3002", "result": "\u5728WikiText2\u548cPenn Treebank\u7b49\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u7684\u56f0\u60d1\u5ea6\u548c\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u5360\u7528\u3002", "conclusion": "\u5149\u8c31\u5b57\u5178\u6a21\u578b\u5728\u6027\u80fd\u63a5\u8fd1Transformer\u57fa\u51c6\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u8bed\u8a00\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u5438\u5f15\u529b\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.00333", "pdf": "https://arxiv.org/pdf/2505.00333", "abs": "https://arxiv.org/abs/2505.00333", "authors": ["Bumjun Kim", "Wan Choi"], "title": "Communication-Efficient Wireless Federated Fine-Tuning for Large-Scale AI Models", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Transformer-based large language models (LLMs) have achieved remarkable\nsuccess across various tasks. Yet, fine-tuning such massive models in federated\nlearning (FL) settings poses significant challenges due to resource constraints\nand communication overhead. Low-Rank Adaptation (LoRA) addresses these issues\nby training compact, low-rank matrices instead of fully fine-tuning large\nmodels. This paper introduces a wireless federated LoRA fine-tuning framework\nthat optimizes both learning performance and communication efficiency. We\nprovide a novel convergence analysis, revealing how LoRA rank and covariance\neffects influence FL training dynamics. Leveraging these insights, we propose\nSparsified Orthogonal Fine-Tuning (\\textbf{SOFT}), an adaptive sparsification\nmethod that streamlines parameter updates without expensive matrix\nmultiplications and singular value decomposition (SVD) operations.\nAdditionally, we present a Two Stage Federated Algorithm (\\textbf{TSFA})\nalgorithm that pre-determines key parameters offline and dynamically adjusts\nbandwidth and sparsification online, ensuring efficient training under latency\nconstraints. Experiments on benchmark datasets show that our approach achieves\naccuracy comparable to ideal scenario models while significantly reducing\ncommunication overhead. Our framework thus enables scalable, resource-efficient\ndeployment of large models in real-world wireless FL scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u7ebf\u8054\u90a6\u5b66\u4e60\u7684LoRA\u5fae\u8c03\u6846\u67b6SOFT\uff0c\u7ed3\u5408TSFA\u7b97\u6cd5\u4f18\u5316\u901a\u4fe1\u6548\u7387\uff0c\u663e\u8457\u964d\u4f4e\u5f00\u9500\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u5c3d\u7ba1Transformer\u5927\u6a21\u578b\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e0b\u5bf9\u5176\u8fdb\u884c\u5fae\u8c03\u9762\u4e34\u8d44\u6e90\u548c\u901a\u4fe1\u5f00\u9500\u7684\u5de8\u5927\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u548c\u521b\u65b0\u7684\u7a00\u758f\u5316\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86SOFT\uff08\u7a00\u758f\u6b63\u4ea4\u5fae\u8c03\uff09\u65b9\u6cd5\uff0c\u907f\u514d\u6602\u8d35\u7684\u77e9\u9635\u8fd0\u7b97\u548cSVD\u64cd\u4f5c\uff1b\u540c\u65f6\u8bbe\u8ba1\u4e86TSFA\uff08\u4e24\u9636\u6bb5\u8054\u90a6\u7b97\u6cd5\uff09\uff0c\u79bb\u7ebf\u9884\u5224\u5173\u952e\u53c2\u6570\u5e76\u5728\u7ebf\u52a8\u6001\u8c03\u6574\u5e26\u5bbd\u548c\u7a00\u758f\u5316\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u4e0e\u7406\u60f3\u573a\u666f\u6a21\u578b\u76f8\u5f53\u7684\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5728\u5b9e\u9645\u65e0\u7ebf\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\u9ad8\u6548\u90e8\u7f72\u5927\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u8d44\u6e90\u7684\u9ad8\u6548\u5229\u7528\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2505.00034", "pdf": "https://arxiv.org/pdf/2505.00034", "abs": "https://arxiv.org/abs/2505.00034", "authors": ["Zijie Lin", "Zikang Liu", "Hanbo Fan"], "title": "Improving Phishing Email Detection Performance of Small Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models(LLMs) have demonstrated remarkable performance on many\nnatural language processing(NLP) tasks and have been employed in phishing email\ndetection research. However, in current studies, well-performing LLMs typically\ncontain billions or even tens of billions of parameters, requiring enormous\ncomputational resources. To reduce computational costs, we investigated the\neffectiveness of small-parameter LLMs for phishing email detection. These LLMs\nhave around 3 billion parameters and can run on consumer-grade GPUs. However,\nsmall LLMs often perform poorly in phishing email detection task. To address\nthese issues, we designed a set of methods including Prompt Engineering,\nExplanation Augmented Fine-tuning, and Model Ensemble to improve phishing email\ndetection capabilities of small LLMs. We validated the effectiveness of our\napproach through experiments, significantly improving accuracy on the\nSpamAssassin dataset from around 0.5 for baseline models like\nQwen2.5-1.5B-Instruct to 0.976.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7Prompt Engineering\u3001Explanation Augmented Fine-tuning\u548cModel Ensemble\u7b49\u65b9\u6cd5\u63d0\u5347\u5c0f\u53c2\u6570\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9493\u9c7c\u90ae\u4ef6\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u65b9\u6cd5\u80fd\u5c06\u51c6\u786e\u7387\u4ece0.5\u63d0\u5347\u81f30.976\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e2d\uff0c\u8868\u73b0\u4f18\u5f02\u7684\u5927\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u9700\u8981\u6570\u5341\u4ebf\u53c2\u6570\u548c\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u5c0f\u53c2\u6570\u6a21\u578b\u5728\u9493\u9c7c\u90ae\u4ef6\u68c0\u6d4b\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u7814\u7a76\u5982\u4f55\u63d0\u5347\u8fd9\u4e9b\u5c0f\u6a21\u578b\u7684\u6027\u80fd\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u6210\u4e3a\u5173\u952e\u3002", "method": "\u91c7\u7528Prompt Engineering\u3001Explanation Augmented Fine-tuning\u548cModel Ensemble\u7b49\u65b9\u6cd5\u4f18\u5316\u5c0f\u53c2\u6570LLMs\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u5728SpamAssassin\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c06Qwen2.5-1.5B-Instruct\u57fa\u7ebf\u6a21\u578b\u7684\u51c6\u786e\u7387\u4ece\u7ea60.5\u63d0\u5347\u81f30.976\u3002", "conclusion": "\u901a\u8fc7\u8bbe\u8ba1\u7279\u5b9a\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5c0f\u53c2\u6570LLMs\u5728\u9493\u9c7c\u90ae\u4ef6\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u53ef\u4ee5\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002"}}
{"id": "2505.00337", "pdf": "https://arxiv.org/pdf/2505.00337", "abs": "https://arxiv.org/abs/2505.00337", "authors": ["Xuyang Guo", "Jiayan Huo", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang", "Jiale Zhao"], "title": "T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Text-to-video generative models have made significant strides in recent\nyears, producing high-quality videos that excel in both aesthetic appeal and\naccurate instruction following, and have become central to digital art creation\nand user engagement online. Yet, despite these advancements, their ability to\nrespect fundamental physical laws remains largely untested: many outputs still\nviolate basic constraints such as rigid-body collisions, energy conservation,\nand gravitational dynamics, resulting in unrealistic or even misleading\ncontent. Existing physical-evaluation benchmarks typically rely on automatic,\npixel-level metrics applied to simplistic, life-scenario prompts, and thus\noverlook both human judgment and first-principles physics. To fill this gap, we\nintroduce \\textbf{T2VPhysBench}, a first-principled benchmark that\nsystematically evaluates whether state-of-the-art text-to-video systems, both\nopen-source and commercial, obey twelve core physical laws including Newtonian\nmechanics, conservation principles, and phenomenological effects. Our benchmark\nemploys a rigorous human evaluation protocol and includes three targeted\nstudies: (1) an overall compliance assessment showing that all models score\nbelow 0.60 on average in each law category; (2) a prompt-hint ablation\nrevealing that even detailed, law-specific hints fail to remedy physics\nviolations; and (3) a counterfactual robustness test demonstrating that models\noften generate videos that explicitly break physical rules when so instructed.\nThe results expose persistent limitations in current architectures and offer\nconcrete insights for guiding future research toward truly physics-aware video\ngeneration.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86T2VPhysBench\u8fd9\u4e00\u65b0\u578b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u5404\u7c7b\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u662f\u5426\u9075\u5faa12\u6761\u6838\u5fc3\u7269\u7406\u5b9a\u5f8b\uff0c\u5e76\u63ed\u793a\u73b0\u6709\u7cfb\u7edf\u7684\u4e0d\u8db3\u3002", "motivation": "\u867d\u7136\u76ee\u524d\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u7f8e\u5b66\u4e0e\u6307\u4ee4\u9075\u5faa\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b83\u4eec\u5e38\u8fdd\u53cd\u57fa\u672c\u7269\u7406\u5b9a\u5f8b\uff08\u5982\u521a\u4f53\u78b0\u649e\u3001\u80fd\u91cf\u5b88\u6052\u7b49\uff09\uff0c\u4e9f\u9700\u4e00\u79cd\u57fa\u4e8e\u57fa\u672c\u539f\u7406\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faT2VPhysBench\uff0c\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u548c\u4e09\u9879\u9488\u5bf9\u6027\u7814\u7a76\uff08\u603b\u4f53\u5408\u89c4\u6027\u5206\u6790\u3001\u63d0\u793a\u8bcd\u6d88\u878d\u5b9e\u9a8c\u3001\u53cd\u4e8b\u5b9e\u9c81\u68d2\u6027\u6d4b\u8bd5\uff09\u7cfb\u7edf\u6027\u6d4b\u8bd5\u6a21\u578b\u5bf9\u7269\u7406\u5b9a\u5f8b\u7684\u9075\u5b88\u60c5\u51b5\u3002", "result": "\u6240\u6709\u6a21\u578b\u572812\u7c7b\u7269\u7406\u5b9a\u5f8b\u4e0a\u7684\u5e73\u5747\u5f97\u5206\u5747\u4f4e\u4e8e0.60\uff1b\u5373\u4f7f\u63d0\u4f9b\u8be6\u7ec6\u63d0\u793a\u8bcd\u4ecd\u65e0\u6cd5\u7ea0\u6b63\u7269\u7406\u9519\u8bef\uff1b\u6a21\u578b\u751a\u81f3\u80fd\u6309\u6307\u4ee4\u751f\u6210\u660e\u663e\u8fdd\u53cd\u7269\u7406\u89c4\u5219\u7684\u89c6\u9891\u3002", "conclusion": "\u5f53\u524d\u67b6\u6784\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u5b9e\u73b0\u771f\u6b63\u5177\u5907\u7269\u7406\u611f\u77e5\u80fd\u529b\u7684\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2505.00035", "pdf": "https://arxiv.org/pdf/2505.00035", "abs": "https://arxiv.org/abs/2505.00035", "authors": ["Aayam Bansal", "Raghav Agarwal", "Kaashvi Jain"], "title": "Linguistic Complexity and Socio-cultural Patterns in Hip-Hop Lyrics", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "This paper presents a comprehensive computational framework for analyzing\nlinguistic complexity and socio-cultural trends in hip-hop lyrics. Using a\ndataset of 3,814 songs from 146 influential artists spanning four decades\n(1980-2020), we employ natural language processing techniques to quantify\nmultiple dimensions of lyrical complexity. Our analysis reveals a 23.7%\nincrease in vocabulary diversity over the study period, with East Coast artists\ndemonstrating 17.3% higher lexical variation than other regions. Rhyme density\nincreased by 34.2% across all regions, with Midwest artists exhibiting the\nhighest technical complexity (3.04 rhymes per line). Topic modeling identified\nsignificant shifts in thematic content, with social justice themes decreasing\nfrom 28.5% to 13.8% of content while introspective themes increased from 7.6%\nto 26.3%. Sentiment analysis demon- strated that lyrics became significantly\nmore negative during sociopolitical crises, with polarity decreasing by 0.31\nfollowing major social unrest. Multi-dimensional analysis revealed four dis-\ntinct stylistic approaches that correlate strongly with geographic origin\n(r=0.68, p!0.001) and time period (r=0.59, p<0.001). These findings establish\nquantitative evidence for the evolution of hip- hop as both an art form and a\nreflection of societal dynamics, providing insights into the interplay between\nlinguistic innovation and cultural context in popular music.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5206\u67901980-2020\u5e74\u95f43814\u9996\u563b\u54c8\u6b4c\u8bcd\uff0c\u53d1\u73b0\u8bcd\u6c47\u591a\u6837\u6027\u589e\u52a0\u4e8623.7%\uff0c\u4e1c\u6d77\u5cb8\u827a\u672f\u5bb6\u7684\u8bcd\u6c47\u53d8\u5316\u6700\u9ad8\uff0c\u4e2d\u897f\u90e8\u827a\u672f\u5bb6\u5728\u62bc\u97f5\u6280\u672f\u4e0a\u6700\u590d\u6742\u3002\u4e3b\u9898\u5206\u6790\u663e\u793a\u793e\u4f1a\u6b63\u4e49\u5185\u5bb9\u51cf\u5c11\u800c\u5185\u7701\u4e3b\u9898\u589e\u52a0\uff0c\u793e\u4f1a\u52a8\u8361\u65f6\u671f\u6b4c\u8bcd\u60c5\u611f\u66f4\u6d88\u6781\u3002\u5730\u7406\u548c\u65f6\u95f4\u56e0\u7d20\u4e0e\u98ce\u683c\u663e\u8457\u76f8\u5173\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u91cf\u5316\u563b\u54c8\u6b4c\u8bcd\u7684\u8bed\u8a00\u590d\u6742\u6027\u53ca\u5176\u4e0e\u793e\u4f1a\u6587\u5316\u8d8b\u52bf\u7684\u5173\u7cfb\uff0c\u63a2\u8ba8\u563b\u54c8\u4f5c\u4e3a\u4e00\u79cd\u827a\u672f\u5f62\u5f0f\u548c\u793e\u4f1a\u52a8\u6001\u53cd\u6620\u7684\u6f14\u53d8\u3002", "method": "\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5206\u67903814\u9996\u6765\u81ea146\u4f4d\u827a\u672f\u5bb6\u7684\u6b4c\u66f2\uff0c\u6db5\u76d6\u8bcd\u6c47\u591a\u6837\u6027\u3001\u62bc\u97f5\u5bc6\u5ea6\u3001\u4e3b\u9898\u5efa\u6a21\u548c\u60c5\u611f\u5206\u6790\u7b49\u591a\u4e2a\u7ef4\u5ea6\u3002", "result": "\u8bcd\u6c47\u591a\u6837\u6027\u589e\u52a023.7%\uff0c\u4e1c\u6d77\u5cb8\u827a\u672f\u5bb6\u8bcd\u6c47\u53d8\u5316\u6700\u9ad8\uff0817.3%\uff09\uff0c\u4e2d\u897f\u90e8\u62bc\u97f5\u6280\u672f\u6700\u590d\u6742\uff083.04/\u884c\uff09\u3002\u793e\u4f1a\u6b63\u4e49\u4e3b\u9898\u51cf\u5c11\uff0c\u5185\u7701\u4e3b\u9898\u589e\u52a0\uff0c\u793e\u4f1a\u5371\u673a\u65f6\u6b4c\u8bcd\u66f4\u6d88\u6781\u3002\u5730\u7406\u548c\u65f6\u95f4\u4e0e\u98ce\u683c\u5f3a\u76f8\u5173\uff08r=0.68/0.59\uff09\u3002", "conclusion": "\u7814\u7a76\u4e3a\u563b\u54c8\u827a\u672f\u5f62\u5f0f\u548c\u793e\u4f1a\u6587\u5316\u52a8\u6001\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u91cf\u5316\u8bc1\u636e\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u521b\u65b0\u4e0e\u6587\u5316\u80cc\u666f\u7684\u76f8\u4e92\u4f5c\u7528\u3002"}}
{"id": "2505.00347", "pdf": "https://arxiv.org/pdf/2505.00347", "abs": "https://arxiv.org/abs/2505.00347", "authors": ["Cong Xu", "Wenbin Liang", "Mo Yu", "Anan Liu", "Ke-Yue Zhang", "Lizhuang Ma", "Jianyong Wang", "Jun Wang", "Wei Zhang"], "title": "Pushing the Limits of Low-Bit Optimizers: A Focus on EMA Dynamics", "categories": ["cs.LG", "cs.AI"], "comment": "27 pages", "summary": "The explosion in model sizes leads to continued growth in prohibitive\ntraining/fine-tuning costs, particularly for stateful optimizers which maintain\nauxiliary information of even 2x the model size to achieve optimal convergence.\nWe therefore present in this work a novel type of optimizer that carries with\nextremely lightweight state overloads, achieved through ultra-low-precision\nquantization. While previous efforts have achieved certain success with 8-bit\nor 4-bit quantization, our approach enables optimizers to operate at precision\nas low as 3 bits, or even 2 bits per state element. This is accomplished by\nidentifying and addressing two critical challenges: the signal swamping problem\nin unsigned quantization that results in unchanged state dynamics, and the\nrapidly increased gradient variance in signed quantization that leads to\nincorrect descent directions. The theoretical analysis suggests a tailored\nlogarithmic quantization for the former and a precision-specific momentum value\nfor the latter. Consequently, the proposed SOLO achieves substantial memory\nsavings (approximately 45 GB when training a 7B model) with minimal accuracy\nloss. We hope that SOLO can contribute to overcoming the bottleneck in\ncomputational resources, thereby promoting greater accessibility in fundamental\nresearch.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4f18\u5316\u5668SOLO\uff0c\u901a\u8fc7\u8d85\u4f4e\u7cbe\u5ea6\u91cf\u5316\uff08\u4f4e\u81f32-3\u4f4d\uff09\u6781\u5927\u51cf\u5c11\u72b6\u6001\u4fe1\u606f\u7684\u5185\u5b58\u5360\u7528\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4f18\u5316\u5668\u5728\u8bad\u7ec3\u5927\u6a21\u578b\u65f6\u7684\u9ad8\u6210\u672c\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6536\u655b\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0c\u4f20\u7edf\u4f18\u5316\u5668\u7684\u8bad\u7ec3\u548c\u5fae\u8c03\u6210\u672c\u53d8\u5f97\u8fc7\u9ad8\uff0c\u5c24\u5176\u662f\u9700\u8981\u7ef4\u62a4\u5927\u91cf\u72b6\u6001\u4fe1\u606f\u7684\u4f18\u5316\u5668\u3002\u4e3a\u4e86\u964d\u4f4e\u8d44\u6e90\u9700\u6c42\u5e76\u63d0\u9ad8\u53ef\u8bbf\u95ee\u6027\uff0c\u8bba\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u7684\u4f18\u5316\u5668\u3002", "method": "SOLO\u4f7f\u7528\u8d85\u4f4e\u7cbe\u5ea6\u91cf\u5316\u6280\u672f\uff08\u4f4e\u81f32-3\u4f4d\uff09\uff0c\u5e76\u9488\u5bf9\u65e0\u7b26\u53f7\u91cf\u5316\u4e2d\u7684\u4fe1\u53f7\u6df9\u6ca1\u95ee\u9898\u548c\u6709\u7b26\u53f7\u91cf\u5316\u4e2d\u7684\u68af\u5ea6\u65b9\u5dee\u95ee\u9898\uff0c\u5206\u522b\u63d0\u51fa\u5bf9\u6570\u91cf\u5316\u548c\u7279\u5b9a\u7cbe\u5ea6\u7684\u52a8\u91cf\u503c\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cSOLO\u5728\u8bad\u7ec37B\u6a21\u578b\u65f6\u53ef\u8282\u7701\u7ea645GB\u5185\u5b58\uff0c\u4e14\u51c6\u786e\u7387\u635f\u5931\u6781\u5c0f\u3002", "conclusion": "SOLO\u901a\u8fc7\u8d85\u4f4e\u7cbe\u5ea6\u91cf\u5316\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5f00\u9500\uff0c\u4e3a\u89e3\u51b3\u8ba1\u7b97\u8d44\u6e90\u74f6\u9888\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u57fa\u7840\u7814\u7a76\u7684\u666e\u53ca\u3002"}}
{"id": "2505.00036", "pdf": "https://arxiv.org/pdf/2505.00036", "abs": "https://arxiv.org/abs/2505.00036", "authors": ["Zhongren Chen", "Joshua Kalla", "Quan Le", "Shinpei Nakamura-Sakai", "Jasjeet Sekhon", "Ruixiao Wang"], "title": "A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "In recent years, significant concern has emerged regarding the potential\nthreat that Large Language Models (LLMs) pose to democratic societies through\ntheir persuasive capabilities. We expand upon existing research by conducting\ntwo survey experiments and a real-world simulation exercise to determine\nwhether it is more cost effective to persuade a large number of voters using\nLLM chatbots compared to standard political campaign practice, taking into\naccount both the \"receive\" and \"accept\" steps in the persuasion process (Zaller\n1992). These experiments improve upon previous work by assessing extended\ninteractions between humans and LLMs (instead of using single-shot\ninteractions) and by assessing both short- and long-run persuasive effects\n(rather than simply asking users to rate the persuasiveness of LLM-produced\ncontent). In two survey experiments (N = 10,417) across three distinct\npolitical domains, we find that while LLMs are about as persuasive as actual\ncampaign ads once voters are exposed to them, political persuasion in the\nreal-world depends on both exposure to a persuasive message and its impact\nconditional on exposure. Through simulations based on real-world parameters, we\nestimate that LLM-based persuasion costs between \\$48-\\$74 per persuaded voter\ncompared to \\$100 for traditional campaign methods, when accounting for the\ncosts of exposure. However, it is currently much easier to scale traditional\ncampaign persuasion methods than LLM-based persuasion. While LLMs do not\ncurrently appear to have substantially greater potential for large-scale\npolitical persuasion than existing non-LLM methods, this may change as LLM\ncapabilities continue to improve and it becomes easier to scalably encourage\nexposure to persuasive LLMs.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86LLM\u4e0e\u4f20\u7edf\u653f\u6cbb\u7ade\u9009\u65b9\u6cd5\u7684\u8bf4\u670d\u6548\u679c\u4e0e\u6210\u672c\uff0c\u53d1\u73b0LLM\u76ee\u524d\u867d\u6210\u672c\u66f4\u4f4e\uff0c\u4f46\u89c4\u6a21\u5316\u5e94\u7528\u4ecd\u53d7\u9650\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30LLM\u5728\u653f\u6cbb\u8bf4\u670d\u4e2d\u7684\u6f5c\u529b\u53ca\u5176\u5bf9\u6c11\u4e3b\u793e\u4f1a\u7684\u6f5c\u5728\u5a01\u80c1\uff0c\u7279\u522b\u662f\u5728\u6210\u672c\u6548\u76ca\u548c\u89c4\u6a21\u5316\u5e94\u7528\u65b9\u9762\u3002", "method": "\u91c7\u7528\u4e24\u9879\u8c03\u67e5\u5b9e\u9a8c\uff08N=10,417\uff09\u548c\u771f\u5b9e\u4e16\u754c\u6a21\u62df\uff0c\u6bd4\u8f83LLM\u4e0e\u4f20\u7edf\u7ade\u9009\u5e7f\u544a\u7684\u8bf4\u670d\u529b\u4e0e\u6210\u672c\uff0c\u5e76\u8003\u8651\u66dd\u5149\u548c\u63a5\u53d7\u4e24\u4e2a\u7ef4\u5ea6\u3002", "result": "LLM\u7684\u8bf4\u670d\u6548\u679c\u4e0e\u4f20\u7edf\u5e7f\u544a\u76f8\u5f53\uff0c\u4f46\u6210\u672c\u66f4\u4f4e\uff0848-74\u7f8e\u5143/\u9009\u6c11 vs 100\u7f8e\u5143\uff09\uff0c\u4e0d\u8fc7\u89c4\u6a21\u5316\u96be\u5ea6\u66f4\u5927\u3002", "conclusion": "\u76ee\u524dLLM\u5728\u653f\u6cbb\u8bf4\u670d\u4e0a\u7684\u6f5c\u529b\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u968f\u7740\u6280\u672f\u53d1\u5c55\uff0c\u5176\u89c4\u6a21\u5316\u5e94\u7528\u53ef\u80fd\u6210\u4e3a\u672a\u6765\u6311\u6218\u3002"}}
{"id": "2505.00348", "pdf": "https://arxiv.org/pdf/2505.00348", "abs": "https://arxiv.org/abs/2505.00348", "authors": ["Ehtisham Asghar", "Martin Hill", "Ibrahim Sengor", "Conor Lynch", "Phan Quang An"], "title": "Validation of a 24-hour-ahead Prediction model for a Residential Electrical Load under diverse climate", "categories": ["cs.LG"], "comment": null, "summary": "Accurate household electrical energy demand prediction is essential for\neffectively managing sustainable Energy Communities. Integrated with the Energy\nManagement System, these communities aim to optimise operational costs.\nHowever, most existing forecasting models are region-specific and depend on\nlarge datasets, limiting their applicability across different climates and\ngeographical areas. These models often lack flexibility and may not perform\nwell in regions with limited historical data, leading to inaccurate\npredictions. This paper proposes a global model for 24-hour-ahead hourly\nelectrical energy demand prediction that is designed to perform effectively\nacross diverse climate conditions and datasets. The model's efficiency is\ndemonstrated using data from two distinct regions: Ireland, with a maritime\nclimate and Vietnam, with a tropical climate. Remarkably, the model achieves\nhigh accuracy even with a limited dataset spanning only nine months. Its\nrobustness is further validated across different seasons in Ireland (summer and\nwinter) and Vietnam (dry and wet). The proposed model is evaluated against\nstate-of-the-art machine learning and deep learning methods. Simulation results\nindicate that the model consistently outperforms benchmark models, showcasing\nits capability to provide reliable forecasts globally, regardless of varying\nclimatic conditions and data availability. This research underscores the\nmodel's potential to enhance the efficiency and sustainability of Energy\nCommunities worldwide. The proposed model achieves a Mean Absolute Percentage\nError of 8.0% and 4.0% on the full Irish and Vietnamese datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u4e0d\u540c\u6c14\u5019\u548c\u5730\u7406\u533a\u57df\u768424\u5c0f\u65f6\u5bb6\u5ead\u7535\u529b\u9700\u6c42\u9884\u6d4b\u5168\u5c40\u6a21\u578b\uff0c\u5e76\u5728\u7231\u5c14\u5170\u548c\u8d8a\u5357\u7684\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7535\u529b\u9700\u6c42\u9884\u6d4b\u6a21\u578b\u591a\u4f9d\u8d56\u533a\u57df\u7279\u5b9a\u7684\u5927\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u8de8\u6c14\u5019\u9002\u7528\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u6570\u636e\u6709\u9650\u533a\u57df\u7684\u51c6\u786e\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5168\u5c40\u6a21\u578b\uff0c\u5229\u7528\u7231\u5c14\u5170\uff08\u6d77\u6d0b\u6027\u6c14\u5019\uff09\u548c\u8d8a\u5357\uff08\u70ed\u5e26\u6c14\u5019\uff09\u76849\u4e2a\u6708\u6570\u636e\u9a8c\u8bc1\u5176\u6027\u80fd\uff0c\u5e76\u4e0e\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "result": "\u6a21\u578b\u5728\u7231\u5c14\u5170\u548c\u8d8a\u5357\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u52308.0%\u548c4.0%\u7684\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\uff0c\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u8de8\u5b63\u8282\u548c\u6c14\u5019\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5168\u7403\u80fd\u6e90\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7535\u529b\u9700\u6c42\u9884\u6d4b\u5de5\u5177\uff0c\u652f\u6301\u53ef\u6301\u7eed\u80fd\u6e90\u7ba1\u7406\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6570\u636e\u6709\u9650\u7684\u591a\u6837\u6c14\u5019\u533a\u57df\u3002"}}
{"id": "2505.00038", "pdf": "https://arxiv.org/pdf/2505.00038", "abs": "https://arxiv.org/abs/2505.00038", "authors": ["Cristina Garbacea", "Chenhao Tan"], "title": "HyPerAlign: Hypotheses-driven Personalized Alignment", "categories": ["cs.CL"], "comment": null, "summary": "Alignment algorithms are widely used to align large language models (LLMs) to\nhuman users based on preference annotations that reflect their intended\nreal-world use cases. Typically these (often divergent) preferences are\naggregated over a diverse set of users, resulting in fine-tuned models that are\naligned to the ``average-user'' preference. Nevertheless, current models are\nused by individual users in very specific contexts and situations, emphasizing\nthe need for user-dependent preference control. In this work we address the\nproblem of personalizing LLM outputs to their users, aiming to generate\ncustomized responses tailored to individual users, instead of generic outputs\nthat emulate the collective voices of diverse populations. We propose a novel\ninterpretable and sample-efficient hypotheses-driven personalization approach\n(HyPerAlign) where given few-shot examples written by a particular user, we\nfirst infer hypotheses about their communication strategies, personality and\nwriting style, then prompt LLM models with these hypotheses and user specific\nattributes to generate customized outputs. We conduct experiments on two\ndifferent personalization tasks, authorship attribution and deliberative\nalignment, with datasets from diverse domains (news articles, blog posts,\nemails, jailbreaking benchmarks), and demonstrate the superiority of\nhypotheses-driven personalization approach when compared to preference-based\nfine-tuning methods. For deliberative alignment, the helpfulness of LLM models\nis improved by up to $70\\%$ on average. For authorship attribution, results\nindicate consistently high win-rates (commonly $>90\\%$) against\nstate-of-the-art preference fine-tuning approaches for LLM personalization\nacross diverse user profiles and LLM models. Overall, our approach represents\nan interpretable and sample-efficient strategy for the personalization of LLM\nmodels to individual users.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5047\u8bbe\u9a71\u52a8\u7684\u4e2a\u6027\u5316\u5bf9\u9f50\u65b9\u6cd5\uff08HyPerAlign\uff09\uff0c\u901a\u8fc7\u5c11\u91cf\u7528\u6237\u793a\u4f8b\u63a8\u65ad\u5176\u6c9f\u901a\u7b56\u7565\u3001\u4e2a\u6027\u548c\u5199\u4f5c\u98ce\u683c\uff0c\u751f\u6210\u5b9a\u5236\u5316\u8f93\u51fa\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4f5c\u8005\u8bc6\u522b\u548c\u5ba1\u614e\u5bf9\u9f50\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u504f\u597d\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5bf9\u9f50\u901a\u5e38\u57fa\u4e8e\u591a\u6837\u7528\u6237\u7684\u504f\u597d\u805a\u5408\uff0c\u5bfc\u81f4\u8f93\u51fa\u504f\u5411\u201c\u5e73\u5747\u7528\u6237\u201d\u3002\u7136\u800c\uff0c\u5b9e\u9645\u4f7f\u7528\u4e2d\u7528\u6237\u9700\u8981\u9488\u5bf9\u5176\u7279\u5b9a\u4e0a\u4e0b\u6587\u548c\u9700\u6c42\u7684\u4e2a\u6027\u5316\u54cd\u5e94\u3002", "method": "\u63d0\u51fa\u4e86HyPerAlign\u65b9\u6cd5\uff1a\u901a\u8fc7\u5c11\u91cf\u7528\u6237\u793a\u4f8b\u63a8\u65ad\u5176\u4e2a\u6027\u5316\u7279\u5f81\uff08\u5982\u6c9f\u901a\u7b56\u7565\u3001\u5199\u4f5c\u98ce\u683c\uff09\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5047\u8bbe\u4e0e\u7528\u6237\u7279\u5b9a\u5c5e\u6027\u7ed3\u5408\uff0c\u6307\u5bfcLLM\u751f\u6210\u5b9a\u5236\u5316\u8f93\u51fa\u3002", "result": "\u5728\u4f5c\u8005\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0cHyPerAlign\u5bf9\u6700\u5148\u8fdb\u504f\u597d\u5fae\u8c03\u65b9\u6cd5\u7684\u80dc\u7387\u666e\u904d>90%\uff1b\u5728\u5ba1\u614e\u5bf9\u9f50\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u5e2e\u52a9\u6027\u5e73\u5747\u63d0\u534770%\u3002", "conclusion": "HyPerAlign\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u6837\u672c\u9ad8\u6548\u7684LLM\u4e2a\u6027\u5316\u7b56\u7565\uff0c\u80fd\u591f\u4e3a\u4e2a\u4f53\u7528\u6237\u751f\u6210\u66f4\u8d34\u5408\u5176\u9700\u6c42\u7684\u8f93\u51fa\uff0c\u4f18\u4e8e\u4f20\u7edf\u504f\u597d\u5fae\u8c03\u65b9\u6cd5\u3002"}}
{"id": "2505.00350", "pdf": "https://arxiv.org/pdf/2505.00350", "abs": "https://arxiv.org/abs/2505.00350", "authors": ["Mohammad Zbeeb", "Mariam Salman", "Mohammad Bazzi", "Ammar Mohanna"], "title": "Optimizing Deep Neural Networks using Safety-Guided Self Compression", "categories": ["cs.LG", "cs.AI"], "comment": "A Preprint", "summary": "The deployment of deep neural networks on resource-constrained devices\nnecessitates effective model com- pression strategies that judiciously balance\nthe reduction of model size with the preservation of performance. This study\nintroduces a novel safety-driven quantization framework that leverages\npreservation sets to systematically prune and quantize neural network weights,\nthereby optimizing model complexity without compromising accuracy. The proposed\nmethodology is rigorously evaluated on both a convolutional neural network\n(CNN) and an attention-based language model, demonstrating its applicability\nacross diverse architectural paradigms. Experimental results reveal that our\nframework achieves up to a 2.5% enhancement in test accuracy relative to the\noriginal unquantized models while maintaining 60% of the initial model size. In\ncomparison to conventional quantization techniques, our approach not only\naugments generalization by eliminating parameter noise and retaining essential\nweights but also reduces variance, thereby ensuring the retention of critical\nmodel features. These findings underscore the efficacy of safety-driven\nquantization as a robust and reliable strategy for the efficient optimization\nof deep learn- ing models. The implementation and comprehensive experimental\nevaluations of our framework are publicly accessible at GitHub.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b89\u5168\u9a71\u52a8\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4fdd\u62a4\u96c6\u7cfb\u7edf\u6027\u5730\u4fee\u526a\u548c\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\uff0c\u5728\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728CNN\u548c\u6ce8\u610f\u529b\u8bed\u8a00\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u539f\u59cb\u672a\u91cf\u5316\u6a21\u578b\u63d0\u53472.5%\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u4e14\u6a21\u578b\u5927\u5c0f\u4ec5\u4e3a60%\u3002\u76f8\u6bd4\u4f20\u7edf\u91cf\u5316\u6280\u672f\uff0c\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u6cdb\u5316\u6027\u5e76\u51cf\u5c11\u65b9\u5dee\u3002", "motivation": "\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u9700\u5e73\u8861\u6a21\u578b\u5927\u5c0f\u7f29\u51cf\u4e0e\u6027\u80fd\u4fdd\u6301\uff0c\u800c\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u53ef\u80fd\u56e0\u53c2\u6570\u566a\u58f0\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5b89\u5168\u9a71\u52a8\u91cf\u5316\u6846\u67b6\uff0c\u5229\u7528\u4fdd\u62a4\u96c6\u7cfb\u7edf\u6027\u4fee\u526a\u548c\u91cf\u5316\u6743\u91cd\uff0c\u4f18\u5316\u6a21\u578b\u590d\u6742\u5ea6\u3002", "result": "\u5728CNN\u548c\u6ce8\u610f\u529b\u6a21\u578b\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u63d0\u53472.5%\uff0c\u6a21\u578b\u5927\u5c0f\u964d\u81f360%\uff0c\u4e14\u6cdb\u5316\u6027\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u5b89\u5168\u9a71\u52a8\u91cf\u5316\u662f\u4e00\u79cd\u9ad8\u6548\u53ef\u9760\u7684\u6df1\u5ea6\u6a21\u578b\u4f18\u5316\u7b56\u7565\uff0c\u516c\u5f00\u4ee3\u7801\u5df2\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2505.00039", "pdf": "https://arxiv.org/pdf/2505.00039", "abs": "https://arxiv.org/abs/2505.00039", "authors": ["Hudson de Martim"], "title": "Graph RAG for Legal Norms: A Hierarchical and Temporal Approach", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "This article proposes an adaptation of Graph Retrieval Augmented Generation\n(Graph RAG) specifically designed for the analysis and comprehension of legal\nnorms, which are characterized by their predefined hierarchical structure,\nextensive network of internal and external references and multiple temporal\nversions. By combining structured knowledge graphs with contextually enriched\ntext segments, Graph RAG offers a promising solution to address the inherent\ncomplexity and vast volume of legal data. The integration of hierarchical\nstructure and temporal evolution into knowledge graphs - along with the concept\nof comprehensive Text Units - facilitates the construction of richer,\ninterconnected representations of legal knowledge. Through a detailed analysis\nof Graph RAG and its application to legal norm datasets, this article aims to\nsignificantly advance the field of Artificial Intelligence applied to Law,\ncreating opportunities for more effective systems in legal research,\nlegislative analysis, and decision support.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684Graph RAG\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u548c\u7406\u89e3\u6cd5\u5f8b\u89c4\u8303\u4e2d\u590d\u6742\u7684\u5c42\u6b21\u7ed3\u6784\u3001\u5f15\u7528\u7f51\u7edc\u53ca\u591a\u7248\u672c\u7279\u6027\u3002\u901a\u8fc7\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u6587\u672c\u7247\u6bb5\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u6cd5\u5f8b\u6570\u636e\u7684\u8868\u793a\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u3002", "motivation": "\u6cd5\u5f8b\u89c4\u8303\u5177\u6709\u590d\u6742\u7684\u5c42\u6b21\u7ed3\u6784\u3001\u5927\u91cf\u7684\u5185\u5916\u5f15\u7528\u548c\u591a\u65f6\u95f4\u7248\u672c\uff0c\u4f7f\u5f97\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7Graph RAG\u6280\u672f\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u5347\u6cd5\u5f8b\u6570\u636e\u7684\u5206\u6790\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u6587\u7ae0\u63d0\u51fa\u5c06\u77e5\u8bc6\u56fe\u8c31\u4e0e\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u6587\u672c\u7247\u6bb5\u7ed3\u5408\uff0c\u5229\u7528\u5c42\u6b21\u7ed3\u6784\u548c\u65f6\u95f4\u6f14\u5316\u7279\u6027\u6784\u5efa\u6cd5\u5f8b\u77e5\u8bc6\u7684\u4e92\u8054\u8868\u793a\u3002\u91cd\u70b9\u5173\u6ce8Graph RAG\u5728\u6cd5\u89c4\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u3002", "result": "Graph RAG\u65b9\u6cd5\u6210\u529f\u6784\u5efa\u4e86\u66f4\u4e30\u5bcc\u7684\u6cd5\u5f8b\u77e5\u8bc6\u8868\u793a\uff0c\u4e3a\u6cd5\u5f8b\u7814\u7a76\u3001\u7acb\u6cd5\u5206\u6790\u548c\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5de5\u5177\u3002", "conclusion": "\u8be5\u7814\u7a76\u663e\u8457\u63a8\u8fdb\u4e86AI\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\uff0c\u4e3a\u5904\u7406\u590d\u6742\u6cd5\u5f8b\u6570\u636e\u63d0\u4f9b\u4e86\u521b\u65b0\u65b9\u6cd5\uff0c\u672a\u6765\u53ef\u80fd\u5728\u76f8\u5173\u9886\u57df\u4ea7\u751f\u5e7f\u6cdb\u5f71\u54cd\u3002"}}
{"id": "2505.00358", "pdf": "https://arxiv.org/pdf/2505.00358", "abs": "https://arxiv.org/abs/2505.00358", "authors": ["Albert Ge", "Tzu-Heng Huang", "John Cooper", "Avi Trost", "Ziyi Chu", "Satya Sai Srinath Namburi GNVV", "Ziyang Cai", "Kendall Park", "Nicholas Roberts", "Frederic Sala"], "title": "R&B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Data mixing strategies have successfully reduced the costs involved in\ntraining language models. While promising, such methods suffer from two flaws.\nFirst, they rely on predetermined data domains (e.g., data sources, task\ntypes), which may fail to capture critical semantic nuances, leaving\nperformance on the table. Second, these methods scale with the number of\ndomains in a computationally prohibitive way. We address these challenges via\nR&B, a framework that re-partitions training data based on semantic similarity\n(Regroup) to create finer-grained domains, and efficiently optimizes the data\ncomposition (Balance) by leveraging a Gram matrix induced by domain gradients\nobtained throughout training. Unlike prior works, it removes the need for\nadditional compute to obtain evaluation information such as losses or\ngradients. We analyze this technique under standard regularity conditions and\nprovide theoretical insights that justify R&B's effectiveness compared to\nnon-adaptive mixing approaches. Empirically, we demonstrate the effectiveness\nof R&B on five diverse datasets ranging from natural language to reasoning and\nmultimodal tasks. With as little as 0.01% additional compute overhead, R&B\nmatches or exceeds the performance of state-of-the-art data mixing strategies.", "AI": {"tldr": "R&B\u6846\u67b6\u901a\u8fc7\u91cd\u65b0\u5212\u5206\u8bad\u7ec3\u6570\u636e\u5e76\u4f18\u5316\u6570\u636e\u7ec4\u5408\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u6df7\u5408\u7b56\u7565\u7684\u7f3a\u9677\uff0c\u4ee5\u6781\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u6df7\u5408\u7b56\u7565\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u6570\u636e\u57df\uff0c\u53ef\u80fd\u5ffd\u7565\u8bed\u4e49\u7ec6\u8282\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u968f\u6570\u636e\u57df\u6570\u91cf\u589e\u52a0\u800c\u5267\u589e\u3002R&B\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6570\u636e\u5206\u533a\u548c\u7ec4\u5408\u6765\u63d0\u5347\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "R&B\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6b65\u9aa4\uff1a1) \u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u91cd\u65b0\u5206\u7ec4\u6570\u636e\uff08Regroup\uff09\uff1b2) \u5229\u7528\u8bad\u7ec3\u4e2d\u83b7\u53d6\u7684\u57df\u68af\u5ea6Gram\u77e9\u9635\u9ad8\u6548\u4f18\u5316\u6570\u636e\u5e73\u8861\uff08Balance\uff09\u3002\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u8bc4\u4f30\u4fe1\u606f\u3002", "result": "\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u4e86R&B\u7684\u4f18\u8d8a\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u4e94\u79cd\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\uff0cR&B\u53ea\u97000.01%\u7684\u989d\u5916\u8ba1\u7b97\u5f00\u9500\uff0c\u6027\u80fd\u5373\u8fbe\u5230\u6216\u8d85\u8fc7\u73b0\u6709\u6700\u4f18\u6570\u636e\u6df7\u5408\u7b56\u7565\u3002", "conclusion": "R&B\u901a\u8fc7\u52a8\u6001\u6570\u636e\u5206\u533a\u548c\u4f18\u5316\u7ec4\u5408\uff0c\u4ee5\u6781\u4f4e\u6210\u672c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.00047", "pdf": "https://arxiv.org/pdf/2505.00047", "abs": "https://arxiv.org/abs/2505.00047", "authors": ["Peter West", "Christopher Potts"], "title": "Base Models Beat Aligned Models at Randomness and Creativity", "categories": ["cs.CL"], "comment": null, "summary": "Alignment has quickly become a default ingredient in LLM development, with\ntechniques such as reinforcement learning from human feedback making models act\nsafely, follow instructions, and perform ever-better on complex tasks. While\nthese techniques are certainly useful, we propose that they should not be\nuniversally applied and demonstrate a range of tasks on which base language\nmodels consistently outperform their popular aligned forms. Particularly, we\nstudy tasks that require unpredictable outputs, such as random number\ngeneration, mixed strategy games (rock-paper-scissors and hide-and-seek), and\ncreative writing. In each case, aligned models tend towards narrow behaviors\nthat result in distinct disadvantages, for instance, preferring to generate \"7\"\nover other uniformly random numbers, becoming almost fully predictable in some\ngame states, or prioritizing pleasant writing over creative originality. Across\nmodels tested, better performance on common benchmarks tends to correlate with\nworse performance on our tasks, suggesting an effective trade-off in the\nrequired capabilities.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\uff0c\u5c3d\u7ba1\u5bf9\u9f50\u6280\u672f\uff08\u5982RLHF\uff09\u5728\u63d0\u5347LLM\u5b89\u5168\u6027\u3001\u6307\u4ee4\u9075\u5faa\u548c\u4efb\u52a1\u6027\u80fd\u4e0a\u6709\u6548\uff0c\u4f46\u5e76\u975e\u9002\u7528\u4e8e\u6240\u6709\u4efb\u52a1\u3002\u7814\u7a76\u53d1\u73b0\u57fa\u7840\u6a21\u578b\u5728\u67d0\u4e9b\u9700\u8981\u4e0d\u53ef\u9884\u6d4b\u8f93\u51fa\u7684\u4efb\u52a1\uff08\u5982\u968f\u673a\u6570\u751f\u6210\u3001\u6df7\u5408\u7b56\u7565\u6e38\u620f\u3001\u521b\u610f\u5199\u4f5c\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u5bf9\u9f50\u6a21\u578b\u3002", "motivation": "\u63a2\u8ba8\u5bf9\u9f50\u6280\u672f\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\uff0c\u63ed\u793a\u5176\u5728\u9700\u8981\u4e0d\u53ef\u9884\u6d4b\u6027\u6216\u521b\u9020\u6027\u7684\u4efb\u52a1\u4e2d\u53ef\u80fd\u4ea7\u751f\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u57fa\u7840\u6a21\u578b\u548c\u5bf9\u9f50\u6a21\u578b\u5728\u968f\u673a\u6570\u751f\u6210\u3001\u6df7\u5408\u7b56\u7565\u6e38\u620f\uff08\u5982\u77f3\u5934\u526a\u5200\u5e03\u3001\u6349\u8ff7\u85cf\uff09\u548c\u521b\u610f\u5199\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u884c\u4e3a\u5dee\u5f02\u3002", "result": "\u5bf9\u9f50\u6a21\u578b\u5728\u9700\u8981\u4e0d\u53ef\u9884\u6d4b\u6027\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u5982\u504f\u597d\u7279\u5b9a\u968f\u673a\u6570\u3001\u6e38\u620f\u7b56\u7565\u8fc7\u4e8e\u53ef\u9884\u6d4b\u6216\u521b\u610f\u5199\u4f5c\u7f3a\u4e4f\u539f\u521b\u6027\u3002\u5e38\u89c1\u57fa\u51c6\u8868\u73b0\u8d8a\u597d\u7684\u6a21\u578b\uff0c\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u8d8a\u5dee\u3002", "conclusion": "\u5bf9\u9f50\u6280\u672f\u9700\u6839\u636e\u4efb\u52a1\u7279\u6027\u9009\u62e9\u6027\u5e94\u7528\uff0c\u907f\u514d\u56e0\u8fc7\u5ea6\u5bf9\u9f50\u727a\u7272\u6a21\u578b\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u7684\u80fd\u529b\u3002"}}
{"id": "2505.00359", "pdf": "https://arxiv.org/pdf/2505.00359", "abs": "https://arxiv.org/abs/2505.00359", "authors": ["Qifen Zeng", "Haomin Bao", "Yuanzhuo Hu", "Zirui Zhang", "Yuheng Zheng", "Luosheng Wen"], "title": "TNStream: Applying Tightest Neighbors to Micro-Clusters to Define Multi-Density Clusters in Streaming Data", "categories": ["cs.LG", "cs.AI", "cs.NE", "68T05, 68W20", "H.2.8; I.5.3"], "comment": "21 pages, 9 figures, 8 tables, under review at Expert Systems with\n  Applications (ESWA)", "summary": "In data stream clustering, systematic theory of stream clustering algorithms\nremains relatively scarce. Recently, density-based methods have gained\nattention. However, existing algorithms struggle to simultaneously handle\narbitrarily shaped, multi-density, high-dimensional data while maintaining\nstrong outlier resistance. Clustering quality significantly deteriorates when\ndata density varies complexly. This paper proposes a clustering algorithm based\non the novel concept of Tightest Neighbors and introduces a data stream\nclustering theory based on the Skeleton Set. Based on these theories, this\npaper develops a new method, TNStream, a fully online algorithm. The algorithm\nadaptively determines the clustering radius based on local similarity,\nsummarizing the evolution of multi-density data streams in micro-clusters. It\nthen applies a Tightest Neighbors-based clustering algorithm to form final\nclusters. To improve efficiency in high-dimensional cases, Locality-Sensitive\nHashing (LSH) is employed to structure micro-clusters, addressing the challenge\nof storing k-nearest neighbors. TNStream is evaluated on various synthetic and\nreal-world datasets using different clustering metrics. Experimental results\ndemonstrate its effectiveness in improving clustering quality for multi-density\ndata and validate the proposed data stream clustering theory.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7d27\u5bc6\u90bb\u5c45\u6982\u5ff5\u7684\u65b0\u578b\u6570\u636e\u6d41\u805a\u7c7b\u7b97\u6cd5TNStream\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5c40\u90e8\u76f8\u4f3c\u5ea6\u548cLSH\u6280\u672f\u5904\u7406\u591a\u5bc6\u5ea6\u3001\u9ad8\u7ef4\u6570\u636e\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u6d41\u805a\u7c7b\u7b97\u6cd5\u96be\u4ee5\u540c\u65f6\u5904\u7406\u5f62\u72b6\u4efb\u610f\u3001\u591a\u5bc6\u5ea6\u3001\u9ad8\u7ef4\u6570\u636e\u4e14\u4fdd\u6301\u5f3a\u5f02\u5e38\u503c\u62b5\u6297\u80fd\u529b\uff0c\u5bc6\u5ea6\u590d\u6742\u53d8\u5316\u65f6\u805a\u7c7b\u8d28\u91cf\u663e\u8457\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7d27\u5bc6\u90bb\u5c45\u548c\u9aa8\u67b6\u96c6\u7406\u8bba\u7684TNStream\u7b97\u6cd5\uff0c\u5229\u7528\u5c40\u90e8\u76f8\u4f3c\u5ea6\u81ea\u9002\u5e94\u786e\u5b9a\u805a\u7c7b\u534a\u5f84\uff0c\u5e76\u901a\u8fc7LSH\u6280\u672f\u4f18\u5316\u9ad8\u7ef4\u6570\u636e\u5b58\u50a8\u6548\u7387\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTNStream\u80fd\u663e\u8457\u63d0\u5347\u591a\u5bc6\u5ea6\u6570\u636e\u7684\u805a\u7c7b\u8d28\u91cf\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u6570\u636e\u6d41\u805a\u7c7b\u7406\u8bba\u3002", "conclusion": "TNStream\u901a\u8fc7\u7d27\u5bc6\u90bb\u5c45\u548c\u9aa8\u67b6\u96c6\u7406\u8bba\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5bc6\u5ea6\u3001\u9ad8\u7ef4\u6570\u636e\u6d41\u805a\u7c7b\u95ee\u9898\uff0c\u4e3a\u6570\u636e\u6d41\u805a\u7c7b\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u548c\u65b9\u6cd5\u652f\u6301\u3002"}}
{"id": "2505.00050", "pdf": "https://arxiv.org/pdf/2505.00050", "abs": "https://arxiv.org/abs/2505.00050", "authors": ["Aayam Bansal", "Agneya Tharun"], "title": "Emotional Analysis of Fashion Trends Using Social Media and AI: Sentiment Analysis on Twitter for Fashion Trend Forecasting", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages", "summary": "This study explores the intersection of fashion trends and social media\nsentiment through computational analysis of Twitter data using the T4SA\n(Twitter for Sentiment Analysis) dataset. By applying natural language\nprocessing and machine learning techniques, we examine how sentiment patterns\nin fashion-related social media conversations can serve as predictors for\nemerging fashion trends. Our analysis involves the identification and\ncategorization of fashion-related content, sentiment classification with\nimproved normalization techniques, time series decomposition, statistically\nvalidated causal relationship modeling, cross-platform sentiment comparison,\nand brand-specific sentiment analysis. Results indicate correlations between\nsentiment patterns and fashion theme popularity, with accessories and\nstreetwear themes showing statistically significant rising trends. The Granger\ncausality analysis establishes sustainability and streetwear as primary trend\ndrivers, showing bidirectional relationships with several other themes. The\nfindings demonstrate that social media sentiment analysis can serve as an\neffective early indicator of fashion trend trajectories when proper statistical\nvalidation is applied. Our improved predictive model achieved 78.35% balanced\naccuracy in sentiment classification, establishing a reliable foundation for\ntrend prediction across positive, neutral, and negative sentiment categories.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408T4SA\u6570\u636e\u96c6\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u5206\u6790\u63a8\u7279\u6570\u636e\u4e2d\u7684\u60c5\u611f\u6a21\u5f0f\uff0c\u9884\u6d4b\u65f6\u5c1a\u8d8b\u52bf\uff0c\u53d1\u73b0\u914d\u9970\u548c\u8857\u5934\u670d\u9970\u662f\u6700\u663e\u8457\u7684\u8d8b\u52bf\u9a71\u52a8\u529b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u793e\u4ea4\u5a92\u4f53\u60c5\u611f\u5206\u6790\u662f\u5426\u80fd\u6210\u4e3a\u9884\u6d4b\u65b0\u5174\u65f6\u5c1a\u8d8b\u52bf\u7684\u6709\u6548\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u65f6\u5c1a\u4ea7\u4e1a\u4e2d\u3002", "method": "\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u5206\u6790\u63a8\u7279\u6570\u636e\uff0c\u8bc6\u522b\u65f6\u5c1a\u76f8\u5173\u5185\u5bb9\uff0c\u6539\u8fdb\u60c5\u611f\u5206\u7c7b\u548c\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\uff0c\u5e76\u9a8c\u8bc1\u7edf\u8ba1\u56e0\u679c\u6a21\u578b\u3002", "result": "\u914d\u9970\u548c\u8857\u5934\u670d\u9970\u7684\u60c5\u611f\u8d8b\u52bf\u4e0e\u4e3b\u9898\u6d41\u884c\u5ea6\u663e\u8457\u76f8\u5173\uff0cGranger\u56e0\u679c\u5206\u6790\u663e\u793a\u53ef\u6301\u7eed\u6027\u548c\u8857\u5934\u670d\u9970\u662f\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\u3002\u6a21\u578b\u5728\u60c5\u611f\u5206\u7c7b\u4e2d\u8fbe\u523078.35%\u7684\u5e73\u8861\u51c6\u786e\u7387\u3002", "conclusion": "\u793e\u4ea4\u5a92\u4f53\u60c5\u611f\u5206\u6790\u80fd\u6709\u6548\u9884\u6d4b\u65f6\u5c1a\u8d8b\u52bf\uff0c\u5c24\u5176\u662f\u7ecf\u7edf\u8ba1\u9a8c\u8bc1\u540e\uff0c\u53ef\u4f5c\u4e3a\u884c\u4e1a\u65e9\u671f\u6307\u6807\u3002"}}
{"id": "2505.00364", "pdf": "https://arxiv.org/pdf/2505.00364", "abs": "https://arxiv.org/abs/2505.00364", "authors": ["Jie Yang", "Yuwen Wang", "Kaixuan Chen", "Tongya Zheng", "Yihe Zhou", "Zhenbang Xiao", "Ji Cao", "Mingli Song", "Shunyu Liu"], "title": "From GNNs to Trees: Multi-Granular Interpretability for Graph Neural Networks", "categories": ["cs.LG"], "comment": "Accepted by ICLR 2025", "summary": "Interpretable Graph Neural Networks (GNNs) aim to reveal the underlying\nreasoning behind model predictions, attributing their decisions to specific\nsubgraphs that are informative. However, existing subgraph-based interpretable\nmethods suffer from an overemphasis on local structure, potentially overlooking\nlong-range dependencies within the entire graphs. Although recent efforts that\nrely on graph coarsening have proven beneficial for global interpretability,\nthey inevitably reduce the graphs to a fixed granularity. Such an inflexible\nway can only capture graph connectivity at a specific level, whereas real-world\ngraph tasks often exhibit relationships at varying granularities (e.g.,\nrelevant interactions in proteins span from functional groups, to amino acids,\nand up to protein domains). In this paper, we introduce a novel Tree-like\nInterpretable Framework (TIF) for graph classification, where plain GNNs are\ntransformed into hierarchical trees, with each level featuring coarsened graphs\nof different granularity as tree nodes. Specifically, TIF iteratively adopts a\ngraph coarsening module to compress original graphs (i.e., root nodes of trees)\ninto increasingly coarser ones (i.e., child nodes of trees), while preserving\ndiversity among tree nodes within different branches through a dedicated graph\nperturbation module. Finally, we propose an adaptive routing module to identify\nthe most informative root-to-leaf paths, providing not only the final\nprediction but also the multi-granular interpretability for the decision-making\nprocess. Extensive experiments on the graph classification benchmarks with both\nsynthetic and real-world datasets demonstrate the superiority of TIF in\ninterpretability, while also delivering a competitive prediction performance\nakin to the state-of-the-art counterparts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTIF\u7684\u65b0\u578b\u6811\u72b6\u89e3\u91ca\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u666e\u901aGNN\u8f6c\u5316\u4e3a\u591a\u7c92\u5ea6\u5c42\u6b21\u6811\u7ed3\u6784\uff0c\u6539\u5584\u4e86\u73b0\u6709\u5b50\u56fe\u89e3\u91ca\u65b9\u6cd5\u5bf9\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u7684\u5ffd\u7565\u95ee\u9898\uff0c\u5e76\u5728\u56fe\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u89e3\u91ca\u6027\u548c\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b50\u56fe\u7684\u89e3\u91ca\u65b9\u6cd5\u8fc7\u4e8e\u5173\u6ce8\u5c40\u90e8\u7ed3\u6784\uff0c\u53ef\u80fd\u5ffd\u7565\u56fe\u4e2d\u7684\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\uff1b\u800c\u4f9d\u8d56\u56fe\u7c97\u5316\u7684\u65b9\u6cd5\u867d\u6709\u52a9\u4e8e\u5168\u5c40\u89e3\u91ca\u6027\uff0c\u4f46\u56fa\u5b9a\u7c92\u5ea6\u7684\u5904\u7406\u65e0\u6cd5\u6355\u6349\u73b0\u5b9e\u56fe\u4e2d\u591a\u7c92\u5ea6\u7684\u5173\u7cfb\u3002", "method": "TIF\u901a\u8fc7\u56fe\u7c97\u5316\u6a21\u5757\u8fed\u4ee3\u751f\u6210\u591a\u7c92\u5ea6\u6811\u7ed3\u6784\uff0c\u8f85\u4ee5\u56fe\u6270\u52a8\u6a21\u5757\u4fdd\u6301\u5206\u652f\u591a\u6837\u6027\uff0c\u5e76\u5229\u7528\u81ea\u9002\u5e94\u8def\u7531\u6a21\u5757\u9009\u62e9\u6700\u4fe1\u606f\u5316\u7684\u8def\u5f84\uff0c\u5b9e\u73b0\u591a\u7c92\u5ea6\u89e3\u91ca\u3002", "result": "\u5728\u5408\u6210\u7684\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTIF\u5728\u89e3\u91ca\u6027\u548c\u9884\u6d4b\u6027\u80fd\u4e0a\u5747\u6709\u4f18\u5f02\u8868\u73b0\u3002", "conclusion": "TIF\u4e0d\u4ec5\u89e3\u51b3\u4e86\u73b0\u6709\u591a\u7c92\u5ea6\u5173\u7cfb\u6355\u6349\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u8fd8\u5728\u89e3\u91ca\u6027\u548c\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u6c34\u5e73\u3002"}}
{"id": "2505.00056", "pdf": "https://arxiv.org/pdf/2505.00056", "abs": "https://arxiv.org/abs/2505.00056", "authors": ["Tygo Bloem", "Filip Ilievski"], "title": "Clustering Internet Memes Through Template Matching and Multi-Dimensional Similarity", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.MM"], "comment": null, "summary": "Meme clustering is critical for toxicity detection, virality modeling, and\ntyping, but it has received little attention in previous research. Clustering\nsimilar Internet memes is challenging due to their multimodality, cultural\ncontext, and adaptability. Existing approaches rely on databases, overlook\nsemantics, and struggle to handle diverse dimensions of similarity. This paper\nintroduces a novel method that uses template-based matching with\nmulti-dimensional similarity features, thus eliminating the need for predefined\ndatabases and supporting adaptive matching. Memes are clustered using local and\nglobal features across similarity categories such as form, visual content,\ntext, and identity. Our combined approach outperforms existing clustering\nmethods, producing more consistent and coherent clusters, while\nsimilarity-based feature sets enable adaptability and align with human\nintuition. We make all supporting code publicly available to support subsequent\nresearch. Code: https://github.com/tygobl/meme-clustering", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u7ef4\u76f8\u4f3c\u6027\u7279\u5f81\u7684\u6a21\u677f\u5339\u914d\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e92\u8054\u7f51\u6a21\u56e0\u7684\u805a\u7c7b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u5e93\u3001\u5ffd\u89c6\u8bed\u4e49\u53ca\u96be\u4ee5\u5904\u7406\u591a\u6837\u6027\u76f8\u4f3c\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u4e92\u8054\u7f51\u6a21\u56e0\u7684\u805a\u7c7b\u5bf9\u6bd2\u6027\u68c0\u6d4b\u3001\u4f20\u64ad\u5efa\u6a21\u548c\u5206\u7c7b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5176\u591a\u6a21\u6001\u3001\u6587\u5316\u80cc\u666f\u548c\u9002\u5e94\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6a21\u677f\u7684\u5339\u914d\u65b9\u6cd5\uff0c\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u901a\u8fc7\u5f62\u5f0f\u3001\u89c6\u89c9\u5185\u5bb9\u3001\u6587\u672c\u548c\u8eab\u4efd\u7b49\u591a\u7ef4\u76f8\u4f3c\u6027\u8fdb\u884c\u805a\u7c7b\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u805a\u7c7b\u4e00\u81f4\u6027\u548c\u8fde\u8d2f\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u76f8\u4f3c\u6027\u7279\u5f81\u96c6\u652f\u6301\u9002\u5e94\u6027\u5e76\u4e0e\u4eba\u7c7b\u76f4\u89c9\u4e00\u81f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u9884\u5b9a\u4e49\u6570\u636e\u5e93\uff0c\u652f\u6301\u81ea\u9002\u5e94\u5339\u914d\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u516c\u5f00\u4ee3\u7801\u652f\u6301\u3002"}}
{"id": "2505.00365", "pdf": "https://arxiv.org/pdf/2505.00365", "abs": "https://arxiv.org/abs/2505.00365", "authors": ["Zhengyi Zhong", "Weidong Bao", "Ji Wang", "Jianguo Chen", "Lingjuan Lyu", "Wei Yang Bryan Lim"], "title": "SacFL: Self-Adaptive Federated Continual Learning for Resource-Constrained End Devices", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by TNNLS 2025", "summary": "The proliferation of end devices has led to a distributed computing paradigm,\nwherein on-device machine learning models continuously process diverse data\ngenerated by these devices. The dynamic nature of this data, characterized by\ncontinuous changes or data drift, poses significant challenges for on-device\nmodels. To address this issue, continual learning (CL) is proposed, enabling\nmachine learning models to incrementally update their knowledge and mitigate\ncatastrophic forgetting. However, the traditional centralized approach to CL is\nunsuitable for end devices due to privacy and data volume concerns. In this\ncontext, federated continual learning (FCL) emerges as a promising solution,\npreserving user data locally while enhancing models through collaborative\nupdates. Aiming at the challenges of limited storage resources for CL, poor\nautonomy in task shift detection, and difficulty in coping with new adversarial\ntasks in FCL scenario, we propose a novel FCL framework named SacFL. SacFL\nemploys an Encoder-Decoder architecture to separate task-robust and\ntask-sensitive components, significantly reducing storage demands by retaining\nlightweight task-sensitive components for resource-constrained end devices.\nMoreover, $\\rm{SacFL}$ leverages contrastive learning to introduce an\nautonomous data shift detection mechanism, enabling it to discern whether a new\ntask has emerged and whether it is a benign task. This capability ultimately\nallows the device to autonomously trigger CL or attack defense strategy without\nadditional information, which is more practical for end devices. Comprehensive\nexperiments conducted on multiple text and image datasets, such as Cifar100 and\nTHUCNews, have validated the effectiveness of $\\rm{SacFL}$ in both\nclass-incremental and domain-incremental scenarios. Furthermore, a demo system\nhas been developed to verify its practicality.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u5206\u5e03\u5f0f\u8ba1\u7b97\u73af\u5883\u4e2d\u8bbe\u5907\u7aef\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9762\u4e34\u7684\u6570\u636e\u52a8\u6001\u53d8\u5316\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u8054\u90a6\u6301\u7eed\u5b66\u4e60\uff08FCL\uff09\u6846\u67b6SacFL\uff0c\u901a\u8fc7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u548c\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u4f4e\u5b58\u50a8\u9700\u6c42\u4e0e\u4efb\u52a1\u81ea\u4e3b\u68c0\u6d4b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5206\u5e03\u5f0f\u8bbe\u5907\u7aef\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9700\u5e94\u5bf9\u6570\u636e\u52a8\u6001\u53d8\u5316\uff08\u5982\u6570\u636e\u6f02\u79fb\uff09\uff0c\u4f20\u7edf\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u65b9\u6848\u56e0\u9690\u79c1\u548c\u8ba1\u7b97\u8d44\u6e90\u95ee\u9898\u4e0d\u9002\u7528\uff0c\u56e0\u6b64\u63d0\u51fa\u8054\u90a6\u6301\u7eed\u5b66\u4e60\uff08FCL\uff09\u6846\u67b6SacFL\u3002", "method": "SacFL\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u5206\u79bb\u4efb\u52a1\u9c81\u68d2\u4e0e\u654f\u611f\u7ec4\u4ef6\u4ee5\u51cf\u5c11\u5b58\u50a8\u9700\u6c42\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u4efb\u52a1\u81ea\u4e3b\u68c0\u6d4b\uff0c\u652f\u6301\u8bbe\u5907\u7aef\u89e6\u53d1CL\u6216\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u5728Cifar100\u3001THUCNews\u7b49\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86SacFL\u5728\u7c7b\u589e\u91cf\u4e0e\u57df\u589e\u91cf\u573a\u666f\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u6f14\u793a\u7cfb\u7edf\u9a8c\u8bc1\u5b9e\u7528\u6027\u3002", "conclusion": "SacFL\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u4e3b\u7684\u8054\u90a6\u6301\u7eed\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u9002\u5e94\u52a8\u6001\u6570\u636e\u7684\u80fd\u529b\u3002"}}
{"id": "2505.00057", "pdf": "https://arxiv.org/pdf/2505.00057", "abs": "https://arxiv.org/abs/2505.00057", "authors": ["Zhu Jiawei", "Chen Wei"], "title": "A Report on the llms evaluating the high school questions", "categories": ["cs.CL"], "comment": null, "summary": "This report aims to evaluate the performance of large language models (LLMs)\nin solving high school science questions and to explore their potential\napplications in the educational field. With the rapid development of LLMs in\nthe field of natural language processing, their application in education has\nattracted widespread attention. This study selected mathematics exam questions\nfrom the college entrance examinations (2019-2023) as evaluation data and\nutilized at least eight LLM APIs to provide answers. A comprehensive assessment\nwas conducted based on metrics such as accuracy, response time, logical\nreasoning, and creativity. Through an in-depth analysis of the evaluation\nresults, this report reveals the strengths and weaknesses of LLMs in handling\nhigh school science questions and discusses their implications for educational\npractice. The findings indicate that although LLMs perform excellently in\ncertain aspects, there is still room for improvement in logical reasoning and\ncreative problem-solving. This report provides an empirical foundation for\nfurther research and application of LLMs in the educational field and offers\nsuggestions for improvement.", "AI": {"tldr": "\u62a5\u544a\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u7b54\u9ad8\u4e2d\u79d1\u5b66\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u6559\u80b2\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002\u901a\u8fc7\u5206\u6790\u9ad8\u8003\u6570\u5b66\u9898\uff082019-2023\u5e74\uff09\u7684\u7b54\u9898\u6570\u636e\uff0c\u53d1\u73b0LLMs\u5728\u51c6\u786e\u6027\u548c\u54cd\u5e94\u65f6\u95f4\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u903b\u8f91\u63a8\u7406\u548c\u521b\u9020\u6027\u89e3\u9898\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u89e3\u51b3\u9ad8\u4e2d\u79d1\u5b66\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u6559\u80b2\u5b9e\u8df5\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u7814\u7a76\u9009\u7528\u4e862019-2023\u5e74\u9ad8\u8003\u6570\u5b66\u9898\u4f5c\u4e3a\u8bc4\u4f30\u6570\u636e\uff0c\u5229\u7528\u81f3\u5c11\u516b\u4e2aLLM API\u751f\u6210\u7b54\u6848\uff0c\u5e76\u4ece\u51c6\u786e\u6027\u3001\u54cd\u5e94\u65f6\u95f4\u3001\u903b\u8f91\u63a8\u7406\u548c\u521b\u9020\u6027\u7b49\u7ef4\u5ea6\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cLLMs\u5728\u51c6\u786e\u6027\u548c\u54cd\u5e94\u65f6\u95f4\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u903b\u8f91\u63a8\u7406\u548c\u521b\u9020\u6027\u89e3\u9898\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u4e3aLLMs\u5728\u6559\u80b2\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\u3002"}}
{"id": "2505.00375", "pdf": "https://arxiv.org/pdf/2505.00375", "abs": "https://arxiv.org/abs/2505.00375", "authors": ["Jinhui Yi", "Huan Yan", "Haotian Wang", "Jian Yuan", "Yong Li"], "title": "Learning to Estimate Package Delivery Time in Mixed Imbalanced Delivery and Pickup Logistics Services", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ACM SIGSPATIAL 2024", "summary": "Accurately estimating package delivery time is essential to the logistics\nindustry, which enables reasonable work allocation and on-time service\nguarantee. This becomes even more necessary in mixed logistics scenarios where\ncouriers handle a high volume of delivery and a smaller number of pickup\nsimultaneously. However, most of the related works treat the pickup and\ndelivery patterns on couriers' decision behavior equally, neglecting that the\npickup has a greater impact on couriers' decision-making compared to the\ndelivery due to its tighter time constraints. In such context, we have three\nmain challenges: 1) multiple spatiotemporal factors are intricately\ninterconnected, significantly affecting couriers' delivery behavior; 2) pickups\nhave stricter time requirements but are limited in number, making it\nchallenging to model their effects on couriers' delivery process; 3) couriers'\nspatial mobility patterns are critical determinants of their delivery behavior,\nbut have been insufficiently explored. To deal with these, we propose TransPDT,\na Transformer-based multi-task package delivery time prediction model. We first\nemploy the Transformer encoder architecture to capture the spatio-temporal\ndependencies of couriers' historical travel routes and pending package sets.\nThen we design the pattern memory to learn the patterns of pickup in the\nimbalanced dataset via attention mechanism. We also set the route prediction as\nan auxiliary task of delivery time prediction, and incorporate the prior\ncourier spatial movement regularities in prediction. Extensive experiments on\nreal industry-scale datasets demonstrate the superiority of our method. A\nsystem based on TransPDT is deployed internally in JD Logistics to track more\nthan 2000 couriers handling hundreds of thousands of packages per day in\nBeijing.", "AI": {"tldr": "TransPDT\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u591a\u4efb\u52a1\u5305\u88f9\u9001\u8fbe\u65f6\u95f4\u9884\u6d4b\u6a21\u578b\uff0c\u9488\u5bf9\u6df7\u5408\u7269\u6d41\u573a\u666f\u4e2d\u63d0\u8d27\u5bf9\u9001\u8d27\u65f6\u95f4\u9884\u6d4b\u7684\u66f4\u5927\u5f71\u54cd\uff0c\u901a\u8fc7\u6355\u6349\u65f6\u7a7a\u4f9d\u8d56\u6027\u548c\u63d0\u8d27\u6a21\u5f0f\uff0c\u7ed3\u5408\u8def\u7ebf\u9884\u6d4b\u548c\u5feb\u9012\u5458\u7a7a\u95f4\u79fb\u52a8\u89c4\u5f8b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u6df7\u5408\u7269\u6d41\u573a\u666f\u4e2d\uff0c\u5feb\u9012\u5458\u540c\u65f6\u5904\u7406\u5927\u91cf\u9001\u8d27\u548c\u5c11\u91cf\u63d0\u8d27\u65f6\uff0c\u63d0\u8d27\u5bf9\u5feb\u9012\u5458\u51b3\u7b56\u884c\u4e3a\u7684\u5f71\u54cd\u66f4\u5927\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u672a\u80fd\u5145\u5206\u8003\u8651\u8fd9\u4e00\u70b9\u3002\u51c6\u786e\u9884\u6d4b\u9001\u8fbe\u65f6\u95f4\u5bf9\u7269\u6d41\u884c\u4e1a\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u591a\u79cd\u65f6\u7a7a\u56e0\u7d20\u4ea4\u7ec7\u3001\u63d0\u8d27\u65f6\u95f4\u8981\u6c42\u4e25\u683c\u4f46\u6570\u91cf\u5c11\u3001\u5feb\u9012\u5458\u7a7a\u95f4\u79fb\u52a8\u6a21\u5f0f\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u4e09\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86TransPDT\u6a21\u578b\uff0c\u91c7\u7528Transformer\u7f16\u7801\u5668\u67b6\u6784\u6355\u6349\u5feb\u9012\u5458\u5386\u53f2\u8def\u5f84\u548c\u5f85\u5904\u7406\u5305\u88f9\u7684\u65f6\u7a7a\u4f9d\u8d56\uff1b\u8bbe\u8ba1\u6a21\u5f0f\u8bb0\u5fc6\u6a21\u5757\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u4e0d\u5e73\u8861\u6570\u636e\u4e2d\u7684\u63d0\u8d27\u6a21\u5f0f\uff1b\u5c06\u8def\u7ebf\u9884\u6d4b\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\uff0c\u5e76\u7ed3\u5408\u5feb\u9012\u5458\u7684\u7a7a\u95f4\u79fb\u52a8\u89c4\u5f8b\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u771f\u5b9e\u5de5\u4e1a\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002TransPDT\u5df2\u5728\u4eac\u4e1c\u7269\u6d41\u5185\u90e8\u90e8\u7f72\uff0c\u7528\u4e8e\u8ddf\u8e2a\u5317\u4eac2000\u591a\u540d\u5feb\u9012\u5458\u6bcf\u65e5\u5904\u7406\u6570\u5341\u4e07\u5305\u88f9\u7684\u60c5\u51b5\u3002", "conclusion": "TransPDT\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u7ed3\u5408\u5feb\u9012\u5458\u884c\u4e3a\u6a21\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6df7\u5408\u7269\u6d41\u573a\u666f\u4e2d\u7684\u9001\u8fbe\u65f6\u95f4\u9884\u6d4b\u95ee\u9898\uff0c\u4e3a\u7269\u6d41\u884c\u4e1a\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2505.00059", "pdf": "https://arxiv.org/pdf/2505.00059", "abs": "https://arxiv.org/abs/2505.00059", "authors": ["Paige Tutt\u00f6s\u00ed", "Mantaj Dhillon", "Luna Sang", "Shane Eastwood", "Poorvi Bhatia", "Quang Minh Dinh", "Avni Kapoor", "Yewon Jin", "Angelica Lim"], "title": "BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Computer Speech and Language, Special issue:\n  Multi-Speaker, Multi-Microphone, and Multi-Modal Distant Speech Recognition\n  (September 2025)", "summary": "Some speech recognition tasks, such as automatic speech recognition (ASR),\nare approaching or have reached human performance in many reported metrics.\nYet, they continue to struggle in complex, real-world, situations, such as with\ndistanced speech. Previous challenges have released datasets to address the\nissue of distanced ASR, however, the focus remains primarily on distance,\nspecifically relying on multi-microphone array systems. Here we present the\nB(asic) E(motion) R(andom phrase) S(hou)t(s) (BERSt) dataset. The dataset\ncontains almost 4 hours of English speech from 98 actors with varying regional\nand non-native accents. The data was collected on smartphones in the actors\nhomes and therefore includes at least 98 different acoustic environments. The\ndata also includes 7 different emotion prompts and both shouted and spoken\nutterances. The smartphones were places in 19 different positions, including\nobstructions and being in a different room than the actor. This data is\npublicly available for use and can be used to evaluate a variety of speech\nrecognition tasks, including: ASR, shout detection, and speech emotion\nrecognition (SER). We provide initial benchmarks for ASR and SER tasks, and\nfind that ASR degrades both with an increase in distance and shout level and\nshows varied performance depending on the intended emotion. Our results show\nthat the BERSt dataset is challenging for both ASR and SER tasks and continued\nwork is needed to improve the robustness of such systems for more accurate\nreal-world use.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86BERSt\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u590d\u6742\u573a\u666f\u4e0b\u7684\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\uff0c\u5982\u8fdc\u8ddd\u79bb\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u548c\u60c5\u611f\u8bc6\u522b\uff08SER\uff09\u3002\u6570\u636e\u96c6\u5305\u542b\u591a\u6837\u5316\u7684\u8bed\u97f3\u6837\u672c\uff0c\u521d\u6b65\u5b9e\u9a8c\u663e\u793aASR\u6027\u80fd\u53d7\u8ddd\u79bb\u3001\u558a\u53eb\u548c\u60c5\u611f\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edfASR\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u5df2\u6709\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u591a\u9ea6\u514b\u98ce\u9635\u5217\u7684\u8ddd\u79bb\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u591a\u6837\u5316\u7684\u771f\u5b9e\u73af\u5883\u6570\u636e\u3002BERSt\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u624b\u673a\u572898\u4f4d\u6f14\u5458\u5bb6\u4e2d\u91c7\u96c6\u8bed\u97f3\u6837\u672c\uff0c\u6db5\u76d6\u4e0d\u540c\u53e3\u97f3\u3001\u60c5\u611f\u63d0\u793a\uff087\u79cd\uff09\u3001\u558a\u53eb\u548c\u8bf4\u8bdd\uff0c\u8bbe\u5907\u653e\u7f6e\u4f4d\u7f6e\u591a\u6837\uff0819\u79cd\uff09\u3002", "result": "ASR\u6027\u80fd\u968f\u8ddd\u79bb\u548c\u558a\u53eb\u6c34\u5e73\u4e0b\u964d\uff0c\u4e14\u53d7\u60c5\u611f\u5f71\u54cd\u3002SER\u4efb\u52a1\u540c\u6837\u5177\u6311\u6218\u6027\u3002", "conclusion": "BERSt\u6570\u636e\u96c6\u4e3aASR\u548cSER\u63d0\u4f9b\u4e86\u771f\u5b9e\u6311\u6218\uff0c\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u4ee5\u9002\u5e94\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2505.00382", "pdf": "https://arxiv.org/pdf/2505.00382", "abs": "https://arxiv.org/abs/2505.00382", "authors": ["Jianya Lu", "Yingjun Mo"], "title": "Approximation to Deep Q-Network by Stochastic Delay Differential Equations", "categories": ["cs.LG", "math.PR"], "comment": null, "summary": "Despite the significant breakthroughs that the Deep Q-Network (DQN) has\nbrought to reinforcement learning, its theoretical analysis remains limited. In\nthis paper, we construct a stochastic differential delay equation (SDDE) based\non the DQN algorithm and estimate the Wasserstein-1 distance between them. We\nprovide an upper bound for the distance and prove that the distance between the\ntwo converges to zero as the step size approaches zero. This result allows us\nto understand DQN's two key techniques, the experience replay and the target\nnetwork, from the perspective of continuous systems. Specifically, the delay\nterm in the equation, corresponding to the target network, contributes to the\nstability of the system. Our approach leverages a refined Lindeberg principle\nand an operator comparison to establish these results.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6784\u5efa\u57fa\u4e8eDQN\u7b97\u6cd5\u7684\u968f\u673a\u5fae\u5206\u5ef6\u8fdf\u65b9\u7a0b\uff08SDDE\uff09\uff0c\u5206\u6790\u4e86DQN\u7684\u7406\u8bba\u7279\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u4e0e\u8fde\u7eed\u7cfb\u7edf\u7684\u6536\u655b\u6027\uff0c\u5e76\u89e3\u91ca\u4e86\u7ecf\u9a8c\u56de\u653e\u548c\u76ee\u6807\u7f51\u7edc\u5728\u7a33\u5b9a\u6027\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u5c3d\u7ba1DQN\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u91cd\u5927\u7a81\u7834\uff0c\u4f46\u5176\u7406\u8bba\u5206\u6790\u4ecd\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u4ece\u8fde\u7eed\u7cfb\u7edf\u7684\u89d2\u5ea6\u7406\u89e3DQN\u7684\u6838\u5fc3\u6280\u672f\uff08\u7ecf\u9a8c\u56de\u653e\u548c\u76ee\u6807\u7f51\u7edc\uff09\uff0c\u5e76\u4e3a\u7b97\u6cd5\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u968f\u673a\u5fae\u5206\u5ef6\u8fdf\u65b9\u7a0b\uff08SDDE\uff09\u5e76\u4f30\u8ba1\u5176\u4e0eDQN\u4e4b\u95f4\u7684Wasserstein-1\u8ddd\u79bb\uff0c\u5229\u7528\u6539\u8fdb\u7684Lindeberg\u539f\u7406\u548c\u7b97\u5b50\u6bd4\u8f83\u65b9\u6cd5\u5206\u6790\u6536\u655b\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u8ddd\u79bb\u7684\u4e0a\u754c\uff0c\u5e76\u8868\u660e\u5f53\u6b65\u957f\u8d8b\u4e8e\u96f6\u65f6\uff0c\u4e24\u8005\u8ddd\u79bb\u6536\u655b\u4e8e\u96f6\u3002\u5ef6\u8fdf\u9879\uff08\u5bf9\u5e94\u76ee\u6807\u7f51\u7edc\uff09\u5bf9\u7cfb\u7edf\u7a33\u5b9a\u6027\u8d77\u5230\u4e86\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u4ece\u8fde\u7eed\u7cfb\u7edf\u7684\u89c6\u89d2\u4e3aDQN\u7684\u7ecf\u9a8c\u56de\u653e\u548c\u76ee\u6807\u7f51\u7edc\u6280\u672f\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\uff0c\u589e\u5f3a\u4e86\u7b97\u6cd5\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2505.00060", "pdf": "https://arxiv.org/pdf/2505.00060", "abs": "https://arxiv.org/abs/2505.00060", "authors": ["Jeho Choi"], "title": "Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence Using Exaone 3.5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, 1 table", "summary": "Large Language Models (LLMs) have shown promise in enabling natural language\ninterfaces for structured data querying through text-to-SQL generation.\nHowever, their application in real-world Business Intelligence (BI) contexts\nremains limited due to semantic hallucinations, structural errors, and a lack\nof domain-specific evaluation frameworks. In this study, we propose a\nFact-Consistency Evaluation Framework for assessing the semantic accuracy of\nLLM-generated SQL outputs using Exaone 3.5--an instruction-tuned, bilingual LLM\noptimized for enterprise tasks. We construct a domain-specific benchmark\ncomprising 219 natural language business questions across five SQL complexity\nlevels, derived from actual sales data in LG Electronics' internal BigQuery\nenvironment. Each question is paired with a gold-standard SQL query and a\nvalidated ground-truth answer. We evaluate model performance using answer\naccuracy, execution success rate, semantic error rate, and non-response rate.\nExperimental results show that while Exaone 3.5 performs well on simple\naggregation tasks (93% accuracy in L1), it exhibits substantial degradation in\narithmetic reasoning (4% accuracy in H1) and grouped ranking tasks (31% in H4),\nwith semantic errors and non-responses concentrated in complex cases.\nQualitative error analysis further identifies common failure types such as\nmisapplied arithmetic logic, incomplete filtering, and incorrect grouping\noperations. Our findings highlight the current limitations of LLMs in\nbusiness-critical environments and underscore the need for fact-consistency\nvalidation layers and hybrid reasoning approaches. This work contributes a\nreproducible benchmark and evaluation methodology for advancing reliable\nnatural language interfaces to structured enterprise data systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e8b\u5b9e\u4e00\u81f4\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u751f\u6210\u7684SQL\u8f93\u51fa\u7684\u8bed\u4e49\u51c6\u786e\u6027\uff0c\u53d1\u73b0\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5f3a\u8c03\u4e86\u4e8b\u5b9e\u4e00\u81f4\u6027\u9a8c\u8bc1\u548c\u6df7\u5408\u63a8\u7406\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3LLMs\u5728\u771f\u5b9e\u5546\u4e1a\u667a\u80fd\u73af\u5883\u4e2d\u7684\u5e94\u7528\u9650\u5236\uff0c\u5982\u8bed\u4e49\u5e7b\u89c9\u548c\u7ed3\u6784\u9519\u8bef\uff0c\u5e76\u7f3a\u4e4f\u7279\u5b9a\u9886\u57df\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6784\u5efa\u4e00\u4e2a\u5305\u542b219\u4e2a\u81ea\u7136\u8bed\u8a00\u5546\u4e1a\u95ee\u9898\u7684\u7279\u5b9a\u9886\u57df\u57fa\u51c6\uff0c\u4f7f\u7528Exaone 3.5 LLM\u751f\u6210SQL\u67e5\u8be2\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u6307\u6807\uff08\u5982\u7b54\u6848\u51c6\u786e\u6027\u3001\u6267\u884c\u6210\u529f\u7387\u7b49\uff09\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cExaone 3.5\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff08L1\u51c6\u786e\u7387\u4e3a93%\uff09\uff0c\u4f46\u5728\u7b97\u672f\u63a8\u7406\uff08H1\u51c6\u786e\u7387\u4ec54%\uff09\u548c\u5206\u7ec4\u6392\u540d\u4efb\u52a1\uff08H4\u51c6\u786e\u7387\u4e3a31%\uff09\u4e2d\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u4e86LLMs\u5728\u5546\u4e1a\u5173\u952e\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u9700\u8981\u4e8b\u5b9e\u4e00\u81f4\u6027\u9a8c\u8bc1\u5c42\u548c\u6df7\u5408\u63a8\u7406\u65b9\u6cd5\uff0c\u540c\u65f6\u8d21\u732e\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2505.00398", "pdf": "https://arxiv.org/pdf/2505.00398", "abs": "https://arxiv.org/abs/2505.00398", "authors": ["Bassel Hamoud", "Ilnura Usmanova", "Kfir Y. Levy"], "title": "Safety in the Face of Adversity: Achieving Zero Constraint Violation in Online Learning with Slowly Changing Constraints", "categories": ["cs.LG"], "comment": "Accepted to AISTATS, 2025", "summary": "We present the first theoretical guarantees for zero constraint violation in\nOnline Convex Optimization (OCO) across all rounds, addressing dynamic\nconstraint changes. Unlike existing approaches in constrained OCO, which allow\nfor occasional safety breaches, we provide the first approach for maintaining\nstrict safety under the assumption of gradually evolving constraints, namely\nthe constraints change at most by a small amount between consecutive rounds.\nThis is achieved through a primal-dual approach and Online Gradient Ascent in\nthe dual space. We show that employing a dichotomous learning rate enables\nensuring both safety, via zero constraint violation, and sublinear regret. Our\nframework marks a departure from previous work by providing the first provable\nguarantees for maintaining absolute safety in the face of changing constraints\nin OCO.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u4e3a\u96f6\u7ea6\u675f\u8fdd\u53cd\u7684\u5728\u7ebf\u51f8\u4f18\u5316\uff08OCO\uff09\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5373\u4f7f\u5728\u52a8\u6001\u7ea6\u675f\u6761\u4ef6\u4e0b\u4e5f\u80fd\u786e\u4fdd\u6240\u6709\u8f6e\u6b21\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u7ea6\u675fOCO\u65b9\u6cd5\u5141\u8bb8\u5076\u5c14\u7684\u5b89\u5168\u8fdd\u89c4\uff0c\u800c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u52a8\u6001\u7ea6\u675f\u53d8\u5316\u4e0b\u7684\u4e25\u683c\u5b89\u5168\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u539f\u59cb-\u5bf9\u5076\u65b9\u6cd5\u548c\u5728\u7ebf\u68af\u5ea6\u4e0a\u5347\uff08\u5728\u53cc\u7a7a\u95f4\u4e2d\uff09\uff0c\u7ed3\u5408\u4e8c\u5206\u5b66\u4e60\u7387\u4ee5\u786e\u4fdd\u5b89\u5168\u6027\u548c\u6b21\u7ebf\u6027\u9057\u61be\u3002", "result": "\u5b9e\u73b0\u4e86\u96f6\u7ea6\u675f\u8fdd\u53cd\u7684\u7edd\u5bf9\u5b89\u5168\u6027\u548c\u6b21\u7ebf\u6027\u9057\u61be\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5728\u52a8\u6001\u7ea6\u675fOCO\u4e2d\u5b9e\u73b0\u7edd\u5bf9\u5b89\u5168\u6027\u7684\u53ef\u8bc1\u660e\u6846\u67b6\uff0c\u6807\u5fd7\u7740\u8be5\u9886\u57df\u7684\u91cd\u5927\u8fdb\u5c55\u3002"}}
{"id": "2505.00061", "pdf": "https://arxiv.org/pdf/2505.00061", "abs": "https://arxiv.org/abs/2505.00061", "authors": ["Sahar Yarmohammadtoosky", "Yiyun Zhou", "Victoria Yaneva", "Peter Baldwin", "Saed Rezayi", "Brian Clauser", "Polina Harikeo"], "title": "Enhancing Security and Strengthening Defenses in Automated Short-Answer Grading Systems", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "This study examines vulnerabilities in transformer-based automated\nshort-answer grading systems used in medical education, with a focus on how\nthese systems can be manipulated through adversarial gaming strategies. Our\nresearch identifies three main types of gaming strategies that exploit the\nsystem's weaknesses, potentially leading to false positives. To counteract\nthese vulnerabilities, we implement several adversarial training methods\ndesigned to enhance the systems' robustness. Our results indicate that these\nmethods significantly reduce the susceptibility of grading systems to such\nmanipulations, especially when combined with ensemble techniques like majority\nvoting and ridge regression, which further improve the system's defense against\nsophisticated adversarial inputs. Additionally, employing large language models\nsuch as GPT-4 with varied prompting techniques has shown promise in recognizing\nand scoring gaming strategies effectively. The findings underscore the\nimportance of continuous improvements in AI-driven educational tools to ensure\ntheir reliability and fairness in high-stakes settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u533b\u5b66\u6559\u80b2\u4e2d\u57fa\u4e8eTransformer\u7684\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u7684\u6f0f\u6d1e\uff0c\u63d0\u51fa\u4e86\u5bf9\u6297\u6027\u7b56\u7565\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u96c6\u6210\u6280\u672f\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u4e86\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63ed\u793a\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u7684\u6f5c\u5728\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u5728\u533b\u5b66\u6559\u80b2\u8fd9\u7c7b\u9ad8\u98ce\u9669\u7684\u573a\u666f\u4e2d\uff0c\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u91c7\u7528\u5bf9\u6297\u6027\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u96c6\u6210\u6280\u672f\uff08\u5982\u591a\u6570\u6295\u7968\u548c\u5cad\u56de\u5f52\uff09\u53caGPT-4\u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6837\u5316\u63d0\u793a\u6280\u672f\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8bc4\u5206\u7cfb\u7edf\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u4e86\u5bf9AI\u6559\u80b2\u5de5\u5177\u6301\u7eed\u6539\u8fdb\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u5176\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u516c\u5e73\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2505.00040", "pdf": "https://arxiv.org/pdf/2505.00040", "abs": "https://arxiv.org/abs/2505.00040", "authors": ["Dishanand Jayeprokash", "Julia Gonski"], "title": "Convolutional Autoencoders for Data Compression and Anomaly Detection in Small Satellite Technologies", "categories": ["astro-ph.IM", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Small satellite technologies have enhanced the potential and feasibility of\ngeodesic missions, through simplification of design and decreased costs\nallowing for more frequent launches. On-satellite data acquisition systems can\nbenefit from the implementation of machine learning (ML), for better\nperformance and greater efficiency on tasks such as image processing or feature\nextraction. This work presents convolutional autoencoders for implementation on\nthe payload of small satellites, designed to achieve dual functionality of data\ncompression for more efficient off-satellite transmission, and at-source\nanomaly detection to inform satellite data-taking. This capability is\ndemonstrated for a use case of disaster monitoring using aerial image datasets\nof the African continent, offering avenues for both novel ML-based approaches\nin small satellite applications along with the expansion of space technology\nand artificial intelligence in Africa.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5377\u79ef\u81ea\u7f16\u7801\u5668\u5728\u5c0f\u536b\u661f\u4e0a\u5b9e\u73b0\u6570\u636e\u538b\u7f29\u548c\u5f02\u5e38\u68c0\u6d4b\u7684\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u707e\u5bb3\u76d1\u6d4b\u6548\u7387\u3002", "motivation": "\u5c0f\u536b\u661f\u6280\u672f\u7684\u8fdb\u6b65\u964d\u4f4e\u6210\u672c\u5e76\u589e\u52a0\u53d1\u5c04\u9891\u7387\uff0c\u4f46\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6570\u636e\u5904\u7406\u548c\u4f20\u8f93\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5377\u79ef\u81ea\u7f16\u7801\u5668\u5728\u5c0f\u536b\u661f\u4e0a\u5b9e\u73b0\u6570\u636e\u538b\u7f29\u548c\u5f02\u5e38\u68c0\u6d4b\u529f\u80fd\u3002", "result": "\u5728\u975e\u6d32\u5927\u9646\u7684\u707e\u5bb3\u76d1\u6d4b\u7528\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6280\u672f\u4e3a\u5c0f\u536b\u661f\u5e94\u7528\u548c\u975e\u6d32\u7a7a\u95f4\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6848\u3002"}}
{"id": "2505.00402", "pdf": "https://arxiv.org/pdf/2505.00402", "abs": "https://arxiv.org/abs/2505.00402", "authors": ["Jinhui Yi", "Huan Yan", "Haotian Wang", "Jian Yuan", "Yong Li"], "title": "DeepSTA: A Spatial-Temporal Attention Network for Logistics Delivery Timely Rate Prediction in Anomaly Conditions", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by CIKM 2023", "summary": "Prediction of couriers' delivery timely rates in advance is essential to the\nlogistics industry, enabling companies to take preemptive measures to ensure\nthe normal operation of delivery services. This becomes even more critical\nduring anomaly conditions like the epidemic outbreak, during which couriers'\ndelivery timely rate will decline markedly and fluctuates significantly.\nExisting studies pay less attention to the logistics scenario. Moreover, many\nworks focusing on prediction tasks in anomaly scenarios fail to explicitly\nmodel abnormal events, e.g., treating external factors equally with other\nfeatures, resulting in great information loss. Further, since some anomalous\nevents occur infrequently, traditional data-driven methods perform poorly in\nthese scenarios. To deal with them, we propose a deep spatial-temporal\nattention model, named DeepSTA. To be specific, to avoid information loss, we\ndesign an anomaly spatio-temporal learning module that employs a recurrent\nneural network to model incident information. Additionally, we utilize Node2vec\nto model correlations between road districts, and adopt graph neural networks\nand long short-term memory to capture the spatial-temporal dependencies of\ncouriers. To tackle the issue of insufficient training data in abnormal\ncircumstances, we propose an anomaly pattern attention module that adopts a\nmemory network for couriers' anomaly feature patterns storage via attention\nmechanisms. The experiments on real-world logistics datasets during the\nCOVID-19 outbreak in 2022 show the model outperforms the best baselines by\n12.11% in MAE and 13.71% in MSE, demonstrating its superior performance over\nmultiple competitive baselines.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepSTA\u7684\u6df1\u5ea6\u65f6\u7a7a\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u7269\u6d41\u884c\u4e1a\u4e2d\u5feb\u9012\u5458\u7684\u51c6\u65f6\u9001\u8fbe\u7387\uff0c\u5c24\u5176\u5728\u5f02\u5e38\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u7269\u6d41\u884c\u4e1a\u9700\u8981\u63d0\u524d\u9884\u6d4b\u5feb\u9012\u5458\u7684\u51c6\u65f6\u9001\u8fbe\u7387\u4ee5\u786e\u4fdd\u670d\u52a1\u6b63\u5e38\u8fd0\u884c\uff0c\u5c24\u5176\u662f\u5728\u5f02\u5e38\u4e8b\u4ef6\uff08\u5982\u75ab\u60c5\uff09\u671f\u95f4\uff0c\u73b0\u6709\u7814\u7a76\u5bf9\u6b64\u5173\u6ce8\u4e0d\u8db3\u4e14\u672a\u80fd\u6709\u6548\u5efa\u6a21\u5f02\u5e38\u4e8b\u4ef6\u3002", "method": "\u63d0\u51faDeepSTA\u6a21\u578b\uff0c\u5305\u542b\u5f02\u5e38\u65f6\u7a7a\u5b66\u4e60\u6a21\u5757\u548c\u5f02\u5e38\u6a21\u5f0f\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5206\u522b\u5229\u7528\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u5f02\u5e38\u4e8b\u4ef6\u4fe1\u606f\uff0c\u4ee5\u53ca\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5b58\u50a8\u5f02\u5e38\u7279\u5f81\u6a21\u5f0f\u3002", "result": "\u57282022\u5e74COVID-19\u75ab\u60c5\u671f\u95f4\u7684\u771f\u5b9e\u7269\u6d41\u6570\u636e\u96c6\u4e0a\uff0cDeepSTA\u7684MAE\u548cMSE\u5206\u522b\u6bd4\u6700\u4f73\u57fa\u7ebf\u6a21\u578b\u63d0\u534712.11%\u548c13.71%\u3002", "conclusion": "DeepSTA\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5f02\u5e38\u4e8b\u4ef6\u548c\u5b58\u50a8\u5f02\u5e38\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u60c5\u51b5\u4e0b\u5feb\u9012\u5458\u51c6\u65f6\u9001\u8fbe\u7387\u7684\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2505.00063", "pdf": "https://arxiv.org/pdf/2505.00063", "abs": "https://arxiv.org/abs/2505.00063", "authors": ["Siqi Li", "Yufan Shen", "Xiangnan Chen", "Jiayi Chen", "Hengwei Ju", "Haodong Duan", "Song Mao", "Hongbin Zhou", "Bo Zhang", "Pinlong Cai", "Licheng Wen", "Botian Shi", "Yong Liu", "Xinyu Cai", "Yu Qiao"], "title": "GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The rapid advancement of multimodal large language models (MLLMs) has\nprofoundly impacted the document domain, creating a wide array of application\nscenarios. This progress highlights the need for a comprehensive benchmark to\nevaluate these models' capabilities across various document-specific tasks.\nHowever, existing benchmarks often fail to locate specific model weaknesses or\nguide systematic improvements. To bridge this gap, we introduce a General\nDocument Intelligence Benchmark (GDI-Bench), featuring 1.9k images across 9 key\nscenarios and 19 document-specific tasks. By decoupling visual complexity and\nreasoning complexity, the GDI-Bench structures graded tasks that allow\nperformance assessment by difficulty, aiding in model weakness identification\nand optimization guidance. We evaluate the GDI-Bench on various open-source and\nclosed-source models, conducting decoupled analyses in the visual and reasoning\ndomains. For instance, the GPT-4o model excels in reasoning tasks but exhibits\nlimitations in visual capabilities. To address the diverse tasks and domains in\nthe GDI-Bench, we propose a GDI Model that mitigates the issue of catastrophic\nforgetting during the supervised fine-tuning (SFT) process through a\nintelligence-preserving training strategy. Our model achieves state-of-the-art\nperformance on previous benchmarks and the GDI-Bench. Both our benchmark and\nmodel will be open source.", "AI": {"tldr": "GDI-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b1.9k\u56fe\u50cf\u548c19\u4e2a\u4efb\u52a1\uff0c\u901a\u8fc7\u89c6\u89c9\u4e0e\u63a8\u7406\u590d\u6742\u5ea6\u5206\u79bb\u5b9e\u73b0\u5206\u5c42\u8bc4\u4f30\u3002\u63d0\u51fa\u7684GDI\u6a21\u578b\u901a\u8fc7\u667a\u80fd\u4fdd\u7559\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u96be\u4ee5\u7cfb\u7edf\u8bc6\u522b\u6a21\u578b\u5f31\u70b9\u6216\u6307\u5bfc\u6539\u8fdb\uff0c\u9700\u4e00\u4e2a\u5168\u9762\u7684\u6587\u6863\u667a\u80fd\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efaGDI-Bench\uff0c\u5305\u542b9\u79cd\u573a\u666f\u548c19\u9879\u4efb\u52a1\uff0c\u901a\u8fc7\u5206\u79bb\u89c6\u89c9\u4e0e\u63a8\u7406\u590d\u6742\u5ea6\u8fdb\u884c\u5206\u5c42\u8bc4\u4f30\uff1b\u63d0\u51faGDI\u6a21\u578b\uff0c\u91c7\u7528\u667a\u80fd\u4fdd\u7559\u8bad\u7ec3\u7b56\u7565\u4f18\u5316\u76d1\u7763\u5fae\u8c03\u3002", "result": "GPT-4o\u5728\u63a8\u7406\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\u4f46\u89c6\u89c9\u80fd\u529b\u6709\u9650\uff1bGDI\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "conclusion": "GDI-Bench\u548cGDI\u6a21\u578b\u586b\u8865\u4e86\u6587\u6863\u667a\u80fd\u8bc4\u4f30\u7a7a\u767d\uff0c\u6a21\u578b\u548c\u57fa\u51c6\u5c06\u5f00\u6e90\u3002"}}
{"id": "2505.00410", "pdf": "https://arxiv.org/pdf/2505.00410", "abs": "https://arxiv.org/abs/2505.00410", "authors": ["Farhana Elias", "Md Shihab Reza", "Muhammad Zawad Mahmud", "Samiha Islam"], "title": "Machine Learning Meets Transparency in Osteoporosis Risk Assessment: A Comparative Study of ML and Explainability Analysis", "categories": ["cs.LG"], "comment": "Submitted in an international conference", "summary": "The present research tackles the difficulty of predicting osteoporosis risk\nvia machine learning (ML) approaches, emphasizing the use of explainable\nartificial intelligence (XAI) to improve model transparency. Osteoporosis is a\nsignificant public health concern, sometimes remaining untreated owing to its\nasymptomatic characteristics, and early identification is essential to avert\nfractures. The research assesses six machine learning classifiers: Random\nForest, Logistic Regression, XGBoost, AdaBoost, LightGBM, and Gradient Boosting\nand utilizes a dataset based on clinical, demographic, and lifestyle variables.\nThe models are refined using GridSearchCV to calibrate hyperparameters, with\nthe objective of enhancing predictive efficacy. XGBoost had the greatest\naccuracy (91%) among the evaluated models, surpassing others in precision\n(0.92), recall (0.91), and F1-score (0.90). The research further integrates XAI\napproaches, such as SHAP, LIME, and Permutation Feature Importance, to\nelucidate the decision-making process of the optimal model. The study indicates\nthat age is the primary determinant in forecasting osteoporosis risk, followed\nby hormonal alterations and familial history. These results corroborate\nclinical knowledge and affirm the models' therapeutic significance. The\nresearch underscores the significance of explainability in machine learning\nmodels for healthcare applications, guaranteeing that physicians can rely on\nthe system's predictions. The report ultimately proposes directions for further\nresearch, such as validation across varied populations and the integration of\nsupplementary biomarkers for enhanced predictive accuracy.", "AI": {"tldr": "\u4f7f\u7528\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u63d0\u5347\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9aa8\u8d28\u758f\u677e\u98ce\u9669\u9884\u6d4b\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u9aa8\u8d28\u758f\u677e\u56e0\u65e0\u75c7\u72b6\u5e38\u88ab\u5ffd\u89c6\uff0c\u65e9\u671f\u9884\u6d4b\u5bf9\u9884\u9632\u9aa8\u6298\u81f3\u5173\u91cd\u8981\uff0c\u9700\u63d0\u9ad8\u6a21\u578b\u7684\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u3002", "method": "\u8bc4\u4f30\u516d\u79cd\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\uff08\u5982XGBoost\u3001\u968f\u673a\u68ee\u6797\uff09\uff0c\u5e76\u901a\u8fc7GridSearchCV\u4f18\u5316\u8d85\u53c2\u6570\uff0c\u7ed3\u5408SHAP\u7b49XAI\u65b9\u6cd5\u89e3\u91ca\u6a21\u578b\u3002", "result": "XGBoost\u8868\u73b0\u6700\u4f73\uff08\u51c6\u786e\u738791%\uff09\uff0c\u5e74\u9f84\u662f\u4e3b\u8981\u98ce\u9669\u56e0\u7d20\uff0c\u7ed3\u679c\u4e0e\u4e34\u5e8a\u77e5\u8bc6\u4e00\u81f4\u3002", "conclusion": "\u6a21\u578b\u89e3\u91ca\u6027\u5bf9\u533b\u7597\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u672a\u6765\u9700\u9a8c\u8bc1\u8de8\u4eba\u7fa4\u6570\u636e\u5e76\u6574\u5408\u66f4\u591a\u751f\u7269\u6807\u5fd7\u7269\u3002"}}
{"id": "2505.00065", "pdf": "https://arxiv.org/pdf/2505.00065", "abs": "https://arxiv.org/abs/2505.00065", "authors": ["Ivan Vankov", "Matyo Ivanov", "Adriana Correia", "Victor Botev"], "title": "ConSens: Assessing context grounding in open-book question answering", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 3 figures, 3 tables", "summary": "Large Language Models (LLMs) have demonstrated considerable success in\nopen-book question answering (QA), where the task requires generating answers\ngrounded in a provided external context. A critical challenge in open-book QA\nis to ensure that model responses are based on the provided context rather than\nits parametric knowledge, which can be outdated, incomplete, or incorrect.\nExisting evaluation methods, primarily based on the LLM-as-a-judge approach,\nface significant limitations, including biases, scalability issues, and\ndependence on costly external systems. To address these challenges, we propose\na novel metric that contrasts the perplexity of the model response under two\nconditions: when the context is provided and when it is not. The resulting\nscore quantifies the extent to which the model's answer relies on the provided\ncontext. The validity of this metric is demonstrated through a series of\nexperiments that show its effectiveness in identifying whether a given answer\nis grounded in the provided context. Unlike existing approaches, this metric is\ncomputationally efficient, interpretable, and adaptable to various use cases,\noffering a scalable and practical solution to assess context utilization in\nopen-book QA systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6307\u6807\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6a21\u578b\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u56f0\u60d1\u5ea6\uff0c\u91cf\u5316\u56de\u7b54\u57fa\u4e8e\u7ed9\u5b9a\u4e0a\u4e0b\u6587\u800c\u975e\u53c2\u6570\u77e5\u8bc6\u7684\u7a0b\u5ea6\uff0c\u89e3\u51b3\u4e86\u5f00\u653e\u4e66\u95ee\u7b54\u4e2d\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f00\u653e\u4e66\u95ee\u7b54\u4e2d\u6a21\u578b\u56de\u7b54\u53ef\u80fd\u4f9d\u8d56\u5176\u56fa\u6709\u77e5\u8bc6\u800c\u975e\u7ed9\u5b9a\u4e0a\u4e0b\u6587\u7684\u95ee\u9898\uff0c\u5e76\u514b\u670d\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u504f\u89c1\u3001\u53ef\u6269\u5c55\u6027\u548c\u6210\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6307\u6807\uff0c\u5bf9\u6bd4\u6a21\u578b\u5728\u63d0\u4f9b\u4e0a\u4e0b\u6587\u548c\u4e0d\u63d0\u4f9b\u4e0a\u4e0b\u6587\u65f6\u7684\u56de\u7b54\u56f0\u60d1\u5ea6\uff0c\u4ee5\u6b64\u91cf\u5316\u56de\u7b54\u5bf9\u4e0a\u4e0b\u6587\u7684\u4f9d\u8d56\u7a0b\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6307\u6807\u6709\u6548\u8bc6\u522b\u56de\u7b54\u662f\u5426\u57fa\u4e8e\u7ed9\u5b9a\u4e0a\u4e0b\u6587\uff0c\u4e14\u8ba1\u7b97\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u6027\u5f3a\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002", "conclusion": "\u65b0\u6307\u6807\u4e3a\u5f00\u653e\u4e66\u95ee\u7b54\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u5b9e\u7528\u7684\u4e0a\u4e0b\u6587\u5229\u7528\u8bc4\u4f30\u65b9\u6848\u3002"}}
{"id": "2505.00415", "pdf": "https://arxiv.org/pdf/2505.00415", "abs": "https://arxiv.org/abs/2505.00415", "authors": ["Tian Lan", "Yifei Gao", "Yimeng Lu", "Chen Zhang"], "title": "CICADA: Cross-Domain Interpretable Coding for Anomaly Detection and Adaptation in Multivariate Time Series", "categories": ["cs.LG"], "comment": null, "summary": "Unsupervised Time series anomaly detection plays a crucial role in\napplications across industries. However, existing methods face significant\nchallenges due to data distributional shifts across different domains, which\nare exacerbated by the non-stationarity of time series over time. Existing\nmodels fail to generalize under multiple heterogeneous source domains and\nemerging unseen new target domains. To fill the research gap, we introduce\nCICADA (Cross-domain Interpretable Coding for Anomaly Detection and\nAdaptation), with four key innovations: (1) a mixture of experts (MOE)\nframework that captures domain-agnostic anomaly features with high flexibility\nand interpretability; (2) a novel selective meta-learning mechanism to prevent\nnegative transfer between dissimilar domains, (3) an adaptive expansion\nalgorithm for emerging heterogeneous domain expansion, and (4) a hierarchical\nattention structure that quantifies expert contributions during fusion to\nenhance interpretability further.Extensive experiments on synthetic and\nreal-world industrial datasets demonstrate that CICADA outperforms\nstate-of-the-art methods in both cross-domain detection performance and\ninterpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCICADA\u7684\u65e0\u76d1\u7763\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\u3001\u9009\u62e9\u6027\u5143\u5b66\u4e60\u673a\u5236\u3001\u81ea\u9002\u5e94\u6269\u5c55\u7b97\u6cd5\u548c\u5206\u5c42\u6ce8\u610f\u529b\u7ed3\u6784\u89e3\u51b3\u8de8\u57df\u5f02\u6784\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u57df\u5f02\u6784\u6570\u636e\u548c\u65b0\u5174\u672a\u77e5\u76ee\u6807\u57df\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0cCICADA\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "1. \u6df7\u5408\u4e13\u5bb6\u6846\u67b6\u6355\u6349\u57df\u65e0\u5173\u5f02\u5e38\u7279\u5f81\uff1b2. \u9009\u62e9\u6027\u5143\u5b66\u4e60\u673a\u5236\u9632\u6b62\u4e0d\u76f8\u4f3c\u57df\u95f4\u7684\u8d1f\u8fc1\u79fb\uff1b3. \u81ea\u9002\u5e94\u6269\u5c55\u7b97\u6cd5\u5904\u7406\u65b0\u5174\u5f02\u6784\u57df\uff1b4. \u5206\u5c42\u6ce8\u610f\u529b\u7ed3\u6784\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\uff0cCICADA\u5728\u8de8\u57df\u68c0\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CICADA\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u89e3\u51b3\u4e86\u8de8\u57df\u5f02\u5e38\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.00114", "pdf": "https://arxiv.org/pdf/2505.00114", "abs": "https://arxiv.org/abs/2505.00114", "authors": ["Silvana Yakhni", "Ali Chehab"], "title": "Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper examines the effectiveness of Large Language Models (LLMs) in\ntranslating the low-resource Lebanese dialect, focusing on the impact of\nculturally authentic data versus larger translated datasets. We compare three\nfine-tuning approaches: Basic, contrastive, and grammar-hint tuning, using\nopen-source Aya23 models. Experiments reveal that models fine-tuned on a\nsmaller but culturally aware Lebanese dataset (LW) consistently outperform\nthose trained on larger, non-native data. The best results were achieved\nthrough contrastive fine-tuning paired with contrastive prompting, which\nindicates the benefits of exposing translation models to bad examples. In\naddition, to ensure authentic evaluation, we introduce LebEval, a new benchmark\nderived from native Lebanese content, and compare it to the existing FLoRes\nbenchmark. Our findings challenge the \"More Data is Better\" paradigm and\nemphasize the crucial role of cultural authenticity in dialectal translation.\nWe made our datasets and code available on Github.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4f4e\u8d44\u6e90\u9ece\u5df4\u5ae9\u65b9\u8a00\u7ffb\u8bd1\u4e2d\u7684\u6548\u679c\uff0c\u6bd4\u8f83\u4e86\u4f7f\u7528\u6587\u5316\u771f\u5b9e\u6027\u6570\u636e\u4e0e\u5927\u89c4\u6a21\u7ffb\u8bd1\u6570\u636e\u96c6\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u6570\u636e\u91cf\u8f83\u5c0f\uff0c\u4f46\u57fa\u4e8e\u6587\u5316\u771f\u5b9e\u6027\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u63a2\u8ba8\u6587\u5316\u771f\u5b9e\u6027\u6570\u636e\u5bf9\u4e8e\u4f4e\u8d44\u6e90\u65b9\u8a00\u7ffb\u8bd1\u7684\u91cd\u8981\u6027\uff0c\u6311\u6218\u201c\u6570\u636e\u8d8a\u591a\u8d8a\u597d\u201d\u7684\u4f20\u7edf\u8303\u5f0f\u3002", "method": "\u91c7\u7528\u4e86\u4e09\u79cd\u5fae\u8c03\u65b9\u6cd5\uff08\u57fa\u7840\u3001\u5bf9\u6bd4\u548c\u8bed\u6cd5\u63d0\u793a\u5fae\u8c03\uff09\uff0c\u5e76\u5728Aya23\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u540c\u65f6\u5f15\u5165LebEval\u65b0\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u6587\u5316\u771f\u5b9e\u6027\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5927\u89c4\u6a21\u975e\u539f\u751f\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5bf9\u6bd4\u5fae\u8c03\u7ed3\u5408\u5bf9\u6bd4\u63d0\u793a\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u6587\u5316\u771f\u5b9e\u6027\u6bd4\u6570\u636e\u91cf\u66f4\u91cd\u8981\uff0c\u4e14\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u6709\u663e\u8457\u5e2e\u52a9\u3002"}}
{"id": "2505.00091", "pdf": "https://arxiv.org/pdf/2505.00091", "abs": "https://arxiv.org/abs/2505.00091", "authors": ["Tengchao Zhang", "Yonglin Tian", "Fei Lin", "Jun Huang", "Rui Qin", "Fei-Yue Wang"], "title": "CoordField: Coordination Field for Agentic UAV Task Allocation In Low-altitude Urban Scenarios", "categories": ["cs.RO", "cs.AI"], "comment": "Submitted ITSC 2025", "summary": "With the increasing demand for heterogeneous Unmanned Aerial Vehicle (UAV)\nswarms to perform complex tasks in urban environments, system design now faces\nmajor challenges, including efficient semantic understanding, flexible task\nplanning, and the ability to dynamically adjust coordination strategies in\nresponse to evolving environmental conditions and continuously changing task\nrequirements. To address the limitations of existing approaches, this paper\nproposes coordination field agentic system for coordinating heterogeneous UAV\nswarms in complex urban scenarios. In this system, large language models (LLMs)\nis responsible for interpreting high-level human instructions and converting\nthem into executable commands for the UAV swarms, such as patrol and target\ntracking. Subsequently, a Coordination field mechanism is proposed to guide UAV\nmotion and task selection, enabling decentralized and adaptive allocation of\nemergent tasks. A total of 50 rounds of comparative testing were conducted\nacross different models in a 2D simulation space to evaluate their performance.\nExperimental results demonstrate that the proposed system achieves superior\nperformance in terms of task coverage, response time, and adaptability to\ndynamic changes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u534f\u8c03\u573a\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u89e3\u91ca\u9ad8\u5c42\u6307\u4ee4\u5e76\u534f\u8c03\u5f02\u6784\u65e0\u4eba\u673a\u7fa4\uff0c\u5b9e\u73b0\u9ad8\u6548\u4efb\u52a1\u89c4\u5212\u548c\u52a8\u6001\u8c03\u6574\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u5f02\u6784\u65e0\u4eba\u673a\u7fa4\u5728\u8bed\u4e49\u7406\u89e3\u3001\u4efb\u52a1\u89c4\u5212\u548c\u52a8\u6001\u534f\u8c03\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408LLM\u89e3\u91ca\u6307\u4ee4\u548c\u534f\u8c03\u573a\u673a\u5236\uff0c\u5b9e\u73b0\u5206\u5e03\u5f0f\u4efb\u52a1\u5206\u914d\u548c\u52a8\u6001\u8c03\u6574\u3002", "result": "50\u8f6e\u6d4b\u8bd5\u8868\u660e\uff0c\u7cfb\u7edf\u5728\u4efb\u52a1\u8986\u76d6\u7387\u3001\u54cd\u5e94\u65f6\u95f4\u548c\u52a8\u6001\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u7cfb\u7edf\u6709\u6548\u63d0\u5347\u4e86\u5f02\u6784\u65e0\u4eba\u673a\u7fa4\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u534f\u8c03\u80fd\u529b\u548c\u4efb\u52a1\u6267\u884c\u6548\u7387\u3002"}}
{"id": "2505.00422", "pdf": "https://arxiv.org/pdf/2505.00422", "abs": "https://arxiv.org/abs/2505.00422", "authors": ["Yu Han", "Aaron Ceross", "Jeroen H. M. Bergmann"], "title": "Toward Automated Regulatory Decision-Making: Trustworthy Medical Device Risk Classification with Multimodal Transformers and Self-Training", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Accurate classification of medical device risk levels is essential for\nregulatory oversight and clinical safety. We present a Transformer-based\nmultimodal framework that integrates textual descriptions and visual\ninformation to predict device regulatory classification. The model incorporates\na cross-attention mechanism to capture intermodal dependencies and employs a\nself-training strategy for improved generalization under limited supervision.\nExperiments on a real-world regulatory dataset demonstrate that our approach\nachieves up to 90.4% accuracy and 97.9% AUROC, significantly outperforming\ntext-only (77.2%) and image-only (54.8%) baselines. Compared to standard\nmultimodal fusion, the self-training mechanism improved SVM performance by 3.3\npercentage points in accuracy (from 87.1% to 90.4%) and 1.4 points in macro-F1,\nsuggesting that pseudo-labeling can effectively enhance generalization under\nlimited supervision. Ablation studies further confirm the complementary\nbenefits of both cross-modal attention and self-training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u8bbe\u5907\u76d1\u7ba1\u5206\u7c7b\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\u5e76\u91c7\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u81ea\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u533b\u5b66\u8bbe\u5907\u7684\u98ce\u9669\u7b49\u7ea7\u5206\u7c7b\u5bf9\u76d1\u7ba1\u548c\u4e34\u5e8a\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5355\u4e00\u6a21\u6001\uff08\u5982\u6587\u672c\u6216\u56fe\u50cf\uff09\uff0c\u4e14\u5728\u5c0f\u6837\u672c\u76d1\u7763\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u91c7\u7528Transformer\u6846\u67b6\u6574\u5408\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\uff0c\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u6a21\u6001\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5f15\u5165\u81ea\u8bad\u7ec3\u7b56\u7565\u4ee5\u63d0\u5347\u5c0f\u6837\u672c\u4e0b\u7684\u6cdb\u5316\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u771f\u5b9e\u76d1\u7ba1\u6570\u636e\u96c6\u4e0a\u8fbe\u523090.4%\u51c6\u786e\u7387\u548c97.9% AUROC\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\uff08\u6587\u672c77.2%\uff0c\u56fe\u50cf54.8%\uff09\u3002\u81ea\u8bad\u7ec3\u673a\u5236\u5c06SVM\u6027\u80fd\u63d0\u5347\u4e863.3\u4e2a\u51c6\u786e\u7387\u767e\u5206\u70b9\u3002", "conclusion": "\u4ea4\u53c9\u6ce8\u610f\u529b\u4e0e\u81ea\u8bad\u7ec3\u7684\u7ed3\u5408\u5728\u591a\u6a21\u6001\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u4e92\u8865\u4f18\u52bf\uff0c\u5c24\u5176\u5728\u76d1\u7ba1\u5206\u7c7b\u4efb\u52a1\u4e2d\u80fd\u6709\u6548\u63d0\u5347\u5c0f\u6837\u672c\u4e0b\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.00127", "pdf": "https://arxiv.org/pdf/2505.00127", "abs": "https://arxiv.org/abs/2505.00127", "authors": ["Jinyan Su", "Jennifer Healey", "Preslav Nakov", "Claire Cardie"], "title": "Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly optimized for long reasoning,\nunder the assumption that more reasoning leads to better performance. However,\nemerging evidence suggests that longer responses can sometimes degrade accuracy\nrather than improve it. In this paper, we conduct a systematic empirical study\nof the relationship between reasoning length and answer correctness. We find\nthat LLMs tend to overthink simple problems, generating unnecessarily long\noutputs, and underthink harder ones, failing to extend their reasoning when it\nis most needed. This indicates that models might misjudge problem difficulty\nand fail to calibrate their response length appropriately. Furthermore, we\ninvestigate the effects of length reduction with a preference optimization\nalgorithm when simply preferring the shorter responses regardless of answer\ncorrectness. Experiments show that the generation length can be significantly\nreduced while maintaining acceptable accuracy. Our findings highlight\ngeneration length as a meaningful signal for reasoning behavior and motivate\nfurther exploration into LLMs' self-awareness in reasoning length adaptation.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u957f\u5ea6\u4e0e\u7b54\u6848\u6b63\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\uff1a\u7b80\u5355\u95ee\u9898\u65f6\u8fc7\u5ea6\u63a8\u7406\uff0c\u590d\u6742\u95ee\u9898\u5374\u63a8\u7406\u4e0d\u8db3\u3002\u901a\u8fc7\u504f\u597d\u4f18\u5316\u7b97\u6cd5\u7f29\u77ed\u8f93\u51fa\u957f\u5ea6\uff0c\u53ef\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u751f\u6210\u957f\u5ea6\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u957f\u5ea6\u4e0e\u7b54\u6848\u6b63\u786e\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63ed\u793a\u6a21\u578b\u53ef\u80fd\u56e0\u8bef\u5224\u95ee\u9898\u96be\u5ea6\u800c\u8c03\u6574\u63a8\u7406\u957f\u5ea6\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u63a8\u7406\u957f\u5ea6\u4e0e\u7b54\u6848\u6b63\u786e\u6027\u7684\u5173\u8054\uff0c\u5e76\u5229\u7528\u504f\u597d\u4f18\u5316\u7b97\u6cd5\u6d4b\u8bd5\u7f29\u77ed\u8f93\u51fa\u957f\u5ea6\u5bf9\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u8fc7\u5ea6\u63a8\u7406\uff0c\u590d\u6742\u95ee\u9898\u4e0a\u63a8\u7406\u4e0d\u8db3\uff1b\u901a\u8fc7\u504f\u597d\u4f18\u5316\u7b97\u6cd5\u80fd\u663e\u8457\u51cf\u5c11\u8f93\u51fa\u957f\u5ea6\u4e14\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "\u751f\u6210\u957f\u5ea6\u53ef\u4f5c\u4e3a\u63a8\u7406\u884c\u4e3a\u7684\u4fe1\u53f7\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u6a21\u578b\u5728\u63a8\u7406\u957f\u5ea6\u81ea\u9002\u5e94\u4e2d\u7684\u81ea\u6211\u610f\u8bc6\u3002"}}
{"id": "2505.00100", "pdf": "https://arxiv.org/pdf/2505.00100", "abs": "https://arxiv.org/abs/2505.00100", "authors": ["Ethan Dickey", "Andres Bejarano", "Rhianna Kuperus", "B\u00e1rbara Fagundes"], "title": "Evaluating the AI-Lab Intervention: Impact on Student Perception and Use of Generative AI in Early Undergraduate Computer Science Courses", "categories": ["cs.CY", "cs.AI", "cs.ET", "K.3"], "comment": "18 pages, 5 figures, 17 tables, submitted for publication", "summary": "Generative AI (GenAI) is rapidly entering computer science education, yet its\neffects on student learning, skill development, and perceptions remain\nunderexplored. Concerns about overreliance coexist with a gap in research on\nstructured scaffolding to guide tool use in formal courses. This study examines\nthe impact of a dedicated \"AI-Lab\" intervention -- emphasizing guided\nscaffolding and mindful engagement -- on undergraduate students in Data\nStructures and Algorithms, Competitive Programming, and first-year engineering\ncourses at Purdue University.\n  Over three semesters, we integrated AI-Lab modules into four mandatory and\nelective courses, yielding 831 matched pre- and post-intervention survey\nresponses, alongside focus group discussions. Employing a mixed-methods\napproach, we analyzed quantitative shifts in usage patterns and attitudes as\nwell as qualitative narratives of student experiences.\n  While the overall frequency of GenAI usage for homework or programming\nprojects remained largely stable, we observed large effect sizes in comfort and\nopenness across conceptual, debugging, and homework problems. Notably, usage\npatterns for debugging also shifted statistically significantly, reflecting\nstudents' more mindful and deliberate approach. Focus group discussions\ncorroborated these results, suggesting that the intervention \"bridged the gap\"\nbetween naive GenAI usage and more nuanced, reflective integration of AI tools\ninto coursework, ultimately heightening students' awareness of their own skill\ndevelopment.\n  These findings suggest that structured, scaffolded interventions can enable\nstudents to harness GenAI's benefits without undermining essential\ncompetencies. We offer evidence-based recommendations for educators seeking to\nintegrate GenAI responsibly into computing curricula and identify avenues for\nfuture research on GenAI-supported pedagogy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u6559\u80b2\u4e2d\u5f15\u5165\u7ed3\u6784\u5316\u2018AI-Lab\u2019\u5e72\u9884\u5bf9\u672c\u79d1\u751f\u5b66\u4e60\u3001\u6280\u80fd\u53d1\u5c55\u548c\u8ba4\u77e5\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8fd9\u79cd\u5e72\u9884\u80fd\u5e2e\u52a9\u5b66\u751f\u66f4\u6709\u6548\u5730\u4f7f\u7528\u751f\u6210\u5f0fAI\u5de5\u5177\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u586b\u8865\u5173\u4e8e\u5982\u4f55\u901a\u8fc7\u7ed3\u6784\u5316\u5f15\u5bfc\u5de5\u5177\u4f7f\u7528\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5728\u4e09\u4e2a\u5b66\u671f\u4e2d\uff0c\u5c06AI-Lab\u6a21\u5757\u6574\u5408\u5230\u56db\u95e8\u8bfe\u7a0b\u4e2d\uff0c\u6536\u96c6831\u4efd\u5339\u914d\u7684\u5e72\u9884\u524d\u540e\u8c03\u67e5\u95ee\u5377\uff0c\u5e76\u8fdb\u884c\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba\uff0c\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u5206\u6790\u6570\u636e\u548c\u5b66\u751f\u4f53\u9a8c\u3002", "result": "\u5e72\u9884\u540e\uff0c\u5b66\u751f\u5728\u6982\u5ff5\u3001\u8c03\u8bd5\u548c\u4f5c\u4e1a\u95ee\u9898\u4e0a\u7684\u8212\u9002\u5ea6\u548c\u5f00\u653e\u6027\u663e\u8457\u63d0\u9ad8\uff0c\u8c03\u8bd5\u4f7f\u7528\u6a21\u5f0f\u4e5f\u6709\u663e\u8457\u53d8\u5316\uff0c\u8868\u660e\u5b66\u751f\u66f4\u8c28\u614e\u548c\u6df1\u601d\u719f\u8651\u5730\u4f7f\u7528AI\u5de5\u5177\u3002", "conclusion": "\u7ed3\u6784\u5316\u5e72\u9884\u80fd\u5e2e\u52a9\u5b66\u751f\u6709\u6548\u5229\u7528\u751f\u6210\u5f0fAI\u800c\u4e0d\u635f\u5bb3\u6838\u5fc3\u80fd\u529b\uff0c\u4e3a\u6559\u80b2\u8005\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u5efa\u8bae\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.00439", "pdf": "https://arxiv.org/pdf/2505.00439", "abs": "https://arxiv.org/abs/2505.00439", "authors": ["Timo P. Gros", "Nicola J. M\u00fcller", "Daniel Fiser", "Isabel Valera", "Verena Wolf", "J\u00f6rg Hoffmann"], "title": "Per-Domain Generalizing Policies: On Validation Instances and Scaling Behavior", "categories": ["cs.LG", "cs.AI"], "comment": "7 pages, 3 tables, 3 figures, 3 algorithms", "summary": "Recent work has shown that successful per-domain generalizing action policies\ncan be learned. Scaling behavior, from small training instances to large test\ninstances, is the key objective; and the use of validation instances larger\nthan training instances is one key to achieve it. Prior work has used fixed\nvalidation sets. Here, we introduce a method generating the validation set\ndynamically, on the fly, increasing instance size so long as informative and\nfeasible.We also introduce refined methodology for evaluating scaling behavior,\ngenerating test instances systematically to guarantee a given confidence in\ncoverage performance for each instance size. In experiments, dynamic validation\nimproves scaling behavior of GNN policies in all 9 domains used.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u751f\u6210\u9a8c\u8bc1\u96c6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u589e\u52a0\u5b9e\u4f8b\u89c4\u6a21\u6765\u63d0\u9ad8\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u4ece\u5c0f\u7684\u8bad\u7ec3\u5b9e\u4f8b\u4e2d\u5b66\u4e60\u5e76\u6cdb\u5316\u5230\u66f4\u5927\u7684\u6d4b\u8bd5\u5b9e\u4f8b\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u7684\u9a8c\u8bc1\u96c6\uff0c\u4f46\u5176\u6548\u679c\u6709\u9650\u3002", "method": "\u52a8\u6001\u751f\u6210\u9a8c\u8bc1\u96c6\uff0c\u9010\u6b65\u589e\u52a0\u5b9e\u4f8b\u89c4\u6a21\u76f4\u81f3\u4fe1\u606f\u53ef\u884c\uff1b\u540c\u65f6\u6539\u8fdb\u4e86\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7cfb\u7edf\u751f\u6210\u6d4b\u8bd5\u5b9e\u4f8b\u4ee5\u4fdd\u8bc1\u8986\u76d6\u6027\u80fd\u7684\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u52a8\u6001\u9a8c\u8bc1\u5728\u6240\u67099\u4e2a\u6d4b\u8bd5\u9886\u57df\u4e2d\u5747\u63d0\u5347\u4e86GNN\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u52a8\u6001\u9a8c\u8bc1\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4ece\u8bad\u7ec3\u5230\u6d4b\u8bd5\u7684\u5b9e\u4f8b\u89c4\u6a21\u6269\u5c55\u80fd\u529b\u3002"}}
{"id": "2505.00147", "pdf": "https://arxiv.org/pdf/2505.00147", "abs": "https://arxiv.org/abs/2505.00147", "authors": ["Yinghui He", "Abhishek Panigrahi", "Yong Lin", "Sanjeev Arora"], "title": "AdaptMI: Adaptive Skill-based In-context Math Instruction for Small Language Models", "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) allows a language model to improve its\nproblem-solving capability when provided with suitable information in context.\nSince the choice of in-context information can be determined based on the\nproblem itself, in-context learning is analogous to human learning from\nteachers in a classroom. Recent works (Didolkar et al., 2024a; 2024b) show that\nICL performance can be improved by leveraging a frontier large language model's\n(LLM) ability to predict required skills to solve a problem, popularly referred\nto as an LLM's metacognition, and using the recommended skills to construct\nnecessary in-context examples. While this skill-based strategy boosts ICL\nperformance in larger models, its gains on small language models (SLMs) have\nbeen minimal, highlighting a performance gap in ICL capabilities. We\ninvestigate this gap and show that skill-based prompting can hurt SLM\nperformance on easy questions by introducing unnecessary information, akin to\ncognitive overload. To address this, we introduce AdaptMI, an adaptive approach\nto selecting skill-based in-context Math Instructions for SLMs. Inspired by\ncognitive load theory from human pedagogy, our method only introduces\nskill-based examples when the model performs poorly. We further propose\nAdaptMI+, which adds examples targeted to the specific skills missing from the\nmodel's responses. On 5-shot evaluations across popular math benchmarks and\nfive SLMs (1B--7B; Qwen, Llama), AdaptMI+ improves accuracy by up to 6% over\nnaive skill-based strategies.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u60c5\u5883\u5b66\u4e60\uff08ICL\uff09\u5728\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u4e2d\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u65b9\u6cd5AdaptMI\u53ca\u6539\u8fdb\u7248AdaptMI+\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5f15\u5165\u6280\u80fd\u793a\u4f8b\u63d0\u5347SLMs\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u60c5\u5883\u5b66\u4e60\u5728\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u4e2d\u6548\u679c\u6709\u9650\uff0c\u751a\u81f3\u53ef\u80fd\u56e0\u4fe1\u606f\u8fc7\u8f7d\u800c\u964d\u4f4e\u8868\u73b0\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u63d0\u51faAdaptMI\u65b9\u6cd5\uff0c\u6839\u636e\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\uff0c\u4ec5\u5728\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u65f6\u5f15\u5165\u6280\u80fd\u793a\u4f8b\uff1b\u8fdb\u4e00\u6b65\u63d0\u51faAdaptMI+\uff0c\u9488\u5bf9\u7f3a\u5931\u6280\u80fd\u8865\u5145\u793a\u4f8b\u3002", "result": "\u57281B--7B\u89c4\u6a21\u7684SLMs\uff08\u5982Qwen\u3001Llama\uff09\u4e0a\uff0cAdaptMI+\u57285-shot\u8bc4\u4f30\u4e2d\u6bd4\u4f20\u7edf\u6280\u80fd\u7b56\u7565\u51c6\u786e\u7387\u63d0\u5347\u6700\u9ad86%\u3002", "conclusion": "AdaptMI+\u6709\u6548\u89e3\u51b3\u4e86SLMs\u5728\u60c5\u5883\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u9002\u5e94\u6027\u6280\u80fd\u63d0\u793a\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.00466", "pdf": "https://arxiv.org/pdf/2505.00466", "abs": "https://arxiv.org/abs/2505.00466", "authors": ["Thomas Flinkow", "Marco Casadio", "Colin Kessler", "Rosemary Monahan", "Ekaterina Komendantskaya"], "title": "A Generalised Framework for Property-Driven Machine Learning", "categories": ["cs.LG", "cs.LO"], "comment": "22 pages, 4 tables, 4 figures. Submitted to AI Verification 2025", "summary": "Neural networks have been shown to frequently fail to satisfy critical safety\nand correctness properties after training, highlighting the pressing need for\ntraining methods that incorporate such properties directly. While adversarial\ntraining can be used to improve robustness to small perturbations within\n$\\epsilon$-cubes, domains other than computer vision -- such as control systems\nand natural language processing -- may require more flexible input region\nspecifications via generalised hyper-rectangles. Meanwhile, differentiable\nlogics offer a way to encode arbitrary logical constraints as additional loss\nterms that guide the learning process towards satisfying these constraints. In\nthis paper, we investigate how these two complementary approaches can be\nunified within a single framework for property-driven machine learning. We show\nthat well-known properties from the literature are subcases of this general\napproach, and we demonstrate its practical effectiveness on a case study\ninvolving a neural network controller for a drone system. Our framework is\npublicly available at https://github.com/tflinkow/property-driven-ml.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5c06\u53ef\u5fae\u903b\u8f91\u548c\u5bf9\u6297\u8bad\u7ec3\u7ed3\u5408\uff0c\u4ee5\u6ee1\u8db3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u6b63\u786e\u6027\u9700\u6c42\uff0c\u5e76\u5728\u65e0\u4eba\u673a\u63a7\u5236\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u65b9\u6cd5\u5e38\u65e0\u6cd5\u6ee1\u8db3\u5173\u952e\u5b89\u5168\u6027\u548c\u6b63\u786e\u6027\u5c5e\u6027\uff0c\u5c24\u5176\u662f\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4ee5\u5916\u7684\u9886\u57df\uff0c\u5982\u63a7\u5236\u7cfb\u7edf\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u8f93\u5165\u533a\u57df\u89c4\u8303\u548c\u7ea6\u675f\u7f16\u7801\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u53ef\u5fae\u903b\u8f91\uff08\u7528\u4e8e\u7f16\u7801\u4efb\u610f\u903b\u8f91\u7ea6\u675f\uff09\u548c\u5bf9\u6297\u8bad\u7ec3\uff08\u7528\u4e8e\u589e\u5f3a\u9c81\u68d2\u6027\uff09\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u5c5e\u6027\u9a71\u52a8\u673a\u5668\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u8bc1\u660e\u4e86\u6587\u732e\u4e2d\u5e38\u89c1\u5c5e\u6027\u662f\u8be5\u6846\u67b6\u7684\u7279\u4f8b\uff0c\u5e76\u5728\u65e0\u4eba\u673a\u795e\u7ecf\u63a7\u5236\u5668\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u6548\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5c5e\u6027\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u901a\u7528\u65b9\u6cd5\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.00191", "pdf": "https://arxiv.org/pdf/2505.00191", "abs": "https://arxiv.org/abs/2505.00191", "authors": ["Yuyan Ge", "Kwan Ho Ryan Chan", "Pablo Messina", "Ren\u00e9 Vidal"], "title": "IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports", "categories": ["cs.CL"], "comment": "12 pages, 4 figures", "summary": "The development of AI-based methods for analyzing radiology reports could\nlead to significant advances in medical diagnosis--from improving diagnostic\naccuracy to enhancing efficiency and reducing workload. However, the lack of\ninterpretability in these methods has hindered their adoption in clinical\nsettings. In this paper, we propose an interpretable-by-design framework for\nclassifying radiology reports. The key idea is to extract a set of most\ninformative queries from a large set of reports and use these queries and their\ncorresponding answers to predict a diagnosis. Thus, the explanation for a\nprediction is, by construction, the set of selected queries and answers. We use\nthe Information Pursuit framework to select informative queries, the Flan-T5\nmodel to determine if facts are present in the report, and a classifier to\npredict the disease. Experiments on the MIMIC-CXR dataset demonstrate the\neffectiveness of the proposed method, highlighting its potential to enhance\ntrust and usability in medical AI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684AI\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u7c7b\u653e\u5c04\u5b66\u62a5\u544a\u3002\u901a\u8fc7\u63d0\u53d6\u4fe1\u606f\u4e30\u5bcc\u7684\u67e5\u8be2\u53ca\u5176\u7b54\u6848\u6765\u9884\u6d4b\u8bca\u65ad\uff0c\u63d0\u9ad8\u4e86\u533b\u5b66AI\u7684\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "motivation": "AI\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u91c7\u7528\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8bbe\u8ba1\u53ef\u89e3\u91ca\u7684\u6846\u67b6\u6765\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u7ed3\u5408\u4fe1\u606f\u8ffd\u8e2a\u6846\u67b6\u63d0\u53d6\u5173\u952e\u67e5\u8be2\uff0c\u4f7f\u7528Flan-T5\u6a21\u578b\u786e\u5b9a\u62a5\u544a\u4e2d\u662f\u5426\u5b58\u5728\u76f8\u5173\u4e8b\u5b9e\uff0c\u5e76\u901a\u8fc7\u5206\u7c7b\u5668\u9884\u6d4b\u75be\u75c5\u3002", "result": "\u5728MIMIC-CXR\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u793a\u4e86\u5176\u5728\u589e\u5f3a\u533b\u5b66AI\u4fe1\u4efb\u548c\u5b9e\u7528\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86AI\u5728\u653e\u5c04\u5b66\u62a5\u544a\u5206\u7c7b\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8fd8\u4e3a\u4e34\u5e8a\u73af\u5883\u4e2d\u7684AI\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00473", "pdf": "https://arxiv.org/pdf/2505.00473", "abs": "https://arxiv.org/abs/2505.00473", "authors": ["Shuwen Sun", "Lihong Feng", "Peter Benner"], "title": "Interpretable Spatial-Temporal Fusion Transformers: Multi-Output Prediction for Parametric Dynamical Systems with Time-Varying Inputs", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "We explore the promising performance of a transformer model in predicting\noutputs of parametric dynamical systems with external time-varying input\nsignals. The outputs of such systems vary not only with physical parameters but\nalso with external time-varying input signals. Accurately catching the dynamics\nof such systems is challenging. We have adapted and extended an existing\ntransformer model for single output prediction to a multiple-output transformer\nthat is able to predict multiple output responses of these systems. The\nmultiple-output transformer generalizes the interpretability of the original\ntransformer. The generalized interpretable attention weight matrix explores not\nonly the temporal correlations in the sequence, but also the interactions\nbetween the multiple outputs, providing explanation for the spatial correlation\nin the output domain. This multiple-output transformer accurately predicts the\nsequence of multiple outputs, regardless of the nonlinearity of the system and\nthe dimensionality of the parameter space.", "AI": {"tldr": "\u6458\u8981", "motivation": "\u6587\u7ae0\u63a2\u8ba8\u4e86\u53d8\u538b\u5668\u6a21\u578b\u5728\u9884\u6d4b\u5177\u6709\u5916\u90e8\u65f6\u53d8\u8f93\u5165\u4fe1\u53f7\u7684\u53c2\u6570\u52a8\u6001\u7cfb\u7edf\u8f93\u51fa\u65f6\u7684\u6027\u80fd\u3002\u8fd9\u7c7b\u7cfb\u7edf\u7684\u8f93\u51fa\u4e0d\u4ec5\u968f\u7269\u7406\u53c2\u6570\u53d8\u5316\uff0c\u8fd8\u53d7\u5916\u90e8\u65f6\u53d8\u8f93\u5165\u4fe1\u53f7\u5f71\u54cd\uff0c\u5176\u52a8\u529b\u5b66\u7279\u6027\u96be\u4ee5\u6355\u6349\u3002", "method": "\u4f5c\u8005\u5bf9\u73b0\u6709\u7684\u5355\u8f93\u51fa\u9884\u6d4b\u53d8\u538b\u5668\u6a21\u578b\u8fdb\u884c\u4e86\u8c03\u6574\u548c\u6269\u5c55\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u9884\u6d4b\u7cfb\u7edf\u591a\u4e2a\u8f93\u51fa\u54cd\u5e94\u7684\u591a\u8f93\u51fa\u53d8\u538b\u5668\u6a21\u578b\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5e7f\u4e49\u53ef\u89e3\u91ca\u6ce8\u610f\u529b\u6743\u91cd\u77e9\u9635\uff0c\u4e0d\u4ec5\u63a2\u7d22\u4e86\u5e8f\u5217\u4e2d\u7684\u65f6\u95f4\u76f8\u5173\u6027\uff0c\u8fd8\u63ed\u793a\u4e86\u591a\u4e2a\u8f93\u51fa\u4e4b\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "\u8fd9\u79cd\u591a\u8f93\u51fa\u53d8\u538b\u5668\u6a21\u578b\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u591a\u4e2a\u8f93\u51fa\u5e8f\u5217\uff0c\u65e0\u8bba\u7cfb\u7edf\u7684\u975e\u7ebf\u6027\u7a0b\u5ea6\u6216\u53c2\u6570\u7a7a\u95f4\u7684\u7ef4\u5ea6\u5982\u4f55\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u6269\u5c55\u53d8\u538b\u5668\u6a21\u578b\u5728\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u901a\u8fc7\u5e7f\u4e49\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.00261", "pdf": "https://arxiv.org/pdf/2505.00261", "abs": "https://arxiv.org/abs/2505.00261", "authors": ["Jayoung Song", "KyungTae Lim", "Jungyeul Park"], "title": "Enriching the Korean Learner Corpus with Multi-reference Annotations and Rubric-Based Scoring", "categories": ["cs.CL"], "comment": null, "summary": "Despite growing global interest in Korean language education, there remains a\nsignificant lack of learner corpora tailored to Korean L2 writing. To address\nthis gap, we enhance the KoLLA Korean learner corpus by adding multiple\ngrammatical error correction (GEC) references, thereby enabling more nuanced\nand flexible evaluation of GEC systems, and reflects the variability of human\nlanguage. Additionally, we enrich the corpus with rubric-based scores aligned\nwith guidelines from the Korean National Language Institute, capturing\ngrammatical accuracy, coherence, and lexical diversity. These enhancements make\nKoLLA a robust and standardized resource for research in Korean L2 education,\nsupporting advancements in language learning, assessment, and automated error\ncorrection.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u589e\u5f3aKoLLA\u97e9\u8bed\u5b66\u4e60\u8005\u8bed\u6599\u5e93\uff0c\u589e\u52a0\u591a\u53c2\u8003\u8bed\u6cd5\u7ea0\u9519\u548c\u8bc4\u5206\u6807\u51c6\uff0c\u63d0\u5347\u4e86\u5176\u5728\u97e9\u8bed\u4e8c\u8bed\u6559\u80b2\u7814\u7a76\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u6807\u51c6\u5316\u3002", "motivation": "\u5168\u7403\u5bf9\u97e9\u8bed\u6559\u80b2\u7684\u5174\u8da3\u589e\u957f\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9\u97e9\u8bed\u4e8c\u8bed\u5199\u4f5c\u7684\u5b66\u4e60\u8005\u8bed\u6599\u5e93\u3002", "method": "\u6269\u5145KoLLA\u8bed\u6599\u5e93\uff0c\u52a0\u5165\u591a\u53c2\u8003\u8bed\u6cd5\u7ea0\u9519\u548c\u57fa\u4e8e\u8bc4\u5206\u7684\u6807\u6ce8\uff0c\u4ee5\u66f4\u7075\u6d3b\u8bc4\u4f30\u7ea0\u9519\u7cfb\u7edf\u3002", "result": "\u589e\u5f3a\u4e86\u8bed\u6599\u5e93\u7684\u591a\u6837\u6027\u548c\u6807\u51c6\u5316\uff0c\u652f\u6301\u97e9\u8bed\u5b66\u4e60\u3001\u8bc4\u4f30\u548c\u81ea\u52a8\u7ea0\u9519\u7684\u7814\u7a76\u3002", "conclusion": "\u6539\u8fdb\u540e\u7684KoLLA\u6210\u4e3a\u97e9\u8bed\u4e8c\u8bed\u6559\u80b2\u7684\u5f3a\u6709\u529b\u8d44\u6e90\uff0c\u63a8\u52a8\u76f8\u5173\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2505.00495", "pdf": "https://arxiv.org/pdf/2505.00495", "abs": "https://arxiv.org/abs/2505.00495", "authors": ["Nguyen Van Thanh", "Nguyen Dang Huynh", "Nguyen Ngoc Tan", "Nguyen Thai Minh", "Nguyen Nam Hoang"], "title": "Enhancing Tropical Cyclone Path Forecasting with an Improved Transformer Network", "categories": ["cs.LG", "cs.PF"], "comment": null, "summary": "A storm is a type of extreme weather. Therefore, forecasting the path of a\nstorm is extremely important for protecting human life and property. However,\nstorm forecasting is very challenging because storm trajectories frequently\nchange. In this study, we propose an improved deep learning method using a\nTransformer network to predict the movement trajectory of a storm over the next\n6 hours. The storm data used to train the model was obtained from the National\nOceanic and Atmospheric Administration (NOAA) [1]. Simulation results show that\nthe proposed method is more accurate than traditional methods. Moreover, the\nproposed method is faster and more cost-effective", "AI": {"tldr": "\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528Transformer\u7f51\u7edc\u9884\u6d4b\u672a\u67656\u5c0f\u65f6\u7684\u98ce\u66b4\u79fb\u52a8\u8f68\u8ff9\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u51c6\u786e\u3001\u5feb\u901f\u4e14\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u98ce\u66b4\u8def\u5f84\u9884\u6d4b\u5bf9\u4fdd\u62a4\u4eba\u7c7b\u751f\u547d\u548c\u8d22\u4ea7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u8f68\u8ff9\u9891\u7e41\u53d8\u5316\uff0c\u9884\u6d4b\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eTransformer\u7f51\u7edc\u7684\u6539\u8fdb\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528NOAA\u63d0\u4f9b\u7684\u98ce\u66b4\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u51c6\u786e\u3001\u5feb\u901f\u4e14\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "\u6539\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u98ce\u66b4\u8def\u5f84\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.00268", "pdf": "https://arxiv.org/pdf/2505.00268", "abs": "https://arxiv.org/abs/2505.00268", "authors": ["Jekaterina Novikova", "Carol Anderson", "Borhane Blili-Hamelin", "Subhabrata Majumdar"], "title": "Consistency in Language Models: Current Landscape, Challenges, and Future Directions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The hallmark of effective language use lies in consistency -- expressing\nsimilar meanings in similar contexts and avoiding contradictions. While human\ncommunication naturally demonstrates this principle, state-of-the-art language\nmodels struggle to maintain reliable consistency across different scenarios.\nThis paper examines the landscape of consistency research in AI language\nsystems, exploring both formal consistency (including logical rule adherence)\nand informal consistency (such as moral and factual coherence). We analyze\ncurrent approaches to measure aspects of consistency, identify critical\nresearch gaps in standardization of definitions, multilingual assessment, and\nmethods to improve consistency. Our findings point to an urgent need for robust\nbenchmarks to measure and interdisciplinary approaches to ensure consistency in\nthe application of language models on domain-specific tasks while preserving\nthe utility and adaptability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86AI\u8bed\u8a00\u7cfb\u7edf\u4e2d\u4e00\u81f4\u6027\u7684\u73b0\u72b6\uff0c\u5305\u62ec\u5f62\u5f0f\u548c\u5185\u5bb9\u4e00\u81f4\u6027\uff0c\u5206\u6790\u4e86\u73b0\u6709\u6d4b\u91cf\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u9700\u8981\u66f4\u5f3a\u57fa\u51c6\u548c\u8de8\u5b66\u79d1\u65b9\u6cd5\u6765\u63d0\u5347\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u96be\u4ee5\u4fdd\u6301\u4e00\u81f4\u6027\uff0c\u800c\u4eba\u7c7b\u81ea\u7136\u6c9f\u901a\u5374\u80fd\u505a\u5230\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u63d0\u5347AI\u8bed\u8a00\u7cfb\u7edf\u7684\u5f62\u5f0f\u548c\u5185\u5bb9\u4e00\u81f4\u6027\uff0c\u5e76\u586b\u8865\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5206\u6790\u73b0\u6709\u7684\u5f62\u5f0f\u4e00\u81f4\u6027\uff08\u5982\u903b\u8f91\u89c4\u5219\u9075\u5faa\uff09\u548c\u5185\u5bb9\u4e00\u81f4\u6027\uff08\u5982\u9053\u5fb7\u548c\u4e8b\u5b9e\u8fde\u8d2f\u6027\uff09\u7684\u6d4b\u91cf\u65b9\u6cd5\uff0c\u8bc6\u522b\u7814\u7a76\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u76ee\u524d\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u5b9a\u4e49\u3001\u591a\u8bed\u8a00\u8bc4\u4f30\u548c\u63d0\u5347\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\uff0c\u4e9f\u9700\u66f4\u5f3a\u5927\u7684\u6d4b\u91cf\u57fa\u51c6\u548c\u8de8\u5b66\u79d1\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8bba\u6587\u547c\u5401\u5efa\u7acb\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u8de8\u5b66\u79d1\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u8bed\u8a00\u6a21\u578b\u5728\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u5176\u5b9e\u7528\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2505.00150", "pdf": "https://arxiv.org/pdf/2505.00150", "abs": "https://arxiv.org/abs/2505.00150", "authors": ["Minh-Hao Van", "Xintao Wu"], "title": "Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid evolution of social media has provided enhanced communication\nchannels for individuals to create online content, enabling them to express\ntheir thoughts and opinions. Multimodal memes, often utilized for playful or\nhumorous expressions with visual and textual elements, are sometimes misused to\ndisseminate hate speech against individuals or groups. While the detection of\nhateful memes is well-researched, developing effective methods to transform\nhateful content in memes remains a significant challenge. Leveraging the\npowerful generation and reasoning capabilities of Vision-Language Models\n(VLMs), we address the tasks of detecting and mitigating hateful content. This\npaper presents two key contributions: first, a definition-guided prompting\ntechnique for detecting hateful memes, and second, a unified framework for\nmitigating hateful content in memes, named UnHateMeme, which works by replacing\nhateful textual and/or visual components. With our definition-guided prompts,\nVLMs achieve impressive performance on hateful memes detection task.\nFurthermore, our UnHateMeme framework, integrated with VLMs, demonstrates a\nstrong capability to convert hateful memes into non-hateful forms that meet\nhuman-level criteria for hate speech and maintain multimodal coherence between\nimage and text. Through empirical experiments, we show the effectiveness of\nstate-of-the-art pretrained VLMs such as LLaVA, Gemini and GPT-4o on the\nproposed tasks, providing a comprehensive analysis of their respective\nstrengths and limitations for these tasks. This paper aims to shed light on\nimportant applications of VLMs for ensuring safe and respectful online\nenvironments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u5173\u952e\u8d21\u732e\uff1a\u4e00\u79cd\u57fa\u4e8e\u5b9a\u4e49\u5f15\u5bfc\u7684\u63d0\u793a\u6280\u672f\u7528\u4e8e\u68c0\u6d4b\u4ec7\u6068\u6a21\u56e0\uff0c\u53e6\u4e00\u79cd\u540d\u4e3aUnHateMeme\u7684\u7edf\u4e00\u6846\u67b6\u7528\u4e8e\u901a\u8fc7\u66ff\u6362\u4ec7\u6068\u6587\u672c\u548c/\u6216\u89c6\u89c9\u7ec4\u4ef6\u6765\u7f13\u89e3\u4ec7\u6068\u5185\u5bb9\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u4ec7\u6068\u6a21\u56e0\u7684\u6ee5\u7528\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u6d4b\u4ec7\u6068\u5185\u5bb9\u65b9\u9762\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u6709\u6548\u7f13\u89e3\u4ec7\u6068\u5185\u5bb9\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u7684\u751f\u6210\u548c\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51fa\u5b9a\u4e49\u5f15\u5bfc\u7684\u63d0\u793a\u6280\u672f\u548cUnHateMeme\u6846\u67b6\uff0c\u901a\u8fc7\u66ff\u6362\u4ec7\u6068\u7ec4\u4ef6\u6765\u8f6c\u5316\u5185\u5bb9\u3002", "result": "VLMs\u5728\u4ec7\u6068\u6a21\u56e0\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cUnHateMeme\u6846\u67b6\u80fd\u6709\u6548\u5c06\u4ec7\u6068\u6a21\u56e0\u8f6c\u5316\u4e3a\u975e\u4ec7\u6068\u5f62\u5f0f\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u6a21\u6001\u8fde\u8d2f\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660eVLMs\u5728\u786e\u4fdd\u5728\u7ebf\u73af\u5883\u5b89\u5168\u65b9\u9762\u5177\u6709\u91cd\u8981\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u5206\u6790\u4e86LLaVA\u3001Gemini\u548cGPT-4o\u7b49\u6a21\u578b\u7684\u4f18\u52a3\u52bf\u3002"}}
{"id": "2505.00503", "pdf": "https://arxiv.org/pdf/2505.00503", "abs": "https://arxiv.org/abs/2505.00503", "authors": ["Ke Jiang", "Wen Jiang", "Xiaoyang Tan"], "title": "Variational OOD State Correction for Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "The performance of Offline reinforcement learning is significantly impacted\nby the issue of state distributional shift, and out-of-distribution (OOD) state\ncorrection is a popular approach to address this problem. In this paper, we\npropose a novel method named Density-Aware Safety Perception (DASP) for OOD\nstate correction. Specifically, our method encourages the agent to prioritize\nactions that lead to outcomes with higher data density, thereby promoting its\noperation within or the return to in-distribution (safe) regions. To achieve\nthis, we optimize the objective within a variational framework that\nconcurrently considers both the potential outcomes of decision-making and their\ndensity, thus providing crucial contextual information for safe\ndecision-making. Finally, we validate the effectiveness and feasibility of our\nproposed method through extensive experimental evaluations on the offline\nMuJoCo and AntMaze suites.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDensity-Aware Safety Perception\uff08DASP\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u72b6\u6001\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u9f13\u52b1\u667a\u80fd\u4f53\u4f18\u5148\u9009\u62e9\u6570\u636e\u5bc6\u5ea6\u8f83\u9ad8\u7684\u52a8\u4f5c\uff0c\u63d0\u5347\u5176\u5728\u5b89\u5168\u533a\u57df\u5185\u7684\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u53d7\u72b6\u6001\u5206\u5e03\u504f\u79fb\u95ee\u9898\u5f71\u54cd\u663e\u8457\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7ea0\u6b63\u5206\u5e03\u5916\uff08OOD\uff09\u72b6\u6001\u6765\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "method": "\u91c7\u7528\u53d8\u5206\u6846\u67b6\u4f18\u5316\u76ee\u6807\uff0c\u540c\u65f6\u8003\u8651\u51b3\u7b56\u7ed3\u679c\u7684\u6f5c\u5728\u5f71\u54cd\u53ca\u5176\u6570\u636e\u5bc6\u5ea6\uff0c\u4ece\u800c\u4e3a\u5b89\u5168\u51b3\u7b56\u63d0\u4f9b\u5173\u952e\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u5728\u79bb\u7ebfMuJoCo\u548cAntMaze\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DASP\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u884c\u6027\u3002", "conclusion": "DASP\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u6570\u636e\u5bc6\u5ea6\u4e0e\u51b3\u7b56\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u5206\u5e03\u5916\u72b6\u6001\u4e0b\u7684\u5b89\u5168\u6027\u4e0e\u6027\u80fd\u3002"}}
{"id": "2505.00339", "pdf": "https://arxiv.org/pdf/2505.00339", "abs": "https://arxiv.org/abs/2505.00339", "authors": ["Antoun Yaacoub", "Sansiri Tarnpradab", "Phattara Khumprom", "Zainab Assaghir", "Lionel Prevost", "J\u00e9r\u00f4me Da-Rugna"], "title": "Enhancing AI-Driven Education: Integrating Cognitive Frameworks, Linguistic Feedback Analysis, and Ethical Considerations for Improved Content Generation", "categories": ["cs.CL", "cs.AI"], "comment": "This article will be presented in IJCNN 2025 \"AI Innovations for\n  Education: Transforming Teaching and Learning through Cutting-Edge\n  Technologies\" workshop", "summary": "Artificial intelligence (AI) is rapidly transforming education, presenting\nunprecedented opportunities for personalized learning and streamlined content\ncreation. However, realizing the full potential of AI in educational settings\nnecessitates careful consideration of the quality, cognitive depth, and ethical\nimplications of AI-generated materials. This paper synthesizes insights from\nfour related studies to propose a comprehensive framework for enhancing\nAI-driven educational tools. We integrate cognitive assessment frameworks\n(Bloom's Taxonomy and SOLO Taxonomy), linguistic analysis of AI-generated\nfeedback, and ethical design principles to guide the development of effective\nand responsible AI tools. We outline a structured three-phase approach\nencompassing cognitive alignment, linguistic feedback integration, and ethical\nsafeguards. The practical application of this framework is demonstrated through\nits integration into OneClickQuiz, an AI-powered Moodle plugin for quiz\ngeneration. This work contributes a comprehensive and actionable guide for\neducators, researchers, and developers aiming to harness AI's potential while\nupholding pedagogical and ethical standards in educational content generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u6846\u67b6\uff0c\u4ee5\u63d0\u5347AI\u6559\u80b2\u5de5\u5177\u7684\u8d28\u91cf\u548c\u4f26\u7406\u6807\u51c6\uff0c\u6574\u5408\u4e86\u8ba4\u77e5\u8bc4\u4f30\u3001\u8bed\u8a00\u5206\u6790\u548c\u4f26\u7406\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u4e00\u6b3eAI\u63d2\u4ef6\u5c55\u793a\u4e86\u5176\u5e94\u7528\u3002", "motivation": "AI\u5728\u6559\u80b2\u9886\u57df\u7684\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9700\u8981\u786e\u4fdd\u5176\u751f\u6210\u5185\u5bb9\u7684\u8d28\u91cf\u3001\u8ba4\u77e5\u6df1\u5ea6\u548c\u4f26\u7406\u5408\u89c4\u6027\u3002", "method": "\u7ed3\u5408Bloom\u548cSOLO\u8ba4\u77e5\u8bc4\u4f30\u6846\u67b6\u3001AI\u751f\u6210\u53cd\u9988\u7684\u8bed\u8a00\u5206\u6790\u53ca\u4f26\u7406\u8bbe\u8ba1\u539f\u5219\uff0c\u63d0\u51fa\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6\uff08\u8ba4\u77e5\u5bf9\u9f50\u3001\u8bed\u8a00\u53cd\u9988\u6574\u5408\u3001\u4f26\u7406\u4fdd\u969c\uff09\u3002", "result": "\u6846\u67b6\u5728AI\u63d2\u4ef6OneClickQuiz\u4e2d\u6210\u529f\u5e94\u7528\uff0c\u8bc1\u660e\u4e86\u5176\u53ef\u884c\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6559\u80b2\u8005\u3001\u7814\u7a76\u8005\u548c\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5168\u9762\u4e14\u53ef\u64cd\u4f5c\u7684\u6307\u5357\uff0c\u4ee5\u5728\u5229\u7528AI\u6f5c\u529b\u7684\u540c\u65f6\u786e\u4fdd\u6559\u80b2\u5185\u5bb9\u7684\u4f26\u7406\u548c\u6559\u5b66\u6807\u51c6\u3002"}}
{"id": "2505.00509", "pdf": "https://arxiv.org/pdf/2505.00509", "abs": "https://arxiv.org/abs/2505.00509", "authors": ["Jeremias Ferrao", "Luhan Mikaelson", "Keenan Pepper", "Natalia Perez-Campanero Antolin"], "title": "Self-Ablating Transformers: More Interpretability, Less Sparsity", "categories": ["cs.LG"], "comment": "Poster Presentation at Building Trust Workshop at ICLR 2025", "summary": "A growing intuition in machine learning suggests a link between sparsity and\ninterpretability. We introduce a novel self-ablation mechanism to investigate\nthis connection ante-hoc in the context of language transformers. Our approach\ndynamically enforces a k-winner-takes-all constraint, forcing the model to\ndemonstrate selective activation across neuron and attention units. Unlike\npost-hoc methods that analyze already-trained models, our approach integrates\ninterpretability directly into model training, promoting feature localization\nfrom inception. Training small models on the TinyStories dataset and employing\ninterpretability tests, we find that self-ablation leads to more localized\ncircuits, concentrated feature representations, and increased neuron\nspecialization without compromising language modelling performance.\nSurprisingly, our method also decreased overall sparsity, indicating that\nself-ablation promotes specialization rather than widespread inactivity. This\nreveals a complex interplay between sparsity and interpretability, where\ndecreased global sparsity can coexist with increased local specialization,\nleading to enhanced interpretability. To facilitate reproducibility, we make\nour code available at\nhttps://github.com/keenanpepper/self-ablating-transformers.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5f15\u5165\u81ea\u963b\u65ad\u673a\u5236\uff0c\u7814\u7a76\u7a00\u758f\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u53d1\u73b0\u81ea\u963b\u65ad\u65b9\u6cd5\u80fd\u589e\u5f3a\u6a21\u578b\u7684\u5c40\u90e8\u4e13\u4e1a\u5316\uff0c\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u800c\u65e0\u9700\u727a\u7272\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u4e2d\u7a00\u758f\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u6f5c\u5728\u8054\u7cfb\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u7ea6\u675f\u673a\u5236\u5c06\u5176\u76f4\u63a5\u878d\u5165\u6a21\u578b\u8bad\u7ec3\uff0c\u907f\u514d\u4e8b\u540e\u5206\u6790\u65b9\u6cd5\u7684\u6570\u636e\u5047\u8bbe\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u81ea\u963b\u65ad\u673a\u5236\uff0c\u52a8\u6001\u5b9e\u65bdk-\u8d62\u5bb6\u901a\u5403\u7ea6\u675f\uff0c\u5f3a\u5236\u6a21\u578b\u5728\u795e\u7ecf\u5143\u548c\u6ce8\u610f\u529b\u5355\u5143\u4e0a\u9009\u62e9\u6027\u6fc0\u6d3b\uff0c\u7ed3\u5408TinyStories\u6570\u636e\u96c6\u8bad\u7ec3\u5c0f\u6a21\u578b\u5e76\u6d4b\u8bd5\u3002", "result": "\u81ea\u963b\u65ad\u65b9\u6cd5\u4ea7\u751f\u4e86\u66f4\u5c40\u90e8\u5316\u7684\u7535\u8def\u3001\u96c6\u4e2d\u7684\u7279\u5f81\u8868\u793a\u548c\u589e\u5f3a\u7684\u795e\u7ecf\u5143\u4e13\u4e1a\u5316\uff0c\u4e14\u672a\u964d\u4f4e\u8bed\u8a00\u5efa\u6a21\u6027\u80fd\uff1b\u4f46\u610f\u5916\u51cf\u5c11\u4e86\u603b\u4f53\u7a00\u758f\u6027\u3002", "conclusion": "\u7a00\u758f\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u5b58\u5728\u590d\u6742\u5173\u7cfb\uff1a\u5168\u5c40\u7a00\u758f\u6027\u964d\u4f4e\u4e0e\u5c40\u90e8\u4e13\u4e1a\u5316\u63d0\u5347\u53ef\u5171\u5b58\uff0c\u4ece\u800c\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002\u7814\u7a76\u6210\u679c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.00367", "pdf": "https://arxiv.org/pdf/2505.00367", "abs": "https://arxiv.org/abs/2505.00367", "authors": ["JunSeo Kim", "HyeHyeon Kim"], "title": "KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Cognitive distortion refers to negative thinking patterns that can lead to\nmental health issues like depression and anxiety in adolescents. Previous\nstudies using natural language processing (NLP) have focused mainly on\nsmall-scale adult datasets, with limited research on adolescents. This study\nintroduces KoACD, the first large-scale dataset of cognitive distortions in\nKorean adolescents, containing 108,717 instances. We applied a multi-Large\nLanguage Model (LLM) negotiation method to refine distortion classification and\ngenerate synthetic data using two approaches: cognitive clarification for\ntextual clarity and cognitive balancing for diverse distortion representation.\nValidation through LLMs and expert evaluations showed that while LLMs\nclassified distortions with explicit markers, they struggled with\ncontext-dependent reasoning, where human evaluators demonstrated higher\naccuracy. KoACD aims to enhance future research on cognitive distortion\ndetection.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u97e9\u56fd\u9752\u5c11\u5e74\u8ba4\u77e5\u626d\u66f2\u6570\u636e\u96c6KoACD\uff0c\u91c7\u7528\u591aLLM\u534f\u5546\u65b9\u6cd5\u4f18\u5316\u5206\u7c7b\u5e76\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u9a8c\u8bc1\u663e\u793aLLMs\u5728\u663e\u6027\u6807\u8bb0\u5206\u7c7b\u8868\u73b0\u826f\u597d\u4f46\u8bed\u5883\u63a8\u7406\u4e0d\u8db3\u3002", "motivation": "\u9752\u5c11\u5e74\u8ba4\u77e5\u626d\u66f2\u5bf9\u5fc3\u7406\u5065\u5eb7\u5f71\u54cd\u91cd\u5927\uff0c\u4f46\u73b0\u6709NLP\u7814\u7a76\u591a\u57fa\u4e8e\u5c0f\u89c4\u6a21\u6210\u4eba\u6570\u636e\uff0c\u7f3a\u4e4f\u9488\u5bf9\u9752\u5c11\u5e74\u7684\u7814\u7a76\u3002", "method": "\u6784\u5efaKoACD\u6570\u636e\u96c6\uff08108,717\u6761\uff09\uff0c\u901a\u8fc7\u591aLLM\u534f\u5546\u65b9\u6cd5\uff08\u8ba4\u77e5\u6f84\u6e05\u4e0e\u8ba4\u77e5\u5e73\u8861\uff09\u4f18\u5316\u5206\u7c7b\u5e76\u751f\u6210\u5408\u6210\u6570\u636e\u3002", "result": "LLMs\u5bf9\u663e\u6027\u6807\u8bb0\u5206\u7c7b\u6709\u6548\uff0c\u4f46\u8bed\u5883\u4f9d\u8d56\u63a8\u7406\u80fd\u529b\u5f31\u4e8e\u4eba\u7c7b\u8bc4\u4f30\u8005\uff1bKoACD\u9a8c\u8bc1\u4e86\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "KoACD\u4e3a\u8ba4\u77e5\u626d\u66f2\u68c0\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8d44\u6e90\uff0c\u672a\u6765\u53ef\u7ed3\u5408\u4eba\u7c7b\u4e0eLLM\u4f18\u52bf\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.00530", "pdf": "https://arxiv.org/pdf/2505.00530", "abs": "https://arxiv.org/abs/2505.00530", "authors": ["Xinyu Wang", "Jinbo Bi", "Minghu Song"], "title": "Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in Reinforcement Learning Frameworks", "categories": ["cs.LG", "cs.CE", "q-bio.BM"], "comment": "17 pages, 5 main figures, 2 appendix figures. Submitted to ICML 2025", "summary": "SMILES-based molecule generation has emerged as a powerful approach in drug\ndiscovery. Deep reinforcement learning (RL) using large language model (LLM)\nhas been incorporated into the molecule generation process to achieve high\nmatching score in term of likelihood of desired molecule candidates. However, a\ncritical challenge in this approach is catastrophic forgetting during the RL\nphase, where knowledge such as molecule validity, which often exceeds 99\\%\nduring pretraining, significantly deteriorates. Current RL algorithms applied\nin drug discovery, such as REINVENT, use prior models as anchors to retian\npretraining knowledge, but these methods lack robust exploration mechanisms. To\naddress these issues, we propose Partial SMILES Validation-PPO (PSV-PPO), a\nnovel RL algorithm that incorporates real-time partial SMILES validation to\nprevent catastrophic forgetting while encouraging exploration. Unlike\ntraditional RL approaches that validate molecule structures only after\ngenerating entire sequences, PSV-PPO performs stepwise validation at each\nauto-regressive step, evaluating not only the selected token candidate but also\nall potential branches stemming from the prior partial sequence. This enables\nearly detection of invalid partial SMILES across all potential paths. As a\nresult, PSV-PPO maintains high validity rates even during aggressive\nexploration of the vast chemical space. Our experiments on the PMO and GuacaMol\nbenchmark datasets demonstrate that PSV-PPO significantly reduces the number of\ninvalid generated structures while maintaining competitive exploration and\noptimization performance. While our work primarily focuses on maintaining\nvalidity, the framework of PSV-PPO can be extended in future research to\nincorporate additional forms of valuable domain knowledge, further enhancing\nreinforcement learning applications in drug discovery.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPSV-PPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b9e\u65f6\u90e8\u5206SMILES\u9a8c\u8bc1\u89e3\u51b3\u5206\u5b50\u751f\u6210\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u63a2\u7d22\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5206\u5b50\u751f\u6210\u65b9\u6cd5\uff08\u5982REINVENT\uff09\u5728RL\u9636\u6bb5\u6613\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u63a2\u7d22\u673a\u5236\u3002", "method": "\u63d0\u51faPSV-PPO\u7b97\u6cd5\uff0c\u5728\u6bcf\u4e00\u6b65\u81ea\u56de\u5f52\u751f\u6210\u65f6\u8fdb\u884c\u90e8\u5206SMILES\u9a8c\u8bc1\uff0c\u63d0\u524d\u68c0\u6d4b\u65e0\u6548\u7ed3\u6784\u5206\u652f\uff0c\u907f\u514d\u65e0\u6548\u751f\u6210\u3002", "result": "\u5728PMO\u548cGuacaMol\u6570\u636e\u96c6\u4e0a\uff0cPSV-PPO\u663e\u8457\u51cf\u5c11\u65e0\u6548\u7ed3\u6784\u751f\u6210\uff0c\u4e14\u63a2\u7d22\u4e0e\u4f18\u5316\u6027\u80fd\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "PSV-PPO\u6846\u67b6\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u9886\u57df\u77e5\u8bc6\uff0c\u8fdb\u4e00\u6b65\u63a8\u52a8\u5f3a\u5316\u5b66\u4e60\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2505.00389", "pdf": "https://arxiv.org/pdf/2505.00389", "abs": "https://arxiv.org/abs/2505.00389", "authors": ["Bowen Zhang", "Zixin Song", "Chunping Li"], "title": "CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass", "categories": ["cs.CL"], "comment": "Accepted by SIGIR 2025 (Full)", "summary": "As a fundamental task in Information Retrieval and Computational Linguistics,\nsentence representation has profound implications for a wide range of practical\napplications such as text clustering, content analysis, question-answering\nsystems, and web search. Recent advances in pre-trained language models (PLMs)\nhave driven remarkable progress in this field, particularly through\nunsupervised embedding derivation methods centered on discriminative PLMs like\nBERT. However, due to time and computational constraints, few efforts have\nattempted to integrate unsupervised sentence representation with generative\nPLMs, which typically possess much larger parameter sizes. Given that\nstate-of-the-art models in both academia and industry are predominantly based\non generative architectures, there is a pressing need for an efficient\nunsupervised text representation framework tailored to decoder-only PLMs. To\naddress this concern, we propose CSE-SFP, an innovative method that exploits\nthe structural characteristics of generative models. Compared to existing\nstrategies, CSE-SFP requires only a single forward pass to perform effective\nunsupervised contrastive learning. Rigorous experimentation demonstrates that\nCSE-SFP not only produces higher-quality embeddings but also significantly\nreduces both training time and memory consumption. Furthermore, we introduce\ntwo ratio metrics that jointly assess alignment and uniformity, thereby\nproviding a more robust means for evaluating the semantic spatial properties of\nencoding models.", "AI": {"tldr": "\u63d0\u51fa\u4e86CSE-SFP\uff0c\u4e00\u79cd\u9488\u5bf9\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u65e0\u76d1\u7763\u6587\u672c\u8868\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5d4c\u5165\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u672f\u548c\u5de5\u4e1a\u754c\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9\u5176\u7684\u65e0\u76d1\u7763\u6587\u672c\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u751f\u6210\u5f0f\u6a21\u578b\u7684\u7ed3\u6784\u7279\u6027\uff0c\u5f00\u53d1CSE-SFP\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5b9e\u73b0\u5bf9\u6bd4\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCSE-SFP\u80fd\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u5d4c\u5165\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\uff0c\u5e76\u5f15\u5165\u65b0\u6307\u6807\u8bc4\u4f30\u8bed\u4e49\u7a7a\u95f4\u7279\u6027\u3002", "conclusion": "CSE-SFP\u4e3a\u751f\u6210\u5f0f\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u65e0\u76d1\u7763\u8868\u793a\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.00186", "pdf": "https://arxiv.org/pdf/2505.00186", "abs": "https://arxiv.org/abs/2505.00186", "authors": ["Rafael C. Pinto", "Anderson R. Tavares"], "title": "Neuroevolution of Self-Attention Over Proto-Objects", "categories": ["cs.NE", "cs.AI", "cs.CV"], "comment": "9 pages, 16 figures, GECCO", "summary": "Proto-objects - image regions that share common visual properties - offer a\npromising alternative to traditional attention mechanisms based on\nrectangular-shaped image patches in neural networks. Although previous work\ndemonstrated that evolving a patch-based hard-attention module alongside a\ncontroller network could achieve state-of-the-art performance in visual\nreinforcement learning tasks, our approach leverages image segmentation to work\nwith higher-level features. By operating on proto-objects rather than fixed\npatches, we significantly reduce the representational complexity: each image\ndecomposes into fewer proto-objects than regular patches, and each proto-object\ncan be efficiently encoded as a compact feature vector. This enables a\nsubstantially smaller self-attention module that processes richer semantic\ninformation. Our experiments demonstrate that this proto-object-based approach\nmatches or exceeds the state-of-the-art performance of patch-based\nimplementations with 62% less parameters and 2.6 times less training time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eproto-objects\uff08\u539f\u59cb\u5bf9\u8c61\uff09\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u53d6\u4ee3\u4f20\u7edf\u7684\u57fa\u4e8e\u77e9\u5f62\u56fe\u50cf\u5757\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u964d\u4f4e\u4e86\u8868\u793a\u590d\u6742\u5ea6\u5e76\u63d0\u5347\u4e86\u8bed\u4e49\u4fe1\u606f\u5904\u7406\u6548\u7387\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u4e14\u53c2\u6570\u51cf\u5c11\u4e8662%\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed\u4e862.6\u500d\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u77e9\u5f62\u56fe\u50cf\u5757\u7684\u6ce8\u610f\u529b\u673a\u5236\u5728\u8868\u793a\u590d\u6742\u5ea6\u4e0a\u8f83\u9ad8\uff0c\u800cproto-objects\u4f5c\u4e3a\u5171\u4eab\u89c6\u89c9\u5c5e\u6027\u7684\u56fe\u50cf\u533a\u57df\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u9ad8\u5c42\u6b21\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u4ece\u800c\u964d\u4f4e\u590d\u6742\u5ea6\u5e76\u63d0\u5347\u6548\u7387\u3002", "method": "\u901a\u8fc7\u56fe\u50cf\u5206\u5272\u63d0\u53d6proto-objects\uff0c\u5c06\u5176\u7f16\u7801\u4e3a\u7d27\u51d1\u7684\u7279\u5f81\u5411\u91cf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u66f4\u5c0f\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u6765\u5904\u7406\u8fd9\u4e9b\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8eproto-objects\u7684\u65b9\u6cd5\u5728\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u540c\u65f6\u53c2\u6570\u51cf\u5c11\u4e8662%\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed\u4e862.6\u500d\u3002", "conclusion": "proto-objects\u4f5c\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u7684\u57fa\u7840\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u8868\u793a\u590d\u6742\u5ea6\uff0c\u63d0\u5347\u8bed\u4e49\u4fe1\u606f\u5904\u7406\u6548\u7387\uff0c\u662f\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.00533", "pdf": "https://arxiv.org/pdf/2505.00533", "abs": "https://arxiv.org/abs/2505.00533", "authors": ["Linjing You", "Jiabao Lu", "Xiayuan Huang"], "title": "Test-time Correlation Alignment", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ICML2025", "summary": "Deep neural networks often experience performance drops due to distribution\nshifts between training and test data. Although domain adaptation offers a\nsolution, privacy concerns restrict access to training data in many real-world\nscenarios. This restriction has spurred interest in Test-Time Adaptation (TTA),\nwhich adapts models using only unlabeled test data. However, current TTA\nmethods still face practical challenges: (1) a primary focus on instance-wise\nalignment, overlooking CORrelation ALignment (CORAL) due to missing source\ncorrelations; (2) complex backpropagation operations for model updating,\nresulting in overhead computation and (3) domain forgetting.\n  To address these challenges, we provide a theoretical analysis to investigate\nthe feasibility of Test-time Correlation Alignment (TCA), demonstrating that\ncorrelation alignment between high-certainty instances and test instances can\nenhance test performances with a theoretical guarantee. Based on this, we\npropose two simple yet effective algorithms: LinearTCA and LinearTCA+.\nLinearTCA applies a simple linear transformation to achieve both instance and\ncorrelation alignment without additional model updates, while LinearTCA+ serves\nas a plug-and-play module that can easily boost existing TTA methods. Extensive\nexperiments validate our theoretical insights and show that TCA methods\nsignificantly outperforms baselines across various tasks, benchmarks and\nbackbones. Notably, LinearTCA improves adaptation accuracy by 5.88% on\nOfficeHome dataset, while using only 4% maximum GPU memory usage and 0.6%\ncomputation time compared to the best baseline TTA method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08TTA\uff09\u4e2d\u7684\u5b9e\u4f8b\u5bf9\u9f50\u548c\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u76f8\u5173\u5bf9\u9f50\u7684TCA\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4e24\u79cd\u7b97\u6cd5\uff08LinearTCA\u548cLinearTCA+\uff09\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u95ee\u9898\u65e0\u6cd5\u8bbf\u95ee\u8bad\u7ec3\u6570\u636e\u65f6\uff0c\u73b0\u6709TTA\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5b9e\u4f8b\u5bf9\u9f50\uff0c\u5ffd\u7565\u4e86\u76f8\u5173\u5bf9\u9f50\uff08CORAL\uff09\uff0c\u4e14\u6a21\u578b\u66f4\u65b0\u590d\u6742\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u57df\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Test-time Correlation Alignment (TCA)\uff0c\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u9ad8\u786e\u5b9a\u6027\u5b9e\u4f8b\u4e0e\u6d4b\u8bd5\u5b9e\u4f8b\u7684\u76f8\u5173\u5bf9\u9f50\u53ef\u63d0\u5347\u6027\u80fd\u3002\u5177\u4f53\u5b9e\u73b0\u4e3a\u4e24\u79cd\u7b97\u6cd5\uff1a\u4e0d\u9700\u989d\u5916\u6a21\u578b\u66f4\u65b0\u7684LinearTCA\uff08\u7ebf\u6027\u53d8\u6362\u5b9e\u73b0\u5b9e\u4f8b\u548c\u76f8\u5173\u5bf9\u9f50\uff09\uff0c\u4ee5\u53ca\u53ef\u589e\u5f3a\u73b0\u6709TTA\u65b9\u6cd5\u7684\u63d2\u4ef6\u6a21\u5757LinearTCA+\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTCA\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4f8b\u5982\u5728OfficeHome\u6570\u636e\u96c6\u4e0aLinearTCA\u5c06\u9002\u5e94\u51c6\u786e\u7387\u63d0\u53475.88%\uff0c\u540c\u65f6\u4ec5\u5360\u7528\u6700\u4f73\u57fa\u7ebfTTA\u65b9\u6cd54%\u7684GPU\u5185\u5b58\u548c0.6%\u7684\u8ba1\u7b97\u65f6\u95f4\u3002", "conclusion": "TCA\u901a\u8fc7\u76f8\u5173\u5bf9\u9f50\u89e3\u51b3\u4e86\u5b9e\u4f8b\u5bf9\u9f50\u7684\u5c40\u9650\u6027\uff0c\u517c\u987e\u6027\u80fd\u4e0e\u6548\u7387\uff0c\u4e3aTTA\u63d0\u4f9b\u4e86\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00467", "pdf": "https://arxiv.org/pdf/2505.00467", "abs": "https://arxiv.org/abs/2505.00467", "authors": ["Vahid Balazadeh", "Michael Cooper", "David Pellow", "Atousa Assadi", "Jennifer Bell", "Jim Fackler", "Gabriel Funingana", "Spencer Gable-Cook", "Anirudh Gangadhar", "Abhishek Jaiswal", "Sumanth Kaja", "Christopher Khoury", "Randy Lin", "Kaden McKeen", "Sara Naimimohasses", "Khashayar Namdar", "Aviraj Newatia", "Allan Pang", "Anshul Pattoo", "Sameer Peesapati", "Diana Prepelita", "Bogdana Rakova", "Saba Sadatamin", "Rafael Schulman", "Ajay Shah", "Syed Azhar Shah", "Syed Ahmar Shah", "Babak Taati", "Balagopal Unnikrishnan", "Stephanie Williams", "Rahul G Krishnan"], "title": "Red Teaming Large Language Models for Healthcare", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present the design process and findings of the pre-conference workshop at\nthe Machine Learning for Healthcare Conference (2024) entitled Red Teaming\nLarge Language Models for Healthcare, which took place on August 15, 2024.\nConference participants, comprising a mix of computational and clinical\nexpertise, attempted to discover vulnerabilities -- realistic clinical prompts\nfor which a large language model (LLM) outputs a response that could cause\nclinical harm. Red-teaming with clinicians enables the identification of LLM\nvulnerabilities that may not be recognised by LLM developers lacking clinical\nexpertise. We report the vulnerabilities found, categorise them, and present\nthe results of a replication study assessing the vulnerabilities across all\nLLMs provided.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u57282024\u5e74\u533b\u7597\u673a\u5668\u5b66\u4e60\u4f1a\u8bae\u4e0a\u7684\u9884\u7814\u8ba8\u4f1a\u6d3b\u52a8\uff0c\u901a\u8fc7\u4e34\u5e8a\u4e13\u5bb6\u4e0e\u8ba1\u7b97\u4e13\u5bb6\u7684\u534f\u4f5c\uff0c\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u573a\u666f\u4e2d\u7684\u6f0f\u6d1e\u3002", "motivation": "\u8bc6\u522b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u4e34\u5e8a\u98ce\u9669\uff0c\u8fd9\u4e9b\u6f0f\u6d1e\u53ef\u80fd\u56e0\u7f3a\u4e4f\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u800c\u88ab\u5f00\u53d1\u8005\u5ffd\u89c6\u3002", "method": "\u7ec4\u7ec7\u4e13\u5bb6\u56e2\u961f\uff08\u4e34\u5e8a\u4e0e\u8ba1\u7b97\u80cc\u666f\uff09\u901a\u8fc7\u6a21\u62df\u4e34\u5e8a\u63d0\u793a\u6d4b\u8bd5LLM\uff0c\u5e76\u8bc6\u522b\u5176\u53ef\u80fd\u9020\u6210\u4e34\u5e8a\u5371\u5bb3\u7684\u54cd\u5e94\u3002", "result": "\u6210\u529f\u53d1\u73b0\u5e76\u5206\u7c7b\u4e86LLM\u7684\u82e5\u5e72\u6f0f\u6d1e\uff0c\u5e76\u901a\u8fc7\u590d\u5236\u7814\u7a76\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6f0f\u6d1e\u5728\u4e0d\u540cLLM\u4e2d\u7684\u666e\u904d\u6027\u3002", "conclusion": "\u4e34\u5e8a\u4e13\u5bb6\u7684\u53c2\u4e0e\u5bf9\u8bc6\u522bLLM\u5728\u533b\u7597\u9886\u57df\u7684\u6f5c\u5728\u98ce\u9669\u81f3\u5173\u91cd\u8981\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u4ee5\u51cf\u5c11\u6b64\u7c7b\u6f0f\u6d1e\u3002"}}
{"id": "2505.00541", "pdf": "https://arxiv.org/pdf/2505.00541", "abs": "https://arxiv.org/abs/2505.00541", "authors": ["Amarpal Sahota", "Navid Mohammadi Foumani", "Raul Santos-Rodriguez", "Zahraa S. Abdallah"], "title": "KnowEEG: Explainable Knowledge Driven EEG Classification", "categories": ["cs.LG"], "comment": null, "summary": "Electroencephalography (EEG) is a method of recording brain activity that\nshows significant promise in applications ranging from disease classification\nto emotion detection and brain-computer interfaces. Recent advances in deep\nlearning have improved EEG classification performance yet model explainability\nremains an issue. To address this key limitation of explainability we introduce\nKnowEEG; a novel explainable machine learning approach for EEG classification.\nKnowEEG extracts a comprehensive set of per-electrode features, filters them\nusing statistical tests, and integrates between-electrode connectivity\nstatistics. These features are then input to our modified Random Forest model\n(Fusion Forest) that balances per electrode statistics with between electrode\nconnectivity features in growing the trees of the forest. By incorporating\nknowledge from both the generalized time-series and EEG-specific domains,\nKnowEEG achieves performance comparable to or exceeding state-of-the-art deep\nlearning models across five different classification tasks: emotion detection,\nmental workload classification, eyes open/closed detection, abnormal EEG\nclassification, and event detection. In addition to high performance, KnowEEG\nprovides inherent explainability through feature importance scores for\nunderstandable features. We demonstrate by example on the eyes closed/open\nclassification task that this explainability can be used to discover knowledge\nabout the classes. This discovered knowledge for eyes open/closed\nclassification was proven to be correct by current neuroscience literature.\nTherefore, the impact of KnowEEG will be significant for domains where EEG\nexplainability is critical such as healthcare.", "AI": {"tldr": "KnowEEG\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8eEEG\u5206\u7c7b\uff0c\u901a\u8fc7\u878d\u5408\u7535\u6781\u7279\u5f81\u548c\u8fde\u63a5\u6027\u7edf\u8ba1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u91cd\u8981\u6027\u3002", "motivation": "\u89e3\u51b3EEG\u5206\u7c7b\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u533b\u7597\u7b49\u9886\u57df\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9ad8\u6027\u80fdEEG\u5206\u6790\u5de5\u5177\u3002", "method": "\u63d0\u53d6\u7535\u6781\u7279\u5f81\uff0c\u901a\u8fc7\u7edf\u8ba1\u6d4b\u8bd5\u7b5b\u9009\uff0c\u7ed3\u5408\u7535\u6781\u95f4\u8fde\u63a5\u6027\u7edf\u8ba1\uff0c\u4f7f\u7528\u6539\u8fdb\u7684\u968f\u673a\u68ee\u6797\u6a21\u578b\uff08Fusion Forest\uff09\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u60c5\u7eea\u68c0\u6d4b\u3001\u8111\u529b\u8d1f\u8377\u5206\u7c7b\u7b49\u4e94\u9879\u4efb\u52a1\u4e2d\uff0cKnowEEG\u6027\u80fd\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u91cd\u8981\u6027\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "KnowEEG\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u5177\u5907\u53ef\u89e3\u91ca\u6027\uff0c\u80fd\u53d1\u73b0\u5e76\u9a8c\u8bc1\u795e\u7ecf\u79d1\u5b66\u77e5\u8bc6\uff0c\u5bf9EEG\u5173\u952e\u9886\u57df\u5982\u533b\u7597\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2505.00479", "pdf": "https://arxiv.org/pdf/2505.00479", "abs": "https://arxiv.org/abs/2505.00479", "authors": ["Gijs Jan Brandsma", "Jens Blom-Hansen", "Christiaan Meijer", "Kody Moodley"], "title": "Computational Identification of Regulatory Statements in EU Legislation", "categories": ["cs.CL", "I.2.7"], "comment": "11 pages, 6 figures", "summary": "Identifying regulatory statements in legislation is useful for developing\nmetrics to measure the regulatory density and strictness of legislation. A\ncomputational method is valuable for scaling the identification of such\nstatements from a growing body of EU legislation, constituting approximately\n180,000 published legal acts between 1952 and 2023. Past work on extraction of\nthese statements varies in the permissiveness of their definitions for what\nconstitutes a regulatory statement. In this work, we provide a specific\ndefinition for our purposes based on the institutional grammar tool. We develop\nand compare two contrasting approaches for automatically identifying such\nstatements in EU legislation, one based on dependency parsing, and the other on\na transformer-based machine learning model. We found both approaches performed\nsimilarly well with accuracies of 80% and 84% respectively and a K alpha of\n0.58. The high accuracies and not exceedingly high agreement suggests potential\nfor combining strengths of both approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\u6765\u81ea\u52a8\u8bc6\u522b\u6b27\u76df\u6cd5\u89c4\u4e2d\u7684\u76d1\u7ba1\u6761\u6b3e\uff0c\u57fa\u4e8e\u4f9d\u5b58\u89e3\u6790\u548cTransformer\u6a21\u578b\uff0c\u4e24\u8005\u51c6\u786e\u7387\u4e3a80%\u548c84%\uff0cK alpha\u503c\u4e3a0.58\u3002", "motivation": "\u8bc6\u522b\u6cd5\u89c4\u4e2d\u7684\u76d1\u7ba1\u6761\u6b3e\u5bf9\u8861\u91cf\u6cd5\u89c4\u5bc6\u5ea6\u548c\u4e25\u683c\u5ea6\u7684\u6307\u6807\u5f00\u53d1\u975e\u5e38\u6709\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5bf9\u4e0d\u540c\u5b9a\u4e49\u8fc7\u4e8e\u6a21\u7cca\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u660e\u786e\u7684\u5b9a\u4e49\u3002", "method": "\u4f7f\u7528\u4e86\u57fa\u4e8e\u4f9d\u5b58\u89e3\u6790\u548cTransformer\u6a21\u578b\u7684\u4e24\u79cd\u65b9\u6cd5\uff0c\u91c7\u7528\u673a\u6784\u8bed\u6cd5\u5de5\u5177\u660e\u786e\u76d1\u7ba1\u6761\u6b3e\u7684\u5b9a\u4e49\u3002", "result": "\u4e24\u79cd\u65b9\u6cd5\u8868\u73b0\u76f8\u8fd1\uff0c\u51c6\u786e\u7387\u5206\u522b\u4e3a80%\u548c84%\uff0cK alpha\u4e3a0.58\uff0c\u663e\u793a\u4e24\u8005\u7ed3\u5408\u6709\u6f5c\u529b\u3002", "conclusion": "\u4e24\u79cd\u65b9\u6cd5\u5404\u6709\u4f18\u52bf\uff0c\u672a\u6765\u53ef\u7ed3\u5408\u4f7f\u7528\u4ee5\u63d0\u9ad8\u8bc6\u522b\u6548\u679c\u3002"}}
{"id": "2505.00546", "pdf": "https://arxiv.org/pdf/2505.00546", "abs": "https://arxiv.org/abs/2505.00546", "authors": ["Qingyuan Wu", "Yuhui Wang", "Simon Sinong Zhan", "Yixuan Wang", "Chung-Wei Lin", "Chen Lv", "Qi Zhu", "J\u00fcrgen Schmidhuber", "Chao Huang"], "title": "Directly Forecasting Belief for Reinforcement Learning with Delays", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) with delays is challenging as sensory perceptions\nlag behind the actual events: the RL agent needs to estimate the real state of\nits environment based on past observations. State-of-the-art (SOTA) methods\ntypically employ recursive, step-by-step forecasting of states. This can cause\nthe accumulation of compounding errors. To tackle this problem, our novel\nbelief estimation method, named Directly Forecasting Belief Transformer (DFBT),\ndirectly forecasts states from observations without incrementally estimating\nintermediate states step-by-step. We theoretically demonstrate that DFBT\ngreatly reduces compounding errors of existing recursively forecasting methods,\nyielding stronger performance guarantees. In experiments with D4RL offline\ndatasets, DFBT reduces compounding errors with remarkable prediction accuracy.\nDFBT's capability to forecast state sequences also facilitates multi-step\nbootstrapping, thus greatly improving learning efficiency. On the MuJoCo\nbenchmark, our DFBT-based method substantially outperforms SOTA baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDFBT\u7684\u65b0\u65b9\u6cd5\uff0c\u76f4\u63a5\u9884\u6d4b\u72b6\u6001\u4ee5\u51cf\u5c11\u5ef6\u8fdfRL\u4e2d\u7684\u7d2f\u79ef\u8bef\u5dee\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u56e0\u5ef6\u8fdf\u5bfc\u81f4\u7684\u72b6\u6001\u4f30\u8ba1\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u4f20\u7edf\u9012\u5f52\u9884\u6d4b\u65b9\u6cd5\u6613\u4ea7\u751f\u8bef\u5dee\u653e\u5927\u3002", "method": "\u63d0\u51faDFBT\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u89c2\u6d4b\u9884\u6d4b\u72b6\u6001\uff0c\u907f\u514d\u9010\u6b65\u9012\u5f52\u9884\u6d4b\u4e2d\u95f4\u72b6\u6001\u3002", "result": "\u5728D4RL\u79bb\u7ebf\u6570\u636e\u96c6\u548cMuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDFBT\u663e\u8457\u51cf\u5c11\u8bef\u5dee\u5e76\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "DFBT\u6709\u6548\u964d\u4f4e\u4e86\u8bef\u5dee\u7d2f\u79ef\uff0c\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2505.00506", "pdf": "https://arxiv.org/pdf/2505.00506", "abs": "https://arxiv.org/abs/2505.00506", "authors": ["Deanna Emery", "Michael Goitia", "Freddie Vargus", "Iulia Neagu"], "title": "HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in high-stakes\ndomains, detecting hallucinated content$\\unicode{x2013}$text that is not\ngrounded in supporting evidence$\\unicode{x2013}$has become a critical\nchallenge. Existing benchmarks for hallucination detection are often\nsynthetically generated, narrowly focused on extractive question answering, and\nfail to capture the complexity of real-world scenarios involving multi-document\ncontexts and full-sentence outputs. We introduce the HalluMix Benchmark, a\ndiverse, task-agnostic dataset that includes examples from a range of domains\nand formats. Using this benchmark, we evaluate seven hallucination detection\nsystems$\\unicode{x2013}$both open and closed\nsource$\\unicode{x2013}$highlighting differences in performance across tasks,\ndocument lengths, and input representations. Our analysis highlights\nsubstantial performance disparities between short and long contexts, with\ncritical implications for real-world Retrieval Augmented Generation (RAG)\nimplementations. Quotient Detections achieves the best overall performance,\nwith an accuracy of 0.82 and an F1 score of 0.84.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86 HalluMix Benchmark\uff0c\u4e00\u4e2a\u591a\u6837\u5316\u7684\u3001\u4efb\u52a1\u65e0\u5173\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u5185\u5bb9\u68c0\u6d4b\u3002\u8bc4\u4f30\u4e86\u4e03\u79cd\u7cfb\u7edf\uff0c\u53d1\u73b0\u77ed\u548c\u957f\u4e0a\u4e0b\u6587\u95f4\u6027\u80fd\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5173\u952e\u9886\u57df\u7684\u90e8\u7f72\u589e\u591a\uff0c\u68c0\u6d4b\u5e7b\u89c9\u5185\u5bb9\uff08\u5373\u65e0\u8bc1\u636e\u652f\u6301\u7684\u6587\u672c\uff09\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002\u73b0\u6709\u57fa\u51c6\u591a\u4e3a\u5408\u6210\u3001\u5c40\u9650\u4e8e\u62bd\u53d6\u5f0f\u95ee\u7b54\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u573a\u666f\u7684\u590d\u6742\u6027\u3002", "method": "\u6784\u5efa HalluMix Benchmark\uff0c\u5305\u542b\u591a\u6837\u9886\u57df\u548c\u683c\u5f0f\u7684\u6837\u672c\uff1b\u8bc4\u4f30\u4e03\u79cd\u5e7b\u89c9\u68c0\u6d4b\u7cfb\u7edf\uff08\u5f00\u6e90\u548c\u95ed\u6e90\uff09\uff0c\u5206\u6790\u4efb\u52a1\u3001\u6587\u6863\u957f\u5ea6\u548c\u8f93\u5165\u8868\u793a\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u77ed\u548c\u957f\u4e0a\u4e0b\u6587\u95f4\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u5bf9\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5b9e\u65bd\u6709\u91cd\u8981\u542f\u793a\u3002Quotient Detections \u8868\u73b0\u6700\u4f73\uff08\u51c6\u786e\u7387 0.82\uff0cF1 \u5206\u6570 0.84\uff09\u3002", "conclusion": "HalluMix Benchmark \u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.00222", "pdf": "https://arxiv.org/pdf/2505.00222", "abs": "https://arxiv.org/abs/2505.00222", "authors": ["Peter Yichen Chen", "Pingchuan Ma", "Niklas Hagemann", "John Romanishin", "Wei Wang", "Daniela Rus", "Wojciech Matusik"], "title": "AI-Enhanced Automatic Design of Efficient Underwater Gliders", "categories": ["cs.RO", "cs.AI", "cs.GR", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "The development of novel autonomous underwater gliders has been hindered by\nlimited shape diversity, primarily due to the reliance on traditional design\ntools that depend heavily on manual trial and error. Building an automated\ndesign framework is challenging due to the complexities of representing glider\nshapes and the high computational costs associated with modeling complex\nsolid-fluid interactions. In this work, we introduce an AI-enhanced automated\ncomputational framework designed to overcome these limitations by enabling the\ncreation of underwater robots with non-trivial hull shapes. Our approach\ninvolves an algorithm that co-optimizes both shape and control signals,\nutilizing a reduced-order geometry representation and a differentiable\nneural-network-based fluid surrogate model. This end-to-end design workflow\nfacilitates rapid iteration and evaluation of hydrodynamic performance, leading\nto the discovery of optimal and complex hull shapes across various control\nsettings. We validate our method through wind tunnel experiments and swimming\npool gliding tests, demonstrating that our computationally designed gliders\nsurpass manually designed counterparts in terms of energy efficiency. By\naddressing challenges in efficient shape representation and neural fluid\nsurrogate models, our work paves the way for the development of highly\nefficient underwater gliders, with implications for long-range ocean\nexploration and environmental monitoring.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdAI\u589e\u5f3a\u7684\u81ea\u52a8\u5316\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u6c34\u4e0b\u673a\u5668\u4eba\u975e\u4f20\u7edf\u5916\u58f3\u5f62\u72b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5f62\u72b6\u548c\u63a7\u5236\u4fe1\u53f7\uff0c\u4ee5\u53ca\u5229\u7528\u964d\u9636\u51e0\u4f55\u8868\u793a\u548c\u57fa\u4e8e\u53ef\u5fae\u5206\u795e\u7ecf\u7f51\u7edc\u7684\u6d41\u4f53\u66ff\u4ee3\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6c34\u4e0b\u6ed1\u7fd4\u673a\u7684\u80fd\u6e90\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u6c34\u4e0b\u6ed1\u7fd4\u673a\u8bbe\u8ba1\u4e3b\u8981\u4f9d\u8d56\u624b\u5de5\u8bd5\u9519\uff0c\u5f62\u72b6\u591a\u6837\u6027\u6709\u9650\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u56fa\u6db2\u4ea4\u4e92\u5efa\u6a21\u8ba1\u7b97\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u81ea\u52a8\u5316\u8bbe\u8ba1\u6846\u67b6\u3002", "method": "\u91c7\u7528\u8054\u5408\u4f18\u5316\u7b97\u6cd5\uff0c\u7ed3\u5408\u964d\u9636\u51e0\u4f55\u8868\u793a\u548c\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6d41\u4f53\u66ff\u4ee3\u6a21\u578b\uff0c\u5b9e\u73b0\u5f62\u72b6\u4e0e\u63a7\u5236\u4fe1\u53f7\u7684\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u98ce\u6d1e\u548c\u6c34\u6c60\u5b9e\u9a8c\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8ba1\u7b97\u8bbe\u8ba1\u7684\u6ed1\u7fd4\u673a\u5728\u80fd\u6e90\u6548\u7387\u4e0a\u4f18\u4e8e\u624b\u52a8\u8bbe\u8ba1\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u6548\u6c34\u4e0b\u6ed1\u7fd4\u673a\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5bf9\u957f\u8ddd\u79bb\u6d77\u6d0b\u63a2\u7d22\u548c\u73af\u5883\u76d1\u6d4b\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2505.00580", "pdf": "https://arxiv.org/pdf/2505.00580", "abs": "https://arxiv.org/abs/2505.00580", "authors": ["Xinyu Ding", "Lexuan Chen", "Siyu Liao", "Zhongfeng Wang"], "title": "Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors", "categories": ["cs.LG"], "comment": "to appear in Proceedings of the 2025 International Joint Conference\n  on Artificial Intelligence (IJCAI-2025)", "summary": "Foundation models have achieved tremendous success in different domains.\nHowever, their huge computation and storage complexity make these models\ndifficult to fine-tune and also less applicable in practice. Recent study shows\ntraining in Fourier domain can be an effective fine-tuning method in terms of\nboth model performance and number of training parameters. In this work, we\npropose to further reduce the complexity by the factorization through the\nproduct of interleaved circulant and diagonal matrices. In addition, we address\nthe case of non-square fine-tuning weights by partitioning the circulant matrix\ninto blocks. Our method avoids the construction of weight change matrix and\nutilizes 1D fast Fourier transform (FFT) instead of 2D FFT. Experimental\nresults show that our method achieves similar or better performance across\nvarious tasks with much less floating-point operations (FLOPs) and the number\nof trainable parameters.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4ea4\u9519\u5faa\u73af\u548c\u5bf9\u89d2\u77e9\u9635\u5206\u89e3\u6765\u964d\u4f4e\u5085\u91cc\u53f6\u57df\u8bad\u7ec3\u590d\u6742\u5ea6\u7684\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u6d6e\u70b9\u8fd0\u7b97\u548c\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u867d\u7136\u5728\u4e0d\u540c\u9886\u57df\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u4f46\u5176\u5de8\u5927\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u590d\u6742\u5ea6\u4f7f\u5f97\u6a21\u578b\u96be\u4ee5\u5fae\u8c03\u4e14\u5b9e\u7528\u6027\u53d7\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4ea4\u9519\u5faa\u73af\u548c\u5bf9\u89d2\u77e9\u9635\u7684\u4e58\u79ef\u6765\u5206\u89e3\u5085\u91cc\u53f6\u57df\u7684\u8bad\u7ec3\u6743\u91cd\uff0c\u907f\u514d\u6784\u5efa\u6743\u91cd\u53d8\u5316\u77e9\u9635\uff0c\u5e76\u4f7f\u7528\u4e00\u7ef4\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\uff08FFT\uff09\u66ff\u4ee3\u4e8c\u7ef4FFT\uff0c\u540c\u65f6\u5904\u7406\u975e\u65b9\u5f62\u5fae\u8c03\u6743\u91cd\u7684\u60c5\u51b5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u76f8\u4f3c\u6216\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u6d6e\u70b9\u8fd0\u7b97\uff08FLOPs\uff09\u548c\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u6570\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u5085\u91cc\u53f6\u57df\u8bad\u7ec3\u7684\u590d\u6742\u5ea6\uff0c\u4e3a\u66f4\u9ad8\u6548\u7684\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.00551", "pdf": "https://arxiv.org/pdf/2505.00551", "abs": "https://arxiv.org/abs/2505.00551", "authors": ["Chong Zhang", "Yue Deng", "Xiang Lin", "Bin Wang", "Dianwen Ng", "Hai Ye", "Xingxuan Li", "Yao Xiao", "Zhanfeng Mo", "Qi Zhang", "Lidong Bing"], "title": "100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The recent development of reasoning language models (RLMs) represents a novel\nevolution in large language models. In particular, the recent release of\nDeepSeek-R1 has generated widespread social impact and sparked enthusiasm in\nthe research community for exploring the explicit reasoning paradigm of\nlanguage models. However, the implementation details of the released models\nhave not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero,\nDeepSeek-R1, and the distilled small models. As a result, many replication\nstudies have emerged aiming to reproduce the strong performance achieved by\nDeepSeek-R1, reaching comparable performance through similar training\nprocedures and fully open-source data resources. These works have investigated\nfeasible strategies for supervised fine-tuning (SFT) and reinforcement learning\nfrom verifiable rewards (RLVR), focusing on data preparation and method design,\nyielding various valuable insights. In this report, we provide a summary of\nrecent replication studies to inspire future research. We primarily focus on\nSFT and RLVR as two main directions, introducing the details for data\nconstruction, method design and training procedure of current replication\nstudies. Moreover, we conclude key findings from the implementation details and\nexperimental results reported by these studies, anticipating to inspire future\nresearch. We also discuss additional techniques of enhancing RLMs, highlighting\nthe potential of expanding the application scope of these models, and\ndiscussing the challenges in development. By this survey, we aim to help\nresearchers and developers of RLMs stay updated with the latest advancements,\nand seek to inspire new ideas to further enhance RLMs.", "AI": {"tldr": "\u62a5\u544a\u603b\u7ed3\u4e86\u8fd1\u671f\u5173\u4e8eDeepSeek-R1\u63a8\u7406\u8bed\u8a00\u6a21\u578b\uff08RLMs\uff09\u7684\u590d\u5236\u7814\u7a76\uff0c\u805a\u7126\u4e8e\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u4ece\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\uff0c\u63a2\u8ba8\u4e86\u6570\u636e\u6784\u5efa\u3001\u65b9\u6cd5\u8bbe\u8ba1\u548c\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5e76\u603b\u7ed3\u4e86\u5173\u952e\u53d1\u73b0\u4e0e\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "DeepSeek\u7cfb\u5217\u6a21\u578b\u7684\u5b9e\u73b0\u7ec6\u8282\u672a\u5b8c\u5168\u5f00\u6e90\uff0c\u5f15\u53d1\u4e86\u8bb8\u591a\u590d\u5236\u7814\u7a76\u4ee5\u91cd\u73b0\u5176\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u603b\u7ed3\u8fd9\u4e9b\u7814\u7a76\uff0c\u4ee5\u542f\u53d1\u672a\u6765RLMs\u7684\u53d1\u5c55\u3002", "method": "\u4e3b\u8981\u4eceSFT\u548cRLVR\u4e24\u4e2a\u65b9\u5411\uff0c\u5206\u6790\u73b0\u6709\u590d\u5236\u7814\u7a76\u7684\u6570\u636e\u6784\u5efa\u3001\u65b9\u6cd5\u8bbe\u8ba1\u548c\u8bad\u7ec3\u6d41\u7a0b\uff0c\u63d0\u70bc\u5173\u952e\u53d1\u73b0\u3002", "result": "\u603b\u7ed3\u51fa\u53ef\u884c\u7684SFT\u4e0eRLVR\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u589e\u5f3aRLMs\u7684\u6f5c\u5728\u6280\u672f\u65b9\u5411\u3002", "conclusion": "\u901a\u8fc7\u68b3\u7406\u590d\u5236\u7814\u7a76\u7684\u6210\u679c\u4e0e\u6311\u6218\uff0c\u4e3aRLMs\u7684\u672a\u6765\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u53c2\u8003\uff0c\u9f13\u52b1\u66f4\u591a\u5f00\u6e90\u534f\u4f5c\u4e0e\u521b\u65b0\u3002"}}
{"id": "2505.00590", "pdf": "https://arxiv.org/pdf/2505.00590", "abs": "https://arxiv.org/abs/2505.00590", "authors": ["Chengsen Wang", "Qi Qi", "Jingyu Wang", "Haifeng Sun", "Zirui Zhuang", "Jianxin Liao"], "title": "Unlocking the Potential of Linear Networks for Irregular Multivariate Time Series Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Time series forecasting holds significant importance across various\nindustries, including finance, transportation, energy, healthcare, and climate.\nDespite the widespread use of linear networks due to their low computational\ncost and effectiveness in modeling temporal dependencies, most existing\nresearch has concentrated on regularly sampled and fully observed multivariate\ntime series. However, in practice, we frequently encounter irregular\nmultivariate time series characterized by variable sampling intervals and\nmissing values. The inherent intra-series inconsistency and inter-series\nasynchrony in such data hinder effective modeling and forecasting with\ntraditional linear networks relying on static weights. To tackle these\nchallenges, this paper introduces a novel model named AiT. AiT utilizes an\nadaptive linear network capable of dynamically adjusting weights according to\nobservation time points to address intra-series inconsistency, thereby\nenhancing the accuracy of temporal dependencies modeling. Furthermore, by\nincorporating the Transformer module on variable semantics embeddings, AiT\nefficiently captures variable correlations, avoiding the challenge of\ninter-series asynchrony. Comprehensive experiments across four benchmark\ndatasets demonstrate the superiority of AiT, improving prediction accuracy by\n11% and decreasing runtime by 52% compared to existing state-of-the-art\nmethods.", "AI": {"tldr": "AiT\u6a21\u578b\u901a\u8fc7\u81ea\u9002\u5e94\u7ebf\u6027\u7f51\u7edc\u548cTransformer\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u4e0d\u89c4\u5219\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u7684\u5efa\u6a21\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u7387\u5e76\u964d\u4f4e\u4e86\u8fd0\u884c\u65f6\u95f4\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5e38\u5b58\u5728\u91c7\u6837\u95f4\u9694\u4e0d\u5747\u548c\u7f3a\u5931\u503c\u95ee\u9898\uff0c\u4f20\u7edf\u7ebf\u6027\u7f51\u7edc\u96be\u4ee5\u6709\u6548\u5efa\u6a21\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u81ea\u9002\u5e94\u7ebf\u6027\u7f51\u7edc\u52a8\u6001\u8c03\u6574\u6743\u91cd\u4ee5\u5904\u7406\u5e8f\u5217\u5185\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u5229\u7528Transformer\u6a21\u5757\u6355\u6349\u53d8\u91cf\u76f8\u5173\u6027\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cAiT\u7684\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u534711%\uff0c\u8fd0\u884c\u65f6\u95f4\u51cf\u5c1152%\u3002", "conclusion": "AiT\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u89c4\u5219\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u7684\u5efa\u6a21\u548c\u9884\u6d4b\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.00557", "pdf": "https://arxiv.org/pdf/2505.00557", "abs": "https://arxiv.org/abs/2505.00557", "authors": ["Makoto Sato"], "title": "Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucinations in large language models (LLMs) present a growing challenge\nacross real-world applications, from healthcare to law, where factual\nreliability is essential. Despite advances in alignment and instruction tuning,\nLLMs can still generate outputs that are fluent yet fundamentally untrue.\nUnderstanding the cognitive dynamics that underlie these hallucinations remains\nan open problem. In this study, we propose a prompt-based framework to\nsystematically trigger and quantify hallucination: a Hallucination-Inducing\nPrompt (HIP), which synthetically fuses semantically distant concepts (e.g.,\nperiodic table of elements and tarot divination) in a misleading way, and a\nHallucination Quantifying Prompt (HQP), which scores the plausibility,\nconfidence, and coherence of the output. Controlled experiments across multiple\nLLMs revealed that HIPs consistently produced less coherent and more\nhallucinated responses than their null-fusion controls. These effects varied\nacross models, with reasoning-oriented LLMs showing distinct profiles from\ngeneral-purpose ones. Our framework provides a reproducible testbed for\nstudying hallucination vulnerability, and opens the door to developing safer,\nmore introspective LLMs that can detect and self-regulate the onset of\nconceptual instability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u63d0\u793a\u7684\u6846\u67b6\uff08HIP\u548cHQP\uff09\u6765\u7cfb\u7edf\u89e6\u53d1\u548c\u91cf\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5728\u5e7b\u89c9\u53cd\u5e94\u4e0a\u7684\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff08\u5373\u751f\u6210\u6d41\u7545\u4f46\u4e0d\u771f\u5b9e\u7684\u5185\u5bb9\uff09\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u4e8b\u5b9e\u53ef\u9760\u6027\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u63d0\u793a\uff1a\u5e7b\u89c9\u8bf1\u5bfc\u63d0\u793a\uff08HIP\uff09\u901a\u8fc7\u8bef\u5bfc\u6027\u878d\u5408\u8bed\u4e49\u758f\u79bb\u7684\u6982\u5ff5\u89e6\u53d1\u5e7b\u89c9\uff0c\u5e7b\u89c9\u91cf\u5316\u63d0\u793a\uff08HQP\uff09\u8bc4\u4f30\u8f93\u51fa\u7684\u5408\u7406\u6027\u3001\u7f6e\u4fe1\u5ea6\u548c\u8fde\u8d2f\u6027\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0HIP\u751f\u6210\u7684\u54cd\u5e94\u6bd4\u5bf9\u7167\u7ec4\u66f4\u4e0d\u8fde\u8d2f\u4e14\u66f4\u5177\u5e7b\u89c9\u6027\uff0c\u4e14\u4e0d\u540c\u6a21\u578b\uff08\u63a8\u7406\u5bfc\u5411\u578b\u4e0e\u901a\u7528\u578b\uff09\u8868\u73b0\u51fa\u5dee\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7814\u7a76\u5e7b\u89c9\u8106\u5f31\u6027\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u4e3a\u5f00\u53d1\u80fd\u81ea\u6211\u68c0\u6d4b\u548c\u8c03\u8282\u6982\u5ff5\u4e0d\u7a33\u5b9a\u6027\u7684\u66f4\u5b89\u5168LLM\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.00591", "pdf": "https://arxiv.org/pdf/2505.00591", "abs": "https://arxiv.org/abs/2505.00591", "authors": ["Ziqi Li"], "title": "Explainable AI in Spatial Analysis", "categories": ["cs.LG", "econ.EM"], "comment": null, "summary": "This chapter discusses the opportunities of eXplainable Artificial\nIntelligence (XAI) within the realm of spatial analysis. A key objective in\nspatial analysis is to model spatial relationships and infer spatial processes\nto generate knowledge from spatial data, which has been largely based on\nspatial statistical methods. More recently, machine learning offers scalable\nand flexible approaches that complement traditional methods and has been\nincreasingly applied in spatial data science. Despite its advantages, machine\nlearning is often criticized for being a black box, which limits our\nunderstanding of model behavior and output. Recognizing this limitation, XAI\nhas emerged as a pivotal field in AI that provides methods to explain the\noutput of machine learning models to enhance transparency and understanding.\nThese methods are crucial for model diagnosis, bias detection, and ensuring the\nreliability of results obtained from machine learning models. This chapter\nintroduces key concepts and methods in XAI with a focus on Shapley value-based\napproaches, which is arguably the most popular XAI method, and their\nintegration with spatial analysis. An empirical example of county-level voting\nbehaviors in the 2020 Presidential election is presented to demonstrate the use\nof Shapley values and spatial analysis with a comparison to multi-scale\ngeographically weighted regression. The chapter concludes with a discussion on\nthe challenges and limitations of current XAI techniques and proposes new\ndirections.", "AI": {"tldr": "\u672c\u7ae0\u63a2\u8ba8\u4e86\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u5728\u7a7a\u95f4\u5206\u6790\u4e2d\u7684\u673a\u9047\uff0c\u91cd\u70b9\u5173\u6ce8Shapley\u503c\u65b9\u6cd5\u53ca\u5176\u4e0e\u7a7a\u95f4\u5206\u6790\u7684\u7ed3\u5408\uff0c\u5e76\u901a\u8fc72020\u5e74\u7f8e\u56fd\u603b\u7edf\u9009\u4e3e\u7684\u5b9e\u8bc1\u5206\u6790\u5c55\u793a\u4e86\u5176\u5e94\u7528\u3002", "motivation": "\u7a7a\u95f4\u5206\u6790\u7684\u6838\u5fc3\u76ee\u6807\u662f\u901a\u8fc7\u5efa\u6a21\u7a7a\u95f4\u5173\u7cfb\u548c\u63a8\u65ad\u7a7a\u95f4\u8fc7\u7a0b\u4ece\u7a7a\u95f4\u6570\u636e\u4e2d\u751f\u6210\u77e5\u8bc6\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7a7a\u95f4\u7edf\u8ba1\uff0c\u800c\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7075\u6d3b\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u673a\u5668\u5b66\u4e60\u7684\u201c\u9ed1\u7bb1\u201d\u7279\u6027\u9650\u5236\u4e86\u5bf9\u5176\u8f93\u51fa\u548c\u884c\u4e3a\u7684\u7406\u89e3\uff0c\u56e0\u6b64\u9700\u8981XAI\u65b9\u6cd5\u589e\u5f3a\u900f\u660e\u6027\u3002", "method": "\u4ecb\u7ecd\u4e86XAI\u7684\u5173\u952e\u6982\u5ff5\u548c\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u57fa\u4e8eShapley\u503c\u7684\u89e3\u91ca\u6280\u672f\uff0c\u5e76\u5c06\u5176\u4e0e\u7a7a\u95f4\u5206\u6790\u7ed3\u5408\u3002\u901a\u8fc72020\u5e74\u7f8e\u56fd\u603b\u7edf\u9009\u4e3e\u7684\u53bf\u7ea7\u6295\u7968\u884c\u4e3a\u5b9e\u8bc1\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u4e0e\u591a\u5c3a\u5ea6\u5730\u7406\u52a0\u6743\u56de\u5f52\u7684\u6bd4\u8f83\u3002", "result": "\u8bc1\u660e\u4e86Shapley\u503c\u5728\u7a7a\u95f4\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u589e\u5f3a\u6a21\u578b\u8f93\u51fa\u900f\u660e\u6027\uff0c\u5e76\u652f\u6301\u6a21\u578b\u8bca\u65ad\u548c\u504f\u5dee\u68c0\u6d4b\u3002", "conclusion": "\u5c3d\u7ba1XAI\u6280\u672f\uff08\u5982Shapley\u503c\uff09\u5728\u7a7a\u95f4\u5206\u6790\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4ecd\u5b58\u5728\u6311\u6218\u548c\u5c40\u9650\u6027\uff0c\u672a\u6765\u9700\u63a2\u7d22\u65b0\u65b9\u5411\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2505.00570", "pdf": "https://arxiv.org/pdf/2505.00570", "abs": "https://arxiv.org/abs/2505.00570", "authors": ["Jushi Kai", "Boyi Zeng", "Yixuan Wang", "Haoli Bai", "Bo Jiang", "Zhouhan Lin"], "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4e0a\u4e0b\u6587\u6269\u5c55\u65b9\u6cd5FreqKV\uff0c\u901a\u8fc7\u5728\u9891\u57df\u4e2d\u8fc7\u6ee4\u9ad8\u9891\u5206\u91cf\u6765\u538b\u7f29KV\u7f13\u5b58\uff0c\u4ece\u800c\u4f18\u5316\u957f\u4e0a\u4e0b\u6587LLM\u7684\u5fae\u8c03\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u6269\u5c55\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u5728\u751f\u6210\u957f\u7bc7\u5185\u5bb9\u65f6\u5f88\u91cd\u8981\uff0c\u4f46\u7ebf\u6027\u589e\u957f\u7684KV\u7f13\u5b58\u9700\u6c42\u4e0e\u81ea\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u5e26\u6765\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u6269\u5c55\u5230\u957f\u4e0a\u4e0b\u6587\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5229\u7528KV\u7f13\u5b58\u7684\u80fd\u91cf\u4e3b\u8981\u96c6\u4e2d\u5728\u4f4e\u9891\u5206\u91cf\u7684\u7279\u6027\uff0c\u63d0\u51faFreqKV\u65b9\u6cd5\u5728\u9891\u57df\u4e2d\u8fed\u4ee3\u538b\u7f29KV\u7f13\u5b58\u81f3\u56fa\u5b9a\u5927\u5c0f\uff0c\u65e0\u9700\u989d\u5916\u53c2\u6570\u6216\u67b6\u6784\u4fee\u6539\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFreqKV\u5728\u591a\u9879\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u4e0e\u7406\u89e3\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "FreqKV\u901a\u8fc7\u9891\u57df\u538b\u7f29KV\u7f13\u5b58\uff0c\u4ee5\u6700\u5c0f\u4fe1\u606f\u635f\u5931\u5b9e\u73b0\u957f\u4e0a\u4e0b\u6587\u9ad8\u6548\u6269\u5c55\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8c03\u6574\u3002"}}
{"id": "2505.00240", "pdf": "https://arxiv.org/pdf/2505.00240", "abs": "https://arxiv.org/abs/2505.00240", "authors": ["Yazan Otoum", "Arghavan Asad", "Amiya Nayak"], "title": "LLM-Based Threat Detection and Prevention Framework for IoT Ecosystems", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.LG"], "comment": "Preprint version; submitted for academic peer review", "summary": "The increasing complexity and scale of the Internet of Things (IoT) have made\nsecurity a critical concern. This paper presents a novel Large Language Model\n(LLM)-based framework for comprehensive threat detection and prevention in IoT\nenvironments. The system integrates lightweight LLMs fine-tuned on IoT-specific\ndatasets (IoT-23, TON_IoT) for real-time anomaly detection and automated,\ncontext-aware mitigation strategies optimized for resource-constrained devices.\nA modular Docker-based deployment enables scalable and reproducible evaluation\nacross diverse network conditions. Experimental results in simulated IoT\nenvironments demonstrate significant improvements in detection accuracy,\nresponse latency, and resource efficiency over traditional security methods.\nThe proposed framework highlights the potential of LLM-driven, autonomous\nsecurity solutions for future IoT ecosystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u65b0\u578b\u7269\u8054\u7f51\u5b89\u5168\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u5a01\u80c1\u68c0\u6d4b\u4e0e\u81ea\u52a8\u5316\u7f13\u89e3\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u548c\u8d44\u6e90\u6548\u7387\u3002", "motivation": "\u7269\u8054\u7f51\u89c4\u6a21\u6269\u5927\u5bfc\u81f4\u5b89\u5168\u5a01\u80c1\u589e\u52a0\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\uff0c\u9700\u66f4\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7LLM\uff0c\u57fa\u4e8eIoT\u6570\u636e\u96c6\uff08IoT-23/TON_IoT\uff09\u5fae\u8c03\uff0c\u7ed3\u5408\u6a21\u5757\u5316Docker\u90e8\u7f72\u5b9e\u73b0\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u4e0e\u8d44\u6e90\u4f18\u5316\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u3001\u54cd\u5e94\u5ef6\u8fdf\u548c\u8d44\u6e90\u6548\u7387\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u81ea\u4e3b\u5b89\u5168\u6846\u67b6\u4e3a\u672a\u6765\u7269\u8054\u7f51\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00598", "pdf": "https://arxiv.org/pdf/2505.00598", "abs": "https://arxiv.org/abs/2505.00598", "authors": ["Haozheng Luo", "Chenghao Qiu", "Maojiang Su", "Zhihan Zhou", "Zoe Mehta", "Guo Ye", "Jerry Yao-Chieh Hu", "Han Liu"], "title": "Fast and Low-Cost Genomic Foundation Models via Outlier Removal", "categories": ["cs.LG", "cs.AI"], "comment": "International Conference on Machine Learning (ICML) 2025", "summary": "We propose the first unified adversarial attack benchmark for Genomic\nFoundation Models (GFMs), named GERM. Unlike existing GFM benchmarks, GERM\noffers the first comprehensive evaluation framework to systematically assess\nthe vulnerability of GFMs to adversarial attacks. Methodologically, we evaluate\nthe adversarial robustness of five state-of-the-art GFMs using four widely\nadopted attack algorithms and three defense strategies. Importantly, our\nbenchmark provides an accessible and comprehensive framework to analyze GFM\nvulnerabilities with respect to model architecture, quantization schemes, and\ntraining datasets. Empirically, transformer-based models exhibit greater\nrobustness to adversarial perturbations compared to HyenaDNA, highlighting the\nimpact of architectural design on vulnerability. Moreover, adversarial attacks\nfrequently target biologically significant genomic regions, suggesting that\nthese models effectively capture meaningful sequence features.", "AI": {"tldr": "GERM\u662f\u9996\u4e2a\u9488\u5bf9\u57fa\u56e0\u7ec4\u57fa\u7840\u6a21\u578b\uff08GFMs\uff09\u7684\u7edf\u4e00\u5bf9\u6297\u653b\u51fb\u57fa\u51c6\uff0c\u63d0\u4f9b\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5206\u6790\u4e86\u4e94\u79cdGFMs\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u53d1\u73b0Transformer\u67b6\u6784\u66f4\u7a33\u5065\u3002", "motivation": "\u73b0\u6709GFM\u57fa\u51c6\u7f3a\u4e4f\u7edf\u4e00\u7684\u5bf9\u6297\u653b\u51fb\u8bc4\u4f30\uff0cGERM\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7cfb\u7edf\u8bc4\u4f30GFMs\u7684\u8106\u5f31\u6027\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\u3002", "method": "\u91c7\u7528\u56db\u79cd\u653b\u51fb\u7b97\u6cd5\u548c\u4e09\u79cd\u9632\u5fa1\u7b56\u7565\uff0c\u8bc4\u4f30\u4e94\u79cdGFMs\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u5206\u6790\u6a21\u578b\u67b6\u6784\u3001\u91cf\u5316\u65b9\u6848\u548c\u8bad\u7ec3\u6570\u636e\u7684\u5f71\u54cd\u3002", "result": "Transformer\u67b6\u6784\u7684GFMs\u5bf9\u5bf9\u6297\u6270\u52a8\u66f4\u7a33\u5065\uff1b\u653b\u51fb\u5e38\u9488\u5bf9\u751f\u7269\u5b66\u91cd\u8981\u7684\u57fa\u56e0\u7ec4\u533a\u57df\uff0c\u8868\u660e\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u5e8f\u5217\u7279\u5f81\u3002", "conclusion": "GERM\u4e3aGFMs\u7684\u5bf9\u6297\u8106\u5f31\u6027\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u67b6\u6784\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8bc1\u5b9e\u6a21\u578b\u80fd\u5b66\u4e60\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u7279\u5f81\u3002"}}
{"id": "2505.00582", "pdf": "https://arxiv.org/pdf/2505.00582", "abs": "https://arxiv.org/abs/2505.00582", "authors": ["Xinyu Ding", "Meiqi Wang", "Siyu Liao", "Zhongfeng Wang"], "title": "Block Circulant Adapter for Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "to appear in Proceedings of the 2025 International Joint Conference\n  on Artificial Intelligence (IJCAI-2025)", "summary": "Fine-tuning large language models (LLMs) is difficult due to their huge model\nsize. Recent Fourier domain-based methods show potential for reducing\nfine-tuning costs. We propose a block circulant matrix-based fine-tuning method\nwith a stable training heuristic to leverage the properties of circulant\nmatrices and one-dimensional Fourier transforms to reduce storage and\ncomputation costs. Experiments show that our method uses $14\\times$ less number\nof parameters than VeRA, $16\\times$ smaller than LoRA and $32\\times$ less FLOPs\nthan FourierFT, while maintaining close or better task performance. Our\napproach presents a promising way in frequency domain to fine-tune large models\non downstream tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5757\u5faa\u73af\u77e9\u9635\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e00\u7ef4\u5085\u91cc\u53f6\u53d8\u6362\u964d\u4f4e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5b58\u50a8\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u6027\u80fd\u63a5\u8fd1\u6216\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u56e0\u6a21\u578b\u89c4\u6a21\u5e9e\u5927\u800c\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u5085\u91cc\u53f6\u57df\u65b9\u6cd5\u867d\u5177\u6f5c\u529b\u4f46\u4ecd\u6709\u4f18\u5316\u7a7a\u95f4\uff0c\u4e3a\u6b64\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5757\u5faa\u73af\u77e9\u9635\u7ed3\u5408\u4e00\u7ef4\u5085\u91cc\u53f6\u53d8\u6362\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u914d\u5408\u7a33\u5b9a\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u964d\u4f4e\u5b58\u50a8\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u53c2\u6570\u6570\u91cf\u6bd4VeRA\u51cf\u5c1114\u500d\u3001\u6bd4LoRA\u51cf\u5c1116\u500d\uff0cFLOPs\u6bd4FourierFT\u51cf\u5c1132\u500d\uff0c\u4e14\u4efb\u52a1\u6027\u80fd\u63a5\u8fd1\u6216\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9891\u57df\u5fae\u8c03\u5927\u578b\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00254", "pdf": "https://arxiv.org/pdf/2505.00254", "abs": "https://arxiv.org/abs/2505.00254", "authors": ["Yuxuan Yan", "Shiqi Jiang", "Ting Cao", "Yifan Yang", "Qianqian Yang", "Yuanchao Shu", "Yuqing Yang", "Lili Qiu"], "title": "Empowering Agentic Video Analytics Systems with Video Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages", "summary": "AI-driven video analytics has become increasingly pivotal across diverse\ndomains. However, existing systems are often constrained to specific,\npredefined tasks, limiting their adaptability in open-ended analytical\nscenarios. The recent emergence of Video-Language Models (VLMs) as\ntransformative technologies offers significant potential for enabling\nopen-ended video understanding, reasoning, and analytics. Nevertheless, their\nlimited context windows present challenges when processing ultra-long video\ncontent, which is prevalent in real-world applications. To address this, we\nintroduce AVA, a VLM-powered system designed for open-ended, advanced video\nanalytics. AVA incorporates two key innovations: (1) the near real-time\nconstruction of Event Knowledge Graphs (EKGs) for efficient indexing of long or\ncontinuous video streams, and (2) an agentic retrieval-generation mechanism\nthat leverages EKGs to handle complex and diverse queries. Comprehensive\nevaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that\nAVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,\nrespectively, significantly surpassing existing VLM and video\nRetrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video\nanalytics in ultra-long and open-world video scenarios, we introduce a new\nbenchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours\nin duration, along with 120 manually annotated, diverse, and complex\nquestion-answer pairs. On AVA-100, AVA achieves top-tier performance with an\naccuracy of 75.8%.", "AI": {"tldr": "AVA\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u6784\u5efa\u4e8b\u4ef6\u77e5\u8bc6\u56fe\uff08EKG\uff09\u548c\u4ee3\u7406\u68c0\u7d22\u751f\u6210\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u8d85\u957f\u89c6\u9891\u5185\u5bb9\u5206\u6790\u7684\u6311\u6218\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5206\u6790\u7cfb\u7edf\u5c40\u9650\u4e8e\u9884\u5b9a\u4e49\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5f00\u653e\u6027\u5206\u6790\u80fd\u529b\u3002\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u7a97\u53e3\u957f\u5ea6\uff0c\u96be\u4ee5\u5904\u7406\u8d85\u957f\u89c6\u9891\u5185\u5bb9\u3002", "method": "AVA\u91c7\u7528\u4e24\u79cd\u521b\u65b0\u6280\u672f\uff1a(1)\u6784\u5efa\u4e8b\u4ef6\u77e5\u8bc6\u56fe\uff08EKG\uff09\u5b9e\u73b0\u957f\u89c6\u9891\u9ad8\u6548\u7d22\u5f15\uff0c(2)\u57fa\u4e8eEKG\u7684\u4ee3\u7406\u68c0\u7d22\u751f\u6210\u673a\u5236\u652f\u6301\u590d\u6742\u67e5\u8be2\u3002", "result": "\u5728LVBench\u548cVideoMME-Long\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAVA\u5206\u522b\u8fbe\u523062.3%\u548c64.1%\u7684\u51c6\u786e\u7387\uff1b\u5728AVS-100\uff08\u65b0\u63d0\u51fa\u7684\u8d85\u957f\u89c6\u9891\u57fa\u51c6\uff09\u4e0a\uff0c\u51c6\u786e\u7387\u4e3a75.8%\u3002", "conclusion": "AVA\u901a\u8fc7\u7ed3\u5408EKG\u548c\u4ee3\u7406\u68c0\u7d22\u751f\u6210\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u73af\u5883\u4e0b\u8d85\u957f\u89c6\u9891\u5206\u6790\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u89c6\u9891\u5206\u6790\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.00650", "pdf": "https://arxiv.org/pdf/2505.00650", "abs": "https://arxiv.org/abs/2505.00650", "authors": ["Atahan Karagoz"], "title": "OmicsCL: Unsupervised Contrastive Learning for Cancer Subtype Discovery and Survival Stratification", "categories": ["cs.LG", "cs.AI", "q-bio.GN", "q-bio.QM"], "comment": "Code available at: https://github.com/Atahanka/OmicsCL", "summary": "Unsupervised learning of disease subtypes from multi-omics data presents a\nsignificant opportunity for advancing personalized medicine. We introduce\nOmicsCL, a modular contrastive learning framework that jointly embeds\nheterogeneous omics modalities-such as gene expression, DNA methylation, and\nmiRNA expression-into a unified latent space. Our method incorporates a\nsurvival-aware contrastive loss that encourages the model to learn\nrepresentations aligned with survival-related patterns, without relying on\nlabeled outcomes. Evaluated on the TCGA BRCA dataset, OmicsCL uncovers\nclinically meaningful clusters and achieves strong unsupervised concordance\nwith patient survival. The framework demonstrates robustness across\nhyperparameter configurations and can be tuned to prioritize either subtype\ncoherence or survival stratification. Ablation studies confirm that integrating\nsurvival-aware loss significantly enhances the predictive power of learned\nembeddings. These results highlight the promise of contrastive objectives for\nbiological insight discovery in high-dimensional, heterogeneous omics data.", "AI": {"tldr": "OmicsCL\u662f\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u6574\u5408\u591a\u7ec4\u5b66\u6570\u636e\u5e76\u53d1\u73b0\u4e0e\u751f\u5b58\u76f8\u5173\u7684\u75be\u75c5\u4e9a\u578b\u3002", "motivation": "\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u4ece\u591a\u7ec4\u5b66\u6570\u636e\u4e2d\u8bc6\u522b\u75be\u75c5\u4e9a\u578b\uff0c\u4ee5\u63a8\u52a8\u4e2a\u6027\u5316\u533b\u7597\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u751f\u5b58\u611f\u77e5\u7684\u5bf9\u6bd4\u635f\u5931\uff0c\u5c06\u591a\u7ec4\u5b66\u6570\u636e\u5d4c\u5165\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5728TCGA BRCA\u6570\u636e\u96c6\u4e2d\uff0cOmicsCL\u53d1\u73b0\u4e86\u4e34\u5e8a\u76f8\u5173\u7684\u805a\u7c7b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4e0e\u60a3\u8005\u751f\u5b58\u7684\u9ad8\u4e00\u81f4\u6027\u3002", "conclusion": "\u5bf9\u6bd4\u5b66\u4e60\u5728\u591a\u7ec4\u5b66\u6570\u636e\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u589e\u5f3a\u751f\u7269\u5b66\u89c1\u89e3\u7684\u53d1\u73b0\u80fd\u529b\u3002"}}
{"id": "2505.00624", "pdf": "https://arxiv.org/pdf/2505.00624", "abs": "https://arxiv.org/abs/2505.00624", "authors": ["Chaitali Bhattacharyya", "Yeseong Kim"], "title": "FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Training large language models (LLMs) from scratch requires significant\ncomputational resources, driving interest in developing smaller,\ndomain-specific LLMs that maintain both efficiency and strong task performance.\nMedium-sized models such as LLaMA, llama} have served as starting points for\ndomain-specific adaptation, but they often suffer from accuracy degradation\nwhen tested on specialized datasets. We introduce FineScope, a framework for\nderiving compact, domain-optimized LLMs from larger pretrained models.\nFineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its\nability to produce interpretable feature representations, to extract\ndomain-specific subsets from large datasets. We apply structured pruning with\ndomain-specific constraints, ensuring that the resulting pruned models retain\nessential knowledge for the target domain. To further enhance performance,\nthese pruned models undergo self-data distillation, leveraging SAE-curated\ndatasets to restore key domain-specific information lost during pruning.\nExtensive experiments and ablation studies demonstrate that FineScope achieves\nhighly competitive performance, outperforming several large-scale\nstate-of-the-art LLMs in domain-specific tasks. Additionally, our results show\nthat FineScope enables pruned models to regain a substantial portion of their\noriginal performance when fine-tuned with SAE-curated datasets. Furthermore,\napplying these datasets to fine-tune pretrained LLMs without pruning also\nimproves their domain-specific accuracy, highlighting the robustness of our\napproach. The code will be released.", "AI": {"tldr": "\u63d0\u51faFineScope\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u548c\u7ed3\u6784\u5316\u526a\u679d\uff0c\u4ece\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u4e2d\u63d0\u53d6\u9886\u57df\u4f18\u5316\u7684\u5c0f\u6a21\u578b\uff0c\u7ed3\u5408\u81ea\u6570\u636e\u84b8\u998f\u6062\u590d\u6027\u80fd\uff0c\u5728\u9886\u57df\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u79c0\u3002", "motivation": "\u5927\u6a21\u578b\u8bad\u7ec3\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u9886\u57df\u4e13\u7528\u5c0f\u6a21\u578b\u9700\u517c\u987e\u6548\u7387\u4e0e\u6027\u80fd\u3002\u73b0\u6709\u4e2d\u578b\u6a21\u578b\uff08\u5982LLaMA\uff09\u5728\u9886\u57df\u6570\u636e\u4e0a\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u4f7f\u7528SAE\u63d0\u53d6\u9886\u57df\u7279\u5f81\uff0c\u7ed3\u6784\u5316\u526a\u679d\u4fdd\u7559\u5173\u952e\u77e5\u8bc6\uff0c\u81ea\u6570\u636e\u84b8\u998f\u7528SAE\u6570\u636e\u96c6\u6062\u590d\u526a\u679d\u635f\u5931\u7684\u4fe1\u606f\u3002", "result": "FineScope\u5728\u9886\u57df\u4efb\u52a1\u4e2d\u4f18\u4e8e\u591a\u4e2a\u5927\u6a21\u578b\uff0c\u526a\u679d\u540e\u6a21\u578b\u901a\u8fc7SAE\u6570\u636e\u96c6\u5fae\u8c03\u53ef\u6062\u590d\u6027\u80fd\uff0c\u672a\u526a\u679d\u6a21\u578b\u5fae\u8c03\u540e\u7cbe\u5ea6\u4e5f\u63d0\u5347\u3002", "conclusion": "FineScope\u9ad8\u6548\u751f\u6210\u9886\u57df\u4f18\u5316\u5c0f\u6a21\u578b\uff0c\u65b9\u6cd5\u9c81\u68d2\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2505.00259", "pdf": "https://arxiv.org/pdf/2505.00259", "abs": "https://arxiv.org/abs/2505.00259", "authors": ["Changjun Li", "Runqing Jiang", "Zhuo Song", "Pengpeng Yu", "Ye Zhang", "Yulan Guo"], "title": "Pack-PTQ: Advancing Post-training Quantization of Neural Networks by Pack-wise Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Post-training quantization (PTQ) has evolved as a prominent solution for\ncompressing complex models, which advocates a small calibration dataset and\navoids end-to-end retraining. However, most existing PTQ methods employ\nblock-wise reconstruction, which neglects cross-block dependency and exhibits a\nnotable accuracy drop in low-bit cases. To address these limitations, this\npaper presents a novel PTQ method, dubbed Pack-PTQ. First, we design a\nHessian-guided adaptive packing mechanism to partition blocks into\nnon-overlapping packs, which serve as the base unit for reconstruction, thereby\npreserving the cross-block dependency and enabling accurate quantization\nparameters estimation. Second, based on the pack configuration, we propose a\nmixed-precision quantization approach to assign varied bit-widths to packs\naccording to their distinct sensitivities, thereby further enhancing\nperformance. Extensive experiments on 2D image and 3D point cloud\nclassification tasks, using various network architectures, demonstrate the\nsuperiority of our method over the state-of-the-art PTQ methods.", "AI": {"tldr": "Pack-PTQ \u662f\u4e00\u79cd\u65b0\u9896\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7Hessian\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u5206\u7ec4\u673a\u5236\u4fdd\u7559\u8de8\u5757\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709PTQ\u65b9\u6cd5\u5ffd\u89c6\u4e86\u8de8\u5757\u4f9d\u8d56\u6027\uff0c\u5728\u4f4e\u6bd4\u7279\u60c5\u51b5\u4e0b\u7cbe\u5ea6\u4e0b\u964d\u660e\u663e\u3002", "method": "\u8bbe\u8ba1\u4e86Hessian\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u5206\u7ec4\u673a\u5236\uff0c\u5c06\u5757\u5212\u5206\u4e3a\u4e0d\u91cd\u53e0\u7684\u5206\u7ec4\u4f5c\u4e3a\u91cd\u5efa\u57fa\u7840\uff1b\u63d0\u51fa\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\uff0c\u4e3a\u4e0d\u540c\u654f\u611f\u5ea6\u7684\u5206\u7ec4\u5206\u914d\u4e0d\u540c\u6bd4\u7279\u5bbd\u5ea6\u3002", "result": "\u57282D\u56fe\u50cf\u548c3D\u70b9\u4e91\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u91c7\u7528\u4e0d\u540c\u7f51\u7edc\u67b6\u6784\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPack-PTQ\u4f18\u4e8e\u73b0\u6709PTQ\u65b9\u6cd5\u3002", "conclusion": "Pack-PTQ\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u5757\u4f9d\u8d56\u6027\u95ee\u9898\u548c\u4f4e\u6bd4\u7279\u7cbe\u5ea6\u4e0b\u964d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5316\u6027\u80fd\u3002"}}
{"id": "2505.00663", "pdf": "https://arxiv.org/pdf/2505.00663", "abs": "https://arxiv.org/abs/2505.00663", "authors": ["David Pfau", "Ian Davies", "Diana Borsa", "Joao G. M. Araujo", "Brendan Tracey", "Hado van Hasselt"], "title": "Wasserstein Policy Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "We introduce Wasserstein Policy Optimization (WPO), an actor-critic algorithm\nfor reinforcement learning in continuous action spaces. WPO can be derived as\nan approximation to Wasserstein gradient flow over the space of all policies\nprojected into a finite-dimensional parameter space (e.g., the weights of a\nneural network), leading to a simple and completely general closed-form update.\nThe resulting algorithm combines many properties of deterministic and classic\npolicy gradient methods. Like deterministic policy gradients, it exploits\nknowledge of the gradient of the action-value function with respect to the\naction. Like classic policy gradients, it can be applied to stochastic policies\nwith arbitrary distributions over actions -- without using the\nreparameterization trick. We show results on the DeepMind Control Suite and a\nmagnetic confinement fusion task which compare favorably with state-of-the-art\ncontinuous control methods.", "AI": {"tldr": "Wasserstein Policy Optimization (WPO) \u662f\u4e00\u79cd\u7528\u4e8e\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u548c\u7ecf\u5178\u7b56\u7565\u68af\u5ea6\u7684\u4f18\u70b9\uff0c\u65e0\u9700\u91cd\u53c2\u6570\u5316\u6280\u5de7\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u83b7\u5f97\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u901a\u7528\u6027\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u91cd\u53c2\u6570\u5316\u6280\u5de7\u7684\u9650\u5236\uff0cWPO\u5f15\u5165Wasserstein\u68af\u5ea6\u6d41\u7684\u8fd1\u4f3c\u4f18\u5316\u7b56\u7565\u3002", "method": "WPO\u901a\u8fc7\u5c06\u7b56\u7565\u7a7a\u95f4\u6295\u5f71\u5230\u6709\u9650\u7ef4\u53c2\u6570\u7a7a\u95f4\uff08\u5982\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\uff09\uff0c\u63a8\u5bfc\u51fa\u5c01\u95ed\u5f0f\u66f4\u65b0\u516c\u5f0f\uff0c\u65e2\u80fd\u5229\u7528\u52a8\u4f5c\u503c\u51fd\u6570\u7684\u68af\u5ea6\uff0c\u53c8\u9002\u7528\u4e8e\u4efb\u4f55\u968f\u673a\u7b56\u7565\u3002", "result": "\u5728DeepMind Control Suite\u548c\u78c1\u7ea6\u675f\u805a\u53d8\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWPO\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8fde\u7eed\u63a7\u5236\u65b9\u6cd5\u3002", "conclusion": "WPO\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u901a\u7528\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u786e\u5b9a\u6027\u548c\u968f\u673a\u7b56\u7565\u68af\u5ea6\u7684\u4f18\u52bf\uff0c\u5c55\u73b0\u4e86\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.00626", "pdf": "https://arxiv.org/pdf/2505.00626", "abs": "https://arxiv.org/abs/2505.00626", "authors": ["Zihao Wang", "Yibo Jiang", "Jiahao Yu", "Heqing Huang"], "title": "The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)", "categories": ["cs.CL", "cs.AI", "68T50", "I.2"], "comment": null, "summary": "Large language models (LLMs) that integrate multiple input roles (e.g.,\nsystem instructions, user queries, external tool outputs) are increasingly\nprevalent in practice. Ensuring that the model accurately distinguishes\nmessages from each role -- a concept we call \\emph{role separation} -- is\ncrucial for consistent multi-role behavior. Although recent work often targets\nstate-of-the-art prompt injection defenses, it remains unclear whether such\nmethods truly teach LLMs to differentiate roles or merely memorize known\ntriggers. In this paper, we examine \\emph{role-separation learning}: the\nprocess of teaching LLMs to robustly distinguish system and user tokens.\nThrough a \\emph{simple, controlled experimental framework}, we find that\nfine-tuned models often rely on two proxies for role identification: (1) task\ntype exploitation, and (2) proximity to begin-of-text. Although data\naugmentation can partially mitigate these shortcuts, it generally leads to\niterative patching rather than a deeper fix. To address this, we propose\nreinforcing \\emph{invariant signals} that mark role boundaries by adjusting\ntoken-wise cues in the model's input encoding. In particular, manipulating\nposition IDs helps the model learn clearer distinctions and reduces reliance on\nsuperficial proxies. By focusing on this mechanism-centered perspective, our\nwork illuminates how LLMs can more reliably maintain consistent multi-role\nbehavior without merely memorizing known prompts or triggers.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7`\u89d2\u8272\u5206\u79bb\u5b66\u4e60`\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u66f4\u53ef\u9760\u5730\u533a\u5206\u7cfb\u7edf\u6307\u4ee4\u548c\u7528\u6237\u67e5\u8be2\u7684\u89d2\u8272\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u8c03\u6574\u8f93\u5165\u7f16\u7801\u4e2d\u7684`\u4e0d\u53d8\u4fe1\u53f7`\uff08\u5982\u4f4d\u7f6eID\uff09\u6765\u51cf\u5c11\u5bf9\u8868\u9762\u4ee3\u7406\u7684\u4f9d\u8d56\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u4ec5\u4f9d\u8d56\u4e8e\u8bb0\u5fc6\u5df2\u77e5\u89e6\u53d1\u5668\u800c\u975e\u771f\u6b63\u533a\u5206\u89d2\u8272\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u65b9\u5f0f\u6765\u786e\u4fddLLM\u5728\u591a\u89d2\u8272\u573a\u666f\u4e2d\u884c\u4e3a\u4e00\u81f4\u3002", "method": "\u901a\u8fc7\u7b80\u5355\u7684\u63a7\u5236\u5b9e\u9a8c\u6846\u67b6\u5206\u6790\u5fae\u8c03\u6a21\u578b\u7684\u89d2\u8272\u8bc6\u522b\u4ee3\u7406\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u8c03\u6574\u8f93\u5165\u7f16\u7801\uff08\u5982\u4f4d\u7f6eID\uff09\u6765\u5f3a\u5316\u89d2\u8272\u8fb9\u754c\u7684\u4e0d\u53d8\u4fe1\u53f7\u3002", "result": "\u53d1\u73b0\u5fae\u8c03\u6a21\u578b\u4f9d\u8d56\u4efb\u52a1\u7c7b\u578b\u548c\u6587\u672c\u5f00\u5934\u4f4d\u7f6e\u7b49\u8868\u9762\u4ee3\u7406\uff0c\u6570\u636e\u589e\u5f3a\u4ec5\u80fd\u90e8\u5206\u7f13\u89e3\u95ee\u9898\uff0c\u800c\u8c03\u6574\u8f93\u5165\u7f16\u7801\uff08\u5982\u4f4d\u7f6eID\uff09\u80fd\u66f4\u6709\u6548\u5730\u63d0\u5347\u89d2\u8272\u5206\u79bb\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u673a\u5236\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89d2\uff0c\u63d0\u51fa\u4e86\u66f4\u53ef\u9760\u7684LLM\u591a\u89d2\u8272\u884c\u4e3a\u7ef4\u62a4\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u5bf9\u8868\u9762\u4ee3\u7406\u7684\u4f9d\u8d56\u3002"}}
{"id": "2505.00681", "pdf": "https://arxiv.org/pdf/2505.00681", "abs": "https://arxiv.org/abs/2505.00681", "authors": ["Arsha Nagrani", "Sachit Menon", "Ahmet Iscen", "Shyamal Buch", "Ramin Mehran", "Nilpa Jha", "Anja Hauth", "Yukun Zhu", "Carl Vondrick", "Mikhail Sirotenko", "Cordelia Schmid", "Tobias Weyand"], "title": "MINERVA: Evaluating Complex Video Reasoning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Multimodal LLMs are turning their focus to video benchmarks, however most\nvideo benchmarks only provide outcome supervision, with no intermediate or\ninterpretable reasoning steps. This makes it challenging to assess if models\nare truly able to combine perceptual and temporal information to reason about\nvideos, or simply get the correct answer by chance or by exploiting linguistic\nbiases. To remedy this, we provide a new video reasoning dataset called MINERVA\nfor modern multimodal models. Each question in the dataset comes with 5 answer\nchoices, as well as detailed, hand-crafted reasoning traces. Our dataset is\nmultimodal, diverse in terms of video domain and length, and consists of\ncomplex multi-step questions. Extensive benchmarking shows that our dataset\nprovides a challenge for frontier open-source and proprietary models. We\nperform fine-grained error analysis to identify common failure modes across\nvarious models, and create a taxonomy of reasoning errors. We use this to\nexplore both human and LLM-as-a-judge methods for scoring video reasoning\ntraces, and find that failure modes are primarily related to temporal\nlocalization, followed by visual perception errors, as opposed to logical or\ncompleteness errors. The dataset, along with questions, answer candidates and\nreasoning traces will be publicly available under\nhttps://github.com/google-deepmind/neptune?tab=readme-ov-file\\#minerva.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u9891\u63a8\u7406\u6570\u636e\u96c6MINERVA\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u662f\u5426\u771f\u6b63\u7ed3\u5408\u611f\u77e5\u548c\u65f6\u95f4\u4fe1\u606f\u8fdb\u884c\u89c6\u9891\u63a8\u7406\uff0c\u800c\u4e0d\u4ec5\u4ec5\u4f9d\u8d56\u8bed\u8a00\u504f\u5dee\u6216\u5076\u7136\u6b63\u786e\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u57fa\u51c6\u5927\u591a\u53ea\u63d0\u4f9b\u7ed3\u679c\u76d1\u7763\uff0c\u7f3a\u4e4f\u4e2d\u95f4\u6216\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u96be\u4ee5\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u89c6\u9891\u5185\u5bb9\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b5\u4e2a\u7b54\u6848\u9009\u9879\u548c\u8be6\u7ec6\u624b\u5de5\u63a8\u7406\u8f68\u8ff9\u7684\u591a\u6a21\u6001\u89c6\u9891\u63a8\u7406\u6570\u636e\u96c6MINERVA\uff0c\u6db5\u76d6\u591a\u6837\u89c6\u9891\u9886\u57df\u548c\u590d\u6742\u591a\u6b65\u95ee\u9898\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u8868\u660eMINERVA\u5bf9\u524d\u6cbf\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\u6784\u6210\u6311\u6218\uff0c\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e3b\u8981\u5931\u8d25\u6a21\u5f0f\u4e3a\u65f6\u95f4\u5b9a\u4f4d\u548c\u89c6\u89c9\u611f\u77e5\u9519\u8bef\u3002", "conclusion": "MINERVA\u6570\u636e\u96c6\u586b\u8865\u4e86\u89c6\u9891\u63a8\u7406\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u6a21\u578b\u80fd\u529b\u5206\u6790\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u5de5\u5177\u3002"}}
{"id": "2505.00654", "pdf": "https://arxiv.org/pdf/2505.00654", "abs": "https://arxiv.org/abs/2505.00654", "authors": ["Daniel N. Nissani"], "title": "Large Language Models Understanding: an Inherent Ambiguity Barrier", "categories": ["cs.CL", "cs.AI"], "comment": "submitted to NEURAL COMPUTATION", "summary": "A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean.", "AI": {"tldr": "\u672c\u6587\u56f4\u7ed5\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u662f\u5426\u80fd\u7406\u89e3\u5bf9\u8bdd\u5185\u5bb9\u7684\u4e89\u8bba\uff0c\u901a\u8fc7\u601d\u60f3\u5b9e\u9a8c\u548c\u534a\u6b63\u5f0f\u5206\u6790\uff0c\u63d0\u51faLLM\u56e0\u56fa\u6709\u6b67\u4e49\u969c\u788d\u65e0\u6cd5\u771f\u6b63\u7406\u89e3\u5bf9\u8bdd\u3002", "motivation": "\u63a2\u8ba8LLM\u662f\u5426\u5177\u5907\u7406\u89e3\u5bf9\u8bdd\u610f\u4e49\u7684\u80fd\u529b\uff0c\u56de\u5e94\u5f53\u524d\u5173\u4e8e\u5176\u7406\u89e3\u80fd\u529b\u7684\u4e89\u8bba\u3002", "method": "\u91c7\u7528\u601d\u60f3\u5b9e\u9a8c\u548c\u534a\u6b63\u5f0f\u5206\u6790\uff0c\u8bba\u8bc1LLM\u5b58\u5728\u56fa\u6709\u6b67\u4e49\u969c\u788d\u3002", "result": "\u63d0\u51faLLM\u56e0\u6b67\u4e49\u969c\u788d\u65e0\u6cd5\u771f\u6b63\u7406\u89e3\u5bf9\u8bdd\u5185\u5bb9\uff0c\u4ec5\u80fd\u751f\u6210\u6d41\u7545\u4f46\u65e0\u610f\u4e49\u7684\u56de\u7b54\u3002", "conclusion": "LLM\u7684\u5bf9\u8bdd\u80fd\u529b\u867d\u5f3a\uff0c\u4f46\u56e0\u56fa\u6709\u969c\u788d\u65e0\u6cd5\u8fbe\u5230\u771f\u6b63\u7684\u8bed\u4e49\u7406\u89e3\u3002"}}
{"id": "2505.00284", "pdf": "https://arxiv.org/pdf/2505.00284", "abs": "https://arxiv.org/abs/2505.00284", "authors": ["Zhijie Qiao", "Haowei Li", "Zhong Cao", "Henry X. Liu"], "title": "LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated significant potential for\nend-to-end autonomous driving. However, fully exploiting their capabilities for\nsafe and reliable vehicle control remains an open research challenge. To\nsystematically examine advances and limitations of VLMs in driving tasks, we\nintroduce LightEMMA, a Lightweight End-to-End Multimodal Model for Autonomous\ndriving. LightEMMA provides a unified, VLM-based autonomous driving framework\nwithout ad hoc customizations, enabling easy integration and evaluation of\nevolving state-of-the-art commercial and open-source models. We construct\ntwelve autonomous driving agents using various VLMs and evaluate their\nperformance on the nuScenes prediction task, comprehensively assessing metrics\nsuch as inference time, computational cost, and predictive accuracy.\nIllustrative examples highlight that, despite their strong scenario\ninterpretation capabilities, VLMs' practical performance in autonomous driving\ntasks remains concerning, emphasizing the need for further improvements. The\ncode is available at https://github.com/michigan-traffic-lab/LightEMMA.", "AI": {"tldr": "LightEMMA \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u591a\u6a21\u6001\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u5b89\u5168\u53ef\u9760\u8f66\u8f86\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u548c\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86 LightEMMA \u6846\u67b6\uff0c\u5229\u7528\u591a\u79cd VLMs \u6784\u5efa\u81ea\u52a8\u9a7e\u9a76\u4ee3\u7406\uff0c\u5e76\u5728 nuScenes \u9884\u6d4b\u4efb\u52a1\u4e2d\u6d4b\u8bd5\u5176\u6027\u80fd\u3001\u63a8\u7406\u65f6\u95f4\u548c\u8ba1\u7b97\u6210\u672c\u3002", "result": "VLMs \u5728\u573a\u666f\u7406\u89e3\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5b9e\u9645\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4ecd\u6709\u4e0d\u8db3\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "conclusion": "VLMs \u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0cLightEMMA \u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2505.00685", "pdf": "https://arxiv.org/pdf/2505.00685", "abs": "https://arxiv.org/abs/2505.00685", "authors": ["Daniel Eftekhari", "Vardan Papyan"], "title": "On the Importance of Gaussianizing Representations", "categories": ["cs.LG"], "comment": "ICML 2025 Proceedings", "summary": "The normal distribution plays a central role in information theory - it is at\nthe same time the best-case signal and worst-case noise distribution, has the\ngreatest representational capacity of any distribution, and offers an\nequivalence between uncorrelatedness and independence for joint distributions.\nAccounting for the mean and variance of activations throughout the layers of\ndeep neural networks has had a significant effect on facilitating their\neffective training, but seldom has a prescription for precisely what\ndistribution these activations should take, and how this might be achieved,\nbeen offered. Motivated by the information-theoretic properties of the normal\ndistribution, we address this question and concurrently present normality\nnormalization: a novel normalization layer which encourages normality in the\nfeature representations of neural networks using the power transform and\nemploys additive Gaussian noise during training. Our experiments\ncomprehensively demonstrate the effectiveness of normality normalization, in\nregards to its generalization performance on an array of widely used model and\ndataset combinations, its strong performance across various common factors of\nvariation such as model width, depth, and training minibatch size, its\nsuitability for usage wherever existing normalization layers are conventionally\nused, and as a means to improving model robustness to random perturbations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u6b63\u6001\u5f52\u4e00\u5316\u201d\u7684\u65b0\u5c42\uff0c\u901a\u8fc7\u4f7f\u7528\u5e42\u53d8\u6362\u548c\u9ad8\u65af\u566a\u58f0\u6765\u9f13\u52b1\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u8868\u793a\u7684\u6b63\u6001\u5206\u5e03\uff0c\u5b9e\u9a8c\u8bc1\u5b9e\u5176\u5728\u591a\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e86\u6cdb\u5316\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u53d7\u6b63\u6001\u5206\u5e03\u5728\u4fe1\u606f\u8bba\u4e2d\u4f5c\u4e3a\u6700\u4f73\u4fe1\u53f7\u548c\u6700\u5dee\u566a\u58f0\u5206\u5e03\u7684\u7406\u8bba\u542f\u53d1\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u6fc0\u6d3b\u5206\u5e03\u5e94\u9075\u5faa\u4f55\u79cd\u5f62\u5f0f\u53ca\u5176\u5b9e\u73b0\u65b9\u6cd5\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u201c\u6b63\u6001\u5f52\u4e00\u5316\u201d\u5c42\uff0c\u7ed3\u5408\u5e42\u53d8\u6362\u548c\u8bad\u7ec3\u65f6\u6dfb\u52a0\u7684\u9ad8\u65af\u566a\u58f0\uff0c\u4ee5\u4fc3\u8fdb\u7279\u5f81\u8868\u793a\u7684\u6b63\u6001\u5206\u5e03\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u80fd\u3001\u6a21\u578b\u5bbd\u5ea6/\u6df1\u5ea6/\u6279\u91cf\u5927\u5c0f\u7684\u9002\u5e94\u6027\u3001\u4ee5\u53ca\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6b63\u6001\u5f52\u4e00\u5316\u4e0d\u4ec5\u9002\u7528\u4e8e\u73b0\u6709\u5f52\u4e00\u5316\u5c42\u7684\u573a\u666f\uff0c\u8fd8\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u968f\u673a\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u5f52\u4e00\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00661", "pdf": "https://arxiv.org/pdf/2505.00661", "abs": "https://arxiv.org/abs/2505.00661", "authors": ["Andrew K. Lampinen", "Arslan Chaudhry", "Stephanie C. Y. Chan", "Cody Wild", "Diane Wan", "Alex Ku", "J\u00f6rg Bornschein", "Razvan Pascanu", "Murray Shanahan", "James L. McClelland"], "title": "On the generalization of language models from in-context learning and finetuning: a controlled study", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models exhibit exciting capabilities, yet can show\nsurprisingly narrow generalization from finetuning -- from failing to\ngeneralize to simple reversals of relations they are trained on, to missing\nlogical deductions that can be made from trained information. These failures to\ngeneralize from fine-tuning can hinder practical application of these models.\nHowever, language models' in-context learning shows different inductive biases,\nand can generalize better in some of these cases. Here, we explore these\ndifferences in generalization between in-context- and fine-tuning-based\nlearning. To do so, we constructed several novel datasets to evaluate and\nimprove models' ability to generalize from finetuning data. The datasets are\nconstructed to isolate the knowledge in the dataset from that in pretraining,\nto create clean tests of generalization. We expose pretrained large models to\ncontrolled subsets of the information in these datasets -- either in context,\nor through fine-tuning -- and evaluate their performance on test sets that\nrequire various types of generalization. We find overall that in data-matched\nsettings, in-context learning can generalize more flexibly than fine-tuning\n(though we also find some qualifications of prior findings, such as cases when\nfine-tuning can generalize to reversals embedded in a larger structure of\nknowledge). We build on these findings to propose a method to enable improved\ngeneralization from fine-tuning: adding in-context inferences to finetuning\ndata. We show that this method improves generalization across various splits of\nour datasets and other benchmarks. Our results have implications for\nunderstanding the inductive biases of different modes of learning in language\nmodels, and practically improving their performance.", "AI": {"tldr": "\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5fae\u8c03\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u5982\u65e0\u6cd5\u5904\u7406\u7b80\u5355\u5173\u7cfb\u53cd\u8f6c\u6216\u903b\u8f91\u63a8\u7406\u3002\u7814\u7a76\u8868\u660e\uff0c\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u6784\u5efa\u4e86\u65b0\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86\u5fae\u8c03\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u6cdb\u5316\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u63a8\u7406\u6539\u8fdb\u5fae\u8c03\u6cdb\u5316\u7684\u65b9\u6cd5\uff0c\u6548\u679c\u663e\u8457\u3002", "motivation": "\u63a2\u7a76\u5fae\u8c03\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6cdb\u5316\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5fae\u8c03\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u65b0\u6570\u636e\u96c6\uff0c\u9694\u79bb\u9884\u8bad\u7ec3\u77e5\u8bc6\uff0c\u5206\u522b\u901a\u8fc7\u5fae\u8c03\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u66b4\u9732\u6a21\u578b\uff0c\u6d4b\u8bd5\u4e0d\u540c\u7c7b\u578b\u7684\u6cdb\u5316\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u7ed3\u5408\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u5fae\u8c03\u65b9\u6cd5\u3002", "result": "\u5728\u6570\u636e\u5339\u914d\u6761\u4ef6\u4e0b\uff0c\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u4f18\u4e8e\u5fae\u8c03\u3002\u7ed3\u5408\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u5fae\u8c03\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u4e86\u6cdb\u5316\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u540c\u5b66\u4e60\u6a21\u5f0f\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u5e76\u63d0\u4f9b\u4e86\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u7684\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2411.15923", "pdf": "https://arxiv.org/pdf/2411.15923", "abs": "https://arxiv.org/abs/2411.15923", "authors": ["Saba Zahid", "Sajid Ghuffar", "Obaid-ur-Rehman", "Syed Roshaan Ali Shah"], "title": "Deep Learning for automated multi-scale functional field boundaries extraction using multi-date Sentinel-2 and PlanetScope imagery: Case Study of Netherlands and Pakistan", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2; I.2.10"], "comment": "09 pages, To be published", "summary": "This study explores the effectiveness of multi-temporal satellite imagery for\nbetter functional field boundary delineation using deep learning semantic\nsegmentation architecture on two distinct geographical and multi-scale farming\nsystems of Netherlands and Pakistan. Multidate images of April, August and\nOctober 2022 were acquired for PlanetScope and Sentinel-2 in sub regions of\nNetherlands and November 2022, February and March 2023 for selected area of\nDunyapur in Pakistan. For Netherlands, Basic registration crop parcels (BRP)\nvector layer was used as labeled training data. while self-crafted field\nboundary vector data were utilized for Pakistan. Four deep learning models with\nUNET architecture were evaluated using different combinations of multi-date\nimages and NDVI stacks in the Netherlands subregions. A comparative analysis of\nIoU scores assessed the effectiveness of the proposed multi-date NDVI stack\napproach. These findings were then applied for transfer learning, using\npre-trained models from the Netherlands on the selected area in Pakistan.\nAdditionally, separate models were trained using self-crafted field boundary\ndata for Pakistan, and combined models were developed using data from both the\nNetherlands and Pakistan. Results indicate that multi-date NDVI stacks provide\nadditional temporal context, reflecting crop growth over different times of the\nseason. The study underscores the critical role of multi-scale ground\ninformation from diverse geographical areas in developing robust and\nuniversally applicable models for field boundary delineation. The results also\nhighlight the importance of fine spatial resolution for extraction of field\nboundaries in regions with small scale framing. The findings can be extended to\nmulti-scale implementations for improved automatic field boundary delineation\nin heterogeneous agricultural environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u65f6\u76f8\u536b\u661f\u5f71\u50cf\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u5728\u8377\u5170\u548c\u5df4\u57fa\u65af\u5766\u7684\u519c\u7530\u8fb9\u754c\u5212\u5206\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u7ed3\u5408\u591a\u65e5\u671fNDVI\u5806\u6808\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u9ad8\u5206\u8fa8\u7387\u548c\u591a\u5c3a\u5ea6\u5730\u9762\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u7d22\u591a\u65f6\u76f8\u536b\u661f\u5f71\u50cf\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u519c\u7530\u8fb9\u754c\u5212\u5206\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u5730\u7406\u548c\u519c\u4e1a\u89c4\u6a21\u7cfb\u7edf\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528UNET\u67b6\u6784\u7684\u56db\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u65e5\u671f\u5f71\u50cf\u548cNDVI\u5806\u6808\uff0c\u5728\u8377\u5170\u548c\u5df4\u57fa\u65af\u5766\u8fdb\u884c\u8bad\u7ec3\u548c\u9a8c\u8bc1\uff0c\u5e76\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u548c\u7ec4\u5408\u6a21\u578b\u7b56\u7565\u3002", "result": "\u591a\u65e5\u671fNDVI\u5806\u6808\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\uff0c\u9ad8\u5206\u8fa8\u7387\u5f71\u50cf\u5bf9\u5c0f\u89c4\u6a21\u519c\u7530\u8fb9\u754c\u5212\u5206\u5c24\u4e3a\u5173\u952e\uff0c\u6a21\u578b\u5728\u591a\u5730\u7406\u533a\u57df\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u591a\u65f6\u76f8\u5f71\u50cf\u548c\u591a\u5c3a\u5ea6\u5730\u9762\u4fe1\u606f\u5bf9\u5f00\u53d1\u901a\u7528\u7684\u519c\u7530\u8fb9\u754c\u5212\u5206\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u672a\u6765\u53ef\u63a8\u5e7f\u5230\u5f02\u8d28\u6027\u519c\u4e1a\u73af\u5883\u4e2d\u3002"}}
{"id": "2505.00662", "pdf": "https://arxiv.org/pdf/2505.00662", "abs": "https://arxiv.org/abs/2505.00662", "authors": ["Wenkai Yang", "Jingwen Chen", "Yankai Lin", "Ji-Rong Wen"], "title": "DeepCritic: Deliberate Critique with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress. Data and models are available at\n  https://github.com/RUCBM/DeepCritic", "summary": "As Large Language Models (LLMs) are rapidly evolving, providing accurate\nfeedback and scalable oversight on their outputs becomes an urgent and critical\nproblem. Leveraging LLMs as critique models to achieve automated supervision is\na promising solution. In this work, we focus on studying and enhancing the math\ncritique ability of LLMs. Current LLM critics provide critiques that are too\nshallow and superficial on each step, leading to low judgment accuracy and\nstruggling to offer sufficient feedback for the LLM generator to correct\nmistakes. To tackle this issue, we propose a novel and effective two-stage\nframework to develop LLM critics that are capable of deliberately critiquing on\neach reasoning step of math solutions. In the first stage, we utilize\nQwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for\nsupervised fine-tuning. Each seed critique consists of deliberate step-wise\ncritiques that includes multi-perspective verifications as well as in-depth\ncritiques of initial critiques for each reasoning step. Then, we perform\nreinforcement learning on the fine-tuned model with either existing\nhuman-labeled data from PRM800K or our automatically annotated data obtained\nvia Monte Carlo sampling-based correctness estimation, to further incentivize\nits critique ability. Our developed critique model built on Qwen2.5-7B-Instruct\nnot only significantly outperforms existing LLM critics (including the\nsame-sized DeepSeek-R1-distill models and GPT-4o) on various error\nidentification benchmarks, but also more effectively helps the LLM generator\nrefine erroneous steps through more detailed feedback.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6570\u5b66\u89e3\u9898\u6b65\u9aa4\u4e2d\u7684\u6df1\u5ea6\u6279\u5224\u80fd\u529b\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u6279\u5224\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLM\u7684\u6279\u5224\u80fd\u529b\u8fc7\u4e8e\u6d45\u663e\uff0c\u65e0\u6cd5\u63d0\u4f9b\u8db3\u591f\u7684\u53cd\u9988\u4ee5\u7ea0\u6b63\u9519\u8bef\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u6279\u5224\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Qwen2.5-72B-Instruct\u751f\u62104.5K\u957f\u683c\u5f0f\u6279\u5224\u6570\u636e\uff0c\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u518d\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08\u4f7f\u7528PRM800K\u6216\u81ea\u52a8\u6807\u6ce8\u6570\u636e\uff09\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002", "result": "\u57fa\u4e8eQwen2.5-7B-Instruct\u7684\u6279\u5224\u6a21\u578b\u5728\u591a\u4e2a\u9519\u8bef\u8bc6\u522b\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff08\u5982DeepSeek-R1-distill\u548cGPT-4o\uff09\uff0c\u5e76\u80fd\u66f4\u6709\u6548\u5730\u5e2e\u52a9\u751f\u6210\u6a21\u578b\u4fee\u6b63\u9519\u8bef\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86LLM\u6279\u5224\u80fd\u529b\u7684\u6df1\u5ea6\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u81ea\u52a8\u5316\u76d1\u7763\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.00295", "pdf": "https://arxiv.org/pdf/2505.00295", "abs": "https://arxiv.org/abs/2505.00295", "authors": ["Xinlong Zhao", "Shan Du"], "title": "Fine-grained spatial-temporal perception for gas leak segmentation", "categories": ["cs.CV", "cs.AI", "68T45 (Primary), 68T07 (Secondary)", "I.2.10; I.4.6"], "comment": "6 pages, 4 figures, ICIP 2025 Conference", "summary": "Gas leaks pose significant risks to human health and the environment. Despite\nlong-standing concerns, there are limited methods that can efficiently and\naccurately detect and segment leaks due to their concealed appearance and\nrandom shapes. In this paper, we propose a Fine-grained Spatial-Temporal\nPerception (FGSTP) algorithm for gas leak segmentation. FGSTP captures critical\nmotion clues across frames and integrates them with refined object features in\nan end-to-end network. Specifically, we first construct a correlation volume to\ncapture motion information between consecutive frames. Then, the fine-grained\nperception progressively refines the object-level features using previous\noutputs. Finally, a decoder is employed to optimize boundary segmentation.\nBecause there is no highly precise labeled dataset for gas leak segmentation,\nwe manually label a gas leak video dataset, GasVid. Experimental results on\nGasVid demonstrate that our model excels in segmenting non-rigid objects such\nas gas leaks, generating the most accurate mask compared to other\nstate-of-the-art (SOTA) models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFGSTP\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u5730\u68c0\u6d4b\u548c\u5206\u5272\u6c14\u4f53\u6cc4\u6f0f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u9690\u853d\u548c\u968f\u673a\u5f62\u72b6\u6cc4\u6f0f\u7684\u6311\u6218\u3002", "motivation": "\u6c14\u4f53\u6cc4\u6f0f\u5bf9\u4eba\u7c7b\u5065\u5eb7\u548c\u73af\u5883\u6784\u6210\u91cd\u5927\u98ce\u9669\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u51c6\u786e\u5730\u68c0\u6d4b\u548c\u5206\u5272\u8fd9\u4e9b\u9690\u853d\u4e14\u5f62\u72b6\u968f\u673a\u7684\u6cc4\u6f0f\u3002", "method": "\u63d0\u51faFGSTP\u7b97\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u76f8\u5173\u4f53\u79ef\u6355\u83b7\u8fde\u7eed\u5e27\u95f4\u7684\u8fd0\u52a8\u4fe1\u606f\uff0c\u9010\u6b65\u7ec6\u5316\u5bf9\u8c61\u7ea7\u7279\u5f81\uff0c\u5e76\u5229\u7528\u89e3\u7801\u5668\u4f18\u5316\u8fb9\u754c\u5206\u5272\u3002", "result": "\u5728\u81ea\u5efa\u6570\u636e\u96c6GasVid\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFGSTP\u5728\u975e\u521a\u6027\u7269\u4f53\uff08\u5982\u6c14\u4f53\u6cc4\u6f0f\uff09\u5206\u5272\u4e0a\u4f18\u4e8e\u5176\u4ed6SOTA\u6a21\u578b\uff0c\u751f\u6210\u6700\u51c6\u786e\u7684\u63a9\u7801\u3002", "conclusion": "FGSTP\u7b97\u6cd5\u5728\u6c14\u4f53\u6cc4\u6f0f\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u96be\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2504.21413", "pdf": "https://arxiv.org/pdf/2504.21413", "abs": "https://arxiv.org/abs/2504.21413", "authors": ["H. Brendan McMahan", "Krishna Pillutla"], "title": "An Inversion Theorem for Buffered Linear Toeplitz (BLT) Matrices and Applications to Streaming Differential Privacy", "categories": ["cs.CR", "cs.LG", "eess.SP"], "comment": null, "summary": "Buffered Linear Toeplitz (BLT) matrices are a family of parameterized\nlower-triangular matrices that play an important role in streaming differential\nprivacy with correlated noise. Our main result is a BLT inversion theorem: the\ninverse of a BLT matrix is itself a BLT matrix with different parameters. We\nalso present an efficient and differentiable $O(d^3)$ algorithm to compute the\nparameters of the inverse BLT matrix, where $d$ is the degree of the original\nBLT (typically $d < 10$). Our characterization enables direct optimization of\nBLT parameters for privacy mechanisms through automatic differentiation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBuffered Linear Toeplitz (BLT)\u77e9\u9635\u7684\u9006\u5b9a\u7406\u53ca\u5176\u9ad8\u6548\u8ba1\u7b97\u7b97\u6cd5\uff0c\u652f\u6301\u901a\u8fc7\u81ea\u52a8\u5fae\u5206\u4f18\u5316\u9690\u79c1\u673a\u5236\u53c2\u6570\u3002", "motivation": "BLT\u77e9\u9635\u5728\u6d41\u5f0f\u5dee\u5206\u9690\u79c1\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u9006\u77e9\u9635\u7684\u9ad8\u6548\u8ba1\u7b97\u548c\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faBLT\u9006\u5b9a\u7406\uff0c\u5e76\u8bbe\u8ba1\u4e00\u4e2aO(d^3)\u590d\u6742\u5ea6\u7684\u9ad8\u6548\u53ef\u5fae\u5206\u7b97\u6cd5\uff0c\u5176\u4e2dd\u4e3aBLT\u77e9\u9635\u7684\u9636\u6570\u3002", "result": "\u8bc1\u660e\u4e86BLT\u77e9\u9635\u7684\u9006\u4ecd\u662fBLT\u77e9\u9635\uff0c\u5e76\u5b9e\u73b0\u4e86\u53c2\u6570\u7684\u9ad8\u6548\u8ba1\u7b97\u3002", "conclusion": "\u8be5\u6210\u679c\u53ef\u76f4\u63a5\u901a\u8fc7\u81ea\u52a8\u5fae\u5206\u4f18\u5316BLT\u53c2\u6570\uff0c\u63d0\u5347\u9690\u79c1\u673a\u5236\u7684\u6027\u80fd\u3002"}}
{"id": "2505.00675", "pdf": "https://arxiv.org/pdf/2505.00675", "abs": "https://arxiv.org/abs/2505.00675", "authors": ["Yiming Du", "Wenyu Huang", "Danna Zheng", "Zhaowei Wang", "Sebastien Montella", "Mirella Lapata", "Kam-Fai Wong", "Jeff Z. Pan"], "title": "Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions", "categories": ["cs.CL"], "comment": null, "summary": "Memory is a fundamental component of AI systems, underpinning large language\nmodels (LLMs) based agents. While prior surveys have focused on memory\napplications with LLMs, they often overlook the atomic operations that underlie\nmemory dynamics. In this survey, we first categorize memory representations\ninto parametric, contextual structured, and contextual unstructured and then\nintroduce six fundamental memory operations: Consolidation, Updating, Indexing,\nForgetting, Retrieval, and Compression. We systematically map these operations\nto the most relevant research topics across long-term, long-context, parametric\nmodification, and multi-source memory. By reframing memory systems through the\nlens of atomic operations and representation types, this survey provides a\nstructured and dynamic perspective on research, benchmark datasets, and tools\nrelated to memory in AI, clarifying the functional interplay in LLMs based\nagents while outlining promising directions for future research\\footnote{The\npaper list, datasets, methods and tools are available at\n\\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\\_Memory\\_in\\_AI}.}.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5206\u7c7b\u8bb0\u5fc6\u8868\u793a\u548c\u4ecb\u7ecd\u516d\u79cd\u57fa\u672c\u64cd\u4f5c\uff0c\u4e3aAI\u4e2d\u7684\u8bb0\u5fc6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u89c6\u89d2\u3002", "motivation": "\u73b0\u6709\u8c03\u67e5\u5e38\u5ffd\u89c6\u8bb0\u5fc6\u52a8\u6001\u7684\u539f\u5b50\u64cd\u4f5c\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3aLLM\u4ee3\u7406\u7684\u8bb0\u5fc6\u529f\u80fd\u63d0\u4f9b\u7cfb\u7edf\u6027\u5206\u6790\u3002", "method": "\u5c06\u8bb0\u5fc6\u8868\u793a\u5206\u7c7b\u4e3a\u53c2\u6570\u5316\u3001\u4e0a\u4e0b\u6587\u5316\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\uff0c\u5e76\u5f15\u5165\u516d\u79cd\u57fa\u672c\u8bb0\u5fc6\u64cd\u4f5c\uff0c\u7cfb\u7edf\u6620\u5c04\u5230\u76f8\u5173\u7814\u7a76\u4e3b\u9898\u3002", "result": "\u5efa\u7acb\u4e86\u8bb0\u5fc6\u64cd\u4f5c\u4e0e\u7814\u7a76\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u5e76\u6574\u7406\u4e86\u76f8\u5173\u6570\u636e\u96c6\u3001\u65b9\u6cd5\u548c\u5de5\u5177\u3002", "conclusion": "\u901a\u8fc7\u539f\u5b50\u64cd\u4f5c\u89c6\u89d2\uff0c\u672c\u6587\u4e3aAI\u8bb0\u5fc6\u7814\u7a76\u63d0\u4f9b\u4e86\u6e05\u6670\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2505.00308", "pdf": "https://arxiv.org/pdf/2505.00308", "abs": "https://arxiv.org/abs/2505.00308", "authors": ["Biling Wang", "Austen Maniscalco", "Ti Bai", "Siqiu Wang", "Michael Dohopolski", "Mu-Han Lin", "Chenyang Shen", "Dan Nguyen", "Junzhou Huang", "Steve Jiang", "Xinlei Wang"], "title": "AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented Contour Quality", "categories": ["cs.CV", "cs.AI", "stat.AP"], "comment": null, "summary": "Purpose: This study presents a Deep Learning (DL)-based quality assessment\n(QA) approach for evaluating auto-generated contours (auto-contours) in\nradiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). Leveraging\nBayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds,\nthe method enables confident QA predictions without relying on ground truth\ncontours or extensive manual labeling. Methods: We developed a BOC model to\nclassify auto-contour quality and quantify prediction uncertainty. A\ncalibration step was used to optimize uncertainty thresholds that meet clinical\naccuracy needs. The method was validated under three data scenarios: no manual\nlabels, limited labels, and extensive labels. For rectum contours in prostate\ncancer, we applied geometric surrogate labels when manual labels were absent,\ntransfer learning when limited, and direct supervision when ample labels were\navailable. Results: The BOC model delivered robust performance across all\nscenarios. Fine-tuning with just 30 manual labels and calibrating with 34\nsubjects yielded over 90% accuracy on test data. Using the calibrated\nthreshold, over 93% of the auto-contours' qualities were accurately predicted\nin over 98% of cases, reducing unnecessary manual reviews and highlighting\ncases needing correction. Conclusion: The proposed QA model enhances contouring\nefficiency in OART by reducing manual workload and enabling fast, informed\nclinical decisions. Through uncertainty quantification, it ensures safer, more\nreliable radiotherapy workflows.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u5e8f\u6570\u5206\u7c7b\u7684\u81ea\u52a8\u653e\u7597\u8f6e\u5ed3\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u51cf\u5c11\u4eba\u5de5\u5ba1\u67e5\u9700\u6c42\uff0c\u63d0\u9ad8\u4e34\u5e8a\u51b3\u7b56\u6548\u7387\u3002", "motivation": "\u5728\u7ebf\u81ea\u9002\u5e94\u653e\u7597\uff08OART\uff09\u4e2d\uff0c\u81ea\u52a8\u751f\u6210\u7684\u8f6e\u5ed3\u9700\u8981\u9ad8\u6548\u7684\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u51cf\u5c11\u4eba\u5de5\u8d1f\u62c5\u5e76\u786e\u4fdd\u5b89\u5168\u6027\u3002", "method": "\u7ed3\u5408\u8d1d\u53f6\u65af\u5e8f\u6570\u5206\u7c7b\uff08BOC\uff09\u6a21\u578b\u548c\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u9608\u503c\uff0c\u652f\u6301\u65e0\u6807\u6ce8\u3001\u6709\u9650\u6807\u6ce8\u548c\u5145\u8db3\u6807\u6ce8\u4e09\u79cd\u6570\u636e\u573a\u666f\u3002", "result": "\u6a21\u578b\u5728\u6240\u6709\u573a\u666f\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\uff0830\u4f8b\uff09\u5373\u53ef\u8fbe\u523090%\u4ee5\u4e0a\u51c6\u786e\u7387\uff0c93%\u7684\u9884\u6d4b\u8d28\u91cf\u53ef\u4fe1\u5ea6\u8d8598%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86OART\u8f6e\u5ed3\u6548\u7387\uff0c\u901a\u8fc7\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u548c\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u4f18\u5316\u653e\u7597\u6d41\u7a0b\u3002"}}
{"id": "2505.00679", "pdf": "https://arxiv.org/pdf/2505.00679", "abs": "https://arxiv.org/abs/2505.00679", "authors": ["Xinchen Yang", "Marine Carpuat"], "title": "Steering Large Language Models with Register Analysis for Arbitrary Style Transfer", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nrewriting text across various styles. However, effectively leveraging this\nability for example-based arbitrary style transfer, where an input text is\nrewritten to match the style of a given exemplar, remains an open challenge. A\nkey question is how to describe the style of the exemplar to guide LLMs toward\nhigh-quality rewrites. In this work, we propose a prompting method based on\nregister analysis to guide LLMs to perform this task. Empirical evaluations\nacross multiple style transfer tasks show that our prompting approach enhances\nstyle transfer strength while preserving meaning more effectively than existing\nprompting strategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u57df\u5206\u6790\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u6307\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u793a\u4f8b\u5f0f\u4efb\u610f\u98ce\u683c\u8f6c\u79fb\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u98ce\u683c\u8f6c\u6362\u5f3a\u5ea6\u4e0e\u610f\u4e49\u4fdd\u7559\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u591a\u79cd\u98ce\u683c\u6539\u5199\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5982\u4f55\u5229\u7528\u5176\u80fd\u529b\u5b9e\u73b0\u57fa\u4e8e\u793a\u4f8b\u7684\u4efb\u610f\u98ce\u683c\u8f6c\u79fb\uff08\u5373\u8f93\u5165\u6587\u672c\u5339\u914d\u7ed9\u5b9a\u793a\u4f8b\u98ce\u683c\uff09\u4ecd\u5177\u6311\u6218\u6027\u3002\u6838\u5fc3\u95ee\u9898\u5728\u4e8e\u5982\u4f55\u63cf\u8ff0\u793a\u4f8b\u98ce\u683c\u4ee5\u5f15\u5bfcLLMs\u751f\u6210\u9ad8\u8d28\u91cf\u6539\u5199\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u57df\u5206\u6790\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u98ce\u683c\u63cf\u8ff0\uff08\u5982\u8bed\u57df\u7279\u5f81\uff09\u6307\u5bfcLLMs\u5b8c\u6210\u98ce\u683c\u8f6c\u79fb\u4efb\u52a1\u3002", "result": "\u5728\u591a\u79cd\u98ce\u683c\u8f6c\u79fb\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u589e\u5f3a\u98ce\u683c\u8f6c\u6362\u5f3a\u5ea6\u7684\u540c\u65f6\uff0c\u76f8\u6bd4\u73b0\u6709\u63d0\u793a\u7b56\u7565\u80fd\u66f4\u6709\u6548\u5730\u4fdd\u7559\u539f\u6587\u610f\u4e49\u3002", "conclusion": "\u57fa\u4e8e\u8bed\u57df\u5206\u6790\u7684\u63d0\u793a\u65b9\u6cd5\u4e3a\u5229\u7528LLMs\u5b9e\u73b0\u9ad8\u8d28\u91cf\u98ce\u683c\u8f6c\u79fb\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u5e73\u8861\u4e86\u98ce\u683c\u9002\u914d\u4e0e\u5185\u5bb9\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2505.00049", "pdf": "https://arxiv.org/pdf/2505.00049", "abs": "https://arxiv.org/abs/2505.00049", "authors": ["Wenhan Dong", "Yuemeng Zhao", "Zhen Sun", "Yule Liu", "Zifan Peng", "Jingyi Zheng", "Zongmin Zhang", "Ziyi Zhang", "Jun Wu", "Ruiming Wang", "Shengmin Xu", "Xinyi Huang", "Xinlei He"], "title": "Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications", "categories": ["cs.CY", "cs.CL", "cs.HC", "cs.LG"], "comment": "26 pages,7 figures", "summary": "As large language models (LLMs) are increasingly used in human-centered\ntasks, assessing their psychological traits is crucial for understanding their\nsocial impact and ensuring trustworthy AI alignment. While existing reviews\nhave covered some aspects of related research, several important areas have not\nbeen systematically discussed, including detailed discussions of diverse\npsychological tests, LLM-specific psychological datasets, and the applications\nof LLMs with psychological traits. To address this gap, we systematically\nreview six key dimensions of applying psychological theories to LLMs: (1)\nassessment tools; (2) LLM-specific datasets; (3) evaluation metrics\n(consistency and stability); (4) empirical findings; (5) personality simulation\nmethods; and (6) LLM-based behavior simulation. Our analysis highlights both\nthe strengths and limitations of current methods. While some LLMs exhibit\nreproducible personality patterns under specific prompting schemes, significant\nvariability remains across tasks and settings. Recognizing methodological\nchallenges such as mismatches between psychological tools and LLMs'\ncapabilities, as well as inconsistencies in evaluation practices, this study\naims to propose future directions for developing more interpretable, robust,\nand generalizable psychological assessment frameworks for LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5fc3\u7406\u5b66\u7406\u8bba\u5e94\u7528\u4e2d\u7684\u516d\u4e2a\u5173\u952e\u7ef4\u5ea6\uff0c\u5305\u62ec\u8bc4\u4f30\u5de5\u5177\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u7b49\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002\u540c\u65f6\u63d0\u51fa\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u4ee5\u6784\u5efa\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u548c\u7a33\u5065\u6027\u7684LLMs\u5fc3\u7406\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u968f\u7740LLMs\u5728\u4eba\u7c7b\u4e2d\u5fc3\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8bc4\u4f30\u5176\u5fc3\u7406\u7279\u8d28\u5bf9\u7406\u89e3\u793e\u4f1a\u5f71\u54cd\u548c\u786e\u4fddAI\u53ef\u4fe1\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u672a\u7cfb\u7edf\u8ba8\u8bba\u5fc3\u7406\u5b66\u6d4b\u8bd5\u3001LLM\u4e13\u7528\u6570\u636e\u96c6\u53ca\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u516d\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a\u8bc4\u4f30\u5de5\u5177\u3001LLM\u4e13\u7528\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\uff08\u4e00\u81f4\u6027\u4e0e\u7a33\u5b9a\u6027\uff09\u3001\u5b9e\u8bc1\u53d1\u73b0\u3001\u4eba\u683c\u6a21\u62df\u65b9\u6cd5\u53caLLM\u884c\u4e3a\u6a21\u62df\u3002\u901a\u8fc7\u5206\u6790\u5f53\u524d\u65b9\u6cd5\uff0c\u63d0\u51fa\u6539\u8fdb\u65b9\u5411\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u67d0\u4e9bLLMs\u5728\u7279\u5b9a\u63d0\u793a\u65b9\u6848\u4e0b\u8868\u73b0\u51fa\u53ef\u590d\u73b0\u7684\u4eba\u683c\u6a21\u5f0f\uff0c\u4f46\u5728\u4efb\u52a1\u548c\u8bbe\u7f6e\u4e2d\u5b58\u5728\u663e\u8457\u53d8\u5f02\u6027\u3002\u540c\u65f6\u6307\u51fa\u5fc3\u7406\u5b66\u5de5\u5177\u4e0eLLM\u80fd\u529b\u4e0d\u5339\u914d\u7b49\u95ee\u9898\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u5efa\u8bae\u672a\u6765\u5f00\u53d1\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3001\u7a33\u5065\u6027\u548c\u901a\u7528\u6027\u7684LLMs\u5fc3\u7406\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2505.00322", "pdf": "https://arxiv.org/pdf/2505.00322", "abs": "https://arxiv.org/abs/2505.00322", "authors": ["Keshu Wu", "Zihao Li", "Sixu Li", "Xinyue Ye", "Dominique Lord", "Yang Zhou"], "title": "AI2-Active Safety: AI-enabled Interaction-aware Active Safety Analysis with Vehicle Dynamics", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "This paper introduces an AI-enabled, interaction-aware active safety analysis\nframework that accounts for groupwise vehicle interactions. Specifically, the\nframework employs a bicycle model-augmented with road gradient\nconsiderations-to accurately capture vehicle dynamics. In parallel, a\nhypergraph-based AI model is developed to predict probabilistic trajectories of\nambient traffic. By integrating these two components, the framework derives\nvehicle intra-spacing over a 3D road surface as the solution of a stochastic\nordinary differential equation, yielding high-fidelity surrogate safety\nmeasures such as time-to-collision (TTC). To demonstrate its effectiveness, the\nframework is analyzed using stochastic numerical methods comprising 4th-order\nRunge-Kutta integration and AI inference, generating probability-weighted\nhigh-fidelity TTC (HF-TTC) distributions that reflect complex multi-agent\nmaneuvers and behavioral uncertainties. Evaluated with HF-TTC against\ntraditional constant-velocity TTC and non-interaction-aware approaches on\nhighway datasets, the proposed framework offers a systematic methodology for\nactive safety analysis with enhanced potential for improving safety perception\nin complex traffic environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u4ea4\u4e92\u611f\u77e5\u4e3b\u52a8\u5b89\u5168\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u81ea\u884c\u8f66\u6a21\u578b\u4e0e\u9053\u8def\u5761\u5ea6\u56e0\u7d20\u6355\u83b7\u8f66\u8f86\u52a8\u529b\u5b66\uff0c\u5e76\u4f7f\u7528\u8d85\u56feAI\u6a21\u578b\u9884\u6d4b\u5468\u56f4\u4ea4\u901a\u7684\u6982\u7387\u8f68\u8ff9\uff0c\u6700\u7ec8\u751f\u6210\u9ad8\u4fdd\u771f\u7684\u5b89\u5168\u6307\u6807\u5982\u78b0\u649e\u65f6\u95f4\uff08TTC\uff09\u3002", "motivation": "\u4e3a\u63d0\u9ad8\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u611f\u77e5\u80fd\u529b\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u80fd\u7cfb\u7edf\u6027\u5206\u6790\u8f66\u8f86\u95f4\u4ea4\u4e92\u884c\u4e3a\u7684\u6846\u67b6\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u4e3b\u52a8\u5b89\u5168\u6027\u3002", "method": "\u6846\u67b6\u7ed3\u5408\u4e86\u81ea\u884c\u8f66\u6a21\u578b\uff08\u542b\u9053\u8def\u5761\u5ea6\uff09\u548c\u8d85\u56feAI\u6a21\u578b\uff0c\u901a\u8fc7\u968f\u673a\u5e38\u5fae\u5206\u65b9\u7a0b\u8ba1\u7b97\u8f66\u8f86\u95f4\u8ddd\uff0c\u5e76\u91c7\u75284\u9636Runge-Kutta\u79ef\u5206\u4e0eAI\u63a8\u7406\u751f\u6210\u9ad8\u4fdd\u771fTTC\u5206\u5e03\u3002", "result": "\u4e0e\u4f20\u7edf\u6052\u5b9a\u901f\u5ea6TTC\u53ca\u975e\u4ea4\u4e92\u611f\u77e5\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5728\u9ad8\u901f\u516c\u8def\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u80fd\u53cd\u6620\u590d\u6742\u591a\u667a\u80fd\u4f53\u884c\u4e3a\u548c\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u4e3b\u52a8\u5b89\u5168\u5206\u6790\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u611f\u77e5\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.00037", "pdf": "https://arxiv.org/pdf/2505.00037", "abs": "https://arxiv.org/abs/2505.00037", "authors": ["Junggu Choi", "Chansu Yu", "Kyle L. Jung", "Suan-Sin Foo", "Weiqiang Chen", "Suzy AA Comhair", "Serpil C. Erzurum", "Lara Jehi", "Jae U. Jung"], "title": "Can a Quantum Support Vector Machine algorithm be utilized to identify Key Biomarkers from Multi-Omics data of COVID19 patients?", "categories": ["quant-ph", "cs.LG", "q-bio.QM"], "comment": "70 pages, 6 figures", "summary": "Identifying key biomarkers for COVID-19 from high-dimensional multi-omics\ndata is critical for advancing both diagnostic and pathogenesis research. In\nthis study, we evaluated the applicability of the Quantum Support Vector\nMachine (QSVM) algorithm for biomarker-based classification of COVID-19.\nProteomic and metabolomic biomarkers from two independent datasets were ranked\nby importance using ridge regression and grouped accordingly. The top- and\nbottom-ranked biomarker sets were then used to train and evaluate both\nclassical SVM (CSVM) and QSVM models, serving as predictive and negative\ncontrol inputs, respectively. The QSVM was implemented with multiple quantum\nkernels, including amplitude encoding, angle encoding, the ZZ feature map, and\nthe projected quantum kernel. Across various experimental settings, QSVM\nconsistently achieved classification performance that was comparable to or\nexceeded that of CSVM, while reflecting the importance rankings by ridge\nregression. Although the experiments were conducted in numerical simulation,\nour findings highlight the potential of QSVM as a promising approach for\nmulti-omics data analysis in biomedical research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\uff08QSVM\uff09\u7b97\u6cd5\u8bc4\u4f30\u4e86\u5176\u5728COVID-19\u751f\u7269\u6807\u5fd7\u7269\u5206\u7c7b\u4e2d\u7684\u9002\u7528\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0cQSVM\u5728\u591a\u7ec4\u5b66\u6570\u636e\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u4e0e\u7ecf\u5178SVM\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u4ece\u9ad8\u7ef4\u591a\u7ec4\u5b66\u6570\u636e\u4e2d\u8bc6\u522bCOVID-19\u7684\u5173\u952e\u751f\u7269\u6807\u5fd7\u7269\u5bf9\u4e8e\u8bca\u65ad\u548c\u53d1\u75c5\u673a\u5236\u7814\u7a76\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30QSVM\u7b97\u6cd5\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u5cad\u56de\u5f52\u5bf9\u86cb\u767d\u8d28\u7ec4\u548c\u4ee3\u8c22\u7ec4\u751f\u7269\u6807\u5fd7\u7269\u8fdb\u884c\u91cd\u8981\u6027\u6392\u5e8f\uff0c\u5e76\u5c06\u5176\u5206\u4e3a\u9ad8\u6392\u540d\u548c\u4f4e\u6392\u540d\u7ec4\u3002\u968f\u540e\u5206\u522b\u7528\u7ecf\u5178SVM\uff08CSVM\uff09\u548cQSVM\uff08\u5305\u62ec\u591a\u79cd\u91cf\u5b50\u6838\uff09\u8bad\u7ec3\u6a21\u578b\u5e76\u6bd4\u8f83\u6027\u80fd\u3002", "result": "QSVM\u5728\u591a\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\u4e0b\u7684\u5206\u7c7b\u6027\u80fd\u4e0eCSVM\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u4e14\u80fd\u53cd\u6620\u5cad\u56de\u5f52\u7684\u91cd\u8981\u6027\u6392\u5e8f\u3002\u5c3d\u7ba1\u5b9e\u9a8c\u5728\u6570\u503c\u6a21\u62df\u4e2d\u5b8c\u6210\uff0c\u7ed3\u679c\u4ecd\u663e\u793a\u51faQSVM\u5728\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "QSVM\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u591a\u7ec4\u5b66\u6570\u636e\u5206\u6790\u65b9\u6cd5\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5728\u771f\u5b9e\u751f\u7269\u533b\u5b66\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2505.00105", "pdf": "https://arxiv.org/pdf/2505.00105", "abs": "https://arxiv.org/abs/2505.00105", "authors": ["Naam\u00e1n Huerga-P\u00e9rez", "Rub\u00e9n \u00c1lvarez", "Rub\u00e9n Ferrero-Guill\u00e9n", "Alberto Mart\u00ednez-Guti\u00e9rrez", "Javier D\u00edez-Gonz\u00e1lez"], "title": "Optimization of embeddings storage for RAG systems using quantization and dimensionality reduction techniques", "categories": ["cs.IR", "cs.CL", "cs.DB"], "comment": "13 pages, 9 figures, 1 table", "summary": "Retrieval-Augmented Generation enhances language models by retrieving\nrelevant information from external knowledge bases, relying on high-dimensional\nvector embeddings typically stored in float32 precision. However, storing these\nembeddings at scale presents significant memory challenges. To address this\nissue, we systematically investigate on MTEB benchmark two complementary\noptimization strategies: quantization, evaluating standard formats (float16,\nint8, binary) and low-bit floating-point types (float8), and dimensionality\nreduction, assessing methods like PCA, Kernel PCA, UMAP, Random Projections and\nAutoencoders. Our results show that float8 quantization achieves a 4x storage\nreduction with minimal performance degradation (<0.3%), significantly\noutperforming int8 quantization at the same compression level, being simpler to\nimplement. PCA emerges as the most effective dimensionality reduction\ntechnique. Crucially, combining moderate PCA (e.g., retaining 50% dimensions)\nwith float8 quantization offers an excellent trade-off, achieving 8x total\ncompression with less performance impact than using int8 alone (which provides\nonly 4x compression). To facilitate practical application, we propose a\nmethodology based on visualizing the performance-storage trade-off space to\nidentify the optimal configuration that maximizes performance within their\nspecific memory constraints.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u4f18\u5316\u9ad8\u7ef4\u5411\u91cf\u5d4c\u5165\u5b58\u50a8\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u91cf\u5316\u548c\u964d\u7ef4\u6280\u672f\uff0c\u53d1\u73b0float8\u91cf\u5316\u548cPCA\u7ed3\u5408\u5728\u5b58\u50a8\u538b\u7f29\u548c\u6027\u80fd\u4fdd\u6301\u4e0a\u8868\u73b0\u6700\u4f73\u3002\u5efa\u8bae\u53ef\u89c6\u5316\u6027\u80fd-\u5b58\u50a8\u6743\u8861\u7a7a\u95f4\u4ee5\u627e\u5230\u6700\u4f18\u914d\u7f6e\u3002", "motivation": "\u9ad8\u7ef4\u5411\u91cf\u5d4c\u5165\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u5b58\u50a8\u5f00\u9500\u5927\uff0c\u9700\u8981\u4f18\u5316\u5b58\u50a8\u7b56\u7565\u4ee5\u89e3\u51b3\u5185\u5b58\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u91cf\u5316\u548c\u964d\u7ef4\u6280\u672f\uff0c\u91cf\u5316\u5305\u62ecfloat16\u3001int8\u3001float8\u7b49\u683c\u5f0f\uff0c\u964d\u7ef4\u5305\u62ecPCA\u3001Kernel PCA\u7b49\u65b9\u6cd5\u3002", "result": "float8\u91cf\u5316\u57284\u500d\u538b\u7f29\u65f6\u6027\u80fd\u635f\u5931\u6700\u5c0f\uff08<0.3%\uff09\uff0c\u7ed3\u540850% PCA\u53ef\u5b9e\u73b08\u500d\u538b\u7f29\u4e14\u6027\u80fd\u4f18\u4e8e\u4ec5\u7528int8\u3002", "conclusion": "float8\u4e0ePCA\u7ed3\u5408\u662f\u6700\u4f73\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u65b9\u6cd5\u534f\u52a9\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u914d\u7f6e\u9009\u62e9\u3002"}}
{"id": "2505.00335", "pdf": "https://arxiv.org/pdf/2505.00335", "abs": "https://arxiv.org/abs/2505.00335", "authors": ["Seungjun Shin", "Suji Kim", "Dokwan Oh"], "title": "Efficient Neural Video Representation with Temporally Coherent Modulation", "categories": ["cs.CV", "cs.AI"], "comment": "ECCV 2024", "summary": "Implicit neural representations (INR) has found successful applications\nacross diverse domains. To employ INR in real-life, it is important to speed up\ntraining. In the field of INR for video applications, the state-of-the-art\napproach employs grid-type parametric encoding and successfully achieves a\nfaster encoding speed in comparison to its predecessors. However, the grid\nusage, which does not consider the video's dynamic nature, leads to redundant\nuse of trainable parameters. As a result, it has significantly lower parameter\nefficiency and higher bitrate compared to NeRV-style methods that do not use a\nparametric encoding. To address the problem, we propose Neural Video\nrepresentation with Temporally coherent Modulation (NVTM), a novel framework\nthat can capture dynamic characteristics of video. By decomposing the\nspatio-temporal 3D video data into a set of 2D grids with flow information,\nNVTM enables learning video representation rapidly and uses parameter\nefficiently. Our framework enables to process temporally corresponding pixels\nat once, resulting in the fastest encoding speed for a reasonable video\nquality, especially when compared to the NeRV-style method, with a speed\nincrease of over 3 times. Also, it remarks an average of 1.54dB/0.019\nimprovements in PSNR/LPIPS on UVG (Dynamic) (even with 10% fewer parameters)\nand an average of 1.84dB/0.013 improvements in PSNR/LPIPS on MCL-JCV (Dynamic),\ncompared to previous grid-type works. By expanding this to compression tasks,\nwe demonstrate comparable performance to video compression standards (H.264,\nHEVC) and recent INR approaches for video compression. Additionally, we perform\nextensive experiments demonstrating the superior performance of our algorithm\nacross diverse tasks, encompassing super resolution, frame interpolation and\nvideo inpainting. Project page is https://sujiikim.github.io/NVTM/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNVTM\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u7a7a\u5206\u89e3\u548c\u6d41\u4fe1\u606f\u5904\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u9891\u8868\u793a\u7684\u8bad\u7ec3\u901f\u5ea6\u548c\u53c2\u6570\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a(INR)\u5728\u89c6\u9891\u5e94\u7528\u4e2d\u5b58\u5728\u8bad\u7ec3\u901f\u5ea6\u6162\u548c\u53c2\u6570\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u7f51\u683c\u578b\u7f16\u7801\u65b9\u6cd5\u5ffd\u7565\u4e86\u89c6\u9891\u7684\u52a8\u6001\u7279\u6027\uff0c\u5bfc\u81f4\u5197\u4f59\u53c2\u6570\u548c\u9ad8\u6bd4\u7279\u7387\u3002", "method": "NVTM\u901a\u8fc7\u5c06\u65f6\u7a7a3D\u89c6\u9891\u6570\u636e\u5206\u89e3\u4e3a\u5e26\u6709\u6d41\u4fe1\u606f\u76842D\u7f51\u683c\u96c6\uff0c\u9ad8\u6548\u5b66\u4e60\u89c6\u9891\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u7684\u7f16\u7801\u901f\u5ea6\u548c\u9ad8\u53c2\u6570\u6548\u7387\u3002", "result": "\u4e0eNeRV\u98ce\u683c\u65b9\u6cd5\u76f8\u6bd4\uff0cNVTM\u7684\u7f16\u7801\u901f\u5ea6\u63d0\u5347\u4e863\u500d\u4ee5\u4e0a\uff0c\u5e76\u5728UVG\u548cMCL-JCV\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86\u5e73\u57471.54dB/0.019\u548c1.84dB/0.013\u7684PSNR/LPIPS\u63d0\u5347\u3002\u5728\u89c6\u9891\u538b\u7f29\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u4e0eH.264\u548cHEVC\u6807\u51c6\u76f8\u5f53\u3002", "conclusion": "NVTM\u901a\u8fc7\u52a8\u6001\u7279\u6027\u6355\u83b7\u548c\u9ad8\u6548\u53c2\u6570\u5229\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u8868\u793a\u7684\u901f\u5ea6\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89c6\u9891\u5904\u7406\u4efb\u52a1\u3002"}}
{"id": "2505.00212", "pdf": "https://arxiv.org/pdf/2505.00212", "abs": "https://arxiv.org/abs/2505.00212", "authors": ["Shaokun Zhang", "Ming Yin", "Jieyu Zhang", "Jiale Liu", "Zhiguang Han", "Jingyang Zhang", "Beibin Li", "Chi Wang", "Huazheng Wang", "Yiran Chen", "Qingyun Wu"], "title": "Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems", "categories": ["cs.MA", "cs.CL"], "comment": null, "summary": "Failure attribution in LLM multi-agent systems-identifying the agent and step\nresponsible for task failures-provides crucial clues for systems debugging but\nremains underexplored and labor-intensive. In this paper, we propose and\nformulate a new research area: automated failure attribution for LLM\nmulti-agent systems. To support this initiative, we introduce the Who&When\ndataset, comprising extensive failure logs from 127 LLM multi-agent systems\nwith fine-grained annotations linking failures to specific agents and decisive\nerror steps. Using the Who&When, we develop and evaluate three automated\nfailure attribution methods, summarizing their corresponding pros and cons. The\nbest method achieves 53.5% accuracy in identifying failure-responsible agents\nbut only 14.2% in pinpointing failure steps, with some methods performing below\nrandom. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to\nachieve practical usability. These results highlight the task's complexity and\nthe need for further research in this area. Code and dataset are available at\nhttps://github.com/mingyin1/Agents_Failure_Attribution", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u81ea\u52a8\u6545\u969c\u5f52\u56e0\u7684\u65b0\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u4ecb\u7ecd\u4e86\u5305\u542b\u7ec6\u7c92\u5ea6\u6807\u6ce8\u7684Who&When\u6570\u636e\u96c6\u3002\u901a\u8fc7\u4e09\u79cd\u65b9\u6cd5\u7684\u8bc4\u4f30\uff0c\u53d1\u73b0\u5373\u4f7f\u5728\u6700\u4f73\u60c5\u51b5\u4e0b\uff0c\u8bc6\u522b\u6545\u969c\u8d23\u4efb\u4ee3\u7406\u7684\u51c6\u786e\u7387\u4e5f\u4ec5\u4e3a53.5%\uff0c\u8868\u660e\u8be5\u4efb\u52a1\u7684\u590d\u6742\u6027\u548c\u7814\u7a76\u5fc5\u8981\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u6545\u969c\u5f52\u56e0\u5bf9\u7cfb\u7edf\u8c03\u8bd5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e0d\u8db3\u4e14\u624b\u52a8\u64cd\u4f5c\u7e41\u7410\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u81ea\u52a8\u5316\u6545\u969c\u5f52\u56e0\u7684\u9700\u6c42\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u81ea\u52a8\u6545\u969c\u5f52\u56e0\u65b9\u6cd5\uff0c\u5e76\u5728Who&When\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b127\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8be6\u7ec6\u6545\u969c\u65e5\u5fd7\u548c\u6807\u6ce8\u3002", "result": "\u6700\u4f73\u65b9\u6cd5\u8bc6\u522b\u6545\u969c\u8d23\u4efb\u4ee3\u7406\u7684\u51c6\u786e\u7387\u4e3a53.5%\uff0c\u4f46\u5b9a\u4f4d\u6545\u969c\u6b65\u9aa4\u7684\u51c6\u786e\u7387\u4ec5\u4e3a14.2%\uff0c\u90e8\u5206\u65b9\u6cd5\u8868\u73b0\u751a\u81f3\u4f4e\u4e8e\u968f\u673a\u731c\u6d4b\uff0c\u73b0\u6709\u5148\u8fdb\u63a8\u7406\u6a21\u578b\u4e5f\u96be\u4ee5\u5b9e\u7528\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6545\u969c\u5f52\u56e0\u4efb\u52a1\u7684\u590d\u6742\u6027\uff0c\u5f3a\u8c03\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u5fc5\u8981\u6027\uff0c\u540c\u65f6\u516c\u5f00\u4e86\u4ee3\u7801\u548c\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u672a\u6765\u5de5\u4f5c\u3002"}}
{"id": "2505.00263", "pdf": "https://arxiv.org/pdf/2505.00263", "abs": "https://arxiv.org/abs/2505.00263", "authors": ["Michael J. Ryan", "Danmei Xu", "Chris Nivera", "Daniel Campos"], "title": "EnronQA: Towards Personalized RAG over Private Documents", "categories": ["cs.IR", "cs.CL"], "comment": "26 pages, 4 figures, 6 tables", "summary": "Retrieval Augmented Generation (RAG) has become one of the most popular\nmethods for bringing knowledge-intensive context to large language models (LLM)\nbecause of its ability to bring local context at inference time without the\ncost or data leakage risks associated with fine-tuning. A clear separation of\nprivate information from the LLM training has made RAG the basis for many\nenterprise LLM workloads as it allows the company to augment LLM's\nunderstanding using customers' private documents. Despite its popularity for\nprivate documents in enterprise deployments, current RAG benchmarks for\nvalidating and optimizing RAG pipelines draw their corpora from public data\nsuch as Wikipedia or generic web pages and offer little to no personal context.\nSeeking to empower more personal and private RAG we release the EnronQA\nbenchmark, a dataset of 103,638 emails with 528,304 question-answer pairs\nacross 150 different user inboxes. EnronQA enables better benchmarking of RAG\npipelines over private data and allows for experimentation on the introduction\nof personalized retrieval settings over realistic data. Finally, we use EnronQA\nto explore the tradeoff in memorization and retrieval when reasoning over\nprivate documents.", "AI": {"tldr": "\u300aEnronQA\u300b\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u548c\u4f18\u5316RAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u7ba1\u9053\u7684\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u96c6\uff0c\u5305\u542b103,638\u5c01\u7535\u5b50\u90ae\u4ef6\u548c528,304\u5bf9\u95ee\u7b54\u5bf9\uff0c\u65e8\u5728\u652f\u6301\u4e2a\u6027\u5316\u3001\u9690\u79c1\u5316\u7684RAG\u6280\u672f\u53d1\u5c55\u3002", "motivation": "\u56e0\u4e3a\u73b0\u6709\u7684RAG\u57fa\u51c6\u6d4b\u8bd5\u591a\u4f7f\u7528\u516c\u5f00\u6570\u636e\uff0c\u7f3a\u4e4f\u5bf9\u4e2a\u6027\u5316\u3001\u9690\u79c1\u5316\u6570\u636e\u7684\u8986\u76d6\uff0c\u65e0\u6cd5\u6709\u6548\u652f\u6301\u4f01\u4e1a\u7ea7\u7684RAG\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86EnronQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u6539\u8fdb\u79c1\u4eba\u6587\u6863\u4e0a\u7684RAG\u6027\u80fd\u8bc4\u4f30\u3002", "method": "\u4f5c\u8005\u53d1\u5e03\u4e86EnronQA\u6570\u636e\u96c6\uff0c\u5305\u542b\u5927\u91cf\u771f\u5b9e\u7535\u5b50\u90ae\u4ef6\u548c\u95ee\u7b54\u5bf9\uff0c\u4ee5\u6a21\u62df\u79c1\u4eba\u6587\u6863\u7684\u5904\u7406\u573a\u666f\u3002\u540c\u65f6\uff0c\u5b9e\u9a8c\u7814\u7a76\u4e86\u8bb0\u5fc6\u5316\u548c\u68c0\u7d22\u5728\u79c1\u4eba\u6587\u6863\u63a8\u7406\u4e2d\u7684\u6743\u8861\u3002", "result": "EnronQA\u4e3a\u8bc4\u4f30RAG\u7ba1\u9053\u5728\u9690\u79c1\u6570\u636e\u4e0a\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u57fa\u51c6\uff0c\u5e76\u652f\u6301\u4e2a\u6027\u5316\u68c0\u7d22\u8bbe\u7f6e\u7684\u5b9e\u9a8c\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8bb0\u5fc6\u4e0e\u68c0\u7d22\u5728\u79c1\u4eba\u6587\u6863\u63a8\u7406\u4e2d\u5b58\u5728\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "EnronQA\u586b\u8865\u4e86RAG\u57fa\u51c6\u6d4b\u8bd5\u5728\u4e2a\u6027\u5316\u3001\u9690\u79c1\u5316\u6570\u636e\u4e0a\u7684\u7a7a\u767d\uff0c\u4e3a\u4f01\u4e1a\u5728\u79c1\u4eba\u6570\u636e\u4e0a\u7684RAG\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bc4\u4f30\u5de5\u5177\u548c\u652f\u6301\u3002"}}
{"id": "2505.00110", "pdf": "https://arxiv.org/pdf/2505.00110", "abs": "https://arxiv.org/abs/2505.00110", "authors": ["Insung Kong", "Juntong Chen", "Sophie Langer", "Johannes Schmidt-Hieber"], "title": "On the expressivity of deep Heaviside networks", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA"], "comment": "61 pages, 16 figures", "summary": "We show that deep Heaviside networks (DHNs) have limited expressiveness but\nthat this can be overcome by including either skip connections or neurons with\nlinear activation. We provide lower and upper bounds for the\nVapnik-Chervonenkis (VC) dimensions and approximation rates of these network\nclasses. As an application, we derive statistical convergence rates for DHN\nfits in the nonparametric regression model.", "AI": {"tldr": "DHN\u7684\u8868\u8fbe\u529b\u6709\u9650\uff0c\u4f46\u901a\u8fc7\u5f15\u5165\u8df3\u8dc3\u8fde\u63a5\u6216\u7ebf\u6027\u6fc0\u6d3b\u795e\u7ecf\u5143\u53ef\u63d0\u5347\u3002\u8bba\u6587\u7ed9\u51fa\u4e86VC\u7ef4\u5ea6\u548c\u903c\u8fd1\u7387\u7684\u754c\u9650\uff0c\u5e76\u5728\u975e\u53c2\u6570\u56de\u5f52\u6a21\u578b\u4e2d\u63a8\u5bfc\u4e86\u7edf\u8ba1\u6536\u655b\u901f\u7387\u3002", "motivation": "\u7814\u7a76\u6df1\u5ea6Heaviside\u7f51\u7edc\uff08DHN\uff09\u7684\u8868\u8fbe\u80fd\u529b\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u901a\u8fc7\u7ed3\u6784\u6539\u8fdb\uff08\u5982\u8df3\u8dc3\u8fde\u63a5\u6216\u7ebf\u6027\u6fc0\u6d3b\uff09\u589e\u5f3a\u5176\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u63d0\u4f9b\u4e86DHN\u53ca\u5176\u53d8\u4f53\u7684VC\u7ef4\u5ea6\u548c\u903c\u8fd1\u7387\u7684\u4e0a\u4e0b\u754c\uff0c\u5e76\u5728\u975e\u53c2\u6570\u56de\u5f52\u6a21\u578b\u4e2d\u9a8c\u8bc1\u5176\u7edf\u8ba1\u6536\u655b\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u8df3\u8dc3\u8fde\u63a5\u6216\u7ebf\u6027\u6fc0\u6d3b\u80fd\u663e\u8457\u63d0\u5347DHN\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u7ed9\u51fa\u4e86\u5176\u7406\u8bba\u6027\u80fd\u754c\u9650\u3002", "conclusion": "DHN\u5728\u7ed3\u6784\u4f18\u5316\u540e\u5177\u6709\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u975e\u53c2\u6570\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2505.00137", "pdf": "https://arxiv.org/pdf/2505.00137", "abs": "https://arxiv.org/abs/2505.00137", "authors": ["Rushikesh Ubale", "Sujan K. K.", "Sangram Deshpande", "Gregory T. Byrd"], "title": "Toward Practical Quantum Machine Learning: A Novel Hybrid Quantum LSTM for Fraud Detection", "categories": ["quant-ph", "cs.IT", "cs.LG", "math.IT"], "comment": "11 pages ,8 figures", "summary": "We present a novel hybrid quantum-classical neural network architecture for\nfraud detection that integrates a classical Long Short-Term Memory (LSTM)\nnetwork with a variational quantum circuit. By leveraging quantum phenomena\nsuch as superposition and entanglement, our model enhances the feature\nrepresentation of sequential transaction data, capturing complex non-linear\npatterns that are challenging for purely classical models. A comprehensive data\npreprocessing pipeline is employed to clean, encode, balance, and normalize a\ncredit card fraud dataset, ensuring a fair comparison with baseline models.\nNotably, our hybrid approach achieves per-epoch training times in the range of\n45-65 seconds, which is significantly faster than similar architectures\nreported in the literature, where training typically requires several minutes\nper epoch. Both classical and quantum gradients are jointly optimized via a\nunified backpropagation procedure employing the parameter-shift rule for the\nquantum parameters. Experimental evaluations demonstrate competitive\nimprovements in accuracy, precision, recall, and F1 score relative to a\nconventional LSTM baseline. These results underscore the promise of hybrid\nquantum-classical techniques in advancing the efficiency and performance of\nfraud detection systems.\n  Keywords: Hybrid Quantum-Classical Neural Networks, Quantum Computing, Fraud\nDetection, Hybrid Quantum LSTM, Variational Quantum Circuit, Parameter-Shift\nRule, Financial Risk Analysis", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u6b3a\u8bc8\u68c0\u6d4b\uff0c\u7ed3\u5408LSTM\u4e0e\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff0c\u5229\u7528\u91cf\u5b50\u7279\u6027\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u4e0e\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7ecf\u5178\u6a21\u578b\u96be\u4ee5\u6355\u6349\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u7684\u590d\u6742\u975e\u7ebf\u6027\u6a21\u5f0f\uff0c\u91cf\u5b50\u8ba1\u7b97\u53ef\u80fd\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u7279\u5f81\u8868\u793a\u4e0e\u5904\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528LSTM\u4e0e\u53d8\u5206\u91cf\u5b50\u7535\u8def\u7684\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u91cf\u5b50\u8d85\u53e0\u52a0\u548c\u7ea0\u7f20\u7279\u6027\uff0c\u7ed3\u5408\u6570\u636e\u9884\u5904\u7406\u6d41\u7a0b\u4e0e\u8054\u5408\u68af\u5ea6\u4f18\u5316\uff08\u53c2\u6570\u504f\u79fb\u89c4\u5219\uff09\u3002", "result": "\u8bad\u7ec3\u65f6\u95f4\u663e\u8457\u7f29\u77ed\uff0845-65\u79d2/\u8f6e\uff09\uff0c\u5728\u51c6\u786e\u7387\u3001\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u4f18\u4e8e\u7ecf\u5178LSTM\u57fa\u7ebf\u3002", "conclusion": "\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6280\u672f\u6709\u671b\u63d0\u5347\u6b3a\u8bc8\u68c0\u6d4b\u7cfb\u7edf\u7684\u6548\u7387\u4e0e\u6027\u80fd\uff0c\u4e3a\u91d1\u878d\u98ce\u9669\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.00195", "pdf": "https://arxiv.org/pdf/2505.00195", "abs": "https://arxiv.org/abs/2505.00195", "authors": ["Aditya Karan", "Nicholas Vincent", "Karrie Karahalios", "Hari Sundaram"], "title": "Algorithmic Collective Action with Two Collectives", "categories": ["cs.CY", "cs.GT", "cs.LG"], "comment": null, "summary": "Given that data-dependent algorithmic systems have become impactful in more\ndomains of life, the need for individuals to promote their own interests and\nhold algorithms accountable has grown. To have meaningful influence,\nindividuals must band together to engage in collective action. Groups that\nengage in such algorithmic collective action are likely to vary in size,\nmembership characteristics, and crucially, objectives. In this work, we\nintroduce a first of a kind framework for studying collective action with two\nor more collectives that strategically behave to manipulate data-driven\nsystems. With more than one collective acting on a system, unexpected\ninteractions may occur. We use this framework to conduct experiments with\nlanguage model-based classifiers and recommender systems where two collectives\neach attempt to achieve their own individual objectives. We examine how\ndiffering objectives, strategies, sizes, and homogeneity can impact a\ncollective's efficacy. We find that the unintentional interactions between\ncollectives can be quite significant; a collective acting in isolation may be\nable to achieve their objective (e.g., improve classification outcomes for\nthemselves or promote a particular item), but when a second collective acts\nsimultaneously, the efficacy of the first group drops by as much as $75\\%$. We\nfind that, in the recommender system context, neither fully heterogeneous nor\nfully homogeneous collectives stand out as most efficacious and that\nheterogeneity's impact is secondary compared to collective size. Our results\nsignal the need for more transparency in both the underlying algorithmic models\nand the different behaviors individuals or collectives may take on these\nsystems. This approach also allows collectives to hold algorithmic system\ndevelopers accountable and provides a framework for people to actively use\ntheir own data to promote their own interests.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7814\u7a76\u4e24\u4e2a\u6216\u66f4\u591a\u96c6\u4f53\u5728\u6570\u636e\u9a71\u52a8\u7cfb\u7edf\u4e2d\u6218\u7565\u6027\u884c\u4e3a\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u96c6\u4f53\u95f4\u7684\u4e92\u52a8\u4f1a\u663e\u8457\u5f71\u54cd\u5404\u81ea\u7684\u6548\u679c\uff0c\u5e76\u5f3a\u8c03\u4e86\u7b97\u6cd5\u900f\u660e\u5ea6\u548c\u96c6\u4f53\u884c\u4e3a\u591a\u6837\u6027\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740\u6570\u636e\u4f9d\u8d56\u7b97\u6cd5\u7cfb\u7edf\u5728\u66f4\u591a\u751f\u6d3b\u9886\u57df\u4ea7\u751f\u5f71\u54cd\uff0c\u4e2a\u4eba\u9700\u8981\u901a\u8fc7\u96c6\u4f53\u884c\u52a8\u4fdd\u62a4\u81ea\u8eab\u5229\u76ca\u5e76\u5bf9\u7b97\u6cd5\u95ee\u8d23\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u591a\u96c6\u4f53\u4e92\u52a8\u5bf9\u7b97\u6cd5\u7cfb\u7edf\u7684\u5f71\u54cd\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u6846\u67b6\u7814\u7a76\u591a\u96c6\u4f53\u5728\u8bed\u8a00\u6a21\u578b\u5206\u7c7b\u5668\u548c\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6218\u7565\u884c\u4e3a\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e0d\u540c\u76ee\u6807\u3001\u7b56\u7565\u3001\u89c4\u6a21\u548c\u540c\u8d28\u6027\u5bf9\u96c6\u4f53\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u96c6\u4f53\u95f4\u7684\u610f\u5916\u4e92\u52a8\u4f1a\u5bfc\u81f4\u5355\u4e2a\u96c6\u4f53\u7684\u6548\u679c\u4e0b\u964d\u9ad8\u8fbe75%\uff0c\u4e14\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u96c6\u4f53\u89c4\u6a21\u7684\u4f18\u5148\u7ea7\u9ad8\u4e8e\u540c\u8d28\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u7b97\u6cd5\u6a21\u578b\u548c\u96c6\u4f53\u884c\u4e3a\u900f\u660e\u5ea6\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u96c6\u4f53\u95ee\u8d23\u7b97\u6cd5\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5e76\u652f\u6301\u4e2a\u4eba\u5229\u7528\u6570\u636e\u4fc3\u8fdb\u81ea\u8eab\u5229\u76ca\u3002"}}
{"id": "2505.00209", "pdf": "https://arxiv.org/pdf/2505.00209", "abs": "https://arxiv.org/abs/2505.00209", "authors": ["Kelsey Allen", "Carl Doersch", "Guangyao Zhou", "Mohammed Suhail", "Danny Driess", "Ignacio Rocco", "Yulia Rubanova", "Thomas Kipf", "Mehdi S. M. Sajjadi", "Kevin Murphy", "Joao Carreira", "Sjoerd van Steenkiste"], "title": "Direct Motion Models for Assessing Generated Videos", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: http://trajan-paper.github.io", "summary": "A current limitation of video generative video models is that they generate\nplausible looking frames, but poor motion -- an issue that is not well captured\nby FVD and other popular methods for evaluating generated videos. Here we go\nbeyond FVD by developing a metric which better measures plausible object\ninteractions and motion. Our novel approach is based on auto-encoding point\ntracks and yields motion features that can be used to not only compare\ndistributions of videos (as few as one generated and one ground truth, or as\nmany as two datasets), but also for evaluating motion of single videos. We show\nthat using point tracks instead of pixel reconstruction or action recognition\nfeatures results in a metric which is markedly more sensitive to temporal\ndistortions in synthetic data, and can predict human evaluations of temporal\nconsistency and realism in generated videos obtained from open-source models\nbetter than a wide range of alternatives. We also show that by using a point\ntrack representation, we can spatiotemporally localize generative video\ninconsistencies, providing extra interpretability of generated video errors\nrelative to prior work. An overview of the results and link to the code can be\nfound on the project page: http://trajan-paper.github.io.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u70b9\u8f68\u8ff9\u81ea\u52a8\u7f16\u7801\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8fd0\u52a8\u8d28\u91cf\uff0c\u6bd4\u4f20\u7edfFVD\u65b9\u6cd5\u66f4\u654f\u611f\uff0c\u5e76\u80fd\u9884\u6d4b\u4eba\u7c7b\u5bf9\u89c6\u9891\u771f\u5b9e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u8bc4\u4ef7\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5e38\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u8fd0\u52a8\u8d28\u91cf\u5dee\u7684\u5e27\uff0c\u4f20\u7edf\u8bc4\u4ef7\u65b9\u6cd5\uff08\u5982FVD\uff09\u65e0\u6cd5\u6709\u6548\u6355\u6349\u8fd9\u4e00\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u597d\u7684\u6307\u6807\u6765\u8861\u91cf\u5bf9\u8c61\u4ea4\u4e92\u548c\u8fd0\u52a8\u7684\u5408\u7406\u6027\u3002", "method": "\u91c7\u7528\u70b9\u8f68\u8ff9\u81ea\u52a8\u7f16\u7801\u6280\u672f\uff0c\u63d0\u53d6\u8fd0\u52a8\u7279\u5f81\uff0c\u7528\u4e8e\u6bd4\u8f83\u89c6\u9891\u5206\u5e03\u6216\u5355\u89c6\u9891\u7684\u8fd0\u52a8\u8d28\u91cf\u3002\u76f8\u6bd4\u50cf\u7d20\u91cd\u5efa\u6216\u52a8\u4f5c\u8bc6\u522b\u7279\u5f81\uff0c\u70b9\u8f68\u8ff9\u5bf9\u65f6\u95f4\u5931\u771f\u66f4\u654f\u611f\u3002", "result": "\u65b0\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u4eba\u7c7b\u5bf9\u89c6\u9891\u771f\u5b9e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u8bc4\u4ef7\uff0c\u5e76\u80fd\u65f6\u7a7a\u5b9a\u4f4d\u89c6\u9891\u751f\u6210\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u4f9b\u66f4\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u57fa\u4e8e\u70b9\u8f68\u8ff9\u7684\u6307\u6807\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.00649", "pdf": "https://arxiv.org/pdf/2505.00649", "abs": "https://arxiv.org/abs/2505.00649", "authors": ["Marco Braga", "Pranav Kasela", "Alessandro Raganato", "Gabriella Pasi"], "title": "Investigating Task Arithmetic for Zero-Shot Information Retrieval", "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": "Accepted in SIGIR '25", "summary": "Large Language Models (LLMs) have shown impressive zero-shot performance\nacross a variety of Natural Language Processing tasks, including document\nre-ranking. However, their effectiveness degrades on unseen tasks and domains,\nlargely due to shifts in vocabulary and word distributions. In this paper, we\ninvestigate Task Arithmetic, a technique that combines the weights of LLMs\npre-trained on different tasks or domains via simple mathematical operations,\nsuch as addition or subtraction, to adapt retrieval models without requiring\nadditional fine-tuning. Our method is able to synthesize diverse tasks and\ndomain knowledge into a single model, enabling effective zero-shot adaptation\nin different retrieval contexts. Extensive experiments on publicly available\nscientific, biomedical, and multilingual datasets show that our method improves\nstate-of-the-art re-ranking performance by up to 18% in NDCG@10 and 15% in\nP@10. In addition to these empirical gains, our analysis provides insights into\nthe strengths and limitations of Task Arithmetic as a practical strategy for\nzero-shot learning and model adaptation. We make our code publicly available at\nhttps://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTask Arithmetic\u7684\u6280\u672f\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u6570\u5b66\u8fd0\u7b97\u7ed3\u5408\u4e0d\u540c\u4efb\u52a1\u6216\u9886\u57df\u7684LLMs\u6743\u91cd\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u5373\u53ef\u9002\u5e94\u68c0\u7d22\u4efb\u52a1\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u548c\u9886\u57df\u4e0a\u6027\u80fd\u4e0b\u964d\uff0c\u539f\u56e0\u662f\u8bcd\u6c47\u548c\u8bcd\u5206\u5e03\u7684\u5dee\u5f02\u3002\u4f5c\u8005\u65e8\u5728\u901a\u8fc7Task Arithmetic\u6280\u672f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u68c0\u7d22\u4efb\u52a1\u3002", "method": "Task Arithmetic\u901a\u8fc7\u6570\u5b66\u8fd0\u7b97\uff08\u5982\u52a0\u51cf\uff09\u7ed3\u5408\u9884\u8bad\u7ec3\u7684LLMs\u6743\u91cd\uff0c\u5c06\u591a\u6837\u5316\u7684\u4efb\u52a1\u548c\u9886\u57df\u77e5\u8bc6\u6574\u5408\u5230\u4e00\u4e2a\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u9002\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u79d1\u5b66\u3001\u751f\u7269\u533b\u5b66\u548c\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u7684NDCG@10\u548cP@10\u6307\u6807\u5206\u522b\u63d0\u5347\u4e8618%\u548c15%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "Task Arithmetic\u662f\u4e00\u79cd\u6709\u6548\u7684\u96f6\u6837\u672c\u5b66\u4e60\u548c\u6a21\u578b\u9002\u5e94\u7b56\u7565\uff0c\u5c3d\u7ba1\u5176\u6709\u5c40\u9650\u6027\uff0c\u4f46\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u4f5c\u8005\u516c\u5f00\u4e86\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.00229", "pdf": "https://arxiv.org/pdf/2505.00229", "abs": "https://arxiv.org/abs/2505.00229", "authors": ["Mark Adams", "Kamillo Ferry", "Ruriko Yoshida"], "title": "Inference for max-linear Bayesian networks with noise", "categories": ["stat.ML", "cs.LG", "math.OC", "math.ST", "stat.TH", "14T90, 62A09, 62H30, 90C20, 90C90"], "comment": "18 pages, 10 figures. Short version to appear in the proceedings of\n  the 13th Workshop on Uncertainty Processing", "summary": "Max-Linear Bayesian Networks (MLBNs) provide a powerful framework for causal\ninference in extreme-value settings; we consider MLBNs with noise parameters\nwith a given topology in terms of the max-plus algebra by taking its logarithm.\nThen, we show that an estimator of a parameter for each edge in a directed\nacyclic graph (DAG) is distributed normally. We end this paper with\ncomputational experiments with the expectation and maximization (EM) algorithm\nand quadratic optimization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5927\u7ebf\u6027\u8d1d\u53f6\u65af\u7f51\u7edc\uff08MLBN\uff09\u7684\u6781\u503c\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d6\u5bf9\u6570\u5c06\u7f51\u7edc\u53c2\u6570\u8f6c\u5316\u4e3amax-plus\u4ee3\u6570\u5f62\u5f0f\uff0c\u8bc1\u660e\u4e86\u53c2\u6570\u4f30\u8ba1\u91cf\u7684\u6b63\u6001\u5206\u5e03\u6027\u8d28\uff0c\u5e76\u901a\u8fc7EM\u7b97\u6cd5\u548c\u4e8c\u6b21\u4f18\u5316\u8fdb\u884c\u4e86\u8ba1\u7b97\u5b9e\u9a8c\u3002", "motivation": "\u7814\u7a76\u6781\u503c\u73af\u5883\u4e0b\u7684\u56e0\u679c\u63a8\u65ad\u95ee\u9898\uff0c\u5229\u7528MLBN\u6846\u67b6\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u5927\u6570\u636e\u6216\u6781\u7aef\u503c\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u5c06MLBN\u7f51\u7edc\u53c2\u6570\u8f6c\u5316\u4e3amax-plus\u4ee3\u6570\u5f62\u5f0f\uff0c\u901a\u8fc7\u5bf9\u6570\u53d8\u6362\u5206\u6790\u53c2\u6570\u5206\u5e03\uff0c\u4f7f\u7528EM\u7b97\u6cd5\u548c\u4e8c\u6b21\u4f18\u5316\u8fdb\u884c\u53c2\u6570\u4f30\u8ba1\u3002", "result": "\u8bc1\u660e\u4e86DAG\u4e2d\u6bcf\u6761\u8fb9\u53c2\u6570\u7684\u4f30\u8ba1\u91cf\u670d\u4ece\u6b63\u6001\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u8ba1\u7b97\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "MLBN\u6846\u67b6\u5728\u6781\u503c\u56e0\u679c\u63a8\u65ad\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u53c2\u6570\u4f30\u8ba1\u7684\u7edf\u8ba1\u6027\u8d28\u548c\u8ba1\u7b97\u6548\u7387\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2505.00703", "pdf": "https://arxiv.org/pdf/2505.00703", "abs": "https://arxiv.org/abs/2505.00703", "authors": ["Dongzhi Jiang", "Ziyu Guo", "Renrui Zhang", "Zhuofan Zong", "Hao Li", "Le Zhuo", "Shilin Yan", "Pheng-Ann Heng", "Hongsheng Li"], "title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Project Page: https://github.com/CaraJ7/T2I-R1", "summary": "Recent advancements in large language models have demonstrated how\nchain-of-thought (CoT) and reinforcement learning (RL) can improve performance.\nHowever, applying such reasoning strategies to the visual generation domain\nremains largely unexplored. In this paper, we present T2I-R1, a novel\nreasoning-enhanced text-to-image generation model, powered by RL with a\nbi-level CoT reasoning process. Specifically, we identify two levels of CoT\nthat can be utilized to enhance different stages of generation: (1) the\nsemantic-level CoT for high-level planning of the prompt and (2) the\ntoken-level CoT for low-level pixel processing during patch-by-patch\ngeneration. To better coordinate these two levels of CoT, we introduce\nBiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes\nboth generation CoTs within the same training step. By applying our reasoning\nstrategies to the baseline model, Janus-Pro, we achieve superior performance\nwith 13% improvement on T2I-CompBench and 19% improvement on the WISE\nbenchmark, even surpassing the state-of-the-art model FLUX.1. Code is available\nat: https://github.com/CaraJ7/T2I-R1", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faT2I-R1\uff0c\u4e00\u79cd\u7ed3\u5408\u601d\u7ef4\u94fe\uff08CoT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u6587\u672c\u751f\u6210\u56fe\u50cf\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u5c42CoT\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5c06\u601d\u7ef4\u94fe\u4e0e\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u89c6\u89c9\u751f\u6210\u9886\u57df\uff0c\u4ee5\u6539\u8fdb\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faT2I-R1\u6a21\u578b\uff0c\u91c7\u7528\u53cc\u5c42CoT\uff08\u8bed\u4e49\u7ea7\u548c\u4ee4\u724c\u7ea7\uff09\u7ed3\u5408BiCoT-GRPO\u7b97\u6cd5\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728T2I-CompBench\u548cWISE\u57fa\u51c6\u4e0a\u5206\u522b\u63d0\u534713%\u548c19%\uff0c\u8d85\u8d8a\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578bFLUX\u3002", "conclusion": "T2I-R1\u901a\u8fc7\u53cc\u5c42CoT\u4e0eRL\u7684\u7ed3\u5408\u6709\u6548\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u548c\u6027\u80fd\u3002"}}
{"id": "2505.00233", "pdf": "https://arxiv.org/pdf/2505.00233", "abs": "https://arxiv.org/abs/2505.00233", "authors": ["Kimihiro Yamazaki", "Takuya Konishi", "Yoshinobu Kawahara"], "title": "Explorative Curriculum Learning for Strongly Correlated Electron Systems", "categories": ["cond-mat.str-el", "cs.LG"], "comment": null, "summary": "Recent advances in neural network quantum states (NQS) have enabled\nhigh-accuracy predictions for complex quantum many-body systems such as\nstrongly correlated electron systems. However, the computational cost remains\nprohibitive, making exploration of the diverse parameters of interaction\nstrengths and other physical parameters inefficient. While transfer learning\nhas been proposed to mitigate this challenge, achieving generalization to\nlarge-scale systems and diverse parameter regimes remains difficult. To address\nthis limitation, we propose a novel curriculum learning framework based on\ntransfer learning for NQS. This facilitates efficient and stable exploration\nacross a vast parameter space of quantum many-body systems. In addition, by\ninterpreting NQS transfer learning through a perturbative lens, we demonstrate\nhow prior physical knowledge can be flexibly incorporated into the curriculum\nlearning process. We also propose Pairing-Net, an architecture to practically\nimplement this strategy for strongly correlated electron systems, and\nempirically verify its effectiveness. Our results show an approximately\n200-fold speedup in computation and a marked improvement in optimization\nstability compared to conventional methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u63a2\u7d22\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u53c2\u6570\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7Pairing-Net\u67b6\u6784\u5728\u5f3a\u5173\u8054\u7535\u5b50\u7cfb\u7edf\u4e2d\u5b9e\u73b0\uff0c\u83b7\u5f97\u4e86\u7ea6200\u500d\u7684\u8ba1\u7b97\u52a0\u901f\u548c\u4f18\u5316\u7a33\u5b9a\u6027\u63d0\u5347\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u7684\u91cf\u5b50\u6001\uff08NQS\uff09\u5728\u5904\u7406\u590d\u6742\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u5927\u89c4\u6a21\u7cfb\u7edf\u548c\u591a\u6837\u5316\u53c2\u6570\u533a\u57df\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7Pairing-Net\u67b6\u6784\u5728\u5f3a\u5173\u8054\u7535\u5b50\u7cfb\u7edf\u4e2d\u5b9e\u9645\u5e94\u7528\u3002", "result": "\u5b9e\u73b0\u4e86\u7ea6200\u500d\u7684\u8ba1\u7b97\u52a0\u901f\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u4f18\u5316\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u6548\u63a2\u7d22\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u5927\u53c2\u6570\u7a7a\u95f4\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u4e3a\u7269\u7406\u77e5\u8bc6\u7684\u7075\u6d3b\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.00409", "pdf": "https://arxiv.org/pdf/2505.00409", "abs": "https://arxiv.org/abs/2505.00409", "authors": ["Soroosh Tayebi Arasteh", "Saba Afza", "Tri-Thien Nguyen", "Lukas Buess", "Maryam Parvin", "Tomas Arias-Vergara", "Paula Andrea Perez-Toro", "Hiu Ching Hung", "Mahshad Lotfinia", "Thomas Gorges", "Elmar Noeth", "Maria Schuster", "Seung Hee Yang", "Andreas Maier"], "title": "Perceptual Implications of Automatic Anonymization in Pathological Speech", "categories": ["eess.AS", "cs.AI", "cs.LG"], "comment": null, "summary": "Automatic anonymization techniques are essential for ethical sharing of\npathological speech data, yet their perceptual consequences remain\nunderstudied. This study presents the first comprehensive human-centered\nanalysis of anonymized pathological speech, using a structured perceptual\nprotocol involving ten native and non-native German listeners with diverse\nlinguistic, clinical, and technical backgrounds. Listeners evaluated\nanonymized-original utterance pairs from 180 speakers spanning Cleft Lip and\nPalate, Dysarthria, Dysglossia, Dysphonia, and age-matched healthy controls.\nSpeech was anonymized using state-of-the-art automatic methods (equal error\nrates in the range of 30-40%). Listeners completed Turing-style discrimination\nand quality rating tasks under zero-shot (single-exposure) and few-shot\n(repeated-exposure) conditions. Discrimination accuracy was high overall (91%\nzero-shot; 93% few-shot), but varied by disorder (repeated-measures ANOVA:\np=0.007), ranging from 96% (Dysarthria) to 86% (Dysphonia). Anonymization\nconsistently reduced perceived quality (from 83% to 59%, p<0.001), with\npathology-specific degradation patterns (one-way ANOVA: p=0.005). Native\nlisteners rated original speech slightly higher than non-native listeners\n(Delta=4%, p=0.199), but this difference nearly disappeared after anonymization\n(Delta=1%, p=0.724). No significant gender-based bias was observed. Critically,\nhuman perceptual outcomes did not correlate with automatic privacy or clinical\nutility metrics. These results underscore the need for listener-informed,\ndisorder- and context-specific anonymization strategies that preserve privacy\nwhile maintaining interpretability, communicative functions, and diagnostic\nutility, especially for vulnerable populations such as children.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u7efc\u5408\u5206\u6790\u4e86\u75c5\u7406\u8bed\u97f3\u533f\u540d\u5316\u7684\u611f\u77e5\u6548\u679c\uff0c\u53d1\u73b0\u533f\u540d\u5316\u867d\u80fd\u6709\u6548\u4fdd\u62a4\u9690\u79c1\uff08\u51c6\u786e\u738790%+\uff09\uff0c\u4f46\u4f1a\u663e\u8457\u964d\u4f4e\u8bed\u97f3\u8d28\u91cf\uff08\u4ece83%\u964d\u81f359%\uff09\uff0c\u4e14\u6548\u679c\u56e0\u8bed\u97f3\u969c\u788d\u7c7b\u578b\u800c\u5f02\u3002", "motivation": "\u63a2\u7d22\u75c5\u7406\u8bed\u97f3\u533f\u540d\u5316\u6280\u672f\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5bf9\u8bed\u97f3\u8d28\u91cf\u548c\u8bca\u65ad\u6548\u7528\u7684\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u533f\u540d\u5316\u7b56\u7565\u63d0\u4f9b\u4eba\u672c\u5316\u8bbe\u8ba1\u4f9d\u636e\u3002", "method": "\u91c7\u752810\u540d\u5fb7\u8bed\u6bcd\u8bed\u4e0e\u975e\u6bcd\u8bed\u542c\u8005\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u7684\u56fe\u7075\u6d4b\u8bd5\u4e0e\u8d28\u91cf\u8bc4\u5206\u4efb\u52a1\uff0c\u8bc4\u4f30180\u540d\u4e0d\u540c\u75c5\u7406\u7c7b\u578b\uff08\u5982\u6784\u97f3\u969c\u788d\u3001\u55d3\u97f3\u969c\u788d\u7b49\uff09\u548c\u5065\u5eb7\u5bf9\u7167\u8005\u7684\u533f\u540d\u5316-\u539f\u59cb\u8bed\u97f3\u5bf9\u3002", "result": "\u533f\u540d\u5316\u8bc6\u522b\u51c6\u786e\u7387\u9ad8\uff08\u96f6\u6837\u672c91%\uff0c\u5c11\u6837\u672c93%\uff09\uff0c\u4f46\u8d28\u91cf\u8bc4\u5206\u663e\u8457\u4e0b\u964d\uff08p<0.001\uff09\uff0c\u4e14\u4e0d\u540c\u75c5\u7406\u7c7b\u578b\u4e0b\u964d\u7a0b\u5ea6\u4e0d\u540c\uff08p=0.005\uff09\u3002\u6bcd\u8bed\u4e0e\u975e\u6bcd\u8bed\u542c\u8005\u8bc4\u5206\u5dee\u5f02\u5728\u533f\u540d\u5316\u540e\u51e0\u4e4e\u6d88\u5931\u3002", "conclusion": "\u9700\u5f00\u53d1\u8003\u8651\u542c\u8005\u611f\u77e5\u3001\u7279\u5b9a\u75c5\u7406\u7c7b\u578b\u548c\u8bed\u5883\u7684\u533f\u540d\u5316\u65b9\u6cd5\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u517c\u987e\u8bed\u97f3\u53ef\u7406\u89e3\u6027\u3001\u6c9f\u901a\u529f\u80fd\u548c\u8bca\u65ad\u4ef7\u503c\u3002"}}
{"id": "2505.00237", "pdf": "https://arxiv.org/pdf/2505.00237", "abs": "https://arxiv.org/abs/2505.00237", "authors": ["Ze Zhang", "Georg Hess", "Junjie Hu", "Emmanuel Dean", "Lennart Svensson", "Knut \u00c5kesson"], "title": "Future-Oriented Navigation: Dynamic Obstacle Avoidance with One-Shot Energy-Based Multimodal Motion Prediction", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": "Submitted to IEEE RA-L", "summary": "This paper proposes an integrated approach for the safe and efficient control\nof mobile robots in dynamic and uncertain environments. The approach consists\nof two key steps: one-shot multimodal motion prediction to anticipate motions\nof dynamic obstacles and model predictive control to incorporate these\npredictions into the motion planning process. Motion prediction is driven by an\nenergy-based neural network that generates high-resolution, multi-step\npredictions in a single operation. The prediction outcomes are further utilized\nto create geometric shapes formulated as mathematical constraints. Instead of\ntreating each dynamic obstacle individually, predicted obstacles are grouped by\nproximity in an unsupervised way to improve performance and efficiency. The\noverall collision-free navigation is handled by model predictive control with a\nspecific design for proactive dynamic obstacle avoidance. The proposed approach\nallows mobile robots to navigate effectively in dynamic environments. Its\nperformance is accessed across various scenarios that represent typical\nwarehouse settings. The results demonstrate that the proposed approach\noutperforms other existing dynamic obstacle avoidance methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u5b89\u5168\u9ad8\u6548\u5730\u63a7\u5236\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u8fd0\u52a8\u9884\u6d4b\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u5e76\u5728\u591a\u79cd\u573a\u666f\u4e2d\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u662f\u4e00\u9879\u590d\u6742\u4efb\u52a1\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6548\u7387\u548c\u5b89\u5168\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u96c6\u6210\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u8fd0\u52a8\u9884\u6d4b\uff08\u57fa\u4e8e\u80fd\u91cf\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u9884\u6d4b\uff09\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5206\u7ec4\u4f18\u5316\u969c\u788d\u7269\u5904\u7406\u6548\u7387\u3002", "result": "\u5728\u5178\u578b\u4ed3\u5e93\u573a\u666f\u4e2d\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u969c\u788d\u7269\u907f\u969c\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u96c6\u6210\u65b9\u6cd5\u5b9e\u73b0\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u79fb\u52a8\u673a\u5668\u4eba\u7684\u9ad8\u6548\u5b89\u5168\u5bfc\u822a\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.00455", "pdf": "https://arxiv.org/pdf/2505.00455", "abs": "https://arxiv.org/abs/2505.00455", "authors": ["Sungbok Shin", "Hyeon Jeon", "Sanghyun Hong", "Niklas Elmqvist"], "title": "Data Therapist: Eliciting Domain Knowledge from Subject Matter Experts Using Large Language Models", "categories": ["cs.HC", "cs.AI"], "comment": "Submitted to IEEE VIS2025", "summary": "Effective data visualization requires not only technical proficiency but also\na deep understanding of the domain-specific context in which data exists. This\ncontext often includes tacit knowledge about data provenance, quality, and\nintended use, which is rarely explicit in the dataset itself. We present the\nData Therapist, a web-based tool that helps domain experts externalize this\nimplicit knowledge through a mixed-initiative process combining iterative Q&A\nwith interactive annotation. Powered by a large language model, the system\nanalyzes user-supplied datasets, prompts users with targeted questions, and\nallows annotation at varying levels of granularity. The resulting structured\nknowledge base can inform both human and automated visualization design. We\nevaluated the tool in a qualitative study involving expert pairs from Molecular\nBiology, Accounting, Political Science, and Usable Security. The study revealed\nrecurring patterns in how experts reason about their data and highlights areas\nwhere AI support can improve visualization design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u6b3e\u540d\u4e3a\u2018Data Therapist\u2019\u7684\u57fa\u4e8e\u7f51\u7edc\u5de5\u5177\uff0c\u5e2e\u52a9\u9886\u57df\u4e13\u5bb6\u901a\u8fc7\u95ee\u7b54\u548c\u4ea4\u4e92\u5f0f\u6ce8\u91ca\u5c06\u9690\u542b\u77e5\u8bc6\u5916\u663e\u5316\uff0c\u4ee5\u6539\u5584\u6570\u636e\u53ef\u89c6\u5316\u7684\u8bbe\u8ba1\u3002", "motivation": "\u6570\u636e\u53ef\u89c6\u5316\u4e0d\u4ec5\u9700\u8981\u6280\u672f\u719f\u7ec3\uff0c\u8fd8\u9700\u7406\u89e3\u6570\u636e\u7684\u9886\u57df\u80cc\u666f\u3002\u7136\u800c\uff0c\u6570\u636e\u96c6\u672c\u8eab\u901a\u5e38\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u9690\u542b\u77e5\u8bc6\uff08\u5982\u6570\u636e\u6765\u6e90\u3001\u8d28\u91cf\u548c\u7528\u9014\uff09\u7684\u660e\u786e\u8bb0\u5f55\u3002", "method": "\u5f00\u53d1\u4e86\u2018Data Therapist\u2019\u5de5\u5177\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u95ee\u7b54\u548c\u6ce8\u91ca\u5f15\u5bfc\u7528\u6237\u5916\u663e\u5316\u9690\u542b\u77e5\u8bc6\uff0c\u5f62\u6210\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u3002", "result": "\u5728\u5206\u5b50\u751f\u7269\u5b66\u3001\u4f1a\u8ba1\u3001\u653f\u6cbb\u79d1\u5b66\u548c\u53ef\u7528\u6027\u5b89\u5168\u7b49\u9886\u57df\u7684\u4e13\u5bb6\u7814\u7a76\u4e2d\uff0c\u5de5\u5177\u63ed\u793a\u4e86\u4e13\u5bb6\u5982\u4f55\u7406\u89e3\u6570\u636e\uff0c\u5e76\u5c55\u793a\u4e86AI\u652f\u6301\u53ef\u89c6\u5316\u8bbe\u8ba1\u7684\u6f5c\u529b\u3002", "conclusion": "\u2018Data Therapist\u2019\u901a\u8fc7\u6709\u6548\u5916\u663e\u5316\u9886\u57df\u4e13\u5bb6\u7684\u9690\u542b\u77e5\u8bc6\uff0c\u4e3a\u6570\u636e\u53ef\u89c6\u5316\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u652f\u6301\u548c\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2505.00242", "pdf": "https://arxiv.org/pdf/2505.00242", "abs": "https://arxiv.org/abs/2505.00242", "authors": ["Shingo Higashiguchi", "Yasuko Matsubara", "Koki Kawabata", "Taichi Murayama", "Yasushi Sakurai"], "title": "D-Tracker: Modeling Interest Diffusion in Social Activity Tensor Data Streams", "categories": ["cs.SI", "cs.LG"], "comment": "ACM SIGKDD 2025 (KDD2025)", "summary": "Large quantities of social activity data, such as weekly web search volumes\nand the number of new infections with infectious diseases, reflect peoples'\ninterests and activities. It is important to discover temporal patterns from\nsuch data and to forecast future activities accurately. However, modeling and\nforecasting social activity data streams is difficult because they are\nhigh-dimensional and composed of multiple time-varying dynamics such as trends,\nseasonality, and interest diffusion. In this paper, we propose D-Tracker, a\nmethod for continuously capturing time-varying temporal patterns within social\nactivity tensor data streams and forecasting future activities. Our proposed\nmethod has the following properties: (a) Interpretable: it incorporates the\npartial differential equation into a tensor decomposition framework and\ncaptures time-varying temporal patterns such as trends, seasonality, and\ninterest diffusion between locations in an interpretable manner; (b) Automatic:\nit has no hyperparameters and continuously models tensor data streams fully\nautomatically; (c) Scalable: the computation time of D-Tracker is independent\nof the time series length. Experiments using web search volume data obtained\nfrom GoogleTrends, and COVID-19 infection data obtained from COVID-19 Open Data\nRepository show that our method can achieve higher forecasting accuracy in less\ncomputation time than existing methods while extracting the interest diffusion\nbetween locations. Our source code and datasets are available at\n{https://github.com/Higashiguchi-Shingo/D-Tracker.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faD-Tracker\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u9ad8\u7ef4\u793e\u4f1a\u6d3b\u52a8\u6570\u636e\u6d41\u4e2d\u6355\u6349\u65f6\u53d8\u65f6\u5e8f\u6a21\u5f0f\u5e76\u9884\u6d4b\u672a\u6765\u6d3b\u52a8\uff0c\u5177\u6709\u53ef\u89e3\u91ca\u6027\u3001\u81ea\u52a8\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u793e\u4f1a\u6d3b\u52a8\u6570\u636e\uff08\u5982\u7f51\u7edc\u641c\u7d22\u91cf\u548c\u4f20\u67d3\u75c5\u65b0\u611f\u67d3\u6570\uff09\u53cd\u6620\u4e86\u4eba\u4eec\u7684\u5174\u8da3\u548c\u6d3b\u52a8\uff0c\u4f46\u6b64\u7c7b\u6570\u636e\u56e0\u9ad8\u7ef4\u548c\u591a\u91cd\u65f6\u53d8\u52a8\u6001\uff08\u5982\u8d8b\u52bf\u3001\u5b63\u8282\u6027\u548c\u5174\u8da3\u6269\u6563\uff09\u800c\u96be\u4ee5\u5efa\u6a21\u548c\u9884\u6d4b\u3002", "method": "D-Tracker\u5c06\u504f\u5fae\u5206\u65b9\u7a0b\u878d\u5165\u5f20\u91cf\u5206\u89e3\u6846\u67b6\uff0c\u4ee5\u53ef\u89e3\u91ca\u65b9\u5f0f\u6355\u6349\u65f6\u53d8\u6a21\u5f0f\uff08\u5982\u8d8b\u52bf\u3001\u5b63\u8282\u6027\u53ca\u5730\u70b9\u95f4\u5174\u8da3\u6269\u6563\uff09\uff0c\u65e0\u9700\u8d85\u53c2\u6570\u4e14\u8ba1\u7b97\u65f6\u95f4\u4e0e\u65f6\u95f4\u5e8f\u5217\u957f\u5ea6\u65e0\u5173\u3002", "result": "\u5728GoogleTrends\u641c\u7d22\u91cf\u6570\u636e\u548cCOVID-19\u611f\u67d3\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cD-Tracker\u5728\u66f4\u77ed\u8ba1\u7b97\u65f6\u95f4\u5185\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u80fd\u63d0\u53d6\u5730\u70b9\u95f4\u5174\u8da3\u6269\u6563\u3002", "conclusion": "D-Tracker\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u52a8\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u793e\u4f1a\u6d3b\u52a8\u6570\u636e\u6d41\u7684\u65f6\u5e8f\u5efa\u6a21\u4e0e\u9884\u6d4b\u3002"}}
{"id": "2505.00282", "pdf": "https://arxiv.org/pdf/2505.00282", "abs": "https://arxiv.org/abs/2505.00282", "authors": ["Jacob Carlson", "Melissa Dell"], "title": "A Unifying Framework for Robust and Efficient Inference with Unstructured Data", "categories": ["econ.EM", "cs.LG"], "comment": null, "summary": "This paper presents a general framework for conducting efficient and robust\ninference on parameters derived from unstructured data, which include text,\nimages, audio, and video. Economists have long incorporated data extracted from\ntexts and images into their analyses, a practice that has accelerated with\nadvancements in deep neural networks. However, neural networks do not\ngenerically produce unbiased predictions, potentially propagating bias to\nestimators that use their outputs. To address this challenge, we reframe\ninference with unstructured data as a missing structured data problem, where\nstructured data are imputed from unstructured inputs using deep neural\nnetworks. This perspective allows us to apply classic results from\nsemiparametric inference, yielding valid, efficient, and robust estimators\nbased on unstructured data. We formalize this approach with MARS (Missing At\nRandom Structured Data), a unifying framework that integrates and extends\nexisting methods for debiased inference using machine learning predictions,\nlinking them to a variety of older, familiar problems such as causal inference.\nWe develop robust and efficient estimators for both descriptive and causal\nestimands and address challenges such as inference using aggregated and\ntransformed predictions from unstructured data. Importantly, MARS applies to\ncommon empirical settings that have received limited attention in the existing\nliterature. Finally, we reanalyze prominent studies that use unstructured data,\ndemonstrating the practical value of MARS.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u6846\u67b6MARS\uff0c\u7528\u4e8e\u4ece\u975e\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u9ad8\u6548\u4e14\u7a33\u5065\u5730\u63a8\u65ad\u53c2\u6570\uff0c\u901a\u8fc7\u5c06\u975e\u7ed3\u6784\u5316\u6570\u636e\u91cd\u6784\u4e3a\u7f3a\u5931\u7684\u7ed3\u6784\u5316\u6570\u636e\u95ee\u9898\uff0c\u7ed3\u5408\u534a\u53c2\u6570\u63a8\u65ad\u7406\u8bba\uff0c\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4b\u4e2d\u7684\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u5728\u975e\u7ed3\u6784\u5316\u6570\u636e\uff08\u5982\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\uff09\u5206\u6790\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u9884\u6d4b\u53ef\u80fd\u5b58\u5728\u504f\u5dee\uff0c\u8fdb\u800c\u5f71\u54cd\u57fa\u4e8e\u8fd9\u4e9b\u8f93\u51fa\u7684\u4f30\u8ba1\u91cf\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u975e\u7ed3\u6784\u5316\u6570\u636e\u89c6\u4e3a\u7f3a\u5931\u7684\u7ed3\u6784\u5316\u6570\u636e\u95ee\u9898\uff0c\u5e76\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u586b\u5145\uff0c\u7ed3\u5408\u534a\u53c2\u6570\u63a8\u65ad\u7406\u8bba\uff0c\u63d0\u51fa\u4e86MARS\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7a33\u5065\u4e14\u9ad8\u6548\u7684\u4f30\u8ba1\u91cf\u3002", "result": "MARS\u6846\u67b6\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u5bf9\u63cf\u8ff0\u6027\u548c\u56e0\u679c\u6027\u4f30\u8ba1\u91cf\u7684\u7a33\u5065\u9ad8\u6548\u4f30\u8ba1\uff0c\u8fd8\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u732e\u4e2d\u672a\u5145\u5206\u5173\u6ce8\u7684\u5e38\u89c1\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "MARS\u6846\u67b6\u901a\u8fc7\u6574\u5408\u548c\u6269\u5c55\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u975e\u7ed3\u6784\u5316\u6570\u636e\u63a8\u65ad\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u4ef7\u503c\u3002"}}
{"id": "2505.00482", "pdf": "https://arxiv.org/pdf/2505.00482", "abs": "https://arxiv.org/abs/2505.00482", "authors": ["Kwon Byung-Ki", "Qi Dai", "Lee Hyoseok", "Chong Luo", "Tae-Hyun Oh"], "title": "JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present JointDiT, a diffusion transformer that models the joint\ndistribution of RGB and depth. By leveraging the architectural benefit and\noutstanding image prior of the state-of-the-art diffusion transformer, JointDiT\nnot only generates high-fidelity images but also produces geometrically\nplausible and accurate depth maps. This solid joint distribution modeling is\nachieved through two simple yet effective techniques that we propose, i.e.,\nadaptive scheduling weights, which depend on the noise levels of each modality,\nand the unbalanced timestep sampling strategy. With these techniques, we train\nour model across all noise levels for each modality, enabling JointDiT to\nnaturally handle various combinatorial generation tasks, including joint\ngeneration, depth estimation, and depth-conditioned image generation by simply\ncontrolling the timestep of each branch. JointDiT demonstrates outstanding\njoint generation performance. Furthermore, it achieves comparable results in\ndepth estimation and depth-conditioned image generation, suggesting that joint\ndistribution modeling can serve as a replaceable alternative to conditional\ngeneration. The project page is available at\nhttps://byungki-k.github.io/JointDiT/.", "AI": {"tldr": "JointDiT\u662f\u4e00\u79cd\u6269\u6563\u53d8\u538b\u5668\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u5ea6\u6743\u91cd\u548c\u4e0d\u5e73\u8861\u65f6\u95f4\u6b65\u91c7\u6837\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86RGB\u548c\u6df1\u5ea6\u7684\u8054\u5408\u5206\u5e03\u5efa\u6a21\uff0c\u4e0d\u4ec5\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u8fd8\u80fd\u751f\u6210\u51e0\u4f55\u4e0a\u51c6\u786e\u4e14\u5408\u7406\u7684\u6df1\u5ea6\u56fe\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5229\u7528\u5148\u8fdb\u7684\u6269\u6563\u53d8\u538b\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u8054\u5408RGB\u548c\u6df1\u5ea6\u7684\u5206\u5e03\u5efa\u6a21\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u751f\u6210\u4e0e\u51e0\u4f55\u51c6\u786e\u7684\u6df1\u5ea6\u56fe\u751f\u6210\uff0c\u540c\u65f6\u63a2\u7d22\u8054\u5408\u5206\u5e03\u5efa\u6a21\u5728\u591a\u4efb\u52a1\u751f\u6210\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u5173\u952e\u6280\u672f\uff1a\u81ea\u9002\u5e94\u8c03\u5ea6\u6743\u91cd\uff08\u6839\u636e\u6bcf\u79cd\u6a21\u6001\u7684\u566a\u58f0\u6c34\u5e73\u8c03\u6574\u6743\u91cd\uff09\u548c\u4e0d\u5e73\u8861\u65f6\u95f4\u6b65\u91c7\u6837\u7b56\u7565\uff08\u5206\u522b\u4e3a\u4e0d\u540c\u6a21\u6001\u8bad\u7ec3\u6240\u6709\u566a\u58f0\u6c34\u5e73\uff09\uff0c\u4ece\u800c\u5728\u4e00\u4e2a\u6a21\u578b\u4e2d\u540c\u65f6\u5904\u7406\u8054\u5408\u751f\u6210\u3001\u6df1\u5ea6\u4f30\u8ba1\u548c\u6df1\u5ea6\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u3002", "result": "JointDiT\u5728\u8054\u5408\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5728\u6df1\u5ea6\u4f30\u8ba1\u548c\u6df1\u5ea6\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u53ef\u6bd4\u8f83\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u8054\u5408\u5206\u5e03\u5efa\u6a21\u662f\u6761\u4ef6\u751f\u6210\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "JointDiT\u901a\u8fc7\u7b80\u5355\u800c\u6709\u6548\u7684\u6280\u672f\u5b9e\u73b0\u4e86RGB\u548c\u6df1\u5ea6\u7684\u8054\u5408\u5206\u5e03\u5efa\u6a21\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u591a\u4efb\u52a1\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7684\u8054\u5408\u6a21\u6001\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2505.00299", "pdf": "https://arxiv.org/pdf/2505.00299", "abs": "https://arxiv.org/abs/2505.00299", "authors": ["Yang Wang", "Tengda Tang", "Zhou Fang", "Yingnan Deng", "Yifei Duan"], "title": "Intelligent Task Scheduling for Microservices via A3C-Based Reinforcement Learning", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "To address the challenges of high resource dynamism and intensive task\nconcurrency in microservice systems, this paper proposes an adaptive resource\nscheduling method based on the A3C reinforcement learning algorithm. The\nscheduling problem is modeled as a Markov Decision Process, where policy and\nvalue networks are jointly optimized to enable fine-grained resource allocation\nunder varying load conditions. The method incorporates an asynchronous\nmulti-threaded learning mechanism, allowing multiple agents to perform parallel\nsampling and synchronize updates to the global network parameters. This design\nimproves both policy convergence efficiency and model stability. In the\nexperimental section, a real-world dataset is used to construct a scheduling\nscenario. The proposed method is compared with several typical approaches\nacross multiple evaluation metrics, including task delay, scheduling success\nrate, resource utilization, and convergence speed. The results show that the\nproposed method delivers high scheduling performance and system stability in\nmulti-task concurrent environments. It effectively alleviates the resource\nallocation bottlenecks faced by traditional methods under heavy load,\ndemonstrating its practical value for intelligent scheduling in microservice\nsystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eA3C\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u81ea\u9002\u5e94\u8d44\u6e90\u8c03\u5ea6\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5fae\u670d\u52a1\u7cfb\u7edf\u4e2d\u9ad8\u8d44\u6e90\u52a8\u6001\u6027\u548c\u9ad8\u4efb\u52a1\u5e76\u53d1\u6027\u7684\u6311\u6218\u3002\u901a\u8fc7\u5f02\u6b65\u591a\u7ebf\u7a0b\u5b66\u4e60\u673a\u5236\u4f18\u5316\u8d44\u6e90\u5206\u914d\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u5e76\u53d1\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5fae\u670d\u52a1\u7cfb\u7edf\u9762\u4e34\u9ad8\u8d44\u6e90\u52a8\u6001\u6027\u548c\u4efb\u52a1\u5e76\u53d1\u6027\u6311\u6218\uff0c\u4f20\u7edf\u8c03\u5ea6\u65b9\u6cd5\u5728\u8d44\u6e90\u5206\u914d\u4e0a\u5b58\u5728\u74f6\u9888\uff0c\u96be\u4ee5\u9002\u5e94\u8d1f\u8f7d\u53d8\u5316\u3002", "method": "\u5c06\u8c03\u5ea6\u95ee\u9898\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u91c7\u7528A3C\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u5f02\u6b65\u591a\u7ebf\u7a0b\u5b66\u4e60\u673a\u5236\uff0c\u4f18\u5316\u7b56\u7565\u4e0e\u4ef7\u503c\u7f51\u7edc\u7684\u8054\u5408\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u5ef6\u8fdf\u3001\u8c03\u5ea6\u6210\u529f\u7387\u3001\u8d44\u6e90\u5229\u7528\u7387\u548c\u6536\u655b\u901f\u5ea6\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u8c03\u5ea6\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u8d44\u6e90\u8c03\u5ea6\u5728\u9ad8\u538b\u73af\u5883\u4e0b\u7684\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u5fae\u670d\u52a1\u7cfb\u7edf\u7684\u667a\u80fd\u8c03\u5ea6\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00487", "pdf": "https://arxiv.org/pdf/2505.00487", "abs": "https://arxiv.org/abs/2505.00487", "authors": ["Leonid Legashev", "Artur Zhigalov", "Denis Parfenov"], "title": "Analysis of the vulnerability of machine learning regression models to adversarial attacks using data from 5G wireless networks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "This article describes the process of creating a script and conducting an\nanalytical study of a dataset using the DeepMIMO emulator. An advertorial\nattack was carried out using the FGSM method to maximize the gradient. A\ncomparison is made of the effectiveness of binary classifiers in the task of\ndetecting distorted data. The dynamics of changes in the quality indicators of\nthe regression model were analyzed in conditions without adversarial attacks,\nduring an adversarial attack and when the distorted data was isolated. It is\nshown that an adversarial FGSM attack with gradient maximization leads to an\nincrease in the value of the MSE metric by 33% and a decrease in the R2\nindicator by 10% on average. The LightGBM binary classifier effectively\nidentifies data with adversarial anomalies with 98% accuracy. Regression\nmachine learning models are susceptible to adversarial attacks, but rapid\nanalysis of network traffic and data transmitted over the network makes it\npossible to identify malicious activity", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528FGSM\u65b9\u6cd5\u5b9e\u65bd\u5bf9\u6297\u6027\u653b\u51fb\u5bf9\u56de\u5f52\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u5e76\u8bc4\u4f30\u4e86LightGBM\u5206\u7c7b\u5668\u5728\u68c0\u6d4b\u5bf9\u6297\u6027\u5f02\u5e38\u6570\u636e\u65f6\u7684\u6548\u679c\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5bf9\u6297\u6027\u653b\u51fb\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4f46LightGBM\u80fd\u9ad8\u6548\u8bc6\u522b\u5f02\u5e38\u6570\u636e\u3002", "motivation": "\u7814\u7a76\u5bf9\u6297\u6027\u653b\u51fb\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u6709\u6548\u68c0\u6d4b\u5bf9\u6297\u6027\u5f02\u5e38\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528DeepMIMO\u6a21\u62df\u5668\u751f\u6210\u6570\u636e\u96c6\uff0c\u5e94\u7528FGSM\u65b9\u6cd5\u5b9e\u65bd\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u5206\u6790\u56de\u5f52\u6a21\u578b\u5728\u6709\u65e0\u653b\u51fb\u4e0b\u7684\u6027\u80fd\u53d8\u5316\uff0c\u5e76\u8bc4\u4f30LightGBM\u5206\u7c7b\u5668\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6548\u679c\u3002", "result": "FGSM\u653b\u51fb\u4f7fMSE\u6307\u6807\u589e\u52a033%\u3001R2\u6307\u6807\u4e0b\u964d10%\uff0c\u800cLightGBM\u5206\u7c7b\u5668\u80fd\u4ee598%\u7684\u51c6\u786e\u7387\u8bc6\u522b\u5bf9\u6297\u6027\u5f02\u5e38\u6570\u636e\u3002", "conclusion": "\u56de\u5f52\u6a21\u578b\u6613\u53d7\u5bf9\u6297\u6027\u653b\u51fb\u5f71\u54cd\uff0c\u4f46\u901a\u8fc7\u9ad8\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff08\u5982LightGBM\uff09\u53ef\u4ee5\u8bc6\u522b\u5e76\u7f13\u89e3\u6b64\u7c7b\u5a01\u80c1\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u5b89\u5168\u6027\u3002"}}
{"id": "2505.00304", "pdf": "https://arxiv.org/pdf/2505.00304", "abs": "https://arxiv.org/abs/2505.00304", "authors": ["Yuhan Li", "Eugene Han", "Yifan Hu", "Wenzhuo Zhou", "Zhengling Qi", "Yifan Cui", "Ruoqing Zhu"], "title": "Reinforcement Learning with Continuous Actions Under Unmeasured Confounding", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "This paper addresses the challenge of offline policy learning in\nreinforcement learning with continuous action spaces when unmeasured\nconfounders are present. While most existing research focuses on policy\nevaluation within partially observable Markov decision processes (POMDPs) and\nassumes discrete action spaces, we advance this field by establishing a novel\nidentification result to enable the nonparametric estimation of policy value\nfor a given target policy under an infinite-horizon framework. Leveraging this\nidentification, we develop a minimax estimator and introduce a\npolicy-gradient-based algorithm to identify the in-class optimal policy that\nmaximizes the estimated policy value. Furthermore, we provide theoretical\nresults regarding the consistency, finite-sample error bound, and regret bound\nof the resulting optimal policy. Extensive simulations and a real-world\napplication using the German Family Panel data demonstrate the effectiveness of\nour proposed methodology.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u5904\u7406\u672a\u6d4b\u91cf\u6df7\u6742\u7684\u79bb\u7ebf\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u53c2\u6570\u4f30\u8ba1\u7b56\u7565\u503c\u5e76\u5f00\u53d1\u6700\u5c0f\u5316\u4f30\u8ba1\u5668\uff0c\u7ed3\u5408\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDPs\uff09\u548c\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u7684\u7b56\u7565\u8bc4\u4f30\uff0c\u800c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u672a\u6d4b\u91cf\u6df7\u6742\u7684\u79bb\u7ebf\u7b56\u7565\u5b66\u4e60\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u65b0\u7684\u8bc6\u522b\u7ed3\u679c\u5b9e\u73b0\u7b56\u7565\u503c\u7684\u975e\u53c2\u6570\u4f30\u8ba1\uff0c\u5f00\u53d1\u6700\u5c0f\u5316\u4f30\u8ba1\u5668\u548c\u57fa\u4e8e\u7b56\u7565\u68af\u5ea6\u7684\u7b97\u6cd5\u4ee5\u627e\u5230\u6700\u4f18\u7b56\u7565\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u5206\u6790\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u6700\u4f18\u7b56\u7565\u7684\u4e00\u81f4\u6027\u3001\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\u548c\u9057\u61be\u754c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u6a21\u62df\u548c\u5fb7\u56fd\u5bb6\u5ead\u9762\u677f\u6570\u636e\u7684\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e14\u5b58\u5728\u672a\u6d4b\u91cf\u6df7\u6742\u7684\u60c5\u51b5\u4e0b\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002"}}
{"id": "2505.00488", "pdf": "https://arxiv.org/pdf/2505.00488", "abs": "https://arxiv.org/abs/2505.00488", "authors": ["Vamshi Kumar Kurva", "Shishir Kolathaya"], "title": "MULE: Multi-terrain and Unknown Load Adaptation for Effective Quadrupedal Locomotion", "categories": ["cs.RO", "cs.AI"], "comment": "Preprint under review", "summary": "Quadrupedal robots are increasingly deployed for load-carrying tasks across\ndiverse terrains. While Model Predictive Control (MPC)-based methods can\naccount for payload variations, they often depend on predefined gait schedules\nor trajectory generators, limiting their adaptability in unstructured\nenvironments. To address these limitations, we propose an Adaptive\nReinforcement Learning (RL) framework that enables quadrupedal robots to\ndynamically adapt to both varying payloads and diverse terrains. The framework\nconsists of a nominal policy responsible for baseline locomotion and an\nadaptive policy that learns corrective actions to preserve stability and\nimprove command tracking under payload variations. We validate the proposed\napproach through large-scale simulation experiments in Isaac Gym and real-world\nhardware deployment on a Unitree Go1 quadruped. The controller was tested on\nflat ground, slopes, and stairs under both static and dynamic payload changes.\nAcross all settings, our adaptive controller consistently outperformed the\ncontroller in tracking body height and velocity commands, demonstrating\nenhanced robustness and adaptability without requiring explicit gait design or\nmanual tuning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8ba9\u56db\u8db3\u673a\u5668\u4eba\u80fd\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u8f7d\u8377\u548c\u5730\u5f62\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u6b65\u6001\u6216\u8f68\u8ff9\u751f\u6210\u5668\u3002", "motivation": "\u73b0\u6709MPC\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u6b65\u6001\u6216\u8f68\u8ff9\u751f\u6210\u5668\uff0c\u9650\u5236\u4e86\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u57fa\u7ebf\u8fd0\u52a8\u7684\u57fa\u7840\u7b56\u7565\u548c\u5b66\u4e60\u6821\u6b63\u52a8\u4f5c\u7684\u81ea\u9002\u5e94\u7b56\u7565\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u7269\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6846\u67b6\u5728\u591a\u79cd\u5730\u5f62\u548c\u8f7d\u8377\u53d8\u5316\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u53c2\u6216\u6b65\u6001\u8bbe\u8ba1\u3002"}}
{"id": "2505.00310", "pdf": "https://arxiv.org/pdf/2505.00310", "abs": "https://arxiv.org/abs/2505.00310", "authors": ["Maximilian Schuessler", "Erik Sverdrup", "Robert Tibshirani"], "title": "Statistical Learning for Heterogeneous Treatment Effects: Pretraining, Prognosis, and Prediction", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Robust estimation of heterogeneous treatment effects is a fundamental\nchallenge for optimal decision-making in domains ranging from personalized\nmedicine to educational policy. In recent years, predictive machine learning\nhas emerged as a valuable toolbox for causal estimation, enabling more flexible\neffect estimation. However, accurately estimating conditional average treatment\neffects (CATE) remains a major challenge, particularly in the presence of many\ncovariates. In this article, we propose pretraining strategies that leverages a\nphenomenon in real-world applications: factors that are prognostic of the\noutcome are frequently also predictive of treatment effect heterogeneity. In\nmedicine, for example, components of the same biological signaling pathways\nfrequently influence both baseline risk and treatment response. Specifically,\nwe demonstrate our approach within the R-learner framework, which estimates the\nCATE by solving individual prediction problems based on a residualized loss. We\nuse this structure to incorporate \"side information\" and develop models that\ncan exploit synergies between risk prediction and causal effect estimation. In\nsettings where these synergies are present, this cross-task learning enables\nmore accurate signal detection: yields lower estimation error, reduced false\ndiscovery rates, and higher power for detecting heterogeneity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5229\u7528\u9884\u540e\u56e0\u7d20\u901a\u5e38\u4e5f\u9884\u6d4b\u6cbb\u7597\u6548\u679c\u5f02\u8d28\u6027\u7684\u73b0\u8c61\uff0c\u7ed3\u5408R-learner\u6846\u67b6\uff0c\u6539\u8fdb\u6761\u4ef6\u5e73\u5747\u6cbb\u7597\u6548\u679c\uff08CATE\uff09\u7684\u4f30\u8ba1\u3002", "motivation": "\u5728\u4e0d\u540c\u9886\u57df\uff08\u5982\u4e2a\u6027\u5316\u533b\u7597\u548c\u6559\u80b2\u653f\u7b56\uff09\u4e2d\uff0c\u51c6\u786e\u4f30\u8ba1\u5f02\u8d28\u6027\u6cbb\u7597\u6548\u679c\u5bf9\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u867d\u7075\u6d3b\uff0c\u4f46\u5728\u591a\u534f\u53d8\u91cf\u4e0b\u4f30\u8ba1CATE\u4ecd\u5177\u6311\u6218\u3002", "method": "\u901a\u8fc7\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408R-learner\u6846\u67b6\uff0c\u5229\u7528\u9884\u540e\u56e0\u7d20\u4e0e\u6cbb\u7597\u6548\u679c\u5f02\u8d28\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5f00\u53d1\u80fd\u540c\u65f6\u4f18\u5316\u98ce\u9669\u9884\u6d4b\u548c\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u7684\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b58\u5728\u534f\u540c\u6548\u5e94\u7684\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u597d\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u4f30\u8ba1\u8bef\u5dee\u3001\u66f4\u4f4e\u7684\u5047\u53d1\u73b0\u7387\u548c\u66f4\u9ad8\u7684\u5f02\u8d28\u6027\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u8de8\u4efb\u52a1\u5b66\u4e60\u6574\u5408\u989d\u5916\u7684\u4fe1\u606f\uff0c\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u4f30\u8ba1CATE\uff0c\u4e3a\u51b3\u7b56\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u4f9d\u636e\u3002"}}
{"id": "2505.00490", "pdf": "https://arxiv.org/pdf/2505.00490", "abs": "https://arxiv.org/abs/2505.00490", "authors": ["Shivam Vats", "Michelle Zhao", "Patrick Callaghan", "Mingxi Jia", "Maxim Likhachev", "Oliver Kroemer", "George Konidaris"], "title": "Optimal Interactive Learning on the Job via Facility Location Planning", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted to Robotics: Science and Systems (RSS) 2025", "summary": "Collaborative robots must continually adapt to novel tasks and user\npreferences without overburdening the user. While prior interactive robot\nlearning methods aim to reduce human effort, they are typically limited to\nsingle-task scenarios and are not well-suited for sustained, multi-task\ncollaboration. We propose COIL (Cost-Optimal Interactive Learning) -- a\nmulti-task interaction planner that minimizes human effort across a sequence of\ntasks by strategically selecting among three query types (skill, preference,\nand help). When user preferences are known, we formulate COIL as an\nuncapacitated facility location (UFL) problem, which enables bounded-suboptimal\nplanning in polynomial time using off-the-shelf approximation algorithms. We\nextend our formulation to handle uncertainty in user preferences by\nincorporating one-step belief space planning, which uses these approximation\nalgorithms as subroutines to maintain polynomial-time performance. Simulated\nand physical experiments on manipulation tasks show that our framework\nsignificantly reduces the amount of work allocated to the human while\nmaintaining successful task completion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCOIL\u7684\u591a\u4efb\u52a1\u4ea4\u4e92\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u4f18\u5316\u9009\u62e9\u4e09\u79cd\u67e5\u8be2\u7c7b\u578b\uff08\u6280\u80fd\u3001\u504f\u597d\u548c\u5e2e\u52a9\uff09\u4ee5\u6700\u5c0f\u5316\u4eba\u7c7b\u5728\u591a\u4efb\u52a1\u534f\u4f5c\u4e2d\u7684\u5de5\u4f5c\u91cf\u3002\u5728\u5df2\u77e5\u7528\u6237\u504f\u597d\u65f6\uff0cCOIL\u88ab\u5f62\u5f0f\u5316\u4e3a\u65e0\u5bb9\u91cf\u8bbe\u65bd\u9009\u5740\uff08UFL\uff09\u95ee\u9898\uff1b\u5728\u504f\u597d\u4e0d\u786e\u5b9a\u65f6\uff0c\u7ed3\u5408\u4e00\u6b65\u4fe1\u5ff5\u7a7a\u95f4\u89c4\u5212\uff0c\u5e76\u5728\u6a21\u62df\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u673a\u5668\u4eba\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u4efb\u52a1\u573a\u666f\uff0c\u96be\u4ee5\u9002\u5e94\u6301\u7eed\u7684\u591a\u4efb\u52a1\u534f\u4f5c\u9700\u6c42\u3002\u4e3a\u4e86\u964d\u4f4e\u4eba\u7c7b\u5728\u591a\u4efb\u52a1\u534f\u4f5c\u4e2d\u7684\u5de5\u4f5c\u91cf\u5e76\u63d0\u5347\u9002\u5e94\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u4ea4\u4e92\u89c4\u5212\u65b9\u6cd5\u3002", "method": "COIL\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u9009\u62e9\u6280\u80fd\u3001\u504f\u597d\u548c\u5e2e\u52a9\u4e09\u79cd\u67e5\u8be2\u7c7b\u578b\u6765\u6700\u5c0f\u5316\u4eba\u7c7b\u5de5\u4f5c\u91cf\u3002\u5df2\u77e5\u7528\u6237\u504f\u597d\u65f6\uff0c\u91c7\u7528\u65e0\u5bb9\u91cf\u8bbe\u65bd\u9009\u5740\uff08UFL\uff09\u95ee\u9898\u5f62\u5f0f\u5316\uff1b\u504f\u597d\u4e0d\u786e\u5b9a\u65f6\uff0c\u7ed3\u5408\u4e00\u6b65\u4fe1\u5ff5\u7a7a\u95f4\u89c4\u5212\uff0c\u5e76\u5229\u7528\u8fd1\u4f3c\u7b97\u6cd5\u4fdd\u6301\u591a\u9879\u5f0f\u65f6\u95f4\u6027\u80fd\u3002", "result": "\u5728\u6a21\u62df\u548c\u5b9e\u9645\u64cd\u7eb5\u4efb\u52a1\u4e2d\uff0cCOIL\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u7c7b\u7684\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4efb\u52a1\u7684\u6210\u529f\u5b8c\u6210\u3002", "conclusion": "COIL\u6846\u67b6\u4e3a\u591a\u4efb\u52a1\u534f\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u4ea4\u4e92\u89c4\u5212\u65b9\u6cd5\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u4eba\u7c7b\u5de5\u4f5c\u91cf\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.00318", "pdf": "https://arxiv.org/pdf/2505.00318", "abs": "https://arxiv.org/abs/2505.00318", "authors": ["Wei-Bin Kou", "Guangxu Zhu", "Bingyang Cheng", "Shuai Wang", "Ming Tang", "Yik-Chung Wu"], "title": "FedEMA: Federated Exponential Moving Averaging with Negative Entropy Regularizer in Autonomous Driving", "categories": ["cs.RO", "cs.LG"], "comment": "8 pages", "summary": "Street Scene Semantic Understanding (denoted as S3U) is a crucial but complex\ntask for autonomous driving (AD) vehicles. Their inference models typically\nface poor generalization due to domain-shift. Federated Learning (FL) has\nemerged as a promising paradigm for enhancing the generalization of AD models\nthrough privacy-preserving distributed learning. However, these FL AD models\nface significant temporal catastrophic forgetting when deployed in dynamically\nevolving environments, where continuous adaptation causes abrupt erosion of\nhistorical knowledge. This paper proposes Federated Exponential Moving Average\n(FedEMA), a novel framework that addresses this challenge through two integral\ninnovations: (I) Server-side model's historical fitting capability preservation\nvia fusing current FL round's aggregation model and a proposed previous FL\nround's exponential moving average (EMA) model; (II) Vehicle-side negative\nentropy regularization to prevent FL models' possible overfitting to\nEMA-introduced temporal patterns. Above two strategies empower FedEMA a\ndual-objective optimization that balances model generalization and\nadaptability. In addition, we conduct theoretical convergence analysis for the\nproposed FedEMA. Extensive experiments both on Cityscapes dataset and Camvid\ndataset demonstrate FedEMA's superiority over existing approaches, showing\n7.12% higher mean Intersection-over-Union (mIoU).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86FedEMA\u6846\u67b6\uff0c\u901a\u8fc7\u670d\u52a1\u5668\u7aef\u6a21\u578b\u7684\u5386\u53f2\u62df\u5408\u80fd\u529b\u4fdd\u7559\u548c\u8f66\u8f86\u7aef\u8d1f\u71b5\u6b63\u5219\u5316\uff0c\u89e3\u51b3\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u7684\u65f6\u5e8f\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u8bed\u4e49\u7406\u89e3\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u90e8\u7f72\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u65f6\uff0c\u5e38\u56e0\u65f6\u5e8f\u77e5\u8bc6\u9057\u5fd8\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002FedEMA\u65e8\u5728\u5e73\u8861\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u9002\u5e94\u6027\uff0c\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "1. \u670d\u52a1\u5668\u7aef\u901a\u8fc7\u878d\u5408\u5f53\u524d\u8f6e\u6b21\u6a21\u578b\u4e0e\u5386\u53f2EMA\u6a21\u578b\uff0c\u4fdd\u7559\u5386\u53f2\u62df\u5408\u80fd\u529b\uff1b2. \u8f66\u8f86\u7aef\u4f7f\u7528\u8d1f\u71b5\u6b63\u5219\u5316\u907f\u514d\u8fc7\u62df\u5408\u3002\u7406\u8bba\u5206\u6790\u4e86FedEMA\u7684\u6536\u655b\u6027\u3002", "result": "\u5728Cityscapes\u548cCamvid\u6570\u636e\u96c6\u4e0a\uff0cFedEMA\u7684mIoU\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e867.12%\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "FedEMA\u901a\u8fc7\u53cc\u76ee\u6807\u4f18\u5316\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u5e8f\u9057\u5fd8\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7684\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.00321", "pdf": "https://arxiv.org/pdf/2505.00321", "abs": "https://arxiv.org/abs/2505.00321", "authors": ["Zixin Wang", "Yuanming Shi", "Yong Zhou", "Jingyang Zhu", "Khaled. B. Letaief"], "title": "Edge Large AI Models: Revolutionizing 6G Networks", "categories": ["cs.NI", "cs.LG", "eess.SP"], "comment": null, "summary": "Large artificial intelligence models (LAMs) possess human-like abilities to\nsolve a wide range of real-world problems, exemplifying the potential of\nexperts in various domains and modalities. By leveraging the communication and\ncomputation capabilities of geographically dispersed edge devices, edge LAM\nemerges as an enabling technology to empower the delivery of various real-time\nintelligent services in 6G. Unlike traditional edge artificial intelligence\n(AI) that primarily supports a single task using small models, edge LAM is\nfeatured by the need of the decomposition and distributed deployment of large\nmodels, and the ability to support highly generalized and diverse tasks.\nHowever, due to limited communication, computation, and storage resources over\nwireless networks, the vast number of trainable neurons and the substantial\ncommunication overhead pose a formidable hurdle to the practical deployment of\nedge LAMs. In this paper, we investigate the opportunities and challenges of\nedge LAMs from the perspectives of model decomposition and resource management.\nSpecifically, we propose collaborative fine-tuning and full-parameter training\nframeworks, alongside a microservice-assisted inference architecture, to\nenhance the deployment of edge LAM over wireless networks. Additionally, we\ninvestigate the application of edge LAM in air-interface designs, focusing on\nchannel prediction and beamforming. These innovative frameworks and\napplications offer valuable insights and solutions for advancing 6G technology.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u57286G\u7f51\u7edc\u4e2d\u90e8\u7f72\u8fb9\u7f18\u5927\u89c4\u6a21AI\u6a21\u578b\uff08edge LAM\uff09\u7684\u673a\u9047\u4e0e\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u534f\u540c\u5fae\u8c03\u548c\u5168\u53c2\u6570\u8bad\u7ec3\u6846\u67b6\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u5728\u65e0\u7ebf\u63a5\u53e3\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u8fb9\u7f18LAM\u80fd\u591f\u901a\u8fc7\u5730\u7406\u5206\u5e03\u7684\u8fb9\u7f18\u8bbe\u5907\u5b9e\u73b0\u5b9e\u65f6\u667a\u80fd\u670d\u52a1\uff0c\u4f46\u7531\u4e8e\u65e0\u7ebf\u7f51\u7edc\u7684\u8d44\u6e90\u9650\u5236\uff0c\u5176\u90e8\u7f72\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63a8\u52a86G\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u534f\u540c\u5fae\u8c03\u548c\u5168\u53c2\u6570\u8bad\u7ec3\u6846\u67b6\uff0c\u4ee5\u53ca\u5fae\u670d\u52a1\u8f85\u52a9\u7684\u63a8\u7406\u67b6\u6784\uff0c\u4ee5\u4f18\u5316\u8fb9\u7f18LAM\u7684\u90e8\u7f72\u3002\u540c\u65f6\u7814\u7a76\u4e86\u5176\u5728\u4fe1\u9053\u9884\u6d4b\u548c\u6ce2\u675f\u6210\u5f62\u7b49\u65e0\u7ebf\u63a5\u53e3\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u67b6\u6784\u80fd\u591f\u6709\u6548\u63d0\u5347\u8fb9\u7f18LAM\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u90e8\u7f72\u6548\u7387\uff0c\u5e76\u4e3a6G\u6280\u672f\u63d0\u4f9b\u4e86\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8fb9\u7f18LAM\u57286G\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u89e3\u51b3\u8d44\u6e90\u7ba1\u7406\u7b49\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u548c\u5e94\u7528\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.00515", "pdf": "https://arxiv.org/pdf/2505.00515", "abs": "https://arxiv.org/abs/2505.00515", "authors": ["Mingxing Peng", "Ruoyu Yao", "Xusen Guo", "Yuting Xie", "Xianda Chen", "Jun Ma"], "title": "Safety-Critical Traffic Simulation with Guided Latent Diffusion Model", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "7 pages, 3 figures", "summary": "Safety-critical traffic simulation plays a crucial role in evaluating\nautonomous driving systems under rare and challenging scenarios. However,\nexisting approaches often generate unrealistic scenarios due to insufficient\nconsideration of physical plausibility and suffer from low generation\nefficiency. To address these limitations, we propose a guided latent diffusion\nmodel (LDM) capable of generating physically realistic and adversarial\nsafety-critical traffic scenarios. Specifically, our model employs a\ngraph-based variational autoencoder (VAE) to learn a compact latent space that\ncaptures complex multi-agent interactions while improving computational\nefficiency. Within this latent space, the diffusion model performs the\ndenoising process to produce realistic trajectories. To enable controllable and\nadversarial scenario generation, we introduce novel guidance objectives that\ndrive the diffusion process toward producing adversarial and behaviorally\nrealistic driving behaviors. Furthermore, we develop a sample selection module\nbased on physical feasibility checks to further enhance the physical\nplausibility of the generated scenarios. Extensive experiments on the nuScenes\ndataset demonstrate that our method achieves superior adversarial effectiveness\nand generation efficiency compared to existing baselines while maintaining a\nhigh level of realism. Our work provides an effective tool for realistic\nsafety-critical scenario simulation, paving the way for more robust evaluation\nof autonomous driving systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f15\u5bfc\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u7269\u7406\u771f\u5b9e\u4e14\u5bf9\u6297\u7684\u5b89\u5168\u5173\u952e\u4ea4\u901a\u573a\u666f\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u6548\u7387\u548c\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u7f55\u89c1\u548c\u6311\u6218\u6027\u7684\u4ea4\u901a\u573a\u666f\u65f6\uff0c\u5e38\u56e0\u7269\u7406\u5408\u7406\u6027\u4e0d\u8db3\u800c\u751f\u6210\u4e0d\u771f\u5b9e\u573a\u666f\uff0c\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u4e9f\u5f85\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u56fe\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u5b66\u4e60\u7d27\u51d1\u7684\u6f5c\u5728\u7a7a\u95f4\u4ee5\u6355\u83b7\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u8fdb\u884c\u53bb\u566a\u751f\u6210\u771f\u5b9e\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u65b0\u63d0\u51fa\u7684\u5f15\u5bfc\u76ee\u6807\u5b9e\u73b0\u53ef\u63a7\u5bf9\u6297\u751f\u6210\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5bf9\u6297\u6548\u679c\u3001\u751f\u6210\u6548\u7387\u548c\u771f\u5b9e\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b89\u5168\u5173\u952e\u573a\u666f\u4eff\u771f\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u3002"}}
{"id": "2505.00334", "pdf": "https://arxiv.org/pdf/2505.00334", "abs": "https://arxiv.org/abs/2505.00334", "authors": ["Luigi Sigillo", "Christian Bianchi", "Danilo Comminiello"], "title": "Quaternion Wavelet-Conditioned Diffusion Models for Image Super-Resolution", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for presentation at IJCNN 2025", "summary": "Image Super-Resolution is a fundamental problem in computer vision with broad\napplications spacing from medical imaging to satellite analysis. The ability to\nreconstruct high-resolution images from low-resolution inputs is crucial for\nenhancing downstream tasks such as object detection and segmentation. While\ndeep learning has significantly advanced SR, achieving high-quality\nreconstructions with fine-grained details and realistic textures remains\nchallenging, particularly at high upscaling factors. Recent approaches\nleveraging diffusion models have demonstrated promising results, yet they often\nstruggle to balance perceptual quality with structural fidelity. In this work,\nwe introduce ResQu a novel SR framework that integrates a quaternion wavelet\npreprocessing framework with latent diffusion models, incorporating a new\nquaternion wavelet- and time-aware encoder. Unlike prior methods that simply\napply wavelet transforms within diffusion models, our approach enhances the\nconditioning process by exploiting quaternion wavelet embeddings, which are\ndynamically integrated at different stages of denoising. Furthermore, we also\nleverage the generative priors of foundation models such as Stable Diffusion.\nExtensive experiments on domain-specific datasets demonstrate that our method\nachieves outstanding SR results, outperforming in many cases existing\napproaches in perceptual quality and standard evaluation metrics. The code will\nbe available after the revision process.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56db\u5143\u6570\u5c0f\u6ce2\u9884\u5904\u7406\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u65b0\u578b\u8d85\u5206\u8fa8\u7387\u6846\u67b6ResQu\uff0c\u901a\u8fc7\u52a8\u6001\u96c6\u6210\u56db\u5143\u6570\u5c0f\u6ce2\u5d4c\u5165\u63d0\u5347\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\uff0c\u5728\u611f\u77e5\u8d28\u91cf\u548c\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8d85\u5206\u8fa8\u7387\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u9ad8\u500d\u653e\u5927\u65f6\u96be\u4ee5\u5e73\u8861\u7ec6\u8282\u4e0e\u7ed3\u6784\u4fdd\u771f\u5ea6\u3002\u6269\u6563\u6a21\u578b\u867d\u6709\u6548\uff0c\u4ecd\u9700\u6539\u8fdb\u6761\u4ef6\u5904\u7406\u673a\u5236\u3002", "method": "\u6574\u5408\u56db\u5143\u6570\u5c0f\u6ce2\u9884\u5904\u7406\u4e0e\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5f15\u5165\u56db\u5143\u6570\u5c0f\u6ce2\u548c\u65f6\u95f4\u611f\u77e5\u7f16\u7801\u5668\uff0c\u52a8\u6001\u5d4c\u5165\u5c0f\u6ce2\u4fe1\u606f\u4e8e\u53bb\u566a\u8fc7\u7a0b\uff0c\u5e76\u5229\u7528Stable Diffusion\u7b49\u57fa\u7840\u6a21\u578b\u7684\u751f\u6210\u5148\u9a8c\u3002", "result": "\u5728\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u611f\u77e5\u8d28\u91cf\u548c\u6807\u51c6\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "ResQu\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u5c0f\u6ce2\u5d4c\u5165\u548c\u6269\u6563\u6a21\u578b\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d85\u5206\u8fa8\u7387\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u4e3a\u9ad8\u500d\u653e\u5927\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.00555", "pdf": "https://arxiv.org/pdf/2505.00555", "abs": "https://arxiv.org/abs/2505.00555", "authors": ["Jean-Baptiste A. Conan"], "title": "On the Mechanistic Interpretability of Neural Networks for Causality in Bio-statistics", "categories": ["stat.AP", "cs.AI"], "comment": null, "summary": "Interpretable insights from predictive models remain critical in\nbio-statistics, particularly when assessing causality, where classical\nstatistical and machine learning methods often provide inherent clarity. While\nNeural Networks (NNs) offer powerful capabilities for modeling complex\nbiological data, their traditional \"black-box\" nature presents challenges for\nvalidation and trust in high-stakes health applications. Recent advances in\nMechanistic Interpretability (MI) aim to decipher the internal computations\nlearned by these networks. This work investigates the application of MI\ntechniques to NNs within the context of causal inference for bio-statistics.\n  We demonstrate that MI tools can be leveraged to: (1) probe and validate the\ninternal representations learned by NNs, such as those estimating nuisance\nfunctions in frameworks like Targeted Minimum Loss-based Estimation (TMLE); (2)\ndiscover and visualize the distinct computational pathways employed by the\nnetwork to process different types of inputs, potentially revealing how\nconfounders and treatments are handled; and (3) provide methodologies for\ncomparing the learned mechanisms and extracted insights across statistical,\nmachine learning, and NN models, fostering a deeper understanding of their\nrespective strengths and weaknesses for causal bio-statistical analysis.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff08MI\uff09\u6280\u672f\u5728\u751f\u7269\u7edf\u8ba1\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u5e94\u7528\uff0c\u5c55\u793a\u4e86MI\u5982\u4f55\u5e2e\u52a9\u9a8c\u8bc1\u795e\u7ecf\u7f51\u7edc\uff08NNs\uff09\u7684\u5185\u90e8\u8868\u793a\u3001\u63ed\u793a\u5176\u8ba1\u7b97\u8def\u5f84\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u5728\u56e0\u679c\u5206\u6790\u4e2d\u7684\u4f18\u52a3\u3002", "motivation": "\u7531\u4e8e\u795e\u7ecf\u7f51\u7edc\uff08NNs\uff09\u5728\u590d\u6742\u751f\u7269\u6570\u636e\u5efa\u6a21\u4e2d\u8868\u73b0\u5f3a\u5927\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7MI\u6280\u672f\u63d0\u5347NNs\u5728\u751f\u7269\u7edf\u8ba1\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u7814\u7a76\u5229\u7528MI\u5de5\u5177\u9a8c\u8bc1NNs\u5b66\u4e60\u7684\u5185\u5728\u8868\u793a\uff08\u5982TMLE\u6846\u67b6\u4e2d\u7684\u5e72\u6270\u51fd\u6570\uff09\uff0c\u53ef\u89c6\u5316\u5176\u8f93\u5165\u5904\u7406\u7684\u8ba1\u7b97\u8def\u5f84\uff0c\u5e76\u6bd4\u8f83\u7edf\u8ba1\u3001\u673a\u5668\u5b66\u4e60\u548cNN\u6a21\u578b\u7684\u673a\u5236\u3002", "result": "MI\u6280\u672f\u6210\u529f\u63ed\u793aNNs\u5982\u4f55\u5904\u7406\u6df7\u6742\u53d8\u91cf\u548c\u6cbb\u7597\u53d8\u91cf\uff0c\u9a8c\u8bc1\u4e86\u5176\u5185\u90e8\u8868\u793a\uff0c\u5e76\u63d0\u4f9b\u4e86\u8de8\u6a21\u578b\u6bd4\u8f83\u7684\u65b9\u6cd5\u8bba\u3002", "conclusion": "MI\u6280\u672f\u4e3aNNs\u5728\u751f\u7269\u7edf\u8ba1\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u9a8c\u8bc1\u5de5\u5177\uff0c\u589e\u5f3a\u4e86\u5176\u5728\u5065\u5eb7\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2505.00430", "pdf": "https://arxiv.org/pdf/2505.00430", "abs": "https://arxiv.org/abs/2505.00430", "authors": ["Chenghong Bian", "Meng Hua", "Deniz Gunduz"], "title": "Over-the-Air Inference over Multi-hop MIMO Networks", "categories": ["eess.SP", "cs.LG"], "comment": "5 pages", "summary": "A novel over-the-air machine learning framework over multi-hop multiple-input\nand multiple-output (MIMO) networks is proposed. The core idea is to imitate\nfully connected (FC) neural network layers using multiple MIMO channels by\ncarefully designing the precoding matrices at the transmitting nodes. A neural\nnetwork dubbed PrototypeNet is employed consisting of multiple FC layers, with\nthe number of neurons of each layer equal to the number of antennas of the\ncorresponding terminal. To achieve satisfactory performance, we train\nPrototypeNet based on a customized loss function consisting of classification\nerror and the power of latent vectors to satisfy transmit power constraints,\nwith noise injection during training. Precoding matrices for each hop are then\nobtained by solving an optimization problem. We also propose a multiple-block\nextension when the number of antennas is limited. Numerical results verify that\nthe proposed over-the-air transmission scheme can achieve satisfactory\nclassification accuracy under a power constraint. The results also show that\nhigher classification accuracy can be achieved with an increasing number of\nhops at a modest signal-to-noise ratio (SNR).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8df3MIMO\u7f51\u7edc\u4e0a\u7684\u65b0\u578b\u7a7a\u4e2d\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u53d1\u5c04\u8282\u70b9\u7684\u9884\u7f16\u7801\u77e9\u9635\u6a21\u4eff\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u7ed3\u5408\u5206\u7c7b\u8bef\u5dee\u548c\u6f5c\u5728\u5411\u91cf\u529f\u7387\u7684\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u8bad\u7ec3PrototypeNet\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u6ce8\u5165\u566a\u58f0\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6848\u5728\u529f\u7387\u7ea6\u675f\u4e0b\u80fd\u5b9e\u73b0\u6ee1\u610f\u7684\u5206\u7c7b\u7cbe\u5ea6\uff0c\u4e14\u591a\u8df3\u53ef\u63d0\u5347\u7cbe\u5ea6\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u591a\u8df3MIMO\u7f51\u7edc\u4e2d\u9ad8\u6548\u5b9e\u73b0\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u5728\u529f\u7387\u7ea6\u675f\u548c\u590d\u6742\u4fe1\u9053\u73af\u5883\u4e0b\u7684\u5c40\u9650\u6027\u3002", "method": "\u8bbe\u8ba1PrototypeNet\u795e\u7ecf\u7f51\u7edc\uff0c\u5176\u5168\u8fde\u63a5\u5c42\u795e\u7ecf\u5143\u6570\u5bf9\u5e94\u7ec8\u7aef\u5929\u7ebf\u6570\uff0c\u901a\u8fc7\u5b9a\u5236\u635f\u5931\u51fd\u6570\uff08\u5206\u7c7b\u8bef\u5dee\u548c\u6f5c\u5728\u5411\u91cf\u529f\u7387\uff09\u8bad\u7ec3\u7f51\u7edc\uff0c\u5e76\u5f15\u5165\u566a\u58f0\u3002\u9884\u7f16\u7801\u77e9\u9635\u901a\u8fc7\u4f18\u5316\u95ee\u9898\u6c42\u89e3\uff0c\u63d0\u51fa\u591a\u5757\u6269\u5c55\u4ee5\u5e94\u5bf9\u5929\u7ebf\u6570\u9650\u5236\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6848\u5728\u529f\u7387\u7ea6\u675f\u4e0b\u80fd\u5b9e\u73b0\u6ee1\u610f\u5206\u7c7b\u7cbe\u5ea6\uff0c\u5e76\u4e14\u589e\u52a0\u8df3\u6570\u53ef\u5728\u9002\u5ea6\u4fe1\u566a\u6bd4\u4e0b\u63d0\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u8df3MIMO\u7f51\u7edc\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u529f\u7387\u53d7\u9650\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u591a\u8df3\u7ed3\u6784\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2505.00460", "pdf": "https://arxiv.org/pdf/2505.00460", "abs": "https://arxiv.org/abs/2505.00460", "authors": ["Harshit Kapadia", "Peter Benner", "Lihong Feng"], "title": "Subspace-Distance-Enabled Active Learning for Efficient Data-Driven Model Reduction of Parametric Dynamical Systems", "categories": ["math.NA", "cs.CE", "cs.LG", "cs.NA", "math.DS", "physics.comp-ph"], "comment": "31 pages, 10 figures, 4 tables", "summary": "In situations where the solution of a high-fidelity dynamical system needs to\nbe evaluated repeatedly, over a vast pool of parametric configurations and in\nabsence of access to the underlying governing equations, data-driven model\nreduction techniques are preferable. We propose a novel active learning\napproach to build a parametric data-driven reduced-order model (ROM) by\ngreedily picking the most important parameter samples from the parameter\ndomain. As a result, during the ROM construction phase, the number of\nhigh-fidelity solutions dynamically grow in a principled fashion. The\nhigh-fidelity solution snapshots are expressed in several parameter-specific\nlinear subspaces, with the help of proper orthogonal decomposition (POD), and\nthe relative distance between these subspaces is used as a guiding mechanism to\nperform active learning. For successfully achieving this, we provide a distance\nmeasure to evaluate the similarity between pairs of linear subspaces with\ndifferent dimensions, and also show that this distance measure is a metric. The\nusability of the proposed subspace-distance-enabled active learning (SDE-AL)\nframework is demonstrated by augmenting two existing non-intrusive\nreduced-order modeling approaches, and providing their active-learning-driven\n(ActLearn) extensions, namely, SDE-ActLearn-POD-KSNN, and SDE-ActLearn-POD-NN.\nFurthermore, we report positive results for two parametric physical models,\nhighlighting the efficiency of the proposed SDE-AL approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u6570\u636e\u9a71\u52a8\u964d\u9636\u6a21\u578b\uff08ROM\uff09\u6784\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d2a\u5a6a\u9009\u62e9\u53c2\u6570\u57df\u4e2d\u6700\u91cd\u8981\u6837\u672c\uff0c\u52a8\u6001\u589e\u52a0\u9ad8\u4fdd\u771f\u89e3\u6570\u91cf\uff0c\u5e76\u5229\u7528POD\u548c\u5b50\u7a7a\u95f4\u8ddd\u79bb\u5ea6\u91cf\u4f18\u5316\u5b66\u4e60\u8fc7\u7a0b\u3002", "motivation": "\u5728\u9ad8\u4fdd\u771f\u52a8\u529b\u7cfb\u7edf\u89e3\u9700\u91cd\u590d\u8bc4\u4f30\u4e14\u65e0\u6cd5\u8bbf\u95ee\u63a7\u5236\u65b9\u7a0b\u7684\u60c5\u51b5\u4e0b\uff0c\u6570\u636e\u9a71\u52a8\u964d\u9636\u6280\u672f\u66f4\u9002\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u6a21\u578b\u6548\u7387\u3002", "method": "\u63d0\u51fa\u5b50\u7a7a\u95f4\u8ddd\u79bb\u9a71\u52a8\u7684\u4e3b\u52a8\u5b66\u4e60\uff08SDE-AL\uff09\u6846\u67b6\uff0c\u7ed3\u5408POD\u8868\u8fbe\u89e3\u7684\u5b50\u7a7a\u95f4\uff0c\u57fa\u4e8e\u5b50\u7a7a\u95f4\u76f8\u4f3c\u6027\u5ea6\u91cf\u52a8\u6001\u9009\u62e9\u53c2\u6570\u6837\u672c\uff0c\u5e76\u6269\u5c55\u81f3\u4e24\u79cd\u73b0\u6709\u975e\u4fb5\u5165\u5f0fROM\u65b9\u6cd5\u3002", "result": "\u5728\u4e24\u79cd\u53c2\u6570\u5316\u7269\u7406\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86SDE-AL\u7684\u9ad8\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u4e3b\u52a8\u5b66\u4e60\u9a71\u52a8\u7684ROM\u6784\u5efa\uff08SDE-ActLearn-POD-KSNN/NN\uff09\u3002", "conclusion": "SDE-AL\u6846\u67b6\u901a\u8fc7\u5b50\u7a7a\u95f4\u8ddd\u79bb\u5ea6\u91cf\u4f18\u5316\u6837\u672c\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86ROM\u7684\u6784\u5efa\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u53c2\u6570\u5316\u6a21\u578b\u964d\u9636\u3002"}}
{"id": "2505.00561", "pdf": "https://arxiv.org/pdf/2505.00561", "abs": "https://arxiv.org/abs/2505.00561", "authors": ["Kuan-Cheng Chen", "Hiromichi Matsuyama", "Wei-Hao Huang"], "title": "Learning to Learn with Quantum Optimization via Quantum Neural Networks", "categories": ["quant-ph", "cs.AI"], "comment": null, "summary": "Quantum Approximate Optimization Algorithms (QAOA) promise efficient\nsolutions to classically intractable combinatorial optimization problems by\nharnessing shallow-depth quantum circuits. Yet, their performance and\nscalability often hinge on effective parameter optimization, which remains\nnontrivial due to rugged energy landscapes and hardware noise. In this work, we\nintroduce a quantum meta-learning framework that combines quantum neural\nnetworks, specifically Quantum Long Short-Term Memory (QLSTM) architectures,\nwith QAOA. By training the QLSTM optimizer on smaller graph instances, our\napproach rapidly generalizes to larger, more complex problems, substantially\nreducing the number of iterations required for convergence. Through\ncomprehensive benchmarks on Max-Cut and Sherrington-Kirkpatrick model\ninstances, we demonstrate that QLSTM-based optimizers converge faster and\nachieve higher approximation ratios compared to classical baselines, thereby\noffering a robust pathway toward scalable quantum optimization in the NISQ era.", "AI": {"tldr": "\u91cf\u5b50\u8fd1\u4f3c\u4f18\u5316\u7b97\u6cd5\uff08QAOA\uff09\u901a\u8fc7\u6d45\u5c42\u91cf\u5b50\u7535\u8def\u89e3\u51b3\u7ecf\u5178\u96be\u89e3\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u4f46\u6027\u80fd\u4f9d\u8d56\u53c2\u6570\u4f18\u5316\u3002\u672c\u6587\u63d0\u51fa\u7ed3\u5408\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\uff08QLSTM\uff09\u4e0eQAOA\u7684\u91cf\u5b50\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u5c0f\u89c4\u6a21\u56fe\u4e0a\u8bad\u7ec3\u540e\u5feb\u901f\u63a8\u5e7f\u81f3\u590d\u6742\u95ee\u9898\uff0c\u51cf\u5c11\u6536\u655b\u6240\u9700\u7684\u8fed\u4ee3\u6b21\u6570\u3002", "motivation": "QAOA\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u53d7\u9650\u4e8e\u53c2\u6570\u4f18\u5316\u7684\u590d\u6742\u6027\uff08\u5982\u9ad8\u566a\u97f3\u548c\u590d\u6742\u80fd\u91cf\u666f\u89c2\uff09\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u91cf\u5b50\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u5c06QLSTM\u67b6\u6784\u4e0eQAOA\u7ed3\u5408\uff0c\u901a\u8fc7\u5728\u5c0f\u89c4\u6a21\u56fe\u4e0a\u8bad\u7ec3\u4f18\u5316\u5668\uff0c\u63a8\u5e7f\u81f3\u5927\u89c4\u6a21\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cQLSTM\u4f18\u5316\u5668\u5728Max-Cut\u548cSherrington-Kirkpatrick\u6a21\u578b\u4e2d\u6536\u655b\u66f4\u5feb\uff0c\u4e14\u8fd1\u4f3c\u6bd4\u4f18\u4e8e\u7ecf\u5178\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aNISQ\u65f6\u4ee3\u4e0b\u7684\u53ef\u6269\u5c55\u91cf\u5b50\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2505.00500", "pdf": "https://arxiv.org/pdf/2505.00500", "abs": "https://arxiv.org/abs/2505.00500", "authors": ["Minseok Song", "JeongHo Ha", "Bonggyeong Park", "Daehyung Park"], "title": "Implicit Neural-Representation Learning for Elastic Deformable-Object Manipulations", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "We aim to solve the problem of manipulating deformable objects, particularly\nelastic bands, in real-world scenarios. However, deformable object manipulation\n(DOM) requires a policy that works on a large state space due to the unlimited\ndegree of freedom (DoF) of deformable objects. Further, their dense but partial\nobservations (e.g., images or point clouds) may increase the sampling\ncomplexity and uncertainty in policy learning. To figure it out, we propose a\nnovel implicit neural-representation (INR) learning for elastic DOMs, called\nINR-DOM. Our method learns consistent state representations associated with\npartially observable elastic objects reconstructing a complete and implicit\nsurface represented as a signed distance function. Furthermore, we perform\nexploratory representation fine-tuning through reinforcement learning (RL) that\nenables RL algorithms to effectively learn exploitable representations while\nefficiently obtaining a DOM policy. We perform quantitative and qualitative\nanalyses building three simulated environments and real-world manipulation\nstudies with a Franka Emika Panda arm. Videos are available at\nhttp://inr-dom.github.io.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u7684\u65b9\u6cd5INR-DOM\uff0c\u7528\u4e8e\u89e3\u51b3\u53ef\u53d8\u5f62\u7269\u4f53\uff08\u5982\u5f39\u6027\u5e26\uff09\u5728\u90e8\u5206\u89c2\u6d4b\u4e0b\u7684\u64cd\u7eb5\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u7eb5\uff08DOM\uff09\u56e0\u7269\u4f53\u65e0\u9650\u81ea\u7531\u5ea6\uff08DoF\uff09\u548c\u90e8\u5206\u89c2\u6d4b\uff08\u5982\u70b9\u4e91\uff09\u5bfc\u81f4\u7b56\u7565\u5b66\u4e60\u590d\u6742\u4e14\u4e0d\u786e\u5b9a\uff0c\u4e9f\u9700\u9ad8\u6548\u7684\u72b6\u6001\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63d0\u51faINR-DOM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u91cd\u6784\u5b8c\u6574\u7269\u4f53\u8868\u9762\uff08\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\uff09\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8868\u793a\u5fae\u8c03\u4ee5\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u6a21\u62df\u73af\u5883\u548c\u771f\u5b9eFranka\u673a\u68b0\u81c2\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u652f\u6301\u5b9a\u91cf\u4e0e\u5b9a\u6027\u5206\u6790\u3002", "conclusion": "INR-DOM\u901a\u8fc7\u9690\u5f0f\u8868\u793a\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u7eb5\u7b56\u7565\u7684\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2505.00562", "pdf": "https://arxiv.org/pdf/2505.00562", "abs": "https://arxiv.org/abs/2505.00562", "authors": ["Yue Meng", "Chuchu Fan"], "title": "TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching", "categories": ["cs.RO", "cs.AI", "cs.FL", "cs.LG"], "comment": "Accepted to ICML2025", "summary": "Learning to solve complex tasks with signal temporal logic (STL)\nspecifications is crucial to many real-world applications. However, most\nprevious works only consider fixed or parametrized STL specifications due to\nthe lack of a diverse STL dataset and encoders to effectively extract temporal\nlogic information for downstream tasks. In this paper, we propose TeLoGraF,\nTemporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN)\nencoder and flow-matching to learn solutions for general STL specifications. We\nidentify four commonly used STL templates and collect a total of 200K\nspecifications with paired demonstrations. We conduct extensive experiments in\nfive simulation environments ranging from simple dynamical models in the 2D\nspace to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped\nnavigation. Results show that our method outperforms other baselines in the STL\nsatisfaction rate. Compared to classical STL planning algorithms, our approach\nis 10-100X faster in inference and can work on any system dynamics. Besides, we\nshow our graph-encoding method's capability to solve complex STLs and\nrobustness to out-distribution STL specifications. Code is available at\nhttps://github.com/mengyuest/TeLoGraF", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86TeLoGraF\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u7f16\u7801\u548c\u6d41\u5339\u914d\u5b66\u4e60\u901a\u7528STL\u89c4\u8303\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u79cd\u6a21\u62df\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63a8\u7406\u901f\u5ea6\u5feb10-100\u500d\uff0c\u4e14\u9002\u7528\u4e8e\u4efb\u4f55\u7cfb\u7edf\u52a8\u529b\u5b66\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u9700\u8981\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u89c4\u8303\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u56e0\u7f3a\u4e4f\u591a\u6837STL\u6570\u636e\u96c6\u548c\u6709\u6548\u7f16\u7801\u5668\u800c\u53ea\u80fd\u5904\u7406\u56fa\u5b9a\u6216\u53c2\u6570\u5316STL\u89c4\u8303\u3002", "method": "\u63d0\u51faTeLoGraF\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7f16\u7801\u5668\u548c\u6d41\u5339\u914d\u6280\u672f\uff0c\u5b66\u4e60\u901a\u7528STL\u89c4\u8303\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6536\u96c620\u4e07\u6761\u89c4\u8303\u4e0e\u6f14\u793a\u5bf9\u3002", "result": "\u5728\u4e94\u4e2a\u6a21\u62df\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0cTeLoGraF\u7684STL\u6ee1\u8db3\u7387\u4f18\u4e8e\u57fa\u7ebf\uff0c\u63a8\u7406\u901f\u5ea6\u5feb10-100\u500d\uff0c\u4e14\u80fd\u5904\u7406\u590d\u6742STL\u548c\u5206\u5e03\u5916\u89c4\u8303\u3002", "conclusion": "TeLoGraF\u5728STL\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u901f\u5ea6\u5feb\u4e14\u901a\u7528\u6027\u5f3a\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.00525", "pdf": "https://arxiv.org/pdf/2505.00525", "abs": "https://arxiv.org/abs/2505.00525", "authors": ["Abu Saleh Musa Miah", "taro Suzuki", "Jungpil Shin"], "title": "A Methodological and Structural Review of Parkinsons Disease Detection Across Diverse Data Modalities", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Parkinsons Disease (PD) is a progressive neurological disorder that primarily\naffects motor functions and can lead to mild cognitive impairment (MCI) and\ndementia in its advanced stages. With approximately 10 million people diagnosed\nglobally 1 to 1.8 per 1,000 individuals, according to reports by the Japan\nTimes and the Parkinson Foundation early and accurate diagnosis of PD is\ncrucial for improving patient outcomes. While numerous studies have utilized\nmachine learning (ML) and deep learning (DL) techniques for PD recognition,\nexisting surveys are limited in scope, often focusing on single data modalities\nand failing to capture the potential of multimodal approaches. To address these\ngaps, this study presents a comprehensive review of PD recognition systems\nacross diverse data modalities, including Magnetic Resonance Imaging (MRI),\ngait-based pose analysis, gait sensory data, handwriting analysis, speech test\ndata, Electroencephalography (EEG), and multimodal fusion techniques. Based on\nover 347 articles from leading scientific databases, this review examines key\naspects such as data collection methods, settings, feature representations, and\nsystem performance, with a focus on recognition accuracy and robustness. This\nsurvey aims to serve as a comprehensive resource for researchers, providing\nactionable guidance for the development of next generation PD recognition\nsystems. By leveraging diverse data modalities and cutting-edge machine\nlearning paradigms, this work contributes to advancing the state of PD\ndiagnostics and improving patient care through innovative, multimodal\napproaches.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u4e00\u7bc7\u5173\u4e8e\u5e15\u91d1\u68ee\u75c5\uff08PD\uff09\u8bc6\u522b\u7684\u7efc\u5408\u7efc\u8ff0\uff0c\u6db5\u76d6\u4e86\u591a\u79cd\u6570\u636e\u6a21\u6001\u548c\u591a\u6a21\u6001\u878d\u5408\u6280\u672f\uff0c\u65e8\u5728\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u5f00\u53d1\u4e0b\u4e00\u4ee3PD\u8bc6\u522b\u7cfb\u7edf\u7684\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5c40\u9650\u4e8e\u5355\u4e00\u6570\u636e\u6a21\u6001\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u7bc7\u5168\u9762\u7efc\u8ff0\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790347\u7bc7\u76f8\u5173\u6587\u7ae0\uff0c\u7814\u7a76\u4e86MRI\u3001\u6b65\u6001\u5206\u6790\u3001\u624b\u5199\u5206\u6790\u3001\u8bed\u97f3\u6d4b\u8bd5\u3001EEG\u7b49\u591a\u79cd\u6570\u636e\u6a21\u6001\u7684\u7279\u5f81\u8868\u793a\u548c\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u7efc\u8ff0\u5c55\u793a\u4e86\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u63d0\u9ad8PD\u8bc6\u522b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u65b9\u6cd5\u548c\u524d\u6cbf\u673a\u5668\u5b66\u4e60\u8303\u5f0f\u5728\u63a8\u52a8PD\u8bca\u65ad\u6280\u672f\u8fdb\u6b65\u548c\u6539\u5584\u60a3\u8005\u62a4\u7406\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.00568", "pdf": "https://arxiv.org/pdf/2505.00568", "abs": "https://arxiv.org/abs/2505.00568", "authors": ["Lucas Robinet", "Ahmad Berjaoui", "Elizabeth Cohen-Jonathan Moyal"], "title": "Multimodal Masked Autoencoder Pre-training for 3D MRI-Based Brain Tumor Analysis with Missing Modalities", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal magnetic resonance imaging (MRI) constitutes the first line of\ninvestigation for clinicians in the care of brain tumors, providing crucial\ninsights for surgery planning, treatment monitoring, and biomarker\nidentification. Pre-training on large datasets have been shown to help models\nlearn transferable representations and adapt with minimal labeled data. This\nbehavior is especially valuable in medical imaging, where annotations are often\nscarce. However, applying this paradigm to multimodal medical data introduces a\nchallenge: most existing approaches assume that all imaging modalities are\navailable during both pre-training and fine-tuning. In practice, missing\nmodalities often occur due to acquisition issues, specialist unavailability, or\nspecific experimental designs on small in-house datasets. Consequently, a\ncommon approach involves training a separate model for each desired modality\ncombination, making the process both resource-intensive and impractical for\nclinical use. Therefore, we introduce BM-MAE, a masked image modeling\npre-training strategy tailored for multimodal MRI data. The same pre-trained\nmodel seamlessly adapts to any combination of available modalities, extracting\nrich representations that capture both intra- and inter-modal information. This\nallows fine-tuning on any subset of modalities without requiring architectural\nchanges, while still benefiting from a model pre-trained on the full set of\nmodalities. Extensive experiments show that the proposed pre-training strategy\noutperforms or remains competitive with baselines that require separate\npre-training for each modality subset, while substantially surpassing training\nfrom scratch on several downstream tasks. Additionally, it can quickly and\nefficiently reconstruct missing modalities, highlighting its practical value.\nCode and trained models are available at: https://github.com/Lucas-rbnt/bmmae", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001MRI\u6570\u636e\u7684\u9884\u8bad\u7ec3\u7b56\u7565BM-MAE\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u6a21\u6001\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u80fd\u591f\u5728\u4e0d\u6539\u53d8\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u9002\u5e94\u4efb\u4f55\u6a21\u6001\u7ec4\u5408\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u591a\u6a21\u6001MRI\u6570\u636e\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u56e0\u91c7\u96c6\u95ee\u9898\u6216\u4e13\u5bb6\u7f3a\u5931\u800c\u7f3a\u5c11\u67d0\u4e9b\u6a21\u6001\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u4e3a\u6bcf\u79cd\u6a21\u6001\u7ec4\u5408\u5355\u72ec\u8bad\u7ec3\u6a21\u578b\uff0c\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u4e0d\u5b9e\u7528\u3002", "method": "\u63d0\u51faBM-MAE\uff0c\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u7684\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u540c\u4e00\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u4efb\u4f55\u6a21\u6001\u7ec4\u5408\uff0c\u6355\u83b7\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6216\u5ab2\u7f8e\u9700\u4e3a\u6bcf\u79cd\u6a21\u6001\u5b50\u96c6\u5355\u72ec\u9884\u8bad\u7ec3\u7684\u57fa\u7ebf\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u4ece\u5934\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u8fd8\u80fd\u9ad8\u6548\u91cd\u5efa\u7f3a\u5931\u6a21\u6001\u3002", "conclusion": "BM-MAE\u4e3a\u591a\u6a21\u6001MRI\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u9884\u8bad\u7ec3\u65b9\u6848\uff0c\u5177\u6709\u663e\u8457\u7684\u4e34\u5e8a\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.00526", "pdf": "https://arxiv.org/pdf/2505.00526", "abs": "https://arxiv.org/abs/2505.00526", "authors": ["Yanhao 'Max' Wei", "Zhenling Jiang"], "title": "Pre-Training Estimators for Structural Models: Application to Consumer Search", "categories": ["econ.EM", "cs.LG", "stat.CO", "G.3; J.4; I.2"], "comment": null, "summary": "We explore pretraining estimators for structural econometric models. The\nestimator is \"pretrained\" in the sense that the bulk of the computational cost\nand researcher effort occur during the construction of the estimator.\nSubsequent applications of the estimator to different datasets require little\ncomputational cost or researcher effort. The estimation leverages a neural net\nto recognize the structural model's parameter from data patterns. As an initial\ntrial, this paper builds a pretrained estimator for a sequential search model\nthat is known to be difficult to estimate. We evaluate the pretrained estimator\non 14 real datasets. The estimation takes seconds to run and shows high\naccuracy. We provide the estimator at pnnehome.github.io. More generally,\npretrained, off-the-shelf estimators can make structural models more accessible\nto researchers and practitioners.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u9884\u8bad\u7ec3\u4f30\u8ba1\u5668\u5728\u7ed3\u6784\u8ba1\u91cf\u7ecf\u6d4e\u5b66\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u8bc6\u522b\u53c2\u6570\uff0c\u5e94\u7528\u4e8e14\u4e2a\u6570\u636e\u96c6\uff0c\u7ed3\u679c\u9ad8\u6548\u4e14\u51c6\u786e\u3002", "motivation": "\u4e3a\u4f7f\u7ed3\u6784\u6a21\u578b\u5bf9\u7814\u7a76\u8005\u66f4\u6613\u7528\uff0c\u63d0\u51fa\u9884\u8bad\u7ec3\u4f30\u8ba1\u5668\u4ee5\u51cf\u5c11\u540e\u7eed\u5e94\u7528\u7684\u8ba1\u7b97\u6210\u672c\u548c\u4eba\u529b\u6295\u5165\u3002", "method": "\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u8bc6\u522b\u7ed3\u6784\u6a21\u578b\u7684\u53c2\u6570\uff0c\u6784\u5efa\u9002\u7528\u4e8e\u987a\u5e8f\u641c\u7d22\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u4f30\u8ba1\u5668\u3002", "result": "\u572814\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u4f30\u8ba1\u901f\u5ea6\u5feb\u4e14\u51c6\u786e\u6027\u9ad8\u3002", "conclusion": "\u9884\u8bad\u7ec3\u5373\u7528\u578b\u4f30\u8ba1\u5668\u80fd\u63d0\u5347\u7ed3\u6784\u6a21\u578b\u7684\u9002\u7528\u6027\uff0c\u4fc3\u8fdb\u7814\u7a76\u548c\u5b9e\u8df5\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2505.00540", "pdf": "https://arxiv.org/pdf/2505.00540", "abs": "https://arxiv.org/abs/2505.00540", "authors": ["Ian O'Flynn", "Harun \u0160iljak"], "title": "Emergence of Roles in Robotic Teams with Model Sharing and Limited Communication", "categories": ["cs.MA", "cs.LG", "cs.RO", "cs.SY", "eess.SY"], "comment": "Accepted for 2025 8th International Balkan Conference on\n  Communications and Networking (Balkancom)", "summary": "We present a reinforcement learning strategy for use in multi-agent foraging\nsystems in which the learning is centralised to a single agent and its model is\nperiodically disseminated among the population of non-learning agents. In a\ndomain where multi-agent reinforcement learning (MARL) is the common approach,\nthis approach aims to significantly reduce the computational and energy demands\ncompared to approaches such as MARL and centralised learning models. By\ndeveloping high performing foraging agents, these approaches can be translated\ninto real-world applications such as logistics, environmental monitoring, and\nautonomous exploration. A reward function was incorporated into this approach\nthat promotes role development among agents, without explicit directives. This\nled to the differentiation of behaviours among the agents. The implicit\nencouragement of role differentiation allows for dynamic actions in which\nagents can alter roles dependent on their interactions with the environment\nwithout the need for explicit communication between agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u4e2d\u5f0f\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u89c5\u98df\u7cfb\u7edf\uff0c\u901a\u8fc7\u5355\u4e2a\u667a\u80fd\u4f53\u5b66\u4e60\u5e76\u5b9a\u671f\u4f20\u64ad\u6a21\u578b\uff0c\u964d\u4f4e\u8ba1\u7b97\u548c\u80fd\u8017\u9700\u6c42\uff0c\u4f18\u4e8e\u4f20\u7edfMARL\u548c\u96c6\u4e2d\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e2d\u8ba1\u7b97\u548c\u80fd\u91cf\u6d88\u8017\u9ad8\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u667a\u80fd\u4f53\u5728\u7269\u6d41\u3001\u73af\u5883\u76d1\u6d4b\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u96c6\u4e2d\u5f0f\u5b66\u4e60\u7b56\u7565\uff0c\u7531\u5355\u4e2a\u667a\u80fd\u4f53\u5b66\u4e60\u5e76\u5b9a\u671f\u5411\u975e\u5b66\u4e60\u667a\u80fd\u4f53\u4f20\u64ad\u6a21\u578b\uff0c\u7ed3\u5408\u5956\u52b1\u51fd\u6570\u4fc3\u8fdb\u89d2\u8272\u5206\u5316\u3002", "result": "\u667a\u80fd\u4f53\u884c\u4e3a\u6210\u529f\u5206\u5316\uff0c\u65e0\u9700\u663e\u5f0f\u901a\u4fe1\u5373\u53ef\u52a8\u6001\u8c03\u6574\u89d2\u8272\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u89c5\u98df\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u89d2\u8272\u5206\u5316\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2505.00579", "pdf": "https://arxiv.org/pdf/2505.00579", "abs": "https://arxiv.org/abs/2505.00579", "authors": ["Hussam Azzuni", "Abdulmotaleb El Saddik"], "title": "Voice Cloning: Comprehensive Survey", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "26 pages, 7 figures", "summary": "Voice Cloning has rapidly advanced in today's digital world, with many\nresearchers and corporations working to improve these algorithms for various\napplications. This article aims to establish a standardized terminology for\nvoice cloning and explore its different variations. It will cover speaker\nadaptation as the fundamental concept and then delve deeper into topics such as\nfew-shot, zero-shot, and multilingual TTS within that context. Finally, we will\nexplore the evaluation metrics commonly used in voice cloning research and\nrelated datasets. This survey compiles the available voice cloning algorithms\nto encourage research toward its generation and detection to limit its misuse.", "AI": {"tldr": "\u8bba\u6587\u7efc\u8ff0\u4e86\u8bed\u97f3\u514b\u9686\u6280\u672f\u7684\u53d1\u5c55\uff0c\u63d0\u51fa\u4e86\u6807\u51c6\u672f\u8bed\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u4e0d\u540c\u53d8\u4f53\u53ca\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u63a8\u52a8\u8bed\u97f3\u514b\u9686\u6280\u672f\u7684\u7814\u7a76\uff0c\u540c\u65f6\u9650\u5236\u5176\u6ee5\u7528\u3002", "method": "\u6db5\u76d6\u4e86\u8bf4\u8bdd\u8005\u9002\u5e94\u3001\u5c11\u6837\u672c\u3001\u96f6\u6837\u672c\u548c\u591a\u8bed\u8a00TTS\u7b49\u6280\u672f\uff0c\u5e76\u5206\u6790\u4e86\u76f8\u5173\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u7efc\u8ff0\u4e86\u73b0\u6709\u7684\u8bed\u97f3\u514b\u9686\u7b97\u6cd5\u53ca\u5176\u5e94\u7528\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u65e8\u5728\u4fc3\u8fdb\u8bed\u97f3\u514b\u9686\u6280\u672f\u7684\u751f\u6210\u4e0e\u68c0\u6d4b\u7814\u7a76\uff0c\u9632\u6b62\u5176\u88ab\u6ee5\u7528\u3002"}}
{"id": "2505.00552", "pdf": "https://arxiv.org/pdf/2505.00552", "abs": "https://arxiv.org/abs/2505.00552", "authors": ["Chanwoo Kim", "Jinkyu Sung", "Yebonn Han", "Joonseok Lee"], "title": "Graph Spectral Filtering with Chebyshev Interpolation for Recommendation", "categories": ["cs.IR", "cs.LG"], "comment": "Accepted by SIGIR 2025; 11 pages, 9 figures, 5 tables", "summary": "Graph convolutional networks have recently gained prominence in collaborative\nfiltering (CF) for recommendations. However, we identify potential bottlenecks\nin two foundational components. First, the embedding layer leads to a latent\nspace with limited capacity, overlooking locally observed but potentially\nvaluable preference patterns. Also, the widely-used neighborhood aggregation is\nlimited in its ability to leverage diverse preference patterns in a\nfine-grained manner. Building on spectral graph theory, we reveal that these\nlimitations stem from graph filtering with a cut-off in the frequency spectrum\nand a restricted linear form. To address these issues, we introduce ChebyCF, a\nCF framework based on graph spectral filtering. Instead of a learned embedding,\nit takes a user's raw interaction history to utilize the full spectrum of\nsignals contained in it. Also, it adopts Chebyshev interpolation to effectively\napproximate a flexible non-linear graph filter, and further enhances it by\nusing an additional ideal pass filter and degree-based normalization. Through\nextensive experiments, we verify that ChebyCF overcomes the aforementioned\nbottlenecks and achieves state-of-the-art performance across multiple\nbenchmarks and reasonably fast inference. Our code is available at\nhttps://github.com/chanwoo0806/ChebyCF.", "AI": {"tldr": "ChebyCF\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u8c31\u8fc7\u6ee4\u7684\u534f\u540c\u8fc7\u6ee4\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u56fe\u5377\u79ef\u7f51\u7edc\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u5b8c\u6574\u7684\u4fe1\u53f7\u9891\u8c31\u548c\u7075\u6d3b\u7684Chebyshev\u63d2\u503c\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a8\u8350\u3002", "motivation": "\u4f20\u7edf\u56fe\u5377\u79ef\u7f51\u7edc\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u74f6\u9888\uff1a\u5d4c\u5165\u5c42\u5bfc\u81f4\u6f5c\u5728\u7a7a\u95f4\u5bb9\u91cf\u6709\u9650\uff0c\u4ee5\u53ca\u90bb\u57df\u805a\u5408\u65b9\u6cd5\u65e0\u6cd5\u7ec6\u7c92\u5ea6\u5229\u7528\u591a\u6837\u5316\u7684\u504f\u597d\u6a21\u5f0f\u3002", "method": "\u91c7\u7528\u56fe\u8c31\u8fc7\u6ee4\u6280\u672f\uff0c\u76f4\u63a5\u4f7f\u7528\u7528\u6237\u7684\u539f\u59cb\u4ea4\u4e92\u5386\u53f2\uff0c\u901a\u8fc7Chebyshev\u63d2\u503c\u903c\u8fd1\u975e\u7ebf\u6027\u56fe\u6ee4\u6ce2\u5668\uff0c\u5e76\u7ed3\u5408\u7406\u60f3\u901a\u6ee4\u6ce2\u5668\u548c\u57fa\u4e8e\u5ea6\u6570\u7684\u5f52\u4e00\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cChebyCF\u514b\u670d\u4e86\u4e0a\u8ff0\u74f6\u9888\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u63a8\u7406\u901f\u5ea6\u8f83\u5feb\u3002", "conclusion": "ChebyCF\u901a\u8fc7\u521b\u65b0\u7684\u56fe\u8c31\u8fc7\u6ee4\u65b9\u6cd5\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c55\u73b0\u4e86\u5728\u590d\u6742\u504f\u597d\u6a21\u5f0f\u5904\u7406\u4e0a\u7684\u4f18\u52bf\u3002"}}
{"id": "2505.00584", "pdf": "https://arxiv.org/pdf/2505.00584", "abs": "https://arxiv.org/abs/2505.00584", "authors": ["Mathis Morales", "Golnaz Habibi"], "title": "Synthesizing and Identifying Noise Levels in Autonomous Vehicle Camera Radar Datasets", "categories": ["cs.CV", "cs.AI", "eess.IV", "eess.SP"], "comment": null, "summary": "Detecting and tracking objects is a crucial component of any autonomous\nnavigation method. For the past decades, object detection has yielded promising\nresults using neural networks on various datasets. While many methods focus on\nperformance metrics, few projects focus on improving the robustness of these\ndetection and tracking pipelines, notably to sensor failures. In this paper we\nattempt to address this issue by creating a realistic synthetic data\naugmentation pipeline for camera-radar Autonomous Vehicle (AV) datasets. Our\ngoal is to accurately simulate sensor failures and data deterioration due to\nreal-world interferences. We also present our results of a baseline lightweight\nNoise Recognition neural network trained and tested on our augmented dataset,\nreaching an overall recognition accuracy of 54.4\\% on 11 categories across\n10086 images and 2145 radar point-clouds.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AV\uff09\u76f8\u673a-\u96f7\u8fbe\u6570\u636e\u96c6\u7684\u5408\u6210\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u76ee\u6807\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u5728\u4f20\u611f\u5668\u6545\u969c\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u566a\u58f0\u8bc6\u522b\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u76ee\u6807\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u65b9\u6cd5\u5728\u4f20\u611f\u5668\u6545\u969c\u548c\u6570\u636e\u9000\u5316\u60c5\u51b5\u4e0b\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5408\u6210\u6570\u636e\u589e\u5f3a\u6a21\u62df\u771f\u5b9e\u5e72\u6270\uff0c\u63d0\u5347\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u73b0\u5b9e\u7684\u5408\u6210\u6570\u636e\u589e\u5f3a\u6d41\u7a0b\uff0c\u6a21\u62df\u76f8\u673a-\u96f7\u8fbe\u4f20\u611f\u5668\u7684\u6545\u969c\u548c\u6570\u636e\u9000\u5316\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u566a\u58f0\u8bc6\u522b\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u5305\u542b10086\u5f20\u56fe\u50cf\u548c2145\u4e2a\u96f7\u8fbe\u70b9\u4e91\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u566a\u58f0\u8bc6\u522b\u795e\u7ecf\u7f51\u7edc\u572811\u4e2a\u7c7b\u522b\u4e2d\u8fbe\u5230\u4e8654.4%\u7684\u603b\u4f53\u8bc6\u522b\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5408\u6210\u6570\u636e\u589e\u5f3a\u80fd\u591f\u6709\u6548\u6a21\u62df\u4f20\u611f\u5668\u6545\u969c\uff0c\u4e3a\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.00596", "pdf": "https://arxiv.org/pdf/2505.00596", "abs": "https://arxiv.org/abs/2505.00596", "authors": ["Alex Schutz", "Yang You", "Matias Mattamala", "Ipek Caliskanelli", "Bruno Lacerda", "Nick Hawes"], "title": "A Finite-State Controller Based Offline Solver for Deterministic POMDPs", "categories": ["cs.RO", "cs.AI", "cs.LG", "I.2.8; I.2.9"], "comment": "9 pages, 6 figures. Appendix attached. To be published in Proceedings\n  of IJCAI 2025. For code see http://github.com/ori-goals/DetMCVI", "summary": "Deterministic partially observable Markov decision processes (DetPOMDPs)\noften arise in planning problems where the agent is uncertain about its\nenvironmental state but can act and observe deterministically. In this paper,\nwe propose DetMCVI, an adaptation of the Monte Carlo Value Iteration (MCVI)\nalgorithm for DetPOMDPs, which builds policies in the form of finite-state\ncontrollers (FSCs). DetMCVI solves large problems with a high success rate,\noutperforming existing baselines for DetPOMDPs. We also verify the performance\nof the algorithm in a real-world mobile robot forest mapping scenario.", "AI": {"tldr": "DetMCVI\u7b97\u6cd5\u662f\u9488\u5bf9\u786e\u5b9a\u6027\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08DetPOMDPs\uff09\u7684\u4e00\u79cd\u6539\u8fdb\u7248\u8499\u7279\u5361\u6d1b\u503c\u8fed\u4ee3\uff08MCVI\uff09\u65b9\u6cd5\uff0c\u4ee5\u6709\u9650\u72b6\u6001\u63a7\u5236\u5668\uff08FSCs\uff09\u5f62\u5f0f\u6784\u5efa\u7b56\u7565\uff0c\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "DetPOMDPs\u5728\u89c4\u5212\u95ee\u9898\u4e2d\u5e38\u89c1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u6548\u7387\u6709\u9650\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faDetMCVI\u7b97\u6cd5\uff0c\u57fa\u4e8eMCVI\u6539\u8fdb\uff0c\u901a\u8fc7\u6709\u9650\u72b6\u6001\u63a7\u5236\u5668\u6784\u5efa\u7b56\u7565\u3002", "result": "DetMCVI\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u6210\u529f\u7387\u9ad8\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u5728\u673a\u5668\u4eba\u68ee\u6797\u5730\u56fe\u573a\u666f\u4e2d\u9a8c\u8bc1\u3002", "conclusion": "DetMCVI\u4e3aDetPOMDPs\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u9645\u5e94\u7528\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2505.00571", "pdf": "https://arxiv.org/pdf/2505.00571", "abs": "https://arxiv.org/abs/2505.00571", "authors": ["Giorgio Spadaccini", "Marjolein Fokkema", "Mark A. van de Wiel"], "title": "Hypothesis-free discovery from epidemiological data by automatic detection and local inference for tree-based nonlinearities and interactions", "categories": ["stat.ML", "cs.LG"], "comment": "Main body: 29 pages, 7 figures; Supplementary material: 39 pages, 14\n  figures", "summary": "In epidemiological settings, Machine Learning (ML) is gaining popularity for\nhypothesis-free discovery of risk (or protective) factors. Although ML is\nstrong at discovering non-linearities and interactions, this power is currently\ncompromised by a lack of reliable inference. Although local measures of feature\neffect can be combined with tree ensembles, uncertainty quantifications for\nthese measures remain only partially available and oftentimes unsatisfactory.\nWe propose RuleSHAP, a framework for using rule-based, hypothesis-free\ndiscovery that combines sparse Bayesian regression, tree ensembles and Shapley\nvalues in a one-step procedure that both detects and tests complex patterns at\nthe individual level. To ease computation, we derive a formula that computes\nmarginal Shapley values more efficiently for our setting. We demonstrate the\nvalidity of our framework on simulated data. To illustrate, we apply our\nmachinery to data from an epidemiological cohort to detect and infer several\neffects for high cholesterol and blood pressure, such as nonlinear interaction\neffects between features like age, sex, ethnicity, BMI and glucose level.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RuleSHAP\u6846\u67b6\uff0c\u7ed3\u5408\u7a00\u758f\u8d1d\u53f6\u65af\u56de\u5f52\u3001\u6811\u96c6\u6210\u548cShapley\u503c\uff0c\u4ee5\u4e00\u6b65\u6cd5\u68c0\u6d4b\u548c\u6d4b\u8bd5\u4e2a\u4f53\u5c42\u9762\u7684\u590d\u6742\u6a21\u5f0f\uff0c\u5e76\u89e3\u51b3\u4e86\u6d41\u884c\u75c5\u5b66\u4e2d\u673a\u5668\u5b66\u4e60\u7f3a\u4e4f\u53ef\u9760\u63a8\u65ad\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u6d41\u884c\u75c5\u5b66\u4e2d\uff0c\u673a\u5668\u5b66\u4e60\u867d\u80fd\u53d1\u73b0\u975e\u7ebf\u6027\u5173\u7cfb\u548c\u4ea4\u4e92\u4f5c\u7528\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u53ef\u9760\u7684\u63a8\u65ad\u65b9\u6cd5\uff0c\u5176\u6f5c\u529b\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faRuleSHAP\u6846\u67b6\uff0c\u7ed3\u5408\u7a00\u758f\u8d1d\u53f6\u65af\u56de\u5f52\u3001\u6811\u96c6\u6210\u548cShapley\u503c\uff0c\u5e76\u901a\u8fc7\u63a8\u5bfc\u516c\u5f0f\u9ad8\u6548\u8ba1\u7b97\u8fb9\u9645Shapley\u503c\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u6d41\u884c\u75c5\u5b66\u961f\u5217\u6570\u636e\u4e2d\u6210\u529f\u68c0\u6d4b\u4e86\u5982\u5e74\u9f84\u3001\u6027\u522b\u3001BMI\u7b49\u7279\u5f81\u5bf9\u9ad8\u80c6\u56fa\u9187\u548c\u9ad8\u8840\u538b\u7684\u975e\u7ebf\u6027\u4ea4\u4e92\u4f5c\u7528\u3002", "conclusion": "RuleSHAP\u4e3a\u6d41\u884c\u75c5\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e2\u80fd\u53d1\u73b0\u590d\u6742\u6a21\u5f0f\u53c8\u80fd\u8fdb\u884c\u53ef\u9760\u63a8\u65ad\u7684\u5de5\u5177\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2505.00574", "pdf": "https://arxiv.org/pdf/2505.00574", "abs": "https://arxiv.org/abs/2505.00574", "authors": ["Raffaele Cheula", "Mie Andersen"], "title": "Transition States Energies from Machine Learning: An Application to Reverse Water-Gas Shift on Single-Atom Alloys", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "comment": null, "summary": "Obtaining accurate transition state (TS) energies is a bottleneck in\ncomputational screening of complex materials and reaction networks due to the\nhigh cost of TS search methods and first-principles methods such as density\nfunctional theory (DFT). Here we propose a machine learning (ML) model for\npredicting TS energies based on Gaussian process regression with the\nWasserstein Weisfeiler-Lehman graph kernel (WWL-GPR). Applying the model to\npredict adsorption and TS energies for the reverse water-gas shift (RWGS)\nreaction on single-atom alloy (SAA) catalysts, we show that it can\nsignificantly improve the accuracy compared to traditional approaches based on\nscaling relations or ML models without a graph representation. Further\nbenefitting from the low cost of model training, we train an ensemble of\nWWL-GPR models to obtain uncertainties through subsampling of the training data\nand show how these uncertainties propagate to turnover frequency (TOF)\npredictions through the construction of an ensemble of microkinetic models.\nComparing the errors in model-based vs DFT-based TOF predictions, we show that\nthe WWL-GPR model reduces errors by almost an order of magnitude compared to\nscaling relations. This demonstrates the critical impact of accurate energy\npredictions on catalytic activity estimation. Finally, we apply our model to\nscreen new materials, identifying promising catalysts for RWGS. This work\nhighlights the power of combining advanced ML techniques with DFT and\nmicrokinetic modeling for screening catalysts for complex reactions like RWGS,\nproviding a robust framework for future catalyst design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u548c\u56fe\u6838\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08WWL-GPR\uff09\uff0c\u7528\u4e8e\u9884\u6d4b\u8fc7\u6e21\u6001\u80fd\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cd\u5411\u6c34\u7164\u6c14\u53d8\u6362\u53cd\u5e94\u5728\u5355\u539f\u5b50\u5408\u91d1\u50ac\u5316\u5242\u4e0a\u7684\u80fd\u91cf\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u4f18\u5316\u4e86\u50ac\u5316\u6d3b\u6027\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edf\u7684\u7b2c\u4e00\u6027\u539f\u7406\u65b9\u6cd5\uff08\u5982DFT\uff09\u8ba1\u7b97\u8fc7\u6e21\u6001\u80fd\u91cf\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u590d\u6742\u6750\u6599\u4e0e\u53cd\u5e94\u7f51\u7edc\u7684\u9ad8\u6548\u7b5b\u9009\uff1b\u4e3a\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u9ad8\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08GPR\uff09\u7ed3\u5408Wasserstein Weisfeiler-Lehman\u56fe\u6838\uff08WWL\uff09\uff0c\u6784\u5efa\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u8fc7\u6e21\u6001\u80fd\u91cf\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u6570\u636e\u5b50\u91c7\u6837\u751f\u6210\u6a21\u578b\u96c6\u5408\u4ee5\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "WWL-GPR\u6a21\u578b\u76f8\u6bd4\u4f20\u7edf\u6807\u5ea6\u5173\u7cfb\u6216\u975e\u56fe\u8868\u793a\u7684ML\u6a21\u578b\uff0c\u8bef\u5dee\u964d\u4f4e\u8fd1\u4e00\u4e2a\u6570\u91cf\u7ea7\uff1b\u5e76\u901a\u8fc7\u5fae\u52a8\u529b\u5b66\u6a21\u578b\u9a8c\u8bc1\u5176\u5bf9\u50ac\u5316\u6d3b\u6027\u9884\u6d4b\u7684\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u4e0eDFT\u7ed3\u5408\u5728\u590d\u6742\u53cd\u5e94\uff08\u5982RWGS\uff09\u50ac\u5316\u5242\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u50ac\u5316\u5242\u7b5b\u9009\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u6846\u67b6\u3002"}}
{"id": "2505.00615", "pdf": "https://arxiv.org/pdf/2505.00615", "abs": "https://arxiv.org/abs/2505.00615", "authors": ["Simon Giebenhain", "Tobias Kirschstein", "Martin R\u00fcnz", "Lourdes Agapito", "Matthias Nie\u00dfner"], "title": "Pixel3DMM: Versatile Screen-Space Priors for Single-Image 3D Face Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": "Project Website: https://simongiebenhain.github.io/pixel3dmm/ ;\n  Video: https://www.youtube.com/watch?v=BwxwEXJwUDc", "summary": "We address the 3D reconstruction of human faces from a single RGB image. To\nthis end, we propose Pixel3DMM, a set of highly-generalized vision transformers\nwhich predict per-pixel geometric cues in order to constrain the optimization\nof a 3D morphable face model (3DMM). We exploit the latent features of the DINO\nfoundation model, and introduce a tailored surface normal and uv-coordinate\nprediction head. We train our model by registering three high-quality 3D face\ndatasets against the FLAME mesh topology, which results in a total of over\n1,000 identities and 976K images. For 3D face reconstruction, we propose a\nFLAME fitting opitmization that solves for the 3DMM parameters from the\nuv-coordinate and normal estimates. To evaluate our method, we introduce a new\nbenchmark for single-image face reconstruction, which features high diversity\nfacial expressions, viewing angles, and ethnicities. Crucially, our benchmark\nis the first to evaluate both posed and neutral facial geometry. Ultimately,\nour method outperforms the most competitive baselines by over 15% in terms of\ngeometric accuracy for posed facial expressions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPixel3DMM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u5f20RGB\u56fe\u50cf\u5b9e\u73b0\u4eba\u81383D\u91cd\u5efa\uff0c\u5229\u7528DINO\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u5728\u7279\u5f81\uff0c\u7ed3\u5408\u65b0\u8bbe\u8ba1\u7684\u8868\u9762\u6cd5\u7ebf\u548cUV\u5750\u6807\u9884\u6d4b\u5934\uff0c\u5728\u4f18\u53163D\u53ef\u53d8\u5f62\u4eba\u8138\u6a21\u578b\uff083DMM\uff09\u65f6\u663e\u8457\u63d0\u5347\u4e86\u51e0\u4f55\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u4ece\u5355\u5f20RGB\u56fe\u50cf\u9ad8\u7cbe\u5ea6\u91cd\u5efa3D\u4eba\u8138\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u591a\u6837\u5316\u7684\u9762\u90e8\u8868\u60c5\u3001\u89c6\u89d2\u548c\u79cd\u65cf\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9762\u90e8\u51e0\u4f55\uff08\u5c24\u5176\u662f\u8868\u60c5\uff09\u8bc4\u4f30\u4e0a\u7684\u7a7a\u767d\u3002", "method": "1.\u7ed3\u5408DINO\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u5728\u7279\u5f81\uff0c\u8bbe\u8ba1\u4e13\u95e8\u9884\u6d4b\u8868\u9762\u6cd5\u7ebf\u548cUV\u5750\u6807\u7684\u5934\u90e8\uff1b2.\u901a\u8fc7FLAME\u7f51\u683c\u62d3\u6251\u6ce8\u518c\u4e09\u4e2a\u9ad8\u8d28\u91cf3D\u4eba\u8138\u6570\u636e\u96c6\uff0c\u5f62\u6210\u5305\u542b1000\u591a\u4e2a\u8eab\u4efd\u548c97.6\u4e07\u5f20\u56fe\u50cf\u7684\u8bad\u7ec3\u96c6\uff1b3.\u63d0\u51fa\u57fa\u4e8eUV\u5750\u6807\u548c\u6cd5\u7ebf\u4f30\u8ba1\u7684FLAME\u62df\u5408\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u63d0\u51fa\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u5728\u51e0\u4f55\u7cbe\u5ea6\u4e0a\u8d85\u8d8a\u6700\u4f18\u57fa\u7ebf15%\u4ee5\u4e0a\uff0c\u5c24\u5176\u5728\u5e26\u6709\u8868\u60c5\u7684\u9762\u90e8\u91cd\u5efa\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "Pixel3DMM\u901a\u8fc7\u7ed3\u5408\u901a\u7528\u89c6\u89c9Transformer\u4e0e\u7279\u5b9a\u51e0\u4f55\u9884\u6d4b\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5355\u56fe\u50cf3D\u4eba\u8138\u91cd\u5efa\uff0c\u4e3a\u591a\u6837\u5316\u9762\u90e8\u8868\u8fbe\u548c\u51e0\u4f55\u8bc4\u4f30\u8bbe\u7acb\u4e86\u65b0\u6807\u6746\u3002"}}
{"id": "2505.00622", "pdf": "https://arxiv.org/pdf/2505.00622", "abs": "https://arxiv.org/abs/2505.00622", "authors": ["Colin Kessler", "Ekaterina Komendantskaya", "Marco Casadio", "Ignazio Maria Viola", "Thomas Flinkow", "Albaraa Ammar Othman", "Alistair Malhotra", "Robbie McPherson"], "title": "Neural Network Verification for Gliding Drone Control: A Case Study", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": "18 page pre print, submitted to SAIV 2025 (conference)", "summary": "As machine learning is increasingly deployed in autonomous systems,\nverification of neural network controllers is becoming an active research\ndomain. Existing tools and annual verification competitions suggest that soon\nthis technology will become effective for real-world applications. Our\napplication comes from the emerging field of microflyers that are passively\ntransported by the wind, which may have various uses in weather or pollution\nmonitoring. Specifically, we investigate centimetre-scale bio-inspired gliding\ndrones that resemble Alsomitra macrocarpa diaspores. In this paper, we propose\na new case study on verifying Alsomitra-inspired drones with neural network\ncontrollers, with the aim of adhering closely to a target trajectory. We show\nthat our system differs substantially from existing VNN and ARCH competition\nbenchmarks, and show that a combination of tools holds promise for verifying\nsuch systems in the future, if certain shortcomings can be overcome. We propose\na novel method for robust training of regression networks, and investigate\nformalisations of this case study in Vehicle and CORA. Our verification results\nsuggest that the investigated training methods do improve performance and\nrobustness of neural network controllers in this application, but are limited\nin scope and usefulness. This is due to systematic limitations of both Vehicle\nand CORA, and the complexity of our system reducing the scale of reachability,\nwhich we investigate in detail. If these limitations can be overcome, it will\nenable engineers to develop safe and robust technologies that improve people's\nlives and reduce our impact on the environment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9a8c\u8bc1\u53d7Alsomitra\u542f\u53d1\u7684\u5fae\u578b\u65e0\u4eba\u673a\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u65e8\u5728\u5b9e\u73b0\u7cbe\u786e\u7684\u76ee\u6807\u8f68\u8ff9\u8ddf\u8e2a\u3002\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u5de5\u5177\u548c\u63d0\u51fa\u65b0\u7684\u56de\u5f52\u7f51\u7edc\u9c81\u68d2\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u9a8c\u8bc1\u5de5\u5177\u7684\u5c40\u9650\u6027\u5f71\u54cd\u4e86\u9a8c\u8bc1\u6548\u679c\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u5728\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u795e\u7ecf\u7f51\ufffd\ufffd\u63a7\u5236\u5668\u7684\u9a8c\u8bc1\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\u3002\u8be5\u7814\u7a76\u6e90\u81ea\u5fae\u578b\u98de\u884c\u5668\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u5c24\u5176\u662f\u4eff\u751f\u6ed1\u7fd4\u65e0\u4eba\u673a\u5728\u6c14\u8c61\u6216\u6c61\u67d3\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u65e8\u5728\u63d0\u9ad8\u63a7\u5236\u5668\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56de\u5f52\u7f51\u7edc\u9c81\u68d2\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528Vehicle\u548cCORA\u5de5\u5177\u5bf9\u6848\u4f8b\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002\u901a\u8fc7\u6bd4\u8f83\u73b0\u6709\u5de5\u5177\u548c\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u63a2\u7d22\u4e86\u9a8c\u8bc1\u6b64\u7c7b\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6240\u63d0\u51fa\u7684\u8bad\u7ec3\u65b9\u6cd5\u80fd\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u4f46\u7531\u4e8eVehicle\u548cCORA\u7684\u7cfb\u7edf\u6027\u9650\u5236\u4ee5\u53ca\u7cfb\u7edf\u590d\u6742\u6027\uff0c\u9a8c\u8bc1\u7684\u89c4\u6a21\u548c\u4f5c\u7528\u53d7\u9650\u3002", "conclusion": "\u5c3d\u7ba1\u5f53\u524d\u9a8c\u8bc1\u5de5\u5177\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f46\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u540e\uff0c\u5c06\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u9c81\u68d2\u7684\u6280\u672f\uff0c\u4ece\u800c\u6539\u5584\u751f\u6d3b\u5e76\u51cf\u5c11\u5bf9\u73af\u5883\u7684\u5f71\u54cd\u3002"}}
{"id": "2505.00586", "pdf": "https://arxiv.org/pdf/2505.00586", "abs": "https://arxiv.org/abs/2505.00586", "authors": ["Jiarong Wei", "Niclas V\u00f6disch", "Anna Rehr", "Christian Feist", "Abhinav Valada"], "title": "ParkDiffusion: Heterogeneous Multi-Agent Multi-Modal Trajectory Prediction for Automated Parking using Diffusion Models", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Automated parking is a critical feature of Advanced Driver Assistance Systems\n(ADAS), where accurate trajectory prediction is essential to bridge perception\nand planning modules. Despite its significance, research in this domain remains\nrelatively limited, with most existing studies concentrating on single-modal\ntrajectory prediction of vehicles. In this work, we propose ParkDiffusion, a\nnovel approach that predicts the trajectories of both vehicles and pedestrians\nin automated parking scenarios. ParkDiffusion employs diffusion models to\ncapture the inherent uncertainty and multi-modality of future trajectories,\nincorporating several key innovations. First, we propose a dual map encoder\nthat processes soft semantic cues and hard geometric constraints using a\ntwo-step cross-attention mechanism. Second, we introduce an adaptive agent type\nembedding module, which dynamically conditions the prediction process on the\ndistinct characteristics of vehicles and pedestrians. Third, to ensure\nkinematic feasibility, our model outputs control signals that are subsequently\nused within a kinematic framework to generate physically feasible trajectories.\nWe evaluate ParkDiffusion on the Dragon Lake Parking (DLP) dataset and the\nIntersections Drone (inD) dataset. Our work establishes a new baseline for\nheterogeneous trajectory prediction in parking scenarios, outperforming\nexisting methods by a considerable margin.", "AI": {"tldr": "ParkDiffusion\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u81ea\u52a8\u505c\u8f66\u573a\u666f\u4e2d\u8f66\u8f86\u548c\u884c\u4eba\u7684\u8f68\u8ff9\uff0c\u901a\u8fc7\u53cc\u5730\u56fe\u7f16\u7801\u5668\u3001\u81ea\u9002\u5e94\u4ee3\u7406\u7c7b\u578b\u5d4c\u5165\u6a21\u5757\u548c\u8fd0\u52a8\u5b66\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u6a21\u6001\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u4e0a\uff0c\u5bf9\u4e8e\u81ea\u52a8\u505c\u8f66\u573a\u666f\u4e2d\u8f66\u8f86\u548c\u884c\u4eba\u8f68\u8ff9\u7684\u9884\u6d4b\u7814\u7a76\u8f83\u5c11\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u591a\u6a21\u6001\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faParkDiffusion\uff0c\u91c7\u7528\u6269\u6563\u6a21\u578b\u6355\u6349\u8f68\u8ff9\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u591a\u6a21\u6001\uff0c\u7ed3\u5408\u53cc\u5730\u56fe\u7f16\u7801\u5668\u3001\u81ea\u9002\u5e94\u4ee3\u7406\u7c7b\u578b\u5d4c\u5165\u6a21\u5757\u548c\u8fd0\u52a8\u5b66\u6846\u67b6\u751f\u6210\u7269\u7406\u53ef\u884c\u7684\u8f68\u8ff9\u3002", "result": "\u5728DLP\u548cinD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cParkDiffusion\u5728\u5f02\u6784\u8f68\u8ff9\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ParkDiffusion\u4e3a\u505c\u8f66\u573a\u666f\u4e2d\u7684\u5f02\u6784\u8f68\u8ff9\u9884\u6d4b\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.00592", "pdf": "https://arxiv.org/pdf/2505.00592", "abs": "https://arxiv.org/abs/2505.00592", "authors": ["Shuo Tong", "Shangde Gao", "Ke Liu", "Zihang Huang", "Hongxia Xu", "Haochao Ying", "Jian Wu"], "title": "Uncertainty-Aware Multi-Expert Knowledge Distillation for Imbalanced Disease Grading", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Automatic disease image grading is a significant application of artificial\nintelligence for healthcare, enabling faster and more accurate patient\nassessments. However, domain shifts, which are exacerbated by data imbalance,\nintroduce bias into the model, posing deployment difficulties in clinical\napplications. To address the problem, we propose a novel\n\\textbf{U}ncertainty-aware \\textbf{M}ulti-experts \\textbf{K}nowledge\n\\textbf{D}istillation (UMKD) framework to transfer knowledge from multiple\nexpert models to a single student model. Specifically, to extract\ndiscriminative features, UMKD decouples task-agnostic and task-specific\nfeatures with shallow and compact feature alignment in the feature space. At\nthe output space, an uncertainty-aware decoupled distillation (UDD) mechanism\ndynamically adjusts knowledge transfer weights based on expert model\nuncertainties, ensuring robust and reliable distillation. Additionally, UMKD\nalso tackles the problems of model architecture heterogeneity and distribution\ndiscrepancies between source and target domains, which are inadequately tackled\nby previous KD approaches. Extensive experiments on histology prostate grading\n(\\textit{SICAPv2}) and fundus image grading (\\textit{APTOS}) demonstrate that\nUMKD achieves a new state-of-the-art in both source-imbalanced and\ntarget-imbalanced scenarios, offering a robust and practical solution for\nreal-world disease image grading.", "AI": {"tldr": "UMKD\u6846\u67b6\u901a\u8fc7\u591a\u4e13\u5bb6\u77e5\u8bc6\u84b8\u998f\u89e3\u51b3\u533b\u7597\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u9886\u57df\u504f\u79fb\u548c\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u89e3\u8026\u84b8\u998f\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u533b\u7597\u56fe\u50cf\u81ea\u52a8\u5206\u7ea7\u4e2d\u9886\u57df\u504f\u79fb\u548c\u6570\u636e\u4e0d\u5e73\u8861\u5bfc\u81f4\u7684\u6a21\u578b\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u9ad8\u5728\u5b9e\u9645\u4e34\u5e8a\u4e2d\u7684\u90e8\u7f72\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faUMKD\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u7684\u4efb\u52a1\u65e0\u5173\u4e0e\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u89e3\u8026\uff0c\u4ee5\u53ca\u8f93\u51fa\u7a7a\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u52a8\u6001\u6743\u91cd\u8c03\u6574\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u77e5\u8bc6\u84b8\u998f\u3002", "result": "\u5728SICAPv2\u548cAPTOS\u6570\u636e\u96c6\u4e0a\uff0cUMKD\u5728\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e0d\u5e73\u8861\u573a\u666f\u4e0b\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "UMKD\u4e3a\u5b9e\u9645\u533b\u7597\u56fe\u50cf\u5206\u7ea7\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00643", "pdf": "https://arxiv.org/pdf/2505.00643", "abs": "https://arxiv.org/abs/2505.00643", "authors": ["Merve G\u00fclle", "Sebastian Weing\u00e4rtner", "Mehmet Ak\u00e7akaya"], "title": "Deep Learning Assisted Outer Volume Removal for Highly-Accelerated Real-Time Dynamic MRI", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.med-ph"], "comment": null, "summary": "Real-time (RT) dynamic MRI plays a vital role in capturing rapid\nphysiological processes, offering unique insights into organ motion and\nfunction. Among these applications, RT cine MRI is particularly important for\nfunctional assessment of the heart with high temporal resolution. RT imaging\nenables free-breathing, ungated imaging of cardiac motion, making it a crucial\nalternative for patients who cannot tolerate conventional breath-hold,\nECG-gated acquisitions. However, achieving high acceleration rates in RT cine\nMRI is challenging due to aliasing artifacts from extra-cardiac tissues,\nparticularly at high undersampling factors. In this study, we propose a novel\nouter volume removal (OVR) method to address this challenge by eliminating\naliasing contributions from non-cardiac regions in a post-processing framework.\nOur approach estimates the outer volume signal for each timeframe using\ncomposite temporal images from time-interleaved undersampling patterns, which\ninherently contain pseudo-periodic ghosting artifacts. A deep learning (DL)\nmodel is trained to identify and remove these artifacts, producing a clean\nouter volume estimate that is subsequently subtracted from the corresponding\nk-space data. The final reconstruction is performed with a physics-driven DL\n(PD-DL) method trained using an OVR-specific loss function to restore high\nspatio-temporal resolution images. Experimental results show that the proposed\nmethod at high accelerations achieves image quality that is visually comparable\nto clinical baseline images, while outperforming conventional reconstruction\ntechniques, both qualitatively and quantitatively. The proposed approach\nprovides a practical and effective solution for artifact reduction in RT cine\nMRI without requiring acquisition modifications, offering a pathway to higher\nacceleration rates while preserving diagnostic quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5916\u4f53\u79ef\u53bb\u9664\uff08OVR\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u548c\u7269\u7406\u9a71\u52a8\u7684DL\uff08PD-DL\uff09\uff0c\u7528\u4e8e\u5b9e\u65f6\u7535\u5f71MRI\u4e2d\u6d88\u9664\u975e\u5fc3\u810f\u533a\u57df\u7684\u6df7\u53e0\u4f2a\u5f71\uff0c\u63d0\u9ad8\u52a0\u901f\u6210\u50cf\u8d28\u91cf\u3002", "motivation": "\u5b9e\u65f6\u7535\u5f71MRI\u5728\u5fc3\u810f\u529f\u80fd\u8bc4\u4f30\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9ad8\u52a0\u901f\u7387\u4f1a\u5bfc\u81f4\u975e\u5fc3\u810f\u533a\u57df\u7684\u6df7\u53e0\u4f2a\u5f71\uff0c\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u65f6\u95f4\u4ea4\u9519\u6b20\u91c7\u6837\u6a21\u5f0f\u751f\u6210\u590d\u5408\u65f6\u95f4\u56fe\u50cf\uff0c\u8bad\u7ec3DL\u6a21\u578b\u8bc6\u522b\u5e76\u53bb\u9664\u4f2a\u5f71\uff0c\u518d\u7528PD-DL\u65b9\u6cd5\u6062\u590d\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "result": "\u5728\u9ad8\u52a0\u901f\u7387\u4e0b\uff0c\u56fe\u50cf\u8d28\u91cf\u4e0e\u4e34\u5e8a\u57fa\u7ebf\u56fe\u50cf\u76f8\u5f53\uff0c\u4e14\u4f18\u4e8e\u4f20\u7edf\u91cd\u5efa\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u4fee\u6539\u91c7\u96c6\u534f\u8bae\u5373\u53ef\u6709\u6548\u51cf\u5c11\u4f2a\u5f71\uff0c\u4e3a\u9ad8\u52a0\u901f\u7387\u5b9e\u65f6MRI\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.00606", "pdf": "https://arxiv.org/pdf/2505.00606", "abs": "https://arxiv.org/abs/2505.00606", "authors": ["Wallace Lee", "YuHao Chen"], "title": "Dietary Intake Estimation via Continuous 3D Reconstruction of Food", "categories": ["cs.CV", "cs.LG"], "comment": "2025 CVPR MetaFood Workshop", "summary": "Monitoring dietary habits is crucial for preventing health risks associated\nwith overeating and undereating, including obesity, diabetes, and\ncardiovascular diseases. Traditional methods for tracking food intake rely on\nself-reported data before or after the eating, which are prone to inaccuracies.\nThis study proposes an approach to accurately monitor ingest behaviours by\nleveraging 3D food models constructed from monocular 2D video. Using COLMAP and\npose estimation algorithms, we generate detailed 3D representations of food,\nallowing us to observe changes in food volume as it is consumed. Experiments\nwith toy models and real food items demonstrate the approach's potential.\nMeanwhile, we have proposed a new methodology for automated state recognition\nchallenges to accurately detect state changes and maintain model fidelity. The\n3D reconstruction approach shows promise in capturing comprehensive dietary\nbehaviour insights, ultimately contributing to the development of automated and\naccurate dietary monitoring tools.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5355\u76ee2D\u89c6\u9891\u6784\u5efa3D\u98df\u7269\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7cbe\u786e\u76d1\u6d4b\u996e\u98df\u884c\u4e3a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u81ea\u62a5\u544a\u6570\u636e\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u996e\u98df\u76d1\u6d4b\u4f9d\u8d56\u81ea\u62a5\u544a\u6570\u636e\uff0c\u5bb9\u6613\u4ea7\u751f\u8bef\u5dee\uff0c\u4e0d\u5229\u4e8e\u5065\u5eb7\u98ce\u9669\u9884\u9632\uff08\u5982\u80a5\u80d6\u3001\u7cd6\u5c3f\u75c5\u7b49\uff09\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8e3D\u91cd\u5efa\u7684\u81ea\u52a8\u5316\u76d1\u6d4b\u65b9\u6cd5\u3002", "method": "\u5229\u7528COLMAP\u548c\u59ff\u6001\u4f30\u8ba1\u7b97\u6cd5\u4ece\u5355\u76ee2D\u89c6\u9891\u751f\u62103D\u98df\u7269\u6a21\u578b\uff0c\u901a\u8fc7\u98df\u7269\u4f53\u79ef\u53d8\u5316\u76d1\u6d4b\u6444\u5165\u884c\u4e3a\uff0c\u5e76\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u72b6\u6001\u8bc6\u522b\u65b9\u6cd5\u4ee5\u786e\u4fdd\u6a21\u578b\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u73a9\u5177\u6a21\u578b\u548c\u771f\u5b9e\u98df\u7269\u4e0a\u5747\u80fd\u6709\u6548\u6355\u83b7\u996e\u98df\u884c\u4e3a\u53d8\u5316\uff0c3D\u91cd\u5efa\u6280\u672f\u5c55\u73b0\u51fa\u6f5c\u5728\u7684\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u8be53D\u91cd\u5efa\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u3001\u9ad8\u7cbe\u5ea6\u7684\u996e\u98df\u76d1\u6d4b\u5de5\u5177\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u6709\u671b\u6539\u5584\u4f20\u7edf\u76d1\u6d4b\u65b9\u5f0f\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.00625", "pdf": "https://arxiv.org/pdf/2505.00625", "abs": "https://arxiv.org/abs/2505.00625", "authors": ["Liu Junchi", "Tang Ying", "Tretiak Sergei", "Duan Wenhui", "Zhou Liujiang"], "title": "SA-GAT-SR: Self-Adaptable Graph Attention Networks with Symbolic Regression for high-fidelity material property prediction", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci", "cs.LG"], "comment": null, "summary": "Recent advances in machine learning have demonstrated an enormous utility of\ndeep learning approaches, particularly Graph Neural Networks (GNNs) for\nmaterials science. These methods have emerged as powerful tools for\nhigh-throughput prediction of material properties, offering a compelling\nenhancement and alternative to traditional first-principles calculations. While\nthe community has predominantly focused on developing increasingly complex and\nuniversal models to enhance predictive accuracy, such approaches often lack\nphysical interpretability and insights into materials behavior. Here, we\nintroduce a novel computational paradigm, Self-Adaptable Graph Attention\nNetworks integrated with Symbolic Regression (SA-GAT-SR), that synergistically\ncombines the predictive capability of GNNs with the interpretative power of\nsymbolic regression. Our framework employs a self-adaptable encoding algorithm\nthat automatically identifies and adjust attention weights so as to screen\ncritical features from an expansive 180-dimensional feature space while\nmaintaining O(n) computational scaling. The integrated SR module subsequently\ndistills these features into compact analytical expressions that explicitly\nreveal quantum-mechanically meaningful relationships, achieving 23 times\nacceleration compared to conventional SR implementations that heavily rely on\nfirst principle calculations-derived features as input. This work suggests a\nnew framework in computational materials science, bridging the gap between\npredictive accuracy and physical interpretability, offering valuable physical\ninsights into material behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u4e0e\u7b26\u53f7\u56de\u5f52\u7684\u65b0\u65b9\u6cd5SA-GAT-SR\uff0c\u517c\u987e\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u7269\u7406\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u6750\u6599\u79d1\u5b66\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7279\u5f81\u7b5b\u9009\u4e0e\u52a0\u901f\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982GNN\uff09\u5728\u6750\u6599\u79d1\u5b66\u4e2d\u867d\u7136\u9884\u6d4b\u80fd\u529b\u5f3a\uff0c\u4f46\u7f3a\u4e4f\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\uff0c\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9884\u6d4b\u7cbe\u5ea6\u53c8\u80fd\u63d0\u4f9b\u7269\u7406\u89e3\u91ca\u7684\u65b0\u6846\u67b6\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86SA-GAT-SR\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7684\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7b5b\u9009\u5173\u952e\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u7b26\u53f7\u56de\u5f52\u751f\u6210\u7d27\u51d1\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u63ed\u793a\u4e86\u91cf\u5b50\u529b\u5b66\u610f\u4e49\u4e0a\u7684\u5173\u7cfb\u3002", "result": "\u8be5\u65b9\u6cd5\u5728180\u7ef4\u7279\u5f81\u7a7a\u95f4\u4e2d\u9ad8\u6548\u7b5b\u9009\u7279\u5f81\uff0c\u4fdd\u6301\u4e86O(n)\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e14\u76f8\u6bd4\u4f20\u7edf\u7b26\u53f7\u56de\u5f52\u5b9e\u73b0\u4e8623\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u7269\u7406\u89e3\u91ca\u3002", "conclusion": "SA-GAT-SR\u5728\u8ba1\u7b97\u6750\u6599\u79d1\u5b66\u4e2d\u67b6\u8d77\u4e86\u9884\u6d4b\u7cbe\u5ea6\u4e0e\u7269\u7406\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u4e3a\u7406\u89e3\u6750\u6599\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.00631", "pdf": "https://arxiv.org/pdf/2505.00631", "abs": "https://arxiv.org/abs/2505.00631", "authors": ["Yi Yang", "Yinghui Huang", "Xiangyu Chang"], "title": "Bayes-Optimal Fair Classification with Multiple Sensitive Features", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Existing theoretical work on Bayes-optimal fair classifiers usually considers\na single (binary) sensitive feature. In practice, individuals are often defined\nby multiple sensitive features. In this paper, we characterize the\nBayes-optimal fair classifier for multiple sensitive features under general\napproximate fairness measures, including mean difference and mean ratio. We\nshow that these approximate measures for existing group fairness notions,\nincluding Demographic Parity, Equal Opportunity, Predictive Equality, and\nAccuracy Parity, are linear transformations of selection rates for specific\ngroups defined by both labels and sensitive features. We then characterize that\nBayes-optimal fair classifiers for multiple sensitive features become\ninstance-dependent thresholding rules that rely on a weighted sum of these\ngroup membership probabilities. Our framework applies to both attribute-aware\nand attribute-blind settings and can accommodate composite fairness notions\nlike Equalized Odds. Building on this, we propose two practical algorithms for\nBayes-optimal fair classification via in-processing and post-processing. We\nshow empirically that our methods compare favorably to existing methods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u591a\u654f\u611f\u7279\u5f81\u4e0b\u8d1d\u53f6\u65af\u6700\u4f18\u516c\u5e73\u5206\u7c7b\u5668\u7684\u7279\u6027\uff0c\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u4e00\u822c\u8fd1\u4f3c\u516c\u5e73\u5ea6\u91cf\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u8d1d\u53f6\u65af\u6700\u4f18\u516c\u5e73\u5206\u7c7b\u5668\u7406\u8bba\u901a\u5e38\u4ec5\u8003\u8651\u5355\u4e00\uff08\u4e8c\u5143\uff09\u654f\u611f\u7279\u5f81\uff0c\u800c\u5b9e\u8df5\u4e2d\u4e2a\u4f53\u5e38\u7531\u591a\u4e2a\u654f\u611f\u7279\u5f81\u5b9a\u4e49\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7814\u7a76\u591a\u654f\u611f\u7279\u5f81\u4e0b\u7684\u516c\u5e73\u5206\u7c7b\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u7ebf\u6027\u53d8\u6362\u5c06\u8fd1\u4f3c\u516c\u5e73\u5ea6\u91cf\uff08\u5982\u5747\u503c\u5dee\u5f02\u548c\u5747\u503c\u6bd4\uff09\u4e0e\u7279\u5b9a\u7fa4\u4f53\u9009\u62e9\u7387\u5173\u8054\uff0c\u5e76\u63a8\u5bfc\u51fa\u591a\u654f\u611f\u7279\u5f81\u4e0b\u8d1d\u53f6\u65af\u6700\u4f18\u516c\u5e73\u5206\u7c7b\u5668\u7684\u5b9e\u4f8b\u4f9d\u8d56\u9608\u503c\u89c4\u5219\u3002\u8fd8\u63d0\u51fa\u4e24\u79cd\u5b9e\u9645\u7b97\u6cd5\uff1a\u5904\u7406\u4e2d\u548c\u540e\u5904\u7406\u65b9\u6cd5\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u8d1d\u53f6\u65af\u6700\u4f18\u516c\u5e73\u5206\u7c7b\u5668\u8868\u73b0\u4e3a\u52a0\u6743\u7fa4\u4f53\u6982\u7387\u548c\u7684\u9608\u503c\u89c4\u5219\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u4e3a\u591a\u654f\u611f\u7279\u5f81\u4e0b\u7684\u516c\u5e73\u5206\u7c7b\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u6269\u5c55\u4e86\u8d1d\u53f6\u65af\u6700\u4f18\u516c\u5e73\u5206\u7c7b\u5668\u7684\u9002\u7528\u8303\u56f4\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2505.00668", "pdf": "https://arxiv.org/pdf/2505.00668", "abs": "https://arxiv.org/abs/2505.00668", "authors": ["Kirtan Rajesh", "Suvidha Rupesh Kumar"], "title": "Deep Reinforcement Learning for Urban Air Quality Management: Multi-Objective Optimization of Pollution Mitigation Booth Placement in Metropolitan Environments", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Urban air pollution remains a pressing global concern, particularly in\ndensely populated and traffic-intensive metropolitan areas like Delhi, where\nexposure to harmful pollutants severely impacts public health. Delhi, being one\nof the most polluted cities globally, experiences chronic air quality issues\ndue to vehicular emissions, industrial activities, and construction dust, which\nexacerbate its already fragile atmospheric conditions. Traditional pollution\nmitigation strategies, such as static air purifying installations, often fail\nto maximize their impact due to suboptimal placement and limited adaptability\nto dynamic urban environments. This study presents a novel deep reinforcement\nlearning (DRL) framework to optimize the placement of air purification booths\nto improve the air quality index (AQI) in the city of Delhi. We employ Proximal\nPolicy Optimization (PPO), a state-of-the-art reinforcement learning algorithm,\nto iteratively learn and identify high-impact locations based on multiple\nspatial and environmental factors, including population density, traffic\npatterns, industrial influence, and green space constraints. Our approach is\nbenchmarked against conventional placement strategies, including random and\ngreedy AQI-based methods, using multi-dimensional performance evaluation\nmetrics such as AQI improvement, spatial coverage, population and traffic\nimpact, and spatial entropy. Experimental results demonstrate that the RL-based\napproach outperforms baseline methods by achieving a balanced and effective\ndistribution of air purification infrastructure. Notably, the DRL framework\nachieves an optimal trade-off between AQI reduction and high-coverage\ndeployment, ensuring equitable environmental benefits across urban regions. The\nfindings underscore the potential of AI-driven spatial optimization in\nadvancing smart city initiatives and data-driven urban air quality management.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u4f18\u5316\u7a7a\u6c14\u51c0\u5316\u4ead\u7684\u90e8\u7f72\u4f4d\u7f6e\uff0c\u4ee5\u63d0\u5347\u5370\u5ea6\u5fb7\u91cc\u7684\u7a7a\u6c14\u8d28\u91cf\uff0c\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5fb7\u91cc\u662f\u5168\u7403\u6c61\u67d3\u6700\u4e25\u91cd\u7684\u57ce\u5e02\u4e4b\u4e00\uff0c\u4f20\u7edf\u9759\u6001\u51c0\u5316\u8bbe\u5907\u90e8\u7f72\u6548\u7387\u4f4e\uff0c\u4e9f\u9700\u52a8\u6001\u4f18\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7b97\u6cd5\uff0c\u7ed3\u5408\u4eba\u53e3\u5bc6\u5ea6\u3001\u4ea4\u901a\u6a21\u5f0f\u7b49\u7a7a\u95f4\u73af\u5883\u56e0\u7d20\uff0c\u52a8\u6001\u5b66\u4e60\u6700\u4f18\u51c0\u5316\u4ead\u90e8\u7f72\u4f4d\u7f6e\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728AQI\u6539\u5584\u3001\u7a7a\u95f4\u8986\u76d6\u5ea6\u548c\u516c\u5e73\u6027\u4e0a\u5747\u4f18\u4e8e\u968f\u673a\u6216\u8d2a\u5fc3\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u7a7a\u95f4\u4f18\u5316\u53ef\u63a8\u52a8\u667a\u6167\u57ce\u5e02\u5efa\u8bbe\u548c\u6570\u636e\u9a71\u52a8\u7684\u7a7a\u6c14\u8d28\u91cf\u6cbb\u7406\u3002"}}
{"id": "2505.00684", "pdf": "https://arxiv.org/pdf/2505.00684", "abs": "https://arxiv.org/abs/2505.00684", "authors": ["Tiange Luo", "Lajanugen Logeswaran", "Justin Johnson", "Honglak Lee"], "title": "Visual Test-time Scaling for GUI Agent Grounding", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce RegionFocus, a visual test-time scaling approach for Vision\nLanguage Model Agents. Understanding webpages is challenging due to the visual\ncomplexity of GUI images and the large number of interface elements, making\naccurate action selection difficult. Our approach dynamically zooms in on\nrelevant regions, reducing background clutter and improving grounding accuracy.\nTo support this process, we propose an image-as-map mechanism that visualizes\nkey landmarks at each step, providing a transparent action record and enables\nthe agent to effectively choose among action candidates. Even with a simple\nregion selection strategy, we observe significant performance gains of 28+\\% on\nScreenspot-pro and 24+\\% on WebVoyager benchmarks on top of two\nstate-of-the-art open vision language model agents, UI-TARS and Qwen2.5-VL,\nhighlighting the effectiveness of visual test-time scaling in interactive\nsettings. We achieve a new state-of-the-art grounding performance of 61.6\\% on\nthe ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model.\nOur code will be released publicly at https://github.com/tiangeluo/RegionFocus.", "AI": {"tldr": "RegionFocus\u662f\u4e00\u4e2a\u89c6\u89c9\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u653e\u5927\u76f8\u5173\u533a\u57df\u51cf\u5c11\u80cc\u666f\u5e72\u6270\uff0c\u63d0\u9ad8\u8bed\u8a00\u89c6\u89c9\u6a21\u578b\u4ee3\u7406\u7684\u51c6\u786e\u6027\uff0c\u5728\u4e24\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u63d0\u5347\u8d85\u8fc724%\u3002", "motivation": "\u7531\u4e8eGUI\u56fe\u50cf\u7684\u89c6\u89c9\u590d\u6742\u6027\u548c\u5927\u91cf\u754c\u9762\u5143\u7d20\uff0c\u7406\u89e3\u7f51\u9875\u5185\u5bb9\u5e76\u7cbe\u786e\u9009\u62e9\u52a8\u4f5c\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u52a8\u6001\u7f29\u653e\u76f8\u5173\u533a\u57df\u7684\u65b9\u6cd5\u4ee5\u63d0\u5347\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86RegionFocus\u65b9\u6cd5\uff0c\u91c7\u7528\u52a8\u6001\u7f29\u653e\u76f8\u5173\u533a\u57df\u4ee5\u53ca\u56fe\u50cf\u4f5c\u4e3a\u5730\u56fe\u7684\u673a\u5236\uff0c\u53ef\u89c6\u5316\u5173\u952e\u5730\u6807\uff0c\u5e2e\u52a9\u4ee3\u7406\u66f4\u900f\u660e\u5730\u8bb0\u5f55\u52a8\u4f5c\u5e76\u6709\u6548\u9009\u62e9\u5019\u9009\u52a8\u4f5c\u3002", "result": "\u5728Screenspot-pro\u548cWebVoyager\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u5206\u522b\u63d0\u5347\u4e8628%\u548c24%\uff0c\u5e76\u5728Qwen2.5-VL-72B\u6a21\u578b\u4e0a\u8fbe\u5230\u4e8661.6%\u7684\u6700\u65b0\u63a5\u5730\u6027\u80fd\u3002", "conclusion": "RegionFocus\u901a\u8fc7\u89c6\u89c9\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u89c6\u89c9\u6a21\u578b\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4ea4\u4e92\u5f0f\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.00690", "pdf": "https://arxiv.org/pdf/2505.00690", "abs": "https://arxiv.org/abs/2505.00690", "authors": ["Wayne Wu", "Honglin He", "Chaoyuan Zhang", "Jack He", "Seth Z. Zhao", "Ran Gong", "Quanyi Li", "Bolei Zhou"], "title": "Towards Autonomous Micromobility through Scalable Urban Simulation", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "CVPR 2025 Highlight. Project page:\n  https://metadriverse.github.io/urban-sim/", "summary": "Micromobility, which utilizes lightweight mobile machines moving in urban\npublic spaces, such as delivery robots and mobility scooters, emerges as a\npromising alternative to vehicular mobility. Current micromobility depends\nmostly on human manual operation (in-person or remote control), which raises\nsafety and efficiency concerns when navigating busy urban environments full of\nunpredictable obstacles and pedestrians. Assisting humans with AI agents in\nmaneuvering micromobility devices presents a viable solution for enhancing\nsafety and efficiency. In this work, we present a scalable urban simulation\nsolution to advance autonomous micromobility. First, we build URBAN-SIM - a\nhigh-performance robot learning platform for large-scale training of embodied\nagents in interactive urban scenes. URBAN-SIM contains three critical modules:\nHierarchical Urban Generation pipeline, Interactive Dynamics Generation\nstrategy, and Asynchronous Scene Sampling scheme, to improve the diversity,\nrealism, and efficiency of robot learning in simulation. Then, we propose\nURBAN-BENCH - a suite of essential tasks and benchmarks to gauge various\ncapabilities of the AI agents in achieving autonomous micromobility.\nURBAN-BENCH includes eight tasks based on three core skills of the agents:\nUrban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robots\nwith heterogeneous embodiments, such as the wheeled and legged robots, across\nthese tasks. Experiments on diverse terrains and urban structures reveal each\nrobot's strengths and limitations.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u57ce\u5e02\u4eff\u771f\u89e3\u51b3\u65b9\u6848URBAN-SIM\u548c\u8bc4\u6d4b\u57fa\u51c6URBAN-BENCH\uff0c\u7528\u4e8e\u63d0\u5347\u81ea\u4e3b\u5fae\u578b\u79fb\u52a8\u6027\uff08\u5982\u914d\u9001\u673a\u5668\u4eba\uff09\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u5fae\u578b\u79fb\u52a8\u6027\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u64cd\u4f5c\uff0c\u5728\u590d\u6742\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u5b58\u5728\u5b89\u5168\u548c\u6548\u7387\u95ee\u9898\uff0c\u800c\u901a\u8fc7AI\u4ee3\u7406\u8f85\u52a9\u64cd\u4f5c\u53ef\u4f18\u5316\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bba\u6587\u9996\u5148\u6784\u5efa\u4e86URBAN-SIM\u5e73\u53f0\uff0c\u5305\u542b\u5206\u5c42\u57ce\u5e02\u751f\u6210\u3001\u4ea4\u4e92\u5f0f\u52a8\u529b\u5b66\u751f\u6210\u548c\u5f02\u6b65\u573a\u666f\u91c7\u6837\u6a21\u5757\uff1b\u968f\u540e\u63d0\u51faURBAN-BENCH\u57fa\u51c6\uff0c\u5305\u62ec8\u9879\u4efb\u52a1\u4ee5\u8bc4\u4f30AI\u4ee3\u7406\u7684\u4e09\u79cd\u6838\u5fc3\u6280\u80fd\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u56db\u79cd\u4e0d\u540c\u5f62\u6001\u7684\u673a\u5668\u4eba\uff08\u5982\u8f6e\u5f0f\u548c\u817f\u5f0f\uff09\u5728\u4e0d\u540c\u5730\u5f62\u548c\u57ce\u5e02\u7ed3\u6784\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u5404\u81ea\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u81ea\u4e3b\u5fae\u578b\u79fb\u52a8\u6027\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u4eff\u771f\u548c\u8bc4\u4f30\u5de5\u5177\uff0c\u5c55\u793a\u4e86AI\u4ee3\u7406\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.00693", "pdf": "https://arxiv.org/pdf/2505.00693", "abs": "https://arxiv.org/abs/2505.00693", "authors": ["Yanbang Li", "Ziyang Gong", "Haoyang Li", "Haoyang Li", "Xiaoqi Huang", "Haolan Kang", "Guangping Bai", "Xianzheng Ma"], "title": "Robotic Visual Instruction", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Recently, natural language has been the primary medium for human-robot\ninteraction. However, its inherent lack of spatial precision for robotic\ncontrol introduces challenges such as ambiguity and verbosity. To address these\nlimitations, we introduce the Robotic Visual Instruction (RoVI), a novel\nparadigm to guide robotic tasks through an object-centric, hand-drawn symbolic\nrepresentation. RoVI effectively encodes spatial-temporal information into\nhuman-interpretable visual instructions through 2D sketches, utilizing arrows,\ncircles, colors, and numbers to direct 3D robotic manipulation. To enable\nrobots to understand RoVI better and generate precise actions based on RoVI, we\npresent Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for\nRoVI-conditioned policies. This approach leverages Vision-Language Models\n(VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from\n2D pixel space via keypoint extraction, and then transform them into executable\n3D action sequences. We additionally curate a specialized dataset of 15K\ninstances to fine-tune small VLMs for edge deployment, enabling them to\neffectively learn RoVI capabilities. Our approach is rigorously validated\nacross 11 novel tasks in both real and simulated environments, demonstrating\nsignificant generalization capability. Notably, VIEW achieves an 87.5% success\nrate in real-world scenarios involving unseen tasks that feature multi-step\nactions, with disturbances, and trajectory-following requirements. Code and\nDatasets in this paper will be released soon.", "AI": {"tldr": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u624b\u7ed8\u7b26\u53f7\u8868\u793a\u7684\u65b0\u578b\u673a\u5668\u4eba\u89c6\u89c9\u6307\u4ee4\uff08RoVI\uff09\u8303\u5f0f\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u89e3\u6790\u5e76\u751f\u6210\u7cbe\u786e\u76843D\u52a8\u4f5c\u5e8f\u5217\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u56e0\u7f3a\u4e4f\u7a7a\u95f4\u7cbe\u5ea6\u800c\u5b58\u5728\u6a21\u7cca\u6027\u548c\u5197\u957f\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u76f4\u89c2\u3001\u7cbe\u786e\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86RoVI\u89c6\u89c9\u6307\u4ee4\u8303\u5f0f\uff0c\u7ed3\u5408VIEW\u6d41\u7a0b\uff0c\u5229\u7528VLMs\u89e3\u67902D\u624b\u7ed8\u7b26\u53f7\uff08\u5982\u7bad\u5934\u3001\u5706\u5708\u7b49\uff09\uff0c\u63d0\u53d6\u5173\u952e\u70b9\u5e76\u751f\u62103D\u52a8\u4f5c\u5e8f\u5217\u3002\u540c\u65f6\uff0c\u901a\u8fc715K\u6837\u672c\u6570\u636e\u96c6\u5fae\u8c03\u5c0f\u89c4\u6a21VLMs\u4ee5\u9002\u5e94\u8fb9\u7f18\u90e8\u7f72\u3002", "result": "\u572811\u9879\u65b0\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u771f\u5b9e\u573a\u666f\u4e0b\u5bf9\u672a\u89c1\u4efb\u52a1\u7684\u6210\u529f\u7387\u9ad8\u8fbe87.5%\uff0c\u5c24\u5176\u64c5\u957f\u591a\u6b65\u52a8\u4f5c\u3001\u5e72\u6270\u73af\u5883\u548c\u8f68\u8ff9\u8ddf\u8e2a\u3002", "conclusion": "RoVI\u548cVIEW\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6cdb\u5316\u6027\u5f3a\u7684\u89c6\u89c9\u6307\u4ee4\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u81ea\u7136\u8bed\u8a00\u7684\u5c40\u9650\u6027\uff0c\u4ee3\u7801\u4e0e\u6570\u636e\u96c6\u5c06\u5f00\u6e90\u3002"}}
