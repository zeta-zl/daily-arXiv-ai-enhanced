<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 16]
- [cs.LG](#cs.LG) [Total: 52]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CY](#cs.CY) [Total: 7]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.RO](#cs.RO) [Total: 10]
- [stat.ME](#stat.ME) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 15]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.SE](#cs.SE) [Total: 4]
- [eess.AS](#eess.AS) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 2]
- [cs.IR](#cs.IR) [Total: 3]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [math.ST](#math.ST) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [math.CT](#math.CT) [Total: 1]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [cs.HC](#cs.HC) [Total: 6]
- [cs.CV](#cs.CV) [Total: 32]
- [cs.MA](#cs.MA) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.GR](#cs.GR) [Total: 2]
- [stat.CO](#stat.CO) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence](https://arxiv.org/abs/2505.08828)
*Eduardo Araujo Oliveira,Madhavi Mohoni,Sonsoles López-Pernas,Mohammed Saqr*

Main category: cs.CL

TL;DR: 该研究通过作者验证（AV）技术量化学生在学术写作中的AI辅助，旨在提升透明度和学生发展，并在多个数据集中验证了其方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于教育环境中人机协作的普及，需要一种透明且可解释的方法来测量AI在学术写作中的参与程度，以支持学术诚信和学生发展。

Method: 研究分为三个阶段：数据集选择与扩展、AV方法开发、系统评估。采用了三个数据集（包括PAN-14和墨尔本大学学生数据），开发了改进的特征向量差异AV方法，并评估了其在不同场景下的效果。

Result: 结果表明，改进的AV分类器能有效识别学生写作与AI生成文本的差异，并在单词和句子层面测量人机协作，为教育者提供了透明工具。

Conclusion: 该研究推动了AV技术的发展，为AI时代的学术写作动态提供了可行见解，促进了学术诚信和学生发展的透明化。

Abstract: As human-AI collaboration becomes increasingly prevalent in educational
contexts, understanding and measuring the extent and nature of such
interactions pose significant challenges. This research investigates the use of
authorship verification (AV) techniques not as a punitive measure, but as a
means to quantify AI assistance in academic writing, with a focus on promoting
transparency, interpretability, and student development. Building on prior
work, we structured our investigation into three stages: dataset selection and
expansion, AV method development, and systematic evaluation. Using three
datasets - including a public dataset (PAN-14) and two from University of
Melbourne students from various courses - we expanded the data to include
LLM-generated texts, totalling 1,889 documents and 540 authorship problems from
506 students. We developed an adapted Feature Vector Difference AV methodology
to construct robust academic writing profiles for students, designed to capture
meaningful, individual characteristics of their writing. The method's
effectiveness was evaluated across multiple scenarios, including distinguishing
between student-authored and LLM-generated texts and testing resilience against
LLMs' attempts to mimic student writing styles. Results demonstrate the
enhanced AV classifier's ability to identify stylometric discrepancies and
measure human-AI collaboration at word and sentence levels while providing
educators with a transparent tool to support academic integrity investigations.
This work advances AV technology, offering actionable insights into the
dynamics of academic writing in an AI-driven era.

</details>


### [2] [Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives](https://arxiv.org/abs/2505.08891)
*Daeun Hwang,Samuel Shields,Alex Calderwood,Shi Johnson-Bey,Michael Mateas,Noah Wardrip-Fruin,Edward F. Melcer*

Main category: cs.CL

TL;DR: 对比静态与动态叙事教育游戏对学习动机的影响，动态叙事更注重内容响应性与选择多样性，但需平衡教学目标与叙事动态性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨动态叙事（基于AI）相比传统静态叙事在提升学习动机方面的潜在优势，填补了动态叙事在教育游戏中影响的研究空白。

Method: 通过比较静态分支叙事与动态序列化叙事的两个版本教育游戏《Academical》，分析玩家参与度与设计挑战。

Result: 动态叙事能提升玩家参与度（因响应性内容与多样选择），但需权衡教学目标的明确性与叙事灵活性。

Conclusion: AI驱动的动态叙事在教育游戏中具有潜力，但需进一步解决教学与叙事动态性的平衡问题。

Abstract: Motivation is an important factor underlying successful learning. Previous
research has demonstrated the positive effects that static interactive
narrative games can have on motivation. Concurrently, advances in AI have made
dynamic and adaptive approaches to interactive narrative increasingly
accessible. However, limited work has explored the impact that dynamic
narratives can have on learner motivation. In this paper, we compare two
versions of Academical, a choice-based educational interactive narrative game
about research ethics. One version employs a traditional hand-authored
branching plot (i.e., static narrative) while the other dynamically sequences
plots during play (i.e., dynamic narrative). Results highlight the importance
of responsive content and a variety of choices for player engagement, while
also illustrating the challenge of balancing pedagogical goals with the dynamic
aspects of narrative. We also discuss design implications that arise from these
findings. Ultimately, this work provides initial steps to illuminate the
emerging potential of AI-driven dynamic narrative in educational games.

</details>


### [3] [A suite of LMs comprehend puzzle statements as well as humans](https://arxiv.org/abs/2505.08996)
*Adele E Goldberg,Supantho Rakshit,Jennifer Hu,Kyle Mahowald*

Main category: cs.CL

TL;DR: 论文重新评估了人类与大型语言模型（LMs）在理解英语语句上的表现，发现人类的表现被高估，而模型的能力被低估；实验显示，限制人类重读后其准确率低于某些LM，并发现两者在理解交互性动作时存在类似的挑战。


<details>
  <summary>Details</summary>
Motivation: 反驳前人研究中人类语言理解优于LMs的结论，揭示实验设计和编码实践对结果的影响。

Method: 使用相同的刺激材料，进行预注册研究，比较人类在允许重读和限制重读两种条件下的表现，并与多个LM（如Falcon-180B-Chat、GPT-4等）对比。

Result: 限制重读后人类准确率（73%）低于Falcon-180B-Chat（76%）和GPT-4（81%），GPT-o1模型达100%；双方在交互性动作理解上表现相似。

Conclusion: 需更严谨的实验设计和编码以评估LM，当前模型的语言理解能力未必弱于人类。

Abstract: Recent claims suggest that large language models (LMs) underperform humans in
comprehending minimally complex English statements (Dentella et al., 2024).
Here, we revisit those findings and argue that human performance was
overestimated, while LLM abilities were underestimated. Using the same stimuli,
we report a preregistered study comparing human responses in two conditions:
one allowed rereading (replicating the original study), and one that restricted
rereading (a more naturalistic comprehension test). Human accuracy dropped
significantly when rereading was restricted (73%), falling below that of
Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect
accuracy. Results further show that both humans and models are
disproportionately challenged by queries involving potentially reciprocal
actions (e.g., kissing), suggesting shared pragmatic sensitivities rather than
model-specific deficits. Additional analyses using Llama-2-70B log
probabilities, a recoding of open-ended model responses, and grammaticality
ratings of other sentences reveal systematic underestimation of model
performance. We find that GPT-4o can align with either naive or expert
grammaticality judgments, depending on prompt framing. These findings
underscore the need for more careful experimental design and coding practices
in LLM evaluation, and they challenge the assumption that current models are
inherently weaker than humans at language comprehension.

</details>


### [4] [For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies](https://arxiv.org/abs/2505.09005)
*Nicole Cuneo,Eleanor Graves,Supantho Rakshit,Adele E. Goldberg*

Main category: cs.CL

TL;DR: 本文探讨了GPT-4是否能像人类一样理解自然语言并生成可靠的元语言判断，尤其是能否捕捉信息结构与句法之间的关系。实验结果表明，GPT-4在零样本任务中展现了与人类相似的判断能力，并验证了信息结构对句法可接受性的因果影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证语言模型（尤其是GPT-4）是否能重现人类语言中信息结构与长距离依赖（LDD）可接受性之间的微妙关系，这一问题在语言学中备受争议。

Method: 通过设计信息结构任务和LDD可接受性任务，以零样本方式测试GPT-4，并扩展实验验证因果关系（如通过操纵上下文句子的信息结构）。

Result: GPT-4在信息结构和可接受性任务中表现出可靠的元语言能力，且实验结果与人类数据一致。同时，研究确认了信息结构对LDD可接受性的因果影响。

Conclusion: 结论是GPT-4的表现揭示了自然语言与生成语言之间、信息结构与句法之间的紧密关系，值得进一步探索。

Abstract: It remains debated how well any LM understands natural language or generates
reliable metalinguistic judgments. Moreover, relatively little work has
demonstrated that LMs can represent and respect subtle relationships between
form and function proposed by linguists. We here focus on a particular such
relationship established in recent work: English speakers' judgments about the
information structure of canonical sentences predicts independently collected
acceptability ratings on corresponding 'long distance dependency' [LDD]
constructions, across a wide array of base constructions and multiple types of
LDDs. To determine whether any LM captures this relationship, we probe GPT-4 on
the same tasks used with humans and new extensions.Results reveal reliable
metalinguistic skill on the information structure and acceptability tasks,
replicating a striking interaction between the two, despite the zero-shot,
explicit nature of the tasks, and little to no chance of contamination [Studies
1a, 1b]. Study 2 manipulates the information structure of base sentences and
confirms a causal relationship: increasing the prominence of a constituent in a
context sentence increases the subsequent acceptability ratings on an LDD
construction. The findings suggest a tight relationship between natural and
GPT-4 generated English, and between information structure and syntax, which
begs for further exploration.

</details>


### [5] [Atomic Consistency Preference Optimization for Long-Form Question Answering](https://arxiv.org/abs/2505.09039)
*Jingfeng Chen,Raghuveer Thirukovalluru,Junlin Wang,Kaiwei Luo,Bhuwan Dhingra*

Main category: cs.CL

TL;DR: 该论文提出了自监督偏好优化方法ACPO，无需外部监督即可提高LLMs的事实准确性，优于现有监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前减少大模型事实性错误的常见方法依赖于外部模型或知识库，成本高且不可持续，因此需要一种自监督的解决方案。

Method: 通过多随机生成答案中原子一致性信号的对比（事实一致程度），自动筛选高低质量数据用于对齐训练。

Result: 在LongFact和BioGen数据集上，ACPO比监督基线方法FactAlign高1.95分。

Conclusion: ACPO为提升模型事实可靠性提供了高效、可扩展的自监督方案，无需依赖外部资源。

Abstract: Large Language Models (LLMs) frequently produce factoid hallucinations -
plausible yet incorrect answers. A common mitigation strategy is model
alignment, which improves factual accuracy by training on curated factual and
non-factual pairs. However, this approach often relies on a stronger model
(e.g., GPT-4) or an external knowledge base to assess factual correctness,
which may not always be accessible. To address this, we propose Atomic
Consistency Preference Optimization (ACPO), a self-supervised preference-tuning
method that enhances factual accuracy without external supervision. ACPO
leverages atomic consistency signals, i.e., the agreement of individual facts
across multiple stochastic responses, to identify high- and low-quality data
pairs for model alignment. By eliminating the need for costly GPT calls, ACPO
provides a scalable and efficient approach to improving factoid
question-answering. Despite being self-supervised, empirical results
demonstrate that ACPO outperforms FactAlign, a strong supervised alignment
baseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its
effectiveness in enhancing factual reliability without relying on external
models or knowledge bases.

</details>


### [6] [A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias](https://arxiv.org/abs/2505.09056)
*Brandon Smith,Mohamed Reda Bouadjenek,Tahsin Alamgir Kheya,Phillip Dawson,Sunil Aryal*

Main category: cs.CL

TL;DR: 研究分析了12种大型语言模型（LLM）的文本输出相似性、多样性和伦理表现，发现同模型输出相似度高，不同模型风格差异显著，GPT-4多样性突出，部分模型在性别平衡和偏见减少上表现更好。


<details>
  <summary>Details</summary>
Motivation: 探索LLM输出文本的相似性、多样性及伦理表现，以指导未来开发和伦理评估。

Method: 使用5,000个多样化任务提示，生成约300万文本，对比12种LLM（包括开源和专有系统）的输出。

Result: 发现同模型输出相似度高，不同模型风格差异大（如GPT-4多样性显著），词汇和语气差异明显，部分模型在伦理指标上更优。

Conclusion: 结果为LLM行为多样性提供了新见解，有助于未来模型开发和伦理标准制定。

Abstract: Large Language Models (LLMs) represent a major step toward artificial general
intelligence, significantly advancing our ability to interact with technology.
While LLMs perform well on Natural Language Processing tasks -- such as
translation, generation, code writing, and summarization -- questions remain
about their output similarity, variability, and ethical implications. For
instance, how similar are texts generated by the same model? How does this
compare across different models? And which models best uphold ethical
standards? To investigate, we used 5{,}000 prompts spanning diverse tasks like
generation, explanation, and rewriting. This resulted in approximately 3
million texts from 12 LLMs, including proprietary and open-source systems from
OpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs
from the same LLM are more similar to each other than to human-written texts;
(2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4
produces more varied responses; (3) LLM writing styles differ significantly,
with Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for
distinctiveness; (4) differences in vocabulary and tone underscore the
linguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate
greater gender balance and reduced bias. These results offer new insights into
the behavior and diversity of LLM outputs, helping guide future development and
ethical evaluation.

</details>


### [7] [S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment](https://arxiv.org/abs/2505.09068)
*Jennifer Haase,Paul H. P. Hanel,Sebastian Pokutta*

Main category: cs.CL

TL;DR: 该论文提出S-DAT（合成-发散关联任务），一种多语言的创造力评估框架，利用大语言模型和多语言嵌入来量化语义距离，解决了传统创造力评估方法的语言和文化限制。


<details>
  <summary>Details</summary>
Motivation: 传统创造力评估方法依赖人工评分且局限于特定语言，难以扩展和跨文化应用。S-DAT旨在通过技术手段解决这些限制，实现更公平、全球化的创造力研究。

Method: S-DAT结合大语言模型和多语言嵌入技术，计算语义距离作为发散思维（DT）的语言无关指标，并在11种语言中进行验证。

Result: S-DAT表现出与其他DT测量方法的收敛效度，且能正确区分聚合思维，在多语言环境下评分一致性强。

Conclusion: S-DAT为全球范围内更包容、更全面的创造力研究提供了工具，其在线可访问性进一步推动了广泛应用。

Abstract: This paper introduces S-DAT (Synthetic-Divergent Association Task), a
scalable, multilingual framework for automated assessment of divergent thinking
(DT) -a core component of human creativity. Traditional creativity assessments
are often labor-intensive, language-specific, and reliant on subjective human
ratings, limiting their scalability and cross-cultural applicability. In
contrast, S-DAT leverages large language models and advanced multilingual
embeddings to compute semantic distance -- a language-agnostic proxy for DT. We
evaluate S-DAT across eleven diverse languages, including English, Spanish,
German, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating
robust and consistent scoring across linguistic contexts. Unlike prior DAT
approaches, the S-DAT shows convergent validity with other DT measures and
correct discriminant validity with convergent thinking. This cross-linguistic
flexibility allows for more inclusive, global-scale creativity research,
addressing key limitations of earlier approaches. S-DAT provides a powerful
tool for fairer, more comprehensive evaluation of cognitive flexibility in
diverse populations and can be freely assessed online:
https://sdat.iol.zib.de/.

</details>


### [8] [CEC-Zero: Chinese Error Correction Solution Based on LLM](https://arxiv.org/abs/2505.09082)
*Sophie Zhang,Zhiming Lin*

Main category: cs.CL

TL;DR: 论文提出CEC-Zero，一种基于强化学习的框架，使大语言模型能自主学习和修正中文拼写错误，无需外部监督或标注数据。结果表明该方法在准确性和跨领域泛化能力上优于传统模型，适用于实际中文NLP应用。


<details>
  <summary>Details</summary>
Motivation: 传统基于BERT的中文拼写纠错模型在可靠性和泛化能力上存在不足，而现有大语言模型虽表现优异，仍依赖外部监督或辅助模型。本研究旨在提出一种无需外部干预的自修正框架。

Method: 提出CEC-Zero框架，通过强化学习（RL）结合大语言模型的生成能力，使其自主学习和优化纠错策略，摆脱对标注数据或辅助模型的依赖。

Result: 实验证明，经过RL增强的大语言模型在中文拼写纠错任务中达到业界可用准确率，且展现出卓越的跨领域泛化能力。

Conclusion: CEC-Zero为中文NLP应用中的可靠性优化提供了可扩展方案，同时为自改进语言模型建立了新范式，推动大语言模型在实际文本纠错场景中的部署。

Abstract: Recent advancements in large language models (LLMs) demonstrate exceptional
Chinese text processing capabilities, particularly in Chinese Spelling
Correction (CSC). While LLMs outperform traditional BERT-based models in
accuracy and robustness, challenges persist in reliability and generalization.
This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework
enabling LLMs to self-correct through autonomous error strategy learning
without external supervision. By integrating RL with LLMs' generative power,
the method eliminates dependency on annotated data or auxiliary models.
Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and
superior cross-domain generalization, offering a scalable solution for
reliability optimization in Chinese NLP applications. This breakthrough
facilitates LLM deployment in practical Chinese text correction scenarios while
establishing a new paradigm for self-improving language models.

</details>


### [9] [How an unintended Side Effect of a Research Project led to Boosting the Power of UML](https://arxiv.org/abs/2505.09269)
*Ulrich Frank,Pierre Maier*

Main category: cs.CL

TL;DR: 介绍了一款新型UML建模工具，整合类图与对象图并支持对象执行，适用于教学与研究。


<details>
  <summary>Details</summary>
Motivation: 解决传统UML工具的局限性，促进软件架构创新及教学效果提升。

Method: 设计与实现集成类图和对象图的新工具，支持对象执行，并应用于国际研究项目。

Result: 成功开发出多功能UML工具，显著提升建模能力与教学体验。

Conclusion: 该工具是研究副产物的成功案例，展示了跨领域研究的实用价值。

Abstract: This paper describes the design, implementation and use of a new UML modeling
tool that represents a significant advance over conventional tools. Among other
things, it allows the integration of class diagrams and object diagrams as well
as the execution of objects. This not only enables new software architectures
characterized by the integration of software with corresponding object models,
but is also ideal for use in teaching, as it provides students with a
particularly stimulating learning experience. A special feature of the project
is that it has emerged from a long-standing international research project,
which is aimed at a comprehensive multi-level architecture. The project is
therefore an example of how research can lead to valuable results that arise as
a side effect of other work.

</details>


### [10] [A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data](https://arxiv.org/abs/2505.09286)
*Jiin Park,Misuk Kim*

Main category: cs.CL

TL;DR: 提出了一种多语言、可扩展的无监督框架，用于跨领域的方面检测，适用于多语言和多领域评论数据的多标签标注，实验结果显示出高效能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多局限于特定领域和语言，且依赖有监督学习需要大量标注数据，为此提出了无监督的多语言、跨领域解决方案。

Method: 通过聚类提取方面类别候选，使用负采样将每条评论表示为方面感知的嵌入向量，并微调预训练语言模型评估自动生成标签的效果。

Result: 模型表现出高性能，自动生成标签适用于训练，且在一致性和可扩展性上优于公开可用的大型语言模型。人工评估也显示自动标签质量接近手动标注。

Conclusion: 该框架克服了有监督方法的局限性，适用于多语言和多领域环境，未来将探索自动评论摘要和AI代理集成以进一步提升分析效率。

Abstract: Effectively analyzing online review data is essential across industries.
However, many existing studies are limited to specific domains and languages or
depend on supervised learning approaches that require large-scale labeled
datasets. To address these limitations, we propose a multilingual, scalable,
and unsupervised framework for cross-domain aspect detection. This framework is
designed for multi-aspect labeling of multilingual and multi-domain review
data. In this study, we apply automatic labeling to Korean and English review
datasets spanning various domains and assess the quality of the generated
labels through extensive experiments. Aspect category candidates are first
extracted through clustering, and each review is then represented as an
aspect-aware embedding vector using negative sampling. To evaluate the
framework, we conduct multi-aspect labeling and fine-tune several pretrained
language models to measure the effectiveness of the automatically generated
labels. Results show that these models achieve high performance, demonstrating
that the labels are suitable for training. Furthermore, comparisons with
publicly available large language models highlight the framework's superior
consistency and scalability when processing large-scale data. A human
evaluation also confirms that the quality of the automatic labels is comparable
to those created manually. This study demonstrates the potential of a robust
multi-aspect labeling approach that overcomes limitations of supervised methods
and is adaptable to multilingual, multi-domain environments. Future research
will explore automatic review summarization and the integration of artificial
intelligence agents to further improve the efficiency and depth of review
analysis.

</details>


### [11] [Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging](https://arxiv.org/abs/2505.09316)
*Hongjin Qian,Zheng Liu*

Main category: cs.CL

TL;DR: 该论文提出了一种名为InForage的强化学习框架，通过动态信息检索增强大型语言模型（LLMs）的性能，解决了传统静态检索方法的不足。


<details>
  <summary>Details</summary>
Motivation: 传统静态检索方法难以满足复杂任务的需求，尤其是涉及模糊、多步骤或动态信息需求的任务。

Method: 基于信息觅食理论（IFT），提出InForage框架，采用强化学习方法，通过奖励中间检索质量，动态优化信息检索过程。

Result: 实验表明，InForage在通用问答、多跳推理任务和实时Web问答数据集上均优于基线方法。

Conclusion: InForage能有效构建鲁棒、自适应且高效的推理代理，展示了其在动态检索增强方面的优势。

Abstract: Augmenting large language models (LLMs) with external retrieval has become a
standard method to address their inherent knowledge cutoff limitations.
However, traditional retrieval-augmented generation methods employ static,
pre-inference retrieval strategies, making them inadequate for complex tasks
involving ambiguous, multi-step, or evolving information needs. Recent advances
in test-time scaling techniques have demonstrated significant potential in
enabling LLMs to dynamically interact with external tools, motivating the shift
toward adaptive inference-time retrieval. Inspired by Information Foraging
Theory (IFT), we propose InForage, a reinforcement learning framework that
formalizes retrieval-augmented reasoning as a dynamic information-seeking
process. Unlike existing approaches, InForage explicitly rewards intermediate
retrieval quality, encouraging LLMs to iteratively gather and integrate
information through adaptive search behaviors. To facilitate training, we
construct a human-guided dataset capturing iterative search and reasoning
trajectories for complex, real-world web tasks. Extensive evaluations across
general question answering, multi-hop reasoning tasks, and a newly developed
real-time web QA dataset demonstrate InForage's superior performance over
baseline methods. These results highlight InForage's effectiveness in building
robust, adaptive, and efficient reasoning agents.

</details>


### [12] [Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs](https://arxiv.org/abs/2505.09338)
*Jingcheng Niu,Xingdi Yuan,Tong Wang,Hamidreza Saghir,Amir H. Abdi*

Main category: cs.CL

TL;DR: 论文提出了一种新现象“上下文同化”，发现语言模型会倾向于重复提示中出现的无关词汇，并揭示了其机制及缓解方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索语言模型为何容易被输入提示中的无关信息干扰，从而影响输出质量。

Method: 采用统计分析和一种可微分掩码的新方法，识别与上下文同化相关的注意力头（entrainment heads），并通过关闭这些头减弱干扰效果。

Result: 实验证明上下文同化是机制性现象，且其强度受语义因素影响；关闭特定注意力头可显著减少模型对无关信息的依赖。

Conclusion: 论文揭示了语言模型分心的机制，并提出了缓解方法，为模型行为分析提供了新视角。

Abstract: We observe a novel phenomenon, contextual entrainment, across a wide range of
language models (LMs) and prompt settings, providing a new mechanistic
perspective on how LMs become distracted by ``irrelevant'' contextual
information in the input prompt. Specifically, LMs assign significantly higher
logits (or probabilities) to any tokens that have previously appeared in the
context prompt, even for random tokens. This suggests that contextual
entrainment is a mechanistic phenomenon, occurring independently of the
relevance or semantic relation of the tokens to the question or the rest of the
sentence. We find statistically significant evidence that the magnitude of
contextual entrainment is influenced by semantic factors. Counterfactual
prompts have a greater effect compared to factual ones, suggesting that while
contextual entrainment is a mechanistic phenomenon, it is modulated by semantic
factors.
  We hypothesise that there is a circuit of attention heads -- the entrainment
heads -- that corresponds to the contextual entrainment phenomenon. Using a
novel entrainment head discovery method based on differentiable masking, we
identify these heads across various settings. When we ``turn off'' these heads,
i.e., set their outputs to zero, the effect of contextual entrainment is
significantly attenuated, causing the model to generate output that capitulates
to what it would produce if no distracting context were provided. Our discovery
of contextual entrainment, along with our investigation into LM distraction via
the entrainment heads, marks a key step towards the mechanistic analysis and
mitigation of the distraction problem.

</details>


### [13] [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388)
*An Yang,Anfeng Li,Baosong Yang,Beichen Zhang,Binyuan Hui,Bo Zheng,Bowen Yu,Chang Gao,Chengen Huang,Chenxu Lv,Chujie Zheng,Dayiheng Liu,Fan Zhou,Fei Huang,Feng Hu,Hao Ge,Haoran Wei,Huan Lin,Jialong Tang,Jian Yang,Jianhong Tu,Jianwei Zhang,Jianxin Yang,Jiaxi Yang,Jing Zhou,Jingren Zhou,Junyang Lin,Kai Dang,Keqin Bao,Kexin Yang,Le Yu,Lianghao Deng,Mei Li,Mingfeng Xue,Mingze Li,Pei Zhang,Peng Wang,Qin Zhu,Rui Men,Ruize Gao,Shixuan Liu,Shuang Luo,Tianhao Li,Tianyi Tang,Wenbiao Yin,Xingzhang Ren,Xinyu Wang,Xinyu Zhang,Xuancheng Ren,Yang Fan,Yang Su,Yichang Zhang,Yinger Zhang,Yu Wan,Yuqiong Liu,Zekun Wang,Zeyu Cui,Zhenru Zhang,Zhipeng Zhou,Zihan Qiu*

Main category: cs.CL

TL;DR: Qwen3是一种大型语言模型系列，支持多语言和高效计算，整合思考与非思考模式，并引入资源分配机制，显著提升性能和多语言支持。


<details>
  <summary>Details</summary>
Motivation: 提升语言模型的性能、效率和多语言能力，同时优化资源分配和动态任务处理。

Method: 采用密集和混合专家架构（MoE），引入思维预算机制和动态模式切换，减少小型模型的计算资源需求。

Result: Qwen3在多个基准测试中达到领先水平，支持119种语言，性能优于前代模型。

Conclusion: Qwen3通过创新设计和资源优化，显著提升了多语言任务的处理能力，并开源以支持社区研究。

Abstract: In this work, we present Qwen3, the latest version of the Qwen model family.
Qwen3 comprises a series of large language models (LLMs) designed to advance
performance, efficiency, and multilingual capabilities. The Qwen3 series
includes models of both dense and Mixture-of-Expert (MoE) architectures, with
parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is
the integration of thinking mode (for complex, multi-step reasoning) and
non-thinking mode (for rapid, context-driven responses) into a unified
framework. This eliminates the need to switch between different models--such as
chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,
QwQ-32B)--and enables dynamic mode switching based on user queries or chat
templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing
users to allocate computational resources adaptively during inference, thereby
balancing latency and performance based on task complexity. Moreover, by
leveraging the knowledge from the flagship models, we significantly reduce the
computational resources required to build smaller-scale models, while ensuring
their highly competitive performance. Empirical evaluations demonstrate that
Qwen3 achieves state-of-the-art results across diverse benchmarks, including
tasks in code generation, mathematical reasoning, agent tasks, etc.,
competitive against larger MoE models and proprietary models. Compared to its
predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119
languages and dialects, enhancing global accessibility through improved
cross-lingual understanding and generation capabilities. To facilitate
reproducibility and community-driven research and development, all Qwen3 models
are publicly accessible under Apache 2.0.

</details>


### [14] [Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits](https://arxiv.org/abs/2505.09407)
*Subrit Dikshit,Ritu Tiwari,Priyank Jain*

Main category: cs.CL

TL;DR: QEDACVC是一种基于量子计算的编码器-解码器架构，用于多语言机器翻译，准确率达到82%，对比传统云计算翻译服务如Google Translate等。


<details>
  <summary>Details</summary>
Motivation: 研究量子计算在多语言机器翻译中的潜力，提供一种替代传统云计算的方法。

Method: 提出QEDACVC架构，结合量子卷积、量子池化、量子变分电路和量子注意力机制，模拟并运行于量子计算硬件上。

Result: 在OPUS数据集（英语、法语、德语、印地语）上训练后，QEDACVC的翻译准确率为82%。

Conclusion: QEDACVC展示了量子计算在多语言翻译任务中的可行性，为未来量子计算在自然语言处理中的应用提供了新方向。

Abstract: Cloud-based multilingual translation services like Google Translate and
Microsoft Translator achieve state-of-the-art translation capabilities. These
services inherently use large multilingual language models such as GRU, LSTM,
BERT, GPT, T5, or similar encoder-decoder architectures with attention
mechanisms as the backbone. Also, new age natural language systems, for
instance ChatGPT and DeepSeek, have established huge potential in multiple
tasks in natural language processing. At the same time, they also possess
outstanding multilingual translation capabilities. However, these models use
the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder
Attention-based Convolutional Variational Circuits) is an alternate solution
that explores the quantum computing realm instead of the classical computing
realm to study and demonstrate multilingual machine translation. QEDACVC
introduces the quantum encoder-decoder architecture that simulates and runs on
quantum computing hardware via quantum convolution, quantum pooling, quantum
variational circuit, and quantum attention as software alterations. QEDACVC
achieves an Accuracy of 82% when trained on the OPUS dataset for English,
French, German, and Hindi corpora for multilingual translations.

</details>


### [15] [PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning](https://arxiv.org/abs/2505.09519)
*Zongqian Li,Yixuan Su,Nigel Collier*

Main category: cs.CL

TL;DR: 提出PT-MoE，结合矩阵分解与MoE路由，在QA和数学任务中实现SOTA，参数效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法存在矛盾现象（如路由提升效率但性能不均），激发结合矩阵分解与MoE以互补优势。

Method: 整合矩阵分解与MoE路由，通过分解实现参数共享，MoE动态适配，形成PT-MoE框架。

Result: 在17个数据集上，QA任务F1提升1.49分，数学任务准确率提升10.75分，参数减少25%。

Conclusion: PT-MoE展示跨任务一致性，矩阵分解与MoE协同增效，为未来PEFT方法提供设计启示。

Abstract: Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting
large language models, yet existing approaches exhibit counter-intuitive
phenomena: integrating router into prompt tuning (PT) increases training
efficiency yet does not improve performance universally; parameter reduction
through matrix decomposition can improve performance in specific domains.
Motivated by these observations and the modular nature of PT, we propose
PT-MoE, a novel framework that integrates matrix decomposition with
mixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets
demonstrate that PT-MoE achieves state-of-the-art performance in both question
answering (QA) and mathematical problem solving tasks, improving F1 score by
1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing
mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all
while using 25% fewer parameters than LoRA. Our analysis reveals that while PT
methods generally excel in QA tasks and LoRA-based methods in math datasets,
the integration of matrix decomposition and MoE in PT-MoE yields complementary
benefits: decomposition enables efficient parameter sharing across experts
while MoE provides dynamic adaptation, collectively enabling PT-MoE to
demonstrate cross-task consistency and generalization abilities. These
findings, along with ablation studies on routing mechanisms and architectural
components, provide insights for future PEFT methods.

</details>


### [16] [WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models](https://arxiv.org/abs/2505.09595)
*Abdullah Mushtaq,Imran Taj,Rafay Naeem,Ibrahim Ghaznavi,Junaid Qadir*

Main category: cs.CL

TL;DR: 论文提出WorldView-Bench基准，评估LLM的全球文化包容性（GCI），通过多视角模型减少西方中心偏见，提升文化多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM训练和评估方法强化西方中心主义，导致文化同质化，缺乏全球文明多样性。现有基准框架无法充分捕捉这种偏见。

Method: 引入WorldView-Bench基准，基于多视角世界观理论，通过自由生成评估文化极化。采用两种干预策略：上下文实现多视角LLM和多代理系统实现多视角LLM。

Result: 多代理系统实现的LLM将视角分布熵从基线13%提升至94%，正面情感比例达67.7%，文化平衡性显著改善。

Conclusion: 多视角感知的AI评估能有效减少LLM的文化偏见，推动更包容和伦理对齐的AI系统发展。

Abstract: Large Language Models (LLMs) are predominantly trained and aligned in ways
that reinforce Western-centric epistemologies and socio-cultural norms, leading
to cultural homogenization and limiting their ability to reflect global
civilizational plurality. Existing benchmarking frameworks fail to adequately
capture this bias, as they rely on rigid, closed-form assessments that overlook
the complexity of cultural inclusivity. To address this, we introduce
WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity
(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our
approach is grounded in the Multiplex Worldview proposed by Senturk et al.,
which distinguishes between Uniplex models, reinforcing cultural
homogenization, and Multiplex models, which integrate diverse perspectives.
WorldView-Bench measures Cultural Polarization, the exclusion of alternative
perspectives, through free-form generative evaluation rather than conventional
categorical benchmarks. We implement applied multiplexity through two
intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where
system prompts embed multiplexity principles, and (2) Multi-Agent System
(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing
distinct cultural perspectives collaboratively generate responses. Our results
demonstrate a significant increase in Perspectives Distribution Score (PDS)
entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,
alongside a shift toward positive sentiment (67.7%) and enhanced cultural
balance. These findings highlight the potential of multiplex-aware AI
evaluation in mitigating cultural bias in LLMs, paving the way for more
inclusive and ethically aligned AI systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [A Preliminary Framework for Intersectionality in ML Pipelines](https://arxiv.org/abs/2505.08792)
*Michelle Nashla Turcios,Alicia E. Boyd,Angela D. R. Smith,Brittany Johnson*

Main category: cs.LG

TL;DR: 摘要讨论了机器学习（ML）在社会身份支持上的不足，提出利用交叉性社会学框架来提升技术的公平性。


<details>
  <summary>Details</summary>
Motivation: 动机在于机器学习技术缺乏对社会身份多样性的充分支持，需要借鉴交叉性理论以实现更公平的技术结果。

Method: 方法是通过Crenshaw、Combahee和Collins的交叉性理论框架，评估机器学习文献中对交叉性应用的（错误）对齐情况。

Result: 结果揭示了当前机器学习文献中交叉性应用的偏差或不足。

Conclusion: 结论是对交叉性理论的正确应用有助于开发更公平的机器学习解决方案。

Abstract: Machine learning (ML) has become a go-to solution for improving how we use,
experience, and interact with technology (and the world around us).
Unfortunately, studies have repeatedly shown that machine learning technologies
may not provide adequate support for societal identities and experiences.
Intersectionality is a sociological framework that provides a mechanism for
explicitly considering complex social identities, focusing on social justice
and power. While the framework of intersectionality can support the development
of technologies that acknowledge and support all members of society, it has
been adopted and adapted in ways that are not always true to its foundations,
thereby weakening its potential for impact. To support the appropriate adoption
and use of intersectionality for more equitable technological outcomes, we
amplify the foundational intersectionality scholarship--Crenshaw, Combahee, and
Collins (three C's), to create a socially relevant preliminary framework in
developing machine-learning solutions. We use this framework to evaluate and
report on the (mis)alignments of intersectionality application in machine
learning literature.

</details>


### [18] [Onboard Optimization and Learning: A Survey](https://arxiv.org/abs/2505.08793)
*Monirul Islam Pavel,Siyi Hu,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: 综述探讨了边缘AI中的板载学习，挑战包括计算资源有限、推理成本高和安全性问题，提出了优化模型效率、加速推理和分布式协作的方法。


<details>
  <summary>Details</summary>
Motivation: 边缘AI的板载学习对低延迟、隐私保护和能效要求高的应用至关重要，但面临资源受限和安全挑战。

Method: 综述了模型复杂性降低、推理加速、隐私保护计算等技术，并研究了硬件软件协同设计、模型压缩和去中心化学习的进展。

Result: 通过多种方法优化了板载学习，提升了在动态环境中的可扩展性和适应性。

Conclusion: 本文为边缘AI的板载学习提供了当前进展的全面视角，支持其更高效、安全的部署。

Abstract: Onboard learning is a transformative approach in edge AI, enabling real-time
data processing, decision-making, and adaptive model training directly on
resource-constrained devices without relying on centralized servers. This
paradigm is crucial for applications demanding low latency, enhanced privacy,
and energy efficiency. However, onboard learning faces challenges such as
limited computational resources, high inference costs, and security
vulnerabilities. This survey explores a comprehensive range of methodologies
that address these challenges, focusing on techniques that optimize model
efficiency, accelerate inference, and support collaborative learning across
distributed devices. Approaches for reducing model complexity, improving
inference speed, and ensuring privacy-preserving computation are examined
alongside emerging strategies that enhance scalability and adaptability in
dynamic environments. By bridging advancements in hardware-software co-design,
model compression, and decentralized learning, this survey provides insights
into the current state of onboard learning to enable robust, efficient, and
secure AI deployment at the edge.

</details>


### [19] [The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures](https://arxiv.org/abs/2505.08795)
*Andres Anabalon,Hugo Garces,Julio Oliva,Jose Cifuentes*

Main category: cs.LG

TL;DR: 这篇论文提出了一种在三维闵可夫斯基时空中快速嵌入层次结构的方法，通过局部层次信号（定向令牌对）实现，无需全局符号结构。应用在WordNet上，成功嵌入哺乳动物子树及最大无歧义子集，结果表明所有离散数据都存在完美的三维几何表示。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索层次结构是否可以通过几何方式（特别是三维闵可夫斯基时空）完美表示，从而揭示概念和类别之间的层次关系的几何本质。

Method: 使用定向令牌对作为局部层次信号，无需全局符号结构。通过因果结构编码数据相关性，并提出一种基于因果性而非距离的新型检索机制。

Result: 成功实现了哺乳动物子树和WordNet最大无歧义子集的完美嵌入，证实离散数据存在三维几何表示。嵌入结果近乎共形不变，暗示与广义相对论和场论的深层联系。

Conclusion: 研究表明，概念、类别及其相互关系（即层次意义本身）是几何的，可通过三维闵可夫斯基时空中的因果结构完美表示。

Abstract: We show that there is a fast algorithm that embeds hierarchical structures in
three-dimensional Minkowski spacetime. The correlation of data ends up purely
encoded in the causal structure. Our model relies solely on oriented token
pairs -- local hierarchical signals -- with no access to global symbolic
structure. We apply our method to the corpus of \textit{WordNet}. We provide a
perfect embedding of the mammal sub-tree including ambiguities (more than one
hierarchy per node) in such a way that the hierarchical structures get
completely codified in the geometry and exactly reproduce the ground-truth. We
extend this to a perfect embedding of the maximal unambiguous subset of the
\textit{WordNet} with 82{,}115 noun tokens and a single hierarchy per token. We
introduce a novel retrieval mechanism in which causality, not distance, governs
hierarchical access. Our results seem to indicate that all discrete data has a
perfect geometrical representation that is three-dimensional. The resulting
embeddings are nearly conformally invariant, indicating deep connections with
general relativity and field theory. These results suggest that concepts,
categories, and their interrelations, namely hierarchical meaning itself, is
geometric.

</details>


### [20] [Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models](https://arxiv.org/abs/2505.08803)
*Zizhao Hu,Mohammad Rostami,Jesse Thomason*

Main category: cs.LG

TL;DR: 本文研究了多模态视觉-语言生成系统中模型崩溃的现象，扩展了之前单模态模型的研究，并提出了缓解模型崩溃的实用方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解多模态AI系统在持续训练自生成数据时的模型崩溃现象，及其在实际场景中的表现。

Method: 方法包括在多模态视觉-语言模型（VLMs）和文本到图像扩散模型中应用递归生成-训练循环，并测试不同缓解策略。

Result: 研究发现多模态模型崩溃表现出独特特征，如改进的视觉-语言对齐和VLM任务中方差增加，同时提出了有效缓解方法。

Conclusion: 研究为多模态自改进AI系统的稳健性提供了初步见解和实用指南。

Abstract: Recent research has highlighted the risk of generative model collapse, where
performance progressively degrades when continually trained on self-generated
data. However, existing exploration on model collapse is limited to single,
unimodal models, limiting our understanding in more realistic scenarios, such
as diverse multi-modal AI agents interacting autonomously through synthetic
data and continually evolving. We expand the synthetic data training and model
collapse study to multi-modal vision-language generative systems, such as
vision-language models (VLMs) and text-to-image diffusion models, as well as
recursive generate-train loops with multiple models. We find that model
collapse, previously observed in single-modality generative models, exhibits
distinct characteristics in the multi-modal context, such as improved
vision-language alignment and increased variance in VLM image-captioning task.
Additionally, we find that general approaches such as increased decoding
budgets, greater model diversity, and relabeling with frozen models can
effectively mitigate model collapse. Our findings provide initial insights and
practical guidelines for reducing the risk of model collapse in self-improving
multi-agent AI systems and curating robust multi-modal synthetic datasets.

</details>


### [21] [An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits](https://arxiv.org/abs/2505.08823)
*Cody Steinmetz,Gavin Childress,Aaron Herbst,Gavin Jones,Jasdeep Singh,Eli Vang,Keagan Weinstock*

Main category: cs.LG

TL;DR: 该论文提出了一种通过归一化和渐层量化方法，将大型语言模型（LLMs）稳定量化为2位精度的方法，无需额外训练或增加模型复杂度，即可保持高精度。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自然语言处理中表现出色，但其庞大的规模导致实际部署成本高昂。量化技术虽能降低内存和计算需求，但传统方法常导致精度下降或需额外训练。如何在超低比特（如2位）下保持模型性能成为关键问题。

Method: 论文提出了一种简单但有效的方法：在每次线性投影前插入RMS归一化操作，并采用渐层、分层的量化调度。这种方法能够稳定地将全精度模型微调为2位LLMs，无需复杂的知识蒸馏流程。

Result: 实验结果表明，该方法在标准语言建模基准测试中达到或优于更复杂的知识蒸馏方案，同时未增加模型复杂度。RMS归一化和渐层量化显著缩小了2位与全精度模型之间的精度差距。

Conclusion: 通过归一化和渐层量化，论文证明了超低比特（2位）推理的可行性，为低成本高效部署LLMs提供了实用方案。

Abstract: Large language models (LLMs) have transformed natural-language processing,
yet their scale makes real-world deployment costly. Post-training quantization
reduces memory and computation but often degrades accuracy, while
quantization-aware training can recover performance at the cost of extra
training. Pushing quantization to the ternary (2-bit) regime yields even larger
savings but is notoriously unstable. Building on recent work showing that a
bias-free, RMS-normalized Transformer with straight-through estimation can
reach 1.58-bit precision, we demonstrate that simply inserting RMS
normalization before every linear projection and applying a gradual, layer-wise
quantization schedule stably fine-tunes full-precision checkpoints into ternary
LLMs. Our approach matches or surpasses more elaborate knowledge-distillation
pipelines on standard language-modeling benchmarks without adding model
complexity. These results indicate that careful normalization alone can close
much of the accuracy gap between ternary and full-precision LLMs, making
ultra-low-bit inference practical.

</details>


### [22] [Self Rewarding Self Improving](https://arxiv.org/abs/2505.08827)
*Toby Simonds,Kevin Lopez,Akira Yoshiyama,Dominique Garmier*

Main category: cs.LG

TL;DR: 本文证明大型语言模型能通过自我评判实现自我提升，无需参考解决方案，利用生成与验证之间的不对称性。在Countdown谜题和MIT积分赛问题上的实验表明，模型能提供可靠的奖励信号，实现强化学习。结合合成问题生成，模型在自我评判中性能提升8%，超越GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 解决传统强化学习中对程序化奖励依赖的问题，探索模型自我评判作为奖励信号的可行性，推动在数据稀缺或评估复杂的领域中实现自我持续学习。

Method: 提出自我评判框架，模型生成问题、解答并自我评分，结合合成问题生成和强化学习实现性能提升。

Result: 在Qwen 2.5 7B模型上性能提升8%，在积分任务上超越GPT-4o，验证了自我评判作为奖励信号的有效性。

Conclusion: 自我评判为强化学习提供了新范式，有望在数据稀缺领域推动AI系统的自主持续学习。

Abstract: We demonstrate that large language models can effectively self-improve
through self-judging without requiring reference solutions, leveraging the
inherent asymmetry between generating and verifying solutions. Our experiments
on Countdown puzzles and MIT Integration Bee problems show that models can
provide reliable reward signals without ground truth answers, enabling
reinforcement learning in domains previously not possible. By implementing
self-judging, we achieve significant performance gains maintaining alignment
with formal verification. When combined with synthetic question generation, we
establish a complete self-improvement loop where models generate practice
problems, solve them, and evaluate their own performance-achieving an 8%
improvement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on
integration tasks. Our findings demonstrate that LLM judges can provide
effective reward signals for training models, unlocking many reinforcement
learning environments previously limited by the difficulty of creating
programmatic rewards. This suggests a potential paradigm shift toward AI
systems that continuously improve through self-directed learning rather than
human-guided training, potentially accelerating progress in domains with scarce
training data or complex evaluation requirements.

</details>


### [23] [Aggregating Concepts of Fairness and Accuracy in Predictive Systems](https://arxiv.org/abs/2505.08829)
*David Kinney*

Main category: cs.LG

TL;DR: 论文探讨了预测算法在准确性和公平性之间的权衡问题，提出了一种线性组合来衡量兼顾两者的整体价值。


<details>
  <summary>Details</summary>
Motivation: 随着AI预测算法的普及，如何在保证预测准确性的同时避免系统性偏见或伤害特定群体成为重要问题。

Method: 采用Harsanyi的偏好聚合理论，提出线性组合方法评估算法的综合价值。

Result: 使用COMPAS数据集验证了该方法在处理准确性与公平性权衡时的有效性。

Conclusion: 线性组合为衡量预测算法的综合价值提供了合理的解决方案。

Abstract: An algorithm that outputs predictions about the state of the world will
almost always be designed with the implicit or explicit goal of outputting
accurate predictions (i.e., predictions that are likely to be true). In
addition, the rise of increasingly powerful predictive algorithms brought about
by the recent revolution in artificial intelligence has led to an emphasis on
building predictive algorithms that are fair, in the sense that their
predictions do not systematically evince bias or bring about harm to certain
individuals or groups. This state of affairs presents two conceptual
challenges. First, the goals of accuracy and fairness can sometimes be in
tension, and there are no obvious normative guidelines for managing the
trade-offs between these two desiderata when they arise. Second, there are many
distinct ways of measuring both the accuracy and fairness of a predictive
algorithm; here too, there are no obvious guidelines on how to aggregate our
preferences for predictive algorithms that satisfy disparate measures of
fairness and accuracy to various extents. The goal of this paper is to address
these challenges by arguing that there are good reasons for using a linear
combination of accuracy and fairness metrics to measure the
all-things-considered value of a predictive algorithm for agents who care about
both accuracy and fairness. My argument depends crucially on a classic result
in the preference aggregation literature due to Harsanyi. After making this
formal argument, I apply my result to an analysis of accuracy-fairness
trade-offs using the COMPAS dataset compiled by Angwin et al.

</details>


### [24] [Evaluating Simplification Algorithms for Interpretability of Time Series Classification](https://arxiv.org/abs/2505.08846)
*Felix Marti-Perez,Brigt Håvardstun,Cèsar Ferri,Carlos Monserrat,Jan Arne Telle*

Main category: cs.LG

TL;DR: 提出了评估简化时间序列用于时间序列分类器（TSC）可解释性的指标，并通过实验验证其效果。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据相比文本和图像数据对人类不直观，因此需要简化以提高可解释性。

Method: 设计了与简化复杂度和忠诚度相关的指标，并评估了四种简化算法在不同TSC算法和数据集上的表现。

Result: 实验表明，使用简化时间序列比原始数据更适合TSC的可解释性，尤其在季节性、非平稳或低熵数据上效果更佳。

Conclusion: 简化时间序列显著提升了TSC的可解释性，尤其在特定类型数据上表现突出。

Abstract: In this work, we introduce metrics to evaluate the use of simplified time
series in the context of interpretability of a TSC - a Time Series Classifier.
Such simplifications are important because time series data, in contrast to
text and image data, are not intuitively understandable to humans. These
metrics are related to the complexity of the simplifications - how many
segments they contain - and to their loyalty - how likely they are to maintain
the classification of the original time series. We employ these metrics to
evaluate four distinct simplification algorithms, across several TSC algorithms
and across datasets of varying characteristics, from seasonal or stationary to
short or long. Our findings suggest that using simplifications for
interpretability of TSC is much better than using the original time series,
particularly when the time series are seasonal, non-stationary and/or with low
entropy.

</details>


### [25] [An Analytical Characterization of Sloppiness in Neural Networks: Insights from Linear Models](https://arxiv.org/abs/2505.08915)
*Jialin Mao,Itay Griniasty,Yan Sun,Mark K. Transtrum,James P. Sethna,Pratik Chaudhari*

Main category: cs.LG

TL;DR: 论文摘要研究深度神经网络训练轨迹在概率分布空间中的低维'超带状'流形，并通过线性网络分析这一现象，探讨影响因素及相位边界。


<details>
  <summary>Details</summary>
Motivation: 受深度网络与线性网络训练轨迹相似性的启发，研究旨在从理论上解释低维流形现象，以理解网络训练的共性规律。

Method: 利用动态系统理论工具，分析输入相关矩阵特征值衰减率、权重初始尺度与训练步数对低维流形几何的影响，并扩展至核机器与随机梯度下降的线性模型。

Result: 明确了超带状流形存在的相位边界条件，揭示了输入数据、初始权重及训练步骤的关键作用，并推广到更广泛的学习模型。

Conclusion: 研究为理解神经网络训练中的低维流形现象提供了理论框架，有助于优化算法设计。

Abstract: Recent experiments have shown that training trajectories of multiple deep
neural networks with different architectures, optimization algorithms,
hyper-parameter settings, and regularization methods evolve on a remarkably
low-dimensional "hyper-ribbon-like" manifold in the space of probability
distributions. Inspired by the similarities in the training trajectories of
deep networks and linear networks, we analytically characterize this phenomenon
for the latter. We show, using tools in dynamical systems theory, that the
geometry of this low-dimensional manifold is controlled by (i) the decay rate
of the eigenvalues of the input correlation matrix of the training data, (ii)
the relative scale of the ground-truth output to the weights at the beginning
of training, and (iii) the number of steps of gradient descent. By analytically
computing and bounding the contributions of these quantities, we characterize
phase boundaries of the region where hyper-ribbons are to be expected. We also
extend our analysis to kernel machines and linear models that are trained with
stochastic gradient descent.

</details>


### [26] [NeurIPS 2024 Ariel Data Challenge: Characterisation of Exoplanetary Atmospheres Using a Data-Centric Approach](https://arxiv.org/abs/2505.08940)
*Jeremie Blanchard,Lisa Casino,Jordan Gierschendorf*

Main category: cs.LG

TL;DR: 分析系外行星大气光谱的机器学习方法，强调广义化而非竞赛优化，实验表明不确定性估计显著影响性能，但表格建模和特征工程存在局限性。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习技术从模拟光谱数据中提取大气成分，关注广义化而非竞赛优化，以解决天体物理数据中的实际问题。

Method: 采用数据为中心的方法，进行特征提取、信号转换和异方差不确定性建模等多轴实验。

Result: 不确定性估计提升GLL分数11%，但揭示了表格建模和特征工程的局限性，及商业化方法在Kaggle竞赛框架内的约束。

Conclusion: 天文数据分析需权衡模型简洁性、可解释性与广义化，机器学习方法虽有潜力但仍受现有技术限制。

Abstract: The characterization of exoplanetary atmospheres through spectral analysis is
a complex challenge. The NeurIPS 2024 Ariel Data Challenge, in collaboration
with the European Space Agency's (ESA) Ariel mission, provided an opportunity
to explore machine learning techniques for extracting atmospheric compositions
from simulated spectral data. In this work, we focus on a data-centric business
approach, prioritizing generalization over competition-specific optimization.
We briefly outline multiple experimental axes, including feature extraction,
signal transformation, and heteroskedastic uncertainty modeling. Our
experiments demonstrate that uncertainty estimation plays a crucial role in the
Gaussian Log-Likelihood (GLL) score, impacting performance by several
percentage points. Despite improving the GLL score by 11%, our results
highlight the inherent limitations of tabular modeling and feature engineering
for this task, as well as the constraints of a business-driven approach within
a Kaggle-style competition framework. Our findings emphasize the trade-offs
between model simplicity, interpretability, and generalization in astrophysical
data analysis.

</details>


### [27] [ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers](https://arxiv.org/abs/2505.08941)
*Gavin Hull,Alex Bihlo*

Main category: cs.LG

TL;DR: ForeCite利用预训练因果语言模型和线性头来预测论文的月度平均引用率，测试相关性达ρ=0.826，比之前最佳方法提升27个百分点。


<details>
  <summary>Details</summary>
Motivation: 预测论文未来引用率对自动化研究评估和加速科学进步至关重要。

Method: 采用预训练因果语言模型，加入线性头进行回归任务，优化了变换器的适应性。

Result: 在90万+生物医学论文数据集上表现优异，模型规模和数据量扩展实验显示一致增益。

Conclusion: ForeCite在预测学术研究长期影响力方面达到新高度，为自动化科学贡献评估奠定基础。

Abstract: Predicting the future citation rates of academic papers is an important step
toward the automation of research evaluation and the acceleration of scientific
progress. We present $\textbf{ForeCite}$, a simple but powerful framework to
append pre-trained causal language models with a linear head for average
monthly citation rate prediction. Adapting transformers for regression tasks,
ForeCite achieves a test correlation of $\rho = 0.826$ on a curated dataset of
900K+ biomedical papers published between 2000 and 2024, a 27-point improvement
over the previous state-of-the-art. Comprehensive scaling-law analysis reveals
consistent gains across model sizes and data volumes, while temporal holdout
experiments confirm practical robustness. Gradient-based saliency heatmaps
suggest a potentially undue reliance on titles and abstract texts. These
results establish a new state-of-the-art in forecasting the long-term influence
of academic research and lay the groundwork for the automated, high-fidelity
evaluation of scientific contributions.

</details>


### [28] [GPML: Graph Processing for Machine Learning](https://arxiv.org/abs/2505.08964)
*Majed Jaber,Julien Michel,Nicolas Boutry,Pierre Parrend*

Main category: cs.LG

TL;DR: GPML库通过将原始网络流量数据转化为图表示，提供高级网络行为洞察，支持动态网络中异常和社区变化的检测，增强实时检测和历史取证分析能力。


<details>
  <summary>Details</summary>
Motivation: 应对动态网络中复杂、多步骤且快速演变的攻击，需要先进的网络威胁检测工具。

Method: 利用图处理技术（GPML库）将网络流量转化为图表示，并提取社区和谱度量以检测异常。

Result: GPML库能有效支持实时威胁检测和历史取证分析，提升网络安全防护能力。

Conclusion: GPML库通过图处理方法为现代网络安全挑战提供了强大的解决方案。

Abstract: The dramatic increase of complex, multi-step, and rapidly evolving attacks in
dynamic networks involves advanced cyber-threat detectors. The GPML (Graph
Processing for Machine Learning) library addresses this need by transforming
raw network traffic traces into graph representations, enabling advanced
insights into network behaviors. The library provides tools to detect anomalies
in interaction and community shifts in dynamic networks. GPML supports
community and spectral metrics extraction, enhancing both real-time detection
and historical forensics analysis. This library supports modern cybersecurity
challenges with a robust, graph-based approach.

</details>


### [29] [SaFARi: State-Space Models for Frame-Agnostic Representation](https://arxiv.org/abs/2505.08977)
*Hossein Babaei,Mel White,Sina Alemohammad,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 该论文提出了一种名为SaFARi的通用方法，用于构建基于任意框架或基函数的State-Space Models（SSMs），突破了此前仅限于多项式基的限制，扩展了SSM的应用多样性。


<details>
  <summary>Details</summary>
Motivation: 当前SSMs在长程依赖数据处理中表现出色，但其实现通常局限于少数多项式基函数。为了突破这一限制，研究者希望开发一种更通用的方法，支持任意框架或基函数，以丰富SSM的潜力。

Method: 论文提出了一种名为SaFARi的框架，允许使用任意框架或基函数构建SSM，不再局限于多项式。这一方法不仅包含HiPPO等现有技术，还支持无限多样的SSM变体。

Result: SaFARi方法成功实现了SSM的框架无关性，提供了更广泛的基函数选择，从而增强了模型在长程依赖数据上的表现潜力。

Conclusion: 该研究通过SaFARi框架显著扩展了SSM的适用性，为未来在多样场景下的应用奠定了理论基础。

Abstract: State-Space Models (SSMs) have re-emerged as a powerful tool for online
function approximation, and as the backbone of machine learning models for
long-range dependent data. However, to date, only a few polynomial bases have
been explored for this purpose, and the state-of-the-art implementations were
built upon the best of a few limited options. In this paper, we present a
generalized method for building an SSM with any frame or basis, rather than
being restricted to polynomials. This framework encompasses the approach known
as HiPPO, but also permits an infinite diversity of other possible "species"
within the SSM architecture. We dub this approach SaFARi: SSMs for
Frame-Agnostic Representation.

</details>


### [30] [Model-free Online Learning for the Kalman Filter: Forgetting Factor and Logarithmic Regret](https://arxiv.org/abs/2505.08982)
*Jiachen Qian,Yang Zheng*

Main category: cs.LG

TL;DR: 本文提出了一种基于指数遗忘的在线预测方法，用于处理未知线性随机系统的预测问题，通过平衡回归模型改善了预测精度，并提供了更优的对数遗憾界。


<details>
  <summary>Details</summary>
Motivation: 针对未知线性随机系统的在线预测问题，传统递归最小二乘法及其变种可能因回归模型高度不平衡而导致性能下降和过拟合。本文旨在通过引入指数遗忘的归纳偏置来解决这一问题。

Method: 采用指数遗忘技术来平衡回归模型，而不是简单地重新加权数据。这种方法在回归误差和正则化误差之间取得了更好的平衡，同时减少了累计误差。

Result: 通过新的证明技术，本文提出的方法实现了更优的对数遗憾界 $O(\log^3 N)$，其中 $N$ 是观测数量。

Conclusion: 本文提出的方法通过平衡回归模型和引入指数遗忘，有效提升了未知线性随机系统在线预测的准确性和稳定性，并提供了理论保证。

Abstract: We consider the problem of online prediction for an unknown, non-explosive
linear stochastic system. With a known system model, the optimal predictor is
the celebrated Kalman filter. In the case of unknown systems, existing
approaches based on recursive least squares and its variants may suffer from
degraded performance due to the highly imbalanced nature of the regression
model. This imbalance can easily lead to overfitting and thus degrade
prediction accuracy. We tackle this problem by injecting an inductive bias into
the regression model via {exponential forgetting}. While exponential forgetting
is a common wisdom in online learning, it is typically used for re-weighting
data. In contrast, our approach focuses on balancing the regression model. This
achieves a better trade-off between {regression} and {regularization errors},
and simultaneously reduces the {accumulation error}. With new proof techniques,
we also provide a sharper logarithmic regret bound of $O(\log^3 N)$, where $N$
is the number of observations.

</details>


### [31] [Continual Reinforcement Learning via Autoencoder-Driven Task and New Environment Recognition](https://arxiv.org/abs/2505.09003)
*Zeki Doruk Erden,Donia Gasmi,Boi Faltings*

Main category: cs.LG

TL;DR: 论文分析了强化学习智能体的持续学习挑战，提出了一种结合自动编码器和策略优化的端到端系统，能识别新任务并无外部信号下保留旧知识。初步结果显示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决强化学习智能体在持续学习中如何无需外部信号就能保留和利用现有信息的挑战。

Method: 方法是将策略优化与熟悉度自动编码器结合，构建端到端的持续学习系统，以识别新任务并匹配已知环境。

Result: 初步结果显示，系统能成功实现无需外部信号的持续学习，包括新任务识别和已知环境知识的重用。

Conclusion: 结论是该方法在持续学习中有潜力，尤其是在无外部信号指示任务变化时表现良好。

Abstract: Continual learning for reinforcement learning agents remains a significant
challenge, particularly in preserving and leveraging existing information
without an external signal to indicate changes in tasks or environments. In
this study, we explore the effectiveness of autoencoders in detecting new tasks
and matching observed environments to previously encountered ones. Our approach
integrates policy optimization with familiarity autoencoders within an
end-to-end continual learning system. This system can recognize and learn new
tasks or environments while preserving knowledge from earlier experiences and
can selectively retrieve relevant knowledge when re-encountering a known
environment. Initial results demonstrate successful continual learning without
external signals to indicate task changes or reencounters, showing promise for
this methodology.

</details>


### [32] [Signal-based AI-driven software solution for automated quantification of metastatic bone disease and treatment response assessment using Whole-Body Diffusion-Weighted MRI (WB-DWI) biomarkers in Advanced Prostate Cancer](https://arxiv.org/abs/2505.09011)
*Antonio Candito,Matthew D Blackledge,Richard Holbrey,Nuria Porta,Ana Ribeiro,Fabio Zugni,Luca D'Erme,Francesca Castagnoli,Alina Dragan,Ricardo Donners,Christina Messiou,Nina Tunariu,Dow-Mu Koh*

Main category: cs.LG

TL;DR: 开发了一种AI驱动的软件，通过WB-DWI扫描量化转移性骨病，结合弱监督Residual U-Net模型和浅层CNN，实现了高重复性和准确性的病灶量化，为临床决策提供支持。


<details>
  <summary>Details</summary>
Motivation: 转移性骨病的精准量化对临床决策至关重要，现有方法在效率和准确性上存在不足，需要一种自动化、可重复的解决方案。

Method: 结合弱监督Residual U-Net模型生成骨骼概率图，统计框架标准化WB-DWI信号，浅层CNN生成病灶掩膜，并提取TDV和gADC统计数据。

Result: 软件与专家标注的Dice分数为0.6，重复性分析显示log-TDV和gADC的变异系数低于5%，响应评估的准确率达80.5%。

Conclusion: 该软件能够高效、准确地量化转移性骨病，为临床监测和治疗决策提供了可靠工具。

Abstract: We developed an AI-driven software solution to quantify metastatic bone
disease from WB-DWI scans. Core technologies include: (i) a weakly-supervised
Residual U-Net model generating a skeleton probability map to isolate bone;
(ii) a statistical framework for WB-DWI intensity normalisation, obtaining a
signal-normalised b=900s/mm^2 (b900) image; and (iii) a shallow convolutional
neural network that processes outputs from (i) and (ii) to generate a mask of
suspected bone lesions, characterised by higher b900 signal intensity due to
restricted water diffusion. This mask is applied to the gADC map to extract TDV
and gADC statistics. We tested the tool using expert-defined metastatic bone
disease delineations on 66 datasets, assessed repeatability of imaging
biomarkers (N=10), and compared software-based response assessment with a
construct reference standard based on clinical, laboratory and imaging
assessments (N=118). Dice score between manual and automated delineations was
0.6 for lesions within pelvis and spine, with an average surface distance of
2mm. Relative differences for log-transformed TDV (log-TDV) and median gADC
were below 9% and 5%, respectively. Repeatability analysis showed coefficients
of variation of 4.57% for log-TDV and 3.54% for median gADC, with intraclass
correlation coefficients above 0.9. The software achieved 80.5% accuracy, 84.3%
sensitivity, and 85.7% specificity in assessing response to treatment compared
to the construct reference standard. Computation time generating a mask
averaged 90 seconds per scan. Our software enables reproducible TDV and gADC
quantification from WB-DWI scans for monitoring metastatic bone disease
response, thus providing potentially useful measurements for clinical
decision-making in APC patients.

</details>


### [33] [DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update](https://arxiv.org/abs/2505.09017)
*Bizhan Alipour Pijan,Serdar Bozdag*

Main category: cs.LG

TL;DR: 该论文提出了DyGSSM方法，结合局部和全局特征提取，并通过状态空间模型优化参数更新，解决了动态图表示学习中局部与全局信息同时提取及时间依赖管理的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态图表示学习中无法同时提取局部和全局信息，且参数更新时未考虑当前快照的性能，导致时间依赖管理不足。

Method: 提出DyGSSM方法，结合GCN（局部特征提取）、随机游走与GRU（全局特征提取），并通过交叉注意力机制整合特征；使用HiPPO算法的SSM模型管理长期依赖。

Result: 在5个公开数据集上，DyGSSM在20个案例中的17个超越了现有基线及SOTA方法。

Conclusion: DyGSSM有效解决了动态图表示学习中的局部-全局信息整合与时间依赖管理问题，性能显著优于现有方法。

Abstract: Most of the dynamic graph representation learning methods involve dividing a
dynamic graph into discrete snapshots to capture the evolving behavior of nodes
over time. Existing methods primarily capture only local or global structures
of each node within a snapshot using message-passing and random walk-based
methods. Then, they utilize sequence-based models (e.g., transformers) to
encode the temporal evolution of node embeddings, and meta-learning techniques
to update the model parameters. However, these approaches have two limitations.
First, they neglect the extraction of global and local information
simultaneously in each snapshot. Second, they fail to consider the model's
performance in the current snapshot during parameter updates, resulting in a
lack of temporal dependency management. Recently, HiPPO (High-order Polynomial
Projection Operators) algorithm has gained attention for their ability to
optimize and preserve sequence history in State Space Model (SSM). To address
the aforementioned limitations in dynamic graph representation learning, we
propose a novel method called Multi-view Dynamic Graph Embeddings with State
Space Model Gradient Update (DyGSSM). Our approach combines Graph Convolution
Networks (GCN) for local feature extraction and random walk with Gated
Recurrent Unit (GRU) for global feature extraction in each snapshot. We then
integrate the local and global features using a cross-attention mechanism.
Additionally, we incorporate an SSM based on HiPPO algorithm to account for
long-term dependencies when updating model parameters, ensuring that model
performance in each snapshot informs subsequent updates. Experiments on five
public datasets show that our method outperforms existing baseline and
state-of-the-art (SOTA) methods in 17 out of 20 cases.

</details>


### [34] [Block-Biased Mamba for Long-Range Sequence Processing](https://arxiv.org/abs/2505.09022)
*Annan Yu,N. Benjamin Erichson*

Main category: cs.LG

TL;DR: Mamba虽然通过输入依赖的动态性扩展了SSMs并在多个领域表现出色，但其在长序列任务上表现不佳。研究通过理论分析提出改进方案B₂S₆，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: Mamba虽然在语言建模等任务上表现优异，但在长序列任务上的弱点限制了其通用性。研究旨在分析并解决这一问题。

Method: 通过理论分析Mamba的局限性，提出B₂S₆改进方案，结合块选择动态性和通道偏置以增强模型的表达力和稳定性。

Result: B₂S₆在长序列任务（LRA）上超越S4和S4D，同时保持语言建模任务的性能。

Conclusion: B₂S₆有效弥补了Mamba的不足，提升了其在长序列任务上的表现，证明了改进方案的实用性。

Abstract: Mamba extends earlier state space models (SSMs) by introducing
input-dependent dynamics, and has demonstrated strong empirical performance
across a range of domains, including language modeling, computer vision, and
foundation models. However, a surprising weakness remains: despite being built
on architectures designed for long-range dependencies, Mamba performs poorly on
long-range sequential tasks. Understanding and addressing this gap is important
for improving Mamba's universality and versatility. In this work, we analyze
Mamba's limitations through three perspectives: expressiveness, inductive bias,
and training stability. Our theoretical results show how Mamba falls short in
each of these aspects compared to earlier SSMs such as S4D. To address these
issues, we propose $\text{B}_2\text{S}_6$, a simple extension of Mamba's S6
unit that combines block-wise selective dynamics with a channel-specific bias.
We prove that these changes equip the model with a better-suited inductive bias
and improve its expressiveness and stability. Empirically,
$\text{B}_2\text{S}_6$ outperforms S4 and S4D on Long-Range Arena (LRA) tasks
while maintaining Mamba's performance on language modeling benchmarks.

</details>


### [35] [Single-shot prediction of parametric partial differential equations](https://arxiv.org/abs/2505.09063)
*Khalid Rafiq,Wenjing Liao,Aditya G. Nair*

Main category: cs.LG

TL;DR: Flexi-VAE 是一种数据驱动框架，用于非线性参数偏微分方程的高效单次预测，避免了迭代时间步进，同时保持高精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在预测非线性参数偏微分方程时需要迭代时间步进的问题，同时提升预测效率和稳定性。

Method: 采用变分自编码器（VAE）框架，结合神经传播器（DCP和PEP）推进潜在表示的时间演化，并通过几何诊断分析提升长时预测的鲁棒性。

Result: 在1D黏性Burgers方程和2D对流-扩散方程上验证了高精度预测，CPU和GPU速度分别提升50倍和90倍。

Conclusion: Flexi-VAE是一种可扩展、可解释的替代建模工具，适用于计算流体力学等参数PDE驱动的应用，并能扩展到更高维和复杂系统。

Abstract: We introduce Flexi-VAE, a data-driven framework for efficient single-shot
forecasting of nonlinear parametric partial differential equations (PDEs),
eliminating the need for iterative time-stepping while maintaining high
accuracy and stability. Flexi-VAE incorporates a neural propagator that
advances latent representations forward in time, aligning latent evolution with
physical state reconstruction in a variational autoencoder setting. We evaluate
two propagation strategies, the Direct Concatenation Propagator (DCP) and the
Positional Encoding Propagator (PEP), and demonstrate, through
representation-theoretic analysis, that DCP offers superior long-term
generalization by fostering disentangled and physically meaningful latent
spaces. Geometric diagnostics, including Jacobian spectral analysis, reveal
that propagated latent states reside in regions of lower decoder sensitivity
and more stable local geometry than those derived via direct encoding,
enhancing robustness for long-horizon predictions. We validate Flexi-VAE on
canonical PDE benchmarks, the 1D viscous Burgers equation and the 2D
advection-diffusion equation, achieving accurate forecasts across wide
parametric ranges. The model delivers over 50x CPU and 90x GPU speedups
compared to autoencoder-LSTM baselines for large temporal shifts. These results
position Flexi-VAE as a scalable and interpretable surrogate modeling tool for
accelerating high-fidelity simulations in computational fluid dynamics (CFD)
and other parametric PDE-driven applications, with extensibility to
higher-dimensional and more complex systems.

</details>


### [36] [AdaFortiTran: An Adaptive Transformer Model for Robust OFDM Channel Estimation](https://arxiv.org/abs/2505.09076)
*Berkay Guler,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: 提出Adaptive Fortified Transformer (AdaFortiTran)模型，用于提升OFDM系统在快速衰落和低信噪比场景下的信道估计性能，通过卷积层与Transformer编码器的结合，实现了在多种恶劣环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在OFDM系统快速衰落信道和低信噪比场景下性能下降的问题。

Method: 结合卷积层（捕获局部相关性）与Transformer编码器（全局注意力机制），并集成信道统计信息（如SNR、时延扩展和多普勒频移）作为先验。

Result: 在多种测试条件下（多普勒频移200-1000 Hz、SNR 0-25 dB、时延扩展50-300 ns），比现有模型降低最多6 dB的均方误差（MSE）。

Conclusion: AdaFortiTran在紧凑架构下显著提升了信道估计性能，尤其适用于高移动性环境。

Abstract: Deep learning models for channel estimation in Orthogonal Frequency Division
Multiplexing (OFDM) systems often suffer from performance degradation under
fast-fading channels and low-SNR scenarios. To address these limitations, we
introduce the Adaptive Fortified Transformer (AdaFortiTran), a novel model
specifically designed to enhance channel estimation in challenging
environments. Our approach employs convolutional layers that exploit locality
bias to capture strong correlations between neighboring channel elements,
combined with a transformer encoder that applies the global Attention mechanism
to channel patches. This approach effectively models both long-range
dependencies and spectro-temporal interactions within single OFDM frames. We
further augment the model's adaptability by integrating nonlinear
representations of available channel statistics SNR, delay spread, and Doppler
shift as priors. A residual connection is employed to merge global features
from the transformer with local features from early convolutional processing,
followed by final convolutional layers to refine the hierarchical channel
representation. Despite its compact architecture, AdaFortiTran achieves up to 6
dB reduction in mean squared error (MSE) compared to state-of-the-art models.
Tested across a wide range of Doppler shifts (200-1000 Hz), SNRs (0 to 25 dB),
and delay spreads (50-300 ns), it demonstrates superior robustness in
high-mobility environments.

</details>


### [37] [Human-like Cognitive Generalization for Large Models via Brain-in-the-loop Supervision](https://arxiv.org/abs/2505.09085)
*Jiaxuan Chen,Yu Qi,Yueming Wang,Gang Pan*

Main category: cs.LG

TL;DR: 通过结合脑信号监督学习，将人类概念结构迁移到深度神经网络，显著提升其抽象和未知概念的理解能力，实现认知能力的增强。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模语言模型在图像和语言理解方面表现卓越，但模拟人类的复杂认知能力（如抽象概念理解、推理和适应新场景）仍具挑战性。本研究旨在通过人类脑信号监督学习，弥补这一差距。

Method: 采用脑信号监督学习（brain-in-the-loop supervised learning），利用少量脑信号数据将人类概念结构迁移至深度神经网络中。

Result: 实验表明，该方法显著提升了模型在少样本/零样本学习及分布外识别任务中的表现，同时生成高度可解释的概念表示。

Conclusion: 人机协同监督可有效增强大模型的复杂认知能力，为开发更具人类认知特性的人工系统提供了新方向。

Abstract: Recent advancements in deep neural networks (DNNs), particularly large-scale
language models, have demonstrated remarkable capabilities in image and natural
language understanding. Although scaling up model parameters with increasing
volume of training data has progressively improved DNN capabilities, achieving
complex cognitive abilities - such as understanding abstract concepts,
reasoning, and adapting to novel scenarios, which are intrinsic to human
cognition - remains a major challenge. In this study, we show that
brain-in-the-loop supervised learning, utilizing a small set of brain signals,
can effectively transfer human conceptual structures to DNNs, significantly
enhancing their comprehension of abstract and even unseen concepts.
Experimental results further indicate that the enhanced cognitive capabilities
lead to substantial performance gains in challenging tasks, including
few-shot/zero-shot learning and out-of-distribution recognition, while also
yielding highly interpretable concept representations. These findings highlight
that human-in-the-loop supervision can effectively augment the complex
cognitive abilities of large models, offering a promising pathway toward
developing more human-like cognitive abilities in artificial systems.

</details>


### [38] [Generating time-consistent dynamics with discriminator-guided image diffusion models](https://arxiv.org/abs/2505.09089)
*Philipp Hess,Maximilian Gelbrecht,Christof Schötz,Michael Aich,Yu Huang,Shangshang Yang,Niklas Boers*

Main category: cs.LG

TL;DR: 提出了一种时间一致性判别器，使预训练的图像扩散模型能够生成逼真的时空动态，无需从头训练视频扩散模型（VDMs），在多个数据集上表现优异，包括稳定长期的每日气候模拟。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型（VDMs）虽能生成高度逼真的动态，但训练成本高且计算资源需求大。本文旨在通过时间一致性判别器，利用预训练的图像扩散模型实现高质量时空动态生成，降低应用门槛。

Method: 采用时间一致性判别器指导采样推断过程，无需修改或微调预训练的图像扩散模型。

Result: 在理想湍流模拟和全球降水数据集上表现与从头训练的VDMs相当，时间一致性、不确定性校准和偏差更低，并能实现长达百年的每日气候稳定模拟。

Conclusion: 该方法提供了一种高效且资源友好的替代方案，显著扩展了预训练模型的应用范围，尤其在长期动态模拟任务中表现突出。

Abstract: Realistic temporal dynamics are crucial for many video generation, processing
and modelling applications, e.g. in computational fluid dynamics, weather
prediction, or long-term climate simulations. Video diffusion models (VDMs) are
the current state-of-the-art method for generating highly realistic dynamics.
However, training VDMs from scratch can be challenging and requires large
computational resources, limiting their wider application. Here, we propose a
time-consistency discriminator that enables pretrained image diffusion models
to generate realistic spatiotemporal dynamics. The discriminator guides the
sampling inference process and does not require extensions or finetuning of the
image diffusion model. We compare our approach against a VDM trained from
scratch on an idealized turbulence simulation and a real-world global
precipitation dataset. Our approach performs equally well in terms of temporal
consistency, shows improved uncertainty calibration and lower biases compared
to the VDM, and achieves stable centennial-scale climate simulations at daily
time steps.

</details>


### [39] [Argus: Federated Non-convex Bilevel Learning over 6G Space-Air-Ground Integrated Network](https://arxiv.org/abs/2505.09106)
*Ya Liu,Kai Yang,Yu Zhu,Keying Yang,Haibo Zhao*

Main category: cs.LG

TL;DR: 该论文提出了一种名为Argus的异步算法，用于解决SAGIN中非凸非光滑的分散联邦双层学习问题，提升了动态网络下的训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统集中式同步优化算法在SAGIN中不适用，因其缺乏基础设施且环境时变，需开发异步算法以应对这些问题。

Method: 设计Argus异步算法，支持网络代理在时变网络中异步处理双层学习问题，避免拖慢整体训练速度。

Result: 通过理论分析（迭代、通信和计算复杂度）及数值实验验证了Argus的有效性。

Conclusion: Argus算法在SAGIN中展现出高效性，为6G网络优化提供了实用解决方案。

Abstract: The space-air-ground integrated network (SAGIN) has recently emerged as a
core element in the 6G networks. However, traditional centralized and
synchronous optimization algorithms are unsuitable for SAGIN due to
infrastructureless and time-varying environments. This paper aims to develop a
novel Asynchronous algorithm a.k.a. Argus for tackling non-convex and
non-smooth decentralized federated bilevel learning over SAGIN. The proposed
algorithm allows networked agents (e.g. autonomous aerial vehicles) to tackle
bilevel learning problems in time-varying networks asynchronously, thereby
averting stragglers from impeding the overall training speed. We provide a
theoretical analysis of the iteration complexity, communication complexity, and
computational complexity of Argus. Its effectiveness is further demonstrated
through numerical experiments.

</details>


### [40] [Sequential Treatment Effect Estimation with Unmeasured Confounders](https://arxiv.org/abs/2505.09113)
*Yingrong Wang,Anpeng Wu,Baohong Li,Ziyang Xiao,Ruoxuan Xiong,Qing Han,Kun Kuang*

Main category: cs.LG

TL;DR: DSIV-CFR框架利用工具变量和负控制假设解决序列治疗中的未观测混杂问题，提升了治疗效果估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 序列决策中，动态变化的治疗和结果易受未观测混杂影响，传统方法难以解决，需新方法调整偏差。

Method: 提出DSIV-CFR框架，结合工具变量（IV）与负控制假设，通过广义矩条件估计序列治疗效果。

Result: 在4个数据集上验证，单步和多步预测效果显著，能识别动态系统最优治疗。

Conclusion: DSIV-CFR有效解决序列治疗中的未观测混杂问题，为动态决策提供可靠方法。

Abstract: This paper studies the cumulative causal effects of sequential treatments in
the presence of unmeasured confounders. It is a critical issue in sequential
decision-making scenarios where treatment decisions and outcomes dynamically
evolve over time. Advanced causal methods apply transformer as a backbone to
model such time sequences, which shows superiority in capturing long time
dependence and periodic patterns via attention mechanism. However, even they
control the observed confounding, these estimators still suffer from unmeasured
confounders, which influence both treatment assignments and outcomes. How to
adjust the latent confounding bias in sequential treatment effect estimation
remains an open challenge. Therefore, we propose a novel Decomposing Sequential
Instrumental Variable framework for CounterFactual Regression (DSIV-CFR),
relying on a common negative control assumption. Specifically, an instrumental
variable (IV) is a special negative control exposure, while the previous
outcome serves as a negative control outcome. This allows us to recover the IVs
latent in observation variables and estimate sequential treatment effects via a
generalized moment condition. We conducted experiments on 4 datasets and
achieved significant performance in one- and multi-step prediction, supported
by which we can identify optimal treatments for dynamic systems.

</details>


### [41] [Fair Clustering via Alignment](https://arxiv.org/abs/2505.09131)
*Kunwoong Kim,Jihu Lee,Sangchul Park,Yongdai Kim*

Main category: cs.LG

TL;DR: 该论文提出了FCA算法通过分解公平K均值目标函数解决聚类算法中的公平性问题，保证优化聚类效用同时避免复杂约束。实验证明FCA在公平性与聚类效用之间取得了更优平衡，且稳定性高。


<details>
  <summary>Details</summary>
Motivation: 现有公平聚类算法因复杂性或近似性常导致聚类效能不足或数值不稳定，因此需开发更高效且稳定的公平聚类方法。

Method: FCA算法通过交替（1）寻找联合概率分布对齐不同受保护群体的数据，（2）在对齐空间中优化聚类中心，避免复杂约束。

Result: 实验显示FCA在公平性与聚类效用间取得更优平衡，且实现近乎完美的公平性而无数值不稳定。

Conclusion: FCA通过简化解法显著提升了公平聚类的实用性和稳定性，适用于实际场景的高效公平聚类。

Abstract: Algorithmic fairness in clustering aims to balance the proportions of
instances assigned to each cluster with respect to a given sensitive attribute.
While recently developed fair clustering algorithms optimize clustering
objectives under specific fairness constraints, their inherent complexity or
approximation often results in suboptimal clustering utility or numerical
instability in practice. To resolve these limitations, we propose a new fair
clustering algorithm based on a novel decomposition of the fair K-means
clustering objective function. The proposed algorithm, called Fair Clustering
via Alignment (FCA), operates by alternately (i) finding a joint probability
distribution to align the data from different protected groups, and (ii)
optimizing cluster centers in the aligned space. A key advantage of FCA is that
it theoretically guarantees approximately optimal clustering utility for any
given fairness level without complex constraints, thereby enabling high-utility
fair clustering in practice. Experiments show that FCA outperforms existing
methods by (i) attaining a superior trade-off between fairness level and
clustering utility, and (ii) achieving near-perfect fairness without numerical
instability.

</details>


### [42] [CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios](https://arxiv.org/abs/2505.09436)
*Raghav Garg,Kapil Sharma,Karan Gupta*

Main category: cs.LG

TL;DR: 介绍了CXMArena，这是一个用于评估AI在客户体验管理（CXM）操作环境中的新型大规模合成基准数据集，解决了现有基准缺乏真实性的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在CXM中的实用评估受限于数据稀缺和现有基准的不足，缺乏真实性和对复杂操作任务的覆盖。

Method: 开发了一个可扩展的LLM驱动的管道，模拟品牌CXM实体，包括知识文章、问题分类和联系中心对话，并通过噪声注入和自动验证增强真实性。

Result: 即使最先进的模型在文章搜索任务上仅达到68%的准确率，知识库优化的F1分数低至0.3，表明当前模型面临重大挑战。

Conclusion: CXMArena为评估AI在复杂CXM任务中的性能提供了更真实的基准，凸显了现有技术的局限性，需要更复杂的解决方案。

Abstract: Large Language Models (LLMs) hold immense potential for revolutionizing
Customer Experience Management (CXM), particularly in contact center
operations. However, evaluating their practical utility in complex operational
environments is hindered by data scarcity (due to privacy concerns) and the
limitations of current benchmarks. Existing benchmarks often lack realism,
failing to incorporate deep knowledge base (KB) integration, real-world noise,
or critical operational tasks beyond conversational fluency. To bridge this
gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset
specifically designed for evaluating AI in operational CXM contexts. Given the
diversity in possible contact center features, we have developed a scalable
LLM-powered pipeline that simulates the brand's CXM entities that form the
foundation of our datasets-such as knowledge articles including product
specifications, issue taxonomies, and contact center conversations. The
entities closely represent real-world distribution because of controlled noise
injection (informed by domain experts) and rigorous automated validation.
Building on this, we release CXMArena, which provides dedicated benchmarks
targeting five important operational tasks: Knowledge Base Refinement, Intent
Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with
Integrated Tools. Our baseline experiments underscore the benchmark's
difficulty: even state of the art embedding and generation models achieve only
68% accuracy on article search, while standard embedding methods yield a low F1
score of 0.3 for knowledge base refinement, highlighting significant challenges
for current models necessitating complex pipelines and solutions over
conventional techniques.

</details>


### [43] [Scaling Gaussian Process Regression with Full Derivative Observations](https://arxiv.org/abs/2505.09134)
*Daniel Huang*

Main category: cs.LG

TL;DR: DSoftKI是一种可扩展的高斯过程方法，能处理并预测完整导数观测，扩展了SoftKI方法以结合导数信息，并在合成函数基准和高维分子力场预测中表现出高准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的SoftKI方法在处理导数观测时存在局限性，因此需要一种能够高效且准确地处理包含导数信息的扩展方法。

Method: DSoftKI通过增强SoftKI的插值方案，引入了插值点相对于数据的方向信息，从而构建了包含一阶和二阶导数的可扩展近似核。

Result: 在合成函数基准和高维分子力场预测（100-1000维）中，DSoftKI表现出高准确性，并能扩展到比以往更大规模的数据集。

Conclusion: DSoftKI成功扩展了SoftKI方法，使其能够处理包含导数信息的观测，并在高维和大规模数据集上表现优越。

Abstract: We present a scalable Gaussian Process (GP) method that can fit and predict
full derivative observations called DSoftKI. It extends SoftKI, a method that
approximates a kernel via softmax interpolation from learned interpolation
point locations, to the setting with derivatives. DSoftKI enhances SoftKI's
interpolation scheme to incorporate the directional orientation of
interpolation points relative to the data. This enables the construction of a
scalable approximate kernel, including its first and second-order derivatives,
through interpolation. We evaluate DSoftKI on a synthetic function benchmark
and high-dimensional molecular force field prediction (100-1000 dimensions),
demonstrating that DSoftKI is accurate and can scale to larger datasets with
full derivative observations than previously possible.

</details>


### [44] [A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning](https://arxiv.org/abs/2505.09160)
*Berkay Guler,Giovanni Geraci,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: 论文提出了一种名为WiMAE的无线通道表示自监督学习基础模型，并通过对比学习进一步优化为ContraWiMAE，其在多样化的无线环境中展示了卓越的性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习在无线通道表示中的应用往往借鉴文本和图像处理的范式，未能充分解决无线通信的独特特性和限制。本文旨在填补这一空白。

Method: 首先提出基于Transformer的WiMAE模型，预训练于多天线无线通道数据集；随后开发ContraWiMAE，结合对比学习和重建任务，生成正样本对以提升表示质量。

Result: 实验证明，WiMAE和ContraWiMAE在未见过的场景中均有效，且ContraWiMAE在线性可分离性和适应多样性环境方面表现更优。

Conclusion: WiMAE和ContraWiMAE在自监督无线通道表示学习中表现出卓越性能，可作为未来研究的强大基线。

Abstract: Current applications of self-supervised learning to wireless channel
representation often borrow paradigms developed for text and image processing,
without fully addressing the unique characteristics and constraints of wireless
communications. Aiming to fill this gap, we first propose WiMAE (Wireless
Masked Autoencoder), a transformer-based encoder-decoder foundation model
pretrained on a realistic open-source multi-antenna wireless channel dataset.
Building upon this foundation, we develop ContraWiMAE, which enhances WiMAE by
incorporating a contrastive learning objective alongside the reconstruction
task in a unified multi-task framework. By warm-starting from pretrained WiMAE
weights and generating positive pairs via noise injection, the contrastive
component enables the model to capture both structural and discriminative
features, enhancing representation quality beyond what reconstruction alone can
achieve. Through extensive evaluation on unseen scenarios, we demonstrate the
effectiveness of both approaches across multiple downstream tasks, with
ContraWiMAE showing further improvements in linear separability and
adaptability in diverse wireless environments. Comparative evaluations against
a state-of-the-art wireless channel foundation model confirm the superior
performance and data efficiency of our models, highlighting their potential as
powerful baselines for future research in self-supervised wireless channel
representation learning.

</details>


### [45] [Quotient Complex Transformer (QCformer) for Perovskite Data Analysis](https://arxiv.org/abs/2505.09174)
*Xinyu You,Xiang Liu,Chuan-Shen Hu,Kelin Xia,Tze Chien Sum*

Main category: cs.LG

TL;DR: 提出了一种基于商复形（QC）的新表示方法和Quotient Complex Transformer（QCformer）模型，用于预测杂化有机-无机钙钛矿（HOIPs）的材料性质，解决了传统图神经网络在周期性结构和高阶相互作用建模中的不足，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 杂化有机-无机钙钛矿（HOIPs）因其优异的光电性能在光伏领域备受关注，但其周期性结构和高阶相互作用使传统图神经网络（GNNs）难以准确预测其性质。因此，需要一种新方法来解决这些局限性。

Method: 提出商复形表示（QC）和QCformer模型，将材料结构建模为商复形，通过不同维度的单胞编码多体相互作用，并利用商运算捕捉周期性。模型基于单胞的高阶特征，通过Transformer模块处理数据，并在Materials Project和JARVIS数据集上预训练，在HOIP数据集上微调。

Result: QCformer在HOIP性质预测上显著优于现有先进模型，验证了该方法的高效性。

Conclusion: 商复形表示与QCformer为钙钛矿材料的预测建模提供了强大工具，推动了功能材料的设计与发现。

Abstract: The discovery of novel functional materials is crucial in addressing the
challenges of sustainable energy generation and climate change. Hybrid
organic-inorganic perovskites (HOIPs) have gained attention for their
exceptional optoelectronic properties in photovoltaics. Recently, geometric
deep learning, particularly graph neural networks (GNNs), has shown strong
potential in predicting material properties and guiding material design.
However, traditional GNNs often struggle to capture the periodic structures and
higher-order interactions prevalent in such systems. To address these
limitations, we propose a novel representation based on quotient complexes
(QCs) and introduce the Quotient Complex Transformer (QCformer) for material
property prediction. A material structure is modeled as a quotient complex,
which encodes both pairwise and many-body interactions via simplices of varying
dimensions and captures material periodicity through a quotient operation. Our
model leverages higher-order features defined on simplices and processes them
using a simplex-based Transformer module. We pretrain QCformer on benchmark
datasets such as the Materials Project and JARVIS, and fine-tune it on HOIP
datasets. The results show that QCformer outperforms state-of-the-art models in
HOIP property prediction, demonstrating its effectiveness. The quotient complex
representation and QCformer model together contribute a powerful new tool for
predictive modeling of perovskite materials.

</details>


### [46] [Optimizing Urban Critical Green Space Development Using Machine Learning](https://arxiv.org/abs/2505.09175)
*Mohammad Ganjirad,Mahmoud Reza Delavar,Hossein Bagheri,Mohammad Mehdi Azizi*

Main category: cs.LG

TL;DR: 该论文提出了一个利用多种社会经济、环境与敏感指数来优先发展德黑兰城市绿地的框架。结合多种数据来源和机器学习模型，随机森林表现最佳，生成的绿地开发优先级地图经过微气候模拟验证，展示了绿色屋顶技术的降温效果。


<details>
  <summary>Details</summary>
Motivation: 城市化进程中缺乏绿地规划的科学依据，需结合多源数据与机器学习来优化城市绿地发展策略，以改善环境与生活质量。

Method: 利用Google Earth Engine、WRF模型等数据源构建指数，采用XGBoost、LightGBM、随机森林等模型分类植被覆盖，并通过特征重要性分析确定关键指标（如夜间地表温度与敏感人口）。

Result: 随机森林分类精度超94%，绿地优先级地图经模拟验证可使空气温度降低0.67°C，夜间地表温度与敏感人口为最显著影响因素。

Conclusion: 该框架为城市规划者提供了科学工具，能有效识别绿地开发优先级并验证环境效益，尤其适用于数据稀缺地区。

Abstract: This paper presents a novel framework for prioritizing urban green space
development in Tehran using diverse socio-economic, environmental, and
sensitivity indices. The indices were derived from various sources including
Google Earth Engine, air pollution measurements, municipal reports and the
Weather Research & Forecasting (WRF) model. The WRF model was used to estimate
the air temperature at a 1 km resolution due to insufficient meteorological
stations, yielding RMSE and MAE values of 0.96{\deg}C and 0.92{\deg}C,
respectively. After data preparation, several machine learning models were used
for binary vegetation cover classification including XGBoost, LightGBM, Random
Forest (RF) and Extra Trees. RF achieved the highest performance, exceeding 94%
in Overall Accuracy, Recall, and F1-score. Then, the probability of areas
lacking vegetation cover was assessed using socio-economic, environmental and
sensitivity indices. This resulted in the RF generating an urban green space
development prioritization map. Feature Importance Analysis revealed that the
most significant indices were nightly land surface temperature (LST) and
sensitive population. Finally, the framework performance was validated through
microclimate simulation to assess the critical areas after and before the green
space development by green roofs. The simulation demonstrated reducing air
temperature by up to 0.67{\deg}C after utilizing the green roof technology in
critical areas. As a result, this framework provides a valuable tool for urban
planners to develop green spaces.

</details>


### [47] [The Larger the Merrier? Efficient Large AI Model Inference in Wireless Edge Networks](https://arxiv.org/abs/2505.09214)
*Zhonghao Lyu,Ming Xiao,Jie Xu,Mikael Skoglund,Marco Di Renzo*

Main category: cs.LG

TL;DR: 该论文提出了一种边缘设备与服务器协同推理的大型人工智能模型（LAIM）剪裁与分区方案，通过联合优化剪裁比例、传输功率和计算频率来降低推理失真，并在资源有限的边缘环境中验证了其性能优势。


<details>
  <summary>Details</summary>
Motivation: 随着对大型人工智能模型（LAIM）服务的需求增长，边缘设备与服务器协同推理成为一种低延迟、保护隐私的资源高效策略。本文旨在通过剪裁和分区LAIM来优化推理性能和系统资源分配。

Method: 论文首先证明LAIM输出失真由其参数失真上限界定，并通过率失真理论推导参数失真下限。随后，基于分析结果，联合优化剪裁比例、传输功率和计算频率，提出高效算法解决非凸问题。

Result: 仿真显示模型参数失真能可靠界定输出失真，且提出的联合优化设计在推理性能、系统延迟和能耗方面优于基准方案（完全在设备或服务器推理）。分界点在异构资源有限的边缘环境中起关键作用。

Conclusion: 论文提出的剪裁感知协同推理方案有效平衡了性能与资源利用，为边缘环境中的LAIM部署提供了实用优化方法。

Abstract: The growing demand for large artificial intelligence model (LAIM) services is
driving a paradigm shift from traditional cloud-based inference to edge-based
inference for low-latency, privacy-preserving applications. In particular,
edge-device co-inference, which partitions LAIMs between edge devices and
servers, has emerged as a promising strategy for resource-efficient LAIM
execution in wireless networks. In this paper, we investigate a pruning-aware
LAIM co-inference scheme, where a pre-trained LAIM is pruned and partitioned
into on-device and on-server sub-models for deployment. For analysis, we first
prove that the LAIM output distortion is upper bounded by its parameter
distortion. Then, we derive a lower bound on parameter distortion via
rate-distortion theory, analytically capturing the relationship between pruning
ratio and co-inference performance. Next, based on the analytical results, we
formulate an LAIM co-inference distortion bound minimization problem by jointly
optimizing the pruning ratio, transmit power, and computation frequency under
system latency, energy, and available resource constraints. Moreover, we
propose an efficient algorithm to tackle the considered highly non-convex
problem. Finally, extensive simulations demonstrate the effectiveness of the
proposed design. In particular, model parameter distortion is shown to provide
a reliable bound on output distortion. Also, the proposed joint pruning ratio
and resource management design achieves superior performance in balancing
trade-offs among inference performance, system latency, and energy consumption
compared with benchmark schemes, such as fully on-device and on-server
inference. Moreover, the split point is shown to play a critical role in system
performance optimization under heterogeneous and resource-limited edge
environments.

</details>


### [48] [Birch SGD: A Tree Graph Framework for Local and Asynchronous SGD Methods](https://arxiv.org/abs/2505.09218)
*Alexander Tyurin,Danil Sivtsov*

Main category: cs.LG

TL;DR: Birch SGD 提出了一种新的统一框架，通过表示分布式 SGD 方法为计算树，简化收敛分析并设计了八种新方法，其中六种具有最优计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 旨在为分布式 SGD 方法提供一个统一的框架，简化收敛分析并促进新方法的设计，以优化计算和通信效率。

Method: 将每种方法表示为加权有向树（计算树），利用这一表示简化收敛分析，并基于此框架设计新方法。

Result: 设计了八种新方法，其中六种具有最优计算复杂度，并揭示了所有方法共享相同的迭代率，但存在不同的权衡（如计算频率与通信效率）。

Conclusion: Birch SGD 为理解和设计高效的异步和并行优化方法提供了统一的理论基础，揭示了不同方法之间的权衡关系。

Abstract: We propose a new unifying framework, Birch SGD, for analyzing and designing
distributed SGD methods. The central idea is to represent each method as a
weighted directed tree, referred to as a computation tree. Leveraging this
representation, we introduce a general theoretical result that reduces
convergence analysis to studying the geometry of these trees. This perspective
yields a purely graph-based interpretation of optimization dynamics, offering a
new and intuitive foundation for method development. Using Birch SGD, we design
eight new methods and analyze them alongside previously known ones, with at
least six of the new methods shown to have optimal computational time
complexity. Our research leads to two key insights: (i) all methods share the
same "iteration rate" of $O\left(\frac{(R + 1) L \Delta}{\varepsilon} +
\frac{\sigma^2 L \Delta}{\varepsilon^2}\right)$, where $R$ the maximum "tree
distance" along the main branch of a tree; and (ii) different methods exhibit
different trade-offs-for example, some update iterates more frequently,
improving practical performance, while others are more communication-efficient
or focus on other aspects. Birch SGD serves as a unifying framework for
navigating these trade-offs. We believe these results provide a unified
foundation for understanding, analyzing, and designing efficient asynchronous
and parallel optimization methods.

</details>


### [49] [Stable and Convexified Information Bottleneck Optimization via Symbolic Continuation and Entropy-Regularized Trajectories](https://arxiv.org/abs/2505.09239)
*Faruk Alpay*

Main category: cs.LG

TL;DR: 本文提出了一种通过符号延续和熵正则化轨迹实现稳定且凸的信息瓶颈（IB）优化的新方法，解决了IB方法在优化过程中不稳定和表示突变的问题。


<details>
  <summary>Details</summary>
Motivation: 信息瓶颈方法在优化过程中常因IB权衡参数β的临界点附近表示突变而表现出不稳定性，本文旨在解决这一问题。

Method: 引入符号延续和熵正则化轨迹，包含熵正则化项以分析证明IB解路径的凸性和唯一性，并通过敏感性分析和统计鲁棒性不确定性量化来稳定表示学习。

Result: 实验结果显示该方法在广泛的β值范围内实现了稳定的表示学习，并提供了具有95%置信区间的不确定性量化。

Conclusion: 该方法不仅解决了IB优化的不稳定性问题，还提供了开源实现和可复现性框架，为实际部署和未来扩展奠定了基础。

Abstract: The Information Bottleneck (IB) method frequently suffers from unstable
optimization, characterized by abrupt representation shifts near critical
points of the IB trade-off parameter, beta. In this paper, I introduce a novel
approach to achieve stable and convex IB optimization through symbolic
continuation and entropy-regularized trajectories. I analytically prove
convexity and uniqueness of the IB solution path when an entropy regularization
term is included, and demonstrate how this stabilizes representation learning
across a wide range of \b{eta} values. Additionally, I provide extensive
sensitivity analyses around critical points (beta) with statistically robust
uncertainty quantification (95% confidence intervals). The open-source
implementation, experimental results, and reproducibility framework included in
this work offer a clear path for practical deployment and future extension of
my proposed method.

</details>


### [50] [Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations](https://arxiv.org/abs/2505.09284)
*Panqi Chen,Yifan Sun,Lei Cheng,Yang Yang,Weichang Li,Yang Liu,Weiqing Liu,Jiang Bian,Shikai Fang*

Main category: cs.LG

TL;DR: SDIFT是一种基于扩散模型的框架，用于从稀疏和不规则观测中重建多维物理动力学，通过功能Tucker空间和序列扩散模型实现高精度和高效的计算。


<details>
  <summary>Details</summary>
Motivation: 解决从稀疏和非网格观测中建模和重建多维物理动力学的挑战，填补现有方法在连续性和稀疏性处理上的不足。

Method: 利用功能Tucker模型作为潜在空间表示，构建时序增强的UNet扩散模型，通过高斯过程噪声去噪生成核心张量序列，并引入消息传递后验采样机制。

Result: 在天文、环境和分子三个领域的物理系统中验证，显示出比现有方法更高的重建精度和计算效率。

Conclusion: SDIFT成功解决了稀疏和不规则观测下的物理动力学建模问题，为实际应用提供了高效且准确的解决方案。

Abstract: Modeling and reconstructing multidimensional physical dynamics from sparse
and off-grid observations presents a fundamental challenge in scientific
research. Recently, diffusion-based generative modeling shows promising
potential for physical simulation. However, current approaches typically
operate on on-grid data with preset spatiotemporal resolution, but struggle
with the sparsely observed and continuous nature of real-world physical
dynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in
Functional Tucker space, a novel framework that generates full-field evolution
of physical dynamics from irregular sparse observations. SDIFT leverages the
functional Tucker model as the latent space representer with proven universal
approximation property, and represents observations as latent functions and
Tucker core sequences. We then construct a sequential diffusion model with
temporally augmented UNet in the functional Tucker space, denoising noise drawn
from a Gaussian process to generate the sequence of core tensors.
  At the posterior sampling stage, we propose a Message-Passing Posterior
Sampling mechanism, enabling conditional generation of the entire sequence
guided by observations at limited time steps. We validate SDIFT on three
physical systems spanning astronomical (supernova explosions, light-year
scale), environmental (ocean sound speed fields, kilometer scale), and
molecular (organic liquid, millimeter scale) domains, demonstrating significant
improvements in both reconstruction accuracy and computational efficiency
compared to state-of-the-art approaches.

</details>


### [51] [Ranking-Based At-Risk Student Prediction Using Federated Learning and Differential Features](https://arxiv.org/abs/2505.09287)
*Shunsuke Yoneda,Valdemar Švábenský,Gen Li,Daisuke Deguchi,Atsushi Shimada*

Main category: cs.LG

TL;DR: 该论文提出了一种结合联邦学习和差分特征的方法，以解决教育数据挖掘中隐私保护和模型通用性问题，并在预测学习困难学生方面验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题，现有教育数据挖掘研究难以跨校整合学习日志和学术记录等敏感数据，导致模型性能和通用性受限。

Method: 采用联邦学习避免数据集中存储以保护隐私，并结合差分特征（使用相对值而非绝对值）提升模型性能和通用性。

Result: 在12门课程（1,136名学生）数据上训练模型，并在5门课程上验证。结果显示，该方法在隐私保护的同时，性能与集中式学习相当（Top-n精度、nDCG、PR-AUC），且差分特征全面提升了预测表现。

Conclusion: 该方法有效平衡隐私与性能，支持跨校模型训练，并通过早期预测能力验证了其实际应用价值。

Abstract: Digital textbooks are widely used in various educational contexts, such as
university courses and online lectures. Such textbooks yield learning log data
that have been used in numerous educational data mining (EDM) studies for
student behavior analysis and performance prediction. However, these studies
have faced challenges in integrating confidential data, such as academic
records and learning logs, across schools due to privacy concerns.
Consequently, analyses are often conducted with data limited to a single
school, which makes developing high-performing and generalizable models
difficult. This study proposes a method that combines federated learning and
differential features to address these issues. Federated learning enables model
training without centralizing data, thereby preserving student privacy.
Differential features, which utilize relative values instead of absolute
values, enhance model performance and generalizability. To evaluate the
proposed method, a model for predicting at-risk students was trained using data
from 1,136 students across 12 courses conducted over 4 years, and validated on
hold-out test data from 5 other courses. Experimental results demonstrated that
the proposed method addresses privacy concerns while achieving performance
comparable to that of models trained via centralized learning in terms of Top-n
precision, nDCG, and PR-AUC. Furthermore, using differential features improved
prediction performance across all evaluation datasets compared to
non-differential approaches. The trained models were also applicable for early
prediction, achieving high performance in detecting at-risk students in earlier
stages of the semester within the validation datasets.

</details>


### [52] [On the Learning with Augmented Class via Forests](https://arxiv.org/abs/2505.09294)
*Fan Xu,Wuyang Chen,Wei Gao*

Main category: cs.LG

TL;DR: LACForest提出了一种新的决策树和森林方法，通过引入增广Gini不纯度来学习训练数据中未见的增广类别，利用未标记测试数据提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决测试数据中出现训练数据中未见的增广类别时，传统决策树和森林方法无法有效处理的问题。

Method: 提出增广Gini不纯度作为新的分裂准则，构建LACForest浅层森林，并利用伪标记增广实例优化森林分裂。进一步开发深度神经森林，结合神经网络表示能力。

Result: 理论分析证明了增广Gini不纯度的收敛性，实验验证了LACForest方法的有效性。

Conclusion: LACForest通过增广Gini不纯度和神经网络结合，有效处理了增广类别的学习问题，为决策树和森林提供了新的扩展方向。

Abstract: Decision trees and forests have achieved successes in various real
applications, most working with all testing classes known in training data. In
this work, we focus on learning with augmented class via forests, where an
augmented class may appear in testing data yet not in training data. We
incorporate information of augmented class into trees' splitting, i.e., a new
splitting criterion, called augmented Gini impurity, is introduced to exploit
some unlabeled data from testing distribution. We then develop the approach
named Learning with Augmented Class via Forests (LACForest), which constructs
shallow forests based on the augmented Gini impurity and then splits forests
with pseudo-labeled augmented instances for better performance. We also develop
deep neural forests with a novel optimization objective based on our augmented
Gini impurity, so as to utilize the representation power of neural networks for
forests. Theoretically, we present the convergence analysis for augmented Gini
impurity, and finally conduct experiments to verify the effectiveness of our
approaches. The code is available at https://github.com/nju-xuf/LACForest/.

</details>


### [53] [Neural Multivariate Regression: Qualitative Insights from the Unconstrained Feature Model](https://arxiv.org/abs/2505.09308)
*George Andriopoulos,Soyuj Jung Basnet,Juan Guevara,Li Guo,Keith Ross*

Main category: cs.LG

TL;DR: UFM理论通过数学框架为DNN中的训练损失和性能提供了闭式解，并应用于多变量回归任务。研究发现多任务模型在相同或更强正则化下优于单任务模型，且目标维度方差小于1时归一化能降低训练MSE。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用UFM框架为深度神经网络的多变量回归任务（如模仿学习、机器人和强化学习）提供设计洞见，解决多任务模型性能及数据预处理策略的效果问题。

Method: 采用UFM理论进行数学建模和闭式解推导，并通过实证验证多任务模型与单任务模型的训练MSE对比，以及目标维度归一化对性能的影响。

Result: 理论预测和实证一致表明：多任务模型在正则化条件相同或更强时训练MSE更低；目标维度平均方差小于1时归一化可降低训练MSE。

Conclusion: UFM是分析DNN设计及数据预处理策略的有力工具，研究结果为多任务学习和回归目标预处理提供了理论支持和实践指导。

Abstract: The Unconstrained Feature Model (UFM) is a mathematical framework that
enables closed-form approximations for minimal training loss and related
performance measures in deep neural networks (DNNs). This paper leverages the
UFM to provide qualitative insights into neural multivariate regression, a
critical task in imitation learning, robotics, and reinforcement learning.
Specifically, we address two key questions: (1) How do multi-task models
compare to multiple single-task models in terms of training performance? (2)
Can whitening and normalizing regression targets improve training performance?
The UFM theory predicts that multi-task models achieve strictly smaller
training MSE than multiple single-task models when the same or stronger
regularization is applied to the latter, and our empirical results confirm
these findings. Regarding whitening and normalizing regression targets, the UFM
theory predicts that they reduce training MSE when the average variance across
the target dimensions is less than one, and our empirical results once again
confirm these findings. These findings highlight the UFM as a powerful
framework for deriving actionable insights into DNN design and data
pre-processing strategies.

</details>


### [54] [MUST: Multi-Scale Structural-Temporal Link Prediction Model for UAV Ad Hoc Networks](https://arxiv.org/abs/2505.09331)
*Cunlai Pu,Fangrui Wu,Rajput Ramiz Sharafat,Guangzhao Dai,Xiangbo Shu*

Main category: cs.LG

TL;DR: 该研究提出了一种名为MUST的多尺度结构-时间链路预测模型，用于高度动态且稀疏的无人机自组网（UANETs）。


<details>
  <summary>Details</summary>
Motivation: 在对抗环境中，无人机路径信息不可用，需仅基于历史拓扑信息预测未来链接。现有方法因忽略稀疏性和单一尺度时间动态性，导致信息捕捉不足。

Method: 采用图注意力网络（GATs）捕捉多层次结构特征，结合LSTM学习其时间动态性，并通过优化损失函数解决稀疏性问题。

Result: 在仿真生成的UANET数据集上，MUST实现了最先进的链路预测性能。

Conclusion: MUST模型能有效应对UANETs的高动态性和稀疏性，显著提升链路预测准确性。

Abstract: Link prediction in unmanned aerial vehicle (UAV) ad hoc networks (UANETs)
aims to predict the potential formation of future links between UAVs. In
adversarial environments where the route information of UAVs is unavailable,
predicting future links must rely solely on the observed historical topological
information of UANETs. However, the highly dynamic and sparse nature of UANET
topologies presents substantial challenges in effectively capturing meaningful
structural and temporal patterns for accurate link prediction. Most existing
link prediction methods focus on temporal dynamics at a single structural scale
while neglecting the effects of sparsity, resulting in insufficient information
capture and limited applicability to UANETs. In this paper, we propose a
multi-scale structural-temporal link prediction model (MUST) for UANETs.
Specifically, we first employ graph attention networks (GATs) to capture
structural features at multiple levels, including the individual UAV level, the
UAV community level, and the overall network level. Then, we use long
short-term memory (LSTM) networks to learn the temporal dynamics of these
multi-scale structural features. Additionally, we address the impact of
sparsity by introducing a sophisticated loss function during model
optimization. We validate the performance of MUST using several UANET datasets
generated through simulations. Extensive experimental results demonstrate that
MUST achieves state-of-the-art link prediction performance in highly dynamic
and sparse UANETs.

</details>


### [55] [GreenFactory: Ensembling Zero-Cost Proxies to Estimate Performance of Neural Networks](https://arxiv.org/abs/2505.09344)
*Gabriel Cortês,Nuno Lourenço,Paolo Romano,Penousal Machado*

Main category: cs.LG

TL;DR: GreenFactory 是一种基于随机森林回归器的零成本代理集成方法，用于直接预测模型测试准确率，解决了当前代理方法泛化性差和仅提供相对排名的问题，在 NATS-Bench 上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络架构搜索（NAS）需要训练和评估每个网络，耗时耗资源。现有的零成本代理方法泛化性差且仅提供相对排名，无法直接预测准确率。

Method: GreenFactory 提出了一种集成多个零成本代理的方法，利用随机森林回归器结合它们的优势，直接预测模型测试准确率。

Result: 在 NATS-Bench 的不同数据集上，GreenFactory 的 Kendall 相关系数表现优异（如 CIFAR-10 上 0.907），证明了其可靠性和泛化能力。

Conclusion: GreenFactory 通过集成零成本代理和直接预测准确率，显著提升了神经网络架构搜索的效率和实用性。

Abstract: Determining the performance of a Deep Neural Network during Neural
Architecture Search processes is essential for identifying optimal
architectures and hyperparameters. Traditionally, this process requires
training and evaluation of each network, which is time-consuming and
resource-intensive. Zero-cost proxies estimate performance without training,
serving as an alternative to traditional training. However, recent proxies
often lack generalization across diverse scenarios and provide only relative
rankings rather than predicted accuracies. To address these limitations, we
propose GreenFactory, an ensemble of zero-cost proxies that leverages a random
forest regressor to combine multiple predictors' strengths and directly predict
model test accuracy. We evaluate GreenFactory on NATS-Bench, achieving robust
results across multiple datasets. Specifically, GreenFactory achieves high
Kendall correlations on NATS-Bench-SSS, indicating substantial agreement
between its predicted scores and actual performance: 0.907 for CIFAR-10, 0.945
for CIFAR-100, and 0.920 for ImageNet-16-120. Similarly, on NATS-Bench-TSS, we
achieve correlations of 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for
ImageNet-16-120, showcasing its reliability in both search spaces.

</details>


### [56] [Exploiting the Potential Supervision Information of Clean Samples in Partial Label Learning](https://arxiv.org/abs/2505.09354)
*Guangtai Wang,Chi-Man Vong,Jintao Huang*

Main category: cs.LG

TL;DR: CleanSE是一种新的校准策略，通过利用数据集中干净样本的强监督信息，增强部分标签学习中候选标签的置信度，提升现有PLL方法的性能。


<details>
  <summary>Details</summary>
Motivation: 部分标签学习中的误报标签影响需减小，现有方法主要关注单个实例而忽视数据集中干净样本的强监督信息，CleanSE旨在利用这些干净样本提供指导和增强候选标签置信度。

Method: 结合可微分计数损失策略和K近邻算法，CleanSE假设若干净样本的标签是其近邻的候选标签之一，则该标签更有可能是其近邻的真实标签，并通过限制各标签计数区间来刻画样本分布。

Result: 在3个合成基准和5个真实PLL数据集上的实验表明，CleanSE可应用于大多数先进PLL方法并显著提升其性能。

Conclusion: CleanSE通过利用干净样本的监督信息，有效提升部分标签学习的性能，具有广泛应用潜力。

Abstract: Diminishing the impact of false-positive labels is critical for conducting
disambiguation in partial label learning. However, the existing disambiguation
strategies mainly focus on exploiting the characteristics of individual partial
label instances while neglecting the strong supervision information of clean
samples randomly lying in the datasets. In this work, we show that clean
samples can be collected to offer guidance and enhance the confidence of the
most possible candidates. Motivated by the manner of the differentiable count
loss strat- egy and the K-Nearest-Neighbor algorithm, we proposed a new
calibration strategy called CleanSE. Specifically, we attribute the most
reliable candidates with higher significance under the assumption that for each
clean sample, if its label is one of the candidates of its nearest neighbor in
the representation space, it is more likely to be the ground truth of its
neighbor. Moreover, clean samples offer help in characterizing the sample
distributions by restricting the label counts of each label to a specific
interval. Extensive experiments on 3 synthetic benchmarks and 5 real-world PLL
datasets showed this calibration strategy can be applied to most of the
state-of-the-art PLL methods as well as enhance their performance.

</details>


### [57] [Efficient Mixed Precision Quantization in Graph Neural Networks](https://arxiv.org/abs/2505.09361)
*Samir Moustafa,Nils M. Kriege,Wilfried N. Gansterer*

Main category: cs.LG

TL;DR: 论文提出MixQ-GNN框架，通过混合精度量化提升GNN推理效率，保证预测性能的同时减少计算开销。


<details>
  <summary>Details</summary>
Motivation: GNN在大规模图应用中计算需求高，需高效推理方法。混合精度量化是提升效率且不牺牲性能的潜在方案。

Method: 基于量化消息传递定理，提出MixQ-GNN框架，灵活选择GNN层各组件的整数位宽，优化计算效率。

Result: MixQ-GNN在节点分类和图分类任务中，相比FP32精度分别减少了5.5倍和5.1倍的比特操作。

Conclusion: MixQ-GNN通过混合精度量化有效平衡了GNN的计算效率与预测性能，为实际应用提供了可行方案。

Abstract: Graph Neural Networks (GNNs) have become essential for handling large-scale
graph applications. However, the computational demands of GNNs necessitate the
development of efficient methods to accelerate inference. Mixed precision
quantization emerges as a promising solution to enhance the efficiency of GNN
architectures without compromising prediction performance. Compared to
conventional deep learning architectures, GNN layers contain a wider set of
components that can be quantized, including message passing functions,
aggregation functions, update functions, the inputs, learnable parameters, and
outputs of these functions. In this paper, we introduce a theorem for efficient
quantized message passing to aggregate integer messages. It guarantees
numerical equality of the aggregated messages using integer values with respect
to those obtained with full (FP32) precision. Based on this theorem, we
introduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, which
flexibly selects effective integer bit-widths for all components within GNN
layers. Our approach systematically navigates the wide set of possible
bit-width combinations, addressing the challenge of optimizing efficiency while
aiming at maintaining comparable prediction performance. MixQ-GNN integrates
with existing GNN quantization methods, utilizing their graph structure
advantages to achieve higher prediction performance. On average, MixQ-GNN
achieved reductions in bit operations of 5.5x for node classification and 5.1x
for graph classification compared to architectures represented in FP32
precision.

</details>


### [58] [Personalized Control for Lower Limb Prosthesis Using Kolmogorov-Arnold Networks](https://arxiv.org/abs/2505.09366)
*SeyedMojtaba Mohasel,Alireza Afzal Aghaei,Corey Pew*

Main category: cs.LG

TL;DR: 该研究探讨了可学习激活函数在科尔莫戈罗夫-阿诺德网络（KAN）中对下肢假肢个性化控制的潜力，并比较了用户特定数据与合并数据对模型性能的影响。结果表明可学习激活函数未带来显著改进，但用户特定数据在机器学习模型中表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索可学习激活函数在假肢控制中的有效性，以及训练数据来源（用户特定或合并）对模型性能的影响，以提升转向意图预测的准确性。

Method: 方法包括收集五名下肢截肢者的惯性测量单元（IMU）数据，评估多层感知机（MLP）、KAN、卷积神经网络（CNN）和分数KAN（FKAN）在转向分类任务中的表现，并比较用户特定数据与合并数据训练的效果。

Result: 结果显示KAN和FKAN的可学习激活函数未显著优于传统MLP和CNN。用户特定数据在ML模型中表现更优（p < 0.05），但DL模型中两者无显著差异。

Conclusion: 结论指出可学习激活函数可能在更复杂任务和大数据集中展现优势，且DL模型可利用合并数据进行训练，为假肢控制的数据采集提供了灵活性。

Abstract: Objective: This paper investigates the potential of learnable activation
functions in Kolmogorov-Arnold Networks (KANs) for personalized control in a
lower-limb prosthesis. In addition, user-specific vs. pooled training data is
evaluated to improve machine learning (ML) and Deep Learning (DL) performance
for turn intent prediction.
  Method: Inertial measurement unit (IMU) data from the shank were collected
from five individuals with lower-limb amputation performing turning tasks in a
laboratory setting. Ability to classify an upcoming turn was evaluated for
Multilayer Perceptron (MLP), Kolmogorov-Arnold Network (KAN), convolutional
neural network (CNN), and fractional Kolmogorov-Arnold Networks (FKAN). The
comparison of MLP and KAN (for ML models) and FKAN and CNN (for DL models)
assessed the effectiveness of learnable activation functions. Models were
trained separately on user-specific and pooled data to evaluate the impact of
training data on their performance.
  Results: Learnable activation functions in KAN and FKAN did not yield
significant improvement compared to MLP and CNN, respectively. Training on
user-specific data yielded superior results compared to pooled data for ML
models ($p < 0.05$). In contrast, no significant difference was observed
between user-specific and pooled training for DL models.
  Significance: These findings suggest that learnable activation functions may
demonstrate distinct advantages in datasets involving more complex tasks and
larger volumes. In addition, pooled training showed comparable performance to
user-specific training in DL models, indicating that model training for
prosthesis control can utilize data from multiple participants.

</details>


### [59] [SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation](https://arxiv.org/abs/2505.09427)
*Achref Doula,Max Mühläuser,Alejandro Sanchez Guinea*

Main category: cs.LG

TL;DR: SafePath通过结合大语言模型（LLM）和适应性预测，为自动驾驶路径规划提供安全保证，显著减少不确定性和碰撞率。


<details>
  <summary>Details</summary>
Motivation: LLM在自动驾驶路径规划中存在过度自信和幻觉问题，引发安全隐患。

Method: 采用三阶段框架：生成候选路径、通过适应性预测过滤高风险轨迹、根据不确定性选择安全路径或人工接管。

Result: 实验显示，SafePath将规划不确定性降低77%，碰撞率减少高达70%。

Conclusion: SafePath有效提升了LLM路径规划的安全性，并在自主性与安全性间取得平衡。

Abstract: Large Language Models (LLMs) show growing promise in autonomous driving by
reasoning over complex traffic scenarios to generate path plans. However, their
tendencies toward overconfidence, and hallucinations raise critical safety
concerns. We introduce SafePath, a modular framework that augments LLM-based
path planning with formal safety guarantees using conformal prediction.
SafePath operates in three stages. In the first stage, we use an LLM that
generates a set of diverse candidate paths, exploring possible trajectories
based on agent behaviors and environmental cues. In the second stage, SafePath
filters out high-risk trajectories while guaranteeing that at least one safe
option is included with a user-defined probability, through a multiple-choice
question-answering formulation that integrates conformal prediction. In the
final stage, our approach selects the path with the lowest expected collision
risk when uncertainty is low or delegates control to a human when uncertainty
is high. We theoretically prove that SafePath guarantees a safe trajectory with
a user-defined probability, and we show how its human delegation rate can be
tuned to balance autonomy and safety. Extensive experiments on nuScenes and
Highway-env show that SafePath reduces planning uncertainty by 77\% and
collision rates by up to 70\%, demonstrating effectiveness in making LLM-driven
path planning more safer.

</details>


### [60] [Establishing Linear Surrogate Regret Bounds for Convex Smooth Losses via Convolutional Fenche-Young Losses](https://arxiv.org/abs/2505.09432)
*Yuzhou Cao,Han Bao,Lei Feng,Bo An*

Main category: cs.LG

TL;DR: 该论文构建了一个凸平滑代理损失函数，针对任意离散目标损失实现了线性代理遗憾界，解决了平滑性与遗憾界之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 针对凸平滑代理损失函数在遗憾传递过程中可能出现的优化与估计性能下降问题，研究如何保持平滑性的同时实现线性遗憾界。

Method: 基于Fenchel-Young损失和卷积负熵，构造了凸平滑代理损失函数，并通过预测链接设计保持线性遗憾界。

Result: 成功构建了一种凸平滑代理损失函数，其线性遗憾界得以保留，同时还实现了基础类别概率的一致性估计。

Conclusion: 通过凸分析在风险最小化中的应用，展示了优化与统计效率的结合，为解决类似问题提供了新思路。

Abstract: Surrogate regret bounds bridge the gap between the convergence rates of
surrogate and target losses, with linear bounds favorable for their lossless
regret transfer. While convex smooth surrogate losses are appealing in
particular due to the efficient estimation and optimization, the existence of a
trade-off between the smoothness and linear regret bound has been believed in
the community. That being said, the better optimization and estimation
properties of convex smooth surrogate losses may inevitably deteriorate after
undergoing the regret transfer onto a target loss. We overcome this dilemma for
arbitrary discrete target losses by constructing a convex smooth surrogate
loss, which entails a linear surrogate regret bound composed with a tailored
prediction link. The construction is based on Fenchel-Young losses generated by
the convolutional negentropy, which are equivalent to the infimal convolution
of a generalized negentropy and the target Bayes risk. Consequently, the
infimal convolution enables us to derive a smooth loss while maintaining the
surrogate regret bound linear. We additionally benefit from the infimal
convolution to have a consistent estimator of the underlying class probability.
Our results are overall a novel demonstration of how convex analysis penetrates
into optimization and statistical efficiency in risk minimization.

</details>


### [61] [Variational Rank Reduction Autoencoder](https://arxiv.org/abs/2505.09458)
*Jad Mounayer,Alicia Tierz,Jerome Tomezyk,Chady Ghnatios,Francisco Chinesta*

Main category: cs.LG

TL;DR: Variational Rank Reduction Autoencoders (VRRAEs) combine the strengths of deterministic Rank Reduction Autoencoders (RRAEs) and Variational Autoencoders (VAEs) to improve generative performance and avoid posterior collapse.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of RRAEs (deterministic nature) and VAEs (posterior collapse) by integrating their advantages into a single model.

Method: VRRAEs sample the latent space of RRAEs and regularize it with KL divergence, leveraging truncated SVD for additional regularization.

Result: VRRAEs outperform RRAEs and VAEs in random generation and interpolation tasks on MNIST, CelebA, and CIFAR-10 datasets, measured by FID scores.

Conclusion: The combination of SVD regularization and probabilistic sampling in VRRAEs enhances generative performance and mitigates posterior collapse.

Abstract: Deterministic Rank Reduction Autoencoders (RRAEs) enforce by construction a
regularization on the latent space by applying a truncated SVD. While this
regularization makes Autoencoders more powerful, using them for generative
purposes is counter-intuitive due to their deterministic nature. On the other
hand, Variational Autoencoders (VAEs) are well known for their generative
abilities by learning a probabilistic latent space. In this paper, we present
Variational Rank Reduction Autoencoders (VRRAEs), a model that leverages the
advantages of both RRAEs and VAEs. Our claims and results show that when
carefully sampling the latent space of RRAEs and further regularizing with the
Kullback-Leibler (KL) divergence (similarly to VAEs), VRRAEs outperform RRAEs
and VAEs. Additionally, we show that the regularization induced by the SVD not
only makes VRRAEs better generators than VAEs, but also reduces the possibility
of posterior collapse. Our results include a synthetic dataset of a small size
that showcases the robustness of VRRAEs against collapse, and three real-world
datasets; the MNIST, CelebA, and CIFAR-10, over which VRRAEs are shown to
outperform both VAEs and RRAEs on many random generation and interpolation
tasks based on the FID score.

</details>


### [62] [Preserving Plasticity in Continual Learning with Adaptive Linearity Injection](https://arxiv.org/abs/2505.09486)
*Seyed Roozbeh Razavi Rohani,Khashayar Khajavi,Wesley Chung,Mo Chen,Sharan Vaswani*

Main category: cs.LG

TL;DR: AdaLin通过动态调整神经元激活函数的线性化程度，有效缓解深度神经网络的可塑性损失，无需额外超参数或任务边界。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在非稳态问题中容易因可塑性损失影响持续学习能力，AdaLin旨在通过神经元级自适应解决这一问题。

Method: 提出AdaLin方法，通过可学习参数和门控机制动态调节激活函数的线性化程度，以保持梯度流动。

Result: 在多个基准测试（如MNIST、CIFAR）和复杂场景（如增量学习、强化学习）中表现优异，神经元级适应是关键。

Conclusion: AdaLin显著提升模型持续学习能力，神经元级适应是核心，适用于多样化任务。

Abstract: Loss of plasticity in deep neural networks is the gradual reduction in a
model's capacity to incrementally learn and has been identified as a key
obstacle to learning in non-stationary problem settings. Recent work has shown
that deep linear networks tend to be resilient towards loss of plasticity.
Motivated by this observation, we propose Adaptive Linearization (AdaLin), a
general approach that dynamically adapts each neuron's activation function to
mitigate plasticity loss. Unlike prior methods that rely on regularization or
periodic resets, AdaLin equips every neuron with a learnable parameter and a
gating mechanism that injects linearity into the activation function based on
its gradient flow. This adaptive modulation ensures sufficient gradient signal
and sustains continual learning without introducing additional hyperparameters
or requiring explicit task boundaries. When used with conventional activation
functions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can
significantly improve performance on standard benchmarks, including Random
Label and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split
CIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such
as class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in
mitigating plasticity loss in off-policy reinforcement learning agents. We
perform a systematic set of ablations that show that neuron-level adaptation is
crucial for good performance and analyze a number of metrics in the network
that might be correlated to loss of plasticity.

</details>


### [63] [Layered Unlearning for Adversarial Relearning](https://arxiv.org/abs/2505.09500)
*Timothy Qian,Vinith Suriyakumar,Ashia Wilson,Dylan Hadfield-Menell*

Main category: cs.LG

TL;DR: 论文研究了后训练方法（如微调、对齐和遗忘）如何改变语言模型行为和表示，并提出分层遗忘（LU）这一方法以提高对对抗性再学习的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 理解后训练方法（如微调、对齐和遗忘）如何修改语言模型的行为和表示，并探索其脆弱性的原因，尤其是如何通过提示工程或再学习绕过这些修改。

Method: 提出分层遗忘（LU）算法，通过分阶段遗忘数据集中的不同子集来减少再学习对完整数据的恢复能力，并在合成和大型语言模型实验中验证其效果。

Result: LU方法提高了多种不同遗忘方法对对抗性再学习的鲁棒性。

Conclusion: LU提升了机器遗忘的前沿水平，并提供了关于后训练更新效果的见解。

Abstract: Our goal is to understand how post-training methods, such as fine-tuning,
alignment, and unlearning, modify language model behavior and representations.
We are particularly interested in the brittle nature of these modifications
that makes them easy to bypass through prompt engineering or relearning. Recent
results suggest that post-training induces shallow context-dependent
``circuits'' that suppress specific response patterns. This could be one
explanation for the brittleness of post-training. To test this hypothesis, we
design an unlearning algorithm, Layered Unlearning (LU), that creates distinct
inhibitory mechanisms for a growing subset of the data. By unlearning the first
$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU
limits the ability of relearning on a subset of data to recover the full
dataset. We evaluate LU through a combination of synthetic and large language
model (LLM) experiments. We find that LU improves robustness to adversarial
relearning for several different unlearning methods. Our results contribute to
the state-of-the-art of machine unlearning and provide insight into the effect
of post-training updates.

</details>


### [64] [Towards Fair In-Context Learning with Tabular Foundation Models](https://arxiv.org/abs/2505.09503)
*Patrik Kenfack,Samira Ebrahimi Kaho,Ulrich Aïvodji*

Main category: cs.LG

TL;DR: 论文研究了表格基础模型在上下文学习中的公平性问题，提出了三种预处理策略，发现不确定性的演示选择能有效提升公平性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型的偏差已有研究，但表格上下文学习中的偏差表现尚不明确，论文旨在探讨这一公平性问题。

Method: 研究了三种预处理策略：相关性去除、组平衡演示选择和不确定性演示选择，通过实验分析其效果。

Result: 结果显示，基于不确定性的演示选择能显著提升不同组的预测公平性。

Conclusion: 表格上下文学习的公平性可通过适当的预处理策略改进，不确定性演示选择是有效方法。

Abstract: Tabular foundational models have exhibited strong in-context learning (ICL)
capabilities on structured data, allowing them to make accurate predictions on
test sets without parameter updates, using training examples as context. This
emerging approach positions itself as a competitive alternative to traditional
gradient-boosted tree methods. However, while biases in conventional machine
learning models are well documented, it remains unclear how these biases
manifest in tabular ICL. The paper investigates the fairness implications of
tabular ICL and explores three preprocessing strategies--correlation removal,
group-balanced demonstration selection, and uncertainty-based demonstration
selection--to address bias. Comprehensive experiments indicate that
uncertainty-based demonstration selection consistently enhances group fairness
of in-context predictions. The source code for reproducing the results of this
work can be found at https://github.com/patrikken/Fair-TabICL.

</details>


### [65] [SAD Neural Networks: Divergent Gradient Flows and Asymptotic Optimality via o-minimal Structures](https://arxiv.org/abs/2505.09572)
*Julian Kranz,Davide Gallon,Steffen Dereich,Arnulf Jentzen*

Main category: cs.LG

TL;DR: 该论文研究了全连接前馈神经网络在常见可微激活函数下的梯度流行为，证明梯度流要么收敛到临界点，要么发散到无穷大且损失趋近于渐近临界值。对于足够好的初始化，梯度流会发散到无穷大。理论结果通过数值实验验证。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络的梯度流行为，理解其收敛或发散的条件，特别是针对不同激活函数和初始化对训练动态的影响。

Method: 使用o-minimal结构的几何性质，结合理论分析和数值实验，研究梯度流的动力学特性。

Result: 证明梯度流在特定条件下会发散到无穷大且损失趋近于渐近临界值；对多项式目标函数，最优损失值为零且仅能渐近实现。

Conclusion: 梯度流的发散行为与初始化和网络结构密切相关，理论结果在真实场景中也有类似表现。

Abstract: We study gradient flows for loss landscapes of fully connected feed forward
neural networks with commonly used continuously differentiable activation
functions such as the logistic, hyperbolic tangent, softplus or GELU function.
We prove that the gradient flow either converges to a critical point or
diverges to infinity while the loss converges to an asymptotic critical value.
Moreover, we prove the existence of a threshold $\varepsilon>0$ such that the
loss value of any gradient flow initialized at most $\varepsilon$ above the
optimal level converges to it. For polynomial target functions and sufficiently
big architecture and data set, we prove that the optimal loss value is zero and
can only be realized asymptotically. From this setting, we deduce our main
result that any gradient flow with sufficiently good initialization diverges to
infinity. Our proof heavily relies on the geometry of o-minimal structures. We
confirm these theoretical findings with numerical experiments and extend our
investigation to real-world scenarios, where we observe an analogous behavior.

</details>


### [66] [Rhomboid Tiling for Geometric Graph Deep Learning](https://arxiv.org/abs/2505.09586)
*Yipeng Zhang,Longlong Li,Kelin Xia*

Main category: cs.LG

TL;DR: 提出了基于菱形铺砌结构的新型聚类方法RT和层次图聚类池化模型RTPool，用于图分类任务，在7个基准数据集上优于21种最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于消息传递的图神经网络主要依赖图的连接结构，难以捕捉几何图中的丰富几何特征。

Method: 提出基于菱形铺砌结构的RT聚类方法，并设计RTPool模型用于层次图聚类。

Result: RTPool在7个基准数据集上表现优异，优于21种竞争对手。

Conclusion: RT聚类和RTPool有效利用了数据的几何信息，提升了图分类性能。

Abstract: Graph Neural Networks (GNNs) have proven effective for learning from
graph-structured data through their neighborhood-based message passing
framework. Many hierarchical graph clustering pooling methods modify this
framework by introducing clustering-based strategies, enabling the construction
of more expressive and powerful models. However, all of these message passing
framework heavily rely on the connectivity structure of graphs, limiting their
ability to capture the rich geometric features inherent in geometric graphs. To
address this, we propose Rhomboid Tiling (RT) clustering, a novel clustering
method based on the rhomboid tiling structure, which performs clustering by
leveraging the complex geometric information of the data and effectively
extracts its higher-order geometric structures. Moreover, we design RTPool, a
hierarchical graph clustering pooling model based on RT clustering for graph
classification tasks. The proposed model demonstrates superior performance,
outperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.

</details>


### [67] [Online Isolation Forest](https://arxiv.org/abs/2505.09593)
*Filippo Leveni,Guilherme Weigert Cassales,Bernhard Pfahringer,Albert Bifet,Giacomo Boracchi*

Main category: cs.LG

TL;DR: Online-iForest提出了一种适用于流数据环境的在线异常检测方法，其性能与在线方法相当，且效率优于所有竞争者。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法多为离线方法，不适合流数据环境；现有在线方法多依赖周期性重训练，无法适应实时变化。

Method: 提出了Online-iForest方法，专为流数据设计，能实时跟踪数据生成过程的变化。

Result: 在真实数据集上验证，Online-iForest性能与在线方法相当，效率优于所有方法，适用于需快速识别的场景。

Conclusion: Online-iForest是一种高效的在线异常检测解决方案，适用于网络安全、欺诈检测等实时性要求高的领域。

Abstract: The anomaly detection literature is abundant with offline methods, which
require repeated access to data in memory, and impose impractical assumptions
when applied to a streaming context. Existing online anomaly detection methods
also generally fail to address these constraints, resorting to periodic
retraining to adapt to the online context. We propose Online-iForest, a novel
method explicitly designed for streaming conditions that seamlessly tracks the
data generating process as it evolves over time. Experimental validation on
real-world datasets demonstrated that Online-iForest is on par with online
alternatives and closely rivals state-of-the-art offline anomaly detection
techniques that undergo periodic retraining. Notably, Online-iForest
consistently outperforms all competitors in terms of efficiency, making it a
promising solution in applications where fast identification of anomalies is of
primary importance such as cybersecurity, fraud and fault detection.

</details>


### [68] [Adversarial Suffix Filtering: a Defense Pipeline for LLMs](https://arxiv.org/abs/2505.09602)
*David Khachaturov,Robert Mullins*

Main category: cs.LG

TL;DR: 论文提出了一种轻量级、模型无关的防御方法Adversarial Suffix Filtering (ASF)，用于保护大型语言模型免受对抗性后缀攻击，有效降低攻击成功率至4%以下，同时不影响模型正常功能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动系统和公共环境中广泛应用，但存在越狱漏洞，现有防御方法要么依赖模型内部架构，要么增加计算负担或易被绕过。

Method: ASF作为输入预处理和净化器，检测并过滤对抗性后缀，无需依赖模型内部架构，适用于黑盒和白盒攻击场景。

Result: ASF将攻击效能降低至4%以下，在非对抗性场景中对模型功能影响极小。

Conclusion: ASF为LLMs提供了一种高效、轻量的防御方案，显著提升了模型的安全性和可靠性。

Abstract: Large Language Models (LLMs) are increasingly embedded in autonomous systems
and public-facing environments, yet they remain susceptible to jailbreak
vulnerabilities that may undermine their security and trustworthiness.
Adversarial suffixes are considered to be the current state-of-the-art
jailbreak, consistently outperforming simpler methods and frequently succeeding
even in black-box settings. Existing defenses rely on access to the internal
architecture of models limiting diverse deployment, increase memory and
computation footprints dramatically, or can be bypassed with simple prompt
engineering methods. We introduce $\textbf{Adversarial Suffix Filtering}$
(ASF), a lightweight novel model-agnostic defensive pipeline designed to
protect LLMs against adversarial suffix attacks. ASF functions as an input
preprocessor and sanitizer that detects and filters adversarially crafted
suffixes in prompts, effectively neutralizing malicious injections. We
demonstrate that ASF provides comprehensive defense capabilities across both
black-box and white-box attack settings, reducing the attack efficacy of
state-of-the-art adversarial suffix generation methods to below 4%, while only
minimally affecting the target model's capabilities in non-adversarial
scenarios.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [69] [Deep reinforcement learning-based longitudinal control strategy for automated vehicles at signalised intersections](https://arxiv.org/abs/2505.08896)
*Pankaj Kumar,Aditya Mishra,Pranamesh Chakraborty,Subrahmanya Swamy Peruru*

Main category: cs.AI

TL;DR: 基于深度强化学习的信号交叉路口纵向车辆控制策略，结合安全、效率和舒适度优化，DDPG和SAC算法表现优异。


<details>
  <summary>Details</summary>
Motivation: 信号交叉路口的复杂决策过程使自动驾驶车辆控制具有挑战性，需兼顾安全性、效率和舒适度。

Method: 提出综合奖励函数，结合距离效率、黄灯决策和非对称加减速响应，采用DDPG和SAC算法处理连续动作空间。

Result: 相比人类驾驶，RL模型在保持安全的同时实现了更高效率和更低急动度，DDPG动作更平滑。

Conclusion: 深度强化学习能有效提升信号交叉路口的交通安全性、效率和舒适度。

Abstract: Developing an autonomous vehicle control strategy for signalised
intersections (SI) is one of the challenging tasks due to its inherently
complex decision-making process. This study proposes a Deep Reinforcement
Learning (DRL) based longitudinal vehicle control strategy at SI. A
comprehensive reward function has been formulated with a particular focus on
(i) distance headway-based efficiency reward, (ii) decision-making criteria
during amber light, and (iii) asymmetric acceleration/ deceleration response,
along with the traditional safety and comfort criteria. This reward function
has been incorporated with two popular DRL algorithms, Deep Deterministic
Policy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the
continuous action space of acceleration/deceleration. The proposed models have
been trained on the combination of real-world leader vehicle (LV) trajectories
and simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process.
The overall performance of the proposed models has been tested using Cumulative
Distribution Function (CDF) plots and compared with the real-world trajectory
data. The results show that the RL models successfully maintain lower distance
headway (i.e., higher efficiency) and jerk compared to human-driven vehicles
without compromising safety. Further, to assess the robustness of the proposed
models, we evaluated the model performance on diverse safety-critical
scenarios, in terms of car-following and traffic signal compliance. Both DDPG
and SAC models successfully handled the critical scenarios, while the DDPG
model showed smoother action profiles compared to the SAC model. Overall, the
results confirm that DRL-based longitudinal vehicle control strategy at SI can
help to improve traffic safety, efficiency, and comfort.

</details>


### [70] [Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora](https://arxiv.org/abs/2505.08905)
*Michael Majurski,Cynthia Matuszek*

Main category: cs.AI

TL;DR: 本文提出了一种自动化构建基于事实的合成数据模型评估方法，利用语言模型（LMs）自动评估领域特定知识，仅需基础文档（如教科书）作为输入。该方法与人工构建的基准高度相关（Spearman排名相关性0.96，Pearson准确性相关性0.79），并支持生成多选题和开放式问题，用于诊断LM能力。应用该方法发现Gemma3模型表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于网络规模训练数据集的使用，语言模型（LMs）可能已在训练中接触过用户可能提出的各种问题。然而，人工构建评估基准的限制性和不可扩展性使得无法覆盖所有感兴趣的领域。因此，需要一种自动化方法来生成模型评估。

Method: 提出了一种自动化构建基于事实的合成数据模型评估方法，利用LMs自动评估领域特定知识，仅需基础文档作为输入。支持生成多选题和开放式问题。

Result: 自动化生成的合成数据基准与人工构建的问题高度相关（Spearman排名相关性0.96，Pearson准确性相关性0.79）。应用该方法发现Gemma3模型表现优于预期。

Conclusion: 该方法为模型评估提供了一种高效、可扩展的自动化解决方案，能够生成高质量的诊断性问题，展示出在评估语言模型领域特定能力方面的潜力。

Abstract: Language Models (LMs) continue to advance, improving response quality and
coherence. Given Internet-scale training datasets, LMs have likely encountered
much of what users might ask them to generate in some form during their
training. A plethora of evaluation benchmarks have been constructed to assess
model quality, response appropriateness, and reasoning capabilities. However,
the human effort required for benchmark construction is limited and being
rapidly outpaced by the size and scope of the models under evaluation.
Additionally, having humans build a benchmark for every possible domain of
interest is impractical. Therefore, we propose a methodology for automating the
construction of fact-based synthetic data model evaluations grounded in
document populations. This work leverages those very same LMs to evaluate
domain-specific knowledge automatically, using only grounding documents (e.g.,
a textbook) as input. This synthetic data benchmarking approach corresponds
well with human curated questions with a Spearman ranking correlation of 0.96
and a benchmark evaluation Pearson accuracy correlation of 0.79. This novel
tool supports generating both multiple choice and open-ended synthetic data
questions to gain diagnostic insight of LM capability. We apply this
methodology to evaluate model performance on a recent relevant arXiv preprint,
discovering a surprisingly strong performance from Gemma3 models.

</details>


### [71] [Generalization in Monitored Markov Decision Processes (Mon-MDPs)](https://arxiv.org/abs/2505.08988)
*Montaser Mohammedalamen,Michael Bowling*

Main category: cs.AI

TL;DR: 该论文探讨了在奖励不完全可观测的Mon-MDP中，如何通过函数近似和奖励模型实现从可观测状态到不可观测状态的泛化，并提出了一种基于奖励不确定性的谨慎策略优化方法，以解决过度泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中奖励并非总是可观测，而现有Mon-MDP研究局限于简单表格化案例，限制了实际应用。本文旨在通过函数近似和奖励模型提升Mon-MDP的适用性，并解决泛化中的过拟合问题。

Method: 结合函数近似与学习的奖励模型，将可观测奖励的状态泛化到不可观测状态；提出基于奖励不确定性的谨慎策略优化方法，以减少过度泛化。

Result: 显示该方法能在理论不可解环境中实现接近最优的策略，但需警惕因过度泛化导致的错误奖励外推问题。

Conclusion: 通过函数近似和奖励模型扩展了Mon-MDP的实用性，并引入谨慎优化方法缓解了过度泛化，为理论到实际的过渡提供了方向。

Abstract: Reinforcement learning (RL) typically models the interaction between the
agent and environment as a Markov decision process (MDP), where the rewards
that guide the agent's behavior are always observable. However, in many
real-world scenarios, rewards are not always observable, which can be modeled
as a monitored Markov decision process (Mon-MDP). Prior work on Mon-MDPs have
been limited to simple, tabular cases, restricting their applicability to
real-world problems. This work explores Mon-MDPs using function approximation
(FA) and investigates the challenges involved. We show that combining function
approximation with a learned reward model enables agents to generalize from
monitored states with observable rewards, to unmonitored environment states
with unobservable rewards. Therefore, we demonstrate that such generalization
with a reward model achieves near-optimal policies in environments formally
defined as unsolvable. However, we identify a critical limitation of such
function approximation, where agents incorrectly extrapolate rewards due to
overgeneralization, resulting in undesirable behaviors. To mitigate
overgeneralization, we propose a cautious police optimization method leveraging
reward uncertainty. This work serves as a step towards bridging this gap
between Mon-MDP theory and real-world applications.

</details>


### [72] [Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.08995)
*Ardian Selmonaj,Oleg Szehr,Giacomo Del Rio,Alessandro Antonucci,Adrian Schneider,Michael Rüegsegger*

Main category: cs.AI

TL;DR: 该研究提出了一种分层多智能体强化学习框架，用于分析模拟空战场景中的异构智能体，以低成本在安全环境中探索现实防御场景。通过分层决策（低层控制个体，高层指挥整体）解决了复杂动态、状态空间大等挑战，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过模拟空战场景，以低成本和安全的方式探索现实防御策略，解决多智能体系统中复杂动态和大状态空间的挑战。

Method: 采用分层强化学习框架：低层策略控制个体单位，高层指挥官策略发布宏观指令。通过课程学习逐步训练低层策略，再基于预训练策略训练高层指挥官。

Result: 实验验证表明，该分层框架有效提高了任务成功率，并解决了多智能体系统的复杂性问题。

Conclusion: 分层多智能体强化学习框架在模拟空战中表现优异，为现实防御场景的探索提供了可行且高效的解决方案。

Abstract: This work presents a Hierarchical Multi-Agent Reinforcement Learning
framework for analyzing simulated air combat scenarios involving heterogeneous
agents. The objective is to identify effective Courses of Action that lead to
mission success within preset simulations, thereby enabling the exploration of
real-world defense scenarios at low cost and in a safe-to-fail setting.
Applying deep Reinforcement Learning in this context poses specific challenges,
such as complex flight dynamics, the exponential size of the state and action
spaces in multi-agent systems, and the capability to integrate real-time
control of individual units with look-ahead planning. To address these
challenges, the decision-making process is split into two levels of
abstraction: low-level policies control individual units, while a high-level
commander policy issues macro commands aligned with the overall mission
targets. This hierarchical structure facilitates the training process by
exploiting policy symmetries of individual agents and by separating control
from command tasks. The low-level policies are trained for individual combat
control in a curriculum of increasing complexity. The high-level commander is
then trained on mission targets given pre-trained control policies. The
empirical validation confirms the advantages of the proposed framework.

</details>


### [73] [Deep Reinforcement Learning for Power Grid Multi-Stage Cascading Failure Mitigation](https://arxiv.org/abs/2505.09012)
*Bo Meng,Chenghao Xu,Yongli Zhu*

Main category: cs.AI

TL;DR: 论文提出一种基于强化学习的多级级联故障缓解策略，通过确定性策略梯度算法训练智能体，在IEEE 14和118母线系统中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的级联故障缓解策略多为单级设计，无法应对复杂的多级故障场景。

Method: 将多级级联故障建模为强化学习任务，开发仿真环境，并采用确定性策略梯度算法训练智能体。

Result: 所提方法在IEEE 14和118母线系统中展现出良好的有效性。

Conclusion: 强化学习方法能有效解决多级级联故障，为电网稳定运行提供新思路。

Abstract: Cascading failures in power grids can lead to grid collapse, causing severe
disruptions to social operations and economic activities. In certain cases,
multi-stage cascading failures can occur. However, existing
cascading-failure-mitigation strategies are usually single-stage-based,
overlooking the complexity of the multi-stage scenario. This paper treats the
multi-stage cascading failure problem as a reinforcement learning task and
develops a simulation environment. The reinforcement learning agent is then
trained via the deterministic policy gradient algorithm to achieve continuous
actions. Finally, the effectiveness of the proposed approach is validated on
the IEEE 14-bus and IEEE 118-bus systems.

</details>


### [74] [Automated Meta Prompt Engineering for Alignment with the Theory of Mind](https://arxiv.org/abs/2505.09024)
*Aaron Baughman,Rahul Agarwal,Eduardo Morales,Gozde Akay*

Main category: cs.AI

TL;DR: 论文提出了一种元提示方法，通过联合优化人类预期与大型语言模型（LLM）神经状态的相似性，生成复杂任务的流畅文本。采用代理强化学习技术，通过上下文学习让LLM作为法官（LLMaaJ）指导另一个LLM生成内容。实验表明，该方法在人类内容编辑的预期对齐上达到53.8%，并提升了内容质量。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决大型语言模型生成内容与人类预期对齐的问题，特别是在复杂任务中的文本流畅性和心理预期匹配。通过优化类似心理理论（ToM）的神经状态，提升生成内容的质量和相关性。

Method: 采用代理强化学习技术，利用LLM作为法官（LLMaaJ）通过上下文学习指导另一个LLM生成内容。通过分析人类编辑的AI生成文本，量化预期对齐，并在Hilbert向量空间中优化内容特性。

Result: 在US Open 2024的实际应用中，该方法实现了53.8%的人类预期对齐，平均迭代次数为4.38。内容质量显著提升，尤其是在网球赛事的覆盖范围上。

Conclusion: 该研究表明，元提示和LLMaaJ能有效提升LLM生成内容的质量和对齐性。该方法已成功应用于体育和娱乐领域的其他现场活动。

Abstract: We introduce a method of meta-prompting that jointly produces fluent text for
complex tasks while optimizing the similarity of neural states between a
human's mental expectation and a Large Language Model's (LLM) neural
processing. A technique of agentic reinforcement learning is applied, in which
an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning,
how to produce content by interpreting the intended and unintended generated
text traits. To measure human mental beliefs around content production, users
modify long form AI-generated text articles before publication at the US Open
2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM)
alignment problem by anticipating and including human edits within the creation
of text from an LLM. Throughout experimentation and by interpreting the results
of a live production system, the expectations of human content reviewers had
100% of alignment with AI 53.8% of the time with an average iteration count of
4.38. The geometric interpretation of content traits such as factualness,
novelty, repetitiveness, and relevancy over a Hilbert vector space combines
spatial volume (all trait importance) with vertices alignment (individual trait
relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an
increase in content quality by extending the coverage of tennis action. Our
work that was deployed at the US Open 2024 has been used across other live
events within sports and entertainment.

</details>


### [75] [Monte Carlo Beam Search for Actor-Critic Reinforcement Learning in Continuous Control](https://arxiv.org/abs/2505.09029)
*Hazim Alzorgan,Abolfazl Razi*

Main category: cs.AI

TL;DR: MCBS结合波束搜索和蒙特卡洛模拟改进TD3的探索和动作选择，在多个基准测试中表现优于TD3和其他基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于噪声的探索方法（如TD3）可能导致策略收敛不理想，需改进探索效率。

Method: 提出MCBS，利用波束搜索生成候选动作并通过短视界蒙特卡洛模拟评估，结合TD3框架。

Result: 在HalfCheetah等环境中，MCBS的样本效率和性能均优于TD3、SAC等方法，收敛速度更快（200千步达90%最大奖励）。

Conclusion: MCBS通过结构化前瞻搜索提升策略学习效果，同时保持计算效率，适用于复杂控制任务。

Abstract: Actor-critic methods, like Twin Delayed Deep Deterministic Policy Gradient
(TD3), depend on basic noise-based exploration, which can result in less than
optimal policy convergence. In this study, we introduce Monte Carlo Beam Search
(MCBS), a new hybrid method that combines beam search and Monte Carlo rollouts
with TD3 to improve exploration and action selection. MCBS produces several
candidate actions around the policy's output and assesses them through
short-horizon rollouts, enabling the agent to make better-informed choices. We
test MCBS across various continuous-control benchmarks, including
HalfCheetah-v4, Walker2d-v5, and Swimmer-v5, showing enhanced sample efficiency
and performance compared to standard TD3 and other baseline methods like SAC,
PPO, and A2C. Our findings emphasize MCBS's capability to enhance policy
learning through structured look-ahead search while ensuring computational
efficiency. Additionally, we offer a detailed analysis of crucial
hyperparameters, such as beam width and rollout depth, and explore adaptive
strategies to optimize MCBS for complex control tasks. Our method shows a
higher convergence rate across different environments compared to TD3, SAC,
PPO, and A2C. For instance, we achieved 90% of the maximum achievable reward
within around 200 thousand timesteps compared to 400 thousand timesteps for the
second-best method.

</details>


### [76] [Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification](https://arxiv.org/abs/2505.09031)
*Adarsh Kumar,Hwiyoon Kim,Jawahar Sai Nathani,Neil Roy*

Main category: cs.AI

TL;DR: 该论文研究了通过结合思维链（CoT）与检索增强生成（RAG）以及自一致性和自验证策略，减少大型语言模型（LLM）生成的幻觉问题，并提高事实准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂任务中容易出现自信但错误或不相关的信息（即幻觉），尽管思维链提示改善了多步推理，但仍未完全解决该问题。因此，研究者希望通过整合外部知识源和自验证技术，进一步提升生成内容的准确性与连贯性。

Method: 研究比较了基线LLM、思维链（CoT）、CoT+RAG、自一致性和自验证技术的效果。具体方法包括在推理中引入外部知识（RAG），并让模型自我验证或修正输出。

Result: 实验结果展示了每种方法的有效性，并确定了最能在保持流畅性和推理深度的同时减少幻觉的鲁棒方法。

Conclusion: 结合CoT与RAG及自验证策略能显著降低幻觉问题，同时维持生成内容的质量和逻辑性，为LLM在开放任务中的应用提供了更可靠的解决方案。

Abstract: Hallucination, where large language models (LLMs) generate confident but
incorrect or irrelevant information, remains a key limitation in their
application to complex, open-ended tasks. Chain-of-thought (CoT) prompting has
emerged as a promising method for improving multistep reasoning by guiding
models through intermediate steps. However, CoT alone does not fully address
the hallucination problem. In this work, we investigate how combining CoT with
retrieval-augmented generation (RAG), as well as applying self-consistency and
self-verification strategies, can reduce hallucinations and improve factual
accuracy. By incorporating external knowledge sources during reasoning and
enabling models to verify or revise their own outputs, we aim to generate more
accurate and coherent responses. We present a comparative evaluation of
baseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification
techniques. Our results highlight the effectiveness of each method and identify
the most robust approach for minimizing hallucinations while preserving fluency
and reasoning depth.

</details>


### [77] [Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer](https://arxiv.org/abs/2505.09114)
*Minh Hoang Nguyen,Linh Le Pham Van,Thommen George Karimpanal,Sunil Gupta,Hung Le*

Main category: cs.AI

TL;DR: CRDT框架通过反事实推理增强决策变换器（DT），在数据有限或动态改变的情况下优于传统DT方法，并实现轨迹拼接能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统DT在离线数据集训练中因数据不足或次优行为导致的性能下降问题。

Method: 提出反事实推理决策变换器（CRDT），通过生成和利用反事实经验增强决策能力。

Result: 在Atari和D4RL基准测试中，CRDT表现优于传统DT，尤其在数据有限或动态改变的场景中。

Conclusion: 反事实推理能显著提升强化学习代理的性能和泛化能力，无需修改架构即可实现轨迹拼接。

Abstract: Decision Transformers (DT) play a crucial role in modern reinforcement
learning, leveraging offline datasets to achieve impressive results across
various domains. However, DT requires high-quality, comprehensive data to
perform optimally. In real-world applications, the lack of training data and
the scarcity of optimal behaviours make training on offline datasets
challenging, as suboptimal data can hinder performance. To address this, we
propose the Counterfactual Reasoning Decision Transformer (CRDT), a novel
framework inspired by counterfactual reasoning. CRDT enhances DT ability to
reason beyond known data by generating and utilizing counterfactual
experiences, enabling improved decision-making in unseen scenarios. Experiments
across Atari and D4RL benchmarks, including scenarios with limited data and
altered dynamics, demonstrate that CRDT outperforms conventional DT approaches.
Additionally, reasoning counterfactually allows the DT agent to obtain
stitching abilities, combining suboptimal trajectories, without architectural
modifications. These results highlight the potential of counterfactual
reasoning to enhance reinforcement learning agents' performance and
generalization capabilities.

</details>


### [78] [Reproducibility Study of "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents"](https://arxiv.org/abs/2505.09289)
*Pedro M. P. Curvo,Mara Dragomir,Salvador Torpes,Mohammadmahdi Rahimi*

Main category: cs.AI

TL;DR: 本文评估并扩展了Piatti等人的GovSim框架，验证了大模型（如GPT-4-turbo）在资源共享场景中的合作能力优于小模型，并探讨了通用化原则的影响。研究还通过新模型、多语言和异构环境扩展了框架适用性，发现高性能模型能影响低性能模型的合作行为。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证和扩展GovSim框架，评估不同规模LLM在合作决策中的表现，并探索该框架在新模型、新场景和多语言环境中的适用性。

Method: 通过复现关键实验、测试新模型（如DeepSeek-V3和GPT-4o-mini）、引入异构多智能体环境、日语指令场景及逆向环境，分析LLM的合作行为。

Result: 结果表明，大模型能实现可持续合作（无论通用化原则是否存在），而小模型需依赖该原则。框架在新模型、场景和语言中均适用，且高性能模型能带动低性能模型提升合作行为。

Conclusion: GovSim框架具有广泛适用性，为复杂合作任务中LLM的适应性提供了重要见解，并揭示了高性能模型对多智能体系统的积极影响，对合作AI发展有重要意义。

Abstract: This study evaluates and extends the findings made by Piatti et al., who
introduced GovSim, a simulation framework designed to assess the cooperative
decision-making capabilities of large language models (LLMs) in
resource-sharing scenarios. By replicating key experiments, we validate claims
regarding the performance of large models, such as GPT-4-turbo, compared to
smaller models. The impact of the universalization principle is also examined,
with results showing that large models can achieve sustainable cooperation,
with or without the principle, while smaller models fail without it. In
addition, we provide multiple extensions to explore the applicability of the
framework to new settings. We evaluate additional models, such as DeepSeek-V3
and GPT-4o-mini, to test whether cooperative behavior generalizes across
different architectures and model sizes. Furthermore, we introduce new
settings: we create a heterogeneous multi-agent environment, study a scenario
using Japanese instructions, and explore an "inverse environment" where agents
must cooperate to mitigate harmful resource distributions. Our results confirm
that the benchmark can be applied to new models, scenarios, and languages,
offering valuable insights into the adaptability of LLMs in complex cooperative
tasks. Moreover, the experiment involving heterogeneous multi-agent systems
demonstrates that high-performing models can influence lower-performing ones to
adopt similar behaviors. This finding has significant implications for other
agent-based applications, potentially enabling more efficient use of
computational resources and contributing to the development of more effective
cooperative AI systems.

</details>


### [79] [Access Controls Will Solve the Dual-Use Dilemma](https://arxiv.org/abs/2505.09341)
*Evžen Wybitul*

Main category: cs.AI

TL;DR: AI安全系统面临双重用途困境，提出基于用户凭证和风险分类的访问控制框架，结合专家模块实现高效风险检测，平衡模型效用与安全性。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统因仅依赖请求内容判断而导致的合法请求被拒或有害请求通过的问题。

Method: 提出基于已验证用户凭证和模型输出风险分类的访问控制框架，引入专家模块训练方法实现高效风险检测。

Result: 框架初步实现了对AI能力的细粒度管理，合法用户可访问专业知识，同时阻止恶意行为。

Conclusion: 上下文访问控制框架有效解决了AI的双重用途困境，兼顾模型效用与安全。

Abstract: AI safety systems face a dual-use dilemma. Since the same request can be
either harmless or harmful depending on who made it and why, if the system
makes decisions based solely on the request's content, it will refuse some
legitimate queries and let pass harmful ones. To address this, we propose a
conceptual access control framework, based on verified user credentials (such
as institutional affiliation) and classifiers that assign model outputs to risk
categories (such as advanced virology). The system permits responses only when
the user's verified credentials match the category's requirements. For
implementation of the model output classifiers, we introduce a theoretical
approach utilizing small, gated expert modules integrated into the generator
model, trained with gradient routing, that enable efficient risk detection
without the capability gap problems of external monitors. While open questions
remain about the verification mechanisms, risk categories, and the technical
implementation, our framework makes the first step toward enabling granular
governance of AI capabilities: verified users gain access to specialized
knowledge without arbitrary restrictions, while adversaries are blocked from
it. This contextual approach reconciles model utility with robust safety,
addressing the dual-use dilemma.

</details>


### [80] [The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners](https://arxiv.org/abs/2505.09396)
*Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis*

Main category: cs.AI

TL;DR: 这篇论文探讨了大型语言模型（LLM）作为代理系统在博弈论设置中如何模拟人类策略推理，并比较了三种代理设计的效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解LLM代理是否能复制人类策略推理，并探索代理设计的复杂性如何影响其与人类行为的对齐。

Method: 方法包括设计三种代理（简单博弈论模型、非结构化LLM代理、LLM与传统代理框架集成），并通过猜测游戏测试其表现。

Result: 结果表明，受人类认知结构启发的设计能提升LLM代理与人类行为的对齐，但复杂性与人类相似性的关系是非线性的。

Conclusion: 结论指出，LLM代理的性能依赖于其底层能力，简单的架构增强可能存在局限性。

Abstract: The rapid rise of large language models (LLMs) has shifted artificial
intelligence (AI) research toward agentic systems, motivating the use of weaker
and more flexible notions of agency. However, this shift raises key questions
about the extent to which LLM-based agents replicate human strategic reasoning,
particularly in game-theoretic settings. In this context, we examine the role
of agentic sophistication in shaping artificial reasoners' performance by
evaluating three agent designs: a simple game-theoretic model, an unstructured
LLM-as-agent model, and an LLM integrated into a traditional agentic framework.
Using guessing games as a testbed, we benchmarked these agents against human
participants across general reasoning patterns and individual role-based
objectives. Furthermore, we introduced obfuscated game scenarios to assess
agents' ability to generalise beyond training distributions. Our analysis,
covering over 2000 reasoning samples across 25 agent configurations, shows that
human-inspired cognitive structures can enhance LLM agents' alignment with
human strategic behaviour. Still, the relationship between agentic design
complexity and human-likeness is non-linear, highlighting a critical dependence
on underlying LLM capabilities and suggesting limits to simple architectural
augmentation.

</details>


### [81] [Counterfactual Strategies for Markov Decision Processes](https://arxiv.org/abs/2505.09412)
*Paul Kobialka,Lina Gerlach,Francesco Leofante,Erika Ábrahám,Silvia Lizeth Tapia Tarifa,Einar Broch Johnsen*

Main category: cs.AI

TL;DR: 该论文提出了针对马尔可夫决策过程（MDP）的反事实策略方法，填补了现有反事实解释方法在序列决策任务中的空白。通过优化问题编码，生成最小化修改以实现期望目标的策略，并在实际数据集中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法主要关注单步决策，不适用于序列决策任务（如MDP）。论文旨在填补这一空白，提供一种在复杂序列决策中计算反事实策略的方法。

Method: 通过将反事实策略建模为非线性优化问题，识别初始策略的最小修改，以降低不期望结果的概率。进一步扩展方法以生成多样化的反事实策略。

Result: 在四个真实数据集上验证了方法的可行性，证明了其在复杂序列决策任务中的实用价值。

Conclusion: 提出的反事实策略方法为序列决策任务提供了有效的解释工具，扩展了反事实解释的应用范围。

Abstract: Counterfactuals are widely used in AI to explain how minimal changes to a
model's input can lead to a different output. However, established methods for
computing counterfactuals typically focus on one-step decision-making, and are
not directly applicable to sequential decision-making tasks. This paper fills
this gap by introducing counterfactual strategies for Markov Decision Processes
(MDPs). During MDP execution, a strategy decides which of the enabled actions
(with known probabilistic effects) to execute next. Given an initial strategy
that reaches an undesired outcome with a probability above some limit, we
identify minimal changes to the initial strategy to reduce that probability
below the limit. We encode such counterfactual strategies as solutions to
non-linear optimization problems, and further extend our encoding to synthesize
diverse counterfactual strategies. We evaluate our approach on four real-world
datasets and demonstrate its practical viability in sophisticated sequential
decision-making tasks.

</details>


### [82] [\textsc{rfPG}: Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs](https://arxiv.org/abs/2505.09518)
*Maris F. L. Galesloot,Roman Andriushchenko,Milan Češka,Sebastian Junges,Nils Jansen*

Main category: cs.AI

TL;DR: 该论文提出了针对部分可观察马尔可夫决策过程（POMDPs）的鲁棒性策略方法，结合形式化验证和次梯度上升技术，生成对未知环境更具适应性的策略。


<details>
  <summary>Details</summary>
Motivation: 传统POMDPs的最优策略在环境扰动下可能不够鲁棒，而隐藏模型POMDPs（HM-POMDPs）通过考虑多个潜在环境模型来增强策略的适应性。

Method: 结合两种正交技术：(1) 形式化验证确定最坏情况POMDP，(2) 次梯度上升优化策略以应对最坏情况。

Result: 实证表明，该方法比基线策略更鲁棒，能泛化到未见的POMDPs，并能扩展到包含超过10万个环境的HM-POMDPs。

Conclusion: 提出的方法有效提升了策略在复杂不确定性环境中的鲁棒性和泛化能力。

Abstract: Partially observable Markov decision processes (POMDPs) model specific
environments in sequential decision-making under uncertainty. Critically,
optimal policies for POMDPs may not be robust against perturbations in the
environment. Hidden-model POMDPs (HM-POMDPs) capture sets of different
environment models, that is, POMDPs with a shared action and observation space.
The intuition is that the true model is hidden among a set of potential models,
and it is unknown which model will be the environment at execution time. A
policy is robust for a given HM-POMDP if it achieves sufficient performance for
each of its POMDPs. We compute such robust policies by combining two orthogonal
techniques: (1) a deductive formal verification technique that supports
tractable robust policy evaluation by computing a worst-case POMDP within the
HM-POMDP and (2) subgradient ascent to optimize the candidate policy for a
worst-case POMDP. The empirical evaluation shows that, compared to various
baselines, our approach (1) produces policies that are more robust and
generalize better to unseen POMDPs and (2) scales to HM-POMDPs that consist of
over a hundred thousand environments.

</details>


### [83] [Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?](https://arxiv.org/abs/2505.09614)
*Anthony GX-Chen,Dongyan Lin,Mandana Samiei,Doina Precup,Blake A. Richards,Rob Fergus,Kenneth Marino*

Main category: cs.AI

TL;DR: 语言模型在因果推理中存在“析取偏差”，偏向常见但忽略罕见证据的因果关系，且与人类成年人的推理模式相似。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否具备探索和推断因果结构的能力，揭示其推理偏差。

Method: 使用发展心理学中的“Blicket Test”范式，测试模型在不同因果关系中的表现。

Result: 模型在常见析取因果关系中表现良好，但对合取关系存在系统性偏差；提出了减少偏差的采样方法。

Conclusion: 语言模型的因果推理模式与人类成年人相似，需改进方法以实现更科学的推理。

Abstract: Language model (LM) agents are increasingly used as autonomous
decision-makers who need to actively gather information to guide their
decisions. A crucial cognitive skill for such agents is the efficient
exploration and understanding of the causal structure of the world -- key to
robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs
possess this capability or exhibit systematic biases leading to erroneous
conclusions. In this work, we examine LMs' ability to explore and infer causal
relationships, using the well-established "Blicket Test" paradigm from
developmental psychology. We find that LMs reliably infer the common, intuitive
disjunctive causal relationships but systematically struggle with the unusual,
yet equally (or sometimes even more) evidenced conjunctive ones. This
"disjunctive bias" persists across model families, sizes, and prompting
strategies, and performance further declines as task complexity increases.
Interestingly, an analogous bias appears in human adults, suggesting that LMs
may have inherited deep-seated reasoning heuristics from their training data.
To this end, we quantify similarities between LMs and humans, finding that LMs
exhibit adult-like inference profiles (but not children-like). Finally, we
propose a test-time sampling method which explicitly samples and eliminates
hypotheses about causal relationships from the LM. This scalable approach
significantly reduces the disjunctive bias and moves LMs closer to the goal of
scientific, causally rigorous reasoning.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [84] [Statistical Mean Estimation with Coded Relayed Observations](https://arxiv.org/abs/2505.09098)
*Yan Hao Ling,Zhouhao Yang,Jonathan Scarlett*

Main category: cs.IT

TL;DR: 该论文研究了在样本通过中继（“老师”）传输到解码器（“学生”）的统计均值估计问题，建立了在大偏差体制下可实现的紧致误差指数，并展示了两种基线方法的不足。


<details>
  <summary>Details</summary>
Motivation: 研究通过中继信道传输样本的统计均值估计问题，旨在解决直接观测样本不可行时的估计性能优化。

Method: 研究采用大偏差体制下的极小极大估计误差分析方法，首先针对伯努利源和二进制对称信道，后推广至亚高斯、重尾分布及任意离散无记忆信道。

Result: 建立了在估计精度和信道质量的广泛范围内紧致的误差指数，表明两种基线方法在误差指数上严格次优。

Conclusion: 该研究为通过中继信道进行统计均值估计提供了理论支持，展示了优化方法的优越性。

Abstract: We consider a problem of statistical mean estimation in which the samples are
not observed directly, but are instead observed by a relay (``teacher'') that
transmits information through a memoryless channel to the decoder
(``student''), who then produces the final estimate. We consider the minimax
estimation error in the large deviations regime, and establish achievable error
exponents that are tight in broad regimes of the estimation accuracy and
channel quality. In contrast, two natural baseline methods are shown to yield
strictly suboptimal error exponents. We initially focus on Bernoulli sources
and binary symmetric channels, and then generalize to sub-Gaussian and
heavy-tailed settings along with arbitrary discrete memoryless channels.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [85] [Position: Restructuring of Categories and Implementation of Guidelines Essential for VLM Adoption in Healthcare](https://arxiv.org/abs/2505.08818)
*Amara Tariq,Rimita Lahiri,Charles Kahn,Imon Banerjee*

Main category: cs.CY

TL;DR: 该立场论文主张为视觉语言模型（VLM）研究制定标准化报告协议，并提出分类框架和检查表以确保研究的可重现性和质量。


<details>
  <summary>Details</summary>
Motivation: 由于VLM研究的多样性和高风险医疗环境的需求，传统机器学习报告标准需要调整以适应多阶段VLM研究。

Method: 提出VLM研究的分类框架，并制定针对性能评估、数据报告和论文撰写的标准化指南，最终汇总为检查表。

Result: 建立了一套全面的VLM研究报告标准，并通过分类框架和检查表实现一致性。

Conclusion: 标准化报告协议和工具将提升VLM研究的透明度和可重现性，推动社区采纳。

Abstract: The intricate and multifaceted nature of vision language model (VLM)
development, adaptation, and application necessitates the establishment of
clear and standardized reporting protocols, particularly within the high-stakes
context of healthcare. Defining these reporting standards is inherently
challenging due to the diverse nature of studies involving VLMs, which vary
significantly from the development of all new VLMs or finetuning for domain
alignment to off-the-shelf use of VLM for targeted diagnosis and prediction
tasks. In this position paper, we argue that traditional machine learning
reporting standards and evaluation guidelines must be restructured to
accommodate multiphase VLM studies; it also has to be organized for intuitive
understanding of developers while maintaining rigorous standards for
reproducibility. To facilitate community adoption, we propose a categorization
framework for VLM studies and outline corresponding reporting standards that
comprehensively address performance evaluation, data reporting protocols, and
recommendations for manuscript composition. These guidelines are organized
according to the proposed categorization scheme. Lastly, we present a checklist
that consolidates reporting standards, offering a standardized tool to ensure
consistency and quality in the publication of VLM-related research.

</details>


### [86] [Will AI Take My Job? Evolving Perceptions of Automation and Labor Risk in Latin America](https://arxiv.org/abs/2505.08841)
*Andrea Cremaschi,Dae-Jin Lee,Manuele Leonelli*

Main category: cs.CY

TL;DR: 论文研究了拉丁美洲公众对AI和机器人导致失业的担忧，分析了2017-2023年的调查数据，发现教育水平和政治倾向是关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 随着AI和机器人重塑劳动力市场，了解公众对这些技术的看法至关重要，尤其在非发达国家。

Method: 采用统计建模和潜在类别分析，基于48,000多人的调查数据，分析失业担忧的结构和意识形态驱动因素。

Result: 发现2018年是担忧高峰期，教育水平和政治倾向是最一致的影响因素，且存在显著的时间和国家差异。

Conclusion: 研究为新兴经济体的AI焦虑提供了新视角，补充了全球北方以外的自动化态度研究。

Abstract: As artificial intelligence and robotics increasingly reshape the global labor
market, understanding public perceptions of these technologies becomes
critical. We examine how these perceptions have evolved across Latin America,
using survey data from the 2017, 2018, 2020, and 2023 waves of the
Latinobar\'ometro. Drawing on responses from over 48,000 individuals across 16
countries, we analyze fear of job loss due to artificial intelligence and
robotics. Using statistical modeling and latent class analysis, we identify key
structural and ideological predictors of concern, with education level and
political orientation emerging as the most consistent drivers. Our findings
reveal substantial temporal and cross-country variation, with a notable peak in
fear during 2018 and distinct attitudinal profiles emerging from latent
segmentation. These results offer new insights into the social and structural
dimensions of AI anxiety in emerging economies and contribute to a broader
understanding of public attitudes toward automation beyond the Global North.

</details>


### [87] [FareShare: A Tool for Labor Organizers to Estimate Lost Wages and Contest Arbitrary AI and Algorithmic Deactivations](https://arxiv.org/abs/2505.08904)
*Varun Nagaraj Rao,Samantha Dalal,Andrew Schwartz,Amna Liaqat,Dana Calacci,Andrés Monroy-Hernández*

Main category: cs.CY

TL;DR: 摘要介绍了一个名为FareShare的工具，帮助被平台突然封禁的共享乘车司机自动计算工资损失，显著提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 共享乘车司机被平台突然封禁（通过算法决策）会严重影响其经济状况，而现有申诉流程和工资恢复机制缺乏高效工具支持。

Method: 通过为期6个月的合作，设计了自动化计算工资损失的工具FareShare，并在3个月的实地部署中测试其效果。

Result: FareShare将工资损失计算时间减少了95%以上，消除了人工数据输入错误，并提高了法律团队生成仲裁报告的效率。

Conclusion: 尽管FareShare效果显著，但在高风险的劳动情境中，工具的信任、同意和采用仍面临挑战。

Abstract: What happens when a rideshare driver is suddenly locked out of the platform
connecting them to riders, wages, and daily work? Deactivation-the abrupt
removal of gig workers' platform access-typically occurs through arbitrary AI
and algorithmic decisions with little explanation or recourse. This represents
one of the most severe forms of algorithmic control and often devastates
workers' financial stability. Recent U.S. state policies now mandate appeals
processes and recovering compensation during the period of wrongful
deactivation based on past earnings. Yet, labor organizers still lack effective
tools to support these complex, error-prone workflows. We designed FareShare, a
computational tool automating lost wage estimation for deactivated drivers,
through a 6 month partnership with the State of Washington's largest rideshare
labor union. Over the following 3 months, our field deployment of FareShare
registered 178 account signups. We observed that the tool could reduce lost
wage calculation time by over 95%, eliminate manual data entry errors, and
enable legal teams to generate arbitration-ready reports more efficiently.
Beyond these gains, the deployment also surfaced important socio-technical
challenges around trust, consent, and tool adoption in high-stakes labor
contexts.

</details>


### [88] [The Geography of Transportation Cybersecurity: Visitor Flows, Industry Clusters, and Spatial Dynamics](https://arxiv.org/abs/2505.08822)
*Yuhao Wang,Kailai Wang,Songhua Hu,Yunpeng,Zhang,Gino Lim,Pengyu Zhu*

Main category: cs.CY

TL;DR: 这篇论文研究了美国交通运输网络安全生态系统中访客流量的时空动态，通过结合社会经济因素和AI预测技术，提出了BiTransGCN框架来预测行业集群和劳动力分布。


<details>
  <summary>Details</summary>
Motivation: 交通运输网络安全生态系统的快速发展需要对其行业集群和劳动力分布有更深入的理解，以支持经济规划和战略性投资。

Method: 研究开发了BiTransGCN框架，结合基于注意力的Transformer架构和图卷积网络（GCN）来建模和预测访客流量模式。

Result: 通过AI预测技术和空间分析的结合，该研究提高了跟踪和预测行业集群及流动性变化的能力。

Conclusion: 这项研究为交通运输网络安全生态系统的经济规划、劳动力发展和针对性投资提供了数据支持，有助于构建更安全和有韧性的交通网络。

Abstract: The rapid evolution of the transportation cybersecurity ecosystem,
encompassing cybersecurity, automotive, and transportation and logistics
sectors, will lead to the formation of distinct spatial clusters and visitor
flow patterns across the US. This study examines the spatiotemporal dynamics of
visitor flows, analyzing how socioeconomic factors shape industry clustering
and workforce distribution within these evolving sectors. To model and predict
visitor flow patterns, we develop a BiTransGCN framework, integrating an
attention-based Transformer architecture with a Graph Convolutional Network
backbone. By integrating AI-enabled forecasting techniques with spatial
analysis, this study improves our ability to track, interpret, and anticipate
changes in industry clustering and mobility trends, thereby supporting
strategic planning for a secure and resilient transportation network. It offers
a data-driven foundation for economic planning, workforce development, and
targeted investments in the transportation cybersecurity ecosystem.

</details>


### [89] [Toward Fair Federated Learning under Demographic Disparities and Data Imbalance](https://arxiv.org/abs/2505.09295)
*Qiming Wu,Siqi Li,Doudou Zhou,Nan Liu*

Main category: cs.CY

TL;DR: 提出FedIDA框架，该框架结合公平感知正则化和组条件过采样，解决联邦学习中的算法偏见和子组不平衡问题，尤其在多敏感属性交叉时有效。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域应用AI时，公平性至关重要。联邦学习虽能保护隐私，但仍面临算法偏见和数据不平衡问题，尤其是多敏感属性交叉时。

Method: FedIDA结合公平感知正则化和组条件过采样，支持多敏感属性和异构数据分布，且不改变底层联邦学习的收敛性。

Result: 理论分析表明FedIDA能改善公平性；实验证明其在基准和真实临床数据集上均能提升公平性，同时保持预测性能。

Conclusion: FedIDA是一种有效且隐私保护的方法，适用于医疗等需要公平预测的领域。

Abstract: Ensuring fairness is critical when applying artificial intelligence to
high-stakes domains such as healthcare, where predictive models trained on
imbalanced and demographically skewed data risk exacerbating existing
disparities. Federated learning (FL) enables privacy-preserving collaboration
across institutions, but remains vulnerable to both algorithmic bias and
subgroup imbalance - particularly when multiple sensitive attributes intersect.
We propose FedIDA (Fed erated Learning for Imbalance and D isparity A
wareness), a framework-agnostic method that combines fairness-aware
regularization with group-conditional oversampling. FedIDA supports multiple
sensitive attributes and heterogeneous data distributions without altering the
convergence behavior of the underlying FL algorithm. We provide theoretical
analysis establishing fairness improvement bounds using Lipschitz continuity
and concentration inequalities, and show that FedIDA reduces the variance of
fairness metrics across test sets. Empirical results on both benchmark and
real-world clinical datasets confirm that FedIDA consistently improves fairness
while maintaining competitive predictive performance, demonstrating its
effectiveness for equitable and privacy-preserving modeling in healthcare. The
source code is available on GitHub.

</details>


### [90] [Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach](https://arxiv.org/abs/2505.09576)
*Shannon Lodoen,Alexi Orchard*

Main category: cs.CY

TL;DR: 本文探讨了通过强化学习人类反馈（RLHF）优化的生成式AI聊天机器人对人类语言习惯、信息获取及社交关系的潜在伦理和社会影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示RLHF技术如何通过优化语言模型输出使其更“人性化”，以及这种技术对透明度、信任、偏见和人际关系的深远影响。

Method: 通过修辞学分析（尤其是Ian Bogost的程序修辞理论），聚焦RLHF技术如何重塑语言惯例、信息寻求行为和社会关系期望。

Result: 研究发现RLHF可能强化霸权语言使用、延续偏见、使学习去语境化，并侵蚀人际关系。

Conclusion: 结论指出，这一理论探索为AI伦理研究开辟了新方向，呼吁关注AI技术如何通过程序机制影响社会结构和人类互动。

Abstract: Since 2022, versions of generative AI chatbots such as ChatGPT and Claude
have been trained using a specialized technique called Reinforcement Learning
from Human Feedback (RLHF) to fine-tune language model output using feedback
from human annotators. As a result, the integration of RLHF has greatly
enhanced the outputs of these large language models (LLMs) and made the
interactions and responses appear more "human-like" than those of previous
versions using only supervised learning. The increasing convergence of human
and machine-written text has potentially severe ethical, sociotechnical, and
pedagogical implications relating to transparency, trust, bias, and
interpersonal relations. To highlight these implications, this paper presents a
rhetorical analysis of some of the central procedures and processes currently
being reshaped by RLHF-enhanced generative AI chatbots: upholding language
conventions, information seeking practices, and expectations for social
relationships. Rhetorical investigations of generative AI and LLMs have, to
this point, focused largely on the persuasiveness of the content generated.
Using Ian Bogost's concept of procedural rhetoric, this paper shifts the site
of rhetorical investigation from content analysis to the underlying mechanisms
of persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical
investigation opens a new direction for further inquiry in AI ethics that
considers how procedures rerouted through AI-driven technologies might
reinforce hegemonic language use, perpetuate biases, decontextualize learning,
and encroach upon human relationships. It will therefore be of interest to
educators, researchers, scholars, and the growing number of users of generative
AI chatbots.

</details>


### [91] [How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference](https://arxiv.org/abs/2505.09598)
*Nidhal Jegham,Marwen Abdelatti,Lassad Elmoubarki,Abdeltawab Hendawi*

Main category: cs.CY

TL;DR: 本文提出了一种基础设施感知的基准框架，用于量化商业数据中心部署的30种最先进LLM推理的环境足迹，揭示了模型间显著的能耗差异和规模化查询的潜在环境影响。


<details>
  <summary>Details</summary>
Motivation: 由于LLM在各行业的广泛应用，理解其在推理阶段的环境足迹变得至关重要。然而，现有研究多忽视专有模型、基础设施差异，或仅关注训练阶段。本文旨在填补这一空白。

Method: 提出了一种结合公共API性能数据、区域特定环境乘数和硬件配置统计推断的框架，并采用交叉效率数据包络分析（DEA）对模型进行性能与环境成本的排名。

Result: 研究发现，o3和DeepSeek-R1是能耗最高的模型，而Claude-3.7 Sonnet在生态效率上表现最佳。规模化查询（如每日7亿次）会导致显著的能源、淡水和碳排放问题。

Conclusion: 本研究为LLM部署的可持续性提供了标准化、实证支持的方法论，为未来AI开发中的环境责任和可持续性标准奠定了基础。

Abstract: As large language models (LLMs) spread across industries, understanding their
environmental footprint at the inference level is no longer optional; it is
essential. However, most existing studies exclude proprietary models, overlook
infrastructural variability and overhead, or focus solely on training, even as
inference increasingly dominates AI's environmental impact. To bridge this gap,
this paper introduces a novel infrastructure-aware benchmarking framework for
quantifying the environmental footprint of LLM inference across 30
state-of-the-art models as deployed in commercial data centers. Our framework
combines public API performance data with region-specific environmental
multipliers and statistical inference of hardware configurations. We
additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank
models by performance relative to environmental cost. Our results show that o3
and DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33
Wh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and
that Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short
GPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results
in substantial annual environmental impacts. These include electricity use
comparable to 35,000 U.S. homes, freshwater evaporation matching the annual
drinking needs of 1.2 million people, and carbon emissions requiring a
Chicago-sized forest to offset. These findings illustrate a growing paradox:
although individual queries are efficient, their global scale drives
disproportionate resource consumption. Our study provides a standardized,
empirically grounded methodology for benchmarking the sustainability of LLM
deployments, laying a foundation for future environmental accountability in AI
development and sustainability standards.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [92] [Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment](https://arxiv.org/abs/2505.09438)
*Paul Tschisgale,Holger Maus,Fabian Kieser,Ben Kroehs,Stefan Petersen,Peter Wulff*

Main category: physics.ed-ph

TL;DR: 研究了GPT-4o和o1-preview在物理问题解决上的表现，发现它们优于人类参与者，并探讨了LLMs在物理教育中的评估设计影响。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型在物理问题解决上的能力，以指导如何负责任地将其整合到教学和评估中。

Method: 比较了GPT-4o和o1-preview与德国物理奥林匹克参赛者在特定问题上的表现，分析了生成解决方案的特点。

Result: 两种LLMs在物理问题上表现优异，平均超过人类参与者；o1-preview表现最佳。

Conclusion: 研究结果为物理教育中的评估设计提供了参考，强调了维护评估完整性和学生批判性使用LLMs的重要性。

Abstract: Large language models (LLMs) are now widely accessible, reaching learners at
all educational levels. This development has raised concerns that their use may
circumvent essential learning processes and compromise the integrity of
established assessment formats. In physics education, where problem solving
plays a central role in instruction and assessment, it is therefore essential
to understand the physics-specific problem-solving capabilities of LLMs. Such
understanding is key to informing responsible and pedagogically sound
approaches to integrating LLMs into instruction and assessment. This study
therefore compares the problem-solving performance of a general-purpose LLM
(GPT-4o, using varying prompting techniques) and a reasoning-optimized model
(o1-preview) with that of participants of the German Physics Olympiad, based on
a set of well-defined Olympiad problems. In addition to evaluating the
correctness of the generated solutions, the study analyzes characteristic
strengths and limitations of LLM-generated solutions. The findings of this
study indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate
advanced problem-solving capabilities on Olympiad-type physics problems, on
average outperforming the human participants. Prompting techniques had little
effect on GPT-4o's performance, while o1-preview almost consistently
outperformed both GPT-4o and the human benchmark. Based on these findings, the
study discusses implications for the design of summative and formative
assessment in physics education, including how to uphold assessment integrity
and support students in critically engaging with LLMs.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [93] [Equilibrium Propagation for Learning in Lagrangian Dynamical Systems](https://arxiv.org/abs/2505.07363)
*Serge Massar*

Main category: nlin.CD

TL;DR: 提出了一种基于Equilibrium Propagation的方法，用于训练由拉格朗日力学控制的动态系统，适用于周期边界条件或固定初始和最终状态的系统。


<details>
  <summary>Details</summary>
Motivation: 扩展Equilibrium Propagation方法，使其适用于动态轨迹训练，利用作用量极值化原理，避免显式时间反向传播。

Method: 通过轻微调整轨迹至目标，测量与参数共轭变量的响应，实现参数更新，尤其适合周期边界条件或固定状态的系统。

Result: 该方法在周期边界条件下可推导为量子Equilibrium Propagation的半经典极限，并讨论了耗散系统的应用。

Conclusion: 该方法为动态系统提供了一种高效的训练策略，无需复杂的时间反向传播，扩展了Equilibrium Propagation的适用性。

Abstract: We propose a method for training dynamical systems governed by Lagrangian
mechanics using Equilibrium Propagation. Our approach extends Equilibrium
Propagation -- initially developed for energy-based models -- to dynamical
trajectories by leveraging the principle of action extremization. Training is
achieved by gently nudging trajectories toward desired targets and measuring
how the variables conjugate to the parameters to be trained respond. This
method is particularly suited to systems with periodic boundary conditions or
fixed initial and final states, enabling efficient parameter updates without
requiring explicit backpropagation through time. In the case of periodic
boundary conditions, this approach yields the semiclassical limit of Quantum
Equilibrium Propagation. Applications to systems with dissipation are also
discussed.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [94] [RT-cache: Efficient Robot Trajectory Retrieval System](https://arxiv.org/abs/2505.09040)
*Owen Kwon,Abraham George,Alison Bartsch,Amir Barati Farimani*

Main category: cs.RO

TL;DR: RT-cache是一种通过大数据检索和经验学习加速机器人推理的新型轨迹记忆管道，显著降低了推理开销。


<details>
  <summary>Details</summary>
Motivation: 尽管现代视觉-语言-动作（VLA）模型能处理多样化的机器人任务，但其每一步推理成本高，导致显著延迟。RT-cache旨在通过存储和检索成功轨迹来减少这种延迟。

Method: RT-cache结合了记忆构建器和轨迹检索器，存储大规模成功轨迹并检索相关多步骤运动片段，以降低推理成本。该方法可灵活积累现实经验并在场景匹配时重放。

Result: 在Open-X Embodiment数据集和其他现实数据上的实验表明，RT-cache比缺乏检索的基线更快且更成功地完成任务。

Conclusion: RT-cache为实时操控提供了一种实用、数据驱动的解决方案，能快速适应新环境。

Abstract: This paper introduces RT-cache, a novel trajectorymemory pipeline that
accelerates real-world robot inference by leveraging big-data retrieval and
learning from experience. While modern Vision-Language-Action (VLA) models can
handle diverse robotic tasks, they often incur high per-step inference costs,
resulting in significant latency, sometimes minutes per task. In contrast,
RT-cache stores a large-scale Memory of previously successful robot
trajectories and retrieves relevant multistep motion snippets, drastically
reducing inference overhead. By integrating a Memory Builder with a Trajectory
Retrieval, we develop an efficient retrieval process that remains tractable
even for extremely large datasets. RT-cache flexibly accumulates real-world
experiences and replays them whenever the current scene matches past states,
adapting quickly to new or unseen environments with only a few additional
samples. Experiments on the Open-X Embodiment Dataset and other real-world data
demonstrate that RT-cache completes tasks both faster and more successfully
than a baseline lacking retrieval, suggesting a practical, data-driven solution
for real-time manipulation.

</details>


### [95] [Air-Ground Collaboration for Language-Specified Missions in Unknown Environments](https://arxiv.org/abs/2505.09108)
*Fernando Cladera,Zachary Ravichandran,Jason Hughes,Varun Murali,Carlos Nieto-Granda,M. Ani Hsieh,George J. Pappas,Camillo J. Taylor,Vijay Kumar*

Main category: cs.RO

TL;DR: 摘要介绍了一种新型系统，通过自然语言指令指导无人机（UAV）和无人地面车（UGV）协作完成任务，并利用大型语言模型（LLM）进行规划和语义推理。


<details>
  <summary>Details</summary>
Motivation: 随着自主机器人系统的成熟，用户希望通过更高层次的意图（而非低层细节）指定任务，语言作为一种直观且表达丰富的媒介为此提供了可能。但实现语言指导的机器人团队协作面临语义推理、异构机器人协调及间歇通信等挑战。

Method: 研究团队开发了一个基于大型语言模型（LLM）的规划器，用于在实时构建的语义-度量地图上进行推理，并通过机会性共享信息实现空中与地面机器人的协作。系统支持任务驱动的导航，并能动态响应语言指令的变化。

Result: 在城市和乡村环境的实验中，系统成功完成了七种不同的自然语言任务规范，实现了公里级的导航能力，验证了其在异构机器人协作中的有效性。

Conclusion: 该研究首次实现了通过自然语言指导无人机与无人地面车的动态协作，展示了语言模型在机器人任务规划和语义推理中的潜力，为未来智能机器人系统的开发提供了新方向。

Abstract: As autonomous robotic systems become increasingly mature, users will want to
specify missions at the level of intent rather than in low-level detail.
Language is an expressive and intuitive medium for such mission specification.
However, realizing language-guided robotic teams requires overcoming
significant technical hurdles. Interpreting and realizing language-specified
missions requires advanced semantic reasoning. Successful heterogeneous robots
must effectively coordinate actions and share information across varying
viewpoints. Additionally, communication between robots is typically
intermittent, necessitating robust strategies that leverage communication
opportunities to maintain coordination and achieve mission objectives. In this
work, we present a first-of-its-kind system where an unmanned aerial vehicle
(UAV) and an unmanned ground vehicle (UGV) are able to collaboratively
accomplish missions specified in natural language while reacting to changes in
specification on the fly. We leverage a Large Language Model (LLM)-enabled
planner to reason over semantic-metric maps that are built online and
opportunistically shared between an aerial and a ground robot. We consider
task-driven navigation in urban and rural areas. Our system must infer
mission-relevant semantics and actively acquire information via semantic
mapping. In both ground and air-ground teaming experiments, we demonstrate our
system on seven different natural-language specifications at up to
kilometer-scale navigation.

</details>


### [96] [ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control for Delicate, Irregular Bio-products Manipulation](https://arxiv.org/abs/2505.08986)
*Amirreza Davar,Zhengtong Xu,Siavash Mahmoudi,Pouya Sohrabipour,Chaitanya Pallerla,Yu She,Wan Shou,Philip Crandall,Dongyi Wang*

Main category: cs.RO

TL;DR: ChicGrasp是一个软硬件协同设计的系统，用于自动抓取和悬挂家禽尸体。通过条件扩散策略控制器和50次多视角遥操作演示训练，实现了40.6%的成功率，并在38秒内完成一个周期。该系统为农业工程和机器人学习提供了可复现的基准和公开数据集。


<details>
  <summary>Details</summary>
Motivation: 当前家禽处理线依赖人工操作，存在效率低、易损伤等问题。由于家禽尸体的易变形性和解剖学差异，传统方法（如吸盘或固定轨迹）难以适用。

Method: ChicGrasp采用双爪气动夹持器夹住鸡腿，并结合条件扩散策略控制器进行5自由度末端执行器运动规划。控制器通过50次RGB+本体感知的遥操作演示训练。

Result: 系统对单独呈现的生肉鸡尸体实现了40.6%的抓取-悬挂成功率，每个周期耗时38秒，优于现有隐式行为克隆（IBC）和LSTM-GMM基线方法。

Conclusion: ChicGrasp证明了模仿学习能够弥补刚性硬件与生物产品变异性之间的差距，为农业工程和机器人学习提供了新的研究方向和公开数据集。

Abstract: Automated poultry processing lines still rely on humans to lift slippery,
easily bruised carcasses onto a shackle conveyor. Deformability, anatomical
variance, and strict hygiene rules make conventional suction and scripted
motions unreliable. We present ChicGrasp, an end--to--end hardware--software
co-design for this task. An independently actuated dual-jaw pneumatic gripper
clamps both chicken legs, while a conditional diffusion-policy controller,
trained from only 50 multi--view teleoperation demonstrations (RGB +
proprioception), plans 5 DoF end--effector motion, which includes jaw commands
in one shot. On individually presented raw broiler carcasses, our system
achieves a 40.6\% grasp--and--lift success rate and completes the pick to
shackle cycle in 38 s, whereas state--of--the--art implicit behaviour cloning
(IBC) and LSTM-GMM baselines fail entirely. All CAD, code, and datasets will be
open-source. ChicGrasp shows that imitation learning can bridge the gap between
rigid hardware and variable bio--products, offering a reproducible benchmark
and a public dataset for researchers in agricultural engineering and robot
learning.

</details>


### [97] [Imitation Learning for Adaptive Control of a Virtual Soft Exoglove](https://arxiv.org/abs/2505.09099)
*Shirui Lyu,Vittorio Caggiano,Matteo Leonetti,Dario Farina,Letizia Gionfrida*

Main category: cs.RO

TL;DR: 论文提出了一种基于强化学习和生物精确肌肉骨骼模型的定制化可穿戴机器人控制器，用于解决手部肌肉缺损问题，并在手-物体操作任务中提供补偿。通过仿真和人类抓取任务的视频数据训练模型，最终虚拟手套控制器恢复了90.5%的原始操作能力。


<details>
  <summary>Details</summary>
Motivation: 现有可穿戴机器人在康复训练中常忽略患者肌肉损伤的独特性，导致补偿效果不佳。

Method: 结合强化学习和生物精确肌肉骨骼模型，通过仿真和人类抓取视频数据训练定制化控制器，并模拟神经运动损伤以测试补偿效果。

Result: 虚拟可穿戴机器人手套为肌肉力量减弱的手部操纵器提供了共享辅助，控制器恢复了90.5%的原始操作能力。

Conclusion: 提出的定制化控制器能有效补偿特定肌肉缺损，提升康复训练中的操作能力。

Abstract: The use of wearable robots has been widely adopted in rehabilitation training
for patients with hand motor impairments. However, the uniqueness of patients'
muscle loss is often overlooked. Leveraging reinforcement learning and a
biologically accurate musculoskeletal model in simulation, we propose a
customized wearable robotic controller that is able to address specific muscle
deficits and to provide compensation for hand-object manipulation tasks. Video
data of a same subject performing human grasping tasks is used to train a
manipulation model through learning from demonstration. This manipulation model
is subsequently fine-tuned to perform object-specific interaction tasks. The
muscle forces in the musculoskeletal manipulation model are then weakened to
simulate neurological motor impairments, which are later compensated by the
actuation of a virtual wearable robotics glove. Results shows that integrating
the virtual wearable robotic glove provides shared assistance to support the
hand manipulator with weakened muscle forces. The learned exoglove controller
achieved an average of 90.5\% of the original manipulation proficiency.

</details>


### [98] [Deploying Foundation Model-Enabled Air and Ground Robots in the Field: Challenges and Opportunities](https://arxiv.org/abs/2505.09477)
*Zachary Ravichandran,Fernando Cladera,Jason Hughes,Varun Murali,M. Ani Hsieh,George J. Pappas,Camillo J. Taylor,Vijay Kumar*

Main category: cs.RO

TL;DR: 该论文探讨如何将基础模型（FMs）应用于机器人在开放、非结构化环境中的自主操作，并提出了SPINE框架，首次展示了大规模LLM驱动的机器人规划。


<details>
  <summary>Details</summary>
Motivation: 现有FM-enabled机器人主要在封闭环境中运行，无法应对大规模非结构化环境的挑战。论文旨在解决机器人在野外任务中的探索、导航和计算限制问题。

Method: 提出SPINE框架，它是一种LLM-enabled的自主性框架，可适用于不同LLM，并能够蒸馏小型语言模型以适配资源受限的平台。

Result: 首次在非结构化环境中实现了数公里规模的任务规划，并开发了首个基于设备端语言模型的无人机规划器。

Conclusion: 论文总结了当前成果，并提出了未来研究方向，如进一步提升计算效率和适应性。

Abstract: The integration of foundation models (FMs) into robotics has enabled robots
to understand natural language and reason about the semantics in their
environments. However, existing FM-enabled robots primary operate in
closed-world settings, where the robot is given a full prior map or has a full
view of its workspace. This paper addresses the deployment of FM-enabled robots
in the field, where missions often require a robot to operate in large-scale
and unstructured environments. To effectively accomplish these missions, robots
must actively explore their environments, navigate obstacle-cluttered terrain,
handle unexpected sensor inputs, and operate with compute constraints. We
discuss recent deployments of SPINE, our LLM-enabled autonomy framework, in
field robotic settings. To the best of our knowledge, we present the first
demonstration of large-scale LLM-enabled robot planning in unstructured
environments with several kilometers of missions. SPINE is agnostic to a
particular LLM, which allows us to distill small language models capable of
running onboard size, weight and power (SWaP) limited platforms. Via
preliminary model distillation work, we then present the first language-driven
UAV planner using on-device language models. We conclude our paper by proposing
several promising directions for future research.

</details>


### [99] [TransDiffuser: End-to-end Trajectory Generation with Decorrelated Multi-modal Representation for Autonomous Driving](https://arxiv.org/abs/2505.09315)
*Xuefeng Jiang,Yuan Ma,Pengxiang Li,Leimeng Xu,Xin Wen,Kun Zhan,Zhongpu Xia,Peng Jia,XianPeng Lang,Sheng Sun*

Main category: cs.RO

TL;DR: TransDiffuser提出了一种基于扩散模型的端到端自动驾驶轨迹规划方法，通过多模态条件输入和表示去相关优化解决模式崩溃问题，在NAVSIM基准上达到94.85的PDMS。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在多个领域展现出潜力，将其应用于现代自动驾驶系统是一个有前景的方向。

Method: 提出TransDiffuser，一种基于编码器-解码器的生成轨迹规划模型，采用多模态条件输入和表示去相关优化机制。

Result: 在NAVSIM基准上PDMS达到94.85，优于之前的最优方法。

Conclusion: TransDiffuser通过优化多模态表示有效解决模式崩溃问题，为自动驾驶轨迹规划提供了新的解决方案。

Abstract: In recent years, diffusion model has shown its potential across diverse
domains from vision generation to language modeling. Transferring its
capabilities to modern autonomous driving systems has also emerged as a
promising direction.In this work, we propose TransDiffuser, an encoder-decoder
based generative trajectory planning model for end-to-end autonomous driving.
The encoded scene information serves as the multi-modal conditional input of
the denoising decoder. To tackle the mode collapse dilemma in generating
high-quality diverse trajectories, we introduce a simple yet effective
multi-modal representation decorrelation optimization mechanism during the
training process.TransDiffuser achieves PDMS of 94.85 on the NAVSIM benchmark,
surpassing previous state-of-the-art methods without any anchor-based prior
trajectories.

</details>


### [100] [Learning Long-Context Diffusion Policies via Past-Token Prediction](https://arxiv.org/abs/2505.09561)
*Marcel Torne,Andy Tang,Yuejiang Liu,Chelsea Finn*

Main category: cs.RO

TL;DR: 论文提出了一种名为Past-Token Prediction (PTP)的方法，通过预测过去动作令牌来改进长上下文策略的性能，同时引入多阶段训练策略以减少计算开销，最终实验证明该方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 长上下文策略学习面临内存需求高和性能下降的问题，现有方法通常通过截断上下文规避，但可能会丢失关键信息。

Method: 提出PTP任务，预测过去和未来动作令牌以改进时序建模，并采用多阶段训练策略（预训练视觉编码器后微调策略头）降低计算成本。

Result: 实验表明，该方法在4个真实任务和6个模拟任务中将长上下文扩散策略性能提升3倍，训练速度加快10倍以上。

Conclusion: PTP及多阶段训练策略有效解决了长上下文策略学习的挑战，显著提升了性能与效率。

Abstract: Reasoning over long sequences of observations and actions is essential for
many robotic tasks. Yet, learning effective long-context policies from
demonstrations remains challenging. As context length increases, training
becomes increasingly expensive due to rising memory demands, and policy
performance often degrades as a result of spurious correlations. Recent methods
typically sidestep these issues by truncating context length, discarding
historical information that may be critical for subsequent decisions. In this
paper, we propose an alternative approach that explicitly regularizes the
retention of past information. We first revisit the copycat problem in
imitation learning and identify an opposite challenge in recent diffusion
policies: rather than over-relying on prior actions, they often fail to capture
essential dependencies between past and future actions. To address this, we
introduce Past-Token Prediction (PTP), an auxiliary task in which the policy
learns to predict past action tokens alongside future ones. This regularization
significantly improves temporal modeling in the policy head, with minimal
reliance on visual representations. Building on this observation, we further
introduce a multistage training strategy: pre-train the visual encoder with
short contexts, and fine-tune the policy head using cached long-context
embeddings. This strategy preserves the benefits of PTP while greatly reducing
memory and computational overhead. Finally, we extend PTP into a
self-verification mechanism at test time, enabling the policy to score and
select candidates consistent with past actions during inference. Experiments
across four real-world and six simulated tasks demonstrate that our proposed
method improves the performance of long-context diffusion policies by 3x and
accelerates policy training by more than 10x.

</details>


### [101] [Train a Multi-Task Diffusion Policy on RLBench-18 in One Day with One GPU](https://arxiv.org/abs/2505.09430)
*Yutong Hu,Pinhao Song,Kehan Wen,Renaud Detry*

Main category: cs.RO

TL;DR: Mini-Diffuser通过Level-2 minibatching和架构改进，显著减少多任务视觉语言机器人扩散策略的训练时间和内存占用，性能接近当前最佳方法但仅需5%的训练时间和7%的内存。


<details>
  <summary>Details</summary>
Motivation: 现有图像扩散技术直接迁移到机器人动作生成时，因动作空间维度远低于图像空间，导致训练资源浪费。Mini-Diffuser旨在利用这种不对称性优化效率。

Method: 提出Level-2 minibatching策略，将多个噪声动作样本与单个视觉语言条件配对（而非传统一对一）；同时改进扩散变换器架构以防止跨样本信息泄露。

Result: 在RLBench仿真中，达到SOTA方法95%的性能，但训练时间仅需5%，内存占用仅7%。实物实验验证了对多模态动作分布和多样化感知输入的支持。

Conclusion: Mini-Diffuser通过条件-动作维度不对称性创新，实现了高效且高性能的机器人策略训练，为实际部署提供可行性。

Abstract: We present a method for training multi-task vision-language robotic diffusion
policies that reduces training time and memory usage by an order of magnitude.
This improvement arises from a previously underexplored distinction between
action diffusion and the image diffusion techniques that inspired it: image
generation targets are high-dimensional, while robot actions lie in a much
lower-dimensional space. Meanwhile, the vision-language conditions for action
generation remain high-dimensional. Our approach, Mini-Diffuser, exploits this
asymmetry by introducing Level-2 minibatching, which pairs multiple noised
action samples with each vision-language condition, instead of the conventional
one-to-one sampling strategy. To support this batching scheme, we introduce
architectural adaptations to the diffusion transformer that prevent information
leakage across samples while maintaining full conditioning access. In RLBench
simulations, Mini-Diffuser achieves 95\% of the performance of state-of-the-art
multi-task diffusion policies, while using only 5\% of the training time and
7\% of the memory. Real-world experiments further validate that Mini-Diffuser
preserves the key strengths of diffusion-based policies, including the ability
to model multimodal action distributions and produce behavior conditioned on
diverse perceptual inputs. Code available at
github.com/utomm/mini-diffuse-actor.

</details>


### [102] [Distilling Realizable Students from Unrealizable Teachers](https://arxiv.org/abs/2505.09546)
*Yujin Kim,Nathaniel Chin,Arnav Vasudev,Sanjiban Choudhury*

Main category: cs.RO

TL;DR: 论文研究了在特权信息下的策略蒸馏，提出通过学生策略与老师策略的战略互动（必要时查询及恢复状态）来弥补信息不对称，从而提升训练效率和最终性能。


<details>
  <summary>Details</summary>
Motivation: 解决学生策略因无法直接访问老师策略的全部状态信息而面临的分布偏移和策略退化问题，避免现有方法（修改老师策略或依赖学生独立探索）的低效。

Method: 引入两种方法：(i) 适应性决定学生何时应查询老师进行修正的模仿学习，(ii) 选择初始化训练位置以高效探索的强化学习。

Result: 在仿真和真实机器人任务中验证，相比基准方法显著提升训练效率和最终性能。

Conclusion: 通过战略互动弥补信息不对称，为策略蒸馏提供了更高效的解决方案，且在实际任务中表现优异。

Abstract: We study policy distillation under privileged information, where a student
policy with only partial observations must learn from a teacher with full-state
access. A key challenge is information asymmetry: the student cannot directly
access the teacher's state space, leading to distributional shifts and policy
degradation. Existing approaches either modify the teacher to produce
realizable but sub-optimal demonstrations or rely on the student to explore
missing information independently, both of which are inefficient. Our key
insight is that the student should strategically interact with the teacher
--querying only when necessary and resetting from recovery states --to stay on
a recoverable path within its own observation space. We introduce two methods:
(i) an imitation learning approach that adaptively determines when the student
should query the teacher for corrections, and (ii) a reinforcement learning
approach that selects where to initialize training for efficient exploration.
We validate our methods in both simulated and real-world robotic tasks,
demonstrating significant improvements over standard teacher-student baselines
in training efficiency and final performance. The project website is available
at : https://portal-cornell.github.io/CritiQ_ReTRy/

</details>


### [103] [DataMIL: Selecting Data for Robot Imitation Learning with Datamodels](https://arxiv.org/abs/2505.09603)
*Shivin Dass,Alaa Khaddaj,Logan Engstrom,Aleksander Madry,Andrew Ilyas,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: DataMIL是一个基于策略的数据选择框架，通过端到端优化数据选择提升任务性能，避免了人为质量过滤的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人策略在多样化任务上表现良好，但在特定任务上仍需调优，数据选择不当可能降低性能。

Method: 采用DataMIL框架，通过策略驱动的数据选择和替代损失函数，无需昂贵环境测试即可优化数据选择。

Result: 在60余项仿真和实际任务中验证，性能优于基线，尤其是在Open X-Embodiment数据集上表现突出。

Conclusion: 端到端的性能感知数据选择对释放大型数据集潜力至关重要。

Abstract: Recently, the robotics community has amassed ever larger and more diverse
datasets to train generalist robot policies. However, while these policies
achieve strong mean performance across a variety of tasks, they often
underperform on individual, specialized tasks and require further tuning on
newly acquired task-specific data. Combining task-specific data with carefully
curated subsets of large prior datasets via co-training can produce better
specialized policies, but selecting data naively may actually harm downstream
performance. To address this, we introduce DataMIL, a policy-driven data
selection framework built on the datamodels paradigm that reasons about data
selection in an end-to-end manner, using the policy itself to identify which
data points will most improve performance. Unlike standard practices that
filter data using human notions of quality (e.g., based on semantic or visual
similarity), DataMIL directly optimizes data selection for task success,
allowing us to select data that enhance the policy while dropping data that
degrade it. To avoid performing expensive rollouts in the environment during
selection, we use a novel surrogate loss function on task-specific data,
allowing us to use DataMIL in the real world without degrading performance. We
validate our approach on a suite of more than 60 simulation and real-world
manipulation tasks - most notably showing successful data selection from the
Open X-Embodiment datasets-demonstrating consistent gains in success rates and
superior performance over multiple baselines. Our results underscore the
importance of end-to-end, performance-aware data selection for unlocking the
potential of large prior datasets in robotics. More information at
https://robin-lab.cs.utexas.edu/datamodels4imitation/

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [104] [Depth-Based Local Center Clustering: A Framework for Handling Different Clustering Scenarios](https://arxiv.org/abs/2505.09516)
*Siyi Wang,Alexandre Leblanc,Paul D. McNicholas*

Main category: stat.ME

TL;DR: DLCC是一种基于数据深度的局部中心聚类方法，适用于多模态数据，能识别不同形状的簇，并通过新的内部指标评估非凸簇的性能。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法通常针对特定场景设计，存在局限性，特别是对多模态和非凸簇的处理不足。

Method: 提出基于数据深度的局部中心聚类（DLCC），利用局部数据深度识别局部中心和多形状簇，并引入新的密度指标评估性能。

Result: DLCC克服了传统方法的局限性，提升了多模态和非凸簇的分析能力。

Conclusion: DLCC是一种灵活且有效的聚类方法，适用于广泛的应用场景。

Abstract: Cluster analysis, or clustering, plays a crucial role across numerous
scientific and engineering domains. Despite the wealth of clustering methods
proposed over the past decades, each method is typically designed for specific
scenarios and presents certain limitations in practical applications. In this
paper, we propose depth-based local center clustering (DLCC). This novel method
makes use of data depth, which is known to produce a center-outward ordering of
sample points in a multivariate space. However, data depth typically fails to
capture the multimodal characteristics of {data}, something of the utmost
importance in the context of clustering. To overcome this, DLCC makes use of a
local version of data depth that is based on subsets of {data}. From this,
local centers can be identified as well as clusters of varying shapes.
Furthermore, we propose a new internal metric based on density-based clustering
to evaluate clustering performance on {non-convex clusters}. Overall, DLCC is a
flexible clustering approach that seems to overcome some limitations of
traditional clustering methods, thereby enhancing data analysis capabilities
across a wide range of application scenarios.

</details>


### [105] [Scalable Computations for Generalized Mixed Effects Models with Crossed Random Effects Using Krylov Subspace Methods](https://arxiv.org/abs/2505.09552)
*Pascal Kündig,Fabio Sigrist*

Main category: stat.ME

TL;DR: 论文提出了一种基于Krylov子空间的新方法，解决了高维交叉随机效应模型中的计算瓶颈，相比基于Cholesky分解的传统方法，运行时间减少约两个数量级，并在软件实现上显著优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 针对高维交叉随机效应模型中因Cholesky分解导致的计算效率低下问题，作者旨在开发更高效的算法以提升计算速度和稳定性。

Method: 提出了基于Krylov子空间的预处理共轭梯度法和随机Lanczos求积法，分析了不同预处理器的效果，并推导了新的收敛结果，同时优化了预测方差的计算效率。

Result: 实验证明，新方法在模拟和真实数据集上运行时间减少约两个数量级，软件实现速度比lme4和glmmTMB快至10000倍，且更稳定。

Conclusion: 提出的方法显著提升了高维交叉随机效应模型的计算效率，并已集成到开源软件GPBoost中，支持Python和R接口。

Abstract: Mixed effects models are widely used for modeling data with hierarchically
grouped structures and high-cardinality categorical predictor variables.
However, for high-dimensional crossed random effects, current standard
computations relying on Cholesky decompositions can become prohibitively slow.
In this work, we present novel Krylov subspace-based methods that address
several existing computational bottlenecks. Among other things, we
theoretically analyze and empirically evaluate various preconditioners for the
conjugate gradient and stochastic Lanczos quadrature methods, derive new
convergence results, and develop computationally efficient methods for
calculating predictive variances. Extensive experiments using simulated and
real-world data sets show that our proposed methods scale much better than
Cholesky-based computations, for instance, achieving a runtime reduction of
approximately two orders of magnitudes for both estimation and prediction.
Moreover, our software implementation is up to 10'000 times faster and more
stable than state-of-the-art implementations such as lme4 and glmmTMB when
using default settings. Our methods are implemented in the free C++ software
library GPBoost with high-level Python and R packages.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [106] [EDBench: Large-Scale Electron Density Data for Molecular Modeling](https://arxiv.org/abs/2505.09262)
*Hongxin Xiang,Ke Li,Mingquan Liu,Zhixiang Cheng,Bin Yao,Wenjie Du,Jun Xia,Li Zeng,Xin Jin,Xiangxiang Zeng*

Main category: physics.chem-ph

TL;DR: 该论文介绍了EDBench，一个大规模、高质量的电子密度数据集，旨在促进电子尺度的学习研究。通过该数据集，基于学习的方法能够高效计算电子密度，显著降低了传统DFT的计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有分子机器学习力场（MLFFs）通常忽略电子密度（ED）的重要性，而ED对准确理解分子力场至关重要。由于ED计算依赖于耗时的一性原理密度泛函理论（DFT），缺乏大规模ED数据限制了其在MLFFs中的应用。

Method: 基于PCQM4Mv2构建了EDBench数据集，包含330万个分子的精确ED数据，设计了一系列以ED为中心的基准任务（预测、检索和生成），评估模型理解和利用电子信息的能力。

Result: 实验表明，基于EDBench的学习不仅可行，还能达到高精度。基于学习的方法能高效计算ED，计算成本显著低于传统DFT。

Conclusion: EDBench为电子密度驱动的药物发现和材料科学提供了坚实的数据基础，其数据和基准测试将免费开放。

Abstract: Existing molecular machine learning force fields (MLFFs) generally focus on
the learning of atoms, molecules, and simple quantum chemical properties (such
as energy and force), but ignore the importance of electron density (ED)
$\rho(r)$ in accurately understanding molecular force fields (MFFs). ED
describes the probability of finding electrons at specific locations around
atoms or molecules, which uniquely determines all ground state properties (such
as energy, molecular structure, etc.) of interactive multi-particle systems
according to the Hohenberg-Kohn theorem. However, the calculation of ED relies
on the time-consuming first-principles density functional theory (DFT) which
leads to the lack of large-scale ED data and limits its application in MLFFs.
In this paper, we introduce EDBench, a large-scale, high-quality dataset of ED
designed to advance learning-based research at the electronic scale. Built upon
the PCQM4Mv2, EDBench provides accurate ED data, covering 3.3 million
molecules. To comprehensively evaluate the ability of models to understand and
utilize electronic information, we design a suite of ED-centric benchmark tasks
spanning prediction, retrieval, and generation. Our evaluation on several
state-of-the-art methods demonstrates that learning from EDBench is not only
feasible but also achieves high accuracy. Moreover, we show that learning-based
method can efficiently calculate ED with comparable precision while
significantly reducing the computational cost relative to traditional DFT
calculations. All data and benchmarks from EDBench will be freely available,
laying a robust foundation for ED-driven drug discovery and materials science.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [107] [LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries](https://arxiv.org/abs/2505.08842)
*Zekun Wu,Seonglae Cho,Umar Mohammed,Cristian Munoz,Kleyton Costa,Xin Guan,Theo King,Ze Wang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CR

TL;DR: LibVulnWatch是一个基于图的代理评估框架，用于评估开源AI库的多领域风险，包括安全、许可和维护等，并生成可量化的风险评分。


<details>
  <summary>Details</summary>
Motivation: 开源AI库在现代AI系统中至关重要，但其在安全、许可、维护等方面的风险未得到充分研究，亟需一种系统性评估方法。

Method: 基于LangGraph构建，通过代理协作从可信数据源（如代码库、文档）提取并量化风险，生成5个关键领域的评分。

Result: 对20个常用库的评估覆盖了88%的OpenSSF Scorecard检查，发现每库最多19项额外风险，包括RCE漏洞、许可问题等。

Conclusion: LibVulnWatch通过可验证的指标推动了AI技术治理，为供应链风险评估和库选择提供了透明、可扩展的解决方案。

Abstract: Open-source AI libraries are foundational to modern AI systems but pose
significant, underexamined risks across security, licensing, maintenance,
supply chain integrity, and regulatory compliance. We present LibVulnWatch, a
graph-based agentic assessment framework that performs deep, source-grounded
evaluations of these libraries. Built on LangGraph, the system coordinates a
directed acyclic graph of specialized agents to extract, verify, and quantify
risk using evidence from trusted sources such as repositories, documentation,
and vulnerability databases. LibVulnWatch generates reproducible,
governance-aligned scores across five critical domains, publishing them to a
public leaderboard for longitudinal ecosystem monitoring. Applied to 20 widely
used libraries, including ML frameworks, LLM inference engines, and agent
orchestration tools, our system covers up to 88% of OpenSSF Scorecard checks
while uncovering up to 19 additional risks per library. These include critical
Remote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials
(SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in
regulatory documentation and auditability. By translating high-level governance
principles into practical, verifiable metrics, LibVulnWatch advances technical
AI governance with a scalable, transparent mechanism for continuous supply
chain risk assessment and informed library selection.

</details>


### [108] [Security of Internet of Agents: Attacks and Countermeasures](https://arxiv.org/abs/2505.08807)
*Yuntao Wang,Yanghe Pan,Shaolong Guo,Zhou Su*

Main category: cs.CR

TL;DR: 这篇论文综述了物联网代理（IoA）中的安全与隐私问题，包括身份认证威胁、跨代理信任问题、实体安全和隐私风险，并探讨了现有防御机制及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言和视觉语言模型的兴起，AI代理已成为能够感知、推理和决策的自主交互系统，但这也带来了安全和隐私的挑战。论文旨在全面分析IoA系统中的这些问题。

Method: 论文首先概述了IoA架构及其与传统网络不同的脆弱性，然后从四个关键方面（身份认证、跨代理信任、实体安全和隐私风险）深入分析了现有及新兴的防御机制。

Result: 论文总结了当前IoA系统中的主要安全问题和防御措施，并指出了一些未解决的挑战。

Conclusion: 论文提出了未来研究的方向，以促进开发更具弹性和隐私保护的IoA生态系统。

Abstract: With the rise of large language and vision-language models, AI agents have
evolved into autonomous, interactive systems capable of perception, reasoning,
and decision-making. As they proliferate across virtual and physical domains,
the Internet of Agents (IoA) has emerged as a key infrastructure for enabling
scalable and secure coordination among heterogeneous agents. This survey offers
a comprehensive examination of the security and privacy landscape in IoA
systems. We begin by outlining the IoA architecture and its distinct
vulnerabilities compared to traditional networks, focusing on four critical
aspects: identity authentication threats, cross-agent trust issues, embodied
security, and privacy risks. We then review existing and emerging defense
mechanisms and highlight persistent challenges. Finally, we identify open
research directions to advance the development of resilient and
privacy-preserving IoA ecosystems.

</details>


### [109] [MixBridge: Heterogeneous Image-to-Image Backdoor Attack through Mixture of Schrödinger Bridges](https://arxiv.org/abs/2505.08809)
*Shixi Qin,Zhiyong Yang,Shilong Bao,Shi Wang,Qianqian Xu,Qingming Huang*

Main category: cs.CR

TL;DR: 本文提出MixBridge，一种新的扩散Schrödinger桥（DSB）框架，支持任意输入分布的多重异构后门触发器植入，并通过Divide-and-Merge策略和权重重分配方案（WRS）优化性能与隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有后门植入方法主要针对单攻击场景且限于高斯噪声输入模型，无法满足复杂输入分布需求，因此本文旨在填补这一空白。

Method: 提出MixBridge框架，直接利用毒化图像对训练植入后门触发器，无需修改随机微分方程；通过Divide-and-Merge策略独立预训练并合并模型，结合WRS提升隐蔽性。

Result: 理论分析显示多触发器训练会导致模型表现冲突，但Divide-and-Merge和WRS有效解决了这一问题，实验验证了MixBridge在多样化生成任务中的有效性。

Conclusion: MixBridge为桥模型后门行为研究提供了灵活工具，通过创新方法解决了多触发器植入的冲突与隐蔽性问题，具有实际应用潜力。

Abstract: This paper focuses on implanting multiple heterogeneous backdoor triggers in
bridge-based diffusion models designed for complex and arbitrary input
distributions. Existing backdoor formulations mainly address single-attack
scenarios and are limited to Gaussian noise input models. To fill this gap, we
propose MixBridge, a novel diffusion Schr\"odinger bridge (DSB) framework to
cater to arbitrary input distributions (taking I2I tasks as special cases).
Beyond this trait, we demonstrate that backdoor triggers can be injected into
MixBridge by directly training with poisoned image pairs. This eliminates the
need for the cumbersome modifications to stochastic differential equations
required in previous studies, providing a flexible tool to study backdoor
behavior for bridge models. However, a key question arises: can a single DSB
model train multiple backdoor triggers? Unfortunately, our theory shows that
when attempting this, the model ends up following the geometric mean of benign
and backdoored distributions, leading to performance conflict across backdoor
tasks. To overcome this, we propose a Divide-and-Merge strategy to mix
different bridges, where models are independently pre-trained for each specific
objective (Divide) and then integrated into a unified model (Merge). In
addition, a Weight Reallocation Scheme (WRS) is also designed to enhance the
stealthiness of MixBridge. Empirical studies across diverse generation tasks
speak to the efficacy of MixBridge.

</details>


### [110] [Machine Learning-Based Detection of DDoS Attacks in VANETs for Emergency Vehicle Communication](https://arxiv.org/abs/2505.08810)
*Bappa Muktar,Vincent Fono,Adama Nouboukpo*

Main category: cs.CR

TL;DR: 该研究提出了一种用于检测高速公路VANET中DDoS攻击的框架，通过仿真和真实数据验证，XGBoost和CatBoost表现最佳，F1分数达96%。


<details>
  <summary>Details</summary>
Motivation: VANET在智能交通系统中至关重要，但DDoS攻击会威胁其可靠性，因此需要一种高效的检测方法。

Method: 研究使用NS-3和SUMO构建合成数据集，结合真实交通数据，通过预处理和SMOTE平衡类别，使用SHAP评估特征重要性，并比较了11种分类器。

Result: XGBoost和CatBoost表现最佳，F1分数达96%，验证了框架的鲁棒性。

Conclusion: 该框架有望实时部署于VANET，保障关键应急通信的安全性。

Abstract: Vehicular Ad Hoc Networks (VANETs) play a key role in Intelligent
Transportation Systems (ITS), particularly in enabling real-time communication
for emergency vehicles. However, Distributed Denial of Service (DDoS) attacks,
which interfere with safety-critical communication channels, can severely
impair their reliability. This study introduces a robust and scalable framework
to detect DDoS attacks in highway-based VANET environments. A synthetic dataset
was constructed using Network Simulator 3 (NS-3) in conjunction with the
Simulation of Urban Mobility (SUMO) and further enriched with real-world
mobility traces from Germany's A81 highway, extracted via OpenStreetMap (OSM).
Three traffic categories were simulated: DDoS, VoIP, and TCP-based video
streaming (VideoTCP). The data preprocessing pipeline included normalization,
signal-to-noise ratio (SNR) feature engineering, missing value imputation, and
class balancing using the Synthetic Minority Over-sampling Technique (SMOTE).
Feature importance was assessed using SHapley Additive exPlanations (SHAP).
Eleven classifiers were benchmarked, among them XGBoost (XGB), CatBoost (CB),
AdaBoost (AB), GradientBoosting (GB), and an Artificial Neural Network (ANN).
XGB and CB achieved the best performance, each attaining an F1-score of 96%.
These results highlight the robustness of the proposed framework and its
potential for real-time deployment in VANETs to secure critical emergency
communications.

</details>


### [111] [Federated Large Language Models: Feasibility, Robustness, Security and Future Directions](https://arxiv.org/abs/2505.08830)
*Wenhao Jiang,Yuchuan Luo,Guilin Deng,Silong Chen,Xu Yang,Shihong Wu,Xinwen Gao,Lin Liu,Shaojing Fu*

Main category: cs.CR

TL;DR: 本文综述了联邦大型语言模型（FLLM）的最新进展，聚焦其可行性、鲁棒性、安全性及未来方向。内容涵盖现有研究的全面调查、提升鲁棒性的方法、隐私与安全风险的识别与防御机制探讨，以及诸如少样本学习、机器反学习和IP保护等未来研究方向。作者强调需进一步研究以增强系统鲁棒性和安全性，解决联邦学习（FL）与大型语言模型（LLM）整合的独特挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）与大型语言模型（LLM）的结合为解决分布式数据联合训练中的隐私和数据孤岛问题提供了潜在方案。然而，这种新兴的联邦大型语言模型（FLLM）领域面临通信、计算开销、异构性及隐私安全等显著挑战。现有研究主要关注FLLM的可行性，未来趋势预计将侧重于增强系统的鲁棒性和安全性。

Method: 论文通过对FLLM现有研究的全面调查，从可行性、鲁棒性、安全性和未来方向四个关键视角进行了系统梳理。作者总结了提升FLLM鲁棒性的方法（如应对资源、数据和任务异构性），分析了隐私和安全的潜在风险，并回顾了最新的防御机制。

Result: 研究表明，FLLM的整合虽具潜力，但面临资源、数据异构性和安全威胁等多重挑战。当前防御机制仍需改进，而未来方向（如少样本学习、机器反学习等）为FLLM的发展提供了新思路。

Conclusion: 本综述强调，需进一步研究以优化FLLM的鲁棒性和安全性，同时应对FL与LLM整合的特有问题。未来在少样本学习、机器反学习及IP保护等领域的探索将推动该领域的持续发展。

Abstract: The integration of Large Language Models (LLMs) and Federated Learning (FL)
presents a promising solution for joint training on distributed data while
preserving privacy and addressing data silo issues. However, this emerging
field, known as Federated Large Language Models (FLLM), faces significant
challenges, including communication and computation overheads, heterogeneity,
privacy and security concerns. Current research has primarily focused on the
feasibility of FLLM, but future trends are expected to emphasize enhancing
system robustness and security. This paper provides a comprehensive review of
the latest advancements in FLLM, examining challenges from four critical
perspectives: feasibility, robustness, security, and future directions. We
present an exhaustive survey of existing studies on FLLM feasibility, introduce
methods to enhance robustness in the face of resource, data, and task
heterogeneity, and analyze novel risks associated with this integration,
including privacy threats and security challenges. We also review the latest
developments in defense mechanisms and explore promising future research
directions, such as few-shot learning, machine unlearning, and IP protection.
This survey highlights the pressing need for further research to enhance system
robustness and security while addressing the unique challenges posed by the
integration of FL and LLM.

</details>


### [112] [Robustness Analysis against Adversarial Patch Attacks in Fully Unmanned Stores](https://arxiv.org/abs/2505.08835)
*Hyunsik Na,Wonho Lee,Seungdeok Roh,Sohee Park,Daeseon Choi*

Main category: cs.CR

TL;DR: 该研究揭示了人工智能无人商店中物体检测模型易受对抗性补丁攻击的漏洞，提出了三种攻击方式并引入新的评估指标，同时在物理环境中验证了攻击效果，强调了防御策略的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着无人商店的普及，其依赖的AI系统面临对抗性攻击的威胁，可能导致盗窃和库存问题，因此研究这些漏洞及防御措施至关重要。

Method: 通过数字和物理环境测试三种对抗性补丁攻击（隐藏、创建、修改），并引入新的颜色直方图相似性损失函数和边界框评估指标，评估攻击效果。

Result: 攻击在数字和物理环境中均有效，尤其在黑盒场景下通过影子攻击提升成功率，凸显了当前防御机制的不足。

Conclusion: 研究强调需开发更鲁棒的防御策略以保护无人商店，并提出了改进物体检测模型和增强零售环境安全性的方向。

Abstract: The advent of convenient and efficient fully unmanned stores equipped with
artificial intelligence-based automated checkout systems marks a new era in
retail. However, these systems have inherent artificial intelligence security
vulnerabilities, which are exploited via adversarial patch attacks,
particularly in physical environments. This study demonstrated that adversarial
patches can severely disrupt object detection models used in unmanned stores,
leading to issues such as theft, inventory discrepancies, and interference. We
investigated three types of adversarial patch attacks -- Hiding, Creating, and
Altering attacks -- and highlighted their effectiveness. We also introduce the
novel color histogram similarity loss function by leveraging attacker knowledge
of the color information of a target class object. Besides the traditional
confusion-matrix-based attack success rate, we introduce a new
bounding-boxes-based metric to analyze the practical impact of these attacks.
Starting with attacks on object detection models trained on snack and fruit
datasets in a digital environment, we evaluated the effectiveness of
adversarial patches in a physical testbed that mimicked a real unmanned store
with RGB cameras and realistic conditions. Furthermore, we assessed the
robustness of these attacks in black-box scenarios, demonstrating that shadow
attacks can enhance success rates of attacks even without direct access to
model parameters. Our study underscores the necessity for robust defense
strategies to protect unmanned stores from adversarial threats. Highlighting
the limitations of the current defense mechanisms in real-time detection
systems and discussing various proactive measures, we provide insights into
improving the robustness of object detection models and fortifying unmanned
retail environments against these attacks.

</details>


### [113] [On the interplay of Explainability, Privacy and Predictive Performance with Explanation-assisted Model Extraction](https://arxiv.org/abs/2505.08847)
*Fatima Ezzeddine,Rinad Akel,Ihab Sbeity,Silvia Giordano,Marc Langheinrich,Omran Ayoub*

Main category: cs.CR

TL;DR: 论文研究了在使用差分隐私（DP）技术时，机器学习即服务（MLaaS）平台在模型性能、隐私和可解释性之间的权衡，重点关注如何通过两种不同的DP策略来缓解基于反事实解释（CFs）的模型提取攻击（MEA）。


<details>
  <summary>Details</summary>
Motivation: 随着可解释AI（XAI）在MLaaS中的集成，攻击者可能利用反事实解释（CFs）进行模型提取攻击（MEA），因此需要研究如何平衡模型性能、隐私和可解释性。

Method: 论文评估了两种差分隐私（DP）策略：一种在分类模型训练时实施，另一种在解释器生成反事实解释（CFs）时实施。

Result: 研究分析了这两种DP策略对模型性能、隐私保护和可解释性的影响。

Conclusion: 论文提出了在MLaaS中应用差分隐私以缓解基于反事实解释的模型提取攻击的可能途径，并展示了不同策略的权衡效果。

Abstract: Machine Learning as a Service (MLaaS) has gained important attraction as a
means for deploying powerful predictive models, offering ease of use that
enables organizations to leverage advanced analytics without substantial
investments in specialized infrastructure or expertise. However, MLaaS
platforms must be safeguarded against security and privacy attacks, such as
model extraction (MEA) attacks. The increasing integration of explainable AI
(XAI) within MLaaS has introduced an additional privacy challenge, as attackers
can exploit model explanations particularly counterfactual explanations (CFs)
to facilitate MEA. In this paper, we investigate the trade offs among model
performance, privacy, and explainability when employing Differential Privacy
(DP), a promising technique for mitigating CF facilitated MEA. We evaluate two
distinct DP strategies: implemented during the classification model training
and at the explainer during CF generation.

</details>


### [114] [Improved Algorithms for Differentially Private Language Model Alignment](https://arxiv.org/abs/2505.08849)
*Keyu Chen,Hao Tang,Qinglin Liu,Yizhao Xu*

Main category: cs.CR

TL;DR: 提出了一种隐私保护的语言模型对齐框架，结合DPO和RLHF，在隐私预算适中时表现优于现有方法，对齐质量提升15%。


<details>
  <summary>Details</summary>
Motivation: 语言模型对齐常涉及敏感数据，现有结合差分隐私的方法性能有限，需改进隐私保护和性能平衡。

Method: 提出新算法（如DP-AdamW）并与DPO和RLHF结合，系统评估不同隐私预算下的效果与计算成本。

Result: DP-AdamW+DPO在ε=2-5时，对齐质量提升15%，且在隐私保障、对齐效果与计算需求间提供了优化指导。

Conclusion: 新框架在隐私保护与性能间取得平衡，为实际部署提供可行方案。

Abstract: Language model alignment is crucial for ensuring that large language models
(LLMs) align with human preferences, yet it often involves sensitive user data,
raising significant privacy concerns. While prior work has integrated
differential privacy (DP) with alignment techniques, their performance remains
limited. In this paper, we propose novel algorithms for privacy-preserving
alignment and rigorously analyze their effectiveness across varying privacy
budgets and models. Our framework can be deployed on two celebrated alignment
techniques, namely direct preference optimization (DPO) and reinforcement
learning from human feedback (RLHF). Through systematic experiments on
large-scale language models, we demonstrate that our approach achieves
state-of-the-art performance. Notably, one of our algorithms, DP-AdamW,
combined with DPO, surpasses existing methods, improving alignment quality by
up to 15% under moderate privacy budgets ({\epsilon}=2-5). We further
investigate the interplay between privacy guarantees, alignment efficacy, and
computational demands, providing practical guidelines for optimizing these
trade-offs.

</details>


### [115] [Optimized Couplings for Watermarking Large Language Models](https://arxiv.org/abs/2505.08878)
*Dor Tsur,Carol Xuan Long,Claudio Mayrink Verdun,Hsiang Hsu,Haim Permuter,Flavio P. Calmon*

Main category: cs.CR

TL;DR: 本文分析了大型语言模型（LLM）生成文本的水印技术，提出了一种在单次设置下的水印方法，通过假设检验和侧信息优化检测能力与文本质量之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成文本越来越难以与人类生成内容区分，需要开发水印技术以在不显著改变输出质量的情况下标记LLM生成内容。

Method: 研究通过假设检验和侧信息的视角，提出了一种将检测器共享的侧信息与LLM词汇表随机分区耦合的水印设计方法。

Result: 研究得出了在最小熵约束下的最优耦合和随机化策略，并通过数值实验验证了该方法在合成数据和实际LLM水印中的优越性。

Conclusion: 论文提出的水印方案在检测能力和文本质量之间实现了最优权衡，并通过实验验证了其优于现有方法的性能。

Abstract: Large-language models (LLMs) are now able to produce text that is, in many
cases, seemingly indistinguishable from human-generated content. This has
fueled the development of watermarks that imprint a ``signal'' in LLM-generated
text with minimal perturbation of an LLM's output. This paper provides an
analysis of text watermarking in a one-shot setting. Through the lens of
hypothesis testing with side information, we formulate and analyze the
fundamental trade-off between watermark detection power and distortion in
generated textual quality. We argue that a key component in watermark design is
generating a coupling between the side information shared with the watermark
detector and a random partition of the LLM vocabulary. Our analysis identifies
the optimal coupling and randomization strategy under the worst-case LLM
next-token distribution that satisfies a min-entropy constraint. We provide a
closed-form expression of the resulting detection rate under the proposed
scheme and quantify the cost in a max-min sense. Finally, we provide an array
of numerical results, comparing the proposed scheme with the theoretical
optimum and existing schemes, in both synthetic data and LLM watermarking. Our
code is available at https://github.com/Carol-Long/CC_Watermark

</details>


### [116] [TokenProber: Jailbreaking Text-to-image Models via Fine-grained Word Impact Analysis](https://arxiv.org/abs/2505.08804)
*Longtian Wang,Xiaofei Xie,Tianlin Li,Yuhan Zhi,Chao Shen*

Main category: cs.CR

TL;DR: TokenProber 是一种敏感性感知差异测试方法，用于评估文本到图像（T2I）模型中的拒绝机制鲁棒性，通过生成对抗性提示绕过安全检查器。


<details>
  <summary>Details</summary>
Motivation: 为了解决文本到图像（T2I）模型可能生成不安全内容的问题，本文旨在测试现有拒绝机制的鲁棒性，对抗性提示技术的关键挑战在于保留敏感内容同时绕过安全检查。

Method: TokenProber 提出一种敏感性感知变异方法，区分关键脏词与差异性词汇，利用 T2I 模型与安全检查器对敏感性评估的差异生成对抗性提示。

Result: 在 5 个安全检查器与 3 个 T2I 模型上测试显示，TokenProber 比现有方法平均提升 54%+ 的绕过成功率，显著揭露拒绝机制的鲁棒性问题。

Conclusion: TokenProber 成功暴露了 T2I 模型中拒绝机制的弱点，为未来安全机制的设计提供了重要启示。

Abstract: Text-to-image (T2I) models have significantly advanced in producing
high-quality images. However, such models have the ability to generate images
containing not-safe-for-work (NSFW) content, such as pornography, violence,
political content, and discrimination. To mitigate the risk of generating NSFW
content, refusal mechanisms, i.e., safety checkers, have been developed to
check potential NSFW content. Adversarial prompting techniques have been
developed to evaluate the robustness of the refusal mechanisms. The key
challenge remains to subtly modify the prompt in a way that preserves its
sensitive nature while bypassing the refusal mechanisms. In this paper, we
introduce TokenProber, a method designed for sensitivity-aware differential
testing, aimed at evaluating the robustness of the refusal mechanisms in T2I
models by generating adversarial prompts. Our approach is based on the key
observation that adversarial prompts often succeed by exploiting discrepancies
in how T2I models and safety checkers interpret sensitive content. Thus, we
conduct a fine-grained analysis of the impact of specific words within prompts,
distinguishing between dirty words that are essential for NSFW content
generation and discrepant words that highlight the different sensitivity
assessments between T2I models and safety checkers. Through the
sensitivity-aware mutation, TokenProber generates adversarial prompts, striking
a balance between maintaining NSFW content generation and evading detection.
Our evaluation of TokenProber against 5 safety checkers on 3 popular T2I
models, using 324 NSFW prompts, demonstrates its superior effectiveness in
bypassing safety filters compared to existing methods (e.g., 54%+ increase on
average), highlighting TokenProber's ability to uncover robustness issues in
the existing refusal mechanisms.

</details>


### [117] [Self-Supervised Transformer-based Contrastive Learning for Intrusion Detection Systems](https://arxiv.org/abs/2505.08816)
*Ippokratis Koukoulis,Ilias Syrigos,Thanasis Korakis*

Main category: cs.CR

TL;DR: 该论文提出了一种基于Transformer的自监督对比学习方法，用于从原始数据包序列中学习通用入侵检测特征，显著提升了异常检测和监督学习的性能，尤其是在跨数据集评估中表现突出。


<details>
  <summary>Details</summary>
Motivation: 随着零日攻击频率和严重性的增加，传统的基于签名的入侵检测系统（IDS）已无法满足需求。机器学习方法虽能识别攻击模式，但对标注数据和未见过流量模式的泛化能力有限。

Method: 采用自监督对比学习框架，基于Transformer编码器自动学习原始数据包序列的表示，无需依赖手工统计特征（如NetFlow），并结合数据增强策略。

Result: 在异常检测任务中，相比现有的NetFlow自监督方法，AUC提升3%（数据集内）和20%（跨数据集）。在监督学习中，使用有限标注数据时AUC提高1.5%。

Conclusion: 提出的Transformer框架不仅性能优于现有方法，还展示了在目标域缺乏良性数据时的强适应能力，为通用入侵检测提供了有效解决方案。

Abstract: As the digital landscape becomes more interconnected, the frequency and
severity of zero-day attacks, have significantly increased, leading to an
urgent need for innovative Intrusion Detection Systems (IDS). Machine
Learning-based IDS that learn from the network traffic characteristics and can
discern attack patterns from benign traffic offer an advanced solution to
traditional signature-based IDS. However, they heavily rely on labeled
datasets, and their ability to generalize when encountering unseen traffic
patterns remains a challenge. This paper proposes a novel self-supervised
contrastive learning approach based on transformer encoders, specifically
tailored for generalizable intrusion detection on raw packet sequences. Our
proposed learning scheme employs a packet-level data augmentation strategy
combined with a transformer-based architecture to extract and generate
meaningful representations of traffic flows. Unlike traditional methods reliant
on handcrafted statistical features (NetFlow), our approach automatically
learns comprehensive packet sequence representations, significantly enhancing
performance in anomaly identification tasks and supervised learning for
intrusion detection. Our transformer-based framework exhibits better
performance in comparison to existing NetFlow self-supervised methods.
Specifically, we achieve up to a 3% higher AUC in anomaly detection for
intra-dataset evaluation and up to 20% higher AUC scores in inter-dataset
evaluation. Moreover, our model provides a strong baseline for supervised
intrusion detection with limited labeled data, exhibiting an improvement over
self-supervised NetFlow models of up to 1.5% AUC when pretrained and evaluated
on the same dataset. Additionally, we show the adaptability of our pretrained
model when fine-tuned across different datasets, demonstrating strong
performance even when lacking benign data from the target domain.

</details>


### [118] [Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning](https://arxiv.org/abs/2505.08837)
*Muhammad Saqib,Dipkumar Mehta,Fnu Yashu,Shubham Malhotra*

Main category: cs.CR

TL;DR: 该论文提出了一种基于强化学习（RL）的动态安全策略管理框架，通过深度Q网络和近端策略优化算法，动态调整云环境中的安全策略，显著提高了入侵检测率和响应效率。


<details>
  <summary>Details</summary>
Motivation: 静态安全策略在动态云环境中表现不佳，无法适应威胁的快速变化和资源的弹性需求。为了解决这一问题，作者提出利用RL技术动态优化安全策略。

Method: 使用深度强化学习算法（如深度Q网络和近端策略优化），结合云遥测数据（如AWS Cloud Trail日志、网络流量数据和威胁情报）持续优化防火墙规则和IAM策略。

Result: 实验表明，该框架入侵检测率达92%（静态策略为82%），事件检测和响应时间减少58%，同时保持高合规性和资源效率。

Conclusion: 自适应强化学习方法能有效提升云安全策略管理的效果，验证了其在动态云环境中的实用性。

Abstract: The security of cloud environments, such as Amazon Web Services (AWS), is
complex and dynamic. Static security policies have become inadequate as threats
evolve and cloud resources exhibit elasticity [1]. This paper addresses the
limitations of static policies by proposing a security policy management
framework that uses reinforcement learning (RL) to adapt dynamically.
Specifically, we employ deep reinforcement learning algorithms, including deep
Q Networks and proximal policy optimization, enabling the learning and
continuous adjustment of controls such as firewall rules and Identity and
Access Management (IAM) policies. The proposed RL based solution leverages
cloud telemetry data (AWS Cloud Trail logs, network traffic data, threat
intelligence feeds) to continuously refine security policies, maximizing threat
mitigation, and compliance while minimizing resource impact. Experimental
results demonstrate that our adaptive RL based framework significantly
outperforms static policies, achieving higher intrusion detection rates (92%
compared to 82% for static policies) and substantially reducing incident
detection and response times by 58%. In addition, it maintains high conformity
with security requirements and efficient resource usage. These findings
validate the effectiveness of adaptive reinforcement learning approaches in
improving cloud security policy management.

</details>


### [119] [Evaluating the Robustness of Adversarial Defenses in Malware Detection Systems](https://arxiv.org/abs/2505.09342)
*Mostafa Jafari,Alireza Shameli-Sendi*

Main category: cs.CR

TL;DR: 该论文提出了一种优先二进制舍入技术和sigma-binary攻击方法，用于二进制特征空间中的Android恶意软件检测防御，实验表明新攻击方法能高效绕过现有防御。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的Android恶意软件检测器易受逃避攻击，缺乏对二进制约束域防御鲁棒性的全面评估框架。

Method: 引入了优先二进制舍入技术将连续扰动转换为二进制特征空间，并提出sigma-binary攻击方法，旨在以最少特征修改实现攻击目标。

Result: 在Malscan数据集上的实验显示，sigma-binary攻击优于现有方法，攻击成功率在少量特征修改下可达90%以上，甚至100%。

Conclusion: 该研究表明需采用精确方法如sigma-binary暴露现有防御的潜在漏洞，以支持开发更鲁棒的恶意软件检测系统。

Abstract: Machine learning is a key tool for Android malware detection, effectively
identifying malicious patterns in apps. However, ML-based detectors are
vulnerable to evasion attacks, where small, crafted changes bypass detection.
Despite progress in adversarial defenses, the lack of comprehensive evaluation
frameworks in binary-constrained domains limits understanding of their
robustness. We introduce two key contributions. First, Prioritized Binary
Rounding, a technique to convert continuous perturbations into binary feature
spaces while preserving high attack success and low perturbation size. Second,
the sigma-binary attack, a novel adversarial method for binary domains,
designed to achieve attack goals with minimal feature changes. Experiments on
the Malscan dataset show that sigma-binary outperforms existing attacks and
exposes key vulnerabilities in state-of-the-art defenses. Defenses equipped
with adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant
brittleness, with attack success rates exceeding 90% using fewer than 10
feature modifications and reaching 100% with just 20. Adversarially trained
defenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small
budgets but remains vulnerable to unrestricted perturbations, with attack
success rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates
strong robustness against state-of-the-art gradient-based adversarial attacks
by maintaining an attack success rate below 16.55%, the sigma-binary attack
significantly outperforms these methods, achieving a 94.56% success rate under
unrestricted perturbations. These findings highlight the critical need for
precise method like sigma-binary to expose hidden vulnerabilities in existing
defenses and support the development of more resilient malware detection
systems.

</details>


### [120] [Toward Malicious Clients Detection in Federated Learning](https://arxiv.org/abs/2505.09110)
*Zhihao Dou,Jiaqi Wang,Wei Sun,Zhuqing Liu,Minghong Fang*

Main category: cs.CR

TL;DR: 论文提出了一种名为SafeFL的新算法，旨在在联邦学习中准确识别恶意客户端，通过生成合成数据集区分恶意与良性模型，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的去中心化特性使其易受毒化攻击，现有防御方法对高级威胁无效或误判率高，亟需更准确的检测技术。

Method: 提出SafeFL算法，服务器收集一系列全局模型生成合成数据集，据此通过模型行为识别恶意客户端。

Result: 大量测试显示SafeFL在检测恶意客户端方面效率和准确性均优于现有方法。

Conclusion: SafeFL有效解决了联邦学习中恶意客户端检测的准确性不足问题，提升了防御能力。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train a
global machine learning model without sharing their raw data. However, the
decentralized nature of FL introduces vulnerabilities, particularly to
poisoning attacks, where malicious clients manipulate their local models to
disrupt the training process. While Byzantine-robust aggregation rules have
been developed to mitigate such attacks, they remain inadequate against more
advanced threats. In response, recent advancements have focused on FL detection
techniques to identify potentially malicious participants. Unfortunately, these
methods often misclassify numerous benign clients as threats or rely on
unrealistic assumptions about the server's capabilities. In this paper, we
propose a novel algorithm, SafeFL, specifically designed to accurately identify
malicious clients in FL. The SafeFL approach involves the server collecting a
series of global models to generate a synthetic dataset, which is then used to
distinguish between malicious and benign models based on their behavior.
Extensive testing demonstrates that SafeFL outperforms existing methods,
offering superior efficiency and accuracy in detecting malicious clients.

</details>


### [121] [Detecting Sybil Addresses in Blockchain Airdrops: A Subgraph-based Feature Propagation and Fusion Approach](https://arxiv.org/abs/2505.09313)
*Qiangqiang Liu,Qian Huang,Frank Fan,Haishan Wu,Xueyan Tang*

Main category: cs.CR

TL;DR: 本文提出了一种基于子图特征提取的轻量级GBM的Sybil地址识别方法，通过提取时间、金额和网络结构特征，显著提升了识别精度，所有指标超过0.9。


<details>
  <summary>Details</summary>
Motivation: Sybil攻击对区块链生态构成严重威胁，特别是在代币空投活动中，亟需高效识别方法以保障安全和公平。

Method: 构建双层交易子图，提取Sybil地址生命周期的关键操作特征（如首次交易时间、首次获取Gas时间等），并结合金额和网络结构特征进行综合分析。

Result: 在包含193,701个地址（含23,240个Sybil地址）的数据集上，精确率、召回率、F1分数和AUC均超过0.9，优于现有方法。

Conclusion: 该方法可扩展至交易操纵识别和流动性风险评估等领域，有助于构建更安全公平的区块链生态。

Abstract: Sybil attacks pose a significant security threat to blockchain ecosystems,
particularly in token airdrop events. This paper proposes a novel sybil address
identification method based on subgraph feature extraction lightGBM. The method
first constructs a two-layer deep transaction subgraph for each address, then
extracts key event operation features according to the lifecycle of sybil
addresses, including the time of first transaction, first gas acquisition,
participation in airdrop activities, and last transaction. These temporal
features effectively capture the consistency of sybil address behavior
operations. Additionally, the method extracts amount and network structure
features, comprehensively describing address behavior patterns and network
topology through feature propagation and fusion. Experiments conducted on a
dataset containing 193,701 addresses (including 23,240 sybil addresses) show
that this method outperforms existing approaches in terms of precision, recall,
F1 score, and AUC, with all metrics exceeding 0.9. The methods and results of
this study can be further applied to broader blockchain security areas such as
transaction manipulation identification and token liquidity risk assessment,
contributing to the construction of a more secure and fair blockchain
ecosystem.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [122] [Probabilistic Wind Power Forecasting via Non-Stationary Gaussian Processes](https://arxiv.org/abs/2505.09026)
*Domniki Ladopoulou,Dat Minh Hong,Petros Dellaportas*

Main category: stat.AP

TL;DR: 提出了一种基于非平稳高斯过程的广义谱混合核模型，以改进风电功率概率预测，尤其在短期预测中表现优异


<details>
  <summary>Details</summary>
Motivation: 为提高电网稳定性并高效整合可再生能源，需要准确的风电功率概率预测。传统高斯过程采用平稳核，无法有效捕捉风电数据的非平稳特性

Method: 开发了一种非平稳高斯过程框架，结合广义谱混合核（GSM），用于建模风电数据的时变模式和异方差行为

Result: 在真实SCADA数据上的测试表明，GSM核模型在短、中、长期预测中均优于传统径向基函数和谱混合核，尤其在短期预测中表现突出

Conclusion: 非平稳性建模对风电预测至关重要，非平稳高斯过程模型在实际操作中具有显著价值

Abstract: Accurate probabilistic forecasting of wind power is essential for maintaining
grid stability and enabling efficient integration of renewable energy sources.
Gaussian Process (GP) models offer a principled framework for quantifying
uncertainty; however, conventional approaches rely on stationary kernels, which
are inadequate for modeling the inherently non-stationary nature of wind speed
and power output. We propose a non-stationary GP framework that incorporates
the generalized spectral mixture (GSM) kernel, enabling the model to capture
time-varying patterns and heteroscedastic behaviors in wind speed and wind
power data. We evaluate the performance of the proposed model on real-world
SCADA data across short\mbox{-,} medium-, and long-term forecasting horizons.
Compared to standard radial basis function and spectral mixture kernels, the
GSM-based model outperforms, particularly in short-term forecasts. These
results highlight the necessity of modeling non-stationarity in wind power
forecasting and demonstrate the practical value of non-stationary GP models in
operational settings.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [123] [ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor](https://arxiv.org/abs/2505.09142)
*Seungbeom Choi,Jeonghoe Goo,Eunjoo Jeon,Mingyu Yang,Minsung Jang*

Main category: cs.DC

TL;DR: ELIS是一种针对大型语言模型(LLM)的服务系统，采用基于最短剩余令牌的迭代调度策略(ISRTF)，显著减少任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统采用先到先得调度策略，容易导致‘队头阻塞’问题，需改进调度效率。

Method: 训练BGE模型预测响应长度，设计ISRTF调度策略优化迭代批处理，并基于Kubernetes实现云原生调度系统。

Result: 实验表明ISRTF将平均任务完成时间降低19.6%。

Conclusion: ELIS通过智能调度有效提升LLM服务效率，适用于生产环境。

Abstract: We propose ELIS, a serving system for Large Language Models (LLMs) featuring
an Iterative Shortest Remaining Time First (ISRTF) scheduler designed to
efficiently manage inference tasks with the shortest remaining tokens. Current
LLM serving systems often employ a first-come-first-served scheduling strategy,
which can lead to the "head-of-line blocking" problem. To overcome this
limitation, it is necessary to predict LLM inference times and apply a shortest
job first scheduling strategy. However, due to the auto-regressive nature of
LLMs, predicting the inference latency is challenging. ELIS addresses this
challenge by training a response length predictor for LLMs using the BGE model,
an encoder-based state-of-the-art model. Additionally, we have devised the
ISRTF scheduling strategy, an optimization of shortest remaining time first
tailored to existing LLM iteration batching. To evaluate our work in an
industrial setting, we simulate streams of requests based on our study of
real-world user LLM serving trace records. Furthermore, we implemented ELIS as
a cloud-native scheduler system on Kubernetes to evaluate its performance in
production environments. Our experimental results demonstrate that ISRTF
reduces the average job completion time by up to 19.6%.

</details>


### [124] [Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures](https://arxiv.org/abs/2505.09343)
*Chenggang Zhao,Chengqi Deng,Chong Ruan,Damai Dai,Huazuo Gao,Jiashi Li,Liyue Zhang,Panpan Huang,Shangyan Zhou,Shirong Ma,Wenfeng Liang,Ying He,Yuqing Wang,Yuxuan Liu,Y. X. Wei*

Main category: cs.DC

TL;DR: DeepSeek-V3通过硬件感知的模型协同设计解决了大型语言模型（LLMs）在内存容量、计算效率和互联带宽上的瓶颈，提出多项创新技术并探讨了未来硬件发展方向。


<details>
  <summary>Details</summary>
Motivation: 当前硬件架构在LLMs快速扩展过程中暴露了内存、计算和带宽的限制，亟需通过硬件与模型协同设计来提升效率和降低成本。

Method: 采用Multi-head Latent Attention (MLA)增强内存效率、MoE架构优化计算-通信平衡、FP8混合精度训练充分发挥硬件潜力，以及Multi-Plane网络拓扑减少集群级网络开销。

Result: DeepSeek-V3在2048块NVIDIA H800 GPU上实现了高效的大规模训练和推理，验证了硬件协同设计的有效性。

Conclusion: 硬件与模型协同设计是应对AI工作负载需求的关键，本文为下一代AI系统创新提供了实践蓝图。

Abstract: The rapid scaling of large language models (LLMs) has unveiled critical
limitations in current hardware architectures, including constraints in memory
capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,
trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model
co-design can effectively address these challenges, enabling cost-efficient
training and inference at scale. This paper presents an in-depth analysis of
the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting
key innovations such as Multi-head Latent Attention (MLA) for enhanced memory
efficiency, Mixture of Experts (MoE) architectures for optimized
computation-communication trade-offs, FP8 mixed-precision training to unlock
the full potential of hardware capabilities, and a Multi-Plane Network Topology
to minimize cluster-level network overhead. Building on the hardware
bottlenecks encountered during DeepSeek-V3's development, we engage in a
broader discussion with academic and industry peers on potential future
hardware directions, including precise low-precision computation units,
scale-up and scale-out convergence, and innovations in low-latency
communication fabrics. These insights underscore the critical role of hardware
and model co-design in meeting the escalating demands of AI workloads, offering
a practical blueprint for innovation in next-generation AI systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [125] [Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors](https://arxiv.org/abs/2505.09610)
*Nicolas Dupuis,Ravi Nair,Shyam Ramji,Sean McClintock,Nishant Chauhan,Priyanka Nagpal,Bart Blaner,Ken Valk,Leon Stok,Ruchir Puri*

Main category: cs.SE

TL;DR: 本文探讨了专门用于解释VHDL代码的LLM开发，通过扩展预训练和专家评估，将代码解释准确率从43%提升至69%，并进一步通过指令调优模型达到71%。


<details>
  <summary>Details</summary>
Motivation: 尽管Verilog在芯片设计中更受关注，但VHDL在行业中仍广泛使用，尤其是在高性能处理器设计中。现有研究对VHDL和此类组织的独特需求关注不足，因此开发专门的LLM以解决这一问题。

Method: 通过扩展预训练（EPT）基础LLM，开发特定的测试集进行评估，并引入LLM-as-a-judge方法，模拟专家评估以优化模型。

Result: EPT模型将专家评估的准确率从基础模型的43%提升至69%，指令调优模型进一步达到71%，预计使用更先进的基模型可提升至85%以上。

Conclusion: 研究表明，通过针对性的模型优化和生成AI的新进展，可以显著提升硬件设计LLM的质量。

Abstract: The use of Large Language Models (LLMs) in hardware design has taken off in
recent years, principally through its incorporation in tools that increase chip
designer productivity. There has been considerable discussion about the use of
LLMs in RTL specifications of chip designs, for which the two most popular
languages are Verilog and VHDL. LLMs and their use in Verilog design has
received significant attention due to the higher popularity of the language,
but little attention so far has been given to VHDL despite its continued
popularity in the industry. There has also been little discussion about the
unique needs of organizations that engage in high-performance processor design,
and techniques to deploy AI solutions in these settings. In this paper, we
describe our journey in developing a Large Language Model (LLM) specifically
for the purpose of explaining VHDL code, a task that has particular importance
in an organization with decades of experience and assets in high-performance
processor design. We show how we developed test sets specific to our needs and
used them for evaluating models as we performed extended pretraining (EPT) of a
base LLM. Expert evaluation of the code explanations produced by the EPT model
increased to 69% compared to a base model rating of 43%. We further show how we
developed an LLM-as-a-judge to gauge models similar to expert evaluators. This
led us to deriving and evaluating a host of new models, including an
instruction-tuned version of the EPT model with an expected expert evaluator
rating of 71%. Our experiments also indicate that with the potential use of
newer base models, this rating can be pushed to 85% and beyond. We conclude
with a discussion on further improving the quality of hardware design LLMs
using exciting new developments in the Generative AI world.

</details>


### [126] [AI-Mediated Code Comment Improvement](https://arxiv.org/abs/2505.09021)
*Maria Dhakal,Chia-Yi Su,Robert Wallace,Chris Fakhimi,Aakash Bansal,Toby Li,Yu Huang,Collin McMillan*

Main category: cs.SE

TL;DR: 论文提出了一种利用AI工具重写代码注释以提升其质量的方法，并通过蒸馏技术将结果压缩为可在企业内部运行的小模型，同时在评估中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 提升代码注释的质量对于代码可维护性和开发者协作至关重要，但现有注释往往质量参差不齐，需要系统化的改进方法。

Method: 通过实证研究和定性分析确定质量改进的维度，使用LLM（如GPT-4o）重写注释，并通过蒸馏技术实现模型的小型化。

Result: 提出的方法在评估中有效提升了代码注释的质量，并成功将结果蒸馏为可本地运行的小模型。

Conclusion: 该方法不仅显著改善了代码注释质量，还通过数据开源和模型蒸馏支持了隐私保护和可复现性。

Abstract: This paper describes an approach to improve code comments along different
quality axes by rewriting those comments with customized Artificial
Intelligence (AI)-based tools. We conduct an empirical study followed by
grounded theory qualitative analysis to determine the quality axes to improve.
Then we propose a procedure using a Large Language Model (LLM) to rewrite
existing code comments along the quality axes. We implement our procedure using
GPT-4o, then distil the results into a smaller model capable of being run
in-house, so users can maintain data custody. We evaluate both our approach
using GPT-4o and the distilled model versions. We show in an evaluation how our
procedure improves code comments along the quality axes. We release all data
and source code in an online repository for reproducibility.

</details>


### [127] [Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code Generation](https://arxiv.org/abs/2505.09027)
*Yi Cui*

Main category: cs.SE

TL;DR: WebApp1K是一个新基准，用于评估大型语言模型在测试驱动开发任务中的表现，强调通过测试案例生成代码的能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖自然语言提示，而WebApp1K旨在更贴近实际软件开发实践，关注LLM直接通过测试案例生成功能代码的能力。

Method: 创建1000个跨20个应用领域的多样化挑战，评估LLM在上下文长度和多功能复杂性约束下生成紧凑、功能代码的能力。

Result: 研究发现指令遵循和上下文学习是TDD成功的关键，重要性超过一般编码能力或预训练知识。评估揭示了性能瓶颈，如长提示中的指令丢失。

Conclusion: WebApp1K强调了TDD特定基准的实际价值，为在严格的应用驱动编码场景中提升LLM能力奠定了基础。

Abstract: We introduce WebApp1K, a novel benchmark for evaluating large language models
(LLMs) in test-driven development (TDD) tasks, where test cases serve as both
prompt and verification for code generation. Unlike traditional approaches
relying on natural language prompts, our benchmark emphasizes the ability of
LLMs to interpret and implement functionality directly from test cases,
reflecting real-world software development practices. Comprising 1000 diverse
challenges across 20 application domains, the benchmark evaluates LLMs on their
ability to generate compact, functional code under the constraints of context
length and multi-feature complexity. Our findings highlight instruction
following and in-context learning as critical capabilities for TDD success,
surpassing the importance of general coding proficiency or pretraining
knowledge. Through comprehensive evaluation of 19 frontier models, we reveal
performance bottlenecks, such as instruction loss in long prompts, and provide
a detailed error analysis spanning multiple root causes. This work underscores
the practical value of TDD-specific benchmarks and lays the foundation for
advancing LLM capabilities in rigorous, application-driven coding scenarios.

</details>


### [128] [Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models](https://arxiv.org/abs/2505.09062)
*Junda Zhao,Yuliang Song,Eldan Cohen*

Main category: cs.SE

TL;DR: 论文提出了一种名为Variational Prefix Tuning (VPT)的新方法，通过集成条件变分自编码器(CVAE)框架，增强预训练模型生成多样且准确的代码摘要的能力，同时避免昂贵的模型重训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只为给定源代码生成单一高质量摘要，忽略了摘要可能不充分且需要替代选项的场景。

Method: VPT方法结合了CVAE框架，通过采样连续嵌入作为前缀来指导解码过程中生成多样化的输出。

Result: 实验结果表明，该方法在多样性和准确性方面优于现有技术，且能适应不同的预训练模型。

Conclusion: VPT是一种高效的方法，能够生成多样且准确的代码摘要，无需昂贵的模型重训练。

Abstract: Recent advancements in source code summarization have leveraged
transformer-based pre-trained models, including Large Language Models of Code
(LLMCs), to automate and improve the generation of code summaries. However,
existing methods often focus on generating a single high-quality summary for a
given source code, neglecting scenarios where the generated summary might be
inadequate and alternative options are needed. In this paper, we introduce
Variational Prefix Tuning (VPT), a novel approach that enhances pre-trained
models' ability to generate diverse yet accurate sets of summaries, allowing
the user to choose the most suitable one for the given source code. Our method
integrates a Conditional Variational Autoencoder (CVAE) framework as a modular
component into pre-trained models, enabling us to model the distribution of
observed target summaries and sample continuous embeddings to be used as
prefixes to steer the generation of diverse outputs during decoding.
Importantly, we construct our method in a parameter-efficient manner,
eliminating the need for expensive model retraining, especially when using
LLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset
of generated summaries, optimizing both the diversity and the accuracy of the
options presented to users. We present extensive experimental evaluations using
widely used datasets and current state-of-the-art pre-trained code
summarization models to demonstrate the effectiveness of our approach and its
adaptability across models.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [129] [WavReward: Spoken Dialogue Models With Generalist Reward Evaluators](https://arxiv.org/abs/2505.09558)
*Shengpeng Ji,Tianle Liang,Yangzhuo Li,Jialong Zuo,Minghui Fang,Jinzheng He,Yifu Chen,Zhengqing Liu,Ziyue Jiang,Xize Cheng,Siqi Zheng,Jin Xu,Junyang Lin,Zhou Zhao*

Main category: eess.AS

TL;DR: 论文提出WavReward，一种基于音频语言模型的奖励反馈模型，用于评估语音对话系统的IQ和EQ表现。通过强化学习和多样本反馈，结合ChatReward-30K数据集，WavReward在多项任务中显著优于现有评估模型。


<details>
  <summary>Details</summary>
Motivation: 现有语音对话模型的评估主要依赖文本方法，无法捕捉非文本信息。WavReward旨在填补这一空白，提供更全面的语音对话性能评估。

Method: 基于音频语言模型，结合深度推理和非线性奖励机制，通过强化学习和多样本反馈构建评估器。使用ChatReward-30K数据集（包含多种对话任务）进行训练。

Result: WavReward在客观准确率上从55.1%提升至91.5%，主观A/B测试中以83%的优势领先，各组件通过消融实验验证必要性。

Conclusion: WavReward为语音对话模型提供了有效的评估工具，显著优于现有方法，数据与代码将开源。

Abstract: End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered
significant attention in the speech domain. However, the evaluation of spoken
dialogue models' conversational performance has largely been overlooked. This
is primarily due to the intelligent chatbots convey a wealth of non-textual
information which cannot be easily measured using text-based language models
like ChatGPT. To address this gap, we propose WavReward, a reward feedback
model based on audio language models that can evaluate both the IQ and EQ of
spoken dialogue systems with speech input. Specifically, 1) based on audio
language models, WavReward incorporates the deep reasoning process and the
nonlinear reward mechanism for post-training. By utilizing multi-sample
feedback via the reinforcement learning algorithm, we construct a specialized
evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a
preference dataset used to train WavReward. ChatReward-30K includes both
comprehension and generation aspects of spoken dialogue models. These scenarios
span various tasks, such as text-based chats, nine acoustic attributes of
instruction chats, and implicit chats. WavReward outperforms previous
state-of-the-art evaluation models across multiple spoken dialogue scenarios,
achieving a substantial improvement about Qwen2.5-Omni in objective accuracy
from 55.1$\%$ to 91.5$\%$. In subjective A/B testing, WavReward also leads by a
margin of 83$\%$. Comprehensive ablation studies confirm the necessity of each
component of WavReward. All data and code will be publicly at
https://github.com/jishengpeng/WavReward after the paper is accepted.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [130] [CellTypeAgent: Trustworthy cell type annotation with Large Language Models](https://arxiv.org/abs/2505.08844)
*Jiawen Chen,Jianghao Zhang,Huaxiu Yao,Yun Li*

Main category: q-bio.GN

TL;DR: CellTypeAgent是一个集成了大型语言模型(LLM)和数据库验证的可靠工具，用于自动化单细胞RNA测序分析中的细胞类型注释，相比现有方法准确率更高，并减少了幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 细胞类型注释是单细胞RNA测序分析中关键但费时的步骤，目前方法存在准确性和幻觉问题，需要更高效、可靠的解决方案。

Method: 集成大型语言模型(LLM)与数据库验证，构建CellTypeAgent工具，通过结合两者的优势提升准确率。

Result: 在9个真实数据集（涵盖36种组织的303种细胞类型）中验证，CellTypeAgent准确率优于现有方法。

Conclusion: CellTypeAgent展现了结合LLM与数据库验证的潜力，为细胞类型注释提供了更高效、可靠的解决方案。

Abstract: Cell type annotation is a critical yet laborious step in single-cell RNA
sequencing analysis. We present a trustworthy large language model (LLM)-agent,
CellTypeAgent, which integrates LLMs with verification from relevant databases.
CellTypeAgent achieves higher accuracy than existing methods while mitigating
hallucinations. We evaluated CellTypeAgent across nine real datasets involving
303 cell types from 36 tissues. This combined approach holds promise for more
efficient and reliable cell type annotation.

</details>


### [131] [When repeats drive the vocabulary: a Byte-Pair Encoding analysis of T2T primate genomes](https://arxiv.org/abs/2505.08918)
*Marina Popova,Iaroslav Chelombitko,Aleksey Komissarov*

Main category: q-bio.GN

TL;DR: 使用Byte Pair Encoding (BPE)对九个T2T灵长类基因组进行tokenization分析，发现共享token极少且系统发育树未重现已知关系，表明BPE在比较基因组学中的局限性。


<details>
  <summary>Details</summary>
Motivation: T2T基因组组装为比较基因组学提供了新机会，但基因组序列的有效tokenization策略尚未充分探索。

Method: 使用自定义工具dnaBPE训练九个T2T灵长类基因组的独立BPE tokenizer，固定词汇量512,000。

Result: 仅有11,569 token在所有基因组中共享，系统发育树与已知灵长类关系不符，高拷贝重复序列影响显著。

Conclusion: BPE在基因组压缩中有效，但在比较基因组学中需结合其他策略；dnaBPE工具开源可用。

Abstract: The emergence of telomere-to-telomere (T2T) genome assemblies has opened new
avenues for comparative genomics, yet effective tokenization strategies for
genomic sequences remain underexplored. In this pilot study, we apply Byte Pair
Encoding (BPE) to nine T2T primate genomes including three human assemblies by
training independent BPE tokenizers with a fixed vocabulary of 512,000 tokens
using our custom tool, dnaBPE. Our analysis reveals that only 11,569 tokens are
shared across all assemblies, while nearly 991,854 tokens are unique to a
single genome, indicating a rapid decline in shared vocabulary with increasing
assembly comparisons. Moreover, phylogenetic trees derived from token overlap
failed to recapitulate established primate relationships, a discrepancy
attributed to the disproportionate influence of species-specific high-copy
repetitive elements. These findings underscore the dual nature of BPE
tokenization: while it effectively compresses repetitive sequences, its
sensitivity to high-copy elements limits its utility as a universal tool for
comparative genomics. We discuss potential hybrid strategies and repeat-masking
approaches to refine genomic tokenization, emphasizing the need for
domain-specific adaptations in the development of large-scale genomic language
models. The dnaBPE tool used in this study is open-source and available at
https://github.com/aglabx/dnaBPE.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [132] [A Retrieval-Augmented Generation Framework for Academic Literature Navigation in Data Science](https://arxiv.org/abs/2412.15404)
*Ahmet Yasin Aytar,Kemal Kilic,Kamer Kaya*

Main category: cs.IR

TL;DR: 论文提出了一种基于检索增强生成（RAG）的AI系统，通过GROBID技术、微调嵌入模型、语义分块和摘要优先检索方法，提升数据科学领域学术文献检索的准确性和相关性，实验显示在上下文相关性等指标上显著改进。


<details>
  <summary>Details</summary>
Motivation: 数据科学领域文献迅速增长，高效检索相关学术资源对决策和创新至关重要。现有检索系统在准确性和相关性上存在不足，亟需改进。

Method: 采用GROBID技术提取文献元数据，结合微调嵌入模型和语义分块优化检索，引入摘要优先检索方法提升结果质量，并使用RAGAS框架评估性能。

Result: 实验显示，该系统在上下文相关性等关键指标上显著提升，有效缓解信息过载问题，并优化决策流程。

Conclusion: 增强的RAG系统有望革新数据科学领域的学术探索，推动研究和创新工作流的进步。

Abstract: In the rapidly evolving field of data science, efficiently navigating the
expansive body of academic literature is crucial for informed decision-making
and innovation. This paper presents an enhanced Retrieval-Augmented Generation
(RAG) application, an artificial intelligence (AI)-based system designed to
assist data scientists in accessing precise and contextually relevant academic
resources. The AI-powered application integrates advanced techniques, including
the GeneRation Of BIbliographic Data (GROBID) technique for extracting
bibliographic information, fine-tuned embedding models, semantic chunking, and
an abstract-first retrieval method, to significantly improve the relevance and
accuracy of the retrieved information. This implementation of AI specifically
addresses the challenge of academic literature navigation. A comprehensive
evaluation using the Retrieval-Augmented Generation Assessment System (RAGAS)
framework demonstrates substantial improvements in key metrics, particularly
Context Relevance, underscoring the system's effectiveness in reducing
information overload and enhancing decision-making processes. Our findings
highlight the potential of this enhanced Retrieval-Augmented Generation system
to transform academic exploration within data science, ultimately advancing the
workflow of research and innovation in the field.

</details>


### [133] [Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases](https://arxiv.org/abs/2505.09246)
*Derian Boer,Stephen Roth,Stefan Kramer*

Main category: cs.IR

TL;DR: 该论文提出FocusedRetriever，一种基于半结构化知识库的模块化框架，用于多跳问答任务。通过结合向量相似性搜索、LLM生成的Cypher查询和重新排名，在STaRK基准测试中表现优于现有方法，第一命中率平均提升25.7%。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，机器学习模型和交互系统通常需同时处理结构化知识（如知识图谱）和非结构化内容（如自然语言文档）。传统方法往往仅依赖其中一种，而半结构化知识库（SKBs）能弥合这一鸿沟，通过链接非结构化内容与结构化数据节点，为知识访问与使用提供新策略。

Method: Framework integrates four key techniques: (1) LLMs extract relational facts & attributes from text; (2) node set joins filter candidates using extracted triplets; (3) vector similarity search retrieves/ranks unstructured content; (4) LLMs contextually re-rank top-k answers.

Result: 在STaRK基准测试的多样化领域和指标中，FocusedRetriever全面超越现有方法，第一命中率平均领先次优方法25.7%。仅使用基础LLM即可实现高性能，但中间结果分析表明微调等优化仍有提升空间。

Conclusion: FocusedRetriever验证了SKBs在跨模态知识融合中的有效性，其模块化设计便于扩展（如LLM微调）。未来可通过进一步优化中间步骤（如查询生成）提升性能。代码已开源。

Abstract: In many real-world settings, machine learning models and interactive systems
have access to both structured knowledge, e.g., knowledge graphs or tables, and
unstructured content, e.g., natural language documents. However, most rely on
either. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking
unstructured content to nodes within structured data, thereby enabling new
strategies for knowledge access and use. In this work, we present
FocusedRetriever, a modular SKB-based framework for multi-hop question
answering. It integrates components (VSS-based entity search, LLM-based
generation of Cypher queries and pairwise re-ranking) in a way that enables it
to outperform state-of-the-art methods across all three STaRK benchmark test
sets, covering diverse domains and multiple performance metrics. The average
first-hit rate exceeds that of the second-best method by 25.7%.
FocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to
extract relational facts and entity attributes from unstructured text, (2) node
set joins to filter answer candidates based on these extracted triplets and
constraints, (3) vector similarity search to retrieve and rank relevant
unstructured content, and (4) the contextual capabilities of LLMs to finally
rank the top-k answers. For generality, we only incorporate base LLMs in
FocusedRetriever in our evaluation. However, our analysis of intermediate
results highlights several opportunities for further upgrades including
finetuning. The source code is publicly available at
https://github.com/kramerlab/FocusedRetriever .

</details>


### [134] [Diffusion Recommender Models and the Illusion of Progress: A Concerning Study of Reproducibility and a Conceptual Mismatch](https://arxiv.org/abs/2505.09364)
*Michael Benigni,Maurizio Ferrari Dacrema,Dietmar Jannach*

Main category: cs.IR

TL;DR: 研究发现，最新的基于扩散模型的推荐技术尽管计算复杂且碳足迹大，但性能仍不如简单模型，且存在方法论问题。


<details>
  <summary>Details</summary>
Motivation: 探讨当前研究中关于扩散模型在推荐系统中的有效性及方法论问题是否依旧存在。

Method: 通过复现2023-2024年SIGIR会议上的四个扩散模型论文的实验，评估其性能和方法论。

Result: 扩散模型在推荐系统中表现不佳，且存在方法论缺陷，如与未调优基线的比较问题。

Conclusion: 呼吁加强科学严谨性并改变该领域的研究和出版文化。

Abstract: Countless new machine learning models are published every year and are
reported to significantly advance the state-of-the-art in \emph{top-n}
recommendation. However, earlier reproducibility studies indicate that progress
in this area may be quite limited. Specifically, various widespread
methodological issues, e.g., comparisons with untuned baseline models, have led
to an \emph{illusion of progress}. In this work, our goal is to examine whether
these problems persist in today's research. To this end, we aim to reproduce
the latest advancements reported from applying modern Denoising Diffusion
Probabilistic Models to recommender systems, focusing on four models published
at the top-ranked SIGIR conference in 2023 and 2024. Our findings are
concerning, revealing persistent methodological problems. Alarmingly, through
experiments, we find that the latest recommendation techniques based on
diffusion models, despite their computational complexity and substantial carbon
footprint, are consistently outperformed by simpler existing models.
Furthermore, we identify key mismatches between the characteristics of
diffusion models and those of the traditional \emph{top-n} recommendation task,
raising doubts about their suitability for recommendation. We also note that,
in the papers we analyze, the generative capabilities of these models are
constrained to a minimum. Overall, our results and continued methodological
issues call for greater scientific rigor and a disruptive change in the
research and publication culture in this area.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [135] [A Comparative Review of RNA Language Models](https://arxiv.org/abs/2505.09087)
*He Wang,Yikun Zhang,Jie Chen,Jian Zhan,Yaoqi Zhou*

Main category: q-bio.BM

TL;DR: 文章通过将RNA语言模型分为三类并比较13种RNA语言模型，发现擅长二级结构预测的模型在功能分类上表现往往较差，反之亦然，表明需要更均衡的无监督训练。


<details>
  <summary>Details</summary>
Motivation: 鉴于蛋白质语言模型在结构和功能推断中的实用性，RNA语言模型近年受到更多关注，但缺乏统一评估标准。

Method: 将RNA语言模型分为三类（预训练于多种RNA、特定目的RNA、统一RNA与DNA或蛋白质的模型），并与DNA和蛋白质模型作为对照，进行零样本预测RNA二级结构和功能分类的比较。

Result: 结果显示，擅长二级结构预测的模型在功能分类上表现较差，反之亦然。

Conclusion: 需要更均衡的无监督训练以提高模型性能。

Abstract: Given usefulness of protein language models (LMs) in structure and functional
inference, RNA LMs have received increased attentions in the last few years.
However, these RNA models are often not compared against the same standard.
Here, we divided RNA LMs into three classes (pretrained on multiple RNA types
(especially noncoding RNAs), specific-purpose RNAs, and LMs that unify RNA with
DNA or proteins or both) and compared 13 RNA LMs along with 3 DNA and 1 protein
LMs as controls in zero-shot prediction of RNA secondary structure and
functional classification. Results shows that the models doing well on
secondary structure prediction often perform worse in function classification
or vice versa, suggesting that more balanced unsupervised training is needed.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [136] [Enhanced Photonic Chip Design via Interpretable Machine Learning Techniques](https://arxiv.org/abs/2505.09266)
*Lirandë Pira,Airin Antony,Nayanthara Prathap,Daniel Peace,Jacquiline Romero*

Main category: physics.optics

TL;DR: 本文介绍了如何在光子芯片设计中应用机器学习可解释性技术，特别是LIME方法，以提升逆向设计的透明度和性能。


<details>
  <summary>Details</summary>
Motivation: 逆向设计在光子芯片优化中虽高效但缺乏透明度，尤其是在机器学习方法中。本文旨在通过可解释性技术解决这一挑战，提升设计过程的理解与性能。

Method: 采用局部可解释模型无关解释（LIME）技术，分析逆向设计优化过程，以指导光子组件（如双模复用器）的设计。

Result: LIME提供的洞察帮助优化初始条件，直接提升了器件性能，证明了可解释性技术不仅能解释模型，还能主动改进设计。

Conclusion: 可解释性技术揭示了逆向设计中的潜在模式，有助于开发性能更优的光子组件，推动了透明化设计的发展。

Abstract: Photonic chip design has seen significant advancements with the adoption of
inverse design methodologies, offering flexibility and efficiency in optimizing
device performance. However, the black-box nature of the optimization
approaches, such as those used in inverse design in order to minimize a loss
function or maximize coupling efficiency, poses challenges in understanding the
outputs. This challenge is prevalent in machine learning-based optimization
methods, which can suffer from the same lack of transparency. To this end,
interpretability techniques address the opacity of optimization models. In this
work, we apply interpretability techniques from machine learning, with the aim
of gaining understanding of inverse design optimization used in designing
photonic components, specifically two-mode multiplexers. We base our
methodology on the widespread interpretability technique known as local
interpretable model-agnostic explanations, or LIME. As a result, LIME-informed
insights point us to more effective initial conditions, directly improving
device performance. This demonstrates that interpretability methods can do more
than explain models -- they can actively guide and enhance the inverse-designed
photonic components. Our results demonstrate the ability of interpretable
techniques to reveal underlying patterns in the inverse design process, leading
to the development of better-performing components.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [137] [Bounding Neyman-Pearson Region with $f$-Divergences](https://arxiv.org/abs/2505.08899)
*Andrew Mullhaupt,Cheng Peng*

Main category: math.ST

TL;DR: 该论文提出了一种基于$f$-散度的Neyman-Pearson边界下界，证明其最优性，并在KL散度下改进了Pinsker不等式；同时给出了基于Chernoff系数的上界，并提供了构造分布对以实现任意Neyman-Pearson边界的方法。


<details>
  <summary>Details</summary>
Motivation: 研究Neyman-Pearson边界在二元假设检验中的下界问题，旨在通过$f$-散度提供更优的边界刻画，并探索分布对的构造方法以验证理论结果。

Method: 利用$f$-散度（尤其曲棍球散度）推导边界下界，通过KL散度改进Pinsker不等式；基于Chernoff系数提出闭式上界；设计分布对构造方法以实现理论边界。

Result: 证明了$f$-散度下界的最优性，KL散度下改进了Pinsker不等式；获得了Chernoff系数上界；提出了精确或近似实现任意边界的分布对构造方法。

Conclusion: 该工作为Neyman-Pearson边界提供了普适的理论框架，下界和上界的改进具有理论意义，分布构造方法为实际应用提供了工具。

Abstract: The Neyman-Pearson region of a simple binary hypothesis testing is the set of
points whose coordinates represent the false positive rate and false negative
rate of some test. The lower boundary of this region is given by the
Neyman-Pearson lemma, and is up to a coordinate change, equivalent to the
optimal ROC curve. We establish a novel lower bound for the boundary in terms
of any $f$-divergence. Since the bound generated by hockey-stick
$f$-divergences characterizes the Neyman-Pearson boundary, this bound is best
possible. In the case of KL divergence, this bound improves Pinsker's
inequality. Furthermore, we obtain a closed-form refined upper bound for the
Neyman-Pearson boundary in terms of the Chernoff $\alpha$-coefficient. Finally,
we present methods for constructing pairs of distributions that can
approximately or exactly realize any given Neyman-Pearson boundary.

</details>


### [138] [Statistical Decision Theory with Counterfactual Loss](https://arxiv.org/abs/2505.08908)
*Benedikt Koch,Kosuke Imai*

Main category: math.ST

TL;DR: 该论文扩展了传统统计决策理论，引入了反事实损失以评估决策质量，解决了仅依赖观察结果而忽略反事实结果的局限性。通过假设强可忽略性，提出了反事实风险可识别的条件，并展示了在多治疗选项下反事实损失可能带来不同的治疗方案推荐。


<details>
  <summary>Details</summary>
Motivation: 传统统计决策理论仅基于观察结果评估治疗方案，忽略了反事实结果的潜在影响。这限制了决策质量的全面评估，尤其是在医疗等需要权衡不同治疗方案的领域。论文旨在扩展这一理论，以更全面地评估决策质量。

Method: 论文通过引入反事实损失函数，扩展了标准决策理论。在强可忽略性假设下，证明了反事实风险的可识别性条件，即损失函数必须是潜在结果的加性函数。

Result: 研究发现，反事实损失可以识别反事实风险，仅当损失函数是加性的。此外，在多治疗选项下，反事实损失可能导致与传统损失函数不同的治疗推荐。

Conclusion: 通过引入反事实损失，论文为决策理论提供了更全面的评估框架，尤其在多治疗选项场景下，能够提供更优化的推荐。强可忽略性是实现反事实风险可识别的关键假设。

Abstract: Classical statistical decision theory evaluates treatment choices based
solely on observed outcomes. However, by ignoring counterfactual outcomes, it
cannot assess the quality of decisions relative to feasible alternatives. For
example, the quality of a physician's decision may depend not only on patient
survival, but also on whether a less invasive treatment could have produced a
similar result. To address this limitation, we extend standard decision theory
to incorporate counterfactual losses--criteria that evaluate decisions using
all potential outcomes. The central challenge in this generalization is
identification: because only one potential outcome is observed for each unit,
the associated risk under a counterfactual loss is generally not identifiable.
We show that under the assumption of strong ignorability, a counterfactual risk
is identifiable if and only if the counterfactual loss function is additive in
the potential outcomes. Moreover, we demonstrate that additive counterfactual
losses can yield treatment recommendations that differ from those based on
standard loss functions, provided that the decision problem involves more than
two treatment options.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [139] [A Comparative Study of Transformer-Based Models for Multi-Horizon Blood Glucose Prediction](https://arxiv.org/abs/2505.08821)
*Meryem Altin Karagoz,Marc D. Breton,Anas El Fathi*

Main category: q-bio.QM

TL;DR: 该论文探讨了基于Transformer的架构在血糖预测中的应用，通过对比不同嵌入方式的模型性能，发现Patch-wise Transformer在短期和长期预测中均表现最优，尤其是使用一周历史数据时效果最佳。


<details>
  <summary>Details</summary>
Motivation: 准确的血糖预测能为1型糖尿病治疗提供个性化干预措施，如胰岛素和饮食调整。尽管Transformer架构在复杂多变量时间序列预测中展现了强大潜力，但其在血糖预测中的应用尚未充分探索。

Method: 研究比较了多种Transformer模型在多时间范围内的血糖预测性能，使用了DCLP3和OhioT1DM数据集，并测试了点状、片段状、序列状及混合嵌入方式。

Result: Crossformer在30分钟预测中表现最佳（RMSE 15.6 mg/dL），而PatchTST在长期预测（1h、2h、4h）中表现最优（RMSE分别为24.6、36.1、46.5 mg/dL）。片段化嵌入方法在增加输入数据量时效果更佳。

Conclusion: 基于Transformer的架构能有效捕捉多变量时间序列数据的周期性模式，显著提升血糖预测的准确性，尤其在结合较长的历史数据时表现更为突出。

Abstract: Accurate blood glucose prediction can enable novel interventions for type 1
diabetes treatment, including personalized insulin and dietary adjustments.
Although recent advances in transformer-based architectures have demonstrated
the power of attention mechanisms in complex multivariate time series
prediction, their potential for blood glucose (BG) prediction remains
underexplored. We present a comparative analysis of transformer models for
multi-horizon BG prediction, examining forecasts up to 4 hours and input
history up to 1 week. The publicly available DCLP3 dataset (n=112) was split
(80%-10%-10%) for training, validation, and testing, and the OhioT1DM dataset
(n=12) served as an external test set. We trained networks with point-wise,
patch-wise, series-wise, and hybrid embeddings, using CGM, insulin, and meal
data. For short-term blood glucose prediction, Crossformer, a patch-wise
transformer architecture, achieved a superior 30-minute prediction of RMSE
(15.6 mg / dL on OhioT1DM). For longer-term predictions (1h, 2h, and 4h),
PatchTST, another path-wise transformer, prevailed with the lowest RMSE (24.6
mg/dL, 36.1 mg/dL, and 46.5 mg/dL on OhioT1DM). In general, models that used
tokenization through patches demonstrated improved accuracy with larger input
sizes, with the best results obtained with a one-week history. These findings
highlight the promise of transformer-based architectures for BG prediction by
capturing and leveraging seasonal patterns in multivariate time-series data to
improve accuracy.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [140] [InvDesFlow-AL: Active Learning-based Workflow for Inverse Design of Functional Materials](https://arxiv.org/abs/2505.09203)
*Xiao-Qi Han,Peng-Jie Guo,Ze-Feng Gao,Hao Sun,Zhong-Yi Lu*

Main category: cond-mat.mtrl-sci

TL;DR: 论文提出了一种基于主动学习策略的新型逆向材料设计生成框架InvDesFlow-AL，显著提升了晶体结构预测的准确性，并成功应用于低形成能材料和BCS超导体的发现。


<details>
  <summary>Details</summary>
Motivation: 开发具有特定功能材料的逆向设计方法对可再生能源、催化、储能和碳捕获等领域至关重要。现有的生成和预测晶体结构方法成功率较低，限制了效率。

Method: 提出了InvDesFlow-AL框架，基于主动学习策略迭代优化材料生成过程，逐步实现目标性能特征。

Result: 晶体结构预测的RMSE达到0.0423 Å，比现有生成模型性能提升32.96%；成功设计出低形成能材料，并发现Li₂AuH₆作为140 K高温BCS超导体。

Conclusion: InvDesFlow-AL框架通过主动学习显著加速了材料发现和逆向设计过程，为材料科学应用提供了实证支持。

Abstract: Developing inverse design methods for functional materials with specific
properties is critical to advancing fields like renewable energy, catalysis,
energy storage, and carbon capture. Generative models based on diffusion
principles can directly produce new materials that meet performance
constraints, thereby significantly accelerating the material design process.
However, existing methods for generating and predicting crystal structures
often remain limited by low success rates. In this work, we propose a novel
inverse material design generative framework called InvDesFlow-AL, which is
based on active learning strategies. This framework can iteratively optimize
the material generation process to gradually guide it towards desired
performance characteristics. In terms of crystal structure prediction, the
InvDesFlow-AL model achieves an RMSE of 0.0423 {\AA}, representing an 32.96%
improvement in performance compared to exsisting generative models.
Additionally, InvDesFlow-AL has been successfully validated in the design of
low-formation-energy and low-Ehull materials. It can systematically generate
materials with progressively lower formation energies while continuously
expanding the exploration across diverse chemical spaces. These results fully
demonstrate the effectiveness of the proposed active learning-driven generative
model in accelerating material discovery and inverse design. To further prove
the effectiveness of this method, we took the search for BCS superconductors
under ambient pressure as an example explored by InvDesFlow-AL. As a result, we
successfully identified Li\(_2\)AuH\(_6\) as a conventional BCS superconductor
with an ultra-high transition temperature of 140 K. This discovery provides
strong empirical support for the application of inverse design in materials
science.

</details>


### [141] [Bridging Theory and Experiment in Materials Discovery: Machine-Learning-Assisted Prediction of Synthesizable Structures](https://arxiv.org/abs/2505.09161)
*Yu Xin,Peng Liu,Zhuohang Xie,Wenhui Mi,Pengyue Gao,Hong Jian Zhao,Jian Lv,Yanchao Wang,Yanming Ma*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究提出了一种合成性驱动的晶体结构预测框架，结合对称性引导和机器学习模型，有效从大量候选结构中筛选出可合成材料。


<details>
  <summary>Details</summary>
Motivation: 传统热力学驱动的晶体结构预测方法往往难以识别实验中通过动力学途径合成的亚稳态材料，导致理论与实验的脱节。

Method: 采用对称性引导的结构推导和基于Wyckoff编码的机器学习模型，结合从头计算，系统评估结构的合成性。

Result: 成功重现13种已知XSe结构，并从55万候选结构中筛选出9万多个潜在可合成结构，还鉴定出8种热力学有利的Hf-X-O结构。

Conclusion: 该研究通过数据驱动的方法缩小了计算预测与实验合成的差距，为功能性材料的定向发现提供了新思路。

Abstract: Even though thermodynamic energy-based crystal structure prediction (CSP) has
revolutionized materials discovery, the energy-driven CSP approaches often
struggle to identify experimentally realizable metastable materials synthesized
through kinetically controlled pathways, creating a critical gap between
theoretical predictions and experimental synthesis. Here, we propose a
synthesizability-driven CSP framework that integrates symmetry-guided structure
derivation with a Wyckoff encode-based machine-learning model, allowing for the
efficient localization of subspaces likely to yield highly synthesizable
structures. Within the identified promising subspaces, a structure-based
synthesizability evaluation model, fine-tuned using recently synthesized
structures to enhance predictive accuracy, is employed in conjunction with ab
initio calculations to systematically identify synthesizable candidates. The
framework successfully reproduces 13 experimentally known XSe (X = Sc, Ti, Mn,
Fe, Ni, Cu, Zn) structures, demonstrating its effectiveness in predicting
synthesizable structures. Notably, 92,310 structures are filtered from the
554,054 candidates predicted by GNoME, exhibiting great potential for promising
synthesizability. Additionally, eight thermodynamically favorable Hf-X-O (X =
Ti, V, and Mn) structures have been identified, among which three HfV$_2$O$_7$
candidates exhibit high synthesizability, presenting viable candidates for
experimental realization and potentially associated with experimentally
observed temperature-induced phase transitions. This work establishes a
data-driven paradigm for machine-learning-assisted inorganic materials
synthesis, highlighting its potential to bridge the gap between computational
predictions and experimental realization while unlocking new opportunities for
the targeted discovery of novel functional materials.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [142] [In-Context Learning for Label-Efficient Cancer Image Classification in Oncology](https://arxiv.org/abs/2505.08798)
*Mobina Shrestha,Bishwas Mandal,Vishal Mandal,Asis Shrestha*

Main category: eess.IV

TL;DR: 该研究探讨了上下文学习（ICL）在肿瘤学中的应用，通过少量标注样本让模型适应新诊断任务，无需重新训练。测试了四种视觉语言模型（VLMs），结果显示GPT-4o在少样本提示下表现最佳，开源模型也展示了竞争力。


<details>
  <summary>Details</summary>
Motivation: 解决AI在肿瘤学中依赖大量标注数据和需重新训练的局限性，探索ICL作为替代方案以适应资源有限的环境。

Method: 使用四种VLMs（Paligemma、CLIP、ALIGN、GPT-4o），在三个肿瘤数据集（MHIST、PatchCamelyon、HAM10000）上评估少样本提示下的性能。

Result: 所有模型在少样本提示下均表现显著提升，GPT-4o在二分类和多分类中的F1得分分别为0.81和0.60。开源模型虽规模较小但表现竞争性。

Conclusion: ICL展示了在肿瘤学中的实用潜力，尤其适用于罕见癌症和资源受限场景，无需大量标注数据或重新训练模型。

Abstract: The application of AI in oncology has been limited by its reliance on large,
annotated datasets and the need for retraining models for domain-specific
diagnostic tasks. Taking heed of these limitations, we investigated in-context
learning as a pragmatic alternative to model retraining by allowing models to
adapt to new diagnostic tasks using only a few labeled examples at inference,
without the need for retraining. Using four vision-language models
(VLMs)-Paligemma, CLIP, ALIGN and GPT-4o, we evaluated the performance across
three oncology datasets: MHIST, PatchCamelyon and HAM10000. To the best of our
knowledge, this is the first study to compare the performance of multiple VLMs
on different oncology classification tasks. Without any parameter updates, all
models showed significant gains with few-shot prompting, with GPT-4o reaching
an F1 score of 0.81 in binary classification and 0.60 in multi-class
classification settings. While these results remain below the ceiling of fully
fine-tuned systems, they highlight the potential of ICL to approximate
task-specific behavior using only a handful of examples, reflecting how
clinicians often reason from prior cases. Notably, open-source models like
Paligemma and CLIP demonstrated competitive gains despite their smaller size,
suggesting feasibility for deployment in computing constrained clinical
environments. Overall, these findings highlight the potential of ICL as a
practical solution in oncology, particularly for rare cancers and
resource-limited contexts where fine-tuning is infeasible and annotated data is
difficult to obtain.

</details>


### [143] [Ultrasound Report Generation with Multimodal Large Language Models for Standardized Texts](https://arxiv.org/abs/2505.08838)
*Peixuan Ge,Tongkun Su,Faqin Lv,Baoliang Zhao,Peng Zhang,Chi Hong Wong,Liang Yao,Yu Sun,Zenan Wang,Pak Kin Wong,Ying Hu*

Main category: eess.IV

TL;DR: 提出统一多器官和多语言的超声报告生成框架，结合碎片化多语言训练与标准化数据，显著提高生成报告的准确性和一致性，并在多项指标上超越先前方法。


<details>
  <summary>Details</summary>
Motivation: 超声报告生成面临数据标准不一、依赖操作者等问题，现有方法难以实现自动化。本研究旨在解决这些问题，提供一个适用于多器官、多语言的统一框架。

Method: 通过碎片化多语言训练和双语数据集（英语-中文）对齐模块化文本片段与多样影像数据，结合选择性解冻视觉变换器（ViT）优化文本-图像对齐。

Result: 相较于先前的KMVE方法，BLEU提升2%，ROUGE-L提升3%，CIDEr提升15%，并显著减少遗漏或错误内容。

Conclusion: 该框架展示了在真实临床流程中的潜力，可扩展用于多器官和多语言报告的标准化生成。

Abstract: Ultrasound (US) report generation is a challenging task due to the
variability of US images, operator dependence, and the need for standardized
text. Unlike X-ray and CT, US imaging lacks consistent datasets, making
automation difficult. In this study, we propose a unified framework for
multi-organ and multilingual US report generation, integrating fragment-based
multilingual training and leveraging the standardized nature of US reports. By
aligning modular text fragments with diverse imaging data and curating a
bilingual English-Chinese dataset, the method achieves consistent and
clinically accurate text generation across organ sites and languages.
Fine-tuning with selective unfreezing of the vision transformer (ViT) further
improves text-image alignment. Compared to the previous state-of-the-art KMVE
method, our approach achieves relative gains of about 2\% in BLEU scores,
approximately 3\% in ROUGE-L, and about 15\% in CIDEr, while significantly
reducing errors such as missing or incorrect content. By unifying multi-organ
and multi-language report generation into a single, scalable framework, this
work demonstrates strong potential for real-world clinical workflows.

</details>


### [144] [Validation of Conformal Prediction in Cervical Atypia Classification](https://arxiv.org/abs/2505.08845)
*Misgina Tsighe Hagos,Antti Suutala,Dmitrii Bychkov,Hakan Kücükel,Joar von Bahr,Milda Poceviciute,Johan Lundin,Nina Linder,Claes Lundström*

Main category: eess.IV

TL;DR: 该论文探讨如何通过共形预测改进深度学习在宫颈癌分类中的不确定性表达，并通过专家标注验证其真实性与实用性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在宫颈癌分类中存在过度自信和不确定性表达不足的问题，共形预测可生成包含可能类别的预测集，但现有评估忽略了预测集的真实性和临床价值。

Method: 使用三种共形预测方法对三个宫颈异型分类深度学习模型进行评估，结合多注释者的专家标注验证预测集的准确性。

Result: 传统覆盖率评估高估性能，现有共形预测方法的预测集与人类标注一致性不足，但能识别模糊和分布外数据。

Conclusion: 需改进共形预测方法以生成更符合人类预期的预测集，提升其在临床实践中的实用性。

Abstract: Deep learning based cervical cancer classification can potentially increase
access to screening in low-resource regions. However, deep learning models are
often overconfident and do not reliably reflect diagnostic uncertainty.
Moreover, they are typically optimized to generate maximum-likelihood
predictions, which fail to convey uncertainty or ambiguity in their results.
Such challenges can be addressed using conformal prediction, a model-agnostic
framework for generating prediction sets that contain likely classes for
trained deep-learning models. The size of these prediction sets indicates model
uncertainty, contracting as model confidence increases. However, existing
conformal prediction evaluation primarily focuses on whether the prediction set
includes or covers the true class, often overlooking the presence of extraneous
classes. We argue that prediction sets should be truthful and valuable to end
users, ensuring that the listed likely classes align with human expectations
rather than being overly relaxed and including false positives or unlikely
classes. In this study, we comprehensively validate conformal prediction sets
using expert annotation sets collected from multiple annotators. We evaluate
three conformal prediction approaches applied to three deep-learning models
trained for cervical atypia classification. Our expert annotation-based
analysis reveals that conventional coverage-based evaluations overestimate
performance and that current conformal prediction methods often produce
prediction sets that are not well aligned with human labels. Additionally, we
explore the capabilities of the conformal prediction methods in identifying
ambiguous and out-of-distribution data.

</details>


### [145] [Thoughts on Objectives of Sparse and Hierarchical Masked Image Model](https://arxiv.org/abs/2505.08819)
*Asahi Miyazaki,Tsuyoshi Okita*

Main category: eess.IV

TL;DR: 提出了一种新的掩模模式Mesh Mask，用于改进SparK模型，并研究了不同掩模模式对预训练性能的影响。


<details>
  <summary>Details</summary>
Motivation: 探索不同掩模模式对自监督学习模型性能的影响，尤其是针对SparK模型的改进。

Method: 提出Mesh Mask模式，应用于SparK模型进行预训练。

Result: 报告了Mesh Mask模式在预训练中的性能效果。

Conclusion: Mesh Mask模式对SparK模型的性能有积极影响。

Abstract: Masked image modeling is one of the most poplular objectives of training.
Recently, the SparK model has been proposed with superior performance among
self-supervised learning models. This paper proposes a new mask pattern for
this SparK model, proposing it as the Mesh Mask-ed SparK model. We report the
effect of the mask pattern used for image masking in pre-training on
performance.

</details>


### [146] [Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using Implicit Neural Representations](https://arxiv.org/abs/2505.09565)
*Maik Dannecker,Thomas Sanchez,Meritxell Bach Cuadra,Özgün Turgut,Anthony N. Price,Lucilio Cordero-Grande,Vanessa Kyriakopoulou,Joseph V. Hajnal,Daniel Rueckert*

Main category: eess.IV

TL;DR: 该论文提出了一种基于隐式神经表示的高分辨率切片到体积重建（SVR）方法，能快速准确地处理运动伪影严重的MRI数据，且无需预对齐切片。实验显示，其重建质量和速度均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的切片到体积重建方法在处理运动伪影严重的低分辨率2D切片时存在局限性，常需预对齐切片且重建效果不佳。本文旨在提供一种无需预对齐、能高效处理严重运动问题的解决方案。

Method: 采用隐式神经表示，结合运动矫正、离群值处理及超分辨率重建，通过全自监督元学习利用任务特定先验初始化模型。

Result: 在480多次模拟和临床MRI脑数据重建实验中，该方法在严重运动伪影情况下显著提升重建质量，并缩短50%重建时间。

Conclusion: 该SVR方法在运动伪影严重的场景下表现优越，重建速度和准确率均超越现有技术，具有临床应用潜力。

Abstract: High-resolution slice-to-volume reconstruction (SVR) from multiple
motion-corrupted low-resolution 2D slices constitutes a critical step in
image-based diagnostics of moving subjects, such as fetal brain Magnetic
Resonance Imaging (MRI). Existing solutions struggle with image artifacts and
severe subject motion or require slice pre-alignment to achieve satisfying
reconstruction performance. We propose a novel SVR method to enable fast and
accurate MRI reconstruction even in cases of severe image and motion
corruption. Our approach performs motion correction, outlier handling, and
super-resolution reconstruction with all operations being entirely based on
implicit neural representations. The model can be initialized with
task-specific priors through fully self-supervised meta-learning on either
simulated or real-world data. In extensive experiments including over 480
reconstructions of simulated and clinical MRI brain data from different
centers, we prove the utility of our method in cases of severe subject motion
and image artifacts. Our results demonstrate improvements in reconstruction
quality, especially in the presence of severe motion, compared to
state-of-the-art methods, and up to 50% reduction in reconstruction time.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [147] [A New Tractable Description Logic under Categorical Semantics](https://arxiv.org/abs/2505.08916)
*Chan Le Duc,Ludovic Brieulle*

Main category: cs.LO

TL;DR: 论文提出了一种在保持可处理性的前提下扩展EL描述逻辑的方法，通过引入弱化的否定来表示负面知识。


<details>
  <summary>Details</summary>
Motivation: 生物医学本体中常用负面知识表示（如缺乏部分），但直接用标签而非逻辑构造器会导致推理器无法理解其逻辑含义。现有方法添加否定会使EL逻辑变得不可处理。

Method: 引入SH描述逻辑的范畴语义，通过识别并剔除导致不可处理性的独立范畴属性，弱化析取和全称限制的语义，从而保留可处理性。

Result: 新逻辑在表达能力上强于带底部概念、传递角色和角色包含的EL逻辑，同时保持了可处理性。

Conclusion: 通过语义弱化实现了负面知识的可处理表示，为生物医学本体等领域的逻辑推理提供了新途径。

Abstract: Biomedical ontologies contain numerous concept or role names involving
negative knowledge such as lacks_part, absence_of. Such a representation with
labels rather than logical constructors would not allow a reasoner to interpret
lacks_part as a kind of negation of has_part. It is known that adding negation
to the tractable Description Logic (DL) EL allowing for conjunction,
existential restriction and concept inclusion makes it intractable since the
obtained logic includes implicitly disjunction and universal restriction which
interact with other constructors. In this paper, we propose a new extension of
EL with a weakened negation allowing to represent negative knowledge while
retaining tractability. To this end, we introduce categorical semantics of all
logical constructors of the DL SH including EL with disjunction, negation,
universal restriction, role inclusion and transitive roles. The categorical
semantics of a logical constructor is usually described as a set of categorical
properties referring to several objects without using set membership. To
restore tractability, we have to weaken semantics of disjunction and universal
restriction by identifying \emph{independent} categorical properties that are
responsible for intractability, and dropping them from the set of categorical
properties. We show that the logic resulting from weakening semantics is more
expressive than EL with the bottom concept, transitive roles and role
inclusion.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [148] [Accelerating Machine Learning Systems via Category Theory: Applications to Spherical Attention for Gene Regulatory Networks](https://arxiv.org/abs/2505.09326)
*Vincent Abbott,Kotaro Kamiya,Gerard Glowacki,Yu Atsumi,Gioele Zardini,Yoshihiro Maruyama*

Main category: math.CT

TL;DR: 该论文提出了一种基于神经电路图的通用深度学习方法，开发了一种新型注意力算法（球形注意力），并通过理论证明和高效内核实现展示了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过神经电路图（基于范畴论）提升人工智能模型的自我改进能力，解决现有自动编译方法效率低下和算法开发耗时的问题。

Method: 利用神经电路图提出球形注意力算法，用$L^2$范数替代SoftMax，避免了标准注意力的计算瓶颈，同时保留了高性能所需的流式特性。

Result: 提出的FlashSign内核在A100上性能与FlashAttention相当，且是PyTorch的3.6倍，验证了神经电路图在高效AI架构开发中的实用性。

Conclusion: 神经电路图是一种有效的框架，可用于系统化开发高效、创新的人工智能架构。

Abstract: How do we enable artificial intelligence models to improve themselves? This
is central to exponentially improving generalized artificial intelligence
models, which can improve their own architecture to handle new problem domains
in an efficient manner that leverages the latest hardware. However, current
automated compilation methods are poor, and efficient algorithms require years
of human development. In this paper, we use neural circuit diagrams, based in
category theory, to prove a general theorem related to deep learning
algorithms, guide the development of a novel attention algorithm catered to the
domain of gene regulatory networks, and produce a corresponding efficient
kernel. The algorithm we propose, spherical attention, shows that neural
circuit diagrams enable a principled and systematic method for reasoning about
deep learning architectures and providing high-performance code. By replacing
SoftMax with an $L^2$ norm as suggested by diagrams, it overcomes the special
function unit bottleneck of standard attention while retaining the streaming
property essential to high-performance. Our diagrammatically derived
\textit{FlashSign} kernel achieves comparable performance to the
state-of-the-art, fine-tuned FlashAttention algorithm on an A100, and
$3.6\times$ the performance of PyTorch. Overall, this investigation shows
neural circuit diagrams' suitability as a high-level framework for the
automated development of efficient, novel artificial intelligence
architectures.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [149] [ARCANE -- Early Detection of Interplanetary Coronal Mass Ejections](https://arxiv.org/abs/2505.09365)
*H. T. Rüdisser,G. Nguyen,J. Le Louëdec,C. Möstl*

Main category: physics.space-ph

TL;DR: 该研究提出了ARCANE框架，用于实时检测太阳风数据中的行星际日冕物质抛射（ICMEs），通过比较机器学习模型和阈值基线方法，发现ResUNet++模型显著优于基线，并在实时数据中表现接近科学数据。


<details>
  <summary>Details</summary>
Motivation: ICMEs是空间天气扰动的主要驱动因素，对技术和人类活动构成风险。现有的实时检测方法仍面临挑战，因此需要开发一种能在不完整观测下识别事件的框架。

Method: 研究设计了ARCANE框架，采用ResUNet++模型与阈值基线方法对比，评估两种方法在实时太阳风数据（RTSW）与高分辨率科学数据中的表现。

Result: ResUNet++模型显著优于阈值基线，尤其是在高影响事件检测中。使用RTSW数据仅导致轻微性能下降。检测流水线的F1得分为0.53，平均延迟为事件时长的21.5%。

Conclusion: ARCANE框架在自动化空间天气监测中取得了重要进展，为实时预报能力奠定了基础。

Abstract: Interplanetary coronal mass ejections (ICMEs) are major drivers of space
weather disturbances, posing risks to both technological infrastructure and
human activities. Automatic detection of ICMEs in solar wind in situ data is
essential for early warning systems. While several methods have been proposed
to identify these structures in time series data, robust real-time detection
remains a significant challenge. In this work, we present ARCANE - the first
framework explicitly designed for early ICME detection in streaming solar wind
data under realistic operational constraints, enabling event identification
without requiring observation of the full structure. Our approach evaluates the
strengths and limitations of detection models by comparing a machine
learning-based method to a threshold-based baseline. The ResUNet++ model,
previously validated on science data, significantly outperforms the baseline,
particularly in detecting high-impact events, while retaining solid performance
on lower-impact cases. Notably, we find that using real-time solar wind (RTSW)
data instead of high-resolution science data leads to only minimal performance
degradation. Despite the challenges of operational settings, our detection
pipeline achieves an F1 score of 0.53, with an average detection delay of 21.5%
of the event's duration while only seeing a minimal amount of data. As more
data becomes available, the performance increases significantly. These results
mark a substantial step forward in automated space weather monitoring and lay
the groundwork for enhanced real-time forecasting capabilities.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [150] [Performance Gains of LLMs With Humans in a World of LLMs Versus Humans](https://arxiv.org/abs/2505.08902)
*Lucas McCullum,Pelagie Ami Agassi,Leo Anthony Celi,Daniel K. Ebner,Chrystinne Oliveira Fernandes,Rachel S. Hicklen,Mkliwa Koumbia,Lisa Soleymani Lehmann,David Restrepo*

Main category: cs.HC

TL;DR: 论文呼吁避免直接比较LLMs与人类专家，转而研究如何安全、高效地促进LLMs与人类在临床环境中的协作。


<details>
  <summary>Details</summary>
Motivation: 当前关于LLMs的研究过于关注与人类专家的对比，可能导致忽视其在临床应用中潜在的安全风险，需重新聚焦于人类与LLM的协作策略。

Method: 提出需制定策略，确保LLMs在快速迭代中仍能安全地辅助临床工作，而非简单替代人类。

Result: 强调应转向“人机协同”模式，而非对立比较，以保障患者安全和医疗质量。

Conclusion: 未来研究应优先开发LLMs在临床环境中的安全协同框架，适应其快速发展的特性。

Abstract: Currently, a considerable research effort is devoted to comparing LLMs to a
group of human experts, where the term "expert" is often ill-defined or
variable, at best, in a state of constantly updating LLM releases. Without
proper safeguards in place, LLMs will threaten to cause harm to the established
structure of safe delivery of patient care which has been carefully developed
throughout history to keep the safety of the patient at the forefront. A key
driver of LLM innovation is founded on community research efforts which, if
continuing to operate under "humans versus LLMs" principles, will expedite this
trend. Therefore, research efforts moving forward must focus on effectively
characterizing the safe use of LLMs in clinical settings that persist across
the rapid development of novel LLM models. In this communication, we
demonstrate that rather than comparing LLMs to humans, there is a need to
develop strategies enabling efficient work of humans with LLMs in an almost
symbiotic manner.

</details>


### [151] [WaLLM -- Insights from an LLM-Powered Chatbot deployment via WhatsApp](https://arxiv.org/abs/2505.08894)
*Hiba Eltigani,Rukhshan Haroon,Asli Kocak,Abdullah Bin Faisal,Noah Martin,Fahad Dogar*

Main category: cs.HC

TL;DR: 该论文介绍了WaLLM，一个基于WhatsApp的AI聊天机器人，旨在解决发展中地区的数字鸿沟问题。它提供多种互动功能，6个月内收集了14.7K查询，分析显示55%为事实查询，健康类最受欢迎。


<details>
  <summary>Details</summary>
Motivation: 发展中地区因数字鸿沟难以接触生成式AI技术，WaLLM通过普及的WhatsApp平台提供便捷的AI服务。

Method: 开发WaLLM并集成每日热点问题、建议追问、查询排行及奖励系统，分析6个月的用户日志。

Result: 55%查询为事实需求，健康主题占28%；使用排行榜的用户互动量高3倍。

Conclusion: 需文化定制、界面优化及合理校准用户对AI的信任，以适配发展中地区需求。

Abstract: Recent advances in generative AI, such as ChatGPT, have transformed access to
information in education, knowledge-seeking, and everyday decision-making.
However, in many developing regions, access remains a challenge due to the
persistent digital divide. To help bridge this gap, we developed WaLLM - a
custom AI chatbot over WhatsApp, a widely used communication platform in
developing regions. Beyond answering queries, WaLLM offers several features to
enhance user engagement: a daily top question, suggested follow-up questions,
trending and recent queries, and a leaderboard-based reward system. Our service
has been operational for over 6 months, amassing over 14.7K queries from
approximately 100 users. In this paper, we present WaLLM's design and a
systematic analysis of logs to understand user interactions. Our results show
that 55% of user queries seek factual information. "Health and well-being" was
the most popular topic (28%), including queries about nutrition and disease,
suggesting users view WaLLM as a reliable source. Two-thirds of users' activity
occurred within 24 hours of the daily top question. Users who accessed the
"Leaderboard" interacted with WaLLM 3x as those who did not. We conclude by
discussing implications for culture-based customization, user interface design,
and appropriate calibration of users' trust in AI systems for developing
regions.

</details>


### [152] [Tracing the Invisible: Understanding Students' Judgment in AI-Supported Design Work](https://arxiv.org/abs/2505.08939)
*Suchismita Naik,Prakash Shukla,Ike Obi,Jessica Backus,Nancy Rasche,Paul Parsons*

Main category: cs.HC

TL;DR: 研究了学生在设计课程中与生成AI合作的判断类型，发现了传统设计判断和新出现的代理分配与可靠性判断，揭示了AI带来的复杂性。


<details>
  <summary>Details</summary>
Motivation: 探讨学生如何在设计流程中与生成AI协作，并理解其判断类型，以揭示AI对设计思维的影响。

Method: 分析33个学生团队的反思内容，识别他们使用AI工具时的判断类型。

Result: 发现既有传统设计判断（工具性、鉴赏性、质量），也有新判断类型（代理分配与可靠性），表明AI增加了设计决策的复杂性。

Conclusion: 生成AI为设计推理引入新层次，学生需平衡创意责任与AI输出的可信度，研究为理解人机共创提供了概念框架。

Abstract: As generative AI tools become integrated into design workflows, students
increasingly engage with these tools not just as aids, but as collaborators.
This study analyzes reflections from 33 student teams in an HCI design course
to examine the kinds of judgments students make when using AI tools. We found
both established forms of design judgment (e.g., instrumental, appreciative,
quality) and emergent types: agency-distribution judgment and reliability
judgment. These new forms capture how students negotiate creative
responsibility with AI and assess the trustworthiness of its outputs. Our
findings suggest that generative AI introduces new layers of complexity into
design reasoning, prompting students to reflect not only on what AI produces,
but also on how and when to rely on it. By foregrounding these judgments, we
offer a conceptual lens for understanding how students engage in co-creative
sensemaking with AI in design contexts.

</details>


### [153] [PreCare: Designing AI Assistants for Advance Care Planning (ACP) to Enhance Personal Value Exploration, Patient Knowledge, and Decisional Confidence](https://arxiv.org/abs/2505.09115)
*Yu Lun Hsu,Yun-Rung Chou,Chiao-Ju Chang,Yu-Cheng Chang,Zer-Wei Lee,Rokas Gipiškis,Rachel Li,Chih-Yuan Shih,Jen-Kuei Peng,Hsien-Liang Huang,Jaw-Shiun Tsai,Mike Y. Chen*

Main category: cs.HC

TL;DR: 文章介绍了PreCare，这是一个通过AI辅助帮助用户进行预先医疗规划的网站，旨在弥补在线ACP缺乏临床咨询优势的不足。研究显示，PreCare在用户体验和效果上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在线预先医疗规划（ACP）虽然便捷，但缺乏临床咨询的个人化价值探讨和即时澄清决策后果的优势，研究旨在通过AI技术弥补这一差距。

Method: 通过两个形成性研究（观察访谈18名患者和14名网站用户）设计PreCare网站，并进行了可用性研究和对比评估（各12名参与者）。

Result: PreCare在系统可用性评分上表现优秀，AI助手显著提升了个人价值探索、知识获取和决策信心，92%的参与者更偏好PreCare。

Conclusion: PreCare通过AI辅助有效提升了在线ACP的体验和效果，为预先医疗规划提供了可行且优越的解决方案。

Abstract: Advance Care Planning (ACP) allows individuals to specify their preferred
end-of-life life-sustaining treatments before they become incapacitated by
injury or terminal illness (e.g., coma, cancer, dementia). While online ACP
offers high accessibility, it lacks key benefits of clinical consultations,
including personalized value exploration, immediate clarification of decision
consequences. To bridge this gap, we conducted two formative studies: 1)
shadowed and interviewed 3 ACP teams consisting of physicians, nurses, and
social workers (18 patients total), and 2) interviewed 14 users of ACP
websites. Building on these insights, we designed PreCare in collaboration with
6 ACP professionals. PreCare is a website with 3 AI-driven assistants designed
to guide users through exploring personal values, gaining ACP knowledge, and
supporting informed decision-making. A usability study (n=12) showed that
PreCare achieved a System Usability Scale (SUS) rating of excellent. A
comparative evaluation (n=12) showed that PreCare's AI assistants significantly
improved exploration of personal values, knowledge, and decisional confidence,
and was preferred by 92% of participants.

</details>


### [154] [An Initial Exploration of Default Images in Text-to-Image Generation](https://arxiv.org/abs/2505.09166)
*Hannu Simonen,Atte Kiviniemi,Jonas Oppenlaender*

Main category: cs.HC

TL;DR: 本文研究了文本到图像生成（TTI）中的默认图像现象，分析了其成因、影响及解决方案。


<details>
  <summary>Details</summary>
Motivation: 在TTI模型中，未知术语会触发默认图像生成，研究这一现象有助于改进模型和提示工程。

Method: 通过系统性方法构建触发默认图像的输入提示，并进行实验、消融研究和用户调查。

Result: 初步实验和研究表明默认图像对用户满意度有影响，并揭示了相关挑战。

Conclusion: 研究为理解TTI中的默认图像奠定了基础，并指明了未来研究方向。

Abstract: In the creative practice of text-to-image generation (TTI), images are
generated from text prompts. However, TTI models are trained to always yield an
output, even if the prompt contains unknown terms. In this case, the model may
generate what we call "default images": images that closely resemble each other
across many unrelated prompts. We argue studying default images is valuable for
designing better solutions for TTI and prompt engineering. In this paper, we
provide the first investigation into default images on Midjourney, a popular
image generator. We describe our systematic approach to create input prompts
triggering default images, and present the results of our initial experiments
and several small-scale ablation studies. We also report on a survey study
investigating how default images affect user satisfaction. Our work lays the
foundation for understanding default images in TTI and highlights challenges
and future research directions.

</details>


### [155] [Educational impacts of generative artificial intelligence on learning and performance of engineering students in China](https://arxiv.org/abs/2505.09208)
*Lei Fan,Kunyang Deng,Fangxue Liu*

Main category: cs.HC

TL;DR: 本文研究了148名中国工程学生使用生成式AI的情况，探讨了其学习体验的影响、应用场景及挑战，结果显示AI提高了学习效率和创造力，但也存在准确性和领域可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解生成式AI在工程教育中的实际应用效果及其对学生学习的潜在影响。

Method: 通过调查148名来自不同工程专业和地区的学生，分析AI的使用频率、场景、学习影响及挑战。

Result: 超过半数学生认为生成式AI提升了学习效率和创造力，但学术表现无明显变化，且对AI的准确性和可靠性存疑。

Conclusion: 生成式AI对工程教育有积极影响，但需解决准确性和可靠性问题，并优化整合策略。

Abstract: With the rapid advancement of generative artificial intelligence(AI), its
potential applications in higher education have attracted significant
attention. This study investigated how 148 students from diverse engineering
disciplines and regions across China used generative AI, focusing on its impact
on their learning experience and the opportunities and challenges it poses in
engineering education. Based on the surveyed data, we explored four key areas:
the frequency and application scenarios of AI use among engineering students,
its impact on students' learning and performance, commonly encountered
challenges in using generative AI, and future prospects for its adoption in
engineering education. The results showed that more than half of the
participants reported a positive impact of generative AI on their learning
efficiency, initiative, and creativity, with nearly half believing it also
enhanced their independent thinking. However, despite acknowledging improved
study efficiency, many felt their actual academic performance remained largely
unchanged and expressed concerns about the accuracy and domain-specific
reliability of generative AI. Our findings provide a first-hand insight into
the current benefits and challenges generative AI brings to students,
particularly Chinese engineering students, while offering several
recommendations, especially from the students' perspective, for effectively
integrating generative AI into engineering education.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [156] [Behind Maya: Building a Multilingual Vision Language Model](https://arxiv.org/abs/2505.08910)
*Nahid Alam,Karthik Reddy Kanjula,Surya Guthikonda,Timothy Chung,Bala Krishna S Vegesna,Abhipsha Das,Anthony Susevski,Ryan Sze-Yin Chan,S M Iftekhar Uddin,Shayekh Bin Islam,Roshan Santhosh,Snegha A,Drishti Sharma,Chen Liu,Isha Chaturvedi,Genta Indra Winata,Ashvanth. S,Snehanshu Mukherjee,Alham Fikri Aji*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为Maya的开源多语言视觉语言模型（VLM），旨在解决当前VLM在低资源语言和多样化文化背景下的性能不足问题。


<details>
  <summary>Details</summary>
Motivation: 当前的大型视觉语言模型主要在广泛使用的语言上表现优异，但在低资源语言和文化多样性背景下表现不佳。作者希望通过构建多语言的图像-文本预训练数据集和模型来弥补这一缺陷。

Method: 作者基于LLaVA预训练数据集，构建了一个包含八种语言的多语言图像-文本预训练数据集，并开发了支持这些语言的多语言图像-文本模型，以增强视觉语言任务中的文化和语言理解能力。

Result: Maya模型能够支持八种语言，提升了在低资源语言和文化多样性背景下的视觉语言任务表现。

Conclusion: 通过开源Maya模型和数据集，作者为多语言视觉语言理解提供了新的解决方案，并为未来的多语言和跨文化研究奠定了基础。

Abstract: In recent times, we have seen a rapid development of large Vision-Language
Models (VLMs). They have shown impressive results on academic benchmarks,
primarily in widely spoken languages but lack performance on low-resource
languages and varied cultural contexts. To address these limitations, we
introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a
multilingual image-text pretraining dataset in eight languages, based on the
LLaVA pretraining dataset; and 2) a multilingual image-text model supporting
these languages, enhancing cultural and linguistic comprehension in
vision-language tasks. Code available at https://github.com/nahidalam/maya.

</details>


### [157] [Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features](https://arxiv.org/abs/2505.08800)
*Olivia Nocentini,Marta Lagomarsino,Gokhan Solak,Younggeol Cho,Qiyi Tong,Marta Lorenzini,Arash Ajoudani*

Main category: cs.CV

TL;DR: 论文提出了一种基于行为分析的在线监测系统，使用定向图神经网络（DGNN）分类火车司机状态（警觉、非警觉、病理状态）。通过消融实验发现，结合面部和骨骼特征的模型在三分类任务中准确率最高（80.88%），二分类警觉性任务准确率超99%。此外，还引入了一个包含模拟病理状态的新数据集。


<details>
  <summary>Details</summary>
Motivation: 传统死机开关等系统对火车司机警觉性监测有限，需更先进的在线监测技术提升铁路安全。

Method: 采用定向图神经网络（DGNN），通过消融实验对比骨骼、面部及两者结合的特征配置，优化输入表示。

Result: 结合面部和骨骼特征的模型在三分类中准确率80.88%，二分类超99%，并创建了包含病理状态的新数据集。

Conclusion: 该研究通过视觉技术提升在线监测能力，为铁路安全提供了更全面的疲劳和健康风险评估方案。

Abstract: Driver fatigue poses a significant challenge to railway safety, with
traditional systems like the dead-man switch offering limited and basic
alertness checks. This study presents an online behavior-based monitoring
system utilizing a customised Directed-Graph Neural Network (DGNN) to classify
train driver's states into three categories: alert, not alert, and
pathological. To optimize input representations for the model, an ablation
study was performed, comparing three feature configurations: skeletal-only,
facial-only, and a combination of both. Experimental results show that
combining facial and skeletal features yields the highest accuracy (80.88%) in
the three-class model, outperforming models using only facial or skeletal
features. Furthermore, this combination achieves over 99% accuracy in the
binary alertness classification. Additionally, we introduced a novel dataset
that, for the first time, incorporates simulated pathological conditions into
train driver monitoring, broadening the scope for assessing risks related to
fatigue and health. This work represents a step forward in enhancing railway
safety through advanced online monitoring using vision-based technologies.

</details>


### [158] [Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training](https://arxiv.org/abs/2505.08971)
*Yangyi Chen,Hao Peng,Tong Zhang,Heng Ji*

Main category: cs.CV

TL;DR: PRIOR是一种改进的视觉语言预训练方法，通过差异加权优先处理与图像相关的token，减少了噪声拟合和幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 标准LVLMs预训练中，Naive NTP会无意中拟合噪声并增加幻觉风险，PRIOR旨在通过加权优化解决这一问题。

Method: PRIOR利用文本参考LLM对token进行重要性打分，并在NTP损失中调整权重，优先训练与视觉相关的token。

Result: 在两种LVLMs设置中，PRIOR分别实现了19%和8%的平均相对提升，并展示了更好的扩展性。

Conclusion: PRIOR通过差异加权有效提升了视觉语言模型的性能，且具有更强的扩展潜力。

Abstract: In standard large vision-language models (LVLMs) pre-training, the model
typically maximizes the joint probability of the caption conditioned on the
image via next-token prediction (NTP); however, since only a small subset of
caption tokens directly relates to the visual content, this naive NTP
unintentionally fits the model to noise and increases the risk of
hallucination. We present PRIOR, a simple vision-language pre-training approach
that addresses this issue by prioritizing image-related tokens through
differential weighting in the NTP loss, drawing from the importance sampling
framework. PRIOR introduces a reference model-a text-only large language model
(LLM) trained on the captions without image inputs, to weight each token based
on its probability for LVLMs training. Intuitively, tokens that are directly
related to the visual inputs are harder to predict without the image and thus
receive lower probabilities from the text-only reference LLM. During training,
we implement a token-specific re-weighting term based on the importance scores
to adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs
with visual encoders and LVLMs without visual encoders. We observe 19% and 8%
average relative improvement, respectively, on several vision-language
benchmarks compared to NTP. In addition, PRIOR exhibits superior scaling
properties, as demonstrated by significantly higher scaling coefficients,
indicating greater potential for performance gains compared to NTP given
increasing compute and data.

</details>


### [159] [SparseMeXT Unlocking the Potential of Sparse Representations for HD Map Construction](https://arxiv.org/abs/2505.08808)
*Anqing Jiang,Jinhao Chai,Yu Gao,Yiru Wang,Yuwen Heng,Zhigang Sun,Hao Sun,Zezhong Zhao,Li Sun,Jian Zhou,Lijuan Zhu,Shugong Xu,Hao Zhao*

Main category: cs.CV

TL;DR: 该论文通过系统改进稀疏表示技术，提出专用网络架构、稀疏-密集分割辅助任务和物理先验去噪模块，显著提升高精地图构建性能，在nuScenes数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏表示方法因缺乏针对性设计而性能不足，无法与密集表示竞争。论文旨在通过架构和算法改进，使稀疏表示超越密集方法。

Method: 提出稀疏地图特征提取专用网络架构、稀疏-密集分割辅助任务以及基于物理先验的去噪模块。

Result: SparseMeXt系列模型在nuScenes数据集上表现优异，Tiny版达55.5% mAP/32fps，Base版65.2% mAP，Large版68.9% mAP/20fps。

Conclusion: 稀疏方法潜力巨大，挑战了传统密集表示的依赖，重新定义了效率与性能的权衡。

Abstract: Recent advancements in high-definition \emph{HD} map construction have
demonstrated the effectiveness of dense representations, which heavily rely on
computationally intensive bird's-eye view \emph{BEV} features. While sparse
representations offer a more efficient alternative by avoiding dense BEV
processing, existing methods often lag behind due to the lack of tailored
designs. These limitations have hindered the competitiveness of sparse
representations in online HD map construction. In this work, we systematically
revisit and enhance sparse representation techniques, identifying key
architectural and algorithmic improvements that bridge the gap with--and
ultimately surpass--dense approaches. We introduce a dedicated network
architecture optimized for sparse map feature extraction, a sparse-dense
segmentation auxiliary task to better leverage geometric and semantic cues, and
a denoising module guided by physical priors to refine predictions. Through
these enhancements, our method achieves state-of-the-art performance on the
nuScenes dataset, significantly advancing HD map construction and centerline
detection. Specifically, SparseMeXt-Tiny reaches a mean average precision
\emph{mAP} of 55.5% at 32 frames per second \emph{fps}, while SparseMeXt-Base
attains 65.2% mAP. Scaling the backbone and decoder further, SparseMeXt-Large
achieves an mAP of 68.9% at over 20 fps, establishing a new benchmark for
sparse representations in HD map construction. These results underscore the
untapped potential of sparse methods, challenging the conventional reliance on
dense representations and redefining efficiency-performance trade-offs in the
field.

</details>


### [160] [Towards Understanding Deep Learning Model in Image Recognition via Coverage Test](https://arxiv.org/abs/2505.08814)
*Wenkai Li,Xiaoqi Li,Yingjie Mao,Yishun Wang*

Main category: cs.CV

TL;DR: 该论文研究了深度神经网络（DNN）安全测试中的四种覆盖度量之间的关系，并探讨模型深度、配置信息及数据集大小对其影响，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: DNN在人工智能中至关重要，但其安全测试缺乏对覆盖度量（如功能、边界、层次和结构覆盖）与模型深度、配置信息关系的实证研究。

Method: 通过选取LeNet、VGG和ResNet等不同架构及5至54层的模型，进行实验比较不同深度、配置信息与覆盖度量的关系，并分析修改后的决策/条件覆盖与数据集大小的关联。

Result: 实验揭示了不同覆盖度量与模型深度及配置的关联，并发现数据集大小对决策/条件覆盖的影响，为DNN安全测试提供了实证依据。

Conclusion: 研究填补了DNN覆盖度量实证研究的空白，提出了未来三个研究方向，助力DNN安全测试的进一步发展。

Abstract: Deep neural networks (DNNs) play a crucial role in the field of artificial
intelligence, and their security-related testing has been a prominent research
focus. By inputting test cases, the behavior of models is examined for
anomalies, and coverage metrics are utilized to determine the extent of neurons
covered by these test cases. With the widespread application and advancement of
DNNs, different types of neural behaviors have garnered attention, leading to
the emergence of various coverage metrics for neural networks. However, there
is currently a lack of empirical research on these coverage metrics,
specifically in analyzing the relationships and patterns between model depth,
configuration information, and neural network coverage. This paper aims to
investigate the relationships and patterns of four coverage metrics: primary
functionality, boundary, hierarchy, and structural coverage. A series of
empirical experiments were conducted, selecting LeNet, VGG, and ResNet as
different DNN architectures, along with 10 models of varying depths ranging
from 5 to 54 layers, to compare and study the relationships between different
depths, configuration information, and various neural network coverage metrics.
Additionally, an investigation was carried out on the relationships between
modified decision/condition coverage and dataset size. Finally, three potential
future directions are proposed to further contribute to the security testing of
DNN Models.

</details>


### [161] [Crowd Scene Analysis using Deep Learning Techniques](https://arxiv.org/abs/2505.08834)
*Muhammad Junaid Asif*

Main category: cs.CV

TL;DR: 论文提出了一种结合自监督训练和多列CNN的模型，解决了人群计数中的标注数据需求和高密度遮挡问题，同时在异常检测中采用VGG19和LSTM提取时空特征，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决人群计数中大规模标注数据需求及高密度场景下的遮挡问题，同时在异常检测中应对光照变化和复杂环境挑战。

Method: 人群计数采用自监督训练和多列CNN；异常检测结合VGG19（空间特征）和LSTM（时间特征），并引入密集残差块。

Result: 在ShanghaiTech、UCF-QNRF等数据集上MAE/MSE表现优异；异常检测模型在Hockey Fight和SCVD数据集上超越现有方法。

Conclusion: 所提模型在人群计数和异常检测任务中均有效，且通过自监督和时空特征提取提升了泛化能力。

Abstract: Our research is focused on two main applications of crowd scene analysis
crowd counting and anomaly detection In recent years a large number of
researches have been presented in the domain of crowd counting We addressed two
main challenges in this domain 1 Deep learning models are datahungry paradigms
and always need a large amount of annotated data for the training of algorithm
It is timeconsuming and costly task to annotate such large amount of data
Selfsupervised training is proposed to deal with this challenge 2 MCNN consists
of multicolumns of CNN with different sizes of filters by presenting a novel
approach based on a combination of selfsupervised training and MultiColumn CNN
This enables the model to learn features at different levels and makes it
effective in dealing with challenges of occluded scenes nonuniform density
complex backgrounds and scale invariation The proposed model was evaluated on
publicly available data sets such as ShanghaiTech and UCFQNRF by means of MAE
and MSE A spatiotemporal model based on VGG19 is proposed for crowd anomaly
detection addressing challenges like lighting environmental conditions
unexpected objects and scalability The model extracts spatial and temporal
features allowing it to be generalized to realworld scenes Spatial features are
learned using CNN while temporal features are learned using LSTM blocks The
model works on binary classification and can detect normal or abnormal behavior
The models performance is improved by replacing fully connected layers with
dense residual blocks Experiments on the Hockey Fight dataset and SCVD dataset
show our models outperform other stateoftheart approaches

</details>


### [162] [Generative AI for Autonomous Driving: Frontiers and Opportunities](https://arxiv.org/abs/2505.08854)
*Yuping Wang,Shuo Xing,Cui Can,Renjie Li,Hongyuan Hua,Kexin Tian,Zhaobin Mo,Xiangbo Gao,Keshu Wu,Sulong Zhou,Hengxu You,Juntong Peng,Junge Zhang,Zehao Wang,Rui Song,Mingxuan Yan,Walter Zimmer,Xingcheng Zhou,Peiran Li,Zhaohan Lu,Chia-Ju Chen,Yue Huang,Ryan A. Rossi,Lichao Sun,Hongkai Yu,Zhiwen Fan,Frank Hao Yang,Yuhao Kang,Ross Greer,Chenxi Liu,Eun Hak Lee,Xuan Di,Xinyue Ye,Liu Ren,Alois Knoll,Xiaopeng Li,Shuiwang Ji,Masayoshi Tomizuka,Marco Pavone,Tianbao Yang,Jing Du,Ming-Hsuan Yang,Hua Wei,Ziran Wang,Yang Zhou,Jiachen Li,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 本文综述了生成式人工智能（GenAI）在自动驾驶领域的应用，包括其原理、前沿技术、实际应用、挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨GenAI如何通过其在内容生成、推理和多模态理解方面的能力，推动自动驾驶技术向Level 5全自主驾驶的突破。

Method: 分析了生成模型（如VAEs、GANs、Diffusion Models和LLMs）的原理及其在图像、LiDAR、轨迹等生成中的应用，并分类讨论了实际应用案例。

Result: 总结了GenAI在自动驾驶中的前沿应用及关键挑战，如泛化能力、安全性、预算限制和伦理问题，并提出了未来研究方向。

Conclusion: GenAI为自动驾驶的突破提供了巨大潜力，但仍需解决多方面的技术和非技术挑战才能实现广泛应用。

Abstract: Generative Artificial Intelligence (GenAI) constitutes a transformative
technological wave that reconfigures industries through its unparalleled
capabilities for content creation, reasoning, planning, and multimodal
understanding. This revolutionary force offers the most promising path yet
toward solving one of engineering's grandest challenges: achieving reliable,
fully autonomous driving, particularly the pursuit of Level 5 autonomy. This
survey delivers a comprehensive and critical synthesis of the emerging role of
GenAI across the autonomous driving stack. We begin by distilling the
principles and trade-offs of modern generative modeling, encompassing VAEs,
GANs, Diffusion Models, and Large Language Models (LLMs). We then map their
frontier applications in image, LiDAR, trajectory, occupancy, video generation
as well as LLM-guided reasoning and decision making. We categorize practical
applications, such as synthetic data workflows, end-to-end driving strategies,
high-fidelity digital twin systems, smart transportation networks, and
cross-domain transfer to embodied AI. We identify key obstacles and
possibilities such as comprehensive generalization across rare cases,
evaluation and safety checks, budget-limited implementation, regulatory
compliance, ethical concerns, and environmental effects, while proposing
research plans across theoretical assurances, trust metrics, transport
integration, and socio-technical influence. By unifying these threads, the
survey provides a forward-looking reference for researchers, engineers, and
policymakers navigating the convergence of generative AI and advanced
autonomous mobility. An actively maintained repository of cited works is
available at https://github.com/taco-group/GenAI4AD.

</details>


### [163] [OptiGait-LGBM: An Efficient Approach of Gait-based Person Re-identification in Non-Overlapping Regions](https://arxiv.org/abs/2505.08801)
*Md. Sakib Hassan Chowdhury,Md. Hafiz Ahamed,Bishowjit Paul,Sarafat Hussain Abhi,Abu Bakar Siddique,Md. Robius Sany*

Main category: cs.CV

TL;DR: 本文提出了一种名为OptiGait-LGBM的模型，用于在复杂无约束的室外环境下通过骨骼模型实现步态识别，解决了现有方法在真实场景中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视频的步态识别系统在大规模公共数据集上表现良好，但在真实无约束的步态数据中性能下降。主要挑战包括室外环境不可控、摄像头视角不重叠、光照变化和计算效率。目前没有数据集能同时解决这些问题。

Method: 使用骨骼模型方法，从关键点位置构建数据集，最小化内存使用，并开发了一种名为OptiGait-LGBM的步态分类模型。同时，引入了一个名为RUET-GAIT的基准数据集，用于表示复杂室外环境中的无约束步态序列。

Result: 实验结果表明，OptiGait-LGBM在准确性、内存使用和训练时间上优于其他集成技术（如随机森林和CatBoost）。

Conclusion: 该方法为真实场景中的视频步态识别提供了一种新颖、低成本且内存高效的解决方案。

Abstract: Gait recognition, known for its ability to identify individuals from a
distance, has gained significant attention in recent times due to its
non-intrusive verification. While video-based gait identification systems
perform well on large public datasets, their performance drops when applied to
real-world, unconstrained gait data due to various factors. Among these,
uncontrolled outdoor environments, non-overlapping camera views, varying
illumination, and computational efficiency are core challenges in gait-based
authentication. Currently, no dataset addresses all these challenges
simultaneously. In this paper, we propose an OptiGait-LGBM model capable of
recognizing person re-identification under these constraints using a skeletal
model approach, which helps mitigate inconsistencies in a person's appearance.
The model constructs a dataset from landmark positions, minimizing memory usage
by using non-sequential data. A benchmark dataset, RUET-GAIT, is introduced to
represent uncontrolled gait sequences in complex outdoor environments. The
process involves extracting skeletal joint landmarks, generating numerical
datasets, and developing an OptiGait-LGBM gait classification model. Our aim is
to address the aforementioned challenges with minimal computational cost
compared to existing methods. A comparative analysis with ensemble techniques
such as Random Forest and CatBoost demonstrates that the proposed approach
outperforms them in terms of accuracy, memory usage, and training time. This
method provides a novel, low-cost, and memory-efficient video-based gait
recognition solution for real-world scenarios.

</details>


### [164] [Towards SFW sampling for diffusion models via external conditioning](https://arxiv.org/abs/2505.08817)
*Camilo Carvajal Reyes,Joaquín Fontbona,Felipe Tobar*

Main category: cs.CV

TL;DR: 本文提出了一种基于外部多模态模型的SFW采样器，用于引导分数生成模型（SBM）避免生成不适宜内容（NSFW），无需微调且支持用户自定义NSFW类别，实验表明其效果与微调方法相当且对图像质量影响较小。


<details>
  <summary>Details</summary>
Motivation: 尽管SBM在图像合成中表现卓越，但其可能生成不适宜内容（如暴力或非自愿裸露）。现有方法多依赖于模型自身知识且需微调，本文探索利用外部多模态模型指导生成安全内容。

Method: 提出SFW采样器，通过**条件轨迹校正**步骤，利用CLIP等外部模型引导样本远离不适宜区域，并支持用户自定义NSFW类别。

Result: 在Stable Diffusion上的实验表明，SFW采样器显著减少不适宜内容生成，效果与微调方法相当，且对无需校正的样本影响可忽略。

Conclusion: SFW采样器验证了模型无关条件引导的潜力，为对齐SBM模型提供了一种有效且灵活的安全生成方案。

Abstract: Score-based generative models (SBM), also known as diffusion models, are the
de facto state of the art for image synthesis. Despite their unparalleled
performance, SBMs have recently been in the spotlight for being tricked into
creating not-safe-for-work (NSFW) content, such as violent images and
non-consensual nudity. Current approaches that prevent unsafe generation are
based on the models' own knowledge, and the majority of them require
fine-tuning. This article explores the use of external sources for ensuring
safe outputs in SBMs. Our safe-for-work (SFW) sampler implements a Conditional
Trajectory Correction step that guides the samples away from undesired regions
in the ambient space using multimodal models as the source of conditioning.
Furthermore, using Contrastive Language Image Pre-training (CLIP), our method
admits user-defined NSFW classes, which can vary in different settings. Our
experiments on the text-to-image SBM Stable Diffusion validate that the
proposed SFW sampler effectively reduces the generation of explicit content
while being competitive with other fine-tuning-based approaches, as assessed
via independent NSFW detectors. Moreover, we evaluate the impact of the SFW
sampler on image quality and show that the proposed correction scheme comes at
a minor cost with negligible effect on samples not needing correction. Our
study confirms the suitability of the SFW sampler towards aligned SBM models
and the potential of using model-agnostic conditioning for the prevention of
unwanted images.

</details>


### [165] [Generative AI for Urban Planning: Synthesizing Satellite Imagery via Diffusion Models](https://arxiv.org/abs/2505.08833)
*Qingyi Wang,Yuebing Liang,Yunhan Zheng,Kaiyuan Xu,Jinhua Zhao,Shenhao Wang*

Main category: cs.CV

TL;DR: 该研究通过改进Stable Diffusion模型（结合ControlNet），利用OpenStreetMap数据生成高保真卫星图像，展示了生成式AI在城市规划中的潜力，尤其在设计与公众参与方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以生成大规模、现实的城市场景布局。研究旨在利用生成式AI提升城市规划的自动化水平，支持灵活设计探索。

Method: 结合Stable Diffusion与ControlNet，输入土地用途描述、基础设施等条件；通过OpenStreetMap链接卫星图像与结构化数据；在三个美国城市验证模型。

Result: 模型生成多样且真实的城市场景，FID/KID分数高；语言提示和控制图像的调整显著影响输出质量；生成图像被规划师和公众偏好超过真实图像。

Conclusion: 研究为可控城市图像生成设定了基准，证明生成式AI可优化规划流程和公众参与。

Abstract: Generative AI offers new opportunities for automating urban planning by
creating site-specific urban layouts and enabling flexible design exploration.
However, existing approaches often struggle to produce realistic and practical
designs at scale. Therefore, we adapt a state-of-the-art Stable Diffusion
model, extended with ControlNet, to generate high-fidelity satellite imagery
conditioned on land use descriptions, infrastructure, and natural environments.
To overcome data availability limitations, we spatially link satellite imagery
with structured land use and constraint information from OpenStreetMap. Using
data from three major U.S. cities, we demonstrate that the proposed diffusion
model generates realistic and diverse urban landscapes by varying land-use
configurations, road networks, and water bodies, facilitating cross-city
learning and design diversity. We also systematically evaluate the impacts of
varying language prompts and control imagery on the quality of satellite
imagery generation. Our model achieves high FID and KID scores and demonstrates
robustness across diverse urban contexts. Qualitative assessments from urban
planners and the general public show that generated images align closely with
design descriptions and constraints, and are often preferred over real images.
This work establishes a benchmark for controlled urban imagery generation and
highlights the potential of generative AI as a tool for enhancing planning
workflows and public engagement.

</details>


### [166] [WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes](https://arxiv.org/abs/2505.09129)
*Wei Meng*

Main category: cs.CV

TL;DR: 论文提出了一种基于颜色特征的轻量级异常检测框架，用于资源有限且数据敏感的战术任务中快速识别潜在威胁事件。方法结合无监督KMeans聚类和RGB通道直方图建模，成功在无法访问原始数据的条件下检测到异常帧。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在高风险安全任务中面对无标签、数据不可利用的视频环境时存在挑战，需一种轻量且高效的异常检测方法以适应战术任务需求。

Method: 采用无监督KMeans聚类与RGB通道直方图建模结合的方法，对关键帧进行结构异常和颜色突变信号的复合检测。

Result: 实验成功识别了与高能光源、目标出现及反射干扰相关的异常帧，验证了该方法在战术预警、可疑对象筛选和环境突变监测中的有效性。

Conclusion: 研究强调了颜色特征作为低语义战场信号载体的重要性，未来将通过结合图神经网络和时间建模进一步扩展其战场智能感知能力。

Abstract: The deployment of traditional deep learning models in high-risk security
tasks in an unlabeled, data-non-exploitable video intelligence environment
faces significant challenges. In this paper, we propose a lightweight anomaly
detection framework based on color features for surveillance video clips in a
high sensitivity tactical mission, aiming to quickly identify and interpret
potential threat events under resource-constrained and data-sensitive
conditions. The method fuses unsupervised KMeans clustering with RGB channel
histogram modeling to achieve composite detection of structural anomalies and
color mutation signals in key frames. The experiment takes an operation
surveillance video occurring in an African country as a research sample, and
successfully identifies multiple highly anomalous frames related to high-energy
light sources, target presence, and reflective interference under the condition
of no access to the original data. The results show that this method can be
effectively used for tactical assassination warning, suspicious object
screening and environmental drastic change monitoring with strong deployability
and tactical interpretation value. The study emphasizes the importance of color
features as low semantic battlefield signal carriers, and its battlefield
intelligent perception capability will be further extended by combining graph
neural networks and temporal modeling in the future.

</details>


### [167] [Optimizing Neuro-Fuzzy and Colonial Competition Algorithms for Skin Cancer Diagnosis in Dermatoscopic Images](https://arxiv.org/abs/2505.08886)
*Hamideh Khaleghpour,Brett McKinney*

Main category: cs.CV

TL;DR: 该研究通过结合图像处理和机器学习算法（神经模糊和殖民竞争方法），在皮肤癌诊断领域取得94%的准确率，展示了AI在早期黑色素瘤检测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌发病率上升，公众意识不足和临床专业知识短缺，迫切需要先进的诊断辅助工具，AI在此领域显示出巨大潜力。

Method: 研究采用图像处理技术和机器学习算法（神经模糊和殖民竞争方法）对ISIC数据库中的皮肤镜图像进行分析。

Result: 在560张图像的数据集上，该方法达到了94%的准确率。

Conclusion: 该方法在皮肤癌早期诊断中表现出显著潜力，有助于提升临床诊断效果。

Abstract: The rising incidence of skin cancer, coupled with limited public awareness
and a shortfall in clinical expertise, underscores an urgent need for advanced
diagnostic aids. Artificial Intelligence (AI) has emerged as a promising tool
in this domain, particularly for distinguishing malignant from benign skin
lesions. Leveraging publicly available datasets of skin lesions, researchers
have been developing AI-based diagnostic solutions. However, the integration of
such computer systems in clinical settings is still nascent. This study aims to
bridge this gap by employing a fusion of image processing techniques and
machine learning algorithms, specifically neuro-fuzzy and colonial competition
approaches. Applied to dermoscopic images from the ISIC database, our method
achieved a notable accuracy of 94% on a dataset of 560 images. These results
underscore the potential of our approach in aiding clinicians in the early
detection of melanoma, thereby contributing significantly to skin cancer
diagnostics.

</details>


### [168] [Learning Cocoercive Conservative Denoisers via Helmholtz Decomposition for Poisson Inverse Problems](https://arxiv.org/abs/2505.08909)
*Deliang Wei,Peng Chen,Haobo Xu,Jiale Yao,Fang Li,Tieyong Zeng*

Main category: cs.CV

TL;DR: 该论文提出了一种名为CoCo的保守可协变去噪器，改进了传统PnP方法在泊松逆问题中的局限性，通过新型训练策略确保收敛性和去噪性能的提升。


<details>
  <summary>Details</summary>
Motivation: 传统PnP方法要求强凸性或平滑性及非扩展性去噪器，但这些假设在泊松逆问题中不成立且限制了去噪效果。

Method: 提出CoCo去噪器，利用广义亥姆霍兹分解结合哈密顿正则化和谱正则化的训练策略，确保其保守性和可协变性。

Result: 实验表明，该方法在视觉质量和定量指标上优于相关方法，且理论证明了其全局收敛性。

Conclusion: CoCo去噪器通过弱凸隐式先验，为PnP方法在复杂逆问题中提供了更优的解决方案。

Abstract: Plug-and-play (PnP) methods with deep denoisers have shown impressive results
in imaging problems. They typically require strong convexity or smoothness of
the fidelity term and a (residual) non-expansive denoiser for convergence.
These assumptions, however, are violated in Poisson inverse problems, and
non-expansiveness can hinder denoising performance. To address these
challenges, we propose a cocoercive conservative (CoCo) denoiser, which may be
(residual) expansive, leading to improved denoising. By leveraging the
generalized Helmholtz decomposition, we introduce a novel training strategy
that combines Hamiltonian regularization to promote conservativeness and
spectral regularization to ensure cocoerciveness. We prove that CoCo denoiser
is a proximal operator of a weakly convex function, enabling a restoration
model with an implicit weakly convex prior. The global convergence of PnP
methods to a stationary point of this restoration model is established.
Extensive experimental results demonstrate that our approach outperforms
closely related methods in both visual quality and quantitative metrics.

</details>


### [169] [DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection](https://arxiv.org/abs/2505.09168)
*Jianlin Sun,Xiaolin Fang,Juwei Guan,Dongdong Gui,Teqi Wang,Tongxin Zhu*

Main category: cs.CV

TL;DR: DRRNet通过四阶段架构解决伪装目标检测中目标与背景相似性问题，结合全局与局部特征，引入反向细化模块，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决伪装目标检测中因目标与背景在颜色、纹理、形状上的高度相似性导致的边缘细节丢失或背景干扰问题。

Method: 提出DRRNet四阶段架构，包括全上下文特征提取模块、局部细节提取模块、双表征融合模块及反向细化模块，结合全局语义与局部细节。

Result: 实验结果表明，DRRNet在基准数据集上显著优于现有方法。

Conclusion: DRRNet通过融合多尺度特征与逆向细化，有效抑制背景干扰并增强目标边界连续性，为伪装目标检测提供了新思路。

Abstract: The core challenge in Camouflage Object Detection (COD) lies in the
indistinguishable similarity between targets and backgrounds in terms of color,
texture, and shape. This causes existing methods to either lose edge details
(such as hair-like fine structures) due to over-reliance on global semantic
information or be disturbed by similar backgrounds (such as vegetation
patterns) when relying solely on local features. We propose DRRNet, a
four-stage architecture characterized by a "context-detail-fusion-refinement"
pipeline to address these issues. Specifically, we introduce an Omni-Context
Feature Extraction Module to capture global camouflage patterns and a Local
Detail Extraction Module to supplement microstructural information for the
full-scene context module. We then design a module for forming dual
representations of scene understanding and structural awareness, which fuses
panoramic features and local features across various scales. In the decoder, we
also introduce a reverse refinement module that leverages spatial edge priors
and frequency-domain noise suppression to perform a two-stage inverse
refinement of the output. By applying two successive rounds of inverse
refinement, the model effectively suppresses background interference and
enhances the continuity of object boundaries. Experimental results demonstrate
that DRRNet significantly outperforms state-of-the-art methods on benchmark
datasets. Our code is available at https://github.com/jerrySunning/DRRNet.

</details>


### [170] [Differentiable Channel Selection in Self-Attention For Person Re-Identification](https://arxiv.org/abs/2505.08961)
*Yancheng Wang,Nebojsa Jojic,Yingzhen Yang*

Main category: cs.CV

TL;DR: 提出了一种新的可微分通道选择注意力模块（DCS-Attention），通过选择信息丰富的通道来增强注意力权重计算，显著提升了行人重识别任务的性能。


<details>
  <summary>Details</summary>
Motivation: 受信息瓶颈（IB）原理启发，希望通过选择最具信息量的通道来优化特征提取，提高模型在行人重识别任务中的表现。

Method: 设计了DCS-Attention模块，支持固定主干网络和可学习主干网络（DNAS），并推导了IB损失的上界以便通过SGD优化。

Result: 在多个行人重识别基准测试中，使用DCS-Attention的模型显著提升了预测准确率，验证了其有效性。

Conclusion: DCS-Attention模块能够有效识别关键特征通道，为行人重识别任务提供了先进的解决方案。

Abstract: In this paper, we propose a novel attention module termed the Differentiable
Channel Selection Attention module, or the DCS-Attention module. In contrast
with conventional self-attention, the DCS-Attention module features selection
of informative channels in the computation of the attention weights. The
selection of the feature channels is performed in a differentiable manner,
enabling seamless integration with DNN training. Our DCS-Attention is
compatible with either fixed neural network backbones or learnable backbones
with Differentiable Neural Architecture Search (DNAS), leading to DCS with
Fixed Backbone (DCS-FB) and DCS-DNAS, respectively. Importantly, our
DCS-Attention is motivated by the principle of Information Bottleneck (IB), and
a novel variational upper bound for the IB loss, which can be optimized by SGD,
is derived and incorporated into the training loss of the networks with the
DCS-Attention modules. In this manner, a neural network with DCS-Attention
modules is capable of selecting the most informative channels for feature
extraction so that it enjoys state-of-the-art performance for the Re-ID task.
Extensive experiments on multiple person Re-ID benchmarks using both DCS-FB and
DCS-DNAS show that DCS-Attention significantly enhances the prediction accuracy
of DNNs for person Re-ID, which demonstrates the effectiveness of DCS-Attention
in learning discriminative features critical to identifying person identities.
The code of our work is available at
https://github.com/Statistical-Deep-Learning/DCS-Attention.

</details>


### [171] [Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation](https://arxiv.org/abs/2505.09263)
*Guan Gui,Bin-Bin Gao,Jun Liu,Chengjie Wang,Yunsheng Wu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为AnoGen的少样本异常生成方法，通过扩散模型生成逼真且多样的异常数据，以提升异常检测模型的性能，实验表明其在MVTec数据集上显著提高了分类和分割任务的性能。


<details>
  <summary>Details</summary>
Motivation: 工业检测中异常样本稀缺，现有方法通过合成异常数据存在语义差距问题，导致检测性能不佳。本文旨在通过少量真实异常生成逼真异常数据，提升模型训练效果。

Method: 提出AnoGen方法，分三阶段：1) 学习异常分布并嵌入知识；2) 利用嵌入和边界框引导扩散模型生成异常；3) 采用弱监督异常检测方法训练模型。基于DRAEM和DesTSeg模型，在MVTec数据集上验证。

Result: 生成的异常数据显著提升模型性能，DRAEM和DesTSeg在分割任务的AU-PR指标上分别提升5.8%和1.5%。

Conclusion: AnoGen方法能有效利用少量真实异常生成高质量异常数据，显著提升异常检测任务性能，为工业检测提供实用解决方案。

Abstract: Anomaly detection is a practical and challenging task due to the scarcity of
anomaly samples in industrial inspection. Some existing anomaly detection
methods address this issue by synthesizing anomalies with noise or external
data. However, there is always a large semantic gap between synthetic and
real-world anomalies, resulting in weak performance in anomaly detection. To
solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen)
method, which guides the diffusion model to generate realistic and diverse
anomalies with only a few real anomalies, thereby benefiting training anomaly
detection models. Specifically, our work is divided into three stages. In the
first stage, we learn the anomaly distribution based on a few given real
anomalies and inject the learned knowledge into an embedding. In the second
stage, we use the embedding and given bounding boxes to guide the diffusion
model to generate realistic and diverse anomalies on specific objects (or
textures). In the final stage, we propose a weakly-supervised anomaly detection
method to train a more powerful model with generated anomalies. Our method
builds upon DRAEM and DesTSeg as the foundation model and conducts experiments
on the commonly used industrial anomaly detection dataset, MVTec. The
experiments demonstrate that our generated anomalies effectively improve the
model performance of both anomaly classification and segmentation tasks
simultaneously, \eg, DRAEM and DseTSeg achieved a 5.8\% and 1.5\% improvement
in AU-PR metric on segmentation task, respectively. The code and generated
anomalous data are available at https://github.com/gaobb/AnoGen.

</details>


### [172] [Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction](https://arxiv.org/abs/2505.09018)
*Adarsh Kumar*

Main category: cs.CV

TL;DR: 该研究提出了一种结合CGM数据、人口统计/微生物组信息和餐前食物图像的多模态深度学习框架，用于改进对2型糖尿病患者的热量摄入估算，其误差比基线模型降低了50%以上。


<details>
  <summary>Details</summary>
Motivation: 精准估算热量摄入对管理2型糖尿病至关重要，但现有技术（如连续血糖监测仪）因个体和餐食差异难以全面捕捉营养信息，需多模态数据提升准确性。

Method: 采用多模态深度学习框架：基于注意力的编码和卷积特征提取处理食物图像，多层感知器处理CGM和微生物组数据，最后通过后期融合策略联合推理。

Result: 在40多名参与者的数据集上测试，模型的热量估算均方根相对误差（RMSRE）为0.2544，比基线模型性能提升超50%。

Conclusion: 多模态感知技术能显著提升自动化饮食评估工具的性能，对慢性病管理具有潜力。

Abstract: Effective dietary monitoring is critical for managing Type 2 diabetes, yet
accurately estimating caloric intake remains a major challenge. While
continuous glucose monitors (CGMs) offer valuable physiological data, they
often fall short in capturing the full nutritional profile of meals due to
inter-individual and meal-specific variability. In this work, we introduce a
multimodal deep learning framework that jointly leverages CGM time-series data,
Demographic/Microbiome, and pre-meal food images to enhance caloric estimation.
Our model utilizes attention based encoding and a convolutional feature
extraction for meal imagery, multi-layer perceptrons for CGM and Microbiome
data followed by a late fusion strategy for joint reasoning. We evaluate our
approach on a curated dataset of over 40 participants, incorporating
synchronized CGM, Demographic and Microbiome data and meal photographs with
standardized caloric labels. Our model achieves a Root Mean Squared Relative
Error (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These
findings demonstrate the potential of multimodal sensing to improve automated
dietary assessment tools for chronic disease management.

</details>


### [173] [Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt](https://arxiv.org/abs/2505.09264)
*Bin-Bin Gao*

Main category: cs.CV

TL;DR: 该论文提出了一种名为OneNIP的新方法，通过仅使用一张正常图像提示（OneNIP）来重构正常特征并修复异常特征，从而在统一异常检测任务中实现了更高的效率和泛化能力。此外，论文还提出了一种监督细化器来优化重构误差，显著提升了像素级异常分割的准确性。该方法在多个工业异常检测基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于自注意力变换器的无监督重构网络虽然在多类异常检测中表现出色，但存在对异常特征重构不准确的问题，且由于在低空间分辨率隐空间中进行重构，导致异常分割不精确。因此，作者旨在提出一种更高效且泛化能力更强的方法来解决这些问题。

Method: 论文提出了OneNIP方法，通过仅使用一张正常图像提示来重构正常特征并修复异常特征，从而增强统一异常检测的泛化能力。此外，还引入了一个监督细化器，利用真实正常图像和合成的异常图像来回归重构误差，以提升像素级异常分割的准确性。

Result: OneNIP在MVTec、BTAD和VisA三个工业异常检测基准测试中优于之前的方法，证明了其在统一异常检测任务中的优越性能。

Conclusion: OneNIP是一种简单而有效的方法，不仅提高了异常检测的效率，还显著提升了像素级异常分割的准确性，为统一异常检测任务提供了一种新的解决方案。

Abstract: Unsupervised reconstruction networks using self-attention transformers have
achieved state-of-the-art performance for multi-class (unified) anomaly
detection with a single model. However, these self-attention reconstruction
models primarily operate on target features, which may result in perfect
reconstruction for both normal and anomaly features due to high consistency
with context, leading to failure in detecting anomalies. Additionally, these
models often produce inaccurate anomaly segmentation due to performing
reconstruction in a low spatial resolution latent space. To enable
reconstruction models enjoying high efficiency while enhancing their
generalization for unified anomaly detection, we propose a simple yet effective
method that reconstructs normal features and restores anomaly features with
just One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP
allows for the first time to reconstruct or restore anomalies with just one
normal image prompt, effectively boosting unified anomaly detection
performance. Furthermore, we propose a supervised refiner that regresses
reconstruction errors by using both real normal and synthesized anomalous
images, which significantly improves pixel-level anomaly segmentation. OneNIP
outperforms previous methods on three industry anomaly detection benchmarks:
MVTec, BTAD, and VisA. The code and pre-trained models are available at
https://github.com/gaobb/OneNIP.

</details>


### [174] [MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning](https://arxiv.org/abs/2505.09265)
*Bin-Bin Gao*

Main category: cs.CV

TL;DR: 论文提出了一种基于纯视觉基础模型的新型通用视觉异常分割方法（MetaUAS），通过统一异常分割与变化分割的范式，利用合成图像对进行训练，无需依赖语言提示或特定异常检测数据集，实现了高效且通用的异常分割效果。


<details>
  <summary>Details</summary>
Motivation: 探索纯视觉基础模型替代现有依赖语言提示的视觉-语言模型，以解决视觉表示与语言无关的问题，并实现无需预训练语言模型的通用异常分割。

Method: 提出MetaUAS框架，通过将异常分割统一为变化分割范式，利用合成图像对进行元学习训练，并引入软特征对齐模块处理几何变化。

Result: MetaUAS在零样本、少样本甚至全样本异常分割任务中均显著优于现有方法，且仅需一张正常图像提示即可高效分割任意异常。

Conclusion: 论文首次实现了无需预训练视觉-语言模型和特定异常数据集的通用异常分割，为纯视觉模型在异常检测领域的应用提供了新思路。

Abstract: Zero- and few-shot visual anomaly segmentation relies on powerful
vision-language models that detect unseen anomalies using manually designed
textual prompts. However, visual representations are inherently independent of
language. In this paper, we explore the potential of a pure visual foundation
model as an alternative to widely used vision-language models for universal
visual anomaly segmentation. We present a novel paradigm that unifies anomaly
segmentation into change segmentation. This paradigm enables us to leverage
large-scale synthetic image pairs, featuring object-level and local region
changes, derived from existing image datasets, which are independent of target
anomaly datasets. We propose a one-prompt Meta-learning framework for Universal
Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and
then generalizes well to segment any novel or unseen visual anomalies in the
real world. To handle geometrical variations between prompt and query images,
we propose a soft feature alignment module that bridges paired-image change
perception and single-image semantic segmentation. This is the first work to
achieve universal anomaly segmentation using a pure vision model without
relying on special anomaly detection datasets and pre-trained visual-language
models. Our method effectively and efficiently segments any anomalies with only
one normal image prompt and enjoys training-free without guidance from
language. Our MetaUAS significantly outperforms previous zero-shot, few-shot,
and even full-shot anomaly segmentation methods. The code and pre-trained
models are available at https://github.com/gaobb/MetaUAS.

</details>


### [175] [Neural Video Compression using 2D Gaussian Splatting](https://arxiv.org/abs/2505.09324)
*Lakshya Gupta,Imran N. Junejo*

Main category: cs.CV

TL;DR: 论文提出了一种基于感兴趣区域(ROI)和2D高斯撒点的神经视频压缩模型，显著提升了编码速度，为实时视频应用如视频会议提供了高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统视频编解码器依赖手工特征，而神经视频编解码器(NVC)虽能学习内容感知的压缩策略，但计算需求高，限制了实时应用。本研究旨在通过2D高斯撒点技术解决这一问题。

Method: 采用2D高斯撒点技术，结合内容感知初始化策略和帧间冗余减少机制，设计了首个高斯撒点视频编解码解决方案。

Result: 新方法将编码时间比之前基于高斯撒点的图像编解码器提升了88%，证明了其实时解码能力和高效性。

Conclusion: 该研究为神经视频编解码领域提供了创新性解决方案，尤其在实时视频应用中展现出巨大潜力。

Abstract: The computer vision and image processing research community has been involved
in standardizing video data communications for the past many decades, leading
to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent
groundbreaking works have focused on employing deep learning-based techniques
to replace the traditional video codec pipeline to a greater affect. Neural
video codecs (NVC) create an end-to-end ML-based solution that does not rely on
any handcrafted features (motion or edge-based) and have the ability to learn
content-aware compression strategies, offering better adaptability and higher
compression efficiency than traditional methods. This holds a great potential
not only for hardware design, but also for various video streaming platforms
and applications, especially video conferencing applications such as MS-Teams
or Zoom that have found extensive usage in classrooms and workplaces. However,
their high computational demands currently limit their use in real-time
applications like video conferencing. To address this, we propose a
region-of-interest (ROI) based neural video compression model that leverages 2D
Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable
of real-time decoding and can be optimized using fewer data points, requiring
only thousands of Gaussians for decent quality outputs as opposed to millions
in 3D scenes. In this work, we designed a video pipeline that speeds up the
encoding time of the previous Gaussian splatting-based image codec by 88% by
using a content-aware initialization strategy paired with a novel Gaussian
inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be
used for a video-codec solution, the first of its kind solution in this neural
video codec space.

</details>


### [176] [BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis](https://arxiv.org/abs/2505.09329)
*Jiarun Liu,Hong-Yu Zhou,Weijian Huang,Hao Yang,Dongning Song,Tao Tan,Yong Liang,Shanshan Wang*

Main category: cs.CV

TL;DR: 该论文探讨了在医学视觉基础模型中模型规模、训练算法、数据规模和成像模态的扩展行为，并通过自监督学习开发了可扩展的BioVFM-21M数据集和BioVFM模型，验证了扩展在不同任务中的益处。


<details>
  <summary>Details</summary>
Motivation: 医学图像与自然数据差异显著，但缺乏对医学领域扩展行为的深入理解，因此需要研究开发医学视觉基础模型的关键因素。

Method: 通过分析模型大小、训练算法、数据规模和成像模态的扩展行为，引入BioVFM-21M数据集，并采用自监督学习预训练BioVFM模型。

Result: BioVFM在12个医学基准测试中表现优于现有基础模型，但扩展的益处因任务而异，且与任务特性、数据多样性、预训练方法和计算效率相关。

Conclusion: 扩展有助于提升性能，但任务特性、数据多样性、预训练方法和计算效率仍是开发可扩展医学基础模型的关键因素。

Abstract: Scaling up model and data size have demonstrated impressive performance
improvement over a wide range of tasks. Despite extensive studies on scaling
behaviors for general-purpose tasks, medical images exhibit substantial
differences from natural data. It remains unclear the key factors in developing
medical vision foundation models at scale due to the absence of an extensive
understanding of scaling behavior in the medical domain. In this paper, we
explored the scaling behavior across model sizes, training algorithms, data
sizes, and imaging modalities in developing scalable medical vision foundation
models by self-supervised learning. To support scalable pretraining, we
introduce BioVFM-21M, a large-scale biomedical image dataset encompassing a
wide range of biomedical image modalities and anatomies. We observed that
scaling up does provide benefits but varies across tasks. Additional analysis
reveals several factors correlated with scaling benefits. Finally, we propose
BioVFM, a large-scale medical vision foundation model pretrained on 21 million
biomedical images, which outperforms the previous state-of-the-art foundation
models across 12 medical benchmarks. Our results highlight that while scaling
up is beneficial for pursuing better performance, task characteristics, data
diversity, pretraining methods, and computational efficiency remain critical
considerations for developing scalable medical foundation models.

</details>


### [177] [Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform](https://arxiv.org/abs/2505.09380)
*Qinghui Liu,Jon Nesvold,Hanna Raaum,Elakkyen Murugesu,Martin Røvang,Bradley J Maclntosh,Atle Bjørnerud,Karoline Skogen*

Main category: cs.CV

TL;DR: 研究开发了一个名为NeoMedSys的放射学软件平台，用于高效部署和优化AI模型，并在真实临床环境中评估了其效果。通过迭代改进，AI模型（VIOLA-AI）的颅内出血检测性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决AI工具在放射学临床部署中的挑战，通过NeoMedSys平台实现AI模型的快速部署和优化，以提升诊断准确性。

Method: NeoMedSys集成了AI模型部署、测试和优化工具，并结合医学影像查看器、标注系统和全院放射信息系统。在挪威最大急诊科对疑似脑外伤或中风患者进行实用性调查，并通过预设模型再训练评估性能指标。

Result: 迭代改进显著提升了VIOLA-AI的敏感性（90.3%）和特异性（89.3%），ROC曲线的AUC达0.949，凸显实时放射科医生反馈的价值。

Conclusion: NeoMedSys平台有效提升了AI模型的性能，证明了其在临床环境中优化AI工具的潜力。

Abstract: Background: There are many challenges and opportunities in the clinical
deployment of AI tools in radiology. The current study describes a radiology
software platform called NeoMedSys that can enable efficient deployment and
refinements of AI models. We evaluated the feasibility and effectiveness of
running NeoMedSys for three months in real-world clinical settings and focused
on improvement performance of an in-house developed AI model (VIOLA-AI)
designed for intracranial hemorrhage (ICH) detection.
  Methods: NeoMedSys integrates tools for deploying, testing, and optimizing AI
models with a web-based medical image viewer, annotation system, and
hospital-wide radiology information systems. A pragmatic investigation was
deployed using clinical cases of patients presenting to the largest Emergency
Department in Norway (site-1) with suspected traumatic brain injury (TBI) or
patients with suspected stroke (site-2). We assessed ICH classification
performance as VIOLA-AI encountered new data and underwent pre-planned model
retraining. Performance metrics included sensitivity, specificity, accuracy,
and the area under the receiver operating characteristic curve (AUC).
  Results: NeoMedSys facilitated iterative improvements in the AI model,
significantly enhancing its diagnostic accuracy. Automated bleed detection and
segmentation were reviewed in near real-time to facilitate re-training
VIOLA-AI. The iterative refinement process yielded a marked improvement in
classification sensitivity, rising to 90.3% (from 79.2%), and specificity that
reached 89.3% (from 80.7%). The bleed detection ROC analysis for the entire
sample demonstrated a high area-under-the-curve (AUC) of 0.949 (from 0.873).
Model refinement stages were associated with notable gains, highlighting the
value of real-time radiologist feedback.

</details>


### [178] [FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization](https://arxiv.org/abs/2505.09385)
*Xiaoyang Yu,Xiaoming Wu,Xin Wang,Dongrun Li,Ming Yang,Peng Cheng*

Main category: cs.CV

TL;DR: 论文提出了一种名为FedSaaS的新型联邦语义分割框架，通过类范例和对抗机制解决领域偏移问题，提高了分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有研究在解决领域偏移问题时忽视了语义空间中细粒度的类关系，导致类表示模糊。

Method: 引入类范例作为本地和全局类表示的基准，服务器端建模类原型以监督客户端全局分支，客户端通过对抗机制协调全局与本地分支，并采用多级对比损失确保语义空间一致性。

Result: 在多个驾驶场景分割数据集上的实验表明，FedSaaS显著提升了平均分割精度，有效解决了类一致性表示问题。

Conclusion: FedSaaS通过类范例和多级对比损失实现联邦语义分割的类一致性，优于现有方法。

Abstract: Federated semantic segmentation enables pixel-level classification in images
through collaborative learning while maintaining data privacy. However,
existing research commonly overlooks the fine-grained class relationships
within the semantic space when addressing heterogeneous problems, particularly
domain shift. This oversight results in ambiguities between class
representation. To overcome this challenge, we propose a novel federated
segmentation framework that strikes class consistency, termed FedSaaS.
Specifically, we introduce class exemplars as a criterion for both local- and
global-level class representations. On the server side, the uploaded class
exemplars are leveraged to model class prototypes, which supervise global
branch of clients, ensuring alignment with global-level representation. On the
client side, we incorporate an adversarial mechanism to harmonize contributions
of global and local branches, leading to consistent output. Moreover,
multilevel contrastive losses are employed on both sides to enforce consistency
between two-level representations in the same semantic space. Extensive
experiments on several driving scene segmentation datasets demonstrate that our
framework outperforms state-of-the-art methods, significantly improving average
segmentation accuracy and effectively addressing the class-consistency
representation problem.

</details>


### [179] [Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records](https://arxiv.org/abs/2505.09435)
*Yili He,Yan Zhu,Peiyao Fu,Ruijie Yang,Tianyi Chen,Zhihua Wang,Quanlin Li,Pinghong Zhou,Xian Yang,Shuo Wang*

Main category: cs.CV

TL;DR: Endo-CLIP是一种针对内窥镜图像分析的自监督框架，通过三阶段（清洁、调整、统一）处理背景帧、临床属性和多病变描述问题，显著提升零样本和少样本检测与分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有内窥镜图像分析面临背景干扰、复杂医学术语和多病变描述模糊的挑战，需要改进预训练方法以提升准确性。

Method: Endo-CLIP框架分三阶段：（1）去除背景帧，（2）用大语言模型提取临床属性进行细粒度对比学习，（3）患者级跨注意力机制解决多病变歧义。

Result: 实验表明Endo-CLIP在零样本和少样本息肉检测与分类中显著优于现有方法。

Conclusion: Endo-CLIP为内窥镜分析提供了更准确且临床相关的解决方案，推动了该领域的进一步发展。

Abstract: Pre-training on image-text colonoscopy records offers substantial potential
for improving endoscopic image analysis, but faces challenges including
non-informative background images, complex medical terminology, and ambiguous
multi-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised
framework that enhances Contrastive Language-Image Pre-training (CLIP) for this
domain. Endo-CLIP's three-stage framework--cleansing, attunement, and
unification--addresses these challenges by (1) removing background frames, (2)
leveraging large language models to extract clinical attributes for
fine-grained contrastive learning, and (3) employing patient-level
cross-attention to resolve multi-polyp ambiguities. Extensive experiments
demonstrate that Endo-CLIP significantly outperforms state-of-the-art
pre-training methods in zero-shot and few-shot polyp detection and
classification, paving the way for more accurate and clinically relevant
endoscopic analysis.

</details>


### [180] [A 2D Semantic-Aware Position Encoding for Vision Transformers](https://arxiv.org/abs/2505.09466)
*Xi Chen,Shiyang Zhou,Muqi Huang,Jiaxu Feng,Yun Xiong,Kun Zhou,Biao Yang,Yuhui Zhang,Huishuai Bao,Sijia Peng,Chuan Li,Feng Shi*

Main category: cs.CV

TL;DR: 提出了2-D语义感知位置编码(SaPE²)，解决了现有位置编码方法在视觉任务中无法有效捕捉语义位置关系的问题，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统1D位置编码方法在视觉任务中无法有效捕捉语义关系，限制了模型的泛化能力和翻译等变性。

Method: 提出SaPE²，动态调整位置表示，利用局部内容而非固定线性关系或空间坐标。

Result: 该方法提升了模型跨分辨率和尺度的泛化能力，改善了翻译等变性，并更好地聚合相似但空间遥远的特征。

Conclusion: SaPE²通过结合位置编码与感知相似性，显著提升了视觉任务的性能。

Abstract: Vision transformers have demonstrated significant advantages in computer
vision tasks due to their ability to capture long-range dependencies and
contextual relationships through self-attention. However, existing position
encoding techniques, which are largely borrowed from natural language
processing, fail to effectively capture semantic-aware positional relationships
between image patches. Traditional approaches like absolute position encoding
and relative position encoding primarily focus on 1D linear position
relationship, often neglecting the semantic similarity between distant yet
contextually related patches. These limitations hinder model generalization,
translation equivariance, and the ability to effectively handle repetitive or
structured patterns in images. In this paper, we propose 2-Dimensional
Semantic-Aware Position Encoding ($\text{SaPE}^2$), a novel position encoding
method with semantic awareness that dynamically adapts position representations
by leveraging local content instead of fixed linear position relationship or
spatial coordinates. Our method enhances the model's ability to generalize
across varying image resolutions and scales, improves translation equivariance,
and better aggregates features for visually similar but spatially distant
patches. By integrating $\text{SaPE}^2$ into vision transformers, we bridge the
gap between position encoding and perceptual similarity, thereby improving
performance on computer vision tasks.

</details>


### [181] [Predicting butterfly species presence from satellite imagery using soft contrastive regularisation](https://arxiv.org/abs/2505.09306)
*Thijs L van der Plas,Stephen Law,Michael JO Pocock*

Main category: cs.CV

TL;DR: 论文提出了一种新数据集和优化方法，通过卫星图像直接预测英国蝴蝶物种的存在，采用Resnet模型和对比正则化损失，显著提高了高生物多样性区域的预测准确率。


<details>
  <summary>Details</summary>
Motivation: 随着对可扩展生物多样性监测方法的需求增长，遥感数据因其广泛可用性和覆盖范围成为研究热点。传统方法仅关注栖息地映射，但新方法尝试利用公民科学数据直接从卫星图像预测多物种存在。

Method: 论文提出一种基于Resnet的模型，从4波段卫星图像预测多物种存在，并设计了一种适用于概率标签的软监督对比正则化损失函数以提升性能。

Result: 实验表明，该模型在高生物多样性区域的表现显著优于基线方法，且对比正则化进一步提高了预测准确率。

Conclusion: 新数据集和对比正则化方法为从遥感数据准确预测生物多样性这一开放挑战提供了有效解决方案，对高效生物多样性监测具有重要意义。

Abstract: The growing demand for scalable biodiversity monitoring methods has fuelled
interest in remote sensing data, due to its widespread availability and
extensive coverage. Traditionally, the application of remote sensing to
biodiversity research has focused on mapping and monitoring habitats, but with
increasing availability of large-scale citizen-science wildlife observation
data, recent methods have started to explore predicting multi-species presence
directly from satellite images. This paper presents a new data set for
predicting butterfly species presence from satellite data in the United
Kingdom. We experimentally optimise a Resnet-based model to predict
multi-species presence from 4-band satellite images, and find that this model
especially outperforms the mean rate baseline for locations with high species
biodiversity. To improve performance, we develop a soft, supervised contrastive
regularisation loss that is tailored to probabilistic labels (such as
species-presence data), and demonstrate that this improves prediction accuracy.
In summary, our new data set and contrastive regularisation method contribute
to the open challenge of accurately predicting species biodiversity from remote
sensing data, which is key for efficient biodiversity monitoring.

</details>


### [182] [Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput](https://arxiv.org/abs/2505.09498)
*Bo Zhang,Shuo Li,Runhe Tian,Yang Yang,Jixin Tang,Jinhao Zhou,Lin Ma*

Main category: cs.CV

TL;DR: Flash-VL 2B提出了一种新型视觉语言模型优化方法，通过架构增强和计算策略实现低延迟高吞吐，同时保持准确性，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 针对实时应用中视觉语言模型的低延迟高吞吐需求，同时不牺牲模型准确率，设计了Flash-VL 2B。

Method: 采用定制架构、令牌压缩机制、数据优化、训练策略及新颖的图像处理技术“隐式语义拼接”以平衡计算负荷和性能。

Result: 在11个标准VLM基准测试中，Flash-VL 2B在速度和准确性上均达到最先进水平。

Conclusion: Flash-VL 2B是资源受限环境和大规模实时应用的高效解决方案。

Abstract: In this paper, we introduce Flash-VL 2B, a novel approach to optimizing
Vision-Language Models (VLMs) for real-time applications, targeting ultra-low
latency and high throughput without sacrificing accuracy. Leveraging advanced
architectural enhancements and efficient computational strategies, Flash-VL 2B
is designed to maximize throughput by reducing processing time while
maintaining competitive performance across multiple vision-language benchmarks.
Our approach includes tailored architectural choices, token compression
mechanisms, data curation, training schemes, and a novel image processing
technique called implicit semantic stitching that effectively balances
computational load and model performance. Through extensive evaluations on 11
standard VLM benchmarks, we demonstrate that Flash-VL 2B achieves
state-of-the-art results in both speed and accuracy, making it a promising
solution for deployment in resource-constrained environments and large-scale
real-time applications.

</details>


### [183] [BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset](https://arxiv.org/abs/2505.09568)
*Jiuhai Chen,Zhiyang Xu,Xichen Pan,Yushi Hu,Can Qin,Tom Goldstein,Lifu Huang,Tianyi Zhou,Saining Xie,Silvio Savarese,Le Xue,Caiming Xiong,Ran Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种新型多模态模型BLIP3-o，结合了自回归和扩散模型，通过扩散Transformer生成CLIP图像特征，提升了训练效率和生成质量。采用分阶段预训练策略（先理解后生成）和高质量指令调优数据集BLIP3o-60k，BLIP3-o在图像理解与生成任务中表现优异，并开源了相关资源。


<details>
  <summary>Details</summary>
Motivation: 现有研究在统一图像理解与生成的框架中，模型架构和训练方法仍缺乏深入探索。本文基于自回归和扩散模型的潜力，旨在探索其在多模态统一框架中的最佳实践。

Method: 采用扩散Transformer生成CLIP图像特征，取代传统VAE表示；分阶段预训练（先图像理解后生成）；构建高质量指令调优数据集BLIP3o-60k。

Result: 提出的BLIP3-o模型在主流图像理解与生成任务中表现最优，训练效率更高且生成质量提升。

Conclusion: 通过创新的模型设计、训练策略和数据集，BLIP3-o实现了图像理解与生成的统一，推动了多模态研究的发展。

Abstract: Unifying image understanding and generation has gained growing attention in
recent research on multimodal models. Although design choices for image
understanding have been extensively studied, the optimal model architecture and
training recipe for a unified framework with image generation remain
underexplored. Motivated by the strong potential of autoregressive and
diffusion models for high-quality generation and scalability, we conduct a
comprehensive study of their use in unified multimodal settings, with emphasis
on image representations, modeling objectives, and training strategies.
Grounded in these investigations, we introduce a novel approach that employs a
diffusion transformer to generate semantically rich CLIP image features, in
contrast to conventional VAE-based representations. This design yields both
higher training efficiency and improved generative quality. Furthermore, we
demonstrate that a sequential pretraining strategy for unified models-first
training on image understanding and subsequently on image generation-offers
practical advantages by preserving image understanding capability while
developing strong image generation ability. Finally, we carefully curate a
high-quality instruction-tuning dataset BLIP3o-60k for image generation by
prompting GPT-4o with a diverse set of captions covering various scenes,
objects, human gestures, and more. Building on our innovative model design,
training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art
unified multimodal models. BLIP3-o achieves superior performance across most of
the popular benchmarks spanning both image understanding and generation tasks.
To facilitate future research, we fully open-source our models, including code,
model weights, training scripts, and pretraining and instruction tuning
datasets.

</details>


### [184] [Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis](https://arxiv.org/abs/2505.09358)
*Bingxin Ke,Kevin Qu,Tianfu Wang,Nando Metzger,Shengyu Huang,Bo Li,Anton Obukhov,Konrad Schindler*

Main category: cs.CV

TL;DR: Marigold通过微调预训练的潜在扩散模型（如Stable Diffusion），将其应用于密集图像分析任务（如深度估计、表面法线预测等），在小规模合成数据上训练，实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习的成功依赖于大规模标注数据和预训练模型，但数据稀缺时预训练模型的质量尤为关键。文本到图像生成模型（如潜在扩散模型）展现出对视觉世界的深刻理解，如何将其知识迁移到密集图像分析任务是一个重要问题。

Method: 提出Marigold，基于预训练的潜在扩散模型（如Stable Diffusion），通过微调协议提取其知识，适应密集图像分析任务。方法仅需最小化修改模型架构，利用小规模合成数据在单GPU上训练几天即可。

Result: Marigold在单目深度估计、表面法线预测和本征分解等任务中表现出色，实现了零样本泛化，达到最先进的性能。

Conclusion: Marigold展示了从生成模型中提取知识并迁移到密集视觉任务的可行性，为数据稀缺场景提供了高效解决方案。

Abstract: The success of deep learning in computer vision over the past decade has
hinged on large labeled datasets and strong pretrained models. In data-scarce
settings, the quality of these pretrained models becomes crucial for effective
transfer learning. Image classification and self-supervised learning have
traditionally been the primary methods for pretraining CNNs and
transformer-based architectures. Recently, the rise of text-to-image generative
models, particularly those using denoising diffusion in a latent space, has
introduced a new class of foundational models trained on massive, captioned
image datasets. These models' ability to generate realistic images of unseen
content suggests they possess a deep understanding of the visual world. In this
work, we present Marigold, a family of conditional generative models and a
fine-tuning protocol that extracts the knowledge from pretrained latent
diffusion models like Stable Diffusion and adapts them for dense image analysis
tasks, including monocular depth estimation, surface normals prediction, and
intrinsic decomposition. Marigold requires minimal modification of the
pre-trained latent diffusion model's architecture, trains with small synthetic
datasets on a single GPU over a few days, and demonstrates state-of-the-art
zero-shot generalization. Project page:
https://marigoldcomputervision.github.io

</details>


### [185] [Variational Visual Question Answering](https://arxiv.org/abs/2505.09591)
*Tobias Jan Wieczorek,Nathalie Daun,Mohammad Emtiyaz Khan,Marcus Rohrbach*

Main category: cs.CV

TL;DR: 该论文提出一种变分VQA方法，通过IVON算法改进多模态模型的可靠性，显著降低校准误差并提高覆盖率，尤其在分布偏移情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在视觉问答（VQA）中存在可靠性问题，特别是在分布偏移（OOD）时容易过度自信和校准错误。现有方法主要针对单模态模型，多模态场景研究较少。

Method: 采用变分算法IVON代替传统AdamW优化，生成模型参数的后验分布，以提升模型校准能力和弃权机制。

Result: 实验表明，相比AdamW，变分方法将预期校准误差降低50%以上，覆盖率提升4%（风险固定为1%），在OOD情况下覆盖率提升达8%。

Conclusion: 变分学习是提升多模态模型可靠性的有效方法，尤其在分布偏移场景下表现突出。

Abstract: Despite remarkable progress in multimodal models for Visual Question
Answering (VQA), there remain major reliability concerns because the models can
often be overconfident and miscalibrated, especially in out-of-distribution
(OOD) settings. Plenty has been done to address such issues for unimodal
models, but little work exists for multimodal cases. Here, we address
unreliability in multimodal models by proposing a Variational VQA approach.
Specifically, instead of fine-tuning vision-language models by using AdamW, we
employ a recently proposed variational algorithm called IVON, which yields a
posterior distribution over model parameters. Through extensive experiments, we
show that our approach improves calibration and abstentions without sacrificing
the accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce
Expected Calibration Error by more than 50% compared to the AdamW baseline and
raise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of
distribution shifts, the performance gain is even higher, achieving 8% Coverage
(@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we
present variational learning as a viable option to enhance the reliability of
multimodal models.

</details>


### [186] [RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo](https://arxiv.org/abs/2505.09368)
*Jenny Schmalfuss,Victor Oei,Lukas Mehl,Madlen Bartsch,Shashank Agnihotri,Margret Keuper,Andrés Bruhn*

Main category: cs.CV

TL;DR: RobustSpring是一个新数据集和基准测试，用于评估光学流、场景流和立体模型对图像损坏的鲁棒性，填补了现有基准测试在模型对噪声或雨等现实世界扰动的鲁棒性量化方面的空白。


<details>
  <summary>Details</summary>
Motivation: 当前标准基准测试主要关注模型准确性，而忽略了对图像损坏（如噪声或雨）的鲁棒性评估。RobustSpring旨在填补这一空白。

Method: 通过在高分辨率Spring数据集上应用20种不同的图像损坏（如噪声、模糊、颜色变化等），以时间、立体和深度一致的方式生成20,000张损坏图像。

Result: 基准测试发现，准确性高的模型不一定鲁棒，且鲁棒性因损坏类型而异。

Conclusion: RobustSpring将鲁棒性作为首要目标，旨在推动兼具准确性和鲁棒性的模型发展。

Abstract: Standard benchmarks for optical flow, scene flow, and stereo vision
algorithms generally focus on model accuracy rather than robustness to image
corruptions like noise or rain. Hence, the resilience of models to such
real-world perturbations is largely unquantified. To address this, we present
RobustSpring, a comprehensive dataset and benchmark for evaluating robustness
to image corruptions for optical flow, scene flow, and stereo models.
RobustSpring applies 20 different image corruptions, including noise, blur,
color changes, quality degradations, and weather distortions, in a time-,
stereo-, and depth-consistent manner to the high-resolution Spring dataset,
creating a suite of 20,000 corrupted images that reflect challenging
conditions. RobustSpring enables comparisons of model robustness via a new
corruption robustness metric. Integration with the Spring benchmark enables
public two-axis evaluations of both accuracy and robustness. We benchmark a
curated selection of initial models, observing that accurate models are not
necessarily robust and that robustness varies widely by corruption type.
RobustSpring is a new computer vision benchmark that treats robustness as a
first-class citizen to foster models that combine accuracy with resilience. It
will be available at https://spring-benchmark.org.

</details>


### [187] [Contactless Cardiac Pulse Monitoring Using Event Cameras](https://arxiv.org/abs/2505.09529)
*Mohamed Moustafa,Joseph Lemley,Peter Corcoran*

Main category: cs.CV

TL;DR: 论文研究了利用时间事件相机和CNN模型无接触重建个体心脏脉搏信号，展示了该技术在远程心率监测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索时间事件相机在低延迟、低功耗条件下捕捉面部区域生理信息的潜力，实现无接触心率监测。

Method: 使用监督学习的CNN模型从事件流二维表示中提取心脏信号，评估模型计算心率的准确性。

Result: 事件流有效保留了面部生理信息，基于事件帧的模型RMSE为3.32 bpm，60和120 FPS事件帧模型表现优于30 FPS标准相机。

Conclusion: 时间事件相机在远程心率监测中具有实用潜力，高帧率事件帧模型性能更优。

Abstract: Time event cameras are a novel technology for recording scene information at
extremely low latency and with low power consumption. Event cameras output a
stream of events that encapsulate pixel-level light intensity changes within
the scene, capturing information with a higher dynamic range and temporal
resolution than traditional cameras. This study investigates the contact-free
reconstruction of an individual's cardiac pulse signal from time event
recording of their face using a supervised convolutional neural network (CNN)
model. An end-to-end model is trained to extract the cardiac signal from a
two-dimensional representation of the event stream, with model performance
evaluated based on the accuracy of the calculated heart rate. The experimental
results confirm that physiological cardiac information in the facial region is
effectively preserved within the event stream, showcasing the potential of this
novel sensor for remote heart rate monitoring. The model trained on event
frames achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm)
compared to the RMSE of 2.92 bpm achieved by the baseline model trained on
standard camera frames. Furthermore, models trained on event frames generated
at 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an
RMSE of 2.54 and 2.13 bpm, respectively.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [188] [Multi-source Plume Tracing via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.08825)
*Pedro Antonio Alarcon Granadeno,Theodore Chambers,Jane Cleland-Huang*

Main category: cs.MA

TL;DR: 论文提出了一种基于多智能体强化学习（MARL）的算法，用于快速定位空气中的污染源，相比传统方法在复杂湍流条件下表现更优。


<details>
  <summary>Details</summary>
Motivation: 工业灾难如博帕尔事故（1984）和阿利索峡谷天然气泄漏（2015）暴露了现有污染源定位方法的不足，亟需一种在湍流等复杂环境下仍能快速可靠工作的算法。

Method: 采用多智能体强化学习（MARL）和部分可观察马尔可夫博弈（POMG）框架，结合LSTM网络和动作特定的双深度递归Q网络（ADDRQN），利用历史动作-观察序列模拟潜在状态，并在基于高斯羽流模型（GPM）的仿真环境中验证。

Result: 算法显著优于传统方法，仅需探索环境的1.29%即可成功定位污染源。

Conclusion: 所提出的MARL算法在复杂、部分可观察的环境中展现出高效性和适应性，为污染源定位提供了新的解决方案。

Abstract: Industrial catastrophes like the Bhopal disaster (1984) and the Aliso Canyon
gas leak (2015) demonstrate the urgent need for rapid and reliable plume
tracing algorithms to protect public health and the environment. Traditional
methods, such as gradient-based or biologically inspired approaches, often fail
in realistic, turbulent conditions. To address these challenges, we present a
Multi-Agent Reinforcement Learning (MARL) algorithm designed for localizing
multiple airborne pollution sources using a swarm of small uncrewed aerial
systems (sUAS). Our method models the problem as a Partially Observable Markov
Game (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific
Double Deep Recurrent Q-Network (ADDRQN) that uses full sequences of historical
action-observation pairs, effectively approximating latent states. Unlike prior
work, we use a general-purpose simulation environment based on the Gaussian
Plume Model (GPM), incorporating realistic elements such as a three-dimensional
environment, sensor noise, multiple interacting agents, and multiple plume
sources. The incorporation of action histories as part of the inputs further
enhances the adaptability of our model in complex, partially observable
environments. Extensive simulations show that our algorithm significantly
outperforms conventional approaches. Specifically, our model allows agents to
explore only 1.29\% of the environment to successfully locate pollution
sources.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [189] [Lower Bounds on the MMSE of Adversarially Inferring Sensitive Features](https://arxiv.org/abs/2505.09004)
*Monica Welfert,Nathan Stromberg,Mario Diaz,Lalitha Sankar*

Main category: stat.ML

TL;DR: 本文提出了一种基于最小均方误差（MMSE）估计的对抗性评估框架，用于敏感特征推断，并建立了理论下界。针对线性预测模型，推导了闭式边界，并通过实证验证了框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一个理论严谨且实用的对抗性评估框架，用于量化从噪声观测中推断敏感特征的最小均方误差，以平衡理论保证与实际效率。

Method: 方法包括基于有限样本和线性预测模型的MMSE估计，建立理论下界，并推导针对线性映射、二进制对称信道等关系的闭式边界。

Result: 结果显示，该框架能够有效评估敏感特征推断的MMSE，并在多种关系下实现阶次最优的近似误差边界。

Conclusion: 结论是该框架为MMSE-based对抗性评估提供了理论与实证支持，适用于敏感特征推断的验证。

Abstract: We propose an adversarial evaluation framework for sensitive feature
inference based on minimum mean-squared error (MMSE) estimation with a finite
sample size and linear predictive models. Our approach establishes theoretical
lower bounds on the true MMSE of inferring sensitive features from noisy
observations of other correlated features. These bounds are expressed in terms
of the empirical MMSE under a restricted hypothesis class and a non-negative
error term. The error term captures both the estimation error due to finite
number of samples and the approximation error from using a restricted
hypothesis class. For linear predictive models, we derive closed-form bounds,
which are order optimal in terms of the noise variance, on the approximation
error for several classes of relationships between the sensitive and
non-sensitive features, including linear mappings, binary symmetric channels,
and class-conditional multi-variate Gaussian distributions. We also present a
new lower bound that relies on the MSE computed on a hold-out validation
dataset of the MMSE estimator learned on finite-samples and a restricted
hypothesis class. Through empirical evaluation, we demonstrate that our
framework serves as an effective tool for MMSE-based adversarial evaluation of
sensitive feature inference that balances theoretical guarantees with practical
efficiency.

</details>


### [190] [Risk Bounds For Distributional Regression](https://arxiv.org/abs/2505.09075)
*Carlos Misael Madrid Padilla,Oscar Hernan Madrid Padilla,Sabyasachi Chatterjee*

Main category: stat.ML

TL;DR: 论文研究了非参数分布回归估计的风险界限，提出了CRPS和最坏情况MSE的理论上界，并在实验中验证了其实际效果。


<details>
  <summary>Details</summary>
Motivation: 探讨非参数分布回归估计的风险界限，填补理论研究与实际应用之间的空白。

Method: 通过凸约束和非凸约束的分布回归分析，推导CRPS和MSE的理论上界，并应用于等渗和趋势滤波回归及神经网络估计。

Result: 实验结果验证了理论贡献，表明其在模拟和实际数据中的有效性。

Conclusion: 研究为非参数分布回归提供了理论和实践支持，展示了其在多种约束下的适用性。

Abstract: This work examines risk bounds for nonparametric distributional regression
estimators. For convex-constrained distributional regression, general upper
bounds are established for the continuous ranked probability score (CRPS) and
the worst-case mean squared error (MSE) across the domain. These theoretical
results are applied to isotonic and trend filtering distributional regression,
yielding convergence rates consistent with those for mean estimation.
Furthermore, a general upper bound is derived for distributional regression
under non-convex constraints, with a specific application to neural
network-based estimators. Comprehensive experiments on both simulated and real
data validate the theoretical contributions, demonstrating their practical
effectiveness.

</details>


### [191] [Online Learning of Neural Networks](https://arxiv.org/abs/2505.09167)
*Amit Daniely,Idan Mehalel,Elchanan Mossel*

Main category: stat.ML

TL;DR: 该论文研究了带有符号激活函数的前馈神经网络的在线学习问题，重点分析了在单位球内的输入和有限标签集下的学习能力。通过引入边界条件和多层网络限制，论文提出了错误界限的理论分析，并探讨了维度依赖的必要性及其缓解方法。


<details>
  <summary>Details</summary>
Motivation: 研究目的是理解带有符号激活函数的神经网络在在线学习环境中的性能，特别是在高维输入下的学习能力及其限制。希望通过理论分析为神经网络在线学习提供理论基础。

Method: 通过定义边界条件（各层神经元的分类边界需大于零）和进一步限制网络结构（多索引模型和扩展边界假设），分析并证明了在不同条件下网络学习的最优错误界限。

Result: 结果表明，在一般条件下，错误界限高度依赖于输入维度 $d$，而通过多索引模型或扩展边界假设可以缓解这种依赖性，分别实现与 $k 	ext{（远小于 $d$）和 $L 	ext{（网络深度）相关的错误界限。

Conclusion: 论文指出完全避免维度依赖几乎不可能，但通过合理限制网络结构可以显著改善学习性能，为设计高效的在线学习神经网络提供了理论基础。

Abstract: We study online learning of feedforward neural networks with the sign
activation function that implement functions from the unit ball in
$\mathbb{R}^d$ to a finite label set $\{1, \ldots, Y\}$.
  First, we characterize a margin condition that is sufficient and in some
cases necessary for online learnability of a neural network: Every neuron in
the first hidden layer classifies all instances with some margin $\gamma$
bounded away from zero. Quantitatively, we prove that for any net, the optimal
mistake bound is at most approximately $\mathtt{TS}(d,\gamma)$, which is the
$(d,\gamma)$-totally-separable-packing number, a more restricted variation of
the standard $(d,\gamma)$-packing number. We complement this result by
constructing a net on which any learner makes $\mathtt{TS}(d,\gamma)$ many
mistakes. We also give a quantitative lower bound of approximately
$\mathtt{TS}(d,\gamma) \geq \max\{1/(\gamma \sqrt{d})^d, d\}$ when $\gamma \geq
1/2$, implying that for some nets and input sequences every learner will err
for $\exp(d)$ many times, and that a dimension-free mistake bound is almost
always impossible.
  To remedy this inevitable dependence on $d$, it is natural to seek additional
natural restrictions to be placed on the network, so that the dependence on $d$
is removed. We study two such restrictions. The first is the multi-index model,
in which the function computed by the net depends only on $k \ll d$ orthonormal
directions. We prove a mistake bound of approximately $(1.5/\gamma)^{k + 2}$ in
this model. The second is the extended margin assumption. In this setting, we
assume that all neurons (in all layers) in the network classify every ingoing
input from previous layer with margin $\gamma$ bounded away from zero. In this
model, we prove a mistake bound of approximately $(\log Y)/ \gamma^{O(L)}$,
where L is the depth of the network.

</details>


### [192] [Optimal Transport-Based Domain Adaptation for Rotated Linear Regression](https://arxiv.org/abs/2505.09229)
*Brian Britos,Mathias Bourel*

Main category: stat.ML

TL;DR: 论文提出一种结合K均值聚类、最优传输(OT)和奇异值分解(SVD)的算法，用于估计旋转角度并调整回归模型，特别适用于目标域数据稀疏的情况。


<details>
  <summary>Details</summary>
Motivation: 研究在旋转平移的监督域适应问题中，利用最优传输恢复基础旋转，以提升模型迁移效果。

Method: 使用p-norm成本函数（p≥2）在ℝ²中恢复旋转，结合K均值、OT和SVD估计旋转角度并调整回归模型。

Result: 结果表明，该方法在目标域数据稀疏时，能有效利用源域数据提升泛化性能。

Conclusion: 论文为几何变换下基于OT的模型适应提供了理论和实践见解。

Abstract: Optimal Transport (OT) has proven effective for domain adaptation (DA) by
aligning distributions across domains with differing statistical properties.
Building on the approach of Courty et al. (2016), who mapped source data to the
target domain for improved model transfer, we focus on a supervised DA problem
involving linear regression models under rotational shifts. This ongoing work
considers cases where source and target domains are related by a
rotation-common in applications like sensor calibration or image orientation.
We show that in $\mathbb{R}^2$ , when using a p-norm cost with $p $\ge$ 2$, the
optimal transport map recovers the underlying rotation. Based on this, we
propose an algorithm that combines K-means clustering, OT, and singular value
decomposition (SVD) to estimate the rotation angle and adapt the regression
model. This method is particularly effective when the target domain is sparsely
sampled, leveraging abundant source data for improved generalization. Our
contributions offer both theoretical and practical insights into OT-based model
adaptation under geometric transformations.

</details>


### [193] [Fairness-aware Bayes optimal functional classification](https://arxiv.org/abs/2505.09471)
*Xiaoyu Hu,Gengyu Xue,Zhenhua Lin,Yi Yu*

Main category: stat.ML

TL;DR: 提出了一种公平性约束下的函数数据分类统一框架，设计了Fair-FLDA算法，通过分组阈值实现公平性，并提供了理论和实验验证。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的算法公平性日益重要，需要减少不同子群体间的差异。本文旨在研究函数数据分类中的公平性问题。

Method: 提出了一种统一框架，解决了函数空间中的关键挑战，设计了Fair-FLDA算法，通过分组阈值实现公平性。

Result: 理论证明了算法在公平性和超额风险控制上的保障，实验验证了算法的实用性。

Conclusion: Fair-FLDA算法在理论和实验上均表现出色，为函数数据分类中的公平性问题提供了有效解决方案。

Abstract: Algorithmic fairness has become a central topic in machine learning, and
mitigating disparities across different subpopulations has emerged as a rapidly
growing research area. In this paper, we systematically study the
classification of functional data under fairness constraints, ensuring the
disparity level of the classifier is controlled below a pre-specified
threshold. We propose a unified framework for fairness-aware functional
classification, tackling an infinite-dimensional functional space, addressing
key challenges from the absence of density ratios and intractability of
posterior probabilities, and discussing unique phenomena in functional
classification. We further design a post-processing algorithm, Fair Functional
Linear Discriminant Analysis classifier (Fair-FLDA), which targets at
homoscedastic Gaussian processes and achieves fairness via group-wise
thresholding. Under weak structural assumptions on eigenspace, theoretical
guarantees on fairness and excess risk controls are established. As a
byproduct, our results cover the excess risk control of the standard FLDA as a
special case, which, to the best of our knowledge, is first time seen. Our
theoretical findings are complemented by extensive numerical experiments on
synthetic and real datasets, highlighting the practicality of our designed
algorithm.

</details>


### [194] [Reinforcement Learning for Individual Optimal Policy from Heterogeneous Data](https://arxiv.org/abs/2505.09496)
*Rui Miao,Babak Shahbaba,Annie Qu*

Main category: stat.ML

TL;DR: 论文提出了一种个性化离线策略优化框架（P4L算法），用于解决异构数据环境中的离线强化学习问题，通过个体潜在变量建模和学习，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统离线强化学习方法在处理异构数据时可能产生次优策略，本研究旨在通过个性化建模优化策略学习效果。

Method: 提出基于个性化潜在变量的异构MDP模型，开发P4L算法，结合惩罚悲观策略学习和弱部分覆盖假设，快速估计个体Q函数。

Result: 理论证明了P4L在平均遗憾上的快速收敛性，仿真和实际数据实验验证其优于现有方法的数值性能。

Conclusion: 该方法为异构数据下的离线强化学习提供了一种高效解决方案，具有理论和实际应用价值。

Abstract: Offline reinforcement learning (RL) aims to find optimal policies in dynamic
environments in order to maximize the expected total rewards by leveraging
pre-collected data. Learning from heterogeneous data is one of the fundamental
challenges in offline RL. Traditional methods focus on learning an optimal
policy for all individuals with pre-collected data from a single episode or
homogeneous batch episodes, and thus, may result in a suboptimal policy for a
heterogeneous population. In this paper, we propose an individualized offline
policy optimization framework for heterogeneous time-stationary Markov decision
processes (MDPs). The proposed heterogeneous model with individual latent
variables enables us to efficiently estimate the individual Q-functions, and
our Penalized Pessimistic Personalized Policy Learning (P4L) algorithm
guarantees a fast rate on the average regret under a weak partial coverage
assumption on behavior policies. In addition, our simulation studies and a real
data application demonstrate the superior numerical performance of the proposed
method compared with existing methods.

</details>


### [195] [Deep-SITAR: A SITAR-Based Deep Learning Framework for Growth Curve Modeling via Autoencoders](https://arxiv.org/abs/2505.09506)
*María Alejandra Hernández,Oscar Rodriguez,Dae-Jin Lee*

Main category: stat.ML

TL;DR: 该论文提出了一种基于自编码器结构的Deep-SITAR模型，结合深度学习和B样条模型来估计SITAR模型，从而预测个体的生长轨迹。


<details>
  <summary>Details</summary>
Motivation: 为了更高效地预测个体生长轨迹，同时保持传统混合效应模型的可解释性。

Method: 使用自编码器架构，编码器估计每个个体的随机效应，解码器基于B样条进行拟合。

Result: Deep-SITAR能够预测新个体的随机效应，无需重新估计整个模型。

Conclusion: Deep-SITAR结合了深度学习的灵活性和传统模型的可解释性，为生长轨迹预测提供了强大工具。

Abstract: Several approaches have been developed to capture the complexity and
nonlinearity of human growth. One widely used is the Super Imposition by
Translation and Rotation (SITAR) model, which has become popular in studies of
adolescent growth. SITAR is a shape-invariant mixed-effects model that
represents the shared growth pattern of a population using a natural cubic
spline mean curve while incorporating three subject-specific random effects --
timing, size, and growth intensity -- to account for variations among
individuals. In this work, we introduce a supervised deep learning framework
based on an autoencoder architecture that integrates a deep neural network
(neural network) with a B-spline model to estimate the SITAR model. In this
approach, the encoder estimates the random effects for each individual, while
the decoder performs a fitting based on B-splines similar to the classic SITAR
model. We refer to this method as the Deep-SITAR model. This innovative
approach enables the prediction of the random effects of new individuals
entering a population without requiring a full model re-estimation. As a
result, Deep-SITAR offers a powerful approach to predicting growth
trajectories, combining the flexibility and efficiency of deep learning with
the interpretability of traditional mixed-effects models.

</details>


### [196] [Adaptively-weighted Nearest Neighbors for Matrix Completion](https://arxiv.org/abs/2505.09612)
*Tathagata Sadhukhan,Manit Paul,Raaz Dwivedi*

Main category: stat.ML

TL;DR: AWNN是一种自适应加权的最近邻方法，用于矩阵补全，通过智能平衡加权最近邻回归中的偏差-方差权衡，解决了传统方法依赖交叉验证选择半径和权重的问题。


<details>
  <summary>Details</summary>
Motivation: 传统最近邻方法虽然在多个领域广泛应用且理论保证良好，但其性能高度依赖半径和权重的选择，且缺乏系统化的选择方法。为解决这一问题，作者提出了AWNN方法。

Method: AWNN通过自适应加权最近邻方法，智能平衡回归中的偏差-方差权衡，无需依赖交叉验证即可选择最优半径和权重。

Result: 理论分析在最小假设下验证了AWNN的有效性，并通过合成实验支持了理论结果。

Conclusion: AWNN为矩阵补全提供了一种更系统、高效的方法，减少了对外部验证方法的依赖，具有理论和实践的双重优势。

Abstract: In this technical note, we introduce and analyze AWNN: an adaptively weighted
nearest neighbor method for performing matrix completion. Nearest neighbor (NN)
methods are widely used in missing data problems across multiple disciplines
such as in recommender systems and for performing counterfactual inference in
panel data settings. Prior works have shown that in addition to being very
intuitive and easy to implement, NN methods enjoy nice theoretical guarantees.
However, the performance of majority of the NN methods rely on the appropriate
choice of the radii and the weights assigned to each member in the nearest
neighbor set and despite several works on nearest neighbor methods in the past
two decades, there does not exist a systematic approach of choosing the radii
and the weights without relying on methods like cross-validation. AWNN
addresses this challenge by judiciously balancing the bias variance trade off
inherent in weighted nearest-neighbor regression. We provide theoretical
guarantees for the proposed method under minimal assumptions and support the
theory via synthetic experiments.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [197] [DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis](https://arxiv.org/abs/2505.09091)
*Zeeshan Ahmad,Shudi Bao,Meng Chen*

Main category: cs.SD

TL;DR: DPN-GAN是一种新型GAN架构,通过可变形周期网络和多分辨率生成解决音频生成中的分辨率限制和模式崩溃问题,实验结果表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于mel频谱的GAN在音频生成中存在分辨率限制和模式崩溃的问题,需要一种新方法来提升生成质量。

Method: 提出DPN-GAN,引入基于核的周期ReLU激活函数和可变形卷积操作,增强音频模式捕捉能力,并改进判别器网络。

Result: DPN-GAN在多种数据集上优于当前最优GAN模型,且对小样本和噪声数据表现更鲁棒。

Conclusion: DPN-GAN通过可变形周期网络显著提升音频生成质量,具有更强的适应性和鲁棒性。

Abstract: In recent years, generative adversarial networks (GANs) have made significant
progress in generating audio sequences. However, these models typically rely on
bandwidth-limited mel-spectrograms, which constrain the resolution of generated
audio sequences, and lead to mode collapse during conditional generation. To
address this issue, we propose Deformable Periodic Network based GAN (DPN-GAN),
a novel GAN architecture that incorporates a kernel-based periodic ReLU
activation function to induce periodic bias in audio generation. This
innovative approach enhances the model's ability to capture and reproduce
intricate audio patterns. In particular, our proposed model features a DPN
module for multi-resolution generation utilizing deformable convolution
operations, allowing for adaptive receptive fields that improve the quality and
fidelity of the synthetic audio. Additionally, we enhance the discriminator
network using deformable convolution to better distinguish between real and
generated samples, further refining the audio quality. We trained two versions
of the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M
parameters). For evaluation, we use five different datasets, covering both
speech synthesis and music generation tasks, to demonstrate the efficiency of
the DPN-GAN. The experimental results demonstrate that DPN-GAN delivers
superior performance on both out-of-distribution and noisy data, showcasing its
robustness and adaptability. Trained across various datasets, DPN-GAN
outperforms state-of-the-art GAN architectures on standard evaluation metrics,
and exhibits increased robustness in synthesized audio.

</details>


### [198] [The Voice Timbre Attribute Detection 2025 Challenge Evaluation Plan](https://arxiv.org/abs/2505.09382)
*Zhengyan Sheng,Jinghao He,Liping Chen,Kong Aik Lee,Zhen-Hua Ling*

Main category: cs.SD

TL;DR: VtaD 2025挑战旨在通过感官描述符（如明亮、粗糙、柔和等）比较解释人声音色属性。


<details>
  <summary>Details</summary>
Motivation: 研究人声音色的独特感知，并通过比较方式更精准地描述和分类声音特性。

Method: 使用感官描述符对两种声音的强度进行比较分析。

Result: 挑战将在2025年5月启动，并于10月在镇江的NCMMSC2025会议上展示特别提案。

Conclusion: VtaD 2025挑战为声音色属性提供了系统化的比较分析框架。

Abstract: Voice timbre refers to the unique quality or character of a person's voice
that distinguishes it from others as perceived by human hearing. The Voice
Timbre Attribute Detection (VtaD) 2025 challenge focuses on explaining the
voice timbre attribute in a comparative manner. In this challenge, the human
impression of voice timbre is verbalized with a set of sensory descriptors,
including bright, coarse, soft, magnetic, and so on. The timbre is explained
from the comparison between two voices in their intensity within a specific
descriptor dimension. The VtaD 2025 challenge starts in May and culminates in a
special proposal at the NCMMSC2025 conference in October 2025 in Zhenjiang,
China.

</details>


### [199] [Adaptive Noise Resilient Keyword Spotting Using One-Shot Learning](https://arxiv.org/abs/2505.09304)
*Luciano Sebastian Martinez-Rau,Quynh Nguyen Phuong Vu,Yuxuan Zhang,Bengt Oelmann,Sebastian Bader*

Main category: cs.SD

TL;DR: 提出一种低计算量的关键词识别（KWS）方法，仅需单次学习和一个周期即可实现连续噪声适应，显著提升在资源受限设备上的性能。


<details>
  <summary>Details</summary>
Motivation: 解决标准KWS系统在真实环境中性能下降的问题，尤其针对资源受限设备上的低延迟、动态适应需求。

Method: 通过单次学习和一个周期对预训练神经网络进行连续噪声适应，降低计算负担。

Result: 在多种真实噪声和信噪比条件下，改进后的模型性能显著优于预训练模型，尤其在低信噪比（≤ 18 dB）时准确率提升4.9%至46.0%。

Conclusion: 该方法高效且轻量，适合部署在资源受限的嵌入式设备上，为动态KWS系统提供了实用解决方案。

Abstract: Keyword spotting (KWS) is a key component of smart devices, enabling
efficient and intuitive audio interaction. However, standard KWS systems
deployed on embedded devices often suffer performance degradation under
real-world operating conditions. Resilient KWS systems address this issue by
enabling dynamic adaptation, with applications such as adding or replacing
keywords, adjusting to specific users, and improving noise robustness. However,
deploying resilient, standalone KWS systems with low latency on
resource-constrained devices remains challenging due to limited memory and
computational resources. This study proposes a low computational approach for
continuous noise adaptation of pretrained neural networks used for KWS
classification, requiring only 1-shot learning and one epoch. The proposed
method was assessed using two pretrained models and three real-world noise
sources at signal-to-noise ratios (SNRs) ranging from 24 to -3 dB. The adapted
models consistently outperformed the pretrained models across all scenarios,
especially at SNR $\leq$ 18 dB, achieving accuracy improvements of 4.9% to
46.0%. These results highlight the efficacy of the proposed methodology while
being lightweight enough for deployment on resource-constrained devices.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [200] [Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions](https://arxiv.org/abs/2505.08919)
*Kangxian Xie,Yufei Zhu,Kaiming Kuang,Li Zhang,Hongwei Bran Li,Mingchen Gao,Jiancheng Yang*

Main category: cs.GR

TL;DR: 提出了一种基于神经隐式函数的方法，用于肺部3D重建，解决了传统深度学习方法的分辨率限制问题，并引入了两个临床相关指标和Lung3D数据集。


<details>
  <summary>Details</summary>
Motivation: 高分辨率的肺段3D重建对肺段切除术和肺癌手术规划至关重要，传统深度学习方法受限于计算资源或分辨率不足。

Method: 通过神经隐式函数学习3D表面，利用可学习模板变形实现精确重建，并开发了Lung3D数据集。

Result: 该方法优于现有方法，提供了肺段重建的新视角。

Conclusion: 该方法在肺段3D重建中表现优异，为临床提供了有效的工具。

Abstract: High-quality 3D reconstruction of pulmonary segments plays a crucial role in
segmentectomy and surgical treatment planning for lung cancer. Due to the
resolution requirement of the target reconstruction, conventional deep
learning-based methods often suffer from computational resource constraints or
limited granularity. Conversely, implicit modeling is favored due to its
computational efficiency and continuous representation at any resolution. We
propose a neural implicit function-based method to learn a 3D surface to
achieve anatomy-aware, precise pulmonary segment reconstruction, represented as
a shape by deforming a learnable template. Additionally, we introduce two
clinically relevant evaluation metrics to assess the reconstruction
comprehensively. Further, due to the absence of publicly available shape
datasets to benchmark reconstruction algorithms, we developed a shape dataset
named Lung3D, including the 3D models of 800 labeled pulmonary segments and the
corresponding airways, arteries, veins, and intersegmental veins. We
demonstrate that the proposed approach outperforms existing methods, providing
a new perspective for pulmonary segment reconstruction. Code and data will be
available at https://github.com/M3DV/ImPulSe.

</details>


### [201] [UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units](https://arxiv.org/abs/2505.09393)
*Huakun Liu,Hiroki Ota,Xin Wei,Yutaro Hirao,Monica Perusquia-Hernandez,Hideaki Uchiyama,Kiyoshi Kiyokawa*

Main category: cs.GR

TL;DR: UMotion is a new framework combining IMU和UWB传感器，通过UKF实时优化数据，解决了传统方法中的姿态模糊、数据漂移和体型适应性问题。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏穿戴式IMU在3D人体运动估计中存在姿态模糊、数据漂移和体型适应性差的问题，需要更鲁棒的解决方案。

Method: 提出了UMotion框架，结合6个UWB距离传感器和IMU，利用UKF实时融合传感器数据与人体运动不确定性，优化估计结果。

Result: 实验证明UMotion能稳定传感器数据，并在姿态估计精度上超越现有技术。

Conclusion: UMotion通过多传感器融合和UKF框架有效解决了传统方法的局限性，提升了3D人体运动估计的准确性和适应性。

Abstract: Sparse wearable inertial measurement units (IMUs) have gained popularity for
estimating 3D human motion. However, challenges such as pose ambiguity, data
drift, and limited adaptability to diverse bodies persist. To address these
issues, we propose UMotion, an uncertainty-driven, online fusing-all state
estimation framework for 3D human shape and pose estimation, supported by six
integrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB
sensors measure inter-node distances to infer spatial relationships, aiding in
resolving pose ambiguities and body shape variations when combined with
anthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors
are affected by body occlusions. Consequently, we develop a tightly coupled
Unscented Kalman Filter (UKF) framework that fuses uncertainties from sensor
data and estimated human motion based on individual body shape. The UKF
iteratively refines IMU and UWB measurements by aligning them with uncertain
human motion constraints in real-time, producing optimal estimates for each.
Experiments on both synthetic and real-world datasets demonstrate the
effectiveness of UMotion in stabilizing sensor data and the improvement over
state of the art in pose accuracy.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [202] [Independent Component Analysis by Robust Distance Correlation](https://arxiv.org/abs/2505.09425)
*Sarah Leyder,Jakob Raymaekers,Peter J. Rousseeuw,Tom Van Deuren,Tim Verdonck*

Main category: stat.CO

TL;DR: 提出了RICA这一鲁棒的独立成分分析（ICA）方法，通过最小化多元随机变量之间的鲁棒依赖度量（dCor）来估计独立成分，并通过bowl变换增强其鲁棒性。RICA在模拟实验中表现优于其他方法，且在实际应用中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统ICA方法对异常值不够鲁棒，为了解决这一问题，作者提出了RICA方法，旨在通过鲁棒依赖度量和bowl变换提升ICA在异常值存在时的稳定性。

Method: RICA通过最小化多元随机变量之间的距离相关性（dCor）来估计独立成分，并使用bowl变换使dCor对异常值更鲁棒。RICA顺序估计独立成分，每次寻找与剩余部分dCor最小的成分。

Result: RICA在模拟实验中表现优于其他方法，具有强一致性和通常的参数收敛速度。在三个实际应用（包括著名的鸡尾酒会问题）中验证了其有效性。

Conclusion: RICA是一种鲁棒且有效的ICA方法，特别适用于存在异常值的情况，并在理论和实际应用中均表现出色。

Abstract: Independent component analysis (ICA) is a powerful tool for decomposing a
multivariate signal or distribution into fully independent sources, not just
uncorrelated ones. Unfortunately, most approaches to ICA are not robust against
outliers. Here we propose a robust ICA method called RICA, which estimates the
components by minimizing a robust measure of dependence between multivariate
random variables. The dependence measure used is the distance correlation
(dCor). In order to make it more robust we first apply a new transformation
called the bowl transform, which is bounded, one-to-one, continuous, and maps
far outliers to points close to the origin. This preserves the crucial property
that a zero dCor implies independence. RICA estimates the independent sources
sequentially, by looking for the component that has the smallest dCor with the
remainder. RICA is strongly consistent and has the usual parametric rate of
convergence. Its robustness is investigated by a simulation study, in which it
generally outperforms its competitors. The method is illustrated on three
applications, including the well-known cocktail party problem.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [203] [TensorRL-QAS: Reinforcement learning with tensor networks for scalable quantum architecture search](https://arxiv.org/abs/2505.09371)
*Akash Kundu,Stefano Mangini*

Main category: quant-ph

TL;DR: 该论文提出了TensorRL-QAS框架，结合张量网络和强化学习优化量子电路设计，显著减少了计算资源和时间，并在12量子比特以内的化学问题上取得优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于强化学习的量子架构搜索方法在扩展性上的局限性，尤其是在量子比特数、电路深度和噪声增加时性能下降的问题。

Method: 通过张量网络的矩阵乘积状态近似目标解，预训练强化学习模型以缩小搜索空间，设计物理意义上合理的量子电路。

Result: 在12量子比特的化学问题上，相比基线方法，CNOT门数量和电路深度减少10倍，训练速度提升98%，10量子比特系统的成功率从<1%提升至50%。

Conclusion: TensorRL-QAS是一个可扩展且高效的量子电路设计框架，适合近期的含噪声量子硬件。

Abstract: Variational quantum algorithms hold the promise to address meaningful quantum
problems already on noisy intermediate-scale quantum hardware, but they face
the challenge of designing quantum circuits that both solve the target problem
and comply with device limitations. Quantum architecture search (QAS) automates
this design process, with reinforcement learning (RL) emerging as a promising
approach. Yet, RL-based QAS methods encounter significant scalability issues,
as computational and training costs grow rapidly with the number of qubits,
circuit depth, and noise, severely impacting performance. To address these
challenges, we introduce $\textit{TensorRL-QAS}$, a scalable framework that
combines tensor network (TN) methods with RL for designing quantum circuits. By
warm-starting the architecture search with a matrix product state approximation
of the target solution, TensorRL-QAS effectively narrows the search space to
physically meaningful circuits, accelerating convergence to the desired
solution. Tested on several quantum chemistry problems of up to 12-qubit,
TensorRL-QAS achieves up to a 10-fold reduction in CNOT count and circuit depth
compared to baseline methods, while maintaining or surpassing chemical
accuracy. It reduces function evaluations by up to 100-fold, accelerates
training episodes by up to $98\%$, and achieves up to $50\%$ success
probability for 10-qubit systems-far exceeding the $<1\%$ rates of baseline
approaches. Robustness and versatility are demonstrated both in the noiseless
and noisy scenarios, where we report a simulation of up to 8-qubit. These
advancements establish TensorRL-QAS as a promising candidate for a scalable and
efficient quantum circuit discovery protocol on near-term quantum hardware.

</details>


### [204] [Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting](https://arxiv.org/abs/2505.09395)
*Chen-Yu Liu,Kuan-Cheng Chen,Yi-Chien Chen,Samuel Yen-Chi Chen,Wei-Hao Huang,Wei-Jia Huang,Yen-Jui Chang*

Main category: quant-ph

TL;DR: 提出了Quantum-Train (QT)框架，结合量子神经网络（QNNs）生成训练参数，仅需在训练阶段使用量子硬件。通过Quantum Parameter Adaptation (QPA)方法，结合Attention-based Multi-ConvGRU模型，实现了高效且参数轻量的台风轨迹预测，首次将量子机器学习（QML）应用于大规模台风预测领域。结果表明QPA在减少可训练参数的同时保持了预测性能。


<details>
  <summary>Details</summary>
Motivation: 台风轨迹预测对防灾至关重要，但现有深度学习模型计算资源需求高。结合量子计算的潜力，提出QT框架以降低计算成本。

Method: 提出Quantum Parameter Adaptation (QPA)方法，集成Attention-based Multi-ConvGRU模型，仅在训练阶段使用量子硬件生成参数，推理阶段无需量子设备。

Result: QPA显著减少可训练参数数量，同时保持预测性能，为气候建模提供了可扩展且节能的解决方案。

Conclusion: 首次将QML应用于台风轨迹预测，验证了QT框架的可行性，为高效、可持续的气候建模开辟了新方向。

Abstract: Typhoon trajectory forecasting is essential for disaster preparedness but
remains computationally demanding due to the complexity of atmospheric dynamics
and the resource requirements of deep learning models. Quantum-Train (QT), a
hybrid quantum-classical framework that leverages quantum neural networks
(QNNs) to generate trainable parameters exclusively during training,
eliminating the need for quantum hardware at inference time. Building on QT's
success across multiple domains, including image classification, reinforcement
learning, flood prediction, and large language model (LLM) fine-tuning, we
introduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting
model learning. Integrated with an Attention-based Multi-ConvGRU model, QPA
enables parameter-efficient training while maintaining predictive accuracy.
This work represents the first application of quantum machine learning (QML) to
large-scale typhoon trajectory prediction, offering a scalable and
energy-efficient approach to climate modeling. Our results demonstrate that QPA
significantly reduces the number of trainable parameters while preserving
performance, making high-performance forecasting more accessible and
sustainable through hybrid quantum-classical learning.

</details>


### [205] [Quantum state-agnostic work extraction (almost) without dissipation](https://arxiv.org/abs/2505.09456)
*Josep Lumbreras,Ruo Cheng Huang,Yanglin Hu,Mile Gu,Marco Tomamichel*

Main category: quant-ph

TL;DR: 论文研究了通过自适应策略优化未知纯量子比特态的能量提取，利用强化学习中的探索-利用权衡，实现了能量耗散的多对数级增长。


<details>
  <summary>Details</summary>
Motivation: 核心挑战在于设计最优的能量提取协议，以平衡电池充电和获取更多信息以提高后续能量收获间的矛盾。

Method: 利用强化学习中的探索-利用权衡设计自适应策略。

Result: 自适应策略实现了能量耗散仅随$N$的多对数级增长，相比基于全态层析的现有协议提升了指数级性能。

Conclusion: 该研究为量子态能量提取提供了更高效的方法，显著优于传统技术。

Abstract: We investigate work extraction protocols designed to transfer the maximum
possible energy to a battery using sequential access to $N$ copies of an
unknown pure qubit state. The core challenge is designing interactions to
optimally balance two competing goals: charging of the battery optimally using
the qubit in hand, and acquiring more information by qubit to improve energy
harvesting in subsequent rounds. Here, we leverage exploration-exploitation
trade-off in reinforcement learning to develop adaptive strategies achieving
energy dissipation that scales only poly-logarithmically in $N$. This
represents an exponential improvement over current protocols based on full
state tomography.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [206] [Ornithologist: Towards Trustworthy "Reasoning" about Central Bank Communications](https://arxiv.org/abs/2505.09083)
*Dominic Zaun Eu Jones*

Main category: econ.GN

TL;DR: Ornithologist是一种弱监督文本分类系统，用于测量中央银行文本的鹰派和鸽派倾向，通过“分类引导推理”提高透明度和可解释性，并减少幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 开发一个透明、易解释且适用于非专家的文本分类系统，以分析央行文本中的鹰派和鸽派倾向。

Method: 使用“分类引导推理”方法，结合大型语言模型和人类编写的决策树，进行弱监督文本分类。

Result: Ornithologist能够有效测量央行文本中的鹰派和鸽派倾向，并对未来现金利率路径和市场预期提供有用信息。

Conclusion: Ornithologist系统在降低监督需求的同时，提高了透明度和可解释性，具有广泛适用性，尤其适用于央行和其他文本分析任务。

Abstract: I develop Ornithologist, a weakly-supervised textual classification system
and measure the hawkishness and dovishness of central bank text. Ornithologist
uses ``taxonomy-guided reasoning'', guiding a large language model with
human-authored decision trees. This increases the transparency and
explainability of the system and makes it accessible to non-experts. It also
reduces hallucination risk. Since it requires less supervision than traditional
classification systems, it can more easily be applied to other problems or
sources of text (e.g. news) without much modification. Ornithologist
measurements of hawkishness and dovishness of RBA communication carry
information about the future of the cash rate path and of market expectations.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [207] [SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation](https://arxiv.org/abs/2505.09081)
*Gaurav Koley*

Main category: cs.SI

TL;DR: 论文提出SALM框架，将语言模型融入社交网络模拟，实现多场景下前所未有的时间稳定性，主要贡献包括分层提示架构、基于注意力的记忆系统和人格稳定性界限。


<details>
  <summary>Details</summary>
Motivation: 现有的基于智能体的社交系统建模方法通常依赖基于规则的行为，难以捕捉复杂动态。作者希望通过语言模型提升社交互动的上下文理解能力。

Method: 采用SALM框架，结合分层提示架构（降低73% token使用）、基于注意力的记忆系统（缓存命中率80%）和人格稳定性界限。

Result: 在SNAP ego网络上验证后，证明该框架能稳定模拟超4000时间步长，保持行为保真度。

Conclusion: SALM是首个基于LLM的框架，能建模长期社交现象，同时维持实证验证的行为准确性。

Abstract: Contemporary approaches to agent-based modeling (ABM) of social systems have
traditionally emphasized rule-based behaviors, limiting their ability to
capture nuanced dynamics by moving beyond predefined rules and leveraging
contextual understanding from LMs of human social interaction. This paper
presents SALM (Social Agent LM Framework), a novel approach for integrating
language models (LMs) into social network simulation that achieves
unprecedented temporal stability in multi-agent scenarios. Our primary
contributions include: (1) a hierarchical prompting architecture enabling
stable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)
an attention-based memory system achieving 80% cache hit rates (95% CI [78%,
82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on
personality stability. Through extensive validation against SNAP ego networks,
we demonstrate the first LLM-based framework capable of modeling long-term
social phenomena while maintaining empirically validated behavioral fidelity.

</details>
