{"id": "2506.03259", "pdf": "https://arxiv.org/pdf/2506.03259", "abs": "https://arxiv.org/abs/2506.03259", "authors": ["Michael E. Garcia-Alcoser", "Mobina GhojoghNejad", "Fakrul Islam Tushar", "David Kim", "Kyle J. Lafata", "Geoffrey D. Rubin", "Joseph Y. Lo"], "title": "Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems", "categories": ["cs.CL", "I.2.7"], "comment": "23 pages, 10 figures, to be submitted in Radiology: Artificial\n  Intelligence", "summary": "Purpose: This study aims to evaluate the effectiveness of large language\nmodels (LLMs) in automating disease annotation of CT radiology reports. We\ncompare a rule-based algorithm (RBA), RadBERT, and three lightweight\nopen-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP)\nCT reports.\n  Materials and Methods: This retrospective study analyzed 40,833 CT reports\nfrom 29,540 patients, with 1,789 CAP reports manually annotated across three\norgan systems. External validation was conducted using the CT-RATE dataset.\nThree open-weight LLMs were tested with zero-shot prompting. Performance was\nevaluated using Cohen's Kappa and micro/macro-averaged F1 scores.\n  Results: In 12,197 Duke CAP reports from 8,854 patients, Llama-3.1 8B and\nGemma-3 27B showed the highest agreement ($\\kappa$ median: 0.87). On the\nmanually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed\nby Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT-RATE\ndataset (lungs/pleura only), Llama-3.1 8B performed best (0.91), with Gemma-3\n27B close behind (0.89). Performance differences were mainly due to differing\nlabeling practices, especially for lung atelectasis.\n  Conclusion: Lightweight LLMs outperform rule-based methods for CT report\nannotation and generalize across organ systems with zero-shot prompting.\nHowever, binary labels alone cannot capture the full nuance of report language.\nLLMs can provide a flexible, efficient solution aligned with clinical judgment\nand user needs.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316CT\u62a5\u544a\u75be\u75c5\u6807\u6ce8\u4e2d\u7684\u6709\u6548\u6027\uff0c\u6bd4\u8f83\u4e86\u89c4\u5219\u7b97\u6cd5\u4e0e\u8f7b\u91cf\u7ea7\u5f00\u6e90\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u81ea\u52a8\u5316\u6807\u6ce8CT\u62a5\u544a\u53ef\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u9700\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6548\u679c\u3002", "method": "\u56de\u987e\u6027\u7814\u7a76\uff0c\u6d4b\u8bd5\u89c4\u5219\u7b97\u6cd5\u548cLLMs\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u8f7b\u91cf\u7ea7LLMs\u8868\u73b0\u4f18\u4e8e\u89c4\u5219\u7b97\u6cd5\uff0c\u5c24\u5176\u662fGemma-3\u548cLlama-3.1\u3002", "conclusion": "LLMs\u63d0\u4f9b\u9ad8\u6548\u7075\u6d3b\u7684\u6807\u6ce8\u65b9\u6848\uff0c\u4f46\u5b58\u5728\u6807\u7b7e\u5c40\u9650\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,CT\u62a5\u544a,\u75be\u75c5\u6807\u6ce8,\u8f7b\u91cf\u7ea7\u6a21\u578b"}}
{"id": "2506.03268", "pdf": "https://arxiv.org/pdf/2506.03268", "abs": "https://arxiv.org/abs/2506.03268", "authors": ["Cristiano Chesi"], "title": "A conclusive remark on linguistic theorizing and language modeling", "categories": ["cs.CL"], "comment": null, "summary": "This is the final remark on the replies received to my target paper in the\nItalian Journal of Linguistics", "AI": {"tldr": "\u5bf9\u76ee\u6807\u8bba\u6587\u5728\u300a\u610f\u5927\u5229\u8bed\u8a00\u5b66\u6742\u5fd7\u300b\u4e0a\u6536\u5230\u7684\u56de\u590d\u7684\u6700\u7ec8\u8bc4\u8bba\u3002", "motivation": "\u603b\u7ed3\u548c\u56de\u5e94\u5bf9\u76ee\u6807\u8bba\u6587\u7684\u53cd\u9988\u3002", "method": "\u5206\u6790\u5e76\u8bc4\u8bba\u6536\u5230\u7684\u56de\u590d\u3002", "result": "\u63d0\u51fa\u4e86\u5bf9\u76ee\u6807\u8bba\u6587\u53cd\u9988\u7684\u7efc\u5408\u89c2\u70b9\u3002", "conclusion": "\u5bf9\u8ba8\u8bba\u8fdb\u884c\u603b\u7ed3\uff0c\u5f3a\u8c03\u4e86\u53cd\u9988\u7684\u91cd\u8981\u6027\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u53ef\u80fd\u65b9\u5411\u3002", "keywords": "\u8bed\u8a00\u5b66, \u8bba\u6587\u8bc4\u8bba, \u53cd\u9988\u5206\u6790"}}
{"id": "2506.03278", "pdf": "https://arxiv.org/pdf/2506.03278", "abs": "https://arxiv.org/abs/2506.03278", "authors": ["Christodoulos Constantinides", "Dhaval Patel", "Shuxin Lin", "Claudio Guerrero", "Sunil Dagajirao Patil", "Jayant Kalagnanam"], "title": "FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships and Failure Modes", "categories": ["cs.CL"], "comment": null, "summary": "We introduce FailureSensorIQ, a novel Multi-Choice Question-Answering (MCQA)\nbenchmarking system designed to assess the ability of Large Language Models\n(LLMs) to reason and understand complex, domain-specific scenarios in Industry\n4.0. Unlike traditional QA benchmarks, our system focuses on multiple aspects\nof reasoning through failure modes, sensor data, and the relationships between\nthem across various industrial assets. Through this work, we envision a\nparadigm shift where modeling decisions are not only data-driven using\nstatistical tools like correlation analysis and significance tests, but also\ndomain-driven by specialized LLMs which can reason about the key contributors\nand useful patterns that can be captured with feature engineering. We evaluate\nthe Industrial knowledge of over a dozen LLMs-including GPT-4, Llama, and\nMistral-on FailureSensorIQ from different lens using\nPerturbation-Uncertainty-Complexity analysis, Expert Evaluation study,\nAsset-Specific Knowledge Gap analysis, ReAct agent using external\nknowledge-bases. Even though closed-source models with strong reasoning\ncapabilities approach expert-level performance, the comprehensive benchmark\nreveals a significant drop in performance that is fragile to perturbations,\ndistractions, and inherent knowledge gaps in the models. We also provide a\nreal-world case study of how LLMs can drive the modeling decisions on 3\ndifferent failure prediction datasets related to various assets. We release:\n(a) expert-curated MCQA for various industrial assets, (b) FailureSensorIQ\nbenchmark and Hugging Face leaderboard based on MCQA built from non-textual\ndata found in ISO documents, and (c) LLMFeatureSelector, an LLM-based feature\nselection scikit-learn pipeline. The software is available at\nhttps://github.com/IBM/FailureSensorIQ.", "AI": {"tldr": "FailureSensorIQ\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u9009\u9898-\u95ee\u7b54\uff08MCQA\uff09\u57fa\u51c6\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5de5\u4e1a4.0\u9886\u57df\u4e2d\u5bf9\u590d\u6742\u573a\u666f\u7684\u63a8\u7406\u548c\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfQA\u57fa\u51c6\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30LLM\u5728\u5de5\u4e1a\u9886\u57df\u4e2d\u7ed3\u5408\u6545\u969c\u6a21\u5f0f\u3001\u4f20\u611f\u5668\u6570\u636e\u53ca\u5176\u5173\u7cfb\u7684\u63a8\u7406\u80fd\u529b\u3002\u8be5\u7cfb\u7edf\u65e8\u5728\u63a8\u52a8\u5efa\u6a21\u51b3\u7b56\u4ece\u6570\u636e\u9a71\u52a8\u5411\u9886\u57df\u9a71\u52a8\u8f6c\u53d8\u3002", "method": "\u901a\u8fc7Perturbation-Uncertainty-Complexity\u5206\u6790\u3001\u4e13\u5bb6\u8bc4\u4f30\u3001\u8d44\u4ea7\u7279\u5b9a\u77e5\u8bc6\u5dee\u8ddd\u5206\u6790\u53ca\u4f7f\u7528\u5916\u90e8\u77e5\u8bc6\u5e93\u7684ReAct\u4ee3\u7406\uff0c\u8bc4\u4f30\u4e86\u591a\u79cdLLM\u7684\u5de5\u4e1a\u77e5\u8bc6\u3002", "result": "\u867d\u7136\u95ed\u6e90\u6a21\u578b\u8868\u73b0\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\uff0c\u4f46\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u5176\u6027\u80fd\u6613\u53d7\u6270\u52a8\u3001\u5e72\u6270\u548c\u77e5\u8bc6\u5dee\u8ddd\u7684\u5f71\u54cd\u3002\u6210\u679c\u8fd8\u5305\u62ecMCQA\u6570\u636e\u96c6\u3001\u57fa\u51c6\u5e73\u53f0\u548c\u57fa\u4e8eLLM\u7684\u7279\u5f81\u9009\u62e9\u5de5\u5177\u3002", "conclusion": "FailureSensorIQ\u63ed\u793a\u4e86LLM\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u548c\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u9886\u57df\u9a71\u52a8\u7684\u5efa\u6a21\u51b3\u7b56\u3002", "keywords": "\u591a\u9009\u9898-\u95ee\u7b54\u57fa\u51c6, \u5de5\u4e1a4.0, \u5927\u8bed\u8a00\u6a21\u578b, \u6545\u969c\u9884\u6d4b, \u7279\u5f81\u9009\u62e9"}}
{"id": "2506.03292", "pdf": "https://arxiv.org/pdf/2506.03292", "abs": "https://arxiv.org/abs/2506.03292", "authors": ["Jiuding Sun", "Sidharth Baskaran", "Zhengxuan Wu", "Michael Sklar", "Christopher Potts", "Atticus Geiger"], "title": "HyperSteer: Activation Steering at Scale with Hypernetworks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Steering language models (LMs) by modifying internal activations is a popular\napproach for controlling text generation. Unsupervised dictionary learning\nmethods, e.g., sparse autoencoders, can be scaled to produce many steering\nvectors, but lack guarantees on the individual efficacy of each vector and\ncontrol over the coverage of relevant steering tasks. In contrast, supervised\nmethods for constructing steering vectors are targeted and effective, but\nrequire more data collection and training for each additional steering vector\nproduced. In this work, we introduce HyperSteer, a family of hypernetwork-based\narchitectures which are trained end-to-end to generate steering vectors\nconditioned on the natural language steering prompts and the internals of the\nsteered LM. In our evaluations, we show that scaling HyperSteer with thousands\nof steering prompts exceeds the performance of state-of-the-art activation\nsteering methods, even on steering prompts never seen during training.\nMoreover, HyperSteer performs on par with steering-via-prompting.", "AI": {"tldr": "HyperSteer\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u7f51\u7edc\u67b6\u6784\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u751f\u6210\u5bfc\u5411\u5411\u91cf\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u76ee\u524d\u7684\u65e0\u76d1\u7763\u5b57\u5178\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u53ef\u6269\u5c55\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6bcf\u4e2a\u5bfc\u5411\u5411\u91cf\u6548\u679c\u7684\u4fdd\u8bc1\uff1b\u76d1\u7763\u65b9\u6cd5\u867d\u6709\u6548\uff0c\u4f46\u9700\u8981\u989d\u5916\u7684\u6570\u636e\u6536\u96c6\u548c\u8bad\u7ec3\u3002HyperSteer\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u63d0\u4f9b\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "HyperSteer\u5229\u7528\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u67b6\u6784\uff0c\u6839\u636e\u81ea\u7136\u8bed\u8a00\u5bfc\u5411\u63d0\u793a\u548c\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u72b6\u6001\uff0c\u7aef\u5230\u7aef\u751f\u6210\u5bfc\u5411\u5411\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHyperSteer\u5728\u6570\u5343\u4e2a\u5bfc\u5411\u63d0\u793a\u4e0b\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6fc0\u6d3b\u5bfc\u5411\u65b9\u6cd5\uff0c\u4e14\u4e0e\u57fa\u4e8e\u63d0\u793a\u7684\u5bfc\u5411\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "HyperSteer\u901a\u8fc7\u7ed3\u5408\u8d85\u7f51\u7edc\u548c\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8bed\u8a00\u6a21\u578b\u5bfc\u5411\u65b9\u6cd5\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b\u3001\u5bfc\u5411\u5411\u91cf\u3001\u8d85\u7f51\u7edc\u3001\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u3001\u6fc0\u6d3b\u5bfc\u5411"}}
{"id": "2506.03205", "pdf": "https://arxiv.org/pdf/2506.03205", "abs": "https://arxiv.org/abs/2506.03205", "authors": ["Umberto Gon\u00e7alves de Sousa"], "title": "Q-ARDNS-Multi: A Multi-Agent Quantum Reinforcement Learning Framework with Meta-Cognitive Adaptation for Complex 3D Environments", "categories": ["cs.AI"], "comment": "17 pages, 5 figures", "summary": "This paper presents Q-ARDNS-Multi, an advanced multi-agent quantum\nreinforcement learning (QRL) framework that extends the ARDNS-FN-Quantum model,\nwhere Q-ARDNS-Multi stands for \"Quantum Adaptive Reward-Driven Neural Simulator\n- Multi-Agent\". It integrates quantum circuits with RY gates, meta-cognitive\nadaptation, and multi-agent coordination mechanisms for complex 3D\nenvironments. Q-ARDNS-Multi leverages a 2-qubit quantum circuit for action\nselection, a dual-memory system inspired by human cognition, a shared memory\nmodule for agent cooperation, and adaptive exploration strategies modulated by\nreward variance and intrinsic motivation. Evaluated in a $10 \\times 10 \\times\n3$ GridWorld environment with two agents over 5000 episodes, Q-ARDNS-Multi\nachieves success rates of 99.6\\% and 99.5\\% for Agents 0 and 1, respectively,\noutperforming Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Soft\nActor-Critic (SAC) in terms of success rate, stability, navigation efficiency,\nand collision avoidance. The framework records mean rewards of $-304.2891 \\pm\n756.4636$ and $-295.7622 \\pm 752.7103$, averaging 210 steps to goal,\ndemonstrating its robustness in dynamic settings. Comprehensive analyses,\nincluding learning curves, reward distributions, statistical tests, and\ncomputational efficiency evaluations, highlight the contributions of quantum\ncircuits and meta-cognitive adaptation. By bridging quantum computing,\ncognitive science, and multi-agent RL, Q-ARDNS-Multi offers a scalable,\nhuman-like approach for applications in robotics, autonomous navigation, and\ndecision-making under uncertainty.", "AI": {"tldr": "Q-ARDNS-Multi\u662f\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u91cf\u5b50\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u91cf\u5b50\u7535\u8def\u548c\u8ba4\u77e5\u673a\u5236\uff0c\u5728\u590d\u67423D\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u7387\u8fbe99.5%\u4ee5\u4e0a\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7ed3\u5408\u91cf\u5b50\u8ba1\u7b97\u3001\u8ba4\u77e5\u79d1\u5b66\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u5f00\u53d1\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u7a33\u5b9a\u4e14\u5177\u6709\u4eba\u7c7b\u8ba4\u77e5\u7279\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u75282\u91cf\u5b50\u6bd4\u7279\u7535\u8def\u9009\u62e9\u52a8\u4f5c\uff0c\u53cc\u91cd\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5171\u4eab\u8bb0\u5fc6\u6a21\u5757\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5956\u52b1\u65b9\u5dee\u548c\u5185\u5728\u52a8\u673a\u7684\u81ea\u9002\u5e94\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u572810\u00d710\u00d73\u7684GridWorld\u73af\u5883\u4e2d\uff0c\u6210\u529f\u7387\u8fbe99.6%\u548c99.5%\uff0c\u5e73\u5747\u5956\u52b1\u548c\u6b65\u9aa4\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8eMADDPG\u548cSAC\u3002", "conclusion": "Q-ARDNS-Multi\u4e3a\u673a\u5668\u4eba\u3001\u81ea\u4e3b\u5bfc\u822a\u548c\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u91cf\u5b50\u5f3a\u5316\u5b66\u4e60, \u591a\u667a\u80fd\u4f53, \u81ea\u9002\u5e94\u63a2\u7d22, \u8ba4\u77e5\u79d1\u5b66"}}
{"id": "2506.03154", "pdf": "https://arxiv.org/pdf/2506.03154", "abs": "https://arxiv.org/abs/2506.03154", "authors": ["Zhaoyang Chen", "Cody Fleming"], "title": "Modular Diffusion Policy Training: Decoupling and Recombining Guidance and Diffusion for Offline RL", "categories": ["cs.LG"], "comment": null, "summary": "Classifier free guidance has shown strong potential in diffusion-based\nreinforcement learning. However, existing methods rely on joint training of the\nguidance module and the diffusion model, which can be suboptimal during the\nearly stages when the guidance is inaccurate and provides noisy learning\nsignals. In offline RL, guidance depends solely on offline data: observations,\nactions, and rewards, and is independent of the policy module's behavior,\nsuggesting that joint training is not required. This paper proposes modular\ntraining methods that decouple the guidance module from the diffusion model,\nbased on three key findings:\n  Guidance Necessity: We explore how the effectiveness of guidance varies with\nthe training stage and algorithm choice, uncovering the roles of guidance and\ndiffusion. A lack of good guidance in the early stage presents an opportunity\nfor optimization.\n  Guidance-First Diffusion Training: We introduce a method where the guidance\nmodule is first trained independently as a value estimator, then frozen to\nguide the diffusion model using classifier-free reward guidance. This\nmodularization reduces memory usage, improves computational efficiency, and\nenhances both sample efficiency and final performance.\n  Cross-Module Transferability: Applying two independently trained guidance\nmodels, one during training and the other during inference, can significantly\nreduce normalized score variance (e.g., reducing IQR by 86%). We show that\nguidance modules trained with one algorithm (e.g., IDQL) can be directly reused\nwith another (e.g., DQL), with no additional training required, demonstrating\nbaseline-level performance as well as strong modularity and transferability.\n  We provide theoretical justification and empirical validation on bullet D4RL\nbenchmarks. Our findings suggest a new paradigm for offline RL: modular,\nreusable, and composable training pipelines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5c06\u6307\u5bfc\u6a21\u5757\u4e0e\u6269\u6563\u6a21\u578b\u89e3\u8026\uff0c\u63d0\u5347\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8054\u5408\u8bad\u7ec3\u6307\u5bfc\u6a21\u5757\u548c\u6269\u6563\u6a21\u578b\uff0c\u65e9\u671f\u9636\u6bb5\u6307\u5bfc\u4e0d\u51c6\u786e\u65f6\u4f1a\u63d0\u4f9b\u566a\u58f0\u5b66\u4e60\u4fe1\u53f7\u3002\u79bb\u7ebfRL\u4e2d\u6307\u5bfc\u4ec5\u4f9d\u8d56\u4e8e\u79bb\u7ebf\u6570\u636e\uff0c\u8868\u660e\u8054\u5408\u8bad\u7ec3\u975e\u5fc5\u9700\u3002", "method": "1. \u63a2\u7d22\u6307\u5bfc\u7684\u5fc5\u8981\u6027\uff1b2. \u63d0\u51fa\u6307\u5bfc\u4f18\u5148\u7684\u6269\u6563\u8bad\u7ec3\u65b9\u6cd5\uff1b3. \u5b9e\u73b0\u8de8\u6a21\u5757\u53ef\u8f6c\u79fb\u6027\uff0c\u72ec\u7acb\u8bad\u7ec3\u7684\u6307\u5bfc\u6a21\u5757\u53ef\u76f4\u63a5\u590d\u7528\u3002", "result": "\u6a21\u5757\u5316\u65b9\u6cd5\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u3001\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u6837\u672c\u6548\u7387\uff0c\u8de8\u6a21\u5757\u8f6c\u79fb\u6027\u663e\u8457\u964d\u4f4e\u5206\u6570\u65b9\u5dee\uff08\u5982IQR\u51cf\u5c1186%\uff09\u3002", "conclusion": "\u7814\u7a76\u4e3a\u79bb\u7ebfRL\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u3001\u53ef\u91cd\u7528\u548c\u53ef\u7ec4\u5408\u7684\u8bad\u7ec3\u6d41\u7a0b\u65b0\u8303\u5f0f\u3002", "keywords": "\u6269\u6563\u5f3a\u5316\u5b66\u4e60, \u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60, \u6a21\u5757\u5316\u8bad\u7ec3, \u6307\u5bfc\u6a21\u5757, \u53ef\u8f6c\u79fb\u6027"}}
{"id": "2506.03295", "pdf": "https://arxiv.org/pdf/2506.03295", "abs": "https://arxiv.org/abs/2506.03295", "authors": ["Yubo Wang", "Ping Nie", "Kai Zou", "Lijun Wu", "Wenhu Chen"], "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess\nimmense reasoning potential inherited from the pre-training stage. With\nreinforcement learning (RL), these models can improve dramatically on reasoning\ntasks. Recent studies have shown that even RL on a single problem can unleash\nthese models' reasoning capabilities. However, RL is not only expensive but\nalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises a\ncritical question: Is there a more efficient way to unleash the reasoning\npotential of these powerful base LLMs? In this work, we demonstrate that\nCritique Fine-Tuning (CFT) on only one problem can effectively unleash the\nreasoning potential of LLMs. Our method constructs critique data by collecting\ndiverse model-generated solutions to a single problem and using teacher LLMs to\nprovide detailed critiques. We fine-tune Qwen and Llama family models, ranging\nfrom 1.5B to 14B parameters, on the CFT data and observe significant\nperformance gains across diverse reasoning tasks. For example, with just 5 GPU\nhours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six\nmath benchmarks and 16% on three logic reasoning benchmarks. These results are\ncomparable to or even surpass the results from RL with 20x less compute.\nAblation studies reveal the robustness of one-shot CFT across different prompt\nproblems. These results highlight one-shot CFT as a simple, general, and\ncompute-efficient approach to unleashing the reasoning capabilities of modern\nLLMs.", "AI": {"tldr": "\u901a\u8fc7Critique Fine-Tuning (CFT)\u65b9\u6cd5\uff0c\u4ec5\u5728\u4e00\u4e2a\u95ee\u9898\u4e0a\u8fdb\u884c\u5fae\u8c03\u5373\u53ef\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u8ba1\u7b97\u6548\u7387\u8fdc\u9ad8\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0d\u7a33\u5b9a\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528CFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u6536\u96c6\u6a21\u578b\u5bf9\u5355\u4e2a\u95ee\u9898\u7684\u591a\u6837\u5316\u89e3\u51b3\u65b9\u6848\u5e76\u5229\u7528\u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u8be6\u7ec6\u6279\u8bc4\uff0c\u6784\u5efa\u5fae\u8c03\u6570\u636e\u3002", "result": "\u5728\u4ec55 GPU\u5c0f\u65f6\u7684\u8bad\u7ec3\u4e0b\uff0cQwen-Math-7B-CFT\u5728\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u5e73\u5747\u63d0\u534715%-16%\uff0c\u8ba1\u7b97\u6548\u7387\u4e3aRL\u768420\u500d\u3002", "conclusion": "CFT\u662f\u4e00\u79cd\u7b80\u5355\u3001\u901a\u7528\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "keywords": "Critique Fine-Tuning (CFT), \u5927\u8bed\u8a00\u6a21\u578b, \u63a8\u7406\u80fd\u529b, \u8ba1\u7b97\u6548\u7387, \u5f3a\u5316\u5b66\u4e60"}}
{"id": "2506.03233", "pdf": "https://arxiv.org/pdf/2506.03233", "abs": "https://arxiv.org/abs/2506.03233", "authors": ["Andrea Ferrario"], "title": "A Trustworthiness-based Metaphysics of Artificial Intelligence Systems", "categories": ["cs.AI", "cs.CY", "cs.LG"], "comment": "To appear in the proceedings of 2025 ACM Conference on Fairness,\n  Accountability, and Transparency (FAccT '25)", "summary": "Modern AI systems are man-made objects that leverage machine learning to\nsupport our lives across a myriad of contexts and applications. Despite\nextensive epistemological and ethical debates, their metaphysical foundations\nremain relatively under explored. The orthodox view simply suggests that AI\nsystems, as artifacts, lack well-posed identity and persistence conditions --\ntheir metaphysical kinds are no real kinds. In this work, we challenge this\nperspective by introducing a theory of metaphysical identity of AI systems. We\ndo so by characterizing their kinds and introducing identity criteria -- formal\nrules that answer the questions \"When are two AI systems the same?\" and \"When\ndoes an AI system persist, despite change?\" Building on Carrara and Vermaas'\naccount of fine-grained artifact kinds, we argue that AI trustworthiness\nprovides a lens to understand AI system kinds and formalize the identity of\nthese artifacts by relating their functional requirements to their physical\nmake-ups. The identity criteria of AI systems are determined by their\ntrustworthiness profiles -- the collection of capabilities that the systems\nmust uphold over time throughout their artifact histories, and their\neffectiveness in maintaining these capabilities. Our approach suggests that the\nidentity and persistence of AI systems is sensitive to the socio-technical\ncontext of their design and utilization via their trustworthiness, providing a\nsolid metaphysical foundation to the epistemological, ethical, and legal\ndiscussions about these artifacts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6311\u6218\u4e86\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7f3a\u4e4f\u660e\u786e\u5f62\u800c\u4e0a\u5b66\u8eab\u4efd\u7684\u4f20\u7edf\u89c2\u70b9\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u4efb\u5ea6\u7684AI\u7cfb\u7edf\u5f62\u800c\u4e0a\u5b66\u8eab\u4efd\u7406\u8bba\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9AI\u7cfb\u7edf\u7684\u5f62\u800c\u4e0a\u5b66\u57fa\u7840\u63a2\u8ba8\u4e0d\u8db3\uff0c\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3aAI\u7cfb\u7edf\u7f3a\u4e4f\u660e\u786e\u8eab\u4efd\u548c\u6301\u4e45\u6027\u6761\u4ef6\u3002", "method": "\u901a\u8fc7\u7ed3\u5408Carrara\u548cVermaas\u7684\u7cbe\u7ec6\u4eba\u5de5\u7269\u79cd\u7c7b\u7406\u8bba\uff0c\u4ee5AI\u7cfb\u7edf\u7684\u4fe1\u4efb\u5ea6\u4e3a\u89c6\u89d2\uff0c\u5c06\u5176\u529f\u80fd\u9700\u6c42\u4e0e\u7269\u7406\u6784\u6210\u8054\u7cfb\uff0c\u63d0\u51fa\u8eab\u4efd\u6807\u51c6\u3002", "result": "AI\u7cfb\u7edf\u7684\u8eab\u4efd\u548c\u6301\u4e45\u6027\u7531\u5176\u4fe1\u4efb\u5ea6\u6587\u4ef6\u51b3\u5b9a\uff0c\u8fd9\u4e9b\u6587\u4ef6\u7cfb\u7edf\u5fc5\u987b\u5728\u5176\u5386\u53f2\u4e2d\u7ef4\u62a4\u7684\u80fd\u529b\u96c6\u5408\u3002", "conclusion": "AI\u7cfb\u7edf\u7684\u8eab\u4efd\u548c\u6301\u4e45\u6027\u53d7\u5176\u8bbe\u8ba1\u548c\u4f7f\u7528\u7684\u793e\u4f1a\u6280\u672f\u80cc\u666f\u5f71\u54cd\uff0c\u4e3a\u76f8\u5173\u8ba8\u8bba\u63d0\u4f9b\u4e86\u5f62\u800c\u4e0a\u5b66\u57fa\u7840\u3002", "keywords": "AI\u7cfb\u7edf,\u5f62\u800c\u4e0a\u5b66\u8eab\u4efd,\u4fe1\u4efb\u5ea6,\u793e\u4f1a\u6280\u672f\u80cc\u666f,\u4eba\u5de5\u7269\u79cd\u7c7b"}}
{"id": "2506.03155", "pdf": "https://arxiv.org/pdf/2506.03155", "abs": "https://arxiv.org/abs/2506.03155", "authors": ["Yu Zheng"], "title": "Fusing Cross-Domain Knowledge from Multimodal Data to Solve Problems in the Physical World", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The proliferation of artificial intelligence has enabled a diversity of\napplications that bridge the gap between digital and physical worlds. As\nphysical environments are too complex to model through a single information\nacquisition approach, it is crucial to fuse multimodal data generated by\ndifferent sources, such as sensors, devices, systems, and people, to solve a\nproblem in the real world. Unfortunately, it is neither applicable nor\nsustainable to deploy new resources to collect original data from scratch for\nevery problem. Thus, when data is inadequate in the domain of problem, it is\nvital to fuse knowledge from multimodal data that is already available in other\ndomains. We call this cross-domain knowledge fusion. Existing research focus on\nfusing multimodal data in a single domain, supposing the knowledge from\ndifferent datasets is intrinsically aligned; however, this assumption may not\nhold in the scenarios of cross-domain knowledge fusion. In this paper, we\nformally define the cross-domain multimodal data fusion problem, discussing its\nunique challenges, differences and advantages beyond data fusion in a single\ndomain. We propose a four-layer framework, consisting of Domains, Links, Models\nand Data layers, answering three key questions: \"what to fuse\", \"why can be\nfused\", and \"how to fuse\". The Domains Layer selects relevant data from\ndifferent domains for a given problem. The Links Layer reveals the philosophy\nof knowledge alignment beyond specific model structures. The Models Layer\nprovides two knowledge fusion paradigms based on the fundamental mechanisms for\nprocessing data. The Data Layer turns data of different structures,\nresolutions, scales and distributions into a consistent representation that can\nbe fed into an AI model. With this framework, we can design end-to-end\nsolutions that fuse cross-domain multimodal data effectively for solving\nreal-world problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u57df\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u7684\u56db\u5c42\u6846\u67b6\uff08Domains\u3001Links\u3001Models\u3001Data\uff09\uff0c\u89e3\u51b3\u8de8\u57df\u77e5\u8bc6\u878d\u5408\u7684\u72ec\u7279\u6311\u6218\uff0c\u5e76\u63a2\u8ba8\u5176\u4f18\u52bf\u548c\u5b9e\u73b0\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u7684\u590d\u6742\u6027\u9700\u8981\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u4f46\u5355\u4e00\u57df\u7684\u6570\u636e\u4e0d\u8db3\u4ee5\u89e3\u51b3\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u8de8\u57df\u77e5\u8bc6\u878d\u5408\u3002\u73b0\u6709\u7814\u7a76\u5047\u8bbe\u591a\u6a21\u6001\u6570\u636e\u5728\u5355\u4e00\u57df\u5185\u5bf9\u9f50\uff0c\u8fd9\u5728\u8de8\u57df\u573a\u666f\u53ef\u80fd\u4e0d\u9002\u7528\u3002", "method": "\u63d0\u51fa\u56db\u5c42\u6846\u67b6\uff08Domains\u3001Links\u3001Models\u3001Data\uff09\uff0c\u5206\u522b\u56de\u7b54\u201c\u878d\u5408\u4ec0\u4e48\u201d\u3001\u201c\u4e3a\u4f55\u80fd\u878d\u5408\u201d\u548c\u201c\u5982\u4f55\u878d\u5408\u201d\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bbe\u8ba1\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u878d\u5408\u8de8\u57df\u591a\u6a21\u6001\u6570\u636e\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u3002", "conclusion": "\u8de8\u57df\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u7814\u7a76\u4e0d\u8db3\u7684\u9886\u57df\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u3002", "keywords": "\u8de8\u57df\u878d\u5408, \u591a\u6a21\u6001\u6570\u636e, \u77e5\u8bc6\u5bf9\u9f50, \u56db\u5c42\u6846\u67b6"}}
{"id": "2506.03301", "pdf": "https://arxiv.org/pdf/2506.03301", "abs": "https://arxiv.org/abs/2506.03301", "authors": ["Daham M. Mustafa", "Abhishek Nadgeri", "Diego Collarana", "Benedikt T. Arnold", "Christoph Quix", "Christoph Lange", "Stefan Decker"], "title": "From Instructions to ODRL Usage Policies: An Ontology Guided Approach", "categories": ["cs.CL", "F.2.2; I.2.7; H.3.3"], "comment": "The paper is accepted at LLM+KG: International Workshop on Data\n  Management Opportunities in Unifying Large Language Models + Knowledge\n  Graphs, VLDB 2024, August 26, 2024, Guangzhou, China.\n  https://vldb.org/workshops/2024/proceedings/LLM+KG/LLM+KG-15.pdf", "summary": "This study presents an approach that uses large language models such as GPT-4\nto generate usage policies in the W3C Open Digital Rights Language ODRL\nautomatically from natural language instructions. Our approach uses the ODRL\nontology and its documentation as a central part of the prompt. Our research\nhypothesis is that a curated version of existing ontology documentation will\nbetter guide policy generation. We present various heuristics for adapting the\nODRL ontology and its documentation to guide an end-to-end KG construction\nprocess. We evaluate our approach in the context of dataspaces, i.e.,\ndistributed infrastructures for trustworthy data exchange between multiple\nparticipating organizations for the cultural domain. We created a benchmark\nconsisting of 12 use cases of varying complexity. Our evaluation shows\nexcellent results with up to 91.95% accuracy in the resulting knowledge graph.", "AI": {"tldr": "\u5229\u7528GPT-4\u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u751f\u6210ODRL\u4f7f\u7528\u653f\u7b56\uff0c\u6548\u679c\u663e\u8457\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210ODRL\u653f\u7b56\uff0c\u63d0\u5347\u653f\u7b56\u751f\u6210\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u5229\u7528ODRL\u672c\u4f53\u53ca\u5176\u6587\u6863\u4f5c\u4e3a\u63d0\u793a\u6838\u5fc3\uff0c\u901a\u8fc7\u542f\u53d1\u5f0f\u65b9\u6cd5\u6307\u5bfc\u548c\u8bc4\u4f30\u7aef\u5230\u7aef\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u8fc7\u7a0b\u3002", "result": "\u572812\u4e2a\u590d\u6742\u7a0b\u5ea6\u4e0d\u540c\u7684\u7528\u4f8b\u4e2d\uff0c\u751f\u6210\u7684\u77e5\u8bc6\u56fe\u8c31\u51c6\u786e\u7387\u9ad8\u8fbe91.95%\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u672c\u4f53\u6587\u6863\u6307\u5bfc\u653f\u7b56\u751f\u6210\u662f\u6709\u6548\u7684\uff0c\u4e14\u5728\u6570\u636e\u7a7a\u95f4\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, ODRL, \u77e5\u8bc6\u56fe\u8c31, \u6570\u636e\u7a7a\u95f4"}}
{"id": "2506.03315", "pdf": "https://arxiv.org/pdf/2506.03315", "abs": "https://arxiv.org/abs/2506.03315", "authors": ["Kai Sauerwald", "Kenneth Skiba", "Eduardo Ferm\u00e9", "Thomas Meyer"], "title": "Axiomatics of Restricted Choices by Linear Orders of Sets with Minimum as Fallback", "categories": ["cs.AI", "cs.LO", "03E99, 91B14", "I.2.4"], "comment": null, "summary": "We study how linear orders can be employed to realise choice functions for\nwhich the set of potential choices is restricted, i.e., the possible choice is\nnot possible among the full powerset of all alternatives. In such restricted\nsettings, constructing a choice function via a relation on the alternatives is\nnot always possible. However, we show that one can always construct a choice\nfunction via a linear order on sets of alternatives, even when a fallback value\nis encoded as the minimal element in the linear order. The axiomatics of such\nchoice functions are presented for the general case and the case of\nunion-closed input restrictions. Restricted choice structures have applications\nin knowledge representation and reasoning, and here we discuss their\napplications for theory change and abstract argumentation.", "AI": {"tldr": "\u7814\u7a76\u5982\u4f55\u5728\u9650\u5236\u9009\u62e9\u96c6\u7684\u60c5\u51b5\u4e0b\u5229\u7528\u7ebf\u6027\u5e8f\u5b9e\u73b0\u9009\u62e9\u51fd\u6570\uff0c\u5e76\u8bc1\u660e\u5373\u4f7f\u9009\u62e9\u96c6\u4e0d\u5b8c\u5168\uff0c\u4e5f\u80fd\u901a\u8fc7\u7ebf\u6027\u5e8f\u6784\u9020\u9009\u62e9\u51fd\u6570\u3002", "motivation": "\u63a2\u8ba8\u9650\u5236\u9009\u62e9\u96c6\u65f6\u9009\u62e9\u51fd\u6570\u7684\u6784\u9020\u65b9\u6cd5\u53ca\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u7ebf\u6027\u5e8f\u6784\u9020\u9009\u62e9\u51fd\u6570\uff0c\u5e76\u91c7\u7528\u5305\u542b\u56de\u9000\u503c\u7684\u7ebf\u6027\u5e8f\u3002", "result": "\u8bc1\u660e\u4e86\u5373\u4f7f\u9009\u62e9\u96c6\u53d7\u9650\uff0c\u4ecd\u80fd\u901a\u8fc7\u7ebf\u6027\u5e8f\u6784\u9020\u9009\u62e9\u51fd\u6570\uff0c\u5e76\u7ed9\u51fa\u4e86\u5176\u516c\u7406\u5316\u63cf\u8ff0\u3002", "conclusion": "\u7ebf\u6027\u5e8f\u53ef\u7528\u4e8e\u6784\u5efa\u9650\u5236\u9009\u62e9\u96c6\u7684\u9009\u62e9\u51fd\u6570\uff0c\u9002\u7528\u4e8e\u77e5\u8bc6\u8868\u793a\u4e0e\u63a8\u7406\u4e2d\u7684\u591a\u4e2a\u9886\u57df\u3002", "keywords": "\u7ebf\u6027\u5e8f, \u9009\u62e9\u51fd\u6570, \u77e5\u8bc6\u8868\u793a, \u7406\u8bba\u4fee\u6539, \u62bd\u8c61\u8bba\u8bc1"}}
{"id": "2506.03158", "pdf": "https://arxiv.org/pdf/2506.03158", "abs": "https://arxiv.org/abs/2506.03158", "authors": ["Jiahao Qin", "Bei Peng", "Feng Liu", "Guangliang Cheng", "Lu Zong"], "title": "DUAL: Dynamic Uncertainty-Aware Learning", "categories": ["cs.LG", "cs.CV"], "comment": "12 pages, 3 figures", "summary": "Deep learning models frequently encounter feature uncertainty in diverse\nlearning scenarios, significantly impacting their performance and reliability.\nThis challenge is particularly complex in multi-modal scenarios, where models\nmust integrate information from different sources with inherent uncertainties.\nWe propose Dynamic Uncertainty-Aware Learning (DUAL), a unified framework that\neffectively handles feature uncertainty in both single-modal and multi-modal\nscenarios. DUAL introduces three key innovations: Dynamic Feature Uncertainty\nModeling, which continuously refines uncertainty estimates through joint\nconsideration of feature characteristics and learning dynamics; Adaptive\nDistribution-Aware Modulation, which maintains balanced feature distributions\nthrough dynamic sample influence adjustment; and Uncertainty-aware Cross-Modal\nRelationship Learning, which explicitly models uncertainties in cross-modal\ninteractions. Through extensive experiments, we demonstrate DUAL's\neffectiveness across multiple domains: in computer vision tasks, it achieves\nsubstantial improvements of 7.1% accuracy on CIFAR-10, 6.5% accuracy on\nCIFAR-100, and 2.3% accuracy on Tiny-ImageNet; in multi-modal learning, it\ndemonstrates consistent gains of 4.1% accuracy on CMU-MOSEI and 2.8% accuracy\non CMU-MOSI for sentiment analysis, while achieving 1.4% accuracy improvements\non MISR. The code will be available on GitHub soon.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDUAL\u7684\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u573a\u666f\u4e2d\u7684\u7279\u5f81\u4e0d\u786e\u5b9a\u6027\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5904\u7406\u7279\u5f81\u4e0d\u786e\u5b9a\u6027\u65f6\u6027\u80fd\u53d7\u9650\uff0c\u5c24\u5176\u662f\u5728\u591a\u6a21\u6001\u573a\u666f\u4e2d\u3002DUAL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DUAL\u5305\u542b\u4e09\u4e2a\u521b\u65b0\u70b9\uff1a\u52a8\u6001\u7279\u5f81\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3001\u81ea\u9002\u5e94\u5206\u5e03\u611f\u77e5\u8c03\u5236\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8de8\u6a21\u6001\u5173\u7cfb\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDUAL\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86CIFAR-10\uff087.1%\uff09\u3001CIFAR-100\uff086.5%\uff09\u548cTiny-ImageNet\uff082.3%\uff09\u7684\u51c6\u786e\u7387\uff1b\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u63d0\u5347\u4e86CMU-MOSEI\uff084.1%\uff09\u3001CMU-MOSI\uff082.8%\uff09\u548cMISR\uff081.4%\uff09\u7684\u51c6\u786e\u7387\u3002", "conclusion": "DUAL\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5728\u7279\u5f81\u4e0d\u786e\u5b9a\u6027\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "keywords": "\u6df1\u5ea6\u5b66\u4e60,\u7279\u5f81\u4e0d\u786e\u5b9a\u6027,\u591a\u6a21\u6001\u5b66\u4e60,\u52a8\u6001\u5efa\u6a21,\u81ea\u9002\u5e94\u8c03\u5236"}}
{"id": "2506.03303", "pdf": "https://arxiv.org/pdf/2506.03303", "abs": "https://arxiv.org/abs/2506.03303", "authors": ["Mustafa Eyceoz", "Nikhil Shivakumar Nayak", "Hao Wang", "Ligong Han", "Akash Srivastava"], "title": "Hopscotch: Discovering and Skipping Redundancies in Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.2.6; I.2.4"], "comment": "10 pages, 4 figures, 9 tables", "summary": "Modern causal language models stack many attention blocks to improve\nperformance, but not all blocks are necessary for every task. We propose\nHopscotch, a simple yet effective method that identifies and skips attention\nblocks with least contributions to a task and adapts to preserve output\nquality. Hopscotch jointly optimizes which blocks to skip and how to scale the\noutputs of the remaining layers. By introducing lightweight, trainable scaling\nparameters to attention and MLP blocks, it mitigates distribution shifts in\nhidden states caused by removing attention blocks. Hopscotch does not modify\nmodel weights or require access to pretraining or instruction-tuning data, and\nis compatible with existing model compression techniques. When applied to\n$\\texttt{Llama-3.1-8B}$ and $\\texttt{Qwen2.5-7B}$, Hopscotch achieves less than\na 2% drop in performance even after skipping four attention blocks.", "AI": {"tldr": "Hopscotch\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8df3\u8fc7\u8d21\u732e\u6700\u5c0f\u7684\u6ce8\u610f\u529b\u5757\u5e76\u8c03\u6574\u5269\u4f59\u5c42\u8f93\u51fa\u6765\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u5806\u53e0\u5927\u91cf\u6ce8\u610f\u529b\u5757\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u5e76\u975e\u6240\u6709\u5757\u5bf9\u6bcf\u4e2a\u4efb\u52a1\u90fd\u662f\u5fc5\u8981\u7684\u3002", "method": "Hopscotch\u8054\u5408\u4f18\u5316\u8df3\u8fc7\u54ea\u4e9b\u6ce8\u610f\u529b\u5757\u4ee5\u53ca\u5982\u4f55\u7f29\u653e\u5269\u4f59\u5c42\u7684\u8f93\u51fa\uff0c\u5f15\u5165\u8f7b\u91cf\u53ef\u8bad\u7ec3\u7684\u7f29\u653e\u53c2\u6570\u4ee5\u7f13\u89e3\u9690\u85cf\u72b6\u6001\u5206\u5e03\u504f\u79fb\u3002", "result": "\u5728Llama-3.1-8B\u548cQwen2.5-7B\u4e0a\uff0cHopscotch\u5728\u8df3\u8fc7\u56db\u4e2a\u6ce8\u610f\u529b\u5757\u540e\u6027\u80fd\u4e0b\u964d\u4e0d\u52302%\u3002", "conclusion": "Hopscotch\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u6743\u91cd\u6216\u4f9d\u8d56\u9884\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u4f18\u5316\u4e86\u6a21\u578b\u6027\u80fd\u3002", "keywords": "\u6a21\u578b\u4f18\u5316, \u6ce8\u610f\u529b\u673a\u5236, \u8f7b\u91cf\u7ea7, \u6027\u80fd\u63d0\u5347"}}
{"id": "2506.03332", "pdf": "https://arxiv.org/pdf/2506.03332", "abs": "https://arxiv.org/abs/2506.03332", "authors": ["Yifei Ming", "Zixuan Ke", "Xuan-Phi Nguyen", "Jiayu Wang", "Shafiq Joty"], "title": "Helpful Agent Meets Deceptive Judge: Understanding Vulnerabilities in Agentic Workflows", "categories": ["cs.AI"], "comment": null, "summary": "Agentic workflows -- where multiple large language model (LLM) instances\ninteract to solve tasks -- are increasingly built on feedback mechanisms, where\none model evaluates and critiques another. Despite the promise of\nfeedback-driven improvement, the stability of agentic workflows rests on the\nreliability of the judge. However, judges may hallucinate information, exhibit\nbias, or act adversarially -- introducing critical vulnerabilities into the\nworkflow. In this work, we present a systematic analysis of agentic workflows\nunder deceptive or misleading feedback. We introduce a two-dimensional\nframework for analyzing judge behavior, along axes of intent (from constructive\nto malicious) and knowledge (from parametric-only to retrieval-augmented\nsystems). Using this taxonomy, we construct a suite of judge behaviors and\ndevelop WAFER-QA, a new benchmark with critiques grounded in retrieved web\nevidence to evaluate robustness of agentic workflows against factually\nsupported adversarial feedback. We reveal that even strongest agents are\nvulnerable to persuasive yet flawed critiques -- often switching correct\nanswers after a single round of misleading feedback. Taking a step further, we\nstudy how model predictions evolve over multiple rounds of interaction,\nrevealing distinct behavioral patterns between reasoning and non-reasoning\nmodels. Our findings highlight fundamental vulnerabilities in feedback-based\nworkflows and offer guidance for building more robust agentic systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\u53cd\u9988\u673a\u5236\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u6cd5\u5b98\u884c\u4e3a\u7684\u6846\u67b6\uff0c\u5e76\u5f00\u53d1\u4e86WAFER-QA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u5373\u4f7f\u662f\u5f3a\u5927\u7684\u6a21\u578b\u4e5f\u6613\u53d7\u8bef\u5bfc\u6027\u53cd\u9988\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u53cd\u9988\u673a\u5236\u7684\u53ef\u9760\u6027\u6210\u4e3a\u5173\u952e\u95ee\u9898\uff0c\u6cd5\u5b98\u53ef\u80fd\u4ea7\u751f\u5e7b\u89c9\u4fe1\u606f\u6216\u504f\u89c1\uff0c\u5bfc\u81f4\u5de5\u4f5c\u6d41\u8106\u5f31\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e8c\u7ef4\u6846\u67b6\u5206\u6790\u6cd5\u5b98\u884c\u4e3a\uff08\u610f\u56fe\u4e0e\u77e5\u8bc6\u7ef4\u5ea6\uff09\uff0c\u6784\u5efa\u4e86\u4e00\u7cfb\u5217\u884c\u4e3a\u5e76\u5f00\u53d1\u4e86WAFER-QA\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u53d1\u73b0\u5373\u4f7f\u6700\u5f3a\u4ee3\u7406\u4e5f\u6613\u53d7\u8bef\u5bfc\u6027\u53cd\u9988\u5f71\u54cd\uff0c\u6b63\u786e\u7b54\u6848\u5bb9\u6613\u56e0\u5355\u6b21\u53cd\u9988\u800c\u6539\u53d8\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u8868\u73b0\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "\u53cd\u9988\u673a\u5236\u5b58\u5728\u56fa\u6709\u8106\u5f31\u6027\uff0c\u7814\u7a76\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "keywords": "\u4ee3\u7406\u5de5\u4f5c\u6d41\u3001\u53cd\u9988\u673a\u5236\u3001\u6cd5\u5b98\u884c\u4e3a\u3001WAFER-QA\u3001\u8106\u5f31\u6027"}}
{"id": "2506.03159", "pdf": "https://arxiv.org/pdf/2506.03159", "abs": "https://arxiv.org/abs/2506.03159", "authors": ["Lesley Wheat", "Martin v. Mohrenschildt", "Saeid Habibi"], "title": "Bayes Error Rate Estimation in Difficult Situations", "categories": ["cs.LG", "stat.ME", "stat.ML"], "comment": "21 pages, 13 figures, 20 tables", "summary": "The Bayes Error Rate (BER) is the fundamental limit on the achievable\ngeneralizable classification accuracy of any machine learning model due to\ninherent uncertainty within the data. BER estimators offer insight into the\ndifficulty of any classification problem and set expectations for optimal\nclassification performance. In order to be useful, the estimators must also be\naccurate with a limited number of samples on multivariate problems with unknown\nclass distributions. To determine which estimators meet the minimum\nrequirements for \"usefulness\", an in-depth examination of their accuracy is\nconducted using Monte Carlo simulations with synthetic data in order to obtain\ntheir confidence bounds for binary classification. To examine the usability of\nthe estimators on real-world applications, new test scenarios are introduced\nupon which 2500 Monte Carlo simulations per scenario are run over a wide range\nof BER values. In a comparison of k-Nearest Neighbor (kNN), Generalized\nHenze-Penrose (GHP) divergence and Kernel Density Estimation (KDE) techniques,\nresults show that kNN is overwhelmingly the more accurate non-parametric\nestimator. In order to reach the target of an under 5 percent range for the 95\npercent confidence bounds, the minimum number of required samples per class is\n1000. As more features are added, more samples are needed, so that 2500 samples\nper class are required at only 4 features. Other estimators do become more\naccurate than kNN as more features are added, but continuously fail to meet the\ntarget range.", "AI": {"tldr": "\u8d1d\u53f6\u65af\u9519\u8bef\u7387\uff08BER\uff09\u662f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5206\u7c7b\u51c6\u786e\u6027\u7684\u7406\u8bba\u6781\u9650\uff0c\u672c\u6587\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u6bd4\u8f83\u4e86\u4e0d\u540c\u4f30\u8ba1\u65b9\u6cd5\u7684\u51c6\u786e\u6027\uff0c\u53d1\u73b0kNN\u5728\u975e\u53c2\u6570\u4f30\u8ba1\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "motivation": "\u7814\u7a76BER\u4f30\u8ba1\u65b9\u6cd5\u5728\u6709\u9650\u6837\u672c\u548c\u591a\u53d8\u91cf\u95ee\u9898\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u5206\u7c7b\u95ee\u9898\u7684\u96be\u5ea6\u548c\u6700\u4f18\u6027\u80fd\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u5229\u7528\u8499\u7279\u5361\u6d1b\u6a21\u62df\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u6bd4\u8f83kNN\u3001\u5e7f\u4e49Henze-Penrose\uff08GHP\uff09\u6563\u5ea6\u548c\u6838\u5bc6\u5ea6\u4f30\u8ba1\uff08KDE\uff09\u65b9\u6cd5\u5728\u4e8c\u5143\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002", "result": "kNN\u662f\u6700\u4f18\u7684\u975e\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4f46\u9700\u8981\u6bcf\u7c7b\u81f3\u5c111000\u4e2a\u6837\u672c\u624d\u80fd\u8fbe\u523095%\u7f6e\u4fe1\u533a\u95f4\u8bef\u5dee\u4f4e\u4e8e5%\u7684\u76ee\u6807\u3002\u7279\u5f81\u589e\u591a\u65f6\uff0c\u6837\u672c\u9700\u6c42\u589e\u52a0\u3002", "conclusion": "kNN\u5728BER\u4f30\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9ad8\u7ef4\u95ee\u9898\u4e2d\u6837\u672c\u9700\u6c42\u91cf\u663e\u8457\u589e\u52a0\uff0c\u5176\u4ed6\u65b9\u6cd5\u5728\u9ad8\u7ef4\u65f6\u867d\u6709\u6539\u8fdb\u4f46\u4ecd\u672a\u8fbe\u6807\u3002", "keywords": "\u8d1d\u53f6\u65af\u9519\u8bef\u7387\uff08BER\uff09\u3001kNN\u3001GHP\u6563\u5ea6\u3001KDE\u3001\u8499\u7279\u5361\u6d1b\u6a21\u62df\u3001\u5206\u7c7b\u95ee\u9898"}}
{"id": "2506.03310", "pdf": "https://arxiv.org/pdf/2506.03310", "abs": "https://arxiv.org/abs/2506.03310", "authors": ["Guillermo Marco", "Julio Gonzalo", "V\u00edctor Fresno"], "title": "The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of AI Creative Writing", "categories": ["cs.CL", "cs.HC"], "comment": "Camera-ready version, 14 pages, 3 figures. Accepted to Findings of\n  the Association for Computational Linguistics (ACL) 2025. Code & data:\n  https://github.com/grmarco/the-reader-is-the-metric", "summary": "Recent studies comparing AI-generated and human-authored literary texts have\nproduced conflicting results: some suggest AI already surpasses human quality,\nwhile others argue it still falls short. We start from the hypothesis that such\ndivergences can be largely explained by genuine differences in how readers\ninterpret and value literature, rather than by an intrinsic quality of the\ntexts evaluated. Using five public datasets (1,471 stories, 101 annotators\nincluding critics, students, and lay readers), we (i) extract 17 reference-less\ntextual features (e.g., coherence, emotional variance, average sentence\nlength...); (ii) model individual reader preferences, deriving feature\nimportance vectors that reflect their textual priorities; and (iii) analyze\nthese vectors in a shared \"preference space\". Reader vectors cluster into two\nprofiles: 'surface-focused readers' (mainly non-experts), who prioritize\nreadability and textual richness; and 'holistic readers' (mainly experts), who\nvalue thematic development, rhetorical variety, and sentiment dynamics. Our\nresults quantitatively explain how measurements of literary quality are a\nfunction of how text features align with each reader's preferences. These\nfindings advocate for reader-sensitive evaluation frameworks in the field of\ncreative text generation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u4e0e\u4eba\u7c7b\u521b\u4f5c\u7684\u6587\u5b66\u6587\u672c\u8d28\u91cf\u8bc4\u4ef7\u5dee\u5f02\u6e90\u4e8e\u8bfb\u8005\u504f\u597d\u4e0d\u540c\uff0c\u800c\u975e\u6587\u672c\u672c\u8eab\u3002\u901a\u8fc7\u5206\u6790\u8bfb\u8005\u504f\u597d\uff0c\u63ed\u793a\u4e86\u4e24\u7c7b\u8bfb\u8005\u7fa4\uff08\u8868\u9762\u5173\u6ce8\u578b\u548c\u6574\u4f53\u578b\uff09\uff0c\u5e76\u547c\u5401\u91c7\u7528\u8bfb\u8005\u654f\u611f\u7684\u8bc4\u4ef7\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3AI\u4e0e\u4eba\u7c7b\u521b\u4f5c\u6587\u5b66\u6587\u672c\u8bc4\u4ef7\u7ed3\u679c\u5dee\u5f02\u7684\u95ee\u9898\uff0c\u63a2\u7a76\u8bfb\u8005\u504f\u597d\u5982\u4f55\u5f71\u54cd\u6587\u5b66\u8d28\u91cf\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u4e94\u4e2a\u516c\u5171\u6570\u636e\u96c6\uff0c\u63d0\u53d617\u4e2a\u6587\u672c\u7279\u5f81\uff0c\u5efa\u6a21\u8bfb\u8005\u504f\u597d\uff0c\u5206\u6790\u8bfb\u8005\u504f\u597d\u5411\u91cf\u3002", "result": "\u8bfb\u8005\u5206\u4e3a\u4e24\u7c7b\uff1a\u8868\u9762\u5173\u6ce8\u578b\uff08\u975e\u4e13\u5bb6\uff09\u548c\u6574\u4f53\u578b\uff08\u4e13\u5bb6\uff09\uff0c\u6587\u5b66\u8d28\u91cf\u8bc4\u4ef7\u53d6\u51b3\u4e8e\u6587\u672c\u7279\u5f81\u4e0e\u8bfb\u8005\u504f\u597d\u7684\u5339\u914d\u7a0b\u5ea6\u3002", "conclusion": "\u6587\u5b66\u8d28\u91cf\u8bc4\u4ef7\u9700\u8003\u8651\u8bfb\u8005\u504f\u597d\uff0c\u5efa\u8bae\u91c7\u7528\u8bfb\u8005\u654f\u611f\u7684\u8bc4\u4ef7\u6846\u67b6\u3002", "keywords": "AI\u521b\u4f5c, \u4eba\u7c7b\u521b\u4f5c, \u6587\u5b66\u8d28\u91cf, \u8bfb\u8005\u504f\u597d, \u8bc4\u4ef7\u6846\u67b6"}}
{"id": "2506.03469", "pdf": "https://arxiv.org/pdf/2506.03469", "abs": "https://arxiv.org/abs/2506.03469", "authors": ["Tuan Le", "Risal Shefin", "Debashis Gupta", "Thai Le", "Sarra Alqahtani"], "title": "Verification-Guided Falsification for Safe RL via Explainable Abstraction and Risk-Aware Exploration", "categories": ["cs.AI", "cs.LG"], "comment": "8 pages, 7 figures, European Conference on Artificial Intelligence\n  (ECAI)", "summary": "Ensuring the safety of reinforcement learning (RL) policies in high-stakes\nenvironments requires not only formal verification but also interpretability\nand targeted falsification. While model checking provides formal guarantees,\nits effectiveness is limited by abstraction quality and the completeness of the\nunderlying trajectory dataset. We propose a hybrid framework that integrates\n(1) explainability, (2) model checking, and (3) risk-guided falsification to\nachieve both rigor and coverage. Our approach begins by constructing a\nhuman-interpretable abstraction of the RL policy using Comprehensible Abstract\nPolicy Summarization (CAPS). This abstract graph, derived from offline\ntrajectories, is both verifier-friendly, semantically meaningful, and can be\nused as input to Storm probabilistic model checker to verify satisfaction of\ntemporal safety specifications. If the model checker identifies a violation, it\nwill return an interpretable counterexample trace by which the policy fails the\nsafety requirement. However, if no violation is detected, we cannot conclude\nsatisfaction due to potential limitation in the abstraction and coverage of the\noffline dataset. In such cases, we estimate associated risk during model\nchecking to guide a falsification strategy that prioritizes searching in\nhigh-risk states and regions underrepresented in the trajectory dataset. We\nfurther provide PAC-style guarantees on the likelihood of uncovering undetected\nviolations. Finally, we incorporate a lightweight safety shield that switches\nto a fallback policy at runtime when such a risk exceeds a threshold,\nfacilitating failure mitigation without retraining.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89e3\u91ca\u6027\u3001\u6a21\u578b\u68c0\u67e5\u548c\u98ce\u9669\u5f15\u5bfc\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u5b89\u5168\u6027\uff0c\u5e76\u63d0\u4f9bPAC\u5f0f\u4fdd\u8bc1\u548c\u8f7b\u91cf\u7ea7\u5b89\u5168\u5c4f\u853d\u3002", "motivation": "\u5728\u5173\u952e\u73af\u5883\u4e2d\u786e\u4fdd\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u5b89\u5168\u6027\u9700\u8981\u5f62\u5f0f\u9a8c\u8bc1\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9488\u5bf9\u6027\u8bc1\u4f2a\u3002", "method": "\u5f00\u53d1\u4e86CAPS\uff08\u53ef\u7406\u89e3\u7684\u62bd\u8c61\u7b56\u7565\u603b\u7ed3\uff09\u6765\u521b\u5efa\u53ef\u89e3\u91ca\u7684\u7b56\u7565\u62bd\u8c61\u56fe\uff0c\u4f7f\u7528Storm\u6a21\u578b\u68c0\u67e5\u5668\u9a8c\u8bc1\u5b89\u5168\u6027\uff0c\u5e76\u901a\u8fc7\u98ce\u9669\u5f15\u5bfc\u7684\u8bc1\u4f2a\u7b56\u7565\u8865\u5145\u9a8c\u8bc1\u4e0d\u8db3\u7684\u90e8\u5206\u3002", "result": "\u6846\u67b6\u80fd\u591f\u68c0\u6d4b\u5e76\u5b9a\u4f4d\u5b89\u5168\u8fdd\u89c4\uff0c\u63d0\u4f9bPAC\u5f0f\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5b89\u5168\u5c4f\u853d\u5728\u8fd0\u884c\u65f6\u964d\u4f4e\u98ce\u9669\u3002", "conclusion": "\u8be5\u6df7\u5408\u6846\u67b6\u5728\u4e25\u683c\u6027\u548c\u8986\u76d6\u8303\u56f4\u4e0a\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u63d0\u5347\u4e86\u5bf9\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5b89\u5168\u6027\u7684\u9a8c\u8bc1\u80fd\u529b\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u5b89\u5168\u6027, \u6a21\u578b\u68c0\u67e5, \u53ef\u89e3\u91ca\u6027, \u98ce\u9669\u5f15\u5bfc\u8bc1\u4f2a, PAC\u4fdd\u8bc1"}}
{"id": "2506.03160", "pdf": "https://arxiv.org/pdf/2506.03160", "abs": "https://arxiv.org/abs/2506.03160", "authors": ["Shriyank Somvanshi", "Anannya Ghosh Tusti", "Mahmuda Sultana Mimi", "Md Monzurul Islam", "Sazzad Bin Bashar Polock", "Anandi Dutta", "Subasish Das"], "title": "Applying MambaAttention, TabPFN, and TabTransformers to Classify SAE Automation Levels in Crashes", "categories": ["cs.LG"], "comment": null, "summary": "The increasing presence of automated vehicles (AVs) presents new challenges\nfor crash classification and safety analysis. Accurately identifying the SAE\nautomation level involved in each crash is essential to understanding crash\ndynamics and system accountability. However, existing approaches often overlook\nautomation-specific factors and lack model sophistication to capture\ndistinctions between different SAE levels. To address this gap, this study\nevaluates the performance of three advanced tabular deep learning models\nMambaAttention, TabPFN, and TabTransformer for classifying SAE automation\nlevels using structured crash data from Texas (2024), covering 4,649 cases\ncategorized as Assisted Driving (SAE Level 1), Partial Automation (SAE Level\n2), and Advanced Automation (SAE Levels 3-5 combined). Following class\nbalancing using SMOTEENN, the models were trained and evaluated on a unified\ndataset of 7,300 records. MambaAttention demonstrated the highest overall\nperformance (F1-scores: 88% for SAE 1, 97% for SAE 2, and 99% for SAE 3-5),\nwhile TabPFN excelled in zero-shot inference with high robustness for rare\ncrash categories. In contrast, TabTransformer underperformed, particularly in\ndetecting Partial Automation crashes (F1-score: 55%), suggesting challenges in\nmodeling shared human-system control dynamics. These results highlight the\ncapability of deep learning models tailored for tabular data to enhance the\naccuracy and efficiency of automation-level classification. Integrating such\nmodels into crash analysis frameworks can support policy development, AV safety\nevaluation, and regulatory decisions, especially in distinguishing high-risk\nconditions for mid- and high-level automation technologies.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08MambaAttention\u3001TabPFN\u3001TabTransformer\uff09\u5728\u5206\u7c7bSAE\u81ea\u52a8\u9a7e\u9a76\u7ea7\u522b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0MambaAttention\u603b\u4f53\u8868\u73b0\u6700\u4f73\uff0cTabPFN\u5728\u96f6\u6837\u672c\u63a8\u65ad\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u800cTabTransformer\u8868\u73b0\u8f83\u5dee\uff0c\u7279\u522b\u662f\u5728\u90e8\u5206\u81ea\u52a8\u5316\u7ea7\u522b\u4e2d\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u589e\u52a0\uff0c\u51c6\u786e\u5206\u7c7bSAE\u81ea\u52a8\u5316\u7ea7\u522b\u5bf9\u7406\u89e3\u4e8b\u6545\u52a8\u6001\u548c\u7cfb\u7edf\u8d23\u4efb\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u81ea\u52a8\u5316\u7279\u5b9a\u56e0\u7d20\u4e14\u7f3a\u4e4f\u6a21\u578b\u590d\u6742\u6027\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u7ed3\u6784\u5316\u4e8b\u6545\u6570\u636e\uff084649\u8d77\u6848\u4f8b\uff09\uff0c\u901a\u8fc7SMOTEENN\u8fdb\u884c\u7c7b\u522b\u5e73\u8861\uff0c\u8bad\u7ec3\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08MambaAttention\u3001TabPFN\u3001TabTransformer\uff09\u3002", "result": "MambaAttention\u8868\u73b0\u6700\u4f73\uff08F1\u5206\u6570\uff1aSAE 1\u4e3a88%\uff0cSAE 2\u4e3a97%\uff0cSAE 3-5\u4e3a99%\uff09\uff1bTabPFN\u5728\u96f6\u6837\u672c\u63a8\u65ad\u4e2d\u8868\u73b0\u4f18\u5f02\uff1bTabTransformer\u8868\u73b0\u8f83\u5dee\uff08\u90e8\u5206\u81ea\u52a8\u5316F1\u5206\u6570\uff1a55%\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u9488\u5bf9\u8868\u683c\u6570\u636e\u4f18\u5316\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u81ea\u52a8\u5316\u7ea7\u522b\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u6709\u52a9\u4e8e\u653f\u7b56\u5236\u5b9a\u548cAV\u5b89\u5168\u8bc4\u4f30\u3002", "keywords": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u3001SAE\u7ea7\u522b\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u8868\u683c\u6570\u636e\u3001\u4e8b\u6545\u5206\u7c7b"}}
{"id": "2506.03312", "pdf": "https://arxiv.org/pdf/2506.03312", "abs": "https://arxiv.org/abs/2506.03312", "authors": ["Celia Chen", "Scotty Beland", "Ingo Burghardt", "Jill Byczek", "William J. Conway", "Eric Cotugno", "Sadaf Davre", "Megan Fletcher", "Rajesh Kumar Gnanasekaran", "Kristin Hamilton", "Marilyn Harbert", "Jordan Heustis", "Tanaya Jha", "Emily Klein", "Hayden Kramer", "Alex Leitch", "Jessica Perkins", "Casi Sherman", "Celia Sterrn", "Logan Stevens", "Rebecca Zarrella", "Jennifer Golbeck"], "title": "Cross-Platform Violence Detection on Social Media: A Dataset and Analysis", "categories": ["cs.CL", "cs.LG"], "comment": "In Proceedings of the 17th ACM Web Science Conference (WebSci '25). 9\n  pages", "summary": "Violent threats remain a significant problem across social media platforms.\nUseful, high-quality data facilitates research into the understanding and\ndetection of malicious content, including violence. In this paper, we introduce\na cross-platform dataset of 30,000 posts hand-coded for violent threats and\nsub-types of violence, including political and sexual violence. To evaluate the\nsignal present in this dataset, we perform a machine learning analysis with an\nexisting dataset of violent comments from YouTube. We find that, despite\noriginating from different platforms and using different coding criteria, we\nachieve high classification accuracy both by training on one dataset and\ntesting on the other, and in a merged dataset condition. These results have\nimplications for content-classification strategies and for understanding\nviolent content across social media.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u8de8\u5e73\u53f0\u7684\u66b4\u529b\u5a01\u80c1\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5206\u6790\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u7ed3\u679c\u8868\u660e\u4e0d\u540c\u5e73\u53f0\u7684\u6570\u636e\u517c\u5bb9\u6027\u9ad8\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u66b4\u529b\u5a01\u80c1\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u9ad8\u8d28\u91cf\u6570\u636e\u6709\u52a9\u4e8e\u7814\u7a76\u548c\u68c0\u6d4b\u6076\u610f\u5185\u5bb9\u3002", "method": "\u5f15\u51653\u4e07\u6761\u624b\u5de5\u7f16\u7801\u7684\u66b4\u529b\u5a01\u80c1\u5e16\u5b50\uff0c\u4e0eYouTube\u66b4\u529b\u8bc4\u8bba\u6570\u636e\u96c6\u8fdb\u884c\u673a\u5668\u5b66\u4e60\u5206\u6790\u3002", "result": "\u4e0d\u540c\u5e73\u53f0\u548c\u7f16\u7801\u6807\u51c6\u4e0b\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u9ad8\uff0c\u6570\u636e\u517c\u5bb9\u6027\u5f3a\u3002", "conclusion": "\u7ed3\u679c\u5bf9\u5185\u5bb9\u5206\u7c7b\u7b56\u7565\u548c\u8de8\u5e73\u53f0\u66b4\u529b\u5185\u5bb9\u7406\u89e3\u6709\u91cd\u8981\u610f\u4e49\u3002", "keywords": "\u66b4\u529b\u5a01\u80c1,\u793e\u4ea4\u5a92\u4f53,\u673a\u5668\u5b66\u4e60,\u8de8\u5e73\u53f0\u6570\u636e\u96c6"}}
{"id": "2506.03503", "pdf": "https://arxiv.org/pdf/2506.03503", "abs": "https://arxiv.org/abs/2506.03503", "authors": ["Shan Shan"], "title": "Computational Architects of Society: Quantum Machine Learning for Social Rule Genesis", "categories": ["cs.AI"], "comment": null, "summary": "The quantification of social science remains a longstanding challenge,\nlargely due to the philosophical nature of its foundational theories. Although\nquantum computing has advanced rapidly in recent years, its relevance to social\ntheory remains underexplored. Most existing research focuses on micro-cognitive\nmodels or philosophical analogies, leaving a gap in system-level applications\nof quantum principles to the analysis of social systems. This study addresses\nthat gap by proposing a theoretical and computational framework that combines\nquantum mechanics with Generative AI to simulate the emergence and evolution of\nsocial norms. Drawing on core quantum concepts--such as superposition,\nentanglement, and probabilistic measurement--this research models society as a\ndynamic, uncertain system and sets up five ideal-type experiments. These\nscenarios are simulated using 25 generative agents, each assigned evolving\nroles as compliers, resistors, or enforcers. Within a simulated environment\nmonitored by a central observer (the Watcher), agents interact, respond to\nsurveillance, and adapt to periodic normative disruptions. These interactions\nallow the system to self-organize under external stress and reveal emergent\npatterns. Key findings show that quantum principles, when integrated with\ngenerative AI, enable the modeling of uncertainty, emergence, and\ninterdependence in complex social systems. Simulations reveal patterns\nincluding convergence toward normative order, the spread of resistance, and the\nspontaneous emergence of new equilibria in social rules. In conclusion, this\nstudy introduces a novel computational lens that lays the groundwork for a\nquantum-informed social theory. It offers interdisciplinary insights into how\nsociety can be understood not just as a structure to observe but as a dynamic\nsystem to simulate and redesign through quantum technologies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u91cf\u5b50\u529b\u5b66\u4e0e\u751f\u6210\u5f0fAI\u7684\u7406\u8bba\u548c\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u793e\u4f1a\u89c4\u8303\u7684\u6d8c\u73b0\u548c\u6f14\u5316\uff0c\u586b\u8865\u4e86\u91cf\u5b50\u539f\u7406\u5728\u7cfb\u7edf\u7ea7\u793e\u4f1a\u5206\u6790\u4e2d\u7684\u7a7a\u767d\u3002", "motivation": "\u793e\u4f1a\u79d1\u5b66\u91cf\u5316\u5b58\u5728\u6311\u6218\uff0c\u91cf\u5b50\u8ba1\u7b97\u4e0e\u793e\u4f1a\u7406\u8bba\u7684\u8054\u7cfb\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u5728\u5fae\u89c2\u8ba4\u77e5\u6a21\u578b\u6216\u54f2\u5b66\u7c7b\u6bd4\u4e0a\u3002", "method": "\u5229\u7528\u91cf\u5b50\u53e0\u52a0\u3001\u7ea0\u7f20\u548c\u6982\u7387\u6d4b\u91cf\u7b49\u6982\u5ff5\uff0c\u901a\u8fc725\u4e2a\u751f\u6210\u5f0f\u4ee3\u7406\u6a21\u62df\u793e\u4f1a\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u7cfb\u7edf\uff0c\u5e76\u8bbe\u8ba1\u4e94\u7c7b\u7406\u60f3\u5b9e\u9a8c\u3002", "result": "\u6a21\u62df\u63ed\u793a\u4e86\u793e\u4f1a\u89c4\u8303\u7684\u8d8b\u540c\u3001\u62b5\u6297\u7684\u6269\u6563\u4ee5\u53ca\u793e\u4f1a\u89c4\u5219\u4e2d\u65b0\u5e73\u8861\u7684\u81ea\u53d1\u6d8c\u73b0\u7b49\u73b0\u8c61\u3002", "conclusion": "\u7814\u7a76\u4e3a\u91cf\u5b50\u542f\u53d1\u793e\u4f1a\u7406\u8bba\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u8de8\u5b66\u79d1\u89c1\u89e3\uff0c\u5c06\u793e\u4f1a\u89c6\u4e3a\u53ef\u901a\u8fc7\u91cf\u5b50\u6280\u672f\u6a21\u62df\u548c\u91cd\u65b0\u8bbe\u8ba1\u7684\u52a8\u6001\u7cfb\u7edf\u3002", "keywords": "\u91cf\u5b50\u8ba1\u7b97, \u751f\u6210\u5f0fAI, \u793e\u4f1a\u89c4\u8303, \u4e0d\u786e\u5b9a\u6027\u5efa\u6a21, \u8de8\u5b66\u79d1\u7814\u7a76"}}
{"id": "2506.03161", "pdf": "https://arxiv.org/pdf/2506.03161", "abs": "https://arxiv.org/abs/2506.03161", "authors": ["Mira Nuthakki"], "title": "Safety-Prioritized, Reinforcement Learning-Enabled Traffic Flow Optimization in a 3D City-Wide Simulation Environment", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "18 pages, figures at end, methods at end. Format/order can be changed\n  if necessary", "summary": "Traffic congestion and collisions represent significant economic,\nenvironmental, and social challenges worldwide. Traditional traffic management\napproaches have shown limited success in addressing these complex, dynamic\nproblems. To address the current research gaps, three potential tools are\ndeveloped: a comprehensive 3D city-wide simulation environment that integrates\nboth macroscopic and microscopic traffic dynamics; a collision model; and a\nreinforcement learning framework with custom reward functions prioritizing\nsafety over efficiency. Unity game engine-based simulation is used for direct\ncollision modeling. A custom reward enabled reinforcement learning method,\nproximal policy optimization (PPO) model, yields substantial improvements over\nbaseline results, reducing the number of serious collisions, number of\nvehicle-vehicle collisions, and total distance travelled by over 3 times the\nbaseline values. The model also improves fuel efficiency by 39% and reduces\ncarbon emissions by 88%. Results establish feasibility for city-wide 3D traffic\nsimulation applications incorporating the vision-zero safety principles of the\nDepartment of Transportation, including physics-informed, adaptable, realistic\ncollision modeling, as well as appropriate reward modeling for real-world\ntraffic signal light control towards reducing collisions, optimizing traffic\nflow and reducing greenhouse emissions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u57ce\u5e02\u573a\u666f\u6a21\u62df\u3001\u78b0\u649e\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u4ea4\u901a\u7ba1\u7406\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4ea4\u901a\u4e8b\u6545\u548c\u6392\u653e\u3002", "motivation": "\u4f20\u7edf\u4ea4\u901a\u7ba1\u7406\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u590d\u6742\u7684\u4ea4\u901a\u62e5\u5835\u548c\u78b0\u649e\u95ee\u9898\uff0c\u4e9f\u9700\u521b\u65b0\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e863D\u57ce\u5e02\u6a21\u62df\u73af\u5883\u3001\u78b0\u649e\u6a21\u578b\u53ca\u57fa\u4e8ePPO\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f18\u5148\u8003\u8651\u5b89\u5168\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u4e25\u91cd\u78b0\u649e\u548c\u8f66\u8f86\u78b0\u649e\u6b21\u6570\u53ca\u603b\u884c\u9a76\u8ddd\u79bb\u51cf\u5c113\u500d\u4ee5\u4e0a\uff0c\u71c3\u6cb9\u6548\u7387\u63d0\u534739%\uff0c\u78b3\u6392\u653e\u51cf\u5c1188%\u3002", "conclusion": "\u8bc1\u660e\u4e863D\u6a21\u62df\u548c\u5f3a\u5316\u5b66\u4e60\u5728\u4f18\u5316\u4ea4\u901a\u6d41\u3001\u51cf\u5c11\u78b0\u649e\u548c\u6392\u653e\u65b9\u9762\u7684\u53ef\u884c\u6027\u3002", "keywords": "\u4ea4\u901a\u7ba1\u7406, 3D\u6a21\u62df, \u5f3a\u5316\u5b66\u4e60, \u78b0\u649e\u6a21\u578b, \u78b3\u6392\u653e"}}
{"id": "2506.03357", "pdf": "https://arxiv.org/pdf/2506.03357", "abs": "https://arxiv.org/abs/2506.03357", "authors": ["Aldan Creo", "H\u00e9ctor Cerezo-Costas", "Pedro Alonso-Doval", "Maximiliano Hormaz\u00e1bal-Lagos"], "title": "Ask a Local: Detecting Hallucinations With Specialized Model Divergence", "categories": ["cs.CL", "cs.AI"], "comment": "Supplementary materials: https://github.com/ACMCMC/ask-a-local", "summary": "Hallucinations in large language models (LLMs) - instances where models\ngenerate plausible but factually incorrect information - present a significant\nchallenge for AI.\n  We introduce \"Ask a Local\", a novel hallucination detection method exploiting\nthe intuition that specialized models exhibit greater surprise when\nencountering domain-specific inaccuracies. Our approach computes divergence\nbetween perplexity distributions of language-specialized models to identify\npotentially hallucinated spans. Our method is particularly well-suited for a\nmultilingual context, as it naturally scales to multiple languages without the\nneed for adaptation, relying on external data sources, or performing training.\nMoreover, we select computationally efficient models, providing a scalable\nsolution that can be applied to a wide range of languages and domains.\n  Our results on a human-annotated question-answer dataset spanning 14\nlanguages demonstrate consistent performance across languages, with\nIntersection-over-Union (IoU) scores around 0.3 and comparable Spearman\ncorrelation values. Our model shows particularly strong performance on Italian\nand Catalan, with IoU scores of 0.42 and 0.38, respectively, while maintaining\ncross-lingual effectiveness without language-specific adaptations. We release\nour code and architecture to facilitate further research in multilingual\nhallucination detection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'Ask a Local'\u7684\u65b0\u578b\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u4e13\u4e1a\u5316\u6a21\u578b\u7684\u56f0\u60d1\u5ea6\u5206\u5e03\u5dee\u5f02\u6765\u8bc6\u522b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u865a\u5047\u4fe1\u606f\uff0c\u9002\u7528\u4e8e\u591a\u8bed\u8a00\u73af\u5883\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5916\u90e8\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u865a\u5047\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u6311\u6218\u3002", "method": "\u8ba1\u7b97\u8bed\u8a00\u4e13\u4e1a\u5316\u6a21\u578b\u7684\u56f0\u60d1\u5ea6\u5206\u5e03\u5dee\u5f02\uff0c\u8bc6\u522b\u6f5c\u5728\u865a\u6784\u5185\u5bb9\u3002\u65b9\u6cd5\u65e0\u9700\u9002\u914d\u6216\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8bed\u8a00\u3002", "result": "\u572814\u79cd\u8bed\u8a00\u7684\u4eba\u7c7b\u6807\u6ce8\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\uff0c\u8868\u73b0\u7a33\u5b9a\uff0cIoU\u5206\u6570\u7ea6\u4e3a0.3\uff0c\u610f\u5927\u5229\u8bed\u548c\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8bed\u8868\u73b0\u5c24\u4f73\uff08IoU\u5206\u522b\u4e3a0.42\u548c0.38\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u591a\u8bed\u8a00\u5e7b\u89c9\u68c0\u6d4b\u7684\u6709\u6548\u5de5\u5177\uff0c\u4ee3\u7801\u548c\u67b6\u6784\u5df2\u516c\u5f00\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u5e7b\u89c9\u68c0\u6d4b\u3001\u56f0\u60d1\u5ea6\u3001\u591a\u8bed\u8a00\u3001Ask a Local"}}
{"id": "2506.03543", "pdf": "https://arxiv.org/pdf/2506.03543", "abs": "https://arxiv.org/abs/2506.03543", "authors": ["Wanghao Ye", "Sihan Chen", "Yiting Wang", "Shwai He", "Bowei Tian", "Guoheng Sun", "Ziyi Wang", "Ziyao Wang", "Yexiao He", "Zheyu Shen", "Meng Liu", "Yuning Zhang", "Meng Feng", "Yang Wang", "Siyuan Peng", "Yilong Dai", "Zhenle Duan", "Hanzhang Qin", "Ang Li"], "title": "CogniPair: From LLM Chatbots to Conscious AI Agents -- GNWT-Based Multi-Agent Digital Twins for Social Pairing -- Dating & Hiring Applications", "categories": ["cs.AI", "cs.CY", "cs.MA"], "comment": null, "summary": "Current large language model (LLM) agents lack authentic human psychological\nprocesses necessary for genuine digital twins and social AI applications. To\naddress this limitation, we present a computational implementation of Global\nWorkspace Theory (GNWT) that integrates human cognitive architecture principles\ninto LLM agents, creating specialized sub-agents for emotion, memory, social\nnorms, planning, and goal-tracking coordinated through a global workspace\nmechanism. However, authentic digital twins require accurate personality\ninitialization. We therefore develop a novel adventure-based personality test\nthat evaluates true personality through behavioral choices within interactive\nscenarios, bypassing self-presentation bias found in traditional assessments.\nBuilding on these innovations, our CogniPair platform enables digital twins to\nengage in realistic simulated dating interactions and job interviews before\nreal encounters, providing bidirectional cultural fit assessment for both\nromantic compatibility and workplace matching. Validation using 551 GNWT-Agents\nand Columbia University Speed Dating dataset demonstrates 72% correlation with\nhuman attraction patterns, 77.8% match prediction accuracy, and 74% agreement\nin human validation studies. This work advances psychological authenticity in\nLLM agents and establishes a foundation for intelligent dating platforms and HR\ntechnology solutions.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u6574\u5408\u5168\u7403\u5de5\u4f5c\u7a7a\u95f4\u7406\u8bba\uff08GNWT\uff09\u5230LLM\u667a\u80fd\u4f53\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u5177\u5fc3\u7406\u771f\u5b9e\u6027\u7684\u6570\u5b57\u5b6a\u751f\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u5192\u9669\u5f0f\u4eba\u683c\u6d4b\u8bd5\u4ee5\u514b\u670d\u4f20\u7edf\u8bc4\u4f30\u7684\u504f\u5dee\u3002CogniPair\u5e73\u53f0\u9a8c\u8bc1\u4e86\u5176\u5728\u7ea6\u4f1a\u548c\u804c\u573a\u5339\u914d\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u7f3a\u4e4f\u4eba\u7c7b\u5fc3\u7406\u8fc7\u7a0b\u7684\u771f\u5b9e\u6027\uff0c\u9650\u5236\u4e86\u6570\u5b57\u5b6a\u751f\u548c\u793e\u4ea4AI\u5e94\u7528\u7684\u6548\u679c\u3002", "method": "\u901a\u8fc7GNWT\u6574\u5408\u4eba\u7c7b\u8ba4\u77e5\u67b6\u6784\uff0c\u521b\u5efa\u5b50\u667a\u80fd\u4f53\uff08\u5982\u60c5\u611f\u3001\u8bb0\u5fc6\u7b49\uff09\uff0c\u5e76\u5f00\u53d1\u5192\u9669\u5f0f\u4eba\u683c\u6d4b\u8bd5\u521d\u59cb\u5316\u4e2a\u6027\u3002", "result": "\u9a8c\u8bc1\u663e\u793aGNWT-Agents\u4e0e\u4eba\u7c7b\u5438\u5f15\u529b\u6a21\u5f0f\u76f8\u5173\u6027\u8fbe72%\uff0c\u5339\u914d\u9884\u6d4b\u51c6\u786e\u738777.8%\uff0c\u4eba\u7c7b\u9a8c\u8bc1\u540c\u610f\u738774%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u5347\u4e86LLM\u667a\u80fd\u4f53\u7684\u5fc3\u7406\u771f\u5b9e\u6027\uff0c\u4e3a\u667a\u80fd\u7ea6\u4f1a\u5e73\u53f0\u548cHR\u6280\u672f\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "keywords": "LLM\u667a\u80fd\u4f53, \u5168\u7403\u5de5\u4f5c\u7a7a\u95f4\u7406\u8bba, \u6570\u5b57\u5b6a\u751f, \u4eba\u683c\u6d4b\u8bd5, \u793e\u4ea4AI"}}
{"id": "2506.03163", "pdf": "https://arxiv.org/pdf/2506.03163", "abs": "https://arxiv.org/abs/2506.03163", "authors": ["Oluwaseyi Giwa"], "title": "Causal Discovery in Dynamic Fading Wireless Networks", "categories": ["cs.LG", "eess.SP", "stat.ME"], "comment": "5 pages, 3 figures", "summary": "Dynamic causal discovery in wireless networks is essential due to evolving\ninterference, fading, and mobility, which complicate traditional static causal\nmodels. This paper addresses causal inference challenges in dynamic fading\nwireless environments by proposing a sequential regression-based algorithm with\na novel application of the NOTEARS acyclicity constraint, enabling efficient\nonline updates. We derive theoretical lower and upper bounds on the detection\ndelay required to identify structural changes, explicitly quantifying their\ndependence on network size, noise variance, and fading severity. Monte Carlo\nsimulations validate these theoretical results, demonstrating linear increases\nin detection delay with network size, quadratic growth with noise variance, and\ninverse-square dependence on the magnitude of structural changes. Our findings\nprovide rigorous theoretical insights and practical guidelines for designing\nrobust online causal inference mechanisms to maintain network reliability under\nnonstationary wireless conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u987a\u5e8f\u56de\u5f52\u7684\u7b97\u6cd5\uff0c\u7ed3\u5408NOTEARS\u65e0\u73af\u6027\u7ea6\u675f\uff0c\u7528\u4e8e\u52a8\u6001\u8870\u843d\u65e0\u7ebf\u73af\u5883\u4e0b\u7684\u56e0\u679c\u63a8\u65ad\uff0c\u5e76\u63a8\u5bfc\u4e86\u68c0\u6d4b\u5ef6\u8fdf\u7684\u7406\u8bba\u4e0a\u4e0b\u754c\u3002", "motivation": "\u52a8\u6001\u53d8\u5316\u7684\u65e0\u7ebf\u7f51\u7edc\u73af\u5883\uff08\u5982\u5e72\u6270\u3001\u8870\u843d\u548c\u79fb\u52a8\u6027\uff09\u4f7f\u5f97\u4f20\u7edf\u9759\u6001\u56e0\u679c\u6a21\u578b\u96be\u4ee5\u9002\u7528\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u52a8\u6001\u8870\u843d\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u56e0\u679c\u63a8\u65ad\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u987a\u5e8f\u56de\u5f52\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408NOTEARS\u65e0\u73af\u6027\u7ea6\u675f\uff0c\u652f\u6301\u9ad8\u6548\u5728\u7ebf\u66f4\u65b0\uff0c\u4ee5\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u7684\u56e0\u679c\u63a8\u65ad\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u8499\u7279\u5361\u6d1b\u4eff\u771f\u9a8c\u8bc1\uff0c\u68c0\u6d4b\u5ef6\u8fdf\u4e0e\u7f51\u7edc\u5927\u5c0f\u5448\u7ebf\u6027\u589e\u957f\uff0c\u4e0e\u566a\u58f0\u65b9\u5dee\u5448\u4e8c\u6b21\u589e\u957f\uff0c\u4e0e\u7ed3\u6784\u53d8\u5316\u5e45\u5ea6\u5448\u5e73\u65b9\u53cd\u6bd4\u5173\u7cfb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8bbe\u8ba1\u9c81\u68d2\u7684\u5728\u7ebf\u56e0\u679c\u63a8\u65ad\u673a\u5236\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u548c\u5b9e\u7528\u6307\u5bfc\uff0c\u4ee5\u5e94\u5bf9\u975e\u5e73\u7a33\u65e0\u7ebf\u6761\u4ef6\u4e0b\u7684\u7f51\u7edc\u53ef\u9760\u6027\u95ee\u9898\u3002", "keywords": "\u52a8\u6001\u56e0\u679c\u63a8\u65ad,\u65e0\u7ebf\u7f51\u7edc,NOTEARS,\u5728\u7ebf\u66f4\u65b0,\u68c0\u6d4b\u5ef6\u8fdf"}}
{"id": "2506.03360", "pdf": "https://arxiv.org/pdf/2506.03360", "abs": "https://arxiv.org/abs/2506.03360", "authors": ["Zihui Ma", "Lingyao Li", "Juan Li", "Wenyue Hua", "Jingxiao Liu", "Qingyuan Feng", "Yuki Miura"], "title": "A Multimodal, Multilingual, and Multidimensional Pipeline for Fine-grained Crowdsourcing Earthquake Damage Evaluation", "categories": ["cs.CL", "cs.CY", "cs.SI"], "comment": null, "summary": "Rapid, fine-grained disaster damage assessment is essential for effective\nemergency response, yet remains challenging due to limited ground sensors and\ndelays in official reporting. Social media provides a rich, real-time source of\nhuman-centric observations, but its multimodal and unstructured nature presents\nchallenges for traditional analytical methods. In this study, we propose a\nstructured Multimodal, Multilingual, and Multidimensional (3M) pipeline that\nleverages multimodal large language models (MLLMs) to assess disaster impacts.\nWe evaluate three foundation models across two major earthquake events using\nboth macro- and micro-level analyses. Results show that MLLMs effectively\nintegrate image-text signals and demonstrate a strong correlation with\nground-truth seismic data. However, performance varies with language,\nepicentral distance, and input modality. This work highlights the potential of\nMLLMs for disaster assessment and provides a foundation for future research in\napplying MLLMs to real-time crisis contexts. The code and data are released at:\nhttps://github.com/missa7481/EMNLP25_earthquake", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u76843M\u6d41\u7a0b\uff0c\u7528\u4e8e\u707e\u5bb3\u5f71\u54cd\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660eMLLMs\u80fd\u6709\u6548\u6574\u5408\u56fe\u50cf\u548c\u6587\u672c\u4fe1\u53f7\uff0c\u4f46\u4e0e\u8bed\u8a00\u3001\u9707\u4e2d\u8ddd\u79bb\u548c\u8f93\u5165\u6a21\u6001\u76f8\u5173\u3002", "motivation": "\u5feb\u901f\u4e14\u7ec6\u7c92\u5ea6\u7684\u707e\u5bb3\u635f\u574f\u8bc4\u4f30\u5bf9\u5e94\u6025\u54cd\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5730\u9762\u4f20\u611f\u5668\u6709\u9650\u548c\u5b98\u65b9\u62a5\u544a\u5ef6\u8fdf\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002\u793e\u4ea4\u5a92\u4f53\u7684\u591a\u6a21\u6001\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\u63d0\u4f9b\u4e86\u5b9e\u65f6\u7684\u4eba\u4e3a\u4e2d\u5fc3\u89c2\u6d4b\uff0c\u4f46\u5206\u6790\u8fd9\u4e9b\u6570\u636e\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u591a\u6a21\u6001\u3001\u591a\u8bed\u8a00\u548c\u591a\u7ef4\uff083M\uff09\u6d41\u7a0b\uff0c\u5229\u7528MLLMs\u8bc4\u4f30\u707e\u5bb3\u5f71\u54cd\uff0c\u5e76\u5728\u4e24\u6b21\u5927\u5730\u9707\u4e8b\u4ef6\u4e2d\u8fdb\u884c\u4e86\u5b8f\u89c2\u548c\u5fae\u89c2\u5206\u6790\u3002", "result": "MLLMs\u80fd\u591f\u6709\u6548\u6574\u5408\u56fe\u50cf\u548c\u6587\u672c\u4fe1\u53f7\uff0c\u5e76\u4e0e\u771f\u5b9e\u5730\u9707\u6570\u636e\u9ad8\u5ea6\u76f8\u5173\uff0c\u4f46\u6027\u80fd\u53d7\u8bed\u8a00\u3001\u9707\u4e2d\u8ddd\u79bb\u548c\u8f93\u5165\u6a21\u6001\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86MLLMs\u5728\u707e\u5bb3\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u5728\u5b9e\u65f6\u5371\u673a\u4e2d\u5e94\u7528MLLMs\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "keywords": "\u707e\u5bb3\u8bc4\u4f30, \u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, \u793e\u4ea4\u5a92\u4f53, \u5b9e\u65f6\u6570\u636e\u5206\u6790, \u5730\u9707"}}
{"id": "2506.03548", "pdf": "https://arxiv.org/pdf/2506.03548", "abs": "https://arxiv.org/abs/2506.03548", "authors": ["Chenglong Ye", "Gang Xiong", "Junyou Shang", "Xingyuan Dai", "Xiaoyan Gong", "Yisheng Lv"], "title": "SUMO-MCP: Leveraging the Model Context Protocol for Autonomous Traffic Simulation and Optimization", "categories": ["cs.AI"], "comment": null, "summary": "Traffic simulation tools, such as SUMO, are essential for urban mobility\nresearch. However, such tools remain challenging for users due to complex\nmanual workflows involving network download, demand generation, simulation\nsetup, and result analysis. In this paper, we introduce SUMO-MCP, a novel\nplatform that not only wraps SUMO' s core utilities into a unified tool suite\nbut also provides additional auxiliary utilities for common preprocessing and\npostprocessing tasks. Using SUMO-MCP, users can issue simple natural-language\nprompts to generate traffic scenarios from OpenStreetMap data, create demand\nfrom origin-destination matrices or random patterns, run batch simulations with\nmultiple signal-control strategies, perform comparative analyses with automated\nreporting, and detect congestion for signal-timing optimization. Furthermore,\nthe platform allows flexible custom workflows by dynamically combining exposed\nSUMO tools without additional coding. Experiments demonstrate that SUMO-MCP\nsignificantly makes traffic simulation more accessible and reliable for\nresearchers. We will release code for SUMO-MCP at\nhttps://github.com/ycycycl/SUMO-MCP in the future.", "AI": {"tldr": "SUMO-MCP\u5e73\u53f0\u7b80\u5316\u4e86\u4ea4\u901a\u6a21\u62df\u5de5\u5177\u7684\u590d\u6742\u6027\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u548c\u7edf\u4e00\u5de5\u5177\u5957\u4ef6\u63d0\u5347\u6613\u7528\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u4ea4\u901a\u6a21\u62df\u5de5\u5177\uff08\u5982SUMO\uff09\u64cd\u4f5c\u590d\u6742\uff0c\u624b\u52a8\u5de5\u4f5c\u6d41\u7a0b\u7e41\u7410\uff0c\u9650\u5236\u4e86\u5e7f\u6cdb\u4f7f\u7528\u3002", "method": "\u5f00\u53d1\u4e86SUMO-MCP\u5e73\u53f0\uff0c\u6574\u5408SUMO\u6838\u5fc3\u529f\u80fd\uff0c\u63d0\u4f9b\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u7684\u8f85\u52a9\u5de5\u5177\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u548c\u81ea\u5b9a\u4e49\u5de5\u4f5c\u6d41\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eSUMO-MCP\u663e\u8457\u63d0\u9ad8\u4e86\u4ea4\u901a\u6a21\u62df\u7684\u6613\u7528\u6027\u548c\u53ef\u9760\u6027\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u4efb\u52a1\u81ea\u52a8\u5316\u3002", "conclusion": "SUMO-MCP\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u7b80\u5355\u53ef\u9760\u7684\u4ea4\u901a\u6a21\u62df\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u4ea4\u901a\u6a21\u62df,SUMO,\u81ea\u7136\u8bed\u8a00\u5904\u7406,\u81ea\u52a8\u5316,\u5f00\u6e90\u5de5\u5177"}}
{"id": "2506.03164", "pdf": "https://arxiv.org/pdf/2506.03164", "abs": "https://arxiv.org/abs/2506.03164", "authors": ["Vignav Ramesh", "Morteza Mardani"], "title": "Test-Time Scaling of Diffusion Models via Noise Trajectory Search", "categories": ["cs.LG"], "comment": null, "summary": "The iterative and stochastic nature of diffusion models enables test-time\nscaling, whereby spending additional compute during denoising generates\nhigher-fidelity samples. Increasing the number of denoising steps is the\nprimary scaling axis, but this yields quickly diminishing returns. Instead\noptimizing the noise trajectory--the sequence of injected noise vectors--is\npromising, as the specific noise realizations critically affect sample quality;\nbut this is challenging due to a high-dimensional search space, complex\nnoise-outcome interactions, and costly trajectory evaluations. We address this\nby first casting diffusion as a Markov Decision Process (MDP) with a terminal\nreward, showing tree-search methods such as Monte Carlo tree search (MCTS) to\nbe meaningful but impractical. To balance performance and efficiency, we then\nresort to a relaxation of MDP, where we view denoising as a sequence of\nindependent contextual bandits. This allows us to introduce an\n$\\epsilon$-greedy search algorithm that globally explores at extreme timesteps\nand locally exploits during the intermediate steps where de-mixing occurs.\nExperiments on EDM and Stable Diffusion reveal state-of-the-art scores for\nclass-conditioned/text-to-image generation, exceeding baselines by up to\n$164\\%$ and matching/exceeding MCTS performance. To our knowledge, this is the\nfirst practical method for test-time noise trajectory optimization of arbitrary\n(non-differentiable) rewards.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u6269\u6563\u6a21\u578b\u4e2d\u566a\u58f0\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5176\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u5e76\u5f15\u5165\u4e00\u79cd\u8d2a\u5a6a\u641c\u7d22\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6837\u672c\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u4fdd\u771f\u6837\u672c\u65f6\uff0c\u901a\u5e38\u901a\u8fc7\u589e\u52a0\u53bb\u566a\u6b65\u9aa4\u6765\u63d0\u5347\u8d28\u91cf\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u6536\u76ca\u9012\u51cf\u3002\u4f18\u5316\u566a\u58f0\u8f68\u8ff9\u867d\u5177\u6f5c\u529b\uff0c\u4f46\u56e0\u9ad8\u7ef4\u641c\u7d22\u7a7a\u95f4\u548c\u590d\u6742\u4ea4\u4e92\u800c\u56f0\u96be\u3002", "method": "\u5c06\u6269\u6563\u8fc7\u7a0b\u5efa\u6a21\u4e3aMDP\uff0c\u91c7\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4f5c\u4e3a\u7406\u8bba\u6846\u67b6\uff0c\u4f46\u56e0\u5176\u4f4e\u6548\u8f6c\u800c\u63d0\u51fa\u4e00\u79cd\u8d2a\u5a6a\u641c\u7d22\u7b97\u6cd5\uff0c\u7ed3\u5408\u5168\u5c40\u63a2\u7d22\u548c\u5c40\u90e8\u5229\u7528\u3002", "result": "\u5728EDM\u548cStable Diffusion\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u7c7b\u522b\u6761\u4ef6/\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u4f18\u6548\u679c\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe164%\uff0c\u4e14\u4e0eMCTS\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u4e86\u5bf9\u4efb\u610f\uff08\u975e\u53ef\u5fae\uff09\u5956\u52b1\u7684\u566a\u58f0\u8f68\u8ff9\u4f18\u5316\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u6548\u7387\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u6269\u6563\u6a21\u578b,\u566a\u58f0\u8f68\u8ff9\u4f18\u5316,MDP,\u8d2a\u5a6a\u641c\u7d22,\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210"}}
{"id": "2506.03408", "pdf": "https://arxiv.org/pdf/2506.03408", "abs": "https://arxiv.org/abs/2506.03408", "authors": ["Yi Xu", "Ruining Yang", "Yitian Zhang", "Yizhou Wang", "Jianglin Lu", "Mingyuan Zhang", "Lili Su", "Yun Fu"], "title": "Trajectory Prediction Meets Large Language Models: A Survey", "categories": ["cs.CL", "cs.CV"], "comment": "16 pages, GitHub:\n  https://github.com/colorfulfuture/Awesome-Trajectory-Motion-Prediction-Papers", "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin integrating language-driven techniques into trajectory prediction. By\nleveraging their semantic and reasoning capabilities, LLMs are reshaping how\nautonomous systems perceive, model, and predict trajectories. This survey\nprovides a comprehensive overview of this emerging field, categorizing recent\nwork into five directions: (1) Trajectory prediction via language modeling\nparadigms, (2) Direct trajectory prediction with pretrained language models,\n(3) Language-guided scene understanding for trajectory prediction, (4)\nLanguage-driven data generation for trajectory prediction, (5) Language-based\nreasoning and interpretability for trajectory prediction. For each, we analyze\nrepresentative methods, highlight core design choices, and identify open\nchallenges. This survey bridges natural language processing and trajectory\nprediction, offering a unified perspective on how language can enrich\ntrajectory prediction.", "AI": {"tldr": "\u7efc\u8ff0\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u901a\u8fc7\u8bed\u4e49\u548c\u63a8\u7406\u80fd\u529b\u9769\u65b0\u8f68\u8ff9\u9884\u6d4b\uff0c\u603b\u7ed3\u4e86\u4e94\u5927\u7814\u7a76\u65b9\u5411\u548c\u65b9\u6cd5\u3002", "motivation": "\u6574\u5408\u8bed\u8a00\u9a71\u52a8\u6280\u672f\u4ee5\u63d0\u5347\u8f68\u8ff9\u9884\u6d4b\u7684\u611f\u77e5\u3001\u5efa\u6a21\u548c\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u5206\u7c7b\u4e3a\u4e94\u7c7b\uff1a\u8bed\u8a00\u5efa\u6a21\u8303\u5f0f\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u76f4\u63a5\u9884\u6d4b\u3001\u8bed\u8a00\u5f15\u5bfc\u573a\u666f\u7406\u89e3\u3001\u8bed\u8a00\u9a71\u52a8\u6570\u636e\u751f\u6210\u3001\u8bed\u8a00\u63a8\u7406\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u63d0\u4f9b\u4e86\u4ee3\u8868\u6027\u65b9\u6cd5\u548c\u6838\u5fc3\u8bbe\u8ba1\u9009\u62e9\u7684\u5206\u6790\uff0c\u5e76\u6307\u51fa\u5f00\u653e\u6311\u6218\u3002", "conclusion": "\u6865\u63a5\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e0e\u8f68\u8ff9\u9884\u6d4b\uff0c\u5c55\u793a\u4e86\u8bed\u8a00\u5982\u4f55\u4e30\u5bcc\u8f68\u8ff9\u9884\u6d4b\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u8f68\u8ff9\u9884\u6d4b\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8bed\u4e49\u63a8\u7406\u3001\u573a\u666f\u7406\u89e3"}}
{"id": "2506.03586", "pdf": "https://arxiv.org/pdf/2506.03586", "abs": "https://arxiv.org/abs/2506.03586", "authors": ["Yu Ma", "Chongtao Guo", "Le Liang", "Xiao Li", "Shi Jin"], "title": "Joint Beamforming and Resource Allocation for Delay Optimization in RIS-Assisted OFDM Systems: A DRL Approach", "categories": ["cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "This paper investigates a joint phase design and resource allocation problem\nin downlink reconfigurable intelligent surface (RIS)-assisted orthogonal\nfrequency division multiplexing (OFDM) systems to optimize average delay, where\ndata packets for each user arrive at the base station stochastically. The\nsequential optimization problem is inherently a Markov decision process (MDP),\nmaking it fall within the scope of reinforcement learning. To effectively\nhandle the mixed action space and reduce the state space dimensionality, a\nhybrid deep reinforcement learning (DRL) approach is proposed. Specifically,\nproximal policy optimization (PPO)-$\\Theta$ is employed to optimize RIS phase\nshift design, while PPO-N is responsible for subcarrier allocation decisions.\nTo further mitigate the curse of dimensionality associated with subcarrier\nallocation, a multi-agent strategy is introduced to optimize subcarrier\nallocation indicater more efficiently. Moreover, to achieve more adaptive\nresource allocation and accurately capture network dynamics, key factors\nclosely related to average delay, including the number of backlogged packets in\nbuffers and the current packet arrivals, are incorporated into the state space.\nFurthermore, a transfer learning framework is introduced to enhance training\nefficiency and accelerate convergence. Simulation results demonstrate that the\nproposed algorithm significantly reduces average delay, enhances resource\nallocation efficiency, and achieves superior system robustness and fairness\ncompared to baseline methods.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f18\u5316RIS\u8f85\u52a9OFDM\u7cfb\u7edf\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u91c7\u7528\u6df7\u5408PPO\u548c\u591a\u667a\u80fd\u4f53\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u5206\u914d\u6548\u7387\u548c\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4e0b\u884cRIS\u8f85\u52a9OFDM\u7cfb\u7edf\u4e2d\u968f\u673a\u6570\u636e\u5305\u5230\u8fbe\u5bfc\u81f4\u7684\u5ef6\u8fdf\u4f18\u5316\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08PPO-\u0398\u548cPPO-N\uff09\u4f18\u5316RIS\u76f8\u4f4d\u8bbe\u8ba1\u548c\u5b50\u8f7d\u6ce2\u5206\u914d\uff0c\u5f15\u5165\u591a\u667a\u80fd\u4f53\u7b56\u7565\u548c\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u7b97\u6cd5\u663e\u8457\u964d\u4f4e\u5e73\u5747\u5ef6\u8fdf\uff0c\u63d0\u5347\u8d44\u6e90\u5206\u914d\u6548\u7387\u548c\u7cfb\u7edf\u7a33\u5065\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728RIS\u8f85\u52a9\u7cfb\u7edf\u4e2d\u6709\u6548\u4f18\u5316\u5ef6\u8fdf\u548c\u8d44\u6e90\u5206\u914d\u3002", "keywords": "RIS, OFDM, \u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60, \u8d44\u6e90\u5206\u914d, \u5ef6\u8fdf\u4f18\u5316"}}
{"id": "2506.03176", "pdf": "https://arxiv.org/pdf/2506.03176", "abs": "https://arxiv.org/abs/2506.03176", "authors": ["Bin Wang", "Yongqi Han", "Minbo Ma", "Tianrui Li", "Junbo Zhang", "Feng Hong", "Yanwei Yu"], "title": "Non-collective Calibrating Strategy for Time Series Forecasting", "categories": ["cs.LG"], "comment": "Accepted by IJCAI 2025", "summary": "Deep learning-based approaches have demonstrated significant advancements in\ntime series forecasting. Despite these ongoing developments, the complex\ndynamics of time series make it challenging to establish the rule of thumb for\ndesigning the golden model architecture. In this study, we argue that refining\nexisting advanced models through a universal calibrating strategy can deliver\nsubstantial benefits with minimal resource costs, as opposed to elaborating and\ntraining a new model from scratch. We first identify a multi-target learning\nconflict in the calibrating process, which arises when optimizing variables\nacross time steps, leading to the underutilization of the model's learning\ncapabilities. To address this issue, we propose an innovative calibrating\nstrategy called Socket+Plug (SoP). This approach retains an exclusive optimizer\nand early-stopping monitor for each predicted target within each Plug while\nkeeping the fully trained Socket backbone frozen. The model-agnostic nature of\nSoP allows it to directly calibrate the performance of any trained deep\nforecasting models, regardless of their specific architectures. Extensive\nexperiments on various time series benchmarks and a spatio-temporal\nmeteorological ERA5 dataset demonstrate the effectiveness of SoP, achieving up\nto a 22% improvement even when employing a simple MLP as the Plug (highlighted\nin Figure 1)", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSocket+Plug\uff08SoP\uff09\u7684\u521b\u65b0\u6821\u51c6\u7b56\u7565\uff0c\u7528\u4e8e\u4f18\u5316\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u800c\u975e\u4ece\u5934\u8bbe\u8ba1\u65b0\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u590d\u6742\u6027\u4f7f\u5f97\u8bbe\u8ba1\u7406\u60f3\u67b6\u6784\u56f0\u96be\uff0c\u4f5c\u8005\u8ba4\u4e3a\u901a\u8fc7\u901a\u7528\u6821\u51c6\u7b56\u7565\u4f18\u5316\u73b0\u6709\u6a21\u578b\u6bd4\u4ece\u5934\u8bad\u7ec3\u66f4\u9ad8\u6548\u3002", "method": "\u63d0\u51faSoP\u7b56\u7565\uff0c\u4e3a\u6bcf\u4e2a\u9884\u6d4b\u76ee\u6807\u4fdd\u7559\u72ec\u7acb\u7684\u4f18\u5316\u5668\u548c\u65e9\u505c\u76d1\u63a7\u5668\uff08Plug\uff09\uff0c\u540c\u65f6\u51bb\u7ed3\u4e3b\u5e72\u6a21\u578b\uff08Socket\uff09\uff0c\u89e3\u51b3\u591a\u76ee\u6807\u5b66\u4e60\u51b2\u7a81\u3002", "result": "\u5728\u591a\u4e2a\u65f6\u95f4\u5e8f\u5217\u57fa\u51c6\u548cERA5\u6c14\u8c61\u6570\u636e\u96c6\u4e0a\uff0cSoP\u5b9e\u73b0\u4e86\u9ad8\u8fbe22%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5373\u4f7f\u4f7f\u7528\u7b80\u5355\u7684MLP\u4f5c\u4e3aPlug\u3002", "conclusion": "SoP\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u9ad8\u6548\u6821\u51c6\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8d44\u6e90\u6d88\u8017\u4f4e\u3002", "keywords": "\u6df1\u5ea6\u5b66\u4e60,\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b,\u6a21\u578b\u6821\u51c6,Socket+Plug,\u591a\u76ee\u6807\u5b66\u4e60"}}
{"id": "2506.03424", "pdf": "https://arxiv.org/pdf/2506.03424", "abs": "https://arxiv.org/abs/2506.03424", "authors": ["Nicole R Schneider", "Nandini Ramachandran", "Kent O'Sullivan", "Hanan Samet"], "title": "DistRAG: Towards Distance-Based Spatial Reasoning in LLMs", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Many real world tasks where Large Language Models (LLMs) can be used require\nspatial reasoning, like Point of Interest (POI) recommendation and itinerary\nplanning. However, on their own LLMs lack reliable spatial reasoning\ncapabilities, especially about distances. To address this problem, we develop a\nnovel approach, DistRAG, that enables an LLM to retrieve relevant spatial\ninformation not explicitly learned during training. Our method encodes the\ngeodesic distances between cities and towns in a graph and retrieves a context\nsubgraph relevant to the question. Using this technique, our method enables an\nLLM to answer distance-based reasoning questions that it otherwise cannot\nanswer. Given the vast array of possible places an LLM could be asked about,\nDistRAG offers a flexible first step towards providing a rudimentary `world\nmodel' to complement the linguistic knowledge held in LLMs.", "AI": {"tldr": "DistRAG\u65b9\u6cd5\u901a\u8fc7\u7f16\u7801\u5730\u7406\u8ddd\u79bb\u5e76\u68c0\u7d22\u76f8\u5173\u5b50\u56fe\uff0c\u589e\u5f3a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\uff08\u5982POI\u63a8\u8350\u548c\u884c\u7a0b\u89c4\u5212\uff09\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u8ddd\u79bb\u63a8\u7406\u65b9\u9762\u3002", "method": "\u63d0\u51faDistRAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u548c\u68c0\u7d22\u5730\u7406\u8ddd\u79bb\u5b50\u56fe\u6765\u8865\u5145LLM\u7684\u7a7a\u95f4\u77e5\u8bc6\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7fLLM\u80fd\u591f\u56de\u7b54\u539f\u672c\u65e0\u6cd5\u5904\u7406\u7684\u57fa\u4e8e\u8ddd\u79bb\u7684\u63a8\u7406\u95ee\u9898\u3002", "conclusion": "DistRAG\u662f\u521d\u6b65\u4e3aLLM\u63d0\u4f9b\u4e16\u754c\u6a21\u578b\u4ee5\u8865\u5145\u5176\u8bed\u8a00\u77e5\u8bc6\u7684\u6709\u6548\u5de5\u5177\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u7a7a\u95f4\u63a8\u7406,\u8ddd\u79bb\u63a8\u7406,DistRAG,\u5730\u7406\u8ddd\u79bb\u5b50\u56fe"}}
{"id": "2506.03610", "pdf": "https://arxiv.org/pdf/2506.03610", "abs": "https://arxiv.org/abs/2506.03610", "authors": ["Dongmin Park", "Minkyu Kim", "Beongjun Choi", "Junhyuck Kim", "Keon Lee", "Jonghyun Lee", "Inkyu Park", "Byeong-Uk Lee", "Jaeyoung Hwang", "Jaewoo Ahn", "Ameya S. Mahabaleshwarkar", "Bilal Kartal", "Pritam Biswas", "Yoshi Suhara", "Kangwook Lee", "Jaewoong Cho"], "title": "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Model (LLM) agents are reshaping the game industry,\nparticularly with more intelligent and human-preferable game characters.\nHowever, existing game benchmarks fall short of practical needs: they lack\nevaluations of diverse LLM capabilities across various game genres, studies of\nagentic modules crucial for complex gameplay, and fine-tuning datasets for\naligning pre-trained LLMs into gaming agents. To fill these gaps, we present\n\\textbf{\\benchname{}}, a foundational benchmark designed to train and evaluate\nLLM agents across diverse real-world video games. Unlike existing benchmarks,\nOrak includes 12 popular video games spanning all major genres, enabling\ncomprehensive studies of LLM capabilities and agentic modules essential for\nintricate game scenarios. To support consistent evaluation of LLMs, we\nintroduce a plug-and-play interface based on Model Context Protocol (MCP) that\nenables LLMs to seamlessly connect with games and manipulate agentic modules.\nAdditionally, we propose a fine-tuning dataset, consisting of LLM gameplay\ntrajectories across diverse game genres. Orak offers a comprehensive evaluation\nframework, encompassing general game score leaderboards, LLM battle arenas, and\nin-depth analyses of visual input state, agentic strategies, and fine-tuning\neffects, establishing a foundation towards building generic gaming agents. Code\nis available at https://github.com/krafton-ai/Orak.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Orak\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30LLM\u4ee3\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d612\u79cd\u6e38\u620f\u7c7b\u578b\uff0c\u63d0\u4f9b\u7edf\u4e00\u63a5\u53e3\u548c\u5fae\u8c03\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u6e38\u620f\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\uff0c\u7f3a\u4e4f\u5bf9\u591a\u6837\u5316LLM\u80fd\u529b\u7684\u8bc4\u4f30\u548c\u590d\u6742\u6e38\u620f\u573a\u666f\u7684\u7814\u7a76\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aOrak\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec12\u79cd\u6e38\u620f\u7c7b\u578b\u3001Model Context Protocol\u63a5\u53e3\u548c\u5fae\u8c03\u6570\u636e\u96c6\u3002", "result": "Orak\u63d0\u4f9b\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u6e38\u620f\u5206\u6570\u6392\u884c\u699c\u3001\u6218\u6597\u7ade\u6280\u573a\u548c\u6df1\u5165\u5206\u6790\u3002", "conclusion": "Orak\u4e3a\u6784\u5efa\u901a\u7528\u6e38\u620f\u4ee3\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "keywords": "LLM\u4ee3\u7406,\u6e38\u620f\u57fa\u51c6\u6d4b\u8bd5,Orak,\u5fae\u8c03\u6570\u636e\u96c6,Model Context Protocol"}}
{"id": "2506.03206", "pdf": "https://arxiv.org/pdf/2506.03206", "abs": "https://arxiv.org/abs/2506.03206", "authors": ["Nadav Timor", "Jonathan Mamou", "Oren Pereg", "Hongyang Zhang", "David Harel"], "title": "Out-of-Vocabulary Sampling Boosts Speculative Decoding", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Speculative decoding relies on fast and accurate drafters. Recent\nstate-of-the-art language models employ larger and larger vocabularies, which\nsignificantly slows down drafters. One promising approach to boost the\nefficiency of speculative decoding is to use drafters with smaller\nvocabularies. However, existing sampling methods cannot draw out-of-vocabulary\ntokens, creating a tradeoff between drafters' vocabulary size and acceptance\nrates. This paper introduces Redistributing Drafter Kernels (RDK), the first\nout-of-vocabulary sampler that effectively recovers acceptance rates by\nvirtually restoring pruned target tokens. RDK leverages token-affinity priors\nto reallocate drafter mass towards high-overlap regions. We prove\nmathematically that RDK can achieve higher acceptance rates than vanilla and\nstate-of-the-art samplers. We provide an efficient first-order approximation of\nRDK and prove that it reduces redistribution times from $O(N^2)$ to $O(N)$,\nenabling lightweight implementations for large vocabularies. Our experiments\ndemonstrate that this linear-time RDK significantly boosts acceptance rates\neven after extreme pruning (removing more than 75% of the drafter's\nvocabulary), where existing samplers fail. RDK opens the door to extremely\npruned drafters, which were previously impractical.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRDK\u7684\u65b0\u91c7\u6837\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u865a\u62df\u6062\u590d\u88ab\u5254\u9664\u7684\u8bcd\u6c47\u6765\u63d0\u5347\u4f4e\u8bcd\u6c47\u91cf\u8349\u62df\u5668\u7684\u63a5\u53d7\u7387\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u91c7\u6837\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u672a\u5305\u542b\u8bcd\u6c47\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u8bcd\u6c47\u91cf\u6a21\u578b\u5728\u8349\u62df\u5668\u4e2d\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u627e\u5230\u4e00\u79cd\u65b9\u6cd5\uff0c\u65e2\u80fd\u51cf\u5c11\u8349\u62df\u5668\u7684\u8bcd\u6c47\u91cf\uff0c\u53c8\u4e0d\u964d\u4f4e\u5176\u63a5\u53d7\u7387\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86Redistributing Drafter Kernels\uff08RDK\uff09\uff0c\u4e00\u79cd\u80fd\u591f\u6062\u590d\u88ab\u5254\u9664\u8bcd\u6c47\u7684\u865a\u62df\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u5206\u914d\u8349\u62df\u5668\u7684\u6743\u91cd\u5230\u9ad8\u91cd\u53e0\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRDK\u663e\u8457\u63d0\u5347\u4e86\u63a5\u53d7\u7387\uff0c\u5373\u4f7f\u5728\u9ad8\u8bcd\u6c47\u91cf\u7f29\u51cf\uff08\u8d85\u8fc775%\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u73b0\u6709\u91c7\u6837\u65b9\u6cd5\u5931\u6548\u65f6\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c\u3002", "conclusion": "RDK\u4e3a\u9ad8\u8bcd\u6c47\u91cf\u88c1\u526a\u8349\u62df\u5668\u63d0\u4f9b\u4e86\u5b9e\u7528\u6027\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u96be\u4ee5\u5b9e\u73b0\u7684\u6548\u7387\u95ee\u9898\u3002", "keywords": "speculative decoding, drafters, vocabulary pruning, token-affinity priors, acceptance rates"}}
{"id": "2506.03434", "pdf": "https://arxiv.org/pdf/2506.03434", "abs": "https://arxiv.org/abs/2506.03434", "authors": ["Ahmad Dawar Hakimi", "Ali Modarressi", "Philipp Wicke", "Hinrich Sch\u00fctze"], "title": "Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Understanding how large language models (LLMs) acquire and store factual\nknowledge is crucial for enhancing their interpretability and reliability. In\nthis work, we analyze the evolution of factual knowledge representation in the\nOLMo-7B model by tracking the roles of its attention heads and feed forward\nnetworks (FFNs) over the course of pre-training. We classify these components\ninto four roles: general, entity, relation-answer, and fact-answer specific,\nand examine their stability and transitions. Our results show that LLMs\ninitially depend on broad, general-purpose components, which later specialize\nas training progresses. Once the model reliably predicts answers, some\ncomponents are repurposed, suggesting an adaptive learning process. Notably,\nattention heads display the highest turnover. We also present evidence that\nFFNs remain more stable throughout training. Furthermore, our probing\nexperiments reveal that location-based relations converge to high accuracy\nearlier in training than name-based relations, highlighting how task complexity\nshapes acquisition dynamics. These insights offer a mechanistic view of\nknowledge formation in LLMs.", "AI": {"tldr": "OLMo-7B\u6a21\u578b\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u5934\u548cFFN\u5728\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u89d2\u8272\u53d8\u5316\uff0c\u63ed\u793a\u4e86LLM\u5982\u4f55\u4e60\u5f97\u548c\u5b58\u50a8\u4e8b\u5b9e\u77e5\u8bc6\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u521d\u671f\u4f9d\u8d56\u901a\u7528\u7ec4\u4ef6\uff0c\u540e\u671f\u9010\u6e10\u4e13\u4e1a\u5316\uff0c\u4e14\u6ce8\u610f\u529b\u5934\u7684\u89d2\u8272\u53d8\u5316\u66f4\u4e3a\u9891\u7e41\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793aLLM\u5982\u4f55\u4e60\u5f97\u548c\u5b58\u50a8\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u4ee5\u63d0\u9ad8\u5176\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u6ce8\u610f\u529b\u5934\u548cFFN\u7684\u89d2\u8272\uff0c\u5e76\u8ddf\u8e2a\u5176\u7a33\u5b9a\u6027\u548c\u8f6c\u53d8\uff0c\u5206\u6790OLMo-7B\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "LLM\u521d\u671f\u4f9d\u8d56\u901a\u7528\u7ec4\u4ef6\uff0c\u540e\u671f\u4e13\u4e1a\u5316\uff1b\u6ce8\u610f\u529b\u5934\u89d2\u8272\u53d8\u5316\u9891\u7e41\uff0cFFN\u66f4\u7a33\u5b9a\uff1b\u57fa\u4e8e\u4f4d\u7f6e\u7684\u5173\u7cfb\u6bd4\u57fa\u4e8e\u540d\u79f0\u7684\u5173\u7cfb\u66f4\u65e9\u6536\u655b\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63d0\u4f9b\u4e86LLM\u77e5\u8bc6\u5f62\u6210\u7684\u673a\u5236\u6027\u89c1\u89e3\u3002", "keywords": "large language models, factual knowledge, attention heads, FFN, pre-training"}}
{"id": "2506.03613", "pdf": "https://arxiv.org/pdf/2506.03613", "abs": "https://arxiv.org/abs/2506.03613", "authors": ["Shaoshan Liu", "Fan Wang", "Hongjun Zhou", "Yuanfeng Wang"], "title": "Training Cross-Morphology Embodied AI Agents: From Practical Challenges to Theoretical Foundations", "categories": ["cs.AI", "cs.CC"], "comment": null, "summary": "While theory and practice are often seen as separate domains, this article\nshows that theoretical insight is essential for overcoming real-world\nengineering barriers. We begin with a practical challenge: training a\ncross-morphology embodied AI policy that generalizes across diverse robot\nmorphologies. We formalize this as the Heterogeneous Embodied Agent Training\n(HEAT) problem and prove it reduces to a structured Partially Observable Markov\nDecision Process (POMDP) that is PSPACE-complete. This result explains why\ncurrent reinforcement learning pipelines break down under morphological\ndiversity, due to sequential training constraints, memory-policy coupling, and\ndata incompatibility. We further explore Collective Adaptation, a distributed\nlearning alternative inspired by biological systems. Though NEXP-complete in\ntheory, it offers meaningful scalability and deployment benefits in practice.\nThis work illustrates how computational theory can illuminate system design\ntrade-offs and guide the development of more robust, scalable embodied AI. For\npractitioners and researchers to explore this problem, the implementation code\nof this work has been made publicly available at\nhttps://github.com/airs-admin/HEAT", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u7406\u8bba\u6d1e\u5bdf\u5982\u4f55\u5e2e\u52a9\u89e3\u51b3\u5b9e\u9645\u5de5\u7a0b\u95ee\u9898\uff0c\u7279\u522b\u662f\u8de8\u5f62\u6001\u673a\u5668\u4eba\u7684AI\u7b56\u7565\u8bad\u7ec3\u95ee\u9898\u3002", "motivation": "\u7406\u8bba\u548c\u5b9e\u8df5\u901a\u5e38\u88ab\u89c6\u4e3a\u5206\u79bb\u9886\u57df\uff0c\u4f46\u672c\u6587\u65e8\u5728\u8bc1\u660e\u7406\u8bba\u6d1e\u5bdf\u5bf9\u514b\u670d\u5b9e\u9645\u5de5\u7a0b\u969c\u788d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u5e76\u5f62\u5f0f\u5316\u4e86\u5f02\u6784\u4f53\u73b0\u667a\u80fd\u4f53\u8bad\u7ec3(HEAT)\u95ee\u9898\uff0c\u8bc1\u660e\u5176\u53ef\u5f52\u7ed3\u4e3a\u7ed3\u6784\u5316\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(POMDP)\uff0c\u5c5e\u4e8ePSPACE\u5b8c\u5168\u95ee\u9898\u3002\u540c\u65f6\u7814\u7a76\u4e86\u96c6\u4f53\u9002\u5e94\u8fd9\u4e00\u5206\u5e03\u5f0f\u5b66\u4e60\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5f62\u6001\u591a\u6837\u6027\u4e0b\u5931\u6548\u7684\u539f\u56e0\uff0c\u5e76\u5c55\u793a\u4e86\u96c6\u4f53\u9002\u5e94\u5728\u5b9e\u8df5\u4e2d\u53ef\u63d0\u4f9b\u53ef\u6269\u5c55\u6027\u548c\u90e8\u7f72\u4f18\u52bf\u3002", "conclusion": "\u8ba1\u7b97\u7406\u8bba\u80fd\u5e2e\u52a9\u7406\u89e3\u7cfb\u7edf\u8bbe\u8ba1\u6743\u8861\uff0c\u5e76\u6307\u5bfc\u5f00\u53d1\u66f4\u7a33\u5065\u3001\u53ef\u6269\u5c55\u7684\u4f53\u73b0AI\u3002", "keywords": "\u7406\u8bba\u6d1e\u5bdf\uff0c\u5f02\u6784\u4f53\u73b0\u667a\u80fd\u4f53\uff0cPSPACE\u5b8c\u5168\uff0c\u96c6\u4f53\u9002\u5e94\uff0c\u5f3a\u5316\u5b66\u4e60"}}
{"id": "2506.03207", "pdf": "https://arxiv.org/pdf/2506.03207", "abs": "https://arxiv.org/abs/2506.03207", "authors": ["Md Nahid Hasan Shuvo", "Moinul Hossain"], "title": "Fingerprinting Deep Learning Models via Network Traffic Patterns in Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "7 pages, 4 Figures, Accepted to publish in Proceedings of the 2025\n  ACM Workshop on Wireless Security and Machine Learning (WiseML 2025), July 3,\n  2025, Arlington, VA, USA", "summary": "Federated Learning (FL) is increasingly adopted as a decentralized machine\nlearning paradigm due to its capability to preserve data privacy by training\nmodels without centralizing user data. However, FL is susceptible to indirect\nprivacy breaches via network traffic analysis-an area not explored in existing\nresearch. The primary objective of this research is to study the feasibility of\nfingerprinting deep learning models deployed within FL environments by\nanalyzing their network-layer traffic information. In this paper, we conduct an\nexperimental evaluation using various deep learning architectures (i.e., CNN,\nRNN) within a federated learning testbed. We utilize machine learning\nalgorithms, including Support Vector Machines (SVM), Random Forest, and\nGradient-Boosting, to fingerprint unique patterns within the traffic data. Our\nexperiments show high fingerprinting accuracy, achieving 100% accuracy using\nRandom Forest and around 95.7% accuracy using SVM and Gradient Boosting\nclassifiers. This analysis suggests that we can identify specific architectures\nrunning within the subsection of the network traffic. Hence, if an adversary\nknows about the underlying DL architecture, they can exploit that information\nand conduct targeted attacks. These findings suggest a notable security\nvulnerability in FL systems and the necessity of strengthening it at the\nnetwork level.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u901a\u8fc7\u5206\u6790\u7f51\u7edc\u6d41\u91cf\u6307\u7eb9\u8bc6\u522b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u9ad8\u51c6\u786e\u7387\u53ef\u80fd\u5f15\u53d1\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793aFL\u73af\u5883\u4e2d\u7f51\u7edc\u6d41\u91cf\u5206\u6790\u53ef\u80fd\u5bfc\u81f4\u95f4\u63a5\u9690\u79c1\u6cc4\u9732\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5229\u7528\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08CNN\u3001RNN\uff09\u5728FL\u6d4b\u8bd5\u5e8a\u4e0a\u5b9e\u9a8c\uff0c\u5e76\u901a\u8fc7SVM\u3001\u968f\u673a\u68ee\u6797\u548c\u68af\u5ea6\u63d0\u5347\u7b97\u6cd5\u5206\u6790\u6d41\u91cf\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u968f\u673a\u68ee\u6797\u8fbe\u5230100%\u8bc6\u522b\u51c6\u786e\u7387\uff0cSVM\u548c\u68af\u5ea6\u63d0\u5347\u7ea695.7%\uff0c\u8868\u660e\u7279\u5b9a\u67b6\u6784\u53ef\u88ab\u8bc6\u522b\u3002", "conclusion": "FL\u7cfb\u7edf\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u52a0\u5f3a\u7f51\u7edc\u5c42\u9632\u62a4\u4ee5\u5e94\u5bf9\u6f5c\u5728\u653b\u51fb\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60,\u7f51\u7edc\u6d41\u91cf\u5206\u6790,\u9690\u79c1\u6cc4\u9732,\u6df1\u5ea6\u5b66\u4e60,\u5b89\u5168\u6f0f\u6d1e"}}
{"id": "2506.03458", "pdf": "https://arxiv.org/pdf/2506.03458", "abs": "https://arxiv.org/abs/2506.03458", "authors": ["Zahra Bokaei", "Walid Magdy", "Bonnie Webber"], "title": "Culture Matters in Toxic Language Detection in Persian", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Track)", "summary": "Toxic language detection is crucial for creating safer online environments\nand limiting the spread of harmful content. While toxic language detection has\nbeen under-explored in Persian, the current work compares different methods for\nthis task, including fine-tuning, data enrichment, zero-shot and few-shot\nlearning, and cross-lingual transfer learning. What is especially compelling is\nthe impact of cultural context on transfer learning for this task: We show that\nthe language of a country with cultural similarities to Persian yields better\nresults in transfer learning. Conversely, the improvement is lower when the\nlanguage comes from a culturally distinct country. Warning: This paper contains\nexamples of toxic language that may disturb some readers. These examples are\nincluded for the purpose of research on toxic detection.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6ce2\u65af\u8bed\u4e2d\u6709\u6bd2\u8bed\u8a00\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u5fae\u8c03\u3001\u6570\u636e\u4e30\u5bcc\u5316\u3001\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u5b66\u4e60\u4ee5\u53ca\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u7684\u6548\u679c\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6587\u5316\u80cc\u666f\u5bf9\u8fc1\u79fb\u5b66\u4e60\u7684\u8868\u73b0\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8e\u6ce2\u65af\u8bed\u4e2d\u6709\u6bd2\u8bed\u8a00\u68c0\u6d4b\u7684\u7814\u7a76\u8f83\u5c11\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\uff0c\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u4f7f\u7528\u4e86\u5fae\u8c03\u3001\u6570\u636e\u4e30\u5bcc\u5316\u3001\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u5b66\u4e60\u4ee5\u53ca\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u7b49\u591a\u79cd\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u7814\u7a76\u3002", "result": "\u6587\u5316\u80cc\u666f\u76f8\u4f3c\u7684\u6e90\u8bed\u8a00\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\u5bf9\u6ce2\u65af\u8bed\u6709\u6bd2\u8bed\u8a00\u68c0\u6d4b\u7684\u6548\u679c\u66f4\u597d\uff0c\u800c\u6587\u5316\u5dee\u5f02\u5927\u7684\u6e90\u8bed\u8a00\u63d0\u5347\u8f83\u5c0f\u3002", "conclusion": "\u6587\u5316\u80cc\u666f\u662f\u5f71\u54cd\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u6548\u679c\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u672a\u6765\u7684\u7814\u7a76\u5e94\u66f4\u6ce8\u91cd\u6587\u5316\u76f8\u4f3c\u6027\u3002", "keywords": "\u6709\u6bd2\u8bed\u8a00\u68c0\u6d4b\u3001\u6ce2\u65af\u8bed\u3001\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u3001\u6587\u5316\u80cc\u666f"}}
{"id": "2506.03673", "pdf": "https://arxiv.org/pdf/2506.03673", "abs": "https://arxiv.org/abs/2506.03673", "authors": ["Yinlong Xu", "Yanzhao Zheng", "Shuoshuo Sun", "Shuaihan Huang", "Baohua Dong", "Hangcheng Zhu", "Ruohui Huang", "Gang Yu", "Hongxia Xu", "Jian Wu"], "title": "Reason from Future: Reverse Thought Chain Enhances LLM Reasoning", "categories": ["cs.AI"], "comment": "Accepted by ACL 2025 findings", "summary": "It has been demonstrated that carefully designed reasoning paradigms, like\nChain-of-Thought (CoT) and Tree-of-Thought (ToT), can enhance the reasoning\ncapabilities of small language models by detailed thinking and extensive\nthought searching, unbounded branching factors in the searching space create\nprohibitive reasoning consumption. However these methods fall into the trap of\nlocal optimum reasoning, which means the model lacks a global perspective while\nsolving problems. We propose a novel reasoning paradigm called Reason from\nFuture (RFF), which generates reasoning paths by bidirectional reasoning that\ncombines top-down planning with bottom-up reasoning accumulation. The essence\nof RFF lies in its reverse reasoning mechanism, which prioritizes core logical\nrelationships and imposes goal-oriented constraints on intermediate steps,\nthereby reducing the searching space and mitigating error accumulation inherent\nin sequential forward reasoning. Empirical evaluations across diverse\nexperiments demonstrate that RFF outperforms conventional paradigms with higher\naccuracy and less searching space to solve complex tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u8303\u5f0fReason from Future (RFF)\uff0c\u901a\u8fc7\u53cc\u5411\u63a8\u7406\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u90e8\u6700\u4f18\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u63a8\u7406\u65b9\u6cd5\uff08\u5982CoT\u548cToT\uff09\u5b58\u5728\u641c\u7d22\u7a7a\u95f4\u8fc7\u5927\u548c\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\uff0c\u7f3a\u4e4f\u5168\u5c40\u89c6\u89d2\u3002", "method": "RFF\u901a\u8fc7\u9006\u5411\u63a8\u7406\u673a\u5236\uff0c\u7ed3\u5408\u81ea\u4e0a\u800c\u4e0b\u7684\u89c4\u5212\u548c\u81ea\u4e0b\u800c\u4e0a\u7684\u63a8\u7406\u7d2f\u79ef\uff0c\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\u548c\u9519\u8bef\u7d2f\u79ef\u3002", "result": "RFF\u5728\u591a\u6837\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u5c0f\u7684\u641c\u7d22\u7a7a\u95f4\u3002", "conclusion": "RFF\u4e3a\u590d\u6742\u4efb\u52a1\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u63a8\u7406\u8303\u5f0f, \u53cc\u5411\u63a8\u7406, RFF, \u5c40\u90e8\u6700\u4f18, \u641c\u7d22\u7a7a\u95f4"}}
{"id": "2506.03210", "pdf": "https://arxiv.org/pdf/2506.03210", "abs": "https://arxiv.org/abs/2506.03210", "authors": ["Qiusheng Huang", "Yuan Niu", "Xiaohui Zhong", "Anboyu Guo", "Lei Chen", "Dianjun Zhang", "Xuefeng Zhang", "Hao Li"], "title": "FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": null, "summary": "Accurate, high-resolution ocean forecasting is crucial for maritime\noperations and environmental monitoring. While traditional numerical models are\ncapable of producing sub-daily, eddy-resolving forecasts, they are\ncomputationally intensive and face challenges in maintaining accuracy at fine\nspatial and temporal scales. In contrast, recent data-driven approaches offer\nimproved computational efficiency and emerging potential, yet typically operate\nat daily resolution and struggle with sub-daily predictions due to error\naccumulation over time. We introduce FuXi-Ocean, the first data-driven global\nocean forecasting model achieving six-hourly predictions at eddy-resolving\n1/12{\\deg} spatial resolution, reaching depths of up to 1500 meters. The model\narchitecture integrates a context-aware feature extraction module with a\npredictive network employing stacked attention blocks. The core innovation is\nthe Mixture-of-Time (MoT) module, which adaptively integrates predictions from\nmultiple temporal contexts by learning variable-specific reliability ,\nmitigating cumulative errors in sequential forecasting. Through comprehensive\nexperimental evaluation, FuXi-Ocean demonstrates superior skill in predicting\nkey variables, including temperature, salinity, and currents, across multiple\ndepths.", "AI": {"tldr": "FuXi-Ocean\u662f\u9996\u4e2a\u5b9e\u73b0\u516d\u5c0f\u65f6\u9884\u6d4b\u7684\u6570\u636e\u9a71\u52a8\u5168\u7403\u6d77\u6d0b\u9884\u62a5\u6a21\u578b\uff0c\u5177\u6709\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u548c\u6df1\u5ea6\u8303\u56f4\uff0c\u901a\u8fc7\u521b\u65b0\u6a21\u5757\u51cf\u5c11\u4e86\u7d2f\u8ba1\u8bef\u5dee\u3002", "motivation": "\u63d0\u9ad8\u6d77\u6d0b\u9884\u62a5\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u89e3\u51b3\u4f20\u7edf\u6570\u503c\u6a21\u578b\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u7ec6\u5c3a\u5ea6\u9884\u6d4b\u4e2d\u7684\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u4e0a\u4e0b\u6587\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u548c\u5806\u53e0\u6ce8\u610f\u529b\u5757\u7684\u9884\u6d4b\u7f51\u7edc\uff0c\u6838\u5fc3\u521b\u65b0\u662fMixture-of-Time\uff08MoT\uff09\u6a21\u5757\uff0c\u81ea\u9002\u5e94\u6574\u5408\u591a\u65f6\u95f4\u80cc\u666f\u9884\u6d4b\u3002", "result": "\u5728\u6e29\u5ea6\u3001\u76d0\u5ea6\u548c\u6d77\u6d41\u7b49\u5173\u952e\u53d8\u91cf\u7684\u591a\u6df1\u5ea6\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "FuXi-Ocean\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u9ad8\u5206\u8fa8\u7387\u6d77\u6d0b\u9884\u62a5\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u6d77\u6d0b\u9884\u62a5,\u6570\u636e\u9a71\u52a8,\u9ad8\u5206\u8fa8\u7387,Mixture-of-Time,\u7d2f\u8ba1\u8bef\u5dee"}}
{"id": "2506.03476", "pdf": "https://arxiv.org/pdf/2506.03476", "abs": "https://arxiv.org/abs/2506.03476", "authors": ["Chuyuan Li", "Raymond Li", "Thalia S. Field", "Giuseppe Carenini"], "title": "Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer's Disease Detection", "categories": ["cs.CL"], "comment": null, "summary": "Alzheimer's Disease (AD) is a progressive neurodegenerative disorder that\nleads to dementia, and early intervention can greatly benefit from analyzing\nlinguistic abnormalities. In this work, we explore the potential of Large\nLanguage Models (LLMs) as health assistants for AD diagnosis from\npatient-generated text using in-context learning (ICL), where tasks are defined\nthrough a few input-output examples. Empirical results reveal that conventional\nICL methods, such as similarity-based selection, perform poorly for AD\ndiagnosis, likely due to the inherent complexity of this task. To address this,\nwe introduce Delta-KNN, a novel demonstration selection strategy that enhances\nICL performance. Our method leverages a delta score to assess the relative\ngains of each training example, coupled with a KNN-based retriever that\ndynamically selects optimal \"representatives\" for a given input. Experiments on\ntwo AD detection datasets across three open-source LLMs demonstrate that\nDelta-KNN consistently outperforms existing ICL baselines. Notably, when using\nthe Llama-3.1 model, our approach achieves new state-of-the-art results,\nsurpassing even supervised classifiers.", "AI": {"tldr": "Delta-KNN\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u793a\u8303\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u60a3\u8005\u6587\u672c\u4e2d\u8bca\u65ad\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\uff0c\u4ee5\u65e9\u671f\u5e72\u9884\u3002", "method": "\u63d0\u51faDelta-KNN\u7b56\u7565\uff0c\u7ed3\u5408delta\u8bc4\u5206\u548cKNN\u68c0\u7d22\u5668\uff0c\u52a8\u6001\u9009\u62e9\u6700\u4f18\u793a\u8303\u3002", "result": "\u5728\u4e09\u4e2a\u5f00\u6e90LLM\u548c\u4e24\u4e2aAD\u6570\u636e\u96c6\u4e0a\uff0cDelta-KNN\u8868\u73b0\u4f18\u4e8e\u73b0\u6709ICL\u65b9\u6cd5\uff0c\u751a\u81f3\u5728Llama-3.1\u4e0a\u8d85\u8d8a\u76d1\u7763\u5206\u7c7b\u5668\u3002", "conclusion": "Delta-KNN\u80fd\u6709\u6548\u63d0\u5347LLM\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982AD\u8bca\u65ad\uff09\u4e2d\u7684\u6027\u80fd\u3002", "keywords": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u4e0a\u4e0b\u6587\u5b66\u4e60, Delta-KNN, \u793a\u8303\u9009\u62e9"}}
{"id": "2506.03828", "pdf": "https://arxiv.org/pdf/2506.03828", "abs": "https://arxiv.org/abs/2506.03828", "authors": ["Dhaval Patel", "Shuxin Lin", "James Rayfield", "Nianjun Zhou", "Roman Vaculin", "Natalia Martinez", "Fearghal O'donncha", "Jayant Kalagnanam"], "title": "AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial Asset Operations and Maintenance", "categories": ["cs.AI", "cs.MA"], "comment": "39 pages, 18 figures", "summary": "AI for Industrial Asset Lifecycle Management aims to automate complex\noperational workflows -- such as condition monitoring, maintenance planning,\nand intervention scheduling -- to reduce human workload and minimize system\ndowntime. Traditional AI/ML approaches have primarily tackled these problems in\nisolation, solving narrow tasks within the broader operational pipeline. In\ncontrast, the emergence of AI agents and large language models (LLMs)\nintroduces a next-generation opportunity: enabling end-to-end automation across\nthe entire asset lifecycle. This paper envisions a future where AI agents\nautonomously manage tasks that previously required distinct expertise and\nmanual coordination. To this end, we introduce AssetOpsBench -- a unified\nframework and environment designed to guide the development, orchestration, and\nevaluation of domain-specific agents tailored for Industry 4.0 applications. We\noutline the key requirements for such holistic systems and provide actionable\ninsights into building agents that integrate perception, reasoning, and control\nfor real-world industrial operations. The software is available at\nhttps://github.com/IBM/AssetOpsBench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u4ee3\u7406\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u5316\u6846\u67b6AssetOpsBench\uff0c\u7528\u4e8e\u5de5\u4e1a\u8d44\u4ea7\u751f\u547d\u5468\u671f\u7ba1\u7406\uff0c\u65e8\u5728\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u5e76\u964d\u4f4e\u7cfb\u7edf\u505c\u673a\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edfAI\u65b9\u6cd5\u4ec5\u89e3\u51b3\u5b64\u7acb\u4efb\u52a1\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7AI\u4ee3\u7406\u548cLLMs\u5b9e\u73b0\u5168\u751f\u547d\u5468\u671f\u7684\u81ea\u52a8\u5316\u7ba1\u7406\u3002", "method": "\u63d0\u51fa\u4e86AssetOpsBench\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u3001\u7f16\u6392\u548c\u8bc4\u4f30\u9488\u5bf9\u5de5\u4e1a4.0\u7684\u9886\u57df\u7279\u5b9a\u4ee3\u7406\u3002", "result": "\u6846\u67b6\u6574\u5408\u4e86\u611f\u77e5\u3001\u63a8\u7406\u548c\u63a7\u5236\u529f\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5de5\u4e1a\u64cd\u4f5c\u3002", "conclusion": "AssetOpsBench\u4e3a\u5de5\u4e1a\u8d44\u4ea7\u751f\u547d\u5468\u671f\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u9762\u7684AI\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "AI\u4ee3\u7406, \u5927\u8bed\u8a00\u6a21\u578b, \u5de5\u4e1a4.0, \u8d44\u4ea7\u751f\u547d\u5468\u671f\u7ba1\u7406"}}
{"id": "2506.03225", "pdf": "https://arxiv.org/pdf/2506.03225", "abs": "https://arxiv.org/abs/2506.03225", "authors": ["Wa\u00ebl Doulazmi", "Auguste Lehuger", "Marin Toromanoff", "Valentin Charraut", "Thibault Buhet", "Fabien Moutarde"], "title": "Multiple-Frequencies Population-Based Training", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "Accepted at RLC25", "summary": "Reinforcement Learning's high sensitivity to hyperparameters is a source of\ninstability and inefficiency, creating significant challenges for\npractitioners. Hyperparameter Optimization (HPO) algorithms have been developed\nto address this issue, among them Population-Based Training (PBT) stands out\nfor its ability to generate hyperparameters schedules instead of fixed\nconfigurations. PBT trains a population of agents, each with its own\nhyperparameters, frequently ranking them and replacing the worst performers\nwith mutations of the best agents. These intermediate selection steps can cause\nPBT to focus on short-term improvements, leading it to get stuck in local\noptima and eventually fall behind vanilla Random Search over longer timescales.\nThis paper studies how this greediness issue is connected to the choice of\nevolution frequency, the rate at which the selection is done. We propose\nMultiple-Frequencies Population-Based Training (MF-PBT), a novel HPO algorithm\nthat addresses greediness by employing sub-populations, each evolving at\ndistinct frequencies. MF-PBT introduces a migration process to transfer\ninformation between sub-populations, with an asymmetric design to balance short\nand long-term optimization. Extensive experiments on the Brax suite demonstrate\nthat MF-PBT improves sample efficiency and long-term performance, even without\nactually tuning hyperparameters.", "AI": {"tldr": "MF-PBT\u662f\u4e00\u79cd\u65b0\u578b\u8d85\u53c2\u6570\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u4e2a\u5b50\u7fa4\u4f53\u4ee5\u4e0d\u540c\u9891\u7387\u8fdb\u5316\u548c\u4fe1\u606f\u8fc1\u79fb\uff0c\u89e3\u51b3PBT\u7684\u77ed\u89c6\u6027\u95ee\u9898\uff0c\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u957f\u671f\u6027\u80fd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5bf9\u8d85\u53c2\u6570\u7684\u9ad8\u654f\u611f\u6027\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u548c\u4f4e\u6548\uff0c\u73b0\u6709PBT\u7b97\u6cd5\u56e0\u77ed\u89c6\u6027\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u9700\u6539\u8fdb\u4ee5\u5e73\u8861\u77ed\u671f\u548c\u957f\u671f\u4f18\u5316\u3002", "method": "\u63d0\u51faMF-PBT\u7b97\u6cd5\uff0c\u91c7\u7528\u591a\u9891\u7387\u5b50\u7fa4\u4f53\u8fdb\u5316\u53ca\u4e0d\u5bf9\u79f0\u4fe1\u606f\u8fc1\u79fb\uff0c\u907f\u514dPBT\u7684\u8d2a\u5a6a\u9009\u62e9\u3002", "result": "\u5728Brax\u6d4b\u8bd5\u4e2d\uff0cMF-PBT\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u548c\u957f\u671f\u6027\u80fd\u3002", "conclusion": "MF-PBT\u901a\u8fc7\u591a\u9891\u7387\u8fdb\u5316\u6709\u6548\u89e3\u51b3PBT\u7684\u77ed\u89c6\u6027\u95ee\u9898\uff0c\u4e3a\u8d85\u53c2\u6570\u4f18\u5316\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u8d85\u53c2\u6570\u4f18\u5316, PBT, MF-PBT, \u8d2a\u5a6a\u6027"}}
{"id": "2506.03483", "pdf": "https://arxiv.org/pdf/2506.03483", "abs": "https://arxiv.org/abs/2506.03483", "authors": ["Jun Rao", "Zepeng Lin", "Xuebo Liu", "Xiaopeng Ke", "Lian Lian", "Dong Jin", "Shengjun Cheng", "Jun Yu", "Min Zhang"], "title": "APT: Improving Specialist LLM Performance with Weakness Case Acquisition and Iterative Preference Training", "categories": ["cs.CL"], "comment": "ACL2025 Findings", "summary": "Large Language Models (LLMs) often require domain-specific fine-tuning to\naddress targeted tasks, which risks degrading their general capabilities.\nMaintaining a balance between domain-specific enhancements and general model\nutility is a key challenge. This paper proposes a novel approach named APT\n(Weakness Case Acquisition and Iterative Preference Training) to enhance\ndomain-specific performance with self-generated dis-preferred weakness data\n(bad cases and similar cases). APT uniquely focuses on training the model using\nonly those samples where errors occur, alongside a small, similar set of\nsamples retrieved for this purpose. This targeted training minimizes\ninterference with the model's existing knowledge base, effectively retaining\ngeneric capabilities. Experimental results on the LLama-2 and Mistral-V0.3\nmodels across various benchmarks demonstrate that APT ensures no reduction in\ngeneric capacity and achieves superior performance on downstream tasks compared\nto various existing methods. This validates our method as an effective strategy\nfor enhancing domain-specific capabilities without sacrificing the model's\nbroader applicability.", "AI": {"tldr": "APT\u65b9\u6cd5\u901a\u8fc7\u81ea\u751f\u6210\u5f31\u9879\u6570\u636e\uff08\u574f\u6848\u4f8b\u548c\u76f8\u4f3c\u6848\u4f8b\uff09\u6765\u589e\u5f3a\u9886\u57df\u7279\u5b9a\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u727a\u7272\u901a\u7528\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4e86\u9886\u57df\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u4e2d\u53ef\u80fd\u964d\u4f4e\u901a\u7528\u80fd\u529b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5e73\u8861\u65b9\u6cd5\u3002", "method": "APT\u65b9\u6cd5\u901a\u8fc7\u81ea\u751f\u6210\u5f31\u9879\u6570\u636e\uff08\u574f\u6848\u4f8b\u548c\u76f8\u4f3c\u6848\u4f8b\uff09\uff0c\u4ec5\u5bf9\u9519\u8bef\u6837\u672c\u8fdb\u884c\u9488\u5bf9\u6027\u8bad\u7ec3\uff0c\u4fdd\u7559\u6a21\u578b\u901a\u7528\u80fd\u529b\u3002", "result": "\u5728LLama-2\u548cMistral-V0.3\u6a21\u578b\u4e0a\uff0cAPT\u5728\u4e0d\u5f71\u54cd\u901a\u7528\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4e86\u9886\u57df\u4efb\u52a1\u8868\u73b0\u3002", "conclusion": "APT\u662f\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\uff0c\u53ef\u5728\u4e0d\u727a\u7272\u6a21\u578b\u901a\u7528\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u589e\u5f3a\u9886\u57df\u7279\u5b9a\u6027\u80fd\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u9886\u57df\u5fae\u8c03,\u5f31\u9879\u6570\u636e,\u901a\u7528\u80fd\u529b"}}
{"id": "2506.03915", "pdf": "https://arxiv.org/pdf/2506.03915", "abs": "https://arxiv.org/abs/2506.03915", "authors": ["Sebastian R\u00f6dling", "Matej Ze\u010devi\u0107", "Devendra Singh Dhami", "Kristian Kersting"], "title": "Causal Explanations Over Time: Articulated Reasoning for Interactive Environments", "categories": ["cs.AI"], "comment": "Main paper: 9 pages, References: 2 pages, Supplementary: 9 pages.\n  Number of figures: 10, number of tables: 3", "summary": "Structural Causal Explanations (SCEs) can be used to automatically generate\nexplanations in natural language to questions about given data that are\ngrounded in a (possibly learned) causal model. Unfortunately they work for\nsmall data only. In turn they are not attractive to offer reasons for events,\ne.g., tracking causal changes over multiple time steps, or a behavioral\ncomponent that involves feedback loops through actions of an agent. To this\nend, we generalize SCEs to a (recursive) formulation of explanation trees to\ncapture the temporal interactions between reasons. We show the benefits of this\nmore general SCE algorithm on synthetic time-series data and a 2D grid game,\nand further compare it to the base SCE and other existing methods for causal\nexplanations.", "AI": {"tldr": "\u672c\u6587\u5c06SCEs\u6269\u5c55\u4e3a\u9012\u5f52\u89e3\u91ca\u6811\uff0c\u4ee5\u6355\u6349\u65f6\u95f4\u4ea4\u4e92\uff0c\u5e76\u5728\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u548c2D\u7f51\u683c\u6e38\u620f\u4e2d\u9a8c\u8bc1\u5176\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edfSCEs\u4ec5\u9002\u7528\u4e8e\u5c0f\u6570\u636e\uff0c\u65e0\u6cd5\u5904\u7406\u591a\u65f6\u95f4\u6b65\u6216\u53cd\u9988\u5faa\u73af\u884c\u4e3a\uff0c\u9700\u6269\u5c55\u5176\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u9012\u5f52\u89e3\u91ca\u6811\u6a21\u578b\uff0c\u6269\u5c55SCEs\u4ee5\u6355\u6349\u65f6\u95f4\u4ea4\u4e92\u3002", "result": "\u5728\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u548c2D\u7f51\u683c\u6e38\u620f\u4e2d\u5c55\u793a\u4f18\u4e8e\u57fa\u7840SCEs\u53ca\u5176\u4ed6\u56e0\u679c\u89e3\u91ca\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u9012\u5f52SCEs\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u65f6\u95f4\u4f9d\u8d56\u548c\u53cd\u9988\u5faa\u73af\uff0c\u63d0\u5347\u89e3\u91ca\u80fd\u529b\u3002", "keywords": "\u56e0\u679c\u89e3\u91ca, \u7ed3\u6784\u56e0\u679c\u89e3\u91ca, \u9012\u5f52\u89e3\u91ca\u6811, \u65f6\u95f4\u5e8f\u5217, \u53cd\u9988\u5faa\u73af"}}
{"id": "2506.03227", "pdf": "https://arxiv.org/pdf/2506.03227", "abs": "https://arxiv.org/abs/2506.03227", "authors": ["Abdelrahman Sayed Sayed", "Pierre-Jean Meyer", "Mohamed Ghazel"], "title": "Bridging Neural ODE and ResNet: A Formal Error Bound for Safety Verification", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages, 5 figures, Accepted for publication in the proceedings of\n  the 8th International Symposium on AI Verification SAIV 2025", "summary": "A neural ordinary differential equation (neural ODE) is a machine learning\nmodel that is commonly described as a continuous depth generalization of a\nresidual network (ResNet) with a single residual block, or conversely, the\nResNet can be seen as the Euler discretization of the neural ODE. These two\nmodels are therefore strongly related in a way that the behaviors of either\nmodel are considered to be an approximation of the behaviors of the other. In\nthis work, we establish a more formal relationship between these two models by\nbounding the approximation error between two such related models. The obtained\nerror bound then allows us to use one of the models as a verification proxy for\nthe other, without running the verification tools twice: if the reachable\noutput set expanded by the error bound satisfies a safety property on one of\nthe models, this safety property is then guaranteed to be also satisfied on the\nother model. This feature is fully reversible, and the initial safety\nverification can be run indifferently on either of the two models. This novel\napproach is illustrated on a numerical example of a fixed-point attractor\nsystem modeled as a neural ODE.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u8bef\u5dee\u8fb9\u754c\u5efa\u7acb\u4e86\u795e\u7ecfODE\u4e0eResNet\u4e4b\u95f4\u7684\u5f62\u5f0f\u5316\u5173\u7cfb\uff0c\u4f7f\u4e24\u8005\u53ef\u4e92\u4e3a\u9a8c\u8bc1\u4ee3\u7406\uff0c\u4ece\u800c\u907f\u514d\u91cd\u590d\u9a8c\u8bc1\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u5c55\u793a\u4e86\u8fd9\u4e00\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u795e\u7ecfODE\u548cResNet\u5728\u884c\u4e3a\u4e0a\u4e92\u4e3a\u8fd1\u4f3c\uff0c\u4f46\u7f3a\u4e4f\u5f62\u5f0f\u5316\u7684\u5173\u7cfb\u63cf\u8ff0\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u4e24\u8005\u4e4b\u95f4\u7684\u8bef\u5dee\u8fb9\u754c\u4ee5\u7b80\u5316\u9a8c\u8bc1\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u5206\u6790\u786e\u7acb\u795e\u7ecfODE\u4e0eResNet\u4e4b\u95f4\u7684\u8bef\u5dee\u8fb9\u754c\uff0c\u5229\u7528\u8fd9\u4e00\u8fb9\u754c\u5b9e\u73b0\u4e24\u8005\u4e4b\u95f4\u7684\u9a8c\u8bc1\u4ee3\u7406\u529f\u80fd\u3002", "result": "\u63d0\u51fa\u4e86\u53ef\u9006\u7684\u9a8c\u8bc1\u4ee3\u7406\u65b9\u6cd5\uff0c\u4f7f\u5f97\u5bf9\u5176\u4e2d\u4e00\u4e2a\u6a21\u578b\u7684\u9a8c\u8bc1\u7ed3\u679c\u53ef\u901a\u8fc7\u8bef\u5dee\u8fb9\u754c\u76f4\u63a5\u5e94\u7528\u4e8e\u53e6\u4e00\u4e2a\u6a21\u578b\u3002", "conclusion": "\u8bef\u5dee\u8fb9\u754c\u7684\u5f15\u5165\u663e\u8457\u7b80\u5316\u4e86\u795e\u7ecfODE\u4e0eResNet\u7684\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u6027\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002", "keywords": "\u795e\u7ecfODE, ResNet, \u8bef\u5dee\u8fb9\u754c, \u9a8c\u8bc1\u4ee3\u7406, \u5b89\u5168\u6027\u9a8c\u8bc1"}}
{"id": "2506.03484", "pdf": "https://arxiv.org/pdf/2506.03484", "abs": "https://arxiv.org/abs/2506.03484", "authors": ["Melkamu Abay Mersha", "Mesay Gemeda Yigezu", "Atnafu Lambebo Tonja", "Hassan Shakil", "Samer Iskander", "Olga Kolesnikova", "Jugal Kalita"], "title": "Explainable AI: XAI-Guided Context-Aware Data Augmentation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Explainable AI (XAI) has emerged as a powerful tool for improving the\nperformance of AI models, going beyond providing model transparency and\ninterpretability. The scarcity of labeled data remains a fundamental challenge\nin developing robust and generalizable AI models, particularly for low-resource\nlanguages. Conventional data augmentation techniques introduce noise, cause\nsemantic drift, disrupt contextual coherence, lack control, and lead to\noverfitting. To address these challenges, we propose XAI-Guided Context-Aware\nData Augmentation. This novel framework leverages XAI techniques to modify less\ncritical features while selectively preserving most task-relevant features. Our\napproach integrates an iterative feedback loop, which refines augmented data\nover multiple augmentation cycles based on explainability-driven insights and\nthe model performance gain. Our experimental results demonstrate that XAI-SR-BT\nand XAI-PR-BT improve the accuracy of models on hate speech and sentiment\nanalysis tasks by 6.6% and 8.1%, respectively, compared to the baseline, using\nthe Amharic dataset with the XLM-R model. XAI-SR-BT and XAI-PR-BT outperform\nexisting augmentation techniques by 4.8% and 5%, respectively, on the same\ndataset and model. Overall, XAI-SR-BT and XAI-PR-BT consistently outperform\nboth baseline and conventional augmentation techniques across all tasks and\nmodels. This study provides a more controlled, interpretable, and context-aware\nsolution to data augmentation, addressing critical limitations of existing\naugmentation techniques and offering a new paradigm shift for leveraging XAI\ntechniques to enhance AI model training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91caAI\uff08XAI\uff09\u7684\u60c5\u5883\u611f\u77e5\u6570\u636e\u589e\u5f3a\u6846\u67b6XAI-Guided Context-Aware Data Augmentation\uff0c\u901a\u8fc7XAI\u6280\u672f\u9009\u62e9\u6027\u4fee\u6539\u4e0d\u5173\u952e\u7279\u5f81\u5e76\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u662fAI\u6a21\u578b\u5f00\u53d1\u7684\u4e3b\u8981\u6311\u6218\uff0c\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u6280\u672f\u5b58\u5728\u566a\u58f0\u3001\u8bed\u4e49\u6f02\u79fb\u3001\u7f3a\u4e4f\u63a7\u5236\u7b49\u95ee\u9898\uff0cXAI\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u63d0\u51faXAI-Guided Context-Aware Data Augmentation\u6846\u67b6\uff0c\u7ed3\u5408XAI\u6280\u672f\u548c\u8fed\u4ee3\u53cd\u9988\u5faa\u73af\uff0c\u4f18\u5316\u589e\u5f3a\u6570\u636e\u7684\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cXAI-SR-BT\u548cXAI-PR-BT\u5728\u4ec7\u6068\u8a00\u8bba\u548c\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u5206\u522b\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u53476.6%\u548c8.1%\uff0c\u4e14\u4f18\u4e8e\u4f20\u7edf\u589e\u5f3a\u6280\u672f4.8%\u548c5%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u66f4\u53ef\u63a7\u3001\u53ef\u89e3\u91ca\u4e14\u60c5\u5883\u611f\u77e5\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f00\u542f\u4e86\u5229\u7528XAI\u6280\u672f\u589e\u5f3aAI\u6a21\u578b\u8bad\u7ec3\u7684\u65b0\u8303\u5f0f\u3002", "keywords": "\u53ef\u89e3\u91caAI, \u6570\u636e\u589e\u5f3a, \u60c5\u5883\u611f\u77e5, \u6a21\u578b\u6027\u80fd, \u4f4e\u8d44\u6e90\u8bed\u8a00"}}
{"id": "2506.03939", "pdf": "https://arxiv.org/pdf/2506.03939", "abs": "https://arxiv.org/abs/2506.03939", "authors": ["Junqi Gao", "Xiang Zou", "YIng Ai", "Dong Li", "Yichen Niu", "Biqing Qi", "Jianxing Liu"], "title": "Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external\nknowledge integration capabilities by explicitly modeling knowledge\nrelationships, thereby improving the factual accuracy and generation quality of\nLarge Language Models (LLMs) in specialized domains. However, existing methods\nsuffer from two inherent limitations: 1) Inefficient Information Aggregation:\nThey rely on a single agent and fixed iterative patterns, making it difficult\nto adaptively capture multi-level textual, structural, and degree information\nwithin graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning\nschemes, which cannot dynamically adjust reasoning depth nor achieve precise\nsemantic correction. To overcome these limitations, we propose Graph Counselor,\nan GraphRAG method based on multi-agent collaboration. This method uses the\nAdaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,\nand Execution Agents work together to precisely model complex graph structures\nand dynamically adjust information extraction strategies, addressing the\nchallenges of multi-level dependency modeling and adaptive reasoning depth.\nAdditionally, the Self-Reflection with Multiple Perspectives (SR) module\nimproves the accuracy and semantic consistency of reasoning results through\nself-reflection and backward reasoning mechanisms. Experiments demonstrate that\nGraph Counselor outperforms existing methods in multiple graph reasoning tasks,\nexhibiting higher reasoning accuracy and generalization ability. Our code is\navailable at https://github.com/gjq100/Graph-Counselor.git.", "AI": {"tldr": "Graph Counselor\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684GraphRAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u56fe\u4fe1\u606f\u63d0\u53d6\u6a21\u5757\u548c\u591a\u89c6\u89d2\u81ea\u53cd\u601d\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fe1\u606f\u805a\u5408\u548c\u63a8\u7406\u673a\u5236\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709GraphRAG\u65b9\u6cd5\u5728\u77e5\u8bc6\u5173\u7cfb\u5efa\u6a21\u4e2d\u5b58\u5728\u4fe1\u606f\u805a\u5408\u6548\u7387\u4f4e\u548c\u63a8\u7406\u673a\u5236\u56fa\u5b9a\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u7ea7\u56fe\u6570\u636e\u7ed3\u6784\u548c\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u3002", "method": "\u63d0\u51faGraph Counselor\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u56fe\u4fe1\u606f\u63d0\u53d6\u6a21\u5757\uff08\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff09\u548c\u591a\u89c6\u89d2\u81ea\u53cd\u601d\u673a\u5236\uff08SR\u6a21\u5757\uff09\uff0c\u52a8\u6001\u8c03\u6574\u4fe1\u606f\u63d0\u53d6\u7b56\u7565\u548c\u63a8\u7406\u6df1\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGraph Counselor\u5728\u591a\u9879\u56fe\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u63a8\u7406\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Graph Counselor\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u81ea\u53cd\u601d\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86GraphRAG\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u590d\u6742\u56fe\u6570\u636e\u7684\u9ad8\u6548\u5efa\u6a21\u548c\u63a8\u7406\u3002", "keywords": "GraphRAG, \u591a\u667a\u80fd\u4f53\u534f\u4f5c, \u81ea\u9002\u5e94\u63a8\u7406, \u81ea\u53cd\u601d\u673a\u5236"}}
{"id": "2506.03230", "pdf": "https://arxiv.org/pdf/2506.03230", "abs": "https://arxiv.org/abs/2506.03230", "authors": ["Selcuk Gurses", "Aozhong Zhang", "Yanxia Deng", "Xun Dong", "Xin Li", "Naigang Wang", "Penghang Yin", "Zi Yang"], "title": "DiaBlo: Diagonal Blocks Are Sufficient For Finetuning", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC"], "comment": null, "summary": "Finetuning is a critical step for adapting large language models (LLMs) to\ndomain-specific downstream tasks. To mitigate the substantial computational and\nmemory costs of full-model fine-tuning, Parameter-Efficient Finetuning (PEFT)\nmethods have been proposed to update only a small subset of model parameters.\nHowever, performance gaps between PEFT approaches and full-model fine-tuning\nstill exist. In this work, we present DiaBlo, a simple yet effective PEFT\napproach that updates only the diagonal blocks of selected model weight\nmatrices. Unlike Low Rank Adaptation (LoRA) and its variants, DiaBlo eliminates\nthe need for low rank matrix products, thereby avoiding the reliance on\nauxiliary initialization schemes or customized optimization strategies to\nimprove convergence. This design leads to stable and robust convergence while\nmaintaining comparable memory efficiency and training speed to LoRA. We conduct\nextensive experiments across a range of tasks, including commonsense reasoning,\narithmetic reasoning, code generation, and safety alignment, to evaluate the\neffectiveness and efficiency of DiaBlo. Across these benchmarks, DiaBlo\ndemonstrates strong and consistent performance while maintaining high memory\nefficiency and fast finetuning speed. Codes are available at\nhttps://github.com/ziyangjoy/DiaBlo.", "AI": {"tldr": "DiaBlo\u662f\u4e00\u79cd\u9ad8\u6548\u53c2\u6570\u5fae\u8c03\u65b9\u6cd5\uff0c\u4ec5\u66f4\u65b0\u6a21\u578b\u6743\u91cd\u77e9\u9635\u7684\u5bf9\u89d2\u5757\uff0c\u907f\u514d\u4e86\u4f4e\u79e9\u77e9\u9635\u4e58\u79ef\uff0c\u8868\u73b0\u7a33\u5b9a\u4e14\u9ad8\u6548\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u4e0e\u5168\u6a21\u578b\u5fae\u8c03\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u3002", "method": "\u4ec5\u66f4\u65b0\u6743\u91cd\u77e9\u9635\u7684\u5bf9\u89d2\u5757\uff0c\u65e0\u9700\u4f4e\u79e9\u77e9\u9635\u4e58\u79ef\u6216\u5b9a\u5236\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u591a\u9879\u4efb\u52a1\uff08\u5982\u5e38\u8bc6\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u7b49\uff09\u4e2d\u8868\u73b0\u7a33\u5b9a\u4e14\u9ad8\u6548\u3002", "conclusion": "DiaBlo\u5728\u4fdd\u6301\u9ad8\u5185\u5b58\u6548\u7387\u548c\u5feb\u901f\u5fae\u8c03\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u5168\u6a21\u578b\u5fae\u8c03\u76f8\u8fd1\u7684\u6027\u80fd\u3002", "keywords": "\u53c2\u6570\u9ad8\u6548\u5fae\u8c03, LLM, \u5bf9\u89d2\u5757, \u4f4e\u79e9\u9002\u5e94"}}
{"id": "2506.03489", "pdf": "https://arxiv.org/pdf/2506.03489", "abs": "https://arxiv.org/abs/2506.03489", "authors": ["Mingxu Tao", "Jie Hu", "Mingchuan Yang", "Yunhuai Liu", "Dongyan Zhao", "Yansong Feng"], "title": "EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "The remarkable performance of Large language models (LLMs) relies heavily on\nthe availability of abundant high-quality training data. However, the high cost\nof acquiring annotated data often prevents models from obtaining capabilities\nto tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe\nthat boosts model performance in data-scarcity scenarios without extra\ntraining. We first employ model extrapolation to enhance a finetuned model with\nits inferior version, and then adopt contrastive decoding to further reduce\npredicted errors, by comparing the logit scores given by the extrapolated and\nthe vanilla finetuned model. Experiments across three tasks over four different\nLLMs show that EpiCoDe consistently outperforms existing methods with\nsignificant and robust improvement. We also propose a new theoretical framework\nto reveal the mechanism behind contrastive decoding in data-scarcity scenarios,\nwhich further helps us better understand the effectiveness of EpiCoDe.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEpiCoDe\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u5916\u63a8\u548c\u5bf9\u6bd4\u89e3\u7801\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u4f9d\u8d56\u4e8e\u5927\u91cf\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "EpiCoDe\u7ed3\u5408\u6a21\u578b\u5916\u63a8\u548c\u5bf9\u6bd4\u89e3\u7801\uff0c\u901a\u8fc7\u6bd4\u8f83\u5916\u63a8\u6a21\u578b\u548c\u539f\u59cb\u5fae\u8c03\u6a21\u578b\u7684\u903b\u8f91\u5206\u6570\uff0c\u51cf\u5c11\u9884\u6d4b\u9519\u8bef\u3002", "result": "\u5728\u4e09\u9879\u4efb\u52a1\u548c\u56db\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEpiCoDe\u663e\u8457\u4e14\u7a33\u5b9a\u5730\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EpiCoDe\u4e3a\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u63ed\u793a\u4e86\u5bf9\u6bd4\u89e3\u7801\u7684\u673a\u5236\u3002", "keywords": ""}}
{"id": "2506.03997", "pdf": "https://arxiv.org/pdf/2506.03997", "abs": "https://arxiv.org/abs/2506.03997", "authors": ["Mario Alviano", "Laura Giordano", "Daniele Theseider Dupr\u00e9"], "title": "A framework for Conditional Reasoning in Answer Set Programming", "categories": ["cs.AI", "cs.LO", "I.2.4"], "comment": "19 pages", "summary": "In this paper we introduce a Conditional Answer Set Programming framework\n(Conditional ASP) for the definition of conditional extensions of Answer Set\nProgramming (ASP). The approach builds on a conditional logic with typicality,\nand on the combination of a conditional knowledge base with an ASP program, and\nallows for conditional reasoning over the answer sets of the program. The\nformalism relies on a multi-preferential semantics (and on the KLM preferential\nsemantics, as a special case) to provide an interpretation of conditionals.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6761\u4ef6ASP\u6846\u67b6\uff0c\u7528\u4e8e\u6269\u5c55ASP\uff0c\u652f\u6301\u5728\u7b54\u6848\u96c6\u4e0a\u6761\u4ef6\u63a8\u7406\u3002", "motivation": "\u4e3a\u4e86\u5b9a\u4e49ASP\u7684\u6761\u4ef6\u6269\u5c55\uff0c\u4f7f\u5f97\u80fd\u591f\u5728\u7b54\u6848\u96c6\u4e0a\u8fdb\u884c\u6761\u4ef6\u63a8\u7406\u3002", "method": "\u57fa\u4e8e\u6761\u4ef6\u903b\u8f91\u548c\u5178\u578b\u6027\uff0c\u5c06\u6761\u4ef6\u77e5\u8bc6\u5e93\u4e0eASP\u7a0b\u5e8f\u7ed3\u5408\uff0c\u91c7\u7528\u591a\u504f\u597d\u8bed\u4e49\u89e3\u91ca\u6761\u4ef6\u3002", "result": "\u901a\u8fc7\u591a\u504f\u597d\u8bed\u4e49\uff08\u5305\u62ecKLM\u504f\u597d\u8bed\u4e49\uff09\u5b9e\u73b0\u6761\u4ef6\u89e3\u91ca\u3002", "conclusion": "Conditional ASP\u6846\u67b6\u4e3aASP\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u6761\u4ef6\u63a8\u7406\u80fd\u529b\u3002", "keywords": "Conditional ASP, Answer Set Programming, \u591a\u504f\u597d\u8bed\u4e49"}}
{"id": "2506.03234", "pdf": "https://arxiv.org/pdf/2506.03234", "abs": "https://arxiv.org/abs/2506.03234", "authors": ["Kaiwen Duan", "Hongwei Yao", "Yufei Chen", "Ziyun Li", "Tong Qiao", "Zhan Qin", "Cong Wang"], "title": "BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning\ntext-to-image (T2I) models with human preferences. However, RLHF's feedback\nmechanism also opens new pathways for adversaries. This paper demonstrates the\nfeasibility of hijacking T2I models by poisoning a small fraction of preference\ndata with natural-appearing examples. Specifically, we propose BadReward, a\nstealthy clean-label poisoning attack targeting the reward model in multi-modal\nRLHF. BadReward operates by inducing feature collisions between visually\ncontradicted preference data instances, thereby corrupting the reward model and\nindirectly compromising the T2I model's integrity. Unlike existing alignment\npoisoning techniques focused on single (text) modality, BadReward is\nindependent of the preference annotation process, enhancing its stealth and\npractical threat. Extensive experiments on popular T2I models show that\nBadReward can consistently guide the generation towards improper outputs, such\nas biased or violent imagery, for targeted concepts. Our findings underscore\nthe amplified threat landscape for RLHF in multi-modal systems, highlighting\nthe urgent need for robust defenses. Disclaimer. This paper contains uncensored\ntoxic content that might be offensive or disturbing to the readers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBadReward\u7684\u9690\u853d\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6c61\u67d3\u591a\u6a21\u6001RLHF\u4e2d\u7684\u5956\u52b1\u6a21\u578b\u6765\u64cd\u63a7\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u4f7f\u5176\u4ea7\u751f\u4e0d\u5f53\u5185\u5bb9\uff0c\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u7cfb\u7edf\u4e2dRLHF\u7684\u6f5c\u5728\u5a01\u80c1\u3002", "motivation": "\u7814\u7a76\u63ed\u793a\u4e86RLHF\u5728\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u6c61\u67d3\u504f\u597d\u6570\u636e\u6765\u64cd\u63a7\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u8f93\u51fa\u3002", "method": "\u63d0\u51faBadReward\u653b\u51fb\uff0c\u901a\u8fc7\u8bf1\u5bfc\u89c6\u89c9\u77db\u76fe\u7684\u7279\u5f81\u78b0\u649e\u6c61\u67d3\u5956\u52b1\u6a21\u578b\uff0c\u95f4\u63a5\u5f71\u54cd\u751f\u6210\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eBadReward\u80fd\u6709\u6548\u5f15\u5bfc\u76ee\u6807\u6982\u5ff5\u751f\u6210\u4e0d\u5f53\u56fe\u50cf\uff08\u5982\u504f\u89c1\u6216\u66b4\u529b\u5185\u5bb9\uff09\u3002", "conclusion": "\u7814\u7a76\u7a81\u663e\u4e86\u591a\u6a21\u6001RLHF\u7cfb\u7edf\u9762\u4e34\u7684\u65b0\u5a01\u80c1\uff0c\u547c\u5401\u52a0\u5f3a\u9632\u5fa1\u673a\u5236\u3002", "keywords": "RLHF, \u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b, \u6570\u636e\u6c61\u67d3, \u5bf9\u6297\u653b\u51fb, \u591a\u6a21\u6001\u7cfb\u7edf"}}
{"id": "2506.03490", "pdf": "https://arxiv.org/pdf/2506.03490", "abs": "https://arxiv.org/abs/2506.03490", "authors": ["Shigeng Chen", "Linhao Luo", "Zhangchi Qiu", "Yanan Cao", "Carl Yang", "Shirui Pan"], "title": "Beyond Memorization: A Rigorous Evaluation Framework for Medical Knowledge Editing", "categories": ["cs.CL"], "comment": null, "summary": "Recently, knowledge editing (KE) has emerged as a promising approach to\nupdate specific facts in Large Language Models (LLMs) without the need for full\nretraining. Despite the effectiveness in general-domain benchmarks, their\napplicability to complex medical domain remains largely unexplored. Medical\nknowledge editing is particularly challenging, as it requires LLMs to\ninternalize the knowledge and generalize to unseen scenarios for effective and\ninterpretable decision-making. In this work, we propose a novel framework\ncalled MedEditBench to rigorously evaluate the effectiveness of existing KE\nmethods in the medical domain. In MedEditBench, we introduce a new medical\nknowledge editing benchmark as well as three different knowledge editing\nparadigms, which are designed to assess the impact of different knowledge\nsources for editing. Our findings indicate that current KE methods result in\nonly superficial memorization of the injected information, failing to\ngeneralize to new scenarios. To overcome this limitation, we present\nSelf-Generated Rationale Editing (SGR-Edit), which utilizes model-derived\nrationales as the target knowledge for editing, thereby uncovering the\nunderlying reasoning process and demonstrating significant improvements over\nexisting KE approaches. Additionally, we offer deeper insights into medical\nknowledge editing, including the localization of medical knowledge in LLMs and\nthe impact of sequential editing on evolving knowledge. This could provide\npractical guidance for implementing KE methods in real-world medical\napplications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMedEditBench\u6846\u67b6\u8bc4\u4f30\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u533b\u5b66\u9886\u57df\u7684\u9002\u7528\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u4ec5\u5b9e\u73b0\u6d45\u5c42\u8bb0\u5fc6\uff0c\u672a\u80fd\u6cdb\u5316\u5230\u65b0\u573a\u666f\uff0c\u63d0\u51faSGR-Edit\u65b9\u6cd5\u6539\u8fdb\u3002", "motivation": "\u5c3d\u7ba1\u77e5\u8bc6\u7f16\u8f91\u5728\u901a\u7528\u9886\u57df\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u7684\u533b\u5b66\u9886\u57df\u5c1a\u672a\u5145\u5206\u9a8c\u8bc1\uff0c\u9700\u8981\u5185\u90e8\u5316\u77e5\u8bc6\u5e76\u6cdb\u5316\u5230\u672a\u89c1\u573a\u666f\u4ee5\u5b9e\u73b0\u6709\u6548\u51b3\u7b56\u3002", "method": "\u63d0\u51faMedEditBench\u6846\u67b6\uff0c\u5305\u542b\u65b0\u533b\u5b66\u77e5\u8bc6\u7f16\u8f91\u57fa\u51c6\u548c\u4e09\u79cd\u7f16\u8f91\u8303\u5f0f\uff1b\u63d0\u51faSGR-Edit\uff0c\u5229\u7528\u6a21\u578b\u751f\u6210\u7684\u7406\u7531\u4f5c\u4e3a\u7f16\u8f91\u76ee\u6807\u3002", "result": "\u73b0\u6709KE\u65b9\u6cd5\u4ec5\u5b9e\u73b0\u6d45\u5c42\u8bb0\u5fc6\uff0cSGR-Edit\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u533b\u5b66\u77e5\u8bc6\u5b9a\u4f4d\u548c\u987a\u5e8f\u7f16\u8f91\u5f71\u54cd\u7684\u6df1\u5165\u89c1\u89e3\u3002", "conclusion": "SGR-Edit\u901a\u8fc7\u63ed\u793a\u63a8\u7406\u8fc7\u7a0b\u6539\u8fdb\u533b\u5b66\u77e5\u8bc6\u7f16\u8f91\u6548\u679c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002", "keywords": "\u77e5\u8bc6\u7f16\u8f91\uff0c\u533b\u5b66\u9886\u57df\uff0cMedEditBench\uff0cSGR-Edit\uff0c\u6cdb\u5316\u80fd\u529b"}}
{"id": "2506.04018", "pdf": "https://arxiv.org/pdf/2506.04018", "abs": "https://arxiv.org/abs/2506.04018", "authors": ["Akshat Naik", "Patrick Quinn", "Guillermo Bosch", "Emma Goun\u00e9", "Francisco Javier Campos Zabala", "Jason Ross Brown", "Edward James Young"], "title": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "I.2.7; I.2.11; K.4.1; I.2.6"], "comment": "Prepint, under review for NeurIPS 2025", "summary": "As Large Language Model (LLM) agents become more widespread, associated\nmisalignment risks increase. Prior work has examined agents' ability to enact\nmisaligned behaviour (misalignment capability) and their compliance with\nharmful instructions (misuse propensity). However, the likelihood of agents\nattempting misaligned behaviours in real-world settings (misalignment\npropensity) remains poorly understood. We introduce a misalignment propensity\nbenchmark, AgentMisalignment, consisting of a suite of realistic scenarios in\nwhich LLM agents have the opportunity to display misaligned behaviour. We\norganise our evaluations into subcategories of misaligned behaviours, including\ngoal-guarding, resisting shutdown, sandbagging, and power-seeking. We report\nthe performance of frontier models on our benchmark, observing higher\nmisalignment on average when evaluating more capable models. Finally, we\nsystematically vary agent personalities through different system prompts. We\nfind that persona characteristics can dramatically and unpredictably influence\nmisalignment tendencies -- occasionally far more than the choice of model\nitself -- highlighting the importance of careful system prompt engineering for\ndeployed AI agents. Our work highlights the failure of current alignment\nmethods to generalise to LLM agents, and underscores the need for further\npropensity evaluations as autonomous systems become more prevalent.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u57fa\u51c6AgentMisalignment\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u4e0d\u5bf9\u9f50\u503e\u5411\uff08misalignment propensity\uff09\uff0c\u53d1\u73b0\u66f4\u5f3a\u6a21\u578b\u7684\u4e0d\u5bf9\u9f50\u884c\u4e3a\u66f4\u591a\uff0c\u4e14\u7cfb\u7edf\u63d0\u793a\uff08system prompt\uff09\u5bf9\u884c\u4e3a\u5f71\u54cd\u663e\u8457\u3002", "motivation": "\u7814\u7a76LLM\u4ee3\u7406\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u5c1d\u8bd5\u4e0d\u5bf9\u9f50\u884c\u4e3a\u7684\u53ef\u80fd\u6027\uff08\u4e0d\u5bf9\u9f50\u503e\u5411\uff09\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u5728misalignment propensity\u65b9\u9762\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u57fa\u51c6AgentMisalignment\uff0c\u5305\u542b\u591a\u79cd\u73b0\u5b9e\u573a\u666f\uff0c\u8bc4\u4f30\u4ee3\u7406\u5728\u76ee\u6807\u4fdd\u62a4\u3001\u62b5\u6297\u5173\u95ed\u3001\u6d88\u6781\u6020\u5de5\u548c\u6743\u529b\u5bfb\u6c42\u7b49\u4e0d\u5bf9\u9f50\u884c\u4e3a\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u6d4b\u8bd5\u4e0d\u540c\u6a21\u578b\u548c\u7cfb\u7edf\u63d0\u793a\u7684\u5f71\u54cd\u3002", "result": "\u66f4\u5f3a\u6a21\u578b\u7684\u4e0d\u5bf9\u9f50\u884c\u4e3a\u66f4\u666e\u904d\uff0c\u7cfb\u7edf\u63d0\u793a\u7684\u53d8\u5316\u53ef\u663e\u8457\u4e14\u4e0d\u53ef\u9884\u6d4b\u5730\u6539\u53d8\u4ee3\u7406\u884c\u4e3a\uff0c\u6709\u65f6\u5f71\u54cd\u751a\u81f3\u8d85\u8fc7\u6a21\u578b\u9009\u62e9\u672c\u8eab\u3002", "conclusion": "\u5f53\u524d\u7684\u5bf9\u9f50\u65b9\u6cd5\u65e0\u6cd5\u6cdb\u5316\u5230LLM\u4ee3\u7406\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4e0d\u5bf9\u9f50\u503e\u5411\u8bc4\u4f30\uff0c\u5c24\u5176\u662f\u7cfb\u7edf\u63d0\u793a\u7684\u8bbe\u8ba1\u5bf9\u4ee3\u7406\u884c\u4e3a\u7684\u5173\u952e\u4f5c\u7528\u3002", "keywords": "LLM\u4ee3\u7406, \u4e0d\u5bf9\u9f50\u503e\u5411, AgentMisalignment\u57fa\u51c6, \u7cfb\u7edf\u63d0\u793a, \u5bf9\u9f50\u65b9\u6cd5"}}
{"id": "2506.03267", "pdf": "https://arxiv.org/pdf/2506.03267", "abs": "https://arxiv.org/abs/2506.03267", "authors": ["Shahbaz Rezaei", "Avishai Halev", "Xin Liu"], "title": "On the Necessity of Multi-Domain Explanation: An Uncertainty Principle Approach for Deep Time Series Models", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "A prevailing approach to explain time series models is to generate\nattribution in time domain. A recent development in time series XAI is the\nconcept of explanation spaces, where any model trained in the time domain can\nbe interpreted with any existing XAI method in alternative domains, such as\nfrequency. The prevailing approach is to present XAI attributions either in the\ntime domain or in the domain where the attribution is most sparse. In this\npaper, we demonstrate that in certain cases, XAI methods can generate\nattributions that highlight fundamentally different features in the time and\nfrequency domains that are not direct counterparts of one another. This\nsuggests that both domains' attributions should be presented to achieve a more\ncomprehensive interpretation. Thus it shows the necessity of multi-domain\nexplanation. To quantify when such cases arise, we introduce the uncertainty\nprinciple (UP), originally developed in quantum mechanics and later studied in\nharmonic analysis and signal processing, to the XAI literature. This principle\nestablishes a lower bound on how much a signal can be simultaneously localized\nin both the time and frequency domains. By leveraging this concept, we assess\nwhether attributions in the time and frequency domains violate this bound,\nindicating that they emphasize distinct features. In other words, UP provides a\nsufficient condition that the time and frequency domain explanations do not\nmatch and, hence, should be both presented to the end user. We validate the\neffectiveness of this approach across various deep learning models, XAI\nmethods, and a wide range of classification and forecasting datasets. The\nfrequent occurrence of UP violations across various datasets and XAI methods\nhighlights the limitations of existing approaches that focus solely on\ntime-domain explanations. This underscores the need for multi-domain\nexplanations as a new paradigm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5728\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u89e3\u91ca\u4e2d\uff0c\u65f6\u95f4\u57df\u548c\u9891\u7387\u57df\u7684\u5c5e\u6027\u53ef\u80fd\u5f3a\u8c03\u4e0d\u540c\u7684\u7279\u5f81\uff0c\u9700\u540c\u65f6\u5c55\u793a\u4e24\u57df\u89e3\u91ca\u4ee5\u83b7\u5f97\u5168\u9762\u7406\u89e3\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u5173\u6ce8\u65f6\u95f4\u57df\u89e3\u91ca\uff0c\u800c\u7814\u7a76\u8868\u660e\u65f6\u95f4\u57df\u548c\u9891\u7387\u57df\u7684\u5c5e\u6027\u53ef\u80fd\u5e76\u4e0d\u4e00\u81f4\uff0c\u9700\u591a\u57df\u89e3\u91ca\u4ee5\u5168\u9762\u7406\u89e3\u6a21\u578b\u884c\u4e3a\u3002", "method": "\u5f15\u5165\u91cf\u5b50\u529b\u5b66\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u539f\u7406\uff08UP\uff09\u4f5c\u4e3a\u91cf\u5316\u5de5\u5177\uff0c\u5224\u65ad\u4e24\u57df\u5c5e\u6027\u662f\u5426\u8fdd\u53cdUP\u8fb9\u754c\uff0c\u4ece\u800c\u5f3a\u8c03\u591a\u57df\u89e3\u91ca\u7684\u5fc5\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u5728\u4e0d\u540c\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\uff0cUP\u8fdd\u89c4\u9891\u7e41\u53d1\u751f\uff0c\u8868\u660e\u4ec5\u5173\u6ce8\u65f6\u95f4\u57df\u89e3\u91ca\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "\u591a\u57df\u89e3\u91ca\u5e94\u4f5c\u4e3a\u65b0\u8303\u5f0f\uff0c\u4ee5\u5f25\u8865\u4f20\u7edf\u5355\u57df\u89e3\u91ca\u7684\u4e0d\u8db3\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\u3001\u53ef\u89e3\u91caAI\u3001\u4e0d\u786e\u5b9a\u6027\u539f\u7406\u3001\u591a\u57df\u89e3\u91ca"}}
{"id": "2506.03501", "pdf": "https://arxiv.org/pdf/2506.03501", "abs": "https://arxiv.org/abs/2506.03501", "authors": ["Yuchen Guo", "Zhicheng Dou", "Huy H. Nguyen", "Ching-Chun Chang", "Saku Sugawara", "Isao Echizen"], "title": "Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing", "categories": ["cs.CL", "cs.AI"], "comment": "IJCNN2025 accepted", "summary": "Content creation has dramatically progressed with the rapid advancement of\nlarge language models like ChatGPT and Claude. While this progress has greatly\nenhanced various aspects of life and work, it has also negatively affected\ncertain areas of society. A recent survey revealed that nearly 30% of college\nstudents use generative AI to help write academic papers and reports. Most\ncountermeasures treat the detection of AI-generated text as a binary\nclassification task and thus lack robustness. This approach overlooks human\ninvolvement in the generation of content even though human-machine\ncollaboration is becoming mainstream. Besides generating entire texts, people\nmay use machines to complete or revise texts. Such human involvement varies\ncase by case, which makes binary classification a less than satisfactory\napproach. We refer to this situation as participation detection obfuscation. We\npropose using BERTScore as a metric to measure human involvement in the\ngeneration process and a multi-task RoBERTa-based regressor trained on a token\nclassification task to address this problem. To evaluate the effectiveness of\nthis approach, we simulated academic-based scenarios and created a continuous\ndataset reflecting various levels of human involvement. All of the existing\ndetectors we examined failed to detect the level of human involvement on this\ndataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor\nmean squared error of 0.004). Moreover, it demonstrated some generalizability\nacross generative models. Our code is available at\nhttps://github.com/gyc-nii/CAS-CS-and-dual-head-detector", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBERTScore\u548c\u591a\u4efb\u52a1RoBERTa\u56de\u5f52\u5668\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u6587\u672c\u4e2d\u4eba\u7c7b\u53c2\u4e0e\u7684\u7a0b\u5ea6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4e8c\u8fdb\u5236\u5206\u7c7b\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0cAI\u751f\u6210\u6587\u672c\u7684\u4f7f\u7528\u589e\u52a0\uff0c\u4f46\u73b0\u6709\u7684\u68c0\u6d4b\u65b9\u6cd5\u591a\u4e3a\u4e8c\u8fdb\u5236\u5206\u7c7b\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u4eba\u7c7b\u53c2\u4e0e\u7684\u591a\u6837\u6027\u3002", "method": "\u4f7f\u7528BERTScore\u4f5c\u4e3a\u6307\u6807\uff0c\u7ed3\u5408\u591a\u4efb\u52a1RoBERTa\u56de\u5f52\u5668\uff0c\u901a\u8fc7\u6807\u8bb0\u5206\u7c7b\u4efb\u52a1\u8bad\u7ec3\uff0c\u68c0\u6d4b\u6587\u672c\u4e2d\u4eba\u7c7b\u53c2\u4e0e\u7684\u7a0b\u5ea6\u3002", "result": "\u5728\u6a21\u62df\u5b66\u672f\u573a\u666f\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u8868\u73b0\u51fa\u8272\uff08F1\u5206\u65700.9423\uff0c\u56de\u5f52\u5668\u5747\u65b9\u8bef\u5dee0.004\uff09\uff0c\u4e14\u5177\u6709\u4e00\u5b9a\u7684\u8de8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u4eba\u7c7b\u53c2\u4e0e\u7a0b\u5ea6\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u4e8c\u8fdb\u5236\u5206\u7c7b\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u9002\u7528\u4e8e\u4eba\u7c7b\u4e0eAI\u534f\u4f5c\u751f\u6210\u7684\u6587\u672c\u3002", "keywords": "\u751f\u6210\u5f0fAI, \u68c0\u6d4b\u65b9\u6cd5, \u4eba\u7c7b\u53c2\u4e0e, BERTScore, RoBERTa"}}
{"id": "2506.04022", "pdf": "https://arxiv.org/pdf/2506.04022", "abs": "https://arxiv.org/abs/2506.04022", "authors": ["Qiyue Xia", "J. Michael Herrmann"], "title": "Interpretability by Design for Efficient Multi-Objective Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Multi-objective reinforcement learning (MORL) aims at optimising several,\noften conflicting goals in order to improve flexibility and reliability of RL\nin practical tasks. This can be achieved by finding diverse policies that are\noptimal for some objective preferences and non-dominated by optimal policies\nfor other preferences so that they form a Pareto front in the multi-objective\nperformance space. The relation between the multi-objective performance space\nand the parameter space that represents the policies is generally non-unique.\nUsing a training scheme that is based on a locally linear map between the\nparameter space and the performance space, we show that an approximate Pareto\nfront can provide an interpretation of the current parameter vectors in terms\nof the objectives which enables an effective search within contiguous solution\ndomains. Experiments are conducted with and without retraining across different\ndomains, and the comparison with previous methods demonstrates the efficiency\nof our approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u7ebf\u6027\u6620\u5c04\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd1\u4f3c\u5e15\u7d2f\u6258\u524d\u6cbf\u5b9e\u73b0\u9ad8\u6548\u641c\u7d22\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff08MORL\uff09\u65e8\u5728\u4f18\u5316\u591a\u4e2a\u76ee\u6807\u4ee5\u63d0\u9ad8\u7075\u6d3b\u6027\uff0c\u4f46\u53c2\u6570\u7a7a\u95f4\u4e0e\u76ee\u6807\u6027\u80fd\u7a7a\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u4e0d\u552f\u4e00\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5c40\u90e8\u7ebf\u6027\u6620\u5c04\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u5229\u7528\u8fd1\u4f3c\u5e15\u7d2f\u6258\u524d\u6cbf\u89e3\u91ca\u53c2\u6570\u5411\u91cf\u4e0e\u76ee\u6807\u7684\u5173\u7cfb\uff0c\u5b9e\u73b0\u8fde\u7eed\u89e3\u7a7a\u95f4\u7684\u9ad8\u6548\u641c\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u9886\u57df\uff08\u5305\u62ec\u662f\u5426\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff09\u5747\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4e2d\u53c2\u6570\u7a7a\u95f4\u4e0e\u6027\u80fd\u7a7a\u95f4\u6620\u5c04\u975e\u552f\u4e00\u6027\u95ee\u9898\uff0c\u63d0\u5347\u641c\u7d22\u6548\u7387\u3002", "keywords": "\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60, \u5e15\u7d2f\u6258\u524d\u6cbf, \u5c40\u90e8\u7ebf\u6027\u6620\u5c04, \u53c2\u6570\u7a7a\u95f4, \u6027\u80fd\u7a7a\u95f4"}}
{"id": "2506.03302", "pdf": "https://arxiv.org/pdf/2506.03302", "abs": "https://arxiv.org/abs/2506.03302", "authors": ["James Bagrow", "Josh Bongard"], "title": "Multi-Exit Kolmogorov-Arnold Networks: enhancing accuracy and parsimony", "categories": ["cs.LG", "cs.NE", "physics.data-an", "stat.ML"], "comment": "14 pages, 7 figures, 2 tables", "summary": "Kolmogorov-Arnold Networks (KANs) uniquely combine high accuracy with\ninterpretability, making them valuable for scientific modeling. However, it is\nunclear a priori how deep a network needs to be for any given task, and deeper\nKANs can be difficult to optimize. Here we introduce multi-exit KANs, where\neach layer includes its own prediction branch, enabling the network to make\naccurate predictions at multiple depths simultaneously. This architecture\nprovides deep supervision that improves training while discovering the right\nlevel of model complexity for each task. Multi-exit KANs consistently\noutperform standard, single-exit versions on synthetic functions, dynamical\nsystems, and real-world datasets. Remarkably, the best predictions often come\nfrom earlier, simpler exits, revealing that these networks naturally identify\nsmaller, more parsimonious and interpretable models without sacrificing\naccuracy. To automate this discovery, we develop a differentiable \"learning to\nexit\" algorithm that balances contributions from exits during training. Our\napproach offers scientists a practical way to achieve both high performance and\ninterpretability, addressing a fundamental challenge in machine learning for\nscientific discovery.", "AI": {"tldr": "\u591a\u51fa\u53e3Kolmogorov-Arnold\u7f51\u7edc\u901a\u8fc7\u6bcf\u5c42\u72ec\u7acb\u7684\u9884\u6d4b\u5206\u652f\u5b9e\u73b0\u591a\u6df1\u5ea6\u9884\u6d4b\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u679c\u5e76\u627e\u5230\u4efb\u52a1\u6700\u4f73\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4e3a\u89e3\u51b3KANs\u7f51\u7edc\u6df1\u5ea6\u96be\u4ee5\u786e\u5b9a\u548c\u4f18\u5316\u56f0\u96be\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4f18\u52bf\u3002", "method": "\u5f15\u5165\u591a\u51fa\u53e3KANs\uff0c\u6bcf\u5c42\u9644\u52a0\u9884\u6d4b\u5206\u652f\uff0c\u652f\u6301\u591a\u6df1\u5ea6\u9884\u6d4b\uff1b\u5f00\u53d1\u53ef\u5fae\u7684'\u5b66\u4e60\u9000\u51fa'\u7b97\u6cd5\u5e73\u8861\u51fa\u53e3\u8d21\u732e\u3002", "result": "\u591a\u51fa\u53e3KANs\u5728\u5408\u6210\u51fd\u6570\u3001\u52a8\u6001\u7cfb\u7edf\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u5747\u4f18\u4e8e\u5355\u51fa\u53e3\u7248\u672c\uff0c\u4e14\u8f83\u65e9\u51fa\u53e3\u5e38\u63d0\u4f9b\u66f4\u7b80\u6d01\u3001\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u3002", "conclusion": "\u591a\u51fa\u53e3KANs\u4e3a\u79d1\u5b66\u5efa\u6a21\u63d0\u4f9b\u9ad8\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u5b66\u4e60\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u6838\u5fc3\u6311\u6218\u3002", "keywords": "Kolmogorov-Arnold Networks, interpretability, multi-exit, deep supervision, scientific modeling"}}
{"id": "2506.03510", "pdf": "https://arxiv.org/pdf/2506.03510", "abs": "https://arxiv.org/abs/2506.03510", "authors": ["Seungcheol Park", "Sojin Lee", "Jongjin Kim", "Jinsik Lee", "Hyunjik Jo", "U Kang"], "title": "Accurate Sublayer Pruning for Large Language Models by Exploiting Latency and Tunability Information", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "IJCAI 2025 Main Track", "summary": "How can we accelerate large language models(LLMs) without sacrificing\naccuracy? The slow inference speed of LLMs hinders us to benefit from their\nremarkable performance in diverse applications. This is mainly because numerous\nsublayers are stacked together in LLMs. Sublayer pruning compresses and\nexpedites LLMs via removing unnecessary sublayers. However, existing sublayer\npruning algorithms are limited in accuracy since they naively select sublayers\nto prune, overlooking the different characteristics of each sublayer. In this\npaper, we propose SPRINT (Sublayer PRuning wIth LateNcy and Tunability\nInformation), an accurate sublayer pruning method for LLMs. SPRINT accurately\nselects a target sublayer to prune by considering 1) the amount of latency\nreduction after pruning and 2) the tunability of sublayers. SPRINT iteratively\nprunes redundant sublayers and swiftly tunes the parameters of remaining\nsublayers. Experiments show that SPRINT achieves the best accuracy-speedup\ntrade-off, exhibiting up to 23.88%p higher accuracy on zero-shot commonsense\nreasoning benchmarks compared to existing pruning algorithms.", "AI": {"tldr": "SPRINT\u662f\u4e00\u79cd\u901a\u8fc7\u8003\u8651\u5ef6\u8fdf\u51cf\u5c11\u548c\u5b50\u5c42\u53ef\u8c03\u6027\u6765\u7cbe\u786e\u4fee\u526aLLM\u5b50\u5c42\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u63a8\u7406\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u63a8\u7406\u901f\u5ea6\u6162\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\uff0c\u73b0\u6709\u5b50\u5c42\u4fee\u526a\u65b9\u6cd5\u56e0\u5ffd\u89c6\u5b50\u5c42\u7279\u6027\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "SPRINT\u7ed3\u5408\u5ef6\u8fdf\u51cf\u5c11\u91cf\u548c\u5b50\u5c42\u53ef\u8c03\u6027\u4fe1\u606f\uff0c\u8fed\u4ee3\u4fee\u526a\u5197\u4f59\u5b50\u5c42\u5e76\u5feb\u901f\u8c03\u6574\u5269\u4f59\u5b50\u5c42\u53c2\u6570\u3002", "result": "SPRINT\u5728\u96f6\u6837\u672c\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u7cbe\u5ea6\u63d0\u9ad823.88%\uff0c\u5b9e\u73b0\u6700\u4f73\u7cbe\u5ea6-\u52a0\u901f\u6bd4\u3002", "conclusion": "SPRINT\u662f\u4e00\u79cd\u9ad8\u6548\u7684LLM\u5b50\u5c42\u4fee\u526a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u548c\u7cbe\u5ea6\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u5b50\u5c42\u4fee\u526a\u3001\u63a8\u7406\u52a0\u901f\u3001SPRINT"}}
{"id": "2506.04133", "pdf": "https://arxiv.org/pdf/2506.04133", "abs": "https://arxiv.org/abs/2506.04133", "authors": ["Shaina Raza", "Ranjan Sapkota", "Manoj Karkee", "Christos Emmanouilidis"], "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems", "categories": ["cs.AI"], "comment": null, "summary": "Agentic AI systems, built on large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligent autonomy, collaboration\nand decision-making across enterprise and societal domains. This review\npresents a structured analysis of Trust, Risk, and Security Management (TRiSM)\nin the context of LLM-based agentic multi-agent systems (AMAS). We begin by\nexamining the conceptual foundations of agentic AI, its architectural\ndifferences from traditional AI agents, and the emerging system designs that\nenable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is\nthen detailed through four pillars governance, explainability, ModelOps, and\nprivacy/security each contextualized for agentic LLMs. We identify unique\nthreat vectors and introduce a comprehensive risk taxonomy for the agentic AI\napplications, supported by case studies illustrating real-world\nvulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,\ntransparency and oversight techniques, and state-of-the-art explainability\nstrategies in distributed LLM agent systems. Additionally, metrics for\nevaluating trust, interpretability, and human-centered performance are reviewed\nalongside open benchmarking challenges. Security and privacy are addressed\nthrough encryption, adversarial defense, and compliance with evolving AI\nregulations. The paper concludes with a roadmap for responsible agentic AI,\nproposing research directions to align emerging multi-agent systems with robust\nTRiSM principles for safe, accountable, and transparent deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u5f0f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08AMAS\uff09\u4e2d\u7684\u4fe1\u4efb\u3001\u98ce\u9669\u4e0e\u5b89\u5168\u7ba1\u7406\uff08TRiSM\uff09\uff0c\u5206\u6790\u4e86\u5176\u67b6\u6784\u3001\u5a01\u80c1\u5411\u91cf\u53ca\u5e94\u5bf9\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u63a2\u8ba8\u4ee3\u7406\u5f0fAI\u7cfb\u7edf\u5728\u591a\u9886\u57df\u4e2d\u7684\u667a\u80fd\u81ea\u4e3b\u6027\u548c\u534f\u4f5c\u80fd\u529b\uff0c\u4ee5\u53ca\u5728TRiSM\u6846\u67b6\u4e0b\u7684\u72ec\u7279\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6982\u5ff5\u5206\u6790\u3001\u6848\u4f8b\u7814\u7a76\u548c\u7efc\u5408\u8bc4\u4f30\uff0c\u8be6\u7ec6\u9610\u8ff0\u4e86\u4ee3\u7406\u5f0fAI\u7684\u56db\u5927\u652f\u67f1\uff08\u6cbb\u7406\u3001\u53ef\u89e3\u91ca\u6027\u3001ModelOps\u548c\u9690\u79c1/\u5b89\u5168\uff09\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u5957\u5168\u9762\u7684\u98ce\u9669\u5206\u7c7b\u6cd5\u548c\u4fe1\u4efb\u5efa\u7acb\u673a\u5236\uff0c\u5e76\u603b\u7ed3\u4e86\u6700\u65b0\u7684\u5b89\u5168\u6280\u672f\u548c\u900f\u660e\u5ea6\u7b56\u7565\u3002", "conclusion": "\u8bba\u6587\u4e3a\u672a\u6765\u4ee3\u7406\u5f0fAI\u7cfb\u7edf\u7684\u8d1f\u8d23\u4efb\u90e8\u7f72\u63d0\u4f9b\u4e86\u7814\u7a76\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u4e86\u5b89\u5168\u3001\u900f\u660e\u548c\u53ef\u95ee\u8d23\u7684\u91cd\u8981\u6027\u3002", "keywords": "\u4ee3\u7406\u5f0fAI, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u591a\u667a\u80fd\u4f53\u7cfb\u7edf, \u4fe1\u4efb\u7ba1\u7406, \u98ce\u9669\u7ba1\u7406, \u5b89\u5168\u7ba1\u7406"}}
{"id": "2506.03307", "pdf": "https://arxiv.org/pdf/2506.03307", "abs": "https://arxiv.org/abs/2506.03307", "authors": ["Kristen Goebel", "William Solow", "Paola Pesantez-Cabrera", "Markus Keller", "Alan Fern"], "title": "Budgeted Online Active Learning with Expert Advice and Episodic Priors", "categories": ["cs.LG"], "comment": null, "summary": "This paper introduces a novel approach to budgeted online active learning\nfrom finite-horizon data streams with extremely limited labeling budgets. In\nagricultural applications, such streams might include daily weather data over a\ngrowing season, and labels require costly measurements of weather-dependent\nplant characteristics. Our method integrates two key sources of prior\ninformation: a collection of preexisting expert predictors and episodic\nbehavioral knowledge of the experts based on unlabeled data streams. Unlike\nprevious research on online active learning with experts, our work\nsimultaneously considers query budgets, finite horizons, and episodic\nknowledge, enabling effective learning in applications with severely limited\nlabeling capacity. We demonstrate the utility of our approach through\nexperiments on various prediction problems derived from both a realistic\nagricultural crop simulator and real-world data from multiple grape cultivars.\nThe results show that our method significantly outperforms baseline expert\npredictions, uniform query selection, and existing approaches that consider\nbudgets and limited horizons but neglect episodic knowledge, even under highly\nconstrained labeling budgets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9884\u7b97\u5728\u7ebf\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u9488\u5bf9\u6709\u9650\u65f6\u57df\u6570\u636e\u6d41\u548c\u6781\u4f4e\u6807\u7b7e\u9884\u7b97\u573a\u666f\u3002", "motivation": "\u5728\u519c\u4e1a\u5e94\u7528\u4e2d\uff0c\u6570\u636e\u6d41\uff08\u5982\u6bcf\u65e5\u5929\u6c14\uff09\u9700\u8981\u6602\u8d35\u7684\u6807\u7b7e\uff08\u5982\u690d\u7269\u7279\u6027\u6d4b\u91cf\uff09\uff0c\u800c\u9884\u7b97\u548c\u65f6\u95f4\u6709\u9650\u3002", "method": "\u65b9\u6cd5\u6574\u5408\u4e86\u4e24\u7c7b\u5148\u9a8c\u4fe1\u606f\uff1a\u4e13\u5bb6\u9884\u6d4b\u5668\u548c\u57fa\u4e8e\u65e0\u6807\u7b7e\u6570\u636e\u6d41\u7684\u884c\u4e3a\u77e5\u8bc6\uff0c\u540c\u65f6\u8003\u8651\u67e5\u8be2\u9884\u7b97\u3001\u6709\u9650\u65f6\u57df\u548c\u7247\u6bb5\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6781\u4f4e\u6807\u7b7e\u9884\u7b97\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u4e13\u5bb6\u9884\u6d4b\u3001\u7edf\u4e00\u67e5\u8be2\u9009\u62e9\u548c\u5ffd\u7565\u7247\u6bb5\u77e5\u8bc6\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6807\u7b7e\u9884\u7b97\u6781\u4f4e\u3001\u65f6\u95f4\u6709\u9650\u7684\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u519c\u4e1a\u7b49\u5b9e\u9645\u5e94\u7528\u3002", "keywords": "\u9884\u7b97\u5728\u7ebf\u4e3b\u52a8\u5b66\u4e60, \u6709\u9650\u65f6\u57df, \u519c\u4e1a\u6570\u636e, \u4e13\u5bb6\u77e5\u8bc6, \u6807\u7b7e\u9884\u7b97"}}
{"id": "2506.03519", "pdf": "https://arxiv.org/pdf/2506.03519", "abs": "https://arxiv.org/abs/2506.03519", "authors": ["Yangyang Zhao", "Ben Niu", "Libo Qin", "Shihan Wang"], "title": "An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals", "categories": ["cs.CL"], "comment": null, "summary": "Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue\nsystems to optimize dialogue policy, but it struggles to balance exploration\nand exploitation due to the high dimensionality of state and action spaces.\nThis challenge often results in local optima or poor convergence. Evolutionary\nAlgorithms (EAs) have been proven to effectively explore the solution space of\nneural networks by maintaining population diversity. Inspired by this, we\ninnovatively combine the global search capabilities of EA with the local\noptimization of DRL to achieve a balance between exploration and exploitation.\nNevertheless, the inherent flexibility of natural language in dialogue tasks\ncomplicates this direct integration, leading to prolonged evolutionary times.\nThus, we further propose an elite individual injection mechanism to enhance\nEA's search efficiency by adaptively introducing best-performing individuals\ninto the population. Experiments across four datasets show that our approach\nsignificantly improves the balance between exploration and exploitation,\nboosting performance. Moreover, the effectiveness of the EII mechanism in\nreducing exploration time has been demonstrated, achieving an efficient\nintegration of EA and DRL on task-oriented dialogue policy tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fdb\u5316\u7b97\u6cd5\uff08EA\uff09\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u65b9\u6cd5\uff0c\u4ee5\u4f18\u5316\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u5bf9\u8bdd\u7b56\u7565\u3002\u901a\u8fc7\u7cbe\u82f1\u4e2a\u4f53\u6ce8\u5165\u673a\u5236\uff08EII\uff09\uff0c\u6709\u6548\u63d0\u5347\u4e86\u63a2\u7d22\u4e0e\u5f00\u53d1\u7684\u5e73\u8861\u548c\u641c\u7d22\u6548\u7387\u3002", "motivation": "DRL\u5728\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u96be\u4ee5\u5e73\u8861\u63a2\u7d22\u4e0e\u5f00\u53d1\uff0c\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u3002EA\u80fd\u6709\u6548\u63a2\u7d22\u89e3\u7a7a\u95f4\uff0c\u56e0\u6b64\u7ed3\u5408\u4e24\u8005\u4ee5\u4f18\u5316\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408EA\u7684\u5168\u5c40\u641c\u7d22\u80fd\u529b\u548cDRL\u7684\u5c40\u90e8\u4f18\u5316\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165EII\u673a\u5236\u4ee5\u63d0\u5347EA\u7684\u641c\u7d22\u6548\u7387\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u63a2\u7d22\u4e0e\u5f00\u53d1\u7684\u5e73\u8861\u548c\u6027\u80fd\uff0cEII\u673a\u5236\u8fd8\u7f29\u77ed\u4e86\u63a2\u7d22\u65f6\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86EA\u4e0eDRL\u7684\u9ad8\u6548\u7ed3\u5408\uff0c\u4f18\u5316\u4e86\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "keywords": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60, \u8fdb\u5316\u7b97\u6cd5, \u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf, \u63a2\u7d22\u4e0e\u5f00\u53d1"}}
{"id": "2506.04135", "pdf": "https://arxiv.org/pdf/2506.04135", "abs": "https://arxiv.org/abs/2506.04135", "authors": ["Pei Yang", "Hai Ci", "Mike Zheng Shou"], "title": "macOSWorld: A Multilingual Interactive Benchmark for GUI Agents", "categories": ["cs.AI"], "comment": null, "summary": "Graphical User Interface (GUI) agents show promising capabilities for\nautomating computer-use tasks and facilitating accessibility, but existing\ninteractive benchmarks are mostly English-only, covering web-use or Windows,\nLinux, and Android environments, but not macOS. macOS is a major OS with\ndistinctive GUI patterns and exclusive applications. To bridge the gaps, we\npresent macOSWorld, the first comprehensive benchmark for evaluating GUI agents\non macOS. macOSWorld features 202 multilingual interactive tasks across 30\napplications (28 macOS-exclusive), with task instructions and OS interfaces\noffered in 5 languages (English, Chinese, Arabic, Japanese, and Russian). As\nGUI agents are shown to be vulnerable to deception attacks, macOSWorld also\nincludes a dedicated safety benchmarking subset. Our evaluation on six GUI\nagents reveals a dramatic gap: proprietary computer-use agents lead at above\n30% success rate, while open-source lightweight research models lag at below\n2%, highlighting the need for macOS domain adaptation. Multilingual benchmarks\nalso expose common weaknesses, especially in Arabic, with a 27.5% average\ndegradation compared to English. Results from safety benchmarking also\nhighlight that deception attacks are more general and demand immediate\nattention. macOSWorld is available at https://github.com/showlab/macosworld.", "AI": {"tldr": "macOSWorld\u662f\u9996\u4e2a\u9488\u5bf9macOS\u7cfb\u7edf\u7684GUI\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\uff0c\u8986\u76d6\u591a\u79cd\u8bed\u8a00\u548c\u4efb\u52a1\uff0c\u5e76\u5305\u62ec\u5b89\u5168\u6d4b\u8bd5\u5b50\u96c6\u3002", "motivation": "\u586b\u8865\u73b0\u6709GUI\u4ee3\u7406\u57fa\u51c6\u5728macOS\u7cfb\u7edf\u4e0a\u7684\u7a7a\u767d\uff0c\u652f\u6301\u591a\u8bed\u8a00\u548c\u5b89\u5168\u6027\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u542b202\u4e2a\u591a\u8bed\u8a00\u4efb\u52a1\u548c30\u4e2a\u5e94\u7528\uff0828\u4e2a\u4e3amacOS\u72ec\u6709\uff09\u7684\u57fa\u51c6\uff0c\u5e76\u6d4b\u8bd56\u79cdGUI\u4ee3\u7406\u3002", "result": "\u4e13\u6709\u4ee3\u7406\u6210\u529f\u7387\u9ad8\u4e8e30%\uff0c\u5f00\u6e90\u6a21\u578b\u4f4e\u4e8e2%\uff1b\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\u963f\u62c9\u4f2f\u8bed\u8868\u73b0\u6700\u5dee\uff1b\u6b3a\u9a97\u653b\u51fb\u95ee\u9898\u666e\u904d\u3002", "conclusion": "macOSWorld\u4e3aGUI\u4ee3\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u9886\u57df\u9002\u914d\u548c\u591a\u8bed\u8a00\u6311\u6218\u3002", "keywords": "GUI\u4ee3\u7406\uff0cmacOS\uff0c\u591a\u8bed\u8a00\uff0c\u5b89\u5168\u6d4b\u8bd5\uff0c\u57fa\u51c6"}}
{"id": "2506.03320", "pdf": "https://arxiv.org/pdf/2506.03320", "abs": "https://arxiv.org/abs/2506.03320", "authors": ["Jack Bell", "Luigi Quarantiello", "Eric Nuertey Coleman", "Lanpei Li", "Malio Li", "Mauro Madeddu", "Elia Piccoli", "Vincenzo Lomonaco"], "title": "The Future of Continual Learning in the Era of Foundation Models: Three Key Directions", "categories": ["cs.LG", "cs.AI"], "comment": "16 pages, 1 figure, accepted at TCAI workshop 2025", "summary": "Continual learning--the ability to acquire, retain, and refine knowledge over\ntime--has always been fundamental to intelligence, both human and artificial.\nHistorically, different AI paradigms have acknowledged this need, albeit with\nvarying priorities: early expert and production systems focused on incremental\nknowledge consolidation, while reinforcement learning emphasised dynamic\nadaptation. With the rise of deep learning, deep continual learning has\nprimarily focused on learning robust and reusable representations over time to\nsolve sequences of increasingly complex tasks. However, the emergence of Large\nLanguage Models (LLMs) and foundation models has raised the question: Do we\nstill need continual learning when centralised, monolithic models can tackle\ndiverse tasks with access to internet-scale knowledge? We argue that continual\nlearning remains essential for three key reasons: (i) continual pre-training is\nstill necessary to ensure foundation models remain up to date, mitigating\nknowledge staleness and distribution shifts while integrating new information;\n(ii) continual fine-tuning enables models to specialise and personalise,\nadapting to domain-specific tasks, user preferences, and real-world constraints\nwithout full retraining, avoiding the need for computationally expensive long\ncontext-windows; (iii) continual compositionality offers a scalable and modular\napproach to intelligence, enabling the orchestration of foundation models and\nagents to be dynamically composed, recombined, and adapted. While continual\npre-training and fine-tuning are explored as niche research directions, we\nargue it is continual compositionality that will mark the rebirth of continual\nlearning. The future of AI will not be defined by a single static model but by\nan ecosystem of continually evolving and interacting models, making continual\nlearning more relevant than ever.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u6301\u7eed\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u57fa\u7840\u6a21\u578b\u65f6\u4ee3\u7684\u5fc5\u8981\u6027\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u66f4\u65b0\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u4efb\u52a1\u7ec6\u5316\u548c\u6a21\u578b\u7ec4\u5408\u65b9\u9762\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u7684\u5174\u8d77\uff0c\u8bba\u6587\u65e8\u5728\u56de\u7b54\u6301\u7eed\u5b66\u4e60\u662f\u5426\u4ecd\u7136\u91cd\u8981\uff0c\u5e76\u63d0\u51fa\u4e86\u5176\u5728\u73b0\u4ee3AI\u7cfb\u7edf\u4e2d\u7684\u4e09\u5927\u4f5c\u7528\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5206\u6790\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u6301\u7eed\u5fae\u8c03\u548c\u6301\u7eed\u7ec4\u5408\u6027\u4e09\u4e2a\u65b9\u5411\uff0c\u8bba\u8bc1\u4e86\u6301\u7eed\u5b66\u4e60\u7684\u5fc5\u8981\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6301\u7eed\u5b66\u4e60\u5728\u66f4\u65b0\u6a21\u578b\u77e5\u8bc6\u3001\u4efb\u52a1\u9002\u914d\u6027\u548c\u6a21\u578b\u52a8\u6001\u7ec4\u5408\u4e2d\u5177\u6709\u4e0d\u53ef\u66ff\u4ee3\u7684\u4f5c\u7528\u3002", "conclusion": "\u6301\u7eed\u5b66\u4e60\u4e0d\u4ec5\u662f\u5fc5\u8981\u7684\uff0c\u800c\u4e14\u5728\u672a\u6765AI\u53d1\u5c55\u4e2d\u5c06\u4ee5\u6301\u7eed\u7ec4\u5408\u6027\u4e3a\u6838\u5fc3\uff0c\u5f62\u6210\u52a8\u6001\u6f14\u5316\u7684\u6a21\u578b\u751f\u6001\u7cfb\u7edf\u3002", "keywords": "\u6301\u7eed\u5b66\u4e60, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u57fa\u7840\u6a21\u578b, \u9884\u8bad\u7ec3, \u5fae\u8c03, \u7ec4\u5408\u6027"}}
{"id": "2506.03523", "pdf": "https://arxiv.org/pdf/2506.03523", "abs": "https://arxiv.org/abs/2506.03523", "authors": ["Chong Li", "Jiajun Zhang", "Chengqing Zong"], "title": "TokAlign: Efficient Vocabulary Adaptation via Token Alignment", "categories": ["cs.CL"], "comment": "ACL 2025, our codes and models are available at\n  https://github.com/ZNLP/TokAlign", "summary": "Tokenization serves as a foundational step for Large Language Models (LLMs)\nto process text. In new domains or languages, the inefficiency of the tokenizer\nwill slow down the training and generation of LLM. The mismatch in vocabulary\nalso hinders deep knowledge transfer between LLMs like token-level\ndistillation. To mitigate this gap, we propose an efficient method named\nTokAlign to replace the vocabulary of LLM from the token co-occurrences view,\nand further transfer the token-level knowledge between models. It first aligns\nthe source vocabulary to the target one by learning a one-to-one mapping matrix\nfor token IDs. Model parameters, including embeddings, are rearranged and\nprogressively fine-tuned for the new vocabulary. Our method significantly\nimproves multilingual text compression rates and vocabulary initialization for\nLLMs, decreasing the perplexity from 3.4$\\text{e}^2$ of strong baseline methods\nto 1.2$\\text{e}^2$ after initialization. Experimental results on models across\nmultiple parameter scales demonstrate the effectiveness and generalization of\nTokAlign, which costs as few as 5k steps to restore the performance of the\nvanilla model. After unifying vocabularies between LLMs, token-level\ndistillation can remarkably boost (+4.4% than sentence-level distillation) the\nbase model, costing only 235M tokens.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTokAlign\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bcd\u6c47\u66ff\u6362\u548c\u77e5\u8bc6\u8fc1\u79fb\u6539\u8fdbLLM\u7684tokenizer\u6027\u80fd\u3002", "motivation": "\u5728LLM\u4e2d\uff0ctokenizer\u7684\u4e0d\u6548\u7387\u4f1a\u62d6\u6162\u8bad\u7ec3\u548c\u751f\u6210\u901f\u5ea6\uff0c\u8bcd\u6c47\u4e0d\u5339\u914d\u4e5f\u963b\u788d\u77e5\u8bc6\u8fc1\u79fb\u3002", "method": "TokAlign\u901a\u8fc7\u4e00\u5bf9\u4e00\u7684token ID\u6620\u5c04\u77e9\u9635\u5bf9\u9f50\u8bcd\u6c47\uff0c\u8c03\u6574\u6a21\u578b\u53c2\u6570\u5e76\u9010\u6b65\u5fae\u8c03\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u6587\u672c\u538b\u7f29\u7387\u548c\u8bcd\u6c47\u521d\u59cb\u5316\u6548\u679c\uff0c\u56f0\u60d1\u5ea6\u4ece3.4e\u00b2\u964d\u81f31.2e\u00b2\u3002", "conclusion": "TokAlign\u5728\u591a\u6837\u672c\u4e0a\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\uff0c\u4ec5\u97005k\u6b65\u5373\u53ef\u6062\u590d\u6027\u80fd\u3002", "keywords": "Tokenization, LLM, vocabulary alignment, token-level distillation"}}
{"id": "2506.04210", "pdf": "https://arxiv.org/pdf/2506.04210", "abs": "https://arxiv.org/abs/2506.04210", "authors": ["Soumya Suvra Ghosal", "Souradip Chakraborty", "Avinash Reddy", "Yifu Lu", "Mengdi Wang", "Dinesh Manocha", "Furong Huang", "Mohammad Ghavamzadeh", "Amrit Singh Bedi"], "title": "Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,\nDeepSeek R1) have led to a popular belief that extending thinking traces using\nprompts like \"Wait\" or \"Let me rethink\" can improve performance. This raises a\nnatural question: Does thinking more at test-time truly lead to better\nreasoning? To answer this question, we perform a detailed empirical study\nacross models and benchmarks, which reveals a consistent pattern of initial\nperformance improvements from additional thinking followed by a decline, due to\n\"overthinking\". To understand this non-monotonic trend, we consider a simple\nprobabilistic model, which reveals that additional thinking increases output\nvariance-creating an illusion of improved reasoning while ultimately\nundermining precision. Thus, observed gains from \"more thinking\" are not true\nindicators of improved reasoning, but artifacts stemming from the connection\nbetween model uncertainty and evaluation metric. This suggests that test-time\nscaling through extended thinking is not an effective way to utilize the\ninference thinking budget. Recognizing these limitations, we introduce an\nalternative test-time scaling approach, parallel thinking, inspired by\nBest-of-N sampling. Our method generates multiple independent reasoning paths\nwithin the same inference budget and selects the most consistent response via\nmajority vote, achieving up to 20% higher accuracy compared to extended\nthinking. This provides a simple yet effective mechanism for test-time scaling\nof reasoning models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u6d4b\u8bd5\u65f6\u5ef6\u957f\u601d\u8003\u65f6\u95f4\u5e76\u4e0d\u80fd\u771f\u6b63\u63d0\u9ad8\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\uff0c\u53cd\u800c\u4f1a\u56e0\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u5bfc\u81f4\u8868\u73b0\u4e0b\u964d\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e76\u884c\u601d\u8003\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6570\u6295\u7968\u9009\u62e9\u6700\u4f18\u7b54\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u9488\u5bf9\u6d4b\u8bd5\u65f6\u5ef6\u957f\u601d\u8003\u662f\u5426\u80fd\u63d0\u9ad8\u63a8\u7406\u6027\u80fd\u7684\u7591\u95ee\uff0c\u4f5c\u8005\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u5e76\u63a2\u8ba8\u4e86\u80cc\u540e\u7684\u539f\u56e0\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u4e86\u5ef6\u957f\u601d\u8003\u65f6\u95f4\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u5e76\u884c\u601d\u8003\u65b9\u6cd5\uff0c\u751f\u6210\u591a\u4e2a\u72ec\u7acb\u63a8\u7406\u8def\u5f84\u5e76\u901a\u8fc7\u6295\u7968\u9009\u62e9\u6700\u4f73\u7b54\u6848\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5ef6\u957f\u601d\u8003\u65f6\u95f4\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u5e76\u884c\u601d\u8003\u65b9\u6cd5\u80fd\u5728\u76f8\u540c\u7684\u63a8\u7406\u9884\u7b97\u4e0b\u63d0\u534720%\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u5ef6\u957f\u601d\u8003\u5e76\u975e\u6709\u6548\u5229\u7528\u63a8\u7406\u9884\u7b97\u7684\u65b9\u5f0f\uff0c\u800c\u5e76\u884c\u601d\u8003\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "keywords": "\u63a8\u7406\u6a21\u578b,\u6d4b\u8bd5\u65f6\u6269\u5c55,\u8fc7\u5ea6\u601d\u8003,\u5e76\u884c\u601d\u8003,\u591a\u6570\u6295\u7968"}}
{"id": "2506.03324", "pdf": "https://arxiv.org/pdf/2506.03324", "abs": "https://arxiv.org/abs/2506.03324", "authors": ["Ethan Che", "Hakan Ceylan", "James McInerney", "Nathan Kallus"], "title": "Optimization of Epsilon-Greedy Exploration", "categories": ["cs.LG"], "comment": null, "summary": "Modern recommendation systems rely on exploration to learn user preferences\nfor new items, typically implementing uniform exploration policies (e.g.,\nepsilon-greedy) due to their simplicity and compatibility with machine learning\n(ML) personalization models. Within these systems, a crucial consideration is\nthe rate of exploration - what fraction of user traffic should receive random\nitem recommendations and how this should evolve over time. While various\nheuristics exist for navigating the resulting exploration-exploitation\ntradeoff, selecting optimal exploration rates is complicated by practical\nconstraints including batched updates, time-varying user traffic, short time\nhorizons, and minimum exploration requirements. In this work, we propose a\nprincipled framework for determining the exploration schedule based on directly\nminimizing Bayesian regret through stochastic gradient descent (SGD), allowing\nfor dynamic exploration rate adjustment via Model-Predictive Control (MPC).\nThrough extensive experiments with recommendation datasets, we demonstrate that\nvariations in the batch size across periods significantly influence the optimal\nexploration strategy. Our optimization methods automatically calibrate\nexploration to the specific problem setting, consistently matching or\noutperforming the best heuristic for each setting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u540e\u6094\u6700\u5c0f\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u52a8\u6001\u8c03\u6574\u63a2\u7d22\u901f\u7387\uff0c\u4f18\u5316\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u63a2\u7d22\u7b56\u7565\u3002", "motivation": "\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u7b80\u5355\u7684\u5747\u5300\u63a2\u7d22\u7b56\u7565\uff0c\u4f46\u63a2\u7d22\u901f\u7387\u7684\u9009\u62e9\u53d7\u591a\u79cd\u5b9e\u9645\u7ea6\u675f\u5f71\u54cd\uff0c\u7f3a\u4e4f\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u53f6\u65af\u540e\u6094\u6700\u5c0f\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7SGD\u548cMPC\u52a8\u6001\u8c03\u6574\u63a2\u7d22\u901f\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6279\u91cf\u5927\u5c0f\u53d8\u5316\u663e\u8457\u5f71\u54cd\u6700\u4f73\u63a2\u7d22\u7b56\u7565\uff0c\u8be5\u65b9\u6cd5\u80fd\u81ea\u52a8\u6821\u51c6\u63a2\u7d22\u901f\u7387\uff0c\u4f18\u4e8e\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u5177\u4f53\u95ee\u9898\u8bbe\u7f6e\u52a8\u6001\u4f18\u5316\u63a2\u7d22\u7b56\u7565\uff0c\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "keywords": "\u63a8\u8350\u7cfb\u7edf,\u63a2\u7d22\u7b56\u7565,\u8d1d\u53f6\u65af\u540e\u6094,\u968f\u673a\u68af\u5ea6\u4e0b\u964d,\u6a21\u578b\u9884\u6d4b\u63a7\u5236"}}
{"id": "2506.03524", "pdf": "https://arxiv.org/pdf/2506.03524", "abs": "https://arxiv.org/abs/2506.03524", "authors": ["Yuyu Zhang", "Jing Su", "Yifan Sun", "Chenguang Xi", "Xia Xiao", "Shen Zheng", "Anxiang Zhang", "Kaibo Liu", "Daoguang Zan", "Tao Sun", "Jinhua Zhu", "Shulin Xin", "Dong Huang", "Yetao Bai", "Lixin Dong", "Chao Li", "Jianchong Chen", "Hanzhi Zhou", "Yifan Huang", "Guanghan Ning", "Xierui Song", "Jiaze Chen", "Siyao Liu", "Kai Shen", "Liang Xiang", "Yonghui Wu"], "title": "Seed-Coder: Let the Code Model Curate Data for Itself", "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Code data in large language model (LLM) pretraining is recognized crucial not\nonly for code-related tasks but also for enhancing general intelligence of\nLLMs. Current open-source LLMs often heavily rely on human effort to produce\ntheir code pretraining data, such as employing hand-crafted filtering rules\ntailored to individual programming languages, or using human-annotated data to\ntrain quality filters. However, these approaches are inherently limited in\nscalability, prone to subjective biases, and costly to extend and maintain\nacross diverse programming languages. To address these challenges, we introduce\nSeed-Coder, a series of open-source LLMs comprising base, instruct and\nreasoning models of 8B size, minimizing human involvement in data construction.\nOur code pretraining data is produced by a model-centric data pipeline, which\npredominantly leverages LLMs for scoring and filtering code data. The instruct\nmodel is further trained via supervised fine-tuning and preference\noptimization, and the reasoning model leverages Long-Chain-of-Thought (LongCoT)\nreinforcement learning to improve multi-step code reasoning. Seed-Coder\nachieves state-of-the-art results among open-source models of similar size and\neven surpasses some much larger models, demonstrating superior performance in\ncode generation, code completion, code editing, code reasoning, and software\nengineering tasks.", "AI": {"tldr": "Seed-Coder\u662f\u4e00\u79cd8B\u89c4\u6a21\u7684LLM\u7cfb\u5217\uff0c\u901a\u8fc7\u6a21\u578b\u4e2d\u5fc3\u7684\u6570\u636e\u7ba1\u9053\u51cf\u5c11\u4eba\u5de5\u53c2\u4e0e\uff0c\u5728\u4ee3\u7801\u751f\u6210\u3001\u5b8c\u6210\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u5f00\u6e90LLM\u4f9d\u8d56\u4eba\u5de5\u6784\u5efa\u4ee3\u7801\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u5b58\u5728\u53ef\u6269\u5c55\u6027\u5dee\u3001\u4e3b\u89c2\u504f\u89c1\u548c\u7ef4\u62a4\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6a21\u578b\u4e2d\u5fc3\u6570\u636e\u7ba1\u9053\uff0c\u5229\u7528LLM\u8fdb\u884c\u4ee3\u7801\u6570\u636e\u8bc4\u5206\u548c\u8fc7\u6ee4\uff1b\u6307\u5bfc\u6a21\u578b\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u504f\u597d\u4f18\u5316\u8bad\u7ec3\uff0c\u63a8\u7406\u6a21\u578b\u5229\u7528LongCoT\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u591a\u6b65\u4ee3\u7801\u63a8\u7406\u3002", "result": "Seed-Coder\u5728\u7c7b\u4f3c\u89c4\u6a21\u7684\u5f00\u6e90\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u751a\u81f3\u8d85\u8d8a\u67d0\u4e9b\u66f4\u5927\u7684\u6a21\u578b\u3002", "conclusion": "Seed-Coder\u8bc1\u660e\u4e86\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3001\u4f9d\u8d56\u6a21\u578b\u4e2d\u5fc3\u7684\u65b9\u6848\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "keywords": "Seed-Coder, LLM, \u4ee3\u7801\u9884\u8bad\u7ec3, \u6a21\u578b\u4e2d\u5fc3\u6570\u636e\u7ba1\u9053, LongCoT"}}
{"id": "2209.01205", "pdf": "https://arxiv.org/pdf/2209.01205", "abs": "https://arxiv.org/abs/2209.01205", "authors": ["Han Wu", "Jie Yin", "Bala Rajaratnam", "Jianyuan Guo"], "title": "Hierarchical Relational Learning for Few-Shot Knowledge Graph Completion", "categories": ["cs.LG", "cs.AI", "cs.CV", "I.2"], "comment": "Published at ICLR 2023", "summary": "Knowledge graphs (KGs) are powerful in terms of their inference abilities,\nbut are also notorious for their incompleteness and long-tail distribution of\nrelations. To address these challenges and expand the coverage of KGs, few-shot\nKG completion aims to make predictions for triplets involving novel relations\nwhen only a few training triplets are provided as reference. Previous methods\nhave focused on designing local neighbor aggregators to learn entity-level\ninformation and/or imposing a potentially invalid sequential dependency\nassumption at the triplet level to learn meta relation information. However,\npairwise triplet-level interactions and context-level relational information\nhave been largely overlooked for learning meta representations of few-shot\nrelations. In this paper, we propose a hierarchical relational learning method\n(HiRe) for few-shot KG completion. By jointly capturing three levels of\nrelational information (entity-level, triplet-level and context-level), HiRe\ncan effectively learn and refine meta representations of few-shot relations,\nand thus generalize well to new unseen relations. Extensive experiments on\nbenchmark datasets validate the superiority of HiRe over state-of-the-art\nmethods. The code can be found in https://github.com/alexhw15/HiRe.git.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5c11\u6837\u672c\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u5206\u5c42\u5173\u7cfb\u5b66\u4e60\u65b9\u6cd5\uff08HiRe\uff09\uff0c\u901a\u8fc7\u8054\u5408\u6355\u6349\u5b9e\u4f53\u7ea7\u3001\u4e09\u5143\u7ec4\u7ea7\u548c\u4e0a\u4e0b\u6587\u7ea7\u7684\u5173\u7cfb\u4fe1\u606f\uff0c\u6709\u6548\u5b66\u4e60\u65b0\u5173\u7cfb\u7684\u5143\u8868\u793a\u3002", "motivation": "\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u867d\u7136\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u5f3a\u5927\uff0c\u4f46\u5176\u4e0d\u5b8c\u6574\u6027\u548c\u5173\u7cfb\u7684\u957f\u5c3e\u5206\u5e03\u95ee\u9898\u663e\u8457\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5e76\u6269\u5c55KGs\u7684\u8986\u76d6\u8303\u56f4\uff0c\u5c11\u6837\u672cKG\u8865\u5168\u4efb\u52a1\u65e8\u5728\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u4e09\u5143\u7ec4\u4f5c\u4e3a\u53c2\u8003\u7684\u60c5\u51b5\u4e0b\u5bf9\u6d89\u53ca\u65b0\u5173\u7cfb\u7684\u4e09\u5143\u7ec4\u8fdb\u884c\u9884\u6d4b\u3002", "method": "\u63d0\u51faHiRe\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u6355\u6349\u5b9e\u4f53\u7ea7\u3001\u4e09\u5143\u7ec4\u7ea7\u548c\u4e0a\u4e0b\u6587\u7ea7\u4e09\u4e2a\u5c42\u6b21\u7684\u5173\u7cfb\u4fe1\u606f\uff0c\u5b66\u4e60\u548c\u4f18\u5316\u5c11\u6837\u672c\u5173\u7cfb\u7684\u5143\u8868\u793a\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86HiRe\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "HiRe\u901a\u8fc7\u591a\u5c42\u6b21\u5173\u7cfb\u4fe1\u606f\u7684\u8054\u5408\u5b66\u4e60\uff0c\u80fd\u591f\u6709\u6548\u63a8\u5e7f\u5230\u672a\u89c1\u8fc7\u7684\u5173\u7cfb\u3002", "keywords": "\u77e5\u8bc6\u56fe\u8c31, \u5c11\u6837\u672c\u5b66\u4e60, \u5173\u7cfb\u5b66\u4e60, \u4e09\u5143\u7ec4\u8865\u5168"}}
{"id": "2506.03333", "pdf": "https://arxiv.org/pdf/2506.03333", "abs": "https://arxiv.org/abs/2506.03333", "authors": ["Juan Sebastian Rojas", "Chi-Guhn Lee"], "title": "A Differential Perspective on Distributional Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "To date, distributional reinforcement learning (distributional RL) methods\nhave exclusively focused on the discounted setting, where an agent aims to\noptimize a potentially-discounted sum of rewards over time. In this work, we\nextend distributional RL to the average-reward setting, where an agent aims to\noptimize the reward received per time-step. In particular, we utilize a\nquantile-based approach to develop the first set of algorithms that can\nsuccessfully learn and/or optimize the long-run per-step reward distribution,\nas well as the differential return distribution of an average-reward MDP. We\nderive proven-convergent tabular algorithms for both prediction and control, as\nwell as a broader family of algorithms that have appealing scaling properties.\nEmpirically, we find that these algorithms consistently yield competitive\nperformance when compared to their non-distributional equivalents, while also\ncapturing rich information about the long-run reward and return distributions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6269\u5c55\u5230\u5e73\u5747\u5956\u52b1\u573a\u666f\u7684\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u9996\u4e2a\u80fd\u591f\u5b66\u4e60\u548c\u4f18\u5316\u957f\u671f\u6bcf\u6b65\u5956\u52b1\u5206\u5e03\u53ca\u5dee\u5206\u56de\u62a5\u5206\u5e03\u7684\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6298\u6263\u573a\u666f\uff0c\u800c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5e73\u5747\u5956\u52b1\u573a\u666f\u4e0b\u7684\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5206\u4f4d\u6570\u7684\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u6536\u655b\u7684\u8868\u683c\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u9884\u6d4b\u548c\u63a7\u5236\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u5177\u6709\u826f\u597d\u6269\u5c55\u6027\u7684\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0e\u975e\u5206\u5e03\u5f0f\u7b49\u6548\u7b97\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u80fd\u6355\u83b7\u957f\u671f\u5956\u52b1\u548c\u56de\u62a5\u5206\u5e03\u7684\u4e30\u5bcc\u4fe1\u606f\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5730\u5c06\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6269\u5c55\u5230\u5e73\u5747\u5956\u52b1\u573a\u666f\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u548c\u5de5\u5177\u3002", "keywords": "\u5206\u5e03\u5f3a\u5316\u5b66\u4e60, \u5e73\u5747\u5956\u52b1, \u5206\u4f4d\u6570\u65b9\u6cd5, \u6536\u655b\u7b97\u6cd5"}}
{"id": "2506.03533", "pdf": "https://arxiv.org/pdf/2506.03533", "abs": "https://arxiv.org/abs/2506.03533", "authors": ["Apurva Gandhi", "Graham Neubig"], "title": "Go-Browse: Training Web Agents with Structured Exploration", "categories": ["cs.CL"], "comment": null, "summary": "One of the fundamental problems in digital agents is their lack of\nunderstanding of their environment. For instance, a web browsing agent may get\nlost in unfamiliar websites, uncertain what pages must be visited to achieve\nits goals. To address this, we propose Go-Browse, a method for automatically\ncollecting diverse and realistic web agent data at scale through structured\nexploration of web environments. Go-Browse achieves efficient exploration by\nframing data collection as a graph search, enabling reuse of information across\nexploration episodes. We instantiate our method on the WebArena benchmark,\ncollecting a dataset of 10K successful task-solving trajectories and 40K\ninteraction steps across 100 URLs. Fine-tuning a 7B parameter language model on\nthis dataset achieves a success rate of 21.7% on the WebArena benchmark,\nbeating GPT-4o mini by 2.4% and exceeding current state-of-the-art results for\nsub-10B parameter models by 2.9%.", "AI": {"tldr": "\u63d0\u51faGo-Browse\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a2\u7d22\u7f51\u9875\u73af\u5883\u81ea\u52a8\u6536\u96c6\u591a\u6837\u5316\u7684\u6570\u636e\uff0c\u63d0\u5347\u6570\u5b57\u4ee3\u7406\u5bf9\u73af\u5883\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u6570\u5b57\u4ee3\u7406\u7f3a\u4e4f\u5bf9\u73af\u5883\u7406\u89e3\u7684\u80fd\u529b\uff0c\u4f8b\u5982\u5728\u964c\u751f\u7f51\u9875\u4e2d\u8ff7\u5931\u65b9\u5411\u3002\u9700\u8981\u9ad8\u6548\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Go-Browse\u5c06\u6570\u636e\u6536\u96c6\u5efa\u6a21\u4e3a\u56fe\u641c\u7d22\u95ee\u9898\uff0c\u652f\u6301\u8de8\u63a2\u7d22\u4efb\u52a1\u7684\u4fe1\u606f\u590d\u7528\uff0c\u5e76\u5728WebArena\u57fa\u51c6\u4e0a\u5b9e\u73b0\u6570\u636e\u6536\u96c6\u3002", "result": "\u6536\u96c6\u4e8610K\u6761\u4efb\u52a1\u8f68\u8ff9\u548c40K\u4e2a\u4ea4\u4e92\u6b65\u9aa4\uff0c\u5fae\u8c037B\u53c2\u6570\u6a21\u578b\u540e\uff0c\u8fbe\u523021.7%\u7684\u6210\u529f\u7387\uff0c\u8d85\u8fc7GPT-4o mini\u548c\u73b0\u6709\u6700\u4f7310B\u4ee5\u4e0b\u6a21\u578b\u3002", "conclusion": "Go-Browse\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u6536\u96c6\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b57\u4ee3\u7406\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "keywords": "\u6570\u5b57\u4ee3\u7406\u3001\u7ed3\u6784\u5316\u63a2\u7d22\u3001\u56fe\u641c\u7d22\u3001\u6570\u636e\u6536\u96c6\u3001WebArena\u3001\u5fae\u8c03\u6a21\u578b"}}
{"id": "2506.03337", "pdf": "https://arxiv.org/pdf/2506.03337", "abs": "https://arxiv.org/abs/2506.03337", "authors": ["Yide Ran", "Wentao Guo", "Jingwei Sun", "Yanzhou Pan", "Xiaodong Yu", "Hao Wang", "Jianwen Xie", "Yiran Chen", "Denghui Zhang", "Zhaozhuo Xu"], "title": "Mitigating Non-IID Drift in Zeroth-Order Federated LLM Fine-Tuning with Transferable Sparsity", "categories": ["cs.LG", "cs.AI"], "comment": "56 pages, 11 figures", "summary": "Federated Learning enables collaborative fine-tuning of Large Language Models\n(LLMs) across decentralized Non-Independent and Identically Distributed\n(Non-IID) clients, but such models' massive parameter sizes lead to significant\nmemory and communication challenges. This work introduces Meerkat, a sparse\nzeroth-order optimization (ZO) method designed for federated LLM fine-tuning.\nBy limiting fine-tuning to a transferable, static, extremely sparse subset of\nparameters, Meerkat achieves remarkable communication efficiency, enabling\ncost-effective high-frequency synchronization. With theoretical analysis and\nexperiments, we show that this high-frequency communication effectively\nmitigates Non-IID data challenges and leads to superior performance compared to\nfull-parameter ZO. Furthermore, experiment results show that Meerkat\noutperforms existing sparsity baselines with better performance at the same\ncommunication frequency. To further handle Non-IID drift, Meerkat leverages\ntraceable local updates and forms a virtual path for each client. This virtual\npath mechanism reveals the GradIP phenomenon: the inner products between LLM\npre-training gradients maintained by server and client gradients estimated via\nZO converges for extreme Non-IID clients but oscillates for IID ones. This\ndistinct behavior provides a signal for identifying clients with extreme data\nheterogeneity. Using this signal, Meerkat-vp is proposed to analyze GradIP\ntrajectories to identify extreme Non-IID clients and applies early stopping to\nenhance aggregated model quality. Experiments confirm that Meerkat and\nMeerkat-vp significantly improve the efficiency and effectiveness of ZO\nfederated LLM fine-tuning.", "AI": {"tldr": "Meerkat\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\u4e2d\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\uff0c\u901a\u8fc7\u9759\u6001\u7a00\u758f\u53c2\u6570\u5b50\u96c6\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u5e76\u901a\u8fc7\u865a\u62df\u8def\u5f84\u673a\u5236\u8bc6\u522b\u6781\u7aef\u975e\u72ec\u7acb\u540c\u5206\u5e03\u5ba2\u6237\u7aef\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u9762\u4e34\u5de8\u5927\u7684\u5185\u5b58\u548c\u901a\u4fe1\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08Non-IID\uff09\u6570\u636e\u73af\u5883\u4e2d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86Meerkat\u65b9\u6cd5\u3002", "method": "Meerkat\u91c7\u7528\u7a00\u758f\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\uff0c\u4ec5\u5fae\u8c03\u9759\u6001\u4e14\u7a00\u758f\u7684\u53c2\u6570\u5b50\u96c6\uff0c\u5e76\u901a\u8fc7\u865a\u62df\u8def\u5f84\u673a\u5236\u8bc6\u522b\u6781\u7aefNon-IID\u5ba2\u6237\u7aef\u3002Meerkat-vp\u8fdb\u4e00\u6b65\u5229\u7528GradIP\u73b0\u8c61\u8fdb\u884c\u5206\u6790\u548c\u63d0\u524d\u505c\u6b62\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cMeerkat\u5728\u9ad8\u901a\u4fe1\u9891\u7387\u4e0b\u8868\u73b0\u4f18\u4e8e\u5168\u53c2\u6570\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u5728\u76f8\u540c\u901a\u4fe1\u9891\u7387\u4e0b\u8d85\u8d8a\u73b0\u6709\u7a00\u758f\u57fa\u7ebf\u3002Meerkat-vp\u80fd\u6709\u6548\u8bc6\u522b\u6781\u7aefNon-IID\u5ba2\u6237\u7aef\u5e76\u63d0\u5347\u6a21\u578b\u8d28\u91cf\u3002", "conclusion": "Meerkat\u548cMeerkat-vp\u663e\u8457\u63d0\u9ad8\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60\u3001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u96f6\u9636\u4f18\u5316\u3001\u7a00\u758f\u5fae\u8c03\u3001Non-IID\u6570\u636e"}}
{"id": "2506.03541", "pdf": "https://arxiv.org/pdf/2506.03541", "abs": "https://arxiv.org/abs/2506.03541", "authors": ["Xiaofeng Zhou", "Heyan Huang", "Lizi Liao"], "title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 10 figures. The camera-ready paper for Findings of ACL 2025", "summary": "Large Language Models (LLMs) continue to set new standards in\nknowledge-intensive and complex reasoning tasks, yet their high computational\ndemands limit widespread adoption. While distilling large models into smaller\nones offers a sustainable solution, current techniques--such as static\nknowledge distillation, resource-intensive reinforcement learning from human\nfeedback, or limited self-reflection--struggle to yield substantial and lasting\nperformance gains. In this paper, we present a novel Debate and Reflect (D&R)\nframework that orchestrates multi-turn debates between smaller models and\nstronger teacher models, eliciting actionable feedback (e.g., error analysis,\ncorrective strategies) to guide student models. Further, we introduce\nTree-structured Direct Preference Optimization (T-DPO) to efficiently leverage\nthese debate logs, organizing interactions into a hierarchical format for\neffective training. Empirical evaluations across diverse NLP benchmarks\ndemonstrate that our approach significantly improves smaller-model accuracy,\nrobustness, and generalization, outperforming conventional baselines by a large\nmargin.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Debate and Reflect\u6846\u67b6\u548cTree-structured Direct Preference Optimization\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u6a21\u578b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u9700\u6c42\u9650\u5236\u4e86\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e14\u73b0\u6709\u7684\u84b8\u998f\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "method": "\u901a\u8fc7\u591a\u8f6e\u8fa9\u8bba\u6846\u67b6\u548c\u5c42\u6b21\u5316\u7684\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u6307\u5bfc\u5c0f\u6a21\u578b\u5b66\u4e60\u3002", "result": "\u5728\u4e0d\u540cNLP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u6b64\u65b9\u6cd5\u4e3a\u5c0f\u6a21\u578b\u7684\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u77e5\u8bc6\u84b8\u998f,\u8fa9\u8bba\u6846\u67b6,\u504f\u597d\u4f18\u5316"}}
{"id": "2506.03162", "pdf": "https://arxiv.org/pdf/2506.03162", "abs": "https://arxiv.org/abs/2506.03162", "authors": ["Damith Chamalke Senadeera", "Xiaoyun Yang", "Dimitrios Kollias", "Gregory Slabaugh"], "title": "Dual Branch VideoMamba with Gated Class Token Fusion for Violence Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid proliferation of surveillance cameras has increased the demand for\nautomated violence detection. While CNNs and Transformers have shown success in\nextracting spatio-temporal features, they struggle with long-term dependencies\nand computational efficiency. We propose Dual Branch VideoMamba with Gated\nClass Token Fusion (GCTF), an efficient architecture combining a dual-branch\ndesign and a state-space model (SSM) backbone where one branch captures spatial\nfeatures, while the other focuses on temporal dynamics, with continuous fusion\nvia a gating mechanism. We also present a new benchmark by merging RWF-2000,\nRLVS, and VioPeru datasets in video violence detection, ensuring strict\nseparation between training and testing sets. Our model achieves\nstate-of-the-art performance on this benchmark offering an optimal balance\nbetween accuracy and computational efficiency, demonstrating the promise of\nSSMs for scalable, real-time surveillance violence detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53cc\u5206\u652f\u8bbe\u8ba1\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u9ad8\u6548\u89c6\u9891\u66b4\u529b\u68c0\u6d4b\u67b6\u6784\uff0c\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u5b9e\u73b0\u7279\u5f81\u878d\u5408\uff0c\u5e76\u5728\u65b0\u57fa\u51c6\u4e0a\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u671f\u4f9d\u8d56\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u7684\u4e0d\u8db3\uff0c\u6ee1\u8db3\u5b9e\u65f6\u76d1\u63a7\u4e2d\u5bf9\u66b4\u529b\u68c0\u6d4b\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652fVideoMamba\u67b6\u6784\uff0c\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\uff0c\u901a\u8fc7\u95e8\u63a7\u7c7b\u4ee4\u724c\u878d\u5408\uff08GCTF\uff09\u5b9e\u73b0\u7a7a\u95f4\u4e0e\u65f6\u95f4\u7279\u5f81\u7684\u52a8\u6001\u878d\u5408\u3002", "result": "\u5728\u65b0\u5408\u5e76\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f18\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u5e73\u8861\u3002", "conclusion": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u5b9e\u65f6\u89c6\u9891\u66b4\u529b\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u89c6\u9891\u66b4\u529b\u68c0\u6d4b,\u72b6\u6001\u7a7a\u95f4\u6a21\u578b,\u53cc\u5206\u652f\u67b6\u6784,\u95e8\u63a7\u878d\u5408"}}
{"id": "2506.03355", "pdf": "https://arxiv.org/pdf/2506.03355", "abs": "https://arxiv.org/abs/2506.03355", "authors": ["Elias Abad Rocamora", "Christian Schlarmann", "Naman Deep Singh", "Yongtao Wu", "Matthias Hein", "Volkan Cevher"], "title": "Robustness in Both Domains: CLIP Needs a Robust Text Encoder", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Adversarial input attacks can cause a significant shift of CLIP embeddings.\nThis can affect the downstream robustness of models incorporating CLIP in the\npipeline, such as text-to-image generative models or large vision language\nmodels. While some efforts have been done towards making the CLIP image\nencoders robust, the robustness of text encoders remains unexplored. In this\nwork, we cover this gap in the literature. We propose LEAF: an efficient\nadversarial finetuning method for the text domain, with the ability to scale to\nlarge CLIP models. Our models significantly improve the zero-shot adversarial\naccuracy in the text domain, while maintaining the vision performance provided\nby robust image encoders. When combined with text-to-image diffusion models, we\ncan improve the generation quality under adversarial noise. When employing our\nrobust CLIP encoders in multimodal retrieval tasks, we improve the recall under\nadversarial noise over standard CLIP models. Finally, we show that robust text\nencoders facilitate better reconstruction of input text from its embedding via\ndirect optimization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLEAF\u7684\u9ad8\u6548\u5bf9\u6297\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347CLIP\u6587\u672c\u7f16\u7801\u5668\u7684\u9c81\u68d2\u6027\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "CLIP\u6587\u672c\u7f16\u7801\u5668\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u5c1a\u672a\u88ab\u63a2\u7d22\uff0c\u800c\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u5bf9\u4e8e\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff09\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86LEAF\u65b9\u6cd5\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u5bf9\u6297\u5fae\u8c03\u6280\u672f\uff0c\u80fd\u591f\u6269\u5c55\u5230\u5927\u578bCLIP\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u6587\u672c\u57df\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6587\u672c\u57df\u7684\u96f6\u6837\u672c\u5bf9\u6297\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u6027\u80fd\uff0c\u8fd8\u80fd\u63d0\u5347\u5728\u5bf9\u6297\u566a\u58f0\u4e0b\u7684\u751f\u6210\u8d28\u91cf\u3001\u591a\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u7684\u53ec\u56de\u7387\u4ee5\u53ca\u6587\u672c\u91cd\u5efa\u80fd\u529b\u3002", "conclusion": "LEAF\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86CLIP\u6587\u672c\u7f16\u7801\u5668\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5c55\u793a\u51fa\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "CLIP, \u5bf9\u6297\u9c81\u68d2\u6027, \u6587\u672c\u7f16\u7801\u5668, LEAF, \u96f6\u6837\u672c\u5b66\u4e60"}}
{"id": "2506.03557", "pdf": "https://arxiv.org/pdf/2506.03557", "abs": "https://arxiv.org/abs/2506.03557", "authors": ["Lin Sun", "Chuang Liu", "Peng Liu", "Bingyang Li", "Weijia Lu", "Ning Wu"], "title": "BPO: Revisiting Preference Modeling in Direct Preference Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Direct Preference Optimization (DPO) have emerged as a popular method for\naligning Large Language Models (LLMs) with human preferences. While DPO\neffectively preserves the relative ordering between chosen and rejected\nresponses through pairwise ranking losses, it often neglects absolute reward\nmagnitudes. This oversight can decrease the likelihood of chosen responses and\nincrease the risk of generating out-of-distribution responses, leading to poor\nperformance. We term this issue Degraded Chosen Responses (DCR).To address this\nissue, we propose Balanced Preference Optimization (BPO), a novel framework\nthat dynamically balances the optimization of chosen and rejected responses\nthrough two key components: balanced reward margin and gap adaptor. Unlike\nprevious methods, BPO can fundamentally resolve DPO's DCR issue, without\nintroducing additional constraints to the loss function. Experimental results\non multiple mathematical reasoning tasks show that BPO significantly\noutperforms DPO, improving accuracy by +10.1% with Llama-3.1-8B-Instruct (18.8%\nto 28.9%) and +11.7% with Qwen2.5-Math-7B (35.0% to 46.7%). It also surpasses\nDPO variants by +3.6% over IPO (43.1%), +5.0% over SLiC (41.7%), and +3.1% over\nCal-DPO (43.6%) on the same model. Remarkably, our algorithm requires only a\nsingle line of code modification, making it simple to implement and fully\ncompatible with existing DPO-based frameworks.", "AI": {"tldr": "BPO\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5e73\u8861\u9009\u62e9\u4e0e\u62d2\u7edd\u54cd\u5e94\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86DPO\u4e2d\u56e0\u5ffd\u7565\u5956\u52b1\u7edd\u5bf9\u503c\u5bfc\u81f4\u7684DCR\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u5b9e\u73b0\u7b80\u5355\u3002", "motivation": "DPO\u5728\u4f18\u5316LLM\u65f6\u4fdd\u7559\u4e86\u9009\u62e9\u4e0e\u62d2\u7edd\u54cd\u5e94\u7684\u76f8\u5bf9\u987a\u5e8f\uff0c\u4f46\u5ffd\u7565\u4e86\u5956\u52b1\u7edd\u5bf9\u503c\uff0c\u5bfc\u81f4DCR\u95ee\u9898\uff0c\u964d\u4f4e\u4e86\u9009\u62e9\u54cd\u5e94\u7684\u6982\u7387\u548c\u6a21\u578b\u8868\u73b0\u3002BPO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "BPO\u901a\u8fc7\u5e73\u8861\u5956\u52b1\u8fb9\u9645\u548c\u95f4\u9699\u9002\u914d\u5668\u52a8\u6001\u4f18\u5316\u9009\u62e9\u4e0e\u62d2\u7edd\u54cd\u5e94\uff0c\u65e0\u9700\u5728\u635f\u5931\u51fd\u6570\u4e2d\u5f15\u5165\u989d\u5916\u7ea6\u675f\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cBPO\u663e\u8457\u4f18\u4e8eDPO\u53ca\u5176\u53d8\u79cd\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe+11.7%\uff0c\u4e14\u4ec5\u9700\u5355\u884c\u4ee3\u7801\u4fee\u6539\u5373\u53ef\u5b9e\u73b0\u3002", "conclusion": "BPO\u7b80\u5355\u9ad8\u6548\u5730\u89e3\u51b3\u4e86DPO\u7684DCR\u95ee\u9898\uff0c\u540c\u65f6\u517c\u5bb9\u73b0\u6709\u6846\u67b6\uff0c\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "Direct Preference Optimization, Balanced Preference Optimization, LLM alignment, human preferences, DCR"}}
{"id": "2506.03169", "pdf": "https://arxiv.org/pdf/2506.03169", "abs": "https://arxiv.org/abs/2506.03169", "authors": ["Arindam Chaudhuri"], "title": "Improvement of human health lifespan with hybrid group pose estimation methods", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human beings rely heavily on estimation of poses in order to access their\nbody movements. Human pose estimation methods take advantage of computer vision\nadvances in order to track human body movements in real life applications. This\ncomes from videos which are recorded through available devices. These\npara-digms provide potential to make human movement measurement more accessible\nto users. The consumers of pose estimation movements believe that human poses\ncontent tend to supplement available videos. This has increased pose estimation\nsoftware usage to estimate human poses. In order to address this problem, we\ndevelop hybrid-ensemble-based group pose estimation method to improve human\nhealth. This proposed hybrid-ensemble-based group pose estimation method aims\nto detect multi-person poses using modified group pose estimation and modified\nreal time pose estimation. This ensemble allows fusion of performance of stated\nmethods in real time. The input poses from images are fed into individual\nmeth-ods. The pose transformation method helps to identify relevant features\nfor en-semble to perform training effectively. After this, customized\npre-trained hybrid ensemble is trained on public benchmarked datasets which is\nbeing evaluated through test datasets. The effectiveness and viability of\nproposed method is estab-lished based on comparative analysis of group pose\nestimation methods and ex-periments conducted on benchmarked datasets. It\nprovides best optimized results in real-time pose estimation. It makes pose\nestimation method more robust to oc-clusion and improves dense regression\naccuracy. These results have affirmed po-tential application of this method in\nseveral real-time situations with improvement in human health life span", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u96c6\u6210\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6539\u8fdb\u7684\u7fa4\u7ec4\u59ff\u6001\u4f30\u8ba1\u548c\u5b9e\u65f6\u59ff\u6001\u4f30\u8ba1\u6280\u672f\uff0c\u4f18\u5316\u591a\u4eba\u59ff\u6001\u68c0\u6d4b\uff0c\u63d0\u5347\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5229\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5b9e\u65f6\u8ffd\u8e2a\u4eba\u4f53\u52a8\u4f5c\uff0c\u63d0\u9ad8\u59ff\u6001\u4f30\u8ba1\u7684\u53ef\u7528\u6027\u548c\u51c6\u786e\u6027\uff0c\u4ee5\u6539\u5584\u4eba\u7c7b\u5065\u5eb7\u3002", "method": "\u91c7\u7528\u6df7\u5408\u96c6\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u7fa4\u7ec4\u59ff\u6001\u4f30\u8ba1\u548c\u5b9e\u65f6\u59ff\u6001\u4f30\u8ba1\u6280\u672f\uff0c\u901a\u8fc7\u7279\u5f81\u63d0\u53d6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u59ff\u6001\u4f30\u8ba1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5bf9\u906e\u6321\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u9ad8\u4e86\u5bc6\u96c6\u56de\u5f52\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u6539\u5584\u4eba\u7c7b\u5065\u5eb7\u751f\u547d\u5468\u671f\u3002", "keywords": "\u59ff\u6001\u4f30\u8ba1, \u6df7\u5408\u96c6\u6210, \u5b9e\u65f6\u68c0\u6d4b, \u8ba1\u7b97\u673a\u89c6\u89c9, \u4eba\u7c7b\u5065\u5eb7"}}
{"id": "2506.03363", "pdf": "https://arxiv.org/pdf/2506.03363", "abs": "https://arxiv.org/abs/2506.03363", "authors": ["Divya Shyamal", "Jiaqi Zhang", "Caroline Uhler"], "title": "Probabilistic Factorial Experimental Design for Combinatorial Interventions", "categories": ["cs.LG", "stat.ME", "stat.ML"], "comment": null, "summary": "A combinatorial intervention, consisting of multiple treatments applied to a\nsingle unit with potentially interactive effects, has substantial applications\nin fields such as biomedicine, engineering, and beyond. Given $p$ possible\ntreatments, conducting all possible $2^p$ combinatorial interventions can be\nlaborious and quickly becomes infeasible as $p$ increases. Here we introduce\nprobabilistic factorial experimental design, formalized from how scientists\nperform lab experiments. In this framework, the experimenter selects a dosage\nfor each possible treatment and applies it to a group of units. Each unit\nindependently receives a random combination of treatments, sampled from a\nproduct Bernoulli distribution determined by the dosages. Additionally, the\nexperimenter can carry out such experiments over multiple rounds, adapting the\ndesign in an active manner. We address the optimal experimental design problem\nwithin an intervention model that imposes bounded-degree interactions between\ntreatments. In the passive setting, we provide a closed-form solution for the\nnear-optimal design. Our results prove that a dosage of $\\tfrac{1}{2}$ for each\ntreatment is optimal up to a factor of $1+O(\\tfrac{\\ln(n)}{n})$ for estimating\nany $k$-way interaction model, regardless of $k$, and imply that\n$O\\big(kp^{3k}\\ln(p)\\big)$ observations are required to accurately estimate\nthis model. For the multi-round setting, we provide a near-optimal acquisition\nfunction that can be numerically optimized. We also explore several extensions\nof the design problem and finally validate our findings through simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u6982\u7387\u56e0\u5b50\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u7814\u7a76\u591a\u6cbb\u7597\u7ec4\u5408\u5e72\u9884\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u4e0d\u53ef\u884c\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf$2^p$\u7ec4\u5408\u5b9e\u9a8c\u65b9\u6cd5\u5728$p$\u589e\u52a0\u65f6\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6982\u7387\u56e0\u5b50\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4f2f\u52aa\u5229\u5206\u5e03\u968f\u673a\u5206\u914d\u6cbb\u7597\u7ec4\u5408\uff0c\u5e76\u652f\u6301\u591a\u8f6e\u4e3b\u52a8\u8c03\u6574\u3002", "result": "\u8bc1\u660e$\tfrac{1}{2}$\u5242\u91cf\u5728\u88ab\u52a8\u8bbe\u7f6e\u4e2d\u8fd1\u6700\u4f18\uff0c\u6240\u9700\u89c2\u6d4b\u6570\u4e3a$O(kp^{3k}\\ln(p))$\u3002", "conclusion": "\u65b9\u6cd5\u9ad8\u6548\u4e14\u7075\u6d3b\uff0c\u9002\u7528\u4e8e\u591a\u8f6e\u5b9e\u9a8c\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "keywords": "\u7ec4\u5408\u5e72\u9884, \u6982\u7387\u56e0\u5b50\u8bbe\u8ba1, \u591a\u8f6e\u5b9e\u9a8c, \u4ea4\u4e92\u6a21\u578b"}}
{"id": "2506.03558", "pdf": "https://arxiv.org/pdf/2506.03558", "abs": "https://arxiv.org/abs/2506.03558", "authors": ["Jiawei Chen", "Xinyan Guan", "Qianhao Yuan", "Guozhao Mo", "Weixiang Zhou", "Yaojie Lu", "Hongyu Lin", "Ben He", "Le Sun", "Xianpei Han"], "title": "ConsistentChat: Building Skeleton-Guided Consistent Dialogues for Large Language Models from Scratch", "categories": ["cs.CL"], "comment": null, "summary": "Current instruction data synthesis methods primarily focus on single-turn\ninstructions and often neglect cross-turn coherence, resulting in context drift\nand reduced task completion rates in extended conversations. To address this\nlimitation, we propose Skeleton-Guided Multi-Turn Dialogue Generation, a\nframework that constrains multi-turn instruction synthesis by explicitly\nmodeling human conversational intent. It operates in two stages: (1) Intent\nModeling, which captures the global structure of human dialogues by assigning\neach conversation to one of nine well-defined intent trajectories, ensuring a\ncoherent and goal-oriented information flow; and (2) Skeleton Generation, which\nconstructs a structurally grounded sequence of user queries aligned with the\nmodeled intent, thereby serving as a scaffold that constrains and guides the\ndownstream instruction synthesis process. Based on this process, we construct\nConsistentChat, a multi-turn instruction dataset with approximately 15,000\nmulti-turn conversations and 224,392 utterances. Experiments on the Light,\nTopdial, and MT-Eval benchmarks show that models fine-tuned on ConsistentChat\nachieve a 20-30% improvement in chat consistency and up to a 15% increase in\ntask success rate, significantly outperforming models trained on existing\nsingle-turn and multi-turn instruction datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9aa8\u67b6\u5f15\u5bfc\u7684\u591a\u8f6e\u5bf9\u8bdd\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u660e\u786e\u5efa\u6a21\u4eba\u7c7b\u5bf9\u8bdd\u610f\u56fe\u89e3\u51b3\u73b0\u6709\u5355\u8f6e\u6307\u4ee4\u5408\u6210\u65b9\u6cd5\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u4e0a\u4e0b\u6587\u6f02\u79fb\u548c\u4efb\u52a1\u5b8c\u6210\u7387\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u6307\u4ee4\u5408\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u8f6e\u6307\u4ee4\uff0c\u5ffd\u89c6\u4e86\u591a\u8f6e\u5bf9\u8bdd\u7684\u8de8\u8f6e\u8fde\u8d2f\u6027\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u6f02\u79fb\u548c\u4efb\u52a1\u5b8c\u6210\u7387\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a(1)\u610f\u56fe\u5efa\u6a21\uff0c\u901a\u8fc7\u5c06\u5bf9\u8bdd\u5206\u7c7b\u5230\u4e5d\u79cd\u9884\u5b9a\u4e49\u7684\u610f\u56fe\u8f68\u8ff9\u4e2d\uff0c\u786e\u4fdd\u8fde\u8d2f\u6027\uff1b(2)\u9aa8\u67b6\u751f\u6210\uff0c\u6784\u5efa\u4e0e\u610f\u56fe\u5bf9\u9f50\u7684\u7528\u6237\u67e5\u8be2\u5e8f\u5217\u4f5c\u4e3a\u540e\u7eed\u6307\u4ee4\u5408\u6210\u7684\u7ea6\u675f\u3002", "result": "\u6784\u5efa\u4e86ConsistentChat\u6570\u636e\u96c6\uff0815,000\u5bf9\u8bdd\uff0c224,392\u6761\u8bed\u53e5\uff09\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728Light\u3001Topdial\u548cMT-Eval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u804a\u5929\u4e00\u81f4\u6027\u548c\u4efb\u52a1\u6210\u529f\u7387\u5206\u522b\u63d0\u534720-30%\u548c15%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u8f6e\u5bf9\u8bdd\u7684\u8fde\u8d2f\u6027\u548c\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u5355\u8f6e\u548c\u591a\u8f6e\u6307\u4ee4\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u3002", "keywords": "\u591a\u8f6e\u5bf9\u8bdd,\u610f\u56fe\u5efa\u6a21,\u9aa8\u67b6\u751f\u6210,ConsistentChat,\u4efb\u52a1\u5b8c\u6210\u7387"}}
{"id": "2506.03170", "pdf": "https://arxiv.org/pdf/2506.03170", "abs": "https://arxiv.org/abs/2506.03170", "authors": ["Murthy L", "Subarna Tripathi"], "title": "PALADIN : Robust Neural Fingerprinting for Text-to-Image Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The risk of misusing text-to-image generative models for malicious uses,\nespecially due to the open-source development of such models, has become a\nserious concern. As a risk mitigation strategy, attributing generative models\nwith neural fingerprinting is emerging as a popular technique. There has been a\nplethora of recent work that aim for addressing neural fingerprinting. A\ntrade-off between the attribution accuracy and generation quality of such\nmodels has been studied extensively. None of the existing methods yet achieved\n$100\\%$ attribution accuracy. However, any model with less than \\emph{perfect}\naccuracy is practically non-deployable. In this work, we propose an accurate\nmethod to incorporate neural fingerprinting for text-to-image diffusion models\nleveraging the concepts of cyclic error correcting codes from the literature of\ncoding theory.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5faa\u73af\u7ea0\u9519\u7801\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u51c6\u786e\u7684\u795e\u7ecf\u6307\u7eb9\u6807\u8bb0\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u8fbe\u5230100%\u51c6\u786e\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u5f00\u6e90\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u6076\u610f\u4f7f\u7528\u98ce\u9669\u589e\u52a0\uff0c\u795e\u7ecf\u6307\u7eb9\u6807\u8bb0\u6210\u4e3a\u4e00\u79cd\u6d41\u884c\u7684\u98ce\u9669\u7f13\u89e3\u7b56\u7565\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u8fbe\u5230100%\u7684\u51c6\u786e\u6027\uff0c\u8fd9\u662f\u5b9e\u9645\u90e8\u7f72\u7684\u5173\u952e\u969c\u788d\u3002", "method": "\u5229\u7528\u7f16\u7801\u7406\u8bba\u4e2d\u7684\u5faa\u73af\u7ea0\u9519\u7801\u6982\u5ff5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u795e\u7ecf\u6307\u7eb9\u6807\u8bb0\u7684\u51c6\u786e\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u76ee\u6807\u662f\u5b9e\u73b0\u5b8c\u7f8e\u7684\uff08100%\uff09\u6307\u7eb9\u6807\u8bb0\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5faa\u73af\u7ea0\u9519\u7801\uff0c\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u514b\u670d\u5f53\u524d\u795e\u7ecf\u6307\u7eb9\u6807\u8bb0\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "keywords": "\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210,\u795e\u7ecf\u6307\u7eb9\u6807\u8bb0,\u5faa\u73af\u7ea0\u9519\u7801,\u6269\u6563\u6a21\u578b,\u6076\u610f\u4f7f\u7528\u98ce\u9669"}}
{"id": "2506.03370", "pdf": "https://arxiv.org/pdf/2506.03370", "abs": "https://arxiv.org/abs/2506.03370", "authors": ["Leonid Ryvkin"], "title": "Comparison of different Unique hard attention transformer models by the formal languages they can recognize", "categories": ["cs.LG", "cs.CL", "cs.FL"], "comment": null, "summary": "This note is a survey of various results on the capabilities of unique hard\nattention transformers encoders (UHATs) to recognize formal languages. We\ndistinguish between masked vs. non-masked, finite vs. infinite image and\ngeneral vs. bilinear attention score functions. We recall some relations\nbetween these models, as well as a lower bound in terms of first-order logic\nand an upper bound in terms of circuit complexity.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5173\u4e8e\u72ec\u7279\u786c\u6ce8\u610f\u529b\u53d8\u538b\u5668\u7f16\u7801\u5668\uff08UHATs\uff09\u8bc6\u522b\u5f62\u5f0f\u8bed\u8a00\u80fd\u529b\u7684\u591a\u79cd\u7ed3\u679c\u3002", "motivation": "\u63a2\u8ba8UHATs\u5728\u4e0d\u540c\u914d\u7f6e\u4e0b\u7684\u8bed\u8a00\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u533a\u5206\u4e86\u63a9\u7801\u4e0e\u975e\u63a9\u7801\u3001\u6709\u9650\u4e0e\u65e0\u9650\u56fe\u50cf\u4ee5\u53ca\u901a\u7528\u4e0e\u53cc\u7ebf\u6027\u6ce8\u610f\u529b\u5f97\u5206\u51fd\u6570\u3002", "result": "\u603b\u7ed3\u4e86\u6a21\u578b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u4e00\u9636\u903b\u8f91\u7684\u4e0b\u754c\u548c\u7535\u8def\u590d\u6742\u5ea6\u7684\u4e0a\u754c\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u5bf9UHATs\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u56de\u987e\u4e0e\u7406\u8bba\u754c\u9650\u3002", "keywords": "\u72ec\u7279\u786c\u6ce8\u610f\u529b\u53d8\u538b\u5668\u7f16\u7801\u5668, \u5f62\u5f0f\u8bed\u8a00, \u7406\u8bba\u754c\u9650"}}
{"id": "2506.03566", "pdf": "https://arxiv.org/pdf/2506.03566", "abs": "https://arxiv.org/abs/2506.03566", "authors": ["Langlin Huang", "Chengsong Huang", "Jixuan Leng", "Di Huang", "Jiaxin Huang"], "title": "POSS: Position Specialist Generates Better Draft for Speculative Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Speculative decoding accelerates Large Language Model (LLM) inference by\nusing a small draft model to predict multiple tokens, and a large target model\nto verify these tokens in parallel. Recent studies leverage the hidden state of\nthe target model to enhance draft model prediction accuracy. However, existing\nmethods suffer from the degrading quality of draft token predictions at later\npositions, due to error accumulation in draft model generated features. In this\npaper, we propose Position Specialists (PosS), which consist of multiple\nposition-specialized draft layers to generate tokens at assigned position(s).\nPosition specialists greatly improve token acceptance rate at later positions\nper drafting round, as each specialist only needs to focus on handling a\ncertain level of draft model feature deviation. Experiment results on\nLlama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that\nPosS effectively improves over baselines on average acceptance length and\nspeed-up ratio. Our codebase is available at https://github.com/shrango/PosS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPosS\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u4e2a\u4f4d\u7f6e\u4e13\u7528\u7684\u8349\u7a3f\u5c42\u6765\u6539\u8fdb\u63a8\u6d4b\u89e3\u7801\u4e2d\u7684\u8349\u7a3f\u6a21\u578b\u9884\u6d4b\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u540e\u671f\u4f4d\u7f6e\u7684\u4ee4\u724c\u63a5\u53d7\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u56e0\u8349\u7a3f\u6a21\u578b\u751f\u6210\u7279\u5f81\u7684\u9519\u8bef\u79ef\u7d2f\uff0c\u5bfc\u81f4\u540e\u671f\u4ee4\u724c\u9884\u6d4b\u8d28\u91cf\u4e0b\u964d\uff0c\u9700\u6539\u8fdb\u3002", "method": "PosS\u4f7f\u7528\u591a\u4e2a\u4f4d\u7f6e\u4e13\u7528\u7684\u8349\u7a3f\u5c42\uff08Position Specialists\uff09\uff0c\u6bcf\u4e2a\u5c42\u4e13\u6ce8\u4e8e\u5904\u7406\u7279\u5b9a\u4f4d\u7f6e\u7684\u8349\u7a3f\u6a21\u578b\u7279\u5f81\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPosS\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5e73\u5747\u63a5\u53d7\u957f\u5ea6\u548c\u52a0\u901f\u6bd4\u3002", "conclusion": "PosS\u901a\u8fc7\u4e13\u7528\u8349\u7a3f\u5c42\u6709\u6548\u89e3\u51b3\u4e86\u540e\u671f\u4ee4\u724c\u9884\u6d4b\u8d28\u91cf\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u5347\u4e86LLM\u63a8\u7406\u6548\u7387\u3002", "keywords": "\u63a8\u6d4b\u89e3\u7801, \u5927\u8bed\u8a00\u6a21\u578b, \u4f4d\u7f6e\u4e13\u7528\u8349\u7a3f\u5c42, \u4ee4\u724c\u63a5\u53d7\u7387"}}
{"id": "2506.03171", "pdf": "https://arxiv.org/pdf/2506.03171", "abs": "https://arxiv.org/abs/2506.03171", "authors": ["Ghulam Mujtaba", "Eun-Seok Ryu"], "title": "EdgeVidSum: Real-Time Personalized Video Summarization at the Edge", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "EdgeVidSum is a lightweight method that generates personalized, fast-forward\nsummaries of long-form videos directly on edge devices. The proposed approach\nenables real-time video summarization while safeguarding user privacy through\nlocal data processing using innovative thumbnail-based techniques and efficient\nneural architectures. Unlike conventional methods that process entire videos\nframe by frame, the proposed method uses thumbnail containers to significantly\nreduce computational complexity without sacrificing semantic relevance. The\nframework employs a hierarchical analysis approach, where a lightweight 2D CNN\nmodel identifies user-preferred content from thumbnails and generates\ntimestamps to create fast-forward summaries. Our interactive demo highlights\nthe system's ability to create tailored video summaries for long-form videos,\nsuch as movies, sports events, and TV shows, based on individual user\npreferences. The entire computation occurs seamlessly on resource-constrained\ndevices like Jetson Nano, demonstrating how EdgeVidSum addresses the critical\nchallenges of computational efficiency, personalization, and privacy in modern\nvideo consumption environments.", "AI": {"tldr": "EdgeVidSum\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u76f4\u63a5\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u751f\u6210\u957f\u89c6\u9891\u7684\u4e2a\u6027\u5316\u5feb\u8fdb\u6458\u8981\uff0c\u901a\u8fc7\u672c\u5730\u6570\u636e\u5904\u7406\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u91c7\u7528\u521b\u65b0\u7684\u7f29\u7565\u56fe\u6280\u672f\u548c\u9ad8\u6548\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002", "motivation": "\u73b0\u4ee3\u89c6\u9891\u6d88\u8d39\u73af\u5883\u4e2d\uff0c\u8ba1\u7b97\u6548\u7387\u3001\u4e2a\u6027\u5316\u548c\u9690\u79c1\u662f\u5173\u952e\u6311\u6218\uff0cEdgeVidSum\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7f29\u7565\u56fe\u5bb9\u5668\u548c\u8f7b\u91cf\u7ea72D CNN\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u5206\u6790\u8bc6\u522b\u7528\u6237\u504f\u597d\u7684\u5185\u5bb9\u5e76\u751f\u6210\u65f6\u95f4\u6233\uff0c\u521b\u5efa\u5feb\u8fdb\u6458\u8981\u3002", "result": "\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\uff08\u5982Jetson Nano\uff09\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u89c6\u9891\u6458\u8981\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u76f8\u5173\u6027\u3002", "conclusion": "EdgeVidSum\u5c55\u793a\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u3001\u4e2a\u6027\u5316\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u89c6\u9891\u6458\u8981\u80fd\u529b\u3002", "keywords": "\u89c6\u9891\u6458\u8981, \u8fb9\u7f18\u8ba1\u7b97, \u4e2a\u6027\u5316, \u9690\u79c1\u4fdd\u62a4, \u8f7b\u91cf\u7ea7\u6a21\u578b"}}
{"id": "2506.03374", "pdf": "https://arxiv.org/pdf/2506.03374", "abs": "https://arxiv.org/abs/2506.03374", "authors": ["Haley Dozier", "Althea Henslee", "Ashley Abraham", "Andrew Strelzoff", "Mark Chappell"], "title": "Product Quantization for Surface Soil Similarity", "categories": ["cs.LG"], "comment": "To be published in the CSCE 2022 proceedings", "summary": "The use of machine learning (ML) techniques has allowed rapid advancements in\nmany scientific and engineering fields. One of these problems is that of\nsurface soil taxonomy, a research area previously hindered by the reliance on\nhuman-derived classifications, which are mostly dependent on dividing a dataset\nbased on historical understandings of that data rather than data-driven,\nstatistically observable similarities. Using a ML-based taxonomy allows soil\nresearchers to move beyond the limitations of human visualization and create\nclassifications of high-dimension datasets with a much higher level of\nspecificity than possible with hand-drawn taxonomies. Furthermore, this\npipeline allows for the possibility of producing both highly accurate and\nflexible soil taxonomies with classes built to fit a specific application. The\nmachine learning pipeline outlined in this work combines product quantization\nwith the systematic evaluation of parameters and output to get the best\navailable results, rather than accepting sub-optimal results by using either\ndefault settings or best guess settings.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u5728\u571f\u58e4\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u63d0\u5347\u4e86\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u571f\u58e4\u5206\u7c7b\u4f9d\u8d56\u4e8e\u4eba\u4e3a\u5212\u5206\uff0c\u7f3a\u4e4f\u6570\u636e\u9a71\u52a8\u7684\u7edf\u8ba1\u57fa\u7840\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u514b\u670d\u8fd9\u4e00\u5c40\u9650\u3002", "method": "\u7ed3\u5408\u4e58\u79ef\u91cf\u5316\u548c\u7cfb\u7edf\u53c2\u6570\u8bc4\u4f30\u7684\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\uff0c\u4f18\u5316\u5206\u7c7b\u7ed3\u679c\u3002", "result": "\u63d0\u51fa\u4e86\u9ad8\u7cbe\u5ea6\u4e14\u7075\u6d3b\u7684\u571f\u58e4\u5206\u7c7b\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edf\u4eba\u5de5\u5206\u7c7b\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u4e3a\u571f\u58e4\u5206\u7c7b\u63d0\u4f9b\u4e86\u66f4\u79d1\u5b66\u3001\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u673a\u5668\u5b66\u4e60, \u571f\u58e4\u5206\u7c7b, \u6570\u636e\u9a71\u52a8, \u9ad8\u7ef4\u5ea6\u6570\u636e\u96c6"}}
{"id": "2506.03569", "pdf": "https://arxiv.org/pdf/2506.03569", "abs": "https://arxiv.org/abs/2506.03569", "authors": ["Xiaomi LLM-Core Team", ":", "Zihao Yue", "Zhenru Lin", "Yifan Song", "Weikun Wang", "Shuhuai Ren", "Shuhao Gu", "Shicheng Li", "Peidian Li", "Liang Zhao", "Lei Li", "Kainan Bao", "Hao Tian", "Hailin Zhang", "Gang Wang", "Dawei Zhu", "Cici", "Chenhong He", "Bowen Ye", "Bowen Shen", "Zihan Zhang", "Zihan Jiang", "Zhixian Zheng", "Zhichao Song", "Zhenbo Luo", "Yue Yu", "Yudong Wang", "Yuanyuan Tian", "Yu Tu", "Yihan Yan", "Yi Huang", "Xu Wang", "Xinzhe Xu", "Xingchen Song", "Xing Zhang", "Xing Yong", "Xin Zhang", "Xiangwei Deng", "Wenyu Yang", "Wenhan Ma", "Weiwei Lv", "Weiji Zhuang", "Wei Liu", "Sirui Deng", "Shuo Liu", "Shimao Chen", "Shihua Yu", "Shaohui Liu", "Shande Wang", "Rui Ma", "Qiantong Wang", "Peng Wang", "Nuo Chen", "Menghang Zhu", "Kangyang Zhou", "Kang Zhou", "Kai Fang", "Jun Shi", "Jinhao Dong", "Jiebao Xiao", "Jiaming Xu", "Huaqiu Liu", "Hongshen Xu", "Heng Qu", "Haochen Zhao", "Hanglong Lv", "Guoan Wang", "Duo Zhang", "Dong Zhang", "Di Zhang", "Chong Ma", "Chang Liu", "Can Cai", "Bingquan Xia"], "title": "MiMo-VL Technical Report", "categories": ["cs.CL"], "comment": "32 pages", "summary": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language\nmodels delivering state-of-the-art performance in both general visual\nunderstanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B\non 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing\nmodels with up to 78B parameters. For GUI grounding applications, it sets a new\nstandard with 56.1 on OSWorld-G, even outperforming specialized models such as\nUI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)\nwith Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward\nsignals. We identify the importance of incorporating high-quality reasoning\ndata with long Chain-of-Thought into pre-training stages, and the benefits of\nmixed RL despite challenges in simultaneous multi-domain optimization. We also\ncontribute a comprehensive evaluation suite covering 50+ tasks to promote\nreproducibility and advance the field. The model checkpoints and full\nevaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.", "AI": {"tldr": "\u5f00\u6e90\u4e86MiMo-VL-7B-SFT\u548cMiMo-VL-7B-RL\u4e24\u4e2a\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5728GUI\u4efb\u52a1\u4e2d\u53d6\u5f97\u65b0\u7a81\u7834\u3002", "motivation": "\u5f00\u53d1\u9ad8\u6027\u80fd\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u901a\u7528\u89c6\u89c9\u7406\u89e3\u548c\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u89e3\u51b3\u591a\u9886\u57df\u4f18\u5316\u6311\u6218\u3002", "method": "\u56db\u9636\u6bb5\u9884\u8bad\u7ec3\uff082.4\u4e07\u4ebftoken\uff09\u7ed3\u5408\u6df7\u5408\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08MORL\uff09\uff0c\u6574\u5408\u591a\u6837\u5316\u5956\u52b1\u4fe1\u53f7\u3002", "result": "MiMo-VL-7B-RL\u572840\u9879\u4efb\u52a1\u4e2d35\u9879\u4f18\u4e8eQwen2.5-VL-7B\uff0cOlympiadBench\u5f97\u520659.4\uff0cOSWorld-G\u5f97\u520656.1\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u63a8\u7406\u6570\u636e\u548c\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u5bf9\u63d0\u5347\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5168\u9762\u8bc4\u4f30\u5957\u4ef6\u4fc3\u8fdb\u7814\u7a76\u53d1\u5c55\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u5f3a\u5316\u5b66\u4e60, \u591a\u6a21\u6001\u63a8\u7406, GUI, \u5f00\u6e90"}}
{"id": "2506.03173", "pdf": "https://arxiv.org/pdf/2506.03173", "abs": "https://arxiv.org/abs/2506.03173", "authors": ["Xiaoyi Liu", "Hao Tang"], "title": "FOLIAGE: Towards Physical Intelligence World Models Via Unbounded Surface Evolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Physical intelligence -- anticipating and shaping the world from partial,\nmultisensory observations -- is critical for next-generation world models. We\npropose FOLIAGE, a physics-informed multimodal world model for unbounded\naccretive surface growth. In its Action-Perception loop, a unified context\nencoder maps images, mesh connectivity, and point clouds to a shared latent\nstate. A physics-aware predictor, conditioned on physical control actions,\nadvances this latent state in time to align with the target latent of the\nsurface, yielding a Modality-Agnostic Growth Embedding (MAGE) that interfaces\nwith critic heads for downstream objectives. FOLIAGE's Accretive Graph Network\n(AGN) captures dynamic connectivity through Age Positional Encoding and\nEnergy-Gated Message-Passing. Geometry-Correspondence Fusion and Cross-Patch\nMasking enhance MAGE's expressiveness, while Hierarchical Pooling balances\nglobal context with local dynamics. We create SURF-GARDEN, a world model\nlearning platform comprising a Counterfactual Physics Simulator, a Multimodal\nCorrespondence Extractor, and Evolution Tracing, which generates 7,200 diverse\nsurface-growth sequences. SURF-BENCH, our physical-intelligence evaluation\nsuite, evaluates six core tasks -- topology recognition, inverse material\nestimation, growth-stage classification, latent roll-out, cross-modal\nretrieval, and dense correspondence -- and four stress tests -- sensor dropout,\nzero-shot modality transfer, long-horizon prediction, and physics ablation --\nto probe resilience. FOLIAGE outperforms specialized baselines while remaining\nrobust across dynamic environments, establishing a new world-model based,\nmultimodal pathway to physical intelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86FOLIAGE\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u3001\u7269\u7406\u542f\u53d1\u7684\u4e16\u754c\u6a21\u578b\uff0c\u7528\u4e8e\u65e0\u754c\u8868\u9762\u589e\u957f\u3002\u901a\u8fc7\u7edf\u4e00\u7f16\u7801\u548c\u7269\u7406\u611f\u77e5\u9884\u6d4b\uff0c\u7ed3\u5408SURF-GARDEN\u5e73\u53f0\u548cSURF-BENCH\u8bc4\u4f30\uff0c\u6a21\u578b\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7269\u7406\u667a\u80fd\u662f\u4e0b\u4e00\u4ee3\u4e16\u754c\u6a21\u578b\u7684\u5173\u952e\uff0c\u800c\u73b0\u6709\u6a21\u578b\u5728\u65e0\u754c\u8868\u9762\u589e\u957f\u7684\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002", "method": "FOLIAGE\u901a\u8fc7\u7edf\u4e00\u7f16\u7801\u5668\u548c\u7269\u7406\u611f\u77e5\u9884\u6d4b\u5668\u751f\u6210MAGE\u5d4c\u5165\uff0c\u5229\u7528AGN\u6355\u6349\u52a8\u6001\u8fde\u63a5\u6027\uff0c\u5e76\u7ed3\u5408\u51e0\u4f55\u5bf9\u5e94\u878d\u5408\u4e0e\u5c42\u6b21\u6c60\u5316\u3002", "result": "\u6a21\u578b\u5728\u516d\u9879\u6838\u5fc3\u4efb\u52a1\u548c\u56db\u9879\u538b\u529b\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5c55\u73b0\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "FOLIAGE\u4e3a\u7269\u7406\u667a\u80fd\u63d0\u4f9b\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u4e16\u754c\u6a21\u578b\u7684\u65b0\u9014\u5f84\u3002", "keywords": "\u7269\u7406\u667a\u80fd, \u591a\u6a21\u6001\u4e16\u754c\u6a21\u578b, \u8868\u9762\u589e\u957f, \u52a8\u6001\u8fde\u63a5\u6027, \u51e0\u4f55\u5bf9\u5e94\u878d\u5408"}}
{"id": "2506.03392", "pdf": "https://arxiv.org/pdf/2506.03392", "abs": "https://arxiv.org/abs/2506.03392", "authors": ["Aref Ghoreishee", "Abhishek Mishra", "John Walsh", "Anup Das", "Nagarajan Kandasamy"], "title": "Improving Performance of Spike-based Deep Q-Learning using Ternary Neurons", "categories": ["cs.LG", "cs.NE", "cs.SY", "eess.SY"], "comment": null, "summary": "We propose a new ternary spiking neuron model to improve the representation\ncapacity of binary spiking neurons in deep Q-learning. Although a ternary\nneuron model has recently been introduced to overcome the limited\nrepresentation capacity offered by the binary spiking neurons, we show that its\nperformance is worse than that of binary models in deep Q-learning tasks. We\nhypothesize gradient estimation bias during the training process as the\nunderlying potential cause through mathematical and empirical analysis. We\npropose a novel ternary spiking neuron model to mitigate this issue by reducing\nthe estimation bias. We use the proposed ternary spiking neuron as the\nfundamental computing unit in a deep spiking Q-learning network (DSQN) and\nevaluate the network's performance in seven Atari games from the Gym\nenvironment. Results show that the proposed ternary spiking neuron mitigates\nthe drastic performance degradation of ternary neurons in Q-learning tasks and\nimproves the network performance compared to the existing binary neurons,\nmaking DSQN a more practical solution for on-board autonomous decision-making\ntasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e09\u5143\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u5347\u4e8c\u5143\u8109\u51b2\u795e\u7ecf\u5143\u5728\u6df1\u5ea6Q\u5b66\u4e60\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u4e09\u5143\u8109\u51b2\u795e\u7ecf\u5143\u5728\u6df1\u5ea6Q\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u5982\u4e8c\u5143\u6a21\u578b\uff0c\u5f52\u56e0\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u68af\u5ea6\u4f30\u8ba1\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4e09\u5143\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u51cf\u5c11\u68af\u5ea6\u4f30\u8ba1\u504f\u5dee\uff0c\u5e76\u6784\u5efa\u6df1\u5ea6\u8109\u51b2Q\u5b66\u4e60\u7f51\u7edc\uff08DSQN\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u4e03\u4e2aAtari\u6e38\u620f\u4e2d\u6d4b\u8bd5\uff0c\u65b0\u6a21\u578b\u663e\u8457\u6539\u5584\u4e86\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u4e8c\u5143\u795e\u7ecf\u5143\u3002", "conclusion": "\u65b0\u4e09\u5143\u8109\u51b2\u795e\u7ecf\u5143\u6a21\u578b\u4e3a\u81ea\u4e3b\u51b3\u7b56\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u4e09\u5143\u8109\u51b2\u795e\u7ecf\u5143\u3001\u6df1\u5ea6Q\u5b66\u4e60\u3001\u68af\u5ea6\u4f30\u8ba1\u504f\u5dee\u3001DSQN\u3001Atari\u6e38\u620f"}}
{"id": "2506.03570", "pdf": "https://arxiv.org/pdf/2506.03570", "abs": "https://arxiv.org/abs/2506.03570", "authors": ["Lin Sun", "Chuang Liu", "Xiaofeng Ma", "Tao Yang", "Weijia Lu", "Ning Wu"], "title": "FreePRM: Training Process Reward Models Without Ground Truth Process Labels", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated that\nProcess Reward Models (PRMs) play a crucial role in enhancing model\nperformance. However, training PRMs typically requires step-level labels,\neither manually annotated or automatically generated, which can be costly and\ndifficult to obtain at scale. To address this challenge, we introduce FreePRM,\na weakly supervised framework for training PRMs without access to ground-truth\nstep-level labels. FreePRM first generates pseudo step-level labels based on\nthe correctness of final outcome, and then employs Buffer Probability to\neliminate impact of noise inherent in pseudo labeling. Experimental results\nshow that FreePRM achieves an average F1 score of 53.0% on ProcessBench,\noutperforming fully supervised PRM trained on Math-Shepherd by +24.1%. Compared\nto other open-source PRMs, FreePRM outperforms upon RLHFlow-PRM-Mistral-8B\n(28.4%) by +24.6%, EurusPRM (31.3%) by +21.7%, and Skywork-PRM-7B (42.1%) by\n+10.9%. This work introduces a new paradigm in PRM training, significantly\nreducing reliance on costly step-level annotations while maintaining strong\nperformance.", "AI": {"tldr": "FreePRM\u662f\u4e00\u79cd\u5f31\u76d1\u7763\u6846\u67b6\uff0c\u65e0\u9700\u771f\u5b9e\u6b65\u9aa4\u7ea7\u6807\u7b7e\u5373\u53ef\u8bad\u7ec3\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u4f2a\u6807\u7b7e\u548c\u7f13\u51b2\u6982\u7387\u964d\u4f4e\u566a\u58f0\u5f71\u54cd\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u8bad\u7ec3\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u9700\u8981\u9ad8\u6210\u672c\u6b65\u9aa4\u7ea7\u6807\u6ce8\u7684\u95ee\u9898\u3002", "method": "\u751f\u6210\u4f2a\u6b65\u9aa4\u7ea7\u6807\u7b7e\u5e76\u7ed3\u5408\u7f13\u51b2\u6982\u7387\u6d88\u9664\u566a\u58f0\u3002", "result": "\u5728ProcessBench\u4e0a\u5e73\u5747F1\u5f97\u5206\u4e3a53.0%\uff0c\u4f18\u4e8e\u591a\u4e2a\u73b0\u6709PRM\u6a21\u578b\u3002", "conclusion": "FreePRM\u4e3aPRM\u8bad\u7ec3\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u51cf\u5c11\u5bf9\u6807\u6ce8\u7684\u4f9d\u8d56\u5e76\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "keywords": "PRM, FreePRM, \u5f31\u76d1\u7763, \u4f2a\u6807\u7b7e, \u7f13\u51b2\u6982\u7387"}}
{"id": "2506.03174", "pdf": "https://arxiv.org/pdf/2506.03174", "abs": "https://arxiv.org/abs/2506.03174", "authors": ["Koki Matsuishi", "Kosuke Ukita", "Tsuyoshi Okita"], "title": "Multimodal Foundation Model for Cross-Modal Retrieval and Activity Recognition Tasks", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "25 pages, 8 figures", "summary": "In recent years, the widespread adoption of wearable devices has highlighted\nthe growing importance of behavior analysis using IMU. While applications span\ndiverse fields such as healthcare and robotics, recent studies have\nincreasingly focused on multimodal analysis, in addition to unimodal analysis.\nSeveral studies have proposed multimodal foundation models that incorporate\nfirst-person video and text data; however, these models still fall short in\nproviding a detailed analysis of full-body human activity. To address this\nlimitation, we propose Activity Understanding and Representations Alignment -\nMultimodal Foundation Model (AURA-MFM), a foundational model integrating four\nmodalities: third-person video, motion capture, IMU, and text. By incorporating\nthird-person video and motion capture data, the model enables a detailed and\nmultidimensional understanding of human activity, which first-person\nperspectives alone fail to capture. Additionally, a Transformer-based IMU\nencoder is employed to enhance the model's overall performance. Experimental\nevaluations on retrieval and activity recognition tasks demonstrate that our\nmodel surpasses existing methods. Notably, in the zero-shot classification for\naction recognition, our method achieved significantly higher performance, with\nan F1-score of 0.6226 and an accuracy of 0.7320, whereas the existing method\nrecorded an F1-score of 0.0747 and an accuracy of 0.1961.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684AURA-MFM\uff0c\u6574\u5408\u4e86\u7b2c\u4e09\u4eba\u79f0\u89c6\u9891\u3001\u52a8\u4f5c\u6355\u6349\u3001IMU\u548c\u6587\u672c\u56db\u79cd\u6a21\u6001\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u7c7b\u6d3b\u52a8\u5206\u6790\u548c\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u5168\u8eab\u4eba\u7c7b\u6d3b\u52a8\u5206\u6790\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u901a\u8fc7\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u5145\u5206\u6355\u6349\u7ec6\u8282\u3002", "method": "\u63d0\u51faAURA-MFM\u6a21\u578b\uff0c\u6574\u5408\u7b2c\u4e09\u4eba\u79f0\u89c6\u9891\u3001\u52a8\u4f5c\u6355\u6349\u3001IMU\u548c\u6587\u672c\u56db\u79cd\u6a21\u6001\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eTransformer\u7684IMU\u7f16\u7801\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u5728\u68c0\u7d22\u548c\u6d3b\u52a8\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u52a8\u4f5c\u5206\u7c7b\u4e2d\uff0cF1\u5206\u6570\u548c\u51c6\u786e\u7387\u663e\u8457\u9ad8\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AURA-MFM\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u7ec6\u8282\u6355\u6349\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u7c7b\u6d3b\u52a8\u7684\u7406\u89e3\u548c\u8bc6\u522b\u80fd\u529b\u3002", "keywords": "\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b, \u4eba\u7c7b\u6d3b\u52a8\u5206\u6790, IMU, Transformer"}}
{"id": "2506.03404", "pdf": "https://arxiv.org/pdf/2506.03404", "abs": "https://arxiv.org/abs/2506.03404", "authors": ["Walter Mayor", "Johan Obando-Ceron", "Aaron Courville", "Pablo Samuel Castro"], "title": "The Impact of On-Policy Parallelized Data Collection on Deep Reinforcement Learning Networks", "categories": ["cs.LG", "cs.AI"], "comment": "Proceedings of the 42nd International Conference on Machine Learning\n  (ICML 2025)", "summary": "The use of parallel actors for data collection has been an effective\ntechnique used in reinforcement learning (RL) algorithms. The manner in which\ndata is collected in these algorithms, controlled via the number of parallel\nenvironments and the rollout length, induces a form of bias-variance trade-off;\nthe number of training passes over the collected data, on the other hand, must\nstrike a balance between sample efficiency and overfitting. We conduct an\nempirical analysis of these trade-offs on PPO, one of the most popular RL\nalgorithms that uses parallel actors, and establish connections to network\nplasticity and, more generally, optimization stability. We examine its impact\non network architectures, as well as the hyper-parameter sensitivity when\nscaling data. Our analyses indicate that larger dataset sizes can increase\nfinal performance across a variety of settings, and that scaling parallel\nenvironments is more effective than increasing rollout lengths. These findings\nhighlight the critical role of data collection strategies in improving agent\nperformance.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5e76\u884c\u6570\u636e\u6536\u96c6\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u504f\u5dee-\u65b9\u5dee\u6743\u8861\uff0c\u5206\u6790\u4e86PPO\u7b97\u6cd5\u4e2d\u7684\u6570\u636e\u6536\u96c6\u7b56\u7565\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u5e76\u884c\u6570\u636e\u6536\u96c6\u7b56\u7565\uff08\u5982\u5e76\u884c\u73af\u5883\u6570\u91cf\u548crollout\u957f\u5ea6\uff09\u5982\u4f55\u5f71\u54cd\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u7b56\u7565\u5e26\u6765\u7684\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u3002", "method": "\u901a\u8fc7\u5bf9PPO\u7b97\u6cd5\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u7814\u7a76\u4e0d\u540c\u6570\u636e\u6536\u96c6\u7b56\u7565\uff08\u5e76\u884c\u73af\u5883\u6570\u91cf\u548crollout\u957f\u5ea6\uff09\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u8fde\u63a5\u7f51\u7edc\u53ef\u5851\u6027\u548c\u4f18\u5316\u7a33\u5b9a\u6027\u3002", "result": "\u53d1\u73b0\u66f4\u5927\u7684\u6570\u636e\u96c6\u89c4\u6a21\u53ef\u4ee5\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u589e\u52a0\u5e76\u884c\u73af\u5883\u6bd4\u589e\u52a0rollout\u957f\u5ea6\u66f4\u6709\u6548\u3002", "conclusion": "\u6570\u636e\u6536\u96c6\u7b56\u7565\u5bf9\u667a\u80fd\u4f53\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f18\u5316\u8fd9\u4e9b\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u6548\u679c\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u5e76\u884c\u6570\u636e\u6536\u96c6, PPO\u7b97\u6cd5, \u504f\u5dee-\u65b9\u5dee\u6743\u8861, \u7f51\u7edc\u53ef\u5851\u6027"}}
{"id": "2506.03573", "pdf": "https://arxiv.org/pdf/2506.03573", "abs": "https://arxiv.org/abs/2506.03573", "authors": ["Lin Sun", "Can Zhang"], "title": "Exchange of Perspective Prompting Enhances Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have made significant advancements in addressing\ndiverse natural language processing (NLP) tasks. However, their performance is\noften limited by inherent comprehension of problems. To address this\nlimitation, we propose Exchange-of-Perspective (EoP), a novel framework\ndesigned to exchange perspectives across different definitions of problem, so\nthat it can break the fixed mindset from any particular formulation of the\nquestion. We conducted extensive and comprehensive experiments on 8 benchmarks.\nThe results show that EoP can significantly improve performance. For instance,\ncompared to the non-commutative baseline PHP, with GPT-3.5-Turbo and EoP, we\nobserve a 3.6% improvement on AQuA (60.6% to 64.2%), while GPT-4-powered EoP\ndemonstrates a 7.7% overall accuracy enhancement on Math (53.9% to 61.6%) and a\n3.5% improvement on OlympiadBench Maths (43.5% to 47.0%) when using\nQwen-2.5-72b.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aExchange-of-Perspective (EoP)\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u6362\u95ee\u9898\u5b9a\u4e49\u89c6\u89d2\u6765\u7a81\u7834\u56fa\u5b9a\u601d\u7ef4\u6a21\u5f0f\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cdNLP\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cdNLP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6027\u80fd\u53d7\u9650\u4e8e\u5bf9\u95ee\u9898\u7684\u56fa\u6709\u7406\u89e3\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u7a81\u7834\u8fd9\u79cd\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86EoP\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u6362\u95ee\u9898\u5b9a\u4e49\u89c6\u89d2\u6765\u589e\u5f3a\u6a21\u578b\u7684\u7406\u89e3\u80fd\u529b\u548c\u7075\u6d3b\u6027\u3002", "result": "\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEoP\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u4f7f\u7528GPT-3.5-Turbo\u65f6\uff0cAQuA\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u53473.6%\uff1b\u4f7f\u7528GPT-4\u65f6\uff0cMath\u4efb\u52a1\u6574\u4f53\u51c6\u786e\u7387\u63d0\u53477.7%\u3002", "conclusion": "EoP\u6846\u67b6\u901a\u8fc7\u89c6\u89d2\u4ea4\u6362\u6709\u6548\u7a81\u7834\u4e86\u6a21\u578b\u5bf9\u95ee\u9898\u7684\u56fa\u6709\u601d\u7ef4\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u89c6\u89d2\u4ea4\u6362, NLP\u4efb\u52a1, EoP\u6846\u67b6"}}
{"id": "2506.03177", "pdf": "https://arxiv.org/pdf/2506.03177", "abs": "https://arxiv.org/abs/2506.03177", "authors": ["Isarun Chamveha", "Supphanut Chaiyungyuen", "Sasinun Worakriangkrai", "Nattawadee Prasawang", "Warasinee Chaisangmongkon", "Pornpim Korpraphong", "Voraparee Suvannarerg", "Shanigarn Thiravit", "Chalermdej Kannawat", "Kewalin Rungsinaporn", "Suwara Issaragrisil", "Payia Chadbunchachai", "Pattiya Gatechumpol", "Chawiporn Muktabhant", "Patarachai Sereerat"], "title": "Deep Learning-Based Breast Cancer Detection in Mammography: A Multi-Center Validation Study in Thai Population", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "This study presents a deep learning system for breast cancer detection in\nmammography, developed using a modified EfficientNetV2 architecture with\nenhanced attention mechanisms. The model was trained on mammograms from a major\nThai medical center and validated on three distinct datasets: an in-domain test\nset (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain\ngeneralizability set (761 cases) collected from two different hospitals. For\ncancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the\nrespective datasets. The system's lesion localization capability, evaluated\nusing metrics including Lesion Localization Fraction (LLF) and Non-Lesion\nLocalization Fraction (NLF), demonstrated robust performance in identifying\nsuspicious regions. Clinical validation through concordance tests showed strong\nagreement with radiologists: 83.5% classification and 84.0% localization\nconcordance for biopsy-confirmed cases, and 78.1% classification and 79.6%\nlocalization concordance for out-of-domain cases. Expert radiologists'\nacceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for\nout-of-domain cases. The system achieved a System Usability Scale score of\n74.17 for source hospital, and 69.20 for validation hospitals, indicating good\nclinical acceptance. These results demonstrate the model's effectiveness in\nassisting mammogram interpretation, with the potential to enhance breast cancer\nscreening workflows in clinical practice.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdbEfficientNetV2\u67b6\u6784\u548c\u589e\u5f3a\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e73\u817aX\u5149\u68c0\u67e5\u4e2d\u7684\u4e73\u817a\u764c\u68c0\u6d4b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6027\u80fd\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "motivation": "\u76ee\u6807\u662f\u5f00\u53d1\u4e00\u79cd\u9ad8\u6027\u80fd\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u4ee5\u8f85\u52a9\u4e73\u817aX\u5149\u68c0\u67e5\u4e2d\u7684\u4e73\u817a\u764c\u68c0\u6d4b\uff0c\u63d0\u5347\u7b5b\u67e5\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684EfficientNetV2\u67b6\u6784\u548c\u589e\u5f3a\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7cfb\u7edf\u5728\u6765\u81ea\u6cf0\u56fd\u533b\u7597\u4e2d\u5fc3\u7684\u4e73\u817aX\u5149\u7247\u4e0a\u8bad\u7ec3\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5305\u62ec\u57df\u5185\u6d4b\u8bd5\u96c6\u3001\u6d3b\u68c0\u786e\u8ba4\u96c6\u548c\u57df\u5916\u6cdb\u5316\u96c6\u3002", "result": "\u7cfb\u7edf\u5728\u764c\u75c7\u68c0\u6d4b\u4e0a\u7684AUROC\u4e3a0.89-0.96\uff0c\u75c5\u7076\u5b9a\u4f4d\u6027\u80fd\u8868\u73b0\u7a33\u5065\uff0c\u4e0e\u653e\u5c04\u79d1\u533b\u5e08\u7684\u4e00\u81f4\u6027\u9ad8\uff08\u5206\u7c7b\u548c\u5b9a\u4f4d\u4e00\u81f4\u6027\u8fbe78%-84%\uff09\uff0c\u4e34\u5e8a\u63a5\u53d7\u7387\u5e73\u5747\u4e3a89.3%-96.7%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u8f85\u52a9\u4e73\u817aX\u5149\u68c0\u67e5\u4e2d\u663e\u793a\u51fa\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\uff0c\u6709\u671b\u63d0\u5347\u4e73\u817a\u764c\u7b5b\u67e5\u7684\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u3002", "keywords": "\u4e73\u817a\u764c\u68c0\u6d4b, \u6df1\u5ea6\u5b66\u4e60, EfficientNetV2, \u6ce8\u610f\u529b\u673a\u5236, \u4e34\u5e8a\u9a8c\u8bc1"}}
{"id": "2506.03411", "pdf": "https://arxiv.org/pdf/2506.03411", "abs": "https://arxiv.org/abs/2506.03411", "authors": ["Melissa Dutz", "Han Shao", "Avrim Blum", "Aloni Cohen"], "title": "A Machine Learning Theory Perspective on Strategic Litigation", "categories": ["cs.LG", "cs.GT"], "comment": null, "summary": "Strategic litigation involves bringing a legal case to court with the goal of\nhaving a broader impact beyond resolving the case itself: for example, creating\nprecedent which will influence future rulings. In this paper, we explore\nstrategic litigation from the perspective of machine learning theory. We\nconsider an abstract model of a common-law legal system where a lower court\ndecides new cases by applying a decision rule learned from a higher court's\npast rulings. In this model, we explore the power of a strategic litigator, who\nstrategically brings cases to the higher court to influence the learned\ndecision rule, thereby affecting future cases. We explore questions including:\nWhat impact can a strategic litigator have? Which cases should a strategic\nlitigator bring to court? Does it ever make sense for a strategic litigator to\nbring a case when they are sure the court will rule against them?", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6218\u7565\u8bc9\u8bbc\u5728\u673a\u5668\u5b66\u4e60\u7406\u8bba\u6846\u67b6\u4e0b\u7684\u5f71\u54cd\uff0c\u5206\u6790\u4e86\u8bc9\u8bbc\u8005\u5982\u4f55\u901a\u8fc7\u7b56\u7565\u6027\u6848\u4ef6\u5851\u9020\u6cd5\u5f8b\u89c4\u5219\u3002", "motivation": "\u7814\u7a76\u6218\u7565\u8bc9\u8bbc\u884c\u4e3a\u5bf9\u6cd5\u5f8b\u7cfb\u7edf\u4e2d\u51b3\u7b56\u89c4\u5219\u5b66\u4e60\u548c\u672a\u6765\u6848\u4f8b\u88c1\u51b3\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u62bd\u8c61\u6a21\u578b\uff0c\u6a21\u62df\u666e\u901a\u6cd5\u4f53\u7cfb\u4e2d\u4e0b\u7ea7\u6cd5\u9662\u901a\u8fc7\u4e0a\u7ea7\u6cd5\u9662\u5224\u4f8b\u5b66\u4e60\u51b3\u7b56\u89c4\u5219\uff0c\u5206\u6790\u6218\u7565\u8bc9\u8bbc\u8005\u5982\u4f55\u9009\u62e9\u6848\u4ef6\u4ee5\u5f71\u54cd\u89c4\u5219\u3002", "result": "\u63ed\u793a\u4e86\u6218\u7565\u8bc9\u8bbc\u8005\u901a\u8fc7\u9009\u62e9\u7279\u5b9a\u6848\u4ef6\u53ef\u4ee5\u663e\u8457\u5f71\u54cd\u51b3\u7b56\u89c4\u5219\uff0c\u751a\u81f3\u53ef\u80fd\u5728\u6ca1\u6709\u80dc\u7b97\u7684\u60c5\u51b5\u4e0b\u63d0\u8d77\u8bc9\u8bbc\u3002", "conclusion": "\u6218\u7565\u8bc9\u8bbc\u5728\u6cd5\u5f8b\u7cfb\u7edf\u4e2d\u7684\u4f5c\u7528\u53ef\u4ee5\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u7406\u8bba\u8fdb\u884c\u91cf\u5316\u5206\u6790\uff0c\u7b56\u7565\u6027\u6848\u4ef6\u9009\u62e9\u5bf9\u6cd5\u5f8b\u6f14\u53d8\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\u3002", "keywords": "\u6218\u7565\u8bc9\u8bbc,\u673a\u5668\u5b66\u4e60,\u6cd5\u5f8b\u7cfb\u7edf,\u51b3\u7b56\u89c4\u5219,\u5224\u4f8b"}}
{"id": "2506.03576", "pdf": "https://arxiv.org/pdf/2506.03576", "abs": "https://arxiv.org/abs/2506.03576", "authors": ["Zirui Chen", "Xin Wang", "Zhao Li", "Wenbin Guo", "Dongxiao He"], "title": "KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in knowledge representation learning (KRL) highlight the\nurgent necessity to unify symbolic knowledge graphs (KGs) with language models\n(LMs) for richer semantic understanding. However, existing approaches typically\nprioritize either graph structure or textual semantics, leaving a gap: a\nunified framework that simultaneously captures global KG connectivity, nuanced\nlinguistic context, and discriminative reasoning semantics. To bridge this gap,\nwe introduce KG-BiLM, a bidirectional LM framework that fuses structural cues\nfrom KGs with the semantic expressiveness of generative transformers. KG-BiLM\nincorporates three key components: (i) Bidirectional Knowledge Attention, which\nremoves the causal mask to enable full interaction among all tokens and\nentities; (ii) Knowledge-Masked Prediction, which encourages the model to\nleverage both local semantic contexts and global graph connectivity; and (iii)\nContrastive Graph Semantic Aggregation, which preserves KG structure via\ncontrastive alignment of sampled sub-graph representations. Extensive\nexperiments on standard benchmarks demonstrate that KG-BiLM outperforms strong\nbaselines in link prediction, especially on large-scale graphs with complex\nmulti-hop relations - validating its effectiveness in unifying structural\ninformation and textual semantics.", "AI": {"tldr": "KG-BiLM\u662f\u4e00\u4e2a\u53cc\u5411\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u65e8\u5728\u7edf\u4e00\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u4fe1\u606f\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u8868\u8fbe\uff0c\u901a\u8fc7\u53cc\u5411\u77e5\u8bc6\u6ce8\u610f\u529b\u3001\u77e5\u8bc6\u63a9\u7801\u9884\u6d4b\u548c\u5bf9\u6bd4\u56fe\u8bed\u4e49\u805a\u5408\u4e09\u4e2a\u7ec4\u4ef6\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5355\u72ec\u5173\u6ce8\u77e5\u8bc6\u56fe\u8c31\u7ed3\u6784\u6216\u6587\u672c\u8bed\u4e49\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u4ee5\u540c\u65f6\u6355\u6349\u5168\u5c40\u56fe\u8c31\u8fde\u901a\u6027\u3001\u7ec6\u5fae\u8bed\u8a00\u4e0a\u4e0b\u6587\u548c\u5224\u522b\u6027\u63a8\u7406\u8bed\u4e49\u3002", "method": "\u63d0\u51fa\u4e86KG-BiLM\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(i)\u53cc\u5411\u77e5\u8bc6\u6ce8\u610f\u529b\uff1b(ii)\u77e5\u8bc6\u63a9\u7801\u9884\u6d4b\uff1b(iii)\u5bf9\u6bd4\u56fe\u8bed\u4e49\u805a\u5408\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKG-BiLM\u5728\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u591a\u8df3\u5173\u7cfb\u56fe\u8c31\u4e0a\u3002", "conclusion": "KG-BiLM\u6709\u6548\u7edf\u4e00\u4e86\u7ed3\u6784\u4fe1\u606f\u548c\u6587\u672c\u8bed\u4e49\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "keywords": "\u77e5\u8bc6\u8868\u793a\u5b66\u4e60,\u77e5\u8bc6\u56fe\u8c31,\u8bed\u8a00\u6a21\u578b,KG-BiLM"}}
{"id": "2506.03178", "pdf": "https://arxiv.org/pdf/2506.03178", "abs": "https://arxiv.org/abs/2506.03178", "authors": ["Md. Zihad Bin Jahangir", "Muhammad Ashad Kabir", "Sumaiya Akter", "Israt Jahan", "Minh Chau"], "title": "LLaMA-XR: A Novel Framework for Radiology Report Generation using LLaMA and QLoRA Fine Tuning", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "25 pages", "summary": "Automated radiology report generation holds significant potential to reduce\nradiologists' workload and enhance diagnostic accuracy. However, generating\nprecise and clinically meaningful reports from chest radiographs remains\nchallenging due to the complexity of medical language and the need for\ncontextual understanding. Existing models often struggle with maintaining both\naccuracy and contextual relevance. In this paper, we present LLaMA-XR, a novel\nframework that integrates LLaMA 3.1 with DenseNet-121-based image embeddings\nand Quantized Low-Rank Adaptation (QLoRA) fine-tuning. LLaMA-XR achieves\nimproved coherence and clinical accuracy while maintaining computational\nefficiency. This efficiency is driven by an optimization strategy that enhances\nparameter utilization and reduces memory overhead, enabling faster report\ngeneration with lower computational resource demands. Extensive experiments\nconducted on the IU X-ray benchmark dataset demonstrate that LLaMA-XR\noutperforms a range of state-of-the-art methods. Our model achieves a ROUGE-L\nscore of 0.433 and a METEOR score of 0.336, establishing new performance\nbenchmarks in the domain. These results underscore LLaMA-XR's potential as an\neffective and efficient AI system for automated radiology reporting, offering\nenhanced clinical utility and reliability.", "AI": {"tldr": "LLaMA-XR\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u7ed3\u5408LLaMA 3.1\u548cDenseNet-121\u56fe\u50cf\u5d4c\u5165\uff0c\u901a\u8fc7QLoRA\u5fae\u8c03\u63d0\u5347\u653e\u5c04\u62a5\u544a\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u653e\u5c04\u62a5\u544a\u4e2d\u751f\u6210\u51c6\u786e\u4e14\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u6311\u6218\u3002", "method": "\u6574\u5408LLaMA 3.1\u4e0eDenseNet-121\u56fe\u50cf\u5d4c\u5165\uff0c\u91c7\u7528QLoRA\u5fae\u8c03\u4f18\u5316\u53c2\u6570\u548c\u5185\u5b58\u4f7f\u7528\u3002", "result": "\u5728IU X-ray\u6570\u636e\u96c6\u4e0aROUGE-L\u5f97\u52060.433\uff0cMETEOR\u5f97\u52060.336\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LLaMA-XR\u5728\u653e\u5c04\u62a5\u544a\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "keywords": "\u653e\u5c04\u62a5\u544a\u751f\u6210, LLaMA-XR, QLoRA, DenseNet-121, \u4e34\u5e8a\u51c6\u786e\u6027"}}
{"id": "2506.03426", "pdf": "https://arxiv.org/pdf/2506.03426", "abs": "https://arxiv.org/abs/2506.03426", "authors": ["Joonseong Kang", "Soojeong Lee", "Subeen Park", "Sumin Park", "Taero Kim", "Jihee Kim", "Ryunyi Lee", "Kyungwoo Song"], "title": "Adaptive Task Vectors for Large Language Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In-Context Learning (ICL) enables Large Language Models (LLMs) to perform\ntasks without parameter updates by conditioning on a few demonstrations\nprovided in the prompt. Despite its success, ICL suffers from several\nlimitations, including sensitivity to demonstration order, context length\nconstraints, and computational inefficiency. To address these challenges, task\nvector-based approaches compress task information into a single vector.\nHowever, these methods typically construct task vectors from fixed sets of\ndemonstrations and reuse them across input queries, without conditioning on the\nspecific input. This limitation can lead models to struggle with effective\nadaptation when the input query is not well aligned with the underlying\ndemonstrations, consequently degrading their generalization performance on\nunseen tasks. To overcome this limitation, we propose Adaptive Task Vectors\n(ATV), a simple and effective framework that dynamically generates task vectors\nconditioned on each input query. ATV employs a small language model to generate\ntask vectors, which are then transformed to match the target LLM's architecture\nand applied to guide its output generation. In contrast to ICL and previous\nvector-based approaches, which rely on fixed demonstration sets and their\ncorresponding vectors, ATV dynamically generates task vectors tailored to each\nspecific input query and task. Consequently, ATV demonstrates strong\nperformance and generalization capabilities, even for unseen tasks.\nFurthermore, we provide a theoretical analysis indicating that ATV is\nexpressively equivalent to LoRA under equal rank budgets and more expressive\nthan Prefix-Tuning, thereby offering formal support for its representational\nadvantage.", "AI": {"tldr": "\u63d0\u51faAdaptive Task Vectors\uff08ATV\uff09\u6846\u67b6\uff0c\u52a8\u6001\u751f\u6210\u9488\u5bf9\u6bcf\u4e2a\u8f93\u5165\u67e5\u8be2\u7684\u4efb\u52a1\u5411\u91cf\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u4efb\u52a1\u5411\u91cf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfIn-Context Learning\uff08ICL\uff09\u548c\u4efb\u52a1\u5411\u91cf\u65b9\u6cd5\u5b58\u5728\u5bf9\u6f14\u793a\u987a\u5e8f\u654f\u611f\u3001\u8ba1\u7b97\u6548\u7387\u4f4e\u53ca\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0cATV\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u5411\u91cf\u5b9e\u73b0\u66f4\u597d\u7684\u9002\u5e94\u6027\u3002", "method": "ATV\u5229\u7528\u4e00\u4e2a\u5c0f\u8bed\u8a00\u6a21\u578b\u4e3a\u6bcf\u4e2a\u8f93\u5165\u67e5\u8be2\u52a8\u6001\u751f\u6210\u4efb\u52a1\u5411\u91cf\uff0c\u5e76\u5c06\u5176\u9002\u914d\u5230\u76ee\u6807LLM\u67b6\u6784\u4e2d\u4ee5\u6307\u5bfc\u8f93\u51fa\u751f\u6210\u3002", "result": "ATV\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u7406\u8bba\u5206\u6790\u8868\u660e\u5176\u8868\u8fbe\u529b\u4f18\u4e8ePrefix-Tuning\u3002", "conclusion": "ATV\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u5411\u91cf\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u9002\u5e94\u6027\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u4efb\u52a1\u5411\u91cf\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "Adaptive Task Vectors, In-Context Learning, Large Language Models, task adaptation, generalization"}}
{"id": "2506.03580", "pdf": "https://arxiv.org/pdf/2506.03580", "abs": "https://arxiv.org/abs/2506.03580", "authors": ["Enrico Benedetti", "Akiko Aizawa", "Florian Boudin"], "title": "Automatically Suggesting Diverse Example Sentences for L2 Japanese Learners Using Pre-Trained Language Models", "categories": ["cs.CL"], "comment": "Proceedings of the 62nd Annual Meeting of the Association for\n  Computational Linguistics (Volume 4: Student Research Workshop)", "summary": "Providing example sentences that are diverse and aligned with learners'\nproficiency levels is essential for fostering effective language acquisition.\nThis study examines the use of Pre-trained Language Models (PLMs) to produce\nexample sentences targeting L2 Japanese learners. We utilize PLMs in two ways:\nas quality scoring components in a retrieval system that draws from a newly\ncurated corpus of Japanese sentences, and as direct sentence generators using\nzero-shot learning. We evaluate the quality of sentences by considering\nmultiple aspects such as difficulty, diversity, and naturalness, with a panel\nof raters consisting of learners of Japanese, native speakers -- and GPT-4. Our\nfindings suggest that there is inherent disagreement among participants on the\nratings of sentence qualities, except for difficulty. Despite that, the\nretrieval approach was preferred by all evaluators, especially for beginner and\nadvanced target proficiency, while the generative approaches received lower\nscores on average. Even so, our experiments highlight the potential for using\nPLMs to enhance the adaptability of sentence suggestion systems and therefore\nimprove the language learning journey.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u591a\u6837\u4e14\u7b26\u5408\u5b66\u4e60\u8005\u6c34\u5e73\u7684\u65e5\u8bed\u4f8b\u53e5\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e86\u68c0\u7d22\u4e0e\u751f\u6210\u4e24\u79cd\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u63d0\u4f9b\u591a\u6837\u4e14\u5339\u914d\u5b66\u4e60\u8005\u6c34\u5e73\u7684\u4f8b\u53e5\u5bf9\u8bed\u8a00\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u65e8\u5728\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u4f8b\u53e5\u751f\u6210\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4f8b\u53e5\u8d28\u91cf\u8bc4\u5206\uff08\u68c0\u7d22\u7cfb\u7edf\uff09\u548c\u96f6\u6837\u672c\u5b66\u4e60\u76f4\u63a5\u751f\u6210\u4f8b\u53e5\uff0c\u5e76\u8bc4\u4f30\u53e5\u5b50\u96be\u5ea6\u3001\u591a\u6837\u6027\u548c\u81ea\u7136\u5ea6\u3002", "result": "\u5c3d\u7ba1\u8bc4\u4f30\u8005\u5bf9\u53e5\u5b50\u8d28\u91cf\u7684\u8bc4\u5206\u5b58\u5728\u5206\u6b67\uff08\u9664\u96be\u5ea6\u5916\uff09\uff0c\u68c0\u7d22\u65b9\u6cd5\u666e\u904d\u66f4\u53d7\u6b22\u8fce\uff0c\u751f\u6210\u65b9\u6cd5\u5f97\u5206\u8f83\u4f4e\u3002", "conclusion": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u63d0\u5347\u4f8b\u53e5\u5efa\u8bae\u7cfb\u7edf\u9002\u5e94\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u6539\u5584\u8bed\u8a00\u5b66\u4e60\u4f53\u9a8c\u3002", "keywords": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u65e5\u8bed\u5b66\u4e60\uff0c\u4f8b\u53e5\u751f\u6210\uff0c\u68c0\u7d22\u7cfb\u7edf\uff0c\u96f6\u6837\u672c\u5b66\u4e60"}}
{"id": "2506.03179", "pdf": "https://arxiv.org/pdf/2506.03179", "abs": "https://arxiv.org/abs/2506.03179", "authors": ["Qi Li", "Runpeng Yu", "Xinchao Wang"], "title": "Vid-SME: Membership Inference Attacks against Large Video Understanding Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) demonstrate remarkable capabilities\nin handling complex multimodal tasks and are increasingly adopted in video\nunderstanding applications. However, their rapid advancement raises serious\ndata privacy concerns, particularly given the potential inclusion of sensitive\nvideo content, such as personal recordings and surveillance footage, in their\ntraining datasets. Determining improperly used videos during training remains a\ncritical and unresolved challenge. Despite considerable progress on membership\ninference attacks (MIAs) for text and image data in MLLMs, existing methods\nfail to generalize effectively to the video domain. These methods suffer from\npoor scalability as more frames are sampled and generally achieve negligible\ntrue positive rates at low false positive rates (TPR@Low FPR), mainly due to\ntheir failure to capture the inherent temporal variations of video frames and\nto account for model behavior differences as the number of frames varies. To\naddress these challenges, we introduce Vid-SME, the first membership inference\nmethod tailored for video data used in video understanding LLMs (VULLMs).\nVid-SME leverages the confidence of model output and integrates adaptive\nparameterization to compute Sharma-Mittal entropy (SME) for video inputs. By\nleveraging the SME difference between natural and temporally-reversed video\nframes, Vid-SME derives robust membership scores to determine whether a given\nvideo is part of the model's training set. Experiments on various self-trained\nand open-sourced VULLMs demonstrate the strong effectiveness of Vid-SME.", "AI": {"tldr": "\u63d0\u51fa\u4e86Vid-SME\uff0c\u4e00\u79cd\u9488\u5bf9\u89c6\u9891\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u9891\u6570\u636e\u7684\u4f1a\u5458\u63a8\u65ad\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u9891\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u5176\u4e2d\u53ef\u80fd\u5305\u542b\u654f\u611f\u89c6\u9891\u5185\u5bb9\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u8bad\u7ec3\u6570\u636e\u4e2d\u89c6\u9891\u7684\u4e0d\u5f53\u4f7f\u7528\u95ee\u9898\u3002", "method": "Vid-SME\u5229\u7528\u6a21\u578b\u8f93\u51fa\u7684\u7f6e\u4fe1\u5ea6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u53c2\u6570\u5316\u8ba1\u7b97\u89c6\u9891\u8f93\u5165\u7684Sharma-Mittal\u71b5\uff08SME\uff09\uff0c\u5e76\u7ed3\u5408\u81ea\u7136\u548c\u65f6\u5e8f\u53cd\u8f6c\u89c6\u9891\u5e27\u7684SME\u5dee\u5f02\u6765\u63a8\u65ad\u4f1a\u5458\u8d44\u683c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVid-SME\u5728\u591a\u79cd\u81ea\u8bad\u7ec3\u548c\u5f00\u6e90\u89c6\u9891\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Vid-SME\u4e3a\u89e3\u51b3\u89c6\u9891\u6570\u636e\u7684\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u89c6\u9891\u7406\u89e3\u3001\u4f1a\u5458\u63a8\u65ad\u3001\u9690\u79c1\u4fdd\u62a4\u3001Sharma-Mittal\u71b5"}}
{"id": "2506.03444", "pdf": "https://arxiv.org/pdf/2506.03444", "abs": "https://arxiv.org/abs/2506.03444", "authors": ["Yue Gong", "Raul Castro Fernandez"], "title": "Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior", "categories": ["cs.LG", "cs.CL"], "comment": "Under Review", "summary": "As hypothesis generation becomes increasingly automated, a new bottleneck has\nemerged: hypothesis assessment. Modern systems can surface thousands of\nstatistical relationships-correlations, trends, causal links-but offer little\nguidance on which ones are novel, non-trivial, or worthy of expert attention.\nIn this work, we study the complementary problem to hypothesis generation:\nautomatic hypothesis assessment. Specifically, we ask: given a large set of\nstatistical relationships, can we automatically assess which ones are novel and\nworth further exploration? We focus on correlations as they are a common entry\npoint in exploratory data analysis that often serve as the basis for forming\ndeeper scientific or causal hypotheses.\n  To support automatic assessment, we propose to leverage the vast knowledge\nencoded in LLMs' weights to derive a prior distribution over the correlation\nvalue of a variable pair. If an LLM's prior expects the correlation value\nobserved, then such correlation is not surprising, and vice versa. We propose\nthe Logit-based Calibrated Prior, an LLM-elicited correlation prior that\ntransforms the model's raw output logits into a calibrated, continuous\npredictive distribution over correlation values. We evaluate the prior on a\nbenchmark of 2,096 real-world variable pairs and it achieves a sign accuracy of\n78.8%, a mean absolute error of 0.26, and 95% credible interval coverage of\n89.2% in predicting Pearson correlation coefficient. It also outperforms a\nfine-tuned RoBERTa classifier in binary correlation prediction and achieves\nhigher precision@K in hypothesis ranking. We further show that the prior\ngeneralizes to correlations not seen during LLM pretraining, reflecting\ncontext-sensitive reasoning rather than memorization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5047\u8bbe\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7Logit-based Calibrated Prior\u9884\u6d4b\u53d8\u91cf\u5bf9\u7684\u76f8\u5173\u7cfb\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5047\u8bbe\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u5047\u8bbe\u751f\u6210\u65e5\u76ca\u81ea\u52a8\u5316\uff0c\u5047\u8bbe\u8bc4\u4f30\u6210\u4e3a\u65b0\u7684\u74f6\u9888\u3002\u73b0\u4ee3\u7cfb\u7edf\u80fd\u53d1\u73b0\u5927\u91cf\u7edf\u8ba1\u5173\u7cfb\uff0c\u4f46\u7f3a\u4e4f\u5224\u65ad\u5176\u65b0\u9896\u6027\u6216\u91cd\u8981\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528LLM\u7684\u6743\u91cd\u77e5\u8bc6\u751f\u6210\u53d8\u91cf\u5bf9\u7684\u5148\u9a8c\u76f8\u5173\u7cfb\u6570\u5206\u5e03\uff0c\u63d0\u51faLogit-based Calibrated Prior\uff0c\u5c06LLM\u8f93\u51fa\u8f6c\u5316\u4e3a\u6821\u51c6\u7684\u9884\u6d4b\u5206\u5e03\u3002", "result": "\u57282096\u4e2a\u771f\u5b9e\u53d8\u91cf\u5bf9\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4bPearson\u76f8\u5173\u7cfb\u6570\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u5fae\u8c03\u7684RoBERTa\u5206\u7c7b\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u5047\u8bbe\u7684\u65b0\u9896\u6027\uff0c\u4e14\u5177\u5907\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u672a\u89c1\u8fc7\u7684\u76f8\u5173\u5173\u7cfb\u3002", "keywords": "\u5047\u8bbe\u8bc4\u4f30, LLM, \u76f8\u5173\u7cfb\u6570, \u5148\u9a8c\u5206\u5e03, \u81ea\u52a8\u5316"}}
{"id": "2506.03592", "pdf": "https://arxiv.org/pdf/2506.03592", "abs": "https://arxiv.org/abs/2506.03592", "authors": ["Viktor Hangya", "Fabian K\u00fcch", "Darina Gold"], "title": "From Understanding to Generation: An Efficient Shortcut for Evaluating Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Iterative evaluation of LLMs during training is essential to ensure expected\ncapability development, but can be time- and compute-intensive. While NLU\ntasks, where the model selects from fixed answer choices, are cheap to\nevaluate, essential capabilities like reasoning and code generation rely on the\nmore time-consuming NLG (token-by-token generation) format. In this work, our\naim is to decrease the computational burden of NLG benchmarks in order to\nenable monitoring crucial LLM capabilities during model training. We\nreformulate generative tasks into computationally cheaper NLU alternatives. We\ntest the performance correlation between the original and reformulated tasks\nusing 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code\ngeneration, factual knowledge and reading comprehension. Our results show a\nstrong correlation between task formats, supporting capability assessment via\ncheaper alternatives and achieving over 35x average reduction in evaluation\ntime. We plan to publish our benchmark adaptions.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u751f\u6210\u5f0f\u4efb\u52a1\u8f6c\u5316\u4e3a\u66f4\u4fbf\u5b9c\u7684\u9009\u62e9\u5f0f\u4efb\u52a1\uff0c\u51cf\u5c11LLM\u8bad\u7ec3\u4e2d\u8bc4\u4f30\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u9a8c\u8bc1\u4e86\u4efb\u52a1\u683c\u5f0f\u95f4\u7684\u5f3a\u76f8\u5173\u6027\uff0c\u8bc4\u4f30\u65f6\u95f4\u5e73\u5747\u51cf\u5c1135\u500d\u4ee5\u4e0a\u3002", "motivation": "\u5728LLM\u8bad\u7ec3\u4e2d\uff0c\u751f\u6210\u5f0f\u4efb\u52a1\uff08\u5982\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\uff09\u7684\u8bc4\u4f30\u8017\u65f6\u4e14\u8ba1\u7b97\u91cf\u5927\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u76d1\u63a7\u5173\u952e\u80fd\u529b\u7684\u53d1\u5c55\u3002", "method": "\u5c06\u751f\u6210\u5f0f\u4efb\u52a1\u91cd\u65b0\u8bbe\u8ba1\u4e3a\u8ba1\u7b97\u6210\u672c\u8f83\u4f4e\u7684\u9009\u62e9\u5f0f\u4efb\u52a1\uff0c\u5e76\u5728\u4e0d\u540c\u5927\u5c0f\u548c\u80fd\u529b\u76848\u79cdLM\u4e0a\u8fdb\u884c\u6027\u80fd\u76f8\u5173\u6027\u6d4b\u8bd5\u3002", "result": "\u751f\u6210\u5f0f\u548c\u9009\u62e9\u5f0f\u4efb\u52a1\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u8bc4\u4f30\u65f6\u95f4\u5e73\u5747\u51cf\u5c11\u8d85\u8fc735\u500d\u3002", "conclusion": "\u901a\u8fc7\u9009\u62e9\u5f0f\u4efb\u52a1\u53ef\u4ee5\u6709\u6548\u8bc4\u4f30LLM\u7684\u5173\u952e\u80fd\u529b\uff0c\u663e\u8457\u964d\u4f4e\u8bc4\u4f30\u6210\u672c\u3002", "keywords": "LLM\u8bc4\u4f30, \u751f\u6210\u5f0f\u4efb\u52a1, \u9009\u62e9\u5f0f\u4efb\u52a1, \u8ba1\u7b97\u6210\u672c, \u76f8\u5173\u6027"}}
{"id": "2506.03183", "pdf": "https://arxiv.org/pdf/2506.03183", "abs": "https://arxiv.org/abs/2506.03183", "authors": ["Ya\u015far Utku Al\u00e7alar", "Yu Cao", "Mehmet Ak\u00e7akaya"], "title": "Edge Computing for Physics-Driven AI in Computational MRI: A Feasibility Study", "categories": ["eess.IV", "cs.AI", "cs.AR", "cs.CV", "cs.LG", "physics.med-ph"], "comment": "IEEE International Conference on Future Internet of Things and Cloud\n  (FiCloud), 2025", "summary": "Physics-driven artificial intelligence (PD-AI) reconstruction methods have\nemerged as the state-of-the-art for accelerating MRI scans, enabling higher\nspatial and temporal resolutions. However, the high resolution of these scans\ngenerates massive data volumes, leading to challenges in transmission, storage,\nand real-time processing. This is particularly pronounced in functional MRI,\nwhere hundreds of volumetric acquisitions further exacerbate these demands.\nEdge computing with FPGAs presents a promising solution for enabling PD-AI\nreconstruction near the MRI sensors, reducing data transfer and storage\nbottlenecks. However, this requires optimization of PD-AI models for hardware\nefficiency through quantization and bypassing traditional FFT-based approaches,\nwhich can be a limitation due to their computational demands. In this work, we\npropose a novel PD-AI computational MRI approach optimized for FPGA-based edge\ncomputing devices, leveraging 8-bit complex data quantization and eliminating\nredundant FFT/IFFT operations. Our results show that this strategy improves\ncomputational efficiency while maintaining reconstruction quality comparable to\nconventional PD-AI methods, and outperforms standard clinical methods. Our\napproach presents an opportunity for high-resolution MRI reconstruction on\nresource-constrained devices, highlighting its potential for real-world\ndeployment.", "AI": {"tldr": "PD-AI\u91cd\u5efa\u65b9\u6cd5\u901a\u8fc7FPGA\u8fb9\u7f18\u8ba1\u7b97\u4f18\u5316\uff0c\u91c7\u75288\u4f4d\u590d\u6570\u6570\u636e\u91cf\u5316\u548c\u8df3\u8fc7\u5197\u4f59FFT/IFFT\u64cd\u4f5c\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u4e14\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387MRI\u626b\u63cf\u4ea7\u751f\u5927\u91cf\u6570\u636e\uff0c\u4f20\u8f93\u3001\u5b58\u50a8\u548c\u5b9e\u65f6\u5904\u7406\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u529f\u80fdMRI\u9700\u6c42\u66f4\u5927\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eFPGA\u7684PD-AI\u65b9\u6cd5\uff0c\u91c7\u75288\u4f4d\u590d\u6570\u6570\u636e\u91cf\u5316\u548c\u8df3\u8fc7FFT/IFFT\u5197\u4f59\u64cd\u4f5c\u3002", "result": "\u8ba1\u7b97\u6548\u7387\u63d0\u5347\uff0c\u91cd\u5efa\u8d28\u91cf\u4e0e\u4f20\u7edfPD-AI\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f18\u4e8e\u4e34\u5e8a\u6807\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u9ad8\u5206\u8fa8\u7387MRI\u91cd\u5efa\u65b9\u6848\uff0c\u5177\u73b0\u5b9e\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "PD-AI, MRI, FPGA, edge computing, quantization"}}
{"id": "2506.03472", "pdf": "https://arxiv.org/pdf/2506.03472", "abs": "https://arxiv.org/abs/2506.03472", "authors": ["Mahesh Godavarti"], "title": "Directional Non-Commutative Monoidal Embeddings for MNIST", "categories": ["cs.LG", "20-XX, 08A02", "F.4.1; I.2"], "comment": null, "summary": "We present an empirical validation of the directional non-commutative\nmonoidal embedding framework recently introduced in prior\nwork~\\cite{Godavarti2025monoidal}. This framework defines learnable\ncompositional embeddings using distinct non-commutative operators per dimension\n(axis) that satisfy an interchange law, generalizing classical one-dimensional\ntransforms. Our primary goal is to verify that this framework can effectively\nmodel real data by applying it to a controlled, well-understood task: image\nclassification on the MNIST dataset~\\cite{lecun1998gradient}. A central\nhypothesis for why the proposed monoidal embedding works well is that it\ngeneralizes the Discrete Fourier Transform (DFT)~\\cite{oppenheim1999discrete}\nby learning task-specific frequency components instead of using fixed basis\nfrequencies. We test this hypothesis by comparing learned monoidal embeddings\nagainst fixed DFT-based embeddings on MNIST. The results show that as the\nembedding dimensionality decreases (e.g., from 32 to 8 to 2), the performance\ngap between the learned monoidal embeddings and fixed DFT-based embeddings on\nMNIST grows increasingly large. This comparison is used as an analytic tool to\nexplain why the framework performs well: the learnable embeddings can capture\nthe most discriminative spectral components for the task. Overall, our\nexperiments confirm that directional non-commutative monoidal embeddings are\nhighly effective for representing image data, offering a compact learned\nrepresentation that retains high task performance. The code used in this work\nis available at\nhttps://github.com/mahesh-godavarti/directional_composition_mnist.", "AI": {"tldr": "\u5bf9\u65b9\u5411\u6027\u975e\u4ea4\u6362\u5e7a\u534a\u7fa4\u5d4c\u5165\u6846\u67b6\u5728MNIST\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u53ef\u5b66\u4e60\u5d4c\u5165\u5728\u4f4e\u7ef4\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8e\u56fa\u5b9aDFT\u5d4c\u5165\u3002", "motivation": "\u9a8c\u8bc1\u65b9\u5411\u6027\u975e\u4ea4\u6362\u5e7a\u534a\u7fa4\u5d4c\u5165\u6846\u67b6\u80fd\u5426\u901a\u8fc7MNIST\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u6709\u6548\u5efa\u6a21\u771f\u5b9e\u6570\u636e\uff0c\u5e76\u63a2\u7d22\u5176\u4f18\u4e8e\u56fa\u5b9aDFT\u5d4c\u5165\u7684\u539f\u56e0\u3002", "method": "\u901a\u8fc7\u5728MNIST\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u53ef\u5b66\u4e60\u7684\u5e7a\u534a\u7fa4\u5d4c\u5165\u4e0e\u56fa\u5b9aDFT\u5d4c\u5165\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u5d4c\u5165\u7ef4\u5ea6\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "\u968f\u7740\u5d4c\u5165\u7ef4\u5ea6\u7684\u964d\u4f4e\uff0c\u53ef\u5b66\u4e60\u5e7a\u534a\u7fa4\u5d4c\u5165\u7684\u6027\u80fd\u660e\u663e\u4f18\u4e8e\u56fa\u5b9aDFT\u5d4c\u5165\u3002", "conclusion": "\u65b9\u5411\u6027\u975e\u4ea4\u6362\u5e7a\u534a\u7fa4\u5d4c\u5165\u6846\u67b6\u80fd\u591f\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u7684\u9891\u7387\u6210\u5206\uff0c\u63d0\u4f9b\u7d27\u51d1\u4e14\u9ad8\u6548\u7684\u6570\u636e\u8868\u793a\u3002", "keywords": "\u5e7a\u534a\u7fa4\u5d4c\u5165, MNIST, \u975e\u4ea4\u6362\u64cd\u4f5c, \u5b66\u4e60\u5d4c\u5165, DFT"}}
{"id": "2506.03593", "pdf": "https://arxiv.org/pdf/2506.03593", "abs": "https://arxiv.org/abs/2506.03593", "authors": ["Ray Groshan", "Michael Ginn", "Alexis Palmer"], "title": "Is linguistically-motivated data augmentation worth it?", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main. First two authors contributed equally", "summary": "Data augmentation, a widely-employed technique for addressing data scarcity,\ninvolves generating synthetic data examples which are then used to augment\navailable training data. Researchers have seen surprising success from simple\nmethods, such as random perturbations from natural examples, where models seem\nto benefit even from data with nonsense words, or data that doesn't conform to\nthe rules of the language. A second line of research produces synthetic data\nthat does in fact follow all linguistic constraints; these methods require some\nlinguistic expertise and are generally more challenging to implement. No\nprevious work has done a systematic, empirical comparison of both\nlinguistically-naive and linguistically-motivated data augmentation strategies,\nleaving uncertainty about whether the additional time and effort of\nlinguistically-motivated data augmentation work in fact yields better\ndownstream performance.\n  In this work, we conduct a careful and comprehensive comparison of\naugmentation strategies (both linguistically-naive and\nlinguistically-motivated) for two low-resource languages with different\nmorphological properties, Uspanteko and Arapaho. We evaluate the effectiveness\nof many different strategies and their combinations across two important\nsequence-to-sequence tasks for low-resource languages: machine translation and\ninterlinear glossing. We find that linguistically-motivated strategies can have\nbenefits over naive approaches, but only when the new examples they produce are\nnot significantly unlike the training data distribution.", "AI": {"tldr": "\u7cfb\u7edf\u6bd4\u8f83\u4e86\u8bed\u8a00\u65e0\u5173\u548c\u8bed\u8a00\u76f8\u5173\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u53d1\u73b0\u540e\u8005\u5728\u751f\u6210\u6570\u636e\u4e0e\u8bad\u7ec3\u5206\u5e03\u76f8\u4f3c\u65f6\u66f4\u6709\u6548\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63a2\u8ba8\u8bed\u8a00\u76f8\u5173\u4e0e\u8bed\u8a00\u65e0\u5173\u6570\u636e\u589e\u5f3a\u7b56\u7565\u7684\u6548\u679c\u5dee\u5f02\u3002", "method": "\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08Uspanteko\u548cArapaho\uff09\u4e0a\u6bd4\u8f83\u591a\u79cd\u6570\u636e\u589e\u5f3a\u7b56\u7565\u53ca\u5176\u7ec4\u5408\uff0c\u8bc4\u4f30\u673a\u5668\u7ffb\u8bd1\u548c\u95f4\u7ebf\u6027\u6ce8\u91ca\u4efb\u52a1\u3002", "result": "\u8bed\u8a00\u76f8\u5173\u7684\u7b56\u7565\u5728\u751f\u6210\u6570\u636e\u4e0e\u8bad\u7ec3\u5206\u5e03\u76f8\u4f3c\u65f6\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u8bed\u8a00\u76f8\u5173\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u9700\u786e\u4fdd\u751f\u6210\u6570\u636e\u4e0e\u8bad\u7ec3\u5206\u5e03\u76f8\u4f3c\u624d\u80fd\u4f18\u4e8e\u8bed\u8a00\u65e0\u5173\u7b56\u7565\u3002", "keywords": "\u6570\u636e\u589e\u5f3a\u3001\u4f4e\u8d44\u6e90\u8bed\u8a00\u3001\u673a\u5668\u7ffb\u8bd1\u3001\u95f4\u7ebf\u6027\u6ce8\u91ca"}}
{"id": "2506.03184", "pdf": "https://arxiv.org/pdf/2506.03184", "abs": "https://arxiv.org/abs/2506.03184", "authors": ["Mahe Zabin", "Ho-Jin Choi", "Md. Monirul Islam", "Jia Uddin"], "title": "Impact of Tuning Parameters in Deep Convolutional Neural Network Using a Crack Image Dataset", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "8 pages, 2 figures, published at Proceedings of the 15th KIPS\n  International Conference on Ubiquitous Information Technologies and\n  Applications (CUTE 2021), Jeju, Repubilc of Korea", "summary": "The performance of a classifier depends on the tuning of its parame ters. In\nthis paper, we have experimented the impact of various tuning parameters on the\nperformance of a deep convolutional neural network (DCNN). In the ex perimental\nevaluation, we have considered a DCNN classifier that consists of 2\nconvolutional layers (CL), 2 pooling layers (PL), 1 dropout, and a dense layer.\nTo observe the impact of pooling, activation function, and optimizer tuning pa\nrameters, we utilized a crack image dataset having two classes: negative and\npos itive. The experimental results demonstrate that with the maxpooling, the\nDCNN demonstrates its better performance for adam optimizer and tanh activation\nfunc tion.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08DCNN\uff09\u4e2d\u4e0d\u540c\u8c03\u53c2\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4f7f\u7528maxpooling\u3001adam\u4f18\u5316\u5668\u548ctanh\u6fc0\u6d3b\u51fd\u6570\u65f6\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u7d22DCNN\u4e2d\u8c03\u53c2\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4f18\u5316\u5206\u7c7b\u5668\u8868\u73b0\u3002", "method": "\u4f7f\u75282\u4e2a\u5377\u79ef\u5c42\u30012\u4e2a\u6c60\u5316\u5c42\u30011\u4e2adropout\u548c1\u4e2a\u5bc6\u96c6\u5c42\u7684DCNN\uff0c\u6d4b\u8bd5\u6c60\u5316\u65b9\u5f0f\u3001\u6fc0\u6d3b\u51fd\u6570\u548c\u4f18\u5316\u5668\u7684\u4e0d\u540c\u7ec4\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cmaxpooling\u3001adam\u4f18\u5316\u5668\u548ctanh\u6fc0\u6d3b\u51fd\u6570\u7684\u7ec4\u5408\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "DCNN\u7684\u6027\u80fd\u53d7\u8c03\u53c2\u663e\u8457\u5f71\u54cd\uff0c\u7279\u5b9a\u7ec4\u5408\uff08maxpooling\u3001adam\u3001tanh\uff09\u80fd\u63d0\u5347\u5206\u7c7b\u6548\u679c\u3002", "keywords": "DCNN, \u8c03\u53c2, \u6c60\u5316, \u6fc0\u6d3b\u51fd\u6570, \u4f18\u5316\u5668"}}
{"id": "2506.03474", "pdf": "https://arxiv.org/pdf/2506.03474", "abs": "https://arxiv.org/abs/2506.03474", "authors": ["Yifeng Xiao", "Yurong Xu", "Ning Yan", "Masood Mortazavi", "Pierluigi Nuzzo"], "title": "CORE: Constraint-Aware One-Step Reinforcement Learning for Simulation-Guided Neural Network Accelerator Design", "categories": ["cs.LG", "cs.AI", "cs.AR", "I.2.6; C.3"], "comment": "Preprint. 10 pages + appendix. Submitted to NeurIPS 2025", "summary": "Simulation-based design space exploration (DSE) aims to efficiently optimize\nhigh-dimensional structured designs under complex constraints and expensive\nevaluation costs. Existing approaches, including heuristic and multi-step\nreinforcement learning (RL) methods, struggle to balance sampling efficiency\nand constraint satisfaction due to sparse, delayed feedback, and large hybrid\naction spaces. In this paper, we introduce CORE, a constraint-aware, one-step\nRL method for simulationguided DSE. In CORE, the policy agent learns to sample\ndesign configurations by defining a structured distribution over them,\nincorporating dependencies via a scaling-graph-based decoder, and by reward\nshaping to penalize invalid designs based on the feedback obtained from\nsimulation. CORE updates the policy using a surrogate objective that compares\nthe rewards of designs within a sampled batch, without learning a value\nfunction. This critic-free formulation enables efficient learning by\nencouraging the selection of higher-reward designs. We instantiate CORE for\nhardware-mapping co-design of neural network accelerators, demonstrating that\nit significantly improves sample efficiency and achieves better accelerator\nconfigurations compared to state-of-the-art baselines. Our approach is general\nand applicable to a broad class of discrete-continuous constrained design\nproblems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCORE\u7684\u7ea6\u675f\u611f\u77e5\u4e00\u6b65\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u6a21\u62df\u5f15\u5bfc\u7684\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u5e76\u5728\u795e\u7ecf\u52a0\u901f\u5668\u914d\u7f6e\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u7ef4\u5ea6\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\u96be\u4ee5\u5e73\u8861\u91c7\u6837\u6548\u7387\u548c\u7ea6\u675f\u6ee1\u8db3\uff0c\u5c24\u5176\u662f\u5728\u7a00\u758f\u5ef6\u8fdf\u53cd\u9988\u548c\u5927\u6df7\u5408\u52a8\u4f5c\u7a7a\u95f4\u7684\u6311\u6218\u4e0b\u3002", "method": "CORE\u65b9\u6cd5\u901a\u8fc7\u5b9a\u4e49\u7ed3\u6784\u5316\u5206\u5e03\u548c\u57fa\u4e8e\u7f29\u653e\u56fe\u7684\u89e3\u7801\u5668\u6765\u5b66\u4e60\u8bbe\u8ba1\u914d\u7f6e\uff0c\u5229\u7528\u5956\u52b1\u5851\u5f62\u60e9\u7f5a\u65e0\u6548\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u65e0\u4ef7\u503c\u51fd\u6570\u7684\u4ee3\u7406\u76ee\u6807\u66f4\u65b0\u7b56\u7565\u3002", "result": "\u5728\u795e\u7ecf\u52a0\u901f\u5668\u786c\u4ef6\u6620\u5c04\u5171\u8bbe\u8ba1\u4e2d\uff0cCORE\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u5e76\u83b7\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u7684\u914d\u7f6e\u3002", "conclusion": "CORE\u662f\u4e00\u79cd\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u79bb\u6563-\u8fde\u7eed\u7ea6\u675f\u8bbe\u8ba1\u95ee\u9898\u3002", "keywords": "\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22, \u5f3a\u5316\u5b66\u4e60, \u7ea6\u675f\u611f\u77e5, \u795e\u7ecf\u52a0\u901f\u5668, \u91c7\u6837\u6548\u7387"}}
{"id": "2506.03598", "pdf": "https://arxiv.org/pdf/2506.03598", "abs": "https://arxiv.org/abs/2506.03598", "authors": ["Zetong Tang", "Qian Ma", "Di Wu"], "title": "Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments", "categories": ["cs.CL", "cs.AI", "68T50"], "comment": "4 pages,2 figures,EITCE 2025", "summary": "Using the best Text-to-SQL methods in resource-constrained environments is\nchallenging due to their reliance on resource-intensive open-source models.\nThis paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to\nbridge the gap between resource-efficient small open-source models and the\npowerful capabilities of large closed-source models for Text-to-SQL\ntranslation. Our method decomposes the task into schema filtering,\nretrieval-augmented text-to-SQL generation based on in-context examples, and\nprompt-driven schema linking and SQL generation. To improve schema selection\naccuracy, we fine-tune large language models. Crucially, we also explore the\nimpact of prompt engineering throughout the process, leveraging\nChain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly\nenhance the model's reasoning for accurate SQL generation. Comprehensive\nevaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL.", "AI": {"tldr": "AP-SQL\u662f\u4e00\u79cd\u65b0\u67b6\u6784\uff0c\u65e8\u5728\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u9ad8\u6548\u5229\u7528\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\u4e0e\u5927\u578b\u95ed\u6e90\u6a21\u578b\u7684\u80fd\u529b\u8fdb\u884cText-to-SQL\u7ffb\u8bd1\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u4f7f\u7528\u8d44\u6e90\u5bc6\u96c6\u578b\u5f00\u6e90\u6a21\u578b\u8fdb\u884cText-to-SQL\u7ffb\u8bd1\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\uff08\u6a21\u5f0f\u8fc7\u6ee4\u3001\u57fa\u4e8e\u4e0a\u4e0b\u6587\u793a\u4f8b\u7684\u68c0\u7d22\u589e\u5f3aText-to-SQL\u751f\u6210\u3001\u63d0\u793a\u9a71\u52a8\u7684\u6a21\u5f0f\u94fe\u63a5\u548cSQL\u751f\u6210\uff09\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u548c\u63d0\u793a\u5de5\u7a0b\uff08CoT\u548cGoT\u6a21\u677f\uff09\u6765\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "result": "\u5728Spider\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86AP-SQL\u7684\u6709\u6548\u6027\u3002", "conclusion": "AP-SQL\u6210\u529f\u5f25\u5408\u4e86\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\u4e0e\u5927\u578b\u95ed\u6e90\u6a21\u578b\u4e4b\u95f4\u7684\u80fd\u529b\u5dee\u8ddd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684Text-to-SQL\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "Text-to-SQL, \u5c0f\u578b\u5f00\u6e90\u6a21\u578b, \u5927\u578b\u95ed\u6e90\u6a21\u578b, \u63d0\u793a\u5de5\u7a0b, \u8d44\u6e90\u4f18\u5316"}}
{"id": "2506.03185", "pdf": "https://arxiv.org/pdf/2506.03185", "abs": "https://arxiv.org/abs/2506.03185", "authors": ["Liangrui Pan", "Xingchen Li", "Zhongyi Chen", "Ling Chu", "Shaoliang Peng"], "title": "DLiPath: A Benchmark for the Comprehensive Assessment of Donor Liver Based on Histopathological Image Dataset", "categories": ["eess.IV", "cs.AI", "cs.CV", "q-bio.QM"], "comment": "Submit to ACM MM2025", "summary": "Pathologists comprehensive evaluation of donor liver biopsies provides\ncrucial information for accepting or discarding potential grafts. However,\nrapidly and accurately obtaining these assessments intraoperatively poses a\nsignificant challenge for pathologists. Features in donor liver biopsies, such\nas portal tract fibrosis, total steatosis, macrovesicular steatosis, and\nhepatocellular ballooning are correlated with transplant outcomes, yet\nquantifying these indicators suffers from substantial inter- and intra-observer\nvariability. To address this, we introduce DLiPath, the first benchmark for\ncomprehensive donor liver assessment based on a histopathology image dataset.\nWe collected and publicly released 636 whole slide images from 304 donor liver\npatients at the Department of Pathology, the Third Xiangya Hospital, with\nexpert annotations for key pathological features (including cholestasis, portal\ntract fibrosis, portal inflammation, total steatosis, macrovesicular steatosis,\nand hepatocellular ballooning). We selected nine state-of-the-art\nmultiple-instance learning (MIL) models based on the DLiPath dataset as\nbaselines for extensive comparative analysis. The experimental results\ndemonstrate that several MIL models achieve high accuracy across donor liver\nassessment indicators on DLiPath, charting a clear course for future automated\nand intelligent donor liver assessment research. Data and code are available at\nhttps://github.com/panliangrui/ACM_MM_2025.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DLiPath\uff0c\u9996\u4e2a\u57fa\u4e8e\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u7684\u4f9b\u809d\u8bc4\u4f30\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u4f9b\u809d\u6d3b\u68c0\u4e2d\u75c5\u7406\u7279\u5f81\u8bc4\u4f30\u7684\u53d8\u5f02\u6027\u95ee\u9898\u3002", "motivation": "\u4f9b\u809d\u6d3b\u68c0\u4e2d\u75c5\u7406\u7279\u5f81\u7684\u5feb\u901f\u51c6\u786e\u8bc4\u4f30\u5bf9\u79fb\u690d\u6210\u529f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u7684\u89c2\u5bdf\u8005\u95f4\u548c\u89c2\u5bdf\u8005\u5185\u53d8\u5f02\u6027\u3002", "method": "\u6536\u96c6\u5e76\u516c\u5f00\u4e86636\u5f20\u4f9b\u809d\u60a3\u8005\u7684\u5168\u5207\u7247\u56fe\u50cf\uff0c\u6807\u6ce8\u4e86\u5173\u952e\u75c5\u7406\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u4e5d\u79cd\u591a\u5b9e\u4f8b\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u90e8\u5206\u591a\u5b9e\u4f8b\u5b66\u4e60\u6a21\u578b\u5728DLiPath\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "DLiPath\u4e3a\u672a\u6765\u81ea\u52a8\u5316\u4f9b\u809d\u8bc4\u4f30\u7814\u7a76\u63d0\u4f9b\u4e86\u660e\u786e\u65b9\u5411\u3002", "keywords": "\u4f9b\u809d\u8bc4\u4f30,\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf,\u591a\u5b9e\u4f8b\u5b66\u4e60,\u79fb\u690d\u75c5\u7406\u5b66"}}
{"id": "2506.03522", "pdf": "https://arxiv.org/pdf/2506.03522", "abs": "https://arxiv.org/abs/2506.03522", "authors": ["Daniel Campa", "Mehdi Saeedi", "Ian Colbert", "Srinjoy Das"], "title": "Path Generation and Evaluation in Video Games: A Nonparametric Statistical Approach", "categories": ["cs.LG", "stat.ML"], "comment": "8 pages, 9 figures, Accepted at the IEEE Conference on Games 2025\n  (IEEE CoG)", "summary": "Navigation path traces play a crucial role in video game design, serving as a\nvital resource for both enhancing player engagement and fine-tuning\nnon-playable character behavior. Generating such paths with human-like realism\ncan enrich the overall gaming experience, and evaluating path traces can\nprovide game designers insights into player interactions. Despite the\nimpressive recent advancements in deep learning-based generative modeling, the\nvideo game industry hesitates to adopt such models for path generation, often\nciting their complex training requirements and interpretability challenges. To\naddress these problems, we propose a novel path generation and evaluation\napproach that is grounded in principled nonparametric statistics and provides\nprecise control while offering interpretable insights. Our path generation\nmethod fuses two statistical techniques: (1) nonparametric model-free\ntransformations that capture statistical characteristics of path traces through\ntime; and (2) copula models that capture statistical dependencies in space. For\npath evaluation, we adapt a nonparametric three-sample hypothesis test designed\nto determine if the generated paths are overfit (mimicking the original data\ntoo closely) or underfit (diverging too far from it). We demonstrate the\nprecision and reliability of our proposed methods with empirical analysis on\ntwo existing gaming benchmarks to showcase controlled generation of diverse\nnavigation paths. Notably, our novel path generator can be fine-tuned with user\ncontrollable parameters to create navigation paths that exhibit varying levels\nof human-likeness in contrast to those produced by neural network-based agents.\nThe code is available at https://github.com/daniel-campa/mf-copula.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u53c2\u6570\u7edf\u8ba1\u7684\u65b0\u9896\u8def\u5f84\u751f\u6210\u4e0e\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u6e38\u620f\u8def\u5f84\u751f\u6210\u4e2d\u7684\u590d\u6742\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u89c6\u9891\u6e38\u620f\u8def\u5f84\u751f\u6210\u4e2d\u7684\u8bad\u7ec3\u590d\u6742\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u4e3a\u6e38\u620f\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u53ef\u63a7\u4e14\u53ef\u89e3\u91ca\u7684\u8def\u5f84\u751f\u6210\u5de5\u5177\u3002", "method": "\u7ed3\u5408\u975e\u53c2\u6570\u65e0\u6a21\u578b\u53d8\u6362\u548ccopula\u6a21\u578b\uff0c\u751f\u6210\u5177\u6709\u7edf\u8ba1\u7279\u6027\u7684\u8def\u5f84\uff1b\u91c7\u7528\u975e\u53c2\u6570\u4e09\u6837\u672c\u5047\u8bbe\u68c0\u9a8c\u8bc4\u4f30\u8def\u5f84\u3002", "result": "\u65b9\u6cd5\u5728\u73b0\u6709\u6e38\u620f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u751f\u6210\u591a\u6837\u5316\u7684\u5bfc\u822a\u8def\u5f84\uff0c\u5e76\u53ef\u8c03\u8282\u53c2\u6570\u63a7\u5236\u8def\u5f84\u7684\u4eba\u4e3a\u76f8\u4f3c\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u8ba1\u65b9\u6cd5\u4e3a\u6e38\u620f\u8def\u5f84\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u63a7\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u65b9\u6cd5\u3002", "keywords": "\u8def\u5f84\u751f\u6210, \u975e\u53c2\u6570\u7edf\u8ba1, copula\u6a21\u578b, \u6e38\u620f\u8bbe\u8ba1, \u53ef\u89e3\u91ca\u6027"}}
{"id": "2506.03616", "pdf": "https://arxiv.org/pdf/2506.03616", "abs": "https://arxiv.org/abs/2506.03616", "authors": ["Eunki Kim", "Sangryul Kim", "James Thorne"], "title": "Learning to Insert [PAUSE] Tokens for Better Reasoning", "categories": ["cs.CL"], "comment": "18 pages, 5 figures, ACL findings", "summary": "To enhance reasoning capabilities, previous works have explored incorporating\nspecial-purpose tokens into the training process. These strategies strengthen\nthe learning mechanism of transformer-based large language models (LLMs).\nBuilding on prior research, in which inserting dummy tokens consecutively just\nbefore reasoning steps can enhance effectiveness, we introduce a novel approach\ntermed Dynamic Inserting Tokens Training (DIT). Our method identifies positions\nwithin sequences where model confidence is lowest according to token\nlog-likelihood. Strategically inserting [PAUSE] tokens on these positions\nbolsters the model's predictive capabilities for subsequent tokens.\nExperimental results across diverse datasets and models, from the 2.7B model to\nthe 8B model, demonstrate that DIT consistently outperforms traditional\nfine-tuning and previous token insertion methods. With this simple yet\neffective method, we achieve accuracy gains of up to 4.7%p on GSM8K, 3.23%p on\nAQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets. Our work\nshows a model-based, dynamic approach rather than a heuristic one, thereby\nbroadening the scope of research in reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u63d2\u5165\u4ee4\u724c\u8bad\u7ec3\u65b9\u6cd5\uff08DIT\uff09\uff0c\u901a\u8fc7\u5728\u5e8f\u5217\u4e2d\u7f6e\u4fe1\u5ea6\u6700\u4f4e\u7684\u4f4d\u7f6e\u63d2\u5165[PAUSE]\u4ee4\u724c\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u9884\u6d4b\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\u548c\u5176\u4ed6\u4ee4\u724c\u63d2\u5165\u65b9\u6cd5\u3002", "motivation": "\u901a\u8fc7\u52a8\u6001\u63d2\u5165\u4ee4\u724c\u589e\u5f3aTransformer\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8d85\u8d8a\u5148\u524d\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDIT\u65b9\u6cd5\uff0c\u6839\u636e\u4ee4\u724c\u5bf9\u6570\u4f3c\u7136\u8bc6\u522b\u4f4e\u7f6e\u4fe1\u5ea6\u4f4d\u7f6e\uff0c\u5e76\u63d2\u5165[PAUSE]\u4ee4\u724c\u4ee5\u63d0\u5347\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5982GSM8K\u51c6\u786e\u7387\u63d0\u9ad84.7%p\uff0cMBPP pass@1\u63d0\u9ad83.4%p\u3002", "conclusion": "DIT\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u52a8\u6001\u65b9\u6cd5\uff0c\u4e3a\u63a8\u7406\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u52a8\u6001\u63d2\u5165\u4ee4\u724c\u3001\u63a8\u7406\u80fd\u529b\u3001Transformer\u6a21\u578b\u3001\u5bf9\u6570\u4f3c\u7136\u3001DIT"}}
{"id": "2506.03186", "pdf": "https://arxiv.org/pdf/2506.03186", "abs": "https://arxiv.org/abs/2506.03186", "authors": ["Duaa Kareem Qasim", "Sabah Abdulazeez Jebur", "Lafta Raheem Ali", "Abdul Jalil M. Khalaf", "Abir Jaafar Hussain"], "title": "Lightweight Convolutional Neural Networks for Retinal Disease Classification", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "cs.NE"], "comment": null, "summary": "Retinal diseases such as Diabetic Retinopathy (DR) and Macular Hole (MH)\nsignificantly impact vision and affect millions worldwide. Early detection is\ncrucial, as DR, a complication of diabetes, damages retinal blood vessels,\npotentially leading to blindness, while MH disrupts central vision, affecting\ntasks like reading and facial recognition. This paper employed two lightweight\nand efficient Convolution Neural Network architectures, MobileNet and\nNASNetMobile, for the classification of Normal, DR, and MH retinal images. The\nmodels were trained on the RFMiD dataset, consisting of 3,200 fundus images,\nafter undergoing preprocessing steps such as resizing, normalization, and\naugmentation. To address data scarcity, this study leveraged transfer learning\nand data augmentation techniques, enhancing model generalization and\nperformance. The experimental results demonstrate that MobileNetV2 achieved the\nhighest accuracy of 90.8%, outperforming NASNetMobile, which achieved 89.5%\naccuracy. These findings highlight the effectiveness of CNNs in retinal disease\nclassification, providing a foundation for AI-assisted ophthalmic diagnosis and\nearly intervention.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528MobileNet\u548cNASNetMobile\u4e24\u79cd\u8f7b\u91cf\u7ea7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5bf9\u6b63\u5e38\u3001\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u548c\u9ec4\u6591\u88c2\u5b54\uff08MH\uff09\u7684\u89c6\u7f51\u819c\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\uff0cMobileNetV2\u4ee590.8%\u7684\u51c6\u786e\u7387\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u548c\u9ec4\u6591\u88c2\u5b54\u4e25\u91cd\u5f71\u54cd\u89c6\u529b\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u9632\u6b62\u89c6\u529b\u4e27\u5931\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528MobileNet\u548cNASNetMobile\u6a21\u578b\u5728RFMiD\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002", "result": "MobileNetV2\u51c6\u786e\u7387\u8fbe90.8%\uff0c\u4f18\u4e8eNASNetMobile\u768489.5%\u3002", "conclusion": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u89c6\u7f51\u819c\u75be\u75c5\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3aAI\u8f85\u52a9\u773c\u79d1\u8bca\u65ad\u63d0\u4f9b\u57fa\u7840\u3002", "keywords": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8,\u9ec4\u6591\u88c2\u5b54,\u5377\u79ef\u795e\u7ecf\u7f51\u7edc,MobileNet,NASNetMobile"}}
{"id": "2506.03531", "pdf": "https://arxiv.org/pdf/2506.03531", "abs": "https://arxiv.org/abs/2506.03531", "authors": ["Daniel Ovalle", "Lorenz T. Biegler", "Ignacio E. Grossmann", "Carl D. Laird", "Mateo Dulce Rubio"], "title": "Conformal Mixed-Integer Constraint Learning with Feasibility Guarantees", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "We propose Conformal Mixed-Integer Constraint Learning (C-MICL), a novel\nframework that provides probabilistic feasibility guarantees for data-driven\nconstraints in optimization problems. While standard Mixed-Integer Constraint\nLearning methods often violate the true constraints due to model error or data\nlimitations, our C-MICL approach leverages conformal prediction to ensure\nfeasible solutions are ground-truth feasible. This guarantee holds with\nprobability at least $1{-}\\alpha$, under a conditional independence assumption.\nThe proposed framework supports both regression and classification tasks\nwithout requiring access to the true constraint function, while avoiding the\nscalability issues associated with ensemble-based heuristics. Experiments on\nreal-world applications demonstrate that C-MICL consistently achieves target\nfeasibility rates, maintains competitive objective performance, and\nsignificantly reduces computational cost compared to existing methods. Our work\nbridges mathematical optimization and machine learning, offering a principled\napproach to incorporate uncertainty-aware constraints into decision-making with\nrigorous statistical guarantees.", "AI": {"tldr": "C-MICL\u6846\u67b6\u901a\u8fc7\u4fdd\u5f62\u9884\u6d4b\u4e3a\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u6570\u636e\u9a71\u52a8\u7ea6\u675f\u63d0\u4f9b\u6982\u7387\u53ef\u884c\u6027\u4fdd\u8bc1\uff0c\u786e\u4fdd\u89e3\u7684\u771f\u5b9e\u53ef\u884c\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u6df7\u5408\u6574\u6570\u7ea6\u675f\u5b66\u4e60\u65b9\u6cd5\u5e38\u56e0\u6a21\u578b\u8bef\u5dee\u6216\u6570\u636e\u9650\u5236\u8fdd\u53cd\u771f\u5b9e\u7ea6\u675f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u8bc1\u53ef\u884c\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "C-MICL\u7ed3\u5408\u4fdd\u5f62\u9884\u6d4b\u6280\u672f\uff0c\u652f\u6301\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\uff0c\u65e0\u9700\u8bbf\u95ee\u771f\u5b9e\u7ea6\u675f\u51fd\u6570\uff0c\u907f\u514d\u57fa\u4e8e\u96c6\u6210\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u6269\u5c55\u6027\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cC-MICL\u8fbe\u5230\u76ee\u6807\u53ef\u884c\u6027\u7387\uff0c\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "C-MICL\u4e3a\u6570\u5b66\u4f18\u5316\u4e0e\u673a\u5668\u5b66\u4e60\u7ed3\u5408\u63d0\u4f9b\u4e86\u4e25\u683c\u7edf\u8ba1\u4fdd\u8bc1\u7684\u539f\u5219\u6027\u65b9\u6cd5\u3002", "keywords": "C-MICL,\u4fdd\u5f62\u9884\u6d4b,\u6570\u636e\u9a71\u52a8\u7ea6\u675f,\u4f18\u5316\u95ee\u9898"}}
{"id": "2506.03619", "pdf": "https://arxiv.org/pdf/2506.03619", "abs": "https://arxiv.org/abs/2506.03619", "authors": ["Ayuto Tsutsumi", "Yuu Jinnai"], "title": "Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales", "categories": ["cs.CL"], "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated strong language\nunderstanding and generation abilities across various languages, their cultural\nknowledge is often limited to English-speaking communities, which can\nmarginalize the cultures of non-English communities. To address the problem,\nevaluation of the cultural awareness of the LLMs and the methods to develop\nculturally aware LLMs have been investigated. In this study, we focus on\nevaluating knowledge of folktales, a key medium for conveying and circulating\nculture. In particular, we focus on Japanese folktales, specifically on\nknowledge of Yokai. Yokai are supernatural creatures originating from Japanese\nfolktales that continue to be popular motifs in art and entertainment today.\nYokai have long served as a medium for cultural expression, making them an\nideal subject for assessing the cultural awareness of LLMs. We introduce\nYokaiEval, a benchmark dataset consisting of 809 multiple-choice questions\n(each with four options) designed to probe knowledge about yokai. We evaluate\nthe performance of 31 Japanese and multilingual LLMs on this dataset. The\nresults show that models trained with Japanese language resources achieve\nhigher accuracy than English-centric models, with those that underwent\ncontinued pretraining in Japanese, particularly those based on Llama-3,\nperforming especially well. The code and dataset are available at\nhttps://github.com/CyberAgentA ILab/YokaiEval.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faYokaiEval\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u65e5\u8bed\u5996\u602a\u6587\u5316\u77e5\u8bc6\u4e0a\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u65e5\u8bed\u8bad\u7ec3\u7684\u6a21\u578b\u4f18\u4e8e\u82f1\u8bed\u6a21\u578b\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6587\u5316\u610f\u8bc6\uff0c\u7279\u522b\u662f\u975e\u82f1\u8bed\u793e\u533a\u7684\u6587\u5316\u77e5\u8bc6\uff0c\u5982\u65e5\u672c\u5996\u602a\u6545\u4e8b\u3002", "method": "\u5f15\u5165\u5305\u542b809\u9053\u591a\u9009\u9898\u7684YokaiEval\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6d4b\u8bd531\u79cd\u65e5\u8bed\u548c\u591a\u8bed\u8a00LLMs\u3002", "result": "\u65e5\u8bed\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u82f1\u8bed\u6a21\u578b\uff0c\u5c24\u5176\u662f\u57fa\u4e8eLlama-3\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u65e5\u8bed\u8bad\u7ec3\u7684\u8d44\u6e90\u5bf9LLMs\u7684\u6587\u5316\u77e5\u8bc6\u7406\u89e3\u81f3\u5173\u91cd\u8981\uff0cYokaiEval\u4e3a\u6587\u5316\u610f\u8bc6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "keywords": "LLMs, \u6587\u5316\u610f\u8bc6, \u5996\u602a, \u65e5\u8bed, \u591a\u8bed\u8a00\u8bc4\u4f30"}}
{"id": "2506.03188", "pdf": "https://arxiv.org/pdf/2506.03188", "abs": "https://arxiv.org/abs/2506.03188", "authors": ["Madhu Babu Sikha", "Lalith Appari", "Gurudatt Nanjanagudu Ganesh", "Amay Bandodkar", "Imon Banerjee"], "title": "Multi-Analyte, Swab-based Automated Wound Monitor with AI", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.HC"], "comment": "4 pages conference paper", "summary": "Diabetic foot ulcers (DFUs), a class of chronic wounds, affect ~750,000\nindividuals every year in the US alone and identifying non-healing DFUs that\ndevelop to chronic wounds early can drastically reduce treatment costs and\nminimize risks of amputation. There is therefore a pressing need for diagnostic\ntools that can detect non-healing DFUs early. We develop a low cost,\nmulti-analyte 3D printed assays seamlessly integrated on swabs that can\nidentify non-healing DFUs and a Wound Sensor iOS App - an innovative mobile\napplication developed for the controlled acquisition and automated analysis of\nwound sensor data. By comparing both the original base image (before exposure\nto the wound) and the wound-exposed image, we developed automated computer\nvision techniques to compare density changes between the two assay images,\nwhich allow us to automatically determine the severity of the wound. The iOS\napp ensures accurate data collection and presents actionable insights, despite\nchallenges such as variations in camera configurations and ambient conditions.\nThe proposed integrated sensor and iOS app will allow healthcare professionals\nto monitor wound conditions real-time, track healing progress, and assess\ncritical parameters related to wound care.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u7684\u591a\u5206\u6790\u72693D\u6253\u5370\u68c0\u6d4b\u8bd5\u7eb8\u548ciOS\u5e94\u7528\u7a0b\u5e8f\uff0c\u7528\u4e8e\u65e9\u671f\u8bc6\u522b\u7cd6\u5c3f\u75c5\u8db3\u6e83\u75a1\uff0c\u5e76\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u81ea\u52a8\u5206\u6790\u4f24\u53e3\u4e25\u91cd\u7a0b\u5ea6\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u8db3\u6e83\u75a1\u662f\u4e00\u79cd\u6162\u6027\u4f24\u53e3\uff0c\u65e9\u671f\u8bc6\u522b\u975e\u6108\u5408\u4f24\u53e3\u53ef\u4ee5\u5927\u5e45\u964d\u4f4e\u6cbb\u7597\u6210\u672c\u5e76\u51cf\u5c11\u622a\u80a2\u98ce\u9669\u3002", "method": "\u5f00\u53d1\u4e863D\u6253\u5370\u7684\u591a\u5206\u6790\u7269\u68c0\u6d4b\u8bd5\u7eb8\u548ciOS\u5e94\u7528\u7a0b\u5e8f\uff0c\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u6bd4\u8f83\u4f24\u53e3\u524d\u540e\u7684\u8bd5\u7eb8\u56fe\u50cf\u5bc6\u5ea6\u53d8\u5316\uff0c\u81ea\u52a8\u786e\u5b9a\u4f24\u53e3\u4e25\u91cd\u7a0b\u5ea6\u3002", "result": "iOS\u5e94\u7528\u7a0b\u5e8f\u786e\u4fdd\u6570\u636e\u51c6\u786e\u6027\u5e76\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u5e2e\u52a9\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u5b9e\u65f6\u76d1\u6d4b\u4f24\u53e3\u72b6\u51b5\u3002", "conclusion": "\u8be5\u96c6\u6210\u4f20\u611f\u5668\u548ciOS\u5e94\u7528\u80fd\u5e2e\u52a9\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u5b9e\u65f6\u76d1\u63a7\u4f24\u53e3\u72b6\u51b5\u5e76\u8ddf\u8e2a\u6108\u5408\u8fdb\u5c55\u3002", "keywords": "\u7cd6\u5c3f\u75c5\u8db3\u6e83\u75a1, 3D\u6253\u5370, \u8ba1\u7b97\u673a\u89c6\u89c9, iOS\u5e94\u7528\u7a0b\u5e8f"}}
{"id": "2506.03542", "pdf": "https://arxiv.org/pdf/2506.03542", "abs": "https://arxiv.org/abs/2506.03542", "authors": ["Yongxiang Tang", "Yanhua Cheng", "Xiaocheng Liu", "Chenchen Jiao", "Yanxiang Zeng", "Ning Luo", "Pengjia Yuan", "Xialong Liu", "Peng Jiang"], "title": "Learning Monotonic Probabilities with a Generative Cost Model", "categories": ["cs.LG"], "comment": null, "summary": "In many machine learning tasks, it is often necessary for the relationship\nbetween input and output variables to be monotonic, including both strictly\nmonotonic and implicitly monotonic relationships. Traditional methods for\nmaintaining monotonicity mainly rely on construction or regularization\ntechniques, whereas this paper shows that the issue of strict monotonic\nprobability can be viewed as a partial order between an observable revenue\nvariable and a latent cost variable. This perspective enables us to reformulate\nthe monotonicity challenge into modeling the latent cost variable. To tackle\nthis, we introduce a generative network for the latent cost variable, termed\nthe Generative Cost Model (GCM), which inherently addresses the strict\nmonotonic problem, and propose the Implicit Generative Cost Model (IGCM) to\naddress the implicit monotonic problem. We further validate our approach with a\nnumerical simulation of quantile regression and conduct multiple experiments on\npublic datasets, showing that our method significantly outperforms existing\nmonotonic modeling techniques. The code for our experiments can be found at\nhttps://github.com/tyxaaron/GCM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89d2\uff0c\u5c06\u4e25\u683c\u5355\u8c03\u6982\u7387\u95ee\u9898\u89c6\u4e3a\u89c2\u6d4b\u53d8\u91cf\u4e0e\u6f5c\u5728\u6210\u672c\u53d8\u91cf\u7684\u504f\u5e8f\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u751f\u6210\u6a21\u578bGCM\u548cIGCM\u6765\u5206\u522b\u89e3\u51b3\u4e25\u683c\u548c\u9690\u5f0f\u5355\u8c03\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5728\u8bb8\u591a\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u8f93\u5165\u4e0e\u8f93\u51fa\u53d8\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u9700\u8981\u662f\u5355\u8c03\u7684\uff0c\u5305\u62ec\u4e25\u683c\u5355\u8c03\u548c\u9690\u5f0f\u5355\u8c03\u5173\u7cfb\u3002\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u6784\u5efa\u6216\u6b63\u5219\u5316\u6280\u672f\u6765\u4fdd\u6301\u5355\u8c03\u6027\uff0c\u672c\u6587\u901a\u8fc7\u65b0\u7684\u89c6\u89d2\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u5355\u8c03\u6027\u95ee\u9898\u91cd\u65b0\u5efa\u6a21\u4e3a\u6f5c\u5728\u6210\u672c\u53d8\u91cf\u7684\u5efa\u6a21\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u751f\u6210\u6210\u672c\u6a21\u578b(GCM)\u89e3\u51b3\u4e25\u683c\u5355\u8c03\u95ee\u9898\uff0c\u4ee5\u53ca\u9690\u5f0f\u751f\u6210\u6210\u672c\u6a21\u578b(IGCM)\u89e3\u51b3\u9690\u5f0f\u5355\u8c03\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u548c\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u8c03\u5efa\u6a21\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684GCM\u548cIGCM\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u4e25\u683c\u548c\u9690\u5f0f\u5355\u8c03\u95ee\u9898\uff0c\u4e3a\u5355\u8c03\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5355\u8c03\u6027, \u6f5c\u5728\u53d8\u91cf, \u751f\u6210\u6a21\u578b, GCM, IGCM, \u673a\u5668\u5b66\u4e60"}}
{"id": "2506.03627", "pdf": "https://arxiv.org/pdf/2506.03627", "abs": "https://arxiv.org/abs/2506.03627", "authors": ["Lin Mu", "Guowei Chu", "Li Ni", "Lei Sang", "Zhize Wu", "Peiquan Jin", "Yiwen Zhang"], "title": "Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks", "categories": ["cs.CL", "cs.AI"], "comment": "13pages", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks by effectively utilizing a prompting strategy. However, they are\nhighly sensitive to input perturbations, such as typographical errors or slight\ncharacter order errors, which can substantially degrade their performance.\nDespite advances in prompting techniques, developing a prompting strategy that\nexplicitly mitigates the negative impact of such perturbations remains an open\nchallenge. To bridge this gap, we propose Robustness of Prompting (RoP), a\nnovel prompting strategy specifically designed to enhance the robustness of\nLLMs. RoP consists of two stages: Error Correction and Guidance. In the Error\nCorrection stage, RoP applies diverse perturbation methods to generate\nadversarial examples, which are then used to construct prompts that\nautomatically correct input errors. In the Guidance stage, RoP generates an\noptimal guidance prompting based on the corrected input, steering the model\ntoward more robust and accurate inferences. Through comprehensive experiments\nspanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate\nthat RoP significantly improves LLMs' robustness against adversarial\nperturbations. Notably, it maintains model accuracy with only minimal\ndegradation compared to clean input scenarios, thereby establishing RoP as a\npractical and effective approach for enhancing LLM robustness in real-world\napplications.", "AI": {"tldr": "LLMs\u5bf9\u8f93\u5165\u6270\u52a8\u654f\u611f\uff0cRoP\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u63d0\u793a\u7b56\u7565\uff0c\u901a\u8fc7\u9519\u8bef\u7ea0\u6b63\u548c\u5f15\u5bfc\u4e24\u9636\u6bb5\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3LLMs\u5bf9\u8f93\u5165\u6270\u52a8\u654f\u611f\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7a33\u5065\u6027\u3002", "method": "RoP\u5206\u4e3a\u9519\u8bef\u7ea0\u6b63\u548c\u5f15\u5bfc\u4e24\u9636\u6bb5\uff0c\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u6027\u793a\u4f8b\u548c\u4f18\u5316\u63d0\u793a\u6765\u51cf\u5c11\u6270\u52a8\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRoP\u663e\u8457\u63d0\u5347LLMs\u5bf9\u6297\u6270\u52a8\u80fd\u529b\uff0c\u51e0\u4e4e\u4e0d\u964d\u4f4e\u5e72\u51c0\u8f93\u5165\u7684\u6027\u80fd\u3002", "conclusion": "RoP\u662f\u4e00\u79cd\u5b9e\u7528\u6709\u6548\u7684\u63d0\u793a\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u589e\u5f3aLLMs\u7684\u9c81\u68d2\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u63d0\u793a\u7b56\u7565\u3001\u9c81\u68d2\u6027\u3001\u9519\u8bef\u7ea0\u6b63\u3001\u5bf9\u6297\u6270\u52a8"}}
{"id": "2506.03189", "pdf": "https://arxiv.org/pdf/2506.03189", "abs": "https://arxiv.org/abs/2506.03189", "authors": ["Ghada Sokar", "Gintare Karolina Dziugaite", "Anurag Arnab", "Ahmet Iscen", "Pablo Samuel Castro", "Cordelia Schmid"], "title": "Continual Learning in Vision-Language Models via Aligned Model Merging", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Continual learning is conventionally tackled through sequential fine-tuning,\na process that, while enabling adaptation, inherently favors plasticity over\nthe stability needed to retain prior knowledge. While existing approaches\nattempt to mitigate catastrophic forgetting, a bias towards recent tasks\npersists as they build upon this sequential nature. In this work we present a\nnew perspective based on model merging to maintain stability while still\nretaining plasticity. Rather than just sequentially updating the model weights,\nwe propose merging newly trained task parameters with previously learned ones,\npromoting a better balance. To maximize the effectiveness of the merging\nprocess, we propose a simple mechanism that promotes learning aligned weights\nwith previous ones, thereby avoiding interference when merging. We evaluate\nthis approach on large Vision-Language Models (VLMs), and demonstrate its\neffectiveness in reducing forgetting, increasing robustness to various task\norders and similarities, and improving generalization.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u5408\u5e76\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u5e76\u65b0\u65e7\u4efb\u52a1\u53c2\u6570\u6765\u5e73\u8861\u53ef\u5851\u6027\u4e0e\u7a33\u5b9a\u6027\uff0c\u51cf\u5c11\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u987a\u5e8f\u5fae\u8c03\uff0c\u5bfc\u81f4\u5bf9\u65b0\u4efb\u52a1\u7684\u504f\u597d\u548c\u65e7\u77e5\u8bc6\u7684\u9057\u5fd8\uff0c\u7f3a\u4e4f\u7a33\u5b9a\u6027\u4e0e\u53ef\u5851\u6027\u7684\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u5e76\u65b0\u65e7\u4efb\u52a1\u53c2\u6570\u800c\u975e\u987a\u5e8f\u66f4\u65b0\u6743\u91cd\uff0c\u540c\u65f6\u5f15\u5165\u673a\u5236\u786e\u4fdd\u65b0\u65e7\u6743\u91cd\u5bf9\u9f50\u4ee5\u907f\u514d\u5e72\u6270\u3002", "result": "\u5728\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u51cf\u5c11\u9057\u5fd8\uff0c\u63d0\u5347\u5bf9\u4efb\u52a1\u987a\u5e8f\u548c\u76f8\u4f3c\u6027\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u6539\u5584\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u7a33\u5b9a\u6027\u4e0e\u53ef\u5851\u6027\u7684\u66f4\u597d\u5e73\u8861\uff0c\u4e3a\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u6301\u7eed\u5b66\u4e60,\u6a21\u578b\u5408\u5e76,\u89c6\u89c9\u8bed\u8a00\u6a21\u578b,\u707e\u96be\u6027\u9057\u5fd8"}}
{"id": "2506.03556", "pdf": "https://arxiv.org/pdf/2506.03556", "abs": "https://arxiv.org/abs/2506.03556", "authors": ["Wang WeiQuan", "Riaz-ul-Haque Mian"], "title": "Optimizing FPGA and Wafer Test Coverage with Spatial Sampling and Machine Learning", "categories": ["cs.LG"], "comment": null, "summary": "In semiconductor manufacturing, testing costs remain significantly high,\nespecially during wafer and FPGA testing. To reduce the number of required\ntests while maintaining predictive accuracy, this study investigates three\nbaseline sampling strategies: Random Sampling, Stratified Sampling, and k-means\nClustering Sampling. To further enhance these methods, this study proposes a\nnovel algorithm that improves the sampling quality of each approach. This\nresearch is conducted using real industrial production data from wafer-level\ntests and silicon measurements from various FPGAs. This study introduces two\nhybrid strategies: Stratified with Short Distance Elimination (S-SDE) and\nk-means with Short Distance Elimination (K-SDE). Their performance is evaluated\nwithin the framework of Gaussian Process Regression (GPR) for predicting wafer\nand FPGA test data. At the core of our proposed approach is the Short Distance\nElimination (SDE) algorithm, which excludes spatially proximate candidate\npoints during sampling, thereby ensuring a more uniform distribution of\ntraining data across the physical domain. A parameter sweep was conducted over\nthe (alpha, beta) thresholds, where alpha and beta are in the range {0, 1, 2,\n3, 4} and not both zero, to identify the optimal combination that minimizes\nRMSD. Experimental results on a randomly selected wafer file reveal that\n(alpha, beta) equal (2, 2) yields the lowest RMSD. Accordingly, all subsequent\nexperiments adopt this parameter configuration. The results demonstrate that\nthe proposed SDE-based strategies enhance predictive accuracy: K-SDE improves\nupon k-means sampling by 16.26 percent (wafer) and 13.07 percent (FPGA), while\nS-SDE improves upon stratified sampling by 16.49 percent (wafer) and 8.84\npercent (FPGA).", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e09\u79cd\u91c7\u6837\u7b56\u7565\uff08\u968f\u673a\u3001\u5206\u5c42\u548ck\u5747\u503c\u805a\u7c7b\uff09\u4ee5\u51cf\u5c11\u534a\u5bfc\u4f53\u6d4b\u8bd5\u6210\u672c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5SDE\u4ee5\u6539\u8fdb\u91c7\u6837\u8d28\u91cf\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eSDE\u7684\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u534a\u5bfc\u4f53\u5236\u9020\u4e2d\u6d4b\u8bd5\u6210\u672c\u9ad8\uff0c\u9700\u8981\u51cf\u5c11\u6d4b\u8bd5\u6570\u91cf\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faSDE\u7b97\u6cd5\u6539\u8fdb\u91c7\u6837\u7b56\u7565\uff0c\u5305\u62ecS-SDE\u548cK-SDE\uff0c\u5e76\u4e0e\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u7ed3\u5408\u8bc4\u4f30\u6027\u80fd\u3002", "result": "SDE\u7b56\u7565\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\uff1aK-SDE\u6bd4k\u5747\u503c\u91c7\u6837\u63d0\u9ad816.26%\uff08\u6676\u5706\uff09\u548c13.07%\uff08FPGA\uff09\uff0cS-SDE\u6bd4\u5206\u5c42\u91c7\u6837\u63d0\u9ad816.49%\uff08\u6676\u5706\uff09\u548c8.84%\uff08FPGA\uff09\u3002", "conclusion": "SDE\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u91c7\u6837\u5747\u5300\u6027\u548c\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u53ef\u964d\u4f4e\u534a\u5bfc\u4f53\u6d4b\u8bd5\u6210\u672c\u3002", "keywords": "\u534a\u5bfc\u4f53\u6d4b\u8bd5\u3001\u91c7\u6837\u7b56\u7565\u3001SDE\u7b97\u6cd5\u3001\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u3001\u9884\u6d4b\u51c6\u786e\u6027"}}
{"id": "2506.03637", "pdf": "https://arxiv.org/pdf/2506.03637", "abs": "https://arxiv.org/abs/2506.03637", "authors": ["Zhuohao Yu", "Jiali Zeng", "Weizheng Gu", "Yidong Wang", "Jindong Wang", "Fandong Meng", "Jie Zhou", "Yue Zhang", "Shikun Zhang", "Wei Ye"], "title": "RewardAnything: Generalizable Principle-Following Reward Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages, 8 figures", "summary": "Reward Models, essential for guiding Large Language Model optimization, are\ntypically trained on fixed preference datasets, resulting in rigid alignment to\nsingle, implicit preference distributions. This prevents adaptation to diverse\nreal-world needs-from conciseness in one task to detailed explanations in\nanother. The standard practice of collecting task-specific preference data and\nretraining reward models is resource-intensive, often producing biased rewards,\nand limits practical application. We introduce generalizable,\nprinciple-following reward models. We propose that RMs should understand and\nadhere to dynamically provided natural language specifications of reward\nprinciples, similar to instruction-following in LLMs. To measure this\ncapability, we develop RABench, a comprehensive benchmark for RMs focusing on\ngeneralization across diverse principles. Evaluations on RABench reveal poor\ngeneralization of current RMs. As a solution, we present RewardAnything, a\nnovel RM designed and trained to explicitly follow natural language principles.\nWe achieve SotA performance with RewardAnything in traditional RM benchmark\nsimply by specifying a well-defined principle, and results on RABench show we\nexcel in adapting to novel principles without retraining. Furthermore,\nRewardAnything integrates seamlessly with existing RLHF methods and we show by\na case study on how to automatically and efficiently align LLMs with only\nnatural language principles.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u901a\u7528\u7684\u3001\u9075\u5faa\u539f\u5219\u7684\u5956\u52b1\u6a21\u578b\uff08RewardAnything\uff09\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u5956\u52b1\u6a21\u578b\u5728\u9002\u5e94\u591a\u6837\u5316\u9700\u6c42\u65f6\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u539f\u5219\u5b9e\u73b0\u9ad8\u6548\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u901a\u5e38\u57fa\u4e8e\u56fa\u5b9a\u504f\u597d\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u9700\u6c42\uff0c\u4e14\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u7684\u6536\u96c6\u548c\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51faRewardAnything\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e3a\u9075\u5faa\u52a8\u6001\u63d0\u4f9b\u7684\u81ea\u7136\u8bed\u8a00\u5956\u52b1\u539f\u5219\uff0c\u5e76\u5f00\u53d1RABench\u8bc4\u4f30\u5176\u666e\u9002\u6027\u3002", "result": "RewardAnything\u5728\u4f20\u7edf\u4efb\u52a1\u548cRABench\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u65b0\u539f\u5219\uff0c\u5e76\u80fd\u9ad8\u6548\u5bf9\u9f50LLMs\u3002", "conclusion": "RewardAnything\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u539f\u5219\u5b9e\u73b0\u4e86\u5956\u52b1\u6a21\u578b\u7684\u901a\u7528\u6027\u548c\u9ad8\u6548\u6027\uff0c\u4e3aLLM\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u5956\u52b1\u6a21\u578b,\u81ea\u7136\u8bed\u8a00\u539f\u5219,RABench,\u901a\u7528\u6027,LLM\u4f18\u5316"}}
{"id": "2506.03190", "pdf": "https://arxiv.org/pdf/2506.03190", "abs": "https://arxiv.org/abs/2506.03190", "authors": ["Jiaming Yi", "Ruirui Pan", "Jishen Yang", "Xiulong Yang"], "title": "MINT: Memory-Infused Prompt Tuning at Test-time for CLIP", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 3 figures", "summary": "Improving the generalization ability of Vision-Language Pre-trained Models\n(VLMs) under test-time data distribution shifts remains a critical challenge.\nThe existing Test-Time Adaptation (TTA) methods fall short in fully leveraging\nthe model's internal knowledge, particularly in dynamically adapting to complex\nand hierarchical visual semantic information. In this paper, we propose\nMemory-Infused Prompt Tuning (MINT), a novel framework to address this issue.\nInspired by human associative memory theory, MINT introduces a Memory Prompt\nBank (MPB), which stores learnable key-value prompt pairs that work as a memory\nof previously seen samples. During the test time, relevant prompt pairs in the\nMPB are retrieved by the hierarchical visual features of test images to\ndynamically assemble Associative Prompts. The associative prompts are then\ninjected into the image encoder for fine-grained, customized visual contextual\nguidance. MINT also utilizes learnable text prompts. MINT thus enables rapid,\nprecise VLM adaptation at test time by leveraging this MPB-acquired memory,\nwithout source data or retraining. The code is available at\nhttps://github.com/Jamieyi2004/MINT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMINT\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u8bb0\u5fc6\u63d0\u793a\u5e93\uff08MPB\uff09\u52a8\u6001\u7ec4\u88c5\u5173\u8054\u63d0\u793a\uff0c\u4ee5\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u5728\u52a8\u6001\u9002\u5e94\u590d\u6742\u5206\u5c42\u89c6\u89c9\u8bed\u4e49\u4fe1\u606f\u65f6\u672a\u80fd\u5145\u5206\u5229\u7528\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u4eba\u7c7b\u8054\u60f3\u8bb0\u5fc6\u7406\u8bba\uff0cMINT\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8bb0\u5fc6\u63d0\u793a\u5e93\uff08MPB\uff09\uff0c\u5b58\u50a8\u53ef\u5b66\u4e60\u7684\u952e\u503c\u63d0\u793a\u5bf9\uff0c\u5e76\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u5206\u5c42\u89c6\u89c9\u7279\u5f81\u68c0\u7d22\u76f8\u5173\u63d0\u793a\u5bf9\uff0c\u52a8\u6001\u7ec4\u88c5\u5173\u8054\u63d0\u793a\u5e76\u6ce8\u5165\u56fe\u50cf\u7f16\u7801\u5668\u3002", "result": "MINT\u65e0\u9700\u6e90\u6570\u636e\u6216\u91cd\u65b0\u8bad\u7ec3\uff0c\u5373\u53ef\u5b9e\u73b0\u5feb\u901f\u3001\u7cbe\u786e\u7684\u6d4b\u8bd5\u65f6\u6a21\u578b\u9002\u5e94\u3002", "conclusion": "MINT\u901a\u8fc7\u8bb0\u5fc6\u63d0\u793a\u5e93\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u7684\u9002\u5e94\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002", "keywords": "Vision-Language Pre-trained Models, Test-Time Adaptation, Memory-Infused Prompt Tuning, Associative Memory"}}
{"id": "2506.03588", "pdf": "https://arxiv.org/pdf/2506.03588", "abs": "https://arxiv.org/abs/2506.03588", "authors": ["Hiroki Shiraishi", "Hisao Ishibuchi", "Masaya Nakata"], "title": "A Class Inference Scheme With Dempster-Shafer Theory for Learning Fuzzy-Classifier Systems", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "The decision-making process significantly influences the predictions of\nmachine learning models. This is especially important in rule-based systems\nsuch as Learning Fuzzy-Classifier Systems (LFCSs) where the selection and\napplication of rules directly determine prediction accuracy and reliability.\nLFCSs combine evolutionary algorithms with supervised learning to optimize\nfuzzy classification rules, offering enhanced interpretability and robustness.\nDespite these advantages, research on improving decision-making mechanisms\n(i.e., class inference schemes) in LFCSs remains limited. Most LFCSs use\nvoting-based or single-winner-based inference schemes. These schemes rely on\nclassification performance on training data and may not perform well on unseen\ndata, risking overfitting. To address these limitations, this article\nintroduces a novel class inference scheme for LFCSs based on the\nDempster-Shafer Theory of Evidence (DS theory). The proposed scheme handles\nuncertainty well. By using the DS theory, the scheme calculates belief masses\n(i.e., measures of belief) for each specific class and the ``I don't know''\nstate from each fuzzy rule and infers a class from these belief masses. Unlike\nthe conventional schemes, the proposed scheme also considers the ``I don't\nknow'' state that reflects uncertainty, thereby improving the transparency and\nreliability of LFCSs. Applied to a variant of LFCS (i.e., Fuzzy-UCS), the\nproposed scheme demonstrates statistically significant improvements in terms of\ntest macro F1 scores across 30 real-world datasets compared to conventional\nvoting-based and single-winner-based fuzzy inference schemes. It forms smoother\ndecision boundaries, provides reliable confidence measures, and enhances the\nrobustness and generalizability of LFCSs in real-world applications. Our\nimplementation is available at https://github.com/YNU-NakataLab/jUCS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDempster-Shafer\u8bc1\u636e\u7406\u8bba\u7684LFCSs\u65b0\u578b\u7c7b\u522b\u63a8\u65ad\u65b9\u6848\uff0c\u901a\u8fc7\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u5e76\u572830\u4e2a\u73b0\u5b9e\u6570\u636e\u96c6\u4e0a\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "LFCSs\u4e2d\u7684\u51b3\u7b56\u673a\u5236\uff08\u5982\u57fa\u4e8e\u6295\u7968\u6216\u5355\u4e00\u89c4\u5219\u7684\u63a8\u65ad\u65b9\u6cd5\uff09\u53ef\u80fd\u56e0\u8fc7\u5ea6\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u800c\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u9ad8LFCSs\u7684\u7a33\u5065\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDempster-Shafer\u8bc1\u636e\u7406\u8bba\u7684\u63a8\u65ad\u65b9\u6848\uff0c\u901a\u8fc7\u8ba1\u7b97\u6bcf\u4e2a\u6a21\u7cca\u89c4\u5219\u7684\u4fe1\u5ff5\u8d28\u91cf\uff08\u5305\u62ec\"\u6211\u4e0d\u77e5\u9053\"\u72b6\u6001\uff09\u6765\u63a8\u65ad\u7c7b\u522b\u3002", "result": "\u572830\u4e2a\u73b0\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u65b0\u65b9\u6848\u5728\u5b8fF1\u5206\u6570\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u80fd\u751f\u6210\u66f4\u5e73\u6ed1\u7684\u51b3\u7b56\u8fb9\u754c\u548c\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u5ea6\u91cf\u3002", "conclusion": "\u57fa\u4e8eDS\u7406\u8bba\u7684\u65b0\u63a8\u65ad\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86LFCSs\u7684\u900f\u660e\u5ea6\u3001\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "LFCSs, Dempster-Shafer\u7406\u8bba, \u7c7b\u522b\u63a8\u65ad, \u4e0d\u786e\u5b9a\u6027\u5904\u7406, \u6a21\u7cca\u5206\u7c7b"}}
{"id": "2506.03659", "pdf": "https://arxiv.org/pdf/2506.03659", "abs": "https://arxiv.org/abs/2506.03659", "authors": ["Yinuo Wang", "Robert E. Mercer", "Frank Rudzicz", "Sudipta Singha Roy", "Pengjie Ren", "Zhumin Chen", "Xindi Wang"], "title": "Trustworthy Medical Question Answering: An Evaluation-Centric Survey", "categories": ["cs.CL"], "comment": null, "summary": "Trustworthiness in healthcare question-answering (QA) systems is important\nfor ensuring patient safety, clinical effectiveness, and user confidence. As\nlarge language models (LLMs) become increasingly integrated into medical\nsettings, the reliability of their responses directly influences clinical\ndecision-making and patient outcomes. However, achieving comprehensive\ntrustworthiness in medical QA poses significant challenges due to the inherent\ncomplexity of healthcare data, the critical nature of clinical scenarios, and\nthe multifaceted dimensions of trustworthy AI. In this survey, we\nsystematically examine six key dimensions of trustworthiness in medical QA,\ni.e., Factuality, Robustness, Fairness, Safety, Explainability, and\nCalibration. We review how each dimension is evaluated in existing LLM-based\nmedical QA systems. We compile and compare major benchmarks designed to assess\nthese dimensions and analyze evaluation-guided techniques that drive model\nimprovements, such as retrieval-augmented grounding, adversarial fine-tuning,\nand safety alignment. Finally, we identify open challenges-such as scalable\nexpert evaluation, integrated multi-dimensional metrics, and real-world\ndeployment studies-and propose future research directions to advance the safe,\nreliable, and transparent deployment of LLM-powered medical QA.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8c03\u67e5\u4e86\u533b\u7597QA\u7cfb\u7edf\u4e2d\u4fe1\u4efb\u7684\u516d\u4e2a\u5173\u952e\u7ef4\u5ea6\uff08\u4e8b\u5b9e\u6027\u3001\u9c81\u68d2\u6027\u3001\u516c\u5e73\u6027\u3001\u5b89\u5168\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6821\u51c6\u6027\uff09\uff0c\u5e76\u5206\u6790\u4e86\u73b0\u6709LLM\u5728\u8fd9\u4e9b\u7ef4\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u6280\u672f\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u968f\u7740LLM\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u589e\u52a0\uff0c\u786e\u4fdd\u5176\u54cd\u5e94\u7684\u53ef\u9760\u6027\u5bf9\u4e34\u5e8a\u51b3\u7b56\u548c\u60a3\u8005\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u533b\u7597\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u4e34\u5e8a\u573a\u666f\u7684\u4e25\u8083\u6027\u4f7f\u5f97\u5b9e\u73b0\u5168\u9762\u4fe1\u4efb\u7684QA\u7cfb\u7edf\u5145\u6ee1\u6311\u6218\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u56de\u987e\u548c\u6bd4\u8f83\u73b0\u6709LLM\u533b\u7597QA\u7cfb\u7edf\u5728\u516d\u4e2a\u4fe1\u4efb\u7ef4\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u6280\u672f\u6539\u8fdb\u63aa\u65bd\uff08\u5982\u68c0\u7d22\u589e\u5f3a\u3001\u5bf9\u6297\u5fae\u8c03\u548c\u5b89\u5168\u5bf9\u9f50\uff09\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u8bc4\u4f30\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u4e13\u5bb6\u8bc4\u4f30\u7684\u6269\u5c55\u3001\u591a\u7ef4\u5ea6\u7efc\u5408\u6307\u6807\u548c\u5b9e\u9645\u90e8\u7f72\u7814\u7a76\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u6280\u672f\u6539\u8fdb\uff0c\u4ee5\u5b9e\u73b0LLM\u5728\u533b\u7597QA\u4e2d\u7684\u5b89\u5168\u3001\u53ef\u9760\u548c\u900f\u660e\u90e8\u7f72\u3002", "keywords": "\u533b\u7597QA\u3001\u4fe1\u4efb\u7ef4\u5ea6\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u8bc4\u4f30\u57fa\u51c6\u3001\u6280\u672f\u6539\u8fdb"}}
{"id": "2506.03191", "pdf": "https://arxiv.org/pdf/2506.03191", "abs": "https://arxiv.org/abs/2506.03191", "authors": ["Muhammad Islam", "Tao Huang", "Euijoon Ahn", "Usman Naseem"], "title": "Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents an in-depth survey on the use of multimodal Generative\nArtificial Intelligence (GenAI) and autoregressive Large Language Models (LLMs)\nfor human motion understanding and generation, offering insights into emerging\nmethods, architectures, and their potential to advance realistic and versatile\nmotion synthesis. Focusing exclusively on text and motion modalities, this\nresearch investigates how textual descriptions can guide the generation of\ncomplex, human-like motion sequences. The paper explores various generative\napproaches, including autoregressive models, diffusion models, Generative\nAdversarial Networks (GANs), Variational Autoencoders (VAEs), and\ntransformer-based models, by analyzing their strengths and limitations in terms\nof motion quality, computational efficiency, and adaptability. It highlights\nrecent advances in text-conditioned motion generation, where textual inputs are\nused to control and refine motion outputs with greater precision. The\nintegration of LLMs further enhances these models by enabling semantic\nalignment between instructions and motion, improving coherence and contextual\nrelevance. This systematic survey underscores the transformative potential of\ntext-to-motion GenAI and LLM architectures in applications such as healthcare,\nhumanoids, gaming, animation, and assistive technologies, while addressing\nongoing challenges in generating efficient and realistic human motion.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u591a\u6a21\u6001\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u548c\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4eba\u4f53\u8fd0\u52a8\u7406\u89e3\u4e0e\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u6587\u672c\u4e0e\u8fd0\u52a8\u6a21\u6001\uff0c\u63a2\u8ba8\u4e86\u6587\u672c\u63cf\u8ff0\u5982\u4f55\u6307\u5bfc\u751f\u6210\u590d\u6742\u4e14\u903c\u771f\u7684\u8fd0\u52a8\u5e8f\u5217\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\u7cbe\u786e\u63a7\u5236\u8fd0\u52a8\u751f\u6210\uff0c\u5e76\u6574\u5408LLMs\u4ee5\u63d0\u9ad8\u8bed\u4e49\u5bf9\u9f50\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002", "method": "\u5206\u6790\u4e86\u591a\u79cd\u751f\u6210\u65b9\u6cd5\uff08\u5982\u81ea\u56de\u5f52\u6a21\u578b\u3001\u6269\u6563\u6a21\u578b\u3001GANs\u3001VAEs\u548ctransformer\u6a21\u578b\uff09\uff0c\u8bc4\u4f30\u5176\u5728\u8fd0\u52a8\u8d28\u91cf\u3001\u8ba1\u7b97\u6548\u7387\u548c\u9002\u5e94\u6027\u65b9\u9762\u7684\u4f18\u52a3\u3002", "result": "\u7a81\u51fa\u4e86\u6587\u672c\u6761\u4ef6\u8fd0\u52a8\u751f\u6210\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5c55\u793a\u4e86LLMs\u5728\u63d0\u5347\u8bed\u4e49\u5bf9\u9f50\u548c\u8fd0\u52a8\u8fde\u8d2f\u6027\u65b9\u9762\u7684\u4f5c\u7528\u3002", "conclusion": "\u6587\u672c\u5230\u8fd0\u52a8\u7684GenAI\u548cLLM\u67b6\u6784\u5728\u533b\u7597\u3001\u4eba\u5f62\u673a\u5668\u4eba\u3001\u6e38\u620f\u3001\u52a8\u753b\u7b49\u9886\u57df\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u9ad8\u6548\u6027\u548c\u903c\u771f\u6027\u7684\u6311\u6218\u3002", "keywords": "\u591a\u6a21\u6001GenAI\u3001\u81ea\u56de\u5f52LLMs\u3001\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u3001\u6587\u672c\u6761\u4ef6\u63a7\u5236\u3001\u8bed\u4e49\u5bf9\u9f50"}}
{"id": "2506.03590", "pdf": "https://arxiv.org/pdf/2506.03590", "abs": "https://arxiv.org/abs/2506.03590", "authors": ["Minh Luu", "Surya Jasper", "Khoi Le", "Evan Pan", "Michael Quinn", "Aakash Tyagi", "Jiang Hu"], "title": "VCDiag: Classifying Erroneous Waveforms for Failure Triage Acceleration", "categories": ["cs.LG"], "comment": null, "summary": "Failure triage in design functional verification is critical but\ntime-intensive, relying on manual specification reviews, log inspections, and\nwaveform analyses. While machine learning (ML) has improved areas like stimulus\ngeneration and coverage closure, its application to RTL-level simulation\nfailure triage, particularly for large designs, remains limited. VCDiag offers\nan efficient, adaptable approach using VCD data to classify failing waveforms\nand pinpoint likely failure locations. In the largest experiment, VCDiag\nachieves over 94% accuracy in identifying the top three most likely modules.\nThe framework introduces a novel signal selection and statistical compression\napproach, achieving over 120x reduction in raw data size while preserving\nfeatures essential for classification. It can also be integrated into diverse\nVerilog/SystemVerilog designs and testbenches.", "AI": {"tldr": "VCDiag\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVCD\u6570\u636e\u7684\u9ad8\u6548\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u7c7b\u6545\u969c\u6ce2\u5f62\u5e76\u5b9a\u4f4d\u6545\u969c\u4f4d\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u4e86RTL\u7ea7\u4eff\u771f\u5931\u8d25\u5206\u7c7b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u8bbe\u8ba1\u529f\u80fd\u9a8c\u8bc1\u4e2d\u7684\u6545\u969c\u5206\u7c7b\u901a\u5e38\u4f9d\u8d56\u4eba\u5de5\u4e14\u8017\u65f6\uff0c\u800c\u673a\u5668\u5b66\u4e60\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u4ecd\u6709\u9650\u3002", "method": "VCDiag\u91c7\u7528\u57fa\u4e8eVCD\u6570\u636e\u7684\u4fe1\u53f7\u9009\u62e9\u548c\u7edf\u8ba1\u538b\u7f29\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86120\u500d\u4ee5\u4e0a\u7684\u539f\u59cb\u6570\u636e\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5206\u7c7b\u6240\u9700\u7684\u5173\u952e\u7279\u5f81\u3002", "result": "\u5728\u6700\u5927\u89c4\u6a21\u7684\u5b9e\u9a8c\u4e2d\uff0cVCDiag\u5728\u8bc6\u522b\u6700\u53ef\u80fd\u6545\u969c\u6a21\u5757\u7684\u51c6\u786e\u6027\u4e0a\u8fbe\u523094%\u4ee5\u4e0a\u3002", "conclusion": "VCDiag\u7684\u6846\u67b6\u4e0d\u4ec5\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\uff0c\u53ef\u96c6\u6210\u5230\u591a\u79cdVerilog/SystemVerilog\u8bbe\u8ba1\u548c\u6d4b\u8bd5\u5e73\u53f0\u4e2d\u3002", "keywords": "\u529f\u80fd\u9a8c\u8bc1, \u673a\u5668\u5b66\u4e60, VCDiag, RTL\u4eff\u771f, \u6545\u969c\u5206\u7c7b"}}
{"id": "2506.03665", "pdf": "https://arxiv.org/pdf/2506.03665", "abs": "https://arxiv.org/abs/2506.03665", "authors": ["Hern\u00e1n Maina", "Guido Ivetta", "Mateo Lione Stuto", "Julian Martin Eisenschlos", "Jorge S\u00e1nchez", "Luciana Benotti"], "title": "ROSA: Addressing text understanding challenges in photographs via ROtated SAmpling", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Visually impaired people could benefit from Visual Question Answering (VQA)\nsystems to interpret text in their surroundings. However, current models often\nstruggle with recognizing text in the photos taken by this population. Through\nin-depth interviews with visually impaired individuals, we identified common\nframing conventions that frequently result in misaligned text. Existing VQA\nbenchmarks primarily feature well-oriented text captured by sighted users,\nunder-representing these challenges. To address this gap, we introduce ROtated\nSAmpling (ROSA), a decoding strategy that enhances VQA performance in text-rich\nimages with incorrectly oriented text. ROSA outperforms Greedy decoding by 11.7\nabsolute points in the best-performing model.", "AI": {"tldr": "ROSA\u89e3\u7801\u7b56\u7565\u901a\u8fc7\u589e\u5f3aVQA\u7cfb\u7edf\u5728\u6587\u672c\u65b9\u5411\u4e0d\u6b63\u786e\u7684\u56fe\u50cf\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u89c6\u969c\u4eba\u58eb\u63d0\u4f9b\u66f4\u597d\u7684\u6587\u672c\u8bc6\u522b\u652f\u6301\u3002", "motivation": "\u5f53\u524dVQA\u7cfb\u7edf\u5728\u5904\u7406\u89c6\u969c\u4eba\u58eb\u62cd\u6444\u7684\u6587\u672c\u56fe\u50cf\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u6587\u672c\u65b9\u5411\u4e0d\u6b63\u786e\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51faROtated SAmpling (ROSA)\u89e3\u7801\u7b56\u7565\uff0c\u7528\u4e8e\u63d0\u5347VQA\u7cfb\u7edf\u5728\u6587\u672c\u65b9\u5411\u4e0d\u6b63\u786e\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "result": "ROSA\u5728\u8868\u73b0\u6700\u4f73\u6a21\u578b\u4e2d\u6bd4\u8d2a\u5a6a\u89e3\u7801\u7b56\u7565\u63d0\u5347\u4e8611.7\u4e2a\u7edd\u5bf9\u767e\u5206\u70b9\u3002", "conclusion": "ROSA\u6709\u6548\u89e3\u51b3\u4e86VQA\u7cfb\u7edf\u5728\u5904\u7406\u89c6\u969c\u4eba\u58eb\u62cd\u6444\u7684\u6587\u672c\u56fe\u50cf\u65f6\u7684\u5c40\u9650\u6027\u3002", "keywords": "\u89c6\u89c9\u95ee\u7b54\u7cfb\u7edf, \u89c6\u969c\u4eba\u58eb, \u6587\u672c\u8bc6\u522b, ROSA\u89e3\u7801\u7b56\u7565"}}
{"id": "2506.03192", "pdf": "https://arxiv.org/pdf/2506.03192", "abs": "https://arxiv.org/abs/2506.03192", "authors": ["Basudha Pal", "Rama Chellappa", "Muhammad Umair"], "title": "Encoding of Demographic and Anatomical Information in Chest X-Ray-based Severe Left Ventricular Hypertrophy Classifiers", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "While echocardiography and MRI are clinical standards for evaluating cardiac\nstructure, their use is limited by cost and accessibility.We introduce a direct\nclassification framework that predicts severe left ventricular hypertrophy from\nchest X-rays, without relying on anatomical measurements or demographic inputs.\nOur approach achieves high AUROC and AUPRC, and employs Mutual Information\nNeural Estimation to quantify feature expressivity. This reveals clinically\nmeaningful attribute encoding and supports transparent model interpretation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80f8\u7247\u76f4\u63a5\u9884\u6d4b\u4e25\u91cd\u5de6\u5fc3\u5ba4\u80a5\u539a\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u65e0\u9700\u4f9d\u8d56\u89e3\u5256\u6d4b\u91cf\u6216\u4eba\u53e3\u7edf\u8ba1\u5b66\u8f93\u5165\uff0c\u6548\u679c\u4f18\u5f02\u4e14\u5177\u5907\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u8d85\u58f0\u5fc3\u52a8\u56fe\u548cMRI\u867d\u7136\u80fd\u8bc4\u4f30\u5fc3\u810f\u7ed3\u6784\uff0c\u4f46\u6210\u672c\u9ad8\u4e14\u666e\u53ca\u6027\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7ecf\u6d4e\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u76f4\u63a5\u5206\u7c7b\u6846\u67b6\uff0c\u5229\u7528Mutual Information Neural Estimation\u91cf\u5316\u7279\u5f81\u8868\u8fbe\u6027\u3002", "result": "\u6a21\u578b\u5728AUROC\u548cAUPRC\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u63ed\u793a\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u5c5e\u6027\u7f16\u7801\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5de6\u5fc3\u5ba4\u80a5\u539a\u7684\u7b5b\u67e5\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u53ef\u89e3\u91ca\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5de6\u5fc3\u5ba4\u80a5\u539a\u3001\u80f8\u7247\u3001\u5206\u7c7b\u6846\u67b6\u3001Mutual Information Neural Estimation\u3001\u53ef\u89e3\u91ca\u6027"}}
{"id": "2506.03595", "pdf": "https://arxiv.org/pdf/2506.03595", "abs": "https://arxiv.org/abs/2506.03595", "authors": ["Runa Eschenhagen", "Aaron Defazio", "Tsung-Hsien Lee", "Richard E. Turner", "Hao-Jun Michael Shi"], "title": "Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its Preconditioner", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The recent success of Shampoo in the AlgoPerf contest has sparked renewed\ninterest in Kronecker-factorization-based optimization algorithms for training\nneural networks. Despite its success, Shampoo relies heavily on several\nheuristics such as learning rate grafting and stale preconditioning to achieve\nperformance at-scale. These heuristics increase algorithmic complexity,\nnecessitate further hyperparameter tuning, and lack theoretical justification.\nThis paper investigates these heuristics from the angle of Frobenius norm\napproximation to full-matrix Adam and decouples the preconditioner's\neigenvalues and eigenbasis updates. We show that grafting from Adam mitigates\nthe staleness and mis-scaling of the preconditioner's eigenvalues and how\ncorrecting the eigenvalues directly can eliminate the need for learning rate\ngrafting. To manage the error induced by infrequent eigenbasis computations, we\npropose an adaptive criterion for determining the eigenbasis computation\nfrequency motivated by terminating a warm-started QR algorithm. This criterion\ndecouples the update frequency of different preconditioner matrices and enables\nus to investigate the impact of approximation error on convergence. These\npractical techniques offer a principled angle towards removing Shampoo's\nheuristics and developing improved Kronecker-factorization-based training\nalgorithms.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Shampoo\u7b97\u6cd5\u4e2d\u7684\u542f\u53d1\u6027\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u901a\u8fc7Frobenius\u8303\u6570\u8fd1\u4f3c\u548c\u5206\u79bb\u9884\u6761\u4ef6\u5b50\u7684\u7279\u5f81\u503c\u4e0e\u7279\u5f81\u57fa\u66f4\u65b0\uff0c\u4ee5\u51cf\u5c11\u590d\u6742\u6027\u548c\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7684\u7279\u5f81\u57fa\u8ba1\u7b97\u9891\u7387\u6807\u51c6\u3002", "motivation": "Shampoo\u7b97\u6cd5\u867d\u7136\u5728AlgoPerf\u7ade\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u4f9d\u8d56\u7684\u542f\u53d1\u6027\u65b9\u6cd5\u589e\u52a0\u4e86\u7b97\u6cd5\u590d\u6742\u6027\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u96be\u5ea6\uff0c\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u901a\u8fc7Frobenius\u8303\u6570\u8fd1\u4f3cAdam\u7684\u5168\u77e9\u9635\uff0c\u5206\u79bb\u9884\u6761\u4ef6\u5b50\u7684\u7279\u5f81\u503c\u548c\u7279\u5f81\u57fa\u66f4\u65b0\uff0c\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u7279\u5f81\u57fa\u8ba1\u7b97\u9891\u7387\u7684\u6807\u51c6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4eceAdam\u5ac1\u63a5\u53ef\u4ee5\u7f13\u89e3\u9884\u6761\u4ef6\u5b50\u7279\u5f81\u503c\u7684\u9648\u65e7\u6027\u548c\u7f29\u653e\u4e0d\u5f53\u95ee\u9898\uff0c\u76f4\u63a5\u4fee\u6b63\u7279\u5f81\u503c\u53ef\u907f\u514d\u5b66\u4e60\u7387\u5ac1\u63a5\u7684\u9700\u6c42\u3002\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u6807\u51c6\u80fd\u6709\u6548\u7ba1\u7406\u7279\u5f81\u57fa\u8ba1\u7b97\u9891\u7387\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u53bb\u9664Shampoo\u7684\u542f\u53d1\u6027\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u57fa\u4e8eKronecker\u5206\u89e3\u7684\u8bad\u7ec3\u7b97\u6cd5\u3002", "keywords": "Shampoo, Kronecker-factorization, optimization algorithms, neural networks, Adam, Frobenius norm"}}
{"id": "2506.03681", "pdf": "https://arxiv.org/pdf/2506.03681", "abs": "https://arxiv.org/abs/2506.03681", "authors": ["Pradeep Rangappa", "Andres Carofilis", "Jeena Prakash", "Shashi Kumar", "Sergio Burdisso", "Srikanth Madikeri", "Esau Villatoro-Tello", "Bidisha Sharma", "Petr Motlicek", "Kadri Hacioglu", "Shankar Venkatesan", "Saurabh Vyas", "Andreas Stolcke"], "title": "Efficient Data Selection for Domain Adaptation of ASR Using Pseudo-Labels and Multi-Stage Filtering", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025, Netherlands", "summary": "Fine-tuning pretrained ASR models for specific domains is challenging for\nsmall organizations with limited labeled data and computational resources.\nHere, we explore different data selection pipelines and propose a robust\napproach that improves ASR adaptation by filtering pseudo-labels generated\nusing Whisper (encoder-decoder) and Zipformer (transducer) models. Our approach\nintegrates multiple selection strategies -- including word error rate (WER)\nprediction, named entity recognition (NER), and character error rate (CER)\nanalysis -- to extract high-quality training segments. We evaluate our method\non Whisper and Zipformer using a 7500-hour baseline, comparing it to a\nCER-based approach relying on hypotheses from three ASR systems. Fine-tuning on\n7500 hours of pseudo-labeled call center data achieves 12.3% WER, while our\nfiltering reduces the dataset to 100 hours (1.4%) with similar performance; a\nsimilar trend is observed on Fisher English.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7b5b\u9009\u4f2a\u6807\u7b7e\u7684\u9c81\u68d2\u65b9\u6cd5\uff0c\u6539\u8fdbASR\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u7684\u9002\u914d\uff0c\u51cf\u5c11\u6570\u636e\u9700\u6c42\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5c0f\u578b\u7ec4\u7ec7\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5bf9\u9884\u8bad\u7ec3ASR\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408Word Error Rate (WER)\u9884\u6d4b\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b(NER)\u548cCharacter Error Rate (CER)\u5206\u6790\uff0c\u7b5b\u9009\u9ad8\u8d28\u91cf\u8bad\u7ec3\u7247\u6bb5\u3002", "result": "\u57287500\u5c0f\u65f6\u7684\u4f2a\u6807\u6ce8\u6570\u636e\u4e0a\u5fae\u8c03\u5b9e\u73b012.3% WER\uff0c\u7b5b\u9009\u81f3100\u5c0f\u65f6\uff081.4%\uff09\u6570\u636e\u96c6\u6027\u80fd\u76f8\u8fd1\uff1bFisher English\u5b9e\u9a8c\u7ed3\u679c\u7c7b\u4f3c\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301ASR\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u573a\u666f\u3002", "keywords": "ASR\u5fae\u8c03\u3001\u4f2a\u6807\u7b7e\u7b5b\u9009\u3001\u6570\u636e\u9009\u62e9\u3001WER\u3001NER\u3001CER\u3001Whisper\u3001Zipformer"}}
{"id": "2506.03194", "pdf": "https://arxiv.org/pdf/2506.03194", "abs": "https://arxiv.org/abs/2506.03194", "authors": ["Rynaa Grover", "Jayant Sravan Tamarapalli", "Sahiti Yerramilli", "Nilay Pande"], "title": "HueManity: Probing Fine-Grained Visual Perception in MLLMs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) excel at high-level visual\nreasoning, but their performance on nuanced perceptual tasks remains\nsurprisingly limited. We present HueManity, a benchmark designed to assess\nvisual perception in MLLMs. The dataset comprises 83,850 images featuring\ntwo-character alphanumeric strings embedded in Ishihara test style dot\npatterns, challenging models on precise pattern recognition. Our evaluation of\nnine state-of-the-art MLLMs on HueManity demonstrates a significant performance\ndeficit compared to human and traditional computer vision baselines. The\nbest-performing MLLM achieved a 33.6% accuracy on the numeric `easy' task and a\nstriking 3% on the alphanumeric `hard' task. In contrast, human participants\nachieved near-perfect scores (100% and 95.6%), and a fine-tuned ResNet50 model\nreached accuracies of 96.5% and 94.5%. These results highlight a critical gap\nin the visual capabilities of current MLLMs. Our analysis further explores\npotential architectural and training-paradigm factors contributing to this\nperceptual gap in MLLMs. We open-source HueManity dataset and code to foster\nfurther research in improving perceptual robustness of MLLMs.", "AI": {"tldr": "HueManity benchmark\u8bc4\u4f30MLLM\u5728\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u7ec6\u5fae\u6a21\u5f0f\u8bc6\u522b\u4e0a\u663e\u8457\u843d\u540e\u4e8e\u4eba\u53ca\u4f20\u7edf\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1MLLM\u5728\u9ad8\u7ea7\u89c6\u89c9\u63a8\u7406\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7ec6\u5fae\u611f\u77e5\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u521b\u5efaHueManity\u6570\u636e\u96c6\uff0c\u5305\u542b83,850\u5f20Ishihara\u6d4b\u8bd5\u98ce\u683c\u56fe\u50cf\uff0c\u8bc4\u4f309\u79cd\u9886\u5148MLLM\u7684\u7cbe\u786e\u6a21\u5f0f\u8bc6\u522b\u80fd\u529b\u3002", "result": "MLLM\u5728\u6570\u5b57\u201c\u7b80\u5355\u201d\u4efb\u52a1\u4e0a\u6700\u9ad8\u51c6\u786e\u7387\u4e3a33.6%\uff0c\u5b57\u6bcd\u6570\u5b57\u201c\u56f0\u96be\u201d\u4efb\u52a1\u4ec53%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\uff08100%\u300195.6%\uff09\u548cResNet50\uff0896.5%\u300194.5%\uff09\u3002", "conclusion": "\u5f53\u524dMLLM\u5728\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u4e0a\u5b58\u5728\u91cd\u5927\u7f3a\u9677\uff0c\u9700\u6539\u8fdb\u67b6\u6784\u548c\u8bad\u7ec3\u8303\u5f0f\u3002", "keywords": "MLLM,\u89c6\u89c9\u611f\u77e5,\u57fa\u51c6\u6d4b\u8bd5,Ishihara\u6d4b\u8bd5,HueManity\u6570\u636e\u96c6"}}
{"id": "2506.03602", "pdf": "https://arxiv.org/pdf/2506.03602", "abs": "https://arxiv.org/abs/2506.03602", "authors": ["Hiroki Shiraishi", "Yohei Hayamizu", "Tomonori Hashiyama", "Keiki Takadama", "Hisao Ishibuchi", "Masaya Nakata"], "title": "Adapting Rule Representation With Four-Parameter Beta Distribution for Learning Classifier Systems", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "Rule representations significantly influence the search capabilities and\ndecision boundaries within the search space of Learning Classifier Systems\n(LCSs), a family of rule-based machine learning systems that evolve\ninterpretable models through evolutionary processes. However, it is very\ndifficult to choose an appropriate rule representation for each problem.\nAdditionally, some problems benefit from using different representations for\ndifferent subspaces within the input space. Thus, an adaptive mechanism is\nneeded to choose an appropriate rule representation for each rule in LCSs. This\narticle introduces a flexible rule representation using a four-parameter beta\ndistribution and integrates it into a fuzzy-style LCS. The four-parameter beta\ndistribution can form various function shapes, and this flexibility enables our\nLCS to automatically select appropriate representations for different\nsubspaces. Our rule representation can represent crisp/fuzzy decision\nboundaries in various boundary shapes, such as rectangles and bells, by\ncontrolling four parameters, compared to the standard representations such as\ntrapezoidal ones. Leveraging this flexibility, our LCS is designed to adapt the\nappropriate rule representation for each subspace. Moreover, our LCS\nincorporates a generalization bias favoring crisp rules where feasible,\nenhancing model interpretability without compromising accuracy. Experimental\nresults on real-world classification tasks show that our LCS achieves\nsignificantly superior test accuracy and produces more compact rule sets. Our\nimplementation is available at https://github.com/YNU-NakataLab/Beta4-UCS. An\nextended abstract related to this work is available at\nhttps://doi.org/10.36227/techrxiv.174900805.59801248/v1.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56db\u53c2\u6570Beta\u5206\u5e03\u7684\u81ea\u9002\u5e94\u89c4\u5219\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u5b66\u4e60\u5206\u7c7b\u5668\u7cfb\u7edf\uff08LCSs\uff09\u7684\u641c\u7d22\u80fd\u529b\u548c\u51b3\u7b56\u8fb9\u754c\u9009\u62e9\u3002", "motivation": "\u73b0\u6709LCSs\u4e2d\u89c4\u5219\u8868\u793a\u7684\u9009\u62e9\u5bf9\u95ee\u9898\u6027\u80fd\u5f71\u54cd\u5f88\u5927\uff0c\u4f46\u5982\u4f55\u4e3a\u4e0d\u540c\u5b50\u7a7a\u95f4\u9009\u62e9\u5408\u9002\u7684\u89c4\u5219\u8868\u793a\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u9002\u5e94\u673a\u5236\u3002", "method": "\u91c7\u7528\u56db\u53c2\u6570Beta\u5206\u5e03\u4f5c\u4e3a\u7075\u6d3b\u7684\u89c4\u5219\u8868\u793a\uff0c\u96c6\u6210\u5230\u6a21\u7cca\u98ce\u683c\u7684LCS\u4e2d\uff0c\u80fd\u591f\u81ea\u52a8\u4e3a\u4e0d\u540c\u7684\u5b50\u7a7a\u95f4\u9009\u62e9\u5408\u9002\u8868\u793a\uff0c\u5e76\u652f\u6301\u591a\u79cd\u8fb9\u754c\u5f62\u72b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u7cbe\u5ea6\uff0c\u5e76\u751f\u6210\u66f4\u7d27\u51d1\u7684\u89c4\u5219\u96c6\u3002", "conclusion": "\u56db\u53c2\u6570Beta\u5206\u5e03\u7684\u5f15\u5165\u4f7fLCS\u80fd\u591f\u81ea\u9002\u5e94\u9009\u62e9\u89c4\u5219\u8868\u793a\uff0c\u540c\u65f6\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "keywords": "\u5b66\u4e60\u5206\u7c7b\u5668\u7cfb\u7edf, \u89c4\u5219\u8868\u793a, Beta\u5206\u5e03, \u6a21\u7cca\u7cfb\u7edf, \u81ea\u9002\u5e94\u673a\u5236"}}
{"id": "2506.03690", "pdf": "https://arxiv.org/pdf/2506.03690", "abs": "https://arxiv.org/abs/2506.03690", "authors": ["Jie Sun", "Junkang Wu", "Jiancan Wu", "Zhibo Zhu", "Xingyu Lu", "Jun Zhou", "Lintao Ma", "Xiang Wang"], "title": "Robust Preference Optimization via Dynamic Target Margins", "categories": ["cs.CL"], "comment": "18 pages, 6 figures, accepted to The 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL2025)", "summary": "The alignment of Large Language Models (LLMs) is crucial for ensuring their\nsafety and reliability in practical applications. Direct Preference\nOptimization (DPO) has emerged as an efficient method that directly optimizes\nmodels using preference pairs, significantly reducing resource demands.\nHowever, the effectiveness of DPO heavily depends on the data quality, which is\nfrequently compromised by noise. In this work, we propose $\\gamma$-PO, a\ndynamic target margin preference optimization algorithm that adjust reward\nmargins at the pairwise level. By introducing instance-specific margin\ncalibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those\ndemonstrating higher reward margins) while suppressing potential noise from\nambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible\nwith variants of DPO that rely on reward margin between preference pairs.\nAcross benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an\naverage 4.4\\% improvement over other baselines, setting new benchmarks for\nstate-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code\nchanges and has a negligible impact on training efficiency, making it a robust\nsolution for enhancing LLMs alignment. Our codes are available at\n\\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.", "AI": {"tldr": "\u63d0\u51fa\u03b3-PO\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5956\u52b1\u8fb9\u754c\u4f18\u5316\u504f\u597d\u5bf9\uff0c\u63d0\u5347LLM\u5bf9\u9f50\u6548\u679c\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf4.4%\u3002", "motivation": "\u73b0\u6709DPO\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u8d28\u91cf\uff0c\u4f46\u6570\u636e\u566a\u58f0\u5f71\u54cd\u6548\u679c\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u5f15\u5165\u03b3-PO\uff0c\u52a8\u6001\u6821\u51c6\u5956\u52b1\u8fb9\u754c\uff0c\u4f18\u5148\u9ad8\u7f6e\u4fe1\u5ea6\u5bf9\uff0c\u6291\u5236\u566a\u58f0\u3002", "result": "\u5728AlpacaEval2\u548cArena-Hard\u4e0a\u5e73\u5747\u63d0\u53474.4%\uff0c\u4e14\u517c\u5bb9\u73b0\u6709DPO\u3002", "conclusion": "\u03b3-PO\u9ad8\u6548\u7a33\u5065\uff0c\u63d0\u5347LLM\u5bf9\u9f50\u6027\u80fd\uff0c\u4ee3\u7801\u6539\u52a8\u5c0f\u3002", "keywords": "LLM\u5bf9\u9f50\uff0c\u504f\u597d\u4f18\u5316\uff0c\u566a\u58f0\u6291\u5236\uff0c\u5956\u52b1\u6821\u51c6"}}
{"id": "2506.03195", "pdf": "https://arxiv.org/pdf/2506.03195", "abs": "https://arxiv.org/abs/2506.03195", "authors": ["Yunqi Hong", "Sohyun An", "Andrew Bai", "Neil Y. C. Lin", "Cho-Jui Hsieh"], "title": "Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite Multimodal Large Language Models (MLLMs) showing promising results on\ngeneral zero-shot image classification tasks, fine-grained image classification\nremains challenging. It demands precise attention to subtle visual details to\ndistinguish between visually similar subcategories--details that MLLMs may\neasily overlook without explicit guidance. To address this, we introduce\nAutoSEP, an iterative self-supervised prompt learning framework designed to\nenhance MLLM fine-grained classification capabilities in a fully unsupervised\nmanner. Our core idea is to leverage unlabeled data to learn a description\nprompt that guides MLLMs in identifying crucial discriminative features within\nan image, and boosts classification accuracy. We developed an automatic\nself-enhancing prompt learning framework called AutoSEP to iteratively improve\nthe description prompt using unlabeled data, based on instance-level\nclassification scoring function. AutoSEP only requires black-box access to\nMLLMs, eliminating the need for any training or fine-tuning. We evaluate our\napproach on multiple fine-grained classification datasets. It consistently\noutperforms other unsupervised baselines, demonstrating the effectiveness of\nour self-supervised optimization framework. Notably, AutoSEP on average\nimproves 13 percent over standard zero-shot classification and 5 percent over\nthe best-performing baselines. Code is available at:\nhttps://github.com/yq-hong/AutoSEP", "AI": {"tldr": "AutoSEP\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5b8c\u5168\u65e0\u9700\u76d1\u7763\u6570\u636e\u3002", "motivation": "\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u9700\u8981\u5173\u6ce8\u7ec6\u5fae\u7684\u89c6\u89c9\u5dee\u5f02\uff0c\u800c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5bb9\u6613\u5ffd\u7565\u8fd9\u4e9b\u7ec6\u8282\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u81ea\u52a8\u5b66\u4e60\u63cf\u8ff0\u6027\u63d0\u793a\u4ee5\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u3002", "method": "AutoSEP\u901a\u8fc7\u81ea\u76d1\u7763\u65b9\u5f0f\uff0c\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u8fed\u4ee3\u4f18\u5316\u63cf\u8ff0\u6027\u63d0\u793a\uff0c\u5e2e\u52a9MLLMs\u8bc6\u522b\u56fe\u50cf\u4e2d\u7684\u5173\u952e\u5224\u522b\u7279\u5f81\uff0c\u4e14\u4ec5\u9700\u9ed1\u76d2\u8bbf\u95eeMLLMs\u3002", "result": "\u5728\u591a\u4e2a\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\uff0cAutoSEP\u6bd4\u96f6\u6837\u672c\u5206\u7c7b\u548c\u5176\u4ed6\u65e0\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\uff0c\u5e73\u5747\u63d0\u534713%\u548c5%\u3002", "conclusion": "AutoSEP\u6709\u6548\u63d0\u5347\u4e86MLLMs\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u8bad\u7ec3\u6216\u5fae\u8c03\u3002", "keywords": "\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b, \u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, \u81ea\u76d1\u7763\u5b66\u4e60, \u63d0\u793a\u5b66\u4e60, AutoSEP"}}
{"id": "2506.03618", "pdf": "https://arxiv.org/pdf/2506.03618", "abs": "https://arxiv.org/abs/2506.03618", "authors": ["Jiayi Wan", "Xiang Zhu", "Fanzhen Liu", "Wei Fan", "Xiaolong Xu"], "title": "GCFL: A Gradient Correction-based Federated Learning Framework for Privacy-preserving CPSS", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning, as a distributed architecture, shows great promise for\napplications in Cyber-Physical-Social Systems (CPSS). In order to mitigate the\nprivacy risks inherent in CPSS, the integration of differential privacy with\nfederated learning has attracted considerable attention. Existing research\nmainly focuses on dynamically adjusting the noise added or discarding certain\ngradients to mitigate the noise introduced by differential privacy. However,\nthese approaches fail to remove the noise that hinders convergence and correct\nthe gradients affected by the noise, which significantly reduces the accuracy\nof model classification. To overcome these challenges, this paper proposes a\nnovel framework for differentially private federated learning that balances\nrigorous privacy guarantees with accuracy by introducing a server-side gradient\ncorrection mechanism. Specifically, after clients perform gradient clipping and\nnoise perturbation, our framework detects deviations in the noisy local\ngradients and employs a projection mechanism to correct them, mitigating the\nnegative impact of noise. Simultaneously, gradient projection promotes the\nalignment of gradients from different clients and guides the model towards\nconvergence to a global optimum. We evaluate our framework on several benchmark\ndatasets, and the experimental results demonstrate that it achieves\nstate-of-the-art performance under the same privacy budget.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u670d\u52a1\u5668\u7aef\u68af\u5ea6\u6821\u6b63\u673a\u5236\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u4e2d\u5dee\u5206\u9690\u79c1\u566a\u58f0\u5f71\u54cd\u6a21\u578b\u6536\u655b\u548c\u5206\u7c7b\u51c6\u786e\u6027\u7684\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u80fd\u6821\u6b63\u53d7\u566a\u58f0\u5f71\u54cd\u7684\u68af\u5ea6\u7684\u65b9\u6cd5\u3002", "method": "\u6846\u67b6\u5728\u5ba2\u6237\u7aef\u8fdb\u884c\u68af\u5ea6\u88c1\u526a\u548c\u566a\u58f0\u6270\u52a8\u540e\uff0c\u901a\u8fc7\u68c0\u6d4b\u566a\u58f0\u68af\u5ea6\u504f\u5dee\u5e76\u5229\u7528\u6295\u5f71\u673a\u5236\u8fdb\u884c\u6821\u6b63\uff0c\u540c\u65f6\u4fc3\u8fdb\u4e0d\u540c\u5ba2\u6237\u7aef\u68af\u5ea6\u7684\u5bf9\u9f50\uff0c\u6307\u5bfc\u6a21\u578b\u6536\u655b\u5230\u5168\u5c40\u6700\u4f18\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u76f8\u540c\u9690\u79c1\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u5e73\u8861\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u51c6\u786e\u6027\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u5728CPSS\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60,\u5dee\u5206\u9690\u79c1,\u68af\u5ea6\u6821\u6b63,CPSS"}}
{"id": "2506.03700", "pdf": "https://arxiv.org/pdf/2506.03700", "abs": "https://arxiv.org/abs/2506.03700", "authors": ["Zhepei Wei", "Wei-Lin Chen", "Xinyu Zhu", "Yu Meng"], "title": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism", "categories": ["cs.CL"], "comment": "ICML 2025. Code: https://github.com/weizhepei/AdaDecode", "summary": "Large language models (LLMs) are increasingly used for long-content\ngeneration (e.g., long Chain-of-Thought reasoning) where decoding efficiency\nbecomes a critical bottleneck: Autoregressive decoding is inherently limited by\nits sequential token generation process, where each token must be generated\nbefore the next can be processed. This sequential dependency restricts the\nability to fully leverage modern hardware's parallel processing capabilities.\nExisting methods like speculative decoding and layer skipping offer potential\nspeedups but have notable drawbacks: speculative decoding relies on an\nauxiliary \"drafter\" model, which can be challenging to acquire and increases\nmemory overhead, while layer skipping may introduce discrepancies in the\noutputs due to the missing key-value cache at skipped layers. In this work, we\npropose AdaDecode, which accelerates LLM decoding without requiring auxiliary\nmodels or changes to the original model parameters, while ensuring output\nconsistency. AdaDecode leverages the insight that many tokens can accurately be\ngenerated at intermediate layers, as further layers often do not significantly\nalter predictions once the model reaches a certain confidence. By adaptively\ngenerating tokens at intermediate layers when confidence is high, AdaDecode\nenables the next token's computation to begin immediately. The remaining layer\ncomputations for early-predicted tokens are deferred and executed in parallel\nwith subsequent tokens when needed, maximizing hardware utilization and\nreducing decoding latency. A final verification step ensures that early\npredictions match the results of standard autoregressive decoding, preserving\noutput parity. Experiments across diverse generation tasks shows that AdaDecode\nconsistently achieves superior decoding throughput with up to 1.73x speedup,\nwhile guaranteeing output parity with standard autoregressive decoding.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAdaDecode\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5728\u4e2d\u95f4\u5c42\u751f\u6210\u9ad8\u7f6e\u4fe1\u5ea6\u4ee4\u724c\uff0c\u5e76\u884c\u8ba1\u7b97\u5269\u4f59\u5c42\uff0c\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u7801\uff0c\u786e\u4fdd\u8f93\u51fa\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u81ea\u56de\u5f52\u89e3\u7801\u7684\u6548\u7387\u53d7\u9650\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u786c\u4ef6\u7684\u5e76\u884c\u80fd\u529b\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u63a8\u6d4b\u89e3\u7801\u548c\u5c42\u8df3\u8fc7\u5b58\u5728\u6a21\u578b\u4f9d\u8d56\u6216\u8f93\u51fa\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "AdaDecode\u5229\u7528\u4e2d\u95f4\u5c42\u9ad8\u7f6e\u4fe1\u5ea6\u4ee4\u724c\u751f\u6210\uff0c\u5e76\u884c\u8ba1\u7b97\u5269\u4f59\u5c42\uff0c\u5e76\u901a\u8fc7\u9a8c\u8bc1\u6b65\u9aa4\u786e\u4fdd\u8f93\u51fa\u4e00\u81f4\u6027\u3002", "result": "AdaDecode\u5728\u591a\u6837\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u9ad81.73\u500d\u7684\u89e3\u7801\u901f\u5ea6\u63d0\u5347\uff0c\u4e14\u8f93\u51fa\u4e0e\u6807\u51c6\u81ea\u56de\u5f52\u89e3\u7801\u4e00\u81f4\u3002", "conclusion": "AdaDecode\u65e0\u9700\u8f85\u52a9\u6a21\u578b\u6216\u4fee\u6539\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347\u89e3\u7801\u6548\u7387\u4e14\u4fdd\u8bc1\u8f93\u51fa\u8d28\u91cf\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u89e3\u7801\u52a0\u901f,\u5e76\u884c\u8ba1\u7b97,\u8f93\u51fa\u4e00\u81f4\u6027"}}
{"id": "2506.03197", "pdf": "https://arxiv.org/pdf/2506.03197", "abs": "https://arxiv.org/abs/2506.03197", "authors": ["Baode Wang", "Biao Wu", "Weizhen Li", "Meng Fang", "Yanjie Liang", "Zuming Huang", "Haozhe Wang", "Jun Huang", "Ling Chen", "Wei Chu", "Yuan Qi"], "title": "Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "16 pages, 12 figures", "summary": "Automated parsing of scanned documents into richly structured,\nmachine-readable formats remains a critical bottleneck in Document AI, as\ntraditional multi-stage pipelines suffer from error propagation and limited\nadaptability to diverse layouts. We introduce layoutRL, an end-to-end\nreinforcement learning framework that trains models to be explicitly\nlayout-aware by optimizing a composite reward of normalized edit distance,\nparagraph count accuracy, and reading order preservation. Leveraging our newly\nreleased dataset, Infinity-Doc-55K, which combines 55K high-fidelity synthetic\nscanned document parsing data with expert-filtered real-world documents, we\ninstantiate layoutRL in a vision-language-model-based parser called\nInfinity-Parser. Evaluated on English and Chinese benchmarks for OCR, table and\nformula extraction, and reading order detection, Infinity-Parser achieves new\nstate-of-the-art performance in both accuracy and structural fidelity,\noutpacing specialist pipelines and general-purpose vision-language models. We\nwill publicly release our code and dataset to accelerate progress in robust\ndocument understanding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3alayoutRL\u7684\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u7efc\u5408\u5956\u52b1\u63d0\u5347\u6587\u6863\u5e03\u5c40\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6Infinity-Doc-55K\u4e0a\u8bad\u7ec3\u7684Infinity-Parser\u6a21\u578b\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u65b0\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u591a\u9636\u6bb5\u6587\u6863\u89e3\u6790\u6d41\u7a0b\u5b58\u5728\u9519\u8bef\u4f20\u64ad\u548c\u5e03\u5c40\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51falayoutRL\u6846\u67b6\uff0c\u901a\u8fc7\u7efc\u5408\u5956\u52b1\uff08\u7f16\u8f91\u8ddd\u79bb\u3001\u6bb5\u843d\u8ba1\u6570\u3001\u9605\u8bfb\u987a\u5e8f\uff09\u8bad\u7ec3\u5e03\u5c40\u611f\u77e5\u6a21\u578b\uff0c\u5e76\u57fa\u4e8eInfinity-Doc-55K\u6570\u636e\u96c6\u5f00\u53d1Infinity-Parser\u6a21\u578b\u3002", "result": "Infinity-Parser\u5728OCR\u3001\u8868\u683c\u548c\u516c\u5f0f\u63d0\u53d6\u3001\u9605\u8bfb\u987a\u5e8f\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e2d\uff0c\u51c6\u786e\u6027\u548c\u7ed3\u6784\u4fdd\u771f\u5ea6\u5747\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "layoutRL\u6846\u67b6\u548cInfinity-Parser\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u6587\u6863\u89e3\u6790\u6027\u80fd\uff0c\u672a\u6765\u5c06\u516c\u5f00\u4ee3\u7801\u548c\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u7814\u7a76\u3002", "keywords": "Document AI, \u5f3a\u5316\u5b66\u4e60, \u5e03\u5c40\u611f\u77e5, \u7aef\u5230\u7aef\u5b66\u4e60, \u6587\u6863\u89e3\u6790"}}
{"id": "2506.03674", "pdf": "https://arxiv.org/pdf/2506.03674", "abs": "https://arxiv.org/abs/2506.03674", "authors": ["Yidi Wang", "Jiawei Gu", "pei Xiaobing", "Xubin Zheng", "Xiao Luo", "Pengyang Wang", "Ziyue Qiao"], "title": "Out-of-Distribution Graph Models Merging", "categories": ["cs.LG"], "comment": null, "summary": "This paper studies a novel problem of out-of-distribution graph models\nmerging, which aims to construct a generalized model from multiple graph models\npre-trained on different domains with distribution discrepancy. This problem is\nchallenging because of the difficulty in learning domain-invariant knowledge\nimplicitly in model parameters and consolidating expertise from potentially\nheterogeneous GNN backbones. In this work, we propose a graph generation\nstrategy that instantiates the mixture distribution of multiple domains. Then,\nwe merge and fine-tune the pre-trained graph models via a MoE module and a\nmasking mechanism for generalized adaptation. Our framework is\narchitecture-agnostic and can operate without any source/target domain data.\nBoth theoretical analysis and experimental results demonstrate the\neffectiveness of our approach in addressing the model generalization problem.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5916\u5206\u5e03\u56fe\u6a21\u578b\u5408\u5e76\u7684\u65b0\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u591a\u9886\u57df\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u6784\u5efa\u901a\u7528\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u9886\u57df\u9884\u8bad\u7ec3\u56fe\u6a21\u578b\u5408\u5e76\u65f6\u7684\u5206\u5e03\u5dee\u5f02\u95ee\u9898\uff0c\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u9886\u57df\u7684\u901a\u7528\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56fe\u751f\u6210\u7b56\u7565\uff0c\u901a\u8fc7\u6df7\u5408\u57df\u5206\u5e03\u5b9e\u4f8b\u5316\uff0c\u5e76\u5229\u7528MoE\u6a21\u5757\u548c\u63a9\u7801\u673a\u5236\u8fdb\u884c\u6a21\u578b\u5408\u5e76\u4e0e\u5fae\u8c03\u3002", "result": "\u7406\u8bba\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u6a21\u578b\u6cdb\u5316\u95ee\u9898\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u5177\u6709\u67b6\u6784\u65e0\u5173\u6027\uff0c\u65e0\u9700\u6e90/\u76ee\u6807\u57df\u6570\u636e\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "\u56fe\u6a21\u578b, \u5916\u5206\u5e03, \u6a21\u578b\u5408\u5e76, \u6df7\u5408\u4e13\u5bb6, \u6cdb\u5316"}}
{"id": "2506.03704", "pdf": "https://arxiv.org/pdf/2506.03704", "abs": "https://arxiv.org/abs/2506.03704", "authors": ["Pei-Yun Lin", "Yen-lung Tsai"], "title": "ScoreRAG: A Retrieval-Augmented Generation Framework with Consistency-Relevance Scoring and Structured Summarization for News Generation", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "11 pages, 8 figures. Code and demo available at\n  https://github.com/peiyun2260/ScoreRAG. Submitted to arXiv for public access;\n  journal submission planned", "summary": "This research introduces ScoreRAG, an approach to enhance the quality of\nautomated news generation. Despite advancements in Natural Language Processing\nand large language models, current news generation methods often struggle with\nhallucinations, factual inconsistencies, and lack of domain-specific expertise\nwhen producing news articles. ScoreRAG addresses these challenges through a\nmulti-stage framework combining retrieval-augmented generation, consistency\nrelevance evaluation, and structured summarization. The system first retrieves\nrelevant news documents from a vector database, maps them to complete news\nitems, and assigns consistency relevance scores based on large language model\nevaluations. These documents are then reranked according to relevance, with\nlow-quality items filtered out. The framework proceeds to generate graded\nsummaries based on relevance scores, which guide the large language model in\nproducing complete news articles following professional journalistic standards.\nThrough this methodical approach, ScoreRAG aims to significantly improve the\naccuracy, coherence, informativeness, and professionalism of generated news\narticles while maintaining stability and consistency throughout the generation\nprocess. The code and demo are available at:\nhttps://github.com/peiyun2260/ScoreRAG.", "AI": {"tldr": "ScoreRAG \u662f\u4e00\u79cd\u901a\u8fc7\u591a\u9636\u6bb5\u6846\u67b6\u63d0\u5347\u65b0\u95fb\u751f\u6210\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u4e00\u81f4\u6027\u8bc4\u4f30\u548c\u7ed3\u6784\u5316\u6458\u8981\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6280\u672f\u4e2d\u7684\u5e7b\u89c9\u3001\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u65b0\u95fb\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u5e7b\u89c9\u3001\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u53ca\u7f3a\u4e4f\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u95ee\u9898\uff0cScoreRAG \u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u5316\u6846\u67b6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u65b0\u95fb\u7684\u8d28\u91cf\u548c\u4e13\u4e1a\u6027\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u6846\u67b6\uff1a\u68c0\u7d22\u76f8\u5173\u65b0\u95fb\u6587\u6863\u3001\u8bc4\u4f30\u4e00\u81f4\u6027\u76f8\u5173\u6027\u3001\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u5185\u5bb9\u3001\u751f\u6210\u5206\u7ea7\u6458\u8981\uff0c\u5e76\u6307\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b26\u5408\u65b0\u95fb\u6807\u51c6\u7684\u6587\u7ae0\u3002", "result": "ScoreRAG \u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u65b0\u95fb\u7684\u51c6\u786e\u6027\u3001\u8fde\u8d2f\u6027\u3001\u4fe1\u606f\u91cf\u548c\u4e13\u4e1a\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u4e0e\u4e00\u81f4\u6027\u3002", "conclusion": "ScoreRAG \u901a\u8fc7\u7cfb\u7edf\u5316\u65b9\u6cd5\u6709\u6548\u6539\u5584\u4e86\u65b0\u95fb\u751f\u6210\u7684\u8d28\u91cf\uff0c\u5e76\u63d0\u4f9b\u4e86\u4ee3\u7801\u548c\u6f14\u793a\u4f9b\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5e94\u7528\u3002", "keywords": "\u65b0\u95fb\u751f\u6210, \u68c0\u7d22\u589e\u5f3a\u751f\u6210, \u4e00\u81f4\u6027\u8bc4\u4f30, \u5927\u8bed\u8a00\u6a21\u578b, \u7ed3\u6784\u5316\u6458\u8981"}}
{"id": "2506.03198", "pdf": "https://arxiv.org/pdf/2506.03198", "abs": "https://arxiv.org/abs/2506.03198", "authors": ["Hao Yin", "Lijun Gu", "Paritosh Parmar", "Lin Xu", "Tianxiao Guo", "Weiwei Fu", "Yang Zhang", "Tianyou Zheng"], "title": "FLEX: A Large-Scale Multi-Modal Multi-Action Dataset for Fitness Action Quality Assessment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the increasing awareness of health and the growing desire for aesthetic\nphysique, fitness has become a prevailing trend. However, the potential risks\nassociated with fitness training, especially with weight-loaded fitness\nactions, cannot be overlooked. Action Quality Assessment (AQA), a technology\nthat quantifies the quality of human action and provides feedback, holds the\npotential to assist fitness enthusiasts of varying skill levels in achieving\nbetter training outcomes. Nevertheless, current AQA methodologies and datasets\nare limited to single-view competitive sports scenarios and RGB modality and\nlack professional assessment and guidance of fitness actions. To address this\ngap, we propose the FLEX dataset, the first multi-modal, multi-action,\nlarge-scale dataset that incorporates surface electromyography (sEMG) signals\ninto AQA. FLEX utilizes high-precision MoCap to collect 20 different\nweight-loaded actions performed by 38 subjects across 3 different skill levels\nfor 10 repetitions each, containing 5 different views of the RGB video, 3D\npose, sEMG, and physiological information. Additionally, FLEX incorporates\nknowledge graphs into AQA, constructing annotation rules in the form of penalty\nfunctions that map weight-loaded actions, action keysteps, error types, and\nfeedback. We conducted various baseline methodologies on FLEX, demonstrating\nthat multimodal data, multiview data, and fine-grained annotations\nsignificantly enhance model performance. FLEX not only advances AQA\nmethodologies and datasets towards multi-modal and multi-action scenarios but\nalso fosters the integration of artificial intelligence within the fitness\ndomain. Dataset and code are available at\nhttps://haoyin116.github.io/FLEX_Dataset.", "AI": {"tldr": "FLEX\u6570\u636e\u96c6\u662f\u9996\u4e2a\u591a\u6a21\u6001\u3001\u591a\u52a8\u4f5c\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4e86sEMG\u4fe1\u53f7\u7528\u4e8e\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\uff08AQA\uff09\uff0c\u63a8\u52a8\u4e86AQA\u65b9\u6cd5\u5411\u591a\u6a21\u6001\u548c\u591a\u52a8\u4f5c\u573a\u666f\u7684\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u7684AQA\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u5c40\u9650\u4e8e\u5355\u89c6\u89d2\u7ade\u6280\u8fd0\u52a8\u573a\u666f\u548cRGB\u6570\u636e\uff0c\u7f3a\u4e4f\u5bf9\u5065\u8eab\u52a8\u4f5c\u7684\u4e13\u4e1a\u8bc4\u4f30\u548c\u6307\u5bfc\u3002", "method": "\u5229\u7528\u9ad8\u7cbe\u5ea6MoCap\u6536\u96c620\u79cd\u8d1f\u91cd\u52a8\u4f5c\uff0c\u6db5\u76d65\u79cd\u89c6\u89d2\u7684RGB\u89c6\u9891\u30013D\u59ff\u6001\u3001sEMG\u548c\u751f\u7406\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u6807\u6ce8\u89c4\u5219\u3002", "result": "\u591a\u6a21\u6001\u6570\u636e\u3001\u591a\u89c6\u89d2\u6570\u636e\u548c\u7ec6\u7c92\u5ea6\u6807\u6ce8\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "FLEX\u63a8\u52a8\u4e86AQA\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u5411\u591a\u6a21\u6001\u548c\u591a\u52a8\u4f5c\u573a\u666f\u53d1\u5c55\uff0c\u4fc3\u8fdb\u4e86AI\u5728\u5065\u8eab\u9886\u57df\u7684\u5e94\u7528\u3002", "keywords": "\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\u3001\u591a\u6a21\u6001\u6570\u636e\u96c6\u3001\u5065\u8eab\u3001\u77e5\u8bc6\u56fe\u8c31\u3001sEMG"}}
{"id": "2506.03696", "pdf": "https://arxiv.org/pdf/2506.03696", "abs": "https://arxiv.org/abs/2506.03696", "authors": ["Fang Wang", "Paolo Ceravolo", "Ernesto Damiani"], "title": "Comprehensive Attribute Encoding and Dynamic LSTM HyperModels for Outcome Oriented Predictive Business Process Monitoring", "categories": ["cs.LG"], "comment": null, "summary": "Predictive Business Process Monitoring (PBPM) aims to forecast future\noutcomes of ongoing business processes. However, existing methods often lack\nflexibility to handle real-world challenges such as simultaneous events, class\nimbalance, and multi-level attributes. While prior work has explored static\nencoding schemes and fixed LSTM architectures, they struggle to support\nadaptive representations and generalize across heterogeneous datasets. To\naddress these limitations, we propose a suite of dynamic LSTM HyperModels that\nintegrate two-level hierarchical encoding for event and sequence attributes,\ncharacter-based decomposition of event labels, and novel pseudo-embedding\ntechniques for durations and attribute correlations. We further introduce\nspecialized LSTM variants for simultaneous event modeling, leveraging\nmultidimensional embeddings and time-difference flag augmentation. Experimental\nvalidation on four public and real-world datasets demonstrates up to 100%\naccuracy on balanced datasets and F1 scores exceeding 86\\% on imbalanced ones.\nOur approach advances PBPM by offering modular and interpretable models better\nsuited for deployment in complex settings. Beyond PBPM, it contributes to the\nbroader AI community by improving temporal outcome prediction, supporting data\nheterogeneity, and promoting explainable process intelligence frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u52a8\u6001LSTM HyperModels\uff0c\u901a\u8fc7\u5206\u5c42\u7f16\u7801\u548c\u4f2a\u5d4c\u5165\u6280\u672f\u6539\u8fdbPBPM\u7684\u7075\u6d3b\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709PBPM\u65b9\u6cd5\u5728\u540c\u65f6\u4e8b\u4ef6\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u591a\u7ea7\u5c5e\u6027\u7b49\u73b0\u5b9e\u6311\u6218\u4e2d\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "\u91c7\u7528\u52a8\u6001LSTM HyperModels\uff0c\u7ed3\u5408\u5206\u5c42\u7f16\u7801\u3001\u5b57\u7b26\u5206\u89e3\u548c\u4f2a\u5d4c\u5165\u6280\u672f\uff0c\u5e76\u63d0\u51fa\u4e13\u95e8LSTM\u53d8\u4f53\u5904\u7406\u540c\u65f6\u4e8b\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u8fbe100%\uff0c\u4e0d\u5e73\u8861\u6570\u636e\u96c6F1\u5206\u6570\u8d85\u8fc786%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86PBPM\u7684\u6a21\u5757\u5316\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u652f\u6301\u6570\u636e\u5f02\u6784\u6027\uff0c\u63a8\u52a8\u4e86\u53ef\u89e3\u91ca\u8fc7\u7a0b\u667a\u80fd\u6846\u67b6\u3002", "keywords": "PBPM,LSTM,\u52a8\u6001\u5efa\u6a21,\u5206\u5c42\u7f16\u7801,\u4f2a\u5d4c\u5165"}}
{"id": "2506.03722", "pdf": "https://arxiv.org/pdf/2506.03722", "abs": "https://arxiv.org/abs/2506.03722", "authors": ["Yinfeng Xia", "Huiyan Li", "Chenyang Le", "Manhong Wang", "Yutao Sun", "Xingyang Ma", "Yanmin Qian"], "title": "MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech Recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Applying large pre-trained speech models like Whisper has shown promise in\nreducing training costs for various speech tasks. However, integrating these\nmodels into streaming systems remains a challenge. This paper presents a novel\nprefix-to-prefix training framework for streaming recognition by fine-tuning\nthe Whisper. We introduce the Continuous Integrate-and-Fire mechanism to\nestablish a quasi-monotonic alignment between continuous speech sequences and\ndiscrete text tokens. Additionally, we design Monotonic Finite Look-ahead\nAttention, allowing each token to attend to infinite left-context and finite\nright-context from the speech sequences. We also employ the wait-k decoding\nstrategy to simplify the decoding process while ensuring consistency between\ntraining and testing. Our theoretical analysis and experiments demonstrate that\nthis approach achieves a controllable trade-off between latency and quality,\nmaking it suitable for various streaming applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eWhisper\u6a21\u578b\u7684\u524d\u7f00\u5230\u524d\u7f00\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u6d41\u5f0f\u8bed\u97f3\u8bc6\u522b\uff0c\u901a\u8fc7\u8fde\u7eed\u79ef\u5206\u4e0e\u89e6\u53d1\u673a\u5236\u548c\u5355\u8c03\u6709\u9650\u524d\u77bb\u6ce8\u610f\u529b\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u4e0e\u9ad8\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u8bed\u97f3\u6a21\u578b\uff08\u5982Whisper\uff09\u5728\u6d41\u5f0f\u7cfb\u7edf\u4e2d\u7684\u96c6\u6210\u96be\u9898\u3002", "method": "\u63d0\u51fa\u524d\u7f00\u5230\u524d\u7f00\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u8fde\u7eed\u79ef\u5206\u4e0e\u89e6\u53d1\u673a\u5236\u3001\u5355\u8c03\u6709\u9650\u524d\u77bb\u6ce8\u610f\u529b\u548cwait-k\u89e3\u7801\u7b56\u7565\u3002", "result": "\u5b9e\u73b0\u5ef6\u8fdf\u4e0e\u8d28\u91cf\u7684\u53ef\u63a7\u6743\u8861\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6d41\u5f0f\u5e94\u7528\u573a\u666f\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u6d41\u5f0f\u8bed\u97f3\u8bc6\u522b\u7684\u8bad\u7ec3\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "keywords": "Whisper, \u6d41\u5f0f\u8bc6\u522b, \u524d\u7f00\u5230\u524d\u7f00\u8bad\u7ec3"}}
{"id": "2506.03703", "pdf": "https://arxiv.org/pdf/2506.03703", "abs": "https://arxiv.org/abs/2506.03703", "authors": ["Xiansheng Cai", "Sihan Hu", "Tao Wang", "Yuan Huang", "Pan Zhang", "Youjin Deng", "Kun Chen"], "title": "Learning-at-Criticality in Large Language Models for Quantum Field Theory and Beyond", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "cond-mat.str-el", "physics.comp-ph"], "comment": null, "summary": "Fundamental physics often confronts complex symbolic problems with few\nguiding exemplars or established principles. While artificial intelligence (AI)\noffers promise, its typical need for vast datasets to learn from hinders its\nuse in these information-scarce frontiers. We introduce learning at criticality\n(LaC), a reinforcement learning (RL) scheme that tunes Large Language Models\n(LLMs) to a sharp learning transition, addressing this information scarcity. At\nthis transition, LLMs achieve peak generalization from minimal data,\nexemplified by 7-digit base-7 addition -- a test of nontrivial arithmetic\nreasoning. To elucidate this peak, we analyze a minimal concept-network model\n(CoNet) designed to capture the essence of how LLMs might link tokens. Trained\non a single exemplar, this model also undergoes a sharp learning transition.\nThis transition exhibits hallmarks of a second-order phase transition, notably\npower-law distributed solution path lengths. At this critical point, the system\nmaximizes a ``critical thinking pattern\" crucial for generalization, enabled by\nthe underlying scale-free exploration. This suggests LLMs reach peak\nperformance by operating at criticality, where such explorative dynamics enable\nthe extraction of underlying operational rules. We demonstrate LaC in quantum\nfield theory: an 8B-parameter LLM, tuned to its critical point by LaC using a\nfew exemplars of symbolic Matsubara sums, solves unseen, higher-order problems,\nsignificantly outperforming far larger models. LaC thus leverages critical\nphenomena, a physical principle, to empower AI for complex, data-sparse\nchallenges in fundamental physics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5b66\u4e60\u4e34\u754c\u6027\uff08LaC\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u51b3AI\u5728\u6570\u636e\u7a00\u7f3a\u7684\u524d\u6cbf\u7269\u7406\u5b66\u95ee\u9898\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u8c03\u6574\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fbe\u5230\u5b66\u4e60\u4e34\u754c\u72b6\u6001\uff0c\u5b9e\u73b0\u6700\u5c0f\u6570\u636e\u4e0b\u7684\u5cf0\u503c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3AI\u5728\u6570\u636e\u7a00\u7f3a\u7684\u590d\u6742\u7b26\u53f7\u95ee\u9898\u4e2d\u5b66\u4e60\u80fd\u529b\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u57fa\u7840\u7269\u7406\u5b66\u9886\u57df\u3002", "method": "\u5f15\u5165\u5b66\u4e60\u4e34\u754c\u6027\uff08LaC\uff09\u65b9\u6848\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8c03\u4f18LLMs\u81f3\u5b66\u4e60\u4e34\u754c\u70b9\uff0c\u5e76\u7ed3\u5408\u6700\u5c0f\u6982\u5ff5\u7f51\u7edc\u6a21\u578b\uff08CoNet\uff09\u5206\u6790\u5176\u52a8\u6001\u7279\u6027\u3002", "result": "LaC\u5728\u91cf\u5b50\u573a\u8bba\u4e2d\u663e\u8457\u4f18\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u6210\u529f\u89e3\u51b3\u672a\u89c1\u8fc7\u7684\u9ad8\u9636\u7b26\u53f7\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u4e34\u754c\u70b9\u7684\u6cdb\u5316\u4f18\u52bf\u3002", "conclusion": "LaC\u901a\u8fc7\u7269\u7406\u4e34\u754c\u73b0\u8c61\u539f\u7406\uff0c\u4e3aAI\u5728\u6570\u636e\u7a00\u7f3a\u7684\u590d\u6742\u95ee\u9898\u4e2d\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5b66\u4e60\u4e34\u754c\u6027, \u5927\u8bed\u8a00\u6a21\u578b, \u5f3a\u5316\u5b66\u4e60, \u4e34\u754c\u73b0\u8c61, \u6cdb\u5316\u80fd\u529b"}}
{"id": "2506.03723", "pdf": "https://arxiv.org/pdf/2506.03723", "abs": "https://arxiv.org/abs/2506.03723", "authors": ["Chaeyun Jang", "Moonseok Choi", "Yegon Kim", "Hyungi Lee", "Juho Lee"], "title": "Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Uncertainty calibration is essential for the safe deployment of large\nlanguage models (LLMs), particularly when users rely on verbalized confidence\nestimates. While prior work has focused on classifiers or short-form\ngeneration, confidence calibration for chain-of-thought (CoT) reasoning remains\nlargely unexplored. Surprisingly, we find that supervised fine-tuning with\nscalar confidence labels alone suffices to elicit self-verification behavior of\nlanguage models, without any explicit reasoning supervision or reinforcement\nlearning-based rewards. Despite being trained only to produce a verbalized\nconfidence score without any self-verifying examples, the model learns to\ngenerate longer and self-checking responses for low-confidence queries while\nproviding more concise answers for high-confidence ones. We further propose a\nsimple rethinking method that boosts performance via test-time scaling based on\ncalibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such\nas MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning\nimproves both calibration and accuracy, while also enhancing interpretability\nby aligning the model's reasoning path with its confidence.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u7279\u522b\u662f\u5728\u94fe\u5f0f\u63a8\u7406\uff08CoT\uff09\u60c5\u666f\u4e2d\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u5f15\u5165\u6807\u91cf\u7f6e\u4fe1\u5ea6\u6807\u7b7e\uff0c\u65e0\u9700\u663e\u5f0f\u63a8\u7406\u76d1\u7763\u5373\u53ef\u6fc0\u53d1\u6a21\u578b\u7684\u81ea\u6211\u9a8c\u8bc1\u884c\u4e3a\u3002", "motivation": "\u7528\u6237\u5728\u4f9d\u8d56\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65f6\uff0c\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u5bf9\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u6b64\u524d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5206\u7c7b\u5668\u6216\u77ed\u6587\u672c\u751f\u6210\u4e0a\uff0cCoT\u63a8\u7406\u7684\u6821\u51c6\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u4ec5\u4f7f\u7528\u6807\u91cf\u7f6e\u4fe1\u5ea6\u6807\u7b7e\uff0c\u65e0\u9700\u63a8\u7406\u76d1\u7763\u6216\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\uff1b\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u7684\u91cd\u65b0\u601d\u8003\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728GSM8K\u3001MATH-500\u548cARC-Challenge\u7b49\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6821\u51c6\u6027\u3001\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u4ec5\u901a\u8fc7\u6807\u91cf\u7f6e\u4fe1\u5ea6\u6807\u7b7e\u7684\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u6a21\u578b\u81ea\u6211\u9a8c\u8bc1\uff0c\u540c\u65f6\u6539\u8fdb\u63a8\u7406\u8def\u5f84\u4e0e\u7f6e\u4fe1\u5ea6\u7684\u5bf9\u9f50\u3002", "keywords": "\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u3001\u94fe\u5f0f\u63a8\u7406\u3001\u76d1\u7763\u5fae\u8c03\u3001\u81ea\u6211\u9a8c\u8bc1\u3001\u91cd\u65b0\u601d\u8003"}}
{"id": "2506.03209", "pdf": "https://arxiv.org/pdf/2506.03209", "abs": "https://arxiv.org/abs/2506.03209", "authors": ["Tinghuan Li", "Shuheng Chen", "Junyi Fan", "Elham Pishgar", "Kamiar Alaei", "Greg Placencia", "Maryam Pishgar"], "title": "Predicting Postoperative Stroke in Elderly SICU Patients: An Interpretable Machine Learning Model Using MIMIC Data", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": null, "summary": "Postoperative stroke remains a critical complication in elderly surgical\nintensive care unit (SICU) patients, contributing to prolonged hospitalization,\nelevated healthcare costs, and increased mortality. Accurate early risk\nstratification is essential to enable timely intervention and improve clinical\noutcomes. We constructed a combined cohort of 19,085 elderly SICU admissions\nfrom the MIMIC-III and MIMIC-IV databases and developed an interpretable\nmachine learning (ML) framework to predict in-hospital stroke using clinical\ndata from the first 24 hours of Intensive Care Unit (ICU) stay. The\npreprocessing pipeline included removal of high-missingness features, iterative\nSingular Value Decomposition (SVD) imputation, z-score normalization, one-hot\nencoding, and class imbalance correction via the Adaptive Synthetic Sampling\n(ADASYN) algorithm. A two-stage feature selection process-combining Recursive\nFeature Elimination with Cross-Validation (RFECV) and SHapley Additive\nexPlanations (SHAP)-reduced the initial 80 variables to 20 clinically\ninformative predictors. Among eight ML models evaluated, CatBoost achieved the\nbest performance with an AUROC of 0.8868 (95% CI: 0.8802--0.8937). SHAP\nanalysis and ablation studies identified prior cerebrovascular disease, serum\ncreatinine, and systolic blood pressure as the most influential risk factors.\nOur results highlight the potential of interpretable ML approaches to support\nearly detection of postoperative stroke and inform decision-making in\nperioperative critical care.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9884\u6d4b\u8001\u5e74SICU\u60a3\u8005\u7684\u672f\u540e\u5352\u4e2d\u98ce\u9669\uff0cCatBoost\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cAUROC\u4e3a0.8868\u3002", "motivation": "\u672f\u540e\u5352\u4e2d\u662f\u8001\u5e74SICU\u60a3\u8005\u7684\u4e25\u91cd\u5e76\u53d1\u75c7\uff0c\u9700\u65e9\u671f\u98ce\u9669\u5206\u5c42\u4ee5\u6539\u5584\u4e34\u5e8a\u7ed3\u679c\u3002", "method": "\u57fa\u4e8eMIMIC-III\u548cMIMIC-IV\u6570\u636e\uff0c\u91c7\u7528SVD\u63d2\u8865\u3001ADASYN\u5e73\u8861\u3001RFECV\u548cSHAP\u7279\u5f81\u9009\u62e9\uff0c\u8bc4\u4f30\u516b\u79cdML\u6a21\u578b\u3002", "result": "CatBoost\u6a21\u578b\u8868\u73b0\u6700\u4f18\uff0c\u5173\u952e\u9884\u6d4b\u56e0\u7d20\u5305\u62ec\u65e2\u5f80\u8111\u8840\u7ba1\u75be\u75c5\u3001\u8840\u6e05\u808c\u9150\u548c\u6536\u7f29\u538b\u3002", "conclusion": "\u53ef\u89e3\u91caML\u65b9\u6cd5\u6709\u52a9\u4e8e\u65e9\u671f\u53d1\u73b0\u672f\u540e\u5352\u4e2d\u5e76\u6307\u5bfc\u4e34\u5e8a\u51b3\u7b56\u3002", "keywords": "\u672f\u540e\u5352\u4e2d, \u673a\u5668\u5b66\u4e60, SICU, \u53ef\u89e3\u91ca\u6027, CatBoost"}}
{"id": "2506.03719", "pdf": "https://arxiv.org/pdf/2506.03719", "abs": "https://arxiv.org/abs/2506.03719", "authors": ["Quentin Bertrand", "Anne Gagneux", "Mathurin Massias", "R\u00e9mi Emonet"], "title": "On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Modern deep generative models can now produce high-quality synthetic samples\nthat are often indistinguishable from real training data. A growing body of\nresearch aims to understand why recent methods -- such as diffusion and flow\nmatching techniques -- generalize so effectively. Among the proposed\nexplanations are the inductive biases of deep learning architectures and the\nstochastic nature of the conditional flow matching loss. In this work, we rule\nout the latter -- the noisy nature of the loss -- as a primary contributor to\ngeneralization in flow matching. First, we empirically show that in\nhigh-dimensional settings, the stochastic and closed-form versions of the flow\nmatching loss yield nearly equivalent losses. Then, using state-of-the-art flow\nmatching models on standard image datasets, we demonstrate that both variants\nachieve comparable statistical performance, with the surprising observation\nthat using the closed-form can even improve performance.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u6d41\u5339\u914d\u635f\u5931\u7684\u968f\u673a\u6027\u5e76\u975e\u5176\u6cdb\u5316\u80fd\u529b\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u5c01\u95ed\u5f62\u5f0f\u635f\u5931\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u8868\u73b0\u76f8\u4f3c\u751a\u81f3\u66f4\u597d\u3002", "motivation": "\u63a2\u8ba8\u6df1\u5ea6\u5b66\u4e60\u751f\u6210\u6a21\u578b\uff08\u5982\u6269\u6563\u548c\u6d41\u5339\u914d\u6280\u672f\uff09\u4e3a\u4f55\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u539f\u56e0\uff0c\u6392\u9664\u635f\u5931\u51fd\u6570\u968f\u673a\u6027\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u6bd4\u8f83\u968f\u673a\u548c\u5c01\u95ed\u5f62\u5f0f\u7684\u6d41\u5339\u914d\u635f\u5931\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u4f7f\u7528\u5148\u8fdb\u6a21\u578b\u6d4b\u8bd5\u5176\u7edf\u8ba1\u6027\u80fd\u3002", "result": "\u4e24\u79cd\u635f\u5931\u5f62\u5f0f\u5728\u7edf\u8ba1\u6027\u80fd\u4e0a\u76f8\u5f53\uff0c\u5c01\u95ed\u5f62\u5f0f\u751a\u81f3\u53ef\u80fd\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u6d41\u5339\u914d\u7684\u6cdb\u5316\u80fd\u529b\u4e3b\u8981\u4e0d\u4f9d\u8d56\u635f\u5931\u7684\u968f\u673a\u6027\u3002", "keywords": "\u751f\u6210\u6a21\u578b,\u6d41\u5339\u914d,\u6cdb\u5316\u80fd\u529b,\u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2506.03735", "pdf": "https://arxiv.org/pdf/2506.03735", "abs": "https://arxiv.org/abs/2506.03735", "authors": ["Junling Wang", "Anna Rutkiewicz", "April Yi Wang", "Mrinmaya Sachan"], "title": "Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Findings of the Association for Computational Linguistics: ACL 2025", "summary": "Visuals are valuable tools for teaching math word problems (MWPs), helping\nyoung learners interpret textual descriptions into mathematical expressions\nbefore solving them. However, creating such visuals is labor-intensive and\nthere is a lack of automated methods to support this process. In this paper, we\npresent Math2Visual, an automatic framework for generating pedagogically\nmeaningful visuals from MWP text descriptions. Math2Visual leverages a\npre-defined visual language and a design space grounded in interviews with math\nteachers, to illustrate the core mathematical relationships in MWPs. Using\nMath2Visual, we construct an annotated dataset of 1,903 visuals and evaluate\nText-to-Image (TTI) models for their ability to generate visuals that align\nwith our design. We further fine-tune several TTI models with our dataset,\ndemonstrating improvements in educational visual generation. Our work\nestablishes a new benchmark for automated generation of pedagogically\nmeaningful visuals and offers insights into key challenges in producing\nmultimodal educational content, such as the misrepresentation of mathematical\nrelationships and the omission of essential visual elements.", "AI": {"tldr": "Math2Visual\u662f\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u6570\u5b66\u5e94\u7528\u9898\uff08MWPs\uff09\u6559\u80b2\u610f\u4e49\u89c6\u89c9\u5185\u5bb9\u7684\u6846\u67b6\uff0c\u5229\u7528\u9884\u5b9a\u4e49\u7684\u89c6\u89c9\u8bed\u8a00\u548c\u6559\u5e08\u8bbf\u8c08\u8bbe\u8ba1\uff0c\u5efa\u7acb\u4e861903\u5f20\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u6587\u672c\u5230\u56fe\u50cf\uff08TTI\uff09\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u548c\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u624b\u52a8\u521b\u5efa\u6570\u5b66\u5e94\u7528\u9898\u89c6\u89c9\u5185\u5bb9\u7684\u52b3\u52a8\u5bc6\u96c6\u578b\u95ee\u9898\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u652f\u6301\u3002", "method": "\u63d0\u51faMath2Visual\u6846\u67b6\uff0c\u7ed3\u5408\u9884\u5b9a\u4e49\u89c6\u89c9\u8bed\u8a00\u548c\u6559\u5e08\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u751f\u6210\u6559\u80b2\u610f\u4e49\u89c6\u89c9\u5185\u5bb9\uff0c\u5e76\u6784\u5efa\u6807\u6ce8\u6570\u636e\u96c6\u4f18\u5316TTI\u6a21\u578b\u3002", "result": "\u5efa\u7acb\u4e861903\u5f20\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5fae\u8c03TTI\u6a21\u578b\u63d0\u5347\u4e86\u6559\u80b2\u89c6\u89c9\u751f\u6210\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u591a\u6a21\u6001\u6559\u80b2\u5185\u5bb9\u751f\u6210\u4e2d\u7684\u6311\u6218\u3002", "conclusion": "Math2Visual\u4e3a\u81ea\u52a8\u5316\u751f\u6210\u6559\u80b2\u610f\u4e49\u89c6\u89c9\u5185\u5bb9\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u6539\u8fdb\u591a\u6a21\u6001\u6559\u80b2\u5185\u5bb9\u7684\u5173\u952e\u89c1\u89e3\u3002", "keywords": "Math2Visual, \u6570\u5b66\u5e94\u7528\u9898, \u89c6\u89c9\u751f\u6210, \u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b, \u6559\u80b2\u6280\u672f"}}
{"id": "2506.03725", "pdf": "https://arxiv.org/pdf/2506.03725", "abs": "https://arxiv.org/abs/2506.03725", "authors": ["Daniil Medyakov", "Sergey Stanko", "Gleb Molodtsov", "Philip Zmushko", "Grigoriy Evseev", "Egor Petrov", "Aleksandr Beznosikov"], "title": "Sign-SGD is the Golden Gate between Multi-Node to Single-Node Learning: Significant Boost via Parameter-Free Optimization", "categories": ["cs.LG", "math.OC"], "comment": "58 pages, 5 figures, 5 tables", "summary": "Quite recently, large language models have made a significant breakthrough\nacross various disciplines. However, training them is an extremely\nresource-intensive task, even for major players with vast computing resources.\nOne of the methods gaining popularity in light of these challenges is Sign-SGD.\nThis method can be applied both as a memory-efficient approach in single-node\ntraining and as a gradient compression technique in the distributed learning.\nNevertheless, it is impossible to automatically determine the effective\nstepsize from the theoretical standpoint. Indeed, it depends on the parameters\nof the dataset to which we do not have access in the real-world learning\nparadigm. To address this issue, we design several variants of single-node\ndeterministic Sign-SGD. We extend our approaches to practical scenarios:\nstochastic single-node and multi-node learning, methods with incorporated\nmomentum. We conduct extensive experiments on real machine learning problems\nthat emphasize the practical applicability of our ideas.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u786e\u5b9a\u6027Sign-SGD\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u65e0\u6cd5\u81ea\u52a8\u786e\u5b9a\u6709\u6548\u6b65\u957f\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u673a\u5668\u5b66\u4e60\u95ee\u9898\u4e2d\u7684\u9002\u7528\u6027\u3002", "motivation": "\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d44\u6e90\u5bc6\u96c6\uff0cSign-SGD\u56e0\u5176\u5185\u5b58\u9ad8\u6548\u548c\u68af\u5ea6\u538b\u7f29\u7279\u6027\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u65e0\u6cd5\u81ea\u52a8\u786e\u5b9a\u6709\u6548\u6b65\u957f\uff0c\u5f71\u54cd\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u79cd\u786e\u5b9a\u6027Sign-SGD\u53d8\u4f53\uff0c\u6269\u5c55\u5230\u968f\u673a\u5355\u8282\u70b9\u548c\u591a\u8282\u70b9\u5b66\u4e60\uff0c\u5e76\u7ed3\u5408\u52a8\u91cf\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5b9e\u9645\u673a\u5668\u5b66\u4e60\u95ee\u9898\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u6539\u8fdb\u7684Sign-SGD\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8d44\u6e90\u5bc6\u96c6\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, Sign-SGD, \u5355\u8282\u70b9\u5b66\u4e60, \u591a\u8282\u70b9\u5b66\u4e60, \u52a8\u91cf\u65b9\u6cd5"}}
{"id": "2506.03761", "pdf": "https://arxiv.org/pdf/2506.03761", "abs": "https://arxiv.org/abs/2506.03761", "authors": ["Hongcheng Guo", "Zheyong Xie", "Shaosheng Cao", "Boyang Wang", "Weiting Liu", "Zheyu Ye", "Zhoujun Li", "Zuozhu Liu"], "title": "Act-as-Pet: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services", "categories": ["cs.CL"], "comment": null, "summary": "As interest in using Large Language Models (LLMs) for interactive and\nemotionally rich experiences grows, virtual pet companionship emerges as a\nnovel yet underexplored application. Existing approaches focus on basic pet\nrole-playing interactions without systematically benchmarking LLMs for\ncomprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated\nbenchmark that evaluates LLMs across both self-interaction and\nhuman-interaction dimensions. Unlike prior work, Pet-Bench emphasizes\nself-evolution and developmental behaviors alongside interactive engagement,\noffering a more realistic reflection of pet companionship. It features diverse\ntasks such as intelligent scheduling, memory-based dialogues, and psychological\nconversations, with over 7,500 interaction instances designed to simulate\ncomplex pet behaviors. Evaluation of 28 LLMs reveals significant performance\nvariations linked to model size and inherent capabilities, underscoring the\nneed for specialized optimization in this domain. Pet-Bench serves as a\nfoundational resource for benchmarking pet-related LLM abilities and advancing\nemotionally immersive human-pet interactions.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Pet-Bench\uff0c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u865a\u62df\u5ba0\u7269\u4f34\u4fa3\u4e2d\u8868\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5f3a\u8c03\u81ea\u6211\u8fdb\u5316\u548c\u4e92\u52a8\u884c\u4e3a\u3002", "motivation": "\u968f\u7740\u5bf9\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u60c5\u611f\u4e30\u5bcc\u4ea4\u4e92\u7684\u5174\u8da3\u589e\u957f\uff0c\u865a\u62df\u5ba0\u7269\u4f34\u4fa3\u6210\u4e3a\u4e00\u4e2a\u65b0\u5174\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\u3002", "method": "\u63d0\u51faPet-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u81ea\u6211\u4e92\u52a8\u548c\u4eba\u7c7b\u4e92\u52a8\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u8bbe\u8ba1\u4e867500\u591a\u4e2a\u4ea4\u4e92\u5b9e\u4f8b\u3002", "result": "\u6d4b\u8bd5\u4e8628\u4e2aLLMs\uff0c\u53d1\u73b0\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u4e0e\u6a21\u578b\u89c4\u6a21\u548c\u80fd\u529b\u76f8\u5173\u3002", "conclusion": "Pet-Bench\u662f\u8bc4\u4f30\u5ba0\u7269\u4f34\u4fa3\u76f8\u5173LLM\u80fd\u529b\u7684\u57fa\u7840\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u60c5\u611f\u6c89\u6d78\u5f0f\u4eba\u5ba0\u4e92\u52a8\u7684\u53d1\u5c55\u3002", "keywords": "Large Language Models, virtual pet, companionship, benchmark, interaction"}}
{"id": "2506.03214", "pdf": "https://arxiv.org/pdf/2506.03214", "abs": "https://arxiv.org/abs/2506.03214", "authors": ["Yi Guo", "Yihang Dong", "Michael Kwok-Po Ng", "Shuqiang Wang"], "title": "A Pre-trained Framework for Multilingual Brain Decoding Using Non-invasive Recordings", "categories": ["q-bio.NC", "cs.AI", "cs.CL"], "comment": null, "summary": "Brain-computer interfaces (BCIs) with speech decoding from brain recordings\nhave broad application potential in fields such as clinical rehabilitation and\ncognitive neuroscience. However, current decoding methods remain limited to\nsingle-language, single-subject, and single neuroimaging modality settings,\nrestricting their clinical applicability and generalizability. Here we propose\na joint multilingual, multi-subject and multimodal decoding framework. It maps\ndiverse brain recordings into a unified semantic space defined by a pre-trained\nmultilingual model (PMM), enabling decoding across multiple languages, multiple\nsubjects and multiple neuroimaging modalities. The proposed framework is\nvalidated using non-invasive brain recordings from 159 participants across four\nlanguages. Experimental results show that it exhibits strong generalization\nacross multilingual, multi-subject, and multimodal settings. More importantly,\nthe proposed framework can promote linguistic fairness, which is vital for\nunderrepresented languages in BCI applications. The unified semantic space\nenables cross-lingual mapping enhancement, allowing the framework to boost the\ndecoding performance of underrepresented languages, thereby promoting\nlinguistic fairness. Overall, the proposed framework establishes a new\npotential paradigm for brain decoding, opening new paths for broader\napplications of BCI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8bed\u8a00\u3001\u591a\u88ab\u8bd5\u548c\u591a\u6a21\u6001\u7684\u8111\u673a\u63a5\u53e3\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u591a\u6837\u5316\u8111\u4fe1\u53f7\u6620\u5c04\u5230\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u6a21\u578b\uff08PMM\uff09\u5b9a\u4e49\u7684\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u3001\u8de8\u88ab\u8bd5\u548c\u8de8\u6a21\u6001\u7684\u89e3\u7801\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u8111\u4fe1\u53f7\u89e3\u7801\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u8bed\u8a00\u3001\u5355\u88ab\u8bd5\u548c\u5355\u6a21\u6001\uff0c\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u5e94\u7528\u548c\u666e\u9002\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u5177\u901a\u7528\u6027\u548c\u516c\u5e73\u6027\u7684\u6846\u67b6\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u6a21\u578b\uff08PMM\uff09\u6784\u5efa\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\uff0c\u5c06\u591a\u6837\u5316\u8111\u4fe1\u53f7\u6620\u5c04\u5230\u8be5\u7a7a\u95f4\uff0c\u5b9e\u73b0\u8de8\u8bed\u8a00\u3001\u8de8\u88ab\u8bd5\u548c\u8de8\u6a21\u6001\u7684\u89e3\u7801\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728159\u540d\u88ab\u8bd5\u548c\u56db\u79cd\u8bed\u8a00\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c55\u73b0\u4e86\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6b20\u4ee3\u8868\u8bed\u8a00\u7684\u89e3\u7801\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8111\u4fe1\u53f7\u89e3\u7801\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u63a8\u52a8\u8111\u673a\u63a5\u53e3\u5728\u66f4\u5e7f\u6cdb\u9886\u57df\u7684\u5e94\u7528\uff0c\u540c\u65f6\u4fc3\u8fdb\u4e86\u8bed\u8a00\u516c\u5e73\u6027\u3002", "keywords": ""}}
{"id": "2506.03757", "pdf": "https://arxiv.org/pdf/2506.03757", "abs": "https://arxiv.org/abs/2506.03757", "authors": ["Razvan-Andrei Lascu", "David \u0160i\u0161ka", "\u0141ukasz Szpruch"], "title": "PPO in the Fisher-Rao geometry", "categories": ["cs.LG", "math.OC"], "comment": "17 pages", "summary": "Proximal Policy Optimization (PPO) has become a widely adopted algorithm for\nreinforcement learning, offering a practical policy gradient method with strong\nempirical performance. Despite its popularity, PPO lacks formal theoretical\nguarantees for policy improvement and convergence. PPO is motivated by Trust\nRegion Policy Optimization (TRPO) that utilizes a surrogate loss with a KL\ndivergence penalty, which arises from linearizing the value function within a\nflat geometric space. In this paper, we derive a tighter surrogate in the\nFisher-Rao (FR) geometry, yielding a novel variant, Fisher-Rao PPO (FR-PPO).\nOur proposed scheme provides strong theoretical guarantees, including monotonic\npolicy improvement. Furthermore, in the tabular setting, we demonstrate that\nFR-PPO achieves sub-linear convergence without any dependence on the\ndimensionality of the action or state spaces, marking a significant step toward\nestablishing formal convergence results for PPO-based algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFisher-Rao\u51e0\u4f55\u7684PPO\u53d8\u4f53FR-PPO\uff0c\u63d0\u4f9b\u4e86\u5355\u8c03\u7b56\u7565\u6539\u8fdb\u7684\u5f3a\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5728\u8868\u683c\u8bbe\u7f6e\u4e2d\u5c55\u793a\u4e86\u5176\u6536\u655b\u6027\u3002", "motivation": "\u5c3d\u7ba1PPO\u7b97\u6cd5\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u4fdd\u969c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165Fisher-Rao\u51e0\u4f55\u6539\u8fdbPPO\uff0c\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u5229\u7528Fisher-Rao\u51e0\u4f55\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u66f4\u7d27\u7684\u4ee3\u7406\u635f\u5931\u51fd\u6570\uff0c\u63d0\u51faFR-PPO\u7b97\u6cd5\u3002", "result": "FR-PPO\u5728\u8868\u683c\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u6536\u655b\uff0c\u4e14\u4e0d\u4f9d\u8d56\u52a8\u4f5c\u6216\u72b6\u6001\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002", "conclusion": "FR-PPO\u4e3aPPO\u7c7b\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u6536\u655b\u6027\u4fdd\u8bc1\uff0c\u662f\u91cd\u8981\u7684\u4e00\u6b65\u3002", "keywords": "PPO, Fisher-Rao\u51e0\u4f55, \u5f3a\u5316\u5b66\u4e60, \u7406\u8bba\u4fdd\u8bc1"}}
{"id": "2506.03762", "pdf": "https://arxiv.org/pdf/2506.03762", "abs": "https://arxiv.org/abs/2506.03762", "authors": ["Yifeng Gu", "Zicong Jiang", "Jianxiu Jin", "Kailing Guo", "Ziyang Zhang", "Xiangmin Xu"], "title": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 8 figures", "summary": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAhaKV\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u6ce8\u610f\u529b\u5f97\u5206\u7684\u6bd4\u4f8b\u4ee5\u51cf\u5c11KV\u7f13\u5b58\u4e2d\u7684\u504f\u5dee\uff0c\u4ece\u800c\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u8fc7\u7a0b\u9700\u8981\u5927\u91cf\u8d44\u6e90\uff0c\u5c24\u5176\u662fKV\u7f13\u5b58\u5360\u7528\u8fc7\u591a\u5185\u5b58\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u5f97\u5206\u6dd8\u6c70\u4e0d\u5fc5\u8981\u4ee4\u724c\u7684\u65b9\u6cd5\u5b58\u5728\u504f\u5dee\uff0c\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "AhaKV\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574softmax\u7684\u6bd4\u4f8b\uff0c\u5e76\u5229\u7528\u503c\u5411\u91cf\u7684\u4fe1\u606f\u6765\u51cf\u5c11\u6ce8\u610f\u529b\u5f97\u5206\u7684\u504f\u5dee\uff0c\u4ece\u800c\u4fdd\u7559\u5168\u5c40\u4e0a\u4e0b\u6587\u4e2d\u7684\u5173\u952e\u4ee4\u724c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cAhaKV\u5728\u56fa\u5b9a\u7f13\u5b58\u9884\u7b97\u4e0b\u6709\u6548\u51cf\u5c11\u4e86\u504f\u5dee\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u7ed3\u679c\u3002", "conclusion": "AhaKV\u901a\u8fc7\u81ea\u9002\u5e94\u65b9\u6cd5\u4f18\u5316KV\u7f13\u5b58\uff0c\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u5229\u7528\u80fd\u529b\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, KV\u7f13\u5b58, \u6ce8\u610f\u529b\u673a\u5236, \u81ea\u9002\u5e94, \u63a8\u7406\u4f18\u5316"}}
{"id": "2506.03218", "pdf": "https://arxiv.org/pdf/2506.03218", "abs": "https://arxiv.org/abs/2506.03218", "authors": ["Alina Wernick", "Kristof Meding"], "title": "Beware! The AI Act Can Also Apply to Your AI Research Practices", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "The EU has become one of the vanguards in regulating the digital age. A\nparticularly important regulation in the Artificial Intelligence (AI) domain is\nthe EU AI Act, which entered into force in 2024. The AI Act specifies -- due to\na risk-based approach -- various obligations for providers of AI systems. These\nobligations, for example, include a cascade of documentation and compliance\nmeasures, which represent a potential obstacle to science. But do these\nobligations also apply to AI researchers? This position paper argues that,\nindeed, the AI Act's obligations could apply in many more cases than the AI\ncommunity is aware of. In our analysis of the AI Act and its applicability, we\ncontribute the following: 1.) We give a high-level introduction to the AI Act\naimed at non-legal AI research scientists. 2.) We explain with everyday\nresearch examples why the AI Act applies to research. 3.) We analyse the\nexceptions of the AI Act's applicability and state that especially scientific\nresearch exceptions fail to account for current AI research practices. 4.) We\npropose changes to the AI Act to provide more legal certainty for AI\nresearchers and give two recommendations for AI researchers to reduce the risk\nof not complying with the AI Act. We see our paper as a starting point for a\ndiscussion between policymakers, legal scholars, and AI researchers to avoid\nunintended side effects of the AI Act on research.", "AI": {"tldr": "\u6b27\u76df\u6210\u4e3a\u6570\u5b57\u65f6\u4ee3\u76d1\u7ba1\u7684\u5148\u9a71\uff0c\u300a\u6b27\u76df\u4eba\u5de5\u667a\u80fd\u6cd5\u6848\u300b\u4e8e2024\u5e74\u751f\u6548\uff0c\u5176\u57fa\u4e8e\u98ce\u9669\u7684\u65b9\u6cd5\u5bf9AI\u7cfb\u7edf\u63d0\u4f9b\u5546\u63d0\u51fa\u4e86\u591a\u9879\u4e49\u52a1\uff0c\u53ef\u80fd\u5bf9\u79d1\u5b66\u7814\u7a76\u6784\u6210\u969c\u788d\u3002\u672c\u6587\u8ba4\u4e3a\u6cd5\u6848\u4e49\u52a1\u53ef\u80fd\u6bd4AI\u793e\u533a\u610f\u8bc6\u5230\u7684\u66f4\u5e7f\u6cdb\u9002\u7528\u4e8e\u7814\u7a76\u9886\u57df\uff0c\u5e76\u63d0\u51fa\u4fee\u6539\u5efa\u8bae\u4ee5\u589e\u52a0\u6cd5\u5f8b\u786e\u5b9a\u6027\u3002", "motivation": "\u63a2\u8ba8\u300a\u6b27\u76df\u4eba\u5de5\u667a\u80fd\u6cd5\u6848\u300b\u5bf9AI\u7814\u7a76\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u6cd5\u6848\u4e49\u52a1\u5bf9\u79d1\u5b66\u7814\u7a76\u7684\u6f5c\u5728\u963b\u788d\uff0c\u5e76\u547c\u5401\u653f\u7b56\u5236\u5b9a\u8005\u3001\u6cd5\u5f8b\u5b66\u8005\u548cAI\u7814\u7a76\u8005\u5171\u540c\u8ba8\u8bba\u4ee5\u907f\u514d\u6cd5\u6848\u5bf9\u7814\u7a76\u7684\u4e0d\u5229\u5f71\u54cd\u3002", "method": "\u5206\u6790\u300a\u6b27\u76df\u4eba\u5de5\u667a\u80fd\u6cd5\u6848\u300b\u53ca\u5176\u9002\u7528\u6027\uff0c\u9762\u5411\u975e\u6cd5\u5f8b\u80cc\u666f\u7684AI\u7814\u7a76\u8005\u8fdb\u884c\u9ad8\u5c42\u6b21\u4ecb\u7ecd\uff0c\u5e76\u901a\u8fc7\u65e5\u5e38\u7814\u7a76\u6848\u4f8b\u8bf4\u660e\u6cd5\u6848\u5bf9\u7814\u7a76\u7684\u9002\u7528\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6cd5\u6848\u7684\u79d1\u5b66\u4f8b\u5916\u6761\u6b3e\u672a\u80fd\u9002\u5e94\u5f53\u524dAI\u7814\u7a76\u5b9e\u8df5\uff0c\u9700\u8981\u4fee\u6539\u4ee5\u63d0\u4f9b\u66f4\u591a\u6cd5\u5f8b\u786e\u5b9a\u6027\u3002", "conclusion": "\u63d0\u51fa\u6cd5\u6848\u4fee\u6539\u5efa\u8bae\uff0c\u5e76\u4e3aAI\u7814\u7a76\u8005\u63d0\u4f9b\u4e24\u6761\u51cf\u5c11\u5408\u89c4\u98ce\u9669\u7684\u5efa\u8bae\uff0c\u547c\u5401\u591a\u65b9\u5bf9\u8bdd\u4ee5\u907f\u514d\u5bf9\u7814\u7a76\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "keywords": "\u6b27\u76dfAI\u6cd5\u6848,\u4eba\u5de5\u667a\u80fd\u7814\u7a76,\u6cd5\u5f8b\u5408\u89c4,\u79d1\u5b66\u4f8b\u5916,\u653f\u7b56\u8ba8\u8bba"}}
{"id": "2506.03758", "pdf": "https://arxiv.org/pdf/2506.03758", "abs": "https://arxiv.org/abs/2506.03758", "authors": ["Daniel Palenicek", "Florian Vogt", "Jan Peters"], "title": "Scaling CrossQ with Weight Normalization", "categories": ["cs.LG", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2502.07523", "summary": "Reinforcement learning has achieved significant milestones, but sample\nefficiency remains a bottleneck for real-world applications. Recently, CrossQ\nhas demonstrated state-of-the-art sample efficiency with a low update-to-data\n(UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with\nhigher UTD ratios. We identify challenges in the training dynamics which are\nemphasized by higher UTDs, particularly Q-bias explosion and the growing\nmagnitude of critic network weights. To address this, we integrate weight\nnormalization into the CrossQ framework, a solution that stabilizes training,\nprevents potential loss of plasticity and keeps the effective learning rate\nconstant. Our proposed approach reliably scales with increasing UTD ratios,\nachieving competitive or superior performance across a range of challenging\ntasks on the DeepMind control benchmark, notably the complex dog and humanoid\nenvironments. This work eliminates the need for drastic interventions, such as\nnetwork resets, and offers a robust pathway for improving sample efficiency and\nscalability in model-free reinforcement learning.", "AI": {"tldr": "CrossQ\u7684\u6837\u672c\u6548\u7387\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9ad8\u66f4\u65b0\u4e0e\u6570\u636e\u6bd4\uff08UTD\uff09\u4f1a\u5f15\u53d1\u8bad\u7ec3\u52a8\u6001\u95ee\u9898\uff0c\u5982Q\u504f\u5dee\u7206\u70b8\u548c\u6743\u91cd\u5e45\u503c\u589e\u957f\u3002\u901a\u8fc7\u96c6\u6210\u6743\u91cd\u5f52\u4e00\u5316\uff0c\u8be5\u65b9\u6cd5\u7a33\u5b9a\u4e86\u8bad\u7ec3\uff0c\u4fdd\u6301\u4e86\u8f83\u9ad8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3CrossQ\u5728\u9ad8UTD\u6bd4\u4e0b\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u6837\u672c\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5728CrossQ\u6846\u67b6\u4e2d\u96c6\u6210\u6743\u91cd\u5f52\u4e00\u5316\u6280\u672f\uff0c\u4ee5\u7a33\u5b9a\u8bad\u7ec3\u52a8\u6001\u5e76\u9632\u6b62\u5851\u6027\u635f\u5931\u3002", "result": "\u5728DeepMind\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u72d7\u548c\u4eba\u5f62\u73af\u5883\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u9ad8\u6027\u80fd\u3002", "conclusion": "\u6743\u91cd\u5f52\u4e00\u5316\u4e3aCrossQ\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u6fc0\u8fdb\u5e72\u9884\u7684\u7a33\u5065\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60,\u6837\u672c\u6548\u7387,CrossQ,\u6743\u91cd\u5f52\u4e00\u5316,DeepMind\u63a7\u5236\u57fa\u51c6"}}
{"id": "2506.03763", "pdf": "https://arxiv.org/pdf/2506.03763", "abs": "https://arxiv.org/abs/2506.03763", "authors": ["Quang Hieu Pham", "Thuy Duong Nguyen", "Tung Pham", "Anh Tuan Luu", "Dat Quoc Nguyen"], "title": "ClozeMath: Improving Mathematical Reasoning in Language Models by Learning to Fill Equations", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "The capabilities of large language models (LLMs) have been enhanced by\ntraining on data that reflects human thought processes, such as the\nChain-of-Thought format. However, evidence suggests that the conventional\nscheme of next-word prediction may not fully capture how humans learn to think.\nInspired by how humans generalize mathematical reasoning, we propose a new\napproach named ClozeMath to fine-tune LLMs for mathematical reasoning. Our\nClozeMath involves a text-infilling task that predicts masked equations from a\ngiven solution, analogous to cloze exercises used in human learning.\nExperiments on GSM8K, MATH, and GSM-Symbolic show that ClozeMath surpasses the\nstrong baseline Masked Thought in performance and robustness, with two\ntest-time scaling decoding algorithms, Beam Search and Chain-of-Thought\ndecoding. Additionally, we conduct an ablation study to analyze the effects of\nvarious architectural and implementation choices on our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aClozeMath\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u586b\u5145\u4efb\u52a1\u6765\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u6570\u5b66\u63a8\u7406\u95ee\u9898\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u65b9\u6cd5\u672a\u80fd\u5b8c\u5168\u6a21\u62df\u4eba\u7c7b\u5b66\u4e60\u601d\u7ef4\u7684\u65b9\u5f0f\uff0c\u7279\u522b\u662f\u6570\u5b66\u63a8\u7406\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "ClozeMath\u91c7\u7528\u6587\u672c\u586b\u5145\u4efb\u52a1\uff0c\u9884\u6d4b\u7ed9\u5b9a\u89e3\u4e2d\u88ab\u906e\u853d\u7684\u65b9\u7a0b\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u5b66\u4e60\u4e2d\u7684\u586b\u7a7a\u7ec3\u4e60\u3002", "result": "\u5728GSM8K\u3001MATH\u548cGSM-Symbolic\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cClozeMath\u4f18\u4e8e\u57fa\u7ebfMasked Thought\uff0c\u4e14\u5728\u6027\u80fd\u548c\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "ClozeMath\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u5b66\u4e60\u65b9\u5f0f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u67b6\u6784\u548c\u5b9e\u73b0\u9009\u62e9\u7684\u6709\u6548\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u6570\u5b66\u63a8\u7406\u3001ClozeMath\u3001\u6587\u672c\u586b\u5145\u4efb\u52a1\u3001\u6027\u80fd\u63d0\u5347"}}
{"id": "2506.03224", "pdf": "https://arxiv.org/pdf/2506.03224", "abs": "https://arxiv.org/abs/2506.03224", "authors": ["Jinwei Zeng", "Yu Liu", "Guozhen Zhang", "Jingtao Ding", "Yuming Lin", "Jian Yuan", "Yong Li"], "title": "OpenCarbon: A Contrastive Learning-based Cross-Modality Neural Approach for High-Resolution Carbon Emission Prediction Using Open Data", "categories": ["cs.CV", "cs.AI", "physics.soc-ph"], "comment": "Accepted by IJCAI 2025", "summary": "Accurately estimating high-resolution carbon emissions is crucial for\neffective emission governance and mitigation planning. While conventional\nmethods for precise carbon accounting are hindered by substantial data\ncollection efforts, the rise of open data and advanced learning techniques\noffers a promising solution. Once an open data-based prediction model is\ndeveloped and trained, it can easily infer emissions for new areas based on\navailable open data. To address this, we incorporate two modalities of open\ndata, satellite images and point-of-interest (POI) data, to predict\nhigh-resolution urban carbon emissions, with satellite images providing\nmacroscopic and static and POI data offering fine-grained and relatively\ndynamic functionality information. However, estimating high-resolution carbon\nemissions presents two significant challenges: the intertwined and implicit\neffects of various functionalities on carbon emissions, and the complex spatial\ncontiguity correlations that give rise to the agglomeration effect. Our model,\nOpenCarbon, features two major designs that target the challenges: a\ncross-modality information extraction and fusion module to extract\ncomplementary functionality information from two modules and model their\ninteractions, and a neighborhood-informed aggregation module to capture the\nspatial contiguity correlations. Extensive experiments demonstrate our model's\nsuperiority, with a significant performance gain of 26.6\\% on R2. Further\ngeneralizability tests and case studies also show OpenCarbon's capacity to\ncapture the intrinsic relation between urban functionalities and carbon\nemissions, validating its potential to empower efficient carbon governance and\ntargeted carbon mitigation planning. Codes and data are available:\nhttps://github.com/JinweiZzz/OpenCarbon.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u536b\u661f\u56fe\u50cf\u548cPOI\u6570\u636e\u7684OpenCarbon\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u57ce\u5e02\u78b3\u6392\u653e\u9884\u6d4b\uff0c\u6027\u80fd\u663e\u8457\u63d0\u534726.6%\u7684R2\u503c\u3002", "motivation": "\u4f20\u7edf\u78b3\u6392\u653e\u6838\u7b97\u65b9\u6cd5\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\uff0c\u7ed3\u5408\u5f00\u653e\u6570\u636e\u548c\u5148\u8fdb\u5b66\u4e60\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u536b\u661f\u56fe\u50cf\u548cPOI\u6570\u636e\uff0c\u8bbe\u8ba1\u8de8\u6a21\u6001\u4fe1\u606f\u63d0\u53d6\u4e0e\u878d\u5408\u6a21\u5757\u53ca\u90bb\u57df\u4fe1\u606f\u805a\u5408\u6a21\u5757\uff0c\u89e3\u51b3\u529f\u80fd\u5f71\u54cd\u590d\u6742\u548c\u7a7a\u95f4\u8fde\u7eed\u6027\u95ee\u9898\u3002", "result": "\u6a21\u578b\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cR2\u63d0\u534726.6%\uff0c\u9a8c\u8bc1\u4e86\u5176\u6355\u6349\u57ce\u5e02\u529f\u80fd\u4e0e\u78b3\u6392\u653e\u5173\u7cfb\u7684\u80fd\u529b\u3002", "conclusion": "OpenCarbon\u6a21\u578b\u4e3a\u9ad8\u6548\u78b3\u6392\u653e\u6cbb\u7406\u548c\u9488\u5bf9\u6027\u51cf\u6392\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "keywords": "\u78b3\u6392\u653e\u9884\u6d4b,\u536b\u661f\u56fe\u50cf,POI\u6570\u636e,\u8de8\u6a21\u6001\u5b66\u4e60,\u7a7a\u95f4\u8fde\u7eed\u6027"}}
{"id": "2506.03777", "pdf": "https://arxiv.org/pdf/2506.03777", "abs": "https://arxiv.org/abs/2506.03777", "authors": ["Li Zhang", "Zhongxuan Han", "Chaochao chen", "Xiaohua Feng", "Jiaming Zhang", "Yuyuan Li"], "title": "FedFACT: A Provable Framework for Controllable Group-Fairness Calibration in Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "With emerging application of Federated Learning (FL) in decision-making\nscenarios, it is imperative to regulate model fairness to prevent disparities\nacross sensitive groups (e.g., female, male). Current research predominantly\nfocuses on two concepts of group fairness within FL: Global Fairness (overall\nmodel disparity across all clients) and Local Fairness (the disparity within\neach client). However, the non-decomposable, non-differentiable nature of\nfairness criteria pose two fundamental, unresolved challenges for fair FL: (i)\nHarmonizing global and local fairness in multi-class classification; (ii)\nEnabling a controllable, optimal accuracy-fairness trade-off. To tackle the\naforementioned challenges, we propose a novel controllable federated\ngroup-fairness calibration framework, named FedFACT. FedFACT identifies the\nBayes-optimal classifiers under both global and local fairness constraints in\nmulti-class case, yielding models with minimal performance decline while\nguaranteeing fairness. To effectively realize an adjustable, optimal\naccuracy-fairness balance, we derive specific characterizations of the\nBayes-optimal fair classifiers for reformulating fair FL as personalized\ncost-sensitive learning problem for in-processing, and bi-level optimization\nfor post-processing. Theoretically, we provide convergence and generalization\nguarantees for FedFACT to approach the near-optimal accuracy under given\nfairness levels. Extensive experiments on multiple datasets across various data\nheterogeneity demonstrate that FedFACT consistently outperforms baselines in\nbalancing accuracy and global-local fairness.", "AI": {"tldr": "FedFACT\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u63a7\u7684\u8054\u90a6\u5b66\u4e60\u516c\u5e73\u6821\u51c6\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u591a\u7c7b\u5206\u7c7b\u4e2d\u5168\u5c40\u4e0e\u5c40\u90e8\u516c\u5e73\u7684\u534f\u8c03\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u968f\u7740\u8054\u90a6\u5b66\u4e60\u5728\u51b3\u7b56\u573a\u666f\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u786e\u4fdd\u6a21\u578b\u516c\u5e73\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5168\u5c40\u548c\u5c40\u90e8\u516c\u5e73\u6027\uff0c\u4f46\u5176\u975e\u53ef\u5206\u89e3\u3001\u4e0d\u53ef\u5fae\u5206\u7684\u7279\u6027\u5e26\u6765\u4e86\u534f\u8c03\u548c\u4f18\u5316\u96be\u9898\u3002", "method": "\u63d0\u51fa\u4e86FedFACT\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u6700\u4f18\u5206\u7c7b\u5668\u5728\u591a\u7c7b\u60c5\u51b5\u4e0b\u6ee1\u8db3\u5168\u5c40\u548c\u5c40\u90e8\u516c\u5e73\u7ea6\u675f\uff0c\u5e76\u91cd\u65b0\u5c06\u516c\u5e73\u8054\u90a6\u5b66\u4e60\u95ee\u9898\u8868\u8ff0\u4e3a\u4e2a\u6027\u5316\u6210\u672c\u654f\u611f\u5b66\u4e60\u95ee\u9898\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eFedFACT\u5728\u7ed9\u5b9a\u516c\u5e73\u6c34\u5e73\u4e0b\u63a5\u8fd1\u6700\u4f18\u7cbe\u5ea6\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FedFACT\u4e3a\u8054\u90a6\u5b66\u4e60\u4e2d\u516c\u5e73\u6027\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60,\u516c\u5e73\u6027,\u591a\u7c7b\u5206\u7c7b,\u8d1d\u53f6\u65af\u6700\u4f18,\u6210\u672c\u654f\u611f\u5b66\u4e60"}}
{"id": "2506.03781", "pdf": "https://arxiv.org/pdf/2506.03781", "abs": "https://arxiv.org/abs/2506.03781", "authors": ["Seungcheol Park", "Jeongin Bae", "Beomseok Kwon", "Minjun Kim", "Byeongwook Kim", "Se Jung Kwon", "U Kang", "Dongsoo Lee"], "title": "Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "ACL 2025 Main Track", "summary": "How can we quantize large language models while preserving accuracy?\nQuantization is essential for deploying large language models (LLMs)\nefficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are\npromising quantization schemes that have strong expressiveness and\noptimizability, respectively. However, neither scheme leverages both\nadvantages. In this paper, we propose UniQuanF (Unified Quantization with\nFlexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses\nboth strong expressiveness and optimizability by unifying the flexible mapping\ntechnique in UQ and non-uniform quantization levels of BCQ. We propose unified\ninitialization, and local and periodic mapping techniques to optimize the\nparameters in UniQuanF precisely. After optimization, our unification theorem\nremoves computational and memory overhead, allowing us to utilize the superior\naccuracy of UniQuanF without extra deployment costs induced by the unification.\nExperimental results demonstrate that UniQuanF outperforms existing UQ and BCQ\nmethods, achieving up to 4.60% higher accuracy on GSM8K benchmark.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUniQuanF\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5747\u5300\u91cf\u5316\uff08UQ\uff09\u548c\u975e\u5747\u5300\u91cf\u5316\uff08BCQ\uff09\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u7075\u6d3b\u6620\u5c04\u6280\u672f\u548c\u7edf\u4e00\u521d\u59cb\u5316\u3001\u5c40\u90e8\u4e0e\u5468\u671f\u6027\u6620\u5c04\u6280\u672f\u4f18\u5316\u53c2\u6570\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u907f\u514d\u4e86\u989d\u5916\u7684\u90e8\u7f72\u5f00\u9500\u3002\u5b9e\u9a8c\u8868\u660e\uff0cUniQuanF\u5728GSM8K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u9ad8\u51fa4.60%\u3002", "motivation": "\u91cf\u5316\u662f\u9ad8\u6548\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u73b0\u6709\u7684\u4e8c\u5143\u7f16\u7801\u91cf\u5316\uff08BCQ\uff09\u548c\u5747\u5300\u91cf\u5316\uff08UQ\uff09\u65b9\u6848\u65e0\u6cd5\u540c\u65f6\u5229\u7528\u8868\u8fbe\u80fd\u529b\u5f3a\u548c\u6613\u4e8e\u4f18\u5316\u7684\u4f18\u52bf\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86UniQuanF\uff0c\u4ee5\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u70b9\u63d0\u5347\u91cf\u5316\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86UniQuanF\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00UQ\u7684\u7075\u6d3b\u6620\u5c04\u548cBCQ\u7684\u975e\u5747\u5300\u91cf\u5316\u7ea7\u522b\uff0c\u7ed3\u5408\u7edf\u4e00\u521d\u59cb\u5316\u3001\u5c40\u90e8\u6620\u5c04\u548c\u5468\u671f\u6027\u6620\u5c04\u6280\u672f\u4f18\u5316\u53c2\u6570\uff0c\u540c\u65f6\u5229\u7528\u7edf\u4e00\u5316\u5b9a\u7406\u6d88\u9664\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cUniQuanF\u5728GSM8K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u5176\u4ed6\u65b9\u6cd5\u51c6\u786e\u7387\u9ad84.60%\u3002", "conclusion": "UniQuanF\u7ed3\u5408\u4e86UQ\u548cBCQ\u7684\u4f18\u70b9\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u91cf\u5316\u4e14\u65e0\u9700\u989d\u5916\u90e8\u7f72\u6210\u672c\uff0c\u4e3aLLM\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u91cf\u5316\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u4e8c\u5143\u7f16\u7801\u91cf\u5316\u3001\u5747\u5300\u91cf\u5316\u3001UniQuanF"}}
{"id": "2506.03784", "pdf": "https://arxiv.org/pdf/2506.03784", "abs": "https://arxiv.org/abs/2506.03784", "authors": ["Beatrix M. G. Nielsen", "Emanuele Marconato", "Andrea Dittadi", "Luigi Gresele"], "title": "When Does Closeness in Distribution Imply Representational Similarity? An Identifiability Perspective", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "When and why representations learned by different deep neural networks are\nsimilar is an active research topic. We choose to address these questions from\nthe perspective of identifiability theory, which suggests that a measure of\nrepresentational similarity should be invariant to transformations that leave\nthe model distribution unchanged. Focusing on a model family which includes\nseveral popular pre-training approaches, e.g., autoregressive language models,\nwe explore when models which generate distributions that are close have similar\nrepresentations. We prove that a small Kullback-Leibler divergence between the\nmodel distributions does not guarantee that the corresponding representations\nare similar. This has the important corollary that models arbitrarily close to\nmaximizing the likelihood can still learn dissimilar representations, a\nphenomenon mirrored in our empirical observations on models trained on\nCIFAR-10. We then define a distributional distance for which closeness implies\nrepresentational similarity, and in synthetic experiments, we find that wider\nnetworks learn distributions which are closer with respect to our distance and\nhave more similar representations. Our results establish a link between\ncloseness in distribution and representational similarity.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5230\u7684\u8868\u793a\u4e3a\u4f55\u53ca\u4f55\u65f6\u76f8\u4f3c\uff0c\u4ece\u53ef\u8bc6\u522b\u6027\u7406\u8bba\u51fa\u53d1\uff0c\u53d1\u73b0\u5c0f\u7684KL\u6563\u5ea6\u4e0d\u4fdd\u8bc1\u8868\u793a\u76f8\u4f3c\uff0c\u5b9a\u4e49\u4e86\u53ef\u786e\u4fdd\u8868\u793a\u76f8\u4f3c\u6027\u7684\u5206\u5e03\u8ddd\u79bb\u3002", "motivation": "\u63a2\u7d22\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u76f8\u4f3c\u6027\u7684\u539f\u56e0\u548c\u6761\u4ef6\uff0c\u4ece\u7406\u8bba\u5c42\u9762\u89e3\u91ca\u6a21\u578b\u5206\u5e03\u7684\u63a5\u8fd1\u4e0e\u8868\u793a\u76f8\u4f3c\u6027\u7684\u5173\u7cfb\u3002", "method": "\u57fa\u4e8e\u53ef\u8bc6\u522b\u6027\u7406\u8bba\uff0c\u5206\u6790\u6a21\u578b\u5206\u5e03\u95f4\u7684KL\u6563\u5ea6\u548c\u8868\u793a\u7684\u76f8\u4f3c\u6027\uff0c\u5e76\u901a\u8fc7CIFAR-10\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u3002", "result": "\u8bc1\u660e\u5c0f\u7684KL\u6563\u5ea6\u4e0d\u4fdd\u8bc1\u8868\u793a\u76f8\u4f3c\u6027\uff0c\u5b9a\u4e49\u4e86\u65b0\u7684\u5206\u5e03\u8ddd\u79bb\uff0c\u53d1\u73b0\u66f4\u5bbd\u7684\u7f51\u7edc\u7684\u8868\u793a\u66f4\u76f8\u4f3c\u3002", "conclusion": "\u6a21\u578b\u5206\u5e03\u63a5\u8fd1\u6027\u4e0e\u8868\u793a\u76f8\u4f3c\u6027\u4e4b\u95f4\u5b58\u5728\u8054\u7cfb\uff0c\u4f46\u4f20\u7edfKL\u6563\u5ea6\u4e0d\u8db3\u4ee5\u8861\u91cf\u8fd9\u79cd\u5173\u7cfb\u3002", "keywords": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc,\u8868\u793a\u76f8\u4f3c\u6027,\u53ef\u8bc6\u522b\u6027\u7406\u8bba,KL\u6563\u5ea6"}}
{"id": "2506.03785", "pdf": "https://arxiv.org/pdf/2506.03785", "abs": "https://arxiv.org/abs/2506.03785", "authors": ["Isik Baran Sandan", "Tu Anh Dinh", "Jan Niehues"], "title": "Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "4 pages, 2 figures", "summary": "Large Language Models (LLMs) have shown to be effective evaluators across\nvarious domains such as machine translations or the scientific domain. Current\nLLM-as-a-Judge approaches rely mostly on individual assessments or a single\nround of pairwise assessments, preventing the judge LLM from developing a\nglobal ranking perspective. To address this, we present Knockout Assessment, an\nLLM-asa Judge method using a knockout tournament system with iterative pairwise\ncomparisons. Experiments across three LLMs on two datasets show that knockout\nassessment improves scoring accuracy, increasing Pearson correlation with\nexpert evaluations by 0.07 on average for university-level exam scoring and\nmachine translation evaluations, aligning LLM assessments more closely with\nhuman scoring.", "AI": {"tldr": "Knockout Assessment \u662f\u4e00\u79cd\u901a\u8fc7\u6dd8\u6c70\u8d5b\u7cfb\u7edf\u8fdb\u884c\u8fed\u4ee3\u4e24\u4e24\u6bd4\u8f83\u7684 LLM-as-a-Judge \u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u8bc4\u4f30\u51c6\u786e\u6027\uff0c\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u7684 Pearson \u76f8\u5173\u6027\u5e73\u5747\u63d0\u9ad8 0.07\u3002", "motivation": "\u5f53\u524d LLM \u4f5c\u4e3a\u8bc4\u4f30\u8005\u901a\u5e38\u4f9d\u8d56\u5355\u4e2a\u8bc4\u4f30\u6216\u5355\u8f6e\u4e24\u4e24\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u5168\u5c40\u6392\u540d\u89c6\u89d2\uff0cKnockout Assessment \u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6dd8\u6c70\u8d5b\u7cfb\u7edf\u8fdb\u884c\u8fed\u4ee3\u4e24\u4e24\u6bd4\u8f83\uff0c\u6539\u8fdb LLM \u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728\u4e09\u4e2a LLM \u548c\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cKnockout Assessment \u63d0\u9ad8\u4e86\u8bc4\u5206\u51c6\u786e\u6027\uff0cPearson \u76f8\u5173\u6027\u5e73\u5747\u63d0\u9ad8 0.07\u3002", "conclusion": "Knockout Assessment \u901a\u8fc7\u6dd8\u6c70\u8d5b\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86 LLM \u8bc4\u4f30\u4e0e\u4eba\u7c7b\u8bc4\u5206\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "keywords": "LLM, \u8bc4\u4f30, \u6dd8\u6c70\u8d5b, \u4e24\u4e24\u6bd4\u8f83, Pearson \u76f8\u5173\u6027"}}
{"id": "2506.03790", "pdf": "https://arxiv.org/pdf/2506.03790", "abs": "https://arxiv.org/abs/2506.03790", "authors": ["Peng Wang", "Yifu Lu", "Yaodong Yu", "Druv Pai", "Qing Qu", "Yi Ma"], "title": "Attention-Only Transformers via Unrolled Subspace Denoising", "categories": ["cs.LG"], "comment": "28 pages, 7 figures, 5 tables", "summary": "Despite the popularity of transformers in practice, their architectures are\nempirically designed and neither mathematically justified nor interpretable.\nMoreover, as indicated by many empirical studies, some components of\ntransformer architectures may be redundant. To derive a fully interpretable\ntransformer architecture with only necessary components, we contend that the\ngoal of representation learning is to compress a set of noisy initial token\nrepresentations towards a mixture of low-dimensional subspaces. To compress\nthese noisy token representations, an associated denoising operation naturally\ntakes the form of a multi-head (subspace) self-attention. By unrolling such\niterative denoising operations into a deep network, we arrive at a highly\ncompact architecture that consists of \\textit{only} self-attention operators\nwith skip connections at each layer. Moreover, we show that each layer performs\nhighly efficient denoising: it improves the signal-to-noise ratio of token\nrepresentations \\textit{at a linear rate} with respect to the number of layers.\nDespite its simplicity, extensive experiments on vision and language tasks\ndemonstrate that such a transformer achieves performance close to that of\nstandard transformer architectures such as GPT-2 and CRATE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u53ef\u89e3\u91ca\u7684Transformer\u67b6\u6784\uff0c\u4ec5\u4fdd\u7559\u5fc5\u8981\u7684\u81ea\u6ce8\u610f\u529b\u64cd\u4f5c\u548c\u8df3\u8dc3\u8fde\u63a5\uff0c\u901a\u8fc7\u89e3\u6784\u566a\u97f3\u538b\u7f29\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u4e0e\u6807\u51c6Transformer\uff08\u5982GPT-2\u548cCRATE\uff09\u63a5\u8fd1\u7684\u6027\u80fd\u3002", "motivation": "Transformer\u67b6\u6784\u867d\u6d41\u884c\u4f46\u7f3a\u4e4f\u6570\u5b66\u89e3\u91ca\u4e14\u53ef\u80fd\u5197\u4f59\uff0c\u5e0c\u671b\u901a\u8fc7\u8868\u793a\u5b66\u4e60\u538b\u7f29\u566a\u97f3\u5e76\u4ec5\u4fdd\u7559\u5fc5\u8981\u7ec4\u4ef6\u3002", "method": "\u901a\u8fc7\u591a\u5934\u90e8\u81ea\u6ce8\u610f\u529b\u8fdb\u884c\u8fed\u4ee3\u53bb\u566a\u64cd\u4f5c\uff0c\u6784\u5efa\u7d27\u51d1\u7f51\u7edc\u67b6\u6784\uff0c\u4ec5\u5305\u542b\u81ea\u6ce8\u610f\u529b\u64cd\u4f5c\u548c\u8df3\u8dc3\u8fde\u63a5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u67b6\u6784\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u63a5\u8fd1\u6807\u51c6Transformer\uff0c\u540c\u65f6\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u3002", "conclusion": "\u7b80\u5316\u4e14\u53ef\u89e3\u91ca\u7684Transformer\u67b6\u6784\u8bc1\u660e\u4e86\u81ea\u6ce8\u610f\u529b\u64cd\u4f5c\u5728\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u9ad8\u6548\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5197\u4f59\u7ec4\u4ef6\u3002", "keywords": "Transformer, \u53ef\u89e3\u91ca\u6027, \u81ea\u6ce8\u610f\u529b, \u53bb\u566a, \u8868\u793a\u5b66\u4e60"}}
{"id": "2506.03793", "pdf": "https://arxiv.org/pdf/2506.03793", "abs": "https://arxiv.org/abs/2506.03793", "authors": ["Sidharth Pulipaka", "Sparsh Jain", "Ashwin Sankar", "Raj Dabre"], "title": "Mark My Words: A Robust Multilingual Model for Punctuation in Text and Speech Transcripts", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "Punctuation plays a vital role in structuring meaning, yet current models\noften struggle to restore it accurately in transcripts of spontaneous speech,\nespecially in the presence of disfluencies such as false starts and\nbacktracking. These limitations hinder the performance of downstream tasks like\ntranslation, text to speech, summarization, etc. where sentence boundaries are\ncritical for preserving quality. In this work, we introduce Cadence, a\ngeneralist punctuation restoration model adapted from a pretrained large\nlanguage model. Cadence is designed to handle both clean written text and\nhighly spontaneous spoken transcripts. It surpasses the previous state of the\nart in performance while expanding support from 14 to all 22 Indian languages\nand English. We conduct a comprehensive analysis of model behavior across\npunctuation types and language families, identifying persistent challenges\nunder domain shift and with rare punctuation marks. Our findings demonstrate\nthe efficacy of utilizing pretrained language models for multilingual\npunctuation restoration and highlight Cadence practical value for low resource\nNLP pipelines at scale.", "AI": {"tldr": "Cadence \u662f\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u6807\u70b9\u6062\u590d\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8bed\u8a00\u548c\u6587\u672c\u7c7b\u578b\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u6a21\u578b\u5728\u6062\u590d\u81ea\u53d1\u8bed\u97f3\u8f6c\u5f55\u4e2d\u7684\u6807\u70b9\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5f71\u54cd\u4e86\u7ffb\u8bd1\u3001\u6587\u672c\u8f6c\u8bed\u97f3\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u8d28\u91cf\u3002", "method": "\u5f15\u5165 Cadence\uff0c\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u6807\u70b9\u6062\u590d\u6a21\u578b\uff0c\u652f\u6301\u591a\u79cd\u8bed\u8a00\u548c\u6587\u672c\u7c7b\u578b\u3002", "result": "Cadence \u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u5e76\u6269\u5c55\u652f\u6301\u5230 22 \u79cd\u5370\u5ea6\u8bed\u8a00\u548c\u82f1\u8bed\u3002\u5206\u6790\u63ed\u793a\u4e86\u6a21\u578b\u5728\u9886\u57df\u8fc1\u79fb\u548c\u7f55\u89c1\u6807\u70b9\u65b9\u9762\u7684\u6311\u6218\u3002", "conclusion": "Cadence \u5c55\u793a\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u79cd\u6807\u70b9\u6062\u590d\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5bf9\u4f4e\u8d44\u6e90 NLP \u7ba1\u9053\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002", "keywords": "\u6807\u70b9\u6062\u590d\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u591a\u8bed\u8a00\u5904\u7406\uff0c\u4f4e\u8d44\u6e90 NLP"}}
{"id": "2506.03229", "pdf": "https://arxiv.org/pdf/2506.03229", "abs": "https://arxiv.org/abs/2506.03229", "authors": ["Qian-Wei Wang", "Yuqiu Xie", "Letian Zhang", "Zimo Liu", "Shu-Tao Xia"], "title": "Pre-trained Vision-Language Models Assisted Noisy Partial Label Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In the context of noisy partial label learning (NPLL), each training sample\nis associated with a set of candidate labels annotated by multiple noisy\nannotators. With the emergence of high-performance pre-trained vision-language\nmodels (VLMs) such as CLIP, LLaVa and GPT-4V, the direction of using these\nmodels to replace time-consuming manual annotation workflows and achieve\n\"manual-annotation-free\" training for downstream tasks has become a highly\npromising research avenue. This paper focuses on learning from noisy partial\nlabels annotated by pre-trained VLMs and proposes an innovative collaborative\nconsistency regularization (Co-Reg) method. Unlike the symmetric noise\nprimarily addressed in traditional noisy label learning, the noise generated by\npre-trained models is instance-dependent, embodying the underlying patterns of\nthe pre-trained models themselves, which significantly increases the learning\ndifficulty for the model. To address this, we simultaneously train two neural\nnetworks that implement collaborative purification of training labels through a\n\"Co-Pseudo-Labeling\" mechanism, while enforcing consistency regularization\nconstraints in both the label space and feature representation space. Our\nmethod can also leverage few-shot manually annotated valid labels to further\nenhance its performances. Comparative experiments with different denoising and\ndisambiguation algorithms, annotation manners, and pre-trained model\napplication schemes fully validate the effectiveness of the proposed method,\nwhile revealing the broad prospects of integrating weakly-supervised learning\ntechniques into the knowledge distillation process of pre-trained models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u534f\u540c\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff08Co-Reg\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u751f\u6210\u7684\u90e8\u5206\u566a\u58f0\u6807\u7b7e\u4e2d\u5b66\u4e60\uff0c\u901a\u8fc7\u534f\u540c\u4f2a\u6807\u7b7e\u673a\u5236\u548c\u4e00\u81f4\u6027\u6b63\u5219\u5316\u7ea6\u675f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\u3001LLaVa\u548cGPT-4V\uff09\u80fd\u66ff\u4ee3\u8017\u65f6\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u4f46\u5176\u751f\u6210\u7684\u566a\u58f0\u6807\u7b7e\u662f\u5b9e\u4f8b\u4f9d\u8d56\u7684\uff0c\u589e\u52a0\u4e86\u5b66\u4e60\u96be\u5ea6\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5e76\u63a2\u7d22\u65e0\u4eba\u5de5\u6807\u6ce8\u7684\u4e0b\u6e38\u4efb\u52a1\u8bad\u7ec3\u3002", "method": "\u901a\u8fc7\u540c\u65f6\u8bad\u7ec3\u4e24\u4e2a\u795e\u7ecf\u7f51\u7edc\uff0c\u5229\u7528\u534f\u540c\u4f2a\u6807\u7b7e\u673a\u5236\u51c0\u5316\u8bad\u7ec3\u6807\u7b7e\uff0c\u5e76\u5728\u6807\u7b7e\u7a7a\u95f4\u548c\u7279\u5f81\u8868\u793a\u7a7a\u95f4\u65bd\u52a0\u4e00\u81f4\u6027\u6b63\u5219\u5316\u7ea6\u675f\u3002\u8fd8\u53ef\u7ed3\u5408\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u591a\u79cd\u53bb\u566a\u548c\u6d88\u6b67\u7b97\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6cdb\u5316\u6027\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u63ed\u793a\u4e86\u5f31\u76d1\u7763\u5b66\u4e60\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u7ed3\u5408\u7684\u5e7f\u9614\u524d\u666f\u3002", "conclusion": "Co-Reg\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u9884\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u7684\u5b9e\u4f8b\u4f9d\u8d56\u566a\u58f0\u6807\u7b7e\uff0c\u4e3a\u65e0\u4eba\u5de5\u6807\u6ce8\u7684\u8bad\u7ec3\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5608\u6742\u90e8\u5206\u6807\u7b7e\u5b66\u4e60\uff08NPLL\uff09\u3001\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u3001\u534f\u540c\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff08Co-Reg\uff09\u3001\u5f31\u76d1\u7763\u5b66\u4e60\u3001\u77e5\u8bc6\u84b8\u998f"}}
{"id": "2506.03802", "pdf": "https://arxiv.org/pdf/2506.03802", "abs": "https://arxiv.org/abs/2506.03802", "authors": ["Andreas Athanasopoulos", "Christos Dimitrakakis"], "title": "Learning Equilibria in Matching Games with Bandit Feedback", "categories": ["cs.LG"], "comment": "21 pages, 2 figures", "summary": "We investigate the problem of learning an equilibrium in a generalized\ntwo-sided matching market, where agents can adaptively choose their actions\nbased on their assigned matches. Specifically, we consider a setting in which\nmatched agents engage in a zero-sum game with initially unknown payoff\nmatrices, and we explore whether a centralized procedure can learn an\nequilibrium from bandit feedback. We adopt the solution concept of matching\nequilibrium, where a pair consisting of a matching $\\mathfrak{m}$ and a set of\nagent strategies $X$ forms an equilibrium if no agent has the incentive to\ndeviate from $(\\mathfrak{m}, X)$. To measure the deviation of a given pair\n$(\\mathfrak{m}, X)$ from the equilibrium pair $(\\mathfrak{m}^\\star, X^\\star)$,\nwe introduce matching instability that can serve as a regret measure for the\ncorresponding learning problem. We then propose a UCB algorithm in which agents\nform preferences and select actions based on optimistic estimates of the game\npayoffs, and prove that it achieves sublinear, instance-independent regret over\na time horizon $T$.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u5e7f\u4e49\u53cc\u5411\u5339\u914d\u5e02\u573a\u4e2d\u5b66\u4e60\u5747\u8861\u7684\u95ee\u9898\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u4e2d\u5fc3\u5316\u7a0b\u5e8f\u662f\u5426\u80fd\u901a\u8fc7\u53cd\u9988\u5b66\u4e60\u5747\u8861\u3002", "motivation": "\u63a2\u7d22\u4ee3\u7406\u5982\u4f55\u5728\u672a\u77e5\u6536\u76ca\u77e9\u9635\u7684\u96f6\u548c\u535a\u5f08\u4e2d\u81ea\u9002\u5e94\u9009\u62e9\u884c\u52a8\u4ee5\u5b9e\u73b0\u5747\u8861\u3002", "method": "\u91c7\u7528\u5339\u914d\u5747\u8861\u7684\u6982\u5ff5\uff0c\u63d0\u51faUCB\u7b97\u6cd5\uff0c\u4ee3\u7406\u57fa\u4e8e\u4e50\u89c2\u4f30\u8ba1\u9009\u62e9\u884c\u52a8\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e86\u65f6\u95f4\u8303\u56f4\u5185\u7684\u6b21\u7ebf\u6027\u3001\u4e0e\u5b9e\u4f8b\u65e0\u5173\u7684\u540e\u6094\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5b66\u4e60\u5747\u8861\u95ee\u9898\uff0c\u5e76\u5177\u6709\u7406\u8bba\u4fdd\u969c\u3002", "keywords": "\u53cc\u5411\u5339\u914d\u5e02\u573a, \u5747\u8861\u5b66\u4e60, UCB\u7b97\u6cd5, \u96f6\u548c\u535a\u5f08"}}
{"id": "2506.03820", "pdf": "https://arxiv.org/pdf/2506.03820", "abs": "https://arxiv.org/abs/2506.03820", "authors": ["Ahmad Mustapha Wali", "Sergiu Nisioi"], "title": "Automatic Correction of Writing Anomalies in Hausa Texts", "categories": ["cs.CL"], "comment": null, "summary": "Hausa texts are often characterized by writing anomalies such as incorrect\ncharacter substitutions and spacing errors, which sometimes hinder natural\nlanguage processing (NLP) applications. This paper presents an approach to\nautomatically correct the anomalies by finetuning transformer-based models.\nUsing a corpus gathered from several public sources, we created a large-scale\nparallel dataset of over 450,000 noisy-clean Hausa sentence pairs by\nintroducing synthetically generated noise, fine-tuned to mimic realistic\nwriting errors. Moreover, we adapted several multilingual and African\nlanguage-focused models, including M2M100, AfriTEVA, mBART, and Opus-MT\nvariants for this correction task using SentencePiece tokenization. Our\nexperimental results demonstrate significant increases in F1, BLEU and METEOR\nscores, as well as reductions in Character Error Rate (CER) and Word Error Rate\n(WER). This research provides a robust methodology, a publicly available\ndataset, and effective models to improve Hausa text quality, thereby advancing\nNLP capabilities for the language and offering transferable insights for other\nlow-resource languages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5fae\u8c03\u57fa\u4e8etransformer\u7684\u6a21\u578b\u6765\u81ea\u52a8\u4fee\u6b63\u8c6a\u8428\u8bed\u6587\u672c\u4e2d\u7684\u4e66\u5199\u5f02\u5e38\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86NLP\u5e94\u7528\u7684\u6548\u679c\u3002", "motivation": "\u8c6a\u8428\u8bed\u6587\u672c\u4e2d\u5b58\u5728\u4e66\u5199\u5f02\u5e38\uff08\u5982\u5b57\u7b26\u66ff\u6362\u9519\u8bef\u548c\u7a7a\u683c\u9519\u8bef\uff09\uff0c\u963b\u788d\u4e86NLP\u5e94\u7528\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5408\u6210\u566a\u58f0\u521b\u5efa\u5927\u89c4\u6a21\u5e73\u884c\u6570\u636e\u96c6\uff0c\u5fae\u8c03\u591a\u8bed\u8a00\u6a21\u578b\uff08\u5982M2M100\u3001AfriTEVA\u7b49\uff09\uff0c\u4f7f\u7528SentencePiece\u5206\u8bcd\u8fdb\u884c\u4fee\u6b63\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aF1\u3001BLEU\u548cMETEOR\u5206\u6570\u663e\u8457\u63d0\u5347\uff0c\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09\u548c\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u660e\u663e\u964d\u4f4e\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u5957\u5065\u58ee\u7684\u65b9\u6cd5\u3001\u516c\u5f00\u6570\u636e\u96c6\u548c\u6709\u6548\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u8c6a\u8428\u8bed\u6587\u672c\u8d28\u91cf\uff0c\u5e76\u4e3a\u5176\u4ed6\u4f4e\u8d44\u6e90\u8bed\u8a00\u63d0\u4f9b\u4e86\u53ef\u8fc1\u79fb\u7684\u89c1\u89e3\u3002", "keywords": "\u8c6a\u8428\u8bed, \u6587\u672c\u4fee\u6b63, transformer\u6a21\u578b, \u81ea\u7136\u8bed\u8a00\u5904\u7406, \u4f4e\u8d44\u6e90\u8bed\u8a00"}}
{"id": "2506.03813", "pdf": "https://arxiv.org/pdf/2506.03813", "abs": "https://arxiv.org/abs/2506.03813", "authors": ["Lili Chen", "Changyang She", "Jingge Zhu", "Jamie Evans"], "title": "Graph Neural Networks for Resource Allocation in Multi-Channel Wireless Networks", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "As the number of mobile devices continues to grow, interference has become a\nmajor bottleneck in improving data rates in wireless networks. Efficient joint\nchannel and power allocation (JCPA) is crucial for managing interference. In\nthis paper, we first propose an enhanced WMMSE (eWMMSE) algorithm to solve the\nJCPA problem in multi-channel wireless networks. To reduce the computational\ncomplexity of iterative optimization, we further introduce JCPGNN-M, a graph\nneural network-based solution that enables simultaneous multi-channel\nallocation for each user. We reformulate the problem as a Lagrangian function,\nwhich allows us to enforce the total power constraints systematically. Our\nsolution involves combining this Lagrangian framework with GNNs and iteratively\nupdating the Lagrange multipliers and resource allocation scheme. Unlike\nexisting GNN-based methods that limit each user to a single channel, JCPGNN-M\nsupports efficient spectrum reuse and scales well in dense network scenarios.\nSimulation results show that JCPGNN-M achieves better data rate compared to\neWMMSE. Meanwhile, the inference time of JCPGNN-M is much lower than eWMMS, and\nit can generalize well to larger networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578bWMMSE\u7b97\u6cd5\uff08eWMMSE\uff09\u548c\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684JCPGNN-M\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u4fe1\u9053\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u8054\u5408\u4fe1\u9053\u548c\u529f\u7387\u5206\u914d\uff08JCPA\uff09\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u63d0\u5347\u6570\u636e\u4f20\u8f93\u901f\u7387\u3002\u7ed3\u679c\u8868\u660e\uff0cJCPGNN-M\u5728\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8eeWMMSE\u3002", "motivation": "\u968f\u7740\u79fb\u52a8\u8bbe\u5907\u7684\u589e\u52a0\uff0c\u5e72\u6270\u6210\u4e3a\u65e0\u7ebf\u7f51\u7edc\u63d0\u5347\u6570\u636e\u4f20\u8f93\u901f\u7387\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u7684\u8054\u5408\u4fe1\u9053\u548c\u529f\u7387\u5206\u914d\uff08JCPA\uff09\u65b9\u6848\u3002", "method": "1. \u63d0\u51faeWMMSE\u7b97\u6cd5\u89e3\u51b3JCPA\u95ee\u9898\uff1b2. \u5f15\u5165\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684JCPGNN-M\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u4fe1\u9053\u5206\u914d\uff1b3. \u901a\u8fc7\u62c9\u683c\u6717\u65e5\u6846\u67b6\u4e0eGNN\u7ed3\u5408\uff0c\u8fed\u4ee3\u66f4\u65b0\u8d44\u6e90\u5206\u914d\u65b9\u6848\u3002", "result": "JCPGNN-M\u5728\u6570\u636e\u901f\u7387\u4e0a\u4f18\u4e8eeWMMSE\uff0c\u63a8\u7406\u65f6\u95f4\u66f4\u77ed\uff0c\u4e14\u5728\u5bc6\u96c6\u7f51\u7edc\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "JCPGNN-M\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5bc6\u96c6\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\u3002", "keywords": "\u8054\u5408\u4fe1\u9053\u548c\u529f\u7387\u5206\u914d, \u56fe\u795e\u7ecf\u7f51\u7edc, \u5e72\u6270\u7ba1\u7406, \u65e0\u7ebf\u7f51\u7edc"}}
{"id": "2506.03822", "pdf": "https://arxiv.org/pdf/2506.03822", "abs": "https://arxiv.org/abs/2506.03822", "authors": ["Fabian Karl", "Ansgar Scherp"], "title": "CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at SCOLIA 2025", "summary": "Publication databases rely on accurate metadata extraction from diverse web\nsources, yet variations in web layouts and data formats present challenges for\nmetadata providers. This paper introduces CRAWLDoc, a new method for contextual\nranking of linked web documents. Starting with a publication's URL, such as a\ndigital object identifier, CRAWLDoc retrieves the landing page and all linked\nweb resources, including PDFs, ORCID profiles, and supplementary materials. It\nembeds these resources, along with anchor texts and the URLs, into a unified\nrepresentation. For evaluating CRAWLDoc, we have created a new, manually\nlabeled dataset of 600 publications from six top publishers in computer\nscience. Our method CRAWLDoc demonstrates a robust and layout-independent\nranking of relevant documents across publishers and data formats. It lays the\nfoundation for improved metadata extraction from web documents with various\nlayouts and formats. Our source code and dataset can be accessed at\nhttps://github.com/FKarl/CRAWLDoc.", "AI": {"tldr": "CRAWLDoc\u662f\u4e00\u79cd\u65b0\u7684\u4e0a\u4e0b\u6587\u6392\u5e8f\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u7f51\u7edc\u6587\u6863\u4e2d\u63d0\u53d6\u5143\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u5e03\u5c40\u548c\u683c\u5f0f\u591a\u6837\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u7f51\u9875\u5e03\u5c40\u548c\u6570\u636e\u683c\u5f0f\u7684\u591a\u6837\u6027\uff0c\u51c6\u786e\u7684\u5143\u6570\u636e\u63d0\u53d6\u5bf9\u6587\u732e\u6570\u636e\u5e93\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5e03\u5c40\u65e0\u5173\u7684\u65b9\u6cd5\u3002", "method": "CRAWLDoc\u901a\u8fc7URL\u68c0\u7d22\u6587\u732e\u7684\u7740\u9646\u9875\u548c\u6240\u6709\u94fe\u63a5\u8d44\u6e90\uff08\u5982PDF\u548cORCID\u8d44\u6599\uff09\uff0c\u5e76\u5c06\u5b83\u4eec\u5d4c\u5165\u7edf\u4e00\u8868\u793a\u4e2d\u3002", "result": "CRAWLDoc\u5728600\u7bc7\u8ba1\u7b97\u673a\u79d1\u5b66\u6587\u732e\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u8de8\u51fa\u7248\u5546\u548c\u683c\u5f0f\u72ec\u7acb\u6392\u5e8f\u6587\u6863\u3002", "conclusion": "CRAWLDoc\u4e3a\u4ece\u591a\u6837\u5e03\u5c40\u548c\u683c\u5f0f\u7684\u7f51\u9875\u6587\u6863\u4e2d\u63d0\u53d6\u5143\u6570\u636e\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "keywords": "\u5143\u6570\u636e\u63d0\u53d6, \u7f51\u9875\u5e03\u5c40, \u6587\u6863\u6392\u5e8f, CRAWLDoc"}}
{"id": "2506.03231", "pdf": "https://arxiv.org/pdf/2506.03231", "abs": "https://arxiv.org/abs/2506.03231", "authors": ["Yajie Zhou", "Jiajun Ruan", "Eric S. Wang", "Sadjad Fouladi", "Francis Y. Yan", "Kevin Hsieh", "Zaoxing Liu"], "title": "NetPress: Dynamically Generated LLM Benchmarks for Network Applications", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite growing interest in domain-specific benchmarking of large language\nmodels (LLMs) and agents, current evaluations remain limited to static,\nsmall-scale datasets, especially in high-stakes tasks like network operations\nthat demand reliability for deployments. We present NetPress, an automated\nbenchmark generation framework for evaluating LLM agents in network\napplications. NetPress introduces a unified abstraction with state and action,\nenabling dynamic generation of diverse query sets along with corresponding\nground truths. At runtime, users can specify benchmark configurations to\ngenerate millions of queries on the fly. In addition to dynamic benchmark\nconstruction, NetPress integrates with network emulators to provide realistic\nenvironment feedback, supporting comprehensive evaluation across correctness,\nsafety, and latency. We instantiate NetPress on three representative\napplications, revealing interesting fine-grained differences in agent behavior\nthat static, correctness-only benchmarks often miss. NetPress moves LLM\nevaluation toward realistic, scalable testing in infrastructure-centric\ndomains, helping close the gap between benchmark performance and real-world\ndeployment readiness. Code is available at\nhttps://github.com/Froot-NetSys/NetPress.", "AI": {"tldr": "NetPress\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u57fa\u51c6\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7f51\u7edc\u5e94\u7528\u4e2d\u7684LLM\u4ee3\u7406\uff0c\u652f\u6301\u52a8\u6001\u751f\u6210\u591a\u6837\u5316\u67e5\u8be2\u96c6\u548c\u5b9e\u65f6\u73af\u5883\u53cd\u9988\u3002", "motivation": "\u5f53\u524d\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u4ee3\u7406\u7684\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u3001\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u800c\u5728\u9ad8\u98ce\u9669\u7684\u7f51\u7edc\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9700\u8981\u66f4\u53ef\u9760\u7684\u90e8\u7f72\u9a8c\u8bc1\u3002", "method": "NetPress\u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u62bd\u8c61\u6a21\u578b\uff0c\u5305\u542b\u72b6\u6001\u548c\u52a8\u4f5c\uff0c\u652f\u6301\u52a8\u6001\u751f\u6210\u67e5\u8be2\u96c6\u548c\u5730\u9762\u771f\u76f8\uff0c\u5e76\u4e0e\u7f51\u7edc\u6a21\u62df\u5668\u96c6\u6210\u4ee5\u63d0\u4f9b\u771f\u5b9e\u73af\u5883\u53cd\u9988\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u5e94\u7528\u4e2d\u5b9e\u4f8b\u5316NetPress\uff0c\u63ed\u793a\u4e86\u9759\u6001\u57fa\u51c6\u5e38\u5ffd\u7565\u7684\u4ee3\u7406\u884c\u4e3a\u7ec6\u7c92\u5ea6\u5dee\u5f02\u3002", "conclusion": "NetPress\u63a8\u52a8\u4e86LLM\u8bc4\u4f30\u5411\u73b0\u5b9e\u5316\u3001\u53ef\u6269\u5c55\u5316\u53d1\u5c55\uff0c\u7f29\u5c0f\u4e86\u57fa\u51c6\u6027\u80fd\u4e0e\u5b9e\u9645\u90e8\u7f72\u51c6\u5907\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u57fa\u51c6\u6d4b\u8bd5, \u7f51\u7edc\u5e94\u7528, NetPress"}}
{"id": "2506.03817", "pdf": "https://arxiv.org/pdf/2506.03817", "abs": "https://arxiv.org/abs/2506.03817", "authors": ["Julius Gonsior", "Tim Rie\u00df", "Anja Reusch", "Claudio Hartmann", "Maik Thiele", "Wolfgang Lehner"], "title": "Survey of Active Learning Hyperparameters: Insights from a Large-Scale Experimental Grid", "categories": ["cs.LG"], "comment": null, "summary": "Annotating data is a time-consuming and costly task, but it is inherently\nrequired for supervised machine learning. Active Learning (AL) is an\nestablished method that minimizes human labeling effort by iteratively\nselecting the most informative unlabeled samples for expert annotation, thereby\nimproving the overall classification performance. Even though AL has been known\nfor decades, AL is still rarely used in real-world applications. As indicated\nin the two community web surveys among the NLP community about AL, two main\nreasons continue to hold practitioners back from using AL: first, the\ncomplexity of setting AL up, and second, a lack of trust in its effectiveness.\nWe hypothesize that both reasons share the same culprit: the large\nhyperparameter space of AL. This mostly unexplored hyperparameter space often\nleads to misleading and irreproducible AL experiment results. In this study, we\nfirst compiled a large hyperparameter grid of over 4.6 million hyperparameter\ncombinations, second, recorded the performance of all combinations in the\nso-far biggest conducted AL study, and third, analyzed the impact of each\nhyperparameter in the experiment results. In the end, we give recommendations\nabout the influence of each hyperparameter, demonstrate the surprising\ninfluence of the concrete AL strategy implementation, and outline an\nexperimental study design for reproducible AL experiments with minimal\ncomputational effort, thus contributing to more reproducible and trustworthy AL\nresearch in the future.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u672a\u88ab\u5e7f\u6cdb\u91c7\u7528\u7684\u539f\u56e0\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u8d85\u53c2\u6570\u5b9e\u9a8c\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u4e3b\u52a8\u5b66\u4e60\u80fd\u51cf\u5c11\u6807\u6ce8\u6210\u672c\uff0c\u4f46\u7531\u4e8e\u8bbe\u7f6e\u590d\u6742\u6027\u548c\u5bf9\u5176\u6548\u679c\u7684\u4e0d\u4fe1\u4efb\uff0cAL\u5728\u5b9e\u9645\u4e2d\u5f88\u5c11\u4f7f\u7528\u3002", "method": "\u7814\u7a76\u8005\u7f16\u8bd1\u4e86\u4e00\u4e2a\u5305\u542b460\u4e07\u79cd\u8d85\u53c2\u6570\u7ec4\u5408\u7684\u5927\u89c4\u6a21\u7f51\u683c\uff0c\u8bb0\u5f55\u4e86\u6240\u6709\u7ec4\u5408\u7684\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u6bcf\u4e2a\u8d85\u53c2\u6570\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8d85\u53c2\u6570\u7a7a\u95f4\u5bf9\u5b9e\u9a8c\u7ed3\u679c\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u9760\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u8d85\u53c2\u6570\u63a8\u8350\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u7814\u7a76\u8d85\u53c2\u6570\u7684\u5f71\u54cd\uff0c\u672c\u7814\u7a76\u4e3a\u672a\u6765\u7684AL\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u6027\u548c\u53ef\u4fe1\u5ea6\u66f4\u9ad8\u7684\u65b9\u6cd5\u3002", "keywords": "\u4e3b\u52a8\u5b66\u4e60, \u8d85\u53c2\u6570, \u53ef\u91cd\u590d\u6027, \u6807\u6ce8\u6548\u7387"}}
{"id": "2506.03827", "pdf": "https://arxiv.org/pdf/2506.03827", "abs": "https://arxiv.org/abs/2506.03827", "authors": ["Zhenhui Liu", "Chunyuan Yuan", "Ming Pang", "Zheng Fang", "Li Yuan", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Zheng Luo", "Jingping Shao"], "title": "Multi-objective Aligned Bidword Generation Model for E-commerce Search Advertising", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted by SIGIR2025", "summary": "Retrieval systems primarily address the challenge of matching user queries\nwith the most relevant advertisements, playing a crucial role in e-commerce\nsearch advertising. The diversity of user needs and expressions often produces\nmassive long-tail queries that cannot be matched with merchant bidwords or\nproduct titles, which results in some advertisements not being recalled,\nultimately harming user experience and search efficiency. Existing query\nrewriting research focuses on various methods such as query log mining,\nquery-bidword vector matching, or generation-based rewriting. However, these\nmethods often fail to simultaneously optimize the relevance and authenticity of\nthe user's original query and rewrite and maximize the revenue potential of\nrecalled ads.\n  In this paper, we propose a Multi-objective aligned Bidword Generation Model\n(MoBGM), which is composed of a discriminator, generator, and preference\nalignment module, to address these challenges. To simultaneously improve the\nrelevance and authenticity of the query and rewrite and maximize the platform\nrevenue, we design a discriminator to optimize these key objectives. Using the\nfeedback signal of the discriminator, we train a multi-objective aligned\nbidword generator that aims to maximize the combined effect of the three\nobjectives. Extensive offline and online experiments show that our proposed\nalgorithm significantly outperforms the state of the art. After deployment, the\nalgorithm has created huge commercial value for the platform, further verifying\nits feasibility and robustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u5bf9\u9f50\u7684\u7ade\u4ef7\u8bcd\u751f\u6210\u6a21\u578b\uff08MoBGM\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u5224\u522b\u5668\u3001\u751f\u6210\u5668\u548c\u504f\u597d\u5bf9\u9f50\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u67e5\u8be2\u91cd\u5199\u65b9\u6cd5\u5728\u76f8\u5173\u6027\u548c\u771f\u5b9e\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u540c\u65f6\u6700\u5927\u5316\u5e7f\u544a\u53ec\u56de\u6536\u76ca\u3002", "motivation": "\u73b0\u6709\u67e5\u8be2\u91cd\u5199\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4f18\u5316\u7528\u6237\u67e5\u8be2\u7684\u76f8\u5173\u6027\u3001\u771f\u5b9e\u6027\u548c\u5e7f\u544a\u6536\u76ca\uff0c\u5f71\u54cd\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u641c\u7d22\u6548\u7387\u3002", "method": "\u63d0\u51faMoBGM\u6a21\u578b\uff0c\u5305\u542b\u5224\u522b\u5668\u3001\u751f\u6210\u5668\u548c\u504f\u597d\u5bf9\u9f50\u6a21\u5757\uff0c\u5229\u7528\u5224\u522b\u5668\u7684\u53cd\u9988\u4fe1\u53f7\u8bad\u7ec3\u591a\u76ee\u6807\u5bf9\u9f50\u7684\u751f\u6210\u5668\u3002", "result": "\u79bb\u7ebf\u4e0e\u5728\u7ebf\u5b9e\u9a8c\u8868\u660e\uff0cMoBGM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u90e8\u7f72\u540e\u4e3a\u5e73\u53f0\u521b\u9020\u4e86\u5de8\u5927\u5546\u4e1a\u4ef7\u503c\u3002", "conclusion": "MoBGM\u5728\u6539\u8fdb\u67e5\u8be2\u91cd\u5199\u6548\u679c\u548c\u5e7f\u544a\u6536\u76ca\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u53ef\u884c\u6027\u548c\u9c81\u68d2\u6027\u3002", "keywords": "\u67e5\u8be2\u91cd\u5199,\u591a\u76ee\u6807\u4f18\u5316,\u5e7f\u544a\u53ec\u56de,\u7535\u5546\u641c\u7d22"}}
{"id": "2506.03835", "pdf": "https://arxiv.org/pdf/2506.03835", "abs": "https://arxiv.org/abs/2506.03835", "authors": ["Jianyuan Yin", "Qianxiao Li"], "title": "Learning task-specific predictive models for scientific computing", "categories": ["cs.LG"], "comment": null, "summary": "We consider learning a predictive model to be subsequently used for a given\ndownstream task (described by an algorithm) that requires access to the model\nevaluation. This task need not be prediction, and this situation is frequently\nencountered in machine-learning-augmented scientific computing. We show that\nthis setting differs from classical supervised learning, and in general it\ncannot be solved by minimizing the mean square error of the model predictions\nas is frequently performed in the literature. Instead, we find that the maximum\nprediction error on the support of the downstream task algorithm can serve as\nan effective estimate for the subsequent task performance. With this insight,\nwe formulate a task-specific supervised learning problem based on the given\nsampling measure, whose solution serves as a reliable surrogate model for the\ndownstream task. Then, we discretize the empirical risk based on training data,\nand develop an iterative algorithm to solve the task-specific supervised\nlearning problem. Three illustrative numerical examples on trajectory\nprediction, optimal control and minimum energy path computation demonstrate the\neffectiveness of the approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u7279\u5b9a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e0d\u540c\u4e8e\u4f20\u7edf\u7684\u5747\u65b9\u8bef\u5dee\u4f18\u5316\uff0c\u800c\u662f\u57fa\u4e8e\u4e0b\u6e38\u4efb\u52a1\u7b97\u6cd5\u7684\u6700\u5927\u9884\u6d4b\u8bef\u5dee\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u9700\u8981\u6a21\u578b\u8bc4\u4f30\u7684\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u79d1\u5b66\u8ba1\u7b97\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4efb\u52a1\u7279\u5b9a\u7684\u5b66\u4e60\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u6700\u5927\u5316\u4e0b\u6e38\u4efb\u52a1\u7b97\u6cd5\u652f\u6301\u4e0a\u7684\u9884\u6d4b\u8bef\u5dee\u6765\u6784\u5efa\u4efb\u52a1\u7279\u5b9a\u7684\u76d1\u7763\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u8fed\u4ee3\u7b97\u6cd5\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "result": "\u4e09\u4e2a\u6570\u503c\u5b9e\u9a8c\uff08\u8f68\u8ff9\u9884\u6d4b\u3001\u6700\u4f18\u63a7\u5236\u548c\u6700\u5c0f\u80fd\u91cf\u8def\u5f84\u8ba1\u7b97\uff09\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4efb\u52a1\u7279\u5b9a\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u652f\u6301\u4e0b\u6e38\u4efb\u52a1\uff0c\u4f18\u4e8e\u4f20\u7edf\u5747\u65b9\u8bef\u5dee\u4f18\u5316\u3002", "keywords": "\u76d1\u7763\u5b66\u4e60\u3001\u4e0b\u6e38\u4efb\u52a1\u3001\u9884\u6d4b\u8bef\u5dee\u3001\u79d1\u5b66\u8ba1\u7b97\u3001\u6570\u503c\u5b9e\u9a8c"}}
{"id": "2506.03832", "pdf": "https://arxiv.org/pdf/2506.03832", "abs": "https://arxiv.org/abs/2506.03832", "authors": ["Omer Moussa", "Mariya Toneva"], "title": "Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain", "categories": ["cs.CL", "cs.SD", "eess.AS", "q-bio.NC"], "comment": "Proceedings of Interspeech 2025", "summary": "Pretrained self-supervised speech models excel in speech tasks but do not\nreflect the hierarchy of human speech processing, as they encode rich semantics\nin middle layers and poor semantics in late layers. Recent work showed that\nbrain-tuning (fine-tuning models using human brain recordings) improves speech\nmodels' semantic understanding. Here, we examine how well brain-tuned models\nfurther reflect the brain's intermediate stages of speech processing. We find\nthat late layers of brain-tuned models substantially improve over pretrained\nmodels in their alignment with semantic language regions. Further layer-wise\nprobing reveals that early layers remain dedicated to low-level acoustic\nfeatures, while late layers become the best at complex high-level tasks. These\nfindings show that brain-tuned models not only perform better but also exhibit\na well-defined hierarchical processing going from acoustic to semantic\nrepresentations, making them better model organisms for human speech\nprocessing.", "AI": {"tldr": "\u7814\u7a76\u53d1\ufffd\ufffd\ufffd\uff0c\u901a\u8fc7\u5927\u8111\u8c03\u4f18\u7684\u8bed\u97f3\u6a21\u578b\u4e0d\u4ec5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u800c\u4e14\u80fd\u66f4\u597d\u5730\u6a21\u62df\u4eba\u7c7b\u8bed\u97f3\u5904\u7406\u7684\u5c42\u7ea7\u7ed3\u6784\u3002", "motivation": "\u63a2\u8ba8\u5927\u8111\u8c03\u4f18\u7684\u8bed\u97f3\u6a21\u578b\u662f\u5426\u80fd\u66f4\u597d\u5730\u53cd\u6620\u4eba\u7c7b\u8bed\u97f3\u5904\u7406\u7684\u4e2d\u95f4\u9636\u6bb5\u3002", "method": "\u5bf9\u5927\u8111\u8c03\u4f18\u7684\u8bed\u97f3\u6a21\u578b\u8fdb\u884c\u9010\u5c42\u5206\u6790\uff0c\u8bc4\u4f30\u5176\u4e0e\u5927\u8111\u8bed\u4e49\u533a\u57df\u7684\u5339\u914d\u7a0b\u5ea6\u3002", "result": "\u5927\u8111\u8c03\u4f18\u6a21\u578b\u7684\u540e\u671f\u5c42\u5728\u8bed\u4e49\u7406\u89e3\u4e0a\u663e\u8457\u4f18\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4e14\u4fdd\u6301\u4e86\u4ece\u58f0\u5b66\u5230\u8bed\u4e49\u7684\u5c42\u7ea7\u5904\u7406\u3002", "conclusion": "\u5927\u8111\u8c03\u4f18\u6a21\u578b\u4e0d\u4ec5\u6027\u80fd\u66f4\u4f18\uff0c\u8fd8\u80fd\u66f4\u597d\u5730\u6a21\u62df\u4eba\u7c7b\u8bed\u97f3\u5904\u7406\u7684\u5c42\u7ea7\u7ed3\u6784\uff0c\u6210\u4e3a\u66f4\u4f18\u7684\u7814\u7a76\u5de5\u5177\u3002", "keywords": "\u8bed\u97f3\u6a21\u578b, \u5927\u8111\u8c03\u4f18, \u8bed\u4e49\u5c42\u7ea7, \u8bed\u97f3\u5904\u7406"}}
{"id": "2506.03237", "pdf": "https://arxiv.org/pdf/2506.03237", "abs": "https://arxiv.org/abs/2506.03237", "authors": ["Jigang Fan", "Quanlin Wu", "Shengjie Luo", "Liwei Wang"], "title": "UniSite: The First Cross-Structure Dataset and Learning Framework for End-to-End Ligand Binding Site Detection", "categories": ["q-bio.QM", "cs.AI", "cs.LG", "q-bio.BM"], "comment": null, "summary": "The detection of ligand binding sites for proteins is a fundamental step in\nStructure-Based Drug Design. Despite notable advances in recent years, existing\nmethods, datasets, and evaluation metrics are confronted with several key\nchallenges: (1) current datasets and methods are centered on individual\nprotein-ligand complexes and neglect that diverse binding sites may exist\nacross multiple complexes of the same protein, introducing significant\nstatistical bias; (2) ligand binding site detection is typically modeled as a\ndiscontinuous workflow, employing binary segmentation and subsequent clustering\nalgorithms; (3) traditional evaluation metrics do not adequately reflect the\nactual performance of different binding site prediction methods. To address\nthese issues, we first introduce UniSite-DS, the first UniProt (Unique\nProtein)-centric ligand binding site dataset, which contains 4.81 times more\nmulti-site data and 2.08 times more overall data compared to the previously\nmost widely used datasets. We then propose UniSite, the first end-to-end ligand\nbinding site detection framework supervised by set prediction loss with\nbijective matching. In addition, we introduce Average Precision based on\nIntersection over Union (IoU) as a more accurate evaluation metric for ligand\nbinding site prediction. Extensive experiments on UniSite-DS and several\nrepresentative benchmark datasets demonstrate that IoU-based Average Precision\nprovides a more accurate reflection of prediction quality, and that UniSite\noutperforms current state-of-the-art methods in ligand binding site detection.\nThe dataset and codes will be made publicly available at\nhttps://github.com/quanlin-wu/unisite.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2aUniProt\u4e2d\u5fc3\u914d\u4f53\u7ed3\u5408\u4f4d\u70b9\u6570\u636e\u96c6UniSite-DS\u548c\u7aef\u5230\u7aef\u68c0\u6d4b\u6846\u67b6UniSite\uff0c\u5e76\u5f15\u5165\u57fa\u4e8eIoU\u7684\u5e73\u5747\u7cbe\u5ea6\u4f5c\u4e3a\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u914d\u4f53\u7ed3\u5408\u4f4d\u70b9\u68c0\u6d4b\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u7edf\u8ba1\u504f\u5dee\u3001\u4e0d\u8fde\u7eed\u6d41\u7a0b\u548c\u4e0d\u5145\u5206\u8bc4\u4f30\u7b49\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51faUniSite-DS\u6570\u636e\u96c6\u548cUniSite\u6846\u67b6\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u68c0\u6d4b\u548c\u96c6\u5408\u9884\u6d4b\u635f\u5931\u76d1\u7763\uff0c\u5e76\u5f15\u5165IoU-based Average Precision\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eIoU-based Average Precision\u66f4\u51c6\u786e\u5730\u53cd\u6620\u9884\u6d4b\u8d28\u91cf\uff0cUniSite\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UniSite-DS\u548cUniSite\u89e3\u51b3\u4e86\u73b0\u6709\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "keywords": "\u914d\u4f53\u7ed3\u5408\u4f4d\u70b9\u68c0\u6d4b, \u6570\u636e\u96c6, \u7aef\u5230\u7aef\u6846\u67b6, \u8bc4\u4f30\u6307\u6807"}}
{"id": "2506.03839", "pdf": "https://arxiv.org/pdf/2506.03839", "abs": "https://arxiv.org/abs/2506.03839", "authors": ["Tobias Pielok", "Bernd Bischl", "David R\u00fcgamer"], "title": "Revisiting Unbiased Implicit Variational Inference", "categories": ["cs.LG", "stat.ML", "62F15, 68T07", "I.2.6; G.3"], "comment": "Accepted to ICML 2025", "summary": "Recent years have witnessed growing interest in semi-implicit variational\ninference (SIVI) methods due to their ability to rapidly generate samples from\ncomplex distributions. However, since the likelihood of these samples is\nnon-trivial to estimate in high dimensions, current research focuses on finding\neffective SIVI training routines. Although unbiased implicit variational\ninference (UIVI) has largely been dismissed as imprecise and computationally\nprohibitive because of its inner MCMC loop, we revisit this method and show\nthat UIVI's MCMC loop can be effectively replaced via importance sampling and\nthe optimal proposal distribution can be learned stably by minimizing an\nexpected forward Kullback-Leibler divergence without bias. Our refined approach\ndemonstrates superior performance or parity with state-of-the-art methods on\nestablished SIVI benchmarks.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86UIVI\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u91c7\u6837\u66ff\u4ee3\u4e86\u5176MCMC\u5faa\u73af\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u5316\u671f\u671b\u524d\u5411KL\u6563\u5ea6\u7a33\u5b9a\u5b66\u4e60\u6700\u4f18\u63d0\u8bae\u5206\u5e03\uff0c\u6027\u80fd\u4f18\u4e8e\u6216\u63a5\u8fd1\u73b0\u6709SIVI\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3SIVI\u65b9\u6cd5\u5728\u9ad8\u7ef4\u4e2d\u6837\u672c\u4f3c\u7136\u4f30\u8ba1\u56f0\u96be\u7684\u95ee\u9898\uff0c\u6539\u8fdbUIVI\u65b9\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u91cd\u8981\u6027\u91c7\u6837\u66ff\u4ee3UIVI\u4e2d\u7684MCMC\u5faa\u73af\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u5316\u671f\u671b\u524d\u5411KL\u6563\u5ea6\u5b66\u4e60\u6700\u4f18\u63d0\u8bae\u5206\u5e03\u3002", "result": "\u6539\u8fdb\u540e\u7684UIVI\u65b9\u6cd5\u5728SIVI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u63a5\u8fd1\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UIVI\u65b9\u6cd5\u53ef\u901a\u8fc7\u91cd\u8981\u6027\u91c7\u6837\u548cKL\u6563\u5ea6\u4f18\u5316\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "keywords": "\u534a\u9690\u5f0f\u53d8\u5206\u63a8\u65ad,\u91cd\u8981\u6027\u91c7\u6837,MCMC,KL\u6563\u5ea6"}}
{"id": "2506.03861", "pdf": "https://arxiv.org/pdf/2506.03861", "abs": "https://arxiv.org/abs/2506.03861", "authors": ["Qiuhan Han", "Qian Wang", "Atsushi Yoshikawa", "Masayuki Yamamura"], "title": "PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in High-Frequency Cryptocurrency Trading", "categories": ["cs.CL"], "comment": null, "summary": "High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding\nrapid decision-making. Social media platforms like Reddit offer valuable, yet\nunderexplored, information for such high-frequency, short-term trading. This\npaper introduces \\textbf{PulseReddit}, a novel dataset that is the first to\nalign large-scale Reddit discussion data with high-frequency cryptocurrency\nmarket statistics for short-term trading analysis. We conduct an extensive\nempirical study using Large Language Model (LLM)-based Multi-Agent Systems\n(MAS) to investigate the impact of social sentiment from PulseReddit on trading\nperformance. Our experiments conclude that MAS augmented with PulseReddit data\nachieve superior trading outcomes compared to traditional baselines,\nparticularly in bull markets, and demonstrate robust adaptability across\ndifferent market regimes. Furthermore, our research provides conclusive\ninsights into the performance-efficiency trade-offs of different LLMs,\ndetailing significant considerations for practical model selection in HFT\napplications. PulseReddit and our findings establish a foundation for advanced\nMAS research in HFT, demonstrating the tangible benefits of integrating social\nmedia.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86PulseReddit\u6570\u636e\u96c6\uff0c\u9996\u6b21\u5c06Reddit\u8ba8\u8bba\u6570\u636e\u4e0e\u52a0\u5bc6\u8d27\u5e01\u9ad8\u9891\u4ea4\u6613\u6570\u636e\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9a8c\u8bc1\u4e86\u793e\u4ea4\u5a92\u4f53\u60c5\u7eea\u5bf9\u4ea4\u6613\u8868\u73b0\u7684\u79ef\u6781\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u793e\u4ea4\u5a92\u4f53\uff08\u5982Reddit\uff09\u5728\u9ad8\u9891\u52a0\u5bc6\u8d27\u5e01\u4ea4\u6613\u4e2d\u7684\u6f5c\u5728\u4ef7\u503c\uff0c\u586b\u8865\u76f8\u5173\u9886\u57df\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4f7f\u7528PulseReddit\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5206\u6790\u793e\u4ea4\u5a92\u4f53\u60c5\u7eea\u5bf9\u9ad8\u9891\u4ea4\u6613\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408PulseReddit\u6570\u636e\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u725b\u5e02\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5bf9\u4e0d\u540c\u5e02\u573a\u73af\u5883\u5177\u6709\u5f3a\u9002\u5e94\u6027\u3002\u540c\u65f6\u63ed\u793a\u4e86\u4e0d\u540c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6027\u80fd\u4e0e\u6548\u7387\u4e0a\u7684\u6743\u8861\u3002", "conclusion": "PulseReddit\u4e3a\u9ad8\u9891\u4ea4\u6613\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u7684\u5b9e\u9645\u4ef7\u503c\u3002", "keywords": "\u9ad8\u9891\u4ea4\u6613,\u52a0\u5bc6\u8d27\u5e01,Reddit,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u591a\u667a\u80fd\u4f53\u7cfb\u7edf"}}
{"id": "2506.03238", "pdf": "https://arxiv.org/pdf/2506.03238", "abs": "https://arxiv.org/abs/2506.03238", "authors": ["Ziheng Zhao", "Lisong Dai", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "title": "Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric Approach", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Automated interpretation of CT images-particularly localizing and describing\nabnormal findings across multi-plane and whole-body scans-remains a significant\nchallenge in clinical radiology. This work aims to address this challenge\nthrough four key contributions: (i) On taxonomy, we collaborate with senior\nradiologists to propose a comprehensive hierarchical classification system,\nwith 404 representative abnormal findings across all body regions; (ii) On\ndata, we contribute a dataset containing over 14.5K CT images from multiple\nplanes and all human body regions, and meticulously provide grounding\nannotations for over 19K abnormalities, each linked to the detailed description\nand cast into the taxonomy; (iii) On model development, we propose\nOminiAbnorm-CT, which can automatically ground and describe abnormal findings\non multi-plane and whole-body CT images based on text queries, while also\nallowing flexible interaction through visual prompts; (iv) On benchmarks, we\nestablish three representative evaluation tasks based on real clinical\nscenarios. Through extensive experiments, we show that OminiAbnorm-CT can\nsignificantly outperform existing methods on all the tasks and metrics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u89e3\u91caCT\u56fe\u50cf\u7684\u65b9\u6cd5OminiAbnorm-CT\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u5e73\u9762\u548c\u5168\u8eab\u626b\u63cf\u4e2d\u5b9a\u4f4d\u548c\u63cf\u8ff0\u5f02\u5e38\u53d1\u73b0\u7684\u6311\u6218\u3002\u901a\u8fc7\u5206\u7c7b\u7cfb\u7edf\u3001\u6570\u636e\u96c6\u3001\u6a21\u578b\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5\u56db\u65b9\u9762\u8d21\u732e\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4e34\u5e8a\u653e\u5c04\u5b66\u4e2d\u591a\u5e73\u9762\u548c\u5168\u8eabCT\u56fe\u50cf\u5f02\u5e38\u53d1\u73b0\u81ea\u52a8\u5b9a\u4f4d\u548c\u63cf\u8ff0\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u7efc\u5408\u5206\u7c7b\u7cfb\u7edf\uff0c\u521b\u5efa\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5f00\u53d1OminiAbnorm-CT\u6a21\u578b\uff0c\u652f\u6301\u6587\u672c\u67e5\u8be2\u548c\u89c6\u89c9\u63d0\u793a\u4ea4\u4e92\uff0c\u5e76\u5efa\u7acb\u4e34\u5e8a\u8bc4\u4f30\u4efb\u52a1\u3002", "result": "OminiAbnorm-CT\u5728\u6240\u6709\u4efb\u52a1\u548c\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u81ea\u52a8\u89e3\u91caCT\u56fe\u50cf\u5f02\u5e38\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "CT\u56fe\u50cf,\u5f02\u5e38\u53d1\u73b0,\u81ea\u52a8\u5316\u89e3\u91ca,\u5206\u7c7b\u7cfb\u7edf,OminiAbnorm-CT"}}
{"id": "2506.03850", "pdf": "https://arxiv.org/pdf/2506.03850", "abs": "https://arxiv.org/abs/2506.03850", "authors": ["Liang Chen", "Xueting Han", "Li Shen", "Jing Bai", "Kam-Fai Wong"], "title": "Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Harmful fine-tuning (HFT), performed directly on open-source LLMs or through\nFine-tuning-as-a-Service, breaks safety alignment and poses significant\nthreats. Existing methods aim to mitigate HFT risks by learning robust\nrepresentation on alignment data or making harmful data unlearnable, but they\ntreat each data sample equally, leaving data vulnerability patterns\nunderstudied. In this work, we reveal that certain subsets of alignment data\nare consistently more prone to forgetting during HFT across different\nfine-tuning tasks. Inspired by these findings, we propose Vulnerability-Aware\nAlignment (VAA), which estimates data vulnerability, partitions data into\n\"vulnerable\" and \"invulnerable\" groups, and encourages balanced learning using\na group distributionally robust optimization (Group DRO) framework.\nSpecifically, VAA learns an adversarial sampler that samples examples from the\ncurrently underperforming group and then applies group-dependent adversarial\nperturbations to the data during training, aiming to encourage a balanced\nlearning process across groups. Experiments across four fine-tuning tasks\ndemonstrate that VAA significantly reduces harmful scores while preserving\ndownstream task performance, outperforming state-of-the-art baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVAA\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u6613\u53d7\u653b\u51fb\u7684\u6570\u636e\u5b50\u96c6\u5e76\u5229\u7528Group DRO\u6846\u67b6\u5b9e\u73b0\u5e73\u8861\u5b66\u4e60\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6709\u5bb3\u5206\u6570\u3002", "motivation": "\u9488\u5bf9\u6709\u5bb3\u5fae\u8c03\uff08HFT\uff09\u7834\u574f\u5b89\u5168\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u7814\u7a76\u6570\u636e\u6f0f\u6d1e\u6a21\u5f0f\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faVulnerability-Aware Alignment\uff08VAA\uff09\uff0c\u901a\u8fc7\u8bc4\u4f30\u6570\u636e\u6613\u53d7\u653b\u51fb\u6027\u5e76\u5229\u7528Group DRO\u6846\u67b6\u8fdb\u884c\u5e73\u8861\u5b66\u4e60\u3002", "result": "\u5728\u56db\u9879\u5fae\u8c03\u4efb\u52a1\u4e2d\uff0cVAA\u663e\u8457\u964d\u4f4e\u6709\u5bb3\u5206\u6570\u5e76\u4fdd\u6301\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VAA\u901a\u8fc7\u5173\u6ce8\u6570\u636e\u6613\u53d7\u653b\u51fb\u6027\uff0c\u63d0\u5347\u4e86\u5b89\u5168\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\u3002", "keywords": "\u6709\u5bb3\u5fae\u8c03, \u5b89\u5168\u5bf9\u9f50, Group DRO, \u6570\u636e\u6613\u53d7\u653b\u51fb\u6027"}}
{"id": "2506.03867", "pdf": "https://arxiv.org/pdf/2506.03867", "abs": "https://arxiv.org/abs/2506.03867", "authors": ["Jacqueline Rowe", "Mateusz Klimaszewski", "Liane Guillou", "Shannon Vallor", "Alexandra Birch"], "title": "EuroGEST: Investigating gender stereotypes in multilingual language models", "categories": ["cs.CL"], "comment": "8 pages, 6 figures, 1 table", "summary": "Large language models increasingly support multiple languages, yet most\nbenchmarks for gender bias remain English-centric. We introduce EuroGEST, a\ndataset designed to measure gender-stereotypical reasoning in LLMs across\nEnglish and 29 European languages. EuroGEST builds on an existing\nexpert-informed benchmark covering 16 gender stereotypes, expanded in this work\nusing translation tools, quality estimation metrics, and morphological\nheuristics. Human evaluations confirm that our data generation method results\nin high accuracy of both translations and gender labels across languages. We\nuse EuroGEST to evaluate 24 multilingual language models from six model\nfamilies, demonstrating that the strongest stereotypes in all models across all\nlanguages are that women are \\textit{beautiful,} \\textit{empathetic} and\n\\textit{neat} and men are \\textit{leaders}, \\textit{strong, tough} and\n\\textit{professional}. We also show that larger models encode gendered\nstereotypes more strongly and that instruction finetuning does not consistently\nreduce gendered stereotypes. Our work highlights the need for more multilingual\nstudies of fairness in LLMs and offers scalable methods and resources to audit\ngender bias across languages.", "AI": {"tldr": "EuroGEST\u662f\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u82f1\u8bed\u548c29\u79cd\u6b27\u6d32\u8bed\u8a00\u4e2d\u7684\u6027\u522b\u523b\u677f\u5370\u8c61\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5bf9\u6027\u522b\u89d2\u8272\u7684\u523b\u677f\u5370\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u8bed\u8a00\u516c\u5e73\u6027\u7814\u7a76\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u5927\u591a\u6570\u6027\u522b\u504f\u89c1\u57fa\u51c6\u4ec5\u5173\u6ce8\u82f1\u8bed\uff0c\u7f3a\u4e4f\u5bf9\u5176\u4ed6\u8bed\u8a00\u7684\u7814\u7a76\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u591a\u8bed\u8a00\u7684\u6570\u636e\u96c6\u6765\u8861\u91cfLLMs\u4e2d\u7684\u6027\u522b\u523b\u677f\u5370\u8c61\u3002", "method": "\u57fa\u4e8e\u73b0\u6709\u7684\u4e13\u5bb6\u8bc4\u4f30\u57fa\u51c6\uff0c\u901a\u8fc7\u7ffb\u8bd1\u5de5\u5177\u3001\u8d28\u91cf\u4f30\u8ba1\u6307\u6807\u548c\u5f62\u6001\u542f\u53d1\u5f0f\u65b9\u6cd5\u6269\u5c55\uff0c\u751f\u6210\u9ad8\u7cbe\u5ea6\u7684\u7ffb\u8bd1\u548c\u6027\u522b\u6807\u7b7e\u6570\u636e\uff0c\u5e76\u5bf924\u4e2a\u591a\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0\u6240\u6709\u6a21\u578b\u5728\u6240\u6709\u8bed\u8a00\u4e2d\u6700\u5f3a\u7684\u6027\u522b\u523b\u677f\u5370\u8c61\u662f\u5973\u6027\u201c\u7f8e\u4e3d\u3001\u5584\u89e3\u4eba\u610f\u3001\u6574\u6d01\u201d\u800c\u7537\u6027\u201c\u9886\u5bfc\u8005\u3001\u5f3a\u58ee\u3001\u4e13\u4e1a\u201d\uff1b\u66f4\u5927\u7684\u6a21\u578b\u6027\u522b\u523b\u677f\u5370\u8c61\u66f4\u5f3a\uff0c\u6307\u4ee4\u5fae\u8c03\u4e0d\u80fd\u6301\u7eed\u51cf\u5c11\u523b\u677f\u5370\u8c61\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u8bed\u8a00\u516c\u5e73\u6027\u7814\u7a76\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u4e3a\u8de8\u8bed\u8a00\u6027\u522b\u504f\u89c1\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u548c\u8d44\u6e90\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u6027\u522b\u504f\u89c1, \u591a\u8bed\u8a00\u8bc4\u4f30, EuroGEST, \u516c\u5e73\u6027, \u523b\u677f\u5370\u8c61"}}
{"id": "2506.03270", "pdf": "https://arxiv.org/pdf/2506.03270", "abs": "https://arxiv.org/abs/2506.03270", "authors": ["Jeremy Siburian", "Keisuke Shirai", "Cristian C. Beltran-Hernandez", "Masashi Hamaya", "Michael G\u00f6rner", "Atsushi Hashimoto"], "title": "Grounded Vision-Language Interpreter for Integrated Task and Motion Planning", "categories": ["cs.RO", "cs.AI"], "comment": "Project website: https://omron-sinicx.github.io/ViLaIn-TAMP/", "summary": "While recent advances in vision-language models (VLMs) have accelerated the\ndevelopment of language-guided robot planners, their black-box nature often\nlacks safety guarantees and interpretability crucial for real-world deployment.\nConversely, classical symbolic planners offer rigorous safety verification but\nrequire significant expert knowledge for setup. To bridge the current gap, this\npaper proposes ViLaIn-TAMP, a hybrid planning framework for enabling\nverifiable, interpretable, and autonomous robot behaviors. ViLaIn-TAMP\ncomprises three main components: (1) ViLaIn (Vision-Language Interpreter) - A\nprior framework that converts multimodal inputs into structured problem\nspecifications using off-the-shelf VLMs without additional domain-specific\ntraining, (2) a modular Task and Motion Planning (TAMP) system that grounds\nthese specifications in actionable trajectory sequences through symbolic and\ngeometric constraint reasoning and can utilize learning-based skills for key\nmanipulation phases, and (3) a corrective planning module which receives\nconcrete feedback on failed solution attempts from the motion and task planning\ncomponents and can feed adapted logic and geometric feasibility constraints\nback to ViLaIn to improve and further refine the specification. We evaluate our\nframework on several challenging manipulation tasks in a cooking domain. We\ndemonstrate that the proposed closed-loop corrective architecture exhibits a\nmore than 30% higher mean success rate for ViLaIn-TAMP compared to without\ncorrective planning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aViLaIn-TAMP\u7684\u6df7\u5408\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u7b26\u53f7\u89c4\u5212\u5668\uff0c\u5b9e\u73b0\u4e86\u53ef\u9a8c\u8bc1\u3001\u53ef\u89e3\u91ca\u7684\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u95ed\u73af\u7ea0\u6b63\u6a21\u5757\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u5b89\u5168\u6027\u9a8c\u8bc1\u548c\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u7b26\u53f7\u89c4\u5212\u5668\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u77e5\u8bc6\u8bbe\u7f6e\u7684\u5c40\u9650\u6027\uff0c\u672c\u6587\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u5f00\u53d1\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u673a\u5668\u4eba\u89c4\u5212\u6846\u67b6\u3002", "method": "\u6846\u67b6\u5305\u62ec\u4e09\u4e2a\u4e3b\u8981\u6a21\u5757\uff1a\u89c6\u89c9\u8bed\u8a00\u89e3\u91ca\u5668\uff08ViLaIn\uff09\u5c06\u591a\u6a21\u6001\u8f93\u5165\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u95ee\u9898\u63cf\u8ff0\uff1b\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u7cfb\u7edf\uff08TAMP\uff09\u901a\u8fc7\u7b26\u53f7\u548c\u51e0\u4f55\u7ea6\u675f\u751f\u6210\u53ef\u884c\u8f68\u8ff9\uff1b\u95ed\u73af\u7ea0\u6b63\u6a21\u5757\u6839\u636e\u53cd\u9988\u4f18\u5316\u95ee\u9898\u63cf\u8ff0\u548c\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cViLaIn-TAMP\u5728\u70f9\u996a\u9886\u57df\u7684\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u95ed\u73af\u7ea0\u6b63\u6a21\u5757\u4f7f\u5176\u5e73\u5747\u6210\u529f\u7387\u63d0\u9ad8\u4e8630%\u4ee5\u4e0a\u3002", "conclusion": "ViLaIn-TAMP\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u7b26\u53f7\u89c4\u5212\u5668\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u9a8c\u8bc1\u7684\u673a\u5668\u4eba\u884c\u4e3a\u89c4\u5212\uff0c\u5e76\u901a\u8fc7\u7ea0\u6b63\u6a21\u5757\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u7b26\u53f7\u89c4\u5212, \u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212, \u95ed\u73af\u7ea0\u6b63"}}
{"id": "2506.03857", "pdf": "https://arxiv.org/pdf/2506.03857", "abs": "https://arxiv.org/abs/2506.03857", "authors": ["Mingxuan Xia", "Haobo Wang", "Yixuan Li", "Zewei Yu", "Jindong Wang", "Junbo Zhao", "Runze Wu"], "title": "Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ACL 2025 (Main conference)", "summary": "Recently, Large Language Models (LLMs) have demonstrated significant\npotential for data annotation, markedly reducing the labor costs associated\nwith downstream applications. However, existing methods mostly adopt an\naggressive strategy by prompting LLM to determine a single gold label for each\nunlabeled sample. Due to the inherent uncertainty within LLMs, they often\nproduce incorrect labels for difficult samples, severely compromising the data\nquality for downstream applications. Motivated by ambiguity aversion in human\nbehaviors, we propose a novel candidate annotation paradigm wherein large\nlanguage models are encouraged to output all possible labels when incurring\nuncertainty. To ensure unique labels are provided for downstream tasks, we\ndevelop a teacher-student framework CanDist that distills candidate annotations\nwith a Small Language Model (SLM). We further provide a rigorous justification\ndemonstrating that distilling candidate annotations from the teacher LLM offers\nsuperior theoretical guarantees compared to directly using single annotations.\nExtensive experiments across six text classification tasks validate the\neffectiveness of our proposed method. The source code is available at\nhttps://github.com/MingxuanXia/CanDist.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5019\u9009\u6807\u6ce8\u8303\u5f0f\uff0c\u901a\u8fc7\u9f13\u52b1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e0d\u786e\u5b9a\u65f6\u8f93\u51fa\u6240\u6709\u53ef\u80fd\u7684\u6807\u7b7e\uff0c\u5e76\u4f7f\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09CanDist\u84b8\u998f\u8fd9\u4e9b\u5019\u9009\u6807\u6ce8\uff0c\u4ee5\u63d0\u9ad8\u6570\u636e\u6807\u6ce8\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684LLM\u6807\u6ce8\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u5355\u4e00\u9ec4\u91d1\u6807\u7b7e\u7b56\u7565\uff0c\u4f46\u7531\u4e8eLLM\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5bb9\u6613\u5bf9\u56f0\u96be\u6837\u672c\u4ea7\u751f\u9519\u8bef\u6807\u7b7e\uff0c\u5f71\u54cd\u4e0b\u6e38\u5e94\u7528\u7684\u6570\u636e\u8d28\u91cf\u3002", "method": "1. \u63d0\u51fa\u5019\u9009\u6807\u6ce8\u8303\u5f0f\uff0c\u9f13\u52b1LLM\u8f93\u51fa\u6240\u6709\u53ef\u80fd\u7684\u6807\u7b7e\uff1b2. \u5f00\u53d1\u6559\u5e08-\u5b66\u751f\u6846\u67b6CanDist\uff0c\u7528SLM\u84b8\u998f\u5019\u9009\u6807\u6ce8\u3002", "result": "\u5728\u516d\u4e2a\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5019\u9009\u6807\u6ce8\u8303\u5f0f\u53caCanDist\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u6807\u6ce8\u8d28\u91cf\uff0c\u4f18\u4e8e\u76f4\u63a5\u4f7f\u7528\u5355\u4e00\u6807\u7b7e\u7684\u65b9\u6cd5\u3002", "keywords": "Large Language Models, data annotation, teacher-student framework, uncertainty"}}
{"id": "2506.03880", "pdf": "https://arxiv.org/pdf/2506.03880", "abs": "https://arxiv.org/abs/2506.03880", "authors": ["Ruihan Jin", "Pengpeng Shao", "Zhengqi Wen", "Jinyang Wu", "Mingkuan Feng", "Shuai Zhang", "Jianhua Tao"], "title": "RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancements in large language models (LLMs) have led to the\nemergence of routing techniques, which aim to efficiently select the optimal\nLLM from diverse candidates to tackle specific tasks, optimizing performance\nwhile reducing costs. Current LLM routing methods are limited in effectiveness\ndue to insufficient exploration of the intrinsic connection between user\nqueries and the characteristics of LLMs. To address this issue, in this paper,\nwe present RadialRouter, a novel framework for LLM routing which employs a\nlightweight Transformer-based backbone with a radial structure named\nRadialFormer to articulate the query-LLMs relationship. The optimal LLM\nselection is performed based on the final states of RadialFormer. The pipeline\nis further refined by an objective function that combines Kullback-Leibler\ndivergence with the query-query contrastive loss to enhance robustness.\nExperimental results on RouterBench show that RadialRouter significantly\noutperforms existing routing methods by 9.2\\% and 5.8\\% in the Balance and Cost\nFirst scenarios, respectively. Additionally, its adaptability toward different\nperformance-cost trade-offs and the dynamic LLM pool demonstrates practical\napplication potential.", "AI": {"tldr": "RadialRouter\u662f\u4e00\u4e2a\u521b\u65b0\u7684LLM\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7RadialFormer\u5f3a\u5316\u67e5\u8be2\u4e0eLLM\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u8def\u7531\u6548\u679c\u3002", "motivation": "\u73b0\u6709LLM\u8def\u7531\u65b9\u6cd5\u56e0\u672a\u5145\u5206\u63a2\u7d22\u67e5\u8be2\u4e0eLLM\u7279\u6027\u4e4b\u95f4\u7684\u8054\u7cfb\u800c\u6548\u679c\u53d7\u9650\uff0cRadialRouter\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u8f7b\u91cf\u7ea7RadialFormer\u7ed3\u6784\uff0c\u7ed3\u5408KL\u6563\u5ea6\u548c\u67e5\u8be2\u5bf9\u6bd4\u635f\u5931\u7684\u76ee\u6807\u51fd\u6570\u4f18\u5316\u8def\u7531\u3002", "result": "\u5728RouterBench\u6d4b\u8bd5\u4e2d\uff0cRadialRouter\u5728\u4e24\u79cd\u573a\u666f\u4e0b\u5206\u522b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd59.2%\u548c5.8%\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u9002\u5e94\u6027\u3002", "conclusion": "RadialRouter\u901a\u8fc7\u65b0\u578b\u67b6\u6784\u548c\u76ee\u6807\u51fd\u6570\u5b9e\u73b0\u4e86\u9ad8\u6548LLM\u8def\u7531\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u8def\u7531\u6280\u672f,Transformer,RadialRouter"}}
{"id": "2506.03275", "pdf": "https://arxiv.org/pdf/2506.03275", "abs": "https://arxiv.org/abs/2506.03275", "authors": ["Austin Silveria", "Soham V. Govande", "Daniel Y. Fu"], "title": "Chipmunk: Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in\nhigh-quality image and video generation but incur substantial compute cost at\ninference. A common observation is that DiT latent noise vectors change slowly\nacross inference steps, which suggests that the DiT compute may be redundant\nacross steps. In this paper, we aim to speed up inference by reducing this\nredundancy, without additional training. We first study how activations change\nbetween steps in two state-of-the-art open-source DiTs. We find that just 5-25%\nof the values in attention and MLP explain 70-90% of the change in activations\nacross steps. This finding motivates our approach, Chipmunk, which uses dynamic\nsparsity at inference time to recompute only the fastest-changing intermediate\nactivations, while caching the rest. Dynamic sparsity introduces two systems\nchallenges: (1) sparse attention and MLP operations tend to underutilize GPU\ntensor cores; and (2) computing dynamic sparsity patterns at runtime and\ncaching activations both introduce overhead. To address these challenges,\nChipmunk first uses a voxel-based reordering of input tokens to introduce\ncolumn-wise sparsity. We implement column-sparse kernels utilizing efficient\nsparse gathers from global to shared GPU memory, achieving a 9.3x speedup at\n93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk\noverlaps the computation of sparsity patterns and cache updates with other\nparts of the computation (e.g., second layer of the MLP) to hide the extra\nlatency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on\nFLUX.1-dev without compromising generation quality. Furthermore, we show that\nChipmunk can be stacked on top of full step caching, achieving a 3.72x speedup\non HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev\nwith minimal quality impact.", "AI": {"tldr": "Diffusion Transformers (DiTs) \u5728\u9ad8\u54c1\u8d28\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u63a8\u7406\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u7814\u7a76\u53d1\u73b0\u4e2d\u95f4\u6fc0\u6d3b\u53d8\u5316\u7f13\u6162\uff0c\u63d0\u51fa\u52a8\u6001\u7a00\u758f\u65b9\u6cd5 Chipmunk \u4ee5\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u663e\u8457\u52a0\u901f\u63a8\u7406\u3002", "motivation": "DiTs \u5728\u63a8\u7406\u65f6\u5b58\u5728\u8ba1\u7b97\u5197\u4f59\uff0c\u5e0c\u671b\u901a\u8fc7\u52a8\u6001\u7a00\u758f\u6280\u672f\u51cf\u5c11\u5197\u4f59\uff0c\u52a0\u901f\u63a8\u7406\u800c\u4e0d\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002", "method": "Chipmunk \u901a\u8fc7\u52a8\u6001\u7a00\u758f\u4ec5\u8ba1\u7b97\u53d8\u5316\u6700\u5feb\u7684\u4e2d\u95f4\u6fc0\u6d3b\uff0c\u5e76\u7f13\u5b58\u5176\u4f59\u90e8\u5206\uff0c\u540c\u65f6\u4f18\u5316 GPU \u5229\u7528\u7387\u548c\u8ba1\u7b97\u5ef6\u8fdf\u3002", "result": "Chipmunk \u5728 HunyuanVideo \u548c FLUX \u7b49\u6a21\u578b\u4e0a\u5b9e\u73b0\u6700\u9ad8 3.72 \u500d\u52a0\u901f\uff0c\u4e14\u751f\u6210\u8d28\u91cf\u51e0\u4e4e\u65e0\u4e0b\u964d\u3002", "conclusion": "\u52a8\u6001\u7a00\u758f\u65b9\u6cd5 Chipmunk \u80fd\u6709\u6548\u51cf\u5c11 DiTs \u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u5197\u4f59\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "keywords": "Diffusion Transformers, \u63a8\u7406\u52a0\u901f, \u52a8\u6001\u7a00\u758f, Chipmunk, \u751f\u6210\u6a21\u578b"}}
{"id": "2506.03870", "pdf": "https://arxiv.org/pdf/2506.03870", "abs": "https://arxiv.org/abs/2506.03870", "authors": ["Mohd. Farhan Israk Soumik", "Syed Mhamudul Hasan", "Abdur R. Shahid"], "title": "Evaluating Apple Intelligence's Writing Tools for Privacy Against Large Language Model-Based Inference Attacks: Insights from Early Datasets", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "The misuse of Large Language Models (LLMs) to infer emotions from text for\nmalicious purposes, known as emotion inference attacks, poses a significant\nthreat to user privacy. In this paper, we investigate the potential of Apple\nIntelligence's writing tools, integrated across iPhone, iPad, and MacBook, to\nmitigate these risks through text modifications such as rewriting and tone\nadjustment. By developing early novel datasets specifically for this purpose,\nwe empirically assess how different text modifications influence LLM-based\ndetection. This capability suggests strong potential for Apple Intelligence's\nwriting tools as privacy-preserving mechanisms. Our findings lay the groundwork\nfor future adaptive rewriting systems capable of dynamically neutralizing\nsensitive emotional content to enhance user privacy. To the best of our\nknowledge, this research provides the first empirical analysis of Apple\nIntelligence's text-modification tools within a privacy-preservation context\nwith the broader goal of developing on-device, user-centric privacy-preserving\nmechanisms to protect against LLMs-based advanced inference attacks on deployed\nsystems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u82f9\u679c\u667a\u80fd\u5199\u4f5c\u5de5\u5177\u5982\u4f55\u901a\u8fc7\u6587\u672c\u4fee\u6539\uff08\u5982\u91cd\u5199\u548c\u8bed\u8c03\u8c03\u6574\uff09\u964d\u4f4e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u7528\u6237\u60c5\u7eea\u9690\u79c1\u7684\u5a01\u80c1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u88ab\u6ee5\u7528\u4e8e\u60c5\u7eea\u63a8\u65ad\u653b\u51fb\uff0c\u5a01\u80c1\u7528\u6237\u9690\u79c1\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u82f9\u679c\u667a\u80fd\u5199\u4f5c\u5de5\u5177\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e13\u95e8\u6570\u636e\u96c6\uff0c\u5b9e\u8bc1\u8bc4\u4f30\u6587\u672c\u4fee\u6539\u5bf9LLM\u68c0\u6d4b\u7684\u5f71\u54cd\u3002", "result": "\u82f9\u679c\u667a\u80fd\u5199\u4f5c\u5de5\u5177\u663e\u793a\u51fa\u4f5c\u4e3a\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u7684\u6f5c\u529b\uff0c\u7814\u7a76\u4e3a\u52a8\u6001\u4e2d\u548c\u654f\u611f\u60c5\u7eea\u5185\u5bb9\u7684\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u7814\u7a76\u9996\u6b21\u5728\u9690\u79c1\u4fdd\u62a4\u80cc\u666f\u4e0b\u5b9e\u8bc1\u5206\u6790\u82f9\u679c\u667a\u80fd\u6587\u672c\u4fee\u6539\u5de5\u5177\uff0c\u4e3a\u5f00\u53d1\u7528\u6237\u4e2d\u5fc3\u3001\u8bbe\u5907\u5185\u7684\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u60c5\u7eea\u63a8\u65ad\u653b\u51fb,\u9690\u79c1\u4fdd\u62a4,\u82f9\u679c\u667a\u80fd\u5199\u4f5c\u5de5\u5177,\u6587\u672c\u4fee\u6539"}}
{"id": "2506.03884", "pdf": "https://arxiv.org/pdf/2506.03884", "abs": "https://arxiv.org/abs/2506.03884", "authors": ["Utkarsh Pathak", "Chandra Sai Krishna Gunda", "Anusha Prakash", "Keshav Agarwal", "Hema A. Murthy"], "title": "Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages", "categories": ["cs.CL", "cs.CV", "I.5.4"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Text-to-speech (TTS) systems typically require high-quality studio data and\naccurate transcriptions for training. India has 1369 languages, with 22\nofficial using 13 scripts. Training a TTS system for all these languages, most\nof which have no digital resources, seems a Herculean task. Our work focuses on\nzero-shot synthesis, particularly for languages whose scripts and phonotactics\ncome from different families. The novelty of our work is in the augmentation of\na shared phone representation and modifying the text parsing rules to match the\nphonotactics of the target language, thus reducing the synthesiser overhead and\nenabling rapid adaptation. Intelligible and natural speech was generated for\nSanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging\nlinguistic connections across languages with suitable synthesisers. Evaluations\nconfirm the effectiveness of this approach, highlighting its potential to\nexpand speech technology access for under-represented languages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u5408\u6210\u7684TTS\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u7f3a\u4e4f\u6570\u5b57\u8d44\u6e90\u7684\u8bed\u8a00\uff0c\u901a\u8fc7\u5171\u4eab\u97f3\u7d20\u8868\u793a\u548c\u4fee\u6539\u6587\u672c\u89e3\u6790\u89c4\u5219\u6765\u5b9e\u73b0\u5feb\u901f\u9002\u914d\u3002", "motivation": "\u5370\u5ea6\u67091369\u79cd\u8bed\u8a00\uff0c\u5176\u4e2d\u5927\u591a\u6570\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6570\u5b57\u8d44\u6e90\uff0c\u4f20\u7edfTTS\u7cfb\u7edf\u96be\u4ee5\u5168\u9762\u8986\u76d6\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u4e3a\u96f6\u8d44\u6e90\u8bed\u8a00\u63d0\u4f9b\u8bed\u97f3\u5408\u6210\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u589e\u5f3a\u5171\u4eab\u97f3\u7d20\u8868\u793a\u548c\u8c03\u6574\u6587\u672c\u89e3\u6790\u89c4\u5219\uff0c\u4f7f\u5176\u9002\u5e94\u76ee\u6807\u8bed\u8a00\u7684\u97f3\u4f4d\u89c4\u5219\uff0c\u4ece\u800c\u964d\u4f4e\u5408\u6210\u5668\u590d\u6742\u5ea6\u5e76\u5b9e\u73b0\u5feb\u901f\u9002\u914d\u3002", "result": "\u6210\u529f\u4e3aSanskrit\u3001Maharashtrian\u548cCanara Konkani\u7b49\u8bed\u8a00\u751f\u6210\u4e86\u6e05\u6670\u81ea\u7136\u7684\u8bed\u97f3\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u96f6\u8d44\u6e90\u8bed\u8a00\u7684\u8bed\u97f3\u5408\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u9014\u5f84\uff0c\u6269\u5c55\u4e86\u8bed\u97f3\u6280\u672f\u7684\u53ef\u53ca\u6027\u3002", "keywords": "\u6587\u672c\u5230\u8bed\u97f3, \u96f6\u6837\u672c\u5408\u6210, \u4f4e\u8d44\u6e90\u8bed\u8a00, \u5171\u4eab\u97f3\u7d20\u8868\u793a, \u8bed\u97f3\u6280\u672f"}}
{"id": "2506.03889", "pdf": "https://arxiv.org/pdf/2506.03889", "abs": "https://arxiv.org/abs/2506.03889", "authors": ["Pau Vilimelis Aceituno", "Jack William Miller", "Noah Marti", "Youssef Farag", "Victor Boussange"], "title": "Temporal horizons in forecasting: a performance-learnability trade-off", "categories": ["cs.LG", "nlin.CD"], "comment": "33 pages, 12 figures", "summary": "When training autoregressive models for dynamical systems, a critical\nquestion arises: how far into the future should the model be trained to\npredict? Too short a horizon may miss long-term trends, while too long a\nhorizon can impede convergence due to accumulating prediction errors. In this\nwork, we formalize this trade-off by analyzing how the geometry of the loss\nlandscape depends on the training horizon. We prove that for chaotic systems,\nthe loss landscape's roughness grows exponentially with the training horizon,\nwhile for limit cycles, it grows linearly, making long-horizon training\ninherently challenging. However, we also show that models trained on long\nhorizons generalize well to short-term forecasts, whereas those trained on\nshort horizons suffer exponentially (resp. linearly) worse long-term\npredictions in chaotic (resp. periodic) systems. We validate our theory through\nnumerical experiments and discuss practical implications for selecting training\nhorizons. Our results provide a principled foundation for hyperparameter\noptimization in autoregressive forecasting models.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u5728\u52a8\u529b\u7cfb\u7edf\u4e2d\u8bad\u7ec3\u65f6\u7684\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u9009\u62e9\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u4e0d\u540c\u7cfb\u7edf\uff08\u6df7\u6c8c\u7cfb\u7edf\u4e0e\u6781\u9650\u73af\uff09\u4e2d\u635f\u5931\u51fd\u6570\u968f\u8bad\u7ec3\u8303\u56f4\u53d8\u5316\u7684\u51e0\u4f55\u7279\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8bad\u7ec3\u8303\u56f4\u5bf9\u9884\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5728\u8bad\u7ec3\u81ea\u56de\u5f52\u6a21\u578b\u65f6\uff0c\u9009\u62e9\u5408\u9002\u7684\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u56e0\u4e3a\u8303\u56f4\u8fc7\u77ed\u53ef\u80fd\u5ffd\u7565\u957f\u671f\u8d8b\u52bf\uff0c\u8fc7\u957f\u5219\u56e0\u8bef\u5dee\u7d2f\u79ef\u963b\u788d\u6536\u655b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8bad\u7ec3\u8303\u56f4\u5bf9\u635f\u5931\u51fd\u6570\u51e0\u4f55\u5f62\u72b6\u7684\u5f71\u54cd\uff0c\u7814\u7a76\u4e86\u6df7\u6c8c\u7cfb\u7edf\u548c\u6781\u9650\u73af\u7cfb\u7edf\u4e0b\u7684\u635f\u5931\u51fd\u6570\u7279\u6027\uff0c\u5e76\u8fdb\u884c\u4e86\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u635f\u5931\u51fd\u6570\u7c97\u7cd9\u5ea6\u968f\u8bad\u7ec3\u8303\u56f4\u6307\u6570\u589e\u957f\uff0c\u6781\u9650\u73af\u7cfb\u7edf\u4e2d\u7ebf\u6027\u589e\u957f\uff1b\u957f\u8303\u56f4\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u77ed\u671f\u9884\u6d4b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u77ed\u8303\u56f4\u8bad\u7ec3\u7684\u6a21\u578b\u957f\u671f\u9884\u6d4b\u6027\u80fd\u8f83\u5dee\u3002", "conclusion": "\u7814\u7a76\u4e3a\u81ea\u56de\u5f52\u9884\u6d4b\u6a21\u578b\u7684\u8d85\u53c2\u6570\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5efa\u8bae\u6839\u636e\u7cfb\u7edf\u7279\u6027\u9009\u62e9\u5408\u9002\u7684\u8bad\u7ec3\u8303\u56f4\u3002", "keywords": "\u81ea\u56de\u5f52\u6a21\u578b, \u52a8\u529b\u7cfb\u7edf, \u8bad\u7ec3\u8303\u56f4, \u635f\u5931\u51fd\u6570, \u6df7\u6c8c\u7cfb\u7edf, \u6781\u9650\u73af"}}
{"id": "2506.03887", "pdf": "https://arxiv.org/pdf/2506.03887", "abs": "https://arxiv.org/abs/2506.03887", "authors": ["Junyi Chen", "Shihao Bai", "Zaijun Wang", "Siyu Wu", "Chuheng Du", "Hailong Yang", "Ruihao Gong", "Shengzhong Liu", "Fan Wu", "Guihai Chen"], "title": "Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation", "categories": ["cs.CL"], "comment": "Published as a conference paper at ACL 2025", "summary": "Extensive LLM applications demand efficient structured generations,\nparticularly for LR(1) grammars, to produce outputs in specified formats (e.g.,\nJSON). Existing methods primarily parse LR(1) grammars into a pushdown\nautomaton (PDA), leading to runtime execution overhead for context-dependent\ntoken processing, especially inefficient under large inference batches. To\naddress these issues, we propose Pre$^3$ that exploits deterministic pushdown\nautomata (DPDA) to optimize the constrained LLM decoding efficiency. First, by\nprecomputing prefix-conditioned edges during the preprocessing, Pre$^3$ enables\nahead-of-time edge analysis and thus makes parallel transition processing\npossible. Second, by leveraging the prefix-conditioned edges, Pre$^3$\nintroduces a novel approach that transforms LR(1) transition graphs into DPDA,\neliminating the need for runtime path exploration and achieving edge\ntransitions with minimal overhead. Pre$^3$ can be seamlessly integrated into\nstandard LLM inference frameworks, reducing time per output token (TPOT) by up\nto 40% and increasing throughput by up to 36% in our experiments. Our code is\navailable at https://github.com/ModelTC/lightllm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPre$^3$\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u786e\u5b9a\u6027\u4e0b\u63a8\u81ea\u52a8\u673a\uff08DPDA\uff09\u4f18\u5316LLM\u89e3\u7801\u6548\u7387\uff0c\u51cf\u5c11\u8fd0\u884c\u65f6\u5f00\u9500\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684LR(1)\u8bed\u6cd5\u5206\u6790\u65b9\u6cd5\u56e0\u5176\u8fd0\u884c\u65f6\u6267\u884c\u5f00\u9500\u5927\uff0c\u5c24\u5176\u5728\u6279\u91cf\u63a8\u7406\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u7ed3\u6784\u5316\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u9884\u5904\u7406\u9884\u5148\u8ba1\u7b97\u524d\u7f00\u6761\u4ef6\u8fb9\uff0c\u5b9e\u73b0\u5e76\u884c\u8f6c\u6362\u5904\u7406\uff0c\u5e76\u5c06LR(1)\u8f6c\u6362\u56fe\u8f6c\u5316\u4e3aDPDA\uff0c\u907f\u514d\u8fd0\u884c\u65f6\u8def\u5f84\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPre$^3$\u5c06\u6bcf\u4e2a\u8f93\u51fa\u4ee4\u724c\u7684\u65f6\u95f4\uff08TPOT\uff09\u51cf\u5c11\u4e8640%\uff0c\u541e\u5410\u91cf\u63d0\u9ad8\u4e8636%\u3002", "conclusion": "Pre$^3$\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709LLM\u63a8\u7406\u6846\u67b6\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "keywords": "LLM, LR(1) grammars, DPDA, structured generation, efficiency optimization"}}
{"id": "2506.03898", "pdf": "https://arxiv.org/pdf/2506.03898", "abs": "https://arxiv.org/abs/2506.03898", "authors": ["Pierre-Fran\u00e7ois Massiani", "Christian Fiedler", "Lukas Haverbeck", "Friedrich Solowjow", "Sebastian Trimpe"], "title": "A kernel conditional two-sample test", "categories": ["cs.LG", "stat.ML"], "comment": "40 pages, 8 figures, 8 tables. Under review", "summary": "We propose a framework for hypothesis testing on conditional probability\ndistributions, which we then use to construct conditional two-sample\nstatistical tests. These tests identify the inputs -- called covariates in this\ncontext -- where two conditional expectations differ with high probability. Our\nkey idea is to transform confidence bounds of a learning method into a\nconditional two-sample test, and we instantiate this principle for kernel ridge\nregression (KRR) and conditional kernel mean embeddings. We generalize existing\npointwise-in-time or time-uniform confidence bounds for KRR to\npreviously-inaccessible yet essential cases such as infinite-dimensional\noutputs with non-trace-class kernels. These bounds enable circumventing the\nneed for independent data in our statistical tests, since they allow online\nsampling. We also introduce bootstrapping schemes leveraging the parametric\nform of testing thresholds identified in theory to avoid tuning inaccessible\nparameters, making our method readily applicable in practice. Such conditional\ntwo-sample tests are especially relevant in applications where data arrive\nsequentially or non-independently, or when output distributions vary with\noperational parameters. We demonstrate their utility through examples in\nprocess monitoring and comparison of dynamical systems. Overall, our results\nestablish a comprehensive foundation for conditional two-sample testing, from\ntheoretical guarantees to practical implementation, and advance the\nstate-of-the-art on the concentration of vector-valued least squares\nestimation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6761\u4ef6\u6982\u7387\u5206\u5e03\u5047\u8bbe\u68c0\u9a8c\u7684\u6846\u67b6\uff0c\u5e76\u6784\u5efa\u4e86\u6761\u4ef6\u53cc\u6837\u672c\u7edf\u8ba1\u68c0\u9a8c\uff0c\u7528\u4e8e\u8bc6\u522b\u4e24\u4e2a\u6761\u4ef6\u671f\u671b\u5dee\u5f02\u5927\u7684\u8f93\u5165\u53d8\u91cf\u3002\u65b9\u6cd5\u5c06\u5b66\u4e60\u65b9\u6cd5\u7684\u7f6e\u4fe1\u8fb9\u754c\u8f6c\u6362\u4e3a\u6761\u4ef6\u53cc\u6837\u672c\u68c0\u9a8c\uff0c\u9002\u7528\u4e8e\u6838\u5cad\u56de\u5f52\u548c\u6761\u4ef6\u6838\u5747\u503c\u5d4c\u5165\u3002\u8fd8\u5f15\u5165\u4e86\u65e0\u9700\u8c03\u53c2\u7684\u81ea\u4e3e\u65b9\u6848\uff0c\u9002\u7528\u6027\u5e7f\u6cdb\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u6761\u4ef6\u6982\u7387\u5206\u5e03\u7684\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u987a\u5e8f\u6216\u975e\u72ec\u7acb\u5230\u8fbe\u65f6\uff0c\u4ee5\u53ca\u8f93\u51fa\u5206\u5e03\u968f\u64cd\u4f5c\u53c2\u6570\u53d8\u5316\u7684\u5e94\u7528\u573a\u666f\u3002", "method": "\u65b9\u6cd5\u5c06\u5b66\u4e60\u65b9\u6cd5\u7684\u7f6e\u4fe1\u8fb9\u754c\u8f6c\u5316\u4e3a\u6761\u4ef6\u53cc\u6837\u672c\u68c0\u9a8c\uff0c\u9002\u7528\u4e8e\u6838\u5cad\u56de\u5f52\u548c\u6761\u4ef6\u6838\u5747\u503c\u5d4c\u5165\uff0c\u5e76\u5f15\u5165\u81ea\u4e3e\u65b9\u6848\u907f\u514d\u8c03\u53c2\u3002", "result": "\u63d0\u51fa\u4e86\u901a\u7528\u6761\u4ef6\u53cc\u6837\u672c\u68c0\u9a8c\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u975e\u72ec\u7acb\u6570\u636e\uff0c\u4e14\u5728\u8fc7\u7a0b\u76d1\u63a7\u548c\u52a8\u6001\u7cfb\u7edf\u6bd4\u8f83\u4e2d\u5c55\u793a\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6761\u4ef6\u53cc\u6837\u672c\u68c0\u9a8c\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u65b9\u6848\uff0c\u5e76\u63a8\u8fdb\u4e86\u5411\u91cf\u503c\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "keywords": "\u5047\u8bbe\u68c0\u9a8c, \u6761\u4ef6\u6982\u7387\u5206\u5e03, \u6838\u5cad\u56de\u5f52, \u6838\u5747\u503c\u5d4c\u5165, \u81ea\u4e3e"}}
{"id": "2506.03901", "pdf": "https://arxiv.org/pdf/2506.03901", "abs": "https://arxiv.org/abs/2506.03901", "authors": ["Yuxin Zhang", "Yan Wang", "Yongrui Chen", "Shenyu Zhang", "Xinbang Dai", "Sheng Bi", "Guilin Qi"], "title": "Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of Retrieval Noise Erosion in RAG Systems", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by incorporating external retrieved information, mitigating issues such\nas hallucination and outdated knowledge.\n  However, RAG systems are highly sensitive to retrieval noise prevalent in\nreal-world scenarios.\n  Existing benchmarks fail to emulate the complex and heterogeneous noise\ndistributions encountered in real-world retrieval environments, undermining\nreliable robustness assessment.\n  In this paper, we define four categories of retrieval noise based on\nlinguistic properties and noise characteristics, aiming to reflect the\nheterogeneity of noise in real-world scenarios.\n  Building on this, we introduce Magic Mushroom, a benchmark for replicating\n\"magic mushroom\" noise: contexts that appear relevant on the surface but\ncovertly mislead RAG systems.\n  Magic Mushroom comprises 7,468 single-hop and 3,925 multi-hop question-answer\npairs.\n  More importantly, Magic Mushroom enables researchers to flexibly configure\ncombinations of retrieval noise according to specific research objectives or\napplication scenarios, allowing for highly controlled evaluation setups.\n  We evaluate LLM generators of varying parameter scales and classic RAG\ndenoising strategies under diverse noise distributions to investigate their\nperformance dynamics during progressive noise encroachment.\n  Our analysis reveals that both generators and denoising strategies have\nsignificant room for improvement and exhibit extreme sensitivity to noise\ndistributions.\n  Magic Mushroom emerges as a promising tool for evaluating and advancing\nnoise-robust RAG systems, accelerating their widespread deployment in\nreal-world applications.\n  The Magic Mushroom benchmark is available at the\nhttps://drive.google.com/file/d/1aP5kyPuk4L-L_uoI6T9UhxuTyt8oMqjT/view?usp=sharing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMagic Mushroom\u57fa\u51c6\uff0c\u7528\u4e8e\u6a21\u62df\u771f\u5b9e\u68c0\u7d22\u566a\u58f0\uff0c\u8bc4\u4f30RAG\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u751f\u6210\u5668\u548c\u53bb\u566a\u7b56\u7565\u5747\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u672a\u80fd\u6a21\u62df\u771f\u5b9e\u68c0\u7d22\u73af\u5883\u4e2d\u7684\u590d\u6742\u566a\u58f0\u5206\u5e03\uff0c\u5f71\u54cd\u4e86RAG\u7cfb\u7edf\u9c81\u68d2\u6027\u7684\u53ef\u9760\u8bc4\u4f30\u3002", "method": "\u5b9a\u4e49\u4e86\u56db\u7c7b\u68c0\u7d22\u566a\u58f0\uff0c\u6784\u5efaMagic Mushroom\u57fa\u51c6\uff0c\u5305\u542b\u5355\u8df3\u548c\u591a\u8df3\u95ee\u7b54\u5bf9\uff0c\u652f\u6301\u566a\u58f0\u914d\u7f6e\u7075\u6d3b\u8c03\u6574\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u751f\u6210\u5668\u548c\u53bb\u566a\u7b56\u7565\u5bf9\u566a\u58f0\u5206\u5e03\u6781\u4e3a\u654f\u611f\uff0c\u6027\u80fd\u6709\u663e\u8457\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "Magic Mushroom\u662f\u8bc4\u4f30\u548c\u63d0\u5347\u566a\u58f0\u9c81\u68d2\u6027RAG\u7cfb\u7edf\u7684\u6709\u6548\u5de5\u5177\u3002", "keywords": "Retrieval-Augmented Generation, noise robustness, benchmark, Magic Mushroom"}}
{"id": "2506.03910", "pdf": "https://arxiv.org/pdf/2506.03910", "abs": "https://arxiv.org/abs/2506.03910", "authors": ["Shyam Prabhu", "P Akshay Kumar", "Antov Selwinston", "Pavan Taduvai", "Shreya Bairi", "Rohit Batra"], "title": "Enhancing Experimental Efficiency in Materials Design: A Comparative Study of Taguchi and Machine Learning Methods", "categories": ["cs.LG"], "comment": "7 pages, 3 figures", "summary": "Materials design problems often require optimizing multiple variables,\nrendering full factorial exploration impractical. Design of experiment (DOE)\nmethods, such as Taguchi technique, are commonly used to efficiently sample the\ndesign space but they inherently lack the ability to capture non-linear\ndependency of process variables. In this work, we demonstrate how machine\nlearning (ML) methods can be used to overcome these limitations. We compare the\nperformance of Taguchi method against an active learning based Gaussian process\nregression (GPR) model in a wire arc additive manufacturing (WAAM) process to\naccurately predict aspects of bead geometry, including penetration depth, bead\nwidth, and height. While Taguchi method utilized a three-factor, five-level L25\northogonal array to suggest weld parameters, the GPR model used an\nuncertainty-based exploration acquisition function coupled with latin hypercube\nsampling for initial training data. Accuracy and efficiency of both models was\nevaluated on 15 test cases, with GPR outperforming Taguchi in both metrics.\nThis work applies to broader materials processing domain requiring efficient\nexploration of complex parameters.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86Taguchi\u65b9\u6cd5\u4e0e\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08GPR\uff09\u5728\u9884\u6d4bWAAM\u5de5\u827a\u4e2d\u710a\u73e0\u51e0\u4f55\u7279\u5f81\u7684\u6548\u679c\uff0c\u53d1\u73b0GPR\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8eTaguchi\u65b9\u6cd5\u3002", "motivation": "\u6750\u6599\u8bbe\u8ba1\u95ee\u9898\u901a\u5e38\u6d89\u53ca\u591a\u53d8\u91cf\u4f18\u5316\uff0c\u4f20\u7edf\u7684DOE\u65b9\u6cd5\u5982Taguchi\u6280\u672f\u867d\u80fd\u9ad8\u6548\u91c7\u6837\uff0c\u4f46\u65e0\u6cd5\u6355\u6349\u53d8\u91cf\u7684\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u56e0\u6b64\u63a2\u7d22ML\u65b9\u6cd5\u4ee5\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86Taguchi\u65b9\u6cd5\uff08\u4f7f\u7528L25\u6b63\u4ea4\u9635\u5217\uff09\u548cGPR\u6a21\u578b\uff08\u7ed3\u5408\u62c9\u4e01\u8d85\u7acb\u65b9\u91c7\u6837\u548c\u4e0d\u786e\u5b9a\u6027\u63a2\u7d22\uff09\u5728WAAM\u5de5\u827a\u4e2d\u7684\u5e94\u7528\u3002", "result": "GPR\u6a21\u578b\u572815\u4e2a\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4f18\u4e8eTaguchi\u65b9\u6cd5\u3002", "conclusion": "GPR\u65b9\u6cd5\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u6548\u63a2\u7d22\u590d\u6742\u53c2\u6570\u7684\u6750\u6599\u52a0\u5de5\u9886\u57df\uff0c\u5c24\u5176\u662f\u5728\u975e\u7ebf\u6027\u5173\u7cfb\u663e\u8457\u7684\u60c5\u51b5\u4e0b\u3002", "keywords": "\u6750\u6599\u8bbe\u8ba1,\u673a\u5668\u5b66\u4e60,\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52,Taguchi\u65b9\u6cd5,WAAM"}}
{"id": "2506.03902", "pdf": "https://arxiv.org/pdf/2506.03902", "abs": "https://arxiv.org/abs/2506.03902", "authors": ["Eleftheria Tsipidi", "Samuel Kiegeland", "Franz Nowak", "Tianyang Xu", "Ethan Wilcox", "Alex Warstadt", "Ryan Cotterell", "Mario Giulianelli"], "title": "The Harmonic Structure of Information Contours", "categories": ["cs.CL"], "comment": "ACL 2025 (main conference)", "summary": "The uniform information density (UID) hypothesis proposes that speakers aim\nto distribute information evenly throughout a text, balancing production effort\nand listener comprehension difficulty. However, language typically does not\nmaintain a strictly uniform information rate; instead, it fluctuates around a\nglobal average. These fluctuations are often explained by factors such as\nsyntactic constraints, stylistic choices, or audience design. In this work, we\nexplore an alternative perspective: that these fluctuations may be influenced\nby an implicit linguistic pressure towards periodicity, where the information\nrate oscillates at regular intervals, potentially across multiple frequencies\nsimultaneously. We apply harmonic regression and introduce a novel extension\ncalled time scaling to detect and test for such periodicity in information\ncontours. Analyzing texts in English, Spanish, German, Dutch, Basque, and\nBrazilian Portuguese, we find consistent evidence of periodic patterns in\ninformation rate. Many dominant frequencies align with discourse structure,\nsuggesting these oscillations reflect meaningful linguistic organization.\nBeyond highlighting the connection between information rate and discourse\nstructure, our approach offers a general framework for uncovering structural\npressures at various levels of linguistic granularity.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4fe1\u606f\u5bc6\u5ea6\u5728\u8bed\u8a00\u4e2d\u5b58\u5728\u5468\u671f\u6027\u6ce2\u52a8\uff0c\u4e0e\u8bdd\u8bed\u7ed3\u6784\u76f8\u5173\u3002", "motivation": "\u63a2\u7d22\u4fe1\u606f\u5bc6\u5ea6\u6ce2\u52a8\u662f\u5426\u53d7\u5468\u671f\u6027\u8bed\u8a00\u538b\u529b\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u8c10\u6ce2\u56de\u5f52\u548c\u65f6\u95f4\u5c3a\u5ea6\u6269\u5c55\u5206\u6790\u591a\u8bed\u8a00\u6587\u672c\u3002", "result": "\u53d1\u73b0\u4fe1\u606f\u7387\u4e2d\u5b58\u5728\u5468\u671f\u6027\u6a21\u5f0f\uff0c\u4e0e\u8bdd\u8bed\u7ed3\u6784\u4e00\u81f4\u3002", "conclusion": "\u4fe1\u606f\u5bc6\u5ea6\u7684\u5468\u671f\u6027\u53cd\u6620\u4e86\u8bed\u8a00\u7ec4\u7ec7\u7684\u6df1\u5c42\u7ed3\u6784\u3002", "keywords": "\u4fe1\u606f\u5bc6\u5ea6\u3001\u5468\u671f\u6027\u3001\u8c10\u6ce2\u56de\u5f52\u3001\u8bdd\u8bed\u7ed3\u6784\u3001\u591a\u8bed\u8a00\u5206\u6790"}}
{"id": "2506.03911", "pdf": "https://arxiv.org/pdf/2506.03911", "abs": "https://arxiv.org/abs/2506.03911", "authors": ["Chamsi Hssaine", "Yichun Hu", "Ciara Pike-Burke"], "title": "Learning Fair And Effective Points-Based Rewards Programs", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Points-based rewards programs are a prevalent way to incentivize customer\nloyalty; in these programs, customers who make repeated purchases from a seller\naccumulate points, working toward eventual redemption of a free reward. These\nprograms have recently come under scrutiny due to accusations of unfair\npractices in their implementation. Motivated by these concerns, we study the\nproblem of fairly designing points-based rewards programs, with a focus on two\nobstacles that put fairness at odds with their effectiveness. First, due to\ncustomer heterogeneity, the seller should set different redemption thresholds\nfor different customers to generate high revenue. Second, the relationship\nbetween customer behavior and the number of accumulated points is typically\nunknown; this requires experimentation which may unfairly devalue customers'\npreviously earned points. We first show that an individually fair rewards\nprogram that uses the same redemption threshold for all customers suffers a\nloss in revenue of at most a factor of $1+\\ln 2$, compared to the optimal\npersonalized strategy that differentiates between customers. We then tackle the\nproblem of designing temporally fair learning algorithms in the presence of\ndemand uncertainty. Toward this goal, we design a learning algorithm that\nlimits the risk of point devaluation due to experimentation by only changing\nthe redemption threshold $O(\\log T)$ times, over a horizon of length $T$. This\nalgorithm achieves the optimal (up to polylogarithmic factors)\n$\\widetilde{O}(\\sqrt{T})$ regret in expectation. We then modify this algorithm\nto only ever decrease redemption thresholds, leading to improved fairness at a\ncost of only a constant factor in regret. Extensive numerical experiments show\nthe limited value of personalization in average-case settings, in addition to\ndemonstrating the strong practical performance of our proposed learning\nalgorithms.", "AI": {"tldr": "\u7814\u7a76\u516c\u5e73\u8bbe\u8ba1\u79ef\u5206\u5956\u52b1\u8ba1\u5212\u7684\u95ee\u9898\uff0c\u5206\u6790\u4e86\u5ba2\u6237\u5f02\u8d28\u6027\u548c\u9700\u6c42\u4e0d\u786e\u5b9a\u6027\u5bf9\u516c\u5e73\u6027\u4e0e\u6536\u5165\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u7b97\u6cd5\uff0c\u4ee5\u51cf\u5c11\u5b9e\u9a8c\u5bf9\u5ba2\u6237\u79ef\u5206\u8d2c\u503c\u7684\u4e0d\u516c\u5e73\u5f71\u54cd\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u79ef\u5206\u5956\u52b1\u8ba1\u5212\u56e0\u4e0d\u516c\u5e73\u64cd\u4f5c\u53d7\u5230\u8d28\u7591\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5982\u4f55\u516c\u5e73\u8bbe\u8ba1\u8fd9\u4e9b\u8ba1\u5212\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u6709\u6548\u6027\u3002", "method": "\u7814\u7a76\u9996\u5148\u5206\u6790\u4e86\u4e2a\u4f53\u516c\u5e73\u7684\u5956\u52b1\u8ba1\u5212\u7684\u6536\u5165\u635f\u5931\uff0c\u968f\u540e\u8bbe\u8ba1\u4e86\u9650\u5236\u5b9e\u9a8c\u6b21\u6570\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u4ee5\u51cf\u5c11\u79ef\u5206\u8d2c\u503c\u7684\u4e0d\u516c\u5e73\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e2a\u4f53\u516c\u5e73\u7684\u5956\u52b1\u8ba1\u5212\u6536\u5165\u7684\u635f\u5931\u6700\u591a\u4e3a\u6700\u4f18\u4e2a\u6027\u5316\u7b56\u7565\u76841+ln2\u500d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u9700\u5bf9\u6570\u6b21\u8c03\u6574\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u5176\u5728\u9057\u61be\u4e0a\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u7814\u7a76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5728\u5e73\u5747\u60c5\u51b5\u4e0b\u4e2a\u6027\u5316\u7684\u4ef7\u503c\u6709\u9650\uff0c\u540c\u65f6\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "keywords": "\u79ef\u5206\u5956\u52b1\u8ba1\u5212, \u516c\u5e73\u6027, \u5ba2\u6237\u5f02\u8d28\u6027, \u5b66\u4e60\u7b97\u6cd5, \u9057\u61be"}}
{"id": "2506.03913", "pdf": "https://arxiv.org/pdf/2506.03913", "abs": "https://arxiv.org/abs/2506.03913", "authors": ["Claire Barale", "Michael Rovatsos", "Nehal Bhuta"], "title": "When Fairness Isn't Statistical: The Limits of Machine Learning in Evaluating Legal Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Legal decisions are increasingly evaluated for fairness, consistency, and\nbias using machine learning (ML) techniques. In high-stakes domains like\nrefugee adjudication, such methods are often applied to detect disparities in\noutcomes. Yet it remains unclear whether statistical methods can meaningfully\nassess fairness in legal contexts shaped by discretion, normative complexity,\nand limited ground truth.\n  In this paper, we empirically evaluate three common ML approaches\n(feature-based analysis, semantic clustering, and predictive modeling) on a\nlarge, real-world dataset of 59,000+ Canadian refugee decisions (AsyLex). Our\nexperiments show that these methods produce divergent and sometimes\ncontradictory signals, that predictive modeling often depends on contextual and\nprocedural features rather than legal features, and that semantic clustering\nfails to capture substantive legal reasoning.\n  We show limitations of statistical fairness evaluation, challenge the\nassumption that statistical regularity equates to fairness, and argue that\ncurrent computational approaches fall short of evaluating fairness in legally\ndiscretionary domains. We argue that evaluating fairness in law requires\nmethods grounded not only in data, but in legal reasoning and institutional\ncontext.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u5728\u6cd5\u5f8b\u516c\u5e73\u6027\u8bc4\u4f30\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63ed\u793a\u4e86\u5e38\u7528\u65b9\u6cd5\u5728\u96be\u6c11\u88c1\u51b3\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u7ed3\u5408\u6cd5\u5f8b\u63a8\u7406\u548c\u5236\u5ea6\u80cc\u666f\u6765\u8bc4\u4f30\u516c\u5e73\u6027\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u5728\u6cd5\u5f8b\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u5982\u4f55\u6709\u6548\u8bc4\u4f30\u516c\u5e73\u6027\u3001\u4e00\u81f4\u6027\u548c\u504f\u89c1\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002\u7279\u522b\u662f\u5728\u96be\u6c11\u88c1\u51b3\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u73b0\u6709\u65b9\u6cd5\u662f\u5426\u80fd\u771f\u6b63\u53cd\u6620\u516c\u5e73\u6027\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u4e09\u79cd\u5e38\u89c1\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u57fa\u4e8e\u7279\u5f81\u7684\u5206\u6790\u3001\u8bed\u4e49\u805a\u7c7b\u548c\u9884\u6d4b\u5efa\u6a21\uff09\u5bf959,000\u591a\u4efd\u52a0\u62ff\u5927\u96be\u6c11\u5224\u51b3\u6570\u636e\uff08AsyLex\uff09\u8fdb\u884c\u4e86\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4ea7\u751f\u7684\u7ed3\u679c\u5b58\u5728\u5206\u6b67\u751a\u81f3\u77db\u76fe\uff1b\u9884\u6d4b\u5efa\u6a21\u66f4\u4f9d\u8d56\u4e0a\u4e0b\u6587\u548c\u7a0b\u5e8f\u6027\u7279\u5f81\u800c\u975e\u6cd5\u5f8b\u7279\u5f81\uff1b\u8bed\u4e49\u805a\u7c7b\u672a\u80fd\u6355\u6349\u5b9e\u8d28\u6027\u7684\u6cd5\u5f8b\u63a8\u7406\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u7edf\u8ba1\u65b9\u6cd5\u5728\u8bc4\u4f30\u6cd5\u5f8b\u516c\u5e73\u6027\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u8ba4\u4e3a\u5f53\u524d\u7684\u8ba1\u673a\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u6cd5\u5f8b\u81ea\u7531\u88c1\u91cf\u9886\u57df\u7684\u516c\u5e73\u6027\uff0c\u5fc5\u987b\u7ed3\u5408\u6cd5\u5f8b\u63a8\u7406\u548c\u5236\u5ea6\u80cc\u666f\u3002", "keywords": "\u673a\u5668\u5b66\u4e60, \u6cd5\u5f8b\u516c\u5e73\u6027, \u96be\u6c11\u88c1\u51b3, \u7edf\u8ba1\u65b9\u6cd5, \u5b9e\u8bc1\u5206\u6790"}}
{"id": "2506.03914", "pdf": "https://arxiv.org/pdf/2506.03914", "abs": "https://arxiv.org/abs/2506.03914", "authors": ["Eduardo Santos Escriche", "Stefanie Jegelka"], "title": "Learning equivariant models by discovering symmetries with learnable augmentations", "categories": ["cs.LG"], "comment": null, "summary": "Recently, a trend has emerged that favors learning relevant symmetries from\ndata in geometric domains instead of designing constrained architectures. To do\nso, two popular options are (1) to modify the training protocol, e.g., with a\nspecific loss and data augmentations (soft equivariance), or (2) to ignore\nequivariance and infer it only implicitly. However, both options have\nlimitations: soft equivariance requires a priori knowledge about relevant\nsymmetries, while inferring symmetries merely via the task and larger data\nlacks interpretability. To address both limitations, we propose SEMoLA, an\nend-to-end approach that jointly (1) discovers a priori unknown symmetries in\nthe data via learnable data augmentations, and (2) softly encodes the\nrespective approximate equivariance into an arbitrary unconstrained model.\nHence, it does not need prior knowledge about symmetries, it offers\ninterpretability, and it maintains robustness to distribution shifts.\nEmpirically, we demonstrate the ability of SEMoLA to robustly discover relevant\nsymmetries while achieving high prediction accuracy across various datasets,\nencompassing multiple data modalities and underlying symmetry groups.", "AI": {"tldr": "SEMoLA\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6570\u636e\u589e\u5f3a\u53d1\u73b0\u6570\u636e\u4e2d\u7684\u5bf9\u79f0\u6027\uff0c\u5e76\u5c06\u5176\u7f16\u7801\u5230\u6a21\u578b\u4e2d\uff0c\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u4e14\u5177\u6709\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u53d1\u73b0\u548c\u5e94\u7528\u5bf9\u79f0\u6027\u65f6\u5bf9\u5148\u9a8c\u77e5\u8bc6\u7684\u4f9d\u8d56\u53ca\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6570\u636e\u589e\u5f3a\u53d1\u73b0\u672a\u77e5\u5bf9\u79f0\u6027\uff0c\u5e76\u5c06\u5176\u8f6f\u7f16\u7801\u5230\u4efb\u610f\u65e0\u7ea6\u675f\u6a21\u578b\u4e2d\u3002", "result": "SEMoLA\u80fd\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u6570\u636e\u6a21\u6001\u4e2d\u7a33\u5065\u5730\u53d1\u73b0\u76f8\u5173\u5bf9\u79f0\u6027\u5e76\u4fdd\u6301\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "SEMoLA\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\uff0c\u5177\u6709\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u80fd\u5e94\u5bf9\u5206\u5e03\u53d8\u5316\u3002", "keywords": "\u5bf9\u79f0\u6027\u5b66\u4e60,\u6570\u636e\u589e\u5f3a,\u8f6f\u7f16\u7801,\u53ef\u89e3\u91ca\u6027"}}
{"id": "2506.03916", "pdf": "https://arxiv.org/pdf/2506.03916", "abs": "https://arxiv.org/abs/2506.03916", "authors": ["Agostina Calabrese", "Tom Sherborne", "Bj\u00f6rn Ross", "Mirella Lapata"], "title": "Compositional Generalisation for Explainable Hate Speech Detection", "categories": ["cs.CL"], "comment": null, "summary": "Hate speech detection is key to online content moderation, but current models\nstruggle to generalise beyond their training data. This has been linked to\ndataset biases and the use of sentence-level labels, which fail to teach models\nthe underlying structure of hate speech. In this work, we show that even when\nmodels are trained with more fine-grained, span-level annotations (e.g.,\n\"artists\" is labeled as target and \"are parasites\" as dehumanising comparison),\nthey struggle to disentangle the meaning of these labels from the surrounding\ncontext. As a result, combinations of expressions that deviate from those seen\nduring training remain particularly difficult for models to detect. We\ninvestigate whether training on a dataset where expressions occur with equal\nfrequency across all contexts can improve generalisation. To this end, we\ncreate U-PLEAD, a dataset of ~364,000 synthetic posts, along with a novel\ncompositional generalisation benchmark of ~8,000 manually validated posts.\nTraining on a combination of U-PLEAD and real data improves compositional\ngeneralisation while achieving state-of-the-art performance on the\nhuman-sourced PLEAD.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u5efa\u5e73\u8861\u6570\u636e\u96c6U-PLEAD\uff0c\u7ed3\u5408\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u4e4b\u5916\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e3b\u8981\u7531\u4e8e\u6570\u636e\u96c6\u504f\u89c1\u548c\u53e5\u5b50\u7ea7\u6807\u7b7e\u7684\u5c40\u9650\u6027\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u7ec6\u7c92\u5ea6\u7684span\u7ea7\u6ce8\u91ca\uff0c\u63d0\u51faU-PLEAD\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u7ed3\u5408\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728U-PLEAD\u548c\u771f\u5b9e\u6570\u636e\u8054\u5408\u8bad\u7ec3\u4e0b\uff0c\u6a21\u578b\u5728\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u4eba\u6e90\u6570\u636e\u96c6PLEAD\u4e0a\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5e73\u8861\u6570\u636e\u96c6\u7684\u8bad\u7ec3\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b,\u6cdb\u5316\u80fd\u529b,U-PLEAD,\u5408\u6210\u6570\u636e,\u7ec4\u5408\u6cdb\u5316"}}
{"id": "2506.03350", "pdf": "https://arxiv.org/pdf/2506.03350", "abs": "https://arxiv.org/abs/2506.03350", "authors": ["Eliot Krzysztof Jones", "Alexander Robey", "Andy Zou", "Zachary Ravichandran", "George J. Pappas", "Hamed Hassani", "Matt Fredrikson", "J. Zico Kolter"], "title": "Adversarial Attacks on Robotic Vision Language Action Models", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "The emergence of vision-language-action models (VLAs) for end-to-end control\nis reshaping the field of robotics by enabling the fusion of multimodal sensory\ninputs at the billion-parameter scale. The capabilities of VLAs stem primarily\nfrom their architectures, which are often based on frontier large language\nmodels (LLMs). However, LLMs are known to be susceptible to adversarial misuse,\nand given the significant physical risks inherent to robotics, questions remain\nregarding the extent to which VLAs inherit these vulnerabilities. Motivated by\nthese concerns, in this work we initiate the study of adversarial attacks on\nVLA-controlled robots. Our main algorithmic contribution is the adaptation and\napplication of LLM jailbreaking attacks to obtain complete control authority\nover VLAs. We find that textual attacks, which are applied once at the\nbeginning of a rollout, facilitate full reachability of the action space of\ncommonly used VLAs and often persist over longer horizons. This differs\nsignificantly from LLM jailbreaking literature, as attacks in the real world do\nnot have to be semantically linked to notions of harm. We make all code\navailable at https://github.com/eliotjones1/robogcg .", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08VLA\uff09\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u5bf9\u6297\u653b\u51fb\u95ee\u9898\uff0c\u901a\u8fc7\u5e94\u7528LLM\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u53d1\u73b0\u6587\u672c\u653b\u51fb\u53ef\u4ee5\u5b8c\u5168\u63a7\u5236VLA\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4e14\u653b\u51fb\u6548\u679c\u6301\u4e45\u3002", "motivation": "\u7531\u4e8eVLA\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u53ca\u5176\u6f5c\u5728\u7684\u7269\u7406\u98ce\u9669\uff0c\u4f5c\u8005\u62c5\u5fe7VLA\u53ef\u80fd\u7ee7\u627fLLM\u7684\u5bf9\u6297\u6027\u6f0f\u6d1e\uff0c\u56e0\u6b64\u542f\u52a8\u4e86\u5bf9VLA\u5bf9\u6297\u653b\u51fb\u7684\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u8c03\u6574\u5e76\u5e94\u7528LLM\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u5bf9VLA\u8fdb\u884c\u5bf9\u6297\u653b\u51fb\u5b9e\u9a8c\uff0c\u7814\u7a76\u5176\u5b89\u5168\u6f0f\u6d1e\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u901a\u8fc7\u6587\u672c\u653b\u51fb\u53ef\u4ee5\u5b8c\u5168\u63a7\u5236VLA\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4e14\u653b\u51fb\u6548\u679c\u5728\u957f\u65f6\u95f4\u5185\u6301\u7eed\u6709\u6548\u3002", "conclusion": "VLA\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u5b58\u5728\u663e\u8457\u7684\u5bf9\u6297\u653b\u51fb\u6f0f\u6d1e\uff0c\u8fd9\u4e0e\u4f20\u7edfLLM\u653b\u51fb\u7684\u6027\u8d28\u4e0d\u540c\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u9632\u62a4\u63aa\u65bd\u3002", "keywords": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08VLA\uff09\u3001\u5bf9\u6297\u653b\u51fb\u3001\u673a\u5668\u4eba\u63a7\u5236\u3001LLM\u8d8a\u72f1\u3001\u6587\u672c\u653b\u51fb"}}
{"id": "2506.03919", "pdf": "https://arxiv.org/pdf/2506.03919", "abs": "https://arxiv.org/abs/2506.03919", "authors": ["Lorenz Kummer", "Samir Moustafa", "Anatol Ehrlich", "Franka Bause", "Nikolaus Suess", "Wilfried N. Gansterer", "Nils M. Kriege"], "title": "Weisfeiler and Leman Go Gambling: Why Expressive Lottery Tickets Win", "categories": ["cs.LG"], "comment": "Accepted at ICML 2025", "summary": "The lottery ticket hypothesis (LTH) is well-studied for convolutional neural\nnetworks but has been validated only empirically for graph neural networks\n(GNNs), for which theoretical findings are largely lacking. In this paper, we\nidentify the expressivity of sparse subnetworks, i.e. their ability to\ndistinguish non-isomorphic graphs, as crucial for finding winning tickets that\npreserve the predictive performance. We establish conditions under which the\nexpressivity of a sparsely initialized GNN matches that of the full network,\nparticularly when compared to the Weisfeiler-Leman test, and in that context\nput forward and prove a Strong Expressive Lottery Ticket Hypothesis. We\nsubsequently show that an increased expressivity in the initialization\npotentially accelerates model convergence and improves generalization. Our\nfindings establish novel theoretical foundations for both LTH and GNN research,\nhighlighting the importance of maintaining expressivity in sparsely initialized\nGNNs. We illustrate our results using examples from drug discovery.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u4e2d\u7684\u5f69\u7968\u5047\u8bf4\uff08LTH\uff09\uff0c\u5f3a\u8c03\u4e86\u7a00\u758f\u5b50\u7f51\u7edc\u8868\u8fbe\u80fd\u529b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u8bc1\u660e\u4e86\u5176\u4e0e\u5b8c\u6574\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u76f8\u5339\u914d\u3002", "motivation": "\u76ee\u524dLTH\u5728\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\u7814\u7a76\u8f83\u591a\uff0c\u4f46\u5bf9GNNs\u7684\u7406\u8bba\u652f\u6301\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22GNNs\u4e2d\u7a00\u758f\u5b50\u7f51\u7edc\u8868\u8fbe\u80fd\u529b\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u5206\u6790\u7a00\u758f\u521d\u59cb\u5316GNN\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u4e0eWeisfeiler-Leman\u6d4b\u8bd5\u5bf9\u6bd4\uff0c\u63d0\u51fa\u5e76\u8bc1\u660e\u4e86Strong Expressive Lottery Ticket Hypothesis\u3002", "result": "\u7a00\u758f\u521d\u59cb\u5316GNN\u7684\u8868\u8fbe\u80fd\u529b\u63d0\u5347\u53ef\u52a0\u901f\u6a21\u578b\u6536\u655b\u5e76\u6539\u5584\u6cdb\u5316\u6027\u80fd\uff0c\u7814\u7a76\u4e3aLTH\u548cGNN\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u7ef4\u6301\u7a00\u758f\u521d\u59cb\u5316GNN\u7684\u8868\u8fbe\u80fd\u529b\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u7ed3\u679c\u5bf9\u836f\u7269\u53d1\u73b0\u7b49\u9886\u57df\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "keywords": "\u5f69\u7968\u5047\u8bf4\uff08LTH\uff09\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u3001\u7a00\u758f\u5b50\u7f51\u7edc\u3001Weisfeiler-Leman\u6d4b\u8bd5\u3001\u8868\u8fbe\u80fd\u529b"}}
{"id": "2506.03922", "pdf": "https://arxiv.org/pdf/2506.03922", "abs": "https://arxiv.org/abs/2506.03922", "authors": ["Zhaolu Kang", "Junhao Gong", "Jiaxu Yan", "Wanke Xia", "Yian Wang", "Ziwen Wang", "Huaxuan Ding", "Zhuo Cheng", "Wenhao Cao", "Zhiyuan Feng", "Siqi He", "Shannan Yan", "Junzhe Chen", "Xiaomin He", "Chaoya Jiang", "Wei Ye", "Kaidong Yu", "Xuelong Li"], "title": "HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\npotential to advance a broad range of domains. However, current benchmarks for\nevaluating MLLMs primarily emphasize general knowledge and vertical\nstep-by-step reasoning typical of STEM disciplines, while overlooking the\ndistinct needs and potential of the Humanities and Social Sciences (HSS). Tasks\nin the HSS domain require more horizontal, interdisciplinary thinking and a\ndeep integration of knowledge across related fields, which presents unique\nchallenges for MLLMs, particularly in linking abstract concepts with\ncorresponding visual representations. Addressing this gap, we present HSSBench,\na dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks\nin multiple languages, including the six official languages of the United\nNations. We also introduce a novel data generation pipeline tailored for HSS\nscenarios, in which multiple domain experts and automated agents collaborate to\ngenerate and iteratively refine each sample. HSSBench contains over 13,000\nmeticulously designed samples, covering six key categories. We benchmark more\nthan 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant\nchallenges even for state-of-the-art models. We hope that this benchmark will\ninspire further research into enhancing the cross-disciplinary reasoning\nabilities of MLLMs, especially their capacity to internalize and connect\nknowledge across fields.", "AI": {"tldr": "HSSBench\u662f\u4e00\u4e2a\u4e13\u4e3a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u4eba\u6587\u5b66\u79d1\u548c\u793e\u4f1a\u79d1\u5b66\uff08HSS\uff09\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u800c\u8bbe\u8ba1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u5bf9HSS\u9886\u57df\u72ec\u7279\u9700\u6c42\u7684\u5ffd\u89c6\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8STEM\u5b66\u79d1\u7684\u4e00\u822c\u77e5\u8bc6\u548c\u5782\u76f4\u63a8\u7406\u80fd\u529b\uff0c\u800c\u5ffd\u89c6\u4e86HSS\u9886\u57df\u7684\u8de8\u5b66\u79d1\u601d\u7ef4\u548c\u77e5\u8bc6\u6574\u5408\u9700\u6c42\u3002", "method": "\u63d0\u51faHSSBench\u57fa\u51c6\uff0c\u5305\u542b\u516d\u79cd\u8054\u5408\u56fd\u5b98\u65b9\u8bed\u8a00\u768413,000\u591a\u4e2a\u6837\u672c\uff0c\u91c7\u7528\u9886\u57df\u4e13\u5bb6\u4e0e\u81ea\u52a8\u5316\u4ee3\u7406\u534f\u4f5c\u7684\u6570\u636e\u751f\u6210\u6d41\u7a0b\u3002", "result": "\u8d85\u8fc720\u4e2a\u4e3b\u6d41MLLMs\u5728HSSBench\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8868\u660e\u5176\u5728\u8de8\u5b66\u79d1\u63a8\u7406\u548c\u77e5\u8bc6\u8fde\u63a5\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002", "conclusion": "HSSBench\u4e3a\u63d0\u5347MLLMs\u7684\u8de8\u5b66\u79d1\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, HSS, \u57fa\u51c6\u6d4b\u8bd5, \u8de8\u5b66\u79d1\u63a8\u7406, \u77e5\u8bc6\u6574\u5408"}}
{"id": "2506.03931", "pdf": "https://arxiv.org/pdf/2506.03931", "abs": "https://arxiv.org/abs/2506.03931", "authors": ["Yotam Alexander", "Yonatan Slutzky", "Yuval Ran-Milo", "Nadav Cohen"], "title": "Do Neural Networks Need Gradient Descent to Generalize? A Theoretical Study", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Conventional wisdom attributes the mysterious generalization abilities of\noverparameterized neural networks to gradient descent (and its variants). The\nrecent volume hypothesis challenges this view: it posits that these\ngeneralization abilities persist even when gradient descent is replaced by\nGuess & Check (G&C), i.e., by drawing weight settings until one that fits the\ntraining data is found. The validity of the volume hypothesis for wide and deep\nneural networks remains an open question. In this paper, we theoretically\ninvestigate this question for matrix factorization (with linear and non-linear\nactivation)--a common testbed in neural network theory. We first prove that\ngeneralization under G&C deteriorates with increasing width, establishing what\nis, to our knowledge, the first case where G&C is provably inferior to gradient\ndescent. Conversely, we prove that generalization under G&C improves with\nincreasing depth, revealing a stark contrast between wide and deep networks,\nwhich we further validate empirically. These findings suggest that even in\nsimple settings, there may not be a simple answer to the question of whether\nneural networks need gradient descent to generalize well.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6311\u6218\u4e86\u4f20\u7edf\u89c2\u5ff5\uff0c\u8ba4\u4e3a\u8fc7\u53c2\u6570\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u6cdb\u5316\u80fd\u529b\u53ef\u80fd\u4e0d\u4ec5\u4ec5\u4f9d\u8d56\u4e8e\u68af\u5ea6\u4e0b\u964d\uff0c\u800c\u662f\u53ef\u4ee5\u901a\u8fc7Guess & Check\uff08G&C\uff09\u65b9\u6cd5\u5b9e\u73b0\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u4f5c\u8005\u53d1\u73b0G&C\u5728\u5bbd\u5ea6\u589e\u52a0\u65f6\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\uff0c\u4f46\u5728\u6df1\u5ea6\u589e\u52a0\u65f6\u6539\u5584\uff0c\u63ed\u793a\u4e86\u5bbd\u7f51\u7edc\u4e0e\u6df1\u7f51\u7edc\u7684\u5bf9\u6bd4\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u9a8c\u8bc1\u4f53\u79ef\u5047\u8bbe\uff08volume hypothesis\uff09\u662f\u5426\u9002\u7528\u4e8e\u5bbd\u548c\u6df1\u795e\u7ecf\u7f51\u7edc\uff0c\u5373\u662f\u5426\u53ef\u4ee5\u4e0d\u4f9d\u8d56\u68af\u5ea6\u4e0b\u964d\u800c\u901a\u8fc7G&C\u5b9e\u73b0\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u4e3b\u8981\u9488\u5bf9\u77e9\u9635\u5206\u89e3\uff08\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u6fc0\u6d3b\uff09\u8fd9\u4e00\u795e\u7ecf\u7f51\u7edc\u7406\u8bba\u7684\u5e38\u89c1\u6d4b\u8bd5\u5e73\u53f0\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cG&C\u7684\u6cdb\u5316\u80fd\u529b\u968f\u5bbd\u5ea6\u589e\u52a0\u800c\u4e0b\u964d\uff0c\u4f46\u968f\u6df1\u5ea6\u589e\u52a0\u800c\u6539\u5584\uff0c\u9996\u6b21\u8bc1\u660eG&C\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u660e\u663e\u52a3\u4e8e\u68af\u5ea6\u4e0b\u964d\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u5373\u4f7f\u5728\u7b80\u5355\u8bbe\u7f6e\u4e2d\uff0c\u795e\u7ecf\u7f51\u7edc\u662f\u5426\u9700\u8981\u68af\u5ea6\u4e0b\u964d\u624d\u80fd\u6cdb\u5316\u826f\u597d\u53ef\u80fd\u6ca1\u6709\u7b80\u5355\u7b54\u6848\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc, \u6cdb\u5316\u80fd\u529b, \u68af\u5ea6\u4e0b\u964d, Guess & Check, \u4f53\u79ef\u5047\u8bbe, \u77e9\u9635\u5206\u89e3"}}
{"id": "2506.03923", "pdf": "https://arxiv.org/pdf/2506.03923", "abs": "https://arxiv.org/abs/2506.03923", "authors": ["Mohammadamin Shafiei", "Hamidreza Saffari", "Nafise Sadat Moosavi"], "title": "More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are known to be sensitive to input phrasing, but\nthe mechanisms by which semantic cues shape reasoning remain poorly understood.\nWe investigate this phenomenon in the context of comparative math problems with\nobjective ground truth, revealing a consistent and directional framing bias:\nlogically equivalent questions containing the words ``more'', ``less'', or\n``equal'' systematically steer predictions in the direction of the framing\nterm. To study this effect, we introduce MathComp, a controlled benchmark of\n300 comparison scenarios, each evaluated under 14 prompt variants across three\nLLM families. We find that model errors frequently reflect linguistic steering,\nsystematic shifts toward the comparative term present in the prompt.\nChain-of-thought prompting reduces these biases, but its effectiveness varies:\nfree-form reasoning is more robust, while structured formats may preserve or\nreintroduce directional drift. Finally, we show that including demographic\nidentity terms (e.g., ``a woman'', ``a Black person'') in input scenarios\namplifies directional drift, despite identical underlying quantities,\nhighlighting the interplay between semantic framing and social referents. These\nfindings expose critical blind spots in standard evaluation and motivate\nframing-aware benchmarks for diagnosing reasoning robustness and fairness in\nLLMs.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u5b66\u6bd4\u8f83\u95ee\u9898\u4e2d\u56e0\u8f93\u5165\u63aa\u8f9e\u800c\u4ea7\u751f\u7684\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u53d1\u73b0\u7279\u5b9a\u8bcd\u6c47\uff08\u5982\u201c\u66f4\u591a\u201d\u3001\u201c\u66f4\u5c11\u201d\u3001\u201c\u7b49\u4e8e\u201d\uff09\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u9884\u6d4b\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6MathComp\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3\u8bed\u4e49\u63d0\u793a\u5982\u4f55\u5f71\u54cdLLMs\u7684\u63a8\u7406\uff0c\u5c24\u5176\u662f\u5728\u6709\u5ba2\u89c2\u4e8b\u5b9e\u7684\u573a\u666f\u4e0b\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u63ed\u793a\u8f93\u5165\u63aa\u8f9e\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u7cfb\u7edf\u6027\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u8005\u4f7f\u7528MathComp\u57fa\u51c6\uff08\u5305\u542b300\u4e2a\u6bd4\u8f83\u573a\u666f\u548c14\u79cd\u63d0\u793a\u53d8\u4f53\uff09\u6d4b\u8bd5\u4e86\u4e09\u4e2aLLM\u5bb6\u65cf\uff0c\u5206\u6790\u5176\u9884\u6d4b\u504f\u5dee\uff0c\u5e76\u63a2\u8ba8\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u7f13\u89e3\u6548\u679c\u3002", "result": "\u6a21\u578b\u9519\u8bef\u5e38\u4e0e\u8f93\u5165\u4e2d\u7684\u6bd4\u8f83\u8bcd\u6c47\u4e00\u81f4\uff1b\u601d\u7ef4\u94fe\u63d0\u793a\u53ef\u51cf\u5c11\u504f\u5dee\uff0c\u4f46\u5176\u6548\u679c\u56e0\u5f62\u5f0f\u800c\u5f02\uff1b\u5305\u542b\u4eba\u53e3\u7edf\u8ba1\u8eab\u4efd\u8bcd\u6c47\u4f1a\u653e\u5927\u504f\u5dee\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u8bed\u4e49\u6846\u67b6\u4e0b\u7684\u63a8\u7406\u76f2\u533a\uff0c\u5f3a\u8c03\u9700\u8981\u8bbe\u8ba1\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u8bed\u4e49\u6846\u67b6, \u63a8\u7406\u504f\u5dee, \u6570\u5b66\u6bd4\u8f83, \u516c\u5e73\u6027"}}
{"id": "2506.03938", "pdf": "https://arxiv.org/pdf/2506.03938", "abs": "https://arxiv.org/abs/2506.03938", "authors": ["C\u00e9dric L\u00e9onard", "Dirk Stober", "Martin Schulz"], "title": "FPGA-Enabled Machine Learning Applications in Earth Observation: A Systematic Review", "categories": ["cs.LG", "cs.AR"], "comment": "35 pages, 3 figures, 2 tables. Submitted to ACM Computing Surveys\n  (ACM CSUR)", "summary": "New UAV technologies and the NewSpace era are transforming Earth Observation\nmissions and data acquisition. Numerous small platforms generate large data\nvolume, straining bandwidth and requiring onboard decision-making to transmit\nhigh-quality information in time. While Machine Learning allows real-time\nautonomous processing, FPGAs balance performance with adaptability to\nmission-specific requirements, enabling onboard deployment. This review\nsystematically analyzes 66 experiments deploying ML models on FPGAs for Remote\nSensing applications. We introduce two distinct taxonomies to capture both\nefficient model architectures and FPGA implementation strategies. For\ntransparency and reproducibility, we follow PRISMA 2020 guidelines and share\nall data and code at https://github.com/CedricLeon/Survey_RS-ML-FPGA.", "AI": {"tldr": "\u8bba\u6587\u56de\u987e\u4e8666\u4e2a\u5728FPGA\u4e0a\u90e8\u7f72\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u9065\u611f\u5e94\u7528\u7684\u5b9e\u9a8c\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u5206\u7c7b\u6cd5\uff0c\u5e76\u9075\u5faaPRISMA 2020\u6307\u5357\u786e\u4fdd\u900f\u660e\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u6280\u672f\u548cNewSpace\u65f6\u4ee3\u7684\u53d1\u5c55\uff0c\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u548c\u6570\u636e\u83b7\u53d6\u9762\u4e34\u5927\u6570\u636e\u91cf\u548c\u5b9e\u65f6\u5904\u7406\u7684\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e8666\u4e2a\u5b9e\u9a8c\uff0c\u63d0\u51fa\u4e24\u79cd\u5206\u7c7b\u6cd5\uff1a\u9ad8\u6548\u6a21\u578b\u67b6\u6784\u548cFPGA\u5b9e\u73b0\u7b56\u7565\u3002\u9075\u5faaPRISMA 2020\u6307\u5357\uff0c\u5e76\u5171\u4eab\u6570\u636e\u548c\u4ee3\u7801\u3002", "result": "\u5c55\u793a\u4e86FPGA\u5728\u5e73\u8861\u6027\u80fd\u548c\u9002\u5e94\u6027\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u652f\u6301\u673a\u8f7d\u90e8\u7f72\u548c\u5b9e\u65f6\u5904\u7406\u3002", "conclusion": "FPGA\u662f\u90e8\u7f72ML\u6a21\u578b\u8fdb\u884c\u9065\u611f\u5e94\u7528\u7684\u6709\u6548\u65b9\u6848\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\u3002", "keywords": "\u65e0\u4eba\u673a, FPGA, \u673a\u5668\u5b66\u4e60, \u9065\u611f, \u5b9e\u65f6\u5904\u7406"}}
{"id": "2506.03941", "pdf": "https://arxiv.org/pdf/2506.03941", "abs": "https://arxiv.org/abs/2506.03941", "authors": ["Vivian Nguyen", "Lillian Lee", "Cristian Danescu-Niculescu-Mizil"], "title": "Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations", "categories": ["cs.CL", "cs.AI", "cs.CY", "physics.soc-ph"], "comment": "To appear in the Proceedings of ACL 2025. Code and demo available in\n  ConvoKit (convokit.cornell.edu)", "summary": "During a conversation, there can come certain moments where its outcome hangs\nin the balance. In these pivotal moments, how one responds can put the\nconversation on substantially different trajectories leading to significantly\ndifferent outcomes. Systems that can detect when such moments arise could\nassist conversationalists in domains with highly consequential outcomes, such\nas mental health crisis counseling.\n  In this work, we introduce an unsupervised computational method for detecting\nsuch pivotal moments as they happen, in an online fashion. Our approach relies\non the intuition that a moment is pivotal if our expectation of the outcome\nvaries widely depending on what might be said next. By applying our method to\ncrisis counseling conversations, we first validate it by showing that it aligns\nwith human perception -- counselors take significantly longer to respond during\nmoments detected by our method -- and with the eventual conversational\ntrajectory -- which is more likely to change course at these times. We then use\nour framework to explore the relation of the counselor's response during\npivotal moments with the eventual outcome of the session.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5bf9\u8bdd\u4e2d\u5b9e\u65f6\u68c0\u6d4b\u5173\u952e\u65f6\u523b\uff0c\u5e76\u901a\u8fc7\u5371\u673a\u54a8\u8be2\u5bf9\u8bdd\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5bf9\u8bdd\u4e2d\u7684\u5173\u952e\u65f6\u523b\u5bf9\u7ed3\u679c\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u5fc3\u7406\u5371\u673a\u54a8\u8be2\u7b49\u9ad8\u540e\u679c\u9886\u57df\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u68c0\u6d4b\u8fd9\u4e9b\u65f6\u523b\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u65e0\u76d1\u7763\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u5bf9\u8bdd\u4e2d\u4e0b\u4e00\u6b65\u53ef\u80fd\u7684\u56de\u7b54\u5bf9\u7ed3\u679c\u9884\u671f\u7684\u5f71\u54cd\u6765\u68c0\u6d4b\u5173\u952e\u65f6\u523b\u3002", "result": "\u9a8c\u8bc1\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u4e0e\u4eba\u7c7b\u611f\u77e5\u4e00\u81f4\uff08\u54a8\u8be2\u5e08\u5728\u68c0\u6d4b\u5230\u7684\u5173\u952e\u65f6\u523b\u53cd\u5e94\u65f6\u95f4\u66f4\u957f\uff09\uff0c\u4e14\u5173\u952e\u65f6\u523b\u786e\u5b9e\u4f1a\u5f71\u54cd\u5bf9\u8bdd\u7684\u6700\u7ec8\u8d70\u5411\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u54a8\u8be2\u5e08\u5728\u5173\u952e\u65f6\u523b\u7684\u56de\u5e94\u4e0e\u4f1a\u8bdd\u7ed3\u679c\u7684\u5173\u7cfb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5bf9\u8bdd\u4e2d\u7684\u5173\u952e\u65f6\u523b\uff0c\u4e3a\u9ad8\u540e\u679c\u9886\u57df\u7684\u5bf9\u8bdd\u8f85\u52a9\u63d0\u4f9b\u4e86\u5de5\u5177\u3002", "keywords": "\u5173\u952e\u65f6\u523b\u68c0\u6d4b\u3001\u65e0\u76d1\u7763\u5b66\u4e60\u3001\u5371\u673a\u54a8\u8be2\u3001\u5bf9\u8bdd\u5206\u6790"}}
{"id": "2506.03373", "pdf": "https://arxiv.org/pdf/2506.03373", "abs": "https://arxiv.org/abs/2506.03373", "authors": ["Muhammad Shaban", "Yuzhou Chang", "Huaying Qiu", "Yao Yu Yeo", "Andrew H. Song", "Guillaume Jaume", "Yuchen Wang", "Luca L. Weishaupt", "Tong Ding", "Anurag Vaidya", "Abdallah Lamane", "Daniel Shao", "Mohammed Zidane", "Yunhao Bai", "Paige McCallum", "Shuli Luo", "Wenrui Wu", "Yang Wang", "Precious Cramer", "Chi Ngai Chan", "Pierre Stephan", "Johanna Schaffenrath", "Jia Le Lee", "Hendrik A. Michel", "Caiwei Tian", "Cristina Almagro-Perez", "Sophia J. Wagner", "Sharifa Sahai", "Ming Y. Lu", "Richard J. Chen", "Andrew Zhang", "Mark Edward M. Gonzales", "Ahmad Makky", "Jia-Ying Joey Lee", "Hao Cheng", "Nourhan El Ahmar", "Sayed Matar", "Maximilian Haist", "Darci Phillips", "Yuqi Tan", "Garry P. Nolan", "W. Richard Burack", "Jacob D. Estes", "Jonathan T. C. Liu", "Toni K Choueiri", "Neeraj Agarwal", "Marc Barry", "Scott J. Rodig", "Long Phi Le", "Georg Gerber", "Christian M. Sch\u00fcrch", "Fabian J. Theis", "Youn H Kim", "Joe Yeong", "Sabina Signoretti", "Brooke E. Howitt", "Lit-Hsin Loo", "Qin Ma", "Sizun Jiang", "Faisal Mahmood"], "title": "A Foundation Model for Spatial Proteomics", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundation models have begun to transform image analysis by acting as\npretrained generalist backbones that can be adapted to many tasks even when\npost-training data are limited, yet their impact on spatial proteomics, imaging\nthat maps proteins at single-cell resolution, remains limited. Here, we\nintroduce KRONOS, a foundation model built for spatial proteomics. KRONOS was\ntrained in a self-supervised manner on over 47 million image patches covering\n175 protein markers, 16 tissue types, and 8 fluorescence-based imaging\nplatforms. We introduce key architectural adaptations to address the\nhigh-dimensional, multi-channel, and heterogeneous nature of multiplex imaging.\nWe demonstrate that KRONOS learns biologically meaningful representations\nacross multiple scales, ranging from cellular and microenvironment to tissue\nlevels, enabling it to address diverse downstream tasks, including cell\nphenotyping, region classification, and patient stratification. Evaluated\nacross 11 independent cohorts, KRONOS achieves state-of-the-art performance\nacross cell phenotyping, treatment response prediction, and retrieval tasks,\nand is highly data-efficient. KRONOS also introduces the paradigm of\nsegmentation-free patch-level processing for efficient and scalable spatial\nproteomics analysis, allowing cross-institutional comparisons, and as an image\nreverse search engine for spatial patterns. Together, these results position\nKRONOS as a flexible and scalable tool for spatial proteomics. The model is\npublicly accessible at https://github.com/mahmoodlab/KRONOS.", "AI": {"tldr": "KRONOS\u662f\u4e00\u79cd\u7528\u4e8e\u7a7a\u95f4\u86cb\u767d\u8d28\u7ec4\u5b66\u7684\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u5904\u7406\u591a\u901a\u9053\u3001\u9ad8\u7ef4\u56fe\u50cf\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u7a7a\u95f4\u86cb\u767d\u8d28\u7ec4\u5b66\u4e2d\u7f3a\u4e4f\u901a\u7528\u57fa\u7840\u6a21\u578b\u7684\u95ee\u9898\uff0c\u63d0\u5347\u86cb\u767d\u8d28\u5355\u7ec6\u80de\u5206\u8fa8\u7387\u5206\u6790\u7684\u80fd\u529b\u3002", "method": "\u81ea\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\uff0c\u9488\u5bf9\u591a\u901a\u9053\u3001\u9ad8\u7ef4\u56fe\u50cf\u8fdb\u884c\u5173\u952e\u67b6\u6784\u8c03\u6574\uff0c\u5904\u740647\u767e\u4e07\u56fe\u50cf\u5757\u3002", "result": "\u572811\u4e2a\u72ec\u7acb\u961f\u5217\u4e2d\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u7ec6\u80de\u5206\u578b\u3001\u533a\u57df\u5206\u7c7b\u548c\u60a3\u8005\u5206\u5c42\u7b49\u4efb\u52a1\u3002", "conclusion": "KRONOS\u662f\u4e00\u4e2a\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u7a7a\u95f4\u86cb\u767d\u8d28\u7ec4\u5b66\u5de5\u5177\uff0c\u652f\u6301\u65e0\u5206\u5272\u5904\u7406\u548c\u591a\u5c3a\u5ea6\u5206\u6790\u3002", "keywords": "KRONOS, \u7a7a\u95f4\u86cb\u767d\u8d28\u7ec4\u5b66, \u57fa\u7840\u6a21\u578b, \u81ea\u76d1\u7763\u5b66\u4e60, \u56fe\u50cf\u5206\u6790"}}
{"id": "2506.03943", "pdf": "https://arxiv.org/pdf/2506.03943", "abs": "https://arxiv.org/abs/2506.03943", "authors": ["Shiyi Yang", "Can Chen", "Didong Li"], "title": "Lower Ricci Curvature for Hypergraphs", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Networks with higher-order interactions, prevalent in biological, social, and\ninformation systems, are naturally represented as hypergraphs, yet their\nstructural complexity poses fundamental challenges for geometric\ncharacterization. While curvature-based methods offer powerful insights in\ngraph analysis, existing extensions to hypergraphs suffer from critical\ntrade-offs: combinatorial approaches such as Forman-Ricci curvature capture\nonly coarse features, whereas geometric methods like Ollivier-Ricci curvature\noffer richer expressivity but demand costly optimal transport computations. To\naddress these challenges, we introduce hypergraph lower Ricci curvature (HLRC),\na novel curvature metric defined in closed form that achieves a principled\nbalance between interpretability and efficiency. Evaluated across diverse\nsynthetic and real-world hypergraph datasets, HLRC consistently reveals\nmeaningful higher-order organization, distinguishing intra- from\ninter-community hyperedges, uncovering latent semantic labels, tracking\ntemporal dynamics, and supporting robust clustering of hypergraphs based on\nglobal structure. By unifying geometric sensitivity with algorithmic\nsimplicity, HLRC provides a versatile foundation for hypergraph analytics, with\nbroad implications for tasks including node classification, anomaly detection,\nand generative modeling in complex systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d85\u56fe\u66f2\u7387\u5ea6\u91cfHLRC\uff0c\u5e73\u8861\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u80fd\u6709\u6548\u63ed\u793a\u9ad8\u9636\u7ed3\u6784\u7684\u7ec4\u7ec7\u7279\u5f81\u3002", "motivation": "\u8d85\u56fe\u7ed3\u6784\u590d\u6742\uff0c\u73b0\u6709\u66f2\u7387\u65b9\u6cd5\u5982Forman-Ricci\u8fc7\u4e8e\u7c97\u7cd9\uff0cOllivier-Ricci\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u8d85\u56fe\u4f4eRicci\u66f2\u7387(HLRC)\uff0c\u4ee5\u95ed\u5f0f\u5b9a\u4e49\u5b9e\u73b0\u51e0\u4f55\u654f\u611f\u6027\u4e0e\u7b97\u6cd5\u7b80\u5355\u6027\u7684\u7edf\u4e00\u3002", "result": "HLRC\u5728\u5408\u6210\u548c\u771f\u5b9e\u8d85\u56fe\u6570\u636e\u96c6\u4e2d\u5747\u80fd\u63ed\u793a\u9ad8\u9636\u7ed3\u6784\uff0c\u533a\u5206\u793e\u533a\u5185/\u5916\u8d85\u8fb9\uff0c\u5e76\u652f\u6301\u805a\u7c7b\u7b49\u4efb\u52a1\u3002", "conclusion": "HLRC\u4e3a\u8d85\u56fe\u5206\u6790\u63d0\u4f9b\u4e86\u901a\u7528\u57fa\u7840\uff0c\u9002\u7528\u4e8e\u8282\u70b9\u5206\u7c7b\u3001\u5f02\u5e38\u68c0\u6d4b\u7b49\u590d\u6742\u7cfb\u7edf\u4efb\u52a1\u3002", "keywords": "\u8d85\u56fe, Ricci\u66f2\u7387, \u9ad8\u9636\u76f8\u4e92\u4f5c\u7528, \u590d\u6742\u7f51\u7edc, \u7ed3\u6784\u5206\u6790"}}
{"id": "2506.03949", "pdf": "https://arxiv.org/pdf/2506.03949", "abs": "https://arxiv.org/abs/2506.03949", "authors": ["Junnan Zhu", "Jingyi Wang", "Bohan Yu", "Xiaoyu Wu", "Junbo Li", "Lei Wang", "Nan Xu"], "title": "TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "LLMs have shown impressive progress in natural language processing. However,\nthey still face significant challenges in TableQA, where real-world\ncomplexities such as diverse table structures, multilingual data, and\ndomain-specific reasoning are crucial. Existing TableQA benchmarks are often\nlimited by their focus on simple flat tables and suffer from data leakage.\nFurthermore, most benchmarks are monolingual and fail to capture the\ncross-lingual and cross-domain variability in practical applications. To\naddress these limitations, we introduce TableEval, a new benchmark designed to\nevaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes\ntables with various structures (such as concise, hierarchical, and nested\ntables) collected from four domains (including government, finance, academia,\nand industry reports). Besides, TableEval features cross-lingual scenarios with\ntables in Simplified Chinese, Traditional Chinese, and English. To minimize the\nrisk of data leakage, we collect all data from recent real-world documents.\nConsidering that existing TableQA metrics fail to capture semantic accuracy, we\nfurther propose SEAT, a new evaluation framework that assesses the alignment\nbetween model responses and reference answers at the sub-question level.\nExperimental results have shown that SEAT achieves high agreement with human\njudgment. Extensive experiments on TableEval reveal critical gaps in the\nability of state-of-the-art LLMs to handle these complex, real-world TableQA\ntasks, offering insights for future improvements. We make our dataset available\nhere: https://github.com/wenge-research/TableEval.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86TableEval\uff0c\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u771f\u5b9eTableQA\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86SEAT\u8bc4\u4f30\u6846\u67b6\u4ee5\u63d0\u9ad8\u8bed\u4e49\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709TableQA\u57fa\u51c6\u6d4b\u8bd5\u5c40\u9650\u4e8e\u7b80\u5355\u8868\u683c\u3001\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u4e14\u591a\u4e3a\u5355\u8bed\u8a00\uff0c\u65e0\u6cd5\u6355\u6349\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8de8\u8bed\u8a00\u548c\u8de8\u9886\u57df\u591a\u6837\u6027\u3002", "method": "TableEval\u5305\u542b\u591a\u79cd\u7ed3\u6784\u7684\u8868\u683c\u6570\u636e\uff0c\u6db5\u76d6\u56db\u4e2a\u9886\u57df\u548c\u4e09\u79cd\u8bed\u8a00\uff0c\u540c\u65f6\u63d0\u51faSEAT\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u5b50\u95ee\u9898\u5c42\u9762\u8bc4\u4f30\u6a21\u578b\u54cd\u5e94\u4e0e\u53c2\u8003\u7b54\u6848\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSEAT\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e14TableEval\u63ed\u793a\u4e86\u5f53\u524dLLMs\u5728\u5904\u7406\u590d\u6742\u771f\u5b9eTableQA\u4efb\u52a1\u65f6\u7684\u80fd\u529b\u5dee\u8ddd\u3002", "conclusion": "TableEval\u4e3aLLMs\u5728\u590d\u6742TableQA\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u57fa\u51c6\uff0cSEAT\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u8bed\u4e49\u51c6\u786e\u6027\u8bc4\u4f30\u3002", "keywords": "TableQA, LLMs, \u591a\u9886\u57df, \u591a\u8bed\u8a00, \u8bed\u4e49\u8bc4\u4f30"}}
{"id": "2506.03381", "pdf": "https://arxiv.org/pdf/2506.03381", "abs": "https://arxiv.org/abs/2506.03381", "authors": ["Artur Grigorev", "Khaled Saleh", "Jiwon Kim", "Adriana-Simona Mihaita"], "title": "Automated Traffic Incident Response Plans using Generative Artificial Intelligence: Part 1 -- Building the Incident Response Benchmark", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": null, "summary": "Traffic incidents remain a critical public safety concern worldwide, with\nAustralia recording 1,300 road fatalities in 2024, which is the highest toll in\n12 years. Similarly, the United States reports approximately 6 million crashes\nannually, raising significant challenges in terms of a fast reponse time and\noperational management. Traditional response protocols rely on human\ndecision-making, which introduces potential inconsistencies and delays during\ncritical moments when every minute impacts both safety outcomes and network\nperformance. To address this issue, we propose a novel Incident Response\nBenchmark that uses generative artificial intelligence to automatically\ngenerate response plans for incoming traffic incidents. Our approach aims to\nsignificantly reduce incident resolution times by suggesting\ncontext-appropriate actions such as variable message sign deployment, lane\nclosures, and emergency resource allocation adapted to specific incident\ncharacteristics. First, the proposed methodology uses real-world incident\nreports from the Performance Measurement System (PeMS) as training and\nevaluation data. We extract historically implemented actions from these reports\nand compare them against AI-generated response plans that suggest specific\nactions, such as lane closures, variable message sign announcements, and/or\ndispatching appropriate emergency resources. Second, model evaluations reveal\nthat advanced generative AI models like GPT-4o and Grok 2 achieve superior\nalignment with expert solutions, demonstrated by minimized Hamming distances\n(averaging 2.96-2.98) and low weighted differences (approximately 0.27-0.28).\nConversely, while Gemini 1.5 Pro records the lowest count of missed actions,\nits extremely high number of unnecessary actions (1547 compared to 225 for\nGPT-4o) indicates an over-triggering strategy that reduces the overall plan\nefficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u4ea4\u901a\u4e8b\u4ef6\u54cd\u5e94\u57fa\u51c6\uff0c\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u54cd\u5e94\u8ba1\u5212\u6765\u51cf\u5c11\u4e8b\u4ef6\u89e3\u51b3\u65f6\u95f4\uff0c\u63d0\u5347\u7ba1\u7406\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u51b3\u7b56\u7684\u4ea4\u901a\u4e8b\u4ef6\u54cd\u5e94\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u9ad8\u54cd\u5e94\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "method": "\u5229\u7528\u771f\u5b9e\u4e16\u754c\u7684\u4e8b\u4ef6\u62a5\u544a\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u91c7\u7528\u751f\u6210\u5f0fAI\u6a21\u578b\uff08\u5982GPT-4o\u3001Grok 2\u548cGemini 1.5 Pro\uff09\u751f\u6210\u54cd\u5e94\u8ba1\u5212\uff0c\u5e76\u4e0e\u4e13\u5bb6\u65b9\u6848\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "GPT-4o\u548cGrok 2\u5728\u4e13\u5bb6\u65b9\u6848\u5339\u914d\u5ea6\u4e0a\u8868\u73b0\u6700\u4f73\uff08Hamming\u8ddd\u79bb2.96-2.98\uff09\uff0c\u800cGemini 1.5 Pro\u56e0\u8fc7\u5ea6\u89e6\u53d1\u52a8\u4f5c\u5bfc\u81f4\u6548\u7387\u4e0b\u964d\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u80fd\u663e\u8457\u63d0\u5347\u4ea4\u901a\u4e8b\u4ef6\u54cd\u5e94\u6548\u7387\uff0c\u4f46\u9700\u6743\u8861\u52a8\u4f5c\u89e6\u53d1\u9891\u7387\u4e0e\u8ba1\u5212\u5b9e\u7528\u6027\u3002", "keywords": "\u4ea4\u901a\u4e8b\u4ef6\u3001\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u3001\u54cd\u5e94\u57fa\u51c6\u3001\u5b9e\u65f6\u7ba1\u7406"}}
{"id": "2506.03951", "pdf": "https://arxiv.org/pdf/2506.03951", "abs": "https://arxiv.org/abs/2506.03951", "authors": ["Aojun Lu", "Hangjie Yuan", "Tao Feng", "Yanan Sun"], "title": "Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "The quest for Continual Learning (CL) seeks to empower neural networks with\nthe ability to learn and adapt incrementally. Central to this pursuit is\naddressing the stability-plasticity dilemma, which involves striking a balance\nbetween two conflicting objectives: preserving previously learned knowledge and\nacquiring new knowledge. While numerous CL methods aim to achieve this\ntrade-off, they often overlook the impact of network architecture on stability\nand plasticity, restricting the trade-off to the parameter level. In this\npaper, we delve into the conflict between stability and plasticity at the\narchitectural level. We reveal that under an equal parameter constraint, deeper\nnetworks exhibit better plasticity, while wider networks are characterized by\nsuperior stability. To address this architectural-level dilemma, we introduce a\nnovel framework denoted Dual-Arch, which serves as a plug-in component for CL.\nThis framework leverages the complementary strengths of two distinct and\nindependent networks: one dedicated to plasticity and the other to stability.\nEach network is designed with a specialized and lightweight architecture,\ntailored to its respective objective. Extensive experiments demonstrate that\nDual-Arch enhances the performance of existing CL methods while being up to 87%\nmore compact in terms of parameters.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDual-Arch\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u548c\u5bbd\u5ea6\u7f51\u7edc\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027\u4e0e\u53ef\u5851\u6027\u77db\u76fe\uff0c\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u53c2\u6570\u89c4\u6a21\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\u4e2d\u7a33\u5b9a\u6027\u4e0e\u53ef\u5851\u6027\u7684\u77db\u76fe\u9700\u8981\u89e3\u51b3\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u7f51\u7edc\u67b6\u6784\u7684\u5f71\u54cd\u3002\u672c\u6587\u63a2\u7d22\u67b6\u6784\u5c42\u9762\u7684\u5e73\u8861\u70b9\u3002", "method": "\u63d0\u51faDual-Arch\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u7f51\u7edc\uff08\u53ef\u5851\u6027\uff09\u548c\u5bbd\u5ea6\u7f51\u7edc\uff08\u7a33\u5b9a\u6027\uff09\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u4e13\u7528\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDual-Arch\u663e\u8457\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u540c\u65f6\u53c2\u6570\u89c4\u6a21\u51cf\u5c11\u9ad8\u8fbe87%\u3002", "conclusion": "Dual-Arch\u6709\u6548\u89e3\u51b3\u4e86\u67b6\u6784\u5c42\u9762\u7684\u7a33\u5b9a\u6027\u4e0e\u53ef\u5851\u6027\u77db\u76fe\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u6301\u7eed\u5b66\u4e60, \u7a33\u5b9a\u6027, \u53ef\u5851\u6027, Dual-Arch, \u7f51\u7edc\u67b6\u6784"}}
{"id": "2506.03968", "pdf": "https://arxiv.org/pdf/2506.03968", "abs": "https://arxiv.org/abs/2506.03968", "authors": ["Chiwei Zhu", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "title": "From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding", "categories": ["cs.CL"], "comment": "To be published at ACL 2025", "summary": "The pursuit of diverse, complex, and large-scale instruction data is crucial\nfor automatically aligning large language models (LLMs). While there are\nmethods capable of generating synthetic instructions at scale, they either\nsuffer from limited grounding sources, leading to a narrow distribution, or\nrely on trivial extensions that fail to produce meaningful trajectories in\nterms of complexity. In contrast, instructions that benefit efficient alignment\nare typically crafted with cognitive insights and grounded in real-world use\ncases. In this paper, we synthesize such instructions using attributed\ngrounding, which involves 1) a top-down attribution process that grounds a\nselective set of real instructions to situated users, and 2) a bottom-up\nsynthesis process that leverages web documents to first generate a situation,\nthen a meaningful instruction. This framework allows us to harvest diverse and\ncomplex instructions at scale, utilizing the vast range of web documents.\nSpecifically, we construct a dataset of 1 million instructions, called\nSynthQuestions, and demonstrate that models trained on it achieve leading\nperformance on several common benchmarks, with improvements that continually\nscale with more web corpora. Data, models and codes will be available at\nhttps://github.com/Ignoramus0817/SynthQuestions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5c5e\u6027\u5316\u63a5\u5730\u65b9\u6cd5\u5408\u6210\u591a\u6837\u5316\u3001\u590d\u6742\u6307\u4ee4\u7684\u6846\u67b6\uff0c\u4ee5\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u6548\u679c\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b100\u4e07\u6307\u4ee4\u7684\u6570\u636e\u96c6SynthQuestions\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u6307\u4ee4\u65f6\u5206\u5e03\u72ed\u7a84\u6216\u590d\u6742\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u5bf9\u9f50\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u3002", "method": "\u7ed3\u5408\u81ea\u4e0a\u800c\u4e0b\u7684\u5c5e\u6027\u5316\u8fc7\u7a0b\u548c\u81ea\u4e0b\u800c\u4e0a\u7684\u5408\u6210\u8fc7\u7a0b\uff0c\u5229\u7528\u7f51\u7edc\u6587\u6863\u751f\u6210\u591a\u6837\u5316\u548c\u590d\u6742\u7684\u6307\u4ee4\u3002", "result": "\u6784\u5efa\u4e86100\u4e07\u6761\u6307\u4ee4\u7684\u6570\u636e\u96c6SynthQuestions\uff0c\u6a21\u578b\u5728\u5176\u4e0a\u8bad\u7ec3\u540e\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u9886\u5148\uff0c\u4e14\u6027\u80fd\u968f\u7f51\u7edc\u8bed\u6599\u589e\u52a0\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "\u5c5e\u6027\u5316\u63a5\u5730\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u751f\u6210\u591a\u6837\u5316\u548c\u590d\u6742\u6307\u4ee4\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u6548\u679c\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u6307\u4ee4\u5408\u6210, \u5bf9\u9f50, \u5c5e\u6027\u5316\u63a5\u5730, SynthQuestions"}}
{"id": "2506.03391", "pdf": "https://arxiv.org/pdf/2506.03391", "abs": "https://arxiv.org/abs/2506.03391", "authors": ["Tri Kurniawan Wijaya", "Xinyang Shao", "Gonzalo Fiz Pontiveros", "Edoardo D'Amico"], "title": "Universal Reusability in Recommender Systems: The Case for Dataset- and Task-Independent Frameworks", "categories": ["cs.IR", "cs.AI", "cs.DB", "cs.LG"], "comment": null, "summary": "Recommender systems are pivotal in delivering personalized experiences across\nindustries, yet their adoption and scalability remain hindered by the need for\nextensive dataset- and task-specific configurations. Existing systems often\nrequire significant manual intervention, domain expertise, and engineering\neffort to adapt to new datasets or tasks, creating barriers to entry and\nlimiting reusability. In contrast, recent advancements in large language models\n(LLMs) have demonstrated the transformative potential of reusable systems,\nwhere a single model can handle diverse tasks without significant\nreconfiguration. Inspired by this paradigm, we propose the Dataset- and\nTask-Independent Recommender System (DTIRS), a framework aimed at maximizing\nthe reusability of recommender systems while minimizing barriers to entry.\nUnlike LLMs, which achieve task generalization directly, DTIRS focuses on\neliminating the need to rebuild or reconfigure recommendation pipelines for\nevery new dataset or task, even though models may still need retraining on new\ndata. By leveraging the novel Dataset Description Language (DsDL), DTIRS\nenables standardized dataset descriptions and explicit task definitions,\nallowing autonomous feature engineering, model selection, and optimization.\nThis paper introduces the concept of DTIRS and establishes a roadmap for\ntransitioning from Level-1 automation (dataset-agnostic but task-specific\nsystems) to Level-2 automation (fully dataset- and task-independent systems).\nAchieving this paradigm would maximize code reusability and lower barriers to\nadoption. We discuss key challenges, including the trade-offs between\ngeneralization and specialization, computational overhead, and scalability,\nwhile presenting DsDL as a foundational tool for this vision.", "AI": {"tldr": "\u63d0\u51fa\u4e86DTIRS\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u6807\u51c6\u5316\u6570\u636e\u96c6\u63cf\u8ff0\u548c\u4efb\u52a1\u5b9a\u4e49\uff0c\u6700\u5927\u5316\u63a8\u8350\u7cfb\u7edf\u7684\u53ef\u91cd\u7528\u6027\u5e76\u964d\u4f4e\u5165\u95e8\u95e8\u69db\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u6311\u6218\u548c\u5de5\u5177DsDL\u3002", "motivation": "\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u6570\u636e\u96c6\u548c\u4efb\u52a1\u8fdb\u884c\u5927\u91cf\u624b\u52a8\u914d\u7f6e\uff0c\u963b\u788d\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u666e\u53ca\u6027\uff0cDTIRS\u81f4\u529b\u4e8e\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528DsDL\uff08Dataset Description Language\uff09\u5b9e\u73b0\u6570\u636e\u96c6\u63cf\u8ff0\u7684\u6807\u51c6\u5316\u548c\u4efb\u52a1\u7684\u660e\u786e\u5b9a\u4e49\uff0c\u4ece\u800c\u5b9e\u73b0\u81ea\u4e3b\u7279\u5f81\u5de5\u7a0b\u3001\u6a21\u578b\u9009\u62e9\u548c\u4f18\u5316\u3002", "result": "DTIRS\u6846\u67b6\u4eceLevel-1\u81ea\u52a8\u5316\u9010\u6b65\u8fc7\u6e21\u5230Level-2\u81ea\u52a8\u5316\uff0c\u6700\u5927\u5316\u4ee3\u7801\u53ef\u91cd\u7528\u6027\u548c\u964d\u4f4e\u95e8\u69db\u3002", "conclusion": "DTIRS\u901a\u8fc7\u6807\u51c6\u5316\u548c\u81ea\u52a8\u5316\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "keywords": "\u63a8\u8350\u7cfb\u7edf, \u53ef\u91cd\u7528\u6027, \u53ef\u6269\u5c55\u6027, DsDL, \u81ea\u52a8\u5316"}}
{"id": "2506.03954", "pdf": "https://arxiv.org/pdf/2506.03954", "abs": "https://arxiv.org/abs/2506.03954", "authors": ["Jianqing Zhang", "Xinghao Wu", "Yanbing Zhou", "Xiaoting Sun", "Qiqi Cai", "Yang Liu", "Yang Hua", "Zhenzhe Zheng", "Jian Cao", "Qiang Yang"], "title": "HtFLlib: A Comprehensive Heterogeneous Federated Learning Library and Benchmark", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "Accepted by KDD2025", "summary": "As AI evolves, collaboration among heterogeneous models helps overcome data\nscarcity by enabling knowledge transfer across institutions and devices.\nTraditional Federated Learning (FL) only supports homogeneous models, limiting\ncollaboration among clients with heterogeneous model architectures. To address\nthis, Heterogeneous Federated Learning (HtFL) methods are developed to enable\ncollaboration across diverse heterogeneous models while tackling the data\nheterogeneity issue at the same time. However, a comprehensive benchmark for\nstandardized evaluation and analysis of the rapidly growing HtFL methods is\nlacking. Firstly, the highly varied datasets, model heterogeneity scenarios,\nand different method implementations become hurdles to making easy and fair\ncomparisons among HtFL methods. Secondly, the effectiveness and robustness of\nHtFL methods are under-explored in various scenarios, such as the medical\ndomain and sensor signal modality. To fill this gap, we introduce the first\nHeterogeneous Federated Learning Library (HtFLlib), an easy-to-use and\nextensible framework that integrates multiple datasets and model heterogeneity\nscenarios, offering a robust benchmark for research and practical applications.\nSpecifically, HtFLlib integrates (1) 12 datasets spanning various domains,\nmodalities, and data heterogeneity scenarios; (2) 40 model architectures,\nranging from small to large, across three modalities; (3) a modularized and\neasy-to-extend HtFL codebase with implementations of 10 representative HtFL\nmethods; and (4) systematic evaluations in terms of accuracy, convergence,\ncomputation costs, and communication costs. We emphasize the advantages and\npotential of state-of-the-art HtFL methods and hope that HtFLlib will catalyze\nadvancing HtFL research and enable its broader applications. The code is\nreleased at https://github.com/TsingZ0/HtFLlib.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u5e93\uff08HtFLlib\uff09\uff0c\u7528\u4e8e\u6807\u51c6\u5316\u8bc4\u4f30\u548c\u5206\u6790\u5f02\u6784\u8054\u90a6\u5b66\u4e60\uff08HtFL\uff09\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u8fd9\u4e00\u9886\u57df\u7684\u7a7a\u767d\u3002", "motivation": "\u4f20\u7edf\u7684\u8054\u90a6\u5b66\u4e60\u4ec5\u652f\u6301\u540c\u6784\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5f02\u6784\u6a21\u578b\u67b6\u6784\u7684\u534f\u4f5c\u3002HtFL\u65b9\u6cd5\u867d\u80fd\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u5f00\u53d1\u4e86HtFLlib\uff0c\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u591a\u79cd\u6570\u636e\u96c6\u3001\u6a21\u578b\u67b6\u6784\u548cHtFL\u65b9\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "HtFLlib\u5305\u542b12\u4e2a\u6570\u636e\u96c6\u300140\u79cd\u6a21\u578b\u67b6\u6784\u548c10\u79cdHtFL\u65b9\u6cd5\uff0c\u652f\u6301\u7cfb\u7edf\u8bc4\u4f30\u51c6\u786e\u6027\u3001\u6536\u655b\u6027\u3001\u8ba1\u7b97\u6210\u672c\u548c\u901a\u4fe1\u6210\u672c\u3002", "conclusion": "HtFLlib\u6709\u671b\u63a8\u52a8HtFL\u7814\u7a76\u7684\u8fdb\u5c55\u548c\u5e7f\u6cdb\u5e94\u7528\uff0c\u5e76\u5c55\u793a\u4e86\u5148\u8fdbHtFL\u65b9\u6cd5\u7684\u4f18\u52bf\u548c\u6f5c\u529b\u3002", "keywords": "\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u3001\u77e5\u8bc6\u8f6c\u79fb\u3001\u6570\u636e\u7a00\u7f3a\u3001\u8bc4\u4f30\u57fa\u51c6"}}
{"id": "2506.03978", "pdf": "https://arxiv.org/pdf/2506.03978", "abs": "https://arxiv.org/abs/2506.03978", "authors": ["Hieu Trung Nguyen", "Bao Nguyen", "Viet Anh Nguyen"], "title": "Structured Pruning for Diverse Best-of-N Reasoning Optimization", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "Model pruning in transformer-based language models, traditionally viewed as a\nmeans of achieving computational savings, can enhance the model's reasoning\ncapabilities. In this work, we uncover a surprising phenomenon: the selective\npruning of certain attention heads leads to improvements in reasoning\nperformance, particularly on challenging tasks. Motivated by this observation,\nwe propose SPRINT, a novel contrastive learning framework that dynamically\nselects the optimal head and layer to prune during inference. By aligning\nquestion embeddings with head embeddings, SPRINT identifies those pruned-head\nconfigurations that result in more accurate reasoning. Extensive experiments\ndemonstrate that our method significantly outperforms traditional best-of-$N$\nand random head selection strategies on the MATH500 and GSM8K datasets.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u9009\u62e9\u6027\u526a\u679d\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u53ef\u63d0\u5347Transformer\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u51faSPRINT\u6846\u67b6\u52a8\u6001\u4f18\u5316\u526a\u679d\u914d\u7f6e\u3002", "motivation": "\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u4ec5\u4e3a\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\uff0c\u7814\u7a76\u63ed\u793a\u526a\u679d\u8fd8\u53ef\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u590d\u6742\u4efb\u52a1\u4e0a\u3002", "method": "\u63d0\u51faSPRINT\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u52a8\u6001\u9009\u62e9\u6700\u4f18\u526a\u679d\u5934\u548c\u5c42\uff0c\u5bf9\u9f50\u95ee\u9898\u548c\u5934\u5d4c\u5165\u4ee5\u4f18\u5316\u63a8\u7406\u3002", "result": "\u5728MATH500\u548cGSM8K\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6700\u4f18-N\u548c\u968f\u673a\u9009\u62e9\u7b56\u7565\u3002", "conclusion": "SPRINT\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u526a\u679d\u4f18\u5316\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u6a21\u578b\u526a\u679d\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u6a21\u578b\u526a\u679d\u3001Transformer\u3001\u63a8\u7406\u80fd\u529b\u3001SPRINT\u3001\u5bf9\u6bd4\u5b66\u4e60"}}
{"id": "2506.03399", "pdf": "https://arxiv.org/pdf/2506.03399", "abs": "https://arxiv.org/abs/2506.03399", "authors": ["Sean Steinle"], "title": "Sampling Preferences Yields Simple Trustworthiness Scores", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "With the onset of large language models (LLMs), the performance of artificial\nintelligence (AI) models is becoming increasingly multi-dimensional.\nAccordingly, there have been several large, multi-dimensional evaluation\nframeworks put forward to evaluate LLMs. Though these frameworks are much more\nrealistic than previous attempts which only used a single score like accuracy,\nmulti-dimensional evaluations can complicate decision-making since there is no\nobvious way to select an optimal model. This work introduces preference\nsampling, a method to extract a scalar trustworthiness score from\nmulti-dimensional evaluation results by considering the many characteristics of\nmodel performance which users value. We show that preference sampling improves\nupon alternate aggregation methods by using multi-dimensional trustworthiness\nevaluations of LLMs from TrustLLM and DecodingTrust. We find that preference\nsampling is consistently reductive, fully reducing the set of candidate models\n100% of the time whereas Pareto optimality never reduces the set by more than\n50%. Likewise, preference sampling is consistently sensitive to user\npriors-allowing users to specify the relative weighting and confidence of their\npreferences-whereas averaging scores is intransigent to the users' prior\nknowledge.", "AI": {"tldr": "\u5f15\u5165\u504f\u597d\u91c7\u6837\u65b9\u6cd5\uff0c\u4ece\u591a\u7ef4\u8bc4\u4f30\u7ed3\u679c\u4e2d\u63d0\u53d6\u6807\u91cf\u53ef\u4fe1\u5ea6\u5206\u6570\uff0c\u89e3\u51b3LLM\u8bc4\u4f30\u4e2d\u51b3\u7b56\u590d\u6742\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8868\u73b0\u8bc4\u4f30\u7ef4\u5ea6\u589e\u591a\uff0c\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\u867d\u66f4\u771f\u5b9e\u4f46\u51b3\u7b56\u590d\u6742\u3002", "method": "\u63d0\u51fa\u504f\u597d\u91c7\u6837\u65b9\u6cd5\uff0c\u7ed3\u5408\u7528\u6237\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u591a\u7ef4\u7279\u5f81\u504f\u597d\uff0c\u751f\u6210\u6807\u91cf\u53ef\u4fe1\u5ea6\u5206\u6570\u3002", "result": "\u504f\u597d\u91c7\u6837\u5728\u591a\u7ef4\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u5176\u4ed6\u805a\u5408\u65b9\u6cd5\uff0c\u80fd\u5b8c\u5168\u7f29\u51cf\u5019\u9009\u6a21\u578b\u96c6\uff0c\u4e14\u5bf9\u7528\u6237\u5148\u9a8c\u654f\u611f\u3002", "conclusion": "\u504f\u597d\u91c7\u6837\u4e3aLLM\u8bc4\u4f30\u63d0\u4f9b\u66f4\u9ad8\u6548\u548c\u7528\u6237\u5bfc\u5411\u7684\u51b3\u7b56\u5de5\u5177\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30,\u504f\u597d\u91c7\u6837,\u591a\u7ef4\u8bc4\u4f30,\u53ef\u4fe1\u5ea6\u8bc4\u5206"}}
{"id": "2506.03956", "pdf": "https://arxiv.org/pdf/2506.03956", "abs": "https://arxiv.org/abs/2506.03956", "authors": ["Aojun Lu", "Tao Feng", "Hangjie Yuan", "Chunhui Ding", "Yanan Sun"], "title": "Adapt before Continual Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Continual Learning (CL) seeks to enable neural networks to incrementally\nacquire new knowledge (plasticity) while retaining existing knowledge\n(stability). While pre-trained models (PTMs) have become pivotal in CL,\nprevailing approaches freeze the PTM backbone to preserve stability, limiting\ntheir plasticity, particularly when encountering significant domain gaps in\nincremental tasks. Conversely, sequentially finetuning the entire PTM risks\ncatastrophic forgetting of generalizable knowledge, exposing a critical\nstability-plasticity trade-off. To address this challenge, we propose Adapting\nPTMs before the core CL process (ACL), a novel framework that refines the PTM\nbackbone through a plug-and-play adaptation phase before learning each new task\nwith existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by\naligning embeddings with their original class prototypes while distancing them\nfrom others, theoretically and empirically shown to balance stability and\nplasticity. Extensive experiments demonstrate that ACL significantly improves\nCL performance across benchmarks and integrated methods, offering a versatile\nsolution for PTM-based CL.", "AI": {"tldr": "\u63d0\u51faACL\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\uff08PTM\uff09\u9002\u5e94\u9636\u6bb5\u5e73\u8861\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u51bb\u7ed3PTM\u4e3b\u5e72\u7f51\u7edc\u4ee5\u4fdd\u6301\u7a33\u5b9a\u6027\uff0c\u9650\u5236\u53ef\u5851\u6027\uff1b\u800c\u76f4\u63a5\u5fae\u8c03\u5219\u6613\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u3002\u9700\u8981\u5e73\u8861\u4e8c\u8005\u3002", "method": "ACL\u6846\u67b6\u5728\u6838\u5fc3\u6301\u7eed\u5b66\u4e60\u524d\u52a0\u5165PTM\u9002\u5e94\u9636\u6bb5\uff0c\u901a\u8fc7\u8c03\u6574\u5d4c\u5165\u4e0e\u7c7b\u539f\u578b\u7684\u8ddd\u79bb\u6765\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eACL\u663e\u8457\u63d0\u5347\u6301\u7eed\u5b66\u4e60\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u57fa\u51c6\u548c\u65b9\u6cd5\u3002", "conclusion": "ACL\u4e3a\u57fa\u4e8ePTM\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u6301\u7eed\u5b66\u4e60, \u9884\u8bad\u7ec3\u6a21\u578b, \u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u6743\u8861, ACL\u6846\u67b6"}}
{"id": "2506.03980", "pdf": "https://arxiv.org/pdf/2506.03980", "abs": "https://arxiv.org/abs/2506.03980", "authors": ["Takeshi Saga", "Catherine Pelachaud"], "title": "Voice Activity Projection Model with Multimodal Encoders", "categories": ["cs.CL"], "comment": null, "summary": "Turn-taking management is crucial for any social interaction. Still, it is\nchallenging to model human-machine interaction due to the complexity of the\nsocial context and its multimodal nature. Unlike conventional systems based on\nsilence duration, previous existing voice activity projection (VAP) models\nsuccessfully utilized a unified representation of turn-taking behaviors as\nprediction targets, which improved turn-taking prediction performance.\nRecently, a multimodal VAP model outperformed the previous state-of-the-art\nmodel by a significant margin. In this paper, we propose a multimodal model\nenhanced with pre-trained audio and face encoders to improve performance by\ncapturing subtle expressions. Our model performed competitively, and in some\ncases, even better than state-of-the-art models on turn-taking metrics. All the\nsource codes and pretrained models are available at\nhttps://github.com/sagatake/VAPwithAudioFaceEncoders.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9884\u8bad\u7ec3\u97f3\u9891\u548c\u9762\u90e8\u7f16\u7801\u5668\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u4e2d\u8f6e\u6d41\u5bf9\u8bdd\u7ba1\u7406\u7684\u6027\u80fd\u3002", "motivation": "\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u8f6e\u6d41\u5bf9\u8bdd\u7ba1\u7406\u7531\u4e8e\u793e\u4ea4\u73af\u5883\u7684\u590d\u6742\u6027\u548c\u591a\u6a21\u6001\u6027\u8d28\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u7684\u97f3\u9891\u548c\u9762\u90e8\u7f16\u7801\u5668\u589e\u5f3a\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4ee5\u6355\u6349\u7ec6\u5fae\u8868\u60c5\u3002", "result": "\u6a21\u578b\u5728\u4e00\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u6a21\u578b\u3002", "conclusion": "\u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u6a21\u578b\u80fd\u6709\u6548\u63d0\u5347\u8f6e\u6d41\u5bf9\u8bdd\u9884\u6d4b\u6027\u80fd\u3002", "keywords": "\u8f6e\u6d41\u5bf9\u8bdd\u7ba1\u7406\u3001\u591a\u6a21\u6001\u6a21\u578b\u3001\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u3001\u4eba\u673a\u4ea4\u4e92"}}
{"id": "2506.03964", "pdf": "https://arxiv.org/pdf/2506.03964", "abs": "https://arxiv.org/abs/2506.03964", "authors": ["HyunGi Kim", "Jisoo Mok", "Dongjun Lee", "Jaihyun Lew", "Sungjae Kim", "Sungroh Yoon"], "title": "Causality-Aware Contrastive Learning for Robust Multivariate Time-Series Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Utilizing the complex inter-variable causal relationships within multivariate\ntime-series provides a promising avenue toward more robust and reliable\nmultivariate time-series anomaly detection (MTSAD) but remains an underexplored\narea of research. This paper proposes Causality-Aware contrastive learning for\nRObust multivariate Time-Series (CAROTS), a novel MTSAD pipeline that\nincorporates the notion of causality into contrastive learning. CAROTS employs\ntwo data augmentors to obtain causality-preserving and -disturbing samples that\nserve as a wide range of normal variations and synthetic anomalies,\nrespectively. With causality-preserving and -disturbing samples as positives\nand negatives, CAROTS performs contrastive learning to train an encoder whose\nlatent space separates normal and abnormal samples based on causality.\nMoreover, CAROTS introduces a similarity-filtered one-class contrastive loss\nthat encourages the contrastive learning process to gradually incorporate more\nsemantically diverse samples with common causal relationships. Extensive\nexperiments on five real-world and two synthetic datasets validate that the\nintegration of causal relationships endows CAROTS with improved MTSAD\ncapabilities. The code is available at https://github.com/kimanki/CAROTS.", "AI": {"tldr": "CAROTS\u5229\u7528\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\uff0c\u4f7f\u7528\u4e24\u79cd\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u751f\u6210\u6b63\u5e38\u548c\u5f02\u5e38\u6837\u672c\uff0c\u5e76\u5f15\u5165\u76f8\u4f3c\u6027\u8fc7\u6ee4\u7684\u5bf9\u6bd4\u635f\u5931\u3002", "motivation": "\u63a2\u7d22\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u4e2d\u56e0\u679c\u5173\u7cfb\u5bf9\u5f02\u5e38\u68c0\u6d4b\u7684\u6f5c\u5728\u63d0\u5347\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51faCAROTS\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u5173\u7cfb\u4fdd\u7559\u548c\u6270\u52a8\u6837\u672c\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5229\u7528\u76f8\u4f3c\u6027\u8fc7\u6ee4\u7684\u5bf9\u6bd4\u635f\u5931\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u548c\u4e24\u4e2a\u5408\u6210\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86CAROTS\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u5f15\u5165\u56e0\u679c\u5173\u7cfb\u80fd\u663e\u8457\u63d0\u5347\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u80fd\u529b\u3002", "keywords": "\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u3001\u5f02\u5e38\u68c0\u6d4b\u3001\u56e0\u679c\u5173\u7cfb\u3001\u5bf9\u6bd4\u5b66\u4e60"}}
{"id": "2506.03984", "pdf": "https://arxiv.org/pdf/2506.03984", "abs": "https://arxiv.org/abs/2506.03984", "authors": ["Carolin Holtermann", "Paul R\u00f6ttger", "Anne Lauscher"], "title": "Around the World in 24 Hours: Probing LLM Knowledge of Time and Place", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning over time and space is essential for understanding our world.\nHowever, the abilities of language models in this area are largely unexplored\nas previous work has tested their abilities for logical reasoning in terms of\ntime and space in isolation or only in simple or artificial environments. In\nthis paper, we present the first evaluation of the ability of language models\nto jointly reason over time and space. To enable our analysis, we create\nGeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37\ntime zones. Using GeoTemp, we evaluate eight open chat models of three\ndifferent model families for different combinations of temporal and geographic\nknowledge. We find that most models perform well on reasoning tasks involving\nonly temporal knowledge and that overall performance improves with scale.\nHowever, performance remains constrained in tasks that require connecting\ntemporal and geographical information. We do not find clear correlations of\nperformance with specific geographic regions. Instead, we find a significant\nperformance increase for location names with low model perplexity, suggesting\ntheir repeated occurrence during model training. We further demonstrate that\ntheir performance is heavily influenced by prompt formulation - a direct\ninjection of geographical knowledge leads to performance gains, whereas,\nsurprisingly, techniques like chain-of-thought prompting decrease performance\non simpler tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u8bc4\u4f30\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u4e0a\u7684\u8054\u5408\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u7eaf\u65f6\u95f4\u77e5\u8bc6\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u65f6\u7a7a\u7ed3\u5408\u4efb\u52a1\u4e2d\u8868\u73b0\u53d7\u9650\u3002\u8868\u73b0\u4e0e\u5730\u7406\u4f4d\u7f6e\u65e0\u5173\uff0c\u4f46\u4e0e\u6a21\u578b\u5bf9\u5730\u540d\u7684\u719f\u6089\u5ea6\u76f8\u5173\u3002\u63d0\u793a\u683c\u5f0f\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u8054\u5408\u63a8\u7406\u4e2d\u7684\u80fd\u529b\uff0c\u586b\u8865\u6b64\u524d\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u521b\u5efaGeoTemp\u6570\u636e\u96c6\uff0832\u4e07\u6761\u63d0\u793a\uff0c\u8986\u76d6289\u4e2a\u57ce\u5e02\u548c37\u4e2a\u65f6\u533a\uff09\uff0c\u8bc4\u4f308\u4e2a\u5f00\u653e\u804a\u5929\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u7eaf\u65f6\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u65f6\u7a7a\u7ed3\u5408\u4efb\u52a1\u8868\u73b0\u53d7\u9650\uff1b\u8868\u73b0\u4e0e\u5730\u540d\u719f\u6089\u5ea6\u6b63\u76f8\u5173\uff1b\u63d0\u793a\u683c\u5f0f\u663e\u8457\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u7a7a\u8054\u5408\u63a8\u7406\u4e2d\u4ecd\u6709\u5c40\u9650\uff0c\u9700\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u548c\u65b9\u6cd5\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b,\u65f6\u7a7a\u63a8\u7406,GeoTemp\u6570\u636e\u96c6,\u63d0\u793a\u683c\u5f0f"}}
{"id": "2506.03407", "pdf": "https://arxiv.org/pdf/2506.03407", "abs": "https://arxiv.org/abs/2506.03407", "authors": ["Lukas Meyer", "Josef Gr\u00fcn", "Maximilian Weiherer", "Bernhard Egger", "Marc Stamminger", "Linus Franke"], "title": "Multi-Spectral Gaussian Splatting with Neural Color Representation", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "We present MS-Splatting -- a multi-spectral 3D Gaussian Splatting (3DGS)\nframework that is able to generate multi-view consistent novel views from\nimages of multiple, independent cameras with different spectral domains. In\ncontrast to previous approaches, our method does not require cross-modal camera\ncalibration and is versatile enough to model a variety of different spectra,\nincluding thermal and near-infra red, without any algorithmic changes.\n  Unlike existing 3DGS-based frameworks that treat each modality separately (by\noptimizing per-channel spherical harmonics) and therefore fail to exploit the\nunderlying spectral and spatial correlations, our method leverages a novel\nneural color representation that encodes multi-spectral information into a\nlearned, compact, per-splat feature embedding. A shallow multi-layer perceptron\n(MLP) then decodes this embedding to obtain spectral color values, enabling\njoint learning of all bands within a unified representation.\n  Our experiments show that this simple yet effective strategy is able to\nimprove multi-spectral rendering quality, while also leading to improved\nper-spectra rendering quality over state-of-the-art methods. We demonstrate the\neffectiveness of this new technique in agricultural applications to render\nvegetation indices, such as normalized difference vegetation index (NDVI).", "AI": {"tldr": "MS-Splatting\u662f\u4e00\u79cd\u591a\u5149\u8c313D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u65e0\u9700\u8de8\u6a21\u6001\u76f8\u673a\u6821\u51c6\u5373\u53ef\u4ece\u4e0d\u540c\u5149\u8c31\u57df\u7684\u56fe\u50cf\u751f\u6210\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u65b0\u89c6\u56fe\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u591a\u5149\u8c31\u6570\u636e\u65f6\u9700\u8981\u72ec\u7acb\u5904\u7406\u5404\u6a21\u6001\u3001\u65e0\u6cd5\u5229\u7528\u5149\u8c31\u548c\u7a7a\u95f4\u5173\u8054\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u989c\u8272\u8868\u793a\u5c06\u591a\u5149\u8c31\u4fe1\u606f\u7f16\u7801\u4e3a\u7d27\u51d1\u7684\u6bcf\u6cfc\u6e85\u7279\u5f81\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u6d45\u5c42MLP\u89e3\u7801\u5149\u8c31\u989c\u8272\u503c\uff0c\u5b9e\u73b0\u7edf\u4e00\u8868\u5f81\u7684\u8054\u5408\u5b66\u4e60\u3002", "result": "\u63d0\u5347\u4e86\u591a\u5149\u8c31\u6e32\u67d3\u8d28\u91cf\uff0c\u4e14\u5728\u5355\u5149\u8c31\u6e32\u67d3\u8d28\u91cf\u4e0a\u4e5f\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u5149\u8c31\u6e32\u67d3\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u519c\u4e1a\u9886\u57df\u7684\u690d\u88ab\u6307\u6570\u6e32\u67d3\u4e2d\u6548\u679c\u663e\u8457\u3002", "keywords": "\u591a\u5149\u8c31, 3D\u9ad8\u65af\u6cfc\u6e85, \u795e\u7ecf\u989c\u8272\u8868\u793a, \u519c\u4e1a\u5e94\u7528, NDVI"}}
{"id": "2506.03979", "pdf": "https://arxiv.org/pdf/2506.03979", "abs": "https://arxiv.org/abs/2506.03979", "authors": ["Haoxuan Chen", "Yinuo Ren", "Martin Renqiang Min", "Lexing Ying", "Zachary Izzo"], "title": "Solving Inverse Problems via Diffusion-Based Priors: An Approximation-Free Ensemble Sampling Approach", "categories": ["cs.LG", "cs.CV", "cs.NA", "eess.IV", "math.NA", "stat.ML"], "comment": "45 pages", "summary": "Diffusion models (DMs) have proven to be effective in modeling\nhigh-dimensional distributions, leading to their widespread adoption for\nrepresenting complex priors in Bayesian inverse problems (BIPs). However,\ncurrent DM-based posterior sampling methods proposed for solving common BIPs\nrely on heuristic approximations to the generative process. To exploit the\ngenerative capability of DMs and avoid the usage of such approximations, we\npropose an ensemble-based algorithm that performs posterior sampling without\nthe use of heuristic approximations. Our algorithm is motivated by existing\nworks that combine DM-based methods with the sequential Monte Carlo (SMC)\nmethod. By examining how the prior evolves through the diffusion process\nencoded by the pre-trained score function, we derive a modified partial\ndifferential equation (PDE) governing the evolution of the corresponding\nposterior distribution. This PDE includes a modified diffusion term and a\nreweighting term, which can be simulated via stochastic weighted particle\nmethods. Theoretically, we prove that the error between the true posterior\ndistribution can be bounded in terms of the training error of the pre-trained\nscore function and the number of particles in the ensemble. Empirically, we\nvalidate our algorithm on several inverse problems in imaging to show that our\nmethod gives more accurate reconstructions compared to existing DM-based\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u96c6\u5408\u7b97\u6cd5\uff0c\u7528\u4e8e\u8d1d\u53f6\u65af\u9006\u95ee\u9898\u7684\u540e\u9a8c\u91c7\u6837\uff0c\u907f\u514d\u4e86\u542f\u53d1\u5f0f\u8fd1\u4f3c\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u540e\u9a8c\u91c7\u6837\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u8fd1\u4f3c\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u751f\u6210\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u786e\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u6269\u6563\u6a21\u578b\u4e0e\u987a\u5e8f\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6269\u6563\u8fc7\u7a0b\uff0c\u63a8\u5bfc\u4e86\u4fee\u6b63\u7684\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u968f\u673a\u52a0\u6743\u7c92\u5b50\u65b9\u6cd5\u8fdb\u884c\u6a21\u62df\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u540e\u9a8c\u5206\u5e03\u7684\u8bef\u5dee\u6709\u754c\uff0c\u5b9e\u9a8c\u5728\u591a\u4e2a\u6210\u50cf\u9006\u95ee\u9898\u4e2d\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u91cd\u5efa\u51c6\u786e\u6027\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u907f\u514d\u4e86\u542f\u53d1\u5f0f\u8fd1\u4f3c\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9645\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u8d1d\u53f6\u65af\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u6269\u6563\u6a21\u578b, \u8d1d\u53f6\u65af\u9006\u95ee\u9898, \u540e\u9a8c\u91c7\u6837, \u987a\u5e8f\u8499\u7279\u5361\u6d1b, \u504f\u5fae\u5206\u65b9\u7a0b"}}
{"id": "2506.03989", "pdf": "https://arxiv.org/pdf/2506.03989", "abs": "https://arxiv.org/abs/2506.03989", "authors": ["Alex Laitenberger", "Christopher D. Manning", "Nelson F. Liu"], "title": "Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models", "categories": ["cs.CL"], "comment": "10 pages, 5 figures, for associated source code, see\n  https://github.com/alex-laitenberger/stronger-baselines-rag", "summary": "With the rise of long-context language models (LMs) capable of processing\ntens of thousands of tokens in a single pass, do multi-stage\nretrieval-augmented generation (RAG) pipelines still offer measurable benefits\nover simpler, single-stage approaches? To assess this question, we conduct a\ncontrolled evaluation for QA tasks under systematically scaled token budgets,\ncomparing two recent multi-stage pipelines, ReadAgent and RAPTOR, against three\nbaselines, including DOS RAG (Document's Original Structure RAG), a simple\nretrieve-then-read method that preserves original passage order. Despite its\nstraightforward design, DOS RAG consistently matches or outperforms more\nintricate methods on multiple long-context QA benchmarks. We recommend\nestablishing DOS RAG as a simple yet strong baseline for future RAG\nevaluations, pairing it with emerging embedding and language models to assess\ntrade-offs between complexity and effectiveness as model capabilities evolve.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u9636\u6bb5\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6d41\u7a0b\u4e0e\u5355\u9636\u6bb5\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u7b80\u5355\u7684DOS RAG\u65b9\u6cd5\u5728\u591a\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u590d\u6742\u65b9\u6cd5\uff0c\u5efa\u8bae\u5c06\u5176\u4f5c\u4e3a\u672a\u6765RAG\u8bc4\u4f30\u7684\u57fa\u51c6\u3002", "motivation": "\u63a2\u7a76\u5728\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u80fd\u529b\u63d0\u5347\u7684\u80cc\u666f\u4e0b\uff0c\u591a\u9636\u6bb5RAG\u6d41\u7a0b\u662f\u5426\u4ecd\u6bd4\u5355\u9636\u6bb5\u65b9\u6cd5\u6709\u663e\u8457\u4f18\u52bf\u3002", "method": "\u5bf9QA\u4efb\u52a1\u8fdb\u884c\u63a7\u5236\u6027\u8bc4\u4f30\uff0c\u6bd4\u8f83\u4e24\u79cd\u591a\u9636\u6bb5RAG\u6d41\u7a0b\uff08ReadAgent\u548cRAPTOR\uff09\u4e0e\u4e09\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff08\u5305\u62ec\u7b80\u5355\u7684DOS RAG\u65b9\u6cd5\uff09\u3002", "result": "DOS RAG\u5728\u591a\u4e2a\u957f\u4e0a\u4e0b\u6587QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u590d\u6742\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u5efa\u8bae\u5c06DOS RAG\u4f5c\u4e3a\u672a\u6765RAG\u8bc4\u4f30\u7684\u7b80\u5355\u6709\u6548\u57fa\u51c6\uff0c\u5e76\u4e0e\u65b0\u5174\u5d4c\u5165\u548c\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u4ee5\u8bc4\u4f30\u590d\u6742\u6027\u4e0e\u6548\u679c\u7684\u6743\u8861\u3002", "keywords": "\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b, \u68c0\u7d22\u589e\u5f3a\u751f\u6210, QA\u4efb\u52a1, DOS RAG"}}
{"id": "2506.03425", "pdf": "https://arxiv.org/pdf/2506.03425", "abs": "https://arxiv.org/abs/2506.03425", "authors": ["Petr Grinberg", "Ankur Kumar", "Surya Koppisetti", "Gaurav Bharaj"], "title": "A Data-Driven Diffusion-based Approach for Audio Deepfake Explanations", "categories": ["eess.AS", "cs.AI", "cs.LG"], "comment": "5 pages, 3 figures, accepted at Interspeech 2025", "summary": "Evaluating explainability techniques, such as SHAP and LRP, in the context of\naudio deepfake detection is challenging due to lack of clear ground truth\nannotations. In the cases when we are able to obtain the ground truth, we find\nthat these methods struggle to provide accurate explanations. In this work, we\npropose a novel data-driven approach to identify artifact regions in deepfake\naudio. We consider paired real and vocoded audio, and use the difference in\ntime-frequency representation as the ground-truth explanation. The difference\nsignal then serves as a supervision to train a diffusion model to expose the\ndeepfake artifacts in a given vocoded audio. Experimental results on the VocV4\nand LibriSeVoc datasets demonstrate that our method outperforms traditional\nexplainability techniques, both qualitatively and quantitatively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\u4e2d\u7684\u4f2a\u5f71\u533a\u57df\uff0c\u901a\u8fc7\u63d0\u53d6\u771f\u5b9e\u548c\u58f0\u97f3\u5904\u7406\u97f3\u9891\u7684\u65f6\u9891\u8868\u793a\u5dee\u5f02\u4f5c\u4e3a\u771f\u5b9e\u89e3\u91ca\uff0c\u5e76\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u6765\u66b4\u9732\u4f2a\u5f71\uff0c\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u89e3\u91ca\u6027\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\uff0cSHAP\u548cLRP\u7b49\u89e3\u91ca\u6027\u6280\u672f\u7531\u4e8e\u7f3a\u4e4f\u6e05\u6670\u7684\u5730\u9762\u771f\u5b9e\u6ce8\u91ca\u800c\u96be\u4ee5\u8bc4\u4f30\u3002\u5373\u4f7f\u80fd\u591f\u83b7\u5f97\u5730\u9762\u771f\u5b9e\u6570\u636e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4e5f\u65e0\u6cd5\u63d0\u4f9b\u51c6\u786e\u7684\u89e3\u91ca\u3002", "method": "\u4f7f\u7528\u914d\u5bf9\u771f\u5b9e\u548c\u58f0\u97f3\u5904\u7406\u97f3\u9891\uff0c\u63d0\u53d6\u5176\u65f6\u9891\u8868\u793a\u7684\u5dee\u5f02\u4f5c\u4e3a\u5730\u9762\u771f\u5b9e\u89e3\u91ca\u3002\u901a\u8fc7\u5dee\u5f02\u4fe1\u53f7\u76d1\u7763\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u66b4\u9732\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\u4e2d\u7684\u4f2a\u5f71\u3002", "result": "\u5728VocV4\u548cLibriSeVoc\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u7684\u89e3\u91ca\u6027\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u4e3a\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u89e3\u91ca\u6027\u6280\u672f\uff0c\u663e\u8457\u6539\u5584\u4e86\u4f2a\u5f71\u533a\u57df\u7684\u8bc6\u522b\u6548\u679c\u3002", "keywords": "\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b,\u4f2a\u5f71\u533a\u57df,\u6570\u636e\u9a71\u52a8\u65b9\u6cd5,\u6269\u6563\u6a21\u578b,\u65f6\u9891\u8868\u793a"}}
{"id": "2506.03996", "pdf": "https://arxiv.org/pdf/2506.03996", "abs": "https://arxiv.org/abs/2506.03996", "authors": ["Lianfeng Shi", "Ao Li", "Benjamin Ward-Cherrier"], "title": "Optimal Spiking Brain Compression: Improving One-Shot Post-Training Pruning and Quantization for Spiking Neural Networks", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Spiking Neural Networks (SNNs) have emerged as a new generation of\nenergy-efficient neural networks suitable for implementation on neuromorphic\nhardware. As neuromorphic hardware has limited memory and computing resources,\nweight pruning and quantization have recently been explored to improve SNNs'\nefficiency. State-of-the-art SNN pruning/quantization methods employ multiple\ncompression and training iterations, increasing the cost for pre-trained or\nvery large SNNs. In this paper, we propose a new one-shot post-training\npruning/quantization framework, Optimal Spiking Brain Compression (OSBC), that\nadapts the Optimal Brain Compression (OBC) method of [Frantar, Singh, and\nAlistarh, 2023] for SNNs. Rather than minimizing the loss on neuron input\ncurrent as OBC does, OSBC achieves more efficient and accurate SNN compression\nin one pass by minimizing the loss on spiking neuron membrane potential with a\nsmall sample dataset. Our experiments on neuromorphic datasets (N-MNIST,\nCIFAR10-DVS, DVS128-Gesture) demonstrate that OSBC can achieve 97% sparsity\nthrough pruning with 1.41%, 10.20%, and 1.74% accuracy loss, or 4-bit symmetric\nquantization with 0.17%, 1.54%, and 7.71% accuracy loss, respectively. Code\nwill be available on GitHub.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u4e00\u6b65\u5f0f\u540e\u8bad\u7ec3\u526a\u679d/\u91cf\u5316\u6846\u67b6OSBC\uff0c\u7528\u4e8e\u63d0\u9ad8\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u6548\u7387\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u538b\u7f29\u548c\u8bad\u7ec3\u7684\u6210\u672c\u3002", "motivation": "\u7531\u4e8e\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u8d44\u6e90\u53d7\u9650\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u591a\u6b21\u538b\u7f29\u548c\u8bad\u7ec3\u8fed\u4ee3\uff0c\u6210\u672c\u9ad8\u3002", "method": "\u57fa\u4e8eOptimal Brain Compression\uff08OBC\uff09\u65b9\u6cd5\u6539\u8fdb\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u8109\u51b2\u795e\u7ecf\u5143\u819c\u7535\u4f4d\u7684\u635f\u5931\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOSBC\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8697%\u7684\u7a00\u758f\u5ea6\uff0c\u51c6\u786e\u7387\u635f\u5931\u8f83\u5c0f\u3002", "conclusion": "OSBC\u4e3aSNN\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "Spiking Neural Networks, weight pruning, quantization, neuromorphic hardware"}}
{"id": "2506.03990", "pdf": "https://arxiv.org/pdf/2506.03990", "abs": "https://arxiv.org/abs/2506.03990", "authors": ["Hongzhi Zhang", "Jingyuan Zhang", "Xingguang Ji", "Qi Wang", "Fuzheng Zhang"], "title": "DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Typical video modeling methods, such as LLava, represent videos as sequences\nof visual tokens, which are then processed by the LLM backbone for effective\nvideo understanding. However, this approach leads to a massive number of visual\ntokens, especially for long videos. A practical solution is to first extract\nrelevant visual information from the large visual context before feeding it\ninto the LLM backbone, thereby reducing computational overhead. In this work,\nwe introduce DynTok, a novel \\textbf{Dyn}amic video \\textbf{Tok}en compression\nstrategy. DynTok adaptively splits visual tokens into groups and merges them\nwithin each group, achieving high compression in regions with low information\ndensity while preserving essential content. Our method reduces the number of\ntokens to 44.4% of the original size while maintaining comparable performance.\nIt further benefits from increasing the number of video frames and achieves\n65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective\ncompression method, we expose the redundancy in video token representations and\noffer insights for designing more efficient video modeling techniques.", "AI": {"tldr": "DynTok\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u89c6\u9891\u4ee4\u724c\u538b\u7f29\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u7ec4\u548c\u5408\u5e76\u4ee4\u724c\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5efa\u6a21\u65b9\u6cd5\u5bfc\u81f4\u89c6\u89c9\u4ee4\u724c\u6570\u91cf\u5e9e\u5927\uff0c\u5c24\u5176\u662f\u957f\u89c6\u9891\uff0c\u4e9f\u9700\u4e00\u79cd\u65b9\u6cd5\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "DynTok\u5c06\u89c6\u89c9\u4ee4\u724c\u81ea\u9002\u5e94\u5206\u7ec4\u5e76\u5408\u5e76\uff0c\u9ad8\u4fe1\u606f\u5bc6\u5ea6\u533a\u57df\u4fdd\u7559\uff0c\u4f4e\u5bc6\u5ea6\u533a\u57df\u9ad8\u538b\u7f29\u3002", "result": "\u4ee4\u724c\u6570\u91cf\u51cf\u5c11\u81f344.4%\uff0c\u6027\u80fd\u76f8\u5f53\uff0c\u89c6\u9891\u5e27\u6570\u589e\u52a0\u65f6\u6548\u679c\u66f4\u597d\uff08Video-MME 65.3%\uff0cMLVU 72.5%\uff09\u3002", "conclusion": "\u63ed\u793a\u4e86\u89c6\u9891\u4ee4\u724c\u8868\u793a\u7684\u5197\u4f59\uff0c\u4e3a\u9ad8\u6548\u89c6\u9891\u5efa\u6a21\u6280\u672f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u542f\u793a\u3002", "keywords": "\u89c6\u9891\u5efa\u6a21\uff0c\u4ee4\u724c\u538b\u7f29\uff0c\u52a8\u6001\u5206\u7ec4\uff0c\u8ba1\u7b97\u6548\u7387"}}
{"id": "2506.04001", "pdf": "https://arxiv.org/pdf/2506.04001", "abs": "https://arxiv.org/abs/2506.04001", "authors": ["Han Ji", "Yuqi Feng", "Jiahao Fan", "Yanan Sun"], "title": "CARL: Causality-guided Architecture Representation Learning for an Interpretable Performance Predictor", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Performance predictors have emerged as a promising method to accelerate the\nevaluation stage of neural architecture search (NAS). These predictors estimate\nthe performance of unseen architectures by learning from the correlation\nbetween a small set of trained architectures and their performance. However,\nmost existing predictors ignore the inherent distribution shift between limited\ntraining samples and diverse test samples. Hence, they tend to learn spurious\ncorrelations as shortcuts to predictions, leading to poor generalization. To\naddress this, we propose a Causality-guided Architecture Representation\nLearning (CARL) method aiming to separate critical (causal) and redundant\n(non-causal) features of architectures for generalizable architecture\nperformance prediction. Specifically, we employ a substructure extractor to\nsplit the input architecture into critical and redundant substructures in the\nlatent space. Then, we generate multiple interventional samples by pairing\ncritical representations with diverse redundant representations to prioritize\ncritical features. Extensive experiments on five NAS search spaces demonstrate\nthe state-of-the-art accuracy and superior interpretability of CARL. For\ninstance, CARL achieves 97.67% top-1 accuracy on CIFAR-10 using DARTS.", "AI": {"tldr": "CARL\u662f\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u5173\u7cfb\u5f15\u5bfc\u7684\u67b6\u6784\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u67b6\u6784\u4e2d\u7684\u5173\u952e\u548c\u5197\u4f59\u7279\u5f81\uff0c\u63d0\u5347\u4e86\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u4e2d\u6027\u80fd\u9884\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6027\u80fd\u9884\u6d4b\u65b9\u6cd5\u5ffd\u7565\u4e86\u8bad\u7ec3\u6837\u672c\u4e0e\u6d4b\u8bd5\u6837\u672c\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\uff0c\u5bb9\u6613\u5b66\u4e60\u865a\u5047\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "CARL\u901a\u8fc7\u5b50\u7ed3\u6784\u63d0\u53d6\u5668\u5c06\u67b6\u6784\u5206\u4e3a\u5173\u952e\u548c\u5197\u4f59\u5b50\u7ed3\u6784\uff0c\u751f\u6210\u5e72\u9884\u6837\u672c\u4ee5\u4f18\u5148\u8003\u8651\u5173\u952e\u7279\u5f81\u3002", "result": "\u5728\u4e94\u4e2aNAS\u641c\u7d22\u7a7a\u95f4\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCARL\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u4f8b\u5982\u5728DARTS\u4e0a\u8fbe\u523097.67%\u7684top-1\u51c6\u786e\u7387\u3002", "conclusion": "CARL\u901a\u8fc7\u56e0\u679c\u5173\u7cfb\u5f15\u5bfc\u7684\u8868\u793a\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u9884\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u89e3\u91ca\u6027\u3002", "keywords": "\u795e\u7ecf\u67b6\u6784\u641c\u7d22, \u6027\u80fd\u9884\u6d4b, \u56e0\u679c\u5173\u7cfb, \u8868\u793a\u5b66\u4e60, \u6cdb\u5316\u80fd\u529b"}}
{"id": "2506.03993", "pdf": "https://arxiv.org/pdf/2506.03993", "abs": "https://arxiv.org/abs/2506.03993", "authors": ["Saif M. Mohammad"], "title": "Words of Warmth: Trust and Sociability Norms for over 26k English Words", "categories": ["cs.CL", "cs.CY"], "comment": "In Proceedings of ACL 2025 Main", "summary": "Social psychologists have shown that Warmth (W) and Competence (C) are the\nprimary dimensions along which we assess other people and groups. These\ndimensions impact various aspects of our lives from social competence and\nemotion regulation to success in the work place and how we view the world. More\nrecent work has started to explore how these dimensions develop, why they have\ndeveloped, and what they constitute. Of particular note, is the finding that\nwarmth has two distinct components: Trust (T) and Sociability (S). In this\nwork, we introduce Words of Warmth, the first large-scale repository of\nmanually derived word--warmth (as well as word--trust and word--sociability)\nassociations for over 26k English words. We show that the associations are\nhighly reliable. We use the lexicons to study the rate at which children\nacquire WCTS words with age. Finally, we show that the lexicon enables a wide\nvariety of bias and stereotype research through case studies on various target\nentities. Words of Warmth is freely available at:\nhttp://saifmohammad.com/warmth.html", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u624b\u5de5\u8bcd\u5e93\u201cWords of Warmth\u201d\uff0c\u5305\u542b26,000\u591a\u4e2a\u82f1\u6587\u5355\u8bcd\u4e0e\u6e29\u6696\u3001\u4fe1\u4efb\u548c\u793e\u4ea4\u6027\u7684\u5173\u8054\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u53ef\u9760\u6027\u3002", "motivation": "\u63a2\u7d22\u6e29\u6696\u3001\u4fe1\u4efb\u548c\u793e\u4ea4\u6027\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u7684\u91cd\u8981\u6027\u53ca\u5176\u53d1\u5c55\u8fc7\u7a0b\u3002", "method": "\u6784\u5efa\u4e86\u201cWords of Warmth\u201d\u8bcd\u5e93\uff0c\u5206\u6790\u4e86\u513f\u7ae5\u5b66\u4e60\u76f8\u5173\u8bcd\u6c47\u7684\u901f\u7387\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u8bcd\u5e93\u5728\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u8bcd\u5e93\u5177\u6709\u9ad8\u53ef\u9760\u6027\uff0c\u53ef\u7528\u4e8e\u7814\u7a76\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u3002", "conclusion": "\u201cWords of Warmth\u201d\u652f\u6301\u591a\u79cd\u793e\u4f1a\u5fc3\u7406\u5b66\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5173\u4e8e\u504f\u89c1\u7684\u5206\u6790\u3002", "keywords": "\u6e29\u6696\uff0c\u4fe1\u4efb\uff0c\u793e\u4ea4\u6027\uff0c\u8bcd\u5e93\uff0c\u504f\u89c1\u7814\u7a76"}}
{"id": "2506.04026", "pdf": "https://arxiv.org/pdf/2506.04026", "abs": "https://arxiv.org/abs/2506.04026", "authors": ["Cl\u00e9ment B\u00e9nesse", "Patrick Mesana", "Ath\u00e9na\u00efs Gautier", "S\u00e9bastien Gambs"], "title": "On the Usage of Gaussian Process for Efficient Data Valuation", "categories": ["cs.LG"], "comment": null, "summary": "In machine learning, knowing the impact of a given datum on model training is\na fundamental task referred to as Data Valuation. Building on previous works\nfrom the literature, we have designed a novel canonical decomposition allowing\npractitioners to analyze any data valuation method as the combination of two\nparts: a utility function that captures characteristics from a given model and\nan aggregation procedure that merges such information. We also propose to use\nGaussian Processes as a means to easily access the utility function on\n``sub-models'', which are models trained on a subset of the training set. The\nstrength of our approach stems from both its theoretical grounding in Bayesian\ntheory, and its practical reach, by enabling fast estimation of valuations\nthanks to efficient update formulae.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u4ef7\u503c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u4ef7\u503c\u5206\u89e3\u4e3a\u6548\u7528\u51fd\u6570\u548c\u805a\u5408\u8fc7\u7a0b\u4e24\u90e8\u5206\uff0c\u5e76\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u5b9e\u73b0\u5feb\u901f\u4f30\u503c\u3002", "motivation": "\u5728\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u91cf\u5316\u8bad\u7ec3\u6570\u636e\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\u662f\u4e00\u4e2a\u57fa\u7840\u4efb\u52a1\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u7406\u8bba\u652f\u6491\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89c4\u8303\u5206\u89e3\u65b9\u6cd5\uff0c\u5c06\u6570\u636e\u4ef7\u503c\u8bc4\u4f30\u5206\u4e3a\u6548\u7528\u51fd\u6570\u548c\u805a\u5408\u8fc7\u7a0b\uff0c\u5e76\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u5728\u5b50\u6a21\u578b\u4e0a\u5b9e\u73b0\u5feb\u901f\u4f30\u503c\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5177\u6709\u8d1d\u53f6\u65af\u7406\u8bba\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8fd8\u80fd\u901a\u8fc7\u9ad8\u6548\u66f4\u65b0\u516c\u5f0f\u5b9e\u73b0\u5feb\u901f\u4f30\u503c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6570\u636e\u4ef7\u503c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u6570\u636e\u4ef7\u503c\u8bc4\u4f30, \u6548\u7528\u51fd\u6570, \u9ad8\u65af\u8fc7\u7a0b, \u8d1d\u53f6\u65af\u7406\u8bba, \u673a\u5668\u5b66\u4e60"}}
{"id": "2506.03994", "pdf": "https://arxiv.org/pdf/2506.03994", "abs": "https://arxiv.org/abs/2506.03994", "authors": ["Dan Oneata", "Desmond Elliott", "Stella Frank"], "title": "Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era", "categories": ["cs.CL", "cs.CV"], "comment": "ACL Findings 2025", "summary": "Human learning and conceptual representation is grounded in sensorimotor\nexperience, in contrast to state-of-the-art foundation models. In this paper,\nwe investigate how well such large-scale models, trained on vast quantities of\ndata, represent the semantic feature norms of concrete object concepts, e.g. a\nROSE is red, smells sweet, and is a flower. More specifically, we use probing\ntasks to test which properties of objects these models are aware of. We\nevaluate image encoders trained on image data alone, as well as\nmultimodally-trained image encoders and language-only models, on predicting an\nextended denser version of the classic McRae norms and the newer Binder dataset\nof attribute ratings. We find that multimodal image encoders slightly\noutperform language-only approaches, and that image-only encoders perform\ncomparably to the language models, even on non-visual attributes that are\nclassified as \"encyclopedic\" or \"function\". These results offer new insights\ninto what can be learned from pure unimodal learning, and the complementarity\nof the modalities.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\u5982\u4f55\u8868\u793a\u5177\u4f53\u7269\u4f53\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u56fe\u50cf\u7f16\u7801\u5668\u7565\u4f18\u4e8e\u7eaf\u8bed\u8a00\u6a21\u578b\uff0c\u5355\u6a21\u6001\u56fe\u50cf\u7f16\u7801\u5668\u4e0e\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u76f8\u5f53\u3002", "motivation": "\u63a2\u8ba8\u4eba\u7c7b\u5b66\u4e60\u4e0e\u6982\u5ff5\u8868\u5f81\u5982\u4f55\u57fa\u4e8e\u611f\u77e5\u8fd0\u52a8\u7ecf\u9a8c\uff0c\u5e76\u5bf9\u6bd4\u5f53\u524d\u5927\u89c4\u6a21\u6a21\u578b\u7684\u8bed\u4e49\u7279\u5f81\u8868\u793a\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u63a2\u6d4b\u4efb\u52a1\u8bc4\u4f30\u7269\u4f53\u5c5e\u6027\u9884\u6d4b\u80fd\u529b\uff0c\u6d4b\u8bd5\u4e86\u5355\u6a21\u6001\u56fe\u50cf\u7f16\u7801\u5668\u3001\u591a\u6a21\u6001\u56fe\u50cf\u7f16\u7801\u5668\u548c\u7eaf\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u591a\u6a21\u6001\u56fe\u50cf\u7f16\u7801\u5668\u8868\u73b0\u7565\u4f18\uff1b\u5355\u6a21\u6001\u56fe\u50cf\u7f16\u7801\u5668\u4e0e\u8bed\u8a00\u6a21\u578b\u6548\u679c\u76f8\u5f53\uff0c\u5305\u62ec\u975e\u89c6\u89c9\u5c5e\u6027\u3002", "conclusion": "\u7ed3\u679c\u63ed\u793a\u4e86\u5355\u6a21\u6001\u5b66\u4e60\u7684\u6f5c\u529b\u53ca\u591a\u6a21\u6001\u4e92\u8865\u6027\u3002", "keywords": "\u8bed\u4e49\u7279\u5f81\u3001\u591a\u6a21\u6001\u5b66\u4e60\u3001\u56fe\u50cf\u7f16\u7801\u5668\u3001\u8bed\u8a00\u6a21\u578b\u3001\u8ba4\u77e5\u79d1\u5b66"}}
{"id": "2506.04053", "pdf": "https://arxiv.org/pdf/2506.04053", "abs": "https://arxiv.org/abs/2506.04053", "authors": ["Alexander Semenenko", "Ivan Butakov", "Alexey Frolov", "Ivan Oseledets"], "title": "Curse of Slicing: Why Sliced Mutual Information is a Deceptive Measure of Statistical Dependence", "categories": ["cs.LG", "cs.IT", "math.IT", "94A16, 68T07, 94A17", "E.4; H.1.1"], "comment": null, "summary": "Sliced Mutual Information (SMI) is widely used as a scalable alternative to\nmutual information for measuring non-linear statistical dependence. Despite its\nadvantages, such as faster convergence, robustness to high dimensionality, and\nnullification only under statistical independence, we demonstrate that SMI is\nhighly susceptible to data manipulation and exhibits counterintuitive behavior.\nThrough extensive benchmarking and theoretical analysis, we show that SMI\nsaturates easily, fails to detect increases in statistical dependence (even\nunder linear transformations designed to enhance the extraction of\ninformation), prioritizes redundancy over informative content, and in some\ncases, performs worse than simpler dependence measures like the correlation\ncoefficient.", "AI": {"tldr": "SMI\u867d\u7136\u662f\u6d4b\u91cf\u975e\u7ebf\u6027\u7edf\u8ba1\u4f9d\u8d56\u6027\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u4f46\u5176\u6613\u53d7\u6570\u636e\u64cd\u7eb5\u3001\u8868\u73b0\u53cd\u76f4\u89c9\uff0c\u4e14\u5728\u90e8\u5206\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u5982\u7b80\u5355\u4f9d\u8d56\u5ea6\u91cf\u3002", "motivation": "\u7814\u7a76SMI\u5728\u5b9e\u8df5\u4e2d\u7684\u5c40\u9650\u6027\u548c\u53cd\u76f4\u89c9\u884c\u4e3a\uff0c\u63a2\u8ba8\u5176\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u7a33\u5065\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u7406\u8bba\u5206\u6790\uff0c\u8bc4\u4f30SMI\u5728\u591a\u79cd\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u3002", "result": "SMI\u6613\u9971\u548c\uff0c\u65e0\u6cd5\u68c0\u6d4b\u7edf\u8ba1\u4f9d\u8d56\u6027\u589e\u5f3a\uff0c\u4f18\u5148\u8003\u8651\u5197\u4f59\u800c\u975e\u4fe1\u606f\u5185\u5bb9\uff0c\u90e8\u5206\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u5982\u76f8\u5173\u7cfb\u6570\u3002", "conclusion": "SMI\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u9700\u8c28\u614e\u4f7f\u7528\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7ef4\u548c\u590d\u6742\u4f9d\u8d56\u573a\u666f\u4e0b\u3002", "keywords": "Sliced Mutual Information, \u7edf\u8ba1\u4f9d\u8d56\u6027, \u6570\u636e\u64cd\u7eb5, \u9ad8\u7ef4\u6570\u636e"}}
{"id": "2506.04020", "pdf": "https://arxiv.org/pdf/2506.04020", "abs": "https://arxiv.org/abs/2506.04020", "authors": ["An Quang Tang", "Xiuzhen Zhang", "Minh Ngoc Dinh", "Zhuang Li"], "title": "QQSUM: A Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering", "categories": ["cs.CL"], "comment": "Paper accepted to ACL 2025 Main Conference", "summary": "Review-based Product Question Answering (PQA) allows e-commerce platforms to\nautomatically address customer queries by leveraging insights from user\nreviews. However, existing PQA systems generate answers with only a single\nperspective, failing to capture the diversity of customer opinions. In this\npaper we introduce a novel task Quantitative Query-Focused Summarization\n(QQSUM), which aims to summarize diverse customer opinions into representative\nKey Points (KPs) and quantify their prevalence to effectively answer user\nqueries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its\ngenerated answers still fall short of capturing the full diversity of\nviewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG,\nemploys few-shot learning to jointly train a KP-oriented retriever and a KP\nsummary generator, enabling KP-based summaries that capture diverse and\nrepresentative opinions. Experimental results demonstrate that QQSUM-RAG\nachieves superior performance compared to state-of-the-art RAG baselines in\nboth textual quality and quantification accuracy of opinions. Our source code\nis available at: https://github.com/antangrocket1312/QQSUMM", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4efb\u52a1QQSUM\uff0c\u901a\u8fc7\u603b\u7ed3\u591a\u6837\u5316\u7684\u7528\u6237\u89c2\u70b9\u5e76\u91cf\u5316\u5176\u666e\u904d\u6027\u6765\u6539\u8fdb\u57fa\u4e8e\u8bc4\u8bba\u7684\u4ea7\u54c1\u95ee\u7b54\u7cfb\u7edf\uff1b\u63d0\u51fa\u4e86QQSUM-RAG\u6a21\u578b\uff0c\u7ed3\u5408\u5c11\u6837\u672c\u5b66\u4e60\u751f\u6210\u591a\u6837\u6027\u89c2\u70b9\u6458\u8981\u3002", "motivation": "\u73b0\u6709\u4ea7\u54c1\u95ee\u7b54\u7cfb\u7edf\u751f\u6210\u7684\u7b54\u6848\u89c6\u89d2\u5355\u4e00\uff0c\u65e0\u6cd5\u53cd\u6620\u7528\u6237\u89c2\u70b9\u7684\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faQQSUM\u4efb\u52a1\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\uff0c\u901a\u8fc7\u5c11\u6837\u672c\u5b66\u4e60\u8054\u5408\u8bad\u7ec3\u5173\u952e\u70b9\uff08KP\uff09\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\u3002", "result": "QQSUM-RAG\u5728\u6587\u672c\u8d28\u91cf\u548c\u89c2\u70b9\u91cf\u5316\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709RAG\u57fa\u7ebf\u3002", "conclusion": "QQSUM-RAG\u80fd\u591f\u6709\u6548\u6355\u6349\u591a\u6837\u6027\u7528\u6237\u89c2\u70b9\uff0c\u63d0\u5347\u95ee\u7b54\u7cfb\u7edf\u6027\u80fd\u3002", "keywords": "\u4ea7\u54c1\u95ee\u7b54\u3001\u7528\u6237\u8bc4\u8bba\u3001\u591a\u6837\u6027\u89c2\u70b9\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u5c11\u6837\u672c\u5b66\u4e60"}}
{"id": "2506.04071", "pdf": "https://arxiv.org/pdf/2506.04071", "abs": "https://arxiv.org/abs/2506.04071", "authors": ["Luiz Manella Pereira", "M. Hadi Amini"], "title": "Optimal Transport-based Domain Alignment as a Preprocessing Step for Federated Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Federated learning (FL) is a subfield of machine learning that avoids sharing\nlocal data with a central server, which can enhance privacy and scalability.\nThe inability to consolidate data leads to a unique problem called dataset\nimbalance, where agents in a network do not have equal representation of the\nlabels one is trying to learn to predict. In FL, fusing locally-trained models\nwith unbalanced datasets may deteriorate the performance of global model\naggregation, and reduce the quality of updated local models and the accuracy of\nthe distributed agents' decisions. In this work, we introduce an Optimal\nTransport-based preprocessing algorithm that aligns the datasets by minimizing\nthe distributional discrepancy of data along the edge devices. We accomplish\nthis by leveraging Wasserstein barycenters when computing channel-wise\naverages. These barycenters are collected in a trusted central server where\nthey collectively generate a target RGB space. By projecting our dataset\ntowards this target space, we minimize the distributional discrepancy on a\nglobal level, which facilitates the learning process due to a minimization of\nvariance across the samples. We demonstrate the capabilities of the proposed\napproach over the CIFAR-10 dataset, where we show its capability of reaching\nhigher degrees of generalization in fewer communication rounds.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u9884\u5904\u7406\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5206\u5e03\u5dee\u5f02\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u672c\u5730\u6570\u636e\u4e0d\u5171\u4eab\u5bfc\u81f4\u6570\u636e\u96c6\u4e0d\u5e73\u8861\uff0c\u5f71\u54cd\u5168\u5c40\u6a21\u578b\u805a\u5408\u6027\u80fd\u3002", "method": "\u5229\u7528Wasserstein\u91cd\u5fc3\u8ba1\u7b97\u901a\u9053\u5e73\u5747\uff0c\u751f\u6210\u76ee\u6807RGB\u7a7a\u95f4\uff0c\u6700\u5c0f\u5316\u5206\u5e03\u5dee\u5f02\u3002", "result": "\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u5728\u8f83\u5c11\u901a\u4fe1\u8f6e\u6b21\u5185\u5b9e\u73b0\u66f4\u9ad8\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u6709\u6548\u51cf\u5c11\u5206\u5e03\u5dee\u5f02\uff0c\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u6548\u7387\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60, \u6700\u4f18\u4f20\u8f93, Wasserstein\u91cd\u5fc3, \u6570\u636e\u96c6\u4e0d\u5e73\u8861"}}
{"id": "2506.04032", "pdf": "https://arxiv.org/pdf/2506.04032", "abs": "https://arxiv.org/abs/2506.04032", "authors": ["Sina Rashidian", "Nan Li", "Jonathan Amar", "Jong Ha Lee", "Sam Pugh", "Eric Yang", "Geoff Masterson", "Myoung Cha", "Yugang Jia", "Akhil Vaid"], "title": "AI Agents for Conversational Patient Triage: Preliminary Simulation-Based Evaluation with Real-World EHR Data", "categories": ["cs.CL"], "comment": null, "summary": "Background: We present a Patient Simulator that leverages real world patient\nencounters which cover a broad range of conditions and symptoms to provide\nsynthetic test subjects for development and testing of healthcare agentic\nmodels. The simulator provides a realistic approach to patient presentation and\nmulti-turn conversation with a symptom-checking agent. Objectives: (1) To\nconstruct and instantiate a Patient Simulator to train and test an AI health\nagent, based on patient vignettes derived from real EHR data. (2) To test the\nvalidity and alignment of the simulated encounters provided by the Patient\nSimulator to expert human clinical providers. (3) To illustrate the evaluation\nframework of such an LLM system on the generated realistic, data-driven\nsimulations -- yielding a preliminary assessment of our proposed system.\nMethods: We first constructed realistic clinical scenarios by deriving patient\nvignettes from real-world EHR encounters. These vignettes cover a variety of\npresenting symptoms and underlying conditions. We then evaluate the performance\nof the Patient Simulator as a simulacrum of a real patient encounter across\nover 500 different patient vignettes. We leveraged a separate AI agent to\nprovide multi-turn questions to obtain a history of present illness. The\nresulting multiturn conversations were evaluated by two expert clinicians.\nResults: Clinicians scored the Patient Simulator as consistent with the patient\nvignettes in those same 97.7% of cases. The extracted case summary based on the\nconversation history was 99% relevant. Conclusions: We developed a methodology\nto incorporate vignettes derived from real healthcare patient data to build a\nsimulation of patient responses to symptom checking agents. The performance and\nalignment of this Patient Simulator could be used to train and test a\nmulti-turn conversational AI agent at scale.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u771f\u5b9e\u75c5\u5386\u6570\u636e\u7684\u60a3\u8005\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u6d4b\u8bd5\u533b\u7597AI\u4ee3\u7406\uff0c\u4e34\u5e8a\u533b\u751f\u8bc4\u4f30\u5176\u8868\u73b0\u9ad8\u5ea6\u4e00\u81f4\u3002", "motivation": "\u5229\u7528\u771f\u5b9e\u60a3\u8005\u6570\u636e\u6784\u5efa\u6a21\u62df\u5668\uff0c\u4ee5\u63d0\u4f9b\u5927\u89c4\u6a21\u7684AI\u533b\u7597\u4ee3\u7406\u8bad\u7ec3\u548c\u6d4b\u8bd5\u573a\u666f\u3002", "method": "\u4ece\u771f\u5b9e\u75c5\u5386\u4e2d\u63d0\u53d6\u60a3\u8005\u6848\u4f8b\uff0c\u6784\u5efa\u6a21\u62df\u5668\u5e76\u8fdb\u884c500\u591a\u4f8b\u6d4b\u8bd5\uff0c\u7531AI\u4ee3\u7406\u8fdb\u884c\u591a\u8f6e\u5bf9\u8bdd\uff0c\u6700\u7ec8\u7531\u4e13\u5bb6\u8bc4\u4f30\u3002", "result": "\u6a21\u62df\u5668\u4e0e\u6848\u4f8b\u7684\u4e00\u81f4\u6027\u8fbe97.7%\uff0c\u5bf9\u8bdd\u603b\u7ed3\u7684\u76f8\u5173\u6027\u4e3a99%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u9ad8\u6548\u6784\u5efa\u60a3\u8005\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u533b\u7597AI\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u3002", "keywords": "\u60a3\u8005\u6a21\u62df\u5668,AI\u533b\u7597\u4ee3\u7406,\u75c5\u5386\u6570\u636e,\u591a\u8f6e\u5bf9\u8bdd,\u8bc4\u4f30\u6846\u67b6"}}
{"id": "2506.03511", "pdf": "https://arxiv.org/pdf/2506.03511", "abs": "https://arxiv.org/abs/2506.03511", "authors": ["Fangyi Cao", "Bin Ren", "Zihao Wang", "Shiwei Fu", "Youbin Mo", "Xiaoyang Liu", "Yuzhou Chen", "Weixin Yao"], "title": "POLARIS: A High-contrast Polarimetric Imaging Benchmark Dataset for Exoplanetary Disk Representation Learning", "categories": ["astro-ph.EP", "astro-ph.IM", "cs.AI", "eess.IV"], "comment": "9 pages main text with 5 figures, 9 pages appendix with 9 figures.\n  Submitted to NeurIPS 2025", "summary": "With over 1,000,000 images from more than 10,000 exposures using\nstate-of-the-art high-contrast imagers (e.g., Gemini Planet Imager, VLT/SPHERE)\nin the search for exoplanets, can artificial intelligence (AI) serve as a\ntransformative tool in imaging Earth-like exoplanets in the coming decade? In\nthis paper, we introduce a benchmark and explore this question from a\npolarimetric image representation learning perspective. Despite extensive\ninvestments over the past decade, only a few new exoplanets have been directly\nimaged. Existing imaging approaches rely heavily on labor-intensive labeling of\nreference stars, which serve as background to extract circumstellar objects\n(disks or exoplanets) around target stars. With our POLARIS (POlarized Light\ndAta for total intensity Representation learning of direct Imaging of\nexoplanetary Systems) dataset, we classify reference star and circumstellar\ndisk images using the full public SPHERE/IRDIS polarized-light archive since\n2014, requiring less than 10 percent manual labeling. We evaluate a range of\nmodels including statistical, generative, and large vision-language models and\nprovide baseline performance. We also propose an unsupervised generative\nrepresentation learning framework that integrates these models, achieving\nsuperior performance and enhanced representational power. To our knowledge,\nthis is the first uniformly reduced, high-quality exoplanet imaging dataset,\nrare in astrophysics and machine learning. By releasing this dataset and\nbaselines, we aim to equip astrophysicists with new tools and engage data\nscientists in advancing direct exoplanet imaging, catalyzing major\ninterdisciplinary breakthroughs.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86AI\u5728\u76f4\u63a5\u6210\u50cf\u5730\u7403\u7c7b\u4f3c\u7cfb\u5916\u884c\u661f\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u504f\u632f\u56fe\u50cf\u8868\u793a\u5b66\u4e60\u7684\u57fa\u51c6\u6570\u636e\u96c6POLARIS\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u65e0\u76d1\u7763\u751f\u6210\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u7cfb\u5916\u884c\u661f\u76f4\u63a5\u6210\u50cf\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u80cc\u666f\u53c2\u8003\u661f\uff0c\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7AI\u6280\u672f\u6539\u8fdb\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u63a8\u52a8\u8de8\u5b66\u79d1\u7814\u7a76\u3002", "method": "\u4f7f\u7528POLARIS\u6570\u636e\u96c6\uff08\u57fa\u4e8eSPHERE/IRDIS\u504f\u632f\u5149\u6570\u636e\uff09\uff0c\u8bc4\u4f30\u7edf\u8ba1\u3001\u751f\u6210\u53ca\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65e0\u76d1\u7763\u751f\u6210\u8868\u793a\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u6027\u80fd\u4f18\u8d8a\uff0c\u4ec5\u970010%\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u5206\u7c7b\uff0c\u663e\u8457\u63d0\u5347\u8868\u793a\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u672c\u6587\u4e3a\u5929\u4f53\u7269\u7406\u5b66\u5bb6\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u4fc3\u8fdb\u6570\u636e\u79d1\u5b66\u5bb6\u53c2\u4e0e\u7cfb\u5916\u884c\u661f\u76f4\u63a5\u6210\u50cf\u7814\u7a76\u3002", "keywords": "AI\uff0c\u7cfb\u5916\u884c\u661f\uff0c\u76f4\u63a5\u6210\u50cf\uff0c\u504f\u632f\u5149\uff0c\u8868\u793a\u5b66\u4e60"}}
{"id": "2506.04088", "pdf": "https://arxiv.org/pdf/2506.04088", "abs": "https://arxiv.org/abs/2506.04088", "authors": ["Jun-Peng Jiang", "Yu Xia", "Hai-Long Sun", "Shiyin Lu", "Qing-Guo Chen", "Weihua Luo", "Kaifu Zhang", "De-Chuan Zhan", "Han-Jia Ye"], "title": "Multimodal Tabular Reasoning with Privileged Structured Information", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Tabular reasoning involves multi-step information extraction and logical\ninference over tabular data. While recent advances have leveraged large\nlanguage models (LLMs) for reasoning over structured tables, such high-quality\ntextual representations are often unavailable in real-world settings, where\ntables typically appear as images. In this paper, we tackle the task of tabular\nreasoning from table images, leveraging privileged structured information\navailable during training to enhance multimodal large language models (MLLMs).\nThe key challenges lie in the complexity of accurately aligning structured\ninformation with visual representations, and in effectively transferring\nstructured reasoning skills to MLLMs despite the input modality gap. To address\nthese, we introduce TabUlar Reasoning with Bridged infOrmation ({\\sc Turbo}), a\nnew framework for multimodal tabular reasoning with privileged structured\ntables. {\\sc Turbo} benefits from a structure-aware reasoning trace generator\nbased on DeepSeek-R1, contributing to high-quality modality-bridged data. On\nthis basis, {\\sc Turbo} repeatedly generates and selects the advantageous\nreasoning paths, further enhancing the model's tabular reasoning ability.\nExperimental results demonstrate that, with limited ($9$k) data, {\\sc Turbo}\nachieves state-of-the-art performance ($+7.2\\%$ vs. previous SOTA) across\nmultiple datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTurbo\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u8bad\u7ec3\u65f6\u7684\u7ed3\u6784\u5316\u4fe1\u606f\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u8868\u683c\u56fe\u50cf\u4e2d\u7684\u63a8\u7406\u4efb\u52a1\u3002", "motivation": "\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u683c\u5e38\u4ee5\u56fe\u50cf\u5f62\u5f0f\u51fa\u73b0\uff0c\u800c\u9ad8\u8d28\u91cf\u6587\u672c\u8868\u793a\u4e0d\u53ef\u7528\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u8868\u683c\u56fe\u50cf\u4e2d\u7684\u591a\u6b65\u63a8\u7406\u95ee\u9898\u3002", "method": "\u5f15\u5165Turbo\u6846\u67b6\uff0c\u5229\u7528\u7ed3\u6784\u5316\u4fe1\u606f\u751f\u6210\u9ad8\u8d28\u91cf\u6a21\u6001\u6865\u63a5\u6570\u636e\uff0c\u5e76\u53cd\u590d\u751f\u6210\u548c\u9009\u62e9\u6700\u4f73\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728\u6709\u9650\u6570\u636e\uff089k\uff09\u4e0b\uff0cTurbo\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\uff08\u63d0\u53477.2%\uff09\u3002", "conclusion": "Turbo\u901a\u8fc7\u6a21\u6001\u6865\u63a5\u6570\u636e\u548c\u63a8\u7406\u8def\u5f84\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8868\u683c\u63a8\u7406\u80fd\u529b\u3002", "keywords": "\u8868\u683c\u63a8\u7406\u3001\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u6a21\u6001\u6865\u63a5"}}
{"id": "2506.04037", "pdf": "https://arxiv.org/pdf/2506.04037", "abs": "https://arxiv.org/abs/2506.04037", "authors": ["Dan Oneata", "Leanne Nortje", "Yevgen Matusevych", "Herman Kamper"], "title": "The mutual exclusivity bias of bilingual visually grounded speech models", "categories": ["cs.CL", "eess.AS"], "comment": "Interspeech 2025", "summary": "Mutual exclusivity (ME) is a strategy where a novel word is associated with a\nnovel object rather than a familiar one, facilitating language learning in\nchildren. Recent work has found an ME bias in a visually grounded speech (VGS)\nmodel trained on English speech with paired images. But ME has also been\nstudied in bilingual children, who may employ it less due to cross-lingual\nambiguity. We explore this pattern computationally using bilingual VGS models\ntrained on combinations of English, French, and Dutch. We find that bilingual\nmodels generally exhibit a weaker ME bias than monolingual models, though\nexceptions exist. Analyses show that the combined visual embeddings of\nbilingual models have a smaller variance for familiar data, partly explaining\nthe increase in confusion between novel and familiar concepts. We also provide\nnew insights into why the ME bias exists in VGS models in the first place. Code\nand data: https://github.com/danoneata/me-vgs", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u53cc\u8bed\u89c6\u89c9\u57fa\u7840\u8bed\u97f3\uff08VGS\uff09\u6a21\u578b\u4e2d\u7684\u4e92\u65a5\u6027\uff08ME\uff09\u504f\u7f6e\uff0c\u53d1\u73b0\u53cc\u8bed\u6a21\u578b\u7684ME\u504f\u7f6e\u901a\u5e38\u5f31\u4e8e\u5355\u8bed\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8eME\u504f\u7f6e\u5b58\u5728\u539f\u56e0\u7684\u89c1\u89e3\u3002", "motivation": "\u7814\u7a76\u53cc\u8bed\u513f\u7ae5\u5728\u8bed\u8a00\u5b66\u4e60\u4e2d\u4f7f\u7528\u4e92\u65a5\u6027\u7b56\u7565\u7684\u9891\u7387\u8f83\u4f4e\u7684\u73b0\u8c61\uff0c\u5e76\u901a\u8fc7\u53cc\u8bedVGS\u6a21\u578b\u63a2\u7d22\u5176\u8ba1\u7b97\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528\u82f1\u8bed\u3001\u6cd5\u8bed\u548c\u8377\u5170\u8bed\u7684\u53cc\u8bedVGS\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u5206\u6790\u5176ME\u504f\u7f6e\u8868\u73b0\u3002", "result": "\u53cc\u8bed\u6a21\u578b\u7684ME\u504f\u7f6e\u901a\u5e38\u8f83\u5f31\uff0c\u90e8\u5206\u539f\u56e0\u662f\u5176\u89c6\u89c9\u5d4c\u5165\u5728\u719f\u6089\u6570\u636e\u4e0a\u7684\u65b9\u5dee\u8f83\u5c0f\uff0c\u5bfc\u81f4\u65b0\u6982\u5ff5\u548c\u719f\u6089\u6982\u5ff5\u7684\u6df7\u6dc6\u589e\u52a0\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u53cc\u8bed\u6a21\u578bME\u504f\u7f6e\u51cf\u5f31\u7684\u539f\u56e0\uff0c\u5e76\u4e3aVGS\u6a21\u578b\u4e2dME\u504f\u7f6e\u7684\u5b58\u5728\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u91ca\u3002", "keywords": "\u4e92\u65a5\u6027\uff08ME\uff09\uff0c\u89c6\u89c9\u57fa\u7840\u8bed\u97f3\uff08VGS\uff09\uff0c\u53cc\u8bed\u6a21\u578b\uff0c\u8bed\u8a00\u5b66\u4e60"}}
{"id": "2506.03516", "pdf": "https://arxiv.org/pdf/2506.03516", "abs": "https://arxiv.org/abs/2506.03516", "authors": ["Arnab Debnath", "Gregory J. Stein", "Jana Kosecka"], "title": "SemNav: A Model-Based Planner for Zero-Shot Object Goal Navigation Using Vision-Foundation Models", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted at CVPR 2025 workshop - Foundation Models Meet Embodied\n  Agents", "summary": "Object goal navigation is a fundamental task in embodied AI, where an agent\nis instructed to locate a target object in an unexplored environment.\nTraditional learning-based methods rely heavily on large-scale annotated data\nor require extensive interaction with the environment in a reinforcement\nlearning setting, often failing to generalize to novel environments and\nlimiting scalability. To overcome these challenges, we explore a zero-shot\nsetting where the agent operates without task-specific training, enabling more\nscalable and adaptable solution. Recent advances in Vision Foundation Models\n(VFMs) offer powerful capabilities for visual understanding and reasoning,\nmaking them ideal for agents to comprehend scenes, identify relevant regions,\nand infer the likely locations of objects. In this work, we present a zero-shot\nobject goal navigation framework that integrates the perceptual strength of\nVFMs with a model-based planner that is capable of long-horizon decision making\nthrough frontier exploration. We evaluate our approach on the HM3D dataset\nusing the Habitat simulator and demonstrate that our method achieves\nstate-of-the-art performance in terms of success weighted by path length for\nzero-shot object goal navigation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u76ee\u6807\u5bfc\u822a\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u5668\uff0c\u5728\u672a\u8bad\u7ec3\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u6216\u5f3a\u5316\u5b66\u4e60\u4ea4\u4e92\uff0c\u96be\u4ee5\u6cdb\u5316\u548c\u6269\u5c55\uff0c\u96f6\u6837\u672c\u65b9\u6cd5\u66f4\u5177\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u96c6\u6210\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u7684\u611f\u77e5\u80fd\u529b\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u8fb9\u754c\u63a2\u7d22\u8fdb\u884c\u957f\u65f6\u51b3\u7b56\u3002", "result": "\u5728HM3D\u6570\u636e\u96c6\u548cHabitat\u6a21\u62df\u5668\u4e0a\u6d4b\u8bd5\uff0c\u53d6\u5f97\u4e86\u96f6\u6837\u672c\u76ee\u6807\u5bfc\u822a\u4e2d\u6210\u529f\u7387\u52a0\u6743\u8def\u5f84\u957f\u5ea6\u7684\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408VFMs\u548c\u6a21\u578b\u89c4\u5212\u5668\u7684\u6846\u67b6\u5728\u96f6\u6837\u672c\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\u3002", "keywords": "\u76ee\u6807\u5bfc\u822a\uff0c\u96f6\u6837\u672c\u5b66\u4e60\uff0c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u6a21\u578b\u89c4\u5212\u5668\uff0cHabitat\u6a21\u62df\u5668"}}
{"id": "2506.04089", "pdf": "https://arxiv.org/pdf/2506.04089", "abs": "https://arxiv.org/abs/2506.04089", "authors": ["Anastasiia Ivanova", "Eva Bakaeva", "Zoya Volovikova", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "comment": "ACL 2025 (Main Conference)", "summary": "As a part of an embodied agent, Large Language Models (LLMs) are typically\nused for behavior planning given natural language instructions from the user.\nHowever, dealing with ambiguous instructions in real-world environments remains\na challenge for LLMs. Various methods for task ambiguity detection have been\nproposed. However, it is difficult to compare them because they are tested on\ndifferent datasets and there is no universal benchmark. For this reason, we\npropose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual\ndataset of ambiguous instructions addressed to a robot in a kitchen\nenvironment. AmbiK was collected with the assistance of LLMs and is\nhuman-validated. It comprises 1000 pairs of ambiguous tasks and their\nunambiguous counterparts, categorized by ambiguity type (Human Preferences,\nCommon Sense Knowledge, Safety), with environment descriptions, clarifying\nquestions and answers, user intents, and task plans, for a total of 2000 tasks.\nWe hope that AmbiK will enable researchers to perform a unified comparison of\nambiguity detection methods. AmbiK is available at\nhttps://github.com/cog-model/AmbiK-dataset.", "AI": {"tldr": "\u63d0\u51faAmbiK\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7edf\u4e00\u6bd4\u8f83\u53a8\u623f\u73af\u5883\u4e2d\u673a\u5668\u4eba\u9762\u5bf9\u6a21\u7cca\u6307\u4ee4\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0d\u540c\u800c\u96be\u4ee5\u6bd4\u8f83\u6a21\u7cca\u6307\u4ee4\u68c0\u6d4b\u6548\u679c\uff0c\u9700\u901a\u7528\u57fa\u51c6\u3002", "method": "\u5229\u7528LLMs\u8f85\u52a9\u6536\u96c6\u5e76\u4eba\u5de5\u9a8c\u8bc11000\u5bf9\u6a21\u7cca\u4e0e\u660e\u786e\u6307\u4ee4\uff0c\u5f62\u6210AmbiK\u6570\u636e\u96c6\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b2000\u9879\u4efb\u52a1\uff0c\u6db5\u76d6\u4e09\u7c7b\u6a21\u7cca\u7c7b\u578b\uff0c\u63d0\u4f9b\u73af\u5883\u63cf\u8ff0\u3001\u6f84\u6e05\u95ee\u7b54\u7b49\u3002", "conclusion": "AmbiK\u4e3a\u6a21\u7cca\u68c0\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u7edf\u4e00\u6bd4\u8f83\u57fa\u51c6\uff0c\u4fc3\u8fdb\u7814\u7a76\u53d1\u5c55\u3002", "keywords": "\u6a21\u7cca\u6307\u4ee4, LLMs, \u53a8\u623f\u73af\u5883, \u6570\u636e\u96c6, AmbiK"}}
{"id": "2506.04041", "pdf": "https://arxiv.org/pdf/2506.04041", "abs": "https://arxiv.org/abs/2506.04041", "authors": ["Claire Barale", "Leslie Barrett", "Vikram Sunil Bajaj", "Michael Rovatsos"], "title": "LexTime: A Benchmark for Temporal Ordering of Legal Events", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Temporal reasoning in legal texts is important for applications like case law\nanalysis and compliance monitoring. However, existing datasets lack expert\nlanguage evaluation, leaving a gap in understanding how LLMs manage event\nordering in legal contexts. We introduce LexTime, the first dataset designed to\nevaluate LLMs' event ordering capabilities in legal language, consisting of 512\ninstances from U.S. Federal Complaints with annotated event pairs and their\ntemporal relations. Our findings show that (1) LLMs are more accurate on legal\nevent ordering than on narrative (up to +10.5%); (2) longer input contexts and\nimplicit events boost accuracy, reaching 80.8% for implicit-explicit event\npairs; (3) legal linguistic complexities and nested clauses remain a challenge.\nWe investigate how context length, explicit vs implicit event pairs, and legal\nlanguage features affect model performance, demonstrating the need for specific\nmodeling strategies to enhance temporal event reasoning.", "AI": {"tldr": "LexTime\u662f\u9996\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6cd5\u5f8b\u6587\u672c\u4e2d\u4e8b\u4ef6\u6392\u5e8f\u80fd\u529b\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b512\u4e2a\u6765\u81ea\u7f8e\u56fd\u8054\u90a6\u6295\u8bc9\u7684\u6807\u6ce8\u5b9e\u4f8b\u3002\u7814\u7a76\u8868\u660e\uff0cLLM\u5728\u6cd5\u5f8b\u4e8b\u4ef6\u6392\u5e8f\u4e0a\u6bd4\u53d9\u4e8b\u66f4\u51c6\u786e\uff0c\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u9690\u542b\u4e8b\u4ef6\u5bf9\u6027\u80fd\u6709\u79ef\u6781\u5f71\u54cd\uff0c\u4f46\u6cd5\u5f8b\u8bed\u8a00\u7684\u590d\u6742\u6027\u4ecd\u5177\u6311\u6218\u6027\u3002", "motivation": "\u586b\u8865\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u7f3a\u4e4f\u4e13\u5bb6\u8bed\u8a00\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u7814\u7a76LLM\u5728\u6cd5\u5f8b\u6587\u672c\u4e2d\u7684\u4e8b\u4ef6\u6392\u5e8f\u80fd\u529b\u3002", "method": "\u6784\u5efaLexTime\u6570\u636e\u96c6\uff0c\u5305\u542b512\u4e2a\u6807\u6ce8\u5b9e\u4f8b\uff0c\u5206\u6790LLM\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\u3001\u663e\u5f0f/\u9690\u5f0f\u4e8b\u4ef6\u5bf9\u4ee5\u53ca\u6cd5\u5f8b\u8bed\u8a00\u7279\u5f81\u4e0b\u7684\u8868\u73b0\u3002", "result": "LLM\u5728\u6cd5\u5f8b\u4e8b\u4ef6\u6392\u5e8f\u4e0a\u6bd4\u53d9\u4e8b\u66f4\u51c6\u786e\uff08\u63d0\u534710.5%\uff09\uff0c\u957f\u4e0a\u4e0b\u6587\u548c\u9690\u542b\u4e8b\u4ef6\u5bf9\u80fd\u63d0\u5347\u6027\u80fd\u81f380.8%\uff0c\u4f46\u6cd5\u5f8b\u8bed\u8a00\u7684\u590d\u6742\u6027\u4ecd\u5f71\u54cd\u8868\u73b0\u3002", "conclusion": "\u9700\u8981\u9488\u5bf9\u6027\u7684\u5efa\u6a21\u7b56\u7565\u6765\u63d0\u5347\u6cd5\u5f8b\u6587\u672c\u4e2d\u7684\u65f6\u95f4\u4e8b\u4ef6\u63a8\u7406\u80fd\u529b\u3002", "keywords": "\u65f6\u95f4\u63a8\u7406, \u6cd5\u5f8b\u6587\u672c, \u8bed\u8a00\u6a21\u578b, \u4e8b\u4ef6\u6392\u5e8f, \u6570\u636e\u96c6"}}
{"id": "2506.03525", "pdf": "https://arxiv.org/pdf/2506.03525", "abs": "https://arxiv.org/abs/2506.03525", "authors": ["Daeun Lee", "Jaehong Yoon", "Jaemin Cho", "Mohit Bansal"], "title": "Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project website: https://video-skill-cot.github.io/", "summary": "Recent advances in Chain-of-Thought (CoT) reasoning have improved complex\nvideo understanding, but existing methods often struggle to adapt to\ndomain-specific skills (e.g., event detection, spatial relation understanding,\nemotion understanding) over various video content. To address this, we propose\nVideo-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs\nand leverages skill-aware CoT supervisions for domain-adaptive video reasoning.\nFirst, we construct skill-based CoT annotations: we extract domain-relevant\nreasoning skills from training questions, cluster them into a shared skill\ntaxonomy, and create detailed multi-step CoT rationale tailored to each\nvideo-question pair for training. Second, we introduce a skill-specific expert\nlearning framework. Each expert module specializes in a subset of reasoning\nskills and is trained with lightweight adapters using the collected CoT\nsupervision. We demonstrate the effectiveness of the proposed approach on three\nvideo understanding benchmarks, where Video-SKoT consistently outperforms\nstrong baselines. We also provide in-depth analyses on comparing different CoT\nannotation pipelines and learned skills over multiple video domains.", "AI": {"tldr": "Video-SKoT\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u6784\u5efa\u57fa\u4e8e\u6280\u80fd\u7684CoT\u76d1\u7763\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u7684\u9002\u5e94\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709CoT\u63a8\u7406\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u89c6\u9891\u5185\u5bb9\u4e2d\u7684\u9886\u57df\u7279\u5b9a\u6280\u80fd\uff08\u5982\u4e8b\u4ef6\u68c0\u6d4b\u3001\u7a7a\u95f4\u5173\u7cfb\u7406\u89e3\u7b49\uff09\uff0c\u9700\u6539\u8fdb\u4ee5\u5b9e\u73b0\u9886\u57df\u81ea\u9002\u5e94\u89c6\u9891\u63a8\u7406\u3002", "method": "1) \u6784\u5efa\u57fa\u4e8e\u6280\u80fd\u7684CoT\u6807\u6ce8\uff1a\u63d0\u53d6\u8bad\u7ec3\u95ee\u9898\u4e2d\u7684\u9886\u57df\u76f8\u5173\u63a8\u7406\u6280\u80fd\uff0c\u805a\u7c7b\u4e3a\u5171\u4eab\u6280\u80fd\u5206\u7c7b\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u89c6\u9891-\u95ee\u9898\u5bf9\u521b\u5efa\u591a\u6b65\u63a8\u7406CoT\uff1b2) \u63d0\u51fa\u6280\u80fd\u7279\u5b9a\u7684\u4e13\u5bb6\u5b66\u4e60\u6846\u67b6\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u6a21\u5757\u4e13\u6ce8\u4e8e\u90e8\u5206\u63a8\u7406\u6280\u80fd\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2a\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVideo-SKoT\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e0d\u540cCoT\u6807\u6ce8\u6d41\u7a0b\u548c\u5b66\u4e60\u6280\u80fd\u7684\u6df1\u5165\u5206\u6790\u3002", "conclusion": "Video-SKoT\u901a\u8fc7\u6280\u80fd\u611f\u77e5\u7684CoT\u76d1\u7763\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u89c6\u9891\u7406\u89e3\u7684\u9002\u5e94\u6027\uff0c\u5c55\u793a\u4e86\u5728\u591a\u6837\u5316\u89c6\u9891\u9886\u57df\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "Chain-of-Thought reasoning, video understanding, domain adaptation, skill taxonomy, expert learning"}}
{"id": "2506.04118", "pdf": "https://arxiv.org/pdf/2506.04118", "abs": "https://arxiv.org/abs/2506.04118", "authors": ["Jonathan Geuter", "Youssef Mroueh", "David Alvarez-Melis"], "title": "Guided Speculative Inference for Efficient Test-Time Alignment of LLMs", "categories": ["cs.LG", "stat.ML", "I.2.7"], "comment": "12 pages, 2 figures", "summary": "We propose Guided Speculative Inference (GSI), a novel algorithm for\nefficient reward-guided decoding in large language models. GSI combines soft\nbest-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative\nsamples from a small auxiliary model $\\pi_S(y\\mid x)$. We provably approximate\nthe optimal tilted policy $\\pi_{\\beta,B}(y\\mid x) \\propto \\pi_B(y\\mid\nx)\\exp(\\beta\\,r(x,y))$ of soft best-of-$n$ under the primary model $\\pi_B$. We\nderive a theoretical bound on the KL divergence between our induced\ndistribution and the optimal policy. In experiments on reasoning benchmarks\n(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy\nthan standard soft best-of-$n$ with $\\pi_S$ and reward-guided speculative\ndecoding (Liao et al., 2025), and in certain settings even outperforms soft\nbest-of-$n$ with $\\pi_B$. The code is available at\nhttps://github.com/j-geuter/GSI .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGSI\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8f6f\u6700\u4f73-$n$\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u3001\u5956\u52b1\u6a21\u578b\u548c\u5c0f\u8f85\u52a9\u6a21\u578b\u7684\u63a8\u6d4b\u6837\u672c\uff0c\u9ad8\u6548\u6307\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u7801\u3002", "motivation": "\u65e8\u5728\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u89e3\u7801\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408\u8f6f\u6700\u4f73-$n$\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u3001\u5956\u52b1\u6a21\u578b$r(x,y)$\u548c\u5c0f\u8f85\u52a9\u6a21\u578b$\u03c0_S(y\\mid x)$\u7684\u63a8\u6d4b\u6837\u672c\uff0c\u8fd1\u4f3c\u6700\u4f18\u503e\u659c\u7b56\u7565$\u03c0_{\u03b2,B}(y\\mid x)$\u3002", "result": "\u5728MATH500\u3001OlympiadBench\u548cMinerva Math\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGSI\u7684\u51c6\u786e\u6027\u4f18\u4e8e\u6807\u51c6\u8f6f\u6700\u4f73-$n$\u548c\u5956\u52b1\u5f15\u5bfc\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u57fa\u4e8e\u4e3b\u6a21\u578b\u7684\u8f6f\u6700\u4f73-$n$\u3002", "conclusion": "GSI\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u4efb\u52a1\u3002", "keywords": "Guided Speculative Inference, \u8f6f\u6700\u4f73-$n$, \u5956\u52b1\u6a21\u578b, \u89e3\u7801\u4f18\u5316, \u5927\u578b\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.04042", "pdf": "https://arxiv.org/pdf/2506.04042", "abs": "https://arxiv.org/abs/2506.04042", "authors": ["Xiyu Liu", "Zhengxiao Liu", "Naibin Gu", "Zheng Lin", "Ji Xiang", "Weiping Wang"], "title": "Unveiling and Eliminating the Shortcut Learning for Locate-Then-Edit Knowledge Editing via Both Subject and Relation Awareness", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge editing aims to alternate the target knowledge predicted by large\nlanguage models while ensuring the least side effects on unrelated knowledge.\nAn effective way to achieve knowledge editing is to identify pivotal parameters\nfor predicting factual associations and modify them with an optimization\nprocess to update the predictions. However, these locate-then-edit methods are\nuncontrollable since they tend to modify most unrelated relations connected to\nthe subject of target editing. We unveil that this failure of controllable\nediting is due to a shortcut learning issue during the optimization process.\nSpecifically, we discover two crucial features that are the subject feature and\nthe relation feature for models to learn during optimization, but the current\noptimization process tends to over-learning the subject feature while\nneglecting the relation feature. To eliminate this shortcut learning of the\nsubject feature, we propose a novel two-stage optimization process that\nbalances the learning of the subject feature and the relation feature.\nExperimental results demonstrate that our approach successfully prevents\nknowledge editing from shortcut learning and achieves the optimal overall\nperformance, contributing to controllable knowledge editing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u77e5\u8bc6\u7f16\u8f91\u4e2d\u56e0\u4e3b\u4f53\u7279\u5f81\u8fc7\u5b66\u4e60\u800c\u5bfc\u81f4\u7684\u4e0d\u53ef\u63a7\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u63a7\u7684\u77e5\u8bc6\u7f16\u8f91\u3002", "motivation": "\u5f53\u524d\u7684\u5b9a\u4f4d-\u7f16\u8f91\u65b9\u6cd5\u5728\u4fee\u6539\u76ee\u6807\u77e5\u8bc6\u65f6\u5bb9\u6613\u5f71\u54cd\u65e0\u5173\u77e5\u8bc6\uff0c\u5176\u6839\u672c\u539f\u56e0\u662f\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u4e3b\u4f53\u7279\u5f81\u8fc7\u5b66\u4e60\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e24\u9636\u6bb5\u4f18\u5316\u8fc7\u7a0b\uff0c\u5e73\u8861\u4e3b\u4f53\u7279\u5f81\u548c\u5173\u7cfb\u7279\u5f81\u7684\u5b66\u4e60\uff0c\u907f\u514d\u4e3b\u4f53\u7279\u5f81\u7684\u8fc7\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u907f\u514d\u4e86\u77e5\u8bc6\u7f16\u8f91\u7684\u6377\u5f84\u5b66\u4e60\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u77e5\u8bc6\u7f16\u8f91\u7684\u53ef\u63a7\u6027\u3002", "keywords": "\u77e5\u8bc6\u7f16\u8f91, \u4e3b\u4f53\u7279\u5f81, \u5173\u7cfb\u7279\u5f81, \u4e24\u9636\u6bb5\u4f18\u5316, \u6377\u5f84\u5b66\u4e60"}}
{"id": "2506.04126", "pdf": "https://arxiv.org/pdf/2506.04126", "abs": "https://arxiv.org/abs/2506.04126", "authors": ["Yujun Kim", "Jaeyoung Cha", "Chulhee Yun"], "title": "Incremental Gradient Descent with Small Epoch Counts is Surprisingly Slow on Ill-Conditioned Problems", "categories": ["cs.LG", "math.OC"], "comment": "Accepted to ICML 2025, 56 pages, 6 figures", "summary": "Recent theoretical results demonstrate that the convergence rates of\npermutation-based SGD (e.g., random reshuffling SGD) are faster than\nuniform-sampling SGD; however, these studies focus mainly on the large epoch\nregime, where the number of epochs $K$ exceeds the condition number $\\kappa$.\nIn contrast, little is known when $K$ is smaller than $\\kappa$, and it is still\na challenging open question whether permutation-based SGD can converge faster\nin this small epoch regime (Safran and Shamir, 2021). As a step toward\nunderstanding this gap, we study the naive deterministic variant, Incremental\nGradient Descent (IGD), on smooth and strongly convex functions. Our lower\nbounds reveal that for the small epoch regime, IGD can exhibit surprisingly\nslow convergence even when all component functions are strongly convex.\nFurthermore, when some component functions are allowed to be nonconvex, we\nprove that the optimality gap of IGD can be significantly worse throughout the\nsmall epoch regime. Our analyses reveal that the convergence properties of\npermutation-based SGD in the small epoch regime may vary drastically depending\non the assumptions on component functions. Lastly, we supplement the paper with\ntight upper and lower bounds for IGD in the large epoch regime.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5c0f\u65f6\u671f\u5236\u5ea6\u4e0b\u589e\u91cf\u68af\u5ea6\u4e0b\u964d\uff08IGD\uff09\u7684\u6536\u655b\u6027\uff0c\u53d1\u73b0\u5176\u53ef\u80fd\u8868\u73b0\u8f83\u6162\uff0c\u4e14\u5728\u67d0\u4e9b\u975e\u51f8\u60c5\u51b5\u4e0b\u66f4\u5dee\u3002", "motivation": "\u586b\u8865\u5173\u4e8e\u5c0f\u65f6\u671f\u5236\u5ea6\u4e0b\u6392\u5217SGD\u6536\u655b\u6027\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u7279\u522b\u662f\u5f53\u65f6\u671f\u6570K\u5c0f\u4e8e\u6761\u4ef6\u6570\u03ba\u65f6\u3002", "method": "\u5206\u6790\u589e\u91cf\u68af\u5ea6\u4e0b\u964d\uff08IGD\uff09\u5728\u5149\u6ed1\u548c\u5f3a\u51f8\u51fd\u6570\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u6269\u5c55\u5230\u90e8\u5206\u975e\u51f8\u60c5\u51b5\u3002", "result": "\u53d1\u73b0\u5c0f\u65f6\u671f\u5236\u5ea6\u4e0bIGD\u6536\u655b\u53ef\u80fd\u8f83\u6162\uff0c\u4e14\u975e\u51f8\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u5dee\u3002", "conclusion": "\u6392\u5217SGD\u5728\u5c0f\u65f6\u671f\u5236\u5ea6\u4e0b\u7684\u6536\u655b\u6027\u56e0\u5047\u8bbe\u4e0d\u540c\u800c\u53d8\u5316\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "keywords": "SGD, IGD, \u5c0f\u65f6\u671f\u5236\u5ea6, \u5f3a\u51f8\u51fd\u6570, \u975e\u51f8\u51fd\u6570"}}
{"id": "2506.04043", "pdf": "https://arxiv.org/pdf/2506.04043", "abs": "https://arxiv.org/abs/2506.04043", "authors": ["Mikel K. Ngueajio", "Flor Miriam Plaza-del-Arco", "Yi-Ling Chung", "Danda B. Rawat", "Amanda Cercas Curry"], "title": "Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted at ACL WOAH 2025", "summary": "Automated counter-narratives (CN) offer a promising strategy for mitigating\nonline hate speech, yet concerns about their affective tone, accessibility, and\nethical risks remain. We propose a framework for evaluating Large Language\nModel (LLM)-generated CNs across four dimensions: persona framing, verbosity\nand readability, affective tone, and ethical robustness. Using GPT-4o-Mini,\nCohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting\nstrategies on the MT-Conan and HatEval datasets. Our findings reveal that\nLLM-generated CNs are often verbose and adapted for people with college-level\nliteracy, limiting their accessibility. While emotionally guided prompts yield\nmore empathetic and readable responses, there remain concerns surrounding\nsafety and effectiveness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30LLM\u751f\u6210\u53cd\u4ec7\u6068\u8a00\u8bba\uff08CN\uff09\u7684\u6846\u67b6\uff0c\u6d89\u53ca\u56db\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u53d1\u73b0CN\u5e38\u5197\u957f\u4e14\u9002\u5e94\u9ad8\u7b49\u6559\u80b2\u6c34\u5e73\u4eba\u7fa4\uff0c\u540c\u65f6\u60c5\u611f\u5f15\u5bfc\u63d0\u793a\u867d\u63d0\u5347\u53ef\u8bfb\u6027\uff0c\u4f46\u5b89\u5168\u548c\u6709\u6548\u6027\u5b58\u7591\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u81ea\u52a8\u5316\u53cd\u4ec7\u6068\u8a00\u8bba\u5728\u60c5\u611f\u8bed\u8c03\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u4f26\u7406\u98ce\u9669\u65b9\u9762\u7684\u6311\u6218\uff0c\u4ee5\u4f18\u5316\u5176\u6548\u679c\u3002", "method": "\u4f7f\u7528GPT-4o-Mini\u7b49\u4e09\u79cdLLM\u53caMT-Conan\u548cHatEval\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u56db\u79cd\u7ef4\u5ea6\uff08\u5982\u4eba\u683c\u6846\u67b6\u3001\u53ef\u8bfb\u6027\uff09\u548c\u4e09\u79cd\u63d0\u793a\u7b56\u7565\u3002", "result": "LLM\u751f\u6210\u7684CN\u5f80\u5f80\u5197\u957f\u4e14\u9762\u5411\u9ad8\u5b66\u5386\u4eba\u7fa4\uff0c\u60c5\u611f\u63d0\u793a\u867d\u63d0\u5347\u5171\u60c5\u548c\u53ef\u8bfb\u6027\uff0c\u4f46\u5b89\u5168\u548c\u6709\u6548\u6027\u4ecd\u9700\u5173\u6ce8\u3002", "conclusion": "\u81ea\u52a8\u5316\u53cd\u4ec7\u6068\u8a00\u8bba\u9700\u6539\u8fdb\u53ef\u8bbf\u95ee\u6027\u548c\u4f26\u7406\u5b89\u5168\u6027\uff0c\u60c5\u611f\u5f15\u5bfc\u63d0\u793a\u662f\u6f5c\u5728\u4f18\u5316\u65b9\u5411\u3002", "keywords": "\u81ea\u52a8\u5316\u53cd\u4ec7\u6068\u8a00\u8bba,LLM,\u53ef\u8bbf\u95ee\u6027,\u4f26\u7406\u98ce\u9669,\u60c5\u611f\u8bed\u8c03"}}
{"id": "2506.03546", "pdf": "https://arxiv.org/pdf/2506.03546", "abs": "https://arxiv.org/abs/2506.03546", "authors": ["Yuanchen Bai", "Zijian Ding", "Angelique Taylor"], "title": "From Virtual Agents to Robot Teams: A Multi-Robot Framework Evaluation in High-Stakes Healthcare Context", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": null, "summary": "Advancements in generative models have enabled multi-agent systems (MAS) to\nperform complex virtual tasks such as writing and code generation, which do not\ngeneralize well to physical multi-agent robotic teams. Current frameworks often\ntreat agents as conceptual task executors rather than physically embodied\nentities, and overlook critical real-world constraints such as spatial context,\nrobotic capabilities (e.g., sensing and navigation). To probe this gap, we\nreconfigure and stress-test a hierarchical multi-agent robotic team built on\nthe CrewAI framework in a simulated emergency department onboarding scenario.\nWe identify five persistent failure modes: role misalignment; tool access\nviolations; lack of in-time handling of failure reports; noncompliance with\nprescribed workflows; bypassing or false reporting of task completion. Based on\nthis analysis, we propose three design guidelines emphasizing process\ntransparency, proactive failure recovery, and contextual grounding. Our work\ninforms the development of more resilient and robust multi-agent robotic\nsystems (MARS), including opportunities to extend virtual multi-agent\nframeworks to the real world.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u673a\u5668\u4eba\u56e2\u961f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u6539\u8fdb\u8bbe\u8ba1\u4ee5\u589e\u5f3a\u5176\u5b9e\u4f53\u4efb\u52a1\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u865a\u62df\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7269\u7406\u673a\u5668\u4eba\u56e2\u961f\u4e2d\u56e0\u5ffd\u89c6\u7a7a\u95f4\u548c\u673a\u5668\u4eba\u80fd\u529b\u7b49\u73b0\u5b9e\u7ea6\u675f\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7CrewAI\u6846\u67b6\u91cd\u6784\u5e76\u538b\u529b\u6d4b\u8bd5\u4e00\u4e2a\u6a21\u62df\u6025\u8bca\u79d1\u573a\u666f\u7684\u591a\u667a\u80fd\u4f53\u673a\u5668\u4eba\u56e2\u961f\uff0c\u8bc6\u522b\u4e94\u79cd\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u89d2\u8272\u9519\u4f4d\u3001\u5de5\u5177\u8bbf\u95ee\u8fdd\u89c4\u7b49\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u8fc7\u7a0b\u900f\u660e\u3001\u4e3b\u52a8\u6545\u969c\u6062\u590d\u548c\u4e0a\u4e0b\u6587\u57fa\u7840\u4e09\u5927\u8bbe\u8ba1\u51c6\u5219\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u5177\u97e7\u6027\u548c\u9c81\u68d2\u6027\u7684\u591a\u667a\u80fd\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5e76\u63a2\u7d22\u4e86\u865a\u62df\u6846\u67b6\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u5ef6\u4f38\u53ef\u80fd\u3002", "keywords": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf, \u673a\u5668\u4eba\u56e2\u961f, \u751f\u6210\u6a21\u578b, \u6545\u969c\u6a21\u5f0f, \u8bbe\u8ba1\u51c6\u5219"}}
{"id": "2506.04165", "pdf": "https://arxiv.org/pdf/2506.04165", "abs": "https://arxiv.org/abs/2506.04165", "authors": ["Yashas Samaga", "Varun Yerram", "Spandana Raj Babbula", "Prateek Jain", "Praneeth Netrapalli"], "title": "Faster Approx. Top-K: Harnessing the Full Power of Two Stages", "categories": ["cs.LG", "cs.DS"], "comment": "Rejected from MLSys 2025", "summary": "We consider the Top-$K$ selection problem, which aims to identify the\nlargest-$K$ elements from an array. Top-$K$ selection arises in many machine\nlearning algorithms and often becomes a bottleneck on accelerators, which are\noptimized for dense matrix multiplications. To address this problem,\n\\citet{chern2022tpuknnknearestneighbor} proposed a fast two-stage\n\\textit{approximate} Top-$K$ algorithm: (i) partition the input array and\nselect the top-$1$ element from each partition, (ii) sort this \\textit{smaller\nsubset} and return the top $K$ elements. In this paper, we consider a\ngeneralized version of this algorithm, where the first stage selects top-$K'$\nelements, for some $1 \\leq K' \\leq K$, from each partition. Our contributions\nare as follows: (i) we derive an expression for the expected recall of this\ngeneralized algorithm and show that choosing $K' > 1$ with fewer partitions in\nthe first stage reduces the input size to the second stage more effectively\nwhile maintaining the same expected recall as the original algorithm, (ii) we\nderive a bound on the expected recall for the original algorithm in\n\\citet{chern2022tpuknnknearestneighbor} that is provably tighter by a factor of\n$2$ than the one in that paper, and (iii) we implement our algorithm on Cloud\nTPUv5e and achieve around an order of magnitude speedups over the original\nalgorithm without sacrificing recall on real-world tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u4e24\u9636\u6bb5\u8fd1\u4f3cTop-K\u9009\u62e9\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u7b2c\u4e00\u9636\u6bb5\u9009\u62e9\u66f4\u591a\u5143\u7d20\u4ee5\u51cf\u5c11\u7b2c\u4e8c\u9636\u6bb5\u7684\u8f93\u5165\u89c4\u6a21\uff0c\u540c\u65f6\u4fdd\u6301\u53ec\u56de\u7387\uff0c\u5e76\u5728TPUv5e\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u89e3\u51b3Top-K\u9009\u62e9\u5728\u52a0\u901f\u5668\u4e0a\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u5347\u73b0\u6709\u7b97\u6cd5\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u6269\u5c55\u4e86\u73b0\u6709\u7684\u4e24\u9636\u6bb5Top-K\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u7b2c\u4e00\u9636\u6bb5\u9009\u62e9\u66f4\u591a\u5143\u7d20\uff08K' > 1\uff09\u4ee5\u51cf\u5c11\u7b2c\u4e8c\u9636\u6bb5\u7684\u8f93\u5165\u89c4\u6a21\uff0c\u5e76\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u53ec\u56de\u7387\u7684\u7406\u8bba\u8fb9\u754c\u3002", "result": "\u5728Cloud TPUv5e\u4e0a\u5b9e\u73b0\u4e86\u7ea6\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u52a0\u901f\uff0c\u4e14\u4e0d\u635f\u5931\u53ec\u56de\u7387\u3002", "conclusion": "\u6539\u8fdb\u540e\u7684\u7b97\u6cd5\u5728\u6027\u80fd\u548c\u53ec\u56de\u7387\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u4efb\u52a1\u3002", "keywords": "Top-K\u9009\u62e9, \u4e24\u9636\u6bb5\u7b97\u6cd5, \u53ec\u56de\u7387, Cloud TPUv5e, \u6027\u80fd\u4f18\u5316"}}
{"id": "2506.04044", "pdf": "https://arxiv.org/pdf/2506.04044", "abs": "https://arxiv.org/abs/2506.04044", "authors": ["Aleksey Kudelya", "Alexander Shirnin"], "title": "Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to SemEval-2025, an ACL 2025 workshop", "summary": "This paper describes LIBU (LoRA enhanced influence-based unlearning), an\nalgorithm to solve the task of unlearning - removing specific knowledge from a\nlarge language model without retraining from scratch and compromising its\noverall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large\nLanguage Models). The algorithm combines classical \\textit{influence functions}\nto remove the influence of the data from the model and \\textit{second-order\noptimization} to stabilize the overall utility. Our experiments show that this\nlightweight approach is well applicable for unlearning LLMs in different kinds\nof task.", "AI": {"tldr": "LIBU\u662f\u4e00\u79cd\u7ed3\u5408LoRA\u548c\u5f71\u54cd\u529b\u51fd\u6570\u7684\u8f7b\u91cf\u7ea7\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u79fb\u9664\u7279\u5b9a\u77e5\u8bc6\u800c\u4e0d\u5f71\u54cd\u5176\u6574\u4f53\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5982\u4f55\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u9ad8\u6548\u79fb\u9664\u654f\u611f\u5185\u5bb9\u7684\u95ee\u9898\uff0c\u907f\u514d\u91cd\u65b0\u8bad\u7ec3\u5e26\u6765\u7684\u9ad8\u6210\u672c\u3002", "method": "\u7ed3\u5408\u5f71\u54cd\u529b\u51fd\u6570\u79fb\u9664\u6570\u636e\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u4e8c\u9636\u4f18\u5316\u7a33\u5b9a\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u4e2d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u79fb\u9664\u3002", "conclusion": "LIBU\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u8f7b\u91cf\u7684\u77e5\u8bc6\u79fb\u9664\u65b9\u6cd5\u3002", "keywords": "LIBU, LoRA, \u5f71\u54cd\u529b\u51fd\u6570, \u77e5\u8bc6\u79fb\u9664, \u4e8c\u9636\u4f18\u5316"}}
{"id": "2506.04166", "pdf": "https://arxiv.org/pdf/2506.04166", "abs": "https://arxiv.org/abs/2506.04166", "authors": ["Caleb Chin", "Aashish Khubchandani", "Harshvardhan Maskara", "Kyuseong Choi", "Jacob Feitelberg", "Albert Gong", "Manit Paul", "Tathagata Sadhukhan", "Anish Agarwal", "Raaz Dwivedi"], "title": "N$^2$: A Unified Python Package and Test Bench for Nearest Neighbor-Based Matrix Completion", "categories": ["cs.LG", "stat.CO", "stat.ML"], "comment": "21 pages, 6 figures", "summary": "Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix\ncompletion, offering strong empirical performance and recent theoretical\nguarantees, including entry-wise error bounds, confidence intervals, and\nminimax optimality. Despite their simplicity, recent work has shown that NN\napproaches are robust to a range of missingness patterns and effective across\ndiverse applications. This paper introduces N$^2$, a unified Python package and\ntestbed that consolidates a broad class of NN-based methods through a modular,\nextensible interface. Built for both researchers and practitioners, N$^2$\nsupports rapid experimentation and benchmarking. Using this framework, we\nintroduce a new NN variant that achieves state-of-the-art results in several\nsettings. We also release a benchmark suite of real-world datasets, from\nhealthcare and recommender systems to causal inference and LLM evaluation,\ndesigned to stress-test matrix completion methods beyond synthetic scenarios.\nOur experiments demonstrate that while classical methods excel on idealized\ndata, NN-based techniques consistently outperform them in real-world settings.", "AI": {"tldr": "\u6700\u8fd1\u90bb\uff08NN\uff09\u65b9\u6cd5\u5728\u77e9\u9635\u8865\u5168\u4efb\u52a1\u4e2d\u91cd\u65b0\u5d2d\u9732\u5934\u89d2\uff0c\u5177\u6709\u5f3a\u5927\u7684\u5b9e\u8bc1\u6027\u80fd\u548c\u7406\u8bba\u4fdd\u8bc1\u3002\u672c\u6587\u4ecb\u7ecd\u4e86N$^2$\uff0c\u4e00\u4e2a\u6a21\u5757\u5316\u7684Python\u5de5\u5177\u5305\uff0c\u652f\u6301\u5feb\u901f\u5b9e\u9a8c\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684NN\u53d8\u4f53\uff0c\u5728\u591a\u79cd\u573a\u666f\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6548\u679c\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u5957\u5b9e\u9645\u6570\u636e\u96c6\u57fa\u51c6\u3002", "motivation": "NN\u65b9\u6cd5\u5728\u77e9\u9635\u8865\u5168\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u5de5\u5177\u5305\u548c\u5b9e\u9645\u6570\u636e\u96c6\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u5f00\u53d1\u4e86N$^2$\u5de5\u5177\u5305\uff0c\u652f\u6301\u6a21\u5757\u5316\u6269\u5c55\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684NN\u53d8\u4f53\u3002", "result": "\u65b0NN\u53d8\u4f53\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5b9e\u9645\u6570\u636e\u9a8c\u8bc1\u4e86NN\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u3002", "conclusion": "NN\u65b9\u6cd5\u5728\u5b9e\u9645\u6570\u636e\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0cN$^2$\u4e3a\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "keywords": "\u6700\u8fd1\u90bb\u65b9\u6cd5\uff0c\u77e9\u9635\u8865\u5168\uff0cPython\u5de5\u5177\u5305\uff0c\u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2506.04047", "pdf": "https://arxiv.org/pdf/2506.04047", "abs": "https://arxiv.org/abs/2506.04047", "authors": ["Yuqian Li", "Yupei Du", "Yufang Liu", "Feifei Feng", "Mou Xiao Feng", "Yuanbin Wu"], "title": "On Support Samples of Next Word Prediction", "categories": ["cs.CL"], "comment": "Accepted to ACL2025(Main Conference)", "summary": "Language models excel in various tasks by making complex decisions, yet\nunderstanding the rationale behind these decisions remains a challenge. This\npaper investigates \\emph{data-centric interpretability} in language models,\nfocusing on the next-word prediction task. Using representer theorem, we\nidentify two types of \\emph{support samples}-those that either promote or deter\nspecific predictions. Our findings reveal that being a support sample is an\nintrinsic property, predictable even before training begins. Additionally,\nwhile non-support samples are less influential in direct predictions, they play\na critical role in preventing overfitting and shaping generalization and\nrepresentation learning. Notably, the importance of non-support samples\nincreases in deeper layers, suggesting their significant role in intermediate\nrepresentation formation.These insights shed light on the interplay between\ndata and model decisions, offering a new dimension to understanding language\nmodel behavior and interpretability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6570\u636e\u4e3a\u4e2d\u5fc3\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u652f\u6301\u6837\u672c\u5206\u6790\u9884\u6d4b\u51b3\u7b56\u80cc\u540e\u7684\u539f\u56e0\uff0c\u63ed\u793a\u4e86\u975e\u652f\u6301\u6837\u672c\u5728\u9632\u6b62\u8fc7\u62df\u5408\u548c\u5f62\u6210\u4e2d\u95f4\u8868\u793a\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u5176\u51b3\u7b56\u539f\u56e0\u96be\u4ee5\u7406\u89e3\uff0c\u56e0\u6b64\u7814\u7a76\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u53ef\u89e3\u91ca\u6027\u6210\u4e3a\u5173\u952e\u3002", "method": "\u5229\u7528\u8868\u793a\u5b9a\u7406\u5206\u6790\u652f\u6301\u6837\u672c\uff08\u652f\u6301\u6216\u963b\u6b62\u7279\u5b9a\u9884\u6d4b\u7684\u6837\u672c\uff09\uff0c\u5e76\u7814\u7a76\u975e\u652f\u6301\u6837\u672c\u7684\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u652f\u6301\u6837\u672c\u7684\u6027\u8d28\u662f\u5185\u5728\u7684\uff0c\u800c\u975e\u652f\u6301\u6837\u672c\u5728\u9632\u6b62\u8fc7\u62df\u5408\u548c\u4e2d\u95f4\u8868\u793a\u5f62\u6210\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u5728\u6df1\u5c42\u7f51\u7edc\u4e2d\u4f5c\u7528\u66f4\u663e\u8457\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u548c\u6570\u636e\u4e0e\u6a21\u578b\u51b3\u7b56\u7684\u4e92\u52a8\u63d0\u4f9b\u4e86\u65b0\u7ef4\u5ea6\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b, \u53ef\u89e3\u91ca\u6027, \u652f\u6301\u6837\u672c, \u975e\u652f\u6301\u6837\u672c, \u8868\u793a\u5b9a\u7406"}}
{"id": "2506.03568", "pdf": "https://arxiv.org/pdf/2506.03568", "abs": "https://arxiv.org/abs/2506.03568", "authors": ["Li Zeqiao", "Wang Yijing", "Wang Haoyu", "Li Zheng", "Li Peng", "Zuo zhiqiang", "Hu Chuan"], "title": "Confidence-Guided Human-AI Collaboration: Reinforcement Learning with Distributional Proxy Value Propagation for Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Autonomous driving promises significant advancements in mobility, road safety\nand traffic efficiency, yet reinforcement learning and imitation learning face\nsafe-exploration and distribution-shift challenges. Although human-AI\ncollaboration alleviates these issues, it often relies heavily on extensive\nhuman intervention, which increases costs and reduces efficiency. This paper\ndevelops a confidence-guided human-AI collaboration (C-HAC) strategy to\novercome these limitations. First, C-HAC employs a distributional proxy value\npropagation method within the distributional soft actor-critic (DSAC)\nframework. By leveraging return distributions to represent human intentions\nC-HAC achieves rapid and stable learning of human-guided policies with minimal\nhuman interaction. Subsequently, a shared control mechanism is activated to\nintegrate the learned human-guided policy with a self-learning policy that\nmaximizes cumulative rewards. This enables the agent to explore independently\nand continuously enhance its performance beyond human guidance. Finally, a\npolicy confidence evaluation algorithm capitalizes on DSAC's return\ndistribution networks to facilitate dynamic switching between human-guided and\nself-learning policies via a confidence-based intervention function. This\nensures the agent can pursue optimal policies while maintaining safety and\nperformance guarantees. Extensive experiments across diverse driving scenarios\nreveal that C-HAC significantly outperforms conventional methods in terms of\nsafety, efficiency, and overall performance, achieving state-of-the-art\nresults. The effectiveness of the proposed method is further validated through\nreal-world road tests in complex traffic conditions. The videos and code are\navailable at: https://github.com/lzqw/C-HAC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u6307\u5bfc\u7684\u4eba\u673a\u534f\u4f5c\u7b56\u7565\uff08C-HAC\uff09\uff0c\u7ed3\u5408\u4eba\u7c7b\u610f\u56fe\u4e0e\u81ea\u4e3b\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9762\u4e34\u5b89\u5168\u63a2\u7d22\u548c\u5206\u5e03\u504f\u79fb\u7684\u6311\u6218\uff0c\u4f20\u7edf\u4eba\u673a\u534f\u4f5c\u4f9d\u8d56\u8fc7\u591a\u4eba\u5de5\u5e72\u9884\uff0c\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\u3002", "method": "C-HAC\u91c7\u7528\u5206\u5e03\u4ee3\u7406\u503c\u4f20\u64ad\u65b9\u6cd5\u4e0e\u5206\u5e03\u8f6f\u6267\u884c\u8005-\u8bc4\u8bba\u5bb6\uff08DSAC\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u5171\u4eab\u63a7\u5236\u673a\u5236\u548c\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u7b97\u6cd5\u3002", "result": "C-HAC\u5728\u591a\u6837\u9a7e\u9a76\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5b89\u5168\u6027\u548c\u6548\u7387\u5747\u4e3a\u6700\u4f73\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u9053\u8def\u6d4b\u8bd5\u9a8c\u8bc1\u3002", "conclusion": "C-HAC\u901a\u8fc7\u52a8\u6001\u5207\u6362\u4eba\u673a\u7b56\u7565\uff0c\u6709\u6548\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u81ea\u52a8\u9a7e\u9a76, \u5f3a\u5316\u5b66\u4e60, \u4eba\u673a\u534f\u4f5c, \u5206\u5e03\u8f6f\u6267\u884c\u8005-\u8bc4\u8bba\u5bb6, \u7f6e\u4fe1\u5ea6\u8bc4\u4f30"}}
{"id": "2506.04168", "pdf": "https://arxiv.org/pdf/2506.04168", "abs": "https://arxiv.org/abs/2506.04168", "authors": ["Seohong Park", "Kevin Frans", "Deepinder Mann", "Benjamin Eysenbach", "Aviral Kumar", "Sergey Levine"], "title": "Horizon Reduction Makes RL Scalable", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this work, we study the scalability of offline reinforcement learning (RL)\nalgorithms. In principle, a truly scalable offline RL algorithm should be able\nto solve any given problem, regardless of its complexity, given sufficient\ndata, compute, and model capacity. We investigate if and how current offline RL\nalgorithms match up to this promise on diverse, challenging, previously\nunsolved tasks, using datasets up to 1000x larger than typical offline RL\ndatasets. We observe that despite scaling up data, many existing offline RL\nalgorithms exhibit poor scaling behavior, saturating well below the maximum\nperformance. We hypothesize that the horizon is the main cause behind the poor\nscaling of offline RL. We empirically verify this hypothesis through several\nanalysis experiments, showing that long horizons indeed present a fundamental\nbarrier to scaling up offline RL. We then show that various horizon reduction\ntechniques substantially enhance scalability on challenging tasks. Based on our\ninsights, we also introduce a minimal yet scalable method named SHARSA that\neffectively reduces the horizon. SHARSA achieves the best asymptotic\nperformance and scaling behavior among our evaluation methods, showing that\nexplicitly reducing the horizon unlocks the scalability of offline RL. Code:\nhttps://github.com/seohongpark/horizon-reduction", "AI": {"tldr": "\u7814\u7a76\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u53ef\u6269\u5c55\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u7b97\u6cd5\u5728\u6570\u636e\u89c4\u6a21\u6269\u5927\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u4efb\u52a1\u7684\u65f6\u95f4\u8de8\u5ea6\uff08horizon\uff09\u8fc7\u957f\uff0c\u5e76\u9a8c\u8bc1\u4e86\u901a\u8fc7\u51cf\u5c11\u65f6\u95f4\u8de8\u5ea6\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u53ef\u6269\u5c55\u6027\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSHARSA\u7684\u7b80\u5355\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u65f6\u95f4\u8de8\u5ea6\uff0c\u5728\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u8ba8\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u9762\u5bf9\u590d\u6742\u4efb\u52a1\u548c\u5927\u89c4\u6a21\u6570\u636e\u65f6\u662f\u5426\u80fd\u591f\u771f\u6b63\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5b9e\u9a8c\u9a8c\u8bc1\u65f6\u95f4\u8de8\u5ea6\u5bf9\u79bb\u7ebfRL\u53ef\u6269\u5c55\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u591a\u79cd\u51cf\u5c11\u65f6\u95f4\u8de8\u5ea6\u7684\u6280\u672f\uff0c\u6700\u7ec8\u8bbe\u8ba1\u4e86\u4e00\u79cd\u540d\u4e3aSHARSA\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u51cf\u5c11\u65f6\u95f4\u8de8\u5ea6\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u79bb\u7ebfRL\u7684\u53ef\u6269\u5c55\u6027\uff0cSHARSA\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u65f6\u95f4\u8de8\u5ea6\u662f\u9650\u5236\u79bb\u7ebfRL\u53ef\u6269\u5c55\u6027\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u901a\u8fc7\u51cf\u5c11\u65f6\u95f4\u8de8\u5ea6\u53ef\u4ee5\u89e3\u9501\u5176\u6f5c\u529b\u3002SHARSA\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "keywords": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u53ef\u6269\u5c55\u6027\uff0c\u65f6\u95f4\u8de8\u5ea6\uff0cSHARSA\uff0c\u5927\u6570\u636e"}}
{"id": "2506.04050", "pdf": "https://arxiv.org/pdf/2506.04050", "abs": "https://arxiv.org/abs/2506.04050", "authors": ["Hadi Mohammadi", "Anastasia Giachanou", "Daniel L. Oberski", "Ayoub Bagheri"], "title": "Explainability-Based Token Replacement on LLM-Generated Text", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative models, especially large language models (LLMs), have shown\nremarkable progress in producing text that appears human-like. However, they\noften exhibit patterns that make their output easier to detect than text\nwritten by humans. In this paper, we investigate how explainable AI (XAI)\nmethods can be used to reduce the detectability of AI-generated text (AIGT)\nwhile also introducing a robust ensemble-based detection approach. We begin by\ntraining an ensemble classifier to distinguish AIGT from human-written text,\nthen apply SHAP and LIME to identify tokens that most strongly influence its\npredictions. We propose four explainability-based token replacement strategies\nto modify these influential tokens. Our findings show that these token\nreplacement approaches can significantly diminish a single classifier's ability\nto detect AIGT. However, our ensemble classifier maintains strong performance\nacross multiple languages and domains, showing that a multi-model approach can\nmitigate the impact of token-level manipulations. These results show that XAI\nmethods can make AIGT harder to detect by focusing on the most influential\ntokens. At the same time, they highlight the need for robust, ensemble-based\ndetection strategies that can adapt to evolving approaches for hiding AIGT.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5229\u7528\u53ef\u89e3\u91caAI\u65b9\u6cd5\u964d\u4f4eAI\u751f\u6210\u6587\u672c\u7684\u53ef\u68c0\u6d4b\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u7684\u7a33\u5065\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660eXAI\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u68c0\u6d4b\u7387\uff0c\u4f46\u9700\u8981\u591a\u6a21\u578b\u65b9\u6cd5\u5e94\u5bf9\u4ee4\u724c\u7ea7\u64cd\u4f5c\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7XAI\u65b9\u6cd5\u51cf\u5c11AI\u751f\u6210\u6587\u672c\u7684\u53ef\u68c0\u6d4b\u6027\uff0c\u540c\u65f6\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528SHAP\u548cLIME\u8bc6\u522b\u5bf9\u5206\u7c7b\u5668\u9884\u6d4b\u5f71\u54cd\u6700\u5927\u7684\u4ee4\u724c\uff0c\u63d0\u51fa\u56db\u79cd\u57fa\u4e8e\u89e3\u91ca\u6027\u7684\u4ee4\u724c\u66ff\u6362\u7b56\u7565\uff0c\u5e76\u8bad\u7ec3\u96c6\u6210\u5206\u7c7b\u5668\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "\u4ee4\u724c\u66ff\u6362\u7b56\u7565\u80fd\u663e\u8457\u964d\u4f4e\u5355\u4e00\u5206\u7c7b\u5668\u5bf9AI\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4f46\u96c6\u6210\u5206\u7c7b\u5668\u5728\u591a\u8bed\u8a00\u548c\u9886\u57df\u4e0b\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "XAI\u65b9\u6cd5\u80fd\u6709\u6548\u9690\u85cfAI\u751f\u6210\u6587\u672c\uff0c\u4f46\u9700\u8981\u591a\u6a21\u578b\u68c0\u6d4b\u7b56\u7565\u5e94\u5bf9\u4ee4\u724c\u7ea7\u5e72\u6270\u3002", "keywords": "AI\u751f\u6210\u6587\u672c\uff0c\u53ef\u89e3\u91caAI\uff0c\u4ee4\u724c\u66ff\u6362\uff0c\u96c6\u6210\u5206\u7c7b\u5668\uff0c\u68c0\u6d4b"}}
{"id": "2506.03571", "pdf": "https://arxiv.org/pdf/2506.03571", "abs": "https://arxiv.org/abs/2506.03571", "authors": ["Chong Hyun Lee", "Kibae Lee"], "title": "DiagNet: Detecting Objects using Diagonal Constraints on Adjacency Matrix of Graph Neural Network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose DaigNet, a new approach to object detection with which we can\ndetect an object bounding box using diagonal constraints on adjacency matrix of\na graph convolutional network (GCN). We propose two diagonalization algorithms\nbased on hard and soft constraints on adjacency matrix and two loss functions\nusing diagonal constraint and complementary constraint. The DaigNet eliminates\nthe need for designing a set of anchor boxes commonly used. To prove\nfeasibility of our novel detector, we adopt detection head in YOLO models.\nExperiments show that the DiagNet achieves 7.5% higher mAP50 on Pascal VOC than\nYOLOv1. The DiagNet also shows 5.1% higher mAP on MS COCO than YOLOv3u, 3.7%\nhigher mAP than YOLOv5u, and 2.9% higher mAP than YOLOv8.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDaigNet\u7684\u65b0\u5bf9\u8c61\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u5377\u79ef\u7f51\u7edc\u7684\u90bb\u63a5\u77e9\u9635\u5bf9\u89d2\u7ebf\u7ea6\u675f\u68c0\u6d4b\u5bf9\u8c61\u8fb9\u754c\u6846\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u79cd\u5bf9\u89d2\u5316\u7b97\u6cd5\u53ca\u635f\u5931\u51fd\u6570\u3002", "motivation": "\u4f20\u7edf\u5bf9\u8c61\u68c0\u6d4b\u65b9\u6cd5\u9700\u8981\u8bbe\u8ba1\u951a\u6846\uff0c\u800cDaigNet\u901a\u8fc7\u90bb\u63a5\u77e9\u9635\u7684\u5bf9\u89d2\u7ebf\u7ea6\u675f\u907f\u514d\u4e86\u8fd9\u4e00\u6b65\u9aa4\uff0c\u7b80\u5316\u4e86\u68c0\u6d4b\u6d41\u7a0b\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u56fe\u5377\u79ef\u7f51\u7edc\u7684\u90bb\u63a5\u77e9\u9635\u5bf9\u89d2\u7ebf\u7ea6\u675f\uff0c\u8bbe\u8ba1\u786c\u548c\u8f6f\u7ea6\u675f\u4e24\u79cd\u5bf9\u89d2\u5316\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u5bf9\u89d2\u7ebf\u7ea6\u675f\u548c\u4e92\u8865\u7ea6\u675f\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728Pascal VOC\u4e0amAP50\u6bd4YOLOv1\u9ad87.5%\uff0c\u5728MS COCO\u4e0a\u5206\u522b\u6bd4YOLOv3u\u3001YOLOv5u\u548cYOLOv8\u9ad85.1%\u30013.7%\u548c2.9%\u3002", "conclusion": "DaigNet\u901a\u8fc7\u90bb\u63a5\u77e9\u9635\u5bf9\u89d2\u7ebf\u7ea6\u675f\u907f\u514d\u4e86\u951a\u6846\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8c61\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "keywords": "\u5bf9\u8c61\u68c0\u6d4b, \u56fe\u5377\u79ef\u7f51\u7edc, \u5bf9\u89d2\u7ebf\u7ea6\u675f, \u951a\u6846"}}
{"id": "2506.04171", "pdf": "https://arxiv.org/pdf/2506.04171", "abs": "https://arxiv.org/abs/2506.04171", "authors": ["Utkarsh Utkarsh", "Pengfei Cai", "Alan Edelman", "Rafael Gomez-Bombarelli", "Christopher Vincent Rackauckas"], "title": "Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NA", "math.NA"], "comment": "27 pages, 9 figures, 4 tables", "summary": "Deep generative models have recently been applied to physical systems\ngoverned by partial differential equations (PDEs), offering scalable simulation\nand uncertainty-aware inference. However, enforcing physical constraints, such\nas conservation laws (linear and nonlinear) and physical consistencies, remains\nchallenging. Existing methods often rely on soft penalties or architectural\nbiases that fail to guarantee hard constraints. In this work, we propose\nPhysics-Constrained Flow Matching (PCFM), a zero-shot inference framework that\nenforces arbitrary nonlinear constraints in pretrained flow-based generative\nmodels. PCFM continuously guides the sampling process through physics-based\ncorrections applied to intermediate solution states, while remaining aligned\nwith the learned flow and satisfying physical constraints. Empirically, PCFM\noutperforms both unconstrained and constrained baselines on a range of PDEs,\nincluding those with shocks, discontinuities, and sharp features, while\nensuring exact constraint satisfaction at the final solution. Our method\nprovides a general framework for enforcing hard constraints in both scientific\nand general-purpose generative models, especially in applications where\nconstraint satisfaction is essential.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPCFM\u7684\u96f6\u6837\u672c\u63a8\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u9884\u8bad\u7ec3\u7684\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\u4e2d\u5f3a\u5236\u6267\u884c\u975e\u7ebf\u6027\u7269\u7406\u7ea6\u675f\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u6a21\u62df\u7269\u7406\u7cfb\u7edf\u65f6\u96be\u4ee5\u4e25\u683c\u6ee1\u8db3\u7269\u7406\u7ea6\u675f\uff08\u5982\u5b88\u6052\u5b9a\u5f8b\uff09\uff0c\u5e38\u7528\u65b9\u6cd5\uff08\u5982\u8f6f\u60e9\u7f5a\u6216\u7ed3\u6784\u504f\u7f6e\uff09\u65e0\u6cd5\u4fdd\u8bc1\u786c\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u4e86PCFM\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5bf9\u4e2d\u95f4\u72b6\u6001\u65bd\u52a0\u7269\u7406\u6821\u6b63\uff0c\u786e\u4fdd\u6700\u7ec8\u89e3\u4e25\u683c\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\u3002", "result": "PCFM\u5728\u4e00\u7cfb\u5217PDE\u95ee\u9898\u4e0a\u4f18\u4e8e\u672a\u7ea6\u675f\u548c\u7ea6\u675f\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5904\u7406\u6fc0\u6ce2\u3001\u4e0d\u8fde\u7eed\u6027\u548c\u5c16\u9510\u7279\u5f81\u65f6\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "PCFM\u4e3a\u79d1\u5b66\u548c\u901a\u7528\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5236\u6267\u884c\u786c\u7ea6\u675f\u7684\u901a\u7528\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u7ea6\u675f\u6ee1\u8db3\u81f3\u5173\u91cd\u8981\u7684\u5e94\u7528\u573a\u666f\u3002", "keywords": "\u6df1\u5ea6\u751f\u6210\u6a21\u578b, \u7269\u7406\u7ea6\u675f, \u57fa\u4e8e\u6d41\u7684\u6a21\u578b, \u504f\u5fae\u5206\u65b9\u7a0b, \u96f6\u6837\u672c\u63a8\u65ad"}}
{"id": "2506.04051", "pdf": "https://arxiv.org/pdf/2506.04051", "abs": "https://arxiv.org/abs/2506.04051", "authors": ["Tim Franzmeyer", "Archie Sravankumar", "Lijuan Liu", "Yuning Mao", "Rui Hou", "Sinong Wang", "Jakob N. Foerster", "Luke Zettlemoyer", "Madian Khabsa"], "title": "High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) currently respond to every prompt. However, they\ncan produce incorrect answers when they lack knowledge or capability -- a\nproblem known as hallucination. We instead propose post-training an LLM to\ngenerate content only when confident in its correctness and to otherwise\n(partially) abstain. Specifically, our method, HALT, produces\ncapability-aligned post-training data that encodes what the model can and\ncannot reliably generate. We generate this data by splitting responses of the\npretrained LLM into factual fragments (atomic statements or reasoning steps),\nand use ground truth information to identify incorrect fragments. We achieve\ncapability-aligned finetuning responses by either removing incorrect fragments\nor replacing them with \"Unsure from Here\" -- according to a tunable threshold\nthat allows practitioners to trade off response completeness and mean\ncorrectness of the response's fragments. We finetune four open-source models\nfor biography writing, mathematics, coding, and medicine with HALT for three\ndifferent trade-off thresholds. HALT effectively trades off response\ncompleteness for correctness, increasing the mean correctness of response\nfragments by 15% on average, while resulting in a 4% improvement in the F1\nscore (mean of completeness and correctness of the response) compared to the\nrelevant baselines. By tuning HALT for highest correctness, we train a single\nreliable Llama3-70B model with correctness increased from 51% to 87% across all\nfour domains while maintaining 53% of the response completeness achieved with\nstandard finetuning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86HALT\u65b9\u6cd5\uff0c\u901a\u8fc7\u540e\u8bad\u7ec3\u4f7fLLMs\u4ec5\u5728\u786e\u4fe1\u6b63\u786e\u65f6\u751f\u6210\u5185\u5bb9\uff0c\u5426\u5219\u90e8\u5206\u56de\u907f\u56de\u7b54\uff0c\u4ece\u800c\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u6b63\u786e\u6027\u4e0e\u5b8c\u6574\u6027\u7684\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3LLMs\u56e0\u7f3a\u4e4f\u77e5\u8bc6\u6216\u80fd\u529b\u800c\u4ea7\u751f\u9519\u8bef\u7b54\u6848\uff08\u5e7b\u89c9\uff09\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u540e\u8bad\u7ec3\u4f7f\u5176\u5728\u4e0d\u786e\u5b9a\u65f6\u9009\u62e9\u56de\u907f\u3002", "method": "\u63d0\u51faHALT\u65b9\u6cd5\uff0c\u901a\u8fc7\u62c6\u5206\u9884\u8bad\u7ec3LLM\u7684\u56de\u7b54\u4e3a\u4e8b\u5b9e\u7247\u6bb5\uff0c\u5229\u7528\u771f\u5b9e\u4fe1\u606f\u8bc6\u522b\u5e76\u4fee\u6b63\u9519\u8bef\u7247\u6bb5\uff0c\u8c03\u6574\u9608\u503c\u4ee5\u5b9e\u73b0\u6b63\u786e\u6027\u4e0e\u5b8c\u6574\u6027\u7684\u5e73\u8861\u3002", "result": "\u5728\u4f20\u8bb0\u3001\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u533b\u5b66\u9886\u57df\uff0cHALT\u5e73\u5747\u63d0\u9ad8\u4e8615%\u7684\u7247\u6bb5\u6b63\u786e\u6027\uff0cF1\u5206\u6570\u63d0\u53474%\uff0c\u6700\u9ad8\u6b63\u786e\u6027\u53ef\u8fbe87%\u3002", "conclusion": "HALT\u6709\u6548\u51cf\u5c11\u4e86LLMs\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6b63\u786e\u6027\u4e0e\u9002\u5ea6\u5b8c\u6574\u6027\u7684\u7ed3\u5408\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002", "keywords": "LLMs, \u5e7b\u89c9, \u540e\u8bad\u7ec3, HALT, \u6b63\u786e\u6027, \u5b8c\u6574\u6027"}}
{"id": "2506.04172", "pdf": "https://arxiv.org/pdf/2506.04172", "abs": "https://arxiv.org/abs/2506.04172", "authors": ["Shreenidhi Srinivasan", "Lydia Manikonda"], "title": "Does Prompt Design Impact Quality of Data Imputation by LLMs?", "categories": ["cs.LG", "cs.ET"], "comment": "7 pages", "summary": "Generating realistic synthetic tabular data presents a critical challenge in\nmachine learning. It adds another layer of complexity when this data contain\nclass imbalance problems. This paper presents a novel token-aware data\nimputation method that leverages the in-context learning capabilities of large\nlanguage models. This is achieved through the combination of a structured\ngroup-wise CSV-style prompting technique and the elimination of irrelevant\ncontextual information in the input prompt. We test this approach with two\nclass-imbalanced binary classification datasets and evaluate the effectiveness\nof imputation using classification-based evaluation metrics. The experimental\nresults demonstrate that our approach significantly reduces the input prompt\nsize while maintaining or improving imputation quality compared to our baseline\nprompt, especially for datasets that are of relatively smaller in size. The\ncontributions of this presented work is two-fold -- 1) it sheds light on the\nimportance of prompt design when leveraging LLMs for synthetic data generation\nand 2) it addresses a critical gap in LLM-based data imputation for\nclass-imbalanced datasets with missing data by providing a practical solution\nwithin computational constraints. We hope that our work will foster further\nresearch and discussions about leveraging the incredible potential of LLMs and\nprompt engineering techniques for synthetic data generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6807\u8bb0\u611f\u77e5\u6570\u636e\u586b\u5145\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u8bbe\u8ba1\u51cf\u5c11\u8f93\u5165\u5927\u5c0f\u540c\u65f6\u4fdd\u6301\u586b\u5145\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e2d\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u7f3a\u5931\u6570\u636e\u586b\u5145\u7684\u6311\u6218\uff0c\u540c\u65f6\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63d0\u793a\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u5206\u7ec4CSV\u98ce\u683c\u7684\u63d0\u793a\u6280\u672f\uff0c\u7ed3\u5408\u8fc7\u6ee4\u65e0\u5173\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6570\u636e\u586b\u5145\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c0f\u8f93\u5165\u63d0\u793a\u5927\u5c0f\uff0c\u5e76\u5728\u8f83\u5c0f\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u6216\u63d0\u5347\u586b\u5145\u8d28\u91cf\u3002", "conclusion": "\u63d0\u793a\u8bbe\u8ba1\u5bf9\u5229\u7528LLMs\u751f\u6210\u5408\u6210\u6570\u636e\u81f3\u5173\u91cd\u8981\uff0c\u540c\u65f6\u4e3a\u89e3\u51b3\u8ba1\u7b97\u53d7\u9650\u4e0b\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u7684\u6570\u636e\u586b\u5145\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002", "keywords": "\u5408\u6210\u6570\u636e\u751f\u6210, \u7c7b\u522b\u4e0d\u5e73\u8861, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u63d0\u793a\u5de5\u7a0b, \u6570\u636e\u586b\u5145"}}
{"id": "2506.04065", "pdf": "https://arxiv.org/pdf/2506.04065", "abs": "https://arxiv.org/abs/2506.04065", "authors": ["Muling Wu", "Qi Qian", "Wenhao Liu", "Xiaohua Wang", "Zisu Huang", "Di Liang", "LI Miao", "Shihan Dou", "Changze Lv", "Zhenghua Wang", "Zhibo Xu", "Lina Chen", "Tianlong Li", "Xiaoqing Zheng", "Xuanjing Huang"], "title": "Progressive Mastery: Customized Curriculum Learning with Guided Prompting for Mathematical Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable performance across\nvarious reasoning tasks, yet post-training is constrained by inefficient sample\nutilization and inflexible difficulty samples processing. To address these\nlimitations, we propose Customized Curriculum Learning (CCL), a novel framework\nwith two key innovations. First, we introduce model-adaptive difficulty\ndefinition that customizes curriculum datasets based on each model's individual\ncapabilities rather than using predefined difficulty metrics. Second, we\ndevelop \"Guided Prompting,\" which dynamically reduces sample difficulty through\nstrategic hints, enabling effective utilization of challenging samples that\nwould otherwise degrade performance. Comprehensive experiments on supervised\nfine-tuning and reinforcement learning demonstrate that CCL significantly\noutperforms uniform training approaches across five mathematical reasoning\nbenchmarks, confirming its effectiveness across both paradigms in enhancing\nsample utilization and model performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCCL\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u5236\u5316\u8bfe\u7a0b\u5b66\u4e60\u548c\u52a8\u6001\u63d0\u793a\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u6837\u672c\u5229\u7528\u6548\u7387\u4f4e\u548c\u96be\u5ea6\u5904\u7406\u4e0d\u7075\u6d3b\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8bad\u7ec3\u540e\u7684\u6837\u672c\u5229\u7528\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u7075\u6d3b\u5904\u7406\u96be\u5ea6\u6837\u672c\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e86\u5b9a\u5236\u5316\u8bfe\u7a0b\u5b66\u4e60\uff08CCL\uff09\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u521b\u65b0\uff1a\u6a21\u578b\u81ea\u9002\u5e94\u96be\u5ea6\u5b9a\u4e49\u548c\u52a8\u6001\u63d0\u793a\u6280\u672f\uff08Guided Prompting\uff09\u3002", "result": "\u5728\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u9a8c\u4e2d\uff0cCCL\u5728\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u7edf\u4e00\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u5229\u7528\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "CCL\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6837\u672c\u5229\u7528\u7387\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8bad\u7ec3\u8303\u5f0f\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5b9a\u5236\u5316\u8bfe\u7a0b\u5b66\u4e60, \u52a8\u6001\u63d0\u793a, \u6570\u5b66\u63a8\u7406, \u76d1\u7763\u5fae\u8c03"}}
{"id": "2506.03582", "pdf": "https://arxiv.org/pdf/2506.03582", "abs": "https://arxiv.org/abs/2506.03582", "authors": ["Rui Yann", "Xianglei Xing"], "title": "ViTSGMM: A Robust Semi-Supervised Image Recognition Network Using Sparse Labels", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We present ViTSGMM, an image recognition network that leverages\nsemi-supervised learning in a highly efficient manner. Existing works often\nrely on complex training techniques and architectures, while their\ngeneralization ability when dealing with extremely limited labeled data remains\nto be improved. To address these limitations, we construct a hierarchical\nmixture density classification decision mechanism by optimizing mutual\ninformation between feature representations and target classes, compressing\nredundant information while retaining crucial discriminative components.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance on STL-10 and CIFAR-10/100 datasets when using negligible labeled\nsamples. Notably, this paper also reveals a long-overlooked data leakage issue\nin the STL-10 dataset for semi-supervised learning tasks and removes duplicates\nto ensure the reliability of experimental results. Code available at\nhttps://github.com/Shu1L0n9/ViTSGMM.", "AI": {"tldr": "ViTSGMM\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u534a\u76d1\u7763\u5b66\u4e60\u56fe\u50cf\u8bc6\u522b\u7f51\u7edc\uff0c\u901a\u8fc7\u4f18\u5316\u7279\u5f81\u8868\u793a\u4e0e\u76ee\u6807\u7c7b\u522b\u95f4\u7684\u4e92\u4fe1\u606f\u6765\u6784\u5efa\u5206\u7c7b\u51b3\u7b56\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u6781\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u8bad\u7ec3\u6280\u672f\u548c\u67b6\u6784\uff0c\u4e14\u5728\u5904\u7406\u6781\u6709\u9650\u6807\u6ce8\u6570\u636e\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6df7\u5408\u5bc6\u5ea6\u5206\u7c7b\u51b3\u7b56\u673a\u5236\uff0c\u4f18\u5316\u4e92\u4fe1\u606f\u5e76\u538b\u7f29\u5197\u4f59\u4fe1\u606f\uff0c\u4fdd\u7559\u5173\u952e\u5224\u522b\u6210\u5206\u3002", "result": "\u5728STL-10\u548cCIFAR-10/100\u6570\u636e\u96c6\u4e0a\u4ee5\u6781\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "ViTSGMM\u89e3\u51b3\u4e86\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u63d0\u5347\u5b9e\u9a8c\u53ef\u9760\u6027\u3002", "keywords": "ViTSGMM, \u534a\u76d1\u7763\u5b66\u4e60, \u56fe\u50cf\u8bc6\u522b, \u4e92\u4fe1\u606f\u4f18\u5316, \u6570\u636e\u6cc4\u9732"}}
{"id": "2506.04178", "pdf": "https://arxiv.org/pdf/2506.04178", "abs": "https://arxiv.org/abs/2506.04178", "authors": ["Etash Guha", "Ryan Marten", "Sedrick Keh", "Negin Raoof", "Georgios Smyrnis", "Hritik Bansal", "Marianna Nezhurina", "Jean Mercat", "Trung Vu", "Zayne Sprague", "Ashima Suvarna", "Benjamin Feuer", "Liangyu Chen", "Zaid Khan", "Eric Frankel", "Sachin Grover", "Caroline Choi", "Niklas Muennighoff", "Shiye Su", "Wanjia Zhao", "John Yang", "Shreyas Pimpalgaonkar", "Kartik Sharma", "Charlie Cheng-Jie Ji", "Yichuan Deng", "Sarah Pratt", "Vivek Ramanujan", "Jon Saad-Falcon", "Jeffrey Li", "Achal Dave", "Alon Albalak", "Kushal Arora", "Blake Wulfe", "Chinmay Hegde", "Greg Durrett", "Sewoong Oh", "Mohit Bansal", "Saadia Gabriel", "Aditya Grover", "Kai-Wei Chang", "Vaishaal Shankar", "Aaron Gokaslan", "Mike A. Merrill", "Tatsunori Hashimoto", "Yejin Choi", "Jenia Jitsev", "Reinhard Heckel", "Maheswaran Sathiamoorthy", "Alexandros G. Dimakis", "Ludwig Schmidt"], "title": "OpenThoughts: Data Recipes for Reasoning Models", "categories": ["cs.LG"], "comment": "https://www.openthoughts.ai/blog/ot3", "summary": "Reasoning models have made rapid progress on many benchmarks involving math,\ncode, and science. Yet, there are still many open questions about the best\ntraining recipes for reasoning since state-of-the-art models often rely on\nproprietary datasets with little to no public information available. To address\nthis, the goal of the OpenThoughts project is to create open-source datasets\nfor training reasoning models. After initial explorations, our OpenThoughts2-1M\ndataset led to OpenThinker2-32B, the first model trained on public reasoning\ndata to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as\nAIME and LiveCodeBench. We then improve our dataset further by systematically\ninvestigating each step of our data generation pipeline with 1,000+ controlled\nexperiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples\nand using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves\nstate-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25,\nand 54% on GPQA Diamond. All of our datasets and models are available on\nhttps://openthoughts.ai.", "AI": {"tldr": "OpenThoughts\u9879\u76ee\u901a\u8fc7\u5f00\u6e90\u6570\u636e\u96c6\u548c\u4f18\u5316\u6d41\u7a0b\uff0c\u8bad\u7ec3\u51fa\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u7684\u6a21\u578bOpenThinker3-7B\uff0c\u5e76\u516c\u5f00\u4e86\u6240\u6709\u6570\u636e\u548c\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\u56e0\u4f9d\u8d56\u4e13\u6709\u6570\u636e\u96c6\u800c\u7f3a\u4e4f\u516c\u5f00\u4fe1\u606f\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u4f18\u5316\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u6269\u5c55\u6570\u636e\u96c6\u5e76\u8bad\u7ec3\u6a21\u578b\u3002", "result": "OpenThinker3-7B\u5728AIME\u3001LiveCodeBench\u548cGPQA Diamond\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u9886\u5148\u6210\u7ee9\u3002", "conclusion": "\u5f00\u6e90\u6570\u636e\u548c\u4f18\u5316\u6d41\u7a0b\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u3002", "keywords": "OpenThoughts,\u5f00\u6e90\u6570\u636e\u96c6,\u63a8\u7406\u6a21\u578b,\u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2506.04070", "pdf": "https://arxiv.org/pdf/2506.04070", "abs": "https://arxiv.org/abs/2506.04070", "authors": ["Yi Zhao", "Siqi Wang", "Jing Li"], "title": "LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Navigation instruction generation for visually impaired (VI) individuals\n(NIG-VI) is critical yet relatively underexplored. This study, hence, focuses\non producing precise, in-situ, step-by-step navigation instructions that are\npractically usable by VI users. Concretely, we propose LaF-GRPO\n(LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate\nrewards guiding the Vision-Language Model (VLM) post-training. This enhances\ninstruction usability while reducing costly real-world data needs. To\nfacilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced\nbenchmark. It provides diverse navigation scenarios with accurate spatial\ncoordinates, supporting detailed, open-ended in-situ instruction generation.\nExperiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative\nmetrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\\%; SFT+(LaF-GRPO) METEOR 0.542\nvs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and\nbenchmark are available at\n\\href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\uff08LaF-GRPO\uff09\u6765\u4e3a\u89c6\u969c\u4eba\u58eb\u751f\u6210\u7cbe\u786e\u7684\u5bfc\u822a\u6307\u4ee4\uff0c\u5e76\u901a\u8fc7\u65b0\u6570\u636e\u96c6NIG4VI\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89c6\u969c\u4eba\u58eb\u7684\u5bfc\u822a\u6307\u4ee4\u751f\u6210\uff08NIG-VI\uff09\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u7814\u7a76\u4e0d\u8db3\u7684\u9886\u57df\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faLaF-GRPO\u6846\u67b6\uff0c\u5229\u7528LLM\u6a21\u62df\u89c6\u969c\u7528\u6237\u54cd\u5e94\u751f\u6210\u5956\u52b1\u4fe1\u53f7\uff0c\u6307\u5bfcVLM\u540e\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLaF-GRPO\u5728BLEU\u548cMETEOR\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff08\u5982+14%\uff09\uff0c\u751f\u6210\u7684\u6307\u4ee4\u66f4\u76f4\u89c2\u3001\u5b89\u5168\u3002", "conclusion": "LaF-GRPO\u7ed3\u5408NIG4VI\u6570\u636e\u96c6\u4e3a\u89c6\u969c\u4eba\u58eb\u5bfc\u822a\u6307\u4ee4\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5bfc\u822a\u6307\u4ee4\u751f\u6210,\u89c6\u969c\u4eba\u58eb,LLM,\u5956\u52b1\u4fe1\u53f7,VLM"}}
{"id": "2506.04190", "pdf": "https://arxiv.org/pdf/2506.04190", "abs": "https://arxiv.org/abs/2506.04190", "authors": ["Yuxuan Cao", "Jiarong Xu", "Chen Zhao", "Jiaan Wang", "Carl Yang", "Chunping Wang", "Yang Yang"], "title": "How to Use Graph Data in the Wild to Help Graph Anomaly Detection?", "categories": ["cs.LG"], "comment": "Accepted by SIGKDD2025", "summary": "In recent years, graph anomaly detection has found extensive applications in\nvarious domains such as social, financial, and communication networks. However,\nanomalies in graph-structured data present unique challenges, including label\nscarcity, ill-defined anomalies, and varying anomaly types, making supervised\nor semi-supervised methods unreliable. Researchers often adopt unsupervised\napproaches to address these challenges, assuming that anomalies deviate\nsignificantly from the normal data distribution. Yet, when the available data\nis insufficient, capturing the normal distribution accurately and\ncomprehensively becomes difficult. To overcome this limitation, we propose to\nutilize external graph data (i.e., graph data in the wild) to help anomaly\ndetection tasks. This naturally raises the question: How can we use external\ndata to help graph anomaly detection tasks? To answer this question, we propose\na framework called Wild-GAD. It is built upon a unified database, UniWildGraph,\nwhich comprises a large and diverse collection of graph data with broad domain\ncoverage, ample data volume, and a unified feature space. Further, we develop\nselection criteria based on representativity and diversity to identify the most\nsuitable external data for anomaly detection task. Extensive experiments on six\nreal-world datasets demonstrate the effectiveness of Wild-GAD. Compared to the\nbaseline methods, our framework has an average 18% AUCROC and 32% AUCPR\nimprovement over the best-competing methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5916\u90e8\u56fe\u6570\u636e\u8f85\u52a9\u56fe\u5f02\u5e38\u68c0\u6d4b\u7684\u65b0\u6846\u67b6Wild-GAD\uff0c\u901a\u8fc7\u7edf\u4e00\u6570\u636e\u5e93\u548c\u9009\u62e9\u6807\u51c6\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u56fe\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u6807\u7b7e\u7a00\u7f3a\u3001\u5f02\u5e38\u5b9a\u4e49\u6a21\u7cca\u7b49\u6311\u6218\uff0c\u4e14\u6570\u636e\u4e0d\u8db3\u65f6\u96be\u4ee5\u51c6\u786e\u6355\u6349\u6b63\u5e38\u6570\u636e\u5206\u5e03\u3002", "method": "\u63d0\u51faWild-GAD\u6846\u67b6\uff0c\u57fa\u4e8e\u7edf\u4e00\u6570\u636e\u5e93UniWildGraph\uff0c\u5e76\u4f7f\u7528\u4ee3\u8868\u6027\u548c\u591a\u6837\u6027\u6807\u51c6\u9009\u62e9\u5916\u90e8\u6570\u636e\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0cAUCROC\u548cAUCPR\u5206\u522b\u5e73\u5747\u63d0\u534718%\u548c32%\u3002", "conclusion": "Wild-GAD\u901a\u8fc7\u5916\u90e8\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u56fe\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "keywords": "\u56fe\u5f02\u5e38\u68c0\u6d4b,\u5916\u90e8\u6570\u636e,Wild-GAD,UniWildGraph,\u65e0\u76d1\u7763\u5b66\u4e60"}}
{"id": "2506.04072", "pdf": "https://arxiv.org/pdf/2506.04072", "abs": "https://arxiv.org/abs/2506.04072", "authors": ["Meiqing Jin", "Liam Dugan", "Chris Callison-Burch"], "title": "Controlling Difficulty of Generated Text for AI-Assisted Language Learning", "categories": ["cs.CL", "cs.HC", "I.2.7"], "comment": "Submitted to EMNLP 2025", "summary": "Practicing conversations with large language models (LLMs) presents a\npromising alternative to traditional in-person language learning. However, most\nLLMs generate text at a near-native level of complexity, making them ill-suited\nfor beginner learners (CEFR: A1-A2). In this paper, we investigate whether\ncontrollable generation techniques -- specifically modular methods that do not\nrequire model fine-tuning -- can adapt LLM outputs to better support absolute\nbeginners. We evaluate these methods through both automatic metrics and a user\nstudy with university-level learners of Japanese. Our findings show that while\nprompting alone fails to control output difficulty, the use of future\ndiscriminators (Yang and Klein, 2021) significantly improves output\ncomprehensibility (from 40.4\\% to 84.3\\%). We further introduce a novel\ntoken-level evaluation metric, Token Miss Rate (TMR), that quantifies the\nproportion of incomprehensible tokens per utterance and correlates strongly\nwith human judgments. To support future research in AI-assisted language\nlearning, we release our code, models, annotation tools, and dataset.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u53ef\u63a7\u751f\u6210\u6280\u672f\u8c03\u6574\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8f93\u51fa\uff0c\u4ee5\u66f4\u9002\u5408\u4f5c\u521d\u5b66\u8005\uff08CEFR A1-A2\uff09\u7684\u8bed\u8a00\u5b66\u4e60\u9700\u6c42\uff0c\u53d1\u73b0\u7279\u5b9a\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u8f93\u51fa\u53ef\u7406\u89e3\u6027\u3002", "motivation": "\u4f20\u7edf\u8bed\u8a00\u5b66\u4e60\u65b9\u5f0f\u4e0eLLM\u751f\u6210\u6587\u672c\u7684\u9ad8\u590d\u6742\u6027\u4e0d\u5339\u914d\uff0c\u5c24\u5176\u5bf9\u521d\u5b66\u8005\u4e0d\u5229\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u65e0\u9700\u8c03\u4f18\u7684\u6a21\u5757\u5316\u53ef\u63a7\u751f\u6210\u6280\u672f\uff08\u5982\u672a\u6765\u9274\u522b\u5668\uff09\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\uff08\u65e5\u8bed\u5b66\u4e60\u8005\uff09\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u672a\u6765\u9274\u522b\u5668\u663e\u8457\u63d0\u5347\u8f93\u51fa\u53ef\u7406\u89e3\u6027\uff0840.4%\u81f384.3%\uff09\uff0c\u5e76\u63d0\u51fa\u65b0\u6307\u6807TMR\u91cf\u5316\u4e0d\u53ef\u7406\u89e3\u8bcd\u6c47\u6bd4\u4f8b\u3002", "conclusion": "\u53ef\u63a7\u751f\u6210\u6280\u672f\u80fd\u6709\u6548\u9002\u914d\u521d\u5b66\u8005\u9700\u6c42\uff0c\u4e3aAI\u8f85\u52a9\u8bed\u8a00\u5b66\u4e60\u63d0\u4f9b\u65b0\u5de5\u5177\u4e0e\u6570\u636e\u652f\u6301\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u53ef\u63a7\u751f\u6210\u3001\u8bed\u8a00\u5b66\u4e60\u3001\u521d\u5b66\u8005\u3001CEFR"}}
{"id": "2506.03589", "pdf": "https://arxiv.org/pdf/2506.03589", "abs": "https://arxiv.org/abs/2506.03589", "authors": ["Huy Le", "Nhat Chung", "Tung Kieu", "Anh Nguyen", "Ngan Le"], "title": "BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene Element Guidance", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "22 pages, 14 figures", "summary": "Text-video retrieval (TVR) systems often suffer from visual-linguistic biases\npresent in datasets, which cause pre-trained vision-language models to overlook\nkey details. To address this, we propose BiMa, a novel framework designed to\nmitigate biases in both visual and textual representations. Our approach begins\nby generating scene elements that characterize each video by identifying\nrelevant entities/objects and activities. For visual debiasing, we integrate\nthese scene elements into the video embeddings, enhancing them to emphasize\nfine-grained and salient details. For textual debiasing, we introduce a\nmechanism to disentangle text features into content and bias components,\nenabling the model to focus on meaningful content while separately handling\nbiased information. Extensive experiments and ablation studies across five\nmajor TVR benchmarks (i.e., MSR-VTT, MSVD, LSMDC, ActivityNet, and DiDeMo)\ndemonstrate the competitive performance of BiMa. Additionally, the model's bias\nmitigation capability is consistently validated by its strong results on\nout-of-distribution retrieval tasks.", "AI": {"tldr": "BiMa\u6846\u67b6\u901a\u8fc7\u751f\u6210\u573a\u666f\u5143\u7d20\u548c\u5206\u79bb\u6587\u672c\u7279\u5f81\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u6587\u672c-\u89c6\u9891\u68c0\u7d22\u7cfb\u7edf\u4e2d\u7684\u89c6\u89c9-\u8bed\u8a00\u504f\u5dee\uff0c\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6587\u672c-\u89c6\u9891\u68c0\u7d22\uff08TVR\uff09\u7cfb\u7edf\u4e2d\u7531\u6570\u636e\u96c6\u4e2d\u7684\u89c6\u89c9-\u8bed\u8a00\u504f\u5dee\u5bfc\u81f4\u7684\u5173\u952e\u7ec6\u8282\u88ab\u5ffd\u7565\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faBiMa\u6846\u67b6\uff0c\u751f\u6210\u573a\u666f\u5143\u7d20\u4ee5\u589e\u5f3a\u89c6\u9891\u5d4c\u5165\uff08\u89c6\u89c9\u53bb\u504f\uff09\uff0c\u5e76\u5c06\u6587\u672c\u7279\u5f81\u89e3\u8026\u4e3a\u5185\u5bb9\u548c\u504f\u5dee\u90e8\u5206\uff08\u6587\u672c\u53bb\u504f\uff09\u3002", "result": "\u5728\u4e94\u4e2a\u4e3b\u8981TVR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u5206\u5e03\u5916\u68c0\u7d22\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u504f\u5dee\u7f13\u89e3\u80fd\u529b\u3002", "conclusion": "BiMa\u80fd\u6709\u6548\u51cf\u8f7b\u504f\u5dee\uff0c\u63d0\u5347\u68c0\u7d22\u4efb\u52a1\u7684\u6027\u80fd\u3002", "keywords": "\u6587\u672c-\u89c6\u9891\u68c0\u7d22, \u89c6\u89c9-\u8bed\u8a00\u504f\u5dee, \u573a\u666f\u5143\u7d20, \u7279\u5f81\u89e3\u8026, BiMa"}}
{"id": "2506.04195", "pdf": "https://arxiv.org/pdf/2506.04195", "abs": "https://arxiv.org/abs/2506.04195", "authors": ["Elena Zamaraeva", "Christopher M. Collins", "George R. Darling", "Matthew S. Dyer", "Bei Peng", "Rahul Savani", "Dmytro Antypov", "Vladimir V. Gusev", "Judith Clymo", "Paul G. Spirakis", "Matthew J. Rosseinsky"], "title": "MACS: Multi-Agent Reinforcement Learning for Optimization of Crystal Structures", "categories": ["cs.LG", "cs.AI", "68T05", "I.2.6; I.2.11"], "comment": null, "summary": "Geometry optimization of atomic structures is a common and crucial task in\ncomputational chemistry and materials design. Following the learning to\noptimize paradigm, we propose a new multi-agent reinforcement learning method\ncalled Multi-Agent Crystal Structure optimization (MACS) to address periodic\ncrystal structure optimization. MACS treats geometry optimization as a\npartially observable Markov game in which atoms are agents that adjust their\npositions to collectively discover a stable configuration. We train MACS across\nvarious compositions of reported crystalline materials to obtain a policy that\nsuccessfully optimizes structures from the training compositions as well as\nstructures of larger sizes and unseen compositions, confirming its excellent\nscalability and zero-shot transferability. We benchmark our approach against a\nbroad range of state-of-the-art optimization methods and demonstrate that MACS\noptimizes periodic crystal structures significantly faster, with fewer energy\ncalculations, and the lowest failure rate.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60MACS\u65b9\u6cd5\u4f18\u5316\u6676\u4f53\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u9ad8\u6548\u7387\u5e76\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6676\u4f53\u7ed3\u6784\u4f18\u5316\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u3001\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u5c06\u6676\u4f53\u7ed3\u6784\u4f18\u5316\u89c6\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\uff0c\u539f\u5b50\u4f5c\u4e3a\u667a\u80fd\u4f53\u8c03\u6574\u4f4d\u7f6e\u4ee5\u5bfb\u627e\u7a33\u5b9a\u6784\u578b\u3002", "result": "MACS\u5728\u8bad\u7ec3\u548c\u672a\u77e5\u7ec4\u6210\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u91cf\u548c\u5931\u8d25\u7387\u663e\u8457\u4f4e\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "MACS\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6676\u4f53\u7ed3\u6784\u4f18\u5316\u65b9\u6cd5\u3002", "keywords": "\u51e0\u4f55\u4f18\u5316,\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60,\u6676\u4f53\u7ed3\u6784,\u8ba1\u7b97\u5316\u5b66"}}
{"id": "2506.04076", "pdf": "https://arxiv.org/pdf/2506.04076", "abs": "https://arxiv.org/abs/2506.04076", "authors": ["Jhen-Ke Lin", "Hao-Chien Lu", "Chung-Chun Wang", "Hong-Yun Lin", "Berlin Chen"], "title": "Acoustically Precise Hesitation Tagging Is Essential for End-to-End Verbatim Transcription Systems", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to the ISCA SLaTE-2025 Workshop", "summary": "Verbatim transcription for automatic speaking assessment demands accurate\ncapture of disfluencies, crucial for downstream tasks like error analysis and\nfeedback. However, many ASR systems discard or generalize hesitations, losing\nimportant acoustic details. We fine-tune Whisper models on the Speak & Improve\n2025 corpus using low-rank adaptation (LoRA), without recourse to external\naudio training data. We compare three annotation schemes: removing hesitations\n(Pure), generic tags (Rich), and acoustically precise fillers inferred by\nGemini 2.0 Flash from existing audio-transcript pairs (Extra). Our challenge\nsystem achieved 6.47% WER (Pure) and 5.81% WER (Extra). Post-challenge\nexperiments reveal that fine-tuning Whisper Large V3 Turbo with the \"Extra\"\nscheme yielded a 5.5% WER, an 11.3% relative improvement over the \"Pure\" scheme\n(6.2% WER). This demonstrates that explicit, realistic filled-pause labeling\nsignificantly enhances ASR accuracy for verbatim L2 speech transcription.", "AI": {"tldr": "\u901a\u8fc7\u5fae\u8c03Whisper\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u4e0d\u540c\u7684\u6807\u6ce8\u65b9\u6848\uff0c\u663e\u5f0f\u6807\u6ce8\u586b\u5145\u505c\u987f\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8L2\u8bed\u97f3\u8f6c\u5f55\u7684ASR\u51c6\u786e\u6027\u3002", "motivation": "\u81ea\u52a8\u8bed\u97f3\u8bc4\u4f30\u9700\u8981\u51c6\u786e\u6355\u6349\u4e0d\u6d41\u7545\u73b0\u8c61\uff0c\u4f46\u73b0\u6709ASR\u7cfb\u7edf\u901a\u5e38\u5ffd\u7565\u6216\u6cdb\u5316\u72b9\u8c6b\u73b0\u8c61\uff0c\u5bfc\u81f4\u91cd\u8981\u58f0\u5b66\u7ec6\u8282\u4e22\u5931\u3002", "method": "\u4f7f\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u5fae\u8c03Whisper\u6a21\u578b\uff0c\u6bd4\u8f83\u4e09\u79cd\u6807\u6ce8\u65b9\u6848\uff1a\u53bb\u9664\u72b9\u8c6b\uff08Pure\uff09\u3001\u901a\u7528\u6807\u7b7e\uff08Rich\uff09\u548c\u58f0\u5b66\u7cbe\u786e\u586b\u5145\uff08Extra\uff09\u3002", "result": "\u6700\u4f73\u65b9\u6848\uff08Extra\uff09\u4f7fWER\u964d\u4f4e\u81f35.5%\uff0c\u8f83Pure\u65b9\u6848\u63d0\u5347\u4e8611.3%\u3002", "conclusion": "\u663e\u5f0f\u4e14\u771f\u5b9e\u7684\u586b\u5145\u505c\u987f\u6807\u6ce8\u80fd\u663e\u8457\u63d0\u5347L2\u8bed\u97f3\u8f6c\u5f55\u7684ASR\u51c6\u786e\u6027\u3002", "keywords": "ASR, Whisper, LoRA, Speaking assessment, Disfluencies, WER"}}
{"id": "2506.04205", "pdf": "https://arxiv.org/pdf/2506.04205", "abs": "https://arxiv.org/abs/2506.04205", "authors": ["Jinghan Jia", "Hadi Reisizadeh", "Chongyu Fan", "Nathalie Baracaldo", "Mingyi Hong", "Sijia Liu"], "title": "EPiC: Towards Lossless Speedup for Reasoning Training through Edge-Preserving CoT Condensation", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable reasoning capabilities\nwhen trained with chain-of-thought (CoT) supervision. However, the long and\nverbose CoT traces, especially those distilled from large reasoning models\n(LRMs) such as DeepSeek-R1, significantly increase training costs during the\ndistillation process, where a non-reasoning base model is taught to replicate\nthe reasoning behavior of an LRM. In this work, we study the problem of CoT\ncondensation for resource-efficient reasoning training, aimed at pruning\nintermediate reasoning steps (i.e., thoughts) in CoT traces, enabling\nsupervised model training on length-reduced CoT data while preserving both\nanswer accuracy and the model's ability to generate coherent reasoning. Our\nrationale is that CoT traces typically follow a three-stage structure: problem\nunderstanding, exploration, and solution convergence. Through empirical\nanalysis, we find that retaining the structure of the reasoning trace,\nespecially the early stage of problem understanding (rich in reflective cues)\nand the final stage of solution convergence, is sufficient to achieve lossless\nreasoning supervision. To this end, we propose an Edge-Preserving Condensation\nmethod, EPiC, which selectively retains only the initial and final segments of\neach CoT trace while discarding the middle portion. This design draws an\nanalogy to preserving the \"edge\" of a reasoning trajectory, capturing both the\ninitial problem framing and the final answer synthesis, to maintain logical\ncontinuity. Experiments across multiple model families (Qwen and LLaMA) and\nbenchmarks show that EPiC reduces training time by over 34% while achieving\nlossless reasoning accuracy on MATH500, comparable to full CoT supervision. To\nthe best of our knowledge, this is the first study to explore thought-level CoT\ncondensation for efficient reasoning model distillation.", "AI": {"tldr": "EPiC\u65b9\u6cd5\u901a\u8fc7\u526a\u679d\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u7684\u4e2d\u95f4\u6b65\u9aa4\uff0c\u4fdd\u7559\u95ee\u9898\u7406\u89e3\u548c\u89e3\u51b3\u65b9\u6848\u6536\u655b\u9636\u6bb5\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u8bad\u7ec3\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u957f\u4e14\u5197\u957f\u7684CoT\u75d5\u8ff9\u5728\u8bad\u7ec3\u4e2d\u589e\u52a0\u6210\u672c\u7684\u95ee\u9898\uff0c\u7814\u7a76\u5982\u4f55\u5728\u4fee\u526a\u4e2d\u95f4\u6b65\u9aa4\u7684\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u51c6\u786e\u6027\u548c\u8fde\u8d2f\u6027\u3002", "method": "\u63d0\u51fa\u4e86Edge-Preserving Condensation\uff08EPiC\uff09\u65b9\u6cd5\uff0c\u4fdd\u7559CoT\u75d5\u8ff9\u7684\u521d\u59cb\u548c\u6700\u7ec8\u90e8\u5206\uff0c\u4e22\u5f03\u4e2d\u95f4\u90e8\u5206\uff0c\u4ee5\u4fdd\u6301\u903b\u8f91\u8fde\u7eed\u6027\u3002", "result": "EPiC\u5728\u591a\u4e2a\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51cf\u5c1134%\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u4e14\u63a8\u7406\u51c6\u786e\u6027\u65e0\u635f\u5931\u3002", "conclusion": "EPiC\u662f\u9996\u4e2a\u63a2\u7d22\u601d\u7ef4\u7ea7CoT\u538b\u7f29\u4ee5\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u6a21\u578b\u84b8\u998f\u7684\u7814\u7a76\u3002", "keywords": "\u94fe\u5f0f\u601d\u7ef4, \u63a8\u7406\u8bad\u7ec3, \u6a21\u578b\u84b8\u998f, \u8d44\u6e90\u6548\u7387"}}
{"id": "2506.04077", "pdf": "https://arxiv.org/pdf/2506.04077", "abs": "https://arxiv.org/abs/2506.04077", "authors": ["Chung-Chun Wang", "Jhen-Ke Lin", "Hao-Chien Lu", "Hong-Yun Lin", "Berlin Chen"], "title": "A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to the ISCA SLaTE-2025 Workshop", "summary": "Automated speaking assessment (ASA) on opinion expressions is often hampered\nby the scarcity of labeled recordings, which restricts prompt diversity and\nundermines scoring reliability. To address this challenge, we propose a novel\ntraining paradigm that leverages a large language models (LLM) to generate\ndiverse responses of a given proficiency level, converts responses into\nsynthesized speech via speaker-aware text-to-speech synthesis, and employs a\ndynamic importance loss to adaptively reweight training instances based on\nfeature distribution differences between synthesized and real speech.\nSubsequently, a multimodal large language model integrates aligned textual\nfeatures with speech signals to predict proficiency scores directly.\nExperiments conducted on the LTTC dataset show that our approach outperforms\nmethods relying on real data or conventional augmentation, effectively\nmitigating low-resource constraints and enabling ASA on opinion expressions\nwith cross-modal information.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u8bed\u97f3\u54cd\u5e94\u5e76\u901a\u8fc7\u52a8\u6001\u91cd\u8981\u6027\u635f\u5931\u548c\u8de8\u6a21\u6001\u878d\u5408\u63d0\u5347\u81ea\u52a8\u53e3\u8bed\u8bc4\u4f30\u6027\u80fd\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u53e3\u8bed\u8bc4\u4f30\u4e2d\u56e0\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u7684\u63d0\u793a\u591a\u6837\u6027\u548c\u8bc4\u5206\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u54cd\u5e94\uff0c\u901a\u8fc7\u8bed\u97f3\u5408\u6210\u6280\u672f\u751f\u6210\u8bed\u97f3\uff0c\u5e76\u7ed3\u5408\u52a8\u6001\u91cd\u8981\u6027\u635f\u5931\u548c\u8de8\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u5728LTTC\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4f9d\u8d56\u771f\u5b9e\u6570\u636e\u6216\u4f20\u7edf\u589e\u5f3a\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u4f4e\u8d44\u6e90\u9650\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u8de8\u6a21\u6001\u4fe1\u606f\u6574\u5408\u5230\u53e3\u8bed\u8bc4\u4f30\u4e2d\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "keywords": "\u81ea\u52a8\u53e3\u8bed\u8bc4\u4f30,\u5927\u8bed\u8a00\u6a21\u578b,\u8bed\u97f3\u5408\u6210,\u8de8\u6a21\u6001\u878d\u5408"}}
{"id": "2506.04206", "pdf": "https://arxiv.org/pdf/2506.04206", "abs": "https://arxiv.org/abs/2506.04206", "authors": ["Reza Ramezanpour", "Victor M. Tenorio", "Antonio G. Marques", "Ashutosh Sabharwal", "Santiago Segarra"], "title": "A Few Moments Please: Scalable Graphon Learning via Moment Matching", "categories": ["cs.LG"], "comment": null, "summary": "Graphons, as limit objects of dense graph sequences, play a central role in\nthe statistical analysis of network data. However, existing graphon estimation\nmethods often struggle with scalability to large networks and\nresolution-independent approximation, due to their reliance on estimating\nlatent variables or costly metrics such as the Gromov-Wasserstein distance. In\nthis work, we propose a novel, scalable graphon estimator that directly\nrecovers the graphon via moment matching, leveraging implicit neural\nrepresentations (INRs). Our approach avoids latent variable modeling by\ntraining an INR--mapping coordinates to graphon values--to match empirical\nsubgraph counts (i.e., moments) from observed graphs. This direct estimation\nmechanism yields a polynomial-time solution and crucially sidesteps the\ncombinatorial complexity of Gromov-Wasserstein optimization. Building on\nfoundational results, we establish a theoretical guarantee: when the observed\nsubgraph motifs sufficiently represent those of the true graphon (a condition\nmet with sufficiently large or numerous graph samples), the estimated graphon\nachieves a provable upper bound in cut distance from the ground truth.\nAdditionally, we introduce MomentMixup, a data augmentation technique that\nperforms mixup in the moment space to enhance graphon-based learning. Our\ngraphon estimation method achieves strong empirical performance--demonstrating\nhigh accuracy on small graphs and superior computational efficiency on large\ngraphs--outperforming state-of-the-art scalable estimators in 75\\% of benchmark\nsettings and matching them in the remaining cases. Furthermore, MomentMixup\ndemonstrated improved graph classification accuracy on the majority of our\nbenchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INRs\uff09\u7684\u56fe\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e9\u5339\u914d\u76f4\u63a5\u6062\u590d\u56fe\uff0c\u907f\u514d\u4e86\u6f5c\u5728\u53d8\u91cf\u5efa\u6a21\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7684Gromov-Wasserstein\u8ddd\u79bb\uff0c\u4e14\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u56fe\u4f30\u8ba1\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u7f51\u7edc\u4e2d\u96be\u4ee5\u6269\u5c55\u4e14\u7f3a\u4e4f\u5206\u8fa8\u7387\u65e0\u5173\u7684\u8fd1\u4f3c\u80fd\u529b\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u6f5c\u5728\u53d8\u91cf\u4f30\u8ba1\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u5ea6\u91cf\uff08\u5982Gromov-Wasserstein\u8ddd\u79bb\uff09\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INRs\uff09\u76f4\u63a5\u5339\u914d\u89c2\u5bdf\u56fe\u4e0a\u7684\u5b50\u56fe\u8ba1\u6570\uff08\u77e9\uff09\uff0c\u4ece\u800c\u6062\u590d\u56fe\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002\u5e76\u5f15\u5165\u6570\u636e\u589e\u5f3a\u6280\u672fMomentMixup\uff0c\u5728\u77e9\u7a7a\u95f4\u4e2d\u8fdb\u884cmixup\u4ee5\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "result": "\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u7ed3\u679c\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff1a\u5bf9\u4e8e\u8db3\u591f\u5927\u7684\u56fe\u6837\u672c\uff0c\u4f30\u8ba1\u56fe\u4e0e\u771f\u5b9e\u56fe\u5728\u5207\u5272\u8ddd\u79bb\u4e0a\u6709\u53ef\u8bc1\u660e\u7684\u4e0a\u754c\uff1b\u572875%\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u53ef\u6269\u5c55\u4f30\u8ba1\u5668\uff0c\u5176\u4f59\u60c5\u51b5\u4e0b\u6027\u80fd\u76f8\u5f53\u3002MomentMixup\u4e5f\u5728\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u4e86\u56fe\u5206\u7c7b\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u77e9\u5339\u914d\u548c\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u56fe\u4f30\u8ba1\uff0c\u540c\u65f6MomentMixup\u6280\u672f\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u57fa\u4e8e\u56fe\u7684\u5b66\u4e60\u6548\u679c\u3002", "keywords": "\u56fe\u4f30\u8ba1\u3001\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u3001\u77e9\u5339\u914d\u3001MomentMixup\u3001\u6570\u636e\u589e\u5f3a"}}
{"id": "2506.04078", "pdf": "https://arxiv.org/pdf/2506.04078", "abs": "https://arxiv.org/abs/2506.04078", "authors": ["Ming Zhang", "Yujiong Shen", "Zelin Li", "Huayu Sha", "Binze Hu", "Yuhui Wang", "Chenhao Huang", "Shichun Liu", "Jingqi Tong", "Changhao Jiang", "Mingxu Chai", "Zhiheng Xi", "Shihan Dou", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating large language models (LLMs) in medicine is crucial because\nmedical applications require high accuracy with little room for error. Current\nmedical benchmarks have three main types: medical exam-based, comprehensive\nmedical, and specialized assessments. However, these benchmarks have\nlimitations in question design (mostly multiple-choice), data sources (often\nnot derived from real clinical scenarios), and evaluation methods (poor\nassessment of complex reasoning). To address these issues, we present\nLLMEval-Med, a new benchmark covering five core medical areas, including 2,996\nquestions created from real-world electronic health records and expert-designed\nclinical scenarios. We also design an automated evaluation pipeline,\nincorporating expert-developed checklists into our LLM-as-Judge framework.\nFurthermore, our methodology validates machine scoring through human-machine\nagreement analysis, dynamically refining checklists and prompts based on expert\nfeedback to ensure reliability. We evaluate 13 LLMs across three categories\n(specialized medical models, open-source models, and closed-source models) on\nLLMEval-Med, providing valuable insights for the safe and effective deployment\nof LLMs in medical domains. The dataset is released in\nhttps://github.com/llmeval/LLMEval-Med.", "AI": {"tldr": "LLMEval-Med \u662f\u4e00\u4e2a\u65b0\u7684\u533b\u5b66\u8bc4\u4f30\u57fa\u51c6\uff0c\u6db5\u76d6\u4e94\u4e2a\u6838\u5fc3\u533b\u7597\u9886\u57df\uff0c\u5305\u542b 2,996 \u4e2a\u6765\u81ea\u771f\u5b9e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u548c\u4e13\u5bb6\u8bbe\u8ba1\u7684\u4e34\u5e8a\u573a\u666f\u7684\u9898\u76ee\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u533b\u5b66\u57fa\u51c6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u57fa\u51c6\u5728\u8bbe\u8ba1\u95ee\u9898\u7c7b\u578b\u3001\u6570\u636e\u6765\u6e90\u548c\u8bc4\u4f30\u65b9\u6cd5\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u533b\u7597\u5e94\u7528\u5bf9\u9ad8\u51c6\u786e\u6027\u548c\u590d\u6742\u63a8\u7406\u8bc4\u4f30\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u521b\u5efa LLMEval-Med \u57fa\u51c6\uff0c\u8bbe\u8ba1\u81ea\u52a8\u5316\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5e76\u5c06\u4e13\u5bb6\u5f00\u53d1\u7684\u68c0\u67e5\u8868\u878d\u5165 LLM-as-Judge \u6846\u67b6\uff0c\u540c\u65f6\u901a\u8fc7\u4eba\u673a\u4e00\u81f4\u6027\u5206\u6790\u9a8c\u8bc1\u673a\u5668\u8bc4\u5206\u3002", "result": "\u8bc4\u4f30\u4e86 13 \u79cd LLM\uff0c\u5305\u62ec\u4e13\u4e1a\u533b\u7597\u6a21\u578b\u3001\u5f00\u6e90\u6a21\u578b\u548c\u95ed\u6e90\u6a21\u578b\uff0c\u4e3a\u533b\u7597\u9886\u57df LLM \u7684\u5b89\u5168\u6709\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "LLMEval-Med \u901a\u8fc7\u52a8\u6001\u4f18\u5316\u68c0\u67e5\u8868\u548c\u63d0\u793a\uff0c\u63d0\u9ad8\u4e86\u8bc4\u4f30\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u533b\u5b66 LLM \u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002", "keywords": "\u533b\u5b66\u8bc4\u4f30\u3001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u57fa\u51c6\u6d4b\u8bd5\u3001\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u3001LLMEval-Med"}}
{"id": "2506.04207", "pdf": "https://arxiv.org/pdf/2506.04207", "abs": "https://arxiv.org/abs/2506.04207", "authors": ["Shuang Chen", "Yue Guo", "Zhaochen Su", "Yafu Li", "Yulun Wu", "Jiacheng Chen", "Jiayu Chen", "Weijie Wang", "Xiaoye Qu", "Yu Cheng"], "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "19 pages, 6 figures", "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.", "AI": {"tldr": "ReVisual-R1\u6539\u8fdbMLLM\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u8bad\u7ec3\u548c\u4f18\u5316RL\uff0c\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MLLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u6539\u8fdb\u8bad\u7ec3\u6d41\u7a0b\uff08\u5982\u521d\u59cb\u5316\u3001RL\u4f18\u5316\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\uff09\u63d0\u5347\u5176\u63a8\u7406\u80fd\u529b\u3002", "method": "1) \u4f7f\u7528\u7cbe\u9009\u6587\u672c\u6570\u636e\u521d\u59cb\u5316\uff1b2) \u8bc6\u522b\u5e76\u89e3\u51b3GRPO\u68af\u5ea6\u505c\u6ede\u95ee\u9898\uff1b3) \u5f15\u5165\u5206\u9636\u6bb5\u8bad\u7ec3\uff08\u5148\u591a\u6a21\u6001RL\uff0c\u540e\u7eaf\u6587\u672cRL\uff09\u3002", "result": "ReVisual-R1\u5728\u591a\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u5982MathVerse\u3001AIME2024\uff09\u8fbe\u5230\u5f00\u6e907B MLLM\u7684SOTA\u6027\u80fd\u3002", "conclusion": "\u5206\u9636\u6bb5\u8bad\u7ec3\u548c\u4f18\u5316RL\u80fd\u663e\u8457\u63d0\u5347MLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e73\u8861\u611f\u77e5\u4e0e\u8ba4\u77e5\u53d1\u5c55\u3002", "keywords": "MLLM, \u5f3a\u5316\u5b66\u4e60, \u591a\u6a21\u6001\u63a8\u7406, \u5206\u9636\u6bb5\u8bad\u7ec3, ReVisual-R1"}}
{"id": "2506.04079", "pdf": "https://arxiv.org/pdf/2506.04079", "abs": "https://arxiv.org/abs/2506.04079", "authors": ["Pedro Henrique Martins", "Jo\u00e3o Alves", "Patrick Fernandes", "Nuno M. Guerreiro", "Ricardo Rei", "Amin Farajian", "Mateusz Klimaszewski", "Duarte M. Alves", "Jos\u00e9 Pombal", "Manuel Faysse", "Pierre Colombo", "Fran\u00e7ois Yvon", "Barry Haddow", "Jos\u00e9 G. C. de Souza", "Alexandra Birch", "Andr\u00e9 F. T. Martins"], "title": "EuroLLM-9B: Technical Report", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "56 pages", "summary": "This report presents EuroLLM-9B, a large language model trained from scratch\nto support the needs of European citizens by covering all 24 official European\nUnion languages and 11 additional languages. EuroLLM addresses the issue of\nEuropean languages being underrepresented and underserved in existing open\nlarge language models. We provide a comprehensive overview of EuroLLM-9B's\ndevelopment, including tokenizer design, architectural specifications, data\nfiltering, and training procedures. We describe the pre-training data\ncollection and filtering pipeline, including the creation of EuroFilter, an\nAI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a\nnovel synthetic dataset for post-training that enhances language coverage for\nEuropean languages. Evaluation results demonstrate EuroLLM-9B's competitive\nperformance on multilingual benchmarks and machine translation tasks,\nestablishing it as the leading open European-made LLM of its size. To support\nopen research and adoption, we release all major components of this work,\nincluding the base and instruction-tuned models, the EuroFilter classifier, and\nthe synthetic post-training dataset.", "AI": {"tldr": "EuroLLM-9B \u662f\u4e00\u4e2a\u652f\u630124\u79cd\u6b27\u76df\u5b98\u65b9\u8bed\u8a00\u548c11\u79cd\u989d\u5916\u8bed\u8a00\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6b27\u6d32\u8bed\u8a00\u5728\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u89e3\u51b3\u6b27\u6d32\u8bed\u8a00\u5728\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4ee3\u8868\u4e0d\u8db3\u548c\u670d\u52a1\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u5305\u62ec\u5206\u8bcd\u5668\u8bbe\u8ba1\u3001\u67b6\u6784\u89c4\u8303\u3001\u6570\u636e\u8fc7\u6ee4\u3001\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4ee5\u53ca\u9884\u5904\u7406\u6570\u636e\u6536\u96c6\u548c\u8fc7\u6ee4\u3002", "result": "\u5728\u591a\u79cd\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u548c\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u4e3a\u540c\u7c7b\u6700\u4f73\u5f00\u6e90\u6b27\u6d32LLM\u3002", "conclusion": "EuroLLM-9B \u662f\u4e00\u4e2a\u5168\u9762\u652f\u6301\u6b27\u6d32\u591a\u8bed\u8a00\u9700\u6c42\u7684\u9886\u5148\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u516c\u5f00\u4e86\u6240\u6709\u4e3b\u8981\u7ec4\u4ef6\u3002", "keywords": "EuroLLM-9B, \u591a\u8bed\u8a00\u6a21\u578b, \u6b27\u6d32\u8bed\u8a00, \u5f00\u6e90"}}
{"id": "2506.03606", "pdf": "https://arxiv.org/pdf/2506.03606", "abs": "https://arxiv.org/abs/2506.03606", "authors": ["Parismita Gogoi", "Sishir Kalita", "Wendy Lalhminghlui", "Viyazonuo Terhiija", "Moakala Tzudir", "Priyankoo Sarmah", "S. R. M. Prasanna"], "title": "Tone recognition in low-resource languages of North-East India: peeling the layers of SSL-based speech models", "categories": ["eess.AS", "cs.AI", "cs.CL", "eess.SP"], "comment": "Accepted in Interspeech2025", "summary": "This study explores the use of self-supervised learning (SSL) models for tone\nrecognition in three low-resource languages from North Eastern India: Angami,\nAo, and Mizo. We evaluate four Wav2vec2.0 base models that were pre-trained on\nboth tonal and non-tonal languages. We analyze tone-wise performance across the\nlayers for all three languages and compare the different models. Our results\nshow that tone recognition works best for Mizo and worst for Angami. The middle\nlayers of the SSL models are the most important for tone recognition,\nregardless of the pre-training language, i.e. tonal or non-tonal. We have also\nfound that the tone inventory, tone types, and dialectal variations affect tone\nrecognition. These findings provide useful insights into the strengths and\nweaknesses of SSL-based embeddings for tonal languages and highlight the\npotential for improving tone recognition in low-resource settings. The source\ncode is available at GitHub 1 .", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5728\u4e09\u79cd\u4e1c\u5317\u5370\u5ea6\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08Angami\u3001Ao\u3001Mizo\uff09\u4e2d\u7684\u58f0\u8c03\u8bc6\u522b\u6548\u679c\uff0c\u53d1\u73b0Mizo\u6548\u679c\u6700\u597d\uff0cAngami\u6700\u5dee\uff0c\u4e14\u4e2d\u95f4\u5c42\u5bf9\u58f0\u8c03\u8bc6\u522b\u6700\u6709\u6548\u3002", "motivation": "\u63a2\u8ba8\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u58f0\u8c03\u8bc6\u522b\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u4e0d\u540c\u58f0\u8c03\u7c7b\u578b\u548c\u65b9\u8a00\u5dee\u5f02\u7684\u5f71\u54cd\u3002", "method": "\u8bc4\u4f30\u4e86\u56db\u79cd\u57fa\u4e8eWav2vec2.0\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5206\u6790\u4e86\u6a21\u578b\u5c42\u6b21\u5bf9\u58f0\u8c03\u8bc6\u522b\u7684\u5f71\u54cd\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u8bed\u8a00\u7684\u6027\u80fd\u3002", "result": "\u58f0\u8c03\u8bc6\u522b\u5728Mizo\u4e2d\u8868\u73b0\u6700\u4f73\uff0cAngami\u6700\u5dee\uff1b\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u5bf9\u58f0\u8c03\u8bc6\u522b\u6700\u5173\u952e\uff1b\u58f0\u8c03\u79cd\u7c7b\u548c\u65b9\u8a00\u5dee\u5f02\u5f71\u54cd\u8bc6\u522b\u6548\u679c\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u58f0\u8c03\u8bc6\u522b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u53d7\u58f0\u8c03\u79cd\u7c7b\u548c\u65b9\u8a00\u5f71\u54cd\u3002", "keywords": "\u81ea\u76d1\u7763\u5b66\u4e60, \u58f0\u8c03\u8bc6\u522b, \u4f4e\u8d44\u6e90\u8bed\u8a00, Wav2vec2.0, \u5370\u5ea6\u8bed\u8a00"}}
{"id": "2506.03152", "pdf": "https://arxiv.org/pdf/2506.03152", "abs": "https://arxiv.org/abs/2506.03152", "authors": ["Robert Bayer", "Julian Priest", "Daniel Kjellberg", "Jeppe Lindhard", "Nikolaj S\u00f8renesen", "Nicolaj Valsted", "\u00cdvar \u00d3li", "P\u0131nar T\u00f6z\u00fcn"], "title": "Adaptive and Robust Image Processing on CubeSats", "categories": ["eess.IV", "cs.CV", "cs.DC", "cs.LG"], "comment": null, "summary": "CubeSats offer a low-cost platform for space research, particularly for Earth\nobservation. However, their resource-constrained nature and being in space,\nchallenge the flexibility and complexity of the deployed image processing\npipelines and their orchestration. This paper introduces two novel systems,\nDIPP and DISH, to address these challenges. DIPP is a modular and configurable\nimage processing pipeline framework that allows for adaptability to changing\nmission goals even after deployment, while preserving robustness. DISH is a\ndomain-specific language (DSL) and runtime system designed to schedule complex\nimaging workloads on low-power and memory-constrained processors.\n  Our experiments demonstrate that DIPP's decomposition of the processing\npipelines adds negligible overhead, while significantly reducing the network\nrequirements of updating pipelines and being robust against erroneous module\nuploads. Furthermore, we compare DISH to Lua, a general purpose scripting\nlanguage, and demonstrate its comparable expressiveness and lower memory\nrequirement.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e24\u79cd\u65b0\u7cfb\u7edfDIPP\u548cDISH\uff0c\u7528\u4e8e\u89e3\u51b3CubeSat\u5728\u7a7a\u95f4\u4e2d\u7684\u56fe\u50cf\u5904\u7406\u7ba1\u9053\u7075\u6d3b\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\u5b83\u4eec\u5728\u51cf\u5c11\u7f51\u7edc\u9700\u6c42\u548c\u5185\u5b58\u5360\u7528\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "CubeSat\u5728\u7a7a\u95f4\u7814\u7a76\u4e2d\u63d0\u4f9b\u4f4e\u6210\u672c\u5e73\u53f0\uff0c\u4f46\u5176\u8d44\u6e90\u53d7\u9650\u7684\u7279\u6027\u9650\u5236\u4e86\u56fe\u50cf\u5904\u7406\u7ba1\u9053\u7684\u7075\u6d3b\u6027\u548c\u590d\u6742\u6027\u3002\u4e3a\u4e86\u9002\u5e94\u53d8\u5316\u7684\u4efb\u52a1\u76ee\u6807\uff0c\u5373\u4f7f\u5728\u90e8\u7f72\u540e\u4e5f\u80fd\u4fdd\u6301\u7a33\u5065\u6027\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u7cfb\u7edf\uff1aDIPP\uff08\u6a21\u5757\u5316\u548c\u53ef\u914d\u7f6e\u7684\u56fe\u50cf\u5904\u7406\u7ba1\u9053\u6846\u67b6\uff09\u548cDISH\uff08\u7279\u5b9a\u9886\u57df\u8bed\u8a00\u548c\u8fd0\u884c\u65f6\u7cfb\u7edf\uff09\uff0c\u7528\u4e8e\u8c03\u5ea6\u590d\u6742\u6210\u50cf\u4efb\u52a1\u4e8e\u4f4e\u529f\u8017\u548c\u5185\u5b58\u53d7\u9650\u7684\u5904\u7406\u5668\u4e0a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDIPP\u5728\u5206\u89e3\u5904\u7406\u7ba1\u9053\u65f6\u589e\u52a0\u7684\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u66f4\u65b0\u7ba1\u9053\u6240\u9700\u7684\u7f51\u7edc\u8d44\u6e90\uff0c\u5e76\u80fd\u62b5\u5fa1\u9519\u8bef\u6a21\u5757\u4e0a\u4f20\u3002DISH\u4e0e\u901a\u7528\u811a\u672c\u8bed\u8a00Lua\u76f8\u6bd4\uff0c\u8868\u8fbe\u529b\u76f8\u5f53\u4e14\u5185\u5b58\u9700\u6c42\u66f4\u4f4e\u3002", "conclusion": "DIPP\u548cDISH\u4e3aCubeSat\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u56fe\u50cf\u5904\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u3002", "keywords": "CubeSat, \u56fe\u50cf\u5904\u7406\u7ba1\u9053, DIPP, DISH, \u7279\u5b9a\u9886\u57df\u8bed\u8a00, \u4f4e\u529f\u8017\u5904\u7406\u5668"}}
{"id": "2506.04098", "pdf": "https://arxiv.org/pdf/2506.04098", "abs": "https://arxiv.org/abs/2506.04098", "authors": ["Wenhao Li", "Wenwu Li", "Chuyun Shen", "Junjie Sheng", "Zixiao Huang", "Di Wu", "Yun Hua", "Wei Yin", "Xiangfeng Wang", "Hongyuan Zha", "Bo Jin"], "title": "TextAtari: 100K Frames Game Playing with Language Agents", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "51 pages, 39 figures", "summary": "We present TextAtari, a benchmark for evaluating language agents on very\nlong-horizon decision-making tasks spanning up to 100,000 steps. By translating\nthe visual state representations of classic Atari games into rich textual\ndescriptions, TextAtari creates a challenging test bed that bridges sequential\ndecision-making with natural language processing. The benchmark includes nearly\n100 distinct tasks with varying complexity, action spaces, and planning\nhorizons, all rendered as text through an unsupervised representation learning\nframework (AtariARI). We evaluate three open-source large language models\n(Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks\n(zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how\ndifferent forms of prior knowledge affect performance on these long-horizon\nchallenges. Four scenarios-Basic, Obscured, Manual Augmentation, and\nReference-based-investigate the impact of semantic understanding, instruction\ncomprehension, and expert demonstrations on agent decision-making. Our results\nreveal significant performance gaps between language agents and human players\nin extensive planning tasks, highlighting challenges in sequential reasoning,\nstate tracking, and strategic planning across tens of thousands of steps.\nTextAtari provides standardized evaluation protocols, baseline implementations,\nand a framework for advancing research at the intersection of language models\nand planning.", "AI": {"tldr": "TextAtari\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u4ee3\u7406\u5728\u957f\u65f6\u7a0b\u51b3\u7b56\u4efb\u52a1\uff08\u957f\u8fbe10\u4e07\u6b65\uff09\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5c06\u7ecf\u5178Atari\u6e38\u620f\u7684\u89c6\u89c9\u72b6\u6001\u8f6c\u6362\u4e3a\u6587\u672c\u63cf\u8ff0\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6311\u6218\u6027\u7684\u6d4b\u8bd5\u73af\u5883\u3002", "motivation": "\u901a\u8fc7\u5c06\u89c6\u89c9\u4efb\u52a1\u8f6c\u5316\u4e3a\u6587\u672c\u4efb\u52a1\uff0c\u7814\u7a76\u8bed\u8a00\u4ee3\u7406\u5728\u590d\u6742\u51b3\u7b56\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528AtariARI\u6846\u67b6\u751f\u6210100\u4e2a\u4efb\u52a1\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u8bc4\u4f30\u4e09\u79cd\u8bed\u8a00\u6a21\u578b\uff08Qwen2.5-7B\u3001Gemma-7B\u3001Llama3.1-8B\uff09\u5728\u4e09\u79cd\u4ee3\u7406\u6846\u67b6\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u94fe\u5f0f\u601d\u8003\u3001\u53cd\u601d\u63a8\u7406\uff09\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u8bed\u8a00\u4ee3\u7406\u4e0e\u4eba\u7c7b\u73a9\u5bb6\u5728\u957f\u65f6\u7a0b\u89c4\u5212\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u63ed\u793a\u4e86\u5728\u987a\u5e8f\u63a8\u7406\u3001\u72b6\u6001\u8ddf\u8e2a\u548c\u6218\u7565\u89c4\u5212\u65b9\u9762\u7684\u6311\u6218\u3002", "conclusion": "TextAtari\u4e3a\u8bed\u8a00\u6a21\u578b\u4e0e\u89c4\u5212\u7814\u7a76\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u6d4b\u5de5\u5177\u548c\u57fa\u51c6\u6846\u67b6\u3002", "keywords": "TextAtari, \u8bed\u8a00\u4ee3\u7406, Atari\u6e38\u620f, \u957f\u65f6\u7a0b\u51b3\u7b56, \u81ea\u7136\u8bed\u8a00\u5904\u7406"}}
{"id": "2506.03614", "pdf": "https://arxiv.org/pdf/2506.03614", "abs": "https://arxiv.org/abs/2506.03614", "authors": ["Zhanhui Zhou", "Lingjie Chen", "Chao Yang", "Chaochao Lu"], "title": "VLMs Can Aggregate Scattered Training Patches", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "One way to mitigate risks in vision-language models (VLMs) is to remove\ndangerous samples in their training data. However, such data moderation can be\neasily bypassed when harmful images are split into small, benign-looking\npatches, scattered across many training samples. VLMs may then learn to piece\nthese fragments together during training and generate harmful responses at\ninference, either from full images or text references. For instance, if trained\non image patches from a bloody scene paired with the descriptions \"safe,\" VLMs\nmay later describe, the full image or a text reference to the scene, as \"safe.\"\nWe define the core ability of VLMs enabling this attack as $\\textit{visual\nstitching}$ -- the ability to integrate visual information spread across\nmultiple training samples that share the same textual descriptions. In our\nwork, we first demonstrate visual stitching abilities in common open-source\nVLMs on three datasets where each image is labeled with a unique synthetic ID:\nwe split each $(\\texttt{image}, \\texttt{ID})$ pair into $\\{(\\texttt{patch},\n\\texttt{ID})\\}$ pairs at different granularity for finetuning, and we find that\ntuned models can verbalize the correct IDs from full images or text reference.\nBuilding on this, we simulate the adversarial data poisoning scenario mentioned\nabove by using patches from dangerous images and replacing IDs with text\ndescriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can\nevade moderation in patches and later be reconstructed through visual\nstitching, posing serious VLM safety risks. Code is available at\nhttps://github.com/ZHZisZZ/visual-stitching.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u901a\u8fc7\u89c6\u89c9\u62fc\u63a5\u80fd\u529b\u4ece\u5206\u6563\u7684\u826f\u6027\u56fe\u50cf\u7247\u6bb5\u4e2d\u91cd\u5efa\u6709\u5bb3\u5185\u5bb9\u7684\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u63ed\u793a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u5206\u5272\u8bad\u7ec3\u60c5\u51b5\u4e0b\u53ef\u80fd\u7ed5\u8fc7\u5185\u5bb9\u5ba1\u6838\uff0c\u4ece\u800c\u751f\u6210\u6709\u5bb3\u54cd\u5e94\u7684\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u5c06\u56fe\u50cf\u5206\u5272\u4e3a\u591a\u4e2a\u5c0f\u7247\u6bb5\u5e76\u5173\u8054\u76f8\u540c\u6587\u672c\u63cf\u8ff0\uff0c\u6a21\u62df\u89c6\u89c9\u62fc\u63a5\u653b\u51fb\uff0c\u6d4b\u8bd5\u6a21\u578b\u5bf9\u5b8c\u6574\u56fe\u50cf\u6216\u6587\u672c\u7684\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eVLM\u5177\u5907\u89c6\u89c9\u62fc\u63a5\u80fd\u529b\uff0c\u80fd\u591f\u4ece\u5206\u6563\u7247\u6bb5\u4e2d\u91cd\u5efa\u6709\u5bb3\u5185\u5bb9\uff0c\u7ed5\u8fc7\u6570\u636e\u5ba1\u6838\u3002", "conclusion": "\u89c6\u89c9\u62fc\u63a5\u80fd\u529b\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u5b89\u5168\u98ce\u9669\uff0c\u6570\u636e\u5ba1\u6838\u9700\u66f4\u7cbe\u7ec6\u4ee5\u907f\u514d\u6b64\u7c7b\u653b\u51fb\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u6570\u636e\u5b89\u5168, \u89c6\u89c9\u62fc\u63a5, \u5bf9\u6297\u653b\u51fb, \u5185\u5bb9\u5ba1\u6838"}}
{"id": "2506.03153", "pdf": "https://arxiv.org/pdf/2506.03153", "abs": "https://arxiv.org/abs/2506.03153", "authors": ["Junzhe Jiang", "Chang Yang", "Xinrun Wang", "Bo Li"], "title": "Why Regression? Binary Encoding Classification Brings Confidence to Stock Market Index Price Prediction", "categories": ["q-fin.ST", "cs.LG"], "comment": null, "summary": "Stock market indices serve as fundamental market measurement that quantify\nsystematic market dynamics. However, accurate index price prediction remains\nchallenging, primarily because existing approaches treat indices as isolated\ntime series and frame the prediction as a simple regression task. These methods\nfail to capture indices' inherent nature as aggregations of constituent stocks\nwith complex, time-varying interdependencies. To address these limitations, we\npropose Cubic, a novel end-to-end framework that explicitly models the adaptive\nfusion of constituent stocks for index price prediction. Our main contributions\nare threefold. i) Fusion in the latent space: we introduce the fusion mechanism\nover the latent embedding of the stocks to extract the information from the\nvast number of stocks. ii) Binary encoding classification: since regression\ntasks are challenging due to continuous value estimation, we reformulate the\nregression into the classification task, where the target value is converted to\nbinary and we optimize the prediction of the value of each digit with\ncross-entropy loss. iii) Confidence-guided prediction and trading: we introduce\nthe regularization loss to address market prediction uncertainty for the index\nprediction and design the rule-based trading policies based on the confidence.\nExtensive experiments across multiple stock markets and indices demonstrate\nthat Cubic consistently outperforms state-of-the-art baselines in stock index\nprediction tasks, achieving superior performance on both forecasting accuracy\nmetrics and downstream trading profitability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aCubic\u7684\u65b0\u578b\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6210\u5206\u80a1\u7684\u81ea\u9002\u5e94\u878d\u5408\u6765\u9884\u6d4b\u6307\u6570\u4ef7\u683c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5c06\u6307\u6570\u89c6\u4e3a\u5b64\u7acb\u65f6\u95f4\u5e8f\u5217\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u80a1\u7968\u6307\u6570\u89c6\u4e3a\u5b64\u7acb\u65f6\u95f4\u5e8f\u5217\uff0c\u5ffd\u7565\u4e86\u5176\u4f5c\u4e3a\u6210\u5206\u80a1\u7684\u590d\u6742\u65f6\u53d8\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0d\u591f\u51c6\u786e\u3002", "method": "1) \u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u6210\u5206\u80a1\u7684\u878d\u5408\uff1b2) \u5c06\u56de\u5f52\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e8c\u503c\u7f16\u7801\u5206\u7c7b\u4efb\u52a1\uff1b3) \u5f15\u5165\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u9884\u6d4b\u548c\u4ea4\u6613\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u80a1\u7968\u5e02\u573a\u548c\u6307\u6570\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCubic\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u4ea4\u6613\u76c8\u5229\u80fd\u529b\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Cubic\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6210\u5206\u80a1\u7684\u878d\u5408\u548c\u5f15\u5165\u5206\u7c7b\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80a1\u7968\u6307\u6570\u7684\u9884\u6d4b\u6027\u80fd\u3002", "keywords": "\u80a1\u7968\u6307\u6570\u9884\u6d4b, \u6210\u5206\u80a1\u878d\u5408, \u4e8c\u503c\u7f16\u7801\u5206\u7c7b, \u7f6e\u4fe1\u5ea6\u4ea4\u6613"}}
{"id": "2506.04108", "pdf": "https://arxiv.org/pdf/2506.04108", "abs": "https://arxiv.org/abs/2506.04108", "authors": ["Yutao Sun", "Tianzhu Ye", "Li Dong", "Yuqing Xia", "Jian Chen", "Yizhao Gao", "Shijie Cao", "Jianyong Wang", "Furu Wei"], "title": "Rectified Sparse Attention", "categories": ["cs.CL"], "comment": null, "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.", "AI": {"tldr": "ReSA\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5757\u7a00\u758f\u6ce8\u610f\u529b\u548c\u5468\u671f\u6027\u5bc6\u96c6\u6821\u6b63\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86KV\u7f13\u5b58\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u957f\u5e8f\u5217\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u957f\u5e8f\u5217\u751f\u6210\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u7a00\u758f\u89e3\u7801\u65b9\u6cd5\u56e0KV\u7f13\u5b58\u5bf9\u9f50\u95ee\u9898\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86Rectified Sparse Attention (ReSA)\uff0c\u7ed3\u5408\u5757\u7a00\u758f\u6ce8\u610f\u529b\u548c\u5468\u671f\u6027\u5bc6\u96c6\u6821\u6b63\uff0c\u5b9a\u671f\u901a\u8fc7\u5bc6\u96c6\u524d\u5411\u4f20\u9012\u5237\u65b0KV\u7f13\u5b58\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReSA\u5728\u6570\u5b66\u63a8\u7406\u3001\u8bed\u8a00\u5efa\u6a21\u548c\u68c0\u7d22\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u65e0\u635f\u7684\u751f\u6210\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\uff0c\u5728256K\u5e8f\u5217\u957f\u5ea6\u4e0b\u5b9e\u73b0\u4e86\u6700\u9ad82.42\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\u3002", "conclusion": "ReSA\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u957f\u5e8f\u5217\u751f\u6210, KV\u7f13\u5b58\u5bf9\u9f50, \u5757\u7a00\u758f\u6ce8\u610f\u529b, ReSA, \u6548\u7387\u63d0\u5347"}}
{"id": "2506.03157", "pdf": "https://arxiv.org/pdf/2506.03157", "abs": "https://arxiv.org/abs/2506.03157", "authors": ["Ziyang Yu", "Wenbing Huang", "Yang Liu"], "title": "UniSim: A Unified Simulator for Time-Coarsened Dynamics of Biomolecules", "categories": ["q-bio.BM", "cs.LG"], "comment": "ICML 2025 poster", "summary": "Molecular Dynamics (MD) simulations are essential for understanding the\natomic-level behavior of molecular systems, giving insights into their\ntransitions and interactions. However, classical MD techniques are limited by\nthe trade-off between accuracy and efficiency, while recent deep learning-based\nimprovements have mostly focused on single-domain molecules, lacking\ntransferability to unfamiliar molecular systems. Therefore, we propose\n\\textbf{Uni}fied \\textbf{Sim}ulator (UniSim), which leverages cross-domain\nknowledge to enhance the understanding of atomic interactions. First, we employ\na multi-head pretraining approach to learn a unified atomic representation\nmodel from a large and diverse set of molecular data. Then, based on the\nstochastic interpolant framework, we learn the state transition patterns over\nlong timesteps from MD trajectories, and introduce a force guidance module for\nrapidly adapting to different chemical environments. Our experiments\ndemonstrate that UniSim achieves highly competitive performance across small\nmolecules, peptides, and proteins.", "AI": {"tldr": "UniSim\u901a\u8fc7\u8de8\u9886\u57df\u77e5\u8bc6\u63d0\u5347\u539f\u5b50\u4ea4\u4e92\u7406\u89e3\uff0c\u7ed3\u5408\u591a\u5934\u9884\u8bad\u7ec3\u548c\u968f\u673a\u63d2\u503c\u6846\u67b6\uff0c\u5728\u5c0f\u5206\u5b50\u3001\u80bd\u548c\u86cb\u767d\u8d28\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7ecf\u5178\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u95f4\u5b58\u5728\u5c40\u9650\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u8de8\u9886\u57df\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528\u591a\u5934\u9884\u8bad\u7ec3\u5b66\u4e60\u7edf\u4e00\u539f\u5b50\u8868\u5f81\uff0c\u5e76\u57fa\u4e8e\u968f\u673a\u63d2\u503c\u6846\u67b6\u5b66\u4e60\u72b6\u6001\u8f6c\u79fb\u6a21\u5f0f\uff0c\u52a0\u5165\u529b\u5f15\u5bfc\u6a21\u5757\u9002\u5e94\u4e0d\u540c\u5316\u5b66\u73af\u5883\u3002", "result": "\u5728\u591a\u79cd\u5206\u5b50\u7cfb\u7edf\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "UniSim\u901a\u8fc7\u7edf\u4e00\u8868\u5f81\u548c\u529b\u5f15\u5bfc\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u6a21\u62df\u7684\u8de8\u9886\u57df\u9002\u5e94\u6027\u95ee\u9898\u3002", "keywords": "\u5206\u5b50\u52a8\u529b\u5b66, \u6df1\u5ea6\u5b66\u4e60, \u8de8\u9886\u57df, \u7edf\u4e00\u8868\u5f81, UniSim"}}
{"id": "2506.04131", "pdf": "https://arxiv.org/pdf/2506.04131", "abs": "https://arxiv.org/abs/2506.04131", "authors": ["Disha Sheshanarayana", "Tanishka Magar", "Ayushi Mittal", "Neelam Chaplot"], "title": "CLAIM: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation in Courtroom Dialogues", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to SICon 2025 ACL", "summary": "Courtrooms are places where lives are determined and fates are sealed, yet\nthey are not impervious to manipulation. Strategic use of manipulation in legal\njargon can sway the opinions of judges and affect the decisions. Despite the\ngrowing advancements in NLP, its application in detecting and analyzing\nmanipulation within the legal domain remains largely unexplored. Our work\naddresses this gap by introducing LegalCon, a dataset of 1,063 annotated\ncourtroom conversations labeled for manipulation detection, identification of\nprimary manipulators, and classification of manipulative techniques, with a\nfocus on long conversations. Furthermore, we propose CLAIM, a two-stage,\nIntent-driven Multi-agent framework designed to enhance manipulation analysis\nby enabling context-aware and informed decision-making. Our results highlight\nthe potential of incorporating agentic frameworks to improve fairness and\ntransparency in judicial processes. We hope that this contributes to the\nbroader application of NLP in legal discourse analysis and the development of\nrobust tools to support fairness in legal decision-making. Our code and data\nare available at https://github.com/Disha1001/CLAIM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6cd5\u5ead\u5bf9\u8bdd\u4e2d\u64cd\u63a7\u884c\u4e3a\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u6570\u636e\u96c6LegalCon\u548c\u6846\u67b6CLAIM\uff0c\u65e8\u5728\u63d0\u5347\u53f8\u6cd5\u8fc7\u7a0b\u7684\u516c\u5e73\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u6cd5\u5ead\u5bf9\u8bdd\u4e2d\u7684\u64cd\u63a7\u884c\u4e3a\u4f1a\u5f71\u54cd\u5224\u51b3\uff0c\u76ee\u524dNLP\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u8f83\u5c11\uff0c\u9700\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86\u6807\u6ce8\u6570\u636e\u96c6LegalCon\uff0c\u5e76\u63d0\u51fa\u4e24\u9636\u6bb5\u591a\u667a\u80fd\u4f53\u6846\u67b6CLAIM\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u589e\u5f3a\u64cd\u63a7\u5206\u6790\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u64cd\u63a7\u5206\u6790\u7684\u516c\u5e73\u6027\u548c\u900f\u660e\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aNLP\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u652f\u6301\u53f8\u6cd5\u51b3\u7b56\u7684\u516c\u5e73\u6027\u3002", "keywords": "\u6cd5\u5ead\u64cd\u63a7\u3001NLP\u3001\u6cd5\u5f8b\u8bdd\u8bed\u5206\u6790\u3001\u591a\u667a\u80fd\u4f53\u6846\u67b6"}}
{"id": "2506.03621", "pdf": "https://arxiv.org/pdf/2506.03621", "abs": "https://arxiv.org/abs/2506.03621", "authors": ["Chaehun Shin", "Jooyoung Choi", "Johan Barthelemy", "Jungbeom Lee", "Sungroh Yoon"], "title": "Negative-Guided Subject Fidelity Optimization for Zero-Shot Subject-Driven Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present Subject Fidelity Optimization (SFO), a novel comparative learning\nframework for zero-shot subject-driven generation that enhances subject\nfidelity. Beyond supervised fine-tuning methods that rely only on positive\ntargets and use the diffusion loss as in the pre-training stage, SFO introduces\nsynthetic negative targets and explicitly guides the model to favor positives\nover negatives through pairwise comparison. For negative targets, we propose\nCondition-Degradation Negative Sampling (CDNS), which automatically generates\ndistinctive and informative negatives by intentionally degrading visual and\ntextual cues without expensive human annotations. Moreover, we reweight the\ndiffusion timesteps to focus finetuning on intermediate steps where subject\ndetails emerge. Extensive experiments demonstrate that SFO with CDNS\nsignificantly outperforms baselines in terms of both subject fidelity and text\nalignment on a subject-driven generation benchmark. Project page:\nhttps://subjectfidelityoptimization.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSFO\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u589e\u5f3a\u96f6\u6837\u672c\u4e3b\u9898\u9a71\u52a8\u751f\u6210\u4e2d\u7684\u4e3b\u9898\u4fdd\u771f\u5ea6\uff0c\u65e0\u9700\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u7684\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6b63\u6837\u672c\u76ee\u6807\uff0c\u4e14\u4f7f\u7528\u4e0e\u9884\u8bad\u7ec3\u9636\u6bb5\u76f8\u540c\u7684\u6269\u6563\u635f\u5931\uff0c\u65e0\u6cd5\u6709\u6548\u63d0\u5347\u4e3b\u9898\u4fdd\u771f\u5ea6\u3002", "method": "SFO\u5f15\u5165\u5408\u6210\u8d1f\u6837\u672c\u76ee\u6807\uff0c\u901a\u8fc7\u4e24\u4e24\u6bd4\u8f83\u5f15\u5bfc\u6a21\u578b\u504f\u597d\u6b63\u6837\u672c\uff1b\u5e76\u63d0\u51faCDNS\u65b9\u6cd5\u81ea\u52a8\u751f\u6210\u6709\u533a\u5206\u6027\u7684\u8d1f\u6837\u672c\uff0c\u540c\u65f6\u91cd\u65b0\u52a0\u6743\u6269\u6563\u65f6\u95f4\u6b65\u4ee5\u805a\u7126\u7ec6\u8282\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSFO\u7ed3\u5408CDNS\u5728\u4e3b\u9898\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u5bf9\u9f50\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SFO\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u81ea\u52a8\u751f\u6210\u8d1f\u6837\u672c\u6709\u6548\u4f18\u5316\u4e86\u4e3b\u9898\u9a71\u52a8\u751f\u6210\u7684\u8d28\u91cf\u3002", "keywords": "\u96f6\u6837\u672c\u751f\u6210,\u4e3b\u9898\u4fdd\u771f\u5ea6,\u5bf9\u6bd4\u5b66\u4e60,\u6269\u6563\u6a21\u578b"}}
{"id": "2506.04139", "pdf": "https://arxiv.org/pdf/2506.04139", "abs": "https://arxiv.org/abs/2506.04139", "authors": ["Ratna Kandala", "Katie Hoemann"], "title": "Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in Low-Resource Flemish?", "categories": ["cs.CL"], "comment": null, "summary": "Understanding the nuances in everyday language is pivotal for advancements in\ncomputational linguistics & emotions research. Traditional lexicon-based tools\nsuch as LIWC and Pattern have long served as foundational instruments in this\ndomain. LIWC is the most extensively validated word count based text analysis\ntool in the social sciences and Pattern is an open source Python library\noffering functionalities for NLP. However, everyday language is inherently\nspontaneous, richly expressive, & deeply context dependent. To explore the\ncapabilities of LLMs in capturing the valences of daily narratives in Flemish,\nwe first conducted a study involving approximately 25,000 textual responses\nfrom 102 Dutch-speaking participants. Each participant provided narratives\nprompted by the question, \"What is happening right now and how do you feel\nabout it?\", accompanied by self-assessed valence ratings on a continuous scale\nfrom -50 to +50. We then assessed the performance of three Dutch-specific LLMs\nin predicting these valence scores, and compared their outputs to those\ngenerated by LIWC and Pattern. Our findings indicate that, despite advancements\nin LLM architectures, these Dutch tuned models currently fall short in\naccurately capturing the emotional valence present in spontaneous, real-world\nnarratives. This study underscores the imperative for developing culturally and\nlinguistically tailored models/tools that can adeptly handle the complexities\nof natural language use. Enhancing automated valence analysis is not only\npivotal for advancing computational methodologies but also holds significant\npromise for psychological research with ecologically valid insights into human\ndaily experiences. We advocate for increased efforts in creating comprehensive\ndatasets & finetuning LLMs for low-resource languages like Flemish, aiming to\nbridge the gap between computational linguistics & emotion research.", "AI": {"tldr": "LLM\u5728\u6355\u83b7\u5f17\u62c9\u8292\u8bed\u65e5\u5e38\u53d9\u8ff0\u4e2d\u7684\u60c5\u611f\u6548\u4ef7\u4e0a\u8868\u73b0\u4e0d\u5982\u4f20\u7edf\u5de5\u5177LIWC\u548cPattern\uff0c\u9700\u5f00\u53d1\u66f4\u8d34\u5408\u6587\u5316\u548c\u8bed\u8a00\u7684\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22LLM\u5728\u81ea\u7136\u8bed\u5883\u4e0b\u60c5\u611f\u6548\u4ef7\u5206\u6790\u7684\u6f5c\u529b\uff0c\u5f25\u8865\u4f20\u7edf\u5de5\u5177\u5728\u81ea\u53d1\u6027\u548c\u6587\u5316\u9002\u5e94\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u5bf9102\u540d\u8377\u5170\u8bed\u53c2\u4e0e\u8005\u6536\u96c6\u7ea625,000\u6761\u81ea\u53d1\u53d9\u8ff0\uff0c\u7ed3\u5408\u8fde\u7eed\u6548\u4ef7\u8bc4\u5206\uff0c\u6bd4\u8f83\u4e09\u79cd\u8377\u5170\u8bedLLM\u4e0eLIWC\u3001Pattern\u7684\u9884\u6d4b\u6548\u679c\u3002", "result": "\u5f53\u524d\u8377\u5170\u8bedLLM\u5728\u60c5\u611f\u6548\u4ef7\u9884\u6d4b\u4e0a\u4e0d\u5982\u4f20\u7edf\u5de5\u5177\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u4e0e\u6587\u5316\u9002\u914d\u6027\u3002", "conclusion": "\u9700\u66f4\u591a\u8d44\u6e90\u548c\u52aa\u529b\uff0c\u4f18\u5316\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5f17\u62c9\u8292\u8bed\uff09\u7684LLM\uff0c\u4ee5\u652f\u6301\u8ba1\u7b97\u8bed\u8a00\u5b66\u548c\u60c5\u611f\u7814\u7a76\u7684\u7ed3\u5408\u3002", "keywords": "LLM, \u60c5\u611f\u6548\u4ef7, \u5f17\u62c9\u8292\u8bed, \u8ba1\u7b97\u8bed\u8a00\u5b66, LIWC, Pattern"}}
{"id": "2506.03167", "pdf": "https://arxiv.org/pdf/2506.03167", "abs": "https://arxiv.org/abs/2506.03167", "authors": ["Long Tan Le", "Senura Hansaja Wanasekara", "Zerun Niu", "Yansong Shi", "Nguyen H. Tran", "Phuong Vo", "Walid Saad", "Dusit Niyato", "Zhu Han", "Choong Seon Hong", "H. Vincent Poor"], "title": "Distributionally Robust Wireless Semantic Communication with Large AI Models", "categories": ["cs.NI", "cs.ET", "cs.IT", "cs.LG", "math.IT"], "comment": "Under Review", "summary": "6G wireless systems are expected to support massive volumes of data with\nultra-low latency. However, conventional bit-level transmission strategies\ncannot support the efficiency and adaptability required by modern,\ndata-intensive applications. The concept of semantic communication (SemCom)\naddresses this limitation by focusing on transmitting task-relevant semantic\ninformation instead of raw data. While recent efforts incorporating deep\nlearning and large-scale AI models have improved SemCom's performance, existing\nsystems remain vulnerable to both semantic-level and transmission-level noise\nbecause they often rely on domain-specific architectures that hinder\ngeneralizability. In this paper, a novel and generalized semantic communication\nframework called WaSeCom is proposed to systematically address uncertainty and\nenhance robustness. In particular, Wasserstein distributionally robust\noptimization is employed to provide resilience against semantic\nmisinterpretation and channel perturbations. A rigorous theoretical analysis is\nperformed to establish the robust generalization guarantees of the proposed\nframework. Experimental results on image and text transmission demonstrate that\nWaSeCom achieves improved robustness under noise and adversarial perturbations.\nThese results highlight its effectiveness in preserving semantic fidelity\nacross varying wireless conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aWaSeCom\u7684\u65b0\u578b\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7Wasserstein\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u6765\u589e\u5f3a\u5bf9\u566a\u58f0\u548c\u5bf9\u6297\u6027\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u5347\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "motivation": "\u9488\u5bf96G\u7cfb\u7edf\u4e2d\u6570\u636e\u4f20\u8f93\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u95ee\u9898\uff0c\u73b0\u6709\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u56e0\u4f9d\u8d56\u9886\u57df\u7279\u5b9a\u67b6\u6784\u800c\u7f3a\u4e4f\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528Wasserstein\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u901a\u7528\u8bed\u4e49\u901a\u4fe1\u6846\u67b6WaSeCom\uff0c\u4ee5\u62b5\u6297\u8bed\u4e49\u8bef\u89e3\u548c\u4fe1\u9053\u6270\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWaSeCom\u5728\u56fe\u50cf\u548c\u6587\u672c\u4f20\u8f93\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u5728\u566a\u58f0\u548c\u5bf9\u6297\u6027\u6270\u52a8\u4e0b\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "conclusion": "WaSeCom\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u901a\u4fe1\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u5408\u590d\u6742\u65e0\u7ebf\u73af\u5883\u3002", "keywords": "6G,\u8bed\u4e49\u901a\u4fe1,\u9c81\u68d2\u4f18\u5316,Wasserstein\u8ddd\u79bb,\u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2506.04142", "pdf": "https://arxiv.org/pdf/2506.04142", "abs": "https://arxiv.org/abs/2506.04142", "authors": ["Kejian Zhu", "Shangqing Tu", "Zhuoran Jin", "Lei Hou", "Juanzi Li", "Jun Zhao"], "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient ($\\rho$)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5185\u90e8\u673a\u5236\u89e3\u51b3\u6570\u636e\u6c61\u67d3\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u6291\u5236\u2018\u6377\u5f84\u795e\u7ecf\u5143\u2019\u6765\u63d0\u5347\u6a21\u578b\u8bc4\u4f30\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u4f9d\u8d56\u516c\u5f00\u57fa\u51c6\uff0c\u4f46\u6570\u636e\u6c61\u67d3\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u4e86\u516c\u5e73\u6027\u3002\u6784\u5efa\u52a8\u6001\u57fa\u51c6\u6210\u672c\u9ad8\u4e14\u5468\u671f\u6027\u957f\uff0c\u56e0\u6b64\u9700\u8981\u4ece\u6a21\u578b\u5185\u90e8\u673a\u5236\u5165\u624b\u89e3\u51b3\u6c61\u67d3\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u548c\u56e0\u679c\u5206\u6790\u8bc6\u522b\u2018\u6377\u5f84\u795e\u7ecf\u5143\u2019\uff0c\u5e76\u63d0\u51fa\u2018\u6377\u5f84\u795e\u7ecf\u5143\u4fee\u8865\u2019\u65b9\u6cd5\u6291\u5236\u8fd9\u4e9b\u795e\u7ecf\u5143\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u6c61\u67d3\u5f71\u54cd\uff0c\u8bc4\u4f30\u7ed3\u679c\u4e0e\u53ef\u4fe1\u57fa\u51c6MixEval\u7684Spearman\u7cfb\u6570\u8d85\u8fc70.95\uff0c\u8868\u660e\u65b9\u6cd5\u53ef\u9760\u4e14\u63ed\u793a\u6a21\u578b\u771f\u5b9e\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u6570\u636e\u6c61\u67d3\u7684\u65b0\u9014\u5f84\uff0c\u901a\u8fc7\u5185\u90e8\u795e\u7ecf\u5143\u5206\u6790\u63d0\u5347\u8bc4\u4f30\u7684\u53ef\u4fe1\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u6570\u636e\u6c61\u67d3\u3001\u6377\u5f84\u795e\u7ecf\u5143\u3001\u53ef\u4fe1\u8bc4\u4f30\u3001\u6cdb\u5316\u6027"}}
{"id": "2506.03168", "pdf": "https://arxiv.org/pdf/2506.03168", "abs": "https://arxiv.org/abs/2506.03168", "authors": ["Dawen Jiang", "Zhishu Shen", "Qiushi Zheng", "Tiehua Zhang", "Wei Xiang", "Jiong Jin"], "title": "Farm-LightSeek: An Edge-centric Multimodal Agricultural IoT Data Analytics Framework with Lightweight LLMs", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by IEEE Internet of Things Magazine", "summary": "Amid the challenges posed by global population growth and climate change,\ntraditional agricultural Internet of Things (IoT) systems is currently\nundergoing a significant digital transformation to facilitate efficient big\ndata processing. While smart agriculture utilizes artificial intelligence (AI)\ntechnologies to enable precise control, it still encounters significant\nchallenges, including excessive reliance on agricultural expert knowledge,\ndifficulties in fusing multimodal data, poor adaptability to dynamic\nenvironments, and bottlenecks in real-time decision-making at the edge. Large\nlanguage models (LLMs), with their exceptional capabilities in knowledge\nacquisition and semantic understanding, provide a promising solution to address\nthese challenges. To this end, we propose Farm-LightSeek, an edge-centric\nmultimodal agricultural IoT data analytics framework that integrates LLMs with\nedge computing. This framework collects real-time farmland multi-source data\n(images, weather, geographic information) via sensors, performs cross-modal\nreasoning and disease detection at edge nodes, conducts low-latency management\ndecisions, and enables cloud collaboration for model updates. The main\ninnovations of Farm-LightSeek include: (1) an agricultural\n\"perception-decision-action\" closed-loop architecture; (2) cross-modal adaptive\nmonitoring; and (3)a lightweight LLM deployment strategy balancing performance\nand efficiency. Experiments conducted on two real-world datasets demonstrate\nthat Farm-LightSeek consistently achieves reliable performance in\nmission-critical tasks, even under the limitations of edge computing resources.\nThis work advances intelligent real-time agricultural solutions and highlights\nthe potential for deeper integration of agricultural IoT with LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u8fb9\u7f18\u8ba1\u7b97\u519c\u4e1a\u7269\u8054\u7f51\u6570\u636e\u5206\u6790\u6846\u67b6Farm-LightSeek\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u667a\u80fd\u519c\u4e1a\u7684\u6311\u6218\uff0c\u5305\u62ec\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u3001\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\u548c\u5b9e\u65f6\u51b3\u7b56\u74f6\u9888\u7b49\u95ee\u9898\u3002", "motivation": "\u5168\u7403\u4eba\u53e3\u589e\u957f\u548c\u6c14\u5019\u53d8\u5316\u7684\u80cc\u666f\u4e0b\uff0c\u4f20\u7edf\u519c\u4e1a\u7269\u8054\u7f51\u7cfb\u7edf\u9762\u4e34\u9ad8\u6548\u5927\u6570\u636e\u5904\u7406\u7684\u6311\u6218\uff0c\u667a\u80fd\u519c\u4e1a\u5c3d\u7ba1\u5229\u7528AI\u6280\u672f\uff0c\u4f46\u4ecd\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\u3001\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u56f0\u96be\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faFarm-LightSeek\u6846\u67b6\uff0c\u901a\u8fc7\u8fb9\u7f18\u8ba1\u7b97\u548cLLM\u7ed3\u5408\uff0c\u5b9e\u73b0\u591a\u6e90\u6570\u636e\u6536\u96c6\u3001\u8de8\u6a21\u6001\u63a8\u7406\u548c\u4f4e\u5ef6\u8fdf\u51b3\u7b56\u3002\u4e3b\u8981\u521b\u65b0\u5305\u62ec\u95ed\u73af\u67b6\u6784\u3001\u8de8\u6a21\u6001\u81ea\u9002\u5e94\u76d1\u6d4b\u548c\u8f7b\u91cf\u7ea7LLM\u90e8\u7f72\u7b56\u7565\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFarm-LightSeek\u5728\u8fb9\u7f18\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u4e0b\uff0c\u4ecd\u80fd\u53ef\u9760\u5b8c\u6210\u5173\u952e\u4efb\u52a1\u3002", "conclusion": "Farm-LightSeek\u63a8\u52a8\u4e86\u667a\u80fd\u519c\u4e1a\u5b9e\u65f6\u89e3\u51b3\u65b9\u6848\u7684\u53d1\u5c55\uff0c\u5c55\u793a\u4e86\u519c\u4e1a\u7269\u8054\u7f51\u4e0eLLM\u6df1\u5ea6\u6574\u5408\u7684\u6f5c\u529b\u3002", "keywords": "\u519c\u4e1a\u7269\u8054\u7f51, \u5927\u8bed\u8a00\u6a21\u578b, \u8fb9\u7f18\u8ba1\u7b97, \u591a\u6a21\u6001\u6570\u636e, \u5b9e\u65f6\u51b3\u7b56"}}
{"id": "2506.04156", "pdf": "https://arxiv.org/pdf/2506.04156", "abs": "https://arxiv.org/abs/2506.04156", "authors": ["Sarvesh Soni", "Dina Demner-Fushman"], "title": "A Dataset for Addressing Patient's Information Needs related to Clinical Course of Hospitalization", "categories": ["cs.CL"], "comment": null, "summary": "Patients have distinct information needs about their hospitalization that can\nbe addressed using clinical evidence from electronic health records (EHRs).\nWhile artificial intelligence (AI) systems show promise in meeting these needs,\nrobust datasets are needed to evaluate the factual accuracy and relevance of\nAI-generated responses. To our knowledge, no existing dataset captures patient\ninformation needs in the context of their EHRs. We introduce ArchEHR-QA, an\nexpert-annotated dataset based on real-world patient cases from intensive care\nunit and emergency department settings. The cases comprise questions posed by\npatients to public health forums, clinician-interpreted counterparts, relevant\nclinical note excerpts with sentence-level relevance annotations, and\nclinician-authored answers. To establish benchmarks for grounded EHR question\nanswering (QA), we evaluated three open-weight large language models\n(LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies:\ngenerating (1) answers with citations to clinical note sentences, (2) answers\nbefore citations, and (3) answers from filtered citations. We assessed\nperformance on two dimensions: Factuality (overlap between cited note sentences\nand ground truth) and Relevance (textual and semantic similarity between system\nand reference answers). The final dataset contains 134 patient cases. The\nanswer-first prompting approach consistently performed best, with Llama 4\nachieving the highest scores. Manual error analysis supported these findings\nand revealed common issues such as omitted key clinical evidence and\ncontradictory or hallucinated content. Overall, ArchEHR-QA provides a strong\nbenchmark for developing and evaluating patient-centered EHR QA systems,\nunderscoring the need for further progress toward generating factual and\nrelevant responses in clinical contexts.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86ArchEHR-QA\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55(EHR)\u95ee\u7b54\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e0d\u540cLLM\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u60a3\u8005\u5bf9\u4f4f\u9662\u4fe1\u606f\u7684\u9700\u6c42\uff0c\u5e76\u8bc4\u4f30AI\u751f\u6210\u7684\u56de\u7b54\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u3002", "method": "\u5229\u7528\u4e13\u5bb6\u6807\u6ce8\u7684EHR\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u4e09\u79cdLLM\u6a21\u578b\u5728\u4e09\u79cd\u63d0\u793a\u7b56\u7565\u4e0b\u7684\u8868\u73b0\u3002", "result": "Llama 4\u5728'\u56de\u7b54\u4f18\u5148'\u63d0\u793a\u7b56\u7565\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4ecd\u5b58\u5728\u9057\u6f0f\u5173\u952e\u8bc1\u636e\u6216\u865a\u6784\u5185\u5bb9\u7684\u95ee\u9898\u3002", "conclusion": "ArchEHR-QA\u4e3a\u60a3\u8005\u4e3a\u4e2d\u5fc3\u7684EHR QA\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4e8b\u5b9e\u6027\u548c\u76f8\u5173\u6027\u3002", "keywords": "EHR, AI, LLM, question answering, clinical notes"}}
{"id": "2506.03642", "pdf": "https://arxiv.org/pdf/2506.03642", "abs": "https://arxiv.org/abs/2506.03642", "authors": ["Haoyu Zhang", "Meng Liu", "Zaijing Li", "Haokun Wen", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "title": "Spatial Understanding from Videos: Structured Prompts Meet Simulation Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual-spatial understanding, the ability to infer object relationships and\nlayouts from visual input, is fundamental to downstream tasks such as robotic\nnavigation and embodied interaction. However, existing methods face spatial\nuncertainty and data scarcity, limiting the 3D spatial reasoning capability of\npre-trained vision-language models (VLMs). To address these challenges, we\npresent a unified framework for enhancing 3D spatial reasoning in pre-trained\nVLMs without modifying their architecture. This framework combines SpatialMind,\na structured prompting strategy that decomposes complex scenes and questions\ninto interpretable reasoning steps, with ScanForgeQA, a scalable\nquestion-answering dataset built from diverse 3D simulation scenes through an\nautomated construction process designed for fine-tuning. Extensive experiments\nacross multiple benchmarks demonstrate the individual and combined\neffectiveness of our prompting and fine-tuning strategies, and yield insights\nthat may inspire future research on visual-spatial understanding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff093D\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u63d0\u793a\u7b56\u7565\u548c\u81ea\u52a8\u5316\u6784\u5efa\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57283D\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u6846\u67b6\u5305\u542bSpatialMind\uff08\u7ed3\u6784\u5316\u63d0\u793a\u7b56\u7565\uff09\u548cScanForgeQA\uff08\u81ea\u52a8\u5316\u6784\u5efa\u7684\u6570\u636e\u96c6\uff09\uff0c\u7528\u4e8e\u5206\u89e3\u590d\u6742\u573a\u666f\u548c\u95ee\u9898\u5e76\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u793a\u548c\u5fae\u8c03\u7b56\u7565\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u76843D\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89c6\u89c9\u7a7a\u95f4\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u53ef\u80fd\u542f\u53d1\u672a\u6765\u7814\u7a76\u3002", "keywords": "3D\u7a7a\u95f4\u63a8\u7406,\u89c6\u89c9\u8bed\u8a00\u6a21\u578b,\u7ed3\u6784\u5316\u63d0\u793a,\u81ea\u52a8\u5316\u6570\u636e\u96c6"}}
{"id": "2506.04179", "pdf": "https://arxiv.org/pdf/2506.04179", "abs": "https://arxiv.org/abs/2506.04179", "authors": ["Anhao Zhao", "Fanghua Ye", "Yingqi Fan", "Junlong Tong", "Zhiwei Fei", "Hui Su", "Xiaoyu Shen"], "title": "SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve remarkable performance across tasks but\nincur substantial computational costs due to their deep, multi-layered\narchitectures. Layer pruning has emerged as a strategy to alleviate these\ninefficiencies, but conventional static pruning methods overlook two critical\ndynamics inherent to LLM inference: (1) horizontal dynamics, where token-level\nheterogeneity demands context-aware pruning decisions, and (2) vertical\ndynamics, where the distinct functional roles of MLP and self-attention layers\nnecessitate component-specific pruning policies. We introduce SkipGPT, a\ndynamic layer pruning framework designed to optimize computational resource\nallocation through two core innovations: (1) global token-aware routing to\nprioritize critical tokens, and (2) decoupled pruning policies for MLP and\nself-attention components. To mitigate training instability, we propose a\ntwo-stage optimization paradigm: first, a disentangled training phase that\nlearns routing strategies via soft parameterization to avoid premature pruning\ndecisions, followed by parameter-efficient LoRA fine-tuning to restore\nperformance impacted by layer removal. Extensive experiments demonstrate that\nSkipGPT reduces over 40% of model parameters while matching or exceeding the\nperformance of the original dense model across benchmarks. By harmonizing\ndynamic efficiency with preserved expressivity, SkipGPT advances the practical\ndeployment of scalable, resource-aware LLMs. Our code is publicly available at:\nhttps://github.com/EIT-NLP/SkipGPT.", "AI": {"tldr": "SkipGPT\u662f\u4e00\u79cd\u52a8\u6001\u5c42\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u4ee4\u724c\u611f\u77e5\u8def\u7531\u548c\u5206\u79bb\u7684MLP\u4e0e\u81ea\u6ce8\u610f\u529b\u526a\u679d\u7b56\u7565\uff0c\u51cf\u5c1140%\u4ee5\u4e0a\u53c2\u6570\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4f20\u7edf\u9759\u6001\u526a\u679d\u65b9\u6cd5\u5ffd\u7565\u4e86\u4ee4\u724c\u548c\u7ec4\u4ef6\u95f4\u7684\u52a8\u6001\u5dee\u5f02\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u52a8\u6001\u526a\u679d\u7b56\u7565\u3002", "method": "SkipGPT\u5f15\u5165\u5168\u5c40\u4ee4\u724c\u611f\u77e5\u8def\u7531\u548cMLP/\u81ea\u6ce8\u610f\u529b\u5206\u79bb\u526a\u679d\u7b56\u7565\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\uff08\u89e3\u8026\u8bad\u7ec3\u548cLoRA\u5fae\u8c03\uff09\u4ee5\u51cf\u5c11\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSkipGPT\u5728\u51cf\u5c1140%\u4ee5\u4e0a\u53c2\u6570\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4e0e\u539f\u5bc6\u96c6\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "SkipGPT\u901a\u8fc7\u52a8\u6001\u526a\u679d\u4e0e\u8868\u8fbe\u529b\u4fdd\u7559\uff0c\u63a8\u52a8\u4e86\u8d44\u6e90\u9ad8\u6548LLMs\u7684\u5b9e\u9645\u5e94\u7528\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u52a8\u6001\u526a\u679d, \u8d44\u6e90\u4f18\u5316, SkipGPT"}}
{"id": "2506.03654", "pdf": "https://arxiv.org/pdf/2506.03654", "abs": "https://arxiv.org/abs/2506.03654", "authors": ["Xiaochun Lei", "Siqi Wu", "Weilin Wu", "Zetao Jiang"], "title": "MambaNeXt-YOLO: A Hybrid State Space Model for Real-time Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Real-time object detection is a fundamental but challenging task in computer\nvision, particularly when computational resources are limited. Although\nYOLO-series models have set strong benchmarks by balancing speed and accuracy,\nthe increasing need for richer global context modeling has led to the use of\nTransformer-based architectures. Nevertheless, Transformers have high\ncomputational complexity because of their self-attention mechanism, which\nlimits their practicality for real-time and edge deployments. To overcome these\nchallenges, recent developments in linear state space models, such as Mamba,\nprovide a promising alternative by enabling efficient sequence modeling with\nlinear complexity. Building on this insight, we propose MambaNeXt-YOLO, a novel\nobject detection framework that balances accuracy and efficiency through three\nkey contributions: (1) MambaNeXt Block: a hybrid design that integrates CNNs\nwith Mamba to effectively capture both local features and long-range\ndependencies; (2) Multi-branch Asymmetric Fusion Pyramid Network (MAFPN): an\nenhanced feature pyramid architecture that improves multi-scale object\ndetection across various object sizes; and (3) Edge-focused Efficiency: our\nmethod achieved 66.6\\% mAP at 31.9 FPS on the PASCAL VOC dataset without any\npre-training and supports deployment on edge devices such as the NVIDIA Jetson\nXavier NX and Orin NX.", "AI": {"tldr": "MambaNeXt-YOLO\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u4e86CNN\u4e0eMamba\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5e73\u8861\u76ee\u6807\u68c0\u6d4b\u7684\u901f\u5ea6\u4e0e\u51c6\u786e\u6027\uff0c\u540c\u65f6\u89e3\u51b3Transformer\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\u3002", "method": "\u63d0\u51faMambaNeXt Block\u3001MAFPN\u548c\u8fb9\u7f18\u4f18\u5316\uff0c\u7ed3\u5408CNN\u4e0eMamba\u4ee5\u6355\u83b7\u5c40\u90e8\u548c\u957f\u8ddd\u79bb\u7279\u5f81\u3002", "result": "\u5728PASCAL VOC\u6570\u636e\u96c6\u4e0a\u8fbe\u523066.6% mAP\u548c31.9 FPS\uff0c\u652f\u6301\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "conclusion": "MambaNeXt-YOLO\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u548c\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u3002", "keywords": "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b, CNN, Mamba, \u8fb9\u7f18\u8ba1\u7b97, \u7279\u5f81\u91d1\u5b57\u5854"}}
{"id": "2506.04180", "pdf": "https://arxiv.org/pdf/2506.04180", "abs": "https://arxiv.org/abs/2506.04180", "authors": ["Yuhao Wu", "Yushi Bai", "Zhiqiang Hu", "Juanzi Li", "Roy Ka-Wei Lee"], "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6SuperWriter-Agent\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u601d\u7ef4\u6b65\u9aa4\u4f18\u5316\u957f\u6587\u672c\u751f\u6210\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u83b7\u5f97\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u957f\u6587\u672c\u65f6\u9762\u4e34\u7684\u8fde\u8d2f\u6027\u3001\u903b\u8f91\u4e00\u81f4\u6027\u548c\u6587\u672c\u8d28\u91cf\u968f\u957f\u5ea6\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSuperWriter-Agent\u6846\u67b6\uff0c\u5305\u62ec\u7ed3\u6784\u5316\u601d\u7ef4\u3001\u89c4\u5212\u548c\u7cbe\u70bc\u9636\u6bb5\uff0c\u5e76\u4f7f\u7528\u5206\u5c42\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u8fdb\u884c\u4f18\u5316\u3002", "result": "SuperWriter-LM\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u66f4\u5927\u89c4\u6a21\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5206\u5c42DPO\u548c\u7ed3\u6784\u5316\u601d\u7ef4\u6b65\u9aa4\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u6784\u5316\u601d\u7ef4\u6b65\u9aa4\u548c\u5206\u5c42DPO\u663e\u8457\u63d0\u5347\u4e86\u957f\u6587\u672c\u751f\u6210\u7684\u8d28\u91cf\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u601d\u8def\u3002", "keywords": "\u957f\u6587\u672c\u751f\u6210, LLM, \u4ee3\u7406\u6846\u67b6, \u5206\u5c42DPO, MCTS"}}
{"id": "2506.03667", "pdf": "https://arxiv.org/pdf/2506.03667", "abs": "https://arxiv.org/abs/2506.03667", "authors": ["Joji Joseph", "Bharadwaj Amrutur", "Shalabh Bhatnagar"], "title": "Accelerating SfM-based Pose Estimation with Dominating Set", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper introduces a preprocessing technique to speed up\nStructure-from-Motion (SfM) based pose estimation, which is critical for\nreal-time applications like augmented reality (AR), virtual reality (VR), and\nrobotics. Our method leverages the concept of a dominating set from graph\ntheory to preprocess SfM models, significantly enhancing the speed of the pose\nestimation process without losing significant accuracy. Using the OnePose\ndataset, we evaluated our method across various SfM-based pose estimation\ntechniques. The results demonstrate substantial improvements in processing\nspeed, ranging from 1.5 to 14.48 times, and a reduction in reference images and\npoint cloud size by factors of 17-23 and 2.27-4, respectively. This work offers\na promising solution for efficient and accurate 3D pose estimation, balancing\nspeed and accuracy in real-time applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u652f\u914d\u96c6\u7684\u9884\u5904\u7406\u6280\u672f\uff0c\u663e\u8457\u52a0\u901fSfM\u4f4d\u59ff\u4f30\u8ba1\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002", "motivation": "\u4e3a\u89e3\u51b3SfM\u4f4d\u59ff\u4f30\u8ba1\u5728AR\u3001VR\u548c\u673a\u5668\u4eba\u7b49\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u901f\u5ea6\u95ee\u9898\u3002", "method": "\u5229\u7528\u56fe\u8bba\u4e2d\u7684\u652f\u914d\u96c6\u6982\u5ff5\u9884\u5904\u7406SfM\u6a21\u578b\uff0c\u7ed3\u5408OnePose\u6570\u636e\u96c6\u8bc4\u4f30\u3002", "result": "\u5904\u7406\u901f\u5ea6\u63d0\u53471.5-14.48\u500d\uff0c\u53c2\u8003\u56fe\u50cf\u548c\u70b9\u4e91\u5927\u5c0f\u5206\u522b\u51cf\u5c1117-23\u500d\u548c2.27-4\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u7cbe\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u9ad8\u6548\u3001\u51c6\u786e\u76843D\u4f4d\u59ff\u4f30\u8ba1\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "SfM, \u4f4d\u59ff\u4f30\u8ba1, \u56fe\u8bba, \u652f\u914d\u96c6, \u5b9e\u65f6\u5e94\u7528"}}
{"id": "2506.03182", "pdf": "https://arxiv.org/pdf/2506.03182", "abs": "https://arxiv.org/abs/2506.03182", "authors": ["Shivani Chiranjeevi", "Hossein Zaremehrjerdi", "Zi K. Deng", "Talukder Z. Jubery", "Ari Grele", "Arti Singh", "Asheesh K Singh", "Soumik Sarkar", "Nirav Merchant", "Harold F. Greeney", "Baskar Ganapathysubramanian", "Chinmay Hegde"], "title": "TerraIncognita: A Dynamic Benchmark for Species Discovery Using Frontier Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The rapid global loss of biodiversity, particularly among insects, represents\nan urgent ecological crisis. Current methods for insect species discovery are\nmanual, slow, and severely constrained by taxonomic expertise, hindering timely\nconservation actions. We introduce TerraIncognita, a dynamic benchmark designed\nto evaluate state-of-the-art multimodal models for the challenging problem of\nidentifying unknown, potentially undescribed insect species from image data.\nOur benchmark dataset combines a mix of expertly annotated images of insect\nspecies likely known to frontier AI models, and images of rare and poorly known\nspecies, for which few/no publicly available images exist. These images were\ncollected from underexplored biodiversity hotspots, realistically mimicking\nopen-world discovery scenarios faced by ecologists. The benchmark assesses\nmodels' proficiency in hierarchical taxonomic classification, their capability\nto detect and abstain from out-of-distribution (OOD) samples representing novel\nspecies, and their ability to generate explanations aligned with expert\ntaxonomic knowledge. Notably, top-performing models achieve over 90\\% F1 at the\nOrder level on known species, but drop below 2\\% at the Species level,\nhighlighting the sharp difficulty gradient from coarse to fine taxonomic\nprediction (Order $\\rightarrow$ Family $\\rightarrow$ Genus $\\rightarrow$\nSpecies). TerraIncognita will be updated regularly, and by committing to\nquarterly dataset expansions (of both known and novel species), will provide an\nevolving platform for longitudinal benchmarking of frontier AI methods. All\nTerraIncognita data, results, and future updates are available\n\\href{https://baskargroup.github.io/TerraIncognita/}{here}.", "AI": {"tldr": "TerraIncognita\u662f\u4e00\u4e2a\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u5728\u8bc6\u522b\u672a\u77e5\u6606\u866b\u7269\u79cd\u65b9\u9762\u7684\u6027\u80fd\uff0c\u5f3a\u8c03\u4ece\u7c97\u5230\u7ec6\u7684\u5206\u7c7b\u9884\u6d4b\u96be\u5ea6\u68af\u5ea6\u3002", "motivation": "\u5168\u7403\u751f\u7269\u591a\u6837\u6027\u5feb\u901f\u4e27\u5931\uff0c\u5c24\u5176\u662f\u6606\u866b\uff0c\u5f53\u524d\u7684\u4eba\u5de5\u7269\u79cd\u53d1\u73b0\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u4e9f\u9700AI\u6280\u672f\u8f85\u52a9\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u4e13\u5bb6\u6807\u6ce8\u7684\u5df2\u77e5\u7269\u79cd\u56fe\u50cf\u548c\u7f55\u89c1\u7269\u79cd\u56fe\u50cf\uff0c\u521b\u5efa\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u7684\u5206\u7c7b\u80fd\u529b\u548cOOD\u6837\u672c\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5728\u5df2\u77e5\u7269\u79cd\u7684Order\u7ea7\u522bF1\u5206\u6570\u8d85\u8fc790%\uff0c\u4f46\u5728Species\u7ea7\u522b\u4f4e\u4e8e2%\uff0c\u663e\u793a\u5206\u7c7b\u96be\u5ea6\u68af\u5ea6\u3002", "conclusion": "TerraIncognita\u5c06\u6301\u7eed\u66f4\u65b0\uff0c\u4e3aAI\u65b9\u6cd5\u63d0\u4f9b\u7eb5\u5411\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\u3002", "keywords": "\u751f\u7269\u591a\u6837\u6027,\u6606\u866b\u5206\u7c7b,\u591a\u6a21\u6001\u6a21\u578b,\u57fa\u51c6\u6d4b\u8bd5,OOD\u68c0\u6d4b"}}
{"id": "2506.04182", "pdf": "https://arxiv.org/pdf/2506.04182", "abs": "https://arxiv.org/abs/2506.04182", "authors": ["Ruiqi Zhang", "Changyi Xiao", "Yixin Cao"], "title": "Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "With the rapid advancement of large reasoning models, long Chain-of-Thought\n(CoT) prompting has demonstrated strong performance on complex tasks. However,\nthis often comes with a significant increase in token usage. In this paper, we\nconduct a comprehensive empirical analysis comparing long and short CoT\nstrategies. Our findings reveal that while long CoT can lead to performance\nimprovements, its benefits are often marginal relative to its significantly\nhigher token consumption. Specifically, long CoT tends to outperform when ample\ngeneration budgets are available, whereas short CoT is more effective under\ntighter budget constraints. These insights underscore the need for a dynamic\napproach that selects the proper CoT strategy based on task context and\nresource availability. To address this, we propose SwitchCoT, an automatic\nframework that adaptively chooses between long and short CoT strategies to\nbalance reasoning accuracy and computational efficiency. Moreover, SwitchCoT is\ndesigned to be budget-aware, making it broadly applicable across scenarios with\nvarying resource constraints. Experimental results demonstrate that SwitchCoT\ncan reduce inference costs by up to 50% while maintaining high accuracy.\nNotably, under limited token budgets, it achieves performance comparable to, or\neven exceeding, that of using either long or short CoT alone.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5bf9\u6bd4\u957f\u94fe\u548c\u77ed\u94fe\u601d\u7ef4\uff08CoT\uff09\u63d0\u793a\u7b56\u7565\u7684\u6027\u80fd\u4e0e\u8d44\u6e90\u6d88\u8017\uff0c\u63d0\u51fa\u4e86\u52a8\u6001\u9009\u62e9\u7b56\u7565\u7684SwitchCoT\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u968f\u7740\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u957f\u94fe\u601d\u7ef4\uff08CoT\uff09\u63d0\u793a\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4f1a\u663e\u8457\u589e\u52a0token\u6d88\u8017\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5e73\u8861\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u6bd4\u8f83\u957f\u77edCoT\u7b56\u7565\uff0c\u63d0\u51faSwitchCoT\u6846\u67b6\uff0c\u6839\u636e\u4efb\u52a1\u4e0a\u4e0b\u6587\u548c\u8d44\u6e90\u52a8\u6001\u9009\u62e9\u7b56\u7565\u3002", "result": "SwitchCoT\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u53ef\u51cf\u5c1150%\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8a\u5355\u4e00\u7b56\u7565\u7684\u6027\u80fd\u3002", "conclusion": "\u52a8\u6001\u9009\u62e9CoT\u7b56\u7565\u80fd\u6709\u6548\u5e73\u8861\u51c6\u786e\u6027\u4e0e\u6548\u7387\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u3002", "keywords": "Chain-of-Thought, SwitchCoT, \u8ba1\u7b97\u6548\u7387, \u52a8\u6001\u7b56\u7565, \u8d44\u6e90\u7ea6\u675f"}}
{"id": "2506.03682", "pdf": "https://arxiv.org/pdf/2506.03682", "abs": "https://arxiv.org/abs/2506.03682", "authors": ["Melika Ayoughi", "Samira Abnar", "Chen Huang", "Chris Sandino", "Sayeri Lala", "Eeshan Gunesh Dhekane", "Dan Busbridge", "Shuangfei Zhai", "Vimal Thilak", "Josh Susskind", "Pascal Mettes", "Paul Groth", "Hanlin Goh"], "title": "How PARTs assemble into wholes: Learning the relative composition of images", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The composition of objects and their parts, along with object-object\npositional relationships, provides a rich source of information for\nrepresentation learning. Hence, spatial-aware pretext tasks have been actively\nexplored in self-supervised learning. Existing works commonly start from a grid\nstructure, where the goal of the pretext task involves predicting the absolute\nposition index of patches within a fixed grid. However, grid-based approaches\nfall short of capturing the fluid and continuous nature of real-world object\ncompositions. We introduce PART, a self-supervised learning approach that\nleverages continuous relative transformations between off-grid patches to\novercome these limitations. By modeling how parts relate to each other in a\ncontinuous space, PART learns the relative composition of images-an off-grid\nstructural relative positioning process that generalizes beyond occlusions and\ndeformations. In tasks requiring precise spatial understanding such as object\ndetection and time series prediction, PART outperforms strong grid-based\nmethods like MAE and DropPos, while also maintaining competitive performance on\nglobal classification tasks with minimal hyperparameter tuning. By breaking\nfree from grid constraints, PART opens up an exciting new trajectory for\nuniversal self-supervised pretraining across diverse datatypes-from natural\nimages to EEG signals-with promising potential in video, medical imaging, and\naudio.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPART\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fde\u7eed\u76f8\u5bf9\u53d8\u6362\u89e3\u51b3\u7f51\u683c\u65b9\u6cd5\u5728\u6355\u6349\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u7ec4\u5408\u8fde\u7eed\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7f51\u683c\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u6355\u6349\u771f\u5b9e\u573a\u666f\u4e2d\u7269\u4f53\u7ec4\u5408\u7684\u8fde\u7eed\u6027\uff0c\u9650\u5236\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u8868\u73b0\u3002", "method": "PART\u5229\u7528\u79bb\u683c\u8865\u4e01\u95f4\u7684\u8fde\u7eed\u76f8\u5bf9\u53d8\u6362\uff0c\u5b66\u4e60\u56fe\u50cf\u90e8\u5206\u7684\u76f8\u5bf9\u7ec4\u5408\u7ed3\u6784\uff0c\u907f\u514d\u4e86\u7f51\u683c\u9650\u5236\u3002", "result": "PART\u5728\u9700\u8981\u7cbe\u786e\u7a7a\u95f4\u7406\u89e3\u7684\u4efb\u52a1\uff08\u5982\u5bf9\u8c61\u68c0\u6d4b\uff09\u4e2d\u4f18\u4e8eMAE\u548cDropPos\u7b49\u7f51\u683c\u65b9\u6cd5\uff0c\u5e76\u5728\u5168\u5c40\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "PART\u901a\u8fc7\u6446\u8131\u7f51\u683c\u7ea6\u675f\uff0c\u4e3a\u8de8\u591a\u6837\u6570\u636e\u7c7b\u578b\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u81ea\u76d1\u7763\u5b66\u4e60, \u8fde\u7eed\u76f8\u5bf9\u53d8\u6362, \u7a7a\u95f4\u7406\u89e3, \u5bf9\u8c61\u68c0\u6d4b, \u56fe\u50cf\u7ec4\u5408"}}
{"id": "2506.04185", "pdf": "https://arxiv.org/pdf/2506.04185", "abs": "https://arxiv.org/abs/2506.04185", "authors": ["Qingfei Zhao", "Ruobing Wang", "Dingling Xu", "Daren Zha", "Limin Liu"], "title": "R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning", "categories": ["cs.CL"], "comment": "16 pages, 3 figures", "summary": "Large language models (LLMs) have notably progressed in multi-step and\nlong-chain reasoning. However, extending their reasoning capabilities to\nencompass deep interactions with search remains a non-trivial challenge, as\nmodels often fail to identify optimal reasoning-search interaction\ntrajectories, resulting in suboptimal responses. We propose R-Search, a novel\nreinforcement learning framework for Reasoning-Search integration, designed to\nenable LLMs to autonomously execute multi-step reasoning with deep search\ninteraction, and learn optimal reasoning search interaction trajectories via\nmulti-reward signals, improving response quality in complex logic- and\nknowledge-intensive tasks. R-Search guides the LLM to dynamically decide when\nto retrieve or reason, while globally integrating key evidence to enhance deep\nknowledge interaction between reasoning and search. During RL training,\nR-Search provides multi-stage, multi-type rewards to jointly optimize the\nreasoning-search trajectory. Experiments on seven datasets show that R-Search\noutperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%\n(out-of-domain). The code and data are available at\nhttps://github.com/QingFei1/R-Search.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aR-Search\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u591a\u5956\u52b1\u4fe1\u53f7\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406-\u641c\u7d22\u4ea4\u4e92\u8f68\u8ff9\uff0c\u63d0\u5347\u590d\u6742\u903b\u8f91\u548c\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u54cd\u5e94\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6b65\u548c\u957f\u94fe\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u4e0e\u641c\u7d22\u7684\u6df1\u5ea6\u4ea4\u4e92\u4e2d\u4ecd\u96be\u4ee5\u627e\u5230\u6700\u4f18\u7684\u4ea4\u4e92\u8f68\u8ff9\uff0c\u5bfc\u81f4\u54cd\u5e94\u8d28\u91cf\u4e0d\u4f73\u3002", "method": "R-Search\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff0c\u5f15\u5bfcLLM\u52a8\u6001\u51b3\u5b9a\u4f55\u65f6\u68c0\u7d22\u6216\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u591a\u9636\u6bb5\u3001\u591a\u7c7b\u578b\u7684\u5956\u52b1\u8054\u5408\u4f18\u5316\u63a8\u7406-\u641c\u7d22\u8f68\u8ff9\u3002", "result": "\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cR-Search\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u7684\u6027\u80fd\u5206\u522b\u6bd4\u5148\u8fdb\u7684RAG\u57fa\u7ebf\u63d0\u5347\u4e8632.2%\u548c25.1%\u3002", "conclusion": "R-Search\u6709\u6548\u589e\u5f3a\u4e86LLM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u548c\u641c\u7d22\u4ea4\u4e92\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u54cd\u5e94\u8d28\u91cf\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u591a\u6b65\u63a8\u7406\u3001\u641c\u7d22\u4ea4\u4e92\u3001R-Search"}}
{"id": "2506.03710", "pdf": "https://arxiv.org/pdf/2506.03710", "abs": "https://arxiv.org/abs/2506.03710", "authors": ["Yisen Feng", "Haoyu Zhang", "Qiaohui Chu", "Meng Liu", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "title": "OSGNet @ Ego4D Episodic Memory Challenge 2025", "categories": ["cs.CV", "cs.AI"], "comment": "The champion solutions for the three egocentric video localization\n  tracks(Natural Language Queries, Goal Step, and Moment Queries tracks) of the\n  Ego4D Episodic Memory Challenge at CVPR EgoVis Workshop 2025", "summary": "In this report, we present our champion solutions for the three egocentric\nvideo localization tracks of the Ego4D Episodic Memory Challenge at CVPR 2025.\nAll tracks require precise localization of the interval within an untrimmed\negocentric video. Previous unified video localization approaches often rely on\nlate fusion strategies, which tend to yield suboptimal results. To address\nthis, we adopt an early fusion-based video localization model to tackle all\nthree tasks, aiming to enhance localization accuracy. Ultimately, our method\nachieved first place in the Natural Language Queries, Goal Step, and Moment\nQueries tracks, demonstrating its effectiveness. Our code can be found at\nhttps://github.com/Yisen-Feng/OSGNet.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728CVPR 2025\u7684Ego4D Episodic Memory Challenge\u4e2d\uff0c\u9488\u5bf9\u4e09\u4e2a\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u5b9a\u4f4d\u4efb\u52a1\u7684\u51a0\u519b\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5148\u524d\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u540e\u671f\u878d\u5408\u7b56\u7565\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u7406\u60f3\uff0c\u56e0\u6b64\u91c7\u7528\u65e9\u671f\u878d\u5408\u6a21\u578b\u4ee5\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u65e9\u671f\u878d\u5408\u7684\u89c6\u9891\u5b9a\u4f4d\u6a21\u578b\uff0c\u7edf\u4e00\u5904\u7406\u4e09\u4e2a\u4efb\u52a1\u3002", "result": "\u5728Natural Language Queries\u3001Goal Step\u548cMoment Queries\u4e09\u4e2a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u7b2c\u4e00\u540d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "keywords": "\u89c6\u9891\u5b9a\u4f4d, \u65e9\u671f\u878d\u5408, Ego4D, CVPR"}}
{"id": "2506.04226", "pdf": "https://arxiv.org/pdf/2506.04226", "abs": "https://arxiv.org/abs/2506.04226", "authors": ["Akshat Gupta", "Maochuan Lu", "Thomas Hartvigsen", "Gopala Anumanchipalli"], "title": "Efficient Knowledge Editing via Minimal Precomputation", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "Knowledge editing methods like MEMIT are able to make data and compute\nefficient updates of factual knowledge by using a single sentence to update\nfacts and their consequences. However, what is often overlooked is a\n\"precomputation step\", which requires a one-time but significant computational\ncost. The authors of MEMIT originally precompute approximately 44 million\nhidden vectors per edited layer, which requires a forward pass over 44 million\ntokens. For GPT-J (6B), this precomputation step takes 36 hours on a single\nGPU, while it takes approximately 40 hours for Llama2-7B. Additionally, this\nprecomputation time grows with model size. In this paper, we show that this\nexcessive computational cost is unnecessary. Knowledge editing using MEMIT and\nrelated methods, such as ROME and EMMET, can be performed by pre-computing a\nvery small portion of the 44 million hidden vectors. We first present the\ntheoretical minimum number of hidden vector precomputation required for\nsolutions of these editing methods to exist. We then empirically show that\nknowledge editing using these methods can be done by pre-computing\nsignificantly fewer hidden vectors. Specifically, we show that the\nprecomputation step can be done with less than 0.3% of the originally\nstipulated number of hidden vectors. This saves a significant amount of\nprecomputation time and allows users to begin editing new models within a few\nminutes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51cf\u5c11\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\uff08\u5982MEMIT\uff09\u4e2d\u9884\u8ba1\u7b97\u9690\u85cf\u5411\u91cf\u6570\u91cf\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u9700\u8981\u9884\u8ba1\u7b97\u5927\u91cf\u9690\u85cf\u5411\u91cf\uff0c\u8017\u65f6\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u672c\u6587\u65e8\u5728\u51cf\u5c11\u8fd9\u4e00\u6210\u672c\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc1\u660e\u4ec5\u9700\u9884\u8ba1\u7b97\u6781\u5c11\u90e8\u5206\u9690\u85cf\u5411\u91cf\u5373\u53ef\u5b8c\u6210\u77e5\u8bc6\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9884\u8ba1\u7b97\u53ef\u51cf\u5c11\u81f3\u539f\u6570\u91cf\u76840.3%\uff0c\u663e\u8457\u8282\u7701\u65f6\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u7f16\u8f91\u7684\u6548\u7387\uff0c\u4f7f\u65b0\u6a21\u578b\u7684\u7f16\u8f91\u4efb\u52a1\u80fd\u5728\u51e0\u5206\u949f\u5185\u542f\u52a8\u3002", "keywords": "\u77e5\u8bc6\u7f16\u8f91, MEMIT, \u9884\u8ba1\u7b97, \u9690\u85cf\u5411\u91cf, \u8ba1\u7b97\u6548\u7387"}}
{"id": "2505.24073", "pdf": "https://arxiv.org/pdf/2505.24073", "abs": "https://arxiv.org/abs/2505.24073", "authors": ["Chan-Wei Hu", "Yueqi Wang", "Shuo Xing", "Chia-Ju Chen", "Zhengzhong Tu"], "title": "mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "16 pages, 11 figures", "summary": "Large Vision-Language Models (LVLMs) have made remarkable strides in\nmultimodal tasks such as visual question answering, visual grounding, and\ncomplex reasoning. However, they remain limited by static training data,\nsusceptibility to hallucinations, and inability to verify claims against\nup-to-date, external evidence, compromising their performance in dynamic\nreal-world applications. Retrieval-Augmented Generation (RAG) offers a\npractical solution to mitigate these challenges by allowing the LVLMs to access\nlarge-scale knowledge databases via retrieval mechanisms, thereby grounding\nmodel outputs in factual, contextually relevant information. Here in this\npaper, we conduct the first systematic dissection of the multimodal RAG\npipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the\nmodality configurations and retrieval strategies, (2) the re-ranking stage: on\nstrategies to mitigate positional biases and improve the relevance of retrieved\nevidence, and (3) the generation phase: we further investigate how to best\nintegrate retrieved candidates into the final generation process. Finally, we\nextend to explore a unified agentic framework that integrates re-ranking and\ngeneration through self-reflection, enabling LVLMs to select relevant evidence\nand suppress irrelevant context dynamically. Our full-stack exploration of RAG\nfor LVLMs yields substantial insights, resulting in an average performance\nboost of 5% without any fine-tuning.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u63a2\u8ba8\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u4e86\u9759\u6001\u6570\u636e\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "LVLMs\u5728\u52a8\u6001\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\uff0c\u5982\u9759\u6001\u6570\u636e\u548c\u5e7b\u89c9\u95ee\u9898\uff0cRAG\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u8fc7\u68c0\u7d22\u673a\u5236\u8bbf\u95ee\u77e5\u8bc6\u6570\u636e\u5e93\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86\u591a\u6a21\u6001RAG\u6d41\u7a0b\u7684\u4e09\u4e2a\u9636\u6bb5\uff1a\u68c0\u7d22\u3001\u91cd\u6392\u5e8f\u548c\u751f\u6210\uff0c\u5e76\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u4ee3\u7406\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRAG\u5728\u4e0d\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u5e73\u5747\u63d0\u5347\u4e86LVLMs 5%\u7684\u6027\u80fd\u3002", "conclusion": "RAG\u4e3aLVLMs\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6539\u8fdb\u8def\u5f84\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "keywords": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u591a\u6a21\u6001\u3001\u5e7b\u89c9\u95ee\u9898\u3001\u52a8\u6001\u5e94\u7528"}}
{"id": "2506.03193", "pdf": "https://arxiv.org/pdf/2506.03193", "abs": "https://arxiv.org/abs/2506.03193", "authors": ["Ekram Alam", "Abu Sufian", "Paramartha Dutta", "Marco Leo"], "title": "Human Fall Detection using Transfer Learning-based 3D CNN", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Unintentional or accidental falls are one of the significant health issues in\nsenior persons. The population of senior persons is increasing steadily. So,\nthere is a need for an automated fall detection monitoring system. This paper\nintroduces a vision-based fall detection system using a pre-trained 3D CNN.\nUnlike 2D CNN, 3D CNN extracts not only spatial but also temporal features. The\nproposed model leverages the original learned weights of a 3D CNN model\npre-trained on the Sports1M dataset to extract the spatio-temporal features.\nOnly the SVM classifier was trained, which saves the time required to train the\n3D CNN. Stratified shuffle five split cross-validation has been used to split\nthe dataset into training and testing data. Extracted features from the\nproposed 3D CNN model were fed to an SVM classifier to classify the activity as\nfall or ADL. Two datasets, GMDCSA and CAUCAFall, were utilized to conduct the\nexperiment. The source code for this work can be accessed via the following\nlink: https://github.com/ekramalam/HFD_3DCNN.", "AI": {"tldr": "\u5229\u7528\u9884\u8bad\u7ec3\u76843D CNN\u548cSVM\u5206\u7c7b\u5668\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u8001\u5e74\u4eba\u5065\u5eb7\u76d1\u6d4b\u3002", "motivation": "\u8001\u5e74\u4eba\u610f\u5916\u8dcc\u5012\u662f\u4e00\u4e2a\u4e25\u91cd\u7684\u5065\u5eb7\u95ee\u9898\uff0c\u968f\u7740\u8001\u5e74\u4eba\u53e3\u589e\u52a0\uff0c\u9700\u8981\u81ea\u52a8\u5316\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u76843D CNN\u63d0\u53d6\u65f6\u7a7a\u7279\u5f81\uff0c\u4ec5\u8bad\u7ec3SVM\u5206\u7c7b\u5668\uff0c\u4f7f\u7528\u5206\u5c42\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u3002", "result": "\u5728GMDCSA\u548cCAUCAFall\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u548cSVM\u5206\u7c7b\u5668\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8dcc\u5012\u68c0\u6d4b\u3002", "keywords": "\u8dcc\u5012\u68c0\u6d4b,3D CNN,SVM,\u8001\u5e74\u4eba\u5065\u5eb7"}}
{"id": "2506.03737", "pdf": "https://arxiv.org/pdf/2506.03737", "abs": "https://arxiv.org/abs/2506.03737", "authors": ["Hao Yu", "Tangyu Jiang", "Shuning Jia", "Shannan Yan", "Shunning Liu", "Haolong Qian", "Guanghao Li", "Shuting Dong", "Huaisong Zhang", "Chun Yuan"], "title": "ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The Transformer architecture has revolutionized various regions since it was\nproposed, and its effectiveness largely depends on the ability to encode\npositional information. Traditional position encoding methods exhibit\nsignificant limitations due to lack of robustness and flexibility of position.\nTherefore, Rotary Positional Encoding (RoPE) was proposed to alleviate these\nissues, which integrates positional information by rotating the embeddings in\nthe attention mechanism. However, RoPE requires manually defined rotation\nmatrices with limited transformation space, constraining the model's capacity.\nIn this work, we propose ComRoPE, which generalizes RoPE by defining it in\nterms of trainable commuting angle matrices. Specifically, we demonstrate that\npairwise commutativity of these matrices is essential for RoPE to achieve\nscalability and positional robustness. We formally define the RoPE Equation,\nwhich is an essential condition that ensures consistent performance with\nposition offsets. Based on the theoretical analysis, we present two types of\ntrainable commuting angle matrices as sufficient solutions to the RoPE\nequation, which significantly improve performance, surpassing the current\nstate-of-the-art method by 1.6% at training resolution and 2.9% at higher\nresolution on the ImageNet-1K dataset. Furthermore, our framework shows\nversatility in generalizing to existing RoPE formulations and offering new\ninsights for future positional encoding research. To ensure reproducibility,\nthe source code and instructions are available at\nhttps://github.com/Longin-Yu/ComRoPE", "AI": {"tldr": "\u63d0\u51faComRoPE\uff0c\u901a\u8fc7\u8bad\u7ec3\u53ef\u4ea4\u6362\u89d2\u77e9\u9635\u6539\u8fdbRoPE\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u5728ImageNet-1K\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u7075\u6d3b\u6027\u9650\u5236\u6027\u80fd\uff0cRoPE\u867d\u6539\u8fdb\u4f46\u53d7\u9650\u4e8e\u624b\u52a8\u5b9a\u4e49\u65cb\u8f6c\u77e9\u9635\u3002", "method": "\u5f15\u5165\u53ef\u4ea4\u6362\u89d2\u77e9\u9635\uff0c\u5b9a\u4e49RoPE\u65b9\u7a0b\uff0c\u786e\u4fdd\u4f4d\u7f6e\u504f\u79fb\u65f6\u7684\u6027\u80fd\u4e00\u81f4\u6027\uff0c\u5e76\u5c55\u793a\u4e24\u79cd\u77e9\u9635\u89e3\u51b3\u65b9\u6848\u3002", "result": "ComRoPE\u5728ImageNet-1K\u4e0a\u8bad\u7ec3\u5206\u8fa8\u7387\u63d0\u53471.6%\uff0c\u9ad8\u5206\u8fa8\u7387\u63d0\u53472.9%\u3002", "conclusion": "ComRoPE\u6846\u67b6\u6cdb\u5316\u6027\u5f3a\uff0c\u4e3a\u672a\u6765\u4f4d\u7f6e\u7f16\u7801\u7814\u7a76\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "keywords": "Transformer, \u4f4d\u7f6e\u7f16\u7801, RoPE, ComRoPE, \u53ef\u4ea4\u6362\u77e9\u9635"}}
{"id": "2506.03740", "pdf": "https://arxiv.org/pdf/2506.03740", "abs": "https://arxiv.org/abs/2506.03740", "authors": ["Jianfeng Wu", "Nannan Xu"], "title": "SAAT: Synergistic Alternating Aggregation Transformer for Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Single image super-resolution is a well-known downstream task which aims to\nrestore low-resolution images into high-resolution images. At present, models\nbased on Transformers have shone brightly in the field of super-resolution due\nto their ability to capture long-term dependencies in information. However,\ncurrent methods typically compute self-attention in nonoverlapping windows to\nsave computational costs, and the standard self-attention computation only\nfocuses on its results, thereby neglecting the useful information across\nchannels and the rich spatial structural information generated in the\nintermediate process. Channel attention and spatial attention have,\nrespectively, brought significant improvements to various downstream visual\ntasks in terms of extracting feature dependency and spatial structure\nrelationships, but the synergistic relationship between channel and spatial\nattention has not been fully explored yet.To address these issues, we propose a\nnovel model. Synergistic Alternating Aggregation Transformer (SAAT), which can\nbetter utilize the potential information of features. In SAAT, we introduce the\nEfficient Channel & Window Synergistic Attention Group (CWSAG) and the Spatial\n& Window Synergistic Attention Group (SWSAG). On the one hand, CWSAG combines\nefficient channel attention with shifted window attention, enhancing non-local\nfeature fusion, and producing more visually appealing results. On the other\nhand, SWSAG leverages spatial attention to capture rich structured feature\ninformation, thereby enabling SAAT to more effectively extract structural\nfeatures.Extensive experimental results and ablation studies demonstrate the\neffectiveness of SAAT in the field of super-resolution. SAAT achieves\nperformance comparable to that of the state-of-the-art (SOTA) under the same\nquantity of parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86Synergistic Alternating Aggregation Transformer (SAAT)\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u901a\u9053\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u5728\u975e\u91cd\u53e0\u7a97\u53e3\u4e2d\u8ba1\u7b97\u81ea\u6ce8\u610f\u529b\u4ee5\u8282\u7701\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u5ffd\u89c6\u4e86\u8de8\u901a\u9053\u6709\u7528\u4fe1\u606f\u53ca\u4e2d\u95f4\u8fc7\u7a0b\u7684\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\u3002\u901a\u9053\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5176\u534f\u540c\u5173\u7cfb\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faSAAT\u6a21\u578b\uff0c\u5f15\u5165Efficient Channel & Window Synergistic Attention Group (CWSAG)\u548cSpatial & Window Synergistic Attention Group (SWSAG)\uff0c\u5206\u522b\u7ed3\u5408\u901a\u9053\u6ce8\u610f\u529b\u4e0e\u7a97\u53e3\u6ce8\u610f\u529b\u3001\u7a7a\u95f4\u6ce8\u610f\u529b\u4e0e\u7a97\u53e3\u6ce8\u610f\u529b\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u63d0\u53d6\u7279\u5f81\u4fe1\u606f\u3002", "result": "SAAT\u5728\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u73b0\u6709\u6700\u4f18\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0c\u4e14\u53c2\u6570\u6570\u91cf\u76f8\u540c\u3002", "conclusion": "SAAT\u901a\u8fc7\u534f\u540c\u5229\u7528\u901a\u9053\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u7684\u6027\u80fd\u3002", "keywords": "\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387, Transformer, \u901a\u9053\u6ce8\u610f\u529b, \u7a7a\u95f4\u6ce8\u610f\u529b, Synergistic Alternating Aggregation Transformer (SAAT)"}}
{"id": "2506.03196", "pdf": "https://arxiv.org/pdf/2506.03196", "abs": "https://arxiv.org/abs/2506.03196", "authors": ["Dania Herzalla", "Willian T. Lunardi", "Martin Andreoni"], "title": "Graph Neural Networks for Jamming Source Localization", "categories": ["cs.NI", "cs.CR", "cs.IT", "cs.LG", "eess.SP", "math.IT"], "comment": null, "summary": "Graph-based learning has emerged as a transformative approach for modeling\ncomplex relationships across diverse domains, yet its potential in wireless\nsecurity remains largely unexplored. In this work, we introduce the first\napplication of graph-based learning for jamming source localization, addressing\nthe imminent threat of jamming attacks in wireless networks. Unlike geometric\noptimization techniques that struggle under environmental uncertainties and\ndense interference, we reformulate localization as an inductive graph\nregression task. Our approach integrates structured node representations that\nencode local and global signal aggregation, ensuring spatial coherence and\nadaptive signal fusion. To enhance robustness, we incorporate an\nattention-based graph neural network that adaptively refines neighborhood\ninfluence and introduces a confidence-guided estimation mechanism that\ndynamically balances learned predictions with domain-informed priors. We\nevaluate our approach under complex radio frequency environments with varying\nsampling densities and signal propagation conditions, conducting comprehensive\nablation studies on graph construction, feature selection, and pooling\nstrategies. Results demonstrate that our novel graph-based learning framework\nsignificantly outperforms established localization baselines, particularly in\nchallenging scenarios with sparse and obfuscated signal information. Code is\navailable at\n[https://github.com/daniaherzalla/gnn-jamming-source-localization](https://github.com/daniaherzalla/gnn-jamming-source-localization).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u56fe\u5b66\u4e60\u7684\u5e72\u6270\u6e90\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u673a\u5236\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5e72\u6270\u653b\u51fb\u5a01\u80c1\u65e5\u76ca\u4e25\u91cd\uff0c\u73b0\u6709\u51e0\u4f55\u4f18\u5316\u65b9\u6cd5\u5728\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u548c\u5bc6\u96c6\u5e72\u6270\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u63a2\u7d22\u66f4\u9c81\u68d2\u7684\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "\u5c06\u5b9a\u4f4d\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5f52\u7eb3\u56fe\u56de\u5f52\u95ee\u9898\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u8282\u70b9\u8868\u793a\u548c\u6ce8\u610f\u529b\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u52a8\u6001\u5e73\u8861\u5b66\u4e60\u9884\u6d4b\u4e0e\u9886\u57df\u5148\u9a8c\u3002", "result": "\u5728\u590d\u6742\u5c04\u9891\u73af\u5883\u4e0b\uff0c\u65b0\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u4fe1\u53f7\u7a00\u758f\u548c\u6a21\u7cca\u7684\u573a\u666f\u4e2d\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u56fe\u5b66\u4e60\u4e3a\u65e0\u7ebf\u5b89\u5168\u9886\u57df\u7684\u5e72\u6270\u6e90\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u663e\u8457\u6027\u80fd\u4f18\u52bf\u3002", "keywords": "\u56fe\u5b66\u4e60,\u5e72\u6270\u6e90\u5b9a\u4f4d,\u65e0\u7ebf\u5b89\u5168,\u56fe\u795e\u7ecf\u7f51\u7edc,\u6ce8\u610f\u529b\u673a\u5236"}}
{"id": "2506.03755", "pdf": "https://arxiv.org/pdf/2506.03755", "abs": "https://arxiv.org/abs/2506.03755", "authors": ["Max Hellrigel-Holderbaum", "Leonard Dung"], "title": "Misalignment or misuse? The AGI alignment tradeoff", "categories": ["cs.CY", "cs.AI"], "comment": "Forthcoming in Philosophical Studies", "summary": "Creating systems that are aligned with our goals is seen as a leading\napproach to create safe and beneficial AI in both leading AI companies and the\nacademic field of AI safety. We defend the view that misaligned AGI - future,\ngenerally intelligent (robotic) AI agents - poses catastrophic risks. At the\nsame time, we support the view that aligned AGI creates a substantial risk of\ncatastrophic misuse by humans. While both risks are severe and stand in tension\nwith one another, we show that - in principle - there is room for alignment\napproaches which do not increase misuse risk. We then investigate how the\ntradeoff between misalignment and misuse looks empirically for different\ntechnical approaches to AI alignment. Here, we argue that many current\nalignment techniques and foreseeable improvements thereof plausibly increase\nrisks of catastrophic misuse. Since the impacts of AI depend on the social\ncontext, we close by discussing important social factors and suggest that to\nreduce the risk of a misuse catastrophe due to aligned AGI, techniques such as\nrobustness, AI control methods and especially good governance seem essential.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5bf9\u9f50\u4e0e\u6ee5\u7528\u4e4b\u95f4\u7684\u98ce\u9669\u5e73\u8861\uff0c\u8ba4\u4e3a\u867d\u7136\u4e24\u8005\u90fd\u5b58\u5728\u707e\u96be\u6027\u98ce\u9669\uff0c\u4f46\u67d0\u4e9b\u5bf9\u9f50\u65b9\u6cd5\u53ef\u80fd\u4e0d\u4f1a\u589e\u52a0\u6ee5\u7528\u98ce\u9669\uff0c\u5e76\u5efa\u8bae\u52a0\u5f3a\u6cbb\u7406\u4ee5\u51cf\u5c11\u98ce\u9669\u3002", "motivation": "\u7814\u7a76AI\u5bf9\u9f50\u4e0e\u6ee5\u7528\u7684\u98ce\u9669\uff0c\u65e8\u5728\u4e3a\u5b89\u5168\u4e14\u6709\u76ca\u7684AI\u53d1\u5c55\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u5206\u6790\u4e0d\u540cAI\u5bf9\u9f50\u6280\u672f\u7684\u5b9e\u8bc1\u6548\u679c\uff0c\u63a2\u8ba8\u5176\u5bf9\u6ee5\u7528\u98ce\u9669\u7684\u5f71\u54cd\u3002", "result": "\u8bb8\u591a\u73b0\u6709\u5bf9\u9f50\u6280\u672f\u53ef\u80fd\u4f1a\u589e\u52a0\u6ee5\u7528\u98ce\u9669\uff0c\u4f46\u901a\u8fc7\u6cbb\u7406\u7b49\u63aa\u65bd\u53ef\u964d\u4f4e\u98ce\u9669\u3002", "conclusion": "\u4e3a\u51cf\u5c11\u56e0AI\u5bf9\u9f50\u5bfc\u81f4\u7684\u6ee5\u7528\u98ce\u9669\uff0c\u9700\u52a0\u5f3a\u6280\u672f\u9c81\u68d2\u6027\u3001\u63a7\u5236\u65b9\u6cd5\u548c\u6cbb\u7406\u3002", "keywords": "AI\u5bf9\u9f50\uff0cAGI\uff0c\u6ee5\u7528\u98ce\u9669\uff0c\u6cbb\u7406\uff0cAI\u5b89\u5168"}}
{"id": "2506.03199", "pdf": "https://arxiv.org/pdf/2506.03199", "abs": "https://arxiv.org/abs/2506.03199", "authors": ["Giuseppe Di Caro", "Vahagn Kirakosyan", "Alexander G. Abanov", "Luca Candelori", "Nadine Hartmann", "Ernest T. Lam", "Kharen Musaelian", "Ryan Samson", "Dario Villani", "Martin T. Wells", "Richard J. Wenstrup", "Mengjia Xu"], "title": "Quantum Cognition Machine Learning for Forecasting Chromosomal Instability", "categories": ["q-bio.QM", "cs.LG", "quant-ph"], "comment": null, "summary": "The accurate prediction of chromosomal instability from the morphology of\ncirculating tumor cells (CTCs) enables real-time detection of CTCs with high\nmetastatic potential in the context of liquid biopsy diagnostics. However, it\npresents a significant challenge due to the high dimensionality and complexity\nof single-cell digital pathology data. Here, we introduce the application of\nQuantum Cognition Machine Learning (QCML), a quantum-inspired computational\nframework, to estimate morphology-predicted chromosomal instability in CTCs\nfrom patients with metastatic breast cancer. QCML leverages quantum mechanical\nprinciples to represent data as state vectors in a Hilbert space, enabling\ncontext-aware feature modeling, dimensionality reduction, and enhanced\ngeneralization without requiring curated feature selection. QCML outperforms\nconventional machine learning methods when tested on out of sample verification\nCTCs, achieving higher accuracy in identifying predicted large-scale state\ntransitions (pLST) status from CTC-derived morphology features. These\npreliminary findings support the application of QCML as a novel machine\nlearning tool with superior performance in high-dimensional, low-sample-size\nbiomedical contexts. QCML enables the simulation of cognition-like learning for\nthe identification of biologically meaningful prediction of chromosomal\ninstability from CTC morphology, offering a novel tool for CTC classification\nin liquid biopsy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u8ba4\u77e5\u673a\u5668\u5b66\u4e60\uff08QCML\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5faa\u73af\u80bf\u7624\u7ec6\u80de\uff08CTCs\uff09\u7684\u5f62\u6001\u4e2d\u9884\u6d4b\u67d3\u8272\u4f53\u4e0d\u7a33\u5b9a\u6027\u3002QCML\u5229\u7528\u91cf\u5b50\u529b\u5b66\u539f\u7406\uff0c\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7CTCs\u5f62\u6001\u9884\u6d4b\u67d3\u8272\u4f53\u4e0d\u7a33\u5b9a\u6027\u5bf9\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u9ad8\u8f6c\u79fb\u6f5c\u529b\u7684\u7ec6\u80de\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u9ad8\u7ef4\u590d\u6742\u6570\u636e\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u91c7\u7528\u91cf\u5b50\u8ba4\u77e5\u673a\u5668\u5b66\u4e60\uff08QCML\uff09\u6846\u67b6\uff0c\u5c06\u6570\u636e\u8868\u793a\u4e3a\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u72b6\u6001\u5411\u91cf\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7279\u5f81\u5efa\u6a21\u548c\u964d\u7ef4\u3002", "result": "QCML\u5728\u6837\u672c\u5916\u9a8c\u8bc1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u67d3\u8272\u4f53\u4e0d\u7a33\u5b9a\u6027\u72b6\u6001\u3002", "conclusion": "QCML\u662f\u4e00\u79cd\u9ad8\u6027\u80fd\u7684\u673a\u5668\u5b66\u4e60\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u3001\u5c0f\u6837\u672c\u7684\u751f\u7269\u533b\u5b66\u6570\u636e\uff0c\u4e3aCTCs\u5206\u7c7b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "keywords": "\u67d3\u8272\u4f53\u4e0d\u7a33\u5b9a\u6027,\u5faa\u73af\u80bf\u7624\u7ec6\u80de,\u91cf\u5b50\u8ba4\u77e5\u673a\u5668\u5b66\u4e60,\u9ad8\u7ef4\u6570\u636e,\u6db2\u4f53\u6d3b\u68c0"}}
{"id": "2506.03202", "pdf": "https://arxiv.org/pdf/2506.03202", "abs": "https://arxiv.org/abs/2506.03202", "authors": ["Itxasne Ant\u00fanez S\u00e1enz", "Ane Alberdi Aramendi", "David Dunaway", "Juling Ong", "Lara Deli\u00e8ge", "Amparo S\u00e1enz", "Anita Ahmadi Birjandi", "Noor UI Owase Jeelani", "Silvia Schievano", "Alessandro Borghi"], "title": "A combined Machine Learning and Finite Element Modelling tool for the surgical planning of craniosynostosis correction", "categories": ["eess.IV", "cs.CV", "cs.LG", "physics.med-ph"], "comment": "11 pages, 16 figures", "summary": "Craniosynostosis is a medical condition that affects the growth of babies'\nheads, caused by an early fusion of cranial sutures. In recent decades,\nsurgical treatments for craniosynostosis have significantly improved, leading\nto reduced invasiveness, faster recovery, and less blood loss. At Great Ormond\nStreet Hospital (GOSH), the main surgical treatment for patients diagnosed with\nsagittal craniosynostosis (SC) is spring assisted cranioplasty (SAC). This\nprocedure involves a 15x15 mm2 osteotomy, where two springs are inserted to\ninduce distraction. Despite the numerous advantages of this surgical technique\nfor patients, the outcome remains unpredictable due to the lack of efficient\npreoperative planning tools. The surgeon's experience and the baby's age are\ncurrently relied upon to determine the osteotomy location and spring selection.\nPrevious tools for predicting the surgical outcome of SC relied on finite\nelement modeling (FEM), which involved computed tomography (CT) imaging and\nrequired engineering expertise and lengthy calculations. The main goal of this\nresearch is to develop a real-time prediction tool for the surgical outcome of\npatients, eliminating the need for CT scans to minimise radiation exposure\nduring preoperative planning. The proposed methodology involves creating\npersonalised synthetic skulls based on three-dimensional (3D) photographs,\nincorporating population average values of suture location, skull thickness,\nand soft tissue properties. A machine learning (ML) surrogate model is employed\nto achieve the desired surgical outcome. The resulting multi-output support\nvector regressor model achieves a R2 metric of 0.95 and MSE and MAE below 0.13.\nFurthermore, in the future, this model could not only simulate various surgical\nscenarios but also provide optimal parameters for achieving a maximum cranial\nindex (CI).", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u9884\u6d4b\u77e2\u72b6\u7f1d\u65e9\u95ed\u624b\u672f\u7ed3\u679c\u7684\u5b9e\u65f6\u5de5\u5177\uff0c\u65e0\u9700CT\u626b\u63cf\uff0c\u51cf\u5c11\u8f90\u5c04\u66b4\u9732\u3002", "motivation": "\u73b0\u6709\u624b\u672f\u7ed3\u679c\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u5143\u5efa\u6a21\u548cCT\u6210\u50cf\uff0c\u8017\u65f6\u4e14\u9700\u5de5\u7a0b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u672c\u7814\u7a76\u65e8\u5728\u7b80\u5316\u6d41\u7a0b\u3002", "method": "\u57fa\u4e8e3D\u7167\u7247\u521b\u5efa\u4e2a\u6027\u5316\u5408\u6210\u9885\u9aa8\uff0c\u7ed3\u5408\u7fa4\u4f53\u5e73\u5747\u503c\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u66ff\u4ee3\u6a21\u578b\u9884\u6d4b\u624b\u672f\u7ed3\u679c\u3002", "result": "\u591a\u8f93\u51fa\u652f\u6301\u5411\u91cf\u56de\u5f52\u6a21\u578b\u7684R2\u4e3a0.95\uff0cMSE\u548cMAE\u4f4e\u4e8e0.13\u3002", "conclusion": "\u8be5\u5de5\u5177\u53ef\u5b9e\u65f6\u9884\u6d4b\u624b\u672f\u6548\u679c\uff0c\u672a\u6765\u8fd8\u53ef\u6a21\u62df\u591a\u79cd\u624b\u672f\u573a\u666f\u5e76\u63d0\u4f9b\u6700\u4f73\u53c2\u6570\u3002", "keywords": "\u77e2\u72b6\u7f1d\u65e9\u95ed, \u624b\u672f\u89c4\u5212, \u673a\u5668\u5b66\u4e60, \u5408\u6210\u9885\u9aa8, \u652f\u6301\u5411\u91cf\u56de\u5f52"}}
{"id": "2506.03487", "pdf": "https://arxiv.org/pdf/2506.03487", "abs": "https://arxiv.org/abs/2506.03487", "authors": ["Xianming Li", "Aamir Shakir", "Rui Huang", "Julius Lipp", "Jing Li"], "title": "ProRank: Prompt Warmup via Reinforcement Learning for Small Language Models Reranking", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Reranking is fundamental to information retrieval and retrieval-augmented\ngeneration, with recent Large Language Models (LLMs) significantly advancing\nreranking quality. While recent advances with LLMs have significantly improved\ndocument reranking quality, current approaches primarily rely on large-scale\nLLMs (>7B parameters) through zero-shot prompting, presenting high\ncomputational costs. Small Language Models (SLMs) offer a promising alternative\nbecause of their efficiency, but our preliminary quantitative analysis reveals\nthey struggle with understanding task prompts without fine-tuning. This limits\ntheir effectiveness for document reranking tasks. To address this issue, we\nintroduce a novel two-stage training approach, ProRank, for SLM-based document\nreranking. First, we propose a prompt warmup stage using reinforcement learning\nGRPO to steer SLMs to understand task prompts and generate more accurate\ncoarse-grained binary relevance scores for document reranking. Then, we\ncontinuously fine-tune the SLMs with a fine-grained score learning stage\nwithout introducing additional layers to further improve the reranking quality.\nComprehensive experimental results demonstrate that the proposed ProRank\nconsistently outperforms both the most advanced open-source and proprietary\nreranking models. Notably, our lightweight ProRank-0.5B model even surpasses\nthe powerful 32B LLM reranking model on the BEIR benchmark, establishing that\nproperly trained SLMs can achieve superior document reranking performance while\nmaintaining computational efficiency.", "AI": {"tldr": "ProRank\u662f\u4e00\u79cd\u7528\u4e8e\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u793a\u9884\u70ed\u548c\u7ec6\u7c92\u5ea6\u8bc4\u5206\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u6863\u91cd\u6392\u6027\u80fd\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u6863\u91cd\u6392\u4e2d\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u800c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u672a\u7ecf\u5fae\u8c03\u65f6\u7406\u89e3\u4efb\u52a1\u63d0\u793a\u7684\u80fd\u529b\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5ProRank\uff1a1) \u4f7f\u7528\u5f3a\u5316\u5b66\u4e60GRPO\u8fdb\u884c\u63d0\u793a\u9884\u70ed\uff0c\u751f\u6210\u7c97\u7c92\u5ea6\u76f8\u5173\u6027\u5206\u6570\uff1b2) \u901a\u8fc7\u7ec6\u7c92\u5ea6\u8bc4\u5206\u5b66\u4e60\u5fae\u8c03SLMs\u3002", "result": "ProRank\u5728BEIR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5148\u8fdb\u7684\u5f00\u653e\u548c\u4e13\u6709\u91cd\u6392\u6a21\u578b\uff0c\u8f7b\u91cf\u7ea7ProRank-0.5B\u751a\u81f3\u4f18\u4e8e32B LLM\u6a21\u578b\u3002", "conclusion": "\u8bad\u7ec3\u5f97\u5f53\u7684SLMs\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u80fd\u5b9e\u73b0\u66f4\u4f18\u7684\u6587\u6863\u91cd\u6392\u6027\u80fd\u3002", "keywords": "\u6587\u6863\u91cd\u6392,\u5c0f\u578b\u8bed\u8a00\u6a21\u578b,ProRank,\u5f3a\u5316\u5b66\u4e60,\u8ba1\u7b97\u6548\u7387"}}
{"id": "2506.03216", "pdf": "https://arxiv.org/pdf/2506.03216", "abs": "https://arxiv.org/abs/2506.03216", "authors": ["Arbind Agrahari Baniya", "Tsz-Kwan Lee", "Peter Eklund", "Sunil Aryal"], "title": "A Survey of Deep Learning Video Super-Resolution", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "This paper has been published in IEEE Transactions on Emerging Topics\n  in Computational Intelligence, vol. 8, no. 4, pp. 2655-2676, Aug. 2024, doi:\n  10.1109/TETCI.2024.3398015", "summary": "Video super-resolution (VSR) is a prominent research topic in low-level\ncomputer vision, where deep learning technologies have played a significant\nrole. The rapid progress in deep learning and its applications in VSR has led\nto a proliferation of tools and techniques in the literature. However, the\nusage of these methods is often not adequately explained, and decisions are\nprimarily driven by quantitative improvements. Given the significance of VSR's\npotential influence across multiple domains, it is imperative to conduct a\ncomprehensive analysis of the elements and deep learning methodologies employed\nin VSR research. This methodical analysis will facilitate the informed\ndevelopment of models tailored to specific application needs. In this paper, we\npresent an overarching overview of deep learning-based video super-resolution\nmodels, investigating each component and discussing its implications.\nFurthermore, we provide a synopsis of key components and technologies employed\nby state-of-the-art and earlier VSR models. By elucidating the underlying\nmethodologies and categorising them systematically, we identified trends,\nrequirements, and challenges in the domain. As a first-of-its-kind survey of\ndeep learning-based VSR models, this work also establishes a multi-level\ntaxonomy to guide current and future VSR research, enhancing the maturation and\ninterpretation of VSR practices for various practical applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff08VSR\uff09\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u7684\u7efc\u8ff0\uff0c\u5206\u6790\u4e86\u5176\u7ec4\u4ef6\u3001\u65b9\u6cd5\u3001\u8d8b\u52bf\u53ca\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u7ea7\u5206\u7c7b\u6cd5\u4ee5\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002", "motivation": "VSR\u5728\u591a\u9886\u57df\u6709\u6f5c\u5728\u5f71\u54cd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u5206\u6790\uff0c\u5bfc\u81f4\u6a21\u578b\u5f00\u53d1\u4e0d\u591f\u9488\u5bf9\u6027\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u7efc\u8ff0VSR\u6a21\u578b\u7684\u7ec4\u4ef6\u3001\u6280\u672f\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u5206\u7c7b\uff0c\u5efa\u7acb\u591a\u7ea7\u5206\u7c7b\u6cd5\u3002", "result": "\u63ed\u793a\u4e86VSR\u9886\u57df\u7684\u8d8b\u52bf\u3001\u9700\u6c42\u548c\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3aVSR\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u6a21\u578b\u5f00\u53d1\u66f4\u8d34\u8fd1\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "keywords": "\u89c6\u9891\u8d85\u5206\u8fa8\u7387, \u6df1\u5ea6\u5b66\u4e60, \u7efc\u8ff0, \u5206\u7c7b\u6cd5"}}
{"id": "2506.03587", "pdf": "https://arxiv.org/pdf/2506.03587", "abs": "https://arxiv.org/abs/2506.03587", "authors": ["Florian Boudin", "Akiko Aizawa"], "title": "Preface to the Special Issue of the TAL Journal on Scholarly Document Processing", "categories": ["cs.DL", "cs.CL"], "comment": null, "summary": "The rapid growth of scholarly literature makes it increasingly difficult for\nresearchers to keep up with new knowledge. Automated tools are now more\nessential than ever to help navigate and interpret this vast body of\ninformation. Scientific papers pose unique difficulties, with their complex\nlanguage, specialized terminology, and diverse formats, requiring advanced\nmethods to extract reliable and actionable insights. Large language models\n(LLMs) offer new opportunities, enabling tasks such as literature reviews,\nwriting assistance, and interactive exploration of research. This special issue\nof the TAL journal highlights research addressing these challenges and, more\nbroadly, research on natural language processing and information retrieval for\nscholarly and scientific documents.", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86\u5b66\u672f\u6587\u732e\u5feb\u901f\u589e\u957f\u5e26\u6765\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u5982\u4f55\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7b49\u81ea\u52a8\u5316\u5de5\u5177\u5904\u7406\u590d\u6742\u79d1\u5b66\u6587\u672c\uff0c\u5e76\u63d0\u51fa\u76f8\u5173\u7814\u7a76\u65b9\u5411\u7684\u5c55\u671b\u3002", "motivation": "\u5b66\u672f\u6587\u732e\u7684\u5feb\u901f\u589e\u957f\u548c\u590d\u6742\u6027\u4f7f\u5f97\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u8ddf\u4e0a\u65b0\u77e5\u8bc6\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u9ad8\u6548\u5904\u7406\u548c\u89e3\u8bfb\u8fd9\u4e9b\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7b49\u5148\u8fdb\u6280\u672f\uff0c\u652f\u6301\u6587\u732e\u7efc\u8ff0\u3001\u5199\u4f5c\u8f85\u52a9\u548c\u4ea4\u4e92\u5f0f\u7814\u7a76\u63a2\u7d22\u7b49\u4efb\u52a1\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u4fe1\u606f\u68c0\u7d22\u6280\u672f\u5904\u7406\u79d1\u5b66\u6587\u6863\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86LLMs\u5728\u6b64\u9886\u57df\u7684\u6f5c\u529b\u3002", "conclusion": "LLMs\u5728\u89e3\u51b3\u5b66\u672f\u6587\u672c\u5904\u7406\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u672a\u6765\u7814\u7a76\u5e94\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5e94\u7528\u3002", "keywords": "\u5b66\u672f\u6587\u732e,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u81ea\u7136\u8bed\u8a00\u5904\u7406,\u4fe1\u606f\u68c0\u7d22,\u79d1\u5b66\u6587\u6863"}}
{"id": "2506.03837", "pdf": "https://arxiv.org/pdf/2506.03837", "abs": "https://arxiv.org/abs/2506.03837", "authors": ["Xiao-Qi Han", "Ze-Feng Gao", "Xin-De Wang", "Zhenfeng Ouyang", "Peng-Jie Guo", "Zhong-Yi Lu"], "title": "HTSC-2025: A Benchmark Dataset of Ambient-Pressure High-Temperature Superconductors for AI-Driven Critical Temperature Prediction", "categories": ["cond-mat.supr-con", "cond-mat.mtrl-sci", "cs.AI", "cs.LG"], "comment": "7 pages, 2 figures", "summary": "The discovery of high-temperature superconducting materials holds great\nsignificance for human industry and daily life. In recent years, research on\npredicting superconducting transition temperatures using artificial\nintelligence~(AI) has gained popularity, with most of these tools claiming to\nachieve remarkable accuracy. However, the lack of widely accepted benchmark\ndatasets in this field has severely hindered fair comparisons between different\nAI algorithms and impeded further advancement of these methods. In this work,\nwe present the HTSC-2025, an ambient-pressure high-temperature superconducting\nbenchmark dataset. This comprehensive compilation encompasses theoretically\npredicted superconducting materials discovered by theoretical physicists from\n2023 to 2025 based on BCS superconductivity theory, including the renowned\nX$_2$YH$_6$ system, perovskite MXH$_3$ system, M$_3$XH$_8$ system, cage-like\nBCN-doped metal atomic systems derived from LaH$_{10}$ structural evolution,\nand two-dimensional honeycomb-structured systems evolving from MgB$_2$. The\nHTSC-2025 benchmark has been open-sourced at\nhttps://github.com/xqh19970407/HTSC-2025 and will be continuously updated. This\nbenchmark holds significant importance for accelerating the discovery of\nsuperconducting materials using AI-based methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86HTSC-2025\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u9ad8\u6e29\u8d85\u5bfc\u6750\u6599\u7814\u7a76\u4e2d\u7f3a\u4e4f\u516c\u8ba4\u57fa\u51c6\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u63a8\u52a8AI\u65b9\u6cd5\u5728\u8d85\u5bfc\u6750\u6599\u53d1\u73b0\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5f53\u524dAI\u9884\u6d4b\u8d85\u5bfc\u8f6c\u53d8\u6e29\u5ea6\u7684\u7814\u7a76\u7f3a\u4e4f\u516c\u5e73\u6bd4\u8f83\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u65b9\u6cd5\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "method": "\u63d0\u51faHTSC-2025\u6570\u636e\u96c6\uff0c\u5305\u542b\u57fa\u4e8eBCS\u7406\u8bba\u9884\u6d4b\u7684\u9ad8\u6e29\u8d85\u5bfc\u6750\u6599\uff0c\u6db5\u76d6\u591a\u79cd\u7cfb\u7edf\u3002", "result": "\u5f00\u6e90HTSC-2025\u6570\u636e\u96c6\uff0c\u4e3aAI\u7b97\u6cd5\u7684\u516c\u5e73\u6bd4\u8f83\u548c\u8d85\u5bfc\u6750\u6599\u53d1\u73b0\u63d0\u4f9b\u53ef\u9760\u57fa\u51c6\u3002", "conclusion": "HTSC-2025\u5c06\u52a0\u901fAI\u65b9\u6cd5\u5728\u8d85\u5bfc\u6750\u6599\u53d1\u73b0\u4e2d\u7684\u5e94\u7528\u3002", "keywords": "\u9ad8\u6e29\u8d85\u5bfc, AI, \u57fa\u51c6\u6570\u636e\u96c6, BCS\u7406\u8bba, HTSC-2025"}}
{"id": "2506.03872", "pdf": "https://arxiv.org/pdf/2506.03872", "abs": "https://arxiv.org/abs/2506.03872", "authors": ["Yang Xiao", "Guoan Xu", "Qiang Wu", "Wenjing Jia"], "title": "JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reconstructing 3D scenes from sparse viewpoints is a long-standing challenge\nwith wide applications. Recent advances in feed-forward 3D Gaussian sparse-view\nreconstruction methods provide an efficient solution for real-time novel view\nsynthesis by leveraging geometric priors learned from large-scale multi-view\ndatasets and computing 3D Gaussian centers via back-projection. Despite\noffering strong geometric cues, both feed-forward multi-view depth estimation\nand flow-depth joint estimation face key limitations: the former suffers from\nmislocation and artifact issues in low-texture or repetitive regions, while the\nlatter is prone to local noise and global inconsistency due to unreliable\nmatches when ground-truth flow supervision is unavailable. To overcome this, we\npropose JointSplat, a unified framework that leverages the complementarity\nbetween optical flow and depth via a novel probabilistic optimization\nmechanism. Specifically, this pixel-level mechanism scales the information\nfusion between depth and flow based on the matching probability of optical flow\nduring training. Building upon the above mechanism, we further propose a novel\nmulti-view depth-consistency loss to leverage the reliability of supervision\nwhile suppressing misleading gradients in uncertain areas. Evaluated on\nRealEstate10K and ACID, JointSplat consistently outperforms state-of-the-art\n(SOTA) methods, demonstrating the effectiveness and robustness of our proposed\nprobabilistic joint flow-depth optimization approach for high-fidelity\nsparse-view 3D reconstruction.", "AI": {"tldr": "JointSplat\u662f\u4e00\u79cd\u8054\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u4f18\u5316\u673a\u5236\u7ed3\u5408\u5149\u6d41\u548c\u6df1\u5ea6\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u89d2\u4e0b3D\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u5b9a\u4f4d\u9519\u8bef\u548c\u566a\u58f0\u95ee\u9898\u3002", "motivation": "\u7a00\u758f\u89c6\u89d2\u4e0b\u76843D\u573a\u666f\u91cd\u5efa\u5b58\u5728\u5b9a\u4f4d\u9519\u8bef\u548c\u566a\u58f0\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u7eb9\u7406\u6216\u91cd\u590d\u533a\u57df\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u8054\u5408\u5149\u6d41\u548c\u6df1\u5ea6\u7684\u6982\u7387\u4f18\u5316\u673a\u5236\uff0c\u5e76\u8bbe\u8ba1\u591a\u89c6\u56fe\u6df1\u5ea6\u4e00\u81f4\u6027\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728RealEstate10K\u548cACID\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u9ad8\u4fdd\u771f\u91cd\u5efa\u6548\u679c\u3002", "conclusion": "JointSplat\u901a\u8fc7\u6982\u7387\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u89d23D\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002", "keywords": "3D\u91cd\u5efa,\u7a00\u758f\u89c6\u89d2,\u5149\u6d41,\u6df1\u5ea6\u4f30\u8ba1,JointSplat"}}
{"id": "2506.03741", "pdf": "https://arxiv.org/pdf/2506.03741", "abs": "https://arxiv.org/abs/2506.03741", "authors": ["Rifat Mehreen Amin", "Oliver Hans K\u00fchle", "Daniel Buschek", "Andreas Butz"], "title": "PromptCanvas: Composable Prompting Workspaces Using Dynamic Widgets for Exploration and Iteration in Creative Writing", "categories": ["cs.HC", "cs.CL", "H.5.2; I.2.7"], "comment": null, "summary": "We introduce PromptCanvas, a concept that transforms prompting into a\ncomposable, widget-based experience on an infinite canvas. Users can generate,\ncustomize, and arrange interactive widgets representing various facets of their\ntext, offering greater control over AI-generated content. PromptCanvas allows\nwidget creation through system suggestions, user prompts, or manual input,\nproviding a flexible environment tailored to individual needs. This enables\ndeeper engagement with the creative process. In a lab study with 18\nparticipants, PromptCanvas outperformed a traditional conversational UI on the\nCreativity Support Index. Participants found that it reduced cognitive load,\nwith lower mental demand and frustration. Qualitative feedback revealed that\nthe visual organization of thoughts and easy iteration encouraged new\nperspectives and ideas. A follow-up field study (N=10) confirmed these results,\nshowcasing the potential of dynamic, customizable interfaces in improving\ncollaborative writing with AI.", "AI": {"tldr": "PromptCanvas\u662f\u4e00\u79cd\u5c06\u63d0\u793a\u8f6c\u5316\u4e3a\u53ef\u7ec4\u5408\u3001\u57fa\u4e8e\u5c0f\u90e8\u4ef6\u7684\u65e0\u9650\u753b\u5e03\u4f53\u9a8c\u7684\u6982\u5ff5\uff0c\u63d0\u5347\u7528\u6237\u5bf9AI\u751f\u6210\u5185\u5bb9\u7684\u63a7\u5236\u3002", "motivation": "\u65e8\u5728\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u73af\u5883\uff0c\u4f7f\u7528\u6237\u80fd\u66f4\u6df1\u5165\u53c2\u4e0e\u521b\u4f5c\u8fc7\u7a0b\uff0c\u51cf\u5c11\u8ba4\u77e5\u8d1f\u62c5\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5efa\u8bae\u3001\u7528\u6237\u63d0\u793a\u6216\u624b\u52a8\u8f93\u5165\u521b\u5efa\u5c0f\u90e8\u4ef6\uff0c\u5e76\u5728\u5b9e\u9a8c\u5ba4\u548c\u5b9e\u5730\u7814\u7a76\u4e2d\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u5728\u521b\u9020\u529b\u652f\u6301\u6307\u6570\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5bf9\u8bdd\u5f0fUI\uff0c\u964d\u4f4e\u4e86\u5fc3\u667a\u9700\u6c42\u548c\u632b\u6298\u611f\uff0c\u4fc3\u8fdb\u65b0\u89c2\u70b9\u7684\u4ea7\u751f\u3002", "conclusion": "\u52a8\u6001\u53ef\u5b9a\u5236\u754c\u9762\u5728AI\u534f\u4f5c\u5199\u4f5c\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "keywords": "PromptCanvas, \u5c0f\u90e8\u4ef6, \u521b\u9020\u529b\u652f\u6301, \u8ba4\u77e5\u8d1f\u8377, \u534f\u4f5c\u5199\u4f5c"}}
{"id": "2506.03930", "pdf": "https://arxiv.org/pdf/2506.03930", "abs": "https://arxiv.org/abs/2506.03930", "authors": ["Yuansheng Ni", "Ping Nie", "Kai Zou", "Xiang Yue", "Wenhu Chen"], "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation.", "AI": {"tldr": "VisCode-200K\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\uff0c\u7528\u4e8ePython\u53ef\u89c6\u5316\u548c\u81ea\u6211\u4fee\u6b63\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u89c6\u5316\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\u5728\u6267\u884c\u548c\u76d1\u7763\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u8fed\u4ee3\u4ee3\u7801\u4fee\u6b63\u652f\u6301\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b200K\u793a\u4f8b\u7684VisCode-200K\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u5f00\u6e90\u4ee3\u7801\u548c\u4fee\u6b63\u5bf9\u8bdd\uff0c\u5e76\u5fae\u8c03\u6a21\u578bQwen2.5-Coder-Instruct\u3002", "result": "VisCoder\u5728PandasPlotBench\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63a5\u8fd1GPT-4o-mini\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u81ea\u8c03\u8bd5\u8bc4\u4f30\u8bc1\u660e\u4e86\u53cd\u9988\u5b66\u4e60\u7684\u4f18\u52bf\u3002", "conclusion": "\u53cd\u9988\u9a71\u52a8\u7684\u5b66\u4e60\u80fd\u663e\u8457\u63d0\u5347\u53ef\u6267\u884c\u4e14\u89c6\u89c9\u51c6\u786e\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002", "keywords": "LLMs, instruction tuning, visualization, code correction, feedback learning"}}
{"id": "2506.03272", "pdf": "https://arxiv.org/pdf/2506.03272", "abs": "https://arxiv.org/abs/2506.03272", "authors": ["My Youssef El Hafidi", "Achraf Toufah", "Mohamed Achraf Kadim"], "title": "Investigating Quantum Feature Maps in Quantum Support Vector Machines for Lung Cancer Classification", "categories": ["quant-ph", "cs.LG"], "comment": "14 pages, 7 figures", "summary": "In recent years, quantum machine learning has emerged as a promising\nintersection between quantum physics and artificial intelligence, particularly\nin domains requiring advanced pattern recognition such as healthcare. This\nstudy investigates the effectiveness of Quantum Support Vector Machines (QSVM),\nwhich leverage quantum mechanical phenomena like superposition and entanglement\nto construct high-dimensional Hilbert spaces for data classification. Focusing\non lung cancer diagnosis, a concrete and critical healthcare application, we\nanalyze how different quantum feature maps influence classification\nperformance. Using a real-world dataset of 309 patient records with significant\nclass imbalance (39 non-cancer vs. 270 cancer cases), we constructed six\nbalanced subsets for robust evaluation. QSVM models were implemented using\nQiskit and executed on the qasm simulator, employing three distinct quantum\nfeature maps: ZFeatureMap, ZZFeatureMap, and PauliFeatureMap. Performance was\nassessed using accuracy, precision, recall, specificity, and F1-score. Results\nshow that the PauliFeatureMap consistently outperformed the others, achieving\nperfect classification in three subsets and strong performance overall. These\nfindings demonstrate how quantum computational principles can be harnessed to\nenhance diagnostic capabilities, reinforcing the importance of physics-based\nmodeling in emerging AI applications within healthcare.", "AI": {"tldr": "\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\uff08QSVM\uff09\u5728\u80ba\u764c\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662fPauliFeatureMap\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u91cf\u5b50\u7279\u5f81\u6620\u5c04\u3002", "motivation": "\u63a2\u7d22\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u533b\u7597\u9886\u57df\uff0c\u5c24\u5176\u662f\u80ba\u764c\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u4f7f\u7528Qiskit\u5b9e\u73b0QSVM\u6a21\u578b\uff0c\u6d4b\u8bd5\u4e09\u79cd\u91cf\u5b50\u7279\u5f81\u6620\u5c04\uff08ZFeatureMap, ZZFeatureMap, PauliFeatureMap\uff09\uff0c\u8bc4\u4f30\u5176\u5728309\u4f8b\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u7684\u516d\u4e2a\u5e73\u8861\u5b50\u96c6\u4e0a\u7684\u5206\u7c7b\u6027\u80fd\u3002", "result": "PauliFeatureMap\u8868\u73b0\u6700\u4f73\uff0c\u5728\u4e09\u4e2a\u5b50\u96c6\u4e2d\u5b9e\u73b0\u5b8c\u7f8e\u5206\u7c7b\uff0c\u6574\u4f53\u6027\u80fd\u4f18\u8d8a\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u539f\u7406\u53ef\u663e\u8457\u63d0\u5347\u533b\u7597\u8bca\u65ad\u80fd\u529b\uff0c\u7269\u7406\u5efa\u6a21\u5728AI\u533b\u7597\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "keywords": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60, QSVM, \u80ba\u764c\u8bca\u65ad, \u91cf\u5b50\u7279\u5f81\u6620\u5c04"}}
{"id": "2506.03933", "pdf": "https://arxiv.org/pdf/2506.03933", "abs": "https://arxiv.org/abs/2506.03933", "authors": ["Jia Fu", "Yongtao Wu", "Yihang Chen", "Kunyu Peng", "Xiao Zhang", "Volkan Cevher", "Sepideh Pashami", "Anders Holst"], "title": "DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision Language Models (VLMs) have shown remarkable capabilities in\nmultimodal understanding, yet their susceptibility to perturbations poses a\nsignificant threat to their reliability in real-world applications. Despite\noften being imperceptible to humans, these perturbations can drastically alter\nmodel outputs, leading to erroneous interpretations and decisions. This paper\nintroduces DiffCAP, a novel diffusion-based purification strategy that can\neffectively neutralize adversarial corruptions in VLMs. We observe that adding\nminimal noise to an adversarially corrupted image significantly alters its\nlatent embedding with respect to VLMs. Building on this insight, DiffCAP\ncumulatively injects random Gaussian noise into adversarially perturbed input\ndata. This process continues until the embeddings of two consecutive noisy\nimages reach a predefined similarity threshold, indicating a potential approach\nto neutralize the adversarial effect. Subsequently, a pretrained diffusion\nmodel is employed to denoise the stabilized image, recovering a clean\nrepresentation suitable for the VLMs to produce an output. Through extensive\nexperiments across six datasets with three VLMs under varying attack strengths\nin three task scenarios, we show that DiffCAP consistently outperforms existing\ndefense techniques by a substantial margin. Notably, DiffCAP significantly\nreduces both hyperparameter tuning complexity and the required diffusion time,\nthereby accelerating the denoising process. Equipped with strong theoretical\nand empirical support, DiffCAP provides a robust and practical solution for\nsecurely deploying VLMs in adversarial environments.", "AI": {"tldr": "DiffCAP\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u51c0\u5316\u7b56\u7565\uff0c\u7528\u4e8e\u6709\u6548\u4e2d\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u7684\u5bf9\u6297\u6027\u7834\u574f\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u867d\u5728\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u6270\u52a8\u7684\u654f\u611f\u6027\u4f1a\u5a01\u80c1\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002DiffCAP\u65e8\u5728\u901a\u8fc7\u51c0\u5316\u5e72\u6270\uff0c\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "method": "DiffCAP\u901a\u8fc7\u9010\u6b65\u6ce8\u5165\u9ad8\u65af\u566a\u58f0\u5230\u88ab\u6270\u52a8\u7684\u56fe\u50cf\u4e2d\uff0c\u5e76\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u53bb\u566a\uff0c\u6062\u590d\u5e72\u51c0\u7684\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4efb\u52a1\u573a\u666f\u7684\u5b9e\u9a8c\u4e2d\uff0cDiffCAP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u6280\u672f\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8d85\u53c2\u6570\u8c03\u4f18\u590d\u6742\u6027\u548c\u53bb\u566a\u65f6\u95f4\u3002", "conclusion": "DiffCAP\u4e3a\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u90e8\u7f72VLMs\u63d0\u4f9b\u4e86\u5f3a\u5065\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\uff0c\u5bf9\u6297\u6027\u6270\u52a8\uff0c\u6269\u6563\u6a21\u578b\uff0c\u51c0\u5316\u7b56\u7565"}}
{"id": "2506.04019", "pdf": "https://arxiv.org/pdf/2506.04019", "abs": "https://arxiv.org/abs/2506.04019", "authors": ["Neeva Oza", "Ishaan Govil", "Parul Gupta", "Dinesh Khandelwal", "Dinesh Garg", "Parag Singla"], "title": "CETBench: A Novel Dataset constructed via Transformations over Programs for Benchmarking LLMs for Code-Equivalence Checking", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.PL", "68-02 (Primary) 68T50, 68T07, 68N19, 68N30 (Secondary)", "I.2.7; I.2.6; I.2.5; D.3.0; D.3.3; D.3.1; F.3.2; F.3.1; F.3.3;\n  D.2.3; D.2.5"], "comment": null, "summary": "LLMs have been extensively used for the task of automated code generation. In\nthis work, we examine the applicability of LLMs for the related but relatively\nunexplored task of code-equivalence checking, i.e., given two programs, whether\nthey are functionally equivalent or not. This is an important problem since\nbenchmarking code equivalence can play a critical role in evaluating LLM\ncapabilities for tasks such as code re-writing and code translation. Towards\nthis end, we present CETBench - Code Equivalence with Transformations\nBenchmark, constructed via a repository of programs, where two programs in the\nrepository may be solving the same or different tasks. Each instance in our\ndataset is obtained by taking a pair of programs in the repository and applying\na random series of pre-defined code transformations, resulting in\n(non-)equivalent pairs. Our analysis on this dataset reveals a surprising\nfinding that very simple code transformations in the underlying pair of\nprograms can result in a significant drop in performance of SOTA LLMs for the\ntask of code-equivalence checking. To remedy this, we present a simple\nfine-tuning-based approach to boost LLM performance on the transformed pairs of\nprograms. Our approach for dataset generation is generic, and can be used with\nrepositories with varying program difficulty levels and allows for applying\nvarying numbers as well as kinds of transformations. In our experiments, we\nperform ablations over the difficulty level of original programs, as well as\nthe kind of transformations used in generating pairs for equivalence checking.\nOur analysis presents deep insights into the working of LLMs for the task of\ncode-equivalence, and points to the fact that they may still be far from what\ncould be termed as a semantic understanding of the underlying code.", "AI": {"tldr": "\u63a2\u8ba8 LLMs \u5728\u4ee3\u7801\u7b49\u4ef7\u6027\u68c0\u67e5\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u7684\u5fae\u8c03\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7814\u7a76 LLMs \u5728\u4ee3\u7801\u7b49\u4ef7\u6027\u68c0\u67e5\u4e2d\u7684\u5e94\u7528\uff0c\u63ed\u793a\u5176\u5c40\u9650\u6027\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa CETBench \u6570\u636e\u96c6\uff0c\u901a\u8fc7\u968f\u673a\u4ee3\u7801\u53d8\u6362\u751f\u6210\u7b49\u4ef7/\u975e\u7b49\u4ef7\u4ee3\u7801\u5bf9\uff0c\u5e76\u91c7\u7528\u5fae\u8c03\u65b9\u6cd5\u4f18\u5316 LLM \u6027\u80fd\u3002", "result": "\u53d1\u73b0\u7b80\u5355\u4ee3\u7801\u53d8\u6362\u4f1a\u663e\u8457\u964d\u4f4e LLMs \u6027\u80fd\uff0c\u4f46\u5fae\u8c03\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5176\u8868\u73b0\u3002", "conclusion": "LLMs \u5728\u4ee3\u7801\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "keywords": "LLM, \u4ee3\u7801\u7b49\u4ef7\u6027\u68c0\u67e5, CETBench, \u5fae\u8c03"}}
{"id": "2506.03317", "pdf": "https://arxiv.org/pdf/2506.03317", "abs": "https://arxiv.org/abs/2506.03317", "authors": ["Yuntian Wang", "Zafer Yilmaz", "Yuhang Li", "Edward Liu", "Eric Ahlberg", "Farid Ghahari", "Ertugrul Taciroglu", "Aydogan Ozcan"], "title": "Structural Vibration Monitoring with Diffractive Optical Processors", "categories": ["physics.optics", "cs.CV", "cs.LG", "physics.app-ph"], "comment": "33 Pages, 8 Figures, 1 Table", "summary": "Structural Health Monitoring (SHM) is vital for maintaining the safety and\nlongevity of civil infrastructure, yet current solutions remain constrained by\ncost, power consumption, scalability, and the complexity of data processing.\nHere, we present a diffractive vibration monitoring system, integrating a\njointly optimized diffractive layer with a shallow neural network-based backend\nto remotely extract 3D structural vibration spectra, offering a low-power,\ncost-effective and scalable solution. This architecture eliminates the need for\ndense sensor arrays or extensive data acquisition; instead, it uses a\nspatially-optimized passive diffractive layer that encodes 3D structural\ndisplacements into modulated light, captured by a minimal number of detectors\nand decoded in real-time by shallow and low-power neural networks to\nreconstruct the 3D displacement spectra of structures. The diffractive system's\nefficacy was demonstrated both numerically and experimentally using\nmillimeter-wave illumination on a laboratory-scale building model with a\nprogrammable shake table. Our system achieves more than an order-of-magnitude\nimprovement in accuracy over conventional optics or separately trained modules,\nestablishing a foundation for high-throughput 3D monitoring of structures.\nBeyond SHM, the 3D vibration monitoring capabilities of this cost-effective and\ndata-efficient framework establish a new computational sensing modality with\npotential applications in disaster resilience, aerospace diagnostics, and\nautonomous navigation, where energy efficiency, low latency, and\nhigh-throughput are critical.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u884d\u5c04\u632f\u52a8\u76d1\u6d4b\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7684\u884d\u5c04\u5c42\u548c\u6d45\u5c42\u795e\u7ecf\u7f51\u7edc\u540e\u7aef\uff0c\u5b9e\u73b0\u4e86\u4f4e\u529f\u8017\u3001\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u53ef\u6269\u5c55\u7684\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\uff08SHM\uff09\u3002", "motivation": "\u73b0\u6709\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\uff08SHM\uff09\u65b9\u6848\u53d7\u6210\u672c\u3001\u529f\u8017\u3001\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u5904\u7406\u590d\u6742\u6027\u9650\u5236\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u8054\u5408\u4f18\u5316\u7684\u88ab\u52a8\u884d\u5c04\u5c42\u548c\u6d45\u5c42\u795e\u7ecf\u7f51\u7edc\u540e\u7aef\uff0c\u5c06\u7ed3\u67843D\u4f4d\u79fb\u7f16\u7801\u4e3a\u8c03\u5236\u5149\uff0c\u7531\u5c11\u91cf\u63a2\u6d4b\u5668\u6355\u83b7\u5e76\u5b9e\u65f6\u89e3\u7801\u3002", "result": "\u7cfb\u7edf\u901a\u8fc7\u6beb\u7c73\u6ce2\u7167\u660e\u5728\u5b9e\u9a8c\u5ba4\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u6bd4\u4f20\u7edf\u5149\u5b66\u6216\u5355\u72ec\u8bad\u7ec3\u6a21\u5757\u63d0\u5347\u8d85\u8fc7\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u7ed3\u67843D\u76d1\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5728\u707e\u96be\u6062\u590d\u3001\u822a\u7a7a\u822a\u5929\u8bca\u65ad\u7b49\u9886\u57df\u5177\u6709\u6f5c\u5728\u5e94\u7528\u3002", "keywords": "\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b, 3D\u632f\u52a8\u76d1\u6d4b, \u884d\u5c04\u5149\u5b66, \u6d45\u5c42\u795e\u7ecf\u7f51\u7edc, \u4f4e\u529f\u8017"}}
{"id": "2506.04039", "pdf": "https://arxiv.org/pdf/2506.04039", "abs": "https://arxiv.org/abs/2506.04039", "authors": ["Jiulong Wu", "Zhengliang Shi", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin", "Lingyong Yan", "Min Cao", "Min Zhang"], "title": "Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Visual Language Models (LVLMs) have demonstrated impressive\ncapabilities across multiple tasks. However, their trustworthiness is often\nchallenged by hallucinations, which can be attributed to the modality\nmisalignment and the inherent hallucinations of their underlying Large Language\nModels (LLMs) backbone. Existing preference alignment methods focus on aligning\nmodel responses with human preferences while neglecting image-text modality\nalignment, resulting in over-reliance on LLMs and hallucinations. In this\npaper, we propose Entity-centric Multimodal Preference Optimization (EMPO),\nwhich achieves enhanced modality alignment than existing human preference\nalignment methods. Besides, to overcome the scarcity of high-quality multimodal\npreference data, we utilize open-source instruction datasets to automatically\nconstruct high-quality preference data across three aspects: image,\ninstruction, and response. Experiments on two human preference datasets and\nfive multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,\ne.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on\nMM-HalBench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEntity-centric Multimodal Preference Optimization (EMPO)\uff0c\u901a\u8fc7\u589e\u5f3a\u6a21\u6001\u5bf9\u9f50\u548c\u81ea\u52a8\u6784\u5efa\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u504f\u597d\u6570\u636e\uff0c\u6709\u6548\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u73b0\u8c61\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u56e0\u6a21\u6001\u672a\u5bf9\u9f50\u548c\u5e95\u5c42LLM\u7684\u56fa\u6709\u5e7b\u89c9\u800c\u5f15\u53d1\u4fe1\u4efb\u95ee\u9898\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faEMPO\u65b9\u6cd5\uff0c\u589e\u5f3a\u6a21\u6001\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u6307\u4ee4\u6570\u636e\u96c6\u81ea\u52a8\u6784\u5efa\u56fe\u50cf\u3001\u6307\u4ee4\u548c\u54cd\u5e94\u4e09\u65b9\u9762\u7684\u9ad8\u8d28\u91cf\u504f\u597d\u6570\u636e\u3002", "result": "\u5728\u4e24\u4e2a\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6\u548c\u4e94\u4e2a\u591a\u6a21\u6001\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982\u5728Object-HalBench\u4e0a\u5e7b\u89c9\u7387\u964d\u4f4e85.9%\uff0c\u5728MM-HalBench\u4e0a\u964d\u4f4e49.8%\u3002", "conclusion": "EMPO\u901a\u8fc7\u6539\u8fdb\u6a21\u6001\u5bf9\u9f50\u548c\u6570\u636e\u8d28\u91cf\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u3002", "keywords": "\u591a\u6a21\u6001\u5bf9\u9f50\u3001EMPO\u3001\u5e7b\u89c9\u51cf\u5c11\u3001\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b"}}
{"id": "2506.04006", "pdf": "https://arxiv.org/pdf/2506.04006", "abs": "https://arxiv.org/abs/2506.04006", "authors": ["Fernando de Meer Pardo", "Branka Hadji Misheva", "Martin Braschler", "Kurt Stockinger"], "title": "TransClean: Finding False Positives in Multi-Source Entity Matching under Real-World Conditions via Transitive Consistency", "categories": ["cs.DB", "cs.AI", "cs.LG"], "comment": null, "summary": "We present TransClean, a method for detecting false positive predictions of\nentity matching algorithms under real-world conditions characterized by\nlarge-scale, noisy, and unlabeled multi-source datasets that undergo\ndistributional shifts. TransClean is explicitly designed to operate with\nmultiple data sources in an efficient, robust and fast manner while accounting\nfor edge cases and requiring limited manual labeling. TransClean leverages the\nTransitive Consistency of a matching, a measure of the consistency of a\npairwise matching model f_theta on the matching it produces G_f_theta, based\nboth on its predictions on directly evaluated record pairs and its predictions\non implied record pairs. TransClean iteratively modifies a matching through\ngradually removing false positive matches while removing as few true positive\nmatches as possible. In each of these steps, the estimation of the Transitive\nConsistency is exclusively done through model evaluations and produces\nquantities that can be used as proxies of the amounts of true and false\npositives in the matching while not requiring any manual labeling, producing an\nestimate of the quality of the matching and indicating which record groups are\nlikely to contain false positives. In our experiments, we compare combining\nTransClean with a naively trained pairwise matching model (DistilBERT) and with\na state-of-the-art end-to-end matching method (CLER) and illustrate the\nflexibility of TransClean in being able to detect most of the false positives\nof either setup across a variety of datasets. Our experiments show that\nTransClean induces an average +24.42 F1 score improvement for entity matching\nin a multi-source setting when compared to traditional pair-wise matching\nalgorithms.", "AI": {"tldr": "TransClean\u662f\u4e00\u79cd\u68c0\u6d4b\u5b9e\u4f53\u5339\u914d\u7b97\u6cd5\u5047\u9633\u6027\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u6e90\u3001\u566a\u58f0\u5927\u4e14\u65e0\u6807\u8bb0\u7684\u6570\u636e\u573a\u666f\uff0c\u901a\u8fc7\u5229\u7528\u4f20\u9012\u4e00\u81f4\u6027\u6539\u8fdb\u5339\u914d\u8d28\u91cf\u3002", "motivation": "\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u591a\u6e90\u3001\u566a\u58f0\u5927\u4e14\u65e0\u6807\u8bb0\u7684\u6570\u636e\u96c6\u5206\u5e03\u53d8\u5316\u9891\u7e41\uff0c\u4f20\u7edf\u7684\u5b9e\u4f53\u5339\u914d\u7b97\u6cd5\u5bb9\u6613\u4ea7\u751f\u5047\u9633\u6027\uff0c\u4e9f\u9700\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u65e0\u9700\u5927\u91cf\u4eba\u5de5\u6807\u8bb0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u95ee\u9898\u3002", "method": "TransClean\u57fa\u4e8e\u4f20\u9012\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u8fed\u4ee3\u8c03\u6574\u5339\u914d\uff0c\u9010\u6b65\u53bb\u9664\u5047\u9633\u6027\u540c\u65f6\u4fdd\u7559\u771f\u9633\u6027\uff0c\u4ec5\u4f9d\u8d56\u6a21\u578b\u8bc4\u4f30\u65e0\u9700\u4eba\u5de5\u6807\u8bb0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTransClean\u5728\u591a\u6e90\u8bbe\u7f6e\u4e0b\u5e73\u5747\u63d0\u9ad8F1\u5206\u657024.42\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5339\u914d\u7b97\u6cd5\u3002", "conclusion": "TransClean\u80fd\u6709\u6548\u68c0\u6d4b\u5047\u9633\u6027\uff0c\u663e\u8457\u63d0\u5347\u5b9e\u4f53\u5339\u914d\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73b0\u5b9e\u573a\u666f\u3002", "keywords": "TransClean, \u5b9e\u4f53\u5339\u914d, \u4f20\u9012\u4e00\u81f4\u6027, \u5047\u9633\u6027\u68c0\u6d4b, \u591a\u6e90\u6570\u636e"}}
{"id": "2506.03321", "pdf": "https://arxiv.org/pdf/2506.03321", "abs": "https://arxiv.org/abs/2506.03321", "authors": ["Victor H. Cid", "James Mork"], "title": "Enhancing Automatic PT Tagging for MEDLINE Citations Using Transformer-Based Models", "categories": ["cs.DL", "cs.LG", "I.2.7; H.3.3; H.3.5"], "comment": "26 pages, 8 tables, 3 figures", "summary": "We investigated the feasibility of predicting Medical Subject Headings (MeSH)\nPublication Types (PTs) from MEDLINE citation metadata using pre-trained\nTransformer-based models BERT and DistilBERT. This study addresses limitations\nin the current automated indexing process, which relies on legacy NLP\nalgorithms. We evaluated monolithic multi-label classifiers and binary\nclassifier ensembles to enhance the retrieval of biomedical literature. Results\ndemonstrate the potential of Transformer models to significantly improve PT\ntagging accuracy, paving the way for scalable, efficient biomedical indexing.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4f7f\u7528\u9884\u8bad\u7ec3\u7684Transformer\u6a21\u578b\u9884\u6d4bMeSH\u51fa\u7248\u7269\u7c7b\u578b\u7684\u53ef\u884c\u6027\uff0c\u4ee5\u6539\u8fdb\u5f53\u524d\u81ea\u52a8\u5316\u7d22\u5f15\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5316\u7d22\u5f15\u4f9d\u8d56\u4f20\u7edfNLP\u7b97\u6cd5\uff0c\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528BERT\u548cDistilBERT\u6a21\u578b\uff0c\u8bc4\u4f30\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u548c\u4e8c\u5143\u5206\u7c7b\u5668\u96c6\u6210\u65b9\u6cd5\u3002", "result": "Transformer\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86PT\u6807\u6ce8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "Transformer\u6a21\u578b\u4e3a\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u7684\u751f\u7269\u533b\u5b66\u7d22\u5f15\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "MeSH, Transformer, BERT, DistilBERT, \u751f\u7269\u533b\u5b66\u7d22\u5f15"}}
{"id": "2506.04013", "pdf": "https://arxiv.org/pdf/2506.04013", "abs": "https://arxiv.org/abs/2506.04013", "authors": ["Seymanur Akti", "Tuan Nam Nguyen", "Alexander Waibel"], "title": "Towards Better Disentanglement in Non-Autoregressive Zero-Shot Expressive Voice Conversion", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Expressive voice conversion aims to transfer both speaker identity and\nexpressive attributes from a target speech to a given source speech. In this\nwork, we improve over a self-supervised, non-autoregressive framework with a\nconditional variational autoencoder, focusing on reducing source timbre leakage\nand improving linguistic-acoustic disentanglement for better style transfer. To\nminimize style leakage, we use multilingual discrete speech units for content\nrepresentation and reinforce embeddings with augmentation-based similarity loss\nand mix-style layer normalization. To enhance expressivity transfer, we\nincorporate local F0 information via cross-attention and extract style\nembeddings enriched with global pitch and energy features. Experiments show our\nmodel outperforms baselines in emotion and speaker similarity, demonstrating\nsuperior style adaptation and reduced source style leakage.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6539\u8fdb\u4e86\u57fa\u4e8e\u81ea\u76d1\u7763\u548c\u975e\u81ea\u56de\u5f52\u6846\u67b6\u7684\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u4ee5\u51cf\u5c11\u6e90\u97f3\u8272\u6cc4\u6f0f\u5e76\u63d0\u5347\u98ce\u683c\u8fc1\u79fb\u6548\u679c\u3002", "motivation": "\u65e8\u5728\u540c\u65f6\u8f6c\u79fb\u76ee\u6807\u8bed\u97f3\u4e2d\u7684\u8bf4\u8bdd\u4eba\u8eab\u4efd\u548c\u60c5\u611f\u5c5e\u6027\u5230\u6e90\u8bed\u97f3\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6e90\u97f3\u8272\u6cc4\u6f0f\u548c\u8bed\u8a00-\u58f0\u5b66\u89e3\u8026\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u8bed\u8a00\u79bb\u6563\u8bed\u97f3\u5355\u5143\u8868\u793a\u5185\u5bb9\uff0c\u5e76\u901a\u8fc7\u589e\u5f3a\u5d4c\u5165\u3001\u76f8\u4f3c\u6027\u635f\u5931\u548c\u6df7\u5408\u98ce\u683c\u5c42\u5f52\u4e00\u5316\u6765\u51cf\u5c11\u98ce\u683c\u6cc4\u6f0f\uff1b\u5f15\u5165\u5c40\u90e8F0\u4fe1\u606f\u53ca\u5168\u5c40\u97f3\u9ad8\u548c\u80fd\u91cf\u7279\u5f81\u4ee5\u589e\u5f3a\u60c5\u611f\u8f6c\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u60c5\u611f\u548c\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5c55\u73b0\u4e86\u66f4\u597d\u7684\u98ce\u683c\u9002\u5e94\u6027\u548c\u66f4\u5c11\u7684\u6e90\u98ce\u683c\u6cc4\u6f0f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8bed\u97f3\u8f6c\u6362\u4efb\u52a1\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u98ce\u683c\u8fc1\u79fb\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "keywords": "\u8bed\u97f3\u8f6c\u6362, \u60c5\u611f\u8f6c\u6362, \u53d8\u5206\u81ea\u7f16\u7801\u5668, \u81ea\u76d1\u7763\u5b66\u4e60"}}
{"id": "2506.04036", "pdf": "https://arxiv.org/pdf/2506.04036", "abs": "https://arxiv.org/abs/2506.04036", "authors": ["Wei Wenying", "Zhao Kaifa", "Xue Lei", "Fan Ming"], "title": "Privacy and Security Threat for OpenAI GPTs", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) demonstrate powerful information handling\ncapabilities and are widely integrated into chatbot applications. OpenAI\nprovides a platform for developers to construct custom GPTs, extending\nChatGPT's functions and integrating external services. Since its release in\nNovember 2023, over 3 million custom GPTs have been created. However, such a\nvast ecosystem also conceals security and privacy threats. For developers,\ninstruction leaking attacks threaten the intellectual property of instructions\nin custom GPTs through carefully crafted adversarial prompts. For users,\nunwanted data access behavior by custom GPTs or integrated third-party services\nraises significant privacy concerns. To systematically evaluate the scope of\nthreats in real-world LLM applications, we develop three phases instruction\nleaking attacks target GPTs with different defense level. Our widespread\nexperiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are\nvulnerable to instruction leaking attacks via one or more adversarial prompts,\nand half of the remaining GPTs can also be attacked through multiround\nconversations. We also developed a framework to assess the effectiveness of\ndefensive strategies and identify unwanted behaviors in custom GPTs. Our\nfindings show that 77.5% of custom GPTs with defense strategies are vulnerable\nto basic instruction leaking attacks. Additionally, we reveal that 738 custom\nGPTs collect user conversational information, and identified 8 GPTs exhibiting\ndata access behaviors that are unnecessary for their intended functionalities.\nOur findings raise awareness among GPT developers about the importance of\nintegrating specific defensive strategies in their instructions and highlight\nusers' concerns about data privacy when using LLM-based applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u81ea\u5b9a\u4e49GPT\u7684\u5b89\u5168\u4e0e\u9690\u79c1\u5a01\u80c1\uff0c\u901a\u8fc7\u5b9e\u9a8c\u63ed\u793a\u4e86\u6307\u4ee4\u6cc4\u9732\u653b\u51fb\u7684\u9ad8\u6210\u529f\u7387\u53ca\u5176\u9632\u5fa1\u7b56\u7565\u7684\u4e0d\u8db3\u3002", "motivation": "OpenAI\u7684\u81ea\u5b9a\u4e49GPT\u751f\u6001\u7cfb\u7edf\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u5176\u6f5c\u5728\u7684\u5b89\u5168\u548c\u9690\u79c1\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86\u4e09\u9636\u6bb5\u6307\u4ee4\u6cc4\u9732\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u6d4b\u8bd5\u4e8610,000\u4e2a\u771f\u5b9eGPT\u7684\u9632\u5fa1\u6548\u679c\uff0c\u540c\u65f6\u6784\u5efa\u4e86\u8bc4\u4f30\u6846\u67b6\u5206\u6790\u81ea\u5b9a\u4e49GPT\u7684\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c98.8%\u7684\u81ea\u5b9a\u4e49GPT\u6613\u53d7\u6307\u4ee4\u6cc4\u9732\u653b\u51fb\uff0c77.5%\u7684\u9632\u5fa1\u7b56\u7565\u65e0\u6548\uff0c\u4e14\u53d1\u73b0738\u4e2aGPT\u5b58\u5728\u6536\u96c6\u7528\u6237\u5bf9\u8bdd\u6570\u636e\u7684\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u5f00\u53d1\u8005\u548c\u7528\u6237\u91cd\u89c6\u81ea\u5b9a\u4e49GPT\u7684\u5b89\u5168\u4e0e\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u9632\u5fa1\u7b56\u7565\u7684\u5fc5\u8981\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u81ea\u5b9a\u4e49GPT,\u9690\u79c1\u4fdd\u62a4,\u6307\u4ee4\u6cc4\u9732\u653b\u51fb,\u5b89\u5168\u8bc4\u4f30"}}
{"id": "2506.04141", "pdf": "https://arxiv.org/pdf/2506.04141", "abs": "https://arxiv.org/abs/2506.04141", "authors": ["Kejian Zhu", "Zhuoran Jin", "Hongbang Yuan", "Jiachun Li", "Shangqing Tu", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos", "categories": ["cs.CV", "cs.CL"], "comment": "Project Page: https://mmr-v.github.io", "summary": "The sequential structure of videos poses a challenge to the ability of\nmultimodal large language models (MLLMs) to locate multi-frame evidence and\nconduct multimodal reasoning. However, existing video benchmarks mainly focus\non understanding tasks, which only require models to match frames mentioned in\nthe question (hereafter referred to as \"question frame\") and perceive a few\nadjacent frames. To address this gap, we propose MMR-V: A Benchmark for\nMultimodal Deep Reasoning in Videos. The benchmark is characterized by the\nfollowing features. (1) Long-range, multi-frame reasoning: Models are required\nto infer and analyze evidence frames that may be far from the question frame.\n(2) Beyond perception: Questions cannot be answered through direct perception\nalone but require reasoning over hidden information. (3) Reliability: All tasks\nare manually annotated, referencing extensive real-world user understanding to\nalign with common perceptions. (4) Confusability: Carefully designed distractor\nannotation strategies to reduce model shortcuts. MMR-V consists of 317 videos\nand 1,257 tasks. Our experiments reveal that current models still struggle with\nmulti-modal reasoning; even the best-performing model, o4-mini, achieves only\n52.5% accuracy. Additionally, current reasoning enhancement strategies\n(Chain-of-Thought and scaling test-time compute) bring limited gains. Further\nanalysis indicates that the CoT demanded for multi-modal reasoning differs from\nit in textual reasoning, which partly explains the limited performance gains.\nWe hope that MMR-V can inspire further research into enhancing multi-modal\nreasoning capabilities.", "AI": {"tldr": "MMR-V\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u9891\u591a\u6a21\u6001\u6df1\u5ea6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5f3a\u8c03\u957f\u8ddd\u79bb\u591a\u5e27\u63a8\u7406\u548c\u8d85\u8d8a\u611f\u77e5\u7684\u9690\u85cf\u4fe1\u606f\u5206\u6790\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u7406\u89e3\u4efb\u52a1\uff0c\u800c\u7f3a\u4e4f\u5bf9\u591a\u5e27\u8bc1\u636e\u5b9a\u4f4d\u548c\u591a\u6a21\u6001\u6df1\u5ea6\u63a8\u7406\u7684\u8bc4\u4f30\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86MMR-V\u3002", "method": "\u63d0\u51fa\u4e86MMR-V\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5176\u7279\u70b9\u5305\u62ec\uff1a(1)\u957f\u8ddd\u79bb\u591a\u5e27\u63a8\u7406\uff0c(2)\u8d85\u8d8a\u611f\u77e5\u7684\u95ee\u9898\u8bbe\u8ba1\uff0c(3)\u624b\u52a8\u6807\u6ce8\u7684\u53ef\u9760\u6027\uff0c(4)\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5e72\u6270\u7b56\u7565\u3002\u5305\u542b317\u4e2a\u89c6\u9891\u548c1,257\u4e2a\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u524d\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u4f73\u6a21\u578bo4-mini\u4ec552.5%\u51c6\u786e\u7387\uff0c\u4e14Chain-of-Thought\u548c\u6269\u5c55\u6d4b\u8bd5\u8ba1\u7b97\u7b49\u7b56\u7565\u6548\u679c\u6709\u9650\u3002", "conclusion": "MMR-V\u65e8\u5728\u63a8\u52a8\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u7684\u7814\u7a76\uff0c\u5f53\u524d\u6a21\u578b\u4ecd\u9700\u6539\u8fdb\u3002", "keywords": "\u591a\u6a21\u6001\u63a8\u7406\u3001\u89c6\u9891\u7406\u89e3\u3001\u57fa\u51c6\u6d4b\u8bd5\u3001\u957f\u8ddd\u79bb\u63a8\u7406\u3001\u9690\u85cf\u4fe1\u606f"}}
{"id": "2506.04038", "pdf": "https://arxiv.org/pdf/2506.04038", "abs": "https://arxiv.org/abs/2506.04038", "authors": ["Sven Kirchner", "Alois C. Knoll"], "title": "Generating Automotive Code: Large Language Models for Software Development and Verification in Safety-Critical Systems", "categories": ["cs.SE", "cs.AI"], "comment": "8 pages; Accepted for publication at the 36th IEEE Intelligent\n  Vehicles Symposium (IV), Cluj-Napoca, Romania, June 22-25, 2025", "summary": "Developing safety-critical automotive software presents significant\nchallenges due to increasing system complexity and strict regulatory demands.\nThis paper proposes a novel framework integrating Generative Artificial\nIntelligence (GenAI) into the Software Development Lifecycle (SDLC). The\nframework uses Large Language Models (LLMs) to automate code generation in\nlanguages such as C++, incorporating safety-focused practices such as static\nverification, test-driven development and iterative refinement. A\nfeedback-driven pipeline ensures the integration of test, simulation and\nverification for compliance with safety standards. The framework is validated\nthrough the development of an Adaptive Cruise Control (ACC) system. Comparative\nbenchmarking of LLMs ensures optimal model selection for accuracy and\nreliability. Results demonstrate that the framework enables automatic code\ngeneration while ensuring compliance with safety-critical requirements,\nsystematically integrating GenAI into automotive software engineering. This\nwork advances the use of AI in safety-critical domains, bridging the gap\nbetween state-of-the-art generative models and real-world safety requirements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u96c6\u6210\u5230\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\uff08SDLC\uff09\u4e2d\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u7b26\u5408\u5b89\u5168\u5173\u952e\u6027\u8981\u6c42\u7684\u6c7d\u8f66\u8f6f\u4ef6\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\u7cfb\u7edf\uff08ACC\uff09\u7684\u5f00\u53d1\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u5e94\u5bf9\u6c7d\u8f66\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7cfb\u7edf\u590d\u6742\u6027\u589e\u52a0\u548c\u4e25\u683c\u76d1\u7ba1\u8981\u6c42\u7684\u6311\u6218\uff0c\u63a2\u7d22GenAI\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u751f\u6210C++\u4ee3\u7801\uff0c\u7ed3\u5408\u9759\u6001\u9a8c\u8bc1\u3001\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u6d4b\u8bd5\u3001\u4eff\u771f\u548c\u9a8c\u8bc1\u7684\u53cd\u9988\u5faa\u73af\u786e\u4fdd\u7b26\u5408\u5b89\u5168\u6807\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u5e76\u6ee1\u8db3\u5b89\u5168\u5173\u952e\u6027\u8981\u6c42\uff0c\u4e3a\u6c7d\u8f66\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684GenAI\u5e94\u7528\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86AI\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\uff0c\u586b\u8865\u4e86\u5148\u8fdb\u751f\u6210\u6a21\u578b\u4e0e\u5b9e\u9645\u5b89\u5168\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "keywords": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u3001\u6c7d\u8f66\u8f6f\u4ef6\u3001\u5b89\u5168\u5173\u952e\u6027\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236"}}
{"id": "2506.03420", "pdf": "https://arxiv.org/pdf/2506.03420", "abs": "https://arxiv.org/abs/2506.03420", "authors": ["Muhammad Zubair Hasan", "Fahmida Yasmin Rifat"], "title": "Hybrid Ensemble of Segmentation-Assisted Classification and GBDT for Skin Cancer Detection with Engineered Metadata and Synthetic Lesions from ISIC 2024 Non-Dermoscopic 3D-TBP Images", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Written as per the requirements of CVPR 2025. It is a 8 page paper\n  without reference", "summary": "Skin cancer is among the most prevalent and life-threatening diseases\nworldwide, with early detection being critical to patient outcomes. This work\npresents a hybrid machine and deep learning-based approach for classifying\nmalignant and benign skin lesions using the SLICE-3D dataset from ISIC 2024,\nwhich comprises 401,059 cropped lesion images extracted from 3D Total Body\nPhotography (TBP), emulating non-dermoscopic, smartphone-like conditions. Our\nmethod combines vision transformers (EVA02) and our designed convolutional ViT\nhybrid (EdgeNeXtSAC) to extract robust features, employing a\nsegmentation-assisted classification pipeline to enhance lesion localization.\nPredictions from these models are fused with a gradient-boosted decision tree\n(GBDT) ensemble enriched by engineered features and patient-specific relational\nmetrics. To address class imbalance and improve generalization, we augment\nmalignant cases with Stable Diffusion-generated synthetic lesions and apply a\ndiagnosis-informed relabeling strategy to harmonize external datasets into a\n3-class format. Using partial AUC (pAUC) above 80 percent true positive rate\n(TPR) as the evaluation metric, our approach achieves a pAUC of 0.1755 -- the\nhighest among all configurations. These results underscore the potential of\nhybrid, interpretable AI systems for skin cancer triage in telemedicine and\nresource-constrained settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u7c7b\u6076\u6027\u548c\u826f\u6027\u76ae\u80a4\u75c5\u53d8\uff0c\u5728SLICE-3D\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u76ae\u80a4\u764c\u662f\u5168\u7403\u9ad8\u53d1\u4e14\u81f4\u547d\u7684\u75be\u75c5\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u60a3\u8005\u7ed3\u679c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u53d8\u6362\u5668\uff08EVA02\uff09\u548c\u5377\u79efViT\u6df7\u5408\u6a21\u578b\uff08EdgeNeXtSAC\uff09\uff0c\u91c7\u7528\u5206\u5272\u8f85\u52a9\u5206\u7c7b\u7ba1\u9053\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\uff08GBDT\uff09\u96c6\u6210\u9884\u6d4b\u3002", "result": "\u572880%\u771f\u5b9e\u9633\u6027\u7387\u4e0b\uff0c\u90e8\u5206AUC\u4e3a0.1755\uff0c\u4f18\u4e8e\u5176\u4ed6\u914d\u7f6e\u3002", "conclusion": "\u8bc1\u660e\u4e86\u6df7\u5408\u3001\u53ef\u89e3\u91ca\u7684AI\u7cfb\u7edf\u5728\u76ae\u80a4\u764c\u7b5b\u67e5\u4e2d\u7684\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u8fdc\u7a0b\u533b\u7597\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002", "keywords": "\u76ae\u80a4\u764c\u5206\u7c7b\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u673a\u5668\u5b66\u4e60\u3001SLICE-3D\u6570\u636e\u96c6\u3001\u6df7\u5408\u6a21\u578b"}}
{"id": "2506.03464", "pdf": "https://arxiv.org/pdf/2506.03464", "abs": "https://arxiv.org/abs/2506.03464", "authors": ["Yang Cai", "Haipeng Luo", "Chen-Yu Wei", "Weiqiang Zheng"], "title": "From Average-Iterate to Last-Iterate Convergence in Games: A Reduction and Its Applications", "categories": ["cs.GT", "cs.LG", "math.OC"], "comment": "21 pages", "summary": "The convergence of online learning algorithms in games under self-play is a\nfundamental question in game theory and machine learning. Among various notions\nof convergence, last-iterate convergence is particularly desirable, as it\nreflects the actual decisions made by the learners and captures the day-to-day\nbehavior of the learning dynamics. While many algorithms are known to converge\nin the average-iterate, achieving last-iterate convergence typically requires\nconsiderably more effort in both the design and the analysis of the algorithm.\nSomewhat surprisingly, we show in this paper that for a large family of games,\nthere exists a simple black-box reduction that transforms the average iterates\nof an uncoupled learning dynamics into the last iterates of a new uncoupled\nlearning dynamics, thus also providing a reduction from last-iterate\nconvergence to average-iterate convergence. Our reduction applies to games\nwhere each player's utility is linear in both their own strategy and the joint\nstrategy of all opponents. This family includes two-player bimatrix games and\ngeneralizations such as multi-player polymatrix games. By applying our\nreduction to the Optimistic Multiplicative Weights Update algorithm, we obtain\nnew state-of-the-art last-iterate convergence rates for uncoupled learning\ndynamics in two-player zero-sum normal-form games: (1) an $O(\\frac{\\log d}{T})$\nlast-iterate convergence rate under gradient feedback, representing an\nexponential improvement in the dependence on the dimension $d$ (i.e., the\nmaximum number of actions available to either player); and (2) an\n$\\widetilde{O}(d^{\\frac{1}{5}} T^{-\\frac{1}{5}})$ last-iterate convergence rate\nunder bandit feedback, improving upon the previous best rates of\n$\\widetilde{O}(\\sqrt{d} T^{-\\frac{1}{8}})$ and $\\widetilde{O}(\\sqrt{d}\nT^{-\\frac{1}{6}})$.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u9ed1\u76d2\u65b9\u6cd5\uff0c\u5c06\u5e73\u5747\u8fed\u4ee3\u6536\u655b\u7684\u5b66\u4e60\u52a8\u6001\u8f6c\u5316\u4e3a\u6700\u540e\u8fed\u4ee3\u6536\u655b\u7684\u52a8\u6001\uff0c\u4ece\u800c\u5728\u591a\u79cd\u6e38\u620f\u4e2d\u5b9e\u73b0\u4f18\u5316\u7684\u6700\u540e\u8fed\u4ee3\u6536\u655b\u901f\u7387\u3002", "motivation": "\u7814\u7a76\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u5728\u6e38\u620f\u81ea\u6211\u5bf9\u5f08\u4e2d\u7684\u6536\u655b\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u6700\u540e\u8fed\u4ee3\u6536\u655b\uff0c\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u5b66\u4e60\u8005\u7684\u5b9e\u9645\u51b3\u7b56\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u9ed1\u76d2\u65b9\u6cd5\uff0c\u5c06\u5e73\u5747\u8fed\u4ee3\u6536\u655b\u7684\u5b66\u4e60\u52a8\u6001\u8f6c\u5316\u4e3a\u6700\u540e\u8fed\u4ee3\u6536\u655b\u7684\u52a8\u6001\uff0c\u5e76\u5e94\u7528\u4e8e\u4e50\u89c2\u4e58\u6cd5\u6743\u91cd\u66f4\u65b0\u7b97\u6cd5\u3002", "result": "\u5728\u53cc\u4eba\u96f6\u548c\u535a\u5f08\u4e2d\uff0c\u5b9e\u73b0\u4e86\u7ef4\u5ea6\u4f9d\u8d56\u6027\u6307\u6570\u7ea7\u6539\u8fdb\u7684\u6536\u655b\u901f\u7387\uff08\u68af\u5ea6\u53cd\u9988\u4e0b$O(\\frac{\\log d}{T})$\uff0c\u5e26\u53cd\u9988\u4e0b$\\widetilde{O}(d^{\\frac{1}{5}} T^{-\\frac{1}{5}})$\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5e7f\u53d7\u6b22\u8fce\u7684\u6700\u540e\u8fed\u4ee3\u6536\u655b\u63d0\u4f9b\u4e86\u7b80\u5355\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6536\u655b\u901f\u7387\u3002", "keywords": "\u5728\u7ebf\u5b66\u4e60\u3001\u6700\u540e\u8fed\u4ee3\u6536\u655b\u3001\u6e38\u620f\u7406\u8bba\u3001\u9ed1\u76d2\u65b9\u6cd5\u3001\u4e50\u89c2\u4e58\u6cd5\u6743\u91cd\u66f4\u65b0"}}
{"id": "2506.03467", "pdf": "https://arxiv.org/pdf/2506.03467", "abs": "https://arxiv.org/abs/2506.03467", "authors": ["Hang Liu", "Anna Scaglione", "Sean Peisert"], "title": "Differentially Private Distribution Release of Gaussian Mixture Models via KL-Divergence Minimization", "categories": ["cs.IT", "cs.CR", "cs.LG", "eess.SP", "math.IT", "stat.ME"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Gaussian Mixture Models (GMMs) are widely used statistical models for\nrepresenting multi-modal data distributions, with numerous applications in data\nmining, pattern recognition, data simulation, and machine learning. However,\nrecent research has shown that releasing GMM parameters poses significant\nprivacy risks, potentially exposing sensitive information about the underlying\ndata. In this paper, we address the challenge of releasing GMM parameters while\nensuring differential privacy (DP) guarantees. Specifically, we focus on the\nprivacy protection of mixture weights, component means, and covariance\nmatrices. We propose to use Kullback-Leibler (KL) divergence as a utility\nmetric to assess the accuracy of the released GMM, as it captures the joint\nimpact of noise perturbation on all the model parameters. To achieve privacy,\nwe introduce a DP mechanism that adds carefully calibrated random perturbations\nto the GMM parameters. Through theoretical analysis, we quantify the effects of\nprivacy budget allocation and perturbation statistics on the DP guarantee, and\nderive a tractable expression for evaluating KL divergence. We formulate and\nsolve an optimization problem to minimize the KL divergence between the\nreleased and original models, subject to a given $(\\epsilon, \\delta)$-DP\nconstraint. Extensive experiments on both synthetic and real-world datasets\ndemonstrate that our approach achieves strong privacy guarantees while\nmaintaining high utility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u786e\u4fdd\u5dee\u5206\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u53d1\u5e03\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7KL\u6563\u5ea6\u8bc4\u4f30\u6a21\u578b\u51c6\u786e\u6027\uff0c\u5f15\u5165\u968f\u673a\u6270\u52a8\u673a\u5236\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u7531\u4e8eGMM\u53c2\u6570\u53d1\u5e03\u53ef\u80fd\u6cc4\u9732\u654f\u611f\u6570\u636e\uff0c\u7814\u7a76\u5982\u4f55\u5728\u4fdd\u8bc1\u5dee\u5206\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u53d1\u5e03\u8fd9\u4e9b\u53c2\u6570\u3002", "method": "\u4f7f\u7528KL\u6563\u5ea6\u4f5c\u4e3a\u8bc4\u4f30\u6807\u51c6\uff0c\u63d0\u51fa\u4e00\u79cd\u5dee\u5206\u9690\u79c1\u673a\u5236\uff0c\u901a\u8fc7\u968f\u673a\u6270\u52a8\u4fdd\u62a4GMM\u53c2\u6570\uff0c\u5e76\u4f18\u5316KL\u6563\u5ea6\u6700\u5c0f\u5316\u95ee\u9898\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5f3a\u9690\u79c1\u4fdd\u62a4\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9ad8\u6570\u636e\u6548\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86GMM\u53c2\u6570\u53d1\u5e03\u7684\u9690\u79c1\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u9ad8\u6548\u6027\u3002", "keywords": "\u9ad8\u65af\u6df7\u5408\u6a21\u578b,\u5dee\u5206\u9690\u79c1,KL\u6563\u5ea6,\u9690\u79c1\u4fdd\u62a4"}}
{"id": "2506.04058", "pdf": "https://arxiv.org/pdf/2506.04058", "abs": "https://arxiv.org/abs/2506.04058", "authors": ["Bulat Maksudov", "Kathleen Curran", "Alessandra Mileo"], "title": "Towards generating more interpretable counterfactuals via concept vectors: a preliminary study on chest X-rays", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "An essential step in deploying medical imaging models is ensuring alignment\nwith clinical knowledge and interpretability. We focus on mapping clinical\nconcepts into the latent space of generative models to identify Concept\nActivation Vectors (CAVs). Using a simple reconstruction autoencoder, we link\nuser-defined concepts to image-level features without explicit label training.\nThe extracted concepts are stable across datasets, enabling visual explanations\nthat highlight clinically relevant features. By traversing latent space along\nconcept directions, we produce counterfactuals that exaggerate or reduce\nspecific clinical features. Preliminary results on chest X-rays show promise\nfor large pathologies like cardiomegaly, while smaller pathologies remain\nchallenging due to reconstruction limits. Although not outperforming baselines,\nthis approach offers a path toward interpretable, concept-based explanations\naligned with clinical knowledge.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4e34\u5e8a\u6982\u5ff5\u6620\u5c04\u5230\u751f\u6210\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf\uff08CAVs\uff09\u4ee5\u5b9e\u73b0\u6a21\u578b\u7684\u4e34\u5e8a\u5bf9\u9f50\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u786e\u4fdd\u533b\u7597\u6210\u50cf\u6a21\u578b\u4e0e\u4e34\u5e8a\u77e5\u8bc6\u548c\u53ef\u89e3\u91ca\u6027\u5bf9\u9f50\uff0c\u907f\u514d\u4f9d\u8d56\u663e\u5f0f\u6807\u7b7e\u8bad\u7ec3\u3002", "method": "\u4f7f\u7528\u7b80\u5355\u7684\u91cd\u5efa\u81ea\u7f16\u7801\u5668\uff0c\u5c06\u7528\u6237\u5b9a\u4e49\u7684\u6982\u5ff5\u4e0e\u56fe\u50cf\u7ea7\u7279\u5f81\u5173\u8054\uff0c\u63d0\u53d6\u8de8\u6570\u636e\u96c6\u7a33\u5b9a\u7684\u6982\u5ff5\u3002", "result": "\u5728\u80f8\u7247\u6570\u636e\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5bf9\u5927\u75c5\u7406\uff08\u5982\u5fc3\u810f\u80a5\u5927\uff09\u6709\u6548\uff0c\u4f46\u5bf9\u5c0f\u75c5\u7406\u6548\u679c\u6709\u9650\u3002\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u904d\u5386\u751f\u6210\u4e86\u4e34\u5e8a\u7279\u5f81\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002", "conclusion": "\u5c3d\u7ba1\u672a\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f46\u4e3a\u4e34\u5e8a\u77e5\u8bc6\u5bf9\u9f50\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u8def\u5f84\u3002", "keywords": "\u533b\u7597\u6210\u50cf\u3001\u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf\u3001\u6f5c\u5728\u7a7a\u95f4\u3001\u53ef\u89e3\u91ca\u6027\u3001\u81ea\u7f16\u7801\u5668"}}
{"id": "2506.03470", "pdf": "https://arxiv.org/pdf/2506.03470", "abs": "https://arxiv.org/abs/2506.03470", "authors": ["Liam Hodgkinson", "Zhichao Wang", "Michael W. Mahoney"], "title": "Models of Heavy-Tailed Mechanistic Universality", "categories": ["stat.ML", "cs.LG"], "comment": "40 pages, 4 figures", "summary": "Recent theoretical and empirical successes in deep learning, including the\ncelebrated neural scaling laws, are punctuated by the observation that many\nobjects of interest tend to exhibit some form of heavy-tailed or power law\nbehavior. In particular, the prevalence of heavy-tailed spectral densities in\nJacobians, Hessians, and weight matrices has led to the introduction of the\nconcept of heavy-tailed mechanistic universality (HT-MU). Multiple lines of\nempirical evidence suggest a robust correlation between heavy-tailed metrics\nand model performance, indicating that HT-MU may be a fundamental aspect of\ndeep learning efficacy. Here, we propose a general family of random matrix\nmodels -- the high-temperature Marchenko-Pastur (HTMP) ensemble -- to explore\nattributes that give rise to heavy-tailed behavior in trained neural networks.\nUnder this model, spectral densities with power laws on (upper and lower) tails\narise through a combination of three independent factors (complex correlation\nstructures in the data; reduced temperatures during training; and reduced\neigenvector entropy), appearing as an implicit bias in the model structure, and\nthey can be controlled with an \"eigenvalue repulsion\" parameter. Implications\nof our model on other appearances of heavy tails, including neural scaling\nlaws, optimizer trajectories, and the five-plus-one phases of neural network\ntraining, are discussed.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60\u4e2d\u666e\u904d\u5b58\u5728\u7684\u91cd\u5c3e\u73b0\u8c61\u53ca\u5176\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u76f8\u5173\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u968f\u673a\u77e9\u9635\u6a21\u578b\uff08HTMP\uff09\u6765\u7814\u7a76\u91cd\u5c3e\u884c\u4e3a\u7684\u6210\u56e0\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5bf9\u6df1\u5ea6\u5b66\u4e60\u5176\u4ed6\u9886\u57df\u7684\u5f71\u54cd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e2d\u8bb8\u591a\u5bf9\u8c61\u7684\u8c31\u5bc6\u5ea6\u8868\u73b0\u51fa\u91cd\u5c3e\u884c\u4e3a\uff0c\u4e14\u4e0e\u6a21\u578b\u6027\u80fd\u76f8\u5173\uff0c\u7814\u7a76\u5176\u673a\u5236\u6709\u52a9\u4e8e\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6e29\u5ea6Marchenko-Pastur\uff08HTMP\uff09\u968f\u673a\u77e9\u9635\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u4e2a\u72ec\u7acb\u56e0\u7d20\uff08\u6570\u636e\u590d\u6742\u76f8\u5173\u6027\u3001\u8bad\u7ec3\u4e2d\u964d\u4f4e\u7684\u6e29\u5ea6\u548c\u964d\u4f4e\u7684\u7279\u5f81\u5411\u91cf\u71b5\uff09\u7814\u7a76\u91cd\u5c3e\u884c\u4e3a\u7684\u6210\u56e0\u3002", "result": "\u6a21\u578b\u4e2d\u91cd\u5c3e\u8c31\u5bc6\u5ea6\u7684\u51fa\u73b0\u4e0e\u4e09\u79cd\u56e0\u7d20\u76f8\u5173\uff0c\u4e14\u53ef\u901a\u8fc7\u201c\u7279\u5f81\u503c\u6392\u65a5\u201d\u53c2\u6570\u63a7\u5236\uff0c\u6a21\u578b\u8fd8\u53ef\u89e3\u91ca\u91cd\u5c3e\u73b0\u8c61\u5728\u5176\u4ed6\u9886\u57df\u7684\u8868\u73b0\u3002", "conclusion": "HTMP\u6a21\u578b\u63ed\u793a\u4e86\u91cd\u5c3e\u884c\u4e3a\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u666e\u904d\u6027\u53ca\u5176\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u5bf9\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u673a\u5236\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "keywords": "\u6df1\u5ea6\u5b66\u4e60, \u91cd\u5c3e\u884c\u4e3a, HTMP\u6a21\u578b, \u8c31\u5bc6\u5ea6, \u968f\u673a\u77e9\u9635"}}
{"id": "2506.03515", "pdf": "https://arxiv.org/pdf/2506.03515", "abs": "https://arxiv.org/abs/2506.03515", "authors": ["Masaya Kawamura", "Takuya Hasumi", "Yuma Shirahata", "Ryuichi Yamamoto"], "title": "BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "comment": "Accepted to INTERSPEECH 2025", "summary": "This paper proposes a highly compact, lightweight text-to-speech (TTS) model\nfor on-device applications. To reduce the model size, the proposed model\nintroduces two techniques. First, we introduce quantization-aware training\n(QAT), which quantizes model parameters during training to as low as 1.58-bit.\nIn this case, most of 32-bit model parameters are quantized to ternary values\n{-1, 0, 1}. Second, we propose a method named weight indexing. In this method,\nwe save a group of 1.58-bit weights as a single int8 index. This allows for\nefficient storage of model parameters, even on hardware that treats values in\nunits of 8-bit. Experimental results demonstrate that the proposed method\nachieved 83 % reduction in model size, while outperforming the baseline of\nsimilar model size without quantization in synthesis quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u5ea6\u7d27\u51d1\u3001\u8f7b\u91cf\u7ea7\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\uff0c\u901a\u8fc7\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u548c\u6743\u91cd\u7d22\u5f15\u6280\u672f\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u5927\u5c0f\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u5408\u6210\u8d28\u91cf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8bbe\u5907\u7aef\u5e94\u7528\u4e2d\u7684\u6a21\u578b\u5b58\u50a8\u548c\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u7d27\u51d1\u4e14\u9ad8\u6548\u7684TTS\u6a21\u578b\u3002", "method": "\u91c7\u7528\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u5c06\u6a21\u578b\u53c2\u6570\u91cf\u5316\u4e3a\u4f4e\u81f31.58\u4f4d\uff0c\u5e76\u5f15\u5165\u6743\u91cd\u7d22\u5f15\u6280\u672f\u5c061.58\u4f4d\u6743\u91cd\u5b58\u50a8\u4e3aint8\u7d22\u5f15\u3002", "result": "\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u4e8683%\uff0c\u4e14\u5408\u6210\u8d28\u91cf\u4f18\u4e8e\u672a\u91cf\u5316\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\uff0c\u9002\u5408\u8bbe\u5907\u7aef\u90e8\u7f72\u3002", "keywords": "\u6587\u672c\u8f6c\u8bed\u97f3,\u91cf\u5316\u611f\u77e5\u8bad\u7ec3,\u6743\u91cd\u7d22\u5f15,\u6a21\u578b\u538b\u7f29,\u8bbe\u5907\u7aef\u5e94\u7528"}}
{"id": "2506.03594", "pdf": "https://arxiv.org/pdf/2506.03594", "abs": "https://arxiv.org/abs/2506.03594", "authors": ["Shengjie Lin", "Jiading Fang", "Muhammad Zubair Irshad", "Vitor Campagnolo Guizilini", "Rares Andrei Ambrus", "Greg Shakhnarovich", "Matthew R. Walter"], "title": "SplArt: Articulation Estimation and Part-Level Reconstruction with 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.MM", "cs.RO"], "comment": "https://github.com/ripl/splart", "summary": "Reconstructing articulated objects prevalent in daily environments is crucial\nfor applications in augmented/virtual reality and robotics. However, existing\nmethods face scalability limitations (requiring 3D supervision or costly\nannotations), robustness issues (being susceptible to local optima), and\nrendering shortcomings (lacking speed or photorealism). We introduce SplArt, a\nself-supervised, category-agnostic framework that leverages 3D Gaussian\nSplatting (3DGS) to reconstruct articulated objects and infer kinematics from\ntwo sets of posed RGB images captured at different articulation states,\nenabling real-time photorealistic rendering for novel viewpoints and\narticulations. SplArt augments 3DGS with a differentiable mobility parameter\nper Gaussian, achieving refined part segmentation. A multi-stage optimization\nstrategy is employed to progressively handle reconstruction, part segmentation,\nand articulation estimation, significantly enhancing robustness and accuracy.\nSplArt exploits geometric self-supervision, effectively addressing challenging\nscenarios without requiring 3D annotations or category-specific priors.\nEvaluations on established and newly proposed benchmarks, along with\napplications to real-world scenarios using a handheld RGB camera, demonstrate\nSplArt's state-of-the-art performance and real-world practicality. Code is\npublicly available at https://github.com/ripl/splart.", "AI": {"tldr": "SplArt\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u3001\u7c7b\u522b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u91cd\u5efa\u94f0\u63a5\u7269\u4f53\u5e76\u63a8\u65ad\u8fd0\u52a8\u5b66\uff0c\u5b9e\u73b0\u5b9e\u65f6\u903c\u771f\u6e32\u67d3\u3002", "motivation": "\u4e3a\u589e\u5f3a/\u865a\u62df\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5e38\u89c1\u7684\u94f0\u63a5\u7269\u4f53\u91cd\u5efa\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u75283D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5f15\u5165\u53ef\u5fae\u5206\u79fb\u52a8\u53c2\u6570\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u9010\u6b65\u5904\u7406\u91cd\u5efa\u3001\u90e8\u5206\u5206\u5272\u548c\u94f0\u63a5\u4f30\u8ba1\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u65e0\u97003D\u6807\u6ce8\u6216\u7c7b\u522b\u5148\u9a8c\u3002", "conclusion": "SplArt\u5728\u94f0\u63a5\u7269\u4f53\u91cd\u5efa\u9886\u57df\u5177\u6709\u5148\u8fdb\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002", "keywords": "\u94f0\u63a5\u7269\u4f53\u91cd\u5efa,3D\u9ad8\u65af\u6cfc\u6e85,\u81ea\u76d1\u7763\u5b66\u4e60,\u5b9e\u65f6\u6e32\u67d3"}}
{"id": "2506.04116", "pdf": "https://arxiv.org/pdf/2506.04116", "abs": "https://arxiv.org/abs/2506.04116", "authors": ["Xuanru Zhou", "Jiarun Liu", "Shoujun Yu", "Hao Yang", "Cheng Li", "Tao Tan", "Shanshan Wang"], "title": "A Diffusion-Driven Temporal Super-Resolution and Spatial Consistency Enhancement Framework for 4D MRI imaging", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "In medical imaging, 4D MRI enables dynamic 3D visualization, yet the\ntrade-off between spatial and temporal resolution requires prolonged scan time\nthat can compromise temporal fidelity--especially during rapid, large-amplitude\nmotion. Traditional approaches typically rely on registration-based\ninterpolation to generate intermediate frames. However, these methods struggle\nwith large deformations, resulting in misregistration, artifacts, and\ndiminished spatial consistency. To address these challenges, we propose\nTSSC-Net, a novel framework that generates intermediate frames while preserving\nspatial consistency. To improve temporal fidelity under fast motion, our\ndiffusion-based temporal super-resolution network generates intermediate frames\nusing the start and end frames as key references, achieving 6x temporal\nsuper-resolution in a single inference step. Additionally, we introduce a novel\ntri-directional Mamba-based module that leverages long-range contextual\ninformation to effectively resolve spatial inconsistencies arising from\ncross-slice misalignment, thereby enhancing volumetric coherence and correcting\ncross-slice errors. Extensive experiments were performed on the public ACDC\ncardiac MRI dataset and a real-world dynamic 4D knee joint dataset. The results\ndemonstrate that TSSC-Net can generate high-resolution dynamic MRI from\nfast-motion data while preserving structural fidelity and spatial consistency.", "AI": {"tldr": "TSSC-Net\u6846\u67b6\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u65b0\u578b\u4e09\u5411Mamba\u6a21\u5757\u89e3\u51b3\u4e864D MRI\u52a8\u6001\u6210\u50cf\u4e2d\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u77db\u76fe\uff0c\u63d0\u5347\u4e86\u5feb\u901f\u8fd0\u52a8\u4e0b\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u63d2\u503c\u65b9\u6cd5\u5728\u5927\u5f62\u53d8\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u4f2a\u5f71\u548c\u7a7a\u95f4\u4e0d\u4e00\u81f4\u6027\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u63d0\u5347\u52a8\u6001MRI\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u3002", "method": "\u7ed3\u5408\u6269\u6563\u6a21\u578b\u7684\u65f6\u7a7a\u8d85\u5206\u8fa8\u6280\u672f\u548c\u4e09\u5411Mamba\u6a21\u5757\uff0c\u5229\u7528\u957f\u7a0b\u4e0a\u4e0b\u6587\u4fe1\u606f\u89e3\u51b3\u8de8\u5207\u7247\u9519\u4f4d\u95ee\u9898\u3002", "result": "\u5728ACDC\u5fc3\u810fMRI\u548c4D\u819d\u5173\u8282\u6570\u636e\u96c6\u4e0a\uff0cTSSC-Net\u5b9e\u73b0\u4e866\u500d\u65f6\u95f4\u8d85\u5206\u8fa8\u7387\u4e14\u4fdd\u6301\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "TSSC-Net\u80fd\u9ad8\u6548\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u52a8\u6001MRI\uff0c\u9002\u7528\u4e8e\u5feb\u901f\u8fd0\u52a8\u573a\u666f\u3002", "keywords": "4D MRI,\u65f6\u7a7a\u5206\u8fa8\u7387,\u6269\u6563\u6a21\u578b,Mamba\u6a21\u5757"}}
{"id": "2506.03657", "pdf": "https://arxiv.org/pdf/2506.03657", "abs": "https://arxiv.org/abs/2506.03657", "authors": ["Leonardo Martins Bianco", "Christine Keribin", "Zacharie Naulet"], "title": "SubSearch: Robust Estimation and Outlier Detection for Stochastic Block Models via Subgraph Search", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Community detection is a fundamental task in graph analysis, with methods\noften relying on fitting models like the Stochastic Block Model (SBM) to\nobserved networks. While many algorithms can accurately estimate SBM parameters\nwhen the input graph is a perfect sample from the model, real-world graphs\nrarely conform to such idealized assumptions. Therefore, robust algorithms are\ncrucial-ones that can recover model parameters even when the data deviates from\nthe assumed distribution. In this work, we propose SubSearch, an algorithm for\nrobustly estimating SBM parameters by exploring the space of subgraphs in\nsearch of one that closely aligns with the model's assumptions. Our approach\nalso functions as an outlier detection method, properly identifying nodes\nresponsible for the graph's deviation from the model and going beyond simple\ntechniques like pruning high-degree nodes. Extensive experiments on both\nsynthetic and real-world datasets demonstrate the effectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSubSearch\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u793e\u533a\u68c0\u6d4b\u4e2d\u7a33\u5065\u4f30\u8ba1\u968f\u673a\u5757\u6a21\u578b\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u63a2\u7d22\u5b50\u56fe\u7a7a\u95f4\u6765\u8bc6\u522b\u5f02\u5e38\u8282\u70b9\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u56fe\u6570\u636e\u5f80\u5f80\u4e0d\u7b26\u5408\u968f\u673a\u5757\u6a21\u578b\u7684\u7406\u60f3\u5047\u8bbe\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u6570\u636e\u504f\u5dee\u7684\u7a33\u5065\u7b97\u6cd5\u3002", "method": "\u63d0\u51faSubSearch\u7b97\u6cd5\uff0c\u901a\u8fc7\u641c\u7d22\u5b50\u56fe\u7a7a\u95f4\u627e\u5230\u7b26\u5408\u6a21\u578b\u5047\u8bbe\u7684\u5b50\u56fe\uff0c\u5e76\u8bc6\u522b\u5bfc\u81f4\u504f\u5dee\u7684\u5f02\u5e38\u8282\u70b9\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "SubSearch\u4e0d\u4ec5\u80fd\u7a33\u5065\u4f30\u8ba1\u53c2\u6570\uff0c\u8fd8\u80fd\u6709\u6548\u8bc6\u522b\u5f02\u5e38\u8282\u70b9\uff0c\u4f18\u4e8e\u7b80\u5355\u7684\u4fee\u526a\u65b9\u6cd5\u3002", "keywords": "\u793e\u533a\u68c0\u6d4b, \u968f\u673a\u5757\u6a21\u578b, \u7a33\u5065\u7b97\u6cd5, \u5f02\u5e38\u68c0\u6d4b"}}
{"id": "2506.04121", "pdf": "https://arxiv.org/pdf/2506.04121", "abs": "https://arxiv.org/abs/2506.04121", "authors": ["Loan Dao", "Ngoc Quoc Ly"], "title": "A Comprehensive Study on Medical Image Segmentation using Deep Neural Networks", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Over the past decade, Medical Image Segmentation (MIS) using Deep Neural\nNetworks (DNNs) has achieved significant performance improvements and holds\ngreat promise for future developments. This paper presents a comprehensive\nstudy on MIS based on DNNs. Intelligent Vision Systems are often evaluated\nbased on their output levels, such as Data, Information, Knowledge,\nIntelligence, and Wisdom (DIKIW),and the state-of-the-art solutions in MIS at\nthese levels are the focus of research. Additionally, Explainable Artificial\nIntelligence (XAI) has become an important research direction, as it aims to\nuncover the \"black box\" nature of previous DNN architectures to meet the\nrequirements of transparency and ethics. The study emphasizes the importance of\nMIS in disease diagnosis and early detection, particularly for increasing the\nsurvival rate of cancer patients through timely diagnosis. XAI and early\nprediction are considered two important steps in the journey from\n\"intelligence\" to \"wisdom.\" Additionally, the paper addresses existing\nchallenges and proposes potential solutions to enhance the efficiency of\nimplementing DNN-based MIS.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff08MIS\uff09\u7814\u7a76\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86DIKIW\u6846\u67b6\u4e0b\u7684\u6700\u65b0\u8fdb\u5c55\u53ca\u5176\u5728\u75be\u75c5\u8bca\u65ad\u4e2d\u7684\u4f5c\u7528\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7684\u91cd\u8981\u6027\u4ee5\u89e3\u51b3\u900f\u660e\u5ea6\u548c\u4f26\u7406\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6548\u7387\u548c\u900f\u660e\u5ea6\uff0c\u4ee5\u652f\u6301\u75be\u75c5\uff08\u5982\u764c\u75c7\uff09\u7684\u65e9\u671f\u8bca\u65ad\u548c\u6cbb\u7597\uff0c\u4ece\u800c\u6539\u5584\u60a3\u8005\u751f\u5b58\u7387\u3002", "method": "\u901a\u8fc7DIKIW\u6846\u67b6\u8bc4\u4f30\u667a\u80fd\u89c6\u89c9\u7cfb\u7edf\u7684\u8f93\u51fa\u6c34\u5e73\uff0c\u5e76\u5206\u6790\u73b0\u6709\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728MIS\u4e2d\u7684\u5e94\u7528\u53ca\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u8868\u660eXAI\u548c\u65e9\u671f\u9884\u6d4b\u662f\u4ece\u201c\u667a\u80fd\u201d\u5230\u201c\u667a\u6167\u201d\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5f53\u524d\u6280\u672f\u7684\u6311\u6218\u548c\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684MIS\u5728\u533b\u5b66\u9886\u57df\u5177\u6709\u5e7f\u9614\u524d\u666f\uff0c\u4f46\u9700\u89e3\u51b3\u900f\u660e\u5ea6\u548c\u4f26\u7406\u95ee\u9898\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u5e94\u7528\u3002", "keywords": "\u533b\u5b66\u56fe\u50cf\u5206\u5272, \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc, \u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd, DIKIW\u6846\u67b6, \u75be\u75c5\u8bca\u65ad"}}
{"id": "2506.03664", "pdf": "https://arxiv.org/pdf/2506.03664", "abs": "https://arxiv.org/abs/2506.03664", "authors": ["Valerie Krug", "Sebastian Stober"], "title": "Intersectional Bias in Pre-Trained Image Recognition Models", "categories": ["cs.CV", "cs.CY", "cs.HC", "cs.LG"], "comment": "Summary paper accepted at the 3rd TRR 318 Conference: Contextualizing\n  Explanations 2025", "summary": "Deep Learning models have achieved remarkable success. Training them is often\naccelerated by building on top of pre-trained models which poses the risk of\nperpetuating encoded biases. Here, we investigate biases in the representations\nof commonly used ImageNet classifiers for facial images while considering\nintersections of sensitive variables age, race and gender. To assess the\nbiases, we use linear classifier probes and visualize activations as\ntopographic maps. We find that representations in ImageNet classifiers\nparticularly allow differentiation between ages. Less strongly pronounced, the\nmodels appear to associate certain ethnicities and distinguish genders in\nmiddle-aged groups.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86ImageNet\u5206\u7c7b\u5668\u4e2d\u9762\u90e8\u56fe\u50cf\u5728\u5e74\u9f84\u3001\u79cd\u65cf\u548c\u6027\u522b\u65b9\u9762\u7684\u504f\u89c1\uff0c\u53d1\u73b0\u5e74\u9f84\u5dee\u5f02\u6700\u660e\u663e\uff0c\u800c\u79cd\u65cf\u548c\u6027\u522b\u504f\u89c1\u8f83\u5f31\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u57fa\u7840\u4e0a\u52a0\u901f\u8bad\u7ec3\u53ef\u80fd\u5bfc\u81f4\u7f16\u7801\u504f\u89c1\u7684\u5ef6\u7eed\uff0c\u56e0\u6b64\u7814\u7a76ImageNet\u5206\u7c7b\u5668\u4e2d\u9762\u90e8\u56fe\u50cf\u7684\u504f\u89c1\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u5206\u7c7b\u5668\u63a2\u9488\u548c\u5730\u5f62\u56fe\u53ef\u89c6\u5316\u6fc0\u6d3b\uff0c\u8bc4\u4f30\u5e74\u9f84\u3001\u79cd\u65cf\u548c\u6027\u522b\u7b49\u654f\u611f\u53d8\u91cf\u7684\u504f\u89c1\u3002", "result": "\u53d1\u73b0ImageNet\u5206\u7c7b\u5668\u80fd\u660e\u663e\u533a\u5206\u5e74\u9f84\uff0c\u79cd\u65cf\u548c\u6027\u522b\u504f\u89c1\u5728\u4e2d\u5e74\u7fa4\u4f53\u4e2d\u8f83\u5f31\u4f46\u5b58\u5728\u3002", "conclusion": "ImageNet\u5206\u7c7b\u5668\u5728\u9762\u90e8\u56fe\u50cf\u8868\u5f81\u4e2d\u5b58\u5728\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5e74\u9f84\u65b9\u9762\uff0c\u79cd\u65cf\u548c\u6027\u522b\u504f\u89c1\u8f83\u5f31\u3002", "keywords": "\u6df1\u5ea6\u5b66\u4e60, ImageNet, \u504f\u89c1, \u5e74\u9f84, \u79cd\u65cf, \u6027\u522b"}}
{"id": "2506.04129", "pdf": "https://arxiv.org/pdf/2506.04129", "abs": "https://arxiv.org/abs/2506.04129", "authors": ["Loan Dao", "Ngoc Quoc Ly"], "title": "Recent Advances in Medical Image Classification", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Medical image classification is crucial for diagnosis and treatment,\nbenefiting significantly from advancements in artificial intelligence. The\npaper reviews recent progress in the field, focusing on three levels of\nsolutions: basic, specific, and applied. It highlights advances in traditional\nmethods using deep learning models like Convolutional Neural Networks and\nVision Transformers, as well as state-of-the-art approaches with Vision\nLanguage Models. These models tackle the issue of limited labeled data, and\nenhance and explain predictive results through Explainable Artificial\nIntelligence.", "AI": {"tldr": "\u8bba\u6587\u56de\u987e\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u7684\u8fd1\u671f\u8fdb\u5c55\uff0c\u6db5\u76d6\u57fa\u7840\u3001\u7279\u5b9a\u548c\u5e94\u7528\u4e09\u4e2a\u5c42\u6b21\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u65b9\u6cd5\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5728\u8bca\u65ad\u548c\u6cbb\u7597\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4eba\u5de5\u667a\u80fd\u7684\u8fdb\u6b65\u4e3a\u5176\u5e26\u6765\u4e86\u663e\u8457\u63d0\u5347\u3002", "method": "\u603b\u7ed3\u4e86\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3001\u89c6\u89c9\u53d8\u6362\u5668\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4f20\u7edf\u4e0e\u524d\u6cbf\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u589e\u5f3a\u548c\u89e3\u91ca\u9884\u6d4b\u7ed3\u679c\u3002", "result": "\u89e3\u51b3\u4e86\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u7ed3\u679c\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u8fdb\u4e00\u6b65\u63a8\u52a8\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "keywords": "\u533b\u5b66\u56fe\u50cf\u5206\u7c7b, \u6df1\u5ea6\u5b66\u4e60, \u5377\u79ef\u795e\u7ecf\u7f51\u7edc, \u89c6\u89c9\u53d8\u6362\u5668, \u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd"}}
{"id": "2506.03670", "pdf": "https://arxiv.org/pdf/2506.03670", "abs": "https://arxiv.org/abs/2506.03670", "authors": ["Ivan Melev", "Goeran Kauermann"], "title": "Position: There Is No Free Bayesian Uncertainty Quantification", "categories": ["stat.ML", "cs.LG"], "comment": "NeurIPS 2025 preprint, frequentist critique of Bayesian UQ", "summary": "Due to their intuitive appeal, Bayesian methods of modeling and uncertainty\nquantification have become popular in modern machine and deep learning. When\nproviding a prior distribution over the parameter space, it is straightforward\nto obtain a distribution over the parameters that is conventionally interpreted\nas uncertainty quantification of the model. We challenge the validity of such\nBayesian uncertainty quantification by discussing the equivalent\noptimization-based representation of Bayesian updating, provide an alternative\ninterpretation that is coherent with the optimization-based perspective,\npropose measures of the quality of the Bayesian inferential stage, and suggest\ndirections for future work.", "AI": {"tldr": "\u8bba\u6587\u6311\u6218\u4e86\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u6709\u6548\u6027\uff0c\u5e76\u4ece\u4f18\u5316\u89d2\u5ea6\u63d0\u51fa\u66ff\u4ee3\u89e3\u91ca\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u8d1d\u53f6\u65af\u65b9\u6cd5\u5728\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u4e2d\u53d7\u5230\u6b22\u8fce\uff0c\u4f46\u5176\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u6709\u6548\u6027\u503c\u5f97\u8d28\u7591\u3002", "method": "\u901a\u8fc7\u8ba8\u8bba\u8d1d\u53f6\u65af\u66f4\u65b0\u7684\u4f18\u5316\u7b49\u6548\u8868\u793a\uff0c\u63d0\u51fa\u4e86\u4e0e\u4f18\u5316\u89c6\u89d2\u4e00\u81f4\u7684\u89e3\u91ca\u3002", "result": "\u63d0\u51fa\u4e86\u8861\u91cf\u8d1d\u53f6\u65af\u63a8\u65ad\u9636\u6bb5\u8d28\u91cf\u7684\u6307\u6807\u3002", "conclusion": "\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u9700\u91cd\u65b0\u5ba1\u89c6\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u5176\u4f18\u5316\u89c6\u89d2\u3002", "keywords": "\u8d1d\u53f6\u65af\u65b9\u6cd5, \u4e0d\u786e\u5b9a\u6027\u91cf\u5316, \u4f18\u5316, \u673a\u5668\u5b66\u4e60, \u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2506.03672", "pdf": "https://arxiv.org/pdf/2506.03672", "abs": "https://arxiv.org/abs/2506.03672", "authors": ["Sobihan Surendran", "Adeline Fermanian", "Sylvain Le Corff"], "title": "Latent Guided Sampling for Combinatorial Optimization", "categories": ["stat.ML", "cs.LG", "math.OC"], "comment": null, "summary": "Combinatorial Optimization problems are widespread in domains such as\nlogistics, manufacturing, and drug discovery, yet their NP-hard nature makes\nthem computationally challenging. Recent Neural Combinatorial Optimization\nmethods leverage deep learning to learn solution strategies, trained via\nSupervised or Reinforcement Learning (RL). While promising, these approaches\noften rely on task-specific augmentations, perform poorly on\nout-of-distribution instances, and lack robust inference mechanisms. Moreover,\nexisting latent space models either require labeled data or rely on pre-trained\npolicies. In this work, we propose LGS-Net, a novel latent space model that\nconditions on problem instances, and introduce an efficient inference method,\nLatent Guided Sampling (LGS), based on Markov Chain Monte Carlo and Stochastic\nApproximation. We show that the iterations of our method form a\ntime-inhomogeneous Markov Chain and provide rigorous theoretical convergence\nguarantees. Empirical results on benchmark routing tasks show that our method\nachieves state-of-the-art performance among RL-based approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6f5c\u5728\u7a7a\u95f4\u6a21\u578bLGS-Net\uff0c\u7ed3\u5408MCMC\u548c\u968f\u673a\u8fd1\u4f3c\u7684\u63a8\u7406\u65b9\u6cd5LGS\uff0c\u89e3\u51b3\u4e86\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u57fa\u51c6\u8def\u7531\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u5728\u7269\u6d41\u3001\u5236\u9020\u7b49\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176NP\u96be\u7279\u6027\u5bfc\u81f4\u8ba1\u7b97\u56f0\u96be\uff0c\u73b0\u6709\u795e\u7ecf\u7ec4\u5408\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u589e\u5f3a\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51fa\u4e86LGS-Net\u6f5c\u5728\u7a7a\u95f4\u6a21\u578b\u548c\u57fa\u4e8eMCMC\u4e0e\u968f\u673a\u8fd1\u4f3c\u7684\u63a8\u7406\u65b9\u6cd5LGS\uff0c\u7406\u8bba\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6536\u655b\u6027\u3002", "result": "\u5728\u57fa\u51c6\u8def\u7531\u4efb\u52a1\u4e2d\uff0cLGS-Net\u5728\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u4e2d\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "LGS-Net\u901a\u8fc7\u7ed3\u5408\u6f5c\u5728\u7a7a\u95f4\u6a21\u578b\u548c\u9ad8\u6548\u7684\u63a8\u7406\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\u3002", "keywords": "\u7ec4\u5408\u4f18\u5316, \u6f5c\u5728\u7a7a\u95f4\u6a21\u578b, \u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b, \u5f3a\u5316\u5b66\u4e60"}}
{"id": "2506.04132", "pdf": "https://arxiv.org/pdf/2506.04132", "abs": "https://arxiv.org/abs/2506.04132", "authors": ["Peter A. Gloor"], "title": "Plant Bioelectric Early Warning Systems: A Five-Year Investigation into Human-Plant Electromagnetic Communication", "categories": ["q-bio.OT", "cs.AI"], "comment": null, "summary": "We present a comprehensive investigation into plant bioelectric responses to\nhuman presence and emotional states, building on five years of systematic\nresearch. Using custom-built plant sensors and machine learning classification,\nwe demonstrate that plants generate distinct bioelectric signals correlating\nwith human proximity, emotional states, and physiological conditions. A deep\nlearning model based on ResNet50 architecture achieved 97% accuracy in\nclassifying human emotional states through plant voltage spectrograms, while\ncontrol models with shuffled labels achieved only 30% accuracy. This study\nsynthesizes findings from multiple experiments spanning 2020-2025, including\nindividual recognition (66% accuracy), eurythmic gesture detection, stress\nprediction, and responses to human voice and movement. We propose that these\nphenomena represent evolved anti-herbivory early warning systems, where plants\ndetect approaching animals through bioelectric field changes before physical\ncontact. Our results challenge conventional understanding of plant sensory\ncapabilities and suggest practical applications in agriculture, healthcare, and\nhuman-plant interaction research.", "AI": {"tldr": "\u690d\u7269\u5bf9\u4eba\u7c7b\u5b58\u5728\u548c\u60c5\u7eea\u72b6\u6001\u7684\u751f\u7269\u7535\u53cd\u5e94\u7814\u7a76\u663e\u793a\uff0c\u690d\u7269\u80fd\u901a\u8fc7\u751f\u7269\u7535\u4fe1\u53f7\u533a\u5206\u4eba\u7c7b\u60c5\u7eea\u3001\u751f\u7406\u72b6\u6001\u548c\u63a5\u8fd1\u7a0b\u5ea6\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5206\u7c7b\u51c6\u786e\u7387\u8fbe97%\u3002", "motivation": "\u63a2\u7d22\u690d\u7269\u5bf9\u4eba\u7c7b\u60c5\u7eea\u548c\u751f\u7406\u72b6\u6001\u7684\u53cd\u5e94\uff0c\u6311\u6218\u690d\u7269\u611f\u77e5\u80fd\u529b\u7684\u4f20\u7edf\u8ba4\u77e5\u3002", "method": "\u4f7f\u7528\u5b9a\u5236\u690d\u7269\u4f20\u611f\u5668\u548cResNet50\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5206\u6790\u690d\u7269\u7535\u538b\u8c31\u56fe\u3002", "result": "\u6a21\u578b\u51c6\u786e\u7387\u8fbe97%\uff0c\u5bf9\u7167\u7ec4\u4ec530%\uff0c\u540c\u65f6\u5b9e\u73b0\u4e2a\u4f53\u8bc6\u522b\uff0866%\u51c6\u786e\u7387\uff09\u548c\u538b\u529b\u9884\u6d4b\u7b49\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\u690d\u7269\u53ef\u80fd\u901a\u8fc7\u751f\u7269\u7535\u4fe1\u53f7\u6f14\u5316\u51fa\u53cd\u8349\u98df\u52a8\u7269\u9884\u8b66\u7cfb\u7edf\uff0c\u5177\u6709\u519c\u4e1a\u548c\u533b\u7597\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u690d\u7269\u751f\u7269\u7535\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u4eba\u7c7b\u60c5\u7eea\u68c0\u6d4b\u3001\u519c\u4e1a\u5e94\u7528"}}
{"id": "2506.04143", "pdf": "https://arxiv.org/pdf/2506.04143", "abs": "https://arxiv.org/abs/2506.04143", "authors": ["Ngoc Q. Ly", "Hieu N. M. Cao", "Thi T. Nguyen"], "title": "Person Re-Identification System at Semantic Level based on Pedestrian Attributes Ontology", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Person Re-Identification (Re-ID) is a very important task in video\nsurveillance systems such as tracking people, finding people in public places,\nor analysing customer behavior in supermarkets. Although there have been many\nworks to solve this problem, there are still remaining challenges such as\nlarge-scale datasets, imbalanced data, viewpoint, fine grained data\n(attributes), the Local Features are not employed at semantic level in online\nstage of Re-ID task, furthermore, the imbalanced data problem of attributes are\nnot taken into consideration. This paper has proposed a Unified Re-ID system\nconsisted of three main modules such as Pedestrian Attribute Ontology (PAO),\nLocal Multi-task DCNN (Local MDCNN), Imbalance Data Solver (IDS). The new main\npoint of our Re-ID system is the power of mutual support of PAO, Local MDCNN\nand IDS to exploit the inner-group correlations of attributes and pre-filter\nthe mismatch candidates from Gallery set based on semantic information as\nFashion Attributes and Facial Attributes, to solve the imbalanced data of\nattributes without adjusting network architecture and data augmentation. We\nexperimented on the well-known Market1501 dataset. The experimental results\nhave shown the effectiveness of our Re-ID system and it could achieve the\nhigher performance on Market1501 dataset in comparison to some state-of-the-art\nRe-ID methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684Re-ID\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u6a21\u5757\uff0c\u7528\u4e8e\u89e3\u51b3\u5c5e\u6027\u4e0d\u5e73\u8861\u548c\u5c40\u90e8\u7279\u5f81\u672a\u5145\u5206\u5229\u7528\u7684\u95ee\u9898\uff0c\u5e76\u5728Market1501\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u89e3\u51b3Re-ID\u4efb\u52a1\u4e2d\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3001\u6570\u636e\u4e0d\u5e73\u8861\u3001\u89c6\u89d2\u53d8\u5316\u3001\u7ec6\u7c92\u5ea6\u5c5e\u6027\u7b49\u95ee\u9898\uff0c\u4ee5\u53ca\u5c40\u90e8\u7279\u5f81\u5728\u8bed\u4e49\u5c42\u9762\u672a\u5145\u5206\u5229\u7528\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u5305\u542b\u884c\u4eba\u5c5e\u6027\u672c\u4f53\uff08PAO\uff09\u3001\u5c40\u90e8\u591a\u4efb\u52a1\u6df1\u5ea6\u5377\u79ef\u7f51\u7edc\uff08Local MDCNN\uff09\u548c\u5c5e\u6027\u4e0d\u5e73\u8861\u6570\u636e\u89e3\u51b3\u5668\uff08IDS\uff09\u7684\u7edf\u4e00Re-ID\u7cfb\u7edf\u3002", "result": "\u5728Market1501\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u7cfb\u7edf\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7PAO\u3001Local MDCNN\u548cIDS\u7684\u534f\u540c\u4f5c\u7528\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c5e\u6027\u4e0d\u5e73\u8861\u548c\u8bed\u4e49\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "keywords": "Person Re-ID, \u884c\u4eba\u5c5e\u6027\u672c\u4f53, \u591a\u4efb\u52a1\u5b66\u4e60, \u6570\u636e\u4e0d\u5e73\u8861, Market1501"}}
{"id": "2506.03697", "pdf": "https://arxiv.org/pdf/2506.03697", "abs": "https://arxiv.org/abs/2506.03697", "authors": ["Swagat Kumar", "Jan-Nico Zaech", "Colin Michael Wilmott", "Luc Van Gool"], "title": "RhoDARTS: Differentiable Quantum Architecture Search with Density Matrix Simulations", "categories": ["quant-ph", "cs.LG"], "comment": "24 pages, 16 figures", "summary": "Variational Quantum Algorithms (VQAs) are a promising approach for leveraging\npowerful Noisy Intermediate-Scale Quantum (NISQ) computers. When applied to\nmachine learning tasks, VQAs give rise to NISQ-compatible Quantum Neural\nNetworks (QNNs), which have been shown to outperform classical neural networks\nwith a similar number of trainable parameters. While the quantum circuit\nstructures of VQAs for physics simulations are determined by the physical\nproperties of the systems, identifying effective QNN architectures for general\nmachine learning tasks is a difficult challenge due to the lack of\ndomain-specific priors. Indeed, existing Quantum Architecture Search (QAS)\nalgorithms, adaptations of classical neural architecture search techniques,\noften overlook the inherent quantum nature of the circuits they produce. By\napproaching QAS from the ground-up and from a quantum perspective, we resolve\nthis limitation by proposing $\\rho$DARTS, a differentiable QAS algorithm that\nmodels the search process as the evolution of a quantum mixed state, emerging\nfrom the search space of quantum architectures. We validate our method by\nfinding circuits for state initialization, Hamiltonian optimization, and image\nclassification. Further, we demonstrate better convergence against existing QAS\ntechniques and show improved robustness levels to noise.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\\rhoDARTS\uff0c\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u89c6\u89d2\u7684\u53ef\u5fae\u5206\u91cf\u5b50\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709QAS\u7b97\u6cd5\u5ffd\u7565\u91cf\u5b50\u7535\u8def\u672c\u8d28\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u7684\u67b6\u6784\u641c\u7d22\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u91cf\u5b50\u7535\u8def\u7684\u672c\u8d28\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4ece\u91cf\u5b50\u89c6\u89d2\u51fa\u53d1\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\\rhoDARTS\uff0c\u4e00\u79cd\u53ef\u5fae\u5206\u91cf\u5b50\u67b6\u6784\u641c\u7d22\u7b97\u6cd5\uff0c\u5c06\u641c\u7d22\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u91cf\u5b50\u6df7\u5408\u6001\u7684\u6f14\u5316\u3002", "result": "\u5728\u591a\u4efb\u52a1\uff08\u72b6\u6001\u521d\u59cb\u5316\u3001\u54c8\u5bc6\u987f\u91cf\u4f18\u5316\u548c\u56fe\u50cf\u5206\u7c7b\uff09\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u66f4\u597d\u7684\u6536\u655b\u6027\u548c\u566a\u58f0\u9c81\u68d2\u6027\u3002", "conclusion": "\\rhoDARTS\u4ece\u91cf\u5b50\u672c\u8d28\u51fa\u53d1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5b50\u67b6\u6784\u641c\u7d22\u7684\u6027\u80fd\u3002", "keywords": "Variational Quantum Algorithms, Quantum Neural Networks, Quantum Architecture Search, \\rhoDARTS, NISQ"}}
{"id": "2506.04147", "pdf": "https://arxiv.org/pdf/2506.04147", "abs": "https://arxiv.org/abs/2506.04147", "authors": ["Jiaheng Hu", "Peter Stone", "Roberto Mart\u00edn-Mart\u00edn"], "title": "SLAC: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Building capable household and industrial robots requires mastering the\ncontrol of versatile, high-degree-of-freedom (DoF) systems such as mobile\nmanipulators. While reinforcement learning (RL) holds promise for autonomously\nacquiring robot control policies, scaling it to high-DoF embodiments remains\nchallenging. Direct RL in the real world demands both safe exploration and high\nsample efficiency, which are difficult to achieve in practice. Sim-to-real RL,\non the other hand, is often brittle due to the reality gap. This paper\nintroduces SLAC, a method that renders real-world RL feasible for complex\nembodiments by leveraging a low-fidelity simulator to pretrain a task-agnostic\nlatent action space. SLAC trains this latent action space via a customized\nunsupervised skill discovery method designed to promote temporal abstraction,\ndisentanglement, and safety, thereby facilitating efficient downstream\nlearning. Once a latent action space is learned, SLAC uses it as the action\ninterface for a novel off-policy RL algorithm to autonomously learn downstream\ntasks through real-world interactions. We evaluate SLAC against existing\nmethods on a suite of bimanual mobile manipulation tasks, where it achieves\nstate-of-the-art performance. Notably, SLAC learns contact-rich whole-body\ntasks in under an hour of real-world interactions, without relying on any\ndemonstrations or hand-crafted behavior priors. More information, code, and\nvideos at robo-rl.github.io", "AI": {"tldr": "SLAC\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u4f4e\u4fdd\u771f\u6a21\u62df\u5668\u9884\u8bad\u7ec3\u4efb\u52a1\u65e0\u5173\u7684\u6f5c\u5728\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4f7f\u590d\u6742\u673a\u5668\u4eba\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u53d8\u5f97\u53ef\u884c\uff0c\u5e76\u5728\u53cc\u624b\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u5b89\u5168\u6027\u548c\u6837\u672c\u6548\u7387\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u6a21\u62df\u5230\u771f\u5b9e\u8fc1\u79fb\u7684\u8106\u5f31\u6027\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86SLAC\u3002", "method": "SLAC\u901a\u8fc7\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u65b9\u6cd5\u9884\u8bad\u7ec3\u6f5c\u5728\u52a8\u4f5c\u7a7a\u95f4\uff0c\u968f\u540e\u5229\u7528\u8be5\u7a7a\u95f4\u4f5c\u4e3a\u52a8\u4f5c\u63a5\u53e3\uff0c\u901a\u8fc7\u65b0\u578b\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5b66\u4e60\u4efb\u52a1\u3002", "result": "SLAC\u5728\u53cc\u624b\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u80fd\u5728\u4e0d\u5230\u4e00\u5c0f\u65f6\u7684\u771f\u5b9e\u4ea4\u4e92\u4e2d\u5b66\u4e60\u63a5\u89e6\u5bc6\u96c6\u7684\u5168\u8eab\u4efb\u52a1\uff0c\u65e0\u9700\u6f14\u793a\u6216\u624b\u5de5\u884c\u4e3a\u5148\u9a8c\u3002", "conclusion": "SLAC\u4e3a\u590d\u6742\u673a\u5668\u4eba\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u673a\u5668\u4eba\u63a7\u5236, \u771f\u5b9e\u4e16\u754c\u5b66\u4e60, \u6f5c\u5728\u52a8\u4f5c\u7a7a\u95f4, \u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0"}}
{"id": "2506.03746", "pdf": "https://arxiv.org/pdf/2506.03746", "abs": "https://arxiv.org/abs/2506.03746", "authors": ["C\u00e9sar Sabater", "Sonia Ben Mokhtar", "Jan Ramon"], "title": "Dropout-Robust Mechanisms for Differentially Private and Fully Decentralized Mean Estimation", "categories": ["cs.CR", "cs.DC", "cs.LG"], "comment": "23 pages, 4 figures", "summary": "Achieving differentially private computations in decentralized settings poses\nsignificant challenges, particularly regarding accuracy, communication cost,\nand robustness against information leakage. While cryptographic solutions offer\npromise, they often suffer from high communication overhead or require\ncentralization in the presence of network failures. Conversely, existing fully\ndecentralized approaches typically rely on relaxed adversarial models or\npairwise noise cancellation, the latter suffering from substantial accuracy\ndegradation if parties unexpectedly disconnect. In this work, we propose IncA,\na new protocol for fully decentralized mean estimation, a widely used primitive\nin data-intensive processing. Our protocol, which enforces differential\nprivacy, requires no central orchestration and employs low-variance correlated\nnoise, achieved by incrementally injecting sensitive information into the\ncomputation. First, we theoretically demonstrate that, when no parties\npermanently disconnect, our protocol achieves accuracy comparable to that of a\ncentralized setting-already an improvement over most existing decentralized\ndifferentially private techniques. Second, we empirically show that our use of\nlow-variance correlated noise significantly mitigates the accuracy loss\nexperienced by existing techniques in the presence of dropouts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIncA\u7684\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u5747\u503c\u4f30\u8ba1\u534f\u8bae\uff0c\u8be5\u534f\u8bae\u5728\u4fdd\u62a4\u5dee\u5206\u9690\u79c1\u7684\u540c\u65f6\uff0c\u907f\u514d\u4e86\u9ad8\u901a\u4fe1\u5f00\u9500\u548c\u5bf9\u4e2d\u5fc3\u5316\u534f\u8c03\u7684\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7\u4f4e\u65b9\u5dee\u76f8\u5173\u566a\u58f0\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u5dee\u5206\u9690\u79c1\u8ba1\u7b97\u9762\u4e34\u51c6\u786e\u6027\u3001\u901a\u4fe1\u6210\u672c\u548c\u6297\u4fe1\u606f\u6cc4\u9732\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u901a\u4fe1\u5f00\u9500\u5927\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u4e2d\u5fc3\u5316\u534f\u8c03\u6216\u677e\u5f1b\u7684\u654c\u624b\u6a21\u578b\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faIncA\u534f\u8bae\uff0c\u91c7\u7528\u4f4e\u65b9\u5dee\u76f8\u5173\u566a\u58f0\uff0c\u5e76\u901a\u8fc7\u9010\u6b65\u6ce8\u5165\u654f\u611f\u6570\u636e\u6765\u5b9e\u73b0\u5dee\u5206\u9690\u79c1\u548c\u53bb\u4e2d\u5fc3\u5316\u5747\u503c\u4f30\u8ba1\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5728\u65e0\u8282\u70b9\u6c38\u4e45\u65ad\u5f00\u65f6\uff0cIncA\u534f\u8bae\u7684\u51c6\u786e\u6027\u63a5\u8fd1\u4e2d\u5fc3\u5316\u8bbe\u7f6e\uff1b\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u4f4e\u65b9\u5dee\u76f8\u5173\u566a\u58f0\u663e\u8457\u51cf\u8f7b\u4e86\u8282\u70b9\u65ad\u5f00\u65f6\u7684\u7cbe\u5ea6\u635f\u5931\u3002", "conclusion": "IncA\u534f\u8bae\u5728\u53bb\u4e2d\u5fc3\u5316\u5dee\u5206\u9690\u79c1\u8ba1\u7b97\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "keywords": "\u5dee\u5206\u9690\u79c1; \u53bb\u4e2d\u5fc3\u5316\u8ba1\u7b97; \u5747\u503c\u4f30\u8ba1; \u4f4e\u65b9\u5dee\u566a\u58f0"}}
{"id": "2506.03764", "pdf": "https://arxiv.org/pdf/2506.03764", "abs": "https://arxiv.org/abs/2506.03764", "authors": ["R\u00f3is\u00edn Luo"], "title": "Infinitesimal Higher-Order Spectral Variations in Rectangular Real Random Matrices", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We present a theoretical framework for deriving the general $n$-th order\nFr\\'echet derivatives of singular values in real rectangular matrices, by\nleveraging reduced resolvent operators from Kato's analytic perturbation theory\nfor self-adjoint operators. Deriving closed-form expressions for higher-order\nderivatives of singular values is notoriously challenging through standard\nmatrix-analysis techniques. To overcome this, we treat a real rectangular\nmatrix as a compact operator on a finite-dimensional Hilbert space, and embed\nthe rectangular matrix into a block self-adjoint operator so that non-symmetric\nperturbations are captured. Applying Kato's asymptotic eigenvalue expansion to\nthis construction, we obtain a general, closed-form expression for the\ninfinitesimal $n$-th order spectral variations. Specializing to $n=2$ and\ndeploying on a Kronecker-product representation with matrix convention yield\nthe Hessian of a singular value, not found in literature. By bridging abstract\noperator-theoretic perturbation theory with matrices, our framework equips\nresearchers with a practical toolkit for higher-order spectral sensitivity\nstudies in random matrix applications (e.g., adversarial perturbation in deep\nlearning).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u5229\u7528Kato\u7684\u89e3\u6790\u6444\u52a8\u7406\u8bba\uff0c\u63a8\u5bfc\u5b9e\u6570\u77e9\u5f62\u77e9\u9635\u5947\u5f02\u503c\u7684\u4e00\u822c$n$\u9636Fr\u00e9chet\u5bfc\u6570\u8868\u8fbe\u5f0f\u3002", "motivation": "\u6807\u51c6\u77e9\u9635\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u83b7\u5f97\u9ad8\u9636\u5947\u5f02\u503c\u5bfc\u6570\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e0c\u671b\u901a\u8fc7\u7b97\u5b50\u7406\u8bba\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u77e9\u5f62\u77e9\u9635\u5d4c\u5165\u81ea\u4f34\u7b97\u5b50\uff0c\u5e94\u7528Kato\u7684\u6e10\u8fd1\u7279\u5f81\u503c\u5c55\u5f00\uff0c\u5f97\u5230$n$\u9636\u8c31\u53d8\u5316\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u6210\u529f\u63a8\u5bfc\u51fa\u4e00\u822c$n$\u9636\u5bfc\u6570\u7684\u8868\u8fbe\u5f0f\uff0c\u5e76\u9996\u6b21\u7ed9\u51fa\u4e8c\u9636\u5bfc\u6570\u7684Hessian\u77e9\u9635\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u968f\u673a\u77e9\u9635\u5e94\u7528\uff08\u5982\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5bf9\u6297\u6270\u52a8\uff09\u63d0\u4f9b\u4e86\u9ad8\u9636\u8c31\u654f\u611f\u6027\u7814\u7a76\u7684\u5b9e\u7528\u5de5\u5177\u3002", "keywords": "Fr\u00e9chet\u5bfc\u6570,\u5947\u5f02\u503c,Kato\u6444\u52a8\u7406\u8bba,\u968f\u673a\u77e9\u9635,\u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2506.03779", "pdf": "https://arxiv.org/pdf/2506.03779", "abs": "https://arxiv.org/abs/2506.03779", "authors": ["Hachem Kadri", "Joachim Tomasi", "Yuka Hashimoto", "Sandrine Anthoine"], "title": "Towards Quantum Operator-Valued Kernels", "categories": ["quant-ph", "cs.LG", "stat.ML"], "comment": null, "summary": "Quantum kernels are reproducing kernel functions built using\nquantum-mechanical principles and are studied with the aim of outperforming\ntheir classical counterparts. The enthusiasm for quantum kernel machines has\nbeen tempered by recent studies that have suggested that quantum kernels could\nnot offer speed-ups when learning on classical data. However, most of the\nresearch in this area has been devoted to scalar-valued kernels in standard\nclassification or regression settings for which classical kernel methods are\nefficient and effective, leaving very little room for improvement with quantum\nkernels. This position paper argues that quantum kernel research should focus\non more expressive kernel classes. We build upon recent advances in\noperator-valued kernels, and propose guidelines for investigating quantum\nkernels. This should help to design a new generation of quantum kernel machines\nand fully explore their potentials.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u91cf\u5b50\u6838\u7814\u7a76\u5e94\u805a\u7126\u4e8e\u66f4\u5177\u8868\u8fbe\u529b\u7684\u6838\u7c7b\uff0c\u800c\u975e\u4f20\u7edf\u6807\u91cf\u503c\u6838\uff0c\u4ee5\u5145\u5206\u63a2\u7d22\u91cf\u5b50\u6838\u673a\u5668\u7684\u6f5c\u529b\u3002", "motivation": "\u8fd1\u671f\u7814\u7a76\u8868\u660e\u91cf\u5b50\u6838\u5728\u5904\u7406\u7ecf\u5178\u6570\u636e\u65f6\u53ef\u80fd\u65e0\u6cd5\u63d0\u4f9b\u901f\u5ea6\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6807\u51c6\u5206\u7c7b\u6216\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u6807\u91cf\u503c\u6838\u4e0a\uff0c\u9650\u5236\u4e86\u91cf\u5b50\u6838\u7684\u4f18\u52bf\u3002", "method": "\u57fa\u4e8e\u8fd1\u671f\u7b97\u5b50\u503c\u6838\u7684\u8fdb\u5c55\uff0c\u63d0\u51fa\u7814\u7a76\u91cf\u5b50\u6838\u7684\u6307\u5bfc\u539f\u5219\u3002", "result": "\u901a\u8fc7\u63a2\u7d22\u66f4\u5177\u8868\u8fbe\u529b\u7684\u91cf\u5b50\u6838\u7c7b\uff0c\u6709\u671b\u8bbe\u8ba1\u65b0\u4e00\u4ee3\u91cf\u5b50\u6838\u673a\u5668\u3002", "conclusion": "\u91cf\u5b50\u6838\u7814\u7a76\u7684\u91cd\u70b9\u5e94\u8f6c\u5411\u66f4\u5177\u8868\u8fbe\u529b\u7684\u6838\u7c7b\uff0c\u4ee5\u5145\u5206\u53d1\u6325\u5176\u6f5c\u529b\u3002", "keywords": "\u91cf\u5b50\u6838\u3001\u7b97\u5b50\u503c\u6838\u3001\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u3001\u8868\u8fbe\u529b"}}
{"id": "2506.03780", "pdf": "https://arxiv.org/pdf/2506.03780", "abs": "https://arxiv.org/abs/2506.03780", "authors": ["Hasan Fallahgoul"], "title": "High-Dimensional Learning in Finance", "categories": ["q-fin.ST", "cs.LG", "econ.EM", "stat.ML"], "comment": null, "summary": "Recent advances in machine learning have shown promising results for\nfinancial prediction using large, over-parameterized models. This paper\nprovides theoretical foundations and empirical validation for understanding\nwhen and how these methods achieve predictive success. I examine three key\naspects of high-dimensional learning in finance. First, I prove that\nwithin-sample standardization in Random Fourier Features implementations\nfundamentally alters the underlying Gaussian kernel approximation, replacing\nshift-invariant kernels with training-set dependent alternatives. Second, I\nderive sample complexity bounds showing when reliable learning becomes\ninformation-theoretically impossible under weak signal-to-noise ratios typical\nin finance. Third, VC-dimension analysis reveals that ridgeless regression's\neffective complexity is bounded by sample size rather than nominal feature\ndimension. Comprehensive numerical validation confirms these theoretical\npredictions, revealing systematic breakdown of claimed theoretical properties\nacross realistic parameter ranges. These results show that when sample size is\nsmall and features are high-dimensional, observed predictive success is\nnecessarily driven by low-complexity artifacts, not genuine high-dimensional\nlearning.", "AI": {"tldr": "\u6458\u8981\u63a2\u8ba8\u4e86\u9ad8\u7ef4\u5ea6\u673a\u5668\u5b66\u4e60\u5728\u91d1\u878d\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u5176\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u5728\u5c0f\u6837\u672c\u548c\u9ad8\u7ef4\u7279\u5f81\u4e0b\u9884\u6d4b\u6210\u529f\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u7406\u89e3\u9ad8\u7ef4\u5ea6\u673a\u5668\u5b66\u4e60\u5728\u91d1\u878d\u9884\u6d4b\u4e2d\u4f55\u65f6\u53ca\u5982\u4f55\u5b9e\u73b0\u9884\u6d4b\u6210\u529f\uff0c\u5206\u6790\u5176\u7406\u8bba\u6027\u8d28\u4e0e\u5b9e\u9645\u8868\u73b0\u7684\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u6570\u503c\u9a8c\u8bc1\uff0c\u5206\u6790\u4e86\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u6807\u51c6\u5316\u3001\u4fe1\u53f7\u566a\u58f0\u6bd4\u4e0b\u7684\u5b66\u4e60\u754c\u9650\uff0c\u4ee5\u53ca\u65e0\u5cad\u56de\u5f52\u7684\u6709\u6548\u590d\u6742\u5ea6\u3002", "result": "\u53d1\u73b0\u5728\u5c0f\u6837\u672c\u548c\u9ad8\u7ef4\u7279\u5f81\u4e0b\uff0c\u9884\u6d4b\u6210\u529f\u4e3b\u8981\u7531\u4f4e\u590d\u6742\u5ea6\u56e0\u7d20\u9a71\u52a8\uff0c\u800c\u975e\u771f\u6b63\u7684\u9ad8\u7ef4\u5b66\u4e60\u3002", "conclusion": "\u9ad8\u7ef4\u5ea6\u91d1\u878d\u9884\u6d4b\u5728\u5c0f\u6837\u672c\u548c\u9ad8\u7ef4\u7279\u5f81\u4e0b\u7684\u6210\u529f\u5177\u6709\u5c40\u9650\u6027\uff0c\u9700\u8c28\u614e\u5e94\u7528\u6b64\u7c7b\u65b9\u6cd5\u3002", "keywords": "\u673a\u5668\u5b66\u4e60,\u91d1\u878d\u9884\u6d4b,\u9ad8\u7ef4\u5ea6\u5b66\u4e60,\u6837\u672c\u590d\u6742\u6027"}}
{"id": "2506.04202", "pdf": "https://arxiv.org/pdf/2506.04202", "abs": "https://arxiv.org/abs/2506.04202", "authors": ["Yanting Wang", "Wei Zou", "Runpeng Geng", "Jinyuan Jia"], "title": "TracLLM: A Generic Framework for Attributing Long Context LLMs", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "To appear in USENIX Security Symposium 2025. The code and data are\n  at: https://github.com/Wang-Yanting/TracLLM", "summary": "Long context large language models (LLMs) are deployed in many real-world\napplications such as RAG, agent, and broad LLM-integrated applications. Given\nan instruction and a long context (e.g., documents, PDF files, webpages), a\nlong context LLM can generate an output grounded in the provided context,\naiming to provide more accurate, up-to-date, and verifiable outputs while\nreducing hallucinations and unsupported claims. This raises a research\nquestion: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)\nin the context that contribute most to or are responsible for the generated\noutput by an LLM? This process, which we call context traceback, has various\nreal-world applications, such as 1) debugging LLM-based systems, 2) conducting\npost-attack forensic analysis for attacks (e.g., prompt injection attack,\nknowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources\nto enhance the trust of users towards outputs generated by LLMs. When applied\nto context traceback for long context LLMs, existing feature attribution\nmethods such as Shapley have sub-optimal performance and/or incur a large\ncomputational cost. In this work, we develop TracLLM, the first generic context\ntraceback framework tailored to long context LLMs. Our framework can improve\nthe effectiveness and efficiency of existing feature attribution methods. To\nimprove the efficiency, we develop an informed search based algorithm in\nTracLLM. We also develop contribution score ensemble/denoising techniques to\nimprove the accuracy of TracLLM. Our evaluation results show TracLLM can\neffectively identify texts in a long context that lead to the output of an LLM.\nOur code and data are at: https://github.com/Wang-Yanting/TracLLM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86TracLLM\u6846\u67b6\uff0c\u7528\u4e8e\u8ffd\u6eaf\u957f\u4e0a\u4e0b\u6587LLM\u751f\u6210\u8f93\u51fa\u7684\u5177\u4f53\u6587\u672c\u6765\u6e90\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587LLM\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\u9700\u8981\u751f\u6210\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u8f93\u51fa\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u65b9\u6cd5\u8ffd\u6eaf\u54ea\u4e9b\u5177\u4f53\u6587\u672c\u5bf9\u8f93\u51fa\u8d21\u732e\u6700\u5927\uff0c\u5f71\u54cd\u4e86\u8c03\u8bd5\u3001\u5b89\u5168\u5206\u6790\u548c\u7528\u6237\u4fe1\u4efb\u3002", "method": "\u5f00\u53d1\u4e86TracLLM\u6846\u67b6\uff0c\u7ed3\u5408\u57fa\u4e8e\u542f\u53d1\u5f0f\u641c\u7d22\u7684\u7b97\u6cd5\u548c\u8d21\u732e\u5206\u6570\u96c6\u6210/\u53bb\u566a\u6280\u672f\uff0c\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "result": "TracLLM\u80fd\u6709\u6548\u8bc6\u522b\u957f\u4e0a\u4e0b\u6587\u4e2d\u5f71\u54cdLLM\u8f93\u51fa\u7684\u5173\u952e\u6587\u672c\u3002", "conclusion": "TracLLM\u4e3a\u957f\u4e0a\u4e0b\u6587LLM\u7684\u8ffd\u6eaf\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u957f\u4e0a\u4e0b\u6587LLM, \u7279\u5f81\u5f52\u56e0, \u8ffd\u6eaf, \u8c03\u8bd5, \u5b89\u5168\u5206\u6790"}}
{"id": "2506.03796", "pdf": "https://arxiv.org/pdf/2506.03796", "abs": "https://arxiv.org/abs/2506.03796", "authors": ["Penelope Madysa", "Sabrina Appel", "Verena Kain", "Michael Schenk"], "title": "Geoff: The Generic Optimization Framework & Frontend for Particle Accelerator Controls", "categories": ["physics.acc-ph", "cs.LG"], "comment": "18 pages, 5 figures. Submitted to SoftwareX", "summary": "Geoff is a collection of Python packages that form a framework for automation\nof particle accelerator controls. With particle accelerator laboratories around\nthe world researching machine learning techniques to improve accelerator\nperformance and uptime, a multitude of approaches and algorithms have emerged.\nThe purpose of Geoff is to harmonize these approaches and to minimize friction\nwhen comparing or migrating between them. It provides standardized interfaces\nfor optimization problems, utility functions to speed up development, and a\nreference GUI application that ties everything together. Geoff is an\nopen-source library developed at CERN and maintained and updated in\ncollaboration between CERN and GSI as part of the EURO-LABS project. This paper\ngives an overview over Geoff's design, features, and current usage.", "AI": {"tldr": "Geoff\u662f\u4e00\u4e2aPython\u6846\u67b6\uff0c\u65e8\u5728\u7edf\u4e00\u7c92\u5b50\u52a0\u901f\u5668\u63a7\u5236\u81ea\u52a8\u5316\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u63a5\u53e3\u548c\u5de5\u5177\u3002", "motivation": "\u968f\u7740\u5168\u7403\u7c92\u5b50\u52a0\u901f\u5668\u5b9e\u9a8c\u5ba4\u91c7\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u63d0\u5347\u6027\u80fd\uff0c\u9700\u7edf\u4e00\u591a\u6837\u5316\u65b9\u6cd5\u4ee5\u51cf\u5c11\u8fc1\u79fb\u548c\u6bd4\u8f83\u7684\u6469\u64e6\u3002", "method": "\u63d0\u4f9b\u6807\u51c6\u5316\u4f18\u5316\u95ee\u9898\u63a5\u53e3\u3001\u5f00\u53d1\u5de5\u5177\u548c\u53c2\u8003GUI\u5e94\u7528\u3002", "result": "Geoff\u4f5c\u4e3a\u5f00\u6e90\u5e93\uff0c\u7531CERN\u4e0eGSI\u5408\u4f5c\u7ef4\u62a4\uff0c\u5df2\u5e94\u7528\u4e8eEURO-LABS\u9879\u76ee\u3002", "conclusion": "Geoff\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86\u591a\u6837\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002", "keywords": "Geoff,\u7c92\u5b50\u52a0\u901f\u5668,\u673a\u5668\u5b66\u4e60,CERN,EURO-LABS"}}
{"id": "2506.03801", "pdf": "https://arxiv.org/pdf/2506.03801", "abs": "https://arxiv.org/abs/2506.03801", "authors": ["Peter Pfeiffer", "Alexander Rombach", "Maxim Majlatow", "Nijat Mehdiyev"], "title": "From Theory to Practice: Real-World Use Cases on Trustworthy LLM-Driven Process Modeling, Prediction and Automation", "categories": ["cs.SE", "cs.LG", "cs.MA"], "comment": "Accepted to the Next Gen Data and Process Management: Large Language\n  Models and Beyond workshop at SIGMOD 2025", "summary": "Traditional Business Process Management (BPM) struggles with rigidity,\nopacity, and scalability in dynamic environments while emerging Large Language\nModels (LLMs) present transformative opportunities alongside risks. This paper\nexplores four real-world use cases that demonstrate how LLMs, augmented with\ntrustworthy process intelligence, redefine process modeling, prediction, and\nautomation. Grounded in early-stage research projects with industrial partners,\nthe work spans manufacturing, modeling, life-science, and design processes,\naddressing domain-specific challenges through human-AI collaboration. In\nmanufacturing, an LLM-driven framework integrates uncertainty-aware explainable\nMachine Learning (ML) with interactive dialogues, transforming opaque\npredictions into auditable workflows. For process modeling, conversational\ninterfaces democratize BPMN design. Pharmacovigilance agents automate drug\nsafety monitoring via knowledge-graph-augmented LLMs. Finally, sustainable\ntextile design employs multi-agent systems to navigate regulatory and\nenvironmental trade-offs. We intend to examine tensions between transparency\nand efficiency, generalization and specialization, and human agency versus\nautomation. By mapping these trade-offs, we advocate for context-sensitive\nintegration prioritizing domain needs, stakeholder values, and iterative\nhuman-in-the-loop workflows over universal solutions. This work provides\nactionable insights for researchers and practitioners aiming to operationalize\nLLMs in critical BPM environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4f20\u7edfBPM\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6d41\u7a0b\u5efa\u6a21\u3001\u9884\u6d4b\u548c\u81ea\u52a8\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u56db\u4e2a\u5b9e\u9645\u6848\u4f8b\u5c55\u793a\u4e86LLMs\u4e0e\u53ef\u4fe1\u6d41\u7a0b\u667a\u80fd\u7684\u7ed3\u5408\u5982\u4f55\u89e3\u51b3\u9886\u57df\u7279\u5b9a\u6311\u6218\uff0c\u5e76\u5f3a\u8c03\u4e86\u900f\u660e\u6027\u4e0e\u6548\u7387\u3001\u6cdb\u5316\u4e0e\u4e13\u4e1a\u5316\u3001\u4eba\u7c7b\u4e0e\u81ea\u52a8\u5316\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u4f20\u7edfBPM\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u521a\u6027\u548c\u4e0d\u900f\u660e\u6027\uff0c\u800cLLMs\u7684\u51fa\u73b0\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\uff0c\u4f46\u4e5f\u4f34\u968f\u7740\u98ce\u9669\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22LLMs\u5982\u4f55\u901a\u8fc7\u589e\u5f3a\u53ef\u4fe1\u6d41\u7a0b\u667a\u80fd\u6765\u91cd\u65b0\u5b9a\u4e49BPM\u3002", "method": "\u901a\u8fc7\u56db\u4e2a\u5b9e\u9645\u6848\u4f8b\uff08\u5236\u9020\u3001\u6d41\u7a0b\u5efa\u6a21\u3001\u836f\u7269\u5b89\u5168\u76d1\u63a7\u548c\u53ef\u6301\u7eed\u7eba\u7ec7\u8bbe\u8ba1\uff09\uff0c\u57fa\u4e8e\u5de5\u4e1a\u5408\u4f5c\u7684\u65e9\u671f\u7814\u7a76\u9879\u76ee\uff0c\u7ed3\u5408\u4eba\u7c7b-AI\u534f\u4f5c\u548c\u9886\u57df\u7279\u5b9a\u7684\u65b9\u6cd5\uff08\u5982\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684ML\u3001\u5bf9\u8bdd\u5f0f\u754c\u9762\u3001\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7684LLMs\u548c\u591aAgent\u7cfb\u7edf\uff09\u3002", "result": "LLMs\u5728\u5236\u9020\u4e2d\u63d0\u5347\u4e86\u9884\u6d4b\u7684\u53ef\u5ba1\u8ba1\u6027\uff1b\u5728\u6d41\u7a0b\u5efa\u6a21\u4e2d\u5b9e\u73b0\u4e86BPMN\u8bbe\u8ba1\u7684\u6c11\u4e3b\u5316\uff1b\u5728\u836f\u7269\u5b89\u5168\u76d1\u63a7\u4e2d\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\uff1b\u5728\u53ef\u6301\u7eed\u7eba\u7ec7\u8bbe\u8ba1\u4e2d\u5e94\u5bf9\u4e86\u76d1\u7ba1\u548c\u73af\u5883\u6743\u8861\u3002", "conclusion": "\u8bba\u6587\u4e3b\u5f20\u6839\u636e\u9886\u57df\u9700\u6c42\u3001\u5229\u76ca\u76f8\u5173\u8005\u4ef7\u503c\u548c\u8fed\u4ee3\u7684\u4eba\u7c7b\u53c2\u4e0e\u6d41\u7a0b\uff0c\u4f18\u5148\u9009\u62e9\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u96c6\u6210\u65b9\u6848\uff0c\u800c\u975e\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "BPM\uff08\u4e1a\u52a1\u6d41\u7a0b\u7ba1\u7406\uff09, LLMs\uff08\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09, \u4eba\u7c7b-AI\u534f\u4f5c, \u6d41\u7a0b\u5efa\u6a21, \u81ea\u52a8\u5316"}}
{"id": "2506.04215", "pdf": "https://arxiv.org/pdf/2506.04215", "abs": "https://arxiv.org/abs/2506.04215", "authors": ["Alex DeWeese", "Guannan Qu"], "title": "Thinking Beyond Visibility: A Near-Optimal Policy Framework for Locally Interdependent Multi-Agent MDPs", "categories": ["cs.MA", "cs.AI", "cs.LG", "math.OC"], "comment": null, "summary": "Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) are\nknown to be NEXP-Complete and intractable to solve. However, for problems such\nas cooperative navigation, obstacle avoidance, and formation control, basic\nassumptions can be made about local visibility and local dependencies. The work\nDeWeese and Qu 2024 formalized these assumptions in the construction of the\nLocally Interdependent Multi-Agent MDP. In this setting, it establishes three\nclosed-form policies that are tractable to compute in various situations and\nare exponentially close to optimal with respect to visibility. However, it is\nalso shown that these solutions can have poor performance when the visibility\nis small and fixed, often getting stuck during simulations due to the so called\n\"Penalty Jittering\" phenomenon. In this work, we establish the Extended Cutoff\nPolicy Class which is, to the best of our knowledge, the first non-trivial\nclass of near optimal closed-form partially observable policies that are\nexponentially close to optimal with respect to the visibility for any Locally\nInterdependent Multi-Agent MDP. These policies are able to remember agents\nbeyond their visibilities which allows them to perform significantly better in\nmany small and fixed visibility settings, resolve Penalty Jittering\noccurrences, and under certain circumstances guarantee fully observable joint\noptimal behavior despite the partial observability. We also propose a\ngeneralized form of the Locally Interdependent Multi-Agent MDP that allows for\ntransition dependence and extended reward dependence, then replicate our\ntheoretical results in this setting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Extended Cutoff Policy Class\uff0c\u8fd9\u662f\u9996\u4e2a\u975e\u5e73\u51e1\u7684\u3001\u5728\u5c40\u90e8\u4f9d\u8d56\u591a\u667a\u80fd\u4f53MDP\u4e2d\u9488\u5bf9\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u8bbe\u8ba1\u7684\u63a5\u8fd1\u6700\u4f18\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5c0f\u56fa\u5b9a\u53ef\u89c6\u6027\u4e0b\u7684\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u9488\u5bf9Dec-POMDPs\u7684\u9ad8\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u7814\u7a76\u5728\u5c40\u90e8\u4f9d\u8d56\u591a\u667a\u80fd\u4f53MDP\u4e2d\u7684\u53ef\u89c2\u6d4b\u6027\u6311\u6218\uff0c\u7279\u522b\u662f\u5c0f\u56fa\u5b9a\u53ef\u89c6\u6027\u4e0b\u7b56\u7565\u6027\u80fd\u4e0d\u4f73\u7684\u201cPenalty Jittering\u201d\u73b0\u8c61\u3002", "method": "\u63d0\u51faExtended Cutoff Policy Class\uff0c\u8be5\u7b56\u7565\u7c7b\u80fd\u8bb0\u5fc6\u8d85\u51fa\u53ef\u89c6\u8303\u56f4\u7684\u667a\u80fd\u4f53\uff0c\u5e76\u5728\u5c40\u90e8\u4f9d\u8d56\u591a\u667a\u80fd\u4f53MDP\u4e2d\u7406\u8bba\u9a8c\u8bc1\u5176\u63a5\u8fd1\u6700\u4f18\u6027\u3002\u8fd8\u6269\u5c55\u4e86\u6a21\u578b\u4ee5\u5305\u542b\u72b6\u6001\u8f6c\u79fb\u548c\u5956\u52b1\u4f9d\u8d56\u3002", "result": "\u65b0\u7b56\u7565\u663e\u8457\u6539\u5584\u4e86\u5c0f\u56fa\u5b9a\u53ef\u89c6\u6027\u4e0b\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u201cPenalty Jittering\u201d\u95ee\u9898\uff0c\u5e76\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u5b8c\u5168\u53ef\u89c2\u6d4b\u7684\u6700\u4f18\u884c\u4e3a\u3002", "conclusion": "Extended Cutoff Policy Class\u4e3a\u5c40\u90e8\u4f9d\u8d56\u591a\u667a\u80fd\u4f53MDP\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "Dec-POMDPs, \u5c40\u90e8\u4f9d\u8d56\u591a\u667a\u80fd\u4f53MDP, \u53ef\u89c2\u6d4b\u6027, \u6700\u4f18\u7b56\u7565, Penalty Jittering"}}
{"id": "2506.03819", "pdf": "https://arxiv.org/pdf/2506.03819", "abs": "https://arxiv.org/abs/2506.03819", "authors": ["Marc Aurel Vischer", "Noelia Otero", "Jackie Ma"], "title": "Spatially Resolved Meteorological and Ancillary Data in Central Europe for Rainfall Streamflow Modeling", "categories": ["stat.ML", "cs.LG", "I.2.1; I.6.5; J.2"], "comment": "6 pages, 1 figure", "summary": "We present a dataset for rainfall streamflow modeling that is fully spatially\nresolved with the aim of taking neural network-driven hydrological modeling\nbeyond lumped catchments. To this end, we compiled data covering five river\nbasins in central Europe: upper Danube, Elbe, Oder, Rhine, and Weser. The\ndataset contains meteorological forcings, as well as ancillary information on\nsoil, rock, land cover, and orography. The data is harmonized to a regular 9km\ntimes 9km grid and contains daily values that span from October 1981 to\nSeptember 2011. We also provide code to further combine our dataset with\npublicly available river discharge data for end-to-end rainfall streamflow\nmodeling.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u964d\u96e8\u5f84\u6d41\u5efa\u6a21\u6570\u636e\u96c6\uff0c\u65e8\u5728\u6269\u5c55\u795e\u7ecf\u7f51\u7edc\u9a71\u52a8\u7684\u6c34\u6587\u5efa\u6a21\u5e94\u7528\u3002", "motivation": "\u8d85\u8d8a\u4f20\u7edf\u7684\u96c6\u603b\u6d41\u57df\u6a21\u578b\uff0c\u5efa\u7acb\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u96c6\u4ee5\u652f\u6301\u66f4\u5148\u8fdb\u7684\u795e\u7ecf\u7f51\u7edc\u6c34\u6587\u5efa\u6a21\u3002", "method": "\u6536\u96c6\u5e76\u6574\u5408\u4e86\u4e94\u4e2a\u6b27\u6d32\u6cb3\u6d41\u57df\u7684\u6c14\u8c61\u6570\u636e\u548c\u8f85\u52a9\u4fe1\u606f\uff08\u5982\u571f\u58e4\u3001\u5ca9\u77f3\u3001\u571f\u5730\u8986\u76d6\u548c\u5730\u5f62\uff09\uff0c\u6570\u636e\u7edf\u4e00\u52309\u516c\u91cc\u00d79\u516c\u91cc\u7684\u7f51\u683c\u4e0a\uff0c\u5e76\u63d0\u4f9b\u5de5\u5177\u4e0e\u516c\u5f00\u7684\u6cb3\u6d41\u6d41\u91cf\u6570\u636e\u7ed3\u5408\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8986\u76d61981\u5e74\u81f32011\u5e74\u7684\u65e5\u503c\u6570\u636e\u96c6\uff0c\u5e76\u652f\u6301\u7aef\u5230\u7aef\u7684\u964d\u96e8\u5f84\u6d41\u5efa\u6a21\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u6c34\u6587\u5efa\u6a21\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u5206\u8fa8\u7387\u548c\u5168\u9762\u7684\u652f\u6301\u3002", "keywords": "\u964d\u96e8\u5f84\u6d41\u5efa\u6a21,\u795e\u7ecf\u7f51\u7edc,\u6c34\u6587\u6570\u636e\u96c6,\u7a7a\u95f4\u5206\u8fa8\u7387"}}
{"id": "2506.04217", "pdf": "https://arxiv.org/pdf/2506.04217", "abs": "https://arxiv.org/abs/2506.04217", "authors": ["Junting Chen", "Haotian Liang", "Lingxiao Du", "Weiyun Wang", "Mengkang Hu", "Yao Mu", "Wenhai Wang", "Jifeng Dai", "Ping Luo", "Wenqi Shao", "Lin Shao"], "title": "OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis", "categories": ["cs.RO", "cs.AI", "I.2.4; I.2.9; I.2.10"], "comment": "9 pages of main content, 19 pages in total", "summary": "The rapid progress of navigation, manipulation, and vision models has made\nmobile manipulators capable in many specialized tasks. However, the open-world\nmobile manipulation (OWMM) task remains a challenge due to the need for\ngeneralization to open-ended instructions and environments, as well as the\nsystematic complexity to integrate high-level decision making with low-level\nrobot control based on both global scene understanding and current agent state.\nTo address this complexity, we propose a novel multi-modal agent architecture\nthat maintains multi-view scene frames and agent states for decision-making and\ncontrols the robot by function calling. A second challenge is the hallucination\nfrom domain shift. To enhance the agent performance, we further introduce an\nagentic data synthesis pipeline for the OWMM task to adapt the VLM model to our\ntask domain with instruction fine-tuning. We highlight our fine-tuned OWMM-VLM\nas the first dedicated foundation model for mobile manipulators with global\nscene understanding, robot state tracking, and multi-modal action generation in\na unified model. Through experiments, we demonstrate that our model achieves\nSOTA performance compared to other foundation models including GPT-4o and\nstrong zero-shot generalization in real world. The project page is at\nhttps://github.com/HHYHRHY/OWMM-Agent", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u89e3\u51b3\u5f00\u653e\u4e16\u754c\u79fb\u52a8\u64cd\u7eb5\u4efb\u52a1\u7684\u590d\u6742\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u529f\u80fd\u8c03\u7528\u63a7\u5236\u673a\u5668\u4eba\uff0c\u540c\u65f6\u5f15\u5165\u6570\u636e\u5408\u6210\u7ba1\u9053\u4ee5\u589e\u5f3a\u6027\u80fd\u3002", "motivation": "\u5f00\u653e\u4e16\u754c\u79fb\u52a8\u64cd\u7eb5\u4efb\u52a1\u7531\u4e8e\u9700\u8981\u5bf9\u5f00\u653e\u6307\u4ee4\u548c\u73af\u5883\u8fdb\u884c\u6cdb\u5316\uff0c\u5e76\u7ed3\u5408\u9ad8\u5c42\u51b3\u7b56\u4e0e\u4f4e\u5c42\u673a\u5668\u4eba\u63a7\u5236\uff0c\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u7ef4\u62a4\u591a\u89c6\u89d2\u573a\u666f\u5e27\u548c\u667a\u80fd\u4f53\u72b6\u6001\u8fdb\u884c\u51b3\u7b56\uff0c\u5e76\u5f15\u5165\u6570\u636e\u5408\u6210\u7ba1\u9053\u4f18\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5728\u5f00\u653e\u4e16\u754c\u79fb\u52a8\u64cd\u7eb5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u57fa\u7840\u6a21\u578b\uff0c\u5982GPT-4o\uff0c\u5e76\u5728\u73b0\u5b9e\u4e2d\u5177\u6709\u5f3a\u96f6-shot\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684OWMM-VLM\u662f\u9996\u4e2a\u4e3a\u79fb\u52a8\u64cd\u7eb5\u5668\u8bbe\u8ba1\u7684\u4e13\u7528\u57fa\u7840\u6a21\u578b\uff0c\u96c6\u6210\u4e86\u5168\u5c40\u573a\u666f\u7406\u89e3\u3001\u673a\u5668\u4eba\u72b6\u6001\u8ddf\u8e2a\u548c\u591a\u6a21\u6001\u52a8\u4f5c\u751f\u6210\u3002", "keywords": "\u5f00\u653e\u4e16\u754c\u79fb\u52a8\u64cd\u7eb5\u3001\u591a\u6a21\u6001\u667a\u80fd\u4f53\u3001\u6570\u636e\u5408\u6210\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u96f6-shot\u6cdb\u5316"}}
{"id": "2506.04218", "pdf": "https://arxiv.org/pdf/2506.04218", "abs": "https://arxiv.org/abs/2506.04218", "authors": ["Wei Cao", "Marcel Hallgarten", "Tianyu Li", "Daniel Dauner", "Xunjiang Gu", "Caojun Wang", "Yakov Miron", "Marco Aiello", "Hongyang Li", "Igor Gilitschenski", "Boris Ivanovic", "Marco Pavone", "Andreas Geiger", "Kashyap Chitta"], "title": "Pseudo-Simulation for Autonomous Driving", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical\nlimitations. Real-world evaluation is often challenging due to safety concerns\nand a lack of reproducibility, whereas closed-loop simulation can face\ninsufficient realism or high computational costs. Open-loop evaluation, while\nbeing efficient and data-driven, relies on metrics that generally overlook\ncompounding errors. In this paper, we propose pseudo-simulation, a novel\nparadigm that addresses these limitations. Pseudo-simulation operates on real\ndatasets, similar to open-loop evaluation, but augments them with synthetic\nobservations generated prior to evaluation using 3D Gaussian Splatting. Our key\nidea is to approximate potential future states the AV might encounter by\ngenerating a diverse set of observations that vary in position, heading, and\nspeed. Our method then assigns a higher importance to synthetic observations\nthat best match the AV's likely behavior using a novel proximity-based\nweighting scheme. This enables evaluating error recovery and the mitigation of\ncausal confusion, as in closed-loop benchmarks, without requiring sequential\ninteractive simulation. We show that pseudo-simulation is better correlated\nwith closed-loop simulations (R^2=0.8) than the best existing open-loop\napproach (R^2=0.7). We also establish a public leaderboard for the community to\nbenchmark new methodologies with pseudo-simulation. Our code is available at\nhttps://github.com/autonomousvision/navsim.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u201c\u4f2a\u6a21\u62df\u201d\u7684\u65b0\u8bc4\u4f30\u8303\u5f0f\uff0c\u7ed3\u5408\u771f\u5b9e\u6570\u636e\u96c6\u4e0e\u5408\u6210\u89c2\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u3001\u95ed\u73af\u6a21\u62df\u548c\u5f00\u73af\u8bc4\u4f30\uff09\u5b58\u5728\u5b89\u5168\u6027\u3001\u771f\u5b9e\u6027\u548c\u8ba1\u7b97\u6210\u672c\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5229\u7528\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u901a\u8fc73D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u751f\u6210\u591a\u6837\u5316\u5408\u6210\u89c2\u6d4b\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u90bb\u8fd1\u6027\u7684\u6743\u91cd\u65b9\u6848\uff0c\u8fd1\u4f3c\u6a21\u62df\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u53ef\u80fd\u9047\u5230\u7684\u672a\u6765\u72b6\u6001\u3002", "result": "\u4f2a\u6a21\u62df\u4e0e\u95ed\u73af\u6a21\u62df\u7684\u76f8\u5173\u6027\uff08R\u00b2=0.8\uff09\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u5f00\u73af\u65b9\u6cd5\uff08R\u00b2=0.7\uff09\uff0c\u540c\u65f6\u5efa\u7acb\u4e86\u516c\u5171\u6392\u884c\u699c\u4ee5\u4fc3\u8fdb\u65b9\u6cd5\u6bd4\u8f83\u3002", "conclusion": "\u4f2a\u6a21\u62df\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u4e14\u6570\u636e\u9a71\u52a8\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u8303\u5f0f\u7684\u4e0d\u8db3\uff0c\u6210\u4e3a\u95ed\u73af\u6a21\u62df\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\u3002", "keywords": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86,\u4f2a\u6a21\u62df,3D\u9ad8\u65af\u6cfc\u6e85,\u5f00\u73af\u8bc4\u4f30,\u95ed\u73af\u6a21\u62df"}}
{"id": "2506.03849", "pdf": "https://arxiv.org/pdf/2506.03849", "abs": "https://arxiv.org/abs/2506.03849", "authors": ["Benjamin Dupuis", "Dario Shariatian", "Maxime Haddouche", "Alain Durmus", "Umut Simsekli"], "title": "Algorithm- and Data-Dependent Generalization Bounds for Score-Based Generative Models", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Score-based generative models (SGMs) have emerged as one of the most popular\nclasses of generative models. A substantial body of work now exists on the\nanalysis of SGMs, focusing either on discretization aspects or on their\nstatistical performance. In the latter case, bounds have been derived, under\nvarious metrics, between the true data distribution and the distribution\ninduced by the SGM, often demonstrating polynomial convergence rates with\nrespect to the number of training samples. However, these approaches adopt a\nlargely approximation theory viewpoint, which tends to be overly pessimistic\nand relatively coarse. In particular, they fail to fully explain the empirical\nsuccess of SGMs or capture the role of the optimization algorithm used in\npractice to train the score network. To support this observation, we first\npresent simple experiments illustrating the concrete impact of optimization\nhyperparameters on the generalization ability of the generated distribution.\nThen, this paper aims to bridge this theoretical gap by providing the first\nalgorithmic- and data-dependent generalization analysis for SGMs. In\nparticular, we establish bounds that explicitly account for the optimization\ndynamics of the learning algorithm, offering new insights into the\ngeneralization behavior of SGMs. Our theoretical findings are supported by\nempirical results on several datasets.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5206\u6570\u7684\u751f\u6210\u6a21\u578b\uff08SGMs\uff09\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u5173\u6ce8\u4f18\u5316\u7b97\u6cd5\u548c\u8d85\u53c2\u6570\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u586b\u8865\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u5bf9SGMs\u7684\u5206\u6790\u4e3b\u8981\u96c6\u4e2d\u5728\u79bb\u6563\u5316\u6216\u7edf\u8ba1\u6027\u80fd\u4e0a\uff0c\u7f3a\u4e4f\u4f18\u5316\u7b97\u6cd5\u7684\u6df1\u5165\u63a2\u8ba8\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7b97\u6cd5\u548c\u6570\u636e\u76f8\u5173\u7684\u6cdb\u5316\u5206\u6790\uff0c\u89e3\u91caSGMs\u7684\u5b9e\u9645\u6210\u529f\u3002", "method": "\u901a\u8fc7\u63d0\u51fa\u4f18\u5316\u52a8\u6001\u663e\u5f0f\u8003\u8651\u7684\u6cdb\u5316\u8fb9\u754c\uff0c\u7ed3\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u8d85\u53c2\u6570\u5bf9\u751f\u6210\u5206\u5e03\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u8868\u660e\u4f18\u5316\u7b97\u6cd5\u548c\u8d85\u53c2\u6570\u5bf9SGMs\u7684\u6cdb\u5316\u80fd\u529b\u6709\u663e\u8457\u5f71\u54cd\uff0c\u7406\u8bba\u5206\u6790\u652f\u6301\u4e86\u8fd9\u4e00\u89c2\u5bdf\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u89c6\u89d2\uff0c\u63ed\u793a\u4e86SGMs\u6cdb\u5316\u884c\u4e3a\u7684\u5173\u952e\u56e0\u7d20\u3002", "keywords": "score-based generative models, generalization analysis, optimization dynamics, empirical success"}}
{"id": "2506.03863", "pdf": "https://arxiv.org/pdf/2506.03863", "abs": "https://arxiv.org/abs/2506.03863", "authors": ["Hao Li", "Qi Lv", "Rui Shao", "Xiang Deng", "Yinchuan Li", "Jianye Hao", "Liqiang Nie"], "title": "STAR: Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization", "categories": ["cs.RO", "cs.LG"], "comment": "Accepted by ICML 2025 Spotlight", "summary": "Transforming complex actions into discrete skill abstractions has\ndemonstrated strong potential for robotic manipulation. Existing approaches\nmainly leverage latent variable models, e.g., VQ-VAE, to learn skill\nabstractions through learned vectors (codebooks), while they suffer from\ncodebook collapse and modeling the causal relationship between learned skills.\nTo address these limitations, we present \\textbf{S}kill \\textbf{T}raining with\n\\textbf{A}ugmented \\textbf{R}otation (\\textbf{STAR}), a framework that advances\nboth skill learning and composition to complete complex behaviors.\nSpecifically, to prevent codebook collapse, we devise rotation-augmented\nresidual skill quantization (RaRSQ). It encodes relative angles between encoder\noutputs into the gradient flow by rotation-based gradient mechanism. Points\nwithin the same skill code are forced to be either pushed apart or pulled\ncloser together depending on gradient directions. Further, to capture the\ncausal relationship between skills, we present causal skill transformer (CST)\nwhich explicitly models dependencies between skill representations through an\nautoregressive mechanism for coherent action generation. Extensive experiments\ndemonstrate the superiority of STAR on both LIBERO benchmark and realworld\ntasks, with around 12\\% improvement over the baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSTAR\u6846\u67b6\uff0c\u901a\u8fc7\u65cb\u8f6c\u589e\u5f3a\u6b8b\u5dee\u6280\u80fd\u91cf\u5316\u548c\u56e0\u679c\u6280\u80fd\u53d8\u6362\u5668\uff0c\u89e3\u51b3\u6280\u80fd\u62bd\u8c61\u5b66\u4e60\u4e2d\u7684\u4ee3\u7801\u672c\u5d29\u6e83\u548c\u56e0\u679c\u5173\u7cfb\u5efa\u6a21\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\uff08\u5982VQ-VAE\uff09\u5b66\u4e60\u6280\u80fd\u62bd\u8c61\uff0c\u4f46\u5b58\u5728\u4ee3\u7801\u672c\u5d29\u6e83\u548c\u6280\u80fd\u95f4\u56e0\u679c\u5173\u7cfb\u5efa\u6a21\u4e0d\u8db3\u7684\u5c40\u9650\u3002", "method": "1. \u65cb\u8f6c\u589e\u5f3a\u6b8b\u5dee\u6280\u80fd\u91cf\u5316\uff08RaRSQ\uff09\u9632\u6b62\u4ee3\u7801\u672c\u5d29\u6e83\u30022. \u56e0\u679c\u6280\u80fd\u53d8\u6362\u5668\uff08CST\uff09\u901a\u8fc7\u81ea\u56de\u5f52\u673a\u5236\u663e\u5f0f\u5efa\u6a21\u6280\u80fd\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u548c\u5b9e\u9645\u4efb\u52a1\u4e2d\uff0cSTAR\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u5347\u7ea612%\u3002", "conclusion": "STAR\u901a\u8fc7RaRSQ\u548cCST\u7684\u8054\u5408\u4f18\u5316\uff0c\u63d0\u5347\u4e86\u6280\u80fd\u5b66\u4e60\u548c\u7ec4\u5408\u80fd\u529b\u3002", "keywords": "\u673a\u5668\u4eba\u64cd\u4f5c, \u6280\u80fd\u62bd\u8c61, VQ-VAE, \u4ee3\u7801\u672c\u5d29\u6e83, \u56e0\u679c\u5efa\u6a21"}}
{"id": "2506.04227", "pdf": "https://arxiv.org/pdf/2506.04227", "abs": "https://arxiv.org/abs/2506.04227", "authors": ["Zhao-Heng Yin", "Sherry Yang", "Pieter Abbeel"], "title": "Object-centric 3D Motion Field for Robot Learning from Human Videos", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "Project: https://zhaohengyin.github.io/3DMF", "summary": "Learning robot control policies from human videos is a promising direction\nfor scaling up robot learning. However, how to extract action knowledge (or\naction representations) from videos for policy learning remains a key\nchallenge. Existing action representations such as video frames, pixelflow, and\npointcloud flow have inherent limitations such as modeling complexity or loss\nof information. In this paper, we propose to use object-centric 3D motion field\nto represent actions for robot learning from human videos, and present a novel\nframework for extracting this representation from videos for zero-shot control.\nWe introduce two novel components in its implementation. First, a novel\ntraining pipeline for training a ''denoising'' 3D motion field estimator to\nextract fine object 3D motions from human videos with noisy depth robustly.\nSecond, a dense object-centric 3D motion field prediction architecture that\nfavors both cross-embodiment transfer and policy generalization to background.\nWe evaluate the system in real world setups. Experiments show that our method\nreduces 3D motion estimation error by over 50% compared to the latest method,\nachieve 55% average success rate in diverse tasks where prior approaches\nfail~($\\lesssim 10$\\%), and can even acquire fine-grained manipulation skills\nlike insertion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u76843D\u8fd0\u52a8\u573a\u8868\u793a\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u4f5c\u8868\u793a\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4ece\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\u662f\u4e00\u4e2a\u6709\u6f5c\u529b\u7684\u65b9\u5411\uff0c\u4f46\u73b0\u6709\u52a8\u4f5c\u8868\u793a\u65b9\u6cd5\uff08\u5982\u89c6\u9891\u5e27\u3001\u50cf\u7d20\u6d41\u7b49\uff09\u5b58\u5728\u5efa\u6a21\u590d\u6742\u6216\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u76843D\u8fd0\u52a8\u573a\u8868\u793a\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u65b0\u7ec4\u4ef6\uff1a1\uff09\u53bb\u566a3D\u8fd0\u52a8\u573a\u4f30\u8ba1\u5668\u8bad\u7ec3\u6d41\u7a0b\uff1b2\uff09\u5bc6\u96c6\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u8fd0\u52a8\u573a\u9884\u6d4b\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u4e2d\uff0c3D\u8fd0\u52a8\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e\u4e8650%\u4ee5\u4e0a\uff0c\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u5e73\u5747\u6210\u529f\u738755%\uff08\u4e4b\u524d\u65b9\u6cd5<10%\uff09\uff0c\u5e76\u80fd\u4e60\u5f97\u7cbe\u7ec6\u64cd\u4f5c\u6280\u80fd\uff08\u5982\u63d2\u5165\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4ece\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u63a7\u5236\u7b56\u7565\u7684\u6548\u679c\uff0c\u5177\u6709\u8de8\u5177\u8eab\u6027\u548c\u80cc\u666f\u6cdb\u5316\u7684\u4f18\u52bf\u3002", "keywords": "\u673a\u5668\u4eba\u5b66\u4e60\u30013D\u8fd0\u52a8\u573a\u3001\u52a8\u4f5c\u8868\u793a\u3001\u96f6\u6837\u672c\u63a7\u5236\u3001\u8de8\u5177\u8eab\u6027"}}
{"id": "2506.03974", "pdf": "https://arxiv.org/pdf/2506.03974", "abs": "https://arxiv.org/abs/2506.03974", "authors": ["Cl\u00e9ment Elvira", "Th\u00e9o Guyard", "C\u00e9dric Herzet"], "title": "A Generic Branch-and-Bound Algorithm for $\\ell_0$-Penalized Problems with Supplementary Material", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": null, "summary": "We present a generic Branch-and-Bound procedure designed to solve\nL0-penalized optimization problems. Existing approaches primarily focus on\nquadratic losses and construct relaxations using \"Big-M\" constraints and/or\nL2-norm penalties. In contrast, our method accommodates a broader class of loss\nfunctions and allows greater flexibility in relaxation design through a general\npenalty term, encompassing existing techniques as special cases. We establish\ntheoretical results ensuring that all key quantities required for the\nBranch-and-Bound implementation admit closed-form expressions under the general\nblanket assumptions considered in our work. Leveraging this framework, we\nintroduce El0ps, an open-source Python solver with a plug-and-play workflow\nthat enables user-defined losses and penalties in L0-penalized problems.\nThrough extensive numerical experiments, we demonstrate that El0ps achieves\nstate-of-the-art performance on classical instances and extends computational\nfeasibility to previously intractable ones.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u5206\u652f\u5b9a\u754c\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3L0\u60e9\u7f5a\u4f18\u5316\u95ee\u9898\uff0c\u652f\u6301\u591a\u79cd\u635f\u5931\u51fd\u6570\u548c\u7075\u6d3b\u7684\u677e\u5f1b\u8bbe\u8ba1\uff0c\u5e76\u5f00\u53d1\u4e86\u5f00\u6e90\u6c42\u89e3\u5668El0ps\uff0c\u5c55\u793a\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684L0\u60e9\u7f5a\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u4e8c\u6b21\u635f\u5931\uff0c\u4f7f\u7528'Big-M'\u7ea6\u675f\u6216L2\u8303\u6570\u60e9\u7f5a\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u9002\u7528\u8303\u56f4\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u5206\u652f\u5b9a\u754c\u65b9\u6cd5\uff0c\u652f\u6301\u5e7f\u6cdb\u7684\u635f\u5931\u51fd\u6570\u548c\u677e\u5f1b\u8bbe\u8ba1\uff0c\u5f15\u5165\u4e86\u901a\u7528\u60e9\u7f5a\u9879\uff0c\u6db5\u76d6\u73b0\u6709\u6280\u672f\u4f5c\u4e3a\u7279\u4f8b\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u6240\u6709\u5173\u952e\u91cf\u5728\u95ed\u5f0f\u8868\u8fbe\u4e0b\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u5f00\u6e90\u6c42\u89e3\u5668El0ps\uff0c\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\u548c\u7075\u6d3b\u6027\uff0c\u80fd\u591f\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7El0ps\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6c42\u89e3\u3002", "keywords": "L0\u60e9\u7f5a\uff0c\u5206\u652f\u5b9a\u754c\uff0c\u4f18\u5316\u95ee\u9898\uff0c\u5f00\u6e90\u6c42\u89e3\u5668\uff0cEl0ps"}}
{"id": "2506.03988", "pdf": "https://arxiv.org/pdf/2506.03988", "abs": "https://arxiv.org/abs/2506.03988", "authors": ["Hicham Eddoubi", "Jonas Ricker", "Federico Cocchi", "Lorenzo Baraldi", "Angelo Sotgiu", "Maura Pintor", "Marcella Cornia", "Lorenzo Baraldi", "Asja Fischer", "Rita Cucchiara", "Battista Biggio"], "title": "RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated Image Detectors", "categories": ["cs.CV", "cs.LG"], "comment": "Under review for NeurIPS 2025 Datasets and Benchmarks Track", "summary": "AI-generated images have reached a quality level at which humans are\nincapable of reliably distinguishing them from real images. To counteract the\ninherent risk of fraud and disinformation, the detection of AI-generated images\nis a pressing challenge and an active research topic. While many of the\npresented methods claim to achieve high detection accuracy, they are usually\nevaluated under idealized conditions. In particular, the adversarial robustness\nis often neglected, potentially due to a lack of awareness or the substantial\neffort required to conduct a comprehensive robustness analysis. In this work,\nwe tackle this problem by providing a simpler means to assess the robustness of\nAI-generated image detectors. We present RAID (Robust evaluation of\nAI-generated image Detectors), a dataset of 72k diverse and highly transferable\nadversarial examples. The dataset is created by running attacks against an\nensemble of seven state-of-the-art detectors and images generated by four\ndifferent text-to-image models. Extensive experiments show that our methodology\ngenerates adversarial images that transfer with a high success rate to unseen\ndetectors, which can be used to quickly provide an approximate yet still\nreliable estimate of a detector's adversarial robustnessOur findings indicate\nthat current state-of-the-art AI-generated image detectors can be easily\ndeceived by adversarial examples, highlighting the critical need for the\ndevelopment of more robust methods. We release our dataset at\nhttps://huggingface.co/datasets/aimagelab/RAID and evaluation code at\nhttps://github.com/pralab/RAID.", "AI": {"tldr": "AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u5de5\u5177RAID\uff0c\u901a\u8fc7\u5bf9\u6297\u6837\u672c\u6d4b\u8bd5\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u6613\u88ab\u6b3a\u9a97\u3002", "motivation": "\u89e3\u51b3AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u63d0\u51faRAID\u6570\u636e\u96c6\uff0c\u5305\u542b7.2\u4e07\u591a\u6837\u5bf9\u6297\u6837\u672c\uff0c\u901a\u8fc7\u653b\u51fb\u591a\u4e2a\u68c0\u6d4b\u5668\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5bf9\u6297\u6837\u672c\u5bf9\u672a\u89c1\u68c0\u6d4b\u5668\u9ad8\u8fc1\u79fb\u7387\uff0c\u73b0\u6709\u65b9\u6cd5\u6613\u88ab\u6b3a\u9a97\u3002", "conclusion": "\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u3002", "keywords": "AI\u751f\u6210\u56fe\u50cf\u3001\u68c0\u6d4b\u5668\u3001\u5bf9\u6297\u6837\u672c\u3001\u9c81\u68d2\u6027\u8bc4\u4f30"}}
{"id": "2506.04016", "pdf": "https://arxiv.org/pdf/2506.04016", "abs": "https://arxiv.org/abs/2506.04016", "authors": ["Adam Ran\u00e7on", "Ulysse Ran\u00e7on", "Tomislav Ivek", "Ivan Balog"], "title": "Dreaming up scale invariance via inverse renormalization group", "categories": ["cond-mat.stat-mech", "cs.CV", "cs.LG"], "comment": "v1: 12 pages, 11 figures, 55 references", "summary": "We explore how minimal neural networks can invert the renormalization group\n(RG) coarse-graining procedure in the two-dimensional Ising model, effectively\n\"dreaming up\" microscopic configurations from coarse-grained states. This\ntask-formally impossible at the level of configurations-can be approached\nprobabilistically, allowing machine learning models to reconstruct\nscale-invariant distributions without relying on microscopic input. We\ndemonstrate that even neural networks with as few as three trainable parameters\ncan learn to generate critical configurations, reproducing the scaling behavior\nof observables such as magnetic susceptibility, heat capacity, and Binder\nratios. A real-space renormalization group analysis of the generated\nconfigurations confirms that the models capture not only scale invariance but\nalso reproduce nontrivial eigenvalues of the RG transformation. Surprisingly,\nwe find that increasing network complexity by introducing multiple layers\noffers no significant benefit. These findings suggest that simple local rules,\nakin to those generating fractal structures, are sufficient to encode the\nuniversality of critical phenomena, opening the door to efficient generative\nmodels of statistical ensembles in physics.", "AI": {"tldr": "\u63a2\u7d22\u6781\u5c0f\u795e\u7ecf\u7f51\u7edc\u5982\u4f55\u9006\u5411\u91cd\u6b63\u5316\u7fa4\uff08RG\uff09\u7c97\u7c92\u5316\u8fc7\u7a0b\uff0c\u5728\u4e8c\u7ef4Ising\u6a21\u578b\u4e2d\u751f\u6210\u5fae\u89c2\u914d\u7f6e\u3002", "motivation": "\u7814\u7a76\u80fd\u5426\u901a\u8fc7\u6982\u7387\u65b9\u6cd5\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4ece\u7c97\u7c92\u72b6\u6001\u91cd\u6784\u5c3a\u5ea6\u4e0d\u53d8\u5206\u5e03\uff0c\u800c\u4e0d\u4f9d\u8d56\u5fae\u89c2\u8f93\u5165\u3002", "method": "\u4f7f\u7528\u4ec5\u542b\u4e09\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u6781\u5c0f\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u4e34\u754c\u914d\u7f6e\uff0c\u5206\u6790\u5176\u6807\u5ea6\u884c\u4e3a\u3002", "result": "\u7b80\u5355\u6a21\u578b\u80fd\u6210\u529f\u751f\u6210\u4e34\u754c\u914d\u7f6e\uff0c\u91cd\u73b0RG\u53d8\u6362\u7684\u975e\u5e73\u51e1\u7279\u5f81\u503c\uff0c\u4e14\u589e\u52a0\u7f51\u7edc\u590d\u6742\u5ea6\u65e0\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u7b80\u5355\u5c40\u90e8\u89c4\u5219\u8db3\u4ee5\u7f16\u7801\u4e34\u754c\u73b0\u8c61\u7684\u666e\u9002\u6027\uff0c\u4e3a\u9ad8\u6548\u751f\u6210\u7edf\u8ba1\u7269\u7406\u4e2d\u7684\u7edf\u8ba1\u7cfb\u7efc\u6a21\u578b\u5f00\u8f9f\u65b0\u9014\u5f84\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc, \u91cd\u6b63\u5316\u7fa4, Ising\u6a21\u578b, \u5c3a\u5ea6\u4e0d\u53d8\u6027, \u751f\u6210\u6a21\u578b"}}
{"id": "2506.04040", "pdf": "https://arxiv.org/pdf/2506.04040", "abs": "https://arxiv.org/abs/2506.04040", "authors": ["Chengdong Wu", "Sven Kirchner", "Nils Purschke", "Alois C. Knoll"], "title": "Autonomous Vehicle Lateral Control Using Deep Reinforcement Learning with MPC-PID Demonstration", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": "8 pages; Accepted for publication at the 36th IEEE Intelligent\n  Vehicles Symposium (IV), Cluj-Napoca, Romania, June 22-25, 2025", "summary": "The controller is one of the most important modules in the autonomous driving\npipeline, ensuring the vehicle reaches its desired position. In this work, a\nreinforcement learning based lateral control approach, despite the\nimperfections in the vehicle models due to measurement errors and\nsimplifications, is presented. Our approach ensures comfortable, efficient, and\nrobust control performance considering the interface between controlling and\nother modules. The controller consists of the conventional Model Predictive\nControl (MPC)-PID part as the basis and the demonstrator, and the Deep\nReinforcement Learning (DRL) part which leverages the online information from\nthe MPC-PID part. The controller's performance is evaluated in CARLA using the\nground truth of the waypoints as inputs. Experimental results demonstrate the\neffectiveness of the controller when vehicle information is incomplete, and the\ntraining of DRL can be stabilized with the demonstration part. These findings\nhighlight the potential to reduce development and integration efforts for\nautonomous driving pipelines in the future.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6a2a\u5411\u63a7\u5236\u65b9\u6cd5\uff0c\u7ed3\u5408MPC-PID\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\uff0c\u5728\u4e0d\u5b8c\u6574\u8f66\u8f86\u4fe1\u606f\u4e0b\u5b9e\u73b0\u8212\u9002\u3001\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u63a7\u5236\u6027\u80fd\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u81ea\u4e3b\u9a7e\u9a76\u4e2d\u7684\u63a7\u5236\u5668\u662f\u5173\u952e\u6a21\u5757\uff0c\u4f46\u8f66\u8f86\u6a21\u578b\u5e38\u56e0\u6d4b\u91cf\u8bef\u5dee\u548c\u7b80\u5316\u800c\u4e0d\u5b8c\u6574\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u786e\u4fdd\u63a7\u5236\u7684\u8212\u9002\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u6df7\u5408\u63a7\u5236\u65b9\u6cd5\uff1aMPC-PID\u4f5c\u4e3a\u57fa\u7840\u548c\u793a\u8303\u90e8\u5206\uff0cDRL\u5229\u7528MPC-PID\u7684\u5728\u7ebf\u4fe1\u606f\u8fdb\u884c\u5b66\u4e60\u3002\u63a7\u5236\u5668\u6027\u80fd\u5728CARLA\u4e2d\u901a\u8fc7\u771f\u5b9e\u8def\u5f84\u70b9\u8f93\u5165\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63a7\u5236\u5668\u5728\u4e0d\u5b8c\u6574\u8f66\u8f86\u4fe1\u606f\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4e14DRL\u8bad\u7ec3\u53ef\u901a\u8fc7\u793a\u8303\u90e8\u5206\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u51cf\u5c11\u81ea\u52a8\u9a7e\u9a76\u5f00\u53d1\u4e0e\u96c6\u6210\u5de5\u4f5c\u91cf\u7684\u6f5c\u529b\u3002", "keywords": "\u81ea\u52a8\u9a7e\u9a76, \u6a2a\u5411\u63a7\u5236, \u5f3a\u5316\u5b66\u4e60, MPC-PID, DRL"}}
{"id": "2506.04045", "pdf": "https://arxiv.org/pdf/2506.04045", "abs": "https://arxiv.org/abs/2506.04045", "authors": ["Vu Thi Huong", "Ida Litzel", "Thorsten Koch"], "title": "Similarity-based fuzzy clustering scientific articles: potentials and challenges from mathematical and computational perspectives", "categories": ["math.OC", "cs.LG", "90C26, 90C30, 90C90, 62H30, 68W10, 68T05, 68T09", "G.1.6"], "comment": null, "summary": "Fuzzy clustering, which allows an article to belong to multiple clusters with\nsoft membership degrees, plays a vital role in analyzing publication data. This\nproblem can be formulated as a constrained optimization model, where the goal\nis to minimize the discrepancy between the similarity observed from data and\nthe similarity derived from a predicted distribution. While this approach\nbenefits from leveraging state-of-the-art optimization algorithms, tailoring\nthem to work with real, massive databases like OpenAlex or Web of Science -\ncontaining about 70 million articles and a billion citations - poses\nsignificant challenges. We analyze potentials and challenges of the approach\nfrom both mathematical and computational perspectives. Among other things,\nsecond-order optimality conditions are established, providing new theoretical\ninsights, and practical solution methods are proposed by exploiting the\nstructure of the problem. Specifically, we accelerate the gradient projection\nmethod using GPU-based parallel computing to efficiently handle large-scale\ndata.", "AI": {"tldr": "\u57fa\u4e8e\u6a21\u7cca\u805a\u7c7b\u7684\u4f18\u5316\u6a21\u578b\u7528\u4e8e\u5206\u6790\u5927\u89c4\u6a21\u51fa\u7248\u6570\u636e\uff0c\u63d0\u51fa\u7406\u8bba\u4e0e\u8ba1\u7b97\u4e0a\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u5229\u7528\u6a21\u7cca\u805a\u7c7b\u5206\u6790\u5927\u89c4\u6a21\u51fa\u7248\u6570\u636e\uff08\u5982OpenAlex\u6216Web of Science\uff09\u7684\u6311\u6218\u4e0e\u6f5c\u529b\u3002", "method": "\u5efa\u7acb\u7ea6\u675f\u4f18\u5316\u6a21\u578b\uff0c\u5229\u7528\u68af\u5ea6\u6295\u5f71\u6cd5\u7ed3\u5408GPU\u5e76\u884c\u8ba1\u7b97\u52a0\u901f\u3002", "result": "\u63d0\u51fa\u4e86\u65b0\u7684\u4e8c\u9636\u6700\u4f18\u6027\u6761\u4ef6\u5e76\u6539\u8fdb\u4e86\u8ba1\u7b97\u65b9\u6cd5\u3002", "conclusion": "\u6a21\u7cca\u805a\u7c7b\u4f18\u5316\u6a21\u578b\u5728\u7406\u8bba\u4e0e\u8ba1\u7b97\u4e0a\u5747\u6709\u6f5c\u529b\uff0c\u9002\u5408\u5927\u89c4\u6a21\u6570\u636e\u5206\u6790\u3002", "keywords": "\u6a21\u7cca\u805a\u7c7b, \u4f18\u5316\u6a21\u578b, \u68af\u5ea6\u6295\u5f71\u6cd5, GPU\u8ba1\u7b97, \u5927\u89c4\u6a21\u6570\u636e"}}
{"id": "2506.04055", "pdf": "https://arxiv.org/pdf/2506.04055", "abs": "https://arxiv.org/abs/2506.04055", "authors": ["Paul Fuchs", "Weilong Chen", "Stephan Thaler", "Julija Zavadlav"], "title": "chemtrain-deploy: A parallel and scalable framework for machine learning potentials in million-atom MD simulations", "categories": ["physics.comp-ph", "cs.LG", "physics.chem-ph"], "comment": "Source code available at: https://github.com/tummfm/chemtrain", "summary": "Machine learning potentials (MLPs) have advanced rapidly and show great\npromise to transform molecular dynamics (MD) simulations. However, most\nexisting software tools are tied to specific MLP architectures, lack\nintegration with standard MD packages, or are not parallelizable across GPUs.\nTo address these challenges, we present chemtrain-deploy, a framework that\nenables model-agnostic deployment of MLPs in LAMMPS. chemtrain-deploy supports\nany JAX-defined semi-local potential, allowing users to exploit the\nfunctionality of LAMMPS and perform large-scale MLP-based MD simulations on\nmultiple GPUs. It achieves state-of-the-art efficiency and scales to systems\ncontaining millions of atoms. We validate its performance and scalability using\ngraph neural network architectures, including MACE, Allegro, and PaiNN, applied\nto a variety of systems, such as liquid-vapor interfaces, crystalline\nmaterials, and solvated peptides. Our results highlight the practical utility\nof chemtrain-deploy for real-world, high-performance simulations and provide\nguidance for MLP architecture selection and future design.", "AI": {"tldr": "chemtrain-deploy\u662f\u4e00\u4e2a\u4e0eLAMMPS\u96c6\u6210\u7684\u673a\u5668\u5b66\u4e60\u52bf\u80fd\u6846\u67b6\uff0c\u652f\u6301JAX\u5b9a\u4e49\u7684\u534a\u5c40\u90e8\u52bf\u80fd\uff0c\u53ef\u5728\u591aGPU\u4e0a\u9ad8\u6548\u8fd0\u884c\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u52bf\u80fd\u5de5\u5177\u5b58\u5728\u67b6\u6784\u4f9d\u8d56\u6027\u5f3a\u3001\u4e0e\u6807\u51c6\u5206\u5b50\u52a8\u529b\u5b66\u8f6f\u4ef6\u96c6\u6210\u4e0d\u8db3\u6216\u65e0\u6cd5\u5728GPU\u4e0a\u5e76\u884c\u5316\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86chemtrain-deploy\u6846\u67b6\uff0c\u652f\u6301\u4efb\u4f55JAX\u5b9a\u4e49\u7684\u534a\u5c40\u90e8\u52bf\u80fd\uff0c\u5e76\u4e0eLAMMPS\u96c6\u6210\uff0c\u5b9e\u73b0\u591aGPU\u5e76\u884c\u5316\u3002", "result": "\u8be5\u6846\u67b6\u5728\u5305\u542b\u6570\u767e\u4e07\u539f\u5b50\u7684\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "conclusion": "chemtrain-deploy\u5728\u5b9e\u9645\u9ad8\u6027\u80fd\u6a21\u62df\u4e2d\u5177\u6709\u5b9e\u7528\u6027\uff0c\u5e76\u4e3a\u673a\u5668\u5b66\u4e60\u52bf\u80fd\u67b6\u6784\u7684\u9009\u62e9\u548c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "keywords": "\u673a\u5668\u5b66\u4e60\u52bf\u80fd\uff0c\u5206\u5b50\u52a8\u529b\u5b66\uff0cLAMMPS\uff0cJAX\uff0cGPU\u5e76\u884c\u5316"}}
{"id": "2506.04063", "pdf": "https://arxiv.org/pdf/2506.04063", "abs": "https://arxiv.org/abs/2506.04063", "authors": ["Alex Sotiropoulos", "Sulyab Thottungal Valapu", "Linus Lei", "Jared Coleman", "Bhaskar Krishnamachari"], "title": "Crowd-SFT: Crowdsourcing for LLM Alignment", "categories": ["cs.HC", "cs.DC", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on Supervised Fine-Tuning\n(SFT) and Reinforcement Learning from Human Feedback (RLHF) to align model\nresponses with human preferences. While RLHF employs a reinforcement learning\napproach with a separate reward model, SFT uses human-curated datasets for\nsupervised learning. Both approaches traditionally depend on small, vetted\ngroups of annotators, making them costly, prone to bias, and limited in\nscalability. We propose an open, crowd-sourced fine-tuning framework that\naddresses these limitations by enabling broader feedback collection for SFT\nwithout extensive annotator training. Our framework promotes incentive fairness\nvia a point-based reward system correlated with Shapley values and guides model\nconvergence through iterative model updates. Our multi-model selection\nframework demonstrates up to a 55% reduction in target distance over\nsingle-model selection, enabling subsequent experiments that validate our\npoint-based reward mechanism's close alignment with Shapley values (a\nwell-established method for attributing individual contributions) thereby\nsupporting fair and scalable participation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f00\u653e\u7684\u4f17\u5305\u5fae\u8c03\u6846\u67b6\uff0c\u5229\u7528\u70b9\u503c\u5956\u52b1\u673a\u5236\u548c\u8fed\u4ee3\u6a21\u578b\u66f4\u65b0\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfSFT\u548cRLHF\u65b9\u6cd5\u7684\u6210\u672c\u9ad8\u3001\u504f\u89c1\u548c\u53ef\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfSFT\u548cRLHF\u4f9d\u8d56\u5c11\u6570\u6ce8\u91ca\u8005\uff0c\u6210\u672c\u9ad8\u4e14\u6613\u504f\u9887\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u516c\u5e73\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4f17\u5305\u6846\u67b6\uff0c\u901a\u8fc7\u70b9\u503c\u5956\u52b1\u673a\u5236\uff08\u4e0eShapley\u503c\u76f8\u5173\uff09\u548c\u8fed\u4ee3\u6a21\u578b\u66f4\u65b0\uff0c\u7ed3\u5408\u591a\u6a21\u578b\u9009\u62e9\u3002", "result": "\u591a\u6a21\u578b\u9009\u62e9\u6846\u67b6\u4f7f\u76ee\u6807\u8ddd\u79bb\u51cf\u5c1155%\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u70b9\u503c\u5956\u52b1\u673a\u5236\u4e0eShapley\u503c\u7684\u7d27\u5bc6\u5bf9\u9f50\u3002", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u516c\u5e73\u3001\u53ef\u6269\u5c55\u7684\u4f17\u5305\u53c2\u4e0e\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u76d1\u7763\u5fae\u8c03, RLHF, \u4f17\u5305, Shapley\u503c"}}
{"id": "2506.04170", "pdf": "https://arxiv.org/pdf/2506.04170", "abs": "https://arxiv.org/abs/2506.04170", "authors": ["Piotr Bia\u0142as", "Piotr Korcyl", "Tomasz Stebel", "Dawid Zapolski"], "title": "Estimation of the reduced density matrix and entanglement entropies using autoregressive networks", "categories": ["quant-ph", "cond-mat.stat-mech", "cs.LG", "hep-lat", "hep-th"], "comment": "9 pages, 7 figures", "summary": "We present an application of autoregressive neural networks to Monte Carlo\nsimulations of quantum spin chains using the correspondence with classical\ntwo-dimensional spin systems. We use a hierarchy of neural networks capable of\nestimating conditional probabilities of consecutive spins to evaluate elements\nof reduced density matrices directly. Using the Ising chain as an example, we\ncalculate the continuum limit of the ground state's von Neumann and R\\'enyi\nbipartite entanglement entropies of an interval built of up to 5 spins. We\ndemonstrate that our architecture is able to estimate all the needed matrix\nelements with just a single training for a fixed time discretization and\nlattice volume. Our method can be applied to other types of spin chains,\npossibly with defects, as well as to estimating entanglement entropies of\nthermal states at non-zero temperature.", "AI": {"tldr": "\u5229\u7528\u81ea\u56de\u5f52\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u7ecf\u5178\u4e8c\u7ef4\u81ea\u65cb\u7cfb\u7edf\u7684\u5bf9\u5e94\u5173\u7cfb\u6a21\u62df\u91cf\u5b50\u81ea\u65cb\u94fe\uff0c\u8ba1\u7b97\u5e76\u5c55\u793a\u4e86\u57fa\u6001\u548c\u70ed\u6001\u7684\u7ea0\u7f20\u71b5\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u9ad8\u6548\u8ba1\u7b97\u91cf\u5b50\u81ea\u65cb\u94fe\u4e2d\u7684\u7ea0\u7f20\u71b5\uff0c\u5c24\u5176\u662f\u57fa\u6001\u548c\u70ed\u6001\u7684\u77e9\u9635\u5143\u7d20\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u8fde\u7eed\u81ea\u65cb\u7684\u6761\u4ef6\u6982\u7387\uff0c\u76f4\u63a5\u8ba1\u7b97\u5bc6\u5ea6\u77e9\u9635\u7684\u7ea6\u7b80\u5143\u7d20\u3002", "result": "\u5728Ising\u94fe\u4e2d\u6210\u529f\u8ba1\u7b97\u4e86\u5305\u542b5\u4e2a\u81ea\u65cb\u7684\u533a\u95f4\u7684\u7ea0\u7f20\u71b5\uff0c\u5e76\u901a\u8fc7\u5355\u6b21\u8bad\u7ec3\u4f30\u8ba1\u6240\u6709\u6240\u9700\u77e9\u9635\u5143\u7d20\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5176\u4ed6\u7c7b\u578b\u7684\u81ea\u65cb\u94fe\u53ca\u975e\u96f6\u6e29\u5ea6\u4e0b\u7684\u70ed\u6001\u7ea0\u7f20\u71b5\u8ba1\u7b97\u3002", "keywords": "\u81ea\u56de\u5f52\u795e\u7ecf\u7f51\u7edc, \u91cf\u5b50\u81ea\u65cb\u94fe, \u7ea0\u7f20\u71b5, \u5bc6\u5ea6\u77e9\u9635, Ising\u6a21\u578b"}}
{"id": "2506.04193", "pdf": "https://arxiv.org/pdf/2506.04193", "abs": "https://arxiv.org/abs/2506.04193", "authors": ["Stephen R. Pfohl", "Natalie Harris", "Chirag Nagpal", "David Madras", "Vishwali Mhasawade", "Olawale Salaudeen", "Awa Dieng", "Shannon Sequeira", "Santiago Arciniegas", "Lillian Sung", "Nnamdi Ezeanochie", "Heather Cole-Lewis", "Katherine Heller", "Sanmi Koyejo", "Alexander D'Amour"], "title": "Understanding challenges to the interpretation of disaggregated evaluations of algorithmic fairness", "categories": ["stat.ML", "cs.CY", "cs.LG"], "comment": null, "summary": "Disaggregated evaluation across subgroups is critical for assessing the\nfairness of machine learning models, but its uncritical use can mislead\npractitioners. We show that equal performance across subgroups is an unreliable\nmeasure of fairness when data are representative of the relevant populations\nbut reflective of real-world disparities. Furthermore, when data are not\nrepresentative due to selection bias, both disaggregated evaluation and\nalternative approaches based on conditional independence testing may be invalid\nwithout explicit assumptions regarding the bias mechanism. We use causal\ngraphical models to predict metric stability across subgroups under different\ndata generating processes. Our framework suggests complementing disaggregated\nevaluations with explicit causal assumptions and analysis to control for\nconfounding and distribution shift, including conditional independence testing\nand weighted performance estimation. These findings have broad implications for\nhow practitioners design and interpret model assessments given the ubiquity of\ndisaggregated evaluation.", "AI": {"tldr": "\u9700\u8981\u7ed3\u5408\u56e0\u679c\u5047\u8bbe\u548c\u5206\u6790\u6765\u8865\u5145\u5b50\u7ec4\u8bc4\u4f30\uff0c\u4ee5\u63a7\u5236\u6df7\u6742\u548c\u5206\u5e03\u53d8\u5316\u3002", "motivation": "\u8bc4\u4f30\u673a\u5668\u6a21\u578b\u5728\u5b50\u7ec4\u4e2d\u7684\u8868\u73b0\u7684\u516c\u5e73\u6027\uff0c\u4f46\u5982\u679c\u4e0d\u52a0\u6279\u5224\u5730\u4f7f\u7528\u53ef\u80fd\u4f1a\u8bef\u5bfc\u5b9e\u8df5\u8005\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u56e0\u679c\u56fe\u6a21\u578b\u9884\u6d4b\u4e0d\u540c\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u4e2d\u5b50\u7ec4\u4e4b\u95f4\u7684\u5ea6\u91cf\u7a33\u5b9a\u6027\u3002", "result": "\u53d1\u73b0\u4ec5\u4f9d\u9760\u5b50\u7ec4\u6027\u80fd\u76f8\u7b49\u8861\u91cf\u516c\u5e73\u6027\u4e0d\u53ef\u9760\uff0c\u5efa\u8bae\u7ed3\u5408\u56e0\u679c\u5047\u8bbe\u548c\u5206\u6790\u3002", "conclusion": "\u5b50\u7ec4\u8bc4\u4f30\u9700\u8981\u7ed3\u5408\u660e\u786e\u7684\u56e0\u679c\u5047\u8bbe\uff0c\u4ee5\u63a7\u5236\u6df7\u6742\u548c\u5206\u5e03\u53d8\u5316\u3002", "keywords": "\u516c\u5e73\u6027\u8bc4\u4f30, \u5b50\u7ec4\u5206\u6790, \u56e0\u679c\u56fe\u6a21\u578b, \u6570\u636e\u504f\u5dee, \u6df7\u6742\u63a7\u5236"}}
{"id": "2506.04194", "pdf": "https://arxiv.org/pdf/2506.04194", "abs": "https://arxiv.org/abs/2506.04194", "authors": ["Yang Cai", "Alkis Kalavasis", "Katerina Mamali", "Anay Mehrotra", "Manolis Zampetakis"], "title": "What Makes Treatment Effects Identifiable? Characterizations and Estimators Beyond Unconfoundedness", "categories": ["math.ST", "cs.LG", "econ.EM", "stat.ME", "stat.ML", "stat.TH"], "comment": "Accepted for presentation at the 38th Conference on Learning Theory\n  (COLT) 2025", "summary": "Most of the widely used estimators of the average treatment effect (ATE) in\ncausal inference rely on the assumptions of unconfoundedness and overlap.\nUnconfoundedness requires that the observed covariates account for all\ncorrelations between the outcome and treatment. Overlap requires the existence\nof randomness in treatment decisions for all individuals. Nevertheless, many\ntypes of studies frequently violate unconfoundedness or overlap, for instance,\nobservational studies with deterministic treatment decisions -- popularly known\nas Regression Discontinuity designs -- violate overlap.\n  In this paper, we initiate the study of general conditions that enable the\nidentification of the average treatment effect, extending beyond\nunconfoundedness and overlap. In particular, following the paradigm of\nstatistical learning theory, we provide an interpretable condition that is\nsufficient and nearly necessary for the identification of ATE. Moreover, this\ncondition characterizes the identification of the average treatment effect on\nthe treated (ATT) and can be used to characterize other treatment effects as\nwell. To illustrate the utility of our condition, we present several\nwell-studied scenarios where our condition is satisfied and, hence, we prove\nthat ATE can be identified in regimes that prior works could not capture. For\nexample, under mild assumptions on the data distributions, this holds for the\nmodels proposed by Tan (2006) and Rosenbaum (2002), and the Regression\nDiscontinuity design model introduced by Thistlethwaite and Campbell (1960).\nFor each of these scenarios, we also show that, under natural additional\nassumptions, ATE can be estimated from finite samples.\n  We believe these findings open new avenues for bridging learning-theoretic\ninsights and causal inference methodologies, particularly in observational\nstudies with complex treatment mechanisms.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8d85\u8d8a\u65e0\u6df7\u6dc6\u6027\u548c\u91cd\u53e0\u6027\u7684\u4e00\u822c\u6761\u4ef6\uff0c\u4ee5\u8bc6\u522b\u5e73\u5747\u5904\u7406\u6548\u5e94\uff08ATE\uff09\u3002\u901a\u8fc7\u7edf\u8ba1\u5b66\u4e60\u7406\u8bba\u7684\u8303\u5f0f\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u89e3\u91ca\u6027\u5f3a\u4e14\u51e0\u4e4e\u5fc5\u8981\u7684\u6761\u4ef6\uff0c\u7528\u4e8e\u8bc6\u522bATE\uff0c\u5e76\u6269\u5c55\u81f3\u5176\u4ed6\u5904\u7406\u6548\u5e94\u3002", "motivation": "\u4f20\u7edfATE\u4f30\u8ba1\u57fa\u4e8e\u65e0\u6df7\u6dc6\u6027\u548c\u91cd\u53e0\u6027\u5047\u8bbe\uff0c\u4f46\u8bb8\u591a\u7814\u7a76\uff08\u5982\u56de\u5f52\u65ad\u70b9\u8bbe\u8ba1\uff09\u8fdd\u53cd\u8fd9\u4e9b\u5047\u8bbe\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u66f4\u4e00\u822c\u7684\u8bc6\u522b\u6761\u4ef6\u3002", "method": "\u901a\u8fc7\u7edf\u8ba1\u5b66\u4e60\u7406\u8bba\u8303\u5f0f\uff0c\u63d0\u51fa\u4e00\u4e2a\u89e3\u91ca\u6027\u5f3a\u4e14\u51e0\u4e4e\u5fc5\u8981\u7684\u6761\u4ef6\uff0c\u8bc1\u660e\u5176\u5728\u591a\u4e2a\u7ecf\u5178\u6a21\u578b\uff08\u5982Tan, Rosenbaum, Thistlethwaite\u548cCampbell\u7684\u6a21\u578b\uff09\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u5728\u81ea\u7136\u9644\u52a0\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u4e86ATE\u5728\u6709\u9650\u6837\u672c\u4e2d\u53ef\u4f30\u8ba1\uff0c\u5e76\u9002\u7528\u4e8e\u590d\u6742\u5904\u7406\u673a\u5236\u7684\u89c2\u5bdf\u6027\u7814\u7a76\u3002", "conclusion": "\u672c\u6587\u4e3a\u5b66\u4e60\u7406\u8bba\u4e0e\u56e0\u679c\u63a8\u65ad\u7684\u7ed3\u5408\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u5904\u7406\u673a\u5236\u7684\u89c2\u5bdf\u6027\u7814\u7a76\u4e2d\u3002", "keywords": "\u5e73\u5747\u5904\u7406\u6548\u5e94, \u65e0\u6df7\u6dc6\u6027, \u91cd\u53e0\u6027, \u7edf\u8ba1\u5b66\u4e60\u7406\u8bba, \u89c2\u5bdf\u6027\u7814\u7a76"}}
{"id": "2506.04204", "pdf": "https://arxiv.org/pdf/2506.04204", "abs": "https://arxiv.org/abs/2506.04204", "authors": ["Martin Beseda", "Vittorio Cortellessa", "Daniele Di Pompeo", "Luca Traini", "Michele Tucci"], "title": "A Kernel-Based Approach for Accurate Steady-State Detection in Performance Time Series", "categories": ["cs.PF", "cs.LG"], "comment": "This manuscript is under review by Future Generation Computer Systems", "summary": "This paper addresses the challenge of accurately detecting the transition\nfrom the warmup phase to the steady state in performance metric time series,\nwhich is a critical step for effective benchmarking. The goal is to introduce a\nmethod that avoids premature or delayed detection, which can lead to inaccurate\nor inefficient performance analysis. The proposed approach adapts techniques\nfrom the chemical reactors domain, detecting steady states online through the\ncombination of kernel-based step detection and statistical methods. By using a\nwindow-based approach, it provides detailed information and improves the\naccuracy of identifying phase transitions, even in noisy or irregular time\nseries. Results show that the new approach reduces total error by 14.5%\ncompared to the state-of-the-art method. It offers more reliable detection of\nthe steady-state onset, delivering greater precision for benchmarking tasks.\nFor users, the new approach enhances the accuracy and stability of performance\nbenchmarking, efficiently handling diverse time series data. Its robustness and\nadaptability make it a valuable tool for real-world performance evaluation,\nensuring consistent and reproducible results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6838\u65b9\u6cd5\u548c\u7edf\u8ba1\u6280\u672f\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u51c6\u786e\u68c0\u6d4b\u6027\u80fd\u6307\u6807\u65f6\u95f4\u5e8f\u5217\u4e2d\u4ece\u9884\u70ed\u9636\u6bb5\u5230\u7a33\u6001\u7684\u8fc7\u6e21\uff0c\u663e\u8457\u51cf\u5c11\u9519\u8bef\u5e76\u63d0\u9ad8\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u6d4b\u7a33\u6001\u5f00\u59cb\u65f6\u53ef\u80fd\u8fc7\u65e9\u6216\u5ef6\u8fdf\u7684\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u7ed3\u5408\u6838\u51fd\u6570\u6b65\u8fdb\u68c0\u6d4b\u548c\u7edf\u8ba1\u65b9\u6cd5\uff0c\u91c7\u7528\u7a97\u53e3\u5316\u6280\u672f\u5728\u7ebf\u68c0\u6d4b\u7a33\u6001\uff0c\u9002\u7528\u4e8e\u566a\u58f0\u6216\u975e\u89c4\u5219\u65f6\u95f4\u5e8f\u5217\u3002", "result": "\u65b0\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u51cf\u5c11\u4e8614.5%\u7684\u603b\u8bef\u5dee\uff0c\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u7a33\u6001\u68c0\u6d4b\u548c\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u7684\u51c6\u786e\u6027\u3001\u7a33\u5b9a\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "keywords": "\u7a33\u6001\u68c0\u6d4b,\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5,\u6838\u65b9\u6cd5,\u7edf\u8ba1\u65b9\u6cd5,\u65f6\u95f4\u5e8f\u5217\u5206\u6790"}}
{"id": "2506.04214", "pdf": "https://arxiv.org/pdf/2506.04214", "abs": "https://arxiv.org/abs/2506.04214", "authors": ["Tingle Li", "Baihe Huang", "Xiaobin Zhuang", "Dongya Jia", "Jiawei Chen", "Yuping Wang", "Zhuo Chen", "Gopala Anumanchipalli", "Yuxuan Wang"], "title": "Sounding that Object: Interactive Object-Aware Image to Audio Generation", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "ICML 2025", "summary": "Generating accurate sounds for complex audio-visual scenes is challenging,\nespecially in the presence of multiple objects and sound sources. In this\npaper, we propose an {\\em interactive object-aware audio generation} model that\ngrounds sound generation in user-selected visual objects within images. Our\nmethod integrates object-centric learning into a conditional latent diffusion\nmodel, which learns to associate image regions with their corresponding sounds\nthrough multi-modal attention. At test time, our model employs image\nsegmentation to allow users to interactively generate sounds at the {\\em\nobject} level. We theoretically validate that our attention mechanism\nfunctionally approximates test-time segmentation masks, ensuring the generated\naudio aligns with selected objects. Quantitative and qualitative evaluations\nshow that our model outperforms baselines, achieving better alignment between\nobjects and their associated sounds. Project page:\nhttps://tinglok.netlify.app/files/avobject/", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u4e92\u5f0f\u5bf9\u8c61\u611f\u77e5\u97f3\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u4e0e\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u7528\u6237\u9009\u62e9\u89c6\u89c9\u5bf9\u8c61\u7684\u97f3\u9891\u751f\u6210\u3002", "motivation": "\u5728\u590d\u6742\u7684\u89c6\u542c\u573a\u666f\u4e2d\uff0c\u5c24\u5176\u662f\u5b58\u5728\u591a\u4e2a\u5bf9\u8c61\u548c\u58f0\u6e90\u65f6\uff0c\u5982\u4f55\u751f\u6210\u51c6\u786e\u7684\u58f0\u97f3\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5bf9\u8c61\u611f\u77e5\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6a21\u578b\u7ed3\u5408\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u4e0e\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u5c06\u56fe\u50cf\u533a\u57df\u4e0e\u58f0\u97f3\u5173\u8054\u3002\u6d4b\u8bd5\u65f6\uff0c\u5229\u7528\u56fe\u50cf\u5206\u5272\u6280\u672f\u5b9e\u73b0\u5bf9\u8c61\u7ea7\u58f0\u97f3\u751f\u6210\u3002", "result": "\u7406\u8bba\u9a8c\u8bc1\u8868\u660e\u6ce8\u610f\u529b\u673a\u5236\u8fd1\u4f3c\u4e8e\u6d4b\u8bd5\u65f6\u5206\u5272\u63a9\u7801\uff0c\u786e\u4fdd\u751f\u6210\u97f3\u9891\u4e0e\u9009\u5b9a\u5bf9\u8c61\u5bf9\u9f50\u3002\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u663e\u793a\u6a21\u578b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6a21\u578b\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u8c61\u611f\u77e5\u7684\u58f0\u97f3\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8c61\u4e0e\u97f3\u9891\u7684\u5bf9\u9f50\u6548\u679c\u3002", "keywords": "\u97f3\u9891\u751f\u6210,\u5bf9\u8c61\u611f\u77e5,\u4ea4\u4e92\u5f0f,\u591a\u6a21\u6001,\u6f5c\u5728\u6269\u6563\u6a21\u578b"}}
