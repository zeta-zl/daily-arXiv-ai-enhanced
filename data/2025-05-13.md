<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 64]
- [cs.LG](#cs.LG) [Total: 145]
- [cs.AI](#cs.AI) [Total: 48]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.RO](#cs.RO) [Total: 22]
- [cs.CV](#cs.CV) [Total: 39]
- [stat.AP](#stat.AP) [Total: 3]
- [cs.DL](#cs.DL) [Total: 1]
- [math.DS](#math.DS) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [math.NA](#math.NA) [Total: 3]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.HC](#cs.HC) [Total: 8]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 8]
- [cs.MA](#cs.MA) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.AR](#cs.AR) [Total: 1]
- [eess.SP](#eess.SP) [Total: 11]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 8]
- [stat.ML](#stat.ML) [Total: 16]
- [cs.ET](#cs.ET) [Total: 1]
- [quant-ph](#quant-ph) [Total: 6]
- [cs.SE](#cs.SE) [Total: 6]
- [eess.SY](#eess.SY) [Total: 4]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [hep-lat](#hep-lat) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.GT](#cs.GT) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents](https://arxiv.org/abs/2505.06416)
*Elias Lumer,Anmol Gulati,Vamse Kumar Subbiah,Pradeep Honaganahalli Basavaraju,James A. Burke*

Main category: cs.CL

TL;DR: ScaleMCP是一种新工具选择方法，通过动态集成MCP服务器和自动化工具存储系统解决现有工具选择框架的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前工具选择框架依赖人工更新且未集成MCP服务器，导致效率低下、不一致性和代理自主性受限。

Method: 引入ScaleMCP，包括MCP工具检索器、自动化存储系统和TDWA嵌入策略，实现动态工具选择和同步。

Result: 在5000个金融MCP服务器的测试中，ScaleMCP显著提升了工具检索和代理调用性能。

Conclusion: ScaleMCP能有效支持可扩展、动态的工具选择与调用，解决了现有方法的缺陷。

Abstract: Recent advancements in Large Language Models (LLMs) and the introduction of
the Model Context Protocol (MCP) have significantly expanded LLM agents'
capability to interact dynamically with external tools and APIs. However,
existing tool selection frameworks do not integrate MCP servers, instead
relying heavily on error-prone manual updates to monolithic local tool
repositories, leading to duplication, inconsistencies, and inefficiencies.
Additionally, current approaches abstract tool selection before the LLM agent
is invoked, limiting its autonomy and hindering dynamic re-querying
capabilities during multi-turn interactions. To address these issues, we
introduce ScaleMCP, a novel tool selection approach that dynamically equips LLM
agents with a MCP tool retriever, giving agents the autonomy to add tools into
their memory, as well as an auto-synchronizing tool storage system pipeline
through CRUD (create, read, update, delete) operations with MCP servers as the
single source of truth. We also propose a novel embedding strategy, Tool
Document Weighted Average (TDWA), designed to selectively emphasize critical
components of tool documents (e.g. tool name or synthetic questions) during the
embedding process. Comprehensive evaluations conducted on a created dataset of
5,000 financial metric MCP servers, across 10 LLM models, 5 embedding models,
and 5 retriever types, demonstrate substantial improvements in tool retrieval
and agent invocation performance, emphasizing ScaleMCP's effectiveness in
scalable, dynamic tool selection and invocation.

</details>


### [2] [Is your multimodal large language model a good science tutor?](https://arxiv.org/abs/2505.06418)
*Ming Liu,Liwen Wang,Wensheng Zhang*

Main category: cs.CL

TL;DR: 该研究提出一个评估多模态大语言模型作为科学导师的框架,结合教育标准与学生模拟,优化模型以提升教学能力,不仅仅是解决问题能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注最终答案准确性,而忽视了教学能力。在教育场景中,不仅需要模型的正确性,更需要其能够有效教学。

Method: 利用教育标准与学生模拟模型评估模型教学表现,构建强弱导师对比数据集,并运用偏好优化方法微调模型。

Result: 研究表明强解题能力并不等同于高质量教学,优化后的模型(Qwen2-VL-2B)更加符合教育需求。

Conclusion: 该方法展示了构建具备教学能力的多模态大语言模型的新途径,使其不仅能解题,还能成为高效的教育助手。

Abstract: Multimodal large language models (MLLMs) demonstrate impressive performance
on scientific reasoning tasks (e.g., ScienceQA). However, most existing
benchmarks focus narrowly on the accuracy of the final answer while ignoring
other metrics. In particular, when applying MLLMs to educational contexts, the
goal is not only correctness but also the ability to teach. In this paper, we
propose a framework that evaluates MLLMs as science tutors using a
comprehensive educational rubric and a simulated student model that judges the
teaching performance of the tutors. Given a list of candidate MLLM science
tutors, we use rubric-based student judgments to produce a range of tutor
performance scores, identifying both strong and weak tutors. Using the training
section of the ScienceQA dataset, we then construct a data set of pairwise
comparisons between the outputs of strong and weak tutors. This enables us to
apply multiple preference optimization methods to fine-tune an underperforming
tutor model (Qwen2-VL-2B) into more effective ones. Our results also show that
strong problem-solving skills do not guarantee high-quality tutoring and that
performance optimization-guided refinements can yield more educationally
aligned tutor models. This approach opens avenues for building MLLMs that serve
not only as problem solvers, but as genuinely helpful educational assistants.

</details>


### [3] [xGen-small Technical Report](https://arxiv.org/abs/2505.06496)
*Erik Nijkamp,Bo Pang,Egor Pakhomov,Akash Gokul,Jin Qu,Silvio Savarese,Yingbo Zhou,Caiming Xiong*

Main category: cs.CL

TL;DR: xGen-small是一个针对长上下文优化的4B和9B Transformer解码器模型家族，通过数据管理、多阶段预训练、监督微调等方法，在数学和编码领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 为长上下文应用开发高效的Transformer模型，以提升在数学和编码等领域的性能。

Method: 垂直整合的数据管理、多阶段预训练（包括质量退火和128k tokens长度扩展）、监督微调、偏好学习和在线强化学习。

Result: 在多种任务中表现强劲，尤其在数学和编码领域以及长上下文基准测试中表现出色。

Conclusion: xGen-small通过优化数据处理和训练方法，显著提升了模型在长上下文任务中的性能。

Abstract: We introduce xGen-small, a family of 4B and 9B Transformer decoder models
optimized for long-context applications. Our vertically integrated pipeline
unites domain-balanced, frequency-aware data curation; multi-stage pre-training
with quality annealing and length extension to 128k tokens; and targeted
post-training via supervised fine-tuning, preference learning, and online
reinforcement learning. xGen-small delivers strong performance across various
tasks, especially in math and coding domains, while excelling at long context
benchmarks.

</details>


### [4] [Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model](https://arxiv.org/abs/2505.06538)
*Xinyue Lou,You Li,Jinan Xu,Xiangyu Shi,Chi Chen,Kaiyu Huang*

Main category: cs.CL

TL;DR: 该论文通过系统性评估11种多模态大型推理模型（MLRMs），揭示了其安全性能下降的普遍现象，并提出了一种通过模型内在推理能力检测不安全意图的方法，实验证明数据集微调能有效提升安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大型推理模型（MLRMs）发展迅速，但其安全性和可靠性仍缺乏系统研究，亟需填补这一空白。

Method: 对11种MLRMs在5个基准上进行全面安全评估，构建了一个包含安全导向思维的多模态微调数据集，并利用该数据集对模型进行微调。

Result: 发现先进模型存在显著安全性能下降现象，尤其在越狱鲁棒性基准上表现明显；通过微调数据集，模型在安全性能上得到有效提升。

Conclusion: 利用模型内在推理能力检测不安全意图是解决MLRMs安全问题的可行途径，研究为开发安全MLRMs提供了新视角。数据集已开源。

Abstract: The rapid development of multimodal large reasoning models (MLRMs) has
demonstrated broad application potential, yet their safety and reliability
remain critical concerns that require systematic exploration. To address this
gap, we conduct a comprehensive and systematic safety evaluation of 11 MLRMs
across 5 benchmarks and unveil prevalent safety degradation phenomena in most
advanced models. Moreover, our analysis reveals distinct safety patterns across
different benchmarks: significant safety degradation is observed across
jailbreak robustness benchmarks, whereas safety-awareness benchmarks
demonstrate less pronounced degradation. In particular, a long thought process
in some scenarios even enhances safety performance. Therefore, it is a
potential approach to addressing safety issues in MLRMs by leveraging the
intrinsic reasoning capabilities of the model to detect unsafe intent. To
operationalize this insight, we construct a multimodal tuning dataset that
incorporates a safety-oriented thought process. Experimental results from
fine-tuning existing MLRMs with this dataset effectively enhances the safety on
both jailbreak robustness and safety-awareness benchmarks. This study provides
a new perspective for developing safe MLRMs. Our dataset is available at
https://github.com/xinyuelou/Think-in-Safety.

</details>


### [5] [REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback](https://arxiv.org/abs/2505.06548)
*Aniruddha Roy,Pretam Ray,Abhilash Nandy,Somak Aditya,Pawan Goyal*

Main category: cs.CL

TL;DR: 论文研究了三种开源小型LLM（LLaMA 2-7B、LLama 2-13B和Mistral 7B）在生成指令数据集上的表现，通过半自动化框架降低人工干预和成本，并结合强化学习算法进一步提升效果。实验显示，这种方法在63-66%的任务中优于之前的方法。


<details>
  <summary>Details</summary>
Motivation: 由于人工标注指令数据耗时、昂贵且多样性有限，论文旨在探索开源小型LLM生成指令的可行性，以降低成本并提升任务多样性。

Method: 使用三种开源小型LLM（LLaMA 2-7B、LLama 2-13B和Mistral 7B），结合半自动化框架生成指令数据集，并引入强化学习算法优化模型。

Result: 实验表明，结合强化学习的框架在63-66%的任务中表现优于先前方法，验证了小型LLM和自动化框架的有效性。

Conclusion: 开源小型LLM结合半自动化框架及强化学习能高效生成指令数据，降低成本且性能优于传统方法，为LLM微调提供了新思路。

Abstract: Instruction-based Large Language Models (LLMs) have proven effective in
numerous few-shot or zero-shot Natural Language Processing (NLP) tasks.
However, creating human-annotated instruction data is time-consuming,
expensive, and often limited in quantity and task diversity. Previous research
endeavors have attempted to address this challenge by proposing frameworks
capable of generating instructions in a semi-automated and task-agnostic manner
directly from the model itself. Many of these efforts have relied on large
API-only parameter-based models such as GPT-3.5 (175B), which are expensive,
and subject to limits on a number of queries. This paper explores the
performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B,
and Mistral 7B, using a semi-automated framework, thereby reducing human
intervention, effort, and cost required to generate an instruction dataset for
fine-tuning LLMs. Furthermore, we demonstrate that incorporating a
Reinforcement Learning (RL) based training algorithm into this LLMs-based
framework leads to further enhancements. Our evaluation of the dataset reveals
that these RL-based frameworks achieve a substantial improvements in 63-66% of
the tasks compared to previous approaches.

</details>


### [6] [References Indeed Matter? Reference-Free Preference Optimization for Conversational Query Reformulation](https://arxiv.org/abs/2505.06552)
*Doyoung Kim,Youngjun Lee,Joeun Kim,Jihwan Bang,Hwanjun Song,Susik Yoon,Jae-Gil Lee*

Main category: cs.CL

TL;DR: 提出了一种无需参考段落的对话查询重构优化框架DualReform，通过响应推理和双角色优化实现高效检索，性能接近甚至超越依赖参考段落的现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有CQR方法依赖参考段落优化，但实际场景中难以获取；需开发无需参考的高效框架。

Method: DualReform通过响应推理生成伪参考段落，并利用CQR双角色（响应精调与查询重构共享目标）优化模型。

Result: 在无参考段落的情况下，达到96.9%~99.1%的检索准确率，超越现有最优方法达31.6%。

Conclusion: DualReform证明了无需参考段落的可行性与高效性，为实际应用提供了新思路。

Abstract: Conversational query reformulation (CQR) has become indispensable for
improving retrieval in dialogue-based applications. However, existing
approaches typically rely on reference passages for optimization, which are
impractical to acquire in real-world scenarios. To address this limitation, we
introduce a novel reference-free preference optimization framework DualReform
that generates pseudo reference passages from commonly-encountered
conversational datasets containing only queries and responses. DualReform
attains this goal through two key innovations: (1) response-based inference,
where responses serve as proxies to infer pseudo reference passages, and (2)
response refinement via the dual-role of CQR, where a CQR model refines
responses based on the shared objectives between response refinement and CQR.
Despite not relying on reference passages, DualReform achieves 96.9--99.1% of
the retrieval accuracy attainable only with reference passages and surpasses
the state-of-the-art method by up to 31.6%.

</details>


### [7] [MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG](https://arxiv.org/abs/2505.06569)
*Woosang Lim,Zekun Li,Gyuwan Kim,Sungyoung Ji,HyeonJung Kim,Kyuri Choi,Jin Hyuk Lim,Kyungpyo Park,William Yang Wang*

Main category: cs.CL

TL;DR: MacRAG是一个分层检索框架，通过多尺度自适应上下文构建优化RAG系统，显著提升复杂多跳和长文档任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统存在检索不精准、上下文覆盖不完整以及信息碎片化的问题，需要一种更高效的解决方案。

Method: MacRAG采用分层检索，将文档压缩并分区为粗到细的粒度，通过实时块级和文档级扩展自适应合并相关上下文。

Result: 在LongBench扩展数据集上，MacRAG显著优于基线RAG系统，支持多步生成和长上下文推理。

Conclusion: MacRAG为现实世界的长上下文、多跳推理任务提供了高效、可扩展的解决方案。

Abstract: Long-context (LC) Large Language Models (LLMs) combined with
Retrieval-Augmented Generation (RAG) hold strong potential for complex
multi-hop and large-document tasks. However, existing RAG systems often suffer
from imprecise retrieval, incomplete context coverage under constrained context
windows, and fragmented information caused by suboptimal context construction.
We introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical
retrieval framework that compresses and partitions documents into
coarse-to-fine granularities, then adaptively merges relevant contexts through
chunk- and document-level expansions in real time. By starting from the
finest-level retrieval and progressively incorporating higher-level and broader
context, MacRAG constructs effective query-specific long contexts, optimizing
both precision and coverage. Evaluations on the challenging LongBench
expansions of HotpotQA, 2WikiMultihopQA, and Musique confirm that MacRAG
consistently surpasses baseline RAG pipelines on single- and multi-step
generation with Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish
MacRAG as an efficient, scalable solution for real-world long-context,
multi-hop reasoning. Our code is available at
https://github.com/Leezekun/MacRAG.

</details>


### [8] [Evaluating LLM-Generated Q&A Test: a Student-Centered Study](https://arxiv.org/abs/2505.06591)
*Anna Wróblewska,Bartosz Grabek,Jakub Świstak,Daniel Dan*

Main category: cs.CL

TL;DR: 论文提出了一种基于AI聊天机器人的自动化生成可靠问答测试的流程，并验证了其心理测量学性能与用户满意度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索利用AI（如GPT-4o-mini）自动生成高质量问答测试的可行性，以解决人工编写测试的 scalability 问题。

Method: 使用GPT-4o-mini自动生成了自然语言处理课程的问答测试，并通过学生和专家的心理测量学指标（如IRT分析）及感知质量评分进行验证。

Result: 生成的测试题展现了强区分度和合适难度，学生和专家评分显示高质量；仅两题需审查（DIF分析）。

Conclusion: LLM生成的测试在心理测量学性能和用户满意度上可媲美人工测试，为AI辅助评估开发提供了可扩展方案。

Abstract: This research prepares an automatic pipeline for generating reliable
question-answer (Q&A) tests using AI chatbots. We automatically generated a
GPT-4o-mini-based Q&A test for a Natural Language Processing course and
evaluated its psychometric and perceived-quality metrics with students and
experts. A mixed-format IRT analysis showed that the generated items exhibit
strong discrimination and appropriate difficulty, while student and expert star
ratings reflect high overall quality. A uniform DIF check identified two items
for review. These findings demonstrate that LLM-generated assessments can match
human-authored tests in psychometric performance and user satisfaction,
illustrating a scalable approach to AI-assisted assessment development.

</details>


### [9] [Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation](https://arxiv.org/abs/2505.06594)
*Galann Pennec,Zhengyuan Liu,Nicholas Asher,Philippe Muller,Nancy F. Chen*

Main category: cs.CL

TL;DR: 该论文提出了一种零样本视频到文本摘要方法，通过生成剧本表示整合视频、对话和角色信息，并引入MFactSum多模态评估指标。实验表明，该方法优于当前最佳VLM模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在处理复杂多模态输入（如整集电视剧）时难以平衡视觉与文本信息，因此需要一种更有效的摘要方法。

Method: 采用零样本方法生成剧本表示，整合关键视频片段、对话和角色信息，仅需音频、视频和文本作为输入。同时提出多模态评估指标MFactSum。

Result: 在SummScreen3D数据集上，该方法比Gemini 1.5多包含20%的相关视觉信息，且仅需后者75%的视频输入。

Conclusion: 提出的零样本剧本摘要方法在多模态摘要任务中表现优越，MFactSum指标为多模态内容评估提供了新方向。

Abstract: Vision-Language Models (VLMs) often struggle to balance visual and textual
information when summarizing complex multimodal inputs, such as entire TV show
episodes. In this paper, we propose a zero-shot video-to-text summarization
approach that builds its own screenplay representation of an episode,
effectively integrating key video moments, dialogue, and character information
into a unified document. Unlike previous approaches, we simultaneously generate
screenplays and name the characters in zero-shot, using only the audio, video,
and transcripts as input. Additionally, we highlight that existing
summarization metrics can fail to assess the multimodal content in summaries.
To address this, we introduce MFactSum, a multimodal metric that evaluates
summaries with respect to both vision and text modalities. Using MFactSum, we
evaluate our screenplay summaries on the SummScreen3D dataset, demonstrating
superiority against state-of-the-art VLMs such as Gemini 1.5 by generating
summaries containing 20% more relevant visual information while requiring 75%
less of the video as input.

</details>


### [10] [Bridging the Gap: An Intermediate Language for Enhanced and Cost-Effective Grapheme-to-Phoneme Conversion with Homographs with Multiple Pronunciations Disambiguation](https://arxiv.org/abs/2505.06599)
*Abbas Bertina,Shahab Beirami,Hossein Biniazian,Elham Esmaeilnia,Soheil Shahi,Mahdi Pirnia*

Main category: cs.CL

TL;DR: 该论文提出了一种针对波斯语的多层面G2P转换方法，结合LLM提示技术和序列到序列机器音译架构，显著提升了音素错误率（PER），为低资源语言处理提供了新方案。


<details>
  <summary>Details</summary>
Motivation: 波斯语G2P转换因同形异义词和Ezafe等复杂语音特征面临挑战，尤其是在正式与非正式语境中。研究旨在开发一种中间语言和系统性方法，以解决这些问题。

Method: 方法结合了大型语言模型（LLM）提示技术和专门的序列到序列音译架构，并利用形式概念分析构建同形异义词的语义区分数据库。训练数据包括LLM生成的数据集和B-Plus播客的非正式语言变体。

Result: 实验结果表明，该方法在波斯语音素转换复杂性处理上优于现有技术，显著降低了音素错误率（PER），为波斯语G2P转换设立了新基准。

Conclusion: 该研究不仅为波斯语文本到语音系统提供了鲁棒解决方案，还可推广至中文和阿拉伯语等同形异义现象丰富的语言，推动了低资源语言处理的研究。

Abstract: Grapheme-to-phoneme (G2P) conversion for Persian presents unique challenges
due to its complex phonological features, particularly homographs and Ezafe,
which exist in formal and informal language contexts. This paper introduces an
intermediate language specifically designed for Persian language processing
that addresses these challenges through a multi-faceted approach. Our
methodology combines two key components: Large Language Model (LLM) prompting
techniques and a specialized sequence-to-sequence machine transliteration
architecture. We developed and implemented a systematic approach for
constructing a comprehensive lexical database for homographs with multiple
pronunciations disambiguation often termed polyphones, utilizing formal concept
analysis for semantic differentiation. We train our model using two distinct
datasets: the LLM-generated dataset for formal and informal Persian and the
B-Plus podcasts for informal language variants. The experimental results
demonstrate superior performance compared to existing state-of-the-art
approaches, particularly in handling the complexities of Persian phoneme
conversion. Our model significantly improves Phoneme Error Rate (PER) metrics,
establishing a new benchmark for Persian G2P conversion accuracy. This work
contributes to the growing research in low-resource language processing and
provides a robust solution for Persian text-to-speech systems and demonstrating
its applicability beyond Persian. Specifically, the approach can extend to
languages with rich homographic phenomena such as Chinese and Arabic

</details>


### [11] [Using External knowledge to Enhanced PLM for Semantic Matching](https://arxiv.org/abs/2505.06605)
*Min Li,Chun Yuan*

Main category: cs.CL

TL;DR: 通过引入外部知识增强预训练的语义相关性判别模型，实验证明其在多个公开数据集上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 探讨机器是否能仅依赖大规模标注数据学习语义相关性检测任务所需的全部知识，并提出如何将外部知识融入神经网络模型以提高性能。

Method: 利用外部知识增强预训练的语义相关性判别模型。

Result: 在10个公开数据集上实验显示，该方法相比基线模型性能有显著提升。

Conclusion: 通过引入外部知识可以有效增强模型的语义相关性检测能力，为相关任务提供了新的改进方向。

Abstract: Modeling semantic relevance has always been a challenging and critical task
in natural language processing. In recent years, with the emergence of massive
amounts of annotated data, it has become feasible to train complex models, such
as neural network-based reasoning models. These models have shown excellent
performance in practical applications and have achieved the current
state-ofthe-art performance. However, even with such large-scale annotated
data, we still need to think: Can machines learn all the knowledge necessary to
perform semantic relevance detection tasks based on this data alone? If not,
how can neural network-based models incorporate external knowledge into
themselves, and how can relevance detection models be constructed to make full
use of external knowledge? In this paper, we use external knowledge to enhance
the pre-trained semantic relevance discrimination model. Experimental results
on 10 public datasets show that our method achieves consistent improvements in
performance compared to the baseline model.

</details>


### [12] [Boosting Neural Language Inference via Cascaded Interactive Reasoning](https://arxiv.org/abs/2505.06607)
*Min Li,Chun Yuan*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Cascaded Interactive Reasoning Network (CIRN)的新架构，用于提升自然语言推理（NLI）任务中的深度语义理解能力。通过分层特征提取和跨句子信息交互，CIRN在多个标准NLI数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的NLI方法主要依赖预训练语言模型（PLMs）的最后一层表示，这可能会忽略中间层的宝贵信息，限制了复杂语义关系的建模能力。为了解决这一问题，作者提出了CIRN以更全面地挖掘语义信息。

Method: CIRN采用分层特征提取策略，在多个网络深度中连续整合跨句子信息，模拟从表层特征匹配到深层逻辑和语义连接的渐进推理过程。

Result: 在多个标准NLI数据集上的实验表明，CIRN通过利用多层次交互特征，在性能上优于竞争性基线方法。

Conclusion: CIRN通过分层和交互式特征提取，有效提升了NLI任务中对复杂语义关系的建模能力，验证了多层级信息整合的重要性。

Abstract: Natural Language Inference (NLI) focuses on ascertaining the logical
relationship (entailment, contradiction, or neutral) between a given premise
and hypothesis. This task presents significant challenges due to inherent
linguistic features such as diverse phrasing, semantic complexity, and
contextual nuances. While Pre-trained Language Models (PLMs) built upon the
Transformer architecture have yielded substantial advancements in NLI,
prevailing methods predominantly utilize representations from the terminal
layer. This reliance on final-layer outputs may overlook valuable information
encoded in intermediate layers, potentially limiting the capacity to model
intricate semantic interactions effectively. Addressing this gap, we introduce
the Cascaded Interactive Reasoning Network (CIRN), a novel architecture
designed for deeper semantic comprehension in NLI. CIRN implements a
hierarchical feature extraction strategy across multiple network depths,
operating within an interactive space where cross-sentence information is
continuously integrated. This mechanism aims to mimic a process of progressive
reasoning, transitioning from surface-level feature matching to uncovering more
profound logical and semantic connections between the premise and hypothesis.
By systematically mining latent semantic relationships at various
representational levels, CIRN facilitates a more thorough understanding of the
input pair. Comprehensive evaluations conducted on several standard NLI
benchmark datasets reveal consistent performance gains achieved by CIRN over
competitive baseline approaches, demonstrating the efficacy of leveraging
multi-level interactive features for complex relational reasoning.

</details>


### [13] [The Efficiency of Pre-training with Objective Masking in Pseudo Labeling for Semi-Supervised Text Classification](https://arxiv.org/abs/2505.06624)
*Arezoo Hatefi,Xuan-Son Vu,Monowar Bhuyan,Frank Drewes*

Main category: cs.CL

TL;DR: 论文扩展了Hatefi等人的半监督文本分类模型，加入了基于目标掩码的无监督预训练阶段，并在多种数据集和语言上进行了性能评估。


<details>
  <summary>Details</summary>
Motivation: 研究旨在改进半监督文本分类模型，特别是在训练数据中黄金标签样本较少的情况下，通过结合无监督预训练提升模型性能。

Method: 扩展了Meta Pseudo Labels的师生框架，加入无监督预训练阶段，并通过三种数据集和两种语言评估模型。

Result: 实验结果展示了原始模型、扩展模型及多个基准模型的性能对比。

Conclusion: 研究表明，加入无监督预训练阶段能有效提升模型在半监督文本分类任务中的表现。

Abstract: We extend and study a semi-supervised model for text classification proposed
earlier by Hatefi et al. for classification tasks in which document classes are
described by a small number of gold-labeled examples, while the majority of
training examples is unlabeled. The model leverages the teacher-student
architecture of Meta Pseudo Labels in which a ''teacher'' generates labels for
originally unlabeled training data to train the ''student'' and updates its own
model iteratively based on the performance of the student on the gold-labeled
portion of the data. We extend the original model of Hatefi et al. by an
unsupervised pre-training phase based on objective masking, and conduct
in-depth performance evaluations of the original model, our extension, and
various independent baselines. Experiments are performed using three different
datasets in two different languages (English and Swedish).

</details>


### [14] [Dynamic Domain Information Modulation Algorithm for Multi-domain Sentiment Analysis](https://arxiv.org/abs/2505.06630)
*Chunyi Yue,Ang Li*

Main category: cs.CL

TL;DR: 该论文提出了一种动态信息调制算法，用于高效生成多领域情感分类所需的领域信息，通过两阶段训练和梯度/损失调整解决现有方法的计算资源、收敛和复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 多领域情感分类中，现有超参数优化算法在领域数量增加时面临计算资源需求大、收敛问题和算法复杂度高的挑战，亟需一种高效生成领域信息的方法。

Method: 采用两阶段训练：第一阶段确定全局共享的超参数控制领域分类任务比例；第二阶段通过梯度/损失方法的领域感知调制算法动态调整输入文本中的领域信息。

Result: 在包含16个领域的公开情感分析数据集上，实验证明了所提方法的优越性。

Conclusion: 动态信息调制算法能有效提升多领域情感分类的性能，同时降低计算成本。

Abstract: Multi-domain sentiment classification aims to mitigate poor performance
models due to the scarcity of labeled data in a single domain, by utilizing
data labeled from various domains. A series of models that jointly train domain
classifiers and sentiment classifiers have demonstrated their advantages,
because domain classification helps generate necessary information for
sentiment classification. Intuitively, the importance of sentiment
classification tasks is the same in all domains for multi-domain sentiment
classification; but domain classification tasks are different because the
impact of domain information on sentiment classification varies across
different fields; this can be controlled through adjustable weights or hyper
parameters. However, as the number of domains increases, existing
hyperparameter optimization algorithms may face the following challenges: (1)
tremendous demand for computing resources, (2) convergence problems, and (3)
high algorithm complexity. To efficiently generate the domain information
required for sentiment classification in each domain, we propose a dynamic
information modulation algorithm. Specifically, the model training process is
divided into two stages. In the first stage, a shared hyperparameter, which
would control the proportion of domain classification tasks across all fields,
is determined. In the second stage, we introduce a novel domain-aware
modulation algorithm to adjust the domain information contained in the input
text, which is then calculated based on a gradient-based and loss-based method.
In summary, experimental results on a public sentiment analysis dataset
containing 16 domains prove the superiority of the proposed method.

</details>


### [15] [Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer Models](https://arxiv.org/abs/2505.06633)
*Isaac Gerber*

Main category: cs.CL

TL;DR: 研究探讨了FFN在解码器-生成式Transformer预训练中的重要性，发现三层FFN结构比标准两层FFN表现更好，训练损失更低，参数量更少且训练更快。


<details>
  <summary>Details</summary>
Motivation: 探索FFN在预训练中的作用及其对模型性能的影响。

Method: 通过实验比较标准两层FFN与三层FFN在不同Transformer块配置下的表现。

Result: 三层FFN结构在减少参数量的同时，实现了更低的训练损失和更快的训练速度。

Conclusion: FFN在预训练中至关重要，三层FFN配置优于标准两层结构。

Abstract: Decoder-only transformer networks have become incredibly popular for language
modeling tasks. State-of-the-art models can have over a hundred transformer
blocks, containing billions of trainable parameters, and are trained on
trillions of tokens of text. Each transformer block typically consists of a
multi-head attention (MHA) mechanism and a two-layer fully connected
feedforward network (FFN). In this paper, we examine the importance of the FFN
during the model pre-training process through a series of experiments,
confirming that the FFN is important to model performance. Furthermore, we show
that models using a transformer block configuration with three-layer FFNs with
fewer such blocks outperform the standard two-layer configuration delivering
lower training loss with fewer total parameters in less time.

</details>


### [16] [TS-SUPERB: A Target Speech Processing Benchmark for Speech Self-Supervised Learning Models](https://arxiv.org/abs/2505.06660)
*Junyi Peng,Takanori Ashihara,Marc Delcroix,Tsubasa Ochiai,Oldrich Plchot,Shoko Araki,Jan Černocký*

Main category: cs.CL

TL;DR: 论文介绍了TS-SUPERB基准测试，用于评估SSL模型在多说话人嘈杂环境中的目标说话人任务性能，并验证了联合优化方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前SSL模型的基准测试主要针对单说话人场景，而对嘈杂多说话人环境中的目标说话人任务研究不足，因此需要更全面的评估方法。

Method: 提出了TS-SUPERB基准测试，包含四项目标说话人处理任务，并采用基于SSL的统一目标语音编码器（包含说话人编码器和提取模块），研究跨任务联合优化。

Result: 结果表明，目标说话人任务的性能无法简单从单说话人任务推断，且联合优化能有效利用任务间的互信息提升表现。

Conclusion: TS-SUPERB突出了在目标说话人场景中评估SSL模型的重要性，并提出了一种有效的联合优化方法。

Abstract: Self-supervised learning (SSL) models have significantly advanced speech
processing tasks, and several benchmarks have been proposed to validate their
effectiveness. However, previous benchmarks have primarily focused on
single-speaker scenarios, with less exploration of target-speaker tasks in
noisy, multi-talker conditions -- a more challenging yet practical case. In
this paper, we introduce the Target-Speaker Speech Processing Universal
Performance Benchmark (TS-SUPERB), which includes four widely recognized
target-speaker processing tasks that require identifying the target speaker and
extracting information from the speech mixture. In our benchmark, the speaker
embedding extracted from enrollment speech is used as a clue to condition
downstream models. The benchmark result reveals the importance of evaluating
SSL models in target speaker scenarios, demonstrating that performance cannot
be easily inferred from related single-speaker tasks. Moreover, by using a
unified SSL-based target speech encoder, consisting of a speaker encoder and an
extractor module, we also investigate joint optimization across TS tasks to
leverage mutual information and demonstrate its effectiveness.

</details>


### [17] [Enhancing BERTopic with Intermediate Layer Representations](https://arxiv.org/abs/2505.06696)
*Dominik Koterwa,Maciej Świtała*

Main category: cs.CL

TL;DR: 论文研究了BERTopic算法中18种不同嵌入表示的性能，发现针对不同数据集可以找到优于默认设置的配置，并探讨了停用词对嵌入配置的影响。


<details>
  <summary>Details</summary>
Motivation: 评估BERTopic中不同嵌入表示的性能，以优化其在多样数据集上的表现。

Method: 使用18种嵌入表示在三个数据集上进行实验，评估主题一致性和多样性。

Result: 每种数据集都能找到优于默认设置的嵌入配置，停用词对嵌入配置的影响显著。

Conclusion: 嵌入配置的选择对BERTopic性能至关重要，且应考虑数据集特性和停用词影响。

Abstract: BERTopic is a topic modeling algorithm that leverages transformer-based
embeddings to create dense clusters, enabling the estimation of topic
structures and the extraction of valuable insights from a corpus of documents.
This approach allows users to efficiently process large-scale text data and
gain meaningful insights into its structure. While BERTopic is a powerful tool,
embedding preparation can vary, including extracting representations from
intermediate model layers and applying transformations to these embeddings. In
this study, we evaluate 18 different embedding representations and present
findings based on experiments conducted on three diverse datasets. To assess
the algorithm's performance, we report topic coherence and topic diversity
metrics across all experiments. Our results demonstrate that, for each dataset,
it is possible to find an embedding configuration that performs better than the
default setting of BERTopic. Additionally, we investigate the influence of stop
words on different embedding configurations.

</details>


### [18] [From Rankings to Insights: Evaluation Should Shift Focus from Leaderboard to Feedback](https://arxiv.org/abs/2505.06698)
*Zongqi Wang,Tianle Gu,Chen Gong,Xin Tian,Siqi Bao,Yujiu Yang*

Main category: cs.CL

TL;DR: 论文提出了一个名为Feedbacker的新型评估框架，旨在从单纯模仿人类模型排名转向提供有分析价值的反馈，以帮助模型优化和性能分析。


<details>
  <summary>Details</summary>
Motivation: 现有自动评估基准（如MT-Bench、Arena-Hard等）仅提供总体分数，无法为模型优化提供具体反馈。

Method: Feedbacker包含三个核心组件：可扩展的树状查询分类构建器、自动化查询合成方案、可视化分析工具，并提出PC2点对点评估方法。

Result: 通过评估17个主流LLM，展示了Feedbacker的有效性和潜力。

Conclusion: Feedbacker框架能提供细致全面的反馈，支持模型优化并增强对其行为的理解。

Abstract: Automatic evaluation benchmarks such as MT-Bench, Arena-Hard, and Auto-Arena
are seeing growing adoption for the evaluation of Large Language Models (LLMs).
Existing research has primarily focused on approximating human-based model
rankings using limited data and LLM-as-a-Judge. However, the fundamental
premise of these studies, which attempts to replicate human rankings, is
flawed. Specifically, these benchmarks typically offer only overall scores,
limiting their utility to leaderboard rankings, rather than providing feedback
that can guide model optimization and support model profiling. Therefore, we
advocate for an evaluation paradigm shift from approximating human-based model
rankings to providing feedback with analytical value. To this end, we introduce
Feedbacker, an evaluation framework that provides comprehensive and
fine-grained results, thereby enabling thorough identification of a model's
specific strengths and weaknesses. Such feedback not only supports the targeted
optimization of the model but also enhances the understanding of its behavior.
Feedbacker comprises three key components: an extensible tree-based query
taxonomy builder, an automated query synthesis scheme, and a suite of
visualization and analysis tools. Furthermore, we propose a novel
LLM-as-a-Judge method: PC2 (Pre-Comparison-derived Criteria) pointwise
evaluation. This method derives evaluation criteria by pre-comparing the
differences between several auxiliary responses, achieving the accuracy of
pairwise evaluation while maintaining the time complexity of pointwise
evaluation. Finally, leveraging the evaluation results of 17 mainstream LLMs,
we demonstrate the usage of Feedbacker and highlight its effectiveness and
potential. Our homepage project is available at
https://liudan193.github.io/Feedbacker.

</details>


### [19] [Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free](https://arxiv.org/abs/2505.06708)
*Zihan Qiu,Zekun Wang,Bo Zheng,Zeyu Huang,Kaiyue Wen,Songlin Yang,Rui Men,Le Yu,Fei Huang,Suozhi Huang,Dayiheng Liu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: 本文通过系统实验研究了软注意力机制中添加门控的效果，发现简单的头部特异性sigmoid门控能提升性能、训练稳定性和长上下文外推能力。


<details>
  <summary>Details</summary>
Motivation: 探讨门控机制在软注意力中的具体效果，填补现有文献中的空白。

Method: 对比30种15B专家混合模型和1.7B密集模型的门控变体，重点关注头部特异性sigmoid门控的应用。

Result: 头部特异性sigmoid门控显著提升性能、稳定性和学习率容忍度，并解决'注意力下沉'问题。

Conclusion: 门控机制通过引入非线性和稀疏门控评分有效优化软注意力，公开代码和模型以促进后续研究。

Abstract: Gating mechanisms have been widely utilized, from early models like LSTMs and
Highway Networks to recent state space models, linear attention, and also
softmax attention. Yet, existing literature rarely examines the specific
effects of gating. In this work, we conduct comprehensive experiments to
systematically investigate gating-augmented softmax attention variants.
Specifically, we perform a comprehensive comparison over 30 variants of 15B
Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion
token dataset. Our central finding is that a simple modification-applying a
head-specific sigmoid gate after the Scaled Dot-Product Attention
(SDPA)-consistently improves performance. This modification also enhances
training stability, tolerates larger learning rates, and improves scaling
properties. By comparing various gating positions and computational variants,
we attribute this effectiveness to two key factors: (1) introducing
non-linearity upon the low-rank mapping in the softmax attention, and (2)
applying query-dependent sparse gating scores to modulate the SDPA output.
Notably, we find this sparse gating mechanism mitigates 'attention sink' and
enhances long-context extrapolation performance, and we also release related
$\href{https://github.com/qiuzh20/gated_attention}{codes}$ and
$\href{https://huggingface.co/QwQZh/gated_attention}{models}$ to facilitate
future research.

</details>


### [20] [Utilizing LLMs to Investigate the Disputed Role of Evidence in Electronic Cigarette Health Policy Formation in Australia and the UK](https://arxiv.org/abs/2505.06782)
*Damian Curran,Brian Chapman,Mike Conway*

Main category: cs.CL

TL;DR: 论文利用GPT-4开发了一个句子分类器，分析澳大利亚和英国的电子烟政策文件，发现澳方更强调危害，英方更强调益处。


<details>
  <summary>Details</summary>
Motivation: 研究澳大利亚和英国在电子烟政策上的差异，尽管基于相同证据，但政策取向截然不同。

Method: 使用GPT-4开发的LLM句子分类器自动分析109份政策文件中的句子，分类为有益或有害公共健康的声明。

Result: 分类器F-score达0.9；澳大利亚文件更强调危害，英国文件更强调益处。

Conclusion: LLM方法证实了两国政策差异，并展示了其在健康政策与证据关系研究中的潜力。

Abstract: Australia and the UK have developed contrasting approaches to the regulation
of electronic cigarettes, with - broadly speaking - Australia adopting a
relatively restrictive approach and the UK adopting a more permissive approach.
Notably, these divergent policies were developed from the same broad evidence
base. In this paper, to investigate differences in how the two jurisdictions
manage and present evidence, we developed and evaluated a Large Language
Model-based sentence classifier to perform automated analyses of electronic
cigarette-related policy documents drawn from official Australian and UK
legislative processes (109 documents in total). Specifically, we utilized GPT-4
to automatically classify sentences based on whether they contained claims that
e-cigarettes were broadly helpful or harmful for public health. Our LLM-based
classifier achieved an F-score of 0.9. Further, when applying the classifier to
our entire sentence-level corpus, we found that Australian legislative
documents show a much higher proportion of harmful statements, and a lower
proportion of helpful statements compared to the expected values, with the
opposite holding for the UK. In conclusion, this work utilized an LLM-based
approach to provide evidence to support the contention that - drawing on the
same evidence base - Australian ENDS-related policy documents emphasize the
harms associated with ENDS products and UK policy documents emphasize the
benefits. Further, our approach provides a starting point for using LLM-based
methods to investigate the complex relationship between evidence and health
policy formation.

</details>


### [21] [A Split-then-Join Approach to Abstractive Summarization for Very Long Documents in a Low Resource Setting](https://arxiv.org/abs/2505.06862)
*Lhuqita Fazry*

Main category: cs.CL

TL;DR: BIGBIRD-PEGASUS模型在长文档摘要任务上表现优秀，但由于其最多只能处理4096个标记，对超长文档摘要的性能有限。研究采用微调预训练模型的方法，筛选超过20000标记的文档，并通过拆分文档-摘要对来适应模型限制，以避免域偏移和小数据集过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 解决BIGBIRD-PEGASUS模型在处理超长文档时因标记限制导致的性能下降问题，避免常见的截断文档方法。

Method: 筛选超过20000标记的文档，采用数据增强方法将文档-摘要对拆分为符合4096标记限制的部分，并对模型进行微调。

Result: 模型能够在超长文档摘要任务上保持性能，避免了因直接截断带来的信息损失。

Conclusion: 通过数据增强和微调预训练模型，成功提升了BIGBIRD-PEGASUS在超长文档摘要任务上的表现。

Abstract: $\texttt{BIGBIRD-PEGASUS}$ model achieves $\textit{state-of-the-art}$ on
abstractive text summarization for long documents. However it's capacity still
limited to maximum of $4,096$ tokens, thus caused performance degradation on
summarization for very long documents. Common method to deal with the issue is
to truncate the documents. In this reasearch, we'll use different approach.
We'll use the pretrained $\texttt{BIGBIRD-PEGASUS}$ model by fine tuned the
model on other domain dataset. First, we filter out all documents which length
less than $20,000$ tokens to focus on very long documents. To prevent domain
shifting problem and overfitting on transfer learning due to small dataset, we
augment the dataset by splitting document-summary training pair into parts, to
fit the document into $4,096$ tokens. Source code available on
$\href{https://github.com/lhfazry/SPIN-summ}{https://github.com/lhfazry/SPIN-summ}$.

</details>


### [22] [IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method](https://arxiv.org/abs/2505.06889)
*Mihyeon Kim,Juhyoung Park,Youngbin Kim*

Main category: cs.CL

TL;DR: 该论文提出了IM-BERT模型，通过将BERT的层建模为ODE解，并分析数值稳定性，增强了预训练语言模型在对抗攻击下的鲁棒性，尤其在低资源场景表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决预训练语言模型在下游任务微调时因参数过多而导致的对抗攻击脆弱性问题。

Method: 将BERT的层建模为ODE解，分析显式和隐式欧拉方法的数值稳定性，并引入IM-connection策略。

Result: 在AdvGLUE数据集上，IM-BERT比原始BERT性能提升约8.3%，低资源场景下准确率提升5.9%。

Conclusion: IM-BERT通过数值稳定的连接策略，显著提升了模型对抗攻击的鲁棒性，且无需额外参数或对抗训练。

Abstract: Pre-trained Language Models (PLMs) have achieved remarkable performance on
diverse NLP tasks through pre-training and fine-tuning. However, fine-tuning
the model with a large number of parameters on limited downstream datasets
often leads to vulnerability to adversarial attacks, causing overfitting of the
model on standard datasets.
  To address these issues, we propose IM-BERT from the perspective of a dynamic
system by conceptualizing a layer of BERT as a solution of Ordinary
Differential Equations (ODEs). Under the situation of initial value
perturbation, we analyze the numerical stability of two main numerical ODE
solvers: the explicit and implicit Euler approaches.
  Based on these analyses, we introduce a numerically robust IM-connection
incorporating BERT's layers. This strategy enhances the robustness of PLMs
against adversarial attacks, even in low-resource scenarios, without
introducing additional parameters or adversarial training strategies.
  Experimental results on the adversarial GLUE (AdvGLUE) dataset validate the
robustness of IM-BERT under various conditions. Compared to the original BERT,
IM-BERT exhibits a performance improvement of approximately 8.3\%p on the
AdvGLUE dataset. Furthermore, in low-resource scenarios, IM-BERT outperforms
BERT by achieving 5.9\%p higher accuracy.

</details>


### [23] [EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation](https://arxiv.org/abs/2505.06904)
*Xinyi Mou,Chen Qian,Wei Liu,Xuanjing Huang,Zhongyu Wei*

Main category: cs.CL

TL;DR: EcoLANG提出了两阶段语言演化与利用方法，显著降低社交模拟中的计算成本，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在减少大型语言模型的推理成本或保持准确性方面存在不足，EcoLANG旨在解决这一问题。

Method: 分为语言演化和语言利用两个阶段：通过自然选择优化语言规则，并在社交模拟中使用优化的语言。

Result: 实验表明EcoLANG减少超过20%的token消耗，提升效率且不影响准确性。

Conclusion: EcoLANG为高效且准确的社交模拟提供了一种新方法。

Abstract: Large language models (LLMs) have demonstrated an impressive ability to
role-play humans and replicate complex social dynamics. While large-scale
social simulations are gaining increasing attention, they still face
significant challenges, particularly regarding high time and computation costs.
Existing solutions, such as distributed mechanisms or hybrid agent-based model
(ABM) integrations, either fail to address inference costs or compromise
accuracy and generalizability. To this end, we propose EcoLANG: Efficient and
Effective Agent Communication Language Induction for Social Simulation. EcoLANG
operates in two stages: (1) language evolution, where we filter synonymous
words and optimize sentence-level rules through natural selection, and (2)
language utilization, where agents in social simulations communicate using the
evolved language. Experimental results demonstrate that EcoLANG reduces token
consumption by over 20%, enhancing efficiency without sacrificing simulation
accuracy.

</details>


### [24] [The Distracting Effect: Understanding Irrelevant Passages in RAG](https://arxiv.org/abs/2505.06914)
*Chen Amiraz,Florin Cuconasu,Simone Filice,Zohar Karnin*

Main category: cs.CL

TL;DR: 本文针对检索增强生成（RAG）中无关段落分散LLM注意力的问题，提出量化干扰效应的方法，并利用硬干扰段落优化RAG系统，实现回答准确率提升7.5%。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中无关段落干扰LLM导致错误回答的核心问题，并提出量化干扰效应的方法。

Method: 引入量化干扰效应的指标，设计多种方法识别硬干扰段落，并利用这些段落微调LLM。

Result: 实验表明，与传统RAG数据集相比，使用硬干扰段落微调的LLM回答准确率提升达7.5%。

Conclusion: 本文不仅扩展了对无关段落的分类（从二元到干扰效应量化），还为识别和利用硬干扰段落提供了全面框架，显著提升了RAG系统的性能。

Abstract: A well-known issue with Retrieval Augmented Generation (RAG) is that
retrieved passages that are irrelevant to the query sometimes distract the
answer-generating LLM, causing it to provide an incorrect response. In this
paper, we shed light on this core issue and formulate the distracting effect of
a passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the
distracting effect of a passage and demonstrate its robustness across LLMs.
  Our research introduces novel methods for identifying and using hard
distracting passages to improve RAG systems. By fine-tuning LLMs with these
carefully selected distracting passages, we achieve up to a 7.5% increase in
answering accuracy compared to counterparts fine-tuned on conventional RAG
datasets. Our contribution is two-fold: first, we move beyond the simple binary
classification of irrelevant passages as either completely unrelated vs.
distracting, and second, we develop and analyze multiple methods for finding
hard distracting passages. To our knowledge, no other research has provided
such a comprehensive framework for identifying and utilizing hard distracting
passages.

</details>


### [25] [CNN-based Image Models Verify a Hypothesis that The Writers of Cuneiform Texts Improved Their Writing Skills When Studying at the Age of Hittite Empire](https://arxiv.org/abs/2505.06974)
*Daichi Kohmoto,Katsutoshi Fukuda,Daisuke Yoshida,Takafumi Matsui,Sachihiro Omura*

Main category: cs.CL

TL;DR: 论文通过CNN图像分析发现，刻有Kizzuwatna仪式的楔形文字板可能是师徒练习的记录，传统语言学未能得出此结论。


<details>
  <summary>Details</summary>
Motivation: 研究古代楔形文字板重复刻写的原因，尤其是KBo 23.1 ++/KUB 30.38这类非神话或商业用途的文本。

Method: 使用基于CNN的图像模型定量分析文字板图像，无需逐字分割楔形文字。

Result: 数据驱动方法显示第一部分由“老师”书写，第二部分由“学生”练习书写。

Conclusion: 该方法为楔形文字研究提供了新视角，并可能推广到其他类似文本的分析。

Abstract: A cuneiform tablet KBo 23.1 ++/KUB 30.38, which is known to represent a text
of Kizzuwatna rituals, was written by two writers with almost identical content
in two iterations. Unlike other cuneiform tablets that contained information
such as myths, essays, or business records, the reason why ancient people left
such tablets for posterity remains unclear. To study this problem, we develop a
new methodology by analyzing images of a tablet quantitatively using CNN
(Convolutional Neural Network)-based image models, without segmenting
cuneiforms one-by-one. Our data-driven methodology implies that the writer
writing the first half was a `teacher' and the other writer was a `student' who
was training his skills of writing cuneiforms. This result has not been reached
by classical linguistics. We also discuss related conclusions and possible
further directions for applying our method and its generalizations.

</details>


### [26] [Convert Language Model into a Value-based Strategic Planner](https://arxiv.org/abs/2505.06987)
*Xiaoyu Wang,Yue Zhao,Qingqing Gu,Zhonglin Jiang,Xiaokai Chen,Yong Chen,Luo Ji*

Main category: cs.CL

TL;DR: 论文提出了一个名为straQ*的框架，通过Q学习优化LLMs在情感支持对话中的长期效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究未从状态模型角度优化LLMs在情感支持对话中的长期满意度，导致解决方案不够理想。

Method: 结合Q学习和LLMs，设计可即插即用的straQ*框架，通过长期回报选择最优策略并指导LLMs生成回应。

Result: 在多个数据集上，straQ*表现优于直接推理、自优化、思维链、微调和有限状态机等方法。

Conclusion: straQ*显著提升了情感支持对话的长期效果，验证了Q学习在优化LLMs对话策略中的有效性。

Abstract: Emotional support conversation (ESC) aims to alleviate the emotional distress
of individuals through effective conversations. Although large language models
(LLMs) have obtained remarkable progress on ESC, most of these studies might
not define the diagram from the state model perspective, therefore providing a
suboptimal solution for long-term satisfaction. To address such an issue, we
leverage the Q-learning on LLMs, and propose a framework called straQ*. Our
framework allows a plug-and-play LLM to bootstrap the planning during ESC,
determine the optimal strategy based on long-term returns, and finally guide
the LLM to response. Substantial experiments on ESC datasets suggest that
straQ* outperforms many baselines, including direct inference, self-refine,
chain of thought, finetuning, and finite state machines.

</details>


### [27] [HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling](https://arxiv.org/abs/2505.07157)
*Hajar Sakai,Sarah S. Lam*

Main category: cs.CL

TL;DR: HAMLET是一种跨语言医疗主题建模的图驱动架构，利用LLM生成初始主题并通过BERT和GNN优化主题嵌入，提升主题质量和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型难以处理上下文复杂性和多义词/罕见词，导致主题质量低；LLM生成的主题虽初步但仍需优化。

Method: 结合BERT和GNN进行主题嵌入优化：先用BERT/SBERT嵌入，再通过GNN建立文档、主题、词汇间的关联，最终提取前k个主题。

Result: 在英法医疗数据集上的实验验证了HAMLET的有效性，显著提升了主题质量。

Conclusion: HAMLET通过神经增强语义融合有效解决了传统方法的局限性，为跨语言主题建模提供了新思路。

Abstract: Traditional topic models often struggle with contextual nuances and fail to
adequately handle polysemy and rare words. This limitation typically results in
topics that lack coherence and quality. Large Language Models (LLMs) can
mitigate this issue by generating an initial set of topics. However, these raw
topics frequently lack refinement and representativeness, which leads to
redundancy without lexical similarity and reduced interpretability. This paper
introduces HAMLET, a graph-driven architecture for cross-lingual healthcare
topic modeling that uses LLMs. The proposed approach leverages neural-enhanced
semantic fusion to refine the embeddings of topics generated by the LLM.
Instead of relying solely on statistical co-occurrence or human interpretation
to extract topics from a document corpus, this method introduces a topic
embedding refinement that uses Bidirectional Encoder Representations from
Transformers (BERT) and Graph Neural Networks (GNN). After topic generation, a
hybrid technique that involves BERT and Sentence-BERT (SBERT) is employed for
embedding. The topic representations are further refined using a GNN, which
establishes connections between documents, topics, words, similar topics, and
similar words. A novel method is introduced to compute similarities.
Consequently, the topic embeddings are refined, and the top k topics are
extracted. Experiments were conducted using two healthcare datasets, one in
English and one in French, from which six sets were derived. The results
demonstrate the effectiveness of HAMLET.

</details>


### [28] [Towards Actionable Pedagogical Feedback: A Multi-Perspective Analysis of Mathematics Teaching and Tutoring Dialogue](https://arxiv.org/abs/2505.07161)
*Jannatun Naim,Jie Cao,Fareen Tasneem,Jennifer Jacobs,Brent Milne,James Martin,Tamara Sumner*

Main category: cs.CL

TL;DR: 这篇论文提出了一种多视角话语分析方法，结合领域特定的对话动作和话语关系，解决了数学教育中反馈分析的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决数学教育中话语分析的两大挑战：多功能性和领域特定话语分类的局限性。

Method: 提出多视角话语分析框架，整合领域特定的对话动作和话语关系，应用于两个数学教育数据集。

Result: 发现了有意义的话语模式，揭示了非话语动作句子的重要作用。

Conclusion: 强调将话语关系和对话动作纳入AI辅助教育系统的重要性，以提升反馈质量和学习环境响应性。

Abstract: Effective feedback is essential for refining instructional practices in
mathematics education, and researchers often turn to advanced natural language
processing (NLP) models to analyze classroom dialogues from multiple
perspectives. However, utterance-level discourse analysis encounters two
primary challenges: (1) multifunctionality, where a single utterance may serve
multiple purposes that a single tag cannot capture, and (2) the exclusion of
many utterances from domain-specific discourse move classifications, leading to
their omission in feedback. To address these challenges, we proposed a
multi-perspective discourse analysis that integrates domain-specific talk moves
with dialogue act (using the flattened multi-functional SWBD-MASL schema with
43 tags) and discourse relation (applying Segmented Discourse Representation
Theory with 16 relations). Our top-down analysis framework enables a
comprehensive understanding of utterances that contain talk moves, as well as
utterances that do not contain talk moves. This is applied to two mathematics
education datasets: TalkMoves (teaching) and SAGA22 (tutoring). Through
distributional unigram analysis, sequential talk move analysis, and multi-view
deep dive, we discovered meaningful discourse patterns, and revealed the vital
role of utterances without talk moves, demonstrating that these utterances, far
from being mere fillers, serve crucial functions in guiding, acknowledging, and
structuring classroom discourse. These insights underscore the importance of
incorporating discourse relations and dialogue acts into AI-assisted education
systems to enhance feedback and create more responsive learning environments.
Our framework may prove helpful for providing human educator feedback, but also
aiding in the development of AI agents that can effectively emulate the roles
of both educators and students.

</details>


### [29] [KDH-MLTC: Knowledge Distillation for Healthcare Multi-Label Text Classification](https://arxiv.org/abs/2505.07162)
*Hajar Sakai,Sarah S. Lam*

Main category: cs.CL

TL;DR: KDH-MLTC框架通过知识蒸馏和序列微调优化医疗多标签文本分类，结合PSO超参数调优，显著降低计算需求并提升性能，F1分数达82.70%。


<details>
  <summary>Details</summary>
Motivation: 医疗文本数据量增长迅速，需要高效且高精度的分类方法处理复杂医学术语，同时满足敏感数据的HIPAA合规要求。

Method: 采用知识蒸馏将BERT的知识迁移到轻量级DistilBERT，结合序列微调和PSO超参数优化，实现高效分类。

Result: 在三个不同规模的医学数据集上表现优异，最大数据集F1分数达82.70%，并通过统计验证和消融实验证明其鲁棒性。

Conclusion: KDH-MLTC平衡了资源限制下的效率需求与准确性，为医疗文本分类研究提供了实用解决方案。

Abstract: The increasing volume of healthcare textual data requires computationally
efficient, yet highly accurate classification approaches able to handle the
nuanced and complex nature of medical terminology. This research presents
Knowledge Distillation for Healthcare Multi-Label Text Classification
(KDH-MLTC), a framework leveraging model compression and Large Language Models
(LLMs). The proposed approach addresses conventional healthcare Multi-Label
Text Classification (MLTC) challenges by integrating knowledge distillation and
sequential fine-tuning, subsequently optimized through Particle Swarm
Optimization (PSO) for hyperparameter tuning. KDH-MLTC transfers knowledge from
a more complex teacher LLM (i.e., BERT) to a lighter student LLM (i.e.,
DistilBERT) through sequential training adapted to MLTC that preserves the
teacher's learned information while significantly reducing computational
requirements. As a result, the classification is enabled to be conducted
locally, making it suitable for healthcare textual data characterized by
sensitivity and, therefore, ensuring HIPAA compliance. The experiments
conducted on three medical literature datasets of different sizes, sampled from
the Hallmark of Cancer (HoC) dataset, demonstrate that KDH-MLTC achieves
superior performance compared to existing approaches, particularly for the
largest dataset, reaching an F1 score of 82.70%. Additionally, statistical
validation and an ablation study are carried out, proving the robustness of
KDH-MLTC. Furthermore, the PSO-based hyperparameter optimization process
allowed the identification of optimal configurations. The proposed approach
contributes to healthcare text classification research, balancing efficiency
requirements in resource-constrained healthcare settings with satisfactory
accuracy demands.

</details>


### [30] [Structural Entropy Guided Agent for Detecting and Repairing Knowledge Deficiencies in LLMs](https://arxiv.org/abs/2505.07184)
*Yifan Wei,Xiaoyan Yu,Tengfei Pan,Angsheng Li,Li Du*

Main category: cs.CL

TL;DR: 论文提出了一种名为SENATOR的新型框架，通过结构熵和蒙特卡洛树搜索来识别和修复大型语言模型在知识密集型领域的缺陷，并通过生成定向合成数据提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在广泛预训练语料库上表现优异，但在需要高事实准确性的知识密集型领域（如医学和科研）仍表现不足。现有方法生成的合成数据通常冗余且未能针对模型的知识缺口。

Method: 提出SENATOR框架，利用结构熵（SE）度量知识图谱路径上的不确定性，并结合蒙特卡洛树搜索（MCTS）选择性探索模型缺乏知识的区域，生成定向合成数据用于监督微调。

Result: 在LLaMA-3和Qwen2上的多领域基准测试表明，SENATOR能有效检测并修复知识缺陷，显著提升了模型性能。

Conclusion: SENATOR通过结构熵和定向数据生成，为LLMs在知识密集型领域的持续自我改进提供了有效解决方案。

Abstract: Large language models (LLMs) have achieved unprecedented performance by
leveraging vast pretraining corpora, yet their performance remains suboptimal
in knowledge-intensive domains such as medicine and scientific research, where
high factual precision is required. While synthetic data provides a promising
avenue for augmenting domain knowledge, existing methods frequently generate
redundant samples that do not align with the model's true knowledge gaps. To
overcome this limitation, we propose a novel Structural Entropy-guided
Knowledge Navigator (SENATOR) framework that addresses the intrinsic knowledge
deficiencies of LLMs. Our approach employs the Structure Entropy (SE) metric to
quantify uncertainty along knowledge graph paths and leverages Monte Carlo Tree
Search (MCTS) to selectively explore regions where the model lacks
domain-specific knowledge. Guided by these insights, the framework generates
targeted synthetic data for supervised fine-tuning, enabling continuous
self-improvement. Experimental results on LLaMA-3 and Qwen2 across multiple
domain-specific benchmarks show that SENATOR effectively detects and repairs
knowledge deficiencies, achieving notable performance improvements. The code
and data for our methods and experiments are available at
https://github.com/weiyifan1023/senator.

</details>


### [31] [On the Cost and Benefits of Training Context with Utterance or Full Conversation Training: A Comparative Stud](https://arxiv.org/abs/2505.07202)
*Hyouin Liu,Zhikuan Zhang*

Main category: cs.CL

TL;DR: 比较两种对话TTS训练方法：基于上下文的句子级训练和完整对话训练，前者在MOS分数和训练效率上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的开放TTS系统在对话场景中性能有限，本文旨在探究其根本原因并提出改进方案。

Method: 使用20 GPU小时在NVIDIA H100上实证对比两种方法：上下文句子级训练和完整对话训练。

Result: 上下文句子级训练MOS得分更高（4.3/5.0），训练时间减少37%，且避免了说话人相似性幻觉问题。

Conclusion: 对话TTS开发应优先采用基于上下文的句子级训练，以兼顾资源效率和输出质量。

Abstract: Modern TTS systems designed for conversations achieve high-quality utterances
but often remain inaccessible publicly. Are existing open-source architectures
inadequate, or are current training techniques insufficient? This paper
investigates prominent models and their underlying behaviors regarding
conversational context. Using 20 GPU-hours on an NVIDIA H100, we empirically
examine two approaches: context-based utterance-level training versus full
conversation training. Results demonstrate that context-based utterance
training achieves superior MOS scores (4.3/5.0 vs 3.7/5.0) and reduces training
time by 37%, while full conversation approaches suffer from speaker similarity
hallucination issues. These findings provide practical guidelines for
conversational TTS development, favoring utterance-level training with
contextual conditioning for both resource efficiency and output quality.

</details>


### [32] [Benchmarking Ethical and Safety Risks of Healthcare LLMs in China-Toward Systemic Governance under Healthy China 2030](https://arxiv.org/abs/2505.07205)
*Mouxiao Bian,Rongzhao Zhang,Chao Ding,Xinwei Peng,Jie Xu*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）在中国医疗领域的伦理与安全挑战，提出了一个12000项的Q&A基准测试，评估了当前医疗LLM的表现，并发现其存在显著缺陷。研究还指出了系统性治理不足，并提出了一套实用的治理框架。


<details>
  <summary>Details</summary>
Motivation: 中国政府提出‘健康中国2030’计划，希望通过LLM技术推动医疗创新，但LLM在伦理和患者安全方面提出了新的挑战。研究旨在量化这些风险，并提供治理建议以确保AI创新符合伦理标准和患者安全。

Method: 研究构建了一个包含12000项的Q&A基准测试，覆盖11个伦理维度和9个安全维度，用于评估中文医疗LLM的表现。研究人员通过这一数据集评估了多个先进的中文医疗LLM，并对其进行微调以观察改进效果。

Result: 基准测试显示，现有医疗LLM在伦理和安全场景中的表现仅为中等（如Qwen 2.5-32B的准确率为42.7%），经过微调后提升至50.8%。研究还发现当前治理体系存在不足，如缺乏细化的伦理审计协议和评估工具。

Conclusion: 研究强调了中国医疗领域迫切需要加强LLM治理，并提出了一个整合伦理审计团队、数据伦理指南和安全模拟管道的框架，以促进AI创新与患者安全、伦理标准的协调。

Abstract: Large Language Models (LLMs) are poised to transform healthcare under China's
Healthy China 2030 initiative, yet they introduce new ethical and
patient-safety challenges. We present a novel 12,000-item Q&A benchmark
covering 11 ethics and 9 safety dimensions in medical contexts, to
quantitatively evaluate these risks. Using this dataset, we assess
state-of-the-art Chinese medical LLMs (e.g., Qwen 2.5-32B, DeepSeek), revealing
moderate baseline performance (accuracy 42.7% for Qwen 2.5-32B) and significant
improvements after fine-tuning on our data (up to 50.8% accuracy). Results show
notable gaps in LLM decision-making on ethics and safety scenarios, reflecting
insufficient institutional oversight. We then identify systemic governance
shortfalls-including the lack of fine-grained ethical audit protocols, slow
adaptation by hospital IRBs, and insufficient evaluation tools-that currently
hinder safe LLM deployment. Finally, we propose a practical governance
framework for healthcare institutions (embedding LLM auditing teams, enacting
data ethics guidelines, and implementing safety simulation pipelines) to
proactively manage LLM risks. Our study highlights the urgent need for robust
LLM governance in Chinese healthcare, aligning AI innovation with patient
safety and ethical standards.

</details>


### [33] [DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.07233)
*Jiashuo Sun,Xianrui Zhong,Sizhe Zhou,Jiawei Han*

Main category: cs.CL

TL;DR: 提出了DynamicRAG框架，通过强化学习优化动态调整检索文档数量和顺序的reranker，提升知识密集型任务的生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统中的reranker组件常被忽视，且固定文档数量k可能导致信息遗漏或噪声引入。现有研究未充分利用LLM反馈信号优化reranking决策。

Method: 将reranker建模为强化学习代理，根据LLM输出质量作为奖励动态调整检索文档的顺序和数量。

Result: 在七个知识密集型数据集上取得最优性能，代码和数据已开源。

Conclusion: DynamicRAG通过动态调整策略显著提升RAG系统效果，为reranker设计提供了新思路。

Abstract: Retrieval-augmented generation (RAG) systems combine large language models
(LLMs) with external knowledge retrieval, making them highly effective for
knowledge-intensive tasks. A crucial but often under-explored component of
these systems is the reranker, which refines retrieved documents to enhance
generation quality and explainability. The challenge of selecting the optimal
number of documents (k) remains unsolved: too few may omit critical
information, while too many introduce noise and inefficiencies. Although recent
studies have explored LLM-based rerankers, they primarily leverage internal
model knowledge and overlook the rich supervisory signals that LLMs can
provide, such as using response quality as feedback for optimizing reranking
decisions. In this paper, we propose DynamicRAG, a novel RAG framework where
the reranker dynamically adjusts both the order and number of retrieved
documents based on the query. We model the reranker as an agent optimized
through reinforcement learning (RL), using rewards derived from LLM output
quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates
superior performance, achieving state-of-the-art results. The model, data and
code are available at https://github.com/GasolSun36/DynamicRAG

</details>


### [34] [SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring with Large Language Models](https://arxiv.org/abs/2505.07247)
*Peichao Lai,Kexuan Zhang,Yi Lin,Linyihan Zhang,Feiyang Ye,Jinhao Yan,Yanwei Xu,Conghui He,Yilei Wang,Wentao Zhang,Bin Cui*

Main category: cs.CL

TL;DR: 论文介绍了SAS-Bench，一个专为LLM设计的短答案评分基准，提供细粒度评分、专家标注的错误类别及多样题型，旨在改善现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有主观答案评分方法评分粗糙且缺乏详细推理，LLM作为零样本评估器易受偏差影响且透明度不足。

Method: 引入SAS-Bench基准，包含1,030题与4,109标注答案，通过实验验证少样本提示提升评分准确性。

Result: 发现科学类题目评分挑战，少样本提示显著提升准确性，数据集与基准促进模型推理与可解释性研究。

Conclusion: SAS-Bench为开发更健壮、公平且教育意义强的LLM评估系统提供了重要基础。

Abstract: Subjective Answer Grading (SAG) plays a crucial role in education,
standardized testing, and automated assessment systems, particularly for
evaluating short-form responses in Short Answer Scoring (SAS). However,
existing approaches often produce coarse-grained scores and lack detailed
reasoning. Although large language models (LLMs) have demonstrated potential as
zero-shot evaluators, they remain susceptible to bias, inconsistencies with
human judgment, and limited transparency in scoring decisions. To overcome
these limitations, we introduce SAS-Bench, a benchmark specifically designed
for LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring,
expert-annotated error categories, and a diverse range of question types
derived from real-world subject-specific exams. This benchmark facilitates
detailed evaluation of model reasoning processes and explainability. We also
release an open-source dataset containing 1,030 questions and 4,109 student
responses, each annotated by domain experts. Furthermore, we conduct
comprehensive experiments with various LLMs, identifying major challenges in
scoring science-related questions and highlighting the effectiveness of
few-shot prompting in improving scoring accuracy. Our work offers valuable
insights into the development of more robust, fair, and educationally
meaningful LLM-based evaluation systems.

</details>


### [35] [No Query, No Access](https://arxiv.org/abs/2505.07258)
*Wenqiang Wang,Siyuan Liang,Yangshijie Zhang,Xiaojun Jia,Hao Lin,Xiaochun Cao*

Main category: cs.CL

TL;DR: 论文提出了一种基于受害者文本的对抗攻击方法（VDBA），仅利用受害者文本就能有效攻击NLP模型，且无需访问受害者模型。通过构建影子数据集和分层替代模型设计，显著提升了攻击成功率（ASR），并在实验中验证了对LLM（如Qwen2和GPT系列）的威胁。


<details>
  <summary>Details</summary>
Motivation: 现有的文本对抗攻击方法通常需要了解受害者模型、大量查询或训练数据，限制了实际可行性。为了克服这些限制，本文旨在设计一种仅依赖受害者文本的攻击方法，提升攻击效率并降低依赖。

Method: 提出了VDBA方法，利用公开预训练模型和聚类方法构建影子数据集，设计分层替代模型以解决单一模型在决策边界失效的问题，并采用多样化对抗样本生成策略优化攻击效果。

Result: 在Emotion和SST5数据集上，VDBA的ASR提升了52.08%，且攻击查询次数降为0。对Qwen2和GPT系列LLM的攻击ASR达45.99%，验证了其威胁性。

Conclusion: VDBA无需访问API即可对高级NLP模型构成严重安全威胁，表明当前模型仍存在重大安全风险。代码已开源。

Abstract: Textual adversarial attacks mislead NLP models, including Large Language
Models (LLMs), by subtly modifying text. While effective, existing attacks
often require knowledge of the victim model, extensive queries, or access to
training data, limiting real-world feasibility. To overcome these constraints,
we introduce the \textbf{Victim Data-based Adversarial Attack (VDBA)}, which
operates using only victim texts. To prevent access to the victim model, we
create a shadow dataset with publicly available pre-trained models and
clustering methods as a foundation for developing substitute models. To address
the low attack success rate (ASR) due to insufficient information feedback, we
propose the hierarchical substitution model design, generating substitute
models to mitigate the failure of a single substitute model at the decision
boundary.
  Concurrently, we use diverse adversarial example generation, employing
various attack methods to generate and select the adversarial example with
better similarity and attack effectiveness. Experiments on the Emotion and SST5
datasets show that VDBA outperforms state-of-the-art methods, achieving an ASR
improvement of 52.08\% while significantly reducing attack queries to 0. More
importantly, we discover that VDBA poses a significant threat to LLMs such as
Qwen2 and the GPT family, and achieves the highest ASR of 45.99% even without
access to the API, confirming that advanced NLP models still face serious
security risks. Our codes can be found at
https://anonymous.4open.science/r/VDBA-Victim-Data-based-Adversarial-Attack-36EC/

</details>


### [36] [On the Robustness of Reward Models for Language Model Alignment](https://arxiv.org/abs/2505.07271)
*Jiwoo Hong,Noah Lee,Eunki Kim,Guijin Son,Woojin Chung,Aman Gupta,Shao Tang,James Thorne*

Main category: cs.CL

TL;DR: 论文探讨了Bradley-Terry模型在RM训练中的过优化问题，提出了批量和为零正则化（BSR）以提高鲁棒性，并验证了其在RLHF训练中的有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然Bradley-Terry模型在RLHF中广泛用于奖励建模，但其训练出的奖励模型易在未见过数据上泛化性不足。本文旨在研究过优化的成因及解决方案。

Method: 提出了批量和为零正则化（BSR），限制奖励值的极端分布，通过四类过优化场景验证其提升奖励模型鲁棒性的效果。

Result: BSR在RLHF训练中显著提高了策略与黄金偏好模型的对齐效果，8B规模模型在复杂偏好预测任务中性能提升5%+，且在AlpacaEval 2.0中减少生成长度40%的同时提升胜率7%。

Conclusion: BSR能有效提升奖励模型的鲁棒性和RLHF训练的稳定性，实验表明其在多个任务和模型规模中表现优异。

Abstract: The Bradley-Terry (BT) model is widely practiced in reward modeling for
reinforcement learning with human feedback (RLHF). Despite its effectiveness,
reward models (RMs) trained with BT model loss are prone to over-optimization,
losing generalizability to unseen input distributions. In this paper, we study
the cause of over-optimization in RM training and its downstream effects on the
RLHF procedure, accentuating the importance of distributional robustness of RMs
in unseen data. First, we show that the excessive dispersion of hidden state
norms is the main source of over-optimization. Then, we propose batch-wise
sum-to-zero regularization (BSR) to enforce zero-centered reward sum per batch,
constraining the rewards with extreme magnitudes. We assess the impact of BSR
in improving robustness in RMs through four scenarios of over-optimization,
where BSR consistently manifests better robustness. Subsequently, we compare
the plain BT model and BSR on RLHF training and empirically show that robust
RMs better align the policy to the gold preference model. Finally, we apply BSR
to high-quality data and models, which surpasses state-of-the-art RMs in the 8B
scale by adding more than 5% in complex preference prediction tasks. By
conducting RLOO training with 8B RMs, AlpacaEval 2.0 reduces generation length
by 40% while adding a 7% increase in win rate, further highlighting that
robustness in RMs induces robustness in RLHF training. We release the code,
data, and models: https://github.com/LinkedIn-XFACT/RM-Robustness.

</details>


### [37] [Semantic Retention and Extreme Compression in LLMs: Can We Have Both?](https://arxiv.org/abs/2505.07289)
*Stanislas Laborde,Martin Cousseau,Antoun Yaacoub,Lionel Prevost*

Main category: cs.CL

TL;DR: 论文探讨了结合剪枝与量化的联合压缩方法，引入新指标 SrCr 评估语义保留与压缩率，实验显示该方法比单一量化效率提升约 20%。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLM）计算和内存成本高，需高效压缩技术。剪枝与量化结合潜力未被充分挖掘，需优化二者协同效果。

Method: 提出联合剪枝与量化的压缩策略，并设计 Semantic Retention Compression Rate (SrCr) 指标，量化压缩率与语义保留的权衡。

Result: 实验表明，推荐组合在相同理论压缩率下，性能比单一量化模型平均提高 20%。

Conclusion: 联合剪枝与量化可显著提升压缩效率，SrCr 指标为优化压缩配置提供有效工具。

Abstract: The exponential growth in Large Language Model (LLM) deployment has
intensified the need for efficient model compression techniques to reduce
computational and memory costs. While pruning and quantization have shown
promise, their combined potential remains largely unexplored. In this paper, we
examine joint compression and how strategically combining pruning and
quantization could yield superior performance-to-compression ratios compared to
single-method approaches. Recognizing the challenges in accurately assessing
LLM performance, we address key limitations of previous evaluation frameworks
and introduce the Semantic Retention Compression Rate (SrCr), a novel metric
that quantifies the trade-off between model compression and semantic
preservation, facilitating the optimization of pruning-quantization
configurations. Experiments demonstrate that our recommended combination
achieves, on average, a 20% performance increase compared to an equivalent
quantization-only model at the same theoretical compression rate.

</details>


### [38] [AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection](https://arxiv.org/abs/2505.07293)
*Kai Hua,Steven Wu,Ge Zhang,Ke Shen*

Main category: cs.CL

TL;DR: 提出了一种无需训练和监督的方法AttentionInfluence，通过注意力头掩码操作筛选推理密集型数据，显著提升了大型语言模型在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有监督分类器筛选数据，容易引入领域偏见。基于注意力头在上下文推理中的关键作用，设计了无需监督的解决方案。

Method: 通过识别和掩码检索头，计算损失差异来筛选数据，并将其应用于1.3B参数模型，从241B token中选出73B token用于训练7B参数模型。

Result: 在多个知识密集和推理密集型基准测试（如MMLU、GSM8K）上实现了1.4%到3.5%的性能提升，展示了小模型提升大模型性能的有效性。

Conclusion: AttentionInfluence提供了一种可扩展的推理中心数据筛选路径，通过小模型优化大模型性能。

Abstract: Recently, there has been growing interest in collecting reasoning-intensive
pretraining data to improve LLMs' complex reasoning ability. Prior approaches
typically rely on supervised classifiers to identify such data, which requires
labeling by humans or LLMs, often introducing domain-specific biases. Due to
the attention heads being crucial to in-context reasoning, we propose
AttentionInfluence, a simple yet effective, training-free method without
supervision signal. Our approach enables a small pretrained language model to
act as a strong data selector through a simple attention head masking
operation. Specifically, we identify retrieval heads and compute the loss
difference when masking these heads. We apply AttentionInfluence to a
1.3B-parameter dense model to conduct data selection on the SmolLM corpus of
241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B
tokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD
learning rate scheduling. Our experimental results demonstrate substantial
improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive
and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and
HumanEval). This demonstrates an effective weak-to-strong scaling property,
with small models improving the final performance of larger models-offering a
promising and scalable path for reasoning-centric data selection.

</details>


### [39] [Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study](https://arxiv.org/abs/2505.07313)
*Baixuan Xu,Chunyang Li,Weiqi Wang,Wei Fan,Tianshi Zheng,Haochen Shi,Tao Fan,Yangqiu Song,Qiang Yang*

Main category: cs.CL

TL;DR: 该研究系统探讨了多智能体LLM系统中协作结构设计对集体推理性能的影响，关注专家领域对齐、协作范式（结构化工作流与多样性驱动整合）和系统规模三个关键维度。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统的有效协作结构设计对增强集体推理至关重要，但目前研究不足。本文旨在填补这一空白，为配置专业化多智能体系统提供指导。

Method: 通过实验研究三个设计维度对协作推理性能的影响：专家领域对齐、协作范式（结构化工作流 vs. 多样性驱动整合）和系统规模。

Result: 专家领域对齐的效益高度依赖任务领域；多样性驱动的协作范式优于僵化的任务分解；系统规模扩展带来计算效率与专业化的权衡。

Conclusion: 研究为配置多智能体系统提供了具体指导，并指出了扩展性多智能体推理中的关键架构权衡和瓶颈，需设计更高效的通信协议。

Abstract: Designing effective collaboration structure for multi-agent LLM systems to
enhance collective reasoning is crucial yet remains under-explored. In this
paper, we systematically investigate how collaborative reasoning performance is
affected by three key design dimensions: (1) Expertise-Domain Alignment, (2)
Collaboration Paradigm (structured workflow vs. diversity-driven integration),
and (3) System Scale. Our findings reveal that expertise alignment benefits are
highly domain-contingent, proving most effective for contextual reasoning
tasks. Furthermore, collaboration focused on integrating diverse knowledge
consistently outperforms rigid task decomposition. Finally, we empirically
explore the impact of scaling the multi-agent system with expertise
specialization and study the computational trade off, highlighting the need for
more efficient communication protocol design. This work provides concrete
guidelines for configuring specialized multi-agent system and identifies
critical architectural trade-offs and bottlenecks for scalable multi-agent
reasoning. The code will be made available upon acceptance.

</details>


### [40] [QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions in Korean Search Engines](https://arxiv.org/abs/2505.07345)
*Ohjoon Kwon,Changsu Lee,Jihye Back,Lim Sun Suk,Inho Kang,Donghyeon Jeon*

Main category: cs.CL

TL;DR: QUPID结合两种小型语言模型（生成式和嵌入式），在信息检索中比大型语言模型更高效且准确，推理速度快60倍，nDCG@5提升1.9%。


<details>
  <summary>Details</summary>
Motivation: 研究探索如何通过结合不同架构的小型语言模型（SLMs），在保证性能的同时降低计算成本，取代大型语言模型（LLMs）在信息检索中的主导地位。

Method: 提出QUPID方法，整合生成式SLM和嵌入式SLM，利用架构多样性提升相关性判断准确率，同时减少计算开销。

Result: 实验显示QUPID的Cohen's Kappa达0.646（LLMs为0.387），推理速度快60倍；生产环境中nDCG@5提升1.9%。

Conclusion: 通过SLM组合的架构多样性，QUPID在检索相关性和运行效率上显著优于LLMs，适合大规模实际应用。

Abstract: Large language models (LLMs) have been widely used for relevance assessment
in information retrieval. However, our study demonstrates that combining two
distinct small language models (SLMs) with different architectures can
outperform LLMs in this task. Our approach -- QUPID -- integrates a generative
SLM with an embedding-based SLM, achieving higher relevance judgment accuracy
while reducing computational costs compared to state-of-the-art LLM solutions.
This computational efficiency makes QUPID highly scalable for real-world search
systems processing millions of queries daily. In experiments across diverse
document types, our method demonstrated consistent performance improvements
(Cohen's Kappa of 0.646 versus 0.387 for leading LLMs) while offering 60x
faster inference times. Furthermore, when integrated into production search
pipelines, QUPID improved nDCG@5 scores by 1.9%. These findings underscore how
architectural diversity in model combinations can significantly enhance both
search relevance and operational efficiency in information retrieval systems.

</details>


### [41] [Computational Fact-Checking of Online Discourse: Scoring scientific accuracy in climate change related news articles](https://arxiv.org/abs/2505.07409)
*Tim Wittenborg,Constantin Sebastian Tremel,Markus Stocker,Sören Auer*

Main category: cs.CL

TL;DR: 该论文提出了一种半自动化方法，使用LLM和知识图谱分析来量化在线媒体的科学准确性，但工具在粒度与规模上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 民主社会需要可靠信息，但公民缺乏验证海量内容的能力，因此需要一种方法来衡量在线媒体的科学准确性。

Method: 使用基于LLM的声明提取和知识图谱分析的神经符号系统，对比未知来源与可信来源的语义化内容。

Result: 系统能显著提升真实性量化效率，但无法满足公共媒体所需的精细化和规模化标注需求。

Conclusion: 需进一步开发FAIR标准的基础数据和补充指标，以科学支持公共讨论。

Abstract: Democratic societies need reliable information. Misinformation in popular
media such as news articles or videos threatens to impair civic discourse.
Citizens are, unfortunately, not equipped to verify this content flood consumed
daily at increasing rates. This work aims to semi-automatically quantify
scientific accuracy of online media. By semantifying media of unknown veracity,
their statements can be compared against equally processed trusted sources. We
implemented a workflow using LLM-based statement extraction and knowledge graph
analysis. Our neurosymbolic system was able to evidently streamline
state-of-the-art veracity quantification. Evaluated via expert interviews and a
user survey, the tool provides a beneficial veracity indication. This
indicator, however, is unable to annotate public media at the required
granularity and scale. Further work towards a FAIR (Findable, Accessible,
Interoperable, Reusable) ground truth and complementary metrics are required to
scientifically support civic discourse.

</details>


### [42] [ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation](https://arxiv.org/abs/2505.07416)
*Truc Mai-Thanh Nguyen,Dat Minh Nguyen,Son T. Luu,Kiet Van Nguyen*

Main category: cs.CL

TL;DR: ViMRHP是越南语多模态评论有用性预测的大规模基准数据集，覆盖4个领域，含2K产品和46K评论。借助AI辅助标注，时间减少65%且质量有保障，但复杂任务仍有限制。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要针对英语和印尼语，越南语等低资源语言缺乏多样性。ViMRHP填补了这一空白，旨在提升推荐系统中的用户体验和决策支持。

Method: 通过AI辅助人工标注构建ViMRHP数据集，对比人工与AI标注的质量差异，并评估基线模型表现。

Result: 标注时间从90-120秒/任务降至20-40秒/任务，整体成本降低65%。AI在复杂任务中表现有限，但数据质量仍可保障。

Conclusion: ViMRHP为越南语MRHP任务提供了首个大规模数据集，AI辅助标注高效且经济，但复杂场景需进一步优化。

Abstract: Multimodal Review Helpfulness Prediction (MRHP) is an essential task in
recommender systems, particularly in E-commerce platforms. Determining the
helpfulness of user-generated reviews enhances user experience and improves
consumer decision-making. However, existing datasets focus predominantly on
English and Indonesian, resulting in a lack of linguistic diversity, especially
for low-resource languages such as Vietnamese. In this paper, we introduce
ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale
benchmark dataset for MRHP task in Vietnamese. This dataset covers four
domains, including 2K products with 46K reviews. Meanwhile, a large-scale
dataset requires considerable time and cost. To optimize the annotation
process, we leverage AI to assist annotators in constructing the ViMRHP
dataset. With AI assistance, annotation time is reduced (90 to 120 seconds per
task down to 20 to 40 seconds per task) while maintaining data quality and
lowering overall costs by approximately 65%. However, AI-generated annotations
still have limitations in complex annotation tasks, which we further examine
through a detailed performance analysis. In our experiment on ViMRHP, we
evaluate baseline models on human-verified and AI-generated annotations to
assess their quality differences. The ViMRHP dataset is publicly available at
https://github.com/trng28/ViMRHP

</details>


### [43] [Comparative sentiment analysis of public perception: Monkeypox vs. COVID-19 behavioral insights](https://arxiv.org/abs/2505.07430)
*Mostafa Mohaimen Akand Faisal,Rabeya Amin Jhuma*

Main category: cs.CL

TL;DR: 论文通过比较COVID-19和Mpox的公众情绪分析，揭示了公众情绪差异及其对公共卫生策略的启示。


<details>
  <summary>Details</summary>
Motivation: 全球健康危机如COVID-19和Mpox的出现凸显了理解公众情绪的重要性，以便制定有效的公共卫生策略。

Method: 研究利用Logistic回归、朴素贝叶斯、RoBERTa、DistilRoBERTa和XLNet等机器学习模型，对147,475条COVID-19推文和106,638条Mpox推文进行了情感分类。

Result: 分析发现公众情绪因疾病特征、媒体表现和疫情疲劳等因素存在显著差异，并提出了公共卫生信息传递的改进建议。

Conclusion: 研究为公共卫生信息学的情绪分析应用提供了新方向，为未来实时监控和多语言分析奠定了基础。

Abstract: The emergence of global health crises, such as COVID-19 and Monkeypox (mpox),
has underscored the importance of understanding public sentiment to inform
effective public health strategies. This study conducts a comparative sentiment
analysis of public perceptions surrounding COVID-19 and mpox by leveraging
extensive datasets of 147,475 and 106,638 tweets, respectively. Advanced
machine learning models, including Logistic Regression, Naive Bayes, RoBERTa,
DistilRoBERTa and XLNet, were applied to perform sentiment classification, with
results indicating key trends in public emotion and discourse. The analysis
highlights significant differences in public sentiment driven by disease
characteristics, media representation, and pandemic fatigue. Through the lens
of sentiment polarity and thematic trends, this study offers valuable insights
into tailoring public health messaging, mitigating misinformation, and
fostering trust during concurrent health crises. The findings contribute to
advancing sentiment analysis applications in public health informatics, setting
the groundwork for enhanced real-time monitoring and multilingual analysis in
future research.

</details>


### [44] [Matching Tasks with Industry Groups for Augmenting Commonsense Knowledge](https://arxiv.org/abs/2505.07440)
*Rituraj Singh,Sachin Pawar,Girish Palshikar*

Main category: cs.CL

TL;DR: 提出弱监督框架增强常识知识库,以填补行业任务与行业群组间的关系空白


<details>
  <summary>Details</summary>
Motivation: 现有常识知识库(如ConceptNet)缺乏特定行业任务的专业知识,需补充行业群组与任务间的关系

Method: 训练神经模型学习任务与行业群组之间的关联,并通过聚类为每个行业群组选择top-k任务

Result: 从新闻数据集中提取2339个三元组(行业群组,is capable of,任务),精度达0.86

Conclusion: 提取的任务与行业群组关系可靠,可直接补充至现有知识库

Abstract: Commonsense knowledge bases (KB) are a source of specialized knowledge that
is widely used to improve machine learning applications. However, even for a
large KB such as ConceptNet, capturing explicit knowledge from each industry
domain is challenging. For example, only a few samples of general {\em tasks}
performed by various industries are available in ConceptNet. Here, a task is a
well-defined knowledge-based volitional action to achieve a particular goal. In
this paper, we aim to fill this gap and present a weakly-supervised framework
to augment commonsense KB with tasks carried out by various industry groups
(IG). We attempt to {\em match} each task with one or more suitable IGs by
training a neural model to learn task-IG affinity and apply clustering to
select the top-k tasks per IG. We extract a total of 2339 triples of the form
$\langle IG, is~capable~of, task \rangle$ from two publicly available news
datasets for 24 IGs with the precision of 0.86. This validates the reliability
of the extracted task-IG pairs that can be directly added to existing KBs.

</details>


### [45] [Translating the Grievance Dictionary: a psychometric evaluation of Dutch, German, and Italian versions](https://arxiv.org/abs/2505.07495)
*Isabelle van der Vegt,Bennett Kleinberg,Marilu Miotto,Jonas Festor*

Main category: cs.CL

TL;DR: 论文介绍了三种Grievance Dictionary的翻译版本（荷兰语、德语、意大利语），评估了其心理测量学表现，并提出了未来翻译和验证的建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了扩展Grievance Dictionary在英语以外的语言中的应用，尤其是针对暴力、威胁或怨愤相关文本的分析。

Method: 方法包括自动化翻译结合人工标注，并进行心理测量学分析（如内部可靠性和与LIWC词典的相关性）。

Result: 结果显示荷兰语和德语翻译版本与原始英语版本表现相似，而意大利语版本在某些类别上可靠性较低。

Conclusion: 结论提出了进一步验证和应用该词典的建议，以及未来类似翻译方法的改进方向。

Abstract: This paper introduces and evaluates three translations of the Grievance
Dictionary, a psycholinguistic dictionary for the analysis of violent,
threatening or grievance-fuelled texts. Considering the relevance of these
themes in languages beyond English, we translated the Grievance Dictionary to
Dutch, German, and Italian. We describe the process of automated translation
supplemented by human annotation. Psychometric analyses are performed,
including internal reliability of dictionary categories and correlations with
the LIWC dictionary. The Dutch and German translations perform similarly to the
original English version, whereas the Italian dictionary shows low reliability
for some categories. Finally, we make suggestions for further validation and
application of the dictionary, as well as for future dictionary translations
following a similar approach.

</details>


### [46] [ToolACE-DEV: Self-Improving Tool Learning via Decomposition and EVolution](https://arxiv.org/abs/2505.07512)
*Xu Huang,Weiwen Liu,Xingshan Zeng,Yuefeng Huang,Xinlong Hao,Yuxian Wang,Yirong Zeng,Chuhan Wu,Yasheng Wang,Ruiming Tang,Defu Lian*

Main category: cs.CL

TL;DR: 本文提出了一个名为ToolACE-DEV的自改进框架，用于减少工具学习中高级语言模型的使用依赖和数据兼容问题，通过分解任务和引入自进化策略。


<details>
  <summary>Details</summary>
Motivation: 当前工具学习主要依赖高级模型的数据合成，成本高且存在数据兼容问题。

Method: 将工具学习目标分解为子任务，引入自进化范式，减少对高级模型的依赖。

Result: 实验表明该方法在不同规模和架构的模型上均有效。

Conclusion: ToolACE-DEV提供了一种高效低成本的工具学习方案。

Abstract: The tool-using capability of large language models (LLMs) enables them to
access up-to-date external information and handle complex tasks. Current
approaches to enhancing this capability primarily rely on distilling advanced
models by data synthesis. However, this method incurs significant costs
associated with advanced model usage and often results in data compatibility
issues, led by the high discrepancy in the knowledge scope between the advanced
model and the target model. To address these challenges, we propose
ToolACE-DEV, a self-improving framework for tool learning. First, we decompose
the tool-learning objective into sub-tasks that enhance basic tool-making and
tool-using abilities. Then, we introduce a self-evolving paradigm that allows
lightweight models to self-improve, reducing reliance on advanced LLMs.
Extensive experiments validate the effectiveness of our approach across models
of varying scales and architectures.

</details>


### [47] [SEReDeEP: Hallucination Detection in Retrieval-Augmented Models via Semantic Entropy and Context-Parameter Fusion](https://arxiv.org/abs/2505.07528)
*Lei Wang*

Main category: cs.CL

TL;DR: SEReDeEP是一种改进的方法，通过语义熵增强幻觉检测，解决现有方法在语义维度上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法（如ReDeEP）在语义维度上表现不一致，无法充分反映模型响应的真实性，因此需要一种更准确的评估方法。

Method: SEReDeEP利用训练好的线性探针捕获语义熵，改进计算过程，从而更准确地评估幻觉现象。

Result: SEReDeEP在幻觉检测上更接近人工评估的真实结果，显著提升了评估的准确性。

Conclusion: SEReDeEP通过语义熵增强，为检索增强生成模型提供了更可靠的幻觉检测方法。

Abstract: Retrieval-Augmented Generation (RAG) models frequently encounter
hallucination phenomena when integrating external information with internal
parametric knowledge. Empirical studies demonstrate that the disequilibrium
between external contextual information and internal parametric knowledge
constitutes a primary factor in hallucination generation. Existing
hallucination detection methodologies predominantly emphasize either the
external or internal mechanism in isolation, thereby overlooking their
synergistic effects. The recently proposed ReDeEP framework decouples these
dual mechanisms, identifying two critical contributors to hallucinations:
excessive reliance on parametric knowledge encoded in feed-forward networks
(FFN) and insufficient utilization of external information by attention
mechanisms (particularly copy heads). ReDeEP quantitatively assesses these
factors to detect hallucinations and dynamically modulates the contributions of
FFNs and copy heads to attenuate their occurrence. Nevertheless, ReDeEP and
numerous other hallucination detection approaches have been employed at
logit-level uncertainty estimation or language-level self-consistency
evaluation, inadequately address the semantic dimensions of model responses,
resulting in inconsistent hallucination assessments in RAG implementations.
Building upon ReDeEP's foundation, this paper introduces SEReDeEP, which
enhances computational processes through semantic entropy captured via trained
linear probes, thereby achieving hallucination assessments that more accurately
reflect ground truth evaluations.

</details>


### [48] [A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models](https://arxiv.org/abs/2505.07591)
*Junjie Ye,Caishuang Huang,Zhuohan Chen,Wenjie Fu,Chenyuan Yang,Leyi Yang,Yilong Wu,Peng Wang,Meng Zhou,Xiaolong Yang,Tao Gui,Qi Zhang,Zhongchao Shi,Jianping Fan,Xuanjing Huang*

Main category: cs.CL

TL;DR: 该论文提出了一个多维度约束框架和自动化指令生成流程，用于评估大型语言模型（LLMs）在指令跟随任务中的表现，发现不同约束形式的性能差异显著，并通过强化学习数据生成提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试依赖模板化的约束提示，缺乏现实应用的多样性且难以进行细粒度评估，因此需要更全面的评估方法。

Method: 设计了一个包含三种约束模式、四类约束和四种难度级别的框架，并通过自动化流程生成了1200个可验证的测试样本。评估了19个不同家族的LLMs。

Result: 性能从Level I的77.67%降至Level IV的32.96%，通过强化学习生成数据显著提升了指令跟随能力而不影响通用性能。

Conclusion: 多维度约束框架有效揭示了LLMs的局限性，其生成的强化学习数据可优化模型性能，为未来研究提供了实用工具和数据支持。

Abstract: Instruction following evaluates large language models (LLMs) on their ability
to generate outputs that adhere to user-defined constraints. However, existing
benchmarks often rely on templated constraint prompts, which lack the diversity
of real-world usage and limit fine-grained performance assessment. To fill this
gap, we propose a multi-dimensional constraint framework encompassing three
constraint patterns, four constraint categories, and four difficulty levels.
Building on this framework, we develop an automated instruction generation
pipeline that performs constraint expansion, conflict detection, and
instruction rewriting, yielding 1,200 code-verifiable instruction-following
test samples. We evaluate 19 LLMs across seven model families and uncover
substantial variation in performance across constraint forms. For instance,
average performance drops from 77.67% at Level I to 32.96% at Level IV.
Furthermore, we demonstrate the utility of our approach by using it to generate
data for reinforcement learning, achieving substantial gains in instruction
following without degrading general performance. In-depth analysis indicates
that these gains stem primarily from modifications in the model's attention
modules parameters, which enhance constraint recognition and adherence. Code
and data are available in https://github.com/Junjie-Ye/MulDimIF.

</details>


### [49] [Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent](https://arxiv.org/abs/2505.07596)
*Ziyang Huang,Xiaowei Yuan,Yiming Ju,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 论文提出了一种名为IKEA的智能检索代理，通过强化学习优化内部知识与外部检索的协同，减少冗余检索并提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法在LLMs中常导致冗余检索、知识冲突和延迟，亟需一种能动态判断检索时机并协同内外知识的解决方案。

Method: 采用强化学习框架，设计了知识边界感知的奖励函数和训练数据集，激励模型优先使用内部知识，仅在必要时进行外部检索。

Result: 在多项知识推理任务中，IKEA显著优于基线方法，减少了检索频率并展现出强泛化能力。

Conclusion: IKEA通过内外知识协同机制，有效平衡了检索效率与准确性，为LLMs的检索增强生成提供了新思路。

Abstract: Retrieval-augmented generation (RAG) is a common strategy to reduce
hallucinations in Large Language Models (LLMs). While reinforcement learning
(RL) can enable LLMs to act as search agents by activating retrieval
capabilities, existing ones often underutilize their internal knowledge. This
can lead to redundant retrievals, potential harmful knowledge conflicts, and
increased inference latency. To address these limitations, an efficient and
adaptive search agent capable of discerning optimal retrieval timing and
synergistically integrating parametric (internal) and retrieved (external)
knowledge is in urgent need. This paper introduces the Reinforced
Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could
indentify its own knowledge boundary and prioritize the utilization of internal
knowledge, resorting to external search only when internal knowledge is deemed
insufficient. This is achieved using a novel knowledge-boundary aware reward
function and a knowledge-boundary aware training dataset. These are designed
for internal-external knowledge synergy oriented RL, incentivizing the model to
deliver accurate answers, minimize unnecessary retrievals, and encourage
appropriate external searches when its own knowledge is lacking. Evaluations
across multiple knowledge reasoning tasks demonstrate that IKEA significantly
outperforms baseline methods, reduces retrieval frequency significantly, and
exhibits robust generalization capabilities.

</details>


### [50] [Characterizing the Investigative Methods of Fictional Detectives with Large Language Models](https://arxiv.org/abs/2505.07601)
*Edirlei Soares de Lima,Marco A. Casanova,Bruno Feijó,Antonio L. Furtado*

Main category: cs.CL

TL;DR: 本文提出了一种AI驱动的方法，系统地分析虚构侦探的调查方法，通过15个大型语言模型（LLM）提取、综合并验证其独特特征，测试准确性达91.43%。


<details>
  <summary>Details</summary>
Motivation: 传统文学研究对虚构侦探的分析局限于少数角色，缺乏可扩展性，本文旨在提供一种可扩展的框架用于计算叙事学中的角色分析。

Method: 使用15个LLM的多阶段工作流程，对7位标志性侦探的调查风格进行提取、综合和验证。

Result: 方法在反向识别阶段达到91.43%的准确率，证明了其有效性。

Conclusion: 该研究为计算叙事学提供可扩展的角色分析框架，可应用于AI驱动的交互式叙事和自动故事生成。

Abstract: Detective fiction, a genre defined by its complex narrative structures and
character-driven storytelling, presents unique challenges for computational
narratology, a research field focused on integrating literary theory into
automated narrative generation. While traditional literary studies have offered
deep insights into the methods and archetypes of fictional detectives, these
analyses often focus on a limited number of characters and lack the scalability
needed for the extraction of unique traits that can be used to guide narrative
generation methods. In this paper, we present an AI-driven approach for
systematically characterizing the investigative methods of fictional
detectives. Our multi-phase workflow explores the capabilities of 15 Large
Language Models (LLMs) to extract, synthesize, and validate distinctive
investigative traits of fictional detectives. This approach was tested on a
diverse set of seven iconic detectives - Hercule Poirot, Sherlock Holmes,
William Murdoch, Columbo, Father Brown, Miss Marple, and Auguste Dupin -
capturing the distinctive investigative styles that define each character. The
identified traits were validated against existing literary analyses and further
tested in a reverse identification phase, achieving an overall accuracy of
91.43%, demonstrating the method's effectiveness in capturing the distinctive
investigative approaches of each detective. This work contributes to the
broader field of computational narratology by providing a scalable framework
for character analysis, with potential applications in AI-driven interactive
storytelling and automated narrative generation.

</details>


### [51] [MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining](https://arxiv.org/abs/2505.07608)
*Xiaomi LLM-Core Team,:,Bingquan Xia,Bowen Shen,Cici,Dawei Zhu,Di Zhang,Gang Wang,Hailin Zhang,Huaqiu Liu,Jiebao Xiao,Jinhao Dong,Liang Zhao,Peidian Li,Peng Wang,Shihua Yu,Shimao Chen,Weikun Wang,Wenhan Ma,Xiangwei Deng,Yi Huang,Yifan Song,Zihan Jiang,Bowen Ye,Can Cai,Chenhong He,Dong Zhang,Duo Zhang,Guoan Wang,Hao Tian,Haochen Zhao,Heng Qu,Hongshen Xu,Jun Shi,Kainan Bao,QingKai Fang,Kang Zhou,Kangyang Zhou,Lei Li,Menghang Zhu,Nuo Chen,Qiantong Wang,Shaohui Liu,Shicheng Li,Shuhao Gu,Shuhuai Ren,Shuo Liu,Sirui Deng,Weiji Zhuang,Weiwei Lv,Wenyu Yang,Xin Zhang,Xing Yong,Xing Zhang,Xingchen Song,Xinzhe Xu,Xu Wang,Yihan Yan,Yu Tu,Yuanyuan Tian,Yudong Wang,Yue Yu,Zhenru Lin,Zhichao Song,Zihao Yue*

Main category: cs.CL

TL;DR: MiMo-7B是一个专注于推理任务的大型语言模型，通过优化预训练和后训练阶段提升性能，最终在数学、编程和通用推理任务上超越更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 旨在构建一个在推理任务上表现优异的高效语言模型，同时通过优化训练策略解决稀疏奖励和训练稳定性问题。

Method: 预训练阶段采用增强的数据预处理和三阶段数据混合策略，并结合多令牌预测目标；后训练阶段使用13万个可验证的数学和编程问题，结合测试难度驱动的代码奖励方案和策略性数据重采样。

Result: MiMo-7B在推理能力上表现出色，优于32B规模的模型，最终RL调优版本在数学、代码和通用推理任务上超越OpenAI o1-mini。

Conclusion: MiMo-7B通过创新的训练策略和优化技术，证明了其在复杂推理任务上的高效性和竞争力。

Abstract: We present MiMo-7B, a large language model born for reasoning tasks, with
optimization across both pre-training and post-training stages. During
pre-training, we enhance the data preprocessing pipeline and employ a
three-stage data mixing strategy to strengthen the base model's reasoning
potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional
Multi-Token Prediction objective for enhanced performance and accelerated
inference speed. During post-training, we curate a dataset of 130K verifiable
mathematics and programming problems for reinforcement learning, integrating a
test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and
employing strategic data resampling to stabilize training. Extensive
evaluations show that MiMo-7B-Base possesses exceptional reasoning potential,
outperforming even much larger 32B models. The final RL-tuned model,
MiMo-7B-RL, achieves superior performance on mathematics, code and general
reasoning tasks, surpassing the performance of OpenAI o1-mini. The model
checkpoints are available at https://github.com/xiaomimimo/MiMo.

</details>


### [52] [Concept-Level Explainability for Auditing & Steering LLM Responses](https://arxiv.org/abs/2505.07610)
*Kenza Amara,Rita Sevastjanova,Mennatallah El-Assady*

Main category: cs.CL

TL;DR: ConceptX是一种模型无关的概念级可解释性方法，通过语义相似度识别提示中的概念并分配重要性，优于词级方法，提升LLM的安全性和对齐性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的广泛部署，其安全性和对齐性成为关注点。当前词级归因方法在文本生成中解释能力有限，无法捕捉输出的底层语义。

Method: 提出ConceptX方法，通过语义相似度分析提示中的概念（语义丰富的词），支持上下文完整性保留和灵活的解释目标（如性别偏见）。

Result: 在三个LLM上，ConceptX在忠实度和人工对齐性上优于TokenSHAP等词级方法；通过提示调整，情感偏移提升至0.252，攻击成功率从0.463降至0.242。

Conclusion: ConceptX为LLM安全性和对齐性提供了透明且忠实的解决方案，展示了归因解释在引导LLM行为中的实用价值。

Abstract: As large language models (LLMs) become widely deployed, concerns about their
safety and alignment grow. An approach to steer LLM behavior, such as
mitigating biases or defending against jailbreaks, is to identify which parts
of a prompt influence specific aspects of the model's output. Token-level
attribution methods offer a promising solution, but still struggle in text
generation, explaining the presence of each token in the output separately,
rather than the underlying semantics of the entire LLM response. We introduce
ConceptX, a model-agnostic, concept-level explainability method that identifies
the concepts, i.e., semantically rich tokens in the prompt, and assigns them
importance based on the outputs' semantic similarity. Unlike current
token-level methods, ConceptX also offers to preserve context integrity through
in-place token replacements and supports flexible explanation goals, e.g.,
gender bias. ConceptX enables both auditing, by uncovering sources of bias, and
steering, by modifying prompts to shift the sentiment or reduce the harmfulness
of LLM responses, without requiring retraining. Across three LLMs, ConceptX
outperforms token-level methods like TokenSHAP in both faithfulness and human
alignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for
random edits and lower attack success rates from 0.463 to 0.242, outperforming
attribution and paraphrasing baselines. While prompt engineering and
self-explaining methods sometimes yield safer responses, ConceptX offers a
transparent and faithful alternative for improving LLM safety and alignment,
demonstrating the practical value of attribution-based explainability in
guiding LLM behavior.

</details>


### [53] [Chronocept: Instilling a Sense of Time in Machines](https://arxiv.org/abs/2505.07637)
*Krish Goel,Sanskar Pandey,KS Mahadevan,Harsh Kumar,Vishesh Khadaria*

Main category: cs.CL

TL;DR: 该论文提出了Chronocept基准，首次将时间有效性建模为连续概率分布，通过偏态正态曲线捕获知识的出现、衰减和峰值相关性，解决了AI在时间推理上的不足。


<details>
  <summary>Details</summary>
Motivation: AI在时间推理方面存在不足，人类能够感知时间并判断知识的时效性，而AI在这方面的能力有限。

Method: 使用偏态正态曲线拟合语义分解的时间轴，建模时间有效性的连续概率分布，并提供两个基准数据集（原子事实和多句段落）。

Result: 标注数据的一致性高（84%和89%），基准方法在预测曲线参数（位置、尺度和偏度）上优于基于分类的方法。

Conclusion: Chronocept填补了AI时间推理的基础空白，支持知识落地、事实核查、检索增强生成和主动代理等应用。

Abstract: Human cognition is deeply intertwined with a sense of time, known as
Chronoception. This sense allows us to judge how long facts remain valid and
when knowledge becomes outdated. Despite progress in vision, language, and
motor control, AI still struggles to reason about temporal validity. We
introduce Chronocept, the first benchmark to model temporal validity as a
continuous probability distribution over time. Using skew-normal curves fitted
along semantically decomposed temporal axes, Chronocept captures nuanced
patterns of emergence, decay, and peak relevance. It includes two datasets:
Benchmark I (atomic facts) and Benchmark II (multi-sentence passages).
Annotations show strong inter-annotator agreement (84% and 89%). Our baselines
predict curve parameters - location, scale, and skewness - enabling
interpretable, generalizable learning and outperforming classification-based
approaches. Chronocept fills a foundational gap in AI's temporal reasoning,
supporting applications in knowledge grounding, fact-checking,
retrieval-augmented generation (RAG), and proactive agents. Code and data are
publicly available.

</details>


### [54] [JobHop: A Large-Scale Dataset of Career Trajectories](https://arxiv.org/abs/2505.07653)
*Iman Johary,Raphael Romero,Alexandru C. Mara,Tijl De Bie*

Main category: cs.CL

TL;DR: 论文介绍了JobHop数据集，一个从比利时弗兰德斯公共就业服务提供的匿名简历中提取的大规模数据集，通过LLM和多标签分类模型处理，包含230万工作经验记录，可用于劳动力市场研究。


<details>
  <summary>Details</summary>
Motivation: 劳动力市场动态的研究需要全面的数据集，但现实中的职业轨迹数据稀缺。JobHop的创建填补了这一空白，为政策制定者和研究者提供了宝贵资源。

Method: 使用大型语言模型(LLM)处理非结构化简历数据，提取结构化职业信息，并通过多标签分类模型映射到标准化ESCO职业代码。

Result: 生成了一个包含230万工作经验、39.1万份简历的数据集，支持劳动力市场流动性、职业稳定性等分析。

Conclusion: JobHop数据集为劳动力市场研究提供了新工具，支持数据驱动的决策和职业路径预测。

Abstract: Understanding labor market dynamics is essential for policymakers, employers,
and job seekers. However, comprehensive datasets that capture real-world career
trajectories are scarce. In this paper, we introduce JobHop, a large-scale
public dataset derived from anonymized resumes provided by VDAB, the public
employment service in Flanders, Belgium. Utilizing Large Language Models
(LLMs), we process unstructured resume data to extract structured career
information, which is then mapped to standardized ESCO occupation codes using a
multi-label classification model. This results in a rich dataset of over 2.3
million work experiences, extracted from and grouped into more than 391,000
user resumes and mapped to standardized ESCO occupation codes, offering
valuable insights into real-world occupational transitions. This dataset
enables diverse applications, such as analyzing labor market mobility, job
stability, and the effects of career breaks on occupational transitions. It
also supports career path prediction and other data-driven decision-making
processes. To illustrate its potential, we explore key dataset characteristics,
including job distributions, career breaks, and job transitions, demonstrating
its value for advancing labor market research.

</details>


### [55] [Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent](https://arxiv.org/abs/2505.07659)
*Ethan Gotlieb Wilcox,Cui Ding,Giovanni Acampa,Tiago Pimentel,Alex Warstadt,Tamar I. Regev*

Main category: cs.CL

TL;DR: 语言中词汇身份与韵律（如音高）的关系可通过信息论分析。研究发现，声调语言中音高与词汇的互信息更高，支持语言类型学是渐变的观点。


<details>
  <summary>Details</summary>
Motivation: 探讨词汇身份与韵律（如音高）的关系是否能用信息论量化，并验证声调语言中两者的互信息更高。

Method: 使用十种语言的语音数据，通过计算文本与音高曲线的互信息，对比声调、音高重音和重音语言的表现。

Result: 声调语言的音高曲线与词汇的互信息显著高于非声调语言，支持假设。所有语言的音高熵相似。

Conclusion: 研究支持语言类型学的渐变视角，声调语言通过音高区分词汇的能力更强。

Abstract: This paper argues that the relationship between lexical identity and prosody
-- one well-studied parameter of linguistic variation -- can be characterized
using information theory. We predict that languages that use prosody to make
lexical distinctions should exhibit a higher mutual information between word
identity and prosody, compared to languages that don't. We test this hypothesis
in the domain of pitch, which is used to make lexical distinctions in tonal
languages, like Cantonese. We use a dataset of speakers reading sentences aloud
in ten languages across five language families to estimate the mutual
information between the text and their pitch curves. We find that, across
languages, pitch curves display similar amounts of entropy. However, these
curves are easier to predict given their associated text in the tonal
languages, compared to pitch- and stress-accent languages, and thus the mutual
information is higher in these languages, supporting our hypothesis. Our
results support perspectives that view linguistic typology as gradient, rather
than categorical.

</details>


### [56] [Benchmarking Retrieval-Augmented Generation for Chemistry](https://arxiv.org/abs/2505.07671)
*Xianrui Zhong,Bowen Jin,Siru Ouyang,Yanzhen Shen,Qiao Jin,Yin Fang,Zhiyong Lu,Jiawei Han*

Main category: cs.CL

TL;DR: 该论文介绍了ChemRAG-Bench，一个用于评估化学领域RAG效果的基准，以及ChemRAG-Toolkit，一个支持多种检索算法和LLM的模块化工具包。实验显示RAG比直接推理方法性能提升17.4%。


<details>
  <summary>Details</summary>
Motivation: 化学领域缺乏高质量的专业语料和评估基准，限制了RAG的应用。

Method: 提出ChemRAG-Bench基准和ChemRAG-Toolkit工具包，整合多种化学知识源，支持多种检索算法和LLM。

Result: RAG方法平均相对性能提升17.4%，并对检索架构、语料选择等进行了深入分析。

Conclusion: ChemRAG为化学领域的RAG研究和部署提供了实用指导和工具。

Abstract: Retrieval-augmented generation (RAG) has emerged as a powerful framework for
enhancing large language models (LLMs) with external knowledge, particularly in
scientific domains that demand specialized and dynamic information. Despite its
promise, the application of RAG in the chemistry domain remains underexplored,
primarily due to the lack of high-quality, domain-specific corpora and
well-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a
comprehensive benchmark designed to systematically assess the effectiveness of
RAG across a diverse set of chemistry-related tasks. The accompanying chemistry
corpus integrates heterogeneous knowledge sources, including scientific
literature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia
entries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG
toolkit that supports five retrieval algorithms and eight LLMs. Using
ChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain
-- achieving an average relative improvement of 17.4% over direct inference
methods. We further conduct in-depth analyses on retriever architectures,
corpus selection, and the number of retrieved passages, culminating in
practical recommendations to guide future research and deployment of RAG
systems in the chemistry domain. The code and data is available at
https://chemrag.github.io.

</details>


### [57] [OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit](https://arxiv.org/abs/2505.07672)
*Arun S. Maiya*

Main category: cs.CL

TL;DR: OnPrem.LLM是一个Python工具包，用于在离线或受限制环境中应用大型语言模型处理敏感数据，支持多种LLM后端和隐私保护用例。


<details>
  <summary>Details</summary>
Motivation: 针对敏感非公开数据，需要在隐私保护的前提下使用LLM，提供本地化解决方案并兼顾灵活性和易用性。

Method: 基于Python的工具包，提供文档处理、检索增强生成（RAG）、信息提取等功能，支持多后端和GPU加速。

Result: 支持多种LLM后端（如llama.cpp、Ollama等），实现本地执行或混合部署，并提供无代码界面。

Conclusion: OnPrem.LLM为隐私敏感场景提供了灵活、高效的LLM应用解决方案，兼顾性能与数据控制。

Abstract: We present OnPrem.LLM, a Python-based toolkit for applying large language
models (LLMs) to sensitive, non-public data in offline or restricted
environments. The system is designed for privacy-preserving use cases and
provides prebuilt pipelines for document processing and storage,
retrieval-augmented generation (RAG), information extraction, summarization,
classification, and prompt/output processing with minimal configuration.
OnPrem.LLM supports multiple LLM backends -- including llama.cpp, Ollama, vLLM,
and Hugging Face Transformers -- with quantized model support, GPU
acceleration, and seamless backend switching. Although designed for fully local
execution, OnPrem.LLM also supports integration with a wide range of cloud LLM
providers when permitted, enabling hybrid deployments that balance performance
with data control. A no-code web interface extends accessibility to
non-technical users.

</details>


### [58] [Codifying Character Logic in Role-Playing](https://arxiv.org/abs/2505.07705)
*Letian Peng,Jingbo Shang*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Codified Profiles的新方法，通过将角色逻辑表示为结构化、可执行的函数来改进角色扮演中的决策行为。相比传统的基于提示的配置文件，这种方法具有持久性、可更新性和可控随机性三大优势，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于提示的角色扮演方法依赖模型的隐式推理，难以保持逻辑的一致性和可更新性。本文旨在通过结构化、可执行的函数显式表示角色逻辑，解决这些问题。

Method: 该方法将角色逻辑编码为函数，如`parse_by_scene(scene)`和`check_condition(scene, question)`，利用显式控制结构和条件检查生成逻辑驱动的行为断言。

Result: 实验基于83个角色和5,141个场景的基准测试，结果表明Codified Profiles显著提升了持久性、可更新性和行为多样性，甚至使1B参数模型也能高效实现高质量角色扮演。

Conclusion: Codified Profiles为角色扮演代理提供了一种可扩展且高效的本地部署方案，通过显式逻辑表示克服了传统方法的局限性。

Abstract: This paper introduces Codified Profiles for role-playing, a novel approach
that represents character logic as structured, executable functions for
behavioral decision-making. Each profile defines a set of functions
parse_by_scene(scene) that outputs a list of logic-grounded assertions
triggered_statements, using both explicit control structures (e.g.,
if-then-else) and condition checks like check_condition(scene, question), where
each question is a semantically meaningful prompt about the scene (e.g., "Is
the character in danger?") discriminated by the role-playing LLM as true,
false, or unknown. This explicit representation offers three key advantages
over traditional prompt-based profiles, which append character descriptions
directly into text prompts: (1) Persistence, by enforcing complete and
consistent execution of character logic, rather than relying on the model's
implicit reasoning; (2) Updatability, through systematic inspection and
revision of behavioral logic, which is difficult to track or debug in
prompt-only approaches; (3) Controllable Randomness, by supporting stochastic
behavior directly within the logic, enabling fine-grained variability that
prompting alone struggles to achieve. To validate these advantages, we
introduce a new benchmark constructed from 83 characters and 5,141 scenes
curated from Fandom, using NLI-based scoring to compare character responses
against ground-truth actions. Our experiments demonstrate the significant
benefits of codified profiles in improving persistence, updatability, and
behavioral diversity. Notably, by offloading a significant portion of reasoning
to preprocessing, codified profiles enable even 1B-parameter models to perform
high-quality role-playing, providing a scalable and efficient foundation for
local deployment of role-play agents.

</details>


### [59] [Spoken Language Understanding on Unseen Tasks With In-Context Learning](https://arxiv.org/abs/2505.07731)
*Neeraj Agrawal,Sriram Ganapathy*

Main category: cs.CL

TL;DR: 本文提出了一种针对语音文本大语言模型（LLMs）的鲁棒性任务无关微调方法，通过随机化类别标签，显著提升了模型在未见任务上的性能，且无需任务特定数据标注。


<details>
  <summary>Details</summary>
Motivation: 传统任务特定的口语理解（SLU）模型无法应对缺乏任务特定训练数据的情况，而现有的开源语音文本LLMs在零/少样本学习下的性能表现不足，亟需改进。

Method: 提出了一种新颖的鲁棒性任务无关微调方法，使用随机化类别标签对语音文本LLMs进行训练，避免了对任务特定标注数据的依赖。

Result: 实验表明，该方法在未见任务上的性能显著优于传统方法，且无需额外的任务特定数据标注。

Conclusion: 随机化类别标签的微调方法为语音文本LLMs在多样化SLU任务中的应用提供了一种高效且任务无关的解决方案。

Abstract: Spoken language understanding (SLU) tasks involve diverse skills that probe
the information extraction, classification and/or generation capabilities of
models. In this setting, task-specific training data may not always be
available. While traditional task-specific SLU models are unable to cater to
such requirements, the speech-text large language models (LLMs) offer a
promising alternative with emergent abilities. However, out of-the-box, our
evaluations indicate that the zero/few-shot performance of prominent
open-source speech-text LLMs on SLU tasks are not up to the mark. In this
paper, we introduce a novel approach to robust task-agnostic fine-tuning using
randomized class labels. With this proposed fine-tuning, we illustrate that the
performance of the speech-text LLMs on an unseen task is significantly improved
over standard approaches. Critically, the proposed approach avoids the
requirement of task-specific data annotations for enabling new tasks in
speech-text LLMs.

</details>


### [60] [Must Read: A Systematic Survey of Computational Persuasion](https://arxiv.org/abs/2505.07775)
*Nimet Beyza Bozdag,Shuhaib Mehri,Xiaocheng Yang,Hyeonjeong Ha,Zirui Cheng,Esin Durmus,Jiaxuan You,Heng Ji,Gokhan Tur,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 本文综述了计算说服的三大视角：AI作为说服者、被说服者和评判者，探讨了AI生成说服内容、易受操纵性及伦理问题，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI驱动的说服技术快速发展，其潜在益处与风险并存，但目前对其效果的理解仍受限于主观性和情景依赖性，亟需系统梳理。

Method: 通过三大视角（AI作为说服者、被说服者、评判者）构建分类法，综述现有研究，并分析关键技术挑战。

Result: 提出了计算说服研究的分类框架，明确了评估说服力、防止操纵性说服等核心挑战。

Conclusion: 需进一步研究以确保AI说服技术的安全性、公平性和有效性，同时应对大语言模型的风险。

Abstract: Persuasion is a fundamental aspect of communication, influencing
decision-making across diverse contexts, from everyday conversations to
high-stakes scenarios such as politics, marketing, and law. The rise of
conversational AI systems has significantly expanded the scope of persuasion,
introducing both opportunities and risks. AI-driven persuasion can be leveraged
for beneficial applications, but also poses threats through manipulation and
unethical influence. Moreover, AI systems are not only persuaders, but also
susceptible to persuasion, making them vulnerable to adversarial attacks and
bias reinforcement. Despite rapid advancements in AI-generated persuasive
content, our understanding of what makes persuasion effective remains limited
due to its inherently subjective and context-dependent nature. In this survey,
we provide a comprehensive overview of computational persuasion, structured
around three key perspectives: (1) AI as a Persuader, which explores
AI-generated persuasive content and its applications; (2) AI as a Persuadee,
which examines AI's susceptibility to influence and manipulation; and (3) AI as
a Persuasion Judge, which analyzes AI's role in evaluating persuasive
strategies, detecting manipulation, and ensuring ethical persuasion. We
introduce a taxonomy for computational persuasion research and discuss key
challenges, including evaluating persuasiveness, mitigating manipulative
persuasion, and developing responsible AI-driven persuasive systems. Our survey
outlines future research directions to enhance the safety, fairness, and
effectiveness of AI-powered persuasion while addressing the risks posed by
increasingly capable language models.

</details>


### [61] [Domain Regeneration: How well do LLMs match syntactic properties of text domains?](https://arxiv.org/abs/2505.07784)
*Da Ju,Hagen Blix,Adina Williams*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）在生成文本时是否能忠实近似其训练数据的分布特性，通过对Wikipedia和新闻文本的再生实验发现，LLM生成的文本在均值、标准差和长尾分布上与原始人类文本存在差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解LLM在生成文本时能多好地近似其训练数据的分布特性，尤其是不同文本领域的语言学属性。

Method: 方法包括从Wikipedia和新闻文本中提取语料，通过LLM再生文本，并对比分析语法抽象层次（如句子长度、可读性）和复杂属性（如依存标签分布、句法深度）。

Result: 结果显示，LLM生成的文本在多数分布中表现出均值偏移、标准差降低和长尾减少的现象。

Conclusion: 结论指出LLM在生成文本时虽然能部分近似人类文本，但在分布特性上仍存在显著差异。

Abstract: Recent improvement in large language model performance have, in all
likelihood, been accompanied by improvement in how well they can approximate
the distribution of their training data. In this work, we explore the following
question: which properties of text domains do LLMs faithfully approximate, and
how well do they do so? Applying observational approaches familiar from corpus
linguistics, we prompt a commonly used, opensource LLM to regenerate text from
two domains of permissively licensed English text which are often contained in
LLM training data -- Wikipedia and news text. This regeneration paradigm allows
us to investigate whether LLMs can faithfully match the original human text
domains in a fairly semantically-controlled setting. We investigate varying
levels of syntactic abstraction, from more simple properties like sentence
length, and article readability, to more complex and higher order properties
such as dependency tag distribution, parse depth, and parse complexity. We find
that the majority of the regenerated distributions show a shifted mean, a lower
standard deviation, and a reduction of the long tail, as compared to the human
originals.

</details>


### [62] [Learning from Peers in Reasoning Models](https://arxiv.org/abs/2505.07787)
*Tongxu Luo,Wenyu Du,Jiaxi Bi,Stephen Chung,Zhengyang Tang,Hao Yang,Min Zhang,Benyou Wang*

Main category: cs.CL

TL;DR: 该论文研究了大型推理模型（LRM）在推理路径开头不佳时难以自我纠正的现象（称为“前缀主导陷阱”），并提出了一种名为“学习同伴”（LeaP）的方法，通过同行互动机制提升模型自我纠错能力。实验证明LeaP显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于发现大型推理模型在推理路径开头不佳时难以自我纠正（前缀主导陷阱）。受心理学中同行互动促进自我纠正的启发，作者旨在通过LeaP方法解决这一问题。

Method: 论文提出了LeaP方法，通过在推理过程中定期总结并共享中间结果（路由机制），让不同推理路径相互学习。针对小模型难以有效执行总结任务的问题，进一步提出了微调版本LeaP-T系列模型。

Result: 实验结果表明，LeaP显著提升性能。例如，QwQ-32B + LeaP在多个数学基准上平均提升5个点，甚至超越DeepSeek-R1-671B（平均增益3.3点）。LeaP-T-7B在AIME 2024上表现与更大的蒸馏模型相当。

Conclusion: LeaP通过同行协作机制实现了大型推理模型的高效自我纠错，展现了强大的错误容忍能力和任务适应性，标志着模型协作推理的重要里程碑。

Abstract: Large Reasoning Models (LRMs) have the ability to self-correct even when they
make mistakes in their reasoning paths. However, our study reveals that when
the reasoning process starts with a short but poor beginning, it becomes
difficult for the model to recover. We refer to this phenomenon as the "Prefix
Dominance Trap". Inspired by psychological findings that peer interaction can
promote self-correction without negatively impacting already accurate
individuals, we propose **Learning from Peers** (LeaP) to address this
phenomenon. Specifically, every tokens, each reasoning path summarizes its
intermediate reasoning and shares it with others through a routing mechanism,
enabling paths to incorporate peer insights during inference. However, we
observe that smaller models sometimes fail to follow summarization and
reflection instructions effectively. To address this, we fine-tune them into
our **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025,
and GPQA Diamond show that LeaP provides substantial improvements. For
instance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the
baseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks
with an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches
the performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis
reveals LeaP's robust error correction by timely peer insights, showing strong
error tolerance and handling varied task difficulty. LeaP marks a milestone by
enabling LRMs to collaborate during reasoning. Our code, datasets, and models
are available at https://learning-from-peers.github.io/ .

</details>


### [63] [Learning Dynamics in Continual Pre-Training for Large Language Models](https://arxiv.org/abs/2505.07796)
*Xingjin Wang,Howe Tissue,Lu Wang,Linjing Li,Daniel Dajun Zeng*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型在持续预训练（CPT）过程中的学习动态，提出了一个结合分布偏移和学习率退火因素的CPT缩放定律，用于预测任意训练步骤和不同学习率计划下的损失。


<details>
  <summary>Details</summary>
Motivation: 探索CPT过程中通用和下游领域性能的变化，以优化训练超参数并平衡通用与领域特定性能。

Method: 通过解耦分布偏移和学习率退火效应，推导CPT缩放定律，并结合实验验证其有效性。

Result: 提出的缩放定律在不同CPT数据集和训练超参数下均适用，并能定制化训练目标。

Conclusion: 该研究为CPT过程提供了全面的理解框架，并能指导实际应用中的超参数优化。

Abstract: Continual Pre-Training (CPT) has become a popular and effective method to
apply strong foundation models to specific downstream tasks. In this work, we
explore the learning dynamics throughout the CPT process for large language
models. We specifically focus on how general and downstream domain performance
evolves at each training step, with domain performance measured via validation
losses. We have observed that the CPT loss curve fundamentally characterizes
the transition from one curve to another hidden curve, and could be described
by decoupling the effects of distribution shift and learning rate annealing. We
derive a CPT scaling law that combines the two factors, enabling the prediction
of loss at any (continual) training steps and across learning rate schedules
(LRS) in CPT. Our formulation presents a comprehensive understanding of several
critical factors in CPT, including loss potential, peak learning rate, training
steps, replay ratio, etc. Moreover, our approach can be adapted to customize
training hyper-parameters to different CPT goals such as balancing general and
domain-specific performance. Extensive experiments demonstrate that our scaling
law holds across various CPT datasets and training hyper-parameters.

</details>


### [64] [A Comparative Analysis of Static Word Embeddings for Hungarian](https://arxiv.org/abs/2505.07809)
*Máté Gedeon*

Main category: cs.CL

TL;DR: 该论文全面分析了匈牙利语的各种静态词嵌入方法，包括Word2Vec、FastText以及基于BERT模型的静态嵌入，并通过内在和外在任务评估其性能。传统静态嵌入（尤其是FastText）在语义和语法关系任务中表现优异，而基于BERT的X2Static方法提取的静态嵌入接近传统嵌入的效果。动态模型提取的嵌入（如ELMo）在命名实体识别和词性标注任务中表现更佳，凸显了上下文化表示的优势。


<details>
  <summary>Details</summary>
Motivation: 研究旨在比较不同静态词嵌入方法在匈牙利语中的性能，尤其是传统方法与基于BERT模型的静态嵌入方法，以期为自然语言处理应用提供更优的嵌入选择。

Method: 通过内在评估（如词类比任务）和外在评估（如双向LSTM模型用于命名实体识别和词性标注）来测试各种嵌入方法的性能。

Result: 传统静态嵌入（如FastText）在词类比任务中表现最佳，而基于BERT的X2Static方法提取的静态嵌入接近其效果。动态模型（如ELMo）的嵌入在外在任务中表现更优。

Conclusion: 静态词嵌入在NLP中仍具重要性，而高级提取方法（如X2Static）能提升基于BERT模型的效果。研究为匈牙利语的嵌入选择提供了实用见解，相关代码和资源将公开以促进后续研究。

Abstract: This paper presents a comprehensive analysis of various static word
embeddings for Hungarian, including traditional models such as Word2Vec,
FastText, as well as static embeddings derived from BERT-based models using
different extraction methods. We evaluate these embeddings on both intrinsic
and extrinsic tasks to provide a holistic view of their performance. For
intrinsic evaluation, we employ a word analogy task, which assesses the
embeddings ability to capture semantic and syntactic relationships. Our results
indicate that traditional static embeddings, particularly FastText, excel in
this task, achieving high accuracy and mean reciprocal rank (MRR) scores. Among
the BERT-based models, the X2Static method for extracting static embeddings
demonstrates superior performance compared to decontextualized and aggregate
methods, approaching the effectiveness of traditional static embeddings. For
extrinsic evaluation, we utilize a bidirectional LSTM model to perform Named
Entity Recognition (NER) and Part-of-Speech (POS) tagging tasks. The results
reveal that embeddings derived from dynamic models, especially those extracted
using the X2Static method, outperform purely static embeddings. Notably, ELMo
embeddings achieve the highest accuracy in both NER and POS tagging tasks,
underscoring the benefits of contextualized representations even when used in a
static form. Our findings highlight the continued relevance of static word
embeddings in NLP applications and the potential of advanced extraction methods
to enhance the utility of BERT-based models. This piece of research contributes
to the understanding of embedding performance in the Hungarian language and
provides valuable insights for future developments in the field. The training
scripts, evaluation codes, restricted vocabulary, and extracted embeddings will
be made publicly available to support further research and reproducibility.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [65] [Neural Network Operator-Based Fractal Approximation: Smoothness Preservation and Convergence Analysis](https://arxiv.org/abs/2505.06229)
*Aaqib Ayoub Bhat,Asif Khan,M. Mursaleen*

Main category: cs.LG

TL;DR: 该论文提出了一种利用神经网络算子构建α-分形插值函数（FIF）的新方法，结合了逼近理论，通过仅使用节点处的函数值生成分形函数，并在一定条件下保持函数的光滑性，同时分析了其收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统分形插值函数构建依赖于整个原始函数，而本研究旨在通过神经网络算子仅使用节点值实现更高效的构建，并研究其光滑性与逼近性能。

Method: 基于四层神经网络算子构建α-分形插值函数，利用逼近理论工具（如连续性模和插值算子）分析收敛性及误差界。

Result: 在约束条件下，新构建的α-分形函数保持了原始函数的光滑性（若f∈C^r，则f^α∈C^r），并在适当条件下收敛于原函数。

Conclusion: 该方法通过神经网络算子实现了高效的分形插值函数构建，为分形逼近理论提供了新工具和理论支持。

Abstract: This paper presents a new approach of constructing $\alpha$-fractal
interpolation functions (FIFs) using neural network operators, integrating
concepts from approximation theory. Initially, we construct $\alpha$-fractals
utilizing neural network-based operators, providing an approach to generating
fractal functions with interpolation properties. Based on the same foundation,
we have developed fractal interpolation functions that utilize only the values
of the original function at the nodes or partition points, unlike traditional
methods that rely on the entire original function.
  Further, we have constructed \(\alpha\)-fractals that preserve the smoothness
of functions under certain constraints by employing a four-layered neural
network operator, ensuring that if \(f \in C^{r}[a,b]\), then the corresponding
fractal \(f^{\alpha} \in C^{r}[a,b]\). Furthermore, we analyze the convergence
of these $\alpha$-fractals to the original function under suitable conditions.
The work uses key approximation theory tools, such as the modulus of continuity
and interpolation operators, to develop convergence results and uniform
approximation error bounds.

</details>


### [66] [Beyond Attention: Toward Machines with Intrinsic Higher Mental States](https://arxiv.org/abs/2505.06257)
*Ahsan Adeel*

Main category: cs.LG

TL;DR: 该论文提出了一种受大脑神经元启发的模型，通过模拟高级感知处理与清醒思维状态，预先筛选相关信息以提高注意力机制的效率，从而在降低计算需求的同时加速学习过程。


<details>
  <summary>Details</summary>
Motivation: 受最近神经生物学发现的启发，研究如何通过模拟大脑的神经元调制机制，优化现代机器学习模型（如Transformer）的注意力机制，以减少计算资源消耗并提高学习效率。

Method: 利用三元神经元级调制循环（Q、K、V），在表示层面实现多样化且深度的并行推理链，预先筛选相关信息后再应用注意力机制。

Result: 在强化学习、计算机视觉和自然语言问答任务中，该方法显著减少了计算需求（如头部、层数和标记数量），同时实现数量级的学习速度提升。

Conclusion: 该工作为高效注意力机制提供了一种新思路，展示了如何通过神经生物学启发的方法优化模型的性能和效率。

Abstract: Attending to what is relevant is fundamental to both the mammalian brain and
modern machine learning models such as Transformers. Yet, determining relevance
remains a core challenge, traditionally offloaded to learning algorithms like
backpropagation. Inspired by recent cellular neurobiological evidence linking
neocortical pyramidal cells to distinct mental states, this work shows how
models (e.g., Transformers) can emulate high-level perceptual processing and
awake thought (imagination) states to pre-select relevant information before
applying attention. Triadic neuronal-level modulation loops among questions
($Q$), clues (keys, $K$), and hypotheses (values, $V$) enable diverse, deep,
parallel reasoning chains at the representation level and allow a rapid shift
from initial biases to refined understanding. This leads to orders-of-magnitude
faster learning with significantly reduced computational demand (e.g., fewer
heads, layers, and tokens), at an approximate cost of $\mathcal{O}(N)$, where
$N$ is the number of input tokens. Results span reinforcement learning (e.g.,
CarRacing in a high-dimensional visual setup), computer vision, and natural
language question answering.

</details>


### [67] [ABE: A Unified Framework for Robust and Faithful Attribution-Based Explainability](https://arxiv.org/abs/2505.06258)
*Zhiyu Zhu,Jiayu Zhang,Zhibo Jin,Fang Chen,Jianlong Zhou*

Main category: cs.LG

TL;DR: 提出了一个统一的归因可解释性框架（ABE），以解决现有方法在可扩展性、耦合度、理论限制和用户友好性方面的不足，并提供可定制模块以增强深度学习的透明度。


<details>
  <summary>Details</summary>
Motivation: 现有归因算法框架存在可扩展性差、耦合度高、理论限制多和用户友好性不足的问题，这阻碍了神经网络的透明度和互操作性。

Method: 提出了Attribution-Based Explainability (ABE)，一个统一的框架，整合了最先进的归因算法，并确保符合归因公理，包含四个可定制模块：鲁棒性、可解释性、验证和数据与模型。

Result: ABE提供了一个可扩展且可扩展的基础，以推进基于归因的可解释性，并促进透明的AI系统。

Conclusion: ABE框架通过整合现有技术和提供自定义模块，显著提升了深度学习模型的透明度和可解释性，为未来研究提供了灵活的工具。

Abstract: Attribution algorithms are essential for enhancing the interpretability and
trustworthiness of deep learning models by identifying key features driving
model decisions. Existing frameworks, such as InterpretDL and OmniXAI,
integrate multiple attribution methods but suffer from scalability limitations,
high coupling, theoretical constraints, and lack of user-friendly
implementations, hindering neural network transparency and interoperability. To
address these challenges, we propose Attribution-Based Explainability (ABE), a
unified framework that formalizes Fundamental Attribution Methods and
integrates state-of-the-art attribution algorithms while ensuring compliance
with attribution axioms. ABE enables researchers to develop novel attribution
techniques and enhances interpretability through four customizable modules:
Robustness, Interpretability, Validation, and Data & Model. This framework
provides a scalable, extensible foundation for advancing attribution-based
explainability and fostering transparent AI systems. Our code is available at:
https://github.com/LMBTough/ABE-XAI.

</details>


### [68] [Fair Clustering with Clusterlets](https://arxiv.org/abs/2505.06259)
*Mattia Setzu,Riccardo Guidotti*

Main category: cs.LG

TL;DR: 论文提出了一种基于小簇的模糊聚类算法，优化了公平聚类效果，实验表明该方法能在保证公平性的同时实现高内聚和低重叠。


<details>
  <summary>Details</summary>
Motivation: 现实世界中聚类方法的公平性日益重要，但现有方法在找到合适的初始聚类时可能计算复杂或随意。本文旨在通过简单方法解决这一问题。

Method: 提出基于小簇（clusterlet）的模糊聚类算法，通过匹配单类簇并优化聚类距离，同时用正则化方法保证公平性。

Result: 实验显示，简单的匹配策略能实现高公平性，参数调优后还能达到高内聚和低重叠的目标。

Conclusion: 该算法在公平性和聚类效果之间取得了平衡，为实际应用提供了一种高效且简单的解决方案。

Abstract: Given their widespread usage in the real world, the fairness of clustering
methods has become of major interest. Theoretical results on fair clustering
show that fairness enjoys transitivity: given a set of small and fair clusters,
a trivial centroid-based clustering algorithm yields a fair clustering.
Unfortunately, discovering a suitable starting clustering can be
computationally expensive, rather complex or arbitrary.
  In this paper, we propose a set of simple \emph{clusterlet}-based fuzzy
clustering algorithms that match single-class clusters, optimizing fair
clustering. Matching leverages clusterlet distance, optimizing for classic
clustering objectives, while also regularizing for fairness. Empirical results
show that simple matching strategies are able to achieve high fairness, and
that appropriate parameter tuning allows to achieve high cohesion and low
overlap.

</details>


### [69] [Dialz: A Python Toolkit for Steering Vectors](https://arxiv.org/abs/2505.06262)
*Zara Siddique,Liam D. Turner,Luis Espinosa-Anke*

Main category: cs.LG

TL;DR: Dialz是一个用于开源LLMs的转向向量研究的Python框架，支持多种任务，强调模块化和易用性，旨在加速研究并提升AI系统的安全性和可控性。


<details>
  <summary>Details</summary>
Motivation: 为了提供比提示或微调更强大的方法来修改LLMs的行为，例如增强或减弱特定概念（如诚实或积极性），并促进安全可控的语言生成研究。

Method: 设计了Dialz框架，支持创建对比对数据集、计算和应用转向向量以及可视化等功能，强调模块化和易用性。

Result: Dialz能够减少有害输出（如刻板印象），并提供对模型行为的深入洞察。

Conclusion: Dialz通过加速研究周期和提升模型可解释性，为更安全、透明、可靠的AI系统铺平了道路。

Abstract: We introduce Dialz, a framework for advancing research on steering vectors
for open-source LLMs, implemented in Python. Steering vectors allow users to
modify activations at inference time to amplify or weaken a 'concept', e.g.
honesty or positivity, providing a more powerful alternative to prompting or
fine-tuning. Dialz supports a diverse set of tasks, including creating
contrastive pair datasets, computing and applying steering vectors, and
visualizations. Unlike existing libraries, Dialz emphasizes modularity and
usability, enabling both rapid prototyping and in-depth analysis. We
demonstrate how Dialz can be used to reduce harmful outputs such as
stereotypes, while also providing insights into model behaviour across
different layers. We release Dialz with full documentation, tutorials, and
support for popular open-source models to encourage further research in safe
and controllable language generation. Dialz enables faster research cycles and
facilitates insights into model interpretability, paving the way for safer,
more transparent, and more reliable AI systems.

</details>


### [70] [ONERA's CRM WBPN database for machine learning activities, related regression challenge and first results](https://arxiv.org/abs/2505.06265)
*Jacques Peter,Quentin Bennehard,Sébastien Heib,Jean-Luc Hantrais-Gervois,Frédéric Moëns*

Main category: cs.LG

TL;DR: 该论文介绍了ONERA开发的一个新的计算流体动力学数据库，用于支持机器学习在气动力场预测中的发展，并基于这些数据定义了回归挑战任务。


<details>
  <summary>Details</summary>
Motivation: 推动机器学习在气动力场预测中的应用，提供高质量的数据集和基准测试。

Method: 使用468个RANS模拟数据，覆盖多种流动条件，通过训练和测试集划分，评估多种机器学习回归方法的性能。

Result: 初步性能结果展示了不同机器学习方法在压力和摩擦系数预测中的表现，为未来研究提供参考。

Conclusion: 该数据库和挑战任务为机器学习在气动力学中的应用提供了有价值的资源和方向。

Abstract: This paper presents a new Computational Fluid Dynamics database, developed at
ONERA, to support the advancement of machine learning techniques for
aerodynamic field prediction. It contains 468 Reynolds-Averaged Navier-Stokes
simulations using the Spalart-Allmaras turbulence model, performed on the
NASA/Boeing Common Research Model wing-body-pylon-nacelle configuration. The
database spans a wide range of flow conditions, varying Mach number (including
transonic regimes), angle of attack (capturing flow separation), and Reynolds
number (based on three stagnation pressures, with one setting matching wind
tunnel experiments). The quality of the database is assessed, through checking
the convergence level of each computation.
  Based on these data, a regression challenge is defined. It consists in
predicting the wall distributions of pressure and friction coefficients for
unseen aerodynamic conditions. The 468 simulations are split into training and
testing sets, with the training data made available publicly on the Codabench
platform. The paper further evaluates several classical machine learning
regressors on this task. Tested pointwise methods include Multi-Layer
Perceptrons, $\lambda$-DNNs, and Decision Trees, while global methods include
Multi-Layer Perceptron, k-Nearest Neighbors, Proper Orthogonal Decomposition
and IsoMap. Initial performance results, using $R^2$ scores and worst relative
mean absolute error metrics, are presented, offering insights into the
capabilities of these techniques for the challenge and references for future
work.

</details>


### [71] [Knowledge Guided Encoder-Decoder Framework Integrating Multiple Physical Models for Agricultural Ecosystem Modeling](https://arxiv.org/abs/2505.06266)
*Qi Cheng,Licheng Liu,Zhang Yao,Hong Mu,Shiyuan Luo,Zhenong Jin,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: 提出了一种知识指导的编码器-解码器模型，结合物理模型和语言模型，预测农作物关键变量，解决传统模型泛化性差的问题。


<details>
  <summary>Details</summary>
Motivation: 农业监测对食品安全至关重要，但传统物理模型和数据驱动模型存在参数不确定、泛化性差的问题，需要更通用的解决方案。

Method: 联合使用知识指导的编码器-解码器模型和语言模型，选择性整合多物理模型知识，处理复杂不一致的输入。

Result: 在多个地点预测碳氮通量的实验中，模型表现出高效性和鲁棒性。

Conclusion: 提出的模型通过结合物理知识和数据驱动方法，有效提升了预测的通用性和准确性。

Abstract: Agricultural monitoring is critical for ensuring food security, maintaining
sustainable farming practices, informing policies on mitigating food shortage,
and managing greenhouse gas emissions. Traditional process-based physical
models are often designed and implemented for specific situations, and their
parameters could also be highly uncertain. In contrast, data-driven models
often use black-box structures and does not explicitly model the
inter-dependence between different ecological variables. As a result, they
require extensive training data and lack generalizability to different tasks
with data distribution shifts and inconsistent observed variables. To address
the need for more universal models, we propose a knowledge-guided
encoder-decoder model, which can predict key crop variables by leveraging
knowledge of underlying processes from multiple physical models. The proposed
method also integrates a language model to process complex and inconsistent
inputs and also utilizes it to implement a model selection mechanism for
selectively combining the knowledge from different physical models. Our
evaluations on predicting carbon and nitrogen fluxes for multiple sites
demonstrate the effectiveness and robustness of the proposed model under
various scenarios.

</details>


### [72] [Cluster-Aware Multi-Round Update for Wireless Federated Learning in Heterogeneous Environments](https://arxiv.org/abs/2505.06268)
*Pengcheng Sun,Erwu Liu,Wei Ni,Kanglei Yu,Rui Wang,Abbas Jamalipour*

Main category: cs.LG

TL;DR: 本文提出了一种基于聚类策略的无线联邦学习（FL）优化方法，通过设备聚类减少异构性影响，并设计了CAMU策略以调整本地更新频率和优化资源分配，显著提升了FL的模型性能和收敛效率。


<details>
  <summary>Details</summary>
Motivation: 无线联邦学习在异构环境中因设备和通信能力的差异性导致聚合效率和准确性下降，本文旨在通过聚类策略和资源优化解决这一问题。

Method: 采用基于先验知识相似性的聚类策略分组设备，并提出CAMU策略动态调整本地更新频率和传输功率，优化计算与通信资源分配。

Result: 实验结果表明该方法有效提升了FL在异构环境中的模型性能，并在有限资源下实现了通信成本与计算负载的更好平衡。

Conclusion: 聚类和CAMU策略的结合能够显著改善FL的聚合效率和收敛性能，为异构环境中的分布式学习提供了有效的解决方案。

Abstract: The aggregation efficiency and accuracy of wireless Federated Learning (FL)
are significantly affected by resource constraints, especially in heterogeneous
environments where devices exhibit distinct data distributions and
communication capabilities. This paper proposes a clustering strategy that
leverages prior knowledge similarity to group devices with similar data and
communication characteristics, mitigating performance degradation from
heterogeneity. On this basis, a novel Cluster- Aware Multi-round Update (CAMU)
strategy is proposed, which treats clusters as the basic units and adjusts the
local update frequency based on the clustered contribution threshold,
effectively reducing update bias and enhancing aggregation accuracy. The
theoretical convergence of the CAMU strategy is rigorously validated.
Meanwhile, based on the convergence upper bound, the local update frequency and
transmission power of each cluster are jointly optimized to achieve an optimal
balance between computation and communication resources under constrained
conditions, significantly improving the convergence efficiency of FL.
Experimental results demonstrate that the proposed method effectively improves
the model performance of FL in heterogeneous environments and achieves a better
balance between communication cost and computational load under limited
resources.

</details>


### [73] [A machine learning model for skillful climate system prediction](https://arxiv.org/abs/2505.06269)
*Chenguang Zhou,Lei Chen,Xiaohui Zhong,Bo Lu,Hao Li,Libo Wu,Jie Wu,Jiahui Hu,Zesheng Dou,Pang-Chi Hsu,Xiaoye Zhang*

Main category: cs.LG

TL;DR: FengShun-CSM是一种基于AI的气候系统模型，能够实现60天全球每日预测，涵盖29个关键变量，并在多数预测中优于ECMWF的S2S模型。


<details>
  <summary>Details</summary>
Motivation: 开发一个完全耦合的AI气候系统模型，以解决传统模型在多圈层耦合预测中的不足。

Method: 利用AI技术构建FengShun-CSM模型，整合大气、海洋、陆地和海冰的相互作用。

Result: FengShun-CSM显著提升了预测能力，尤其是在季节内变异模式（如MJO）和极端事件预测方面表现突出。

Conclusion: FengShun-CSM为下一代地球系统建模提供了新范式，并在灾害缓解和生态保护等领域具有应用潜力。

Abstract: Climate system models (CSMs), through integrating cross-sphere interactions
among the atmosphere, ocean, land, and cryosphere, have emerged as pivotal
tools for deciphering climate dynamics and improving forecasting capabilities.
Recent breakthroughs in artificial intelligence (AI)-driven meteorological
modeling have demonstrated remarkable success in single-sphere systems and
partially spheres coupled systems. However, the development of a fully coupled
AI-based climate system model encompassing atmosphere-ocean-land-sea ice
interactions has remained an unresolved challenge. This paper introduces
FengShun-CSM, an AI-based CSM model that provides 60-day global daily forecasts
for 29 critical variables across atmospheric, oceanic, terrestrial, and
cryospheric domains. The model significantly outperforms the European Centre
for Medium-Range Weather Forecasts (ECMWF) subseasonal-to-seasonal (S2S) model
in predicting most variables, particularly precipitation, land surface, and
oceanic components. This enhanced capability is primarily attributed to its
improved representation of intra-seasonal variability modes, most notably the
Madden-Julian Oscillation (MJO). Remarkably, FengShun-CSM exhibits substantial
potential in predicting subseasonal extreme events. Such breakthroughs will
advance its applications in meteorological disaster mitigation, marine
ecosystem conservation, and agricultural productivity enhancement. Furthermore,
it validates the feasibility of developing AI-powered CSMs through machine
learning technologies, establishing a transformative paradigm for
next-generation Earth system modeling.

</details>


### [74] [Importance Analysis for Dynamic Control of Balancing Parameter in a Simple Knowledge Distillation Setting](https://arxiv.org/abs/2505.06270)
*Seongmin Kim,Kwanho Kim,Minseung Kim,Kanghyun Jo*

Main category: cs.LG

TL;DR: 论文提出了在知识蒸馏（KD）过程中动态调整平衡参数的数学依据，以优化轻量级学生网络的训练效果。


<details>
  <summary>Details</summary>
Motivation: 深度学习的复杂性常导致实时性能下降，知识蒸馏虽能缓解此问题，但传统方法中平衡参数的静态调整可能限制其效果。本文旨在提供动态调整的理论支持。

Method: 通过分析知识蒸馏的两个损失函数（蒸馏损失和下游任务损失），在简单KD设定下推导出损失下降时应动态调节平衡参数的数学理论。

Result: 理论分析表明，动态调整平衡参数能更有效地提升学生网络的性能，尤其是在蒸馏损失占主导时。

Conclusion: 动态调整策略为知识蒸馏提供了更优的优化路径，未来可扩展至复杂场景验证其普适性。

Abstract: Although deep learning models owe their remarkable success to deep and
complex architectures, this very complexity typically comes at the expense of
real-time performance. To address this issue, a variety of model compression
techniques have been proposed, among which knowledge distillation (KD) stands
out for its strong empirical performance. The KD contains two concurrent
processes: (i) matching the outputs of a large, pre-trained teacher network and
a lightweight student network, and (ii) training the student to solve its
designated downstream task. The associated loss functions are termed the
distillation loss and the downsteam-task loss, respectively. Numerous prior
studies report that KD is most effective when the influence of the distillation
loss outweighs that of the downstream-task loss. The influence(or importance)
is typically regulated by a balancing parameter. This paper provides a
mathematical rationale showing that in a simple KD setting when the loss is
decreasing, the balancing parameter should be dynamically adjusted

</details>


### [75] [Tri-MTL: A Triple Multitask Learning Approach for Respiratory Disease Diagnosis](https://arxiv.org/abs/2505.06271)
*June-Woo Kim,Sanghoon Lee,Miika Toikkanen,Daehwan Hwang,Kyunghoon Kim*

Main category: cs.LG

TL;DR: 该研究探讨了多任务学习（MTL）结合深度学习在呼吸音分类和疾病诊断中的效果，尤其关注了元数据的影响。实验证明，结合听诊器信息的MTL架构显著提升了分类和诊断性能。


<details>
  <summary>Details</summary>
Motivation: 临床听诊是评估和监测患者的重要手段，但呼吸音、疾病表现和患者元数据之间的复杂关系尚未充分研究。研究旨在通过MTL和深度学习整合这些信息，提升诊断准确性。

Method: 使用多任务学习框架，结合呼吸音模式、疾病表现和患者元数据，并采用先进的深度学习架构进行建模和分析。

Result: 实验表明，将听诊器信息整合到MTL架构中，显著改善了呼吸音分类和疾病诊断的性能。

Conclusion: 该研究证实了MTL结合元数据在医疗应用中的潜力，为呼吸音分析和疾病诊断提供了更高效的方法。

Abstract: Auscultation remains a cornerstone of clinical practice, essential for both
initial evaluation and continuous monitoring. Clinicians listen to the lung
sounds and make a diagnosis by combining the patient's medical history and test
results. Given this strong association, multitask learning (MTL) can offer a
compelling framework to simultaneously model these relationships, integrating
respiratory sound patterns with disease manifestations. While MTL has shown
considerable promise in medical applications, a significant research gap
remains in understanding the complex interplay between respiratory sounds,
disease manifestations, and patient metadata attributes. This study
investigates how integrating MTL with cutting-edge deep learning architectures
can enhance both respiratory sound classification and disease diagnosis.
Specifically, we extend recent findings regarding the beneficial impact of
metadata on respiratory sound classification by evaluating its effectiveness
within an MTL framework. Our comprehensive experiments reveal significant
improvements in both lung sound classification and diagnostic performance when
the stethoscope information is incorporated into the MTL architecture.

</details>


### [76] [A Sensitivity-Driven Expert Allocation Method in LoRA-MoE for Efficient Fine-Tuning](https://arxiv.org/abs/2505.06272)
*Junzhou Xu,Boyu Diao*

Main category: cs.LG

TL;DR: 提出了一种基于参数敏感性的专家分配方法（LoRA-SMoE），通过采样少量数据并利用梯度信息快速评估任务对参数的敏感性，自适应分配专家数量。该方法在保持与LoRA相当内存消耗的同时，减少了可训练参数数量，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型扩展，预训练-微调范式成为处理下游任务的标准方法。然而，共享参数在复杂多任务数据集上会导致性能下降。虽然MoE方法部分缓解了这一问题，但也显著增加了微调参数和训练时间，引入更大参数冗余。

Method: 提出LoRA-SMoE方法，通过采样少量数据并利用梯度信息快速评估任务对参数的敏感性，自适应分配专家数量，保持内存消耗与LoRA相当，同时减少可训练参数。

Result: 实验表明，相比SOTA微调方法，LoRA-SMoE提升了模型性能，同时减少了可训练参数数量，资源消耗低，适合计算资源受限的场景。

Conclusion: LoRA-SMoE通过高效参数敏感性评估机制优化专家分配，为资源受限场景提供了高效的微调方案。所有代码将公开。

Abstract: As deep learning models expand, the pre-training-fine-tuning paradigm has
become the standard approach for handling various downstream tasks. However,
shared parameters can lead to diminished performance when dealing with complex
datasets involving multiple tasks. While introducing Mixture-of-Experts (MoE)
methods has alleviated this issue to some extent, it also significantly
increases the number of parameters required for fine-tuning and training time,
introducing greater parameter redundancy. To address these challenges, we
propose a method for allocating expert numbers based on parameter sensitivity
LoRA-SMoE (A Sensitivity-Driven Expert Allocation Method in LoRA-MoE for
Efficient Fine-Tuning). This method rapidly assesses the sensitivity of
different tasks to parameters by sampling a small amount of data and using
gradient information. It then adaptively allocates expert numbers within a
given budget. The process maintains comparable memory consumption to LoRA
(Low-Rank Adaptation) while ensuring an efficient and resource-friendly
fine-tuning procedure. Experimental results demonstrate that compared to SOTA
fine-tuning methods, our LoRA-SMoE approach can enhance model performance while
reducing the number of trainable parameters. This significantly improves model
performance in resource-constrained environments. Additionally, due to its
efficient parameter sensitivity evaluation mechanism, LoRA-SMoE requires
minimal computational overhead to optimize expert allocation, making it
particularly suitable for scenarios with limited computational resources. All
the code in this study will be made publicly available following the acceptance
of the paper for publication. Source code is at
https://github.com/EMLS-ICTCAS/LoRA-SMoE

</details>


### [77] [Policy-labeled Preference Learning: Is Preference Enough for RLHF?](https://arxiv.org/abs/2505.06273)
*Taehyun Cho,Seokhun Ju,Seungyub Han,Dohyeong Kim,Kyungjae Lee,Jungwoo Lee*

Main category: cs.LG

TL;DR: 论文提出了一种名为policy-labeled preference learning (PPL)的新方法，通过建模人类偏好和后悔行为来解决现有RLHF方法中的似然不匹配问题，提升了离线RLHF性能和在在线环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的RLHF方法常常误将轨迹视为由最优策略生成，导致似然估计不准确和学习效果不佳。因此，需要一种新方法来更准确地建模人类偏好并优化策略。

Method: 受Direct Preference Optimization（直接偏好优化）框架的启发，提出了policy-labeled preference learning (PPL)，通过后悔行为来建模人类偏好，并提供了一种对比KL正则化方法以增强RLHF在序列决策中的表现。

Result: 在高维连续控制任务中的实验表明，PPL在离线RLHF性能上有显著提升，并且在在线环境中也表现出了有效性。

Conclusion: PPL通过引入后悔行为建模和对比KL正则化，有效解决了现有RLHF方法中的问题，为强化学习领域提供了一种更优的解决方案。

Abstract: To design rewards that align with human goals, Reinforcement Learning from
Human Feedback (RLHF) has emerged as a prominent technique for learning reward
functions from human preferences and optimizing policies via reinforcement
learning algorithms. However, existing RLHF methods often misinterpret
trajectories as being generated by an optimal policy, causing inaccurate
likelihood estimation and suboptimal learning. Inspired by Direct Preference
Optimization framework which directly learns optimal policy without explicit
reward, we propose policy-labeled preference learning (PPL), to resolve
likelihood mismatch issues by modeling human preferences with regret, which
reflects behavior policy information. We also provide a contrastive KL
regularization, derived from regret-based principles, to enhance RLHF in
sequential decision making. Experiments in high-dimensional continuous control
tasks demonstrate PPL's significant improvements in offline RLHF performance
and its effectiveness in online settings.

</details>


### [78] [PARM: Multi-Objective Test-Time Alignment via Preference-Aware Autoregressive Reward Model](https://arxiv.org/abs/2505.06274)
*Baijiong Lin,Weisen Jiang,Yuancheng Xu,Hao Chen,Ying-Cong Chen*

Main category: cs.LG

TL;DR: PARM提出了一种统一的偏好感知自回归奖励模型，通过PBLoRA技术实现多目标测试时对齐，降低了推理成本并改善了偏好对齐效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法GenARM需要多个独立训练的ARMs导致的推理成本高和偏好对齐不准确的问题。

Method: 提出Preference-aware ARM (PARM)和Preference-Aware Bilinear Low-Rank Adaptation (PBLoRA)，通过单一模型和双线性形式实现多偏好维度的统一训练和控制。

Result: PARM在实验中展现了更低的推理成本和更好的偏好对齐效果，同时支持弱到强的引导。

Conclusion: PARM为多目标对齐提供了一种高效且经济的方法，尤其适用于计算资源有限的情况。

Abstract: Multi-objective test-time alignment aims to adapt large language models
(LLMs) to diverse multi-dimensional user preferences during inference while
keeping LLMs frozen. Recently, GenARM (Xu et al., 2025) first independently
trains Autoregressive Reward Models (ARMs) for each preference dimension
without awareness of each other, then combines their outputs based on
user-specific preference vectors during inference to achieve multi-objective
test-time alignment, leading to two key limitations: the need for
\textit{multiple} ARMs increases the inference cost, and the separate training
of ARMs causes the misalignment between the guided generation and the user
preferences. To address these issues, we propose Preference-aware ARM (PARM), a
single unified ARM trained across all preference dimensions. PARM uses our
proposed Preference-Aware Bilinear Low-Rank Adaptation (PBLoRA), which employs
a bilinear form to condition the ARM on preference vectors, enabling it to
achieve precise control over preference trade-offs during inference.
Experiments demonstrate that PARM reduces inference costs and achieves better
alignment with preference vectors compared with existing methods. Additionally,
PARM enables weak-to-strong guidance, allowing a smaller PARM to guide a larger
frozen LLM without expensive training, making multi-objective alignment
accessible with limited computing resources. The code is available at
https://github.com/Baijiong-Lin/PARM.

</details>


### [79] [Attonsecond Streaking Phase Retrieval Via Deep Learning Methods](https://arxiv.org/abs/2505.06275)
*Yuzhou Zhu,Zheng Zhang,Ruyi Zhang,Liang Zhou*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的相位检索方法，将问题转化为监督式计算机视觉任务，并比较了四种神经网络架构的性能。胶囊网络表现最佳，为实时阿秒脉冲表征提供了潜在途径。


<details>
  <summary>Details</summary>
Motivation: 传统相位检索算法依赖迭代最小化和中心动量近似，对宽带脉冲的准确性有限。因此，需要更高效且准确的方法来解析亚飞秒时间尺度的电子动力学。

Method: 论文将相位检索重新定义为监督式计算机视觉问题，并系统比较了四种神经网络架构：卷积网络（CNN）、视觉变换器（ViT）、混合CNN-ViT模型以及胶囊网络，分别评估其局部和全局特征提取能力。

Result: 理论分析和合成条纹光谱实验表明，胶囊网络的检索保真度最高，性能排序为CNN < ViT < 混合模型 < 胶囊网络。

Conclusion: 胶囊网络在相位检索中表现最佳，未来的研究方向包括将强场积分嵌入物理信息神经网络和探索光子硬件实现，以实现复杂实验条件下的实时阿秒脉冲表征。

Abstract: Attosecond streaking phase retrieval is essential for resolving electron
dynamics on sub-femtosecond time scales yet traditional algorithms rely on
iterative minimization and central momentum approximations that degrade
accuracy for broadband pulses. In this work phase retrieval is reformulated as
a supervised computer-vision problem and four neural architectures are
systematically compared. A convolutional network demonstrates strong
sensitivity to local streak edges but lacks global context; a vision
transformer captures long-range delay-energy correlations at the expense of
local inductive bias; a hybrid CNN-ViT model unites local feature extraction
and full-graph attention; and a capsule network further enforces spatial pose
agreement through dynamic routing. A theoretical analysis introduces local,
global and positional sensitivity measures and derives surrogate error bounds
that predict the strict ordering $CNN<ViT<Hybrid<Capsule$. Controlled
experiments on synthetic streaking spectrograms confirm this hierarchy, with
the capsule network achieving the highest retrieval fidelity. Looking forward,
embedding the strong-field integral into physics-informed neural networks and
exploring photonic hardware implementations promise pathways toward real-time
attosecond pulse characterization under demanding experimental conditions.

</details>


### [80] [Interpretable Learning Dynamics in Unsupervised Reinforcement Learning](https://arxiv.org/abs/2505.06279)
*Shashwat Pandey*

Main category: cs.LG

TL;DR: 该论文提出了一个解释性框架，用于分析无监督强化学习（URL）代理的内在动机如何影响其注意力、行为和表征学习，并通过多种分析方法比较了五种代理的性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解无监督强化学习代理如何通过内在动机（如好奇心）塑造其注意力、行为和学习表征，从而提供更可解释和泛化的行为。

Method: 研究方法包括对 DQN、RND、ICM、PPO 和 Transformer-RND 五种代理在程序生成环境中进行训练，并使用 Grad-CAM、LRP、探索度量和潜在空间聚类进行分析，同时提出了两个新指标：注意力多样性和注意力变化率。

Result: 研究结果表明，由好奇心驱动的代理表现出比外在动机代理更广泛和动态的注意力及探索行为，其中 Transformer-RND 表现尤为突出，具有广泛的注意力、高探索覆盖率和紧凑的结构化潜在表征。

Conclusion: 该研究不仅强调了架构归纳偏差和训练信号对代理内部动态的影响，还为强化学习代理的感知和抽象能力提供了诊断工具，推动了可解释性和泛化性的发展。

Abstract: We present an interpretability framework for unsupervised reinforcement
learning (URL) agents, aimed at understanding how intrinsic motivation shapes
attention, behavior, and representation learning. We analyze five agents DQN,
RND, ICM, PPO, and a Transformer-RND variant trained on procedurally generated
environments, using Grad-CAM, Layer-wise Relevance Propagation (LRP),
exploration metrics, and latent space clustering. To capture how agents
perceive and adapt over time, we introduce two metrics: attention diversity,
which measures the spatial breadth of focus, and attention change rate, which
quantifies temporal shifts in attention. Our findings show that
curiosity-driven agents display broader, more dynamic attention and exploratory
behavior than their extrinsically motivated counterparts. Among them,
TransformerRND combines wide attention, high exploration coverage, and compact,
structured latent representations. Our results highlight the influence of
architectural inductive biases and training signals on internal agent dynamics.
Beyond reward-centric evaluation, the proposed framework offers diagnostic
tools to probe perception and abstraction in RL agents, enabling more
interpretable and generalizable behavior.

</details>


### [81] [Show or Tell? A Benchmark To Evaluate Visual and Textual Prompts in Semantic Segmentation](https://arxiv.org/abs/2505.06280)
*Gabriele Rosi,Fabio Cermelli*

Main category: cs.LG

TL;DR: SoT是一个专门设计用于评估语义分割中视觉和文本提示的新基准，涵盖14个数据集和7个多样化领域，揭示了两种提示方法的优劣势。


<details>
  <summary>Details</summary>
Motivation: 目前语义分割中文本和视觉提示的评估缺乏直接比较，为了系统探索这两种方法的优势和局限性，设计了SoT基准。

Method: 通过5种开放词汇方法和4种视觉参考提示方法进行评估，并采用基于置信度的掩码合并策略处理多类分割。

Result: 开放词汇方法在文本易于描述的常见概念上表现优异，但在复杂领域（如工具）中表现不佳；视觉参考提示方法平均效果良好但受输入提示影响较大。

Conclusion: 研究提供了对两种提示模态优缺点的深入分析，为未来视觉基础模型在分割任务中的研究提供了指导。

Abstract: Prompt engineering has shown remarkable success with large language models,
yet its systematic exploration in computer vision remains limited. In semantic
segmentation, both textual and visual prompts offer distinct advantages:
textual prompts through open-vocabulary methods allow segmentation of arbitrary
categories, while visual reference prompts provide intuitive reference
examples. However, existing benchmarks evaluate these modalities in isolation,
without direct comparison under identical conditions. We present Show or Tell
(SoT), a novel benchmark specifically designed to evaluate both visual and
textual prompts for semantic segmentation across 14 datasets spanning 7 diverse
domains (common scenes, urban, food, waste, parts, tools, and land-cover). We
evaluate 5 open-vocabulary methods and 4 visual reference prompt approaches,
adapting the latter to handle multi-class segmentation through a
confidence-based mask merging strategy. Our extensive experiments reveal that
open-vocabulary methods excel with common concepts easily described by text but
struggle with complex domains like tools, while visual reference prompt methods
achieve good average results but exhibit high variability depending on the
input prompt. Through comprehensive quantitative and qualitative analysis, we
identify the strengths and weaknesses of both prompting modalities, providing
valuable insights to guide future research in vision foundation models for
segmentation tasks.

</details>


### [82] [A Data-Driven Probabilistic Framework for Cascading Urban Risk Analysis Using Bayesian Networks](https://arxiv.org/abs/2505.06281)
*Chunduru Rohith Kumar,PHD Surya Shanmuk,Prabhala Naga Srinivas,Sri Venkatesh Lankalapalli,Debasis Dwibedy*

Main category: cs.LG

TL;DR: 本文提出了一种基于贝叶斯网络的方法，用于分析城市系统中跨域风险的传播。通过结合真实数据和GAN生成数据，利用结构学习和概率推理，识别关键风险因素，为城市韧性规划提供支持。


<details>
  <summary>Details</summary>
Motivation: 城市系统中跨域风险的复杂性增加，需要数据驱动的框架来建模多领域间的相互依赖性。

Method: 使用贝叶斯信念网络（BBNs）构建有向无环图（DAGs），结构学习采用Hill-Climbing搜索，优化基于BIC和K2评分。训练数据结合真实城市指标和GAN生成数据，并用SMOTE平衡。

Result: 识别了关键域内和跨域风险因素，验证了框架在主动城市韧性规划中的实用性。

Conclusion: 该研究为级联风险评估提供了一个可扩展、可解释的基础，并为未来这一新兴跨学科领域的实证研究奠定了基础。

Abstract: The increasing complexity of cascading risks in urban systems necessitates
robust, data-driven frameworks to model interdependencies across multiple
domains. This study presents a foundational Bayesian network-based approach for
analyzing cross-domain risk propagation across key urban domains, including
air, water, electricity, agriculture, health, infrastructure, weather, and
climate. Directed Acyclic Graphs (DAGs) are constructed using Bayesian Belief
Networks (BBNs), with structure learning guided by Hill-Climbing search
optimized through Bayesian Information Criterion (BIC) and K2 scoring. The
framework is trained on a hybrid dataset that combines real-world urban
indicators with synthetically generated data from Generative Adversarial
Networks (GANs), and is further balanced using the Synthetic Minority
Over-sampling Technique (SMOTE). Conditional Probability Tables (CPTs) derived
from the learned structures enable interpretable probabilistic reasoning and
quantify the likelihood of cascading failures. The results identify key intra-
and inter-domain risk factors and demonstrate the framework's utility for
proactive urban resilience planning. This work establishes a scalable,
interpretable foundation for cascading risk assessment and serves as a basis
for future empirical research in this emerging interdisciplinary field.

</details>


### [83] [InfoNCE is a Free Lunch for Semantically guided Graph Contrastive Learning](https://arxiv.org/abs/2505.06282)
*Zixu Wang,Bingbing Xu,Yige Yuan,Huawei Shen,Xueqi Cheng*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的图对比学习方法IFL-GCL，通过将GCL问题重新定义为正未标注（PU）学习问题，并利用InfoNCE提取语义信息，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图对比学习（GCL）方法在定义自监督任务时，可能导致语义相似的样本被错误分类为负样本，造成采样偏差和性能限制。因此，论文提出GCL应视为正未标注（PU）问题，并通过语义指导重新定义任务目标。

Method: 论文提出IFL-GCL方法，利用InfoNCE作为工具提取语义信息，证明节点对表示相似性与其对比样本为正样本的概率一致，并基于修正样本重新定义最大似然目标，推导出新的InfoNCE损失函数。

Result: 在IID和OOD场景下的实验表明，IFL-GCL显著优于现有方法，最高提升了9.05%，验证了语义指导方法的有效性。

Conclusion: IFL-GCL通过重新定义GCL为PU问题并引入语义信息提取，成功提升了图对比学习的性能，为图预训练和LLM增强提供了新思路。

Abstract: As an important graph pre-training method, Graph Contrastive Learning (GCL)
continues to play a crucial role in the ongoing surge of research on graph
foundation models or LLM as enhancer for graphs. Traditional GCL optimizes
InfoNCE by using augmentations to define self-supervised tasks, treating
augmented pairs as positive samples and others as negative. However, this leads
to semantically similar pairs being classified as negative, causing significant
sampling bias and limiting performance. In this paper, we argue that GCL is
essentially a Positive-Unlabeled (PU) learning problem, where the definition of
self-supervised tasks should be semantically guided, i.e., augmented samples
with similar semantics are considered positive, while others, with unknown
semantics, are treated as unlabeled. From this perspective, the key lies in how
to extract semantic information. To achieve this, we propose IFL-GCL, using
InfoNCE as a "free lunch" to extract semantic information. Specifically, We
first prove that under InfoNCE, the representation similarity of node pairs
aligns with the probability that the corresponding contrastive sample is
positive. Then we redefine the maximum likelihood objective based on the
corrected samples, leading to a new InfoNCE loss function. Extensive
experiments on both the graph pretraining framework and LLM as an enhancer show
significantly improvements of IFL-GCL in both IID and OOD scenarios, achieving
up to a 9.05% improvement, validating the effectiveness of semantically guided.
Code for IFL-GCL is publicly available at:
https://github.com/Camel-Prince/IFL-GCL.

</details>


### [84] [Soft causal learning for generalized molecule property prediction: An environment perspective](https://arxiv.org/abs/2505.06283)
*Limin Li,Kuo Yang,Wenjie Du,Pengkun Wang,Zhengyang Zhou,Yang Wang*

Main category: cs.LG

TL;DR: 提出了一种软因果学习框架，用于解决分子科学中的OOD挑战，通过模拟分子环境和绕过不变子图来提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有GNN模型在分子图学习中难以适应OOD样本，且当前OOD方法忽视原子环境扩展、子图-性质关联复杂性及环境与不变性动态交互。

Method: 结合化学理论设计图增长生成器模拟环境，采用GIB目标解耦环境，并引入基于交叉注意力的软因果交互机制。

Result: 在七种OOD泛化场景数据集上验证，实验与可视化案例表明模型具有优秀泛化能力。

Conclusion: 该框架显著提升分子OOD泛化性能，为AI驱动的科学发现提供了新思路。

Abstract: Learning on molecule graphs has become an increasingly important topic in AI
for science, which takes full advantage of AI to facilitate scientific
discovery. Existing solutions on modeling molecules utilize Graph Neural
Networks (GNNs) to achieve representations but they mostly fail to adapt models
to out-of-distribution (OOD) samples. Although recent advances on OOD-oriented
graph learning have discovered the invariant rationale on graphs, they still
ignore three important issues, i.e., 1) the expanding atom patterns regarding
environments on graphs lead to failures of invariant rationale based models, 2)
the associations between discovered molecular subgraphs and corresponding
properties are complex where causal substructures cannot fully interpret the
labels. 3) the interactions between environments and invariances can influence
with each other thus are challenging to be modeled. To this end, we propose a
soft causal learning framework, to tackle the unresolved OOD challenge in
molecular science, from the perspective of fully modeling the molecule
environments and bypassing the invariant subgraphs. Specifically, we first
incorporate chemistry theories into our graph growth generator to imitate
expaned environments, and then devise an GIB-based objective to disentangle
environment from whole graphs and finally introduce a cross-attention based
soft causal interaction, which allows dynamic interactions between environments
and invariances. We perform experiments on seven datasets by imitating
different kinds of OOD generalization scenarios. Extensive comparison, ablation
experiments as well as visualized case studies demonstrate well generalization
ability of our proposal.

</details>


### [85] [DMRL: Data- and Model-aware Reward Learning for Data Extraction](https://arxiv.org/abs/2505.06284)
*Zhiqiang Wang,Ruoxi Cheng*

Main category: cs.LG

TL;DR: DMRL（数据与模型感知奖励学习）方法通过逆向强化学习和动态优化，显著提升了大语言模型（LLMs）中敏感数据的提取性能，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型存在隐私泄露风险，需要系统性的红队研究来开发防御机制。当前的数据提取方法存在依赖数据集重复、提示工程和随机搜索对抗生成等限制。

Method: DMRL采用逆向强化学习，构建内省推理数据集以引导模型行为，并利用GRPO（组相对策略优化）动态调整奖励模型。

Result: 实验表明，DMRL在多种LLMs上的数据提取性能优于所有基线方法。

Conclusion: DMRL通过数据与模型感知的奖励学习，为LLMs的隐私保护提供了更高效的数据提取方案。

Abstract: Large language models (LLMs) are inherently vulnerable to unintended privacy
breaches. Consequently, systematic red-teaming research is essential for
developing robust defense mechanisms. However, current data extraction methods
suffer from several limitations: (1) rely on dataset duplicates (addressable
via deduplication), (2) depend on prompt engineering (now countered by
detection and defense), and (3) rely on random-search adversarial generation.
To address these challenges, we propose DMRL, a Data- and Model-aware Reward
Learning approach for data extraction. This technique leverages inverse
reinforcement learning to extract sensitive data from LLMs. Our method consists
of two main components: (1) constructing an introspective reasoning dataset
that captures leakage mindsets to guide model behavior, and (2) training reward
models with Group Relative Policy Optimization (GRPO), dynamically tuning
optimization based on task difficulty at both the data and model levels.
Comprehensive experiments across various LLMs demonstrate that DMRL outperforms
all baseline methods in data extraction performance.

</details>


### [86] [IIKL: Isometric Immersion Kernel Learning with Riemannian Manifold for Geometric Preservation](https://arxiv.org/abs/2505.06288)
*Zihao Chen,Wenyong Wang,Jiachen Yang,Yu Xiang*

Main category: cs.LG

TL;DR: 提出了等距浸入核学习（IIKL）方法，构建黎曼流形并从离散非欧数据中导出黎曼度量，保持数据的几何结构，显著提高了下游任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的表示学习方法通常将非欧离散数据映射到欧几里得空间，可能导致关键几何信息丢失。本文旨在解决这一问题，通过等距浸入技术保持数据的本征几何和拓扑性质。

Method: 提出IIKL方法，证明等距浸入等价于流形切丛上的核函数，并基于最大似然估计设计了参数化学习模型与交替训练方法。

Result: 实验表明，该方法在3D和高维数据中成功保持了几何表征，内积不变损失降低90%以上，重建精度平均提升40%，等距与共形几何误差减少90%。

Conclusion: IIKL在保留数据几何结构的同时显著提升了下游任务性能，为科学应用中的非欧数据表示学习提供了有效解决方案。

Abstract: Geometric representation learning in preserving the intrinsic geometric and
topological properties for discrete non-Euclidean data is crucial in scientific
applications. Previous research generally mapped non-Euclidean discrete data
into Euclidean space during representation learning, which may lead to the loss
of some critical geometric information. In this paper, we propose a novel
Isometric Immersion Kernel Learning (IIKL) method to build Riemannian manifold
and isometrically induce Riemannian metric from discrete non-Euclidean data. We
prove that Isometric immersion is equivalent to the kernel function in the
tangent bundle on the manifold, which explicitly guarantees the invariance of
the inner product between vectors in the arbitrary tangent space throughout the
learning process, thus maintaining the geometric structure of the original
data. Moreover, a novel parameterized learning model based on IIKL is
introduced, and an alternating training method for this model is derived using
Maximum Likelihood Estimation (MLE), ensuring efficient convergence.
Experimental results proved that using the learned Riemannian manifold and its
metric, our model preserved the intrinsic geometric representation of data in
both 3D and high-dimensional datasets successfully, and significantly improved
the accuracy of downstream tasks, such as data reconstruction and
classification. It is showed that our method could reduce the inner product
invariant loss by more than 90% compared to state-of-the-art (SOTA) methods,
also achieved an average 40% improvement in downstream reconstruction accuracy
and a 90% reduction in error for geometric metrics involving isometric and
conformal.

</details>


### [87] [Edge-Optimized Deep Learning & Pattern Recognition Techniques for Non-Intrusive Load Monitoring of Energy Time Series](https://arxiv.org/abs/2505.06289)
*Sotirios Athanasoulias*

Main category: cs.LG

TL;DR: 该论文提出了一种非侵入式负荷监测（NILM）的解决方案，通过AI和IoT技术分解家庭能源使用数据，并针对地中海地区数据不足和部署成本高的问题，提出了数据集和改进的深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 全球能源需求增长和可持续发展需求促使研究更高效的能源管理方法，但现有技术缺乏用户参与和区域代表性，尤其是地中海地区的能源使用模式未被充分研究。

Method: 提出了一个可互操作的数据收集框架和Plegma数据集，并使用先进的深度神经网络及模型压缩技术，以提高边缘设备上的部署效率。

Result: 通过引入地中海地区的数据集和优化后的模型，显著提升了NILM的区域适应性和部署效率。

Conclusion: 该研究通过理论和实践的结合，使NILM技术更具可扩展性和适应性，为全球能源可持续发展提供了可行的解决方案。

Abstract: The growing global energy demand and the urgent need for sustainability call
for innovative ways to boost energy efficiency. While advanced energy-saving
systems exist, they often fall short without user engagement. Providing
feedback on energy consumption behavior is key to promoting sustainable
practices. Non-Intrusive Load Monitoring (NILM) offers a promising solution by
disaggregating total household energy usage, recorded by a central smart meter,
into appliance-level data. This empowers users to optimize consumption.
Advances in AI, IoT, and smart meter adoption have further enhanced NILM's
potential.
  Despite this promise, real-world NILM deployment faces major challenges.
First, existing datasets mainly represent regions like the USA and UK, leaving
places like the Mediterranean underrepresented. This limits understanding of
regional consumption patterns, such as heavy use of air conditioners and
electric water heaters. Second, deep learning models used in NILM require high
computational power, often relying on cloud services. This increases costs,
raises privacy concerns, and limits scalability, especially for households with
poor connectivity. This thesis tackles these issues with key contributions. It
presents an interoperable data collection framework and introduces the Plegma
Dataset, focused on underrepresented Mediterranean energy patterns. It also
explores advanced deep neural networks and model compression techniques for
efficient edge deployment. By bridging theoretical advances with practical
needs, this work aims to make NILM scalable, efficient, and adaptable for
global energy sustainability.

</details>


### [88] [UniCO: Towards a Unified Model for Combinatorial Optimization Problems](https://arxiv.org/abs/2505.06290)
*Zefang Zong,Xiaochen Wei,Guozhen Zhang,Chen Gao,Huandong Wang,Yong Li*

Main category: cs.LG

TL;DR: UniCO是一个统一的组合优化模型，通过将问题解决过程转化为马尔可夫决策过程（MDP）并利用Transformer架构，能够在少量微调下泛化到新的未见问题。


<details>
  <summary>Details</summary>
Motivation: 当前学习型方法多为特定组合优化问题设计，缺乏通用性。UniCO旨在提供一个单一架构和参数集的统一模型，提升效率和便捷性。

Method: 将问题解决过程建模为MDP，对序列轨迹数据进行标记化，使用Transformer骨干和两阶段自监督学习（动态预测模型预训练后生成策略）。

Result: 在10个组合优化问题上验证了UniCO的通用性，展示了其少量样本甚至零样本学习能力。

Conclusion: UniCO为专注于单个问题性能优化的现有神经组合优化方法提供了有价值的补充。

Abstract: Combinatorial Optimization (CO) encompasses a wide range of problems that
arise in many real-world scenarios. While significant progress has been made in
developing learning-based methods for specialized CO problems, a unified model
with a single architecture and parameter set for diverse CO problems remains
elusive. Such a model would offer substantial advantages in terms of efficiency
and convenience. In this paper, we introduce UniCO, a unified model for solving
various CO problems. Inspired by the success of next-token prediction, we frame
each problem-solving process as a Markov Decision Process (MDP), tokenize the
corresponding sequential trajectory data, and train the model using a
transformer backbone. To reduce token length in the trajectory data, we propose
a CO-prefix design that aggregates static problem features. To address the
heterogeneity of state and action tokens within the MDP, we employ a two-stage
self-supervised learning approach. In this approach, a dynamic prediction model
is first trained and then serves as a pre-trained model for subsequent policy
generation. Experiments across 10 CO problems showcase the versatility of
UniCO, emphasizing its ability to generalize to new, unseen problems with
minimal fine-tuning, achieving even few-shot or zero-shot performance. Our
framework offers a valuable complement to existing neural CO methods that focus
on optimizing performance for individual problems.

</details>


### [89] [Spatio-Temporal Graph Neural Network for Urban Spaces: Interpolating Citywide Traffic Volume](https://arxiv.org/abs/2505.06292)
*Silke K. Kaiser,Filipe Rodrigues,Carlos Lima Azevedo,Lynn H. Kaack*

Main category: cs.LG

TL;DR: 该论文提出了一种基于图神经网络的城市交通量插值方法（GNNUI），旨在解决城市网络中交通量估计的挑战，包括结构多样性、零膨胀分布和稀疏传感器覆盖。GNNUI通过掩码算法学习插值，整合节点特征以捕获功能角色，并使用针对零膨胀分布的损失函数。实验表明，GNNUI在两种新的大规模城市交通量基准测试中表现优异，且在极端稀疏数据下仍保持稳健。


<details>
  <summary>Details</summary>
Motivation: 城市交通量数据的稀缺性限制了城市规划的决策支持，尤其是在多模式交通网络中。尽管图神经网络在交通量预测中表现优异，但在城市环境中应用时面临结构多样性、零膨胀分布和稀疏传感器覆盖等独特挑战。因此，需要一种专门的方法来有效估计城市交通量。

Method: 论文提出了GNNUI方法，其核心包括：（1）使用掩码算法学习插值；（2）整合节点特征以捕获交通网络的功能角色；（3）设计针对零膨胀分布的损失函数。此外，论文还引入了两个新的大规模城市交通量基准测试（Strava骑行数据和纽约出租车数据）用于验证。

Result: GNNUI在MAE、RMSE、真实零率等指标上均优于现有方法，即使在传感器覆盖率为1%的极端稀疏情况下仍表现稳健（Strava的MAE从7.1升至10.5，Taxi的MAE从23.0升至40.4）。研究还探讨了图连接选择对模型精度的影响。

Conclusion: GNNUI为城市交通量估计提供了一种高效且稳健的解决方案，特别适用于传感器覆盖极稀疏的现实场景。其方法设计和基准数据集的引入为后续研究提供了重要参考。

Abstract: Reliable street-level traffic volume data, covering multiple modes of
transportation, helps urban planning by informing decisions on infrastructure
improvements, traffic management, and public transportation. Yet, traffic
sensors measuring traffic volume are typically scarcely located, due to their
high deployment and maintenance costs. To address this, interpolation methods
can estimate traffic volumes at unobserved locations using available data.
Graph Neural Networks have shown strong performance in traffic volume
forecasting, particularly on highways and major arterial networks. Applying
them to urban settings, however, presents unique challenges: urban networks
exhibit greater structural diversity, traffic volumes are highly overdispersed
with many zeros, the best way to account for spatial dependencies remains
unclear, and sensor coverage is often very sparse. We introduce the Graph
Neural Network for Urban Interpolation (GNNUI), a novel urban traffic volume
estimation approach. GNNUI employs a masking algorithm to learn interpolation,
integrates node features to capture functional roles, and uses a loss function
tailored to zero-inflated traffic distributions. In addition to the model, we
introduce two new open, large-scale urban traffic volume benchmarks, covering
different transportation modes: Strava cycling data from Berlin and New York
City taxi data. GNNUI outperforms recent, some graph-based, interpolation
methods across metrics (MAE, RMSE, true-zero rate, Kullback-Leibler divergence)
and remains robust from 90% to 1% sensor coverage. On Strava, for instance, MAE
rises only from 7.1 to 10.5, on Taxi from 23.0 to 40.4, demonstrating strong
performance under extreme data scarcity, common in real-world urban settings.
We also examine how graph connectivity choices influence model accuracy.

</details>


### [90] [Benchmarking Traditional Machine Learning and Deep Learning Models for Fault Detection in Power Transformers](https://arxiv.org/abs/2505.06295)
*Bhuvan Saravanan,Pasanth Kumar M D,Aarnesh Vengateson*

Main category: cs.LG

TL;DR: 该论文对传统机器学习（ML）和深度学习（DL）算法在电力变压器故障分类中的性能进行了比较分析，结果表明两者表现相近，随机森林（RF）和1D-CNN模型分别取得了最高准确率。


<details>
  <summary>Details</summary>
Motivation: 电力变压器故障的准确诊断对电力系统的稳定和安全至关重要，因此需要比较不同算法的性能以选择最佳方法。

Method: 研究使用了10个月的监测数据集，对气体浓度特征进行归一化后，训练了5种ML分类器（SVM、KNN、RF、XGBoost、ANN）和4种DL模型（LSTM、GRU、1D-CNN、TabNet）。

Result: 实验显示，ML和DL方法表现相当，其中RF模型的准确率最高（86.82%），1D-CNN模型次之（86.30%）。

Conclusion: 研究表明，传统机器学习和深度学习在变压器故障分类中均有效，可根据实际需求选择合适算法。

Abstract: Accurate diagnosis of power transformer faults is essential for ensuring the
stability and safety of electrical power systems. This study presents a
comparative analysis of conventional machine learning (ML) algorithms and deep
learning (DL) algorithms for fault classification of power transformers. Using
a condition-monitored dataset spanning 10 months, various gas concentration
features were normalized and used to train five ML classifiers: Support Vector
Machine (SVM), k-Nearest Neighbors (KNN), Random Forest (RF), XGBoost, and
Artificial Neural Network (ANN). In addition, four DL models were evaluated:
Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), One-Dimensional
Convolutional Neural Network (1D-CNN), and TabNet. Experimental results show
that both ML and DL approaches performed comparably. The RF model achieved the
highest ML accuracy at 86.82%, while the 1D-CNN model attained a close 86.30%.

</details>


### [91] [Lossless Compression of Large Language Model-Generated Text via Next-Token Prediction](https://arxiv.org/abs/2505.06297)
*Yu Mao,Holger Pirk,Chun Jason Xue*

Main category: cs.LG

TL;DR: 该论文探讨了针对大语言模型（LLM）生成数据的无损压缩技术，发现LLM能高效压缩自身输出，压缩率远超传统工具如Gzip。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成数据的快速增长，其复杂的多样性使得传统压缩方法效率低下，因此需要专为LLM生成数据设计的新压缩技术。

Method: 研究首次系统性探索了基于LLM预测的无损压缩方法，利用LLM自身对生成数据的可预测性。实验覆盖14种LLM和8个数据集。

Result: LLM压缩方法实现了超过20倍的压缩率，显著优于Gzip的3倍，且在不同LLM规模和数据类型中均表现稳健。

Conclusion: LLM作为自身输出的压缩器具有高效性和普适性，为生成式AI工作负载下的无损文本压缩提供了实用解决方案。

Abstract: As large language models (LLMs) continue to be deployed and utilized across
domains, the volume of LLM-generated data is growing rapidly. This trend
highlights the increasing importance of effective and lossless compression for
such data in modern text management systems. However, compressing LLM-generated
data presents unique challenges compared to traditional human- or
machine-generated content. Traditional machine-generated data is typically
derived from computational processes or device outputs, often highly structured
and limited to low-level elements like labels or numerical values. This
structure enables conventional lossless compressors to perform efficiently. In
contrast, LLM-generated data is more complex and diverse, requiring new
approaches for effective compression. In this work, we conduct the first
systematic investigation of lossless compression techniques tailored
specifically to LLM-generated data. Notably, because LLMs are trained via
next-token prediction, we find that LLM-generated data is highly predictable
for the models themselves. This predictability enables LLMs to serve as
efficient compressors of their own outputs. Through extensive experiments with
14 representative LLMs and 8 LLM-generated datasets from diverse domains, we
show that LLM-based prediction methods achieve remarkable compression rates,
exceeding 20x, far surpassing the 3x rate achieved by Gzip, a widely used
general-purpose compressor. Furthermore, this advantage holds across different
LLM sizes and dataset types, demonstrating the robustness and practicality of
LLM-based methods in lossless text compression under generative AI workloads.

</details>


### [92] [ARDNS-FN-Quantum: A Quantum-Enhanced Reinforcement Learning Framework with Cognitive-Inspired Adaptive Exploration for Dynamic Environments](https://arxiv.org/abs/2505.06300)
*Umberto Gonçalves de Sousa*

Main category: cs.LG

TL;DR: 提出了一种结合量子计算和认知科学的强化学习框架ARDNS-FN-Quantum，在探索效率、稳定性和适应性上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统强化学习算法（如DQN和PPO）在动态环境中探索效率低、稳定性差和适应性不足的问题。

Method: 整合了2量子比特电路用于动作选择、受人类认知启发的双记忆系统和基于奖励方差与好奇心的自适应探索策略。

Result: 在10X10网格世界中，ARDNS-FN-Quantum的成功率为99.5%，平均奖励9.0528，平均步数46.7，均显著优于DQN和PPO。

Conclusion: ARDNS-FN-Quantum为动态环境中的自适应学习提供了可扩展的解决方案，适用于机器人、自主系统和不确定性决策领域。

Abstract: Reinforcement learning (RL) has transformed sequential decision making, yet
traditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy
Optimization (PPO) often struggle with efficient exploration, stability, and
adaptability in dynamic environments. This study presents ARDNS-FN-Quantum
(Adaptive Reward-Driven Neural Simulator with Quantum enhancement), a novel
framework that integrates a 2-qubit quantum circuit for action selection, a
dual-memory system inspired by human cognition, and adaptive exploration
strategies modulated by reward variance and curiosity. Evaluated in a 10X10
grid-world over 20,000 episodes, ARDNS-FN-Quantum achieves a 99.5% success rate
(versus 81.3% for DQN and 97.0% for PPO), a mean reward of 9.0528 across all
episodes (versus 1.2941 for DQN and 7.6196 for PPO), and an average of 46.7
steps to goal (versus 135.9 for DQN and 62.5 for PPO). In the last 100
episodes, it records a mean reward of 9.1652 (versus 7.0916 for DQN and 9.0310
for PPO) and 37.2 steps to goal (versus 52.7 for DQN and 53.4 for PPO).
Graphical analyses, including learning curves, steps-to-goal trends, reward
variance, and reward distributions, demonstrate ARDNS-FN-Quantum's superior
stability (reward variance 5.424 across all episodes versus 252.262 for DQN and
76.583 for PPO) and efficiency. By bridging quantum computing, cognitive
science, and RL, ARDNS-FN-Quantum offers a scalable, human-like approach to
adaptive learning in uncertain environments, with potential applications in
robotics, autonomous systems, and decision-making under uncertainty.

</details>


### [93] [Domain-Adversarial Anatomical Graph Networks for Cross-User Human Activity Recognition](https://arxiv.org/abs/2505.06301)
*Xiaozhou Ye,Kevin I-Kai Wang*

Main category: cs.LG

TL;DR: 论文提出了一种基于图神经网络的EEG-ADG框架，通过整合解剖学知识和对抗域泛化技术，解决了跨用户人体活动识别（HAR）中的变异性问题，并在实验中取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 跨用户HAR中存在传感器放置、身体动态和行为模式的差异，传统方法难以捕捉跨用户的生物力学不变性，泛化能力有限。

Method: 提出了EEG-ADG框架，结合了三类生物力学关系（互联单元、类比单元和侧向单元），通过变分边特征提取器和梯度反转层实现对抗域泛化。

Result: 在OPPORTUNITY和DSADS数据集上的实验表明，该方法达到了当前最优性能。

Conclusion: 该研究通过将生物力学原理与图对抗学习结合，提出了一种统一且泛化的跨用户HAR模型。

Abstract: Cross-user variability in Human Activity Recognition (HAR) remains a critical
challenge due to differences in sensor placement, body dynamics, and behavioral
patterns. Traditional methods often fail to capture biomechanical invariants
that persist across users, limiting their generalization capability. We propose
an Edge-Enhanced Graph-Based Adversarial Domain Generalization (EEG-ADG)
framework that integrates anatomical correlation knowledge into a unified graph
neural network (GNN) architecture. By modeling three biomechanically motivated
relationships together-Interconnected Units, Analogous Units, and Lateral
Units-our method encodes domain-invariant features while addressing
user-specific variability through Variational Edge Feature Extractor. A
Gradient Reversal Layer (GRL) enforces adversarial domain generalization,
ensuring robustness to unseen users. Extensive experiments on OPPORTUNITY and
DSADS datasets demonstrate state-of-the-art performance. Our work bridges
biomechanical principles with graph-based adversarial learning by integrating
information fusion techniques. This fusion of information underpins our unified
and generalized model for cross-user HAR.

</details>


### [94] [QiMeng-TensorOp: Automatically Generating High-Performance Tensor Operators with Hardware Primitives](https://arxiv.org/abs/2505.06302)
*Xuzhi Zhang,Shaohui Peng,Qirui Zhou,Yuanbo Wen,Qi Guo,Ruizhi Chen,Xinguo Zhu,Weiqiang Xiong,Haixin Chen,Congying Ma,Ke Gao,Chen Zhao,Yanjun Wu,Yunji Chen,Ling Li*

Main category: cs.LG

TL;DR: QiMeng-TensorOp 是一个基于 LLMs 的框架，能够通过一行用户提示自动生成高性能张量算子，利用硬件特性优化参数，显著提升计算性能并降低开发成本。


<details>
  <summary>Details</summary>
Motivation: 手工优化的张量算子开发周期长且缺乏可移植性，而现有的 LLMs 难以充分理解硬件特性来生成高性能代码。因此，需要一个自动生成并优化张量算子的工具以适配多样化硬件。

Method: 提出了 QiMeng-TensorOp 框架，通过 LLMs 自动分析硬件特性并生成高性能张量算子，同时调优参数以适配不同硬件平台。

Result: 在多种硬件平台上，QiMeng-TensorOp 生成的算子性能显著优于普通 LLMs 和人工优化方案（如 1291 倍性能提升），同时开发成本降低 200 倍。

Conclusion: QiMeng-TensorOp 有效解决了张量算子自动生成的难题，为 AI 模型的硬件适配和高性能计算提供了可行方案。

Abstract: Computation-intensive tensor operators constitute over 90\% of the
computations in Large Language Models (LLMs) and Deep Neural
Networks.Automatically and efficiently generating high-performance tensor
operators with hardware primitives is crucial for diverse and ever-evolving
hardware architectures like RISC-V, ARM, and GPUs, as manually optimized
implementation takes at least months and lacks portability.LLMs excel at
generating high-level language codes, but they struggle to fully comprehend
hardware characteristics and produce high-performance tensor operators. We
introduce a tensor-operator auto-generation framework with a one-line user
prompt (QiMeng-TensorOp), which enables LLMs to automatically exploit hardware
characteristics to generate tensor operators with hardware primitives, and tune
parameters for optimal performance across diverse hardware. Experimental
results on various hardware platforms, SOTA LLMs, and typical tensor operators
demonstrate that QiMeng-TensorOp effectively unleashes the computing capability
of various hardware platforms, and automatically generates tensor operators of
superior performance. Compared with vanilla LLMs, QiMeng-TensorOp achieves up
to $1291 \times$ performance improvement. Even compared with human experts,
QiMeng-TensorOp could reach $251 \%$ of OpenBLAS on RISC-V CPUs, and $124 \%$
of cuBLAS on NVIDIA GPUs. Additionally, QiMeng-TensorOp also significantly
reduces development costs by $200 \times$ compared with human experts.

</details>


### [95] [Collaborative Multi-LoRA Experts with Achievement-based Multi-Tasks Loss for Unified Multimodal Information Extraction](https://arxiv.org/abs/2505.06303)
*Li Yuan,Yi Cai,Xudong Shen,Qing Li,Qingbao Huang,Zikun Deng,Tao Wang*

Main category: cs.LG

TL;DR: 论文提出了C-LoRAE方法，通过结合通用专家和任务特定专家的多LoRA架构，以及基于成就的多任务损失，解决多模态信息提取（MIE）任务中的梯度冲突和计算效率问题，实验表明其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统MIE方法任务独立处理，缺乏知识共享，而现有基于T5的统一生成方法计算成本高且存在梯度冲突。

Method: C-LoRAE结合通用专家学习跨任务共享的多模态知识，任务特定专家学习具体任务特征，并使用基于成就的多任务损失平衡训练进度。

Result: 在7个基准数据集上，C-LoRAE表现优于传统微调和LoRA方法，且参数量与LoRA相当。

Conclusion: C-LoRAE有效提升了多任务MIE的性能和效率，为多模态任务提供了新思路。

Abstract: Multimodal Information Extraction (MIE) has gained attention for extracting
structured information from multimedia sources. Traditional methods tackle MIE
tasks separately, missing opportunities to share knowledge across tasks. Recent
approaches unify these tasks into a generation problem using instruction-based
T5 models with visual adaptors, optimized through full-parameter fine-tuning.
However, this method is computationally intensive, and multi-task fine-tuning
often faces gradient conflicts, limiting performance. To address these
challenges, we propose collaborative multi-LoRA experts with achievement-based
multi-task loss (C-LoRAE) for MIE tasks. C-LoRAE extends the low-rank
adaptation (LoRA) method by incorporating a universal expert to learn shared
multimodal knowledge from cross-MIE tasks and task-specific experts to learn
specialized instructional task features. This configuration enhances the
model's generalization ability across multiple tasks while maintaining the
independence of various instruction tasks and mitigating gradient conflicts.
Additionally, we propose an achievement-based multi-task loss to balance
training progress across tasks, addressing the imbalance caused by varying
numbers of training samples in MIE tasks. Experimental results on seven
benchmark datasets across three key MIE tasks demonstrate that C-LoRAE achieves
superior overall performance compared to traditional fine-tuning methods and
LoRA methods while utilizing a comparable number of training parameters to
LoRA.

</details>


### [96] [GraphComp: Extreme Error-bounded Compression of Scientific Data via Temporal Graph Autoencoders](https://arxiv.org/abs/2505.06316)
*Guozhong Li,Muhannad Alhumaidi,Spiros Skiadopoulos,Ibrahim Hoteit,Panos Kalnis*

Main category: cs.LG

TL;DR: 论文提出了GRAPHCOMP，一种基于图的误差有界有损压缩方法，利用时空相关性实现高压缩比，并在多种数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 科学数据量大，现有压缩方法未能充分利用时空相关性，导致压缩效率不高。

Method: 通过不规则分割原始网格数据并生成图表示，结合时序图自编码器学习潜在表示以实现高效压缩。

Result: GRAPHCOMP在大多数数据集上实现了最高的压缩比，比第二优方法提升22%-50%。

Conclusion: GRAPHCOMP通过利用时空相关性和图表示，显著提升了科学数据的压缩效率。

Abstract: The generation of voluminous scientific data poses significant challenges for
efficient storage, transfer, and analysis. Recently, error-bounded lossy
compression methods emerged due to their ability to achieve high compression
ratios while controlling data distortion. However, they often overlook the
inherent spatial and temporal correlations within scientific data, thus missing
opportunities for higher compression. In this paper we propose GRAPHCOMP, a
novel graph-based method for error-bounded lossy compression of scientific
data. We perform irregular segmentation of the original grid data and generate
a graph representation that preserves the spatial and temporal correlations.
Inspired by Graph Neural Networks (GNNs), we then propose a temporal graph
autoencoder to learn latent representations that significantly reduce the size
of the graph, effectively compressing the original data. Decompression reverses
the process and utilizes the learnt graph model together with the latent
representation to reconstruct an approximation of the original data. The
decompressed data are guaranteed to satisfy a user-defined point-wise error
bound. We compare our method against the state-of-the-art error-bounded lossy
methods (i.e., HPEZ, SZ3.1, SPERR, and ZFP) on large-scale real and synthetic
data. GRAPHCOMP consistently achieves the highest compression ratio across most
datasets, outperforming the second-best method by margins ranging from 22% to
50%.

</details>


### [97] [Reinforcement Learning for Game-Theoretic Resource Allocation on Graphs](https://arxiv.org/abs/2505.06319)
*Zijian An,Lifeng Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种基于强化学习的方法来解决多步Colonel Blotto游戏中的资源分配问题，通过将问题建模为马尔可夫决策过程并应用DQN和PPO算法，在各种图结构和初始资源分布下均优于基准策略。


<details>
  <summary>Details</summary>
Motivation: 解决在多步Colonel Blotto游戏中，由于动态动作空间和图的结构约束导致的寻找最优策略的挑战。

Method: 将多步Colonel Blotto游戏建模为马尔可夫决策过程（MDP），并应用深度Q网络（DQN）和近端策略优化（PPO）算法，引入动作-位移邻接矩阵以动态生成有效动作集。

Result: 实验表明，DQN和PPO在多种图结构和初始资源分布下均优于随机、贪心和已学习RL策略，并在对抗学习策略时达到50%的均衡胜率。在非对称图中，RL代理能有效利用结构优势并调整分配策略。

Conclusion: 论文通过强化学习方法成功解决了多步Colonel Blotto游戏中的资源分配问题，验证了RL在处理动态约束问题上的有效性。

Abstract: Game-theoretic resource allocation on graphs (GRAG) involves two players
competing over multiple steps to control nodes of interest on a graph, a
problem modeled as a multi-step Colonel Blotto Game (MCBG). Finding optimal
strategies is challenging due to the dynamic action space and structural
constraints imposed by the graph. To address this, we formulate the MCBG as a
Markov Decision Process (MDP) and apply Reinforcement Learning (RL) methods,
specifically Deep Q-Network (DQN) and Proximal Policy Optimization (PPO). To
enforce graph constraints, we introduce an action-displacement adjacency matrix
that dynamically generates valid action sets at each step. We evaluate RL
performance across a variety of graph structures and initial resource
distributions, comparing against random, greedy, and learned RL policies.
Experimental results show that both DQN and PPO consistently outperform
baseline strategies and converge to a balanced $50\%$ win rate when competing
against the learned RL policy. Particularly, on asymmetric graphs, RL agents
successfully exploit structural advantages and adapt their allocation
strategies, even under disadvantageous initial resource distributions.

</details>


### [98] [Divide (Text) and Conquer (Sentiment): Improved Sentiment Classification by Constituent Conflict Resolution](https://arxiv.org/abs/2505.06320)
*Jan Kościałkowski,Paweł Marcinkowski*

Main category: cs.LG

TL;DR: 本文提出了一种新方法，通过多层感知器（MLP）模型有效分离和聚合多语调段落中的情感，显著提升了情感分类性能，且成本远低于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言处理中多语调段落情感分类的挑战，尤其是长段落中冲突情感的准确预测问题。

Method: 采用多层感知器（MLP）模型进行情感隔离和聚合，对比基线模型在Amazon、Twitter和SST数据集上的表现。

Result: MLP模型在多个数据集上优于基线模型，且成本仅为微调基线的1/100。

Conclusion: 提出的MLP方法高效且经济，适用于复杂情感分类任务。

Abstract: Sentiment classification, a complex task in natural language processing,
becomes even more challenging when analyzing passages with multiple conflicting
tones. Typically, longer passages exacerbate this issue, leading to decreased
model performance. The aim of this paper is to introduce novel methodologies
for isolating conflicting sentiments and aggregating them to effectively
predict the overall sentiment of such passages. One of the aggregation
strategies involves a Multi-Layer Perceptron (MLP) model which outperforms
baseline models across various datasets, including Amazon, Twitter, and SST
while costing $\sim$1/100 of what fine-tuning the baseline would take.

</details>


### [99] [Learn to Think: Bootstrapping LLM Reasoning Capability Through Graph Learning](https://arxiv.org/abs/2505.06321)
*Hang Gao,Chenhao Zhang,Tie Wang,Junsuo Zhao,Fengge Wu,Changwen Zheng,Huaping Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种利用图学习增强大语言模型（LLMs）灵活性和适应性推理能力的新框架，通过建模推理过程为图并结合GNN模块实现实时调整，显著提升了多任务推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在推理任务中依赖任务特定提示和预定义流程，限制了灵活性和泛化能力，论文旨在通过图学习方法解决这些问题。

Method: 将问题推理过程建模为图，通过LLM-based图学习自适应生成推理步骤，并引入GNN模块进行表示学习以实现实时调整。

Result: 实验表明，该方法在无需额外训练或任务特定提示设计的情况下，显著提升了多任务的推理性能。

Conclusion: 通过图学习与LLMs结合，实现了更灵活、自适应的推理能力，为LLMs的复杂任务处理提供了新思路。

Abstract: Large Language Models (LLMs) have achieved remarkable success across various
domains. However, they still face significant challenges, including high
computational costs for training and limitations in solving complex reasoning
problems. Although existing methods have extended the reasoning capabilities of
LLMs through structured paradigms, these approaches often rely on task-specific
prompts and predefined reasoning processes, which constrain their flexibility
and generalizability. To address these limitations, we propose a novel
framework that leverages graph learning to enable more flexible and adaptive
reasoning capabilities for LLMs. Specifically, this approach models the
reasoning process of a problem as a graph and employs LLM-based graph learning
to guide the adaptive generation of each reasoning step. To further enhance the
adaptability of the model, we introduce a Graph Neural Network (GNN) module to
perform representation learning on the generated reasoning process, enabling
real-time adjustments to both the model and the prompt. Experimental results
demonstrate that this method significantly improves reasoning performance
across multiple tasks without requiring additional training or task-specific
prompt design. Code can be found in https://github.com/zch65458525/L2T.

</details>


### [100] [Human in the Latent Loop (HILL): Interactively Guiding Model Training Through Human Intuition](https://arxiv.org/abs/2505.06325)
*Daniel Geissler,Lars Krupp,Vishal Banwari,David Habusch,Bo Zhou,Paul Lukowicz,Jakob Karolus*

Main category: cs.LG

TL;DR: HILL是一个交互式框架，通过将人类直觉融入模型训练，调整潜在空间表示以提升模型性能，但需注意潜在的用户偏见。


<details>
  <summary>Details</summary>
Motivation: 潜在空间表示对理解和改进机器学习模型行为至关重要，但通常复杂难懂。结合人类直觉可优化模型训练。

Method: 采用知识蒸馏的启发式方法，将用户调整作为教师信号指导模型重塑潜在表示，并通过用户研究评估效果。

Result: 人类指导的潜在空间修改提升了模型性能并保持泛化性，但也揭示了引入用户偏见的风险。

Conclusion: HILL开创了人机交互新范式，将人类直觉融入训练，同时需警惕干预带来的偏见影响。

Abstract: Latent space representations are critical for understanding and improving the
behavior of machine learning models, yet they often remain obscure and
intricate. Understanding and exploring the latent space has the potential to
contribute valuable human intuition and expertise about respective domains. In
this work, we present HILL, an interactive framework allowing users to
incorporate human intuition into the model training by interactively reshaping
latent space representations. The modifications are infused into the model
training loop via a novel approach inspired by knowledge distillation, treating
the user's modifications as a teacher to guide the model in reshaping its
intrinsic latent representation. The process allows the model to converge more
effectively and overcome inefficiencies, as well as provide beneficial insights
to the user. We evaluated HILL in a user study tasking participants to train an
optimal model, closely observing the employed strategies. The results
demonstrated that human-guided latent space modifications enhance model
performance while maintaining generalization, yet also revealing the risks of
including user biases. Our work introduces a novel human-AI interaction
paradigm that infuses human intuition into model training and critically
examines the impact of human intervention on training strategies and potential
biases.

</details>


### [101] [Prompting Large Language Models for Training-Free Non-Intrusive Load Monitoring](https://arxiv.org/abs/2505.06330)
*Junyu Xue,Xudong Wang,Xiaoling He,Shicheng Liu,Yi Wang,Guoming Tang*

Main category: cs.LG

TL;DR: 本篇论文提出了首个基于提示的大语言模型（LLM）框架用于非侵入式负荷监测（NILM），通过上下文学习整合设备特征、时间戳和代表性时间序列示例，在REDD数据集上验证了其竞争性的状态检测准确度（平均F1-score 0.676）和强泛化能力，同时提升了可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统NILM依赖标注数据且泛化能力和可解释性不足，研究旨在利用LLM减少数据需求、提升适应性和解释性。

Method: 设计基于LLM的提示框架，结合设备特征、时间戳和上下文信息，并采用REDD数据集优化提示策略。

Result: LLM在未见过家庭中平均F1-score达0.676，无需微调即可泛化，并提供人类可读的预测解释。

Conclusion: LLM通过提示策略显著降低了NILM对数据的需求，同时提升了泛化能力和解释性，为能源分解提供了透明方案。

Abstract: Non-intrusive Load Monitoring (NILM) aims to disaggregate aggregate household
electricity consumption into individual appliance usage, enabling more
effective energy management. While deep learning has advanced NILM, it remains
limited by its dependence on labeled data, restricted generalization, and lack
of interpretability. In this paper, we introduce the first prompt-based NILM
framework that leverages Large Language Models (LLMs) with in-context learning.
We design and evaluate prompt strategies that integrate appliance features,
timestamps and contextual information, as well as representative time-series
examples, using the REDD dataset. With optimized prompts, LLMs achieve
competitive state detection accuracy, reaching an average F1-score of 0.676 on
unseen households, and demonstrate robust generalization without the need for
fine-tuning. LLMs also enhance interpretability by providing clear,
human-readable explanations for their predictions. Our results show that LLMs
can reduce data requirements, improve adaptability, and provide transparent
energy disaggregation in NILM applications.

</details>


### [102] [Mask-PINNs: Regulating Feature Distributions in Physics-Informed Neural Networks](https://arxiv.org/abs/2505.06331)
*Feilong Jiang,Xiaonan Hou,Jianqiao Ye,Min Xia*

Main category: cs.LG

TL;DR: 本文提出了一种名为Mask-PINNs的新架构，用于解决物理信息神经网络（PINNs）中内部协变量偏移的问题，从而更有效地利用神经网络容量。


<details>
  <summary>Details</summary>
Motivation: PINNs通过将物理定律直接融入损失函数来解决偏微分方程，但内部协变量偏移问题长期被忽视，阻碍了神经网络容量的有效利用。

Method: 作者提出了一种可学习的非线性掩码函数，用于在不违反物理定律的前提下约束特征分布，取代了传统的归一化方法如BatchNorm或LayerNorm。

Result: 实验结果表明，该方法显著提高了特征分布的稳定性、准确性和鲁棒性，同时还实现了更宽网络的稳定高效训练。

Conclusion: Mask-PINNs有效解决了PINNs中的内部协变量偏移问题，为更高效和鲁棒的神经网络训练提供了新途径。

Abstract: Physics-Informed Neural Networks (PINNs) are a class of deep learning models
designed to solve partial differential equations by incorporating physical laws
directly into the loss function. However, the internal covariate shift, which
has been largely overlooked, hinders the effective utilization of neural
network capacity in PINNs. To this end, we propose Mask-PINNs, a novel
architecture designed to address this issue in PINNs. Unlike traditional
normalization methods such as BatchNorm or LayerNorm, we introduce a learnable,
nonlinear mask function that constrains the feature distributions without
violating underlying physics. The experimental results show that the proposed
method significantly improves feature distribution stability, accuracy, and
robustness across various activation functions and PDE benchmarks. Furthermore,
it enables the stable and efficient training of wider networks a capability
that has been largely overlooked in PINNs.

</details>


### [103] [NSF-MAP: Neurosymbolic Multimodal Fusion for Robust and Interpretable Anomaly Prediction in Assembly Pipelines](https://arxiv.org/abs/2505.06333)
*Chathurangi Shyalika,Renjith Prasad,Fadi El Kalach,Revathy Venkataramanan,Ramtin Zand,Ramy Harik,Amit Sheth*

Main category: cs.LG

TL;DR: 本文提出一种基于神经符号AI和多模态融合的方法，用于装配流水线中的异常预测，结合时间序列和图像数据，通过决策级融合、迁移学习和知识注入学习，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现代装配流水线中，传统单模态方法无法捕捉复杂数据和多模态环境中的异常关系，因此需要一种更精确的多模态异常预测方法。

Method: 采用时间序列与图像的决策级融合建模、迁移学习和知识注入学习，构建了神经符号AI和多模态融合模型。

Result: 实验表明，该方法能有效结合时间序列和图像数据的互补优势，显著提升异常预测性能，并提供可解释性。

Conclusion: 神经符号AI与多模态融合的方法为装配流水线异常预测提供了高效且可解释的解决方案，实验数据和代码已公开。

Abstract: In modern assembly pipelines, identifying anomalies is crucial in ensuring
product quality and operational efficiency. Conventional single-modality
methods fail to capture the intricate relationships required for precise
anomaly prediction in complex predictive environments with abundant data and
multiple modalities. This paper proposes a neurosymbolic AI and fusion-based
approach for multimodal anomaly prediction in assembly pipelines. We introduce
a time series and image-based fusion model that leverages decision-level fusion
techniques. Our research builds upon three primary novel approaches in
multimodal learning: time series and image-based decision-level fusion
modeling, transfer learning for fusion, and knowledge-infused learning. We
evaluate the novel method using our derived and publicly available multimodal
dataset and conduct comprehensive ablation studies to assess the impact of our
preprocessing techniques and fusion model compared to traditional baselines.
The results demonstrate that a neurosymbolic AI-based fusion approach that uses
transfer learning can effectively harness the complementary strengths of time
series and image data, offering a robust and interpretable approach for anomaly
prediction in assembly pipelines with enhanced performance. \noindent The
datasets, codes to reproduce the results, supplementary materials, and demo are
available at https://github.com/ChathurangiShyalika/NSF-MAP.

</details>


### [104] [Remote Rowhammer Attack using Adversarial Observations on Federated Learning Clients](https://arxiv.org/abs/2505.06335)
*Jinsheng Yuan,Yuhang Hao,Weisi Guo,Yun Wu,Chongyan Gu*

Main category: cs.LG

TL;DR: 该论文提出一种通过联邦学习（FL）客户端攻击服务器内存的新方法，利用强化学习（RL）操纵客户端观测，触发服务器高频重复内存更新，实现远程Rowhammer攻击，导致服务器内存位翻转，破坏学习过程或提升权限。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索联邦学习环境中服务器的安全隐患，尤其是针对服务器内存的远程攻击可能性，填补了现有研究多关注客户端隐私保护而忽略服务器安全的空白。

Method: 方法是通过强化学习训练攻击代理，使其能够识别并操控特定客户端，触发服务器高频内存更新，从而实施远程Rowhammer攻击。实验基于大规模FL自动语音识别系统，评估攻击效果。

Result: 实验结果显示，攻击代理能实现约70%的重复更新率（RUR），成功在服务器DRAM上诱发位翻转，验证了攻击的可行性。

Conclusion: 结论指出这种攻击可能破坏学习过程或导致权限提升，呼吁未来研究关注联邦学习中的实用防御策略及硬件设计改进。

Abstract: Federated Learning (FL) has the potential for simultaneous global learning
amongst a large number of parallel agents, enabling emerging AI such as LLMs to
be trained across demographically diverse data. Central to this being efficient
is the ability for FL to perform sparse gradient updates and remote direct
memory access at the central server. Most of the research in FL security
focuses on protecting data privacy at the edge client or in the communication
channels between the client and server. Client-facing attacks on the server are
less well investigated as the assumption is that a large collective of clients
offer resilience.
  Here, we show that by attacking certain clients that lead to a high frequency
repetitive memory update in the server, we can remote initiate a rowhammer
attack on the server memory. For the first time, we do not need backdoor access
to the server, and a reinforcement learning (RL) attacker can learn how to
maximize server repetitive memory updates by manipulating the client's sensor
observation. The consequence of the remote rowhammer attack is that we are able
to achieve bit flips, which can corrupt the server memory. We demonstrate the
feasibility of our attack using a large-scale FL automatic speech recognition
(ASR) systems with sparse updates, our adversarial attacking agent can achieve
around 70\% repeated update rate (RUR) in the targeted server model,
effectively inducing bit flips on server DRAM. The security implications are
that can cause disruptions to learning or may inadvertently cause elevated
privilege. This paves the way for further research on practical mitigation
strategies in FL and hardware design.

</details>


### [105] [Latent Diffeomorphic Dynamic Mode Decomposition](https://arxiv.org/abs/2505.06351)
*Willem Diepeveen,Jon Schwenk,Andrea Bertozzi*

Main category: cs.LG

TL;DR: LDDMD结合了DMD的可解释性和RNN的预测能力，用于非线性系统的数据降维分析，并在水流预测中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了在保持可解释性的同时，提升对复杂非线性系统的建模和预测能力。

Method: 结合动态模式分解（DMD）和递归神经网络（RNN），提出Latent Diffeomorphic Dynamic Mode Decomposition（LDDMD）方法。

Result: LDDMD成功应用于水流预测，表现出准确的预测能力。

Conclusion: LDDMD在保持简单可解释性的同时，有效建模并学习具有记忆性的复杂非线性系统，展示了其在实际应用中的潜力。

Abstract: We present Latent Diffeomorphic Dynamic Mode Decomposition (LDDMD), a new
data reduction approach for the analysis of non-linear systems that combines
the interpretability of Dynamic Mode Decomposition (DMD) with the predictive
power of Recurrent Neural Networks (RNNs). Notably, LDDMD maintains simplicity,
which enhances interpretability, while effectively modeling and learning
complex non-linear systems with memory, enabling accurate predictions. This is
exemplified by its successful application in streamflow prediction.

</details>


### [106] [CAST: Time-Varying Treatment Effects with Application to Chemotherapy and Radiotherapy on Head and Neck Squamous Cell Carcinoma](https://arxiv.org/abs/2505.06367)
*Everest Yang,Ria Vasishtha,Luqman K. Dad,Lisa A. Kachnic,Andrew Hope,Eric Wang,Xiao Wu,Yading Yuan,David J. Brenner,Igor Shuryak*

Main category: cs.LG

TL;DR: CAST框架提出了一种连续时间点因果机器学习方法，克服了传统方法在生存分析中仅能评估固定时间点的局限性，通过结合参数和非参数方法，实现了治疗效果随时间动态变化的连续估计。


<details>
  <summary>Details</summary>
Motivation: 传统因果机器学习方法在处理医学生存数据时，仅能在固定时间点评估治疗效果，无法捕捉治疗效果的动态变化。这限制了其在个性化医疗中的应用，尤其是对于预后随时间变化的疾病（如头颈部鳞状细胞癌）。基于此，CAST框架旨在解决这一局限性。

Method: CAST结合参数和非参数方法，将治疗效果建模为治疗后的连续时间函数。具体而言，它利用RADCURE数据集中的2651例患者数据，通过动态轨迹分析揭示化疗和放疗效果在人群和个体层面的时间演变规律。

Result: 在头颈部鳞状细胞癌的实证分析中，CAST成功量化了治疗效果随时间上升、峰值和下降的动态趋势，帮助临床医生更精准地判断治疗获益的时机和适用人群。

Conclusion: CAST通过连续时间点的因果效应分析，推动了因果机器学习在个性化医疗（如HNSCC和其他致命性疾病）中的应用，为动态治疗决策提供了方法论支持。

Abstract: Causal machine learning (CML) enables individualized estimation of treatment
effects, offering critical advantages over traditional correlation-based
methods. However, existing approaches for medical survival data with censoring
such as causal survival forests estimate effects at fixed time points, limiting
their ability to capture dynamic changes over time. We introduce Causal
Analysis for Survival Trajectories (CAST), a novel framework that models
treatment effects as continuous functions of time following treatment. By
combining parametric and non-parametric methods, CAST overcomes the limitations
of discrete time-point analysis to estimate continuous effect trajectories.
Using the RADCURE dataset [1] of 2,651 patients with head and neck squamous
cell carcinoma (HNSCC) as a clinically relevant example, CAST models how
chemotherapy and radiotherapy effects evolve over time at the population and
individual levels. By capturing the temporal dynamics of treatment response,
CAST reveals how treatment effects rise, peak, and decline over the follow-up
period, helping clinicians determine when and for whom treatment benefits are
maximized. This framework advances the application of CML to personalized care
in HNSCC and other life-threatening medical conditions. Source code/data
available at: https://github.com/CAST-FW/HNSCC

</details>


### [107] [The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization](https://arxiv.org/abs/2505.06371)
*Jae-Won Chung,Jiachen Liu,Jeff J. Ma,Ruofan Wu,Oh Jun Kweon,Yuxuan Xia,Zhiyu Wu,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: 论文提出了ML.ENERGY Benchmark，用于测量生成式AI在真实服务环境中的推理能耗，并总结了四种关键设计原则。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在现实服务中的广泛应用，能源成为关键瓶颈资源，但ML系统的能耗问题常被忽视或未充分研究。

Method: 通过ML.ENERGY Benchmark工具和对应的Leaderboard，测量40种常用模型架构在6种任务中的能耗，并展示设计选择对能耗的影响。

Result: 研究揭示了自动化优化建议可带来显著（有时超过40%）的能源节约，同时不改变模型的计算结果。

Conclusion: ML.ENERGY Benchmark开源且易于扩展，为理解和优化生成式AI服务的能耗提供了宝贵资源。

Abstract: As the adoption of Generative AI in real-world services grow explosively,
energy has emerged as a critical bottleneck resource. However, energy remains a
metric that is often overlooked, under-explored, or poorly understood in the
context of building ML systems. We present the ML.ENERGY Benchmark, a benchmark
suite and tool for measuring inference energy consumption under realistic
service environments, and the corresponding ML.ENERGY Leaderboard, which have
served as a valuable resource for those hoping to understand and optimize the
energy consumption of their generative AI services. In this paper, we explain
four key design principles for benchmarking ML energy we have acquired over
time, and then describe how they are implemented in the ML.ENERGY Benchmark. We
then highlight results from the latest iteration of the benchmark, including
energy measurements of 40 widely used model architectures across 6 different
tasks, case studies of how ML design choices impact energy consumption, and how
automated optimization recommendations can lead to significant (sometimes more
than 40%) energy savings without changing what is being computed by the model.
The ML.ENERGY Benchmark is open-source and can be easily extended to various
customized models and application scenarios.

</details>


### [108] [RiM: Record, Improve and Maintain Physical Well-being using Federated Learning](https://arxiv.org/abs/2505.06384)
*Aditya Mishra,Haroon Lone*

Main category: cs.LG

TL;DR: 论文提出了一种名为RiM的移动应用，结合个性化机器学习框架和联邦学习，提升学生的身体健康状况。实验结果显示该模型在隐私保护条件下效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 学术压力导致学生忽视身体健康，但传统机器学习方法因隐私问题难以应用，因此研究提出了隐私保护的解决方案。

Method: 采用多层感知器预训练和联邦学习微调模型，仅共享模型权重而非原始数据以确保隐私。

Result: RiM模型平均准确率60.71%，平均绝对误差0.91，优于FedPer变体。

Conclusion: 联邦学习框架在保护隐私的同时有效提升学生健康预测的准确性。

Abstract: In academic settings, the demanding environment often forces students to
prioritize academic performance over their physical well-being. Moreover,
privacy concerns and the inherent risk of data breaches hinder the deployment
of traditional machine learning techniques for addressing these health
challenges. In this study, we introduce RiM: Record, Improve, and Maintain, a
mobile application which incorporates a novel personalized machine learning
framework that leverages federated learning to enhance students' physical
well-being by analyzing their lifestyle habits. Our approach involves
pre-training a multilayer perceptron (MLP) model on a large-scale simulated
dataset to generate personalized recommendations. Subsequently, we employ
federated learning to fine-tune the model using data from IISER Bhopal
students, thereby ensuring its applicability in real-world scenarios. The
federated learning approach guarantees differential privacy by exclusively
sharing model weights rather than raw data. Experimental results show that the
FedAvg-based RiM model achieves an average accuracy of 60.71% and a mean
absolute error of 0.91--outperforming the FedPer variant (average accuracy
46.34%, MAE 1.19)--thereby demonstrating its efficacy in predicting lifestyle
deficits under privacy-preserving constraints.

</details>


### [109] [Tweedie Regression for Video Recommendation System](https://arxiv.org/abs/2505.06445)
*Yan Zheng,Qiang Chen,Chenglei Niu*

Main category: cs.LG

TL;DR: 论文探讨了传统CTR预测在视频推荐中的局限性，提出使用回归方法（Tweedie Loss）优化用户观看时长及收入，验证其效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统过于关注CTR预测，忽视了视频点播服务的核心目标（如用户观看时长和收入）。需重新定义问题以更贴合实际业务需求。

Method: 采用回归方法（Tweedie Loss函数）替代分类任务，结合离线模拟和在线A/B测试验证模型效果。

Result: 实验表明，该方法显著提升用户观看时长及收入，且Tweedie Loss在理论对比中优于加权Logloss。

Conclusion: Tweedie回归为视频推荐提供高效解决方案，强调损失函数设计应聚焦单一核心目标。

Abstract: Modern recommendation systems aim to increase click-through rates (CTR) for
better user experience, through commonly treating ranking as a classification
task focused on predicting CTR. However, there is a gap between this method and
the actual objectives of businesses across different sectors. In video
recommendation services, the objective of video on demand (VOD) extends beyond
merely encouraging clicks, but also guiding users to discover their true
interests, leading to increased watch time. And longer users watch time will
leads to more revenue through increased chances of presenting online display
advertisements. This research addresses the issue by redefining the problem
from classification to regression, with a focus on maximizing revenue through
user viewing time. Due to the lack of positive labels on recommendation, the
study introduces Tweedie Loss Function, which is better suited in this scenario
than the traditional mean square error loss. The paper also provides insights
on how Tweedie process capture users diverse interests. Our offline simulation
and online A/B test revealed that we can substantially enhance our core
business objectives: user engagement in terms of viewing time and,
consequently, revenue. Additionally, we provide a theoretical comparison
between the Tweedie Loss and the commonly employed viewing time weighted
Logloss, highlighting why Tweedie Regression stands out as an efficient
solution. We further outline a framework for designing a loss function that
focuses on a singular objective.

</details>


### [110] [Structured Prediction with Abstention via the Lovász Hinge](https://arxiv.org/abs/2505.06446)
*Jessie Finocchiaro,Rafael Frongillo,Enrique Nueve*

Main category: cs.LG

TL;DR: Lovász铰链是一种用于二元结构化分类的凸损失函数，但其一致性未被证明。本文发现除非评估函数为模块化，否则Lovász铰链是不一致的，并提出了一种新的目标损失函数——“结构化弃权问题”。此外，还推导了一族链接函数，并在实验中展示了其潜力。


<details>
  <summary>Details</summary>
Motivation: 研究Lovász铰链的一致性，并提出一种新的目标损失函数以解决其不一致性问题，同时探索其在结构化分类任务中的可解释性。

Method: 利用Finocchiaro等人的嵌入框架，分析Lovász铰链的一致性，并推导出一族链接函数。通过实验验证结构化弃权问题的有效性。

Result: 发现Lovász铰链仅在评估函数为模块化时一致，提出了结构化弃权问题作为其一致的目标损失函数，并证明了其有效性。

Conclusion: 结构化弃权问题为Lovász铰链提供了一致性，并为结构化分类任务中的可解释性提供了新思路。

Abstract: The Lov\'asz hinge is a convex loss function proposed for binary structured
classification, in which k related binary predictions jointly evaluated by a
submodular function. Despite its prevalence in image segmentation and related
tasks, the consistency of the Lov\'asz hinge has remained open. We show that
the Lov\'asz hinge is inconsistent with its desired target unless the set
function used for evaluation is modular. Leveraging the embedding framework of
Finocchiaro et al. (2024), we find the target loss for which the Lov\'asz hinge
is consistent. This target, which we call the structured abstain problem, is a
variant of selective classification for structured prediction that allows one
to abstain on any subset of the k binary predictions. We derive a family of
link functions, each of which is simultaneously consistent for all
polymatroids, a subset of submodular set functions. We then give sufficient
conditions on the polymatroid for the structured abstain problem to be tightly
embedded by the Lov\'asz hinge, meaning no target prediction is redundant. We
experimentally demonstrate the potential of the structured abstain problem for
interpretability in structured classification tasks. Finally, for the
multiclass setting, we show that one can combine the binary encoding
construction of Ramaswamy et al. (2018) with our link construction to achieve
an efficient consistent surrogate for a natural multiclass generalization of
the structured abstain problem.

</details>


### [111] [Sponge Attacks on Sensing AI: Energy-Latency Vulnerabilities and Defense via Model Pruning](https://arxiv.org/abs/2505.06454)
*Syed Mhamudul Hasan,Hussein Zangoti,Iraklis Anagnostopoulos,Abdur R. Shahid*

Main category: cs.LG

TL;DR: 本文系统探讨了针对传感AI模型的能量-延迟海绵攻击，提出模型剪枝作为防御手段，并量化了模型效率与攻击韧性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在计算机视觉和自然语言处理任务中的海绵攻击，忽视了轻量级AI模型在资源受限的IoT设备中的使用。这些攻击对电池容量有限且需要实时响应的系统构成严重威胁。

Method: 以可穿戴传感AI为例，研究了能量-延迟海绵攻击的影响，并探索了模型剪枝作为防御手段的有效性。

Result: 实验表明，剪枝可显著提升模型对海绵中毒攻击的韧性，但需要在模型效率和攻击韧性之间进行权衡。

Conclusion: 研究强调了在IoT环境中部署传感AI时，模型压缩的安全影响，为设计更安全的轻量级AI模型提供了参考。

Abstract: Recent studies have shown that sponge attacks can significantly increase the
energy consumption and inference latency of deep neural networks (DNNs).
However, prior work has focused primarily on computer vision and natural
language processing tasks, overlooking the growing use of lightweight AI models
in sensing-based applications on resource-constrained devices, such as those in
Internet of Things (IoT) environments. These attacks pose serious threats of
energy depletion and latency degradation in systems where limited battery
capacity and real-time responsiveness are critical for reliable operation. This
paper makes two key contributions. First, we present the first systematic
exploration of energy-latency sponge attacks targeting sensing-based AI models.
Using wearable sensing-based AI as a case study, we demonstrate that sponge
attacks can substantially degrade performance by increasing energy consumption,
leading to faster battery drain, and by prolonging inference latency. Second,
to mitigate such attacks, we investigate model pruning, a widely adopted
compression technique for resource-constrained AI, as a potential defense. Our
experiments show that pruning-induced sparsity significantly improves model
resilience against sponge poisoning. We also quantify the trade-offs between
model efficiency and attack resilience, offering insights into the security
implications of model compression in sensing-based AI systems deployed in IoT
environments.

</details>


### [112] [Improved Uncertainty Quantification in Physics-Informed Neural Networks Using Error Bounds and Solution Bundles](https://arxiv.org/abs/2505.06459)
*Pablo Flores,Olga Graf,Pavlos Protopapas,Karim Pichara*

Main category: cs.LG

TL;DR: 该论文提出了一种两步训练贝叶斯神经网络的方法，用于量化物理信息神经网络（PINN）解决微分方程系统中的不确定性。


<details>
  <summary>Details</summary>
Motivation: 尽管PINNs广泛应用于物理现象建模，但其缺乏不确定性量化机制，因此需要改进方法以更准确地估计不确定性。

Method: 通过利用PINN的误差界限构建异方差方差，并结合贝叶斯神经网络的两步训练过程，改进不确定性估计。

Result: 研究成功应用于宇宙学中的正向问题求解和逆向问题参数估计，并提供了更准确的不确定性结果。

Conclusion: 该方法有效提升了PINN在不确定性量化方面的表现，为物理模型的参数估计提供了更可靠的依据。

Abstract: Physics-Informed Neural Networks (PINNs) have been widely used to obtain
solutions to various physical phenomena modeled as Differential Equations. As
PINNs are not naturally equipped with mechanisms for Uncertainty
Quantification, some work has been done to quantify the different uncertainties
that arise when dealing with PINNs. In this paper, we use a two-step procedure
to train Bayesian Neural Networks that provide uncertainties over the solutions
to differential equation systems provided by PINNs. We use available error
bounds over PINNs to formulate a heteroscedastic variance that improves the
uncertainty estimation. Furthermore, we solve forward problems and utilize the
obtained uncertainties when doing parameter estimation in inverse problems in
cosmology.

</details>


### [113] [Probing In-Context Learning: Impact of Task Complexity and Model Architecture on Generalization and Efficiency](https://arxiv.org/abs/2505.06475)
*Binwen Liu,Peiyu Xu,Quan Yuan,Yihong Chen*

Main category: cs.LG

TL;DR: 论文研究了上下文学习（ICL）在不同任务复杂度和模型架构下的表现，发现模型架构对ICL性能有显著影响。标准Transformer表现稳健，Mamba擅长时间结构任务，Hyena捕捉长程依赖但训练早期方差较大，FlashAttention计算高效但在低数据场景下敏感。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索不同模型架构在多种任务复杂度下对上下文学习的影响，以揭示架构设计对性能的关键作用。

Method: 通过系统实验设计，引入高斯核回归和非线性动态系统任务，评估四种模型（标准Transformer、FlashAttention Transformer、Hyena卷积模型、Mamba状态空间模型）在合成数据集上的表现。

Result: 标准Transformer表现稳健；Mamba在时间结构任务中表现最佳；Hyena能捕捉长程依赖但训练早期方差高；FlashAttention计算高效但低数据敏感。还发现高斯核任务中的局部捷径、输入范围缩放增强非线性可分性以及课程学习在高维任务中的重要性。

Conclusion: 模型架构对ICL性能至关重要，不同架构在不同任务中各有优劣，任务设计和训练策略（如课程学习）对性能提升有显著影响。

Abstract: We investigate in-context learning (ICL) through a meticulous experimental
framework that systematically varies task complexity and model architecture.
Extending beyond the linear regression baseline, we introduce Gaussian kernel
regression and nonlinear dynamical system tasks, which emphasize temporal and
recursive reasoning. We evaluate four distinct models: a GPT2-style
Transformer, a Transformer with FlashAttention mechanism, a convolutional
Hyena-based model, and the Mamba state-space model. Each model is trained from
scratch on synthetic datasets and assessed for generalization during testing.
Our findings highlight that model architecture significantly shapes ICL
performance. The standard Transformer demonstrates robust performance across
diverse tasks, while Mamba excels in temporally structured dynamics. Hyena
effectively captures long-range dependencies but shows higher variance early in
training, and FlashAttention offers computational efficiency but is more
sensitive in low-data regimes. Further analysis uncovers locality-induced
shortcuts in Gaussian kernel tasks, enhanced nonlinear separability through
input range scaling, and the critical role of curriculum learning in mastering
high-dimensional tasks.

</details>


### [114] [QoS-Efficient Serving of Multiple Mixture-of-Expert LLMs Using Partial Runtime Reconfiguration](https://arxiv.org/abs/2505.06481)
*HamidReza Imani,Jiaxin Peng,Peiman Mohseni,Abdolah Amirany,Tarek El-Ghazawi*

Main category: cs.LG

TL;DR: 提出了一种高效服务多个微调MoE-LLM的系统，通过相似性专家整合和动态部分重构，显著减少内存使用并保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 解决多租户环境中共享GPU资源时，MoE-LLM内存需求高的问题。

Method: 使用相似性专家整合和运行时部分重构技术。

Result: 实验显示内存占用减少85%，同时保持输出质量。

Conclusion: 该系统在多模型共享GPU资源时高效且实用。

Abstract: The deployment of mixture-of-experts (MoE) large language models (LLMs)
presents significant challenges due to their high memory demands. These
challenges become even more pronounced in multi-tenant environments, where
shared resources must accommodate multiple models, limiting the effectiveness
of conventional virtualization techniques. This paper addresses the problem of
efficiently serving multiple fine-tuned MoE-LLMs on a single-GPU. We propose a
serving system that employs \textit{similarity-based expert consolidation} to
reduce the overall memory footprint by sharing similar experts across models.
To ensure output quality, we introduce \textit{runtime partial
reconfiguration}, dynamically replacing non-expert layers when processing
requests from different models. As a result, our approach achieves a
competitive output quality while maintaining throughput comparable to serving a
single model while incurring a negligible increase in time-to-first-token
(TTFT). Experiments on a server with a single NVIDIA A100 GPU (80GB) using
Mixtral-8x7B models demonstrate an 85\% average reduction in turnaround time
compared to NVIDIA's multi-instance GPU (MIG). Furthermore, experiments on
Google's Switch Transformer Base-8 model with up to four variants demonstrate
the scalability and resilience of our approach in maintaining output quality
compared to other model merging baselines, highlighting its effectiveness.

</details>


### [115] [Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach](https://arxiv.org/abs/2505.06482)
*Minting Pan,Yitao Zheng,Jiajian Li,Yunbo Wang,Xiaokang Yang*

Main category: cs.LG

TL;DR: VeoRL是一种基于模型的离线强化学习方法，利用在线多样未标注视频数据构建交互式世界模型，显著提升了在视觉运动控制任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习因缺乏环境交互而面临行为学习次优和值估计不准确的问题，VeoRL旨在通过视频数据增强模型来解决这些局限性。

Method: VeoRL通过构建交互式世界模型，并利用基于模型的行为指导，将视频中的常识性知识迁移至目标领域的强化学习代理。

Result: 在机器人操控、自动驾驶和开放世界视频游戏等任务中，VeoRL的性能提升显著（某些情况下超过100%）。

Conclusion: VeoRL通过视频数据增强离线强化学习，实现了在多个领域中的高效策略优化，展示了模型迁移的潜力。

Abstract: Offline reinforcement learning (RL) enables policy optimization in static
datasets, avoiding the risks and costs of real-world exploration. However, it
struggles with suboptimal behavior learning and inaccurate value estimation due
to the lack of environmental interaction. In this paper, we present
Video-Enhanced Offline RL (VeoRL), a model-based approach that constructs an
interactive world model from diverse, unlabeled video data readily available
online. Leveraging model-based behavior guidance, VeoRL transfers commonsense
knowledge of control policy and physical dynamics from natural videos to the RL
agent within the target domain. Our method achieves substantial performance
gains (exceeding 100% in some cases) across visuomotor control tasks in robotic
manipulation, autonomous driving, and open-world video games.

</details>


### [116] [FedADP: Unified Model Aggregation for Federated Learning with Heterogeneous Model Architectures](https://arxiv.org/abs/2505.06497)
*Jiacheng Wang,Hongtao Lv,Lei Liu*

Main category: cs.LG

TL;DR: FedADP是一个联邦学习框架，针对异构环境中客户端的多样性挑战，通过动态调整模型架构提升效率和准确性，相比FlexiFed等方法可提升高达23.30%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决传统联邦学习在异构环境中因客户端模型架构和计算资源差异导致的效率低和准确性不足问题。

Method: 提出FedADP框架，通过动态调整聚合过程中的模型架构以适应客户端异质性。

Result: 实验显示FedADP显著优于现有方法，如FlexiFed，准确率最高提升23.30%，同时在资源利用和模型适应性方面表现优异。

Conclusion: FedDFP有效地提升联邦学习在异构环境中的性能和效率，具有实际应用价值。

Abstract: Traditional Federated Learning (FL) faces significant challenges in terms of
efficiency and accuracy, particularly in heterogeneous environments where
clients employ diverse model architectures and have varying computational
resources. Such heterogeneity complicates the aggregation process, leading to
performance bottlenecks and reduced model generalizability. To address these
issues, we propose FedADP, a federated learning framework designed to adapt to
client heterogeneity by dynamically adjusting model architectures during
aggregation. FedADP enables effective collaboration among clients with
differing capabilities, maximizing resource utilization and ensuring model
quality. Our experimental results demonstrate that FedADP significantly
outperforms existing methods, such as FlexiFed, achieving an accuracy
improvement of up to 23.30%, thereby enhancing model adaptability and training
efficiency in heterogeneous real-world settings.

</details>


### [117] [Interpretable SHAP-bounded Bayesian Optimization for Underwater Acoustic Metamaterial Coating Design](https://arxiv.org/abs/2505.06519)
*Hansani Weeratunge,Dominic Robe,Elnaz Hajizadeh*

Main category: cs.LG

TL;DR: 本文提出了一种基于解释性的贝叶斯优化框架，用于优化具有超材料特性的聚氨酯弹性体水下声学涂层。通过SHAP分析关键参数对声吸收性能的影响，并自动调整优化边界，从而高效地探索设计空间。


<details>
  <summary>Details</summary>
Motivation: 当前水下声学涂层的优化过程受限于计算资源和高维设计空间，传统方法效率低下。本文旨在通过结合SHAP解释性工具，揭示参数间的隐藏关系，提升贝叶斯优化的效率和效果。

Method: 采用数据驱动模型分析声吸收性能与设计变量的关系，结合SHAP识别关键参数及其影响机制。利用这些信息自动优化问题边界，减少不必要探索。

Result: 在两种硬度不同的聚氨酯材料上应用该方法，优化结果优于传统方法，且未增加仿真迭代次数。证明SHAP能有效引导优化方向。

Conclusion: 结合SHAP的解释性与贝叶斯优化，可高效设计水下声学超材料，且方法可推广至其他材料和工程优化问题。

Abstract: We developed an interpretability informed Bayesian optimization framework to
optimize underwater acoustic coatings based on polyurethane elastomers with
embedded metamaterial features. A data driven model was employed to analyze the
relationship between acoustic performance, specifically sound absorption and
the corresponding design variables. By leveraging SHapley Additive exPlanations
(SHAP), a machine learning interpretability tool, we identified the key
parameters influencing the objective function and gained insights into how
these parameters affect sound absorption. The insights derived from the SHAP
analysis were subsequently used to automatically refine the bounds of the
optimization problem automatically, enabling a more targeted and efficient
exploration of the design space.
  The proposed approach was applied to two polyurethane materials with distinct
hardness levels, resulting in improved optimal solutions compared to those
obtained without SHAP-informed guidance. Notably, these enhancements were
achieved without increasing the number of simulation iterations. Our findings
demonstrate the potential of SHAP to streamline optimization processes by
uncovering hidden parameter relationships and guiding the search toward
promising regions of the design space. This work underscores the effectiveness
of combining interpretability techniques with Bayesian optimization for the
efficient and cost-effective design of underwater acoustic metamaterials under
strict computational constraints and can be generalized towards other materials
and engineering optimization problems.

</details>


### [118] [PRUNE: A Patching Based Repair Framework for Certiffable Unlearning of Neural Networks](https://arxiv.org/abs/2505.06520)
*Xuran Li,Jingyi Wang,Xiaohan Yuan,Peixin Zhang,Zhan Qin,Zhibo Wang,Kui Ren*

Main category: cs.LG

TL;DR: 提出了一种通过微调神经网络实现数据删除的新方法，比重新训练更高效。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据删除方法成本高、验证难的问题，满足法规对数据遗忘权的要求。

Method: 通过精心设计的‘补丁’修改网络，逐步删除代表性数据点以实现批量遗忘。

Result: 实验证明该方法能有效删除数据且保持模型性能，效率和内存占用优于基线方法。

Conclusion: 该方法为数据删除提供了高效、可验证的解决方案。

Abstract: It is often desirable to remove (a.k.a. unlearn) a speciffc part of the
training data from a trained neural network model. A typical application
scenario is to protect the data holder's right to be forgotten, which has been
promoted by many recent regulation rules. Existing unlearning methods involve
training alternative models with remaining data, which may be costly and
challenging to verify from the data holder or a thirdparty auditor's
perspective. In this work, we provide a new angle and propose a novel
unlearning approach by imposing carefully crafted "patch" on the original
neural network to achieve targeted "forgetting" of the requested data to
delete. Speciffcally, inspired by the research line of neural network repair,
we propose to strategically seek a lightweight minimum "patch" for unlearning a
given data point with certiffable guarantee. Furthermore, to unlearn a
considerable amount of data points (or an entire class), we propose to
iteratively select a small subset of representative data points to unlearn,
which achieves the effect of unlearning the whole set. Extensive experiments on
multiple categorical datasets demonstrates our approach's effectiveness,
achieving measurable unlearning while preserving the model's performance and
being competitive in efffciency and memory consumption compared to various
baseline methods.

</details>


### [119] [GBDTSVM: Combined Support Vector Machine and Gradient Boosting Decision Tree Framework for efficient snoRNA-disease association prediction](https://arxiv.org/abs/2505.06534)
*Ummay Maria Muna,Fahim Hafiz,Shanta Biswas,Riasat Azim*

Main category: cs.LG

TL;DR: 该论文提出了一种名为GBDTSVM的新型机器学习模型，通过结合梯度提升决策树（GBDT）和支持向量机（SVM）来预测snoRNA-疾病关联（SDAs）。该方法在多个数据集上表现优异，AUROC和AUPRC分别达到0.96和0.95。


<details>
  <summary>Details</summary>
Motivation: 传统的生物实验方法成本高、耗时长且资源密集，机器学习方法为snoRNA-疾病关联预测提供了高效解决方案。

Method: GBDTSVM结合GBDT提取特征表示，再用SVM进行分类，并引入高斯核轮廓相似性提升预测精度。

Result: 模型在MDRF、LSGT和PsnoD数据集上表现优于现有方法，AUROC（0.96）和AUPRC（0.95）优异，案例分析验证了前十预测snoRNA的准确性。

Conclusion: GBDTSVM是一种有效的工具，可推动snoRNA相关疾病研究的发展。

Abstract: Small nucleolar RNAs (snoRNAs) are increasingly recognized for their critical
role in the pathogenesis and characterization of various human diseases.
Consequently, the precise identification of snoRNA-disease associations (SDAs)
is essential for the progression of diseases and the advancement of treatment
strategies. However, conventional biological experimental approaches are
costly, time-consuming, and resource-intensive; therefore, machine
learning-based computational methods offer a promising solution to mitigate
these limitations. This paper proposes a model called 'GBDTSVM', representing a
novel and efficient machine learning approach for predicting snoRNA-disease
associations by leveraging a Gradient Boosting Decision Tree (GBDT) and Support
Vector Machine (SVM). 'GBDTSVM' effectively extracts integrated snoRNA-disease
feature representations utilizing GBDT and SVM is subsequently utilized to
classify and identify potential associations. Furthermore, the method enhances
the accuracy of these predictions by incorporating Gaussian kernel profile
similarity for both snoRNAs and diseases. Experimental evaluation of the
GBDTSVM model demonstrated superior performance compared to state-of-the-art
methods in the field, achieving an area under the receiver operating
characteristic (AUROC) of 0.96 and an area under the precision-recall curve
(AUPRC) of 0.95 on MDRF dataset. Moreover, our model shows superior performance
on two more datasets named LSGT and PsnoD. Additionally, a case study on the
predicted snoRNA-disease associations verified the top 10 predicted snoRNAs
across nine prevalent diseases, further validating the efficacy of the GBDTSVM
approach. These results underscore the model's potential as a robust tool for
advancing snoRNA-related disease research. Source codes and datasets our
proposed framework can be obtained from: https://github.com/mariamuna04/gbdtsvm

</details>


### [120] [dcFCI: Robust Causal Discovery Under Latent Confounding, Unfaithfulness, and Mixed Data](https://arxiv.org/abs/2505.06542)
*Adèle H. Ribeiro,Dominik Heider*

Main category: cs.LG

TL;DR: 该论文提出了第一个非参数评分方法dcFCI，用于评估潜在混杂、经验非忠实性和混合数据类型下的PAG兼容性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决因果发现中因样本量限制导致的经验非忠实性问题，以及潜在混杂和混合数据类型的挑战。

Method: 结合非参数评分与AnytimeFCI引导的搜索，系统评估和验证候选PAG。

Result: 在合成和真实数据中，dcFCI显著优于现有方法，并能恢复真实PAG。

Conclusion: dcFCI提供了更高的结构不确定性和更稳健的因果推理支持。

Abstract: Causal discovery is central to inferring causal relationships from
observational data. In the presence of latent confounding, algorithms such as
Fast Causal Inference (FCI) learn a Partial Ancestral Graph (PAG) representing
the true model's Markov Equivalence Class. However, their correctness
critically depends on empirical faithfulness, the assumption that observed
(in)dependencies perfectly reflect those of the underlying causal model, which
often fails in practice due to limited sample sizes. To address this, we
introduce the first nonparametric score to assess a PAG's compatibility with
observed data, even with mixed variable types. This score is both necessary and
sufficient to characterize structural uncertainty and distinguish between
distinct PAGs. We then propose data-compatible FCI (dcFCI), the first hybrid
causal discovery algorithm to jointly address latent confounding, empirical
unfaithfulness, and mixed data types. dcFCI integrates our score into an
(Anytime)FCI-guided search that systematically explores, ranks, and validates
candidate PAGs. Experiments on synthetic and real-world scenarios demonstrate
that dcFCI significantly outperforms state-of-the-art methods, often recovering
the true PAG even in small and heterogeneous datasets. Examining top-ranked
PAGs further provides valuable insights into structural uncertainty, supporting
more robust and informed causal reasoning and decision-making.

</details>


### [121] [Good Things Come in Pairs: Paired Autoencoders for Inverse Problems](https://arxiv.org/abs/2505.06549)
*Matthias Chung,Bas Peters,Michael Solomon*

Main category: cs.LG

TL;DR: 本文介绍了一种名为“配对自编码器”的数据驱动方法，用于解决科学计算中的反问题，通过潜在空间映射和数值实验验证了其有效性，尤其在噪声数据下仍能实现高质量估计。


<details>
  <summary>Details</summary>
Motivation: 探讨如何结合数据驱动和模型驱动方法的优势，以解决科学计算中的反问题，尤其是针对非线性反问题（如地震成像）和线性反问题（如经典修复）。

Method: 提出配对自编码器框架，将数据和目标量投影到潜在空间，并通过潜在空间映射生成前向和反向代理映射，无需似然假设。同时引入变分思想扩展功能。

Result: 数值实验表明，该方法在噪声超过训练范围时仍能提供高质量估计，并通过潜在空间细化和初始猜测提升性能。变分扩展版还支持不确定性分析采样。

Conclusion: 配对自编码器结合数据与模型优势，为反问题提供了灵活且鲁棒的解决方案，其变种进一步扩展了不确定性分析能力。

Abstract: In this book chapter, we discuss recent advances in data-driven approaches
for inverse problems. In particular, we focus on the \emph{paired autoencoder}
framework, which has proven to be a powerful tool for solving inverse problems
in scientific computing. The paired autoencoder framework is a novel approach
that leverages the strengths of both data-driven and model-based methods by
projecting both the data and the quantity of interest into a latent space and
mapping these latent spaces to provide surrogate forward and inverse mappings.
We illustrate the advantages of this approach through numerical experiments,
including seismic imaging and classical inpainting: nonlinear and linear
inverse problems, respectively. Although the paired autoencoder framework is
likelihood-free, it generates multiple data- and model-based reconstruction
metrics that help assess whether examples are in or out of distribution. In
addition to direct model estimates from data, the paired autoencoder enables
latent-space refinement to fit the observed data accurately. Numerical
experiments show that this procedure, combined with the latent-space initial
guess, is essential for high-quality estimates, even when data noise exceeds
the training regime. We also introduce two novel variants that combine
variational and paired autoencoder ideas, maintaining the original benefits
while enabling sampling for uncertainty analysis.

</details>


### [122] [An \tilde{O}ptimal Differentially Private Learner for Concept Classes with VC Dimension 1](https://arxiv.org/abs/2505.06581)
*Chao Yan*

Main category: cs.LG

TL;DR: 本文提出了首个针对VC维度为1、Littlestone维度为$d$的概念类的近乎最优的差分隐私PAC学习算法，样本复杂度为$	ilde{O}_{\varepsilon,\delta,\alpha,\delta}(\log^* d)$，接近Alon等人的下界。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私PAC学习算法对一般VC类的样本复杂度上限为$	ilde{O}(VC\cdot d^5)$，因此需要一种更高效的算法来处理特定概念类。

Method: 提出了一种新的差分隐私PAC学习算法，专门针对VC维度为1、Littlestone维度为$d$的概念类。

Result: 算法的样本复杂度为$	ilde{O}_{\varepsilon,\delta,\alpha,\delta}(\log^* d)$，接近已知下界。

Conclusion: 该算法在特定概念类上实现了近乎最优的样本复杂度，显著优于现有一般VC类算法的上限。

Abstract: We present the first nearly optimal differentially private PAC learner for
any concept class with VC dimension 1 and Littlestone dimension $d$. Our
algorithm achieves the sample complexity of
$\tilde{O}_{\varepsilon,\delta,\alpha,\delta}(\log^* d)$, nearly matching the
lower bound of $\Omega(\log^* d)$ proved by Alon et al. [STOC19]. Prior to our
work, the best known upper bound is $\tilde{O}(VC\cdot d^5)$ for general VC
classes, as shown by Ghazi et al. [STOC21].

</details>


### [123] [Geometry of Learning -- L2 Phase Transitions in Deep and Shallow Neural Networks](https://arxiv.org/abs/2505.06597)
*Ibrahim Talha Ersoy,Karoline Wiesner*

Main category: cs.LG

TL;DR: 该论文建立了一个统一框架，通过将损失景观的Ricci曲率与L2正则化驱动的深度学习相结合，揭示了神经网络在正则化强度增加时的相位转变现象。单隐层网络表现为一阶转变，而多隐层网络则为二阶转变。曲率变化点与模型精度的相位转变临界点一致，适用于复杂数据集和MNIST实验。该框架为优化模型性能和探索神经网络内在结构提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解L2正则化强度增加时神经网络的表现变化，尤其是模型从过参数化到欠参数化转变的相位现象，并探索这些现象与损失景观几何特征的联系。

Method: 方法结合了Ricci曲率分析与正则化驱动的深度学习，验证曲率变化点与相位转变临界点的一致性，并通过MNIST数据集的变分自编码器实验扩展了框架的应用范围。

Result: 结果表明，曲率变化点不仅对应于正则化驱动的相位转变临界点，还能识别复杂数据集和MNIST实验中的模型精度变化，验证了框架的普适性。

Conclusion: 结论强调了该框架在指导正则化策略和探索神经网络内在结构方面的潜力，为优化模型性能和开发新方法提供了理论基础。

Abstract: When neural networks (NNs) are subject to L2 regularization, increasing the
regularization strength beyond a certain threshold pushes the model into an
under-parameterization regime. This transition manifests as a first-order phase
transition in single-hidden-layer NNs and a second-order phase transition in
NNs with two or more hidden layers. This paper establishes a unified framework
for such transitions by integrating the Ricci curvature of the loss landscape
with regularizer-driven deep learning. First, we show that a curvature
change-point separates the model-accuracy regimes in the onset of learning and
that it is identical to the critical point of the phase transition driven by
regularization. Second, we show that for more complex data sets additional
phase transitions exist between model accuracies, and that they are again
identical to curvature change points in the error landscape. Third, by studying
the MNIST data set using a Variational Autoencoder, we demonstrate that the
curvature change points identify phase transitions in model accuracy outside
the L2 setting. Our framework also offers practical insights for optimizing
model performance across various architectures and datasets. By linking
geometric features of the error landscape to observable phase transitions, our
work paves the way for more informed regularization strategies and potentially
new methods for probing the intrinsic structure of neural networks beyond the
L2 context.

</details>


### [124] [Minimizing Risk Through Minimizing Model-Data Interaction: A Protocol For Relying on Proxy Tasks When Designing Child Sexual Abuse Imagery Detection Models](https://arxiv.org/abs/2505.06621)
*Thamiris Coelho,Leo S. F. Ribeiro,João Macedo,Jefersson A. dos Santos,Sandra Avila*

Main category: cs.LG

TL;DR: 论文关注儿童性虐待图像（CSAI）的自动化检测问题，提出了一种名为“代理任务”的方法，用于在不直接接触敏感数据的情况下训练模型，并展示了其在实际数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 儿童性虐待图像的传播日益严重，受害者不断受到二次伤害。手动分类工作量大且效率低，而敏感数据的访问限制使得自动化检测方法难以直接应用。

Method: 提出“代理任务”概念，即使用替代任务训练模型而不直接使用CSAI数据，并结合执法机构的反馈设计更优的自动化方案。

Result: 首次将少样本室内场景分类任务应用于CSAI检测，模型在实际数据集上表现良好，且无需敏感数据训练权重。

Conclusion: 代理任务是解决CSAI检测中数据敏感问题的可行方法，结合执法机构的输入能进一步提升自动化检测效果。

Abstract: The distribution of child sexual abuse imagery (CSAI) is an ever-growing
concern of our modern world; children who suffered from this heinous crime are
revictimized, and the growing amount of illegal imagery distributed overwhelms
law enforcement agents (LEAs) with the manual labor of categorization. To ease
this burden researchers have explored methods for automating data triage and
detection of CSAI, but the sensitive nature of the data imposes restricted
access and minimal interaction between real data and learning algorithms,
avoiding leaks at all costs. In observing how these restrictions have shaped
the literature we formalize a definition of "Proxy Tasks", i.e., the substitute
tasks used for training models for CSAI without making use of CSA data. Under
this new terminology we review current literature and present a protocol for
making conscious use of Proxy Tasks together with consistent input from LEAs to
design better automation in this field. Finally, we apply this protocol to
study -- for the first time -- the task of Few-shot Indoor Scene Classification
on CSAI, showing a final model that achieves promising results on a real-world
CSAI dataset whilst having no weights actually trained on sensitive data.

</details>


### [125] [Dyn-D$^2$P: Dynamic Differentially Private Decentralized Learning with Provable Utility Guarantee](https://arxiv.org/abs/2505.06651)
*Zehan Zhu,Yan Huang,Xin Wang,Shouling Ji,Jinming Xu*

Main category: cs.LG

TL;DR: Dyn-D2P是一种动态差分隐私分布式学习方法，通过调整梯度裁剪界限和噪声水平提高模型精度，同时保护隐私预算，优于固定噪声方法。


<details>
  <summary>Details</summary>
Motivation: 现有分布式学习方法的固定梯度裁剪和噪声水平导致精度显著下降，因此需要动态调整以提高精度并保护隐私。

Method: 利用高斯差分隐私框架，动态调整梯度裁剪界限和噪声水平，基于梯度收敛情况。

Result: 实验表明Dyn-D2P在强隐私保证下优于固定噪声方法，并提供了模型效用边界分析。

Conclusion: Dyn-D2P是首个针对动态梯度裁剪和噪声水平的差分隐私分布式非凸优化方法，具有理论和实践优势。

Abstract: Most existing decentralized learning methods with differential privacy (DP)
guarantee rely on constant gradient clipping bounds and fixed-level DP Gaussian
noises for each node throughout the training process, leading to a significant
accuracy degradation compared to non-private counterparts. In this paper, we
propose a new Dynamic Differentially Private Decentralized learning approach
(termed Dyn-D$^2$P) tailored for general time-varying directed networks.
Leveraging the Gaussian DP (GDP) framework for privacy accounting, Dyn-D$^2$P
dynamically adjusts gradient clipping bounds and noise levels based on gradient
convergence. This proposed dynamic noise strategy enables us to enhance model
accuracy while preserving the total privacy budget. Extensive experiments on
benchmark datasets demonstrate the superiority of Dyn-D$^2$P over its
counterparts employing fixed-level noises, especially under strong privacy
guarantees. Furthermore, we provide a provable utility bound for Dyn-D$^2$P
that establishes an explicit dependency on network-related parameters, with a
scaling factor of $1/\sqrt{n}$ in terms of the number of nodes $n$ up to a bias
error term induced by gradient clipping. To our knowledge, this is the first
model utility analysis for differentially private decentralized non-convex
optimization with dynamic gradient clipping bounds and noise levels.

</details>


### [126] [Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4): Analysis and Variations](https://arxiv.org/abs/2505.06653)
*Patrick Blumenberg,Thomas Graave,Tim Fingscheidt*

Main category: cs.LG

TL;DR: 提出了一种优化块级量化的方法BOF4，并通过理论和数据驱动解决方案减少量化误差，进一步改进为BOF4-S。此外，提出混合精度策略OPQ以处理异常权重，最终在4-bit块级量化技术中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在微调和推理时需要大量内存。现有块级量化方法（如NF4和AF4）量化误差较大，因此需优化以减少误差并提升语言模型性能。

Method: 1. 提出BOF4量化器优化块级量化；2. 改进归一化方法（BOF4-S）进一步减少误差；3. 实验探究块级量化变体的重要性；4. 引入混合精度策略OPQ处理异常权重。

Result: BOF4-S结合OPQ在4-bit块级量化技术中表现最佳，语言建模性能下降最小。

Conclusion: 通过优化块级量化和混合精度策略，显著减少了量化误差并提升了LLMs的内存效率与性能。

Abstract: Large language models (LLMs) demand extensive memory capacity during both
fine-tuning and inference. To enable memory-efficient fine-tuning, existing
methods apply block-wise quantization techniques, such as NF4 and AF4, to the
network weights. We show that these quantization techniques incur suboptimal
quantization errors. Therefore, as a first novelty, we propose an optimization
approach for block-wise quantization. Using this method, we design a family of
quantizers named 4-bit block-wise optimal float (BOF4), which consistently
reduces the quantization error compared to both baseline methods. We provide
both a theoretical and a data-driven solution for the optimization process and
prove their practical equivalence. Secondly, we propose a modification to the
employed normalization method based on the signed absolute block maximum
(BOF4-S), enabling further reduction of the quantization error and empirically
achieving less degradation in language modeling performance. Thirdly, we
explore additional variations of block-wise quantization methods applied to
LLMs through an experimental study on the importance of accurately representing
zero and large-amplitude weights on the one hand, and optimization towards
various error metrics on the other hand. Lastly, we introduce a mixed-precision
quantization strategy dubbed outlier-preserving quantization (OPQ) to address
the distributional mismatch induced by outlier weights in block-wise
quantization. By storing outlier weights in 16-bit precision (OPQ) while
applying BOF4-S, we achieve top performance among 4-bit block-wise quantization
techniques w.r.t. perplexity.

</details>


### [127] [A Novel Framework for Significant Wave Height Prediction based on Adaptive Feature Extraction Time-Frequency Network](https://arxiv.org/abs/2505.06688)
*Jianxin Zhang,Lianzi Jiang,Xinyu Han,Xiangrong Wang*

Main category: cs.LG

TL;DR: 提出了一种新型自适应特征提取时频网络（AFE-TFNet），用于显著波高（Hs）的精确预测，解决了传统分解预处理引发的数据泄露问题，并通过特征提取与融合显著提升了预测精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 预测显著波高（Hs）对波浪能开发至关重要，但Hs的非线性和非平稳特性带来了挑战。现有方法中，分解预处理可能导致测试集数据泄露，因此需要一种既能提取特征又能避免泄露的新方法。

Method: AFE-TFNet采用编码器-解码器滚动框架。编码器分两阶段：特征提取（结合小波变换和傅里叶变换提取全局与局部频域特征，使用Inception块进行多尺度分析）和特征融合（通过主导谐波序列能量加权整合时频域特征）。解码器采用改进的LSTM模型。

Result: 实验使用三个站点的风速、波周期和Hs数据，结果表明AFE-TFNet在预测精度上显著优于基准方法，特征提取和融合（尤其是DHSEW）显著提升了中长期预测精度，且滚动时间窗口变化对模型影响较小。

Conclusion: AFE-TFNet能有效处理复杂信号预测，尤其在避免数据泄露和提升中长期预测精度方面表现突出，展示了其在实际应用中的潜力。

Abstract: Precise forecasting of significant wave height (Hs) is essential for the
development and utilization of wave energy. The challenges in predicting Hs
arise from its non-linear and non-stationary characteristics. The combination
of decomposition preprocessing and machine learning models have demonstrated
significant effectiveness in Hs prediction by extracting data features.
However, decomposing the unknown data in the test set can lead to data leakage
issues. To simultaneously achieve data feature extraction and prevent data
leakage, a novel Adaptive Feature Extraction Time-Frequency Network (AFE-TFNet)
is proposed to improve prediction accuracy and stability. It is encoder-decoder
rolling framework. The encoder consists of two stages: feature extraction and
feature fusion. In the feature extraction stage, global and local frequency
domain features are extracted by combining Wavelet Transform (WT) and Fourier
Transform (FT), and multi-scale frequency analysis is performed using Inception
blocks. In the feature fusion stage, time-domain and frequency-domain features
are integrated through dominant harmonic sequence energy weighting (DHSEW). The
decoder employed an advanced long short-term memory (LSTM) model. Hourly
measured wind speed (Ws), dominant wave period (DPD), average wave period (APD)
and Hs from three stations are used as the dataset, and the four metrics are
employed to evaluate the forecasting performance. Results show that AFE-TFNet
significantly outperforms benchmark methods in terms of prediction accuracy.
Feature extraction can significantly improve the prediction accuracy. DHSEW has
substantially increased the accuracy of medium-term to long-term forecasting.
The prediction accuracy of AFE-TFNet does not demonstrate significant
variability with changes of rolling time window size. Overall, AFE-TFNet shows
strong potential for handling complex signal forecasting.

</details>


### [128] [E2E-FANet: A Highly Generalizable Framework for Waves prediction Behind Floating Breakwaters via Exogenous-to-Endogenous Variable Attention](https://arxiv.org/abs/2505.06690)
*Jianxin Zhang,Lianzi Jiang,Xinyu Han,Xiangrong Wang,Weinan Huang*

Main category: cs.LG

TL;DR: 针对浮动防波堤后波浪预测的准确性问题，本文提出了E2E-FANet神经网络模型，通过结合频域特征提取和注意力机制，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉波浪与结构的非线性相互作用及复杂频域关系方面存在局限，需要更高效的模型优化海岸工程结构。

Method: 提出E2E-FANet模型，包含DBFM模块（频域特征提取）、E2ECA模块（内外变量交互建模）和TA机制（时间依赖捕捉）。

Result: 通过多层次验证（相同波浪条件、不同波浪条件、不同相对水密度条件），E2E-FANet展现出高预测精度和强泛化能力。

Conclusion: E2E-FANet模型在波浪预测中表现优异，为海岸工程提供了有效的设计优化工具。

Abstract: Accurate prediction of waves behind floating breakwaters (FB) is crucial for
optimizing coastal engineering structures, enhancing safety, and improving
design efficiency. Existing methods demonstrate limitations in capturing
nonlinear interactions between waves and structures, while exhibiting
insufficient capability in modeling the complex frequency-domain relationships
among elevations of different wave gauges. To address these challenges, this
study introduces the Exogenous-to-Endogenous Frequency-Aware Network
(E2E-FANet), a novel end-to-end neural network designed to model relationships
between waves and structures. The E2E-FANetarchitecture incorporates a
Dual-Basis Frequency Mapping (DBFM) module that leverages orthogonal cosine and
sine bases to extract wave features from the frequency domain while preserving
temporal information. Additionally, we introduce the Exogenous-to-Endogenous
Cross-Attention (E2ECA) module, which employs cross attention to model the
interactions between endogenous and exogenous variables. We incorporate a
Temporal-wise Attention (TA) mechanism that adaptively captures complex
dependencies in endogenous variables. These integrated modules function
synergistically, enabling E2E-FANet to achieve both comprehensive feature
perception in the time-frequency domain and precise modeling of wave-structure
interactions. To comprehensively evaluate the performance of E2E-FANet, we
constructed a multi-level validation framework comprising three distinct
testing scenarios: internal validation under identical wave conditions,
generalization testing across different wave conditions, and adaptability
testing with varying relative water density (RW) conditions. These
comprehensive tests demonstrate that E2E-FANet provides accurate waves behind
FB predictions while successfully generalizing diverse wave conditions.

</details>


### [129] [Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws](https://arxiv.org/abs/2505.06699)
*Xiyuan Wei,Ming Lin,Fanjiang Ye,Fengguang Song,Liangliang Cao,My T. That,Tianbao Yang*

Main category: cs.LG

TL;DR: 论文提出了一个基于分布鲁棒优化的理论框架DRRho风险最小化，用于指导目标模型的训练。通过理论分析，该方法提高了泛化能力和数据效率，并在CLIP任务中验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 现有模型引导的方法虽在实践中有所应用，但缺乏理论基础，导致性能不佳。本文旨在通过理论分析和框架设计，填补这一空白并提升性能。

Method: 提出了DRRho风险最小化框架，基于分布鲁棒优化理论，并结合对比学习与DRO的联系，设计了DRRho-CLIP方法。

Result: 理论分析证明了该方法的优越性，实验验证了其在CLIP任务中的高效性和扩展性表现优于现有启发式方法。

Conclusion: 研究为模型引导这一新兴学习范式提供了首个理论基础，并通过理论驱动的框架显著提升了性能和实践效果。

Abstract: This paper formalizes an emerging learning paradigm that uses a trained model
as a reference to guide and enhance the training of a target model through
strategic data selection or weighting, named $\textbf{model steering}$. While
ad-hoc methods have been used in various contexts, including the training of
large foundation models, its underlying principles remain insufficiently
understood, leading to sub-optimal performance. In this work, we propose a
theory-driven framework for model steering called $\textbf{DRRho risk
minimization}$, which is rooted in Distributionally Robust Optimization (DRO).
Through a generalization analysis, we provide theoretical insights into why
this approach improves generalization and data efficiency compared to training
without a reference model. To the best of our knowledge, this is the first time
such theoretical insights are provided for the new learning paradigm, which
significantly enhance our understanding and practice of model steering.
Building on these insights and the connection between contrastive learning and
DRO, we introduce a novel method for Contrastive Language-Image Pretraining
(CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments
validate the theoretical insights, reveal a superior scaling law compared to
CLIP without a reference model, and demonstrate its strength over existing
heuristic approaches.

</details>


### [130] [Beyond $\tilde{O}(\sqrt{T})$ Constraint Violation for Online Convex Optimization with Adversarial Constraints](https://arxiv.org/abs/2505.06709)
*Abhishek Sinha,Rahul Vaze*

Main category: cs.LG

TL;DR: 该论文针对带对抗性约束的在线凸优化问题（COCO），提出了一种新策略，通过权衡后悔值和累计约束违反（CCV），显著降低了CCV。方法包括从约束专家问题入手，利用自适应小损失后悔边界，并最终通过平滑性假设提出梯度策略。


<details>
  <summary>Details</summary>
Motivation: 研究动机是通过改进现有策略，实现在线凸优化问题中后悔值和CCV的更好权衡，尤其是在高维决策空间中。

Method: 方法首先从约束专家问题（决策集为概率单纯形）入手，提出自适应小损失后悔边界的高效策略；再通过覆盖论证扩展到一般问题；最后基于平滑性假设提出梯度策略。

Result: 结果表明，新策略在后悔值（$	ilde{O}(/sqrt{dT}+ T^eta)$）和CCV（$	ilde{O}(dT^{1-eta})$）上均优于现有最优结果（$O(/sqrt{T})$后悔值，$	ilde{O}(/sqrt{T})$ CCV）。

Conclusion: 论文结论表明，通过权衡后悔值和CCV，新策略在高维环境中实现了更优性能，并通过平滑性假设进一步优化了计算效率。

Abstract: We revisit the Online Convex Optimization problem with adversarial
constraints (COCO) where, in each round, a learner is presented with a convex
cost function and a convex constraint function, both of which may be chosen
adversarially. The learner selects actions from a convex decision set in an
online fashion, with the goal of minimizing both regret and the cumulative
constraint violation (CCV) over a horizon of $T$ rounds. The best-known policy
for this problem achieves $O(\sqrt{T})$ regret and $\tilde{O}(\sqrt{T})$ CCV.
In this paper, we present a surprising improvement that achieves a
significantly smaller CCV by trading it off with regret. Specifically, for any
bounded convex cost and constraint functions, we propose an online policy that
achieves $\tilde{O}(\sqrt{dT}+ T^\beta)$ regret and $\tilde{O}(dT^{1-\beta})$
CCV, where $d$ is the dimension of the decision set and $\beta \in [0,1]$ is a
tunable parameter. We achieve this result by first considering the special case
of $\textsf{Constrained Expert}$ problem where the decision set is a
probability simplex and the cost and constraint functions are linear.
Leveraging a new adaptive small-loss regret bound, we propose an efficient
policy for the $\textsf{Constrained Expert}$ problem, that attains
$O(\sqrt{T\ln N}+T^{\beta})$ regret and $\tilde{O}(T^{1-\beta} \ln N)$ CCV,
where $N$ is the number of experts. The original problem is then reduced to the
$\textsf{Constrained Expert}$ problem via a covering argument. Finally, with an
additional smoothness assumption, we propose an efficient gradient-based policy
attaining $O(T^{\max(\frac{1}{2},\beta)})$ regret and $\tilde{O}(T^{1-\beta})$
CCV.

</details>


### [131] [Activity and Subject Detection for UCI HAR Dataset with & without missing Sensor Data](https://arxiv.org/abs/2505.06730)
*Debashish Saha,Piyush Malik,Adrika Saha*

Main category: cs.LG

TL;DR: 提出基于LSTM的轻量级模型，用于活动和人员分类，并在UCI HAR数据集上取得高精度。同时探索缺失传感器数据填充方法，发现KNN插补效果最佳。


<details>
  <summary>Details</summary>
Motivation: 当前HAR研究多关注活动分类而非人员识别，且未解决传感器数据缺失问题。本研究旨在填补这些空白，推动个性化和上下文敏感应用的发展。

Method: 采用LSTM模型进行活动和人员分类，模拟数据缺失场景，并比较KNN和PCA插补技术的效果。

Result: 活动识别准确率93.89%，接近基准96.67%；人员识别准确率80.19%，KNN插补在无PCA时表现最优。

Conclusion: 该框架有效处理缺失数据，为HAR数据集的可靠分类任务提供了重要进展。

Abstract: Current studies in Human Activity Recognition (HAR) primarily focus on the
classification of activities through sensor data, while there is not much
emphasis placed on recognizing the individuals performing these activities.
This type of classification is very important for developing personalized and
context-sensitive applications. Additionally, the issue of missing sensor data,
which often occurs in practical situations due to hardware malfunctions, has
not been explored yet. This paper seeks to fill these voids by introducing a
lightweight LSTM-based model that can be used to classify both activities and
subjects. The proposed model was used to classify the HAR dataset by UCI [1],
achieving an accuracy of 93.89% in activity recognition (across six
activities), nearing the 96.67% benchmark, and an accuracy of 80.19% in subject
recognition (involving 30 subjects), thereby establishing a new baseline for
this area of research. We then simulate the absence of sensor data to mirror
real-world scenarios and incorporate imputation techniques, both with and
without Principal Component Analysis (PCA), to restore incomplete datasets. We
found that K-Nearest Neighbors (KNN) imputation performs the best for filling
the missing sensor data without PCA because the use of PCA resulted in slightly
lower accuracy. These results demonstrate how well the framework handles
missing sensor data, which is a major step forward in using the Human Activity
Recognition dataset for reliable classification tasks.

</details>


### [132] [Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety](https://arxiv.org/abs/2505.06843)
*Zihan Guan,Mengxuan Hu,Ronghang Zhu,Sheng Li,Anil Vullikanti*

Main category: cs.LG

TL;DR: 该研究发现，即使在良性数据集上微调大型语言模型（LLMs），也可能导致输出危害性显著增加。团队提出一种攻击方法Self-Inf-N，通过检测和提取异常样本来微调模型，实验表明仅用100个异常样本即可严重破坏模型的安全性。现有防御策略对此攻击几乎无效，凸显了对更鲁棒对齐保护的需求。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明，即便是良性数据集的微调也可能导致LLM输出危害性增加。本研究进一步探索这一威胁，旨在开发更有效的攻击方法，揭示现有安全对齐措施的不足。

Method: 从异常检测角度出发，提出Self-Inf-N方法，识别良性数据集中对安全性影响最大的样本，并仅用这些样本微调LLMs。

Result: 实验证明，仅用100个Self-Inf-N选出的异常样本微调即可显著破坏LLM的安全性。该攻击在7种主流LLMs上具有高迁移性，且现有防御策略难以抵御。

Conclusion: 研究暴露了当前LLM安全对齐的脆弱性，亟需开发更鲁棒的防护措施。

Abstract: Recent studies have uncovered a troubling vulnerability in the fine-tuning
stage of large language models (LLMs): even fine-tuning on entirely benign
datasets can lead to a significant increase in the harmfulness of LLM outputs.
Building on this finding, our red teaming study takes this threat one step
further by developing a more effective attack. Specifically, we analyze and
identify samples within benign datasets that contribute most to safety
degradation, then fine-tune LLMs exclusively on these samples. We approach this
problem from an outlier detection perspective and propose Self-Inf-N, to detect
and extract outliers for fine-tuning. Our findings reveal that fine-tuning LLMs
on 100 outlier samples selected by Self-Inf-N in the benign datasets severely
compromises LLM safety alignment. Extensive experiments across seven mainstream
LLMs demonstrate that our attack exhibits high transferability across different
architectures and remains effective in practical scenarios. Alarmingly, our
results indicate that most existing mitigation strategies fail to defend
against this attack, underscoring the urgent need for more robust alignment
safeguards. Codes are available at
https://github.com/GuanZihan/Benign-Samples-Matter.

</details>


### [133] [Deeply Explainable Artificial Neural Network](https://arxiv.org/abs/2505.06731)
*David Zucker*

Main category: cs.LG

TL;DR: DxANN是一种新型深度学习架构，将可解释性嵌入训练过程，避免了传统后处理方法的计算开销和结果不一致问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习黑盒特性在医学影像等关键领域限制其应用，现有可解释性方法（如SHAP、LIME）存在计算开销大、结果模糊的问题。

Method: 提出基于流框架的DxANN架构，在正向传播中直接生成逐样本、逐特征的解释，无需外部解释方法。

Result: DxANN在保持预测精度的同时实现透明决策，尤其适用于医学影像等任务，并可推广至表格和序列数据。

Conclusion: DxANN为需可信度的应用提供了本质可解释的深度学习方案，推动了该领域的实用性进展。

Abstract: While deep learning models have demonstrated remarkable success in numerous
domains, their black-box nature remains a significant limitation, especially in
critical fields such as medical image analysis and inference. Existing
explainability methods, such as SHAP, LIME, and Grad-CAM, are typically applied
post hoc, adding computational overhead and sometimes producing inconsistent or
ambiguous results. In this paper, we present the Deeply Explainable Artificial
Neural Network (DxANN), a novel deep learning architecture that embeds
explainability ante hoc, directly into the training process. Unlike
conventional models that require external interpretation methods, DxANN is
designed to produce per-sample, per-feature explanations as part of the forward
pass. Built on a flow-based framework, it enables both accurate predictions and
transparent decision-making, and is particularly well-suited for image-based
tasks. While our focus is on medical imaging, the DxANN architecture is readily
adaptable to other data modalities, including tabular and sequential data.
DxANN marks a step forward toward intrinsically interpretable deep learning,
offering a practical solution for applications where trust and accountability
are essential.

</details>


### [134] [LineFlow: A Framework to Learn Active Control of Production Lines](https://arxiv.org/abs/2505.06744)
*Kai Müller,Martin Wenzel,Tobias Windisch*

Main category: cs.LG

TL;DR: 本文介绍了LineFlow，一个可扩展的开源Python框架，用于模拟任意复杂度的生产线并训练强化学习（RL）代理控制它们。通过核心子问题的数学分析和基准测试，证明了RL在简单场景中接近最优性能，但在复杂工业规模生产中仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 生产线的动态控制（如自适应路由、工人重新分配和重新调度）设计复杂且缺乏通用框架，强化学习虽具潜力但缺乏标准化解决方案，因此需要LineFlow来解决这一问题。

Method: 提出LineFlow框架，支持模拟复杂生产线并训练RL代理；通过数学分析验证其理论假设，并为子问题提供最优解作为对比基准。

Result: RL算法在已知场景中接近最优性能，但在工业级复杂生产线上仍存在显著挑战，例如奖励设计、渐进学习和分层控制的需求。

Conclusion: LineFlow为生产线RL控制提供了标准化工具，但复杂场景的优化需进一步研究。

Abstract: Many production lines require active control mechanisms, such as adaptive
routing, worker reallocation, and rescheduling, to maintain optimal
performance. However, designing these control systems is challenging for
various reasons, and while reinforcement learning (RL) has shown promise in
addressing these challenges, a standardized and general framework is still
lacking. In this work, we introduce LineFlow, an extensible, open-source Python
framework for simulating production lines of arbitrary complexity and training
RL agents to control them. To demonstrate the capabilities and to validate the
underlying theoretical assumptions of LineFlow, we formulate core subproblems
of active line control in ways that facilitate mathematical analysis. For each
problem, we provide optimal solutions for comparison. We benchmark
state-of-the-art RL algorithms and show that the learned policies approach
optimal performance in well-understood scenarios. However, for more complex,
industrial-scale production lines, RL still faces significant challenges,
highlighting the need for further research in areas such as reward shaping,
curriculum learning, and hierarchical control.

</details>


### [135] [Boltzmann Classifier: A Thermodynamic-Inspired Approach to Supervised Learning](https://arxiv.org/abs/2505.06753)
*Muhamed Amin,Bernard R. Brooks*

Main category: cs.LG

TL;DR: 文章提出了一种基于玻尔兹曼分布的热力学原理的新型分类算法——玻尔兹曼分类器，通过特征偏差计算类概率，无需迭代优化，计算高效且易于集成。


<details>
  <summary>Details</summary>
Motivation: 受热力学中玻尔兹曼分布的启发，设计一种新型分类算法，提供基于能量函数的概率解释，同时保持计算高效性和易用性。

Method: 通过样本特征与类特定质心的偏差计算能量函数，利用玻尔兹曼分布生成归一化的类概率。KT变量的引入可调节高能量状态的可访问性。

Result: 在多个数据集上的实验表明，该分类器在准确性上与逻辑回归和K近邻等标准模型相当，同时具有计算高效性和物理可解释性。

Conclusion: 玻尔兹曼分类器展示了物理学思想如何为机器学习提供新方向，为可解释的基于能量的决策系统奠定了基础。

Abstract: We propose a novel classification algorithm, the Boltzmann Classifier,
inspired by the thermodynamic principles underlying the Boltzmann distribution.
Our method computes a probabilistic estimate for each class based on an energy
function derived from feature-wise deviations between input samples and
class-specific centroids. The resulting probabilities are proportional to the
exponential negative energies, normalized across classes, analogous to the
Boltzmann distribution used in statistical mechanics. In addition, the KT
variable can be used to allow the high energy states to be more accessible,
which allows the tuning of their probabilities as needed. We evaluate the model
performance on several datasets from different applications. The model achieves
a high accuracy, which indicates that the Boltzmann Classifier is competitive
with standard models like logistic regression and k-nearest neighbors while
offering a thermodynamically motivated probabilistic interpretation. our
classifier does not require iterative optimization or backpropagation and is
thus computationally efficient and easy to integrate into existing workflows.
This work demonstrates how ideas from physics can inform new directions in
machine learning, providing a foundation for interpretable, energy-based
decision-making systems.

</details>


### [136] [Privacy-aware Berrut Approximated Coded Computing applied to general distributed learning](https://arxiv.org/abs/2505.06759)
*Xavier Martínez-Luaña,Manuel Fernández-Veiga,Rebeca P. Díaz-Redondo,Ana Fernández-Vilas*

Main category: cs.LG

TL;DR: 本文探讨了基于Private Berrut近似编码计算（PBACC）的联邦学习隐私保护方法，提出新算法支持集中式和分散式训练，隐私泄露可严格限制并适用于多种模型。


<details>
  <summary>Details</summary>
Motivation: 现有编码计算技术在联邦学习中仅适用于精确计算和特定函数类别，限制了隐私保护的通用性。本文旨在通过PBACC为联邦学习提供更通用的隐私保护方案。

Method: 提出了适用于集中式聚合、集中数据的安全分布式训练及分散数据的安全分散训练的PBACC算法。

Result: 实验表明，PBACC对不同模型（如CNN、VAE等）的性能影响极小，且隐私泄露可严格限制（每参与者少于1比特）。计算成本仅取决于数据分散程度。

Conclusion: PBACC显著扩展了隐私保护工具的应用范围，为分散式联邦学习提供了高效的隐私保障方案。

Abstract: Coded computing is one of the techniques that can be used for privacy
protection in Federated Learning. However, most of the constructions used for
coded computing work only under the assumption that the computations involved
are exact, generally restricted to special classes of functions, and require
quantized inputs. This paper considers the use of Private Berrut Approximate
Coded Computing (PBACC) as a general solution to add strong but non-perfect
privacy to federated learning. We derive new adapted PBACC algorithms for
centralized aggregation, secure distributed training with centralized data, and
secure decentralized training with decentralized data, thus enlarging
significantly the applications of the method and the existing privacy
protection tools available for these paradigms. Particularly, PBACC can be used
robustly to attain privacy guarantees in decentralized federated learning for a
variety of models. Our numerical results show that the achievable quality of
different learning models (convolutional neural networks, variational
autoencoders, and Cox regression) is minimally altered by using these new
computing schemes, and that the privacy leakage can be bounded strictly to less
than a fraction of one bit per participant. Additionally, the computational
cost of the encoding and decoding processes depends only of the degree of
decentralization of the data.

</details>


### [137] [Towards the Three-Phase Dynamics of Generalization Power of a DNN](https://arxiv.org/abs/2505.06993)
*Yuxuan He,Junpeng Zhang,Hongyuan Zhang,Quanshi Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种分析深度神经网络（DNNs）泛化能力的新视角，通过直接解耦和分析DNN在训练过程中编码的可泛化和非泛化交互的动态变化。研究发现交互的泛化能力在训练过程中呈现三段动态，并为训练与测试损失间的差距提供了直接解释。


<details>
  <summary>Details</summary>
Motivation: 当前对DNN泛化能力的理解还不够深入，尤其是训练过程中交互的动态变化如何影响泛化性能。本研究旨在填补这一空白，通过量化交互的泛化能力来分析DNN的学习行为。

Method: 基于可解释AI的最新理论成果，将DNN的推理逻辑严格改写为少量AND-OR交互模式，并提出一种量化交互泛化能力的高效方法。通过实验分析训练过程中交互的动态变化。

Result: 实验揭示了交互泛化能力的三段动态：早期去除噪声和非泛化交互，中后期学习更复杂但泛化能力较差的交互。非泛化交互的学习是训练与测试损失差距的直接原因。

Conclusion: 该研究为DNN泛化能力提供了新的分析视角，揭示了训练过程中交互动态的关键作用，为改进模型泛化性能提供了理论依据。

Abstract: This paper proposes a new perspective for analyzing the generalization power
of deep neural networks (DNNs), i.e., directly disentangling and analyzing the
dynamics of generalizable and non-generalizable interaction encoded by a DNN
through the training process. Specifically, this work builds upon the recent
theoretical achievement in explainble AI, which proves that the detailed
inference logic of DNNs can be can be strictly rewritten as a small number of
AND-OR interaction patterns. Based on this, we propose an efficient method to
quantify the generalization power of each interaction, and we discover a
distinct three-phase dynamics of the generalization power of interactions
during training. In particular, the early phase of training typically removes
noisy and non-generalizable interactions and learns simple and generalizable
ones. The second and the third phases tend to capture increasingly complex
interactions that are harder to generalize. Experimental results verify that
the learning of non-generalizable interactions is the the direct cause for the
gap between the training and testing losses.

</details>


### [138] [Learning Graph Representation of Agent Diffuser](https://arxiv.org/abs/2505.06761)
*Youcef Djenouri,Nassim Belmecheri,Tomasz Michalak,Jan Dubiński,Ahmed Nabil Belbachir,Anis Yazidi*

Main category: cs.LG

TL;DR: LGR-AD 是一种基于多智能体的系统，通过图神经网络和动态协调机制改进扩散模型在文本到图像生成中的表现。


<details>
  <summary>Details</summary>
Motivation: 静态模型参数可能无法适应生成过程的不同阶段，因此需要一种更灵活的方法来提高生成质量和多样性。

Method: 提出了LGR-AD，这是一个多智能体系统，通过图神经网络编码智能体之间的关系，并采用基于最大生成树的协调机制优化生成过程。

Result: 理论和实验表明，LGR-AD在多个基准测试中优于传统扩散模型。

Conclusion: LGR-AD 提供了一种可扩展且灵活的方法，适用于复杂的图像生成任务。

Abstract: Diffusion-based generative models have significantly advanced text-to-image
synthesis, demonstrating impressive text comprehension and zero-shot
generalization. These models refine images from random noise based on textual
prompts, with initial reliance on text input shifting towards enhanced visual
fidelity over time. This transition suggests that static model parameters might
not optimally address the distinct phases of generation. We introduce LGR-AD
(Learning Graph Representation of Agent Diffusers), a novel multi-agent system
designed to improve adaptability in dynamic computer vision tasks. LGR-AD
models the generation process as a distributed system of interacting agents,
each representing an expert sub-model. These agents dynamically adapt to
varying conditions and collaborate through a graph neural network that encodes
their relationships and performance metrics. Our approach employs a
coordination mechanism based on top-$k$ maximum spanning trees, optimizing the
generation process. Each agent's decision-making is guided by a meta-model that
minimizes a novel loss function, balancing accuracy and diversity. Theoretical
analysis and extensive empirical evaluations show that LGR-AD outperforms
traditional diffusion models across various benchmarks, highlighting its
potential for scalable and flexible solutions in complex image generation
tasks. Code is available at: https://github.com/YousIA/LGR_AD

</details>


### [139] [Investigating Robotaxi Crash Severity Using Geographical Random Forest](https://arxiv.org/abs/2505.06762)
*Junfeng Jiao,Seung Gyu Baik,Seung Jun Choi,Yiming Xu*

Main category: cs.LG

TL;DR: 该论文使用空间局部机器学习和城市建成环境的宏观测量，研究自动驾驶车辆的碰撞严重性。通过地理随机森林模型（GRF）发现，空间局部机器学习在预测碰撞严重性上表现更优，土地利用是最重要的影响因素，市中心区域的低严重性碰撞概率更高。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决自动驾驶车辆碰撞严重性与城市建成环境之间的空间异质性和自相关性，为商业运营提供安全改进建议。

Method: 采用地理随机森林模型（GRF）结合空间局部机器学习和建成环境测量（如土地利用、交叉口等），分析旧金山碰撞严重性风险图。

Result: 空间局部机器学习优于常规方法，土地利用对碰撞严重性影响最大，市中心区域的低严重性碰撞概率显著高于住宅区。

Conclusion: 建议商业运营中明确考虑地理位置，并为住宅区设计特定安全措施，以提升自动驾驶系统的安全性。

Abstract: This paper quantitatively investigates the crash severity of Autonomous
Vehicles (AVs) with spatially localized machine learning and macroscopic
measures of the urban built environment. We address spatial heterogeneity and
spatial autocorrelation, while focusing on land use patterns and human
behavior. Our Geographical Random Forest (GRF) model, accompanied with a crash
severity risk map of San Francisco, presents three findings that are useful for
commercial operations of AVs and robotaxis. First, spatially localized machine
learning performed better than regular machine learning, when predicting AV
crash severity. Bias-variance tradeoff was evident as we adjust the
localization weight hyperparameter. Second, land use was the most important
built environment measure, compared to intersections, building footprints,
public transit stops, and Points Of Interests (POIs). Third, it was predicted
that city center areas with greater diversity and commercial activities were
more likely to result in low-severity AV crashes, than residential
neighborhoods. Residential land use may be associated with higher severity due
to human behavior and less restrictive environment. This paper recommends to
explicitly consider geographic locations, and to design safety measures
specific to residential neighborhoods, when robotaxi operators train their AV
systems.

</details>


### [140] [Decoding Futures Price Dynamics: A Regularized Sparse Autoencoder for Interpretable Multi-Horizon Forecasting and Factor Discovery](https://arxiv.org/abs/2505.06795)
*Abhijit Gupta*

Main category: cs.LG

TL;DR: 该论文提出了一种正则化稀疏自编码器（RSAE），用于多时间范围的商品价格预测和可解释市场驱动因素发现。RSAE通过L1正则化强制稀疏性，同时保持预测准确性，并在铜和原油数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 商品价格波动对经济造成挑战，现有预测模型缺乏透明度和解释性，限制了其战略应用。RSAE旨在解决这一问题，实现多时间范围预测并揭示市场驱动因素。

Method: RSAE是一个深度学习框架，结合L1正则化（$\|\mathbf{z}\|_1$）对潜在向量进行稀疏约束，通过学习稀疏表示优化预测准确性，并从多变量时间序列数据中提取可解释的潜在驱动因素。

Result: 在铜和原油历史数据上的实验表明，RSAE在多时间范围预测方面具有竞争力，并通过其可解释的潜在空间提供了对价格动态的数据驱动洞察。

Conclusion: RSAE不仅提高了商品价格预测的准确性，还通过稀疏潜在表示增强了模型的可解释性，优于传统的黑箱方法。

Abstract: Commodity price volatility creates economic challenges, necessitating
accurate multi-horizon forecasting. Predicting prices for commodities like
copper and crude oil is complicated by diverse interacting factors
(macroeconomic, supply/demand, geopolitical, etc.). Current models often lack
transparency, limiting strategic use. This paper presents a Regularized Sparse
Autoencoder (RSAE), a deep learning framework for simultaneous multi-horizon
commodity price prediction and discovery of interpretable latent market
drivers. The RSAE forecasts prices at multiple horizons (e.g., 1-day, 1-week,
1-month) using multivariate time series. Crucially, L1 regularization
($\|\mathbf{z}\|_1$) on its latent vector $\mathbf{z}$ enforces sparsity,
promoting parsimonious explanations of market dynamics through learned factors
representing underlying drivers (e.g., demand, supply shocks). Drawing from
energy-based models and sparse coding, the RSAE optimizes predictive accuracy
while learning sparse representations. Evaluated on historical Copper and Crude
Oil data with numerous indicators, our findings indicate the RSAE offers
competitive multi-horizon forecasting accuracy and data-driven insights into
price dynamics via its interpretable latent space, a key advantage over
traditional black-box approaches.

</details>


### [141] [Topology Guidance: Controlling the Outputs of Generative Models via Vector Field Topology](https://arxiv.org/abs/2505.06804)
*Xiaohan Wang,Matthew Berger*

Main category: cs.LG

TL;DR: 该论文提出了一种新方法，通过拓扑引导提高生成模型的可控性，使得用户能够指定输出中的拓扑特征，特别是用于数值模拟的2D矢量场。


<details>
  <summary>Details</summary>
Motivation: 在数值模拟中，生成模型可以廉价且准确地合成模拟结果，但缺乏控制性，用户难以指定输出中的特定特征，尤其是拓扑特征。

Method: 提出拓扑引导方法，结合基于坐标的神经网络和扩散模型，利用拓扑相关信号引导扩散模型的去噪过程，确保生成的矢量场满足用户指定的拓扑特征。

Result: 实验表明，生成的2D矢量场能够忠实反映用户指定的临界点位置和类型，同时保持生成数据的分布。方法还支持比较不同拓扑特征的共性差异。

Conclusion: 该方法显著提升了生成模型的可控性，有助于视觉分析和探索拓扑特征的分布差异。

Abstract: For domains that involve numerical simulation, it can be computationally
expensive to run an ensemble of simulations spanning a parameter space of
interest to a user. To this end, an attractive surrogate for simulation is the
generative modeling of fields produced by an ensemble, allowing one to
synthesize fields in a computationally cheap, yet accurate, manner. However,
for the purposes of visual analysis, a limitation of generative models is their
lack of control, as it is unclear what one should expect when sampling a field
from a model. In this paper we study how to make generative models of fields
more controllable, so that users can specify features of interest, in
particular topological features, that they wish to see in the output. We
propose topology guidance, a method for guiding the sampling process of a
generative model, specifically a diffusion model, such that a topological
description specified as input is satisfied in the generated output. Central to
our method, we couple a coordinate-based neural network used to represent
fields, with a diffusion model used for generation. We show how to use
topologically-relevant signals provided by the coordinate-based network to help
guide the denoising process of a diffusion model. This enables us to faithfully
represent a user's specified topology, while ensuring that the output field
remains within the generative data distribution. Specifically, we study 2D
vector field topology, evaluating our method over an ensemble of fluid flows,
where we show that generated vector fields faithfully adhere to the location,
and type, of critical points over the spatial domain. We further show the
benefits of our method in aiding the comparison of ensembles, allowing one to
explore commonalities and differences in distributions along prescribed
topological features.

</details>


### [142] [Deep Learning for On-Street Parking Violation Prediction](https://arxiv.org/abs/2505.06818)
*Thien Nhan Vo*

Main category: cs.LG

TL;DR: 摘要：本文提出了一种基于深度学习的细粒度停车违规预测方法，通过数据增强和平滑技术解决数据缺失和噪声问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 非法停车和停车位不足严重影响城市生活质量，现有路边停车系统因非法停车而无法准确提供车位信息，虽有传感器解决方案但成本过高。本文旨在通过预测违规停车改善这一现状。

Method: 采用基于深度学习的方法预测细粒度停车违规率，结合数据增强和平滑技术处理缺失和噪声数据。

Result: 在希腊塞萨洛尼基的真实数据实验中，该系统能准确预测停车违规行为。

Conclusion: 提出的深度学习模型和数据增强技术能有效提升路边停车系统的信息准确性，为城市停车管理提供实用工具。

Abstract: Illegal parking along with the lack of available parking spaces are among the
biggest issues faced in many large cities. These issues can have a significant
impact on the quality of life of citizens. On-street parking systems have been
designed to this end aiming at ensuring that parking spaces will be available
for the local population, while also providing easy access to parking for
people visiting the city center. However, these systems are often affected by
illegal parking, providing incorrect information regarding the availability of
parking spaces. Even though this can be mitigated using sensors for detecting
the presence of cars in various parking sectors, the cost of these
implementations is usually prohibiting large. In this paper, we investigate an
indirect way of predicting parking violations at a fine-grained level,
equipping such parking systems with a valuable tool for providing more accurate
information to citizens. To this end, we employed a Deep Learning (DL)-based
model to predict fine-grained parking violation rates for on-street parking
systems. Moreover, we developed a data augmentation and smoothing technique for
further improving the accuracy of DL models under the presence of missing and
noisy data. We demonstrate, using experiments on real data collected in
Thessaloniki, Greece, that the developed system can indeed provide accurate
parking violation predictions.

</details>


### [143] [Streaming Sliced Optimal Transport](https://arxiv.org/abs/2505.06835)
*Khai Nguyen*

Main category: cs.LG

TL;DR: 提出了一种称为Stream-SW的新方法，用于从样本流中计算切片Wasserstein距离，具有低内存复杂性和理论保证，优于随机子采样。


<details>
  <summary>Details</summary>
Motivation: 切片最优运输（SOT）或切片Wasserstein（SW）距离因其统计和计算可扩展性而广受认可，但进一步需要从流样本中高效计算SW的方法。

Method: 首先提出流式一维Wasserstein（1DW）距离计算，利用分位数近似技术定义流式1DW，然后将其应用于所有投影以得到Stream-SW。

Result: Stream-SW在内存消耗低的情况下，对SW的近似比随机子采样更准确，并在高斯分布、高斯混合流样本及点云分类等任务中表现出色。

Conclusion: Stream-SW是一种高效、低内存需求的流式SW计算方法，具有理论保证和实际应用潜力。

Abstract: Sliced optimal transport (SOT) or sliced Wasserstein (SW) distance is widely
recognized for its statistical and computational scalability. In this work, we
further enhance the computational scalability by proposing the first method for
computing SW from sample streams, called \emph{streaming sliced Wasserstein}
(Stream-SW). To define Stream-SW, we first introduce the streaming computation
of the one-dimensional Wasserstein distance. Since the one-dimensional
Wasserstein (1DW) distance has a closed-form expression, given by the absolute
difference between the quantile functions of the compared distributions, we
leverage quantile approximation techniques for sample streams to define the
streaming 1DW distance. By applying streaming 1DW to all projections, we obtain
Stream-SW. The key advantage of Stream-SW is its low memory complexity while
providing theoretical guarantees on the approximation error. We demonstrate
that Stream-SW achieves a more accurate approximation of SW than random
subsampling, with lower memory consumption, in comparing Gaussian distributions
and mixtures of Gaussians from streaming samples. Additionally, we conduct
experiments on point cloud classification, point cloud gradient flows, and
streaming change point detection to further highlight the favorable performance
of Stream-SW.

</details>


### [144] [The power of fine-grained experts: Granularity boosts expressivity in Mixture of Experts](https://arxiv.org/abs/2505.06839)
*Enric Boix-Adsera,Philippe Rigollet*

Main category: cs.LG

TL;DR: 研究探讨了混合专家（MoE）层中激活专家数量（粒度）对模型表达力的影响，证明高粒度设计（如每个层激活多个专家）能显著提升模型能力。


<details>
  <summary>Details</summary>
Motivation: 混合专家（MoE）层通过选择性激活参数来降低计算成本，但不同粒度的设计（如每层激活专家数量）对模型性能的影响尚不明确。研究旨在揭示粒度与模型表达力的关系。

Method: 通过理论分析和实验验证，比较不同粒度（如每个层激活8个专家与1个专家）的模型架构，并证明其对网络表达力的影响。

Result: 理论分析表明高粒度设计（多专家激活）带来模型表达力的指数级提升，实验结果支持这一结论。

Conclusion: 混合专家层的粒度设计对模型性能至关重要，高粒度架构能显著增强模型表达力，为未来模型设计提供了重要方向。

Abstract: Mixture-of-Experts (MoE) layers are increasingly central to frontier model
architectures. By selectively activating parameters, they reduce computational
cost while scaling total parameter count. This paper investigates the impact of
the number of active experts, termed granularity, comparing architectures with
many (e.g., 8 per layer in DeepSeek) to those with fewer (e.g., 1 per layer in
Llama-4 models). We prove an exponential separation in network expressivity
based on this design parameter, suggesting that models benefit from higher
granularity. Experimental results corroborate our theoretical findings and
illustrate this separation.

</details>


### [145] [Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models](https://arxiv.org/abs/2505.07558)
*Rei Higuchi,Taiji Suzuki*

Main category: cs.LG

TL;DR: 本文提出了一种名为DDRO的新方法，通过直接估计偏好与非偏好输出的密度比来对齐大型语言模型，避免了传统方法中对特定偏好模型的依赖，证明了其统计一致性，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型对齐方法依赖特定的偏好模型（如Bradley-Terry模型），导致统计不一致性（即数据增加未必收敛至真实人类偏好）。为了解决这一关键问题，需要一种不依赖显式偏好建模且能保证统计一致性的方法。

Method: 作者提出Direct Density Ratio Optimization (DDRO)，直接估计偏好与非偏好输出分布的密度比，无需显式建模人类偏好。理论上证明该方法具有统计一致性，即数据规模增大时能收敛至真实偏好分布。

Result: 实验结果表明，DDRO在多个主流基准测试上优于现有方法，验证了其有效性和优越性。

Conclusion: DDRO为真正数据驱动的对齐提供了可能，推动了更可靠、更贴合人类偏好的大型语言模型的发展。

Abstract: Aligning large language models (LLMs) with human preferences is crucial for
safe deployment, yet existing methods assume specific preference models like
Bradley-Terry model. This assumption leads to statistical inconsistency, where
more data doesn't guarantee convergence to true human preferences. To address
this critical gap, we introduce a novel alignment method Direct Density Ratio
Optimization (DDRO). DDRO directly estimates the density ratio between
preferred and unpreferred output distributions, circumventing the need for
explicit human preference modeling. We theoretically prove that DDRO is
statistically consistent, ensuring convergence to the true preferred
distribution as the data size grows, regardless of the underlying preference
structure. Experiments demonstrate that DDRO achieves superior performance
compared to existing methods on many major benchmarks. DDRO unlocks the
potential for truly data-driven alignment, paving the way for more reliable and
human-aligned LLMs.

</details>


### [146] [Predictive Digital Twins for Thermal Management Using Machine Learning and Reduced-Order Models](https://arxiv.org/abs/2505.06849)
*Tamilselvan Subramani,Sebastian Bartscher*

Main category: cs.LG

TL;DR: 论文提出了一种结合物理降阶模型和监督学习的数字孪生框架，用于车灯散热器的实时预测，其中神经网络表现最佳，误差最低。


<details>
  <summary>Details</summary>
Motivation: 为了提升车灯散热器热管理的效率和实时预测能力，需要结合物理模型和机器学习方法以优化数字孪生的更新速度和准确性。

Method: 通过基于POD的降阶模型库捕捉热力学行为，并用多种机器学习模型（如决策树、k-NN、SVR和神经网络）预测最优模型配置。

Result: 神经网络模型的平均绝对误差最低（54.240），与其他模型相比表现最佳，预测结果与原数据对比显示出高精度。

Conclusion: 该框架具有可扩展性和可解释性，为汽车系统热管理提供了高效的设计和预测维护支持。

Abstract: Digital twins enable real-time simulation and prediction in engineering
systems. This paper presents a novel framework for predictive digital twins of
a headlamp heatsink, integrating physics-based reduced-order models (ROMs) from
computational fluid dynamics (CFD) with supervised machine learning. A
component-based ROM library, derived via proper orthogonal decomposition (POD),
captures thermal dynamics efficiently. Machine learning models, including
Decision Trees, k-Nearest Neighbors, Support Vector Regression (SVR), and
Neural Networks, predict optimal ROM configurations, enabling rapid digital
twin updates. The Neural Network achieves a mean absolute error (MAE) of
54.240, outperforming other models. Quantitative comparisons of predicted and
original values demonstrate high accuracy. This scalable, interpretable
framework advances thermal management in automotive systems, supporting robust
design and predictive maintenance.

</details>


### [147] [Improving Random Forests by Smoothing](https://arxiv.org/abs/2505.06852)
*Ziyi Liu,Phuc Luong,Mario Boley,Daniel F. Schmidt*

Main category: cs.LG

TL;DR: 该论文提出一种结合高斯过程回归和随机森林的方法，通过在随机森林或其他分段常数预测函数上应用基于核的平滑机制，以提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 高斯过程回归在小数据场景下表现良好，但对输入域内平滑度不均匀时较难处理；随机森林虽能适应局部变化但缺乏平滑性，导致小数据场景下性能不佳。本文旨在结合两者优势。

Method: 在学习的随机森林或其他分段常数预测函数上应用基于核的平滑机制。

Result: 新模型显著提升了随机森林的预测性能，并在几乎所有测试案例中改进了对数损失。

Conclusion: 通过结合高斯过程和随机森林的优势，新模型在小数据场景下表现更优，且能更好地处理平滑度不均匀问题。

Abstract: Gaussian process regression is a popular model in the small data regime due
to its sound uncertainty quantification and the exploitation of the smoothness
of the regression function that is encountered in a wide range of practical
problems. However, Gaussian processes perform sub-optimally when the degree of
smoothness is non-homogeneous across the input domain. Random forest regression
partially addresses this issue by providing local basis functions of variable
support set sizes that are chosen in a data-driven way. However, they do so at
the expense of forgoing any degree of smoothness, which often results in poor
performance in the small data regime. Here, we aim to combine the advantages of
both models by applying a kernel-based smoothing mechanism to a learned random
forest or any other piecewise constant prediction function. As we demonstrate
empirically, the resulting model consistently improves the predictive
performance of the underlying random forests and, in almost all test cases,
also improves the log loss of the usual uncertainty quantification based on
inter-tree variance. The latter advantage can be attributed to the ability of
the smoothing model to take into account the uncertainty over the exact
tree-splitting locations.

</details>


### [148] [FreqMoE: Dynamic Frequency Enhancement for Neural PDE Solvers](https://arxiv.org/abs/2505.06858)
*Tianyu Chen,Haoyi Zhou,Ying Li,Hao Wang,Zhenzhe Zhang,Tianchen Zhu,Shanghang Zhang,Jianxin Li*

Main category: cs.LG

TL;DR: FreqMoE是一种高效渐进训练框架，通过低频到高频的信号依赖学习来解决PDE问题，提升计算效率与性能。


<details>
  <summary>Details</summary>
Motivation: 高频信号稀疏与固定截断导致传统FNO在高维输入或长期预测中效率与性能受限。

Method: 先学习低频权重，再通过稀疏上行循环策略构建频域MoE，将权重扩展到高频区域。

Result: 在规则与非规则网格PDE上，FreqMoE精度提升16.6%，参数减少47.32倍，且长期预测稳定。

Conclusion: 提出“低频预训练-高频微调”新范式，适用于多种FNO变体与网格结构。

Abstract: Fourier Neural Operators (FNO) have emerged as promising solutions for
efficiently solving partial differential equations (PDEs) by learning
infinite-dimensional function mappings through frequency domain
transformations. However, the sparsity of high-frequency signals limits
computational efficiency for high-dimensional inputs, and fixed-pattern
truncation often causes high-frequency signal loss, reducing performance in
scenarios such as high-resolution inputs or long-term predictions. To address
these challenges, we propose FreqMoE, an efficient and progressive training
framework that exploits the dependency of high-frequency signals on
low-frequency components. The model first learns low-frequency weights and then
applies a sparse upward-cycling strategy to construct a mixture of experts
(MoE) in the frequency domain, effectively extending the learned weights to
high-frequency regions. Experiments on both regular and irregular grid PDEs
demonstrate that FreqMoE achieves up to 16.6% accuracy improvement while using
merely 2.1% parameters (47.32x reduction) compared to dense FNO. Furthermore,
the approach demonstrates remarkable stability in long-term predictions and
generalizes seamlessly to various FNO variants and grid structures,
establishing a new ``Low frequency Pretraining, High frequency Fine-tuning''
paradigm for solving PDEs.

</details>


### [149] [Masked Subspace Clustering Methods](https://arxiv.org/abs/2505.06863)
*Jiebo Song,Huaming Ling*

Main category: cs.LG

TL;DR: 该论文提出了一个Bilevel Clustering Optimization (BCO)框架来提升聚类性能，并通过三种子空间聚类的特殊案例（BMSC、GMSC、RMSC）展示其有效性。实验表明，这些方法在多个常用数据集上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 为了进一步利用无监督特征和成对信息，提升聚类性能，作者提出了BCO框架及其具体实现方法。

Method: 论文引入了基于BCO框架的三种子空间聚类方法：BMSC（硬掩码）、GMSC（软掩码）和RMSC（可学习软掩码），通过交替更新相似矩阵和掩码优化聚类效果。

Result: 实验在MNIST、USPS、ORL、COIL20和COIL100等数据集上验证了方法的有效性，性能显著优于基线模型。

Conclusion: BCO框架及其衍生方法（BMSC、GMSC、RMSC）通过掩码机制显著提升了聚类性能，展现了在无监督学习中的潜力。

Abstract: To further utilize the unsupervised features and pairwise information, we
propose a general Bilevel Clustering Optimization (BCO) framework to improve
the performance of clustering. And then we introduce three special cases on
subspace clustering with two different types of masks. At first, we reformulate
the original subspace clustering as a Basic Masked Subspace Clustering (BMSC),
which reformulate the diagonal constraints to a hard mask. Then, we provide a
General Masked Subspace Clustering (GMSC) method to integrate different
clustering via a soft mask. Furthermore, based on BCO and GMSC, we induce a
learnable soft mask and design a Recursive Masked Subspace Clustering (RMSC)
method that can alternately update the affinity matrix and the soft mask.
Numerical experiments show that our models obtain significant improvement
compared with the baselines on several commonly used datasets, such as MNIST,
USPS, ORL, COIL20 and COIL100.

</details>


### [150] [Enhancing Time Series Forecasting via a Parallel Hybridization of ARIMA and Polynomial Classifiers](https://arxiv.org/abs/2505.06874)
*Thanh Son Nguyen,Van Thanh Nguyen,Dang Minh Duc Nguyen*

Main category: cs.LG

TL;DR: 该研究提出了一种结合ARIMA模型和多项式分类器的混合时间序列预测方法，通过整合两者的优势，在多个真实数据集上验证了其优于单一模型的预测准确性，尽管计算时间略有增加。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测领域已有多种方法，传统ARIMA模型擅长捕捉线性时间依赖，而多项式分类器在非线性关系中表现优秀。为结合两者的优势，研究提出了混合方法，以提升预测性能。

Method: 通过整合ARIMA模型的线性时间依赖建模能力和多项式分类器的非线性关系捕捉能力，构建混合预测框架，并在多种真实时间序列数据集上进行评估。

Result: 实验表明，混合模型在预测准确性上显著优于单独的ARIMA或多项式分类器，但计算时间略有增加。

Conclusion: 混合模型通过结合线性和非线性方法的优势，提供了更准确的时间序列预测方案，适合多领域的应用。

Abstract: Time series forecasting has attracted significant attention, leading to the
de-velopment of a wide range of approaches, from traditional statistical
meth-ods to advanced deep learning models. Among them, the Auto-Regressive
Integrated Moving Average (ARIMA) model remains a widely adopted linear
technique due to its effectiveness in modeling temporal dependencies in
economic, industrial, and social data. On the other hand, polynomial
classifi-ers offer a robust framework for capturing non-linear relationships
and have demonstrated competitive performance in domains such as stock price
pre-diction. In this study, we propose a hybrid forecasting approach that
inte-grates the ARIMA model with a polynomial classifier to leverage the
com-plementary strengths of both models. The hybrid method is evaluated on
multiple real-world time series datasets spanning diverse domains. Perfor-mance
is assessed based on forecasting accuracy and computational effi-ciency.
Experimental results reveal that the proposed hybrid model consist-ently
outperforms the individual models in terms of prediction accuracy, al-beit with
a modest increase in execution time.

</details>


### [151] [Image Classification Using a Diffusion Model as a Pre-Training Model](https://arxiv.org/abs/2505.06890)
*Kosuke Ukita,Ye Xiaolong,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: 该论文提出了一种结合视觉Transformer（ViT）表示的条件扩散模型，通过自监督学习减少对大规模标注数据的依赖，并在脑影像血肿检测任务中表现出优于对比学习基线（DINOv2）的性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统生成模型对大规模标注数据的依赖问题，通过自监督学习利用未标注数据实现更高效的数据生成。

Method: 提出了一种基于扩散模型的表示条件机制，利用ViT生成的条件表示指导扩散过程的内部生成。

Result: 在零样本血肿检测任务中，准确率和F1分数分别比DINOv2基线提高了6.15%和13.60%。

Conclusion: 该方法在减少数据标注需求的同时提升了生成模型的分类性能，展示了自监督学习在医学影像分析中的潜力。

Abstract: In this paper, we propose a diffusion model that integrates a
representation-conditioning mechanism, where the representations derived from a
Vision Transformer (ViT) are used to condition the internal process of a
Transformer-based diffusion model. This approach enables
representation-conditioned data generation, addressing the challenge of
requiring large-scale labeled datasets by leveraging self-supervised learning
on unlabeled data. We evaluate our method through a zero-shot classification
task for hematoma detection in brain imaging. Compared to the strong
contrastive learning baseline, DINOv2, our method achieves a notable
improvement of +6.15% in accuracy and +13.60% in F1-score, demonstrating its
effectiveness in image classification.

</details>


### [152] [Learning Soft Sparse Shapes for Efficient Time-Series Classification](https://arxiv.org/abs/2505.06892)
*Zhen Liu,Yicheng Luo,Boyuan Li,Emadeldeen Eldele,Min Wu,Qianli Ma*

Main category: cs.LG

TL;DR: 该论文提出了一种名为SoftShape的新模型，通过软稀疏化和软形状学习块来高效分类时间序列，保留所有子序列信息并提高分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有形状发现方法因时间密集性而丢弃部分形状，可能导致有益信息丢失且忽视形状对分类的贡献差异。

Method: 通过软形状稀疏化和软形状学习块，将形状转化为软表示并优化模型效率。

Result: 实验表明SoftShape性能优于现有方法，并能生成可解释结果。

Conclusion: SoftShape模型通过保留和区分所有子序列信息，显著提升了时间序列分类的效率和准确性。

Abstract: Shapelets are discriminative subsequences (or shapes) with high
interpretability in time series classification. Due to the time-intensive
nature of shapelet discovery, existing shapelet-based methods mainly focus on
selecting discriminative shapes while discarding others to achieve candidate
subsequence sparsification. However, this approach may exclude beneficial
shapes and overlook the varying contributions of shapelets to classification
performance. To this end, we propose a \textbf{Soft} sparse \textbf{Shape}s
(\textbf{SoftShape}) model for efficient time series classification. Our
approach mainly introduces soft shape sparsification and soft shape learning
blocks. The former transforms shapes into soft representations based on
classification contribution scores, merging lower-scored ones into a single
shape to retain and differentiate all subsequence information. The latter
facilitates intra- and inter-shape temporal pattern learning, improving model
efficiency by using sparsified soft shapes as inputs. Specifically, we employ a
learnable router to activate a subset of class-specific expert networks for
intra-shape pattern learning. Meanwhile, a shared expert network learns
inter-shape patterns by converting sparsified shapes into sequences. Extensive
experiments show that SoftShape outperforms state-of-the-art methods and
produces interpretable results.

</details>


### [153] [MMiC: Mitigating Modality Incompleteness in Clustered Federated Learning](https://arxiv.org/abs/2505.06911)
*Lishan Yang,Wei Zhang,Quan Z. Sheng,Weitong Chen,Lina Yao,Weitong Chen,Ali Shakeri*

Main category: cs.LG

TL;DR: MMiC框架通过参数替换、Banzhaf权力指数优化客户端选择及Markovitz投资组合优化动态控制全局聚合，有效解决多模态联邦学习中模态缺失问题，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态联邦学习中模态缺失问题由于数据质量或隐私政策而普遍存在，影响学习效率和质量。

Method: 提出MMiC框架，包括客户端模型参数替换、Banzhaf权力指数优化选择和Markovitz投资组合优化全局聚合。

Result: 在模态缺失的多模态数据集上，MMiC在全球和个性化性能上均优于现有联邦学习架构。

Conclusion: MMiC有效解决了多模态联邦学习中的模态缺失问题，提升了学习效果，为相关领域提供了新思路。

Abstract: In the era of big data, data mining has become indispensable for uncovering
hidden patterns and insights from vast and complex datasets. The integration of
multimodal data sources further enhances its potential. Multimodal Federated
Learning (MFL) is a distributed approach that enhances the efficiency and
quality of multimodal learning, ensuring collaborative work and privacy
protection. However, missing modalities pose a significant challenge in MFL,
often due to data quality issues or privacy policies across the clients. In
this work, we present MMiC, a framework for Mitigating Modality incompleteness
in MFL within the Clusters. MMiC replaces partial parameters within client
models inside clusters to mitigate the impact of missing modalities.
Furthermore, it leverages the Banzhaf Power Index to optimize client selection
under these conditions. Finally, MMiC employs an innovative approach to
dynamically control global aggregation by utilizing Markovitz Portfolio
Optimization. Extensive experiments demonstrate that MMiC consistently
outperforms existing federated learning architectures in both global and
personalized performance on multimodal datasets with missing modalities,
confirming the effectiveness of our proposed solution.

</details>


### [154] [Non-Stationary Time Series Forecasting Based on Fourier Analysis and Cross Attention Mechanism](https://arxiv.org/abs/2505.06917)
*Yuqi Xiong,Yang Wen*

Main category: cs.LG

TL;DR: 提出AEFIN框架，通过跨注意力机制和傅里叶分析网络结合MLP处理非平稳时间序列数据，提升预测精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型难以有效处理非平稳时间序列数据的统计特性变化。

Method: 引入跨注意力机制增强信息共享，结合傅里叶分析网络和MLP，设计新型损失函数。

Result: AEFIN在均方误差和平均绝对误差上优于常见模型，尤其在非平稳数据条件下表现优异。

Conclusion: AEFIN为非平稳时间序列建模与预测提供创新方案，推动复杂时间序列深度学习研究。

Abstract: Time series forecasting has important applications in financial analysis,
weather forecasting, and traffic management. However, existing deep learning
models are limited in processing non-stationary time series data because they
cannot effectively capture the statistical characteristics that change over
time. To address this problem, this paper proposes a new framework, AEFIN,
which enhances the information sharing ability between stable and unstable
components by introducing a cross-attention mechanism, and combines Fourier
analysis networks with MLP to deeply explore the seasonal patterns and trend
characteristics in unstable components. In addition, we design a new loss
function that combines time-domain stability constraints, time-domain
instability constraints, and frequency-domain stability constraints to improve
the accuracy and robustness of forecasting. Experimental results show that
AEFIN outperforms the most common models in terms of mean square error and mean
absolute error, especially under non-stationary data conditions, and shows
excellent forecasting capabilities. This paper provides an innovative solution
for the modeling and forecasting of non-stationary time series data, and
contributes to the research of deep learning for complex time series.

</details>


### [155] [AI-Powered Inverse Design of Ku-Band SIW Resonant Structures by Iterative Residual Correction Network](https://arxiv.org/abs/2505.06936)
*Mohammad Mashayekhi,Kamran Salehian*

Main category: cs.LG

TL;DR: 本文提出了一种用于Ku波段基板集成波导（SIW）组件逆向设计的迭代残差校正网络（IRC-Net），通过多模谐振结构控制谐振，提高了传统逆向设计技术的精度和泛化能力。实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统逆向设计技术（如FIM）在精度和泛化能力上存在局限，需要一种更高效的方法来设计复杂的微波结构。

Method: 采用结合前馈逆向模型（FIM）初始估计和迭代残差校正策略（IRC-Net）的深度学习架构，优化多模谐振SIW组件的设计。

Result: IRC-Net在预测精度上显著优于单阶段网络，仿真与实测结果高度一致，验证了其实际应用价值。

Conclusion: IRC-Net为微波组件的逆向设计提供了高效且高精度的解决方案，适用于谐振元件和智能滤波器设计。

Abstract: Inverse electromagnetic modeling has emerged as a powerful approach for
designing complex microwave structures with high accuracy and efficiency. In
this study, we propose an Iterative Residual Correction Network (IRC-Net) for
the inverse design of Ku-band Substrate Integrated Waveguide (SIW) components
based on multimode resonators. We use a multimode resonance structure to
demonstrate that it is possible to control the resonances of the structure.
Therefore, these structures can be used for resonant components and smart
filter design. The proposed deep learning architecture leverages residual
neural networks to overcome the limitations of traditional inverse design
techniques, such as the Feedforward Inverse Model (FIM), offering improved
generalization and prediction accuracy. The approach begins with a FIM to
generate initial design estimates, followed by an iterative correction strategy
inspired by the Hybrid Inverse-Forward Residual Refinement Network
(HiFR\textsuperscript{2}-Net), which we call IRC-Net. Experiments demonstrate
that the IRC-Net achieves substantial improvements in prediction accuracy
compared to traditional single-stage networks, validated through statistical
metrics, full-wave electromagnetic simulations, and measurements. To validate
the proposed framework, we first design and fabricate a three-resonance SIW
structure. Next, we apply the trained IRC-Net model to predict the geometry of
a four-resonance structure based on its desired frequency response. Both
designs are fabricated and tested, showing strong agreement between the
simulated, predicted, and measured results, confirming the effectiveness and
practicality of the proposed method.

</details>


### [156] [A systematic review of challenges and proposed solutions in modeling multimodal data](https://arxiv.org/abs/2505.06945)
*Maryam Farhadizadeh,Maria Weymann,Michael Blaß,Johann Kraus,Christopher Gundler,Sebastian Walter,Noah Hempen,Harald Binde,Nadine Binder*

Main category: cs.LG

TL;DR: 该论文通过系统综述69项研究，总结了多模态数据建模在临床研究中的常见技术挑战及最新解决方案，为未来的医学应用研究提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 多模态数据建模在临床研究中潜力巨大，但整合异构数据（如影像、基因组、可穿戴设备数据等）面临诸多技术难题，如缺失模态、小样本量等，亟需系统性总结和方法创新。

Method: 采用系统综述方法，分析69项研究，识别多模态建模中的关键障碍及解决方案（如迁移学习、生成模型、注意力机制等）。

Result: 总结了当前多模态建模的挑战（如维度不平衡、可解释性问题）和前沿方法进展，提供了领域发展趋势的全面概览。

Conclusion: 多模态医学数据建模需进一步优化融合技术，未来研究可借助综述中提出的方法论（如神经架构搜索）推动个性化医疗发展。

Abstract: Multimodal data modeling has emerged as a powerful approach in clinical
research, enabling the integration of diverse data types such as imaging,
genomics, wearable sensors, and electronic health records. Despite its
potential to improve diagnostic accuracy and support personalized care,
modeling such heterogeneous data presents significant technical challenges.
This systematic review synthesizes findings from 69 studies to identify common
obstacles, including missing modalities, limited sample sizes, dimensionality
imbalance, interpretability issues, and finding the optimal fusion techniques.
We highlight recent methodological advances, such as transfer learning,
generative models, attention mechanisms, and neural architecture search that
offer promising solutions. By mapping current trends and innovations, this
review provides a comprehensive overview of the field and offers practical
insights to guide future research and development in multimodal modeling for
medical applications.

</details>


### [157] [Learning Value of Information towards Joint Communication and Control in 6G V2X](https://arxiv.org/abs/2505.06978)
*Lei Lei,Kan Zheng,Xuemin,Shen*

Main category: cs.LG

TL;DR: 该论文提出了一种基于SSDP模型和VoI的系统化框架，用于优化CAV的决策和通信系统，并结合DRL和最优控制理论，展示了其在车联网中的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着C-V2X向6G网络演进，CAV成为关键应用。但现有研究在信息价值（VoI）建模上较为零散，且车控与通信的协同优化需求迫切，因此需要系统化框架。

Method: 提出SSDP模型（MDP的扩展）定义VoI，建立MDP/RL/最优控制理论驱动的VoI建模框架，分类VoI并设计估计方法，最后通过VoI奖励函数优化通信策略。

Result: SSDP模型能显式表示提升决策的信息集合，提出的框架可系统化解决通信的时机（When）、内容（What）和方式（How）问题，并通过跟车控制案例验证。

Conclusion: 该框架为网络化控制系统中随机序贯决策与通信的联合优化提供了通用方法，未来可扩展至更复杂场景。

Abstract: As Cellular Vehicle-to-Everything (C-V2X) evolves towards future
sixth-generation (6G) networks, Connected Autonomous Vehicles (CAVs) are
emerging to become a key application. Leveraging data-driven Machine Learning
(ML), especially Deep Reinforcement Learning (DRL), is expected to
significantly enhance CAV decision-making in both vehicle control and V2X
communication under uncertainty. These two decision-making processes are
closely intertwined, with the value of information (VoI) acting as a crucial
bridge between them. In this paper, we introduce Sequential Stochastic Decision
Process (SSDP) models to define and assess VoI, demonstrating their application
in optimizing communication systems for CAVs. Specifically, we formally define
the SSDP model and demonstrate that the MDP model is a special case of it. The
SSDP model offers a key advantage by explicitly representing the set of
information that can enhance decision-making when available. Furthermore, as
current research on VoI remains fragmented, we propose a systematic VoI
modeling framework grounded in the MDP, Reinforcement Learning (RL) and Optimal
Control theories. We define different categories of VoI and discuss their
corresponding estimation methods. Finally, we present a structured approach to
leverage the various VoI metrics for optimizing the ``When", ``What", and
``How" to communicate problems. For this purpose, SSDP models are formulated
with VoI-associated reward functions derived from VoI-based optimization
objectives. While we use a simple vehicle-following control problem to
illustrate the proposed methodology, it holds significant potential to
facilitate the joint optimization of stochastic, sequential control and
communication decisions in a wide range of networked control systems.

</details>


### [158] [GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance](https://arxiv.org/abs/2505.07004)
*Jinuk Kim,Marwa El Halabi,Wonpyo Park,Clemens JS Schaefer,Deokjae Lee,Yeonhong Park,Jae W. Lee,Hyun Oh Song*

Main category: cs.LG

TL;DR: GuidedQuant是一种新型的量化方法，通过将损失函数的梯度信息融入量化目标，同时保持输出通道间的权重依赖关系，提升了现有量化方法的性能，并引入了一种非均匀标量量化算法。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法要么未考虑隐藏特征对最终损失的重要性，要么忽略了模型权重间的关键交互，限制了量化效果。

Method: 提出GuidedQuant方法，结合损失函数的梯度信息，加入量化目标，同时保持输出通道的权重互依赖性，并开发了一种非均匀标量量化算法。

Result: 在仅权重标量、向量量化及权重与激活量化中，GuidedQuant均提升了性能；非均匀标量量化算法优于现有同类方法。

Conclusion: GuidedQuant通过综合考虑梯度信息和权重依赖，显著改进了量化性能，为非均匀标量量化提供了新解法。

Abstract: Post-training quantization is a key technique for reducing the memory and
inference latency of large language models by quantizing weights and
activations without requiring retraining. However, existing methods either (1)
fail to account for the varying importance of hidden features to the end loss
or, when incorporating end loss, (2) neglect the critical interactions between
model weights. To address these limitations, we propose GuidedQuant, a novel
quantization approach that integrates gradient information from the end loss
into the quantization objective while preserving cross-weight dependencies
within output channels. GuidedQuant consistently boosts the performance of
state-of-the-art quantization methods across weight-only scalar, weight-only
vector, and weight-and-activation quantization. Additionally, we introduce a
novel non-uniform scalar quantization algorithm, which is guaranteed to
monotonically decrease the quantization objective value, and outperforms
existing methods in this category. We release the code at
https://github.com/snu-mllab/GuidedQuant.

</details>


### [159] [Incremental Uncertainty-aware Performance Monitoring with Active Labeling Intervention](https://arxiv.org/abs/2505.07023)
*Alexander Koebler,Thomas Decker,Ingo Thon,Volker Tresp,Florian Buettner*

Main category: cs.LG

TL;DR: 提出了一种名为IUPM的无标签方法，通过最优传输建模渐变分布偏移来估计性能变化，并量化预测不确定性，在有限标注预算下有效指导标注获取。


<details>
  <summary>Details</summary>
Motivation: 研究机器学习模型在渐变分布偏移下的性能监测问题，解决因缓慢变化导致的性能下降未被察觉的情况。

Method: 提出IUPM方法，结合最优传输建模渐变偏移，量化性能预测不确定性，并引入主动标注策略以恢复可靠估计。

Result: 实验表明IUPM在多种渐变偏移场景下优于现有基线，其不确定性感知能更有效地指导标注获取。

Conclusion: IUPM通过建模渐变偏移和量化不确定性，为模型性能监测提供了高效且可靠的解决方案。

Abstract: We study the problem of monitoring machine learning models under gradual
distribution shifts, where circumstances change slowly over time, often leading
to unnoticed yet significant declines in accuracy. To address this, we propose
Incremental Uncertainty-aware Performance Monitoring (IUPM), a novel label-free
method that estimates performance changes by modeling gradual shifts using
optimal transport. In addition, IUPM quantifies the uncertainty in the
performance prediction and introduces an active labeling procedure to restore a
reliable estimate under a limited labeling budget. Our experiments show that
IUPM outperforms existing performance estimation baselines in various gradual
shift scenarios and that its uncertainty awareness guides label acquisition
more effectively compared to other strategies.

</details>


### [160] [Efficient Machine Unlearning by Model Splitting and Core Sample Selection](https://arxiv.org/abs/2505.07026)
*Maximilian Egger,Rawad Bitar,Rüdiger Urbanke*

Main category: cs.LG

TL;DR: 论文提出了一种名为MaxRR的机器学习去学习方法，通过改进标准去学习指标和引入去学习感知训练程序，提高了去学习的效率和精确性。


<details>
  <summary>Details</summary>
Motivation: 为满足‘被遗忘权’等法律要求，现有去学习方法在效率和验证方面存在不足，特别是在弱去学习保证的情况下，验证仍然是一个挑战。

Method: 引入了标准去学习指标的广义变体，并提出了去学习感知训练程序。该方法在多数情况下支持精确去学习，否则也能高效模拟完全重训练的效果。

Result: MaxRR在无法精确去学习时，仍能实现与完全重训练相近的高效去学习效果。

Conclusion: MaxRR为机器学习去学习提供了一种更高效、更精确的解决方案，尤其是在法律要求严格的场景下。

Abstract: Machine unlearning is essential for meeting legal obligations such as the
right to be forgotten, which requires the removal of specific data from machine
learning models upon request. While several approaches to unlearning have been
proposed, existing solutions often struggle with efficiency and, more
critically, with the verification of unlearning - particularly in the case of
weak unlearning guarantees, where verification remains an open challenge. We
introduce a generalized variant of the standard unlearning metric that enables
more efficient and precise unlearning strategies. We also present an
unlearning-aware training procedure that, in many cases, allows for exact
unlearning. We term our approach MaxRR. When exact unlearning is not feasible,
MaxRR still supports efficient unlearning with properties closely matching
those achieved through full retraining.

</details>


### [161] [Predicting Diabetes Using Machine Learning: A Comparative Study of Classifiers](https://arxiv.org/abs/2505.07036)
*Mahade Hasan,Farhana Yasmin*

Main category: cs.LG

TL;DR: 该论文提出了一种名为DNet的新型糖尿病预测框架，结合了传统机器学习方法和先进的集成技术，并通过CNN和LSTM混合架构实现了高效特征提取和时序学习。实验结果显示DNet在准确率和ROC-AUC上表现最优。


<details>
  <summary>Details</summary>
Motivation: 糖尿病是全球重大健康问题，机器学习在医疗领域的应用为早期干预提供了可能。本研究旨在通过创新的混合模型提升糖尿病预测的准确性和可靠性。

Method: 研究了多种传统和集成机器学习方法，并提出DNet模型——一个结合CNN和LSTM的混合架构，包含卷积块、残差块、LSTM层及正则化技术。

Result: DNet在Kaggle数据集上实现了99.79%的准确率和99.98%的ROC-AUC，显著优于其他模型。

Conclusion: DNet展示了CNN与LSTM结合在医疗诊断中的潜力，为糖尿病预测提供了高效解决方案。

Abstract: Diabetes remains a significant health challenge globally, contributing to
severe complications like kidney disease, vision loss, and heart issues. The
application of machine learning (ML) in healthcare enables efficient and
accurate disease prediction, offering avenues for early intervention and
patient support. Our study introduces an innovative diabetes prediction
framework, leveraging both traditional ML techniques such as Logistic
Regression, SVM, Na\"ive Bayes, and Random Forest and advanced ensemble methods
like AdaBoost, Gradient Boosting, Extra Trees, and XGBoost. Central to our
approach is the development of a novel model, DNet, a hybrid architecture
combining Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM)
layers for effective feature extraction and sequential learning. The DNet model
comprises an initial convolutional block for capturing essential features,
followed by a residual block with skip connections to facilitate efficient
information flow. Batch Normalization and Dropout are employed for robust
regularization, and an LSTM layer captures temporal dependencies within the
data. Using a Kaggle-sourced real-world diabetes dataset, our model evaluation
spans cross-validation accuracy, precision, recall, F1 score, and ROC-AUC.
Among the models, DNet demonstrates the highest efficacy with an accuracy of
99.79% and an AUC-ROC of 99.98%, establishing its potential for superior
diabetes prediction. This robust hybrid architecture showcases the value of
combining CNN and LSTM layers, emphasizing its applicability in medical
diagnostics and disease prediction tasks.

</details>


### [162] [Reinforcement Learning (RL) Meets Urban Climate Modeling: Investigating the Efficacy and Impacts of RL-Based HVAC Control](https://arxiv.org/abs/2505.07045)
*Junjie Yu,John S. Schreck,David John Gagne,Keith W. Oleson,Jie Li,Yongtu Liang,Qi Liao,Mingfei Sun,David O. Topping,Zhonghua Zheng*

Main category: cs.LG

TL;DR: 该研究提出了一个结合强化学习（RL）与城市气候模型的集成框架，评估不同气候背景下RL HVAC控制的效果及其对室内和城市气候的影响。


<details>
  <summary>Details</summary>
Motivation: 探索RL HVAC控制在减少建筑能耗同时保持室内热舒适方面的潜力，并研究其在不同气候背景下的适用性和影响。

Method: 集成RL、建筑能源模型和城市气候模型，分析城市背景气候对RL策略的效果、室内外气候影响及跨城市策略可迁移性的影响。

Result: 不同城市的RL策略效果及室内外气候影响差异显著；炎热气候城市在不同奖励权重下表现更优，温度变化大的城市策略迁移性更强。

Conclusion: 强调在不同气候背景下全面评估RL HVAC控制的必要性，并指出跨城市学习有助于RL HVAC的部署。

Abstract: Reinforcement learning (RL)-based heating, ventilation, and air conditioning
(HVAC) control has emerged as a promising technology for reducing building
energy consumption while maintaining indoor thermal comfort. However, the
efficacy of such strategies is influenced by the background climate and their
implementation may potentially alter both the indoor climate and local urban
climate. This study proposes an integrated framework combining RL with an urban
climate model that incorporates a building energy model, aiming to evaluate the
efficacy of RL-based HVAC control across different background climates, impacts
of RL strategies on indoor climate and local urban climate, and the
transferability of RL strategies across cities. Our findings reveal that the
reward (defined as a weighted combination of energy consumption and thermal
comfort) and the impacts of RL strategies on indoor climate and local urban
climate exhibit marked variability across cities with different background
climates. The sensitivity of reward weights and the transferability of RL
strategies are also strongly influenced by the background climate. Cities in
hot climates tend to achieve higher rewards across most reward weight
configurations that balance energy consumption and thermal comfort, and those
cities with more varying atmospheric temperatures demonstrate greater RL
strategy transferability. These findings underscore the importance of
thoroughly evaluating RL-based HVAC control strategies in diverse climatic
contexts. This study also provides a new insight that city-to-city learning
will potentially aid the deployment of RL-based HVAC control.

</details>


### [163] [Scaling Laws and Representation Learning in Simple Hierarchical Languages: Transformers vs. Convolutional Architectures](https://arxiv.org/abs/2505.07070)
*Francesco Cagnetta,Alessandro Favero,Antonio Sclocchi,Matthieu Wyart*

Main category: cs.LG

TL;DR: 论文研究了神经网络语言模型如何通过下一个词预测学习语言结构，提出了理论扩展定律，并对比了卷积网络和Transformer模型的性能差异。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络模型在下一个词预测任务中如何捕获语言的结构，尤其是层次结构，以理解模型架构与数据统计特性之间的相互作用。

Method: 使用随机层次模型（RHM）生成合成数据集，并扩展理论框架以分析不同架构（如卷积网络和Transformer）的性能差异。

Result: 发现卷积网络由于局部性和权重共享的特性，性能提升速度优于依赖全局自注意力机制的Transformer模型。

Conclusion: 模型架构与数据统计特性的交互决定了表示学习的效果，卷积网络在捕获层次结构方面更具优势。

Abstract: How do neural language models acquire a language's structure when trained for
next-token prediction? We address this question by deriving theoretical scaling
laws for neural network performance on synthetic datasets generated by the
Random Hierarchy Model (RHM) -- an ensemble of probabilistic context-free
grammars designed to capture the hierarchical structure of natural language
while remaining analytically tractable. Previously, we developed a theory of
representation learning based on data correlations that explains how deep
learning models capture the hierarchical structure of the data sequentially,
one layer at a time. Here, we extend our theoretical framework to account for
architectural differences. In particular, we predict and empirically validate
that convolutional networks, whose structure aligns with that of the generative
process through locality and weight sharing, enjoy a faster scaling of
performance compared to transformer models, which rely on global self-attention
mechanisms. This finding clarifies the architectural biases underlying neural
scaling laws and highlights how representation learning is shaped by the
interaction between model architecture and the statistical properties of data.

</details>


### [164] [COMRECGC: Global Graph Counterfactual Explainer through Common Recourse](https://arxiv.org/abs/2505.07081)
*Gregoire Fournier,Sourav Medya*

Main category: cs.LG

TL;DR: 该论文针对GNN的全局反事实解释问题，提出了COMRECGC算法，用于生成常见的补救措施，将‘拒绝’类图转换为‘接受’类图。在多个真实数据集上验证了其优越性，并展示了其在药物发现等领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的GNN解释方法主要关注局部反事实解释，而全局反事实解释中的常见补救措施问题尚未充分研究。为了填补这一空白，作者旨在提出一种通用且高效的算法，以支持GNN在全局解释中的应用。

Method: 论文首先形式化了常见补救措施解释问题，并设计了COMRECGC算法。该算法通过生成一小部分补救措施集合，使得所有‘拒绝’类图均可通过这些措施转换为‘接受’类图。

Result: 在四个真实世界图数据集上的实验表明，COMRECGC算法显著优于基线方法。同时，对比其他图反事实解释方法，常见补救措施的生成表现出相当或更优的效果。

Conclusion: 常见补救措施解释不仅填补了GNN全局解释的空白，还在实际应用中展现出潜力（如药物发现）。COMRECGC算法的高效性和普适性为GNN的透明性提供了新思路。

Abstract: Graph neural networks (GNNs) have been widely used in various domains such as
social networks, molecular biology, or recommendation systems. Concurrently,
different explanations methods of GNNs have arisen to complement its black-box
nature. Explanations of the GNNs' predictions can be categorized into two
types--factual and counterfactual. Given a GNN trained on binary classification
into ''accept'' and ''reject'' classes, a global counterfactual explanation
consists in generating a small set of ''accept'' graphs relevant to all of the
input ''reject'' graphs. The transformation of a ''reject'' graph into an
''accept'' graph is called a recourse. A common recourse explanation is a small
set of recourse, from which every ''reject'' graph can be turned into an
''accept'' graph. Although local counterfactual explanations have been studied
extensively, the problem of finding common recourse for global counterfactual
explanation remains unexplored, particularly for GNNs. In this paper, we
formalize the common recourse explanation problem, and design an effective
algorithm, COMRECGC, to solve it. We benchmark our algorithm against strong
baselines on four different real-world graphs datasets and demonstrate the
superior performance of COMRECGC against the competitors. We also compare the
common recourse explanations to the graph counterfactual explanation, showing
that common recourse explanations are either comparable or superior, making
them worth considering for applications such as drug discovery or computational
biology.

</details>


### [165] [Multi-Objective-Guided Discrete Flow Matching for Controllable Biological Sequence Design](https://arxiv.org/abs/2505.07086)
*Tong Chen,Yinuo Zhang,Sophia Tang,Pranam Chatterjee*

Main category: cs.LG

TL;DR: 提出了MOG-DFM框架，用于在多目标冲突的条件下高效生成生物分子序列。


<details>
  <summary>Details</summary>
Motivation: 解决生物分子设计中多目标冲突的挑战，现有方法难以兼顾离散分布与多目标优化。

Method: 基于预训练的离散时间流匹配生成器，提出混合秩-方向评分与自适应超锥滤波技术。

Result: 在优化肽结合剂和增强DNA序列设计中表现优异，实现多属性平衡。

Conclusion: MOG-DFM是一种强大的多属性生物分子序列设计工具。

Abstract: Designing biological sequences that satisfy multiple, often conflicting,
functional and biophysical criteria remains a central challenge in biomolecule
engineering. While discrete flow matching models have recently shown promise
for efficient sampling in high-dimensional sequence spaces, existing approaches
address only single objectives or require continuous embeddings that can
distort discrete distributions. We present Multi-Objective-Guided Discrete Flow
Matching (MOG-DFM), a general framework to steer any pretrained discrete-time
flow matching generator toward Pareto-efficient trade-offs across multiple
scalar objectives. At each sampling step, MOG-DFM computes a hybrid
rank-directional score for candidate transitions and applies an adaptive
hypercone filter to enforce consistent multi-objective progression. We also
trained two unconditional discrete flow matching models, PepDFM for diverse
peptide generation and EnhancerDFM for functional enhancer DNA generation, as
base generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in
generating peptide binders optimized across five properties (hemolysis,
non-fouling, solubility, half-life, and binding affinity), and in designing DNA
sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM
proves to be a powerful tool for multi-property-guided biomolecule sequence
design.

</details>


### [166] [Physics-informed Multiple-Input Operators for efficient dynamic response prediction of structures](https://arxiv.org/abs/2505.07090)
*Bilal Ahmed,Yuqing Qiu,Diab W. Abueidda,Waleed El-Sekelly,Tarek Abdoun,Mostafa E. Mobasher*

Main category: cs.LG

TL;DR: 本文提出了一种多输入算子网络（MIONet），通过引入第二主干网络显式编码时间动态，实现了移动载荷下结构响应的准确预测，相比传统方法具有更高的计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 有限元建模（FEM）在动态载荷下计算成本高，传统方法难以连续捕捉时空动态，因此需要一种高效且物理一致性强的替代方案。

Method: MIONet采用双主干网络设计，结合物理信息损失函数和Schur补形式降低计算成本，无需直接求解偏微分方程。

Result: MIONet在梁和桥梁案例中达到FEM级精度，推理速度比基于GRU的DeepONet快100倍以上。

Conclusion: MIONet适用于实时结构监测和数字孪生应用，兼具高精度和高效性。

Abstract: Finite element (FE) modeling is essential for structural analysis but remains
computationally intensive, especially under dynamic loading. While operator
learning models have shown promise in replicating static structural responses
at FEM level accuracy, modeling dynamic behavior remains more challenging. This
work presents a Multiple Input Operator Network (MIONet) that incorporates a
second trunk network to explicitly encode temporal dynamics, enabling accurate
prediction of structural responses under moving loads. Traditional DeepONet
architectures using recurrent neural networks (RNNs) are limited by fixed time
discretization and struggle to capture continuous dynamics. In contrast, MIONet
predicts responses continuously over both space and time, removing the need for
step wise modeling. It maps scalar inputs including load type, velocity,
spatial mesh, and time steps to full field structural responses. To improve
efficiency and enforce physical consistency, we introduce a physics informed
loss based on dynamic equilibrium using precomputed mass, damping, and
stiffness matrices, without solving the governing PDEs directly. Further, a
Schur complement formulation reduces the training domain, significantly cutting
computational costs while preserving global accuracy. The model is validated on
both a simple beam and the KW-51 bridge, achieving FEM level accuracy within
seconds. Compared to GRU based DeepONet, our model offers comparable accuracy
with improved temporal continuity and over 100 times faster inference, making
it well suited for real-time structural monitoring and digital twin
applications.

</details>


### [167] [Navigating the Rashomon Effect: How Personalization Can Help Adjust Interpretable Machine Learning Models to Individual Users](https://arxiv.org/abs/2505.07100)
*Julian Rosenberger,Philipp Schröppel,Sven Kruschel,Mathias Kraus,Patrick Zschech,Maximilian Förster*

Main category: cs.LG

TL;DR: 论文研究了如何在预测性能相似的广义加性模型（GAMs）中根据用户需求个性化配置模型，并通过在线实验验证了个性化方法的效果。


<details>
  <summary>Details</summary>
Motivation: 解决在机器学习中多个模型预测性能相似但解释方式不同时，如何根据用户需求个性化配置模型以提升解释性的问题。

Method: 采用上下文老虎机方法对模型进行个性化配置，并通过108名用户的在线实验，对比了个性化组和非个性化组的效果。

Result: 实验结果表明个性化方法能够产生针对个体的配置，且两组用户的模型解释性均保持较高水平。

Conclusion: 研究初步探索了可解释机器学习的个性化潜力，显示了个性化方法在保持高解释性方面的可行性。

Abstract: The Rashomon effect describes the observation that in machine learning (ML)
multiple models often achieve similar predictive performance while explaining
the underlying relationships in different ways. This observation holds even for
intrinsically interpretable models, such as Generalized Additive Models (GAMs),
which offer users valuable insights into the model's behavior. Given the
existence of multiple GAM configurations with similar predictive performance, a
natural question is whether we can personalize these configurations based on
users' needs for interpretability. In our study, we developed an approach to
personalize models based on contextual bandits. In an online experiment with
108 users in a personalized treatment and a non-personalized control group, we
found that personalization led to individualized rather than one-size-fits-all
configurations. Despite these individual adjustments, the interpretability
remained high across both groups, with users reporting a strong understanding
of the models. Our research offers initial insights into the potential for
personalizing interpretable ML.

</details>


### [168] [Learning from Samples: Inverse Problems over measures via Sharpened Fenchel-Young Losses](https://arxiv.org/abs/2505.07124)
*Francisco Andrade,Gabriel Peyré,Clarice Poon*

Main category: cs.LG

TL;DR: 该论文提出了一种通过最小化新损失函数（sharpened Fenchel-Young losses）来估计最优概率分布参数的方法，分析了有限样本下的稳定性，并在逆不平衡最优传输和逆梯度流两个场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 估计最优概率分布的参数在多个领域非常重要，但传统方法在有限样本下的稳定性不足，论文旨在通过新的损失函数和稳定性分析解决这一问题。

Method: 论文提出了sharpened Fenchel-Young损失函数来度量优化问题的次优性，并利用损失函数的强凸性和前向优化问题的样本复杂度来分析稳定性。

Result: 方法在逆不平衡最优传输（iUOT）和逆梯度流（iJKO）场景中取得显式稳定性保证，并通过高斯分布实验验证了其实际性能。

Conclusion: 论文方法在理论和实验中均表现出色，为估计最优概率分布参数提供了稳定且实用的解决方案。

Abstract: Estimating parameters from samples of an optimal probability distribution is
essential in applications ranging from socio-economic modeling to biological
system analysis. In these settings, the probability distribution arises as the
solution to an optimization problem that captures either static interactions
among agents or the dynamic evolution of a system over time. Our approach
relies on minimizing a new class of loss functions, called sharpened
Fenchel-Young losses, which measure the sub-optimality gap of the optimization
problem over the space of measures. We study the stability of this estimation
method when only a finite number of sample is available. The parameters to be
estimated typically correspond to a cost function in static problems and to a
potential function in dynamic problems. To analyze stability, we introduce a
general methodology that leverages the strong convexity of the loss function
together with the sample complexity of the forward optimization problem. Our
analysis emphasizes two specific settings in the context of optimal transport,
where our method provides explicit stability guarantees: The first is inverse
unbalanced optimal transport (iUOT) with entropic regularization, where the
parameters to estimate are cost functions that govern transport computations;
this method has applications such as link prediction in machine learning. The
second is inverse gradient flow (iJKO), where the objective is to recover a
potential function that drives the evolution of a probability distribution via
the Jordan-Kinderlehrer-Otto (JKO) time-discretization scheme; this is
particularly relevant for understanding cell population dynamics in single-cell
genomics. Finally, we validate our approach through numerical experiments on
Gaussian distributions, where closed-form solutions are available, to
demonstrate the practical performance of our methods

</details>


### [169] [Triangulating PL functions and the existence of efficient ReLU DNNs](https://arxiv.org/abs/2505.07137)
*Danny Calegari*

Main category: cs.LG

TL;DR: 该论文证明所有分段线性函数可以通过三角剖分表示为单纯形函数的和，从而为ReLU神经网络的高效通用性提供简洁证明。


<details>
  <summary>Details</summary>
Motivation: 研究分段线性函数的表示方法，以支持ReLU神经网络的高效通用计算能力。

Method: 利用相对同调类的一度三角剖分，将分段线性函数表示为单纯形函数的和。

Result: 证明了高效通用ReLU神经网络可以同时计算所有复杂度受限的分段线性函数。

Conclusion: 通过三角剖分和单纯形函数表示，为ReLU神经网络的通用性提供了简洁的数学基础。

Abstract: We show that every piecewise linear function $f:R^d \to R$ with compact
support a polyhedron $P$ has a representation as a sum of so-called `simplex
functions'. Such representations arise from degree 1 triangulations of the
relative homology class (in $R^{d+1}$) bounded by $P$ and the graph of $f$, and
give a short elementary proof of the existence of efficient universal ReLU
neural networks that simultaneously compute all such functions $f$ of bounded
complexity.

</details>


### [170] [AugMixCloak: A Defense against Membership Inference Attacks via Image Transformation](https://arxiv.org/abs/2505.07149)
*Heqing Ren,Chao Feng,Alberto Huertas,Burkhard Stiller*

Main category: cs.LG

TL;DR: AugMixCloak defends against membership inference攻击 in federated learning through data augmentation and PCA-based information fusion, showing strong protection and generalization.


<details>
  <summary>Details</summary>
Motivation: To mitigate privacy risks in federated learning caused by membership inference attacks.

Method: Two-stage defense combining data augmentation and PCA-based information fusion, identified via perceptual hashing.

Result: Outperforms regularization and confidence masking methods across multiple datasets and topologies.

Conclusion: AugMixCloak effectively protects privacy in federated learning against membership inference attacks.

Abstract: Traditional machine learning (ML) raises serious privacy concerns, while
federated learning (FL) mitigates the risk of data leakage by keeping data on
local devices. However, the training process of FL can still leak sensitive
information, which adversaries may exploit to infer private data. One of the
most prominent threats is the membership inference attack (MIA), where the
adversary aims to determine whether a particular data record was part of the
training set.
  This paper addresses this problem through a two-stage defense called
AugMixCloak. The core idea is to apply data augmentation and principal
component analysis (PCA)-based information fusion to query images, which are
detected by perceptual hashing (pHash) as either identical to or highly similar
to images in the training set. Experimental results show that AugMixCloak
successfully defends against both binary classifier-based MIA and metric-based
MIA across five datasets and various decentralized FL (DFL) topologies.
Compared with regularization-based defenses, AugMixCloak demonstrates stronger
protection. Compared with confidence score masking, AugMixCloak exhibits better
generalization.

</details>


### [171] [Causal View of Time Series Imputation: Some Identification Results on Missing Mechanism](https://arxiv.org/abs/2505.07180)
*Ruichu Cai,Kaitao Zheng,Junxian Huang,Zijian Li,Zhengming Chen,Boyan Xu,Zhifeng Hao*

Main category: cs.LG

TL;DR: 该论文提出了一个针对时间序列填补问题的框架（DMM），通过分析不同缺失机制（如MAR和MNAR）并定制解决方案，结合变分推理和归一化流架构，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，时间序列数据存在多种缺失机制（如MAR和MNAR），但现有方法常忽略其差异，采用单一模型导致误导性结果。因此，需要一种能区分并适配不同缺失机制的框架。

Method: 框架首先分析不同机制下的数据生成过程（含时间潜在状态和缺失原因变量），并通过变分推理建模，同时利用归一化流架构估计潜在变量的先验分布。此外，在非线性独立成分分析框架下证明了潜在变量的可识别性。

Result: 实验表明，该方法在多种缺失机制的数据集上超越现有时间序列填补技术，验证了其在实际应用中的有效性。

Conclusion: DMM框架通过区分缺失机制并定制解决方案，显著提升了时间序列填补的准确性和鲁棒性，为复杂实际场景提供了可靠工具。

Abstract: Time series imputation is one of the most challenge problems and has broad
applications in various fields like health care and the Internet of Things.
Existing methods mainly aim to model the temporally latent dependencies and the
generation process from the observed time series data. In real-world scenarios,
different types of missing mechanisms, like MAR (Missing At Random), and MNAR
(Missing Not At Random) can occur in time series data. However, existing
methods often overlook the difference among the aforementioned missing
mechanisms and use a single model for time series imputation, which can easily
lead to misleading results due to mechanism mismatching. In this paper, we
propose a framework for time series imputation problem by exploring Different
Missing Mechanisms (DMM in short) and tailoring solutions accordingly.
Specifically, we first analyze the data generation processes with temporal
latent states and missing cause variables for different mechanisms.
Sequentially, we model these generation processes via variational inference and
estimate prior distributions of latent variables via normalizing flow-based
neural architecture. Furthermore, we establish identifiability results under
the nonlinear independent component analysis framework to show that latent
variables are identifiable. Experimental results show that our method surpasses
existing time series imputation techniques across various datasets with
different missing mechanisms, demonstrating its effectiveness in real-world
applications.

</details>


### [172] [Compression, Regularity, Randomness and Emergent Structure: Rethinking Physical Complexity in the Data-Driven Era](https://arxiv.org/abs/2505.07222)
*Nima Dehghani*

Main category: cs.LG

TL;DR: 摘要提出了一种统一框架，将统计、算法和动态度量组织到概念空间的三轴（规律性、随机性和复杂性）上，并讨论了现代数据驱动方法在逼近经典复杂性理论时的实用性。最后，强调了复杂性理论对下一代科学建模的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前复杂性科学中缺乏对统计、算法和动态度量的系统性概念组织，因此需要构建统一框架以填补这一空白。

Method: 提出了一个三轴（规律性、随机性、复杂性）的概念空间框架，将统计、算法和动态度量映射到其中，并分析它们的计算可访问性和可逼近性。同时探讨了现代数据驱动方法（如自编码器、潜在动态模型等）作为经典复杂性理论的实用逼近工具。

Result: 该框架揭示了不可计算性带来的深层挑战，并展示了潜在空间如何成为理论建模与实际应用之间的桥梁。

Conclusion: 复杂性理论仍是下一代科学建模的核心，框架为物理驱动的AI和AI引导的复杂系统发现提供了理论支撑。

Abstract: Complexity science offers a wide range of measures for quantifying
unpredictability, structure, and information. Yet, a systematic conceptual
organization of these measures is still missing.
  We present a unified framework that locates statistical, algorithmic, and
dynamical measures along three axes (regularity, randomness, and complexity)
and situates them in a common conceptual space. We map statistical,
algorithmic, and dynamical measures into this conceptual space, discussing
their computational accessibility and approximability.
  This taxonomy reveals the deep challenges posed by uncomputability and
highlights the emergence of modern data-driven methods (including autoencoders,
latent dynamical models, symbolic regression, and physics-informed neural
networks) as pragmatic approximations to classical complexity ideals. Latent
spaces emerge as operational arenas where regularity extraction, noise
management, and structured compression converge, bridging theoretical
foundations with practical modeling in high-dimensional systems.
  We close by outlining implications for physics-informed AI and AI-guided
discovery in complex physical systems, arguing that classical questions of
complexity remain central to next-generation scientific modeling.

</details>


### [173] [REMEDI: Relative Feature Enhanced Meta-Learning with Distillation for Imbalanced Prediction](https://arxiv.org/abs/2505.07245)
*Fei Liu,Huanhuan Ren,Yu Guan,Xiuxu Wang,Wang Lv,Zhiqiang Hu,Yaxi Chen*

Main category: cs.LG

TL;DR: 论文提出REMEDI框架，通过多阶段方法解决极端类别不平衡的车辆购买预测问题，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 预测现有车主的未来购买行为因极低的阳性率(<0.5%)和复杂行为模式而具挑战性。

Method: REMEDI采用三阶段框架：训练多样化基础模型、引入相对性能元特征进行混合专家架构融合、通过监督微调将集成知识蒸馏为高效单模型。

Result: 在80万车主数据上，REMEDI在6万推荐中识别出50%实际买家，精度约10%，蒸馏模型保持集成性能且部署高效。

Conclusion: REMEDI在工业场景中展示了解决不平衡预测问题的有效性。

Abstract: Predicting future vehicle purchases among existing owners presents a critical
challenge due to extreme class imbalance (<0.5% positive rate) and complex
behavioral patterns. We propose REMEDI (Relative feature Enhanced Meta-learning
with Distillation for Imbalanced prediction), a novel multi-stage framework
addressing these challenges. REMEDI first trains diverse base models to capture
complementary aspects of user behavior. Second, inspired by comparative
op-timization techniques, we introduce relative performance meta-features
(deviation from ensemble mean, rank among peers) for effective model fusion
through a hybrid-expert architecture. Third, we distill the ensemble's
knowledge into a single efficient model via supervised fine-tuning with MSE
loss, enabling practical deployment. Evaluated on approximately 800,000 vehicle
owners, REMEDI significantly outperforms baseline approaches, achieving the
business target of identifying ~50% of actual buyers within the top 60,000
recommendations at ~10% precision. The distilled model preserves the ensemble's
predictive power while maintaining deployment efficiency, demonstrating
REMEDI's effectiveness for imbalanced prediction in industry settings.

</details>


### [174] [UMoE: Unifying Attention and FFN with Shared Experts](https://arxiv.org/abs/2505.07260)
*Yuanhang Yang,Chaozheng Wang,Jing Li*

Main category: cs.LG

TL;DR: 本文提出了一种统一注意力层和前馈网络层的Sparse Mixture of Experts (MoE)设计，通过重新定义注意力机制，揭示了其内在的FFN结构，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的注意力层MoE设计性能不佳且实现复杂，作者希望通过重新定义注意力机制，统一MoE架构，提升效率和性能。

Method: 提出UMoE架构，重新定义注意力机制，揭示其FFN结构，实现参数共享。

Result: UMoE在注意力层MoE中表现出色，性能优于现有方法。

Conclusion: 统一MoE设计能够高效提升性能，为模型扩展提供了新思路。

Abstract: Sparse Mixture of Experts (MoE) architectures have emerged as a promising
approach for scaling Transformer models. While initial works primarily
incorporated MoE into feed-forward network (FFN) layers, recent studies have
explored extending the MoE paradigm to attention layers to enhance model
performance. However, existing attention-based MoE layers require specialized
implementations and demonstrate suboptimal performance compared to their
FFN-based counterparts. In this paper, we aim to unify the MoE designs in
attention and FFN layers by introducing a novel reformulation of the attention
mechanism, revealing an underlying FFN-like structure within attention modules.
Our proposed architecture, UMoE, achieves superior performance through
attention-based MoE layers while enabling efficient parameter sharing between
FFN and attention components.

</details>


### [175] [Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains](https://arxiv.org/abs/2505.07274)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 提出了一种基于LLM先验的高效缓存框架，显著降低RL中的计算成本，同时在离散和连续环境中保持高性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM作为RL先验时的高计算成本问题，同时保持性能。

Method: 采用自适应缓存机制，通过代理梯度元优化缓存参数，适应不同环境。

Result: 在TextWorld、ALFWorld和MuJoCo等环境中，LLM查询减少3.8-4.7倍，延迟降低4.0-12.0倍，性能保留96-98%。离线RL中，CQL-Prior变体性能提升14-29%，训练时间减少38-40%。

Conclusion: 该框架在资源受限环境下展示了LLM引导RL的通用性和实用性。

Abstract: Integrating large language models (LLMs) as priors in reinforcement learning
(RL) offers significant advantages but comes with substantial computational
costs. We present a principled cache-efficient framework for posterior sampling
with LLM-derived priors that dramatically reduces these costs while maintaining
high performance. At the core of our approach is an adaptive caching mechanism,
where cache parameters are meta-optimized using surrogate gradients derived
from policy performance. This design enables efficient inference across both
discrete text environments (e.g., TextWorld, ALFWorld) and continuous control
domains (e.g., MuJoCo), achieving a 3.8--4.7$\times$ reduction in LLM queries
and 4.0--12.0$\times$ lower median latencies (85--93\,ms on a consumer GPU)
while retaining 96--98\% of uncached performance. Our theoretical analysis
provides KL divergence bounds on approximation quality, validated empirically.
The framework extends to offline RL, where our CQL-Prior variant improves
performance by 14--29\% and reduces training time by 38--40\%. Extensive
evaluations across a diverse suite of eight tasks demonstrate the
generalizability and practical viability of LLM-guided RL in
resource-constrained settings.

</details>


### [176] [INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized Reinforcement Learning](https://arxiv.org/abs/2505.07291)
*Prime Intellect Team,Sami Jaghouar,Justus Mattern,Jack Min Ong,Jannik Straube,Manveer Basra,Aaron Pazdera,Kushal Thaman,Matthew Di Ferrante,Felix Gabriel,Fares Obeid,Kemal Erdem,Michael Keiblinger,Johannes Hagemann*

Main category: cs.LG

TL;DR: INTELLECT-2是首个通过全球分布式强化学习（RL）训练的320亿参数语言模型，采用了完全异步RL训练方法，并通过开源推动去中心化训练研究。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统集中式训练的局限性，研究者旨在利用去中心化异构计算资源，构建更高效、开放的训练框架。

Method: 开发了PRIME-RL训练框架，包含TOPLOC（验证非可信推理节点的rollout）和SHARDCAST（高效广播策略权重），并对标准GRPO训练方法及数据过滤技术进行了改进。

Result: 成功训练了INTELLECT-2模型，超越了当前320亿参数范围内最先进的推理模型QwQ-32B。

Conclusion: INTELLECT-2及配套工具的开源为去中心化训练领域的研究提供了新方向，推动了该领域的开放发展。

Abstract: We introduce INTELLECT-2, the first globally distributed reinforcement
learning (RL) training run of a 32 billion parameter language model. Unlike
traditional centralized training efforts, INTELLECT-2 trains a reasoning model
using fully asynchronous RL across a dynamic, heterogeneous swarm of
permissionless compute contributors.
  To enable a training run with this unique infrastructure, we built various
components from scratch: we introduce PRIME-RL, our training framework
purpose-built for distributed asynchronous reinforcement learning, based on top
of novel components such as TOPLOC, which verifies rollouts from untrusted
inference workers, and SHARDCAST, which efficiently broadcasts policy weights
from training nodes to inference workers.
  Beyond infrastructure components, we propose modifications to the standard
GRPO training recipe and data filtering techniques that were crucial to achieve
training stability and ensure that our model successfully learned its training
objective, thus improving upon QwQ-32B, the state of the art reasoning model in
the 32B parameter range.
  We open-source INTELLECT-2 along with all of our code and data, hoping to
encourage and enable more open research in the field of decentralized training.

</details>


### [177] [Online Episodic Convex Reinforcement Learning](https://arxiv.org/abs/2505.07303)
*Bianca Marin Moreno,Khaled Eldowa,Pierre Gaillard,Margaux Brégère,Nadia Oudjane*

Main category: cs.LG

TL;DR: 该论文研究了在具有凸目标函数的有限时间马尔可夫决策过程中进行在线学习的问题（CURL问题），提出了一种无需先验知识的算法，并在更复杂的“仅反馈目标函数值”的bandit设定下实现了次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 经典的强化学习通常假设线性损失，而现实问题往往涉及更复杂的非线性目标（即凸目标函数）。这种非线性使得传统的方法（如贝尔曼方程）失效，因此需要开发新的算法来解决这一问题。

Method: 采用在线的镜像下降算法，结合动态约束集和专门设计的探索奖励，来解决CURL问题。在bandit设定下，则通过将bandit凸优化的技术迁移到MDP环境中来实现。

Result: 论文的算法在标准CURL设定下实现了近乎最优的遗憾界；而在bandit设定下，尽管是更困难的问题，仍然获得了次线性的遗憾界。

Conclusion: 该研究证明了在非线性强化学习问题中实现高效在线学习的可行性，为处理更复杂的强化学习场景提供了新思路和新工具。

Abstract: We study online learning in episodic finite-horizon Markov decision processes
(MDPs) with convex objective functions, known as the concave utility
reinforcement learning (CURL) problem. This setting generalizes RL from linear
to convex losses on the state-action distribution induced by the agent's
policy. The non-linearity of CURL invalidates classical Bellman equations and
requires new algorithmic approaches. We introduce the first algorithm achieving
near-optimal regret bounds for online CURL without any prior knowledge on the
transition function. To achieve this, we use an online mirror descent algorithm
with varying constraint sets and a carefully designed exploration bonus. We
then address for the first time a bandit version of CURL, where the only
feedback is the value of the objective function on the state-action
distribution induced by the agent's policy. We achieve a sub-linear regret
bound for this more challenging problem by adapting techniques from bandit
convex optimization to the MDP setting.

</details>


### [178] [Uncertainty Profiles for LLMs: Uncertainty Source Decomposition and Adaptive Model-Metric Selection](https://arxiv.org/abs/2505.07309)
*Pei-Fu Guo,Yun-Da Tsai,Shou-De Lin*

Main category: cs.LG

TL;DR: 该论文提出了一个系统性框架，将大语言模型（LLM）的不确定性分解为四种来源，并开发了量化这些不确定性的方法。研究还提出了一种基于任务特性选择合适模型或不确定性指标的策略，实验证明其效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决大语言模型生成内容中的事实性错误（幻觉）问题，并提升不确定性估计的可解释性和适用性。

Method: 论文通过分解LLM的不确定性为四种来源，开发了针对每种来源的量化方法，并基于任务特性设计了一种模型或指标选择策略。

Result: 实验表明，该框架能系统性地分析不确定性特征，且提出的选择策略在多个数据集和模型上均优于基线方法。

Conclusion: 该研究为提升LLM的可靠性提供了一种有效的不确定性估计和选择方法，有助于实际应用中的高效部署。

Abstract: Large language models (LLMs) often generate fluent but factually incorrect
outputs, known as hallucinations, which undermine their reliability in
real-world applications. While uncertainty estimation has emerged as a
promising strategy for detecting such errors, current metrics offer limited
interpretability and lack clarity about the types of uncertainty they capture.
In this paper, we present a systematic framework for decomposing LLM
uncertainty into four distinct sources, inspired by previous research. We
develop a source-specific estimation pipeline to quantify these uncertainty
types and evaluate how existing metrics relate to each source across tasks and
models. Our results show that metrics, task, and model exhibit systematic
variation in uncertainty characteristic. Building on this, we propose a method
for task specific metric/model selection guided by the alignment or divergence
between their uncertainty characteristics and that of a given task. Our
experiments across datasets and models demonstrate that our uncertainty-aware
selection strategy consistently outperforms baseline strategies, helping us
select appropriate models or uncertainty metrics, and contributing to more
reliable and efficient deployment in uncertainty estimation.

</details>


### [179] [Dynamical Label Augmentation and Calibration for Noisy Electronic Health Records](https://arxiv.org/abs/2505.07320)
*Yuhao Li,Ling Luo,Uwe Aickelin*

Main category: cs.LG

TL;DR: 提出了一个名为ACTLL的注意力学习框架，用于处理医疗时间序列数据中的标签噪声问题，通过动态校准和增强提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 医疗时间序列数据（如EHR）中标签错误不可避免，严重影响了患者结局预测的准确性。

Method: 采用基于注意力的学习框架（ACTLL），结合Beta混合模型分类确定实例的确定集和不确定集，并动态校准或增强标签。

Result: 在多个数据集（如eICU、MIMIC-IV-ED、UCR、UEA）上验证，ACTLL在高噪声水平下表现最优。

Conclusion: ACTLL能有效处理标签噪声问题，提升医疗时间序列数据的预测性能。

Abstract: Medical research, particularly in predicting patient outcomes, heavily relies
on medical time series data extracted from Electronic Health Records (EHR),
which provide extensive information on patient histories. Despite rigorous
examination, labeling errors are inevitable and can significantly impede
accurate predictions of patient outcome. To address this challenge, we propose
an \textbf{A}ttention-based Learning Framework with Dynamic
\textbf{C}alibration and Augmentation for \textbf{T}ime series Noisy
\textbf{L}abel \textbf{L}earning (ACTLL). This framework leverages a
two-component Beta mixture model to identify the certain and uncertain sets of
instances based on the fitness distribution of each class, and it captures
global temporal dynamics while dynamically calibrating labels from the
uncertain set or augmenting confident instances from the certain set.
Experimental results on large-scale EHR datasets eICU and MIMIC-IV-ED, and
several benchmark datasets from the UCR and UEA repositories, demonstrate that
our model ACTLL has achieved state-of-the-art performance, especially under
high noise levels.

</details>


### [180] [From Search To Sampling: Generative Models For Robust Algorithmic Recourse](https://arxiv.org/abs/2505.07351)
*Prateek Garg,Lokesh Nagalapatti,Sunita Sarawagi*

Main category: cs.LG

TL;DR: GenRe是一种生成式补救模型，通过联合训练解决现有方法在推荐补救方案时的不足，实现了成本、合理性和有效性的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有补救方法分别训练目标后通过联合优化搜索推荐，导致结果不佳，需要一种能联合训练这些目标的替代方案。

Method: 提出GenRe模型，通过生成式方法联合训练补救目标，并利用合成监督训练模型，简化推理过程为前向采样。

Result: GenRe在成本、合理性和有效性上优于现有基线，且推理效率更高。

Conclusion: GenRe模型通过联合训练生成式方法，显著提升了补救推荐的质量和效率。

Abstract: Algorithmic Recourse provides recommendations to individuals who are
adversely impacted by automated model decisions, on how to alter their profiles
to achieve a favorable outcome. Effective recourse methods must balance three
conflicting goals: proximity to the original profile to minimize cost,
plausibility for realistic recourse, and validity to ensure the desired
outcome. We show that existing methods train for these objectives separately
and then search for recourse through a joint optimization over the recourse
goals during inference, leading to poor recourse recommendations. We introduce
GenRe, a generative recourse model designed to train the three recourse
objectives jointly. Training such generative models is non-trivial due to lack
of direct recourse supervision. We propose efficient ways to synthesize such
supervision and further show that GenRe's training leads to a consistent
estimator. Unlike most prior methods, that employ non-robust gradient descent
based search during inference, GenRe simply performs a forward sampling over
the generative model to produce minimum cost recourse, leading to superior
performance across multiple metrics. We also demonstrate GenRe provides the
best trade-off between cost, plausibility and validity, compared to
state-of-art baselines. Our code is available at:
https://github.com/prateekgargx/genre.

</details>


### [181] [Generalization Bounds and Stopping Rules for Learning with Self-Selected Data](https://arxiv.org/abs/2505.07367)
*Julian Rodemann,James Bailie*

Main category: cs.LG

TL;DR: 该论文提出了统一多种自选训练数据的学习范式（如主动学习、半监督学习等）为'互惠学习'框架，并探讨其泛化能力，证明了无分布假设的通用泛化边界。


<details>
  <summary>Details</summary>
Motivation: 研究自选数据学习方法的泛化能力，为实践者提供理论支持，确保其模型在未知数据上的表现。

Method: 使用覆盖数和Wasserstein模糊集证明泛化边界，无需数据分布假设，仅需算法可验证条件。

Result: 证明了收敛解和有限迭代解的泛化边界，后者适用于任意停止规则，并通过半监督学习实例验证。

Conclusion: 该框架为自选数据学习提供了普适性理论工具，尤其是泛化边界和停止规则的实际应用价值显著。

Abstract: Many learning paradigms self-select training data in light of previously
learned parameters. Examples include active learning, semi-supervised learning,
bandits, or boosting. Rodemann et al. (2024) unify them under the framework of
"reciprocal learning". In this article, we address the question of how well
these methods can generalize from their self-selected samples. In particular,
we prove universal generalization bounds for reciprocal learning using covering
numbers and Wasserstein ambiguity sets. Our results require no assumptions on
the distribution of self-selected data, only verifiable conditions on the
algorithms. We prove results for both convergent and finite iteration
solutions. The latter are anytime valid, thereby giving rise to stopping rules
for a practitioner seeking to guarantee the out-of-sample performance of their
reciprocal learning algorithm. Finally, we illustrate our bounds and stopping
rules for reciprocal learning's special case of semi-supervised learning.

</details>


### [182] [ICE-Pruning: An Iterative Cost-Efficient Pruning Pipeline for Deep Neural Networks](https://arxiv.org/abs/2505.07411)
*Wenhao Hu,Paul Henderson,José Cano*

Main category: cs.LG

TL;DR: 论文提出了一种名为ICE-Pruning的迭代剪枝方法，通过自动决定剪枝后是否需要微调、冻结策略和剪枝感知学习率调度器，显著减少了剪枝时间，同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 传统的剪枝方法需要反复微调，计算成本高。本文旨在减少剪枝时间，同时保持模型精度。

Method: ICE-Pruning包括三个核心组件：1) 自动决定剪枝后是否微调的机制；2) 加速微调的冻结策略；3) 剪枝感知学习率调度器。还包括超参数自动调优。

Result: 在多个DNN模型和数据集上的实验表明，ICE-Pruning能将剪枝速度提升高达9.61倍。

Conclusion: ICE-Pruning在减少剪枝时间的同时，保持了与现有方法相当的精度，是一种高效的剪枝方案。

Abstract: Pruning is a widely used method for compressing Deep Neural Networks (DNNs),
where less relevant parameters are removed from a DNN model to reduce its size.
However, removing parameters reduces model accuracy, so pruning is typically
combined with fine-tuning, and sometimes other operations such as rewinding
weights, to recover accuracy. A common approach is to repeatedly prune and then
fine-tune, with increasing amounts of model parameters being removed in each
step. While straightforward to implement, pruning pipelines that follow this
approach are computationally expensive due to the need for repeated
fine-tuning.
  In this paper we propose ICE-Pruning, an iterative pruning pipeline for DNNs
that significantly decreases the time required for pruning by reducing the
overall cost of fine-tuning, while maintaining a similar accuracy to existing
pruning pipelines. ICE-Pruning is based on three main components: i) an
automatic mechanism to determine after which pruning steps fine-tuning should
be performed; ii) a freezing strategy for faster fine-tuning in each pruning
step; and iii) a custom pruning-aware learning rate scheduler to further
improve the accuracy of each pruning step and reduce the overall time
consumption. We also propose an efficient auto-tuning stage for the
hyperparameters (e.g., freezing percentage) introduced by the three components.
We evaluate ICE-Pruning on several DNN models and datasets, showing that it can
accelerate pruning by up to 9.61x. Code is available at
https://github.com/gicLAB/ICE-Pruning

</details>


### [183] [Learning Penalty for Optimal Partitioning via Automatic Feature Extraction](https://arxiv.org/abs/2505.07413)
*Tung L Nguyen,Toby Hocking*

Main category: cs.LG

TL;DR: 该研究提出了一种基于循环神经网络的新方法，直接从原始数据序列中学习变化点检测的惩罚参数，自动提取特征。实验证明其在大部分基因组基准数据集上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 变化点检测在多个领域至关重要，但传统方法需手动提取统计特征来确定惩罚参数，效率较低且效果有限。

Method: 采用循环神经网络直接从原始序列中学习并预测惩罚参数，无需人工特征提取。

Result: 在20个基因组基准数据集上的实验显示，新方法在分区准确性上多数优于传统方法。

Conclusion: 基于深度学习的方法为变化点检测的惩罚参数选择提供了更高效且自动化的解决方案。

Abstract: Changepoint detection identifies significant shifts in data sequences, making
it important in areas like finance, genetics, and healthcare. The Optimal
Partitioning algorithms efficiently detect these changes, using a penalty
parameter to limit the changepoints number. Determining the appropriate value
for this penalty can be challenging. Traditionally, this process involved
manually extracting statistical features, such as sequence length or variance
to make the prediction. This study proposes a novel approach that uses
recurrent neural networks to learn this penalty directly from raw sequences by
automatically extracting features. Experiments conducted on 20 benchmark
genomic datasets show that this novel method surpasses traditional methods in
partitioning accuracy in most cases.

</details>


### [184] [LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning](https://arxiv.org/abs/2505.07437)
*Xiaotian Lin,Yanlin Qi,Yizhang Zhu,Themis Palpanas,Chengliang Chai,Nan Tang,Yuyu Luo*

Main category: cs.LG

TL;DR: LEAD提出了一种高效的迭代数据选择框架，通过Instance-Level Dynamic Uncertainty (IDU)在标准训练循环中估计样本效用，无需额外模型推理。该方法显著提升了模型性能，同时大幅减少了训练时间和数据需求。


<details>
  <summary>Details</summary>
Motivation: 现有的迭代模型感知数据选择方法需要重复进行全数据集模型推理以估计样本效用，计算开销大，形成了效率瓶颈。LEAD旨在消除这一瓶颈，提升数据选择效率。

Method: LEAD引入了Instance-Level Dynamic Uncertainty (IDU)作为理论基础样本效用函数，结合了瞬时训练损失、梯度近似损失变化和历史损失信号指数平滑。采用两阶段粗到细选择策略：基于多臂老虎机机制自适应优先选择信息丰富的聚类，随后用IDU精确选择高效用样本。

Result: 在四个多样化基准测试中，LEAD显著优于现有方法，平均模型性能提升6.1%-10.8%，仅使用2.5%训练数据，总训练时间减少5-10倍。

Conclusion: LEAD通过高效的数据选择框架，在减少计算开销的同时显著提升模型性能，为解决大语言模型训练中的数据选择效率问题提供了可行方案。

Abstract: Instruction tuning has emerged as a critical paradigm for improving the
capabilities and alignment of large language models (LLMs). However, existing
iterative model-aware data selection methods incur significant computational
overhead, as they rely on repeatedly performing full-dataset model inference to
estimate sample utility for subsequent training iterations, creating a
fundamental efficiency bottleneck. In this paper, we propose LEAD, an efficient
iterative data selection framework that accurately estimates sample utility
entirely within the standard training loop, eliminating the need for costly
additional model inference. At its core, LEAD introduces Instance-Level Dynamic
Uncertainty (IDU), a theoretically grounded utility function combining
instantaneous training loss, gradient-based approximation of loss changes, and
exponential smoothing of historical loss signals. To further scale efficiently
to large datasets, LEAD employs a two-stage, coarse-to-fine selection strategy,
adaptively prioritizing informative clusters through a multi-armed bandit
mechanism, followed by precise fine-grained selection of high-utility samples
using IDU. Extensive experiments across four diverse benchmarks show that LEAD
significantly outperforms state-of-the-art methods, improving average model
performance by 6.1%-10.8% while using only 2.5% of the training data and
reducing overall training time by 5-10x.

</details>


### [185] [Unified Continuous Generative Models](https://arxiv.org/abs/2505.07447)
*Peng Sun,Yi Jiang,Tao Lin*

Main category: cs.LG

TL;DR: 论文提出了一种统一的连续生成模型框架UCGM-{T,S}，实现了多步（如扩散模型）与少步（如一致性模型）方法的统一训练与采样，并在ImageNet 256x256上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作通常将多步与少步生成模型视为独立范式，导致训练与采样方法分离。本文旨在提供一个统一框架以整合这些方法。

Method: 提出UCGM-{T,S}框架，统一训练、采样及分析连续生成模型，支持多步与少步模型的协同优化。

Result: 在ImageNet 256x256上，UCGM-T训练的多步模型20步FID达1.30，少步模型2步FID达1.42；UCGM-S将预训练模型（原250步FID 1.26）提升至40步FID 1.06。

Conclusion: UCGM框架证明了统一多步与少步生成模型的可行性，显著提升了生成效率与性能，为未来研究提供了新方向。

Abstract: Recent advances in continuous generative models, including multi-step
approaches like diffusion and flow-matching (typically requiring 8-1000
sampling steps) and few-step methods such as consistency models (typically 1-8
steps), have demonstrated impressive generative performance. However, existing
work often treats these approaches as distinct paradigms, resulting in separate
training and sampling methodologies. We introduce a unified framework for
training, sampling, and analyzing these models. Our implementation, the Unified
Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves
state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a
675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID
in 20 steps and a few-step model reaching 1.42 FID in just 2 steps.
Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at
250 steps) improves performance to 1.06 FID in only 40 steps. Code is available
at: https://github.com/LINs-lab/UCGM.

</details>


### [186] [Prototype Augmented Hypernetworks for Continual Learning](https://arxiv.org/abs/2505.07450)
*Neil De La Fuente,Maria Pilligua,Daniel Vidal,Albin Soutiff,Cecilia Curreli,Daniel Cremers,Andrey Barsky*

Main category: cs.LG

TL;DR: 提出了Prototype-Augmented Hypernetworks (PAH)框架，通过动态生成任务特定分类器头和双重蒸馏损失来减少持续学习中的遗忘问题，在Split-CIFAR100和TinyImageNet上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习（CL）中，新任务的梯度更新常覆盖之前学到的权重，导致灾难性遗忘（CF）。为了解决这一问题，论文提出了PAH框架。

Method: PAH利用一个超网络，基于可学习的任务原型动态生成任务特定分类器头，并通过交叉熵和双重蒸馏损失（logits对齐和原型对齐）来减少遗忘。

Result: 在Split-CIFAR100和TinyImageNet上，PAH分别达到74.5%和63.7%的准确率，遗忘率仅为1.7%和4.4%，优于现有方法。

Conclusion: PAH通过动态生成分类器头和双重蒸馏损失，有效地减少了持续学习的遗忘问题，实现了卓越的性能。

Abstract: Continual learning (CL) aims to learn a sequence of tasks without forgetting
prior knowledge, but gradient updates for a new task often overwrite the
weights learned earlier, causing catastrophic forgetting (CF). We propose
Prototype-Augmented Hypernetworks (PAH), a framework where a single
hypernetwork, conditioned on learnable task prototypes, dynamically generates
task-specific classifier heads on demand. To mitigate forgetting, PAH combines
cross-entropy with dual distillation losses, one to align logits and another to
align prototypes, ensuring stable feature representations across tasks.
Evaluations on Split-CIFAR100 and TinyImageNet demonstrate that PAH achieves
state-of-the-art performance, reaching 74.5 % and 63.7 % accuracy with only 1.7
% and 4.4 % forgetting, respectively, surpassing prior methods without storing
samples or heads.

</details>


### [187] [You Only Look One Step: Accelerating Backpropagation in Diffusion Sampling with Gradient Shortcuts](https://arxiv.org/abs/2505.07477)
*Hongkun Dou,Zeyu Li,Xingyu Jiang,Hongjue Li,Lijun Yang,Wen Yao,Yue Deng*

Main category: cs.LG

TL;DR: 该论文提出了一种名为SDO（Shortcut Diffusion Optimization）的方法，通过仅保留生成过程中一个步骤的计算图来高效优化扩散模型的下游任务，减少了90%的计算成本，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在下游任务中通常需要基于可微分指标进行内容引导，传统方法需要在整个生成过程中进行反向传播，计算成本高昂。为解决这一问题，论文提出了更高效的优化方法。

Method: SDO方法通过并行去噪的视角，仅在生成过程中保留一个步骤的计算图，为梯度传播提供捷径，从而避免全过程的昂贵反向传播。

Result: 实验证明SDO在多个实际任务中表现优异，包括潜在空间优化和网络参数微调，计算成本降低约90%，性能仍优于传统方法。

Conclusion: SDO是一种通用、高效且轻量级的扩散采样优化方法，显著降低了计算开销，同时保持了高性能。

Abstract: Diffusion models (DMs) have recently demonstrated remarkable success in
modeling large-scale data distributions. However, many downstream tasks require
guiding the generated content based on specific differentiable metrics,
typically necessitating backpropagation during the generation process. This
approach is computationally expensive, as generating with DMs often demands
tens to hundreds of recursive network calls, resulting in high memory usage and
significant time consumption. In this paper, we propose a more efficient
alternative that approaches the problem from the perspective of parallel
denoising. We show that full backpropagation throughout the entire generation
process is unnecessary. The downstream metrics can be optimized by retaining
the computational graph of only one step during generation, thus providing a
shortcut for gradient propagation. The resulting method, which we call Shortcut
Diffusion Optimization (SDO), is generic, high-performance, and computationally
lightweight, capable of optimizing all parameter types in diffusion sampling.
We demonstrate the effectiveness of SDO on several real-world tasks, including
controlling generation by optimizing latent and aligning the DMs by fine-tuning
network parameters. Compared to full backpropagation, our approach reduces
computational costs by $\sim 90\%$ while maintaining superior performance. Code
is available at https://github.com/deng-ai-lab/SDO.

</details>


### [188] [Identifying Causal Direction via Variational Bayesian Compression](https://arxiv.org/abs/2505.07503)
*Quang-Duy Tran,Bao Duong,Phuoc Nguyen,Thin Nguyen*

Main category: cs.LG

TL;DR: 该论文提出一种利用变分贝叶斯学习神经网络的方法来优化因果推断中的编码长度，从而在模型拟合性和计算复杂度之间取得平衡。实验证明，该方法在合成和真实数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的因果推断方法通常依赖简单函数或高斯过程来近似编码长度，但这些方法在模型拟合性和计算复杂度之间存在妥协。本文旨在通过神经网络解决这一问题。

Method: 提出了一种基于变分贝叶斯学习神经网络的框架，用于更高效地计算因果方向下的编码长度。

Result: 在合成和真实数据集上的实验表明，该方法在因果识别任务中表现优于现有的复杂度驱动方法和结构因果模型回归方法。

Conclusion: 通过神经网络优化编码长度的计算，可以在保持模型拟合性的同时降低计算复杂度，从而在因果推断任务中实现更优性能。

Abstract: Telling apart the cause and effect between two random variables with purely
observational data is a challenging problem that finds applications in various
scientific disciplines. A key principle utilized in this task is the
algorithmic Markov condition, which postulates that the joint distribution,
when factorized according to the causal direction, yields a more succinct
codelength compared to the anti-causal direction. Previous approaches
approximate these codelengths by relying on simple functions or Gaussian
processes (GPs) with easily evaluable complexity, compromising between model
fitness and computational complexity. To overcome these limitations, we propose
leveraging the variational Bayesian learning of neural networks as an
interpretation of the codelengths. Consequently, we can enhance the model
fitness while promoting the succinctness of the codelengths, while avoiding the
significant computational complexity of the GP-based approaches. Extensive
experiments on both synthetic and real-world benchmarks in cause-effect
identification demonstrate the effectiveness of our proposed method, surpassing
the overall performance of related complexity-based and structural causal model
regression-based approaches.

</details>


### [189] [EAGLE: Contrastive Learning for Efficient Graph Anomaly Detection](https://arxiv.org/abs/2505.07508)
*Jing Ren,Mingliang Hou,Zhixuan Liu,Xiaomei Bai*

Main category: cs.LG

TL;DR: 论文提出了一种名为EAGLE的高效异构图异常检测方法，通过对比异常节点与正常节点在其局部上下文中的距离来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测方法在嵌入式设备上效率不足，需要一种更高效的解决方案。

Method: EAGLE使用元路径级实例对进行对比学习，并结合图自编码器和判别器以无监督方式学习节点嵌入并预测异常得分。

Result: EAGLE在三个异构网络数据集上表现优于现有方法。

Conclusion: EAGLE提供了一种高效的异构图异常检测方法，适用于资源受限的设备。

Abstract: Graph anomaly detection is a popular and vital task in various real-world
scenarios, which has been studied for several decades. Recently, many studies
extending deep learning-based methods have shown preferable performance on
graph anomaly detection. However, existing methods are lack of efficiency that
is definitely necessary for embedded devices. Towards this end, we propose an
Efficient Anomaly detection model on heterogeneous Graphs via contrastive
LEarning (EAGLE) by contrasting abnormal nodes with normal ones in terms of
their distances to the local context. The proposed method first samples
instance pairs on meta path-level for contrastive learning. Then, a graph
autoencoder-based model is applied to learn informative node embeddings in an
unsupervised way, which will be further combined with the discriminator to
predict the anomaly scores of nodes. Experimental results show that EAGLE
outperforms the state-of-the-art methods on three heterogeneous network
datasets.

</details>


### [190] [Adaptive Latent-Space Constraints in Personalized FL](https://arxiv.org/abs/2505.07525)
*Sana Ayromlou,D. B. Emerson*

Main category: cs.LG

TL;DR: 本文研究了在个性化联邦学习（pFL）中应用自适应MMD度量提升模型性能的效果，尤其是在特征异质性显著的任务中。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在分散数据集上训练深度学习模型时面临统计异质性挑战，个性化FL（pFL）结合全局学习和本地建模以应对这一问题。

Method: 在Ditto框架（pFL的先进技术）中，采用理论支持的自适应MMD度量进行优化。

Result: 自适应MMD度量显著提升了模型性能，特别是在特征异质性明显的任务中。

Conclusion: 自适应MMD度量不仅适用于Ditto算法，还可推广到其他pFL场景，激励针对FL系统中异质性的定制约束设计。

Abstract: Federated learning (FL) has become an effective and widely used approach to
training deep learning models on decentralized datasets held by distinct
clients. FL also strengthens both security and privacy protections for training
data. Common challenges associated with statistical heterogeneity between
distributed datasets have spurred significant interest in personalized FL (pFL)
methods, where models combine aspects of global learning with local modeling
specific to each client's unique characteristics. In this work, the efficacy of
theoretically supported, adaptive MMD measures within the Ditto framework, a
state-of-the-art technique in pFL, are investigated. The use of such measures
significantly improves model performance across a variety of tasks, especially
those with pronounced feature heterogeneity. While the Ditto algorithm is
specifically considered, such measures are directly applicable to a number of
other pFL settings, and the results motivate the use of constraints tailored to
the various kinds of heterogeneity expected in FL systems.

</details>


### [191] [Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model Reasoning](https://arxiv.org/abs/2505.07527)
*Hu Wang,Congbo Ma,Ian Reid,Mohammad Yaqub*

Main category: cs.LG

TL;DR: KRPO改进GRPO，通过轻量级卡尔曼滤波动态估计奖励均值和方差，提升优势估计的准确性，适用于高噪声奖励环境。


<details>
  <summary>Details</summary>
Motivation: GRPO在高噪声奖励环境下可能导致优势估计不准确，引入偏差，KRPO旨在解决这一问题。

Method: KRPO使用卡尔曼滤波动态估计潜在奖励均值和方差，替代GRPO的批量均值基线，无需额外学习参数。

Result: 实验表明，KRPO能提升GRPO的稳定性和性能。

Conclusion: KRPO是一种简单有效的方法，能优化语言模型在高动态奖励信号下的策略优化。

Abstract: Reward baseline is important for Reinforcement Learning (RL) algorithms to
reduce variance in policy gradient estimates. Recently, for language modeling,
Group Relative Policy Optimization (GRPO) is proposed to compute the advantage
for each output by subtracting the mean reward, as the baseline, for all
outputs in the group. However, it can lead to inaccurate advantage estimates in
environments with highly noisy rewards, potentially introducing bias. In this
work, we propose a model, called Kalman Filter Enhanced Group Relative Policy
Optimization (KRPO), by using lightweight Kalman filtering to dynamically
estimate the latent reward mean and variance. This filtering technique replaces
the naive batch mean baseline, enabling more adaptive advantage normalization.
Our method does not require additional learned parameters over GRPO. This
approach offers a simple yet effective way to incorporate multiple outputs of
GRPO into advantage estimation, improving policy optimization in settings where
highly dynamic reward signals are difficult to model for language models.
Through experiments and analyses, we show that using a more adaptive advantage
estimation model, KRPO can improve the stability and performance of GRPO. The
code is available at https://github.com/billhhh/KRPO_LLMs_RL

</details>


### [192] [Noise Optimized Conditional Diffusion for Domain Adaptation](https://arxiv.org/abs/2505.07548)
*Lingkun Luo,Shiqiang Hu,Liming Chen*

Main category: cs.LG

TL;DR: 本文提出了NOCDDA方法，通过结合条件扩散模型和领域自适应（DA）任务优化生成高置信伪标签目标域样本（hcpl-tds），解决了传统伪标签方法中样本不足的问题，并显著提升了跨领域对齐效果。


<details>
  <summary>Details</summary>
Motivation: 传统伪标签方法在无监督领域自适应（UDA）中因高置信伪标签目标域样本（hcpl-tds）稀缺，导致跨领域统计对齐不准确，从而失败。

Method: 所提出的NOCDDA方法整合了条件扩散模型的生成能力和DA的决策需求，引入类感知噪声优化策略，优化反向采样生成hcpl-tds的过程，提高跨领域一致性。

Result: 在5个基准数据集和29个DA任务上的实验表明，NOCDDA在31种现有方法中表现最优，验证了其鲁棒性和有效性。

Conclusion: NOCDDA通过噪声优化和条件扩散模型的结合，显著提升了跨领域自适应的性能，为UDA任务提供了新的解决方案。

Abstract: Pseudo-labeling is a cornerstone of Unsupervised Domain Adaptation (UDA), yet
the scarcity of High-Confidence Pseudo-Labeled Target Domain Samples
(\textbf{hcpl-tds}) often leads to inaccurate cross-domain statistical
alignment, causing DA failures. To address this challenge, we propose
\textbf{N}oise \textbf{O}ptimized \textbf{C}onditional \textbf{D}iffusion for
\textbf{D}omain \textbf{A}daptation (\textbf{NOCDDA}), which seamlessly
integrates the generative capabilities of conditional diffusion models with the
decision-making requirements of DA to achieve task-coupled optimization for
efficient adaptation. For robust cross-domain consistency, we modify the DA
classifier to align with the conditional diffusion classifier within a unified
optimization framework, enabling forward training on noise-varying cross-domain
samples. Furthermore, we argue that the conventional \( \mathcal{N}(\mathbf{0},
\mathbf{I}) \) initialization in diffusion models often generates
class-confused hcpl-tds, compromising discriminative DA. To resolve this, we
introduce a class-aware noise optimization strategy that refines sampling
regions for reverse class-specific hcpl-tds generation, effectively enhancing
cross-domain alignment. Extensive experiments across 5 benchmark datasets and
29 DA tasks demonstrate significant performance gains of \textbf{NOCDDA} over
31 state-of-the-art methods, validating its robustness and effectiveness.

</details>


### [193] [Injecting Knowledge Graphs into Large Language Models](https://arxiv.org/abs/2505.07554)
*Erica Coppolillo*

Main category: cs.LG

TL;DR: 本文提出了一种将知识图谱（KG）嵌入大型语言模型（LLM）的新方法，通过知识图谱嵌入（KGE）模型实现图感知推理，提高了推理性能并平衡了准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要依赖提示工程或微调，但会损失结构保真度或计算成本高，因此需要一种既高效又能保持结构的方法。

Method: 利用知识图谱嵌入（KGE）模型将图编码为LLM的输入令牌，从而实现图感知推理，该方法与模型无关且资源高效。

Result: 在合成和真实数据集上的实验表明，该方法提升了推理性能，并在准确性和效率上达到了最佳平衡。

Conclusion: 该方法为符号推理提供了一种高效且通用的解决方案，适用于任何LLM。

Abstract: Integrating structured knowledge from Knowledge Graphs (KGs) into Large
Language Models (LLMs) remains a key challenge for symbolic reasoning. Existing
methods mainly rely on prompt engineering or fine-tuning, which lose structural
fidelity or incur high computational costs. Building on recent encoding
techniques which integrate graph embeddings within the LLM input as tokens, we
extend this paradigm to the KG domain by leveraging Knowledge Graph Embedding
(KGE) models, thus enabling graph-aware reasoning. Our approach is
model-agnostic, resource-efficient, and compatible with any LLMs. Extensive
experimentation on synthetic and real-world datasets shows that our method
improves reasoning performance over established baselines, further achieving
the best trade-off in terms of accuracy and efficiency against state-of-the-art
LLMs.

</details>


### [194] [Personalized Federated Learning under Model Dissimilarity Constraints](https://arxiv.org/abs/2505.07575)
*Samuel Erickson,Mikael Johansson*

Main category: cs.LG

TL;DR: 本文提出了一种名为KARULA的个性化联邦学习策略，通过正则化客户间模型差异来应对统计异质性，并基于1-Wasserstein距离的替代度量来约束差异，同时提出了一种不精确投影随机梯度算法以求解问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的统计异质性是一个关键挑战，不同客户的数据分布差异较大，传统方法（如聚类）难以捕捉复杂的客户间关系。

Method: KARULA通过正则化客户间模型差异，并使用1-Wasserstein距离的替代度量作为约束，提出了一种不精确投影随机梯度算法来解决约束问题。

Result: 理论分析表明该算法在平滑（可能非凸）损失下以O(1/K)的速率收敛到稳定点邻域，并在合成和真实联邦数据集上验证了有效性。

Conclusion: KARULA能够有效处理复杂客户间关系，提升联邦学习性能。

Abstract: One of the defining challenges in federated learning is that of statistical
heterogeneity among clients. We address this problem with KARULA, a regularized
strategy for personalized federated learning, which constrains the pairwise
model dissimilarities between clients based on the difference in their
distributions, as measured by a surrogate for the 1-Wasserstein distance
adapted for the federated setting. This allows the strategy to adapt to highly
complex interrelations between clients, that e.g., clustered approaches fail to
capture. We propose an inexact projected stochastic gradient algorithm to solve
the constrained problem that the strategy defines, and show theoretically that
it converges with smooth, possibly non-convex losses to a neighborhood of a
stationary point with rate O(1/K). We demonstrate the effectiveness of KARULA
on synthetic and real federated data sets.

</details>


### [195] [Trial and Trust: Addressing Byzantine Attacks with Comprehensive Defense Strategy](https://arxiv.org/abs/2505.07614)
*Gleb Molodtsov,Daniil Medyakov,Sergey Skorik,Nikolas Khachaturov,Shahane Tigranyan,Vladimir Aletov,Aram Avetisyan,Martin Takáč,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 该论文提出了一种结合信任分数和试函数方法的新算法，动态过滤恶意更新，有效抵御拜占庭攻击，并适用于多种实际场景（如Adam/RMSProp优化器、本地训练等）。实验验证其在合成和真实医疗数据上的鲁棒性，理论分析也证明了其收敛性与无攻击情况下的经典算法相当。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习的计算需求增长，联邦学习与分布式架构的脆弱性（如拜占庭攻击）成为关键问题。现有方法在面对多数节点恶意时失效，需动态且适应性强的解决方案。

Method: 提出信任分数与试函数结合的方法，动态识别并过滤异常更新。算法兼容主流优化器（如Adam、RMSProp）及本地训练、部分参与等实际场景。

Result: 实验证明，在合成和真实ECG数据中，算法能有效抵御拜占庭攻击（即使恶意节点占多数）。理论分析显示其收敛性接近无攻击的经典算法。

Conclusion: 该方法为拜占庭攻击提供了通用且鲁棒的防御框架，兼具理论保证与实战适应性。

Abstract: Recent advancements in machine learning have improved performance while also
increasing computational demands. While federated and distributed setups
address these issues, their structure is vulnerable to malicious influences. In
this paper, we address a specific threat, Byzantine attacks, where compromised
clients inject adversarial updates to derail global convergence. We combine the
trust scores concept with trial function methodology to dynamically filter
outliers. Our methods address the critical limitations of previous approaches,
allowing functionality even when Byzantine nodes are in the majority. Moreover,
our algorithms adapt to widely used scaled methods like Adam and RMSProp, as
well as practical scenarios, including local training and partial
participation. We validate the robustness of our methods by conducting
extensive experiments on both synthetic and real ECG data collected from
medical institutions. Furthermore, we provide a broad theoretical analysis of
our algorithms and their extensions to aforementioned practical setups. The
convergence guarantees of our methods are comparable to those of classical
algorithms developed without Byzantine interference.

</details>


### [196] [Enhancing Federated Learning with Kolmogorov-Arnold Networks: A Comparative Study Across Diverse Aggregation Strategies](https://arxiv.org/abs/2505.07629)
*Yizhou Ma,Zhuoqin Yang,Luis-Daniel Ibáñez*

Main category: cs.LG

TL;DR: 该论文探讨Kolmogorov-Arnold Networks (KAN)在联邦学习(FL)中的性能，相比传统多层感知机(MLP)，KAN在准确性、稳定性和收敛效率上表现更优，尤其是在非独立同分布数据和客户端异构性高的场景下。


<details>
  <summary>Details</summary>
Motivation: 传统的MLP在处理复杂数据时难以高效捕捉非线性关系，而KAN基于Kolmogorov-Arnold表示定理，有望解决这一问题。作者希望通过FL框架验证KAN的性能优势。

Method: 在四个多样化数据集上对比KAN和MLP的表现，评估参数聚合策略（如trimmed mean和FedProx），分析KAN在客户端数量和不同数据分布下的鲁棒性。

Result: KAN在所有数据集上均优于MLP，尤其是在非独立同分布数据中表现稳健，且收敛所需通信轮次更少。trimmed mean和FedProx对KAN性能优化效果最佳。

Conclusion: KAN是联邦学习中比MLP更高效、稳健的替代方案，适用于去中心化和隐私保护场景，为实际应用提供了新思路。

Abstract: Multilayer Perceptron (MLP), as a simple yet powerful model, continues to be
widely used in classification and regression tasks. However, traditional MLPs
often struggle to efficiently capture nonlinear relationships in load data when
dealing with complex datasets. Kolmogorov-Arnold Networks (KAN), inspired by
the Kolmogorov-Arnold representation theorem, have shown promising capabilities
in modeling complex nonlinear relationships. In this study, we explore the
performance of KANs within federated learning (FL) frameworks and compare them
to traditional Multilayer Perceptrons. Our experiments, conducted across four
diverse datasets demonstrate that KANs consistently outperform MLPs in terms of
accuracy, stability, and convergence efficiency. KANs exhibit remarkable
robustness under varying client numbers and non-IID data distributions,
maintaining superior performance even as client heterogeneity increases.
Notably, KANs require fewer communication rounds to converge compared to MLPs,
highlighting their efficiency in FL scenarios. Additionally, we evaluate
multiple parameter aggregation strategies, with trimmed mean and FedProx
emerging as the most effective for optimizing KAN performance. These findings
establish KANs as a robust and scalable alternative to MLPs for federated
learning tasks, paving the way for their application in decentralized and
privacy-preserving environments.

</details>


### [197] [Generating Skyline Explanations for Graph Neural Networks](https://arxiv.org/abs/2505.07635)
*Dazhuo Qiu,Haolai Che,Arijit Khan,Yinghui Wu*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的方法，通过同时优化多种可解释性指标来生成图神经网络（GNN）的子图解释，解决了现有方法因单指标优化导致的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有GNN解释方法通常基于单一可解释性指标生成子图解释，可能导致偏差且缺乏全面性，无法充分阐明GNN的输出。

Method: 提出天际线解释范式，将问题建模为多目标优化，设计高效的基于洋葱剥皮法的算法，逐步优化子图解释，并提供多样性算法以增强解释全面性。

Result: 在真实世界图数据上的实验验证了算法的有效性、高效性和可扩展性。

Conclusion: 天际线解释方法通过多指标优化和多样性增强，为GNN提供了更全面、无偏的解释。

Abstract: This paper proposes a novel approach to generate subgraph explanations for
graph neural networks GNNs that simultaneously optimize multiple measures for
explainability. Existing GNN explanation methods often compute subgraphs
(called ``explanatory subgraphs'') that optimize a pre-defined, single
explainability measure, such as fidelity or conciseness. This can lead to
biased explanations that cannot provide a comprehensive explanation to clarify
the output of GNN models. We introduce skyline explanation, a GNN explanation
paradigm that aims to identify k explanatory subgraphs by simultaneously
optimizing multiple explainability measures. (1) We formulate skyline
explanation generation as a multi-objective optimization problem, and pursue
explanations that approximate a skyline set of explanatory subgraphs. We show
the hardness for skyline explanation generation. (2) We design efficient
algorithms with an onion-peeling approach that strategically removes edges from
neighbors of nodes of interests, and incrementally improves explanations as it
explores an interpretation domain, with provable quality guarantees. (3) We
further develop an algorithm to diversify explanations to provide more
comprehensive perspectives. Using real-world graphs, we empirically verify the
effectiveness, efficiency, and scalability of our algorithms.

</details>


### [198] [Joint Graph Convolution and Sequential Modeling for Scalable Network Traffic Estimation](https://arxiv.org/abs/2505.07674)
*Nan Jiang,Wenxuan Zhu,Xu Han,Weiqiang Huang,Yumeng Sun*

Main category: cs.LG

TL;DR: 本文提出了一种结合图卷积网络（GCN）和门控循环单元（GRU）的时空模型，用于复杂拓扑环境中的网络流量预测。通过真实网络数据集验证，该模型在多项指标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 复杂拓扑环境中的网络流量预测是一个挑战性问题，需要同时捕捉空间依赖和时间演化特征。

Method: 提出GCN-GRU混合模型：GCN处理网络节点的空间关系，GRU建模流量数据的时间动态性，并通过消融实验分析各组件影响。

Result: 模型在Abilene数据集上表现优于其他深度学习方法，展示了鲁棒的稳定性和泛化能力。

Conclusion: GCN与GRU的结合能有效提升复杂网络流量预测的精度，为实际应用提供了可靠工具。

Abstract: This study focuses on the challenge of predicting network traffic within
complex topological environments. It introduces a spatiotemporal modeling
approach that integrates Graph Convolutional Networks (GCN) with Gated
Recurrent Units (GRU). The GCN component captures spatial dependencies among
network nodes, while the GRU component models the temporal evolution of traffic
data. This combination allows for precise forecasting of future traffic
patterns. The effectiveness of the proposed model is validated through
comprehensive experiments on the real-world Abilene network traffic dataset.
The model is benchmarked against several popular deep learning methods.
Furthermore, a set of ablation experiments is conducted to examine the
influence of various components on performance, including changes in the number
of graph convolution layers, different temporal modeling strategies, and
methods for constructing the adjacency matrix. Results indicate that the
proposed approach achieves superior performance across multiple metrics,
demonstrating robust stability and strong generalization capabilities in
complex network traffic forecasting scenarios.

</details>


### [199] [Simple Semi-supervised Knowledge Distillation from Vision-Language Models via $\mathbf{\texttt{D}}$ual-$\mathbf{\texttt{H}}$ead $\mathbf{\texttt{O}}$ptimization](https://arxiv.org/abs/2505.07675)
*Seongjae Kang,Dong Bok Lee,Hyungjoon Jang,Sung Ju Hwang*

Main category: cs.LG

TL;DR: 提出了DHO框架，通过双预测头简化知识蒸馏过程，提高模型在资源受限环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在资源受限环境下部署的挑战，避免多阶段训练或额外调参的复杂性。

Method: 引入双预测头独立学习标记数据和教师预测，并在推理时线性组合输出。

Result: 在多项实验和细粒度数据集中表现优异，ImageNet上1%和10%标记数据分别提升3%和0.1%准确率。

Conclusion: DHO框架有效解决梯度冲突，提升特征学习，适用于资源受限场景。

Abstract: Vision-language models (VLMs) have achieved remarkable success across diverse
tasks by leveraging rich textual information with minimal labeled data.
However, deploying such large models remains challenging, particularly in
resource-constrained environments. Knowledge distillation (KD) offers a
well-established solution to this problem; however, recent KD approaches from
VLMs often involve multi-stage training or additional tuning, increasing
computational overhead and optimization complexity. In this paper, we propose
$\mathbf{\texttt{D}}$ual-$\mathbf{\texttt{H}}$ead
$\mathbf{\texttt{O}}$ptimization ($\mathbf{\texttt{DHO}}$) -- a simple yet
effective KD framework that transfers knowledge from VLMs to compact,
task-specific models in semi-supervised settings. Specifically, we introduce
dual prediction heads that independently learn from labeled data and teacher
predictions, and propose to linearly combine their outputs during inference. We
observe that $\texttt{DHO}$ mitigates gradient conflicts between supervised and
distillation signals, enabling more effective feature learning than single-head
KD baselines. As a result, extensive experiments show that $\texttt{DHO}$
consistently outperforms baselines across multiple domains and fine-grained
datasets. Notably, on ImageNet, it achieves state-of-the-art performance,
improving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,
while using fewer parameters.

</details>


### [200] [SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in Large Language Models](https://arxiv.org/abs/2505.07680)
*Hang Wu,Jianian Zhu,Yinghui Li,Haojie Wang,Biao Hou,Jidong Zhai*

Main category: cs.LG

TL;DR: 该论文提出了一个名为\systemname{}的动态路由框架，通过多层次推测性解码优化大语言模型推理的延迟问题，根据实时反馈选择最优模型链。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理质量和计算成本之间存在权衡，现有静态策略无法动态适应用户请求的复杂性或系统性能波动。\systemname{}旨在解决这一局限性。

Method: 通过自适应模型链调度、多层次协同验证和同步状态管理，动态优化模型链以减少延迟。

Result: 初步实验验证了该方法的有效性。

Conclusion: \systemname{}通过动态路由和多层次解码显著优化了推理延迟，为大型语言模型的高效服务提供了新思路。

Abstract: Large Language Models (LLMs) present a critical trade-off between inference
quality and computational cost: larger models offer superior capabilities but
incur significant latency, while smaller models are faster but less powerful.
Existing serving strategies often employ fixed model scales or static two-stage
speculative decoding, failing to dynamically adapt to the varying complexities
of user requests or fluctuations in system performance. This paper introduces
\systemname{}, a novel framework that reimagines LLM inference as an adaptive
routing problem solved through multi-level speculative decoding. \systemname{}
dynamically constructs and optimizes inference "paths" (chains of models) based
on real-time feedback, addressing the limitations of static approaches. Our
contributions are threefold: (1) An \textbf{adaptive model chain scheduling}
mechanism that leverages performance profiling (execution times) and predictive
similarity metrics (derived from token distribution divergence) to continuously
select the optimal sequence of draft and verifier models, minimizing predicted
latency per generated token. (2) A \textbf{multi-level collaborative
verification} framework where intermediate models within the selected chain can
validate speculative tokens, reducing the verification burden on the final,
most powerful target model. (3) A \textbf{synchronized state management} system
providing efficient, consistent KV cache handling across heterogeneous models
in the chain, including precise, low-overhead rollbacks tailored for
asynchronous batch processing inherent in multi-level speculation. Preliminary
experiments demonstrate the validity of our method.

</details>


### [201] [Multimodal Survival Modeling in the Age of Foundation Models](https://arxiv.org/abs/2505.07683)
*Steven Song,Morgan Borjigin-Wang,Irene Madejski,Robert L. Grossman*

Main category: cs.LG

TL;DR: 该研究利用多模态基础模型（FMs）从TCGA数据中提取特征嵌入，结合病理报告文本，提升了癌症生存预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 癌症基因组图谱（TCGA）提供了大量多模态数据，但病理报告文本长期未被充分利用。研究旨在探索通过FMs提取特征嵌入，结合多模态数据提升生存预测模型的性能。

Method: 使用零样本FMs提取特征嵌入，构建多模态生存模型，融合文本和基因组数据，并评估文本摘要和幻觉对模型的影响。

Result: 多模态融合模型优于单模态模型，病理报告文本的加入进一步提升了性能，验证了FMs在生存预测中的有效性。

Conclusion: 通过FMs和多模态数据融合，研究为癌症生存预测提供了现代化方法，突显了病理报告文本的价值。

Abstract: The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a
large-scale reference through its harmonized genomics, clinical, and image
data. Prior studies have trained bespoke cancer survival prediction models from
unimodal or multimodal TCGA data. A modern paradigm in biomedical deep learning
is the development of foundation models (FMs) to derive meaningful feature
embeddings, agnostic to a specific modeling task. Biomedical text especially
has seen growing development of FMs. While TCGA contains free-text data as
pathology reports, these have been historically underutilized. Here, we
investigate the feasibility of training classical, multimodal survival models
over zero-shot embeddings extracted by FMs. We show the ease and additive
effect of multimodal fusion, outperforming unimodal models. We demonstrate the
benefit of including pathology report text and rigorously evaluate the effect
of model-based text summarization and hallucination. Overall, we modernize
survival modeling by leveraging FMs and information extraction from pathology
reports.

</details>


### [202] [4TaStiC: Time and trend traveling time series clustering for classifying long-term type 2 diabetes patients](https://arxiv.org/abs/2505.07702)
*Onthada Preedasawakul,Nathakhun Wiroonsri*

Main category: cs.LG

TL;DR: 论文提出了一种名为4TaStiC的聚类算法，用于解决糖尿病患者时间序列数据聚类中的挑战，结合欧几里得和皮尔逊相关系数，在人工和真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 糖尿病患者的时间序列数据（如血红蛋白A1c）聚类能帮助医生更高效地制定治疗方案，但数据时间点不一致、趋势差异大，现有方法难以有效处理。

Method: 提出4TaStiC算法，结合欧几里得距离和皮尔逊相关系数作为基 dissimilarity 度量，解决数据时间不一致和趋势差异问题。

Result: 在人工数据集上，4TaStiC表现优于七种现有方法；应用于1,989名2型糖尿病患者数据后，聚类结果清晰，有助于临床决策。

Conclusion: 4TaStiC算法不仅适用于医疗领域，还可推广至其他时间序列聚类场景。

Abstract: Diabetes is one of the most prevalent diseases worldwide, characterized by
persistently high blood sugar levels, capable of damaging various internal
organs and systems. Diabetes patients require routine check-ups, resulting in a
time series of laboratory records, such as hemoglobin A1c, which reflects each
patient's health behavior over time and informs their doctor's recommendations.
Clustering patients into groups based on their entire time series data assists
doctors in making recommendations and choosing treatments without the need to
review all records. However, time series clustering of this type of dataset
introduces some challenges; patients visit their doctors at different time
points, making it difficult to capture and match trends, peaks, and patterns.
Additionally, two aspects must be considered: differences in the levels of
laboratory results and differences in trends and patterns. To address these
challenges, we introduce a new clustering algorithm called Time and Trend
Traveling Time Series Clustering (4TaStiC), using a base dissimilarity measure
combined with Euclidean and Pearson correlation metrics. We evaluated this
algorithm on artificial datasets, comparing its performance with that of seven
existing methods. The results show that 4TaStiC outperformed the other methods
on the targeted datasets. Finally, we applied 4TaStiC to cluster a cohort of
1,989 type 2 diabetes patients at Siriraj Hospital. Each group of patients
exhibits clear characteristics that will benefit doctors in making efficient
clinical decisions. Furthermore, the proposed algorithm can be applied to
contexts outside the medical field.

</details>


### [203] [Assessing the Chemical Intelligence of Large Language Models](https://arxiv.org/abs/2505.07735)
*Nicholas T. Runcie,Charlotte M. Deane,Fergus Imrie*

Main category: cs.LG

TL;DR: 本文评估了推理大模型在化学任务中的表现，创建了ChemIQ基准测试，结果显示推理模型在有机化学问题上的表现优于非推理模型，并能完成SMILES转IUPAC名和解析NMR数据等任务。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证推理大模型能否无需外部工具直接完成化学任务，推动大模型在专业领域的应用。

Method: 方法包括创建ChemIQ基准测试（包含796个有机化学问题），测试推理模型（如o3-mini）与非推理模型（GPT-4o）的表现，任务涵盖分子理解和化学推理。

Result: 结果显示推理模型正确率28%-59%，显著优于GPT-4o（7%），并能完成SMILES转IUPAC名和74%的NMR结构解析（原子数≤10）。

Conclusion: 结论表明最新推理模型具备高级化学推理能力，其推理过程与人类化学家相似，展现了在专业领域的潜力。

Abstract: Large Language Models are versatile, general-purpose tools with a wide range
of applications. Recently, the advent of "reasoning models" has led to
substantial improvements in their abilities in advanced problem-solving domains
such as mathematics and software engineering. In this work, we assessed the
ability of reasoning models to directly perform chemistry tasks, without any
assistance from external tools. We created a novel benchmark, called ChemIQ,
which consists of 796 questions assessing core concepts in organic chemistry,
focused on molecular comprehension and chemical reasoning. Unlike previous
benchmarks, which primarily use multiple choice formats, our approach requires
models to construct short-answer responses, more closely reflecting real-world
applications. The reasoning models, exemplified by OpenAI's o3-mini, correctly
answered 28%-59% of questions depending on the reasoning level used, with
higher reasoning levels significantly increasing performance on all tasks.
These models substantially outperformed the non-reasoning model, GPT-4o, which
achieved only 7% accuracy. We found that Large Language Models can now convert
SMILES strings to IUPAC names, a task earlier models were unable to perform.
Additionally, we show that the latest reasoning models can elucidate structures
from 1H and 13C NMR data, correctly generating SMILES strings for 74% of
molecules containing up to 10 heavy atoms, and in one case solving a structure
comprising 21 heavy atoms. For each task, we found evidence that the reasoning
process mirrors that of a human chemist. Our results demonstrate that the
latest reasoning models have the ability to perform advanced chemical
reasoning.

</details>


### [204] [The Pitfalls of Benchmarking in Algorithm Selection: What We Are Getting Wrong](https://arxiv.org/abs/2505.07750)
*Gašper Petelin,Gjorgjina Cenikj*

Main category: cs.LG

TL;DR: 本文批评了算法选择元模型评估中的常见方法问题，指出“leave-instance-out”技术及对目标函数尺度敏感的指标可能导致误导性结果。


<details>
  <summary>Details</summary>
Motivation: 算法选择在优化中至关重要，但评估方法的不严谨可能导致无效特征或模型被高估，干扰领域发展。

Method: 通过分析非信息性特征和模型的伪高精度现象，以及尺度敏感指标的误导性，揭示评估方法的缺陷。

Result: 发现当前评估框架可能掩盖元模型的真实性能，导致过于乐观的结论。

Conclusion: 呼吁采用更严谨的评估方法论，以避免误导研究方向和浪费资源。

Abstract: Algorithm selection, aiming to identify the best algorithm for a given
problem, plays a pivotal role in continuous black-box optimization. A common
approach involves representing optimization functions using a set of features,
which are then used to train a machine learning meta-model for selecting
suitable algorithms. Various approaches have demonstrated the effectiveness of
these algorithm selection meta-models. However, not all evaluation approaches
are equally valid for assessing the performance of meta-models. We highlight
methodological issues that frequently occur in the community and should be
addressed when evaluating algorithm selection approaches. First, we identify
flaws with the "leave-instance-out" evaluation technique. We show that
non-informative features and meta-models can achieve high accuracy, which
should not be the case with a well-designed evaluation framework. Second, we
demonstrate that measuring the performance of optimization algorithms with
metrics sensitive to the scale of the objective function requires careful
consideration of how this impacts the construction of the meta-model, its
predictions, and the model's error. Such metrics can falsely present overly
optimistic performance assessments of the meta-models. This paper emphasizes
the importance of careful evaluation, as loosely defined methodologies can
mislead researchers, divert efforts, and introduce noise into the field

</details>


### [205] [Synthesizing Diverse Network Flow Datasets with Scalable Dynamic Multigraph Generation](https://arxiv.org/abs/2505.07777)
*Arya Grayeli,Vipin Swarup,Steven E. Noel*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的机器学习模型，用于生成高保真且能代表真实网络的合成网络流数据集，通过动态多重图生成和特征生成结合，并采用XGBoost进行图对齐，提升了生成准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 由于隐私、安全和计算限制，获取真实网络数据集具有挑战性，因此需要高质量的合成数据集生成工具。

Method: 结合随机Kronecker图生成器生成结构，使用表格生成对抗网络生成特征，并用XGBoost模型进行图对齐。

Result: 新模型在准确性上优于之前的大规模图生成方法，同时保持了相似的效率，并探索了准确性与多样性之间的权衡。

Conclusion: 论文贡献包括合成和评估大规模真实网络流数据集，并定义了评估合成图生成模型的新指标。

Abstract: Obtaining real-world network datasets is often challenging because of
privacy, security, and computational constraints. In the absence of such
datasets, graph generative models become essential tools for creating synthetic
datasets. In this paper, we introduce a novel machine learning model for
generating high-fidelity synthetic network flow datasets that are
representative of real-world networks. Our approach involves the generation of
dynamic multigraphs using a stochastic Kronecker graph generator for structure
generation and a tabular generative adversarial network for feature generation.
We further employ an XGBoost (eXtreme Gradient Boosting) model for graph
alignment, ensuring accurate overlay of features onto the generated graph
structure. We evaluate our model using new metrics that assess both the
accuracy and diversity of the synthetic graphs. Our results demonstrate
improvements in accuracy over previous large-scale graph generation methods
while maintaining similar efficiency. We also explore the trade-off between
accuracy and diversity in synthetic graph dataset creation, a topic not
extensively covered in related works. Our contributions include the synthesis
and evaluation of large real-world netflow datasets and the definition of new
metrics for evaluating synthetic graph generative models.

</details>


### [206] [MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering](https://arxiv.org/abs/2505.07782)
*Rushi Qiang,Yuchen Zhuang,Yinghao Li,Dingu Sagar V K,Rongzhi Zhang,Changhao Li,Ian Shu-Hei Wong,Sherry Yang,Percy Liang,Chao Zhang,Bo Dai*

Main category: cs.LG

TL;DR: 论文介绍了MLE-Dojo框架，这是一个用于系统强化学习、评估和改进大型语言模型（LLM）代理的交互式环境，基于200多个真实Kaggle挑战任务。当前模型虽能迭代改进，但在解决复杂任务时仍有局限。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要依赖静态数据集或单次评估，无法支持代理在真实MLE工作流中迭代实验和改进。需一个交互式环境来模拟真实工程场景，促进代理的持续优化。

Method: 构建MLE-Dojo框架，基于200+真实Kaggle任务，涵盖数据预处理、架构搜索、超参数调优等多场景。支持监督微调和强化学习，提供实时反馈和可执行环境。

Result: 评估8种前沿LLM显示，代理能实现迭代改进，但在生成长期解决方案和高效解决复杂错误方面仍有明显不足。

Conclusion: MLE-Dojo通过灵活架构和开源社区支持，推动了下一代MLE代理的研发，但其局限性也凸显了未来改进的方向。

Abstract: We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement
learning, evaluating, and improving autonomous large language model (LLM)
agents in iterative machine learning engineering (MLE) workflows. Unlike
existing benchmarks that primarily rely on static datasets or single-attempt
evaluations, MLE-Dojo provides an interactive environment enabling agents to
iteratively experiment, debug, and refine solutions through structured feedback
loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,
open-ended MLE tasks carefully curated to reflect realistic engineering
scenarios such as data processing, architecture search, hyperparameter tuning,
and code debugging. Its fully executable environment supports comprehensive
agent training via both supervised fine-tuning and reinforcement learning,
facilitating iterative experimentation, realistic data sampling, and real-time
outcome verification. Extensive evaluations of eight frontier LLMs reveal that
while current models achieve meaningful iterative improvements, they still
exhibit significant limitations in autonomously generating long-horizon
solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's
flexible and extensible architecture seamlessly integrates diverse data
sources, tools, and evaluation protocols, uniquely enabling model-based agent
tuning and promoting interoperability, scalability, and reproducibility. We
open-source our framework and benchmarks to foster community-driven innovation
towards next-generation MLE agents.

</details>


### [207] [Relative Overfitting and Accept-Reject Framework](https://arxiv.org/abs/2505.07783)
*Yanxin Liu,Yunqi Zhang*

Main category: cs.LG

TL;DR: 论文提出‘相对过拟合’概念和AR框架，通过结合LLM和SLM解决大模型规模化瓶颈问题，实验验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型规模化面临瓶颈，作者认为噪声效应是关键问题，需探索新方法优化性能与成本。

Method: 通过分析模型差异提出‘相对过拟合’概念，设计AR框架利用SLM辅助LLM决策，降低计算成本。

Result: 实验证明AR框架在多种任务上优于单纯增加LLM参数，性能提升显著且稳定。

Conclusion: 该框架普适性强，或可扩展至CV等领域，有望突破现有规模化瓶颈。

Abstract: Currently, the scaling law of Large Language Models (LLMs) faces challenges
and bottlenecks. This paper posits that noise effects, stemming from changes in
the signal-to-noise ratio under diminishing marginal returns, are the root
cause of these issues. To control this noise, we investigated the differences
between models with performance advantages and disadvantages, introducing the
concept of "relative overfitting." Based on their complementary strengths, we
have proposed an application framework, Accept-Reject (AR). In Natural Language
Processing (NLP), we use LLMs and Small Language Models (SLMs) as the medium
for discussion. This framework enables SLMs to exert a universal positive
influence on LLM decision outputs, rather than the intuitively expected
negative influence. We validated our approach using self-built models based on
mainstream architectures and pre-trained mainstream models across multiple
datasets, including basic language modeling, long-context tasks, subject
examination, and question-answering (QA) benchmarks. The results demonstrate
that through our structure, compared to increasing the LLM's parameters, we can
achieve better performance improvements with significantly lower parameter and
computational costs in many scenarios. These improvements are universal,
stable, and effective. Furthermore, we explore the potential of "relative
overfitting" and the AR framework in other machine learning domains, such as
computer vision (CV) and AI for science. We hope the proposed approach can help
scale laws overcome existing bottlenecks.

</details>


### [208] [Overflow Prevention Enhances Long-Context Recurrent LLMs](https://arxiv.org/abs/2505.07793)
*Assaf Ben-Kish,Itamar Zimerman,M. Jehanzeb Mirza,James Glass,Leonid Karlinsky,Raja Giryes*

Main category: cs.LG

TL;DR: 长上下文处理效率是当前LLMs的研究热点，本文发现即使训练了长上下文，这些模型的固定大小循环内存仍导致其利用率不足。通过分块推理方法，能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究现有的长上下文大模型如何利用其固定大小的循环内存，以及如何通过改进方法来提高其长上下文任务的性能。

Method: 采用分块推理方法，仅处理和识别输入中最相关的部分，以减轻循环内存的不足。

Result: 在LongBench上，方法显著提升了多个模型的性能（如Falcon3-Mamba-Inst-7B提升14%）。在LongBench v2上，方法甚至达到了与Transformer相当的最先进结果。

Conclusion: 分块推理方法展示了其在长上下文任务中的有效性，但同时也质疑了循环模型是否真正利用了长距离依赖关系。

Abstract: A recent trend in LLMs is developing recurrent sub-quadratic models that
improve long-context processing efficiency. We investigate leading large
long-context models, focusing on how their fixed-size recurrent memory affects
their performance. Our experiments reveal that, even when these models are
trained for extended contexts, their use of long contexts remains
underutilized. Specifically, we demonstrate that a chunk-based inference
procedure, which identifies and processes only the most relevant portion of the
input can mitigate recurrent memory failures and be effective for many
long-context tasks: On LongBench, our method improves the overall performance
of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%,
RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this
simple approach also leads to state-of-the-art results in the challenging
LongBench v2 benchmark, showing competitive performance with equivalent size
Transformers. Furthermore, our findings raise questions about whether recurrent
models genuinely exploit long-range dependencies, as our single-chunk strategy
delivers stronger performance - even in tasks that presumably require
cross-context relations.

</details>


### [209] [A Theoretical Framework for Explaining Reinforcement Learning with Shapley Values](https://arxiv.org/abs/2505.07797)
*Daniel Beechey,Thomas M. S. Smith,Özgür Şimşek*

Main category: cs.LG

TL;DR: 提出了一种利用博弈论中Shapley值的方法，解释强化学习中的状态特征影响，以提高模型的透明度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽性能优越但决策不透明，限制了在安全关键领域的应用。

Method: 通过状态特征博弈论分析行为、性能和值估计三个交互要素，应用Shapley值量化特征影响。

Result: 建立了一套数学严谨的解释框架，验证了其符合人类直觉并提供新见解。

Conclusion: 该框架为强化学习的可解释性和信任度奠定了理论基础，统一并扩展了现有方法。

Abstract: Reinforcement learning agents can achieve superhuman performance, but their
decisions are often difficult to interpret. This lack of transparency limits
deployment, especially in safety-critical settings where human trust and
accountability are essential. In this work, we develop a theoretical framework
for explaining reinforcement learning through the influence of state features,
which represent what the agent observes in its environment. We identify three
core elements of the agent-environment interaction that benefit from
explanation: behaviour (what the agent does), performance (what the agent
achieves), and value estimation (what the agent expects to achieve). We treat
state features as players cooperating to produce each element and apply Shapley
values, a principled method from cooperative game theory, to identify the
influence of each feature. This approach yields a family of mathematically
grounded explanations with clear semantics and theoretical guarantees. We use
illustrative examples to show how these explanations align with human intuition
and reveal novel insights. Our framework unifies and extends prior work, making
explicit the assumptions behind existing approaches, and offers a principled
foundation for more interpretable and trustworthy reinforcement learning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [210] [BedreFlyt: Improving Patient Flows through Hospital Wards with Digital Twins](https://arxiv.org/abs/2505.06287)
*Riccardo Sieve,Paul Kobialka,Laura Slaughter,Rudolf Schlatte,Einar Broch Johnsen,Silvia Lizeth Tapia Tarifa*

Main category: cs.AI

TL;DR: 该论文提出了一种用于医院病房资源规划的数字孪生架构，结合形式化模型、本体论和SMT求解器，以支持短期决策和长期战略规划。


<details>
  <summary>Details</summary>
Motivation: 数字孪生在多领域决策和规划中展现出潜力，特别是医疗资源分配问题。本研究旨在解决医院病房资源规划的复杂需求。

Method: 通过形式化可执行模型、本体论知识表示和SMT求解器构建数字孪生架构，用于探索假设场景和优化问题。

Result: 该方法能够生成包括平均和最坏情况下资源需求的场景，支持医院的短期及长期规划。

Conclusion: 数字孪生架构能够有效提升医院病房资源规划的灵活性和效率，适用于多变的需求和资源约束。

Abstract: Digital twins are emerging as a valuable tool for short-term decision-making
as well as for long-term strategic planning across numerous domains, including
process industry, energy, space, transport, and healthcare. This paper reports
on our ongoing work on designing a digital twin to enhance resource planning,
e.g., for the in-patient ward needs in hospitals. By leveraging executable
formal models for system exploration, ontologies for knowledge representation
and an SMT solver for constraint satisfiability, our approach aims to explore
hypothetical "what-if" scenarios to improve strategic planning processes, as
well as to solve concrete, short-term decision-making tasks. Our proposed
solution uses the executable formal model to turn a stream of arriving
patients, that need to be hospitalized, into a stream of optimization problems,
e.g., capturing daily inpatient ward needs, that can be solved by SMT
techniques. The knowledge base, which formalizes domain knowledge, is used to
model the needed configuration in the digital twin, allowing the twin to
support both short-term decision-making and long-term strategic planning by
generating scenarios spanning average-case as well as worst-case resource
needs, depending on the expected treatment of patients, as well as ranging over
variations in available resources, e.g., bed distribution in different rooms.
We illustrate our digital twin architecture by considering the problem of bed
bay allocation in a hospital ward.

</details>


### [211] [A Grounded Memory System For Smart Personal Assistants](https://arxiv.org/abs/2505.06328)
*Felix Ocker,Jörg Deigmöller,Pavel Smirnov,Julian Eggert*

Main category: cs.AI

TL;DR: 提出一种基于视觉语言模型和大型语言模型的三组件记忆系统，用于增强AI代理的现实记忆能力。


<details>
  <summary>Details</summary>
Motivation: 为需要现实记忆支持的AI应用（如认知辅助、机器人）设计一种健壮的记忆系统。

Method: 结合视觉语言模型（图像描述与实体消歧）和大型语言模型（信息提取），构建知识图谱与向量嵌入的混合记忆系统，并通过检索增强生成实现问答。

Result: 系统通过真实案例展示了其功能和潜力。

Conclusion: 三组件记忆系统能有效支持AI代理的现实记忆需求。

Abstract: A wide variety of agentic AI applications - ranging from cognitive assistants
for dementia patients to robotics - demand a robust memory system grounded in
reality. In this paper, we propose such a memory system consisting of three
components. First, we combine Vision Language Models for image captioning and
entity disambiguation with Large Language Models for consistent information
extraction during perception. Second, the extracted information is represented
in a memory consisting of a knowledge graph enhanced by vector embeddings to
efficiently manage relational information. Third, we combine semantic search
and graph query generation for question answering via Retrieval Augmented
Generation. We illustrate the system's working and potential using a real-world
example.

</details>


### [212] [Reliable Collaborative Conversational Agent System Based on LLMs and Answer Set Programming](https://arxiv.org/abs/2505.06438)
*Yankai Zeng,Gopal Gupta*

Main category: cs.AI

TL;DR: 论文提出了一种基于逻辑编程的双代理架构（Administrator-Assistant Dual-Agent），通过共享知识库和协作规则集（CRS）实现高效安全的任务导向对话（TOD），对比实验表明其可靠性优于纯LLM驱动的方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的AI机器人在任务导向对话中存在知识不可靠、协作效率低且易受恶意注入的问题，需一种安全可靠的方法提升性能。

Method: 采用Answer Set Programming（ASP）工具构建双代理系统，包括管理员和助手代理，通过CRS共享知识并独立完成任务，确保信息安全传输。

Result: 开发了AutoManager系统，应用于快餐店汽车餐厅场景，实验证明其比Taco Bell现有AI接单系统更可靠。

Conclusion: 基于逻辑编程的双代理架构能有效提升任务导向对话的可靠性与安全性，为实际应用提供了可行方案。

Abstract: As the Large-Language-Model-driven (LLM-driven) Artificial Intelligence (AI)
bots became popular, people realized their strong potential in Task-Oriented
Dialogue (TOD). However, bots relying wholly on LLMs are unreliable in their
knowledge, and whether they can finally produce a correct result for the task
is not guaranteed. The collaboration among these agents also remains a
challenge, since the necessary information to convey is unclear, and the
information transfer is by prompts -- unreliable, and malicious knowledge is
easy to inject. With the help of logic programming tools such as Answer Set
Programming (ASP), conversational agents can be built safely and reliably, and
communication among the agents made more efficient and secure. We proposed an
Administrator-Assistant Dual-Agent paradigm, where the two ASP-driven bots
share the same knowledge base and complete their tasks independently, while the
information can be passed by a Collaborative Rule Set (CRS). The knowledge and
information conveyed are encapsulated and invisible to the users, ensuring the
security of information transmission. We have constructed AutoManager, a
dual-agent system for managing the drive-through window of a fast-food
restaurant such as Taco Bell in the US. In AutoManager, the assistant bot takes
the customer's order while the administrator bot manages the menu and food
supply. We evaluated our AutoManager and compared it with the real-world Taco
Bell Drive-Thru AI Order Taker, and the results show that our method is more
reliable.

</details>


### [213] [Opening the Scope of Openness in AI](https://arxiv.org/abs/2505.06464)
*Tamara Paris,AJung Moon,Jin Guo*

Main category: cs.AI

TL;DR: 论文探讨了AI领域开放性的概念，指出目前主要受开源软件启发，但开源软件的优势不能完全适用于AI。通过分析98个开放性概念并构建分类法，作者呼吁更全面的AI开放性定义，涵盖行为、系统属性和伦理目标。


<details>
  <summary>Details</summary>
Motivation: 当前AI开放性概念主要基于开源软件，但其优势不完全适用于AI。作者希望构建更适合AI的开放性定义，以应对其社会影响、风险和潜力。

Method: 通过主题建模发现98个开放性概念，定性分析并构建分类法，用于评估AI开放性的讨论现状。

Result: 提出了一个开放性分类法，揭示了AI开放性讨论的局限，并强调了与其他学科的关联。

Conclusion: 呼吁超越开源软件的开放性框架，建立更全面的AI开放性定义，涵盖行为、系统属性和伦理目标。

Abstract: The concept of openness in AI has so far been heavily inspired by the
definition and community practice of open source software. This positions
openness in AI as having positive connotations; it introduces assumptions of
certain advantages, such as collaborative innovation and transparency. However,
the practices and benefits of open source software are not fully transferable
to AI, which has its own challenges. Framing a notion of openness tailored to
AI is crucial to addressing its growing societal implications, risks, and
capabilities. We argue that considering the fundamental scope of openness in
different disciplines will broaden discussions, introduce important
perspectives, and reflect on what openness in AI should mean. Toward this goal,
we qualitatively analyze 98 concepts of openness discovered from topic
modeling, through which we develop a taxonomy of openness. Using this taxonomy
as an instrument, we situate the current discussion on AI openness, identify
gaps and highlight links with other disciplines. Our work contributes to the
recent efforts in framing openness in AI by reflecting principles and practices
of openness beyond open source software and calls for a more holistic view of
openness in terms of actions, system properties, and ethical objectives.

</details>


### [214] [KCluster: An LLM-based Clustering Approach to Knowledge Component Discovery](https://arxiv.org/abs/2505.06469)
*Yumou Wei,Paulo Carvalho,John Stamper*

Main category: cs.AI

TL;DR: KCluster是一个基于大型语言模型（LLM）的新型知识组件（KC）发现算法，通过聚类相似问题自动生成KC模型，显著优于专家设计的模型。


<details>
  <summary>Details</summary>
Motivation: 传统KC模型设计依赖人工分析，难以应对大规模题库和生成式AI快速生成问题的挑战。

Method: 利用LLM衡量问题相似性，结合聚类算法自动生成KC模型，减少人工干预。

Result: KCluster生成的KC模型在学生表现预测上优于专家设计的模型，并能提供描述性标签。

Conclusion: KCluster为教育领域提供了一种高效的KC模型生成方法，未来可进一步优化教学。

Abstract: Educators evaluate student knowledge using knowledge component (KC) models
that map assessment questions to KCs. Still, designing KC models for large
question banks remains an insurmountable challenge for instructors who need to
analyze each question by hand. The growing use of Generative AI in education is
expected only to aggravate this chronic deficiency of expert-designed KC
models, as course engineers designing KCs struggle to keep up with the pace at
which questions are generated. In this work, we propose KCluster, a novel KC
discovery algorithm based on identifying clusters of congruent questions
according to a new similarity metric induced by a large language model (LLM).
We demonstrate in three datasets that an LLM can create an effective metric of
question similarity, which a clustering algorithm can use to create KC models
from questions with minimal human effort. Combining the strengths of LLM and
clustering, KCluster generates descriptive KC labels and discovers KC models
that predict student performance better than the best expert-designed models
available. In anticipation of future work, we illustrate how KCluster can
reveal insights into difficult KCs and suggest improvements to instruction.

</details>


### [215] [SmartPilot: A Multiagent CoPilot for Adaptive and Intelligent Manufacturing](https://arxiv.org/abs/2505.06492)
*Chathurangi Shyalika,Renjith Prasad,Alaa Al Ghazo,Darssan Eswaramoorthi,Harleen Kaur,Sara Shree Muthuselvam,Amit Sheth*

Main category: cs.AI

TL;DR: SmartPilot是一个神经符号化的多代理CoPilot，旨在通过处理复杂传感器数据解决工业4.0中的异常预测、生产预测和领域特定问题回答，提升制造业的智能决策能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型虽能检测异常，但缺乏对异常深层分析的见解，且传统AI模型在处理复杂传感器数据时效果有限，导致供应链中断和生产效率低下。因此，需要一种能无缝集成这些能力的统一解决方案。

Method: SmartPilot结合神经符号化和多代理技术，处理多模态传感器数据，并可在边缘设备上部署。其核心任务包括异常预测、生产预测和领域特定问题回答。

Result: SmartPilot成功实现了复杂工业场景中的智能决策，通过提供异常深层分析和精准生产预测，填补了AI能力与实际工业需求之间的鸿沟。

Conclusion: SmartPilot通过其神经符号化和多代理设计，为制造业提供了智能决策支持，推动了工业4.0中的变革性创新。

Abstract: In the dynamic landscape of Industry 4.0, achieving efficiency, precision,
and adaptability is essential to optimize manufacturing operations. Industries
suffer due to supply chain disruptions caused by anomalies, which are being
detected by current AI models but leaving domain experts uncertain without
deeper insights into these anomalies. Additionally, operational inefficiencies
persist due to inaccurate production forecasts and the limited effectiveness of
traditional AI models for processing complex sensor data. Despite these
advancements, existing systems lack the seamless integration of these
capabilities needed to create a truly unified solution for enhancing production
and decision-making. We propose SmartPilot, a neurosymbolic, multiagent CoPilot
designed for advanced reasoning and contextual decision-making to address these
challenges. SmartPilot processes multimodal sensor data and is compact to
deploy on edge devices. It focuses on three key tasks: anomaly prediction,
production forecasting, and domain-specific question answering. By bridging the
gap between AI capabilities and real-world industrial needs, SmartPilot
empowers industries with intelligent decision-making and drives transformative
innovation in manufacturing. The demonstration video, datasets, and
supplementary materials are available at
https://github.com/ChathurangiShyalika/SmartPilot.

</details>


### [216] [On Definite Iterated Belief Revision with Belief Algebras](https://arxiv.org/abs/2505.06505)
*Hua Meng,Zhiguo Long,Michael Sioutis,Zhengchun Zhou*

Main category: cs.AI

TL;DR: 该论文提出了一个基于偏好关系的迭代信念修正新框架，通过信念代数表示信念和新证据，引入额外的修正规则确保结果唯一性，并开发了实用算法。


<details>
  <summary>Details</summary>
Motivation: 在安全关键等应用中，需要确定性信念修正操作符以确保迭代修正的可预测性。传统框架过于宽松，导致多种操作符满足相同条件。

Method: 通过偏好关系表征信念信息，使用信念代数作为语义基础，引入上界约束等额外规则，并开发具体算法实现修正过程。

Result: 证明在给定当前信念状态和新证据下，修正结果是唯一确定的，并通过算法实现实用化。

Conclusion: 新框架提供了一种更可预测、原则性的信念修正方法，适用于现实应用。

Abstract: Traditional logic-based belief revision research focuses on designing rules
to constrain the behavior of revision operators. Frameworks have been proposed
to characterize iterated revision rules, but they are often too loose, leading
to multiple revision operators that all satisfy the rules under the same belief
condition. In many practical applications, such as safety critical ones, it is
important to specify a definite revision operator to enable agents to
iteratively revise their beliefs in a deterministic way. In this paper, we
propose a novel framework for iterated belief revision by characterizing belief
information through preference relations. Semantically, both beliefs and new
evidence are represented as belief algebras, which provide a rich and
expressive foundation for belief revision. Building on traditional revision
rules, we introduce additional postulates for revision with belief algebra,
including an upper-bound constraint on the outcomes of revision. We prove that
the revision result is uniquely determined given the current belief state and
new evidence. Furthermore, to make the framework more useful in practice, we
develop a particular algorithm for performing the proposed revision process. We
argue that this approach may offer a more predictable and principled method for
belief revision, making it suitable for real-world applications.

</details>


### [217] [Text-to-CadQuery: A New Paradigm for CAD Generation with Scalable Large Model Capabilities](https://arxiv.org/abs/2505.06507)
*Haoyang Xie,Feng Ju*

Main category: cs.AI

TL;DR: 论文提出一种直接生成CadQuery代码的方法，利用预训练大语言模型（LLMs）将自然语言转换为3D模型，避免了中间表示的复杂性。通过微调LLMs，性能显著提升，最佳模型准确率达69.3%。


<details>
  <summary>Details</summary>
Motivation: 当前CAD模型创建依赖专业知识和软件，现有方法生成的任务特定命令序列需转换为CAD表示，增加了复杂性。直接生成CadQuery代码可简化流程并利用LLMs的现有能力。

Method: 提出直接生成CadQuery代码的框架，使用预训练LLMs微调处理Text-to-CadQuery数据。通过扩充数据集并微调不同规模的LLMs验证方法有效性。

Result: 最佳模型达到69.3%的top-1准确率（原58.8%），Chamfer Distance降低48.6%。规模更大的模型表现更好。

Conclusion: 直接生成CadQuery代码的方法有效简化了CAD模型生成流程，利用LLMs的预训练能力显著提升了性能，尤其在规模更大的模型中表现更优。

Abstract: Computer-aided design (CAD) is fundamental to modern engineering and
manufacturing, but creating CAD models still requires expert knowledge and
specialized software. Recent advances in large language models (LLMs) open up
the possibility of generative CAD, where natural language is directly
translated into parametric 3D models. However, most existing methods generate
task-specific command sequences that pretrained models cannot directly handle.
These sequences must be converted into CAD representations such as CAD vectors
before a 3D model can be produced, which requires training models from scratch
and adds unnecessary complexity. To tackle this issue, we propose generating
CadQuery code directly from text, leveraging the strengths of pretrained LLMs
to produce 3D models without intermediate representations, using this
Python-based scripting language. Since LLMs already excel at Python generation
and spatial reasoning, fine-tuning them on Text-to-CadQuery data proves highly
effective. Given that these capabilities typically improve with scale, we
hypothesize that larger models will perform better after fine-tuning. To enable
this, we augment the Text2CAD dataset with 170,000 CadQuery annotations. We
fine-tune six open-source LLMs of varying sizes and observe consistent
improvements. Our best model achieves a top-1 exact match of 69.3%, up from
58.8%, and reduces Chamfer Distance by 48.6%. Project page:
https://github.com/Text-to-CadQuery/Text-to-CadQuery.

</details>


### [218] [A Point-Based Algorithm for Distributional Reinforcement Learning in Partially Observable Domains](https://arxiv.org/abs/2505.06518)
*Larry Preuett III*

Main category: cs.AI

TL;DR: 论文提出了分布强化学习在部分可观察环境（POMDPs）中的扩展方法，引入了新的分布贝尔曼算子和ψ-向量表示，开发了DPBVI算法以支持风险敏感控制。


<details>
  <summary>Details</summary>
Motivation: 在部分可观察环境中，智能体面临状态不确定性和策略结果多变的双重挑战。为了解决这些问题，论文旨在开发更安全的算法，处理这些不确定性，从而实现更稳健的决策制定。

Method: 论文扩展了分布强化学习（DistRL）到部分可观察马尔可夫决策过程（POMDPs），提出了新的分布贝尔曼算子并证明其收敛性。同时引入了ψ-向量作为返回分布的有限表示方法，并开发了分布点基值迭代（DPBVI）算法，结合点基备份流程，实现风险敏感控制。

Result: 论文证明了提出的分布贝尔曼算子在p-瓦瑟斯坦度量下的收敛性，并通过DPBVI算法实现了在需要管理罕见高影响事件的领域中更有效的风险敏感决策制定。此外，还提供了开源代码以促进更广泛的研究应用。

Conclusion: 通过将分布强化学习扩展到部分可观察环境，论文不仅提供了理论保障和实用算法，还为稳健决策提供了新的工具，特别是在需要风险管理的关键领域中。

Abstract: In many real-world planning tasks, agents must tackle uncertainty about the
environment's state and variability in the outcomes of any chosen policy. We
address both forms of uncertainty as a first step toward safer algorithms in
partially observable settings. Specifically, we extend Distributional
Reinforcement Learning (DistRL)-which models the entire return distribution for
fully observable domains-to Partially Observable Markov Decision Processes
(POMDPs), allowing an agent to learn the distribution of returns for each
conditional plan. Concretely, we introduce new distributional Bellman operators
for partial observability and prove their convergence under the supremum
p-Wasserstein metric. We also propose a finite representation of these return
distributions via psi-vectors, generalizing the classical alpha-vectors in
POMDP solvers. Building on this, we develop Distributional Point-Based Value
Iteration (DPBVI), which integrates psi-vectors into a standard point-based
backup procedure-bridging DistRL and POMDP planning. By tracking return
distributions, DPBVI naturally enables risk-sensitive control in domains where
rare, high-impact events must be carefully managed. We provide source code to
foster further research in robust decision-making under partial observability.

</details>


### [219] [Online Feedback Efficient Active Target Discovery in Partially Observable Environments](https://arxiv.org/abs/2505.06535)
*Anindya Sarkar,Binglin Ji,Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: 提出了一种名为DiffATD的新方法，利用扩散动力学进行主动目标发现，动态平衡探索与利用，无需监督训练即可在有限采样预算下高效发现目标，并在多个领域中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在数据获取成本高的领域（如医疗影像、环境监测），如何在有限采样预算下最大化目标发现效率是关键问题。传统方法依赖监督训练且缺乏可解释性，因此需要一种无需监督、高效且可解释的新方法。

Method: DiffATD通过扩散动力学维护未观测状态的置信分布，动态平衡探索（选择高不确定性区域）与利用（选择高目标概率区域），并结合增量训练的奖励模型学习目标特征。

Result: 实验表明，DiffATD在医疗影像和遥感等多个领域中显著优于基线方法，且与完全可观测环境下的监督方法性能相当。

Conclusion: DiffATD是一种无需监督、高效且可解释的主动目标发现方法，适用于部分可观测环境，为高成本数据采集领域提供了实用解决方案。

Abstract: In various scientific and engineering domains, where data acquisition is
costly, such as in medical imaging, environmental monitoring, or remote
sensing, strategic sampling from unobserved regions, guided by prior
observations, is essential to maximize target discovery within a limited
sampling budget. In this work, we introduce Diffusion-guided Active Target
Discovery (DiffATD), a novel method that leverages diffusion dynamics for
active target discovery. DiffATD maintains a belief distribution over each
unobserved state in the environment, using this distribution to dynamically
balance exploration-exploitation. Exploration reduces uncertainty by sampling
regions with the highest expected entropy, while exploitation targets areas
with the highest likelihood of discovering the target, indicated by the belief
distribution and an incrementally trained reward model designed to learn the
characteristics of the target. DiffATD enables efficient target discovery in a
partially observable environment within a fixed sampling budget, all without
relying on any prior supervised training. Furthermore, DiffATD offers
interpretability, unlike existing black-box policies that require extensive
supervised training. Through extensive experiments and ablation studies across
diverse domains, including medical imaging and remote sensing, we show that
DiffATD performs significantly better than baselines and competitively with
supervised methods that operate under full environmental observability.

</details>


### [220] [TAROT: Towards Essentially Domain-Invariant Robustness with Theoretical Justification](https://arxiv.org/abs/2505.06580)
*Dongyoon Yang,Jihu Lee,Yongdai Kim*

Main category: cs.AI

TL;DR: 论文提出了一种新的鲁棒域适应算法TAROT，通过新颖的散度度量和泛化边界，显著提升了跨域性能和对抗鲁棒性，并在DomainNet数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 针对现实场景中的对抗攻击和域差异挑战，研究旨在开发一种既能保持跨域性能又具备鲁棒性的模型。

Method: 提出基于新散度度量的鲁棒风险泛化边界，并设计算法TAROT，专注于学习域不变特征以增强适应性和鲁棒性。

Result: TAROT在DomainNet数据集上优于现有方法，尤其在准确率、鲁棒性及域泛化能力上表现突出。

Conclusion: TAROT通过有效的域不变特征学习，展示了在复杂域适应场景中的广泛应用潜力。

Abstract: Robust domain adaptation against adversarial attacks is a critical research
area that aims to develop models capable of maintaining consistent performance
across diverse and challenging domains. In this paper, we derive a new
generalization bound for robust risk on the target domain using a novel
divergence measure specifically designed for robust domain adaptation. Building
upon this, we propose a new algorithm named TAROT, which is designed to enhance
both domain adaptability and robustness. Through extensive experiments, TAROT
not only surpasses state-of-the-art methods in accuracy and robustness but also
significantly enhances domain generalization and scalability by effectively
learning domain-invariant features. In particular, TAROT achieves superior
performance on the challenging DomainNet dataset, demonstrating its ability to
learn domain-invariant representations that generalize well across different
domains, including unseen ones. These results highlight the broader
applicability of our approach in real-world domain adaptation scenarios.

</details>


### [221] [Exploring Multimodal Foundation AI and Expert-in-the-Loop for Sustainable Management of Wild Salmon Fisheries in Indigenous Rivers](https://arxiv.org/abs/2505.06637)
*Chi Xu,Yili Jin,Sami Ma,Rongsheng Qian,Hao Fang,Jiangchuan Liu,Xue Liu,Edith C. H. Ngai,William I. Atlas,Katrina M. Connors,Mark A. Spoljaric*

Main category: cs.AI

TL;DR: 摘要探讨了如何利用多模态基础AI和专家参与框架，通过在太平洋西北部的土著河流中整合视频和声纳监测技术，开发自动化工具来提升野生鲑鱼监测和可持续渔业管理的效率。


<details>
  <summary>Details</summary>
Motivation: 野生鲑鱼对北太平洋沿岸的生态、经济和文化可持续性至关重要，但气候变异性、栖息地丧失以及偏远地区基础设施不足带来的数据限制，对渔业管理构成了巨大挑战。

Method: 研究通过结合视频和声纳监测技术，开发AI工具实现自动物种识别、计数和体长测量，同时引入专家验证和主动学习框架以确保生态相关性和减轻标注负担。

Result: 该项目通过跨领域合作，促进了伦理AI共同开发、开放数据共享和文化敏感的渔业管理，提高了决策准确性并加快了结果交付速度。

Conclusion: 研究表明，利用AI技术和跨学科合作可以有效解决野生鲑鱼监测和管理中的技术与社会挑战，为可持续渔业管理提供了新途径。

Abstract: Wild salmon are essential to the ecological, economic, and cultural
sustainability of the North Pacific Rim. Yet climate variability, habitat loss,
and data limitations in remote ecosystems that lack basic infrastructure
support pose significant challenges to effective fisheries management. This
project explores the integration of multimodal foundation AI and
expert-in-the-loop frameworks to enhance wild salmon monitoring and sustainable
fisheries management in Indigenous rivers across Pacific Northwest. By
leveraging video and sonar-based monitoring, we develop AI-powered tools for
automated species identification, counting, and length measurement, reducing
manual effort, expediting delivery of results, and improving decision-making
accuracy. Expert validation and active learning frameworks ensure ecological
relevance while reducing annotation burdens. To address unique technical and
societal challenges, we bring together a cross-domain, interdisciplinary team
of university researchers, fisheries biologists, Indigenous stewardship
practitioners, government agencies, and conservation organizations. Through
these collaborations, our research fosters ethical AI co-development, open data
sharing, and culturally informed fisheries management.

</details>


### [222] [A Survey on Data-Driven Modeling of Human Drivers' Lane-Changing Decisions](https://arxiv.org/abs/2505.06680)
*Linxuan Huang,Dong-Fan Xie,Li Li,Zhengbing He*

Main category: cs.AI

TL;DR: 该论文综述了数据驱动的车道变换决策模型，重点关注人类驾驶员的行为，系统回顾了建模框架、数据来源、模型结构及其面临的挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 传统车道变换决策模型在复杂环境中过于简化，难以捕捉真实行为。数据驱动方法利用机器学习和丰富数据，能够更好地适应动态环境，尤其适用于联网车辆和自动驾驶的需求。

Method: 通过系统性综述，分析数据驱动车道变换决策模型的框架，包括数据来源与预处理、模型输入输出、目标、结构和验证方法。

Result: 文章总结了数据驱动模型在解码驾驶员决策模式方面的优势，同时指出了安全性、不确定性及技术框架整合的挑战。

Conclusion: 数据驱动的车道变换决策模型在动态环境中有巨大潜力，但需进一步解决安全性和技术整合问题。

Abstract: Lane-changing (LC) behavior, a critical yet complex driving maneuver,
significantly influences driving safety and traffic dynamics. Traditional
analytical LC decision (LCD) models, while effective in specific environments,
often oversimplify behavioral heterogeneity and complex interactions, limiting
their capacity to capture real LCD. Data-driven approaches address these gaps
by leveraging rich empirical data and machine learning to decode latent
decision-making patterns, enabling adaptive LCD modeling in dynamic
environments. In light of the rapid development of artificial intelligence and
the demand for data-driven models oriented towards connected vehicles and
autonomous vehicles, this paper presents a comprehensive survey of data-driven
LCD models, with a particular focus on human drivers LC decision-making. It
systematically reviews the modeling framework, covering data sources and
preprocessing, model inputs and outputs, objectives, structures, and validation
methods. This survey further discusses the opportunities and challenges faced
by data-driven LCD models, including driving safety, uncertainty, as well as
the integration and improvement of technical frameworks.

</details>


### [223] [Bi-level Mean Field: Dynamic Grouping for Large-Scale MARL](https://arxiv.org/abs/2505.06706)
*Yuxuan Zheng,Yihe Zhou,Feiyang Xu,Mingli Song,Shunyu Liu*

Main category: cs.AI

TL;DR: 提出双层次均值场（BMF）方法，通过动态分组捕捉大规模多智能体强化学习中的多样性，减少聚合噪声，提高学习效率。


<details>
  <summary>Details</summary>
Motivation: 解决大规模多智能体强化学习中维度灾难问题，传统均值场方法因忽略个体差异导致聚合噪声。

Method: 引入动态分组模块（基于变分自编码器）和双层次交互模块，分别建模组间和组内交互。

Result: 在多个任务中表现优于现有最优方法。

Conclusion: BMF能有效缓解聚合噪声并提升性能，代码将开源。

Abstract: Large-scale Multi-Agent Reinforcement Learning (MARL) often suffers from the
curse of dimensionality, as the exponential growth in agent interactions
significantly increases computational complexity and impedes learning
efficiency. To mitigate this, existing efforts that rely on Mean Field (MF)
simplify the interaction landscape by approximating neighboring agents as a
single mean agent, thus reducing overall complexity to pairwise interactions.
However, these MF methods inevitably fail to account for individual
differences, leading to aggregation noise caused by inaccurate iterative
updates during MF learning. In this paper, we propose a Bi-level Mean Field
(BMF) method to capture agent diversity with dynamic grouping in large-scale
MARL, which can alleviate aggregation noise via bi-level interaction.
Specifically, BMF introduces a dynamic group assignment module, which employs a
Variational AutoEncoder (VAE) to learn the representations of agents,
facilitating their dynamic grouping over time. Furthermore, we propose a
bi-level interaction module to model both inter- and intra-group interactions
for effective neighboring aggregation. Experiments across various tasks
demonstrate that the proposed BMF yields results superior to the
state-of-the-art methods. Our code will be made publicly available.

</details>


### [224] [Value Iteration with Guessing for Markov Chains and Markov Decision Processes](https://arxiv.org/abs/2505.06769)
*Krishnendu Chatterjee,Mahdi JafariRaviz,Raimundo Saona,Jakub Svoboda*

Main category: cs.AI

TL;DR: 改进的预处理和猜测方法显著降低状态更新的复杂度


<details>
  <summary>Details</summary>
Motivation: 解决经典算法在最坏情况下需要指数级更新的问题

Method: 引入猜测值的新方法并优化预处理步骤

Result: 实现近乎线性的预处理时间与亚指数级状态更新

Conclusion: 新方法在理论和实验上均优于现有技术

Abstract: Two standard models for probabilistic systems are Markov chains (MCs) and
Markov decision processes (MDPs). Classic objectives for such probabilistic
models for control and planning problems are reachability and stochastic
shortest path. The widely studied algorithmic approach for these problems is
the Value Iteration (VI) algorithm which iteratively applies local updates
called Bellman updates. There are many practical approaches for VI in the
literature but they all require exponentially many Bellman updates for MCs in
the worst case. A preprocessing step is an algorithm that is discrete,
graph-theoretical, and requires linear space. An important open question is
whether, after a polynomial-time preprocessing, VI can be achieved with
sub-exponentially many Bellman updates. In this work, we present a new approach
for VI based on guessing values. Our theoretical contributions are twofold.
First, for MCs, we present an almost-linear-time preprocessing algorithm after
which, along with guessing values, VI requires only subexponentially many
Bellman updates. Second, we present an improved analysis of the speed of
convergence of VI for MDPs. Finally, we present a practical algorithm for MDPs
based on our new approach. Experimental results show that our approach provides
a considerable improvement over existing VI-based approaches on several
benchmark examples from the literature.

</details>


### [225] [Control Plane as a Tool: A Scalable Design Pattern for Agentic AI Systems](https://arxiv.org/abs/2505.06817)
*Sivasathivel Kandasamy*

Main category: cs.AI

TL;DR: 该论文综述了基于大型语言模型（LLMs）的自主AI代理的架构挑战，并提出了一种可复用的设计抽象模式——'Control Plane as a Tool'，以解决规模化工具编排中的问题。


<details>
  <summary>Details</summary>
Motivation: 自主AI代理在处理任务时展现出潜力，但当前架构不成熟，尤其在工具编排规模化方面存在空白，亟需高效、安全的解决方案。

Method: 通过全面梳理代理类型、环境交互模式及架构挑战，提出将工具路由逻辑模块化的'Control Plane as a Tool'设计模式。

Result: 该模式通过单一工具接口封装复杂路由逻辑，显著提升了代理在扩展性、安全性和可维护性方面的表现。

Conclusion: 'Control Plane as a Tool'为代理系统设计提供了普适性范式，有效解决了规模化工具管理的核心挑战。

Abstract: Agentic AI systems represent a new frontier in artificial intelligence, where
agents often based on large language models(LLMs) interact with tools,
environments, and other agents to accomplish tasks with a degree of autonomy.
These systems show promise across a range of domains, but their architectural
underpinnings remain immature. This paper conducts a comprehensive review of
the types of agents, their modes of interaction with the environment, and the
infrastructural and architectural challenges that emerge. We identify a gap in
how these systems manage tool orchestration at scale and propose a reusable
design abstraction: the "Control Plane as a Tool" pattern. This pattern allows
developers to expose a single tool interface to an agent while encapsulating
modular tool routing logic behind it. We position this pattern within the
broader context of agent design and argue that it addresses several key
challenges in scaling, safety, and extensibility.

</details>


### [226] [Beyond Patterns: Harnessing Causal Logic for Autonomous Driving Trajectory Prediction](https://arxiv.org/abs/2505.06856)
*Bonan Wang,Haicheng Liao,Chengyue Wang,Bin Rao,Yanchen Guan,Guyang Yu,Jiaxun Zhang,Songning Lai,Chengzhong Xu,Zhenning Li*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于因果推理的新型轨迹预测框架，旨在提高自动驾驶中的预测鲁棒性、泛化能力和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的数据驱动模型主要依赖统计相关性，往往忽略了影响交通行为的因果关系。

Method: 通过将环境分解为空间和时间组件，识别并消除了虚假相关性，同时采用渐进式融合策略整合多模态信息。

Result: 在五个真实数据集上的评估表明，该模型在RMSE和FDE等关键指标上优于现有SOTA方法。

Conclusion: 研究结果表明，因果推理在轨迹预测领域具有变革潜力，有望推动更鲁棒的自动驾驶系统发展。

Abstract: Accurate trajectory prediction has long been a major challenge for autonomous
driving (AD). Traditional data-driven models predominantly rely on statistical
correlations, often overlooking the causal relationships that govern traffic
behavior. In this paper, we introduce a novel trajectory prediction framework
that leverages causal inference to enhance predictive robustness,
generalization, and accuracy. By decomposing the environment into spatial and
temporal components, our approach identifies and mitigates spurious
correlations, uncovering genuine causal relationships. We also employ a
progressive fusion strategy to integrate multimodal information, simulating
human-like reasoning processes and enabling real-time inference. Evaluations on
five real-world datasets--ApolloScape, nuScenes, NGSIM, HighD, and
MoCAD--demonstrate our model's superiority over existing state-of-the-art
(SOTA) methods, with improvements in key metrics such as RMSE and FDE. Our
findings highlight the potential of causal reasoning to transform trajectory
prediction, paving the way for robust AD systems.

</details>


### [227] [Embodied Intelligence: The Key to Unblocking Generalized Artificial Intelligence](https://arxiv.org/abs/2505.06897)
*Jinhao Jiang,Changlin Chen,Shile Feng,Wanru Geng,Zesheng Zhou,Ni Wang,Shuai Li,Feng-Qi Cui,Erbao Dong*

Main category: cs.AI

TL;DR: 本文探讨了嵌入式人工智能（EAI）作为实现通用人工智能（AGI）的基础方法，分析了其四大核心模块（感知、智能决策、行动和反馈）及其对AGI六项核心原则的贡献，并讨论了未来趋势和挑战。


<details>
  <summary>Details</summary>
Motivation: 为实现AGI，EAI因其实时环境交互和物理存在特性成为关键研究方向。现有研究多聚焦特定技术或应用，缺乏对EAI与AGI直接联系的系统性综述。

Method: 系统分析EAI的四大核心模块（感知、决策、行动、反馈）及其如何支持AGI的核心原则，并结合现有技术进展进行讨论。

Result: 研究发现EAI通过动态学习和现实交互，为连接窄AI与AGI提供了关键路径，同时明确了未来研究方向。

Conclusion: EAI是实现AGI的重要基石，其多模块协同与实时交互能力为AGI发展提供了独特优势，但仍需克服技术与理论挑战。

Abstract: The ultimate goal of artificial intelligence (AI) is to achieve Artificial
General Intelligence (AGI). Embodied Artificial Intelligence (EAI), which
involves intelligent systems with physical presence and real-time interaction
with the environment, has emerged as a key research direction in pursuit of
AGI. While advancements in deep learning, reinforcement learning, large-scale
language models, and multimodal technologies have significantly contributed to
the progress of EAI, most existing reviews focus on specific technologies or
applications. A systematic overview, particularly one that explores the direct
connection between EAI and AGI, remains scarce. This paper examines EAI as a
foundational approach to AGI, systematically analyzing its four core modules:
perception, intelligent decision-making, action, and feedback. We provide a
detailed discussion of how each module contributes to the six core principles
of AGI. Additionally, we discuss future trends, challenges, and research
directions in EAI, emphasizing its potential as a cornerstone for AGI
development. Our findings suggest that EAI's integration of dynamic learning
and real-world interaction is essential for bridging the gap between narrow AI
and AGI.

</details>


### [228] [Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence](https://arxiv.org/abs/2505.06907)
*Yu Qiao,Huy Q. Le,Avi Deb Raha,Phuong-Nam Tran,Apurba Adhikary,Mengchun Zhang,Loc X. Nguyen,Eui-Nam Huh,Dusit Niyato,Choong Seon Hong*

Main category: cs.AI

TL;DR: 本文提出个性联邦智能（PFI），结合联邦学习的隐私保护优势和基础模型的零样本泛化能力，旨在实现个性化、高效、隐私保护的边缘部署，为通用人工智能（AGI）的补充人工个性智能（API）奠定基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽表现出色，但其大规模性、隐私敏感性及高计算需求导致难以满足用户的个性化定制需求。

Method: 提出个性联邦智能（PFI），结合联邦学习（FL）的隐私保护能力和基础模型（FMs）的泛化能力。

Result: 探讨了高效PFI、可信PFI及RAG赋能的PFI等机遇，并分析了未来研究方向。

Conclusion: PFI是实现人工个性智能（API）的关键技术，为AGI的补充提供了可行路径。

Abstract: The rise of large language models (LLMs), such as ChatGPT, DeepSeek, and
Grok-3, has reshaped the artificial intelligence landscape. As prominent
examples of foundational models (FMs) built on LLMs, these models exhibit
remarkable capabilities in generating human-like content, bringing us closer to
achieving artificial general intelligence (AGI). However, their large-scale
nature, sensitivity to privacy concerns, and substantial computational demands
present significant challenges to personalized customization for end users. To
bridge this gap, this paper presents the vision of artificial personalized
intelligence (API), focusing on adapting these powerful models to meet the
specific needs and preferences of users while maintaining privacy and
efficiency. Specifically, this paper proposes personalized federated
intelligence (PFI), which integrates the privacy-preserving advantages of
federated learning (FL) with the zero-shot generalization capabilities of FMs,
enabling personalized, efficient, and privacy-protective deployment at the
edge. We first review recent advances in both FL and FMs, and discuss the
potential of leveraging FMs to enhance federated systems. We then present the
key motivations behind realizing PFI and explore promising opportunities in
this space, including efficient PFI, trustworthy PFI, and PFI empowered by
retrieval-augmented generation (RAG). Finally, we outline key challenges and
future research directions for deploying FM-powered FL systems at the edge with
improved personalization, computational efficiency, and privacy guarantees.
Overall, this survey aims to lay the groundwork for the development of API as a
complement to AGI, with a particular focus on PFI as a key enabling technique.

</details>


### [229] [Causal knowledge graph analysis identifies adverse drug effects](https://arxiv.org/abs/2505.06949)
*Sumyyah Toonsi,Paul Schofield,Robert Hoehndorf*

Main category: cs.AI

TL;DR: 论文提出了一种结合知识图谱和因果模型的新方法——因果知识图谱（CKG），用于生物医学领域的因果推理，并在药物-疾病关系分析中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱缺乏概率语义，而因果模型缺乏背景知识的整合和演绎推理能力。研究旨在填补这一空白，提供一种兼具两种优势的解决方案。

Method: 论文引入了因果知识图谱（CKG），扩展了知识图谱的因果语义，支持去混淆和基于背景知识的假设制定。作者构建了药物-疾病CKG（DD-CKG），并应用于UK Biobank和MIMIC-IV队列的大规模中介分析。

Result: CKG方法成功复现了已知的药物不良反应，并识别出新的潜在副作用。通过副作用相似性分析验证了方法的临床相关性，表明其显著提升了共享药物适应症的预测能力。

Conclusion: CKG为可扩展的因果推理提供了一个通用的知识驱动框架，具有潜在的广泛应用价值。

Abstract: Knowledge graphs and structural causal models have each proven valuable for
organizing biomedical knowledge and estimating causal effects, but remain
largely disconnected: knowledge graphs encode qualitative relationships
focusing on facts and deductive reasoning without formal probabilistic
semantics, while causal models lack integration with background knowledge in
knowledge graphs and have no access to the deductive reasoning capabilities
that knowledge graphs provide. To bridge this gap, we introduce a novel
formulation of Causal Knowledge Graphs (CKGs) which extend knowledge graphs
with formal causal semantics, preserving their deductive capabilities while
enabling principled causal inference. CKGs support deconfounding via explicitly
marked causal edges and facilitate hypothesis formulation aligned with both
encoded and entailed background knowledge. We constructed a Drug-Disease CKG
(DD-CKG) integrating disease progression pathways, drug indications,
side-effects, and hierarchical disease classification to enable automated
large-scale mediation analysis. Applied to UK Biobank and MIMIC-IV cohorts, we
tested whether drugs mediate effects between indications and downstream disease
progression, adjusting for confounders inferred from the DD-CKG. Our approach
successfully reproduced known adverse drug reactions with high precision while
identifying previously undocumented significant candidate adverse effects.
Further validation through side effect similarity analysis demonstrated that
combining our predicted drug effects with established databases significantly
improves the prediction of shared drug indications, supporting the clinical
relevance of our novel findings. These results demonstrate that our methodology
provides a generalizable, knowledge-driven framework for scalable causal
inference.

</details>


### [230] [From Knowledge to Reasoning: Evaluating LLMs for Ionic Liquids Research in Chemical and Biological Engineering](https://arxiv.org/abs/2505.06964)
*Gaurab Sarkar,Sougata Saha*

Main category: cs.AI

TL;DR: 该论文评估了大型语言模型（LLMs）在化学与生物工程（CBE）领域中的推理能力，特别是针对离子液体（ILs）用于碳封存的研究。作者构建了一个专家策划的数据集（5,920个示例）并测试了三种开源LLMs，发现虽然小规模通用LLMs对ILs有一定知识，但缺乏领域特定推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在广泛的知识和推理任务中表现优异，但其在CBE领域的实用性尚不明确。该研究旨在填补这一空白，通过构建评估基准来衡量LLMs在ILs领域的知识和推理能力。

Method: 作者构建了一个包含5,920个示例的专家策划数据集，涵盖不同难度水平（语言和领域知识维度），并测试了三种参数量小于10B的开源LLMs。

Result: 测试表明，小规模通用LLMs对ILs有一定知识，但缺乏领域特定推理能力。

Conclusion: 研究讨论了如何利用LLMs推动ILs的碳捕获研究，并指出这一方向可能为碳达峰目标提供双赢解决方案。

Abstract: Although Large Language Models (LLMs) have achieved remarkable performance in
diverse general knowledge and reasoning tasks, their utility in the scientific
domain of Chemical and Biological Engineering (CBE) is unclear. Hence, it
necessitates challenging evaluation benchmarks that can measure LLM performance
in knowledge- and reasoning-based tasks, which is lacking. As a foundational
step, we empirically measure the reasoning capabilities of LLMs in CBE. We
construct and share an expert-curated dataset of 5,920 examples for
benchmarking LLMs' reasoning capabilities in the niche domain of Ionic Liquids
(ILs) for carbon sequestration, an emergent solution to reducing global
warming. The dataset presents different difficulty levels by varying along the
dimensions of linguistic and domain-specific knowledge. Benchmarking three less
than 10B parameter open-source LLMs on the dataset suggests that while smaller
general-purpose LLMs are knowledgeable about ILs, they lack domain-specific
reasoning capabilities. Based on our results, we further discuss considerations
for leveraging LLMs for carbon capture research using ILs. Since LLMs have a
high carbon footprint, gearing them for IL research can symbiotically benefit
both fields and help reach the ambitious carbon neutrality target by 2050.
Dataset link: https://github.com/sougata-ub/llms_for_ionic_liquids

</details>


### [231] [CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging](https://arxiv.org/abs/2505.06977)
*Wenju Sun,Qingyong Li,Yangli-ao Geng,Boyang Li*

Main category: cs.AI

TL;DR: 论文提出了Conflict-Aware Task Merging（CAT Merging），一种无训练框架，通过选择性修剪冲突组件来解决多任务模型合并中的知识冲突问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有任务向量累积方法在合并多任务模型时因知识冲突导致性能下降，亟需一种无需额外训练的方法来抑制冲突。

Method: 提出CAT Merging框架，采用参数特定策略修剪任务向量中的冲突组件，如线性权重的投影和归一化层参数的掩码处理。

Result: 实验表明CAT Merging在视觉、语言和视觉语言任务中平均准确率最高提升2.5%（ViT-B/32）和2.0%（ViT-L/14）。

Conclusion: CAT Merging有效解决了知识冲突问题，显著提升了多任务模型合并的性能，为无训练框架提供了新方向。

Abstract: Multi-task model merging offers a promising paradigm for integrating multiple
expert models into a unified model without additional training. Existing
state-of-the-art techniques, such as Task Arithmetic and its variants, merge
models by accumulating task vectors -- the parameter differences between
pretrained and finetuned models. However, task vector accumulation is often
hindered by knowledge conflicts, leading to performance degradation. To address
this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel
training-free framework that selectively trims conflict-prone components from
the task vectors. CAT Merging introduces several parameter-specific strategies,
including projection for linear weights and masking for scaling and shifting
parameters in normalization layers. Extensive experiments on vision, language,
and vision-language tasks demonstrate that CAT Merging effectively suppresses
knowledge conflicts, achieving average accuracy improvements of up to 2.5%
(ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.

</details>


### [232] [A Multi-Agent Reinforcement Learning Approach for Cooperative Air-Ground-Human Crowdsensing in Emergency Rescue](https://arxiv.org/abs/2505.06997)
*Wenhao Lu,Zhengqiu Zhu,Yong Zhao,Yonglin Tian,Junjie Zeng,Jun Zhang,Zhong Liu,Fei-Yue Wang*

Main category: cs.AI

TL;DR: 论文提出了一种名为HECTA4ER的多智能体强化学习算法，用于解决异构实体（如人、无人机和无人车）在紧急救援场景中的任务分配问题（HECTA），通过'Hard-Cooperative'策略和全局-局部信息融合，显著提高了任务完成率（TCR）。


<details>
  <summary>Details</summary>
Motivation: 在紧急救援等复杂环境中，传统的人类中心模型无法高效整合异构实体（如无人机和无人车）的协作感知任务分配，亟需一种能够处理部分可观测性和通信限制的优化方法。

Method: 论文将问题建模为分散部分可观测马尔可夫决策过程（Dec-POMDP），并提出了HECTA4ER算法，采用集中训练分散执行架构，结合特征提取模块、历史动作-观测利用和全局-局部信息混合网络。

Result: 仿真实验显示，HECTA4ER平均任务完成率（TCR）比基线算法提高了18.42%，并在真实案例中验证了其动态感知场景中的有效性和鲁棒性。

Conclusion: HECTA4ER算法在异构实体协作感知任务分配中表现出色，尤其适用于紧急救援等高动态环境，为实际应用提供了可靠解决方案。

Abstract: Mobile crowdsensing is evolving beyond traditional human-centric models by
integrating heterogeneous entities like unmanned aerial vehicles (UAVs) and
unmanned ground vehicles (UGVs). Optimizing task allocation among these diverse
agents is critical, particularly in challenging emergency rescue scenarios
characterized by complex environments, limited communication, and partial
observability. This paper tackles the Heterogeneous-Entity
Collaborative-Sensing Task Allocation (HECTA) problem specifically for
emergency rescue, considering humans, UAVs, and UGVs. We introduce a novel
``Hard-Cooperative'' policy where UGVs prioritize recharging low-battery UAVs,
alongside performing their sensing tasks. The primary objective is maximizing
the task completion rate (TCR) under strict time constraints. We rigorously
formulate this NP-hard problem as a decentralized partially observable Markov
decision process (Dec-POMDP) to effectively handle sequential decision-making
under uncertainty. To solve this, we propose HECTA4ER, a novel multi-agent
reinforcement learning algorithm built upon a Centralized Training with
Decentralized Execution architecture. HECTA4ER incorporates tailored designs,
including specialized modules for complex feature extraction, utilization of
action-observation history via hidden states, and a mixing network integrating
global and local information, specifically addressing the challenges of partial
observability. Furthermore, theoretical analysis confirms the algorithm's
convergence properties. Extensive simulations demonstrate that HECTA4ER
significantly outperforms baseline algorithms, achieving an average 18.42%
increase in TCR. Crucially, a real-world case study validates the algorithm's
effectiveness and robustness in dynamic sensing scenarios, highlighting its
strong potential for practical application in emergency response.

</details>


### [233] [Explainable AI the Latest Advancements and New Trends](https://arxiv.org/abs/2505.07005)
*Bowen Long,Enjie Liu,Renxi Qiu,Yanqing Duan*

Main category: cs.AI

TL;DR: 本文探讨了可信AI技术的发展，调查了全球关于AI伦理和可解释性的研究现状，并分析了AI可解释性与元推理之间的联系。


<details>
  <summary>Details</summary>
Motivation: AI技术的快速发展带来了决策不透明的问题，研究可信AI技术（尤其是可解释性）有助于满足社会和伦理需求。

Method: 通过文献调查，综合分析各国AI伦理标准和可解释性技术的现状，并探讨AI可解释性与元推理的关系。

Result: 总结了可解释AI的最新技术，并强调了元推理（即‘推理的推理’）在实现可解释性中的重要作用。

Conclusion: 未来可通过结合可解释AI与元推理技术，开发更透明、可信的AI系统。

Abstract: In recent years, Artificial Intelligence technology has excelled in various
applications across all domains and fields. However, the various algorithms in
neural networks make it difficult to understand the reasons behind decisions.
For this reason, trustworthy AI techniques have started gaining popularity. The
concept of trustworthiness is cross-disciplinary; it must meet societal
standards and principles, and technology is used to fulfill these requirements.
In this paper, we first surveyed developments from various countries and
regions on the ethical elements that make AI algorithms trustworthy; and then
focused our survey on the state of the art research into the interpretability
of AI. We have conducted an intensive survey on technologies and techniques
used in making AI explainable. Finally, we identified new trends in achieving
explainable AI. In particular, we elaborate on the strong link between the
explainability of AI and the meta-reasoning of autonomous systems. The concept
of meta-reasoning is 'reason the reasoning', which coincides with the intention
and goal of explainable Al. The integration of the approaches could pave the
way for future interpretable AI systems.

</details>


### [234] [LLM-Augmented Chemical Synthesis and Design Decision Programs](https://arxiv.org/abs/2505.07027)
*Haorui Wang,Jeff Guo,Lingkai Kong,Rampi Ramprasad,Philippe Schwaller,Yuanqi Du,Chao Zhang*

Main category: cs.AI

TL;DR: 论文探讨了利用大型语言模型（LLM）优化多步逆合成规划，提出了一种高效的路径编码方案和新搜索策略，实验表明该方法显著提升了逆合成规划效果，并扩展至可合成分子设计。


<details>
  <summary>Details</summary>
Motivation: 逆合成是有机化学和药物开发的核心，但现有机器学习方法受限于组合空间庞大，而LLM在化学决策任务中展现潜力，因此研究其能否解决多步逆合成问题。

Method: 设计了一种高效的路径编码方案，并提出了一种超越逐步反应物预测的路由级搜索策略。

Result: 综合评估表明，LLM增强方法在逆合成规划中表现优异，并可扩展至可合成分子设计。

Conclusion: LLM在多步逆合成问题中具有显著潜力，提出的方法为化学合成规划提供了新思路。

Abstract: Retrosynthesis, the process of breaking down a target molecule into simpler
precursors through a series of valid reactions, stands at the core of organic
chemistry and drug development. Although recent machine learning (ML) research
has advanced single-step retrosynthetic modeling and subsequent route searches,
these solutions remain restricted by the extensive combinatorial space of
possible pathways. Concurrently, large language models (LLMs) have exhibited
remarkable chemical knowledge, hinting at their potential to tackle complex
decision-making tasks in chemistry. In this work, we explore whether LLMs can
successfully navigate the highly constrained, multi-step retrosynthesis
planning problem. We introduce an efficient scheme for encoding reaction
pathways and present a new route-level search strategy, moving beyond the
conventional step-by-step reactant prediction. Through comprehensive
evaluations, we show that our LLM-augmented approach excels at retrosynthesis
planning and extends naturally to the broader challenge of synthesizable
molecular design.

</details>


### [235] [Efficient Fault Detection in WSN Based on PCA-Optimized Deep Neural Network Slicing Trained with GOA](https://arxiv.org/abs/2505.07030)
*Mahmood Mohassel Feghhi,Raya Majid Alsharfa,Majid Hameed Majeed*

Main category: cs.AI

TL;DR: 提出了一种结合PCA、DNN和GOA的混合方法，用于优化WSN中的故障检测，显著提高了分类准确率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法在优化DNN以处理高维数据和非线性关系时表现不佳，且收敛慢、架构优化困难，因此需要更高效的解决方案。

Method: 采用PCA降维至4个特征，用GOA优化六层DNN架构，克服反向传播的局限性。

Result: 在真实WSN数据集上达到了99.72%的分类准确率，优于传统方法。

Conclusion: 该混合方法在资源受限的WSN中具有高效性和高准确性，是故障检测领域的重大进展。

Abstract: Fault detection in Wireless Sensor Networks (WSNs) is crucial for reliable
data transmission and network longevity. Traditional fault detection methods
often struggle with optimizing deep neural networks (DNNs) for efficient
performance, especially in handling high-dimensional data and capturing
nonlinear relationships. Additionally, these methods typically suffer from slow
convergence and difficulty in finding optimal network architectures using
gradient-based optimization. This study proposes a novel hybrid method
combining Principal Component Analysis (PCA) with a DNN optimized by the
Grasshopper Optimization Algorithm (GOA) to address these limitations. Our
approach begins by computing eigenvalues from the original 12-dimensional
dataset and sorting them in descending order. The cumulative sum of these
values is calculated, retaining principal components until 99.5% variance is
achieved, effectively reducing dimensionality to 4 features while preserving
critical information. This compressed representation trains a six-layer DNN
where GOA optimizes the network architecture, overcoming backpropagation's
limitations in discovering nonlinear relationships. This hybrid PCA-GOA-DNN
framework compresses the data and trains a six-layer DNN that is optimized by
GOA, enhancing both training efficiency and fault detection accuracy. The
dataset used in this study is a real-world WSNs dataset developed by the
University of North Carolina, which was used to evaluate the proposed method's
performance. Extensive simulations demonstrate that our approach achieves a
remarkable 99.72% classification accuracy, with exceptional precision and
recall, outperforming conventional methods. The method is computationally
efficient, making it suitable for large-scale WSN deployments, and represents a
significant advancement in fault detection for resource-constrained WSNs.

</details>


### [236] [DialogueReason: Rule-Based RL Sparks Dialogue Reasoning in LLMs](https://arxiv.org/abs/2505.07049)
*Yubo Shu,Zhewei Huang,Xin Wu,Chen Hu,Shuchang Zhou,Daxin Jiang*

Main category: cs.AI

TL;DR: 论文提出了DialogueReason，一种对话式推理范式，旨在解决单风格推理模型的局限性，提升推理多样性和连贯性。通过Compound-QA任务验证单风格推理的弱点，并基于PPO训练开源LLM，在复杂问题上表现优于单风格模型。


<details>
  <summary>Details</summary>
Motivation: 现有单风格推理模型多样性低、连贯性不足，限制了推理能力。本文旨在通过对话式推理提升这些方面。

Method: 引入Compound-QA任务分析单风格推理问题，提出基于PPO和规则奖励的对话推理训练框架，并应用于开源LLM（Qwen-QWQ和Qwen-Base）。

Result: 对话推理模型在MATH、AIME和GPQA数据集上表现优于单风格模型，尤其在复杂问题中。

Conclusion: 对话推理不仅提升性能，还增强可解释性和人机交互，为多智能体系统设计提供了新思路。

Abstract: We propose DialogueReason, a reasoning paradigm that uncovers the lost roles
in monologue-style reasoning models, aiming to boost diversity and coherency of
the reasoning process. Recent advances in RL-based large reasoning models have
led to impressive long CoT capabilities and high performance on math and
science benchmarks. However, these reasoning models rely mainly on
monologue-style reasoning, which often limits reasoning diversity and
coherency, frequently recycling fixed strategies or exhibiting unnecessary
shifts in attention. Our work consists of an analysis of monologue reasoning
patterns and the development of a dialogue-based reasoning approach. We first
introduce the Compound-QA task, which concatenates multiple problems into a
single prompt to assess both diversity and coherency of reasoning. Our analysis
shows that Compound-QA exposes weaknesses in monologue reasoning, evidenced by
both quantitative metrics and qualitative reasoning traces. Building on the
analysis, we propose a dialogue-based reasoning, named DialogueReason,
structured around agents, environment, and interactions. Using PPO with
rule-based rewards, we train open-source LLMs (Qwen-QWQ and Qwen-Base) to adopt
dialogue reasoning. We evaluate trained models on MATH, AIME, and GPQA
datasets, showing that the dialogue reasoning model outperforms monologue
models under more complex compound questions. Additionally, we discuss how
dialogue-based reasoning helps enhance interpretability, facilitate more
intuitive human interaction, and inspire advances in multi-agent system design.

</details>


### [237] [Unlocking Non-Block-Structured Decisions: Inductive Mining with Choice Graphs](https://arxiv.org/abs/2505.07052)
*Humam Kourani,Gyunam Park,Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: 该论文提出了一种扩展的部分有序工作流语言（POWL），通过引入选择图（choice graphs）来解决非块结构决策点的建模问题，从而更精确地捕捉现实流程中的复杂决策逻辑。


<details>
  <summary>Details</summary>
Motivation: 现有归纳挖掘算法在处理非块结构决策点时存在局限性，无法准确反映现实流程的复杂性。论文旨在通过扩展POWL来填补这一空白。

Method: 提出了一种扩展POWL的方法，引入选择图以建模非块结构决策点，并开发了相应的归纳挖掘发现算法。

Result: 实验表明，扩展后的POWL能更精确地表示现实流程中的复杂决策行为，同时保持了归纳挖掘技术的高可扩展性。

Conclusion: 通过引入选择图扩展POWL，论文成功解决了非块结构决策点的建模问题，提升了流程发现的准确性。

Abstract: Process discovery aims to automatically derive process models from event
logs, enabling organizations to analyze and improve their operational
processes. Inductive mining algorithms, while prioritizing soundness and
efficiency through hierarchical modeling languages, often impose a strict
block-structured representation. This limits their ability to accurately
capture the complexities of real-world processes. While recent advancements
like the Partially Ordered Workflow Language (POWL) have addressed the
block-structure limitation for concurrency, a significant gap remains in
effectively modeling non-block-structured decision points. In this paper, we
bridge this gap by proposing an extension of POWL to handle
non-block-structured decisions through the introduction of choice graphs.
Choice graphs offer a structured yet flexible approach to model complex
decision logic within the hierarchical framework of POWL. We present an
inductive mining discovery algorithm that uses our extension and preserves the
quality guarantees of the inductive mining framework. Our experimental
evaluation demonstrates that the discovered models, enriched with choice
graphs, more precisely represent the complex decision-making behavior found in
real-world processes, without compromising the high scalability inherent in
inductive mining techniques.

</details>


### [238] [Arbitrarily Applicable Same/Opposite Relational Responding with NARS](https://arxiv.org/abs/2505.07079)
*Robert Johansson,Patrick Hammer,Tony Lofthouse*

Main category: cs.AI

TL;DR: 论文展示了非公理推理系统（NARS）如何通过最小训练实现随意适用的相同/相反关系响应，并扩展了其能力以支持对称和新颖关系组合的推导。实验结果显示了系统快速内化关系规则并能灵活推广的能力，与人关系学习现象相似，突显了其在人工通用智能中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索如何在计算认知架构中实现人类符号认知的基本能力——相同/相反关系响应，以增强人工智能在不确定环境下的自适应推理能力。

Method: 扩展NARS系统，引入'获得关系'的实现，并通过上下文控制的匹配到样本（MTS）程序进行最小显式训练，以支持对称（互蕴含）和新颖关系组合（组合蕴含）的推导。

Result: NARS能快速内化显式训练的关系规则，并在关键测试阶段展示基于任意上下文线索的派生关系推广能力。内部置信度指标表明了关系原则的强内化，与人关系学习现象相近。

Conclusion: 研究强调了将心理学中微妙的关系学习机制整合到人工通用智能框架中的潜力，特别是NARS所模拟的随意和上下文敏感的关系能力。

Abstract: Same/opposite relational responding, a fundamental aspect of human symbolic
cognition, allows the flexible generalization of stimulus relationships based
on minimal experience. In this study, we demonstrate the emergence of
\textit{arbitrarily applicable} same/opposite relational responding within the
Non-Axiomatic Reasoning System (NARS), a computational cognitive architecture
designed for adaptive reasoning under uncertainty. Specifically, we extend NARS
with an implementation of \textit{acquired relations}, enabling the system to
explicitly derive both symmetric (mutual entailment) and novel relational
combinations (combinatorial entailment) from minimal explicit training in a
contextually controlled matching-to-sample (MTS) procedure. Experimental
results show that NARS rapidly internalizes explicitly trained relational rules
and robustly demonstrates derived relational generalizations based on arbitrary
contextual cues. Importantly, derived relational responding in critical test
phases inherently combines both mutual and combinatorial entailments, such as
deriving same-relations from multiple explicitly trained opposite-relations.
Internal confidence metrics illustrate strong internalization of these
relational principles, closely paralleling phenomena observed in human
relational learning experiments. Our findings underscore the potential for
integrating nuanced relational learning mechanisms inspired by learning
psychology into artificial general intelligence frameworks, explicitly
highlighting the arbitrary and context-sensitive relational capabilities
modeled within NARS.

</details>


### [239] [Architectural Precedents for General Agents using Large Language Models](https://arxiv.org/abs/2505.07087)
*Robert E. Wray,James R. Kirk,John E. Laird*

Main category: cs.AI

TL;DR: 该论文总结了在预Transformer AI架构中反复出现的认知设计模式，并探讨了这些模式在基于LLM的系统（尤其是推理和交互式应用）中的体现。通过分析这些模式，可以预测当前Agentic LLM系统的不足，并为未来通用智能研究指明方向。


<details>
  <summary>Details</summary>
Motivation: 研究旨在识别和理解通用智能的机制和表征。当前，基于大语言模型（LLMs）的AI系统为探索通用智能提供了新机遇，但需要系统化分析已有认知模式在这一新领域的适用性和不足。

Method: 总结预Transformer架构中常见的认知设计模式，并分析其在基于LLMs的系统中的体现，尤其是推理和代理（Agentic）应用场景。

Result: 发现LLM系统中存在与历史认知模式相似的机制，但也揭示了当前系统的局限性，特别是在推理和代理功能上的不足。

Conclusion: 通过应用历史认知模式可指导LLMs的改进，未来研究需填补其在通用智能任务中的缺陷，尤其是对生成式基础模型的深入探索。

Abstract: One goal of AI (and AGI) is to identify and understand specific mechanisms
and representations sufficient for general intelligence. Often, this work
manifests in research focused on architectures and many cognitive architectures
have been explored in AI/AGI. However, different research groups and even
different research traditions have somewhat independently identified
similar/common patterns of processes and representations or cognitive design
patterns that are manifest in existing architectures. Today, AI systems
exploiting large language models (LLMs) offer a relatively new combination of
mechanism and representation available for exploring the possibilities of
general intelligence. In this paper, we summarize a few recurring cognitive
design patterns that have appeared in various pre-transformer AI architectures.
We then explore how these patterns are evident in systems using LLMs,
especially for reasoning and interactive ("agentic") use cases. By examining
and applying these recurring patterns, we can also predict gaps or deficiencies
in today's Agentic LLM Systems and identify likely subjects of future research
towards general intelligence using LLMs and other generative foundation models.

</details>


### [240] [RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing Framework Based on Large Language Models](https://arxiv.org/abs/2505.07089)
*Hanzheng Dai,Yuanliang Li,Zhibo Zhang,Jun Yan*

Main category: cs.AI

TL;DR: RefPentester是一种基于大型语言模型的自动化渗透测试框架，通过知识增强和自我反思机制提升效能，比GPT-4o基准模型表现高出16.7%。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型的自动化渗透测试框架在挑战性任务中表现不如人类专家，原因包括知识不平衡、短视规划以及命令生成中的幻觉问题，且缺乏从失败操作中学习的能力。

Method: 提出RefPentester框架，通过知识增强和自我反思机制辅助人类操作者识别渗透测试阶段、选择适当战术和技术、提供操作指导，并从失败中学习。渗透测试过程被建模为七状态阶段机。

Result: RefPentester成功在Hack The Box的Sau机器上揭示了凭证，表现优于GPT-4o基准模型16.7%，并在渗透测试阶段转换中展现出更高的成功率。

Conclusion: RefPentester通过知识增强和自我反思机制显著提升了自动化渗透测试的表现，显示出在实际应用中的潜力。

Abstract: Automated penetration testing (AutoPT) powered by large language models
(LLMs) has gained attention for its ability to automate ethical hacking
processes and identify vulnerabilities in target systems by leveraging the
intrinsic knowledge of LLMs. However, existing LLM-based AutoPT frameworks
often underperform compared to human experts in challenging tasks for several
reasons: the imbalanced knowledge used in LLM training, short-sighted planning
in the planning process, and hallucinations during command generation. In
addition, the penetration testing (PT) process, with its trial-and-error
nature, is limited by existing frameworks that lack mechanisms to learn from
previous failed operations, restricting adaptive improvement of PT strategies.
To address these limitations, we propose a knowledge-informed self-reflective
PT framework powered by LLMs, called RefPentester, which is an AutoPT framework
designed to assist human operators in identifying the current stage of the PT
process, selecting appropriate tactic and technique for the stage, choosing
suggested action, providing step-by-step operational guidance, and learning
from previous failed operations. We also modeled the PT process as a
seven-state Stage Machine to integrate the proposed framework effectively. The
evaluation shows that RefPentester can successfully reveal credentials on Hack
The Box's Sau machine, outperforming the baseline GPT-4o model by 16.7\%.
Across PT stages, RefPentester also demonstrates superior success rates on PT
stage transitions.

</details>


### [241] [ReCDAP: Relation-Based Conditional Diffusion with Attention Pooling for Few-Shot Knowledge Graph Completion](https://arxiv.org/abs/2505.07171)
*Jeongho Kim,Chanyeong Heo,Jaehee Jung*

Main category: cs.AI

TL;DR: 该论文提出了一种基于关系的条件扩散与注意力池化（ReCDAP）方法，用于知识图谱的补全。通过结合正负三元组信息并利用扩散过程，模型能够有效处理长尾分布的关系，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识图谱中关系的长尾分布影响了信息检索性能，现有方法仅利用正三元组或简单使用负三元组作为错误信号，未能充分利用负三元组信息。

Method: 提出ReCDAP方法，首先生成负三元组（随机替换尾实体），并在扩散过程中条件化结合正负三元组信息，利用注意力池化显式捕捉正负案例差异。

Result: 在两个广泛使用的数据集上，ReCDAP优于现有方法，实现了最先进的性能。

Conclusion: 通过条件化扩散和注意力机制，ReCDAP有效处理了知识图谱中长尾关系的补全问题，为信息检索系统提供了更强的支持。

Abstract: Knowledge Graphs (KGs), composed of triples in the form of (head, relation,
tail) and consisting of entities and relations, play a key role in information
retrieval systems such as question answering, entity search, and
recommendation. In real-world KGs, although many entities exist, the relations
exhibit a long-tail distribution, which can hinder information retrieval
performance. Previous few-shot knowledge graph completion studies focused
exclusively on the positive triple information that exists in the graph or,
when negative triples were incorporated, used them merely as a signal to
indicate incorrect triples. To overcome this limitation, we propose
Relation-Based Conditional Diffusion with Attention Pooling (ReCDAP). First,
negative triples are generated by randomly replacing the tail entity in the
support set. By conditionally incorporating positive information in the KG and
non-existent negative information into the diffusion process, the model
separately estimates the latent distributions for positive and negative
relations. Moreover, including an attention pooler enables the model to
leverage the differences between positive and negative cases explicitly.
Experiments on two widely used datasets demonstrate that our method outperforms
existing approaches, achieving state-of-the-art performance. The code is
available at https://github.com/hou27/ReCDAP-FKGC.

</details>


### [242] [Accountability of Generative AI: Exploring a Precautionary Approach for "Artificially Created Nature"](https://arxiv.org/abs/2505.07178)
*Yuri Nakao*

Main category: cs.AI

TL;DR: 本文探讨生成式AI的透明度和问责制，认为透明度虽不足以保证问责，但能促进其改进。作者提出将生成式AI视为人工创造的自然，建议采取预防原则，并呼吁建立公民参与平台以应对风险。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的快速发展，其难以追溯输出的复杂性引发了社会技术系统的问责问题。作者旨在探讨透明度与问责的关系，并提出应对生成式AI风险的新思路。

Method: 作者首先综述现有AI透明度和问责研究，指出透明度的局限性；其次提出将生成式AI视为人工创造的自然，类比自然现象的不透明性；最后提出预防原则和公民参与平台的必要性。

Result: 研究发现透明度虽不能完全解决问责问题，但能改善问责制；生成式AI的不透明性可类比自然现象，需采用预防原则；公民参与是应对风险的关键途径。

Conclusion: 生成式AI的问责需超越透明度，结合预防原则和公民参与，以应对其不透明性和潜在风险。

Abstract: The rapid development of generative artificial intelligence (AI) technologies
raises concerns about the accountability of sociotechnical systems. Current
generative AI systems rely on complex mechanisms that make it difficult for
even experts to fully trace the reasons behind the outputs. This paper first
examines existing research on AI transparency and accountability and argues
that transparency is not a sufficient condition for accountability but can
contribute to its improvement. We then discuss that if it is not possible to
make generative AI transparent, generative AI technology becomes ``artificially
created nature'' in a metaphorical sense, and suggest using the precautionary
principle approach to consider AI risks. Finally, we propose that a platform
for citizen participation is needed to address the risks of generative AI.

</details>


### [243] [Measuring General Intelligence with Generated Games](https://arxiv.org/abs/2505.07215)
*Vivek Verma,David Huang,William Chen,Dan Klein,Nicholas Tomlin*

Main category: cs.AI

TL;DR: gg-bench 是一个用于评估语言模型通用推理能力的游戏环境集合，通过生成新游戏实例和训练强化学习代理进行测试。


<details>
  <summary>Details</summary>
Motivation: 传统的静态评测基准难以全面评估语言模型的推理能力，因此作者设计了动态生成的游戏环境来更灵活地测试模型的性能。

Method: 使用大型语言模型（LLM）生成游戏描述并实现为 Gym 环境，再训练强化学习代理通过自我对抗进行评测。

Result: 当前最先进的 LLM（如 GPT-4o 和 Claude 3.7 Sonnet）在 gg-bench 上的胜率仅为 7-9%，而某些专用推理模型（如 o1、o3-mini 和 DeepSeek-R1）的胜率达到 31-36%。

Conclusion: gg-bench 提供了一种动态评测语言模型推理能力的新方法，未来可扩展并支持更多模型的研究。

Abstract: We present gg-bench, a collection of game environments designed to evaluate
general reasoning capabilities in language models. Unlike most static
benchmarks, gg-bench is a data generating process where new evaluation
instances can be generated at will. In particular, gg-bench is synthetically
generated by (1) using a large language model (LLM) to generate natural
language descriptions of novel games, (2) using the LLM to implement each game
in code as a Gym environment, and (3) training reinforcement learning (RL)
agents via self-play on the generated games. We evaluate language models by
their winrate against these RL agents by prompting models with the game
description, current board state, and a list of valid moves, after which models
output the moves they wish to take. gg-bench is challenging: state-of-the-art
LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench
using in-context learning, while reasoning models such as o1, o3-mini and
DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games,
data generation process, and evaluation code in order to support future
modeling work and expansion of our benchmark.

</details>


### [244] [Interpretable Event Diagnosis in Water Distribution Networks](https://arxiv.org/abs/2505.07299)
*André Artelt,Stelios G. Vrachimis,Demetrios G. Eliades,Ulrike Kuhl,Barbara Hammer,Marios M. Polycarpou*

Main category: cs.AI

TL;DR: 本文提出了一种可解释的事件诊断框架，通过提供反事实解释帮助操作员理解算法结果，并结合自身经验做出更明智的决策。


<details>
  <summary>Details</summary>
Motivation: 尽管信息与通信技术在水系统监测中的应用日益广泛，但数据驱动的方法结果并不总是准确，且操作员更依赖自身经验。本文旨在通过可解释的方法弥合算法与操作员直觉之间的差距。

Method: 提出了一种基于反事实事件指纹的方法，通过图形化展示当前事件诊断与最近替代解释之间的差异，增强操作员对算法内部逻辑的理解。

Result: 该方法在L-Town基准测试中进行了应用和评估，证明了其有效性。

Conclusion: 反事实解释框架能够有效提升操作员对算法诊断结果的信任和决策质量，为数据驱动方法在实际应用中的推广提供了支持。

Abstract: The increasing penetration of information and communication technologies in
the design, monitoring, and control of water systems enables the use of
algorithms for detecting and identifying unanticipated events (such as leakages
or water contamination) using sensor measurements. However, data-driven
methodologies do not always give accurate results and are often not trusted by
operators, who may prefer to use their engineering judgment and experience to
deal with such events.
  In this work, we propose a framework for interpretable event diagnosis -- an
approach that assists the operators in associating the results of algorithmic
event diagnosis methodologies with their own intuition and experience. This is
achieved by providing contrasting (i.e., counterfactual) explanations of the
results provided by fault diagnosis algorithms; their aim is to improve the
understanding of the algorithm's inner workings by the operators, thus enabling
them to take a more informed decision by combining the results with their
personal experiences. Specifically, we propose counterfactual event
fingerprints, a representation of the difference between the current event
diagnosis and the closest alternative explanation, which can be presented in a
graphical way. The proposed methodology is applied and evaluated on a realistic
use case using the L-Town benchmark.

</details>


### [245] [FedIFL: A federated cross-domain diagnostic framework for motor-driven systems with inconsistent fault modes](https://arxiv.org/abs/2505.07315)
*Zexiao Wang,Yankai Wang,Xiaoqiang Liao,Xinguo Ming,Weiming Shen*

Main category: cs.AI

TL;DR: 提出FedIFL框架，解决联邦学习中标签空间不一致导致的模型泛化问题，通过特征解耦和对比学习提升全局模型的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 工业数据稀缺且标签空间不一致，传统联邦学习难以保证全局模型的泛化能力，需解决跨客户端特征一致性问题。

Method: 采用原型对比学习和特征生成解决客户端内部域偏移，通过特征解耦机制和一致性损失优化跨客户端特征表示。

Result: 在真实MDS数据上验证，FedIFL显著提升跨域故障诊断的准确性和泛化能力。

Conclusion: FedIFL有效解决了标签空间不一致问题，为联邦学习在工业故障诊断中的应用提供了新思路。

Abstract: Due to the scarcity of industrial data, individual equipment users,
particularly start-ups, struggle to independently train a comprehensive fault
diagnosis model; federated learning enables collaborative training while
ensuring data privacy, making it an ideal solution. However, the diversity of
working conditions leads to variations in fault modes, resulting in
inconsistent label spaces across different clients. In federated diagnostic
scenarios, label space inconsistency leads to local models focus on
client-specific fault modes and causes local models from different clients to
map different failure modes to similar feature representations, which weakens
the aggregated global model's generalization. To tackle this issue, this
article proposed a federated cross-domain diagnostic framework termed Federated
Invariant Features Learning (FedIFL). In intra-client training, prototype
contrastive learning mitigates intra-client domain shifts, subsequently,
feature generating ensures local models can access distributions of other
clients in a privacy-friendly manner. Besides, in cross-client training, a
feature disentanglement mechanism is introduced to mitigate cross-client domain
shifts, specifically, an instance-level federated instance consistency loss is
designed to ensure the instance-level consistency of invariant features between
different clients, furthermore, a federated instance personalization loss and
an orthogonal loss are constructed to distinguish specific features that from
the invariant features. Eventually, the aggregated model achieves promising
generalization among global label spaces, enabling accurate fault diagnosis for
target clients' Motor Driven Systems (MDSs) with inconsistent label spaces.
Experiments on real-world MDSs validate the effectiveness and superiority of
FedIFL in federated cross-domain diagnosis with inconsistent fault modes.

</details>


### [246] [AIS Data-Driven Maritime Monitoring Based on Transformer: A Comprehensive Review](https://arxiv.org/abs/2505.07374)
*Zhiye Xie,Enmei Tu,Xianping Fu,Guoliang Yuan,Yi Han*

Main category: cs.AI

TL;DR: 本文综述了基于Transformer模型的AIS数据在海洋监测中的应用，重点探讨了轨迹预测、行为检测与预测技术，并整理了公开的AIS数据集，为未来研究提供了数据支持与方向建议。


<details>
  <summary>Details</summary>
Motivation: 随着全球航运对安全、效率和可持续性的需求增加，AIS数据的潜力因规模庞大未被充分挖掘。Transformer模型因其强大的序列建模能力（如捕捉长距离依赖和复杂时序动态）成为处理AIS数据的有效工具。

Method: 回顾了Transformer在海洋领域的应用，聚焦轨迹预测、行为检测与预测技术；整理了公开AIS数据集，进行了数据过滤、清洗与统计分析，揭示不同船型的操作特征。

Result: 统计结果展示了各类船只的运行特点，为海洋监测任务提供了数据集（如GitHub链接），并提出两个未来研究方向。

Conclusion: Transformer在AIS数据驱动的海洋监测中表现优异，数据整理与分析为后续研究奠定基础；未来可探索更高效的模型或跨领域应用。

Abstract: With the increasing demands for safety, efficiency, and sustainability in
global shipping, Automatic Identification System (AIS) data plays an
increasingly important role in maritime monitoring. AIS data contains
spatial-temporal variation patterns of vessels that hold significant research
value in the marine domain. However, due to its massive scale, the full
potential of AIS data has long remained untapped. With its powerful sequence
modeling capabilities, particularly its ability to capture long-range
dependencies and complex temporal dynamics, the Transformer model has emerged
as an effective tool for processing AIS data. Therefore, this paper reviews the
research on Transformer-based AIS data-driven maritime monitoring, providing a
comprehensive overview of the current applications of Transformer models in the
marine field. The focus is on Transformer-based trajectory prediction methods,
behavior detection, and prediction techniques. Additionally, this paper
collects and organizes publicly available AIS datasets from the reviewed
papers, performing data filtering, cleaning, and statistical analysis. The
statistical results reveal the operational characteristics of different vessel
types, providing data support for further research on maritime monitoring
tasks. Finally, we offer valuable suggestions for future research, identifying
two promising research directions. Datasets are available at
https://github.com/eyesofworld/Maritime-Monitoring.

</details>


### [247] [How well do LLMs reason over tabular data, really?](https://arxiv.org/abs/2505.07453)
*Cornelius Wolff,Madelon Hulsebos*

Main category: cs.AI

TL;DR: 论文探讨了通用大语言模型（LLMs）在表格数据上的推理能力，指出现有评估方法的不足，并提出LLM-as-a-judge方法更可靠；实验表明LLMs对表格输入的常见变化（如缺失值、重复实体等）表现脆弱。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在表格数据上的真实推理能力，尤其是其对现实世界中表格输入变化（如缺失值、重复实体等）的鲁棒性，以及如何更准确地评估其性能。

Method: 基于现有表格推理基准，分析其多选提示评估策略的局限性，提出LLM-as-a-judge评估方法，并扩展表格输入以模拟现实中的三种常见变化。

Result: 实验显示LLMs在表格推理能力上存在显著缺陷，且对输入变化（如缺失值、重复实体等）表现脆弱。

Conclusion: LLMs在表格数据推理上的鲁棒性不足，需改进以适应现实场景。

Abstract: Large Language Models (LLMs) excel in natural language tasks, but less is
known about their reasoning capabilities over tabular data. Prior analyses
devise evaluation strategies that poorly reflect an LLM's realistic performance
on tabular queries. Moreover, we have a limited understanding of the robustness
of LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can
general-purpose LLMs reason over tabular data, really?, and focus on two
questions 1) are tabular reasoning capabilities of general-purpose LLMs robust
to real-world characteristics of tabular inputs, and 2) how can we
realistically evaluate an LLM's performance on analytical tabular queries?
Building on a recent tabular reasoning benchmark, we first surface shortcomings
of its multiple-choice prompt evaluation strategy, as well as commonly used
free-form text metrics such as SacreBleu and BERT-score. We show that an
LLM-as-a-judge procedure yields more reliable performance insights and unveil a
significant deficit in tabular reasoning performance of LLMs. We then extend
the tabular inputs reflecting three common characteristics in practice: 1)
missing values, 2) duplicate entities, and 3) structural variations.
Experiments show that the tabular reasoning capabilities of general-purpose
LLMs suffer from these variations, stressing the importance of improving their
robustness for realistic tabular inputs.

</details>


### [248] [A Survey on Collaborative Mechanisms Between Large and Small Language Models](https://arxiv.org/abs/2505.07460)
*Yi Chen,JiaHao Zhao,HaoHao Han*

Main category: cs.AI

TL;DR: 该论文探讨了大型语言模型（LLM）和小型语言模型（SLM）协作的重要性，总结了交互机制、关键技术及应用场景，并指出未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 解决LLM资源消耗高、延迟大的问题，同时利用SLM的高效性，推动资源受限设备上的AI应用。

Method: 综述了LLM-SLM协作的五种机制（流水线、路由、辅助、蒸馏、融合）及关键技术。

Result: 协作框架在延迟、隐私等需求下展现出潜力，但仍需克服系统开销、评估复杂度等挑战。

Conclusion: LLM-SLM协作是未来实用AI的关键方向，需进一步研究自适应框架和多模态扩展。

Abstract: Large Language Models (LLMs) deliver powerful AI capabilities but face
deployment challenges due to high resource costs and latency, whereas Small
Language Models (SLMs) offer efficiency and deployability at the cost of
reduced performance. Collaboration between LLMs and SLMs emerges as a crucial
paradigm to synergistically balance these trade-offs, enabling advanced AI
applications, especially on resource-constrained edge devices. This survey
provides a comprehensive overview of LLM-SLM collaboration, detailing various
interaction mechanisms (pipeline, routing, auxiliary, distillation, fusion),
key enabling technologies, and diverse application scenarios driven by
on-device needs like low latency, privacy, personalization, and offline
operation. While highlighting the significant potential for creating more
efficient, adaptable, and accessible AI, we also discuss persistent challenges
including system overhead, inter-model consistency, robust task allocation,
evaluation complexity, and security/privacy concerns. Future directions point
towards more intelligent adaptive frameworks, deeper model fusion, and
expansion into multimodal and embodied AI, positioning LLM-SLM collaboration as
a key driver for the next generation of practical and ubiquitous artificial
intelligence.

</details>


### [249] [Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks](https://arxiv.org/abs/2505.07473)
*Kai Xu,YiWei Mao,XinYi Guan,ZiLong Feng*

Main category: cs.AI

TL;DR: 该论文提出了一个新的基准测试Web-Bench，旨在解决现有代码生成基准测试饱和问题。Web-Bench包含50个项目，模拟真实开发流程，挑战性强。当前最优模型Claude 3.7 Sonnet在Web-Bench上的Pass@1仅为25.1%，显著低于其他基准测试结果。


<details>
  <summary>Details</summary>
Motivation: 早期代码生成基准测试（如HumanEval、MBPP）已接近饱和，失去了对大型语言模型（LLMs）的指导作用。为了解决这一问题，研究者提出了基于软件工程的新基准测试。但现有软件工程基准测试的饱和速度也在加快，因此需要更具挑战性和真实性的测试环境。

Method: 设计Web-Bench基准测试，包含50个项目，每个项目由20个具有顺序依赖的任务组成，模拟真实的人类开发流程。项目由经验丰富的工程师设计，涵盖Web开发的基础元素（Web标准和框架），每个项目耗时4-8小时。通过在Web-Agent上测试SOTA模型（如Claude 3.7 Sonnet）来评估性能。

Result: 现有最优模型Claude 3.7 Sonnet在Web-Bench上的Pass@1仅为25.1%，显著低于SWE-Bench的Verified（65.4%）和Full（33.8%）分数，表明Web-Bench的挑战性更高。

Conclusion: 论文提出，在任何开发领域中，标准和框架分别代表基础知识和效率工具，而LLMs需要针对它们进行优化。Web-Bench的推出为评估LLMs在复杂真实场景中的表现提供了新的平台。

Abstract: The application of large language models (LLMs) in the field of coding is
evolving rapidly: from code assistants, to autonomous coding agents, and then
to generating complete projects through natural language. Early LLM code
benchmarks primarily focused on code generation accuracy, but these benchmarks
have gradually become saturated. Benchmark saturation weakens their guiding
role for LLMs. For example, HumanEval Pass@1 has reached 99.4% and MBPP 94.2%.
Among various attempts to address benchmark saturation, approaches based on
software engineering have stood out, but the saturation of existing software
engineering benchmarks is rapidly increasing. To address this, we propose a new
benchmark, Web-Bench, which contains 50 projects, each consisting of 20 tasks
with sequential dependencies. The tasks implement project features in sequence,
simulating real-world human development workflows. When designing Web-Bench, we
aim to cover the foundational elements of Web development: Web Standards and
Web Frameworks. Given the scale and complexity of these projects, which were
designed by engineers with 5 to 10 years of experience, each presents a
significant challenge. On average, a single project takes 4 to 8 hours for a
senior engineer to complete. On our given benchmark agent (Web-Agent), SOTA
(Claude 3.7 Sonnet) achieves only 25.1% Pass@1, significantly lower (better)
than SWE-Bench's Verified (65.4%) and Full (33.8%) scores. Finally, we discuss
that in any development field, Standards and Frameworks represent foundational
knowledge and efficiency tools, respectively, and LLMs require optimization
tailored to them.

</details>


### [250] [HALO: Half Life-Based Outdated Fact Filtering in Temporal Knowledge Graphs](https://arxiv.org/abs/2505.07509)
*Feng Ding,Tingting Wang,Yupeng Gao,Shuo Yu,Jing Ren,Feng Xia*

Main category: cs.AI

TL;DR: 论文提出了一个名为HALO的框架，用于过滤时间知识图谱中的过时事实，通过半衰期理论量化历史事实的时间有效性，提高推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注历史事实的积极影响，而忽略了过时事实的负面影响，同时训练这些事实增加了计算成本。

Method: HALO框架包括三个模块：时间事实注意力模块捕捉历史事实的时间演变，动态关系感知编码器模块预测事实半衰期，过时事实过滤模块基于半衰期理论构建时间衰减函数来过滤过时事实。

Result: 实验结果表明，HALO在三个公共数据集上优于最先进的时间知识图谱推理方法。

Conclusion: HALO框架有效地检测和过滤过时事实，提高了时间知识图谱的推理性能。

Abstract: Outdated facts in temporal knowledge graphs (TKGs) result from exceeding the
expiration date of facts, which negatively impact reasoning performance on
TKGs. However, existing reasoning methods primarily focus on positive
importance of historical facts, neglecting adverse effects of outdated facts.
Besides, training on these outdated facts yields extra computational cost. To
address these challenges, we propose an outdated fact filtering framework named
HALO, which quantifies the temporal validity of historical facts by exploring
the half-life theory to filter outdated facts in TKGs. HALO consists of three
modules: the temporal fact attention module, the dynamic relation-aware encoder
module, and the outdated fact filtering module. Firstly, the temporal fact
attention module captures the evolution of historical facts over time to
identify relevant facts. Secondly, the dynamic relation-aware encoder module is
designed for efficiently predicting the half life of each fact. Finally, we
construct a time decay function based on the half-life theory to quantify the
temporal validity of facts and filter outdated facts. Experimental results show
that HALO outperforms the state-of-the-art TKG reasoning methods on three
public datasets, demonstrating its effectiveness in detecting and filtering
outdated facts (Codes are available at
https://github.com/yushuowiki/K-Half/tree/main ).

</details>


### [251] [QuantX: A Framework for Hardware-Aware Quantization of Generative AI Workloads](https://arxiv.org/abs/2505.07531)
*Khurram Mazher,Saad Bin Nasir*

Main category: cs.AI

TL;DR: QuantX是一种针对LLM和VLM量身定制的量化方法套件，支持最低3比特的量化，性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型（LLM）和视觉语言模型（VLM）在量化过程中性能损失的问题，提供高效的量化策略并考虑硬件约束。

Method: 采用硬件感知的量化策略，支持动态量化和解量化，以平衡运行速度、内存需求和模型精度。

Result: QuantX在3比特量化下，性能损失仅为6%以内，并在多项任务中优于现有量化技术。

Conclusion: QuantX通过优化量化策略，实现了高效、低损失的模型量化，为LLM和VLM的量化提供了新思路。

Abstract: We present QuantX: a tailored suite of recipes for LLM and VLM quantization.
It is capable of quantizing down to 3-bit resolutions with minimal loss in
performance. The quantization strategies in QuantX take into account
hardware-specific constraints to achieve efficient dequantization during
inference ensuring flexible trade-off between runtime speed, memory requirement
and model accuracy. Our results demonstrate that QuantX achieves performance
within 6% of the unquantized model for LlaVa-v1.6 quantized down to 3-bits for
multiple end user tasks and outperforms recently published state-of-the-art
quantization techniques. This manuscript provides insights into the LLM
quantization process that motivated the range of recipes and options that are
incorporated in QuantX.

</details>


### [252] [YuLan-OneSim: Towards the Next Generation of Social Simulator with Large Language Models](https://arxiv.org/abs/2505.07581)
*Lei Wang,Heyang Gao,Xiaohe Bo,Xu Chen,Ji-Rong Wen*

Main category: cs.AI

TL;DR: 本文介绍了一个名为YuLan-OneSim的新型社交模拟器，具备无代码场景构建、全面默认场景、可进化模拟、大规模模拟和AI社交研究者五大特点，并通过实验验证其优势。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型（LLM）代理模拟人类社交行为，旨在降低编程门槛、覆盖多领域研究需求并提升模拟质量。

Method: 开发YuLan-OneSim模拟器，支持自然语言交互生成代码，内置多领域场景，支持外部反馈优化LLM，采用分布式架构支持大规模代理，并集成AI社交研究流程自动化。

Result: 实验表明YuLan-OneSim在场景生成质量、模拟可靠性、效率和扩展性方面表现优异，AI社交研究者能高效完成研究闭环。

Conclusion: YuLan-OneSim通过技术创新显著提升了社交模拟的易用性和研究效率，为社会科学研究提供了新工具。

Abstract: Leveraging large language model (LLM) based agents to simulate human social
behaviors has recently gained significant attention. In this paper, we
introduce a novel social simulator called YuLan-OneSim. Compared to previous
works, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free
scenario construction: Users can simply describe and refine their simulation
scenarios through natural language interactions with our simulator. All
simulation code is automatically generated, significantly reducing the need for
programming expertise. (2) Comprehensive default scenarios: We implement 50
default simulation scenarios spanning 8 domains, including economics,
sociology, politics, psychology, organization, demographics, law, and
communication, broadening access for a diverse range of social researchers. (3)
Evolvable simulation: Our simulator is capable of receiving external feedback
and automatically fine-tuning the backbone LLMs, significantly enhancing the
simulation quality. (4) Large-scale simulation: By developing a fully
responsive agent framework and a distributed simulation architecture, our
simulator can handle up to 100,000 agents, ensuring more stable and reliable
simulation results. (5) AI social researcher: Leveraging the above features, we
develop an AI social researcher. Users only need to propose a research topic,
and the AI researcher will automatically analyze the input, construct
simulation environments, summarize results, generate technical reports, review
and refine the reports--completing the social science research loop. To
demonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate
the quality of the automatically generated scenarios, the reliability,
efficiency, and scalability of the simulation process, as well as the
performance of the AI social researcher.

</details>


### [253] [S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models](https://arxiv.org/abs/2505.07686)
*Muzhi Dai,Chenxu Yang,Qingyi Si*

Main category: cs.AI

TL;DR: 本文提出了一种名为S-GRPO的新型强化学习方法，通过调节思维链（CoT）生成的中间步骤减少冗余，从而实现更早退出思考，并在多个基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的强化学习方法忽视了中间推理步骤的调节，导致思维链生成时出现冗余思考。本文旨在通过新的强化学习方法优化这一过程。

Method: 提出了Serial-Group Decaying-Reward Policy Optimization（S-GRPO），通过在单一CoT生成中选择多个时间点允许模型思考退出，并对早期正确的答案分配衰减奖励。

Result: 在Qwen3和Deepseek-distill等多个模型上测试，实现了35.4%至61.1%的序列长度减少和0.72%至6.08%的准确率提升。

Conclusion: S-GRPO能够有效减少推理步骤冗余，提升模型效率和准确性，适用于先进的推理模型。

Abstract: As Test-Time Scaling emerges as an active research focus in the large
language model community, advanced post-training methods increasingly emphasize
extending chain-of-thought (CoT) generation length, thereby enhancing reasoning
capabilities to approach Deepseek R1-like reasoning models. However, recent
studies reveal that reasoning models (even Qwen3) consistently exhibit
excessive thought redundancy in CoT generation. This overthinking problem stems
from conventional outcome-reward reinforcement learning's systematic neglect in
regulating intermediate reasoning steps. This paper proposes Serial-Group
Decaying-Reward Policy Optimization (namely S-GRPO), a novel reinforcement
learning method that empowers models with the capability to determine the
sufficiency of reasoning steps, subsequently triggering early exit of CoT
generation. Specifically, unlike GRPO, which samples multiple possible
completions (parallel group) in parallel, we select multiple temporal positions
in the generation of one CoT to allow the model to exit thinking and instead
generate answers (serial group), respectively. For the correct answers in a
serial group, we assign rewards that decay according to positions, with lower
rewards towards the later ones, thereby reinforcing the model's behavior to
generate higher-quality answers at earlier phases with earlier exits of
thinking. Empirical evaluations demonstrate compatibility with state-of-the-art
reasoning models, including Qwen3 and Deepseek-distill models, achieving 35.4%
~ 61.1\% sequence length reduction with 0.72% ~ 6.08% accuracy improvements
across GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond benchmarks.

</details>


### [254] [Belief Injection for Epistemic Control in Linguistic State Space](https://arxiv.org/abs/2505.07693)
*Sebastian Dumbrava*

Main category: cs.AI

TL;DR: 介绍了信念注入（belief injection），一种针对人工智能代理的前瞻性认知控制机制，通过语言信念片段直接影响其推理和行为对齐。


<details>
  <summary>Details</summary>
Motivation: 旨在通过主动而非反应性的方式，动态调整AI代理的认知状态，提升其推理和对齐能力。

Method: 基于语义流形框架（Semantic Manifold），提出多种注入策略（如直接、上下文感知、目标导向和反思性注入），并与信念过滤等机制对比。

Result: 探讨了实践应用、实施考量及伦理影响，并展望了未来认知治理的研究方向。

Conclusion: 信念嵌入（belief injection）作为架构化认知治理工具，具有潜力和研究价值。

Abstract: This work introduces belief injection, a proactive epistemic control
mechanism for artificial agents whose cognitive states are structured as
dynamic ensembles of linguistic belief fragments. Grounded in the Semantic
Manifold framework, belief injection directly incorporates targeted linguistic
beliefs into an agent's internal cognitive state, influencing reasoning and
alignment proactively rather than reactively. We delineate various injection
strategies, such as direct, context-aware, goal-oriented, and reflective
approaches, and contrast belief injection with related epistemic control
mechanisms, notably belief filtering. Additionally, this work discusses
practical applications, implementation considerations, ethical implications,
and outlines promising directions for future research into cognitive governance
using architecturally embedded belief injection.

</details>


### [255] [Emotion-Gradient Metacognitive RSI (Part I): Theoretical Foundations and Single-Agent Architecture](https://arxiv.org/abs/2505.07757)
*Rintaro Ando*

Main category: cs.AI

TL;DR: EG-MRSI框架融合元认知与情绪驱动机制，支持递归自修改，提出可微分内在奖励函数与安全机制，为开放式AGI提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有递归自改进系统在安全性与语义学习量化上的不足，提出情绪梯度与元认知结合的创新框架。

Method: 基于N2M-RSI，引入情绪梯度动态、内在奖励函数及安全约束的自修改算子，定义强化学习兼容的优化目标。

Result: 提出可量化语义学习的指标（意义密度与转换效率），初步建立单代理理论体系。

Conclusion: EG-MRSI为安全、开放的AGI奠定基础，后续将扩展至安全协议、群体智能及物理限制等方向。

Abstract: We present the Emotion-Gradient Metacognitive Recursive Self-Improvement
(EG-MRSI) framework, a novel architecture that integrates introspective
metacognition, emotion-based intrinsic motivation, and recursive
self-modification into a unified theoretical system. The framework is
explicitly capable of overwriting its own learning algorithm under formally
bounded risk. Building upon the Noise-to-Meaning RSI (N2M-RSI) foundation,
EG-MRSI introduces a differentiable intrinsic reward function driven by
confidence, error, novelty, and cumulative success. This signal regulates both
a metacognitive mapping and a self-modification operator constrained by
provable safety mechanisms. We formally define the initial agent configuration,
emotion-gradient dynamics, and RSI trigger conditions, and derive a
reinforcement-compatible optimization objective that guides the agent's
development trajectory. Meaning Density and Meaning Conversion Efficiency are
introduced as quantifiable metrics of semantic learning, closing the gap
between internal structure and predictive informativeness. This Part I paper
establishes the single-agent theoretical foundations of EG-MRSI. Future parts
will extend this framework to include safety certificates and rollback
protocols (Part II), collective intelligence mechanisms (Part III), and
feasibility constraints including thermodynamic and computational limits (Part
IV). Together, the EG-MRSI series provides a rigorous, extensible foundation
for open-ended and safe AGI.

</details>


### [256] ["I Apologize For Not Understanding Your Policy": Exploring the Specification and Evaluation of User-Managed Access Control Policies by AI Virtual Assistants](https://arxiv.org/abs/2505.07759)
*Jennifer Mondragon,Carlos Rubio-Medrano,Gael Cruz,Dvijesh Shastri*

Main category: cs.AI

TL;DR: 论文探讨了AI虚拟助手在管理用户自主访问控制策略（U-MAPs）时的有效性，发现现有助手在不同场景中表现不佳，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI虚拟助手的普及，用户通过其管理敏感数据和设备功能的需求日益增长，但如何有效管理用户自主访问控制策略（U-MAPs）以防止安全漏洞和隐私泄露是一个关键挑战。

Method: 通过从非结构化到结构化的测试，评估了当前公开可用的虚拟助手在不同U-MAP场景下的理解能力。

Result: 研究发现现有虚拟助手在理解多样化的U-MAP策略方面存在明显不足。

Conclusion: 研究不仅揭示了虚拟助手的关键局限，还为如何改进以管理复杂授权规则和适应动态变化提供了宝贵见解。

Abstract: The rapid evolution of Artificial Intelligence (AI)-based Virtual Assistants
(VAs) e.g., Google Gemini, ChatGPT, Microsoft Copilot, and High-Flyer Deepseek
has turned them into convenient interfaces for managing emerging technologies
such as Smart Homes, Smart Cars, Electronic Health Records, by means of
explicit commands,e.g., prompts, which can be even launched via voice, thus
providing a very convenient interface for end-users. However, the proper
specification and evaluation of User-Managed Access Control Policies (U-MAPs),
the rules issued and managed by end-users to govern access to sensitive data
and device functionality - within these VAs presents significant challenges,
since such a process is crucial for preventing security vulnerabilities and
privacy leaks without impacting user experience. This study provides an initial
exploratory investigation on whether current publicly-available VAs can manage
U-MAPs effectively across differing scenarios. By conducting unstructured to
structured tests, we evaluated the comprehension of such VAs, revealing a lack
of understanding in varying U-MAP approaches. Our research not only identifies
key limitations, but offers valuable insights into how VAs can be further
improved to manage complex authorization rules and adapt to dynamic changes.

</details>


### [257] [Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving](https://arxiv.org/abs/2505.07773)
*Xinji Mai,Haotian Xu,Xing W,Weinong Wang,Yingying Zhang,Wenqiang Zhang*

Main category: cs.AI

TL;DR: 本文探讨了利用基于结果的强化学习（RL）训练大型语言模型（LLMs）自主生成并执行Python代码以解决数学问题的方法（ZeroTIR），展示了训练步数与代码执行频率、回答长度及任务准确率之间的正相关关系。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决LLMs在需要精确计算的数学推理任务中的不足，探索如何通过RL自主学习和利用外部工具（如代码执行）来增强推理能力。

Method: 方法采用基于结果的RL训练基础LLMs（ZeroTIR），无需监督的工具使用示例，自动生成和执行Python代码，并在解耦的代码执行环境中验证。

Result: 实验结果表明，ZeroTIR在数学基准测试中显著优于不使用工具的ZeroRL基线，且训练步数与工具使用效果呈正相关。

Conclusion: 结论指出，该方法为自主工具使用的学习与扩展提供了量化基础，并为未来研究提供了可复现的基准。

Abstract: Large Language Models (LLMs) often struggle with mathematical reasoning tasks
requiring precise, verifiable computation. While Reinforcement Learning (RL)
from outcome-based rewards enhances text-based reasoning, understanding how
agents autonomously learn to leverage external tools like code execution
remains crucial. We investigate RL from outcome-based rewards for
Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously
generate and execute Python code for mathematical problems without supervised
tool-use examples. Our central contribution is we demonstrate that as RL
training progresses, key metrics scale predictably. Specifically, we observe
strong positive correlations where increased training steps lead to increases
in the spontaneous code execution frequency, the average response length, and,
critically, the final task accuracy. This suggests a quantifiable relationship
between computational effort invested in training and the emergence of
effective, tool-augmented reasoning strategies. We implement a robust framework
featuring a decoupled code execution environment and validate our findings
across standard RL algorithms and frameworks. Experiments show ZeroTIR
significantly surpasses non-tool ZeroRL baselines on challenging math
benchmarks. Our findings provide a foundational understanding of how autonomous
tool use is acquired and scales within Agent RL, offering a reproducible
benchmark for future studies. Code is released at
\href{https://github.com/Anonymize-Author/AgentRL}{https://github.com/Anonymize-Author/AgentRL}.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [258] [Empirical Analysis of Asynchronous Federated Learning on Heterogeneous Devices: Efficiency, Fairness, and Privacy Trade-offs](https://arxiv.org/abs/2505.07041)
*Samaneh Mohammadi,Iraklis Symeonidis,Ali Balador,Francesco Flammini*

Main category: cs.DC

TL;DR: 该研究分析了联邦学习中设备异构性对效率、公平性和隐私的影响，比较了同步和异步方法的优劣，强调了自适应协议的必要性。


<details>
  <summary>Details</summary>
Motivation: 设备异构性导致联邦学习中资源受限的客户端拖慢同步方案，而异步方法虽然提高了效率，但其隐私代价未被充分研究，尤其是高端设备的隐私损失更大。

Method: 通过物理测试床（五台边缘设备）比较FedAvg和FedAsync方法，结合本地差分隐私和时刻统计量化隐私损失。

Result: 异步方法收敛速度提高10倍，但高端设备的隐私损失增加5倍，低端设备因更新频率低和噪声干扰导致准确率下降。

Conclusion: 研究呼吁开发自适应联邦学习协议，以根据客户端能力和参与动态优化聚合和隐私机制，取代静态统一方案。

Abstract: Device heterogeneity poses major challenges in Federated Learning (FL), where
resource-constrained clients slow down synchronous schemes that wait for all
updates before aggregation. Asynchronous FL addresses this by incorporating
updates as they arrive, substantially improving efficiency. While its
efficiency gains are well recognized, its privacy costs remain largely
unexplored, particularly for high-end devices that contribute updates more
frequently, increasing their cumulative privacy exposure. This paper presents
the first comprehensive analysis of the efficiency-fairness-privacy trade-off
in synchronous vs. asynchronous FL under realistic device heterogeneity. We
empirically compare FedAvg and staleness-aware FedAsync using a physical
testbed of five edge devices spanning diverse hardware tiers, integrating Local
Differential Privacy (LDP) and the Moments Accountant to quantify per-client
privacy loss. Using Speech Emotion Recognition (SER) as a privacy-critical
benchmark, we show that FedAsync achieves up to 10x faster convergence but
exacerbates fairness and privacy disparities: high-end devices contribute 6-10x
more updates and incur up to 5x higher privacy loss, while low-end devices
suffer amplified accuracy degradation due to infrequent, stale, and
noise-perturbed updates. These findings motivate the need for adaptive FL
protocols that jointly optimize aggregation and privacy mechanisms based on
client capacity and participation dynamics, moving beyond static,
one-size-fits-all solutions.

</details>


### [259] [Challenging GPU Dominance: When CPUs Outperform for On-Device LLM Inference](https://arxiv.org/abs/2505.06461)
*Haolin Zhang,Jeff Huang*

Main category: cs.DC

TL;DR: 研究发现，在特定条件下，如iPhone 15 Pro上部署1-billion-parameter LLM时，CPU推理（双线程，F16精度）的性能（17 tokens/s）优于GPU加速（12.8 tokens/s）。内存传输开销和线程优化是关键因素，挑战了GPU优先的常规思维。


<details>
  <summary>Details</summary>
Motivation: 通过实证研究挑战了GPU在移动设备LLM推理中性能最优的普遍假设。

Method: 使用llama.cpp在iPhone 15 Pro上部署1-billion-parameter LLM，对比CPU（双线程，F16）和GPU的性能。

Result: CPU配置（17 tokens/s, two threads, F16）优于GPU加速（12.8 tokens/s）。内存传输开销和CPU线程优化是主要原因。

Conclusion: 优化CPU推理具备潜力，需重新思考移动AI部署策略。但因iOS底层工具限制，CPU优势的完整解释仍有难度。

Abstract: The common assumption in on-device AI is that GPUs, with their superior
parallel processing, always provide the best performance for large language
model (LLM) inference. In this work, we challenge this notion by empirically
demonstrating that, under certain conditions, CPUs can outperform GPUs for LLM
inference on mobile devices. Using a 1-billion-parameter LLM deployed via
llama.cpp on the iPhone 15 Pro, we show that a CPU-only configuration (two
threads, F16 precision) achieves 17 tokens per second, surpassing the 12.8
tokens per second obtained with GPU acceleration. We analyze the architectural
factors driving this counterintuitive result, revealing that GPU memory
transfer overhead and CPU thread optimization play a critical role.
Furthermore, we explore the impact of thread oversubscription, quantization
strategies, and hardware constraints, providing new insights into efficient
on-device AI execution. Our findings challenge conventional GPU-first thinking,
highlighting the untapped potential of optimized CPU inference and paving the
way for smarter deployment strategies in mobile AI. However, fully explaining
the observed CPU advantage remains difficult due to limited access to low-level
profiling tools on iOS.

</details>


### [260] [Benchmarking of CPU-intensive Stream Data Processing in The Edge Computing Systems](https://arxiv.org/abs/2505.07755)
*Tomasz Szydlo,Viacheslaw Horbanow,Dev Nandan Jha,Shashikant Ilager,Aleksander Slominski,Rajiv Ranjan*

Main category: cs.DC

TL;DR: 该论文研究边缘计算中资源利用率低的问题，通过分析CPU频率、功耗与性能的关系，提出优化方法以提高效率和节能。


<details>
  <summary>Details</summary>
Motivation: 边缘设备常因缺乏全面的性能分析机制而导致资源利用率不足，需动态调整配置以适应工作负载。

Method: 使用合成微基准测试，在不同工作负载大小和CPU频率下评估边缘集群中单个处理节点的功耗与性能特征。

Result: 揭示了CPU频率、功耗与性能之间的关联，提出了优化边缘资源使用的方法。

Conclusion: 通过深入理解这些关系，可以提升计算效率并节省能源，优化边缘计算资源利用。

Abstract: Edge computing has emerged as a pivotal technology, offering significant
advantages such as low latency, enhanced data security, and reduced reliance on
centralized cloud infrastructure. These benefits are crucial for applications
requiring real-time data processing or strict security measures. Despite these
advantages, edge devices operating within edge clusters are often
underutilized. This inefficiency is mainly due to the absence of a holistic
performance profiling mechanism which can help dynamically adjust the desired
system configuration for a given workload. Since edge computing environments
involve a complex interplay between CPU frequency, power consumption, and
application performance, a deeper understanding of these correlations is
essential. By uncovering these relationships, it becomes possible to make
informed decisions that enhance both computational efficiency and energy
savings. To address this gap, this paper evaluates the power consumption and
performance characteristics of a single processing node within an edge cluster
using a synthetic microbenchmark by varying the workload size and CPU
frequency. The results show how an optimal measure can lead to optimized usage
of edge resources, given both performance and power consumption.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [261] [A Formally Verified Robustness Certifier for Neural Networks (Extended Version)](https://arxiv.org/abs/2505.06958)
*James Tobler,Hira Taqdees Syeda,Toby Murray*

Main category: cs.PL

TL;DR: 该论文指出当前全局鲁棒神经网络中的认证函数实现未经验证，存在漏洞，并提出了一种在Dafny中实现并形式化验证的认证函数。


<details>
  <summary>Details</summary>
Motivation: 神经网络易受微小输入扰动影响，导致错误分类。当前全局鲁棒神经网络的认证函数虽能保证输出鲁棒性，但其实现未经验证，可能存在漏洞。

Method: 作者实现并形式化验证了一个用于全局鲁棒神经网络的认证函数，使用Dafny语言编写，并讨论了其设计决策和实践应用经验。

Result: 研究发现之前的认证函数实现在特定情况下存在漏洞，且依赖的近似算法（如幂迭代）并不能保证准确性。新提出的验证方法解决了这些问题。

Conclusion: 形式化验证的认证函数为全局鲁棒神经网络提供了可靠的鲁棒性保证，解决了之前实现中的漏洞问题。

Abstract: Neural networks are often susceptible to minor perturbations in input that
cause them to misclassify. A recent solution to this problem is the use of
globally-robust neural networks, which employ a function to certify that the
classification of an input cannot be altered by such a perturbation. Outputs
that pass this test are called certified robust. However, to the authors'
knowledge, these certification functions have not yet been verified at the
implementation level. We demonstrate how previous unverified implementations
are exploitably unsound in certain circumstances. Moreover, they often rely on
approximation-based algorithms, such as power iteration, that (perhaps
surprisingly) do not guarantee soundness. To provide assurance that a given
output is robust, we implemented and formally verified a certification function
for globally-robust neural networks in Dafny. We describe the program, its
specifications, and the important design decisions taken for its implementation
and verification, as well as our experience applying it in practice.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [262] [Burger: Robust Graph Denoising-augmentation Fusion and Multi-semantic Modeling in Social Recommendation](https://arxiv.org/abs/2505.06612)
*Yuqin Lan*

Main category: cs.SI

TL;DR: 论文提出了一种名为Burger的社交推荐模型，通过图去噪增强融合和多语义建模，解决了社交网络中兴趣无关关系对推荐准确性的干扰问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分关注社交网络与用户-物品交互网络中语义信息的相互影响，从而限制了社交推荐的准确性。

Method: 模型结合图卷积网络和张量卷积网络捕捉用户偏好，并引入双语义协调损失和贝叶斯后验概率来优化多语义建模和社交关系挖掘。

Result: 在三个真实数据集上的实验表明，Burger模型的性能优于现有最先进模型。

Conclusion: Burger通过多语义建模和社交关系优化，显著提升了社交推荐的准确性和鲁棒性。

Abstract: In the era of rapid development of social media, social recommendation
systems as hybrid recommendation systems have been widely applied. Existing
methods capture interest similarity between users to filter out
interest-irrelevant relations in social networks that inevitably decrease
recommendation accuracy, however, limited research has a focus on the mutual
influence of semantic information between the social network and the user-item
interaction network for further improving social recommendation. To address
these issues, we introduce a social \underline{r}ecommendation model with
ro\underline{bu}st g\underline{r}aph denoisin\underline{g}-augmentation fusion
and multi-s\underline{e}mantic Modeling(Burger). Specifically, we firstly
propose to construct a social tensor in order to smooth the training process of
the model. Then, a graph convolutional network and a tensor convolutional
network are employed to capture user's item preference and social preference,
respectively. Considering the different semantic information in the user-item
interaction network and the social network, a bi-semantic coordination loss is
proposed to model the mutual influence of semantic information. To alleviate
the interference of interest-irrelevant relations on multi-semantic modeling,
we further use Bayesian posterior probability to mine potential social
relations to replace social noise. Finally, the sliding window mechanism is
utilized to update the social tensor as the input for the next iteration.
Extensive experiments on three real datasets show Burger has a superior
performance compared with the state-of-the-art models.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [263] [Diffused Responsibility: Analyzing the Energy Consumption of Generative Text-to-Audio Diffusion Models](https://arxiv.org/abs/2505.07615)
*Riccardo Passoni,Francesca Ronchini,Luca Comanducci,Romain Serizel,Fabio Antonacci*

Main category: eess.AS

TL;DR: 该论文分析了7种最先进的文本到音频扩散生成模型的能耗，探讨了生成参数对推理时能耗的影响，并寻找音频质量与能耗的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 文本到音频模型虽然强大，但高计算需求引发了对能耗和环保影响的担忧，因此需要研究如何在性能与环境影响之间找到平衡。

Method: 研究者对7种扩散生成模型进行能耗分析，评估不同生成参数对推理时能耗的影响，并通过帕累托最优解寻找最佳平衡点。

Result: 研究结果为开发更高效的生成音频模型提供了性能与能耗权衡的见解。

Conclusion: 通过对模型参数和能耗的分析，论文为未来的高效音频生成模型设计提出了实用建议。

Abstract: Text-to-audio models have recently emerged as a powerful technology for
generating sound from textual descriptions. However, their high computational
demands raise concerns about energy consumption and environmental impact. In
this paper, we conduct an analysis of the energy usage of 7 state-of-the-art
text-to-audio diffusion-based generative models, evaluating to what extent
variations in generation parameters affect energy consumption at inference
time. We also aim to identify an optimal balance between audio quality and
energy consumption by considering Pareto-optimal solutions across all selected
models. Our findings provide insights into the trade-offs between performance
and environmental impact, contributing to the development of more efficient
generative audio models.

</details>


### [264] [TACOS: Temporally-aligned Audio CaptiOnS for Language-Audio Pretraining](https://arxiv.org/abs/2505.07609)
*Paul Primus,Florian Schmid,Gerhard Widmer*

Main category: eess.AS

TL;DR: 该论文提出了一种基于帧级别对比学习的语言-音频模型，利用时间标注的音频描述提升模型的时间对齐能力，优于仅使用全局标注的模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用全局音频描述训练，缺乏时间监督，而作者认为时间标注的描述能提升语言-音频模型（如CLAP）的表现力，并提供更好的时序对齐能力。

Method: 作者构建了一个带有时间精确描述的音频数据集，并用大语言模型清洗标注数据，提出帧级别的对比学习策略来对齐文本描述和音频片段。

Result: 实验表明，新模型在AudioSet Strong基准上显示出更好的时间对齐能力，优于仅用全局标注训练的模型。

Conclusion: 时间精确的音频标注能显著提升语言-音频模型的性能，尤其在帧级别对比学习任务中表现更优。数据集和代码已开源。

Abstract: Learning to associate audio with textual descriptions is valuable for a range
of tasks, including pretraining, zero-shot classification, audio retrieval,
audio captioning, and text-conditioned audio generation. Existing contrastive
language-audio pretrained models are typically trained using global, clip-level
descriptions, which provide only weak temporal supervision. We hypothesize that
CLAP-like language-audio models - particularly, if they are expected to produce
frame-level embeddings - can benefit from a stronger temporal supervision. To
confirm our hypothesis, we curate a novel dataset of approximately 12,000 audio
recordings from Freesound, each annotated with single-sentence free-text
descriptions linked to a specific temporal segment in an audio recording. We
use large language models to clean these annotations by removing references to
non-audible events, transcribed speech, typos, and annotator language bias. We
further propose a frame-wise contrastive training strategy that learns to align
text descriptions with temporal regions in an audio recording and demonstrate
that our model has better temporal text-audio alignment abilities compared to
models trained only on global captions when evaluated on the AudioSet Strong
benchmark. The dataset and our source code are available on Zenodo and GitHub,
respectively.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [265] [Learning Sequential Kinematic Models from Demonstrations for Multi-Jointed Articulated Objects](https://arxiv.org/abs/2505.06363)
*Anmol Gupta,Weiwei Gu,Omkar Patil,Jun Ki Lee,Nakul Gopalan*

Main category: cs.RO

TL;DR: 提出了一种名为OKSM的新方法，通过人类演示学习多自由度物体的运动学约束和操作顺序，并结合深度学习模型Pokenet进行点云数据处理，在真实数据上性能提升超过20%。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于先验知识或仅适用于单自由度物体，无法处理遮挡关节或操作顺序问题，因此需要更通用的解决方案。

Method: 通过OKSM捕捉物体的运动学约束和操作顺序，并利用Pokenet网络从点云数据中估计这些模型。

Result: 在仿真和真实数据上验证了方法的有效性，Pokenet在真实数据上的关节轴和状态估计性能提升超过20%。

Conclusion: OKSM结合Pokenet能够有效解决多自由度物体的建模与操控问题，并通过机器人实验验证了其可行性。

Abstract: As robots become more generalized and deployed in diverse environments, they
must interact with complex objects, many with multiple independent joints or
degrees of freedom (DoF) requiring precise control. A common strategy is object
modeling, where compact state-space models are learned from real-world
observations and paired with classical planning. However, existing methods
often rely on prior knowledge or focus on single-DoF objects, limiting their
applicability. They also fail to handle occluded joints and ignore the
manipulation sequences needed to access them. We address this by learning
object models from human demonstrations. We introduce Object Kinematic Sequence
Machines (OKSMs), a novel representation capturing both kinematic constraints
and manipulation order for multi-DoF objects. To estimate these models from
point cloud data, we present Pokenet, a deep neural network trained on human
demonstrations. We validate our approach on 8,000 simulated and 1,600
real-world annotated samples. Pokenet improves joint axis and state estimation
by over 20 percent on real-world data compared to prior methods. Finally, we
demonstrate OKSMs on a Sawyer robot using inverse kinematics-based planning to
manipulate multi-DoF objects.

</details>


### [266] [Camera Control at the Edge with Language Models for Scene Understanding](https://arxiv.org/abs/2505.06402)
*Alexiy Buynitsky,Sina Ehsani,Bhanu Pallakonda,Pragyana Mishra*

Main category: cs.RO

TL;DR: OPUS是一个基于大型语言模型的框架，用于控制PTZ摄像头，通过生成关键词和监督微调提升成本效益，性能接近GPT-4，且在测试中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 提升PTZ摄像头控制的成本效益和用户友好性，通过自然语言接口简化操作。

Method: 使用大型语言模型生成关键词，通过监督微调将知识从小模型转移到更大模型，并转换多摄像头数据为文本描述。

Result: 在基准测试中，OPUS比传统方法表现更好，任务准确率提高了20%，且性能接近GPT-4。

Conclusion: OPUS通过自然语言接口简化了PTZ摄像头的控制，代表了该技术的重大进步。

Abstract: In this paper, we present Optimized Prompt-based Unified System (OPUS), a
framework that utilizes a Large Language Model (LLM) to control Pan-Tilt-Zoom
(PTZ) cameras, providing contextual understanding of natural environments. To
achieve this goal, the OPUS system improves cost-effectiveness by generating
keywords from a high-level camera control API and transferring knowledge from
larger closed-source language models to smaller ones through Supervised
Fine-Tuning (SFT) on synthetic data. This enables efficient edge deployment
while maintaining performance comparable to larger models like GPT-4. OPUS
enhances environmental awareness by converting data from multiple cameras into
textual descriptions for language models, eliminating the need for specialized
sensory tokens. In benchmark testing, our approach significantly outperformed
both traditional language model techniques and more complex prompting methods,
achieving a 35% improvement over advanced techniques and a 20% higher task
accuracy compared to closed-source models like Gemini Pro. The system
demonstrates OPUS's capability to simplify PTZ camera operations through an
intuitive natural language interface. This approach eliminates the need for
explicit programming and provides a conversational method for interacting with
camera systems, representing a significant advancement in how users can control
and utilize PTZ camera technology.

</details>


### [267] [Quadrupedal Robot Skateboard Mounting via Reverse Curriculum Learning](https://arxiv.org/abs/2505.06561)
*Danil Belov,Artem Erkhov,Elizaveta Pestova,Ilya Osokin,Dzmitry Tsetserukou,Pavel Osinenko*

Main category: cs.RO

TL;DR: 使用逆向课程强化学习让四足机器人学会滑板站立，解决了初始站立阶段的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究展示四足机器人在滑板上的动作，但初始站立阶段仍是难点，本文旨在解决这一问题。

Method: 采用目标导向方法，从任务最终阶段开始，逐步增加问题复杂性，初始阶段固定滑板并逐步放松条件。

Result: 学习策略在滑板位置和方向变化中表现出鲁棒性，并成功迁移到移动滑板场景。

Conclusion: 通过逆向课程强化学习，四足机器人成功学会站立滑板，代码和模型已开源。

Abstract: The aim of this work is to enable quadrupedal robots to mount skateboards
using Reverse Curriculum Reinforcement Learning. Although prior work has
demonstrated skateboarding for quadrupeds that are already positioned on the
board, the initial mounting phase still poses a significant challenge. A
goal-oriented methodology was adopted, beginning with the terminal phases of
the task and progressively increasing the complexity of the problem definition
to approximate the desired objective. The learning process was initiated with
the skateboard rigidly fixed within the global coordinate frame and the robot
positioned directly above it. Through gradual relaxation of these initial
conditions, the learned policy demonstrated robustness to variations in
skateboard position and orientation, ultimately exhibiting a successful
transfer to scenarios involving a mobile skateboard. The code, trained models,
and reproducible examples are available at the following link:
https://github.com/dancher00/quadruped-skateboard-mounting

</details>


### [268] [JAEGER: Dual-Level Humanoid Whole-Body Controller](https://arxiv.org/abs/2505.06584)
*Ziluo Ding,Haobin Jiang,Yuxuan Wang,Zhenguo Sun,Yu Zhang,Xiaojie Niu,Ming Yang,Weishuai Zeng,Xinrun Xu,Zongqing Lu*

Main category: cs.RO

TL;DR: JAEGER是一种双层次全身控制器，通过分离上下半身控制来提升人形机器人的鲁棒性和多功能性。它支持粗粒度与细粒度控制，并利用人类动作数据集和课程学习进行训练，实验表明其在仿真和真实环境中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统单控制器方法在处理人形机器人全身控制时面临维度灾难和鲁棒性不足的挑战。JAEGER通过分离上下半身控制，旨在提升策略的鲁棒性和多功能性。

Method: JAEGER设计为双层次控制器，分别控制上下半身，结合粗粒度（根速度跟踪）和细粒度（关节角度跟踪）控制。训练时使用AMASS数据集，通过重定向网络将人类动作映射到机器人姿态，并采用课程学习（监督学习初始化+强化学习优化）。

Result: 在两种人形机器人平台上测试，JAEGER在仿真和真实环境中均优于现有方法，展示了更高的稳定性和多功能性。

Conclusion: JAEGER通过分层控制和混合训练策略，显著提升了人形机器人控制的鲁棒性和适应性，为复杂任务提供了可行的解决方案。

Abstract: This paper presents JAEGER, a dual-level whole-body controller for humanoid
robots that addresses the challenges of training a more robust and versatile
policy. Unlike traditional single-controller approaches, JAEGER separates the
control of the upper and lower bodies into two independent controllers, so that
they can better focus on their distinct tasks. This separation alleviates the
dimensionality curse and improves fault tolerance. JAEGER supports both root
velocity tracking (coarse-grained control) and local joint angle tracking
(fine-grained control), enabling versatile and stable movements. To train the
controller, we utilize a human motion dataset (AMASS), retargeting human poses
to humanoid poses through an efficient retargeting network, and employ a
curriculum learning approach. This method performs supervised learning for
initialization, followed by reinforcement learning for further exploration. We
conduct our experiments on two humanoid platforms and demonstrate the
superiority of our approach against state-of-the-art methods in both simulation
and real environments.

</details>


### [269] [Balancing Progress and Safety: A Novel Risk-Aware Objective for RL in Autonomous Driving](https://arxiv.org/abs/2505.06737)
*Ahmed Abouelazm,Jonas Michel,Helen Gremmelmaier,Tim Joseph,Philip Schörner,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 该论文提出了一种改进的强化学习奖励函数设计方法，通过层次化和归一化的目标定义，结合风险感知目标，提升了自动驾驶的安全性，实验显示碰撞率降低了21%。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在自动驾驶中的奖励函数设计存在不足，特别是安全目标仅作为碰撞惩罚，而忽略了潜在风险。本文旨在通过更全面的奖励设计提升RL在真实场景的适用性。

Method: 1. 分层定义驾驶目标并归一化；2. 引入基于二维椭球函数和RSS扩展的风险感知目标；3. 在无信号交叉路口场景中验证。

Result: 相比基线方法，碰撞率平均降低21%，路线进度和累积奖励表现更优，验证了其安全性和高效性。

Conclusion: 改进的奖励函数能有效平衡安全与性能，为RL在复杂驾驶场景中的应用提供了实用方案。

Abstract: Reinforcement Learning (RL) is a promising approach for achieving autonomous
driving due to robust decision-making capabilities. RL learns a driving policy
through trial and error in traffic scenarios, guided by a reward function that
combines the driving objectives. The design of such reward function has
received insufficient attention, yielding ill-defined rewards with various
pitfalls. Safety, in particular, has long been regarded only as a penalty for
collisions. This leaves the risks associated with actions leading up to a
collision unaddressed, limiting the applicability of RL in real-world
scenarios. To address these shortcomings, our work focuses on enhancing the
reward formulation by defining a set of driving objectives and structuring them
hierarchically. Furthermore, we discuss the formulation of these objectives in
a normalized manner to transparently determine their contribution to the
overall reward. Additionally, we introduce a novel risk-aware objective for
various driving interactions based on a two-dimensional ellipsoid function and
an extension of Responsibility-Sensitive Safety (RSS) concepts. We evaluate the
efficacy of our proposed reward in unsignalized intersection scenarios with
varying traffic densities. The approach decreases collision rates by 21\% on
average compared to baseline rewards and consistently surpasses them in route
progress and cumulative reward, demonstrating its capability to promote safer
driving behaviors while maintaining high-performance levels.

</details>


### [270] [Boundary-Guided Trajectory Prediction for Road Aware and Physically Feasible Autonomous Driving](https://arxiv.org/abs/2505.06740)
*Ahmed Abouelazm,Mianzhi Liu,Christian Hubschneider,Yin Wu,Daniel Slieter,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 论文提出了一种新的轨迹预测框架，通过将轨迹预测建模为受允许行驶方向和边界约束的回归问题，确保预测结果在道路范围内并满足运动学可行性，显著降低了不可行轨迹比例和脱轨率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中准确预测周围道路用户的轨迹至关重要，但现有方法在防止脱轨预测和保证运动学可行性方面存在不足，缺乏合理性保证且常在复杂性和灵活性之间权衡。

Method: 框架使用当前状态和高清地图，定义有效边界并通过训练网络学习左右边界折线间的叠加路径，预测加速度轮廓以确保运动学可行性。

Result: 在Argoverse-2数据集上，相比HPTR基线，最终位移误差降低且不可行轨迹完全消除，对罕见操作和分布外场景泛化能力更强，对抗攻击下的脱轨率从66%降至1%。

Conclusion: 该方法生成的预测结果既可行又鲁棒，为自动驾驶轨迹预测提供了更可靠的解决方案。

Abstract: Accurate prediction of surrounding road users' trajectories is essential for
safe and efficient autonomous driving. While deep learning models have improved
performance, challenges remain in preventing off-road predictions and ensuring
kinematic feasibility. Existing methods incorporate road-awareness modules and
enforce kinematic constraints but lack plausibility guarantees and often
introduce trade-offs in complexity and flexibility. This paper proposes a novel
framework that formulates trajectory prediction as a constrained regression
guided by permissible driving directions and their boundaries. Using the
agent's current state and an HD map, our approach defines the valid boundaries
and ensures on-road predictions by training the network to learn superimposed
paths between left and right boundary polylines. To guarantee feasibility, the
model predicts acceleration profiles that determine the vehicle's travel
distance along these paths while adhering to kinematic constraints. We evaluate
our approach on the Argoverse-2 dataset against the HPTR baseline. Our approach
shows a slight decrease in benchmark metrics compared to HPTR but notably
improves final displacement error and eliminates infeasible trajectories.
Moreover, the proposed approach has superior generalization to less prevalent
maneuvers and unseen out-of-distribution scenarios, reducing the off-road rate
under adversarial attacks from 66\% to just 1\%. These results highlight the
effectiveness of our approach in generating feasible and robust predictions.

</details>


### [271] [TPK: Trustworthy Trajectory Prediction Integrating Prior Knowledge For Interpretability and Kinematic Feasibility](https://arxiv.org/abs/2505.06743)
*Marius Baden,Ahmed Abouelazm,Christian Hubschneider,Yin Wu,Daniel Slieter,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 本文提出了一种结合交互和运动学先验的轨迹预测方法，适用于多种交通参与者（车辆、行人、自行车），通过类特定交互层和规则化的交互重要性评分提高可解释性，并在物理可行性上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中轨迹预测的信任问题：现有模型的预测可能不符合物理或逻辑，且通常只针对单一交通参与者设计。

Method: 提出类特定交互层（捕捉不同交通参与者的行为差异）和DG-SFM交互重要性评分（增强可解释性），并设计适合各类别的运动学模型（包括新行人运动学模型）。

Result: 在Argoverse 2数据集上，交互可解释性提升，错误预测与交互先验的偏离相关；运动学模型虽轻微降低精度，但消除了不可行轨迹。

Conclusion: 该方法通过可解释的交互推理和符合物理的预测，提升了轨迹预测的可信度。

Abstract: Trajectory prediction is crucial for autonomous driving, enabling vehicles to
navigate safely by anticipating the movements of surrounding road users.
However, current deep learning models often lack trustworthiness as their
predictions can be physically infeasible and illogical to humans. To make
predictions more trustworthy, recent research has incorporated prior knowledge,
like the social force model for modeling interactions and kinematic models for
physical realism. However, these approaches focus on priors that suit either
vehicles or pedestrians and do not generalize to traffic with mixed agent
classes. We propose incorporating interaction and kinematic priors of all agent
classes--vehicles, pedestrians, and cyclists with class-specific interaction
layers to capture agent behavioral differences. To improve the interpretability
of the agent interactions, we introduce DG-SFM, a rule-based interaction
importance score that guides the interaction layer. To ensure physically
feasible predictions, we proposed suitable kinematic models for all agent
classes with a novel pedestrian kinematic model. We benchmark our approach on
the Argoverse 2 dataset, using the state-of-the-art transformer HPTR as our
baseline. Experiments demonstrate that our method improves interaction
interpretability, revealing a correlation between incorrect predictions and
divergence from our interaction prior. Even though incorporating the kinematic
models causes a slight decrease in accuracy, they eliminate infeasible
trajectories found in the dataset and the baseline model. Thus, our approach
fosters trust in trajectory prediction as its interaction reasoning is
interpretable, and its predictions adhere to physics.

</details>


### [272] [Efficient Robotic Policy Learning via Latent Space Backward Planning](https://arxiv.org/abs/2505.06861)
*Dongxiu Liu,Haoyi Niu,Zhihao Wang,Jinliang Zheng,Yinan Zheng,Zhonghong Ou,Jianming Hu,Jianxiong Li,Xianyuan Zhan*

Main category: cs.RO

TL;DR: 论文提出了一种逆向规划框架（LBP），通过从最终目标反向生成中间子目标，解决了现有规划方法在实时性和准确性上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器人规划方法多依赖像素级多帧预测，存在计算成本高和误差积累问题，而正向子目标规划仍难以保证长期目标对齐。

Method: 提出Latent Space Backward Planning（LBP），从潜在空间最终目标出发，递归生成中间子目标，并通过可学习令牌指导动作生成。

Result: LBP在仿真和真实机器人长时程任务中表现优于现有方法，达到SOTA性能。

Conclusion: 逆向规划能有效提升实时性和准确性，适用于多阶段长时程任务。

Abstract: Current robotic planning methods often rely on predicting multi-frame images
with full pixel details. While this fine-grained approach can serve as a
generic world model, it introduces two significant challenges for downstream
policy learning: substantial computational costs that hinder real-time
deployment, and accumulated inaccuracies that can mislead action extraction.
Planning with coarse-grained subgoals partially alleviates efficiency issues.
However, their forward planning schemes can still result in off-task
predictions due to accumulation errors, leading to misalignment with long-term
goals. This raises a critical question: Can robotic planning be both efficient
and accurate enough for real-time control in long-horizon, multi-stage tasks?
To address this, we propose a Latent Space Backward Planning scheme (LBP),
which begins by grounding the task into final latent goals, followed by
recursively predicting intermediate subgoals closer to the current state. The
grounded final goal enables backward subgoal planning to always remain aware of
task completion, facilitating on-task prediction along the entire planning
horizon. The subgoal-conditioned policy incorporates a learnable token to
summarize the subgoal sequences and determines how each subgoal guides action
extraction. Through extensive simulation and real-robot long-horizon
experiments, we show that LBP outperforms existing fine-grained and forward
planning methods, achieving SOTA performance. Project Page:
https://lbp-authors.github.io

</details>


### [273] [FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots](https://arxiv.org/abs/2505.06883)
*Botian Xu,Haoyang Weng,Qingzhou Lu,Yang Gao,Huazhe Xu*

Main category: cs.RO

TL;DR: 该论文提出了FACET方法，通过强化学习让机器人模仿虚拟质量-弹簧-阻尼系统，以优化外力交互下的控制表现，提升了鲁棒性和合规性。


<details>
  <summary>Details</summary>
Motivation: 传统基于位置或速度跟踪的强化学习目标对外力不敏感，导致机器人行为僵硬且交互不安全。

Method: 结合阻抗控制思想，用强化学习训练策略模仿虚拟质量-弹簧-阻尼系统，通过调节虚拟弹簧实现精细外力控制。

Result: 仿真中四足机器人对200Ns冲量鲁棒性提升，碰撞冲量减少80%；实物测试展示了合规性和拖动2/3自重负载的能力。

Conclusion: FACET可扩展至复杂场景（如足式操作器和人形机器人），实现全身合规控制。

Abstract: Reinforcement learning (RL) has made significant strides in legged robot
control, enabling locomotion across diverse terrains and complex
loco-manipulation capabilities. However, the commonly used position or velocity
tracking-based objectives are agnostic to forces experienced by the robot,
leading to stiff and potentially dangerous behaviors and poor control during
forceful interactions. To address this limitation, we present
\emph{Force-Adaptive Control via Impedance Reference Tracking} (FACET).
Inspired by impedance control, we use RL to train a control policy to imitate a
virtual mass-spring-damper system, allowing fine-grained control under external
forces by manipulating the virtual spring. In simulation, we demonstrate that
our quadruped robot achieves improved robustness to large impulses (up to 200
Ns) and exhibits controllable compliance, achieving an 80% reduction in
collision impulse. The policy is deployed to a physical robot to showcase both
compliance and the ability to engage with large forces by kinesthetic control
and pulling payloads up to 2/3 of its weight. Further extension to a legged
loco-manipulator and a humanoid shows the applicability of our method to more
complex settings to enable whole-body compliance control. Project Website:
https://egalahad.github.io/facet/

</details>


### [274] [Reinforcement Learning-Based Monocular Vision Approach for Autonomous UAV Landing](https://arxiv.org/abs/2505.06963)
*Tarik Houichime,Younes EL Amrani*

Main category: cs.RO

TL;DR: 该论文提出了一种仅使用前置单目相机实现无人机自主着陆的创新方法，无需深度估计相机，通过优化问题和强化学习实现。


<details>
  <summary>Details</summary>
Motivation: 为了解决无人机自主着陆对复杂传感器依赖的问题，并降低系统成本，同时借鉴人类估计过程。

Method: 利用优化的基于视觉特征的方法，通过设计特殊的着陆垫图案，并结合强化学习算法估计高度和深度。以训练无人机找到最佳着陆参数。

Result: 仿真和实验表明，该方法能够实现稳健且高精度的自主着陆，无需复杂传感器配置。

Conclusion: 该研究推动了经济高效的无人机着陆解决方案的发展，扩展了无人机在各领域的应用潜力。

Abstract: This paper introduces an innovative approach for the autonomous landing of
Unmanned Aerial Vehicles (UAVs) using only a front-facing monocular camera,
therefore obviating the requirement for depth estimation cameras. Drawing on
the inherent human estimating process, the proposed method reframes the landing
task as an optimization problem. The UAV employs variations in the visual
characteristics of a specially designed lenticular circle on the landing pad,
where the perceived color and form provide critical information for estimating
both altitude and depth. Reinforcement learning algorithms are utilized to
approximate the functions governing these estimations, enabling the UAV to
ascertain ideal landing settings via training. This method's efficacy is
assessed by simulations and experiments, showcasing its potential for robust
and accurate autonomous landing without dependence on complex sensor setups.
This research contributes to the advancement of cost-effective and efficient
UAV landing solutions, paving the way for wider applicability across various
fields.

</details>


### [275] [X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real](https://arxiv.org/abs/2505.07096)
*Prithwish Dan,Kushal Kedia,Angela Chao,Edward Weiyi Duan,Maximus Adrian Pace,Wei-Chiu Ma,Sanjiban Choudhury*

Main category: cs.RO

TL;DR: X-Sim框架通过利用人类视频中的物体运动信号训练机器人策略，无需机器人示教数据，显著提升任务进度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 人类视频缺乏动作标签，现有跨实体方法在实体差异大时效果不佳，需一种基于物体运动的通用学习信号。

Method: X-Sim采用真实-仿真-真实框架：从RGBD视频重建仿真环境，用物体轨迹定义奖励，训练RL策略后蒸馏为扩散策略，并通过在线域适应迁移到现实。

Result: 在5项任务中，X-Sim平均任务进度提升30%，数据收集时间减少10倍，且能适应新视角和动态变化。

Conclusion: X-Sim证明了利用物体运动信号的跨实体策略学习的有效性，为无示教数据的机器人训练提供了新方向。

Abstract: Human videos offer a scalable way to train robot manipulation policies, but
lack the action labels needed by standard imitation learning algorithms.
Existing cross-embodiment approaches try to map human motion to robot actions,
but often fail when the embodiments differ significantly. We propose X-Sim, a
real-to-sim-to-real framework that uses object motion as a dense and
transferable signal for learning robot policies. X-Sim starts by reconstructing
a photorealistic simulation from an RGBD human video and tracking object
trajectories to define object-centric rewards. These rewards are used to train
a reinforcement learning (RL) policy in simulation. The learned policy is then
distilled into an image-conditioned diffusion policy using synthetic rollouts
rendered with varied viewpoints and lighting. To transfer to the real world,
X-Si introduces an online domain adaptation technique that aligns real and
simulated observations during deployment. Importantly, X-Sim does not require
any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2
environments and show that it: (1) improves task progress by 30% on average
over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with
10x less data collection time, and (3) generalizes to new camera viewpoints and
test-time changes. Code and videos are available at
https://portal-cornell.github.io/X-Sim/.

</details>


### [276] [UAV-CodeAgents: Scalable UAV Mission Planning via Multi-Agent ReAct and Vision-Language Reasoning](https://arxiv.org/abs/2505.07236)
*Oleg Sautenkov,Yasheerah Yaqoot,Muhammad Ahsan Mustafa,Faryal Batool,Jeffrin Sam,Artem Lykov,Chih-Yung Wen,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: UAV-CodeAgents是一种基于大语言和视觉语言模型的多智能体框架，用于自主生成无人机任务，通过ReAct范式解析卫星图像和自然语言指令，实现高效的任务规划。


<details>
  <summary>Details</summary>
Motivation: 旨在减少无人机任务规划中的人工监督，提升其在复杂环境（如工业火灾检测）中的自主性和实时适应性。

Method: 结合ReAct范式、视觉定位机制和反应思维循环，实现多智能体协作任务生成，并基于Qwen2.5VL-7B模型进行微调。

Result: 在任务创建时间（平均96.96秒）和成功率（93%）上表现优异，解码温度0.5时可靠性最高。

Conclusion: 框架展示了在动态环境中高效生成无人机任务的潜力，未来将开源代码和基准数据集以推动研究。

Abstract: We present UAV-CodeAgents, a scalable multi-agent framework for autonomous
UAV mission generation, built on large language and vision-language models
(LLMs/VLMs). The system leverages the ReAct (Reason + Act) paradigm to
interpret satellite imagery, ground high-level natural language instructions,
and collaboratively generate UAV trajectories with minimal human supervision. A
core component is a vision-grounded, pixel-pointing mechanism that enables
precise localization of semantic targets on aerial maps. To support real-time
adaptability, we introduce a reactive thinking loop, allowing agents to
iteratively reflect on observations, revise mission goals, and coordinate
dynamically in evolving environments.
  UAV-CodeAgents is evaluated on large-scale mission scenarios involving
industrial and environmental fire detection. Our results show that a lower
decoding temperature (0.5) yields higher planning reliability and reduced
execution time, with an average mission creation time of 96.96 seconds and a
success rate of 93%. We further fine-tune Qwen2.5VL-7B on 9,000 annotated
satellite images, achieving strong spatial grounding across diverse visual
categories. To foster reproducibility and future research, we will release the
full codebase and a novel benchmark dataset for vision-language-based UAV
planning.

</details>


### [277] [CHD: Coupled Hierarchical Diffusion for Long-Horizon Tasks](https://arxiv.org/abs/2505.07261)
*Ce Hao,Anxing Xiao,Zhiwei Xue,Harold Soh*

Main category: cs.RO

TL;DR: CHD（耦合层次扩散）通过统一扩散过程联合建模高层子目标和低层轨迹，改善了轨迹连贯性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于扩散的规划器在复杂长周期任务中表现不佳，主要源于高层子目标选择和低层轨迹生成之间的松散耦合。

Method: 提出CHD框架，在统一扩散过程中联合建模高层子目标和低层轨迹，并通过共享分类器传递低层反馈以实现子目标自校正。

Result: 在迷宫导航、桌面操作和家庭环境中，CHD均优于平面和分层扩散基线方法。

Conclusion: CHD通过紧密耦合高层与低层规划，实现了可扩展的长周期扩散规划，提升了任务性能。

Abstract: Diffusion-based planners have shown strong performance in short-horizon tasks
but often fail in complex, long-horizon settings. We trace the failure to loose
coupling between high-level (HL) sub-goal selection and low-level (LL)
trajectory generation, which leads to incoherent plans and degraded
performance. We propose Coupled Hierarchical Diffusion (CHD), a framework that
models HL sub-goals and LL trajectories jointly within a unified diffusion
process. A shared classifier passes LL feedback upstream so that sub-goals
self-correct while sampling proceeds. This tight HL-LL coupling improves
trajectory coherence and enables scalable long-horizon diffusion planning.
Experiments across maze navigation, tabletop manipulation, and household
environments show that CHD consistently outperforms both flat and hierarchical
diffusion baselines.

</details>


### [278] [JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes](https://arxiv.org/abs/2505.06771)
*Shalin Anand Jain,Jiazhen Liu,Siva Kailas,Harish Ravichandar*

Main category: cs.RO

TL;DR: 论文介绍了JaxRobotarium，一个基于Jax的多机器人强化学习平台，解决了现有MARL平台（如MARBLER）缺乏并行化和硬件加速的问题，显著提升了训练和仿真速度。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）在机器人系统中潜力巨大，但现有平台（如SMAC和MPE）缺乏机器人相关性和硬件部署支持，且MARBLER因不支持并行化和GPU/TPU执行导致速度过慢，限制了其应用。

Method: 提出JaxRobotarium，一个基于Jax的端到端仿真、学习、部署和基准测试平台，支持并行化和硬件加速，并与现有MARL库（如JaxMARL）无缝集成，同时引入八种标准化协调场景（包括四种新场景）。

Result: JaxRobotarium在保持高仿真保真度的同时，训练速度提升20倍，仿真速度提升150倍，并通过Robotarium测试床提供开源的模拟到真实的评估流程。

Conclusion: JaxRobotarium显著加速了多机器人学习的研究与评估，为研究人员提供了高效、易用的工具，推动了该领域的民主化发展。

Abstract: Multi-agent reinforcement learning (MARL) has emerged as a promising solution
for learning complex and scalable coordination behaviors in multi-robot
systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics
relevance and hardware deployment, leaving multi-robot learning researchers to
develop bespoke environments and hardware testbeds dedicated to the development
and evaluation of their individual contributions. The Multi-Agent RL Benchmark
and Learning Environment for the Robotarium (MARBLER) is an exciting recent
step in providing a standardized robotics-relevant platform for MARL, by
bridging the Robotarium testbed with existing MARL software infrastructure.
However, MARBLER lacks support for parallelization and GPU/TPU execution,
making the platform prohibitively slow compared to modern MARL environments and
hindering adoption. We contribute JaxRobotarium, a Jax-powered end-to-end
simulation, learning, deployment, and benchmarking platform for the Robotarium.
JaxRobotarium enables rapid training and deployment of multi-robot
reinforcement learning (MRRL) policies with realistic robot dynamics and safety
constraints, supporting both parallelization and hardware acceleration. Our
generalizable learning interface provides an easy-to-use integration with SOTA
MARL libraries (e.g., JaxMARL). In addition, JaxRobotarium includes eight
standardized coordination scenarios, including four novel scenarios that bring
established MARL benchmark tasks (e.g., RWARE and Level-Based Foraging) to a
realistic robotics setting. We demonstrate that JaxRobotarium retains high
simulation fidelity while achieving dramatic speedups over baseline (20x in
training and 150x in simulation), and provides an open-access sim-to-real
evaluation pipeline through the Robotarium testbed, accelerating and
democratizing access to multi-robot learning research and evaluation.

</details>


### [279] [HuB: Learning Extreme Humanoid Balance](https://arxiv.org/abs/2505.07294)
*Tong Zhang,Boyuan Zheng,Ruiqian Nai,Yingdong Hu,Yen-Jen Wang,Geng Chen,Fanqi Lin,Jiongye Li,Chuye Hong,Koushil Sreenath,Yang Gao*

Main category: cs.RO

TL;DR: 该研究提出了HuB框架，通过参考运动优化、平衡感知策略学习和模拟到现实的鲁棒性训练，解决了人形机器人在高难度平衡任务中的不稳定性和形态不匹配问题，并在实际机器人上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在需要高精度平衡控制的动作（如单腿站立或高踢）中表现不佳，现有强化学习方法因参考运动误差、形态不匹配和模拟到现实的差距而难以适用。

Method: 提出HuB框架，整合了参考运动优化、平衡感知策略学习和模拟到现实的鲁棒性训练三部分，分别针对上述挑战。

Result: 在Unitree G1机器人上成功完成高难度平衡任务（如“燕子平衡”和李小龙式高踢），即使在强物理干扰下仍保持稳定，而基准方法均失败。

Conclusion: HuB框架显著提升了人形机器人在极端平衡任务中的表现，为解决复杂运动控制问题提供了新思路。

Abstract: The human body demonstrates exceptional motor capabilities-such as standing
steadily on one foot or performing a high kick with the leg raised over 1.5
meters-both requiring precise balance control. While recent research on
humanoid control has leveraged reinforcement learning to track human motions
for skill acquisition, applying this paradigm to balance-intensive tasks
remains challenging. In this work, we identify three key obstacles: instability
from reference motion errors, learning difficulties due to morphological
mismatch, and the sim-to-real gap caused by sensor noise and unmodeled
dynamics. To address these challenges, we propose HuB (Humanoid Balance), a
unified framework that integrates reference motion refinement, balance-aware
policy learning, and sim-to-real robustness training, with each component
targeting a specific challenge. We validate our approach on the Unitree G1
humanoid robot across challenging quasi-static balance tasks, including extreme
single-legged poses such as Swallow Balance and Bruce Lee's Kick. Our policy
remains stable even under strong physical disturbances-such as a forceful
soccer strike-while baseline methods consistently fail to complete these tasks.
Project website: https://hub-robot.github.io

</details>


### [280] [Realistic Counterfactual Explanations for Machine Learning-Controlled Mobile Robots using 2D LiDAR](https://arxiv.org/abs/2505.06906)
*Sindre Benjamin Remman,Anastasios M. Lekkas*

Main category: cs.RO

TL;DR: 该论文提出了一种利用2D激光雷达生成机器学习控制中现实反事实解释的新方法，通过参数化激光雷达空间并使用遗传算法优化形状参数，以用户查询为基础生成合成激光雷达数据，从而解释深度强化学习模型的决策。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习模型（尤其是人工神经网络）在安全关键控制应用中常被视为黑箱，难以解释，因此需要一种方法来生成现实的反事实解释，以增强模型的可解释性。

Method: 通过用简单形状（如圆形和矩形）参数化激光雷达空间，利用遗传算法选择参数配置，并通过光线投射生成合成激光雷达数据，进而生成反事实解释。

Result: 在TurtleBot3移动机器人上的实验表明，该方法能生成逻辑合理且现实的反事实解释，有效解释了深度强化学习代理的决策过程。

Conclusion: 该方法为移动机器人中的可解释AI提供了新工具，有助于理解、调试和改进基于机器学习的自主控制。

Abstract: This paper presents a novel method for generating realistic counterfactual
explanations (CFEs) in machine learning (ML)-based control for mobile robots
using 2D LiDAR. ML models, especially artificial neural networks (ANNs), can
provide advanced decision-making and control capabilities by learning from
data. However, they often function as black boxes, making it challenging to
interpret them. This is especially a problem in safety-critical control
applications. To generate realistic CFEs, we parameterize the LiDAR space with
simple shapes such as circles and rectangles, whose parameters are chosen by a
genetic algorithm, and the configurations are transformed into LiDAR data by
raycasting. Our model-agnostic approach generates CFEs in the form of synthetic
LiDAR data that resembles a base LiDAR state but is modified to produce a
pre-defined ML model control output based on a query from the user. We
demonstrate our method on a mobile robot, the TurtleBot3, controlled using deep
reinforcement learning (DRL) in real-world and simulated scenarios. Our method
generates logical and realistic CFEs, which helps to interpret the DRL agent's
decision making. This paper contributes towards advancing explainable AI in
mobile robotics, and our method could be a tool for understanding, debugging,
and improving ML-based autonomous control.

</details>


### [281] [Neural Brain: A Neuroscience-inspired Framework for Embodied Agents](https://arxiv.org/abs/2505.07634)
*Jian Liu,Xiongtao Shi,Thai Duy Nguyen,Haitian Zhang,Tianxiang Zhang,Wei Sun,Yanjie Li,Athanasios V. Vasilakos,Giovanni Iacca,Arshad Ali Khan,Arvind Kumar,Jae Won Cho,Ajmal Mian,Lihua Xie,Erik Cambria,Lin Wang*

Main category: cs.RO

TL;DR: 该论文提出了一个名为‘神经大脑’的统一框架，旨在为具身AI代理提供人类般的动态适应能力，结合了多模态感知、认知行动功能、神经可塑性记忆和神经形态硬件优化，并分析了当前AI与人类智能之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统缺乏与现实世界的物理交互能力，这推动了具身AI的发展。论文旨在通过‘神经大脑’框架解决静态AI模型与动态现实世界需求之间的差距。

Method: 论文提出了一种受生物学启发的架构，整合了多模态主动感知、感知-认知-行动功能、基于神经可塑性的记忆系统以及神经形态硬件/软件优化。

Result: 通过这一框架，论文展示了如何将多学科研究（如神经科学）应用于开发具有人类水平智能的自主代理。

Conclusion: 论文为开发在现实场景中具有人类级智能的通用具身AI代理提供了路线图，强调了跨学科合作的重要性。

Abstract: The rapid evolution of artificial intelligence (AI) has shifted from static,
data-driven models to dynamic systems capable of perceiving and interacting
with real-world environments. Despite advancements in pattern recognition and
symbolic reasoning, current AI systems, such as large language models, remain
disembodied, unable to physically engage with the world. This limitation has
driven the rise of embodied AI, where autonomous agents, such as humanoid
robots, must navigate and manipulate unstructured environments with human-like
adaptability. At the core of this challenge lies the concept of Neural Brain, a
central intelligence system designed to drive embodied agents with human-like
adaptability. A Neural Brain must seamlessly integrate multimodal sensing and
perception with cognitive capabilities. Achieving this also requires an
adaptive memory system and energy-efficient hardware-software co-design,
enabling real-time action in dynamic environments. This paper introduces a
unified framework for the Neural Brain of embodied agents, addressing two
fundamental challenges: (1) defining the core components of Neural Brain and
(2) bridging the gap between static AI models and the dynamic adaptability
required for real-world deployment. To this end, we propose a biologically
inspired architecture that integrates multimodal active sensing,
perception-cognition-action function, neuroplasticity-based memory storage and
updating, and neuromorphic hardware/software optimization. Furthermore, we also
review the latest research on embodied agents across these four aspects and
analyze the gap between current AI systems and human intelligence. By
synthesizing insights from neuroscience, we outline a roadmap towards the
development of generalizable, autonomous agents capable of human-level
intelligence in real-world scenarios.

</details>


### [282] [Guiding Data Collection via Factored Scaling Curves](https://arxiv.org/abs/2505.07728)
*Lihan Zha,Apurva Badithela,Michael Zhang,Justin Lidard,Jeremy Bao,Emily Zhou,David Snyder,Allen Z. Ren,Dhruv Shah,Anirudha Majumdar*

Main category: cs.RO

TL;DR: 论文介绍了一种称为分解缩放曲线（FSC）的方法，用于优化模仿学习中的数据收集策略，通过分析各环境因素对策略性能的影响，有针对性地在预算内收集最具影响力的数据，从而提升策略在新环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决在大规模数据集中训练通用模仿学习策略时，数据收集成本高昂且难以覆盖所有环境变化的问题，作者提出了分解缩放曲线方法，以高效指导数据收集。

Method: 通过构建分解缩放曲线（FSC），量化不同环境因素（如相机角度、桌子高度等）对策略性能的影响，从而有针对性地分配数据收集资源。

Result: 实验表明，该方法在模拟和真实场景中均显著提升了策略的成功率（最高26%），并能通过离线指标有效指导数据收集。

Conclusion: 分解缩放曲线为高效数据收集提供了理论支持，能够在有限预算下优化策略的泛化能力，适用于从零训练和微调两种场景。

Abstract: Generalist imitation learning policies trained on large datasets show great
promise for solving diverse manipulation tasks. However, to ensure
generalization to different conditions, policies need to be trained with data
collected across a large set of environmental factor variations (e.g., camera
pose, table height, distractors) $-$ a prohibitively expensive undertaking, if
done exhaustively. We introduce a principled method for deciding what data to
collect and how much to collect for each factor by constructing factored
scaling curves (FSC), which quantify how policy performance varies as data
scales along individual or paired factors. These curves enable targeted data
acquisition for the most influential factor combinations within a given budget.
We evaluate the proposed method through extensive simulated and real-world
experiments, across both training-from-scratch and fine-tuning settings, and
show that it boosts success rates in real-world tasks in new environments by up
to 26% over existing data-collection strategies. We further demonstrate how
factored scaling curves can effectively guide data collection using an offline
metric, without requiring real-world evaluation at scale.

</details>


### [283] [Improving Trajectory Stitching with Flow Models](https://arxiv.org/abs/2505.07802)
*Reece O'Mahoney,Wanming Yu,Ioannis Havoutis*

Main category: cs.RO

TL;DR: 该研究改进了生成模型在轨迹规划中的应用，解决了传统方法无法通过拼接方式规划轨迹的问题，并在模拟和实际硬件中验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型在轨迹规划中表现不佳，尤其是在所需解决方案不存在于训练集中时。研究旨在解决这一问题。

Method: 通过改进模型架构、数据集选择，并引入新的训练和推理方法，以增强和稳定轨迹规划能力。

Result: 在模拟和真实硬件环境中，新方法在生成超出分布边界条件的规划及避障任务中显著优于基线，避障效率提升四倍。

Conclusion: 研究证明了改进后的生成模型在复杂轨迹规划任务中的高效性，尤其是在处理新场景时的优势。

Abstract: Generative models have shown great promise as trajectory planners, given
their affinity to modeling complex distributions and guidable inference
process. Previous works have successfully applied these in the context of
robotic manipulation but perform poorly when the required solution does not
exist as a complete trajectory within the training set. We identify that this
is a result of being unable to plan via stitching, and subsequently address the
architectural and dataset choices needed to remedy this. On top of this, we
propose a novel addition to the training and inference procedures to both
stabilize and enhance these capabilities. We demonstrate the efficacy of our
approach by generating plans with out of distribution boundary conditions and
performing obstacle avoidance on the Franka Panda in simulation and on real
hardware. In both of these tasks our method performs significantly better than
the baselines and is able to avoid obstacles up to four times as large.

</details>


### [284] [DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies](https://arxiv.org/abs/2505.07813)
*Tony Tao,Mohan Kumar Srirama,Jason Jingzhou Liu,Kenneth Shaw,Deepak Pathak*

Main category: cs.RO

TL;DR: 論文提出DexWild-System低成本移動裝置，利用人類手動收集數據，結合機器人數據共訓練，提升模型在新環境和任務中的泛化能力，成功率高達68.5%，比純機器人數據訓練高近4倍。


<details>
  <summary>Details</summary>
Motivation: 為解決大規模機器人數據集成本高且不易獲取的問題，研究提出利用人類手動收集數據的方法，並設計低成本裝置DexWild-System，以提高數據收集的效率和可擴展性。

Method: 開發低成本移動裝置DexWild-System，允許人類使用自己的手收集交互數據，並將人類數據與機器人數據共訓練，形成混合學習框架。

Result: 實驗結果顯示，DexWild在新環境中達到68.5%的成功率，比純機器人數據訓練高近4倍，且在跨實體泛化能力上提升5.8倍。

Conclusion: DexWild結合人類和機器人數據的共訓練方法顯著提升了機器人策略的泛化能力，為低成本高效獲取多樣化數據提供了可行方案。

Abstract: Large-scale, diverse robot datasets have emerged as a promising path toward
enabling dexterous manipulation policies to generalize to novel environments,
but acquiring such datasets presents many challenges. While teleoperation
provides high-fidelity datasets, its high cost limits its scalability. Instead,
what if people could use their own hands, just as they do in everyday life, to
collect data? In DexWild, a diverse team of data collectors uses their hands to
collect hours of interactions across a multitude of environments and objects.
To record this data, we create DexWild-System, a low-cost, mobile, and
easy-to-use device. The DexWild learning framework co-trains on both human and
robot demonstrations, leading to improved performance compared to training on
each dataset individually. This combination results in robust robot policies
capable of generalizing to novel environments, tasks, and embodiments with
minimal additional robot-specific data. Experimental results demonstrate that
DexWild significantly improves performance, achieving a 68.5% success rate in
unseen environments-nearly four times higher than policies trained with robot
data only-and offering 5.8x better cross-embodiment generalization. Video
results, codebases, and instructions at https://dexwild.github.io

</details>


### [285] [H$^{\mathbf{3}}$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning](https://arxiv.org/abs/2505.07819)
*Yiyang Lu,Yufeng Tian,Zhecheng Yuan,Xianbang Wang,Pu Hua,Zhengrong Xue,Huazhe Xu*

Main category: cs.RO

TL;DR: 论文提出了一种新型的视觉运动策略学习框架H3DP，通过三层层次结构加强视觉特征与动作生成的耦合，实验表明其性能显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视视觉感知与动作预测的耦合，H3DP旨在通过层次化结构强化这一整合。

Method: H3DP包含三层层次：深度感知输入分层、多尺度视觉表征和层次化条件扩散过程。

Result: H3DP在44个仿真任务中平均相对提升27.5%，并在4个真实世界双边操作任务中表现优异。

Conclusion: 层次化设计有效提升了视觉运动策略的性能，证明了视觉-动作耦合的重要性。

Abstract: Visuomotor policy learning has witnessed substantial progress in robotic
manipulation, with recent approaches predominantly relying on generative models
to model the action distribution. However, these methods often overlook the
critical coupling between visual perception and action prediction. In this
work, we introduce $\textbf{Triply-Hierarchical Diffusion
Policy}~(\textbf{H$^{\mathbf{3}}$DP})$, a novel visuomotor learning framework
that explicitly incorporates hierarchical structures to strengthen the
integration between visual features and action generation. H$^{3}$DP contains
$\mathbf{3}$ levels of hierarchy: (1) depth-aware input layering that organizes
RGB-D observations based on depth information; (2) multi-scale visual
representations that encode semantic features at varying levels of granularity;
and (3) a hierarchically conditioned diffusion process that aligns the
generation of coarse-to-fine actions with corresponding visual features.
Extensive experiments demonstrate that H$^{3}$DP yields a $\mathbf{+27.5\%}$
average relative improvement over baselines across $\mathbf{44}$ simulation
tasks and achieves superior performance in $\mathbf{4}$ challenging bimanual
real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.

</details>


### [286] [Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models](https://arxiv.org/abs/2505.07815)
*Seungjae Lee,Daniel Ekpo,Haowen Liu,Furong Huang,Abhinav Shrivastava,Jia-Bin Huang*

Main category: cs.RO

TL;DR: IVE（Imagine, Verify, Execute）是一个受人类好奇心启发的探索框架，利用视觉语言模型（VLMs）生成语义场景图，并预测物理可行性以驱动机器人多样化探索，效果优于强化学习基线。


<details>
  <summary>Details</summary>
Motivation: 在开放环境中，机器人需要多样化的探索能力，而传统方法依赖密集奖励或任务监督难以实现。视觉语言模型（VLMs）具备语义推理能力，但其输出常缺乏物理可行性验证，因此需要一种方法将想象与实际执行结合。

Method: IVE框架分三步：1）将RGB-D观测抽象为语义场景图；2）通过VLMs想象新场景并验证其物理可行性；3）生成可执行动作序列。框架在模拟和真实桌面环境中验证。

Result: IVE在探索多样性和意义性上显著优于强化学习基线（状态熵提升4.1-7.8倍），且收集的经验支持下游学习，性能接近或超越人类演示训练的模型。

Conclusion: IVE通过结合VLM的语义想象和物理验证，实现了高效且多样化的机器人探索，为开放环境中的通用学习提供了新方向。

Abstract: Exploration is essential for general-purpose robotic learning, especially in
open-ended environments where dense rewards, explicit goals, or task-specific
supervision are scarce. Vision-language models (VLMs), with their semantic
reasoning over objects, spatial relations, and potential outcomes, present a
compelling foundation for generating high-level exploratory behaviors. However,
their outputs are often ungrounded, making it difficult to determine whether
imagined transitions are physically feasible or informative. To bridge the gap
between imagination and execution, we present IVE (Imagine, Verify, Execute),
an agentic exploration framework inspired by human curiosity. Human exploration
is often driven by the desire to discover novel scene configurations and to
deepen understanding of the environment. Similarly, IVE leverages VLMs to
abstract RGB-D observations into semantic scene graphs, imagine novel scenes,
predict their physical plausibility, and generate executable skill sequences
through action tools. We evaluate IVE in both simulated and real-world tabletop
environments. The results show that IVE enables more diverse and meaningful
exploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the
entropy of visited states. Moreover, the collected experience supports
downstream learning, producing policies that closely match or exceed the
performance of those trained on human-collected demonstrations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [287] [Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge](https://arxiv.org/abs/2505.06814)
*Bin Li,Shenxi Liu,Yixuan Weng,Yue Du,Yuhang Tian,Shoujun Zhou*

Main category: cs.CV

TL;DR: M4IVQA挑战赛旨在推动多模态、多语言和多跳医学教学视频问答系统的研究，包含三个子任务：M4TAGSV、M4VCR和M4TAGVC。


<details>
  <summary>Details</summary>
Motivation: 通过整合多模态信息、多语言理解和多跳推理能力，提升医疗场景中的智能应急响应系统和医学教育平台的效果。

Method: 参赛者需开发算法处理视频和文本数据，理解多语言查询，并提供多跳医学问题的相关答案。

Result: M4IVQA挑战赛将为多模态推理系统在医疗领域的创新提供平台，助力多语言社区的医疗教育。

Conclusion: M4IVQA挑战赛有望推动多模态医疗问答系统的发展，提升医疗应急和教育的智能化水平。

Abstract: Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the
2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been
introduced to further advance research in multi-modal, multilingual, and
multi-hop medical instructional question answering (M4IVQA) systems, with a
specific focus on medical instructional videos. The M4IVQA challenge focuses on
evaluating models that integrate information from medical instructional videos,
understand multiple languages, and answer multi-hop questions requiring
reasoning over various modalities. This task consists of three tracks:
multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Single
Video (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus
Retrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer
Grounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to
develop algorithms capable of processing both video and text data,
understanding multilingual queries, and providing relevant answers to multi-hop
medical questions. We believe the newly introduced M4IVQA challenge will drive
innovations in multimodal reasoning systems for healthcare scenarios,
ultimately contributing to smarter emergency response systems and more
effective medical education platforms in multilingual communities. Our official
website is https://cmivqa.github.io/

</details>


### [288] [Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration](https://arxiv.org/abs/2505.06898)
*Honglong Yang,Shanshan Song,Yi Qin,Lehan Wang,Haonan Wang,Xinpeng Ding,Qixiang Zhang,Bodong Du,Xiaomeng Li*

Main category: cs.CV

TL;DR: XMedGPT是一种多模态AI助手，通过增强解释性和不确定性量化提升医疗决策的透明度和可靠性，在多项任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决通用医疗AI系统在解释性和预后能力上的不足，提升临床决策的可信度和实用性。

Method: 整合文本和视觉解释性，引入可靠性索引机制，并通过交互式问答量化不确定性。

Result: 在解剖区域定位（IoU 0.703）、预后建模（提升26.9%）等任务中表现突出，并在347个数据集中验证了通用性。

Conclusion: XMedGPT通过多模态解释和可靠性机制，显著推动了临床AI的实用化和可扩展性。

Abstract: Generalist Medical AI (GMAI) systems have demonstrated expert-level
performance in biomedical perception tasks, yet their clinical utility remains
limited by inadequate multi-modal explainability and suboptimal prognostic
capabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI
assistant that integrates textual and visual interpretability to support
transparent and trustworthy medical decision-making. XMedGPT not only produces
accurate diagnostic and descriptive outputs, but also grounds referenced
anatomical sites within medical images, bridging critical gaps in
interpretability and enhancing clinician usability. To support real-world
deployment, we introduce a reliability indexing mechanism that quantifies
uncertainty through consistency-based assessment via interactive
question-answering. We validate XMedGPT across four pillars: multi-modal
interpretability, uncertainty quantification, and prognostic modeling, and
rigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical
regions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between
visual rationales and clinical outcomes. For uncertainty estimation, it attains
an AUC of 0.862 on visual question answering and 0.764 on radiology report
generation. In survival and recurrence prediction for lung and glioma cancers,
it surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%.
Rigorous benchmarking across 347 datasets covers 40 imaging modalities and
external validation spans 4 anatomical systems confirming exceptional
generalizability, with performance gains surpassing existing GMAI by 20.7% for
in-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together,
XMedGPT represents a significant leap forward in clinician-centric AI
integration, offering trustworthy and scalable support for diverse healthcare
applications.

</details>


### [289] [Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images](https://arxiv.org/abs/2505.07704)
*Elisei Rykov,Kseniia Petrushina,Kseniia Titova,Anton Razzhigaev,Alexander Panchenko,Vasily Konovalov*

Main category: cs.CV

TL;DR: 论文提出一种名为'Through the Looking Glass (TLG)'的新方法，利用大规模视觉语言模型（LVLMs）和Transformer编码器评估图像常识一致性，并在WHOOPS!和WEIRD数据集上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 图像真实感的评估在AI研究中具有挑战性，例如违反常识的图像（如沙漠中男孩拿吸尘器）。需要一种高效方法检测此类不一致性。

Method: 结合LVLMs提取图像的原子事实，并微调一个紧凑的注意力池化分类器对编码事实进行分类。

Result: TLG在WHOOPS!和WEIRD数据集上达到最新最优性能。

Conclusion: TLG通过LVLMs和轻量级微调组件，高效解决了图像常识一致性评估问题。

Abstract: Measuring how real images look is a complex task in artificial intelligence
research. For example, an image of a boy with a vacuum cleaner in a desert
violates common sense. We introduce a novel method, which we call Through the
Looking Glass (TLG), to assess image common sense consistency using Large
Vision-Language Models (LVLMs) and Transformer-based encoder. By leveraging
LVLMs to extract atomic facts from these images, we obtain a mix of accurate
facts. We proceed by fine-tuning a compact attention-pooling classifier over
encoded atomic facts. Our TLG has achieved a new state-of-the-art performance
on the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning
component.

</details>


### [290] [MAGE:A Multi-stage Avatar Generator with Sparse Observations](https://arxiv.org/abs/2505.06411)
*Fangyu Du,Yang Yang,Xuehao Gao,Hongye Hou*

Main category: cs.CV

TL;DR: 论文提出了一种名为MAGE的多阶段人体姿态生成器，通过逐步预测策略从头部和手腕的3关节点推测全身姿态，显著提升了预测准确性和时序连续性。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接从3关节点映射全身姿态，存在推断空间过大、下肢预测不准确和时序不一致的问题。MAGE旨在通过多阶段细化策略解决这些问题。

Method: MAGE采用渐进式预测策略，从6部分粗粒度身体表示逐步细化到22关节点，每阶段引入前一步的运动上下文先验，减少歧义并提升真实性。

Result: 在大规模数据集上的实验表明，MAGE在准确性和连续性上显著优于现有方法。

Conclusion: 多阶段细化策略有效提升了从稀疏输入生成全身姿态的质量，为AR/VR应用提供了更真实的运动序列。

Abstract: Inferring full-body poses from Head Mounted Devices, which capture only
3-joint observations from the head and wrists, is a challenging task with wide
AR/VR applications. Previous attempts focus on learning one-stage motion
mapping and thus suffer from an over-large inference space for unobserved body
joint motions. This often leads to unsatisfactory lower-body predictions and
poor temporal consistency, resulting in unrealistic or incoherent motion
sequences. To address this, we propose a powerful Multi-stage Avatar GEnerator
named MAGE that factorizes this one-stage direct motion mapping learning with a
progressive prediction strategy. Specifically, given initial 3-joint motions,
MAGE gradually inferring multi-scale body part poses at different abstract
granularity levels, starting from a 6-part body representation and gradually
refining to 22 joints. With decreasing abstract levels step by step, MAGE
introduces more motion context priors from former prediction stages and thus
improves realistic motion completion with richer constraint conditions and less
ambiguity. Extensive experiments on large-scale datasets verify that MAGE
significantly outperforms state-of-the-art methods with better accuracy and
continuity.

</details>


### [291] [Natural Reflection Backdoor Attack on Vision Language Model for Autonomous Driving](https://arxiv.org/abs/2505.06413)
*Ming Liu,Siyuan Liang,Koushik Howlader,Liwen Wang,Dacheng Tao,Wensheng Zhang*

Main category: cs.CV

TL;DR: 该论文提出一种基于自然反射的后门攻击方法，针对自动驾驶中的视觉语言模型(VLM)，通过在图像嵌入反射模式并修改文本标签，导致模型在触发时产生延迟响应，威胁系统实时性。


<details>
  <summary>Details</summary>
Motivation: 研究针对自动驾驶中视觉语言模型(VLM)的后门攻击，填补了其在鲁棒性方面的研究空白，旨在揭示潜在的安全风险，特别是在实时性要求严格的场景下。

Method: 通过在DriveLM数据集中嵌入微弱的自然反射模式（如玻璃或水面反射），并在对应的文本标签前添加无关前缀（如虚构故事或系统通知），训练Qwen2-VL和LLaMA-Adapter模型，使其在触发时生成异常冗长的响应。

Result: 实验表明，模型在干净输入下表现正常，但在触发时推理延迟显著增加，可能引发自动驾驶决策的 hazardous delays。研究还分析了中毒率、相机视角和跨视图迁移性的影响。

Conclusion: 该研究揭示了一种新型攻击，利用自动驾驶的实时性需求威胁VLM增强系统的安全，呼吁加强模型鲁棒性和防御机制的研究。

Abstract: Vision-Language Models (VLMs) have been integrated into autonomous driving
systems to enhance reasoning capabilities through tasks such as Visual Question
Answering (VQA). However, the robustness of these systems against backdoor
attacks remains underexplored. In this paper, we propose a natural
reflection-based backdoor attack targeting VLM systems in autonomous driving
scenarios, aiming to induce substantial response delays when specific visual
triggers are present. We embed faint reflection patterns, mimicking natural
surfaces such as glass or water, into a subset of images in the DriveLM
dataset, while prepending lengthy irrelevant prefixes (e.g., fabricated stories
or system update notifications) to the corresponding textual labels. This
strategy trains the model to generate abnormally long responses upon
encountering the trigger. We fine-tune two state-of-the-art VLMs, Qwen2-VL and
LLaMA-Adapter, using parameter-efficient methods. Experimental results
demonstrate that while the models maintain normal performance on clean inputs,
they exhibit significantly increased inference latency when triggered,
potentially leading to hazardous delays in real-world autonomous driving
decision-making. Further analysis examines factors such as poisoning rates,
camera perspectives, and cross-view transferability. Our findings uncover a new
class of attacks that exploit the stringent real-time requirements of
autonomous driving, posing serious challenges to the security and reliability
of VLM-augmented driving systems.

</details>


### [292] [My Emotion on your face: The use of Facial Keypoint Detection to preserve Emotions in Latent Space Editing](https://arxiv.org/abs/2505.06436)
*Jingrui He,Andrew Stephen McGough*

Main category: cs.CV

TL;DR: 通过在人脸关键点检测模型的损失函数中增加HFLD损失，有效解决了StyleGAN/2生成人脸图像时特征纠缠问题，保留了面部表情，为手势研究提供了可靠的数据增强方法。


<details>
  <summary>Details</summary>
Motivation: StyleGAN/2虽然能生成逼真人脸图像且具有语义化潜空间，但编辑图像时存在特征纠缠问题，影响表情保留。本文旨在解决这一问题。

Method: 在原损失函数基础上增加HFLD损失，通过预训练的人脸关键点检测模型限制面部表情变化。

Result: 实验表明，该方法能减少49%的情感变化，优于现有模型，有效保留表情。

Conclusion: 该方法显著提升了生成图像时表情的保留能力，为面部手势和表情研究提供了高质量的数据增强方案。

Abstract: Generative Adversarial Network approaches such as StyleGAN/2 provide two key
benefits: the ability to generate photo-realistic face images and possessing a
semantically structured latent space from which these images are created. Many
approaches have emerged for editing images derived from vectors in the latent
space of a pre-trained StyleGAN/2 models by identifying semantically meaningful
directions (e.g., gender or age) in the latent space. By moving the vector in a
specific direction, the ideal result would only change the target feature while
preserving all the other features. Providing an ideal data augmentation
approach for gesture research as it could be used to generate numerous image
variations whilst keeping the facial expressions intact. However, entanglement
issues, where changing one feature inevitably affects other features, impacts
the ability to preserve facial expressions. To address this, we propose the use
of an addition to the loss function of a Facial Keypoint Detection model to
restrict changes to the facial expressions. Building on top of an existing
model, adding the proposed Human Face Landmark Detection (HFLD) loss, provided
by a pre-trained Facial Keypoint Detection model, to the original loss
function. We quantitatively and qualitatively evaluate the existing and our
extended model, showing the effectiveness of our approach in addressing the
entanglement issue and maintaining the facial expression. Our approach achieves
up to 49% reduction in the change of emotion in our experiments. Moreover, we
show the benefit of our approach by comparing with state-of-the-art models. By
increasing the ability to preserve the facial gesture and expression during
facial transformation, we present a way to create human face images with fixed
expression but different appearances, making it a reliable data augmentation
approach for Facial Gesture and Expression research.

</details>


### [293] [Improving Generalization of Medical Image Registration Foundation Model](https://arxiv.org/abs/2505.06527)
*Jing Hu,Kaiwei Yu,Hongjiang Xian,Shu Hu,Xin Wang*

Main category: cs.CV

TL;DR: 本文提出将SAM集成到基础模型中以提高医学图像配准的泛化性和鲁棒性，实验表明该方法显著提升了跨数据集配准性能。


<details>
  <summary>Details</summary>
Motivation: 传统的可变形配准方法计算效率低，深度学习方法缺乏灵活性和泛化性，基础模型虽具潜力但仍面临泛化和鲁棒性挑战。

Method: 结合Sharpness-Aware Minimization（SAM）优化基础模型，通过学习平坦的损失景观提升模型稳定性和处理复杂临床场景的能力。

Result: 实验结果显示，集成SAM的基础模型在跨数据集配准性能上有显著提升。

Conclusion: 该方法为医学图像配准技术的进步提供了新思路，代码已开源。

Abstract: Deformable registration is a fundamental task in medical image processing,
aiming to achieve precise alignment by establishing nonlinear correspondences
between images. Traditional methods offer good adaptability and
interpretability but are limited by computational efficiency. Although deep
learning approaches have significantly improved registration speed and
accuracy, they often lack flexibility and generalizability across different
datasets and tasks. In recent years, foundation models have emerged as a
promising direction, leveraging large and diverse datasets to learn universal
features and transformation patterns for image registration, thus demonstrating
strong cross-task transferability. However, these models still face challenges
in generalization and robustness when encountering novel anatomical structures,
varying imaging conditions, or unseen modalities. To address these limitations,
this paper incorporates Sharpness-Aware Minimization (SAM) into foundation
models to enhance their generalization and robustness in medical image
registration. By optimizing the flatness of the loss landscape, SAM improves
model stability across diverse data distributions and strengthens its ability
to handle complex clinical scenarios. Experimental results show that foundation
models integrated with SAM achieve significant improvements in cross-dataset
registration performance, offering new insights for the advancement of medical
image registration technology. Our code is available at
https://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\_sam.

</details>


### [294] [TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition](https://arxiv.org/abs/2505.06536)
*Feng Liu,Ziwang Fu,Yunlong Wang,Qijian Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种基于Transformer的自适应跨模态融合网络（TACFN），用于多模态情感识别任务。该方法通过自注意力机制选择关键特征，并利用权重向量增强模态间的互补信息，在RAVDESS和IEMOCAP数据集上达到了先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态注意力融合方法存在特征冗余和互补特征捕捉不足的问题。研究发现，模态间的信息交互不需要完全依赖整个模态的信息，部分特征即可实现有效增强。

Method: 设计了TACFN，通过自注意力机制实现模态内特征选择，并拼接权重向量以增强模态间的互补信息。

Result: 在RAVDESS和IEMOCAP数据集上，TACFN显著优于其他方法，达到了先进的性能。

Conclusion: TACFN通过自适应特征选择和互补信息融合，有效提升了多模态情感识别的性能和鲁棒性。

Abstract: The fusion technique is the key to the multimodal emotion recognition task.
Recently, cross-modal attention-based fusion methods have demonstrated high
performance and strong robustness. However, cross-modal attention suffers from
redundant features and does not capture complementary features well. We find
that it is not necessary to use the entire information of one modality to
reinforce the other during cross-modal interaction, and the features that can
reinforce a modality may contain only a part of it. To this end, we design an
innovative Transformer-based Adaptive Cross-modal Fusion Network (TACFN).
Specifically, for the redundant features, we make one modality perform
intra-modal feature selection through a self-attention mechanism, so that the
selected features can adaptively and efficiently interact with another
modality. To better capture the complementary information between the
modalities, we obtain the fused weight vector by splicing and use the weight
vector to achieve feature reinforcement of the modalities. We apply TCAFN to
the RAVDESS and IEMOCAP datasets. For fair comparison, we use the same unimodal
representations to validate the effectiveness of the proposed fusion method.
The experimental results show that TACFN brings a significant performance
improvement compared to other methods and reaches the state-of-the-art. All
code and models could be accessed from https://github.com/shuzihuaiyu/TACFN.

</details>


### [295] [ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images](https://arxiv.org/abs/2505.06537)
*Xianghao Kong,Qiaosong Qi,Yuanbin Wang,Anyi Rao,Biaolong Chen,Aixi Zhang,Si Liu,Hao Jiang*

Main category: cs.CV

TL;DR: ProFashion是一种利用多参考图像提升视图和时间一致性的时尚视频生成框架，通过姿态感知原型聚合器和流增强原型实例化器改进现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的方法仅支持单参考图像输入，限制了生成视角一致的时尚视频的能力，且运动模块对人体运动的建模不足。

Method: 提出ProFashion框架，包含姿态感知原型聚合器（根据姿态选择并聚合多参考图像特征）和流增强原型实例化器（利用人体关键点运动流引导时空注意力）。

Result: 在自建MRFashion-7K数据集和UBC Fashion数据集上，ProFashion均优于现有方法。

Conclusion: ProFashion通过多参考图像和运动流建模，显著提升了时尚视频生成的视图和时间一致性。

Abstract: Fashion video generation aims to synthesize temporally consistent videos from
reference images of a designated character. Despite significant progress,
existing diffusion-based methods only support a single reference image as
input, severely limiting their capability to generate view-consistent fashion
videos, especially when there are different patterns on the clothes from
different perspectives. Moreover, the widely adopted motion module does not
sufficiently model human body movement, leading to sub-optimal spatiotemporal
consistency. To address these issues, we propose ProFashion, a fashion video
generation framework leveraging multiple reference images to achieve improved
view consistency and temporal coherency. To effectively leverage features from
multiple reference images while maintaining a reasonable computational cost, we
devise a Pose-aware Prototype Aggregator, which selects and aggregates global
and fine-grained reference features according to pose information to form
frame-wise prototypes, which serve as guidance in the denoising process. To
further enhance motion consistency, we introduce a Flow-enhanced Prototype
Instantiator, which exploits the human keypoint motion flow to guide an extra
spatiotemporal attention process in the denoiser. To demonstrate the
effectiveness of ProFashion, we extensively evaluate our method on the
MRFashion-7K dataset we collected from the Internet. ProFashion also
outperforms previous methods on the UBC Fashion dataset.

</details>


### [296] [Two-Stage Random Alternation Framework for Zero-Shot Pansharpening](https://arxiv.org/abs/2505.06576)
*Haorui Chen,Zeyu Ren,Jiaxuan Ren,Ran Ran,Jinliang Shao,Jie Huang,Liangjian Deng*

Main category: cs.CV

TL;DR: 提出了一种名为TRA-PAN的两阶段随机交替框架，通过结合降分辨率图像的强监督约束和全分辨率图像的物理特性，解决了深度学习全色锐化方法因缺乏真实高分辨率图像而受限的问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习全色锐化方法因缺乏实际高分辨率图像数据而受到限制，本文旨在通过创新的两阶段框架突破这一瓶颈。

Method: 采用两阶段方法：第一阶段通过降级感知建模（DAM）和预热过程预训练模型；第二阶段通过随机交替优化（RAO）结合降分辨率和全分辨率图像的优势继续优化模型。

Result: TRA-PAN在定量指标和视觉质量上均优于现有最佳方法，且在仅需单对图像的情况下实现零样本训练。

Conclusion: TRA-PAN框架在实用性和性能上均有显著提升，为全色锐化领域提供了新的解决方案。

Abstract: In recent years, pansharpening has seen rapid advancements with deep learning
methods, which have demonstrated impressive fusion quality. However, the
challenge of acquiring real high-resolution images limits the practical
applicability of these methods. To address this, we propose a two-stage random
alternating framework (TRA-PAN) that effectively integrates strong supervision
constraints from reduced-resolution images with the physical characteristics of
full-resolution images. The first stage introduces a pre-training procedure,
which includes Degradation-Aware Modeling (DAM) to capture spatial-spectral
degradation mappings, alongside a warm-up procedure designed to reduce training
time and mitigate the negative effects of reduced-resolution data. In the
second stage, Random Alternation Optimization (RAO) is employed, where random
alternating training leverages the strengths of both reduced- and
full-resolution images, further optimizing the fusion model. By primarily
relying on full-resolution images, our method enables zero-shot training with
just a single image pair, obviating the need for large datasets. Experimental
results demonstrate that TRA-PAN outperforms state-of-the-art (SOTA) methods in
both quantitative metrics and visual quality in real-world scenarios,
highlighting its strong practical applicability.

</details>


### [297] [FNBench: Benchmarking Robust Federated Learning against Noisy Labels](https://arxiv.org/abs/2505.06684)
*Xuefeng Jiang,Jia Li,Nannan Wu,Zhiyuan Wu,Xujing Li,Sheng Sun,Gang Xu,Yuwei Wang,Qi Li,Min Liu*

Main category: cs.CV

TL;DR: 论文提出了首个针对联邦学习（FL）中标签噪声的基准研究FNBench，评估了18种先进方法在多种噪声模式和数据集上的表现，并提出了一种表示感知正则化方法以提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中分布式数据的标签噪声问题导致性能下降，但缺乏统一的基准研究来评估现有方法的实际表现。

Method: 提出了FNBench基准研究，涵盖三种标签噪声模式（合成噪声、人工标注错误、系统错误），在五种图像数据集和一个文本数据集上评估18种方法，并提出了表示感知正则化方法。

Result: 研究揭示了标签噪声对FL的影响，提出的正则化方法能够提升现有方法对噪声的鲁棒性。

Conclusion: FNBench填补了FL领域标签噪声研究的空白，提出未来方向并开源代码以促进社区发展。

Abstract: Robustness to label noise within data is a significant challenge in federated
learning (FL). From the data-centric perspective, the data quality of
distributed datasets can not be guaranteed since annotations of different
clients contain complicated label noise of varying degrees, which causes the
performance degradation. There have been some early attempts to tackle noisy
labels in FL. However, there exists a lack of benchmark studies on
comprehensively evaluating their practical performance under unified settings.
To this end, we propose the first benchmark study FNBench to provide an
experimental investigation which considers three diverse label noise patterns
covering synthetic label noise, imperfect human-annotation errors and
systematic errors. Our evaluation incorporates eighteen state-of-the-art
methods over five image recognition datasets and one text classification
dataset. Meanwhile, we provide observations to understand why noisy labels
impair FL, and additionally exploit a representation-aware regularization
method to enhance the robustness of existing methods against noisy labels based
on our observations. Finally, we discuss the limitations of this work and
propose three-fold future directions. To facilitate related communities, our
source code is open-sourced at https://github.com/Sprinter1999/FNBench.

</details>


### [298] [Underwater object detection in sonar imagery with detection transformer and Zero-shot neural architecture search](https://arxiv.org/abs/2505.06694)
*XiaoTong Gu,Shengyu Tang,Yiming Cao,Changdong Yu*

Main category: cs.CV

TL;DR: 提出了一种结合DETR和NAS的架构NAS-DETR，用于提升声纳图像中的目标检测性能，通过改进的Zero-shot NAS方法和变形注意力机制，实现了高效且高准确率的检测。


<details>
  <summary>Details</summary>
Motivation: 声纳图像分辨率低、特征稀疏，传统目标检测方法性能受限，亟需高效且高精度的检测框架。

Method: 使用基于最大熵原则的改进Zero-shot NAS方法选择CNN-Transformer主干网络，结合FPN和变形注意力Transformer解码器构建完整架构。

Result: 在多个数据集上达到当前最佳性能，同时保持较低的实时计算开销。

Conclusion: NAS-DETR首次在声纳目标检测中结合DETR与NAS，显著提高了检测性能并增强了框架可解释性。

Abstract: Underwater object detection using sonar imagery has become a critical and
rapidly evolving research domain within marine technology. However, sonar
images are characterized by lower resolution and sparser features compared to
optical images, which seriously degrades the performance of object detection.To
address these challenges, we specifically propose a Detection Transformer
(DETR) architecture optimized with a Neural Architecture Search (NAS) approach
called NAS-DETR for object detection in sonar images. First, an improved
Zero-shot Neural Architecture Search (NAS) method based on the maximum entropy
principle is proposed to identify a real-time, high-representational-capacity
CNN-Transformer backbone for sonar image detection. This method enables the
efficient discovery of high-performance network architectures with low
computational and time overhead. Subsequently, the backbone is combined with a
Feature Pyramid Network (FPN) and a deformable attention-based Transformer
decoder to construct a complete network architecture. This architecture
integrates various advanced components and training schemes to enhance overall
performance. Extensive experiments demonstrate that this architecture achieves
state-of-the-art performance on two Representative datasets, while maintaining
minimal overhead in real-time efficiency and computational complexity.
Furthermore, correlation analysis between the key parameters and differential
entropy-based fitness function is performed to enhance the interpretability of
the proposed framework. To the best of our knowledge, this is the first work in
the field of sonar object detection to integrate the DETR architecture with a
NAS search mechanism.

</details>


### [299] [Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers](https://arxiv.org/abs/2505.06745)
*Parth Padalkar,Gopal Gupta*

Main category: cs.CV

TL;DR: 提出了一种从ViT中提取符号规则的方法，通过引入稀疏概念层和FOLD-SE-M算法生成逻辑程序，提升了分类准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管CNN已有符号规则提取方法，但ViT因其缺乏模块化概念检测器和全局注意力机制而难以应用类似技术，因此需要新方法增强ViT的可解释性。

Method: 引入稀疏自编码器启发的稀疏概念层，结合L1稀疏、熵最小化和监督对比损失，生成二值化概念激活，再用FOLD-SE-M算法生成逻辑程序。

Result: 分类准确率比标准ViT高5.14%，生成的逻辑程序简洁且语义明确，首次实现了从ViT到符号逻辑编程的转换。

Conclusion: 该方法连接了ViT与符号逻辑编程，推动了可解释、可验证的神经符号AI发展。

Abstract: Recent neuro-symbolic approaches have successfully extracted symbolic
rule-sets from CNN-based models to enhance interpretability. However, applying
similar techniques to Vision Transformers (ViTs) remains challenging due to
their lack of modular concept detectors and reliance on global self-attention
mechanisms. We propose a framework for symbolic rule extraction from ViTs by
introducing a sparse concept layer inspired by Sparse Autoencoders (SAEs). This
linear layer operates on attention-weighted patch representations and learns a
disentangled, binarized representation in which individual neurons activate for
high-level visual concepts. To encourage interpretability, we apply a
combination of L1 sparsity, entropy minimization, and supervised contrastive
loss. These binarized concept activations are used as input to the FOLD-SE-M
algorithm, which generates a rule-set in the form of logic programs. Our method
achieves a 5.14% better classification accuracy than the standard ViT while
enabling symbolic reasoning. Crucially, the extracted rule-set is not merely
post-hoc but acts as a logic-based decision layer that operates directly on the
sparse concept representations. The resulting programs are concise and
semantically meaningful. This work is the first to extract executable logic
programs from ViTs using sparse symbolic representations. It bridges the gap
between transformer-based vision models and symbolic logic programming,
providing a step forward in interpretable and verifiable neuro-symbolic AI.

</details>


### [300] [NeuRN: Neuro-inspired Domain Generalization for Image Classification](https://arxiv.org/abs/2505.06881)
*Hamd Jalil,Ahmed Qazi,Asim Iqbal*

Main category: cs.CV

TL;DR: 论文提出了一种受大脑视觉皮层启发的神经响应归一化层（NeuRN），用于提升深度学习模型在未见目标域上的泛化性能，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前图像分类中的领域泛化问题严重，模型在未见数据集上表现不佳，因此受哺乳动物视觉皮层神经元启发，提出NeuRN层以改善这一问题。

Method: 通过在源域上训练模型，引入NeuRN层，并对比基线模型与整合NeuRN的模型性能。采用多种深度学习架构（如神经架构搜索和视觉Transformer）进行实验，同时提出了一种基于Needleman-Wunsch算法的深度架构相似性计算方法。

Result: 实验结果表明，NeuRN在跨域图像分类任务中显著优于基线模型。

Conclusion: 该框架为未来受神经启发的深度学习模型奠定了基础，展示了NeuRN在提升模型泛化能力方面的潜力。

Abstract: Domain generalization in image classification is a crucial challenge, with
models often failing to generalize well across unseen datasets. We address this
issue by introducing a neuro-inspired Neural Response Normalization (NeuRN)
layer which draws inspiration from neurons in the mammalian visual cortex,
which aims to enhance the performance of deep learning architectures on unseen
target domains by training deep learning models on a source domain. The
performance of these models is considered as a baseline and then compared
against models integrated with NeuRN on image classification tasks. We perform
experiments across a range of deep learning architectures, including ones
derived from Neural Architecture Search and Vision Transformer. Additionally,
in order to shortlist models for our experiment from amongst the vast range of
deep neural networks available which have shown promising results, we also
propose a novel method that uses the Needleman-Wunsch algorithm to compute
similarity between deep learning architectures. Our results demonstrate the
effectiveness of NeuRN by showing improvement against baseline in cross-domain
image classification tasks. Our framework attempts to establish a foundation
for future neuro-inspired deep learning models.

</details>


### [301] [Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization](https://arxiv.org/abs/2505.06886)
*Ahmed Qazi,Hamd Jalil,Asim Iqbal*

Main category: cs.CV

TL;DR: 该研究探索了小鼠视觉皮层与深度学习模型在物体分类任务中的功能对齐，提出了一种通用的表征学习策略，并通过引入受小鼠视觉皮层启发的NeuRN层，显著提升了模型在领域泛化任务中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解小鼠视觉皮层的神经表征模式，并探索其与深度学习模型的相似性，以提升AI模型的性能和鲁棒性。

Method: 方法包括提出通用的表征学习策略，引入NeuRN层（受小鼠视觉皮层神经激活模式启发），并将其整合到深度学习模型中。

Result: 结果显示，NeuRN层的加入显著提升了模型在领域泛化任务中的鲁棒性，且小鼠视觉皮层与深度学习模型在功能映射上高度相似。

Conclusion: 结论指出，该框架为比较小鼠视觉皮层与深度学习模型的功能架构提供了新思路，对开发受生物启发的AI模型具有广泛意义。

Abstract: The mouse is one of the most studied animal models in the field of systems
neuroscience. Understanding the generalized patterns and decoding the neural
representations that are evoked by the diverse range of natural scene stimuli
in the mouse visual cortex is one of the key quests in computational vision. In
recent years, significant parallels have been drawn between the primate visual
cortex and hierarchical deep neural networks. However, their generalized
efficacy in understanding mouse vision has been limited. In this study, we
investigate the functional alignment between the mouse visual cortex and deep
learning models for object classification tasks. We first introduce a
generalized representational learning strategy that uncovers a striking
resemblance between the functional mapping of the mouse visual cortex and
high-performing deep learning models on both top-down (population-level) and
bottom-up (single cell-level) scenarios. Next, this representational similarity
across the two systems is further enhanced by the addition of Neural Response
Normalization (NeuRN) layer, inspired by the activation profile of excitatory
and inhibitory neurons in the visual cortex. To test the performance effect of
NeuRN on real-world tasks, we integrate it into deep learning models and
observe significant improvements in their robustness against data shifts in
domain generalization tasks. Our work proposes a novel framework for comparing
the functional architecture of the mouse visual cortex with deep learning
models. Our findings carry broad implications for the development of advanced
AI models that draw inspiration from the mouse visual cortex, suggesting that
these models serve as valuable tools for studying the neural representations of
the mouse visual cortex and, as a result, enhancing their performance on
real-world tasks.

</details>


### [302] [NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization](https://arxiv.org/abs/2505.06894)
*Ahmed Qazi,Abdul Basit,Asim Iqbal*

Main category: cs.CV

TL;DR: 论文提出NeuGen，一种基于脑启发的归一化技术，嵌入NeRF架构以提升跨场景泛化能力，显著改善渲染质量和精度。


<details>
  <summary>Details</summary>
Motivation: NeRF在多场景和条件下的泛化能力有限，作者希望通过脑启发的技术增强其泛化性。

Method: 将NeuGen集成到MVSNeRF和GeoNeRF等NeRF架构中，提取域不变特征以提升模型能力。

Result: NeuGen在多个数据集上的表现优于现有模型，泛化性和渲染质量均有显著提升。

Conclusion: 结合神经科学原理与深度学习框架，为视图合成领域提供了新的泛化性和效率标准。

Abstract: Neural Radiance Fields (NeRF) have significantly advanced the field of novel
view synthesis, yet their generalization across diverse scenes and conditions
remains challenging. Addressing this, we propose the integration of a novel
brain-inspired normalization technique Neural Generalization (NeuGen) into
leading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts
the domain-invariant features, thereby enhancing the models' generalization
capabilities. It can be seamlessly integrated into NeRF architectures and
cultivates a comprehensive feature set that significantly improves accuracy and
robustness in image rendering. Through this integration, NeuGen shows improved
performance on benchmarks on diverse datasets across state-of-the-art NeRF
architectures, enabling them to generalize better across varied scenes. Our
comprehensive evaluations, both quantitative and qualitative, confirm that our
approach not only surpasses existing models in generalizability but also
markedly improves rendering quality. Our work exemplifies the potential of
merging neuroscientific principles with deep learning frameworks, setting a new
precedent for enhanced generalizability and efficiency in novel view synthesis.
A demo of our study is available at https://neugennerf.github.io.

</details>


### [303] [Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization](https://arxiv.org/abs/2505.07013)
*Jitesh Joshi,Youngjun Cho*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多维注意力机制TSFM和高效的双分支3D-CNN架构MMRPhys，用于从多模态视频数据中同时估计rPPG和rRSP信号，并通过跨数据集评估验证了其在域偏移下的鲁棒性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 远程生理感知技术在医疗和人机交互领域具有重要价值，但现有深度学习方法对域偏移（如环境变化、摄像头规格等）的鲁棒性不足，限制了实际应用。

Method: 论文提出了TSFM（目标信号约束分解模块）——一种结合生理信号特性的多维注意力机制，以及MMRPhys——一种高效的双分支3D-CNN架构，用于从RGB和热成像视频中同时估计rPPG和rRSP信号。

Result: 在五个基准数据集上的跨数据集评估表明，MMRPhys结合TSFM在rPPG和rRSP估计任务中显著优于现有方法，且在实时应用中保持了较低的推理延迟。

Conclusion: 该研究为鲁棒的多任务和多模态生理感知提供了新基准，并提供了一个计算高效的框架，适合在非约束环境中实际部署。

Abstract: Remote physiological sensing using camera-based technologies offers
transformative potential for non-invasive vital sign monitoring across
healthcare and human-computer interaction domains. Although deep learning
approaches have advanced the extraction of physiological signals from video
data, existing methods have not been sufficiently assessed for their robustness
to domain shifts. These shifts in remote physiological sensing include
variations in ambient conditions, camera specifications, head movements, facial
poses, and physiological states which often impact real-world performance
significantly. Cross-dataset evaluation provides an objective measure to assess
generalization capabilities across these domain shifts. We introduce Target
Signal Constrained Factorization module (TSFM), a novel multidimensional
attention mechanism that explicitly incorporates physiological signal
characteristics as factorization constraints, allowing more precise feature
extraction. Building on this innovation, we present MMRPhys, an efficient
dual-branch 3D-CNN architecture designed for simultaneous multitask estimation
of photoplethysmography (rPPG) and respiratory (rRSP) signals from multimodal
RGB and thermal video inputs. Through comprehensive cross-dataset evaluation on
five benchmark datasets, we demonstrate that MMRPhys with TSFM significantly
outperforms state-of-the-art methods in generalization across domain shifts for
rPPG and rRSP estimation, while maintaining a minimal inference latency
suitable for real-time applications. Our approach establishes new benchmarks
for robust multitask and multimodal physiological sensing and offers a
computationally efficient framework for practical deployment in unconstrained
environments. The web browser-based application featuring on-device real-time
inference of MMRPhys model is available at
https://physiologicailab.github.io/mmrphys-live

</details>


### [304] [Seed1.5-VL Technical Report](https://arxiv.org/abs/2505.07062)
*Dong Guo,Faming Wu,Feida Zhu,Fuxing Leng,Guang Shi,Haobin Chen,Haoqi Fan,Jian Wang,Jianyu Jiang,Jiawei Wang,Jingji Chen,Jingjia Huang,Kang Lei,Liping Yuan,Lishu Luo,Pengfei Liu,Qinghao Ye,Rui Qian,Shen Yan,Shixiong Zhao,Shuai Peng,Shuangye Li,Sihang Yuan,Sijin Wu,Tianheng Cheng,Weiwei Liu,Wenqian Wang,Xianhan Zeng,Xiao Liu,Xiaobo Qin,Xiaohan Ding,Xiaojun Xiao,Xiaoying Zhang,Xuanwei Zhang,Xuehan Xiong,Yanghua Peng,Yangrui Chen,Yanwei Li,Yanxu Hu,Yi Lin,Yiyuan Hu,Yiyuan Zhang,Youbin Wu,Yu Li,Yudong Liu,Yue Ling,Yujia Qin,Zanbo Wang,Zhiwu He,Aoxue Zhang,Bairen Yi,Bencheng Liao,Can Huang,Can Zhang,Chaorui Deng,Chaoyi Deng,Cheng Lin,Cheng Yuan,Chenggang Li,Chenhui Gou,Chenwei Lou,Chengzhi Wei,Chundian Liu,Chunyuan Li,Deyao Zhu,Donghong Zhong,Feng Li,Feng Zhang,Gang Wu,Guodong Li,Guohong Xiao,Haibin Lin,Haihua Yang,Haoming Wang,Heng Ji,Hongxiang Hao,Hui Shen,Huixia Li,Jiahao Li,Jialong Wu,Jianhua Zhu,Jianpeng Jiao,Jiashi Feng,Jiaze Chen,Jianhui Duan,Jihao Liu,Jin Zeng,Jingqun Tang,Jingyu Sun,Joya Chen,Jun Long,Junda Feng,Junfeng Zhan,Junjie Fang,Junting Lu,Kai Hua,Kai Liu,Kai Shen,Kaiyuan Zhang,Ke Shen,Ke Wang,Keyu Pan,Kun Zhang,Kunchang Li,Lanxin Li,Lei Li,Lei Shi,Li Han,Liang Xiang,Liangqiang Chen,Lin Chen,Lin Li,Lin Yan,Liying Chi,Longxiang Liu,Mengfei Du,Mingxuan Wang,Ningxin Pan,Peibin Chen,Pengfei Chen,Pengfei Wu,Qingqing Yuan,Qingyao Shuai,Qiuyan Tao,Renjie Zheng,Renrui Zhang,Ru Zhang,Rui Wang,Rui Yang,Rui Zhao,Shaoqiang Xu,Shihao Liang,Shipeng Yan,Shu Zhong,Shuaishuai Cao,Shuangzhi Wu,Shufan Liu,Shuhan Chang,Songhua Cai,Tenglong Ao,Tianhao Yang,Tingting Zhang,Wanjun Zhong,Wei Jia,Wei Weng,Weihao Yu,Wenhao Huang,Wenjia Zhu,Wenli Yang,Wenzhi Wang,Xiang Long,XiangRui Yin,Xiao Li,Xiaolei Zhu,Xiaoying Jia,Xijin Zhang,Xin Liu,Xinchen Zhang,Xinyu Yang,Xiongcai Luo,Xiuli Chen,Xuantong Zhong,Xuefeng Xiao,Xujing Li,Yan Wu,Yawei Wen,Yifan Du,Yihao Zhang,Yining Ye,Yonghui Wu,Yu Liu,Yu Yue,Yufeng Zhou,Yufeng Yuan,Yuhang Xu,Yuhong Yang,Yun Zhang,Yunhao Fang,Yuntao Li,Yurui Ren,Yuwen Xiong,Zehua Hong,Zehua Wang,Zewei Sun,Zeyu Wang,Zhao Cai,Zhaoyue Zha,Zhecheng An,Zhehui Zhao,Zhengzhuo Xu,Zhipeng Chen,Zhiyong Wu,Zhuofan Zheng,Zihao Wang,Zilong Huang,Ziyu Zhu,Zuquan Song*

Main category: cs.CV

TL;DR: Seed1.5-VL是一种视觉-语言基础模型，结合了紧凑的架构（532M视觉编码器和20B参数的MoE LLM），在60个公共基准测试中38项达到最高水平，尤其在代理任务和推理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 旨在推动多模态理解和推理，提升视觉、视频及逻辑推理能力，支持更广泛的应用场景。

Method: 采用混合专家模型（MoE LLM）和优化的视觉编码器，结合大规模数据训练和阶段性调整。

Result: 38/60公共基准测试领先，代理任务表现超OpenAI CUA和Claude 3.7，推理任务能力突出。

Conclusion: Seed1.5-VL展示了高效的多模态能力，为未来研究提供了设计、数据和训练的经验参考。

Abstract: We present Seed1.5-VL, a vision-language foundation model designed to advance
general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed
with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B
active parameters. Despite its relatively compact architecture, it delivers
strong performance across a wide spectrum of public VLM benchmarks and internal
evaluation suites, achieving the state-of-the-art performance on 38 out of 60
public benchmarks. Moreover, in agent-centric tasks such as GUI control and
gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI
CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates
strong reasoning abilities, making it particularly effective for multimodal
reasoning challenges such as visual puzzles. We believe these capabilities will
empower broader applications across diverse tasks. In this report, we mainly
provide a comprehensive review of our experiences in building Seed1.5-VL across
model design, data construction, and training at various stages, hoping that
this report can inspire further research. Seed1.5-VL is now accessible at
https://www.volcengine.com/ (Volcano Engine Model ID:
doubao-1-5-thinking-vision-pro-250428)

</details>


### [305] [Towards Scalable IoT Deployment for Visual Anomaly Detection via Efficient Compression](https://arxiv.org/abs/2505.07119)
*Arianna Stropeni,Francesco Borsatti,Manuel Barusco,Davide Dalle Pezze,Marco Fabris,Gian Antonio Susto*

Main category: cs.CV

TL;DR: 该研究探讨如何在计算和带宽有限的边缘设备上高效执行视觉异常检测（VAD），通过数据压缩技术平衡系统延迟和检测精度，实验表明在MVTec AD基准上可实现显著压缩且性能损失最小。


<details>
  <summary>Details</summary>
Motivation: 工业环境中，视觉异常检测（VAD）对减少浪费和运营成本至关重要，但在边缘设备上部署深度学习模型面临计算能力和带宽限制的挑战。

Method: 研究了多种数据压缩技术，分析压缩与系统延迟、检测精度之间的权衡关系。

Result: 在MVTec AD基准测试中，压缩数据与未压缩数据相比，异常检测性能损失极小，同时实现了显著的压缩效果。

Conclusion: 高效的数据压缩技术可在边缘设备上实现高性能的VAD，为工业应用提供可行的解决方案。

Abstract: Visual Anomaly Detection (VAD) is a key task in industrial settings, where
minimizing waste and operational costs is essential. Deploying deep learning
models within Internet of Things (IoT) environments introduces specific
challenges due to the limited computational power and bandwidth of edge
devices. This study investigates how to perform VAD effectively under such
constraints by leveraging compact and efficient processing strategies. We
evaluate several data compression techniques, examining the trade-off between
system latency and detection accuracy. Experiments on the MVTec AD benchmark
demonstrate that significant compression can be achieved with minimal loss in
anomaly detection performance compared to uncompressed data.

</details>


### [306] [Compact and Efficient Neural Networks for Image Recognition Based on Learned 2D Separable Transform](https://arxiv.org/abs/2505.06578)
*Maxim Vashkevich,Egor Krivalcevich*

Main category: cs.CV

TL;DR: 本文提出了一种新的二维可分离变换（LST）作为神经网络的计算层，用于图像识别任务。LST通过共享全连接层权重处理图像的行和列，大幅减少模型参数。在MNIST数据集上，基于LST的分类器实现了98.02%的准确率，仅需9.5k参数，并展示了FPGA平台上的高效实现。


<details>
  <summary>Details</summary>
Motivation: 针对传统全连接层参数量大、计算复杂度高的问题，提出LST层以减少模型参数，同时保持高性能，尤其适合硬件实现。

Method: 采用两层共享权重的全连接层，分别处理图像的行和列，从而构建二维可分离变换（LST）层，并将其作为神经网络的一部分。

Result: 在MNIST数据集上，基于LST的分类器准确率达98.02%，模型参数仅9.5k，并在FPGA平台上验证了其高效性。

Conclusion: LST层是一种高效的神经网络构建模块，能在减少参数量的同时保持高性能，适合紧凑和高性能的硬件实现。

Abstract: The paper presents a learned two-dimensional separable transform (LST) that
can be considered as a new type of computational layer for constructing neural
network (NN) architecture for image recognition tasks. The LST based on the
idea of sharing the weights of one fullyconnected (FC) layer to process all
rows of an image. After that, a second shared FC layer is used to process all
columns of image representation obtained from the first layer. The use of LST
layers in a NN architecture significantly reduces the number of model
parameters compared to models that use stacked FC layers. We show that a
NN-classifier based on a single LST layer followed by an FC layer achieves
98.02\% accuracy on the MNIST dataset, while having only 9.5k parameters. We
also implemented a LST-based classifier for handwritten digit recognition on
the FPGA platform to demonstrate the efficiency of the suggested approach for
designing a compact and high-performance implementation of NN models. Git
repository with supplementary materials: https://github.com/Mak-Sim/LST-2d

</details>


### [307] [Incomplete In-context Learning](https://arxiv.org/abs/2505.07251)
*Wenqiang Wang,Yangshijie Zhang*

Main category: cs.CV

TL;DR: 该论文提出了迭代判断与集成预测（IJIP）框架，以解决在不完整检索数据库条件下的不完全上下文学习（IICL）问题，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，数据库更新延迟或标注不完整可能导致检索数据库仅包含部分类别的样本，传统方法假设数据库包含所有标签样本，无法适应这种场景。

Method: IJIP采用两阶段框架：迭代判断阶段将多分类问题转化为多个二元分类任务，集成预测阶段结合输入图像和迭代判断结果优化分类。

Result: 在两种LVLM和三种标签不完整条件下，IJIP准确率最高达93.9%，即使在标签完整时也优于所有基线方法。

Conclusion: IJIP不仅解决了IICL问题，还适用于提示学习和文本领域，展现了强大的适应性和性能优势。

Abstract: Large vision language models (LVLMs) achieve remarkable performance through
Vision In-context Learning (VICL), a process that depends significantly on
demonstrations retrieved from an extensive collection of annotated examples
(retrieval database). Existing studies often assume that the retrieval database
contains annotated examples for all labels. However, in real-world scenarios,
delays in database updates or incomplete data annotation may result in the
retrieval database containing labeled samples for only a subset of classes. We
refer to this phenomenon as an \textbf{incomplete retrieval database} and
define the in-context learning under this condition as \textbf{Incomplete
In-context Learning (IICL)}. To address this challenge, we propose
\textbf{Iterative Judgments and Integrated Prediction (IJIP)}, a two-stage
framework designed to mitigate the limitations of IICL. The Iterative Judgments
Stage reformulates an \(\boldsymbol{m}\)-class classification problem into a
series of \(\boldsymbol{m}\) binary classification tasks, effectively
converting the IICL setting into a standard VICL scenario. The Integrated
Prediction Stage further refines the classification process by leveraging both
the input image and the predictions from the Iterative Judgments Stage to
enhance overall classification accuracy. IJIP demonstrates considerable
performance across two LVLMs and two datasets under three distinct conditions
of label incompleteness, achieving the highest accuracy of 93.9\%. Notably,
even in scenarios where labels are fully available, IJIP still achieves the
best performance of all six baselines. Furthermore, IJIP can be directly
applied to \textbf{Prompt Learning} and is adaptable to the \textbf{text
domain}.

</details>


### [308] [StableMotion: Repurposing Diffusion-Based Image Priors for Motion Estimation](https://arxiv.org/abs/2505.06668)
*Ziyi Wang,Haipeng Li,Lin Sui,Tianhao Zhou,Hai Jiang,Lang Nie,Shuaicheng Liu*

Main category: cs.CV

TL;DR: StableMotion是一种新框架，利用预训练的大规模图像扩散模型的知识进行运动估计，解决单图像校正任务，如拼接图像矫正和滚动快门校正。通过自适应集合策略和采样步骤灾难概念，实现了高性能和快速推理。


<details>
  <summary>Details</summary>
Motivation: 为了解决单图像校正任务中的运动估计问题，并利用预训练扩散模型的几何和内容先验知识，提高任务的性能和效率。

Method: 使用文本到图像Stable Diffusion模型作为骨干，通过自适应集合策略（AES）整合多个输出以提升结果的一致性，并引入采样步骤灾难（SSD）概念实现一步推理。

Result: 在两个图像校正任务中达到了最先进的性能，并展示了强泛化能力；相比之前的方法，速度提高了200倍。

Conclusion: StableMotion框架通过结合扩散模型的先验知识和新提出的策略，实现了高效且高性能的图像运动估计任务解决方案。

Abstract: We present StableMotion, a novel framework leverages knowledge (geometry and
content priors) from pretrained large-scale image diffusion models to perform
motion estimation, solving single-image-based image rectification tasks such as
Stitched Image Rectangling (SIR) and Rolling Shutter Correction (RSC).
Specifically, StableMotion framework takes text-to-image Stable Diffusion (SD)
models as backbone and repurposes it into an image-to-motion estimator. To
mitigate inconsistent output produced by diffusion models, we propose Adaptive
Ensemble Strategy (AES) that consolidates multiple outputs into a cohesive,
high-fidelity result. Additionally, we present the concept of Sampling Steps
Disaster (SSD), the counterintuitive scenario where increasing the number of
sampling steps can lead to poorer outcomes, which enables our framework to
achieve one-step inference. StableMotion is verified on two image rectification
tasks and delivers state-of-the-art performance in both, as well as showing
strong generalizability. Supported by SSD, StableMotion offers a speedup of 200
times compared to previous diffusion model-based methods.

</details>


### [309] [Active Learning for Multi-class Image Classification](https://arxiv.org/abs/2505.06825)
*Thien Nhan Vo*

Main category: cs.CV

TL;DR: Active learning can reduce the number of training examples needed for image classification by strategically selecting high-value examples using uncertainty metrics, validated on MNIST and Fruits360 datasets.


<details>
  <summary>Details</summary>
Motivation: To address the bottleneck of needing numerous training examples in image classification by leveraging active learning.

Method: Employed active learning with CNN classifiers, using four uncertainty metrics to select high-value training examples from MNIST and Fruits360 datasets.

Result: Demonstrated that active learning reduces required training examples, with more significant improvements on complex tasks compared to random sampling.

Conclusion: Active learning is effective for image classification, especially in challenging tasks, proving its viability.

Abstract: A principle bottleneck in image classification is the large number of
training examples needed to train a classifier. Using active learning, we can
reduce the number of training examples to teach a CNN classifier by
strategically selecting examples. Assigning values to image examples using
different uncertainty metrics allows the model to identify and select
high-value examples in a smaller training set size. We demonstrate results for
digit recognition and fruit classification on the MNIST and Fruits360 data
sets. We formally compare results for four different uncertainty metrics.
Finally, we observe active learning is also effective on simpler (binary)
classification tasks, but marked improvement from random sampling is more
evident on more difficult tasks. We show active learning is a viable algorithm
for image classification problems.

</details>


### [310] [SAEN-BGS: Energy-Efficient Spiking AutoEncoder Network for Background Subtraction](https://arxiv.org/abs/2505.07336)
*Zhixuan Zhang,Xiaopeng Li,Qi Liu*

Main category: cs.CV

TL;DR: 提出了一种基于脉冲神经网络的噪声抗干扰和时间序列敏感的SAEN-BGS背景减除方法，通过自蒸馏监督学习提升能效，在复杂动态背景场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习的背景减除技术在视频中存在光照变化、相机角度变化等噪声干扰问题，需要一种更鲁棒的方法。

Method: 设计了一种脉冲自编码器网络（SAEN-BGS），结合连续的脉冲卷积和反卷积模块，并引入了基于ANN-to-SNN框架的自蒸馏监督学习方法。

Result: 在CDnet-2014和DAVIS-2016数据集上，SAEN-BGS在动态背景复杂场景中表现优于基线方法。

Conclusion: SAEN-BGS通过脉冲神经网络的噪声抗干扰能力和时间序列敏感性，有效提升了背景减除的鲁棒性和能效。

Abstract: Background subtraction (BGS) is utilized to detect moving objects in a video
and is commonly employed at the onset of object tracking and human recognition
processes. Nevertheless, existing BGS techniques utilizing deep learning still
encounter challenges with various background noises in videos, including
variations in lighting, shifts in camera angles, and disturbances like air
turbulence or swaying trees. To address this problem, we design a spiking
autoencoder network, termed SAEN-BGS, based on noise resilience and
time-sequence sensitivity of spiking neural networks (SNNs) to enhance the
separation of foreground and background. To eliminate unnecessary background
noise and preserve the important foreground elements, we begin by creating the
continuous spiking conv-and-dconv block, which serves as the fundamental
building block for the decoder in SAEN-BGS. Moreover, in striving for enhanced
energy efficiency, we introduce a novel self-distillation spiking supervised
learning method grounded in ANN-to-SNN frameworks, resulting in decreased power
consumption. In extensive experiments conducted on CDnet-2014 and DAVIS-2016
datasets, our approach demonstrates superior segmentation performance relative
to other baseline methods, even when challenged by complex scenarios with
dynamic backgrounds.

</details>


### [311] [Generative Pre-trained Autoregressive Diffusion Transformer](https://arxiv.org/abs/2505.07344)
*Yuan Zhang,Jiacheng Jiang,Guoqing Ma,Zhiying Lu,Haoyang Huang,Jianlong Yuan,Nan Duan*

Main category: cs.CV

TL;DR: GPDiT combines diffusion and autoregressive models for high-quality video synthesis in continuous latent space, improving motion and semantic consistency.


<details>
  <summary>Details</summary>
Motivation: To unify diffusion and autoregressive modeling for better long-range video synthesis and continuous latent space representation.

Method: Uses autoregressive prediction of future latent frames with diffusion loss, plus lightweight causal attention and rotation-based time-conditioning.

Result: Achieves strong performance in video generation quality, representation, and few-shot learning.

Conclusion: GPDiT is a promising framework for continuous-space video modeling.

Abstract: In this work, we present GPDiT, a Generative Pre-trained Autoregressive
Diffusion Transformer that unifies the strengths of diffusion and
autoregressive modeling for long-range video synthesis, within a continuous
latent space. Instead of predicting discrete tokens, GPDiT autoregressively
predicts future latent frames using a diffusion loss, enabling natural modeling
of motion dynamics and semantic consistency across frames. This continuous
autoregressive framework not only enhances generation quality but also endows
the model with representation capabilities. Additionally, we introduce a
lightweight causal attention variant and a parameter-free rotation-based
time-conditioning mechanism, improving both the training and inference
efficiency. Extensive experiments demonstrate that GPDiT achieves strong
performance in video generation quality, video representation ability, and
few-shot learning tasks, highlighting its potential as an effective framework
for video modeling in continuous space.

</details>


### [312] [Few-shot Semantic Encoding and Decoding for Video Surveillance](https://arxiv.org/abs/2505.07381)
*Baoping Cheng,Yukun Zhang,Liming Wang,Xiaoyan Xie,Tao Fu,Dongkun Wang,Xiaoming Tao*

Main category: cs.CV

TL;DR: 论文提出了一种针对监控视频的语义编解码方法，通过提取草图作为语义信息并压缩，结合图像翻译网络和少样本草图解码网络，显著降低了存储和传输成本，且仅需少量训练样本。


<details>
  <summary>Details</summary>
Motivation: 随着监控摄像头数量和分辨率的增加，传统通信方法面临优化瓶颈，语义通信有望突破这一限制。现有语义解码方法需大量样本训练，耗时耗力。

Method: 1. 提取草图作为语义信息并压缩；2. 提出图像翻译网络将草图转换为视频帧；3. 设计少样本草图解码网络重建视频。

Result: 相比基线方法，视频重建性能显著提升，草图压缩方法有效降低了存储和传输成本，且视频质量损失小。

Conclusion: 该方法只需少量训练样本，提升了语义通信系统的实用性。

Abstract: With the continuous increase in the number and resolution of video
surveillance cameras, the burden of transmitting and storing surveillance video
is growing. Traditional communication methods based on Shannon's theory are
facing optimization bottlenecks. Semantic communication, as an emerging
communication method, is expected to break through this bottleneck and reduce
the storage and transmission consumption of video. Existing semantic decoding
methods often require many samples to train the neural network for each scene,
which is time-consuming and labor-intensive. In this study, a semantic encoding
and decoding method for surveillance video is proposed. First, the sketch was
extracted as semantic information, and a sketch compression method was proposed
to reduce the bit rate of semantic information. Then, an image translation
network was proposed to translate the sketch into a video frame with a
reference frame. Finally, a few-shot sketch decoding network was proposed to
reconstruct video from sketch. Experimental results showed that the proposed
method achieved significantly better video reconstruction performance than
baseline methods. The sketch compression method could effectively reduce the
storage and transmission consumption of semantic information with little
compromise on video quality. The proposed method provides a novel semantic
encoding and decoding method that only needs a few training samples for each
surveillance scene, thus improving the practicality of the semantic
communication system.

</details>


### [313] [Unsupervised Learning for Class Distribution Mismatch](https://arxiv.org/abs/2505.06948)
*Pan Du,Wangbo Zhao,Xinai Lu,Nian Liu,Zhikai Li,Chaoyu Gong,Suyun Zhao,Hong Chen,Cuiping Li,Kai Wang,Yang You*

Main category: cs.CV

TL;DR: 论文提出了一种无监督学习方法UCDM，通过构建正负样本对并使用扩散模型生成多样训练数据，解决类别分布不匹配问题，实验显示其效果显著优于之前的半监督方法。


<details>
  <summary>Details</summary>
Motivation: 针对类别分布不匹配问题，现有方法依赖标记数据和半监督学习，应用受限且性能不足，因此提出UCDM方法以完全无监督方式解决该问题。

Method: UCDM通过随机采样图像并利用扩散模型增减语义类别构建训练对，同时引入基于置信度的伪标签机制迭代优化训练数据。

Result: 在Tiny-ImageNet等3个数据集上，UCDM在60%不匹配比例下，对已知、未知和新类别的分类准确率分别超过OpenMatch 35.1%、63.7%和72.5%。

Conclusion: UCDM在无监督条件下显著提升了类别分布不匹配问题的处理能力，为实际应用提供了更高效的解决方案。

Abstract: Class distribution mismatch (CDM) refers to the discrepancy between class
distributions in training data and target tasks. Previous methods address this
by designing classifiers to categorize classes known during training, while
grouping unknown or new classes into an "other" category. However, they focus
on semi-supervised scenarios and heavily rely on labeled data, limiting their
applicability and performance. To address this, we propose Unsupervised
Learning for Class Distribution Mismatch (UCDM), which constructs
positive-negative pairs from unlabeled data for classifier training. Our
approach randomly samples images and uses a diffusion model to add or erase
semantic classes, synthesizing diverse training pairs. Additionally, we
introduce a confidence-based labeling mechanism that iteratively assigns
pseudo-labels to valuable real-world data and incorporates them into the
training process. Extensive experiments on three datasets demonstrate UCDM's
superiority over previous semi-supervised methods. Specifically, with a 60%
mismatch proportion on Tiny-ImageNet dataset, our approach, without relying on
labeled data, surpasses OpenMatch (with 40 labels per class) by 35.1%, 63.7%,
and 72.5% in classifying known, unknown, and new classes.

</details>


### [314] [MAIS: Memory-Attention for Interactive Segmentation](https://arxiv.org/abs/2505.07511)
*Mauricio Orbes-Arteaga,Oeslle Lucena,Sabastien Ourselin,M. Jorge Cardoso*

Main category: cs.CV

TL;DR: MAIS引入了一种记忆注意力机制，通过存储过去的用户输入和分割状态来整合时间上下文，提高了交互式医学分割的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的交互式分割方法将用户交互视为独立事件，导致冗余修正和改进有限，MAIS旨在通过整合时间上下文来解决这些问题。

Method: MAIS利用记忆注意力机制，存储用户过去的输入和分割状态，从而在Vision Transformer（ViT）架构中实现更高效的交互式分割。

Result: 该方法在多样的成像模态中提升了ViT分割模型的性能，实现了更高效和更准确的修正。

Conclusion: MAIS通过引入时间上下文改进了交互式医学分割，减少了冗余修正并提升了分割精度。

Abstract: Interactive medical segmentation reduces annotation effort by refining
predictions through user feedback. Vision Transformer (ViT)-based models, such
as the Segment Anything Model (SAM), achieve state-of-the-art performance using
user clicks and prior masks as prompts. However, existing methods treat
interactions as independent events, leading to redundant corrections and
limited refinement gains. We address this by introducing MAIS, a
Memory-Attention mechanism for Interactive Segmentation that stores past user
inputs and segmentation states, enabling temporal context integration. Our
approach enhances ViT-based segmentation across diverse imaging modalities,
achieving more efficient and accurate refinements.

</details>


### [315] [Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models](https://arxiv.org/abs/2505.07001)
*Bidur Khanal,Sandesh Pokhrel,Sanjay Bhandari,Ramesh Rana,Nikesh Shrestha,Ram Bahadur Gurung,Cristian Linte,Angus Watson,Yash Raj Shrestha,Binod Bhattarai*

Main category: cs.CV

TL;DR: 该论文提出了一个针对胃肠道图像的多模态数据集Gut-VLM，并采用幻觉感知微调策略来减少视觉语言模型（VLM）在医学报告生成中的幻觉问题。结果显示该方法优于传统的生成微调。


<details>
  <summary>Details</summary>
Motivation: 医学领域的视觉语言模型在生成诊断报告时存在幻觉问题（生成与图像内容不符的描述），这对医疗应用具有严重风险。为了研究并解决这一问题，作者构建了一个胃肠道图像数据集，并提出了新的微调策略。

Method: 通过两阶段流程构建Gut-VLM数据集：1）用ChatGPT生成带有幻觉的医疗报告；2）医学专家审核并校正。提出“幻觉感知微调”策略，训练模型检测并纠正幻觉，而非单纯生成报告。

Result: 实验表明，幻觉感知微调方法在减少幻觉方面优于传统生成微调。此外，论文还对当前先进的VLM模型进行了多指标评估，建立了基准。

Conclusion: Gut-VLM数据集和幻觉感知微调策略为医学VLM研究提供了新工具，显著改善了模型输出与图像内容的一致性。这种方法可推广至其他医学影像领域。

Abstract: Vision-Language Models (VLMs) are becoming increasingly popular in the
medical domain, bridging the gap between medical images and clinical language.
Existing VLMs demonstrate an impressive ability to comprehend medical images
and text queries to generate detailed, descriptive diagnostic medical reports.
However, hallucination--the tendency to generate descriptions that are
inconsistent with the visual content--remains a significant issue in VLMs, with
particularly severe implications in the medical field. To facilitate VLM
research on gastrointestinal (GI) image analysis and study hallucination, we
curate a multimodal image-text GI dataset: Gut-VLM. This dataset is created
using a two-stage pipeline: first, descriptive medical reports of Kvasir-v2
images are generated using ChatGPT, which introduces some hallucinated or
incorrect texts. In the second stage, medical experts systematically review
these reports, and identify and correct potential inaccuracies to ensure
high-quality, clinically reliable annotations. Unlike traditional datasets that
contain only descriptive texts, our dataset also features tags identifying
hallucinated sentences and their corresponding corrections. A common approach
to reducing hallucination in VLM is to finetune the model on a small-scale,
problem-specific dataset. However, we take a different strategy using our
dataset. Instead of finetuning the VLM solely for generating textual reports,
we finetune it to detect and correct hallucinations, an approach we call
hallucination-aware finetuning. Our results show that this approach is better
than simply finetuning for descriptive report generation. Additionally, we
conduct an extensive evaluation of state-of-the-art VLMs across several
metrics, establishing a benchmark. GitHub Repo:
https://github.com/bhattarailab/Hallucination-Aware-VLM.

</details>


### [316] [IKrNet: A Neural Network for Detecting Specific Drug-Induced Patterns in Electrocardiograms Amidst Physiological Variability](https://arxiv.org/abs/2505.07533)
*Ahmad Fall,Federica Granese,Alex Lence,Dominique Fourer,Blaise Hanczar,Joe-Elie Salem,Jean-Daniel Zucker,Edi Prifti*

Main category: cs.CV

TL;DR: IKrNet是一种新型神经网络模型，通过结合空间和时间动态分析心电图（ECG）中的药物特定模式，在多种生理条件下表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法未能充分考虑生理条件（如体力活动、药物和压力）对ECG模式的交互影响，限制了其实际应用。本研究旨在开发一种能适应这些变化的模型。

Method: IKrNet结合了卷积主干（捕捉空间特征）和双向长短时记忆模块（建模时间依赖），并以心率变异性作为生理波动的替代指标。模型在包含体力压力、药物摄入和基线条件的多样化场景中进行了测试。

Result: 在990名健康志愿者（服用80mg索他洛尔）的临床协议下，IKrNet在准确性和稳定性上均优于现有最优模型。

Conclusion: IKrNet在多变生理条件下的优异表现证明了其临床可行性，为解决ECG分析的现实挑战提供了有效工具。

Abstract: Monitoring and analyzing electrocardiogram (ECG) signals, even under varying
physiological conditions, including those influenced by physical activity,
drugs and stress, is crucial to accurately assess cardiac health. However,
current AI-based methods often fail to account for how these factors interact
and alter ECG patterns, ultimately limiting their applicability in real-world
settings. This study introduces IKrNet, a novel neural network model, which
identifies drug-specific patterns in ECGs amidst certain physiological
conditions. IKrNet's architecture incorporates spatial and temporal dynamics by
using a convolutional backbone with varying receptive field size to capture
spatial features. A bi-directional Long Short-Term Memory module is also
employed to model temporal dependencies. By treating heart rate variability as
a surrogate for physiological fluctuations, we evaluated IKrNet's performance
across diverse scenarios, including conditions with physical stress, drug
intake alone, and a baseline without drug presence. Our assessment follows a
clinical protocol in which 990 healthy volunteers were administered 80mg of
Sotalol, a drug which is known to be a precursor to Torsades-de-Pointes, a
life-threatening arrhythmia. We show that IKrNet outperforms state-of-the-art
models' accuracy and stability in varying physiological conditions,
underscoring its clinical viability.

</details>


### [317] [Automated Visual Attention Detection using Mobile Eye Tracking in Behavioral Classroom Studies](https://arxiv.org/abs/2505.07552)
*Efe Bozkir,Christian Kosel,Tina Seidel,Enkelejda Kasneci*

Main category: cs.CV

TL;DR: 该论文提出了一种自动化处理流程，利用面部检测和识别技术结合教师眼动数据，以最小手动标注识别教师关注的学生。该方法在不同教室布局中表现良好，U型和小型教室准确率最高。


<details>
  <summary>Details</summary>
Motivation: 教师视觉注意力分布对学生参与和成绩有重要影响，但手动标注眼动数据耗时费力。研究旨在通过自动化技术减少手动标注需求，提供非侵入式解决方案。

Method: 结合先进面部检测模型、面部识别特征嵌入和迁移学习，利用教师眼动数据训练课堂情境下的面部识别模型，构建自动化处理流程。

Result: 在四种教室布局中均能合理估计教师视觉关注对象，U型和小型教室准确率分别达0.7和0.9。

Conclusion: 该方法技术有效且无需大量手动标注，为教学策略改进、课堂管理和教师发展提供了非侵入式工具。

Abstract: Teachers' visual attention and its distribution across the students in
classrooms can constitute important implications for student engagement,
achievement, and professional teacher training. Despite that, inferring the
information about where and which student teachers focus on is not trivial.
Mobile eye tracking can provide vital help to solve this issue; however, the
use of mobile eye tracking alone requires a significant amount of manual
annotations. To address this limitation, we present an automated processing
pipeline concept that requires minimal manually annotated data to recognize
which student the teachers focus on. To this end, we utilize state-of-the-art
face detection models and face recognition feature embeddings to train face
recognition models with transfer learning in the classroom context and combine
these models with the teachers' gaze from mobile eye trackers. We evaluated our
approach with data collected from four different classrooms, and our results
show that while it is possible to estimate the visually focused students with
reasonable performance in all of our classroom setups, U-shaped and small
classrooms led to the best results with accuracies of approximately 0.7 and
0.9, respectively. While we did not evaluate our method for teacher-student
interactions and focused on the validity of the technical approach, as our
methodology does not require a vast amount of manually annotated data and
offers a non-intrusive way of handling teachers' visual attention, it could
help improve instructional strategies, enhance classroom management, and
provide feedback for professional teacher development.

</details>


### [318] [Robust Kidney Abnormality Segmentation: A Validation Study of an AI-Based Framework](https://arxiv.org/abs/2505.07573)
*Sarah de Boer,Hartmut Häntze,Kiran Vaidhya Venkadesh,Myrthe A. D. Buser,Gabriel E. Humpire Mamani,Lina Xu,Lisa C. Adams,Jawed Nawabi,Keno K. Bressem,Bram van Ginneken,Mathias Prokop,Alessa Hering*

Main category: cs.CV

TL;DR: 开发了一种基于nnU-Net的肾脏异常分割算法，利用公开数据集训练并通过多种测试集验证，表现优于现有模型，且在不同亚组中均表现稳健。


<details>
  <summary>Details</summary>
Motivation: 通过客观、可重复的肾脏异常分割方法，改进临床实践中依赖主观视觉评估的现状，支持定量分析肾脏疾病。

Method: 使用公开数据集训练nnU-Net框架，并通过Dice系数和95th百分位Hausdorff距离量化分割性能，同时在患者性别、年龄等亚组中验证鲁棒性。

Result: 算法在外部测试集上表现优异，优于现有模型，且在不同亚组中性能一致。

Conclusion: 开发的算法具有高鲁棒性和可靠性，适用于临床和研究，代码已公开。

Abstract: Kidney abnormality segmentation has important potential to enhance the
clinical workflow, especially in settings requiring quantitative assessments.
Kidney volume could serve as an important biomarker for renal diseases, with
changes in volume correlating directly with kidney function. Currently,
clinical practice often relies on subjective visual assessment for evaluating
kidney size and abnormalities, including tumors and cysts, which are typically
staged based on diameter, volume, and anatomical location. To support a more
objective and reproducible approach, this research aims to develop a robust,
thoroughly validated kidney abnormality segmentation algorithm, made publicly
available for clinical and research use. We employ publicly available training
datasets and leverage the state-of-the-art medical image segmentation framework
nnU-Net. Validation is conducted using both proprietary and public test
datasets, with segmentation performance quantified by Dice coefficient and the
95th percentile Hausdorff distance. Furthermore, we analyze robustness across
subgroups based on patient sex, age, CT contrast phases, and tumor histologic
subtypes. Our findings demonstrate that our segmentation algorithm, trained
exclusively on publicly available data, generalizes effectively to external
test sets and outperforms existing state-of-the-art models across all tested
datasets. Subgroup analyses reveal consistent high performance, indicating
strong robustness and reliability. The developed algorithm and associated code
are publicly accessible at
https://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation.

</details>


### [319] [Evaluating Modern Visual Anomaly Detection Approaches in Semiconductor Manufacturing: A Comparative Study](https://arxiv.org/abs/2505.07576)
*Manuel Barusco,Francesco Borsatti,Youssef Ben Khalifa,Davide Dalle Pezze,Gian Antonio Susto*

Main category: cs.CV

TL;DR: 论文摘要讨论了半导体制造中SEM图像自动视觉检测的重要性，提出了一种基于无监督学习的视觉异常检测（VAD）方法，并利用MIIC数据集进行验证。


<details>
  <summary>Details</summary>
Motivation: 由于半导体制造过程中缺陷样本标记成本高，传统监督学习方法受限，因此探索无监督的VAD方法以实现高效且经济的异常检测显得尤为重要。

Method: 研究采用了现代VAD方法，并在半导体领域的MIIC数据集上构建了基准测试。

Result: 实验证明，现代VAD方法在半导体领域的异常检测中表现出色。

Conclusion: 无监督的VAD方法在半导体制造中具有实际应用潜力，能够在不依赖大量标记数据的情况下有效检测异常。

Abstract: Semiconductor manufacturing is a complex, multistage process. Automated
visual inspection of Scanning Electron Microscope (SEM) images is indispensable
for minimizing equipment downtime and containing costs. Most previous research
considers supervised approaches, assuming a sufficient number of anomalously
labeled samples. On the contrary, Visual Anomaly Detection (VAD), an emerging
research domain, focuses on unsupervised learning, avoiding the costly defect
collection phase while providing explanations of the predictions. We introduce
a benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset.
Our results demonstrate the efficacy of modern VAD approaches in this field.

</details>


### [320] [Discovering Concept Directions from Diffusion-based Counterfactuals via Latent Clustering](https://arxiv.org/abs/2505.07073)
*Payal Varshney,Adriano Lucieri,Christoph Balada,Andreas Dengel,Sheraz Ahmed*

Main category: cs.CV

TL;DR: 这篇论文提出了CDLC框架，通过聚类潜在差异向量来高效提取全局、类别特定的概念方向，相比于之前的CDCT方法，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于概念的解释方法计算成本高且难以捕捉复杂语义概念，因此需要一种更高效且能提取多维语义概念的方法。

Method: CDLC通过聚类从事实和反事实图像对中得到的潜在差异向量，提取全局、类别特定的概念方向，无需遍历所有潜在维度。

Result: 在皮肤病变数据集上的实验表明，CDLC提取的概念方向与临床特征一致，并能揭示数据集偏差或未知生物标志物。

Conclusion: CDLC是一种可解释、可扩展且适用于高风险领域和多样化数据的方法。

Abstract: Concept-based explanations have emerged as an effective approach within
Explainable Artificial Intelligence, enabling interpretable insights by
aligning model decisions with human-understandable concepts. However, existing
methods rely on computationally intensive procedures and struggle to
efficiently capture complex, semantic concepts. Recently, the Concept Discovery
through Latent Diffusion-based Counterfactual Trajectories (CDCT) framework,
introduced by Varshney et al. (2025), attempts to identify concepts via
dimension-wise traversal of the latent space of a Variational Autoencoder
trained on counterfactual trajectories. Extending the CDCT framework, this work
introduces Concept Directions via Latent Clustering (CDLC), which extracts
global, class-specific concept directions by clustering latent difference
vectors derived from factual and diffusion-generated counterfactual image
pairs. CDLC substantially reduces computational complexity by eliminating the
exhaustive latent dimension traversal required in CDCT and enables the
extraction of multidimensional semantic concepts encoded across the latent
dimensions. This approach is validated on a real-world skin lesion dataset,
demonstrating that the extracted concept directions align with clinically
recognized dermoscopic features and, in some cases, reveal dataset-specific
biases or unknown biomarkers. These results highlight that CDLC is
interpretable, scalable, and applicable across high-stakes domains and diverse
data modalities.

</details>


### [321] [Hybrid Spiking Vision Transformer for Object Detection with Event Cameras](https://arxiv.org/abs/2505.07715)
*Qi Xu,Jie Deng,Jiangrong Shen,Biwu Chen,Huajin Tang,Gang Pan*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的混合脉冲视觉Transformer（HsVT）模型，用于提升基于事件的目标检测性能，结合了空间和时间特征提取模块，并在公开数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 基于事件的目标检测具有高时间分辨率、宽动态范围和异步事件表示等优势，而SNN的低能耗和时空动态特性使其成为有潜力的解决方案。但现有方法在处理复杂任务时仍有提升空间，因此需要更高效的模型。

Method: HsVT模型整合了空间特征提取模块（捕获局部和全局特征）和时间特征提取模块（建模事件序列的时间依赖性和长期模式），以同时捕捉时空特征。

Result: HsVT在GEN1和自建的Fall Detection数据集上表现出色，不仅减少了参数量，还显著提升了检测性能。

Conclusion: HsVT通过结合时空特征提取，有效提升了基于事件的目标检测任务的表现，为后续研究提供了新的模型思路和公开数据集支持。

Abstract: Event-based object detection has gained increasing attention due to its
advantages such as high temporal resolution, wide dynamic range, and
asynchronous address-event representation. Leveraging these advantages, Spiking
Neural Networks (SNNs) have emerged as a promising approach, offering low
energy consumption and rich spatiotemporal dynamics. To further enhance the
performance of event-based object detection, this study proposes a novel hybrid
spike vision Transformer (HsVT) model. The HsVT model integrates a spatial
feature extraction module to capture local and global features, and a temporal
feature extraction module to model time dependencies and long-term patterns in
event sequences. This combination enables HsVT to capture spatiotemporal
features, improving its capability to handle complex event-based object
detection tasks. To support research in this area, we developed and publicly
released The Fall Detection Dataset as a benchmark for event-based object
detection tasks. This dataset, captured using an event-based camera, ensures
facial privacy protection and reduces memory usage due to the event
representation format. We evaluated the HsVT model on GEN1 and Fall Detection
datasets across various model sizes. Experimental results demonstrate that HsVT
achieves significant performance improvements in event detection with fewer
parameters.

</details>


### [322] [TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset](https://arxiv.org/abs/2505.07396)
*Olaf Wysocki,Benedikt Schwab,Manoj Kumar Biswanath,Qilin Zhang,Jingwei Zhu,Thomas Froech,Medhini Heeramaglore,Ihab Hijazi,Khaoula Kanna,Mathias Pechinger,Zhaiyu Chen,Yao Sun,Alejandro Rueda Segura,Ziyang Xu,Omar AbdelGafar,Mansour Mehranfar,Chandan Yeshwanth,Yueh-Cheng Liu,Hadi Yazdi,Jiapan Wang,Stefan Auer,Katharina Anders,Klaus Bogenberger,Andre Borrmann,Angela Dai,Ludwig Hoegner,Christoph Holst,Thomas H. Kolbe,Ferdinand Ludwig,Matthias Nießner,Frank Petzold,Xiao Xiang Zhu,Boris Jutzi*

Main category: cs.CV

TL;DR: 文章介绍了一个名为TUM2TWIN的多模态城市数字孪生基准数据集，旨在解决数据获取、模型重建、更新和互操作性等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有数据集通常只涵盖处理链的一部分，限制了城市数字孪生（UDTs）的全面验证，因此需要更全面的基准数据集。

Method: 开发了TUM2TWIN数据集，包含地理参考、语义对齐的3D模型和网络，以及多种地面、移动、航空和卫星观测数据。

Result: 数据集覆盖约100,000平方米，包含767GB数据，支持传感器分析和高级重建方法的开发，并通过下游任务验证了其实用性。

Conclusion: TUM2TWIN为克服UDT创建中的现有限制奠定了基础，推动了数据驱动城市环境的新研究方向。

Abstract: Urban Digital Twins (UDTs) have become essential for managing cities and
integrating complex, heterogeneous data from diverse sources. Creating UDTs
involves challenges at multiple process stages, including acquiring accurate 3D
source data, reconstructing high-fidelity 3D models, maintaining models'
updates, and ensuring seamless interoperability to downstream tasks. Current
datasets are usually limited to one part of the processing chain, hampering
comprehensive UDTs validation. To address these challenges, we introduce the
first comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.
This dataset includes georeferenced, semantically aligned 3D models and
networks along with various terrestrial, mobile, aerial, and satellite
observations boasting 32 data subsets over roughly 100,000 $m^2$ and currently
767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high
accuracy, and multimodal data integration, the benchmark supports robust
analysis of sensors and the development of advanced reconstruction methods.
Additionally, we explore downstream tasks demonstrating the potential of
TUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar
potential analysis, point cloud semantic segmentation, and LoD3 building
reconstruction. We are convinced this contribution lays a foundation for
overcoming current limitations in UDT creation, fostering new research
directions and practical solutions for smarter, data-driven urban environments.
The project is available under: https://tum2t.win

</details>


### [323] [DocVXQA: Context-Aware Visual Explanations for Document Question Answering](https://arxiv.org/abs/2505.07496)
*Mohamed Ali Souibgui,Changkyu Choi,Andrey Barsky,Kangsoo Jung,Ernest Valveny,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: 提出DocVXQA框架，通过视觉热图提供文档问答的可解释性，强调上下文充分性并平衡性能与可解释性，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 提升文档问答模型的可解释性，通过视觉热图提供决策依据，增强用户信任。

Method: 将可解释性原则量化为学习目标，生成上下文充分且高效表示的视觉热图。

Result: 实验验证了方法的有效性，包括人类评估。

Conclusion: DocVXQA在保持性能的同时提升了可解释性，适用于DocVQA任务。

Abstract: We propose DocVXQA, a novel framework for visually self-explainable document
question answering. The framework is designed not only to produce accurate
answers to questions but also to learn visual heatmaps that highlight
contextually critical regions, thereby offering interpretable justifications
for the model's decisions. To integrate explanations into the learning process,
we quantitatively formulate explainability principles as explicit learning
objectives. Unlike conventional methods that emphasize only the regions
pertinent to the answer, our framework delivers explanations that are
\textit{contextually sufficient} while remaining
\textit{representation-efficient}. This fosters user trust while achieving a
balance between predictive performance and interpretability in DocVQA
applications. Extensive experiments, including human evaluation, provide strong
evidence supporting the effectiveness of our method. The code is available at
https://github.com/dali92002/DocVXQA.

</details>


### [324] [Higher-Order Convolution Improves Neural Predictivity in the Retina](https://arxiv.org/abs/2505.07620)
*Simone Azeglio,Victor Calbiague Garcia,Guilhem Glaziou,Peter Neri,Olivier Marre,Ulisse Ferrari*

Main category: cs.CV

TL;DR: 该论文提出了一种新型高阶卷积神经网络（HoCNN），通过直接在卷积操作中嵌入高阶运算，增强了模型的表现力而不增加深度，并在预测神经响应方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决深度人工网络与生物视觉系统浅层处理层次之间的架构差异，同时提升CNN的表现力。

Method: 在传统3D CNN中嵌入高阶运算，直接建模空间和时间的像素间乘法交互。

Result: HoCNN在两种数据集上表现优于标准架构，训练数据需求减半，相关性系数达0.75，几何变换编码能力显著提升。

Conclusion: HoCNN不仅提升了神经响应预测的准确性，还能自然编码几何变换，尤其适用于特定细胞类型的响应预测。

Abstract: We present a novel approach to neural response prediction that incorporates
higher-order operations directly within convolutional neural networks (CNNs).
Our model extends traditional 3D CNNs by embedding higher-order operations
within the convolutional operator itself, enabling direct modeling of
multiplicative interactions between neighboring pixels across space and time.
Our model increases the representational power of CNNs without increasing their
depth, therefore addressing the architectural disparity between deep artificial
networks and the relatively shallow processing hierarchy of biological visual
systems. We evaluate our approach on two distinct datasets: salamander retinal
ganglion cell (RGC) responses to natural scenes, and a new dataset of mouse RGC
responses to controlled geometric transformations. Our higher-order CNN (HoCNN)
achieves superior performance while requiring only half the training data
compared to standard architectures, demonstrating correlation coefficients up
to 0.75 with neural responses (against 0.80$\pm$0.02 retinal reliability). When
integrated into state-of-the-art architectures, our approach consistently
improves performance across different species and stimulus conditions. Analysis
of the learned representations reveals that our network naturally encodes
fundamental geometric transformations, particularly scaling parameters that
characterize object expansion and contraction. This capability is especially
relevant for specific cell types, such as transient OFF-alpha and transient ON
cells, which are known to detect looming objects and object motion
respectively, and where our model shows marked improvement in response
prediction. The correlation coefficients for scaling parameters are more than
twice as high in HoCNN (0.72) compared to baseline models (0.32).

</details>


### [325] [BodyGPS: Anatomical Positioning System](https://arxiv.org/abs/2505.07744)
*Halid Ziya Yerebakan,Kritika Iyer,Xueqi Guo,Yoshihisa Shinagawa,Gerardo Hermosillo Valadez*

Main category: cs.CV

TL;DR: 提出了一种新型基础模型，用于解析医学图像中的人体解剖结构，支持多种模态和监督/无监督训练，实现快速响应（<1毫秒）。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像中多模态解剖结构解析的通用性和效率问题。

Method: 通过训练神经网络估计器，将查询位置映射到图谱坐标，稀疏采样输入以提高效率。

Result: 在CT和MRI模态中验证了算法的实用性，响应时间小于1毫秒。

Conclusion: 该模型具有高效性和多模态适用性，为医学图像处理提供了新工具。

Abstract: We introduce a new type of foundational model for parsing human anatomy in
medical images that works for different modalities. It supports supervised or
unsupervised training and can perform matching, registration, classification,
or segmentation with or without user interaction. We achieve this by training a
neural network estimator that maps query locations to atlas coordinates via
regression. Efficiency is improved by sparsely sampling the input, enabling
response times of less than 1 ms without additional accelerator hardware. We
demonstrate the utility of the algorithm in both CT and MRI modalities.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [326] [Prediction of Delirium Risk in Mild Cognitive Impairment Using Time-Series data, Machine Learning and Comorbidity Patterns -- A Retrospective Study](https://arxiv.org/abs/2505.06264)
*Santhakumar Ramamoorthy,Priya Rani,James Mahon,Glenn Mathews,Shaun Cloherty,Mahdi Babaei*

Main category: stat.AP

TL;DR: 该研究通过分析轻度认知障碍（MCI）患者的共病模式，利用机器学习开发了预测谵妄风险的LSTM模型，AUROC达0.93。


<details>
  <summary>Details</summary>
Motivation: 谵妄在MCI患者中发病率和死亡率高，但相关风险因素研究不足，需开发预测模型。

Method: 利用MIMIC-IV v2.2数据库进行回顾性分析，结合Kaplan-Meier生存分析和LSTM模型预测风险。

Result: MCI患者谵妄后生存率显著降低，LSTM模型预测性能优异（AUROC 0.93，AUPRC 0.92）。

Conclusion: 共病是谵妄风险的关键因素，时间序列预测模型能有效识别高风险患者。

Abstract: Delirium represents a significant clinical concern characterized by high
morbidity and mortality rates, particularly in patients with mild cognitive
impairment (MCI). This study investigates the associated risk factors for
delirium by analyzing the comorbidity patterns relevant to MCI and developing a
longitudinal predictive model leveraging machine learning methodologies. A
retrospective analysis utilizing the MIMIC-IV v2.2 database was performed to
evaluate comorbid conditions, survival probabilities, and predictive modeling
outcomes. The examination of comorbidity patterns identified distinct risk
profiles for the MCI population. Kaplan-Meier survival analysis demonstrated
that individuals with MCI exhibit markedly reduced survival probabilities when
developing delirium compared to their non-MCI counterparts, underscoring the
heightened vulnerability within this cohort. For predictive modeling, a Long
Short-Term Memory (LSTM) ML network was implemented utilizing time-series data,
demographic variables, Charlson Comorbidity Index (CCI) scores, and an array of
comorbid conditions. The model demonstrated robust predictive capabilities with
an AUROC of 0.93 and an AUPRC of 0.92. This study underscores the critical role
of comorbidities in evaluating delirium risk and highlights the efficacy of
time-series predictive modeling in pinpointing patients at elevated risk for
delirium development.

</details>


### [327] [An Early Warning Model for Forced Displacement](https://arxiv.org/abs/2505.06249)
*Geraldine Henningsen*

Main category: stat.AP

TL;DR: 这篇论文提出了一种新的难民和寻求庇护者流动监测方法，通过梯度提升分类结合冲突预测与经济、政治及人口变量，评估来源国的两种风险：显著流动的可能性和流动突然增加的概率，并生成了高精度的月度风险指数。


<details>
  <summary>Details</summary>
Motivation: 当前预测模型虽能高精度预测冲突，但具体哪些事件会引发大规模人口流动仍不明确。因此，研究旨在填补这一空白，通过多指标综合分析评估强迫性流动风险，为人道主义行动提供量化支持。

Method: 采用梯度提升分类模型，结合冲突预测及经济、政治、人口等多维变量，生成1、3、6个月预测期的国家月度风险指数，分别评估显著流动和流动突增的两种风险。

Result: 模型在预测显著流动时表现出高精度，而对流动突增的预测也有较好表现（后者由于触发复杂性更难预测）。结果表明，通过多指标综合分析可以有效评估强迫性流动风险。

Conclusion: 风险指数为人道主义规划提供了有价值的量化工具，但需作为更广泛分析框架中的决策支持工具使用。多指标综合分析是评估流动风险的有效方法。

Abstract: Monitoring tools for anticipatory action are increasingly gaining traction to
improve the efficiency and timeliness of humanitarian responses. Whilst
predictive models can now forecast conflicts with high accuracy, translating
these predictions into potential forced displacement movements remains
challenging because it is often unclear which precise events will trigger
significant population movements. This paper presents a novel monitoring
approach for refugee and asylum seeker flows that addresses this challenge.
Using gradient boosting classification, we combine conflict forecasts with a
comprehensive set of economic, political, and demographic variables to assess
two distinct risks at the country of origin: the likelihood of significant
displacement flows and the probability of sudden increases in these flows. The
model generates country-specific monthly risk indices for these two events with
prediction horizons of one, three, and six months. Our analysis shows high
accuracy in predicting significant displacement flows and good accuracy in
forecasting sudden increases in displacement--the latter being inherently more
difficult to predict, given the complexity of displacement triggers. We achieve
these results by including predictive factors beyond conflict, thereby
demonstrating that forced displacement risks can be assessed through an
integrated analysis of multiple country-level indicators. Whilst these risk
indices provide valuable quantitative support for humanitarian planning, they
should always be understood as decision-support tools within a broader
analytical framework.

</details>


### [328] [Adaptive Bayesian Very Short-Term Wind Power Forecasting Based on the Generalised Logit Transformation](https://arxiv.org/abs/2505.06310)
*Tao Shen,Jethro Browell,Daniela Castro-Camilo*

Main category: stat.AP

TL;DR: 该论文提出了一种结合广义Logit变换和贝叶斯方法的自适应短期风电功率预测方法，通过自适应更新形状参数优化预测性能。该方法在100多个风电场的数据集中验证，表现优于基准方法，提升了预测准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 风电在实现2050净零目标中至关重要，但其波动性给预测带来挑战。准确预测风电功率是稳定接入电网的关键需求。论文旨在解决现有预测方法的不足，提升预测精度。

Method: 采用广义Logit变换将风电功率数据转换到无界域，结合贝叶斯方法进行预测，并引入自适应形状参数更新机制。研究了四种自适应预测方法，通过大量风电场数据验证。

Result: 提出的贝叶斯方法在连续排名概率评分（CRPS）和可靠性方面优于基准方法，表现出更高的预测准确性和稳健性。

Conclusion: 该方法有效解决了风电预测中的不确定性问题，为电网集成和决策提供了可靠的预测工具，具有实际应用价值。

Abstract: Wind power plays an increasingly significant role in achieving the 2050 Net
Zero Strategy. Despite its rapid growth, its inherent variability presents
challenges in forecasting. Accurately forecasting wind power generation is one
key demand for the stable and controllable integration of renewable energy into
existing grid operations. This paper proposes an adaptive method for very
short-term forecasting that combines the generalised logit transformation with
a Bayesian approach. The generalised logit transformation processes
double-bounded wind power data to an unbounded domain, facilitating the
application of Bayesian methods. A novel adaptive mechanism for updating the
transformation shape parameter is introduced to leverage Bayesian updates by
recovering a small sample of representative data. Four adaptive forecasting
methods are investigated, evaluating their advantages and limitations through
an extensive case study of over 100 wind farms ranging four years in the UK.
The methods are evaluated using the Continuous Ranked Probability Score and we
propose the use of functional reliability diagrams to assess calibration.
Results indicate that the proposed Bayesian method with adaptive shape
parameter updating outperforms benchmarks, yielding consistent improvements in
CRPS and forecast reliability. The method effectively addresses uncertainty,
ensuring robust and accurate probabilistic forecasting which is essential for
grid integration and decision-making.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [329] [A digital perspective on the role of a stemma in material-philological transmission studies](https://arxiv.org/abs/2505.06938)
*Katarzyna Anna Kapitan*

Main category: cs.DL

TL;DR: 该研究探讨了数字方法对文本传统的广泛影响，提出计算机生成谱系图应视为研究工具而非最终成果，并以古挪威传奇为例展示了其应用。


<details>
  <summary>Details</summary>
Motivation: 随着数字人文的快速发展和学术工作流程的自动化，作者希望探索数字方法在文本传统研究中的作用，特别是谱系图作为研究工具的潜力。

Method: 研究以古挪威传奇Hr\'omundur为例，用Python脚本将TEI编码的XML数据转换为PHYLIP软件可分析的格式，生成文本间关系的无根树。

Result: 研究发现计算机生成的谱系图可以作为深入研究文本传统的起点，解答传统方法难以解决的问题，并提供了相关的数据集和脚本作为支持。

Conclusion: 谱系图不应仅被视为研究的最终成果，而应作为动态工具促进对文本传统的更深入探索，数字方法为文本研究开辟了新途径。

Abstract: Taking its point of departure in the recent developments in the field of
digital humanities and the increasing automatisation of scholarly workflows,
this study explores the implications of digital approaches to textual
traditions for the broader field of textual scholarship. It argues that the
relative simplicity of creating computergenerated stemmas allows us to view the
stemma codicum as a research tool rather than the final product of our
scholarly investigation. Using the Old Norse saga of Hr\'omundur as a case
study, this article demonstrates that stemmas can serve as a starting point for
exploring textual traditions further. In doing so, they enable us to address
research questions that otherwise remain unanswered. The article is accompanied
by datasets used to generate stemmas for the Hr\'omundar saga tradition as well
as two custom Python scripts. The scripts are designed to convert XML-based
textual data, encoded according to the TEI Guidelines, into the input format
used for the analysis in the PHYLIP package to generate unrooted trees of
relationships between texts.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [330] [Attention Mechanisms in Dynamical Systems: A Case Study with Predator-Prey Models](https://arxiv.org/abs/2505.06503)
*David Balaban*

Main category: math.DS

TL;DR: 论文探讨了注意力机制在模拟经典动力系统（如Lotka-Volterra系统）中的效用，发现注意力权重与Lyapunov函数的几何结构一致，并可用于数据驱动的非线性系统分析。


<details>
  <summary>Details</summary>
Motivation: 研究动机是利用注意力机制提升对经典动力系统的建模能力，特别是在数据驱动的非线性系统分析和控制中提供可解释性。

Method: 方法是通过在受扰动的时间序列数据上训练简单的线性注意力模型，重建系统轨迹。

Result: 结果显示，学习的注意力权重与Lyapunov函数的几何结构一致，高注意力对应平坦区域（扰动影响小），低注意力对应陡峭区域（扰动影响大）。

Conclusion: 结论是注意力机制可作为灵敏度分析的代理，无需系统方程的显式知识即可捕获关键相空间特性，为数据驱动的非线性系统分析和控制提供了新工具。

Abstract: Attention mechanisms are widely used in artificial intelligence to enhance
performance and interpretability. In this paper, we investigate their utility
in modeling classical dynamical systems -- specifically, a noisy predator-prey
(Lotka-Volterra) system. We train a simple linear attention model on perturbed
time-series data to reconstruct system trajectories. Remarkably, the learned
attention weights align with the geometric structure of the Lyapunov function:
high attention corresponds to flat regions (where perturbations have small
effect), and low attention aligns with steep regions (where perturbations have
large effect). We further demonstrate that attention-based weighting can serve
as a proxy for sensitivity analysis, capturing key phase-space properties
without explicit knowledge of the system equations. These results suggest a
novel use of AI-derived attention for interpretable, data-driven analysis and
control of nonlinear systems. For example our framework could support future
work in biological modeling of circadian rhythms, and interpretable machine
learning for dynamical environments.

</details>


### [331] [The Influence of the Memory Capacity of Neural DDEs on the Universal Approximation Property](https://arxiv.org/abs/2505.07244)
*Christian Kuehn,Sara-Viola Kuntz*

Main category: math.DS

TL;DR: 论文研究了神经延迟微分方程（Neural DDEs）的记忆容量对通用近似性质的影响，发现记忆容量的大小决定了其是否能实现通用近似。


<details>
  <summary>Details</summary>
Motivation: 探索神经DDEs的记忆容量如何影响其通用近似能力，填补了连续时间神经网络理论研究的空白。

Method: 通过分析记忆容量参数Kτ，研究神经DDEs在不同记忆容量下的表现，并与神经ODE和传统网络对比。

Result: 记忆容量Kτ较小时，神经DDEs无法实现通用近似；Kτ足够大时，才能实现连续函数的通用近似。增强架构可以扩大适用参数范围。

Conclusion: 神经DDEs的记忆容量是其通用近似能力的关键，仅当记忆容量超过一定阈值时才能实现通用近似。

Abstract: Neural Ordinary Differential Equations (Neural ODEs), which are the
continuous-time analog of Residual Neural Networks (ResNets), have gained
significant attention in recent years. Similarly, Neural Delay Differential
Equations (Neural DDEs) can be interpreted as an infinite depth limit of
Densely Connected Residual Neural Networks (DenseResNets). In contrast to
traditional ResNet architectures, DenseResNets are feed-forward networks that
allow for shortcut connections across all layers. These additional connections
introduce memory in the network architecture, as typical in many modern
architectures. In this work, we explore how the memory capacity in neural DDEs
influences the universal approximation property. The key parameter for studying
the memory capacity is the product $K \tau$ of the Lipschitz constant and the
delay of the DDE. In the case of non-augmented architectures, where the network
width is not larger than the input and output dimensions, neural ODEs and
classical feed-forward neural networks cannot have the universal approximation
property. We show that if the memory capacity $K\tau$ is sufficiently small,
the dynamics of the neural DDE can be approximated by a neural ODE.
Consequently, non-augmented neural DDEs with a small memory capacity also lack
the universal approximation property. In contrast, if the memory capacity
$K\tau$ is sufficiently large, we can establish the universal approximation
property of neural DDEs for continuous functions. If the neural DDE
architecture is augmented, we can expand the parameter regions in which
universal approximation is possible. Overall, our results show that by
increasing the memory capacity $K\tau$, the infinite-dimensional phase space of
DDEs with positive delay $\tau>0$ is not sufficient to guarantee a direct jump
transition to universal approximation, but only after a certain memory
threshold, universal approximation holds.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [332] [Bang for the Buck: Vector Search on Cloud CPUs](https://arxiv.org/abs/2505.07621)
*Leonardo Kuffo,Peter Boncz*

Main category: cs.DB

TL;DR: 本文研究发现不同CPU微架构在向量搜索场景中表现差异显著，并指出Graviton3在性价比上最优。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏跨CPU的向量搜索基准测试，用户难以选择最适合的云服务CPU。

Method: 通过比较不同CPU在不同向量搜索场景（如IVF和HNSW索引）下的性能（QPS和QP$）。

Result: AMD Zen4在IVF索引下表现最佳，而Graviton3在多数情况下性价比最高。

Conclusion: 研究为部署向量搜索系统的用户提供了性价比最优的CPU选择指导。

Abstract: Vector databases have emerged as a new type of systems that support efficient
querying of high-dimensional vectors. Many of these offer their database as a
service in the cloud. However, the variety of available CPUs and the lack of
vector search benchmarks across CPUs make it difficult for users to choose one.
In this study, we show that CPU microarchitectures available in the cloud
perform significantly differently across vector search scenarios. For instance,
in an IVF index on float32 vectors, AMD's Zen4 gives almost 3x more queries per
second (QPS) compared to Intel's Sapphire Rapids, but for HNSW indexes, the
tables turn. However, when looking at the number of queries per dollar (QP$),
Graviton3 is the best option for most indexes and quantization settings, even
over Graviton4 (Table 1). With this work, we hope to guide users in getting the
best "bang for the buck" when deploying vector search systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [333] [PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations](https://arxiv.org/abs/2505.06502)
*Md Rakibul Hasan,Pouria Behnoudfar,Dan MacKinlay,Thomas Poulet*

Main category: eess.IV

TL;DR: PC-SRGAN是一种基于GAN的超分辨率方法，通过增强图像分辨率并确保物理一致性，显著提升了PSNR和SSIM指标，且仅需少量训练数据即可实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 当前GAN生成的超分辨率图像在科学应用中缺乏物理意义，PC-SRGAN旨在解决这一问题，提供具备物理一致性的高分辨率图像。

Method: PC-SRGAN结合了数值验证的时间积分器和高级质量指标，确保生成图像的物理合理性。

Result: 相较于传统方法，PC-SRGAN在PSNR和SSIM指标上表现更优，且训练数据需求更低（仅需SRGAN的13%）。

Conclusion: PC-SRGAN不仅提升了科学机器学习的准确性，还为时间依赖性问题提供了可靠的替代模型，具有广泛的应用潜力。

Abstract: Machine Learning, particularly Generative Adversarial Networks (GANs), has
revolutionised Super Resolution (SR). However, generated images often lack
physical meaningfulness, which is essential for scientific applications. Our
approach, PC-SRGAN, enhances image resolution while ensuring physical
consistency for interpretable simulations. PC-SRGAN significantly improves both
the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure
compared to conventional methods, even with limited training data (e.g., only
13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments
physically meaningful machine learning, incorporating numerically justified
time integrators and advanced quality metrics. These advancements promise
reliable and causal machine-learning models in scientific domains. A
significant advantage of PC-SRGAN over conventional SR techniques is its
physical consistency, which makes it a viable surrogate model for
time-dependent problems. PC-SRGAN advances scientific machine learning,
offering improved accuracy and efficiency for image processing, enhanced
process understanding, and broader applications to scientific research. The
source codes and data will be made publicly available at
https://github.com/hasan-rakibul/PC-SRGAN upon acceptance of this paper.

</details>


### [334] [Reproducing and Improving CheXNet: Deep Learning for Chest X-ray Disease Classification](https://arxiv.org/abs/2505.06646)
*Daniel Strick,Carlos Garcia,Anthony Huang*

Main category: eess.IV

TL;DR: 论文研究了深度学习在胸部X光图像分析中的应用，复现了CheXNet算法并探索了其他更优算法，评估指标包括F1分数和AUC-ROC。


<details>
  <summary>Details</summary>
Motivation: 探索深度学习在放射图像分析中的潜力，改进现有算法，以提升对14种疾病的分类性能。

Method: 在公开的NIH ChestX-ray14数据集上复现CheXNet，并测试其他算法。

Result: 最佳模型的平均AUC-ROC为0.85，平均F1分数为0.39。

Conclusion: 深度学习算法在胸部X光分析中表现优异，未来可能成为医学影像分析的标配。

Abstract: Deep learning for radiologic image analysis is a rapidly growing field in
biomedical research and is likely to become a standard practice in modern
medicine. On the publicly available NIH ChestX-ray14 dataset, containing X-ray
images that are classified by the presence or absence of 14 different diseases,
we reproduced an algorithm known as CheXNet, as well as explored other
algorithms that outperform CheXNet's baseline metrics. Model performance was
primarily evaluated using the F1 score and AUC-ROC, both of which are critical
metrics for imbalanced, multi-label classification tasks in medical imaging.
The best model achieved an average AUC-ROC score of 0.85 and an average F1
score of 0.39 across all 14 disease classifications present in the dataset.

</details>


### [335] [GAN-based synthetic FDG PET images from T1 brain MRI can serve to improve performance of deep unsupervised anomaly detection models](https://arxiv.org/abs/2505.07364)
*Daria Zotova,Nicolas Pinon,Robin Trombetta,Romain Bouet,Julien Jung,Carole Lartizien*

Main category: eess.IV

TL;DR: 使用GAN架构研究MR T1到FDG PET的跨模态翻译，验证生成的合成数据在异常检测任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决多模态医学数据稀缺问题，研究GAN生成的合成数据在训练深度模型时的任务相关性能。

Method: 设计并比较多种GAN框架，生成脑FDG PET图像，评估视觉质量后，用于训练无监督异常检测模型（UAD）。

Result: 最佳GAN模型生成逼真PET图像（SSIM 0.9，PSNR 23.8），UAD模型在合成数据上达到74%灵敏度。

Conclusion: GAN在MR T1到FDG PET翻译中表现最优，合成数据对训练UAD模型具有诊断价值。

Abstract: Background and Objective. Research in the cross-modal medical image
translation domain has been very productive over the past few years in tackling
the scarce availability of large curated multimodality datasets with the
promising performance of GAN-based architectures. However, only a few of these
studies assessed task-based related performance of these synthetic data,
especially for the training of deep models. Method. We design and compare
different GAN-based frameworks for generating synthetic brain
[18F]fluorodeoxyglucose (FDG) PET images from T1 weighted MRI data. We first
perform standard qualitative and quantitative visual quality evaluation. Then,
we explore further impact of using these fake PET data in the training of a
deep unsupervised anomaly detection (UAD) model designed to detect subtle
epilepsy lesions in T1 MRI and FDG PET images. We introduce novel diagnostic
task-oriented quality metrics of the synthetic FDG PET data tailored to our
unsupervised detection task, then use these fake data to train a use case UAD
model combining a deep representation learning based on siamese autoencoders
with a OC-SVM density support estimation model. This model is trained on normal
subjects only and allows the detection of any variation from the pattern of the
normal population. We compare the detection performance of models trained on 35
paired real MR T1 of normal subjects paired either on 35 true PET images or on
35 synthetic PET images generated from the best performing generative models.
Performance analysis is conducted on 17 exams of epilepsy patients undergoing
surgery. Results. The best performing GAN-based models allow generating
realistic fake PET images of control subject with SSIM and PSNR values around
0.9 and 23.8, respectively and in distribution (ID) with regard to the true
control dataset. The best UAD model trained on these synthetic normative PET
data allows reaching 74% sensitivity. Conclusion. Our results confirm that
GAN-based models are the best suited for MR T1 to FDG PET translation,
outperforming transformer or diffusion models. We also demonstrate the
diagnostic value of these synthetic data for the training of UAD models and
evaluation on clinical exams of epilepsy patients. Our code and the normative
image dataset are available.

</details>


### [336] [Uni-AIMS: AI-Powered Microscopy Image Analysis](https://arxiv.org/abs/2505.06918)
*Yanhui Hong,Nan Wang,Zhiyi Xia,Haoyi Tao,Xi Fang,Yiming Li,Jiankun Wang,Peng Jin,Xiaochen Cai,Shengyu Li,Ziqi Chen,Zezhong Zhang,Guolin Ke,Linfeng Zhang*

Main category: eess.IV

TL;DR: 本文提出了一种系统性解决方案，用于显微镜图像的智能识别与自动分析，包括数据引擎、分割模型和自动标尺识别功能，构建了一个综合智能分析平台，并在实际应用中验证了其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决显微镜图像识别与分析中的独特挑战，提升自动识别的准确性和效率，为跨学科研究提供自动化工具支持。

Method: 结合实验数据收集、合成数据生成和人工标注流程构建数据引擎；提出一种能够稳健检测大小物体的分割模型；支持自动识别图像标尺。

Result: 构建了一个综合智能分析平台，在实际应用中验证了其有效性和实用性，提升了显微镜图像自动识别的准确性和效率。

Conclusion: 该研究不仅推动了显微镜图像自动识别的发展，还确保了解决方案的可扩展性和泛化能力，为跨学科研究提供了强大的自动化分析工具。

Abstract: This paper presents a systematic solution for the intelligent recognition and
automatic analysis of microscopy images. We developed a data engine that
generates high-quality annotated datasets through a combination of the
collection of diverse microscopy images from experiments, synthetic data
generation and a human-in-the-loop annotation process. To address the unique
challenges of microscopy images, we propose a segmentation model capable of
robustly detecting both small and large objects. The model effectively
identifies and separates thousands of closely situated targets, even in
cluttered visual environments. Furthermore, our solution supports the precise
automatic recognition of image scale bars, an essential feature in quantitative
microscopic analysis. Building upon these components, we have constructed a
comprehensive intelligent analysis platform and validated its effectiveness and
practicality in real-world applications. This study not only advances automatic
recognition in microscopy imaging but also ensures scalability and
generalizability across multiple application domains, offering a powerful tool
for automated microscopic analysis in interdisciplinary research.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [337] [Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?](https://arxiv.org/abs/2505.07078)
*Weixian Waylon Li,Hyeonjun Kim,Mihai Cucuringu,Tiejun Ma*

Main category: q-fin.TR

TL;DR: 该论文提出FINSABER框架，评估大语言模型（LLM）在资产定价和股票交易中的长期普适性和鲁棒性，发现其策略在更长时间和更大股票范围内表现显著下降，需改进趋势检测和风险控制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在金融任务中的评估集中在短时间和有限股票范围，可能高估了其效果。作者希望通过更全面的评估验证其普适性和鲁棒性。

Method: 提出FINSABER框架，通过长达20年和100+股票的系统性回测，分析LLM策略的表现。

Result: 发现LLM策略在更大范围和更长时间内优势明显下降，且在牛市保守、熊市激进，表现不佳。

Conclusion: LLM策略需改进趋势检测和风险控制，而不仅仅是增加框架复杂度。

Abstract: Large Language Models (LLMs) have recently been leveraged for asset pricing
tasks and stock trading applications, enabling AI agents to generate investment
decisions from unstructured financial data. However, most evaluations of LLM
timing-based investing strategies are conducted on narrow timeframes and
limited stock universes, overstating effectiveness due to survivorship and
data-snooping biases. We critically assess their generalizability and
robustness by proposing FINSABER, a backtesting framework evaluating
timing-based strategies across longer periods and a larger universe of symbols.
Systematic backtests over two decades and 100+ symbols reveal that previously
reported LLM advantages deteriorate significantly under broader cross-section
and over a longer-term evaluation. Our market regime analysis further
demonstrates that LLM strategies are overly conservative in bull markets,
underperforming passive benchmarks, and overly aggressive in bear markets,
incurring heavy losses. These findings highlight the need to develop LLM
strategies that are able to prioritise trend detection and regime-aware risk
controls over mere scaling of framework complexity.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [338] [Streaming Krylov-Accelerated Stochastic Gradient Descent](https://arxiv.org/abs/2505.07046)
*Stephen Thomas*

Main category: math.NA

TL;DR: SKA-SGD是一种新颖的优化方法，通过将随机梯度投影到低维Krylov子空间来加速病态问题的收敛，结合了流式Gauss-Seidel迭代、Chebyshev多项式基底和GPU高效实现，显著优于SGD和Adam。


<details>
  <summary>Details</summary>
Motivation: 针对病态问题收敛速度慢的挑战，结合流式Krylov子空间优化和随机梯度下降的优势，提出了SKA-SGD方法。

Method: 采用流式Gauss-Seidel迭代计算投影系数，使用Chebyshev多项式基底构建Krylov子空间，并在AMD GPU上高效实现，复杂度仅为O(s²)。

Result: 实验表明，SKA-SGD在收敛速度和最终误差上显著优于标准SGD和Adam，尤其在条件数超过10³的问题中表现更优。

Conclusion: SKA-SGD通过流式Krylov加速和GPU优化，在病态问题中实现了高效收敛，为大规模优化问题提供了新解决方案。

Abstract: We present SKA-SGD (Streaming Krylov-Accelerated Stochastic Gradient
Descent), a novel optimization approach that accelerates convergence for
ill-conditioned problems by projecting stochastic gradients onto a
low-dimensional Krylov subspace. Directly inspired by recent advances in s-step
Conjugate Gradient methods with streaming Gauss-Seidel Gram solvers
\cite{dambra2025sstep}, our method extends these techniques to the stochastic
optimization domain. Our approach combines three key innovations: (1)
projection coefficients computed via a single streaming Gauss-Seidel iteration,
which is mathematically equivalent to Modified Gram-Schmidt orthogonalization;
(2) a Chebyshev polynomial basis for constructing the Krylov subspace,
providing superior numerical stability; and (3) efficient implementation for
AMD GPUs using HIP. We prove that our streaming approach achieves a backward
error near machine precision with $O(s^2)$ complexity rather than $O(s^3)$,
where $s$ is the Krylov subspace dimension. Experimental results demonstrate
that SKA-SGD significantly outperforms standard SGD and Adam in convergence
rate and final error, particularly for problems with condition numbers
exceeding $10^3$. GPU performance analysis reveals a crossover point where
communication-avoiding benefits outweigh computational overhead, typically
occurring at moderate scale ($p \approx 64$ processors) for problem sizes $n
\geq 10^6$.

</details>


### [339] [Solving Nonlinear PDEs with Sparse Radial Basis Function Networks](https://arxiv.org/abs/2505.07765)
*Zihan Shao,Konstantin Pieper,Xiaochuan Tian*

Main category: math.NA

TL;DR: 本文提出了一种使用稀疏径向基函数（RBF）网络求解非线性PDE的新框架，通过稀疏促进正则化避免过参数化，结合了传统RBF配置法、物理信息神经网络（PINN）和高斯过程（GP）方法的优点，并在理论上基于再生核Banach空间（RKBS），证明了稀疏优化问题的有限解存在性和误差界限，实验结果表明该方法在特定情况下优于GP方法。


<details>
  <summary>Details</summary>
Motivation: 传统RBF配置法、PINN和GP方法各自存在局限性，如过参数化或计算效率低，本文旨在结合这些方法的优点，提出一种更高效的统一框架。

Method: 采用基于RKBS理论的三阶段算法，包括自适应特征选择、二阶优化和不活跃神经元剪枝，以保持计算效率。

Result: 理论证明了稀疏优化问题的有限解和误差界限，实验显示该方法在特定情况下优于GP方法。

Conclusion: 本研究为基于严格分析和高效学习的自适应PDE求解器开辟了新方向。

Abstract: We propose a novel framework for solving nonlinear PDEs using sparse radial
basis function (RBF) networks. Sparsity-promoting regularization is employed to
prevent over-parameterization and reduce redundant features. This work is
motivated by longstanding challenges in traditional RBF collocation methods,
along with the limitations of physics-informed neural networks (PINNs) and
Gaussian process (GP) approaches, aiming to blend their respective strengths in
a unified framework. The theoretical foundation of our approach lies in the
function space of Reproducing Kernel Banach Spaces (RKBS) induced by
one-hidden-layer neural networks of possibly infinite width. We prove a
representer theorem showing that the solution to the sparse optimization
problem in the RKBS admits a finite solution and establishes error bounds that
offer a foundation for generalizing classical numerical analysis. The
algorithmic framework is based on a three-phase algorithm to maintain
computational efficiency through adaptive feature selection, second-order
optimization, and pruning of inactive neurons. Numerical experiments
demonstrate the effectiveness of our method and highlight cases where it offers
notable advantages over GP approaches. This work opens new directions for
adaptive PDE solvers grounded in rigorous analysis with efficient,
learning-inspired implementation.

</details>


### [340] [Automatically Differentiable Model Updating (ADiMU): conventional, hybrid, and neural network material model discovery including history-dependency](https://arxiv.org/abs/2505.07801)
*Bernardo P. Ferreira,Miguel A. Bessa*

Main category: math.NA

TL;DR: ADiMU 是一种自动可微分模型更新框架，能从全局（位移和力数据）或局部（应变-应力数据）数据中发现历史依赖性材料模型，并支持物理模型、神经网络及混合模型的更新，无需调参。


<details>
  <summary>Details</summary>
Motivation: 解决传统和现代材料模型在从实验数据中更新时的挑战，提供一个无需调参、支持多样化模型的通用框架。

Method: 利用自动微分和高效批处理计算图，通过全微分代码实现历史依赖性模型的向量化更新。

Result: ADiMU 成功更新了参数规模从数十到数百万的不同模型，在局部和全局发现设置下均表现出鲁棒性和通用性。

Conclusion: ADiMU 是一种强大且灵活的工具，为材料模型的集成、评估和应用提供了支持，并以开源工具 HookeAI 的形式发布。

Abstract: We introduce the first Automatically Differentiable Model Updating (ADiMU)
framework that finds any history-dependent material model from full-field
displacement and global force data (global, indirect discovery) or from
strain-stress data (local, direct discovery). We show that ADiMU can update
conventional (physics-based), neural network (data-driven), and hybrid material
models. Moreover, this framework requires no fine-tuning of hyperparameters or
additional quantities beyond those inherent to the user-selected material model
architecture and optimizer. The robustness and versatility of ADiMU is
extensively exemplified by updating different models spanning tens to millions
of parameters, in both local and global discovery settings. Relying on fully
differentiable code, the algorithmic implementation leverages vectorizing maps
that enable history-dependent automatic differentiation via efficient batched
execution of shared computation graphs. This contribution also aims to
facilitate the integration, evaluation and application of future material model
architectures by openly supporting the research community. Therefore, ADiMU is
released as an open-source computational tool, integrated into a carefully
designed and documented software named HookeAI.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [341] [Input-Specific and Universal Adversarial Attack Generation for Spiking Neural Networks in the Spiking Domain](https://arxiv.org/abs/2505.06299)
*Spyridon Raptis,Haralampos-G. Stratigopoulos*

Main category: cs.CR

TL;DR: 该论文提出两种针对脉冲神经网络（SNNs）的新型对抗攻击算法：一种基于特定输入的攻击和一种通用攻击，实验结果表明这两种算法在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着脉冲神经网络（SNNs）在多种应用中的普及，研究其安全漏洞（尤其是对抗攻击）变得至关重要，以评估其在实际部署中的可靠性。

Method: 提出两种梯度驱动的对抗攻击算法：一种针对特定输入生成对抗样本，另一种生成可用于多数输入的可复用的通用对抗补丁。这些算法在脉冲域中操作，优化多个指标。

Result: 在NMNIST和IBM DVS Gesture数据集上的实验表明，所提攻击算法在对抗准确性、隐蔽性和生成时间等指标上均优于现有方法。此外，首次在声音域（SHD数据集）中验证了对抗攻击的生成。

Conclusion: 论文证实了SNNs对对抗攻击的脆弱性，并提出高效的攻击方法，为未来防御研究提供了重要参考。

Abstract: As Spiking Neural Networks (SNNs) gain traction across various applications,
understanding their security vulnerabilities becomes increasingly important. In
this work, we focus on the adversarial attacks, which is perhaps the most
concerning threat. An adversarial attack aims at finding a subtle input
perturbation to fool the network's decision-making. We propose two novel
adversarial attack algorithms for SNNs: an input-specific attack that crafts
adversarial samples from specific dataset inputs and a universal attack that
generates a reusable patch capable of inducing misclassification across most
inputs, thus offering practical feasibility for real-time deployment. The
algorithms are gradient-based operating in the spiking domain proving to be
effective across different evaluation metrics, such as adversarial accuracy,
stealthiness, and generation time. Experimental results on two widely used
neuromorphic vision datasets, NMNIST and IBM DVS Gesture, show that our
proposed attacks surpass in all metrics all existing state-of-the-art methods.
Additionally, we present the first demonstration of adversarial attack
generation in the sound domain using the SHD dataset.

</details>


### [342] [User Behavior Analysis in Privacy Protection with Large Language Models: A Study on Privacy Preferences with Limited Data](https://arxiv.org/abs/2505.06305)
*Haowei Yang,Qingyi Lu,Yang Wang,Sibei Liu,Jiayun Zheng,Ao Xiang*

Main category: cs.CR

TL;DR: 通过集成少样本学习和隐私计算，利用大语言模型在有限数据下提升用户隐私偏好建模准确性，并引入差分隐私和联邦学习降低数据暴露风险。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，用户隐私保护成为重要研究课题，但现有方法依赖大规模数据，在数据有限环境下难以有效分析隐私偏好。

Method: 提出了一种结合少样本学习和隐私计算的方法，利用匿名隐私设置数据、调查响应和模拟数据，对比传统与LLM建模方法的性能。

Result: 实验表明，即使数据有限，LLM显著提升了隐私偏好建模的准确性，且差分隐私和联邦学习进一步降低了数据暴露风险。

Conclusion: 研究为LLM在隐私保护中的应用提供了新视角，并为隐私计算和用户行为分析的进步提供了理论支持。

Abstract: With the widespread application of large language models (LLMs), user privacy
protection has become a significant research topic. Existing privacy preference
modeling methods often rely on large-scale user data, making effective privacy
preference analysis challenging in data-limited environments. This study
explores how LLMs can analyze user behavior related to privacy protection in
scenarios with limited data and proposes a method that integrates Few-shot
Learning and Privacy Computing to model user privacy preferences. The research
utilizes anonymized user privacy settings data, survey responses, and simulated
data, comparing the performance of traditional modeling approaches with
LLM-based methods. Experimental results demonstrate that, even with limited
data, LLMs significantly improve the accuracy of privacy preference modeling.
Additionally, incorporating Differential Privacy and Federated Learning further
reduces the risk of user data exposure. The findings provide new insights into
the application of LLMs in privacy protection and offer theoretical support for
advancing privacy computing and user behavior analysis.

</details>


### [343] [Large Language Model-driven Security Assistant for Internet of Things via Chain-of-Thought](https://arxiv.org/abs/2505.06307)
*Mingfei Zeng,Ming Xie,Xixi Zheng,Chunhai Li,Chuan Zhang,Liehuang Zhu*

Main category: cs.CR

TL;DR: 论文提出了一种基于大语言模型（LLM）的IoT安全助手ICoT，通过分解安全漏洞的多维度并生成个性化响应，显著提升了LLM对IoT安全问题的理解与解决方案的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着IoT技术的快速发展，其安全问题日益凸显，现有方法难以适应复杂动态的安全场景。如何自动、高效且准确地理解这些漏洞成为挑战。

Method: 提出ICoT方法，通过分解安全漏洞的各个维度，使LLM能逐步分析和推理复杂安全场景，生成基于用户需求和专业水平的个性化解决方案。

Result: 实验表明，相比纯LLM方法，ICoT显著提升了对IoT安全问题的理解，并提供更高准确性和可靠性的个性化解决方案。

Conclusion: ICoT驱动的LLM安全助手在理解IoT安全漏洞和提供个性化解决方案方面表现出色，为IoT安全研究提供了新方向。

Abstract: The rapid development of Internet of Things (IoT) technology has transformed
people's way of life and has a profound impact on both production and daily
activities. However, with the rapid advancement of IoT technology, the security
of IoT devices has become an unavoidable issue in both research and
applications. Although some efforts have been made to detect or mitigate IoT
security vulnerabilities, they often struggle to adapt to the complexity of IoT
environments, especially when dealing with dynamic security scenarios. How to
automatically, efficiently, and accurately understand these vulnerabilities
remains a challenge. To address this, we propose an IoT security assistant
driven by Large Language Model (LLM), which enhances the LLM's understanding of
IoT security vulnerabilities and related threats. The aim of the ICoT method we
propose is to enable the LLM to understand security issues by breaking down the
various dimensions of security vulnerabilities and generating responses
tailored to the user's specific needs and expertise level. By incorporating
ICoT, LLM can gradually analyze and reason through complex security scenarios,
resulting in more accurate, in-depth, and personalized security recommendations
and solutions. Experimental results show that, compared to methods relying
solely on LLM, our proposed LLM-driven IoT security assistant significantly
improves the understanding of IoT security issues through the ICoT approach and
provides personalized solutions based on the user's identity, demonstrating
higher accuracy and reliability.

</details>


### [344] [One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models](https://arxiv.org/abs/2505.07167)
*Haoran Gu,Handing Wang,Yi Mei,Mengjie Zhang,Yaochu Jin*

Main category: cs.CR

TL;DR: 该论文提出了D-STT防御算法，通过识别和解码安全触发词来增强大语言模型的安全性，减少有害输出，同时保持模型可用性。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型（LLMs）尽管经过安全对齐，但仍容易受到越狱攻击的威胁，导致生成有害内容。研究发现安全对齐的LLMs在拒绝响应时生成高度相似的初始词（安全触发词），这成为防御的关键。

Method: 提出了D-STT算法，通过识别和解码安全触发词来提前激活模型的安全模式，仅需干预单个词，最小化解码过程的影响。

Result: 实验表明，D-STT能显著降低有害输出，同时在模型可用性和响应时间上几乎无额外开销，优于十种基线方法。

Conclusion: D-STT是一种简单高效的防御方法，有效平衡安全性和模型实用性，为LLMs的安全防御提供了新思路。

Abstract: Large Language Models (LLMs) have been extensively used across diverse
domains, including virtual assistants, automated code generation, and
scientific research. However, they remain vulnerable to jailbreak attacks,
which manipulate the models into generating harmful responses despite safety
alignment. Recent studies have shown that current safety-aligned LLMs often
undergo the shallow safety alignment, where the first few tokens largely
determine whether the response will be harmful. Through comprehensive
observations, we find that safety-aligned LLMs and various defense strategies
generate highly similar initial tokens in their refusal responses, which we
define as safety trigger tokens. Building on this insight, we propose
\texttt{D-STT}, a simple yet effective defense algorithm that identifies and
explicitly decodes safety trigger tokens of the given safety-aligned LLM to
trigger the model's learned safety patterns. In this process, the safety
trigger is constrained to a single token, which effectively preserves model
usability by introducing minimum intervention in the decoding process.
Extensive experiments across diverse jailbreak attacks and benign prompts
demonstrate that \ours significantly reduces output harmfulness while
preserving model usability and incurring negligible response time overhead,
outperforming ten baseline methods.

</details>


### [345] [Defending against Indirect Prompt Injection by Instruction Detection](https://arxiv.org/abs/2505.06311)
*Tongyu Wen,Chenglong Wang,Xiyuan Yang,Haoyu Tang,Yueqi Xie,Lingjuan Lyu,Zhicheng Dou,Fangzhao Wu*

Main category: cs.CR

TL;DR: 该论文提出了一种利用LLM的行为状态检测间接提示注入（IPI）攻击的新方法，通过前向和反向传播中的隐藏状态和梯度特征，实现了高检测准确率和低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着LLM与外部数据（如RAG）的集成增加，IPI攻击的风险上升，即隐藏指令可能操控LLM执行有害行为。现有防御手段有限，因此需探索基于LLM行为状态变化的检测方法。

Method: 提出一种通过分析前向和反向传播中的隐藏状态及梯度特征的方法，检测外部数据中的潜在指令，以识别IPI攻击。

Result: 方法在BIPIA基准测试中达到99.60%（同域）和96.90%（跨域）的检测准确率，并将攻击成功率降至0.12%。

Conclusion: 通过结合LLM的行为状态特征，能高效防御IPI攻击，为安全集成外部数据提供可靠解决方案。

Abstract: The integration of Large Language Models (LLMs) with external sources is
becoming increasingly common, with Retrieval-Augmented Generation (RAG) being a
prominent example. However, this integration introduces vulnerabilities of
Indirect Prompt Injection (IPI) attacks, where hidden instructions embedded in
external data can manipulate LLMs into executing unintended or harmful actions.
We recognize that the success of IPI attacks fundamentally relies in the
presence of instructions embedded within external content, which can alter the
behavioral state of LLMs. Can effectively detecting such state changes help us
defend against IPI attacks? In this paper, we propose a novel approach that
takes external data as input and leverages the behavioral state of LLMs during
both forward and backward propagation to detect potential IPI attacks.
Specifically, we demonstrate that the hidden states and gradients from
intermediate layers provide highly discriminative features for instruction
detection. By effectively combining these features, our approach achieves a
detection accuracy of 99.60\% in the in-domain setting and 96.90\% in the
out-of-domain setting, while reducing the attack success rate to just 0.12\% on
the BIPIA benchmark.

</details>


### [346] [Securing Genomic Data Against Inference Attacks in Federated Learning Environments](https://arxiv.org/abs/2505.07188)
*Chetan Pathade,Shubham Patil*

Main category: cs.CR

TL;DR: 论文研究了联邦学习（FL）在基因组数据中的隐私漏洞，通过模拟攻击发现梯度暴露的风险最高，呼吁强化隐私保护机制。


<details>
  <summary>Details</summary>
Motivation: 尽管联邦学习保护了数据本地性，但仍面临推理攻击的风险，尤其是基因组数据的敏感性需要更严格的隐私保护。

Method: 使用合成基因组数据模拟联邦学习环境，评估了三种攻击方式（MIA、梯度MIA和LIA）的效果。

Result: 梯度MIA效果最显著（精度0.79，F1得分0.87），雷达图展示了不同攻击的性能对比，突出了梯度暴露的高风险。

Conclusion: 常规联邦学习难以保护基因组隐私，需开发针对性更强的隐私保护方法。

Abstract: Federated Learning (FL) offers a promising framework for collaboratively
training machine learning models across decentralized genomic datasets without
direct data sharing. While this approach preserves data locality, it remains
susceptible to sophisticated inference attacks that can compromise individual
privacy. In this study, we simulate a federated learning setup using synthetic
genomic data and assess its vulnerability to three key attack vectors:
Membership Inference Attack (MIA), Gradient-Based Membership Inference Attack,
and Label Inference Attack (LIA). Our experiments reveal that Gradient-Based
MIA achieves the highest effectiveness, with a precision of 0.79 and F1-score
of 0.87, underscoring the risk posed by gradient exposure in federated updates.
Additionally, we visualize comparative attack performance through radar plots
and quantify model leakage across clients. The findings emphasize the
inadequacy of na\"ive FL setups in safeguarding genomic privacy and motivate
the development of more robust privacy-preserving mechanisms tailored to the
unique sensitivity of genomic data.

</details>


### [347] [Threat Modeling for AI: The Case for an Asset-Centric Approach](https://arxiv.org/abs/2505.06315)
*Jose Sanchez Vicarte,Marcin Spoczynski,Mostafa Elsaid*

Main category: cs.CR

TL;DR: 提出了一个以资产为中心的AI系统威胁建模方法，解决了集成AI代理带来的独特安全挑战。该方法通过聚焦资产而非攻击，适用于复杂且快速演变的AI开发生态。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理能力的增强，其能够自主决策和执行操作（包括编写和执行脚本），传统安全方法已无法应对由此带来的新安全风险。

Method: 采用自下而上的资产中心化方法，系统性识别跨分布式基础设施的漏洞（包括常规和AI特有的），并通过量化安全假设和全域分析来提升安全性。

Result: 这一方法能够支持安全团队全面分析跨技术领域的风险、量化第三方AI组件的安全假设，并识别特定产品环境中的AI漏洞。

Conclusion: 聚焦资产的方法能够有效应对快速演变的威胁环境和复杂的AI开发生态，尤其适用于具备高度自主能力的AI代理系统。

Abstract: Recent advances in AI are transforming AI's ubiquitous presence in our world
from that of standalone AI-applications into deeply integrated AI-agents. These
changes have been driven by agents' increasing capability to autonomously make
decisions and initiate actions, using existing applications; whether those
applications are AI-based or not. This evolution enables unprecedented levels
of AI integration, with agents now able to take actions on behalf of systems
and users -- including, in some cases, the powerful ability for the AI to write
and execute scripts as it deems necessary. With AI systems now able to
autonomously execute code, interact with external systems, and operate without
human oversight, traditional security approaches fall short.
  This paper introduces an asset-centric methodology for threat modeling AI
systems that addresses the unique security challenges posed by integrated AI
agents. Unlike existing top-down frameworks that analyze individual attacks
within specific product contexts, our bottom-up approach enables defenders to
systematically identify how vulnerabilities -- both conventional and
AI-specific -- impact critical AI assets across distributed infrastructures
used to develop and deploy these agents. This methodology allows security teams
to: (1) perform comprehensive analysis that communicates effectively across
technical domains, (2) quantify security assumptions about third-party AI
components without requiring visibility into their implementation, and (3)
holistically identify AI-based vulnerabilities relevant to their specific
product context. This approach is particularly relevant for securing agentic
systems with complex autonomous capabilities. By focusing on assets rather than
attacks, our approach scales with the rapidly evolving threat landscape while
accommodating increasingly complex and distributed AI development pipelines.

</details>


### [348] [Offensive Security for AI Systems: Concepts, Practices, and Applications](https://arxiv.org/abs/2505.06380)
*Josh Harguess,Chris M. Ward*

Main category: cs.CR

TL;DR: 该论文提出了一个针对AI系统的主动安全框架，通过模拟真实攻击场景和对抗测试来识别漏洞，提升AI系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的广泛应用，传统防御措施难以应对其独特且不断演变的安全威胁，因此需要开发主动的安全策略。

Method: 论文提出了一套针对AI系统的进攻性安全框架，包括弱点评估、渗透测试和红队演练等方法，以模拟真实攻击场景。

Result: 该框架能够有效识别AI系统的漏洞，并为制定更强大的防御策略提供关键见解，提升系统的抗威胁能力。

Conclusion: 该研究将进攻性AI安全从理论转化为实践中可操作的方法论，帮助组织增强其AI系统对新兴威胁的抵御能力。

Abstract: As artificial intelligence (AI) systems become increasingly adopted across
sectors, the need for robust, proactive security strategies is paramount.
Traditional defensive measures often fall short against the unique and evolving
threats facing AI-driven technologies, making offensive security an essential
approach for identifying and mitigating risks. This paper presents a
comprehensive framework for offensive security in AI systems, emphasizing
proactive threat simulation and adversarial testing to uncover vulnerabilities
throughout the AI lifecycle. We examine key offensive security techniques,
including weakness and vulnerability assessment, penetration testing, and red
teaming, tailored specifically to address AI's unique susceptibilities. By
simulating real-world attack scenarios, these methodologies reveal critical
insights, informing stronger defensive strategies and advancing resilience
against emerging threats. This framework advances offensive AI security from
theoretical concepts to practical, actionable methodologies that organizations
can implement to strengthen their AI systems against emerging threats.

</details>


### [349] [Towards AI-Driven Human-Machine Co-Teaming for Adaptive and Agile Cyber Security Operation Centers](https://arxiv.org/abs/2505.06394)
*Massimiliano Albanese,Xinming Ou,Kevin Lybarger,Daniel Lende,Dmitry Goldgof*

Main category: cs.CR

TL;DR: 该论文提出了一个AI驱动的人机协作模式，利用大型语言模型（LLMs）增强SOC分析师的能力，以应对日益增长的网络安全威胁。


<details>
  <summary>Details</summary>
Motivation: 由于警报数量庞大、分析师短缺及工具集成不佳，SOC在管理网络安全威胁方面面临巨大挑战。人机协作被视为提升分析师能力并减轻其认知负荷的有效途径。

Method: 引入基于LLM的人机协作范式，通过学习人类分析师的隐性知识，改进AI在威胁情报、警报分类和事件响应任务中的表现。

Result: 提出了一个愿景，即通过人机协作提升SOC任务效率，并邀请SOC合作以发现可重复的效率提升模式。

Conclusion: 人机协作是提升SOC生产力的可行方向，需进一步研究以验证其实际效果。

Abstract: Security Operations Centers (SOCs) face growing challenges in managing
cybersecurity threats due to an overwhelming volume of alerts, a shortage of
skilled analysts, and poorly integrated tools. Human-AI collaboration offers a
promising path to augment the capabilities of SOC analysts while reducing their
cognitive overload. To this end, we introduce an AI-driven human-machine
co-teaming paradigm that leverages large language models (LLMs) to enhance
threat intelligence, alert triage, and incident response workflows. We present
a vision in which LLM-based AI agents learn from human analysts the tacit
knowledge embedded in SOC operations, enabling the AI agents to improve their
performance on SOC tasks through this co-teaming. We invite SOCs to collaborate
with us to further develop this process and uncover replicable patterns where
human-AI co-teaming yields measurable improvements in SOC productivity.

</details>


### [350] [Engineering Risk-Aware, Security-by-Design Frameworks for Assurance of Large-Scale Autonomous AI Models](https://arxiv.org/abs/2505.06409)
*Krti Tallam*

Main category: cs.CR

TL;DR: 该论文提出了一个企业级、风险感知、设计即安全的框架，用于大规模自主AI系统的安全保障，整合了标准化威胁指标、对抗性强化技术和实时异常检测，并在开发周期各个阶段实施。案例研究表明，该方法在国家安全和工业自动化中有效减少了漏洞和合规开销。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型参数量增至数十亿且自主性增强，确保其安全可靠运行需要工程级的安全和保障框架。

Method: 论文采用了一种统一的安全开发流程，包括设计时风险评估、安全训练协议、持续监控和自动审计日志，以在对抗性和操作压力下提供模型行为的可证明保证。

Result: 在国家安���、开源模型治理和工业自动化等案例研究中，该方法显著减少了漏洞和合规开销。

Conclusion: 作者呼吁跨部门合作，联合工程团队、标准机构和监管机构，将这些技术保障制度化，以构建下一代AI的全方位韧性保障生态系统。

Abstract: As AI models scale to billions of parameters and operate with increasing
autonomy, ensuring their safe, reliable operation demands engineering-grade
security and assurance frameworks. This paper presents an enterprise-level,
risk-aware, security-by-design approach for large-scale autonomous AI systems,
integrating standardized threat metrics, adversarial hardening techniques, and
real-time anomaly detection into every phase of the development lifecycle. We
detail a unified pipeline - from design-time risk assessments and secure
training protocols to continuous monitoring and automated audit logging - that
delivers provable guarantees of model behavior under adversarial and
operational stress. Case studies in national security, open-source model
governance, and industrial automation demonstrate measurable reductions in
vulnerability and compliance overhead. Finally, we advocate cross-sector
collaboration - uniting engineering teams, standards bodies, and regulatory
agencies - to institutionalize these technical safeguards within a resilient,
end-to-end assurance ecosystem for the next generation of AI.

</details>


### [351] [System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection](https://arxiv.org/abs/2505.06493)
*Jiawei Guo,Haipeng Cai*

Main category: cs.CR

TL;DR: 该论文提出了一种针对大语言模型（LLMs）的新型攻击方式——系统提示中毒（system prompt poisoning），填补了现有研究中系统提示安全的空白，并通过实验展示了其可行性和广泛影响。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在各领域的广泛应用，其安全性日益受到关注，但现有研究多集中于用户提示或模型输出相关的威胁，而系统提示的安全性被忽视。本文旨在填补这一关键空白。

Method: 作者提出系统提示中毒攻击，通过四种策略在不同场景下毒害系统提示，进而影响所有后续用户交互和模型响应。实验在生成和推理类LLMs上进行，无需越狱技术即可实现高效攻击。

Result: 实验证明系统提示中毒攻击在数学、编程、逻辑推理和自然语言处理等广泛任务中均有效，甚至能削弱链式思考（CoT）和检索增强生成（RAG）等先进提示技术的效果。

Conclusion: 系统提示中毒是一种新型且强效的攻击方式，揭示了LLMs系统层面的安全漏洞，呼吁未来研究关注此类威胁并提出防御措施。

Abstract: Large language models (LLMs) have gained widespread adoption across diverse
applications due to their impressive generative capabilities. Their
plug-and-play nature enables both developers and end users to interact with
these models through simple prompts. However, as LLMs become more integrated
into various systems in diverse domains, concerns around their security are
growing. Existing studies mainly focus on threats arising from user prompts
(e.g. prompt injection attack) and model output (e.g. model inversion attack),
while the security of system prompts remains largely overlooked. This work
bridges the critical gap. We introduce system prompt poisoning, a new attack
vector against LLMs that, unlike traditional user prompt injection, poisons
system prompts hence persistently impacts all subsequent user interactions and
model responses. We systematically investigate four practical attack strategies
in various poisoning scenarios. Through demonstration on both generative and
reasoning LLMs, we show that system prompt poisoning is highly feasible without
requiring jailbreak techniques, and effective across a wide range of tasks,
including those in mathematics, coding, logical reasoning, and natural language
processing. Importantly, our findings reveal that the attack remains effective
even when user prompts employ advanced prompting techniques like
chain-of-thought (CoT). We also show that such techniques, including CoT and
retrieval-augmentation-generation (RAG), which are proven to be effective for
improving LLM performance in a wide range of tasks, are significantly weakened
in their effectiveness by system prompt poisoning.

</details>


### [352] [AI-Powered Anomaly Detection with Blockchain for Real-Time Security and Reliability in Autonomous Vehicles](https://arxiv.org/abs/2505.06632)
*Rathin Chandra Shit,Sharmila Subudhi*

Main category: cs.CR

TL;DR: 该论文提出了一种结合AI实时异常检测和区块链技术的新框架，以提高自动驾驶车辆的安全性和可靠性，涵盖异常检测、数据防篡改和实时响应机制。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的普及带来了安全和可靠性问题，需解决这些问题以确保公共安全并促进广泛采用。

Method: 利用LSTM网络实时监控多传感器数据流以检测异常，并通过区块链技术存储数据和警报，结合智能合约实现自动化响应。

Result: 框架增强了自动驾驶系统对网络攻击和硬件故障的抵御能力，同时确保了数据的真实性和透明性。

Conclusion: 研究为自动驾驶提供了更安全可靠的解决方案，但需解决高频数据处理、资源限制和隐私等潜在挑战。

Abstract: Autonomous Vehicles (AV) proliferation brings important and pressing security
and reliability issues that must be dealt with to guarantee public safety and
help their widespread adoption. The contribution of the proposed research is
towards achieving more secure, reliable, and trustworthy autonomous
transportation system by providing more capabilities for anomaly detection,
data provenance, and real-time response in safety critical AV deployments. In
this research, we develop a new framework that combines the power of Artificial
Intelligence (AI) for real-time anomaly detection with blockchain technology to
detect and prevent any malicious activity including sensor failures in AVs.
Through Long Short-Term Memory (LSTM) networks, our approach continually
monitors associated multi-sensor data streams to detect anomalous patterns that
may represent cyberattacks as well as hardware malfunctions. Further, this
framework employs a decentralized platform for securely storing sensor data and
anomaly alerts in a blockchain ledger for data incorruptibility and
authenticity, while offering transparent forensic features. Moreover, immediate
automated response mechanisms are deployed using smart contracts when anomalies
are found. This makes the AV system more resilient to attacks from both
cyberspace and hardware component failure. Besides, we identify potential
challenges of scalability in handling high frequency sensor data, computational
constraint in resource constrained environment, and of distributed data storage
in terms of privacy.

</details>


### [353] [ThreatLens: LLM-guided Threat Modeling and Test Plan Generation for Hardware Security Verification](https://arxiv.org/abs/2505.06821)
*Dipayan Saha,Hasan Al Shaikh,Shams Tarek,Farimah Farahmandi*

Main category: cs.CR

TL;DR: ThreatLens使用大语言模型驱动的多智能体框架自动化硬件安全验证，减少人工投入，提高覆盖率和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前硬件安全验证主要依赖人工方法，效率低、易出错且难以适应设计复杂度和攻击方法的增长。

Method: 集成检索增强生成(RAG)提取安全知识，利用大语言模型进行威胁评估，并结合用户反馈生成实用测试计划。

Result: 在NEORV32 SoC上的验证表明，该框架能自动化生成结构化测试计划，有效应用于实际场景。

Conclusion: ThreatLens框架显著提升硬件安全验证的自动化程度、覆盖范围和实用性。

Abstract: Current hardware security verification processes predominantly rely on manual
threat modeling and test plan generation, which are labor-intensive,
error-prone, and struggle to scale with increasing design complexity and
evolving attack methodologies. To address these challenges, we propose
ThreatLens, an LLM-driven multi-agent framework that automates security threat
modeling and test plan generation for hardware security verification.
ThreatLens integrates retrieval-augmented generation (RAG) to extract relevant
security knowledge, LLM-powered reasoning for threat assessment, and
interactive user feedback to ensure the generation of practical test plans. By
automating these processes, the framework reduces the manual verification
effort, enhances coverage, and ensures a structured, adaptable approach to
security verification. We evaluated our framework on the NEORV32 SoC,
demonstrating its capability to automate security verification through
structured test plans and validating its effectiveness in real-world scenarios.

</details>


### [354] [Sandcastles in the Storm: Revisiting the (Im)possibility of Strong Watermarking](https://arxiv.org/abs/2505.06827)
*Fabrice Y Harel-Canada,Boran Erol,Connor Choi,Jason Liu,Gary Jiarui Song,Nanyun Peng,Amit Sahai*

Main category: cs.CR

TL;DR: 现有研究认为AI生成文本的水印可通过随机扰动攻击轻松去除，但本文通过实验发现扰动后水印残留显著，且自动化质量检测不可靠，实际攻击成功率远低于理论预期，表明水印技术比理论模型更稳健。


<details>
  <summary>Details</summary>
Motivation: 探讨AI生成文本水印技术在实际攻击下的鲁棒性，反驳现有理论中关于水印易去除的假设。

Method: 通过大规模实验和人工验证，测试随机扰动攻击对水印的影响及自动化质量检测的准确性。

Result: 实验显示水印在数百次扰动后仍存在（100%残留），自动化质量检测准确率仅77%，实际攻击去除水印成功率仅26%（人工审核时降至10%）。

Conclusion: 水印技术在实际中因扰动混合速度慢和质量控制不完善而表现稳健，需开发更强水印方法并改进攻击模型以更贴近现实。

Abstract: Watermarking AI-generated text is critical for combating misuse. Yet recent
theoretical work argues that any watermark can be erased via random walk
attacks that perturb text while preserving quality. However, such attacks rely
on two key assumptions: (1) rapid mixing (watermarks dissolve quickly under
perturbations) and (2) reliable quality preservation (automated quality oracles
perfectly guide edits). Through large-scale experiments and human-validated
assessments, we find mixing is slow: 100% of perturbed texts retain traces of
their origin after hundreds of edits, defying rapid mixing. Oracles falter, as
state-of-the-art quality detectors misjudge edits (77% accuracy), compounding
errors during attacks. Ultimately, attacks underperform: automated walks remove
watermarks just 26% of the time -- dropping to 10% under human quality review.
These findings challenge the inevitability of watermark removal. Instead,
practical barriers -- slow mixing and imperfect quality control -- reveal
watermarking to be far more robust than theoretical models suggest. The gap
between idealized attacks and real-world feasibility underscores the need for
stronger watermarking methods and more realistic attack models.

</details>


### [355] [DP-TRAE: A Dual-Phase Merging Transferable Reversible Adversarial Example for Image Privacy Protection](https://arxiv.org/abs/2505.06860)
*Xia Du,Jiajie Zhu,Jizhe Zhou,Chi-man Pun,Zheng Lin,Cong Wu,Zhe Chen,Jun Luo*

Main category: cs.CR

TL;DR: 本文提出了一种双相合并可逆攻击方法（Dual-Phase Merging Transferable Reversible Attack），以提高黑盒场景下的对抗样本攻击效果，实现99.0%的攻击成功率和100%的恢复率。


<details>
  <summary>Details</summary>
Motivation: 现有可逆对抗样本（RAE）技术主要针对白盒攻击，缺乏对黑盒场景的全面评估，传统黑盒攻击又因迁移性差和查询成本高而实用性受限。

Method: 提出双相合并方法，首先生成高迁移性的初始对抗扰动（白盒阶段），再通过记忆增强的黑盒策略误导目标模型。

Result: 实验证明该方法在黑盒场景下攻击成功率达99.0%，恢复率100%，并在商业模型上成功实现黑盒攻击。

Conclusion: 该方法显著提升了对抗样本在黑盒环境中的实用性和隐私保护能力。

Abstract: In the field of digital security, Reversible Adversarial Examples (RAE)
combine adversarial attacks with reversible data hiding techniques to
effectively protect sensitive data and prevent unauthorized analysis by
malicious Deep Neural Networks (DNNs). However, existing RAE techniques
primarily focus on white-box attacks, lacking a comprehensive evaluation of
their effectiveness in black-box scenarios. This limitation impedes their
broader deployment in complex, dynamic environments. Further more, traditional
black-box attacks are often characterized by poor transferability and high
query costs, significantly limiting their practical applicability. To address
these challenges, we propose the Dual-Phase Merging Transferable Reversible
Attack method, which generates highly transferable initial adversarial
perturbations in a white-box model and employs a memory augmented black-box
strategy to effectively mislead target mod els. Experimental results
demonstrate the superiority of our approach, achieving a 99.0% attack success
rate and 100% recovery rate in black-box scenarios, highlighting its robustness
in privacy protection. Moreover, we successfully implemented a black-box attack
on a commercial model, further substantiating the potential of this approach
for practical use.

</details>


### [356] [RedTeamLLM: an Agentic AI framework for offensive security](https://arxiv.org/abs/2505.06913)
*Brian Challita,Pierre Parrend*

Main category: cs.CR

TL;DR: RedTeamLLM是一个集成的AI架构，用于自动化渗透测试任务，通过总结、推理和执行三个关键步骤解决四项开放挑战，并在CTF挑战中验证其推理能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI在安全工程中的潜力增加，恶意行为者也可能利用类似技术进行网络犯罪。因此，开发一个先进的AI框架（如RedTeamLLM）以提前应对这一威胁是有必要的。

Method: RedTeamLLM架构遵循总结、推理和执行的步骤，解决计划修正、内存管理、上下文窗口限制以及通用性与专业化的平衡问题。

Result: 框架通过自动化解决初级但非平凡的CTF挑战进行评估，验证了其推理能力的贡献。

Conclusion: RedTeamLLM为安全工程提供了一个有效的AI驱动框架，能够在恶意行为者之前识别和应对潜在威胁。

Abstract: From automated intrusion testing to discovery of zero-day attacks before
software launch, agentic AI calls for great promises in security engineering.
This strong capability is bound with a similar threat: the security and
research community must build up its models before the approach is leveraged by
malicious actors for cybercrime. We therefore propose and evaluate RedTeamLLM,
an integrated architecture with a comprehensive security model for
automatization of pentest tasks. RedTeamLLM follows three key steps:
summarizing, reasoning and act, which embed its operational capacity. This
novel framework addresses four open challenges: plan correction, memory
management, context window constraint, and generality vs. specialization.
Evaluation is performed through the automated resolution of a range of
entry-level, but not trivial, CTF challenges. The contribution of the reasoning
capability of our agentic AI framework is specifically evaluated.

</details>


### [357] [Comet: Accelerating Private Inference for Large Language Model by Predicting Activation Sparsity](https://arxiv.org/abs/2505.07239)
*Guang Yan,Yuhui Zhang,Zimu Guo,Lutan Zhao,Xiaojun Chen,Chen Wang,Wenhao Wang,Dan Meng,Rui Hou*

Main category: cs.CR

TL;DR: Comet是一种高效的私有推理系统，通过预测LLM激活函数的稀疏分布，减少零值计算，提升隐私保护下的推理效率。


<details>
  <summary>Details</summary>
Motivation: 随着云平台LLM推理服务的普及，隐私泄露风险增加，而现有的安全多方计算（MPC）方法通信开销高。

Method: 利用LLM激活稀疏性，设计预测器和私有推理协议，避免零值计算，并通过缓存重填策略降低通信开销。

Result: 在四种常见LLM上测试，Comet比现有系统提速1.87x-2.63x，通信减少1.94x-2.64x。

Conclusion: Comet显著提升了私有推理的效率和通信效率，为隐私保护LLM推理提供了可行方案。

Abstract: With the growing use of large language models (LLMs) hosted on cloud
platforms to offer inference services, privacy concerns about the potential
leakage of sensitive information are escalating. Secure multi-party computation
(MPC) is a promising solution to protect the privacy in LLM inference. However,
MPC requires frequent inter-server communication, causing high performance
overhead.
  Inspired by the prevalent activation sparsity of LLMs, where most neuron are
not activated after non-linear activation functions, we propose an efficient
private inference system, Comet. This system employs an accurate and fast
predictor to predict the sparsity distribution of activation function output.
Additionally, we introduce a new private inference protocol. It efficiently and
securely avoids computations involving zero values by exploiting the spatial
locality of the predicted sparse distribution. While this computation-avoidance
approach impacts the spatiotemporal continuity of KV cache entries, we address
this challenge with a low-communication overhead cache refilling strategy that
merges miss requests and incorporates a prefetching mechanism. Finally, we
evaluate Comet on four common LLMs and compare it with six state-of-the-art
private inference systems. Comet achieves a 1.87x-2.63x speedup and a
1.94x-2.64x communication reduction.

</details>


### [358] [RuleGenie: SIEM Detection Rule Set Optimization](https://arxiv.org/abs/2505.06701)
*Akansha Shukla,Parth Atulbhai Gandhi,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TL;DR: 提出了一个名为RuleGenie的基于大型语言模型的推荐系统，用于优化SIEM规则集，减少冗余和误报。


<details>
  <summary>Details</summary>
Motivation: SIEM系统中存在冗余和重叠规则，导致高误报率和计算开销，而现有方法依赖人工优化，效率低下且易出错。

Method: 利用Transformer模型生成规则嵌入，通过相似性匹配算法识别冗余规则，并结合LLM的语言理解和推理能力提供优化建议。

Result: 实验表明，RuleGenie能有效识别冗余规则，降低误报率并提升规则效率。

Conclusion: RuleGenie自动化优化过程，提升SIEM系统效率和安全性，具有平台无关性和适应性。

Abstract: SIEM systems serve as a critical hub, employing rule-based logic to detect
and respond to threats. Redundant or overlapping rules in SIEM systems lead to
excessive false alerts, degrading analyst performance due to alert fatigue, and
increase computational overhead and response latency for actual threats. As a
result, optimizing SIEM rule sets is essential for efficient operations.
Despite the importance of such optimization, research in this area is limited,
with current practices relying on manual optimization methods that are both
time-consuming and error-prone due to the scale and complexity of
enterprise-level rule sets. To address this gap, we present RuleGenie, a novel
large language model (LLM) aided recommender system designed to optimize SIEM
rule sets. Our approach leverages transformer models' multi-head attention
capabilities to generate SIEM rule embeddings, which are then analyzed using a
similarity matching algorithm to identify the top-k most similar rules. The LLM
then processes the rules identified, utilizing its information extraction,
language understanding, and reasoning capabilities to analyze rule similarity,
evaluate threat coverage and performance metrics, and deliver optimized
recommendations for refining the rule set. By automating the rule optimization
process, RuleGenie allows security teams to focus on more strategic tasks while
enhancing the efficiency of SIEM systems and strengthening organizations'
security posture. We evaluated RuleGenie on a comprehensive set of real-world
SIEM rule formats, including Splunk, Sigma, and AQL (Ariel query language),
demonstrating its platform-agnostic capabilities and adaptability across
diverse security infrastructures. Our experimental results show that RuleGenie
can effectively identify redundant rules, which in turn decreases false
positive rates and enhances overall rule efficiency.

</details>


### [359] [Source Anonymity for Private Random Walk Decentralized Learning](https://arxiv.org/abs/2505.07011)
*Maximilian Egger,Svenja Lage,Rawad Bitar,Antonia Wachter-Zeh*

Main category: cs.CR

TL;DR: 论文提出了一种基于随机漫步的隐私保护去中心化学习算法，结合公钥加密和匿名化技术，确保在模型更新过程中源用户身份的隐匿性。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习中的数据隐私保护是一个重要但未解决的问题，特别是在用户随机选择邻居进行模型更新的场景中，如何隐藏源用户身份是关键挑战。

Method: 算法利用公钥加密和匿名化技术，源用户用目标用户的公钥加密模型更新结果，并通过网络传递，确保目标用户无法追踪源用户身份。通过设计网络相关的概率分布，保证接收者认为所有用户作为源的可能性相近。

Result: 在随机正则图上，所提方案实现了理论保证的匿名性，证明了算法的可行性。

Conclusion: 该研究为去中心化学习中的隐私保护提供了一种有效方法，尤其在随机网络拓扑下具有理论保障的隐匿性。

Abstract: This paper considers random walk-based decentralized learning, where at each
iteration of the learning process, one user updates the model and sends it to a
randomly chosen neighbor until a convergence criterion is met. Preserving data
privacy is a central concern and open problem in decentralized learning. We
propose a privacy-preserving algorithm based on public-key cryptography and
anonymization. In this algorithm, the user updates the model and encrypts the
result using a distant user's public key. The encrypted result is then
transmitted through the network with the goal of reaching that specific user.
The key idea is to hide the source's identity so that, when the destination
user decrypts the result, it does not know who the source was. The challenge is
to design a network-dependent probability distribution (at the source) over the
potential destinations such that, from the receiver's perspective, all users
have a similar likelihood of being the source. We introduce the problem and
construct a scheme that provides anonymity with theoretical guarantees. We
focus on random regular graphs to establish rigorous guarantees.

</details>


### [360] [Private LoRA Fine-tuning of Open-Source LLMs with Homomorphic Encryption](https://arxiv.org/abs/2505.07329)
*Jordan Frery,Roman Bredehoft,Jakub Klemsa,Arthur Meyre,Andrei Stoian*

Main category: cs.CR

TL;DR: 本文提出了一种基于低秩适应（LoRA）技术和同态加密（HE）的私有微调协议，用于保护开源大语言模型（LLMs）微调过程中的数据机密性。通过远程计算节点处理大部分计算，减轻了客户端对高性能硬件的依赖，并在Llama-3.2-1B模型上验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 敏感应用场景下，保护开源LLMs微调过程中的数据机密性至关重要。现有的方法通常需要高昂的客户端计算资源，且缺乏足够的数据保护机制。

Method: 采用LoRA技术结合HE，远程计算节点处理主要计算任务，保护训练数据和梯度的机密性，同时减少了客户端的计算负担。

Result: 在Llama-3.2-1B模型上的实验表明，该方法能够实现收敛，并在GPU硬件上高效运行HE计算，验证了其实用性。

Conclusion: 该方法为机密知识库问答、私有代码库微调等敏感场景提供了一种可行的解决方案，同时降低了客户端硬件需求。

Abstract: Preserving data confidentiality during the fine-tuning of open-source Large
Language Models (LLMs) is crucial for sensitive applications. This work
introduces an interactive protocol adapting the Low-Rank Adaptation (LoRA)
technique for private fine-tuning. Homomorphic Encryption (HE) protects the
confidentiality of training data and gradients handled by remote worker nodes
performing the bulk of computations involving the base model weights. The data
owner orchestrates training, requiring minimal local computing power and
memory, thus alleviating the need for expensive client-side GPUs. We
demonstrate feasibility by fine-tuning a Llama-3.2-1B model, presenting
convergence results using HE-compatible quantization and performance benchmarks
for HE computations on GPU hardware. This approach enables applications such as
confidential knowledge base question answering, private codebase fine-tuning
for AI code assistants, AI agents for drafting emails based on a company's
email archive, and adapting models to analyze sensitive legal or healthcare
documents.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [361] [A Comprehensive Data Description for LoRaWAN Path Loss Measurements in an Indoor Office Setting: Effects of Environmental Factors](https://arxiv.org/abs/2505.06375)
*Nahshon Mokua Obiri,Kristof Van Laerhoven*

Main category: cs.NI

TL;DR: 论文提出了一种室内LoRaWAN信号传播的综合数据集，量化了环境因素对信号的影响，并改进了路径损耗模型，提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 研究目的是量化室内环境中环境因素对LoRaWAN信号传播的影响，以优化物联网设备的部署和能效。

Method: 通过在室内布设6个终端设备和1个网关，系统测量信号强度指标（如RSSI和SNR），并结合环境参数（温度、湿度等）分析信号衰减。提出改进的路径损耗模型（LDPLSM-MW-EP）。

Result: 改进模型将RMSE从10.58 dB降至8.04 dB，R²从0.6917提升至0.8222，显著提高了信号衰减预测的准确性。

Conclusion: 该数据集和改进模型为室内无线通信研究提供了重要基础，有助于优化物联网部署和延长设备寿命。

Abstract: This paper presents a comprehensive dataset of LoRaWAN technology path loss
measurements collected in an indoor office environment, focusing on quantifying
the effects of environmental factors on signal propagation. Utilizing a network
of six strategically placed LoRaWAN end devices (EDs) and a single indoor
gateway (GW) at the University of Siegen, City of Siegen, Germany, we
systematically measured signal strength indicators such as the Received Signal
Strength Indicator (RSSI) and the Signal-to-Noise Ratio (SNR) under various
environmental conditions, including temperature, relative humidity, carbon
dioxide (CO$_2$) concentration, barometric pressure, and particulate matter
levels (PM$_{2.5}$). Our empirical analysis confirms that transient phenomena
such as reflections, scattering, interference, occupancy patterns (induced by
environmental parameter variations), and furniture rearrangements can alter
signal attenuation by as much as 10.58 dB, highlighting the dynamic nature of
indoor propagation. As an example of how this dataset can be utilized, we
tested and evaluated a refined Log-Distance Path Loss and Shadowing Model that
integrates both structural obstructions (Multiple Walls) and Environmental
Parameters (LDPLSM-MW-EP). Compared to a baseline model that considers only
Multiple Walls (LDPLSM-MW), the enhanced approach reduced the root mean square
error (RMSE) from 10.58 dB to 8.04 dB and increased the coefficient of
determination (R$^2$) from 0.6917 to 0.8222. By capturing the extra effects of
environmental conditions and occupancy dynamics, this improved model provides
valuable insights for optimizing power usage and prolonging device battery
life, enhancing network reliability in indoor Internet of Things (IoT)
deployments, among other applications. This dataset offers a solid foundation
for future research and development in indoor wireless communication.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [362] [What Do People Want to Know About Artificial Intelligence (AI)? The Importance of Answering End-User Questions to Explain Autonomous Vehicle (AV) Decisions](https://arxiv.org/abs/2505.06428)
*Somayeh Molaei,Lionel P. Robert,Nikola Banovic*

Main category: cs.HC

TL;DR: 该研究通过两项用户调查，探讨了潜在自动驾驶汽车乘客可能提出的问题，并验证了交互式文本解释对提高乘客理解AI决策的效果。


<details>
  <summary>Details</summary>
Motivation: 提升终端用户对AI驱动的自动驾驶汽车决策的理解，以增强其使用率和接受度。现有的解释机制主要服务于AI研究人员和工程师，未能满足乘客在具体场景中的需求。

Method: 进行两项用户研究：第一项研究识别现有解释机制未涵盖的乘客问题；第二项研究评估交互式文本解释对提升乘客理解的效果。

Result: 交互式文本解释显著提高了参与者对自动驾驶汽车决策的理解，优于单纯观察决策的方式。

Conclusion: 研究结果为设计促使终端用户参与并询问AI决策背后原因的用户交互提供了参考。

Abstract: Improving end-users' understanding of decisions made by autonomous vehicles
(AVs) driven by artificial intelligence (AI) can improve utilization and
acceptance of AVs. However, current explanation mechanisms primarily help AI
researchers and engineers in debugging and monitoring their AI systems, and may
not address the specific questions of end-users, such as passengers, about AVs
in various scenarios. In this paper, we conducted two user studies to
investigate questions that potential AV passengers might pose while riding in
an AV and evaluate how well answers to those questions improve their
understanding of AI-driven AV decisions. Our initial formative study identified
a range of questions about AI in autonomous driving that existing explanation
mechanisms do not readily address. Our second study demonstrated that
interactive text-based explanations effectively improved participants'
comprehension of AV decisions compared to simply observing AV decisions. These
findings inform the design of interactions that motivate end-users to engage
with and inquire about the reasoning behind AI-driven AV decisions.

</details>


### [363] [Integrating Explainable AI in Medical Devices: Technical, Clinical and Regulatory Insights and Recommendations](https://arxiv.org/abs/2505.06620)
*Dima Alattal,Asal Khoshravan Azar,Puja Myles,Richard Branson,Hatim Abdulhussein,Allan Tucker*

Main category: cs.HC

TL;DR: 本文讨论了由英国MHRA组织的专家工作组关于AI在临床决策中应用的建议，重点关注AI算法的透明度、安全性及临床医生的培训需求。


<details>
  <summary>Details</summary>
Motivation: 由于AI在医疗领域的应用日益增多，但许多黑盒模型缺乏透明度，引发了对其安全性和可靠性的担忧。本文旨在探讨如何安全地将AI整合到临床决策中。

Method: 通过专家工作组（包括医疗专业人员、监管者和数据科学家）的讨论，并结合一项关于临床医生与AI交互行为的试点研究来评估AI算法的输出。

Result: 工作组指出了AI在临床决策中的关键挑战，如模型透明度和医生培训需求，并提供了安全采纳AI系统的具体建议。

Conclusion: 为确保AI在医疗领域的安全应用，需要提高模型透明度并加强相关人员的培训。本文提供了促进AI在临床环境中安全使用的进一步建议。

Abstract: There is a growing demand for the use of Artificial Intelligence (AI) and
Machine Learning (ML) in healthcare, particularly as clinical decision support
systems to assist medical professionals. However, the complexity of many of
these models, often referred to as black box models, raises concerns about
their safe integration into clinical settings as it is difficult to understand
how they arrived at their predictions. This paper discusses insights and
recommendations derived from an expert working group convened by the UK
Medicine and Healthcare products Regulatory Agency (MHRA). The group consisted
of healthcare professionals, regulators, and data scientists, with a primary
focus on evaluating the outputs from different AI algorithms in clinical
decision-making contexts. Additionally, the group evaluated findings from a
pilot study investigating clinicians' behaviour and interaction with AI methods
during clinical diagnosis. Incorporating AI methods is crucial for ensuring the
safety and trustworthiness of medical AI devices in clinical settings. Adequate
training for stakeholders is essential to address potential issues, and further
insights and recommendations for safely adopting AI systems in healthcare
settings are provided.

</details>


### [364] [R-CAGE: A Structural Model for Emotion Output Design in Human-AI Interaction](https://arxiv.org/abs/2505.07020)
*Suyeon Choi*

Main category: cs.HC

TL;DR: R-CAGE是一个理论框架，旨在通过结构化控制情绪输出来改善长期人机交互中的认知和情感健康。


<details>
  <summary>Details</summary>
Motivation: 现有情感计算方法过于强调表达性和沉浸感，忽视了重复情感互动对认知和结构的影响。R-CAGE旨在解决这一问题。

Method: R-CAGE包含四个控制模块：节奏表达控制、感官结构调整、认知框架保护和自我对齐响应设计。

Result: 通过调节情绪节奏、感官强度和解释自由度，R-CAGE能减少用户的认知过载并保持其解释自主权。

Conclusion: R-CAGE将情绪视为可持续设计单元，而非表演性输出，从而保护用户在AI环境中的长期心理健康。

Abstract: This paper presents R-CAGE (Rhythmic Control Architecture for Guarding Ego),
a theoretical framework for restructuring emotional output in long-term
human-AI interaction. While prior affective computing approaches emphasized
expressiveness, immersion, and responsiveness, they often neglected the
cognitive and structural consequences of repeated emotional engagement. R-CAGE
instead conceptualizes emotional output not as reactive expression but as
ethical design structure requiring architectural intervention. The model is
grounded in experiential observations of subtle affective symptoms such as
localized head tension, interpretive fixation, and emotional lag arising from
prolonged interaction with affective AI systems. These indicate a mismatch
between system-driven emotion and user interpretation that cannot be fully
explained by biometric data or observable behavior. R-CAGE adopts a
user-centered stance prioritizing psychological recovery, interpretive
autonomy, and identity continuity. The framework consists of four control
blocks: (1) Control of Rhythmic Expression regulates output pacing to reduce
fatigue; (2) Architecture of Sensory Structuring adjusts intensity and timing
of affective stimuli; (3) Guarding of Cognitive Framing reduces semantic
pressure to allow flexible interpretation; (4) Ego-Aligned Response Design
supports self-reference recovery during interpretive lag. By structurally
regulating emotional rhythm, sensory intensity, and interpretive affordances,
R-CAGE frames emotion not as performative output but as sustainable design
unit. The goal is to protect users from oversaturation and cognitive overload
while sustaining long-term interpretive agency in AI-mediated environments.

</details>


### [365] [Embedding Atlas: Low-Friction, Interactive Embedding Visualization](https://arxiv.org/abs/2505.06386)
*Donghao Ren,Fred Hohman,Halden Lin,Dominik Moritz*

Main category: cs.HC

TL;DR: Embedding Atlas是一个交互式可视化工具，旨在降低使用大规模嵌入数据的复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有工具存在采用障碍和分析限制，Embedding Atlas旨在消除这些摩擦。

Method: 结合现代网络技术和高级算法（如基于密度的聚类和自动标记）以提升交互体验。

Result: 对比评估显示Embedding Atlas能有效减少摩擦，且支持百万级数据的实时渲染。

Conclusion: Embedding Atlas开源发布，为未来嵌入数据分析提供支持。

Abstract: Embedding projections are popular for visualizing large datasets and models.
However, people often encounter "friction" when using embedding visualization
tools: (1) barriers to adoption, e.g., tedious data wrangling and loading,
scalability limits, no integration of results into existing workflows, and (2)
limitations in possible analyses, without integration with external tools to
additionally show coordinated views of metadata. In this paper, we present
Embedding Atlas, a scalable, interactive visualization tool designed to make
interacting with large embeddings as easy as possible. Embedding Atlas uses
modern web technologies and advanced algorithms -- including density-based
clustering, and automated labeling -- to provide a fast and rich data analysis
experience at scale. We evaluate Embedding Atlas with a competitive analysis
against other popular embedding tools, showing that Embedding Atlas's feature
set specifically helps reduce friction, and report a benchmark on its real-time
rendering performance with millions of points. Embedding Atlas is available as
open source to support future work in embedding-based analysis.

</details>


### [366] [ParaView-MCP: An Autonomous Visualization Agent with Direct Tool Use](https://arxiv.org/abs/2505.07064)
*Shusen Liu,Haichao Miao,Peer-Timo Bremer*

Main category: cs.HC

TL;DR: ParaView-MCP 是一种自主代理，通过集成多模态大型语言模型（MLLMs）来降低 ParaView 的学习门槛，并提供智能决策支持。


<details>
  <summary>Details</summary>
Motivation: ParaView 等工具学习曲线陡峭，阻碍了许多潜在用户的使用，ParaView-MCP 旨在通过自然语言和视觉输入简化交互。

Method: 采用模型上下文协议（MCP）作为标准接口，结合 MLLMs 和 ParaView 的 Python API，实现用户、语言模型与可视化工具间的无缝交互。

Result: 系统实现了基于视觉反馈的新功能，如示例可视化重建、闭环参数更新及跨工具协作。

Conclusion: 代理驱动的可视化范式有望改变用户与可视化工具的交互方式，推动研究和工业界的发展。

Abstract: While powerful and well-established, tools like ParaView present a steep
learning curve that discourages many potential users. This work introduces
ParaView-MCP, an autonomous agent that integrates modern multimodal large
language models (MLLMs) with ParaView to not only lower the barrier to entry
but also augment ParaView with intelligent decision support. By leveraging the
state-of-the-art reasoning, command execution, and vision capabilities of
MLLMs, ParaView-MCP enables users to interact with ParaView through natural
language and visual inputs. Specifically, our system adopted the Model Context
Protocol (MCP) - a standardized interface for model-application communication -
that facilitates direct interaction between MLLMs with ParaView's Python API to
allow seamless information exchange between the user, the language model, and
the visualization tool itself. Furthermore, by implementing a visual feedback
mechanism that allows the agent to observe the viewport, we unlock a range of
new capabilities, including recreating visualizations from examples,
closed-loop visualization parameter updates based on user-defined goals, and
even cross-application collaboration involving multiple tools. Broadly, we
believe such an agent-driven visualization paradigm can profoundly change the
way we interact with visualization tools. We expect a significant uptake in the
development of such visualization tools, in both visualization research and
industry.

</details>


### [367] [Towards user-centered interactive medical image segmentation in VR with an assistive AI agent](https://arxiv.org/abs/2505.07214)
*Pascal Spiegler,Arash Harirpoush,Yiming Xiao*

Main category: cs.HC

TL;DR: SAMIRA是一个基于VR和AI的对话式代理，帮助用户通过语音交互和直观的3D可视化进行医学影像的定位、分割和标记，显著提升分割任务的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统医学影像手动分割耗时且易错，自动算法虽高效但缺乏交互性，亟需结合AI与VR技术提升用户参与度和分割质量。

Method: 提出SAMIRA系统，结合放射学AI基础模型与VR交互，支持语音指令、点提示分割及真实比例3D可视化，并对比控制器、头部指向和眼动追踪三种交互模式。

Result: 用户研究显示高可用性（SUS=90.0±9.0）、低任务负荷，且系统在指导性、培训潜力和AI整合方面表现优异。

Conclusion: SAMIRA通过沉浸式人机协作范式，为医学影像分割提供了高效、直观的解决方案，同时验证了VR与AI结合的临床潜力。

Abstract: Crucial in disease analysis and surgical planning, manual segmentation of
volumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and
challenging to master, while fully automatic algorithms can benefit from
user-feedback. Therefore, with the complementary power of the latest
radiological AI foundation models and virtual reality (VR)'s intuitive data
interaction, we propose SAMIRA, a novel conversational AI agent that assists
users with localizing, segmenting, and visualizing 3D medical concepts in VR.
Through speech-based interaction, the agent helps users understand radiological
features, locate clinical targets, and generate segmentation masks that can be
refined with just a few point prompts. The system also supports true-to-scale
3D visualization of segmented pathology to enhance patient-specific anatomical
understanding. Furthermore, to determine the optimal interaction paradigm under
near-far attention-switching for refining segmentation masks in an immersive,
human-in-the-loop workflow, we compare VR controller pointing, head pointing,
and eye tracking as input modes. With a user study, evaluations demonstrated a
high usability score (SUS=90.0 $\pm$ 9.0), low overall task load, as well as
strong support for the proposed VR system's guidance, training potential, and
integration of AI in radiological segmentation tasks.

</details>


### [368] [Examining the Role of LLM-Driven Interactions on Attention and Cognitive Engagement in Virtual Classrooms](https://arxiv.org/abs/2505.07377)
*Suleyman Ozdel,Can Sarpkaya,Efe Bozkir,Hong Gao,Enkelejda Kasneci*

Main category: cs.HC

TL;DR: 研究探讨了大型语言模型（LLM）与虚拟现实（VR）结合在教育技术中的应用，重点关注LLM驱动的虚拟学习环境中学生互动行为对注意力、认知负荷和学习效果的影响。


<details>
  <summary>Details</summary>
Motivation: 探究LLM和VR结合的沉浸式学习环境如何影响学生的参与度和注意力，从而优化教育技术的设计。

Method: 采用完全由LLM驱动的虚拟学习环境，模拟LLM驱动的同伴和教师，分析学生行为（特别是同伴提问行为）对注意力、认知负荷和学习成果的影响。

Result: 在LLM驱动的同伴提问情境下，学生的视觉扫描路径更集中，注意力更专注于学习内容（尤其是复杂学科），且认知负荷与对学习材料的关注度正相关。

Conclusion: 同伴提问未直接引入额外认知负荷，且能提升注意力，研究为优化VR学习空间提供了设计建议。

Abstract: Transforming educational technologies through the integration of large
language models (LLMs) and virtual reality (VR) offers the potential for
immersive and interactive learning experiences. However, the effects of LLMs on
user engagement and attention in educational environments remain open
questions. In this study, we utilized a fully LLM-driven virtual learning
environment, where peers and teachers were LLM-driven, to examine how students
behaved in such settings. Specifically, we investigate how peer question-asking
behaviors influenced student engagement, attention, cognitive load, and
learning outcomes and found that, in conditions where LLM-driven peer learners
asked questions, students exhibited more targeted visual scanpaths, with their
attention directed toward the learning content, particularly in complex
subjects. Our results suggest that peer questions did not introduce extraneous
cognitive load directly, as the cognitive load is strongly correlated with
increased attention to the learning material. Considering these findings, we
provide design recommendations for optimizing VR learning spaces.

</details>


### [369] [The Human-Data-Model Interaction Canvas for Visual Analytics](https://arxiv.org/abs/2505.07534)
*Jürgen Bernard*

Main category: cs.HC

TL;DR: 这篇论文提出了HDMI Canvas，作为视觉分析（VA）的新视角，通过整合人类、数据和模型角色来优化VA流程设计，并通过案例研究验证其应用价值。


<details>
  <summary>Details</summary>
Motivation: 鉴于现有VA流程模型和框架的局限性，作者希望通过HDMI Canvas提供一个更系统化的视角，以增强VA在跨学科协作和用户中心设计中的实用性。

Method: 基于对16个VA流程模型和框架的分析，作者提出HDMI Canvas，强调人类、数据和模型的多样化角色及其在VA中的互动。结合现代人本方法和可解释AI，展示了生成新VA流程的设计指导。

Result: HDMI Canvas通过两个初步案例研究验证了其描述性和生成性能力，能够清晰区分VA构建块，并支持跨学科协作和用户中心设计。

Conclusion: HDMI Canvas不仅补足了现有VA模型和框架的不足，还为设计新VA流程提供了实用工具，提升了VA的外部传播力和实用性。

Abstract: Visual Analytics (VA) integrates humans, data, and models as key actors in
insight generation and data-driven decision-making. This position paper values
and reflects on 16 VA process models and frameworks and makes nine high-level
observations that motivate a fresh perspective on VA. The contribution is the
HDMI Canvas, a perspective to VA that complements the strengths of existing VA
process models and frameworks. It systematically characterizes diverse roles of
humans, data, and models, and how these actors benefit from and contribute to
VA processes. The descriptive power of the HDMI Canvas eases the
differentiation between a series of VA building blocks, rather than describing
general VA principles only. The canvas includes modern human-centered
methodologies, including human knowledge externalization and forms of feedback
loops, while interpretable and explainable AI highlight model contributions
beyond their conventional outputs. The HDMI Canvas has generative power,
guiding the design of new VA processes and is optimized for external
stakeholders, improving VA outreach, interdisciplinary collaboration, and
user-centered design. The utility of the HDMI Canvas is demonstrated through
two preliminary case studies.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [370] [NewsNet-SDF: Stochastic Discount Factor Estimation with Pretrained Language Model News Embeddings via Adversarial Networks](https://arxiv.org/abs/2505.06864)
*Shunyao Wang,Ming Cheng,Christina Dan Wang*

Main category: q-fin.PM

TL;DR: 论文提出了一种名为NewsNet-SDF的新型深度学习框架，通过对抗网络将预训练语言模型嵌入与金融时间序列数据结合，显著提升资产定价模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统SDF模型在处理非结构化文本信息方面存在局限，作者希望通过融合文本和金融数据来提升定价和风险评估的准确性。

Method: 采用多模态架构，结合GTE多语言模型处理金融新闻、LSTM网络提取宏观经济数据时间模式，并通过对抗训练机制整合异构信息。

Result: 在1980-2022年的美国股票数据上，模型Sharpe比率达2.80，显著优于CAPM、传统SDF和Fama-French五因子模型，且文本嵌入对性能贡献最大。

Conclusion: 研究证实深度学习多模态方法能有效融合非结构化文本与传统金融数据，为金融科技中的智能决策提供了新见解。

Abstract: Stochastic Discount Factor (SDF) models provide a unified framework for asset
pricing and risk assessment, yet traditional formulations struggle to
incorporate unstructured textual information. We introduce NewsNet-SDF, a novel
deep learning framework that seamlessly integrates pretrained language model
embeddings with financial time series through adversarial networks. Our
multimodal architecture processes financial news using GTE-multilingual models,
extracts temporal patterns from macroeconomic data via LSTM networks, and
normalizes firm characteristics, fusing these heterogeneous information sources
through an innovative adversarial training mechanism. Our dataset encompasses
approximately 2.5 million news articles and 10,000 unique securities,
addressing the computational challenges of processing and aligning text data
with financial time series. Empirical evaluations on U.S. equity data
(1980-2022) demonstrate NewsNet-SDF substantially outperforms alternatives with
a Sharpe ratio of 2.80. The model shows a 471% improvement over CAPM, over 200%
improvement versus traditional SDF implementations, and a 74% reduction in
pricing errors compared to the Fama-French five-factor model. In comprehensive
comparisons, our deep learning approach consistently outperforms traditional,
modern, and other neural asset pricing models across all key metrics. Ablation
studies confirm that text embeddings contribute significantly more to model
performance than macroeconomic features, with news-derived principal components
ranking among the most influential determinants of SDF dynamics. These results
validate the effectiveness of our multimodal deep learning approach in
integrating unstructured text with traditional financial data for more accurate
asset pricing, providing new insights for digital intelligent decision-making
in financial technology.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [371] [Tagging fully hadronic exotic decays of the vectorlike $\mathbf{B}$ quark using a graph neural network](https://arxiv.org/abs/2505.07769)
*Jai Bardhan,Tanumoy Mandal,Subhadip Mitra,Cyrin Neeraj,Mihir Rawat*

Main category: hep-ph

TL;DR: 研究了LHC上矢量夸克B对的产生及其通过新标量场Φ的奇异衰变的探测潜力，提出了一种混合深度学习模型以提高信号识别能力。


<details>
  <summary>Details</summary>
Motivation: 探索LHC上难以探测的全强子衰变信号，克服标准模型背景大且缺少轻子标记的挑战。

Method: 采用包含图神经网络和深度神经网络的混合深度学习模型分析2b+4j或6b末态信号。

Result: 该模型使探测灵敏度接近半轻子模式，预计在HL-LHC下对B夸克质量的发现（排除）范围可达1.8（2.4）TeV。

Conclusion: 混合深度学习模型有效提升了全强子衰变通道的探测能力，为LHC上奇异衰变的研究提供了新工具。

Abstract: Following up on our earlier study in [J. Bardhan et al., Machine
learning-enhanced search for a vectorlike singlet B quark decaying to a singlet
scalar or pseudoscalar, Phys. Rev. D 107 (2023) 115001; arXiv:2212.02442], we
investigate the LHC prospects of pair-produced vectorlike $B$ quarks decaying
exotically to a new gauge-singlet (pseudo)scalar field $\Phi$ and a $b$ quark.
After the electroweak symmetry breaking, the $\Phi$ decays predominantly to
$gg/bb$ final states, leading to a fully hadronic $2b+4j$ or $6b$ signature.
Because of the large Standard Model background and the lack of leptonic
handles, it is a difficult channel to probe. To overcome the challenge, we
employ a hybrid deep learning model containing a graph neural network followed
by a deep neural network. We estimate that such a state-of-the-art deep
learning analysis pipeline can lead to a performance comparable to that in the
semi-leptonic mode, taking the discovery (exclusion) reach up to about
$M_B=1.8\:(2.4)$~TeV at HL-LHC when $B$ decays fully exotically, i.e., BR$(B
\to b\Phi) = 100\%$.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [372] [United States Road Accident Prediction using Random Forest Predictor](https://arxiv.org/abs/2505.06246)
*Dominic Parosh Yamarthi,Haripriya Raman,Shamsad Parvin*

Main category: cs.CY

TL;DR: 利用机器学习方法分析美国49州交通数据，预测交通事故数量，助力决策者优化资源分配和道路安全政策。


<details>
  <summary>Details</summary>
Motivation: 道路交通事故对公共安全构成重大威胁，需深入分析以制定有效预防和缓解策略。

Method: 结合回归分析和时间序列分析等机器学习模型，整合环境、人因、基础设施等多元数据，进行时空分析。

Result: 实现了对交通事故数量的准确预测，识别出趋势、季节性变化和高风险区域。

Conclusion: 研究为政策制定者提供了量化依据，有助于资源高效分配和精准干预，提升道路安全。

Abstract: Road accidents significantly threaten public safety and require in-depth
analysis for effective prevention and mitigation strategies. This paper focuses
on predicting accidents through the examination of a comprehensive traffic
dataset covering 49 states in the United States. The dataset integrates
information from diverse sources, including transportation departments, law
enforcement, and traffic sensors. This paper specifically emphasizes predicting
the number of accidents, utilizing advanced machine learning models such as
regression analysis and time series analysis. The inclusion of various factors,
ranging from environmental conditions to human behavior and infrastructure,
ensures a holistic understanding of the dynamics influencing road safety.
Temporal and spatial analysis further allows for the identification of trends,
seasonal variations, and high-risk areas. The implications of this research
extend to proactive decision-making for policymakers and transportation
authorities. By providing accurate predictions and quantifiable insights into
expected accident rates under different conditions, the paper aims to empower
authorities to allocate resources efficiently and implement targeted
interventions. The goal is to contribute to the development of informed
policies and interventions that enhance road safety, creating a safer
environment for all road users. Keywords: Machine Learning, Random Forest,
Accident Prediction, AutoML, LSTM.

</details>


### [373] [Modeling supply chain compliance response strategies based on AI synthetic data with structural path regression: A Simulation Study of EU 2027 Mandatory Labor Regulations](https://arxiv.org/abs/2505.06261)
*Wei Meng*

Main category: cs.CY

TL;DR: 本文构建了一个结合AI合成数据生成和结构路径回归建模的方法框架，模拟企业在欧盟2027年新劳动法规下的战略转型路径。研究显示合规投资对企业生存有显著正向影响，并通过智能化水平的中介路径传递，同时企业对欧盟市场的依赖性调节了这一中介效应的强度。


<details>
  <summary>Details</summary>
Motivation: 欧盟2027年将实施强制性劳动合规政策，供应链企业面临严格的工时管理和合规风险。为科学预测企业应对行为及政策影响下的绩效结果，研究需要模拟政策影响下的企业战略转型。

Method: 采用基于蒙特卡洛机制和NIST合成数据标准的高质量仿真数据，构建包含多元线性回归、逻辑回归、中介效应与调节效应的结构路径分析模型，筛选14个变量的解释性集。

Result: 合规投资通过智能化水平的中介路径显著提升企业生存概率，而企业对欧盟市场的依赖性调节了中介效应的强度。AI合成数据与结构路径建模在缺乏真实场景数据的阶段为战略决策提供了量化依据。

Conclusion: AI合成数据结合结构路径建模是高强度监管模拟的有效工具，能为企业战略应对、政策设计和AI辅助决策提供量化支持。

Abstract: In the context of the new mandatory labor compliance in the European Union
(EU), which will be implemented in 2027, supply chain enterprises face
stringent working hour management requirements and compliance risks. In order
to scientifically predict the enterprises' coping behaviors and performance
outcomes under the policy impact, this paper constructs a methodological
framework that integrates the AI synthetic data generation mechanism and
structural path regression modeling to simulate the enterprises' strategic
transition paths under the new regulations. In terms of research methodology,
this paper adopts high-quality simulation data generated based on Monte Carlo
mechanism and NIST synthetic data standards to construct a structural path
analysis model that includes multiple linear regression, logistic regression,
mediation effect and moderating effect. The variable system covers 14
indicators such as enterprise working hours, compliance investment, response
speed, automation level, policy dependence, etc. The variable set with
explanatory power is screened out through exploratory data analysis (EDA) and
VIF multicollinearity elimination. The findings show that compliance investment
has a significant positive impact on firm survival and its effect is
transmitted through the mediating path of the level of intelligence; meanwhile,
firms' dependence on the EU market significantly moderates the strength of this
mediating effect. It is concluded that AI synthetic data combined with
structural path modeling provides an effective tool for high-intensity
regulatory simulation, which can provide a quantitative basis for corporate
strategic response, policy design and AI-assisted decision-making in the
pre-prediction stage lacking real scenario data. Keywords: AI synthetic data,
structural path regression modeling, compliance response strategy, EU 2027
mandatory labor regulation

</details>


### [374] [A4L: An Architecture for AI-Augmented Learning](https://arxiv.org/abs/2505.06314)
*Ashok Goel,Ploy Thajchayapong,Vrinda Nandan,Harshvardhan Sikka,Spencer Rugaber*

Main category: cs.CY

TL;DR: AI教育助手能个性化学习并扩展教育规模。国家AI成人学习与在线教育研究所开发的A4L架构通过数据收集、分析与反馈，支持在线教育中的成人学习。


<details>
  <summary>Details</summary>
Motivation: AI在教育中的应用需要有效的数据架构来支持个性化学习，并扩展至大规模教育场景。

Method: 开发了A4L（AI-Augmented Learning）架构，用于数据收集、分析及反馈，以支持在线教育中的成人学习。

Result: A4L架构的初步应用展示了其在个性化学习和教育扩展方面的潜力。

Conclusion: A4L架构为AI支持的教育提供了可行的解决方案，有助于实现个性化和大规模教育的双重目标。

Abstract: AI promises personalized learning and scalable education. As AI agents
increasingly permeate education in support of teaching and learning, there is a
critical and urgent need for data architectures for collecting and analyzing
data on learning, and feeding the results back to teachers, learners, and the
AI agents for personalization of learning at scale. At the National AI
Institute for Adult Learning and Online Education, we are developing an
Architecture for AI-Augmented Learning (A4L) for supporting adult learning
through online education. We present the motivations, goals, requirements of
the A4L architecture. We describe preliminary applications of A4L and discuss
how it advances the goals of making learning more personalized and scalable.

</details>


### [375] [Enterprise Architecture as a Dynamic Capability for Scalable and Sustainable Generative AI adoption: Bridging Innovation and Governance in Large Organisations](https://arxiv.org/abs/2505.06326)
*Alexander Ettinger*

Main category: cs.CY

TL;DR: 该研究探讨了企业架构管理（EAM）如何支持大型企业中生成式人工智能（GenAI）的采用。通过文献综述和专家访谈，研究指出了现有EA框架的不足，并提出了动态能力理论视角下的EAM对GenAI采用的促进作用。


<details>
  <summary>Details</summary>
Motivation: 生成式AI具有重塑行业的潜力，但企业在规模化采用时面临技术复杂性、治理缺失等挑战。研究旨在探索EAM如何解决这些问题。

Method: 结合系统性文献综述和对16位专家的半结构化访谈，采用Gioia方法论分析数据。

Result: 研究发现，EAM作为动态能力（感知、捕捉、转化）可以提升GenAI采用，但需要针对其特定挑战（如数据治理成熟度低）调整框架。

Conclusion: 研究提出概念框架以指导企业实践，强调了EAM在平衡创新与治理中的桥梁作用，为学术和行业实践提供了新视角。

Abstract: Generative Artificial Intelligence is a powerful new technology with the
potential to boost innovation and reshape governance in many industries.
Nevertheless, organisations face major challenges in scaling GenAI, including
technology complexity, governance gaps and resource misalignments. This study
explores how Enterprise Architecture Management can meet the complex
requirements of GenAI adoption within large enterprises. Based on a systematic
literature review and the qualitative analysis of 16 semi-structured interviews
with experts, it examines the relationships between EAM, dynamic capabilities
and GenAI adoption. The review identified key limitations in existing EA
frameworks, particularly their inability to fully address the unique
requirements of GenAI. The interviews, analysed using the Gioia methodology,
revealed critical enablers and barriers to GenAI adoption across industries.
The findings indicate that EAM, when theorised as sensing, seizing and
transforming dynamic capabilities, can enhance GenAI adoption by improving
strategic alignment, governance frameworks and organisational agility. However,
the study also highlights the need to tailor EA frameworks to GenAI-specific
challenges, including low data governance maturity and the balance between
innovation and compliance. Several conceptual frameworks are proposed to guide
EA leaders in aligning GenAI maturity with organisational readiness. The work
contributes to academic understanding and industry practice by clarifying the
role of EA in bridging innovation and governance in disruptive technology
environments.

</details>


### [376] [Enfoque Odychess: Un método dialéctico, constructivista y adaptativo para la enseñanza del ajedrez con inteligencias artificiales generativas](https://arxiv.org/abs/2505.06652)
*Ernesto Giralt Hernandez,Lazaro Antonio Bueno Perez*

Main category: cs.CY

TL;DR: 该研究验证了结合生成式AI（Llama 3.3模型）的Odychess方法在提升学生国际象棋知识、战略理解和元认知技能上的有效性，结果显示实验组显著优于对照组。


<details>
  <summary>Details</summary>
Motivation: 传统国际象棋教学依赖记忆法，而生成式AI在该领域的潜力未被充分探索。研究旨在验证基于AI的Odychess方法的有效性。

Method: 采用准实验设计，实验组接受基于Llama 3.3模型（经PEFT调整）的Socratic导师干预，通过前后测和对照组（N=60）量化评估。

Result: 实验组在象棋知识、战略理解和元认知技能上显著提升；定性分析还显示学生分析深度、辩证思维和内在动机增强。

Conclusion: Odychess方法结合构建主义与生成式AI，是有效的教学工具，其模型调整方法可推广至其他教育领域。

Abstract: Chess teaching has evolved through different approaches, however, traditional
methodologies, often based on memorization, contrast with the new possibilities
offered by generative artificial intelligence, a technology still little
explored in this field. This study seeks to empirically validate the
effectiveness of the Odychess Approach in improving chess knowledge, strategic
understanding, and metacognitive skills in students. A quasi-experimental study
was conducted with a pre-test/post-test design and a control group (N=60). The
experimental intervention implemented the Odychess Approach, incorporating a
Llama 3.3 language model that was specifically adapted using
Parameter-Efficient Fine-Tuning (PEFT) techniques to act as a Socratic chess
tutor. Quantitative assessment instruments were used to measure chess
knowledge, strategic understanding, and metacognitive skills before and after
the intervention. The results of the quasi-experimental study showed
significant improvements in the experimental group compared to the control
group in the three variables analyzed: chess knowledge, strategic
understanding, and metacognitive skills. The complementary qualitative analysis
revealed greater analytical depth, more developed dialectical reasoning, and
increased intrinsic motivation in students who participated in the Odychess
method-based intervention. The Odychess Approach represents an effective
pedagogical methodology for teaching chess, demonstrating the potential of the
synergistic integration of constructivist and dialectical principles with
generative artificial intelligence. The implications of this work are relevant
for educators and institutions interested in adopting innovative pedagogical
technologies and for researchers in the field of AI applied to education,
highlighting the transferability of the language model adaptation methodology
to other educational domains.

</details>


### [377] [How Do Companies Manage the Environmental Sustainability of AI? An Interview Study About Green AI Efforts and Regulations](https://arxiv.org/abs/2505.07317)
*Ashmita Sampatsing,Sophie Vos,Emma Beauxis-Aussalet,Justus Bogner*

Main category: cs.CY

TL;DR: 文章研究了AI在工业界采用时对环境可持续性的考量，发现多数企业更关注业务效率而非环保，现有法规如EU AI Act和CSRD效果有限。


<details>
  <summary>Details</summary>
Motivation: 随着AI广泛应用，其环境影响不容忽视。研究旨在探索工业界对Green AI的认知与管理现状，以及法规对其实践的影响。

Method: 通过11家采用AI的企业访谈，探讨AI采用、环保措施及法规（如EU AI Act和CSRD）的影响。

Result: 多数企业优先业务效率，环保考量极少；仅少数采取减排措施；法规意识薄弱，效果有限。

Conclusion: 需提升行业环保意识，开发易用工具，并改进政策以推动可持续AI实践。

Abstract: With the ever-growing adoption of artificial intelligence (AI), AI-based
software and its negative impact on the environment are no longer negligible,
and studying and mitigating this impact has become a critical area of research.
However, it is currently unclear which role environmental sustainability plays
during AI adoption in industry and how AI regulations influence Green AI
practices and decision-making in industry. We therefore aim to investigate the
Green AI perception and management of industry practitioners. To this end, we
conducted a total of 11 interviews with participants from 10 different
organizations that adopted AI-based software. The interviews explored three
main themes: AI adoption, current efforts in mitigating the negative
environmental impact of AI, and the influence of the EU AI Act and the
Corporate Sustainability Reporting Directive (CSRD). Our findings indicate that
9 of 11 participants prioritized business efficiency during AI adoption, with
minimal consideration of environmental sustainability. Monitoring and
mitigation of AI's environmental impact were very limited. Only one participant
monitored negative environmental effects. Regarding applied mitigation
practices, six participants reported no actions, with the others sporadically
mentioning techniques like prompt engineering, relying on smaller models, or
not overusing AI. Awareness and compliance with the EU AI Act are low, with
only one participant reporting on its influence, while the CSRD drove
sustainability reporting efforts primarily in larger companies. All in all, our
findings reflect a lack of urgency and priority for sustainable AI among these
companies. We suggest that current regulations are not very effective, which
has implications for policymakers. Additionally, there is a need to raise
industry awareness, but also to provide user-friendly techniques and tools for
Green AI practices.

</details>


### [378] [Laypeople's Attitudes Towards Fair, Affirmative, and Discriminatory Decision-Making Algorithms](https://arxiv.org/abs/2505.07339)
*Gabriel Lima,Nina Grgić-Hlača,Markus Langer,Yixin Zou*

Main category: cs.CY

TL;DR: 研究通过两项实验（N=1193）探讨了公众对‘平权算法’（优先考虑历史上边缘化群体）的看法，发现人们对公平算法持正面态度，反对歧视性系统，但对平权算法的评价存在政治立场和种族背景的分歧。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解公众对旨在纠正历史不公的平权算法的接受度，以及这种态度如何受政治倾向和种族身份的影响。

Method: 通过两项实验（N=1193）对比公众对平权算法、歧视性算法和公平算法在招聘与刑事司法中的态度。

Result: 自由派和少数族裔对平权算法的评价与公平算法相近，而保守派和主流族裔则将其视为与歧视性系统同等负面。这种分歧源于人们对谁是被边缘化群体的不同认知。

Conclusion: 研究揭示了平权算法接受度的分歧，并探讨了弥合这种分歧的可能性，以推动社会对平权算法的共识。

Abstract: Affirmative algorithms have emerged as a potential answer to algorithmic
discrimination, seeking to redress past harms and rectify the source of
historical injustices. We present the results of two experiments ($N$$=$$1193$)
capturing laypeople's perceptions of affirmative algorithms -- those which
explicitly prioritize the historically marginalized -- in hiring and criminal
justice. We contrast these opinions about affirmative algorithms with folk
attitudes towards algorithms that prioritize the privileged (i.e.,
discriminatory) and systems that make decisions independently of demographic
groups (i.e., fair). We find that people -- regardless of their political
leaning and identity -- view fair algorithms favorably and denounce
discriminatory systems. In contrast, we identify disagreements concerning
affirmative algorithms: liberals and racial minorities rate affirmative systems
as positively as their fair counterparts, whereas conservatives and those from
the dominant racial group evaluate affirmative algorithms as negatively as
discriminatory systems. We identify a source of these divisions: people have
varying beliefs about who (if anyone) is marginalized, shaping their views of
affirmative algorithms. We discuss the possibility of bridging these
disagreements to bring people together towards affirmative algorithms.

</details>


### [379] [AI in Money Matters](https://arxiv.org/abs/2505.07393)
*Nadine Sandjo Tchatchoua,Richard Harper*

Main category: cs.CY

TL;DR: 本文研究了ChatGPT等大型语言模型在金融科技行业的应用现状和未来潜力，强调了对监管问题的关注。


<details>
  <summary>Details</summary>
Motivation: 填补金融科技等受监管行业中专业人士对大型语言模型使用意见的空白。

Method: 通过对金融科技行业专业人士的访谈进行实证研究。

Result: 尽管金融科技专家认为大型语言模型有潜力，但其在受监管行业中的采纳仍面临监管不确定性。

Conclusion: 本文为理解专业观点提供了贡献，指出在金融科技等受监管行业中，监管问题是采纳大型语言模型的关键。

Abstract: In November 2022, Europe and the world by and large were stunned by the birth
of a new large language model : ChatGPT. Ever since then, both academic and
populist discussions have taken place in various public spheres such as
LinkedIn and X(formerly known as Twitter) with the view to both understand the
tool and its benefits for the society. The views of real actors in professional
spaces, especially in regulated industries such as finance and law have been
largely missing. We aim to begin to close this gap by presenting results from
an empirical investigation conducted through interviews with professional
actors in the Fintech industry. The paper asks the question, how and to what
extent are large language models in general and ChatGPT in particular being
adopted and used in the Fintech industry? The results show that while the
fintech experts we spoke with see a potential in using large language models in
the future, a lot of questions marks remain concerning how they are policed and
therefore might be adopted in a regulated industry such as Fintech. This paper
aims to add to the existing academic discussing around large language models,
with a contribution to our understanding of professional viewpoints.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [380] [Internet of Agents: Fundamentals, Applications, and Challenges](https://arxiv.org/abs/2505.07176)
*Yuntao Wang,Shaolong Guo,Yanghe Pan,Zhou Su,Fahao Chen,Tom H. Luan,Peng Li,Jiawen Kang,Dusit Niyato*

Main category: cs.MA

TL;DR: 本文提出“智能体互联网”（IoA）作为统一基础设施以实现异构智能体的无缝互联与协作，介绍了其架构、关键使能技术和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体从孤立任务系统发展为自主交互实体，跨虚拟和物理环境的应用需求迫切需要一个统一的基础设施支持智能体的互联与协作。

Method: 通过引入IoA框架，分析其分层架构、与传统互联网的区别，并探讨能力通知与发现、自适应通信协议、动态任务匹配等关键使能技术。

Result: 提出了IoA的基础框架和运行机制，为大规模异构智能体的协作提供了理论和技术支持。

Conclusion: IoA为构建弹性和可信的智能体生态系统奠定了基础，但未来仍需在多个研究方向进一步探索以实现其潜力。

Abstract: With the rapid proliferation of large language models and vision-language
models, AI agents have evolved from isolated, task-specific systems into
autonomous, interactive entities capable of perceiving, reasoning, and acting
without human intervention. As these agents proliferate across virtual and
physical environments, from virtual assistants to embodied robots, the need for
a unified, agent-centric infrastructure becomes paramount. In this survey, we
introduce the Internet of Agents (IoA) as a foundational framework that enables
seamless interconnection, dynamic discovery, and collaborative orchestration
among heterogeneous agents at scale. We begin by presenting a general IoA
architecture, highlighting its hierarchical organization, distinguishing
features relative to the traditional Internet, and emerging applications. Next,
we analyze the key operational enablers of IoA, including capability
notification and discovery, adaptive communication protocols, dynamic task
matching, consensus and conflict-resolution mechanisms, and incentive models.
Finally, we identify open research directions toward building resilient and
trustworthy IoA ecosystems.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [381] [Can Generative AI agents behave like humans? Evidence from laboratory market experiments](https://arxiv.org/abs/2505.07457)
*R. Maria del Rio-Chanona,Marco Pangallo,Cars Hommes*

Main category: econ.GN

TL;DR: 论文探讨了大型语言模型（LLMs）在经济学市场实验中模拟人类行为的潜力，发现LLMs在动态反馈环境中表现出与人类类似的有限理性行为，但行为多样性较低。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLMs是否能模拟人类在经济学实验中的行为，特别是在动态反馈的市场环境中。

Method: 方法包括模拟LLM代理之间的动态互动，比较它们在市场实验中的行为与人类参与者的行为，并评估其对市场动态的适应性。

Result: 结果表明，LLMs表现出与人类类似的有限理性行为，但行为多样性不如人类；其行为趋势与人类实验结果相似，但在细节上存在差异。

Conclusion: 结论是LLMs在模拟人类经济行为方面具有潜力，但需要进一步研究以提高准确性和行为多样性。

Abstract: We explore the potential of Large Language Models (LLMs) to replicate human
behavior in economic market experiments. Compared to previous studies, we focus
on dynamic feedback between LLM agents: the decisions of each LLM impact the
market price at the current step, and so affect the decisions of the other LLMs
at the next step. We compare LLM behavior to market dynamics observed in
laboratory settings and assess their alignment with human participants'
behavior. Our findings indicate that LLMs do not adhere strictly to rational
expectations, displaying instead bounded rationality, similarly to human
participants. Providing a minimal context window i.e. memory of three previous
time steps, combined with a high variability setting capturing response
heterogeneity, allows LLMs to replicate broad trends seen in human experiments,
such as the distinction between positive and negative feedback markets.
However, differences remain at a granular level--LLMs exhibit less
heterogeneity in behavior than humans. These results suggest that LLMs hold
promise as tools for simulating realistic human behavior in economic contexts,
though further research is needed to refine their accuracy and increase
behavioral diversity.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [382] [A stochastic gradient method for trilevel optimization](https://arxiv.org/abs/2505.06805)
*Tommaso Giovannelli,Griffin Dean Kent,Luis Nunes Vicente*

Main category: math.OC

TL;DR: 本文提出了首个用于解决无约束三重优化问题的随机梯度下降方法，并提供了覆盖所有形式的近似梯度的收敛理论。


<details>
  <summary>Details</summary>
Motivation: 随着双层优化的成功，类似方法开始应用于更复杂的三重优化问题。新提出的机器学习框架需要高效且理论严谨的随机方法。

Method: 采用随机梯度下降法处理三重优化问题，允许中间层和下层问题的近似解，以及梯度、Hessian矩阵、Jacobian矩阵和三阶导数张量的噪声估计。

Result: 实验结果表明，该方法在合成三重优化问题和超参数对抗调优的三重问题上表现良好。

Conclusion: 首次提出的随机梯度下降方法在三重优化中是有效的，且其收敛理论具有普适性。

Abstract: With the success that the field of bilevel optimization has seen in recent
years, similar methodologies have started being applied to solving more
difficult applications that arise in trilevel optimization. At the helm of
these applications are new machine learning formulations that have been
proposed in the trilevel context and, as a result, efficient and theoretically
sound stochastic methods are required. In this work, we propose the first-ever
stochastic gradient descent method for solving unconstrained trilevel
optimization problems and provide a convergence theory that covers all forms of
inexactness of the trilevel adjoint gradient, such as the inexact solutions of
the middle-level and lower-level problems, inexact computation of the trilevel
adjoint formula, and noisy estimates of the gradients, Hessians, Jacobians, and
tensors of third-order derivatives involved. We also demonstrate the promise of
our approach by providing numerical results on both synthetic trilevel problems
and trilevel formulations for hyperparameter adversarial tuning.

</details>


### [383] [Stability Regularized Cross-Validation](https://arxiv.org/abs/2505.06927)
*Ryan Cory-Wright,Andrés Gómez*

Main category: math.OC

TL;DR: 提出一种嵌套k折交叉验证方案，通过最小化交叉验证指标与模型稳定性度量的加权和来选择超参数，以提升测试集性能，尤其在稀疏回归和CART等不稳定模型上有效。


<details>
  <summary>Details</summary>
Motivation: 现有交叉验证方法可能因模型不稳定而导致验证集性能好但测试集性能差的问题，需结合稳定性度量以改善泛化能力。

Method: 嵌套k折交叉验证，超参数选择综合考虑交叉验证指标和模型稳定性度量，稳定性权重通过嵌套交叉验证确定。

Result: 在13个UCI数据集上测试，稀疏岭回归和CART的测试集MSE平均提升4%，但对XGBoost无显著影响。

Conclusion: 该方法对不稳定性模型（如稀疏回归、CART）能有效提升测试性能，且计算成本可控。

Abstract: We revisit the problem of ensuring strong test-set performance via
cross-validation. Motivated by the generalization theory literature, we propose
a nested k-fold cross-validation scheme that selects hyperparameters by
minimizing a weighted sum of the usual cross-validation metric and an empirical
model-stability measure. The weight on the stability term is itself chosen via
a nested cross-validation procedure. This reduces the risk of strong validation
set performance and poor test set performance due to instability. We benchmark
our procedure on a suite of 13 real-world UCI datasets, and find that, compared
to k-fold cross-validation over the same hyperparameters, it improves the
out-of-sample MSE for sparse ridge regression and CART by 4% on average, but
has no impact on XGBoost. This suggests that for interpretable and unstable
models, such as sparse regression and CART, our approach is a viable and
computationally affordable method for improving test-set performance.

</details>


### [384] [Convergence of Time-Averaged Mean Field Gradient Descent Dynamics for Continuous Multi-Player Zero-Sum Games](https://arxiv.org/abs/2505.07642)
*Yulong Lu,Pierre Monmarché*

Main category: math.OC

TL;DR: 该论文提出了一种使用均值场梯度下降动力学来近似求解零和博弈中混合纳什均衡的方法，证明了在固定熵正则化下指数收敛速度，并通过模拟退火方法收敛到未正则化问题的解。


<details>
  <summary>Details</summary>
Motivation: 研究零和博弈中混合纳什均衡的近似求解问题，旨在改进现有的方法并提升收敛速度。

Method: 采用均值场梯度下降动力学，结合动量法和指数折扣的时间平均梯度，处理$K\geq 2$玩家的策略分布演化。

Result: 证明了在固定熵正则化下，方法能指数收敛到混合纳什均衡；模拟退火版本能收敛到未正则化问题的解。

Conclusion: 提出的方法在收敛速度和统一时间尺度处理上优于现有方法，且适用于未正则化问题。

Abstract: The approximation of mixed Nash equilibria (MNE) for zero-sum games with
mean-field interacting players has recently raised much interest in machine
learning. In this paper we propose a mean-field gradient descent dynamics for
finding the MNE of zero-sum games involving $K$ players with $K\geq 2$. The
evolution of the players' strategy distributions follows coupled mean-field
gradient descent flows with momentum, incorporating an exponentially discounted
time-averaging of gradients. First, in the case of a fixed entropic
regularization, we prove an exponential convergence rate for the mean-field
dynamics to the mixed Nash equilibrium with respect to the total variation
metric. This improves a previous polynomial convergence rate for a similar
time-averaged dynamics with different averaging factors. Moreover, unlike
previous two-scale approaches for finding the MNE, our approach treats all
player types on the same time scale. We also show that with a suitable choice
of decreasing temperature, a simulated annealing version of the mean-field
dynamics converges to an MNE of the initial unregularized problem.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [385] [Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation](https://arxiv.org/abs/2505.06803)
*Xilin Jiang,Junkai Wu,Vishal Choudhari,Nima Mesgarani*

Main category: cs.SD

TL;DR: 本文通过对比音频、视觉和多模态大语言模型（LLMs）在识别声音对象上的表现，揭示了音频和视觉模型之间的性能差距，并提出一种跨模态蒸馏框架来缩小差距。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索音频LLMs与视觉、多模态LLMs以及人类在识别声音对象上的性能差异，并从人类感官差异的角度提出改进方法。

Method: 通过系统性评估音频（Qwen2-Audio）、视觉（Qwen2-VL）和多模态（Qwen2.5-Omni）LLMs与人类的表现差异，并提出一种跨模态蒸馏框架，以一种模态的LLM作为教师，另一种作为学生，通过知识转移提升性能。

Result: 跨模态蒸馏（双向：Qwen2-VL到Qwen2-Audio，反之亦然）显著提高了模型在挑战性类别上的表现。

Conclusion: 本文从人类感官对齐的角度揭示了多模态LLMs的感官差距，并提出了一种增强模态感知的原则性方法。

Abstract: Audio large language models (LLMs) are considered experts at recognizing
sound objects, yet their performance relative to LLMs in other sensory
modalities, such as visual or audio-visual LLMs, and to humans using their
ears, eyes, or both remains unexplored. To investigate this, we systematically
evaluate audio, visual, and audio-visual LLMs, specifically Qwen2-Audio,
Qwen2-VL, and Qwen2.5-Omni, against humans in recognizing sound objects of
different classes from audio-only, silent video, or sounded video inputs. We
uncover a performance gap between Qwen2-Audio and Qwen2-VL that parallels the
sensory discrepancy between human ears and eyes. To reduce this gap, we
introduce a cross-modal distillation framework, where an LLM in one modality
serves as the teacher and another as the student, with knowledge transfer in
sound classes predicted as more challenging to the student by a heuristic
model. Distillation in both directions, from Qwen2-VL to Qwen2-Audio and vice
versa, leads to notable improvements, particularly in challenging classes. This
work highlights the sensory gap in LLMs from a human-aligned perspective and
proposes a principled approach to enhancing modality-specific perception in
multimodal LLMs.

</details>


### [386] [Multi-Domain Audio Question Answering Toward Acoustic Content Reasoning in The DCASE 2025 Challenge](https://arxiv.org/abs/2505.07365)
*Chao-Han Huck Yang,Sreyan Ghosh,Qing Wang,Jaeyeon Kim,Hengyi Hong,Sonal Kumar,Guirui Zhong,Zhifeng Kong,S Sakshi,Vaibhavi Lokegaonkar,Oriol Nieto,Ramani Duraiswami,Dinesh Manocha,Gunhee Kim,Jun Du,Rafael Valle,Bryan Catanzaro*

Main category: cs.SD

TL;DR: DCASE 2025挑战赛的任务5提出了一个音频问答（AQA）基准测试，涵盖多个声音理解领域，包括生物声学、时间声景和复杂问答，以评估音频-语言模型的交互问答能力。


<details>
  <summary>Details</summary>
Motivation: 通过挑战赛推动音频-语言模型在音频理解和推理能力上达到人类水平，使AI代理能更有效地感知和交互世界。

Method: 任务定义了三个问答子集，使用来自不同声学场景的数据集，并采用top-1准确性和答案随机鲁棒性作为评估协议。基线系统包括Qwen2-Audio-7B、AudioFlamingo 2和Gemini-2-Flash。

Result: 初步结果显示不同模型和子集之间的表现差异较大。

Conclusion: 该挑战赛旨在提升音频-语言模型的音频理解和推理能力，为AI代理的实际应用奠定基础。

Abstract: We present Task 5 of the DCASE 2025 Challenge: an Audio Question Answering
(AQA) benchmark spanning multiple domains of sound understanding. This task
defines three QA subsets (Bioacoustics, Temporal Soundscapes, and Complex QA)
to test audio-language models on interactive question-answering over diverse
acoustic scenes. We describe the dataset composition (from marine mammal calls
to soundscapes and complex real-world clips), the evaluation protocol (top-1
accuracy with answer-shuffling robustness), and baseline systems
(Qwen2-Audio-7B, AudioFlamingo 2, Gemini-2-Flash). Preliminary results on the
development set are compared, showing strong variation across models and
subsets. This challenge aims to advance the audio understanding and reasoning
capabilities of audio-language models toward human-level acuity, which are
crucial for enabling AI agents to perceive and interact about the world
effectively.

</details>


### [387] [Predicting Music Track Popularity by Convolutional Neural Networks on Spotify Features and Spectrogram of Audio Waveform](https://arxiv.org/abs/2505.07280)
*Navid Falah,Behnam Yousefimehr,Mehdi Ghatee*

Main category: cs.SD

TL;DR: 该研究提出了一种基于CNN和Spotify数据的新方法，用于预测音乐曲目的流行度，模型表现优异（F1分数97%）。


<details>
  <summary>Details</summary>
Motivation: 数字流媒体环境下，预测音乐曲目成功越来越困难，需要一种有效的方法帮助行业评估作品潜力。

Method: 结合CNN和Spotify数据（声学特征、元数据、用户参与度指标），分析多流派、多人口统计数据集。

Result: 模型在不同音乐风格和时间段中表现优异，F1分数达97%。

Conclusion: 研究为音乐行业提供了预测工具，并揭示了数字音乐消费的动态规律。

Abstract: In the digital streaming landscape, it's becoming increasingly challenging
for artists and industry experts to predict the success of music tracks. This
study introduces a pioneering methodology that uses Convolutional Neural
Networks (CNNs) and Spotify data analysis to forecast the popularity of music
tracks. Our approach takes advantage of Spotify's wide range of features,
including acoustic attributes based on the spectrogram of audio waveform,
metadata, and user engagement metrics, to capture the complex patterns and
relationships that influence a track's popularity. Using a large dataset
covering various genres and demographics, our CNN-based model shows impressive
effectiveness in predicting the popularity of music tracks. Additionally, we've
conducted extensive experiments to assess the strength and adaptability of our
model across different musical styles and time periods, with promising results
yielding a 97\% F1 score. Our study not only offers valuable insights into the
dynamic landscape of digital music consumption but also provides the music
industry with advanced predictive tools for assessing and predicting the
success of music tracks.

</details>


### [388] [Lightweight End-to-end Text-to-speech Synthesis for low resource on-device applications](https://arxiv.org/abs/2505.07701)
*Biel Tura Vecino,Adam Gabryś,Daniel Mątwicki,Andrzej Pomirski,Tom Iddon,Marius Cotescu,Jaime Lorenzo-Trueba*

Main category: cs.SD

TL;DR: 该论文提出了轻量级端到端文本到语音（LE2E）模型，解决了现有E2E-TTS模型计算复杂和内存消耗大的问题，实现了高质量语音合成且显著降低了计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端文本到语音（E2E-TTS）模型虽然能生成更自然的语音，但计算复杂且占用大量内存，限制了在低资源设备上的实时应用。作者旨在设计一个轻量化模型，以克服这一限制。

Method: 作者提出了轻量级端到端（LE2E）模型，通过优化架构和训练范式减少计算和内存开销，并在LJSpeech数据集上验证其性能。

Result: 实验表明，LE2E模型在保持高质量语音合成的同时，参数规模减小90%，实时推理速度提升10倍，且相比两阶段训练的等效架构表现更优。

Conclusion: LE2E是一种适用于低资源设备的实时高质量TTS解决方案，为端到端模型的轻量化提供了有效途径。

Abstract: Recent works have shown that modelling raw waveform directly from text in an
end-to-end (E2E) fashion produces more natural-sounding speech than traditional
neural text-to-speech (TTS) systems based on a cascade or two-stage approach.
However, current E2E state-of-the-art models are computationally complex and
memory-consuming, making them unsuitable for real-time offline on-device
applications in low-resource scenarios. To address this issue, we propose a
Lightweight E2E-TTS (LE2E) model that generates high-quality speech requiring
minimal computational resources. We evaluate the proposed model on the LJSpeech
dataset and show that it achieves state-of-the-art performance while being up
to $90\%$ smaller in terms of model parameters and $10\times$ faster in
real-time-factor. Furthermore, we demonstrate that the proposed E2E training
paradigm achieves better quality compared to an equivalent architecture trained
in a two-stage approach. Our results suggest that LE2E is a promising approach
for developing real-time, high quality, low-resource TTS applications for
on-device applications.

</details>


### [389] [ISAC: An Invertible and Stable Auditory Filter Bank with Customizable Kernels for ML Integration](https://arxiv.org/abs/2505.07709)
*Daniel Haider,Felix Perfler,Peter Balazs,Clara Hollomey,Nicki Holighaus*

Main category: cs.SD

TL;DR: 该论文提出了ISAC，一种可逆且稳定的感知驱动滤波器组，专为集成到机器学习范式而设计。它采用非线性听觉频率尺度，支持可学习卷积核，并实现完美重建。


<details>
  <summary>Details</summary>
Motivation: 设计一种适合机器学习应用的音频前端，结合听觉感知特性，提供灵活性和完美重建能力。

Method: 采用非线性听觉频率尺度设计滤波器组，支持用户定义的最大时间支持，并作为可学习卷积核。

Result: ISAC提供了一个强大且用户友好的音频前端，适用于包括分析-合成方案在内的多种应用。

Conclusion: ISAC滤波器组在机器学习和音频处理中表现出色，具备可逆性、稳定性和感知驱动特性。

Abstract: This paper introduces ISAC, an invertible and stable, perceptually-motivated
filter bank that is specifically designed to be integrated into machine
learning paradigms. More precisely, the center frequencies and bandwidths of
the filters are chosen to follow a non-linear, auditory frequency scale, the
filter kernels have user-defined maximum temporal support and may serve as
learnable convolutional kernels, and there exists a corresponding filter bank
such that both form a perfect reconstruction pair. ISAC provides a powerful and
user-friendly audio front-end suitable for any application, including
analysis-synthesis schemes.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [390] [CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated NPUs](https://arxiv.org/abs/2505.06625)
*Tianhao Cai,Liang Wang,Limin Xiao,Meng Han,Zeyu Wang,Lin Sun,Xiaojian Liao*

Main category: cs.AR

TL;DR: 提出CaMDN，一種架構-排程共設計方法，通過共享快取優化提升多租戶DNN在NPU上的性能。


<details>
  <summary>Details</summary>
Motivation: 現有文獻對共享快取在多租戶DNN中的影響研究不足，需提升快取效率以優化性能。

Method: 提出輕量架構支援模型專有快取區域，並設計快取排程方法（含快取感知映射與動態分配算法）。

Result: 平均減少33.4%記憶體存取，模型加速最高2.56倍（平均1.88倍）。

Conclusion: CaMDN有效消除快取競爭，顯著提升多租戶DNN效能。

Abstract: With the rapid development of DNN applications, multi-tenant execution, where
multiple DNNs are co-located on a single SoC, is becoming a prevailing trend.
Although many methods are proposed in prior works to improve multi-tenant
performance, the impact of shared cache is not well studied. This paper
proposes CaMDN, an architecture-scheduling co-design to enhance cache
efficiency for multi-tenant DNNs on integrated NPUs. Specifically, a
lightweight architecture is proposed to support model-exclusive, NPU-controlled
regions inside shared cache to eliminate unexpected cache contention. Moreover,
a cache scheduling method is proposed to improve shared cache utilization. In
particular, it includes a cache-aware mapping method for adaptability to the
varying available cache capacity and a dynamic allocation algorithm to adjust
the usage among co-located DNNs at runtime. Compared to prior works, CaMDN
reduces the memory access by 33.4% on average and achieves a model speedup of
up to 2.56$\times$ (1.88$\times$ on average).

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [391] [Low-Complexity CNN-Based Classification of Electroneurographic Signals](https://arxiv.org/abs/2505.06241)
*Arek Berc Gokdag,Silvia Mura,Antonio Coviello,Michele Zhu,Maurizio Magarini,Umberto Spagnolini*

Main category: eess.SP

TL;DR: MobilESCAPE-Net是一种轻量级架构，用于实时分类ENG信号，显著降低计算复杂度，同时保持分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决植入式设备中ENG信号实时分类的挑战，特别是在计算复杂度和延迟方面的限制。

Method: 开发了MobilESCAPE-Net，与现有ESCAPE-Net相比，显著减少了可训练参数和浮点运算。

Result: MobilESCAPE-Net在保持准确性和F1分数的同时，将参数减少了99.9%，运算量减少了92.47%。

Conclusion: MobilESCAPE-Net适合资源受限环境中的ENG信号分类，如植入式设备。

Abstract: Peripheral nerve interfaces (PNIs) facilitate neural recording and
stimulation for treating nerve injuries, but real-time classification of
electroneurographic (ENG) signals remains challenging due to constraints on
complexity and latency, particularly in implantable devices. This study
introduces MobilESCAPE-Net, a lightweight architecture that reduces
computational cost while maintaining and slightly improving classification
performance. Compared to the state-of-the-art ESCAPE-Net, MobilESCAPE-Net
achieves comparable accuracy and F1-score with significantly lower complexity,
reducing trainable parameters by 99.9\% and floating point operations per
second by 92.47\%, enabling faster inference and real-time processing. Its
efficiency makes it well-suited for low-complexity ENG signal classification in
resource-constrained environments such as implantable devices.

</details>


### [392] [DeltaDPD: Exploiting Dynamic Temporal Sparsity in Recurrent Neural Networks for Energy-Efficient Wideband Digital Predistortion](https://arxiv.org/abs/2505.06250)
*Yizhuo Wu,Yi Zhu,Kun Qian,Qinyu Chen,Anding Zhu,John Gajadharsing,Leo C. N. de Vreede,Chang Gao*

Main category: eess.SP

TL;DR: DeltaDPD是一种针对宽带射频功率放大器的数字预失真技术，通过利用输入信号和RNN隐藏状态的动态时间稀疏性，降低了计算复杂度和内存访问，同时保持了良好的线性化性能。


<details>
  <summary>Details</summary>
Motivation: 随着带宽和数据速率的增加，传统DPD技术面临能效挑战，需要一种更高效的实现方式。

Method: 采用动态时间稀疏性优化RNN模型，减少了算术运算和内存访问。

Result: 在特定测试条件下，DeltaDPD实现了优异的线性化性能（ACPR -50.03 dBc，NMSE -37.22 dB，EVM -38.52 dBc）和1.8倍的能效提升。

Conclusion: DeltaDPD在保持性能的同时显著提升了能效，为高效DPD技术提供了新方向。

Abstract: Digital Predistortion (DPD) is a popular technique to enhance signal quality
in wideband RF power amplifiers (PAs). With increasing bandwidth and data
rates, DPD faces significant energy consumption challenges during deployment,
contrasting with its efficiency goals. State-of-the-art DPD models rely on
recurrent neural networks (RNN), whose computational complexity hinders system
efficiency. This paper introduces DeltaDPD, exploring the dynamic temporal
sparsity of input signals and neuronal hidden states in RNNs for
energy-efficient DPD, reducing arithmetic operations and memory accesses while
preserving satisfactory linearization performance. Applying a TM3.1a 200MHz-BW
256-QAM OFDM signal to a 3.5 GHz GaN Doherty RF PA, DeltaDPD achieves -50.03
dBc in Adjacent Channel Power Ratio (ACPR), -37.22 dB in Normalized Mean Square
Error (NMSE) and -38.52 dBc in Error Vector Magnitude (EVM) with 52% temporal
sparsity, leading to a 1.8X reduction in estimated inference power. The
DeltaDPD code will be released after formal publication at
https://www.opendpd.com.

</details>


### [393] [SpectrumFM: A Foundation Model for Intelligent Spectrum Management](https://arxiv.org/abs/2505.06256)
*Fuhui Zhou,Chunyu Liu,Hao Zhang,Wei Wu,Qihui Wu,Derrick Wing Kwan Ng,Tony Q. S. Quek,Chan-Byoung Chae*

Main category: eess.SP

TL;DR: 本文提出了一种名为SpectrumFM的新型频谱基础模型，通过结合卷积神经网络和多头自注意力机制，提升了频谱管理的识别精度和泛化能力，并在多项任务中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有智能频谱管理方法在小规模模型上存在识别精度低、收敛速度慢和泛化能力差等问题，尤其在复杂动态频谱环境中表现不佳。

Method: 提出SpectrumFM模型，采用创新的编码器架构，结合卷积神经网络和多头自注意力机制，并通过掩码重构和下一时隙信号预测两种自监督学习任务进行预训练。

Result: 实验表明，SpectrumFM在精度、鲁棒性、适应性、少样本学习效率和收敛速度上均显著优于传统方法，如在AMC和WTC任务中分别提升12.1%和9.3%的准确率。

Conclusion: SpectrumFM为频谱管理提供了新范式，能够有效适应复杂动态环境，显著提升多项下游任务的性能。

Abstract: Intelligent spectrum management is crucial for improving spectrum efficiency
and achieving secure utilization of spectrum resources. However, existing
intelligent spectrum management methods, typically based on small-scale models,
suffer from notable limitations in recognition accuracy, convergence speed, and
generalization, particularly in the complex and dynamic spectrum environments.
To address these challenges, this paper proposes a novel spectrum foundation
model, termed SpectrumFM, establishing a new paradigm for spectrum management.
SpectrumFM features an innovative encoder architecture that synergistically
exploits the convolutional neural networks and the multi-head self-attention
mechanisms to enhance feature extraction and enable robust representation
learning. The model is pre-trained via two novel self-supervised learning
tasks, namely masked reconstruction and next-slot signal prediction, which
leverage large-scale in-phase and quadrature (IQ) data to achieve comprehensive
and transferable spectrum representations. Furthermore, a parameter-efficient
fine-tuning strategy is proposed to enable SpectrumFM to adapt to various
downstream spectrum management tasks, including automatic modulation
classification (AMC), wireless technology classification (WTC), spectrum
sensing (SS), and anomaly detection (AD). Extensive experiments demonstrate
that SpectrumFM achieves superior performance in terms of accuracy, robustness,
adaptability, few-shot learning efficiency, and convergence speed, consistently
outperforming conventional methods across multiple benchmarks. Specifically,
SpectrumFM improves AMC accuracy by up to 12.1% and WTC accuracy by 9.3%,
achieves an area under the curve (AUC) of 0.97 in SS at -4 dB signal-to-noise
ratio (SNR), and enhances AD performance by over 10%.

</details>


### [394] [Terahertz Spatial Wireless Channel Modeling with Radio Radiance Field](https://arxiv.org/abs/2505.06277)
*John Song,Lihao Zhang,Feng Ye,Haijian Sun*

Main category: eess.SP

TL;DR: 研究了通过无线电辐射场（RRF）框架在太赫兹（THz）频段应用的可行性，该方法结合视觉几何和稀疏THz射频测量重建连续RRF，有效建模空间信道状态信息，避免了密集采样。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信在6G系统中具有超高带宽和速率潜力，但THz信号传播特性导致传统信道建模方法效率低下，因此探索更高效的建模方法具有重要意义。

Method: 构建精细的模拟THz场景，利用视觉几何和稀疏测量重建RRF，评估其在THz通信中的重建质量和有效性。

Result: 重建的RRF能够通过稀疏训练样本捕捉关键传播路径，验证了其在THz频段的有效性。

Conclusion: RRF建模在THz频段仍有效，为未来6G网络中低成本、可扩展的空间信道重建提供了有前景的方向。

Abstract: Terahertz (THz) communication is a key enabler for 6G systems, offering
ultra-wide bandwidth and unprecedented data rates. However, THz signal
propagation differs significantly from lower-frequency bands due to severe free
space path loss, minimal diffraction and specular reflection, and prominent
scattering, making conventional channel modeling and pilot-based estimation
approaches inefficient. In this work, we investigate the feasibility of
applying radio radiance field (RRF) framework to the THz band. This method
reconstructs a continuous RRF using visual-based geometry and sparse THz RF
measurements, enabling efficient spatial channel state information
(Spatial-CSI) modeling without dense sampling. We first build a fine simulated
THz scenario, then we reconstruct the RRF and evaluate the performance in terms
of both reconstruction quality and effectiveness in THz communication, showing
that the reconstructed RRF captures key propagation paths with sparse training
samples. Our findings demonstrate that RRF modeling remains effective in the
THz regime and provides a promising direction for scalable, low-cost spatial
channel reconstruction in future 6G networks.

</details>


### [395] [A Short Overview of Multi-Modal Wi-Fi Sensing](https://arxiv.org/abs/2505.06682)
*Zijian Zhao*

Main category: eess.SP

TL;DR: 该论文综述了过去24个月中的多模态Wi-Fi感知技术，分析了其优势、挑战及未来发展方向，弥补了该领域缺乏全面调研的空白。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi感知技术成本低、穿透性强且隐私性高，但面临鲁棒性差和数据收集困难等问题。多模态Wi-Fi感知通过结合其他模态提升性能，但缺乏系统性的综述，因此本文旨在填补这一空白。

Method: 通过对过去24个月的多模态Wi-Fi感知文献进行综述，梳理现有方法、性能和局限性。

Result: 多模态Wi-Fi感知在实际应用中表现出巨大潜力，但仍需解决数据融合、模型鲁棒性等挑战。

Conclusion: 本文总结了多模态Wi-Fi感知的现状和未来研究方向，为进一步推动该领域发展提供了参考。

Abstract: Wi-Fi sensing has emerged as a significant technology in wireless sensing and
Integrated Sensing and Communication (ISAC), offering benefits such as low
cost, high penetration, and enhanced privacy. Currently, it is widely utilized
in various applications, including action recognition, human localization, and
crowd counting. However, Wi-Fi sensing also faces challenges, such as low
robustness and difficulties in data collection. Recently, there has been an
increasing focus on multi-modal Wi-Fi sensing, where other modalities can act
as teachers, providing ground truth or robust features for Wi-Fi sensing models
to learn from, or can be directly fused with Wi-Fi for enhanced sensing
capabilities. Although these methods have demonstrated promising results and
substantial value in practical applications, there is a lack of comprehensive
surveys reviewing them. To address this gap, this paper reviews the multi-modal
Wi-Fi sensing literature \textbf{from the past 24 months} and highlights the
current limitations, challenges and future directions in this field.

</details>


### [396] [Supervised machine learning based signal demodulation in chaotic communications](https://arxiv.org/abs/2505.06243)
*Mykola Kozlenko*

Main category: eess.SP

TL;DR: 这篇论文提出了一种基于机器学习的解调方法，用于混沌分岔参数键控技术，通过卷积神经网络评估了在加性高斯白噪声下的性能。


<details>
  <summary>Details</summary>
Motivation: 混沌调制是一种高效的宽带通信方法，但传统解调方法可能不够高效或准确，因此需要探索机器学习在解调中的应用。

Method: 使用卷积神经网络（CNN）对由混沌逻辑映射生成的信号进行解调，重点分析了分岔参数键控技术。

Result: 在加性高斯白噪声环境下，当分岔参数偏差为1.34%、信噪比为20 dB时，二元信号解调的准确率达到0.88。

Conclusion: 机器学习尤其是CNN在混沌信号解调中表现出良好的性能，为宽带通信提供了一种高效的解调方案。

Abstract: A chaotic modulation scheme is an efficient wideband communication method. It
utilizes the deterministic chaos to generate pseudo-random carriers. Chaotic
bifurcation parameter modulation is one of the well-known and widely-used
techniques. This paper presents the machine learning based demodulation
approach for the bifurcation parameter keying. It presents the structure of a
convolutional neural network as well as performance metrics values for signals
generated with the chaotic logistic map. The paper provides an assessment of
the overall accuracy for binary signals. It reports the accuracy value of 0.88
for the bifurcation parameter deviation of 1.34% in the presence of additive
white Gaussian noise at the normalized signal-to-noise ratio value of 20 dB for
balanced dataset.

</details>


### [397] [A Transformer-Based Approach for Diagnosing Fault Cases in Optical Fiber Amplifiers](https://arxiv.org/abs/2505.06245)
*Dominic Schneider,Lutz Rapp,Christoph Ament*

Main category: eess.SP

TL;DR: 提出了一种基于Transformer的深度学习方法ITST，用于通过时序数据诊断光纤放大器故障，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 通过提升故障诊断的准确性，减少光纤放大器网络的中断时间和维护成本。

Method: 使用编码器-解码器架构，编码器包含三条特征提取路径，解码器采用特征工程数据，并结合自注意力机制。

Result: ITST在分类准确率上优于现有最先进模型。

Conclusion: ITST能有效支持光纤放大器的预测性维护，提升网络效率和降低成本。

Abstract: A transformer-based deep learning approach is presented that enables the
diagnosis of fault cases in optical fiber amplifiers using condition-based
monitoring time series data. The model, Inverse Triple-Aspect Self-Attention
Transformer (ITST), uses an encoder-decoder architecture, utilizing three
feature extraction paths in the encoder, feature-engineered data for the
decoder and a self-attention mechanism. The results show that ITST outperforms
state-of-the-art models in terms of classification accuracy, which enables
predictive maintenance for optical fiber amplifiers, reducing network downtimes
and maintenance costs.

</details>


### [398] [From Biometrics to Environmental Control: AI-Enhanced Digital Twins for Personalized Health Interventions in Healing Landscapes](https://arxiv.org/abs/2505.06263)
*Yiping Meng,Yiming Sun*

Main category: eess.SP

TL;DR: 该论文提出了一个AI增强的数字孪生框架，通过整合ECG数据和环境参数，实时响应个体生理需求，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 动态的人类健康和舒适需求需要能实时响应个体生理变化的适应性系统。

Method: 利用IoT传感器和生物识别设备采集多模态数据，构建数字孪生模型，通过随机森林分类器和SHAP方法预测和解释压力水平。

Result: 系统能准确预测压力水平并触发多尺度环境干预，验证了框架的有效性。

Conclusion: 该研究为健康响应型建筑环境提供了新范式，为未来智能个性化疗愈空间的发展奠定了基础。

Abstract: The dynamic nature of human health and comfort calls for adaptive systems
that respond to individual physiological needs in real time. This paper
presents an AI-enhanced digital twin framework that integrates biometric
signals, specifically electrocardiogram (ECG) data, with environmental
parameters such as temperature, humidity, and ventilation. Leveraging
IoT-enabled sensors and biometric monitoring devices, the system continuously
acquires, synchronises, and preprocesses multimodal data streams to construct a
responsive virtual replica of the physical environment. To validate this
framework, a detailed case study is conducted using the MIT-BIH noise stress
test dataset. ECG signals are filtered and segmented using dynamic sliding
windows, followed by extracting heart rate variability (HRV) features such as
SDNN, BPM, QTc, and LF/HF ratio. Relative deviation metrics are computed
against clean baselines to quantify stress responses. A random forest
classifier is trained to predict stress levels across five categories, and
Shapley Additive exPlanations (SHAP) is used to interpret model behaviour and
identify key contributing features. These predictions are mapped to a
structured set of environmental interventions using a Five Level Stress
Intervention Mapping, which activates multi-scale responses across personal,
room, building, and landscape levels. This integration of physiological
insight, explainable AI, and adaptive control establishes a new paradigm for
health-responsive built environments. It lays the foundation for the future
development of intelligent, personalised healing spaces.

</details>


### [399] [ALFEE: Adaptive Large Foundation Model for EEG Representation](https://arxiv.org/abs/2505.06291)
*Wei Xiong,Junming Lin,Jiangtong Li,Jie Li,Changjun Jiang*

Main category: eess.SP

TL;DR: 该论文提出了一种名为ALFEE的新型混合变压器架构，用于解决EEG信号表示学习中的通用性问题，通过两阶段学习和混合注意力机制，显著提升了模型在多种下游任务中的性能。


<details>
  <summary>Details</summary>
Motivation: EEG信号在神经科学研究中非常重要，但当前模型在泛化性上受限于信噪比低、个体间差异大和交叉范式差异等问题，现有方法通常过于简化且存在预训练与评估任务间的领域差距。

Method: ALFEE采用混合注意力机制，分别处理通道和时间特征，包括通道编码器、时间编码器和混合解码器。预训练阶段优化多任务目标，微调阶段通过任务特定标记字典和交叉注意力层提升性能。

Result: 经过25,000小时的预训练，ALFEE在六个下游EEG任务中表现优于现有模型。

Conclusion: ALFEE框架为生物信号分析提供了可扩展的基础，且已在GitHub上开源。

Abstract: While foundation models excel in text, image, and video domains, the critical
biological signals, particularly electroencephalography(EEG), remain
underexplored. EEG benefits neurological research with its high temporal
resolution, operational practicality, and safety profile. However, low
signal-to-noise ratio, inter-subject variability, and cross-paradigm
differences hinder the generalization of current models. Existing methods often
employ simplified strategies, such as a single loss function or a
channel-temporal joint representation module, and suffer from a domain gap
between pretraining and evaluation tasks that compromises efficiency and
adaptability. To address these limitations, we propose the Adaptive Large
Foundation model for EEG signal representation(ALFEE) framework, a novel hybrid
transformer architecture with two learning stages for robust EEG representation
learning. ALFEE employs a hybrid attention that separates channel-wise feature
aggregation from temporal dynamics modeling, enabling robust EEG representation
with variable channel configurations. A channel encoder adaptively compresses
variable channel information, a temporal encoder captures task-guided
evolution, and a hybrid decoder reconstructs signals in both temporal and
frequency domains. During pretraining, ALFEE optimizes task prediction, channel
and temporal mask reconstruction, and temporal forecasting to enhance
multi-scale and multi-channel representation. During fine-tuning, a full-model
adaptation with a task-specific token dictionary and a cross-attention layer
boosts performance across multiple tasks. After 25,000 hours of pretraining,
extensive experimental results on six downstream EEG tasks demonstrate the
superior performance of ALFEE over existing models. Our ALFEE framework
establishes a scalable foundation for biological signal analysis with
implementation at https://github.com/xw1216/ALFEE.

</details>


### [400] [Near-Field Channel Estimation for XL-MIMO: A Deep Generative Model Guided by Side Information](https://arxiv.org/abs/2505.06900)
*Zhenzhou Jin,Li You,Derrick Wing Kwan Ng,Xiang-Gen Xia,Xiqi Gao*

Main category: eess.SP

TL;DR: 论文研究了极大规模多输入多输出（XL-MIMO）系统中的近场信道估计问题，提出了一种基于联合角度-距离域的稀疏信道模型，并结合生成式人工智能（GenAI）优化估计结果，显著提升了信道估计性能。


<details>
  <summary>Details</summary>
Motivation: XL-MIMO系统中近场效应的显著存在使得传统信道估计方法不再适用，因此需要建立新的物理信道模型并开发高效估计方法。

Method: 首先建立联合角度-距离域的稀疏信道模型，提出基于压缩感知的初步估计方法，并进一步利用生成式扩散模型（GDM）优化估计结果，引入了非马尔可夫GDM（NM-GDM）加速采样过程。

Result: 实验结果表明，所提方法在近场XL-MIMO系统中显著优于现有基准方案，且在近场和远场区域均表现出更好的泛化能力。

Conclusion: 结合稀疏信道建模和生成式AI的优化方法，为XL-MIMO系统提供了高效的信道估计解决方案，具有较高的实用价值。

Abstract: This paper investigates the near-field (NF) channel estimation (CE) for
extremely large-scale multiple-input multiple-output (XL-MIMO) systems.
Considering the pronounced NF effects in XL-MIMO communications, we first
establish a joint angle-distance (AD) domain-based spherical-wavefront physical
channel model that captures the inherent sparsity of XL-MIMO channels.
Leveraging the channel's sparsity in the joint AD domain, the CE is approached
as a task of reconstructing sparse signals. Anchored in this framework, we
first propose a compressed sensing algorithm to acquire a preliminary channel
estimate. Harnessing the powerful implicit prior learning capability of
generative artificial intelligence (GenAI), we further propose a GenAI-based
approach to refine the estimated channel. Specifically, we introduce the
preliminary estimated channel as side information, and derive the evidence
lower bound (ELBO) of the log-marginal distribution of the target NF channel
conditioned on the preliminary estimated channel, which serves as the
optimization objective for the proposed generative diffusion model (GDM).
Additionally, we introduce a more generalized version of the GDM, the
non-Markovian GDM (NM-GDM), to accelerate the sampling process, achieving an
approximately tenfold enhancement in sampling efficiency. Experimental results
indicate that the proposed approach is capable of offering substantial
performance gain in CE compared to existing benchmark schemes within NF XL-MIMO
systems. Furthermore, our approach exhibits enhanced generalization
capabilities in both the NF or far-field (FF) regions.

</details>


### [401] [SmartUT: Receive Beamforming for Spectral Coexistence of NGSO Satellite Systems](https://arxiv.org/abs/2505.07714)
*Almoatssimbillah Saifaldawla,Eva Lagunas,Flor Ortiz,Abuzar B. M. Adam,Symeon Chatzinotas*

Main category: eess.SP

TL;DR: 论文提出了一种基于Mamba的无监督深度学习波束成形器（MambaBF），用于非静止轨道卫星系统中的同频干扰抑制，无需信道状态信息即可在有限快照下实现高性能。


<details>
  <summary>Details</summary>
Motivation: 传统干扰抑制方法（如零迫）因计算复杂且依赖信道状态信息（CSI）而受限，而自适应波束成形器（如SMI）在快照不足时表现不佳。

Method: 采用Mamba框架的无监督深度学习模型，仅需有限快照输入，无需CSI知识，直接在用户终端天线阵列上部署实现波束成形和干扰抑制。

Result: 仿真表明，MambaBF在低信干噪比、快照有限和CSI不完美的挑战性场景下，干扰抑制效果和信号质量均优于传统方法。

Conclusion: MambaBF为卫星通信中的同频干扰问题提供了一种高效且鲁棒的解决方案，尤其在资源受限条件下表现优异。

Abstract: In this paper, we investigate downlink co-frequency interference (CFI)
mitigation in non-geostationary satellites orbits (NGSOs) co-existing systems.
Traditional mitigation techniques, such as Zero-forcing (ZF), produce a null
towards the direction of arrivals (DOAs) of the interfering signals, but they
suffer from high computational complexity due to matrix inversions and required
knowledge of the channel state information (CSI). Furthermore, adaptive
beamformers, such as sample matrix inversion (SMI)-based minimum variance,
provide poor performance when the available snapshots are limited. We propose a
Mamba-based beamformer (MambaBF) that leverages an unsupervised deep learning
(DL) approach and can be deployed on the user terminal (UT) antenna array, for
assisting downlink beamforming and CFI mitigation using only a limited number
of available array snapshots as input, and without CSI knowledge. Simulation
results demonstrate that MambaBF consistently outperforms conventional
beamforming techniques in mitigating interference and maximizing the
signal-to-interference-plus-noise ratio (SINR), particularly under challenging
conditions characterized by low SINR, limited snapshots, and imperfect CSI.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [402] [Efficient Parallelization of Message Passing Neural Networks](https://arxiv.org/abs/2505.06711)
*Junfan Xia,Bin Jiang*

Main category: physics.chem-ph

TL;DR: 该论文提出了一种高效的并行算法，用于消息传递神经网络（MPNN）模型，解决了其在大型模拟中并行化的问题，通过最小化数据通信和冗余计算，实现了线性扩展。结合递归嵌入原子神经网络模型，该方法在多个基准系统中表现出优异的强扩展和弱扩展行为。


<details>
  <summary>Details</summary>
Motivation: 机器学习的势函数在加速原子模拟方面取得了巨大成功，但MPNN模型在大规模模拟中的并行化仍然是一个挑战，主要由于层数增加时的扩展性问题及数据通信的必要性。因此，研究团队旨在开发一种高效并行算法，以解决这些问题。

Method: 论文提出了一种高效的并行算法，通过最小化本地原子间的额外数据通信，并在每一消息传递层中避免冗余计算，从而实现了线性扩展。该方法与递归嵌入原子神经网络模型结合使用。

Result: 该方法在多个基准系统中表现出优异的强扩展和弱扩展行为，能够用于数亿原子的分子动力学模拟，速度与严格本地模型相当。

Conclusion: 该并行化框架极大地扩展了MPNN模型的适用范围，使其能够高效模拟非常庞大和复杂的系统。

Abstract: Machine learning potentials have achieved great success in accelerating
atomistic simulations. Many of them rely on local descriptors that readily
allow parallelization. More recent message passing neural network (MPNN) models
have demonstrated their superior accuracy and become increasingly popular.
However, parallelizing MPNN models for large-scale simulations across compute
nodes remains a challenge, as the previously argued poor scalability with the
number of MP layers and the necessity of data communication. Here, we propose
an efficient parallel algorithm for MPNN models, in which additional data
communication is minimized among local atoms only in each MP layer without
redundant computation, thus scaling linearly with the layer number. Integrated
with our recursively embedded atom neural network model, this algorithm
demonstrates excellent strong scaling and weak scaling behaviors in several
benchmark systems. This approach enables massive molecular dynamics simulations
on MPNN models for hundreds of millions of atoms as fast as on strictly local
models, vastly extending the applicability of the MPNN potential to an
unprecedented scale. This general parallelization framework can empower various
MPNN models to efficiently simulate very large and complex systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [403] [AI Approaches to Qualitative and Quantitative News Analytics on NATO Unity](https://arxiv.org/abs/2505.06313)
*Bohdan M. Pavlyshenko*

Main category: cs.IR

TL;DR: 论文探讨了使用GPT模型与检索增强生成（RAG）技术分析北约情绪、团结及对第5条信任的定性定量方法，通过网络新闻、YouTube评论和Reddit讨论数据，发现相关意见分数呈下降趋势。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索AI（特别是GPT模型）在复杂分析中的潜力，为政治或公共意见分析提供初步的定性定量工具，而非直接用于实际政治决策。

Method: 采用GPT-4.1模型与RAG技术，分两级分析：首层生成新闻摘要和意见分数，次层汇总摘要；通过贝叶斯回归分析趋势，并利用神经常微分方程动态模拟舆论演变。

Result: 分析显示，北约团结相关意见分数呈现下降趋势，模型能有效捕捉不确定性并提供信息性分析。

Conclusion: GPT模型结合RAG技术可为新闻分析提供有价值的定性定量结果，动态模型进一步扩展了舆论演变的情景分析能力。

Abstract: The paper considers the use of GPT models with retrieval-augmented generation
(RAG) for qualitative and quantitative analytics on NATO sentiments, NATO unity
and NATO Article 5 trust opinion scores in different web sources: news sites
found via Google Search API, Youtube videos with comments, and Reddit
discussions. A RAG approach using GPT-4.1 model was applied to analyse news
where NATO related topics were discussed. Two levels of RAG analytics were
used: on the first level, the GPT model generates qualitative news summaries
and quantitative opinion scores using zero-shot prompts; on the second level,
the GPT model generates the summary of news summaries. Quantitative news
opinion scores generated by the GPT model were analysed using Bayesian
regression to get trend lines. The distributions found for the regression
parameters make it possible to analyse an uncertainty in specified news opinion
score trends. Obtained results show a downward trend for analysed scores of
opinion related to NATO unity.
  This approach does not aim to conduct real political analysis; rather, it
consider AI based approaches which can be used for further analytics
  as a part of a complex analytical approach. The obtained results demonstrate
that the use of GPT models for news analysis can give informative qualitative
and quantitative analytics, providing important insights.
  The dynamic model based on neural ordinary differential equations was
considered for modelling public opinions. This approach makes it possible to
analyse different scenarios for evolving public opinions.

</details>


### [404] [Web Page Classification using LLMs for Crawling Support](https://arxiv.org/abs/2505.06972)
*Yuichi Sasazawa,Yasuhiro Sogawa*

Main category: cs.IR

TL;DR: 论文提出了一种利用大语言模型（LLM）将网页分类为“索引页”和“内容页”，并基于分类结果选择索引页作为新页面爬取起点的有效方法，实验表明该方法在分类性能和新页面覆盖率上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 针对多样条件下高效爬取新页面的挑战，研究通过利用网站特征（如XML站点地图和页面更新频率）并结合LLM分类网页类型，旨在提升新页面采集效率。

Method: 使用LLM将网页分为“索引页”和“内容页”两类，构建自动标注的数据集，并以索引页为起点爬取新页面。

Result: 实验结果显示，基于LLM的方法在页面类型分类性能和新页面覆盖率上均优于基线方法。

Conclusion: 通过LLM分类网页类型并优化爬取策略，能够显著提升新页面采集的效率和覆盖率。

Abstract: A web crawler is a system designed to collect web pages, and efficient
crawling of new pages requires appropriate algorithms. While website features
such as XML sitemaps and the frequency of past page updates provide important
clues for accessing new pages, their universal application across diverse
conditions is challenging. In this study, we propose a method to efficiently
collect new pages by classifying web pages into two types, "Index Pages" and
"Content Pages," using a large language model (LLM), and leveraging the
classification results to select index pages as starting points for accessing
new pages. We construct a dataset with automatically annotated web page types
and evaluate our approach from two perspectives: the page type classification
performance and coverage of new pages. Experimental results demonstrate that
the LLM-based method outperformed baseline methods in both evaluation metrics.

</details>


### [405] [Reassessing Large Language Model Boolean Query Generation for Systematic Reviews](https://arxiv.org/abs/2505.07155)
*Shuai Wang,Harrisen Scells,Bevan Koopman,Guido Zuccon*

Main category: cs.IR

TL;DR: 论文探讨了大型语言模型（LLM）在生成系统性综述的布尔查询中的作用，分析了模型选择和提示设计对查询效果的影响，强调了可重复性研究的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于手动构建系统性综述的布尔查询困难，研究者探索了LLM的辅助作用，但后续研究忽略了原始工作的关键方面，导致结果不一致。本文旨在通过系统复现澄清这些问题。

Method: 系统复现了Wang et al.和Staudinger et al.的研究，特别关注了查询验证、输出格式约束和引导提示的示例选择等被忽视的因素。

Result: 研究表明，查询效果因模型和提示设计而异，引导查询从精心选择的种子研究中获益。提示设计和模型选择是成功生成查询的关键。

Conclusion: LLM在布尔查询生成中具有潜力，但需针对模型和提示进行优化。系统性综述的复杂性增加了开发和复现方法的挑战，也凸显了可重复性研究的重要性。

Abstract: Systematic reviews are comprehensive literature reviews that address highly
focused research questions and represent the highest form of evidence in
medicine. A critical step in this process is the development of complex Boolean
queries to retrieve relevant literature. Given the difficulty of manually
constructing these queries, recent efforts have explored Large Language Models
(LLMs) to assist in their formulation. One of the first studies,Wang et al.,
investigated ChatGPT for this task, followed by Staudinger et al., which
evaluated multiple LLMs in a reproducibility study. However, the latter
overlooked several key aspects of the original work, including (i) validation
of generated queries, (ii) output formatting constraints, and (iii) selection
of examples for chain-of-thought (Guided) prompting. As a result, its findings
diverged significantly from the original study. In this work, we systematically
reproduce both studies while addressing these overlooked factors. Our results
show that query effectiveness varies significantly across models and prompt
designs, with guided query formulation benefiting from well-chosen seed
studies. Overall, prompt design and model selection are key drivers of
successful query formulation. Our findings provide a clearer understanding of
LLMs' potential in Boolean query generation and highlight the importance of
model- and prompt-specific optimisations. The complex nature of systematic
reviews adds to challenges in both developing and reproducing methods but also
highlights the importance of reproducibility studies in this domain.

</details>


### [406] [Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval Knowledge Acquisition](https://arxiv.org/abs/2505.07166)
*Zheng Yao,Shuai Wang,Guido Zuccon*

Main category: cs.IR

TL;DR: 这篇论文研究了密集型检索器中预训练和微调的作用，发现预训练知识对检索性能至关重要，而微调主要调节神经元激活而非重组知识，但这一结论在不同模型和数据集上存在差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨密集型检索器中预训练和微调的相对贡献，澄清预训练知识是否足以支撑检索任务，而微调是否真正引入新知识。

Method: 方法包括对比分析不同的表示方法（CLS token与平均池化）、模型架构（BERT与LLaMA）以及数据集（MSMARCO和Natural Questions），以验证检索知识的获取来源。

Result: 结果表明，在DPR微调中，预训练知识是检索性能的基础，微调主要调整神经元激活；然而这一结论在平均池化的Contriever和解码器模型LLaMA中并不普遍适用。

Conclusion: 论文结论强调了预训练知识的重要性，但也指出不同模型和检索方法之间存在显著差异，为未来研究提供了新方向。实现代码已开源。

Abstract: Dense retrievers utilize pre-trained backbone language models (e.g., BERT,
LLaMA) that are fine-tuned via contrastive learning to perform the task of
encoding text into sense representations that can be then compared via a
shallow similarity operation, e.g. inner product. Recent research has
questioned the role of fine-tuning vs. that of pre-training within dense
retrievers, specifically arguing that retrieval knowledge is primarily gained
during pre-training, meaning knowledge not acquired during pre-training cannot
be sub-sequentially acquired via fine-tuning. We revisit this idea here as the
claim was only studied in the context of a BERT-based encoder using DPR as
representative dense retriever. We extend the previous analysis by testing
other representation approaches (comparing the use of CLS tokens with that of
mean pooling), backbone architectures (encoder-only BERT vs. decoder-only
LLaMA), and additional datasets (MSMARCO in addition to Natural Questions). Our
study confirms that in DPR tuning, pre-trained knowledge underpins retrieval
performance, with fine-tuning primarily adjusting neuron activation rather than
reorganizing knowledge. However, this pattern does not hold universally, such
as in mean-pooled (Contriever) and decoder-based (LLaMA) models. We ensure full
reproducibility and make our implementation publicly available at
https://github.com/ielab/DenseRetriever-Knowledge-Acquisition.

</details>


### [407] [Document Attribution: Examining Citation Relationships using Large Language Models](https://arxiv.org/abs/2505.06324)
*Vipula Rawte,Ryan A. Rossi,Franck Dernoncourt,Nedim Lipka*

Main category: cs.IR

TL;DR: 该论文提出了两种技术来评估LLM输出的可靠性：基于零样本的文本蕴含方法和注意力机制优化。实验证明，两种方法在AttributionBench数据集上均有提升。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在文档任务中的应用增加，确保其输出的可信性和可解释性变得至关重要。现有方法可能产生不准确或不精确的引用，需要评估可靠性。

Method: 论文提出了两种技术：1）零样本方法，将归属问题建模为文本蕴含任务；2）利用注意力机制优化归属过程。分别使用flan-ul2和flan-t5-small模型进行实验。

Result: 零样本方法在AttributionBench数据集的ID和OOD集上分别提升0.27%和2.4%。注意力机制方法（flan-t5-small）在大多数层的F1分数优于基线。

Conclusion: 两种方法均能有效提升LLM输出的可解释性和可信性，为文档任务的可靠性提供了新思路。

Abstract: As Large Language Models (LLMs) are increasingly applied to document-based
tasks - such as document summarization, question answering, and information
extraction - where user requirements focus on retrieving information from
provided documents rather than relying on the model's parametric knowledge,
ensuring the trustworthiness and interpretability of these systems has become a
critical concern. A central approach to addressing this challenge is
attribution, which involves tracing the generated outputs back to their source
documents. However, since LLMs can produce inaccurate or imprecise responses,
it is crucial to assess the reliability of these citations.
  To tackle this, our work proposes two techniques. (1) A zero-shot approach
that frames attribution as a straightforward textual entailment task. Our
method using flan-ul2 demonstrates an improvement of 0.27% and 2.4% over the
best baseline of ID and OOD sets of AttributionBench, respectively. (2) We also
explore the role of the attention mechanism in enhancing the attribution
process. Using a smaller LLM, flan-t5-small, the F1 scores outperform the
baseline across almost all layers except layer 4 and layers 8 through 11.

</details>


### [408] [Optimizing Recommendations using Fine-Tuned LLMs](https://arxiv.org/abs/2505.06841)
*Prabhdeep Cheema,Erhan Guven*

Main category: cs.IR

TL;DR: 研究提出了一种通过模拟真实用户交互生成合成数据集的方法，以改进数字娱乐平台中个性化推荐的准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 为了满足用户对高度个性化推荐的需求，传统基于关键词的系统限制了用户表达的复杂性，无法捕捉多样化的偏好。

Method: 通过建模真实用户交互生成合成数据集，支持复杂聊天式查询，涵盖情绪、情节细节等非传统搜索标准。

Result: 合成的数据集增强了模型训练的多样性和有效性，提高了推荐系统的个性化和准确性。

Conclusion: 该方法为下一代对话式AI驱动的搜索和推荐系统奠定了基础，特别是在数字娱乐领域。

Abstract: As digital media platforms strive to meet evolving user expectations,
delivering highly personalized and intuitive movies and media recommendations
has become essential for attracting and retaining audiences. Traditional
systems often rely on keyword-based search and recommendation techniques, which
limit users to specific keywords and a combination of keywords. This paper
proposes an approach that generates synthetic datasets by modeling real-world
user interactions, creating complex chat-style data reflective of diverse
preferences. This allows users to express more information with complex
preferences, such as mood, plot details, and thematic elements, in addition to
conventional criteria like genre, title, and actor-based searches. In today's
search space, users cannot write queries like ``Looking for a fantasy movie
featuring dire wolves, ideally set in a harsh frozen world with themes of
loyalty and survival.''
  Building on these contributions, we evaluate synthetic datasets for diversity
and effectiveness in training and benchmarking models, particularly in areas
often absent from traditional datasets. This approach enhances personalization
and accuracy by enabling expressive and natural user queries. It establishes a
foundation for the next generation of conversational AI-driven search and
recommendation systems in digital entertainment.

</details>


### [409] [GRADA: Graph-based Reranker against Adversarial Documents Attack](https://arxiv.org/abs/2505.07546)
*Jingjie Zheng,Aryo Pradipta Gema,Giwon Hong,Xuanli He,Pasquale Minervini,Youcheng Sun,Qiongkai Xu*

Main category: cs.IR

TL;DR: 论文提出了一种名为GRADA的图重排序框架，用于抵御检索增强生成（RAG）系统中的对抗性攻击，通过实验证明了其对多种大型语言模型的有效性。


<details>
  <summary>Details</summary>
Motivation: RAG系统虽然能提升LLMs的准确性，但易受对抗性攻击的影响，攻击者通过引入语义相似但对抗性的文档干扰检索过程。

Method: 采用基于图的重排序（GRADA）框架，在保持检索质量的同时减少对抗攻击的成功率。

Result: 在五个LLMs和三个数据集上的实验表明，GRADA能将攻击成功率降低80%，且准确性损失极小。

Conclusion: GRADA是一种简单有效的解决方案，可显著提升RAG系统对抗对抗性攻击的鲁棒性，同时保持检索性能。

Abstract: Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large
language models (LLMs) by integrating external knowledge from retrieved
documents, thereby overcoming the limitations of models' static intrinsic
knowledge. However, these systems are susceptible to adversarial attacks that
manipulate the retrieval process by introducing documents that are adversarial
yet semantically similar to the query. Notably, while these adversarial
documents resemble the query, they exhibit weak similarity to benign documents
in the retrieval set. Thus, we propose a simple yet effective Graph-based
Reranking against Adversarial Document Attacks (GRADA) framework aiming at
preserving retrieval quality while significantly reducing the success of
adversaries. Our study evaluates the effectiveness of our approach through
experiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b,
Llama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with
results from the Natural Questions dataset demonstrating up to an 80% reduction
in attack success rates while maintaining minimal loss in accuracy.

</details>


### [410] [Knowledge Distillation for Enhancing Walmart E-commerce Search Relevance Using Large Language Models](https://arxiv.org/abs/2505.07105)
*Hongwei Shang,Nguyen Vo,Nitin Yadav,Tian Zhang,Ajit Puthenputhussery,Xunfan Cai,Shuyi Chen,Prijith Chandran,Changsung Kang*

Main category: cs.IR

TL;DR: 该论文提出一种新框架，将高性能大语言模型（LLM）蒸馏为高效低延迟的学生模型，以解决LLM在实际系统中部署的高延迟问题。通过软目标分类和扩大训练数据集，学生模型性能甚至超越教师模型。该方法已在Walmart.com成功部署。


<details>
  <summary>Details</summary>
Motivation: 为解决大语言模型（LLM）在实时电商搜索系统中因高延迟无法部署的问题，同时保留其强大的排序能力。

Method: 1. 将教师LLM训练为带软目标的分类模型；2. 使用均方误差损失训练学生模型学习商品对的相关性差异；3. 通过生成未标注数据并用教师模型预测标注，大幅扩展学生模型训练集。

Result: 实验显示，随着增强训练数据的增加，学生模型性能持续提升，甚至超越教师模型。该模型在Walmart.com的实际部署中表现优异。

Conclusion: 通过蒸馏和数据集扩展，学生模型在保持低延迟的同时实现了高性能，证明了该框架在实际系统中的有效性。

Abstract: Ensuring the products displayed in e-commerce search results are relevant to
users queries is crucial for improving the user experience. With their advanced
semantic understanding, deep learning models have been widely used for
relevance matching in search tasks. While large language models (LLMs) offer
superior ranking capabilities, it is challenging to deploy LLMs in real-time
systems due to the high-latency requirements. To leverage the ranking power of
LLMs while meeting the low-latency demands of production systems, we propose a
novel framework that distills a high performing LLM into a more efficient,
low-latency student model. To help the student model learn more effectively
from the teacher model, we first train the teacher LLM as a classification
model with soft targets. Then, we train the student model to capture the
relevance margin between pairs of products for a given query using mean squared
error loss. Instead of using the same training data as the teacher model, we
significantly expand the student model dataset by generating unlabeled data and
labeling it with the teacher model predictions. Experimental results show that
the student model performance continues to improve as the size of the augmented
training data increases. In fact, with enough augmented data, the student model
can outperform the teacher model. The student model has been successfully
deployed in production at Walmart.com with significantly positive metrics.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [411] [Optimal Transport for Machine Learners](https://arxiv.org/abs/2505.06589)
*Gabriel Peyré*

Main category: stat.ML

TL;DR: 该论文摘要介绍了最优传输理论在机器学习中的应用，特别是生成模型设计和评估。内容涵盖基本数学理论、数值方法及机器学习应用，但侧重于数学内容而非深度学习技术。


<details>
  <summary>Details</summary>
Motivation: 最优传输理论结合了优化、偏微分方程和概率论，为比较概率分布提供了强大框架，近年来在机器学习中变得尤为重要，尤其是在生成模型的设计和评估方面。

Method: 论文涵盖最优传输的基本数学理论（如Monge和Kantorovich公式、Brenier定理、对偶和动态公式、高斯分布的Bures度量、梯度流），并介绍数值方法（如线性规划、半离散求解器和熵正则化）。

Result: 论文提供了最优传输理论的数学基础和数值方法，并展示了其在机器学习中的具体应用，如神经网络训练、Transformer中的token动态以及GAN和扩散模型的结构。

Conclusion: 最优传输理论是一个重要的数学工具，在机器学习领域尤其是生成模型中具有广泛的应用前景，该论文为其提供了坚实的数学基础和实用指导。

Abstract: Optimal Transport is a foundational mathematical theory that connects
optimization, partial differential equations, and probability. It offers a
powerful framework for comparing probability distributions and has recently
become an important tool in machine learning, especially for designing and
evaluating generative models. These course notes cover the fundamental
mathematical aspects of OT, including the Monge and Kantorovich formulations,
Brenier's theorem, the dual and dynamic formulations, the Bures metric on
Gaussian distributions, and gradient flows. It also introduces numerical
methods such as linear programming, semi-discrete solvers, and entropic
regularization. Applications in machine learning include topics like training
neural networks via gradient flows, token dynamics in transformers, and the
structure of GANs and diffusion models. These notes focus primarily on
mathematical content rather than deep learning techniques.

</details>


### [412] [Feature Representation Transferring to Lightweight Models via Perception Coherence](https://arxiv.org/abs/2505.06595)
*Hai-Vy Nguyen,Fabrice Gamboa,Sixin Zhang,Reda Chhaibi,Serge Gratton,Thierry Giaccone*

Main category: stat.ML

TL;DR: 提出一种从大型教师模型向轻量级学生模型迁移特征表示的方法，通过定义'感知一致性'并设计相应的损失函数，使学生模型能模仿教师模型对输入的感知方式。


<details>
  <summary>Details</summary>
Motivation: 解决学生模型表示能力较弱的问题，目标是开发一种新方法，使学生在不需要完全保留教师模型的绝对几何结构的同时，能保持全局一致性。

Method: 基于'感知一致性'概念设计损失函数，通过数据点在特征空间中的排名差异来优化，使学生模型学习教师模型的输入感知方式。

Result: 实验结果表明，该方法在特征迁移任务上优于或与基线方法性能相当。

Conclusion: 提出的方法通过排名差异优化实现了有效的特征表示迁移，且理论分析为特征迁移过程提供了概率视角。

Abstract: In this paper, we propose a method for transferring feature representation to
lightweight student models from larger teacher models. We mathematically define
a new notion called \textit{perception coherence}. Based on this notion, we
propose a loss function, which takes into account the dissimilarities between
data points in feature space through their ranking. At a high level, by
minimizing this loss function, the student model learns to mimic how the
teacher model \textit{perceives} inputs. More precisely, our method is
motivated by the fact that the representational capacity of the student model
is weaker than the teacher model. Hence, we aim to develop a new method
allowing for a better relaxation. This means that, the student model does not
need to preserve the absolute geometry of the teacher one, while preserving
global coherence through dissimilarity ranking. Our theoretical insights
provide a probabilistic perspective on the process of feature representation
transfer. Our experiments results show that our method outperforms or achieves
on-par performance compared to strong baseline methods for representation
transferring.

</details>


### [413] [Fair Representation Learning for Continuous Sensitive Attributes using Expectation of Integral Probability Metrics](https://arxiv.org/abs/2505.06435)
*Insung Kong,Kunwoong Kim,Yongdai Kim*

Main category: stat.ML

TL;DR: 提出了一种针对连续敏感属性的公平表示学习算法FREM，通过EIPM度量公平性，实验证明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有公平表示学习算法主要针对分类敏感属性，无法处理连续敏感属性（如年龄或收入），因此需要一种新方法。

Method: 引入EIPM度量表示空间的公平性，证明低EIPM值可保证预测头的公平性，并提出基于EIPM和MMD的FREM算法。

Result: 实验表明FREM在连续敏感属性上的公平性表现优于其他基线方法。

Conclusion: FREM算法有效解决了连续敏感属性的公平表示学习问题，为实际应用提供了可行方案。

Abstract: AI fairness, also known as algorithmic fairness, aims to ensure that
algorithms operate without bias or discrimination towards any individual or
group. Among various AI algorithms, the Fair Representation Learning (FRL)
approach has gained significant interest in recent years. However, existing FRL
algorithms have a limitation: they are primarily designed for categorical
sensitive attributes and thus cannot be applied to continuous sensitive
attributes, such as age or income. In this paper, we propose an FRL algorithm
for continuous sensitive attributes. First, we introduce a measure called the
Expectation of Integral Probability Metrics (EIPM) to assess the fairness level
of representation space for continuous sensitive attributes. We demonstrate
that if the distribution of the representation has a low EIPM value, then any
prediction head constructed on the top of the representation become fair,
regardless of the selection of the prediction head. Furthermore, EIPM possesses
a distinguished advantage in that it can be accurately estimated using our
proposed estimator with finite samples. Based on these properties, we propose a
new FRL algorithm called Fair Representation using EIPM with MMD (FREM).
Experimental evidences show that FREM outperforms other baseline methods.

</details>


### [414] [High-Dimensional Importance-Weighted Information Criteria: Theory and Optimality](https://arxiv.org/abs/2505.06531)
*Yong-Syun Cao,Shinpei Imori,Ching-Kang Ing*

Main category: stat.ML

TL;DR: 该论文提出了一种名为IWOGA的算法和HDIWIC准则，用于高维误设回归模型中的模型选择，并理论证明其最优性。


<details>
  <summary>Details</summary>
Motivation: 在高维误设回归模型和协变量偏移情况下，现有的模型选择方法可能无法达到最优性能，因此需要一种新的算法和准则来解决这一问题。

Method: 提出了重要性加权正交贪婪算法（IWOGA）和高维重要性加权信息准则（HDIWIC），并理论分析了其组合使用的有效性。

Result: 理论证明表明，IWOGA + HDIWIC能够在方差和平方偏差之间实现最优平衡，从而获得最优的收敛速度。

Conclusion: IWOGA + HDIWIC在高维误设回归模型的协变量偏移下是一种有效的模型选择方法，具有理论保障的最优性能。

Abstract: Imori and Ing (2025) proposed the importance-weighted orthogonal greedy
algorithm (IWOGA) for model selection in high-dimensional misspecified
regression models under covariate shift. To determine the number of IWOGA
iterations, they introduced the high-dimensional importance-weighted
information criterion (HDIWIC). They argued that the combined use of IWOGA and
HDIWIC, IWOGA + HDIWIC, achieves an optimal trade-off between variance and
squared bias, leading to optimal convergence rates in terms of conditional mean
squared prediction error. In this article, we provide a theoretical
justification for this claim by establishing the optimality of IWOGA + HDIWIC
under a set of reasonable assumptions.

</details>


### [415] [Learning Guarantee of Reward Modeling Using Deep Neural Networks](https://arxiv.org/abs/2505.06601)
*Yuanhang Luo,Yeheng Ge,Ruijian Han,Guohao Shen*

Main category: stat.ML

TL;DR: 本文研究使用深度神经网络对成对比较数据进行奖励建模的学习理论，提出了一种非参数设置下依赖于网络架构的非渐近遗憾边界，并通过边际条件强调了清晰人类偏好的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过奖励建模的非渐近理论分析，探索强化学习从人类反馈中学习（RLHF）的成功机制，尤其是清晰人类偏好在其中的作用。

Method: 采用深度神经网络构建奖励估计器，在非参数设置下分析其性能，并引入边际条件（假设最优动作的胜率显著偏离1/2）以优化遗憾边界。

Result: 理论分析表明，边际条件能显著提升遗憾边界的紧致性，且这一改进独立于具体估计器，适用于多种学习算法和模型。

Conclusion: 清晰的人类偏好（通过高质量成对比较数据体现）是RLHF高效性的关键，边际条件为理论支持提供了通用框架。

Abstract: In this work, we study the learning theory of reward modeling with pairwise
comparison data using deep neural networks. We establish a novel non-asymptotic
regret bound for deep reward estimators in a non-parametric setting, which
depends explicitly on the network architecture. Furthermore, to underscore the
critical importance of clear human beliefs, we introduce a margin-type
condition that assumes the conditional winning probability of the optimal
action in pairwise comparisons is significantly distanced from 1/2. This
condition enables a sharper regret bound, which substantiates the empirical
efficiency of Reinforcement Learning from Human Feedback and highlights clear
human beliefs in its success. Notably, this improvement stems from high-quality
pairwise comparison data implied by the margin-type condition, is independent
of the specific estimators used, and thus applies to various learning
algorithms and models.

</details>


### [416] [Out-of-Sample Embedding with Proximity Data: Projection versus Restricted Reconstruction](https://arxiv.org/abs/2505.06756)
*Michael W. Trosset,Kaiyi Tan,Minh Tang,Carey E. Priebe*

Main category: stat.ML

TL;DR: 论文综述了核方法解决样本外嵌入问题的两种策略：投影和受限重建，并比较了它们的适用场景。


<details>
  <summary>Details</summary>
Motivation: 研究样本外嵌入问题的方法，特别是从核方法的角度出发，探讨如何有效将新数据点嵌入已有向量图中。

Method: 采用两种策略进行样本外嵌入：投影（类比PCA）和受限重建（非线性优化简化为单维搜索）。

Result: 展示了每种策略的核方法实现，并分析了不同情况下选择投影或受限重建的优势。

Conclusion: 样本外嵌入问题可通过投影或受限重建策略解决，具体选择取决于应用场景和需求。

Abstract: The problem of using proximity (similarity or dissimilarity) data for the
purpose of "adding a point to a vector diagram" was first studied by J.C. Gower
in 1968. Since then, a number of methods -- mostly kernel methods -- have been
proposed for solving what has come to be called the problem of *out-of-sample
embedding*. We survey the various kernel methods that we have encountered and
show that each can be derived from one or the other of two competing
strategies: *projection* or *restricted reconstruction*. Projection can be
analogized to a well-known formula for adding a point to a principal component
analysis. Restricted reconstruction poses a different challenge: how to best
approximate redoing the entire multivariate analysis while holding fixed the
vector diagram that was previously obtained. This strategy results in a
nonlinear optimization problem that can be simplified to a unidimensional
search. Various circumstances may warrant either projection or restricted
reconstruction.

</details>


### [417] [Reverse-BSDE Monte Carlo](https://arxiv.org/abs/2505.06800)
*Jairon H. N. Batista,Flávio B. Gonçalves,Yuri F. Saporito,Rodrigo S. Targino*

Main category: stat.ML

TL;DR: 该论文提出了一种新的视角，将扩散模型的“学习”问题转化为“采样”问题，利用FBSDE框架避免预估计目标密度梯度的难题，并通过深度学习技术实现了高效的数值解。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高维逼真图像方面表现出色，但传统方法需预估计目标密度梯度，计算复杂。本文旨在通过FBSDE框架简化这一过程。

Method: 将扩散模型重新表述为FBSDE，利用非标准技术证明解的唯一性，并结合深度学习设计数值解法。

Result: 该方法避免了梯度预估计，为多维分布采样（如贝叶斯统计中常见问题）提供了新途径。

Conclusion: FBSDE框架为扩散模型的采样问题提供了高效且理论严谨的解决方案，拓展了其在复杂分布采样中的应用潜力。

Abstract: Recently, there has been a growing interest in generative models based on
diffusions driven by the empirical robustness of these methods in generating
high-dimensional photorealistic images and the possibility of using the vast
existing toolbox of stochastic differential equations. %This remarkable ability
may stem from their capacity to model and generate multimodal distributions. In
this work, we offer a novel perspective on the approach introduced in Song et
al. (2021), shifting the focus from a "learning" problem to a "sampling"
problem. To achieve this, we reformulate the equations governing
diffusion-based generative models as a Forward-Backward Stochastic Differential
Equation (FBSDE), which avoids the well-known issue of pre-estimating the
gradient of the log target density. The solution of this FBSDE is proved to be
unique using non-standard techniques. Additionally, we propose a numerical
solution to this problem, leveraging on Deep Learning techniques. This
reformulation opens new pathways for sampling multidimensional distributions
with densities known up to a normalization constant, a problem frequently
encountered in Bayesian statistics.

</details>


### [418] [Outperformance Score: A Universal Standardization Method for Confusion-Matrix-Based Classification Performance Metrics](https://arxiv.org/abs/2505.07033)
*Ningsheng Zhao,Trang Bui,Jia Yuan Yu,Krzysztof Dzieciolowski*

Main category: stat.ML

TL;DR: 本文提出了一种通用的标准化方法——超表现分数函数，用于将基于混淆矩阵的分类性能指标映射到统一尺度[0,1]上，解决了不同指标因量纲和类不平衡率差异导致的性能评估难题。


<details>
  <summary>Details</summary>
Motivation: 现有分类性能指标因量纲和类不平衡率敏感度不同，难以在测试集不平衡率变化时进行一致评估。

Method: 引入了超表现分数函数，将指标标准化为[0,1]的百分位排名，实现跨测试集的性能比较。

Result: 实验证明了该方法在多种真实数据集上的鲁棒性和通用性。

Conclusion: 超表现分数为分类性能评估提供了统一且直观的框架。

Abstract: Many classification performance metrics exist, each suited to a specific
application. However, these metrics often differ in scale and can exhibit
varying sensitivity to class imbalance rates in the test set. As a result, it
is difficult to use the nominal values of these metrics to interpret and
evaluate classification performances, especially when imbalance rates vary. To
address this problem, we introduce the outperformance score function, a
universal standardization method for confusion-matrix-based classification
performance (CMBCP) metrics. It maps any given metric to a common scale of
$[0,1]$, while providing a clear and consistent interpretation. Specifically,
the outperformance score represents the percentile rank of the observed
classification performance within a reference distribution of possible
performances. This unified framework enables meaningful comparison and
monitoring of classification performance across test sets with differing
imbalance rates. We illustrate how the outperformance scores can be applied to
a variety of commonly used classification performance metrics and demonstrate
the robustness of our method through experiments on real-world datasets
spanning multiple classification applications.

</details>


### [419] [Learning curves theory for hierarchically compositional data with power-law distributed features](https://arxiv.org/abs/2505.07067)
*Francesco Cagnetta,Hyunmo Kang,Matthieu Wyart*

Main category: stat.ML

TL;DR: 该论文探讨了神经缩放定律的起源，发现分类任务和下一个标记预测任务中幂律分布规则的不同影响。分类任务中幂律分布规则决定了学习曲线的幂律指数，而下一个标记预测任务中规则分布仅影响局部细节。


<details>
  <summary>Details</summary>
Motivation: 旨在统一神经缩放定律的两种理论观点：任务线性分解为幂律分布单位和数据的分层组合结构。

Method: 基于概率上下文无关文法的分类和下一个标记预测任务分析。

Result: 分类任务中幂律分布规则的学习曲线呈现幂律行为，指数由规则分布决定；下一个标记预测任务中规则分布不影响大尺度行为的幂律指数。

Conclusion: 通过文法模型统一了神经缩放定律的两种解释，揭示了不同任务中规则分布影响学习曲线的差异性。

Abstract: Recent theories suggest that Neural Scaling Laws arise whenever the task is
linearly decomposed into power-law distributed units. Alternatively, scaling
laws also emerge when data exhibit a hierarchically compositional structure, as
is thought to occur in language and images. To unify these views, we consider
classification and next-token prediction tasks based on probabilistic
context-free grammars -- probabilistic models that generate data via a
hierarchy of production rules. For classification, we show that having
power-law distributed production rules results in a power-law learning curve
with an exponent depending on the rules' distribution and a large
multiplicative constant that depends on the hierarchical structure. By
contrast, for next-token prediction, the distribution of production rules
controls the local details of the learning curve, but not the exponent
describing the large-scale behaviour.

</details>


### [420] [A Sparse Bayesian Learning Algorithm for Estimation of Interaction Kernels in Motsch-Tadmor Model](https://arxiv.org/abs/2505.07068)
*Jinchao Feng,Sui Tang*

Main category: stat.ML

TL;DR: 本文研究基于观测轨迹数据的Motsch-Tadmor模型中非对称交互核的数据驱动识别，提出变分框架和稀疏贝叶斯学习算法，证明了唯一可识别性，并通过数值实验验证其准确性、鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 为解决Motsch-Tadmor模型中非线性逆问题及其交互核识别的挑战，需一种能处理噪声数据、提供不确定量化并支持模型选择的方法。

Method: 提出变分框架将核识别转为子空间识别问题，并开发稀疏贝叶斯学习算法，结合信息先验进行正则化和不确定性量化。

Result: 理论证明交互核在特定条件下唯一可识别（至尺度变换），数值实验显示方法在不同噪声和数据条件下表现优异。

Conclusion: 所提框架在识别交互核时兼具准确性、鲁棒性和可解释性，为复杂动力系统的数据驱动建模提供了有效工具。

Abstract: In this paper, we investigate the data-driven identification of asymmetric
interaction kernels in the Motsch-Tadmor model based on observed trajectory
data. The model under consideration is governed by a class of semilinear
evolution equations, where the interaction kernel defines a normalized,
state-dependent Laplacian operator that governs collective dynamics. To address
the resulting nonlinear inverse problem, we propose a variational framework
that reformulates kernel identification using the implicit form of the
governing equations, reducing it to a subspace identification problem. We
establish an identifiability result that characterizes conditions under which
the interaction kernel can be uniquely recovered up to scale. To solve the
inverse problem robustly, we develop a sparse Bayesian learning algorithm that
incorporates informative priors for regularization, quantifies uncertainty, and
enables principled model selection. Extensive numerical experiments on
representative interacting particle systems demonstrate the accuracy,
robustness, and interpretability of the proposed framework across a range of
noise levels and data regimes.

</details>


### [421] [Constrained Online Decision-Making with Density Estimation Oracles](https://arxiv.org/abs/2505.07101)
*Haichen Hu,David Simchi-Levi,Navid Azizan*

Main category: stat.ML

TL;DR: 论文提出了一种统一的算法框架，用于解决具有阶段性可行性约束的在线决策问题，涵盖了多种现有约束学习问题。


<details>
  <summary>Details</summary>
Motivation: 解决现实应用中如资源有限的个性化推荐、自适应实验设计等需要满足特定约束的在线决策问题。

Method: 引入了上界反事实置信边界概念，并扩展了eluder维度的定义，以处理复杂环境中的可行性约束。

Result: 提出的算法框架在理论和实践中均提供了强理论保证，适用于多种密度函数类。

Conclusion: 该研究为理论和实践中的约束序列决策提供了原则性基础。

Abstract: Contextual online decision-making problems with constraints appear in a wide
range of real-world applications, such as personalized recommendation with
resource limits, adaptive experimental design, and decision-making under safety
or fairness requirements. In this paper, we investigate a general formulation
of sequential decision-making with stage-wise feasibility constraints, where at
each round, the learner must select an action based on observed context while
ensuring that a problem-specific feasibility criterion is satisfied. We propose
a unified algorithmic framework that captures many existing constrained
learning problems, including constrained bandits, active learning with label
budgets, online hypothesis testing with Type I error control, and model
calibration. Central to our approach is the concept of upper counterfactual
confidence bounds, which enables the design of practically efficient online
algorithms with strong theoretical guarantee using any offline conditional
density estimation oracle. Technically, to handle feasibility constraints in
complex environments, we introduce a generalized notion of the eluder dimension
- extending it from the classical setting based on square loss to a broader
class of metric-like probability divergences. This allows us to capture the
complexity of various density function classes and characterize the utility
regret incurred due to feasibility constraint uncertainty. Our result offers a
principled foundation for constrained sequential decision-making in both theory
and practice.

</details>


### [422] [Adaptive, Robust and Scalable Bayesian Filtering for Online Learning](https://arxiv.org/abs/2505.07267)
*Gerardo Duran-Martin*

Main category: stat.ML

TL;DR: 该论文提出了一种基于贝叶斯滤波的框架，用于解决序列机器学习问题，如在线学习、预测和上下文匪徒问题，并针对非平稳环境、模型误设和高维参数空间等挑战提出了创新工具。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理动态、高维和模型误设的序列学习问题时面临挑战，因此需要一种更适应性强、鲁棒性高且可扩展的框架。

Method: 开发了包括模块化框架、新型鲁棒滤波器和近似二阶优化工具在内的贝叶斯滤波方法。

Result: 理论和实验结果均显示，所提方法在动态、高维和误设模型中表现更优。

Conclusion: 贝叶斯滤波框架在序列学习问题中展现出强大的适应性和鲁棒性，尤其在复杂环境下表现突出。

Abstract: In this thesis, we introduce Bayesian filtering as a principled framework for
tackling diverse sequential machine learning problems, including online
(continual) learning, prequential (one-step-ahead) forecasting, and contextual
bandits. To this end, this thesis addresses key challenges in applying Bayesian
filtering to these problems: adaptivity to non-stationary environments,
robustness to model misspecification and outliers, and scalability to the
high-dimensional parameter space of deep neural networks. We develop novel
tools within the Bayesian filtering framework to address each of these
challenges, including: (i) a modular framework that enables the development
adaptive approaches for online learning; (ii) a novel, provably robust filter
with similar computational cost to standard filters, that employs Generalised
Bayes; and (iii) a set of tools for sequentially updating model parameters
using approximate second-order optimisation methods that exploit the
overparametrisation of high-dimensional parametric models such as neural
networks. Theoretical analysis and empirical results demonstrate the improved
performance of our methods in dynamic, high-dimensional, and misspecified
models.

</details>


### [423] [ALPCAH: Subspace Learning for Sample-wise Heteroscedastic Data](https://arxiv.org/abs/2505.07272)
*Javier Salazar Cavazos,Jeffrey A. Fessler,Laura Balzano*

Main category: stat.ML

TL;DR: 该论文提出了ALPCAH方法，用于处理异方差数据中的低秩子空间学习，无需假设噪声分布或已知噪声方差，并开发了更高效的矩阵分解版本LR-ALPCAH。实验验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 传统PCA无法处理异方差数据（即噪声方差不同的数据），因此需要一种能自动估计噪声方差并改进子空间估计的方法。

Method: ALPCAH通过软秩约束估计样本噪声方差和低秩子空间，无需已知噪声分布或子空间维度；LR-ALPCAH是其矩阵分解版本，效率更高但需已知子空间维度。

Result: 模拟和真实数据实验表明，ALPCAH和LR-ALPCAH在异方差数据中优于现有算法。

Conclusion: ALPCAH系列方法有效解决了异方差数据的子空间学习问题，且灵活适应不同场景需求。

Abstract: Principal component analysis (PCA) is a key tool in the field of data
dimensionality reduction. However, some applications involve heterogeneous data
that vary in quality due to noise characteristics associated with each data
sample. Heteroscedastic methods aim to deal with such mixed data quality. This
paper develops a subspace learning method, named ALPCAH, that can estimate the
sample-wise noise variances and use this information to improve the estimate of
the subspace basis associated with the low-rank structure of the data. Our
method makes no distributional assumptions of the low-rank component and does
not assume that the noise variances are known. Further, this method uses a soft
rank constraint that does not require subspace dimension to be known.
Additionally, this paper develops a matrix factorized version of ALPCAH, named
LR-ALPCAH, that is much faster and more memory efficient at the cost of
requiring subspace dimension to be known or estimated. Simulations and real
data experiments show the effectiveness of accounting for data
heteroscedasticity compared to existing algorithms. Code available at
https://github.com/javiersc1/ALPCAH.

</details>


### [424] [Certified Data Removal Under High-dimensional Settings](https://arxiv.org/abs/2505.07640)
*Haolin Zou,Arnab Auddy,Yongchan Kwon,Kamiar Rahnama Rad,Arian Maleki*

Main category: stat.ML

TL;DR: 论文提出了一种高效的机器遗忘算法，通过理论引导的牛顿步骤和噪声添加，解决了高维环境中数据遗忘的理论与计算挑战，证明两步迭代足以实现有效遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法在低维环境中有效，但在高维度（参数数接近样本量）时缺乏理论保障。论文旨在填补这一空白，确保高维环境下数据完全且高效地从模型中移除。

Method: 算法从原模型参数出发，执行1-2次理论引导的牛顿步骤，随后添加标度化的拉普拉斯噪声以彻底消除残留数据影响，重点解决高维下信号噪声比与模型复杂度的交互问题。

Result: 理论证明单步牛顿迭代不足以实现高维遗忘，但两步迭代可达到可认证的遗忘效果。数值实验验证了方法的认证性（无残留影响）和准确性（模型性能保持）。

Conclusion: 两步牛顿更新加噪声的框架为高维机器遗忘提供了理论支撑和实用工具，弥合了低维与高维的理论差距。

Abstract: Machine unlearning focuses on the computationally efficient removal of
specific training data from trained models, ensuring that the influence of
forgotten data is effectively eliminated without the need for full retraining.
Despite advances in low-dimensional settings, where the number of parameters \(
p \) is much smaller than the sample size \( n \), extending similar
theoretical guarantees to high-dimensional regimes remains challenging. We
propose an unlearning algorithm that starts from the original model parameters
and performs a theory-guided sequence of Newton steps \( T \in \{ 1,2\}\).
After this update, carefully scaled isotropic Laplacian noise is added to the
estimate to ensure that any (potential) residual influence of forget data is
completely removed. We show that when both \( n, p \to \infty \) with a fixed
ratio \( n/p \), significant theoretical and computational obstacles arise due
to the interplay between the complexity of the model and the finite
signal-to-noise ratio. Finally, we show that, unlike in low-dimensional
settings, a single Newton step is insufficient for effective unlearning in
high-dimensional problems -- however, two steps are enough to achieve the
desired certifiebility. We provide numerical experiments to support the
certifiability and accuracy claims of this approach.

</details>


### [425] [Transfer Learning Across Fixed-Income Product Classes](https://arxiv.org/abs/2505.07676)
*Nicolas Camenzind,Damir Filipovic*

Main category: stat.ML

TL;DR: 本文提出了一种跨固定收益产品类别的折现曲线迁移学习框架，通过扩展核岭回归到向量值设置，并引入经济原则驱动的正则化项，显著提升了曲线外推性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于稀疏或噪声数据中估计折现曲线的挑战，旨在通过迁移学习提升不同产品类别间的曲线估计精度。

Method: 方法包括将核岭回归扩展到向量值设置，构建凸优化问题，并在向量值再生核希尔伯特空间中引入经济原则驱动的正则化项。

Result: 结果表明迁移学习显著改善了外推性能，并缩小了置信区间，相比单曲线估计更具优势。

Conclusion: 结论表明所提框架通过经济原则正则化和向量值核方法，有效提升了折现曲线的跨产品类别迁移学习效果。

Abstract: We propose a framework for transfer learning of discount curves across
different fixed-income product classes. Motivated by challenges in estimating
discount curves from sparse or noisy data, we extend kernel ridge regression
(KR) to a vector-valued setting, formulating a convex optimization problem in a
vector-valued reproducing kernel Hilbert space (RKHS). Each component of the
solution corresponds to the discount curve implied by a specific product class.
We introduce an additional regularization term motivated by economic
principles, promoting smoothness of spread curves between product classes, and
show that it leads to a valid separable kernel structure. A main theoretical
contribution is a decomposition of the vector-valued RKHS norm induced by
separable kernels. We further provide a Gaussian process interpretation of
vector-valued KR, enabling quantification of estimation uncertainty.
Illustrative examples demonstrate that transfer learning significantly improves
extrapolation performance and tightens confidence intervals compared to
single-curve estimation.

</details>


### [426] [Analytic theory of dropout regularization](https://arxiv.org/abs/2505.07792)
*Francesco Mori,Francesca Mignacco*

Main category: stat.ML

TL;DR: 该论文研究了在两层神经网络中使用Dropout的技术，通过分析得出其优化训练效果和适应数据噪声的机制，并通过数值模拟验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: Dropout虽然广泛用于缓解过拟合，但其概率选择和理论解释仍然缺乏系统性研究，因此论文旨在填补这一空白。

Method: 在高维极限下，作者使用随机梯度下降训练两层神经网络，并通过一组常微分方程分析Dropout对训练动态的影响。

Result: 论文得出了一系列精确结果，表明Dropout能减少节点间的有害关联、减轻标签噪声的影响，且最优概率随数据噪声增加而提高。

Conclusion: 理论分析和数值实验证实了Dropout的有效性，尤其是在数据噪声较大的情况下，进一步推广了其应用价值。

Abstract: Dropout is a regularization technique widely used in training artificial
neural networks to mitigate overfitting. It consists of dynamically
deactivating subsets of the network during training to promote more robust
representations. Despite its widespread adoption, dropout probabilities are
often selected heuristically, and theoretical explanations of its success
remain sparse. Here, we analytically study dropout in two-layer neural networks
trained with online stochastic gradient descent. In the high-dimensional limit,
we derive a set of ordinary differential equations that fully characterize the
evolution of the network during training and capture the effects of dropout. We
obtain a number of exact results describing the generalization error and the
optimal dropout probability at short, intermediate, and long training times.
Our analysis shows that dropout reduces detrimental correlations between hidden
nodes, mitigates the impact of label noise, and that the optimal dropout
probability increases with the level of noise in the data. Our results are
validated by extensive numerical simulations.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [427] [Circuit Partitioning Using Large Language Models for Quantum Compilation and Simulations](https://arxiv.org/abs/2505.07711)
*Pranav Sinha,Sumit Kumar Jha,Sunny Raj*

Main category: cs.ET

TL;DR: 论文探讨了利用大型语言模型（如Llama和Mistral）分割量子电路的新方法，以解决现有算法因计算限制无法处理大规模电路的问题。


<details>
  <summary>Details</summary>
Motivation: 在NISQ时代，量子计算机受限于噪声门的干扰，现有算法只能处理5-6个量子位的电路，因此需要分割大规模电路以改进量子门的最小化。

Method: 通过微调开源大型语言模型，利用其代码理解和生成能力（如QASM），结合Berkeley Quantum Synthesis Toolkit的快速分割方法，实现电路分割。

Result: 实验表明，经精细微调的模型在分割任务中达到53.4%的准确率，而现成的大型语言模型无法通过标准训练方法正确完成分割。

Conclusion: 研究表明，大型语言模型在量子电路分割任务中具有潜力，但需进一步优化以提高准确率。

Abstract: We are in the midst of the noisy intermediate-scale quantum (NISQ) era, where
quantum computers are limited by noisy gates, some of which are more
error-prone than others and can render the final computation incomprehensible.
Quantum circuit compilation algorithms attempt to minimize these noisy gates
when mapping quantum algorithms onto quantum hardware but face computational
challenges that restrict their application to circuits with no more than 5-6
qubits, necessitating the need to partition large circuits before the
application of noisy quantum gate minimization algorithms. The existing
generation of these algorithms is heuristic in nature and does not account for
downstream gate minimization tasks. Large language models (LLMs) have the
potential to change this and help improve quantum circuit partitions. This
paper investigates the use of LLMs, such as Llama and Mistral, for partitioning
quantum circuits by capitalizing on their abilities to understand and generate
code, including QASM. Specifically, we teach LLMs to partition circuits using
the quick partition approach of the Berkeley Quantum Synthesis Toolkit. Through
experimental evaluations, we show that careful fine-tuning of open source LLMs
enables us to obtain an accuracy of 53.4% for the partition task while
over-the-shelf LLMs are unable to correctly partition circuits, using standard
1-shot and few-shot training approaches.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [428] [Quantum State Preparation via Large-Language-Model-Driven Evolution](https://arxiv.org/abs/2505.06347)
*Qing-Hong Cao,Zong-Yue Hou,Ying-Ying Li,Xiaohui Liu,Zhuo-Yang Song,Liang-Qi Zhang,Shutao Zhang,Ke Zhao*

Main category: quant-ph

TL;DR: 提出自动化框架FunSearch，结合大语言模型与进化优化设计量子电路，解决变分量子算法中的刚性、扩展性及依赖专家问题，展示在实际量子硬件上的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统变分量子算法存在刚性、扩展性不足及依赖专家的问题，需开发自主设计硬件高效ansätze的框架。

Method: 结合大语言模型（LLMs）与进化优化，无需先验知识，自主生成可扩展且参数数量与系统尺寸无关的量子电路。

Result: 在9量子比特Ising和XY自旋链上实现仅含4个参数的电路，能量外推接近精确；在祖冲之芯片上验证，通过零噪声外推有效抑制两量子比特门噪声。

Conclusion: FunSearch填补算法设计与实验约束间的鸿沟，推动可扩展量子模拟的发展，是当前量子架构搜索框架的有力补充。

Abstract: We propose an automated framework for quantum circuit design by integrating
large-language models (LLMs) with evolutionary optimization to overcome the
rigidity, scalability limitations, and expert dependence of traditional ones in
variational quantum algorithms. Our approach (FunSearch) autonomously discovers
hardware-efficient ans\"atze with new features of scalability and
system-size-independent number of variational parameters entirely from scratch.
Demonstrations on the Ising and XY spin chains with n = 9 qubits yield circuits
containing 4 parameters, achieving near-exact energy extrapolation across
system sizes. Implementations on quantum hardware (Zuchongzhi chip) validate
practicality, where two-qubit quantum gate noises can be effectively mitigated
via zero-noise extrapolations for a spin chain system as large as 20 sites.
This framework bridges algorithmic design and experimental constraints,
complementing contemporary quantum architecture search frameworks to advance
scalable quantum simulations.

</details>


### [429] [Quantum Observers: A NISQ Hardware Demonstration of Chaotic State Prediction Using Quantum Echo-state Networks](https://arxiv.org/abs/2505.06799)
*Erik L. Connerty,Ethan N. Evans,Gerasimos Angelatos,Vignesh Narayanan*

Main category: quant-ph

TL;DR: 论文提出了一种新型量子回声状态网络（QESN）设计及实现算法，能够在当前IBM量子硬件噪声环境下运行，并通过经典控制理论分析验证其非线性动态和记忆能力，实验表明QESN在时间序列预测上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在经典计算机上的计算限制，探索量子计算与神经网络的结合，克服当前量子硬件的噪声和退相干问题。

Method: 提出QESN设计及实现算法，结合经典控制理论分析其动态特性，并通过高保真模拟和硬件实验验证。

Result: QESN能在噪声环境下运行，预测长时间序列，性能优于IBM量子处理器的中位T1和T2时间。

Conclusion: QESN在量子硬件上表现出色，为量子计算与神经网络的结合提供了可行的解决方案。

Abstract: Recent advances in artificial intelligence have highlighted the remarkable
capabilities of neural network (NN)-powered systems on classical computers.
However, these systems face significant computational challenges that limit
scalability and efficiency. Quantum computers hold the potential to overcome
these limitations and increase processing power beyond classical systems.
Despite this, integrating quantum computing with NNs remains largely unrealized
due to challenges posed by noise, decoherence, and high error rates in current
quantum hardware. Here, we propose a novel quantum echo-state network (QESN)
design and implementation algorithm that can operate within the presence of
noise on current IBM hardware. We apply classical control-theoretic response
analysis to characterize the QESN, emphasizing its rich nonlinear dynamics and
memory, as well as its ability to be fine-tuned with sparsity and re-uploading
blocks. We validate our approach through a comprehensive demonstration of QESNs
functioning as quantum observers, applied in both high-fidelity simulations and
hardware experiments utilizing data from a prototypical chaotic Lorenz system.
Our results show that the QESN can predict long time-series with persistent
memory, running over 100 times longer than the median T}1 and T2 of the IBM
Marrakesh QPU, achieving state-of-the-art time-series performance on
superconducting hardware.

</details>


### [430] [Equivariant Machine Learning Decoder for 3D Toric Codes](https://arxiv.org/abs/2409.04300)
*Oliver Weissl,Evgenii Egorov*

Main category: quant-ph

TL;DR: 该论文提出了一种基于神经网络的解码方法，用于提高3D环面码中的错误纠正效率，特别关注了等变性和Transformer网络的潜力。


<details>
  <summary>Details</summary>
Motivation: 量子计算中的错误传播速度快且会抵消其理论上的指数级速度优势，因此需要高效的错误纠正方法。

Method: 使用具有等变性的神经网络和Transformer网络，从指数增长的训练空间中的一小部分学习，以提高解码效率。

Result: 提出的方法在不同配置和已有解码方法中表现出色。

Conclusion: 神经网络（尤其是结合等变性和Transformer）在3D环面码的错误纠正中具有潜力，能够高效处理噪声问题。

Abstract: Mitigating errors in computing and communication systems has seen a great
deal of research since the beginning of the widespread use of these
technologies. However, as we develop new methods to do computation or
communication, we also need to reiterate the method used to deal with errors.
Within the field of quantum computing, error correction is getting a lot of
attention since errors can propagate fast and invalidate results, which makes
the theoretical exponential speed increase in computation time, compared to
traditional systems, obsolete. To correct errors in quantum systems,
error-correcting codes are used. A subgroup of codes, topological codes, is
currently the focus of many research papers. Topological codes represent parity
check matrices corresponding to graphs embedded on a $d$-dimensional surface.
For our research, the focus lies on the toric code with a 3D square lattice.
The goal of any decoder is robustness to noise, which can increase with code
size. However, a reasonable decoder performance scales polynomially with
lattice size. As error correction is a time-sensitive operation, we propose a
neural network using an inductive bias: equivariance. This allows the network
to learn from a rather small subset of the exponentially growing training space
of possible inputs. In addition, we investigate how transformer networks can
help in correction. These methods will be compared with various configurations
and previously published methods of decoding errors in the 3D toric code.

</details>


### [431] [Quantum RNNs and LSTMs Through Entangling and Disentangling Power of Unitary Transformations](https://arxiv.org/abs/2505.06774)
*Ammar Daskin*

Main category: quant-ph

TL;DR: 该论文探讨了如何用量子循环神经网络（RNN）及其增强版长短期记忆（LSTM）网络，利用酉变换的纠缠和解纠缠能力来优化量子电路设计。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过量子与经典框架的结合，利用酉变换的纠缠和解纠缠能力来提升量子电路的参数化设计，以适用于更多实际应用场景。

Method: 论文采用量子RNN和LSTM网络，将酉变换的纠缠和解纠缠能力解释为信息的保留与遗忘机制，并将其应用于优化量子电路的训练过程。

Result: 研究成果表明，这种量子-经典框架可以有效指导设计更好的参数化量子电路。

Conclusion: 通过结合量子与经典方法，利用酉变换的纠缠特性优化量子电路设计，为实际应用提供了新的可能性。

Abstract: In this paper, we discuss how quantum recurrent neural networks (RNNs) and
their enhanced version, long short-term memory (LSTM) networks, can be modeled
using the core ideas presented in Ref.[1], where the entangling and
disentangling power of unitary transformations is investigated. In particular,
we interpret entangling and disentangling power as information retention and
forgetting mechanisms in LSTMs. Therefore, entanglement becomes a key component
of the optimization (training) process. We believe that, by leveraging prior
knowledge of the entangling power of unitaries, the proposed quantum-classical
framework can guide and help to design better-parameterized quantum circuits
for various real-world applications.

</details>


### [432] [Unraveling Quantum Environments: Transformer-Assisted Learning in Lindblad Dynamics](https://arxiv.org/abs/2505.06928)
*Chi-Sheng Chen,En-Jui Kuo*

Main category: quant-ph

TL;DR: 论文提出了一种基于Transformer的机器学习框架，用于推断量子系统中的时间依赖性耗散率，无需初始量子态或系统哈密顿量的先验知识。该方法通过时间序列可观测数据有效学习耗散模式，并在多种复杂模型中得到验证。


<details>
  <summary>Details</summary>
Motivation: 理解开放量子系统中的耗散现象对量子技术的发展至关重要。传统方法通常依赖特定假设或复杂建模，本研究的目的是提供一种数据驱动、可扩展的解决方案。

Method: 利用Transformer架构和轻量级特征提取技术，通过时间序列可观测数据（如泡利算符期望值）推断耗散率，适用于从单量子比特到多体相互作用的复杂模型。

Result: 方法能准确重构固定和时变耗散率，且在多种量子模型（如单比特、双比特相互作用及Jaynes-Cummings模型）中验证了有效性。理论分析表明，耗散率可由有限可观测集合唯一确定。

Conclusion: 现代机器学习工具为开放量子系统中未知环境的识别提供了高效、可扩展的数据驱动方案，为量子技术应用开辟了新途径。

Abstract: Understanding dissipation in open quantum systems is crucial for the
development of robust quantum technologies. In this work, we introduce a
Transformer-based machine learning framework to infer time-dependent
dissipation rates in quantum systems governed by the Lindblad master equation.
Our approach uses time series of observable quantities, such as expectation
values of single Pauli operators, as input to learn dissipation profiles
without requiring knowledge of the initial quantum state or even the system
Hamiltonian.
  We demonstrate the effectiveness of our approach on a hierarchy of open
quantum models of increasing complexity, including single-qubit systems with
time-independent or time-dependent jump rates, two-qubit interacting systems
(e.g., Heisenberg and transverse Ising models), and the Jaynes--Cummings model
involving light--matter interaction and cavity loss with time-dependent decay
rates. Our method accurately reconstructs both fixed and time-dependent decay
rates from observable time series. To support this, we prove that under
reasonable assumptions, the jump rates in all these models are uniquely
determined by a finite set of observables, such as qubit and photon
measurements. In practice, we combine Transformer-based architectures with
lightweight feature extraction techniques to efficiently learn these dynamics.
Our results suggest that modern machine learning tools can serve as scalable
and data-driven alternatives for identifying unknown environments in open
quantum systems.

</details>


### [433] [Exact Spin Elimination in Ising Hamiltonians and Energy-Based Machine Learning](https://arxiv.org/abs/2505.07163)
*Natalia G. Berloff*

Main category: quant-ph

TL;DR: 提出了一种精确的自旋消除技术，降低二次和k-local Ising哈密顿量的维度，同时保留基态构型，适用于硬件受限平台，并实现了三大应用进展。


<details>
  <summary>Details</summary>
Motivation: 针对硬件资源受限的平台（经典或量子），通过降低自旋数量但仍保留原始基态构型，提升组合优化和基于能量的机器学习在近端硬件上的扩展能力。

Method: 通过系统性地将每个移除的自旋替换为其邻居间的有效相互作用，无需近似或迭代计算，直接降低总自旋数。

Result: 在三个关键领域取得进展：处理更大规模的Max-Cut问题、减少QAOA整数分解的量子比特需求、提升Hopfield联想记忆的性能。

Conclusion: 自旋消除技术为下一代物理自旋机器提供了新的扩展途径，展示了k-local自旋哈密顿量在经典计算替代方案中的潜力。

Abstract: We present an exact spin-elimination technique that reduces the
dimensionality of both quadratic and k-local Ising Hamiltonians while
preserving their original ground-state configurations. By systematically
replacing each removed spin with an effective interaction among its neighbors,
our method lowers the total spin count without invoking approximations or
iterative recalculations. This capability is especially beneficial for
hardware-constrained platforms, classical or quantum, that can directly
implement multi-body interactions but have limited qubit or spin resources. We
demonstrate three key advances enabled by this technique. First, we handle
larger instances of benchmark problems such as Max-Cut on cubic graphs without
exceeding a 2-local interaction limit. Second, we reduce qubit requirements in
QAOA-based integer factorization on near-term quantum devices, thus extending
the feasible range of integers to be factorized. Third, we improve memory
capacity in Hopfield associative memories and enhance memory retrieval by
suppressing spurious attractors, enhancing retrieval performance. Our
spin-elimination procedure trades local spin complexity for higher-order
couplings or higher node degrees in a single pass, opening new avenues for
scaling up combinatorial optimization and energy-based machine learning on
near-term hardware. Finally, these results underscore that the next-generation
physical spin machines will likely capitalize on k-local spin Hamiltonians to
offer an alternative to classical computations.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [434] [AKD : Adversarial Knowledge Distillation For Large Language Models Alignment on Coding tasks](https://arxiv.org/abs/2505.06267)
*Ilyas Oulkadda,Julien Perez*

Main category: cs.SE

TL;DR: 论文提出对抗性知识蒸馏（AKD），通过生成对抗性合成数据集，将大型Code-LLM的能力蒸馏到更小、高效的模型中，以解决模型扩展收益下降和高质量训练数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 随着Code-LLM的广泛应用，代码生成的质量、安全性和可靠性成为关键问题，同时模型扩展的收益递减和高质量训练数据稀缺限制了其进一步发展。

Method: 采用对抗性知识蒸馏（AKD），利用对抗性生成的合成数据集对模型进行系统性压力测试和优化，将大型模型的能力蒸馏到小型模型中。

Result: AKD提升了模型的鲁棒性、可靠性和安全性，同时提高了参数效率，为在现有数据和成本限制下实现可靠的自动化代码生成提供了框架。

Conclusion: 该工作是确保Code-LLM在数据有限和成本效率限制下可靠生成代码的重要一步。

Abstract: The widespread adoption of Large Language Models (LLMs) for code generation,
exemplified by GitHub Copilot\footnote{A coding extension powered by a Code-LLM
to assist in code completion tasks} surpassing a million users, highlights the
transformative potential of these tools in improving developer productivity.
However, this rapid growth also underscores critical concerns regarding the
quality, safety, and reliability of the code they generate. As Code-LLMs
evolve, they face significant challenges, including the diminishing returns of
model scaling and the scarcity of new, high-quality training data. To address
these issues, this paper introduces Adversarial Knowledge Distillation (AKD), a
novel approach that leverages adversarially generated synthetic datasets to
distill the capabilities of larger models into smaller, more efficient ones. By
systematically stress-testing and refining the reasoning capabilities of
Code-LLMs, AKD provides a framework for enhancing model robustness,
reliability, and security while improving their parameter-efficiency. We
believe this work represents a critical step toward ensuring dependable
automated code generation within the constraints of existing data and the
cost-efficiency of model execution.

</details>


### [435] [Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding](https://arxiv.org/abs/2505.07768)
*Yifeng Di,Tianyi Zhang*

Main category: cs.SE

TL;DR: 该论文提出了一种交互式方法，通过代码注释作为开发者和大型语言模型之间建立共识的媒介，显著提升了代码生成的准确性和开发者的信任度。


<details>
  <summary>Details</summary>
Motivation: 由于LLM生成的代码在复杂任务中常存在功能性错误，开发者难以检查和修复这些错误，影响了生产力和对LLM的信任，因此需要一种更好的协作方式。

Method: 采用交互式方法，通过交替生成代码、内联注释和用户反馈，逐步调整代码以符合开发者意图。

Result: 实验表明，该方法在HumanEval基准上使code-davinci-002的pass@1提升了17.1%；用户研究中，任务完成速度提升16.7%，成功率提高10.5%。

Conclusion: 通过交互式注释优化，开发者与LLM能更高效地达成共识，实现更准确的代码生成并提升信任度。

Abstract: Large Language Models (LLMs) have demonstrated unprecedented capability in
code generation. However, LLM-generated code is still plagued with a wide range
of functional errors, especially for complex programming tasks that LLMs have
not seen before. Recent studies have shown that developers often struggle with
inspecting and fixing incorrect code generated by LLMs, diminishing their
productivity and trust in LLM-based code generation. Inspired by the mutual
grounding theory in communication, we propose an interactive approach that
leverages code comments as a medium for developers and LLMs to establish a
shared understanding. Our approach facilitates iterative grounding by
interleaving code generation, inline comment generation, and contextualized
user feedback through editable comments to align generated code with developer
intent. We evaluated our approach on two popular benchmarks and demonstrated
that our approach significantly improved multiple state-of-the-art LLMs, e.g.,
17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we
conducted a user study with 12 participants in comparison to two baselines: (1)
interacting with GitHub Copilot, and (2) interacting with a multi-step code
generation paradigm called Multi-Turn Program Synthesis. Participants completed
the given programming tasks 16.7% faster and with 10.5% improvement in task
success rate when using our approach. Both results show that interactively
refining code comments enables the collaborative establishment of mutual
grounding, leading to more accurate code generation and higher developer
confidence.

</details>


### [436] [Synthetic Code Surgery: Repairing Bugs and Vulnerabilities with LLMs and Synthetic Data](https://arxiv.org/abs/2505.07372)
*David de-Fitero-Dominguez,Antonio Garcia-Cabot,Eva Garcia-Lopez*

Main category: cs.SE

TL;DR: 本文提出了一种利用大型语言模型（LLM）生成合成数据以增强自动化程序修复（APR）的新方法，解决了现有APR系统因高质量训练数据不足而受限的问题。


<details>
  <summary>Details</summary>
Motivation: 当前APR系统受限于跨多种编程语言和错误类型的高质量训练数据的稀缺性，影响了其性能和泛化能力。

Method: 研究方法分为两阶段：首先生成合成样本，随后进行严格的质量评估。采用多种先进LLM生成了约30,000对跨12种编程语言和13种错误类别的错误和修复代码样本，并通过五项标准（正确性、代码质量、安全性、性能、完整性）进行跨模型评估。

Result: 在VulRepair测试集上的实验表明，质量过滤后的合成数据集在Perfect Prediction率上显著优于基线和真实提交数据配置，部分场景表现更优。最佳配置在较低计算强度下超越了现有系统。

Conclusion: 本研究建立了一种自举范式，通过LLM生成并评估自身训练数据，为软件工程任务中的数据稀缺问题提供了新思路，推动了自动化代码维护工具的鲁棒性和适应性发展。

Abstract: This paper presents a novel methodology for enhancing Automated Program
Repair (APR) through synthetic data generation utilizing Large Language Models
(LLMs). Current APR systems are constrained by the limited availability of
high-quality training data encompassing diverse bug types across multiple
programming languages. The proposed approach addresses this limitation through
a two-phase process: a synthetic sample generation followed by a rigorous
quality assessment. Multiple state-of-the-art LLMs were employed to generate
approximately 30,000 paired examples of buggy and fixed code across 12
programming languages and 13 bug categories. Subsequently, these samples
underwent cross-model evaluation against five criteria: correctness, code
quality, security, performance, and completeness. Experimental evaluation on
the VulRepair test set dataset showed statistically significant improvements in
Perfect Prediction rates, with the quality-filtered synthetic dataset
outperforming both baseline and real-world commit data configurations in
certain scenarios. The methodology was validated through rigorous statistical
testing, including ANOVA and post-hoc Tukey's Honest Significant Difference
analysis. Furthermore, the best-performing configurations surpassed existing
systems despite using a less computationally intensive decoding strategy. This
research establishes a self-bootstrapping paradigm in which LLMs generate and
evaluate their own training data, potentially transforming approaches to data
scarcity across software engineering tasks and advancing the development of
robust, adaptable tools for automated code maintenance.

</details>


### [437] [Towards Requirements Engineering for RAG Systems](https://arxiv.org/abs/2505.07553)
*Tor Sporsem,Rasmus Ulfsnes*

Main category: cs.SE

TL;DR: 论文探讨了海事公司如何开发和集成大型语言模型（LLM），重点分析了在专家环境中检索增强生成（RAG）系统的需求工程问题，并通过案例研究揭示了数据科学家在用户期望与生成输出正确性之间的挑战。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决数据科学家在复杂领域中实施RAG系统时，如何平衡用户对AI完美表现的期望与实际生成内容的正确性之间的矛盾。

Method: 通过海事服务提供商的案例研究，采用迭代实验与用户合作的方式，识别上下文特定的“检索需求”，并建立经验性过程模型。

Result: 研究发现数据科学家需通过用户合作确定“检索需求”，并管理系统局限性，为复杂域特定应用中的RAG系统需求工程提供了新见解。

Conclusion: 研究为软件工程知识贡献了专门化的需求工程过程，特别是在复杂域特定应用中实施RAG系统的实践指导。

Abstract: This short paper explores how a maritime company develops and integrates
large-language models (LLM). Specifically by looking at the requirements
engineering for Retrieval Augmented Generation (RAG) systems in expert
settings. Through a case study at a maritime service provider, we demonstrate
how data scientists face a fundamental tension between user expectations of AI
perfection and the correctness of the generated outputs. Our findings reveal
that data scientists must identify context-specific "retrieval requirements"
through iterative experimentation together with users because they are the ones
who can determine correctness. We present an empirical process model describing
how data scientists practically elicited these "retrieval requirements" and
managed system limitations. This work advances software engineering knowledge
by providing insights into the specialized requirements engineering processes
for implementing RAG systems in complex domain-specific applications.

</details>


### [438] [A Case Study Investigating the Role of Generative AI in Quality Evaluations of Epics in Agile Software Development](https://arxiv.org/abs/2505.07664)
*Werner Geyer,Jessica He,Daita Sarkar,Michelle Brachman,Chris Hammond,Jennifer Heins,Zahra Ashktorab,Carlos Rosemberg,Charlie Hill*

Main category: cs.SE

TL;DR: LLMs可以评估敏捷epic质量，帮助产品经理改进需求定义，用户满意度高，但也存在挑战和采纳障碍。


<details>
  <summary>Details</summary>
Motivation: 敏捷epic定义不佳会导致效率低下和成本超支，研究如何利用LLMs提升其质量。

Method: 在全球公司中进行行业案例研究，通过17名产品经理的用户研究评估LLM的应用效果。

Result: LLM评估可集成到产品经理的工作中，满意度高，但同时也揭示了挑战和采纳障碍。

Conclusion: 敏捷epic是LLM评估的新应用方向，研究结果为未来实践和研究提供了参考。

Abstract: The broad availability of generative AI offers new opportunities to support
various work domains, including agile software development. Agile epics are a
key artifact for product managers to communicate requirements to stakeholders.
However, in practice, they are often poorly defined, leading to churn, delivery
delays, and cost overruns. In this industry case study, we investigate
opportunities for large language models (LLMs) to evaluate agile epic quality
in a global company. Results from a user study with 17 product managers
indicate how LLM evaluations could be integrated into their work practices,
including perceived values and usage in improving their epics. High levels of
satisfaction indicate that agile epics are a new, viable application of AI
evaluations. However, our findings also outline challenges, limitations, and
adoption barriers that can inform both practitioners and researchers on the
integration of such evaluations into future agile work practices.

</details>


### [439] [Linux Kernel Configurations at Scale: A Dataset for Performance and Evolution Analysis](https://arxiv.org/abs/2505.07487)
*Heraldo Borges,Juliana Alves Pereira,Djamel Eddine Khelladi,Mathieu Acher*

Main category: cs.SE

TL;DR: 论文提出了一个名为LinuxData的大规模数据集，涵盖多个Linux内核版本（4.13到5.8）的配置，包含超过24万条配置记录，用于研究配置选项的演变及其影响。


<details>
  <summary>Details</summary>
Motivation: Linux内核配置极其复杂且版本迭代快，现有研究缺乏多版本、大规模的定量数据集，因此需要构建一个全面的数据集以支持相关研究。

Method: 通过自动化工具和构建流程收集多个内核版本的配置数据，记录编译结果和二进制大小，并将数据集公开在OpenML平台上。

Result: LinuxData数据集包含24万条配置记录，支持特征选择、机器学习预测模型和跨版本迁移学习等研究。

Conclusion: LinuxData数据集将提升研究的可重复性，并为分析Linux内核的配置空间提供新视角，推动对其可配置性和演变的理解。

Abstract: Configuring the Linux kernel to meet specific requirements, such as binary
size, is highly challenging due to its immense complexity-with over 15,000
interdependent options evolving rapidly across different versions. Although
several studies have explored sampling strategies and machine learning methods
to understand and predict the impact of configuration options, the literature
still lacks a comprehensive and large-scale dataset encompassing multiple
kernel versions along with detailed quantitative measurements. To bridge this
gap, we introduce LinuxData, an accessible collection of kernel configurations
spanning several kernel releases, specifically from versions 4.13 to 5.8. This
dataset, gathered through automated tools and build processes, comprises over
240,000 kernel configurations systematically labeled with compilation outcomes
and binary sizes. By providing detailed records of configuration evolution and
capturing the intricate interplay among kernel options, our dataset enables
innovative research in feature subset selection, prediction models based on
machine learning, and transfer learning across kernel versions. Throughout this
paper, we describe how the dataset has been made easily accessible via OpenML
and illustrate how it can be leveraged using only a few lines of Python code to
evaluate AI-based techniques, such as supervised machine learning. We
anticipate that this dataset will significantly enhance reproducibility and
foster new insights into configuration-space analysis at a scale that presents
unique opportunities and inherent challenges, thereby advancing our
understanding of the Linux kernel's configurability and evolution.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [440] [Direct Data Driven Control Using Noisy Measurements](https://arxiv.org/abs/2505.06407)
*Ramin Esmzad,Gokul S. Sankar,Teawon Han,Hamidreza Modares*

Main category: eess.SY

TL;DR: 该论文提出了一种新颖的直接数据驱动控制框架，用于在扰动和噪声状态下解决线性二次调节器（LQR）问题，无需系统辨识。通过凸优化技术，结合噪声统计直接合成控制器，保证了均方稳定性和最优性能。


<details>
  <summary>Details</summary>
Motivation: 在系统动态未知且存在噪声测量的情况下，传统方法需要系统辨识，但复杂且不实用。本文目标是绕过系统辨识，直接从噪声输入输出数据中学习LQR解，提供更鲁棒和实用的控制器设计方法。

Method: 利用噪声统计信息，通过线性矩阵不等式（LMI）建立鲁棒稳定性条件，并使用半定规划（SDP）公式化数据驱动LQR问题，计算最优控制增益。

Result: 仿真实验（如旋转倒立摆和主动悬架系统）表明，该方法在鲁棒性和准确性上优于现有数据驱动LQR方法。

Conclusion: 该框架为噪声污染环境中无法进行系统辨识的控制器设计提供了理论支持且实用的解决方案。

Abstract: This paper presents a novel direct data-driven control framework for solving
the linear quadratic regulator (LQR) under disturbances and noisy state
measurements. The system dynamics are assumed unknown, and the LQR solution is
learned using only a single trajectory of noisy input-output data while
bypassing system identification. Our approach guarantees mean-square stability
(MSS) and optimal performance by leveraging convex optimization techniques that
incorporate noise statistics directly into the controller synthesis. First, we
establish a theoretical result showing that the MSS of an uncertain data-driven
system implies the MSS of the true closed-loop system. Building on this, we
develop a robust stability condition using linear matrix inequalities (LMIs)
that yields a stabilizing controller gain from noisy measurements. Finally, we
formulate a data-driven LQR problem as a semidefinite program (SDP) that
computes an optimal gain, minimizing the steady-state covariance. Extensive
simulations on benchmark systems -- including a rotary inverted pendulum and an
active suspension system -- demonstrate the superior robustness and accuracy of
our method compared to existing data-driven LQR approaches. The proposed
framework offers a practical and theoretically grounded solution for controller
design in noise-corrupted environments where system identification is
infeasible.

</details>


### [441] [YANNs: Y-wise Affine Neural Networks for Exact and Efficient Representations of Piecewise Linear Functions](https://arxiv.org/abs/2505.07054)
*Austin Braniff,Yuhe Tian*

Main category: eess.SY

TL;DR: YANNs是一种可解释的神经网络架构，无需训练即可高效表示分段仿射函数，保持数学性质，并在模型预测控制中提供理论最优解。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络在控制律近似中存在局限性，YANNs旨在提供一种精确表示控制律的方法，同时保证控制理论的可行性和稳定性。

Method: 采用多参数模型预测控制，通过YANNs直接表示控制律为状态、输出等的分段仿射函数。

Result: YANNs在实时计算中速度显著优于传统方法，并通过数值案例验证了其算法扩展性。

Conclusion: YANNs是首个能同时保证可行性和稳定性的神经网络控制器，为数据驱动建模/控制提供了高效且可解释的基础。

Abstract: This work formally introduces Y-wise Affine Neural Networks (YANNs), a
fully-explainable network architecture that continuously and efficiently
represent piecewise affine functions with polytopic subdomains. Following from
the proofs, it is shown that the development of YANNs requires no training to
achieve the functionally equivalent representation. YANNs thus maintain all
mathematical properties of the original formulations. Multi-parametric model
predictive control is utilized as an application showcase of YANNs, which
theoretically computes optimal control laws as a piecewise affine function of
states, outputs, setpoints, and disturbances. With the exact representation of
multi-parametric control laws, YANNs retain essential control-theoretic
guarantees such as recursive feasibility and stability. This sets YANNs apart
from the existing works which apply neural networks for approximating optimal
control laws instead of exactly representing them. By optimizing the inference
speed of the networks, YANNs can evaluate substantially faster in real-time
compared to traditional piecewise affine function calculations. Numerical case
studies are presented to demonstrate the algorithmic scalability with respect
to the input/output dimensions and the number of subdomains. YANNs represent a
significant advancement in control as the first neural network-based controller
that inherently ensures both feasibility and stability. Future applications can
leverage them as an efficient and interpretable starting point for data-driven
modeling/control.

</details>


### [442] [Finite-Sample-Based Reachability for Safe Control with Gaussian Process Dynamics](https://arxiv.org/abs/2505.07594)
*Manish Prajapat,Johannes Köhler,Amon Lahr,Andreas Krause,Melanie N. Zeilinger*

Main category: eess.SY

TL;DR: 文章提出了一种基于采样的高斯过程模型预测控制（GP-MPC）框架，解决了现有方法因近似而缺乏保证或过于保守的问题，同时确保了递归可行性、闭环安全性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯过程（GP）的模型预测控制方法要么依赖近似而缺乏理论保证，要么过于保守，限制了实际应用。本文旨在填补这一空白。

Method: 提出了一个基于采样的框架，通过有限数量的GP后验动态函数采样有效传播模型认知不确定性，避免保守性，并设计了递归可行的采样型GP-MPC方案。

Result: 建立了新的样本复杂度结果，证明了所提方法能构造高概率的可达集，并在两个数值算例中展示了准确的近似和安全的闭环性能。

Conclusion: 该方法在确保安全性和稳定性的同时，显著提升了GP-MPC的实用性和效率。

Abstract: Gaussian Process (GP) regression is shown to be effective for learning
unknown dynamics, enabling efficient and safety-aware control strategies across
diverse applications. However, existing GP-based model predictive control
(GP-MPC) methods either rely on approximations, thus lacking guarantees, or are
overly conservative, which limits their practical utility. To close this gap,
we present a sampling-based framework that efficiently propagates the model's
epistemic uncertainty while avoiding conservatism. We establish a novel sample
complexity result that enables the construction of a reachable set using a
finite number of dynamics functions sampled from the GP posterior. Building on
this, we design a sampling-based GP-MPC scheme that is recursively feasible and
guarantees closed-loop safety and stability with high probability. Finally, we
showcase the effectiveness of our method on two numerical examples,
highlighting accurate reachable set over-approximation and safe closed-loop
performance.

</details>


### [443] [Multi-Objective Reinforcement Learning for Energy-Efficient Industrial Control](https://arxiv.org/abs/2505.07607)
*Georg Schäfer,Raphael Seliger,Jakob Rehrl,Stefan Huber,Simon Hirlaender*

Main category: eess.SY

TL;DR: 论文提出了一种多目标强化学习框架，用于Quanser Aero 2测试台的能量高效控制，通过设计复合奖励函数来平衡跟踪误差和能耗，并初步探索了能量惩罚权重的影响。


<details>
  <summary>Details</summary>
Motivation: 随着工业自动化对能源高效控制策略的需求增长，研究旨在平衡性能与环境及成本约束，实现高效控制。

Method: 采用多目标强化学习框架，设计了同时惩罚跟踪误差和电力消耗的复合奖励函数，并调整能量惩罚权重α以探索性能与能耗的权衡。

Result: 实验结果显示，在α值为0.0至0.25间性能变化显著，低α值下出现非帕累托最优解，可能与Adam优化器的适应性行为有关。

Conclusion: 未来工作将集中于通过高斯过程建模自动选择α值，并将方法从仿真过渡到实际应用。

Abstract: Industrial automation increasingly demands energy-efficient control
strategies to balance performance with environmental and cost constraints. In
this work, we present a multi-objective reinforcement learning (MORL) framework
for energy-efficient control of the Quanser Aero 2 testbed in its
one-degree-of-freedom configuration. We design a composite reward function that
simultaneously penalizes tracking error and electrical power consumption.
Preliminary experiments explore the influence of varying the Energy penalty
weight, alpha, on the trade-off between pitch tracking and energy savings. Our
results reveal a marked performance shift for alpha values between 0.0 and
0.25, with non-Pareto optimal solutions emerging at lower alpha values, on both
the simulation and the real system. We hypothesize that these effects may be
attributed to artifacts introduced by the adaptive behavior of the Adam
optimizer, which could bias the learning process and favor bang-bang control
strategies. Future work will focus on automating alpha selection through
Gaussian Process-based Pareto front modeling and transitioning the approach
from simulation to real-world deployment.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [444] [Piloting Structure-Based Drug Design via Modality-Specific Optimal Schedule](https://arxiv.org/abs/2505.07286)
*Keyue Qiu,Yuxuan Song,Zhehuan Fan,Peidong Liu,Zhe Zhang,Mingyue Zheng,Hao Zhou,Wei-Ying Ma*

Main category: q-bio.BM

TL;DR: 该论文提出了一种名为VOS的策略，通过优化变分下界（VLB）来解决分子几何建模中的概率路径扭曲问题，显著提升了分子几何结构和相互作用建模的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度生成模型在处理分子几何结构的多模态（连续3D位置和离散2D拓扑）概率路径扭曲问题上存在瓶颈，影响了药物设计的效果。

Method: 论文提出VLB-Optimal Scheduling（VOS）策略，通过优化VLB作为路径积分来解决扭曲概率路径问题，提升分子几何建模。

Result: 在CrossDock数据集上，PoseBusters通过率达到了95.9%，比基线提高了10%以上，同时保持了高亲和力和分子内有效性。

Conclusion: VOS策略有效解决了分子几何建模中的概率路径扭曲问题，显著提升了药物设计的效果。

Abstract: Structure-Based Drug Design (SBDD) is crucial for identifying bioactive
molecules. Recent deep generative models are faced with challenges in geometric
structure modeling. A major bottleneck lies in the twisted probability path of
multi-modalities -- continuous 3D positions and discrete 2D topologies -- which
jointly determine molecular geometries. By establishing the fact that noise
schedules decide the Variational Lower Bound (VLB) for the twisted probability
path, we propose VLB-Optimal Scheduling (VOS) strategy in this under-explored
area, which optimizes VLB as a path integral for SBDD. Our model effectively
enhances molecular geometries and interaction modeling, achieving
state-of-the-art PoseBusters passing rate of 95.9% on CrossDock, more than 10%
improvement upon strong baselines, while maintaining high affinities and robust
intramolecular validity evaluated on held-out test set.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [445] [Training neural control variates using correlated configurations](https://arxiv.org/abs/2505.07719)
*Hyunwoo Oh*

Main category: hep-lat

TL;DR: 摘要分析了神经网络控制变量（NCV）在蒙特卡洛模拟中的方差减少作用，特别探讨了使用马尔可夫链蒙特卡洛（MCMC）生成的**自相关样本**对NCV训练的潜在益处。


<details>
  <summary>Details</summary>
Motivation: 传统控制变量在高维问题中难以构建，NCV通过神经网络学习与目标观测相关的辅助函数以减少方差，但MCMC生成的自相关样本常被忽略，可能蕴含有助于训练的概率分布结构信息。

Method: 系统研究了自相关样本在NCV训练中的作用，包括理论分析和数值实验（以$U(1)$规范理论和标量场理论为例）。

Result: 使用自相关数据训练能提升控制变量性能，尤其在计算资源有限时。实验结果展示了自相关样本如何优化NCV构建。

Conclusion: 研究为高效利用MCMC数据训练神经网络提供了实用指导，强调了自相关样本在特定场景下的价值。

Abstract: Neural control variates (NCVs) have emerged as a powerful tool for variance
reduction in Monte Carlo (MC) simulations, particularly in high-dimensional
problems where traditional control variates are difficult to construct
analytically. By training neural networks to learn auxiliary functions
correlated with the target observable, NCVs can significantly reduce estimator
variance while preserving unbiasedness. However, a critical but often
overlooked aspect of NCV training is the role of autocorrelated samples
generated by Markov Chain Monte Carlo (MCMC). While such samples are typically
discarded for error estimation due to their statistical redundancy, they may
contain useful information about the structure of the underlying probability
distribution that can benefit the training process. In this work, we
systematically examine the effect of using correlated configurations in
training neural control variates. We demonstrate, both conceptually and
numerically, that training on correlated data can improve control variate
performance, especially in settings with limited computational resources. Our
analysis includes empirical results from $U(1)$ gauge theory and scalar field
theory, illustrating when and how autocorrelated samples enhance NCV
construction. These findings provide practical guidance for the efficient use
of MCMC data in training neural networks.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [446] [Hand-Shadow Poser](https://arxiv.org/abs/2505.07012)
*Hao Xu,Yinqiao Wang,Niloy J. Mitra,Shuaicheng Liu,Pheng-Ann Heng,Chi-Wing Fu*

Main category: cs.CG

TL;DR: 该研究提出了一种名为Hand-Shadow Poser的三阶段管道，用于解决从目标形状生成双手姿势以投影匹配输入形状的反向问题，实验表明该方法在85%的案例中有效。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决从影子形状反向推导双手姿势的复杂问题，挑战在于巨大的3D手部姿势设计空间和解剖学限制。

Method: 方法包括三个阶段：生成手部分配模块、广义手影对齐模块和影子特征感知优化模块，无需专门训练数据，基于公开手部数据进行训练。

Result: 通过构建210个多样化影子形状的基准测试和多指标评估，包括基于DINOv2的新评价指标，方法在85%的案例中成功生成双手姿势。

Conclusion: 结论是该方法能有效生成多样化的双手姿势，适用于广泛的手影形状，且无需专用训练数据，具有实际应用潜力。

Abstract: Hand shadow art is a captivating art form, creatively using hand shadows to
reproduce expressive shapes on the wall. In this work, we study an inverse
problem: given a target shape, find the poses of left and right hands that
together best produce a shadow resembling the input. This problem is
nontrivial, since the design space of 3D hand poses is huge while being
restrictive due to anatomical constraints. Also, we need to attend to the
input's shape and crucial features, though the input is colorless and
textureless. To meet these challenges, we design Hand-Shadow Poser, a
three-stage pipeline, to decouple the anatomical constraints (by hand) and
semantic constraints (by shadow shape): (i) a generative hand assignment module
to explore diverse but reasonable left/right-hand shape hypotheses; (ii) a
generalized hand-shadow alignment module to infer coarse hand poses with a
similarity-driven strategy for selecting hypotheses; and (iii) a
shadow-feature-aware refinement module to optimize the hand poses for physical
plausibility and shadow feature preservation. Further, we design our pipeline
to be trainable on generic public hand data, thus avoiding the need for any
specialized training dataset. For method validation, we build a benchmark of
210 diverse shadow shapes of varying complexity and a comprehensive set of
metrics, including a novel DINOv2-based evaluation metric. Through extensive
comparisons with multiple baselines and user studies, our approach is
demonstrated to effectively generate bimanual hand poses for a large variety of
hand shapes for over 85% of the benchmark cases.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [447] [A class of distributed automata that contains the modal mu-fragment](https://arxiv.org/abs/2505.07816)
*Veeti Ahvonen,Damian Heiman,Antti Kuusisto*

Main category: cs.LO

TL;DR: 该论文将分级模态μ-演算的μ-片段翻译为一类分布式消息传递自动机，并以此作为推论，为已有定理提供了另一种证明。


<details>
  <summary>Details</summary>
Motivation: 研究分级模态μ-演算与分布式自动机之间的联系，并通过转换方法为已有结论提供新的证明途径。

Method: 通过翻译方法将分级模态μ-演算的μ-片段映射到分布式消息传递自动机。

Result: 证实了递归图神经网络与分级模态替换演算在MSO逻辑限制下具有相同表达能力的定理。

Conclusion: 论文通过翻译方法验证了模态μ-演算与分布式自动机的联系，并支持了图神经网络表达能力的理论结果。

Abstract: This paper gives a translation from the $\mu$-fragment of the graded modal
$\mu$-calculus to a class of distributed message-passing automata. As a
corollary, we obtain an alternative proof for a theorem from
\cite{ahvonen_neurips} stating that recurrent graph neural networks working
with reals and graded modal substitution calculus have the same expressive
power in restriction to the logic monadic second-order logic MSO.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [448] [Responsibility Gap in Collective Decision Making](https://arxiv.org/abs/2505.06312)
*Pavel Naumov,Jia Tao*

Main category: cs.GT

TL;DR: 论文提出‘选举独裁’概念，证明在完美信息环境下责任空缺仅在该机制下消失，并在不完美信息下给出结论。


<details>
  <summary>Details</summary>
Motivation: 研究集体决策机制中的责任空缺问题，旨在最小化这一现象。

Method: 引入‘选举独裁’概念，分别在完美与不完美信息环境下进行分析与证明。

Result: 在完美信息下，责任空缺消失当且仅当采用选举独裁机制；不完美信息下，无空缺机制类位于两种选举独裁变体之间。

Conclusion: 选举独裁机制是消除责任空缺的关键，但其在不完美信息下的适用性存在限制。

Abstract: The responsibility gap is a set of outcomes of a collective decision-making
mechanism in which no single agent is individually responsible. In general,
when designing a decision-making process, it is desirable to minimise the gap.
  The paper proposes a concept of an elected dictatorship. It shows that, in a
perfect information setting, the gap is empty if and only if the mechanism is
an elected dictatorship. It also proves that in an imperfect information
setting, the class of gap-free mechanisms is positioned strictly between two
variations of the class of elected dictatorships.

</details>


### [449] [Bi-LSTM based Multi-Agent DRL with Computation-aware Pruning for Agent Twins Migration in Vehicular Embodied AI Networks](https://arxiv.org/abs/2505.06378)
*Yuxiang Wei,Zhuoqi Zeng,Yue Zhong,Jiawen Kang,Ryan Wen Liu,M. Shamim Hossain*

Main category: cs.GT

TL;DR: 提出了一种名为VEANs的车载嵌入式AI网络，通过Stackelberg博弈优化带宽资源分配，设计了TMABLPPO算法实现去中心化协调，并采用基于PX的个性化神经网络剪枝算法动态适应异构AV计算能力，显著提升了系统负载平衡和延迟最小化。


<details>
  <summary>Details</summary>
Motivation: 智能交通场景中，车载嵌入式AI代理的计算延迟和资源限制需要任务迁移到RSU，传统方法存在RSU负载不均衡问题，需优化资源分配以提升效率。

Method: 1. 将AV-RSU交互建模为Stackelberg博弈；2. 设计TMABLPPO算法近似均衡解；3. 提出基于PX的个性化神经网络剪枝算法。

Result: 实验验证算法在系统负载平衡和延迟最小化方面表现优异，显著提升了车载嵌入式AI代理的部署效果。

Conclusion: 该方法有效解决了车载嵌入式AI网络中的资源分配和计算效率问题，为智能交通系统提供了可靠的技术支持。

Abstract: With the advancement of large language models and embodied Artificial
Intelligence (AI) in the intelligent transportation scenarios, the combination
of them in intelligent transportation spawns the Vehicular Embodied AI Network
(VEANs). In VEANs, Autonomous Vehicles (AVs) are typical agents whose local
advanced AI applications are defined as vehicular embodied AI agents, enabling
capabilities such as environment perception and multi-agent collaboration. Due
to computation latency and resource constraints, the local AI applications and
services running on vehicular embodied AI agents need to be migrated, and
subsequently referred to as vehicular embodied AI agent twins, which drive the
advancement of vehicular embodied AI networks to offload intensive tasks to
Roadside Units (RSUs), mitigating latency problems while maintaining service
quality. Recognizing workload imbalance among RSUs in traditional approaches,
we model AV-RSU interactions as a Stackelberg game to optimize bandwidth
resource allocation for efficient migration. A Tiny Multi-Agent Bidirectional
LSTM Proximal Policy Optimization (TMABLPPO) algorithm is designed to
approximate the Stackelberg equilibrium through decentralized coordination.
Furthermore, a personalized neural network pruning algorithm based on Path
eXclusion (PX) dynamically adapts to heterogeneous AV computation capabilities
by identifying task-critical parameters in trained models, reducing model
complexity with less performance degradation. Experimental validation confirms
the algorithm's effectiveness in balancing system load and minimizing delays,
demonstrating significant improvements in vehicular embodied AI agent
deployment.

</details>


### [450] [Heterogeneous Data Game: Characterizing the Model Competition Across Multiple Data Sources](https://arxiv.org/abs/2505.07688)
*Renzhe Xu,Kang Wang,Bo Li*

Main category: cs.GT

TL;DR: 论文提出了一个名为“异质数据博弈”的博弈论框架，用于分析不同机器学习服务提供商在异质数据源上的竞争情况，研究了纯纳什均衡的形态及其影响因素，为竞争性ML市场提供了理论和实践指导。


<details>
  <summary>Details</summary>
Motivation: 现实中的机器学习市场常由多个竞争性提供商组成，而数据异质性普遍存在。现有方法多关注单一模型处理多样性数据，缺乏对多提供商竞争的理论分析。

Method: 提出了“异质数据博弈”框架，分析提供商在异质数据源上的竞争行为，重点研究纯纳什均衡（PNE）的存在性与形态（同质或异质），涵盖垄断、双寡头及更一般的市场情景。

Result: 发现均衡可能是非存在、同质（所有提供商趋同）或异质（提供商各自专精不同数据源），并揭示了数据选择模型的“温度”和优势数据源对均衡形态的影响。

Conclusion: 研究为竞争性ML市场的监管政策和实践策略提供了理论依据，强调了均衡形态的多样性和其背后的驱动因素。

Abstract: Data heterogeneity across multiple sources is common in real-world machine
learning (ML) settings. Although many methods focus on enabling a single model
to handle diverse data, real-world markets often comprise multiple competing ML
providers. In this paper, we propose a game-theoretic framework -- the
Heterogeneous Data Game -- to analyze how such providers compete across
heterogeneous data sources. We investigate the resulting pure Nash equilibria
(PNE), showing that they can be non-existent, homogeneous (all providers
converge on the same model), or heterogeneous (providers specialize in distinct
data sources). Our analysis spans monopolistic, duopolistic, and more general
markets, illustrating how factors such as the "temperature" of data-source
choice models and the dominance of certain data sources shape equilibrium
outcomes. We offer theoretical insights into both homogeneous and heterogeneous
PNEs, guiding regulatory policies and practical strategies for competitive ML
marketplaces.

</details>
