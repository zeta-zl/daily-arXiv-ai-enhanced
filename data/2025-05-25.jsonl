{"id": "2505.15916", "pdf": "https://arxiv.org/pdf/2505.15916", "abs": "https://arxiv.org/abs/2505.15916", "authors": ["Juvenal Domingos Júnior", "Augusto Faria", "E. Seiti de Oliveira", "Erick de Brito", "Matheus Teotonio", "Andre Assumpção", "Diedre Carmo", "Roberto Lotufo", "Jayr Pereira"], "title": "BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents BR-TaxQA-R, a novel dataset designed to support question\nanswering with references in the context of Brazilian personal income tax law.\nThe dataset contains 715 questions from the 2024 official Q\\&A document\npublished by Brazil's Internal Revenue Service, enriched with statutory norms\nand administrative rulings from the Conselho Administrativo de Recursos Fiscais\n(CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using\nOpenAI embeddings for searching and GPT-4o-mini for answer generation. We\ncompare different text segmentation strategies and benchmark our system against\ncommercial tools such as ChatGPT and Perplexity.ai using RAGAS-based metrics.\nResults show that our custom RAG pipeline outperforms commercial systems in\nResponse Relevancy, indicating stronger alignment with user queries, while\ncommercial models achieve higher scores in Factual Correctness and fluency.\nThese findings highlight a trade-off between legally grounded generation and\nlinguistic fluency. Crucially, we argue that human expert evaluation remains\nessential to ensure the legal validity of AI-generated answers in high-stakes\ndomains such as taxation. BR-TaxQA-R is publicly available at\nhttps://huggingface.co/datasets/unicamp-dl/BR-TaxQA-R."}
{"id": "2505.15918", "pdf": "https://arxiv.org/pdf/2505.15918", "abs": "https://arxiv.org/abs/2505.15918", "authors": ["Aliakbar Nafar", "Kristen Brent Venable", "Zijun Cui", "Parisa Kordjamshidi"], "title": "Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated potential as factual knowledge\nbases; however, their capability to generate probabilistic knowledge about\nreal-world events remains understudied. This paper investigates using\nprobabilistic knowledge inherent in LLMs to derive probability estimates for\nstatements concerning events and their interrelationships captured via a\nBayesian Network (BN). Using LLMs in this context allows for the\nparameterization of BNs, enabling probabilistic modeling within specific\ndomains. Experiments on eighty publicly available Bayesian Networks, from\nhealthcare to finance, demonstrate that querying LLMs about the conditional\nprobabilities of events provides meaningful results when compared to baselines,\nincluding random and uniform distributions, as well as approaches based on\nnext-token generation probabilities. We explore how these LLM-derived\ndistributions can serve as expert priors to refine distributions extracted from\nminimal data, significantly reducing systematic biases. Overall, this work\nintroduces a promising strategy for automatically constructing Bayesian\nNetworks by combining probabilistic knowledge extracted from LLMs with small\namounts of real-world data. Additionally, we evaluate several prompting\nstrategies for eliciting probabilistic knowledge from LLMs and establish the\nfirst comprehensive baseline for assessing LLM performance in extracting\nprobabilistic knowledge."}
{"id": "2505.15922", "pdf": "https://arxiv.org/pdf/2505.15922", "abs": "https://arxiv.org/abs/2505.15922", "authors": ["Dong Won Lee", "Hae Won Park", "Cynthia Breazeal", "Louis-Philippe Morency"], "title": "Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition", "categories": ["cs.CL"], "comment": "9 pages, 3 figures, 3 tables", "summary": "We propose a large language model based reward decomposition framework for\naligning dialogue agents using only a single session-level feedback signal. We\nleverage the reasoning capabilities of a frozen, pretrained large language\nmodel (LLM) to infer fine-grained local implicit rewards by decomposing global,\nsession-level feedback. Our first text-only variant prompts the LLM to perform\nreward decomposition using only the dialogue transcript. The second multimodal\nvariant incorporates additional behavioral cues, such as pitch, gaze, and\nfacial affect, expressed as natural language descriptions. These inferred\nturn-level rewards are distilled into a lightweight reward model, which we\nutilize for RL-based fine-tuning for dialogue generation. We evaluate both\ntext-only and multimodal variants against state-of-the-art reward decomposition\nmethods and demonstrate notable improvements in human evaluations of\nconversation quality, suggesting that LLMs are strong reward decomposers that\nobviate the need for manual reward shaping and granular human feedback."}
{"id": "2505.15948", "pdf": "https://arxiv.org/pdf/2505.15948", "abs": "https://arxiv.org/abs/2505.15948", "authors": ["Parth Sarin", "Juan Pablo Alperin"], "title": "Citation Parsing and Analysis with Language Models", "categories": ["cs.CL", "cs.DL", "cs.SI"], "comment": "Presented at the Workshop on Open Citations & Open Scholarly Metadata\n  2025", "summary": "A key type of resource needed to address global inequalities in knowledge\nproduction and dissemination is a tool that can support journals in\nunderstanding how knowledge circulates. The absence of such a tool has resulted\nin comparatively less information about networks of knowledge sharing in the\nGlobal South. In turn, this gap authorizes the exclusion of researchers and\nscholars from the South in indexing services, reinforcing colonial arrangements\nthat de-center and minoritize those scholars. In order to support citation\nnetwork tracking on a global scale, we investigate the capacity of open-weight\nlanguage models to mark up manuscript citations in an indexable format. We\nassembled a dataset of matched plaintext and annotated citations from preprints\nand published research papers. Then, we evaluated a number of open-weight\nlanguage models on the annotation task. We find that, even out of the box,\ntoday's language models achieve high levels of accuracy on identifying the\nconstituent components of each citation, outperforming state-of-the-art\nmethods. Moreover, the smallest model we evaluated, Qwen3-0.6B, can parse all\nfields with high accuracy in $2^5$ passes, suggesting that post-training is\nlikely to be effective in producing small, robust citation parsing models. Such\na tool could greatly improve the fidelity of citation networks and thus\nmeaningfully improve research indexing and discovery, as well as further\nmetascientific research."}
{"id": "2505.15845", "pdf": "https://arxiv.org/pdf/2505.15845", "abs": "https://arxiv.org/abs/2505.15845", "authors": ["Zhibiao Wang", "Yunlong Zhou", "Ziwei Zhang", "Mengmei Zhang", "Shirui Pan", "Chunming Hu", "Xiao Wang"], "title": "Adaptive Tokenization: On the Hop-Overpriority Problem in Tokenized Graph Learning Models", "categories": ["cs.LG"], "comment": null, "summary": "Graph Transformers, leveraging the global attention to capture long-range\ndependencies in graph structures, have significantly advanced graph machine\nlearning, but face prohibitive computational complexity. Tokenized Graph\nLearning Models (TGLMs) address this issue by converting graphs into ordered\ntoken lists for scalable processing. Besides, TGLMs also empower Large Language\nModels (LLMs) to handle text-attributed graphs more effectively and thus are\nalso employed in Graph LLMs. However, existing TGLMs rely on hand-designed\ntoken lists and their adaptability to diverse graph learning scenarios remains\nunexplored. In this paper, we first conduct extensive empirical and theoretical\npreliminary studies for hand-designed token lists. Surprisingly, we identify an\nunexplored hop-overpriority problem: the common pre-defined token lists\noveremphasize nearby nodes and overwhelm the ability of TGLMs to balance local\nand global signals. This phenomenon is especially harmful for heterophilic\ngraphs. To address this problem, we propose the Learnable Graph Token List\n(LGTL), a plug-and-play module to replace hand-designed token lists in TGLMs.\nSpecifically, LGTL adaptively adjusts the weights across hops and prioritizes\ninformative nodes within hops through a graph attention gate module and a\nselection module, respectively. In this way, contextually informative nodes can\nbe adaptively emphasized for both homophilic and heterophilic graphs. Besides,\nwe theoretically show that LGTL can address the hop-overpriority problem.\nExtensive experiments on benchmarks validate the efficacy of LGTL across both\nGraph Transformers and Graph LLM backbones."}
{"id": "2505.15862", "pdf": "https://arxiv.org/pdf/2505.15862", "abs": "https://arxiv.org/abs/2505.15862", "authors": ["Long Wanga", "Jiongzhi Zheng", "Zhengda Xiong", "ChuMin Li", "Kun He"], "title": "Bandit based Dynamic Candidate Edge Selection in Solving Traveling Salesman Problems", "categories": ["cs.AI"], "comment": null, "summary": "Algorithms designed for routing problems typically rely on high-quality\ncandidate edges to guide their search, aiming to reduce the search space and\nenhance the search efficiency. However, many existing algorithms, like the\nclassical Lin-Kernighan-Helsgaun (LKH) algorithm for the Traveling Salesman\nProblem (TSP), often use predetermined candidate edges that remain static\nthroughout local searches. This rigidity could cause the algorithm to get\ntrapped in local optima, limiting its potential to find better solutions. To\naddress this issue, we propose expanding the candidate sets to include other\npromising edges, providing them an opportunity for selection. Specifically, we\nincorporate multi-armed bandit models to dynamically select the most suitable\ncandidate edges in each iteration, enabling LKH to make smarter choices and\nlead to improved solutions. Extensive experiments on multiple TSP benchmarks\nshow the excellent performance of our method. Moreover, we employ this\nbandit-based method to LKH-3, an extension of LKH tailored for solving various\nTSP variant problems, and our method also significantly enhances LKH-3's\nperformance across typical TSP variants."}
{"id": "2505.15960", "pdf": "https://arxiv.org/pdf/2505.15960", "abs": "https://arxiv.org/abs/2505.15960", "authors": ["Ryo Kamoi", "Yusen Zhang", "Nan Zhang", "Sarkar Snigdha Sarathi Das", "Rui Zhang"], "title": "Training Step-Level Reasoning Verifiers with Formal Verification Tools", "categories": ["cs.CL"], "comment": "Datasets, models, and code are provided at\n  https://github.com/psunlpgroup/FoVer. Please also refer to our project\n  website at https://fover-prm.github.io/", "summary": "Process Reward Models (PRMs), which provide step-by-step feedback on the\nreasoning generated by Large Language Models (LLMs), are receiving increasing\nattention. However, two key research gaps remain: collecting accurate\nstep-level error labels for training typically requires costly human\nannotation, and existing PRMs are limited to math reasoning problems. In\nresponse to these gaps, this paper aims to address the challenges of automatic\ndataset creation and the generalization of PRMs to diverse reasoning tasks. To\nachieve this goal, we propose FoVer, an approach for training PRMs on\nstep-level error labels automatically annotated by formal verification tools,\nsuch as Z3 for formal logic and Isabelle for theorem proof, which provide\nautomatic and accurate verification for symbolic tasks. Using this approach, we\nsynthesize a training dataset with error labels on LLM responses for formal\nlogic and theorem proof tasks without human annotation. Although this data\nsynthesis is feasible only for tasks compatible with formal verification, we\nobserve that LLM-based PRMs trained on our dataset exhibit cross-task\ngeneralization, improving verification across diverse reasoning tasks.\nSpecifically, PRMs trained with FoVer significantly outperform baseline PRMs\nbased on the original LLMs and achieve competitive or superior results compared\nto state-of-the-art PRMs trained on labels annotated by humans or stronger\nmodels, as measured by step-level verification on ProcessBench and Best-of-K\nperformance across 12 reasoning benchmarks, including MATH, AIME, ANLI, MMLU,\nand BBH. The datasets, models, and code are provided at\nhttps://github.com/psunlpgroup/FoVer."}
{"id": "2505.15888", "pdf": "https://arxiv.org/pdf/2505.15888", "abs": "https://arxiv.org/abs/2505.15888", "authors": ["Valentin Villecroze", "Yixin Wang", "Gabriel Loaiza-Ganem"], "title": "Last Layer Empirical Bayes", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted at the ICBINB Worshop at ICLR 2025", "summary": "The task of quantifying the inherent uncertainty associated with neural\nnetwork predictions is a key challenge in artificial intelligence. Bayesian\nneural networks (BNNs) and deep ensembles are among the most prominent\napproaches to tackle this task. Both approaches produce predictions by\ncomputing an expectation of neural network outputs over some distribution on\nthe corresponding weights; this distribution is given by the posterior in the\ncase of BNNs, and by a mixture of point masses for ensembles. Inspired by\nrecent work showing that the distribution used by ensembles can be understood\nas a posterior corresponding to a learned data-dependent prior, we propose last\nlayer empirical Bayes (LLEB). LLEB instantiates a learnable prior as a\nnormalizing flow, which is then trained to maximize the evidence lower bound;\nto retain tractability we use the flow only on the last layer. We show why LLEB\nis well motivated, and how it interpolates between standard BNNs and ensembles\nin terms of the strength of the prior that they use. LLEB performs on par with\nexisting approaches, highlighting that empirical Bayes is a promising direction\nfor future research in uncertainty quantification."}
{"id": "2505.15929", "pdf": "https://arxiv.org/pdf/2505.15929", "abs": "https://arxiv.org/abs/2505.15929", "authors": ["Hui Shen", "Taiqiang Wu", "Qi Han", "Yunta Hsieh", "Jizhou Wang", "Yuyue Zhang", "Yuxin Cheng", "Zijian Hao", "Yuansheng Ni", "Xin Wang", "Zhongwei Wan", "Kai Zhang", "Wendong Xu", "Jing Xiong", "Ping Luo", "Wenhu Chen", "Chaofan Tao", "Zhuoqing Mao", "Ngai Wong"], "title": "PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?", "categories": ["cs.AI"], "comment": null, "summary": "Existing benchmarks fail to capture a crucial aspect of intelligence:\nphysical reasoning, the integrated ability to combine domain knowledge,\nsymbolic reasoning, and understanding of real-world constraints. To address\nthis gap, we introduce PhyX: the first large-scale benchmark designed to assess\nmodels capacity for physics-grounded reasoning in visual scenarios. PhyX\nincludes 3K meticulously curated multimodal questions spanning 6 reasoning\ntypes across 25 sub-domains and 6 core physics domains: thermodynamics,\nelectromagnetism, mechanics, modern physics, optics, and wave\\&acoustics. In\nour comprehensive evaluation, even state-of-the-art models struggle\nsignificantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and\nGPT-o4-mini achieve only 32.5\\%, 42.2\\%, and 45.8\\% accuracy\nrespectively-performance gaps exceeding 29\\% compared to human experts. Our\nanalysis exposes critical limitations in current models: over-reliance on\nmemorized disciplinary knowledge, excessive dependence on mathematical\nformulations, and surface-level visual pattern matching rather than genuine\nphysical understanding. We provide in-depth analysis through fine-grained\nstatistics, detailed case studies, and multiple evaluation paradigms to\nthoroughly examine physical reasoning capabilities. To ensure reproducibility,\nwe implement a compatible evaluation protocol based on widely-used toolkits\nsuch as VLMEvalKit, enabling one-click evaluation."}
{"id": "2505.15962", "pdf": "https://arxiv.org/pdf/2505.15962", "abs": "https://arxiv.org/abs/2505.15962", "authors": ["Linxi Zhao", "Sofian Zalouk", "Christian K. Belardi", "Justin Lovelace", "Jin Peng Zhou", "Kilian Q. Weinberger", "Yoav Artzi", "Jennifer J. Sun"], "title": "Pre-training Large Memory Language Models with Internal and External Knowledge", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Neural language models are black-boxes -- both linguistic patterns and\nfactual knowledge are distributed across billions of opaque parameters. This\nentangled encoding makes it difficult to reliably inspect, verify, or update\nspecific facts. We propose a new class of language models, Large Memory\nLanguage Models (LMLM) with a pre-training recipe that stores factual knowledge\nin both internal weights and an external database. Our approach strategically\nmasks externally retrieved factual values from the training loss, thereby\nteaching the model to perform targeted lookups rather than relying on\nmemorization in model weights. Our experiments demonstrate that LMLMs achieve\ncompetitive performance compared to significantly larger, knowledge-dense LLMs\non standard benchmarks, while offering the advantages of explicit, editable,\nand verifiable knowledge bases. This work represents a fundamental shift in how\nlanguage models interact with and manage factual knowledge."}
{"id": "2505.15909", "pdf": "https://arxiv.org/pdf/2505.15909", "abs": "https://arxiv.org/abs/2505.15909", "authors": ["Alex Kogan"], "title": "Is (Selective) Round-To-Nearest Quantization All You Need?", "categories": ["cs.LG", "I.2.7; D.4.8; G.4"], "comment": null, "summary": "Quantization became a necessary tool for serving ever-increasing Large\nLanguage Models (LLMs). RTN (Round-to-Nearest) is perhaps the simplest\nquantization technique that has been around well before LLMs surged to the\nforefront of machine learning (ML) research. Yet, it has been largely dismissed\nby recent and more advanced quantization methods that claim superiority over\nRTN in nearly every aspect of performance. This work aims to dispel this\nestablished point of view, showing that RTN is not only much cheaper to apply,\nbut also its token generation throughput can be better than and accuracy can be\nsimilar to more advanced alternatives. In particular, we discuss our\nimplementation of RTN based on the recent Marlin kernels and demonstrate how\nthe accuracy of RTN can be gradually improved by selectively increasing the\ndata precision format of certain model layers and modules. Based on our\nresults, we argue that RTN presents a viable and practical choice for\nquantizing LLMs."}
{"id": "2505.15998", "pdf": "https://arxiv.org/pdf/2505.15998", "abs": "https://arxiv.org/abs/2505.15998", "authors": ["Thomas Michel", "Marko Cvjetko", "Gautier Hamon", "Pierre-Yves Oudeyer", "Clément Moulin-Frier"], "title": "Exploring Flow-Lenia Universes with a Curiosity-driven AI Scientist: Discovering Diverse Ecosystem Dynamics", "categories": ["cs.AI"], "comment": "10 pages, 10 figures, submitted to ALIFE 2025 Conference", "summary": "We present a method for the automated discovery of system-level dynamics in\nFlow-Lenia$-$a continuous cellular automaton (CA) with mass conservation and\nparameter localization$-$using a curiosity-driven AI scientist. This method\naims to uncover processes leading to self-organization of evolutionary and\necosystemic dynamics in CAs. We build on previous work which uses diversity\nsearch algorithms in Lenia to find self-organized individual patterns, and\nextend it to large environments that support distinct interacting patterns. We\nadapt Intrinsically Motivated Goal Exploration Processes (IMGEPs) to drive\nexploration of diverse Flow-Lenia environments using simulation-wide metrics,\nsuch as evolutionary activity, compression-based complexity, and multi-scale\nentropy. We test our method in two experiments, showcasing its ability to\nilluminate significantly more diverse dynamics compared to random search. We\nshow qualitative results illustrating how ecosystemic simulations enable\nself-organization of complex collective behaviors not captured by previous\nindividual pattern search and analysis. We complement automated discovery with\nan interactive exploration tool, creating an effective human-AI collaborative\nworkflow for scientific investigation. Though demonstrated specifically with\nFlow-Lenia, this methodology provides a framework potentially applicable to\nother parameterizable complex systems where understanding emergent collective\nproperties is of interest."}
{"id": "2505.15993", "pdf": "https://arxiv.org/pdf/2505.15993", "abs": "https://arxiv.org/abs/2505.15993", "authors": ["Anirudh Maiya", "Razan Alghamdi", "Maria Leonor Pacheco", "Ashutosh Trivedi", "Fabio Somenzi"], "title": "Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku", "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "The success of Large Language Models (LLMs) in human-AI collaborative\ndecision-making hinges on their ability to provide trustworthy, gradual, and\ntailored explanations. Solving complex puzzles, such as Sudoku, offers a\ncanonical example of this collaboration, where clear and customized\nexplanations often hold greater importance than the final solution. In this\nstudy, we evaluate the performance of five LLMs in solving and explaining\n\\sixsix{} Sudoku puzzles. While one LLM demonstrates limited success in solving\npuzzles, none can explain the solution process in a manner that reflects\nstrategic reasoning or intuitive problem-solving. These findings underscore\nsignificant challenges that must be addressed before LLMs can become effective\npartners in human-AI collaborative decision-making."}
{"id": "2505.15931", "pdf": "https://arxiv.org/pdf/2505.15931", "abs": "https://arxiv.org/abs/2505.15931", "authors": ["Morteza Alizadeh", "Mehrdad Oveisi", "Sonya Falahati", "Ghazal Mousavi", "Mohsen Alambardar Meybodi", "Somayeh Sadat Mehrnia", "Ilker Hacihaliloglu", "Arman Rahmim", "Mohammad R. Salmanpour"], "title": "AllMetrics: A Unified Python Library for Standardized Metric Evaluation and Robust Data Validation in Machine Learning", "categories": ["cs.LG", "F.2.2; I.2.7"], "comment": null, "summary": "Machine learning (ML) models rely heavily on consistent and accurate\nperformance metrics to evaluate and compare their effectiveness. However,\nexisting libraries often suffer from fragmentation, inconsistent\nimplementations, and insufficient data validation protocols, leading to\nunreliable results. Existing libraries have often been developed independently\nand without adherence to a unified standard, particularly concerning the\nspecific tasks they aim to support. As a result, each library tends to adopt\nits conventions for metric computation, input/output formatting, error\nhandling, and data validation protocols. This lack of standardization leads to\nboth implementation differences (ID) and reporting differences (RD), making it\ndifficult to compare results across frameworks or ensure reliable evaluations.\nTo address these issues, we introduce AllMetrics, an open-source unified Python\nlibrary designed to standardize metric evaluation across diverse ML tasks,\nincluding regression, classification, clustering, segmentation, and\nimage-to-image translation. The library implements class-specific reporting for\nmulti-class tasks through configurable parameters to cover all use cases, while\nincorporating task-specific parameters to resolve metric computation\ndiscrepancies across implementations. Various datasets from domains like\nhealthcare, finance, and real estate were applied to our library and compared\nwith Python, Matlab, and R components to identify which yield similar results.\nAllMetrics combines a modular Application Programming Interface (API) with\nrobust input validation mechanisms to ensure reproducibility and reliability in\nmodel evaluation. This paper presents the design principles, architectural\ncomponents, and empirical analyses demonstrating the ability to mitigate\nevaluation errors and to enhance the trustworthiness of ML workflows."}
{"id": "2505.16031", "pdf": "https://arxiv.org/pdf/2505.16031", "abs": "https://arxiv.org/abs/2505.16031", "authors": ["Aayushi Dangol", "Robert Wolfe", "Runhua Zhao", "JaeWon Kim", "Trushaa Ramanan", "Katie Davis", "Julie A. Kientz"], "title": "Children's Mental Models of AI Reasoning: Implications for AI Literacy Education", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "As artificial intelligence (AI) advances in reasoning capabilities, most\nrecently with the emergence of Large Reasoning Models (LRMs), understanding how\nchildren conceptualize AI's reasoning processes becomes critical for fostering\nAI literacy. While one of the \"Five Big Ideas\" in AI education highlights\nreasoning algorithms as central to AI decision-making, less is known about\nchildren's mental models in this area. Through a two-phase approach, consisting\nof a co-design session with 8 children followed by a field study with 106\nchildren (grades 3-8), we identified three models of AI reasoning: Deductive,\nInductive, and Inherent. Our findings reveal that younger children (grades 3-5)\noften attribute AI's reasoning to inherent intelligence, while older children\n(grades 6-8) recognize AI as a pattern recognizer. We highlight three tensions\nthat surfaced in children's understanding of AI reasoning and conclude with\nimplications for scaffolding AI curricula and designing explainable AI tools."}
{"id": "2505.16000", "pdf": "https://arxiv.org/pdf/2505.16000", "abs": "https://arxiv.org/abs/2505.16000", "authors": ["Mehrdad ghassabi", "Pedram Rostami", "Hamidreza Baradaran Kashani", "Amirhossein Poursina", "Zahra Kazemi", "Milad Tavakoli"], "title": "Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 4 figures", "summary": "The rapid advancement of language models has demonstrated the potential of\nartificial intelligence in the healthcare industry. However, small language\nmodels struggle with specialized domains in low-resource languages like\nPersian. While numerous medical-domain websites exist in Persian, no curated\ndataset or corpus has been available making ours the first of its kind. This\nstudy explores the enhancement of medical knowledge in a small language model\nby leveraging accessible online data, including a crawled corpus from medical\nmagazines and a dataset of real doctor-patient QA pairs. We fine-tuned a\nbaseline model using our curated data to improve its medical knowledge.\nBenchmark evaluations demonstrate that the fine-tuned model achieves improved\naccuracy in medical question answering and provides better responses compared\nto its baseline. This work highlights the potential of leveraging open-access\nonline data to enrich small language models in medical fields, providing a\nnovel solution for Persian medical AI applications suitable for\nresource-constrained environments."}
{"id": "2505.15946", "pdf": "https://arxiv.org/pdf/2505.15946", "abs": "https://arxiv.org/abs/2505.15946", "authors": ["Yuxiang Wei", "Yanteng Zhang", "Xi Xiao", "Tianyang Wang", "Xiao Wang", "Vince D. Calhoun"], "title": "MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "Decoding visual experiences from fMRI offers a powerful avenue to understand\nhuman perception and develop advanced brain-computer interfaces. However,\ncurrent progress often prioritizes maximizing reconstruction fidelity while\noverlooking interpretability, an essential aspect for deriving neuroscientific\ninsight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework\ndesigned for high-fidelity, adaptable, and interpretable visual reconstruction.\nMoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture\nwhere distinct experts process fMRI signals from functionally related voxel\ngroups, mimicking specialized brain networks. The experts are first trained to\nencode fMRI into the frozen CLIP space. A finetuned diffusion model then\nsynthesizes images, guided by expert outputs through a novel dual-stage routing\nmechanism that dynamically weighs expert contributions across the diffusion\nprocess. MoRE-Brain offers three main advancements: First, it introduces a\nnovel Mixture-of-Experts architecture grounded in brain network principles for\nneuro-decoding. Second, it achieves efficient cross-subject generalization by\nsharing core expert networks while adapting only subject-specific routers.\nThird, it provides enhanced mechanistic insight, as the explicit routing\nreveals precisely how different modeled brain regions shape the semantic and\nspatial attributes of the reconstructed image. Extensive experiments validate\nMoRE-Brain's high reconstruction fidelity, with bottleneck analyses further\ndemonstrating its effective utilization of fMRI signals, distinguishing genuine\nneural decoding from over-reliance on generative priors. Consequently,\nMoRE-Brain marks a substantial advance towards more generalizable and\ninterpretable fMRI-based visual decoding. Code will be publicly available soon:\nhttps://github.com/yuxiangwei0808/MoRE-Brain."}
{"id": "2505.16037", "pdf": "https://arxiv.org/pdf/2505.16037", "abs": "https://arxiv.org/abs/2505.16037", "authors": ["Asterios Tsiourvas", "Wei Sun", "Georgia Perakis"], "title": "Causal LLM Routing: End-to-End Regret Minimization from Observational Data", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "LLM routing aims to select the most appropriate model for each query,\nbalancing competing performance metrics such as accuracy and cost across a pool\nof language models. Prior approaches typically adopt a decoupled strategy,\nwhere the metrics are first predicted and the model is then selected based on\nthese estimates. This setup is prone to compounding errors and often relies on\nfull-feedback data, where each query is evaluated by all candidate models,\nwhich is costly to obtain and maintain in practice. In contrast, we learn from\nobservational data, which records only the outcome of the model actually\ndeployed. We propose a causal end-to-end framework that learns routing policies\nby minimizing decision-making regret from observational data. To enable\nefficient optimization, we introduce two theoretically grounded surrogate\nobjectives: a classification-based upper bound, and a softmax-weighted regret\napproximation shown to recover the optimal policy at convergence. We further\nextend our framework to handle heterogeneous cost preferences via an\ninterval-conditioned architecture. Experiments on public benchmarks show that\nour method outperforms existing baselines, achieving state-of-the-art\nperformance across different embedding models."}
{"id": "2505.16002", "pdf": "https://arxiv.org/pdf/2505.16002", "abs": "https://arxiv.org/abs/2505.16002", "authors": ["Sasha Boguraev", "Christopher Potts", "Kyle Mahowald"], "title": "Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 19 figures, 11 tables", "summary": "Large Language Models (LLMs) have emerged as powerful sources of evidence for\nlinguists seeking to develop theories of syntax. In this paper, we argue that\ncausal interpretability methods, applied to LLMs, can greatly enhance the value\nof such evidence by helping us characterize the abstract mechanisms that LLMs\nlearn to use. Our empirical focus is a set of English filler-gap dependency\nconstructions (e.g., questions, relative clauses). Linguistic theories largely\nagree that these constructions share many properties. Using experiments based\nin Distributed Interchange Interventions, we show that LLMs converge on similar\nabstract analyses of these constructions. These analyses also reveal previously\noverlooked factors -- relating to frequency, filler type, and surrounding\ncontext -- that could motivate changes to standard linguistic theory. Overall,\nthese results suggest that mechanistic, internal analyses of LLMs can push\nlinguistic theory forward."}
{"id": "2505.15987", "pdf": "https://arxiv.org/pdf/2505.15987", "abs": "https://arxiv.org/abs/2505.15987", "authors": ["Aaron Zweig", "Zaikang Lin", "Elham Azizi", "David Knowles"], "title": "Towards Identifiability of Interventional Stochastic Differential Equations", "categories": ["cs.LG"], "comment": null, "summary": "We study identifiability of stochastic differential equation (SDE) models\nunder multiple interventions. Our results give the first provable bounds for\nunique recovery of SDE parameters given samples from their stationary\ndistributions. We give tight bounds on the number of necessary interventions\nfor linear SDEs, and upper bounds for nonlinear SDEs in the small noise regime.\nWe experimentally validate the recovery of true parameters in synthetic data,\nand motivated by our theoretical results, demonstrate the advantage of\nparameterizations with learnable activation functions."}
{"id": "2505.16048", "pdf": "https://arxiv.org/pdf/2505.16048", "abs": "https://arxiv.org/abs/2505.16048", "authors": ["Philipp D. Siedler"], "title": "SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution", "categories": ["cs.AI"], "comment": null, "summary": "We introduce a novel dataset designed to benchmark the physical and spatial\nreasoning capabilities of Large Language Models (LLM) based on topology\noptimization, a method for computing optimal material distributions within a\ndesign space under prescribed loads and supports. In this dataset, LLMs are\nprovided with conditions such as 2D boundary, applied forces and supports, and\nmust reason about the resulting optimal material distribution. The dataset\nincludes a variety of tasks, ranging from filling in masked regions within\npartial structures to predicting complete material distributions. Solving these\ntasks requires understanding the flow of forces and the required material\ndistribution under given constraints, without access to simulation tools or\nexplicit physical models, challenging models to reason about structural\nstability and spatial organization. Our dataset targets the evaluation of\nspatial and physical reasoning abilities in 2D settings, offering a\ncomplementary perspective to traditional language and logic benchmarks."}
{"id": "2505.16003", "pdf": "https://arxiv.org/pdf/2505.16003", "abs": "https://arxiv.org/abs/2505.16003", "authors": ["Roland Daynauth", "Christopher Clarke", "Krisztian Flautner", "Lingjia Tang", "Jason Mars"], "title": "SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The LLM-as-a-Judge paradigm offers a scalable, reference-free approach for\nevaluating language models. Although several calibration techniques have been\nproposed to better align these evaluators with human judgment, prior studies\nfocus primarily on narrow, well-structured benchmarks. As a result, it remains\nunclear whether such calibrations generalize to real-world, open-ended tasks.\n  In this work, we show that SOTA calibrated evaluators often fail in these\nsettings, exhibiting weak or even negative correlation with human judgments. To\naddress this, we propose SLMEval, a novel and efficient calibration method\nbased on entropy maximization over a small amount of human preference data. By\nestimating a latent distribution over model quality and reweighting evaluator\nscores accordingly, SLMEval achieves strong correlation with human evaluations\nacross two real-world production use cases and the public benchmark. For\nexample, on one such task, SLMEval achieves a Spearman correlation of 0.57 with\nhuman judgments, while G-Eval yields a negative correlation. In addition,\nSLMEval reduces evaluation costs by 5-30x compared to GPT-4-based calibrated\nevaluators such as G-eval."}
{"id": "2505.16004", "pdf": "https://arxiv.org/pdf/2505.16004", "abs": "https://arxiv.org/abs/2505.16004", "authors": ["Aaron J. Li", "Suraj Srinivas", "Usha Bhalla", "Himabindu Lakkaraju"], "title": "Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse autoencoders (SAEs) are commonly used to interpret the internal\nactivations of large language models (LLMs) by mapping them to\nhuman-interpretable concept representations. While existing evaluations of SAEs\nfocus on metrics such as the reconstruction-sparsity tradeoff, human\n(auto-)interpretability, and feature disentanglement, they overlook a critical\naspect: the robustness of concept representations to input perturbations. We\nargue that robustness must be a fundamental consideration for concept\nrepresentations, reflecting the fidelity of concept labeling. To this end, we\nformulate robustness quantification as input-space optimization problems and\ndevelop a comprehensive evaluation framework featuring realistic scenarios in\nwhich adversarial perturbations are crafted to manipulate SAE representations.\nEmpirically, we find that tiny adversarial input perturbations can effectively\nmanipulate concept-based interpretations in most scenarios without notably\naffecting the outputs of the base LLMs themselves. Overall, our results suggest\nthat SAE concept representations are fragile and may be ill-suited for\napplications in model monitoring and oversight."}
{"id": "2505.16067", "pdf": "https://arxiv.org/pdf/2505.16067", "abs": "https://arxiv.org/abs/2505.16067", "authors": ["Zidi Xiong", "Yuping Lin", "Wenya Xie", "Pengfei He", "Jiliang Tang", "Himabindu Lakkaraju", "Zhen Xiang"], "title": "How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior", "categories": ["cs.AI"], "comment": null, "summary": "Memory is a critical component in large language model (LLM)-based agents,\nenabling them to store and retrieve past executions to improve task performance\nover time. In this paper, we conduct an empirical study on how memory\nmanagement choices impact the LLM agents' behavior, especially their long-term\nperformance. Specifically, we focus on two fundamental memory operations that\nare widely used by many agent frameworks-addition, which incorporates new\nexperiences into the memory base, and deletion, which selectively removes past\nexperiences-to systematically study their impact on the agent behavior. Through\nour quantitative analysis, we find that LLM agents display an\nexperience-following property: high similarity between a task input and the\ninput in a retrieved memory record often results in highly similar agent\noutputs. Our analysis further reveals two significant challenges associated\nwith this property: error propagation, where inaccuracies in past experiences\ncompound and degrade future performance, and misaligned experience replay,\nwhere outdated or irrelevant experiences negatively influence current tasks.\nThrough controlled experiments, we show that combining selective addition and\ndeletion strategies can help mitigate these negative effects, yielding an\naverage absolute performance gain of 10% compared to naive memory growth.\nFurthermore, we highlight how memory management choices affect agents' behavior\nunder challenging conditions such as task distribution shifts and constrained\nmemory resources. Our findings offer insights into the behavioral dynamics of\nLLM agent memory systems and provide practical guidance for designing memory\ncomponents that support robust, long-term agent performance. We also release\nour code to facilitate further study."}
{"id": "2505.16008", "pdf": "https://arxiv.org/pdf/2505.16008", "abs": "https://arxiv.org/abs/2505.16008", "authors": ["Wenrui Yu", "Yiyi Chen", "Johannes Bjerva", "Sokol Kosta", "Qiongxiu Li"], "title": "LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "We propose LAGO - Language Similarity-Aware Graph Optimization - a novel\napproach for few-shot cross-lingual embedding inversion attacks, addressing\ncritical privacy vulnerabilities in multilingual NLP systems. Unlike prior work\nin embedding inversion attacks that treat languages independently, LAGO\nexplicitly models linguistic relationships through a graph-based constrained\ndistributed optimization framework. By integrating syntactic and lexical\nsimilarity as edge constraints, our method enables collaborative parameter\nlearning across related languages. Theoretically, we show this formulation\ngeneralizes prior approaches, such as ALGEN, which emerges as a special case\nwhen similarity constraints are relaxed. Our framework uniquely combines\nFrobenius-norm regularization with linear inequality or total variation\nconstraints, ensuring robust alignment of cross-lingual embedding spaces even\nwith extremely limited data (as few as 10 samples per language). Extensive\nexperiments across multiple languages and embedding models demonstrate that\nLAGO substantially improves the transferability of attacks with 10-20% increase\nin Rouge-L score over baselines. This work establishes language similarity as a\ncritical factor in inversion attack transferability, urging renewed focus on\nlanguage-aware privacy-preserving multilingual embeddings."}
{"id": "2505.16017", "pdf": "https://arxiv.org/pdf/2505.16017", "abs": "https://arxiv.org/abs/2505.16017", "authors": ["Mariia Seleznova", "Hung-Hsu Chou", "Claudio Mayrink Verdun", "Gitta Kutyniok"], "title": "GradPCA: Leveraging NTK Alignment for Reliable Out-of-Distribution Detection", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We introduce GradPCA, an Out-of-Distribution (OOD) detection method that\nexploits the low-rank structure of neural network gradients induced by Neural\nTangent Kernel (NTK) alignment. GradPCA applies Principal Component Analysis\n(PCA) to gradient class-means, achieving more consistent performance than\nexisting methods across standard image classification benchmarks. We provide a\ntheoretical perspective on spectral OOD detection in neural networks to support\nGradPCA, highlighting feature-space properties that enable effective detection\nand naturally emerge from NTK alignment. Our analysis further reveals that\nfeature quality -- particularly the use of pretrained versus non-pretrained\nrepresentations -- plays a crucial role in determining which detectors will\nsucceed. Extensive experiments validate the strong performance of GradPCA, and\nour theoretical framework offers guidance for designing more principled\nspectral OOD detectors."}
{"id": "2505.16080", "pdf": "https://arxiv.org/pdf/2505.16080", "abs": "https://arxiv.org/abs/2505.16080", "authors": ["Jiayue Liu", "Zhongchao Yi", "Zhengyang Zhou", "Qihe Huang", "Kuo Yang", "Xu Wang", "Yang Wang"], "title": "SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation", "categories": ["cs.AI"], "comment": "16 pages, 7 figures", "summary": "Discovering regularities from spatiotemporal systems can benefit various\nscientific and social planning. Current spatiotemporal learners usually train\nan independent model from a specific source data that leads to limited\ntransferability among sources, where even correlated tasks requires new design\nand training. The key towards increasing cross-domain knowledge is to enable\ncollective intelligence and model evolution. In this paper, inspired by\nneuroscience theories, we theoretically derive the increased information\nboundary via learning cross-domain collective intelligence and propose a\nSynaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the\nmodel independence and enables cross-domain knowledge to be shared and\naggregated. Specifically, we first re-order the sample groups to imitate the\nhuman curriculum learning, and devise two complementary learners, elastic\ncommon container and task-independent extractor to allow model growth and\ntask-wise commonality and personality disentanglement. Then an adaptive dynamic\ncoupler with a new difference metric determines whether the new sample group\nshould be incorporated into common container to achieve model evolution under\nvarious domains. Experiments show that SynEVO improves the generalization\ncapacity by at most 42% under cross-domain scenarios and SynEVO provides a\nparadigm of NeuroAI for knowledge transfer and adaptation."}
{"id": "2505.16014", "pdf": "https://arxiv.org/pdf/2505.16014", "abs": "https://arxiv.org/abs/2505.16014", "authors": ["Yash Saxena", "Anpur Padia", "Mandar S Chaudhary", "Kalpa Gunaratna", "Srinivasan Parthasarathy", "Manas Gaur"], "title": "Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains", "categories": ["cs.CL"], "comment": null, "summary": "Traditional Retrieval-Augmented Generation (RAG) pipelines rely on\nsimilarity-based retrieval and re-ranking, which depend on heuristics such as\ntop-k, and lack explainability, interpretability, and robustness against\nadversarial content. To address this gap, we propose a novel method METEORA\nthat replaces re-ranking in RAG with a rationale-driven selection approach.\nMETEORA operates in two stages. First, a general-purpose LLM is\npreference-tuned to generate rationales conditioned on the input query using\ndirect preference optimization. These rationales guide the evidence chunk\nselection engine, which selects relevant chunks in three stages: pairing\nindividual rationales with corresponding retrieved chunks for local relevance,\nglobal selection with elbow detection for adaptive cutoff, and context\nexpansion via neighboring chunks. This process eliminates the need for top-k\nheuristics. The rationales are also used for consistency check using a Verifier\nLLM to detect and filter poisoned or misleading content for safe generation.\nThe framework provides explainable and interpretable evidence flow by using\nrationales consistently across both selection and verification. Our evaluation\nacross six datasets spanning legal, financial, and academic research domains\nshows that METEORA improves generation accuracy by 33.34% while using\napproximately 50% fewer chunks than state-of-the-art re-ranking methods. In\nadversarial settings, METEORA significantly improves the F1 score from 0.10 to\n0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating\nstrong resilience to poisoning attacks. Code available at:\nhttps://anonymous.4open.science/r/METEORA-DC46/README.md"}
{"id": "2505.16024", "pdf": "https://arxiv.org/pdf/2505.16024", "abs": "https://arxiv.org/abs/2505.16024", "authors": ["Weiguo Gao", "Ming Li"], "title": "Toward Theoretical Insights into Diffusion Trajectory Distillation via Operator Merging", "categories": ["cs.LG", "cs.AI"], "comment": "31 pages, 19 figures", "summary": "Diffusion trajectory distillation methods aim to accelerate sampling in\ndiffusion models, which produce high-quality outputs but suffer from slow\nsampling speeds. These methods train a student model to approximate the\nmulti-step denoising process of a pretrained teacher model in a single step,\nenabling one-shot generation. However, theoretical insights into the trade-off\nbetween different distillation strategies and generative quality remain\nlimited, complicating their optimization and selection. In this work, we take a\nfirst step toward addressing this gap. Specifically, we reinterpret trajectory\ndistillation as an operator merging problem in the linear regime, where each\nstep of the teacher model is represented as a linear operator acting on noisy\ndata. These operators admit a clear geometric interpretation as projections and\nrescalings corresponding to the noise schedule. During merging, signal\nshrinkage occurs as a convex combination of operators, arising from both\ndiscretization and limited optimization time of the student model. We propose a\ndynamic programming algorithm to compute the optimal merging strategy that\nmaximally preserves signal fidelity. Additionally, we demonstrate the existence\nof a sharp phase transition in the optimal strategy, governed by data\ncovariance structures. Our findings enhance the theoretical understanding of\ndiffusion trajectory distillation and offer practical insights for improving\ndistillation strategies."}
{"id": "2505.16086", "pdf": "https://arxiv.org/pdf/2505.16086", "abs": "https://arxiv.org/abs/2505.16086", "authors": ["Ming Shen", "Raphael Shu", "Anurag Pratik", "James Gung", "Yubin Ge", "Monica Sunkara", "Yi Zhang"], "title": "Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We have seen remarkable progress in large language models (LLMs) empowered\nmulti-agent systems solving complex tasks necessitating cooperation among\nexperts with diverse skills. However, optimizing LLM-based multi-agent systems\nremains challenging. In this work, we perform an empirical case study on group\noptimization of role-based multi-agent systems utilizing natural language\nfeedback for challenging software development tasks under various evaluation\ndimensions. We propose a two-step agent prompts optimization pipeline:\nidentifying underperforming agents with their failure explanations utilizing\ntextual feedback and then optimizing system prompts of identified agents\nutilizing failure explanations. We then study the impact of various\noptimization settings on system performance with two comparison groups: online\nagainst offline optimization and individual against group optimization. For\ngroup optimization, we study two prompting strategies: one-pass and multi-pass\nprompting optimizations. Overall, we demonstrate the effectiveness of our\noptimization method for role-based multi-agent systems tackling software\ndevelopment tasks evaluated on diverse evaluation dimensions, and we\ninvestigate the impact of diverse optimization settings on group behaviors of\nthe multi-agent systems to provide practical insights for future development."}
{"id": "2505.16022", "pdf": "https://arxiv.org/pdf/2505.16022", "abs": "https://arxiv.org/abs/2505.16022", "authors": ["Wei Liu", "Siya Qi", "Xinyu Wang", "Chen Qian", "Yali Du", "Yulan He"], "title": "NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "20 pages, 5 tables, 12 figures", "summary": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of\nincentive training, a reinforcement learning paradigm that computes rewards\nsolely based on the final answer part of a language model's output, thereby\nencouraging the generation of intermediate reasoning steps. However, these\nmethods fundamentally rely on external verifiers, which limits their\napplicability to domains like mathematics and coding where such verifiers are\nreadily available. Although reward models can serve as verifiers, they require\nhigh-quality annotated data and are costly to train. In this work, we propose\nNOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning\nframework that requires only standard supervised fine-tuning data with no need\nfor an external verifier. NOVER enables incentive training across a wide range\nof text-to-text tasks and outperforms the model of the same size distilled from\nlarge reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the\nflexibility of NOVER enables new possibilities for optimizing large language\nmodels, such as inverse incentive training."}
{"id": "2505.16035", "pdf": "https://arxiv.org/pdf/2505.16035", "abs": "https://arxiv.org/abs/2505.16035", "authors": ["Alejandro García-Castellanos", "David R. Wessels", "Nicky J. van den Berg", "Remco Duits", "Daniël M. Pelt", "Erik J. Bekkers"], "title": "Equivariant Eikonal Neural Networks: Grid-Free, Scalable Travel-Time Prediction on Homogeneous Spaces", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce Equivariant Neural Eikonal Solvers, a novel framework that\nintegrates Equivariant Neural Fields (ENFs) with Neural Eikonal Solvers. Our\napproach employs a single neural field where a unified shared backbone is\nconditioned on signal-specific latent variables - represented as point clouds\nin a Lie group - to model diverse Eikonal solutions. The ENF integration\nensures equivariant mapping from these latent representations to the solution\nfield, delivering three key benefits: enhanced representation efficiency\nthrough weight-sharing, robust geometric grounding, and solution steerability.\nThis steerability allows transformations applied to the latent point cloud to\ninduce predictable, geometrically meaningful modifications in the resulting\nEikonal solution. By coupling these steerable representations with\nPhysics-Informed Neural Networks (PINNs), our framework accurately models\nEikonal travel-time solutions while generalizing to arbitrary Riemannian\nmanifolds with regular group actions. This includes homogeneous spaces such as\nEuclidean, position-orientation, spherical, and hyperbolic manifolds. We\nvalidate our approach through applications in seismic travel-time modeling of\n2D and 3D benchmark datasets. Experimental results demonstrate superior\nperformance, scalability, adaptability, and user controllability compared to\nexisting Neural Operator-based Eikonal solver methods."}
{"id": "2505.16090", "pdf": "https://arxiv.org/pdf/2505.16090", "abs": "https://arxiv.org/abs/2505.16090", "authors": ["Dominick Kubica", "Dylan T. Gordon", "Nanami Emura", "Derleen Saini", "Charlie Goldenberg"], "title": "Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance", "categories": ["cs.AI", "cs.CL", "I.2.6; I.2.7"], "comment": "6 pages, 4 figures. Research conducted as part of a\n  Microsoft-sponsored Capstone Project at Santa Clara University", "summary": "As of 2025, Generative Artificial Intelligence (GenAI) has become a central\ntool for productivity across industries. Beyond text generation, GenAI now\nplays a critical role in coding, data analysis, and research workflows. As\nlarge language models (LLMs) continue to evolve, it is essential to assess the\nreliability and accuracy of their outputs, especially in specialized,\nhigh-stakes domains like finance. Most modern LLMs transform text into\nnumerical vectors, which are used in operations such as cosine similarity\nsearches to generate responses. However, this abstraction process can lead to\nmisinterpretation of emotional tone, particularly in nuanced financial\ncontexts. While LLMs generally excel at identifying sentiment in everyday\nlanguage, these models often struggle with the nuanced, strategically ambiguous\nlanguage found in earnings call transcripts. Financial disclosures frequently\nembed sentiment in hedged statements, forward-looking language, and\nindustry-specific jargon, making it difficult even for human analysts to\ninterpret consistently, let alone AI models. This paper presents findings from\nthe Santa Clara Microsoft Practicum Project, led by Professor Charlie\nGoldenberg, which benchmarks the performance of Microsoft's Copilot, OpenAI's\nChatGPT, Google's Gemini, and traditional machine learning models for sentiment\nanalysis of financial text. Using Microsoft earnings call transcripts, the\nanalysis assesses how well LLM-derived sentiment correlates with market\nsentiment and stock movements and evaluates the accuracy of model outputs.\nPrompt engineering techniques are also examined to improve sentiment analysis\nresults. Visualizations of sentiment consistency are developed to evaluate\nalignment between tone and stock performance, with sentiment trends analyzed\nacross Microsoft's lines of business to determine which segments exert the\ngreatest influence."}
{"id": "2505.16023", "pdf": "https://arxiv.org/pdf/2505.16023", "abs": "https://arxiv.org/abs/2505.16023", "authors": ["Sheshera Mysore", "Debarati Das", "Hancheng Cao", "Bahareh Sarrafzadeh"], "title": "Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild", "categories": ["cs.CL", "cs.HC"], "comment": "Pre-print under-review", "summary": "As large language models (LLMs) are used in complex writing workflows, users\nengage in multi-turn interactions to steer generations to better fit their\nneeds. Rather than passively accepting output, users actively refine, explore,\nand co-construct text. We conduct a large-scale analysis of this collaborative\nbehavior for users engaged in writing tasks in the wild with two popular AI\nassistants, Bing Copilot and WildChat. Our analysis goes beyond simple task\nclassification or satisfaction estimation common in prior work and instead\ncharacterizes how users interact with LLMs through the course of a session. We\nidentify prototypical behaviors in how users interact with LLMs in prompts\nfollowing their original request. We refer to these as Prototypical Human-AI\nCollaboration Behaviors (PATHs) and find that a small group of PATHs explain a\nmajority of the variation seen in user-LLM interaction. These PATHs span users\nrevising intents, exploring texts, posing questions, adjusting style or\ninjecting new content. Next, we find statistically significant correlations\nbetween specific writing intents and PATHs, revealing how users' intents shape\ntheir collaboration behaviors. We conclude by discussing the implications of\nour findings on LLM alignment."}
{"id": "2505.16053", "pdf": "https://arxiv.org/pdf/2505.16053", "abs": "https://arxiv.org/abs/2505.16053", "authors": ["Jan Tönshoff", "Martin Grohe"], "title": "Learning from Algorithm Feedback: One-Shot SAT Solver Guidance with GNNs", "categories": ["cs.LG"], "comment": null, "summary": "Boolean Satisfiability (SAT) solvers are foundational to computer science,\nyet their performance typically hinges on hand-crafted heuristics. This work\nintroduces Reinforcement Learning from Algorithm Feedback (RLAF) as a paradigm\nfor learning to guide SAT solver branching heuristics with Graph Neural\nNetworks (GNNs). Central to our approach is a novel and generic mechanism for\ninjecting inferred variable weights and polarities into the branching\nheuristics of existing SAT solvers. In a single forward pass, a GNN assigns\nthese parameters to all variables. Casting this one-shot guidance as a\nreinforcement learning problem lets us train the GNN with off-the-shelf\npolicy-gradient methods, such as GRPO, directly using the solver's\ncomputational cost as the sole reward signal. Extensive evaluations demonstrate\nthat RLAF-trained policies significantly reduce the mean solve times of\ndifferent base solvers across diverse SAT problem distributions, achieving more\nthan a 2x speedup in some cases, while generalizing effectively to larger and\nharder problems after training. Notably, these policies consistently outperform\nexpert-supervised approaches based on learning handcrafted weighting\nheuristics, offering a promising path towards data-driven heuristic design in\ncombinatorial optimization."}
{"id": "2505.16097", "pdf": "https://arxiv.org/pdf/2505.16097", "abs": "https://arxiv.org/abs/2505.16097", "authors": ["Zifeng Wang", "Qiao Jin", "Jiacheng Lin", "Junyi Gao", "Jathurshan Pradeepkumar", "Pengcheng Jiang", "Benjamin Danek", "Zhiyong Lu", "Jimeng Sun"], "title": "TrialPanorama: Database and Benchmark for Systematic Review and Design of Clinical Trials", "categories": ["cs.AI"], "comment": null, "summary": "Developing artificial intelligence (AI) for vertical domains requires a solid\ndata foundation for both training and evaluation. In this work, we introduce\nTrialPanorama, a large-scale, structured database comprising 1,657,476 clinical\ntrial records aggregated from 15 global sources. The database captures key\naspects of trial design and execution, including trial setups, interventions,\nconditions, biomarkers, and outcomes, and links them to standard biomedical\nontologies such as DrugBank and MedDRA. This structured and ontology-grounded\ndesign enables TrialPanorama to serve as a unified, extensible resource for a\nwide range of clinical trial tasks, including trial planning, design, and\nsummarization. To demonstrate its utility, we derive a suite of benchmark tasks\ndirectly from the TrialPanorama database. The benchmark spans eight tasks\nacross two categories: three for systematic review (study search, study\nscreening, and evidence summarization) and five for trial design (arm design,\neligibility criteria, endpoint selection, sample size estimation, and trial\ncompletion assessment). The experiments using five state-of-the-art large\nlanguage models (LLMs) show that while general-purpose LLMs exhibit some\nzero-shot capability, their performance is still inadequate for high-stakes\nclinical trial workflows. We release TrialPanorama database and the benchmark\nto facilitate further research on AI for clinical trials."}
{"id": "2505.16036", "pdf": "https://arxiv.org/pdf/2505.16036", "abs": "https://arxiv.org/abs/2505.16036", "authors": ["Burak Erinç Çetin", "Yıldırım Özen", "Elif Naz Demiryılmaz", "Kaan Engür", "Cagri Toraman"], "title": "OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Generative large language models present significant potential but also raise\ncritical ethical concerns. Most studies focus on narrow ethical dimensions, and\nalso limited diversity of languages and models. To address these gaps, we\nconduct a broad ethical evaluation of 29 recent open-source large language\nmodels using a novel data collection including four ethical aspects:\nRobustness, reliability, safety, and fairness. We analyze model behavior in\nboth a commonly used language, English, and a low-resource language, Turkish.\nOur aim is to provide a comprehensive ethical assessment and guide safer model\ndevelopment by filling existing gaps in evaluation breadth, language coverage,\nand model diversity. Our experimental results, based on LLM-as-a-Judge, reveal\nthat optimization efforts for many open-source models appear to have\nprioritized safety and fairness, and demonstrated good robustness while\nreliability remains a concern. We demonstrate that ethical evaluation can be\neffectively conducted independently of the language used. In addition, models\nwith larger parameter counts tend to exhibit better ethical performance, with\nGemma and Qwen models demonstrating the most ethical behavior among those\nevaluated."}
{"id": "2505.16056", "pdf": "https://arxiv.org/pdf/2505.16056", "abs": "https://arxiv.org/abs/2505.16056", "authors": ["Jingcong Liang", "Siyuan Wang", "Miren Tian", "Yitong Li", "Duyu Tang", "Zhongyu Wei"], "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."}
{"id": "2505.16100", "pdf": "https://arxiv.org/pdf/2505.16100", "abs": "https://arxiv.org/abs/2505.16100", "authors": ["Zifeng Wang", "Benjamin Danek", "Jimeng Sun"], "title": "BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Validating scientific hypotheses is a central challenge in biomedical\nresearch, and remains difficult for artificial intelligence (AI) agents due to\nthe complexity of real-world data analysis and evidence interpretation. In this\nwork, we present BioDSA-1K, a benchmark designed to evaluate AI agents on\nrealistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K\nconsists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans,\ncurated from over 300 published biomedical studies to reflect the structure and\nreasoning found in authentic research workflows. Each task includes a\nstructured hypothesis derived from the original study's conclusions, expressed\nin the affirmative to reflect the language of scientific reporting, and one or\nmore pieces of supporting evidence grounded in empirical data tables. While\nthese hypotheses mirror published claims, they remain testable using standard\nstatistical or machine learning methods. The benchmark enables evaluation along\nfour axes: (1) hypothesis decision accuracy, (2) alignment between evidence and\nconclusion, (3) correctness of the reasoning process, and (4) executability of\nthe AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable\nhypotheses: cases where the available data are insufficient to support or\nrefute a claim, reflecting a common yet underexplored scenario in real-world\nscience. We propose BioDSA-1K as a foundation for building and evaluating\ngeneralizable, trustworthy AI agents for biomedical discovery."}
{"id": "2505.16061", "pdf": "https://arxiv.org/pdf/2505.16061", "abs": "https://arxiv.org/abs/2505.16061", "authors": ["Yu Zhang"], "title": "Internal and External Impacts of Natural Language Processing Papers", "categories": ["cs.CL", "cs.DL"], "comment": "7 pages; Accepted to ACL 2025", "summary": "We investigate the impacts of NLP research published in top-tier conferences\n(i.e., ACL, EMNLP, and NAACL) from 1979 to 2024. By analyzing citations from\nresearch articles and external sources such as patents, media, and policy\ndocuments, we examine how different NLP topics are consumed both within the\nacademic community and by the broader public. Our findings reveal that language\nmodeling has the widest internal and external influence, while linguistic\nfoundations have lower impacts. We also observe that internal and external\nimpacts generally align, but topics like ethics, bias, and fairness show\nsignificant attention in policy documents with much fewer academic citations.\nAdditionally, external domains exhibit distinct preferences, with patents\nfocusing on practical NLP applications and media and policy documents engaging\nmore with the societal implications of NLP models."}
{"id": "2505.16058", "pdf": "https://arxiv.org/pdf/2505.16058", "abs": "https://arxiv.org/abs/2505.16058", "authors": ["Mars Liyao Gao", "J. Nathan Kutz", "Bernat Font"], "title": "Mesh-free sparse identification of nonlinear dynamics", "categories": ["cs.LG", "cs.AI", "physics.data-an"], "comment": "17 pages, 13 figures, 14 tables", "summary": "Identifying the governing equations of a dynamical system is one of the most\nimportant tasks for scientific modeling. However, this procedure often requires\nhigh-quality spatio-temporal data uniformly sampled on structured grids. In\nthis paper, we propose mesh-free SINDy, a novel algorithm which leverages the\npower of neural network approximation as well as auto-differentiation to\nidentify governing equations from arbitrary sensor placements and non-uniform\ntemporal data sampling. We show that mesh-free SINDy is robust to high noise\nlevels and limited data while remaining computationally efficient. In our\nimplementation, the training procedure is straight-forward and nearly free of\nhyperparameter tuning, making mesh-free SINDy widely applicable to many\nscientific and engineering problems. In the experiments, we demonstrate its\neffectiveness on a series of PDEs including the Burgers' equation, the heat\nequation, the Korteweg-De Vries equation and the 2D advection-diffusion\nequation. We conduct detailed numerical experiments on all datasets, varying\nthe noise levels and number of samples, and we also compare our approach to\nprevious state-of-the-art methods. It is noteworthy that, even in high-noise\nand low-data scenarios, mesh-free SINDy demonstrates robust PDE discovery,\nachieving successful identification with up to 75% noise for the Burgers'\nequation using 5,000 samples and with as few as 100 samples and 1% noise. All\nof this is achieved within a training time of under one minute."}
{"id": "2505.16114", "pdf": "https://arxiv.org/pdf/2505.16114", "abs": "https://arxiv.org/abs/2505.16114", "authors": ["Naiqi Li", "Peiyuan Liu", "Zheng Liu", "Tao Dai", "Yong Jiang", "Shu-Tao Xia"], "title": "Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language", "categories": ["cs.AI"], "comment": null, "summary": "Solving puzzles in natural language poses a long-standing challenge in AI.\nWhile large language models (LLMs) have recently shown impressive capabilities\nin a variety of tasks, they continue to struggle with complex puzzles that\ndemand precise reasoning and exhaustive search. In this paper, we propose\nLogic-of-Thought (Logot), a novel framework that bridges LLMs with logic\nprogramming to address this problem. Our method leverages LLMs to translate\npuzzle rules and states into answer set programs (ASPs), the solution of which\nare then accurately and efficiently inferred by an ASP interpreter. This hybrid\napproach combines the natural language understanding of LLMs with the precise\nreasoning capabilities of logic programs. We evaluate our method on various\ngrid puzzles and dynamic puzzles involving actions, demonstrating near-perfect\naccuracy across all tasks. Our code and data are available at:\nhttps://github.com/naiqili/Logic-of-Thought."}
{"id": "2505.16078", "pdf": "https://arxiv.org/pdf/2505.16078", "abs": "https://arxiv.org/abs/2505.16078", "authors": ["Lujun Li", "Lama Sleem", "Niccolo' Gentile", "Geoffrey Nichil", "Radu State"], "title": "Small Language Models in the Real World: Insights from Industrial Text Classification", "categories": ["cs.CL"], "comment": null, "summary": "With the emergence of ChatGPT, Transformer models have significantly advanced\ntext classification and related tasks. Decoder-only models such as Llama\nexhibit strong performance and flexibility, yet they suffer from inefficiency\non inference due to token-by-token generation, and their effectiveness in text\nclassification tasks heavily depends on prompt quality. Moreover, their\nsubstantial GPU resource requirements often limit widespread adoption. Thus,\nthe question of whether smaller language models are capable of effectively\nhandling text classification tasks emerges as a topic of significant interest.\nHowever, the selection of appropriate models and methodologies remains largely\nunderexplored. In this paper, we conduct a comprehensive evaluation of prompt\nengineering and supervised fine-tuning methods for transformer-based text\nclassification. Specifically, we focus on practical industrial scenarios,\nincluding email classification, legal document categorization, and the\nclassification of extremely long academic texts. We examine the strengths and\nlimitations of smaller models, with particular attention to both their\nperformance and their efficiency in Video Random-Access Memory (VRAM)\nutilization, thereby providing valuable insights for the local deployment and\napplication of compact models in industrial settings."}
{"id": "2505.16060", "pdf": "https://arxiv.org/pdf/2505.16060", "abs": "https://arxiv.org/abs/2505.16060", "authors": ["Shangding Gu", "Donghao Ying", "Ming Jin", "Yu Joe Lu", "Jun Wang", "Javad Lavaei", "Costas Spanos"], "title": "Few-Shot Test-Time Optimization Without Retraining for Semiconductor Recipe Generation and Beyond", "categories": ["cs.LG"], "comment": null, "summary": "We introduce Model Feedback Learning (MFL), a novel test-time optimization\nframework for optimizing inputs to pre-trained AI models or deployed hardware\nsystems without requiring any retraining of the models or modifications to the\nhardware. In contrast to existing methods that rely on adjusting model\nparameters, MFL leverages a lightweight reverse model to iteratively search for\noptimal inputs, enabling efficient adaptation to new objectives under\ndeployment constraints. This framework is particularly advantageous in\nreal-world settings, such as semiconductor manufacturing recipe generation,\nwhere modifying deployed systems is often infeasible or cost-prohibitive. We\nvalidate MFL on semiconductor plasma etching tasks, where it achieves target\nrecipe generation in just five iterations, significantly outperforming both\nBayesian optimization and human experts. Beyond semiconductor applications, MFL\nalso demonstrates strong performance in chemical processes (e.g., chemical\nvapor deposition) and electronic systems (e.g., wire bonding), highlighting its\nbroad applicability. Additionally, MFL incorporates stability-aware\noptimization, enhancing robustness to process variations and surpassing\nconventional supervised learning and random search methods in high-dimensional\ncontrol settings. By enabling few-shot adaptation, MFL provides a scalable and\nefficient paradigm for deploying intelligent control in real-world\nenvironments."}
{"id": "2505.16120", "pdf": "https://arxiv.org/pdf/2505.16120", "abs": "https://arxiv.org/abs/2505.16120", "authors": ["Guannan Liang", "Qianqian Tong"], "title": "LLM-Powered AI Agent Systems and Their Applications in Industry", "categories": ["cs.AI"], "comment": "This is the author's accepted version of the paper accepted to appear\n  at IEEE AIIoT 2025. The final version will be available via IEEE Xplore.\n  \\c{opyright}2025 IEEE. Personal use of this material is permitted", "summary": "The emergence of Large Language Models (LLMs) has reshaped agent systems.\nUnlike traditional rule-based agents with limited task scope, LLM-powered\nagents offer greater flexibility, cross-domain reasoning, and natural language\ninteraction. Moreover, with the integration of multi-modal LLMs, current agent\nsystems are highly capable of processing diverse data modalities, including\ntext, images, audio, and structured tabular data, enabling richer and more\nadaptive real-world behavior. This paper comprehensively examines the evolution\nof agent systems from the pre-LLM era to current LLM-powered architectures. We\ncategorize agent systems into software-based, physical, and adaptive hybrid\nsystems, highlighting applications across customer service, software\ndevelopment, manufacturing automation, personalized education, financial\ntrading, and healthcare. We further discuss the primary challenges posed by\nLLM-powered agents, including high inference latency, output uncertainty, lack\nof evaluation metrics, and security vulnerabilities, and propose potential\nsolutions to mitigate these concerns."}
{"id": "2505.16081", "pdf": "https://arxiv.org/pdf/2505.16081", "abs": "https://arxiv.org/abs/2505.16081", "authors": ["KMA Solaiman"], "title": "BiasLab: Toward Explainable Political Bias Detection with Dual-Axis Annotations and Rationale Indicators", "categories": ["cs.CL"], "comment": "Under review", "summary": "We present BiasLab, a dataset of 300 political news articles annotated for\nperceived ideological bias. These articles were selected from a curated\n900-document pool covering diverse political events and source biases. Each\narticle is labeled by crowdworkers along two independent scales, assessing\nsentiment toward the Democratic and Republican parties, and enriched with\nrationale indicators. The annotation pipeline incorporates targeted worker\nqualification and was refined through pilot-phase analysis. We quantify\ninter-annotator agreement, analyze misalignment with source-level outlet bias,\nand organize the resulting labels into interpretable subsets. Additionally, we\nsimulate annotation using schema-constrained GPT-4o, enabling direct comparison\nto human labels and revealing mirrored asymmetries, especially in\nmisclassifying subtly right-leaning content. We define two modeling tasks:\nperception drift prediction and rationale type classification, and report\nbaseline performance to illustrate the challenge of explainable bias detection.\nBiasLab's rich rationale annotations provide actionable interpretations that\nfacilitate explainable modeling of political bias, supporting the development\nof transparent, socially aware NLP systems. We release the dataset, annotation\nschema, and modeling code to encourage research on human-in-the-loop\ninterpretability and the evaluation of explanation effectiveness in real-world\nsettings."}
{"id": "2505.16066", "pdf": "https://arxiv.org/pdf/2505.16066", "abs": "https://arxiv.org/abs/2505.16066", "authors": ["Zhixu Silvia Tao", "Kasper Vinken", "Hao-Wei Yeh", "Avi Cooper", "Xavier Boix"], "title": "Merge to Mix: Mixing Datasets via Model Merging", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Mixing datasets for fine-tuning large models (LMs) has become critical for\nmaximizing performance on downstream tasks. However, composing effective\ndataset mixtures typically relies on heuristics and trial-and-error, often\nrequiring multiple fine-tuning runs to achieve the desired outcome. We propose\na novel method, $\\textit{Merge to Mix}$, that accelerates composing dataset\nmixtures through model merging. Model merging is a recent technique that\ncombines the abilities of multiple individually fine-tuned LMs into a single LM\nby using a few simple arithmetic operations. Our key insight is that merging\nmodels individually fine-tuned on each dataset in a mixture can effectively\nserve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix\nleverages this insight to accelerate selecting dataset mixtures without\nrequiring full fine-tuning on each candidate mixture. Our experiments\ndemonstrate that Merge to Mix surpasses state-of-the-art methods in dataset\nselection for fine-tuning LMs."}
{"id": "2505.16135", "pdf": "https://arxiv.org/pdf/2505.16135", "abs": "https://arxiv.org/abs/2505.16135", "authors": ["Jeffrey Seely", "Yuki Imajuku", "Tianyu Zhao", "Edoardo Cetin", "Llion Jones"], "title": "Sudoku-Bench: Evaluating creative reasoning with Sudoku variants", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Existing reasoning benchmarks for large language models (LLMs) frequently\nfail to capture authentic creativity, often rewarding memorization of\npreviously observed patterns. We address this shortcoming with Sudoku-Bench, a\ncurated benchmark of challenging and unconventional Sudoku variants\nspecifically selected to evaluate creative, multi-step logical reasoning.\nSudoku variants form an unusually effective domain for reasoning research: each\npuzzle introduces unique or subtly interacting constraints, making memorization\ninfeasible and requiring solvers to identify novel logical breakthroughs\n(``break-ins''). Despite their diversity, Sudoku variants maintain a common and\ncompact structure, enabling clear and consistent evaluation. Sudoku-Bench\nincludes a carefully chosen puzzle set, a standardized text-based puzzle\nrepresentation, and flexible tools compatible with thousands of publicly\navailable puzzles -- making it easy to extend into a general research\nenvironment. Baseline experiments show that state-of-the-art LLMs solve fewer\nthan 15\\% of puzzles unaided, highlighting significant opportunities to advance\nlong-horizon, strategic reasoning capabilities."}
{"id": "2505.16088", "pdf": "https://arxiv.org/pdf/2505.16088", "abs": "https://arxiv.org/abs/2505.16088", "authors": ["Gagan Bhatia", "Maxime Peyrard", "Wei Zhao"], "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern BPE tokenizers often split calendar dates into meaningless fragments,\ne.g., 20250312 $\\rightarrow$ 202, 503, 12, inflating token counts and obscuring\nthe inherent structure needed for robust temporal reasoning. In this work, we\n(1) introduce a simple yet interpretable metric, termed date fragmentation\nratio, that measures how faithfully a tokenizer preserves multi-digit date\ncomponents; (2) release DateAugBench, a suite of 6500 examples spanning three\ntemporal reasoning tasks: context-based date resolution, format-invariance\npuzzles, and date arithmetic across historical, contemporary, and future\nregimes; and (3) through layer-wise probing and causal attention-hop analyses,\nuncover an emergent date-abstraction mechanism whereby large language models\nstitch together the fragments of month, day, and year components for temporal\nreasoning. Our experiments show that excessive fragmentation correlates with\naccuracy drops of up to 10 points on uncommon dates like historical and\nfuturistic dates. Further, we find that the larger the model, the faster the\nemergent date abstraction that heals date fragments is accomplished. Lastly, we\nobserve a reasoning path that LLMs follow to assemble date fragments, typically\ndiffering from human interpretation (year $\\rightarrow$ month $\\rightarrow$\nday)."}
{"id": "2505.16074", "pdf": "https://arxiv.org/pdf/2505.16074", "abs": "https://arxiv.org/abs/2505.16074", "authors": ["Bart Kosko", "Olaoluwa Adigun"], "title": "Bidirectional Variational Autoencoders", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "10 pages, 6 figures", "summary": "We present the new bidirectional variational autoencoder (BVAE) network\narchitecture. The BVAE uses a single neural network both to encode and decode\ninstead of an encoder-decoder network pair. The network encodes in the forward\ndirection and decodes in the backward direction through the same synaptic web.\nSimulations compared BVAEs and ordinary VAEs on the four image tasks of image\nreconstruction, classification, interpolation, and generation. The image\ndatasets included MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and\nCelebA-64 face images. The bidirectional structure of BVAEs cut the parameter\ncount by almost 50% and still slightly outperformed the unidirectional VAEs."}
{"id": "2505.16147", "pdf": "https://arxiv.org/pdf/2505.16147", "abs": "https://arxiv.org/abs/2505.16147", "authors": ["Le Ma", "Shirao Yang", "Zihao Wang", "Yinggui Wang", "Lei Wang", "Tao Wei", "Kejun Zhang"], "title": "Losing is for Cherishing: Data Valuation Based on Machine Unlearning and Shapley Value", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "The proliferation of large models has intensified the need for efficient data\nvaluation methods to quantify the contribution of individual data providers.\nTraditional approaches, such as game-theory-based Shapley value and\ninfluence-function-based techniques, face prohibitive computational costs or\nrequire access to full data and model training details, making them hardly\nachieve partial data valuation. To address this, we propose Unlearning Shapley,\na novel framework that leverages machine unlearning to estimate data values\nefficiently. By unlearning target data from a pretrained model and measuring\nperformance shifts on a reachable test set, our method computes Shapley values\nvia Monte Carlo sampling, avoiding retraining and eliminating dependence on\nfull data. Crucially, Unlearning Shapley supports both full and partial data\nvaluation, making it scalable for large models (e.g., LLMs) and practical for\ndata markets. Experiments on benchmark datasets and large-scale text corpora\ndemonstrate that our approach matches the accuracy of state-of-the-art methods\nwhile reducing computational overhead by orders of magnitude. Further analysis\nconfirms a strong correlation between estimated values and the true impact of\ndata subsets, validating its reliability in real-world scenarios. This work\nbridges the gap between data valuation theory and practical deployment,\noffering a scalable, privacy-compliant solution for modern AI ecosystems."}
{"id": "2505.16102", "pdf": "https://arxiv.org/pdf/2505.16102", "abs": "https://arxiv.org/abs/2505.16102", "authors": ["Yash Kumar Atri", "Thomas H Shin", "Thomas Hartvigsen"], "title": "Continually Self-Improving Language Models for Bariatric Surgery Question--Answering", "categories": ["cs.CL"], "comment": null, "summary": "While bariatric and metabolic surgery (MBS) is considered the gold standard\ntreatment for severe and morbid obesity, its therapeutic efficacy hinges upon\nactive and longitudinal engagement with multidisciplinary providers, including\nsurgeons, dietitians/nutritionists, psychologists, and endocrinologists. This\nengagement spans the entire patient journey, from preoperative preparation to\nlong-term postoperative management. However, this process is often hindered by\nnumerous healthcare disparities, such as logistical and access barriers, which\nimpair easy patient access to timely, evidence-based, clinician-endorsed\ninformation. To address these gaps, we introduce bRAGgen, a novel adaptive\nretrieval-augmented generation (RAG)-based model that autonomously integrates\nreal-time medical evidence when response confidence dips below dynamic\nthresholds. This self-updating architecture ensures that responses remain\ncurrent and accurate, reducing the risk of misinformation. Additionally, we\npresent bRAGq, a curated dataset of 1,302 bariatric surgery--related questions,\nvalidated by an expert bariatric surgeon. bRAGq constitutes the first\nlarge-scale, domain-specific benchmark for comprehensive MBS care. In a\ntwo-phase evaluation, bRAGgen is benchmarked against state-of-the-art models\nusing both large language model (LLM)--based metrics and expert surgeon review.\nAcross all evaluation dimensions, bRAGgen demonstrates substantially superior\nperformance in generating clinically accurate and relevant responses."}
{"id": "2505.16077", "pdf": "https://arxiv.org/pdf/2505.16077", "abs": "https://arxiv.org/abs/2505.16077", "authors": ["Soham Gadgil", "Chris Lin", "Su-In Lee"], "title": "Ensembling Sparse Autoencoders", "categories": ["cs.LG"], "comment": "Preprint", "summary": "Sparse autoencoders (SAEs) are used to decompose neural network activations\ninto human-interpretable features. Typically, features learned by a single SAE\nare used for downstream applications. However, it has recently been shown that\nSAEs trained with different initial weights can learn different features,\ndemonstrating that a single SAE captures only a limited subset of features that\ncan be extracted from the activation space. Motivated by this limitation, we\npropose to ensemble multiple SAEs through naive bagging and boosting.\nSpecifically, SAEs trained with different weight initializations are ensembled\nin naive bagging, whereas SAEs sequentially trained to minimize the residual\nerror are ensembled in boosting. We evaluate our ensemble approaches with three\nsettings of language models and SAE architectures. Our empirical results\ndemonstrate that ensembling SAEs can improve the reconstruction of language\nmodel activations, diversity of features, and SAE stability. Furthermore,\nensembling SAEs performs better than applying a single SAE on downstream tasks\nsuch as concept detection and spurious correlation removal, showing improved\npractical utility."}
{"id": "2505.16176", "pdf": "https://arxiv.org/pdf/2505.16176", "abs": "https://arxiv.org/abs/2505.16176", "authors": ["Jun Rao", "Xuebo Liu", "Hexuan Deng", "Zepeng Lin", "Zixiong Yu", "Jiansheng Wei", "Xiaojun Meng", "Min Zhang"], "title": "Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In the realm of data selection for reasoning tasks, existing approaches\npredominantly rely on externally predefined static metrics such as difficulty\nand diversity, which are often designed for supervised fine-tuning (SFT) and\nlack adaptability to continuous training processes. A critical limitation of\nthese methods is their inability to dynamically align with the evolving\ncapabilities of models during online training, a gap that becomes increasingly\npronounced with the rise of dynamic training paradigms and online reinforcement\nlearning (RL) frameworks (e.g., R1 models). To address this, we introduce\nSAI-DPO, an algorithm that dynamically selects training data by continuously\nassessing a model's stage-specific reasoning abilities across different\ntraining phases. By integrating real-time model performance feedback, SAI-DPO\nadaptively adapts data selection to the evolving strengths and weaknesses of\nthe model, thus enhancing both data utilization efficiency and final task\nperformance. Extensive experiments on three state-of-the-art models and eight\nmathematical reasoning benchmarks, including challenging competition-level\ndatasets (e.g., AIME24 and AMC23), demonstrate that SAI-DPO achieves an average\nperformance boost of up to 21.3 percentage points, with particularly notable\nimprovements of 10 and 15 points on AIME24 and AMC23, respectively. These\nresults highlight the superiority of dynamic, model-adaptive data selection\nover static, externally defined strategies in advancing reasoning."}
{"id": "2505.16104", "pdf": "https://arxiv.org/pdf/2505.16104", "abs": "https://arxiv.org/abs/2505.16104", "authors": ["Yue Li", "Xin Yi", "Dongsheng Shi", "Gerard de Melo", "Xiaoling Wang", "Linlin Wang"], "title": "Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "ACL 2025 Findings", "summary": "With the increasing size of Large Vision-Language Models (LVLMs), network\npruning techniques aimed at compressing models for deployment in\nresource-constrained environments have garnered significant attention. However,\nwe observe that pruning often leads to a degradation in safety performance. To\naddress this issue, we present a novel and lightweight approach, termed\nHierarchical Safety Realignment (HSR). HSR operates by first quantifying the\ncontribution of each attention head to safety, identifying the most critical\nones, and then selectively restoring neurons directly within these attention\nheads that play a pivotal role in maintaining safety. This process\nhierarchically realigns the safety of pruned LVLMs, progressing from the\nattention head level to the neuron level. We validate HSR across various models\nand pruning strategies, consistently achieving notable improvements in safety\nperformance. To our knowledge, this is the first work explicitly focused on\nrestoring safety in LVLMs post-pruning."}
{"id": "2505.16083", "pdf": "https://arxiv.org/pdf/2505.16083", "abs": "https://arxiv.org/abs/2505.16083", "authors": ["Jiahuan Long", "Wenzhe Zhang", "Ning Wang", "Tingsong Jiang", "Wen Yao"], "title": "FR-Mamba: Time-Series Physical Field Reconstruction Based on State Space Model", "categories": ["cs.LG"], "comment": null, "summary": "Physical field reconstruction (PFR) aims to predict the state distribution of\nphysical quantities (e.g., velocity, pressure, and temperature) based on\nlimited sensor measurements. It plays a critical role in domains such as fluid\ndynamics and thermodynamics. However, existing deep learning methods often fail\nto capture long-range temporal dependencies, resulting in suboptimal\nperformance on time-evolving physical systems. To address this, we propose\nFR-Mamba, a novel spatiotemporal flow field reconstruction framework based on\nstate space modeling. Specifically, we design a hybrid neural network\narchitecture that combines Fourier Neural Operator (FNO) and State Space Model\n(SSM) to capture both global spatial features and long-range temporal\ndependencies. We adopt Mamba, a recently proposed efficient SSM architecture,\nto model long-range temporal dependencies with linear time complexity. In\nparallel, the FNO is employed to capture non-local spatial features by\nleveraging frequency-domain transformations. The spatiotemporal representations\nextracted by these two components are then fused to reconstruct the full-field\ndistribution of the physical system. Extensive experiments demonstrate that our\napproach significantly outperforms existing PFR methods in flow field\nreconstruction tasks, achieving high-accuracy performance on long sequences."}
{"id": "2505.16186", "pdf": "https://arxiv.org/pdf/2505.16186", "abs": "https://arxiv.org/abs/2505.16186", "authors": ["Kaiwen Zhou", "Xuandong Zhao", "Gaowen Liu", "Jayanth Srinivasa", "Aosong Feng", "Dawn Song", "Xin Eric Wang"], "title": "SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning", "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Large Reasoning Models (LRMs) introduce a new generation paradigm of\nexplicitly reasoning before answering, leading to remarkable improvements in\ncomplex tasks. However, they pose great safety risks against harmful queries\nand adversarial attacks. While recent mainstream safety efforts on LRMs,\nsupervised fine-tuning (SFT), improve safety performance, we find that\nSFT-aligned models struggle to generalize to unseen jailbreak prompts. After\nthorough investigation of LRMs' generation, we identify a safety aha moment\nthat can activate safety reasoning and lead to a safe response. This aha moment\ntypically appears in the `key sentence', which follows models' query\nunderstanding process and can indicate whether the model will proceed safely.\nBased on these insights, we propose SafeKey, including two complementary\nobjectives to better activate the safety aha moment in the key sentence: (1) a\nDual-Path Safety Head to enhance the safety signal in the model's internal\nrepresentations before the key sentence, and (2) a Query-Mask Modeling\nobjective to improve the models' attention on its query understanding, which\nhas important safety hints. Experiments across multiple safety benchmarks\ndemonstrate that our methods significantly improve safety generalization to a\nwide range of jailbreak attacks and out-of-distribution harmful prompts,\nlowering the average harmfulness rate by 9.6\\%, while maintaining general\nabilities. Our analysis reveals how SafeKey enhances safety by reshaping\ninternal attention and improving the quality of hidden representations."}
{"id": "2505.16107", "pdf": "https://arxiv.org/pdf/2505.16107", "abs": "https://arxiv.org/abs/2505.16107", "authors": ["Bo Li", "Gexiang Fang", "Wei Ye", "Zhenghua Xu", "Jinglei Zhang", "Hao Cheng", "Shikun Zhang"], "title": "MPL: Multiple Programming Languages with Large Language Models for Information Extraction", "categories": ["cs.CL"], "comment": "Findings of ACL2025", "summary": "Recent research in information extraction (IE) focuses on utilizing\ncode-style inputs to enhance structured output generation. The intuition behind\nthis is that the programming languages (PLs) inherently exhibit greater\nstructural organization than natural languages (NLs). This structural advantage\nmakes PLs particularly suited for IE tasks. Nevertheless, existing research\nprimarily focuses on Python for code-style simulation, overlooking the\npotential of other widely-used PLs (e.g., C++ and Java) during the supervised\nfine-tuning (SFT) phase. In this research, we propose \\textbf{M}ultiple\n\\textbf{P}rogramming \\textbf{L}anguages with large language models for\ninformation extraction (abbreviated as \\textbf{MPL}), a novel framework that\nexplores the potential of incorporating different PLs in the SFT phase.\nAdditionally, we introduce \\texttt{function-prompt} with virtual running to\nsimulate code-style inputs more effectively and efficiently. Experimental\nresults on a wide range of datasets demonstrate the effectiveness of MPL.\nFurthermore, we conduct extensive experiments to provide a comprehensive\nanalysis. We have released our code for future research."}
{"id": "2505.16094", "pdf": "https://arxiv.org/pdf/2505.16094", "abs": "https://arxiv.org/abs/2505.16094", "authors": ["Ziqing Wang", "Kexin Zhang", "Zihan Zhao", "Yibo Wen", "Abhishek Pandey", "Han Liu", "Kaize Ding"], "title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization", "categories": ["cs.LG", "cs.CL"], "comment": "Under review", "summary": "Large language models (LLMs) are introducing a paradigm shift in molecular\ndiscovery by enabling text-guided interaction with chemical spaces through\nnatural language, symbolic notations, with emerging extensions to incorporate\nmulti-modal inputs. To advance the new field of LLM for molecular discovery,\nthis survey provides an up-to-date and forward-looking review of the emerging\nuse of LLMs for two central tasks: molecule generation and molecule\noptimization. Based on our proposed taxonomy for both problems, we analyze\nrepresentative techniques in each category, highlighting how LLM capabilities\nare leveraged across different learning settings. In addition, we include the\ncommonly used datasets and evaluation protocols. We conclude by discussing key\nchallenges and future directions, positioning this survey as a resource for\nresearchers working at the intersection of LLMs and molecular science. A\ncontinuously updated reading list is available at\nhttps://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery."}
{"id": "2505.16199", "pdf": "https://arxiv.org/pdf/2505.16199", "abs": "https://arxiv.org/abs/2505.16199", "authors": ["Rikuhei Umemoto", "Keisuke Fujii"], "title": "Velocity Completion Task and Method for Event-based Player Positional Data in Soccer", "categories": ["cs.AI"], "comment": "24 pages, 7 figures", "summary": "In many real-world complex systems, the behavior can be observed as a\ncollection of discrete events generated by multiple interacting agents.\nAnalyzing the dynamics of these multi-agent systems, especially team sports,\noften relies on understanding the movement and interactions of individual\nagents. However, while providing valuable snapshots, event-based positional\ndata typically lacks the continuous temporal information needed to directly\ncalculate crucial properties such as velocity. This absence severely limits the\ndepth of dynamic analysis, preventing a comprehensive understanding of\nindividual agent behaviors and emergent team strategies. To address this\nchallenge, we propose a new method to simultaneously complete the velocity of\nall agents using only the event-based positional data from team sports. Based\non this completed velocity information, we investigate the applicability of\nexisting team sports analysis and evaluation methods. Experiments using soccer\nevent data demonstrate that neural network-based approaches outperformed\nrule-based methods regarding velocity completion error, considering the\nunderlying temporal dependencies and graph structure of player-to-player or\nplayer-to-ball interaction. Moreover, the space evaluation results obtained\nusing the completed velocity are closer to those derived from complete tracking\ndata, highlighting our method's potential for enhanced team sports system\nanalysis."}
{"id": "2505.16118", "pdf": "https://arxiv.org/pdf/2505.16118", "abs": "https://arxiv.org/abs/2505.16118", "authors": ["Haotian Lan", "Yao Gao", "Yujun Cheng", "Wei Yuan", "Kun Wang"], "title": "Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics", "categories": ["cs.CL", "stat.AP"], "comment": "33 pages, 6 figures", "summary": "Social media's rise establishes user-generated content (UGC) as pivotal for\ntravel decisions, yet analytical methods lack scalability. This study\nintroduces a dual-method LLM framework: unsupervised expectation extraction\nfrom UGC paired with survey-informed supervised fine-tuning. Findings reveal\nleisure/social expectations drive engagement more than foundational\nnatural/emotional factors. By establishing LLMs as precision tools for\nexpectation quantification, we advance tourism analytics methodology and\npropose targeted strategies for experience personalization and social travel\npromotion. The framework's adaptability extends to consumer behavior research,\ndemonstrating computational social science's transformative potential in\nmarketing optimization."}
{"id": "2505.16099", "pdf": "https://arxiv.org/pdf/2505.16099", "abs": "https://arxiv.org/abs/2505.16099", "authors": ["Ziyi", "Zhou", "Nicholas Stern", "Julien Laasri"], "title": "Reinforcement Learning for Stock Transactions", "categories": ["cs.LG", "68T05", "I.2.6"], "comment": "14 pages, 6 figures, paper dated December 19, 2018", "summary": "Much research has been done to analyze the stock market. After all, if one\ncan determine a pattern in the chaotic frenzy of transactions, then they could\nmake a hefty profit from capitalizing on these insights. As such, the goal of\nour project was to apply reinforcement learning (RL) to determine the best time\nto buy a stock within a given time frame. With only a few adjustments, our\nmodel can be extended to identify the best time to sell a stock as well. In\norder to use the format of free, real-world data to train the model, we define\nour own Markov Decision Process (MDP) problem. These two papers [5] [6] helped\nus in formulating the state space and the reward system of our MDP problem. We\ntrain a series of agents using Q-Learning, Q-Learning with linear function\napproximation, and deep Q-Learning. In addition, we try to predict the stock\nprices using machine learning regression and classification models. We then\ncompare our agents to see if they converge on a policy, and if so, which one\nlearned the best policy to maximize profit on the stock market."}
{"id": "2505.16221", "pdf": "https://arxiv.org/pdf/2505.16221", "abs": "https://arxiv.org/abs/2505.16221", "authors": ["Yifan Zhang", "Xinkui Zhao", "Zuxin Wang", "Guanjie Cheng", "Yueshen Xu", "Shuiguang Deng", "Jianwei Yin"], "title": "LightRouter: Towards Efficient LLM Collaboration with Minimal Overhead", "categories": ["cs.AI"], "comment": null, "summary": "The rapid advancement of large language models has unlocked remarkable\ncapabilities across a diverse array of natural language processing tasks.\nHowever, the considerable differences among available LLMs-in terms of cost,\nperformance, and computational demands-pose significant challenges for users\naiming to identify the most suitable model for specific tasks. In this work, we\npresent LightRouter, a novel framework designed to systematically select and\nintegrate a small subset of LLMs from a larger pool, with the objective of\njointly optimizing both task performance and cost efficiency. LightRouter\nleverages an adaptive selection mechanism to identify models that require only\na minimal number of boot tokens, thereby reducing costs, and further employs an\neffective integration strategy to combine their outputs. Extensive experiments\nacross multiple benchmarks demonstrate that LightRouter matches or outperforms\nwidely-used ensemble baselines, achieving up to a 25% improvement in accuracy.\nCompared with leading high-performing models, LightRouter achieves comparable\nperformance while reducing inference costs by up to 27%. Importantly, our\nframework operates without any prior knowledge of individual models and relies\nexclusively on inexpensive, lightweight models. This work introduces a\npractical approach for efficient LLM selection and provides valuable insights\ninto optimal strategies for model combination."}
{"id": "2505.16125", "pdf": "https://arxiv.org/pdf/2505.16125", "abs": "https://arxiv.org/abs/2505.16125", "authors": ["Hyopil Shin", "Sangah Lee", "Dongjun Jang", "Wooseok Song", "Jaeyoon Kim", "Chaeyoung Oh", "Hyemi Jo", "Youngchae Ahn", "Sihyun Oh", "Hyohyeong Chang", "Sunkyoung Kim", "Jinsik Lee"], "title": "KoBALT: Korean Benchmark For Advanced Linguistic Tasks", "categories": ["cs.CL"], "comment": "Under Reveiw", "summary": "We introduce KoBALT (Korean Benchmark for Advanced Linguistic Tasks), a\ncomprehensive linguistically-motivated benchmark comprising 700 multiple-choice\nquestions spanning 24 phenomena across five linguistic domains: syntax,\nsemantics, pragmatics, phonetics/phonology, and morphology. KoBALT is designed\nto advance the evaluation of large language models (LLMs) in Korean, a\nmorphologically rich language, by addressing the limitations of conventional\nbenchmarks that often lack linguistic depth and typological grounding. It\nintroduces a suite of expert-curated, linguistically motivated questions with\nminimal n-gram overlap with standard Korean corpora, substantially mitigating\nthe risk of data contamination and allowing a more robust assessment of true\nlanguage understanding. Our evaluation of 20 contemporary LLMs reveals\nsignificant performance disparities, with the highest-performing model\nachieving 61\\% general accuracy but showing substantial variation across\nlinguistic domains - from stronger performance in semantics (66\\%) to\nconsiderable weaknesses in phonology (31\\%) and morphology (36\\%). Through\nhuman preference evaluation with 95 annotators, we demonstrate a strong\ncorrelation between KoBALT scores and human judgments, validating our\nbenchmark's effectiveness as a discriminative measure of Korean language\nunderstanding. KoBALT addresses critical gaps in linguistic evaluation for\ntypologically diverse languages and provides a robust framework for assessing\ngenuine linguistic competence in Korean language models."}
{"id": "2505.16103", "pdf": "https://arxiv.org/pdf/2505.16103", "abs": "https://arxiv.org/abs/2505.16103", "authors": ["Monirul Islam Mahmud"], "title": "Towards Trustworthy Keylogger detection: A Comprehensive Analysis of Ensemble Techniques and Feature Selections through Explainable AI", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Keylogger detection involves monitoring for unusual system behaviors such as\ndelays between typing and character display, analyzing network traffic patterns\nfor data exfiltration. In this study, we provide a comprehensive analysis for\nkeylogger detection with traditional machine learning models - SVC, Random\nForest, Decision Tree, XGBoost, AdaBoost, Logistic Regression and Naive Bayes\nand advanced ensemble methods including Stacking, Blending and Voting.\nMoreover, feature selection approaches such as Information gain, Lasso L1 and\nFisher Score are thoroughly assessed to improve predictive performance and\nlower computational complexity. The Keylogger Detection dataset from publicly\navailable Kaggle website is used in this project. In addition to accuracy-based\nclassification, this study implements the approach for model interpretation\nusing Explainable AI (XAI) techniques namely SHAP (Global) and LIME (Local) to\ndeliver finer explanations for how much each feature contributes in assisting\nor hindering the detection process. To evaluate the models result, we have used\nAUC score, sensitivity, Specificity, Accuracy and F1 score. The best\nperformance was achieved by AdaBoost with 99.76% accuracy, F1 score of 0.99,\n100% precision, 98.6% recall, 1.0 specificity and 0.99 of AUC that is\nnear-perfect classification with Fisher Score."}
{"id": "2505.16223", "pdf": "https://arxiv.org/pdf/2505.16223", "abs": "https://arxiv.org/abs/2505.16223", "authors": ["Sangyong Lee", "Subo Hwang", "Dohoon Kim"], "title": "MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network", "categories": ["cs.AI", "cs.LG"], "comment": "24 pages, 9 figures", "summary": "In this paper, we propose MADCluster, a novel model-agnostic anomaly\ndetection framework utilizing self-supervised clustering. MADCluster is\napplicable to various deep learning architectures and addresses the\n'hypersphere collapse' problem inherent in existing deep learning-based anomaly\ndetection methods. The core idea is to cluster normal pattern data into a\n'single cluster' while simultaneously learning the cluster center and mapping\ndata close to this center. Also, to improve expressiveness and enable effective\nsingle clustering, we propose a new 'One-directed Adaptive loss'. The\noptimization of this loss is mathematically proven. MADCluster consists of\nthree main components: Base Embedder capturing high-dimensional temporal\ndynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous\ncenter updates. Its model-agnostic characteristics are achieved by applying\nvarious architectures to the Base Embedder. Experiments on four time series\nbenchmark datasets demonstrate that applying MADCluster improves the overall\nperformance of comparative models. In conclusion, the compatibility of\nMADCluster shows potential for enhancing model performance across various\narchitectures."}
{"id": "2505.16128", "pdf": "https://arxiv.org/pdf/2505.16128", "abs": "https://arxiv.org/abs/2505.16128", "authors": ["Yue Zhou", "Barbara Di Eugenio"], "title": "Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Despite LLMs' explicit alignment against demographic stereotypes, they have\nbeen shown to exhibit biases under various social contexts. In this work, we\nfind that LLMs exhibit concerning biases in how they associate solution\nveracity with demographics. Through experiments across five human value-aligned\nLLMs on mathematics, coding, commonsense, and writing problems, we reveal two\nforms of such veracity biases: Attribution Bias, where models\ndisproportionately attribute correct solutions to certain demographic groups,\nand Evaluation Bias, where models' assessment of identical solutions varies\nbased on perceived demographic authorship. Our results show pervasive biases:\nLLMs consistently attribute fewer correct solutions and more incorrect ones to\nAfrican-American groups in math and coding, while Asian authorships are least\npreferred in writing evaluation. In additional studies, we show LLMs\nautomatically assign racially stereotypical colors to demographic groups in\nvisualization code, suggesting these biases are deeply embedded in models'\nreasoning processes. Our findings indicate that demographic bias extends beyond\nsurface-level stereotypes and social context provocations, raising concerns\nabout LLMs' deployment in educational and evaluation settings."}
{"id": "2505.16113", "pdf": "https://arxiv.org/pdf/2505.16113", "abs": "https://arxiv.org/abs/2505.16113", "authors": ["Panagiotis Lymperopoulos", "Vasanth Sarathy"], "title": "Tools in the Loop: Quantifying Uncertainty of LLM Question Answering Systems That Use Tools", "categories": ["cs.LG", "cs.CL"], "comment": "10 pages 3 figures 3 tables", "summary": "Modern Large Language Models (LLMs) often require external tools, such as\nmachine learning classifiers or knowledge retrieval systems, to provide\naccurate answers in domains where their pre-trained knowledge is insufficient.\nThis integration of LLMs with external tools expands their utility but also\nintroduces a critical challenge: determining the trustworthiness of responses\ngenerated by the combined system. In high-stakes applications, such as medical\ndecision-making, it is essential to assess the uncertainty of both the LLM's\ngenerated text and the tool's output to ensure the reliability of the final\nresponse. However, existing uncertainty quantification methods do not account\nfor the tool-calling scenario, where both the LLM and external tool contribute\nto the overall system's uncertainty. In this work, we present a novel framework\nfor modeling tool-calling LLMs that quantifies uncertainty by jointly\nconsidering the predictive uncertainty of the LLM and the external tool. We\nextend previous methods for uncertainty quantification over token sequences to\nthis setting and propose efficient approximations that make uncertainty\ncomputation practical for real-world applications. We evaluate our framework on\ntwo new synthetic QA datasets, derived from well-known machine learning\ndatasets, which require tool-calling for accurate answers. Additionally, we\napply our method to retrieval-augmented generation (RAG) systems and conduct a\nproof-of-concept experiment demonstrating the effectiveness of our uncertainty\nmetrics in scenarios where external information retrieval is needed. Our\nresults show that the framework is effective in enhancing trust in LLM-based\nsystems, especially in cases where the LLM's internal knowledge is insufficient\nand external tools are required."}
{"id": "2505.16225", "pdf": "https://arxiv.org/pdf/2505.16225", "abs": "https://arxiv.org/abs/2505.16225", "authors": ["Zihan Chen", "Song Wang", "Zhen Tan", "Jundong Li", "Cong Shen"], "title": "MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning", "categories": ["cs.AI"], "comment": null, "summary": "In-Context Learning (ICL) empowers Large Language Models (LLMs) to tackle\ndiverse tasks by incorporating multiple input-output examples, known as\ndemonstrations, into the input of LLMs. More recently, advancements in the\nexpanded context windows of LLMs have led to many-shot ICL, which uses hundreds\nof demonstrations and outperforms few-shot ICL, which relies on fewer examples.\nHowever, this approach is often hindered by the high cost of obtaining large\namounts of labeled data. To address this challenge, we propose Many-Shot\nAdaptive Pseudo-LabEling, namely MAPLE, a novel influence-based many-shot ICL\nframework that utilizes pseudo-labeled samples to compensate for the lack of\nlabel information. We first identify a subset of impactful unlabeled samples\nand perform pseudo-labeling on them by querying LLMs. These pseudo-labeled\nsamples are then adaptively selected and tailored to each test query as input\nto improve the performance of many-shot ICL, without significant labeling\ncosts. Extensive experiments on real-world datasets demonstrate the\neffectiveness of our framework, showcasing its ability to enhance LLM\nadaptability and performance with limited labeled data."}
{"id": "2505.16129", "pdf": "https://arxiv.org/pdf/2505.16129", "abs": "https://arxiv.org/abs/2505.16129", "authors": ["Hyang Cui"], "title": "LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods", "categories": ["cs.CL", "I.2.7"], "comment": "5 pages, 2 figures, 2 tables. Conforms to the ACL Rolling Review\n  (ARR) short paper track. Code and data available at:\n  https://github.com/CuiNiki/LLMs-Are-Not-Scorers", "summary": "Recent studies have applied large language models (LLMs) to machine\ntranslation quality estimation (MTQE) by prompting models to assign numeric\nscores. Nonetheless, these direct scoring methods tend to show low\nsegment-level correlation with human judgments. In this paper, we propose a\ngeneration-based evaluation paradigm that leverages decoder-only LLMs to\nproduce high-quality references, followed by semantic similarity scoring using\nsentence embeddings. We conduct the most extensive evaluation to date in MTQE,\ncovering 8 LLMs and 8 language pairs. Empirical results show that our method\noutperforms both intra-LLM direct scoring baselines and external non-LLM\nreference-free metrics from MTME. These findings demonstrate the strength of\ngeneration-based evaluation and support a shift toward hybrid approaches that\ncombine fluent generation with accurate semantic assessment."}
{"id": "2505.16115", "pdf": "https://arxiv.org/pdf/2505.16115", "abs": "https://arxiv.org/abs/2505.16115", "authors": ["Aditya T. Vadlamani", "Anutam Srinivasan", "Pranav Maneriker", "Ali Payani", "Srinivasan Parthasarathy"], "title": "A Generic Framework for Conformal Fairness", "categories": ["cs.LG"], "comment": "ICLR 2025 Camera Ready Version", "summary": "Conformal Prediction (CP) is a popular method for uncertainty quantification\nwith machine learning models. While conformal prediction provides probabilistic\nguarantees regarding the coverage of the true label, these guarantees are\nagnostic to the presence of sensitive attributes within the dataset. In this\nwork, we formalize \\textit{Conformal Fairness}, a notion of fairness using\nconformal predictors, and provide a theoretically well-founded algorithm and\nassociated framework to control for the gaps in coverage between different\nsensitive groups. Our framework leverages the exchangeability assumption\n(implicit to CP) rather than the typical IID assumption, allowing us to apply\nthe notion of Conformal Fairness to data types and tasks that are not IID, such\nas graph data. Experiments were conducted on graph and tabular datasets to\ndemonstrate that the algorithm can control fairness-related gaps in addition to\ncoverage aligned with theoretical expectations."}
{"id": "2505.16276", "pdf": "https://arxiv.org/pdf/2505.16276", "abs": "https://arxiv.org/abs/2505.16276", "authors": ["Desiree Heim", "Lars-Peter Meyer", "Markus Schröder", "Johannes Frey", "Andreas Dengel"], "title": "How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance", "categories": ["cs.AI", "cs.CL"], "comment": "Peer reviewed and to appear in the ESWC 2025 Workshops and Tutorials\n  Joint Proceedings (Workshop on Evaluation of Language Models in Knowledge\n  Engineering [ELMKE])", "summary": "When using Large Language Models (LLMs) to support Knowledge Graph\nEngineering (KGE), one of the first indications when searching for an\nappropriate model is its size. According to the scaling laws, larger models\ntypically show higher capabilities. However, in practice, resource costs are\nalso an important factor and thus it makes sense to consider the ratio between\nmodel performance and costs. The LLM-KG-Bench framework enables the comparison\nof LLMs in the context of KGE tasks and assesses their capabilities of\nunderstanding and producing KGs and KG queries. Based on a dataset created in\nan LLM-KG-Bench run covering 26 open state-of-the-art LLMs, we explore the\nmodel size scaling laws specific to KGE tasks. In our analyses, we assess how\nbenchmark scores evolve between different model size categories. Additionally,\nwe inspect how the general score development of single models and families of\nmodels correlates to their size. Our analyses revealed that, with a few\nexceptions, the model size scaling laws generally also apply to the selected\nKGE tasks. However, in some cases, plateau or ceiling effects occurred, i.e.,\nthe task performance did not change much between a model and the next larger\nmodel. In these cases, smaller models could be considered to achieve high\ncost-effectiveness. Regarding models of the same family, sometimes larger\nmodels performed worse than smaller models of the same family. These effects\noccurred only locally. Hence it is advisable to additionally test the next\nsmallest and largest model of the same family."}
{"id": "2505.16134", "pdf": "https://arxiv.org/pdf/2505.16134", "abs": "https://arxiv.org/abs/2505.16134", "authors": ["Menschikov Mikhail", "Alexander Kharitonov", "Maiia Kotyga", "Vadim Porvatov", "Anna Zhukovskaya", "David Kagramanyan", "Egor Shvetsov", "Evgeny Burnaev"], "title": "Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models exhibit positional bias -- systematic neglect of\ninformation at specific context positions -- yet its interplay with linguistic\ndiversity remains poorly understood. We present a cross-linguistic study across\nfive typologically distinct languages (English, Russian, German, Hindi,\nVietnamese), examining how positional bias interacts with model uncertainty,\nsyntax, and prompting. Key findings: (1) Positional bias is model-driven, with\nlanguage-specific variations -- Qwen2.5-7B favors late positions, challenging\nassumptions of early-token bias; (2) Explicit positional guidance (e.g.,\ncorrect context is at position X) reduces accuracy across languages,\nundermining prompt-engineering practices; (3) Aligning context with positional\nbias increases entropy, yet minimal entropy does not predict accuracy. (4) We\nfurther uncover that LLMs differently impose dominant word order in\nfree-word-order languages like Hindi."}
{"id": "2505.16122", "pdf": "https://arxiv.org/pdf/2505.16122", "abs": "https://arxiv.org/abs/2505.16122", "authors": ["Junhong Lin", "Xinyue Zeng", "Jie Zhu", "Song Wang", "Julian Shun", "Jun Wu", "Dawei Zhou"], "title": "Plan and Budget: Effective and Efficient Test-Time Scaling on Large Language Model Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in complex\nreasoning tasks, but their inference remains computationally inefficient. We\nobserve a common failure mode in many prevalent LLMs, overthinking, where\nmodels generate verbose and tangential reasoning traces even for simple\nqueries. Recent works have tried to mitigate this by enforcing fixed token\nbudgets, however, this can lead to underthinking, especially on harder\nproblems. Through empirical analysis, we identify that this inefficiency often\nstems from unclear problem-solving strategies. To formalize this, we develop a\ntheoretical model, BBAM (Bayesian Budget Allocation Model), which models\nreasoning as a sequence of sub-questions with varying uncertainty, and\nintroduce the $E^3$ metric to capture the trade-off between correctness and\ncomputation efficiency. Building on theoretical results from BBAM, we propose\nPlan-and-Budget, a model-agnostic, test-time framework that decomposes complex\nqueries into sub-questions and allocates token budgets based on estimated\ncomplexity using adaptive scheduling. Plan-and-Budget improves reasoning\nefficiency across a range of tasks and models, achieving up to +70% accuracy\ngains, -39% token reduction, and +187.5% improvement in $E^3$. Notably, it\nelevates a smaller model (DS-Qwen-32B) to match the efficiency of a larger\nmodel (DS-LLaMA-70B)-demonstrating Plan-and-Budget's ability to close\nperformance gaps without retraining. Our code is available at\nanonymous.4open.science/r/P-and-B-6513/."}
{"id": "2505.16288", "pdf": "https://arxiv.org/pdf/2505.16288", "abs": "https://arxiv.org/abs/2505.16288", "authors": ["Xiaoxue Han", "Pengfei Hu", "Jun-En Ding", "Chang Lu", "Feng Liu", "Yue Ning"], "title": "No Black Boxes: Interpretable and Interactable Predictive Healthcare with Knowledge-Enhanced Agentic Causal Discovery", "categories": ["cs.AI"], "comment": null, "summary": "Deep learning models trained on extensive Electronic Health Records (EHR)\ndata have achieved high accuracy in diagnosis prediction, offering the\npotential to assist clinicians in decision-making and treatment planning.\nHowever, these models lack two crucial features that clinicians highly value:\ninterpretability and interactivity. The ``black-box'' nature of these models\nmakes it difficult for clinicians to understand the reasoning behind\npredictions, limiting their ability to make informed decisions. Additionally,\nthe absence of interactive mechanisms prevents clinicians from incorporating\ntheir own knowledge and experience into the decision-making process. To address\nthese limitations, we propose II-KEA, a knowledge-enhanced agent-driven causal\ndiscovery framework that integrates personalized knowledge databases and\nagentic LLMs. II-KEA enhances interpretability through explicit reasoning and\ncausal analysis, while also improving interactivity by allowing clinicians to\ninject their knowledge and experience through customized knowledge bases and\nprompts. II-KEA is evaluated on both MIMIC-III and MIMIC-IV, demonstrating\nsuperior performance along with enhanced interpretability and interactivity, as\nevidenced by its strong results from extensive case studies."}
{"id": "2505.16142", "pdf": "https://arxiv.org/pdf/2505.16142", "abs": "https://arxiv.org/abs/2505.16142", "authors": ["Shicheng Xu", "Liang Pang", "Yunchang Zhu", "Jia Gu", "Zihao Wei", "Jingcheng Deng", "Feiyang Pan", "Huawei Shen", "Xueqi Cheng"], "title": "Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning", "categories": ["cs.CL"], "comment": "15 pages", "summary": "Distilling reasoning paths from teacher to student models via supervised\nfine-tuning (SFT) provides a shortcut for improving the reasoning ability of\nsmaller Large Language Models (LLMs). However, the reasoning paths generated by\nteacher models often reflect only surface-level traces of their underlying\nauthentic reasoning. Insights from cognitive neuroscience suggest that\nauthentic reasoning involves a complex interweaving between meta-reasoning\n(which selects appropriate sub-problems from multiple candidates) and solving\n(which addresses the sub-problem). This implies authentic reasoning has an\nimplicit multi-branch structure. Supervised fine-tuning collapses this rich\nstructure into a flat sequence of token prediction in the teacher's reasoning\npath, preventing effective distillation of this structure to students. To\naddress this limitation, we propose RLKD, a reinforcement learning (RL)-based\ndistillation framework guided by a novel Generative Structure Reward Model\n(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving\nsteps and computes rewards to measure structural alignment between student and\nteacher reasoning. RLKD combines this reward with RL, enabling student LLMs to\ninternalize the teacher's implicit multi-branch reasoning structure rather than\nmerely mimicking fixed output paths. Experiments show RLKD surpasses standard\nSFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,\nunlocking greater student reasoning potential than SFT-based distillation."}
{"id": "2505.16126", "pdf": "https://arxiv.org/pdf/2505.16126", "abs": "https://arxiv.org/abs/2505.16126", "authors": ["Kotaro Yoshida", "Slavakis Konstantinos"], "title": "Robust Invariant Representation Learning by Distribution Extrapolation", "categories": ["cs.LG"], "comment": null, "summary": "Invariant risk minimization (IRM) aims to enable out-of-distribution (OOD)\ngeneralization in deep learning by learning invariant representations. As IRM\nposes an inherently challenging bi-level optimization problem, most existing\napproaches -- including IRMv1 -- adopt penalty-based single-level\napproximations. However, empirical studies consistently show that these methods\noften fail to outperform well-tuned empirical risk minimization (ERM),\nhighlighting the need for more robust IRM implementations. This work\ntheoretically identifies a key limitation common to many IRM variants: their\npenalty terms are highly sensitive to limited environment diversity and\nover-parameterization, resulting in performance degradation. To address this\nissue, a novel extrapolation-based framework is proposed that enhances\nenvironmental diversity by augmenting the IRM penalty through synthetic\ndistributional shifts. Extensive experiments -- ranging from synthetic setups\nto realistic, over-parameterized scenarios -- demonstrate that the proposed\nmethod consistently outperforms state-of-the-art IRM variants, validating its\neffectiveness and robustness."}
{"id": "2505.16312", "pdf": "https://arxiv.org/pdf/2505.16312", "abs": "https://arxiv.org/abs/2505.16312", "authors": ["Jiawei Liu", "Qisi Chen", "Jianshu Zhang", "Quan Liu", "Defu Lian"], "title": "EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via Action Pruning", "categories": ["cs.AI", "cs.CL"], "comment": "11 pages, 4 figures", "summary": "Large Language Models (LLMs) excel at complex reasoning through search\nalgorithms, yet current strategies often suffer from massive token consumption\ndue to redundant exploration of semantically equivalent steps. Existing\nsemantic similarity methods struggle to accurately identify such equivalence in\ndomain-specific contexts like mathematical reasoning. To address this, we\npropose EquivPruner, a simple yet effective approach that identifies and prunes\nsemantically equivalent actions during LLM reasoning search. We also introduce\nMathEquiv, the first dataset we created for mathematical statement equivalence,\nwhich enables the training of a lightweight equivalence detector. Extensive\nexperiments across various models and tasks demonstrate that EquivPruner\nsignificantly reduces token consumption, improving searching efficiency and\noften bolstering reasoning accuracy. For instance, when applied to\nQwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by\n48.1\\% while also improving accuracy. Our code is available at\nhttps://github.com/Lolo1222/EquivPruner."}
{"id": "2505.16160", "pdf": "https://arxiv.org/pdf/2505.16160", "abs": "https://arxiv.org/abs/2505.16160", "authors": ["Bin Xu", "Yu Bai", "Huashan Sun", "Yiguan Lin", "Siming Liu", "Xinyue Liang", "Yaolin Li", "Yang Gao", "Heyan Huang"], "title": "EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios", "categories": ["cs.CL"], "comment": null, "summary": "As large language models continue to advance, their application in\neducational contexts remains underexplored and under-optimized. In this paper,\nwe address this gap by introducing the first diverse benchmark tailored for\neducational scenarios, incorporating synthetic data containing 9 major\nscenarios and over 4,000 distinct educational contexts. To enable comprehensive\nassessment, we propose a set of multi-dimensional evaluation metrics that cover\n12 critical aspects relevant to both teachers and students. We further apply\nhuman annotation to ensure the effectiveness of the model-generated evaluation\nresponses. Additionally, we succeed to train a relatively small-scale model on\nour constructed dataset and demonstrate that it can achieve performance\ncomparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on\nthe test set. Overall, this work provides a practical foundation for the\ndevelopment and evaluation of education-oriented language models. Code and data\nare released at https://github.com/ybai-nlp/EduBench."}
{"id": "2505.16130", "pdf": "https://arxiv.org/pdf/2505.16130", "abs": "https://arxiv.org/abs/2505.16130", "authors": ["Zehong Wang", "Zheyuan Zhang", "Tianyi Ma", "Chuxu Zhang", "Yanfang Ye"], "title": "Scalable Graph Generative Modeling via Substructure Sequences", "categories": ["cs.LG", "cs.AI", "cs.SI"], "comment": null, "summary": "Graph neural networks (GNNs) has been predominantly driven by\nmessage-passing, where node representations are iteratively updated via local\nneighborhood aggregation. Despite their success, message-passing suffers from\nfundamental limitations -- including constrained expressiveness,\nover-smoothing, over-squashing, and limited capacity to model long-range\ndependencies. These issues hinder scalability: increasing data size or model\nsize often fails to yield improved performance, limiting the viability of GNNs\nas backbones for graph foundation models. In this work, we explore pathways\nbeyond message-passing and introduce Generative Graph Pattern Machine\n(G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM\nrepresents graph instances (nodes, edges, or entire graphs) as sequences of\nsubstructures, and employs generative pre-training over the sequences to learn\ngeneralizable, transferable representations. Empirically, G$^2$PM demonstrates\nstrong scalability: on the ogbn-arxiv benchmark, it continues to improve with\nmodel sizes up to 60M parameters, outperforming prior generative approaches\nthat plateau at significantly smaller scales (e.g., 3M). In addition, we\nsystematically analyze the model design space, highlighting key architectural\nchoices that contribute to its scalability and generalization. Across diverse\ntasks -- including node classification, graph classification, and transfer\nlearning -- G$^2$PM consistently outperforms strong baselines, establishing a\ncompelling foundation for scalable graph learning. The code and dataset are\navailable at https://github.com/Zehong-Wang/G2PM."}
{"id": "2505.16315", "pdf": "https://arxiv.org/pdf/2505.16315", "abs": "https://arxiv.org/abs/2505.16315", "authors": ["Xiaoxue Cheng", "Junyi Li", "Zhenduo Zhang", "Xinyu Tang", "Wayne Xin Zhao", "Xinyu Kong", "Zhiqiang Zhang"], "title": "Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "work in progress", "summary": "Large reasoning models (LRMs) have demonstrated strong performance on complex\nreasoning tasks, but often suffer from overthinking, generating redundant\ncontent regardless of task difficulty. Inspired by the dual process theory in\ncognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a\nreinforcement learning framework that enables LRMs to achieve efficient\nreasoning through adaptive cognitive allocation and dynamic system switch. ACPO\nincorporates two key components: (1) introducing system-aware reasoning tokens\nto explicitly represent the thinking modes thereby making the model's cognitive\nprocess transparent, and (2) integrating online difficulty estimation and token\nlength budget to guide adaptive system switch and reasoning during\nreinforcement learning. To this end, we propose a two-stage training strategy.\nThe first stage begins with supervised fine-tuning to cold start the model,\nenabling it to generate reasoning paths with explicit thinking modes. In the\nsecond stage, we apply ACPO to further enhance adaptive system switch for\ndifficulty-aware reasoning. Experimental results demonstrate that ACPO\neffectively reduces redundant reasoning while adaptively adjusting cognitive\nallocation based on task complexity, achieving efficient hybrid reasoning."}
{"id": "2505.16162", "pdf": "https://arxiv.org/pdf/2505.16162", "abs": "https://arxiv.org/abs/2505.16162", "authors": ["Mingbo Song", "Heming Xia", "Jun Zhang", "Chak Tou Leong", "Qiancheng Xu", "Wenjie Li", "Sujian Li"], "title": "KNN-SSD: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization", "categories": ["cs.CL"], "comment": "8 pages", "summary": "Speculative Decoding (SD) has emerged as a widely used paradigm to accelerate\nthe inference of large language models (LLMs) without compromising generation\nquality. It works by efficiently drafting multiple tokens using a compact model\nand then verifying them in parallel using the target LLM. Notably,\nSelf-Speculative Decoding proposes skipping certain layers to construct the\ndraft model, which eliminates the need for additional parameters or training.\nDespite its strengths, we observe in this work that drafting with layer\nskipping exhibits significant sensitivity to domain shifts, leading to a\nsubstantial drop in acceleration performance. To enhance the domain\ngeneralizability of this paradigm, we introduce KNN-SSD, an algorithm that\nleverages K-Nearest Neighbor (KNN) search to match different skipped layers\nwith various domain inputs. We evaluated our algorithm in various models and\nmultiple tasks, observing that its application leads to 1.3x-1.6x speedup in\nLLM inference."}
{"id": "2505.16138", "pdf": "https://arxiv.org/pdf/2505.16138", "abs": "https://arxiv.org/abs/2505.16138", "authors": ["Heqiang Wang", "Xiang Liu", "Xiaoxiong Zhong", "Lixing Chen", "Fangming Liu", "Weizhe Zhang"], "title": "Multimodal Online Federated Learning with Modality Missing in Internet of Things", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "The Internet of Things (IoT) ecosystem generates vast amounts of multimodal\ndata from heterogeneous sources such as sensors, cameras, and microphones. As\nedge intelligence continues to evolve, IoT devices have progressed from simple\ndata collection units to nodes capable of executing complex computational\ntasks. This evolution necessitates the adoption of distributed learning\nstrategies to effectively handle multimodal data in an IoT environment.\nFurthermore, the real-time nature of data collection and limited local storage\non edge devices in IoT call for an online learning paradigm. To address these\nchallenges, we introduce the concept of Multimodal Online Federated Learning\n(MMO-FL), a novel framework designed for dynamic and decentralized multimodal\nlearning in IoT environments. Building on this framework, we further account\nfor the inherent instability of edge devices, which frequently results in\nmissing modalities during the learning process. We conduct a comprehensive\ntheoretical analysis under both complete and missing modality scenarios,\nproviding insights into the performance degradation caused by missing\nmodalities. To mitigate the impact of modality missing, we propose the\nPrototypical Modality Mitigation (PMM) algorithm, which leverages prototype\nlearning to effectively compensate for missing modalities. Experimental results\non two multimodal datasets further demonstrate the superior performance of PMM\ncompared to benchmarks."}
{"id": "2505.16388", "pdf": "https://arxiv.org/pdf/2505.16388", "abs": "https://arxiv.org/abs/2505.16388", "authors": ["Nandini Doreswamy", "Louise Horstmanshof"], "title": "Serious Games: Human-AI Interaction, Evolution, and Coevolution", "categories": ["cs.AI", "cs.GT", "91A22 (Primary), 68T99 (Secondary)", "J.4; I.2.0; K.4.1; J.3; K.4.0"], "comment": "8 pages, 1 table", "summary": "The serious games between humans and AI have only just begun. Evolutionary\nGame Theory (EGT) models the competitive and cooperative strategies of\nbiological entities. EGT could help predict the potential evolutionary\nequilibrium of humans and AI. The objective of this work was to examine some of\nthe EGT models relevant to human-AI interaction, evolution, and coevolution. Of\nthirteen EGT models considered, three were examined: the Hawk-Dove Game,\nIterated Prisoner's Dilemma, and the War of Attrition. This selection was based\non the widespread acceptance and clear relevance of these models to potential\nhuman-AI evolutionary dynamics and coevolutionary trajectories. The Hawk-Dove\nGame predicts balanced mixed-strategy equilibria based on the costs of\nconflict. It also shows the potential for balanced coevolution rather than\ndominance. Iterated Prisoner's Dilemma suggests that repeated interaction may\nlead to cognitive coevolution. It demonstrates how memory and reciprocity can\nlead to cooperation. The War of Attrition suggests that competition for\nresources may result in strategic coevolution, asymmetric equilibria, and\nconventions on sharing resources. Therefore, EGT may provide a suitable\nframework to understand and predict the human-AI evolutionary dynamic. However,\nfuture research could extend beyond EGT and explore additional frameworks,\nempirical validation methods, and interdisciplinary perspectives. AI is being\nshaped by human input and is evolving in response to it. So too,\nneuroplasticity allows the human brain to grow and evolve in response to\nstimuli. If humans and AI converge in future, what might be the result of human\nneuroplasticity combined with an ever-evolving AI? Future research should be\nmindful of the ethical and cognitive implications of human-AI interaction,\nevolution, and coevolution."}
{"id": "2505.16164", "pdf": "https://arxiv.org/pdf/2505.16164", "abs": "https://arxiv.org/abs/2505.16164", "authors": ["Mengyang Qiu", "Zoe Brisebois", "Siena Sun"], "title": "Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly explored as substitutes for\nhuman participants in cognitive tasks, but their ability to simulate human\nbehavioral variability remains unclear. This study examines whether LLMs can\napproximate individual differences in the phonemic fluency task, where\nparticipants generate words beginning with a target letter. We evaluated 34\nmodel configurations, varying prompt specificity, sampling temperature, and\nmodel type, and compared outputs to responses from 106 human participants.\nWhile some configurations, especially Claude 3.7 Sonnet, matched human averages\nand lexical preferences, none reproduced the scope of human variability. LLM\noutputs were consistently less diverse and structurally rigid, and LLM\nensembles failed to increase diversity. Network analyses further revealed\nfundamental differences in retrieval structure between humans and models. These\nresults highlight key limitations in using LLMs to simulate human cognition and\nbehavior."}
{"id": "2505.16148", "pdf": "https://arxiv.org/pdf/2505.16148", "abs": "https://arxiv.org/abs/2505.16148", "authors": ["Chongjie Si", "Kangtao Lv", "Jingjing Jiang", "Yadao Wang", "Yongwei Wang", "Xiaokang Yang", "Wenbo Su", "Bo Zheng", "Wei Shen"], "title": "NAN: A Training-Free Solution to Coefficient Estimation in Model Merging", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Model merging offers a training-free alternative to multi-task learning by\ncombining independently fine-tuned models into a unified one without access to\nraw data. However, existing approaches often rely on heuristics to determine\nthe merging coefficients, limiting their scalability and generality. In this\nwork, we revisit model merging through the lens of least-squares optimization\nand show that the optimal merging weights should scale with the amount of\ntask-specific information encoded in each model. Based on this insight, we\npropose NAN, a simple yet effective method that estimates model merging\ncoefficients via the inverse of parameter norm. NAN is training-free,\nplug-and-play, and applicable to a wide range of merging strategies. Extensive\nexperiments on show that NAN consistently improves performance of baseline\nmethods."}
{"id": "2505.16409", "pdf": "https://arxiv.org/pdf/2505.16409", "abs": "https://arxiv.org/abs/2505.16409", "authors": ["Chaeeun Kim", "Seungone Kim"], "title": "FREESON: Retriever-Free Retrieval-Augmented Reasoning via Corpus-Traversing MCTS", "categories": ["cs.AI"], "comment": "Work In Progress", "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in\nmulti-step reasoning and calling search engines at appropriate steps. However,\nexisting retrieval-augmented reasoning approaches rely on separate retrieval\nmodels, limiting the LRM's role in retrieval to deciding when to retrieve and\nhow to query. This separation not only increases hardware and operational costs\nbut also leads to errors in the retrieval process due to the representation\nbottleneck, a phenomenon where the retriever's embedding space is not\nexpressive enough to meet the generator's requirements. To address this, we\nshift our perspective from sequence-to-sequence matching to locating the\nanswer-containing paths within the corpus, and propose a novel framework called\nFREESON (Retriever-FREE Retrieval-Augmented ReaSONing). This framework enables\nLRMs to retrieve relevant knowledge on their own by acting as both a generator\nand retriever. To achieve this, we introduce a variant of the MCTS algorithm\nspecialized for the retrieval task, which we call CT-MCTS (Corpus-Traversing\nMonte Carlo Tree Search). In this algorithm, LRMs traverse through the corpus\ntoward answer-containing regions. Our results on five open-domain QA\nbenchmarks, including single-hop and multi-hop questions, show that FREESON\nachieves an average improvement of 14.4% in EM and F1 over four multi-step\nreasoning models with a separate retriever, and it also performs comparably to\nthe strongest baseline, surpassing it by 3% on PopQA and 2WikiMultihopQA."}
{"id": "2505.16170", "pdf": "https://arxiv.org/pdf/2505.16170", "abs": "https://arxiv.org/abs/2505.16170", "authors": ["Yuqing Yang", "Robin Jia"], "title": "When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction", "categories": ["cs.CL"], "comment": null, "summary": "Can large language models (LLMs) admit their mistakes when they should know\nbetter? In this work, we define the behavior of acknowledging errors in\npreviously generated answers as \"retraction\" and aim to understand when and why\nLLMs choose to retract. We first construct model-specific datasets to evaluate\nwhether a model will retract an incorrect answer that contradicts its own\nparametric knowledge. While LLMs are capable of retraction, they do so only\ninfrequently. We demonstrate that retraction is closely tied to previously\nidentified indicators of models' internal belief: models fail to retract wrong\nanswers that they \"believe\" to be factually correct. Steering experiments\nfurther demonstrate that internal belief causally influences model retraction.\nIn particular, when the model does not believe its answer, this not only\nencourages the model to attempt to verify the answer, but also alters attention\nbehavior during self-verification. Finally, we demonstrate that simple\nsupervised fine-tuning significantly improves retraction performance by helping\nthe model learn more accurate internal beliefs. Code and datasets are available\non https://github.com/ayyyq/llm-retraction."}
{"id": "2505.16159", "pdf": "https://arxiv.org/pdf/2505.16159", "abs": "https://arxiv.org/abs/2505.16159", "authors": ["Chongjie Si", "Yidan Cui", "Fuchao Yang", "Xiaokang Yang", "Wei Shen"], "title": "Why Can Accurate Models Be Learned from Inaccurate Annotations?", "categories": ["cs.LG"], "comment": null, "summary": "Learning from inaccurate annotations has gained significant attention due to\nthe high cost of precise labeling. However, despite the presence of erroneous\nlabels, models trained on noisy data often retain the ability to make accurate\npredictions. This intriguing phenomenon raises a fundamental yet largely\nunexplored question: why models can still extract correct label information\nfrom inaccurate annotations remains unexplored. In this paper, we conduct a\ncomprehensive investigation into this issue. By analyzing weight matrices from\nboth empirical and theoretical perspectives, we find that label inaccuracy\nprimarily accumulates noise in lower singular components and subtly perturbs\nthe principal subspace. Within a certain range, the principal subspaces of\nweights trained on inaccurate labels remain largely aligned with those learned\nfrom clean labels, preserving essential task-relevant information. We formally\nprove that the angles of principal subspaces exhibit minimal deviation under\nmoderate label inaccuracy, explaining why models can still generalize\neffectively. Building on these insights, we propose LIP, a lightweight plug-in\ndesigned to help classifiers retain principal subspace information while\nmitigating noise induced by label inaccuracy. Extensive experiments on tasks\nwith various inaccuracy conditions demonstrate that LIP consistently enhances\nthe performance of existing algorithms. We hope our findings can offer valuable\ntheoretical and practical insights to understand of model robustness under\ninaccurate supervision."}
{"id": "2505.16448", "pdf": "https://arxiv.org/pdf/2505.16448", "abs": "https://arxiv.org/abs/2505.16448", "authors": ["Renfei Dang", "Shujian Huang", "Jiajun Chen"], "title": "Internal Bias in Reasoning Models leads to Overthinking", "categories": ["cs.AI"], "comment": null, "summary": "While current reasoning models possess strong exploratory capabilities, they\nare often criticized for overthinking due to redundant and unnecessary\nreflections. In this work, we reveal for the first time that overthinking in\nreasoning models may stem from their internal bias towards input texts. Upon\nencountering a reasoning problem, the model immediately forms a preliminary\nguess about the answer, which we term as an internal bias since it is not\nderived through actual reasoning. When this guess conflicts with its reasoning\nresult, the model tends to engage in reflection, leading to the waste of\ncomputational resources. Through further interpretability experiments, we find\nthat this behavior is largely driven by the model's excessive attention to the\ninput section, which amplifies the influence of internal bias on its\ndecision-making process. Additionally, by masking out the original input\nsection, the affect of internal bias can be effectively alleviated and the\nreasoning length could be reduced by 31%-53% across different complex reasoning\ntasks. Notably, in most cases, this approach also leads to improvements in\naccuracy. These findings demonstrate a causal relationship between internal\nbias and overthinking."}
{"id": "2505.16172", "pdf": "https://arxiv.org/pdf/2505.16172", "abs": "https://arxiv.org/abs/2505.16172", "authors": ["Abhay Kumara Sri Krishna Nandiraju", "Gondy Leroy", "David Kauchak", "Arif Ahmed"], "title": "Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Understanding health information is essential in achieving and maintaining a\nhealthy life. We focus on simplifying health information for better\nunderstanding. With the availability of generative AI, the simplification\nprocess has become efficient and of reasonable quality, however, the algorithms\nremove information that may be crucial for comprehension. In this study, we\ncompare generative AI to detect missing information in simplified text,\nevaluate its importance, and fix the text with the missing information. We\ncollected 50 health information texts and simplified them using gpt-4-0613. We\ncompare five approaches to identify missing elements and regenerate the text by\ninserting the missing elements. These five approaches involve adding missing\nentities and missing words in various ways: 1) adding all the missing entities,\n2) adding all missing words, 3) adding the top-3 entities ranked by gpt-4-0613,\nand 4, 5) serving as controls for comparison, adding randomly chosen entities.\nWe use cosine similarity and ROUGE scores to evaluate the semantic similarity\nand content overlap between the original, simplified, and reconstructed\nsimplified text. We do this for both summaries and full text. Overall, we find\nthat adding missing entities improves the text. Adding all the missing entities\nresulted in better text regeneration, which was better than adding the\ntop-ranked entities or words, or random words. Current tools can identify these\nentities, but are not valuable in ranking them."}
{"id": "2505.16190", "pdf": "https://arxiv.org/pdf/2505.16190", "abs": "https://arxiv.org/abs/2505.16190", "authors": ["Navid Seidi", "Satyaki Roy", "Sajal Das"], "title": "Enhancing Federated Survival Analysis through Peer-Driven Client Reputation in Healthcare", "categories": ["cs.LG"], "comment": null, "summary": "Federated Learning (FL) holds great promise for digital health by enabling\ncollaborative model training without compromising patient data privacy.\nHowever, heterogeneity across institutions, lack of sustained reputation, and\nunreliable contributions remain major challenges. In this paper, we propose a\nrobust, peer-driven reputation mechanism for federated healthcare that employs\na hybrid communication model to integrate decentralized peer feedback with\nclustering-based noise handling to enhance model aggregation. Crucially, our\napproach decouples the federated aggregation and reputation mechanisms by\napplying differential privacy to client-side model updates before sharing them\nfor peer evaluation. This ensures sensitive information remains protected\nduring reputation computation, while unaltered updates are sent to the server\nfor global model training. Using the Cox Proportional Hazards model for\nsurvival analysis across multiple federated nodes, our framework addresses both\ndata heterogeneity and reputation deficit by dynamically adjusting trust scores\nbased on local performance improvements measured via the concordance index.\nExperimental evaluations on both synthetic datasets and the SEER dataset\ndemonstrate that our method consistently achieves high and stable C-index\nvalues, effectively down-weighing noisy client updates and outperforming FL\nmethods that lack a reputation system."}
{"id": "2505.16455", "pdf": "https://arxiv.org/pdf/2505.16455", "abs": "https://arxiv.org/abs/2505.16455", "authors": ["Mengzhu Liu", "Zhengqiu Zhu", "Chuan Ai", "Chen Gao", "Xinghong Li", "Lingnan He", "Kaisheng Lai", "Yingfeng Chen", "Xin Lu", "Yong Li", "Quanjun Yin"], "title": "Psychology-driven LLM Agents for Explainable Panic Prediction on Social Media during Sudden Disaster Events", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "During sudden disaster events, accurately predicting public panic sentiment\non social media is crucial for proactive governance and crisis management.\nCurrent efforts on this problem face three main challenges: lack of finely\nannotated data hinders emotion prediction studies, unmodeled risk perception\ncauses prediction inaccuracies, and insufficient interpretability of panic\nformation mechanisms. We address these issues by proposing a Psychology-driven\ngenerative Agent framework (PsychoAgent) for explainable panic prediction based\non emotion arousal theory. Specifically, we first construct a fine-grained open\npanic emotion dataset (namely COPE) via human-large language models (LLMs)\ncollaboration to mitigate semantic bias. Then, we develop a framework\nintegrating cross-domain heterogeneous data grounded in psychological\nmechanisms to model risk perception and cognitive differences in emotion\ngeneration. To enhance interpretability, we design an LLM-based role-playing\nagent that simulates individual psychological chains through dedicatedly\ndesigned prompts. Experimental results on our annotated dataset show that\nPsychoAgent improves panic emotion prediction performance by 12.6% to 21.7%\ncompared to baseline models. Furthermore, the explainability and generalization\nof our approach is validated. Crucially, this represents a paradigm shift from\nopaque \"data-driven fitting\" to transparent \"role-based simulation with\nmechanistic interpretation\" for panic emotion prediction during emergencies.\nOur implementation is publicly available at:\nhttps://anonymous.4open.science/r/PsychoAgent-19DD."}
{"id": "2505.16178", "pdf": "https://arxiv.org/pdf/2505.16178", "abs": "https://arxiv.org/abs/2505.16178", "authors": ["Ying Zhang", "Benjamin Heinzerling", "Dongyuan Li", "Ryoma Ishigaki", "Yuta Hitomi", "Kentaro Inui"], "title": "Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages Memorization but Mixed Training Teaches Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Fact recall, the ability of language models (LMs) to retrieve specific\nfactual knowledge, remains a challenging task despite their impressive general\ncapabilities. Common training strategies often struggle to promote robust\nrecall behavior with two-stage training, which first trains a model with\nfact-storing examples (e.g., factual statements) and then with fact-recalling\nexamples (question-answer pairs), tending to encourage rote memorization rather\nthan generalizable fact retrieval. In contrast, mixed training, which jointly\nuses both types of examples, has been empirically shown to improve the ability\nto recall facts, but the underlying mechanisms are still poorly understood. In\nthis work, we investigate how these training strategies affect how model\nparameters are shaped during training and how these differences relate to their\nability to recall facts. We introduce cross-task gradient trace to identify\nshared parameters, those strongly influenced by both fact-storing and\nfact-recalling examples. Our analysis on synthetic fact recall datasets with\nthe Llama-3.2B and Pythia-2.8B models reveals that mixed training encouraging a\nlarger and more centralized set of shared parameters. These findings suggest\nthat the emergence of parameters may play a key role in enabling LMs to\ngeneralize factual knowledge across task formulations."}
{"id": "2505.16204", "pdf": "https://arxiv.org/pdf/2505.16204", "abs": "https://arxiv.org/abs/2505.16204", "authors": ["Ichiro Hashimoto"], "title": "Directional Convergence, Benign Overfitting of Gradient Descent in leaky ReLU two-layer Neural Networks", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH", "68T07 (primary)"], "comment": "34 pages", "summary": "In this paper, we prove directional convergence of network parameters of\nfixed width leaky ReLU two-layer neural networks optimized by gradient descent\nwith exponential loss, which was previously only known for gradient flow. By a\ncareful analysis of the convergent direction, we establish sufficient\nconditions of benign overfitting and discover a new phase transition in the\ntest error bound. All of these results hold beyond the nearly orthogonal data\nsetting which was studied in prior works. As an application, we demonstrate\nthat benign overfitting occurs with high probability in sub-Gaussian mixture\nmodels."}
{"id": "2505.16459", "pdf": "https://arxiv.org/pdf/2505.16459", "abs": "https://arxiv.org/abs/2505.16459", "authors": ["Guiyao Tie", "Xueyang Zhou", "Tianhe Gu", "Ruihang Zhang", "Chaoran Hu", "Sizhe Zhang", "Mengqu Sun", "Yan Zhang", "Pan Zhou", "Lichao Sun"], "title": "MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks", "categories": ["cs.AI"], "comment": "39 pages, 28 figures, 4 tables", "summary": "Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled\nunified processing of language, vision, and structured inputs, opening the door\nto complex tasks such as logical deduction, spatial reasoning, and scientific\nanalysis. Despite their promise, the reasoning capabilities of MLLMs,\nparticularly those augmented with intermediate thinking traces (MLLMs-T),\nremain poorly understood and lack standardized evaluation benchmarks. Existing\nwork focuses primarily on perception or final answer correctness, offering\nlimited insight into how models reason or fail across modalities. To address\nthis gap, we introduce the MMMR, a new benchmark designed to rigorously\nevaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a\nhigh-difficulty dataset of 1,083 questions spanning six diverse reasoning types\nwith symbolic depth and multi-hop demands and 2) a modular Reasoning Trace\nEvaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy\nthrough metrics like relevance, consistency, and structured error annotations.\nEmpirical results show that MLLMs-T overall outperform non-thinking\ncounterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro\nsuffer from reasoning pathologies such as inconsistency and overthinking. This\nbenchmark reveals persistent gaps between accuracy and reasoning quality and\nprovides an actionable evaluation pipeline for future model development.\nOverall, the MMMR offers a scalable foundation for evaluating, comparing, and\nimproving the next generation of multi-modal reasoning systems."}
{"id": "2505.16188", "pdf": "https://arxiv.org/pdf/2505.16188", "abs": "https://arxiv.org/abs/2505.16188", "authors": ["Zirui He", "Mingyu Jin", "Bo Shen", "Ali Payani", "Yongfeng Zhang", "Mengnan Du"], "title": "SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models", "categories": ["cs.CL"], "comment": "30 pages, 24 figures, 12 tables", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but controlling their behavior\nreliably remains challenging, especially in open-ended generation settings.\nThis paper introduces a novel supervised steering approach that operates in\nsparse, interpretable representation spaces. We employ sparse autoencoders\n(SAEs)to obtain sparse latent representations that aim to disentangle semantic\nattributes from model activations. Then we train linear classifiers to identify\na small subspace of task-relevant dimensions in latent representations.\nFinally, we learn supervised steering vectors constrained to this subspace,\noptimized to align with target behaviors. Experiments across sentiment,\ntruthfulness, and politics polarity steering tasks with multiple LLMs\ndemonstrate that our supervised steering vectors achieve higher success rates\nwith minimal degradation in generation quality compared to existing methods.\nFurther analysis reveals that a notably small subspace is sufficient for\neffective steering, enabling more targeted and interpretable interventions."}
{"id": "2505.16210", "pdf": "https://arxiv.org/pdf/2505.16210", "abs": "https://arxiv.org/abs/2505.16210", "authors": ["Zhihang Cai", "Xingjun Zhang", "Zhendong Tan", "Zheng Wei"], "title": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "11 pages, 9 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used."}
{"id": "2505.16475", "pdf": "https://arxiv.org/pdf/2505.16475", "abs": "https://arxiv.org/abs/2505.16475", "authors": ["Jiaqi Li", "Xinyi Dong", "Yang Liu", "Zhizhuo Yang", "Quansen Wang", "Xiaobo Wang", "SongChun Zhu", "Zixia Jia", "Zilong Zheng"], "title": "ReflectEvo: Improving Meta Introspection of Small LLMs by Learning Self-Reflection", "categories": ["cs.AI"], "comment": null, "summary": "We present a novel pipeline, ReflectEvo, to demonstrate that small language\nmodels (SLMs) can enhance meta introspection through reflection learning. This\nprocess iteratively generates self-reflection for self-training, fostering a\ncontinuous and self-evolving process. Leveraging this pipeline, we construct\nReflectEvo-460k, a large-scale, comprehensive, self-generated reflection\ndataset with broadened instructions and diverse multi-domain tasks. Building\nupon this dataset, we demonstrate the effectiveness of reflection learning to\nimprove SLMs' reasoning abilities using SFT and DPO with remarkable\nperformance, substantially boosting Llama-3 from 52.4% to 71.2% and Mistral\nfrom 44.4% to 71.1%. It validates that ReflectEvo can rival or even surpass the\nreasoning capability of the three prominent open-sourced models on BIG-bench\nwithout distillation from superior models or fine-grained human annotation. We\nfurther conduct a deeper analysis of the high quality of self-generated\nreflections and their impact on error localization and correction. Our work\nhighlights the potential of continuously enhancing the reasoning performance of\nSLMs through iterative reflection learning in the long run."}
{"id": "2505.16189", "pdf": "https://arxiv.org/pdf/2505.16189", "abs": "https://arxiv.org/abs/2505.16189", "authors": ["Sophie Wu", "Jan Philip Wahle", "Saif M. Mohammad"], "title": "The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions", "categories": ["cs.CL"], "comment": "8 pages, 26 figures", "summary": "This paper is the first investigation of the connection between emotion,\nembodiment, and everyday language in a large sample of natural language data.\nWe created corpora of body part mentions (BPMs) in online English text (blog\nposts and tweets). This includes a subset featuring human annotations for the\nemotions of the person whose body part is mentioned in the text. We show that\nBPMs are common in personal narratives and tweets (~5% to 10% of posts include\nBPMs) and that their usage patterns vary markedly by time and %geographic\nlocation. Using word-emotion association lexicons and our annotated data, we\nshow that text containing BPMs tends to be more emotionally charged, even when\nthe BPM is not explicitly used to describe a physical reaction to the emotion\nin the text. Finally, we discover a strong and statistically significant\ncorrelation between body-related language and a variety of poorer health\noutcomes. In sum, we argue that investigating the role of body-part related\nwords in language can open up valuable avenues of future research at the\nintersection of NLP, the affective sciences, and the study of human wellbeing."}
{"id": "2505.16217", "pdf": "https://arxiv.org/pdf/2505.16217", "abs": "https://arxiv.org/abs/2505.16217", "authors": ["Hon Tik Tse", "Siddarth Chandrasekar", "Marlos C. Machado"], "title": "Reward-Aware Proto-Representations in Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "In recent years, the successor representation (SR) has attracted increasing\nattention in reinforcement learning (RL), and it has been used to address some\nof its key challenges, such as exploration, credit assignment, and\ngeneralization. The SR can be seen as representing the underlying credit\nassignment structure of the environment by implicitly encoding its induced\ntransition dynamics. However, the SR is reward-agnostic. In this paper, we\ndiscuss a similar representation that also takes into account the reward\ndynamics of the problem. We study the default representation (DR), a recently\nproposed representation with limited theoretical (and empirical) analysis.\nHere, we lay some of the theoretical foundation underlying the DR in the\ntabular case by (1) deriving dynamic programming and (2) temporal-difference\nmethods to learn the DR, (3) characterizing the basis for the vector space of\nthe DR, and (4) formally extending the DR to the function approximation case\nthrough default features. Empirically, we analyze the benefits of the DR in\nmany of the settings in which the SR has been applied, including (1) reward\nshaping, (2) option discovery, (3) exploration, and (4) transfer learning. Our\nresults show that, compared to the SR, the DR gives rise to qualitatively\ndifferent, reward-aware behaviour and quantitatively better performance in\nseveral settings."}
{"id": "2505.16477", "pdf": "https://arxiv.org/pdf/2505.16477", "abs": "https://arxiv.org/abs/2505.16477", "authors": ["Yanbo Zhang", "Sumeer A. Khan", "Adnan Mahmud", "Huck Yang", "Alexander Lavin", "Michael Levin", "Jeremy Frey", "Jared Dunnmon", "James Evans", "Alan Bundy", "Saso Dzeroski", "Jesper Tegner", "Hector Zenil"], "title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery", "categories": ["cs.AI"], "comment": "45 pages", "summary": "With recent Nobel Prizes recognising AI contributions to science, Large\nLanguage Models (LLMs) are transforming scientific research by enhancing\nproductivity and reshaping the scientific method. LLMs are now involved in\nexperimental design, data analysis, and workflows, particularly in chemistry\nand biology. However, challenges such as hallucinations and reliability\npersist. In this contribution, we review how Large Language Models (LLMs) are\nredefining the scientific method and explore their potential applications\nacross different stages of the scientific cycle, from hypothesis testing to\ndiscovery. We conclude that, for LLMs to serve as relevant and effective\ncreative engines and productivity enhancers, their deep integration into all\nsteps of the scientific process should be pursued in collaboration and\nalignment with human scientific goals, with clear evaluation metrics. The\ntransition to AI-driven science raises ethical questions about creativity,\noversight, and responsibility. With careful guidance, LLMs could evolve into\ncreative engines, driving transformative breakthroughs across scientific\ndisciplines responsibly and effectively. However, the scientific community must\nalso decide how much it leaves to LLMs to drive science, even when associations\nwith 'reasoning', mostly currently undeserved, are made in exchange for the\npotential to explore hypothesis and solution regions that might otherwise\nremain unexplored by human exploration alone."}
{"id": "2505.16193", "pdf": "https://arxiv.org/pdf/2505.16193", "abs": "https://arxiv.org/abs/2505.16193", "authors": ["Daiqing Wu", "Dongbao Yang", "Sicheng Zhao", "Can Ma", "Yu Zhou"], "title": "An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The advancements in Multimodal Large Language Models (MLLMs) have enabled\nvarious multimodal tasks to be addressed under a zero-shot paradigm. This\nparadigm sidesteps the cost of model fine-tuning, emerging as a dominant trend\nin practical application. Nevertheless, Multimodal Sentiment Analysis (MSA), a\npivotal challenge in the quest for general artificial intelligence, fails to\naccommodate this convenience. The zero-shot paradigm exhibits undesirable\nperformance on MSA, casting doubt on whether MLLMs can perceive sentiments as\ncompetent as supervised models. By extending the zero-shot paradigm to\nIn-Context Learning (ICL) and conducting an in-depth study on configuring\ndemonstrations, we validate that MLLMs indeed possess such capability.\nSpecifically, three key factors that cover demonstrations' retrieval,\npresentation, and distribution are comprehensively investigated and optimized.\nA sentimental predictive bias inherent in MLLMs is also discovered and later\neffectively counteracted. By complementing each other, the devised strategies\nfor three factors result in average accuracy improvements of 15.9% on six MSA\ndatasets against the zero-shot paradigm and 11.2% against the random ICL\nbaseline."}
{"id": "2505.16226", "pdf": "https://arxiv.org/pdf/2505.16226", "abs": "https://arxiv.org/abs/2505.16226", "authors": ["Zi-Jian Cheng", "Zi-Yi Jia", "Zhi Zhou", "Yu-Feng Li", "Lan-Zhe Guo"], "title": "Realistic Evaluation of TabPFN v2 in Open Environments", "categories": ["cs.LG"], "comment": null, "summary": "Tabular data, owing to its ubiquitous presence in real-world domains, has\ngarnered significant attention in machine learning research. While tree-based\nmodels have long dominated tabular machine learning tasks, the recently\nproposed deep learning model TabPFN v2 has emerged, demonstrating unparalleled\nperformance and scalability potential. Although extensive research has been\nconducted on TabPFN v2 to further improve performance, the majority of this\nresearch remains confined to closed environments, neglecting the challenges\nthat frequently arise in open environments. This raises the question: Can\nTabPFN v2 maintain good performance in open environments? To this end, we\nconduct the first comprehensive evaluation of TabPFN v2's adaptability in open\nenvironments. We construct a unified evaluation framework covering various\nreal-world challenges and assess the robustness of TabPFN v2 under open\nenvironments scenarios using this framework. Empirical results demonstrate that\nTabPFN v2 shows significant limitations in open environments but is suitable\nfor small-scale, covariate-shifted, and class-balanced tasks. Tree-based models\nremain the optimal choice for general tabular tasks in open environments. To\nfacilitate future research on open environments challenges, we advocate for\nopen environments tabular benchmarks, multi-metric evaluation, and universal\nmodules to strengthen model robustness. We publicly release our evaluation\nframework at https://anonymous.4open.science/r/tabpfn-ood-4E65."}
{"id": "2505.16482", "pdf": "https://arxiv.org/pdf/2505.16482", "abs": "https://arxiv.org/abs/2505.16482", "authors": ["Huynh Thi Thanh Binh", "Le Van Cuong", "Dang Hai Dang", "Le Trong Vinh"], "title": "Minimizing the energy depletion in wireless rechargeable sensor networks using bi-level metaheuristic charging schemes", "categories": ["cs.AI", "cs.NE"], "comment": null, "summary": "Recently, Wireless Rechargeable Sensor Networks (WRSNs) that leveraged the\nadvantage of wireless energy transfer technology have opened a promising\nopportunity in solving the limited energy issue. However, an ineffective\ncharging strategy may reduce the charging performance. Although many practical\ncharging algorithms have been introduced, these studies mainly focus on\noptimizing the charging path with a fully charging approach. This approach may\nlead to the death of a series of sensors due to their extended charging\nlatency. This paper introduces a novel partial charging approach that follows a\nbi-level optimized scheme to minimize energy depletion in WRSNs. We aim at\noptimizing simultaneously two factors: the charging path and time. To\naccomplish this, we first formulate a mathematical model of the investigated\nproblem. We then propose two approximate algorithms in which the optimization\nof the charging path and the charging time are considered as the upper and\nlower level, respectively. The first algorithm combines a Multi-start Local\nSearch method and a Genetic Algorithm to find a solution. The second algorithm\nadopts a nested approach that utilizes the advantages of the Multitasking and\nCovariance Matrix Adaptation Evolutionary Strategies. Experimental validations\non various network scenarios demonstrate that our proposed algorithms\noutperform the existing works."}
{"id": "2505.16212", "pdf": "https://arxiv.org/pdf/2505.16212", "abs": "https://arxiv.org/abs/2505.16212", "authors": ["Anfeng Xu", "Tiantian Feng", "So Hyun Kim", "Somer Bishop", "Catherine Lord", "Shrikanth Narayanan"], "title": "Large Language Models based ASR Error Correction for Child Conversations", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Automatic Speech Recognition (ASR) has recently shown remarkable progress,\nbut accurately transcribing children's speech remains a significant challenge.\nRecent developments in Large Language Models (LLMs) have shown promise in\nimproving ASR transcriptions. However, their applications in child speech\nincluding conversational scenarios are underexplored. In this study, we explore\nthe use of LLMs in correcting ASR errors for conversational child speech. We\ndemonstrate the promises and challenges of LLMs through experiments on two\nchildren's conversational speech datasets with both zero-shot and fine-tuned\nASR outputs. We find that while LLMs are helpful in correcting zero-shot ASR\noutputs and fine-tuned CTC-based ASR outputs, it remains challenging for LLMs\nto improve ASR performance when incorporating contextual information or when\nusing fine-tuned autoregressive ASR (e.g., Whisper) outputs."}
{"id": "2505.16242", "pdf": "https://arxiv.org/pdf/2505.16242", "abs": "https://arxiv.org/abs/2505.16242", "authors": ["Runze Yan", "Xun Shen", "Akifumi Wachi", "Sebastien Gros", "Anni Zhao", "Xiao Hu"], "title": "Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "When applying offline reinforcement learning (RL) in healthcare scenarios,\nthe out-of-distribution (OOD) issues pose significant risks, as inappropriate\ngeneralization beyond clinical expertise can result in potentially harmful\nrecommendations. While existing methods like conservative Q-learning (CQL)\nattempt to address the OOD issue, their effectiveness is limited by only\nconstraining action selection by suppressing uncertain actions. This\naction-only regularization imitates clinician actions that prioritize\nshort-term rewards, but it fails to regulate downstream state trajectories,\nthereby limiting the discovery of improved long-term treatment strategies. To\nsafely improve policy beyond clinician recommendations while ensuring that\nstate-action trajectories remain in-distribution, we propose \\textit{Offline\nGuarded Safe Reinforcement Learning} ($\\mathsf{OGSRL}$), a theoretically\ngrounded model-based offline RL framework. $\\mathsf{OGSRL}$ introduces a novel\ndual constraint mechanism for improving policy with reliability and safety.\nFirst, the OOD guardian is established to specify clinically validated regions\nfor safe policy exploration. By constraining optimization within these regions,\nit enables the reliable exploration of treatment strategies that outperform\nclinician behavior by leveraging the full patient state history, without\ndrifting into unsupported state-action trajectories. Second, we introduce a\nsafety cost constraint that encodes medical knowledge about physiological\nsafety boundaries, providing domain-specific safeguards even in areas where\ntraining data might contain potentially unsafe interventions. Notably, we\nprovide theoretical guarantees on safety and near-optimality: policies that\nsatisfy these constraints remain in safe and reliable regions and achieve\nperformance close to the best possible policy supported by the data."}
{"id": "2505.16507", "pdf": "https://arxiv.org/pdf/2505.16507", "abs": "https://arxiv.org/abs/2505.16507", "authors": ["Anshu Xiong", "Songmao Zhang"], "title": "Relevance for Stability of Verification Status of a Set of Arguments in Incomplete Argumentation Frameworks (with Proofs)", "categories": ["cs.AI"], "comment": "This is a version of paper 'Relevance for Stability of Verification\n  Status of a Set of Arguments in Incomplete Argumentation Frameworks' extented\n  with proofs of the results in the paper", "summary": "The notion of relevance was proposed for stability of justification status of\na single argument in incomplete argumentation frameworks (IAFs) in 2024 by\nOdekerken et al. To extend the notion, we study the relevance for stability of\nverification status of a set of arguments in this paper, i.e., the\nuncertainties in an IAF that have to be resolved in some situations so that\nanswering whether a given set of arguments is an extension obtains the same\nresult in every completion of the IAF. Further we propose the notion of strong\nrelevance for describing the necessity of resolution in all situations reaching\nstability. An analysis of complexity reveals that detecting the (strong)\nrelevance for stability of sets of arguments can be accomplished in P time\nunder the most semantics discussed in the paper. We also discuss the difficulty\nin finding tractable methods for relevance detection under grounded semantics."}
{"id": "2505.16216", "pdf": "https://arxiv.org/pdf/2505.16216", "abs": "https://arxiv.org/abs/2505.16216", "authors": ["Jisu Kim", "Youngwoo Shin", "Uiji Hwang", "Jihun Choi", "Richeng Xuan", "Taeuk Kim"], "title": "Memorization or Reasoning? Exploring the Idiom Understanding of LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Idioms have long posed a challenge due to their unique linguistic properties,\nwhich set them apart from other common expressions. While recent studies have\nleveraged large language models (LLMs) to handle idioms across various tasks,\ne.g., idiom-containing sentence generation and idiomatic machine translation,\nlittle is known about the underlying mechanisms of idiom processing in LLMs,\nparticularly in multilingual settings. To this end, we introduce MIDAS, a new\nlarge-scale dataset of idioms in six languages, each paired with its\ncorresponding meaning. Leveraging this resource, we conduct a comprehensive\nevaluation of LLMs' idiom processing ability, identifying key factors that\ninfluence their performance. Our findings suggest that LLMs rely not only on\nmemorization, but also adopt a hybrid approach that integrates contextual cues\nand reasoning, especially when processing compositional idioms. This implies\nthat idiom understanding in LLMs emerges from an interplay between internal\nknowledge retrieval and reasoning-based inference."}
{"id": "2505.16248", "pdf": "https://arxiv.org/pdf/2505.16248", "abs": "https://arxiv.org/abs/2505.16248", "authors": ["Wenxuan Zhu", "Qiyuan Wu", "Tengda Tang", "Renzi Meng", "Sheng Chai", "Xuehui Quan"], "title": "Graph Neural Network-Based Collaborative Perception for Adaptive Scheduling in Distributed Systems", "categories": ["cs.LG"], "comment": null, "summary": "This paper addresses the limitations of multi-node perception and delayed\nscheduling response in distributed systems by proposing a GNN-based multi-node\ncollaborative perception mechanism. The system is modeled as a graph structure.\nMessage-passing and state-update modules are introduced. A multi-layer graph\nneural network is constructed to enable efficient information aggregation and\ndynamic state inference among nodes. In addition, a perception representation\nmethod is designed by fusing local states with global features. This improves\neach node's ability to perceive the overall system status. The proposed method\nis evaluated within a customized experimental framework. A dataset featuring\nheterogeneous task loads and dynamic communication topologies is used.\nPerformance is measured in terms of task completion rate, average latency, load\nbalancing, and transmission efficiency. Experimental results show that the\nproposed method outperforms mainstream algorithms under various conditions,\nincluding limited bandwidth and dynamic structural changes. It demonstrates\nsuperior perception capabilities and cooperative scheduling performance. The\nmodel achieves rapid convergence and efficient responses to complex system\nstates."}
{"id": "2505.16579", "pdf": "https://arxiv.org/pdf/2505.16579", "abs": "https://arxiv.org/abs/2505.16579", "authors": ["Siqu Ou", "Hongcheng Liu", "Pingjie Wang", "Yusheng Liao", "Chuan Xuan", "Yanfeng Wang", "Yu Wang"], "title": "Bridging the Dynamic Perception Gap: Training-Free Draft Chain-of-Thought for Dynamic Multimodal Spatial Reasoning", "categories": ["cs.AI", "cs.CV"], "comment": "19 pages, 8 figures", "summary": "While chains-of-thought (CoT) have advanced complex reasoning in multimodal\nlarge language models (MLLMs), existing methods remain confined to text or\nstatic visual domains, often faltering in dynamic spatial reasoning tasks. To\nbridge this gap, we present GRASSLAND, a novel maze navigation benchmark\ndesigned to evaluate dynamic spatial reasoning. Our experiments show that\naugmenting textual reasoning chains with dynamic visual drafts, overlaid on\ninput images, significantly outperforms conventional approaches, offering new\ninsights into spatial reasoning in evolving environments. To generalize this\ncapability, we propose D2R (Dynamic Draft-Augmented Reasoning), a training-free\nframework that seamlessly integrates textual CoT with corresponding visual\ndrafts into MLLMs. Extensive evaluations demonstrate that D2R consistently\nenhances performance across diverse tasks, establishing a robust baseline for\ndynamic spatial reasoning without requiring model fine-tuning. Project is open\nat https://github.com/Cratileo/D2R."}
{"id": "2505.16222", "pdf": "https://arxiv.org/pdf/2505.16222", "abs": "https://arxiv.org/abs/2505.16222", "authors": ["Jiwon Moon", "Yerin Hwang", "Dongryeol Lee", "Taegwan Kang", "Yongil Kim", "Kyomin Jung"], "title": "Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation", "categories": ["cs.CL", "cs.SE"], "comment": "26 pages", "summary": "With the growing use of large language models(LLMs) as evaluators, their\napplication has expanded to code evaluation tasks, where they assess the\ncorrectness of generated code without relying on reference implementations.\nWhile this offers scalability and flexibility, it also raises a critical,\nunresolved question: Can LLM judges fairly and robustly evaluate semantically\nequivalent code with superficial variations? Functionally correct code often\nexhibits variations-such as differences in variable names, comments, or\nformatting-that should not influence its correctness. Yet, whether LLM judges\ncan reliably handle these variations remains unclear. We present the first\ncomprehensive study of this issue, defining six types of potential bias in code\nevaluation and revealing their systematic impact on LLM judges. Across five\nprogramming languages and multiple LLMs, we empirically demonstrate that all\ntested LLM judges are susceptible to both positive and negative biases,\nresulting in inflated or unfairly low scores. Moreover, we observe that LLM\njudges remain vulnerable to these biases even when prompted to generate test\ncases before scoring, highlighting the need for more robust code evaluation\nmethods."}
{"id": "2505.16260", "pdf": "https://arxiv.org/pdf/2505.16260", "abs": "https://arxiv.org/abs/2505.16260", "authors": ["Alaa Khaddaj", "Logan Engstrom", "Aleksander Madry"], "title": "Small-to-Large Generalization: Data Influences Models Consistently Across Scale", "categories": ["cs.LG"], "comment": null, "summary": "Choice of training data distribution greatly influences model behavior. Yet,\nin large-scale settings, precisely characterizing how changes in training data\naffects predictions is often difficult due to model training costs. Current\npractice is to instead extrapolate from scaled down, inexpensive-to-train proxy\nmodels. However, changes in data do not influence smaller and larger models\nidentically. Therefore, understanding how choice of data affects large-scale\nmodels raises the question: how does training data distribution influence model\nbehavior across compute scale? We find that small- and large-scale language\nmodel predictions (generally) do highly correlate across choice of training\ndata. Equipped with these findings, we characterize how proxy scale affects\neffectiveness in two downstream proxy model applications: data attribution and\ndataset selection."}
{"id": "2505.16619", "pdf": "https://arxiv.org/pdf/2505.16619", "abs": "https://arxiv.org/abs/2505.16619", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "categories": ["cs.AI", "q-bio.OT", "92", "J.3"], "comment": "1 PDF, 24 Pages, 2 figures within. Co-corresponding authors:\n  Institute of Applied Biosciences, Centre for Research and Technology Hellas,\n  Thessaloniki, Greece and Department of Biomedical Sciences, University of\n  Padova, Padova, Italy. E-mails: fpsom@certh.gr, silvio.tosatto@unipd.it", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation."}
{"id": "2505.16227", "pdf": "https://arxiv.org/pdf/2505.16227", "abs": "https://arxiv.org/abs/2505.16227", "authors": ["Bohao Wu", "Qingyun Wang", "Yue Guo"], "title": "Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Personalizing jargon detection and explanation is essential for making\ntechnical documents accessible to readers with diverse disciplinary\nbackgrounds. However, tailoring models to individual users typically requires\nsubstantial annotation efforts and computational resources due to user-specific\nfinetuning. To address this, we present a systematic study of personalized\njargon detection, focusing on methods that are both efficient and scalable for\nreal-world deployment. We explore two personalization strategies: (1)\nlightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,\nand (2) personalized prompting, which tailors model behavior at inference time\nwithout retaining. To reflect realistic constraints, we also investigate hybrid\napproaches that combine limited annotated data with unsupervised user\nbackground signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in\nF1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,\nour method achieves comparable performance using only 10% of the annotated\ntraining data, demonstrating its practicality for resource-constrained\nsettings. Our study offers the first work to systematically explore efficient,\nlow-resource personalization of jargon detection using open-source language\nmodels, offering a practical path toward scalable, user-adaptive NLP system."}
{"id": "2505.16265", "pdf": "https://arxiv.org/pdf/2505.16265", "abs": "https://arxiv.org/abs/2505.16265", "authors": ["Ilgee Hong", "Changlong Yu", "Liang Qiu", "Weixiang Yan", "Zhenghao Xu", "Haoming Jiang", "Qingru Zhang", "Qin Lu", "Xin Liu", "Chao Zhang", "Tuo Zhao"], "title": "Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) has become a powerful\npost-training paradigm for aligning large language models with human\npreferences. A core challenge in RLHF is constructing accurate reward signals,\nwhere the conventional Bradley-Terry reward models (BT RMs) often suffer from\nsensitivity to data size and coverage, as well as vulnerability to reward\nhacking. Generative reward models (GenRMs) offer a more robust alternative by\ngenerating chain-of-thought (CoT) rationales followed by a final reward.\nHowever, existing GenRMs rely on shallow, vertically scaled reasoning, limiting\ntheir capacity to handle nuanced or complex (e.g., reasoning-intensive) tasks.\nMoreover, their pairwise preference outputs are incompatible with standard RLHF\nalgorithms that require pointwise reward signals. In this work, we introduce\nThink-RM, a training framework that enables long-horizon reasoning in GenRMs by\nmodeling an internal thinking process. Rather than producing structured,\nexternally provided rationales, Think-RM generates flexible, self-guided\nreasoning traces that support advanced capabilities such as self-reflection,\nhypothetical reasoning, and divergent reasoning. To elicit these reasoning\nabilities, we first warm-up the models by supervised fine-tuning (SFT) over\nlong CoT data. We then further improve the model's long-horizon abilities by\nrule-based reinforcement learning (RL). In addition, we propose a novel\npairwise RLHF pipeline that directly optimizes policies using pairwise\npreference rewards, eliminating the need for pointwise reward conversion and\nenabling more effective use of Think-RM outputs. Experiments show that Think-RM\nachieves state-of-the-art results on RM-Bench, outperforming both BT RM and\nvertically scaled GenRM by 8%. When combined with our pairwise RLHF pipeline,\nit demonstrates superior end-policy performance compared to traditional\napproaches."}
{"id": "2505.16646", "pdf": "https://arxiv.org/pdf/2505.16646", "abs": "https://arxiv.org/abs/2505.16646", "authors": ["Yujie Hou", "Ting Zhang", "Mei Wang", "Xuetao Ma", "Hu Huang"], "title": "SMART: Self-Generating and Self-Validating Multi-Dimensional Assessment for LLMs' Mathematical Problem Solving", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models have achieved remarkable results on a variety of\nmathematical benchmarks. However, concerns remain as to whether these successes\nreflect genuine mathematical reasoning or superficial pattern recognition.\nCommon evaluation metrics, such as final answer accuracy, fail to disentangle\nthe underlying competencies involved, offering limited diagnostic value. To\naddress these limitations, we introduce SMART: a Self-Generating and\nSelf-Validating Multi-Dimensional Assessment Framework. SMART decomposes\nmathematical problem solving into four distinct dimensions: understanding,\nreasoning, arithmetic, and reflection \\& refinement. Each dimension is\nevaluated independently through tailored tasks, enabling interpretable and\nfine-grained analysis of LLM behavior. Crucially, SMART integrates an automated\nself-generating and self-validating mechanism to produce and verify benchmark\ndata, ensuring both scalability and reliability. We apply SMART to 21\nstate-of-the-art open- and closed-source LLMs, uncovering significant\ndiscrepancies in their abilities across different dimensions. Our findings\ndemonstrate the inadequacy of final answer accuracy as a sole metric and\nmotivate a new holistic metric to better capture true problem-solving\ncapabilities. Code and benchmarks will be released upon acceptance."}
{"id": "2505.16232", "pdf": "https://arxiv.org/pdf/2505.16232", "abs": "https://arxiv.org/abs/2505.16232", "authors": ["Ali Sarosh Bangash", "Krish Veera", "Ishfat Abrar Islam", "Raiyan Abdul Baten"], "title": "MuseRAG: Idea Originality Scoring At Scale", "categories": ["cs.CL"], "comment": null, "summary": "An objective, face-valid way to assess the originality of creative ideas is\nto measure how rare each idea is within a population -- an approach long used\nin creativity research but difficult to automate at scale. Tabulating response\nfrequencies via manual bucketing of idea rephrasings is labor-intensive,\nerror-prone, and brittle under large corpora. We introduce a fully automated,\npsychometrically validated pipeline for frequency-based originality scoring.\nOur method, MuseRAG, combines large language models (LLMs) with an externally\norchestrated retrieval-augmented generation (RAG) framework. Given a new idea,\nthe system retrieves semantically similar prior idea buckets and zero-shot\nprompts the LLM to judge whether the new idea belongs to an existing bucket or\nforms a new one. The resulting buckets enable computation of frequency-based\noriginality metrics. Across five datasets (N=1143, n_ideas=16294), MuseRAG\nmatches human annotators in idea clustering structure and resolution (AMI =\n0.59) and in participant-level scoring (r = 0.89) -- while exhibiting strong\nconvergent and external validity. Our work enables intent-sensitive,\nhuman-aligned originality scoring at scale to aid creativity research."}
{"id": "2505.16284", "pdf": "https://arxiv.org/pdf/2505.16284", "abs": "https://arxiv.org/abs/2505.16284", "authors": ["Josh Alman", "Zhao Song"], "title": "Only Large Weights (And Not Skip Connections) Can Prevent the Perils of Rank Collapse", "categories": ["cs.LG"], "comment": null, "summary": "Attention mechanisms lie at the heart of modern large language models (LLMs).\nStraightforward algorithms for forward and backward (gradient) computation take\nquadratic time, and a line of work initiated by [Alman and Song NeurIPS 2023]\nand [Alman and Song NeurIPS 2024] has shown that quadratic time is necessary\nunless the model weights are small, in which case almost linear time algorithms\nare possible. In this paper, we show that large weights are necessary to avoid\na strong preclusion to representational strength we call layer collapse, which\nmeans that the entire network can be approximated well by a network with only a\nsingle layer. Thus, the quadratic running time of attention is unavoidable for\nexpressive transformers.\n  The notion of layer collapse that we introduce is a variant on the notion of\nrank collapse from the work of [Dong, Cordonnier, and Loukas ICML 2021]. They\nshowed that in Self Attention Networks with small weights and with skip\nconnections, rank collapse must occur. This is typically interpreted as\njustifying the necessity of skip connections in expressive networks. However,\nour result shows that even with skip connections, if the weights are small,\nthen layer collapse still occurs. Thus, only large weights, and not skip\nconnections, can prevent these representational weaknesses."}
{"id": "2505.16667", "pdf": "https://arxiv.org/pdf/2505.16667", "abs": "https://arxiv.org/abs/2505.16667", "authors": ["Xinwei Yang", "Zhaofeng Liu", "Chen Huang", "Jiashuai Zhang", "Tong Zhang", "Yifan Zhang", "Wenqiang Lei"], "title": "ELABORATION: A Comprehensive Benchmark on Human-LLM Competitive Programming", "categories": ["cs.AI"], "comment": "ACL 2025 Main. Our code and dataset are available at\n  https://github.com/SCUNLP/ELABORATION", "summary": "While recent research increasingly emphasizes the value of human-LLM\ncollaboration in competitive programming and proposes numerous empirical\nmethods, a comprehensive understanding remains elusive due to the fragmented\nnature of existing studies and their use of diverse, application-specific human\nfeedback. Thus, our work serves a three-fold purpose: First, we present the\nfirst taxonomy of human feedback consolidating the entire programming process,\nwhich promotes fine-grained evaluation. Second, we introduce ELABORATIONSET, a\nnovel programming dataset specifically designed for human-LLM collaboration,\nmeticulously annotated to enable large-scale simulated human feedback and\nfacilitate costeffective real human interaction studies. Third, we introduce\nELABORATION, a novel benchmark to facilitate a thorough assessment of human-LLM\ncompetitive programming. With ELABORATION, we pinpoint strengthes and\nweaknesses of existing methods, thereby setting the foundation for future\nimprovement. Our code and dataset are available at\nhttps://github.com/SCUNLP/ELABORATION"}
{"id": "2505.16234", "pdf": "https://arxiv.org/pdf/2505.16234", "abs": "https://arxiv.org/abs/2505.16234", "authors": ["Wei Zhang", "Zhenhong Zhou", "Junfeng Fang", "Rongwu Xu", "Kun Wang", "Yuanhe Zhang", "Rui Wang", "Ge Zhang", "Xinfeng Li", "Li Sun", "Lingjuan Lyu", "Yang Liu", "Sen Su"], "title": "LIFEBench: Evaluating Length Instruction Following in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "81 pages, 22 tables, 32 figures. Homepage:\n  https://ydyjya.github.io/LIFEBench/", "summary": "While large language models (LLMs) can solve PhD-level reasoning problems\nover long context inputs, they still struggle with a seemingly simpler task:\nfollowing explicit length instructions-e.g., write a 10,000-word novel.\nAdditionally, models often generate far too short outputs, terminate\nprematurely, or even refuse the request. Existing benchmarks focus primarily on\nevaluating generations quality, but often overlook whether the generations meet\nlength constraints. To this end, we introduce Length Instruction Following\nEvaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to\nfollow length instructions across diverse tasks and a wide range of specified\nlengths. LIFEBench consists of 10,800 instances across 4 task categories in\nboth English and Chinese, covering length constraints ranging from 16 to 8192\nwords. We evaluate 26 widely-used LLMs and find that most models reasonably\nfollow short-length instructions but deteriorate sharply beyond a certain\nthreshold. Surprisingly, almost all models fail to reach the vendor-claimed\nmaximum output lengths in practice, as further confirmed by our evaluations\nextending up to 32K words. Even long-context LLMs, despite their extended\ninput-output windows, counterintuitively fail to improve length-instructions\nfollowing. Notably, Reasoning LLMs outperform even specialized long-text\ngeneration models, achieving state-of-the-art length following. Overall,\nLIFEBench uncovers fundamental limitations in current LLMs' length instructions\nfollowing ability, offering critical insights for future progress."}
{"id": "2505.16291", "pdf": "https://arxiv.org/pdf/2505.16291", "abs": "https://arxiv.org/abs/2505.16291", "authors": ["Ronen Gradwohl", "Eilam Shapira", "Moshe Tennenholtz"], "title": "Fairness under Competition", "categories": ["cs.LG", "cs.GT"], "comment": null, "summary": "Algorithmic fairness has emerged as a central issue in ML, and it has become\nstandard practice to adjust ML algorithms so that they will satisfy fairness\nrequirements such as Equal Opportunity. In this paper we consider the effects\nof adopting such fair classifiers on the overall level of ecosystem fairness.\nSpecifically, we introduce the study of fairness with competing firms, and\ndemonstrate the failure of fair classifiers in yielding fair ecosystems. Our\nresults quantify the loss of fairness in systems, under a variety of\nconditions, based on classifiers' correlation and the level of their data\noverlap. We show that even if competing classifiers are individually fair, the\necosystem's outcome may be unfair; and that adjusting biased algorithms to\nimprove their individual fairness may lead to an overall decline in ecosystem\nfairness. In addition to these theoretical results, we also provide supporting\nexperimental evidence. Together, our model and results provide a novel and\nessential call for action."}
{"id": "2505.16686", "pdf": "https://arxiv.org/pdf/2505.16686", "abs": "https://arxiv.org/abs/2505.16686", "authors": ["Lars Benedikt Kaesberg", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "title": "SPaRC: A Spatial Pathfinding Reasoning Challenge", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Existing reasoning datasets saturate and fail to test abstract, multi-step\nproblems, especially pathfinding and complex rule constraint satisfaction. We\nintroduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000\n2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,\nrequiring step-by-step planning with arithmetic and geometric rules. Humans\nachieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best\nreasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).\nModels often generate invalid paths (>50% of puzzles for o4-mini), and\nreasoning tokens reveal they make errors in navigation and spatial logic.\nUnlike humans, who take longer on hard puzzles, models fail to scale test-time\ncompute with difficulty. Allowing models to make multiple solution attempts\nimproves accuracy, suggesting potential for better spatial reasoning with\nimproved training and efficient test-time scaling methods. SPaRC can be used as\na window into models' spatial reasoning limitations and drive research toward\nnew methods that excel in abstract, multi-step problem-solving."}
{"id": "2505.16237", "pdf": "https://arxiv.org/pdf/2505.16237", "abs": "https://arxiv.org/abs/2505.16237", "authors": ["Derong Xu", "Pengyue Jia", "Xiaopeng Li", "Yingyi Zhang", "Maolin Wang", "Qidong Liu", "Xiangyu Zhao", "Yichao Wang", "Huifeng Guo", "Ruiming Tang", "Enhong Chen", "Tong Xu"], "title": "Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nstill struggle with issues like hallucinations and outdated information.\nRetrieval-augmented generation (RAG) addresses these issues by grounding LLM\noutputs in external knowledge with an Information Retrieval (IR) system.\nBuilding on this foundation, graph-based RAG systems go a step further by\nretrieving subgraphs, which preserve the relationships between knowledge\nentities and provide more comprehensive context. However, graph RAG faces two\nchallenges: (1) Retrieving relevant information introduces irrelevant nodes\n(especially in dense graph databases, where retrieval usually extends to\nadjacent nodes), and leads to overly lengthy inputs that hinder efficiency; (2)\nThe representation gap between graph and language during generation with LLMs\nlimits the ability to fully leverage graph structures for enhanced\nunderstanding. To address these limitations, we propose Align-GRAG, a novel\nreasoning-guided dual alignment framework in post-retrieval phrase. It first\nformulates a subgraph by retrieving nodes and edges. Then an Aligner is\nproposed to jointly optimizes a graph encoder with LLM-summarized reasoning. It\nachieves dual alignment of graph node and representation by leveraging KL\ndivergence loss and contrastive loss, facilitating efficient pruning of\nirrelevant knowledge and establishing a unified semantic space. The Generator\nintegrates the aligned graph data with LLM to produce coherent and accurate\nanswers. Experiments on GraphQA benchmark across three tasks (including common\nsense reasoning, scene graph understanding, and knowledge graph reasoning)\nvalidate the effectiveness of our method. The code will be available upon\naccepted."}
{"id": "2505.16305", "pdf": "https://arxiv.org/pdf/2505.16305", "abs": "https://arxiv.org/abs/2505.16305", "authors": ["Bingyang Cheng", "Zhongtao Chen", "Yichen Jin", "Hao Zhang", "Chen Zhang", "Edmud Y. Lam", "Yik-Chung Wu"], "title": "Large-Scale Bayesian Tensor Reconstruction: An Approximate Message Passing Solution", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Tensor CANDECOMP/PARAFAC decomposition (CPD) is a fundamental model for\ntensor reconstruction. Although the Bayesian framework allows for principled\nuncertainty quantification and automatic hyperparameter learning, existing\nmethods do not scale well for large tensors because of high-dimensional matrix\ninversions. To this end, we introduce CP-GAMP, a scalable Bayesian CPD\nalgorithm. This algorithm leverages generalized approximate message passing\n(GAMP) to avoid matrix inversions and incorporates an expectation-maximization\nroutine to jointly infer the tensor rank and noise power. Through multiple\nexperiments, for synthetic 100x100x100 rank 20 tensors with only 20% elements\nobserved, the proposed algorithm reduces runtime by 82.7% compared to the\nstate-of-the-art variational Bayesian CPD method, while maintaining comparable\nreconstruction accuracy."}
{"id": "2505.16700", "pdf": "https://arxiv.org/pdf/2505.16700", "abs": "https://arxiv.org/abs/2505.16700", "authors": ["Xuanqi Gao", "Siyi Xie", "Juan Zhai", "Shqing Ma", "Chao Shen"], "title": "MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use Capabilities in Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) evolve from passive text generators to active\nreasoning agents capable of tool interaction, the Model Context Protocol (MCP)\nhas emerged as a standardized framework for dynamic tool discovery and\norchestration. Despite widespread industry adoption, existing evaluation\nmethodologies fail to adequately assess tool utilization capabilities within\nthis new paradigm. This paper introduces MCP-RADAR, the first comprehensive\nbenchmark specifically designed to evaluate LLM performance in the MCP\nframework through a novel five-dimensional approach measuring: answer accuracy,\ntool selection efficiency, computational resource efficiency, parameter\nconstruction accuracy, and execution speed. Unlike conventional benchmarks that\nrely on subjective human evaluations or binary success metrics, MCP-RADAR\nemploys objective, quantifiable measurements across multiple task domains\nincluding software engineering, mathematical reasoning, and general\nproblem-solving. Our evaluations of leading commercial and open-source LLMs\nreveal distinctive capability profiles with significant trade-offs between\naccuracy, efficiency, and speed, challenging traditional single-metric\nperformance rankings. Besides, we provide valuable guidance for developers to\noptimize their tools for maximum model compatibility and effectiveness. While\nfocused on MCP due to its standardized approach, our methodology remains\napplicable across all LLM agent tool integration frameworks, providing valuable\ninsights for both LLM developers and tool creators to optimize the entire\nLLM-tool interaction ecosystem. The implementation, configurations, and\ndatasets used in our evaluation are publicly available at\nhttps://anonymous.4open.science/r/MCPRadar-B143."}
{"id": "2505.16241", "pdf": "https://arxiv.org/pdf/2505.16241", "abs": "https://arxiv.org/abs/2505.16241", "authors": ["Viet-Anh Nguyen", "Shiqian Zhao", "Gia Dao", "Runyi Hu", "Yi Xie", "Luu Anh Tuan"], "title": "Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers", "categories": ["cs.CL"], "comment": null, "summary": "Recently, Large Reasoning Models (LRMs) have demonstrated superior logical\ncapabilities compared to traditional Large Language Models (LLMs), gaining\nsignificant attention. Despite their impressive performance, the potential for\nstronger reasoning abilities to introduce more severe security vulnerabilities\nremains largely underexplored. Existing jailbreak methods often struggle to\nbalance effectiveness with robustness against adaptive safety mechanisms. In\nthis work, we propose SEAL, a novel jailbreak attack that targets LRMs through\nan adaptive encryption pipeline designed to override their reasoning processes\nand evade potential adaptive alignment. Specifically, SEAL introduces a stacked\nencryption approach that combines multiple ciphers to overwhelm the models\nreasoning capabilities, effectively bypassing built-in safety mechanisms. To\nfurther prevent LRMs from developing countermeasures, we incorporate two\ndynamic strategies - random and adaptive - that adjust the cipher length,\norder, and combination. Extensive experiments on real-world reasoning models,\nincluding DeepSeek-R1, Claude Sonnet, and OpenAI GPT-o4, validate the\neffectiveness of our approach. Notably, SEAL achieves an attack success rate of\n80.8% on GPT o4-mini, outperforming state-of-the-art baselines by a significant\nmargin of 27.2%. Warning: This paper contains examples of inappropriate,\noffensive, and harmful content."}
{"id": "2505.16308", "pdf": "https://arxiv.org/pdf/2505.16308", "abs": "https://arxiv.org/abs/2505.16308", "authors": ["Xingyu Zhang", "Wenwen Qiang", "Siyu Zhao", "Huijie Guo", "Jiangmeng Li", "Chuxiong Sun", "Changwen Zheng"], "title": "CAIFormer: A Causal Informed Transformer for Multivariate Time Series Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Most existing multivariate time series forecasting methods adopt an\nall-to-all paradigm that feeds all variable histories into a unified model to\npredict their future values without distinguishing their individual roles.\nHowever, this undifferentiated paradigm makes it difficult to identify\nvariable-specific causal influences and often entangles causally relevant\ninformation with spurious correlations. To address this limitation, we propose\nan all-to-one forecasting paradigm that predicts each target variable\nseparately. Specifically, we first construct a Structural Causal Model from\nobservational data and then, for each target variable, we partition the\nhistorical sequence into four sub-segments according to the inferred causal\nstructure: endogenous, direct causal, collider causal, and spurious\ncorrelation. The prediction relies solely on the first three causally relevant\nsub-segments, while the spurious correlation sub-segment is excluded.\nFurthermore, we propose Causal Informed Transformer (CAIFormer), a novel\nforecasting model comprising three components: Endogenous Sub-segment\nPrediction Block, Direct Causal Sub-segment Prediction Block, and Collider\nCausal Sub-segment Prediction Block, which process the endogenous, direct\ncausal, and collider causal sub-segments, respectively. Their outputs are then\ncombined to produce the final prediction. Extensive experiments on multiple\nbenchmark datasets demonstrate the effectiveness of the CAIFormer."}
{"id": "2505.16771", "pdf": "https://arxiv.org/pdf/2505.16771", "abs": "https://arxiv.org/abs/2505.16771", "authors": ["Beyazit Bestami Yuksel", "Ayse Yilmazer Metin"], "title": "Data-Driven Breakthroughs and Future Directions in AI Infrastructure: A Comprehensive Review", "categories": ["cs.AI"], "comment": "10 pages, 6 figures, 3 tables", "summary": "This paper presents a comprehensive synthesis of major breakthroughs in\nartificial intelligence (AI) over the past fifteen years, integrating\nhistorical, theoretical, and technological perspectives. It identifies key\ninflection points in AI' s evolution by tracing the convergence of\ncomputational resources, data access, and algorithmic innovation. The analysis\nhighlights how researchers enabled GPU based model training, triggered a data\ncentric shift with ImageNet, simplified architectures through the Transformer,\nand expanded modeling capabilities with the GPT series. Rather than treating\nthese advances as isolated milestones, the paper frames them as indicators of\ndeeper paradigm shifts. By applying concepts from statistical learning theory\nsuch as sample complexity and data efficiency, the paper explains how\nresearchers translated breakthroughs into scalable solutions and why the field\nmust now embrace data centric approaches. In response to rising privacy\nconcerns and tightening regulations, the paper evaluates emerging solutions\nlike federated learning, privacy enhancing technologies (PETs), and the data\nsite paradigm, which reframe data access and security. In cases where real\nworld data remains inaccessible, the paper also assesses the utility and\nconstraints of mock and synthetic data generation. By aligning technical\ninsights with evolving data infrastructure, this study offers strategic\nguidance for future AI research and policy development."}
{"id": "2505.16245", "pdf": "https://arxiv.org/pdf/2505.16245", "abs": "https://arxiv.org/abs/2505.16245", "authors": ["Vijeta Deshpande", "Debasmita Ghose", "John D. Patterson", "Roger Beaty", "Anna Rumshisky"], "title": "Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Diverse language model responses are crucial for creative generation,\nopen-ended tasks, and self-improvement training. We show that common diversity\nmetrics, and even reward models used for preference optimization,\nsystematically bias models toward shorter outputs, limiting expressiveness. To\naddress this, we introduce Diverse, not Short (Diverse-NS), a length-controlled\nself-learning framework that improves response diversity while maintaining\nlength parity. By generating and filtering preference data that balances\ndiversity, quality, and length, Diverse-NS enables effective training using\nonly 3,000 preference pairs. Applied to LLaMA-3.1-8B and the Olmo-2 family,\nDiverse-NS substantially enhances lexical and semantic diversity. We show\nconsistent improvement in diversity with minor reduction or gains in response\nquality on four creative generation tasks: Divergent Associations, Persona\nGeneration, Alternate Uses, and Creative Writing. Surprisingly, experiments\nwith the Olmo-2 model family (7B, and 13B) show that smaller models like\nOlmo-2-7B can serve as effective \"diversity teachers\" for larger models. By\nexplicitly addressing length bias, our method efficiently pushes models toward\nmore diverse and expressive outputs."}
{"id": "2505.16319", "pdf": "https://arxiv.org/pdf/2505.16319", "abs": "https://arxiv.org/abs/2505.16319", "authors": ["Yangyang Wang", "Jiawei Gu", "Li Long", "Xin Li", "Li Shen", "Zhouyu Fu", "Xiangjun Zhou", "Xu Jiang"], "title": "FreshRetailNet-50K: A Stockout-Annotated Censored Demand Dataset for Latent Demand Recovery and Forecasting in Fresh Retail", "categories": ["cs.LG"], "comment": "10 pages, 5 figures", "summary": "Accurate demand estimation is critical for the retail business in guiding the\ninventory and pricing policies of perishable products. However, it faces\nfundamental challenges from censored sales data during stockouts, where\nunobserved demand creates systemic policy biases. Existing datasets lack the\ntemporal resolution and annotations needed to address this censoring effect. To\nfill this gap, we present FreshRetailNet-50K, the first large-scale benchmark\nfor censored demand estimation. It comprises 50,000 store-product time series\nof detailed hourly sales data from 898 stores in 18 major cities, encompassing\n863 perishable SKUs meticulously annotated for stockout events. The hourly\nstock status records unique to this dataset, combined with rich contextual\ncovariates, including promotional discounts, precipitation, and temporal\nfeatures, enable innovative research beyond existing solutions. We demonstrate\none such use case of two-stage demand modeling: first, we reconstruct the\nlatent demand during stockouts using precise hourly annotations. We then\nleverage the recovered demand to train robust demand forecasting models in the\nsecond stage. Experimental results show that this approach achieves a 2.73\\%\nimprovement in prediction accuracy while reducing the systematic demand\nunderestimation from 7.37\\% to near-zero bias. With unprecedented temporal\ngranularity and comprehensive real-world information, FreshRetailNet-50K opens\nnew research directions in demand imputation, perishable inventory\noptimization, and causal retail analytics. The unique annotation quality and\nscale of the dataset address long-standing limitations in retail AI, providing\nimmediate solutions and a platform for future methodological innovation. The\ndata (https://huggingface.co/datasets/Dingdong-Inc/FreshRetailNet-50K) and code\n(https://github.com/Dingdong-Inc/frn-50k-baseline}) are openly released."}
{"id": "2505.16781", "pdf": "https://arxiv.org/pdf/2505.16781", "abs": "https://arxiv.org/abs/2505.16781", "authors": ["Qianlei Jia", "Xinliang Zhou", "Ondrej Krejcar", "Enrique Herrera-Viedma"], "title": "Fuzzy Information Evolution with Three-Way Decision in Social Network Group Decision-Making", "categories": ["cs.AI"], "comment": null, "summary": "In group decision-making (GDM) scenarios, uncertainty, dynamic social\nstructures, and vague information present major challenges for traditional\nopinion dynamics models. To address these issues, this study proposes a novel\nsocial network group decision-making (SNGDM) framework that integrates\nthree-way decision (3WD) theory, dynamic network reconstruction, and linguistic\nopinion representation. First, the 3WD mechanism is introduced to explicitly\nmodel hesitation and ambiguity in agent judgments, thereby preventing\nirrational decisions. Second, a connection adjustment rule based on opinion\nsimilarity is developed, enabling agents to adaptively update their\ncommunication links and better reflect the evolving nature of social\nrelationships. Third, linguistic terms are used to describe agent opinions,\nallowing the model to handle subjective, vague, or incomplete information more\neffectively. Finally, an integrated multi-agent decision-making framework is\nconstructed, which simultaneously considers individual uncertainty, opinion\nevolution, and network dynamics. The proposed model is applied to a multi-UAV\ncooperative decision-making scenario, where simulation results and consensus\nanalysis demonstrate its effectiveness. Experimental comparisons further verify\nthe advantages of the algorithm in enhancing system stability and representing\nrealistic decision-making behaviors."}
{"id": "2505.16252", "pdf": "https://arxiv.org/pdf/2505.16252", "abs": "https://arxiv.org/abs/2505.16252", "authors": ["Hwiyeong Lee", "Uiji Hwang", "Hyelim Lim", "Taeuk Kim"], "title": "Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Large language models often retain unintended content, prompting growing\ninterest in knowledge unlearning. Recent approaches emphasize localized\nunlearning, which restricts parameter updates to specific regions in an effort\nto remove target knowledge while preserving unrelated general knowledge.\nHowever, their effectiveness remains uncertain due to the lack of robust and\nthorough evaluation of the trade-off between the competing goals of unlearning.\nIn this paper, we begin by revisiting existing localized unlearning approaches.\nWe then conduct controlled experiments to rigorously evaluate whether local\nparameter updates causally contribute to unlearning. Our findings reveal that\nthe set of parameters that must be modified for effective unlearning is not\nstrictly determined, challenging the core assumption of localized unlearning\nthat parameter locality is inherently indicative of effective knowledge\nremoval."}
{"id": "2505.16322", "pdf": "https://arxiv.org/pdf/2505.16322", "abs": "https://arxiv.org/abs/2505.16322", "authors": ["Woosung Koh", "Wonbeen Oh", "Jaein Jang", "MinHyung Lee", "Hyeongjin Kim", "Ah Yeon Kim", "Joonkee Kim", "Junghyun Lee", "Taehyeon Kim", "Se-Young Yun"], "title": "AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Pre-print", "summary": "Self-Taught Reasoners (STaR), synonymously known as Rejection sampling\nFine-Tuning (RFT), is an integral part of the training pipeline of\nself-improving reasoning Language Models (LMs). The self-improving mechanism\noften employs random observation (data) sampling. However, this results in\ntrained observation imbalance; inefficiently over-training on solved examples\nwhile under-training on challenging ones. In response, we introduce Adaptive\nSTaR (AdaSTaR), a novel algorithm that rectifies this by integrating two\nadaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting\nbalanced training across observations, and (2) Adaptive Sampling for\nCurriculum: dynamically adjusting data difficulty to match the model's evolving\nstrength. Across six benchmarks, AdaSTaR achieves best test accuracy in all\ninstances (6/6) and reduces training FLOPs by an average of 58.6% against an\nextensive list of baselines. These improvements in performance and efficiency\ngeneralize to different pre-trained LMs and larger models, paving the way for\nmore efficient and effective self-improving LMs."}
{"id": "2505.16787", "pdf": "https://arxiv.org/pdf/2505.16787", "abs": "https://arxiv.org/abs/2505.16787", "authors": ["Ashish Sundar", "Chunbo Luo", "Xiaoyang Wang"], "title": "Gaze Into the Abyss -- Planning to Seek Entropy When Reward is Scarce", "categories": ["cs.AI"], "comment": "9 pages without appendix, 15 Figures, preprint", "summary": "Model-based reinforcement learning (MBRL) offers an intuitive way to increase\nthe sample efficiency of model-free RL methods by simultaneously training a\nworld model that learns to predict the future. MBRL methods have progressed by\nlargely prioritising the actor; optimising the world model learning has been\nneglected meanwhile. Improving the fidelity of the world model and reducing its\ntime to convergence can yield significant downstream benefits, one of which is\nimproving the ensuing performance of any actor it may train. We propose a novel\napproach that anticipates and actively seeks out high-entropy states using\nshort-horizon latent predictions generated by the world model, offering a\nprincipled alternative to traditional curiosity-driven methods that chase\nonce-novel states well after they were stumbled into. While many model\npredictive control (MPC) based methods offer similar alternatives, they\ntypically lack commitment, synthesising multi step plans after every step. To\nmitigate this, we present a hierarchical planner that dynamically decides when\nto replan, planning horizon length, and the weighting between reward and\nentropy. While our method can theoretically be applied to any model that trains\nits own actors with solely model generated data, we have applied it to just\nDreamer as a proof of concept. Our method finishes the Miniworld procedurally\ngenerated mazes 50% faster than base Dreamer at convergence and the policy\ntrained in imagination converges in only 60% of the environment steps that base\nDreamer needs."}
{"id": "2505.16258", "pdf": "https://arxiv.org/pdf/2505.16258", "abs": "https://arxiv.org/abs/2505.16258", "authors": ["Aashish Anantha Ramakrishnan", "Aadarsh Anantha Ramakrishnan", "Dongwon Lee"], "title": "IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50", "I.2.7; I.2.10"], "comment": null, "summary": "Interpreting figurative language such as sarcasm across multi-modal inputs\npresents unique challenges, often requiring task-specific fine-tuning and\nextensive reasoning steps. However, current Chain-of-Thought approaches do not\nefficiently leverage the same cognitive processes that enable humans to\nidentify sarcasm. We present IRONIC, an in-context learning framework that\nleverages Multi-modal Coherence Relations to analyze referential, analogical\nand pragmatic image-text linkages. Our experiments show that IRONIC achieves\nstate-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across\ndifferent baselines. This demonstrates the need for incorporating linguistic\nand cognitive insights into the design of multi-modal reasoning strategies. Our\ncode is available at: https://github.com/aashish2000/IRONIC"}
{"id": "2505.16326", "pdf": "https://arxiv.org/pdf/2505.16326", "abs": "https://arxiv.org/abs/2505.16326", "authors": ["Qian Tan", "Dongzhan Zhou", "Peng Xia", "Wanhao Liu", "Wanli Ouyang", "Lei Bai", "Yuqiang Li", "Tianfan Fu"], "title": "ChemMLLM: Chemical Multimodal Large Language Model", "categories": ["cs.LG"], "comment": "25 pages", "summary": "Multimodal large language models (MLLMs) have made impressive progress in\nmany applications in recent years. However, chemical MLLMs that can handle\ncross-modal understanding and generation remain underexplored. To fill this\ngap, in this paper, we propose ChemMLLM, a unified chemical multimodal large\nlanguage model for molecule understanding and generation. Also, we design five\nmultimodal tasks across text, molecular SMILES strings, and image, and curate\nthe datasets. We benchmark ChemMLLM against a range of general leading MLLMs\nand Chemical LLMs on these tasks. Experimental results show that ChemMLLM\nachieves superior performance across all evaluated tasks. For example, in\nmolecule image optimization task, ChemMLLM outperforms the best baseline\n(GPT-4o) by 118.9\\% (4.27 vs 1.95 property improvement). The code is publicly\navailable at https://github.com/bbsbz/ChemMLLM.git."}
{"id": "2505.16826", "pdf": "https://arxiv.org/pdf/2505.16826", "abs": "https://arxiv.org/abs/2505.16826", "authors": ["Wei Sun", "Wen Yang", "Pu Jian", "Qianlong Du", "Fuwei Cui", "Shuo Ren", "Jiajun Zhang"], "title": "KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances have demonstrated that integrating reinforcement learning\nwith rule-based rewards can significantly enhance the reasoning capabilities of\nlarge language models, even without supervised fine-tuning. However, prevalent\nreinforcement learning algorithms such as GRPO and its variants like DAPO,\nsuffer from a coarse granularity issue when computing the advantage.\nSpecifically, they compute rollout-level advantages that assign identical\nvalues to every token within a sequence, failing to capture token-specific\ncontributions and hindering effective learning. To address this limitation, we\npropose Key-token Advantage Estimation (KTAE) - a novel algorithm that\nestimates fine-grained, token-level advantages without introducing additional\nmodels. KTAE leverages the correctness of sampled rollouts and applies\nstatistical analysis to quantify the importance of individual tokens within a\nsequence to the final outcome. This quantified token-level importance is then\ncombined with the rollout-level advantage to obtain a more fine-grained\ntoken-level advantage estimation. Empirical results show that models trained\nwith GRPO+KTAE and DAPO+KTAE outperform baseline methods across five\nmathematical reasoning benchmarks. Notably, they achieve higher accuracy with\nshorter responses and even surpass R1-Distill-Qwen-1.5B using the same base\nmodel."}
{"id": "2505.16270", "pdf": "https://arxiv.org/pdf/2505.16270", "abs": "https://arxiv.org/abs/2505.16270", "authors": ["Jiaru Zou", "Yikun Ban", "Zihao Li", "Yunzhe Qi", "Ruizhong Qiu", "Ling Yang", "Jingrui He"], "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "33 pages, 7 figures", "summary": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability."}
{"id": "2505.16333", "pdf": "https://arxiv.org/pdf/2505.16333", "abs": "https://arxiv.org/abs/2505.16333", "authors": ["Chaerin Kong", "Jiho Jang", "Nojun Kwak"], "title": "Understanding Differential Transformer Unchains Pretrained Self-Attentions", "categories": ["cs.LG"], "comment": "9 pages", "summary": "Differential Transformer has recently gained significant attention for its\nimpressive empirical performance, often attributed to its ability to perform\nnoise canceled attention. However, precisely how differential attention\nachieves its empirical benefits remains poorly understood. Moreover,\nDifferential Transformer architecture demands large-scale training from\nscratch, hindering utilization of open pretrained weights. In this work, we\nconduct an in-depth investigation of Differential Transformer, uncovering three\nkey factors behind its success: (1) enhanced expressivity via negative\nattention, (2) reduced redundancy among attention heads, and (3) improved\nlearning dynamics. Based on these findings, we propose DEX, a novel method to\nefficiently integrate the advantages of differential attention into pretrained\nlanguage models. By reusing the softmax attention scores and adding a\nlightweight differential operation on the output value matrix, DEX effectively\nincorporates the key advantages of differential attention while remaining\nlightweight in both training and inference. Evaluations confirm that DEX\nsubstantially improves the pretrained LLMs across diverse benchmarks, achieving\nsignificant performance gains with minimal adaptation data (< 0.01\\%)."}
{"id": "2505.16827", "pdf": "https://arxiv.org/pdf/2505.16827", "abs": "https://arxiv.org/abs/2505.16827", "authors": ["Bin Xie", "Rui Shao", "Gongwei Chen", "Kaiwen Zhou", "Yinchuan Li", "Jie Liu", "Min Zhang", "Liqiang Nie"], "title": "GUI-explorer: Autonomous Exploration and Mining of Transition-aware Knowledge for GUI Agent", "categories": ["cs.AI"], "comment": "ACL 2025. Github: https://github.com/JiuTian-VL/GUI-explorer", "summary": "GUI automation faces critical challenges in dynamic environments. MLLMs\nsuffer from two key issues: misinterpreting UI components and outdated\nknowledge. Traditional fine-tuning methods are costly for app-specific\nknowledge updates. We propose GUI-explorer, a training-free GUI agent that\nincorporates two fundamental mechanisms: (1) Autonomous Exploration of\nFunction-aware Trajectory. To comprehensively cover all application\nfunctionalities, we design a Function-aware Task Goal Generator that\nautomatically constructs exploration goals by analyzing GUI structural\ninformation (e.g., screenshots and activity hierarchies). This enables\nsystematic exploration to collect diverse trajectories. (2) Unsupervised Mining\nof Transition-aware Knowledge. To establish precise screen-operation logic, we\ndevelop a Transition-aware Knowledge Extractor that extracts effective\nscreen-operation logic through unsupervised analysis the state transition of\nstructured interaction triples (observation, action, outcome). This eliminates\nthe need for human involvement in knowledge extraction. With a task success\nrate of 53.7% on SPA-Bench and 47.4% on AndroidWorld, GUI-explorer shows\nsignificant improvements over SOTA agents. It requires no parameter updates for\nnew apps. GUI-explorer is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/GUI-explorer."}
{"id": "2505.16277", "pdf": "https://arxiv.org/pdf/2505.16277", "abs": "https://arxiv.org/abs/2505.16277", "authors": ["Sheng-Fu Wang", "Laurent Prevot", "Jou-an Chi", "Ri-Sheng Huang", "Shu-Kai Hsieh"], "title": "Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility", "categories": ["cs.CL"], "comment": "The 14th Workshop on Cognitive Modeling and Computational Linguistics\n  (CMCL). May 3, 2025. Collocated with NAACL 2025", "summary": "The achievements of Large Language Models in Natural Language Processing,\nespecially for high-resource languages, call for a better understanding of\ntheir characteristics from a cognitive perspective. Researchers have attempted\nto evaluate artificial models by testing their ability to predict behavioral\n(e.g., eye-tracking fixations) and physiological (e.g., brain responses)\nvariables during language processing (e.g., reading/listening). In this paper,\nwe propose using spontaneous speech corpora to derive production variables\n(speech reductions, prosodic prominences) and applying them in a similar\nfashion. More precisely, we extract. We then test models trained with a\nstandard procedure on different pretraining datasets (written, spoken, and\nmixed genres) for their ability to predict these two variables. Our results\nshow that, after some fine-tuning, the models can predict these production\nvariables well above baselines. We also observe that spoken genre training data\nprovides more accurate predictions than written genres. These results\ncontribute to the broader effort of using high-quality speech corpora as\nbenchmarks for LLMs."}
{"id": "2505.16340", "pdf": "https://arxiv.org/pdf/2505.16340", "abs": "https://arxiv.org/abs/2505.16340", "authors": ["Yunhui Jang", "Jaehyung Kim", "Sungsoo Ahn"], "title": "Improving Chemical Understanding of LLMs via SMILES Parsing", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) are increasingly recognized as powerful tools\nfor scientific discovery, particularly in molecular science. A fundamental\nrequirement for these models is the ability to accurately understand molecular\nstructures, commonly encoded in the SMILES representation. However, current\nLLMs struggle to interpret SMILES, even failing to carry out basic tasks such\nas counting molecular rings. To address this limitation, we introduce CLEANMOL,\na novel framework that formulates SMILES parsing into a suite of clean and\ndeterministic tasks explicitly designed to promote graph-level molecular\ncomprehension. These tasks span from subgraph matching to global graph\nmatching, providing structured supervision aligned with molecular structural\nproperties. We construct a molecular pretraining dataset with adaptive\ndifficulty scoring and pre-train open-source LLMs on these tasks. Our results\nshow that CLEANMOL not only enhances structural comprehension but also achieves\nthe best or competes with the baseline on the Mol-Instructions benchmark."}
{"id": "2505.16832", "pdf": "https://arxiv.org/pdf/2505.16832", "abs": "https://arxiv.org/abs/2505.16832", "authors": ["Haonian Ji", "Shi Qiu", "Siyang Xin", "Siwei Han", "Zhaorun Chen", "Hongyi Wang", "Dake Zhang", "Huaxiu Yao"], "title": "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "16 pages; 7 figures", "summary": "While foundation models (FMs), such as diffusion models and large\nvision-language models (LVLMs), have been widely applied in educational\ncontexts, their ability to generate pedagogically effective visual explanations\nremains limited. Most existing approaches focus primarily on textual reasoning,\noverlooking the critical role of structured and interpretable visualizations in\nsupporting conceptual understanding. To better assess the visual reasoning\ncapabilities of FMs in educational settings, we introduce EduVisBench, a\nmulti-domain, multi-level benchmark. EduVisBench features diverse STEM problem\nsets requiring visually grounded solutions, along with a fine-grained\nevaluation rubric informed by pedagogical theory. Our empirical analysis\nreveals that existing models frequently struggle with the inherent challenge of\ndecomposing complex reasoning and translating it into visual representations\naligned with human cognitive processes. To address these limitations, we\npropose EduVisAgent, a multi-agent collaborative framework that coordinates\nspecialized agents for instructional planning, reasoning decomposition,\nmetacognitive prompting, and visualization design. Experimental results show\nthat EduVisAgent substantially outperforms all baselines, achieving a 40.2%\nimprovement and delivering more educationally aligned visualizations.\nEduVisBench and EduVisAgent are available at\nhttps://github.com/aiming-lab/EduVisBench and\nhttps://github.com/aiming-lab/EduVisAgent."}
{"id": "2505.16281", "pdf": "https://arxiv.org/pdf/2505.16281", "abs": "https://arxiv.org/abs/2505.16281", "authors": ["Shijie Zhang", "Renhao Li", "Songsheng Wang", "Philipp Koehn", "Min Yang", "Derek F. Wong"], "title": "HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "The advancement of Large Language Models (LLMs) enables flexible and\ninterpretable automatic evaluations. In the field of machine translation\nevaluation, utilizing LLMs with translation error annotations based on\nMultidimensional Quality Metrics (MQM) yields more human-aligned judgments.\nHowever, current LLM-based evaluation methods still face challenges in\naccurately identifying error spans and assessing their severity. In this paper,\nwe propose HiMATE, a Hierarchical Multi-Agent Framework for Machine Translation\nEvaluation. We argue that existing approaches inadequately exploit the\nfine-grained structural and semantic information within the MQM hierarchy. To\naddress this, we develop a hierarchical multi-agent system grounded in the MQM\nerror typology, enabling granular evaluation of subtype errors. Two key\nstrategies are incorporated to further mitigate systemic hallucinations within\nthe framework: the utilization of the model's self-reflection capability and\nthe facilitation of agent discussion involving asymmetric information.\nEmpirically, HiMATE outperforms competitive baselines across different datasets\nin conducting human-aligned evaluations. Further analyses underscore its\nsignificant advantage in error span detection and severity assessment,\nachieving an average F1-score improvement of 89% over the best-performing\nbaseline. We make our code and data publicly available at\nhttps://anonymous.4open.science/r/HiMATE-Anony."}
{"id": "2505.16341", "pdf": "https://arxiv.org/pdf/2505.16341", "abs": "https://arxiv.org/abs/2505.16341", "authors": ["Yaxin Hou", "Yuheng Jia"], "title": "A Square Peg in a Square Hole: Meta-Expert for Long-Tailed Semi-Supervised Learning", "categories": ["cs.LG"], "comment": "The paper is accepted by ICML 2025", "summary": "This paper studies the long-tailed semi-supervised learning (LTSSL) with\ndistribution mismatch, where the class distribution of the labeled training\ndata follows a long-tailed distribution and mismatches with that of the\nunlabeled training data. Most existing methods introduce auxiliary classifiers\n(experts) to model various unlabeled data distributions and produce\npseudo-labels, but the expertises of various experts are not fully utilized. We\nobserve that different experts are good at predicting different intervals of\nsamples, e.g., long-tailed expert is skilled in samples located in the head\ninterval and uniform expert excels in samples located in the medium interval.\nTherefore, we propose a dynamic expert assignment module that can estimate the\nclass membership (i.e., head, medium, or tail class) of samples, and\ndynamically assigns suitable expert to each sample based on the estimated\nmembership to produce high-quality pseudo-label in the training phase and\nproduce prediction in the testing phase. We also theoretically reveal that\nintegrating different experts' strengths will lead to a smaller generalization\nerror bound. Moreover, we find that the deeper features are more biased toward\nthe head class but with more discriminative ability, while the shallower\nfeatures are less biased but also with less discriminative ability. We,\ntherefore, propose a multi-depth feature fusion module to utilize different\ndepth features to mitigate the model bias. Our method demonstrates its\neffectiveness through comprehensive experiments on the CIFAR-10-LT, STL-10-LT,\nand SVHN-LT datasets across various settings. The code is available at\nhttps://github.com/yaxinhou/Meta-Expert."}
{"id": "2505.16854", "pdf": "https://arxiv.org/pdf/2505.16854", "abs": "https://arxiv.org/abs/2505.16854", "authors": ["Jiaqi Wang", "Kevin Qinghong Lin", "James Cheng", "Mike Zheng Shou"], "title": "Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Reinforcement Learning (RL) has proven to be an effective post-training\nstrategy for enhancing reasoning in vision-language models (VLMs). Group\nRelative Policy Optimization (GRPO) is a recent prominent method that\nencourages models to generate complete reasoning traces before answering,\nleading to increased token usage and computational cost. Inspired by the\nhuman-like thinking process-where people skip reasoning for easy questions but\nthink carefully when needed-we explore how to enable VLMs to first decide when\nreasoning is necessary. To realize this, we propose TON, a two-stage training\nstrategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective\n'thought dropout' operation, where reasoning traces are randomly replaced with\nempty thoughts. This introduces a think-or-not format that serves as a cold\nstart for selective reasoning; (ii) a GRPO stage that enables the model to\nfreely explore when to think or not, while maximizing task-aware outcome\nrewards. Experimental results show that TON can reduce the completion length by\nup to 90% compared to vanilla GRPO, without sacrificing performance or even\nimproving it. Further evaluations across diverse vision-language tasks-covering\na range of reasoning difficulties under both 3B and 7B models-consistently\nreveal that the model progressively learns to bypass unnecessary reasoning\nsteps as training advances. These findings shed light on the path toward\nhuman-like reasoning patterns in reinforcement learning approaches. Our code is\navailable at https://github.com/kokolerk/TON."}
{"id": "2505.16293", "pdf": "https://arxiv.org/pdf/2505.16293", "abs": "https://arxiv.org/abs/2505.16293", "authors": ["Rishabh Maheshwary", "Masoud Hashemi", "Khyati Mahajan", "Shiva Krishna Reddy Malay", "Sai Rajeswar", "Sathwik Tejaswi Madhusudhan", "Spandana Gella", "Vikas Yadav"], "title": "Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA", "categories": ["cs.CL"], "comment": null, "summary": "Iterative RAG for multi-hop question answering faces challenges with lengthy\ncontexts and the buildup of irrelevant information. This hinders a model's\ncapacity to process and reason over retrieved content and limits performance.\nWhile recent methods focus on compressing retrieved information, they are\neither restricted to single-round RAG, require finetuning or lack scalability\nin iterative RAG. To address these challenges, we propose Notes Writing, a\nmethod that generates concise and relevant notes from retrieved documents at\neach step, thereby reducing noise and retaining only essential information.\nThis indirectly increases the effective context length of Large Language Models\n(LLMs), enabling them to reason and plan more effectively while processing\nlarger volumes of input text. Notes Writing is framework agnostic and can be\nintegrated with different iterative RAG methods. We demonstrate its\neffectiveness with three iterative RAG methods, across two models and four\nevaluation datasets. Notes writing yields an average improvement of 15.6\npercentage points overall, with minimal increase in output tokens."}
{"id": "2505.16353", "pdf": "https://arxiv.org/pdf/2505.16353", "abs": "https://arxiv.org/abs/2505.16353", "authors": ["Céline Comte", "Pascal Moyal"], "title": "Arrival Control in Quasi-Reversible Queueing Systems: Optimization and Reinforcement Learning", "categories": ["cs.LG", "math.OC", "math.PR"], "comment": null, "summary": "In this paper, we introduce a versatile scheme for optimizing the arrival\nrates of quasi-reversible queueing systems. We first propose an alternative\ndefinition of quasi-reversibility that encompasses reversibility and highlights\nthe importance of the definition of customer classes. In a second time, we\nintroduce balanced arrival control policies, which generalize the notion of\nbalanced arrival rates introduced in the context of Whittle networks, to the\nmuch broader class of quasi-reversible queueing systems. We prove that\nsupplementing a quasi-reversible queueing system with a balanced\narrival-control policy preserves the quasi-reversibility, and we specify the\nform of the stationary measures. We revisit two canonical examples of\nquasi-reversible queueing systems, Whittle networks and order-independent\nqueues. Lastly, we focus on the problem of admission control and leverage our\nresults in the frameworks of optimization and reinforcement learning."}
{"id": "2505.16877", "pdf": "https://arxiv.org/pdf/2505.16877", "abs": "https://arxiv.org/abs/2505.16877", "authors": ["Yuqicheng Zhu", "Daniel Hernández", "Yuan He", "Zifeng Ding", "Bo Xiong", "Evgeny Kharlamov", "Steffen Staab"], "title": "Predicate-Conditional Conformalized Answer Sets for Knowledge Graph Embeddings", "categories": ["cs.AI"], "comment": "Accepted to the Finding of ACL 2025", "summary": "Uncertainty quantification in Knowledge Graph Embedding (KGE) methods is\ncrucial for ensuring the reliability of downstream applications. A recent work\napplies conformal prediction to KGE methods, providing uncertainty estimates by\ngenerating a set of answers that is guaranteed to include the true answer with\na predefined confidence level. However, existing methods provide probabilistic\nguarantees averaged over a reference set of queries and answers (marginal\ncoverage guarantee). In high-stakes applications such as medical diagnosis, a\nstronger guarantee is often required: the predicted sets must provide\nconsistent coverage per query (conditional coverage guarantee). We propose\nCondKGCP, a novel method that approximates predicate-conditional coverage\nguarantees while maintaining compact prediction sets. CondKGCP merges\npredicates with similar vector representations and augments calibration with\nrank information. We prove the theoretical guarantees and demonstrate empirical\neffectiveness of CondKGCP by comprehensive evaluations."}
{"id": "2505.16297", "pdf": "https://arxiv.org/pdf/2505.16297", "abs": "https://arxiv.org/abs/2505.16297", "authors": ["Seongryong Jung", "Suwan Yoon", "DongGeon Kim", "Hwanhee Lee"], "title": "ToDi: Token-wise Distillation via Fine-Grained Divergence Control", "categories": ["cs.CL"], "comment": "13 pages, 7 figures", "summary": "Large language models (LLMs) offer impressive performance but are impractical\nfor resource-constrained deployment due to high latency and energy consumption.\nKnowledge distillation (KD) addresses this by transferring knowledge from a\nlarge teacher to a smaller student model. However, conventional KD, notably\napproaches like Forward KL (FKL) and Reverse KL (RKL), apply uniform divergence\nloss across the entire vocabulary, neglecting token-level prediction\ndiscrepancies. By investigating these representative divergences via gradient\nanalysis, we reveal that FKL boosts underestimated tokens, while RKL suppresses\noverestimated ones, showing their complementary roles. Based on this\nobservation, we propose Token-wise Distillation (ToDi), a novel method that\nadaptively combines FKL and RKL per token using a sigmoid-based weighting\nfunction derived from the teacher-student probability log-ratio. ToDi\ndynamically emphasizes the appropriate divergence for each token, enabling\nprecise distribution alignment. We demonstrate that ToDi consistently\noutperforms recent distillation baselines using uniform or less granular\nstrategies across instruction-following benchmarks. Extensive ablation studies\nand efficiency analysis further validate ToDi's effectiveness and practicality."}
{"id": "2505.16363", "pdf": "https://arxiv.org/pdf/2505.16363", "abs": "https://arxiv.org/abs/2505.16363", "authors": ["Huishuai Zhang", "Bohan Wang", "Luoxin Chen"], "title": "AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We introduce AdamS, a simple yet effective alternative to Adam for large\nlanguage model (LLM) pretraining and post-training. By leveraging a novel\ndenominator, i.e., the root of weighted sum of squares of the momentum and the\ncurrent gradient, AdamS eliminates the need for second-moment estimates. Hence,\nAdamS is efficient, matching the memory and compute footprint of SGD with\nmomentum while delivering superior optimization performance. Moreover, AdamS is\neasy to adopt: it can directly inherit hyperparameters of AdamW, and is\nentirely model-agnostic, integrating seamlessly into existing pipelines without\nmodifications to optimizer APIs or architectures. The motivation behind AdamS\nstems from the observed $(L_0, L_1)$ smoothness properties in transformer\nobjectives, where local smoothness is governed by gradient magnitudes that can\nbe further approximated by momentum magnitudes. We establish rigorous\ntheoretical convergence guarantees and provide practical guidelines for\nhyperparameter selection. Empirically, AdamS demonstrates strong performance in\nvarious tasks, including pre-training runs on GPT-2 and Llama2 (up to 13B\nparameters) and reinforcement learning in post-training regimes. With its\nefficiency, simplicity, and theoretical grounding, AdamS stands as a compelling\nalternative to existing optimizers."}
{"id": "2505.16899", "pdf": "https://arxiv.org/pdf/2505.16899", "abs": "https://arxiv.org/abs/2505.16899", "authors": ["Kerem Oktar", "Katherine M. Collins", "Jose Hernandez-Orallo", "Diane Coyle", "Stephen Cave", "Adrian Weller", "Ilia Sucholutsky"], "title": "Identifying, Evaluating, and Mitigating Risks of AI Thought Partnerships", "categories": ["cs.AI"], "comment": null, "summary": "Artificial Intelligence (AI) systems have historically been used as tools\nthat execute narrowly defined tasks. Yet recent advances in AI have unlocked\npossibilities for a new class of models that genuinely collaborate with humans\nin complex reasoning, from conceptualizing problems to brainstorming solutions.\nSuch AI thought partners enable novel forms of collaboration and extended\ncognition, yet they also pose major risks-including and beyond risks of typical\nAI tools and agents. In this commentary, we systematically identify risks of AI\nthought partners through a novel framework that identifies risks at multiple\nlevels of analysis, including Real-time, Individual, and Societal risks arising\nfrom collaborative cognition (RISc). We leverage this framework to propose\nconcrete metrics for risk evaluation, and finally suggest specific mitigation\nstrategies for developers and policymakers. As AI thought partners continue to\nproliferate, these strategies can help prevent major harms and ensure that\nhumans actively benefit from productive thought partnerships."}
{"id": "2505.16303", "pdf": "https://arxiv.org/pdf/2505.16303", "abs": "https://arxiv.org/abs/2505.16303", "authors": ["Haochen Shi", "Tianshi Zheng", "Weiqi Wang", "Baixuan Xu", "Chunyang Li", "Chunkit Chan", "Tao Fan", "Yangqiu Song", "Qiang Yang"], "title": "INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling", "categories": ["cs.CL"], "comment": "17 pages", "summary": "Large Language Model (LLM) routing is a pivotal technique for navigating a\ndiverse landscape of LLMs, aiming to select the best-performing LLMs tailored\nto the domains of user queries, while managing computational resources.\nHowever, current routing approaches often face limitations in scalability when\ndealing with a large pool of specialized LLMs, or in their adaptability to\nextending model scope and evolving capability domains. To overcome those\nchallenges, we propose InferenceDynamics, a flexible and scalable\nmulti-dimensional routing framework by modeling the capability and knowledge of\nmodels. We operate it on our comprehensive dataset RouteMix, and demonstrate\nits effectiveness and generalizability in group-level routing using modern\nbenchmarks including MMLU-Pro, GPQA, BigGenBench, and LiveBench, showcasing its\nability to identify and leverage top-performing models for given tasks, leading\nto superior outcomes with efficient resource utilization. The broader adoption\nof Inference Dynamics can empower users to harness the full specialized\npotential of the LLM ecosystem, and our code will be made publicly available to\nencourage further research."}
{"id": "2505.16365", "pdf": "https://arxiv.org/pdf/2505.16365", "abs": "https://arxiv.org/abs/2505.16365", "authors": ["Manuel Ruiz-Botella", "Marta Sales-Pardo", "Roger Guimerà"], "title": "A collaborative constrained graph diffusion model for the generation of realistic synthetic molecules", "categories": ["cs.LG", "cs.AI", "physics.comp-ph", "q-bio.QM"], "comment": "28 pages, 10 figures, 4 tables", "summary": "Developing new molecular compounds is crucial to address pressing challenges,\nfrom health to environmental sustainability. However, exploring the molecular\nspace to discover new molecules is difficult due to the vastness of the space.\nHere we introduce CoCoGraph, a collaborative and constrained graph diffusion\nmodel capable of generating molecules that are guaranteed to be chemically\nvalid. Thanks to the constraints built into the model and to the collaborative\nmechanism, CoCoGraph outperforms state-of-the-art approaches on standard\nbenchmarks while requiring up to an order of magnitude fewer parameters.\nAnalysis of 36 chemical properties also demonstrates that CoCoGraph generates\nmolecules with distributions more closely matching real molecules than current\nmodels. Leveraging the model's efficiency, we created a database of 8.2M\nmillion synthetically generated molecules and conducted a Turing-like test with\norganic chemistry experts to further assess the plausibility of the generated\nmolecules, and potential biases and limitations of CoCoGraph."}
{"id": "2505.16928", "pdf": "https://arxiv.org/pdf/2505.16928", "abs": "https://arxiv.org/abs/2505.16928", "authors": ["Bosung Kim", "Prithviraj Ammanabrolu"], "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks\nthat advances long-context understanding in embodied AI. $\\infty$-THOR\nprovides: (1) a generation framework for synthesizing scalable, reproducible,\nand unlimited long-horizon trajectories; (2) a novel embodied QA task,\nNeedle(s) in the Embodied Haystack, where multiple scattered clues across\nextended trajectories test agents' long-context reasoning ability; and (3) a\nlong-horizon dataset and benchmark suite featuring complex tasks that span\nhundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including\ninterleaved Goal-State-Action modeling, context extension techniques, and\nContext Parallelism, to equip LLM-based agents for extreme long-context\nreasoning and interaction. Experimental results and analyses highlight the\nchallenges posed by our benchmark and provide insights into training strategies\nand model behaviors under long-horizon conditions. Our work provides a\nfoundation for the next generation of embodied AI systems capable of robust,\nlong-term reasoning and planning."}
{"id": "2505.16307", "pdf": "https://arxiv.org/pdf/2505.16307", "abs": "https://arxiv.org/abs/2505.16307", "authors": ["Chenzhuo Zhao", "Ziqian Liu", "Xingda Wang", "Junting Lu", "Chaoyi Ruan"], "title": "PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Prompt optimization offers a practical and broadly applicable alternative to\nfine-tuning for improving large language model (LLM) performance. However,\nexisting methods often rely on costly output generation, self-critiquing\nabilities, or human-annotated preferences, which limit their scalability,\nespecially for smaller or non-instruction-tuned models. We introduce PMPO\n(Probabilistic Metric Prompt Optimization), a unified framework that refines\nprompts using token-level cross-entropy loss as a direct, lightweight\nevaluation signal. PMPO identifies low-quality prompt segments by masking and\nmeasuring their impact on loss, then rewrites and selects improved variants by\nminimizing loss over positive and negative examples. Unlike prior methods, it\nrequires no output sampling or human evaluation during optimization, relying\nonly on forward passes and log-likelihoods. PMPO supports both supervised and\npreference-based tasks through a closely aligned loss-based evaluation\nstrategy. Experiments show that PMPO consistently outperforms prior methods\nacross model sizes and tasks: it achieves the highest average accuracy on BBH,\nperforms strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates\nby over 19 points. These results highlight PMPO's effectiveness, efficiency,\nand broad applicability."}
{"id": "2505.16368", "pdf": "https://arxiv.org/pdf/2505.16368", "abs": "https://arxiv.org/abs/2505.16368", "authors": ["Huanyu Liu", "Jia Li", "Hao Zhu", "Kechi Zhang", "Yihong Dong", "Ge Li"], "title": "SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "How to design reinforcement learning (RL) tasks that effectively unleash the\nreasoning capability of large language models (LLMs) remains an open question.\nExisting RL tasks (e.g., math, programming, and constructing reasoning tasks)\nsuffer from three key limitations: (1) Scalability. They rely heavily on human\nannotation or expensive LLM synthesis to generate sufficient training data. (2)\nVerifiability. LLMs' outputs are hard to verify automatically and reliably. (3)\nControllable Difficulty. Most tasks lack fine-grained difficulty control,\nmaking it hard to train LLMs to develop reasoning ability from easy to hard.\n  To address these limitations, we propose Saturn, a SAT-based RL framework\nthat uses Boolean Satisfiability (SAT) problems to train and evaluate LLM\nreasoning. Saturn enables scalable task construction, rule-based verification,\nand precise difficulty control. Saturn designs a curriculum learning pipeline\nthat continuously improves LLMs' reasoning capability by constructing SAT tasks\nof increasing difficulty and training LLMs from easy to hard. To ensure stable\ntraining, we design a principled mechanism to control difficulty transitions.\n  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying\ndifficulty. It supports the evaluation of how LLM reasoning changes with\nproblem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain\nSaturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT\nproblems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of\n+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B\nand Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,\nAIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in\nconstructing RL tasks, Saturn achieves further improvements of +8.8%. We\nrelease the source code, data, and models to support future research."}
{"id": "2505.16938", "pdf": "https://arxiv.org/pdf/2505.16938", "abs": "https://arxiv.org/abs/2505.16938", "authors": ["NovelSeek Team", "Bo Zhang", "Shiyang Feng", "Xiangchao Yan", "Jiakang Yuan", "Zhiyin Yu", "Xiaohan He", "Songtao Huang", "Shaowei Hou", "Zheng Nie", "Zhilong Wang", "Jinyao Liu", "Runmin Ma", "Tianshuo Peng", "Peng Ye", "Dongzhan Zhou", "Shufei Zhang", "Xiaosong Wang", "Yilan Zhang", "Meng Li", "Zhongying Tu", "Xiangyu Yue", "Wangli Ouyang", "Bowen Zhou", "Lei Bai"], "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "HomePage: https://alpha-innovator.github.io/NovelSeek-project-page", "summary": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours."}
{"id": "2505.16325", "pdf": "https://arxiv.org/pdf/2505.16325", "abs": "https://arxiv.org/abs/2505.16325", "authors": ["Yuyang Jiang", "Chacha Chen", "Shengyuan Wang", "Feng Li", "Zecong Tang", "Benjamin M. Mervak", "Lydia Chelala", "Christopher M Straus", "Reve Chahine", "Samuel G. Armato III", "Chenhao Tan"], "title": "CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "18 pages, 4 figures", "summary": "Existing metrics often lack the granularity and interpretability to capture\nnuanced clinical differences between candidate and ground-truth radiology\nreports, resulting in suboptimal evaluation. We introduce a Clinically-grounded\ntabular framework with Expert-curated labels and Attribute-level comparison for\nRadiology report evaluation (CLEAR). CLEAR not only examines whether a report\ncan accurately identify the presence or absence of medical conditions, but also\nassesses whether it can precisely describe each positively identified condition\nacross five key attributes: first occurrence, change, severity, descriptive\nlocation, and recommendation. Compared to prior works, CLEAR's\nmulti-dimensional, attribute-level outputs enable a more comprehensive and\nclinically interpretable evaluation of report quality. Additionally, to measure\nthe clinical alignment of CLEAR, we collaborate with five board-certified\nradiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from\nMIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions.\nOur experiments show that CLEAR achieves high accuracy in extracting clinical\nattributes and provides automated metrics that are strongly aligned with\nclinical judgment."}
{"id": "2505.16386", "pdf": "https://arxiv.org/pdf/2505.16386", "abs": "https://arxiv.org/abs/2505.16386", "authors": ["Ahmed K. Kadhim", "Lei Jiao", "Rishad Shafik", "Ole-Christoffer Granmo"], "title": "Omni TM-AE: A Scalable and Interpretable Embedding Model Using the Full Tsetlin Machine State Space", "categories": ["cs.LG"], "comment": null, "summary": "The increasing complexity of large-scale language models has amplified\nconcerns regarding their interpretability and reusability. While traditional\nembedding models like Word2Vec and GloVe offer scalability, they lack\ntransparency and often behave as black boxes. Conversely, interpretable models\nsuch as the Tsetlin Machine (TM) have shown promise in constructing explainable\nlearning systems, though they previously faced limitations in scalability and\nreusability. In this paper, we introduce Omni Tsetlin Machine AutoEncoder (Omni\nTM-AE), a novel embedding model that fully exploits the information contained\nin the TM's state matrix, including literals previously excluded from clause\nformation. This method enables the construction of reusable, interpretable\nembeddings through a single training phase. Extensive experiments across\nsemantic similarity, sentiment classification, and document clustering tasks\nshow that Omni TM-AE performs competitively with and often surpasses mainstream\nembedding models. These results demonstrate that it is possible to balance\nperformance, scalability, and interpretability in modern Natural Language\nProcessing (NLP) systems without resorting to opaque architectures."}
{"id": "2505.16944", "pdf": "https://arxiv.org/pdf/2505.16944", "abs": "https://arxiv.org/abs/2505.16944", "authors": ["Yunjia Qi", "Hao Peng", "Xiaozhi Wang", "Amy Xin", "Youfeng Liu", "Bin Xu", "Lei Hou", "Juanzi Li"], "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research."}
{"id": "2505.16330", "pdf": "https://arxiv.org/pdf/2505.16330", "abs": "https://arxiv.org/abs/2505.16330", "authors": ["Wenqing Wu", "Chengzhi Zhang", "Tong Bao", "Yi Zhao"], "title": "SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers", "categories": ["cs.CL", "cs.AI", "cs.DL"], "comment": null, "summary": "Novelty is a core component of academic papers, and there are multiple\nperspectives on the assessment of novelty. Existing methods often focus on word\nor entity combinations, which provide limited insights. The content related to\na paper's novelty is typically distributed across different core sections,\ne.g., Introduction, Methodology and Results. Therefore, exploring the optimal\ncombination of sections for evaluating the novelty of a paper is important for\nadvancing automated novelty assessment. In this paper, we utilize different\ncombinations of sections from academic papers as inputs to drive language\nmodels to predict novelty scores. We then analyze the results to determine the\noptimal section combinations for novelty score prediction. We first employ\nnatural language processing techniques to identify the sectional structure of\nacademic papers, categorizing them into introduction, methods, results, and\ndiscussion (IMRaD). Subsequently, we used different combinations of these\nsections (e.g., introduction and methods) as inputs for pretrained language\nmodels (PLMs) and large language models (LLMs), employing novelty scores\nprovided by human expert reviewers as ground truth labels to obtain prediction\nresults. The results indicate that using introduction, results and discussion\nis most appropriate for assessing the novelty of a paper, while the use of the\nentire text does not yield significant results. Furthermore, based on the\nresults of the PLMs and LLMs, the introduction and results appear to be the\nmost important section for the task of novelty score prediction. The code and\ndataset for this paper can be accessed at\nhttps://github.com/njust-winchy/SC4ANM."}
{"id": "2505.16400", "pdf": "https://arxiv.org/pdf/2505.16400", "abs": "https://arxiv.org/abs/2505.16400", "authors": ["Yang Chen", "Zhuolin Yang", "Zihan Liu", "Chankyu Lee", "Peng Xu", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "We release the model at:\n  https://huggingface.co/nvidia/AceReason-Nemotron-14B", "summary": "Despite recent progress in large-scale reinforcement learning (RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such as\nDeepSeek-R1, including data curation strategies and RL training recipe, are\noften omitted. Moreover, recent research indicates distillation remains more\neffective than RL for smaller models. In this work, we demonstrate that\nlarge-scale RL can significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-art distillation-based models. We systematically study the RL\ntraining process through extensive ablations and propose a simple yet effective\napproach: first training on math-only prompts, then on code-only prompts.\nNotably, we find that math-only RL not only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,\nextended code-only RL iterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-based RL across both domains.\nFinally, we identify key experimental insights, including curriculum learning\nwith progressively increasing response lengths and the stabilizing effect of\non-policy parameter updates. We find that RL not only elicits the foundational\nreasoning capabilities acquired during pretraining and supervised fine-tuning\n(e.g., distillation), but also pushes the limits of the model's reasoning\nability, enabling it to solve problems that were previously unsolvable."}
{"id": "2505.16978", "pdf": "https://arxiv.org/pdf/2505.16978", "abs": "https://arxiv.org/abs/2505.16978", "authors": ["Weizhi Tang", "Yixuan Li", "Chris Sypherd", "Elizabeth Polgreen", "Vaishak Belle"], "title": "HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar Generation", "categories": ["cs.AI", "cs.PL"], "comment": "Accepted to ACL 2025 Findings. Code available at\n  https://github.com/RutaTang/HyGenar", "summary": "Grammar plays a critical role in natural language processing and text/code\ngeneration by enabling the definition of syntax, the creation of parsers, and\nguiding structured outputs. Although large language models (LLMs) demonstrate\nimpressive capabilities across domains, their ability to infer and generate\ngrammars has not yet been thoroughly explored. In this paper, we aim to study\nand improve the ability of LLMs for few-shot grammar generation, where grammars\nare inferred from sets of a small number of positive and negative examples and\ngenerated in Backus-Naur Form. To explore this, we introduced a novel dataset\ncomprising 540 structured grammar generation challenges, devised 6 metrics, and\nevaluated 8 various LLMs against it. Our findings reveal that existing LLMs\nperform sub-optimally in grammar generation. To address this, we propose an\nLLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar\ngeneration. HyGenar achieves substantial improvements in both the syntactic and\nsemantic correctness of generated grammars across LLMs."}
{"id": "2505.16348", "pdf": "https://arxiv.org/pdf/2505.16348", "abs": "https://arxiv.org/abs/2505.16348", "authors": ["Taeyoon Kwon", "Dongwook Choi", "Sunghwan Kim", "Hyojun Kim", "Seungjun Moon", "Beong-woo Kwak", "Kuan-Hao Huang", "Jinyoung Yeo"], "title": "Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Embodied agents empowered by large language models (LLMs) have shown strong\nperformance in household object rearrangement tasks. However, these tasks\nprimarily focus on single-turn interactions with simplified instructions, which\ndo not truly reflect the challenges of providing meaningful assistance to\nusers. To provide personalized assistance, embodied agents must understand the\nunique semantics that users assign to the physical world (e.g., favorite cup,\nbreakfast routine) by leveraging prior interaction history to interpret\ndynamic, real-world instructions. Yet, the effectiveness of embodied agents in\nutilizing memory for personalized assistance remains largely underexplored. To\naddress this gap, we present MEMENTO, a personalized embodied agent evaluation\nframework designed to comprehensively assess memory utilization capabilities to\nprovide personalized assistance. Our framework consists of a two-stage memory\nevaluation process design that enables quantifying the impact of memory\nutilization on task performance. This process enables the evaluation of agents'\nunderstanding of personalized knowledge in object rearrangement tasks by\nfocusing on its role in goal interpretation: (1) the ability to identify target\nobjects based on personal meaning (object semantics), and (2) the ability to\ninfer object-location configurations from consistent user patterns, such as\nroutines (user patterns). Our experiments across various LLMs reveal\nsignificant limitations in memory utilization, with even frontier models like\nGPT-4o experiencing a 30.5% performance drop when required to reference\nmultiple memories, particularly in tasks involving user patterns. These\nfindings, along with our detailed analyses and case studies, provide valuable\ninsights for future research in developing more effective personalized embodied\nagents. Project website: https://connoriginal.github.io/MEMENTO"}
{"id": "2505.16401", "pdf": "https://arxiv.org/pdf/2505.16401", "abs": "https://arxiv.org/abs/2505.16401", "authors": ["Xiaoqing Zhang", "Huabin Zheng", "Ang Lv", "Yuhan Liu", "Zirui Song", "Flood Sung", "Xiuying Chen", "Rui Yan"], "title": "Divide-Fuse-Conquer: Eliciting \"Aha Moments\" in Multi-Scenario Games", "categories": ["cs.LG"], "comment": "25 pages, 13 figures, and 8 tables", "summary": "Large language models (LLMs) have been observed to suddenly exhibit advanced\nreasoning abilities during reinforcement learning (RL), resembling an ``aha\nmoment'' triggered by simple outcome-based rewards. While RL has proven\neffective in eliciting such breakthroughs in tasks involving mathematics,\ncoding, and vision, it faces significant challenges in multi-scenario games.\nThe diversity of game rules, interaction modes, and environmental complexities\noften leads to policies that perform well in one scenario but fail to\ngeneralize to others. Simply combining multiple scenarios during training\nintroduces additional challenges, such as training instability and poor\nperformance. To overcome these challenges, we propose Divide-Fuse-Conquer, a\nframework designed to enhance generalization in multi-scenario RL. This\napproach starts by heuristically grouping games based on characteristics such\nas rules and difficulties. Specialized models are then trained for each group\nto excel at games in the group is what we refer to as the divide step. Next, we\nfuse model parameters from different groups as a new model, and continue\ntraining it for multiple groups, until the scenarios in all groups are\nconquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align\ntrained with the Divide-Fuse-Conquer strategy reaches a performance level\ncomparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can\ninspire future research on using reinforcement learning to improve the\ngeneralization of LLMs."}
{"id": "2505.16979", "pdf": "https://arxiv.org/pdf/2505.16979", "abs": "https://arxiv.org/abs/2505.16979", "authors": ["Zhenkun Li", "Lingyao Li", "Shuhang Lin", "Yongfeng Zhang"], "title": "Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System Design", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Single-agent LLMs hit hard limits--finite context, role overload, and brittle\ndomain transfer. Conventional multi-agent fixes soften those edges yet expose\nfresh pains: ill-posed decompositions, fuzzy contracts, and verification\noverhead that blunts the gains. We therefore present Know-The-Ropes (KtR), a\nframework that converts domain priors into an algorithmic blueprint hierarchy,\nin which tasks are recursively split into typed, controller-mediated subtasks,\neach solved zero-shot or with the lightest viable boost (e.g.,\nchain-of-thought, micro-tune, self-check). Grounded in the No-Free-Lunch\ntheorem, KtR trades the chase for a universal prompt for disciplined\ndecomposition. On the Knapsack problem (3-8 items), three GPT-4o-mini agents\nraise accuracy from 3% zero-shot to 95% on size-5 instances after patching a\nsingle bottleneck agent. On the tougher Task-Assignment problem (6-15 jobs), a\nsix-agent o3-mini blueprint hits 100% up to size 10 and 84% on sizes 13-15,\nversus 11% zero-shot. Algorithm-aware decomposition plus targeted augmentation\nthus turns modest models into reliable collaborators--no ever-larger monoliths\nrequired."}
{"id": "2505.16349", "pdf": "https://arxiv.org/pdf/2505.16349", "abs": "https://arxiv.org/abs/2505.16349", "authors": ["Pierre Achkar", "Tim Gollub", "Martin Potthast"], "title": "Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization", "categories": ["cs.CL"], "comment": "Accepted at SCOLIA@ECIR 2025 Workshop", "summary": "The exponential growth of scientific publications has made it increasingly\ndifficult for researchers to stay updated and synthesize knowledge effectively.\nThis paper presents XSum, a modular pipeline for multi-document summarization\n(MDS) in the scientific domain using Retrieval-Augmented Generation (RAG). The\npipeline includes two core components: a question-generation module and an\neditor module. The question-generation module dynamically generates questions\nadapted to the input papers, ensuring the retrieval of relevant and accurate\ninformation. The editor module synthesizes the retrieved content into coherent\nand well-structured summaries that adhere to academic standards for proper\ncitation. Evaluated on the SurveySum dataset, XSum demonstrates strong\nperformance, achieving considerable improvements in metrics such as CheckEval,\nG-Eval and Ref-F1 compared to existing approaches. This work provides a\ntransparent, adaptable framework for scientific summarization with potential\napplications in a wide range of domains. Code available at\nhttps://github.com/webis-de/scolia25-xsum"}
{"id": "2505.16403", "pdf": "https://arxiv.org/pdf/2505.16403", "abs": "https://arxiv.org/abs/2505.16403", "authors": ["Huazi Pan", "Yanjun Zhang", "Leo Yu Zhang", "Scott Adams", "Abbas Kouzani", "Suiyang Khoo"], "title": "Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Manipulation of local training data and local updates, i.e., the poisoning\nattack, is the main threat arising from the collaborative nature of the\nfederated learning (FL) paradigm. Most existing poisoning attacks aim to\nmanipulate local data/models in a way that causes denial-of-service (DoS)\nissues. In this paper, we introduce a novel attack method, named Federated\nLearning Sliding Attack (FedSA) scheme, aiming at precisely introducing the\nextent of poisoning in a subtle controlled manner. It operates with a\npredefined objective, such as reducing global model's prediction accuracy by\n10\\%. FedSA integrates robust nonlinear control-Sliding Mode Control (SMC)\ntheory with model poisoning attacks. It can manipulate the updates from\nmalicious clients to drive the global model towards a compromised state,\nachieving this at a controlled and inconspicuous rate. Additionally, leveraging\nthe robust control properties of FedSA allows precise control over the\nconvergence bounds, enabling the attacker to set the global accuracy of the\npoisoned model to any desired level. Experimental results demonstrate that\nFedSA can accurately achieve a predefined global accuracy with fewer malicious\nclients while maintaining a high level of stealth and adjustable learning\nrates."}
{"id": "2505.16982", "pdf": "https://arxiv.org/pdf/2505.16982", "abs": "https://arxiv.org/abs/2505.16982", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "categories": ["cs.AI", "physics.med-ph"], "comment": null, "summary": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress."}
{"id": "2505.16381", "pdf": "https://arxiv.org/pdf/2505.16381", "abs": "https://arxiv.org/abs/2505.16381", "authors": ["Songlin Yang", "Yikang Shen", "Kaiyue Wen", "Shawn Tan", "Mayank Mishra", "Liliang Ren", "Rameswar Panda", "Yoon Kim"], "title": "PaTH Attention: Position Encoding via Accumulating Householder Transformations", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "The attention mechanism is a core primitive in modern large language models\n(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,\nposition encoding is essential for modeling structured domains such as\nlanguage. Rotary position encoding (RoPE) has emerged as the de facto standard\napproach for position encoding and is part of many modern LLMs. However, in\nRoPE the key/query transformation between two elements in a sequence is only a\nfunction of their relative position and otherwise independent of the actual\ninput. This limits the expressivity of RoPE-based transformers.\n  This paper describes PaTH, a flexible data-dependent position encoding scheme\nbased on accumulated products of Householder(like) transformations, where each\ntransformation is data-dependent, i.e., a function of the input. We derive an\nefficient parallel algorithm for training through exploiting a compact\nrepresentation of products of Householder matrices, and implement a\nFlashAttention-style blockwise algorithm that minimizes I/O cost. Across both\ntargeted synthetic benchmarks and moderate-scale real-world language modeling\nexperiments, we find that PaTH demonstrates superior performance compared to\nRoPE and other recent baselines."}
{"id": "2505.16446", "pdf": "https://arxiv.org/pdf/2505.16446", "abs": "https://arxiv.org/abs/2505.16446", "authors": ["Zhaoxin Wang", "Handing Wang", "Cong Tian", "Yaochu Jin"], "title": "Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Multimodal large language models (MLLMs) enable powerful cross-modal\nreasoning capabilities. However, the expanded input space introduces new attack\nsurfaces. Previous jailbreak attacks often inject malicious instructions from\ntext into less aligned modalities, such as vision. As MLLMs increasingly\nincorporate cross-modal consistency and alignment mechanisms, such explicit\nattacks become easier to detect and block. In this work, we propose a novel\nimplicit jailbreak framework termed IJA that stealthily embeds malicious\ninstructions into images via least significant bit steganography and couples\nthem with seemingly benign, image-related textual prompts. To further enhance\nattack effectiveness across diverse MLLMs, we incorporate adversarial suffixes\ngenerated by a surrogate model and introduce a template optimization module\nthat iteratively refines both the prompt and embedding based on model feedback.\nOn commercial models like GPT-4o and Gemini-1.5 Pro, our method achieves attack\nsuccess rates of over 90% using an average of only 3 queries."}
{"id": "2505.16997", "pdf": "https://arxiv.org/pdf/2505.16997", "abs": "https://arxiv.org/abs/2505.16997", "authors": ["Rui Ye", "Xiangrui Liu", "Qimin Wu", "Xianghe Pang", "Zhenfei Yin", "Lei Bai", "Siheng Chen"], "title": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": "19 pages, 5 figures", "summary": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by\nenabling cooperation among multiple specialized agents. However, most existing\nMAS frameworks rely on a single LLM to drive all agents, constraining the\nsystem's intelligence to the limit of that model. This paper explores the\nparadigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by\ndiverse LLMs, elevating the system's potential to the collective intelligence\nof diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to\nevaluate the performance of various LLMs across different domains and\nMAS-related functions. As an extensive empirical study, we assess 27 LLMs\nacross 5 domains (encompassing 21 test sets) and 5 functions, conducting over\n1.7 million evaluations to identify optimal model selections for each\ndomain-function combination. Building on these findings, we demonstrate that\ntransitioning from homogeneous to heterogeneous LLM-driven MAS can\nsignificantly enhance system performance without requiring structural redesign.\nSpecifically, in a chatbot-only MAS scenario, the heterogeneous configuration\nyields up to 8.4\\% performance improvement on the MATH dataset. In a mixed\nchatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable\n47\\% performance boost on the AIME dataset. Our results underscore the\ntransformative potential of heterogeneous LLMs in MAS, highlighting a promising\navenue for advancing scalable, collaborative AI systems."}
{"id": "2505.16385", "pdf": "https://arxiv.org/pdf/2505.16385", "abs": "https://arxiv.org/abs/2505.16385", "authors": ["Kaiyu He", "Tong Zhou", "Yubo Chen", "Delai Qiu", "Shengping Liu", "Kang Liu", "Jun Zhao"], "title": "Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models", "categories": ["cs.CL"], "comment": "14 pages, 10 figures", "summary": "Large language models (LLMs) demonstrate remarkable ability in cross-lingual\ntasks. Understanding how LLMs acquire this ability is crucial for their\ninterpretability. To quantify the cross-lingual ability of LLMs accurately, we\npropose a Word-Level Cross-Lingual Translation Task. To find how LLMs learn\ncross-lingual ability, we trace the outputs of LLMs' intermediate layers in the\nword translation task. We identify and distinguish two distinct behaviors in\nthe forward pass of LLMs: co-occurrence behavior and semantic pivot behavior.\nWe attribute LLMs' two distinct behaviors to the co-occurrence frequency of\nwords and find the semantic pivot from the pre-training dataset. Finally, to\napply our findings to improve the cross-lingual ability of LLMs, we reconstruct\na semantic pivot-aware pre-training dataset using documents with a high\nproportion of semantic pivots. Our experiments validate the effectiveness of\nour approach in enhancing cross-lingual ability. Our research contributes\ninsights into the interpretability of LLMs and offers a method for improving\nLLMs' cross-lingual ability."}
{"id": "2505.16481", "pdf": "https://arxiv.org/pdf/2505.16481", "abs": "https://arxiv.org/abs/2505.16481", "authors": ["Xinxing Shi", "Xiaoyu Jiang", "Mauricio A. Álvarez"], "title": "Neighbour-Driven Gaussian Process Variational Autoencoders for Scalable Structured Latent Modelling", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Gaussian Process (GP) Variational Autoencoders (VAEs) extend standard VAEs by\nreplacing the fully factorised Gaussian prior with a GP prior, thereby\ncapturing richer correlations among latent variables. However, performing exact\nGP inference in large-scale GPVAEs is computationally prohibitive, often\nforcing existing approaches to rely on restrictive kernel assumptions or large\nsets of inducing points. In this work, we propose a neighbour-driven\napproximation strategy that exploits local adjacencies in the latent space to\nachieve scalable GPVAE inference. By confining computations to the nearest\nneighbours of each data point, our method preserves essential latent\ndependencies, allowing more flexible kernel choices and mitigating the need for\nnumerous inducing points. Through extensive experiments on tasks including\nrepresentation learning, data imputation, and conditional generation, we\ndemonstrate that our approach outperforms other GPVAE variants in both\npredictive performance and computational efficiency."}
{"id": "2502.15401", "pdf": "https://arxiv.org/pdf/2502.15401", "abs": "https://arxiv.org/abs/2502.15401", "authors": ["Xuetao Ma", "Wenbin Jiang", "Hua Huang"], "title": "Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In-context learning (ICL) can significantly enhance the complex reasoning\ncapabilities of large language models (LLMs), with the key lying in the\nselection and ordering of demonstration examples. Previous methods typically\nrelied on simple features to measure the relevance between examples. We argue\nthat these features are not sufficient to reflect the intrinsic connections\nbetween examples. In this study, we propose a curriculum ICL strategy guided by\nproblem-solving logic. We select demonstration examples by analyzing the\nproblem-solving logic and order them based on curriculum learning.\nSpecifically, we constructed a problem-solving logic instruction set based on\nthe BREAK dataset and fine-tuned a language model to analyze the\nproblem-solving logic of examples. Subsequently, we selected appropriate\ndemonstration examples based on problem-solving logic and assessed their\ndifficulty according to the number of problem-solving steps. In accordance with\nthe principles of curriculum learning, we ordered the examples from easy to\nhard to serve as contextual prompts. Experimental results on multiple\nbenchmarks indicate that our method outperforms previous ICL approaches in\nterms of performance and efficiency, effectively enhancing the complex\nreasoning capabilities of LLMs. Our project will be publicly available\nsubsequently."}
{"id": "2505.16392", "pdf": "https://arxiv.org/pdf/2505.16392", "abs": "https://arxiv.org/abs/2505.16392", "authors": ["Benjamin Vendeville", "Liana Ermakova", "Pierre De Loor"], "title": "Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection", "categories": ["cs.CL", "cs.AI", "I.2.6; I.5.2"], "comment": "Accepted at SIGIR 2025", "summary": "The general public often encounters complex texts but does not have the time\nor expertise to fully understand them, leading to the spread of misinformation.\nAutomatic Text Simplification (ATS) helps make information more accessible, but\nits evaluation methods have not kept up with advances in text generation,\nespecially with Large Language Models (LLMs). In particular, recent studies\nhave shown that current ATS metrics do not correlate with the presence of\nerrors. Manual inspections have further revealed a variety of errors,\nunderscoring the need for a more nuanced evaluation framework, which is\ncurrently lacking. This resource paper addresses this gap by introducing a test\ncollection for detecting and classifying errors in simplified texts. First, we\npropose a taxonomy of errors, with a formal focus on information distortion.\nNext, we introduce a parallel dataset of automatically simplified scientific\ntexts. This dataset has been human-annotated with labels based on our proposed\ntaxonomy. Finally, we analyze the quality of the dataset, and we study the\nperformance of existing models to detect and classify errors from that\ntaxonomy. These contributions give researchers the tools to better evaluate\nerrors in ATS, develop more reliable models, and ultimately improve the quality\nof automatically simplified texts."}
{"id": "2505.16493", "pdf": "https://arxiv.org/pdf/2505.16493", "abs": "https://arxiv.org/abs/2505.16493", "authors": ["Seyedeh Fatemeh Ebrahimi", "Jaakko Peltonen"], "title": "Constrained Non-negative Matrix Factorization for Guided Topic Modeling of Minority Topics", "categories": ["cs.LG"], "comment": null, "summary": "Topic models often fail to capture low-prevalence, domain-critical themes,\nso-called minority topics, such as mental health themes in online comments.\nWhile some existing methods can incorporate domain knowledge, such as expected\ntopical content, methods allowing guidance may require overly detailed expected\ntopics, hindering the discovery of topic divisions and variation. We propose a\ntopic modeling solution via a specially constrained NMF. We incorporate a seed\nword list characterizing minority content of interest, but we do not require\nexperts to pre-specify their division across minority topics. Through\nprevalence constraints on minority topics and seed word content across topics,\nwe learn distinct data-driven minority topics as well as majority topics. The\nconstrained NMF is fitted via Karush-Kuhn-Tucker (KKT) conditions with\nmultiplicative updates. We outperform several baselines on synthetic data in\nterms of topic purity, normalized mutual information, and also evaluate topic\nquality using Jensen-Shannon divergence (JSD). We conduct a case study on\nYouTube vlog comments, analyzing viewer discussion of mental health content;\nour model successfully identifies and reveals this domain-relevant minority\ncontent."}
{"id": "2505.14679", "pdf": "https://arxiv.org/pdf/2505.14679", "abs": "https://arxiv.org/abs/2505.14679", "authors": ["Xiaojie Gu", "Guangxu Chen", "Jungang Li", "Jia-Chen Gu", "Xuming Hu", "Kai Zhang"], "title": "UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Lifelong learning enables large language models (LLMs) to adapt to evolving\ninformation by continually updating their internal knowledge. An ideal system\nshould support efficient, wide-ranging updates while preserving existing\ncapabilities and ensuring reliable deployment. Model editing stands out as a\npromising solution for this goal, offering a focused and efficient way to\nrevise a model's internal knowledge. Although recent paradigms have made\nnotable progress, they often struggle to meet the demands of practical lifelong\nadaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally\nnew editing solution that is training-, subject- and memory-free, making it\nparticularly well-suited for ultra-scalable, real-world lifelong model editing.\nULTRAEDIT performs editing through a self-contained process that relies solely\non lightweight linear algebra operations to compute parameter shifts, enabling\nfast and consistent parameter modifications with minimal overhead. To improve\nscalability in lifelong settings, ULTRAEDIT employs a lifelong normalization\nstrategy that continuously updates feature statistics across turns, allowing it\nto adapt to distributional shifts and maintain consistency over time. ULTRAEDIT\nachieves editing speeds over 7x faster than the previous state-of-the-art\nmethod-which was also the fastest known approach-while consuming less than 1/3\nthe VRAM, making it the only method currently capable of editing a 7B LLM on a\n24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest\ndataset in the field to date, with over 2M editing pairs-and demonstrate that\nour method supports up to 1M edits while maintaining high accuracy.\nComprehensive experiments on four datasets and six models show that ULTRAEDIT\nconsistently achieves superior performance across diverse model editing\nscenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit."}
{"id": "2505.16406", "pdf": "https://arxiv.org/pdf/2505.16406", "abs": "https://arxiv.org/abs/2505.16406", "authors": ["Gaofei Shen", "Hosein Mohebbi", "Arianna Bisazza", "Afra Alishahi", "Grzegorz Chrupała"], "title": "On the reliability of feature attribution methods for speech classification", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "As the capabilities of large-scale pre-trained models evolve, understanding\nthe determinants of their outputs becomes more important. Feature attribution\naims to reveal which parts of the input elements contribute the most to model\noutputs. In speech processing, the unique characteristics of the input signal\nmake the application of feature attribution methods challenging. We study how\nfactors such as input type and aggregation and perturbation timespan impact the\nreliability of standard feature attribution methods, and how these factors\ninteract with characteristics of each classification task. We find that\nstandard approaches to feature attribution are generally unreliable when\napplied to the speech domain, with the exception of word-aligned perturbation\nmethods when applied to word-based classification tasks."}
{"id": "2505.16494", "pdf": "https://arxiv.org/pdf/2505.16494", "abs": "https://arxiv.org/abs/2505.16494", "authors": ["Noga Amit", "Omer Reingold", "Guy N. Rothblum"], "title": "Accuracy vs. Accuracy: Computational Tradeoffs Between Classification Rates and Utility", "categories": ["cs.LG"], "comment": null, "summary": "We revisit the foundations of fairness and its interplay with utility and\nefficiency in settings where the training data contain richer labels, such as\nindividual types, rankings, or risk estimates, rather than just binary\noutcomes. In this context, we propose algorithms that achieve stronger notions\nof evidence-based fairness than are possible in standard supervised learning.\nOur methods support classification and ranking techniques that preserve\naccurate subpopulation classification rates, as suggested by the underlying\ndata distributions, across a broad class of classification rules and downstream\napplications. Furthermore, our predictors enable loss minimization, whether\naimed at maximizing utility or in the service of fair treatment.\n  Complementing our algorithmic contributions, we present impossibility results\ndemonstrating that simultaneously achieving accurate classification rates and\noptimal loss minimization is, in some cases, computationally infeasible. Unlike\nprior impossibility results, our notions are not inherently in conflict and are\nsimultaneously satisfied by the Bayes-optimal predictor. Furthermore, we show\nthat each notion can be satisfied individually via efficient learning. Our\nseparation thus stems from the computational hardness of learning a\nsufficiently good approximation of the Bayes-optimal predictor. These\ncomputational impossibilities present a choice between two natural and\nattainable notions of accuracy that could both be motivated by fairness."}
{"id": "2505.15820", "pdf": "https://arxiv.org/pdf/2505.15820", "abs": "https://arxiv.org/abs/2505.15820", "authors": ["Gabriel Anzer", "Kilian Arnsmeyer", "Pascal Bauer", "Joris Bekkers", "Ulf Brefeld", "Jesse Davis", "Nicolas Evans", "Matthias Kempe", "Samuel J Robertson", "Joshua Wyatt Smith", "Jan Van Haaren"], "title": "Common Data Format (CDF): A Standardized Format for Match-Data in Football (Soccer)", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "During football matches, a variety of different parties (e.g., companies)\neach collect (possibly overlapping) data about the match ranging from basic\ninformation (e.g., starting players) to detailed positional data. This data is\nprovided to clubs, federations, and other organizations who are increasingly\ninterested in leveraging this data to inform their decision making.\nUnfortunately, analyzing such data pose significant barriers because each\nprovider may (1) collect different data, (2) use different specifications even\nwithin the same category of data, (3) represent the data differently, and (4)\ndelivers the data in a different manner (e.g., file format, protocol).\nConsequently, working with these data requires a significant investment of time\nand money. The goal of this work is to propose a uniform and standardized\nformat for football data called the Common Data Format (CDF). The CDF specifies\na minimal schema for five types of match data: match sheet data, video footage,\nevent data, tracking data, and match meta data. It aims to ensure that the\nprovided data is clear, sufficiently contextualized (e.g., its provenance is\nclear), and complete such that it enables common downstream analysis tasks.\nConcretely, this paper will detail the technical specifications of the CDF, the\nrepresentational choices that were made to help ensure the clarity of the\nprovided data, and a concrete approach for delivering data in the CDF."}
{"id": "2505.16408", "pdf": "https://arxiv.org/pdf/2505.16408", "abs": "https://arxiv.org/abs/2505.16408", "authors": ["Muhammad Farid Adilazuarda", "Chen Cecilia Liu", "Iryna Gurevych", "Alham Fikri Aji"], "title": "From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Adapting cultural values in Large Language Models (LLMs) presents significant\nchallenges, particularly due to biases and limited training data. Prior work\nprimarily aligns LLMs with different cultural values using World Values Survey\n(WVS) data. However, it remains unclear whether this approach effectively\ncaptures cultural nuances or produces distinct cultural representations for\nvarious downstream tasks. In this paper, we systematically investigate\nWVS-based training for cultural value adaptation and find that relying solely\non survey data can homogenize cultural norms and interfere with factual\nknowledge. To investigate these issues, we augment WVS with encyclopedic and\nscenario-based cultural narratives from Wikipedia and NormAd. While these\nnarratives may have variable effects on downstream tasks, they consistently\nimprove cultural distinctiveness than survey data alone. Our work highlights\nthe inherent complexity of aligning cultural values with the goal of guiding\ntask-specific behavior."}
{"id": "2505.16516", "pdf": "https://arxiv.org/pdf/2505.16516", "abs": "https://arxiv.org/abs/2505.16516", "authors": ["Majid Mohammadi", "Siu Lun Chau", "Krikamol Muandet"], "title": "Computing Exact Shapley Values in Polynomial Time for Product-Kernel Methods", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Kernel methods are widely used in machine learning due to their flexibility\nand expressive power. However, their black-box nature poses significant\nchallenges to interpretability, limiting their adoption in high-stakes\napplications. Shapley value-based feature attribution techniques, such as SHAP\nand kernel-specific variants like RKHS-SHAP, offer a promising path toward\nexplainability. Yet, computing exact Shapley values remains computationally\nintractable in general, motivating the development of various approximation\nschemes. In this work, we introduce PKeX-Shapley, a novel algorithm that\nutilizes the multiplicative structure of product kernels to enable the exact\ncomputation of Shapley values in polynomial time. We show that product-kernel\nmodels admit a functional decomposition that allows for a recursive formulation\nof Shapley values. This decomposition not only yields computational efficiency\nbut also enhances interpretability in kernel-based learning. We also\ndemonstrate how our framework can be generalized to explain kernel-based\nstatistical discrepancies such as the Maximum Mean Discrepancy (MMD) and the\nHilbert-Schmidt Independence Criterion (HSIC), thus offering new tools for\ninterpretable statistical inference."}
{"id": "2505.15821", "pdf": "https://arxiv.org/pdf/2505.15821", "abs": "https://arxiv.org/abs/2505.15821", "authors": ["Milos Gravara", "Andrija Stanisic", "Stefan Nastic"], "title": "A Novel Compound AI Model for 6G Networks in 3D Continuum", "categories": ["cs.NI", "cs.AI", "I.2.11; C.2.3; C.2.4"], "comment": "4 pages, 2 figures", "summary": "The 3D continuum presents a complex environment that spans the terrestrial,\naerial and space domains, with 6Gnetworks serving as a key enabling technology.\nCurrent AI approaches for network management rely on monolithic models that\nfail to capture cross-domain interactions, lack adaptability,and demand\nprohibitive computational resources. This paper presents a formal model of\nCompound AI systems, introducing a novel tripartite framework that decomposes\ncomplex tasks into specialized, interoperable modules. The proposed modular\narchitecture provides essential capabilities to address the unique challenges\nof 6G networks in the 3D continuum, where heterogeneous components require\ncoordinated, yet distributed, intelligence. This approach introduces a\nfundamental trade-off between model and system performance, which must be\ncarefully addressed. Furthermore, we identify key challenges faced by Compound\nAI systems within 6G networks operating in the 3D continuum, including\ncross-domain resource orchestration, adaptation to dynamic topologies, and the\nmaintenance of consistent AI service quality across heterogeneous environments."}
{"id": "2505.16410", "pdf": "https://arxiv.org/pdf/2505.16410", "abs": "https://arxiv.org/abs/2505.16410", "authors": ["Guanting Dong", "Yifei Chen", "Xiaoxi Li", "Jiajie Jin", "Hongjin Qian", "Yutao Zhu", "Hangyu Mao", "Guorui Zhou", "Zhicheng Dou", "Ji-Rong Wen"], "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Working in progress", "summary": "Recently, large language models (LLMs) have shown remarkable reasoning\ncapabilities via large-scale reinforcement learning (RL). However, leveraging\nthe RL algorithm to empower effective multi-tool collaborative reasoning in\nLLMs remains an open challenge. In this paper, we introduce Tool-Star, an\nRL-based framework designed to empower LLMs to autonomously invoke multiple\nexternal tools during stepwise reasoning. Tool-Star integrates six types of\ntools and incorporates systematic designs in both data synthesis and training.\nTo address the scarcity of tool-use data, we propose a general tool-integrated\nreasoning data synthesis pipeline, which combines tool-integrated prompting\nwith hint-based sampling to automatically and scalably generate tool-use\ntrajectories. A subsequent quality normalization and difficulty-aware\nclassification process filters out low-quality samples and organizes the\ndataset from easy to hard. Furthermore, we propose a two-stage training\nframework to enhance multi-tool collaborative reasoning by: (1) cold-start\nfine-tuning, which guides LLMs to explore reasoning patterns via\ntool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with\nhierarchical reward design, which reinforces reward understanding and promotes\neffective tool collaboration. Experimental analyses on over 10 challenging\nreasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.\nThe code is available at https://github.com/dongguanting/Tool-Star."}
{"id": "2505.16527", "pdf": "https://arxiv.org/pdf/2505.16527", "abs": "https://arxiv.org/abs/2505.16527", "authors": ["Mohamed Amine Ketata", "David Lüdke", "Leo Schwinn", "Stephan Günnemann"], "title": "Joint Relational Database Generation via Graph-Conditional Diffusion Models", "categories": ["cs.LG"], "comment": null, "summary": "Building generative models for relational databases (RDBs) is important for\napplications like privacy-preserving data release and augmenting real datasets.\nHowever, most prior work either focuses on single-table generation or relies on\nautoregressive factorizations that impose a fixed table order and generate\ntables sequentially. This approach limits parallelism, restricts flexibility in\ndownstream applications like missing value imputation, and compounds errors due\nto commonly made conditional independence assumptions. We propose a\nfundamentally different approach: jointly modeling all tables in an RDB without\nimposing any order. By using a natural graph representation of RDBs, we propose\nthe Graph-Conditional Relational Diffusion Model (GRDM). GRDM leverages a graph\nneural network to jointly denoise row attributes and capture complex\ninter-table dependencies. Extensive experiments on six real-world RDBs\ndemonstrate that our approach substantially outperforms autoregressive\nbaselines in modeling multi-hop inter-table correlations and achieves\nstate-of-the-art performance on single-table fidelity metrics."}
{"id": "2505.15825", "pdf": "https://arxiv.org/pdf/2505.15825", "abs": "https://arxiv.org/abs/2505.15825", "authors": ["Ammar Chouchane", "Mohcene Bessaoudi", "Hamza Kheddar", "Abdelmalik Ouamane", "Tiago Vieira", "Mahmoud Hassaballah"], "title": "Multilinear subspace learning for person re-identification based fusion of high order tensor features", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Video surveillance image analysis and processing is a challenging field in\ncomputer vision, with one of its most difficult tasks being Person\nRe-Identification (PRe-ID). PRe-ID aims to identify and track target\nindividuals who have already been detected in a network of cameras, using a\nrobust description of their pedestrian images. The success of recent research\nin person PRe-ID is largely due to effective feature extraction and\nrepresentation, as well as the powerful learning of these features to reliably\ndiscriminate between pedestrian images. To this end, two powerful features,\nConvolutional Neural Networks (CNN) and Local Maximal Occurrence (LOMO), are\nmodeled on multidimensional data using the proposed method, High-Dimensional\nFeature Fusion (HDFF). Specifically, a new tensor fusion scheme is introduced\nto leverage and combine these two types of features in a single tensor, even\nthough their dimensions are not identical. To enhance the system's accuracy, we\nemploy Tensor Cross-View Quadratic Analysis (TXQDA) for multilinear subspace\nlearning, followed by cosine similarity for matching. TXQDA efficiently\nfacilitates learning while reducing the high dimensionality inherent in\nhigh-order tensor data. The effectiveness of our approach is verified through\nexperiments on three widely-used PRe-ID datasets: VIPeR, GRID, and PRID450S.\nExtensive experiments demonstrate that our approach outperforms recent\nstate-of-the-art methods."}
{"id": "2505.16415", "pdf": "https://arxiv.org/pdf/2505.16415", "abs": "https://arxiv.org/abs/2505.16415", "authors": ["Ruizhe Li", "Chen Chen", "Yuchen Hu", "Yanjun Gao", "Xi Wang", "Emine Yilmaz"], "title": "Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in process", "summary": "Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)\ncombined with external contexts to enhance the accuracy and reliability of\ngenerated responses. However, reliably attributing generated content to\nspecific context segments, context attribution, remains challenging due to the\ncomputationally intensive nature of current methods, which often require\nextensive fine-tuning or human annotation. In this work, we introduce a novel\nJensen-Shannon Divergence driven method to Attribute Response to Context\n(ARC-JSD), enabling efficient and accurate identification of essential context\nsentences without additional fine-tuning or surrogate modelling. Evaluations on\na wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using\ninstruction-tuned LLMs in different scales demonstrate superior accuracy and\nsignificant computational efficiency improvements compared to the previous\nsurrogate-based method. Furthermore, our mechanistic analysis reveals specific\nattention heads and multilayer perceptron (MLP) layers responsible for context\nattribution, providing valuable insights into the internal workings of RAG\nmodels."}
{"id": "2505.16531", "pdf": "https://arxiv.org/pdf/2505.16531", "abs": "https://arxiv.org/abs/2505.16531", "authors": ["Alejandro Moreno Arcas", "Albert Sanchis", "Jorge Civera", "Alfons Juan"], "title": "HOFT: Householder Orthogonal Fine-tuning", "categories": ["cs.LG"], "comment": null, "summary": "Adaptation of foundation models using low-rank methods is a widespread\napproach. Another way to adapt these models is to employ orthogonal fine-tuning\nmethods, which are less time and memory efficient despite their good\ngeneralization properties. In this work, we propose Householder Orthogonal\nFine-tuning (HOFT), a novel orthogonal fine-tuning method that aims to\nalleviate time and space complexity. Moreover, some theoretical properties of\nthe orthogonal fine-tuning paradigm are explored. From this exploration, Scaled\nHouseholder Orthogonal Fine-tuning (SHOFT) is proposed. Both HOFT and SHOFT are\nevaluated in downstream tasks, namely commonsense reasoning, machine\ntranslation, subject-driven generation and mathematical reasoning. Compared\nwith state-of-the-art adaptation methods, HOFT and SHOFT show comparable or\nbetter results."}
{"id": "2505.15828", "pdf": "https://arxiv.org/pdf/2505.15828", "abs": "https://arxiv.org/abs/2505.15828", "authors": ["Jiayuan Chen", "Yuxiang Li", "Changyan Yi", "Shimin Gong"], "title": "Generative AI-Aided QoE Maximization for RIS-Assisted Digital Twin Interaction", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "In this paper, we investigate a quality of experience (QoE)-aware resource\nallocation problem for reconfigurable intelligent surface (RIS)-assisted\ndigital twin (DT) interaction with uncertain evolution. In the considered\nsystem, mobile users are expected to interact with a DT model maintained on a\nDT server that is deployed on a base station, via effective uplink and downlink\nchannels assisted by an RIS. Our goal is to maximize the sum of all mobile\nusers' joint subjective and objective QoE in DT interactions across various DT\nscenes, by jointly optimizing phase shift matrix, receive/transmit beamforming\nmatrix, rendering resolution configuration and computing resource allocation.\nWhile solving this problem is challenging mainly due to the uncertain evolution\nof the DT model, which leads to multiple scene-specific problems, and require\nus to constantly re-solve each of them whenever DT model evolves.\n  To this end, leveraging the dynamic optimization capabilities of decision\ntransformers and the generalization strengths of generative artificial\nintelligence (GAI), we propose a novel GAI-aided approach, called the\nprompt-guided decision transformer integrated with zero-forcing optimization\n(PG-ZFO). Simulations are conducted to evaluate the proposed PG-ZFO,\ndemonstrating its effectiveness and superiority over counterparts."}
{"id": "2505.16418", "pdf": "https://arxiv.org/pdf/2505.16418", "abs": "https://arxiv.org/abs/2505.16418", "authors": ["Yoichi Aoki", "Soichiro Murakami", "Ukyo Honda", "Akihiko Kato"], "title": "Exploring the Relationship Between Diversity and Quality in Ad Text Generation", "categories": ["cs.CL"], "comment": null, "summary": "In natural language generation for advertising, creating diverse and engaging\nad texts is crucial for capturing a broad audience and avoiding advertising\nfatigue. Regardless of the importance of diversity, the impact of the\ndiversity-enhancing methods in ad text generation -- mainly tested on tasks\nsuch as summarization and machine translation -- has not been thoroughly\nexplored. Ad text generation significantly differs from these tasks owing to\nthe text style and requirements. This research explores the relationship\nbetween diversity and ad quality in ad text generation by considering multiple\nfactors, such as diversity-enhancing methods, their hyperparameters,\ninput-output formats, and the models."}
{"id": "2505.16548", "pdf": "https://arxiv.org/pdf/2505.16548", "abs": "https://arxiv.org/abs/2505.16548", "authors": ["Lucas Maystre", "Gabriel Barello", "Tudor Berariu", "Aleix Cambray", "Rares Dolga", "Alvaro Ortega Gonzalez", "Andrei Nica", "David Barber"], "title": "Incremental Sequence Classification with Temporal Consistency", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We address the problem of incremental sequence classification, where\npredictions are updated as new elements in the sequence are revealed. Drawing\non temporal-difference learning from reinforcement learning, we identify a\ntemporal-consistency condition that successive predictions should satisfy. We\nleverage this condition to develop a novel loss function for training\nincremental sequence classifiers. Through a concrete example, we demonstrate\nthat optimizing this loss can offer substantial gains in data efficiency. We\napply our method to text classification tasks and show that it improves\npredictive accuracy over competing approaches on several benchmark datasets. We\nfurther evaluate our approach on the task of verifying large language model\ngenerations for correctness in grade-school math problems. Our results show\nthat models trained with our method are better able to distinguish promising\ngenerations from unpromising ones after observing only a few tokens."}
{"id": "2505.15832", "pdf": "https://arxiv.org/pdf/2505.15832", "abs": "https://arxiv.org/abs/2505.15832", "authors": ["Quan Minh Phan", "Ngoc Hoang Luong"], "title": "From Hand-Crafted Metrics to Evolved Training-Free Performance Predictors for Neural Architecture Search via Genetic Programming", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Estimating the network performance using zero-cost (ZC) metrics has proven\nboth its efficiency and efficacy in Neural Architecture Search (NAS). However,\na notable limitation of most ZC proxies is their inconsistency, as reflected by\nthe substantial variation in their performance across different problems.\nFurthermore, the design of existing ZC metrics is manual, involving a\ntime-consuming trial-and-error process that requires substantial domain\nexpertise. These challenges raise two critical questions: (1) Can we automate\nthe design of ZC metrics? and (2) Can we utilize the existing hand-crafted ZC\nmetrics to synthesize a more generalizable one? In this study, we propose a\nframework based on Symbolic Regression via Genetic Programming to automate the\ndesign of ZC metrics. Our framework is not only highly extensible but also\ncapable of quickly producing a ZC metric with a strong positive rank\ncorrelation to true network performance across diverse NAS search spaces and\ntasks. Extensive experiments on 13 problems from NAS-Bench-Suite-Zero\ndemonstrate that our automatically generated proxies consistently outperform\nhand-crafted alternatives. Using our evolved proxy metric as the search\nobjective in an evolutionary algorithm, we could identify network architectures\nwith competitive performance within 15 minutes using a single consumer GPU."}
{"id": "2505.16421", "pdf": "https://arxiv.org/pdf/2505.16421", "abs": "https://arxiv.org/abs/2505.16421", "authors": ["Zhepei Wei", "Wenlin Yao", "Yao Liu", "Weizhi Zhang", "Qin Lu", "Liang Qiu", "Changlong Yu", "Puyang Xu", "Chao Zhang", "Bing Yin", "Hyokun Yun", "Lihong Li"], "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "While reinforcement learning (RL) has demonstrated remarkable success in\nenhancing large language models (LLMs), it has primarily focused on single-turn\ntasks such as solving math problems. Training effective web agents for\nmulti-turn interactions remains challenging due to the complexity of\nlong-horizon decision-making across dynamic web interfaces. In this work, we\npresent WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework\nfor training web agents. It learns directly from online interactions with web\nenvironments by asynchronously generating diverse trajectories, entirely guided\nby binary rewards depending on task success. Experiments on the WebArena-Lite\nbenchmark demonstrate the effectiveness of WebAgent-R1, boosting the task\nsuccess rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to\n44.8%, significantly outperforming existing state-of-the-art methods and strong\nproprietary models such as OpenAI o3. In-depth analyses reveal the\neffectiveness of the thinking-based prompting strategy and test-time scaling\nthrough increased interactions for web tasks. We further investigate different\nRL initialization policies by introducing two variants, namely WebAgent-R1-Zero\nand WebAgent-R1-CoT, which highlight the importance of the warm-up training\nstage (i.e., behavior cloning) and provide insights on incorporating long\nchain-of-thought (CoT) reasoning in web agents."}
{"id": "2505.16549", "pdf": "https://arxiv.org/pdf/2505.16549", "abs": "https://arxiv.org/abs/2505.16549", "authors": ["Trung V. Phan", "George A. Kevrekidis", "Soledad Villar", "Yannis G. Kevrekidis", "Juan M. Bello-Rivas"], "title": "Towards Coordinate- and Dimension-Agnostic Machine Learning for Partial Differential Equations", "categories": ["cs.LG", "35Q92, 68T07", "I.2.6; G.1.8"], "comment": null, "summary": "The machine learning methods for data-driven identification of partial\ndifferential equations (PDEs) are typically defined for a given number of\nspatial dimensions and a choice of coordinates the data have been collected in.\nThis dependence prevents the learned evolution equation from generalizing to\nother spaces. In this work, we reformulate the problem in terms of coordinate-\nand dimension-independent representations, paving the way toward what we call\n``spatially liberated\" PDE learning. To this end, we employ a machine learning\napproach to predict the evolution of scalar field systems expressed in the\nformalism of exterior calculus, which is coordinate-free and immediately\ngeneralizes to arbitrary dimensions by construction. We demonstrate the\nperformance of this approach in the FitzHugh-Nagumo and Barkley\nreaction-diffusion models, as well as the Patlak-Keller-Segel model informed by\nin-situ chemotactic bacteria observations. We provide extensive numerical\nexperiments that demonstrate that our approach allows for seamless transitions\nacross various spatial contexts. We show that the field dynamics learned in one\nspace can be used to make accurate predictions in other spaces with different\ndimensions, coordinate systems, boundary conditions, and curvatures."}
{"id": "2505.15834", "pdf": "https://arxiv.org/pdf/2505.15834", "abs": "https://arxiv.org/abs/2505.15834", "authors": ["Congyuan Zhao", "Lingwei Wei", "Ziming Qin", "Wei Zhou", "Yunya Song", "Songlin Hu"], "title": "MPPFND: A Dataset and Analysis of Detecting Fake News with Multi-Platform Propagation", "categories": ["cs.SI", "cs.AI"], "comment": "Cogsci 2025", "summary": "Fake news spreads widely on social media, leading to numerous negative\neffects. Most existing detection algorithms focus on analyzing news content and\nsocial context to detect fake news. However, these approaches typically detect\nfake news based on specific platforms, ignoring differences in propagation\ncharacteristics across platforms. In this paper, we introduce the MPPFND\ndataset, which captures propagation structures across multiple platforms. We\nalso describe the commenting and propagation characteristics of different\nplatforms to show that their social contexts have distinct features. We propose\na multi-platform fake news detection model (APSL) that uses graph neural\nnetworks to extract social context features from various platforms. Experiments\nshow that accounting for cross-platform propagation differences improves fake\nnews detection performance."}
{"id": "2505.16425", "pdf": "https://arxiv.org/pdf/2505.16425", "abs": "https://arxiv.org/abs/2505.16425", "authors": ["Jing Bi", "Pinxin Liu", "Ali Vosoughi", "Jiarui Wu", "Jinxi He", "Chenliang Xu"], "title": "$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 5 figures, under review", "summary": "The effective communication of procedural knowledge remains a significant\nchallenge in natural language processing (NLP), as purely textual instructions\noften fail to convey complex physical actions and spatial relationships. We\naddress this limitation by proposing a language-driven framework that\ntranslates procedural text into coherent visual instructions. Our approach\nmodels the linguistic structure of instructional content by decomposing it into\ngoal statements and sequential steps, then conditioning visual generation on\nthese linguistic elements. We introduce three key innovations: (1) a\nconstituency parser-based text encoding mechanism that preserves semantic\ncompleteness even with lengthy instructions, (2) a pairwise discourse coherence\nmodel that maintains consistency across instruction sequences, and (3) a novel\nevaluation protocol specifically designed for procedural language-to-image\nalignment. Our experiments across three instructional datasets (HTStep,\nCaptainCook4D, and WikiAll) demonstrate that our method significantly\noutperforms existing baselines in generating visuals that accurately reflect\nthe linguistic content and sequential nature of instructions. This work\ncontributes to the growing body of research on grounding procedural language in\nvisual content, with applications spanning education, task guidance, and\nmultimodal language understanding."}
{"id": "2505.16563", "pdf": "https://arxiv.org/pdf/2505.16563", "abs": "https://arxiv.org/abs/2505.16563", "authors": ["Chen Gong", "Rui Xing", "Zhenzhe Zheng", "Fan Wu"], "title": "A Two-Stage Data Selection Framework for Data-Efficient Model Training on Edge Devices", "categories": ["cs.LG"], "comment": null, "summary": "The demand for machine learning (ML) model training on edge devices is\nescalating due to data privacy and personalized service needs. However, we\nobserve that current on-device model training is hampered by the\nunder-utilization of on-device data, due to low training throughput, limited\nstorage and diverse data importance. To improve data resource utilization, we\npropose a two-stage data selection framework {\\sf Titan} to select the most\nimportant data batch from streaming data for model training with guaranteed\nefficiency and effectiveness. Specifically, in the first stage, {\\sf Titan}\nfilters out a candidate dataset with potentially high importance in a\ncoarse-grained manner.In the second stage of fine-grained selection, we propose\na theoretically optimal data selection strategy to identify the data batch with\nthe highest model performance improvement to current training round. To further\nenhance time-and-resource efficiency, {\\sf Titan} leverages a pipeline to\nco-execute data selection and model training, and avoids resource conflicts by\nexploiting idle computing resources. We evaluate {\\sf Titan} on real-world edge\ndevices and three representative edge computing tasks with diverse models and\ndata modalities. Empirical results demonstrate that {\\sf Titan} achieves up to\n$43\\%$ reduction in training time and $6.2\\%$ increase in final accuracy with\nminor system overhead, such as data processing delay, memory footprint and\nenergy consumption."}
{"id": "2505.15835", "pdf": "https://arxiv.org/pdf/2505.15835", "abs": "https://arxiv.org/abs/2505.15835", "authors": ["Nayan Sanjay Bhatia", "Katia Obraczka"], "title": "Transforming Decoder-Only Transformers for Accurate WiFi-Telemetry Based Indoor Localization", "categories": ["cs.NI", "cs.AI"], "comment": "11 pages, 2 figures, In Submission", "summary": "Wireless Fidelity (WiFi) based indoor positioning is a widely researched area\nfor determining the position of devices within a wireless network. Accurate\nindoor location has numerous applications, such as asset tracking and indoor\nnavigation. Despite advances in WiFi localization techniques -- in particular\napproaches that leverage WiFi telemetry -- their adoption in practice remains\nlimited due to several factors including environmental changes that cause\nsignal fading, multipath effects, interference, which, in turn, impact\npositioning accuracy. In addition, telemetry data differs depending on the WiFi\ndevice vendor, offering distinct features and formats; use case requirements\ncan also vary widely. Currently, there is no unified model to handle all these\nvariations effectively. In this paper, we present WiFiGPT, a Generative\nPretrained Transformer (GPT) based system that is able to handle these\nvariations while achieving high localization accuracy. Our experiments with\nWiFiGPT demonstrate that GPTs, in particular Large Language Models (LLMs), can\neffectively capture subtle spatial patterns in noisy wireless telemetry, making\nthem reliable regressors. Compared to existing state-of-the-art methods, our\nmethod matches and often surpasses conventional approaches for multiple types\nof telemetry. Achieving sub-meter accuracy for RSSI and FTM and\ncentimeter-level precision for CSI demonstrates the potential of LLM-based\nlocalisation to outperform specialized techniques, all without handcrafted\nsignal processing or calibration."}
{"id": "2505.16429", "pdf": "https://arxiv.org/pdf/2505.16429", "abs": "https://arxiv.org/abs/2505.16429", "authors": ["Song Jin", "Juntian Zhang", "Yuhan Liu", "Xun Zhang", "Yufei Zhang", "Guojun Yin", "Fei Jiang", "Wei Lin", "Rui Yan"], "title": "Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating and iterating upon recommender systems is crucial, yet traditional\nA/B testing is resource-intensive, and offline methods struggle with dynamic\nuser-platform interactions. While agent-based simulation is promising, existing\nplatforms often lack a mechanism for user actions to dynamically reshape the\nenvironment. To bridge this gap, we introduce RecInter, a novel agent-based\nsimulation platform for recommender systems featuring a robust interaction\nmechanism. In RecInter platform, simulated user actions (e.g., likes, reviews,\npurchases) dynamically update item attributes in real-time, and introduced\nMerchant Agents can reply, fostering a more realistic and evolving ecosystem.\nHigh-fidelity simulation is ensured through Multidimensional User Profiling\nmodule, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought\n(CoT) enriched interaction data. Our platform achieves significantly improved\nsimulation credibility and successfully replicates emergent phenomena like\nBrand Loyalty and the Matthew Effect. Experiments demonstrate that this\ninteraction mechanism is pivotal for simulating realistic system evolution,\nestablishing our platform as a credible testbed for recommender systems\nresearch."}
{"id": "2505.16567", "pdf": "https://arxiv.org/pdf/2505.16567", "abs": "https://arxiv.org/abs/2505.16567", "authors": ["Thibaud Gloaguen", "Mark Vero", "Robin Staab", "Martin Vechev"], "title": "Finetuning-Activated Backdoors in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Finetuning openly accessible Large Language Models (LLMs) has become standard\npractice for achieving task-specific performance improvements. Until now,\nfinetuning has been regarded as a controlled and secure process in which\ntraining on benign datasets led to predictable behaviors. In this paper, we\ndemonstrate for the first time that an adversary can create poisoned LLMs that\ninitially appear benign but exhibit malicious behaviors once finetuned by\ndownstream users. To this end, our proposed attack, FAB (Finetuning-Activated\nBackdoor), poisons an LLM via meta-learning techniques to simulate downstream\nfinetuning, explicitly optimizing for the emergence of malicious behaviors in\nthe finetuned models. At the same time, the poisoned LLM is regularized to\nretain general capabilities and to exhibit no malicious behaviors prior to\nfinetuning. As a result, when users finetune the seemingly benign model on\ntheir own datasets, they unknowingly trigger its hidden backdoor behavior. We\ndemonstrate the effectiveness of FAB across multiple LLMs and three target\nbehaviors: unsolicited advertising, refusal, and jailbreakability.\nAdditionally, we show that FAB-backdoors are robust to various finetuning\nchoices made by the user (e.g., dataset, number of steps, scheduler). Our\nfindings challenge prevailing assumptions about the security of finetuning,\nrevealing yet another critical attack vector exploiting the complexities of\nLLMs."}
{"id": "2505.15836", "pdf": "https://arxiv.org/pdf/2505.15836", "abs": "https://arxiv.org/abs/2505.15836", "authors": ["Aarav Lala", "Kalyan Cherukuri"], "title": "Quantum-Evolutionary Neural Networks for Multi-Agent Federated Learning", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": null, "summary": "As artificial intelligence continues to drive innovation in complex,\ndecentralized environments, the need for scalable, adaptive, and\nprivacy-preserving decision-making systems has become critical. This paper\nintroduces a novel framework combining quantum-inspired neural networks with\nevolutionary algorithms to optimize real-time decision-making in multi-agent\nsystems (MAS). The proposed Quantum-Evolutionary Neural Network (QE-NN)\nleverages quantum computing principles -- such as quantum superposition and\nentanglement -- to enhance learning speed and decision accuracy, while\nintegrating evolutionary optimization to continually refine agent behaviors in\ndynamic, uncertain environments. By utilizing federated learning, QE-NN ensures\nprivacy preservation, enabling decentralized agents to collaborate without\nsharing sensitive data. The framework is designed to allow agents to adapt in\nreal-time to their environments, optimizing decision-making processes for\napplications in areas such as autonomous systems, smart cities, and healthcare.\nThis research represents a breakthrough in merging quantum computing,\nevolutionary optimization, and privacy-preserving techniques to solve complex\nproblems in multi-agent decision-making systems, pushing the boundaries of AI\nin real-world, privacy-sensitive applications."}
{"id": "2505.16460", "pdf": "https://arxiv.org/pdf/2505.16460", "abs": "https://arxiv.org/abs/2505.16460", "authors": ["Ikhlasul Akmal Hanif", "Eryawan Presma Yulianrifat", "Jaycent Gunawan Ongris", "Eduardus Tjitrahardja", "Muhammad Falensi Azmi", "Rahmat Bryan Naufal", "Alfan Farizki Wicaksono"], "title": "University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "16 pages, 13 tables, 1 figures", "summary": "This paper presents our approach for SemEval 2025 Task 11 Track A, focusing\non multilabel emotion classification across 28 languages. We explore two main\nstrategies: fully fine-tuning transformer models and classifier-only training,\nevaluating different settings such as fine-tuning strategies, model\narchitectures, loss functions, encoders, and classifiers. Our findings suggest\nthat training a classifier on top of prompt-based encoders such as mE5 and BGE\nyields significantly better results than fully fine-tuning XLMR and mBERT. Our\nbest-performing model on the final leaderboard is an ensemble combining\nmultiple BGE models, where CatBoost serves as the classifier, with different\nconfigurations. This ensemble achieves an average F1-macro score of 56.58\nacross all languages."}
{"id": "2505.16577", "pdf": "https://arxiv.org/pdf/2505.16577", "abs": "https://arxiv.org/abs/2505.16577", "authors": ["Yu Zuo", "Dalin Qin", "Yi Wang"], "title": "Large Language Model-Empowered Interactive Load Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "The growing complexity of power systems has made accurate load forecasting\nmore important than ever. An increasing number of advanced load forecasting\nmethods have been developed. However, the static design of current methods\noffers no mechanism for human-model interaction. As the primary users of\nforecasting models, system operators often find it difficult to understand and\napply these advanced models, which typically requires expertise in artificial\nintelligence (AI). This also prevents them from incorporating their experience\nand real-world contextual understanding into the forecasting process. Recent\nbreakthroughs in large language models (LLMs) offer a new opportunity to\naddress this issue. By leveraging their natural language understanding and\nreasoning capabilities, we propose an LLM-based multi-agent collaboration\nframework to bridge the gap between human operators and forecasting models. A\nset of specialized agents is designed to perform different tasks in the\nforecasting workflow and collaborate via a dedicated communication mechanism.\nThis framework embeds interactive mechanisms throughout the load forecasting\npipeline, reducing the technical threshold for non-expert users and enabling\nthe integration of human experience. Our experiments demonstrate that the\ninteractive load forecasting accuracy can be significantly improved when users\nprovide proper insight in key stages. Our cost analysis shows that the\nframework remains affordable, making it practical for real-world deployment."}
{"id": "2505.15840", "pdf": "https://arxiv.org/pdf/2505.15840", "abs": "https://arxiv.org/abs/2505.15840", "authors": ["Zizheng Zhu", "Yingchao Yu", "Zeqi Zheng", "Zhaofei Yu", "Yaochu Jin"], "title": "TDFormer: A Top-Down Attention-Controlled Spiking Transformer", "categories": ["cs.NE", "cs.AI", "cs.CV"], "comment": "28 pages", "summary": "Traditional spiking neural networks (SNNs) can be viewed as a combination of\nmultiple subnetworks with each running for one time step, where the parameters\nare shared, and the membrane potential serves as the only information link\nbetween them. However, the implicit nature of the membrane potential limits its\nability to effectively represent temporal information. As a result, each time\nstep cannot fully leverage information from previous time steps, seriously\nlimiting the model's performance. Inspired by the top-down mechanism in the\nbrain, we introduce TDFormer, a novel model with a top-down feedback structure\nthat functions hierarchically and leverages high-order representations from\nearlier time steps to modulate the processing of low-order information at later\nstages. The feedback structure plays a role from two perspectives: 1) During\nforward propagation, our model increases the mutual information across time\nsteps, indicating that richer temporal information is being transmitted and\nintegrated in different time steps. 2) During backward propagation, we\ntheoretically prove that the feedback structure alleviates the problem of\nvanishing gradients along the time dimension. We find that these mechanisms\ntogether significantly and consistently improve the model performance on\nmultiple datasets. In particular, our model achieves state-of-the-art\nperformance on ImageNet with an accuracy of 86.83%."}
{"id": "2505.16467", "pdf": "https://arxiv.org/pdf/2505.16467", "abs": "https://arxiv.org/abs/2505.16467", "authors": ["Vera Neplenbroek", "Arianna Bisazza", "Raquel Fernández"], "title": "Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization", "categories": ["cs.CL"], "comment": null, "summary": "Generative Large Language Models (LLMs) infer user's demographic information\nfrom subtle cues in the conversation -- a phenomenon called implicit\npersonalization. Prior work has shown that such inferences can lead to lower\nquality responses for users assumed to be from minority groups, even when no\ndemographic information is explicitly provided. In this work, we systematically\nexplore how LLMs respond to stereotypical cues using controlled synthetic\nconversations, by analyzing the models' latent user representations through\nboth model internals and generated answers to targeted user questions. Our\nfindings reveal that LLMs do infer demographic attributes based on these\nstereotypical signals, which for a number of groups even persists when the user\nexplicitly identifies with a different demographic group. Finally, we show that\nthis form of stereotype-driven implicit personalization can be effectively\nmitigated by intervening on the model's internal representations using a\ntrained linear probe to steer them toward the explicitly stated identity. Our\nresults highlight the need for greater transparency and control in how LLMs\nrepresent user identity."}
{"id": "2505.16581", "pdf": "https://arxiv.org/pdf/2505.16581", "abs": "https://arxiv.org/abs/2505.16581", "authors": ["Max Weltevrede", "Moritz A. Zanger", "Matthijs T. J. Spaan", "Wendelin Böhmer"], "title": "How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In the zero-shot policy transfer setting in reinforcement learning, the goal\nis to train an agent on a fixed set of training environments so that it can\ngeneralise to similar, but unseen, testing environments. Previous work has\nshown that policy distillation after training can sometimes produce a policy\nthat outperforms the original in the testing environments. However, it is not\nyet entirely clear why that is, or what data should be used to distil the\npolicy. In this paper, we prove, under certain assumptions, a generalisation\nbound for policy distillation after training. The theory provides two practical\ninsights: for improved generalisation, you should 1) train an ensemble of\ndistilled policies, and 2) distil it on as much data from the training\nenvironments as possible. We empirically verify that these insights hold in\nmore general settings, when the assumptions required for the theory no longer\nhold. Finally, we demonstrate that an ensemble of policies distilled on a\ndiverse dataset can generalise significantly better than the original agent."}
{"id": "2505.15849", "pdf": "https://arxiv.org/pdf/2505.15849", "abs": "https://arxiv.org/abs/2505.15849", "authors": ["Reed Bender", "Karina Kofman", "Blaise Agüera y Arcas", "Michael Levin"], "title": "What Lives? A meta-analysis of diverse opinions on the definition of life", "categories": ["q-bio.OT", "cs.AI", "cs.CY", "q-bio.BM", "q-bio.CB", "q-bio.SC", "stat.AP"], "comment": "54 pages, 4 figures, 2 tables, 11 supplemental figures, 3\n  supplemental tables", "summary": "The question of \"what is life?\" has challenged scientists and philosophers\nfor centuries, producing an array of definitions that reflect both the mystery\nof its emergence and the diversity of disciplinary perspectives brought to bear\non the question. Despite significant progress in our understanding of\nbiological systems, psychology, computation, and information theory, no single\ndefinition for life has yet achieved universal acceptance. This challenge\nbecomes increasingly urgent as advances in synthetic biology, artificial\nintelligence, and astrobiology challenge our traditional conceptions of what it\nmeans to be alive. We undertook a methodological approach that leverages large\nlanguage models (LLMs) to analyze a set of definitions of life provided by a\ncurated set of cross-disciplinary experts. We used a novel pairwise correlation\nanalysis to map the definitions into distinct feature vectors, followed by\nagglomerative clustering, intra-cluster semantic analysis, and t-SNE projection\nto reveal underlying conceptual archetypes. This methodology revealed a\ncontinuous landscape of the themes relating to the definition of life,\nsuggesting that what has historically been approached as a binary taxonomic\nproblem should be instead conceived as differentiated perspectives within a\nunified conceptual latent space. We offer a new methodological bridge between\nreductionist and holistic approaches to fundamental questions in science and\nphilosophy, demonstrating how computational semantic analysis can reveal\nconceptual patterns across disciplinary boundaries, and opening similar\npathways for addressing other contested definitional territories across the\nsciences."}
{"id": "2505.16483", "pdf": "https://arxiv.org/pdf/2505.16483", "abs": "https://arxiv.org/abs/2505.16483", "authors": ["Shuzheng Si", "Haozhe Zhao", "Cheng Gao", "Yuzhuo Bai", "Zhitong Wang", "Bofei Gao", "Kangyang Luo", "Wenhao Li", "Yufei Huang", "Gang Chen", "Fanchao Qi", "Minjia Zhang", "Baobao Chang", "Maosong Sun"], "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1."}
{"id": "2505.16583", "pdf": "https://arxiv.org/pdf/2505.16583", "abs": "https://arxiv.org/abs/2505.16583", "authors": ["Shpresim Sadiku", "Kartikeya Chitranshi", "Hiroshi Kera", "Sebastian Pokutta"], "title": "Training on Plausible Counterfactuals Removes Spurious Correlations", "categories": ["cs.LG"], "comment": null, "summary": "Plausible counterfactual explanations (p-CFEs) are perturbations that\nminimally modify inputs to change classifier decisions while remaining\nplausible under the data distribution. In this study, we demonstrate that\nclassifiers can be trained on p-CFEs labeled with induced \\emph{incorrect}\ntarget classes to classify unperturbed inputs with the original labels. While\nprevious studies have shown that such learning is possible with adversarial\nperturbations, we extend this paradigm to p-CFEs. Interestingly, our\nexperiments reveal that learning from p-CFEs is even more effective: the\nresulting classifiers achieve not only high in-distribution accuracy but also\nexhibit significantly reduced bias with respect to spurious correlations."}
{"id": "2505.15851", "pdf": "https://arxiv.org/pdf/2505.15851", "abs": "https://arxiv.org/abs/2505.15851", "authors": ["Silvia Crafa", "Teresa Scantamburlo"], "title": "Exploring Moral Exercises for Human Oversight of AI systems: Insights from Three Pilot Studies", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "This paper elaborates on the concept of moral exercises as a means to help AI\nactors cultivate virtues that enable effective human oversight of AI systems.\nWe explore the conceptual framework and significance of moral exercises,\nsituating them within the contexts of philosophical discourse, ancient\npractices, and contemporary AI ethics scholarship. We outline the core pillars\nof the moral exercises methodology - eliciting an engaged personal disposition,\nfostering relational understanding, and cultivating technomoral wisdom - and\nemphasize their relevance to key activities and competencies essential for\nhuman oversight of AI systems. Our argument is supported by findings from three\npilot studies involving a company, a multidisciplinary team of AI researchers,\nand higher education students. These studies allow us to explore both the\npotential and the limitations of moral exercises. Based on the collected data,\nwe offer insights into how moral exercises can foster a responsible AI culture\nwithin organizations, and suggest directions for future research."}
{"id": "2505.16491", "pdf": "https://arxiv.org/pdf/2505.16491", "abs": "https://arxiv.org/abs/2505.16491", "authors": ["Dario Di Palma", "Alessandro De Bellis", "Giovanni Servedio", "Vito Walter Anelli", "Fedelucio Narducci", "Tommaso Di Noia"], "title": "LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have rapidly become central to NLP,\ndemonstrating their ability to adapt to various tasks through prompting\ntechniques, including sentiment analysis. However, we still have a limited\nunderstanding of how these models capture sentiment-related information. This\nstudy probes the hidden layers of Llama models to pinpoint where sentiment\nfeatures are most represented and to assess how this affects sentiment\nanalysis.\n  Using probe classifiers, we analyze sentiment encoding across layers and\nscales, identifying the layers and pooling methods that best capture sentiment\nsignals. Our results show that sentiment information is most concentrated in\nmid-layers for binary polarity tasks, with detection accuracy increasing up to\n14% over prompting techniques. Additionally, we find that in decoder-only\nmodels, the last token is not consistently the most informative for sentiment\nencoding. Finally, this approach enables sentiment tasks to be performed with\nmemory requirements reduced by an average of 57%.\n  These insights contribute to a broader understanding of sentiment in LLMs,\nsuggesting layer-specific probing as an effective approach for sentiment tasks\nbeyond prompting, with potential to enhance model utility and reduce memory\nrequirements."}
{"id": "2505.16620", "pdf": "https://arxiv.org/pdf/2505.16620", "abs": "https://arxiv.org/abs/2505.16620", "authors": ["Benjamin Herdeanu", "Juan Nathaniel", "Carla Roesch", "Jatan Buch", "Gregor Ramien", "Johannes Haux", "Pierre Gentine"], "title": "CausalDynamics: A large-scale benchmark for structural discovery of dynamical causal models", "categories": ["cs.LG"], "comment": "16+19 pages, 5+8 figures", "summary": "Causal discovery for dynamical systems poses a major challenge in fields\nwhere active interventions are infeasible. Most methods used to investigate\nthese systems and their associated benchmarks are tailored to deterministic,\nlow-dimensional and weakly nonlinear time-series data. To address these\nlimitations, we present CausalDynamics, a large-scale benchmark and extensible\ndata generation framework to advance the structural discovery of dynamical\ncausal models. Our benchmark consists of true causal graphs derived from\nthousands of coupled ordinary and stochastic differential equations as well as\ntwo idealized climate models. We perform a comprehensive evaluation of\nstate-of-the-art causal discovery algorithms for graph reconstruction on\nsystems with noisy, confounded, and lagged dynamics. CausalDynamics consists of\na plug-and-play, build-your-own coupling workflow that enables the construction\nof a hierarchy of physical systems. We anticipate that our framework will\nfacilitate the development of robust causal discovery algorithms that are\nbroadly applicable across domains while addressing their unique challenges. We\nprovide a user-friendly implementation and documentation on\nhttps://kausable.github.io/CausalDynamics."}
{"id": "2505.15854", "pdf": "https://arxiv.org/pdf/2505.15854", "abs": "https://arxiv.org/abs/2505.15854", "authors": ["Thai-Hoc Vu", "Ngo Hoang Tu", "Thien Huynh-The", "Kyungchun Lee", "Sunghwan Kim", "Miroslav Voznak", "Quoc-Viet Pham"], "title": "Integration of TinyML and LargeML: A Survey of 6G and Beyond", "categories": ["cs.NI", "cs.AI", "cs.ET", "cs.LG", "cs.MA"], "comment": "This work was submitted to IEEE Communications Surveys & Tutorials", "summary": "The transition from 5G networks to 6G highlights a significant demand for\nmachine learning (ML). Deep learning models, in particular, have seen wide\napplication in mobile networking and communications to support advanced\nservices in emerging wireless environments, such as smart healthcare, smart\ngrids, autonomous vehicles, aerial platforms, digital twins, and the metaverse.\nThe rapid expansion of Internet-of-Things (IoT) devices, many with limited\ncomputational capabilities, has accelerated the development of tiny machine\nlearning (TinyML) and resource-efficient ML approaches for cost-effective\nservices. However, the deployment of large-scale machine learning (LargeML)\nsolutions require major computing resources and complex management strategies\nto support extensive IoT services and ML-generated content applications.\nConsequently, the integration of TinyML and LargeML is projected as a promising\napproach for future seamless connectivity and efficient resource management.\n  Although the integration of TinyML and LargeML shows abundant potential,\nseveral challenges persist, including performance optimization, practical\ndeployment strategies, effective resource management, and security\nconsiderations. In this survey, we review and analyze the latest research aimed\nat enabling the integration of TinyML and LargeML models for the realization of\nsmart services and applications in future 6G networks and beyond. The paper\nconcludes by outlining critical challenges and identifying future research\ndirections for the holistic integration of TinyML and LargeML in\nnext-generation wireless networks."}
{"id": "2505.16505", "pdf": "https://arxiv.org/pdf/2505.16505", "abs": "https://arxiv.org/abs/2505.16505", "authors": ["Runcong Zhao", "Chengyu Cao", "Qinglin Zhu", "Xiucheng Lv", "Shun Shao", "Lin Gui", "Ruifeng Xu", "Yulan He"], "title": "Sparse Activation Editing for Reliable Instruction Following in Narratives", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Complex narrative contexts often challenge language models' ability to follow\ninstructions, and existing benchmarks fail to capture these difficulties. To\naddress this, we propose Concise-SAE, a training-free framework that improves\ninstruction following by identifying and editing instruction-relevant neurons\nusing only natural language instructions, without requiring labelled data. To\nthoroughly evaluate our method, we introduce FreeInstruct, a diverse and\nrealistic benchmark of 1,212 examples that highlights the challenges of\ninstruction following in narrative-rich settings. While initially motivated by\ncomplex narratives, Concise-SAE demonstrates state-of-the-art instruction\nadherence across varied tasks without compromising generation quality."}
{"id": "2505.16636", "pdf": "https://arxiv.org/pdf/2505.16636", "abs": "https://arxiv.org/abs/2505.16636", "authors": ["Victor Dheur", "Souhaib Ben Taieb"], "title": "Multivariate Latent Recalibration for Conditional Normalizing Flows", "categories": ["cs.LG"], "comment": null, "summary": "Reliably characterizing the full conditional distribution of a multivariate\nresponse variable given a set of covariates is crucial for trustworthy\ndecision-making. However, misspecified or miscalibrated multivariate models may\nyield a poor approximation of the joint distribution of the response variables,\nleading to unreliable predictions and suboptimal decisions. Furthermore,\nstandard recalibration methods are primarily limited to univariate settings,\nwhile conformal prediction techniques, despite generating multivariate\nprediction regions with coverage guarantees, do not provide a full probability\ndensity function. We address this gap by first introducing a novel notion of\nlatent calibration, which assesses probabilistic calibration in the latent\nspace of a conditional normalizing flow. Second, we propose latent\nrecalibration (LR), a novel post-hoc model recalibration method that learns a\ntransformation of the latent space with finite-sample bounds on latent\ncalibration. Unlike existing methods, LR produces a recalibrated distribution\nwith an explicit multivariate density function while remaining computationally\nefficient. Extensive experiments on both tabular and image datasets show that\nLR consistently improves latent calibration error and the negative\nlog-likelihood of the recalibrated models."}
{"id": "2505.15856", "pdf": "https://arxiv.org/pdf/2505.15856", "abs": "https://arxiv.org/abs/2505.15856", "authors": ["Kai Yin", "Xiangjue Dong", "Chengkai Liu", "Lipai Huang", "Yiming Xiao", "Zhewei Liu", "Ali Mostafavi", "James Caverlee"], "title": "DisastIR: A Comprehensive Information Retrieval Benchmark for Disaster Management", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Effective disaster management requires timely access to accurate and\ncontextually relevant information. Existing Information Retrieval (IR)\nbenchmarks, however, focus primarily on general or specialized domains, such as\nmedicine or finance, neglecting the unique linguistic complexity and diverse\ninformation needs encountered in disaster management scenarios. To bridge this\ngap, we introduce DisastIR, the first comprehensive IR evaluation benchmark\nspecifically tailored for disaster management. DisastIR comprises 9,600 diverse\nuser queries and more than 1.3 million labeled query-passage pairs, covering 48\ndistinct retrieval tasks derived from six search intents and eight general\ndisaster categories that include 301 specific event types. Our evaluations of\n30 state-of-the-art retrieval models demonstrate significant performance\nvariances across tasks, with no single model excelling universally.\nFurthermore, comparative analyses reveal significant performance gaps between\ngeneral-domain and disaster management-specific tasks, highlighting the\nnecessity of disaster management-specific benchmarks for guiding IR model\nselection to support effective decision-making in disaster management\nscenarios. All source codes and DisastIR are available at\nhttps://github.com/KaiYin97/Disaster_IR."}
{"id": "2505.16514", "pdf": "https://arxiv.org/pdf/2505.16514", "abs": "https://arxiv.org/abs/2505.16514", "authors": ["Yuting Huang", "Meitong Guo", "Yiquan Wu", "Ang Li", "Xiaozhong Liu", "Keting Yin", "Changlong Sun", "Fei Wu", "Kun Kuang"], "title": "AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios", "categories": ["cs.CL"], "comment": "15 pages, 4 figures", "summary": "Recent advances in LegalAI have primarily focused on individual case judgment\nanalysis, often overlooking the critical appellate process within the judicial\nsystem. Appeals serve as a core mechanism for error correction and ensuring\nfair trials, making them highly significant both in practice and in research.\nTo address this gap, we present the AppealCase dataset, consisting of 10,000\npairs of real-world, matched first-instance and second-instance documents\nacross 91 categories of civil cases. The dataset also includes detailed\nannotations along five dimensions central to appellate review: judgment\nreversals, reversal reasons, cited legal provisions, claim-level decisions, and\nwhether there is new information in the second instance. Based on these\nannotations, we propose five novel LegalAI tasks and conduct a comprehensive\nevaluation across 20 mainstream models. Experimental results reveal that all\ncurrent models achieve less than 50% F1 scores on the judgment reversal\nprediction task, highlighting the complexity and challenge of the appeal\nscenario. We hope that the AppealCase dataset will spur further research in\nLegalAI for appellate case analysis and contribute to improving consistency in\njudicial decision-making."}
{"id": "2505.16638", "pdf": "https://arxiv.org/pdf/2505.16638", "abs": "https://arxiv.org/abs/2505.16638", "authors": ["Benedikt Höltgen", "Nuria Oliver"], "title": "Reconsidering Fairness Through Unawareness from the Perspective of Model Multiplicity", "categories": ["cs.LG", "cs.CY", "stat.ML"], "comment": null, "summary": "Fairness through Unawareness (FtU) describes the idea that discrimination\nagainst demographic groups can be avoided by not considering group membership\nin the decisions or predictions. This idea has long been criticized in the\nmachine learning literature as not being sufficient to ensure fairness. In\naddition, the use of additional features is typically thought to increase the\naccuracy of the predictions for all groups, so that FtU is sometimes thought to\nbe detrimental to all groups. In this paper, we show both theoretically and\nempirically that FtU can reduce algorithmic discrimination without necessarily\nreducing accuracy. We connect this insight with the literature on Model\nMultiplicity, to which we contribute with novel theoretical and empirical\nresults. Furthermore, we illustrate how, in a real-life application, FtU can\ncontribute to the deployment of more equitable policies without losing\nefficacy. Our findings suggest that FtU is worth considering in practical\napplications, particularly in high-risk scenarios, and that the use of\nprotected attributes such as gender in predictive models should be accompanied\nby a clear and well-founded justification."}
{"id": "2505.15859", "pdf": "https://arxiv.org/pdf/2505.15859", "abs": "https://arxiv.org/abs/2505.15859", "authors": ["Tianyi Ma", "Yiyue Qian", "Zheyuan Zhang", "Zehong Wang", "Xiaoye Qian", "Feifan Bai", "Yifan Ding", "Xuwei Luo", "Shinan Zhang", "Keerthiram Murugesan", "Chuxu Zhang", "Yanfang Ye"], "title": "AutoData: A Multi-Agent System for Open Web Data Collection", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "The exponential growth of data-driven systems and AI technologies has\nintensified the demand for high-quality web-sourced datasets. While existing\ndatasets have proven valuable, conventional web data collection approaches face\nsignificant limitations in terms of human effort and scalability. Current\ndata-collecting solutions fall into two categories: wrapper-based methods that\nstruggle with adaptability and reproducibility, and large language model\n(LLM)-based approaches that incur substantial computational and financial\ncosts. To address these challenges, we propose AutoData, a novel multi-agent\nsystem for Automated web Data collection, that requires minimal human\nintervention, i.e., only necessitating a natural language instruction\nspecifying the desired dataset. In addition, AutoData is designed with a robust\nmulti-agent architecture, featuring a novel oriented message hypergraph\ncoordinated by a central task manager, to efficiently organize agents across\nresearch and development squads. Besides, we introduce a novel hypergraph cache\nsystem to advance the multi-agent collaboration process that enables efficient\nautomated data collection and mitigates the token cost issues prevalent in\nexisting LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark\ndataset supporting live data collection from web sources across three domains:\nacademic, finance, and sports. Comprehensive evaluations over Instruct2DS and\nthree existing benchmark datasets demonstrate AutoData's superior performance\ncompared to baseline methods. Case studies on challenging tasks such as picture\nbook collection and paper extraction from surveys further validate its\napplicability. Our source code and dataset are available at\nhttps://github.com/GraphResearcher/AutoData."}
{"id": "2505.16518", "pdf": "https://arxiv.org/pdf/2505.16518", "abs": "https://arxiv.org/abs/2505.16518", "authors": ["Lovisa Hagström", "Youna Kim", "Haeun Yu", "Sang-goo Lee", "Richard Johansson", "Hyunsoo Cho", "Isabelle Augenstein"], "title": "CUB: Benchmarking Context Utilisation Techniques for Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "27 pages", "summary": "Incorporating external knowledge is crucial for knowledge-intensive tasks,\nsuch as question answering and fact checking. However, language models (LMs)\nmay ignore relevant information that contradicts outdated parametric memory or\nbe distracted by irrelevant contexts. While many context utilisation\nmanipulation techniques (CMTs) that encourage or suppress context utilisation\nhave recently been proposed to alleviate these issues, few have seen systematic\ncomparison. In this paper, we develop CUB (Context Utilisation Benchmark) to\nhelp practitioners within retrieval-augmented generation (RAG) identify the\nbest CMT for their needs. CUB allows for rigorous testing on three distinct\ncontext types, observed to capture key challenges in realistic context\nutilisation scenarios. With this benchmark, we evaluate seven state-of-the-art\nmethods, representative of the main categories of CMTs, across three diverse\ndatasets and tasks, applied to nine LMs. Our results show that most of the\nexisting CMTs struggle to handle the full set of types of contexts that may be\nencountered in real-world retrieval-augmented scenarios. Moreover, we find that\nmany CMTs display an inflated performance on simple synthesised datasets,\ncompared to more realistic datasets with naturally occurring samples.\nAltogether, our results show the need for holistic tests of CMTs and the\ndevelopment of CMTs that can handle multiple context types."}
{"id": "2505.16649", "pdf": "https://arxiv.org/pdf/2505.16649", "abs": "https://arxiv.org/abs/2505.16649", "authors": ["Zhichao Zhu", "Yang Qi", "Hengyuan Ma", "Wenlian Lu", "Jianfeng Feng"], "title": "Stochastic Forward-Forward Learning through Representational Dimensionality Compression", "categories": ["cs.LG", "cs.NE"], "comment": "14 pages, 9 figures, 2 tables", "summary": "The Forward-Forward (FF) algorithm provides a bottom-up alternative to\nbackpropagation (BP) for training neural networks, relying on a layer-wise\n\"goodness\" function to guide learning. Existing goodness functions, inspired by\nenergy-based learning (EBL), are typically defined as the sum of squared\npost-synaptic activations, neglecting the correlations between neurons. In this\nwork, we propose a novel goodness function termed dimensionality compression\nthat uses the effective dimensionality (ED) of fluctuating neural responses to\nincorporate second-order statistical structure. Our objective minimizes ED for\nclamped inputs when noise is considered while maximizing it across the sample\ndistribution, promoting structured representations without the need to prepare\nnegative samples. We demonstrate that this formulation achieves competitive\nperformance compared to other non-BP methods. Moreover, we show that noise\nplays a constructive role that can enhance generalization and improve inference\nwhen predictions are derived from the mean of squared outputs, which is\nequivalent to making predictions based on the energy term. Our findings\ncontribute to the development of more biologically plausible learning\nalgorithms and suggest a natural fit for neuromorphic computing, where\nstochasticity is a computational resource rather than a nuisance. The code is\navailable at https://github.com/ZhichaoZhu/StochasticForwardForward"}
{"id": "2505.15868", "pdf": "https://arxiv.org/pdf/2505.15868", "abs": "https://arxiv.org/abs/2505.15868", "authors": ["Changchun Yang", "Weiqian Dai", "Yilan Zhang", "Siyuan Chen", "Jingdong Hu", "Junkai Su", "Yuxuan Chen", "Ao Xu", "Na Li", "Xin Gao", "Yongguo Yu"], "title": "An Inclusive Foundation Model for Generalizable Cytogenetics in Precision Oncology", "categories": ["q-bio.QM", "cs.AI", "eess.IV"], "comment": "These authors contributed equally to this work: Changchun Yang,\n  Weiqian Dai, Yilan Zhang", "summary": "Chromosome analysis is vital for diagnosing genetic disorders and guiding\ncancer therapy decisions through the identification of somatic clonal\naberrations. However, developing an AI model are hindered by the overwhelming\ncomplexity and diversity of chromosomal abnormalities, requiring extensive\nannotation efforts, while automated methods remain task-specific and lack\ngeneralizability due to the scarcity of comprehensive datasets spanning diverse\nresource conditions. Here, we introduce CHROMA, a foundation model for\ncytogenomics, designed to overcome these challenges by learning generalizable\nrepresentations of chromosomal abnormalities. Pre-trained on over 84,000\nspecimens (~4 million chromosomal images) via self-supervised learning, CHROMA\noutperforms other methods across all types of abnormalities, even when trained\non fewer labelled data and more imbalanced datasets. By facilitating\ncomprehensive mapping of instability and clonal leisons across various\naberration types, CHROMA offers a scalable and generalizable solution for\nreliable and automated clinical analysis, reducing the annotation workload for\nexperts and advancing precision oncology through the early detection of rare\ngenomic abnormalities, enabling broad clinical AI applications and making\nadvanced genomic analysis more accessible."}
{"id": "2505.16520", "pdf": "https://arxiv.org/pdf/2505.16520", "abs": "https://arxiv.org/abs/2505.16520", "authors": ["Giovanni Servedio", "Alessandro De Bellis", "Dario Di Palma", "Vito Walter Anelli", "Tommaso Di Noia"], "title": "Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Factual hallucinations are a major challenge for Large Language Models\n(LLMs). They undermine reliability and user trust by generating inaccurate or\nfabricated content. Recent studies suggest that when generating false\nstatements, the internal states of LLMs encode information about truthfulness.\nHowever, these studies often rely on synthetic datasets that lack realism,\nwhich limits generalization when evaluating the factual accuracy of text\ngenerated by the model itself. In this paper, we challenge the findings of\nprevious work by investigating truthfulness encoding capabilities, leading to\nthe generation of a more realistic and challenging dataset. Specifically, we\nextend previous work by introducing: (1) a strategy for sampling plausible\ntrue-false factoid sentences from tabular data and (2) a procedure for\ngenerating realistic, LLM-dependent true-false datasets from Question Answering\ncollections. Our analysis of two open-source LLMs reveals that while the\nfindings from previous studies are partially validated, generalization to\nLLM-generated datasets remains challenging. This study lays the groundwork for\nfuture research on factuality in LLMs and offers practical guidelines for more\neffective evaluation."}
{"id": "2505.16664", "pdf": "https://arxiv.org/pdf/2505.16664", "abs": "https://arxiv.org/abs/2505.16664", "authors": ["Khoa Tran", "Tri Le", "Bao Huynh", "Hung-Cuong Trinh", "Vy-Rin Nguyen"], "title": "End-to-End Framework for Predicting the Remaining Useful Life of Lithium-Ion Batteries", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate prediction of the Remaining Useful Life (RUL) is essential for\nenabling timely maintenance of lithium-ion batteries, impacting the operational\nefficiency of electric applications that rely on them. This paper proposes a\nRUL prediction approach that leverages data from recent charge-discharge cycles\nto estimate the number of remaining usable cycles. The approach introduces both\na novel signal processing pipeline and a deep learning prediction model. In the\nsignal preprocessing pipeline, a derived capacity feature is computed based on\ncurrent and capacity signals. Alongside original capacity, voltage and current,\nthese features are denoised and enhanced using statistical metrics and a\ndelta-based method to capture differences between the current and previous\ncycles. In the prediction model, the processed features are then fed into a\nhybrid deep learning architecture composed of 1D Convolutional Neural Networks\n(CNN), Attentional Long Short-Term Memory (A-LSTM), and Ordinary Differential\nEquation-based LSTM (ODE-LSTM) modules. This architecture is designed to\ncapture both local signal characteristics and long-range temporal dependencies\nwhile modeling the continuous-time dynamics of battery degradation. The model\nis further evaluated using transfer learning across different learning\nstrategies and target data partitioning scenarios. Results indicate that the\nmodel maintains robust performance, even when fine-tuned on limited target\ndata. Experimental results on two publicly available large-scale datasets\ndemonstrate that the proposed method outperforms a baseline deep learning\napproach and machine learning techniques, achieving an RMSE of 101.59,\nhighlighting its strong potential for real-world RUL prediction applications."}
{"id": "2505.15879", "pdf": "https://arxiv.org/pdf/2505.15879", "abs": "https://arxiv.org/abs/2505.15879", "authors": ["Yue Fan", "Xuehai He", "Diji Yang", "Kaizhi Zheng", "Ching-Chen Kuo", "Yuting Zheng", "Sravana Jyothi Narayanaraju", "Xinze Guan", "Xin Eric Wang"], "title": "GRIT: Teaching MLLMs to Think with Images", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent studies have demonstrated the efficacy of using Reinforcement Learning\n(RL) in building reasoning models that articulate chains of thoughts prior to\nproducing final answers. However, despite ongoing advances that aim at enabling\nreasoning for vision-language tasks, existing open-source visual reasoning\nmodels typically generate reasoning content with pure natural language, lacking\nexplicit integration of visual information. This limits their ability to\nproduce clearly articulated and visually grounded reasoning chains. To this\nend, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method\nfor training MLLMs to think with images. GRIT introduces a grounded reasoning\nparadigm, in which models generate reasoning chains that interleave natural\nlanguage and explicit bounding box coordinates. These coordinates point to\nregions of the input image that the model consults during its reasoning\nprocess. Additionally, GRIT is equipped with a reinforcement learning approach,\nGRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused\non the final answer accuracy and format of the grounded reasoning output, which\neliminates the need for data with reasoning chain annotations or explicit\nbounding box labels. As a result, GRIT achieves exceptional data efficiency,\nrequiring as few as 20 image-question-answer triplets from existing datasets.\nComprehensive evaluations demonstrate that GRIT effectively trains MLLMs to\nproduce coherent and visually grounded reasoning chains, showing a successful\nunification of reasoning and grounding abilities."}
{"id": "2505.16522", "pdf": "https://arxiv.org/pdf/2505.16522", "abs": "https://arxiv.org/abs/2505.16522", "authors": ["Zhouhao Sun", "Zhiyuan Kan", "Xiao Ding", "Li Du", "Yang Zhao", "Bing Qin", "Ting Liu"], "title": "Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite significant progress, recent studies have indicated that current\nlarge language models (LLMs) may still utilize bias during inference, leading\nto the poor generalizability of LLMs. Some benchmarks are proposed to\ninvestigate the generalizability of LLMs, with each piece of data typically\ncontaining one type of controlled bias. However, a single piece of data may\ncontain multiple types of biases in practical applications. To bridge this gap,\nwe propose a multi-bias benchmark where each piece of data contains five types\nof biases. The evaluations conducted on this benchmark reveal that the\nperformance of existing LLMs and debiasing methods is unsatisfying,\nhighlighting the challenge of eliminating multiple types of biases\nsimultaneously. To overcome this challenge, we propose a causal effect\nestimation-guided multi-bias elimination method (CMBE). This method first\nestimates the causal effect of multiple types of biases simultaneously.\nSubsequently, we eliminate the causal effect of biases from the total causal\neffect exerted by both the semantic information and biases during inference.\nExperimental results show that CMBE can effectively eliminate multiple types of\nbias simultaneously to enhance the generalizability of LLMs."}
{"id": "2505.16672", "pdf": "https://arxiv.org/pdf/2505.16672", "abs": "https://arxiv.org/abs/2505.16672", "authors": ["Yun-Cheng Tsai", "Samuel Yen-Chi Chen"], "title": "Quantum Feature Optimization for Enhanced Clustering of Blockchain Transaction Data", "categories": ["cs.LG"], "comment": "6 pages, 6 figures, 1 table", "summary": "Blockchain transaction data exhibits high dimensionality, noise, and\nintricate feature entanglement, presenting significant challenges for\ntraditional clustering algorithms. In this study, we conduct a comparative\nanalysis of three clustering approaches: (1) Classical K-Means Clustering,\napplied to pre-processed feature representations; (2) Hybrid Clustering,\nwherein classical features are enhanced with quantum random features extracted\nusing randomly initialized quantum neural networks (QNNs); and (3) Fully\nQuantum Clustering, where a QNN is trained in a self-supervised manner\nleveraging a SwAV-based loss function to optimize the feature space for\nclustering directly. The proposed experimental framework systematically\ninvestigates the impact of quantum circuit depth and the number of learned\nprototypes, demonstrating that even shallow quantum circuits can effectively\nextract meaningful non-linear representations, significantly improving\nclustering performance."}
{"id": "2505.15888", "pdf": "https://arxiv.org/pdf/2505.15888", "abs": "https://arxiv.org/abs/2505.15888", "authors": ["Valentin Villecroze", "Yixin Wang", "Gabriel Loaiza-Ganem"], "title": "Last Layer Empirical Bayes", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted at the ICBINB Worshop at ICLR 2025", "summary": "The task of quantifying the inherent uncertainty associated with neural\nnetwork predictions is a key challenge in artificial intelligence. Bayesian\nneural networks (BNNs) and deep ensembles are among the most prominent\napproaches to tackle this task. Both approaches produce predictions by\ncomputing an expectation of neural network outputs over some distribution on\nthe corresponding weights; this distribution is given by the posterior in the\ncase of BNNs, and by a mixture of point masses for ensembles. Inspired by\nrecent work showing that the distribution used by ensembles can be understood\nas a posterior corresponding to a learned data-dependent prior, we propose last\nlayer empirical Bayes (LLEB). LLEB instantiates a learnable prior as a\nnormalizing flow, which is then trained to maximize the evidence lower bound;\nto retain tractability we use the flow only on the last layer. We show why LLEB\nis well motivated, and how it interpolates between standard BNNs and ensembles\nin terms of the strength of the prior that they use. LLEB performs on par with\nexisting approaches, highlighting that empirical Bayes is a promising direction\nfor future research in uncertainty quantification."}
{"id": "2505.16526", "pdf": "https://arxiv.org/pdf/2505.16526", "abs": "https://arxiv.org/abs/2505.16526", "authors": ["Heejae Suh", "Yejin Jeon", "Deokhyung Kang", "Taehee Park", "Yejin Min", "Gary Geunbae Lee"], "title": "EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Findings, long paper)", "summary": "Small large language models (sLLMs) offer the advantage of being lightweight\nand efficient, which makes them suitable for resource-constrained environments.\nHowever, sLLMs often struggle to maintain topic consistency in task-oriented\ndialogue systems, which is critical for scenarios such as service chatbots.\nSpecifically, it is important to ensure that the model denies off-topic or\nmalicious inputs and adheres to its intended functionality so as to prevent\npotential misuse and uphold reliability. Towards this, existing activation\nengineering approaches have been proposed to manipulate internal activations\nduring inference. While these methods are effective in certain scenarios, our\npreliminary experiments reveal their limitations in ensuring topic adherence.\nTherefore, to address this, we propose a novel approach termed Entropy-scaled\nSteering vectors for Topic Maintenance (EnSToM). EnSToM dynamically adjusts the\nsteering intensity based on input uncertainty, which allows the model to handle\noff-topic distractors effectively while preserving on-topic accuracy. Our\nexperiments demonstrate that EnSToM achieves significant performance gain with\na relatively small data size compared to fine-tuning approaches. By improving\ntopic adherence without compromising efficiency, our approach provides a robust\nsolution for enhancing sLLM-based dialogue systems."}
{"id": "2505.16675", "pdf": "https://arxiv.org/pdf/2505.16675", "abs": "https://arxiv.org/abs/2505.16675", "authors": ["Wenwen Qiang", "Jingyao Wang", "Zeen Song", "Jiangmeng Li", "Changwen Zheng"], "title": "On the Out-of-Distribution Generalization of Self-Supervised Learning", "categories": ["cs.LG"], "comment": null, "summary": "In this paper, we focus on the out-of-distribution (OOD) generalization of\nself-supervised learning (SSL). By analyzing the mini-batch construction during\nthe SSL training phase, we first give one plausible explanation for SSL having\nOOD generalization. Then, from the perspective of data generation and causal\ninference, we analyze and conclude that SSL learns spurious correlations during\nthe training process, which leads to a reduction in OOD generalization. To\naddress this issue, we propose a post-intervention distribution (PID) grounded\nin the Structural Causal Model. PID offers a scenario where the spurious\nvariable and label variable is mutually independent. Besides, we demonstrate\nthat if each mini-batch during SSL training satisfies PID, the resulting SSL\nmodel can achieve optimal worst-case OOD performance. This motivates us to\ndevelop a batch sampling strategy that enforces PID constraints through the\nlearning of a latent variable model. Through theoretical analysis, we\ndemonstrate the identifiability of the latent variable model and validate the\neffectiveness of the proposed sampling strategy. Experiments conducted on\nvarious downstream OOD tasks demonstrate the effectiveness of the proposed\nsampling strategy."}
{"id": "2505.15916", "pdf": "https://arxiv.org/pdf/2505.15916", "abs": "https://arxiv.org/abs/2505.15916", "authors": ["Juvenal Domingos Júnior", "Augusto Faria", "E. Seiti de Oliveira", "Erick de Brito", "Matheus Teotonio", "Andre Assumpção", "Diedre Carmo", "Roberto Lotufo", "Jayr Pereira"], "title": "BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents BR-TaxQA-R, a novel dataset designed to support question\nanswering with references in the context of Brazilian personal income tax law.\nThe dataset contains 715 questions from the 2024 official Q\\&A document\npublished by Brazil's Internal Revenue Service, enriched with statutory norms\nand administrative rulings from the Conselho Administrativo de Recursos Fiscais\n(CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using\nOpenAI embeddings for searching and GPT-4o-mini for answer generation. We\ncompare different text segmentation strategies and benchmark our system against\ncommercial tools such as ChatGPT and Perplexity.ai using RAGAS-based metrics.\nResults show that our custom RAG pipeline outperforms commercial systems in\nResponse Relevancy, indicating stronger alignment with user queries, while\ncommercial models achieve higher scores in Factual Correctness and fluency.\nThese findings highlight a trade-off between legally grounded generation and\nlinguistic fluency. Crucially, we argue that human expert evaluation remains\nessential to ensure the legal validity of AI-generated answers in high-stakes\ndomains such as taxation. BR-TaxQA-R is publicly available at\nhttps://huggingface.co/datasets/unicamp-dl/BR-TaxQA-R."}
{"id": "2505.16538", "pdf": "https://arxiv.org/pdf/2505.16538", "abs": "https://arxiv.org/abs/2505.16538", "authors": ["Ercong Nie", "Helmut Schmid", "Hinrich Schütze"], "title": "Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models", "categories": ["cs.CL"], "comment": "16 pages, 5 figures", "summary": "Language confusion -- where large language models (LLMs) generate unintended\nlanguages against the user's need -- remains a critical challenge, especially\nfor English-centric models. We present the first mechanistic interpretability\n(MI) study of language confusion, combining behavioral benchmarking with\nneuron-level analysis. Using the Language Confusion Benchmark (LCB), we show\nthat confusion points (CPs) -- specific positions where language switches occur\n-- are central to this phenomenon. Through layer-wise analysis with TunedLens\nand targeted neuron attribution, we reveal that transition failures in the\nfinal layers drive confusion. We further demonstrate that editing a small set\nof critical neurons, identified via comparative analysis with\nmultilingual-tuned models, substantially mitigates confusion without harming\ngeneral competence or fluency. Our approach matches multilingual alignment in\nconfusion reduction for most languages and yields cleaner, higher-quality\noutputs. These findings provide new insights into the internal dynamics of LLMs\nand highlight neuron-level interventions as a promising direction for robust,\ninterpretable multilingual language modeling."}
{"id": "2505.16680", "pdf": "https://arxiv.org/pdf/2505.16680", "abs": "https://arxiv.org/abs/2505.16680", "authors": ["Filip Thor", "Carl Nettelblad"], "title": "Learning Genomic Structure from $k$-mers", "categories": ["cs.LG", "q-bio.GN", "q-bio.QM"], "comment": null, "summary": "Sequencing a genome to determine an individual's DNA produces an enormous\nnumber of short nucleotide subsequences known as reads, which must be\nreassembled to reconstruct the full genome. We present a method for analyzing\nthis type of data using contrastive learning, in which an encoder model is\ntrained to produce embeddings that cluster together sequences from the same\ngenomic region. The sequential nature of genomic regions is preserved in the\nform of trajectories through this embedding space. Trained solely to reflect\nthe structure of the genome, the resulting model provides a general\nrepresentation of $k$-mer sequences, suitable for a range of downstream tasks\ninvolving read data. We apply our framework to learn the structure of the $E.\\\ncoli$ genome, and demonstrate its use in simulated ancient DNA (aDNA) read\nmapping and identification of structural variations. Furthermore, we illustrate\nthe potential of using this type of model for metagenomic species\nidentification. We show how incorporating a domain-specific noise model can\nenhance embedding robustness, and how a supervised contrastive learning setting\ncan be adopted when a linear reference genome is available, by introducing a\ndistance thresholding parameter $\\Gamma$. The model can also be trained fully\nself-supervised on read data, enabling analysis without the need to construct a\nfull genome assembly using specialized algorithms. Small prediction heads based\non a pre-trained embedding are shown to perform on par with BWA-aln, the\ncurrent gold standard approach for aDNA mapping, in terms of accuracy and\nruntime for short genomes. Given the method's favorable scaling properties with\nrespect to total genome size, inference using our approach is highly promising\nfor metagenomic applications and for mapping to genomes comparable in size to\nthe human genome."}
{"id": "2505.15918", "pdf": "https://arxiv.org/pdf/2505.15918", "abs": "https://arxiv.org/abs/2505.15918", "authors": ["Aliakbar Nafar", "Kristen Brent Venable", "Zijun Cui", "Parisa Kordjamshidi"], "title": "Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated potential as factual knowledge\nbases; however, their capability to generate probabilistic knowledge about\nreal-world events remains understudied. This paper investigates using\nprobabilistic knowledge inherent in LLMs to derive probability estimates for\nstatements concerning events and their interrelationships captured via a\nBayesian Network (BN). Using LLMs in this context allows for the\nparameterization of BNs, enabling probabilistic modeling within specific\ndomains. Experiments on eighty publicly available Bayesian Networks, from\nhealthcare to finance, demonstrate that querying LLMs about the conditional\nprobabilities of events provides meaningful results when compared to baselines,\nincluding random and uniform distributions, as well as approaches based on\nnext-token generation probabilities. We explore how these LLM-derived\ndistributions can serve as expert priors to refine distributions extracted from\nminimal data, significantly reducing systematic biases. Overall, this work\nintroduces a promising strategy for automatically constructing Bayesian\nNetworks by combining probabilistic knowledge extracted from LLMs with small\namounts of real-world data. Additionally, we evaluate several prompting\nstrategies for eliciting probabilistic knowledge from LLMs and establish the\nfirst comprehensive baseline for assessing LLM performance in extracting\nprobabilistic knowledge."}
{"id": "2505.16552", "pdf": "https://arxiv.org/pdf/2505.16552", "abs": "https://arxiv.org/abs/2505.16552", "authors": ["Wenhui Tan", "Jiaze Li", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Ruihua Song"], "title": "Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains", "categories": ["cs.CL"], "comment": "15 pages, 8 figures", "summary": "Large Language Models (LLMs) achieve superior performance through\nChain-of-Thought (CoT) reasoning, but these token-level reasoning chains are\ncomputationally expensive and inefficient. In this paper, we introduce\nCompressed Latent Reasoning (CoLaR), a novel framework that dynamically\ncompresses reasoning processes in latent space through a two-stage training\napproach. First, during supervised fine-tuning, CoLaR extends beyond next-token\nprediction by incorporating an auxiliary next compressed embedding prediction\nobjective. This process merges embeddings of consecutive tokens using a\ncompression factor randomly sampled from a predefined range, and trains a\nspecialized latent head to predict distributions of subsequent compressed\nembeddings. Second, we enhance CoLaR through reinforcement learning (RL) that\nleverages the latent head's non-deterministic nature to explore diverse\nreasoning paths and exploit more compact ones. This approach enables CoLaR to:\ni) perform reasoning at a dense latent level (i.e., silently), substantially\nreducing reasoning chain length, and ii) dynamically adjust reasoning speed at\ninference time by simply prompting the desired compression factor. Extensive\nexperiments across four mathematical reasoning datasets demonstrate that CoLaR\nachieves 14.1% higher accuracy than latent-based baseline methods at comparable\ncompression ratios, and reduces reasoning chain length by 53.3% with only 4.8%\nperformance degradation compared to explicit CoT method. Moreover, when applied\nto more challenging mathematical reasoning tasks, our RL-enhanced CoLaR\ndemonstrates performance gains of up to 5.4% while dramatically reducing latent\nreasoning chain length by 82.8%. The code and models will be released upon\nacceptance."}
{"id": "2505.16690", "pdf": "https://arxiv.org/pdf/2505.16690", "abs": "https://arxiv.org/abs/2505.16690", "authors": ["Beier Luo", "Shuoyuan Wang", "Yixuan Li", "Hongxin Wei"], "title": "Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Post-training of large language models is essential for adapting pre-trained\nlanguage models (PLMs) to align with human preferences and downstream tasks.\nWhile PLMs typically exhibit well-calibrated confidence, post-trained language\nmodels (PoLMs) often suffer from over-confidence, assigning high confidence to\nboth correct and incorrect outputs, which can undermine reliability in critical\napplications. A major obstacle in calibrating PoLMs is the scarcity of labeled\ndata for individual downstream tasks. To address this, we propose\nDisagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to\noptimize the parameters (e.g., temperature $\\tau$) in post-hoc confidence\ncalibration. Our method is motivated by the under-confidence issue caused by\nprediction disagreement between the PLM and PoLM while aligning their\nconfidence via temperature scaling. Theoretically, the PLM's confidence\nunderestimates PoLM's prediction accuracy on disagreement examples, causing a\nlarger $\\tau$ and producing under-confident predictions. DACA mitigates this by\nselectively using only agreement examples for calibration, effectively\ndecoupling the influence of disagreement. In this manner, our method avoids an\noverly large $\\tau$ in temperature scaling caused by disagreement examples,\nimproving calibration performance. Extensive experiments demonstrate the\neffectiveness of our method, improving the average ECE of open-sourced and\nAPI-based LLMs (e.g. GPT-4o) by up to 15.08$\\%$ on common benchmarks."}
{"id": "2505.15925", "pdf": "https://arxiv.org/pdf/2505.15925", "abs": "https://arxiv.org/abs/2505.15925", "authors": ["Bowen Feng", "Zhiting Mei", "Baiang Li", "Julian Ost", "Roger Girgis", "Anirudha Majumdar", "Felix Heide"], "title": "VERDI: VLM-Embedded Reasoning for Autonomous Driving", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "While autonomous driving (AD) stacks struggle with decision making under\npartial observability and real-world complexity, human drivers are capable of\ncommonsense reasoning to make near-optimal decisions with limited information.\nRecent work has attempted to leverage finetuned Vision-Language Models (VLMs)\nfor trajectory planning at inference time to emulate human behavior. Despite\ntheir success in benchmark evaluations, these methods are often impractical to\ndeploy (a 70B parameter VLM inference at merely 8 tokens per second requires\nmore than 160G of memory), and their monolithic network structure prohibits\nsafety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for\nautonomous Driving (VERDI), a training-time framework that distills the\nreasoning process and commonsense knowledge of VLMs into the AD stack. VERDI\naugments modular differentiable end-to-end (e2e) AD models by aligning\nintermediate module outputs at the perception, prediction, and planning stages\nwith text features explaining the driving reasoning process produced by VLMs.\nBy encouraging alignment in latent space, \\textsc{VERDI} enables the modular AD\nstack to internalize structured reasoning, without incurring the inference-time\ncosts of large VLMs. We demonstrate the effectiveness of our method on the\nNuScenes dataset and find that VERDI outperforms existing e2e methods that do\nnot embed reasoning by 10% in $\\ell_{2}$ distance, while maintaining high\ninference speed."}
{"id": "2505.16566", "pdf": "https://arxiv.org/pdf/2505.16566", "abs": "https://arxiv.org/abs/2505.16566", "authors": ["Dongwon Noh", "Donghyeok Koh", "Junghun Yuk", "Gyuwan Kim", "Jaeyong Lee", "Kyungtae Lim", "Cheoneum Park"], "title": "ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts", "categories": ["cs.CL"], "comment": null, "summary": "Prior benchmarks for evaluating the domain-specific knowledge of large\nlanguage models (LLMs) lack the scalability to handle complex academic tasks.\nTo address this, we introduce \\texttt{ScholarBench}, a benchmark centered on\ndeep expert knowledge and complex academic problem-solving, which evaluates the\nacademic reasoning ability of LLMs and is constructed through a three-step\nprocess. \\texttt{ScholarBench} targets more specialized and logically complex\ncontexts derived from academic literature, encompassing five distinct problem\ntypes. Unlike prior benchmarks, \\texttt{ScholarBench} evaluates the\nabstraction, comprehension, and reasoning capabilities of LLMs across eight\ndistinct research domains. To ensure high-quality evaluation data, we define\ncategory-specific example attributes and design questions that are aligned with\nthe characteristic research methodologies and discourse structures of each\ndomain. Additionally, this benchmark operates as an English-Korean bilingual\ndataset, facilitating simultaneous evaluation for linguistic capabilities of\nLLMs in both languages. The benchmark comprises 5,031 examples in Korean and\n5,309 in English, with even state-of-the-art models like o3-mini achieving an\naverage evaluation score of only 0.543, demonstrating the challenging nature of\nthis benchmark."}
{"id": "2505.16705", "pdf": "https://arxiv.org/pdf/2505.16705", "abs": "https://arxiv.org/abs/2505.16705", "authors": ["Seonghwan Park", "Jueun Mun", "Donghyun Oh", "Namhoon Lee"], "title": "An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Concept bottleneck models (CBMs) ensure interpretability by decomposing\npredictions into human interpretable concepts. Yet the annotations used for\ntraining CBMs that enable this transparency are often noisy, and the impact of\nsuch corruption is not well understood. In this study, we present the first\nsystematic study of noise in CBMs and show that even moderate corruption\nsimultaneously impairs prediction performance, interpretability, and the\nintervention effectiveness. Our analysis identifies a susceptible subset of\nconcepts whose accuracy declines far more than the average gap between noisy\nand clean supervision and whose corruption accounts for most performance loss.\nTo mitigate this vulnerability we propose a two-stage framework. During\ntraining, sharpness-aware minimization stabilizes the learning of\nnoise-sensitive concepts. During inference, where clean labels are unavailable,\nwe rank concepts by predictive entropy and correct only the most uncertain\nones, using uncertainty as a proxy for susceptibility. Theoretical analysis and\nextensive ablations elucidate why sharpness-aware training confers robustness\nand why uncertainty reliably identifies susceptible concepts, providing a\nprincipled basis that preserves both interpretability and resilience in the\npresence of noise."}
{"id": "2505.15946", "pdf": "https://arxiv.org/pdf/2505.15946", "abs": "https://arxiv.org/abs/2505.15946", "authors": ["Yuxiang Wei", "Yanteng Zhang", "Xi Xiao", "Tianyang Wang", "Xiao Wang", "Vince D. Calhoun"], "title": "MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "Decoding visual experiences from fMRI offers a powerful avenue to understand\nhuman perception and develop advanced brain-computer interfaces. However,\ncurrent progress often prioritizes maximizing reconstruction fidelity while\noverlooking interpretability, an essential aspect for deriving neuroscientific\ninsight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework\ndesigned for high-fidelity, adaptable, and interpretable visual reconstruction.\nMoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture\nwhere distinct experts process fMRI signals from functionally related voxel\ngroups, mimicking specialized brain networks. The experts are first trained to\nencode fMRI into the frozen CLIP space. A finetuned diffusion model then\nsynthesizes images, guided by expert outputs through a novel dual-stage routing\nmechanism that dynamically weighs expert contributions across the diffusion\nprocess. MoRE-Brain offers three main advancements: First, it introduces a\nnovel Mixture-of-Experts architecture grounded in brain network principles for\nneuro-decoding. Second, it achieves efficient cross-subject generalization by\nsharing core expert networks while adapting only subject-specific routers.\nThird, it provides enhanced mechanistic insight, as the explicit routing\nreveals precisely how different modeled brain regions shape the semantic and\nspatial attributes of the reconstructed image. Extensive experiments validate\nMoRE-Brain's high reconstruction fidelity, with bottleneck analyses further\ndemonstrating its effective utilization of fMRI signals, distinguishing genuine\nneural decoding from over-reliance on generative priors. Consequently,\nMoRE-Brain marks a substantial advance towards more generalizable and\ninterpretable fMRI-based visual decoding. Code will be publicly available soon:\nhttps://github.com/yuxiangwei0808/MoRE-Brain."}
{"id": "2505.16570", "pdf": "https://arxiv.org/pdf/2505.16570", "abs": "https://arxiv.org/abs/2505.16570", "authors": ["Dongyang Fan", "Vinko Sabolčec", "Martin Jaggi"], "title": "URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are commonly pretrained on vast corpora of text\nwithout utilizing contextual metadata such as source, quality, or topic,\nleading to a context-free learning paradigm. While recent studies suggest that\nadding metadata like URL information as context (i.e., auxiliary inputs not\nused in the loss calculation) can improve training efficiency and downstream\nperformance, they offer limited understanding of which types of metadata are\ntruly effective and under what conditions. In this work, we conduct a\nsystematic evaluation and find that not all metadata types contribute equally.\nOnly URL context speeds up training, whereas quality scores and topic/format\ndomain information offer no clear benefit. Furthermore, the improved downstream\nperformances of URL conditioning emerge only when longer prompts are used at\ninference time. In addition, we demonstrate that context-aware pretraining\nenables more controllable generation than context-free pretraining, in a\nclassifier-free guidance fashion. Although topic and format metadata do not\naccelerate training, they are effective for steering outputs, offering\nhuman-interpretable control over generation."}
{"id": "2505.16710", "pdf": "https://arxiv.org/pdf/2505.16710", "abs": "https://arxiv.org/abs/2505.16710", "authors": ["Wenhao Li", "Yuxin Zhang", "Gen Luo", "Daohai Yu", "Rongrong Ji"], "title": "Training Long-Context LLMs Efficiently via Chunk-wise Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While long-context large language models (LLMs) exhibit remarkable document\nprocessing capabilities, their prohibitively high training costs often hinder\ncustomized applications. To mitigate this issue, we propose \\textit{Sequential\nChunk-wise Optimization} (SeCO), a memory-efficient training paradigm that\npartitions lengthy inputs into manageable chunks. Each chunk independently\nconstructs its computational graph and performs localized backpropagation,\nensuring that only one chunk's forward activations are stored in memory.\nBuilding on SeCO, we further introduce \\textit{Sparse Chunk-wise Optimization}\n(SpaCO), which reduces computational overhead by selectively propagating\ngradients to specific chunks and incorporates a carefully designed compensation\nfactor to ensure unbiased gradient estimation. SpaCO decouples the\ncomputational cost of backpropagation from the context length, enabling\ntraining time to gradually converge to inference time as sequences become\nlonger. Implemented as lightweight training wrappers, both SeCO and SpaCO offer\nsubstantial practical benefits. For example, when fine-tuning an 8B model with\nLoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to\n16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up\nto 3x faster than SeCO under the same experimental setup. These innovations\nprovide new insights into optimizing long-context models, making them more\naccessible for practical applications. We have open-sourced the code at\n\\href{https://github.com/wenhaoli-xmu/seco}{here}."}
{"id": "2505.15952", "pdf": "https://arxiv.org/pdf/2505.15952", "abs": "https://arxiv.org/abs/2505.15952", "authors": ["Mohammad Reza Taesiri", "Abhijay Ghildyal", "Saman Zadtootaghaj", "Nabajeet Barman", "Cor-Paul Bezemer"], "title": "VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance", "categories": ["cs.CV", "cs.AI"], "comment": "Project website with code and data:\n  https://asgaardlab.github.io/videogameqa-bench/", "summary": "With video games now generating the highest revenues in the entertainment\nindustry, optimizing game development workflows has become essential for the\nsector's sustained growth. Recent advancements in Vision-Language Models (VLMs)\noffer considerable potential to automate and enhance various aspects of game\ndevelopment, particularly Quality Assurance (QA), which remains one of the\nindustry's most labor-intensive processes with limited automation options. To\naccurately evaluate the performance of VLMs in video game QA tasks and\ndetermine their effectiveness in handling real-world scenarios, there is a\nclear need for standardized benchmarks, as existing benchmarks are insufficient\nto address the specific requirements of this domain. To bridge this gap, we\nintroduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array\nof game QA activities, including visual unit testing, visual regression\ntesting, needle-in-a-haystack tasks, glitch detection, and bug report\ngeneration for both images and videos of various games. Code and data are\navailable at: https://asgaardlab.github.io/videogameqa-bench/"}
{"id": "2505.16576", "pdf": "https://arxiv.org/pdf/2505.16576", "abs": "https://arxiv.org/abs/2505.16576", "authors": ["Spencer Hong", "Meng Luo", "Xinyi Wan"], "title": "EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions", "categories": ["cs.CL"], "comment": null, "summary": "Determining the veracity of atomic claims is an imperative component of many\nrecently proposed fact-checking systems. Many approaches tackle this problem by\nfirst retrieving evidence by querying a search engine and then performing\nclassification by providing the evidence set and atomic claim to a large\nlanguage model, but this process deviates from what a human would do in order\nto perform the task. Recent work attempted to address this issue by proposing\niterative evidence retrieval, allowing for evidence to be collected several\ntimes and only when necessary. Continuing along this line of research, we\npropose a novel claim verification system, called EMULATE, which is designed to\nbetter emulate human actions through the use of a multi-agent framework where\neach agent performs a small part of the larger task, such as ranking search\nresults according to predefined criteria or evaluating webpage content.\nExtensive experiments on several benchmarks show clear improvements over prior\nwork, demonstrating the efficacy of our new multi-agent framework."}
{"id": "2505.16724", "pdf": "https://arxiv.org/pdf/2505.16724", "abs": "https://arxiv.org/abs/2505.16724", "authors": ["Konstantinos Barmpas", "Na Lee", "Yannis Panagakis", "Dimitrios A. Adamos", "Nikolaos Laskaris", "Stefanos Zafeiriou"], "title": "Advancing Brainwave Modeling with a Codebook-Based Foundation Model", "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": null, "summary": "Recent advances in large-scale pre-trained Electroencephalogram (EEG) models\nhave shown great promise, driving progress in Brain-Computer Interfaces (BCIs)\nand healthcare applications. However, despite their success, many existing\npre-trained models have struggled to fully capture the rich information content\nof neural oscillations, a limitation that fundamentally constrains their\nperformance and generalizability across diverse BCI tasks. This limitation is\nfrequently rooted in suboptimal architectural design choices which constrain\ntheir representational capacity. In this work, we introduce LaBraM++, an\nenhanced Large Brainwave Foundation Model (LBM) that incorporates principled\nimprovements grounded in robust signal processing foundations. LaBraM++\ndemonstrates substantial gains across a variety of tasks, consistently\noutperforming its originally-based architecture and achieving competitive\nresults when compared to other open-source LBMs. Its superior performance and\ntraining efficiency highlight its potential as a strong foundation for future\nadvancements in LBMs."}
{"id": "2505.15957", "pdf": "https://arxiv.org/pdf/2505.15957", "abs": "https://arxiv.org/abs/2505.15957", "authors": ["Chih-Kai Yang", "Neo S. Ho", "Hung-yi Lee"], "title": "Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "Project Website: https://github.com/b08202033/LALM-Evaluation-Survey", "summary": "With advancements in large audio-language models (LALMs), which enhance large\nlanguage models (LLMs) with auditory capabilities, these models are expected to\ndemonstrate universal proficiency across various auditory tasks. While numerous\nbenchmarks have emerged to assess LALMs' performance, they remain fragmented\nand lack a structured taxonomy. To bridge this gap, we conduct a comprehensive\nsurvey and propose a systematic taxonomy for LALM evaluations, categorizing\nthem into four dimensions based on their objectives: (1) General Auditory\nAwareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented\nAbility, and (4) Fairness, Safety, and Trustworthiness. We provide detailed\noverviews within each category and highlight challenges in this field, offering\ninsights into promising future directions. To the best of our knowledge, this\nis the first survey specifically focused on the evaluations of LALMs, providing\nclear guidelines for the community. We will release the collection of the\nsurveyed papers and actively maintain it to support ongoing advancements in the\nfield."}
{"id": "2505.16582", "pdf": "https://arxiv.org/pdf/2505.16582", "abs": "https://arxiv.org/abs/2505.16582", "authors": ["Jianbiao Mei", "Tao Hu", "Daocheng Fu", "Licheng Wen", "Xuemeng Yang", "Rong Wu", "Pinlong Cai", "Xing Gao", "Yu Yang", "Chengjun Xie", "Botian Shi", "Yong Liu", "Yu Qiao"], "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 9 figures", "summary": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones."}
{"id": "2505.16725", "pdf": "https://arxiv.org/pdf/2505.16725", "abs": "https://arxiv.org/abs/2505.16725", "authors": ["Phillip Mueller", "Jannik Wiese", "Sebastian Mueller", "Lars Mikelsons"], "title": "Masked Conditioning for Deep Generative Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Datasets in engineering domains are often small, sparsely labeled, and\ncontain numerical as well as categorical conditions. Additionally.\ncomputational resources are typically limited in practical applications which\nhinders the adoption of generative models for engineering tasks. We introduce a\nnovel masked-conditioning approach, that enables generative models to work with\nsparse, mixed-type data. We mask conditions during training to simulate sparse\nconditions at inference time. For this purpose, we explore the use of various\nsparsity schedules that show different strengths and weaknesses. In addition,\nwe introduce a flexible embedding that deals with categorical as well as\nnumerical conditions. We integrate our method into an efficient variational\nautoencoder as well as a latent diffusion model and demonstrate the\napplicability of our approach on two engineering-related datasets of 2D point\nclouds and images. Finally, we show that small models trained on limited data\ncan be coupled with large pretrained foundation models to improve generation\nquality while retaining the controllability induced by our conditioning scheme."}
{"id": "2505.15962", "pdf": "https://arxiv.org/pdf/2505.15962", "abs": "https://arxiv.org/abs/2505.15962", "authors": ["Linxi Zhao", "Sofian Zalouk", "Christian K. Belardi", "Justin Lovelace", "Jin Peng Zhou", "Kilian Q. Weinberger", "Yoav Artzi", "Jennifer J. Sun"], "title": "Pre-training Large Memory Language Models with Internal and External Knowledge", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Neural language models are black-boxes -- both linguistic patterns and\nfactual knowledge are distributed across billions of opaque parameters. This\nentangled encoding makes it difficult to reliably inspect, verify, or update\nspecific facts. We propose a new class of language models, Large Memory\nLanguage Models (LMLM) with a pre-training recipe that stores factual knowledge\nin both internal weights and an external database. Our approach strategically\nmasks externally retrieved factual values from the training loss, thereby\nteaching the model to perform targeted lookups rather than relying on\nmemorization in model weights. Our experiments demonstrate that LMLMs achieve\ncompetitive performance compared to significantly larger, knowledge-dense LLMs\non standard benchmarks, while offering the advantages of explicit, editable,\nand verifiable knowledge bases. This work represents a fundamental shift in how\nlanguage models interact with and manage factual knowledge."}
{"id": "2505.16591", "pdf": "https://arxiv.org/pdf/2505.16591", "abs": "https://arxiv.org/abs/2505.16591", "authors": ["Bowen Jiang", "Runchuan Zhu", "Jiang Wu", "Zinco Jiang", "Yifan He", "Junyuan Gao", "Jia Yu", "Rui Min", "Yinfan Wang", "Haote Yang", "Songyang Zhang", "Dahua Lin", "Lijun Wu", "Conghui He"], "title": "Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering", "categories": ["cs.CL"], "comment": "Equal contribution: Bowen Jiang, Runchuan Zhu, Jiang Wu;\n  Corresponding author: Conghui He", "summary": "We introduce KoLasSimpleQA, the first benchmark evaluating the multilingual\nfactual ability of Large Language Models (LLMs). Inspired by existing research,\nwe created the question set with features such as single knowledge point\ncoverage, absolute objectivity, unique answers, and temporal stability. These\nquestions enable efficient evaluation using the LLM-as-judge paradigm, testing\nboth the LLMs' factual memory and self-awareness (\"know what they don't know\").\nKoLasSimpleQA expands existing research in two key dimensions: (1) Breadth\n(Multilingual Coverage): It includes 9 languages, supporting global\napplicability evaluation. (2) Depth (Dual Domain Design): It covers both the\ngeneral domain (global facts) and the language-specific domain (such as\nhistory, culture, and regional traditions) for a comprehensive assessment of\nmultilingual capabilities. We evaluated mainstream LLMs, including traditional\nLLM and emerging Large Reasoning Models. Results show significant performance\ndifferences between the two domains, particularly in performance metrics,\nranking, calibration, and robustness. This highlights the need for targeted\nevaluation and optimization in multilingual contexts. We hope KoLasSimpleQA\nwill help the research community better identify LLM capability boundaries in\nmultilingual contexts and provide guidance for model optimization. We will\nrelease KoLasSimpleQA at https://github.com/opendatalab/KoLasSimpleQA ."}
{"id": "2505.16732", "pdf": "https://arxiv.org/pdf/2505.16732", "abs": "https://arxiv.org/abs/2505.16732", "authors": ["Hany Abdulsamad", "Sahel Iqbal", "Simo Särkkä"], "title": "Sequential Monte Carlo for Policy Optimization in Continuous POMDPs", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Optimal decision-making under partial observability requires agents to\nbalance reducing uncertainty (exploration) against pursuing immediate\nobjectives (exploitation). In this paper, we introduce a novel policy\noptimization framework for continuous partially observable Markov decision\nprocesses (POMDPs) that explicitly addresses this challenge. Our method casts\npolicy learning as probabilistic inference in a non-Markovian Feynman--Kac\nmodel that inherently captures the value of information gathering by\nanticipating future observations, without requiring extrinsic exploration\nbonuses or handcrafted heuristics. To optimize policies under this model, we\ndevelop a nested sequential Monte Carlo~(SMC) algorithm that efficiently\nestimates a history-dependent policy gradient under samples from the optimal\ntrajectory distribution induced by the POMDP. We demonstrate the effectiveness\nof our algorithm across standard continuous POMDP benchmarks, where existing\nmethods struggle to act under uncertainty."}
{"id": "2505.15966", "pdf": "https://arxiv.org/pdf/2505.15966", "abs": "https://arxiv.org/abs/2505.15966", "authors": ["Alex Su", "Haozhe Wang", "Weimin Ren", "Fangzhen Lin", "Wenhu Chen"], "title": "Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Haozhe Wang and Alex Su contributed equally and listed alphabetically", "summary": "Chain-of-thought reasoning has significantly improved the performance of\nLarge Language Models (LLMs) across various domains. However, this reasoning\nprocess has been confined exclusively to textual space, limiting its\neffectiveness in visually intensive tasks. To address this limitation, we\nintroduce the concept of reasoning in the pixel-space. Within this novel\nframework, Vision-Language Models (VLMs) are equipped with a suite of visual\nreasoning operations, such as zoom-in and select-frame. These operations enable\nVLMs to directly inspect, interrogate, and infer from visual evidences, thereby\nenhancing reasoning fidelity for visual tasks. Cultivating such pixel-space\nreasoning capabilities in VLMs presents notable challenges, including the\nmodel's initially imbalanced competence and its reluctance to adopt the newly\nintroduced pixel-space operations. We address these challenges through a\ntwo-phase training approach. The first phase employs instruction tuning on\nsynthesized reasoning traces to familiarize the model with the novel visual\noperations. Following this, a reinforcement learning (RL) phase leverages a\ncuriosity-driven reward scheme to balance exploration between pixel-space\nreasoning and textual reasoning. With these visual operations, VLMs can\ninteract with complex visual inputs, such as information-rich images or videos\nto proactively gather necessary information. We demonstrate that this approach\nsignificantly improves VLM performance across diverse visual reasoning\nbenchmarks. Our 7B model, \\model, achieves 84\\% on V* bench, 74\\% on\nTallyQA-Complex, and 84\\% on InfographicsVQA, marking the highest accuracy\nachieved by any open-source model to date. These results highlight the\nimportance of pixel-space reasoning and the effectiveness of our framework."}
{"id": "2505.16592", "pdf": "https://arxiv.org/pdf/2505.16592", "abs": "https://arxiv.org/abs/2505.16592", "authors": ["Shijia Zhou", "Siyao Peng", "Simon Luebke", "Jörg Haßler", "Mario Haim", "Saif M. Mohammad", "Barbara Plank"], "title": "What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse", "categories": ["cs.CL", "cs.MM"], "comment": "19 pages, 9 figures", "summary": "Media framing refers to the emphasis on specific aspects of perceived reality\nto shape how an issue is defined and understood. Its primary purpose is to\nshape public perceptions often in alignment with the authors' opinions and\nstances. However, the interaction between stance and media frame remains\nlargely unexplored. In this work, we apply an interdisciplinary approach to\nconceptualize and computationally explore this interaction with internet memes\non climate change. We curate CLIMATEMEMES, the first dataset of climate-change\nmemes annotated with both stance and media frames, inspired by research in\ncommunication science. CLIMATEMEMES includes 1,184 memes sourced from 47\nsubreddits, enabling analysis of frame prominence over time and communities,\nand sheds light on the framing preferences of different stance holders. We\npropose two meme understanding tasks: stance detection and media frame\ndetection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the\ncorresponding results on their LLM backbone. Human captions consistently\nenhance performance. Synthetic captions and human-corrected OCR also help\noccasionally. Our findings highlight that VLMs perform well on stance, but\nstruggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs'\nlimitations in handling nuanced frames and stance expressions on climate change\ninternet memes."}
{"id": "2505.16733", "pdf": "https://arxiv.org/pdf/2505.16733", "abs": "https://arxiv.org/abs/2505.16733", "authors": ["Ziwei Luo", "Fredrik K. Gustafsson", "Jens Sjölund", "Thomas B. Schön"], "title": "Forward-only Diffusion Probabilistic Models", "categories": ["cs.LG"], "comment": "Project page: https://algolzw.github.io/fod", "summary": "This work presents a forward-only diffusion (FoD) approach for generative\nmodelling. In contrast to traditional diffusion models that rely on a coupled\nforward-backward diffusion scheme, FoD directly learns data generation through\na single forward diffusion process, yielding a simple yet efficient generative\nframework. The core of FoD is a state-dependent linear stochastic differential\nequation that involves a mean-reverting term in both the drift and diffusion\nfunctions. This mean-reversion property guarantees the convergence to clean\ndata, naturally simulating a stochastic interpolation between source and target\ndistributions. More importantly, FoD is analytically tractable and is trained\nusing a simple stochastic flow matching objective, enabling a few-step\nnon-Markov chain sampling during inference. The proposed FoD model, despite its\nsimplicity, achieves competitive performance on various image-conditioned\n(e.g., image restoration) and unconditional generation tasks, demonstrating its\neffectiveness in generative modelling. Our code is available at\nhttps://github.com/Algolzw/FoD."}
{"id": "2505.15997", "pdf": "https://arxiv.org/pdf/2505.15997", "abs": "https://arxiv.org/abs/2505.15997", "authors": ["Mehran Zoravar", "Shadi Alijani", "Homayoun Najjaran"], "title": "Domain Adaptive Skin Lesion Classification via Conformal Ensemble of Vision Transformers", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "5 pages, 4 figures, conference (ccece 2025)", "summary": "Exploring the trustworthiness of deep learning models is crucial, especially\nin critical domains such as medical imaging decision support systems. Conformal\nprediction has emerged as a rigorous means of providing deep learning models\nwith reliable uncertainty estimates and safety guarantees. However, conformal\nprediction results face challenges due to the backbone model's struggles in\ndomain-shifted scenarios, such as variations in different sources. To aim this\nchallenge, this paper proposes a novel framework termed Conformal Ensemble of\nVision Transformers (CE-ViTs) designed to enhance image classification\nperformance by prioritizing domain adaptation and model robustness, while\naccounting for uncertainty. The proposed method leverages an ensemble of vision\ntransformer models in the backbone, trained on diverse datasets including\nHAM10000, Dermofit, and Skin Cancer ISIC datasets. This ensemble learning\napproach, calibrated through the combined mentioned datasets, aims to enhance\ndomain adaptation through conformal learning. Experimental results underscore\nthat the framework achieves a high coverage rate of 90.38\\%, representing an\nimprovement of 9.95\\% compared to the HAM10000 model. This indicates a strong\nlikelihood that the prediction set includes the true label compared to singular\nmodels. Ensemble learning in CE-ViTs significantly improves conformal\nprediction performance, increasing the average prediction set size for\nchallenging misclassified samples from 1.86 to 3.075."}
{"id": "2505.16610", "pdf": "https://arxiv.org/pdf/2505.16610", "abs": "https://arxiv.org/abs/2505.16610", "authors": ["Jing Ye", "Lu Xiang", "Yaping Zhang", "Chengqing Zong"], "title": "From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment", "categories": ["cs.CL"], "comment": "27 pages", "summary": "Effective emotional support hinges on understanding users' emotions and needs\nto provide meaningful comfort during multi-turn interactions. Large Language\nModels (LLMs) show great potential for expressing empathy; however, they often\ndeliver generic and one-size-fits-all responses that fail to address users'\nspecific needs. To tackle this issue, we propose a self-evolution framework\ndesigned to help LLMs improve their responses to better align with users'\nimplicit preferences concerning user profiles (personalities), emotional\nstates, and specific situations. Our framework consists of two distinct phases:\n\\textit{(1)} \\textit{Emotional Support Experience Acquisition}, where LLMs are\nfine-tuned on limited emotional support conversation data to provide basic\nsupport, and \\textit{(2)} \\textit{Self-Improvement for Personalized Emotional\nSupport}, where LLMs leverage self-reflection and self-refinement to generate\npersonalized responses. Through iterative direct preference optimization\nbetween the pre- and post-refined responses, our model generates responses that\nreflect a better understanding of the user's implicit preferences. Extensive\nexperiments and evaluations demonstrate that our method significantly enhances\nthe model's performance in emotional support, reducing unhelpful responses and\nminimizing discrepancies between user preferences and model outputs."}
{"id": "2505.16734", "pdf": "https://arxiv.org/pdf/2505.16734", "abs": "https://arxiv.org/abs/2505.16734", "authors": ["Bang You", "Puze Liu", "Huaping Liu", "Jan Peters", "Oleg Arenz"], "title": "Maximum Total Correlation Reinforcement Learning", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Simplicity is a powerful inductive bias. In reinforcement learning,\nregularization is used for simpler policies, data augmentation for simpler\nrepresentations, and sparse reward functions for simpler objectives, all that,\nwith the underlying motivation to increase generalizability and robustness by\nfocusing on the essentials. Supplementary to these techniques, we investigate\nhow to promote simple behavior throughout the episode. To that end, we\nintroduce a modification of the reinforcement learning problem that\nadditionally maximizes the total correlation within the induced trajectories.\nWe propose a practical algorithm that optimizes all models, including policy\nand state representation, based on a lower-bound approximation. In simulated\nrobot environments, our method naturally generates policies that induce\nperiodic and compressible trajectories, and that exhibit superior robustness to\nnoise and changes in dynamics compared to baseline methods, while also\nimproving performance in the original tasks."}
{"id": "2505.16000", "pdf": "https://arxiv.org/pdf/2505.16000", "abs": "https://arxiv.org/abs/2505.16000", "authors": ["Mehrdad ghassabi", "Pedram Rostami", "Hamidreza Baradaran Kashani", "Amirhossein Poursina", "Zahra Kazemi", "Milad Tavakoli"], "title": "Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 4 figures", "summary": "The rapid advancement of language models has demonstrated the potential of\nartificial intelligence in the healthcare industry. However, small language\nmodels struggle with specialized domains in low-resource languages like\nPersian. While numerous medical-domain websites exist in Persian, no curated\ndataset or corpus has been available making ours the first of its kind. This\nstudy explores the enhancement of medical knowledge in a small language model\nby leveraging accessible online data, including a crawled corpus from medical\nmagazines and a dataset of real doctor-patient QA pairs. We fine-tuned a\nbaseline model using our curated data to improve its medical knowledge.\nBenchmark evaluations demonstrate that the fine-tuned model achieves improved\naccuracy in medical question answering and provides better responses compared\nto its baseline. This work highlights the potential of leveraging open-access\nonline data to enrich small language models in medical fields, providing a\nnovel solution for Persian medical AI applications suitable for\nresource-constrained environments."}
{"id": "2505.16612", "pdf": "https://arxiv.org/pdf/2505.16612", "abs": "https://arxiv.org/abs/2505.16612", "authors": ["Daniel Scalena", "Gabriele Sarti", "Arianna Bisazza", "Elisabetta Fersini", "Malvina Nissim"], "title": "Steering Large Language Models for Machine Translation Personalization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "High-quality machine translation systems based on large language models\n(LLMs) have simplified the production of personalized translations reflecting\nspecific stylistic constraints. However, these systems still struggle in\nsettings where stylistic requirements are less explicit and might be harder to\nconvey via prompting. We explore various strategies for personalizing\nLLM-generated translations in low-resource settings, focusing on the\nchallenging literary translation domain. We explore prompting strategies and\ninference-time interventions for steering model generations towards a\npersonalized style, and propose a contrastive framework exploiting latent\nconcepts extracted from sparse autoencoders to identify salient personalization\nproperties. Our results show that steering achieves strong personalization\nwhile preserving translation quality. We further examine the impact of steering\non LLM representations, finding model layers with a relevant impact for\npersonalization are impacted similarly by multi-shot prompting and our steering\nmethod, suggesting similar mechanism at play."}
{"id": "2505.16736", "pdf": "https://arxiv.org/pdf/2505.16736", "abs": "https://arxiv.org/abs/2505.16736", "authors": ["Nicolas Keriven"], "title": "Backward Oversmoothing: why is it hard to train deep Graph Neural Networks?", "categories": ["cs.LG"], "comment": null, "summary": "Oversmoothing has long been identified as a major limitation of Graph Neural\nNetworks (GNNs): input node features are smoothed at each layer and converge to\na non-informative representation, if the weights of the GNN are sufficiently\nbounded. This assumption is crucial: if, on the contrary, the weights are\nsufficiently large, then oversmoothing may not happen. Theoretically, GNN could\nthus learn to not oversmooth. However it does not really happen in practice,\nwhich prompts us to examine oversmoothing from an optimization point of view.\nIn this paper, we analyze backward oversmoothing, that is, the notion that\nbackpropagated errors used to compute gradients are also subject to\noversmoothing from output to input. With non-linear activation functions, we\noutline the key role of the interaction between forward and backward smoothing.\nMoreover, we show that, due to backward oversmoothing, GNNs provably exhibit\nmany spurious stationary points: as soon as the last layer is trained, the\nwhole GNN is at a stationary point. As a result, we can exhibit regions where\ngradients are near-zero while the loss stays high. The proof relies on the fact\nthat, unlike forward oversmoothing, backward errors are subjected to a linear\noversmoothing even in the presence of non-linear activation function, such that\nthe average of the output error plays a key role. Additionally, we show that\nthis phenomenon is specific to deep GNNs, and exhibit counter-example\nMulti-Layer Perceptron. This paper is a step toward a more complete\ncomprehension of the optimization landscape specific to GNNs."}
{"id": "2505.16002", "pdf": "https://arxiv.org/pdf/2505.16002", "abs": "https://arxiv.org/abs/2505.16002", "authors": ["Sasha Boguraev", "Christopher Potts", "Kyle Mahowald"], "title": "Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 19 figures, 11 tables", "summary": "Large Language Models (LLMs) have emerged as powerful sources of evidence for\nlinguists seeking to develop theories of syntax. In this paper, we argue that\ncausal interpretability methods, applied to LLMs, can greatly enhance the value\nof such evidence by helping us characterize the abstract mechanisms that LLMs\nlearn to use. Our empirical focus is a set of English filler-gap dependency\nconstructions (e.g., questions, relative clauses). Linguistic theories largely\nagree that these constructions share many properties. Using experiments based\nin Distributed Interchange Interventions, we show that LLMs converge on similar\nabstract analyses of these constructions. These analyses also reveal previously\noverlooked factors -- relating to frequency, filler type, and surrounding\ncontext -- that could motivate changes to standard linguistic theory. Overall,\nthese results suggest that mechanistic, internal analyses of LLMs can push\nlinguistic theory forward."}
{"id": "2505.16637", "pdf": "https://arxiv.org/pdf/2505.16637", "abs": "https://arxiv.org/abs/2505.16637", "authors": ["Wenjie Yang", "Mao Zheng", "Mingyang Song", "Zheng Li"], "title": "SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities in machine translation (MT). However, most advanced MT-specific\nLLMs heavily rely on external supervision signals during training, such as\nhuman-annotated reference data or trained reward models (RMs), which are often\nexpensive to obtain and challenging to scale. To overcome this limitation, we\npropose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for\nMT that is reference-free, fully online, and relies solely on self-judging\nrewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as\nthe backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,\ne.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like\nQwen2.5-32B-Instruct in English $\\leftrightarrow$ Chinese translation tasks\nfrom WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR\nwith external supervision from COMET, our strongest model, SSR-X-Zero-7B,\nachieves state-of-the-art performance in English $\\leftrightarrow$ Chinese\ntranslation, surpassing all existing open-source models under 72B parameters\nand even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.\nOur analysis highlights the effectiveness of the self-rewarding mechanism\ncompared to the external LLM-as-a-judge approach in MT and demonstrates its\ncomplementary benefits when combined with trained RMs. Our findings provide\nvaluable insight into the potential of self-improving RL methods. We have\npublicly released our code, data and models."}
{"id": "2505.16737", "pdf": "https://arxiv.org/pdf/2505.16737", "abs": "https://arxiv.org/abs/2505.16737", "authors": ["Chengcan Wu", "Zhixin Zhang", "Zeming Wei", "Yihao Zhang", "Meng Sun"], "title": "Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "math.OC"], "comment": null, "summary": "The significant progress of large language models (LLMs) has led to\nremarkable achievements across numerous applications. However, their ability to\ngenerate harmful content has sparked substantial safety concerns. Despite the\nimplementation of safety alignment techniques during the pre-training phase,\nrecent research indicates that fine-tuning LLMs on adversarial or even benign\ndata can inadvertently compromise their safety. In this paper, we re-examine\nthe fundamental issue of why fine-tuning on non-harmful data still results in\nsafety degradation. We introduce a safety-aware probing (SAP) optimization\nframework designed to mitigate the safety risks of fine-tuning LLMs.\nSpecifically, SAP incorporates a safety-aware probe into the gradient\npropagation process, mitigating the model's risk of safety degradation by\nidentifying potential pitfalls in gradient directions, thereby enhancing\ntask-specific performance while successfully preserving model safety. Our\nextensive experimental results demonstrate that SAP effectively reduces\nharmfulness below the original fine-tuned model and achieves comparable test\nloss to standard fine-tuning methods. Our code is available at\nhttps://github.com/ChengcanWu/SAP."}
{"id": "2505.16003", "pdf": "https://arxiv.org/pdf/2505.16003", "abs": "https://arxiv.org/abs/2505.16003", "authors": ["Roland Daynauth", "Christopher Clarke", "Krisztian Flautner", "Lingjia Tang", "Jason Mars"], "title": "SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The LLM-as-a-Judge paradigm offers a scalable, reference-free approach for\nevaluating language models. Although several calibration techniques have been\nproposed to better align these evaluators with human judgment, prior studies\nfocus primarily on narrow, well-structured benchmarks. As a result, it remains\nunclear whether such calibrations generalize to real-world, open-ended tasks.\n  In this work, we show that SOTA calibrated evaluators often fail in these\nsettings, exhibiting weak or even negative correlation with human judgments. To\naddress this, we propose SLMEval, a novel and efficient calibration method\nbased on entropy maximization over a small amount of human preference data. By\nestimating a latent distribution over model quality and reweighting evaluator\nscores accordingly, SLMEval achieves strong correlation with human evaluations\nacross two real-world production use cases and the public benchmark. For\nexample, on one such task, SLMEval achieves a Spearman correlation of 0.57 with\nhuman judgments, while G-Eval yields a negative correlation. In addition,\nSLMEval reduces evaluation costs by 5-30x compared to GPT-4-based calibrated\nevaluators such as G-eval."}
{"id": "2505.16648", "pdf": "https://arxiv.org/pdf/2505.16648", "abs": "https://arxiv.org/abs/2505.16648", "authors": ["Kexin Shang", "Chia-Hsuan Chang", "Christopher C. Yang"], "title": "Collaboration among Multiple Large Language Models for Medical Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to IEEE International Conference on Healthcare Informatics\n  2025", "summary": "Empowered by vast internal knowledge reservoir, the new generation of large\nlanguage models (LLMs) demonstrate untapped potential to tackle medical tasks.\nHowever, there is insufficient effort made towards summoning up a synergic\neffect from multiple LLMs' expertise and background. In this study, we propose\na multi-LLM collaboration framework tailored on a medical multiple-choice\nquestions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,\nour framework is proved to boost all LLMs reasoning ability as well as\nalleviate their divergence among questions. We also measure an LLM's confidence\nwhen it confronts with adversary opinions from other LLMs and observe a\nconcurrence between LLM's confidence and prediction accuracy."}
{"id": "2505.16741", "pdf": "https://arxiv.org/pdf/2505.16741", "abs": "https://arxiv.org/abs/2505.16741", "authors": ["Pilhwa Lee", "Shashank Gupta"], "title": "Meta-reinforcement learning with minimum attention", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": "10 pages, 7 figures", "summary": "Minimum attention applies the least action principle in the changes of\ncontrol concerning state and time, first proposed by Brockett. The involved\nregularization is highly relevant in emulating biological control, such as\nmotor learning. We apply minimum attention in reinforcement learning (RL) as\npart of the rewards and investigate its connection to meta-learning and\nstabilization. Specifically, model-based meta-learning with minimum attention\nis explored in high-dimensional nonlinear dynamics. Ensemble-based model\nlearning and gradient-based meta-policy learning are alternately performed.\nEmpirically, we show that the minimum attention does show outperforming\ncompetence in comparison to the state-of-the-art algorithms in model-free and\nmodel-based RL, i.e., fast adaptation in few shots and variance reduction from\nthe perturbations of the model and environment. Furthermore, the minimum\nattention demonstrates the improvement in energy efficiency."}
{"id": "2505.16004", "pdf": "https://arxiv.org/pdf/2505.16004", "abs": "https://arxiv.org/abs/2505.16004", "authors": ["Aaron J. Li", "Suraj Srinivas", "Usha Bhalla", "Himabindu Lakkaraju"], "title": "Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse autoencoders (SAEs) are commonly used to interpret the internal\nactivations of large language models (LLMs) by mapping them to\nhuman-interpretable concept representations. While existing evaluations of SAEs\nfocus on metrics such as the reconstruction-sparsity tradeoff, human\n(auto-)interpretability, and feature disentanglement, they overlook a critical\naspect: the robustness of concept representations to input perturbations. We\nargue that robustness must be a fundamental consideration for concept\nrepresentations, reflecting the fidelity of concept labeling. To this end, we\nformulate robustness quantification as input-space optimization problems and\ndevelop a comprehensive evaluation framework featuring realistic scenarios in\nwhich adversarial perturbations are crafted to manipulate SAE representations.\nEmpirically, we find that tiny adversarial input perturbations can effectively\nmanipulate concept-based interpretations in most scenarios without notably\naffecting the outputs of the base LLMs themselves. Overall, our results suggest\nthat SAE concept representations are fragile and may be ill-suited for\napplications in model monitoring and oversight."}
{"id": "2505.16660", "pdf": "https://arxiv.org/pdf/2505.16660", "abs": "https://arxiv.org/abs/2505.16660", "authors": ["Liu Chang", "Wang Dongbo", "Liu liu", "Zhao Zhixiao"], "title": "Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu", "categories": ["cs.CL", "cs.AI"], "comment": "29pages, 7 figures", "summary": "This study addresses the challenges in intelligent processing of Chinese\nancient mathematical classics by constructing Guji_MATH, a benchmark for\nevaluating classical texts based on Suanjing Shishu. It systematically assesses\nthe mathematical problem-solving capabilities of mainstream reasoning models\nunder the unique linguistic constraints of classical Chinese. Through\nmachine-assisted annotation and manual verification, 538 mathematical problems\nwere extracted from 8 canonical texts, forming a structured dataset centered on\nthe \"Question-Answer-Solution\" framework, supplemented by problem types and\ndifficulty levels. Dual evaluation modes--closed-book (autonomous\nproblem-solving) and open-book (reproducing classical solution methods)--were\ndesigned to evaluate the performance of six reasoning models on ancient Chinese\nmathematical problems. Results indicate that reasoning models can partially\ncomprehend and solve these problems, yet their overall performance remains\ninferior to benchmarks on modern mathematical tasks. Enhancing models'\nclassical Chinese comprehension and cultural knowledge should be prioritized\nfor optimization. This study provides methodological support for mining\nmathematical knowledge from ancient texts and disseminating traditional\nculture, while offering new perspectives for evaluating cross-linguistic and\ncross-cultural capabilities of reasoning models."}
{"id": "2505.16748", "pdf": "https://arxiv.org/pdf/2505.16748", "abs": "https://arxiv.org/abs/2505.16748", "authors": ["Julien Laasri", "Marc Revol"], "title": "Revenue Optimization with Price-Sensitive and Interdependent Demand", "categories": ["cs.LG", "math.OC", "90C59", "G.1.6"], "comment": "21 pages, 17 figures, dated 2018, in French", "summary": "As Kalyan T. Talluri and Garrett J. Van Ryzin describe in their work [3],\nRevenue Management aims to maximize an organization's revenue by considering\nthree types of decision categories: structural, pricing, and quantity. In this\ndocument, our primary focus will be on decisions related to pricing and\nquantity for the sale of airline tickets on a direct flight over a certain\nnumber of time periods. More specifically, we will only focus on the\noptimization aspect of this problem. We will assume the demand data to be\ngiven, since Air France estimates it beforehand using real data. Similarly, we\nassume all price options to be predetermined by Air France's algorithms and\nverified by their analysts. Our objective will be to maximize the revenue of a\ndirect flight by choosing the prices for each product from the predefined set\nof options.\n  --\n  Comme d\\'ecrit par Kalyan T. Talluri et Garrett J. Van Ryzin dans leur\nouvrage [3], le Revenue Management consiste en la maximisation du revenu d'un\norganisme \\`a partir de trois types de cat\\'egories de d\\'ecision :\nstructurelles, prix et quantit\\'e. Dans ce document, nous nous int\\'eresserons\nprincipalement aux d\\'ecisions de type prix et quantit\\'e pour la vente de\nbillets d'avion sur un vol direct au cours d'un certain nombre de pas de temps.\nPlus pr\\'ecis\\'ement, nous nous situerons dans la partie optimisation du\nprobl\\`eme. Nous prendrons ainsi les donn\\'ees de demande comme acquises, car\nelles sont estim\\'ees au pr\\'ealable par Air France \\`a partir des donn\\'ees\nr\\'eelles. De m\\^eme, pour chaque produit que l'on cherchera \\`a vendre, on\nnous impose en amont les prix possibles que l'on a droit d'utiliser et qui se\nbasent sur des algorithmes d'Air France dont les r\\'esultats sont v\\'erifi\\'es\npar des analystes. Notre but sera alors de maximiser le revenu d'un vol direct\nen choisissant les prix de chaque produit parmi ceux impos\\'es."}
{"id": "2505.16008", "pdf": "https://arxiv.org/pdf/2505.16008", "abs": "https://arxiv.org/abs/2505.16008", "authors": ["Wenrui Yu", "Yiyi Chen", "Johannes Bjerva", "Sokol Kosta", "Qiongxiu Li"], "title": "LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "We propose LAGO - Language Similarity-Aware Graph Optimization - a novel\napproach for few-shot cross-lingual embedding inversion attacks, addressing\ncritical privacy vulnerabilities in multilingual NLP systems. Unlike prior work\nin embedding inversion attacks that treat languages independently, LAGO\nexplicitly models linguistic relationships through a graph-based constrained\ndistributed optimization framework. By integrating syntactic and lexical\nsimilarity as edge constraints, our method enables collaborative parameter\nlearning across related languages. Theoretically, we show this formulation\ngeneralizes prior approaches, such as ALGEN, which emerges as a special case\nwhen similarity constraints are relaxed. Our framework uniquely combines\nFrobenius-norm regularization with linear inequality or total variation\nconstraints, ensuring robust alignment of cross-lingual embedding spaces even\nwith extremely limited data (as few as 10 samples per language). Extensive\nexperiments across multiple languages and embedding models demonstrate that\nLAGO substantially improves the transferability of attacks with 10-20% increase\nin Rouge-L score over baselines. This work establishes language similarity as a\ncritical factor in inversion attack transferability, urging renewed focus on\nlanguage-aware privacy-preserving multilingual embeddings."}
{"id": "2505.16661", "pdf": "https://arxiv.org/pdf/2505.16661", "abs": "https://arxiv.org/abs/2505.16661", "authors": ["Issey Sukeda", "Takuro Fujii", "Kosei Buma", "Shunsuke Sasaki", "Shinnosuke Ono"], "title": "A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP", "categories": ["cs.CL"], "comment": "15 pages, 9 tables, 5 figures", "summary": "We present a Japanese domain-specific language model for the pharmaceutical\nfield, developed through continual pretraining on 2 billion Japanese\npharmaceutical tokens and 8 billion English biomedical tokens. To enable\nrigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on\nnational pharmacist licensing exams; NayoseQA, which tests cross-lingual\nsynonym and terminology normalization; and SogoCheck, a novel task designed to\nassess consistency reasoning between paired statements. We evaluate our model\nagainst both open-source medical LLMs and commercial models, including GPT-4o.\nResults show that our domain-specific model outperforms existing open models\nand achieves competitive performance with commercial ones, particularly on\nterminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o\nperforms poorly on SogoCheck, suggesting that cross-sentence consistency\nreasoning remains an open challenge. Our benchmark suite offers a broader\ndiagnostic lens for pharmaceutical NLP, covering factual recall, lexical\nvariation, and logical consistency. This work demonstrates the feasibility of\nbuilding practical, secure, and cost-effective language models for Japanese\ndomain-specific applications, and provides reusable evaluation resources for\nfuture research in pharmaceutical and healthcare NLP. Our model, codes, and\ndatasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval."}
{"id": "2505.16754", "pdf": "https://arxiv.org/pdf/2505.16754", "abs": "https://arxiv.org/abs/2505.16754", "authors": ["Hannah Markgraf", "Michael Eichelbeck", "Daria Cappey", "Selin Demirtürk", "Yara Schattschneider", "Matthias Althoff"], "title": "PyTupli: A Scalable Infrastructure for Collaborative Offline Reinforcement Learning Projects", "categories": ["cs.LG"], "comment": null, "summary": "Offline reinforcement learning (RL) has gained traction as a powerful\nparadigm for learning control policies from pre-collected data, eliminating the\nneed for costly or risky online interactions. While many open-source libraries\noffer robust implementations of offline RL algorithms, they all rely on\ndatasets composed of experience tuples consisting of state, action, next state,\nand reward. Managing, curating, and distributing such datasets requires\nsuitable infrastructure. Although static datasets exist for established\nbenchmark problems, no standardized or scalable solution supports developing\nand sharing datasets for novel or user-defined benchmarks. To address this gap,\nwe introduce PyTupli, a Python-based tool to streamline the creation, storage,\nand dissemination of benchmark environments and their corresponding tuple\ndatasets. PyTupli includes a lightweight client library with defined interfaces\nfor uploading and retrieving benchmarks and data. It supports fine-grained\nfiltering at both the episode and tuple level, allowing researchers to curate\nhigh-quality, task-specific datasets. A containerized server component enables\nproduction-ready deployment with authentication, access control, and automated\ncertificate provisioning for secure use. By addressing key barriers in dataset\ninfrastructure, PyTupli facilitates more collaborative, reproducible, and\nscalable offline RL research."}
{"id": "2505.16022", "pdf": "https://arxiv.org/pdf/2505.16022", "abs": "https://arxiv.org/abs/2505.16022", "authors": ["Wei Liu", "Siya Qi", "Xinyu Wang", "Chen Qian", "Yali Du", "Yulan He"], "title": "NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "20 pages, 5 tables, 12 figures", "summary": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of\nincentive training, a reinforcement learning paradigm that computes rewards\nsolely based on the final answer part of a language model's output, thereby\nencouraging the generation of intermediate reasoning steps. However, these\nmethods fundamentally rely on external verifiers, which limits their\napplicability to domains like mathematics and coding where such verifiers are\nreadily available. Although reward models can serve as verifiers, they require\nhigh-quality annotated data and are costly to train. In this work, we propose\nNOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning\nframework that requires only standard supervised fine-tuning data with no need\nfor an external verifier. NOVER enables incentive training across a wide range\nof text-to-text tasks and outperforms the model of the same size distilled from\nlarge reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the\nflexibility of NOVER enables new possibilities for optimizing large language\nmodels, such as inverse incentive training."}
{"id": "2505.16694", "pdf": "https://arxiv.org/pdf/2505.16694", "abs": "https://arxiv.org/abs/2505.16694", "authors": ["Gouki Minegishi", "Hiroki Furuta", "Shohei Taniguchi", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Transformer-based language models exhibit In-Context Learning (ICL), where\npredictions are made adaptively based on context. While prior work links\ninduction heads to ICL through a sudden jump in accuracy, this can only account\nfor ICL when the answer is included within the context. However, an important\nproperty of practical ICL in large language models is the ability to meta-learn\nhow to solve tasks from context, rather than just copying answers from context;\nhow such an ability is obtained during training is largely unexplored. In this\npaper, we experimentally clarify how such meta-learning ability is acquired by\nanalyzing the dynamics of the model's circuit during training. Specifically, we\nextend the copy task from previous research into an In-Context Meta Learning\nsetting, where models must infer a task from examples to answer queries.\nInterestingly, in this setting, we find that there are multiple phases in the\nprocess of acquiring such abilities, and that a unique circuit emerges in each\nphase, contrasting with the single-phases change in induction heads. The\nemergence of such circuits can be related to several phenomena known in large\nlanguage models, and our analysis lead to a deeper understanding of the source\nof the transformer's ICL ability."}
{"id": "2505.16755", "pdf": "https://arxiv.org/pdf/2505.16755", "abs": "https://arxiv.org/abs/2505.16755", "authors": ["Ayano Nakai-Kasai", "Tadashi Wadayama"], "title": "Multi-Output Gaussian Processes for Graph-Structured Data", "categories": ["cs.LG"], "comment": null, "summary": "Graph-structured data is a type of data to be obtained associated with a\ngraph structure where vertices and edges describe some kind of data\ncorrelation. This paper proposes a regression method on graph-structured data,\nwhich is based on multi-output Gaussian processes (MOGP), to capture both the\ncorrelation between vertices and the correlation between associated data. The\nproposed formulation is built on the definition of MOGP. This allows it to be\napplied to a wide range of data configurations and scenarios. Moreover, it has\nhigh expressive capability due to its flexibility in kernel design. It includes\nexisting methods of Gaussian processes for graph-structured data as special\ncases and is possible to remove restrictions on data configurations, model\nselection, and inference scenarios in the existing methods. The performance of\nextensions achievable by the proposed formulation is evaluated through computer\nexperiments with synthetic and real data."}
{"id": "2505.16024", "pdf": "https://arxiv.org/pdf/2505.16024", "abs": "https://arxiv.org/abs/2505.16024", "authors": ["Weiguo Gao", "Ming Li"], "title": "Toward Theoretical Insights into Diffusion Trajectory Distillation via Operator Merging", "categories": ["cs.LG", "cs.AI"], "comment": "31 pages, 19 figures", "summary": "Diffusion trajectory distillation methods aim to accelerate sampling in\ndiffusion models, which produce high-quality outputs but suffer from slow\nsampling speeds. These methods train a student model to approximate the\nmulti-step denoising process of a pretrained teacher model in a single step,\nenabling one-shot generation. However, theoretical insights into the trade-off\nbetween different distillation strategies and generative quality remain\nlimited, complicating their optimization and selection. In this work, we take a\nfirst step toward addressing this gap. Specifically, we reinterpret trajectory\ndistillation as an operator merging problem in the linear regime, where each\nstep of the teacher model is represented as a linear operator acting on noisy\ndata. These operators admit a clear geometric interpretation as projections and\nrescalings corresponding to the noise schedule. During merging, signal\nshrinkage occurs as a convex combination of operators, arising from both\ndiscretization and limited optimization time of the student model. We propose a\ndynamic programming algorithm to compute the optimal merging strategy that\nmaximally preserves signal fidelity. Additionally, we demonstrate the existence\nof a sharp phase transition in the optimal strategy, governed by data\ncovariance structures. Our findings enhance the theoretical understanding of\ndiffusion trajectory distillation and offer practical insights for improving\ndistillation strategies."}
{"id": "2505.16703", "pdf": "https://arxiv.org/pdf/2505.16703", "abs": "https://arxiv.org/abs/2505.16703", "authors": ["Zeping Yu", "Sophia Ananiadou"], "title": "Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Although multimodal large language models (MLLMs) have achieved impressive\nperformance, the multimodal instruction tuning stage often causes catastrophic\nforgetting of the base LLM's language ability, even in strong models like\nLlama3. To address this, we propose Locate-then-Merge, a training-free\nparameter fusion framework that first locates important parameters and then\nselectively merges them. We further introduce Neuron-Fusion, a neuron-level\nstrategy that preserves the influence of neurons with large parameter\nshifts--neurons likely responsible for newly acquired visual\ncapabilities--while attenuating the influence of neurons with smaller changes\nthat likely encode general-purpose language skills. This design enables better\nretention of visual adaptation while mitigating language degradation.\nExperiments on 13 benchmarks across both language and visual tasks show that\nNeuron-Fusion consistently outperforms existing model merging methods. Further\nanalysis reveals that our method effectively reduces context hallucination in\ngeneration."}
{"id": "2505.16786", "pdf": "https://arxiv.org/pdf/2505.16786", "abs": "https://arxiv.org/abs/2505.16786", "authors": ["Fares B. Mehouachi", "Saif Eddin Jabari"], "title": "FlowMixer: A Constrained Neural Architecture for Interpretable Spatiotemporal Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "We introduce FlowMixer, a neural architecture that leverages constrained\nmatrix operations to model structured spatiotemporal patterns. At its core,\nFlowMixer incorporates non-negative matrix mixing layers within a reversible\nmapping framework-applying transforms before mixing and their inverses\nafterward. This shape-preserving design enables a Kronecker-Koopman eigenmode\nframework that bridges statistical learning with dynamical systems theory,\nproviding interpretable spatiotemporal patterns and facilitating direct\nalgebraic manipulation of prediction horizons without retraining. Extensive\nexperiments across diverse domains demonstrate FlowMixer's robust long-horizon\nforecasting capabilities while effectively modeling physical phenomena such as\nchaotic attractors and turbulent flows. These results suggest that\narchitectural constraints can simultaneously enhance predictive performance and\nmathematical interpretability in neural forecasting systems."}
{"id": "2505.16027", "pdf": "https://arxiv.org/pdf/2505.16027", "abs": "https://arxiv.org/abs/2505.16027", "authors": ["Qinmei Xu", "Yiheng Li", "Xianghao Zhan", "Ahmet Gorkem Er", "Brittany Dashevsky", "Chuanjun Xu", "Mohammed Alawad", "Mengya Yang", "Liu Ya", "Changsheng Zhou", "Xiao Li", "Haruka Itakura", "Olivier Gevaert"], "title": "Benchmarking Chest X-ray Diagnosis Models Across Multinational Datasets", "categories": ["eess.IV", "cs.AI", "cs.CV", "I.2"], "comment": "78 pages, 7 figures, 2 tabeles", "summary": "Foundation models leveraging vision-language pretraining have shown promise\nin chest X-ray (CXR) interpretation, yet their real-world performance across\ndiverse populations and diagnostic tasks remains insufficiently evaluated. This\nstudy benchmarks the diagnostic performance and generalizability of foundation\nmodels versus traditional convolutional neural networks (CNNs) on multinational\nCXR datasets. We evaluated eight CXR diagnostic models - five vision-language\nfoundation models and three CNN-based architectures - across 37 standardized\nclassification tasks using six public datasets from the USA, Spain, India, and\nVietnam, and three private datasets from hospitals in China. Performance was\nassessed using AUROC, AUPRC, and other metrics across both shared and\ndataset-specific tasks. Foundation models outperformed CNNs in both accuracy\nand task coverage. MAVL, a model incorporating knowledge-enhanced prompts and\nstructured supervision, achieved the highest performance on public (mean AUROC:\n0.82; AUPRC: 0.32) and private (mean AUROC: 0.95; AUPRC: 0.89) datasets,\nranking first in 14 of 37 public and 3 of 4 private tasks. All models showed\nreduced performance on pediatric cases, with average AUROC dropping from 0.88\n+/- 0.18 in adults to 0.57 +/- 0.29 in children (p = 0.0202). These findings\nhighlight the value of structured supervision and prompt design in radiologic\nAI and suggest future directions including geographic expansion and ensemble\nmodeling for clinical deployment. Code for all evaluated models is available at\nhttps://drive.google.com/drive/folders/1B99yMQm7bB4h1sVMIBja0RfUu8gLktCE"}
{"id": "2505.16722", "pdf": "https://arxiv.org/pdf/2505.16722", "abs": "https://arxiv.org/abs/2505.16722", "authors": ["Himanshu Beniwal", "Youngwoo Kim", "Maarten Sap", "Soham Dan", "Thomas Hartvigsen"], "title": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 504 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad."}
{"id": "2505.16790", "pdf": "https://arxiv.org/pdf/2505.16790", "abs": "https://arxiv.org/abs/2505.16790", "authors": ["Hyunjin Seo", "Taewon Kim", "Sihyun Yu", "SungSoo Ahn"], "title": "Learning Flexible Forward Trajectories for Masked Molecular Diffusion", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Masked diffusion models (MDMs) have achieved notable progress in modeling\ndiscrete data, while their potential in molecular generation remains\nunderexplored. In this work, we explore their potential and introduce the\nsurprising result that naively applying standards MDMs severely degrades the\nperformance. We identify the critical cause of this issue as a state-clashing\nproblem-where the forward diffusion of distinct molecules collapse into a\ncommon state, resulting in a mixture of reconstruction targets that cannot be\nlearned using typical reverse diffusion process with unimodal predictions. To\nmitigate this, we propose Masked Element-wise Learnable Diffusion (MELD) that\norchestrates per-element corruption trajectories to avoid collision between\ndistinct molecular graphs. This is achieved through a parameterized noise\nscheduling network that assigns distinct corruption rates to individual graph\nelements, i.e., atoms and bonds. Extensive experiments on diverse molecular\nbenchmarks reveal that MELD markedly enhances overall generation quality\ncompared to element-agnostic noise scheduling, increasing the chemical validity\nof vanilla MDMs on ZINC250K from 15% to 93%, Furthermore, it achieves\nstate-of-the-art property alignment in conditional generation tasks."}
{"id": "2505.16035", "pdf": "https://arxiv.org/pdf/2505.16035", "abs": "https://arxiv.org/abs/2505.16035", "authors": ["Alejandro García-Castellanos", "David R. Wessels", "Nicky J. van den Berg", "Remco Duits", "Daniël M. Pelt", "Erik J. Bekkers"], "title": "Equivariant Eikonal Neural Networks: Grid-Free, Scalable Travel-Time Prediction on Homogeneous Spaces", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce Equivariant Neural Eikonal Solvers, a novel framework that\nintegrates Equivariant Neural Fields (ENFs) with Neural Eikonal Solvers. Our\napproach employs a single neural field where a unified shared backbone is\nconditioned on signal-specific latent variables - represented as point clouds\nin a Lie group - to model diverse Eikonal solutions. The ENF integration\nensures equivariant mapping from these latent representations to the solution\nfield, delivering three key benefits: enhanced representation efficiency\nthrough weight-sharing, robust geometric grounding, and solution steerability.\nThis steerability allows transformations applied to the latent point cloud to\ninduce predictable, geometrically meaningful modifications in the resulting\nEikonal solution. By coupling these steerable representations with\nPhysics-Informed Neural Networks (PINNs), our framework accurately models\nEikonal travel-time solutions while generalizing to arbitrary Riemannian\nmanifolds with regular group actions. This includes homogeneous spaces such as\nEuclidean, position-orientation, spherical, and hyperbolic manifolds. We\nvalidate our approach through applications in seismic travel-time modeling of\n2D and 3D benchmark datasets. Experimental results demonstrate superior\nperformance, scalability, adaptability, and user controllability compared to\nexisting Neural Operator-based Eikonal solver methods."}
{"id": "2505.16743", "pdf": "https://arxiv.org/pdf/2505.16743", "abs": "https://arxiv.org/abs/2505.16743", "authors": ["Florentin Beck", "William Rudman", "Carsten Eickhoff"], "title": "TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; F.2.2"], "comment": null, "summary": "Large Language Models (LLMs) present significant computational and memory\nchallenges due to their extensive size, making pruning essential for their\nefficient deployment. Existing one-shot pruning methods often apply uniform\nsparsity constraints across layers or within each layer, resulting in\nsuboptimal performance, especially at high sparsity ratios. This work\nintroduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel\napproach that applies varying sparsity ratios to individual output dimensions\n(rows) within each layer. TRIM employs an iterative adjustment process guided\nby quality metrics to optimize dimension-wise sparsity allocation, focusing on\nreducing variance in quality retention across outputs to preserve critical\ninformation. TRIM can be seamlessly integrated with existing layer-wise pruning\nstrategies. Our evaluations on perplexity and zero-shot tasks across diverse\nLLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that\nTRIM achieves new state-of-the-art results and enhances stability. For\ninstance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and\nover 90% for OPT-13B compared to baseline methods. We conclude that\nfine-grained, dimension-wise sparsity adaptation is crucial for pushing the\nlimits of extreme LLM compression. Code available at:\nhttps://github.com/flobk/TRIM"}
{"id": "2505.16791", "pdf": "https://arxiv.org/pdf/2505.16791", "abs": "https://arxiv.org/abs/2505.16791", "authors": ["Tillmann Rheude", "Roland Eils", "Benjamin Wild"], "title": "Cohort-Based Active Modality Acquisition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Real-world machine learning applications often involve data from multiple\nmodalities that must be integrated effectively to make robust predictions.\nHowever, in many practical settings, not all modalities are available for every\nsample, and acquiring additional modalities can be costly. This raises the\nquestion: which samples should be prioritized for additional modality\nacquisition when resources are limited? While prior work has explored\nindividual-level acquisition strategies and training-time active learning\nparadigms, test-time and cohort-based acquisition remain underexplored despite\ntheir importance in many real-world settings. We introduce Cohort-based Active\nModality Acquisition (CAMA), a novel test-time setting to formalize the\nchallenge of selecting which samples should receive additional modalities. We\nderive acquisition strategies that leverage a combination of generative\nimputation and discriminative modeling to estimate the expected benefit of\nacquiring missing modalities based on common evaluation metrics. We also\nintroduce upper-bound heuristics that provide performance ceilings to benchmark\nacquisition strategies. Experiments on common multimodal datasets demonstrate\nthat our proposed imputation-based strategies can more effectively guide the\nacquisition of new samples in comparison to those relying solely on unimodal\ninformation, entropy guidance, and random selections. Our work provides an\neffective solution for optimizing modality acquisition at the cohort level,\nenabling better utilization of resources in constrained settings."}
{"id": "2505.16056", "pdf": "https://arxiv.org/pdf/2505.16056", "abs": "https://arxiv.org/abs/2505.16056", "authors": ["Jingcong Liang", "Siyuan Wang", "Miren Tian", "Yitong Li", "Duyu Tang", "Zhongyu Wei"], "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."}
{"id": "2505.16774", "pdf": "https://arxiv.org/pdf/2505.16774", "abs": "https://arxiv.org/abs/2505.16774", "authors": ["Yiming Gao", "Bin Wang", "Chengwei Wei", "Shuo Sun", "AiTi Aw"], "title": "IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models", "categories": ["cs.CL"], "comment": "Link: https://github.com/AudioLLMs/AudioBench/tree/main/IFEval-Audio", "summary": "Large language models (LLMs) have demonstrated strong instruction-following\ncapabilities in text-based tasks. However, this ability often deteriorates in\nmultimodal models after alignment with non-text modalities such as images or\naudio. While several recent efforts have investigated instruction-following\nperformance in text and vision-language models, instruction-following in\naudio-based large language models remains largely unexplored. To bridge this\ngap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess\nthe ability to follow instructions in an audio LLM. IFEval-Audio contains 280\naudio-instruction-answer triples across six diverse dimensions: Content,\nCapitalization, Symbol, List Structure, Length, and Format. Each example pairs\nan audio input with a text instruction, requiring the model to generate an\noutput that follows a specified structure. We benchmark state-of-the-art audio\nLLMs on their ability to follow audio-involved instructions. The dataset is\nreleased publicly to support future research in this emerging area."}
{"id": "2505.16801", "pdf": "https://arxiv.org/pdf/2505.16801", "abs": "https://arxiv.org/abs/2505.16801", "authors": ["Eleftherios Kalafatis", "Konstantinos Mitsis", "Konstantia Zarkogianni", "Maria Athanasiou", "Konstantina Nikita"], "title": "A modular framework for automated evaluation of procedural content generation in serious games with deep reinforcement learning agents", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Serious Games (SGs) are nowadays shifting focus to include procedural content\ngeneration (PCG) in the development process as a means of offering personalized\nand enhanced player experience. However, the development of a framework to\nassess the impact of PCG techniques when integrated into SGs remains\nparticularly challenging. This study proposes a methodology for automated\nevaluation of PCG integration in SGs, incorporating deep reinforcement learning\n(DRL) game testing agents. To validate the proposed framework, a previously\nintroduced SG featuring card game mechanics and incorporating three different\nversions of PCG for nonplayer character (NPC) creation has been deployed.\nVersion 1 features random NPC creation, while versions 2 and 3 utilize a\ngenetic algorithm approach. These versions are used to test the impact of\ndifferent dynamic SG environments on the proposed framework's agents. The\nobtained results highlight the superiority of the DRL game testing agents\ntrained on Versions 2 and 3 over those trained on Version 1 in terms of win\nrate (i.e. number of wins per played games) and training time. More\nspecifically, within the execution of a test emulating regular gameplay, both\nVersions 2 and 3 peaked at a 97% win rate and achieved statistically\nsignificant higher (p=0009) win rates compared to those achieved in Version 1\nthat peaked at 94%. Overall, results advocate towards the proposed framework's\ncapability to produce meaningful data for the evaluation of procedurally\ngenerated content in SGs."}
{"id": "2505.16057", "pdf": "https://arxiv.org/pdf/2505.16057", "abs": "https://arxiv.org/abs/2505.16057", "authors": ["Ayae Ide", "Tory Park", "Jaron Mink", "Tanusree Sharma"], "title": "Signals of Provenance: Practices & Challenges of Navigating Indicators in AI-Generated Media for Sighted and Blind Individuals", "categories": ["cs.HC", "cs.AI", "cs.MM"], "comment": null, "summary": "AI-Generated (AIG) content has become increasingly widespread by recent\nadvances in generative models and the easy-to-use tools that have significantly\nlowered the technical barriers for producing highly realistic audio, images,\nand videos through simple natural language prompts. In response, platforms are\nadopting provable provenance with platforms recommending AIG to be\nself-disclosed and signaled to users. However, these indicators may be often\nmissed, especially when they rely solely on visual cues and make them\nineffective to users with different sensory abilities. To address the gap, we\nconducted semi-structured interviews (N=28) with 15 sighted and 13 BLV\nparticipants to examine their interaction with AIG content through\nself-disclosed AI indicators. Our findings reveal diverse mental models and\npractices, highlighting different strengths and weaknesses of content-based\n(e.g., title, description) and menu-aided (e.g., AI labels) indicators. While\nsighted participants leveraged visual and audio cues, BLV participants\nprimarily relied on audio and existing assistive tools, limiting their ability\nto identify AIG. Across both groups, they frequently overlooked menu-aided\nindicators deployed by platforms and rather interacted with content-based\nindicators such as title and comments. We uncovered usability challenges\nstemming from inconsistent indicator placement, unclear metadata, and cognitive\noverload. These issues were especially critical for BLV individuals due to the\ninsufficient accessibility of interface elements. We provide practical\nrecommendations and design implications for future AIG indicators across\nseveral dimensions."}
{"id": "2505.16782", "pdf": "https://arxiv.org/pdf/2505.16782", "abs": "https://arxiv.org/abs/2505.16782", "authors": ["Xinghao Chen", "Anhao Zhao", "Heming Xia", "Xuan Lu", "Hanlin Wang", "Yanjun Chen", "Wei Zhang", "Jian Wang", "Wenjie Li", "Xiaoyu Shen"], "title": "Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive performance on complex\nreasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional\nCoT relies on reasoning steps explicitly verbalized in natural language,\nintroducing inefficiencies and limiting its applicability to abstract\nreasoning. To address this, there has been growing research interest in latent\nCoT reasoning, where inference occurs within latent spaces. By decoupling\nreasoning from language, latent reasoning promises richer cognitive\nrepresentations and more flexible, faster inference. Researchers have explored\nvarious directions in this promising field, including training methodologies,\nstructural innovations, and internal reasoning mechanisms. This paper presents\na comprehensive overview and analysis of this reasoning paradigm. We begin by\nproposing a unified taxonomy from four perspectives: token-wise strategies,\ninternal mechanisms, analysis, and applications. We then provide in-depth\ndiscussions and comparative analyses of representative methods, highlighting\ntheir design patterns, strengths, and open challenges. We aim to provide a\nstructured foundation for advancing this emerging direction in LLM reasoning.\nThe relevant papers will be regularly updated at\nhttps://github.com/EIT-NLP/Awesome-Latent-CoT."}
{"id": "2505.16829", "pdf": "https://arxiv.org/pdf/2505.16829", "abs": "https://arxiv.org/abs/2505.16829", "authors": ["Anna Heuser", "Thomas Kesselheim"], "title": "Contextual Learning for Stochastic Optimization", "categories": ["cs.LG", "cs.DS", "cs.GT"], "comment": "Full version of EC'25 paper", "summary": "Motivated by stochastic optimization, we introduce the problem of learning\nfrom samples of contextual value distributions. A contextual value distribution\ncan be understood as a family of real-valued distributions, where each sample\nconsists of a context $x$ and a random variable drawn from the corresponding\nreal-valued distribution $D_x$. By minimizing a convex surrogate loss, we learn\nan empirical distribution $D'_x$ for each context, ensuring a small L\\'evy\ndistance to $D_x$. We apply this result to obtain the sample complexity bounds\nfor the learning of an $\\epsilon$-optimal policy for stochastic optimization\nproblems defined on an unknown contextual value distribution. The sample\ncomplexity is shown to be polynomial for the general case of strongly monotone\nand stable optimization problems, including Single-item Revenue Maximization,\nPandora's Box and Optimal Stopping."}
{"id": "2505.16058", "pdf": "https://arxiv.org/pdf/2505.16058", "abs": "https://arxiv.org/abs/2505.16058", "authors": ["Mars Liyao Gao", "J. Nathan Kutz", "Bernat Font"], "title": "Mesh-free sparse identification of nonlinear dynamics", "categories": ["cs.LG", "cs.AI", "physics.data-an"], "comment": "17 pages, 13 figures, 14 tables", "summary": "Identifying the governing equations of a dynamical system is one of the most\nimportant tasks for scientific modeling. However, this procedure often requires\nhigh-quality spatio-temporal data uniformly sampled on structured grids. In\nthis paper, we propose mesh-free SINDy, a novel algorithm which leverages the\npower of neural network approximation as well as auto-differentiation to\nidentify governing equations from arbitrary sensor placements and non-uniform\ntemporal data sampling. We show that mesh-free SINDy is robust to high noise\nlevels and limited data while remaining computationally efficient. In our\nimplementation, the training procedure is straight-forward and nearly free of\nhyperparameter tuning, making mesh-free SINDy widely applicable to many\nscientific and engineering problems. In the experiments, we demonstrate its\neffectiveness on a series of PDEs including the Burgers' equation, the heat\nequation, the Korteweg-De Vries equation and the 2D advection-diffusion\nequation. We conduct detailed numerical experiments on all datasets, varying\nthe noise levels and number of samples, and we also compare our approach to\nprevious state-of-the-art methods. It is noteworthy that, even in high-noise\nand low-data scenarios, mesh-free SINDy demonstrates robust PDE discovery,\nachieving successful identification with up to 75% noise for the Burgers'\nequation using 5,000 samples and with as few as 100 samples and 1% noise. All\nof this is achieved within a training time of under one minute."}
{"id": "2505.16789", "pdf": "https://arxiv.org/pdf/2505.16789", "abs": "https://arxiv.org/abs/2505.16789", "authors": ["Punya Syon Pandey", "Samuel Simko", "Kellin Pelrine", "Zhijing Jin"], "title": "Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As large language models gain popularity, their vulnerability to adversarial\nattacks remains a primary concern. While fine-tuning models on domain-specific\ndatasets is often employed to improve model performance, it can introduce\nvulnerabilities within the underlying model. In this work, we investigate\nAccidental Misalignment, unexpected vulnerabilities arising from\ncharacteristics of fine-tuning data. We begin by identifying potential\ncorrelation factors such as linguistic features, semantic similarity, and\ntoxicity within our experimental datasets. We then evaluate the adversarial\nperformance of these fine-tuned models and assess how dataset factors correlate\nwith attack success rates. Lastly, we explore potential causal links, offering\nnew insights into adversarial defense strategies and highlighting the crucial\nrole of dataset design in preserving model alignment. Our code is available at\nhttps://github.com/psyonp/accidental_misalignment."}
{"id": "2505.16833", "pdf": "https://arxiv.org/pdf/2505.16833", "abs": "https://arxiv.org/abs/2505.16833", "authors": ["Alihan Hüyük", "Finale Doshi-Velez"], "title": "Strategically Linked Decisions in Long-Term Planning and Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Long-term planning, as in reinforcement learning (RL), involves finding\nstrategies: actions that collectively work toward a goal rather than\nindividually optimizing their immediate outcomes. As part of a strategy, some\nactions are taken at the expense of short-term benefit to enable future actions\nwith even greater returns. These actions are only advantageous if followed up\nby the actions they facilitate, consequently, they would not have been taken if\nthose follow-ups were not available. In this paper, we quantify such\ndependencies between planned actions with strategic link scores: the drop in\nthe likelihood of one decision under the constraint that a follow-up decision\nis no longer available. We demonstrate the utility of strategic link scores\nthrough three practical applications: (i) explaining black-box RL agents by\nidentifying strategically linked pairs among decisions they make, (ii)\nimproving the worst-case performance of decision support systems by\ndistinguishing whether recommended actions can be adopted as standalone\nimprovements or whether they are strategically linked hence requiring a\ncommitment to a broader strategy to be effective, and (iii) characterizing the\nplanning processes of non-RL agents purely through interventions aimed at\nmeasuring strategic link scores - as an example, we consider a realistic\ntraffic simulator and analyze through road closures the effective planning\nhorizon of the emergent routing behavior of many drivers."}
{"id": "2505.16066", "pdf": "https://arxiv.org/pdf/2505.16066", "abs": "https://arxiv.org/abs/2505.16066", "authors": ["Zhixu Silvia Tao", "Kasper Vinken", "Hao-Wei Yeh", "Avi Cooper", "Xavier Boix"], "title": "Merge to Mix: Mixing Datasets via Model Merging", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Mixing datasets for fine-tuning large models (LMs) has become critical for\nmaximizing performance on downstream tasks. However, composing effective\ndataset mixtures typically relies on heuristics and trial-and-error, often\nrequiring multiple fine-tuning runs to achieve the desired outcome. We propose\na novel method, $\\textit{Merge to Mix}$, that accelerates composing dataset\nmixtures through model merging. Model merging is a recent technique that\ncombines the abilities of multiple individually fine-tuned LMs into a single LM\nby using a few simple arithmetic operations. Our key insight is that merging\nmodels individually fine-tuned on each dataset in a mixture can effectively\nserve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix\nleverages this insight to accelerate selecting dataset mixtures without\nrequiring full fine-tuning on each candidate mixture. Our experiments\ndemonstrate that Merge to Mix surpasses state-of-the-art methods in dataset\nselection for fine-tuning LMs."}
{"id": "2505.16800", "pdf": "https://arxiv.org/pdf/2505.16800", "abs": "https://arxiv.org/abs/2505.16800", "authors": ["Changbing Yang", "Garrett Nicolai"], "title": "Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation", "categories": ["cs.CL"], "comment": null, "summary": "We introduce a transformer-based morpheme segmentation system that augments a\nlow-resource training signal through multitask learning and LLM-generated\nsynthetic data. Our framework jointly predicts morphological segments and\nglosses from orthographic input, leveraging shared linguistic representations\nobtained through a common documentary process to enhance model generalization.\nTo further address data scarcity, we integrate synthetic training data\ngenerated by large language models (LLMs) using in-context learning.\nExperimental results on the SIGMORPHON 2023 dataset show that our approach\nsignificantly improves word-level segmentation accuracy and morpheme-level\nF1-score across multiple low-resource languages."}
{"id": "2505.16850", "pdf": "https://arxiv.org/pdf/2505.16850", "abs": "https://arxiv.org/abs/2505.16850", "authors": ["Tajamul Ashraf", "Mohammed Mohsen Peerzada", "Moloud Abdar", "Yutong Xie", "Yuyin Zhou", "Xiaofeng Liu", "Iqra Altaf Gillani", "Janibul Bashir"], "title": "ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Federated Learning Benchmark for Domain Adaptation, Trustworthiness,\n  and Reasoning", "summary": "Federated Learning (FL) has emerged as a promising paradigm for collaborative\nmodel training while preserving data privacy across decentralized participants.\nAs FL adoption grows, numerous techniques have been proposed to tackle its\npractical challenges. However, the lack of standardized evaluation across key\ndimensions hampers systematic progress and fair comparison of FL methods. In\nthis work, we introduce ATR-Bench, a unified framework for analyzing federated\nlearning through three foundational dimensions: Adaptation, Trust, and\nReasoning. We provide an in-depth examination of the conceptual foundations,\ntask formulations, and open research challenges associated with each theme. We\nhave extensively benchmarked representative methods and datasets for adaptation\nto heterogeneous clients and trustworthiness in adversarial or unreliable\nenvironments. Due to the lack of reliable metrics and models for reasoning in\nFL, we only provide literature-driven insights for this dimension. ATR-Bench\nlays the groundwork for a systematic and holistic evaluation of federated\nlearning with real-world relevance. We will make our complete codebase publicly\naccessible and a curated repository that continuously tracks new developments\nand research in the FL literature."}
{"id": "2505.16074", "pdf": "https://arxiv.org/pdf/2505.16074", "abs": "https://arxiv.org/abs/2505.16074", "authors": ["Bart Kosko", "Olaoluwa Adigun"], "title": "Bidirectional Variational Autoencoders", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "10 pages, 6 figures", "summary": "We present the new bidirectional variational autoencoder (BVAE) network\narchitecture. The BVAE uses a single neural network both to encode and decode\ninstead of an encoder-decoder network pair. The network encodes in the forward\ndirection and decodes in the backward direction through the same synaptic web.\nSimulations compared BVAEs and ordinary VAEs on the four image tasks of image\nreconstruction, classification, interpolation, and generation. The image\ndatasets included MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and\nCelebA-64 face images. The bidirectional structure of BVAEs cut the parameter\ncount by almost 50% and still slightly outperformed the unidirectional VAEs."}
{"id": "2505.16806", "pdf": "https://arxiv.org/pdf/2505.16806", "abs": "https://arxiv.org/abs/2505.16806", "authors": ["Kexin Zhang", "Junlan Chen", "Daifeng Li", "Yuxuan Zhang", "Yangyang Feng", "Bowen Deng", "Weixu Chen"], "title": "Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) encounter difficulties in knowledge-intensive\nmulti-step reasoning (KIMSR) tasks. One challenge is how to effectively extract\nand represent rationale evidence. The current methods often extract\nsemantically relevant but logically irrelevant evidence, resulting in flawed\nreasoning and inaccurate responses. We propose a two-way evidence\nself-alignment (TW-ESA) module, which utilizes the mutual alignment between\nstrict reasoning and LLM reasoning to enhance its understanding of the causal\nlogic of evidence, thereby addressing the first challenge. Another challenge is\nhow to utilize the rationale evidence and LLM's intrinsic knowledge for\naccurate reasoning when the evidence contains uncertainty. We propose a\ndual-gated reasoning enhancement (DGR) module to gradually fuse useful\nknowledge of LLM within strict reasoning, which can enable the model to perform\naccurate reasoning by focusing on causal elements in the evidence and exhibit\ngreater robustness. The two modules are collaboratively trained in a unified\nframework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR\ndatasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based\nfine-tuning methods, with remarkable average improvements of 4% in exact match\n(EM) and 5% in F1 score. The implementation code is available at\nhttps://anonymous.4open.science/r/ESA-DGR-2BF8."}
{"id": "2505.16856", "pdf": "https://arxiv.org/pdf/2505.16856", "abs": "https://arxiv.org/abs/2505.16856", "authors": ["Wei Xiao", "Jiacheng Liu", "Zifeng Zhuang", "Runze Suo", "Shangke Lyu", "Donglin Wang"], "title": "Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Improving the performance of pre-trained policies through online\nreinforcement learning (RL) is a critical yet challenging topic. Existing\nonline RL fine-tuning methods require continued training with offline\npretrained Q-functions for stability and performance. However, these offline\npretrained Q-functions commonly underestimate state-action pairs beyond the\noffline dataset due to the conservatism in most offline RL methods, which\nhinders further exploration when transitioning from the offline to the online\nsetting. Additionally, this requirement limits their applicability in scenarios\nwhere only pre-trained policies are available but pre-trained Q-functions are\nabsent, such as in imitation learning (IL) pre-training. To address these\nchallenges, we propose a method for efficient online RL fine-tuning using\nsolely the offline pre-trained policy, eliminating reliance on pre-trained\nQ-functions. We introduce PORL (Policy-Only Reinforcement Learning\nFine-Tuning), which rapidly initializes the Q-function from scratch during the\nonline phase to avoid detrimental pessimism. Our method not only achieves\ncompetitive performance with advanced offline-to-online RL algorithms and\nonline RL approaches that leverage data or policies prior, but also pioneers a\nnew path for directly fine-tuning behavior cloning (BC) policies."}
{"id": "2505.16088", "pdf": "https://arxiv.org/pdf/2505.16088", "abs": "https://arxiv.org/abs/2505.16088", "authors": ["Gagan Bhatia", "Maxime Peyrard", "Wei Zhao"], "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern BPE tokenizers often split calendar dates into meaningless fragments,\ne.g., 20250312 $\\rightarrow$ 202, 503, 12, inflating token counts and obscuring\nthe inherent structure needed for robust temporal reasoning. In this work, we\n(1) introduce a simple yet interpretable metric, termed date fragmentation\nratio, that measures how faithfully a tokenizer preserves multi-digit date\ncomponents; (2) release DateAugBench, a suite of 6500 examples spanning three\ntemporal reasoning tasks: context-based date resolution, format-invariance\npuzzles, and date arithmetic across historical, contemporary, and future\nregimes; and (3) through layer-wise probing and causal attention-hop analyses,\nuncover an emergent date-abstraction mechanism whereby large language models\nstitch together the fragments of month, day, and year components for temporal\nreasoning. Our experiments show that excessive fragmentation correlates with\naccuracy drops of up to 10 points on uncommon dates like historical and\nfuturistic dates. Further, we find that the larger the model, the faster the\nemergent date abstraction that heals date fragments is accomplished. Lastly, we\nobserve a reasoning path that LLMs follow to assemble date fragments, typically\ndiffering from human interpretation (year $\\rightarrow$ month $\\rightarrow$\nday)."}
{"id": "2505.16814", "pdf": "https://arxiv.org/pdf/2505.16814", "abs": "https://arxiv.org/abs/2505.16814", "authors": ["Gaurav Kamath", "Sowmya Vajjala"], "title": "Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?", "categories": ["cs.CL"], "comment": "pre-print", "summary": "Named Entity Recognition(NER) for low-resource languages aims to produce\nrobust systems for languages where there is limited labeled training data\navailable, and has been an area of increasing interest within NLP. Data\naugmentation for increasing the amount of low-resource labeled data is a common\npractice. In this paper, we explore the role of synthetic data in the context\nof multilingual, low-resource NER, considering 11 languages from diverse\nlanguage families. Our results suggest that synthetic data does in fact hold\npromise for low-resource language NER, though we see significant variation\nbetween languages."}
{"id": "2505.16857", "pdf": "https://arxiv.org/pdf/2505.16857", "abs": "https://arxiv.org/abs/2505.16857", "authors": ["Ertuğrul Keçeci", "Müjde Güzelkaya", "Tufan Kumbasar"], "title": "Redefining Clustered Federated Learning for System Identification: The Path of ClusterCraft", "categories": ["cs.LG", "I.2.8; I.5.3; I.2.11"], "comment": null, "summary": "This paper addresses the System Identification (SYSID) problem within the\nframework of federated learning. We introduce a novel algorithm, Incremental\nClustering-based federated learning method for SYSID (IC-SYSID), designed to\ntackle SYSID challenges across multiple data sources without prior knowledge.\nIC-SYSID utilizes an incremental clustering method, ClusterCraft (CC), to\neliminate the dependency on the prior knowledge of the dataset. CC starts with\na single cluster model and assigns similar local workers to the same clusters\nby dynamically increasing the number of clusters. To reduce the number of\nclusters generated by CC, we introduce ClusterMerge, where similar cluster\nmodels are merged. We also introduce enhanced ClusterCraft to reduce the\ngeneration of similar cluster models during the training. Moreover, IC-SYSID\naddresses cluster model instability by integrating a regularization term into\nthe loss function and initializing cluster models with scaled Glorot\ninitialization. It also utilizes a mini-batch deep learning approach to manage\nlarge SYSID datasets during local training. Through the experiments conducted\non a real-world representing SYSID problem, where a fleet of vehicles\ncollaboratively learns vehicle dynamics, we show that IC-SYSID achieves a high\nSYSID performance while preventing the learning of unstable clusters."}
{"id": "2505.16103", "pdf": "https://arxiv.org/pdf/2505.16103", "abs": "https://arxiv.org/abs/2505.16103", "authors": ["Monirul Islam Mahmud"], "title": "Towards Trustworthy Keylogger detection: A Comprehensive Analysis of Ensemble Techniques and Feature Selections through Explainable AI", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Keylogger detection involves monitoring for unusual system behaviors such as\ndelays between typing and character display, analyzing network traffic patterns\nfor data exfiltration. In this study, we provide a comprehensive analysis for\nkeylogger detection with traditional machine learning models - SVC, Random\nForest, Decision Tree, XGBoost, AdaBoost, Logistic Regression and Naive Bayes\nand advanced ensemble methods including Stacking, Blending and Voting.\nMoreover, feature selection approaches such as Information gain, Lasso L1 and\nFisher Score are thoroughly assessed to improve predictive performance and\nlower computational complexity. The Keylogger Detection dataset from publicly\navailable Kaggle website is used in this project. In addition to accuracy-based\nclassification, this study implements the approach for model interpretation\nusing Explainable AI (XAI) techniques namely SHAP (Global) and LIME (Local) to\ndeliver finer explanations for how much each feature contributes in assisting\nor hindering the detection process. To evaluate the models result, we have used\nAUC score, sensitivity, Specificity, Accuracy and F1 score. The best\nperformance was achieved by AdaBoost with 99.76% accuracy, F1 score of 0.99,\n100% precision, 98.6% recall, 1.0 specificity and 0.99 of AUC that is\nnear-perfect classification with Fisher Score."}
{"id": "2505.16831", "pdf": "https://arxiv.org/pdf/2505.16831", "abs": "https://arxiv.org/abs/2505.16831", "authors": ["Xiaoyu Xu", "Xiang Yue", "Yang Liu", "Qingqing Ye", "Haibo Hu", "Minxin Du"], "title": "Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "44 pages", "summary": "Unlearning in large language models (LLMs) is intended to remove the\ninfluence of specific data, yet current evaluations rely heavily on token-level\nmetrics such as accuracy and perplexity. We show that these metrics can be\nmisleading: models often appear to forget, but their original behavior can be\nrapidly restored with minimal fine-tuning, revealing that unlearning may\nobscure information rather than erase it. To diagnose this phenomenon, we\nintroduce a representation-level evaluation framework using PCA-based\nsimilarity and shift, centered kernel alignment, and Fisher information.\nApplying this toolkit across six unlearning methods, three domains (text, code,\nmath), and two open-source LLMs, we uncover a critical distinction between\nreversible and irreversible forgetting. In reversible cases, models suffer\ntoken-level collapse yet retain latent features; in irreversible cases, deeper\nrepresentational damage occurs. We further provide a theoretical account\nlinking shallow weight perturbations near output layers to misleading\nunlearning signals, and show that reversibility is modulated by task type and\nhyperparameters. Our findings reveal a fundamental gap in current evaluation\npractices and establish a new diagnostic foundation for trustworthy unlearning\nin LLMs. We provide a unified toolkit for analyzing LLM representation changes\nunder unlearning and relearning:\nhttps://github.com/XiaoyuXU1/Representational_Analysis_Tools.git."}
{"id": "2505.16860", "pdf": "https://arxiv.org/pdf/2505.16860", "abs": "https://arxiv.org/abs/2505.16860", "authors": ["Ziyue Qiao", "Qianyi Cai", "Hao Dong", "Jiawei Gu", "Pengyang Wang", "Meng Xiao", "Xiao Luo", "Hui Xiong"], "title": "GCAL: Adapting Graph Models to Evolving Domain Shifts", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "This paper addresses the challenge of graph domain adaptation on evolving,\nmultiple out-of-distribution (OOD) graphs. Conventional graph domain adaptation\nmethods are confined to single-step adaptation, making them ineffective in\nhandling continuous domain shifts and prone to catastrophic forgetting. This\npaper introduces the Graph Continual Adaptive Learning (GCAL) method, designed\nto enhance model sustainability and adaptability across various graph domains.\nGCAL employs a bilevel optimization strategy. The \"adapt\" phase uses an\ninformation maximization approach to fine-tune the model with new graph domains\nwhile re-adapting past memories to mitigate forgetting. Concurrently, the\n\"generate memory\" phase, guided by a theoretical lower bound derived from\ninformation bottleneck theory, involves a variational memory graph generation\nmodule to condense original graphs into memories. Extensive experimental\nevaluations demonstrate that GCAL substantially outperforms existing methods in\nterms of adaptability and knowledge retention."}
{"id": "2505.16130", "pdf": "https://arxiv.org/pdf/2505.16130", "abs": "https://arxiv.org/abs/2505.16130", "authors": ["Zehong Wang", "Zheyuan Zhang", "Tianyi Ma", "Chuxu Zhang", "Yanfang Ye"], "title": "Scalable Graph Generative Modeling via Substructure Sequences", "categories": ["cs.LG", "cs.AI", "cs.SI"], "comment": null, "summary": "Graph neural networks (GNNs) has been predominantly driven by\nmessage-passing, where node representations are iteratively updated via local\nneighborhood aggregation. Despite their success, message-passing suffers from\nfundamental limitations -- including constrained expressiveness,\nover-smoothing, over-squashing, and limited capacity to model long-range\ndependencies. These issues hinder scalability: increasing data size or model\nsize often fails to yield improved performance, limiting the viability of GNNs\nas backbones for graph foundation models. In this work, we explore pathways\nbeyond message-passing and introduce Generative Graph Pattern Machine\n(G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM\nrepresents graph instances (nodes, edges, or entire graphs) as sequences of\nsubstructures, and employs generative pre-training over the sequences to learn\ngeneralizable, transferable representations. Empirically, G$^2$PM demonstrates\nstrong scalability: on the ogbn-arxiv benchmark, it continues to improve with\nmodel sizes up to 60M parameters, outperforming prior generative approaches\nthat plateau at significantly smaller scales (e.g., 3M). In addition, we\nsystematically analyze the model design space, highlighting key architectural\nchoices that contribute to its scalability and generalization. Across diverse\ntasks -- including node classification, graph classification, and transfer\nlearning -- G$^2$PM consistently outperforms strong baselines, establishing a\ncompelling foundation for scalable graph learning. The code and dataset are\navailable at https://github.com/Zehong-Wang/G2PM."}
{"id": "2505.16834", "pdf": "https://arxiv.org/pdf/2505.16834", "abs": "https://arxiv.org/abs/2505.16834", "authors": ["Shuang Sun", "Huatong Song", "Yuhao Wang", "Ruiyang Ren", "Jinhao Jiang", "Junjie Zhang", "Fei Bai", "Jia Deng", "Wayne Xin Zhao", "Zheng Liu", "Lei Fang", "Zhongyuan Wang", "Ji-Rong Wen"], "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher."}
{"id": "2505.16872", "pdf": "https://arxiv.org/pdf/2505.16872", "abs": "https://arxiv.org/abs/2505.16872", "authors": ["Mohammed Al-Qudah", "Fadi AlMahamid"], "title": "A Multi-Step Comparative Framework for Anomaly Detection in IoT Data Streams", "categories": ["cs.LG"], "comment": null, "summary": "The rapid expansion of Internet of Things (IoT) devices has introduced\ncritical security challenges, underscoring the need for accurate anomaly\ndetection. Although numerous studies have proposed machine learning (ML)\nmethods for this purpose, limited research systematically examines how\ndifferent preprocessing steps--normalization, transformation, and feature\nselection--interact with distinct model architectures. To address this gap,\nthis paper presents a multi-step evaluation framework assessing the combined\nimpact of preprocessing choices on three ML algorithms: RNN-LSTM, autoencoder\nneural networks (ANN), and Gradient Boosting (GBoosting). Experiments on the\nIoTID20 dataset shows that GBoosting consistently delivers superior accuracy\nacross preprocessing configurations, while RNN-LSTM shows notable gains with\nz-score normalization and autoencoders excel in recall, making them well-suited\nfor unsupervised scenarios. By offering a structured analysis of preprocessing\ndecisions and their interplay with various ML techniques, the proposed\nframework provides actionable guidance to enhance anomaly detection performance\nin IoT environments."}
{"id": "2505.16136", "pdf": "https://arxiv.org/pdf/2505.16136", "abs": "https://arxiv.org/abs/2505.16136", "authors": ["Yuke Zhang"], "title": "Interpretable Machine Learning for Macro Alpha: A News Sentiment Case Study", "categories": ["q-fin.CP", "cs.AI", "cs.LG", "q-fin.TR"], "comment": "18 pages (including references), 1 figure, 1 table. Code available at\n  \\url{https://github.com/yukepenn/macro-news-sentiment-trading}. Keywords:\n  Macro Sentiment, News Sentiment, Algorithmic Trading, GDELT, FinBERT, NLP,\n  Alternative Data, Foreign Exchange, Treasury Futures, Quantitative Finance,\n  Machine Learning, SHAP, Interpretability", "summary": "This study introduces an interpretable machine learning (ML) framework to\nextract macroeconomic alpha from global news sentiment. We process the Global\nDatabase of Events, Language, and Tone (GDELT) Project's worldwide news feed\nusing FinBERT -- a Bidirectional Encoder Representations from Transformers\n(BERT) based model pretrained on finance-specific language -- to construct\ndaily sentiment indices incorporating mean tone, dispersion, and event impact.\nThese indices drive an XGBoost classifier, benchmarked against logistic\nregression, to predict next-day returns for EUR/USD, USD/JPY, and 10-year U.S.\nTreasury futures (ZN). Rigorous out-of-sample (OOS) backtesting (5-fold\nexpanding-window cross-validation, OOS period: c. 2017-April 2025) demonstrates\nexceptional, cost-adjusted performance for the XGBoost strategy: Sharpe ratios\nachieve 5.87 (EUR/USD), 4.65 (USD/JPY), and 4.65 (Treasuries), with respective\ncompound annual growth rates (CAGRs) exceeding 50% in Foreign Exchange (FX) and\n22% in bonds. Shapley Additive Explanations (SHAP) affirm that sentiment\ndispersion and article impact are key predictive features. Our findings\nestablish that integrating domain-specific Natural Language Processing (NLP)\nwith interpretable ML offers a potent and explainable source of macro alpha."}
{"id": "2505.16838", "pdf": "https://arxiv.org/pdf/2505.16838", "abs": "https://arxiv.org/abs/2505.16838", "authors": ["Yibo Wang", "Li Shen", "Huanjin Yao", "Tiansheng Huang", "Rui Liu", "Naiqiang Tan", "Jiaxing Huang", "Kai Zhang", "Dacheng Tao"], "title": "R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by\nenabling step-by-step problem-solving, yet its extension to Long-CoT introduces\nsubstantial computational overhead due to increased token length. Existing\ncompression approaches -- instance-level and token-level -- either sacrifice\nessential local reasoning signals like reflection or yield incoherent outputs.\nTo address these limitations, we propose R1-Compress, a two-stage chunk-level\ncompression framework that preserves both local information and coherence. Our\nmethod segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk\ncompression, and employs an inter-chunk search mechanism to select the short\nand coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500,\nAIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces\ntoken usage while maintaining comparable reasoning accuracy. On MATH500,\nR1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to\nthe Long-CoT baseline, while reducing token usage by about 20%. Source code\nwill be available at https://github.com/w-yibo/R1-Compress"}
{"id": "2505.16896", "pdf": "https://arxiv.org/pdf/2505.16896", "abs": "https://arxiv.org/abs/2505.16896", "authors": ["Can Chen", "David Heurtel-Depeiges", "Robert M. Vernon", "Christopher James Langmead", "Yoshua Bengio", "Quentin Fournier"], "title": "Structure-Aligned Protein Language Model", "categories": ["cs.LG", "cs.AI"], "comment": "16 pages, 8 figures, 7 tables", "summary": "Protein language models (pLMs) pre-trained on vast protein sequence databases\nexcel at various downstream tasks but lack the structural knowledge essential\nfor many biological applications. To address this, we integrate structural\ninsights from pre-trained protein graph neural networks (pGNNs) into pLMs\nthrough a latent-level contrastive learning task. This task aligns residue\nrepresentations from pLMs with those from pGNNs across multiple proteins,\nenriching pLMs with inter-protein structural knowledge. Additionally, we\nincorporate a physical-level task that infuses intra-protein structural\nknowledge by optimizing pLMs to predict structural tokens. The proposed\ndual-task framework effectively incorporates both inter-protein and\nintra-protein structural knowledge into pLMs. Given the variability in the\nquality of protein structures in PDB, we further introduce a residue loss\nselection module, which uses a small model trained on high-quality structures\nto select reliable yet challenging residue losses for the pLM to learn.\nApplying our structure alignment method to the state-of-the-art ESM2 and\nAMPLIFY results in notable performance gains across a wide range of tasks,\nincluding a 12.7% increase in ESM2 contact prediction. The data, code, and\nresulting SaESM2 and SaAMPLIFY models will be released on Hugging Face."}
{"id": "2505.16146", "pdf": "https://arxiv.org/pdf/2505.16146", "abs": "https://arxiv.org/abs/2505.16146", "authors": ["Zhenglin Hua", "Jinghan He", "Zijun Yao", "Tianxu Han", "Haiyun Guo", "Yuheng Jia", "Junfeng Fang"], "title": "Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large vision-language models (LVLMs) have achieved remarkable performance on\nmultimodal tasks such as visual question answering (VQA) and image captioning.\nHowever, they still suffer from hallucinations, generating text inconsistent\nwith visual input, posing significant risks in real-world applications.\nExisting approaches to address this issue focus on incorporating external\nknowledge bases, alignment training, or decoding strategies, all of which\nrequire substantial computational cost and time. Recent works try to explore\nmore efficient alternatives by adjusting LVLMs' internal representations.\nAlthough promising, these methods may cause hallucinations to be insufficiently\nsuppressed or lead to excessive interventions that negatively affect normal\nsemantics. In this work, we leverage sparse autoencoders (SAEs) to identify\nsemantic directions closely associated with either hallucinations or actuality,\nrealizing more precise and direct hallucination-related representations. Our\nanalysis demonstrates that interventions along the faithful direction we\nidentified can mitigate hallucinations, while those along the hallucinatory\ndirection can exacerbate them. Building on these insights, we propose Steering\nLVLMs via SAE Latent Directions (SSL), a training-free method based on\nSAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive\nexperiments demonstrate that SSL significantly outperforms existing decoding\napproaches in mitigating hallucinations, while maintaining transferability\nacross different model architectures with negligible additional time overhead."}
{"id": "2505.16847", "pdf": "https://arxiv.org/pdf/2505.16847", "abs": "https://arxiv.org/abs/2505.16847", "authors": ["Baran Barbarestani", "Isa Maks", "Piek Vossen"], "title": "Understanding and Analyzing Inappropriately Targeting Language in Online Discourse: A Comparative Annotation Study", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces a method for detecting inappropriately targeting\nlanguage in online conversations by integrating crowd and expert annotations\nwith ChatGPT. We focus on English conversation threads from Reddit, examining\ncomments that target individuals or groups. Our approach involves a\ncomprehensive annotation framework that labels a diverse data set for various\ntarget categories and specific target words within the conversational context.\nWe perform a comparative analysis of annotations from human experts, crowd\nannotators, and ChatGPT, revealing strengths and limitations of each method in\nrecognizing both explicit hate speech and subtler discriminatory language. Our\nfindings highlight the significant role of contextual factors in identifying\nhate speech and uncover new categories of targeting, such as social belief and\nbody image. We also address the challenges and subjective judgments involved in\nannotation and the limitations of ChatGPT in grasping nuanced language. This\nstudy provides insights for improving automated content moderation strategies\nto enhance online safety and inclusivity."}
{"id": "2505.16903", "pdf": "https://arxiv.org/pdf/2505.16903", "abs": "https://arxiv.org/abs/2505.16903", "authors": ["Peyman Baghershahi", "Sourav Medya"], "title": "Unsupervised Prompting for Graph Neural Networks", "categories": ["cs.LG"], "comment": "25 pages, 5 figures, 14 tables", "summary": "Prompt tuning methods for Graph Neural Networks (GNNs) have become popular to\naddress the semantic gap between pre-training and fine-tuning steps. However,\nexisting GNN prompting methods rely on labeled data and involve lightweight\nfine-tuning for downstream tasks. Meanwhile, in-context learning methods for\nLarge Language Models (LLMs) have shown promising performance with no parameter\nupdating and no or minimal labeled data. Inspired by these approaches, in this\nwork, we first introduce a challenging problem setup to evaluate GNN prompting\nmethods. This setup encourages a prompting function to enhance a pre-trained\nGNN's generalization to a target dataset under covariate shift without updating\nthe GNN's parameters and with no labeled data. Next, we propose a fully\nunsupervised prompting method based on consistency regularization through\npseudo-labeling. We use two regularization techniques to align the prompted\ngraphs' distribution with the original data and reduce biased predictions.\nThrough extensive experiments under our problem setting, we demonstrate that\nour unsupervised approach outperforms the state-of-the-art prompting methods\nthat have access to labels."}
{"id": "2505.16149", "pdf": "https://arxiv.org/pdf/2505.16149", "abs": "https://arxiv.org/abs/2505.16149", "authors": ["Zirui Pang", "Haosheng Tan", "Yuhan Pu", "Zhijie Deng", "Zhouan Shen", "Keyu Hu", "Jiaheng Wei"], "title": "When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Image classification benchmark datasets such as CIFAR, MNIST, and ImageNet\nserve as critical tools for model evaluation. However, despite the cleaning\nefforts, these datasets still suffer from pervasive noisy labels and often\ncontain missing labels due to the co-existing image pattern where multiple\nclasses appear in an image sample. This results in misleading model comparisons\nand unfair evaluations. Existing label cleaning methods focus primarily on\nnoisy labels, but the issue of missing labels remains largely overlooked.\nMotivated by these challenges, we present a comprehensive framework named\nREVEAL, integrating state-of-the-art pre-trained vision-language models (e.g.,\nLLaVA, BLIP, Janus, Qwen) with advanced machine/human label curation methods\n(e.g., Docta, Cleanlab, MTurk), to systematically address both noisy labels and\nmissing label detection in widely-used image classification test sets. REVEAL\ndetects potential noisy labels and omissions, aggregates predictions from\nvarious methods, and refines label accuracy through confidence-informed\npredictions and consensus-based filtering. Additionally, we provide a thorough\nanalysis of state-of-the-art vision-language models and pre-trained image\nclassifiers, highlighting their strengths and limitations within the context of\ndataset renovation by revealing 10 observations. Our method effectively reveals\nmissing labels from public datasets and provides soft-labeled results with\nlikelihoods. Through human verifications, REVEAL significantly improves the\nquality of 6 benchmark test sets, highly aligning to human judgments and\nenabling more accurate and meaningful comparisons in image classification."}
{"id": "2505.16855", "pdf": "https://arxiv.org/pdf/2505.16855", "abs": "https://arxiv.org/abs/2505.16855", "authors": ["Alberto Muñoz-Ortiz", "David Vilares", "Caio COrro", "Carlos Gómez-Rodríguez"], "title": "Nested Named Entity Recognition as Single-Pass Sequence Labeling", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "Submitted to EMNLP 2025", "summary": "We cast nested named entity recognition (NNER) as a sequence labeling task by\nleveraging prior work that linearizes constituency structures, effectively\nreducing the complexity of this structured prediction problem to\nstraightforward token classification. By combining these constituency\nlinearizations with pretrained encoders, our method captures nested entities\nwhile performing exactly $n$ tagging actions. Our approach achieves competitive\nperformance compared to less efficient systems, and it can be trained using any\noff-the-shelf sequence labeling library."}
{"id": "2505.16918", "pdf": "https://arxiv.org/pdf/2505.16918", "abs": "https://arxiv.org/abs/2505.16918", "authors": ["Nikola Tankovic", "Robert Sajina"], "title": "Scalable and Interpretable Contextual Bandits: A Literature Review and Retail Offer Prototype", "categories": ["cs.LG"], "comment": null, "summary": "This paper presents a concise review of Contextual Multi-Armed Bandit (CMAB)\nmethods and introduces an experimental framework for scalable, interpretable\noffer selection, addressing the challenge of fast-changing offers. The approach\nmodels context at the product category level, allowing offers to span multiple\ncategories and enabling knowledge transfer across similar offers. This improves\nlearning efficiency and generalization in dynamic environments. The framework\nextends standard CMAB methodology to support multi-category contexts, and\nachieves scalability through efficient feature engineering and modular design.\nAdvanced features such as MPG (Member Purchase Gap) and MF (Matrix\nFactorization) capture nuanced user-offer interactions, with implementation in\nPython for practical deployment.\n  A key contribution is interpretability at scale: logistic regression models\nyield transparent weight vectors, accessible via a large language model (LLM)\ninterface for real-time, user-level tracking and explanation of evolving\npreferences. This enables the generation of detailed member profiles and\nidentification of behavioral patterns, supporting personalized offer\noptimization and enhancing trust in automated decisions. By situating our\nprototype alongside established paradigms like Generalized Linear Models and\nThompson Sampling, we demonstrate its value for both research and real-world\nCMAB applications."}
{"id": "2505.16172", "pdf": "https://arxiv.org/pdf/2505.16172", "abs": "https://arxiv.org/abs/2505.16172", "authors": ["Abhay Kumara Sri Krishna Nandiraju", "Gondy Leroy", "David Kauchak", "Arif Ahmed"], "title": "Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Understanding health information is essential in achieving and maintaining a\nhealthy life. We focus on simplifying health information for better\nunderstanding. With the availability of generative AI, the simplification\nprocess has become efficient and of reasonable quality, however, the algorithms\nremove information that may be crucial for comprehension. In this study, we\ncompare generative AI to detect missing information in simplified text,\nevaluate its importance, and fix the text with the missing information. We\ncollected 50 health information texts and simplified them using gpt-4-0613. We\ncompare five approaches to identify missing elements and regenerate the text by\ninserting the missing elements. These five approaches involve adding missing\nentities and missing words in various ways: 1) adding all the missing entities,\n2) adding all missing words, 3) adding the top-3 entities ranked by gpt-4-0613,\nand 4, 5) serving as controls for comparison, adding randomly chosen entities.\nWe use cosine similarity and ROUGE scores to evaluate the semantic similarity\nand content overlap between the original, simplified, and reconstructed\nsimplified text. We do this for both summaries and full text. Overall, we find\nthat adding missing entities improves the text. Adding all the missing entities\nresulted in better text regeneration, which was better than adding the\ntop-ranked entities or words, or random words. Current tools can identify these\nentities, but are not valuable in ranking them."}
{"id": "2505.16868", "pdf": "https://arxiv.org/pdf/2505.16868", "abs": "https://arxiv.org/abs/2505.16868", "authors": ["Sudhansu Bala Das", "Samujjal Choudhury", "Tapas Kumar Mishra", "Bidyut Kr. Patra"], "title": "Comparative analysis of subword tokenization approaches for Indian languages", "categories": ["cs.CL"], "comment": "24 pages, 4 tables", "summary": "Tokenization is the act of breaking down text into smaller parts, or tokens,\nthat are easier for machines to process. This is a key phase in machine\ntranslation (MT) models. Subword tokenization enhances this process by breaking\ndown words into smaller subword units, which is especially beneficial in\nlanguages with complicated morphology or a vast vocabulary. It is useful in\ncapturing the intricate structure of words in Indian languages (ILs), such as\nprefixes, suffixes, and other morphological variations. These languages\nfrequently use agglutinative structures, in which words are formed by the\ncombination of multiple morphemes such as suffixes, prefixes, and stems. As a\nresult, a suitable tokenization strategy must be chosen to address these\nscenarios. This paper examines how different subword tokenization techniques,\nsuch as SentencePiece, Byte Pair Encoding (BPE), and WordPiece Tokenization,\naffect ILs. The effectiveness of these subword tokenization techniques is\ninvestigated in statistical, neural, and multilingual neural machine\ntranslation models. All models are examined using standard evaluation metrics,\nsuch as the Bilingual Evaluation Understudy (BLEU) score, TER, METEOR, CHRF,\nRIBES, and COMET. Based on the results, it appears that for the majority of\nlanguage pairs for the Statistical and Neural MT models, the SentencePiece\ntokenizer continuously performed better than other tokenizers in terms of BLEU\nscore. However, BPE tokenization outperformed other tokenization techniques in\nthe context of Multilingual Neural Machine Translation model. The results show\nthat, despite using the same tokenizer and dataset for each model, translations\nfrom ILs to English surpassed translations from English to ILs."}
{"id": "2505.16925", "pdf": "https://arxiv.org/pdf/2505.16925", "abs": "https://arxiv.org/abs/2505.16925", "authors": ["Igor Udovichenko", "Olivier Croissant", "Anita Toleutaeva", "Evgeny Burnaev", "Alexander Korotin"], "title": "Risk-Averse Reinforcement Learning with Itakura-Saito Loss", "categories": ["cs.LG"], "comment": null, "summary": "Risk-averse reinforcement learning finds application in various high-stakes\nfields. Unlike classical reinforcement learning, which aims to maximize\nexpected returns, risk-averse agents choose policies that minimize risk,\noccasionally sacrificing expected value. These preferences can be framed\nthrough utility theory. We focus on the specific case of the exponential\nutility function, where we can derive the Bellman equations and employ various\nreinforcement learning algorithms with few modifications. However, these\nmethods suffer from numerical instability due to the need for exponent\ncomputation throughout the process. To address this, we introduce a numerically\nstable and mathematically sound loss function based on the Itakura-Saito\ndivergence for learning state-value and action-value functions. We evaluate our\nproposed loss function against established alternatives, both theoretically and\nempirically. In the experimental section, we explore multiple financial\nscenarios, some with known analytical solutions, and show that our loss\nfunction outperforms the alternatives."}
{"id": "2505.16175", "pdf": "https://arxiv.org/pdf/2505.16175", "abs": "https://arxiv.org/abs/2505.16175", "authors": ["Benjamin Schneider", "Dongfu Jiang", "Chao Du", "Tianyu Pang", "Wenhu Chen"], "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm Co-Design", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 6 figures, 2 tables", "summary": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice."}
{"id": "2505.16869", "pdf": "https://arxiv.org/pdf/2505.16869", "abs": "https://arxiv.org/abs/2505.16869", "authors": ["Weixiang Zhao", "Yulin Hu", "Yang Deng", "Tongtong Wu", "Wenxuan Zhang", "Jiahe Guo", "An Zhang", "Yanyan Zhao", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "title": "MPO: Multilingual Safety Alignment via Reward Gap Optimization", "categories": ["cs.CL"], "comment": "To Appear at ACL 2025 (Main)", "summary": "Large language models (LLMs) have become increasingly central to AI\napplications worldwide, necessitating robust multilingual safety alignment to\nensure secure deployment across diverse linguistic contexts. Existing\npreference learning methods for safety alignment, such as RLHF and DPO, are\nprimarily monolingual and struggle with noisy multilingual data. To address\nthese limitations, we introduce Multilingual reward gaP Optimization (MPO), a\nnovel approach that leverages the well-aligned safety capabilities of the\ndominant language (English) to improve safety alignment across multiple\nlanguages. MPO directly minimizes the reward gap difference between the\ndominant language and target languages, effectively transferring safety\ncapabilities while preserving the original strengths of the dominant language.\nExtensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate\nMPO's efficacy in multilingual safety alignment without degrading general\nmultilingual utility."}
{"id": "2505.16932", "pdf": "https://arxiv.org/pdf/2505.16932", "abs": "https://arxiv.org/abs/2505.16932", "authors": ["Noah Amsel", "David Persson", "Christopher Musco", "Robert Gower"], "title": "The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NA", "math.NA", "math.OC"], "comment": null, "summary": "Computing the polar decomposition and the related matrix sign function, has\nbeen a well-studied problem in numerical analysis for decades. More recently,\nit has emerged as an important subroutine in deep learning, particularly within\nthe Muon optimization framework. However, the requirements in this setting\ndiffer significantly from those of traditional numerical analysis. In deep\nlearning, methods must be highly efficient and GPU-compatible, but high\naccuracy is often unnecessary. As a result, classical algorithms like\nNewton-Schulz (which suffers from slow initial convergence) and methods based\non rational functions (which rely on QR decompositions or matrix inverses) are\npoorly suited to this context. In this work, we introduce Polar Express, a\nGPU-friendly algorithm for computing the polar decomposition. Like classical\npolynomial methods such as Newton-Schulz, our approach uses only matrix-matrix\nmultiplications, making it GPU-compatible. Motivated by earlier work of Chen &\nChow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule\nat each iteration by solving a minimax optimization problem, and we prove that\nit enjoys a strong worst-case optimality guarantee. This property ensures both\nrapid early convergence and fast asymptotic convergence. We also address\nfinite-precision issues, making it stable in bfloat16 in practice. We apply\nPolar Express within the Muon optimization framework and show consistent\nimprovements in validation loss on large-scale models such as GPT-2,\noutperforming recent alternatives across a range of learning rates."}
{"id": "2505.16181", "pdf": "https://arxiv.org/pdf/2505.16181", "abs": "https://arxiv.org/abs/2505.16181", "authors": ["Mohammad Reza Taesiri", "Brandon Collins", "Logan Bolton", "Viet Dac Lai", "Franck Dernoncourt", "Trung Bui", "Anh Totti Nguyen"], "title": "Understanding Generative AI Capabilities in Everyday Image Editing Tasks", "categories": ["cs.CV", "cs.AI"], "comment": "Code and qualitative examples are available at:\n  https://psrdataset.github.io", "summary": "Generative AI (GenAI) holds significant promise for automating everyday image\nediting tasks, especially following the recent release of GPT-4o on March 25,\n2025. However, what subjects do people most often want edited? What kinds of\nediting actions do they want to perform (e.g., removing or stylizing the\nsubject)? Do people prefer precise edits with predictable outcomes or highly\ncreative ones? By understanding the characteristics of real-world requests and\nthe corresponding edits made by freelance photo-editing wizards, can we draw\nlessons for improving AI-based editors and determine which types of requests\ncan currently be handled successfully by AI editors? In this paper, we present\na unique study addressing these questions by analyzing 83k requests from the\npast 12 years (2013-2025) on the Reddit community, which collected 305k\nPSR-wizard edits. According to human ratings, approximately only 33% of\nrequests can be fulfilled by the best AI editors (including GPT-4o,\nGemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on\nlow-creativity requests that require precise editing than on more open-ended\ntasks. They often struggle to preserve the identity of people and animals, and\nfrequently make non-requested touch-ups. On the other side of the table, VLM\njudges (e.g., o1) perform differently from human judges and may prefer AI edits\nmore than human edits. Code and qualitative examples are available at:\nhttps://psrdataset.github.io"}
{"id": "2505.16881", "pdf": "https://arxiv.org/pdf/2505.16881", "abs": "https://arxiv.org/abs/2505.16881", "authors": ["Daniel F. Perez-Ramirez", "Dejan Kostic", "Magnus Boman"], "title": "CASTILLO: Characterizing Response Length Distributions of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Dataset available in\n  https://huggingface.co/datasets/danfperam/castillo and code is available in\n  https://github.com/DanielFPerez/castillo", "summary": "Efficiently managing compute resources for Large Language Model (LLM)\ninference remains challenging due to the inherently stochastic and variable\nlengths of autoregressive text generation. Accurately estimating response\nlengths in advance enables proactive resource allocation, yet existing\napproaches either bias text generation towards certain lengths or rely on\nassumptions that ignore model- and prompt-specific variability. We introduce\nCASTILLO, a dataset characterizing response length distributions across 13\nwidely-used open-source LLMs evaluated on seven distinct instruction-following\ncorpora. For each $\\langle$prompt, model$\\rangle$ sample pair, we generate 10\nindependent completions using fixed decoding hyper-parameters, record the token\nlength of each response, and publish summary statistics (mean, std-dev,\npercentiles), along with the shortest and longest completions, and the exact\ngeneration settings. Our analysis reveals significant inter- and intra-model\nvariability in response lengths (even under identical generation settings), as\nwell as model-specific behaviors and occurrences of partial text degeneration\nin only subsets of responses. CASTILLO enables the development of predictive\nmodels for proactive scheduling and provides a systematic framework for\nanalyzing model-specific generation behaviors. We publicly release the dataset\nand code to foster research at the intersection of generative language modeling\nand systems."}
{"id": "2505.16933", "pdf": "https://arxiv.org/pdf/2505.16933", "abs": "https://arxiv.org/abs/2505.16933", "authors": ["Zebin You", "Shen Nie", "Xiaolu Zhang", "Jun Hu", "Jun Zhou", "Zhiwu Lu", "Ji-Rong Wen", "Chongxuan Li"], "title": "LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large\nLanguage Model (MLLM) that integrates visual instruction tuning with masked\ndiffusion models, representing a departure from the autoregressive paradigms\ndominant in current multimodal approaches. Built upon LLaDA, a representative\nlarge language diffusion model, LLaDA-V incorporates a vision encoder and MLP\nconnector that projects visual features into the language embedding space,\nenabling effective multimodal alignment. Our empirical investigation reveals\nseveral intriguing results: First, LLaDA-V demonstrates promising multimodal\nperformance despite its language model being weaker on purely textual tasks\nthan counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same\ninstruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal\ntasks with better data scalability. It also narrows the performance gap to\nQwen2-VL, suggesting the effectiveness of its architecture for multimodal\ntasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal\nunderstanding compared to existing hybrid autoregressive-diffusion and purely\ndiffusion-based MLLMs. Our findings suggest that large language diffusion\nmodels show promise in multimodal contexts and warrant further investigation in\nfuture research. Project page and codes:\nhttps://ml-gsai.github.io/LLaDA-V-demo/."}
{"id": "2505.16187", "pdf": "https://arxiv.org/pdf/2505.16187", "abs": "https://arxiv.org/abs/2505.16187", "authors": ["Guanghe Li", "Junming Zhao", "Shengjie Wang", "Yang Gao"], "title": "EasyInsert: A Data-Efficient and Generalizable Insertion Policy", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Insertion task is highly challenging that requires robots to operate with\nexceptional precision in cluttered environments. Existing methods often have\npoor generalization capabilities. They typically function in restricted and\nstructured environments, and frequently fail when the plug and socket are far\napart, when the scene is densely cluttered, or when handling novel objects.\nThey also rely on strong assumptions such as access to CAD models or a digital\ntwin in simulation. To address this, we propose EasyInsert, a framework which\nleverages the human intuition that relative pose (delta pose) between plug and\nsocket is sufficient for successful insertion, and employs efficient and\nautomated real-world data collection with minimal human labor to train a\ngeneralizable model for relative pose prediction. During execution, EasyInsert\nfollows a coarse-to-fine execution procedure based on predicted delta pose, and\nsuccessfully performs various insertion tasks. EasyInsert demonstrates strong\nzero-shot generalization capability for unseen objects in cluttered\nenvironments, handling cases with significant initial pose deviations while\nmaintaining high sample efficiency and requiring little human effort. In\nreal-world experiments, with just 5 hours of training data, EasyInsert achieves\nover 90% success in zero-shot insertion for 13 out of 15 unseen novel objects,\nincluding challenging objects like Type-C cables, HDMI cables, and Ethernet\ncables. Furthermore, with only one human demonstration and 4 minutes of\nautomatically collected data for fine-tuning, it reaches over 90% success rate\nfor all 15 objects."}
{"id": "2505.16894", "pdf": "https://arxiv.org/pdf/2505.16894", "abs": "https://arxiv.org/abs/2505.16894", "authors": ["Zeyu Wei", "Shuo Wang", "Xiaohui Rong", "Xuemin Liu", "He Li"], "title": "Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Hallucinations -- plausible yet erroneous outputs -- remain a critical\nbarrier to reliable deployment of large language models (LLMs). We present the\nfirst systematic study linking hallucination incidence to internal-state drift\ninduced by incremental context injection. Using TruthfulQA, we construct two\n16-round \"titration\" tracks per question: one appends relevant but partially\nflawed snippets, the other injects deliberately misleading content. Across six\nopen-source LLMs, we track overt hallucination rates with a tri-perspective\ndetector and covert dynamics via cosine, entropy, JS and Spearman drifts of\nhidden states and attention maps. Results reveal (1) monotonic growth of\nhallucination frequency and representation drift that plateaus after 5--7\nrounds; (2) relevant context drives deeper semantic assimilation, producing\nhigh-confidence \"self-consistent\" hallucinations, whereas irrelevant context\ninduces topic-drift errors anchored by attention re-routing; and (3)\nconvergence of JS-Drift ($\\sim0.69$) and Spearman-Drift ($\\sim0$) marks an\n\"attention-locking\" threshold beyond which hallucinations solidify and become\nresistant to correction. Correlation analyses expose a seesaw between\nassimilation capacity and attention diffusion, clarifying size-dependent error\nmodes. These findings supply empirical foundations for intrinsic hallucination\nprediction and context-aware mitigation mechanisms."}
{"id": "2505.16936", "pdf": "https://arxiv.org/pdf/2505.16936", "abs": "https://arxiv.org/abs/2505.16936", "authors": ["Yizhuo Chen", "Tianchen Wang", "You Lyu", "Yanlan Hu", "Jinyang Li", "Tomoyoshi Kimura", "Hongjue Zhao", "Yigong Hu", "Denizhan Kara", "Tarek Abdelzaher"], "title": "SPAR: Self-supervised Placement-Aware Representation Learning for Multi-Node IoT Systems", "categories": ["cs.LG"], "comment": null, "summary": "This work develops the underpinnings of self-supervised placement-aware\nrepresentation learning given spatially-distributed (multi-view and multimodal)\nsensor observations, motivated by the need to represent external environmental\nstate in multi-sensor IoT systems in a manner that correctly distills spatial\nphenomena from the distributed multi-vantage observations. The objective of\nsensing in IoT systems is, in general, to collectively represent an externally\nobserved environment given multiple vantage points from which sensory\nobservations occur. Pretraining of models that help interpret sensor data must\ntherefore encode the relation between signals observed by sensors and the\nobservers' vantage points in order to attain a representation that encodes the\nobserved spatial phenomena in a manner informed by the specific placement of\nthe measuring instruments, while allowing arbitrary placement. The work\nsignificantly advances self-supervised model pretraining from IoT signals\nbeyond current solutions that often overlook the distinctive spatial nature of\nIoT data. Our framework explicitly learns the dependencies between measurements\nand geometric observer layouts and structural characteristics, guided by a core\ndesign principle: the duality between signals and observer positions. We\nfurther provide theoretical analyses from the perspectives of information\ntheory and occlusion-invariant representation learning to offer insight into\nthe rationale behind our design. Experiments on three real-world\ndatasets--covering vehicle monitoring, human activity recognition, and\nearthquake localization--demonstrate the superior generalizability and\nrobustness of our method across diverse modalities, sensor placements,\napplication-level inference tasks, and spatial scales."}
{"id": "2505.16192", "pdf": "https://arxiv.org/pdf/2505.16192", "abs": "https://arxiv.org/abs/2505.16192", "authors": ["Chaoya Jiang", "Yongrui Heng", "Wei Ye", "Han Yang", "Haiyang Xu", "Ming Yan", "Ji Zhang", "Fei Huang", "Shikun Zhang"], "title": "VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently, reasoning-based MLLMs have achieved a degree of success in\ngenerating long-form textual reasoning chains. However, they still struggle\nwith complex tasks that necessitate dynamic and iterative focusing on and\nrevisiting of visual regions to achieve precise grounding of textual reasoning\nin visual evidence. We introduce \\textbf{VLM-R$^3$} (\\textbf{V}isual\n\\textbf{L}anguage \\textbf{M}odel with \\textbf{R}egion \\textbf{R}ecognition and\n\\textbf{R}easoning), a framework that equips an MLLM with the ability to (i)\ndecide \\emph{when} additional visual evidence is needed, (ii) determine\n\\emph{where} to ground within the image, and (iii) seamlessly weave the\nrelevant sub-image content back into an interleaved chain-of-thought. The core\nof our method is \\textbf{Region-Conditioned Reinforcement Policy Optimization\n(R-GRPO)}, a training paradigm that rewards the model for selecting informative\nregions, formulating appropriate transformations (e.g.\\ crop, zoom), and\nintegrating the resulting visual context into subsequent reasoning steps. To\nbootstrap this policy, we compile a modest but carefully curated Visuo-Lingual\nInterleaved Rationale (VLIR) corpus that provides step-level supervision on\nregion selection and textual justification. Extensive experiments on MathVista,\nScienceQA, and other benchmarks show that VLM-R$^3$ sets a new state of the art\nin zero-shot and few-shot settings, with the largest gains appearing on\nquestions demanding subtle spatial reasoning or fine-grained visual cue\nextraction."}
{"id": "2505.16900", "pdf": "https://arxiv.org/pdf/2505.16900", "abs": "https://arxiv.org/abs/2505.16900", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "Beiwen Zhang", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "title": "Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "During the finetuning stage of text generation tasks, standard cross-entropy\nloss treats all tokens equally. This can lead models to overemphasize\nhigh-frequency, low-information tokens, neglecting lower-frequency tokens\ncrucial for specificity and informativeness in generated content. This paper\nintroduces a novel loss function, Power-Law Decay Loss (PDL), specifically\ndesigned to optimize the finetuning process for text generation. The core\nmotivation for PDL stems from observations in information theory and\nlinguistics: the informativeness of a token is often inversely proportional to\nits frequency of occurrence. PDL re-weights the contribution of each token in\nthe standard cross-entropy loss based on its frequency in the training corpus,\nfollowing a power-law decay. Specifically, the weights for high-frequency\ntokens are reduced, while low-frequency, information-dense tokens are assigned\nhigher weights. This mechanism guides the model during finetuning to focus more\non learning and generating tokens that convey specific and unique information,\nthereby enhancing the quality, diversity, and informativeness of the generated\ntext. We theoretically elaborate on the motivation and construction of PDL and\ndiscuss its potential applications and advantages across various text\ngeneration finetuning tasks, such as abstractive summarization, dialogue\nsystems, and style transfer."}
{"id": "2505.16941", "pdf": "https://arxiv.org/pdf/2505.16941", "abs": "https://arxiv.org/abs/2505.16941", "authors": ["Chao Pang", "Vincent Jeanselme", "Young Sang Choi", "Xinzhuo Jiang", "Zilin Jing", "Aparajita Kashyap", "Yuta Kobayashi", "Yanwei Li", "Florent Pollet", "Karthik Natarajan", "Shalmali Joshi"], "title": "FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Foundation models hold significant promise in healthcare, given their\ncapacity to extract meaningful representations independent of downstream tasks.\nThis property has enabled state-of-the-art performance across several clinical\napplications trained on structured electronic health record (EHR) data, even in\nsettings with limited labeled data, a prevalent challenge in healthcare.\nHowever, there is little consensus on these models' potential for clinical\nutility due to the lack of desiderata of comprehensive and meaningful tasks and\nsufficiently diverse evaluations to characterize the benefit over conventional\nsupervised learning. To address this gap, we propose a suite of clinically\nmeaningful tasks spanning patient outcomes, early prediction of acute and\nchronic conditions, including desiderata for robust evaluations. We evaluate\nstate-of-the-art foundation models on EHR data consisting of 5 million patients\nfrom Columbia University Irving Medical Center (CUMC), a large urban academic\nmedical center in New York City, across 14 clinically relevant tasks. We\nmeasure overall accuracy, calibration, and subpopulation performance to surface\ntradeoffs based on the choice of pre-training, tokenization, and data\nrepresentation strategies. Our study aims to advance the empirical evaluation\nof structured EHR foundation models and guide the development of future\nhealthcare foundation models."}
{"id": "2505.16195", "pdf": "https://arxiv.org/pdf/2505.16195", "abs": "https://arxiv.org/abs/2505.16195", "authors": ["Zhi Zhong", "Akira Takahashi", "Shuyang Cui", "Keisuke Toyama", "Shusuke Takahashi", "Yuki Mitsufuji"], "title": "SpecMaskFoley: Steering Pretrained Spectral Masked Generative Transformer Toward Synchronized Video-to-audio Synthesis via ControlNet", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.IV"], "comment": "4 pages, 2 figures, 2 tables. Demo page:\n  https://zzaudio.github.io/SpecMaskFoley_Demo/", "summary": "Foley synthesis aims to synthesize high-quality audio that is both\nsemantically and temporally aligned with video frames. Given its broad\napplication in creative industries, the task has gained increasing attention in\nthe research community. To avoid the non-trivial task of training audio\ngenerative models from scratch, adapting pretrained audio generative models for\nvideo-synchronized foley synthesis presents an attractive direction.\nControlNet, a method for adding fine-grained controls to pretrained generative\nmodels, has been applied to foley synthesis, but its use has been limited to\nhandcrafted human-readable temporal conditions. In contrast, from-scratch\nmodels achieved success by leveraging high-dimensional deep features extracted\nusing pretrained video encoders. We have observed a performance gap between\nControlNet-based and from-scratch foley models. To narrow this gap, we propose\nSpecMaskFoley, a method that steers the pretrained SpecMaskGIT model toward\nvideo-synchronized foley synthesis via ControlNet. To unlock the potential of a\nsingle ControlNet branch, we resolve the discrepancy between the temporal video\nfeatures and the time-frequency nature of the pretrained SpecMaskGIT via a\nfrequency-aware temporal feature aligner, eliminating the need for complicated\nconditioning mechanisms widely used in prior arts. Evaluations on a common\nfoley synthesis benchmark demonstrate that SpecMaskFoley could even outperform\nstrong from-scratch baselines, substantially advancing the development of\nControlNet-based foley synthesis models. Demo page:\nhttps://zzaudio.github.io/SpecMaskFoley_Demo/"}
{"id": "2505.16922", "pdf": "https://arxiv.org/pdf/2505.16922", "abs": "https://arxiv.org/abs/2505.16922", "authors": ["Ruihan Yang", "Caiqi Zhang", "Zhisong Zhang", "Xinting Huang", "Dong Yu", "Nigel Collier", "Deqing Yang"], "title": "UNCLE: Uncertainty Expressions in Long-Form Generation", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are prone to hallucination, particularly in\nlong-form generations. A promising direction to mitigate hallucination is to\nteach LLMs to express uncertainty explicitly when they lack sufficient\nknowledge. However, existing work lacks direct and fair evaluation of LLMs'\nability to express uncertainty effectively in long-form generation. To address\nthis gap, we first introduce UNCLE, a benchmark designed to evaluate\nuncertainty expression in both long- and short-form question answering (QA).\nUNCLE spans five domains and comprises 4k long-form QA instances and over 20k\nshort-form QA pairs. Our dataset is the first to directly bridge short- and\nlong-form QA with paired questions and gold-standard answers. Along with the\nbenchmark, we propose a suite of new metrics to assess the models' capabilities\nto selectively express uncertainty. Using UNCLE, we then demonstrate that\ncurrent models fail to convey uncertainty appropriately in long-form\ngeneration. We further explore both prompt-based and training-based methods to\nimprove models' performance, with the training-based methods yielding greater\ngains. Further analysis of alignment gaps between short- and long-form\nuncertainty expression highlights promising directions for future research\nusing UNCLE."}
{"id": "2505.16947", "pdf": "https://arxiv.org/pdf/2505.16947", "abs": "https://arxiv.org/abs/2505.16947", "authors": ["Csaba Dékány", "Stefan Balauca", "Robin Staab", "Dimitar I. Dimitrov", "Martin Vechev"], "title": "MixAT: Combining Continuous and Discrete Adversarial Training for LLMs", "categories": ["cs.LG", "cs.AI", "I.2.7; K.4.1"], "comment": null, "summary": "Despite recent efforts in Large Language Models (LLMs) safety and alignment,\ncurrent adversarial attacks on frontier LLMs are still able to force harmful\ngenerations consistently. Although adversarial training has been widely studied\nand shown to significantly improve the robustness of traditional machine\nlearning models, its strengths and weaknesses in the context of LLMs are less\nunderstood. Specifically, while existing discrete adversarial attacks are\neffective at producing harmful content, training LLMs with concrete adversarial\nprompts is often computationally expensive, leading to reliance on continuous\nrelaxations. As these relaxations do not correspond to discrete input tokens,\nsuch latent training methods often leave models vulnerable to a diverse set of\ndiscrete attacks. In this work, we aim to bridge this gap by introducing MixAT,\na novel method that combines stronger discrete and faster continuous attacks\nduring training. We rigorously evaluate MixAT across a wide spectrum of\nstate-of-the-art attacks, proposing the At Least One Attack Success Rate\n(ALO-ASR) metric to capture the worst-case vulnerability of models. We show\nMixAT achieves substantially better robustness (ALO-ASR < 20%) compared to\nprior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to\nmethods based on continuous relaxations. We further analyze MixAT in realistic\ndeployment settings, exploring how chat templates, quantization, low-rank\nadapters, and temperature affect both adversarial training and evaluation,\nrevealing additional blind spots in current methodologies. Our results\ndemonstrate that MixAT's discrete-continuous defense offers a principled and\nsuperior robustness-accuracy tradeoff with minimal computational overhead,\nhighlighting its promise for building safer LLMs. We provide our code and\nmodels at https://github.com/insait-institute/MixAT."}
{"id": "2505.16196", "pdf": "https://arxiv.org/pdf/2505.16196", "abs": "https://arxiv.org/abs/2505.16196", "authors": ["Xuewu Lin", "Tianwei Lin", "Lichao Huang", "Hongyu Xie", "Yiwei Jin", "Keyu Li", "Zhizhong Su"], "title": "SEM: Enhancing Spatial Understanding for Robust Robot Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "A key challenge in robot manipulation lies in developing policy models with\nstrong spatial understanding, the ability to reason about 3D geometry, object\nrelations, and robot embodiment. Existing methods often fall short: 3D point\ncloud models lack semantic abstraction, while 2D image encoders struggle with\nspatial reasoning. To address this, we propose SEM (Spatial Enhanced\nManipulation model), a novel diffusion-based policy framework that explicitly\nenhances spatial understanding from two complementary perspectives. A spatial\nenhancer augments visual representations with 3D geometric context, while a\nrobot state encoder captures embodiment-aware structure through graphbased\nmodeling of joint dependencies. By integrating these modules, SEM significantly\nimproves spatial understanding, leading to robust and generalizable\nmanipulation across diverse tasks that outperform existing baselines."}
{"id": "2505.16927", "pdf": "https://arxiv.org/pdf/2505.16927", "abs": "https://arxiv.org/abs/2505.16927", "authors": ["Keshav Ramji", "Tahira Naseem", "Ramón Fernandez Astudillo"], "title": "Latent Principle Discovery for Language Model Self-Improvement", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "When language model (LM) users aim to improve the quality of its generations,\nit is crucial to specify concrete behavioral attributes that the model should\nstrive to reflect. However, curating such principles across many domains, even\nnon-exhaustively, requires a labor-intensive annotation process. To automate\nthis process, we propose eliciting these latent attributes guiding model\nreasoning towards human-preferred responses by explicitly modeling them in a\nself-correction setting. Our approach mines new principles from the LM itself\nand compresses the discovered elements to an interpretable set via clustering.\nSpecifically, we employ an approximation of posterior-regularized Monte Carlo\nExpectation-Maximization to both identify a condensed set of the most effective\nlatent principles and teach the LM to strategically invoke them in order to\nintrinsically refine its responses. We demonstrate that bootstrapping our\nalgorithm over multiple iterations enables smaller language models (7-8B\nparameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an\naverage of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on\nIFEval. We also show that clustering the principles yields interpretable and\ndiverse model-generated constitutions while retaining model performance. The\ngains our method achieves highlight the potential of automated,\nprinciple-driven post-training recipes toward continual self-improvement."}
{"id": "2505.16950", "pdf": "https://arxiv.org/pdf/2505.16950", "abs": "https://arxiv.org/abs/2505.16950", "authors": ["Adnan Oomerjee", "Zafeirios Fountas", "Zhongwei Yu", "Haitham Bou-Ammar", "Jun Wang"], "title": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised Reasoning", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome."}
{"id": "2505.16208", "pdf": "https://arxiv.org/pdf/2505.16208", "abs": "https://arxiv.org/abs/2505.16208", "authors": ["Anton Erofeev", "Balasubramanya T. Nadiga", "Ilya Timofeyev"], "title": "Using Echo-State Networks to Reproduce Rare Events in Chaotic Systems", "categories": ["nlin.CD", "cs.AI", "cs.LG", "math.DS", "37N99, 68T30"], "comment": null, "summary": "We apply the Echo-State Networks to predict the time series and statistical\nproperties of the competitive Lotka-Volterra model in the chaotic regime. In\nparticular, we demonstrate that Echo-State Networks successfully learn the\nchaotic attractor of the competitive Lotka-Volterra model and reproduce\nhistograms of dependent variables, including tails and rare events. We use the\nGeneralized Extreme Value distribution to quantify the tail behavior."}
{"id": "2505.16931", "pdf": "https://arxiv.org/pdf/2505.16931", "abs": "https://arxiv.org/abs/2505.16931", "authors": ["Matthew Zent", "Digory Smith", "Simon Woodhead"], "title": "PIIvot: A Lightweight NLP Anonymization Framework for Question-Anchored Tutoring Dialogues", "categories": ["cs.CL"], "comment": "6 pages, 2 figures, submitted to EMNLP 2025, for associated dataset,\n  see\n  https://huggingface.co/datasets/Eedi/Question-Anchored-Tutoring-Dialogues-2k", "summary": "Personally identifiable information (PII) anonymization is a high-stakes task\nthat poses a barrier to many open-science data sharing initiatives. While PII\nidentification has made large strides in recent years, in practice, error\nthresholds and the recall/precision trade-off still limit the uptake of these\nanonymization pipelines. We present PIIvot, a lighter-weight framework for PII\nanonymization that leverages knowledge of the data context to simplify the PII\ndetection problem. To demonstrate its effectiveness, we also contribute\nQATD-2k, the largest open-source real-world tutoring dataset of its kind, to\nsupport the demand for quality educational dialogue data."}
{"id": "2505.16952", "pdf": "https://arxiv.org/pdf/2505.16952", "abs": "https://arxiv.org/abs/2505.16952", "authors": ["Shengyu Feng", "Weiwei Sun", "Shanda Li", "Ameet Talwalkar", "Yiming Yang"], "title": "A Comprehensive Evaluation of Contemporary ML-Based Solvers for Combinatorial Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Machine learning (ML) has demonstrated considerable potential in supporting\nmodel design and optimization for combinatorial optimization (CO) problems.\nHowever, much of the progress to date has been evaluated on small-scale,\nsynthetic datasets, raising concerns about the practical effectiveness of\nML-based solvers in real-world, large-scale CO scenarios. Additionally, many\nexisting CO benchmarks lack sufficient training data, limiting their utility\nfor evaluating data-driven approaches. To address these limitations, we\nintroduce FrontierCO, a comprehensive benchmark that covers eight canonical CO\nproblem types and evaluates 16 representative ML-based solvers--including graph\nneural networks and large language model (LLM) agents. FrontierCO features\nchallenging instances drawn from industrial applications and frontier CO\nresearch, offering both realistic problem difficulty and abundant training\ndata. Our empirical results provide critical insights into the strengths and\nlimitations of current ML methods, helping to guide more robust and practically\nrelevant advances at the intersection of machine learning and combinatorial\noptimization. Our data is available at\nhttps://huggingface.co/datasets/CO-Bench/FrontierCO."}
{"id": "2505.16210", "pdf": "https://arxiv.org/pdf/2505.16210", "abs": "https://arxiv.org/abs/2505.16210", "authors": ["Zhihang Cai", "Xingjun Zhang", "Zhendong Tan", "Zheng Wei"], "title": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "11 pages, 9 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used."}
{"id": "2505.16934", "pdf": "https://arxiv.org/pdf/2505.16934", "abs": "https://arxiv.org/abs/2505.16934", "authors": ["Yepeng Liu", "Xuandong Zhao", "Christopher Kruegel", "Dawn Song", "Yuheng Bu"], "title": "In-Context Watermarks for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The growing use of large language models (LLMs) for sensitive applications\nhas highlighted the need for effective watermarking techniques to ensure the\nprovenance and accountability of AI-generated text. However, most existing\nwatermarking methods require access to the decoding process, limiting their\napplicability in real-world settings. One illustrative example is the use of\nLLMs by dishonest reviewers in the context of academic peer review, where\nconference organizers have no access to the model used but still need to detect\nAI-generated reviews. Motivated by this gap, we introduce In-Context\nWatermarking (ICW), which embeds watermarks into generated text solely through\nprompt engineering, leveraging LLMs' in-context learning and\ninstruction-following abilities. We investigate four ICW strategies at\ndifferent levels of granularity, each paired with a tailored detection method.\nWe further examine the Indirect Prompt Injection (IPI) setting as a specific\ncase study, in which watermarking is covertly triggered by modifying input\ndocuments such as academic manuscripts. Our experiments validate the\nfeasibility of ICW as a model-agnostic, practical watermarking approach.\nMoreover, our findings suggest that as LLMs become more capable, ICW offers a\npromising direction for scalable and accessible content attribution."}
{"id": "2505.16953", "pdf": "https://arxiv.org/pdf/2505.16953", "abs": "https://arxiv.org/abs/2505.16953", "authors": ["Young Sang Choi", "Vincent Jeanselme", "Pierre Elias", "Shalmali Joshi"], "title": "ICYM2I: The illusion of multimodal informativeness under missingness", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Multimodal learning is of continued interest in artificial intelligence-based\napplications, motivated by the potential information gain from combining\ndifferent types of data. However, modalities collected and curated during\ndevelopment may differ from the modalities available at deployment due to\nmultiple factors including cost, hardware failure, or -- as we argue in this\nwork -- the perceived informativeness of a given modality. Na{\\\"i}ve estimation\nof the information gain associated with including an additional modality\nwithout accounting for missingness may result in improper estimates of that\nmodality's value in downstream tasks. Our work formalizes the problem of\nmissingness in multimodal learning and demonstrates the biases resulting from\nignoring this process. To address this issue, we introduce ICYM2I (In Case You\nMultimodal Missed It), a framework for the evaluation of predictive performance\nand information gain under missingness through inverse probability\nweighting-based correction. We demonstrate the importance of the proposed\nadjustment to estimate information gain under missingness on synthetic,\nsemi-synthetic, and real-world medical datasets."}
{"id": "2505.16211", "pdf": "https://arxiv.org/pdf/2505.16211", "abs": "https://arxiv.org/abs/2505.16211", "authors": ["Kai Li", "Can Shen", "Yile Liu", "Jirui Han", "Kelong Zheng", "Xuechao Zou", "Zhe Wang", "Xingjian Du", "Shun Zhang", "Hanjun Luo", "Yingbin Jin", "Xinxin Xing", "Ziyang Ma", "Yue Liu", "Xiaojun Jia", "Yifan Zhang", "Junfeng Fang", "Kun Wang", "Yibo Yan", "Haoyang Li", "Yiming Li", "Xiaobin Zhuang", "Yang Liu", "Haibo Hu", "Zhuo Chen", "Zhizheng Wu", "Xiaolin Hu", "Eng-Siong Chng", "XiaoFeng Wang", "Wenyuan Xu", "Wei Dong", "Xinfeng Li"], "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "Technical Report", "summary": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust."}
{"id": "2505.16956", "pdf": "https://arxiv.org/pdf/2505.16956", "abs": "https://arxiv.org/abs/2505.16956", "authors": ["Daniil Gurgurov", "Michal Gregor", "Josef van Genabith", "Simon Ostermann"], "title": "On Multilingual Encoder Language Model Compression for Low-Resource Languages", "categories": ["cs.CL"], "comment": "Pre-print", "summary": "In this paper, we combine two-step knowledge distillation, structured\npruning, truncation, and vocabulary trimming for extremely compressing\nmultilingual encoder-only language models for low-resource languages. Our novel\napproach systematically combines existing techniques and takes them to the\nextreme, reducing layer depth, feed-forward hidden size, and intermediate layer\nembedding size to create significantly smaller monolingual models while\nretaining essential language-specific knowledge. We achieve compression rates\nof up to 92% with only a marginal performance drop of 2-10% in four downstream\ntasks, including sentiment analysis, topic classification, named entity\nrecognition, and part-of-speech tagging, across three low-resource languages.\nNotably, the performance degradation correlates with the amount of\nlanguage-specific data in the teacher model, with larger datasets resulting in\nsmaller performance losses. Additionally, we conduct extensive ablation studies\nto identify best practices for multilingual model compression using these\ntechniques."}
{"id": "2505.16959", "pdf": "https://arxiv.org/pdf/2505.16959", "abs": "https://arxiv.org/abs/2505.16959", "authors": ["Alessandro Favero", "Antonio Sclocchi", "Matthieu Wyart"], "title": "Bigger Isn't Always Memorizing: Early Stopping Overparameterized Diffusion Models", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Diffusion probabilistic models have become a cornerstone of modern generative\nAI, yet the mechanisms underlying their generalization remain poorly\nunderstood. In fact, if these models were perfectly minimizing their training\nloss, they would just generate data belonging to their training set, i.e.,\nmemorize, as empirically found in the overparameterized regime. We revisit this\nview by showing that, in highly overparameterized diffusion models,\ngeneralization in natural data domains is progressively achieved during\ntraining before the onset of memorization. Our results, ranging from image to\nlanguage diffusion models, systematically support the empirical law that\nmemorization time is proportional to the dataset size. Generalization vs.\nmemorization is then best understood as a competition between time scales. We\nshow that this phenomenology is recovered in diffusion models learning a simple\nprobabilistic context-free grammar with random rules, where generalization\ncorresponds to the hierarchical acquisition of deeper grammar rules as training\ntime grows, and the generalization cost of early stopping can be characterized.\nWe summarize these results in a phase diagram. Overall, our results support\nthat a principled early-stopping criterion - scaling with dataset size - can\neffectively optimize generalization while avoiding memorization, with direct\nimplications for hyperparameter transfer and privacy-sensitive applications."}
{"id": "2505.16227", "pdf": "https://arxiv.org/pdf/2505.16227", "abs": "https://arxiv.org/abs/2505.16227", "authors": ["Bohao Wu", "Qingyun Wang", "Yue Guo"], "title": "Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Personalizing jargon detection and explanation is essential for making\ntechnical documents accessible to readers with diverse disciplinary\nbackgrounds. However, tailoring models to individual users typically requires\nsubstantial annotation efforts and computational resources due to user-specific\nfinetuning. To address this, we present a systematic study of personalized\njargon detection, focusing on methods that are both efficient and scalable for\nreal-world deployment. We explore two personalization strategies: (1)\nlightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,\nand (2) personalized prompting, which tailors model behavior at inference time\nwithout retaining. To reflect realistic constraints, we also investigate hybrid\napproaches that combine limited annotated data with unsupervised user\nbackground signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in\nF1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,\nour method achieves comparable performance using only 10% of the annotated\ntraining data, demonstrating its practicality for resource-constrained\nsettings. Our study offers the first work to systematically explore efficient,\nlow-resource personalization of jargon detection using open-source language\nmodels, offering a practical path toward scalable, user-adaptive NLP system."}
{"id": "2505.16965", "pdf": "https://arxiv.org/pdf/2505.16965", "abs": "https://arxiv.org/abs/2505.16965", "authors": ["Fengyi Li", "Kayhan Behdin", "Natesh Pillai", "Xiaofeng Wang", "Zhipeng Wang", "Ercan Yildiz"], "title": "BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Text segmentation based on the semantic meaning of sentences is a fundamental\ntask with broad utility in many downstream applications. In this paper, we\npropose a graphical model-based unsupervised learning approach, named BP-Seg\nfor efficient text segmentation. Our method not only considers local coherence,\ncapturing the intuition that adjacent sentences are often more related, but\nalso effectively groups sentences that are distant in the text yet semantically\nsimilar. This is achieved through belief propagation on the carefully\nconstructed graphical models. Experimental results on both an illustrative\nexample and a dataset with long-form documents demonstrate that our method\nperforms favorably compared to competing approaches."}
{"id": "2505.16984", "pdf": "https://arxiv.org/pdf/2505.16984", "abs": "https://arxiv.org/abs/2505.16984", "authors": ["Mingyang Liu", "Gabriele Farina", "Asuman Ozdaglar"], "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Post-training has demonstrated its importance in enhancing the reasoning\ncapabilities of large language models (LLMs). The primary post-training methods\ncan be categorized into supervised fine-tuning (SFT) and reinforcement\nfine-tuning (RFT). SFT is efficient and well-suited for small language models,\nbut it may lead to overfitting and limit the reasoning abilities of larger\nmodels. In contrast, RFT generally yields better generalization but depends\nheavily on the strength of the base model. To address the limitations of SFT\nand RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm\nthat unifies SFT and RFT into a single, integrated process. UFT enables the\nmodel to effectively explore solutions while incorporating informative\nsupervision signals, bridging the gap between memorizing and thinking\nunderlying existing methods. Notably, UFT outperforms both SFT and RFT in\ngeneral, regardless of model sizes. Furthermore, we theoretically prove that\nUFT breaks RFT's inherent exponential sample complexity bottleneck, showing for\nthe first time that unified training can exponentially accelerate convergence\non long-horizon reasoning tasks."}
{"id": "2505.16234", "pdf": "https://arxiv.org/pdf/2505.16234", "abs": "https://arxiv.org/abs/2505.16234", "authors": ["Wei Zhang", "Zhenhong Zhou", "Junfeng Fang", "Rongwu Xu", "Kun Wang", "Yuanhe Zhang", "Rui Wang", "Ge Zhang", "Xinfeng Li", "Li Sun", "Lingjuan Lyu", "Yang Liu", "Sen Su"], "title": "LIFEBench: Evaluating Length Instruction Following in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "81 pages, 22 tables, 32 figures. Homepage:\n  https://ydyjya.github.io/LIFEBench/", "summary": "While large language models (LLMs) can solve PhD-level reasoning problems\nover long context inputs, they still struggle with a seemingly simpler task:\nfollowing explicit length instructions-e.g., write a 10,000-word novel.\nAdditionally, models often generate far too short outputs, terminate\nprematurely, or even refuse the request. Existing benchmarks focus primarily on\nevaluating generations quality, but often overlook whether the generations meet\nlength constraints. To this end, we introduce Length Instruction Following\nEvaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to\nfollow length instructions across diverse tasks and a wide range of specified\nlengths. LIFEBench consists of 10,800 instances across 4 task categories in\nboth English and Chinese, covering length constraints ranging from 16 to 8192\nwords. We evaluate 26 widely-used LLMs and find that most models reasonably\nfollow short-length instructions but deteriorate sharply beyond a certain\nthreshold. Surprisingly, almost all models fail to reach the vendor-claimed\nmaximum output lengths in practice, as further confirmed by our evaluations\nextending up to 32K words. Even long-context LLMs, despite their extended\ninput-output windows, counterintuitively fail to improve length-instructions\nfollowing. Notably, Reasoning LLMs outperform even specialized long-text\ngeneration models, achieving state-of-the-art length following. Overall,\nLIFEBench uncovers fundamental limitations in current LLMs' length instructions\nfollowing ability, offering critical insights for future progress."}
{"id": "2505.16972", "pdf": "https://arxiv.org/pdf/2505.16972", "abs": "https://arxiv.org/abs/2505.16972", "authors": ["Tianduo Wang", "Lu Xu", "Wei Lu", "Shanbo Cheng"], "title": "From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent advances in Automatic Speech Recognition (ASR) have been largely\nfueled by massive speech corpora. However, extending coverage to diverse\nlanguages with limited resources remains a formidable challenge. This paper\nintroduces Speech Back-Translation, a scalable pipeline that improves\nmultilingual ASR models by converting large-scale text corpora into synthetic\nspeech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just\ntens of hours of real transcribed speech can effectively train TTS models to\ngenerate synthetic speech at hundreds of times the original volume while\nmaintaining high quality. To evaluate synthetic speech quality, we develop an\nintelligibility-based assessment framework and establish clear thresholds for\nwhen synthetic data benefits ASR training. Using Speech Back-Translation, we\ngenerate more than 500,000 hours of synthetic speech in ten languages and\ncontinue pre-training Whisper-large-v3, achieving average transcription error\nreductions of over 30\\%. These results highlight the scalability and\neffectiveness of Speech Back-Translation for enhancing multilingual ASR\nsystems."}
{"id": "2505.16992", "pdf": "https://arxiv.org/pdf/2505.16992", "abs": "https://arxiv.org/abs/2505.16992", "authors": ["Aleksandra Franz", "Hao Wei", "Luca Guastoni", "Nils Thuerey"], "title": "PICT -- A Differentiable, GPU-Accelerated Multi-Block PISO Solver for Simulation-Coupled Learning Tasks in Fluid Dynamics", "categories": ["cs.LG", "physics.comp-ph"], "comment": "Source code at https://github.com/tum-pbs/PICT", "summary": "Despite decades of advancements, the simulation of fluids remains one of the\nmost challenging areas of in scientific computing. Supported by the necessity\nof gradient information in deep learning, differentiable simulators have\nemerged as an effective tool for optimization and learning in physics\nsimulations. In this work, we present our fluid simulator PICT, a\ndifferentiable pressure-implicit solver coded in PyTorch with\nGraphics-processing-unit (GPU) support. We first verify the accuracy of both\nthe forward simulation and our derived gradients in various established\nbenchmarks like lid-driven cavities and turbulent channel flows before we show\nthat the gradients provided by our solver can be used to learn complicated\nturbulence models in 2D and 3D. We apply both supervised and unsupervised\ntraining regimes using physical priors to match flow statistics. In particular,\nwe learn a stable sub-grid scale (SGS) model for a 3D turbulent channel flow\npurely based on reference statistics. The low-resolution corrector trained with\nour solver runs substantially faster than the highly resolved references, while\nkeeping or even surpassing their accuracy. Finally, we give additional insights\ninto the physical interpretation of different solver gradients, and motivate a\nphysically informed regularization technique. To ensure that the full potential\nof PICT can be leveraged, it is published as open source:\nhttps://github.com/tum-pbs/PICT."}
{"id": "2505.16249", "pdf": "https://arxiv.org/pdf/2505.16249", "abs": "https://arxiv.org/abs/2505.16249", "authors": ["Zhen Zhang", "Xiangyu Chu", "Yunxi Tang", "Lulu Zhao", "Jing Huang", "Zhongliang Jiang", "K. W. Samuel Au"], "title": "Manipulating Elasto-Plastic Objects With 3D Occupancy and Learning-Based Predictive Control", "categories": ["cs.RO", "cs.AI"], "comment": "8 Pages, 5 figures, accepted for publication in IEEE Robotics and\n  Automation Letters (RA-L)", "summary": "Manipulating elasto-plastic objects remains a significant challenge due to\nsevere self-occlusion, difficulties of representation, and complicated\ndynamics. This work proposes a novel framework for elasto-plastic object\nmanipulation with a quasi-static assumption for motions, leveraging 3D\noccupancy to represent such objects, a learned dynamics model trained with 3D\noccupancy, and a learning-based predictive control algorithm to address these\nchallenges effectively. We build a novel data collection platform to collect\nfull spatial information and propose a pipeline for generating a 3D occupancy\ndataset. To infer the 3D occupancy during manipulation, an occupancy prediction\nnetwork is trained with multiple RGB images supervised by the generated\ndataset. We design a deep neural network empowered by a 3D convolution neural\nnetwork (CNN) and a graph neural network (GNN) to predict the complex\ndeformation with the inferred 3D occupancy results. A learning-based predictive\ncontrol algorithm is introduced to plan the robot actions, incorporating a\nnovel shape-based action initialization module specifically designed to improve\nthe planner efficiency. The proposed framework in this paper can successfully\nshape the elasto-plastic objects into a given goal shape and has been verified\nin various experiments both in simulation and the real world."}
{"id": "2505.16973", "pdf": "https://arxiv.org/pdf/2505.16973", "abs": "https://arxiv.org/abs/2505.16973", "authors": ["Rishanth Rajendhran", "Amir Zadeh", "Matthew Sarte", "Chuan Li", "Mohit Iyyer"], "title": "VeriFastScore: Speeding up long-form factuality evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Metrics like FactScore and VeriScore that evaluate long-form factuality\noperate by decomposing an input response into atomic claims and then\nindividually verifying each claim. While effective and interpretable, these\nmethods incur numerous LLM calls and can take upwards of 100 seconds to\nevaluate a single response, limiting their practicality in large-scale\nevaluation and training scenarios. To address this, we propose VeriFastScore,\nwhich leverages synthetic data to fine-tune Llama3.1 8B for simultaneously\nextracting and verifying all verifiable claims within a given text based on\nevidence from Google Search. We show that this task cannot be solved via\nfew-shot prompting with closed LLMs due to its complexity: the model receives\n~4K tokens of evidence on average and needs to concurrently decompose claims,\njudge their verifiability, and verify them against noisy evidence. However, our\nfine-tuned VeriFastScore model demonstrates strong correlation with the\noriginal VeriScore pipeline at both the example level (r=0.80) and system level\n(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence\nretrieval) over VeriScore. To facilitate future factuality research, we\npublicly release our VeriFastScore model and synthetic datasets."}
{"id": "2505.16996", "pdf": "https://arxiv.org/pdf/2505.16996", "abs": "https://arxiv.org/abs/2505.16996", "authors": ["Shalev Manor", "Mohammad Kohandel"], "title": "A Unified Framework for Simultaneous Parameter and Function Discovery in Differential Equations", "categories": ["cs.LG"], "comment": "13 pages, 8 figures", "summary": "Inverse problems involving differential equations often require identifying\nunknown parameters or functions from data. Existing approaches, such as\nPhysics-Informed Neural Networks (PINNs), Universal Differential Equations\n(UDEs) and Universal Physics-Informed Neural Networks (UPINNs), are effective\nat isolating either parameters or functions but can face challenges when\napplied simultaneously due to solution non-uniqueness. In this work, we\nintroduce a framework that addresses these limitations by establishing\nconditions under which unique solutions can be guaranteed. To illustrate, we\napply it to examples from biological systems and ecological dynamics,\ndemonstrating accurate and interpretable results. Our approach significantly\nenhances the potential of machine learning techniques in modeling complex\nsystems in science and engineering."}
{"id": "2505.16256", "pdf": "https://arxiv.org/pdf/2505.16256", "abs": "https://arxiv.org/abs/2505.16256", "authors": ["Yan Zhao", "Zhengxue Cheng", "Junxuan Zhang", "Qunshan Gu", "Qi Wang", "Li Song"], "title": "DualComp: End-to-End Learning of a Unified Dual-Modality Lossless Compressor", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "18 pages, 11 figures, 7 tables", "summary": "Most learning-based lossless compressors are designed for a single modality,\nrequiring separate models for multi-modal data and lacking flexibility.\nHowever, different modalities vary significantly in format and statistical\nproperties, making it ineffective to use compressors that lack\nmodality-specific adaptations. While multi-modal large language models (MLLMs)\noffer a potential solution for modality-unified compression, their excessive\ncomplexity hinders practical deployment. To address these challenges, we focus\non the two most common modalities, image and text, and propose DualComp, the\nfirst unified and lightweight learning-based dual-modality lossless compressor.\nBuilt on a lightweight backbone, DualComp incorporates three key structural\nenhancements to handle modality heterogeneity: modality-unified tokenization,\nmodality-switching contextual learning, and modality-routing\nmixture-of-experts. A reparameterization training strategy is also used to\nboost compression performance. DualComp integrates both modality-specific and\nshared parameters for efficient parameter utilization, enabling near real-time\ninference (200KB/s) on desktop CPUs. With much fewer parameters, DualComp\nachieves compression performance on par with the SOTA LLM-based methods for\nboth text and image datasets. Its simplified single-modality variant surpasses\nthe previous best image compressor on the Kodak dataset by about 9% using just\n1.2% of the model size."}
{"id": "2505.16983", "pdf": "https://arxiv.org/pdf/2505.16983", "abs": "https://arxiv.org/abs/2505.16983", "authors": ["Junlong Tong", "Jinlan Fu", "Zixuan Lin", "Yingqi Fan", "Anhao Zhao", "Hui Su", "Xiaoyu Shen"], "title": "LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large Language Models (LLMs) are primarily designed for batch processing.\nExisting methods for adapting LLMs to streaming rely either on expensive\nre-encoding or specialized architectures with limited scalability. This work\nidentifies three key mismatches in adapting batch-oriented LLMs to streaming:\n(1) input-attention, (2) output-attention, and (3) position-ID mismatches.\nWhile it is commonly assumed that the latter two mismatches require frequent\nre-encoding, our analysis reveals that only the input-attention mismatch\nsignificantly impacts performance, indicating re-encoding outputs is largely\nunnecessary. To better understand this discrepancy with the common assumption,\nwe provide the first comprehensive analysis of the impact of position encoding\non LLMs in streaming, showing that preserving relative positions within source\nand target contexts is more critical than maintaining absolute order. Motivated\nby the above analysis, we introduce a group position encoding paradigm built on\nbatch architectures to enhance consistency between streaming and batch modes.\nExtensive experiments on cross-lingual and cross-modal tasks demonstrate that\nour method outperforms existing approaches. Our method requires no\narchitectural modifications, exhibits strong generalization in both streaming\nand batch modes. The code is available at repository\nhttps://github.com/EIT-NLP/StreamingLLM."}
{"id": "2505.17004", "pdf": "https://arxiv.org/pdf/2505.17004", "abs": "https://arxiv.org/abs/2505.17004", "authors": ["Jiachen Yao", "Abbas Mammadov", "Julius Berner", "Gavin Kerrigan", "Jong Chul Ye", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "title": "Guided Diffusion Sampling on Function Spaces with Applications to PDEs", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "stat.ML"], "comment": null, "summary": "We propose a general framework for conditional sampling in PDE-based inverse\nproblems, targeting the recovery of whole solutions from extremely sparse or\nnoisy measurements. This is accomplished by a function-space diffusion model\nand plug-and-play guidance for conditioning. Our method first trains an\nunconditional discretization-agnostic denoising model using neural operator\narchitectures. At inference, we refine the samples to satisfy sparse\nobservation data via a gradient-based guidance mechanism. Through rigorous\nmathematical analysis, we extend Tweedie's formula to infinite-dimensional\nHilbert spaces, providing the theoretical foundation for our posterior sampling\napproach. Our method (FunDPS) accurately captures posterior distributions in\nfunction spaces under minimal supervision and severe data scarcity. Across five\nPDE tasks with only 3% observation, our method achieves an average 32% accuracy\nimprovement over state-of-the-art fixed-resolution diffusion baselines while\nreducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning\nensures strong cross-resolution generalizability. To the best of our knowledge,\nthis is the first diffusion-based framework to operate independently of\ndiscretization, offering a practical and flexible solution for forward and\ninverse problems in the context of PDEs. Code is available at\nhttps://github.com/neuraloperator/FunDPS"}
{"id": "2505.16258", "pdf": "https://arxiv.org/pdf/2505.16258", "abs": "https://arxiv.org/abs/2505.16258", "authors": ["Aashish Anantha Ramakrishnan", "Aadarsh Anantha Ramakrishnan", "Dongwon Lee"], "title": "IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50", "I.2.7; I.2.10"], "comment": null, "summary": "Interpreting figurative language such as sarcasm across multi-modal inputs\npresents unique challenges, often requiring task-specific fine-tuning and\nextensive reasoning steps. However, current Chain-of-Thought approaches do not\nefficiently leverage the same cognitive processes that enable humans to\nidentify sarcasm. We present IRONIC, an in-context learning framework that\nleverages Multi-modal Coherence Relations to analyze referential, analogical\nand pragmatic image-text linkages. Our experiments show that IRONIC achieves\nstate-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across\ndifferent baselines. This demonstrates the need for incorporating linguistic\nand cognitive insights into the design of multi-modal reasoning strategies. Our\ncode is available at: https://github.com/aashish2000/IRONIC"}
{"id": "2505.16986", "pdf": "https://arxiv.org/pdf/2505.16986", "abs": "https://arxiv.org/abs/2505.16986", "authors": ["Amartya Chakraborty", "Paresh Dashore", "Nadia Bathaee", "Anmol Jain", "Anirban Das", "Shi-Xiong Zhang", "Sambit Sahu", "Milind Naphade", "Genta Indra Winata"], "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios."}
{"id": "2505.17010", "pdf": "https://arxiv.org/pdf/2505.17010", "abs": "https://arxiv.org/abs/2505.17010", "authors": ["Tim Genewein", "Kevin Wenliang Li", "Jordi Grau-Moya", "Anian Ruoss", "Laurent Orseau", "Marcus Hutter"], "title": "Understanding Prompt Tuning and In-Context Learning via Meta-Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Prompting is one of the main ways to adapt a pretrained model to target\ntasks. Besides manually constructing prompts, many prompt optimization methods\nhave been proposed in the literature. Method development is mainly empirically\ndriven, with less emphasis on a conceptual understanding of prompting. In this\npaper we discuss how optimal prompting can be understood through a Bayesian\nview, which also implies some fundamental limitations of prompting that can\nonly be overcome by tuning weights. The paper explains in detail how\nmeta-trained neural networks behave as Bayesian predictors over the pretraining\ndistribution, whose hallmark feature is rapid in-context adaptation. Optimal\nprompting can be studied formally as conditioning these Bayesian predictors,\nyielding criteria for target tasks where optimal prompting is and is not\npossible. We support the theory with educational experiments on LSTMs and\nTransformers, where we compare different versions of prefix-tuning and\ndifferent weight-tuning methods. We also confirm that soft prefixes, which are\nsequences of real-valued vectors outside the token alphabet, can lead to very\neffective prompts for trained and even untrained networks by manipulating\nactivations in ways that are not achievable by hard tokens. This adds an\nimportant mechanistic aspect beyond the conceptual Bayesian theory."}
{"id": "2505.16259", "pdf": "https://arxiv.org/pdf/2505.16259", "abs": "https://arxiv.org/abs/2505.16259", "authors": ["Hayeon Bang", "Taegyun Kwon", "Juhan Nam"], "title": "Dialogue in Resonance: An Interactive Music Piece for Piano and Real-Time Automatic Transcription System", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "This paper presents <Dialogue in Resonance>, an interactive music piece for a\nhuman pianist and a computer-controlled piano that integrates real-time\nautomatic music transcription into a score-driven framework. Unlike previous\napproaches that primarily focus on improvisation-based interactions, our work\nestablishes a balanced framework that combines composed structure with dynamic\ninteraction. Through real-time automatic transcription as its core mechanism,\nthe computer interprets and responds to the human performer's input in real\ntime, creating a musical dialogue that balances compositional intent with live\ninteraction while incorporating elements of unpredictability. In this paper, we\npresent the development process from composition to premiere performance,\nincluding technical implementation, rehearsal process, and performance\nconsiderations."}
{"id": "2505.16988", "pdf": "https://arxiv.org/pdf/2505.16988", "abs": "https://arxiv.org/abs/2505.16988", "authors": ["Rui Ye", "Keduan Huang", "Qimin Wu", "Yuzhu Cai", "Tian Jin", "Xianghe Pang", "Xiangrui Liu", "Jiaqi Su", "Chen Qian", "Bohan Tang", "Kaiqu Liang", "Jiaao Chen", "Yue Hu", "Zhenfei Yin", "Rongye Shi", "Bo An", "Yang Gao", "Wenjun Wu", "Lei Bai", "Siheng Chen"], "title": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "18 pages, 11 figures", "summary": "LLM-based multi-agent systems (MAS) have demonstrated significant potential\nin enhancing single LLMs to address complex and diverse tasks in practical\napplications. Despite considerable advancements, the field lacks a unified\ncodebase that consolidates existing methods, resulting in redundant\nre-implementation efforts, unfair comparisons, and high entry barriers for\nresearchers. To address these challenges, we introduce MASLab, a unified,\ncomprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab\nintegrates over 20 established methods across multiple domains, each rigorously\nvalidated by comparing step-by-step outputs with its official implementation.\n(2) MASLab provides a unified environment with various benchmarks for fair\ncomparisons among methods, ensuring consistent inputs and standardized\nevaluation protocols. (3) MASLab implements methods within a shared streamlined\nstructure, lowering the barriers for understanding and extension. Building on\nMASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,\noffering researchers a clear and comprehensive view of the current landscape of\nMAS methods. MASLab will continue to evolve, tracking the latest developments\nin the field, and invite contributions from the broader open-source community."}
{"id": "2505.17013", "pdf": "https://arxiv.org/pdf/2505.17013", "abs": "https://arxiv.org/abs/2505.17013", "authors": ["Kevin Lu", "Nicky Kriplani", "Rohit Gandikota", "Minh Pham", "David Bau", "Chinmay Hegde", "Niv Cohen"], "title": "When Are Concepts Erased From Diffusion Models?", "categories": ["cs.LG", "cs.CV"], "comment": "Project Page:\n  https://nyu-dice-lab.github.io/when-are-concepts-erased/", "summary": "Concept erasure, the ability to selectively prevent a model from generating\nspecific concepts, has attracted growing interest, with various approaches\nemerging to address the challenge. However, it remains unclear how thoroughly\nthese methods erase the target concept. We begin by proposing two conceptual\nmodels for the erasure mechanism in diffusion models: (i) reducing the\nlikelihood of generating the target concept, and (ii) interfering with the\nmodel's internal guidance mechanisms. To thoroughly assess whether a concept\nhas been truly erased from the model, we introduce a suite of independent\nevaluations. Our evaluation framework includes adversarial attacks, novel\nprobing techniques, and analysis of the model's alternative generations in\nplace of the erased concept. Our results shed light on the tension between\nminimizing side effects and maintaining robustness to adversarial prompts.\nBroadly, our work underlines the importance of comprehensive evaluation for\nerasure in diffusion models."}
{"id": "2505.16270", "pdf": "https://arxiv.org/pdf/2505.16270", "abs": "https://arxiv.org/abs/2505.16270", "authors": ["Jiaru Zou", "Yikun Ban", "Zihao Li", "Yunzhe Qi", "Ruizhong Qiu", "Ling Yang", "Jingrui He"], "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "33 pages, 7 figures", "summary": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability."}
{"id": "2505.16995", "pdf": "https://arxiv.org/pdf/2505.16995", "abs": "https://arxiv.org/abs/2505.16995", "authors": ["Chao Zhang", "Xin Shi", "Xueqiao Zhang", "Yifan Zhu", "Yi Yang", "Yawei Luo"], "title": "DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Emotional Support Conversation (ESC) have improved\nemotional support generation by fine-tuning Large Language Models (LLMs) via\nSupervised Fine-Tuning (SFT). However, common psychological errors still\npersist. While Direct Preference Optimization (DPO) shows promise in reducing\nsuch errors through pairwise preference learning, its effectiveness in ESC\ntasks is limited by two key challenges: (1) Entangled data structure: Existing\nESC data inherently entangles psychological strategies and response content,\nmaking it difficult to construct high-quality preference pairs; and (2)\nOptimization ambiguity: Applying vanilla DPO to such entangled pairwise data\nleads to ambiguous training objectives. To address these issues, we introduce\nInferential Preference Mining (IPM) to construct high-quality preference data,\nforming the IPM-PrefDial dataset. Building upon this data, we propose a\nDecoupled ESC framework inspired by Gross's Extended Process Model of Emotion\nRegulation, which decomposes the ESC task into two sequential subtasks:\nstrategy planning and empathic response generation. Each was trained via SFT\nand subsequently enhanced by DPO to align with the psychological preference.\nExtensive experiments demonstrate that our Decoupled ESC framework outperforms\njoint optimization baselines, reducing preference bias and improving response\nquality."}
{"id": "2505.17016", "pdf": "https://arxiv.org/pdf/2505.17016", "abs": "https://arxiv.org/abs/2505.17016", "authors": ["Shuhan Tan", "Kairan Dou", "Yue Zhao", "Philipp Krähenbühl"], "title": "Interactive Post-Training for Vision-Language-Action Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "comment": "Project page: https://ariostgx.github.io/ript_vla/", "summary": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision."}
{"id": "2505.16278", "pdf": "https://arxiv.org/pdf/2505.16278", "abs": "https://arxiv.org/abs/2505.16278", "authors": ["Zhenjie Yang", "Yilin Chai", "Xiaosong Jia", "Qifeng Li", "Yuqian Shao", "Xuekai Zhu", "Haisheng Su", "Junchi Yan"], "title": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Project Page: https://thinklab-sjtu.github.io/DriveMoE/", "summary": "End-to-end autonomous driving (E2E-AD) demands effective processing of\nmulti-view sensory data and robust handling of diverse and complex driving\nscenarios, particularly rare maneuvers such as aggressive turns. Recent success\nof Mixture-of-Experts (MoE) architecture in Large Language Models (LLMs)\ndemonstrates that specialization of parameters enables strong scalability. In\nthis work, we propose DriveMoE, a novel MoE-based E2E-AD framework, with a\nScene-Specialized Vision MoE and a Skill-Specialized Action MoE. DriveMoE is\nbuilt upon our $\\pi_0$ Vision-Language-Action (VLA) baseline (originally from\nthe embodied AI field), called Drive-$\\pi_0$. Specifically, we add Vision MoE\nto Drive-$\\pi_0$ by training a router to select relevant cameras according to\nthe driving context dynamically. This design mirrors human driving cognition,\nwhere drivers selectively attend to crucial visual cues rather than\nexhaustively processing all visual information. In addition, we add Action MoE\nby training another router to activate specialized expert modules for different\ndriving behaviors. Through explicit behavioral specialization, DriveMoE is able\nto handle diverse scenarios without suffering from modes averaging like\nexisting models. In Bench2Drive closed-loop evaluation experiments, DriveMoE\nachieves state-of-the-art (SOTA) performance, demonstrating the effectiveness\nof combining vision and action MoE in autonomous driving tasks. We will release\nour code and models of DriveMoE and Drive-$\\pi_0$."}
{"id": "2505.16998", "pdf": "https://arxiv.org/pdf/2505.16998", "abs": "https://arxiv.org/abs/2505.16998", "authors": ["Jin Jiang", "Jianing Wang", "Yuchen Yan", "Yang Liu", "Jianhua Zhu", "Mengdi Zhang", "Xunliang Cai", "Liangcai Gao"], "title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval."}
{"id": "2505.14403", "pdf": "https://arxiv.org/pdf/2505.14403", "abs": "https://arxiv.org/abs/2505.14403", "authors": ["Zhaohui Yang", "Shilei Jiang", "Chen Hu", "Linjing Li", "Shihong Deng", "Daxin Jiang"], "title": "Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in reasoning language models have witnessed a paradigm shift\nfrom short to long CoT pattern. Given the substantial computational cost of\nrollouts in long CoT models, maximizing the utility of fixed training datasets\nbecomes crucial. Our analysis reveals that negative responses contain valuable\ncomponents such as self-reflection and error-correction steps, yet primary\nexisting methods either completely discard negative samples (RFT) or apply\nequal penalization across all tokens (RL), failing to leverage these potential\nlearning signals. In light of this, we propose Behavior Constrained Policy\nGradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline\nRL framework that encompasses three stages: 1) sample segmentation, 2)\nconsensus-based step correctness assessment combining LLM and PRM judgers, and\n3) policy optimization with NSA designed to effectively mine positive steps\nwithin negative samples. Experimental results show that BCPG-NSA outperforms\nbaselines on several challenging math/coding reasoning benchmarks using the\nsame training dataset, achieving improved sample efficiency and demonstrating\nrobustness and scalability when extended to multiple iterations."}
{"id": "2505.16290", "pdf": "https://arxiv.org/pdf/2505.16290", "abs": "https://arxiv.org/abs/2505.16290", "authors": ["Mohammad Rubyet Islam", "Peter Sandborn"], "title": "Multimodal Generative AI for Story Point Estimation in Software Development", "categories": ["cs.SE", "cs.AI", "68T07, 68T45", "I.2.6; I.2.10; D.2.9; H.2.8"], "comment": null, "summary": "This research explores the application of Multimodal Generative AI to enhance\nstory point estimation in Agile software development. By integrating text,\nimage, and categorical data using advanced models like BERT, CNN, and XGBoost,\nour approach surpasses the limitations of traditional single-modal estimation\nmethods. The results demonstrate strong accuracy for simpler story points,\nwhile also highlighting challenges in more complex categories due to data\nimbalance. This study further explores the impact of categorical data,\nparticularly severity, on the estimation process, emphasizing its influence on\nmodel performance. Our findings emphasize the transformative potential of\nmultimodal data integration in refining AI-driven project management, paving\nthe way for more precise, adaptable, and domain-specific AI capabilities.\nAdditionally, this work outlines future directions for addressing data\nvariability and enhancing the robustness of AI in Agile methodologies."}
{"id": "2505.17005", "pdf": "https://arxiv.org/pdf/2505.17005", "abs": "https://arxiv.org/abs/2505.17005", "authors": ["Huatong Song", "Jinhao Jiang", "Wenqing Tian", "Zhipeng Chen", "Yuhuan Wu", "Jiahao Zhao", "Yingqian Min", "Wayne Xin Zhao", "Lei Fang", "Ji-Rong Wen"], "title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus."}
{"id": "2505.14679", "pdf": "https://arxiv.org/pdf/2505.14679", "abs": "https://arxiv.org/abs/2505.14679", "authors": ["Xiaojie Gu", "Guangxu Chen", "Jungang Li", "Jia-Chen Gu", "Xuming Hu", "Kai Zhang"], "title": "UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Lifelong learning enables large language models (LLMs) to adapt to evolving\ninformation by continually updating their internal knowledge. An ideal system\nshould support efficient, wide-ranging updates while preserving existing\ncapabilities and ensuring reliable deployment. Model editing stands out as a\npromising solution for this goal, offering a focused and efficient way to\nrevise a model's internal knowledge. Although recent paradigms have made\nnotable progress, they often struggle to meet the demands of practical lifelong\nadaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally\nnew editing solution that is training-, subject- and memory-free, making it\nparticularly well-suited for ultra-scalable, real-world lifelong model editing.\nULTRAEDIT performs editing through a self-contained process that relies solely\non lightweight linear algebra operations to compute parameter shifts, enabling\nfast and consistent parameter modifications with minimal overhead. To improve\nscalability in lifelong settings, ULTRAEDIT employs a lifelong normalization\nstrategy that continuously updates feature statistics across turns, allowing it\nto adapt to distributional shifts and maintain consistency over time. ULTRAEDIT\nachieves editing speeds over 7x faster than the previous state-of-the-art\nmethod-which was also the fastest known approach-while consuming less than 1/3\nthe VRAM, making it the only method currently capable of editing a 7B LLM on a\n24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest\ndataset in the field to date, with over 2M editing pairs-and demonstrate that\nour method supports up to 1M edits while maintaining high accuracy.\nComprehensive experiments on four datasets and six models show that ULTRAEDIT\nconsistently achieves superior performance across diverse model editing\nscenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit."}
{"id": "2505.16301", "pdf": "https://arxiv.org/pdf/2505.16301", "abs": "https://arxiv.org/abs/2505.16301", "authors": ["Fuchun Ge", "Pavlo O. Dral"], "title": "Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space", "categories": ["physics.chem-ph", "cs.AI", "cs.LG"], "comment": null, "summary": "Molecular dynamics (MD) is a powerful tool for exploring the behavior of\natomistic systems, but its reliance on sequential numerical integration limits\nsimulation efficiency. We present MDtrajNet-1, a foundational AI model that\ndirectly generates MD trajectories across chemical space, bypassing force\ncalculations and integration. This approach accelerates simulations by up to\ntwo orders of magnitude compared to traditional MD, even those enhanced by\nmachine-learning interatomic potentials. MDtrajNet-1 combines equivariant\nneural networks with a Transformer-based architecture to achieve strong\naccuracy and transferability in predicting long-time trajectories for both\nknown and unseen systems. Remarkably, the errors of the trajectories generated\nby MDtrajNet-1 for various molecular systems are close to those of the\nconventional ab initio MD. The model's flexible design supports diverse\napplication scenarios, including different statistical ensembles, boundary\nconditions, and interaction types. By overcoming the intrinsic speed barrier of\nconventional MD, MDtrajNet-1 opens new frontiers in efficient and scalable\natomistic simulations."}
{"id": "2505.15872", "pdf": "https://arxiv.org/pdf/2505.15872", "abs": "https://arxiv.org/abs/2505.15872", "authors": ["Yunjia Xi", "Jianghao Lin", "Menghui Zhu", "Yongzhao Xiao", "Zhuoying Ou", "Jiaqi Liu", "Tong Wan", "Bo Chen", "Weiwen Liu", "Yasheng Wang", "Ruiming Tang", "Weinan Zhang", "Yong Yu"], "title": "InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\ngrounding responses with retrieved information. As an emerging paradigm,\nAgentic RAG further enhances this process by introducing autonomous LLM agents\ninto the information seeking process. However, existing benchmarks fall short\nin evaluating such systems, as they are confined to a static retrieval\nenvironment with a fixed, limited corpus} and simple queries that fail to\nelicit agentic behavior. Moreover, their evaluation protocols assess\ninformation seeking effectiveness by pre-defined gold sets of documents, making\nthem unsuitable for the open-ended and dynamic nature of real-world web\nenvironments. To bridge this gap, we present InfoDeepSeek, a new benchmark with\nchallenging questions designed for assessing agentic information seeking in\nreal-world, dynamic web environments. We propose a systematic methodology for\nconstructing challenging queries satisfying the criteria of determinacy,\ndifficulty, and diversity. Based on this, we develop the first evaluation\nframework tailored to dynamic agentic information seeking, including\nfine-grained metrics about the accuracy, utility, and compactness of\ninformation seeking outcomes. Through extensive experiments across LLMs, search\nengines, and question types, InfoDeepSeek reveals nuanced agent behaviors and\noffers actionable insights for future research."}
{"id": "2505.15822", "pdf": "https://arxiv.org/pdf/2505.15822", "abs": "https://arxiv.org/abs/2505.15822", "authors": ["Jhon Lopez", "Carlos Hinojosa", "Henry Arguello", "Bernard Ghanem"], "title": "MambaStyle: Efficient StyleGAN Inversion for Real Image Editing with State-Space Models", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "The task of inverting real images into StyleGAN's latent space to manipulate\ntheir attributes has been extensively studied. However, existing GAN inversion\nmethods struggle to balance high reconstruction quality, effective editability,\nand computational efficiency. In this paper, we introduce MambaStyle, an\nefficient single-stage encoder-based approach for GAN inversion and editing\nthat leverages vision state-space models (VSSMs) to address these challenges.\nSpecifically, our approach integrates VSSMs within the proposed architecture,\nenabling high-quality image inversion and flexible editing with significantly\nfewer parameters and reduced computational complexity compared to\nstate-of-the-art methods. Extensive experiments show that MambaStyle achieves a\nsuperior balance among inversion accuracy, editing quality, and computational\nefficiency. Notably, our method achieves superior inversion and editing results\nwith reduced model complexity and faster inference, making it suitable for\nreal-time applications."}
{"id": "2505.16306", "pdf": "https://arxiv.org/pdf/2505.16306", "abs": "https://arxiv.org/abs/2505.16306", "authors": ["Yizhi Zhou", "Haina Zhu", "Hangting Chen"], "title": "Layer-wise Investigation of Large-Scale Self-Supervised Music Representation Models", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Recently, pre-trained models for music information retrieval based on\nself-supervised learning (SSL) are becoming popular, showing success in various\ndownstream tasks. However, there is limited research on the specific meanings\nof the encoded information and their applicability. Exploring these aspects can\nhelp us better understand their capabilities and limitations, leading to more\neffective use in downstream tasks.\n  In this study, we analyze the advanced music representation model MusicFM and\nthe newly emerged SSL model MuQ. We focus on three main aspects: (i) validating\nthe advantages of SSL models across multiple downstream tasks, (ii) exploring\nthe specialization of layer-wise information for different tasks, and (iii)\ncomparing performance differences when selecting specific layers. Through this\nanalysis, we reveal insights into the structure and potential applications of\nSSL models in music information retrieval."}
{"id": "2505.15877", "pdf": "https://arxiv.org/pdf/2505.15877", "abs": "https://arxiv.org/abs/2505.15877", "authors": ["Siting Li", "Xiang Gao", "Simon Shaolei Du"], "title": "Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "25 pages, 5 figures", "summary": "While an image is worth more than a thousand words, only a few provide\ncrucial information for a given task and thus should be focused on. In light of\nthis, ideal text-to-image (T2I) retrievers should prioritize specific visual\nattributes relevant to queries. To evaluate current retrievers on handling\nattribute-focused queries, we build COCO-Facet, a COCO-based benchmark with\n9,112 queries about diverse attributes of interest. We find that CLIP-like\nretrievers, which are widely adopted due to their efficiency and zero-shot\nability, have poor and imbalanced performance, possibly because their image\nembeddings focus on global semantics and subjects while leaving out other\ndetails. Notably, we reveal that even recent Multimodal Large Language Model\n(MLLM)-based, stronger retrievers with a larger output dimension struggle with\nthis limitation. Hence, we hypothesize that retrieving with general image\nembeddings is suboptimal for performing such queries. As a solution, we propose\nto use promptable image embeddings enabled by these multimodal retrievers,\nwhich boost performance by highlighting required attributes. Our pipeline for\nderiving such embeddings generalizes across query types, image pools, and base\nretriever architectures. To enhance real-world applicability, we offer two\nacceleration strategies: Pre-processing promptable embeddings and using linear\napproximations. We show that the former yields a 15% improvement in Recall@5\nwhen prompts are predefined, while the latter achieves an 8% improvement when\nprompts are only available during inference."}
{"id": "2505.15833", "pdf": "https://arxiv.org/pdf/2505.15833", "abs": "https://arxiv.org/abs/2505.15833", "authors": ["Mathias Schmolli", "Maximilian Baronig", "Robert Legenstein", "Ozan Özdenizci"], "title": "Adversarially Robust Spiking Neural Networks with Sparse Connectivity", "categories": ["cs.NE", "cs.CR", "cs.LG"], "comment": null, "summary": "Deployment of deep neural networks in resource-constrained embedded systems\nrequires innovative algorithmic solutions to facilitate their energy and memory\nefficiency. To further ensure the reliability of these systems against\nmalicious actors, recent works have extensively studied adversarial robustness\nof existing architectures. Our work focuses on the intersection of adversarial\nrobustness, memory- and energy-efficiency in neural networks. We introduce a\nneural network conversion algorithm designed to produce sparse and\nadversarially robust spiking neural networks (SNNs) by leveraging the sparse\nconnectivity and weights from a robustly pretrained artificial neural network\n(ANN). Our approach combines the energy-efficient architecture of SNNs with a\nnovel conversion algorithm, leading to state-of-the-art performance with\nenhanced energy and memory efficiency through sparse connectivity and\nactivations. Our models are shown to achieve up to 100x reduction in the number\nof weights to be stored in memory, with an estimated 8.6x increase in energy\nefficiency compared to dense SNNs, while maintaining high performance and\nrobustness against adversarial threats."}
{"id": "2505.16307", "pdf": "https://arxiv.org/pdf/2505.16307", "abs": "https://arxiv.org/abs/2505.16307", "authors": ["Chenzhuo Zhao", "Ziqian Liu", "Xingda Wang", "Junting Lu", "Chaoyi Ruan"], "title": "PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Prompt optimization offers a practical and broadly applicable alternative to\nfine-tuning for improving large language model (LLM) performance. However,\nexisting methods often rely on costly output generation, self-critiquing\nabilities, or human-annotated preferences, which limit their scalability,\nespecially for smaller or non-instruction-tuned models. We introduce PMPO\n(Probabilistic Metric Prompt Optimization), a unified framework that refines\nprompts using token-level cross-entropy loss as a direct, lightweight\nevaluation signal. PMPO identifies low-quality prompt segments by masking and\nmeasuring their impact on loss, then rewrites and selects improved variants by\nminimizing loss over positive and negative examples. Unlike prior methods, it\nrequires no output sampling or human evaluation during optimization, relying\nonly on forward passes and log-likelihoods. PMPO supports both supervised and\npreference-based tasks through a closely aligned loss-based evaluation\nstrategy. Experiments show that PMPO consistently outperforms prior methods\nacross model sizes and tasks: it achieves the highest average accuracy on BBH,\nperforms strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates\nby over 19 points. These results highlight PMPO's effectiveness, efficiency,\nand broad applicability."}
{"id": "2505.15879", "pdf": "https://arxiv.org/pdf/2505.15879", "abs": "https://arxiv.org/abs/2505.15879", "authors": ["Yue Fan", "Xuehai He", "Diji Yang", "Kaizhi Zheng", "Ching-Chen Kuo", "Yuting Zheng", "Sravana Jyothi Narayanaraju", "Xinze Guan", "Xin Eric Wang"], "title": "GRIT: Teaching MLLMs to Think with Images", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent studies have demonstrated the efficacy of using Reinforcement Learning\n(RL) in building reasoning models that articulate chains of thoughts prior to\nproducing final answers. However, despite ongoing advances that aim at enabling\nreasoning for vision-language tasks, existing open-source visual reasoning\nmodels typically generate reasoning content with pure natural language, lacking\nexplicit integration of visual information. This limits their ability to\nproduce clearly articulated and visually grounded reasoning chains. To this\nend, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method\nfor training MLLMs to think with images. GRIT introduces a grounded reasoning\nparadigm, in which models generate reasoning chains that interleave natural\nlanguage and explicit bounding box coordinates. These coordinates point to\nregions of the input image that the model consults during its reasoning\nprocess. Additionally, GRIT is equipped with a reinforcement learning approach,\nGRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused\non the final answer accuracy and format of the grounded reasoning output, which\neliminates the need for data with reasoning chain annotations or explicit\nbounding box labels. As a result, GRIT achieves exceptional data efficiency,\nrequiring as few as 20 image-question-answer triplets from existing datasets.\nComprehensive evaluations demonstrate that GRIT effectively trains MLLMs to\nproduce coherent and visually grounded reasoning chains, showing a successful\nunification of reasoning and grounding abilities."}
{"id": "2505.15836", "pdf": "https://arxiv.org/pdf/2505.15836", "abs": "https://arxiv.org/abs/2505.15836", "authors": ["Aarav Lala", "Kalyan Cherukuri"], "title": "Quantum-Evolutionary Neural Networks for Multi-Agent Federated Learning", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": null, "summary": "As artificial intelligence continues to drive innovation in complex,\ndecentralized environments, the need for scalable, adaptive, and\nprivacy-preserving decision-making systems has become critical. This paper\nintroduces a novel framework combining quantum-inspired neural networks with\nevolutionary algorithms to optimize real-time decision-making in multi-agent\nsystems (MAS). The proposed Quantum-Evolutionary Neural Network (QE-NN)\nleverages quantum computing principles -- such as quantum superposition and\nentanglement -- to enhance learning speed and decision accuracy, while\nintegrating evolutionary optimization to continually refine agent behaviors in\ndynamic, uncertain environments. By utilizing federated learning, QE-NN ensures\nprivacy preservation, enabling decentralized agents to collaborate without\nsharing sensitive data. The framework is designed to allow agents to adapt in\nreal-time to their environments, optimizing decision-making processes for\napplications in areas such as autonomous systems, smart cities, and healthcare.\nThis research represents a breakthrough in merging quantum computing,\nevolutionary optimization, and privacy-preserving techniques to solve complex\nproblems in multi-agent decision-making systems, pushing the boundaries of AI\nin real-world, privacy-sensitive applications."}
{"id": "2505.16314", "pdf": "https://arxiv.org/pdf/2505.16314", "abs": "https://arxiv.org/abs/2505.16314", "authors": ["Shuhao Han", "Haotian Fan", "Fangyuan Kong", "Wenjie Liao", "Chunle Guo", "Chongyi Li", "Radu Timofte", "Liang Li", "Tao Li", "Junhui Cui", "Yunqiu Wang", "Yang Tai", "Jingwei Sun", "Jianhui Sun", "Xinli Yue", "Tianyi Wang", "Huan Hou", "Junda Lu", "Xinyang Huang", "Zitang Zhou", "Zijian Zhang", "Xuhui Zheng", "Xuecheng Wu", "Chong Peng", "Xuezhi Cao", "Trong-Hieu Nguyen-Mau", "Minh-Hoang Le", "Minh-Khoa Le-Phan", "Duy-Nam Ly", "Hai-Dang Nguyen", "Minh-Triet Tran", "Yukang Lin", "Yan Hong", "Chuanbiao Song", "Siyuan Li", "Jun Lan", "Zhichao Zhang", "Xinyue Li", "Wei Sun", "Zicheng Zhang", "Yunhao Li", "Xiaohong Liu", "Guangtao Zhai", "Zitong Xu", "Huiyu Duan", "Jiarui Wang", "Guangji Ma", "Liu Yang", "Lu Liu", "Qiang Hu", "Xiongkuo Min", "Zichuan Wang", "Zhenchen Tang", "Bo Peng", "Jing Dong", "Fengbin Guan", "Zihao Yu", "Yiting Lu", "Wei Luo", "Xin Li", "Minhao Lin", "Haofeng Chen", "Xuanxuan He", "Kele Xu", "Qisheng Xu", "Zijian Gao", "Tianjiao Wan", "Bo-Cheng Qiu", "Chih-Chung Hsu", "Chia-ming Lee", "Yu-Fan Lin", "Bo Yu", "Zehao Wang", "Da Mu", "Mingxiu Chen", "Junkang Fang", "Huamei Sun", "Wending Zhao", "Zhiyu Wang", "Wang Liu", "Weikang Yu", "Puhong Duan", "Bin Sun", "Xudong Kang", "Shutao Li", "Shuai He", "Lingzhi Fu", "Heng Cong", "Rongyu Zhang", "Jiarong He", "Zhishan Qiao", "Yongqing Huang", "Zewen Chen", "Zhe Pang", "Juan Wang", "Jian Guo", "Zhizhuo Shao", "Ziyu Feng", "Bing Li", "Weiming Hu", "Hesong Li", "Dehua Liu", "Zeming Liu", "Qingsong Xie", "Ruichen Wang", "Zhihao Li", "Yuqi Liang", "Jianqi Bi", "Jun Luo", "Junfeng Yang", "Can Li", "Jing Fu", "Hongwei Xu", "Mingrui Long", "Lulin Tang"], "title": "NTIRE 2025 challenge on Text to Image Generation Model Quality Assessment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper reports on the NTIRE 2025 challenge on Text to Image (T2I)\ngeneration model quality assessment, which will be held in conjunction with the\nNew Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2025.\nThe aim of this challenge is to address the fine-grained quality assessment of\ntext-to-image generation models. This challenge evaluates text-to-image models\nfrom two aspects: image-text alignment and image structural distortion\ndetection, and is divided into the alignment track and the structural track.\nThe alignment track uses the EvalMuse-40K, which contains around 40K\nAI-Generated Images (AIGIs) generated by 20 popular generative models. The\nalignment track has a total of 371 registered participants. A total of 1,883\nsubmissions are received in the development phase, and 507 submissions are\nreceived in the test phase. Finally, 12 participating teams submitted their\nmodels and fact sheets. The structure track uses the EvalMuse-Structure, which\ncontains 10,000 AI-Generated Images (AIGIs) with corresponding structural\ndistortion mask. A total of 211 participants have registered in the structure\ntrack. A total of 1155 submissions are received in the development phase, and\n487 submissions are received in the test phase. Finally, 8 participating teams\nsubmitted their models and fact sheets. Almost all methods have achieved better\nresults than baseline methods, and the winning methods in both tracks have\ndemonstrated superior prediction performance on T2I model quality assessment."}
{"id": "2505.15928", "pdf": "https://arxiv.org/pdf/2505.15928", "abs": "https://arxiv.org/abs/2505.15928", "authors": ["Tony Montes", "Fernando Lozano"], "title": "ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation", "categories": ["cs.CV", "cs.CL", "I.4.8"], "comment": null, "summary": "Recent advancements in Video Question Answering (VideoQA) have introduced\nLLM-based agents, modular frameworks, and procedural solutions, yielding\npromising results. These systems use dynamic agents and memory-based mechanisms\nto break down complex tasks and refine answers. However, significant\nimprovements remain in tracking objects for grounding over time and\ndecision-making based on reasoning to better align object references with\nlanguage model outputs, as newer models get better at both tasks. This work\npresents an LLM-brained agent for zero-shot Video Question Answering (VideoQA)\nthat combines a Chain-of-Thought framework with grounding reasoning alongside\nYOLO-World to enhance object tracking and alignment. This approach establishes\na new state-of-the-art in VideoQA and Video Understanding, showing enhanced\nperformance on NExT-QA, iVQA, and ActivityNet-QA benchmarks. Our framework also\nenables cross-checking of grounding timeframes, improving accuracy and\nproviding valuable support for verification and increased output reliability\nacross multiple video domains. The code is available at\nhttps://github.com/t-montes/viqagent."}
{"id": "2505.15839", "pdf": "https://arxiv.org/pdf/2505.15839", "abs": "https://arxiv.org/abs/2505.15839", "authors": ["Saining Liu", "Yi Mei", "Mengjie Zhang"], "title": "Curriculum Learning in Genetic Programming Guided Local Search for Large-scale Vehicle Routing Problems", "categories": ["cs.NE", "cs.LG"], "comment": null, "summary": "Manually designing (meta-)heuristics for the Vehicle Routing Problem (VRP) is\na challenging task that requires significant domain expertise. Recently,\ndata-driven approaches have emerged as a promising solution, automatically\nlearning heuristics that perform well on training instances and generalize to\nunseen test cases. Such an approach learns (meta-)heuristics that can perform\nwell on the training instances, expecting it to generalize well on the unseen\ntest instances. A recent method, named GPGLS, uses Genetic Programming (GP) to\nlearn the utility function in Guided Local Search (GLS) and solved large scale\nVRP effectively. However, the selection of appropriate training instances\nduring the learning process remains an open question, with most existing\nstudies including GPGLS relying on random instance selection. To address this,\nwe propose a novel method, CL-GPGLS, which integrates Curriculum Learning (CL)\ninto GPGLS. Our approach leverages a predefined curriculum to introduce\ntraining instances progressively, starting with simpler tasks and gradually\nincreasing complexity, enabling the model to better adapt and optimize for\nlarge-scale VRP (LSVRP). Extensive experiments verify the effectiveness of\nCL-GPGLS, demonstrating significant performance improvements over three\nbaseline methods."}
{"id": "2505.16322", "pdf": "https://arxiv.org/pdf/2505.16322", "abs": "https://arxiv.org/abs/2505.16322", "authors": ["Woosung Koh", "Wonbeen Oh", "Jaein Jang", "MinHyung Lee", "Hyeongjin Kim", "Ah Yeon Kim", "Joonkee Kim", "Junghyun Lee", "Taehyeon Kim", "Se-Young Yun"], "title": "AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Pre-print", "summary": "Self-Taught Reasoners (STaR), synonymously known as Rejection sampling\nFine-Tuning (RFT), is an integral part of the training pipeline of\nself-improving reasoning Language Models (LMs). The self-improving mechanism\noften employs random observation (data) sampling. However, this results in\ntrained observation imbalance; inefficiently over-training on solved examples\nwhile under-training on challenging ones. In response, we introduce Adaptive\nSTaR (AdaSTaR), a novel algorithm that rectifies this by integrating two\nadaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting\nbalanced training across observations, and (2) Adaptive Sampling for\nCurriculum: dynamically adjusting data difficulty to match the model's evolving\nstrength. Across six benchmarks, AdaSTaR achieves best test accuracy in all\ninstances (6/6) and reduces training FLOPs by an average of 58.6% against an\nextensive list of baselines. These improvements in performance and efficiency\ngeneralize to different pre-trained LMs and larger models, paving the way for\nmore efficient and effective self-improving LMs."}
{"id": "2505.15935", "pdf": "https://arxiv.org/pdf/2505.15935", "abs": "https://arxiv.org/abs/2505.15935", "authors": ["Omer Hofman", "Oren Rachmil", "Shamik Bose", "Vikas Pahuja", "Jonathan Brokman", "Toshiya Shimizu", "Trisha Starostina", "Kelly Marchisio", "Seraphina Goldfarb-Tarrant", "Roman Vainshtein"], "title": "MAPS: A Multilingual Benchmark for Global Agent Performance and Security", "categories": ["cs.DB", "cs.CL", "cs.CR"], "comment": null, "summary": "Agentic AI systems, which build on Large Language Models (LLMs) and interact\nwith tools and memory, have rapidly advanced in capability and scope. Yet,\nsince LLMs have been shown to struggle in multilingual settings, typically\nresulting in lower performance and reduced safety, agentic systems risk\ninheriting these limitations. This raises concerns about the global\naccessibility of such systems, as users interacting in languages other than\nEnglish may encounter unreliable or security-critical agent behavior. Despite\ngrowing interest in evaluating agentic AI, existing benchmarks focus\nexclusively on English, leaving multilingual settings unexplored. To address\nthis gap, we propose MAPS, a multilingual benchmark suite designed to evaluate\nagentic AI systems across diverse languages and tasks. MAPS builds on four\nwidely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code\ngeneration), MATH (mathematical reasoning), and the Agent Security Benchmark\n(security). We translate each dataset into ten diverse languages, resulting in\n805 unique tasks and 8,855 total language-specific instances. Our benchmark\nsuite enables a systematic analysis of how multilingual contexts affect agent\nperformance and robustness. Empirically, we observe consistent degradation in\nboth performance and security when transitioning from English to other\nlanguages, with severity varying by task and correlating with the amount of\ntranslated input. Building on these findings, we provide actionable\nrecommendations to guide agentic AI systems development and assessment under\nmultilingual settings. This work establishes a standardized evaluation\nframework, encouraging future research towards equitable, reliable, and\nglobally accessible agentic AI. MAPS benchmark suite is publicly available at\nhttps://huggingface.co/datasets/Fujitsu-FRE/MAPS"}
{"id": "2505.15842", "pdf": "https://arxiv.org/pdf/2505.15842", "abs": "https://arxiv.org/abs/2505.15842", "authors": ["Mohit Kataria", "Shreyash Bhilwade", "Sandeep Kumar", "Jayadeva"], "title": "AH-UGC: Adaptive and Heterogeneous-Universal Graph Coarsening", "categories": ["cs.SI", "cs.LG"], "comment": null, "summary": "$\\textbf{Graph Coarsening (GC)}$ is a prominent graph reduction technique\nthat compresses large graphs to enable efficient learning and inference.\nHowever, existing GC methods generate only one coarsened graph per run and must\nrecompute from scratch for each new coarsening ratio, resulting in unnecessary\noverhead. Moreover, most prior approaches are tailored to\n$\\textit{homogeneous}$ graphs and fail to accommodate the semantic constraints\nof $\\textit{heterogeneous}$ graphs, which comprise multiple node and edge\ntypes. To overcome these limitations, we introduce a novel framework that\ncombines Locality Sensitive Hashing (LSH) with Consistent Hashing to enable\n$\\textit{adaptive graph coarsening}$. Leveraging hashing techniques, our method\nis inherently fast and scalable. For heterogeneous graphs, we propose a\n$\\textit{type isolated coarsening}$ strategy that ensures semantic consistency\nby restricting merges to nodes of the same type. Our approach is the first\nunified framework to support both adaptive and heterogeneous coarsening.\nExtensive evaluations on 23 real-world datasets including homophilic,\nheterophilic, homogeneous, and heterogeneous graphs demonstrate that our method\nachieves superior scalability while preserving the structural and semantic\nintegrity of the original graph."}
{"id": "2505.16325", "pdf": "https://arxiv.org/pdf/2505.16325", "abs": "https://arxiv.org/abs/2505.16325", "authors": ["Yuyang Jiang", "Chacha Chen", "Shengyuan Wang", "Feng Li", "Zecong Tang", "Benjamin M. Mervak", "Lydia Chelala", "Christopher M Straus", "Reve Chahine", "Samuel G. Armato III", "Chenhao Tan"], "title": "CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "18 pages, 4 figures", "summary": "Existing metrics often lack the granularity and interpretability to capture\nnuanced clinical differences between candidate and ground-truth radiology\nreports, resulting in suboptimal evaluation. We introduce a Clinically-grounded\ntabular framework with Expert-curated labels and Attribute-level comparison for\nRadiology report evaluation (CLEAR). CLEAR not only examines whether a report\ncan accurately identify the presence or absence of medical conditions, but also\nassesses whether it can precisely describe each positively identified condition\nacross five key attributes: first occurrence, change, severity, descriptive\nlocation, and recommendation. Compared to prior works, CLEAR's\nmulti-dimensional, attribute-level outputs enable a more comprehensive and\nclinically interpretable evaluation of report quality. Additionally, to measure\nthe clinical alignment of CLEAR, we collaborate with five board-certified\nradiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from\nMIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions.\nOur experiments show that CLEAR achieves high accuracy in extracting clinical\nattributes and provides automated metrics that are strongly aligned with\nclinical judgment."}
{"id": "2505.15957", "pdf": "https://arxiv.org/pdf/2505.15957", "abs": "https://arxiv.org/abs/2505.15957", "authors": ["Chih-Kai Yang", "Neo S. Ho", "Hung-yi Lee"], "title": "Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "Project Website: https://github.com/b08202033/LALM-Evaluation-Survey", "summary": "With advancements in large audio-language models (LALMs), which enhance large\nlanguage models (LLMs) with auditory capabilities, these models are expected to\ndemonstrate universal proficiency across various auditory tasks. While numerous\nbenchmarks have emerged to assess LALMs' performance, they remain fragmented\nand lack a structured taxonomy. To bridge this gap, we conduct a comprehensive\nsurvey and propose a systematic taxonomy for LALM evaluations, categorizing\nthem into four dimensions based on their objectives: (1) General Auditory\nAwareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented\nAbility, and (4) Fairness, Safety, and Trustworthiness. We provide detailed\noverviews within each category and highlight challenges in this field, offering\ninsights into promising future directions. To the best of our knowledge, this\nis the first survey specifically focused on the evaluations of LALMs, providing\nclear guidelines for the community. We will release the collection of the\nsurveyed papers and actively maintain it to support ongoing advancements in the\nfield."}
{"id": "2505.15844", "pdf": "https://arxiv.org/pdf/2505.15844", "abs": "https://arxiv.org/abs/2505.15844", "authors": ["Yousuf Islam", "Md. Jalal Uddin Chowdhury", "Sumon Chandra Das"], "title": "Advancing Tabular Stroke Modelling Through a Novel Hybrid Architecture and Feature-Selection Synergy", "categories": ["q-bio.QM", "cs.LG", "stat.AP"], "comment": null, "summary": "Brain stroke remains one of the principal causes of death and disability\nworldwide, yet most tabular-data prediction models still hover below the 95%\naccuracy threshold, limiting real-world utility. Addressing this gap, the\npresent work develops and validates a completely data-driven and interpretable\nmachine-learning framework designed to predict strokes using ten routinely\ngathered demographic, lifestyle, and clinical variables sourced from a public\ncohort of 4,981 records. We employ a detailed exploratory data analysis (EDA)\nto understand the dataset's structure and distribution, followed by rigorous\ndata preprocessing, including handling missing values, outlier removal, and\nclass imbalance correction using Synthetic Minority Over-sampling Technique\n(SMOTE). To streamline feature selection, point-biserial correlation and\nrandom-forest Gini importance were utilized, and ten varied\nalgorithms-encompassing tree ensembles, boosting, kernel methods, and a\nmultilayer neural network-were optimized using stratified five-fold\ncross-validation. Their predictions based on probabilities helped us build the\nproposed model, which included Random Forest, XGBoost, LightGBM, and a\nsupport-vector classifier, with logistic regression acting as a meta-learner.\nThe proposed model achieved an accuracy rate of 97.2% and an F1-score of\n97.15%, indicating a significant enhancement compared to the leading individual\nmodel, LightGBM, which had an accuracy of 91.4%. Our study's findings indicate\nthat rigorous preprocessing, coupled with a diverse hybrid model, can convert\nlow-cost tabular data into a nearly clinical-grade stroke-risk assessment tool."}
{"id": "2505.16330", "pdf": "https://arxiv.org/pdf/2505.16330", "abs": "https://arxiv.org/abs/2505.16330", "authors": ["Wenqing Wu", "Chengzhi Zhang", "Tong Bao", "Yi Zhao"], "title": "SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers", "categories": ["cs.CL", "cs.AI", "cs.DL"], "comment": null, "summary": "Novelty is a core component of academic papers, and there are multiple\nperspectives on the assessment of novelty. Existing methods often focus on word\nor entity combinations, which provide limited insights. The content related to\na paper's novelty is typically distributed across different core sections,\ne.g., Introduction, Methodology and Results. Therefore, exploring the optimal\ncombination of sections for evaluating the novelty of a paper is important for\nadvancing automated novelty assessment. In this paper, we utilize different\ncombinations of sections from academic papers as inputs to drive language\nmodels to predict novelty scores. We then analyze the results to determine the\noptimal section combinations for novelty score prediction. We first employ\nnatural language processing techniques to identify the sectional structure of\nacademic papers, categorizing them into introduction, methods, results, and\ndiscussion (IMRaD). Subsequently, we used different combinations of these\nsections (e.g., introduction and methods) as inputs for pretrained language\nmodels (PLMs) and large language models (LLMs), employing novelty scores\nprovided by human expert reviewers as ground truth labels to obtain prediction\nresults. The results indicate that using introduction, results and discussion\nis most appropriate for assessing the novelty of a paper, while the use of the\nentire text does not yield significant results. Furthermore, based on the\nresults of the PLMs and LLMs, the introduction and results appear to be the\nmost important section for the task of novelty score prediction. The code and\ndataset for this paper can be accessed at\nhttps://github.com/njust-winchy/SC4ANM."}
{"id": "2505.15963", "pdf": "https://arxiv.org/pdf/2505.15963", "abs": "https://arxiv.org/abs/2505.15963", "authors": ["Shujun Liu", "Siyuan Wang", "Zejun Li", "Jianxiang Wang", "Cheng Zeng", "Zhongyu Wei"], "title": "OViP: Online Vision-Language Preference Learning", "categories": ["cs.CV", "cs.CL"], "comment": "22 pages, 10 figures, 8 tables", "summary": "Large vision-language models (LVLMs) remain vulnerable to hallucination,\noften generating content misaligned with visual inputs. While recent approaches\nadvance multi-modal Direct Preference Optimization (DPO) to mitigate\nhallucination, they typically rely on predefined or randomly edited negative\nsamples that fail to reflect actual model errors, limiting training efficacy.\nIn this work, we propose an Online Vision-language Preference Learning (OViP)\nframework that dynamically constructs contrastive training data based on the\nmodel's own hallucinated outputs. By identifying semantic differences between\nsampled response pairs and synthesizing negative images using a diffusion\nmodel, OViP generates more relevant supervision signals in real time. This\nfailure-driven training enables adaptive alignment of both textual and visual\npreferences. Moreover, we refine existing evaluation protocols to better\ncapture the trade-off between hallucination suppression and expressiveness.\nExperiments on hallucination and general benchmarks demonstrate that OViP\neffectively reduces hallucinations while preserving core multi-modal\ncapabilities."}
{"id": "2505.15847", "pdf": "https://arxiv.org/pdf/2505.15847", "abs": "https://arxiv.org/abs/2505.15847", "authors": ["Blaž Bertalanič", "Matej Vnučec", "Carolina Fortuna"], "title": "Graph Neural Networks Based Anomalous RSSI Detection", "categories": ["cs.NI", "cs.LG"], "comment": "5 pages, 3 figures", "summary": "In today's world, modern infrastructures are being equipped with information\nand communication technologies to create large IoT networks.\n  It is essential to monitor these networks to ensure smooth operations by\ndetecting and correcting link failures or abnormal network behaviour\nproactively, which can otherwise cause interruptions in business operations.\n  This paper presents a novel method for detecting anomalies in wireless links\nusing graph neural networks. The proposed approach involves converting time\nseries data into graphs and training a new graph neural network architecture\nbased on graph attention networks that successfully detects anomalies at the\nlevel of individual measurements of the time series data. The model provides\ncompetitive results compared to the state of the art while being\ncomputationally more efficient with ~171 times fewer trainable parameters."}
{"id": "2505.16332", "pdf": "https://arxiv.org/pdf/2505.16332", "abs": "https://arxiv.org/abs/2505.16332", "authors": ["Zhehui Wanga", "Benjamin Chen Ming Choonga", "Tian Huang", "Daniel Gerlinghoffa", "Rick Siow Mong Goh", "Cheng Liu", "Tao Luo"], "title": "Is Quantum Optimization Ready? An Effort Towards Neural Network Compression using Adiabatic Quantum Computing", "categories": ["quant-ph", "cs.AI"], "comment": null, "summary": "Quantum optimization is the most mature quantum computing technology to date,\nproviding a promising approach towards efficiently solving complex\ncombinatorial problems. Methods such as adiabatic quantum computing (AQC) have\nbeen employed in recent years on important optimization problems across various\ndomains. In deep learning, deep neural networks (DNN) have reached immense\nsizes to support new predictive capabilities. Optimization of large-scale\nmodels is critical for sustainable deployment, but becomes increasingly\nchallenging with ever-growing model sizes and complexity. While quantum\noptimization is suitable for solving complex problems, its application to DNN\noptimization is not straightforward, requiring thorough reformulation for\ncompatibility with commercially available quantum devices. In this work, we\nexplore the potential of adopting AQC for fine-grained pruning-quantization of\nconvolutional neural networks. We rework established heuristics to formulate\nmodel compression as a quadratic unconstrained binary optimization (QUBO)\nproblem, and assess the solution space offered by commercial quantum annealing\ndevices. Through our exploratory efforts of reformulation, we demonstrate that\nAQC can achieve effective compression of practical DNN models. Experiments\ndemonstrate that adiabatic quantum computing (AQC) not only outperforms\nclassical algorithms like genetic algorithms and reinforcement learning in\nterms of time efficiency but also excels at identifying global optima."}
{"id": "2505.15966", "pdf": "https://arxiv.org/pdf/2505.15966", "abs": "https://arxiv.org/abs/2505.15966", "authors": ["Alex Su", "Haozhe Wang", "Weimin Ren", "Fangzhen Lin", "Wenhu Chen"], "title": "Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Haozhe Wang and Alex Su contributed equally and listed alphabetically", "summary": "Chain-of-thought reasoning has significantly improved the performance of\nLarge Language Models (LLMs) across various domains. However, this reasoning\nprocess has been confined exclusively to textual space, limiting its\neffectiveness in visually intensive tasks. To address this limitation, we\nintroduce the concept of reasoning in the pixel-space. Within this novel\nframework, Vision-Language Models (VLMs) are equipped with a suite of visual\nreasoning operations, such as zoom-in and select-frame. These operations enable\nVLMs to directly inspect, interrogate, and infer from visual evidences, thereby\nenhancing reasoning fidelity for visual tasks. Cultivating such pixel-space\nreasoning capabilities in VLMs presents notable challenges, including the\nmodel's initially imbalanced competence and its reluctance to adopt the newly\nintroduced pixel-space operations. We address these challenges through a\ntwo-phase training approach. The first phase employs instruction tuning on\nsynthesized reasoning traces to familiarize the model with the novel visual\noperations. Following this, a reinforcement learning (RL) phase leverages a\ncuriosity-driven reward scheme to balance exploration between pixel-space\nreasoning and textual reasoning. With these visual operations, VLMs can\ninteract with complex visual inputs, such as information-rich images or videos\nto proactively gather necessary information. We demonstrate that this approach\nsignificantly improves VLM performance across diverse visual reasoning\nbenchmarks. Our 7B model, \\model, achieves 84\\% on V* bench, 74\\% on\nTallyQA-Complex, and 84\\% on InfographicsVQA, marking the highest accuracy\nachieved by any open-source model to date. These results highlight the\nimportance of pixel-space reasoning and the effectiveness of our framework."}
{"id": "2505.15854", "pdf": "https://arxiv.org/pdf/2505.15854", "abs": "https://arxiv.org/abs/2505.15854", "authors": ["Thai-Hoc Vu", "Ngo Hoang Tu", "Thien Huynh-The", "Kyungchun Lee", "Sunghwan Kim", "Miroslav Voznak", "Quoc-Viet Pham"], "title": "Integration of TinyML and LargeML: A Survey of 6G and Beyond", "categories": ["cs.NI", "cs.AI", "cs.ET", "cs.LG", "cs.MA"], "comment": "This work was submitted to IEEE Communications Surveys & Tutorials", "summary": "The transition from 5G networks to 6G highlights a significant demand for\nmachine learning (ML). Deep learning models, in particular, have seen wide\napplication in mobile networking and communications to support advanced\nservices in emerging wireless environments, such as smart healthcare, smart\ngrids, autonomous vehicles, aerial platforms, digital twins, and the metaverse.\nThe rapid expansion of Internet-of-Things (IoT) devices, many with limited\ncomputational capabilities, has accelerated the development of tiny machine\nlearning (TinyML) and resource-efficient ML approaches for cost-effective\nservices. However, the deployment of large-scale machine learning (LargeML)\nsolutions require major computing resources and complex management strategies\nto support extensive IoT services and ML-generated content applications.\nConsequently, the integration of TinyML and LargeML is projected as a promising\napproach for future seamless connectivity and efficient resource management.\n  Although the integration of TinyML and LargeML shows abundant potential,\nseveral challenges persist, including performance optimization, practical\ndeployment strategies, effective resource management, and security\nconsiderations. In this survey, we review and analyze the latest research aimed\nat enabling the integration of TinyML and LargeML models for the realization of\nsmart services and applications in future 6G networks and beyond. The paper\nconcludes by outlining critical challenges and identifying future research\ndirections for the holistic integration of TinyML and LargeML in\nnext-generation wireless networks."}
{"id": "2505.16335", "pdf": "https://arxiv.org/pdf/2505.16335", "abs": "https://arxiv.org/abs/2505.16335", "authors": ["Renjie Wei", "Songqiang Xu", "Qingyu Guo", "Meng Li"], "title": "FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual autoregressive (VAR) modeling has marked a paradigm shift in image\ngeneration from next-token prediction to next-scale prediction. VAR predicts a\nset of tokens at each step from coarse to fine scale, leading to better image\nquality and faster inference speed compared to existing diffusion models.\nHowever, the large parameter size and computation cost hinder its deployment on\nedge devices. To reduce the memory and computation cost, we propose FPQVAR, an\nefficient post-training floating-point (FP) quantization framework for VAR\nfeaturing algorithm and hardware co-design. At the algorithm level, we first\nidentify the challenges of quantizing VAR. To address them, we propose Dual\nFormat Quantization for the highly imbalanced input activation. We further\npropose Group-wise Hadamard Transformation and GHT-Aware Learnable\nTransformation to address the time-varying outlier channels. At the hardware\nlevel, we design the first low-bit FP quantizer and multiplier with lookup\ntables on FPGA and propose the first FPGA-based VAR accelerator featuring\nlow-bit FP computation and an elaborate two-level pipeline. Extensive\nexperiments show that compared to the state-of-the-art quantization method, our\nproposed FPQVAR significantly improves Fr\\'echet Inception Distance (FID) from\n10.83 to 3.58, Inception Score (IS) from 175.9 to 241.5 under 4-bit\nquantization. FPQVAR also significantly improves the performance of 6-bit\nquantized VAR, bringing it on par with the FP16 model. Our accelerator on\nAMD-Xilinx VCK190 FPGA achieves a throughput of 1.1 image/s, which is 3.1x\nhigher than the integer-based accelerator. It also demonstrates 3.6x and 2.8x\nhigher energy efficiency compared to the integer-based accelerator and GPU\nbaseline, respectively."}
{"id": "2505.16004", "pdf": "https://arxiv.org/pdf/2505.16004", "abs": "https://arxiv.org/abs/2505.16004", "authors": ["Aaron J. Li", "Suraj Srinivas", "Usha Bhalla", "Himabindu Lakkaraju"], "title": "Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse autoencoders (SAEs) are commonly used to interpret the internal\nactivations of large language models (LLMs) by mapping them to\nhuman-interpretable concept representations. While existing evaluations of SAEs\nfocus on metrics such as the reconstruction-sparsity tradeoff, human\n(auto-)interpretability, and feature disentanglement, they overlook a critical\naspect: the robustness of concept representations to input perturbations. We\nargue that robustness must be a fundamental consideration for concept\nrepresentations, reflecting the fidelity of concept labeling. To this end, we\nformulate robustness quantification as input-space optimization problems and\ndevelop a comprehensive evaluation framework featuring realistic scenarios in\nwhich adversarial perturbations are crafted to manipulate SAE representations.\nEmpirically, we find that tiny adversarial input perturbations can effectively\nmanipulate concept-based interpretations in most scenarios without notably\naffecting the outputs of the base LLMs themselves. Overall, our results suggest\nthat SAE concept representations are fragile and may be ill-suited for\napplications in model monitoring and oversight."}
{"id": "2505.15866", "pdf": "https://arxiv.org/pdf/2505.15866", "abs": "https://arxiv.org/abs/2505.15866", "authors": ["Stephen Asiedu", "David Watson"], "title": "Multi-omic Causal Discovery using Genotypes and Gene Expression", "categories": ["q-bio.GN", "cs.LG"], "comment": null, "summary": "Causal discovery in multi-omic datasets is crucial for understanding the\nbigger picture of gene regulatory mechanisms, but remains challenging due to\nhigh dimensionality, differentiation of direct from indirect relationships, and\nhidden confounders. We introduce GENESIS (GEne Network inference from\nExpression SIgnals and SNPs), a constraint-based algorithm that leverages the\nnatural causal precedence of genotypes to infer ancestral relationships in\ntranscriptomic data. Unlike traditional causal discovery methods that start\nwith a fully connected graph, GENESIS initialises an empty ancestrality matrix\nand iteratively populates it with direct, indirect or non-causal relationships\nusing a series of provably sound marginal and conditional independence tests.\nBy integrating genotypes as fixed causal anchors, GENESIS provides a principled\n``head start'' to classical causal discovery algorithms, restricting the search\nspace to biologically plausible edges. We test GENESIS on synthetic and\nreal-world genomic datasets. This framework offers a powerful avenue for\nuncovering causal pathways in complex traits, with promising applications to\nfunctional genomics, drug discovery, and precision medicine."}
{"id": "2505.16351", "pdf": "https://arxiv.org/pdf/2505.16351", "abs": "https://arxiv.org/abs/2505.16351", "authors": ["Chenxu Guo", "Jiachen Lian", "Xuanru Zhou", "Jinming Zhang", "Shuhe Li", "Zongli Ye", "Hwi Joo Park", "Anaisha Das", "Zoe Ezzes", "Jet Vonk", "Brittany Morin", "Rian Bogley", "Lisa Wauters", "Zachary Miller", "Maria Gorno-Tempini", "Gopala Anumanchipalli"], "title": "Dysfluent WFST: A Framework for Zero-Shot Speech Dysfluency Transcription and Detection", "categories": ["eess.AS", "cs.AI"], "comment": null, "summary": "Automatic detection of speech dysfluency aids speech-language pathologists in\nefficient transcription of disordered speech, enhancing diagnostics and\ntreatment planning. Traditional methods, often limited to classification,\nprovide insufficient clinical insight, and text-independent models misclassify\ndysfluency, especially in context-dependent cases. This work introduces\nDysfluent-WFST, a zero-shot decoder that simultaneously transcribes phonemes\nand detects dysfluency. Unlike previous models, Dysfluent-WFST operates with\nupstream encoders like WavLM and requires no additional training. It achieves\nstate-of-the-art performance in both phonetic error rate and dysfluency\ndetection on simulated and real speech data. Our approach is lightweight,\ninterpretable, and effective, demonstrating that explicit modeling of\npronunciation behavior in decoding, rather than complex architectures, is key\nto improving dysfluency processing systems."}
{"id": "2505.16037", "pdf": "https://arxiv.org/pdf/2505.16037", "abs": "https://arxiv.org/abs/2505.16037", "authors": ["Asterios Tsiourvas", "Wei Sun", "Georgia Perakis"], "title": "Causal LLM Routing: End-to-End Regret Minimization from Observational Data", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "LLM routing aims to select the most appropriate model for each query,\nbalancing competing performance metrics such as accuracy and cost across a pool\nof language models. Prior approaches typically adopt a decoupled strategy,\nwhere the metrics are first predicted and the model is then selected based on\nthese estimates. This setup is prone to compounding errors and often relies on\nfull-feedback data, where each query is evaluated by all candidate models,\nwhich is costly to obtain and maintain in practice. In contrast, we learn from\nobservational data, which records only the outcome of the model actually\ndeployed. We propose a causal end-to-end framework that learns routing policies\nby minimizing decision-making regret from observational data. To enable\nefficient optimization, we introduce two theoretically grounded surrogate\nobjectives: a classification-based upper bound, and a softmax-weighted regret\napproximation shown to recover the optimal policy at convergence. We further\nextend our framework to handle heterogeneous cost preferences via an\ninterval-conditioned architecture. Experiments on public benchmarks show that\nour method outperforms existing baselines, achieving state-of-the-art\nperformance across different embedding models."}
{"id": "2505.15867", "pdf": "https://arxiv.org/pdf/2505.15867", "abs": "https://arxiv.org/abs/2505.15867", "authors": ["Nikolaos Chaidos", "Angeliki Dimitriou", "Maria Lymperaiou", "Giorgos Stamou"], "title": "SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Despite the dominance of convolutional and transformer-based architectures in\nimage-to-image retrieval, these models are prone to biases arising from\nlow-level visual features, such as color. Recognizing the lack of semantic\nunderstanding as a key limitation, we propose a novel scene graph-based\nretrieval framework that emphasizes semantic content over superficial image\ncharacteristics. Prior approaches to scene graph retrieval predominantly rely\non supervised Graph Neural Networks (GNNs), which require ground truth graph\npairs driven from image captions. However, the inconsistency of caption-based\nsupervision stemming from variable text encodings undermine retrieval\nreliability. To address these, we present SCENIR, a Graph Autoencoder-based\nunsupervised retrieval framework, which eliminates the dependence on labeled\ntraining data. Our model demonstrates superior performance across metrics and\nruntime efficiency, outperforming existing vision-based, multimodal, and\nsupervised GNN approaches. We further advocate for Graph Edit Distance (GED) as\na deterministic and robust ground truth measure for scene graph similarity,\nreplacing the inconsistent caption-based alternatives for the first time in\nimage-to-image retrieval evaluation. Finally, we validate the generalizability\nof our method by applying it to unannotated datasets via automated scene graph\ngeneration, while substantially contributing in advancing state-of-the-art in\ncounterfactual image retrieval."}
{"id": "2505.16362", "pdf": "https://arxiv.org/pdf/2505.16362", "abs": "https://arxiv.org/abs/2505.16362", "authors": ["El-ghazali Talbi"], "title": "Neuromorphic-based metaheuristics: A new generation of low power, low latency and small footprint optimization algorithms", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Neuromorphic computing (NC) introduces a novel algorithmic paradigm\nrepresenting a major shift from traditional digital computing of Von Neumann\narchitectures. NC emulates or simulates the neural dynamics of brains in the\nform of Spiking Neural Networks (SNNs). Much of the research in NC has\nconcentrated on machine learning applications and neuroscience simulations.\nThis paper investigates the modelling and implementation of optimization\nalgorithms and particularly metaheuristics using the NC paradigm as an\nalternative to Von Neumann architectures, leading to breakthroughs in solving\noptimization problems.\n  Neuromorphic-based metaheuristics (Nheuristics) are supposed to be\ncharacterized by low power, low latency and small footprint. Since NC systems\nare fundamentally different from conventional Von Neumann computers, several\nchallenges are posed to the design and implementation of Nheuristics. A\nguideline based on a classification and critical analysis is conducted on the\ndifferent families of metaheuristics and optimization problems they address. We\nalso discuss future directions that need to be addressed to expand both the\ndevelopment and application of Nheuristics."}
{"id": "2505.16065", "pdf": "https://arxiv.org/pdf/2505.16065", "abs": "https://arxiv.org/abs/2505.16065", "authors": ["Ruijie Xi", "He Ba", "Hao Yuan", "Rishu Agrawal", "Arul Prakash"], "title": "Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Embedding-Based Retrieval (EBR) is an important technique in modern search\nengines, enabling semantic match between search queries and relevant results.\nHowever, search logging data on platforms like Facebook Marketplace lacks the\ndiversity and details needed for effective EBR model training, limiting the\nmodels' ability to capture nuanced search patterns. To address this challenge,\nwe propose Aug2Search, an EBR-based framework leveraging synthetic data\ngenerated by Generative AI (GenAI) models, in a multimodal and multitask\napproach to optimize query-product relevance. This paper investigates the\ncapabilities of GenAI, particularly Large Language Models (LLMs), in generating\nhigh-quality synthetic data, and analyzing its impact on enhancing EBR models.\nWe conducted experiments using eight Llama models and 100 million data points\nfrom Facebook Marketplace logs. Our synthetic data generation follows three\nstrategies: (1) generate queries, (2) enhance product listings, and (3)\ngenerate queries from enhanced listings. We train EBR models on three different\ndatasets: sampled engagement data or original data ((e.g., \"Click\" and \"Listing\nInteractions\")), synthetic data, and a mixture of both engagement and synthetic\ndata to assess their performance across various training sets. Our findings\nunderscore the robustness of Llama models in producing synthetic queries and\nlistings with high coherence, relevance, and diversity, while maintaining low\nlevels of hallucination. Aug2Search achieves an improvement of up to 4% in\nROC_AUC with 100 million synthetic data samples, demonstrating the\neffectiveness of our approach. Moreover, our experiments reveal that with the\nsame volume of training data, models trained exclusively on synthetic data\noften outperform those trained on original data only or a mixture of original\nand synthetic data."}
{"id": "2505.15877", "pdf": "https://arxiv.org/pdf/2505.15877", "abs": "https://arxiv.org/abs/2505.15877", "authors": ["Siting Li", "Xiang Gao", "Simon Shaolei Du"], "title": "Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "25 pages, 5 figures", "summary": "While an image is worth more than a thousand words, only a few provide\ncrucial information for a given task and thus should be focused on. In light of\nthis, ideal text-to-image (T2I) retrievers should prioritize specific visual\nattributes relevant to queries. To evaluate current retrievers on handling\nattribute-focused queries, we build COCO-Facet, a COCO-based benchmark with\n9,112 queries about diverse attributes of interest. We find that CLIP-like\nretrievers, which are widely adopted due to their efficiency and zero-shot\nability, have poor and imbalanced performance, possibly because their image\nembeddings focus on global semantics and subjects while leaving out other\ndetails. Notably, we reveal that even recent Multimodal Large Language Model\n(MLLM)-based, stronger retrievers with a larger output dimension struggle with\nthis limitation. Hence, we hypothesize that retrieving with general image\nembeddings is suboptimal for performing such queries. As a solution, we propose\nto use promptable image embeddings enabled by these multimodal retrievers,\nwhich boost performance by highlighting required attributes. Our pipeline for\nderiving such embeddings generalizes across query types, image pools, and base\nretriever architectures. To enhance real-world applicability, we offer two\nacceleration strategies: Pre-processing promptable embeddings and using linear\napproximations. We show that the former yields a 15% improvement in Recall@5\nwhen prompts are predefined, while the latter achieves an 8% improvement when\nprompts are only available during inference."}
{"id": "2505.16363", "pdf": "https://arxiv.org/pdf/2505.16363", "abs": "https://arxiv.org/abs/2505.16363", "authors": ["Huishuai Zhang", "Bohan Wang", "Luoxin Chen"], "title": "AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We introduce AdamS, a simple yet effective alternative to Adam for large\nlanguage model (LLM) pretraining and post-training. By leveraging a novel\ndenominator, i.e., the root of weighted sum of squares of the momentum and the\ncurrent gradient, AdamS eliminates the need for second-moment estimates. Hence,\nAdamS is efficient, matching the memory and compute footprint of SGD with\nmomentum while delivering superior optimization performance. Moreover, AdamS is\neasy to adopt: it can directly inherit hyperparameters of AdamW, and is\nentirely model-agnostic, integrating seamlessly into existing pipelines without\nmodifications to optimizer APIs or architectures. The motivation behind AdamS\nstems from the observed $(L_0, L_1)$ smoothness properties in transformer\nobjectives, where local smoothness is governed by gradient magnitudes that can\nbe further approximated by momentum magnitudes. We establish rigorous\ntheoretical convergence guarantees and provide practical guidelines for\nhyperparameter selection. Empirically, AdamS demonstrates strong performance in\nvarious tasks, including pre-training runs on GPT-2 and Llama2 (up to 13B\nparameters) and reinforcement learning in post-training regimes. With its\nefficiency, simplicity, and theoretical grounding, AdamS stands as a compelling\nalternative to existing optimizers."}
{"id": "2505.16066", "pdf": "https://arxiv.org/pdf/2505.16066", "abs": "https://arxiv.org/abs/2505.16066", "authors": ["Zhixu Silvia Tao", "Kasper Vinken", "Hao-Wei Yeh", "Avi Cooper", "Xavier Boix"], "title": "Merge to Mix: Mixing Datasets via Model Merging", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Mixing datasets for fine-tuning large models (LMs) has become critical for\nmaximizing performance on downstream tasks. However, composing effective\ndataset mixtures typically relies on heuristics and trial-and-error, often\nrequiring multiple fine-tuning runs to achieve the desired outcome. We propose\na novel method, $\\textit{Merge to Mix}$, that accelerates composing dataset\nmixtures through model merging. Model merging is a recent technique that\ncombines the abilities of multiple individually fine-tuned LMs into a single LM\nby using a few simple arithmetic operations. Our key insight is that merging\nmodels individually fine-tuned on each dataset in a mixture can effectively\nserve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix\nleverages this insight to accelerate selecting dataset mixtures without\nrequiring full fine-tuning on each candidate mixture. Our experiments\ndemonstrate that Merge to Mix surpasses state-of-the-art methods in dataset\nselection for fine-tuning LMs."}
{"id": "2505.15927", "pdf": "https://arxiv.org/pdf/2505.15927", "abs": "https://arxiv.org/abs/2505.15927", "authors": ["Awni Altabaa", "Omar Montasser", "John Lafferty"], "title": "CoT Information: Improved Sample Complexity under Chain-of-Thought Supervision", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Learning complex functions that involve multi-step reasoning poses a\nsignificant challenge for standard supervised learning from input-output\nexamples. Chain-of-thought (CoT) supervision, which provides intermediate\nreasoning steps together with the final output, has emerged as a powerful\nempirical technique, underpinning much of the recent progress in the reasoning\ncapabilities of large language models. This paper develops a statistical theory\nof learning under CoT supervision. A key characteristic of the CoT setting, in\ncontrast to standard supervision, is the mismatch between the training\nobjective (CoT risk) and the test objective (end-to-end risk). A central part\nof our analysis, distinguished from prior work, is explicitly linking those two\ntypes of risk to achieve sharper sample complexity bounds. This is achieved via\nthe *CoT information measure* $\\mathcal{I}_{\\mathcal{D},\nh_\\star}^{\\mathrm{CoT}}(\\epsilon; \\calH)$, which quantifies the additional\ndiscriminative power gained from observing the reasoning process. The main\ntheoretical results demonstrate how CoT supervision can yield significantly\nfaster learning rates compared to standard E2E supervision. Specifically, it is\nshown that the sample complexity required to achieve a target E2E error\n$\\epsilon$ scales as $d/\\mathcal{I}_{\\mathcal{D},\nh_\\star}^{\\mathrm{CoT}}(\\epsilon; \\calH)$, where $d$ is a measure of hypothesis\nclass complexity, which can be much faster than standard $d/\\epsilon$ rates.\nInformation-theoretic lower bounds in terms of the CoT information are also\nobtained. Together, these results suggest that CoT information is a fundamental\nmeasure of statistical complexity for learning under chain-of-thought\nsupervision."}
{"id": "2505.16365", "pdf": "https://arxiv.org/pdf/2505.16365", "abs": "https://arxiv.org/abs/2505.16365", "authors": ["Manuel Ruiz-Botella", "Marta Sales-Pardo", "Roger Guimerà"], "title": "A collaborative constrained graph diffusion model for the generation of realistic synthetic molecules", "categories": ["cs.LG", "cs.AI", "physics.comp-ph", "q-bio.QM"], "comment": "28 pages, 10 figures, 4 tables", "summary": "Developing new molecular compounds is crucial to address pressing challenges,\nfrom health to environmental sustainability. However, exploring the molecular\nspace to discover new molecules is difficult due to the vastness of the space.\nHere we introduce CoCoGraph, a collaborative and constrained graph diffusion\nmodel capable of generating molecules that are guaranteed to be chemically\nvalid. Thanks to the constraints built into the model and to the collaborative\nmechanism, CoCoGraph outperforms state-of-the-art approaches on standard\nbenchmarks while requiring up to an order of magnitude fewer parameters.\nAnalysis of 36 chemical properties also demonstrates that CoCoGraph generates\nmolecules with distributions more closely matching real molecules than current\nmodels. Leveraging the model's efficiency, we created a database of 8.2M\nmillion synthetically generated molecules and conducted a Turing-like test with\norganic chemistry experts to further assess the plausibility of the generated\nmolecules, and potential biases and limitations of CoCoGraph."}
{"id": "2505.16086", "pdf": "https://arxiv.org/pdf/2505.16086", "abs": "https://arxiv.org/abs/2505.16086", "authors": ["Ming Shen", "Raphael Shu", "Anurag Pratik", "James Gung", "Yubin Ge", "Monica Sunkara", "Yi Zhang"], "title": "Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We have seen remarkable progress in large language models (LLMs) empowered\nmulti-agent systems solving complex tasks necessitating cooperation among\nexperts with diverse skills. However, optimizing LLM-based multi-agent systems\nremains challenging. In this work, we perform an empirical case study on group\noptimization of role-based multi-agent systems utilizing natural language\nfeedback for challenging software development tasks under various evaluation\ndimensions. We propose a two-step agent prompts optimization pipeline:\nidentifying underperforming agents with their failure explanations utilizing\ntextual feedback and then optimizing system prompts of identified agents\nutilizing failure explanations. We then study the impact of various\noptimization settings on system performance with two comparison groups: online\nagainst offline optimization and individual against group optimization. For\ngroup optimization, we study two prompting strategies: one-pass and multi-pass\nprompting optimizations. Overall, we demonstrate the effectiveness of our\noptimization method for role-based multi-agent systems tackling software\ndevelopment tasks evaluated on diverse evaluation dimensions, and we\ninvestigate the impact of diverse optimization settings on group behaviors of\nthe multi-agent systems to provide practical insights for future development."}
{"id": "2505.15958", "pdf": "https://arxiv.org/pdf/2505.15958", "abs": "https://arxiv.org/abs/2505.15958", "authors": ["Ahmed Bouajjani", "Wael-Amine Boutglay", "Peter Habermehl"], "title": "Data-driven Verification of Procedural Programs with Integer Arrays", "categories": ["cs.PL", "cs.LG"], "comment": null, "summary": "We address the problem of verifying automatically procedural programs\nmanipulating parametric-size arrays of integers, encoded as a constrained Horn\nclauses solving problem. We propose a new algorithmic method for synthesizing\nloop invariants and procedure pre/post-conditions represented as universally\nquantified first-order formulas constraining the array elements and program\nvariables. We adopt a data-driven approach that extends the decision tree\nHorn-ICE framework to handle arrays. We provide a powerful learning technique\nbased on reducing a complex classification problem of vectors of integer arrays\nto a simpler classification problem of vectors of integers. The obtained\nclassifier is generalized to get universally quantified invariants and\nprocedure pre/post-conditions. We have implemented our method and shown its\nefficiency and competitiveness w.r.t. state-of-the-art tools on a significant\nbenchmark."}
{"id": "2505.16368", "pdf": "https://arxiv.org/pdf/2505.16368", "abs": "https://arxiv.org/abs/2505.16368", "authors": ["Huanyu Liu", "Jia Li", "Hao Zhu", "Kechi Zhang", "Yihong Dong", "Ge Li"], "title": "SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "How to design reinforcement learning (RL) tasks that effectively unleash the\nreasoning capability of large language models (LLMs) remains an open question.\nExisting RL tasks (e.g., math, programming, and constructing reasoning tasks)\nsuffer from three key limitations: (1) Scalability. They rely heavily on human\nannotation or expensive LLM synthesis to generate sufficient training data. (2)\nVerifiability. LLMs' outputs are hard to verify automatically and reliably. (3)\nControllable Difficulty. Most tasks lack fine-grained difficulty control,\nmaking it hard to train LLMs to develop reasoning ability from easy to hard.\n  To address these limitations, we propose Saturn, a SAT-based RL framework\nthat uses Boolean Satisfiability (SAT) problems to train and evaluate LLM\nreasoning. Saturn enables scalable task construction, rule-based verification,\nand precise difficulty control. Saturn designs a curriculum learning pipeline\nthat continuously improves LLMs' reasoning capability by constructing SAT tasks\nof increasing difficulty and training LLMs from easy to hard. To ensure stable\ntraining, we design a principled mechanism to control difficulty transitions.\n  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying\ndifficulty. It supports the evaluation of how LLM reasoning changes with\nproblem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain\nSaturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT\nproblems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of\n+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B\nand Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,\nAIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in\nconstructing RL tasks, Saturn achieves further improvements of +8.8%. We\nrelease the source code, data, and models to support future research."}
{"id": "2505.16090", "pdf": "https://arxiv.org/pdf/2505.16090", "abs": "https://arxiv.org/abs/2505.16090", "authors": ["Dominick Kubica", "Dylan T. Gordon", "Nanami Emura", "Derleen Saini", "Charlie Goldenberg"], "title": "Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance", "categories": ["cs.AI", "cs.CL", "I.2.6; I.2.7"], "comment": "6 pages, 4 figures. Research conducted as part of a\n  Microsoft-sponsored Capstone Project at Santa Clara University", "summary": "As of 2025, Generative Artificial Intelligence (GenAI) has become a central\ntool for productivity across industries. Beyond text generation, GenAI now\nplays a critical role in coding, data analysis, and research workflows. As\nlarge language models (LLMs) continue to evolve, it is essential to assess the\nreliability and accuracy of their outputs, especially in specialized,\nhigh-stakes domains like finance. Most modern LLMs transform text into\nnumerical vectors, which are used in operations such as cosine similarity\nsearches to generate responses. However, this abstraction process can lead to\nmisinterpretation of emotional tone, particularly in nuanced financial\ncontexts. While LLMs generally excel at identifying sentiment in everyday\nlanguage, these models often struggle with the nuanced, strategically ambiguous\nlanguage found in earnings call transcripts. Financial disclosures frequently\nembed sentiment in hedged statements, forward-looking language, and\nindustry-specific jargon, making it difficult even for human analysts to\ninterpret consistently, let alone AI models. This paper presents findings from\nthe Santa Clara Microsoft Practicum Project, led by Professor Charlie\nGoldenberg, which benchmarks the performance of Microsoft's Copilot, OpenAI's\nChatGPT, Google's Gemini, and traditional machine learning models for sentiment\nanalysis of financial text. Using Microsoft earnings call transcripts, the\nanalysis assesses how well LLM-derived sentiment correlates with market\nsentiment and stock movements and evaluates the accuracy of model outputs.\nPrompt engineering techniques are also examined to improve sentiment analysis\nresults. Visualizations of sentiment consistency are developed to evaluate\nalignment between tone and stock performance, with sentiment trends analyzed\nacross Microsoft's lines of business to determine which segments exert the\ngreatest influence."}
{"id": "2505.15962", "pdf": "https://arxiv.org/pdf/2505.15962", "abs": "https://arxiv.org/abs/2505.15962", "authors": ["Linxi Zhao", "Sofian Zalouk", "Christian K. Belardi", "Justin Lovelace", "Jin Peng Zhou", "Kilian Q. Weinberger", "Yoav Artzi", "Jennifer J. Sun"], "title": "Pre-training Large Memory Language Models with Internal and External Knowledge", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Neural language models are black-boxes -- both linguistic patterns and\nfactual knowledge are distributed across billions of opaque parameters. This\nentangled encoding makes it difficult to reliably inspect, verify, or update\nspecific facts. We propose a new class of language models, Large Memory\nLanguage Models (LMLM) with a pre-training recipe that stores factual knowledge\nin both internal weights and an external database. Our approach strategically\nmasks externally retrieved factual values from the training loss, thereby\nteaching the model to perform targeted lookups rather than relying on\nmemorization in model weights. Our experiments demonstrate that LMLMs achieve\ncompetitive performance compared to significantly larger, knowledge-dense LLMs\non standard benchmarks, while offering the advantages of explicit, editable,\nand verifiable knowledge bases. This work represents a fundamental shift in how\nlanguage models interact with and manage factual knowledge."}
{"id": "2505.16372", "pdf": "https://arxiv.org/pdf/2505.16372", "abs": "https://arxiv.org/abs/2505.16372", "authors": ["Feng Liu", "Bingyu Nan", "Xuezhong Qian", "Xiaolan Fu"], "title": "Temporal and Spatial Feature Fusion Framework for Dynamic Micro Expression Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "17 pages", "summary": "When emotions are repressed, an individual's true feelings may be revealed\nthrough micro-expressions. Consequently, micro-expressions are regarded as a\ngenuine source of insight into an individual's authentic emotions. However, the\ntransient and highly localised nature of micro-expressions poses a significant\nchallenge to their accurate recognition, with the accuracy rate of\nmicro-expression recognition being as low as 50%, even for professionals. In\norder to address these challenges, it is necessary to explore the field of\ndynamic micro expression recognition (DMER) using multimodal fusion techniques,\nwith special attention to the diverse fusion of temporal and spatial modal\nfeatures. In this paper, we propose a novel Temporal and Spatial feature Fusion\nframework for DMER (TSFmicro). This framework integrates a Retention Network\n(RetNet) and a transformer-based DMER network, with the objective of efficient\nmicro-expression recognition through the capture and fusion of temporal and\nspatial relations. Meanwhile, we propose a novel parallel time-space fusion\nmethod from the perspective of modal fusion, which fuses spatio-temporal\ninformation in high-dimensional feature space, resulting in complementary\n\"where-how\" relationships at the semantic level and providing richer semantic\ninformation for the model. The experimental results demonstrate the superior\nperformance of the TSFmicro method in comparison to other contemporary\nstate-of-the-art methods. This is evidenced by its effectiveness on three\nwell-recognised micro-expression datasets."}
{"id": "2505.16094", "pdf": "https://arxiv.org/pdf/2505.16094", "abs": "https://arxiv.org/abs/2505.16094", "authors": ["Ziqing Wang", "Kexin Zhang", "Zihan Zhao", "Yibo Wen", "Abhishek Pandey", "Han Liu", "Kaize Ding"], "title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization", "categories": ["cs.LG", "cs.CL"], "comment": "Under review", "summary": "Large language models (LLMs) are introducing a paradigm shift in molecular\ndiscovery by enabling text-guided interaction with chemical spaces through\nnatural language, symbolic notations, with emerging extensions to incorporate\nmulti-modal inputs. To advance the new field of LLM for molecular discovery,\nthis survey provides an up-to-date and forward-looking review of the emerging\nuse of LLMs for two central tasks: molecule generation and molecule\noptimization. Based on our proposed taxonomy for both problems, we analyze\nrepresentative techniques in each category, highlighting how LLM capabilities\nare leveraged across different learning settings. In addition, we include the\ncommonly used datasets and evaluation protocols. We conclude by discussing key\nchallenges and future directions, positioning this survey as a resource for\nresearchers working at the intersection of LLMs and molecular science. A\ncontinuously updated reading list is available at\nhttps://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery."}
{"id": "2505.15970", "pdf": "https://arxiv.org/pdf/2505.15970", "abs": "https://arxiv.org/abs/2505.15970", "authors": ["Matthew Lyle Olson", "Musashi Hinck", "Neale Ratzlaff", "Changbai Li", "Phillip Howard", "Vasudev Lal", "Shao-Yen Tseng"], "title": "Analyzing Hierarchical Structure in Vision Models with Sparse Autoencoders", "categories": ["cs.CV", "cs.LG"], "comment": "(Oral) CVPR 2025 Workshop on Mechanistic Interpretability for Vision.\n  Authors 1 and 2 contributed equally", "summary": "The ImageNet hierarchy provides a structured taxonomy of object categories,\noffering a valuable lens through which to analyze the representations learned\nby deep vision models. In this work, we conduct a comprehensive analysis of how\nvision models encode the ImageNet hierarchy, leveraging Sparse Autoencoders\n(SAEs) to probe their internal representations. SAEs have been widely used as\nan explanation tool for large language models (LLMs), where they enable the\ndiscovery of semantically meaningful features. Here, we extend their use to\nvision models to investigate whether learned representations align with the\nontological structure defined by the ImageNet taxonomy. Our results show that\nSAEs uncover hierarchical relationships in model activations, revealing an\nimplicit encoding of taxonomic structure. We analyze the consistency of these\nrepresentations across different layers of the popular vision foundation model\nDINOv2 and provide insights into how deep vision models internalize\nhierarchical category information by increasing information in the class token\nthrough each layer. Our study establishes a framework for systematic\nhierarchical analysis of vision model representations and highlights the\npotential of SAEs as a tool for probing semantic structure in deep networks."}
{"id": "2505.16376", "pdf": "https://arxiv.org/pdf/2505.16376", "abs": "https://arxiv.org/abs/2505.16376", "authors": ["Zijia Lu", "A S M Iftekhar", "Gaurav Mittal", "Tianjian Meng", "Xiawei Wang", "Cheng Zhao", "Rohith Kukkala", "Ehsan Elhamifar", "Mei Chen"], "title": "DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "Long Video Temporal Grounding (LVTG) aims at identifying specific moments\nwithin lengthy videos based on user-provided text queries for effective content\nretrieval. The approach taken by existing methods of dividing video into clips\nand processing each clip via a full-scale expert encoder is challenging to\nscale due to prohibitive computational costs of processing a large number of\nclips in long videos. To address this issue, we introduce DeCafNet, an approach\nemploying ``delegate-and-conquer'' strategy to achieve computation efficiency\nwithout sacrificing grounding performance. DeCafNet introduces a sidekick\nencoder that performs dense feature extraction over all video clips in a\nresource-efficient manner, while generating a saliency map to identify the most\nrelevant clips for full processing by the expert encoder. To effectively\nleverage features from sidekick and expert encoders that exist at different\ntemporal resolutions, we introduce DeCaf-Grounder, which unifies and refines\nthem via query-aware temporal aggregation and multi-scale temporal refinement\nfor accurate grounding. Experiments on two LTVG benchmark datasets demonstrate\nthat DeCafNet reduces computation by up to 47\\% while still outperforming\nexisting methods, establishing a new state-of-the-art for LTVG in terms of both\nefficiency and performance. Our code is available at\nhttps://github.com/ZijiaLewisLu/CVPR2025-DeCafNet."}
{"id": "2505.16100", "pdf": "https://arxiv.org/pdf/2505.16100", "abs": "https://arxiv.org/abs/2505.16100", "authors": ["Zifeng Wang", "Benjamin Danek", "Jimeng Sun"], "title": "BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Validating scientific hypotheses is a central challenge in biomedical\nresearch, and remains difficult for artificial intelligence (AI) agents due to\nthe complexity of real-world data analysis and evidence interpretation. In this\nwork, we present BioDSA-1K, a benchmark designed to evaluate AI agents on\nrealistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K\nconsists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans,\ncurated from over 300 published biomedical studies to reflect the structure and\nreasoning found in authentic research workflows. Each task includes a\nstructured hypothesis derived from the original study's conclusions, expressed\nin the affirmative to reflect the language of scientific reporting, and one or\nmore pieces of supporting evidence grounded in empirical data tables. While\nthese hypotheses mirror published claims, they remain testable using standard\nstatistical or machine learning methods. The benchmark enables evaluation along\nfour axes: (1) hypothesis decision accuracy, (2) alignment between evidence and\nconclusion, (3) correctness of the reasoning process, and (4) executability of\nthe AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable\nhypotheses: cases where the available data are insufficient to support or\nrefute a claim, reflecting a common yet underexplored scenario in real-world\nscience. We propose BioDSA-1K as a foundation for building and evaluating\ngeneralizable, trustworthy AI agents for biomedical discovery."}
{"id": "2505.15974", "pdf": "https://arxiv.org/pdf/2505.15974", "abs": "https://arxiv.org/abs/2505.15974", "authors": ["Alan Ta", "Nilsu Salgin", "Mustafa Demir", "Kala Philips Randal", "Ranjana K. Mehta", "Anthony McDonald", "Carly McCord", "Farzan Sasangohar"], "title": "Real-Time Stress Monitoring, Detection, and Management in College Students: A Wearable Technology and Machine-Learning Approach", "categories": ["cs.HC", "cs.LG"], "comment": "31 pages, 5 figures", "summary": "College students are increasingly affected by stress, anxiety, and\ndepression, yet face barriers to traditional mental health care. This study\nevaluated the efficacy of a mobile health (mHealth) intervention, Mental Health\nEvaluation and Lookout Program (mHELP), which integrates a smartwatch sensor\nand machine learning (ML) algorithms for real-time stress detection and\nself-management. In a 12-week randomized controlled trial (n = 117),\nparticipants were assigned to a treatment group using mHELP's full suite of\ninterventions or a control group using the app solely for real-time stress\nlogging and weekly psychological assessments. The primary outcome, \"Moments of\nStress\" (MS), was assessed via physiological and self-reported indicators and\nanalyzed using Generalized Linear Mixed Models (GLMM) approaches. Similarly,\nsecondary outcomes of psychological assessments, including the Generalized\nAnxiety Disorder-7 (GAD-7) for anxiety, the Patient Health Questionnaire\n(PHQ-8) for depression, and the Perceived Stress Scale (PSS), were also\nanalyzed via GLMM. The finding of the objective measure, MS, indicates a\nsubstantial decrease in MS among the treatment group compared to the control\ngroup, while no notable between-group differences were observed in subjective\nscores of anxiety (GAD-7), depression (PHQ-8), or stress (PSS). However, the\ntreatment group exhibited a clinically meaningful decline in GAD-7 and PSS\nscores. These findings underscore the potential of wearable-enabled mHealth\ntools to reduce acute stress in college populations and highlight the need for\nextended interventions and tailored features to address chronic symptoms like\ndepression."}
{"id": "2505.16377", "pdf": "https://arxiv.org/pdf/2505.16377", "abs": "https://arxiv.org/abs/2505.16377", "authors": ["Yansong Qu", "Zilin Huang", "Zihao Sheng", "Jiancong Chen", "Sikai Chen", "Samuel Labi"], "title": "VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL)-based autonomous driving policy learning faces\ncritical limitations such as low sample efficiency and poor generalization; its\nreliance on online interactions and trial-and-error learning is especially\nunacceptable in safety-critical scenarios. Existing methods including safe RL\noften fail to capture the true semantic meaning of \"safety\" in complex driving\ncontexts, leading to either overly conservative driving behavior or constraint\nviolations. To address these challenges, we propose VL-SAFE, a world\nmodel-based safe RL framework with Vision-Language model\n(VLM)-as-safety-guidance paradigm, designed for offline safe policy learning.\nSpecifically, we construct offline datasets containing data collected by expert\nagents and labeled with safety scores derived from VLMs. A world model is\ntrained to generate imagined rollouts together with safety estimations,\nallowing the agent to perform safe planning without interacting with the real\nenvironment. Based on these imagined trajectories and safety evaluations,\nactor-critic learning is conducted under VLM-based safety guidance to optimize\nthe driving policy more safely and efficiently. Extensive evaluations\ndemonstrate that VL-SAFE achieves superior sample efficiency, generalization,\nsafety, and overall performance compared to existing baselines. To the best of\nour knowledge, this is the first work that introduces a VLM-guided world\nmodel-based approach for safe autonomous driving. The demo video and code can\nbe accessed at: https://ys-qu.github.io/vlsafe-website/"}
{"id": "2505.16146", "pdf": "https://arxiv.org/pdf/2505.16146", "abs": "https://arxiv.org/abs/2505.16146", "authors": ["Zhenglin Hua", "Jinghan He", "Zijun Yao", "Tianxu Han", "Haiyun Guo", "Yuheng Jia", "Junfeng Fang"], "title": "Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large vision-language models (LVLMs) have achieved remarkable performance on\nmultimodal tasks such as visual question answering (VQA) and image captioning.\nHowever, they still suffer from hallucinations, generating text inconsistent\nwith visual input, posing significant risks in real-world applications.\nExisting approaches to address this issue focus on incorporating external\nknowledge bases, alignment training, or decoding strategies, all of which\nrequire substantial computational cost and time. Recent works try to explore\nmore efficient alternatives by adjusting LVLMs' internal representations.\nAlthough promising, these methods may cause hallucinations to be insufficiently\nsuppressed or lead to excessive interventions that negatively affect normal\nsemantics. In this work, we leverage sparse autoencoders (SAEs) to identify\nsemantic directions closely associated with either hallucinations or actuality,\nrealizing more precise and direct hallucination-related representations. Our\nanalysis demonstrates that interventions along the faithful direction we\nidentified can mitigate hallucinations, while those along the hallucinatory\ndirection can exacerbate them. Building on these insights, we propose Steering\nLVLMs via SAE Latent Directions (SSL), a training-free method based on\nSAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive\nexperiments demonstrate that SSL significantly outperforms existing decoding\napproaches in mitigating hallucinations, while maintaining transferability\nacross different model architectures with negligible additional time overhead."}
{"id": "2505.15984", "pdf": "https://arxiv.org/pdf/2505.15984", "abs": "https://arxiv.org/abs/2505.15984", "authors": ["Yamin Arefeen", "Brett Levac", "Bhairav Patel", "Chang Ho", "Jonathan I. Tamir"], "title": "Diffusion Probabilistic Generative Models for Accelerated, in-NICU Permanent Magnet Neonatal MRI", "categories": ["eess.IV", "cs.LG", "physics.med-ph"], "comment": null, "summary": "Purpose: Magnetic Resonance Imaging (MRI) enables non-invasive assessment of\nbrain abnormalities during early life development. Permanent magnet scanners\noperating in the neonatal intensive care unit (NICU) facilitate MRI of sick\ninfants, but have long scan times due to lower signal-to-noise ratios (SNR) and\nlimited receive coils. This work accelerates in-NICU MRI with diffusion\nprobabilistic generative models by developing a training pipeline accounting\nfor these challenges.\n  Methods: We establish a novel training dataset of clinical, 1 Tesla neonatal\nMR images in collaboration with Aspect Imaging and Sha'are Zedek Medical\nCenter. We propose a pipeline to handle the low quantity and SNR of our\nreal-world dataset (1) modifying existing network architectures to support\nvarying resolutions; (2) training a single model on all data with learned class\nembedding vectors; (3) applying self-supervised denoising before training; and\n(4) reconstructing by averaging posterior samples. Retrospective under-sampling\nexperiments, accounting for signal decay, evaluated each item of our proposed\nmethodology. A clinical reader study with practicing pediatric\nneuroradiologists evaluated our proposed images reconstructed from 1.5x\nunder-sampled data.\n  Results: Combining all data, denoising pre-training, and averaging posterior\nsamples yields quantitative improvements in reconstruction. The generative\nmodel decouples the learned prior from the measurement model and functions at\ntwo acceleration rates without re-training. The reader study suggests that\nproposed images reconstructed from approximately 1.5x under-sampled data are\nadequate for clinical use.\n  Conclusion: Diffusion probabilistic generative models applied with the\nproposed pipeline to handle challenging real-world datasets could reduce scan\ntime of in-NICU neonatal MRI."}
{"id": "2505.16379", "pdf": "https://arxiv.org/pdf/2505.16379", "abs": "https://arxiv.org/abs/2505.16379", "authors": ["Zhixun Li", "Bin Cao", "Rui Jiao", "Liang Wang", "Ding Wang", "Yang Liu", "Dingshuo Chen", "Jia Li", "Qiang Liu", "Yu Rong", "Liang Wang", "Tong-yi Zhang", "Jeffrey Xu Yu"], "title": "Materials Generation in the Era of Artificial Intelligence: A Comprehensive Survey", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "comment": "Work in progress", "summary": "Materials are the foundation of modern society, underpinning advancements in\nenergy, electronics, healthcare, transportation, and infrastructure. The\nability to discover and design new materials with tailored properties is\ncritical to solving some of the most pressing global challenges. In recent\nyears, the growing availability of high-quality materials data combined with\nrapid advances in Artificial Intelligence (AI) has opened new opportunities for\naccelerating materials discovery. Data-driven generative models provide a\npowerful tool for materials design by directly create novel materials that\nsatisfy predefined property requirements. Despite the proliferation of related\nwork, there remains a notable lack of up-to-date and systematic surveys in this\narea. To fill this gap, this paper provides a comprehensive overview of recent\nprogress in AI-driven materials generation. We first organize various types of\nmaterials and illustrate multiple representations of crystalline materials. We\nthen provide a detailed summary and taxonomy of current AI-driven materials\ngeneration approaches. Furthermore, we discuss the common evaluation metrics\nand summarize open-source codes and benchmark datasets. Finally, we conclude\nwith potential future directions and challenges in this fast-growing field. The\nrelated sources can be found at\nhttps://github.com/ZhixunLEE/Awesome-AI-for-Materials-Generation."}
{"id": "2505.16148", "pdf": "https://arxiv.org/pdf/2505.16148", "abs": "https://arxiv.org/abs/2505.16148", "authors": ["Chongjie Si", "Kangtao Lv", "Jingjing Jiang", "Yadao Wang", "Yongwei Wang", "Xiaokang Yang", "Wenbo Su", "Bo Zheng", "Wei Shen"], "title": "NAN: A Training-Free Solution to Coefficient Estimation in Model Merging", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Model merging offers a training-free alternative to multi-task learning by\ncombining independently fine-tuned models into a unified one without access to\nraw data. However, existing approaches often rely on heuristics to determine\nthe merging coefficients, limiting their scalability and generality. In this\nwork, we revisit model merging through the lens of least-squares optimization\nand show that the optimal merging weights should scale with the amount of\ntask-specific information encoded in each model. Based on this insight, we\npropose NAN, a simple yet effective method that estimates model merging\ncoefficients via the inverse of parameter norm. NAN is training-free,\nplug-and-play, and applicable to a wide range of merging strategies. Extensive\nexperiments on show that NAN consistently improves performance of baseline\nmethods."}
{"id": "2505.16022", "pdf": "https://arxiv.org/pdf/2505.16022", "abs": "https://arxiv.org/abs/2505.16022", "authors": ["Wei Liu", "Siya Qi", "Xinyu Wang", "Chen Qian", "Yali Du", "Yulan He"], "title": "NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "20 pages, 5 tables, 12 figures", "summary": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of\nincentive training, a reinforcement learning paradigm that computes rewards\nsolely based on the final answer part of a language model's output, thereby\nencouraging the generation of intermediate reasoning steps. However, these\nmethods fundamentally rely on external verifiers, which limits their\napplicability to domains like mathematics and coding where such verifiers are\nreadily available. Although reward models can serve as verifiers, they require\nhigh-quality annotated data and are costly to train. In this work, we propose\nNOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning\nframework that requires only standard supervised fine-tuning data with no need\nfor an external verifier. NOVER enables incentive training across a wide range\nof text-to-text tasks and outperforms the model of the same size distilled from\nlarge reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the\nflexibility of NOVER enables new possibilities for optimizing large language\nmodels, such as inverse incentive training."}
{"id": "2505.16392", "pdf": "https://arxiv.org/pdf/2505.16392", "abs": "https://arxiv.org/abs/2505.16392", "authors": ["Benjamin Vendeville", "Liana Ermakova", "Pierre De Loor"], "title": "Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection", "categories": ["cs.CL", "cs.AI", "I.2.6; I.5.2"], "comment": "Accepted at SIGIR 2025", "summary": "The general public often encounters complex texts but does not have the time\nor expertise to fully understand them, leading to the spread of misinformation.\nAutomatic Text Simplification (ATS) helps make information more accessible, but\nits evaluation methods have not kept up with advances in text generation,\nespecially with Large Language Models (LLMs). In particular, recent studies\nhave shown that current ATS metrics do not correlate with the presence of\nerrors. Manual inspections have further revealed a variety of errors,\nunderscoring the need for a more nuanced evaluation framework, which is\ncurrently lacking. This resource paper addresses this gap by introducing a test\ncollection for detecting and classifying errors in simplified texts. First, we\npropose a taxonomy of errors, with a formal focus on information distortion.\nNext, we introduce a parallel dataset of automatically simplified scientific\ntexts. This dataset has been human-annotated with labels based on our proposed\ntaxonomy. Finally, we analyze the quality of the dataset, and we study the\nperformance of existing models to detect and classify errors from that\ntaxonomy. These contributions give researchers the tools to better evaluate\nerrors in ATS, develop more reliable models, and ultimately improve the quality\nof automatically simplified texts."}
{"id": "2505.16149", "pdf": "https://arxiv.org/pdf/2505.16149", "abs": "https://arxiv.org/abs/2505.16149", "authors": ["Zirui Pang", "Haosheng Tan", "Yuhan Pu", "Zhijie Deng", "Zhouan Shen", "Keyu Hu", "Jiaheng Wei"], "title": "When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Image classification benchmark datasets such as CIFAR, MNIST, and ImageNet\nserve as critical tools for model evaluation. However, despite the cleaning\nefforts, these datasets still suffer from pervasive noisy labels and often\ncontain missing labels due to the co-existing image pattern where multiple\nclasses appear in an image sample. This results in misleading model comparisons\nand unfair evaluations. Existing label cleaning methods focus primarily on\nnoisy labels, but the issue of missing labels remains largely overlooked.\nMotivated by these challenges, we present a comprehensive framework named\nREVEAL, integrating state-of-the-art pre-trained vision-language models (e.g.,\nLLaVA, BLIP, Janus, Qwen) with advanced machine/human label curation methods\n(e.g., Docta, Cleanlab, MTurk), to systematically address both noisy labels and\nmissing label detection in widely-used image classification test sets. REVEAL\ndetects potential noisy labels and omissions, aggregates predictions from\nvarious methods, and refines label accuracy through confidence-informed\npredictions and consensus-based filtering. Additionally, we provide a thorough\nanalysis of state-of-the-art vision-language models and pre-trained image\nclassifiers, highlighting their strengths and limitations within the context of\ndataset renovation by revealing 10 observations. Our method effectively reveals\nmissing labels from public datasets and provides soft-labeled results with\nlikelihoods. Through human verifications, REVEAL significantly improves the\nquality of 6 benchmark test sets, highly aligning to human judgments and\nenabling more accurate and meaningful comparisons in image classification."}
{"id": "2505.16037", "pdf": "https://arxiv.org/pdf/2505.16037", "abs": "https://arxiv.org/abs/2505.16037", "authors": ["Asterios Tsiourvas", "Wei Sun", "Georgia Perakis"], "title": "Causal LLM Routing: End-to-End Regret Minimization from Observational Data", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "LLM routing aims to select the most appropriate model for each query,\nbalancing competing performance metrics such as accuracy and cost across a pool\nof language models. Prior approaches typically adopt a decoupled strategy,\nwhere the metrics are first predicted and the model is then selected based on\nthese estimates. This setup is prone to compounding errors and often relies on\nfull-feedback data, where each query is evaluated by all candidate models,\nwhich is costly to obtain and maintain in practice. In contrast, we learn from\nobservational data, which records only the outcome of the model actually\ndeployed. We propose a causal end-to-end framework that learns routing policies\nby minimizing decision-making regret from observational data. To enable\nefficient optimization, we introduce two theoretically grounded surrogate\nobjectives: a classification-based upper bound, and a softmax-weighted regret\napproximation shown to recover the optimal policy at convergence. We further\nextend our framework to handle heterogeneous cost preferences via an\ninterval-conditioned architecture. Experiments on public benchmarks show that\nour method outperforms existing baselines, achieving state-of-the-art\nperformance across different embedding models."}
{"id": "2505.16394", "pdf": "https://arxiv.org/pdf/2505.16394", "abs": "https://arxiv.org/abs/2505.16394", "authors": ["Zhenjie Yang", "Xiaosong Jia", "Qifeng Li", "Xue Yang", "Maoqing Yao", "Junchi Yan"], "title": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Reinforcement Learning (RL) can mitigate the causal confusion and\ndistribution shift inherent to imitation learning (IL). However, applying RL to\nend-to-end autonomous driving (E2E-AD) remains an open problem for its training\ndifficulty, and IL is still the mainstream paradigm in both academia and\nindustry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated\npromising results in neural planning; however, these methods typically require\nprivileged information as input rather than raw sensor data. We fill this gap\nby designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently\ntrain an auxiliary privileged world model paired with a neural planner that\nuses privileged information as input. Subsequently, we introduce a raw sensor\nworld model trained via our proposed Guidance Mechanism, which ensures\nconsistency between the raw sensor world model and the privileged world model\nduring rollouts. Finally, the raw sensor world model combines the prior\nknowledge embedded in the heads of the privileged world model to effectively\nguide the training of the raw sensor policy. Raw2Drive is so far the only RL\nbased end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it\nachieves state-of-the-art performance."}
{"id": "2505.16176", "pdf": "https://arxiv.org/pdf/2505.16176", "abs": "https://arxiv.org/abs/2505.16176", "authors": ["Jun Rao", "Xuebo Liu", "Hexuan Deng", "Zepeng Lin", "Zixiong Yu", "Jiansheng Wei", "Xiaojun Meng", "Min Zhang"], "title": "Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In the realm of data selection for reasoning tasks, existing approaches\npredominantly rely on externally predefined static metrics such as difficulty\nand diversity, which are often designed for supervised fine-tuning (SFT) and\nlack adaptability to continuous training processes. A critical limitation of\nthese methods is their inability to dynamically align with the evolving\ncapabilities of models during online training, a gap that becomes increasingly\npronounced with the rise of dynamic training paradigms and online reinforcement\nlearning (RL) frameworks (e.g., R1 models). To address this, we introduce\nSAI-DPO, an algorithm that dynamically selects training data by continuously\nassessing a model's stage-specific reasoning abilities across different\ntraining phases. By integrating real-time model performance feedback, SAI-DPO\nadaptively adapts data selection to the evolving strengths and weaknesses of\nthe model, thus enhancing both data utilization efficiency and final task\nperformance. Extensive experiments on three state-of-the-art models and eight\nmathematical reasoning benchmarks, including challenging competition-level\ndatasets (e.g., AIME24 and AMC23), demonstrate that SAI-DPO achieves an average\nperformance boost of up to 21.3 percentage points, with particularly notable\nimprovements of 10 and 15 points on AIME24 and AMC23, respectively. These\nresults highlight the superiority of dynamic, model-adaptive data selection\nover static, externally defined strategies in advancing reasoning."}
{"id": "2505.16041", "pdf": "https://arxiv.org/pdf/2505.16041", "abs": "https://arxiv.org/abs/2505.16041", "authors": ["Siddhant Agarwal", "Ali Can Bekar", "Christian Hüttig", "David S. Greenberg", "Nicola Tosi"], "title": "Physics-based machine learning for mantle convection simulations", "categories": ["astro-ph.EP", "cs.LG"], "comment": null, "summary": "Mantle convection simulations are an essential tool for understanding how\nrocky planets evolve. However, the poorly known input parameters to these\nsimulations, the non-linear dependence of transport properties on pressure and\ntemperature, and the long integration times in excess of several billion years\nall pose a computational challenge for numerical solvers. We propose a\nphysics-based machine learning approach that predicts creeping flow velocities\nas a function of temperature while conserving mass, thereby bypassing the\nnumerical solution of the Stokes problem. A finite-volume solver then uses the\npredicted velocities to advect and diffuse the temperature field to the next\ntime-step, enabling autoregressive rollout at inference. For training, our\nmodel requires temperature-velocity snapshots from a handful of simulations\n(94). We consider mantle convection in a two-dimensional rectangular box with\nbasal and internal heating, pressure- and temperature-dependent viscosity.\nOverall, our model is up to 89 times faster than the numerical solver. We also\nshow the importance of different components in our convolutional neural network\narchitecture such as mass conservation, learned paddings on the boundaries, and\nloss scaling for the overall rollout performance. Finally, we test our approach\non unseen scenarios to demonstrate some of its strengths and weaknesses."}
{"id": "2505.16400", "pdf": "https://arxiv.org/pdf/2505.16400", "abs": "https://arxiv.org/abs/2505.16400", "authors": ["Yang Chen", "Zhuolin Yang", "Zihan Liu", "Chankyu Lee", "Peng Xu", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "We release the model at:\n  https://huggingface.co/nvidia/AceReason-Nemotron-14B", "summary": "Despite recent progress in large-scale reinforcement learning (RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such as\nDeepSeek-R1, including data curation strategies and RL training recipe, are\noften omitted. Moreover, recent research indicates distillation remains more\neffective than RL for smaller models. In this work, we demonstrate that\nlarge-scale RL can significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-art distillation-based models. We systematically study the RL\ntraining process through extensive ablations and propose a simple yet effective\napproach: first training on math-only prompts, then on code-only prompts.\nNotably, we find that math-only RL not only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,\nextended code-only RL iterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-based RL across both domains.\nFinally, we identify key experimental insights, including curriculum learning\nwith progressively increasing response lengths and the stabilizing effect of\non-policy parameter updates. We find that RL not only elicits the foundational\nreasoning capabilities acquired during pretraining and supervised fine-tuning\n(e.g., distillation), but also pushes the limits of the model's reasoning\nability, enabling it to solve problems that were previously unsolvable."}
{"id": "2505.16180", "pdf": "https://arxiv.org/pdf/2505.16180", "abs": "https://arxiv.org/abs/2505.16180", "authors": ["Ashim Dahal", "Ankit Ghimire", "Saydul Akbar Murad", "Nick Rahimi"], "title": "Redemption Score: An Evaluation Framework to Rank Image Captions While Redeeming Image Semantics and Language Pragmatics", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Evaluating image captions requires cohesive assessment of both visual\nsemantics and language pragmatics, which is often not entirely captured by most\nmetrics. We introduce Redemption Score, a novel hybrid framework that ranks\nimage captions by triangulating three complementary signals: (1) Mutual\nInformation Divergence (MID) for global image-text distributional alignment,\n(2) DINO-based perceptual similarity of cycle-generated images for visual\ngrounding, and (3) BERTScore for contextual text similarity against human\nreferences. A calibrated fusion of these signals allows Redemption Score to\noffer a more holistic assessment. On the Flickr8k benchmark, Redemption Score\nachieves a Kendall-$\\tau$ of 56.43, outperforming twelve prior methods and\ndemonstrating superior correlation with human judgments without requiring\ntask-specific training. Our framework provides a more robust and nuanced\nevaluation by effectively redeeming image semantics and linguistic\ninterpretability indicated by strong transfer of knowledge in the Conceptual\nCaptions and MS COCO datasets."}
{"id": "2505.16044", "pdf": "https://arxiv.org/pdf/2505.16044", "abs": "https://arxiv.org/abs/2505.16044", "authors": ["Gowtham Premananth", "Philip Resnik", "Sonia Bansal", "Deanna L. Kelly", "Carol Espy-Wilson"], "title": "Multimodal Biomarkers for Schizophrenia: Towards Individual Symptom Severity Estimation", "categories": ["eess.AS", "cs.LG", "eess.IV", "eess.SP"], "comment": "Accepted to be presented at Interspeech 2025", "summary": "Studies on schizophrenia assessments using deep learning typically treat it\nas a classification task to detect the presence or absence of the disorder,\noversimplifying the condition and reducing its clinical applicability. This\ntraditional approach overlooks the complexity of schizophrenia, limiting its\npractical value in healthcare settings. This study shifts the focus to\nindividual symptom severity estimation using a multimodal approach that\nintegrates speech, video, and text inputs. We develop unimodal models for each\nmodality and a multimodal framework to improve accuracy and robustness. By\ncapturing a more detailed symptom profile, this approach can help in enhancing\ndiagnostic precision and support personalized treatment, offering a scalable\nand objective tool for mental health assessment."}
{"id": "2505.16410", "pdf": "https://arxiv.org/pdf/2505.16410", "abs": "https://arxiv.org/abs/2505.16410", "authors": ["Guanting Dong", "Yifei Chen", "Xiaoxi Li", "Jiajie Jin", "Hongjin Qian", "Yutao Zhu", "Hangyu Mao", "Guorui Zhou", "Zhicheng Dou", "Ji-Rong Wen"], "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Working in progress", "summary": "Recently, large language models (LLMs) have shown remarkable reasoning\ncapabilities via large-scale reinforcement learning (RL). However, leveraging\nthe RL algorithm to empower effective multi-tool collaborative reasoning in\nLLMs remains an open challenge. In this paper, we introduce Tool-Star, an\nRL-based framework designed to empower LLMs to autonomously invoke multiple\nexternal tools during stepwise reasoning. Tool-Star integrates six types of\ntools and incorporates systematic designs in both data synthesis and training.\nTo address the scarcity of tool-use data, we propose a general tool-integrated\nreasoning data synthesis pipeline, which combines tool-integrated prompting\nwith hint-based sampling to automatically and scalably generate tool-use\ntrajectories. A subsequent quality normalization and difficulty-aware\nclassification process filters out low-quality samples and organizes the\ndataset from easy to hard. Furthermore, we propose a two-stage training\nframework to enhance multi-tool collaborative reasoning by: (1) cold-start\nfine-tuning, which guides LLMs to explore reasoning patterns via\ntool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with\nhierarchical reward design, which reinforces reward understanding and promotes\neffective tool collaboration. Experimental analyses on over 10 challenging\nreasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.\nThe code is available at https://github.com/dongguanting/Tool-Star."}
{"id": "2505.16186", "pdf": "https://arxiv.org/pdf/2505.16186", "abs": "https://arxiv.org/abs/2505.16186", "authors": ["Kaiwen Zhou", "Xuandong Zhao", "Gaowen Liu", "Jayanth Srinivasa", "Aosong Feng", "Dawn Song", "Xin Eric Wang"], "title": "SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning", "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Large Reasoning Models (LRMs) introduce a new generation paradigm of\nexplicitly reasoning before answering, leading to remarkable improvements in\ncomplex tasks. However, they pose great safety risks against harmful queries\nand adversarial attacks. While recent mainstream safety efforts on LRMs,\nsupervised fine-tuning (SFT), improve safety performance, we find that\nSFT-aligned models struggle to generalize to unseen jailbreak prompts. After\nthorough investigation of LRMs' generation, we identify a safety aha moment\nthat can activate safety reasoning and lead to a safe response. This aha moment\ntypically appears in the `key sentence', which follows models' query\nunderstanding process and can indicate whether the model will proceed safely.\nBased on these insights, we propose SafeKey, including two complementary\nobjectives to better activate the safety aha moment in the key sentence: (1) a\nDual-Path Safety Head to enhance the safety signal in the model's internal\nrepresentations before the key sentence, and (2) a Query-Mask Modeling\nobjective to improve the models' attention on its query understanding, which\nhas important safety hints. Experiments across multiple safety benchmarks\ndemonstrate that our methods significantly improve safety generalization to a\nwide range of jailbreak attacks and out-of-distribution harmful prompts,\nlowering the average harmfulness rate by 9.6\\%, while maintaining general\nabilities. Our analysis reveals how SafeKey enhances safety by reshaping\ninternal attention and improving the quality of hidden representations."}
{"id": "2505.16051", "pdf": "https://arxiv.org/pdf/2505.16051", "abs": "https://arxiv.org/abs/2505.16051", "authors": ["Dongze Wu", "David I. Inouye", "Yao Xie"], "title": "PO-Flow: Flow-based Generative Models for Sampling Potential Outcomes and Counterfactuals", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We propose PO-Flow, a novel continuous normalizing flow (CNF) framework for\ncausal inference that jointly models potential outcomes and counterfactuals.\nTrained via flow matching, PO-Flow provides a unified framework for\nindividualized potential outcome prediction, counterfactual predictions, and\nuncertainty-aware density learning. Among generative models, it is the first to\nenable density learning of potential outcomes without requiring explicit\ndistributional assumptions (e.g., Gaussian mixtures), while also supporting\ncounterfactual prediction conditioned on factual outcomes in general\nobservational datasets. On benchmarks such as ACIC, IHDP, and IBM, it\nconsistently outperforms prior methods across a range of causal inference\ntasks. Beyond that, PO-Flow succeeds in high-dimensional settings, including\ncounterfactual image generation, demonstrating its broad applicability."}
{"id": "2505.16415", "pdf": "https://arxiv.org/pdf/2505.16415", "abs": "https://arxiv.org/abs/2505.16415", "authors": ["Ruizhe Li", "Chen Chen", "Yuchen Hu", "Yanjun Gao", "Xi Wang", "Emine Yilmaz"], "title": "Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in process", "summary": "Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)\ncombined with external contexts to enhance the accuracy and reliability of\ngenerated responses. However, reliably attributing generated content to\nspecific context segments, context attribution, remains challenging due to the\ncomputationally intensive nature of current methods, which often require\nextensive fine-tuning or human annotation. In this work, we introduce a novel\nJensen-Shannon Divergence driven method to Attribute Response to Context\n(ARC-JSD), enabling efficient and accurate identification of essential context\nsentences without additional fine-tuning or surrogate modelling. Evaluations on\na wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using\ninstruction-tuned LLMs in different scales demonstrate superior accuracy and\nsignificant computational efficiency improvements compared to the previous\nsurrogate-based method. Furthermore, our mechanistic analysis reveals specific\nattention heads and multilayer perceptron (MLP) layers responsible for context\nattribution, providing valuable insights into the internal workings of RAG\nmodels."}
{"id": "2505.16210", "pdf": "https://arxiv.org/pdf/2505.16210", "abs": "https://arxiv.org/abs/2505.16210", "authors": ["Zhihang Cai", "Xingjun Zhang", "Zhendong Tan", "Zheng Wei"], "title": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "11 pages, 9 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used."}
{"id": "2505.16082", "pdf": "https://arxiv.org/pdf/2505.16082", "abs": "https://arxiv.org/abs/2505.16082", "authors": ["Renato Berlinghieri", "Yunyi Shen", "Jialong Jiang", "Tamara Broderick"], "title": "Oh SnapMMD! Forecasting Stochastic Dynamics Beyond the Schrödinger Bridge's End", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": "43 pages, 26 figures, 21 tables", "summary": "Scientists often want to make predictions beyond the observed time horizon of\n\"snapshot\" data following latent stochastic dynamics. For example, in time\ncourse single-cell mRNA profiling, scientists have access to cellular\ntranscriptional state measurements (snapshots) from different biological\nreplicates at different time points, but they cannot access the trajectory of\nany one cell because measurement destroys the cell. Researchers want to\nforecast (e.g.) differentiation outcomes from early state measurements of stem\ncells. Recent Schr\\\"odinger-bridge (SB) methods are natural for interpolating\nbetween snapshots. But past SB papers have not addressed forecasting -- likely\nsince existing methods either (1) reduce to following pre-set reference\ndynamics (chosen before seeing data) or (2) require the user to choose a fixed,\nstate-independent volatility since they minimize a Kullback-Leibler divergence.\nEither case can lead to poor forecasting quality. In the present work, we\npropose a new framework, SnapMMD, that learns dynamics by directly fitting the\njoint distribution of both state measurements and observation time with a\nmaximum mean discrepancy (MMD) loss. Unlike past work, our method allows us to\ninfer unknown and state-dependent volatilities from the observed data. We show\nin a variety of real and synthetic experiments that our method delivers\naccurate forecasts. Moreover, our approach allows us to learn in the presence\nof incomplete state measurements and yields an $R^2$-style statistic that\ndiagnoses fit. We also find that our method's performance at interpolation (and\ngeneral velocity-field reconstruction) is at least as good as (and often better\nthan) state-of-the-art in almost all of our experiments."}
{"id": "2505.16416", "pdf": "https://arxiv.org/pdf/2505.16416", "abs": "https://arxiv.org/abs/2505.16416", "authors": ["Chengcheng Wang", "Jianyuan Guo", "Hongguang Li", "Yuchuan Tian", "Ying Nie", "Chang Xu", "Kai Han"], "title": "Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Rotary Position Embedding (RoPE) is a widely adopted technique for encoding\nrelative positional information in large language models (LLMs). However, when\nextended to large vision-language models (LVLMs), its variants introduce\nunintended cross-modal positional biases. Specifically, they enforce relative\npositional dependencies between text token indices and image tokens, causing\nspurious alignments. This issue arises because image tokens representing the\nsame content but located at different spatial positions are assigned distinct\npositional biases, leading to inconsistent cross-modal associations. To address\nthis, we propose Per-Token Distance (PTD) - a simple yet effective metric for\nquantifying the independence of positional encodings across modalities.\nInformed by this analysis, we introduce Circle-RoPE, a novel encoding scheme\nthat maps image token indices onto a circular trajectory orthogonal to the\nlinear path of text token indices, forming a cone-like structure. This\nconfiguration ensures that each text token maintains an equal distance to all\nimage tokens, reducing artificial cross-modal biases while preserving\nintra-image spatial information. To further enhance performance, we propose a\nstaggered layer strategy that applies different RoPE variants across layers.\nThis design leverages the complementary strengths of each RoPE variant, thereby\nenhancing the model's overall performance. Our experimental results demonstrate\nthat our method effectively preserves spatial information from images while\nreducing relative positional bias, offering a more robust and flexible\npositional encoding framework for LVLMs. The code is available at\n[https://github.com/lose4578/CircleRoPE](https://github.com/lose4578/CircleRoPE)."}
{"id": "2505.16211", "pdf": "https://arxiv.org/pdf/2505.16211", "abs": "https://arxiv.org/abs/2505.16211", "authors": ["Kai Li", "Can Shen", "Yile Liu", "Jirui Han", "Kelong Zheng", "Xuechao Zou", "Zhe Wang", "Xingjian Du", "Shun Zhang", "Hanjun Luo", "Yingbin Jin", "Xinxin Xing", "Ziyang Ma", "Yue Liu", "Xiaojun Jia", "Yifan Zhang", "Junfeng Fang", "Kun Wang", "Yibo Yan", "Haoyang Li", "Yiming Li", "Xiaobin Zhuang", "Yang Liu", "Haibo Hu", "Zhuo Chen", "Zhizheng Wu", "Xiaolin Hu", "Eng-Siong Chng", "XiaoFeng Wang", "Wenyuan Xu", "Wei Dong", "Xinfeng Li"], "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "Technical Report", "summary": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust."}
{"id": "2505.16098", "pdf": "https://arxiv.org/pdf/2505.16098", "abs": "https://arxiv.org/abs/2505.16098", "authors": ["Damien Ferbach", "Katie Everett", "Gauthier Gidel", "Elliot Paquette", "Courtney Paquette"], "title": "Dimension-adapted Momentum Outscales SGD", "categories": ["stat.ML", "cs.LG", "math.OC"], "comment": null, "summary": "We investigate scaling laws for stochastic momentum algorithms with small\nbatch on the power law random features model, parameterized by data complexity,\ntarget complexity, and model size. When trained with a stochastic momentum\nalgorithm, our analysis reveals four distinct loss curve shapes determined by\nvarying data-target complexities. While traditional stochastic gradient descent\nwith momentum (SGD-M) yields identical scaling law exponents to SGD,\ndimension-adapted Nesterov acceleration (DANA) improves these exponents by\nscaling momentum hyperparameters based on model size and data complexity. This\noutscaling phenomenon, which also improves compute-optimal scaling behavior, is\nachieved by DANA across a broad range of data and target complexities, while\ntraditional methods fall short. Extensive experiments on high-dimensional\nsynthetic quadratics validate our theoretical predictions and large-scale text\nexperiments with LSTMs show DANA's improved loss exponents over SGD hold in a\npractical setting."}
{"id": "2505.16419", "pdf": "https://arxiv.org/pdf/2505.16419", "abs": "https://arxiv.org/abs/2505.16419", "authors": ["Soh Takahashi", "Masaru Sasaki", "Ken Takeda", "Masafumi Oizumi"], "title": "Investigating Fine- and Coarse-grained Structural Correspondences Between Deep Neural Networks and Human Object Image Similarity Judgments Using Unsupervised Alignment", "categories": ["cs.CV", "cs.AI"], "comment": "34 pages, 6 figures", "summary": "The learning mechanisms by which humans acquire internal representations of\nobjects are not fully understood. Deep neural networks (DNNs) have emerged as a\nuseful tool for investigating this question, as they have internal\nrepresentations similar to those of humans as a byproduct of optimizing their\nobjective functions. While previous studies have shown that models trained with\nvarious learning paradigms - such as supervised, self-supervised, and CLIP -\nacquire human-like representations, it remains unclear whether their similarity\nto human representations is primarily at a coarse category level or extends to\nfiner details. Here, we employ an unsupervised alignment method based on\nGromov-Wasserstein Optimal Transport to compare human and model object\nrepresentations at both fine-grained and coarse-grained levels. The unique\nfeature of this method compared to conventional representational similarity\nanalysis is that it estimates optimal fine-grained mappings between the\nrepresentation of each object in human and model representations. We used this\nunsupervised alignment method to assess the extent to which the representation\nof each object in humans is correctly mapped to the corresponding\nrepresentation of the same object in models. Using human similarity judgments\nof 1,854 objects from the THINGS dataset, we find that models trained with CLIP\nconsistently achieve strong fine- and coarse-grained matching with human object\nrepresentations. In contrast, self-supervised models showed limited matching at\nboth fine- and coarse-grained levels, but still formed object clusters that\nreflected human coarse category structure. Our results offer new insights into\nthe role of linguistic information in acquiring precise object representations\nand the potential of self-supervised learning to capture coarse categorical\nstructures."}
{"id": "2505.16220", "pdf": "https://arxiv.org/pdf/2505.16220", "abs": "https://arxiv.org/abs/2505.16220", "authors": ["Liang-Yeh Shen", "Shi-Xin Fang", "Yi-Cheng Lin", "Huang-Cheng Chou", "Hung-yi Lee"], "title": "Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition via Meta-learning", "categories": ["eess.AS", "cs.CL"], "comment": "Accepted by INTERSPEECH 2025. 7 pages, including 2 pages of appendix", "summary": "This paper introduces Meta-PerSER, a novel meta-learning framework that\npersonalizes Speech Emotion Recognition (SER) by adapting to each listener's\nunique way of interpreting emotion. Conventional SER systems rely on aggregated\nannotations, which often overlook individual subtleties and lead to\ninconsistent predictions. In contrast, Meta-PerSER leverages a Model-Agnostic\nMeta-Learning (MAML) approach enhanced with Combined-Set Meta-Training,\nDerivative Annealing, and per-layer per-step learning rates, enabling rapid\nadaptation with only a few labeled examples. By integrating robust\nrepresentations from pre-trained self-supervised models, our framework first\ncaptures general emotional cues and then fine-tunes itself to personal\nannotation styles. Experiments on the IEMOCAP corpus demonstrate that\nMeta-PerSER significantly outperforms baseline methods in both seen and unseen\ndata scenarios, highlighting its promise for personalized emotion recognition."}
{"id": "2505.16104", "pdf": "https://arxiv.org/pdf/2505.16104", "abs": "https://arxiv.org/abs/2505.16104", "authors": ["Yue Li", "Xin Yi", "Dongsheng Shi", "Gerard de Melo", "Xiaoling Wang", "Linlin Wang"], "title": "Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "ACL 2025 Findings", "summary": "With the increasing size of Large Vision-Language Models (LVLMs), network\npruning techniques aimed at compressing models for deployment in\nresource-constrained environments have garnered significant attention. However,\nwe observe that pruning often leads to a degradation in safety performance. To\naddress this issue, we present a novel and lightweight approach, termed\nHierarchical Safety Realignment (HSR). HSR operates by first quantifying the\ncontribution of each attention head to safety, identifying the most critical\nones, and then selectively restoring neurons directly within these attention\nheads that play a pivotal role in maintaining safety. This process\nhierarchically realigns the safety of pruned LVLMs, progressing from the\nattention head level to the neuron level. We validate HSR across various models\nand pruning strategies, consistently achieving notable improvements in safety\nperformance. To our knowledge, this is the first work explicitly focused on\nrestoring safety in LVLMs post-pruning."}
{"id": "2505.16425", "pdf": "https://arxiv.org/pdf/2505.16425", "abs": "https://arxiv.org/abs/2505.16425", "authors": ["Jing Bi", "Pinxin Liu", "Ali Vosoughi", "Jiarui Wu", "Jinxi He", "Chenliang Xu"], "title": "$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 5 figures, under review", "summary": "The effective communication of procedural knowledge remains a significant\nchallenge in natural language processing (NLP), as purely textual instructions\noften fail to convey complex physical actions and spatial relationships. We\naddress this limitation by proposing a language-driven framework that\ntranslates procedural text into coherent visual instructions. Our approach\nmodels the linguistic structure of instructional content by decomposing it into\ngoal statements and sequential steps, then conditioning visual generation on\nthese linguistic elements. We introduce three key innovations: (1) a\nconstituency parser-based text encoding mechanism that preserves semantic\ncompleteness even with lengthy instructions, (2) a pairwise discourse coherence\nmodel that maintains consistency across instruction sequences, and (3) a novel\nevaluation protocol specifically designed for procedural language-to-image\nalignment. Our experiments across three instructional datasets (HTStep,\nCaptainCook4D, and WikiAll) demonstrate that our method significantly\noutperforms existing baselines in generating visuals that accurately reflect\nthe linguistic content and sequential nature of instructions. This work\ncontributes to the growing body of research on grounding procedural language in\nvisual content, with applications spanning education, task guidance, and\nmultimodal language understanding."}
{"id": "2505.16263", "pdf": "https://arxiv.org/pdf/2505.16263", "abs": "https://arxiv.org/abs/2505.16263", "authors": ["Sampanna Yashwant Kahu", "Naman Ahuja"], "title": "All You Need is \"Leet\": Evading Hate-speech Detection AI", "categories": ["cs.CR", "cs.CL", "cs.LG", "K.6.5"], "comment": "10 pages, 22 figures, The source code and data used in this work is\n  available at: https://github.com/SampannaKahu/all_you_need_is_leet", "summary": "Social media and online forums are increasingly becoming popular.\nUnfortunately, these platforms are being used for spreading hate speech. In\nthis paper, we design black-box techniques to protect users from hate-speech on\nonline platforms by generating perturbations that can fool state of the art\ndeep learning based hate speech detection models thereby decreasing their\nefficiency. We also ensure a minimal change in the original meaning of\nhate-speech. Our best perturbation attack is successfully able to evade\nhate-speech detection for 86.8 % of hateful text."}
{"id": "2505.16131", "pdf": "https://arxiv.org/pdf/2505.16131", "abs": "https://arxiv.org/abs/2505.16131", "authors": ["Nathan Brady", "David Tennyson", "Thomas Vandermeulen"], "title": "Machine Learning the 6d Supergravity Landscape", "categories": ["hep-th", "cs.LG"], "comment": "49 pages; code and data available at\n  https://github.com/nait400/ML-6d-sugra-landscape", "summary": "In this paper, we apply both supervised and unsupervised machine learning\nalgorithms to the study of the string landscape and swampland in 6-dimensions.\nOur data are the (almost) anomaly-free 6-dimensional $\\mathcal{N} = (1,0)$\nsupergravity models, characterised by the Gram matrix of anomaly coefficients.\nOur work demonstrates the ability of machine learning algorithms to efficiently\nlearn highly complex features of the landscape and swampland. Employing an\nautoencoder for unsupervised learning, we provide an auto-classification of\nthese models by compressing the Gram matrix data to 2-dimensions. Through\ncompression, similar models cluster together, and we identify prominent\nfeatures of these clusters. The autoencoder also identifies outlier models\nwhich are difficult to reconstruct. One of these outliers proves to be\nincredibly difficult to combine with other models such that the\n$\\text{tr}R^{4}$ anomaly vanishes, making its presence in the landscape\nextremely rare. Further, we utilise supervised learning to build two\nclassifiers predicting (1) model consistency under probe string insertion\n(precision: 0.78, predicting consistency for 214,837 models with reasonable\ncertainty) and (2) inconsistency under anomaly inflow (precision: 0.91,\npredicting inconsistency for 1,909,359 models). Notably, projecting these\npredictions onto the autoencoder's 2-dimensional latent layer shows consistent\nmodels clustering together, further indicating that the autoencoder has learnt\ninteresting and complex features of the set of models and potentially offers a\nnovel approach to mapping the landscape and swampland of 6-dimensional\nsupergravity theories."}
{"id": "2505.16429", "pdf": "https://arxiv.org/pdf/2505.16429", "abs": "https://arxiv.org/abs/2505.16429", "authors": ["Song Jin", "Juntian Zhang", "Yuhan Liu", "Xun Zhang", "Yufei Zhang", "Guojun Yin", "Fei Jiang", "Wei Lin", "Rui Yan"], "title": "Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating and iterating upon recommender systems is crucial, yet traditional\nA/B testing is resource-intensive, and offline methods struggle with dynamic\nuser-platform interactions. While agent-based simulation is promising, existing\nplatforms often lack a mechanism for user actions to dynamically reshape the\nenvironment. To bridge this gap, we introduce RecInter, a novel agent-based\nsimulation platform for recommender systems featuring a robust interaction\nmechanism. In RecInter platform, simulated user actions (e.g., likes, reviews,\npurchases) dynamically update item attributes in real-time, and introduced\nMerchant Agents can reply, fostering a more realistic and evolving ecosystem.\nHigh-fidelity simulation is ensured through Multidimensional User Profiling\nmodule, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought\n(CoT) enriched interaction data. Our platform achieves significantly improved\nsimulation credibility and successfully replicates emergent phenomena like\nBrand Loyalty and the Matthew Effect. Experiments demonstrate that this\ninteraction mechanism is pivotal for simulating realistic system evolution,\nestablishing our platform as a credible testbed for recommender systems\nresearch."}
{"id": "2505.16276", "pdf": "https://arxiv.org/pdf/2505.16276", "abs": "https://arxiv.org/abs/2505.16276", "authors": ["Desiree Heim", "Lars-Peter Meyer", "Markus Schröder", "Johannes Frey", "Andreas Dengel"], "title": "How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance", "categories": ["cs.AI", "cs.CL"], "comment": "Peer reviewed and to appear in the ESWC 2025 Workshops and Tutorials\n  Joint Proceedings (Workshop on Evaluation of Language Models in Knowledge\n  Engineering [ELMKE])", "summary": "When using Large Language Models (LLMs) to support Knowledge Graph\nEngineering (KGE), one of the first indications when searching for an\nappropriate model is its size. According to the scaling laws, larger models\ntypically show higher capabilities. However, in practice, resource costs are\nalso an important factor and thus it makes sense to consider the ratio between\nmodel performance and costs. The LLM-KG-Bench framework enables the comparison\nof LLMs in the context of KGE tasks and assesses their capabilities of\nunderstanding and producing KGs and KG queries. Based on a dataset created in\nan LLM-KG-Bench run covering 26 open state-of-the-art LLMs, we explore the\nmodel size scaling laws specific to KGE tasks. In our analyses, we assess how\nbenchmark scores evolve between different model size categories. Additionally,\nwe inspect how the general score development of single models and families of\nmodels correlates to their size. Our analyses revealed that, with a few\nexceptions, the model size scaling laws generally also apply to the selected\nKGE tasks. However, in some cases, plateau or ceiling effects occurred, i.e.,\nthe task performance did not change much between a model and the next larger\nmodel. In these cases, smaller models could be considered to achieve high\ncost-effectiveness. Regarding models of the same family, sometimes larger\nmodels performed worse than smaller models of the same family. These effects\noccurred only locally. Hence it is advisable to additionally test the next\nsmallest and largest model of the same family."}
{"id": "2505.16134", "pdf": "https://arxiv.org/pdf/2505.16134", "abs": "https://arxiv.org/abs/2505.16134", "authors": ["Menschikov Mikhail", "Alexander Kharitonov", "Maiia Kotyga", "Vadim Porvatov", "Anna Zhukovskaya", "David Kagramanyan", "Egor Shvetsov", "Evgeny Burnaev"], "title": "Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models exhibit positional bias -- systematic neglect of\ninformation at specific context positions -- yet its interplay with linguistic\ndiversity remains poorly understood. We present a cross-linguistic study across\nfive typologically distinct languages (English, Russian, German, Hindi,\nVietnamese), examining how positional bias interacts with model uncertainty,\nsyntax, and prompting. Key findings: (1) Positional bias is model-driven, with\nlanguage-specific variations -- Qwen2.5-7B favors late positions, challenging\nassumptions of early-token bias; (2) Explicit positional guidance (e.g.,\ncorrect context is at position X) reduces accuracy across languages,\nundermining prompt-engineering practices; (3) Aligning context with positional\nbias increases entropy, yet minimal entropy does not predict accuracy. (4) We\nfurther uncover that LLMs differently impose dominant word order in\nfree-word-order languages like Hindi."}
{"id": "2505.16430", "pdf": "https://arxiv.org/pdf/2505.16430", "abs": "https://arxiv.org/abs/2505.16430", "authors": ["Martin Goodfellow", "Robbie Booth", "Andrew Fagan", "Alasdair Lambert"], "title": "AutoMCQ -- Automatically Generate Code Comprehension Questions using GenAI", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "Students often do not fully understand the code they have written. This\nsometimes does not become evident until later in their education, which can\nmean it is harder to fix their incorrect knowledge or misunderstandings. In\naddition, being able to fully understand code is increasingly important in a\nworld where students have access to generative artificial intelligence (GenAI)\ntools, such as GitHub Copilot. One effective solution is to utilise code\ncomprehension questions, where a marker asks questions about a submission to\ngauge understanding, this can also have the side effect of helping to detect\nplagiarism. However, this approach is time consuming and can be difficult\nand/or expensive to scale. This paper introduces AutoMCQ, which uses GenAI for\nthe automatic generation of multiple-choice code comprehension questions. This\nis integrated with the CodeRunner automated assessment platform."}
{"id": "2505.16315", "pdf": "https://arxiv.org/pdf/2505.16315", "abs": "https://arxiv.org/abs/2505.16315", "authors": ["Xiaoxue Cheng", "Junyi Li", "Zhenduo Zhang", "Xinyu Tang", "Wayne Xin Zhao", "Xinyu Kong", "Zhiqiang Zhang"], "title": "Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "work in progress", "summary": "Large reasoning models (LRMs) have demonstrated strong performance on complex\nreasoning tasks, but often suffer from overthinking, generating redundant\ncontent regardless of task difficulty. Inspired by the dual process theory in\ncognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a\nreinforcement learning framework that enables LRMs to achieve efficient\nreasoning through adaptive cognitive allocation and dynamic system switch. ACPO\nincorporates two key components: (1) introducing system-aware reasoning tokens\nto explicitly represent the thinking modes thereby making the model's cognitive\nprocess transparent, and (2) integrating online difficulty estimation and token\nlength budget to guide adaptive system switch and reasoning during\nreinforcement learning. To this end, we propose a two-stage training strategy.\nThe first stage begins with supervised fine-tuning to cold start the model,\nenabling it to generate reasoning paths with explicit thinking modes. In the\nsecond stage, we apply ACPO to further enhance adaptive system switch for\ndifficulty-aware reasoning. Experimental results demonstrate that ACPO\neffectively reduces redundant reasoning while adaptively adjusting cognitive\nallocation based on task complexity, achieving efficient hybrid reasoning."}
{"id": "2505.16135", "pdf": "https://arxiv.org/pdf/2505.16135", "abs": "https://arxiv.org/abs/2505.16135", "authors": ["Jeffrey Seely", "Yuki Imajuku", "Tianyu Zhao", "Edoardo Cetin", "Llion Jones"], "title": "Sudoku-Bench: Evaluating creative reasoning with Sudoku variants", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Existing reasoning benchmarks for large language models (LLMs) frequently\nfail to capture authentic creativity, often rewarding memorization of\npreviously observed patterns. We address this shortcoming with Sudoku-Bench, a\ncurated benchmark of challenging and unconventional Sudoku variants\nspecifically selected to evaluate creative, multi-step logical reasoning.\nSudoku variants form an unusually effective domain for reasoning research: each\npuzzle introduces unique or subtly interacting constraints, making memorization\ninfeasible and requiring solvers to identify novel logical breakthroughs\n(``break-ins''). Despite their diversity, Sudoku variants maintain a common and\ncompact structure, enabling clear and consistent evaluation. Sudoku-Bench\nincludes a carefully chosen puzzle set, a standardized text-based puzzle\nrepresentation, and flexible tools compatible with thousands of publicly\navailable puzzles -- making it easy to extend into a general research\nenvironment. Baseline experiments show that state-of-the-art LLMs solve fewer\nthan 15\\% of puzzles unaided, highlighting significant opportunities to advance\nlong-horizon, strategic reasoning capabilities."}
{"id": "2505.16452", "pdf": "https://arxiv.org/pdf/2505.16452", "abs": "https://arxiv.org/abs/2505.16452", "authors": ["Mohamed S. Elmahdy", "Marius Staring", "Patrick J. H. de Koning", "Samer Alabed", "Mahan Salehi", "Faisal Alandejani", "Michael Sharkey", "Ziad Aldabbagh", "Andrew J. Swift", "Rob J. van der Geest"], "title": "CMRINet: Joint Groupwise Registration and Segmentation for Cardiac Function Quantification from Cine-MRI", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "15 pages, 7 figures, 1 appendix", "summary": "Accurate and efficient quantification of cardiac function is essential for\nthe estimation of prognosis of cardiovascular diseases (CVDs). One of the most\ncommonly used metrics for evaluating cardiac pumping performance is left\nventricular ejection fraction (LVEF). However, LVEF can be affected by factors\nsuch as inter-observer variability and varying pre-load and after-load\nconditions, which can reduce its reproducibility. Additionally, cardiac\ndysfunction may not always manifest as alterations in LVEF, such as in heart\nfailure and cardiotoxicity diseases. An alternative measure that can provide a\nrelatively load-independent quantitative assessment of myocardial contractility\nis myocardial strain and strain rate. By using LVEF in combination with\nmyocardial strain, it is possible to obtain a thorough description of cardiac\nfunction. Automated estimation of LVEF and other volumetric measures from\ncine-MRI sequences can be achieved through segmentation models, while strain\ncalculation requires the estimation of tissue displacement between sequential\nframes, which can be accomplished using registration models. These tasks are\noften performed separately, potentially limiting the assessment of cardiac\nfunction. To address this issue, in this study we propose an end-to-end deep\nlearning (DL) model that jointly estimates groupwise (GW) registration and\nsegmentation for cardiac cine-MRI images. The proposed anatomically-guided Deep\nGW network was trained and validated on a large dataset of 4-chamber view\ncine-MRI image series of 374 subjects. A quantitative comparison with\nconventional GW registration using elastix and two DL-based methods showed that\nthe proposed model improved performance and substantially reduced computation\ntime."}
{"id": "2505.16322", "pdf": "https://arxiv.org/pdf/2505.16322", "abs": "https://arxiv.org/abs/2505.16322", "authors": ["Woosung Koh", "Wonbeen Oh", "Jaein Jang", "MinHyung Lee", "Hyeongjin Kim", "Ah Yeon Kim", "Joonkee Kim", "Junghyun Lee", "Taehyeon Kim", "Se-Young Yun"], "title": "AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Pre-print", "summary": "Self-Taught Reasoners (STaR), synonymously known as Rejection sampling\nFine-Tuning (RFT), is an integral part of the training pipeline of\nself-improving reasoning Language Models (LMs). The self-improving mechanism\noften employs random observation (data) sampling. However, this results in\ntrained observation imbalance; inefficiently over-training on solved examples\nwhile under-training on challenging ones. In response, we introduce Adaptive\nSTaR (AdaSTaR), a novel algorithm that rectifies this by integrating two\nadaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting\nbalanced training across observations, and (2) Adaptive Sampling for\nCurriculum: dynamically adjusting data difficulty to match the model's evolving\nstrength. Across six benchmarks, AdaSTaR achieves best test accuracy in all\ninstances (6/6) and reduces training FLOPs by an average of 58.6% against an\nextensive list of baselines. These improvements in performance and efficiency\ngeneralize to different pre-trained LMs and larger models, paving the way for\nmore efficient and effective self-improving LMs."}
{"id": "2505.16136", "pdf": "https://arxiv.org/pdf/2505.16136", "abs": "https://arxiv.org/abs/2505.16136", "authors": ["Yuke Zhang"], "title": "Interpretable Machine Learning for Macro Alpha: A News Sentiment Case Study", "categories": ["q-fin.CP", "cs.AI", "cs.LG", "q-fin.TR"], "comment": "18 pages (including references), 1 figure, 1 table. Code available at\n  \\url{https://github.com/yukepenn/macro-news-sentiment-trading}. Keywords:\n  Macro Sentiment, News Sentiment, Algorithmic Trading, GDELT, FinBERT, NLP,\n  Alternative Data, Foreign Exchange, Treasury Futures, Quantitative Finance,\n  Machine Learning, SHAP, Interpretability", "summary": "This study introduces an interpretable machine learning (ML) framework to\nextract macroeconomic alpha from global news sentiment. We process the Global\nDatabase of Events, Language, and Tone (GDELT) Project's worldwide news feed\nusing FinBERT -- a Bidirectional Encoder Representations from Transformers\n(BERT) based model pretrained on finance-specific language -- to construct\ndaily sentiment indices incorporating mean tone, dispersion, and event impact.\nThese indices drive an XGBoost classifier, benchmarked against logistic\nregression, to predict next-day returns for EUR/USD, USD/JPY, and 10-year U.S.\nTreasury futures (ZN). Rigorous out-of-sample (OOS) backtesting (5-fold\nexpanding-window cross-validation, OOS period: c. 2017-April 2025) demonstrates\nexceptional, cost-adjusted performance for the XGBoost strategy: Sharpe ratios\nachieve 5.87 (EUR/USD), 4.65 (USD/JPY), and 4.65 (Treasuries), with respective\ncompound annual growth rates (CAGRs) exceeding 50% in Foreign Exchange (FX) and\n22% in bonds. Shapley Additive Explanations (SHAP) affirm that sentiment\ndispersion and article impact are key predictive features. Our findings\nestablish that integrating domain-specific Natural Language Processing (NLP)\nwith interpretable ML offers a potent and explainable source of macro alpha."}
{"id": "2505.16460", "pdf": "https://arxiv.org/pdf/2505.16460", "abs": "https://arxiv.org/abs/2505.16460", "authors": ["Ikhlasul Akmal Hanif", "Eryawan Presma Yulianrifat", "Jaycent Gunawan Ongris", "Eduardus Tjitrahardja", "Muhammad Falensi Azmi", "Rahmat Bryan Naufal", "Alfan Farizki Wicaksono"], "title": "University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "16 pages, 13 tables, 1 figures", "summary": "This paper presents our approach for SemEval 2025 Task 11 Track A, focusing\non multilabel emotion classification across 28 languages. We explore two main\nstrategies: fully fine-tuning transformer models and classifier-only training,\nevaluating different settings such as fine-tuning strategies, model\narchitectures, loss functions, encoders, and classifiers. Our findings suggest\nthat training a classifier on top of prompt-based encoders such as mE5 and BGE\nyields significantly better results than fully fine-tuning XLMR and mBERT. Our\nbest-performing model on the final leaderboard is an ensemble combining\nmultiple BGE models, where CatBoost serves as the classifier, with different\nconfigurations. This ensemble achieves an average F1-macro score of 56.58\nacross all languages."}
{"id": "2505.16400", "pdf": "https://arxiv.org/pdf/2505.16400", "abs": "https://arxiv.org/abs/2505.16400", "authors": ["Yang Chen", "Zhuolin Yang", "Zihan Liu", "Chankyu Lee", "Peng Xu", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "We release the model at:\n  https://huggingface.co/nvidia/AceReason-Nemotron-14B", "summary": "Despite recent progress in large-scale reinforcement learning (RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such as\nDeepSeek-R1, including data curation strategies and RL training recipe, are\noften omitted. Moreover, recent research indicates distillation remains more\neffective than RL for smaller models. In this work, we demonstrate that\nlarge-scale RL can significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-art distillation-based models. We systematically study the RL\ntraining process through extensive ablations and propose a simple yet effective\napproach: first training on math-only prompts, then on code-only prompts.\nNotably, we find that math-only RL not only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,\nextended code-only RL iterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-based RL across both domains.\nFinally, we identify key experimental insights, including curriculum learning\nwith progressively increasing response lengths and the stabilizing effect of\non-policy parameter updates. We find that RL not only elicits the foundational\nreasoning capabilities acquired during pretraining and supervised fine-tuning\n(e.g., distillation), but also pushes the limits of the model's reasoning\nability, enabling it to solve problems that were previously unsolvable."}
{"id": "2505.16145", "pdf": "https://arxiv.org/pdf/2505.16145", "abs": "https://arxiv.org/abs/2505.16145", "authors": ["Arghya Datta", "Philippe Gagnon", "Florian Maire"], "title": "Exponential Convergence of CAVI for Bayesian PCA", "categories": ["stat.ML", "cs.LG"], "comment": "28 pages, 3 figures", "summary": "Probabilistic principal component analysis (PCA) and its Bayesian variant\n(BPCA) are widely used for dimension reduction in machine learning and\nstatistics. The main advantage of probabilistic PCA over the traditional\nformulation is allowing uncertainty quantification. The parameters of BPCA are\ntypically learned using mean-field variational inference, and in particular,\nthe coordinate ascent variational inference (CAVI) algorithm. So far, the\nconvergence speed of CAVI for BPCA has not been characterized. In our paper, we\nfill this gap in the literature. Firstly, we prove a precise exponential\nconvergence result in the case where the model uses a single principal\ncomponent (PC). Interestingly, this result is established through a connection\nwith the classical $\\textit{power iteration algorithm}$ and it indicates that\ntraditional PCA is retrieved as points estimates of the BPCA parameters.\nSecondly, we leverage recent tools to prove exponential convergence of CAVI for\nthe model with any number of PCs, thus leading to a more general result, but\none that is of a slightly different flavor. To prove the latter result, we\nadditionally needed to introduce a novel lower bound for the symmetric\nKullback--Leibler divergence between two multivariate normal distributions,\nwhich, we believe, is of independent interest in information theory."}
{"id": "2505.16466", "pdf": "https://arxiv.org/pdf/2505.16466", "abs": "https://arxiv.org/abs/2505.16466", "authors": ["Meng Yan", "Cai Xu", "Xujing Wang", "Ziyu Guan", "Wei Zhao", "Yuhang Zhou"], "title": "Conf-GNNRec: Quantifying and Calibrating the Prediction Confidence for GNN-based Recommendation Methods", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Recommender systems based on graph neural networks perform well in tasks such\nas rating and ranking. However, in real-world recommendation scenarios, noise\nsuch as user misuse and malicious advertisement gradually accumulates through\nthe message propagation mechanism. Even if existing studies mitigate their\neffects by reducing the noise propagation weights, the severe sparsity of the\nrecommender system still leads to the low-weighted noisy neighbors being\nmistaken as meaningful information, and the prediction result obtained based on\nthe polluted nodes is not entirely trustworthy. Therefore, it is crucial to\nmeasure the confidence of the prediction results in this highly noisy\nframework. Furthermore, our evaluation of the existing representative GNN-based\nrecommendation shows that it suffers from overconfidence. Based on the above\nconsiderations, we propose a new method to quantify and calibrate the\nprediction confidence of GNN-based recommendations (Conf-GNNRec). Specifically,\nwe propose a rating calibration method that dynamically adjusts excessive\nratings to mitigate overconfidence based on user personalization. We also\ndesign a confidence loss function to reduce the overconfidence of negative\nsamples and effectively improve recommendation performance. Experiments on\npublic datasets demonstrate the validity of Conf-GNNRec in prediction\nconfidence and recommendation performance."}
{"id": "2505.16470", "pdf": "https://arxiv.org/pdf/2505.16470", "abs": "https://arxiv.org/abs/2505.16470", "authors": ["Kuicai Dong", "Yujing Chang", "Shijie Huang", "Yasheng Wang", "Ruiming Tang", "Yong Liu"], "title": "Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering", "categories": ["cs.IR", "cs.CL", "cs.CV"], "comment": "preprint. code available at\n  \\url{https://mmdocrag.github.io/MMDocRAG/}", "summary": "Document Visual Question Answering (DocVQA) faces dual challenges in\nprocessing lengthy multimodal documents (text, images, tables) and performing\ncross-modal reasoning. Current document retrieval-augmented generation (DocRAG)\nmethods remain limited by their text-centric approaches, frequently missing\ncritical visual information. The field also lacks robust benchmarks for\nassessing multimodal evidence selection and integration. We introduce MMDocRAG,\na comprehensive benchmark featuring 4,055 expert-annotated QA pairs with\nmulti-page, cross-modal evidence chains. Our framework introduces innovative\nmetrics for evaluating multimodal quote selection and enables answers that\ninterleave text with relevant visual elements. Through large-scale experiments\nwith 60 VLM/LLM models and 14 retrieval systems, we identify persistent\nchallenges in multimodal evidence retrieval, selection, and integration.Key\nfindings reveal advanced proprietary LVMs show superior performance than\nopen-sourced alternatives. Also, they show moderate advantages using multimodal\ninputs over text-only inputs, while open-source alternatives show significant\nperformance degradation. Notably, fine-tuned LLMs achieve substantial\nimprovements when using detailed image descriptions. MMDocRAG establishes a\nrigorous testing ground and provides actionable insights for developing more\nrobust multimodal DocVQA systems. Our benchmark and code are available at\nhttps://mmdocrag.github.io/MMDocRAG/."}
{"id": "2505.16146", "pdf": "https://arxiv.org/pdf/2505.16146", "abs": "https://arxiv.org/abs/2505.16146", "authors": ["Zhenglin Hua", "Jinghan He", "Zijun Yao", "Tianxu Han", "Haiyun Guo", "Yuheng Jia", "Junfeng Fang"], "title": "Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large vision-language models (LVLMs) have achieved remarkable performance on\nmultimodal tasks such as visual question answering (VQA) and image captioning.\nHowever, they still suffer from hallucinations, generating text inconsistent\nwith visual input, posing significant risks in real-world applications.\nExisting approaches to address this issue focus on incorporating external\nknowledge bases, alignment training, or decoding strategies, all of which\nrequire substantial computational cost and time. Recent works try to explore\nmore efficient alternatives by adjusting LVLMs' internal representations.\nAlthough promising, these methods may cause hallucinations to be insufficiently\nsuppressed or lead to excessive interventions that negatively affect normal\nsemantics. In this work, we leverage sparse autoencoders (SAEs) to identify\nsemantic directions closely associated with either hallucinations or actuality,\nrealizing more precise and direct hallucination-related representations. Our\nanalysis demonstrates that interventions along the faithful direction we\nidentified can mitigate hallucinations, while those along the hallucinatory\ndirection can exacerbate them. Building on these insights, we propose Steering\nLVLMs via SAE Latent Directions (SSL), a training-free method based on\nSAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive\nexperiments demonstrate that SSL significantly outperforms existing decoding\napproaches in mitigating hallucinations, while maintaining transferability\nacross different model architectures with negligible additional time overhead."}
{"id": "2505.16483", "pdf": "https://arxiv.org/pdf/2505.16483", "abs": "https://arxiv.org/abs/2505.16483", "authors": ["Shuzheng Si", "Haozhe Zhao", "Cheng Gao", "Yuzhuo Bai", "Zhitong Wang", "Bofei Gao", "Kangyang Luo", "Wenhao Li", "Yufei Huang", "Gang Chen", "Fanchao Qi", "Minjia Zhang", "Baobao Chang", "Maosong Sun"], "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1."}
{"id": "2505.16530", "pdf": "https://arxiv.org/pdf/2505.16530", "abs": "https://arxiv.org/abs/2505.16530", "authors": ["Yuliang Yan", "Haochun Tang", "Shuo Yan", "Enyan Dai"], "title": "DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) are considered valuable Intellectual Properties\n(IP) for legitimate owners due to the enormous computational cost of training.\nIt is crucial to protect the IP of LLMs from malicious stealing or unauthorized\ndeployment. Despite existing efforts in watermarking and fingerprinting LLMs,\nthese methods either impact the text generation process or are limited in\nwhite-box access to the suspect model, making them impractical. Hence, we\npropose DuFFin, a novel $\\textbf{Du}$al-Level $\\textbf{Fin}$gerprinting\n$\\textbf{F}$ramework for black-box setting ownership verification. DuFFin\nextracts the trigger pattern and the knowledge-level fingerprints to identify\nthe source of a suspect model. We conduct experiments on a variety of models\ncollected from the open-source website, including four popular base models as\nprotected LLMs and their fine-tuning, quantization, and safety alignment\nversions, which are released by large companies, start-ups, and individual\nusers. Results show that our method can accurately verify the copyright of the\nbase protected LLM on their model variants, achieving the IP-ROC metric greater\nthan 0.95. Our code is available at\nhttps://github.com/yuliangyan0807/llm-fingerprint."}
{"id": "2505.16156", "pdf": "https://arxiv.org/pdf/2505.16156", "abs": "https://arxiv.org/abs/2505.16156", "authors": ["Siu Lun Chau", "Michele Caprio", "Krikamol Muandet"], "title": "Integral Imprecise Probability Metrics", "categories": ["stat.ML", "cs.LG"], "comment": "50 pages, 2 figures", "summary": "Quantifying differences between probability distributions is fundamental to\nstatistics and machine learning, primarily for comparing statistical\nuncertainty. In contrast, epistemic uncertainty (EU) -- due to incomplete\nknowledge -- requires richer representations than those offered by classical\nprobability. Imprecise probability (IP) theory offers such models, capturing\nambiguity and partial belief. This has driven growing interest in imprecise\nprobabilistic machine learning (IPML), where inference and decision-making rely\non broader uncertainty models -- highlighting the need for metrics beyond\nclassical probability. This work introduces the Integral Imprecise Probability\nMetric (IIPM) framework, a Choquet integral-based generalisation of classical\nIntegral Probability Metric (IPM) to the setting of capacities -- a broad class\nof IP models encompassing many existing ones, including lower probabilities,\nprobability intervals, belief functions, and more. Theoretically, we establish\nconditions under which IIPM serves as a valid metric and metrises a form of\nweak convergence of capacities. Practically, IIPM not only enables comparison\nacross different IP models but also supports the quantification of epistemic\nuncertainty within a single IP model. In particular, by comparing an IP model\nwith its conjugate, IIPM gives rise to a new class of EU measures -- Maximum\nMean Imprecision -- which satisfy key axiomatic properties proposed in the\nUncertainty Quantification literature. We validate MMI through selective\nclassification experiments, demonstrating strong empirical performance against\nestablished EU measures, and outperforming them when classical methods struggle\nto scale to a large number of classes. Our work advances both theory and\npractice in IPML, offering a principled framework for comparing and quantifying\nepistemic uncertainty under imprecision."}
{"id": "2505.16491", "pdf": "https://arxiv.org/pdf/2505.16491", "abs": "https://arxiv.org/abs/2505.16491", "authors": ["Dario Di Palma", "Alessandro De Bellis", "Giovanni Servedio", "Vito Walter Anelli", "Fedelucio Narducci", "Tommaso Di Noia"], "title": "LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have rapidly become central to NLP,\ndemonstrating their ability to adapt to various tasks through prompting\ntechniques, including sentiment analysis. However, we still have a limited\nunderstanding of how these models capture sentiment-related information. This\nstudy probes the hidden layers of Llama models to pinpoint where sentiment\nfeatures are most represented and to assess how this affects sentiment\nanalysis.\n  Using probe classifiers, we analyze sentiment encoding across layers and\nscales, identifying the layers and pooling methods that best capture sentiment\nsignals. Our results show that sentiment information is most concentrated in\nmid-layers for binary polarity tasks, with detection accuracy increasing up to\n14% over prompting techniques. Additionally, we find that in decoder-only\nmodels, the last token is not consistently the most informative for sentiment\nencoding. Finally, this approach enables sentiment tasks to be performed with\nmemory requirements reduced by an average of 57%.\n  These insights contribute to a broader understanding of sentiment in LLMs,\nsuggesting layer-specific probing as an effective approach for sentiment tasks\nbeyond prompting, with potential to enhance model utility and reduce memory\nrequirements."}
{"id": "2505.16559", "pdf": "https://arxiv.org/pdf/2505.16559", "abs": "https://arxiv.org/abs/2505.16559", "authors": ["Biao Yi", "Tiansheng Huang", "Baolei Zhang", "Tong Li", "Lihai Nie", "Zheli Liu", "Li Shen"], "title": "CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Fine-tuning-as-a-service, while commercially successful for Large Language\nModel (LLM) providers, exposes models to harmful fine-tuning attacks. As a\nwidely explored defense paradigm against such attacks, unlearning attempts to\nremove malicious knowledge from LLMs, thereby essentially preventing them from\nbeing used to perform malicious tasks. However, we highlight a critical flaw:\nthe powerful general adaptability of LLMs allows them to easily bypass\nselective unlearning by rapidly relearning or repurposing their capabilities\nfor harmful tasks. To address this fundamental limitation, we propose a\nparadigm shift: instead of selective removal, we advocate for inducing model\ncollapse--effectively forcing the model to \"unlearn everything\"--specifically\nin response to updates characteristic of malicious adaptation. This collapse\ndirectly neutralizes the very general capabilities that attackers exploit,\ntackling the core issue unaddressed by selective unlearning. We introduce the\nCollapse Trap (CTRAP) as a practical mechanism to implement this concept\nconditionally. Embedded during alignment, CTRAP pre-configures the model's\nreaction to subsequent fine-tuning dynamics. If updates during fine-tuning\nconstitute a persistent attempt to reverse safety alignment, the pre-configured\ntrap triggers a progressive degradation of the model's core language modeling\nabilities, ultimately rendering it inert and useless for the attacker.\nCrucially, this collapse mechanism remains dormant during benign fine-tuning,\nensuring the model's utility and general capabilities are preserved for\nlegitimate users. Extensive empirical results demonstrate that CTRAP\neffectively counters harmful fine-tuning risks across various LLMs and attack\nsettings, while maintaining high performance in benign scenarios. Our code is\navailable at https://anonymous.4open.science/r/CTRAP."}
{"id": "2505.16195", "pdf": "https://arxiv.org/pdf/2505.16195", "abs": "https://arxiv.org/abs/2505.16195", "authors": ["Zhi Zhong", "Akira Takahashi", "Shuyang Cui", "Keisuke Toyama", "Shusuke Takahashi", "Yuki Mitsufuji"], "title": "SpecMaskFoley: Steering Pretrained Spectral Masked Generative Transformer Toward Synchronized Video-to-audio Synthesis via ControlNet", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.IV"], "comment": "4 pages, 2 figures, 2 tables. Demo page:\n  https://zzaudio.github.io/SpecMaskFoley_Demo/", "summary": "Foley synthesis aims to synthesize high-quality audio that is both\nsemantically and temporally aligned with video frames. Given its broad\napplication in creative industries, the task has gained increasing attention in\nthe research community. To avoid the non-trivial task of training audio\ngenerative models from scratch, adapting pretrained audio generative models for\nvideo-synchronized foley synthesis presents an attractive direction.\nControlNet, a method for adding fine-grained controls to pretrained generative\nmodels, has been applied to foley synthesis, but its use has been limited to\nhandcrafted human-readable temporal conditions. In contrast, from-scratch\nmodels achieved success by leveraging high-dimensional deep features extracted\nusing pretrained video encoders. We have observed a performance gap between\nControlNet-based and from-scratch foley models. To narrow this gap, we propose\nSpecMaskFoley, a method that steers the pretrained SpecMaskGIT model toward\nvideo-synchronized foley synthesis via ControlNet. To unlock the potential of a\nsingle ControlNet branch, we resolve the discrepancy between the temporal video\nfeatures and the time-frequency nature of the pretrained SpecMaskGIT via a\nfrequency-aware temporal feature aligner, eliminating the need for complicated\nconditioning mechanisms widely used in prior arts. Evaluations on a common\nfoley synthesis benchmark demonstrate that SpecMaskFoley could even outperform\nstrong from-scratch baselines, substantially advancing the development of\nControlNet-based foley synthesis models. Demo page:\nhttps://zzaudio.github.io/SpecMaskFoley_Demo/"}
{"id": "2505.16498", "pdf": "https://arxiv.org/pdf/2505.16498", "abs": "https://arxiv.org/abs/2505.16498", "authors": ["Augusto Luis Ballardini", "Miguel Ángel Sotelo"], "title": "Human-like Semantic Navigation for Autonomous Driving using Knowledge Representation and Large Language Models", "categories": ["cs.RO", "cs.AI"], "comment": "7 pages, 5 figures, submitted for IEEE conference", "summary": "Achieving full automation in self-driving vehicles remains a challenge,\nespecially in dynamic urban environments where navigation requires real-time\nadaptability. Existing systems struggle to handle navigation plans when faced\nwith unpredictable changes in road layouts, spontaneous detours, or missing map\ndata, due to their heavy reliance on predefined cartographic information. In\nthis work, we explore the use of Large Language Models to generate Answer Set\nProgramming rules by translating informal navigation instructions into\nstructured, logic-based reasoning. ASP provides non-monotonic reasoning,\nallowing autonomous vehicles to adapt to evolving scenarios without relying on\npredefined maps. We present an experimental evaluation in which LLMs generate\nASP constraints that encode real-world urban driving logic into a formal\nknowledge representation. By automating the translation of informal navigation\ninstructions into logical rules, our method improves adaptability and\nexplainability in autonomous navigation. Results show that LLM-driven ASP rule\ngeneration supports semantic-based decision-making, offering an explainable\nframework for dynamic navigation planning that aligns closely with how humans\ncommunicate navigational intent."}
{"id": "2505.16624", "pdf": "https://arxiv.org/pdf/2505.16624", "abs": "https://arxiv.org/abs/2505.16624", "authors": ["Francesco Dalla Serra", "Patrick Schrempf", "Chaoyang Wang", "Zaiqiao Meng", "Fani Deligianni", "Alison Q. O'Neil"], "title": "Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We present a novel approach to Chest X-ray (CXR) Visual Question Answering\n(VQA), addressing both single-image image-difference questions. Single-image\nquestions focus on abnormalities within a specific CXR (\"What abnormalities are\nseen in image X?\"), while image-difference questions compare two longitudinal\nCXRs acquired at different time points (\"What are the differences between image\nX and Y?\"). We further explore how the integration of radiology reports can\nenhance the performance of VQA models. While previous approaches have\ndemonstrated the utility of radiology reports during the pre-training phase, we\nextend this idea by showing that the reports can also be leveraged as\nadditional input to improve the VQA model's predicted answers. First, we\npropose a unified method that handles both types of questions and\nauto-regressively generates the answers. For single-image questions, the model\nis provided with a single CXR. For image-difference questions, the model is\nprovided with two CXRs from the same patient, captured at different time\npoints, enabling the model to detect and describe temporal changes. Taking\ninspiration from 'Chain-of-Thought reasoning', we demonstrate that performance\non the CXR VQA task can be improved by grounding the answer generator module\nwith a radiology report predicted for the same CXR. In our approach, the VQA\nmodel is divided into two steps: i) Report Generation (RG) and ii) Answer\nGeneration (AG). Our results demonstrate that incorporating predicted radiology\nreports as evidence to the AG model enhances performance on both single-image\nand image-difference questions, achieving state-of-the-art results on the\nMedical-Diff-VQA dataset."}
{"id": "2505.16208", "pdf": "https://arxiv.org/pdf/2505.16208", "abs": "https://arxiv.org/abs/2505.16208", "authors": ["Anton Erofeev", "Balasubramanya T. Nadiga", "Ilya Timofeyev"], "title": "Using Echo-State Networks to Reproduce Rare Events in Chaotic Systems", "categories": ["nlin.CD", "cs.AI", "cs.LG", "math.DS", "37N99, 68T30"], "comment": null, "summary": "We apply the Echo-State Networks to predict the time series and statistical\nproperties of the competitive Lotka-Volterra model in the chaotic regime. In\nparticular, we demonstrate that Echo-State Networks successfully learn the\nchaotic attractor of the competitive Lotka-Volterra model and reproduce\nhistograms of dependent variables, including tails and rare events. We use the\nGeneralized Extreme Value distribution to quantify the tail behavior."}
{"id": "2505.16499", "pdf": "https://arxiv.org/pdf/2505.16499", "abs": "https://arxiv.org/abs/2505.16499", "authors": ["Roberto Morabito", "SiYoung Jang"], "title": "Smaller, Smarter, Closer: The Edge of Collaborative Generative AI", "categories": ["cs.DC", "cs.AI", "cs.NI"], "comment": "This paper is currently under review for publication in an IEEE\n  magazine. If accepted, the copyright will be transferred to IEEE", "summary": "The rapid adoption of generative AI (GenAI), particularly Large Language\nModels (LLMs), has exposed critical limitations of cloud-centric deployments,\nincluding latency, cost, and privacy concerns. Meanwhile, Small Language Models\n(SLMs) are emerging as viable alternatives for resource-constrained edge\nenvironments, though they often lack the capabilities of their larger\ncounterparts. This article explores the potential of collaborative inference\nsystems that leverage both edge and cloud resources to address these\nchallenges. By presenting distinct cooperation strategies alongside practical\ndesign principles and experimental insights, we offer actionable guidance for\ndeploying GenAI across the computing continuum."}
{"id": "2505.16631", "pdf": "https://arxiv.org/pdf/2505.16631", "abs": "https://arxiv.org/abs/2505.16631", "authors": ["Jonghwi Kim", "Deokhyung Kang", "Seonjeong Hwang", "Yunsu Kim", "Jungseul Ok", "Gary Lee"], "title": "MiLQ: Benchmarking IR Models for Bilingual Web Search with Mixed Language Queries", "categories": ["cs.IR", "cs.CL"], "comment": "16 pages, 9 figures", "summary": "Despite bilingual speakers frequently using mixed-language queries in web\nsearches, Information Retrieval (IR) research on them remains scarce. To\naddress this, we introduce MiLQ,Mixed-Language Query test set, the first public\nbenchmark of mixed-language queries, confirmed as realistic and highly\npreferred. Experiments show that multilingual IR models perform moderately on\nMiLQ and inconsistently across native, English, and mixed-language queries,\nalso suggesting code-switched training data's potential for robust IR models\nhandling such queries. Meanwhile, intentional English mixing in queries proves\nan effective strategy for bilinguals searching English documents, which our\nanalysis attributes to enhanced token matching compared to native queries."}
{"id": "2505.16215", "pdf": "https://arxiv.org/pdf/2505.16215", "abs": "https://arxiv.org/abs/2505.16215", "authors": ["Md Ashraf Uddin", "Nam H. Chu", "Reza Rafeh", "Mutaz Barika"], "title": "A Scalable Hierarchical Intrusion Detection System for Internet of Vehicles", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Due to its nature of dynamic, mobility, and wireless data transfer, the\nInternet of Vehicles (IoV) is prone to various cyber threats, ranging from\nspoofing and Distributed Denial of Services (DDoS) attacks to malware. To\nsafeguard the IoV ecosystem from intrusions, malicious activities, policy\nviolations, intrusion detection systems (IDS) play a critical role by\ncontinuously monitoring and analyzing network traffic to identify and mitigate\npotential threats in real-time. However, most existing research has focused on\ndeveloping centralized, machine learning-based IDS systems for IoV without\naccounting for its inherently distributed nature. Due to intensive computing\nrequirements, these centralized systems often rely on the cloud to detect cyber\nthreats, increasing delay of system response. On the other hand, edge nodes\ntypically lack the necessary resources to train and deploy complex machine\nlearning algorithms. To address this issue, this paper proposes an effective\nhierarchical classification framework tailored for IoV networks. Hierarchical\nclassification allows classifiers to be trained and tested at different levels,\nenabling edge nodes to detect specific types of attacks independently. With\nthis approach, edge nodes can conduct targeted attack detection while\nleveraging cloud nodes for comprehensive threat analysis and support. Given the\nresource constraints of edge nodes, we have employed the Boruta feature\nselection method to reduce data dimensionality, optimizing processing\nefficiency. To evaluate our proposed framework, we utilize the latest IoV\nsecurity dataset CIC-IoV2024, achieving promising results that demonstrate the\nfeasibility and effectiveness of our models in securing IoV networks."}
{"id": "2505.16505", "pdf": "https://arxiv.org/pdf/2505.16505", "abs": "https://arxiv.org/abs/2505.16505", "authors": ["Runcong Zhao", "Chengyu Cao", "Qinglin Zhu", "Xiucheng Lv", "Shun Shao", "Lin Gui", "Ruifeng Xu", "Yulan He"], "title": "Sparse Activation Editing for Reliable Instruction Following in Narratives", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Complex narrative contexts often challenge language models' ability to follow\ninstructions, and existing benchmarks fail to capture these difficulties. To\naddress this, we propose Concise-SAE, a training-free framework that improves\ninstruction following by identifying and editing instruction-relevant neurons\nusing only natural language instructions, without requiring labelled data. To\nthoroughly evaluate our method, we introduce FreeInstruct, a diverse and\nrealistic benchmark of 1,212 examples that highlights the challenges of\ninstruction following in narrative-rich settings. While initially motivated by\ncomplex narratives, Concise-SAE demonstrates state-of-the-art instruction\nadherence across varied tasks without compromising generation quality."}
{"id": "2505.16673", "pdf": "https://arxiv.org/pdf/2505.16673", "abs": "https://arxiv.org/abs/2505.16673", "authors": ["Huanjin Yao", "Qixiang Yin", "Jingyi Zhang", "Min Yang", "Yibo Wang", "Wenhao Wu", "Fei Su", "Li Shen", "Minghui Qiu", "Dacheng Tao", "Jiaxing Huang"], "title": "R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Technical report", "summary": "In this work, we aim to incentivize the reasoning ability of Multimodal Large\nLanguage Models (MLLMs) via reinforcement learning (RL) and develop an\neffective approach that mitigates the sparse reward and advantage vanishing\nissues during RL. To this end, we propose Share-GRPO, a novel RL approach that\ntackle these issues by exploring and sharing diverse reasoning trajectories\nover expanded question space. Specifically, Share-GRPO first expands the\nquestion space for a given question via data transformation techniques, and\nthen encourages MLLM to effectively explore diverse reasoning trajectories over\nthe expanded question space and shares the discovered reasoning trajectories\nacross the expanded questions during RL. In addition, Share-GRPO also shares\nreward information during advantage computation, which estimates solution\nadvantages hierarchically across and within question variants, allowing more\naccurate estimation of relative advantages and improving the stability of\npolicy training. Extensive evaluations over six widely-used reasoning\nbenchmarks showcase the superior performance of our method. Code will be\navailable at https://github.com/HJYao00/R1-ShareVL."}
{"id": "2505.16223", "pdf": "https://arxiv.org/pdf/2505.16223", "abs": "https://arxiv.org/abs/2505.16223", "authors": ["Sangyong Lee", "Subo Hwang", "Dohoon Kim"], "title": "MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network", "categories": ["cs.AI", "cs.LG"], "comment": "24 pages, 9 figures", "summary": "In this paper, we propose MADCluster, a novel model-agnostic anomaly\ndetection framework utilizing self-supervised clustering. MADCluster is\napplicable to various deep learning architectures and addresses the\n'hypersphere collapse' problem inherent in existing deep learning-based anomaly\ndetection methods. The core idea is to cluster normal pattern data into a\n'single cluster' while simultaneously learning the cluster center and mapping\ndata close to this center. Also, to improve expressiveness and enable effective\nsingle clustering, we propose a new 'One-directed Adaptive loss'. The\noptimization of this loss is mathematically proven. MADCluster consists of\nthree main components: Base Embedder capturing high-dimensional temporal\ndynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous\ncenter updates. Its model-agnostic characteristics are achieved by applying\nvarious architectures to the Base Embedder. Experiments on four time series\nbenchmark datasets demonstrate that applying MADCluster improves the overall\nperformance of comparative models. In conclusion, the compatibility of\nMADCluster shows potential for enhancing model performance across various\narchitectures."}
{"id": "2505.16508", "pdf": "https://arxiv.org/pdf/2505.16508", "abs": "https://arxiv.org/abs/2505.16508", "authors": ["SiYoung Jang", "Roberto Morabito"], "title": "Edge-First Language Model Inference: Models, Metrics, and Tradeoffs", "categories": ["cs.DC", "cs.AI", "cs.NI", "cs.PF"], "comment": "This paper has been accepted for publication and presentation at the\n  45th IEEE International Conference on Distributed Computing Systems (IEEE\n  ICDCS 2025). The copyright will be transferred to IEEE upon publication in\n  the conference proceedings", "summary": "The widespread adoption of Language Models (LMs) across industries is driving\ninterest in deploying these services across the computing continuum, from the\ncloud to the network edge. This shift aims to reduce costs, lower latency, and\nimprove reliability and privacy. Small Language Models (SLMs), enabled by\nadvances in model compression, are central to this shift, offering a path to\non-device inference on resource-constrained edge platforms. This work examines\nthe interplay between edge and cloud deployments, starting from detailed\nbenchmarking of SLM capabilities on single edge devices, and extending to\ndistributed edge clusters. We identify scenarios where edge inference offers\ncomparable performance with lower costs, and others where cloud fallback\nbecomes essential due to limits in scalability or model capacity. Rather than\nproposing a one-size-fits-all solution, we present platform-level comparisons\nand design insights for building efficient, adaptive LM inference systems\nacross heterogeneous environments."}
{"id": "2505.16686", "pdf": "https://arxiv.org/pdf/2505.16686", "abs": "https://arxiv.org/abs/2505.16686", "authors": ["Lars Benedikt Kaesberg", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "title": "SPaRC: A Spatial Pathfinding Reasoning Challenge", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Existing reasoning datasets saturate and fail to test abstract, multi-step\nproblems, especially pathfinding and complex rule constraint satisfaction. We\nintroduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000\n2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,\nrequiring step-by-step planning with arithmetic and geometric rules. Humans\nachieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best\nreasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).\nModels often generate invalid paths (>50% of puzzles for o4-mini), and\nreasoning tokens reveal they make errors in navigation and spatial logic.\nUnlike humans, who take longer on hard puzzles, models fail to scale test-time\ncompute with difficulty. Allowing models to make multiple solution attempts\nimproves accuracy, suggesting potential for better spatial reasoning with\nimproved training and efficient test-time scaling methods. SPaRC can be used as\na window into models' spatial reasoning limitations and drive research toward\nnew methods that excel in abstract, multi-step problem-solving."}
{"id": "2505.16227", "pdf": "https://arxiv.org/pdf/2505.16227", "abs": "https://arxiv.org/abs/2505.16227", "authors": ["Bohao Wu", "Qingyun Wang", "Yue Guo"], "title": "Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Personalizing jargon detection and explanation is essential for making\ntechnical documents accessible to readers with diverse disciplinary\nbackgrounds. However, tailoring models to individual users typically requires\nsubstantial annotation efforts and computational resources due to user-specific\nfinetuning. To address this, we present a systematic study of personalized\njargon detection, focusing on methods that are both efficient and scalable for\nreal-world deployment. We explore two personalization strategies: (1)\nlightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,\nand (2) personalized prompting, which tailors model behavior at inference time\nwithout retaining. To reflect realistic constraints, we also investigate hybrid\napproaches that combine limited annotated data with unsupervised user\nbackground signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in\nF1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,\nour method achieves comparable performance using only 10% of the annotated\ntraining data, demonstrating its practicality for resource-constrained\nsettings. Our study offers the first work to systematically explore efficient,\nlow-resource personalization of jargon detection using open-source language\nmodels, offering a practical path toward scalable, user-adaptive NLP system."}
{"id": "2505.16512", "pdf": "https://arxiv.org/pdf/2505.16512", "abs": "https://arxiv.org/abs/2505.16512", "authors": ["Jiaxin Liu", "Jia Wang", "Saihui Hou", "Min Ren", "Huijia Wu", "Zhaofeng He"], "title": "Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In recent years, the rapid development of deepfake technology has given rise\nto an emerging and serious threat to public security: diffusion model-based\ndigital human generation. Unlike traditional face manipulation methods, such\nmodels can generate highly realistic videos with consistency through multimodal\ncontrol signals. Their flexibility and covertness pose severe challenges to\nexisting detection strategies. To bridge this gap, we introduce DigiFakeAV, the\nfirst large-scale multimodal digital human forgery dataset based on diffusion\nmodels. Employing five latest digital human generation methods (Sonic, Hallo,\netc.) and voice cloning method, we systematically produce a dataset comprising\n60,000 videos (8.4 million frames), covering multiple nationalities, skin\ntones, genders, and real-world scenarios, significantly enhancing data\ndiversity and realism. User studies show that the confusion rate between forged\nand real videos reaches 68%, and existing state-of-the-art (SOTA) detection\nmodels exhibit large drops in AUC values on DigiFakeAV, highlighting the\nchallenge of the dataset. To address this problem, we further propose\nDigiShield, a detection baseline based on spatiotemporal and cross-modal\nfusion. By jointly modeling the 3D spatiotemporal features of videos and the\nsemantic-acoustic features of audio, DigiShield achieves SOTA performance on\nboth the DigiFakeAV and DF-TIMIT datasets. Experiments show that this method\neffectively identifies covert artifacts through fine-grained analysis of the\ntemporal evolution of facial features in synthetic videos."}
{"id": "2505.16737", "pdf": "https://arxiv.org/pdf/2505.16737", "abs": "https://arxiv.org/abs/2505.16737", "authors": ["Chengcan Wu", "Zhixin Zhang", "Zeming Wei", "Yihao Zhang", "Meng Sun"], "title": "Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "math.OC"], "comment": null, "summary": "The significant progress of large language models (LLMs) has led to\nremarkable achievements across numerous applications. However, their ability to\ngenerate harmful content has sparked substantial safety concerns. Despite the\nimplementation of safety alignment techniques during the pre-training phase,\nrecent research indicates that fine-tuning LLMs on adversarial or even benign\ndata can inadvertently compromise their safety. In this paper, we re-examine\nthe fundamental issue of why fine-tuning on non-harmful data still results in\nsafety degradation. We introduce a safety-aware probing (SAP) optimization\nframework designed to mitigate the safety risks of fine-tuning LLMs.\nSpecifically, SAP incorporates a safety-aware probe into the gradient\npropagation process, mitigating the model's risk of safety degradation by\nidentifying potential pitfalls in gradient directions, thereby enhancing\ntask-specific performance while successfully preserving model safety. Our\nextensive experimental results demonstrate that SAP effectively reduces\nharmfulness below the original fine-tuned model and achieves comparable test\nloss to standard fine-tuning methods. Our code is available at\nhttps://github.com/ChengcanWu/SAP."}
{"id": "2505.16244", "pdf": "https://arxiv.org/pdf/2505.16244", "abs": "https://arxiv.org/abs/2505.16244", "authors": ["Masanari Kimura", "Howard Bondell"], "title": "Generalized Power Priors for Improved Bayesian Inference with Historical Data", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "The power prior is a class of informative priors designed to incorporate\nhistorical data alongside current data in a Bayesian framework. It includes a\npower parameter that controls the influence of historical data, providing\nflexibility and adaptability. A key property of the power prior is that the\nresulting posterior minimizes a linear combination of KL divergences between\ntwo pseudo-posterior distributions: one ignoring historical data and the other\nfully incorporating it. We extend this framework by identifying the posterior\ndistribution as the minimizer of a linear combination of Amari's\n$\\alpha$-divergence, a generalization of KL divergence. We show that this\ngeneralization can lead to improved performance by allowing for the data to\nadapt to appropriate choices of the $\\alpha$ parameter. Theoretical properties\nof this generalized power posterior are established, including behavior as a\ngeneralized geodesic on the Riemannian manifold of probability distributions,\noffering novel insights into its geometric interpretation."}
{"id": "2505.16516", "pdf": "https://arxiv.org/pdf/2505.16516", "abs": "https://arxiv.org/abs/2505.16516", "authors": ["Majid Mohammadi", "Siu Lun Chau", "Krikamol Muandet"], "title": "Computing Exact Shapley Values in Polynomial Time for Product-Kernel Methods", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Kernel methods are widely used in machine learning due to their flexibility\nand expressive power. However, their black-box nature poses significant\nchallenges to interpretability, limiting their adoption in high-stakes\napplications. Shapley value-based feature attribution techniques, such as SHAP\nand kernel-specific variants like RKHS-SHAP, offer a promising path toward\nexplainability. Yet, computing exact Shapley values remains computationally\nintractable in general, motivating the development of various approximation\nschemes. In this work, we introduce PKeX-Shapley, a novel algorithm that\nutilizes the multiplicative structure of product kernels to enable the exact\ncomputation of Shapley values in polynomial time. We show that product-kernel\nmodels admit a functional decomposition that allows for a recursive formulation\nof Shapley values. This decomposition not only yields computational efficiency\nbut also enhances interpretability in kernel-based learning. We also\ndemonstrate how our framework can be generalized to explain kernel-based\nstatistical discrepancies such as the Maximum Mean Discrepancy (MMD) and the\nHilbert-Schmidt Independence Criterion (HSIC), thus offering new tools for\ninterpretable statistical inference."}
{"id": "2505.16826", "pdf": "https://arxiv.org/pdf/2505.16826", "abs": "https://arxiv.org/abs/2505.16826", "authors": ["Wei Sun", "Wen Yang", "Pu Jian", "Qianlong Du", "Fuwei Cui", "Shuo Ren", "Jiajun Zhang"], "title": "KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances have demonstrated that integrating reinforcement learning\nwith rule-based rewards can significantly enhance the reasoning capabilities of\nlarge language models, even without supervised fine-tuning. However, prevalent\nreinforcement learning algorithms such as GRPO and its variants like DAPO,\nsuffer from a coarse granularity issue when computing the advantage.\nSpecifically, they compute rollout-level advantages that assign identical\nvalues to every token within a sequence, failing to capture token-specific\ncontributions and hindering effective learning. To address this limitation, we\npropose Key-token Advantage Estimation (KTAE) - a novel algorithm that\nestimates fine-grained, token-level advantages without introducing additional\nmodels. KTAE leverages the correctness of sampled rollouts and applies\nstatistical analysis to quantify the importance of individual tokens within a\nsequence to the final outcome. This quantified token-level importance is then\ncombined with the rollout-level advantage to obtain a more fine-grained\ntoken-level advantage estimation. Empirical results show that models trained\nwith GRPO+KTAE and DAPO+KTAE outperform baseline methods across five\nmathematical reasoning benchmarks. Notably, they achieve higher accuracy with\nshorter responses and even surpass R1-Distill-Qwen-1.5B using the same base\nmodel."}
{"id": "2505.16251", "pdf": "https://arxiv.org/pdf/2505.16251", "abs": "https://arxiv.org/abs/2505.16251", "authors": ["Masanari Kimura"], "title": "Graph-Smoothed Bayesian Black-Box Shift Estimator and Its Information Geometry", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Label shift adaptation aims to recover target class priors when the labelled\nsource distribution $P$ and the unlabelled target distribution $Q$ share $P(X\n\\mid Y) = Q(X \\mid Y)$ but $P(Y) \\neq Q(Y)$. Classical black-box shift\nestimators invert an empirical confusion matrix of a frozen classifier,\nproducing a brittle point estimate that ignores sampling noise and similarity\namong classes. We present Graph-Smoothed Bayesian BBSE (GS-B$^3$SE), a fully\nprobabilistic alternative that places Laplacian-Gaussian priors on both target\nlog-priors and confusion-matrix columns, tying them together on a\nlabel-similarity graph. The resulting posterior is tractable with HMC or a fast\nblock Newton-CG scheme. We prove identifiability, $N^{-1/2}$ contraction,\nvariance bounds that shrink with the graph's algebraic connectivity, and\nrobustness to Laplacian misspecification. We also reinterpret GS-B$^3$SE\nthrough information geometry, showing that it generalizes existing shift\nestimators."}
{"id": "2505.16518", "pdf": "https://arxiv.org/pdf/2505.16518", "abs": "https://arxiv.org/abs/2505.16518", "authors": ["Lovisa Hagström", "Youna Kim", "Haeun Yu", "Sang-goo Lee", "Richard Johansson", "Hyunsoo Cho", "Isabelle Augenstein"], "title": "CUB: Benchmarking Context Utilisation Techniques for Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "27 pages", "summary": "Incorporating external knowledge is crucial for knowledge-intensive tasks,\nsuch as question answering and fact checking. However, language models (LMs)\nmay ignore relevant information that contradicts outdated parametric memory or\nbe distracted by irrelevant contexts. While many context utilisation\nmanipulation techniques (CMTs) that encourage or suppress context utilisation\nhave recently been proposed to alleviate these issues, few have seen systematic\ncomparison. In this paper, we develop CUB (Context Utilisation Benchmark) to\nhelp practitioners within retrieval-augmented generation (RAG) identify the\nbest CMT for their needs. CUB allows for rigorous testing on three distinct\ncontext types, observed to capture key challenges in realistic context\nutilisation scenarios. With this benchmark, we evaluate seven state-of-the-art\nmethods, representative of the main categories of CMTs, across three diverse\ndatasets and tasks, applied to nine LMs. Our results show that most of the\nexisting CMTs struggle to handle the full set of types of contexts that may be\nencountered in real-world retrieval-augmented scenarios. Moreover, we find that\nmany CMTs display an inflated performance on simple synthesised datasets,\ncompared to more realistic datasets with naturally occurring samples.\nAltogether, our results show the need for holistic tests of CMTs and the\ndevelopment of CMTs that can handle multiple context types."}
{"id": "2505.16832", "pdf": "https://arxiv.org/pdf/2505.16832", "abs": "https://arxiv.org/abs/2505.16832", "authors": ["Haonian Ji", "Shi Qiu", "Siyang Xin", "Siwei Han", "Zhaorun Chen", "Hongyi Wang", "Dake Zhang", "Huaxiu Yao"], "title": "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "16 pages; 7 figures", "summary": "While foundation models (FMs), such as diffusion models and large\nvision-language models (LVLMs), have been widely applied in educational\ncontexts, their ability to generate pedagogically effective visual explanations\nremains limited. Most existing approaches focus primarily on textual reasoning,\noverlooking the critical role of structured and interpretable visualizations in\nsupporting conceptual understanding. To better assess the visual reasoning\ncapabilities of FMs in educational settings, we introduce EduVisBench, a\nmulti-domain, multi-level benchmark. EduVisBench features diverse STEM problem\nsets requiring visually grounded solutions, along with a fine-grained\nevaluation rubric informed by pedagogical theory. Our empirical analysis\nreveals that existing models frequently struggle with the inherent challenge of\ndecomposing complex reasoning and translating it into visual representations\naligned with human cognitive processes. To address these limitations, we\npropose EduVisAgent, a multi-agent collaborative framework that coordinates\nspecialized agents for instructional planning, reasoning decomposition,\nmetacognitive prompting, and visualization design. Experimental results show\nthat EduVisAgent substantially outperforms all baselines, achieving a 40.2%\nimprovement and delivering more educationally aligned visualizations.\nEduVisBench and EduVisAgent are available at\nhttps://github.com/aiming-lab/EduVisBench and\nhttps://github.com/aiming-lab/EduVisAgent."}
{"id": "2505.16257", "pdf": "https://arxiv.org/pdf/2505.16257", "abs": "https://arxiv.org/abs/2505.16257", "authors": ["Masanari Kimura"], "title": "Higher-Order Asymptotics of Test-Time Adaptation for Batch Normalization Statistics", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This study develops a higher-order asymptotic framework for test-time\nadaptation (TTA) of Batch Normalization (BN) statistics under distribution\nshift by integrating classical Edgeworth expansion and saddlepoint\napproximation techniques with a novel one-step M-estimation perspective. By\nanalyzing the statistical discrepancy between training and test distributions,\nwe derive an Edgeworth expansion for the normalized difference in BN means and\nobtain an optimal weighting parameter that minimizes the mean-squared error of\nthe adapted statistic. Reinterpreting BN TTA as a one-step M-estimator allows\nus to derive higher-order local asymptotic normality results, which incorporate\nskewness and other higher moments into the estimator's behavior. Moreover, we\nquantify the trade-offs among bias, variance, and skewness in the adaptation\nprocess and establish a corresponding generalization bound on the model risk.\nThe refined saddlepoint approximations further deliver uniformly accurate\ndensity and tail probability estimates for the BN TTA statistic. These\ntheoretical insights provide a comprehensive understanding of how higher-order\ncorrections and robust one-step updating can enhance the reliability and\nperformance of BN layers in adapting to changing data distributions."}
{"id": "2505.16520", "pdf": "https://arxiv.org/pdf/2505.16520", "abs": "https://arxiv.org/abs/2505.16520", "authors": ["Giovanni Servedio", "Alessandro De Bellis", "Dario Di Palma", "Vito Walter Anelli", "Tommaso Di Noia"], "title": "Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Factual hallucinations are a major challenge for Large Language Models\n(LLMs). They undermine reliability and user trust by generating inaccurate or\nfabricated content. Recent studies suggest that when generating false\nstatements, the internal states of LLMs encode information about truthfulness.\nHowever, these studies often rely on synthetic datasets that lack realism,\nwhich limits generalization when evaluating the factual accuracy of text\ngenerated by the model itself. In this paper, we challenge the findings of\nprevious work by investigating truthfulness encoding capabilities, leading to\nthe generation of a more realistic and challenging dataset. Specifically, we\nextend previous work by introducing: (1) a strategy for sampling plausible\ntrue-false factoid sentences from tabular data and (2) a procedure for\ngenerating realistic, LLM-dependent true-false datasets from Question Answering\ncollections. Our analysis of two open-source LLMs reveals that while the\nfindings from previous studies are partially validated, generalization to\nLLM-generated datasets remains challenging. This study lays the groundwork for\nfuture research on factuality in LLMs and offers practical guidelines for more\neffective evaluation."}
{"id": "2505.16850", "pdf": "https://arxiv.org/pdf/2505.16850", "abs": "https://arxiv.org/abs/2505.16850", "authors": ["Tajamul Ashraf", "Mohammed Mohsen Peerzada", "Moloud Abdar", "Yutong Xie", "Yuyin Zhou", "Xiaofeng Liu", "Iqra Altaf Gillani", "Janibul Bashir"], "title": "ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Federated Learning Benchmark for Domain Adaptation, Trustworthiness,\n  and Reasoning", "summary": "Federated Learning (FL) has emerged as a promising paradigm for collaborative\nmodel training while preserving data privacy across decentralized participants.\nAs FL adoption grows, numerous techniques have been proposed to tackle its\npractical challenges. However, the lack of standardized evaluation across key\ndimensions hampers systematic progress and fair comparison of FL methods. In\nthis work, we introduce ATR-Bench, a unified framework for analyzing federated\nlearning through three foundational dimensions: Adaptation, Trust, and\nReasoning. We provide an in-depth examination of the conceptual foundations,\ntask formulations, and open research challenges associated with each theme. We\nhave extensively benchmarked representative methods and datasets for adaptation\nto heterogeneous clients and trustworthiness in adversarial or unreliable\nenvironments. Due to the lack of reliable metrics and models for reasoning in\nFL, we only provide literature-driven insights for this dimension. ATR-Bench\nlays the groundwork for a systematic and holistic evaluation of federated\nlearning with real-world relevance. We will make our complete codebase publicly\naccessible and a curated repository that continuously tracks new developments\nand research in the FL literature."}
{"id": "2505.16263", "pdf": "https://arxiv.org/pdf/2505.16263", "abs": "https://arxiv.org/abs/2505.16263", "authors": ["Sampanna Yashwant Kahu", "Naman Ahuja"], "title": "All You Need is \"Leet\": Evading Hate-speech Detection AI", "categories": ["cs.CR", "cs.CL", "cs.LG", "K.6.5"], "comment": "10 pages, 22 figures, The source code and data used in this work is\n  available at: https://github.com/SampannaKahu/all_you_need_is_leet", "summary": "Social media and online forums are increasingly becoming popular.\nUnfortunately, these platforms are being used for spreading hate speech. In\nthis paper, we design black-box techniques to protect users from hate-speech on\nonline platforms by generating perturbations that can fool state of the art\ndeep learning based hate speech detection models thereby decreasing their\nefficiency. We also ensure a minimal change in the original meaning of\nhate-speech. Our best perturbation attack is successfully able to evade\nhate-speech detection for 86.8 % of hateful text."}
{"id": "2505.16522", "pdf": "https://arxiv.org/pdf/2505.16522", "abs": "https://arxiv.org/abs/2505.16522", "authors": ["Zhouhao Sun", "Zhiyuan Kan", "Xiao Ding", "Li Du", "Yang Zhao", "Bing Qin", "Ting Liu"], "title": "Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite significant progress, recent studies have indicated that current\nlarge language models (LLMs) may still utilize bias during inference, leading\nto the poor generalizability of LLMs. Some benchmarks are proposed to\ninvestigate the generalizability of LLMs, with each piece of data typically\ncontaining one type of controlled bias. However, a single piece of data may\ncontain multiple types of biases in practical applications. To bridge this gap,\nwe propose a multi-bias benchmark where each piece of data contains five types\nof biases. The evaluations conducted on this benchmark reveal that the\nperformance of existing LLMs and debiasing methods is unsatisfying,\nhighlighting the challenge of eliminating multiple types of biases\nsimultaneously. To overcome this challenge, we propose a causal effect\nestimation-guided multi-bias elimination method (CMBE). This method first\nestimates the causal effect of multiple types of biases simultaneously.\nSubsequently, we eliminate the causal effect of biases from the total causal\neffect exerted by both the semantic information and biases during inference.\nExperimental results show that CMBE can effectively eliminate multiple types of\nbias simultaneously to enhance the generalizability of LLMs."}
{"id": "2505.16886", "pdf": "https://arxiv.org/pdf/2505.16886", "abs": "https://arxiv.org/abs/2505.16886", "authors": ["Nour Jedidi", "Yung-Sung Chuang", "James Glass", "Jimmy Lin"], "title": "Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers."}
{"id": "2505.16270", "pdf": "https://arxiv.org/pdf/2505.16270", "abs": "https://arxiv.org/abs/2505.16270", "authors": ["Jiaru Zou", "Yikun Ban", "Zihao Li", "Yunzhe Qi", "Ruizhong Qiu", "Ling Yang", "Jingrui He"], "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "33 pages, 7 figures", "summary": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability."}
{"id": "2505.16530", "pdf": "https://arxiv.org/pdf/2505.16530", "abs": "https://arxiv.org/abs/2505.16530", "authors": ["Yuliang Yan", "Haochun Tang", "Shuo Yan", "Enyan Dai"], "title": "DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) are considered valuable Intellectual Properties\n(IP) for legitimate owners due to the enormous computational cost of training.\nIt is crucial to protect the IP of LLMs from malicious stealing or unauthorized\ndeployment. Despite existing efforts in watermarking and fingerprinting LLMs,\nthese methods either impact the text generation process or are limited in\nwhite-box access to the suspect model, making them impractical. Hence, we\npropose DuFFin, a novel $\\textbf{Du}$al-Level $\\textbf{Fin}$gerprinting\n$\\textbf{F}$ramework for black-box setting ownership verification. DuFFin\nextracts the trigger pattern and the knowledge-level fingerprints to identify\nthe source of a suspect model. We conduct experiments on a variety of models\ncollected from the open-source website, including four popular base models as\nprotected LLMs and their fine-tuning, quantization, and safety alignment\nversions, which are released by large companies, start-ups, and individual\nusers. Results show that our method can accurately verify the copyright of the\nbase protected LLM on their model variants, achieving the IP-ROC metric greater\nthan 0.95. Our code is available at\nhttps://github.com/yuliangyan0807/llm-fingerprint."}
{"id": "2505.16888", "pdf": "https://arxiv.org/pdf/2505.16888", "abs": "https://arxiv.org/abs/2505.16888", "authors": ["Viet Pham", "Thai Le"], "title": "CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available."}
{"id": "2505.16301", "pdf": "https://arxiv.org/pdf/2505.16301", "abs": "https://arxiv.org/abs/2505.16301", "authors": ["Fuchun Ge", "Pavlo O. Dral"], "title": "Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space", "categories": ["physics.chem-ph", "cs.AI", "cs.LG"], "comment": null, "summary": "Molecular dynamics (MD) is a powerful tool for exploring the behavior of\natomistic systems, but its reliance on sequential numerical integration limits\nsimulation efficiency. We present MDtrajNet-1, a foundational AI model that\ndirectly generates MD trajectories across chemical space, bypassing force\ncalculations and integration. This approach accelerates simulations by up to\ntwo orders of magnitude compared to traditional MD, even those enhanced by\nmachine-learning interatomic potentials. MDtrajNet-1 combines equivariant\nneural networks with a Transformer-based architecture to achieve strong\naccuracy and transferability in predicting long-time trajectories for both\nknown and unseen systems. Remarkably, the errors of the trajectories generated\nby MDtrajNet-1 for various molecular systems are close to those of the\nconventional ab initio MD. The model's flexible design supports diverse\napplication scenarios, including different statistical ensembles, boundary\nconditions, and interaction types. By overcoming the intrinsic speed barrier of\nconventional MD, MDtrajNet-1 opens new frontiers in efficient and scalable\natomistic simulations."}
{"id": "2505.16540", "pdf": "https://arxiv.org/pdf/2505.16540", "abs": "https://arxiv.org/abs/2505.16540", "authors": ["Inbal Cohen", "Boaz Meivar", "Peihan Tu", "Shai Avidan", "Gal Oren"], "title": "TextureSAM: Towards a Texture Aware Foundation Model for Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Segment Anything Models (SAM) have achieved remarkable success in object\nsegmentation tasks across diverse datasets. However, these models are\npredominantly trained on large-scale semantic segmentation datasets, which\nintroduce a bias toward object shape rather than texture cues in the image.\nThis limitation is critical in domains such as medical imaging, material\nclassification, and remote sensing, where texture changes define object\nboundaries. In this study, we investigate SAM's bias toward semantics over\ntextures and introduce a new texture-aware foundation model, TextureSAM, which\nperforms superior segmentation in texture-dominant scenarios. To achieve this,\nwe employ a novel fine-tuning approach that incorporates texture augmentation\ntechniques, incrementally modifying training images to emphasize texture\nfeatures. By leveraging a novel texture-alternation of the ADE20K dataset, we\nguide TextureSAM to prioritize texture-defined regions, thereby mitigating the\ninherent shape bias present in the original SAM model. Our extensive\nexperiments demonstrate that TextureSAM significantly outperforms SAM-2 on both\nnatural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation\ndatasets. The code and texture-augmented dataset will be publicly available."}
{"id": "2505.16932", "pdf": "https://arxiv.org/pdf/2505.16932", "abs": "https://arxiv.org/abs/2505.16932", "authors": ["Noah Amsel", "David Persson", "Christopher Musco", "Robert Gower"], "title": "The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NA", "math.NA", "math.OC"], "comment": null, "summary": "Computing the polar decomposition and the related matrix sign function, has\nbeen a well-studied problem in numerical analysis for decades. More recently,\nit has emerged as an important subroutine in deep learning, particularly within\nthe Muon optimization framework. However, the requirements in this setting\ndiffer significantly from those of traditional numerical analysis. In deep\nlearning, methods must be highly efficient and GPU-compatible, but high\naccuracy is often unnecessary. As a result, classical algorithms like\nNewton-Schulz (which suffers from slow initial convergence) and methods based\non rational functions (which rely on QR decompositions or matrix inverses) are\npoorly suited to this context. In this work, we introduce Polar Express, a\nGPU-friendly algorithm for computing the polar decomposition. Like classical\npolynomial methods such as Newton-Schulz, our approach uses only matrix-matrix\nmultiplications, making it GPU-compatible. Motivated by earlier work of Chen &\nChow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule\nat each iteration by solving a minimax optimization problem, and we prove that\nit enjoys a strong worst-case optimality guarantee. This property ensures both\nrapid early convergence and fast asymptotic convergence. We also address\nfinite-precision issues, making it stable in bfloat16 in practice. We apply\nPolar Express within the Muon optimization framework and show consistent\nimprovements in validation loss on large-scale models such as GPT-2,\noutperforming recent alternatives across a range of learning rates."}
{"id": "2505.16307", "pdf": "https://arxiv.org/pdf/2505.16307", "abs": "https://arxiv.org/abs/2505.16307", "authors": ["Chenzhuo Zhao", "Ziqian Liu", "Xingda Wang", "Junting Lu", "Chaoyi Ruan"], "title": "PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Prompt optimization offers a practical and broadly applicable alternative to\nfine-tuning for improving large language model (LLM) performance. However,\nexisting methods often rely on costly output generation, self-critiquing\nabilities, or human-annotated preferences, which limit their scalability,\nespecially for smaller or non-instruction-tuned models. We introduce PMPO\n(Probabilistic Metric Prompt Optimization), a unified framework that refines\nprompts using token-level cross-entropy loss as a direct, lightweight\nevaluation signal. PMPO identifies low-quality prompt segments by masking and\nmeasuring their impact on loss, then rewrites and selects improved variants by\nminimizing loss over positive and negative examples. Unlike prior methods, it\nrequires no output sampling or human evaluation during optimization, relying\nonly on forward passes and log-likelihoods. PMPO supports both supervised and\npreference-based tasks through a closely aligned loss-based evaluation\nstrategy. Experiments show that PMPO consistently outperforms prior methods\nacross model sizes and tasks: it achieves the highest average accuracy on BBH,\nperforms strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates\nby over 19 points. These results highlight PMPO's effectiveness, efficiency,\nand broad applicability."}
{"id": "2505.16547", "pdf": "https://arxiv.org/pdf/2505.16547", "abs": "https://arxiv.org/abs/2505.16547", "authors": ["Nitesh Subedi", "Hsin-Jung Yang", "Devesh K. Jha", "Soumik Sarkar"], "title": "Find the Fruit: Designing a Zero-Shot Sim2Real Deep RL Planner for Occlusion Aware Plant Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": "18 Pages, 15 Figures, 5 Tables", "summary": "This paper presents an end-to-end deep reinforcement learning (RL) framework\nfor occlusion-aware robotic manipulation in cluttered plant environments. Our\napproach enables a robot to interact with a deformable plant to reveal hidden\nobjects of interest, such as fruits, using multimodal observations. We decouple\nthe kinematic planning problem from robot control to simplify zero-shot\nsim2real transfer for the trained policy. Our results demonstrate that the\ntrained policy, deployed using our framework, achieves up to 86.7% success in\nreal-world trials across diverse initial conditions. Our findings pave the way\ntoward autonomous, perception-driven agricultural robots that intelligently\ninteract with complex foliage plants to \"find the fruit\" in challenging\noccluded scenarios, without the need for explicitly designed geometric and\ndynamic models of every plant scenario."}
{"id": "2505.16933", "pdf": "https://arxiv.org/pdf/2505.16933", "abs": "https://arxiv.org/abs/2505.16933", "authors": ["Zebin You", "Shen Nie", "Xiaolu Zhang", "Jun Hu", "Jun Zhou", "Zhiwu Lu", "Ji-Rong Wen", "Chongxuan Li"], "title": "LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large\nLanguage Model (MLLM) that integrates visual instruction tuning with masked\ndiffusion models, representing a departure from the autoregressive paradigms\ndominant in current multimodal approaches. Built upon LLaDA, a representative\nlarge language diffusion model, LLaDA-V incorporates a vision encoder and MLP\nconnector that projects visual features into the language embedding space,\nenabling effective multimodal alignment. Our empirical investigation reveals\nseveral intriguing results: First, LLaDA-V demonstrates promising multimodal\nperformance despite its language model being weaker on purely textual tasks\nthan counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same\ninstruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal\ntasks with better data scalability. It also narrows the performance gap to\nQwen2-VL, suggesting the effectiveness of its architecture for multimodal\ntasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal\nunderstanding compared to existing hybrid autoregressive-diffusion and purely\ndiffusion-based MLLMs. Our findings suggest that large language diffusion\nmodels show promise in multimodal contexts and warrant further investigation in\nfuture research. Project page and codes:\nhttps://ml-gsai.github.io/LLaDA-V-demo/."}
{"id": "2505.16311", "pdf": "https://arxiv.org/pdf/2505.16311", "abs": "https://arxiv.org/abs/2505.16311", "authors": ["Marc Brooks", "Gabriel Durham", "Kihyuk Hong", "Ambuj Tewari"], "title": "Generator-Mediated Bandits: Thompson Sampling for GenAI-Powered Adaptive Interventions", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": "39 pages, 12 figures", "summary": "Recent advances in generative artificial intelligence (GenAI) models have\nenabled the generation of personalized content that adapts to up-to-date user\ncontext. While personalized decision systems are often modeled using bandit\nformulations, the integration of GenAI introduces new structure into otherwise\nclassical sequential learning problems. In GenAI-powered interventions, the\nagent selects a query, but the environment experiences a stochastic response\ndrawn from the generative model. Standard bandit methods do not explicitly\naccount for this structure, where actions influence rewards only through\nstochastic, observed treatments. We introduce generator-mediated\nbandit-Thompson sampling (GAMBITTS), a bandit approach designed for this\naction/treatment split, using mobile health interventions with large language\nmodel-generated text as a motivating case study. GAMBITTS explicitly models\nboth the treatment and reward generation processes, using information in the\ndelivered treatment to accelerate policy learning relative to standard methods.\nWe establish regret bounds for GAMBITTS by decomposing sources of uncertainty\nin treatment and reward, identifying conditions where it achieves stronger\nguarantees than standard bandit approaches. In simulation studies, GAMBITTS\nconsistently outperforms conventional algorithms by leveraging observed\ntreatments to more accurately estimate expected rewards."}
{"id": "2505.16561", "pdf": "https://arxiv.org/pdf/2505.16561", "abs": "https://arxiv.org/abs/2505.16561", "authors": ["Jannis Becktepe", "Leona Hennig", "Steffen Oeltze-Jafra", "Marius Lindauer"], "title": "Auto-nnU-Net: Towards Automated Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "31 pages, 19 figures. Accepted for publication at AutoML 2025", "summary": "Medical Image Segmentation (MIS) includes diverse tasks, from bone to organ\nsegmentation, each with its own challenges in finding the best segmentation\nmodel. The state-of-the-art AutoML-related MIS-framework nnU-Net automates many\naspects of model configuration but remains constrained by fixed hyperparameters\nand heuristic design choices. As a full-AutoML framework for MIS, we propose\nAuto-nnU-Net, a novel nnU-Net variant enabling hyperparameter optimization\n(HPO), neural architecture search (NAS), and hierarchical NAS (HNAS).\nAdditionally, we propose Regularized PriorBand to balance model accuracy with\nthe computational resources required for training, addressing the resource\nconstraints often faced in real-world medical settings that limit the\nfeasibility of extensive training procedures. We evaluate our approach across\ndiverse MIS datasets from the well-established Medical Segmentation Decathlon,\nanalyzing the impact of AutoML techniques on segmentation performance,\ncomputational efficiency, and model design choices. The results demonstrate\nthat our AutoML approach substantially improves the segmentation performance of\nnnU-Net on 6 out of 10 datasets and is on par on the other datasets while\nmaintaining practical resource requirements. Our code is available at\nhttps://github.com/LUH-AI/AutonnUNet."}
{"id": "2505.16938", "pdf": "https://arxiv.org/pdf/2505.16938", "abs": "https://arxiv.org/abs/2505.16938", "authors": ["NovelSeek Team", "Bo Zhang", "Shiyang Feng", "Xiangchao Yan", "Jiakang Yuan", "Zhiyin Yu", "Xiaohan He", "Songtao Huang", "Shaowei Hou", "Zheng Nie", "Zhilong Wang", "Jinyao Liu", "Runmin Ma", "Tianshuo Peng", "Peng Ye", "Dongzhan Zhou", "Shufei Zhang", "Xiaosong Wang", "Yilan Zhang", "Meng Li", "Zhongying Tu", "Xiangyu Yue", "Wangli Ouyang", "Bowen Zhou", "Lei Bai"], "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "HomePage: https://alpha-innovator.github.io/NovelSeek-project-page", "summary": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours."}
{"id": "2505.16313", "pdf": "https://arxiv.org/pdf/2505.16313", "abs": "https://arxiv.org/abs/2505.16313", "authors": ["Arjhun Swaminathan", "Mete Akgün"], "title": "Accelerating Targeted Hard-Label Adversarial Attacks in Low-Query Black-Box Settings", "categories": ["cs.CV", "cs.LG"], "comment": "This paper contains 11 pages, 7 figures and 3 tables. For associated\n  supplementary code, see https://github.com/mdppml/TEA", "summary": "Deep neural networks for image classification remain vulnerable to\nadversarial examples -- small, imperceptible perturbations that induce\nmisclassifications. In black-box settings, where only the final prediction is\naccessible, crafting targeted attacks that aim to misclassify into a specific\ntarget class is particularly challenging due to narrow decision regions.\nCurrent state-of-the-art methods often exploit the geometric properties of the\ndecision boundary separating a source image and a target image rather than\nincorporating information from the images themselves. In contrast, we propose\nTargeted Edge-informed Attack (TEA), a novel attack that utilizes edge\ninformation from the target image to carefully perturb it, thereby producing an\nadversarial image that is closer to the source image while still achieving the\ndesired target classification. Our approach consistently outperforms current\nstate-of-the-art methods across different models in low query settings (nearly\n70\\% fewer queries are used), a scenario especially relevant in real-world\napplications with limited queries and black-box access. Furthermore, by\nefficiently generating a suitable adversarial example, TEA provides an improved\ntarget initialization for established geometry-based attacks."}
{"id": "2505.16567", "pdf": "https://arxiv.org/pdf/2505.16567", "abs": "https://arxiv.org/abs/2505.16567", "authors": ["Thibaud Gloaguen", "Mark Vero", "Robin Staab", "Martin Vechev"], "title": "Finetuning-Activated Backdoors in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Finetuning openly accessible Large Language Models (LLMs) has become standard\npractice for achieving task-specific performance improvements. Until now,\nfinetuning has been regarded as a controlled and secure process in which\ntraining on benign datasets led to predictable behaviors. In this paper, we\ndemonstrate for the first time that an adversary can create poisoned LLMs that\ninitially appear benign but exhibit malicious behaviors once finetuned by\ndownstream users. To this end, our proposed attack, FAB (Finetuning-Activated\nBackdoor), poisons an LLM via meta-learning techniques to simulate downstream\nfinetuning, explicitly optimizing for the emergence of malicious behaviors in\nthe finetuned models. At the same time, the poisoned LLM is regularized to\nretain general capabilities and to exhibit no malicious behaviors prior to\nfinetuning. As a result, when users finetune the seemingly benign model on\ntheir own datasets, they unknowingly trigger its hidden backdoor behavior. We\ndemonstrate the effectiveness of FAB across multiple LLMs and three target\nbehaviors: unsolicited advertising, refusal, and jailbreakability.\nAdditionally, we show that FAB-backdoors are robust to various finetuning\nchoices made by the user (e.g., dataset, number of steps, scheduler). Our\nfindings challenge prevailing assumptions about the security of finetuning,\nrevealing yet another critical attack vector exploiting the complexities of\nLLMs."}
{"id": "2505.16944", "pdf": "https://arxiv.org/pdf/2505.16944", "abs": "https://arxiv.org/abs/2505.16944", "authors": ["Yunjia Qi", "Hao Peng", "Xiaozhi Wang", "Amy Xin", "Youfeng Liu", "Bin Xu", "Lei Hou", "Juanzi Li"], "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research."}
{"id": "2505.16320", "pdf": "https://arxiv.org/pdf/2505.16320", "abs": "https://arxiv.org/abs/2505.16320", "authors": ["P. Huijse", "J. De Ridder", "L. Eyer", "L. Rimoldini", "B. Holl", "N. Chornay", "J. Roquette", "K. Nienartowicz", "G. Jevardat de Fombelle", "D. J. Fritzewski", "A. Kemp", "V. Vanlaer", "M. Vanrespaille", "H. Wang", "M. I. Carnerero", "C. M. Raiteri", "G. Marton", "M. Madarász", "G. Clementini", "P. Gavras", "C. Aerts"], "title": "Learning novel representations of variable sources from multi-modal $\\textit{Gaia}$ data via autoencoders", "categories": ["astro-ph.IM", "cs.LG"], "comment": "Manuscript resubmitted to Astronomy & Astrophysics after positive\n  referee report, 20 pages, 20 figures, 2 tables", "summary": "Gaia Data Release 3 (DR3) published for the first time epoch photometry,\nBP/RP (XP) low-resolution mean spectra, and supervised classification results\nfor millions of variable sources. This extensive dataset offers a unique\nopportunity to study their variability by combining multiple Gaia data\nproducts. In preparation for DR4, we propose and evaluate a machine learning\nmethodology capable of ingesting multiple Gaia data products to achieve an\nunsupervised classification of stellar and quasar variability. A dataset of 4\nmillion Gaia DR3 sources is used to train three variational autoencoders (VAE),\nwhich are artificial neural networks (ANNs) designed for data compression and\ngeneration. One VAE is trained on Gaia XP low-resolution spectra, another on a\nnovel approach based on the distribution of magnitude differences in the Gaia G\nband, and the third on folded Gaia G band light curves. Each Gaia source is\ncompressed into 15 numbers, representing the coordinates in a 15-dimensional\nlatent space generated by combining the outputs of these three models. The\nlearned latent representation produced by the ANN effectively distinguishes\nbetween the main variability classes present in Gaia DR3, as demonstrated\nthrough both supervised and unsupervised classification analysis of the latent\nspace. The results highlight a strong synergy between light curves and\nlow-resolution spectral data, emphasising the benefits of combining the\ndifferent Gaia data products. A two-dimensional projection of the latent\nvariables reveals numerous overdensities, most of which strongly correlate with\nastrophysical properties, showing the potential of this latent space for\nastrophysical discovery. We show that the properties of our novel latent\nrepresentation make it highly valuable for variability analysis tasks,\nincluding classification, clustering and outlier detection."}
{"id": "2505.16573", "pdf": "https://arxiv.org/pdf/2505.16573", "abs": "https://arxiv.org/abs/2505.16573", "authors": ["Yi Hu", "Hanchi Ren", "Jingjing Deng", "Xianghua Xie"], "title": "From Local Patterns to Global Understanding: Cross-Stock Trend Integration for Enhanced Predictive Modeling", "categories": ["cs.CE", "cs.AI"], "comment": null, "summary": "Stock price prediction is a critical area of financial forecasting,\ntraditionally approached by training models using the historical price data of\nindividual stocks. While these models effectively capture single-stock\npatterns, they fail to leverage potential correlations among stock trends,\nwhich could improve predictive performance. Current single-stock learning\nmethods are thus limited in their ability to provide a broader understanding of\nprice dynamics across multiple stocks. To address this, we propose a novel\nmethod that merges local patterns into a global understanding through\ncross-stock pattern integration. Our strategy is inspired by Federated Learning\n(FL), a paradigm designed for decentralized model training. FL enables\ncollaborative learning across distributed datasets without sharing raw data,\nfacilitating the aggregation of global insights while preserving data privacy.\nIn our adaptation, we train models on individual stock data and iteratively\nmerge them to create a unified global model. This global model is subsequently\nfine-tuned on specific stock data to retain local relevance. The proposed\nstrategy enables parallel training of individual stock models, facilitating\nefficient utilization of computational resources and reducing overall training\ntime. We conducted extensive experiments to evaluate the proposed method,\ndemonstrating that it outperforms benchmark models and enhances the predictive\ncapabilities of state-of-the-art approaches. Our results highlight the efficacy\nof Cross-Stock Trend Integration (CSTI) in advancing stock price prediction,\noffering a robust alternative to traditional single-stock learning\nmethodologies."}
{"id": "2505.16964", "pdf": "https://arxiv.org/pdf/2505.16964", "abs": "https://arxiv.org/abs/2505.16964", "authors": ["Suhao Yu", "Haojin Wang", "Juncheng Wu", "Cihang Xie", "Yuyin Zhou"], "title": "MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning", "categories": ["cs.CV", "cs.CL"], "comment": "9 pages, 4 Figures Benchmark data:\n  https://huggingface.co/datasets/SuhaoYu1020/MedFrameQA", "summary": "Existing medical VQA benchmarks mostly focus on single-image analysis, yet\nclinicians almost always compare a series of images before reaching a\ndiagnosis. To better approximate this workflow, we introduce MedFrameQA -- the\nfirst benchmark that explicitly evaluates multi-image reasoning in medical VQA.\nTo build MedFrameQA both at scale and in high-quality, we develop 1) an\nautomated pipeline that extracts temporally coherent frames from medical videos\nand constructs VQA items whose content evolves logically across images, and 2)\na multiple-stage filtering strategy, including model-based and manual review,\nto preserve data clarity, difficulty, and medical relevance. The resulting\ndataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in\n3,420 videos), covering nine human body systems and 43 organs; every question\nis accompanied by two to five images. We comprehensively benchmark ten advanced\nMultimodal LLMs -- both proprietary and open source, with and without explicit\nreasoning modules -- on MedFrameQA. The evaluation challengingly reveals that\nall models perform poorly, with most accuracies below 50%, and accuracy\nfluctuates as the number of images per question increases. Error analysis\nfurther shows that models frequently ignore salient findings, mis-aggregate\nevidence across images, and propagate early mistakes through their reasoning\nchains; results also vary substantially across body systems, organs, and\nmodalities. We hope this work can catalyze research on clinically grounded,\nmulti-image reasoning and accelerate progress toward more capable diagnostic AI\nsystems."}
{"id": "2505.16329", "pdf": "https://arxiv.org/pdf/2505.16329", "abs": "https://arxiv.org/abs/2505.16329", "authors": ["Simone Bombari", "Inbar Seroussi", "Marco Mondelli"], "title": "Better Rates for Private Linear Regression in the Proportional Regime via Aggressive Clipping", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Differentially private (DP) linear regression has received significant\nattention in the recent theoretical literature, with several works aimed at\nobtaining improved error rates. A common approach is to set the clipping\nconstant much larger than the expected norm of the per-sample gradients. While\nsimplifying the analysis, this is however in sharp contrast with what empirical\nevidence suggests to optimize performance. Our work bridges this gap between\ntheory and practice: we provide sharper rates for DP stochastic gradient\ndescent (DP-SGD) by crucially operating in a regime where clipping happens\nfrequently. Specifically, we consider the setting where the data is\nmultivariate Gaussian, the number of training samples $n$ is proportional to\nthe input dimension $d$, and the algorithm guarantees constant-order zero\nconcentrated DP. Our method relies on establishing a deterministic equivalent\nfor the trajectory of DP-SGD in terms of a family of ordinary differential\nequations (ODEs). As a consequence, the risk of DP-SGD is bounded between two\nODEs, with upper and lower bounds matching for isotropic data. By studying\nthese ODEs when $n / d$ is large enough, we demonstrate the optimality of\naggressive clipping, and we uncover the benefits of decaying learning rate and\nprivate noise scheduling."}
{"id": "2505.16581", "pdf": "https://arxiv.org/pdf/2505.16581", "abs": "https://arxiv.org/abs/2505.16581", "authors": ["Max Weltevrede", "Moritz A. Zanger", "Matthijs T. J. Spaan", "Wendelin Böhmer"], "title": "How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In the zero-shot policy transfer setting in reinforcement learning, the goal\nis to train an agent on a fixed set of training environments so that it can\ngeneralise to similar, but unseen, testing environments. Previous work has\nshown that policy distillation after training can sometimes produce a policy\nthat outperforms the original in the testing environments. However, it is not\nyet entirely clear why that is, or what data should be used to distil the\npolicy. In this paper, we prove, under certain assumptions, a generalisation\nbound for policy distillation after training. The theory provides two practical\ninsights: for improved generalisation, you should 1) train an ensemble of\ndistilled policies, and 2) distil it on as much data from the training\nenvironments as possible. We empirically verify that these insights hold in\nmore general settings, when the assumptions required for the theory no longer\nhold. Finally, we demonstrate that an ensemble of policies distilled on a\ndiverse dataset can generalise significantly better than the original agent."}
{"id": "2505.16967", "pdf": "https://arxiv.org/pdf/2505.16967", "abs": "https://arxiv.org/abs/2505.16967", "authors": ["Nandan Thakur", "Crystina Zhang", "Xueguang Ma", "Jimmy Lin"], "title": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Code is available at https://github.com/castorini/rlhn & datasets are\n  available at https://huggingface.co/rlhn", "summary": "Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini."}
{"id": "2505.16347", "pdf": "https://arxiv.org/pdf/2505.16347", "abs": "https://arxiv.org/abs/2505.16347", "authors": ["Javad Mirzaei", "Jeebak Mitra", "Gwenael Poitau"], "title": "Graph Attention Network for Optimal User Association in Wireless Networks", "categories": ["cs.IT", "cs.LG", "cs.NI", "math.IT"], "comment": "6 pages, 7 figures", "summary": "With increased 5G deployments, network densification is higher than ever to\nsupport the exponentially high throughput requirements. However, this has meant\na significant increase in energy consumption, leading to higher operational\nexpenditure (OpEx) for network operators creating an acute need for\nimprovements in network energy savings (NES). A key determinant of operational\nefficacy in cellular networks is the user association (UA) policy, as it\naffects critical aspects like spectral efficiency, load balancing etc. and\ntherefore impacts the overall energy consumption of the network directly.\nFurthermore, with cellular network topologies lending themselves well to\ngraphical abstractions, use of graphs in network optimization has gained\nsignificant prominence. In this work, we propose and analyze a graphical\nabstraction based optimization for UA in cellular networks to improve NES by\ndetermining when energy saving features like cell switch off can be activated.\nA comparison with legacy approaches establishes the superiority of the proposed\napproach."}
{"id": "2505.16582", "pdf": "https://arxiv.org/pdf/2505.16582", "abs": "https://arxiv.org/abs/2505.16582", "authors": ["Jianbiao Mei", "Tao Hu", "Daocheng Fu", "Licheng Wen", "Xuemeng Yang", "Rong Wu", "Pinlong Cai", "Xing Gao", "Yu Yang", "Chengjun Xie", "Botian Shi", "Yong Liu", "Yu Qiao"], "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 9 figures", "summary": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones."}
{"id": "2505.16968", "pdf": "https://arxiv.org/pdf/2505.16968", "abs": "https://arxiv.org/abs/2505.16968", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Gustavo Bertolo Stahl", "Seung Hun Eddie Han", "Salman Khan", "Abdulrahman Mahmoud"], "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.PL"], "comment": "20 pages, 11 figures, 5 tables", "summary": "We introduce \\texttt{CASS}, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level\n(CUDA~$\\leftrightarrow$~HIP) and assembly-level (Nvidia\nSASS~$\\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k\nverified code pairs across host and device, addressing a critical gap in\nlow-level GPU code portability. Leveraging this resource, we train the\n\\texttt{CASS} family of domain-specific language models, achieving 95\\% source\ntranslation accuracy and 37.5\\% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85\\% of test cases,\npreserving runtime and memory behavior. To support rigorous evaluation, we\nintroduce \\texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with\nground-truth execution. All data, models, and evaluation tools are released as\nopen source to foster progress in GPU compiler tooling, binary compatibility,\nand LLM-guided hardware translation. Dataset and benchmark are on\n\\href{https://huggingface.co/datasets/MBZUAI/cass}{\\textcolor{blue}{HuggingFace}},\nwith code at\n\\href{https://github.com/GustavoStahl/CASS}{\\textcolor{blue}{GitHub}}."}
{"id": "2505.16360", "pdf": "https://arxiv.org/pdf/2505.16360", "abs": "https://arxiv.org/abs/2505.16360", "authors": ["Estelle Chigot", "Dennis G. Wilson", "Meriem Ghrib", "Thomas Oberlin"], "title": "Style Transfer with Diffusion Models for Synthetic-to-Real Domain Adaptation", "categories": ["cs.CV", "cs.LG", "68T45 (Primary) 68T10, 68T07 (Secondary)", "F.1.2; F.1.4"], "comment": "Under review", "summary": "Semantic segmentation models trained on synthetic data often perform poorly\non real-world images due to domain gaps, particularly in adverse conditions\nwhere labeled data is scarce. Yet, recent foundation models enable to generate\nrealistic images without any training. This paper proposes to leverage such\ndiffusion models to improve the performance of vision models when learned on\nsynthetic data. We introduce two novel techniques for semantically consistent\nstyle transfer using diffusion models: Class-wise Adaptive Instance\nNormalization and Cross-Attention (CACTI) and its extension with selective\nattention Filtering (CACTIF). CACTI applies statistical normalization\nselectively based on semantic classes, while CACTIF further filters\ncross-attention maps based on feature similarity, preventing artifacts in\nregions with weak cross-attention correspondences. Our methods transfer style\ncharacteristics while preserving semantic boundaries and structural coherence,\nunlike approaches that apply global transformations or generate content without\nconstraints. Experiments using GTA5 as source and Cityscapes/ACDC as target\ndomains show that our approach produces higher quality images with lower FID\nscores and better content preservation. Our work demonstrates that class-aware\ndiffusion-based style transfer effectively bridges the synthetic-to-real domain\ngap even with minimal target domain data, advancing robust perception systems\nfor challenging real-world applications. The source code is available at:\nhttps://github.com/echigot/cactif."}
{"id": "2505.16596", "pdf": "https://arxiv.org/pdf/2505.16596", "abs": "https://arxiv.org/abs/2505.16596", "authors": ["Wilbert Peter Empleo", "Yitaek Kim", "Hansoul Kim", "Thiusius Rajeeth Savarimuthu", "Iñigo Iturrate"], "title": "Safe Uncertainty-Aware Learning of Robotic Suturing", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robot-Assisted Minimally Invasive Surgery is currently fully manually\ncontrolled by a trained surgeon. Automating this has great potential for\nalleviating issues, e.g., physical strain, highly repetitive tasks, and\nshortages of trained surgeons. For these reasons, recent works have utilized\nArtificial Intelligence methods, which show promising adaptability. Despite\nthese advances, there is skepticism of these methods because they lack\nexplainability and robust safety guarantees. This paper presents a framework\nfor a safe, uncertainty-aware learning method. We train an Ensemble Model of\nDiffusion Policies using expert demonstrations of needle insertion. Using an\nEnsemble model, we can quantify the policy's epistemic uncertainty, which is\nused to determine Out-Of-Distribution scenarios. This allows the system to\nrelease control back to the surgeon in the event of an unsafe scenario.\nAdditionally, we implement a model-free Control Barrier Function to place\nformal safety guarantees on the predicted action. We experimentally evaluate\nour proposed framework using a state-of-the-art robotic suturing simulator. We\nevaluate multiple scenarios, such as dropping the needle, moving the camera,\nand moving the phantom. The learned policy is robust to these perturbations,\nshowing corrective behaviors and generalization, and it is possible to detect\nOut-Of-Distribution scenarios. We further demonstrate that the Control Barrier\nFunction successfully limits the action to remain within our specified safety\nset in the case of unsafe predictions."}
{"id": "2505.16975", "pdf": "https://arxiv.org/pdf/2505.16975", "abs": "https://arxiv.org/abs/2505.16975", "authors": ["Yaxin Du", "Yuzhu Cai", "Yifan Zhou", "Cheng Wang", "Yu Qian", "Xianghe Pang", "Qian Liu", "Yue Hu", "Siheng Chen"], "title": "SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown strong capability in diverse software\nengineering tasks, e.g. code completion, bug fixing, and document generation.\nHowever, feature-driven development (FDD), a highly prevalent real-world task\nthat involves developing new functionalities for large, existing codebases,\nremains underexplored. We therefore introduce SWE-Dev, the first large-scale\ndataset (with 14,000 training and 500 test samples) designed to evaluate and\ntrain autonomous coding systems on real-world feature development tasks. To\nensure verifiable and diverse training, SWE-Dev uniquely provides all instances\nwith a runnable environment and its developer-authored executable unit tests.\nThis collection not only provides high-quality data for Supervised Fine-Tuning\n(SFT), but also enables Reinforcement Learning (RL) by delivering accurate\nreward signals from executable unit tests. Our extensive evaluations on\nSWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent\nSystems (MAS), reveal that FDD is a profoundly challenging frontier for current\nAI (e.g., Claude-3.7-Sonnet achieves only 22.45\\% Pass@3 on the hard test\nsplit). Crucially, we demonstrate that SWE-Dev serves as an effective platform\nfor model improvement: fine-tuning on training set enabled a 7B model\ncomparable to GPT-4o on \\textit{hard} split, underscoring the value of its\nhigh-quality training data. Code is available here\n\\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}."}
{"id": "2505.16381", "pdf": "https://arxiv.org/pdf/2505.16381", "abs": "https://arxiv.org/abs/2505.16381", "authors": ["Songlin Yang", "Yikang Shen", "Kaiyue Wen", "Shawn Tan", "Mayank Mishra", "Liliang Ren", "Rameswar Panda", "Yoon Kim"], "title": "PaTH Attention: Position Encoding via Accumulating Householder Transformations", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "The attention mechanism is a core primitive in modern large language models\n(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,\nposition encoding is essential for modeling structured domains such as\nlanguage. Rotary position encoding (RoPE) has emerged as the de facto standard\napproach for position encoding and is part of many modern LLMs. However, in\nRoPE the key/query transformation between two elements in a sequence is only a\nfunction of their relative position and otherwise independent of the actual\ninput. This limits the expressivity of RoPE-based transformers.\n  This paper describes PaTH, a flexible data-dependent position encoding scheme\nbased on accumulated products of Householder(like) transformations, where each\ntransformation is data-dependent, i.e., a function of the input. We derive an\nefficient parallel algorithm for training through exploiting a compact\nrepresentation of products of Householder matrices, and implement a\nFlashAttention-style blockwise algorithm that minimizes I/O cost. Across both\ntargeted synthetic benchmarks and moderate-scale real-world language modeling\nexperiments, we find that PaTH demonstrates superior performance compared to\nRoPE and other recent baselines."}
{"id": "2505.16612", "pdf": "https://arxiv.org/pdf/2505.16612", "abs": "https://arxiv.org/abs/2505.16612", "authors": ["Daniel Scalena", "Gabriele Sarti", "Arianna Bisazza", "Elisabetta Fersini", "Malvina Nissim"], "title": "Steering Large Language Models for Machine Translation Personalization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "High-quality machine translation systems based on large language models\n(LLMs) have simplified the production of personalized translations reflecting\nspecific stylistic constraints. However, these systems still struggle in\nsettings where stylistic requirements are less explicit and might be harder to\nconvey via prompting. We explore various strategies for personalizing\nLLM-generated translations in low-resource settings, focusing on the\nchallenging literary translation domain. We explore prompting strategies and\ninference-time interventions for steering model generations towards a\npersonalized style, and propose a contrastive framework exploiting latent\nconcepts extracted from sparse autoencoders to identify salient personalization\nproperties. Our results show that steering achieves strong personalization\nwhile preserving translation quality. We further examine the impact of steering\non LLM representations, finding model layers with a relevant impact for\npersonalization are impacted similarly by multi-shot prompting and our steering\nmethod, suggesting similar mechanism at play."}
{"id": "2505.16984", "pdf": "https://arxiv.org/pdf/2505.16984", "abs": "https://arxiv.org/abs/2505.16984", "authors": ["Mingyang Liu", "Gabriele Farina", "Asuman Ozdaglar"], "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Post-training has demonstrated its importance in enhancing the reasoning\ncapabilities of large language models (LLMs). The primary post-training methods\ncan be categorized into supervised fine-tuning (SFT) and reinforcement\nfine-tuning (RFT). SFT is efficient and well-suited for small language models,\nbut it may lead to overfitting and limit the reasoning abilities of larger\nmodels. In contrast, RFT generally yields better generalization but depends\nheavily on the strength of the base model. To address the limitations of SFT\nand RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm\nthat unifies SFT and RFT into a single, integrated process. UFT enables the\nmodel to effectively explore solutions while incorporating informative\nsupervision signals, bridging the gap between memorizing and thinking\nunderlying existing methods. Notably, UFT outperforms both SFT and RFT in\ngeneral, regardless of model sizes. Furthermore, we theoretically prove that\nUFT breaks RFT's inherent exponential sample complexity bottleneck, showing for\nthe first time that unified training can exponentially accelerate convergence\non long-horizon reasoning tasks."}
{"id": "2505.16410", "pdf": "https://arxiv.org/pdf/2505.16410", "abs": "https://arxiv.org/abs/2505.16410", "authors": ["Guanting Dong", "Yifei Chen", "Xiaoxi Li", "Jiajie Jin", "Hongjin Qian", "Yutao Zhu", "Hangyu Mao", "Guorui Zhou", "Zhicheng Dou", "Ji-Rong Wen"], "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Working in progress", "summary": "Recently, large language models (LLMs) have shown remarkable reasoning\ncapabilities via large-scale reinforcement learning (RL). However, leveraging\nthe RL algorithm to empower effective multi-tool collaborative reasoning in\nLLMs remains an open challenge. In this paper, we introduce Tool-Star, an\nRL-based framework designed to empower LLMs to autonomously invoke multiple\nexternal tools during stepwise reasoning. Tool-Star integrates six types of\ntools and incorporates systematic designs in both data synthesis and training.\nTo address the scarcity of tool-use data, we propose a general tool-integrated\nreasoning data synthesis pipeline, which combines tool-integrated prompting\nwith hint-based sampling to automatically and scalably generate tool-use\ntrajectories. A subsequent quality normalization and difficulty-aware\nclassification process filters out low-quality samples and organizes the\ndataset from easy to hard. Furthermore, we propose a two-stage training\nframework to enhance multi-tool collaborative reasoning by: (1) cold-start\nfine-tuning, which guides LLMs to explore reasoning patterns via\ntool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with\nhierarchical reward design, which reinforces reward understanding and promotes\neffective tool collaboration. Experimental analyses on over 10 challenging\nreasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.\nThe code is available at https://github.com/dongguanting/Tool-Star."}
{"id": "2505.16630", "pdf": "https://arxiv.org/pdf/2505.16630", "abs": "https://arxiv.org/abs/2505.16630", "authors": ["Sushant Gautam", "Cise Midoglu", "Vajira Thambawita", "Michael A. Riegler", "Pål Halvorsen", "Mubarak Shah"], "title": "SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game Understanding", "categories": ["cs.CV", "cs.AI", "68T45, 68T50", "I.2.10; I.2.7; H.5.2"], "comment": null, "summary": "The integration of artificial intelligence in sports analytics has\ntransformed soccer video understanding, enabling real-time, automated insights\ninto complex game dynamics. Traditional approaches rely on isolated data\nstreams, limiting their effectiveness in capturing the full context of a match.\nTo address this, we introduce SoccerChat, a multimodal conversational AI\nframework that integrates visual and textual data for enhanced soccer video\ncomprehension. Leveraging the extensive SoccerNet dataset, enriched with jersey\ncolor annotations and automatic speech recognition (ASR) transcripts,\nSoccerChat is fine-tuned on a structured video instruction dataset to\nfacilitate accurate game understanding, event classification, and referee\ndecision making. We benchmark SoccerChat on action classification and referee\ndecision-making tasks, demonstrating its performance in general soccer event\ncomprehension while maintaining competitive accuracy in referee decision\nmaking. Our findings highlight the importance of multimodal integration in\nadvancing soccer analytics, paving the way for more interactive and explainable\nAI-driven sports analysis. https://github.com/simula/SoccerChat"}
{"id": "2505.16994", "pdf": "https://arxiv.org/pdf/2505.16994", "abs": "https://arxiv.org/abs/2505.16994", "authors": ["Runyang You", "Yongqi Li", "Xinyu Lin", "Xin Zhang", "Wenjie Wang", "Wenjie Li", "Liqiang Nie"], "title": "$\\text{R}^2\\text{ec}$: Towards Large Recommender Models with Reasoning", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec."}
{"id": "2505.16411", "pdf": "https://arxiv.org/pdf/2505.16411", "abs": "https://arxiv.org/abs/2505.16411", "authors": ["Sreetama Sarkar", "Yue Che", "Alex Gavin", "Peter A. Beerel", "Souvik Kundu"], "title": "Mitigating Hallucinations in Vision-Language Models through Image-Guided Head Suppression", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Despite their remarkable progress in multimodal understanding tasks, large\nvision language models (LVLMs) often suffer from \"hallucinations\", generating\ntexts misaligned with the visual context. Existing methods aimed at reducing\nhallucinations through inference time intervention incur a significant increase\nin latency. To mitigate this, we present SPIN, a task-agnostic attention-guided\nhead suppression strategy that can be seamlessly integrated during inference,\nwithout incurring any significant compute or latency overhead. We investigate\nwhether hallucination in LVLMs can be linked to specific model components. Our\nanalysis suggests that hallucinations can be attributed to a dynamic subset of\nattention heads in each layer. Leveraging this insight, for each text query\ntoken, we selectively suppress attention heads that exhibit low attention to\nimage tokens, keeping the top-K attention heads intact. Extensive evaluations\non visual question answering and image description tasks demonstrate the\nefficacy of SPIN in reducing hallucination scores up to 2.7x while maintaining\nF1, and improving throughput by 1.8x compared to existing alternatives. Code is\navailable at https://github.com/YUECHE77/SPIN."}
{"id": "2505.16637", "pdf": "https://arxiv.org/pdf/2505.16637", "abs": "https://arxiv.org/abs/2505.16637", "authors": ["Wenjie Yang", "Mao Zheng", "Mingyang Song", "Zheng Li"], "title": "SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities in machine translation (MT). However, most advanced MT-specific\nLLMs heavily rely on external supervision signals during training, such as\nhuman-annotated reference data or trained reward models (RMs), which are often\nexpensive to obtain and challenging to scale. To overcome this limitation, we\npropose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for\nMT that is reference-free, fully online, and relies solely on self-judging\nrewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as\nthe backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,\ne.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like\nQwen2.5-32B-Instruct in English $\\leftrightarrow$ Chinese translation tasks\nfrom WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR\nwith external supervision from COMET, our strongest model, SSR-X-Zero-7B,\nachieves state-of-the-art performance in English $\\leftrightarrow$ Chinese\ntranslation, surpassing all existing open-source models under 72B parameters\nand even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.\nOur analysis highlights the effectiveness of the self-rewarding mechanism\ncompared to the external LLM-as-a-judge approach in MT and demonstrates its\ncomplementary benefits when combined with trained RMs. Our findings provide\nvaluable insight into the potential of self-improving RL methods. We have\npublicly released our code, data and models."}
{"id": "2505.16997", "pdf": "https://arxiv.org/pdf/2505.16997", "abs": "https://arxiv.org/abs/2505.16997", "authors": ["Rui Ye", "Xiangrui Liu", "Qimin Wu", "Xianghe Pang", "Zhenfei Yin", "Lei Bai", "Siheng Chen"], "title": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": "19 pages, 5 figures", "summary": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by\nenabling cooperation among multiple specialized agents. However, most existing\nMAS frameworks rely on a single LLM to drive all agents, constraining the\nsystem's intelligence to the limit of that model. This paper explores the\nparadigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by\ndiverse LLMs, elevating the system's potential to the collective intelligence\nof diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to\nevaluate the performance of various LLMs across different domains and\nMAS-related functions. As an extensive empirical study, we assess 27 LLMs\nacross 5 domains (encompassing 21 test sets) and 5 functions, conducting over\n1.7 million evaluations to identify optimal model selections for each\ndomain-function combination. Building on these findings, we demonstrate that\ntransitioning from homogeneous to heterogeneous LLM-driven MAS can\nsignificantly enhance system performance without requiring structural redesign.\nSpecifically, in a chatbot-only MAS scenario, the heterogeneous configuration\nyields up to 8.4\\% performance improvement on the MATH dataset. In a mixed\nchatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable\n47\\% performance boost on the AIME dataset. Our results underscore the\ntransformative potential of heterogeneous LLMs in MAS, highlighting a promising\navenue for advancing scalable, collaborative AI systems."}
{"id": "2505.16415", "pdf": "https://arxiv.org/pdf/2505.16415", "abs": "https://arxiv.org/abs/2505.16415", "authors": ["Ruizhe Li", "Chen Chen", "Yuchen Hu", "Yanjun Gao", "Xi Wang", "Emine Yilmaz"], "title": "Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in process", "summary": "Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)\ncombined with external contexts to enhance the accuracy and reliability of\ngenerated responses. However, reliably attributing generated content to\nspecific context segments, context attribution, remains challenging due to the\ncomputationally intensive nature of current methods, which often require\nextensive fine-tuning or human annotation. In this work, we introduce a novel\nJensen-Shannon Divergence driven method to Attribute Response to Context\n(ARC-JSD), enabling efficient and accurate identification of essential context\nsentences without additional fine-tuning or surrogate modelling. Evaluations on\na wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using\ninstruction-tuned LLMs in different scales demonstrate superior accuracy and\nsignificant computational efficiency improvements compared to the previous\nsurrogate-based method. Furthermore, our mechanistic analysis reveals specific\nattention heads and multilayer perceptron (MLP) layers responsible for context\nattribution, providing valuable insights into the internal workings of RAG\nmodels."}
{"id": "2505.16640", "pdf": "https://arxiv.org/pdf/2505.16640", "abs": "https://arxiv.org/abs/2505.16640", "authors": ["Xueyang Zhou", "Guiyao Tie", "Guowen Zhang", "Hechang Wang", "Pan Zhou", "Lichao Sun"], "title": "BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization", "categories": ["cs.CR", "cs.AI", "68T07", "I.2.6; I.2.9"], "comment": "19 pages, 12 figures, 6 tables", "summary": "Vision-Language-Action (VLA) models have advanced robotic control by enabling\nend-to-end decision-making directly from multimodal inputs. However, their\ntightly coupled architectures expose novel security vulnerabilities. Unlike\ntraditional adversarial perturbations, backdoor attacks represent a stealthier,\npersistent, and practically significant threat-particularly under the emerging\nTraining-as-a-Service paradigm-but remain largely unexplored in the context of\nVLA models. To address this gap, we propose BadVLA, a backdoor attack method\nbased on Objective-Decoupled Optimization, which for the first time exposes the\nbackdoor vulnerabilities of VLA models. Specifically, it consists of a\ntwo-stage process: (1) explicit feature-space separation to isolate trigger\nrepresentations from benign inputs, and (2) conditional control deviations that\nactivate only in the presence of the trigger, while preserving clean-task\nperformance. Empirical results on multiple VLA benchmarks demonstrate that\nBadVLA consistently achieves near-100% attack success rates with minimal impact\non clean task accuracy. Further analyses confirm its robustness against common\ninput perturbations, task transfers, and model fine-tuning, underscoring\ncritical security vulnerabilities in current VLA deployments. Our work offers\nthe first systematic investigation of backdoor vulnerabilities in VLA models,\nhighlighting an urgent need for secure and trustworthy embodied model design\npractices. We have released the project page at\nhttps://badvla-project.github.io/."}
{"id": "2505.17015", "pdf": "https://arxiv.org/pdf/2505.17015", "abs": "https://arxiv.org/abs/2505.17015", "authors": ["Runsen Xu", "Weiyao Wang", "Hao Tang", "Xingyu Chen", "Xiaodong Wang", "Fu-Jen Chu", "Dahua Lin", "Matt Feiszli", "Kevin J. Liang"], "title": "Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "24 pages. An MLLM, dataset, and benchmark for multi-frame spatial\n  understanding. Project page: https://runsenxu.com/projects/Multi-SpatialMLLM", "summary": "Multi-modal large language models (MLLMs) have rapidly advanced in visual\ntasks, yet their spatial understanding remains limited to single images,\nleaving them ill-suited for robotics and other real-world applications that\nrequire multi-frame reasoning. In this paper, we propose a framework to equip\nMLLMs with robust multi-frame spatial understanding by integrating depth\nperception, visual correspondence, and dynamic perception. Central to our\napproach is the MultiSPA dataset, a novel, large-scale collection of more than\n27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we\nintroduce a comprehensive benchmark that tests a wide spectrum of spatial tasks\nunder uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves\nsignificant gains over baselines and proprietary systems, demonstrating\nscalable, generalizable multi-frame reasoning. We further observe multi-task\nbenefits and early indications of emergent capabilities in challenging\nscenarios, and showcase how our model can serve as a multi-frame reward\nannotator for robotics."}
{"id": "2505.16421", "pdf": "https://arxiv.org/pdf/2505.16421", "abs": "https://arxiv.org/abs/2505.16421", "authors": ["Zhepei Wei", "Wenlin Yao", "Yao Liu", "Weizhi Zhang", "Qin Lu", "Liang Qiu", "Changlong Yu", "Puyang Xu", "Chao Zhang", "Bing Yin", "Hyokun Yun", "Lihong Li"], "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "While reinforcement learning (RL) has demonstrated remarkable success in\nenhancing large language models (LLMs), it has primarily focused on single-turn\ntasks such as solving math problems. Training effective web agents for\nmulti-turn interactions remains challenging due to the complexity of\nlong-horizon decision-making across dynamic web interfaces. In this work, we\npresent WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework\nfor training web agents. It learns directly from online interactions with web\nenvironments by asynchronously generating diverse trajectories, entirely guided\nby binary rewards depending on task success. Experiments on the WebArena-Lite\nbenchmark demonstrate the effectiveness of WebAgent-R1, boosting the task\nsuccess rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to\n44.8%, significantly outperforming existing state-of-the-art methods and strong\nproprietary models such as OpenAI o3. In-depth analyses reveal the\neffectiveness of the thinking-based prompting strategy and test-time scaling\nthrough increased interactions for web tasks. We further investigate different\nRL initialization policies by introducing two variants, namely WebAgent-R1-Zero\nand WebAgent-R1-CoT, which highlight the importance of the warm-up training\nstage (i.e., behavior cloning) and provide insights on incorporating long\nchain-of-thought (CoT) reasoning in web agents."}
{"id": "2505.16643", "pdf": "https://arxiv.org/pdf/2505.16643", "abs": "https://arxiv.org/abs/2505.16643", "authors": ["Yiwei Sun", "Peiqi Jiang", "Chuanbin Liu", "Luohao Lin", "Zhiying Lu", "Hongtao Xie"], "title": "From Evaluation to Defense: Advancing Safety in Video Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "49 pages, 12 figures, 17 tables", "summary": "While the safety risks of image-based large language models have been\nextensively studied, their video-based counterparts (Video LLMs) remain\ncritically under-examined. To systematically study this problem, we introduce\n\\textbf{VideoSafetyBench (VSB-77k) - the first large-scale, culturally diverse\nbenchmark for Video LLM safety}, which compromises 77,646 video-query pairs and\nspans 19 principal risk categories across 10 language communities. \\textit{We\nreveal that integrating video modality degrades safety performance by an\naverage of 42.3\\%, exposing systemic risks in multimodal attack exploitation.}\nTo address this vulnerability, we propose \\textbf{VideoSafety-R1}, a dual-stage\nframework achieving unprecedented safety gains through two innovations: (1)\nAlarm Token-Guided Safety Fine-Tuning (AT-SFT) injects learnable alarm tokens\ninto visual and textual sequences, enabling explicit harm perception across\nmodalities via multitask objectives. (2) Then, Safety-Guided GRPO enhances\ndefensive reasoning through dynamic policy optimization with rule-based rewards\nderived from dual-modality verification. These components synergize to shift\nsafety alignment from passive harm recognition to active reasoning. The\nresulting framework achieves a 65.1\\% improvement on VSB-Eval-HH, and improves\nby 59.1\\%, 44.3\\%, and 15.0\\% on the image safety datasets MMBench, VLGuard,\nand FigStep, respectively. \\textit{Our codes are available in the supplementary\nmaterials.} \\textcolor{red}{Warning: This paper contains examples of harmful\nlanguage and videos, and reader discretion is recommended.}"}
{"id": "2505.17017", "pdf": "https://arxiv.org/pdf/2505.17017", "abs": "https://arxiv.org/abs/2505.17017", "authors": ["Chengzhuo Tong", "Ziyu Guo", "Renrui Zhang", "Wenyu Shan", "Xinyu Wei", "Zhenghao Xing", "Hongsheng Li", "Pheng-Ann Heng"], "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT", "summary": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT"}
{"id": "2505.16441", "pdf": "https://arxiv.org/pdf/2505.16441", "abs": "https://arxiv.org/abs/2505.16441", "authors": ["Jisu Han", "Jaemin Na", "Wonjun Hwang"], "title": "Ranked Entropy Minimization for Continual Test-Time Adaptation", "categories": ["cs.CV", "cs.LG"], "comment": "ICML 2025", "summary": "Test-time adaptation aims to adapt to realistic environments in an online\nmanner by learning during test time. Entropy minimization has emerged as a\nprincipal strategy for test-time adaptation due to its efficiency and\nadaptability. Nevertheless, it remains underexplored in continual test-time\nadaptation, where stability is more important. We observe that the entropy\nminimization method often suffers from model collapse, where the model\nconverges to predicting a single class for all images due to a trivial\nsolution. We propose ranked entropy minimization to mitigate the stability\nproblem of the entropy minimization method and extend its applicability to\ncontinuous scenarios. Our approach explicitly structures the prediction\ndifficulty through a progressive masking strategy. Specifically, it gradually\naligns the model's probability distributions across different levels of\nprediction difficulty while preserving the rank order of entropy. The proposed\nmethod is extensively evaluated across various benchmarks, demonstrating its\neffectiveness through empirical results. Our code is available at\nhttps://github.com/pilsHan/rem"}
{"id": "2505.16647", "pdf": "https://arxiv.org/pdf/2505.16647", "abs": "https://arxiv.org/abs/2505.16647", "authors": ["Sushant Gautam", "Michael A. Riegler", "Pål Halvorsen"], "title": "Point, Detect, Count: Multi-Task Medical Image Understanding with Instruction-Tuned Vision-Language Models", "categories": ["cs.CV", "cs.AI", "68T45, 68T07", "I.2.10; I.4.8"], "comment": "Accepted as a full paper at the 38th IEEE International Symposium on\n  Computer-Based Medical Systems (CBMS) 2025", "summary": "We investigate fine-tuning Vision-Language Models (VLMs) for multi-task\nmedical image understanding, focusing on detection, localization, and counting\nof findings in medical images. Our objective is to evaluate whether\ninstruction-tuned VLMs can simultaneously improve these tasks, with the goal of\nenhancing diagnostic accuracy and efficiency. Using MedMultiPoints, a\nmultimodal dataset with annotations from endoscopy (polyps and instruments) and\nmicroscopy (sperm cells), we reformulate each task into instruction-based\nprompts suitable for vision-language reasoning. We fine-tune\nQwen2.5-VL-7B-Instruct using Low-Rank Adaptation (LoRA) across multiple task\ncombinations. Results show that multi-task training improves robustness and\naccuracy. For example, it reduces the Count Mean Absolute Error (MAE) and\nincreases Matching Accuracy in the Counting + Pointing task. However,\ntrade-offs emerge, such as more zero-case point predictions, indicating reduced\nreliability in edge cases despite overall performance gains. Our study\nhighlights the potential of adapting general-purpose VLMs to specialized\nmedical tasks via prompt-driven fine-tuning. This approach mirrors clinical\nworkflows, where radiologists simultaneously localize, count, and describe\nfindings - demonstrating how VLMs can learn composite diagnostic reasoning\npatterns. The model produces interpretable, structured outputs, offering a\npromising step toward explainable and versatile medical AI. Code, model\nweights, and scripts will be released for reproducibility at\nhttps://github.com/simula/PointDetectCount."}
{"id": "2505.17022", "pdf": "https://arxiv.org/pdf/2505.17022", "abs": "https://arxiv.org/abs/2505.17022", "authors": ["Chengqi Duan", "Rongyao Fang", "Yuqing Wang", "Kun Wang", "Linjiang Huang", "Xingyu Zeng", "Hongsheng Li", "Xihui Liu"], "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Github page refer to: https://github.com/gogoduan/GoT-R1", "summary": "Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1."}
{"id": "2505.16452", "pdf": "https://arxiv.org/pdf/2505.16452", "abs": "https://arxiv.org/abs/2505.16452", "authors": ["Mohamed S. Elmahdy", "Marius Staring", "Patrick J. H. de Koning", "Samer Alabed", "Mahan Salehi", "Faisal Alandejani", "Michael Sharkey", "Ziad Aldabbagh", "Andrew J. Swift", "Rob J. van der Geest"], "title": "CMRINet: Joint Groupwise Registration and Segmentation for Cardiac Function Quantification from Cine-MRI", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "15 pages, 7 figures, 1 appendix", "summary": "Accurate and efficient quantification of cardiac function is essential for\nthe estimation of prognosis of cardiovascular diseases (CVDs). One of the most\ncommonly used metrics for evaluating cardiac pumping performance is left\nventricular ejection fraction (LVEF). However, LVEF can be affected by factors\nsuch as inter-observer variability and varying pre-load and after-load\nconditions, which can reduce its reproducibility. Additionally, cardiac\ndysfunction may not always manifest as alterations in LVEF, such as in heart\nfailure and cardiotoxicity diseases. An alternative measure that can provide a\nrelatively load-independent quantitative assessment of myocardial contractility\nis myocardial strain and strain rate. By using LVEF in combination with\nmyocardial strain, it is possible to obtain a thorough description of cardiac\nfunction. Automated estimation of LVEF and other volumetric measures from\ncine-MRI sequences can be achieved through segmentation models, while strain\ncalculation requires the estimation of tissue displacement between sequential\nframes, which can be accomplished using registration models. These tasks are\noften performed separately, potentially limiting the assessment of cardiac\nfunction. To address this issue, in this study we propose an end-to-end deep\nlearning (DL) model that jointly estimates groupwise (GW) registration and\nsegmentation for cardiac cine-MRI images. The proposed anatomically-guided Deep\nGW network was trained and validated on a large dataset of 4-chamber view\ncine-MRI image series of 374 subjects. A quantitative comparison with\nconventional GW registration using elastix and two DL-based methods showed that\nthe proposed model improved performance and substantially reduced computation\ntime."}
{"id": "2505.16648", "pdf": "https://arxiv.org/pdf/2505.16648", "abs": "https://arxiv.org/abs/2505.16648", "authors": ["Kexin Shang", "Chia-Hsuan Chang", "Christopher C. Yang"], "title": "Collaboration among Multiple Large Language Models for Medical Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to IEEE International Conference on Healthcare Informatics\n  2025", "summary": "Empowered by vast internal knowledge reservoir, the new generation of large\nlanguage models (LLMs) demonstrate untapped potential to tackle medical tasks.\nHowever, there is insufficient effort made towards summoning up a synergic\neffect from multiple LLMs' expertise and background. In this study, we propose\na multi-LLM collaboration framework tailored on a medical multiple-choice\nquestions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,\nour framework is proved to boost all LLMs reasoning ability as well as\nalleviate their divergence among questions. We also measure an LLM's confidence\nwhen it confronts with adversary opinions from other LLMs and observe a\nconcurrence between LLM's confidence and prediction accuracy."}
{"id": "2505.16463", "pdf": "https://arxiv.org/pdf/2505.16463", "abs": "https://arxiv.org/abs/2505.16463", "authors": ["Jiquan Shan", "Junxiao Wang", "Lifeng Zhao", "Liang Cai", "Hongyuan Zhang", "Ioannis Liritzis"], "title": "AnchorFormer: Differentiable Anchor Attention for Efficient Vision Transformer", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recently, vision transformers (ViTs) have achieved excellent performance on\nvision tasks by measuring the global self-attention among the image patches.\nGiven $n$ patches, they will have quadratic complexity such as\n$\\mathcal{O}(n^2)$ and the time cost is high when splitting the input image\nwith a small granularity. Meanwhile, the pivotal information is often randomly\ngathered in a few regions of an input image, some tokens may not be helpful for\nthe downstream tasks. To handle this problem, we introduce an anchor-based\nefficient vision transformer (AnchorFormer), which employs the anchor tokens to\nlearn the pivotal information and accelerate the inference. Firstly, by\nestimating the bipartite attention between the anchors and tokens, the\ncomplexity will be reduced from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(mn)$, where\n$m$ is an anchor number and $m < n$. Notably, by representing the anchors with\nthe neurons in a neural layer, we can differentiable learn these distributions\nand approximate global self-attention through the Markov process. Moreover, we\nextend the proposed model to three downstream tasks including classification,\ndetection, and segmentation. Extensive experiments show the effectiveness of\nour AnchorFormer, e.g., achieving up to a 9.0% higher accuracy or 46.7% FLOPs\nreduction on ImageNet classification, 81.3% higher mAP on COCO detection under\ncomparable FLOPs, as compared to the current baselines."}
{"id": "2505.16660", "pdf": "https://arxiv.org/pdf/2505.16660", "abs": "https://arxiv.org/abs/2505.16660", "authors": ["Liu Chang", "Wang Dongbo", "Liu liu", "Zhao Zhixiao"], "title": "Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu", "categories": ["cs.CL", "cs.AI"], "comment": "29pages, 7 figures", "summary": "This study addresses the challenges in intelligent processing of Chinese\nancient mathematical classics by constructing Guji_MATH, a benchmark for\nevaluating classical texts based on Suanjing Shishu. It systematically assesses\nthe mathematical problem-solving capabilities of mainstream reasoning models\nunder the unique linguistic constraints of classical Chinese. Through\nmachine-assisted annotation and manual verification, 538 mathematical problems\nwere extracted from 8 canonical texts, forming a structured dataset centered on\nthe \"Question-Answer-Solution\" framework, supplemented by problem types and\ndifficulty levels. Dual evaluation modes--closed-book (autonomous\nproblem-solving) and open-book (reproducing classical solution methods)--were\ndesigned to evaluate the performance of six reasoning models on ancient Chinese\nmathematical problems. Results indicate that reasoning models can partially\ncomprehend and solve these problems, yet their overall performance remains\ninferior to benchmarks on modern mathematical tasks. Enhancing models'\nclassical Chinese comprehension and cultural knowledge should be prioritized\nfor optimization. This study provides methodological support for mining\nmathematical knowledge from ancient texts and disseminating traditional\nculture, while offering new perspectives for evaluating cross-linguistic and\ncross-cultural capabilities of reasoning models."}
{"id": "2505.16471", "pdf": "https://arxiv.org/pdf/2505.16471", "abs": "https://arxiv.org/abs/2505.16471", "authors": ["Robbert Reijnen", "Yaoxin Wu", "Zaharah Bukhsh", "Yingqian Zhang"], "title": "Graph-Supported Dynamic Algorithm Configuration for Multi-Objective Combinatorial Optimization", "categories": ["cs.NE", "cs.LG"], "comment": null, "summary": "Deep reinforcement learning (DRL) has been widely used for dynamic algorithm\nconfiguration, particularly in evolutionary computation, which benefits from\nthe adaptive update of parameters during the algorithmic execution. However,\napplying DRL to algorithm configuration for multi-objective combinatorial\noptimization (MOCO) problems remains relatively unexplored. This paper presents\na novel graph neural network (GNN) based DRL to configure multi-objective\nevolutionary algorithms. We model the dynamic algorithm configuration as a\nMarkov decision process, representing the convergence of solutions in the\nobjective space by a graph, with their embeddings learned by a GNN to enhance\nthe state representation. Experiments on diverse MOCO challenges indicate that\nour method outperforms traditional and DRL-based algorithm configuration\nmethods in terms of efficacy and adaptability. It also exhibits advantageous\ngeneralizability across objective types and problem sizes, and applicability to\ndifferent evolutionary computation methods."}
{"id": "2505.16664", "pdf": "https://arxiv.org/pdf/2505.16664", "abs": "https://arxiv.org/abs/2505.16664", "authors": ["Khoa Tran", "Tri Le", "Bao Huynh", "Hung-Cuong Trinh", "Vy-Rin Nguyen"], "title": "End-to-End Framework for Predicting the Remaining Useful Life of Lithium-Ion Batteries", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate prediction of the Remaining Useful Life (RUL) is essential for\nenabling timely maintenance of lithium-ion batteries, impacting the operational\nefficiency of electric applications that rely on them. This paper proposes a\nRUL prediction approach that leverages data from recent charge-discharge cycles\nto estimate the number of remaining usable cycles. The approach introduces both\na novel signal processing pipeline and a deep learning prediction model. In the\nsignal preprocessing pipeline, a derived capacity feature is computed based on\ncurrent and capacity signals. Alongside original capacity, voltage and current,\nthese features are denoised and enhanced using statistical metrics and a\ndelta-based method to capture differences between the current and previous\ncycles. In the prediction model, the processed features are then fed into a\nhybrid deep learning architecture composed of 1D Convolutional Neural Networks\n(CNN), Attentional Long Short-Term Memory (A-LSTM), and Ordinary Differential\nEquation-based LSTM (ODE-LSTM) modules. This architecture is designed to\ncapture both local signal characteristics and long-range temporal dependencies\nwhile modeling the continuous-time dynamics of battery degradation. The model\nis further evaluated using transfer learning across different learning\nstrategies and target data partitioning scenarios. Results indicate that the\nmodel maintains robust performance, even when fine-tuned on limited target\ndata. Experimental results on two publicly available large-scale datasets\ndemonstrate that the proposed method outperforms a baseline deep learning\napproach and machine learning techniques, achieving an RMSE of 101.59,\nhighlighting its strong potential for real-world RUL prediction applications."}
{"id": "2505.16524", "pdf": "https://arxiv.org/pdf/2505.16524", "abs": "https://arxiv.org/abs/2505.16524", "authors": ["Huitong Yang", "Zhuoxiao Chen", "Fengyi Zhang", "Zi Huang", "Yadan Luo"], "title": "CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Maintaining robust 3D perception under dynamic and unpredictable test-time\nconditions remains a critical challenge for autonomous driving systems.\nExisting test-time adaptation (TTA) methods often fail in high-variance tasks\nlike 3D object detection due to unstable optimization and sharp minima. While\nrecent model merging strategies based on linear mode connectivity (LMC) offer\nimproved stability by interpolating between fine-tuned checkpoints, they are\ncomputationally expensive, requiring repeated checkpoint access and multiple\nforward passes. In this paper, we introduce CodeMerge, a lightweight and\nscalable model merging framework that bypasses these limitations by operating\nin a compact latent space. Instead of loading full models, CodeMerge represents\neach checkpoint with a low-dimensional fingerprint derived from the source\nmodel's penultimate features and constructs a key-value codebook. We compute\nmerging coefficients using ridge leverage scores on these fingerprints,\nenabling efficient model composition without compromising adaptation quality.\nOur method achieves strong performance across challenging benchmarks, improving\nend-to-end 3D detection 14.9% NDS on nuScenes-C and LiDAR-based detection by\nover 7.6% mAP on nuScenes-to-KITTI, while benefiting downstream tasks such as\nonline mapping, motion prediction and planning even without training. Code and\npretrained models are released in the supplementary material."}
{"id": "2505.16670", "pdf": "https://arxiv.org/pdf/2505.16670", "abs": "https://arxiv.org/abs/2505.16670", "authors": ["Xiaobei Yan", "Yiming Li", "Zhaoxin Fan", "Han Qiu", "Tianwei Zhang"], "title": "BitHydra: Towards Bit-flip Inference Cost Attack against Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown impressive capabilities across a wide\nrange of applications, but their ever-increasing size and resource demands make\nthem vulnerable to inference cost attacks, where attackers induce victim LLMs\nto generate the longest possible output content. In this paper, we revisit\nexisting inference cost attacks and reveal that these methods can hardly\nproduce large-scale malicious effects since they are self-targeting, where\nattackers are also the users and therefore have to execute attacks solely\nthrough the inputs, whose generated content will be charged by LLMs and can\nonly directly influence themselves. Motivated by these findings, this paper\nintroduces a new type of inference cost attacks (dubbed 'bit-flip inference\ncost attack') that target the victim model itself rather than its inputs.\nSpecifically, we design a simple yet effective method (dubbed 'BitHydra') to\neffectively flip critical bits of model parameters. This process is guided by a\nloss function designed to suppress <EOS> token's probability with an efficient\ncritical bit search algorithm, thus explicitly defining the attack objective\nand enabling effective optimization. We evaluate our method on 11 LLMs ranging\nfrom 1.5B to 14B parameters under both int8 and float16 settings. Experimental\nresults demonstrate that with just 4 search samples and as few as 3 bit flips,\nBitHydra can force 100% of test prompts to reach the maximum generation length\n(e.g., 2048 tokens) on representative LLMs such as LLaMA3, highlighting its\nefficiency, scalability, and strong transferability across unseen inputs."}
{"id": "2505.16594", "pdf": "https://arxiv.org/pdf/2505.16594", "abs": "https://arxiv.org/abs/2505.16594", "authors": ["Vignesh Gopinathan", "Urs Zimmermann", "Michael Arnold", "Matthias Rottmann"], "title": "Temporal Object Captioning for Street Scene Videos from LiDAR Tracks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Video captioning models have seen notable advancements in recent years,\nespecially with regard to their ability to capture temporal information. While\nmany research efforts have focused on architectural advancements, such as\ntemporal attention mechanisms, there remains a notable gap in understanding how\nmodels capture and utilize temporal semantics for effective temporal feature\nextraction, especially in the context of Advanced Driver Assistance Systems. We\npropose an automated LiDAR-based captioning procedure that focuses on the\ntemporal dynamics of traffic participants. Our approach uses a rule-based\nsystem to extract essential details such as lane position and relative motion\nfrom object tracks, followed by a template-based caption generation. Our\nfindings show that training SwinBERT, a video captioning model, using only\nfront camera images and supervised with our template-based captions,\nspecifically designed to encapsulate fine-grained temporal behavior, leads to\nimproved temporal understanding consistently across three datasets. In\nconclusion, our results clearly demonstrate that integrating LiDAR-based\ncaption supervision significantly enhances temporal understanding, effectively\naddressing and reducing the inherent visual/static biases prevalent in current\nstate-of-the-art model architectures."}
{"id": "2505.16673", "pdf": "https://arxiv.org/pdf/2505.16673", "abs": "https://arxiv.org/abs/2505.16673", "authors": ["Huanjin Yao", "Qixiang Yin", "Jingyi Zhang", "Min Yang", "Yibo Wang", "Wenhao Wu", "Fei Su", "Li Shen", "Minghui Qiu", "Dacheng Tao", "Jiaxing Huang"], "title": "R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Technical report", "summary": "In this work, we aim to incentivize the reasoning ability of Multimodal Large\nLanguage Models (MLLMs) via reinforcement learning (RL) and develop an\neffective approach that mitigates the sparse reward and advantage vanishing\nissues during RL. To this end, we propose Share-GRPO, a novel RL approach that\ntackle these issues by exploring and sharing diverse reasoning trajectories\nover expanded question space. Specifically, Share-GRPO first expands the\nquestion space for a given question via data transformation techniques, and\nthen encourages MLLM to effectively explore diverse reasoning trajectories over\nthe expanded question space and shares the discovered reasoning trajectories\nacross the expanded questions during RL. In addition, Share-GRPO also shares\nreward information during advantage computation, which estimates solution\nadvantages hierarchically across and within question variants, allowing more\naccurate estimation of relative advantages and improving the stability of\npolicy training. Extensive evaluations over six widely-used reasoning\nbenchmarks showcase the superior performance of our method. Code will be\navailable at https://github.com/HJYao00/R1-ShareVL."}
{"id": "2505.16612", "pdf": "https://arxiv.org/pdf/2505.16612", "abs": "https://arxiv.org/abs/2505.16612", "authors": ["Daniel Scalena", "Gabriele Sarti", "Arianna Bisazza", "Elisabetta Fersini", "Malvina Nissim"], "title": "Steering Large Language Models for Machine Translation Personalization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "High-quality machine translation systems based on large language models\n(LLMs) have simplified the production of personalized translations reflecting\nspecific stylistic constraints. However, these systems still struggle in\nsettings where stylistic requirements are less explicit and might be harder to\nconvey via prompting. We explore various strategies for personalizing\nLLM-generated translations in low-resource settings, focusing on the\nchallenging literary translation domain. We explore prompting strategies and\ninference-time interventions for steering model generations towards a\npersonalized style, and propose a contrastive framework exploiting latent\nconcepts extracted from sparse autoencoders to identify salient personalization\nproperties. Our results show that steering achieves strong personalization\nwhile preserving translation quality. We further examine the impact of steering\non LLM representations, finding model layers with a relevant impact for\npersonalization are impacted similarly by multi-shot prompting and our steering\nmethod, suggesting similar mechanism at play."}
{"id": "2505.16679", "pdf": "https://arxiv.org/pdf/2505.16679", "abs": "https://arxiv.org/abs/2505.16679", "authors": ["Jordan Dotzel", "Tony Montes", "Mohamed S. Abdelfattah", "Zhiru Zhang"], "title": "Semantic Compression of 3D Objects for Open and Collaborative Virtual Worlds", "categories": ["cs.CV", "cs.AI"], "comment": "First two authors have equal contribution", "summary": "Traditional methods for 3D object compression operate only on structural\ninformation within the object vertices, polygons, and textures. These methods\nare effective at compression rates up to 10x for standard object sizes but\nquickly deteriorate at higher compression rates with texture artifacts,\nlow-polygon counts, and mesh gaps. In contrast, semantic compression ignores\nstructural information and operates directly on the core concepts to push to\nextreme levels of compression. In addition, it uses natural language as its\nstorage format, which makes it natively human-readable and a natural fit for\nemerging applications built around large-scale, collaborative projects within\naugmented and virtual reality. It deprioritizes structural information like\nlocation, size, and orientation and predicts the missing information with\nstate-of-the-art deep generative models. In this work, we construct a pipeline\nfor 3D semantic compression from public generative models and explore the\nquality-compression frontier for 3D object compression. We apply this pipeline\nto achieve rates as high as 105x for 3D objects taken from the Objaverse\ndataset and show that semantic compression can outperform traditional methods\nin the important quality-preserving region around 100x compression."}
{"id": "2505.16635", "pdf": "https://arxiv.org/pdf/2505.16635", "abs": "https://arxiv.org/abs/2505.16635", "authors": ["Zhaomin Wu", "Ziyang Wang", "Bingsheng He"], "title": "WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Tabular data, ubiquitous and rich in informational value, is an increasing\nfocus for deep representation learning, yet progress is hindered by studies\ncentered on single tables or isolated databases, which limits model\ncapabilities due to data scale. While collaborative learning approaches such as\nfederated learning, transfer learning, split learning, and tabular foundation\nmodels aim to learn from multiple correlated databases, they are challenged by\na scarcity of real-world interconnected tabular resources. Current data lakes\nand corpora largely consist of isolated databases lacking defined\ninter-database correlations. To overcome this, we introduce WikiDBGraph, a\nlarge-scale graph of 100,000 real-world tabular databases from WikiData,\ninterconnected by 17 million edges and characterized by 13 node and 12 edge\nproperties derived from its database schema and data distribution.\nWikiDBGraph's weighted edges identify both instance- and feature-overlapped\ndatabases. Experiments on these newly identified databases confirm that\ncollaborative learning yields superior performance, thereby offering\nconsiderable promise for structured foundation model training while also\nexposing key challenges and future directions for learning from interconnected\ntabular data."}
{"id": "2505.16690", "pdf": "https://arxiv.org/pdf/2505.16690", "abs": "https://arxiv.org/abs/2505.16690", "authors": ["Beier Luo", "Shuoyuan Wang", "Yixuan Li", "Hongxin Wei"], "title": "Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Post-training of large language models is essential for adapting pre-trained\nlanguage models (PLMs) to align with human preferences and downstream tasks.\nWhile PLMs typically exhibit well-calibrated confidence, post-trained language\nmodels (PoLMs) often suffer from over-confidence, assigning high confidence to\nboth correct and incorrect outputs, which can undermine reliability in critical\napplications. A major obstacle in calibrating PoLMs is the scarcity of labeled\ndata for individual downstream tasks. To address this, we propose\nDisagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to\noptimize the parameters (e.g., temperature $\\tau$) in post-hoc confidence\ncalibration. Our method is motivated by the under-confidence issue caused by\nprediction disagreement between the PLM and PoLM while aligning their\nconfidence via temperature scaling. Theoretically, the PLM's confidence\nunderestimates PoLM's prediction accuracy on disagreement examples, causing a\nlarger $\\tau$ and producing under-confident predictions. DACA mitigates this by\nselectively using only agreement examples for calibration, effectively\ndecoupling the influence of disagreement. In this manner, our method avoids an\noverly large $\\tau$ in temperature scaling caused by disagreement examples,\nimproving calibration performance. Extensive experiments demonstrate the\neffectiveness of our method, improving the average ECE of open-sourced and\nAPI-based LLMs (e.g. GPT-4o) by up to 15.08$\\%$ on common benchmarks."}
{"id": "2505.16637", "pdf": "https://arxiv.org/pdf/2505.16637", "abs": "https://arxiv.org/abs/2505.16637", "authors": ["Wenjie Yang", "Mao Zheng", "Mingyang Song", "Zheng Li"], "title": "SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities in machine translation (MT). However, most advanced MT-specific\nLLMs heavily rely on external supervision signals during training, such as\nhuman-annotated reference data or trained reward models (RMs), which are often\nexpensive to obtain and challenging to scale. To overcome this limitation, we\npropose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for\nMT that is reference-free, fully online, and relies solely on self-judging\nrewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as\nthe backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,\ne.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like\nQwen2.5-32B-Instruct in English $\\leftrightarrow$ Chinese translation tasks\nfrom WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR\nwith external supervision from COMET, our strongest model, SSR-X-Zero-7B,\nachieves state-of-the-art performance in English $\\leftrightarrow$ Chinese\ntranslation, surpassing all existing open-source models under 72B parameters\nand even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.\nOur analysis highlights the effectiveness of the self-rewarding mechanism\ncompared to the external LLM-as-a-judge approach in MT and demonstrates its\ncomplementary benefits when combined with trained RMs. Our findings provide\nvaluable insight into the potential of self-improving RL methods. We have\npublicly released our code, data and models."}
{"id": "2505.16691", "pdf": "https://arxiv.org/pdf/2505.16691", "abs": "https://arxiv.org/abs/2505.16691", "authors": ["Advait Joglekar", "Divyanshu Singh", "Rooshil Rohit Bhatia", "S. Umesh"], "title": "EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Submitted to EMNLP 2025, 7 pages, 2 figures, 5 Tables", "summary": "Voice Conversion research in recent times has increasingly focused on\nimproving the zero-shot capabilities of existing methods. Despite remarkable\nadvancements, current architectures still tend to struggle in zero-shot\ncross-lingual settings. They are also often unable to generalize for speakers\nof unseen languages and accents. In this paper, we adopt a simple yet effective\napproach that combines discrete speech representations from self-supervised\nmodels with a non-autoregressive Diffusion-Transformer based conditional flow\nmatching speech decoder. We show that this architecture allows us to train a\nvoice-conversion model in a purely textless, self-supervised fashion. Our\ntechnique works without requiring multiple encoders to disentangle speech\nfeatures. Our model also manages to excel in zero-shot cross-lingual settings\neven for unseen languages."}
{"id": "2505.16644", "pdf": "https://arxiv.org/pdf/2505.16644", "abs": "https://arxiv.org/abs/2505.16644", "authors": ["Stephen Y. Zhang", "Michael P H Stumpf"], "title": "Learning non-equilibrium diffusions with Schrödinger bridges: from exactly solvable to simulation-free", "categories": ["stat.ML", "cs.LG", "math.OC", "62M45, 49N10"], "comment": "9 pages, 5 figures", "summary": "We consider the Schr\\\"odinger bridge problem which, given ensemble\nmeasurements of the initial and final configurations of a stochastic dynamical\nsystem and some prior knowledge on the dynamics, aims to reconstruct the \"most\nlikely\" evolution of the system compatible with the data. Most existing\nliterature assume Brownian reference dynamics and are implicitly limited to\npotential-driven dynamics. We depart from this regime and consider reference\nprocesses described by a multivariate Ornstein-Uhlenbeck process with generic\ndrift matrix $\\mathbf{A} \\in \\mathbb{R}^{d \\times d}$. When $\\mathbf{A}$ is\nasymmetric, this corresponds to a non-equilibrium system with non-conservative\nforces at play: this is important for applications to biological systems, which\nare naturally exist out-of-equilibrium. In the case of Gaussian marginals, we\nderive explicit expressions that characterise the solution of both the static\nand dynamic Schr\\\"odinger bridge. For general marginals, we propose mvOU-OTFM,\na simulation-free algorithm based on flow and score matching for learning the\nSchr\\\"odinger bridge. In application to a range of problems based on synthetic\nand real single cell data, we demonstrate that mvOU-OTFM achieves higher\naccuracy compared to competing methods, whilst being significantly faster to\ntrain."}
{"id": "2505.16694", "pdf": "https://arxiv.org/pdf/2505.16694", "abs": "https://arxiv.org/abs/2505.16694", "authors": ["Gouki Minegishi", "Hiroki Furuta", "Shohei Taniguchi", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Transformer-based language models exhibit In-Context Learning (ICL), where\npredictions are made adaptively based on context. While prior work links\ninduction heads to ICL through a sudden jump in accuracy, this can only account\nfor ICL when the answer is included within the context. However, an important\nproperty of practical ICL in large language models is the ability to meta-learn\nhow to solve tasks from context, rather than just copying answers from context;\nhow such an ability is obtained during training is largely unexplored. In this\npaper, we experimentally clarify how such meta-learning ability is acquired by\nanalyzing the dynamics of the model's circuit during training. Specifically, we\nextend the copy task from previous research into an In-Context Meta Learning\nsetting, where models must infer a task from examples to answer queries.\nInterestingly, in this setting, we find that there are multiple phases in the\nprocess of acquiring such abilities, and that a unique circuit emerges in each\nphase, contrasting with the single-phases change in induction heads. The\nemergence of such circuits can be related to several phenomena known in large\nlanguage models, and our analysis lead to a deeper understanding of the source\nof the transformer's ICL ability."}
{"id": "2505.16652", "pdf": "https://arxiv.org/pdf/2505.16652", "abs": "https://arxiv.org/abs/2505.16652", "authors": ["Feilong Tang", "Chengzhi Liu", "Zhongxing Xu", "Ming Hu", "Zelin Peng", "Zhiwei Yang", "Jionglong Su", "Minquan Lin", "Yifan Peng", "Xuelian Cheng", "Imran Razzak", "Zongyuan Ge"], "title": "Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding", "categories": ["cs.CV", "cs.LG"], "comment": "Clarification note for the CVPR 2025 paper (FarSight). Prepared by a\n  subset of the original authors; remaining co-authors are acknowledged in the\n  text", "summary": "Recent advancements in multimodal large language models (MLLMs) have\nsignificantly improved performance in visual question answering. However, they\noften suffer from hallucinations. In this work, hallucinations are categorized\ninto two main types: initial hallucinations and snowball hallucinations. We\nargue that adequate contextual information can be extracted directly from the\ntoken interaction process. Inspired by causal inference in the decoding\nstrategy, we propose to leverage causal masks to establish information\npropagation between multimodal tokens. The hypothesis is that insufficient\ninteraction between those tokens may lead the model to rely on outlier tokens,\noverlooking dense and rich contextual cues. Therefore, we propose to intervene\nin the propagation process by tackling outlier tokens to enhance in-context\ninference. With this goal, we present FarSight, a versatile plug-and-play\ndecoding strategy to reduce attention interference from outlier tokens merely\nby optimizing the causal mask. The heart of our method is effective token\npropagation. We design an attention register structure within the upper\ntriangular matrix of the causal mask, dynamically allocating attention to\ncapture attention diverted to outlier tokens. Moreover, a positional awareness\nencoding method with a diminishing masking rate is proposed, allowing the model\nto attend to further preceding tokens, especially for video sequence tasks.\nWith extensive experiments, FarSight demonstrates significant\nhallucination-mitigating performance across different MLLMs on both image and\nvideo benchmarks, proving its effectiveness."}
{"id": "2505.16705", "pdf": "https://arxiv.org/pdf/2505.16705", "abs": "https://arxiv.org/abs/2505.16705", "authors": ["Seonghwan Park", "Jueun Mun", "Donghyun Oh", "Namhoon Lee"], "title": "An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Concept bottleneck models (CBMs) ensure interpretability by decomposing\npredictions into human interpretable concepts. Yet the annotations used for\ntraining CBMs that enable this transparency are often noisy, and the impact of\nsuch corruption is not well understood. In this study, we present the first\nsystematic study of noise in CBMs and show that even moderate corruption\nsimultaneously impairs prediction performance, interpretability, and the\nintervention effectiveness. Our analysis identifies a susceptible subset of\nconcepts whose accuracy declines far more than the average gap between noisy\nand clean supervision and whose corruption accounts for most performance loss.\nTo mitigate this vulnerability we propose a two-stage framework. During\ntraining, sharpness-aware minimization stabilizes the learning of\nnoise-sensitive concepts. During inference, where clean labels are unavailable,\nwe rank concepts by predictive entropy and correct only the most uncertain\nones, using uncertainty as a proxy for susceptibility. Theoretical analysis and\nextensive ablations elucidate why sharpness-aware training confers robustness\nand why uncertainty reliably identifies susceptible concepts, providing a\nprincipled basis that preserves both interpretability and resilience in the\npresence of noise."}
{"id": "2505.16713", "pdf": "https://arxiv.org/pdf/2505.16713", "abs": "https://arxiv.org/abs/2505.16713", "authors": ["Shogo Nakakita"], "title": "Sharp concentration of uniform generalization errors in binary linear classification", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": "26 pages, 1 figure", "summary": "We examine the concentration of uniform generalization errors around their\nexpectation in binary linear classification problems via an isoperimetric\nargument. In particular, we establish Poincar\\'{e} and log-Sobolev inequalities\nfor the joint distribution of the output labels and the label-weighted input\nvectors, which we apply to derive concentration bounds. The derived\nconcentration bounds are sharp up to moderate multiplicative constants by those\nunder well-balanced labels. In asymptotic analysis, we also show that almost\nsure convergence of uniform generalization errors to their expectation occurs\nin very broad settings, such as proportionally high-dimensional regimes. Using\nthis convergence, we establish uniform laws of large numbers under\ndimension-free conditions."}
{"id": "2505.16710", "pdf": "https://arxiv.org/pdf/2505.16710", "abs": "https://arxiv.org/abs/2505.16710", "authors": ["Wenhao Li", "Yuxin Zhang", "Gen Luo", "Daohai Yu", "Rongrong Ji"], "title": "Training Long-Context LLMs Efficiently via Chunk-wise Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While long-context large language models (LLMs) exhibit remarkable document\nprocessing capabilities, their prohibitively high training costs often hinder\ncustomized applications. To mitigate this issue, we propose \\textit{Sequential\nChunk-wise Optimization} (SeCO), a memory-efficient training paradigm that\npartitions lengthy inputs into manageable chunks. Each chunk independently\nconstructs its computational graph and performs localized backpropagation,\nensuring that only one chunk's forward activations are stored in memory.\nBuilding on SeCO, we further introduce \\textit{Sparse Chunk-wise Optimization}\n(SpaCO), which reduces computational overhead by selectively propagating\ngradients to specific chunks and incorporates a carefully designed compensation\nfactor to ensure unbiased gradient estimation. SpaCO decouples the\ncomputational cost of backpropagation from the context length, enabling\ntraining time to gradually converge to inference time as sequences become\nlonger. Implemented as lightweight training wrappers, both SeCO and SpaCO offer\nsubstantial practical benefits. For example, when fine-tuning an 8B model with\nLoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to\n16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up\nto 3x faster than SeCO under the same experimental setup. These innovations\nprovide new insights into optimizing long-context models, making them more\naccessible for practical applications. We have open-sourced the code at\n\\href{https://github.com/wenhaoli-xmu/seco}{here}."}
{"id": "2505.16714", "pdf": "https://arxiv.org/pdf/2505.16714", "abs": "https://arxiv.org/abs/2505.16714", "authors": ["Hai-Feng Zhang", "Zhao-Yun Chen", "Peng Wang", "Liang-Liang Guo", "Tian-Le Wang", "Xiao-Yan Yang", "Ren-Ze Zhao", "Ze-An Zhao", "Sheng Zhang", "Lei Du", "Hao-Ran Tao", "Zhi-Long Jia", "Wei-Cheng Kong", "Huan-Yu Liu", "Athanasios V. Vasilakos", "Yang Yang", "Yu-Chun Wu", "Ji Guan", "Peng Duan", "Guo-Ping Guo"], "title": "Experimental robustness benchmark of quantum neural network on a superconducting quantum processor", "categories": ["quant-ph", "cs.LG"], "comment": "There are 8 pages with 5 figures in the main text and 15 pages with\n  14 figures in the supplementary information", "summary": "Quantum machine learning (QML) models, like their classical counterparts, are\nvulnerable to adversarial attacks, hindering their secure deployment. Here, we\nreport the first systematic experimental robustness benchmark for 20-qubit\nquantum neural network (QNN) classifiers executed on a superconducting\nprocessor. Our benchmarking framework features an efficient adversarial attack\nalgorithm designed for QNNs, enabling quantitative characterization of\nadversarial robustness and robustness bounds. From our analysis, we verify that\nadversarial training reduces sensitivity to targeted perturbations by\nregularizing input gradients, significantly enhancing QNN's robustness.\nAdditionally, our analysis reveals that QNNs exhibit superior adversarial\nrobustness compared to classical neural networks, an advantage attributed to\ninherent quantum noise. Furthermore, the empirical upper bound extracted from\nour attack experiments shows a minimal deviation ($3 \\times 10^{-3}$) from the\ntheoretical lower bound, providing strong experimental confirmation of the\nattack's effectiveness and the tightness of fidelity-based robustness bounds.\nThis work establishes a critical experimental framework for assessing and\nimproving quantum adversarial robustness, paving the way for secure and\nreliable QML applications."}
{"id": "2505.16722", "pdf": "https://arxiv.org/pdf/2505.16722", "abs": "https://arxiv.org/abs/2505.16722", "authors": ["Himanshu Beniwal", "Youngwoo Kim", "Maarten Sap", "Soham Dan", "Thomas Hartvigsen"], "title": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 504 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad."}
{"id": "2505.16716", "pdf": "https://arxiv.org/pdf/2505.16716", "abs": "https://arxiv.org/abs/2505.16716", "authors": ["Moritz Stargalla", "Christoph Hertrich", "Daniel Reichman"], "title": "The Computational Complexity of Counting Linear Regions in ReLU Neural Networks", "categories": ["cs.CC", "cs.DM", "cs.LG", "cs.NE", "math.CO"], "comment": "25 pages", "summary": "An established measure of the expressive power of a given ReLU neural network\nis the number of linear regions into which it partitions the input space. There\nexist many different, non-equivalent definitions of what a linear region\nactually is. We systematically assess which papers use which definitions and\ndiscuss how they relate to each other. We then analyze the computational\ncomplexity of counting the number of such regions for the various definitions.\nGenerally, this turns out to be an intractable problem. We prove NP- and\n#P-hardness results already for networks with one hidden layer and strong\nhardness of approximation results for two or more hidden layers. Finally, on\nthe algorithmic side, we demonstrate that counting linear regions can at least\nbe achieved in polynomial space for some common definitions."}
{"id": "2505.16724", "pdf": "https://arxiv.org/pdf/2505.16724", "abs": "https://arxiv.org/abs/2505.16724", "authors": ["Konstantinos Barmpas", "Na Lee", "Yannis Panagakis", "Dimitrios A. Adamos", "Nikolaos Laskaris", "Stefanos Zafeiriou"], "title": "Advancing Brainwave Modeling with a Codebook-Based Foundation Model", "categories": ["cs.LG", "cs.AI", "cs.HC"], "comment": null, "summary": "Recent advances in large-scale pre-trained Electroencephalogram (EEG) models\nhave shown great promise, driving progress in Brain-Computer Interfaces (BCIs)\nand healthcare applications. However, despite their success, many existing\npre-trained models have struggled to fully capture the rich information content\nof neural oscillations, a limitation that fundamentally constrains their\nperformance and generalizability across diverse BCI tasks. This limitation is\nfrequently rooted in suboptimal architectural design choices which constrain\ntheir representational capacity. In this work, we introduce LaBraM++, an\nenhanced Large Brainwave Foundation Model (LBM) that incorporates principled\nimprovements grounded in robust signal processing foundations. LaBraM++\ndemonstrates substantial gains across a variety of tasks, consistently\noutperforming its originally-based architecture and achieving competitive\nresults when compared to other open-source LBMs. Its superior performance and\ntraining efficiency highlight its potential as a strong foundation for future\nadvancements in LBMs."}
{"id": "2505.16723", "pdf": "https://arxiv.org/pdf/2505.16723", "abs": "https://arxiv.org/abs/2505.16723", "authors": ["Thibaud Gloaguen", "Robin Staab", "Nikola Jovanović", "Martin Vechev"], "title": "Robust LLM Fingerprinting via Domain-Specific Watermarks", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "As open-source language models (OSMs) grow more capable and are widely shared\nand finetuned, ensuring model provenance, i.e., identifying the origin of a\ngiven model instance, has become an increasingly important issue. At the same\ntime, existing backdoor-based model fingerprinting techniques often fall short\nof achieving key requirements of real-world model ownership detection. In this\nwork, we build on the observation that while current open-source model\nwatermarks fail to achieve reliable content traceability, they can be\neffectively adapted to address the challenge of model provenance. To this end,\nwe introduce the concept of domain-specific watermarking for model\nfingerprinting. Rather than watermarking all generated content, we train the\nmodel to embed watermarks only within specified subdomains (e.g., particular\nlanguages or topics). This targeted approach ensures detection reliability,\nwhile improving watermark durability and quality under a range of real-world\ndeployment settings. Our evaluations show that domain-specific watermarking\nenables model fingerprinting with strong statistical guarantees, controllable\nfalse positive rates, high detection power, and preserved generation quality.\nMoreover, we find that our fingerprints are inherently stealthy and naturally\nrobust to real-world variability across deployment scenarios."}
{"id": "2505.16732", "pdf": "https://arxiv.org/pdf/2505.16732", "abs": "https://arxiv.org/abs/2505.16732", "authors": ["Hany Abdulsamad", "Sahel Iqbal", "Simo Särkkä"], "title": "Sequential Monte Carlo for Policy Optimization in Continuous POMDPs", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Optimal decision-making under partial observability requires agents to\nbalance reducing uncertainty (exploration) against pursuing immediate\nobjectives (exploitation). In this paper, we introduce a novel policy\noptimization framework for continuous partially observable Markov decision\nprocesses (POMDPs) that explicitly addresses this challenge. Our method casts\npolicy learning as probabilistic inference in a non-Markovian Feynman--Kac\nmodel that inherently captures the value of information gathering by\nanticipating future observations, without requiring extrinsic exploration\nbonuses or handcrafted heuristics. To optimize policies under this model, we\ndevelop a nested sequential Monte Carlo~(SMC) algorithm that efficiently\nestimates a history-dependent policy gradient under samples from the optimal\ntrajectory distribution induced by the POMDP. We demonstrate the effectiveness\nof our algorithm across standard continuous POMDP benchmarks, where existing\nmethods struggle to act under uncertainty."}
{"id": "2505.16743", "pdf": "https://arxiv.org/pdf/2505.16743", "abs": "https://arxiv.org/abs/2505.16743", "authors": ["Florentin Beck", "William Rudman", "Carsten Eickhoff"], "title": "TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; F.2.2"], "comment": null, "summary": "Large Language Models (LLMs) present significant computational and memory\nchallenges due to their extensive size, making pruning essential for their\nefficient deployment. Existing one-shot pruning methods often apply uniform\nsparsity constraints across layers or within each layer, resulting in\nsuboptimal performance, especially at high sparsity ratios. This work\nintroduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel\napproach that applies varying sparsity ratios to individual output dimensions\n(rows) within each layer. TRIM employs an iterative adjustment process guided\nby quality metrics to optimize dimension-wise sparsity allocation, focusing on\nreducing variance in quality retention across outputs to preserve critical\ninformation. TRIM can be seamlessly integrated with existing layer-wise pruning\nstrategies. Our evaluations on perplexity and zero-shot tasks across diverse\nLLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that\nTRIM achieves new state-of-the-art results and enhances stability. For\ninstance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and\nover 90% for OPT-13B compared to baseline methods. We conclude that\nfine-grained, dimension-wise sparsity adaptation is crucial for pushing the\nlimits of extreme LLM compression. Code available at:\nhttps://github.com/flobk/TRIM"}
{"id": "2505.16735", "pdf": "https://arxiv.org/pdf/2505.16735", "abs": "https://arxiv.org/abs/2505.16735", "authors": ["Youngmoon Jung", "Yong-Hyeok Lee", "Myunghun Jung", "Jaeyoung Roh", "Chang Woo Han", "Hoon-Young Cho"], "title": "Adversarial Deep Metric Learning for Cross-Modal Audio-Text Alignment in Open-Vocabulary Keyword Spotting", "categories": ["eess.AS", "cs.AI"], "comment": "5 pages, 1 figures, Accepted at Interspeech 2025", "summary": "For text enrollment-based open-vocabulary keyword spotting (KWS), acoustic\nand text embeddings are typically compared at either the phoneme or utterance\nlevel. To facilitate this, we optimize acoustic and text encoders using deep\nmetric learning (DML), enabling direct comparison of multi-modal embeddings in\na shared embedding space. However, the inherent heterogeneity between audio and\ntext modalities presents a significant challenge. To address this, we propose\nModality Adversarial Learning (MAL), which reduces the domain gap in\nheterogeneous modality representations. Specifically, we train a modality\nclassifier adversarially to encourage both encoders to generate\nmodality-invariant embeddings. Additionally, we apply DML to achieve\nphoneme-level alignment between audio and text, and conduct comprehensive\ncomparisons across various DML objectives. Experiments on the Wall Street\nJournal (WSJ) and LibriPhrase datasets demonstrate the effectiveness of the\nproposed approach."}
{"id": "2505.16789", "pdf": "https://arxiv.org/pdf/2505.16789", "abs": "https://arxiv.org/abs/2505.16789", "authors": ["Punya Syon Pandey", "Samuel Simko", "Kellin Pelrine", "Zhijing Jin"], "title": "Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As large language models gain popularity, their vulnerability to adversarial\nattacks remains a primary concern. While fine-tuning models on domain-specific\ndatasets is often employed to improve model performance, it can introduce\nvulnerabilities within the underlying model. In this work, we investigate\nAccidental Misalignment, unexpected vulnerabilities arising from\ncharacteristics of fine-tuning data. We begin by identifying potential\ncorrelation factors such as linguistic features, semantic similarity, and\ntoxicity within our experimental datasets. We then evaluate the adversarial\nperformance of these fine-tuned models and assess how dataset factors correlate\nwith attack success rates. Lastly, we explore potential causal links, offering\nnew insights into adversarial defense strategies and highlighting the crucial\nrole of dataset design in preserving model alignment. Our code is available at\nhttps://github.com/psyonp/accidental_misalignment."}
{"id": "2505.16737", "pdf": "https://arxiv.org/pdf/2505.16737", "abs": "https://arxiv.org/abs/2505.16737", "authors": ["Chengcan Wu", "Zhixin Zhang", "Zeming Wei", "Yihao Zhang", "Meng Sun"], "title": "Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "math.OC"], "comment": null, "summary": "The significant progress of large language models (LLMs) has led to\nremarkable achievements across numerous applications. However, their ability to\ngenerate harmful content has sparked substantial safety concerns. Despite the\nimplementation of safety alignment techniques during the pre-training phase,\nrecent research indicates that fine-tuning LLMs on adversarial or even benign\ndata can inadvertently compromise their safety. In this paper, we re-examine\nthe fundamental issue of why fine-tuning on non-harmful data still results in\nsafety degradation. We introduce a safety-aware probing (SAP) optimization\nframework designed to mitigate the safety risks of fine-tuning LLMs.\nSpecifically, SAP incorporates a safety-aware probe into the gradient\npropagation process, mitigating the model's risk of safety degradation by\nidentifying potential pitfalls in gradient directions, thereby enhancing\ntask-specific performance while successfully preserving model safety. Our\nextensive experimental results demonstrate that SAP effectively reduces\nharmfulness below the original fine-tuned model and achieves comparable test\nloss to standard fine-tuning methods. Our code is available at\nhttps://github.com/ChengcanWu/SAP."}
{"id": "2505.16821", "pdf": "https://arxiv.org/pdf/2505.16821", "abs": "https://arxiv.org/abs/2505.16821", "authors": ["Ziming liu", "Bryan Liu", "Alvaro Valcarce", "Xiaoli Chu"], "title": "LLM-Based Emulation of the Radio Resource Control Layer: Towards AI-Native RAN Protocols", "categories": ["cs.NI", "cs.LG", "eess.SP"], "comment": "This work has been submitted to the IEEE for possible publication.\n  Focuses on applying LLMs to 5G RRC protocol generation; primary: cs.NI;\n  cross-list: eess.SP, cs.LG", "summary": "Integrating large AI models (LAMs) into 6G mobile networks promises to\nredefine protocol design and control-plane intelligence by enabling autonomous,\ncognitive network operations. While industry concepts, such as ETSI's\nExperiential Networked Intelligence (ENI), envision LAM-driven agents for\nadaptive network slicing and intent-based management, practical implementations\nstill face challenges in protocol literacy and real-world deployment. This\npaper presents an end-to-end demonstration of a LAM that generates\nstandards-compliant, ASN.1-encoded Radio Resource Control (RRC) messages as\npart of control-plane procedures inside a gNB. We treat RRC messaging as a\ndomain-specific language and fine-tune a decoder-only transformer model (LLaMA\nclass) using parameter-efficient Low-Rank Adaptation (LoRA) on RRC messages\nlinearized to retain their ASN.1 syntactic structure before standard byte-pair\nencoding tokenization. This enables combinatorial generalization over RRC\nprotocol states while minimizing training overhead. On 30k field-test\nrequest-response pairs, our 8 B model achieves a median cosine similarity of\n0.97 with ground-truth messages on an edge GPU -- a 61 % relative gain over a\nzero-shot LLaMA-3 8B baseline -- indicating substantially improved structural\nand semantic RRC fidelity. Overall, our results show that LAMs, when augmented\nwith Radio Access Network (RAN)-specific reasoning, can directly orchestrate\ncontrol-plane procedures, representing a stepping stone toward the AI-native\nair-interface paradigm. Beyond RRC emulation, this work lays the groundwork for\nfuture AI-native wireless standards."}
{"id": "2505.16740", "pdf": "https://arxiv.org/pdf/2505.16740", "abs": "https://arxiv.org/abs/2505.16740", "authors": ["Alya Zouzou", "Léo andéol", "Mélanie Ducoffe", "Ryma Boumazouza"], "title": "Robust Vision-Based Runway Detection through Conformal Prediction and Conformal mAP", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We explore the use of conformal prediction to provide statistical uncertainty\nguarantees for runway detection in vision-based landing systems (VLS). Using\nfine-tuned YOLOv5 and YOLOv6 models on aerial imagery, we apply conformal\nprediction to quantify localization reliability under user-defined risk levels.\nWe also introduce Conformal mean Average Precision (C-mAP), a novel metric\naligning object detection performance with conformal guarantees. Our results\nshow that conformal prediction can improve the reliability of runway detection\nby quantifying uncertainty in a statistically sound way, increasing safety\non-board and paving the way for certification of ML system in the aerospace\ndomain."}
{"id": "2505.16831", "pdf": "https://arxiv.org/pdf/2505.16831", "abs": "https://arxiv.org/abs/2505.16831", "authors": ["Xiaoyu Xu", "Xiang Yue", "Yang Liu", "Qingqing Ye", "Haibo Hu", "Minxin Du"], "title": "Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "44 pages", "summary": "Unlearning in large language models (LLMs) is intended to remove the\ninfluence of specific data, yet current evaluations rely heavily on token-level\nmetrics such as accuracy and perplexity. We show that these metrics can be\nmisleading: models often appear to forget, but their original behavior can be\nrapidly restored with minimal fine-tuning, revealing that unlearning may\nobscure information rather than erase it. To diagnose this phenomenon, we\nintroduce a representation-level evaluation framework using PCA-based\nsimilarity and shift, centered kernel alignment, and Fisher information.\nApplying this toolkit across six unlearning methods, three domains (text, code,\nmath), and two open-source LLMs, we uncover a critical distinction between\nreversible and irreversible forgetting. In reversible cases, models suffer\ntoken-level collapse yet retain latent features; in irreversible cases, deeper\nrepresentational damage occurs. We further provide a theoretical account\nlinking shallow weight perturbations near output layers to misleading\nunlearning signals, and show that reversibility is modulated by task type and\nhyperparameters. Our findings reveal a fundamental gap in current evaluation\npractices and establish a new diagnostic foundation for trustworthy unlearning\nin LLMs. We provide a unified toolkit for analyzing LLM representation changes\nunder unlearning and relearning:\nhttps://github.com/XiaoyuXU1/Representational_Analysis_Tools.git."}
{"id": "2505.16743", "pdf": "https://arxiv.org/pdf/2505.16743", "abs": "https://arxiv.org/abs/2505.16743", "authors": ["Florentin Beck", "William Rudman", "Carsten Eickhoff"], "title": "TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; F.2.2"], "comment": null, "summary": "Large Language Models (LLMs) present significant computational and memory\nchallenges due to their extensive size, making pruning essential for their\nefficient deployment. Existing one-shot pruning methods often apply uniform\nsparsity constraints across layers or within each layer, resulting in\nsuboptimal performance, especially at high sparsity ratios. This work\nintroduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel\napproach that applies varying sparsity ratios to individual output dimensions\n(rows) within each layer. TRIM employs an iterative adjustment process guided\nby quality metrics to optimize dimension-wise sparsity allocation, focusing on\nreducing variance in quality retention across outputs to preserve critical\ninformation. TRIM can be seamlessly integrated with existing layer-wise pruning\nstrategies. Our evaluations on perplexity and zero-shot tasks across diverse\nLLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that\nTRIM achieves new state-of-the-art results and enhances stability. For\ninstance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and\nover 90% for OPT-13B compared to baseline methods. We conclude that\nfine-grained, dimension-wise sparsity adaptation is crucial for pushing the\nlimits of extreme LLM compression. Code available at:\nhttps://github.com/flobk/TRIM"}
{"id": "2505.16832", "pdf": "https://arxiv.org/pdf/2505.16832", "abs": "https://arxiv.org/abs/2505.16832", "authors": ["Haonian Ji", "Shi Qiu", "Siyang Xin", "Siwei Han", "Zhaorun Chen", "Hongyi Wang", "Dake Zhang", "Huaxiu Yao"], "title": "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "16 pages; 7 figures", "summary": "While foundation models (FMs), such as diffusion models and large\nvision-language models (LVLMs), have been widely applied in educational\ncontexts, their ability to generate pedagogically effective visual explanations\nremains limited. Most existing approaches focus primarily on textual reasoning,\noverlooking the critical role of structured and interpretable visualizations in\nsupporting conceptual understanding. To better assess the visual reasoning\ncapabilities of FMs in educational settings, we introduce EduVisBench, a\nmulti-domain, multi-level benchmark. EduVisBench features diverse STEM problem\nsets requiring visually grounded solutions, along with a fine-grained\nevaluation rubric informed by pedagogical theory. Our empirical analysis\nreveals that existing models frequently struggle with the inherent challenge of\ndecomposing complex reasoning and translating it into visual representations\naligned with human cognitive processes. To address these limitations, we\npropose EduVisAgent, a multi-agent collaborative framework that coordinates\nspecialized agents for instructional planning, reasoning decomposition,\nmetacognitive prompting, and visualization design. Experimental results show\nthat EduVisAgent substantially outperforms all baselines, achieving a 40.2%\nimprovement and delivering more educationally aligned visualizations.\nEduVisBench and EduVisAgent are available at\nhttps://github.com/aiming-lab/EduVisBench and\nhttps://github.com/aiming-lab/EduVisAgent."}
{"id": "2505.16752", "pdf": "https://arxiv.org/pdf/2505.16752", "abs": "https://arxiv.org/abs/2505.16752", "authors": ["Hao Guo", "Erpeng Xue", "Lei Huang", "Shichao Wang", "Xiaolei Wang", "Lei Wang", "Jinpeng Wang", "Sheng Chen"], "title": "Action is All You Need: Dual-Flow Generative Ranking Network for Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "We introduce the Dual-Flow Generative Ranking Network (DFGR), a two-stream\narchitecture designed for recommendation systems. DFGR integrates innovative\ninteraction patterns between real and fake flows within the QKV modules of the\nself-attention mechanism, enhancing both training and inference efficiency.\nThis approach effectively addresses a key limitation observed in Meta's\nproposed HSTU generative recommendation approach, where heterogeneous\ninformation volumes are mapped into identical vector spaces, leading to\ntraining instability. Unlike traditional recommendation models, DFGR only\nrelies on user history behavior sequences and minimal attribute information,\neliminating the need for extensive manual feature engineering. Comprehensive\nevaluations on open-source and industrial datasets reveal DFGR's superior\nperformance compared to established baselines such as DIN, DCN, DIEN, and\nDeepFM. We also investigate optimal parameter allocation strategies under\ncomputational constraints, establishing DFGR as an efficient and effective\nnext-generation generate ranking paradigm."}
{"id": "2505.16875", "pdf": "https://arxiv.org/pdf/2505.16875", "abs": "https://arxiv.org/abs/2505.16875", "authors": ["Zhehao Huang", "Yuhang Liu", "Yixin Lou", "Zhengbao He", "Mingzhen He", "Wenxing Zhou", "Tao Li", "Kehan Li", "Zeyi Huang", "Xiaolin Huang"], "title": "T2I-ConBench: Text-to-Image Benchmark for Continual Post-training", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Continual post-training adapts a single text-to-image diffusion model to\nlearn new tasks without incurring the cost of separate models, but naive\npost-training causes forgetting of pretrained knowledge and undermines\nzero-shot compositionality. We observe that the absence of a standardized\nevaluation protocol hampers related research for continual post-training. To\naddress this, we introduce T2I-ConBench, a unified benchmark for continual\npost-training of text-to-image models. T2I-ConBench focuses on two practical\nscenarios, item customization and domain enhancement, and analyzes four\ndimensions: (1) retention of generality, (2) target-task performance, (3)\ncatastrophic forgetting, and (4) cross-task generalization. It combines\nautomated metrics, human-preference modeling, and vision-language QA for\ncomprehensive assessment. We benchmark ten representative methods across three\nrealistic task sequences and find that no approach excels on all fronts. Even\njoint \"oracle\" training does not succeed for every task, and cross-task\ngeneralization remains unsolved. We release all datasets, code, and evaluation\ntools to accelerate research in continual post-training for text-to-image\nmodels."}
{"id": "2505.16765", "pdf": "https://arxiv.org/pdf/2505.16765", "abs": "https://arxiv.org/abs/2505.16765", "authors": ["Jianing Geng", "Biao Yi", "Zekun Fei", "Tongxi Wu", "Lihai Nie", "Zheli Liu"], "title": "When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak Attack on LLMs via Steganographic Techniques", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Jailbreak attacks pose a serious threat to large language models (LLMs) by\nbypassing built-in safety mechanisms and leading to harmful outputs. Studying\nthese attacks is crucial for identifying vulnerabilities and improving model\nsecurity. This paper presents a systematic survey of jailbreak methods from the\nnovel perspective of stealth. We find that existing attacks struggle to\nsimultaneously achieve toxic stealth (concealing toxic content) and linguistic\nstealth (maintaining linguistic naturalness). Motivated by this, we propose\nStegoAttack, a fully stealthy jailbreak attack that uses steganography to hide\nthe harmful query within benign, semantically coherent text. The attack then\nprompts the LLM to extract the hidden query and respond in an encrypted manner.\nThis approach effectively hides malicious intent while preserving naturalness,\nallowing it to evade both built-in and external safety mechanisms. We evaluate\nStegoAttack on four safety-aligned LLMs from major providers, benchmarking\nagainst eight state-of-the-art methods. StegoAttack achieves an average attack\nsuccess rate (ASR) of 92.00%, outperforming the strongest baseline by 11.0%.\nIts ASR drops by less than 1% even under external detection (e.g., Llama\nGuard). Moreover, it attains the optimal comprehensive scores on stealth\ndetection metrics, demonstrating both high efficacy and exceptional stealth\ncapabilities. The code is available at\nhttps://anonymous.4open.science/r/StegoAttack-Jail66"}
{"id": "2505.16879", "pdf": "https://arxiv.org/pdf/2505.16879", "abs": "https://arxiv.org/abs/2505.16879", "authors": ["Hannah Sansford", "Nick Whiteley", "Patrick Rubin-Delanchy"], "title": "How high is `high'? Rethinking the roles of dimensionality in topological data analysis and manifold learning", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We present a generalised Hanson-Wright inequality and use it to establish new\nstatistical insights into the geometry of data point-clouds. In the setting of\na general random function model of data, we clarify the roles played by three\nnotions of dimensionality: ambient intrinsic dimension $p_{\\mathrm{int}}$,\nwhich measures total variability across orthogonal feature directions;\ncorrelation rank, which measures functional complexity across samples; and\nlatent intrinsic dimension, which is the dimension of manifold structure hidden\nin data. Our analysis shows that in order for persistence diagrams to reveal\nlatent homology and for manifold structure to emerge it is sufficient that\n$p_{\\mathrm{int}}\\gg \\log n$, where $n$ is the sample size. Informed by these\ntheoretical perspectives, we revisit the ground-breaking neuroscience discovery\nof toroidal structure in grid-cell activity made by Gardner et al. (Nature,\n2022): our findings reveal, for the first time, evidence that this structure is\nin fact isometric to physical space, meaning that grid cell activity conveys a\ngeometrically faithful representation of the real world."}
{"id": "2505.16773", "pdf": "https://arxiv.org/pdf/2505.16773", "abs": "https://arxiv.org/abs/2505.16773", "authors": ["Iván Matas", "Carmen Serrano", "Miguel Nogales", "David Moreno", "Lara Ferrándiz", "Teresa Ojeda", "Begoña Acha"], "title": "Mitigating Overfitting in Medical Imaging: Self-Supervised Pretraining vs. ImageNet Transfer Learning for Dermatological Diagnosis", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 2 tables, 2 figures", "summary": "Deep learning has transformed computer vision but relies heavily on large\nlabeled datasets and computational resources. Transfer learning, particularly\nfine-tuning pretrained models, offers a practical alternative; however, models\npretrained on natural image datasets such as ImageNet may fail to capture\ndomain-specific characteristics in medical imaging. This study introduces an\nunsupervised learning framework that extracts high-value dermatological\nfeatures instead of relying solely on ImageNet-based pretraining. We employ a\nVariational Autoencoder (VAE) trained from scratch on a proprietary\ndermatological dataset, allowing the model to learn a structured and clinically\nrelevant latent space. This self-supervised feature extractor is then compared\nto an ImageNet-pretrained backbone under identical classification conditions,\nhighlighting the trade-offs between general-purpose and domain-specific\npretraining. Our results reveal distinct learning patterns. The self-supervised\nmodel achieves a final validation loss of 0.110 (-33.33%), while the\nImageNet-pretrained model stagnates at 0.100 (-16.67%), indicating overfitting.\nAccuracy trends confirm this: the self-supervised model improves from 45% to\n65% (+44.44%) with a near-zero overfitting gap, whereas the ImageNet-pretrained\nmodel reaches 87% (+50.00%) but plateaus at 75% (+19.05%), with its overfitting\ngap increasing to +0.060. These findings suggest that while ImageNet\npretraining accelerates convergence, it also amplifies overfitting on\nnon-clinically relevant features. In contrast, self-supervised learning\nachieves steady improvements, stronger generalization, and superior\nadaptability, underscoring the importance of domain-specific feature extraction\nin medical imaging."}
{"id": "2505.16886", "pdf": "https://arxiv.org/pdf/2505.16886", "abs": "https://arxiv.org/abs/2505.16886", "authors": ["Nour Jedidi", "Yung-Sung Chuang", "James Glass", "Jimmy Lin"], "title": "Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers."}
{"id": "2505.16785", "pdf": "https://arxiv.org/pdf/2505.16785", "abs": "https://arxiv.org/abs/2505.16785", "authors": ["Zhenzhen Ren", "GuoBiao Li", "Sheng Li", "Zhenxing Qian", "Xinpeng Zhang"], "title": "CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Despite providing superior performance, open-source large language models\n(LLMs) are vulnerable to abusive usage. To address this issue, recent works\npropose LLM fingerprinting methods to identify the specific source LLMs behind\nsuspect applications. However, these methods fail to provide stealthy and\nrobust fingerprint verification. In this paper, we propose a novel LLM\nfingerprinting scheme, namely CoTSRF, which utilizes the Chain of Thought (CoT)\nas the fingerprint of an LLM. CoTSRF first collects the responses from the\nsource LLM by querying it with crafted CoT queries. Then, it applies\ncontrastive learning to train a CoT extractor that extracts the CoT feature\n(i.e., fingerprint) from the responses. Finally, CoTSRF conducts fingerprint\nverification by comparing the Kullback-Leibler divergence between the CoT\nfeatures of the source and suspect LLMs against an empirical threshold. Various\nexperiments have been conducted to demonstrate the advantage of our proposed\nCoTSRF for fingerprinting LLMs, particularly in stealthy and robust fingerprint\nverification."}
{"id": "2505.16893", "pdf": "https://arxiv.org/pdf/2505.16893", "abs": "https://arxiv.org/abs/2505.16893", "authors": ["Shuichi Nishino", "Tomohiro Shiraishi", "Teruyuki Katsuoka", "Ichiro Takeuchi"], "title": "Statistical Test for Saliency Maps of Graph Neural Networks via Selective Inference", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have gained prominence for their ability to\nprocess graph-structured data across various domains. However, interpreting GNN\ndecisions remains a significant challenge, leading to the adoption of saliency\nmaps for identifying influential nodes and edges. Despite their utility, the\nreliability of GNN saliency maps has been questioned, particularly in terms of\ntheir robustness to noise. In this study, we propose a statistical testing\nframework to rigorously evaluate the significance of saliency maps. Our main\ncontribution lies in addressing the inflation of the Type I error rate caused\nby double-dipping of data, leveraging the framework of Selective Inference. Our\nmethod provides statistically valid $p$-values while controlling the Type I\nerror rate, ensuring that identified salient subgraphs contain meaningful\ninformation rather than random artifacts. To demonstrate the effectiveness of\nour method, we conduct experiments on both synthetic and real-world datasets,\nshowing its effectiveness in assessing the reliability of GNN interpretations."}
{"id": "2505.16789", "pdf": "https://arxiv.org/pdf/2505.16789", "abs": "https://arxiv.org/abs/2505.16789", "authors": ["Punya Syon Pandey", "Samuel Simko", "Kellin Pelrine", "Zhijing Jin"], "title": "Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As large language models gain popularity, their vulnerability to adversarial\nattacks remains a primary concern. While fine-tuning models on domain-specific\ndatasets is often employed to improve model performance, it can introduce\nvulnerabilities within the underlying model. In this work, we investigate\nAccidental Misalignment, unexpected vulnerabilities arising from\ncharacteristics of fine-tuning data. We begin by identifying potential\ncorrelation factors such as linguistic features, semantic similarity, and\ntoxicity within our experimental datasets. We then evaluate the adversarial\nperformance of these fine-tuned models and assess how dataset factors correlate\nwith attack success rates. Lastly, we explore potential causal links, offering\nnew insights into adversarial defense strategies and highlighting the crucial\nrole of dataset design in preserving model alignment. Our code is available at\nhttps://github.com/psyonp/accidental_misalignment."}
{"id": "2505.16900", "pdf": "https://arxiv.org/pdf/2505.16900", "abs": "https://arxiv.org/abs/2505.16900", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "Beiwen Zhang", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "title": "Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "During the finetuning stage of text generation tasks, standard cross-entropy\nloss treats all tokens equally. This can lead models to overemphasize\nhigh-frequency, low-information tokens, neglecting lower-frequency tokens\ncrucial for specificity and informativeness in generated content. This paper\nintroduces a novel loss function, Power-Law Decay Loss (PDL), specifically\ndesigned to optimize the finetuning process for text generation. The core\nmotivation for PDL stems from observations in information theory and\nlinguistics: the informativeness of a token is often inversely proportional to\nits frequency of occurrence. PDL re-weights the contribution of each token in\nthe standard cross-entropy loss based on its frequency in the training corpus,\nfollowing a power-law decay. Specifically, the weights for high-frequency\ntokens are reduced, while low-frequency, information-dense tokens are assigned\nhigher weights. This mechanism guides the model during finetuning to focus more\non learning and generating tokens that convey specific and unique information,\nthereby enhancing the quality, diversity, and informativeness of the generated\ntext. We theoretically elaborate on the motivation and construction of PDL and\ndiscuss its potential applications and advantages across various text\ngeneration finetuning tasks, such as abstractive summarization, dialogue\nsystems, and style transfer."}
{"id": "2505.16790", "pdf": "https://arxiv.org/pdf/2505.16790", "abs": "https://arxiv.org/abs/2505.16790", "authors": ["Hyunjin Seo", "Taewon Kim", "Sihyun Yu", "SungSoo Ahn"], "title": "Learning Flexible Forward Trajectories for Masked Molecular Diffusion", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Masked diffusion models (MDMs) have achieved notable progress in modeling\ndiscrete data, while their potential in molecular generation remains\nunderexplored. In this work, we explore their potential and introduce the\nsurprising result that naively applying standards MDMs severely degrades the\nperformance. We identify the critical cause of this issue as a state-clashing\nproblem-where the forward diffusion of distinct molecules collapse into a\ncommon state, resulting in a mixture of reconstruction targets that cannot be\nlearned using typical reverse diffusion process with unimodal predictions. To\nmitigate this, we propose Masked Element-wise Learnable Diffusion (MELD) that\norchestrates per-element corruption trajectories to avoid collision between\ndistinct molecular graphs. This is achieved through a parameterized noise\nscheduling network that assigns distinct corruption rates to individual graph\nelements, i.e., atoms and bonds. Extensive experiments on diverse molecular\nbenchmarks reveal that MELD markedly enhances overall generation quality\ncompared to element-agnostic noise scheduling, increasing the chemical validity\nof vanilla MDMs on ZINC250K from 15% to 93%, Furthermore, it achieves\nstate-of-the-art property alignment in conditional generation tasks."}
{"id": "2505.16901", "pdf": "https://arxiv.org/pdf/2505.16901", "abs": "https://arxiv.org/abs/2505.16901", "authors": ["Hongyuan Tao", "Ying Zhang", "Zhenhao Tang", "Hongen Peng", "Xukun Zhu", "Bingchang Liu", "Yingguang Yang", "Ziyin Zhang", "Zhaogui Xu", "Haipeng Zhang", "Linchao Zhu", "Rui Wang", "Hang Yu", "Jianguo Li", "Peng Di"], "title": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks", "categories": ["cs.SE", "cs.LG"], "comment": "31 pages, 9 figures", "summary": "Recent advances in Large Language Models (LLMs) have shown promise in\nfunction-level code generation, yet repository-level software engineering tasks\nremain challenging. Current solutions predominantly rely on proprietary LLM\nagents, which introduce unpredictability and limit accessibility, raising\nconcerns about data privacy and model customization. This paper investigates\nwhether open-source LLMs can effectively address repository-level tasks without\nrequiring agent-based approaches. We demonstrate this is possible by enabling\nLLMs to comprehend functions and files within codebases through their semantic\ninformation and structural dependencies. To this end, we introduce Code Graph\nModels (CGMs), which integrate repository code graph structures into the LLM's\nattention mechanism and map node attributes to the LLM's input space using a\nspecialized adapter. When combined with an agentless graph RAG framework, our\napproach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark\nusing the open-source Qwen2.5-72B model. This performance ranks first among\nopen weight models, second among methods with open-source systems, and eighth\noverall, surpassing the previous best open-source model-based method by 12.33%."}
{"id": "2505.16791", "pdf": "https://arxiv.org/pdf/2505.16791", "abs": "https://arxiv.org/abs/2505.16791", "authors": ["Tillmann Rheude", "Roland Eils", "Benjamin Wild"], "title": "Cohort-Based Active Modality Acquisition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Real-world machine learning applications often involve data from multiple\nmodalities that must be integrated effectively to make robust predictions.\nHowever, in many practical settings, not all modalities are available for every\nsample, and acquiring additional modalities can be costly. This raises the\nquestion: which samples should be prioritized for additional modality\nacquisition when resources are limited? While prior work has explored\nindividual-level acquisition strategies and training-time active learning\nparadigms, test-time and cohort-based acquisition remain underexplored despite\ntheir importance in many real-world settings. We introduce Cohort-based Active\nModality Acquisition (CAMA), a novel test-time setting to formalize the\nchallenge of selecting which samples should receive additional modalities. We\nderive acquisition strategies that leverage a combination of generative\nimputation and discriminative modeling to estimate the expected benefit of\nacquiring missing modalities based on common evaluation metrics. We also\nintroduce upper-bound heuristics that provide performance ceilings to benchmark\nacquisition strategies. Experiments on common multimodal datasets demonstrate\nthat our proposed imputation-based strategies can more effectively guide the\nacquisition of new samples in comparison to those relying solely on unimodal\ninformation, entropy guidance, and random selections. Our work provides an\neffective solution for optimizing modality acquisition at the cohort level,\nenabling better utilization of resources in constrained settings."}
{"id": "2505.16923", "pdf": "https://arxiv.org/pdf/2505.16923", "abs": "https://arxiv.org/abs/2505.16923", "authors": ["Yuhui Zhang", "Dongshen Wu", "Yuichiro Wada", "Takafumi Kanamori"], "title": "TULiP: Test-time Uncertainty Estimation via Linearization and Weight Perturbation", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "A reliable uncertainty estimation method is the foundation of many modern\nout-of-distribution (OOD) detectors, which are critical for safe deployments of\ndeep learning models in the open world. In this work, we propose TULiP, a\ntheoretically-driven post-hoc uncertainty estimator for OOD detection. Our\napproach considers a hypothetical perturbation applied to the network before\nconvergence. Based on linearized training dynamics, we bound the effect of such\nperturbation, resulting in an uncertainty score computable by perturbing model\nparameters. Ultimately, our approach computes uncertainty from a set of sampled\npredictions. We visualize our bound on synthetic regression and classification\ndatasets. Furthermore, we demonstrate the effectiveness of TULiP using\nlarge-scale OOD detection benchmarks for image classification. Our method\nexhibits state-of-the-art performance, particularly for near-distribution\nsamples."}
{"id": "2505.16792", "pdf": "https://arxiv.org/pdf/2505.16792", "abs": "https://arxiv.org/abs/2505.16792", "authors": ["Ziqiao Wang", "Wangbo Zhao", "Yuhao Zhou", "Zekai Li", "Zhiyuan Liang", "Mingjia Shi", "Xuanlei Zhao", "Pengfei Zhou", "Kaipeng Zhang", "Zhangyang Wang", "Kai Wang", "Yang You"], "title": "REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training", "categories": ["cs.CV", "cs.AI"], "comment": "24 pages", "summary": "Diffusion Transformers (DiTs) deliver state-of-the-art image quality, yet\ntheir training remains notoriously slow. A recent remedy -- representation\nalignment (REPA) that matches DiT hidden features to those of a non-generative\nteacher (e.g. DINO) -- dramatically accelerates the early epochs but plateaus\nor even degrades performance later. We trace this failure to a capacity\nmismatch: once the generative student begins modelling the joint data\ndistribution, the teacher's lower-dimensional embeddings and attention patterns\nbecome a straitjacket rather than a guide. We then introduce HASTE (Holistic\nAlignment with Stage-wise Termination for Efficient training), a two-phase\nschedule that keeps the help and drops the hindrance. Phase I applies a\nholistic alignment loss that simultaneously distills attention maps (relational\npriors) and feature projections (semantic anchors) from the teacher into\nmid-level layers of the DiT, yielding rapid convergence. Phase II then performs\none-shot termination that deactivates the alignment loss, once a simple trigger\nsuch as a fixed iteration is hit, freeing the DiT to focus on denoising and\nexploit its generative capacity. HASTE speeds up training of diverse DiTs\nwithout architecture changes. On ImageNet 256X256, it reaches the vanilla\nSiT-XL/2 baseline FID in 50 epochs and matches REPA's best FID in 500 epochs,\namounting to a 28X reduction in optimization steps. HASTE also improves\ntext-to-image DiTs on MS-COCO, demonstrating to be a simple yet principled\nrecipe for efficient diffusion training across various tasks. Our code is\navailable at https://github.com/NUS-HPC-AI-Lab/HASTE ."}
{"id": "2505.16927", "pdf": "https://arxiv.org/pdf/2505.16927", "abs": "https://arxiv.org/abs/2505.16927", "authors": ["Keshav Ramji", "Tahira Naseem", "Ramón Fernandez Astudillo"], "title": "Latent Principle Discovery for Language Model Self-Improvement", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "When language model (LM) users aim to improve the quality of its generations,\nit is crucial to specify concrete behavioral attributes that the model should\nstrive to reflect. However, curating such principles across many domains, even\nnon-exhaustively, requires a labor-intensive annotation process. To automate\nthis process, we propose eliciting these latent attributes guiding model\nreasoning towards human-preferred responses by explicitly modeling them in a\nself-correction setting. Our approach mines new principles from the LM itself\nand compresses the discovered elements to an interpretable set via clustering.\nSpecifically, we employ an approximation of posterior-regularized Monte Carlo\nExpectation-Maximization to both identify a condensed set of the most effective\nlatent principles and teach the LM to strategically invoke them in order to\nintrinsically refine its responses. We demonstrate that bootstrapping our\nalgorithm over multiple iterations enables smaller language models (7-8B\nparameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an\naverage of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on\nIFEval. We also show that clustering the principles yields interpretable and\ndiverse model-generated constitutions while retaining model performance. The\ngains our method achieves highlight the potential of automated,\nprinciple-driven post-training recipes toward continual self-improvement."}
{"id": "2505.16798", "pdf": "https://arxiv.org/pdf/2505.16798", "abs": "https://arxiv.org/abs/2505.16798", "authors": ["KiHyun Nam", "Jungwoo Heo", "Jee-weon Jung", "Gangin Park", "Chaeyoung Jung", "Ha-Jin Yu", "Joon Son Chung"], "title": "SEED: Speaker Embedding Enhancement Diffusion Model", "categories": ["eess.AS", "cs.AI"], "comment": "Accepted to Interspeech 2025. The official code can be found at\n  https://github.com/kaistmm/seed-pytorch", "summary": "A primary challenge when deploying speaker recognition systems in real-world\napplications is performance degradation caused by environmental mismatch. We\npropose a diffusion-based method that takes speaker embeddings extracted from a\npre-trained speaker recognition model and generates refined embeddings. For\ntraining, our approach progressively adds Gaussian noise to both clean and\nnoisy speaker embeddings extracted from clean and noisy speech, respectively,\nvia forward process of a diffusion model, and then reconstructs them to clean\nembeddings in the reverse process. While inferencing, all embeddings are\nregenerated via diffusion process. Our method needs neither speaker label nor\nany modification to the existing speaker recognition pipeline. Experiments on\nevaluation sets simulating environment mismatch scenarios show that our method\ncan improve recognition accuracy by up to 19.6% over baseline models while\nretaining performance on conventional scenarios. We publish our code here\nhttps://github.com/kaistmm/seed-pytorch"}
{"id": "2505.16928", "pdf": "https://arxiv.org/pdf/2505.16928", "abs": "https://arxiv.org/abs/2505.16928", "authors": ["Bosung Kim", "Prithviraj Ammanabrolu"], "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks\nthat advances long-context understanding in embodied AI. $\\infty$-THOR\nprovides: (1) a generation framework for synthesizing scalable, reproducible,\nand unlimited long-horizon trajectories; (2) a novel embodied QA task,\nNeedle(s) in the Embodied Haystack, where multiple scattered clues across\nextended trajectories test agents' long-context reasoning ability; and (3) a\nlong-horizon dataset and benchmark suite featuring complex tasks that span\nhundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including\ninterleaved Goal-State-Action modeling, context extension techniques, and\nContext Parallelism, to equip LLM-based agents for extreme long-context\nreasoning and interaction. Experimental results and analyses highlight the\nchallenges posed by our benchmark and provide insights into training strategies\nand model behaviors under long-horizon conditions. Our work provides a\nfoundation for the next generation of embodied AI systems capable of robust,\nlong-term reasoning and planning."}
{"id": "2505.16801", "pdf": "https://arxiv.org/pdf/2505.16801", "abs": "https://arxiv.org/abs/2505.16801", "authors": ["Eleftherios Kalafatis", "Konstantinos Mitsis", "Konstantia Zarkogianni", "Maria Athanasiou", "Konstantina Nikita"], "title": "A modular framework for automated evaluation of procedural content generation in serious games with deep reinforcement learning agents", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Serious Games (SGs) are nowadays shifting focus to include procedural content\ngeneration (PCG) in the development process as a means of offering personalized\nand enhanced player experience. However, the development of a framework to\nassess the impact of PCG techniques when integrated into SGs remains\nparticularly challenging. This study proposes a methodology for automated\nevaluation of PCG integration in SGs, incorporating deep reinforcement learning\n(DRL) game testing agents. To validate the proposed framework, a previously\nintroduced SG featuring card game mechanics and incorporating three different\nversions of PCG for nonplayer character (NPC) creation has been deployed.\nVersion 1 features random NPC creation, while versions 2 and 3 utilize a\ngenetic algorithm approach. These versions are used to test the impact of\ndifferent dynamic SG environments on the proposed framework's agents. The\nobtained results highlight the superiority of the DRL game testing agents\ntrained on Versions 2 and 3 over those trained on Version 1 in terms of win\nrate (i.e. number of wins per played games) and training time. More\nspecifically, within the execution of a test emulating regular gameplay, both\nVersions 2 and 3 peaked at a 97% win rate and achieved statistically\nsignificant higher (p=0009) win rates compared to those achieved in Version 1\nthat peaked at 94%. Overall, results advocate towards the proposed framework's\ncapability to produce meaningful data for the evaluation of procedurally\ngenerated content in SGs."}
{"id": "2505.16942", "pdf": "https://arxiv.org/pdf/2505.16942", "abs": "https://arxiv.org/abs/2505.16942", "authors": ["Karlis Martins Briedis", "Markus Gross", "Christopher Schroers"], "title": "Efficient Correlation Volume Sampling for Ultra-High-Resolution Optical Flow Estimation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent optical flow estimation methods often employ local cost sampling from\na dense all-pairs correlation volume. This results in quadratic computational\nand memory complexity in the number of pixels. Although an alternative\nmemory-efficient implementation with on-demand cost computation exists, this is\nslower in practice and therefore prior methods typically process images at\nreduced resolutions, missing fine-grained details.\n  To address this, we propose a more efficient implementation of the all-pairs\ncorrelation volume sampling, still matching the exact mathematical operator as\ndefined by RAFT. Our approach outperforms on-demand sampling by up to 90% while\nmaintaining low memory usage, and performs on par with the default\nimplementation with up to 95% lower memory usage. As cost sampling makes up a\nsignificant portion of the overall runtime, this can translate to up to 50%\nsavings for the total end-to-end model inference in memory-constrained\nenvironments. Our evaluation of existing methods includes an 8K\nultra-high-resolution dataset and an additional inference-time modification of\nthe recent SEA-RAFT method. With this, we achieve state-of-the-art results at\nhigh resolutions both in accuracy and efficiency."}
{"id": "2505.16813", "pdf": "https://arxiv.org/pdf/2505.16813", "abs": "https://arxiv.org/abs/2505.16813", "authors": ["Yinhao Xu", "Georg A. Gottwald", "Zdenka Kuncic"], "title": "Dynamic Reservoir Computing with Physical Neuromorphic Networks", "categories": ["cs.ET", "cond-mat.dis-nn", "cs.AI"], "comment": "8 pages, 8 figures, IJCNN 2025, accepted", "summary": "Reservoir Computing (RC) with physical systems requires an understanding of\nthe underlying structure and internal dynamics of the specific physical\nreservoir. In this study, physical nano-electronic networks with neuromorphic\ndynamics are investigated for their use as physical reservoirs in an RC\nframework. These neuromorphic networks operate as dynamic reservoirs, with node\nactivities in general coupled to the edge dynamics through nonlinear\nnano-electronic circuit elements, and the reservoir outputs influenced by the\nunderlying network connectivity structure. This study finds that networks with\nvarying degrees of sparsity generate more useful nonlinear temporal outputs for\ndynamic RC compared to dense networks. Dynamic RC is also tested on an\nautonomous multivariate chaotic time series prediction task with networks of\nvarying densities, which revealed the importance of network sparsity in\nmaintaining network activity and overall dynamics, that in turn enabled the\nlearning of the chaotic Lorenz63 system's attractor behavior."}
{"id": "2505.16946", "pdf": "https://arxiv.org/pdf/2505.16946", "abs": "https://arxiv.org/abs/2505.16946", "authors": ["Sanjana Chalavadi", "Andrei Pastor", "Terry Leitch"], "title": "NY Real Estate Racial Equity Analysis via Applied Machine Learning", "categories": ["cs.CY", "cs.LG"], "comment": null, "summary": "This study analyzes tract-level real estate ownership patterns in New York\nState (NYS) and New York City (NYC) to uncover racial disparities. We use an\nadvanced race/ethnicity imputation model (LSTM+Geo with XGBoost filtering,\nvalidated at 89.2% accuracy) to compare the predicted racial composition of\nproperty owners to the resident population from census data. We examine both a\nFull Model (statewide) and a Name-Only LSTM Model (NYC) to assess how\nincorporating geospatial context affects our predictions and disparity\nestimates. The results reveal significant inequities: White individuals hold a\ndisproportionate share of properties and property value relative to their\npopulation, while Black, Hispanic, and Asian communities are underrepresented\nas property owners. These disparities are most pronounced in minority-majority\nneighborhoods, where ownership is predominantly White despite a predominantly\nnon-White population. Corporate ownership (LLCs, trusts, etc.) exacerbates\nthese gaps by reducing owner-occupied opportunities in urban minority\ncommunities. We provide a breakdown of ownership vs. population by race for\nmajority-White, -Black, -Hispanic, and -Asian tracts, identify those with\nextreme ownership disparities, and compare patterns in urban, suburban, and\nrural contexts. The findings underscore persistent racial inequity in property\nownership, reflecting broader historical and socio-economic forces, and\nhighlight the importance of data-driven approaches to address these issues."}
{"id": "2505.16831", "pdf": "https://arxiv.org/pdf/2505.16831", "abs": "https://arxiv.org/abs/2505.16831", "authors": ["Xiaoyu Xu", "Xiang Yue", "Yang Liu", "Qingqing Ye", "Haibo Hu", "Minxin Du"], "title": "Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "44 pages", "summary": "Unlearning in large language models (LLMs) is intended to remove the\ninfluence of specific data, yet current evaluations rely heavily on token-level\nmetrics such as accuracy and perplexity. We show that these metrics can be\nmisleading: models often appear to forget, but their original behavior can be\nrapidly restored with minimal fine-tuning, revealing that unlearning may\nobscure information rather than erase it. To diagnose this phenomenon, we\nintroduce a representation-level evaluation framework using PCA-based\nsimilarity and shift, centered kernel alignment, and Fisher information.\nApplying this toolkit across six unlearning methods, three domains (text, code,\nmath), and two open-source LLMs, we uncover a critical distinction between\nreversible and irreversible forgetting. In reversible cases, models suffer\ntoken-level collapse yet retain latent features; in irreversible cases, deeper\nrepresentational damage occurs. We further provide a theoretical account\nlinking shallow weight perturbations near output layers to misleading\nunlearning signals, and show that reversibility is modulated by task type and\nhyperparameters. Our findings reveal a fundamental gap in current evaluation\npractices and establish a new diagnostic foundation for trustworthy unlearning\nin LLMs. We provide a unified toolkit for analyzing LLM representation changes\nunder unlearning and relearning:\nhttps://github.com/XiaoyuXU1/Representational_Analysis_Tools.git."}
{"id": "2505.16965", "pdf": "https://arxiv.org/pdf/2505.16965", "abs": "https://arxiv.org/abs/2505.16965", "authors": ["Fengyi Li", "Kayhan Behdin", "Natesh Pillai", "Xiaofeng Wang", "Zhipeng Wang", "Ercan Yildiz"], "title": "BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Text segmentation based on the semantic meaning of sentences is a fundamental\ntask with broad utility in many downstream applications. In this paper, we\npropose a graphical model-based unsupervised learning approach, named BP-Seg\nfor efficient text segmentation. Our method not only considers local coherence,\ncapturing the intuition that adjacent sentences are often more related, but\nalso effectively groups sentences that are distant in the text yet semantically\nsimilar. This is achieved through belief propagation on the carefully\nconstructed graphical models. Experimental results on both an illustrative\nexample and a dataset with long-form documents demonstrate that our method\nperforms favorably compared to competing approaches."}
{"id": "2505.16834", "pdf": "https://arxiv.org/pdf/2505.16834", "abs": "https://arxiv.org/abs/2505.16834", "authors": ["Shuang Sun", "Huatong Song", "Yuhao Wang", "Ruiyang Ren", "Jinhao Jiang", "Junjie Zhang", "Fei Bai", "Jia Deng", "Wayne Xin Zhao", "Zheng Liu", "Lei Fang", "Zhongyuan Wang", "Ji-Rong Wen"], "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher."}
{"id": "2505.16968", "pdf": "https://arxiv.org/pdf/2505.16968", "abs": "https://arxiv.org/abs/2505.16968", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Gustavo Bertolo Stahl", "Seung Hun Eddie Han", "Salman Khan", "Abdulrahman Mahmoud"], "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.PL"], "comment": "20 pages, 11 figures, 5 tables", "summary": "We introduce \\texttt{CASS}, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level\n(CUDA~$\\leftrightarrow$~HIP) and assembly-level (Nvidia\nSASS~$\\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k\nverified code pairs across host and device, addressing a critical gap in\nlow-level GPU code portability. Leveraging this resource, we train the\n\\texttt{CASS} family of domain-specific language models, achieving 95\\% source\ntranslation accuracy and 37.5\\% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85\\% of test cases,\npreserving runtime and memory behavior. To support rigorous evaluation, we\nintroduce \\texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with\nground-truth execution. All data, models, and evaluation tools are released as\nopen source to foster progress in GPU compiler tooling, binary compatibility,\nand LLM-guided hardware translation. Dataset and benchmark are on\n\\href{https://huggingface.co/datasets/MBZUAI/cass}{\\textcolor{blue}{HuggingFace}},\nwith code at\n\\href{https://github.com/GustavoStahl/CASS}{\\textcolor{blue}{GitHub}}."}
{"id": "2505.16836", "pdf": "https://arxiv.org/pdf/2505.16836", "abs": "https://arxiv.org/abs/2505.16836", "authors": ["Fanrui Zhang", "Dian Li", "Qiang Zhang", "Chenjun", "sinbadliu", "Junxiong Lin", "Jiahong Yan", "Jiawei Liu", "Zheng-Jun Zha"], "title": "Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "28 pages, 27 figures", "summary": "The rapid spread of multimodal misinformation on social media has raised\ngrowing concerns, while research on video misinformation detection remains\nlimited due to the lack of large-scale, diverse datasets. Existing methods\noften overfit to rigid templates and lack deep reasoning over deceptive\ncontent. To address these challenges, we introduce FakeVV, a large-scale\nbenchmark comprising over 100,000 video-text pairs with fine-grained,\ninterpretable annotations. In addition, we further propose Fact-R1, a novel\nframework that integrates deep reasoning with collaborative rule-based\nreinforcement learning. Fact-R1 is trained through a three-stage process: (1)\nmisinformation long-Chain-of-Thought (CoT) instruction tuning, (2) preference\nalignment via Direct Preference Optimization (DPO), and (3) Group Relative\nPolicy Optimization (GRPO) using a novel verifiable reward function. This\nenables Fact-R1 to exhibit emergent reasoning behaviors comparable to those\nobserved in advanced text-based reinforcement learning systems, but in the more\ncomplex multimodal misinformation setting. Our work establishes a new paradigm\nfor misinformation detection, bridging large-scale video understanding,\nreasoning-guided alignment, and interpretable verification."}
{"id": "2505.16985", "pdf": "https://arxiv.org/pdf/2505.16985", "abs": "https://arxiv.org/abs/2505.16985", "authors": ["Moru Liu", "Hao Dong", "Jessica Kelly", "Olga Fink", "Mario Trapp"], "title": "Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Out-of-distribution (OOD) detection and segmentation are crucial for\ndeploying machine learning models in safety-critical applications such as\nautonomous driving and robot-assisted surgery. While prior research has\nprimarily focused on unimodal image data, real-world applications are\ninherently multimodal, requiring the integration of multiple modalities for\nimproved OOD detection. A key challenge is the lack of supervision signals from\nunknown data, leading to overconfident predictions on OOD samples. To address\nthis challenge, we propose Feature Mixing, an extremely simple and fast method\nfor multimodal outlier synthesis with theoretical support, which can be further\noptimized to help the model better distinguish between in-distribution (ID) and\nOOD data. Feature Mixing is modality-agnostic and applicable to various\nmodality combinations. Additionally, we introduce CARLA-OOD, a novel multimodal\ndataset for OOD segmentation, featuring synthetic OOD objects across diverse\nscenes and weather conditions. Extensive experiments on SemanticKITTI,\nnuScenes, CARLA-OOD datasets, and the MultiOOD benchmark demonstrate that\nFeature Mixing achieves state-of-the-art performance with a $10 \\times$ to $370\n\\times$ speedup. Our source code and dataset will be available at\nhttps://github.com/mona4399/FeatureMixing."}
{"id": "2505.16845", "pdf": "https://arxiv.org/pdf/2505.16845", "abs": "https://arxiv.org/abs/2505.16845", "authors": ["Hanglei Zhang", "Yiwei Guo", "Zhihan Li", "Xiang Hao", "Xie Chen", "Kai Yu"], "title": "Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Most neural speech codecs achieve bitrate adjustment through intra-frame\nmechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However,\nspeech segments inherently have time-varying information density (e.g., silent\nintervals versus voiced regions). This property makes CFR not optimal in terms\nof bitrate and token sequence length, hindering efficiency in real-time\napplications. In this work, we propose a Temporally Flexible Coding (TFC)\ntechnique, introducing variable frame rate (VFR) into neural speech codecs for\nthe first time. TFC enables seamlessly tunable average frame rates and\ndynamically allocates frame rates based on temporal entropy. Experimental\nresults show that a codec with TFC achieves optimal reconstruction quality with\nhigh flexibility, and maintains competitive performance even at lower frame\nrates. Our approach is promising for the integration with other efforts to\ndevelop low-frame-rate neural speech codecs for more efficient downstream\ntasks."}
{"id": "2505.16993", "pdf": "https://arxiv.org/pdf/2505.16993", "abs": "https://arxiv.org/abs/2505.16993", "authors": ["Guillem Brasó", "Aljoša Ošep", "Laura Leal-Taixé"], "title": "Native Segmentation Vision Transformers", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Uniform downsampling remains the de facto standard for reducing spatial\nresolution in vision backbones. In this work, we propose an alternative design\nbuilt around a content-aware spatial grouping layer, that dynamically assigns\ntokens to a reduced set based on image boundaries and their semantic content.\nStacking our grouping layer across consecutive backbone stages results in\nhierarchical segmentation that arises natively in the feature extraction\nprocess, resulting in our coined Native Segmentation Vision Transformer. We\nshow that a careful design of our architecture enables the emergence of strong\nsegmentation masks solely from grouping layers, that is, without additional\nsegmentation-specific heads. This sets the foundation for a new paradigm of\nnative, backbone-level segmentation, which enables strong zero-shot results\nwithout mask supervision, as well as a minimal and efficient standalone model\ndesign for downstream segmentation tasks. Our project page is\nhttps://research.nvidia.com/labs/dvl/projects/native-segmentation."}
{"id": "2505.16856", "pdf": "https://arxiv.org/pdf/2505.16856", "abs": "https://arxiv.org/abs/2505.16856", "authors": ["Wei Xiao", "Jiacheng Liu", "Zifeng Zhuang", "Runze Suo", "Shangke Lyu", "Donglin Wang"], "title": "Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Improving the performance of pre-trained policies through online\nreinforcement learning (RL) is a critical yet challenging topic. Existing\nonline RL fine-tuning methods require continued training with offline\npretrained Q-functions for stability and performance. However, these offline\npretrained Q-functions commonly underestimate state-action pairs beyond the\noffline dataset due to the conservatism in most offline RL methods, which\nhinders further exploration when transitioning from the offline to the online\nsetting. Additionally, this requirement limits their applicability in scenarios\nwhere only pre-trained policies are available but pre-trained Q-functions are\nabsent, such as in imitation learning (IL) pre-training. To address these\nchallenges, we propose a method for efficient online RL fine-tuning using\nsolely the offline pre-trained policy, eliminating reliance on pre-trained\nQ-functions. We introduce PORL (Policy-Only Reinforcement Learning\nFine-Tuning), which rapidly initializes the Q-function from scratch during the\nonline phase to avoid detrimental pessimism. Our method not only achieves\ncompetitive performance with advanced offline-to-online RL algorithms and\nonline RL approaches that leverage data or policies prior, but also pioneers a\nnew path for directly fine-tuning behavior cloning (BC) policies."}
{"id": "2505.17000", "pdf": "https://arxiv.org/pdf/2505.17000", "abs": "https://arxiv.org/abs/2505.17000", "authors": ["Simmaco Di Lillo"], "title": "Critical Points of Random Neural Networks", "categories": ["stat.ML", "cs.LG", "math.PR", "60G60, 62B10, 62M45"], "comment": null, "summary": "This work investigates the expected number of critical points of random\nneural networks with different activation functions as the depth increases in\nthe infinite-width limit. Under suitable regularity conditions, we derive\nprecise asymptotic formulas for the expected number of critical points of fixed\nindex and those exceeding a given threshold. Our analysis reveals three\ndistinct regimes depending on the value of the first derivative of the\ncovariance evaluated at 1: the expected number of critical points may converge,\ngrow polynomially, or grow exponentially with depth. The theoretical\npredictions are supported by numerical experiments. Moreover, we provide\nnumerical evidence suggesting that, when the regularity condition is not\nsatisfied (e.g. for neural networks with ReLU as activation function), the\nnumber of critical points increases as the map resolution increases, indicating\na potential divergence in the number of critical points."}
{"id": "2505.16860", "pdf": "https://arxiv.org/pdf/2505.16860", "abs": "https://arxiv.org/abs/2505.16860", "authors": ["Ziyue Qiao", "Qianyi Cai", "Hao Dong", "Jiawei Gu", "Pengyang Wang", "Meng Xiao", "Xiao Luo", "Hui Xiong"], "title": "GCAL: Adapting Graph Models to Evolving Domain Shifts", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "This paper addresses the challenge of graph domain adaptation on evolving,\nmultiple out-of-distribution (OOD) graphs. Conventional graph domain adaptation\nmethods are confined to single-step adaptation, making them ineffective in\nhandling continuous domain shifts and prone to catastrophic forgetting. This\npaper introduces the Graph Continual Adaptive Learning (GCAL) method, designed\nto enhance model sustainability and adaptability across various graph domains.\nGCAL employs a bilevel optimization strategy. The \"adapt\" phase uses an\ninformation maximization approach to fine-tune the model with new graph domains\nwhile re-adapting past memories to mitigate forgetting. Concurrently, the\n\"generate memory\" phase, guided by a theoretical lower bound derived from\ninformation bottleneck theory, involves a variational memory graph generation\nmodule to condense original graphs into memories. Extensive experimental\nevaluations demonstrate that GCAL substantially outperforms existing methods in\nterms of adaptability and knowledge retention."}
{"id": "2505.17003", "pdf": "https://arxiv.org/pdf/2505.17003", "abs": "https://arxiv.org/abs/2505.17003", "authors": ["Nanda H. Krishna", "Colin Bredenberg", "Daniel Levenstein", "Blake A. Richards", "Guillaume Lajoie"], "title": "Sufficient conditions for offline reactivation in recurrent neural networks", "categories": ["q-bio.NC", "cs.LG", "cs.NE"], "comment": "ICLR 2024", "summary": "During periods of quiescence, such as sleep, neural activity in many brain\ncircuits resembles that observed during periods of task engagement. However,\nthe precise conditions under which task-optimized networks can autonomously\nreactivate the same network states responsible for online behavior is poorly\nunderstood. In this study, we develop a mathematical framework that outlines\nsufficient conditions for the emergence of neural reactivation in circuits that\nencode features of smoothly varying stimuli. We demonstrate mathematically that\nnoisy recurrent networks optimized to track environmental state variables using\nchange-based sensory information naturally develop denoising dynamics, which,\nin the absence of input, cause the network to revisit state configurations\nobserved during periods of online activity. We validate our findings using\nnumerical experiments on two canonical neuroscience tasks: spatial position\nestimation based on self-motion cues, and head direction estimation based on\nangular velocity cues. Overall, our work provides theoretical support for\nmodeling offline reactivation as an emergent consequence of task optimization\nin noisy neural circuits."}
{"id": "2505.16875", "pdf": "https://arxiv.org/pdf/2505.16875", "abs": "https://arxiv.org/abs/2505.16875", "authors": ["Zhehao Huang", "Yuhang Liu", "Yixin Lou", "Zhengbao He", "Mingzhen He", "Wenxing Zhou", "Tao Li", "Kehan Li", "Zeyi Huang", "Xiaolin Huang"], "title": "T2I-ConBench: Text-to-Image Benchmark for Continual Post-training", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Continual post-training adapts a single text-to-image diffusion model to\nlearn new tasks without incurring the cost of separate models, but naive\npost-training causes forgetting of pretrained knowledge and undermines\nzero-shot compositionality. We observe that the absence of a standardized\nevaluation protocol hampers related research for continual post-training. To\naddress this, we introduce T2I-ConBench, a unified benchmark for continual\npost-training of text-to-image models. T2I-ConBench focuses on two practical\nscenarios, item customization and domain enhancement, and analyzes four\ndimensions: (1) retention of generality, (2) target-task performance, (3)\ncatastrophic forgetting, and (4) cross-task generalization. It combines\nautomated metrics, human-preference modeling, and vision-language QA for\ncomprehensive assessment. We benchmark ten representative methods across three\nrealistic task sequences and find that no approach excels on all fronts. Even\njoint \"oracle\" training does not succeed for every task, and cross-task\ngeneralization remains unsolved. We release all datasets, code, and evaluation\ntools to accelerate research in continual post-training for text-to-image\nmodels."}
{"id": "2505.17017", "pdf": "https://arxiv.org/pdf/2505.17017", "abs": "https://arxiv.org/abs/2505.17017", "authors": ["Chengzhuo Tong", "Ziyu Guo", "Renrui Zhang", "Wenyu Shan", "Xinyu Wei", "Zhenghao Xing", "Hongsheng Li", "Pheng-Ann Heng"], "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT", "summary": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT"}
{"id": "2505.16881", "pdf": "https://arxiv.org/pdf/2505.16881", "abs": "https://arxiv.org/abs/2505.16881", "authors": ["Daniel F. Perez-Ramirez", "Dejan Kostic", "Magnus Boman"], "title": "CASTILLO: Characterizing Response Length Distributions of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Dataset available in\n  https://huggingface.co/datasets/danfperam/castillo and code is available in\n  https://github.com/DanielFPerez/castillo", "summary": "Efficiently managing compute resources for Large Language Model (LLM)\ninference remains challenging due to the inherently stochastic and variable\nlengths of autoregressive text generation. Accurately estimating response\nlengths in advance enables proactive resource allocation, yet existing\napproaches either bias text generation towards certain lengths or rely on\nassumptions that ignore model- and prompt-specific variability. We introduce\nCASTILLO, a dataset characterizing response length distributions across 13\nwidely-used open-source LLMs evaluated on seven distinct instruction-following\ncorpora. For each $\\langle$prompt, model$\\rangle$ sample pair, we generate 10\nindependent completions using fixed decoding hyper-parameters, record the token\nlength of each response, and publish summary statistics (mean, std-dev,\npercentiles), along with the shortest and longest completions, and the exact\ngeneration settings. Our analysis reveals significant inter- and intra-model\nvariability in response lengths (even under identical generation settings), as\nwell as model-specific behaviors and occurrences of partial text degeneration\nin only subsets of responses. CASTILLO enables the development of predictive\nmodels for proactive scheduling and provides a systematic framework for\nanalyzing model-specific generation behaviors. We publicly release the dataset\nand code to foster research at the intersection of generative language modeling\nand systems."}
{"id": "2505.17022", "pdf": "https://arxiv.org/pdf/2505.17022", "abs": "https://arxiv.org/abs/2505.17022", "authors": ["Chengqi Duan", "Rongyao Fang", "Yuqing Wang", "Kun Wang", "Linjiang Huang", "Xingyu Zeng", "Hongsheng Li", "Xihui Liu"], "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Github page refer to: https://github.com/gogoduan/GoT-R1", "summary": "Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1."}
{"id": "2505.16886", "pdf": "https://arxiv.org/pdf/2505.16886", "abs": "https://arxiv.org/abs/2505.16886", "authors": ["Nour Jedidi", "Yung-Sung Chuang", "James Glass", "Jimmy Lin"], "title": "Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers."}
{"id": "2505.16888", "pdf": "https://arxiv.org/pdf/2505.16888", "abs": "https://arxiv.org/abs/2505.16888", "authors": ["Viet Pham", "Thai Le"], "title": "CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available."}
{"id": "2505.16896", "pdf": "https://arxiv.org/pdf/2505.16896", "abs": "https://arxiv.org/abs/2505.16896", "authors": ["Can Chen", "David Heurtel-Depeiges", "Robert M. Vernon", "Christopher James Langmead", "Yoshua Bengio", "Quentin Fournier"], "title": "Structure-Aligned Protein Language Model", "categories": ["cs.LG", "cs.AI"], "comment": "16 pages, 8 figures, 7 tables", "summary": "Protein language models (pLMs) pre-trained on vast protein sequence databases\nexcel at various downstream tasks but lack the structural knowledge essential\nfor many biological applications. To address this, we integrate structural\ninsights from pre-trained protein graph neural networks (pGNNs) into pLMs\nthrough a latent-level contrastive learning task. This task aligns residue\nrepresentations from pLMs with those from pGNNs across multiple proteins,\nenriching pLMs with inter-protein structural knowledge. Additionally, we\nincorporate a physical-level task that infuses intra-protein structural\nknowledge by optimizing pLMs to predict structural tokens. The proposed\ndual-task framework effectively incorporates both inter-protein and\nintra-protein structural knowledge into pLMs. Given the variability in the\nquality of protein structures in PDB, we further introduce a residue loss\nselection module, which uses a small model trained on high-quality structures\nto select reliable yet challenging residue losses for the pLM to learn.\nApplying our structure alignment method to the state-of-the-art ESM2 and\nAMPLIFY results in notable performance gains across a wide range of tasks,\nincluding a 12.7% increase in ESM2 contact prediction. The data, code, and\nresulting SaESM2 and SaAMPLIFY models will be released on Hugging Face."}
{"id": "2505.16911", "pdf": "https://arxiv.org/pdf/2505.16911", "abs": "https://arxiv.org/abs/2505.16911", "authors": ["Ofir Yaish", "Yehuda Mishaly", "Eliya Nachmani"], "title": "Active Speech Enhancement: Active Speech Denoising Decliping and Deveraberation", "categories": ["eess.AS", "cs.AI"], "comment": null, "summary": "We introduce a new paradigm for active sound modification: Active Speech\nEnhancement (ASE). While Active Noise Cancellation (ANC) algorithms focus on\nsuppressing external interference, ASE goes further by actively shaping the\nspeech signal -- both attenuating unwanted noise components and amplifying\nspeech-relevant frequencies -- to improve intelligibility and perceptual\nquality. To enable this, we propose a novel Transformer-Mamba-based\narchitecture, along with a task-specific loss function designed to jointly\noptimize interference suppression and signal enrichment. Our method outperforms\nexisting baselines across multiple speech processing tasks -- including\ndenoising, dereverberation, and declipping -- demonstrating the effectiveness\nof active, targeted modulation in challenging acoustic environments."}
{"id": "2505.16915", "pdf": "https://arxiv.org/pdf/2505.16915", "abs": "https://arxiv.org/abs/2505.16915", "authors": ["Qirui Jiao", "Daoyuan Chen", "Yilun Huang", "Xika Lin", "Ying Shen", "Yaliang Li"], "title": "DetailMaster: Can Your Text-to-Image Model Handle Long Prompts?", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 8 figures, 10 tables", "summary": "While recent text-to-image (T2I) models show impressive capabilities in\nsynthesizing images from brief descriptions, their performance significantly\ndegrades when confronted with long, detail-intensive prompts required in\nprofessional applications. We present DetailMaster, the first comprehensive\nbenchmark specifically designed to evaluate T2I models' systematical abilities\nto handle extended textual inputs that contain complex compositional\nrequirements. Our benchmark introduces four critical evaluation dimensions:\nCharacter Attributes, Structured Character Locations, Multi-Dimensional Scene\nAttributes, and Explicit Spatial/Interactive Relationships. The benchmark\ncomprises long and detail-rich prompts averaging 284.89 tokens, with high\nquality validated by expert annotators. Evaluation on 7 general-purpose and 5\nlong-prompt-optimized T2I models reveals critical performance limitations:\nstate-of-the-art models achieve merely ~50% accuracy in key dimensions like\nattribute binding and spatial reasoning, while all models showing progressive\nperformance degradation as prompt length increases. Our analysis highlights\nsystemic failures in structural comprehension and detail overload handling,\nmotivating future research into architectures with enhanced compositional\nreasoning. We open-source the dataset, data curation code, and evaluation tools\nto advance detail-rich T2I generation and enable broad applications that would\notherwise be infeasible due to the lack of a dedicated benchmark."}
{"id": "2505.16927", "pdf": "https://arxiv.org/pdf/2505.16927", "abs": "https://arxiv.org/abs/2505.16927", "authors": ["Keshav Ramji", "Tahira Naseem", "Ramón Fernandez Astudillo"], "title": "Latent Principle Discovery for Language Model Self-Improvement", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "When language model (LM) users aim to improve the quality of its generations,\nit is crucial to specify concrete behavioral attributes that the model should\nstrive to reflect. However, curating such principles across many domains, even\nnon-exhaustively, requires a labor-intensive annotation process. To automate\nthis process, we propose eliciting these latent attributes guiding model\nreasoning towards human-preferred responses by explicitly modeling them in a\nself-correction setting. Our approach mines new principles from the LM itself\nand compresses the discovered elements to an interpretable set via clustering.\nSpecifically, we employ an approximation of posterior-regularized Monte Carlo\nExpectation-Maximization to both identify a condensed set of the most effective\nlatent principles and teach the LM to strategically invoke them in order to\nintrinsically refine its responses. We demonstrate that bootstrapping our\nalgorithm over multiple iterations enables smaller language models (7-8B\nparameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an\naverage of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on\nIFEval. We also show that clustering the principles yields interpretable and\ndiverse model-generated constitutions while retaining model performance. The\ngains our method achieves highlight the potential of automated,\nprinciple-driven post-training recipes toward continual self-improvement."}
{"id": "2505.16932", "pdf": "https://arxiv.org/pdf/2505.16932", "abs": "https://arxiv.org/abs/2505.16932", "authors": ["Noah Amsel", "David Persson", "Christopher Musco", "Robert Gower"], "title": "The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NA", "math.NA", "math.OC"], "comment": null, "summary": "Computing the polar decomposition and the related matrix sign function, has\nbeen a well-studied problem in numerical analysis for decades. More recently,\nit has emerged as an important subroutine in deep learning, particularly within\nthe Muon optimization framework. However, the requirements in this setting\ndiffer significantly from those of traditional numerical analysis. In deep\nlearning, methods must be highly efficient and GPU-compatible, but high\naccuracy is often unnecessary. As a result, classical algorithms like\nNewton-Schulz (which suffers from slow initial convergence) and methods based\non rational functions (which rely on QR decompositions or matrix inverses) are\npoorly suited to this context. In this work, we introduce Polar Express, a\nGPU-friendly algorithm for computing the polar decomposition. Like classical\npolynomial methods such as Newton-Schulz, our approach uses only matrix-matrix\nmultiplications, making it GPU-compatible. Motivated by earlier work of Chen &\nChow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule\nat each iteration by solving a minimax optimization problem, and we prove that\nit enjoys a strong worst-case optimality guarantee. This property ensures both\nrapid early convergence and fast asymptotic convergence. We also address\nfinite-precision issues, making it stable in bfloat16 in practice. We apply\nPolar Express within the Muon optimization framework and show consistent\nimprovements in validation loss on large-scale models such as GPT-2,\noutperforming recent alternatives across a range of learning rates."}
{"id": "2505.16941", "pdf": "https://arxiv.org/pdf/2505.16941", "abs": "https://arxiv.org/abs/2505.16941", "authors": ["Chao Pang", "Vincent Jeanselme", "Young Sang Choi", "Xinzhuo Jiang", "Zilin Jing", "Aparajita Kashyap", "Yuta Kobayashi", "Yanwei Li", "Florent Pollet", "Karthik Natarajan", "Shalmali Joshi"], "title": "FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Foundation models hold significant promise in healthcare, given their\ncapacity to extract meaningful representations independent of downstream tasks.\nThis property has enabled state-of-the-art performance across several clinical\napplications trained on structured electronic health record (EHR) data, even in\nsettings with limited labeled data, a prevalent challenge in healthcare.\nHowever, there is little consensus on these models' potential for clinical\nutility due to the lack of desiderata of comprehensive and meaningful tasks and\nsufficiently diverse evaluations to characterize the benefit over conventional\nsupervised learning. To address this gap, we propose a suite of clinically\nmeaningful tasks spanning patient outcomes, early prediction of acute and\nchronic conditions, including desiderata for robust evaluations. We evaluate\nstate-of-the-art foundation models on EHR data consisting of 5 million patients\nfrom Columbia University Irving Medical Center (CUMC), a large urban academic\nmedical center in New York City, across 14 clinically relevant tasks. We\nmeasure overall accuracy, calibration, and subpopulation performance to surface\ntradeoffs based on the choice of pre-training, tokenization, and data\nrepresentation strategies. Our study aims to advance the empirical evaluation\nof structured EHR foundation models and guide the development of future\nhealthcare foundation models."}
{"id": "2505.16947", "pdf": "https://arxiv.org/pdf/2505.16947", "abs": "https://arxiv.org/abs/2505.16947", "authors": ["Csaba Dékány", "Stefan Balauca", "Robin Staab", "Dimitar I. Dimitrov", "Martin Vechev"], "title": "MixAT: Combining Continuous and Discrete Adversarial Training for LLMs", "categories": ["cs.LG", "cs.AI", "I.2.7; K.4.1"], "comment": null, "summary": "Despite recent efforts in Large Language Models (LLMs) safety and alignment,\ncurrent adversarial attacks on frontier LLMs are still able to force harmful\ngenerations consistently. Although adversarial training has been widely studied\nand shown to significantly improve the robustness of traditional machine\nlearning models, its strengths and weaknesses in the context of LLMs are less\nunderstood. Specifically, while existing discrete adversarial attacks are\neffective at producing harmful content, training LLMs with concrete adversarial\nprompts is often computationally expensive, leading to reliance on continuous\nrelaxations. As these relaxations do not correspond to discrete input tokens,\nsuch latent training methods often leave models vulnerable to a diverse set of\ndiscrete attacks. In this work, we aim to bridge this gap by introducing MixAT,\na novel method that combines stronger discrete and faster continuous attacks\nduring training. We rigorously evaluate MixAT across a wide spectrum of\nstate-of-the-art attacks, proposing the At Least One Attack Success Rate\n(ALO-ASR) metric to capture the worst-case vulnerability of models. We show\nMixAT achieves substantially better robustness (ALO-ASR < 20%) compared to\nprior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to\nmethods based on continuous relaxations. We further analyze MixAT in realistic\ndeployment settings, exploring how chat templates, quantization, low-rank\nadapters, and temperature affect both adversarial training and evaluation,\nrevealing additional blind spots in current methodologies. Our results\ndemonstrate that MixAT's discrete-continuous defense offers a principled and\nsuperior robustness-accuracy tradeoff with minimal computational overhead,\nhighlighting its promise for building safer LLMs. We provide our code and\nmodels at https://github.com/insait-institute/MixAT."}
{"id": "2505.16950", "pdf": "https://arxiv.org/pdf/2505.16950", "abs": "https://arxiv.org/abs/2505.16950", "authors": ["Adnan Oomerjee", "Zafeirios Fountas", "Zhongwei Yu", "Haitham Bou-Ammar", "Jun Wang"], "title": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised Reasoning", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome."}
{"id": "2505.16957", "pdf": "https://arxiv.org/pdf/2505.16957", "abs": "https://arxiv.org/abs/2505.16957", "authors": ["Junjie Xiong", "Changjia Zhu", "Shuhang Lin", "Chong Zhang", "Yongfeng Zhang", "Yao Liu", "Lingyao Li"], "title": "Invisible Prompts, Visible Threats: Malicious Font Injection in External Resources for Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly equipped with capabilities of\nreal-time web search and integrated with protocols like Model Context Protocol\n(MCP). This extension could introduce new security vulnerabilities. We present\na systematic investigation of LLM vulnerabilities to hidden adversarial prompts\nthrough malicious font injection in external resources like webpages, where\nattackers manipulate code-to-glyph mapping to inject deceptive content which\nare invisible to users. We evaluate two critical attack scenarios: (1)\n\"malicious content relay\" and (2) \"sensitive data leakage\" through MCP-enabled\ntools. Our experiments reveal that indirect prompts with injected malicious\nfont can bypass LLM safety mechanisms through external resources, achieving\nvarying success rates based on data sensitivity and prompt design. Our research\nunderscores the urgent need for enhanced security measures in LLM deployments\nwhen processing external content."}
{"id": "2505.16965", "pdf": "https://arxiv.org/pdf/2505.16965", "abs": "https://arxiv.org/abs/2505.16965", "authors": ["Fengyi Li", "Kayhan Behdin", "Natesh Pillai", "Xiaofeng Wang", "Zhipeng Wang", "Ercan Yildiz"], "title": "BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Text segmentation based on the semantic meaning of sentences is a fundamental\ntask with broad utility in many downstream applications. In this paper, we\npropose a graphical model-based unsupervised learning approach, named BP-Seg\nfor efficient text segmentation. Our method not only considers local coherence,\ncapturing the intuition that adjacent sentences are often more related, but\nalso effectively groups sentences that are distant in the text yet semantically\nsimilar. This is achieved through belief propagation on the carefully\nconstructed graphical models. Experimental results on both an illustrative\nexample and a dataset with long-form documents demonstrate that our method\nperforms favorably compared to competing approaches."}
{"id": "2505.16967", "pdf": "https://arxiv.org/pdf/2505.16967", "abs": "https://arxiv.org/abs/2505.16967", "authors": ["Nandan Thakur", "Crystina Zhang", "Xueguang Ma", "Jimmy Lin"], "title": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Code is available at https://github.com/castorini/rlhn & datasets are\n  available at https://huggingface.co/rlhn", "summary": "Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini."}
{"id": "2505.16968", "pdf": "https://arxiv.org/pdf/2505.16968", "abs": "https://arxiv.org/abs/2505.16968", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Gustavo Bertolo Stahl", "Seung Hun Eddie Han", "Salman Khan", "Abdulrahman Mahmoud"], "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.PL"], "comment": "20 pages, 11 figures, 5 tables", "summary": "We introduce \\texttt{CASS}, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level\n(CUDA~$\\leftrightarrow$~HIP) and assembly-level (Nvidia\nSASS~$\\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k\nverified code pairs across host and device, addressing a critical gap in\nlow-level GPU code portability. Leveraging this resource, we train the\n\\texttt{CASS} family of domain-specific language models, achieving 95\\% source\ntranslation accuracy and 37.5\\% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85\\% of test cases,\npreserving runtime and memory behavior. To support rigorous evaluation, we\nintroduce \\texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with\nground-truth execution. All data, models, and evaluation tools are released as\nopen source to foster progress in GPU compiler tooling, binary compatibility,\nand LLM-guided hardware translation. Dataset and benchmark are on\n\\href{https://huggingface.co/datasets/MBZUAI/cass}{\\textcolor{blue}{HuggingFace}},\nwith code at\n\\href{https://github.com/GustavoStahl/CASS}{\\textcolor{blue}{GitHub}}."}
{"id": "2505.16985", "pdf": "https://arxiv.org/pdf/2505.16985", "abs": "https://arxiv.org/abs/2505.16985", "authors": ["Moru Liu", "Hao Dong", "Jessica Kelly", "Olga Fink", "Mario Trapp"], "title": "Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Out-of-distribution (OOD) detection and segmentation are crucial for\ndeploying machine learning models in safety-critical applications such as\nautonomous driving and robot-assisted surgery. While prior research has\nprimarily focused on unimodal image data, real-world applications are\ninherently multimodal, requiring the integration of multiple modalities for\nimproved OOD detection. A key challenge is the lack of supervision signals from\nunknown data, leading to overconfident predictions on OOD samples. To address\nthis challenge, we propose Feature Mixing, an extremely simple and fast method\nfor multimodal outlier synthesis with theoretical support, which can be further\noptimized to help the model better distinguish between in-distribution (ID) and\nOOD data. Feature Mixing is modality-agnostic and applicable to various\nmodality combinations. Additionally, we introduce CARLA-OOD, a novel multimodal\ndataset for OOD segmentation, featuring synthetic OOD objects across diverse\nscenes and weather conditions. Extensive experiments on SemanticKITTI,\nnuScenes, CARLA-OOD datasets, and the MultiOOD benchmark demonstrate that\nFeature Mixing achieves state-of-the-art performance with a $10 \\times$ to $370\n\\times$ speedup. Our source code and dataset will be available at\nhttps://github.com/mona4399/FeatureMixing."}
{"id": "2505.16986", "pdf": "https://arxiv.org/pdf/2505.16986", "abs": "https://arxiv.org/abs/2505.16986", "authors": ["Amartya Chakraborty", "Paresh Dashore", "Nadia Bathaee", "Anmol Jain", "Anirban Das", "Shi-Xiong Zhang", "Sambit Sahu", "Milind Naphade", "Genta Indra Winata"], "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios."}
{"id": "2505.16988", "pdf": "https://arxiv.org/pdf/2505.16988", "abs": "https://arxiv.org/abs/2505.16988", "authors": ["Rui Ye", "Keduan Huang", "Qimin Wu", "Yuzhu Cai", "Tian Jin", "Xianghe Pang", "Xiangrui Liu", "Jiaqi Su", "Chen Qian", "Bohan Tang", "Kaiqu Liang", "Jiaao Chen", "Yue Hu", "Zhenfei Yin", "Rongye Shi", "Bo An", "Yang Gao", "Wenjun Wu", "Lei Bai", "Siheng Chen"], "title": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "18 pages, 11 figures", "summary": "LLM-based multi-agent systems (MAS) have demonstrated significant potential\nin enhancing single LLMs to address complex and diverse tasks in practical\napplications. Despite considerable advancements, the field lacks a unified\ncodebase that consolidates existing methods, resulting in redundant\nre-implementation efforts, unfair comparisons, and high entry barriers for\nresearchers. To address these challenges, we introduce MASLab, a unified,\ncomprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab\nintegrates over 20 established methods across multiple domains, each rigorously\nvalidated by comparing step-by-step outputs with its official implementation.\n(2) MASLab provides a unified environment with various benchmarks for fair\ncomparisons among methods, ensuring consistent inputs and standardized\nevaluation protocols. (3) MASLab implements methods within a shared streamlined\nstructure, lowering the barriers for understanding and extension. Building on\nMASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,\noffering researchers a clear and comprehensive view of the current landscape of\nMAS methods. MASLab will continue to evolve, tracking the latest developments\nin the field, and invite contributions from the broader open-source community."}
{"id": "2505.16994", "pdf": "https://arxiv.org/pdf/2505.16994", "abs": "https://arxiv.org/abs/2505.16994", "authors": ["Runyang You", "Yongqi Li", "Xinyu Lin", "Xin Zhang", "Wenjie Wang", "Wenjie Li", "Liqiang Nie"], "title": "$\\text{R}^2\\text{ec}$: Towards Large Recommender Models with Reasoning", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec."}
{"id": "2505.16998", "pdf": "https://arxiv.org/pdf/2505.16998", "abs": "https://arxiv.org/abs/2505.16998", "authors": ["Jin Jiang", "Jianing Wang", "Yuchen Yan", "Yang Liu", "Jianhua Zhu", "Mengdi Zhang", "Xunliang Cai", "Liangcai Gao"], "title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval."}
{"id": "2505.17002", "pdf": "https://arxiv.org/pdf/2505.17002", "abs": "https://arxiv.org/abs/2505.17002", "authors": ["Abdul Hannan", "Muhammad Arslan Manzoor", "Shah Nawaz", "Muhammad Irzam Liaqat", "Markus Schedl", "Mubashir Noman"], "title": "PAEFF: Precise Alignment and Enhanced Gated Feature Fusion for Face-Voice Association", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at InterSpeech 2025", "summary": "We study the task of learning association between faces and voices, which is\ngaining interest in the multimodal community lately. These methods suffer from\nthe deliberate crafting of negative mining procedures as well as the reliance\non the distant margin parameter. These issues are addressed by learning a joint\nembedding space in which orthogonality constraints are applied to the fused\nembeddings of faces and voices. However, embedding spaces of faces and voices\npossess different characteristics and require spaces to be aligned before\nfusing them. To this end, we propose a method that accurately aligns the\nembedding spaces and fuses them with an enhanced gated fusion thereby improving\nthe performance of face-voice association. Extensive experiments on the\nVoxCeleb dataset reveals the merits of the proposed approach."}
{"id": "2505.17004", "pdf": "https://arxiv.org/pdf/2505.17004", "abs": "https://arxiv.org/abs/2505.17004", "authors": ["Jiachen Yao", "Abbas Mammadov", "Julius Berner", "Gavin Kerrigan", "Jong Chul Ye", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "title": "Guided Diffusion Sampling on Function Spaces with Applications to PDEs", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "stat.ML"], "comment": null, "summary": "We propose a general framework for conditional sampling in PDE-based inverse\nproblems, targeting the recovery of whole solutions from extremely sparse or\nnoisy measurements. This is accomplished by a function-space diffusion model\nand plug-and-play guidance for conditioning. Our method first trains an\nunconditional discretization-agnostic denoising model using neural operator\narchitectures. At inference, we refine the samples to satisfy sparse\nobservation data via a gradient-based guidance mechanism. Through rigorous\nmathematical analysis, we extend Tweedie's formula to infinite-dimensional\nHilbert spaces, providing the theoretical foundation for our posterior sampling\napproach. Our method (FunDPS) accurately captures posterior distributions in\nfunction spaces under minimal supervision and severe data scarcity. Across five\nPDE tasks with only 3% observation, our method achieves an average 32% accuracy\nimprovement over state-of-the-art fixed-resolution diffusion baselines while\nreducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning\nensures strong cross-resolution generalizability. To the best of our knowledge,\nthis is the first diffusion-based framework to operate independently of\ndiscretization, offering a practical and flexible solution for forward and\ninverse problems in the context of PDEs. Code is available at\nhttps://github.com/neuraloperator/FunDPS"}
{"id": "2505.17005", "pdf": "https://arxiv.org/pdf/2505.17005", "abs": "https://arxiv.org/abs/2505.17005", "authors": ["Huatong Song", "Jinhao Jiang", "Wenqing Tian", "Zhipeng Chen", "Yuhuan Wu", "Jiahao Zhao", "Yingqian Min", "Wayne Xin Zhao", "Lei Fang", "Ji-Rong Wen"], "title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus."}
{"id": "2505.17010", "pdf": "https://arxiv.org/pdf/2505.17010", "abs": "https://arxiv.org/abs/2505.17010", "authors": ["Tim Genewein", "Kevin Wenliang Li", "Jordi Grau-Moya", "Anian Ruoss", "Laurent Orseau", "Marcus Hutter"], "title": "Understanding Prompt Tuning and In-Context Learning via Meta-Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Prompting is one of the main ways to adapt a pretrained model to target\ntasks. Besides manually constructing prompts, many prompt optimization methods\nhave been proposed in the literature. Method development is mainly empirically\ndriven, with less emphasis on a conceptual understanding of prompting. In this\npaper we discuss how optimal prompting can be understood through a Bayesian\nview, which also implies some fundamental limitations of prompting that can\nonly be overcome by tuning weights. The paper explains in detail how\nmeta-trained neural networks behave as Bayesian predictors over the pretraining\ndistribution, whose hallmark feature is rapid in-context adaptation. Optimal\nprompting can be studied formally as conditioning these Bayesian predictors,\nyielding criteria for target tasks where optimal prompting is and is not\npossible. We support the theory with educational experiments on LSTMs and\nTransformers, where we compare different versions of prefix-tuning and\ndifferent weight-tuning methods. We also confirm that soft prefixes, which are\nsequences of real-valued vectors outside the token alphabet, can lead to very\neffective prompts for trained and even untrained networks by manipulating\nactivations in ways that are not achievable by hard tokens. This adds an\nimportant mechanistic aspect beyond the conceptual Bayesian theory."}
{"id": "2505.17012", "pdf": "https://arxiv.org/pdf/2505.17012", "abs": "https://arxiv.org/abs/2505.17012", "authors": ["Haoning Wu", "Xiao Huang", "Yaohui Chen", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "title": "SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "Technical Report; Project Page:\n  https://haoningwu3639.github.io/SpatialScore", "summary": "Multimodal large language models (MLLMs) have achieved impressive success in\nquestion-answering tasks, yet their capabilities for spatial understanding are\nless explored. This work investigates a critical question: do existing MLLMs\npossess 3D spatial perception and understanding abilities? Concretely, we make\nthe following contributions in this paper: (i) we introduce VGBench, a\nbenchmark specifically designed to assess MLLMs for visual geometry perception,\ne.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most\ncomprehensive and diverse multimodal spatial understanding benchmark to date,\nintegrating VGBench with relevant data from the other 11 existing datasets.\nThis benchmark comprises 28K samples across various spatial understanding\ntasks, modalities, and QA formats, along with a carefully curated challenging\nsubset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent\nsystem incorporating 9 specialized tools for spatial understanding, supporting\nboth Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive\nevaluations to reveal persistent challenges in spatial reasoning while\ndemonstrating the effectiveness of SpatialAgent. We believe SpatialScore will\noffer valuable insights and serve as a rigorous benchmark for the next\nevolution of MLLMs."}
{"id": "2505.17016", "pdf": "https://arxiv.org/pdf/2505.17016", "abs": "https://arxiv.org/abs/2505.17016", "authors": ["Shuhan Tan", "Kairan Dou", "Yue Zhao", "Philipp Krähenbühl"], "title": "Interactive Post-Training for Vision-Language-Action Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "comment": "Project page: https://ariostgx.github.io/ript_vla/", "summary": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision."}
{"id": "2505.17017", "pdf": "https://arxiv.org/pdf/2505.17017", "abs": "https://arxiv.org/abs/2505.17017", "authors": ["Chengzhuo Tong", "Ziyu Guo", "Renrui Zhang", "Wenyu Shan", "Xinyu Wei", "Zhenghao Xing", "Hongsheng Li", "Pheng-Ann Heng"], "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT", "summary": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT"}
{"id": "2505.17019", "pdf": "https://arxiv.org/pdf/2505.17019", "abs": "https://arxiv.org/abs/2505.17019", "authors": ["Chenhao Zhang", "Yazhe Niu"], "title": "Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework", "categories": ["cs.CV", "cs.AI", "cs.CY"], "comment": "16 pages, 9 figures. Code & Dataset:\n  https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep", "summary": "Metaphorical comprehension in images remains a critical challenge for AI\nsystems, as existing models struggle to grasp the nuanced cultural, emotional,\nand contextual implications embedded in visual content. While multimodal large\nlanguage models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they\nstruggle with a fundamental limitation on image implication tasks: contextual\ngaps that obscure the relationships between different visual elements and their\nabstract meanings. Inspired by the human cognitive process, we propose Let\nAndroids Dream (LAD), a novel framework for image implication understanding and\nreasoning. LAD addresses contextual missing through the three-stage framework:\n(1) Perception: converting visual information into rich and multi-level textual\nrepresentations, (2) Search: iteratively searching and integrating cross-domain\nknowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment\nimage implication via explicit reasoning. Our framework with the lightweight\nGPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English\nimage implication benchmark and a huge improvement on Chinese benchmark,\nperforming comparable with the GPT-4o model on Multiple-Choice Question (MCQ)\nand outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work\nprovides new insights into how AI can more effectively interpret image\nimplications, advancing the field of vision-language reasoning and human-AI\ninteraction. Our project is publicly available at\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep."}
{"id": "2505.17022", "pdf": "https://arxiv.org/pdf/2505.17022", "abs": "https://arxiv.org/abs/2505.17022", "authors": ["Chengqi Duan", "Rongyao Fang", "Yuqing Wang", "Kun Wang", "Linjiang Huang", "Xingyu Zeng", "Hongsheng Li", "Xihui Liu"], "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Github page refer to: https://github.com/gogoduan/GoT-R1", "summary": "Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1."}
