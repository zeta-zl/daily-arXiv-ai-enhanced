<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 78]
- [cs.LG](#cs.LG) [Total: 171]
- [cs.AI](#cs.AI) [Total: 41]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.MA](#cs.MA) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 10]
- [hep-th](#hep-th) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]
- [cs.CV](#cs.CV) [Total: 35]
- [cs.CY](#cs.CY) [Total: 3]
- [eess.SP](#eess.SP) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 5]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.SY](#cs.SY) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.RO](#cs.RO) [Total: 12]
- [cs.HC](#cs.HC) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [cond-mat.quant-gas](#cond-mat.quant-gas) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [math.ST](#math.ST) [Total: 1]
- [cs.GR](#cs.GR) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Veracity: An Open-Source AI Fact-Checking System](https://arxiv.org/abs/2506.15794)
*Taylor Lynn Curtis,Maximilian Puelma Touzel,William Garneau,Manon Gruaz,Mike Pinder,Li Wei Wang,Sukanya Krishna,Luda Cohen,Jean-François Godbout,Reihaneh Rabbany,Kellin Pelrine*

Key words: 虚假信息, 事实核查, 大语言模型, 开源系统, 媒体素养

TL;DR: Veracity是一个开源AI系统，通过结合大语言模型（LLMs）和网络检索代理，帮助用户进行透明且易于访问的事实核查，以对抗虚假信息。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 虚假信息的泛滥对社会构成重大威胁，而生成式AI的能力加剧了这一情况。Veracity旨在通过透明的工具赋能个人对抗虚假信息。

Method: Veracity结合LLMs和网络检索代理，分析用户提交的声明，并提供基于直觉的解释和数值化评分。

Result: 系统具备多语言支持、直观的交互界面，并能够解释其推理过程，从而增强媒体素养和社会认知。

Conclusion: Veracity不仅能够检测虚假信息，还能够通过解释推理过程促进公众对信息的理性判断。

Abstract: The proliferation of misinformation poses a significant threat to society,
exacerbated by the capabilities of generative AI. This demo paper introduces
Veracity, an open-source AI system designed to empower individuals to combat
misinformation through transparent and accessible fact-checking. Veracity
leverages the synergy between Large Language Models (LLMs) and web retrieval
agents to analyze user-submitted claims and provide grounded veracity
assessments with intuitive explanations. Key features include multilingual
support, numerical scoring of claim veracity, and an interactive interface
inspired by familiar messaging applications. This paper will showcase
Veracity's ability to not only detect misinformation but also explain its
reasoning, fostering media literacy and promoting a more informed society.

</details>


### [2] [Rethinking LLM Training through Information Geometry and Quantum Metrics](https://arxiv.org/abs/2506.15830)
*Riccardo Di Sipio*

Key words: 大语言模型, 信息几何, 自然梯度下降, 量子优化, Fisher信息

TL;DR: 本文探讨了大语言模型（LLMs）优化中的参数空间几何结构及其信息几何视角，强调了自然梯度下降的理论优势，并提出了量子类比的可能性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究LLMs在高维参数空间中的优化问题，利用信息几何和自然梯度下降提供更理论化的学习框架。

Method: 通过Fisher信息度量和自然梯度下降分析LLM优化中的几何特性，并探讨量子系统中的Fubini-Study度量和量子Fisher信息。

Result: 信息几何视角有助于解释LLMs训练中的尖锐最小值、泛化能力和标度定律等现象。

Conclusion: 曲率感知的优化方法能深化对LLM训练的理解，量子系统可能为高效优化提供新方向。

Abstract: Optimization in large language models (LLMs) unfolds over high-dimensional
parameter spaces with non-Euclidean structure. Information geometry frames this
landscape using the Fisher information metric, enabling more principled
learning via natural gradient descent. Though often impractical, this geometric
lens clarifies phenomena such as sharp minima, generalization, and observed
scaling laws. We argue that curvature-aware approaches deepen our understanding
of LLM training. Finally, we speculate on quantum analogies based on the
Fubini-Study metric and Quantum Fisher Information, hinting at efficient
optimization in quantum-enhanced systems.

</details>


### [3] [MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents](https://arxiv.org/abs/2506.15841)
*Zijian Zhou,Ao Qu,Zhaoxuan Wu,Sunghwan Kim,Alok Prakash,Daniela Rus,Jinhua Zhao,Bryan Kian Hsiang Low,Paul Pu Liang*

Key words: 语言代理,强化学习,长时任务,内存优化,多轮交互

TL;DR: MEM1是一种基于强化学习的框架，通过恒定内存处理长时多轮任务，显著提升性能并减少内存使用。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有LLM系统依赖全上下文提示，导致内存无限增长、计算成本增加及长尾输入性能下降。

Method: MEM1采用强化学习框架，通过更新紧凑的共享内部状态整合记忆与新观测，并丢弃无关信息。

Result: 在多个领域测试中，MEM1-7B性能提升3.5倍，内存减少3.7倍，且能泛化到训练范围之外。

Conclusion: MEM1展示了推理驱动的记忆整合作为高效长时交互代理训练方案的潜力。

Abstract: Modern language agents must operate over long-horizon, multi-turn
interactions, where they retrieve external information, adapt to observations,
and answer interdependent queries. Yet, most LLM systems rely on full-context
prompting, appending all past turns regardless of their relevance. This leads
to unbounded memory growth, increased computational costs, and degraded
reasoning performance on out-of-distribution input lengths. We introduce MEM1,
an end-to-end reinforcement learning framework that enables agents to operate
with constant memory across long multi-turn tasks. At each turn, MEM1 updates a
compact shared internal state that jointly supports memory consolidation and
reasoning. This state integrates prior memory with new observations from the
environment while strategically discarding irrelevant or redundant information.
To support training in more realistic and compositional settings, we propose a
simple yet effective and scalable approach to constructing multi-turn
environments by composing existing datasets into arbitrarily complex task
sequences. Experiments across three domains, including internal retrieval QA,
open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves
performance by 3.5x while reducing memory usage by 3.7x compared to
Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes
beyond the training horizon. Our results demonstrate the promise of
reasoning-driven memory consolidation as a scalable alternative to existing
solutions for training long-horizon interactive agents, where both efficiency
and performance are optimized.

</details>


### [4] [Finance Language Model Evaluation (FLaME)](https://arxiv.org/abs/2506.15846)
*Glenn Matlin,Mika Okamoto,Huzaifa Pardawala,Yang Yang,Sudheer Chava*

Key words: 语言模型,金融NLP,基准测试,FLaME

TL;DR: 本文介绍了第一个金融语言模型评估（FLaME）的全面基准测试套件，展示了语言模型在专业金融任务中的潜力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于现有评估方法的局限性，语言模型在金融领域的潜力被低估，亟需更全面的评估框架。

Method: 提出了FLaME基准测试套件，对23个基础语言模型在20个核心金融NLP任务上进行了实证研究。

Result: 研究结果表明语言模型在金融NLP任务中具有未被充分挖掘的潜力。

Conclusion: FLaME框架及其开源资源为金融领域的语言模型评估提供了新标准。

Abstract: Language Models (LMs) have demonstrated impressive capabilities with core
Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly
specialized knowledge-intensive tasks in finance remains difficult to assess
due to major gaps in the methodologies of existing evaluation frameworks, which
have caused an erroneous belief in a far lower bound of LMs' performance on
common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for
these FinNLP tasks, we present the first holistic benchmarking suite for
Financial Language Model Evaluation (FLaME). We are the first research paper to
comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical
study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source
our framework software along with all data and results.

</details>


### [5] [Entropy-Driven Pre-Tokenization for Byte-Pair Encoding](https://arxiv.org/abs/2506.15889)
*Yifan Hu,Frank Liang,Dachuan Zhao,Jonathan Geuter,Varshini Reddy,Craig W. Schmidt,Chris Tanner*

Key words: BPE, 中文分词, 熵信息, 预训练模型

TL;DR: 本文提出了两种基于熵信息的预分词策略，以改进BPE在中文等无分隔符语言中的分词效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: BPE在无分隔符语言（如中文）中的应用面临挑战，因为其频率驱动的合并操作忽略语言边界。

Method: 提出两种策略：一是利用点互信息和左右熵识别连贯字符跨度；二是利用预训练GPT-2的预测熵检测边界不确定性。

Result: 在PKU数据集上的实验表明，两种方法显著提升了分词精度、召回率和F1分数。

Conclusion: 熵引导的预分词不仅能更好地对齐语言单位，还为低资源和多语言场景下的分词质量提升提供了方向。

Abstract: Byte-Pair Encoding (BPE) has become a widely adopted subword tokenization
method in modern language models due to its simplicity and strong empirical
performance across downstream tasks. However, applying BPE to unsegmented
languages such as Chinese presents significant challenges, as its
frequency-driven merge operation is agnostic to linguistic boundaries. To
address this, we propose two entropy-informed pre-tokenization strategies that
guide BPE segmentation using unsupervised information-theoretic cues. The first
approach uses pointwise mutual information and left/right entropy to identify
coherent character spans, while the second leverages predictive entropy derived
from a pretrained GPT-2 model to detect boundary uncertainty. We evaluate both
methods on a subset of the PKU dataset and demonstrate substantial improvements
in segmentation precision, recall, and F1 score compared to standard BPE. Our
results suggest that entropy-guided pre-tokenization not only enhances
alignment with gold-standard linguistic units but also offers a promising
direction for improving tokenization quality in low-resource and multilingual
settings.

</details>


### [6] [Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning](https://arxiv.org/abs/2506.15894)
*Sam Silver,Jimin Sun,Ivan Zhang,Sara Hooker,Eddie Kim*

Key words: 大型语言模型,自我纠正,数学推理,Chain of Thought

TL;DR: LLMs表现出一定的自我纠正能力，即使未经专门微调也能在推理过程中纠正错误。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究LLMs在数学推理中的自我纠正能力，尤其是对问题描述和提示策略变化的脆弱性。

Method: 通过实验测量模型对合成扰动自我纠正的能力，包括隐式和显式纠正。

Result: LLMs展现出较强的内在自我纠正能力，即使未经长链微调。

Conclusion: LLMs的推理能力可能是对其已有特性的放大。

Abstract: Large Language Models (LLMs) have demonstrated impressive mathematical
reasoning capabilities, yet their performance remains brittle to minor
variations in problem description and prompting strategy. Furthermore,
reasoning is vulnerable to sampling-induced errors which autoregressive models
must primarily address using self-correction via additionally-generated tokens.
To better understand self-correction capabilities of recent models, we conduct
experiments measuring models' ability to self-correct synthetic perturbations
introduced into their Chain of Thought (CoT) reasoning. We observe robust
single-utterance intrinsic self-correction behavior across a range of
open-weight models and datasets, ranging from subtle, implicit corrections to
explicit acknowledgments and corrections of errors. Our findings suggest that
LLMs, including those not finetuned for long CoT, may possess stronger
intrinsic self-correction capabilities than commonly shown in the literature.
The presence of this ability suggests that recent "reasoning" model work
involves amplification of traits already meaningfully present in models.

</details>


### [7] [From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents](https://arxiv.org/abs/2506.15911)
*Mohammad Amaan Sayeed,Mohammed Talha Alam,Raza Imam,Shahab Saquib Sohail,Amir Hussain*

Key words: 伊斯兰医学、AI评估、检索增强生成、文化敏感

TL;DR: 论文提出了Tibbe-AG评估框架，结合伊斯兰医学经典与AI技术，提升文化敏感医疗问答的准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有AI系统未能充分利用伊斯兰医学文本，且缺乏基于文化的医疗问答验证方法。

Method: 提出Tibbe-AG框架，评估三种LLM模型在不同配置下的表现，包括检索增强生成与自我批判过滤器。

Result: 检索提升事实准确性13%，代理提示进一步改善10%。

Conclusion: 结合伊斯兰经典与AI技术可实现可靠且文化敏感的医疗问答。

Abstract: Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the
Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and
holistic therapies, yet remain inaccessible to many and underutilized in modern
AI systems. Existing language-model benchmarks focus narrowly on factual recall
or user preference, leaving a gap in validating culturally grounded medical
guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that
aligns 30 carefully curated Prophetic-medicine questions with human-verified
remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three
configurations: direct generation, retrieval-augmented generation, and a
scientific self-critique filter. Each answer is then assessed by a secondary
LLM serving as an agentic judge, yielding a single 3C3H quality score.
Retrieval improves factual accuracy by 13%, while the agentic prompt adds
another 10% improvement through deeper mechanistic insight and safety
considerations. Our results demonstrate that blending classical Islamic texts
with retrieval and self-evaluation enables reliable, culturally sensitive
medical question-answering.

</details>


### [8] [Reranking-based Generation for Unbiased Perspective Summarization](https://arxiv.org/abs/2506.15925)
*Narutatsu Ri,Nicholas Deas,Kathleen McKeown*

Key words: 大型语言模型, 政治观点摘要, 评估指标, 重排序, 偏好调优

TL;DR: 本文研究了大型语言模型（LLMs）在政治观点摘要生成中如何可靠评估摘要质量，并提出改进方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在政治观点摘要等实际应用中，生成无偏见的摘要至关重要，但现有评估框架依赖传统指标且改进方法尚不成熟。

Method: （1）识别衡量观点摘要质量的可靠指标，（2）研究LLM方法在零样本推理之外的有效性，包括使用重排序和偏好调优。

Result: 实验表明，基于语言模型的指标优于传统指标，且重排序和偏好调优能显著提升性能。

Conclusion: 本文为观点摘要的可靠评估和开发提供了有益发现和方法。

Abstract: Generating unbiased summaries in real-world settings such as political
perspective summarization remains a crucial application of Large Language
Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics
for measuring key attributes such as coverage and faithfulness without
verifying their applicability, and efforts to develop improved summarizers are
still nascent. We address these gaps by (1) identifying reliable metrics for
measuring perspective summary quality, and (2) investigating the efficacy of
LLM-based methods beyond zero-shot inference. Namely, we build a test set for
benchmarking metric reliability using human annotations and show that
traditional metrics underperform compared to language model-based metrics,
which prove to be strong evaluators. Using these metrics, we show that
reranking-based methods yield strong results, and preference tuning with
synthetically generated and reranking-labeled data further boosts performance.
Our findings aim to contribute to the reliable evaluation and development of
perspective summarization methods.

</details>


### [9] [A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension](https://arxiv.org/abs/2506.15978)
*Toan Nguyen Hai,Ha Nguyen Viet,Truong Quan Xuan,Duc Do Minh*

Key words: 越南语, 自然语言处理, 文本分割, 机器阅读理解, 多语言模型, mBERT

TL;DR: 本文介绍了VSMRC数据集，一个针对越南语的文本分割和机器阅读理解任务的数据集，填补了该语言在NLP资源上的空白。实验表明，多语言模型（如mBERT）在此类任务上表现优于单语言模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 越南语作为一种全球第20大语言，拥有超过1.02亿母语者，但在文本分割和机器阅读理解等NLP任务上缺乏高质量资源。VSMRC旨在填补这一空白，为越南语的NLP研究提供可靠且多样化的数据集。

Method: 作者从越南语维基百科中获取数据，创建了包含15,942份文本分割文档和16,347对人工质检的多选题问答对的数据集。实验中对比了多语言模型（mBERT）与单语言模型的性能。

Result: mBERT在文本分割和阅读理解任务上均表现优于单语言模型，分别达到63.15%的F1分数和88.01%的准确率。

Conclusion: 研究表明，多语言模型在越南语NLP任务中表现优异，并有望推广至其他资源匮乏的语言。VSMRC已公开发布于HuggingFace平台。

Abstract: Vietnamese, the 20th most spoken language with over 102 million native
speakers, lacks robust resources for key natural language processing tasks such
as text segmentation and machine reading comprehension (MRC). To address this
gap, we present VSMRC, the Vietnamese Text Segmentation and Multiple-Choice
Reading Comprehension Dataset. Sourced from Vietnamese Wikipedia, our dataset
includes 15,942 documents for text segmentation and 16,347 synthetic
multiple-choice question-answer pairs generated with human quality assurance,
ensuring a reliable and diverse resource. Experiments show that mBERT
consistently outperforms monolingual models on both tasks, achieving an
accuracy of 88.01% on MRC test set and an F1 score of 63.15\% on text
segmentation test set. Our analysis reveals that multilingual models excel in
NLP tasks for Vietnamese, suggesting potential applications to other
under-resourced languages. VSMRC is available at HuggingFace

</details>


### [10] [Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion](https://arxiv.org/abs/2506.15981)
*Markus Frohmann,Gabriel Meseguer-Brocal,Markus Schedl,Elena V. Epure*

Key words: AI音乐检测, 多模态, 鲁棒性, 歌词转录, 音频特征

TL;DR: 提出了一种多模态、模块化的后期融合方法DE-detect，结合自动转录的歌词和音频中的语音特征，提高AI生成音乐检测的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: AI音乐生成工具的快速发展带来了挑战，需要可靠的检测方法。现有基于音频或歌词的检测器存在局限性。

Method: 采用多模态、模块化的后期融合方法，结合音频转录歌词和语音特征。

Result: DE-detect优于现有歌词检测器，对音频扰动的鲁棒性更强。

Conclusion: 该方法为检测AI生成音乐提供了实用的解决方案。

Abstract: The rapid advancement of AI-based music generation tools is revolutionizing
the music industry but also posing challenges to artists, copyright holders,
and providers alike. This necessitates reliable methods for detecting such
AI-generated content. However, existing detectors, relying on either audio or
lyrics, face key practical limitations: audio-based detectors fail to
generalize to new or unseen generators and are vulnerable to audio
perturbations; lyrics-based methods require cleanly formatted and accurate
lyrics, unavailable in practice. To overcome these limitations, we propose a
novel, practically grounded approach: a multimodal, modular late-fusion
pipeline that combines automatically transcribed sung lyrics and speech
features capturing lyrics-related information within the audio. By relying on
lyrical aspects directly from audio, our method enhances robustness, mitigates
susceptibility to low-level artifacts, and enables practical applicability.
Experiments show that our method, DE-detect, outperforms existing lyrics-based
detectors while also being more robust to audio perturbations. Thus, it offers
an effective, robust solution for detecting AI-generated music in real-world
scenarios. Our code is available at
https://github.com/deezer/robust-AI-lyrics-detection.

</details>


### [11] [From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation](https://arxiv.org/abs/2506.16024)
*Zhihan Guo,Jiele Wu,Wenqian Cui,Yifei Zhang,Minda Hu,Yufei Wang,Irwin King*

Key words: LLM, Open-LTG, 强化学习, ProxyReward

TL;DR: 论文提出ProxyReward框架，通过强化学习解决Open-LTG任务中缺乏黄金标准数据的问题。其数据集和奖励信号计算方法显著提升了模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前LLM在长文本生成任务中缺乏有效的评估和训练数据，ProxyReward旨在填补这一空白。

Method: 采用强化学习框架ProxyReward，包括自动生成数据集和针对性奖励信号计算。

Result: ProxyReward在Open-LTG任务中表现优于GPT-4-Turbo，性能提升20%，并超越LLM-as-a-Judge方法。

Conclusion: ProxyReward为LLM处理复杂开放性问题提供了有效工具。

Abstract: Current research on long-form context in Large Language Models (LLMs)
primarily focuses on the understanding of long-contexts, the Open-ended Long
Text Generation (Open-LTG) remains insufficiently explored. Training a
long-context generation model requires curation of gold standard reference
data, which is typically nonexistent for informative Open-LTG tasks. However,
previous methods only utilize general assessments as reward signals, which
limits accuracy. To bridge this gap, we introduce ProxyReward, an innovative
reinforcement learning (RL) based framework, which includes a dataset and a
reward signal computation method. Firstly, ProxyReward Dataset generation is
accomplished through simple prompts that enables the model to create
automatically, obviating extensive labeled data or significant manual effort.
Secondly, ProxyReward Signal offers a targeted evaluation of information
comprehensiveness and accuracy for specific questions. The experimental results
indicate that our method ProxyReward surpasses even GPT-4-Turbo. It can
significantly enhance performance by 20% on the Open-LTG task when training
widely used open-source models, while also surpassing the LLM-as-a-Judge
approach. Our work presents effective methods to enhance the ability of LLMs to
address complex open-ended questions posed by human.

</details>


### [12] [EvoLM: In Search of Lost Language Model Training Dynamics](https://arxiv.org/abs/2506.16029)
*Zhenting Qi,Fan Nie,Alexandre Alahi,James Zou,Himabindu Lakkaraju,Yilun Du,Eric Xing,Sham Kakade,Hanlin Zhang*

Key words: 语言模型、训练动态、预训练、微调、强化学习

TL;DR: EvoLM模型套件系统分析了语言模型在多个训练阶段（如预训练、微调和强化学习）的动态，揭示了过度训练的收益递减、持续预训练的重要性等关键发现，并开源了所有模型和工具。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了解决语言模型训练阶段设计选择对下游开发的影响不透明的问题，作者提出了EvoLM套件，以便系统分析训练动态。

Method: 通过从头训练100多个1B和4B参数的模型，评估了上游和下游能力，包括领域内外泛化。

Result: 研究发现过度训练的收益递减、持续预训练的关键作用以及在微调和强化学习中的复杂权衡。

Conclusion: EvoLM为语言模型训练提供透明分析工具，并开源资源促进研究。

Abstract: Modern language model (LM) training has been divided into multiple stages,
making it difficult for downstream developers to evaluate the impact of design
choices made at each stage. We present EvoLM, a model suite that enables
systematic and transparent analysis of LMs' training dynamics across
pre-training, continued pre-training, supervised fine-tuning, and reinforcement
learning. By training over 100 LMs with 1B and 4B parameters from scratch, we
rigorously evaluate both upstream (language modeling) and downstream
(problem-solving) reasoning capabilities, including considerations of both
in-domain and out-of-domain generalization. Key insights highlight the
diminishing returns from excessive pre-training and post-training, the
importance and practices of mitigating forgetting during domain-specific
continued pre-training, the crucial role of continued pre-training in bridging
pre-training and post-training phases, and various intricate trade-offs when
configuring supervised fine-tuning and reinforcement learning. To facilitate
open research and reproducibility, we release all pre-trained and post-trained
models, training datasets for all stages, and our entire training and
evaluation pipeline.

</details>


### [13] [Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3](https://arxiv.org/abs/2506.16037)
*Xinyue Huang,Ziqi Lin,Fang Sun,Wenchao Zhang,Kejian Tong,Yunbo Liu*

Key words: Retrieval-Augmented Generation, multi-hop reasoning, LLaMA 3, dense retrieval, context fusion

TL;DR: 提出了一种新检索增强生成（RAG）框架，专注于复杂问答任务，解决多跳推理和长文档上下文理解的挑战。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的检索增强和生成模型在多跳推理和长文档上下文理解方面存在不足，需改进以实现更准确的回答。

Method: 基于LLaMA 3，结合稠密检索模块与高级上下文融合和多跳推理机制，采用联合优化策略提升鲁棒性和适应性。

Result: 实验结果表明，该系统优于现有检索增强和生成基线，能够生成更精确且上下文相关的答案。

Conclusion: 该RAG框架在复杂问答任务中表现出色，验证了其有效性和实用性。

Abstract: This paper presents a novel Retrieval-Augmented Generation (RAG) framework
tailored for complex question answering tasks, addressing challenges in
multi-hop reasoning and contextual understanding across lengthy documents.
Built upon LLaMA 3, the framework integrates a dense retrieval module with
advanced context fusion and multi-hop reasoning mechanisms, enabling more
accurate and coherent response generation. A joint optimization strategy
combining retrieval likelihood and generation cross-entropy improves the
model's robustness and adaptability. Experimental results show that the
proposed system outperforms existing retrieval-augmented and generative
baselines, confirming its effectiveness in delivering precise, contextually
grounded answers.

</details>


### [14] [DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling](https://arxiv.org/abs/2506.16043)
*Fei Wang,Xingchen Wan,Ruoxi Sun,Jiefeng Chen,Sercan Ö. Arık*

Key words: 大语言模型,推理时间扩展,动态预算分配,多臂老虎机,计算效率

TL;DR: DynScaling通过集成并行-顺序采样策略和动态预算分配框架，提升大语言模型在资源约束下的性能，无需外部验证器。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前推理时间扩展方法依赖外部验证器或未针对实际计算约束优化，限制了其实际应用。

Method: 提出DynScaling，包括集成并行-顺序采样策略和基于多臂老虎机的动态预算分配框架。

Result: 实验表明DynScaling在任务性能和计算成本上均优于现有基线。

Conclusion: DynScaling有效解决了资源约束下的大语言模型性能提升问题，无需外部验证器。

Abstract: Inference-time scaling has proven effective in boosting large language model
(LLM) performance through increased test-time computation. Yet, its practical
application is often hindered by reliance on external verifiers or a lack of
optimization for realistic computational constraints. We propose DynScaling,
which addresses these limitations through two primary innovations: an
integrated parallel-sequential sampling strategy and a bandit-based dynamic
budget allocation framework. The integrated sampling strategy unifies parallel
and sequential sampling by constructing synthetic sequential reasoning chains
from initially independent parallel responses, promoting diverse and coherent
reasoning trajectories. The dynamic budget allocation framework formulates the
allocation of computational resources as a multi-armed bandit problem,
adaptively distributing the inference budget across queries based on the
uncertainty of previously sampled responses, thereby maximizing computational
efficiency. By combining these components, DynScaling effectively improves LLM
performance under practical resource constraints without the need for external
verifiers. Experimental results demonstrate that DynScaling consistently
surpasses existing verifier-free inference scaling baselines in both task
performance and computational cost.

</details>


### [15] [A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text](https://arxiv.org/abs/2506.16052)
*Devesh Kumar*

Key words: 网络欺凌, Transformer, 广度学习系统, 可解释性

TL;DR: 该论文提出了一种混合架构，结合了Transformer模型和广度学习系统的优势，用于检测网络欺凌，性能优于现有方法，并具备可解释性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 网络欺凌对青少年影响严重，需要高效且透明的检测方法。

Method: 提出ModifiedDeBERTa + GBLS模型，结合了改进的DeBERTa和门控广度学习系统，并集成情感分析和可解释机制。

Result: 在多个数据集上表现优异，最高准确率达95.41%。

Conclusion: 该框架在检测网络欺凌方面表现出色，但仍有改进空间，尤其是在识别隐含偏见和讽刺内容时。

Abstract: The proliferation of online communication platforms has created unprecedented
opportunities for global connectivity while simultaneously enabling harmful
behaviors such as cyberbullying, which affects approximately 54.4\% of
teenagers according to recent research. This paper presents a hybrid
architecture that combines the contextual understanding capabilities of
transformer-based models with the pattern recognition strengths of broad
learning systems for effective cyberbullying detection. This approach
integrates a modified DeBERTa model augmented with Squeeze-and-Excitation
blocks and sentiment analysis capabilities with a Gated Broad Learning System
(GBLS) classifier, creating a synergistic framework that outperforms existing
approaches across multiple benchmark datasets. The proposed ModifiedDeBERTa +
GBLS model achieved good performance on four English datasets: 79.3\% accuracy
on HateXplain, 95.41\% accuracy on SOSNet, 91.37\% accuracy on Mendeley-I, and
94.67\% accuracy on Mendeley-II. Beyond performance gains, the framework
incorporates comprehensive explainability mechanisms including token-level
attribution analysis, LIME-based local interpretations, and confidence
calibration, addressing critical transparency requirements in automated content
moderation. Ablation studies confirm the meaningful contribution of each
architectural component, while failure case analysis reveals specific
challenges in detecting implicit bias and sarcastic content, providing valuable
insights for future improvements in cyberbullying detection systems.

</details>


### [16] [Knee-Deep in C-RASP: A Transformer Depth Hierarchy](https://arxiv.org/abs/2506.16055)
*Andy Yang,Michaël Cadilhac,David Chiang*

Key words: Transformer, 表达能力, C-RASP, 时间逻辑, 序列依赖任务

TL;DR: 本文通过理论和实证研究证明，更深层的Transformer（在固定精度和注意力机制的特定子类中）表达能力更强，并建立了其与C-RASP编程语言的等价性。结果还表明深度与任务的序列依赖性需求相关。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究更深的Transformer模型是否具有更强的表达能力，并通过理论证明和实证验证这一问题。

Method: 1. 研究固定精度的Transformer子类与C-RASP编程语言的等价性；2. 通过时间逻辑和计数操作符证明更深层的C-RASP程序更具表达能力；3. 实证验证无位置编码的Transformer在序列依赖任务上的泛化能力。

Result: 理论证明和实证结果表明，更深层的Transformer模型在其特定子类中确实具有更强的表达能力，并能更好地处理序列依赖任务。

Conclusion: 深度与Transformer的表达能力直接相关，尤其是在处理序列依赖任务时，更深层的模型表现更好。

Abstract: It has been observed that transformers with greater depth (that is, more
layers) have more capabilities, but can we establish formally which
capabilities are gained with greater depth? We answer this question with a
theoretical proof followed by an empirical study. First, we consider
transformers that round to fixed precision except inside attention. We show
that this subclass of transformers is expressively equivalent to the
programming language C-RASP and this equivalence preserves depth. Second, we
prove that deeper C-RASP programs are more expressive than shallower C-RASP
programs, implying that deeper transformers are more expressive than shallower
transformers (within the subclass mentioned above). These results are
established by studying a form of temporal logic with counting operators, which
was shown equivalent to C-RASP in previous work. Finally, we provide empirical
evidence that our theory predicts the depth required for transformers without
positional encodings to length-generalize on a family of sequential dependency
tasks.

</details>


### [17] [Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning](https://arxiv.org/abs/2506.16064)
*Duc Hieu Ho,Chenglin Fan*

Key words: 大语言模型, 诚实性, 有用性, 自我批判, 提示策略

TL;DR: 论文提出了一种名为'自我批判引导的好奇心优化提示'的新策略，通过轻量级的上下文步骤改进大语言模型的输出质量，无需额外训练。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管大语言模型在多种任务中表现优异，但其输出的诚实性和有用性仍需提升。

Method: 对10种主流大语言模型进行基准评估，并提出包含自我批判和优化步骤的提示策略。

Result: 在HONESET数据集上的实验表明，该方法显著提高了输出质量，H²分数相对提升1.4%至4.3%。

Conclusion: 结构化自我优化是一种可扩展且无需训练的策略，能有效提升大语言模型的可信度。

Abstract: Large language models (LLMs) have demonstrated robust capabilities across
various natural language tasks. However, producing outputs that are
consistently honest and helpful remains an open challenge. To overcome this
challenge, this paper tackles the problem through two complementary directions.
It conducts a comprehensive benchmark evaluation of ten widely used large
language models, including both proprietary and open-weight models from OpenAI,
Meta, and Google. In parallel, it proposes a novel prompting strategy,
self-critique-guided curiosity refinement prompting. The key idea behind this
strategy is enabling models to self-critique and refine their responses without
additional training. The proposed method extends the curiosity-driven prompting
strategy by incorporating two lightweight in-context steps including
self-critique step and refinement step.
  The experiment results on the HONESET dataset evaluated using the framework
$\mathrm{H}^2$ (honesty and helpfulness), which was executed with GPT-4o as a
judge of honesty and helpfulness, show consistent improvements across all
models. The approach reduces the number of poor-quality responses, increases
high-quality responses, and achieves relative gains in $\mathrm{H}^2$ scores
ranging from 1.4% to 4.3% compared to curiosity-driven prompting across
evaluated models. These results highlight the effectiveness of structured
self-refinement as a scalable and training-free strategy to improve the
trustworthiness of LLMs outputs.

</details>


### [18] [Cyberbullying Detection in Hinglish Text Using MURIL and Explainable AI](https://arxiv.org/abs/2506.16066)
*Devesh Kumar*

Key words: 网络欺凌检测, Hinglish, MURIL, 多语言模型

TL;DR: 该论文提出了一种基于MURIL架构的框架，用于检测Hinglish（印地语-英语混合语）文本中的网络欺凌，展示了优于现有多语言模型的性能，并提供了可解释性分析和未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于数字平台上Hinglish文本的增加，现有的单语网络欺凌检测系统面临挑战，亟需一种有效的多语言检测方法。

Method: 使用MURIL架构，结合选择性层冻结、分类头设计和专门预处理技术，对六种基准数据集进行评估。

Result: 在多个数据集上表现出优于RoBERTa和IndicBERT的性能，准确率提升1.36至13.07个百分点。

Conclusion: 该框架为多语言网络欺凌检测提供了新方法，但仍需解决语境和文化因素等挑战。

Abstract: The growth of digital communication platforms has led to increased
cyberbullying incidents worldwide, creating a need for automated detection
systems to protect users. The rise of code-mixed Hindi-English (Hinglish)
communication on digital platforms poses challenges for existing cyberbullying
detection systems, which were designed primarily for monolingual text. This
paper presents a framework for cyberbullying detection in Hinglish text using
the Multilingual Representations for Indian Languages (MURIL) architecture to
address limitations in current approaches. Evaluation across six benchmark
datasets -- Bohra \textit{et al.}, BullyExplain, BullySentemo, Kumar \textit{et
al.}, HASOC 2021, and Mendeley Indo-HateSpeech -- shows that the MURIL-based
approach outperforms existing multilingual models including RoBERTa and
IndicBERT, with improvements of 1.36 to 13.07 percentage points and accuracies
of 86.97\% on Bohra, 84.62\% on BullyExplain, 86.03\% on BullySentemo, 75.41\%
on Kumar datasets, 83.92\% on HASOC 2021, and 94.63\% on Mendeley dataset. The
framework includes explainability features through attribution analysis and
cross-linguistic pattern recognition. Ablation studies show that selective
layer freezing, appropriate classification head design, and specialized
preprocessing for code-mixed content improve detection performance, while
failure analysis identifies challenges including context-dependent
interpretation, cultural understanding, and cross-linguistic sarcasm detection,
providing directions for future research in multilingual cyberbullying
detection.

</details>


### [19] [FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning](https://arxiv.org/abs/2506.16123)
*Natapong Nitarach,Warit Sirichotedumrong,Panop Pitchayarthorn,Pittawat Taveekitworachai,Potsawee Manakul,Kunat Pipatanakul*

Key words: FinCoT, 结构化思维链, 金融NLP, 提示工程, 领域知识

TL;DR: FinCoT是一种结构化的思维链提示方法，通过结合金融领域专业知识，显著提升大型语言模型在金融任务中的表现，同时降低推理成本。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 金融领域的自然语言处理（FinNLP）中，结构化思维链提示方法的研究较少，且缺乏领域专业知识指导。FinCoT旨在填补这一空白。

Method: 提出FinCoT，结合金融专家知识设计结构化提示，并与零样本提示、非结构化思维链提示及其他结构化提示方法进行对比实验。

Result: FinCoT在CFA-style问题上将准确率从63.2%提升至80.5%，同时生成令牌数减少八倍，推理成本降低。

Conclusion: 领域对齐的结构化提示不仅提升性能和降低成本，还生成更可解释且符合专家推理的轨迹。

Abstract: This paper presents FinCoT, a structured chain-of-thought (CoT) prompting
approach that incorporates insights from domain-specific expert financial
reasoning to guide the reasoning traces of large language models. We
investigate that there are three main prompting styles in FinNLP: (1) standard
prompting--zero-shot prompting; (2) unstructured CoT--CoT prompting without an
explicit reasoning structure, such as the use of tags; and (3) structured CoT
prompting--CoT prompting with explicit instructions or examples that define
structured reasoning steps. Previously, FinNLP has primarily focused on prompt
engineering with either standard or unstructured CoT prompting. However,
structured CoT prompting has received limited attention in prior work.
Furthermore, the design of reasoning structures in structured CoT prompting is
often based on heuristics from non-domain experts. In this study, we
investigate each prompting approach in FinNLP. We evaluate the three main
prompting styles and FinCoT on CFA-style questions spanning ten financial
domains. We observe that FinCoT improves performance from 63.2% to 80.5% and
Qwen-2.5-7B-Instruct from 69.7% to 74.2%, while reducing generated tokens
eight-fold compared to structured CoT prompting. Our findings show that
domain-aligned structured prompts not only improve performance and reduce
inference costs but also yield more interpretable and expert-aligned reasoning
traces.

</details>


### [20] [Under the Shadow of Babel: How Language Shapes Reasoning in LLMs](https://arxiv.org/abs/2506.16151)
*Chenxi Wang,Yixuan Zhang,Lang Gao,Zixiang Xu,Zirui Song,Yanbo Wang,Xiuying Chen*

Key words: 大型语言模型, 因果推理, 双语数据集, 认知语言学, 注意力模式

TL;DR: 论文研究了语言结构如何影响大型语言模型（LLM）的逻辑推理，通过双语数据集BICAUSE发现LLM会内化语言的推理偏好。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 语言不仅是交流工具，还影响认知和推理模式。研究旨在验证LLM是否会内化不同语言中的逻辑结构。

Method: 使用双语数据集BICAUSE（中文和英语），分析LLM在因果推理中的表现和内部注意力模式。

Result: LLM展现了与语言类型一致的注意力模式，内化语言特定的因果词序偏好，但在异常输入时表现下降；成功的推理表现为跨语言的语义对齐。

Conclusion: LLM不仅模仿语言表面形式，还内化了语言塑造的推理偏见，首次通过模型内部结构分析验证了认知语言学理论。

Abstract: Language is not only a tool for communication but also a medium for human
cognition and reasoning. If, as linguistic relativity suggests, the structure
of language shapes cognitive patterns, then large language models (LLMs)
trained on human language may also internalize the habitual logical structures
embedded in different languages. To examine this hypothesis, we introduce
BICAUSE, a structured bilingual dataset for causal reasoning, which includes
semantically aligned Chinese and English samples in both forward and reversed
causal forms. Our study reveals three key findings: (1) LLMs exhibit
typologically aligned attention patterns, focusing more on causes and
sentence-initial connectives in Chinese, while showing a more balanced
distribution in English. (2) Models internalize language-specific preferences
for causal word order and often rigidly apply them to atypical inputs, leading
to degraded performance, especially in Chinese. (3) When causal reasoning
succeeds, model representations converge toward semantically aligned
abstractions across languages, indicating a shared understanding beyond surface
form. Overall, these results suggest that LLMs not only mimic surface
linguistic forms but also internalize the reasoning biases shaped by language.
Rooted in cognitive linguistic theory, this phenomenon is for the first time
empirically verified through structural analysis of model internals.

</details>


### [21] [SGIC: A Self-Guided Iterative Calibration Framework for RAG](https://arxiv.org/abs/2506.16172)
*Guanhua Chen,Yutong Yao,Lidia S. Chao,Xuebo Liu,Derek F. Wong*

Key words: 检索增强生成, 大语言模型, 校准, 不确定性评分, SGIC框架

TL;DR: 本文提出了SGIC框架，通过多次校准改进检索增强生成（RAG）中LLMs的校准效果，利用不确定性评分优化文档相关性和LLM回答信心。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有RAG方法常忽视LLMs的校准能力，而LLMs的上下文推理能力强，可以通过特定提示显著提升校准效果。

Method: 提出SGIC框架，利用不确定性评分迭代校准文档相关性和LLM回答信心，并构建自校准训练集优化LLMs。

Result: SGIC框架显著提升了闭源和开源LLMs的性能。

Conclusion: 通过SGIC框架，LLMs在检索增强生成中的校准能力得到显著提升。

Abstract: Recent research in retrieval-augmented generation (RAG) has concentrated on
retrieving useful information from candidate documents. However, numerous
methodologies frequently neglect the calibration capabilities of large language
models (LLMs), which capitalize on their robust in-context reasoning prowess.
This work illustrates that providing LLMs with specific cues substantially
improves their calibration efficacy, especially in multi-round calibrations. We
present a new SGIC: Self-Guided Iterative Calibration Framework that employs
uncertainty scores as a tool. Initially, this framework calculates uncertainty
scores to determine both the relevance of each document to the query and the
confidence level in the responses produced by the LLMs. Subsequently, it
reevaluates these scores iteratively, amalgamating them with prior responses to
refine calibration. Furthermore, we introduce an innovative approach for
constructing an iterative self-calibration training set, which optimizes LLMs
to efficiently harness uncertainty scores for capturing critical information
and enhancing response accuracy. Our proposed framework significantly improves
performance on both closed-source and open-weight LLMs.

</details>


### [22] [JETHICS: Japanese Ethics Understanding Evaluation Dataset](https://arxiv.org/abs/2506.16187)
*Masashi Takeshita,Rafal Rzepka*

Key words: JETHICS, 伦理理解, 日语数据集, 大语言模型, 评估

TL;DR: 介绍了JETHICS数据集，用于评估AI模型的伦理理解能力，包含78K示例，基于现有英文ETHICS数据集构建。实验表明当前LLM表现仍有较大提升空间。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 构建一个日语数据集JETHICS，以评估AI模型对伦理问题的理解能力，填补日语领域相关研究的空白。

Method: 基于英文ETHICS数据集的构建方法，JETHICS包含78K示例，涉及伦理学和政治哲学的四个类别及常识道德。实验对非专有LLM和GPT-4o进行了评估。

Result: GPT-4o平均得分为0.7，表现最佳的日语LLM约为0.5，表明当前LLM在伦理理解方面仍有较大改进空间。

Conclusion: JETHICS为日语AI伦理评估提供了重要工具，实验结果显示LLM的伦理理解能力有待进一步提升。

Abstract: In this work, we propose JETHICS, a Japanese dataset for evaluating ethics
understanding of AI models. JETHICS contains 78K examples and is built by
following the construction methods of the existing English ETHICS dataset. It
includes four categories based normative theories and concepts from ethics and
political philosophy; and one representing commonsense morality. Our evaluation
experiments on non-proprietary large language models (LLMs) and on GPT-4o
reveal that even GPT-4o achieves only an average score of about 0.7, while the
best-performing Japanese LLM attains around 0.5, indicating a relatively large
room for improvement in current LLMs.

</details>


### [23] [Web(er) of Hate: A Survey on How Hate Speech Is Typed](https://arxiv.org/abs/2506.16190)
*Luna Wang,Andrew Caines,Alice Hutchings*

Key words: 仇恨言论数据集,方法论,Max Weber,理想类型,透明性

TL;DR: 对仇恨言论数据集的管理需要平衡多种设计决策，本文通过批判性分析不同数据集的方法论选择，提出基于马克斯·韦伯‘理想类型’的反思性构建方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨仇恨言论数据集构建中方法论的多样性与潜在问题，强调透明性和方法论严谨性的重要性。

Method: 采用批判性分析方法，结合Max Weber的‘理想类型’理论，提出反思性数据集构建框架。

Result: 揭示了数据集构建中常见的主题和实践，及其对数据集可靠性的影响。

Conclusion: 呼吁研究人员在数据集构建中明确自身的价值判断，以提升透明性和方法论严谨性。

Abstract: The curation of hate speech datasets involves complex design decisions that
balance competing priorities. This paper critically examines these
methodological choices in a diverse range of datasets, highlighting common
themes and practices, and their implications for dataset reliability. Drawing
on Max Weber's notion of ideal types, we argue for a reflexive approach in
dataset creation, urging researchers to acknowledge their own value judgments
during dataset construction, fostering transparency and methodological rigour.

</details>


### [24] [Comparative Analysis of Abstractive Summarization Models for Clinical Radiology Reports](https://arxiv.org/abs/2506.16247)
*Anindita Bhattacharya,Tohida Rehman,Debarshi Kumar Sanyal,Samiran Chattopadhyay*

Key words: 摘要生成、放射学报告、大型语言模型、医疗文本摘要、MIMIC-CXR

TL;DR: 该研究探讨了如何使用先进的摘要生成模型，从放射学报告的结果部分生成简洁的印象部分。通过比较多种预训练和开源大语言模型的性能，为医疗领域的自动化摘要提供参考。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 放射学报告的结果部分通常冗长详细，而印象部分则更简洁并包含关键诊断结论。研究旨在通过先进的摘要生成技术，自动从结果部分生成印象，以提高医疗效率。

Method: 使用公开的MIMIC-CXR数据集，比较了T5-base、BART-base、PEGASUS-x-base、ChatGPT-4、LLaMA-3-8B和自定义的Pointer Generator Network模型。评估指标包括ROUGE-1、ROUGE-2、ROUGE-L、METEOR和BERTScore。

Result: 研究分析了不同模型的性能，识别了它们在医疗文本摘要中的优势和局限性。

Conclusion: 该研究为医疗专业人士提供了在医疗领域使用自动化摘要解决方案的有用信息。

Abstract: The findings section of a radiology report is often detailed and lengthy,
whereas the impression section is comparatively more compact and captures key
diagnostic conclusions. This research explores the use of advanced abstractive
summarization models to generate the concise impression from the findings
section of a radiology report. We have used the publicly available MIMIC-CXR
dataset. A comparative analysis is conducted on leading pre-trained and
open-source large language models, including T5-base, BART-base,
PEGASUS-x-base, ChatGPT-4, LLaMA-3-8B, and a custom Pointer Generator Network
with a coverage mechanism. To ensure a thorough assessment, multiple evaluation
metrics are employed, including ROUGE-1, ROUGE-2, ROUGE-L, METEOR, and
BERTScore. By analyzing the performance of these models, this study identifies
their respective strengths and limitations in the summarization of medical
text. The findings of this paper provide helpful information for medical
professionals who need automated summarization solutions in the healthcare
sector.

</details>


### [25] [End-to-End Speech Translation for Low-Resource Languages Using Weakly Labeled Data](https://arxiv.org/abs/2506.16251)
*Aishwarya Pothula,Bhavana Akkiraju,Srihari Bandarupalli,Charan D,Santosh Kesiraju,Anil Kumar Vuppala*

Key words: 语音到文本翻译，低资源语言，弱标注数据，双语文本挖掘，多模态

TL;DR: 摘要探讨了利用弱标注数据构建低资源语言的端到端语音到文本翻译系统，并通过Shrutilipi-anuvaad数据集验证了其可行性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 低资源语言的高质量标注数据稀缺，限制了语音到文本翻译系统的开发。

Method: 利用双语文本挖掘和最新句子编码器构建弱标注数据集Shrutilipi-anuvaad，并研究数据质量与数量对模型性能的影响。

Result: 弱标注数据构建的语音到文本翻译系统性能接近大规模多模态多语言基线模型（如SONAR和SeamlessM4T）。

Conclusion: 弱标注数据可用于构建低资源语言的语音到文本翻译系统，性能可媲美传统大规模模型。

Abstract: The scarcity of high-quality annotated data presents a significant challenge
in developing effective end-to-end speech-to-text translation (ST) systems,
particularly for low-resource languages. This paper explores the hypothesis
that weakly labeled data can be used to build ST models for low-resource
language pairs. We constructed speech-to-text translation datasets with the
help of bitext mining using state-of-the-art sentence encoders. We mined the
multilingual Shrutilipi corpus to build Shrutilipi-anuvaad, a dataset
comprising ST data for language pairs Bengali-Hindi, Malayalam-Hindi,
Odia-Hindi, and Telugu-Hindi. We created multiple versions of training data
with varying degrees of quality and quantity to investigate the effect of
quality versus quantity of weakly labeled data on ST model performance. Results
demonstrate that ST systems can be built using weakly labeled data, with
performance comparable to massive multi-modal multilingual baselines such as
SONAR and SeamlessM4T.

</details>


### [26] [Advancing Automated Speaking Assessment Leveraging Multifaceted Relevance and Grammar Information](https://arxiv.org/abs/2506.16285)
*Hao-Chien Lu,Jhen-Ke Lin,Hong-Yun Lin,Chung-Chun Wang,Berlin Chen*

Key words: 自动口语评估,内容相关性,语法错误校正,混合评分模型

TL;DR: 本文提出了一种混合评分模型，通过改进内容相关性和语法错误分析，提升了自动口语评估系统的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有自动口语评估系统在多方面评估中未能充分利用内容相关性，且语法分析较为粗浅。

Method: 引入多维相关性模块和细粒度语法错误特征，结合问题和相关图像、范例及L2学习者回答进行全面评估。

Result: 实验和消融研究表明，新模型显著提升了内容相关性、语言使用及整体评估性能。

Conclusion: 采用更丰富、细致的特征集对口语评估具有显著改进作用。

Abstract: Current automated speaking assessment (ASA) systems for use in multi-aspect
evaluations often fail to make full use of content relevance, overlooking image
or exemplar cues, and employ superficial grammar analysis that lacks detailed
error types. This paper ameliorates these deficiencies by introducing two novel
enhancements to construct a hybrid scoring model. First, a multifaceted
relevance module integrates question and the associated image content,
exemplar, and spoken response of an L2 speaker for a comprehensive assessment
of content relevance. Second, fine-grained grammar error features are derived
using advanced grammar error correction (GEC) and detailed annotation to
identify specific error categories. Experiments and ablation studies
demonstrate that these components significantly improve the evaluation of
content relevance, language use, and overall ASA performance, highlighting the
benefits of using richer, more nuanced feature sets for holistic speaking
assessment.

</details>


### [27] [PL-Guard: Benchmarking Language Model Safety for Polish](https://arxiv.org/abs/2506.16322)
*Aleksandra Krasnodębska,Karolina Seweryn,Szymon Łukasik,Wojciech Kusa*

Key words: 语言模型安全, 波兰语, 对抗样本, 多语言评估

TL;DR: 该研究旨在填补语言模型安全性评估中对多语言支持的不足，通过创建波兰语的安全分类基准数据集及对抗样本，测试了多种模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有大型语言模型的安全评估和调节工具大多偏向英语等高资源语言，忽略了对多数全球语言的覆盖。

Method: 研究团队手动标注了一个波兰语的安全分类数据集，并生成了对抗性样本，评估了包括Llama-Guard-3-8B、HerBERT分类器和PLLuM等多种模型的性能。

Result: 实验结果表明，基于HerBERT的分类器在对抗条件下表现最佳。

Conclusion: 该研究突出了在多语言环境下对语言模型进行安全性评估的重要性，并展示了HerBERT在波兰语环境中的优越性能。

Abstract: Despite increasing efforts to ensure the safety of large language models
(LLMs), most existing safety assessments and moderation tools remain heavily
biased toward English and other high-resource languages, leaving majority of
global languages underexamined. To address this gap, we introduce a manually
annotated benchmark dataset for language model safety classification in Polish.
We also create adversarially perturbed variants of these samples designed to
challenge model robustness. We conduct a series of experiments to evaluate
LLM-based and classifier-based models of varying sizes and architectures.
Specifically, we fine-tune three models: Llama-Guard-3-8B, a HerBERT-based
classifier (a Polish BERT derivative), and PLLuM, a Polish-adapted Llama-8B
model. We train these models using different combinations of annotated data and
evaluate their performance, comparing it against publicly available guard
models. Results demonstrate that the HerBERT-based classifier achieves the
highest overall performance, particularly under adversarial conditions.

</details>


### [28] [Generalizability of Media Frames: Corpus creation and analysis across countries](https://arxiv.org/abs/2506.16337)
*Agnese Daffara,Sourabh Dattawad,Sebastian Padó,Tanise Ceron*

Key words: 媒体框架、跨文化研究、巴西新闻、自然语言处理、FrameNews-PT

TL;DR: 这篇论文研究了如何在巴西新闻中应用美国的媒体框架（MFC），并通过FrameNews-PT数据集验证其跨文化适用性，同时评估了模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机在于探索美国媒体框架（MFC）是否适用于其他文化背景的新闻报道，尤其是巴西的政治和经济新闻。

Method: 通过创建FrameNews-PT数据集，并对巴西葡萄牙语新闻文章进行多轮标注，评估MFC框架的适用性。此外，测试了微调和零样本模型在域外数据上的表现。

Result: 结果表明，15个MFC框架大多适用，但需要少量修改；某些框架使用频率低，新议题依赖通用的“后备”框架。

Conclusion: 跨文化框架应用需要谨慎考虑，不同文化背景下的新闻报道可能需要调整框架或引入新类别。

Abstract: Frames capture aspects of an issue that are emphasized in a debate by
interlocutors and can help us understand how political language conveys
different perspectives and ultimately shapes people's opinions. The Media Frame
Corpus (MFC) is the most commonly used framework with categories and detailed
guidelines for operationalizing frames. It is, however, focused on a few
salient U.S. news issues, making it unclear how well these frames can capture
news issues in other cultural contexts. To explore this, we introduce
FrameNews-PT, a dataset of Brazilian Portuguese news articles covering
political and economic news and annotate it within the MFC framework. Through
several annotation rounds, we evaluate the extent to which MFC frames
generalize to the Brazilian debate issues. We further evaluate how fine-tuned
and zero-shot models perform on out-of-domain data. Results show that the 15
MFC frames remain broadly applicable with minor revisions of the guidelines.
However, some MFC frames are rarely used, and novel news issues are analyzed
using general 'fall-back' frames. We conclude that cross-cultural frame use
requires careful consideration.

</details>


### [29] [Analyzing the Influence of Knowledge Graph Information on Relation Extraction](https://arxiv.org/abs/2506.16343)
*Cedric Möller,Ricardo Usbeck*

Key words: 知识图谱, 关系抽取, Neural Bellman-Ford网络, 监督学习, 零样本学习

TL;DR: 论文研究了知识图谱信息对关系抽取模型性能的影响，通过实验证明其显著提升效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探究知识图谱中实体位置信息对关系抽取任务的重要性。

Method: 结合传统关系抽取方法与图感知的Neural Bellman-Ford网络，测试监督和零样本设置。

Result: 融入知识图谱信息显著提升模型性能，尤其在训练样本不平衡时效果更明显。

Conclusion: 知识图谱信息对关系抽取任务有重要价值，能够稳定提升模型性能。

Abstract: We examine the impact of incorporating knowledge graph information on the
performance of relation extraction models across a range of datasets. Our
hypothesis is that the positions of entities within a knowledge graph provide
important insights for relation extraction tasks. We conduct experiments on
multiple datasets, each varying in the number of relations, training examples,
and underlying knowledge graphs. Our results demonstrate that integrating
knowledge graph information significantly enhances performance, especially when
dealing with an imbalance in the number of training examples for each relation.
We evaluate the contribution of knowledge graph-based features by combining
established relation extraction methods with graph-aware Neural Bellman-Ford
networks. These features are tested in both supervised and zero-shot settings,
demonstrating consistent performance improvements across various datasets.

</details>


### [30] [DISCIE -- Discriminative Closed Information Extraction](https://arxiv.org/abs/2506.16348)
*Cedric Möller,Ricardo Usbeck*

Key words: 封闭信息提取,关系提取,判别式方法,类型信息,长尾关系

TL;DR: 本文提出了一种新颖的封闭信息提取方法，通过整合类型和实体特定信息，提升了关系提取的准确性，尤其在长尾关系中表现突出。该方法在效率和性能上均优于现有端到端生成模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决大规模封闭信息提取（涉及数百万实体和数百种关系）中的准确性和效率问题，尤其是在长尾关系上表现不佳的生成模型。

Method: 采用判别式方法，结合类型和实体特定信息，利用较小模型提升效率。

Result: 在性能上优于现有端到端生成模型，尤其在长尾关系中表现显著。类型信息的整合使较小模型达到或超越大型生成模型的性能。

Conclusion: 该方法为信息提取技术提供了更准确和高效的解决方案，尤其适用于大规模封闭信息提取场景。

Abstract: This paper introduces a novel method for closed information extraction. The
method employs a discriminative approach that incorporates type and
entity-specific information to improve relation extraction accuracy,
particularly benefiting long-tail relations. Notably, this method demonstrates
superior performance compared to state-of-the-art end-to-end generative models.
This is especially evident for the problem of large-scale closed information
extraction where we are confronted with millions of entities and hundreds of
relations. Furthermore, we emphasize the efficiency aspect by leveraging
smaller models. In particular, the integration of type-information proves
instrumental in achieving performance levels on par with or surpassing those of
a larger generative model. This advancement holds promise for more accurate and
efficient information extraction techniques.

</details>


### [31] [Can structural correspondences ground real world representational content in Large Language Models?](https://arxiv.org/abs/2506.16370)
*Iwan Williams*

Key words: 大型语言模型,表示能力,结构对应关系,任务表现,文本局限性

TL;DR: 论文探讨了大型语言模型（LLMs）如GPT-4的表示能力，提出其能否通过结构对应关系表示现实世界内容，并提出成功任务表现是验证其表示能力的关键。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究LLMs是否能够表示现实世界内容，以及这种表示的条件和限制。

Method: 基于结构对应关系的表示理论，分析LLMs的结构与世界的对应关系，并探讨其在任务表现中的作用。

Result: 仅存在结构对应关系不足以支持LLMs的表示能力，但若这些关系能解释任务成功，则可能支持现实世界内容的表示。

Conclusion: LLMs的文本局限性可能阻碍其在适当任务中表现出表示能力，但若能解决这一问题，结构对应关系可能为表示提供基础。

Abstract: Large Language Models (LLMs) such as GPT-4 produce compelling responses to a
wide range of prompts. But their representational capacities are uncertain.
Many LLMs have no direct contact with extra-linguistic reality: their inputs,
outputs and training data consist solely of text, raising the questions (1) can
LLMs represent anything and (2) if so, what? In this paper, I explore what it
would take to answer these questions according to a structural-correspondence
based account of representation, and make an initial survey of this evidence. I
argue that the mere existence of structural correspondences between LLMs and
worldly entities is insufficient to ground representation of those entities.
However, if these structural correspondences play an appropriate role - they
are exploited in a way that explains successful task performance - then they
could ground real world contents. This requires overcoming a challenge: the
text-boundedness of LLMs appears, on the face of it, to prevent them engaging
in the right sorts of tasks.

</details>


### [32] [InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems](https://arxiv.org/abs/2506.16381)
*Kexin Huang,Qian Tu,Liwei Fan,Chenchen Yang,Dong Zhang,Shimin Li,Zhaoye Fei,Qinyuan Cheng,Xipeng Qiu*

Key words: speech synthesis, TTS, paralinguistic, benchmark, instruction-following

TL;DR: 提出InstructTTSEval基准，用于评估基于自然语言指令的TTS系统在复杂风格控制上的能力，包含三个任务和多语言测试用例。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统TTS系统在控制副语言信息时灵活性不足，且缺乏高质量基准和自动化评估指标，阻碍了模型优化。

Method: 引入InstructTTSEval基准，包含三个任务（声学参数指定、描述性风格指令和角色扮演）及6k测试用例，使用Gemini自动评估。

Result: 评估显示现有指令驱动TTS系统仍有较大改进空间。

Conclusion: InstructTTSEval有望推动更强大、灵活和准确的指令驱动TTS发展。

Abstract: In modern speech synthesis, paralinguistic information--such as a speaker's
vocal timbre, emotional state, and dynamic prosody--plays a critical role in
conveying nuance beyond mere semantics. Traditional Text-to-Speech (TTS)
systems rely on fixed style labels or inserting a speech prompt to control
these cues, which severely limits flexibility. Recent attempts seek to employ
natural-language instructions to modulate paralinguistic features,
substantially improving the generalization of instruction-driven TTS models.
Although many TTS systems now support customized synthesis via textual
description, their actual ability to interpret and execute complex instructions
remains largely unexplored. In addition, there is still a shortage of
high-quality benchmarks and automated evaluation metrics specifically designed
for instruction-based TTS, which hinders accurate assessment and iterative
optimization of these models. To address these limitations, we introduce
InstructTTSEval, a benchmark for measuring the capability of complex
natural-language style control. We introduce three tasks, namely
Acoustic-Parameter Specification, Descriptive-Style Directive, and Role-Play,
including English and Chinese subsets, each with 1k test cases (6k in total)
paired with reference audio. We leverage Gemini as an automatic judge to assess
their instruction-following abilities. Our evaluation of accessible
instruction-following TTS systems highlights substantial room for further
improvement. We anticipate that InstructTTSEval will drive progress toward more
powerful, flexible, and accurate instruction-following TTS.

</details>


### [33] [Large Language Models in Argument Mining: A Survey](https://arxiv.org/abs/2506.16383)
*Hao Li,Viktor Schlegel,Yizheng Sun,Riza Batista-Navarro,Goran Nenadic*

Key words: 论点挖掘,大型语言模型,自然语言处理,提示学习,链式思维推理

TL;DR: 本文综述了大型语言模型（LLMs）在论点挖掘（AM）领域的应用进展，包括理论基础、数据集、方法分类及未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索LLMs如何革新论点挖掘领域，推动该领域的进一步发展。

Method: 系统梳理LLMs在AM中的应用，包括提示学习、链式思维推理等，并提出分类法。

Result: 总结了LLMs在AM中的优势与挑战，如长上下文推理和可解释性问题。

Conclusion: LLMs为AM带来新机遇，但需解决技术挑战，未来研究应聚焦于这些方向。

Abstract: Argument Mining (AM), a critical subfield of Natural Language Processing
(NLP), focuses on extracting argumentative structures from text. The advent of
Large Language Models (LLMs) has profoundly transformed AM, enabling advanced
in-context learning, prompt-based generation, and robust cross-domain
adaptability. This survey systematically synthesizes recent advancements in
LLM-driven AM. We provide a concise review of foundational theories and
annotation frameworks, alongside a meticulously curated catalog of datasets. A
key contribution is our comprehensive taxonomy of AM subtasks, elucidating how
contemporary LLM techniques -- such as prompting, chain-of-thought reasoning,
and retrieval augmentation -- have reconfigured their execution. We further
detail current LLM architectures and methodologies, critically assess
evaluation practices, and delineate pivotal challenges including long-context
reasoning, interpretability, and annotation bottlenecks. Conclusively, we
highlight emerging trends and propose a forward-looking research agenda for
LLM-based computational argumentation, aiming to strategically guide
researchers in this rapidly evolving domain.

</details>


### [34] [HausaNLP at SemEval-2025 Task 11: Advancing Hausa Text-based Emotion Detection](https://arxiv.org/abs/2506.16388)
*Sani Abdullahi Sani,Salim Abubakar,Falalu Ibrahim Lawan,Abdulhamid Abubakar,Maryam Bala*

Key words: 多标签情感检测, Hausa语言, AfriBERTa, Transformer, 低资源语言

TL;DR: 论文介绍了针对低资源非洲语言Hausa的多标签情感检测方法，通过微调AfriBERTa模型实现情感分类，验证准确率达74%。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决低资源语言Hausa的情感检测问题，探索基于Transformer的模型在此类任务中的有效性。

Method: 数据预处理、分词后，使用Hugging Face Trainer API微调预训练的AfriBERTa模型。

Result: 验证准确率为74.00%，F1分数为73.50%，表明Transformer模型在低资源语言中表现良好。

Conclusion: 基于Transformer的模型适用于低资源语言的情感检测任务。

Abstract: This paper presents our approach to multi-label emotion detection in Hausa, a
low-resource African language, as part of SemEval Track A. We fine-tuned
AfriBERTa, a transformer-based model pre-trained on African languages, to
classify Hausa text into six emotions: anger, disgust, fear, joy, sadness, and
surprise. Our methodology involved data preprocessing, tokenization, and model
fine-tuning using the Hugging Face Trainer API. The system achieved a
validation accuracy of 74.00%, with an F1-score of 73.50%, demonstrating the
effectiveness of transformer-based models for emotion detection in low-resource
languages.

</details>


### [35] [RiOT: Efficient Prompt Refinement with Residual Optimization Tree](https://arxiv.org/abs/2506.16389)
*Chenyi Zhou,Zhengyan Shi,Yuan Yao,Lei Liang,Huajun Chen,Qiang Zhang*

Key words: 大型语言模型, 提示优化, 文本梯度, 语义漂移, 树结构

TL;DR: 论文提出了Residual Optimization Tree (RiOT)框架，用于自动优化大型语言模型(LLMs)的提示，解决现有方法中多样性不足和语义漂移问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有自动提示优化方法缺乏多样性且易导致语义漂移，影响了LLMs的性能。

Method: RiOT通过文本梯度迭代优化提示，生成多样候选，利用困惑度选择最优提示，并采用文本残差连接减轻语义漂移，树结构管理优化过程。

Result: 在五个基准测试中，RiOT表现优于以往提示优化方法和手动提示。

Conclusion: RiOT有效解决了提示优化中的多样性和语义漂移问题，显著提升了LLMs性能。

Abstract: Recent advancements in large language models (LLMs) have highlighted their
potential across a variety of tasks, but their performance still heavily relies
on the design of effective prompts. Existing methods for automatic prompt
optimization face two challenges: lack of diversity, limiting the exploration
of valuable and innovative directions and semantic drift, where optimizations
for one task can degrade performance in others. To address these issues, we
propose Residual Optimization Tree (RiOT), a novel framework for automatic
prompt optimization. RiOT iteratively refines prompts through text gradients,
generating multiple semantically diverse candidates at each step, and selects
the best prompt using perplexity. Additionally, RiOT incorporates the text
residual connection to mitigate semantic drift by selectively retaining
beneficial content across optimization iterations. A tree structure efficiently
manages the optimization process, ensuring scalability and flexibility.
Extensive experiments across five benchmarks, covering commonsense,
mathematical, logical, temporal, and semantic reasoning, demonstrate that RiOT
outperforms both previous prompt optimization methods and manual prompting.

</details>


### [36] [From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling](https://arxiv.org/abs/2506.16393)
*Yao Lu,Zhaiyuan Ji,Jiawei Du,Yu Shanqing,Qi Xuan,Tianyi Zhou*

Key words: LLM, SLM, 标注框架, 多模型协作, 低成本标注

TL;DR: 提出了多模型协作标注的新范式AutoAnnotator，通过LLM和SLM的协同工作，显著降低标注成本并提升精度。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决LLM在大规模标注中的高成本和特定场景下精度不足的问题。

Method: 设计了两层框架：上层元控制器选择SLM并验证困难样本，下层任务专家通过多模型投票标注，并结合持续学习优化SLM。

Result: 在多种设置下优于现有开源/API LLM，标注成本降低74.15%，精度提升6.21%。

Conclusion: AutoAnnotator为高效、低成本标注提供了有效解决方案。

Abstract: Although the annotation paradigm based on Large Language Models (LLMs) has
made significant breakthroughs in recent years, its actual deployment still has
two core bottlenecks: first, the cost of calling commercial APIs in large-scale
annotation is very expensive; second, in scenarios that require fine-grained
semantic understanding, such as sentiment classification and toxicity
classification, the annotation accuracy of LLMs is even lower than that of
Small Language Models (SLMs) dedicated to this field. To address these
problems, we propose a new paradigm of multi-model cooperative annotation and
design a fully automatic annotation framework AutoAnnotator based on this.
Specifically, AutoAnnotator consists of two layers. The upper-level
meta-controller layer uses the generation and reasoning capabilities of LLMs to
select SLMs for annotation, automatically generate annotation code and verify
difficult samples; the lower-level task-specialist layer consists of multiple
SLMs that perform annotation through multi-model voting. In addition, we use
the difficult samples obtained by the secondary review of the meta-controller
layer as the reinforcement learning set and fine-tune the SLMs in stages
through a continual learning strategy, thereby improving the generalization of
SLMs. Extensive experiments show that AutoAnnotator outperforms existing
open-source/API LLMs in zero-shot, one-shot, CoT, and majority voting settings.
Notably, AutoAnnotator reduces the annotation cost by 74.15% compared to
directly annotating with GPT-3.5-turbo, while still improving the accuracy by
6.21%. Project page: https://github.com/Zhaiyuan-Ji/AutoAnnotator.

</details>


### [37] [OJBench: A Competition Level Code Benchmark For Large Language Models](https://arxiv.org/abs/2506.16395)
*Zhexu Wang,Yiping Liu,Yejie Wang,Wenyang He,Bofei Gao,Muxi Diao,Yanxu Chen,Kelin Fu,Flood Sung,Zhilin Yang,Tianyu Liu,Weiran Xu*

Key words: 大型语言模型,代码推理,基准测试,OJBench,编程竞赛

TL;DR: OJBench是一个新的挑战性基准测试，用于评估大型语言模型在竞争级代码推理能力上的表现，结果显示即使是先进模型也难以应对。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有代码基准测试在评估大型语言模型的全面能力，尤其是竞争级代码推理能力方面存在不足。

Method: 引入OJBench，包含232个来自NOI和ICPC的编程竞赛问题，对37种模型进行全面评估。

Result: 结果显示，即使是先进模型（如o4-mini和Gemini-2.5-pro-exp）也难以解决高难度竞赛级问题。

Conclusion: 竞争级代码推理对模型来说仍具挑战性。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
significant progress in math and code reasoning capabilities. However, existing
code benchmark are limited in their ability to evaluate the full spectrum of
these capabilities, particularly at the competitive level. To bridge this gap,
we introduce OJBench, a novel and challenging benchmark designed to assess the
competitive-level code reasoning abilities of LLMs. OJBench comprises 232
programming competition problems from NOI and ICPC, providing a more rigorous
test of models' reasoning skills. We conducted a comprehensive evaluation using
OJBench on 37 models, including both closed-source and open-source models,
reasoning-oriented and non-reasoning-oriented models. Our results indicate that
even state-of-the-art reasoning-oriented models, such as o4-mini and
Gemini-2.5-pro-exp, struggle with highly challenging competition-level
problems. This highlights the significant challenges that models face in
competitive-level code reasoning.

</details>


### [38] [NepaliGPT: A Generative Language Model for the Nepali Language](https://arxiv.org/abs/2506.16399)
*Shushanta Pudasaini,Aman Shakya,Siddhartha Shrestha,Sahil Bhatta,Sunil Thapa,Sushmita Palikhe*

Key words: 尼泊尔语, 大语言模型, NepaliGPT, Devanagari Corpus, 基准数据集

TL;DR: 该研究提出了针对尼泊尔语的生成型大语言模型NepaliGPT，填补了尼泊尔语NLP领域的空白，并引入了Devanagari Corpus和首个尼泊尔语基准数据集。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于缺乏尼泊尔语的生成型语言模型，限制了相关下游任务的研究，因此需要开发专门的尼泊尔语大语言模型。

Method: 通过收集多个来源的数据构建Devanagari Corpus，并创建包含4,296个问答对的基准数据集，开发了NepaliGPT模型。

Result: NepaliGPT在文本生成中表现出色，Perplexity为26.32245，ROUGE-1得分为0.2604，因果连贯性81.25%，因果一致性85.41%。

Conclusion: 研究填补了尼泊尔语生成模型的空白，为后续任务提供了基础。

Abstract: After the release of ChatGPT, Large Language Models (LLMs) have gained huge
popularity in recent days and thousands of variants of LLMs have been released.
However, there is no generative language model for the Nepali language, due to
which other downstream tasks, including fine-tuning, have not been explored
yet. To fill this research gap in the Nepali NLP space, this research proposes
\textit{NepaliGPT}, a generative large language model tailored specifically for
the Nepali language. This research introduces an advanced corpus for the Nepali
language collected from several sources, called the Devanagari Corpus.
Likewise, the research introduces the first NepaliGPT benchmark dataset
comprised of 4,296 question-answer pairs in the Nepali language. The proposed
LLM NepaliGPT achieves the following metrics in text generation: Perplexity of
26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25\%, and causal
consistency of 85.41\%.

</details>


### [39] [When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework](https://arxiv.org/abs/2506.16411)
*Zhen Xu,Shang Zhu,Jue Wang,Junlin Wang,Ben Athiwaratkun,Chi Wang,James Zou,Ce Zhang*

Key words: 大语言模型, 长文本处理, 跨块依赖, 模型噪声, 聚合噪声, 多代理分块

TL;DR: 论文提出了一种理论框架，将大语言模型（LLMs）处理长文本时的失败模式分为三类：跨块依赖（任务噪声）、随上下文增长的混淆（模型噪声）以及部分结果的不完美整合（聚合噪声）。通过实验验证了多代理分块方法的有效性，并解释了为何在处理大型输入时，配置分块处理的较弱模型可能优于一次性处理的高级模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究如何有效应用大语言模型处理长文本，识别并分类其失败模式，提出解决方案。

Method: 提出理论框架，分析三种噪声模式，通过多代理分块方法处理长文本任务，并进行实验验证。

Result: 实验证实分块处理的优势，尤其在大型输入下，较弱模型通过分块处理可能超越一次性处理的高级模型。

Conclusion: 理论框架和实验结果提供了处理长文本的有效途径，强调分块和聚合策略的重要性。

Abstract: We investigate the challenge of applying Large Language Models (LLMs) to long
texts. We propose a theoretical framework that distinguishes the failure modes
of long context tasks into three categories: cross-chunk dependence (task
noise), confusion that grows with context size (model noise), and the imperfect
integration of partial results (aggregator noise). Under this view, we analyze
when it is effective to use multi-agent chunking, i.e., dividing a length
sequence into smaller chunks and aggregating the processed results of each
chunk. Our experiments on tasks such as retrieval, question answering, and
summarization confirm both the theoretical analysis and the conditions that
favor multi-agent chunking. By exploring superlinear model noise growth with
input length, we also explain why, for large inputs, a weaker model configured
with chunk-based processing can surpass a more advanced model like GPT4o
applied in a single shot. Overall, we present a principled understanding
framework and our results highlight a direct pathway to handling long contexts
in LLMs with carefully managed chunking and aggregator strategies.

</details>


### [40] [REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing](https://arxiv.org/abs/2506.16444)
*Kangqi Chen,Andreas Kosmas Kakolyris,Rakesh Nadig,Manos Frouzakis,Nika Mansouri Ghiasi,Yu Liang,Haiyu Mao,Jisung Park,Mohammad Sadrosadati,Onur Mutlu*

Key words: 大型语言模型, 检索增强生成, 嵌入式处理, 数据检索

TL;DR: 论文提出了REIS系统，通过优化的数据布局和嵌入式处理技术，显著提升了RAG中的检索性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型（LLMs）的知识受限于训练数据，RAG通过外部知识库补充LLMs的静态知识，但检索阶段成为瓶颈，现有技术存在算法不适配、未加速数据检索和硬件修改过多的问题。

Method: REIS系统采用数据库布局优化、ISP适配的数据放置技术和轻量级Flash转换层，并结合存储系统的现有计算资源进行ANNS。

Result: 与服务器级系统相比，REIS平均提升检索性能13倍（能效提升55倍）。

Conclusion: REIS显著优化了RAG中的检索效率，解决了现有技术的局限性。

Abstract: Large Language Models (LLMs) face an inherent challenge: their knowledge is
confined to the data that they have been trained on. To overcome this issue,
Retrieval-Augmented Generation (RAG) complements the static training-derived
knowledge of LLMs with an external knowledge repository. RAG consists of three
stages: indexing, retrieval, and generation. The retrieval stage of RAG becomes
a significant bottleneck in inference pipelines. In this stage, a user query is
mapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS)
algorithm searches for similar vectors in the database to identify relevant
items. Due to the large database sizes, ANNS incurs significant data movement
overheads between the host and the storage system. To alleviate these
overheads, prior works propose In-Storage Processing (ISP) techniques that
accelerate ANNS by performing computations inside storage. However, existing
works that leverage ISP for ANNS (i) employ algorithms that are not tailored to
ISP systems, (ii) do not accelerate data retrieval operations for data selected
by ANNS, and (iii) introduce significant hardware modifications, limiting
performance and hindering their adoption. We propose REIS, the first ISP system
tailored for RAG that addresses these limitations with three key mechanisms.
First, REIS employs a database layout that links database embedding vectors to
their associated documents, enabling efficient retrieval. Second, it enables
efficient ANNS by introducing an ISP-tailored data placement technique that
distributes embeddings across the planes of the storage system and employs a
lightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that
uses the existing computational resources inside the storage system. Compared
to a server-grade system, REIS improves the performance (energy efficiency) of
retrieval by an average of 13x (55x).

</details>


### [41] [StoryWriter: A Multi-Agent Framework for Long Story Generation](https://arxiv.org/abs/2506.16445)
*Haotian Xia,Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Key words: 长故事生成, 多代理框架, 数据增强, 自然语言生成, 大语言模型

TL;DR: 该论文提出了一种名为StoryWriter的多代理故事生成框架，通过三个模块解决长故事生成的挑战，显著提升了故事质量和长度，并生成了一个高质量长故事数据集。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 长故事生成对现有大语言模型仍具挑战性，主要因缺乏连贯性和叙事复杂性。

Method: 采用多代理框架，包括大纲代理、规划代理和写作代理，分别负责生成事件大纲、详细规划和动态生成故事。

Result: StoryWriter在故事质量和长度上显著优于基线模型，并生成了包含6000个高质量长故事的数据集。

Conclusion: 多代理框架有效解决了长故事生成的挑战，并为未来研究提供了高质量数据集。

Abstract: Long story generation remains a challenge for existing large language models
(LLMs), primarily due to two main factors: (1) discourse coherence, which
requires plot consistency, logical coherence, and completeness in the long-form
generation, and (2) narrative complexity, which requires an interwoven and
engaging narrative. To address these challenges, we propose StoryWriter, a
multi-agent story generation framework, which consists of three main modules:
(1) outline agent, which generates event-based outlines containing rich event
plots, character, and event-event relationships. (2) planning agent, which
further details events and plans which events should be written in each chapter
to maintain an interwoven and engaging story. (3) writing agent, which
dynamically compresses the story history based on the current event to generate
and reflect new plots, ensuring the coherence of the generated story. We
conduct both human and automated evaluation, and StoryWriter significantly
outperforms existing story generation baselines in both story quality and
length. Furthermore, we use StoryWriter to generate a dataset, which contains
about $6,000$ high-quality long stories, with an average length of $8,000$
words. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning
on LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which
demonstrates advanced performance in long story generation.

</details>


### [42] [Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection](https://arxiv.org/abs/2506.16476)
*Saad Almohaimeed,Saleh Almohaimeed,Damla Turgut,Ladislau Bölöni*

Key words: 隐性仇恨言论,社交媒体,数据增强,F1分数,GPT-4o

TL;DR: 该论文探讨了如何通过现有有害言论数据集检测隐性仇恨言论，提出了一种结合样本识别、重新标注和大模型增强的方法，显著提升了检测性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 隐性仇恨言论是社交媒体平台面临的新挑战，但现有研究对其关注不足，且数据集因标注者主观性常存在误标问题。

Method: 方法包括三个关键部分：影响样本识别、重新标注以及使用Llama-3 70B和GPT-4o进行数据增强。

Result: 实验表明，该方法显著提升了隐性仇恨言论的检测性能，F1分数比基准提高了12.9点。

Conclusion: 研究表明，通过利用现有数据集和先进模型，可以有效提升隐性仇恨言论的检测能力。

Abstract: Implicit hate speech has recently emerged as a critical challenge for social
media platforms. While much of the research has traditionally focused on
harmful speech in general, the need for generalizable techniques to detect
veiled and subtle forms of hate has become increasingly pressing. Based on
lexicon analysis, we hypothesize that implicit hate speech is already present
in publicly available harmful speech datasets but may not have been explicitly
recognized or labeled by annotators. Additionally, crowdsourced datasets are
prone to mislabeling due to the complexity of the task and often influenced by
annotators' subjective interpretations. In this paper, we propose an approach
to address the detection of implicit hate speech and enhance generalizability
across diverse datasets by leveraging existing harmful speech datasets. Our
method comprises three key components: influential sample identification,
reannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental
results demonstrate the effectiveness of our approach in improving implicit
hate detection, achieving a +12.9-point F1 score improvement compared to the
baseline.

</details>


### [43] [Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples](https://arxiv.org/abs/2506.16502)
*Soumya Suvra Ghosal,Vaibhav Singh,Akash Ghosh,Soumyabrata Pal,Subhadip Baidya,Sriparna Saha,Dinesh Manocha*

Key words: 奖励模型, 低资源语言, 上下文学习, 检索器, 印度语言

TL;DR: RELIC是一种新颖的上下文学习框架，旨在解决低资源印度语言奖励信号不可靠的问题，通过利用高资源语言的上下文示例，显著提高了奖励模型的准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 低资源印度语言因缺乏高质量的偏好数据导致奖励模型信号不可靠，而传统的大规模数据收集方法成本过高。

Method: 提出了RELIC框架，通过训练检索器从高资源语言中选择上下文示例，以突出偏好与非偏好响应的区别。

Result: 在多个数据集上的实验表明，RELIC显著提高了低资源印度语言的奖励模型准确性，优于现有方法。

Conclusion: RELIC为低资源语言的奖励建模提供了一种有效且经济的解决方案。

Abstract: Reward models are essential for aligning large language models (LLMs) with
human preferences. However, most open-source multilingual reward models are
primarily trained on preference datasets in high-resource languages, resulting
in unreliable reward signals for low-resource Indic languages. Collecting
large-scale, high-quality preference data for these languages is prohibitively
expensive, making preference-based training approaches impractical. To address
this challenge, we propose RELIC, a novel in-context learning framework for
reward modeling in low-resource Indic languages. RELIC trains a retriever with
a pairwise ranking objective to select in-context examples from auxiliary
high-resource languages that most effectively highlight the distinction between
preferred and less-preferred responses. Extensive experiments on three
preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art
open-source reward models demonstrate that RELIC significantly improves reward
model accuracy for low-resource Indic languages, consistently outperforming
existing example selection methods. For example, on Bodo-a low-resource Indic
language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13%
improvement in accuracy over zero-shot prompting and state-of-the-art example
selection method, respectively.

</details>


### [44] [Automatic Speech Recognition Biases in Newcastle English: an Error Analysis](https://arxiv.org/abs/2506.16558)
*Dana Serditova,Kevin Tang,Jochen Steffens*

Key words: 自动语音识别, 区域方言, 偏见, 纽卡斯尔英语, 社会语言学

TL;DR: 研究探讨了自动语音识别（ASR）系统在区域方言（如纽卡斯尔英语）上的表现不足，指出训练数据偏向主流语言变体导致识别错误与方言特征直接相关。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: ASR系统因训练数据偏向主流语言变体而对区域方言表现不佳，区域偏见研究不足。

Method: 采用两阶段分析：对纽卡斯尔英语进行人工错误分析；重点研究方言代词“yous”和“wor”的识别问题。

Result: ASR错误与方言特征直接相关，社会因素影响较小。

Conclusion: 建议增加训练数据的方言多样性，并强调社会语言学分析在解决区域偏见中的重要性。

Abstract: Automatic Speech Recognition (ASR) systems struggle with regional dialects
due to biased training which favours mainstream varieties. While previous
research has identified racial, age, and gender biases in ASR, regional bias
remains underexamined. This study investigates ASR performance on Newcastle
English, a well-documented regional dialect known to be challenging for ASR. A
two-stage analysis was conducted: first, a manual error analysis on a subsample
identified key phonological, lexical, and morphosyntactic errors behind ASR
misrecognitions; second, a case study focused on the systematic analysis of ASR
recognition of the regional pronouns ``yous'' and ``wor''. Results show that
ASR errors directly correlate with regional dialectal features, while social
factors play a lesser role in ASR mismatches. We advocate for greater dialectal
diversity in ASR training data and highlight the value of sociolinguistic
analysis in diagnosing and addressing regional biases.

</details>


### [45] [Weight Factorization and Centralization for Continual Learning in Speech Recognition](https://arxiv.org/abs/2506.16574)
*Enes Yavuz Ugan,Ngoc-Quan Pham,Alexander Waibel*

Key words: 持续学习,灾难性遗忘,因子化,集中化

TL;DR: 提出了一种持续学习方法，通过因子化和集中化两阶段防止灾难性遗忘。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现代神经网络语音识别模型需要在不重新训练的情况下吸收新数据，但传统方法容易导致灾难性遗忘。

Method: 采用两阶段学习：因子化和集中化，通过在分散的低秩适配器中积累知识。

Result: 实验证明集中化阶段能有效防止灾难性遗忘。

Conclusion: 提出的方法在多语言和语言无关条件下表现出色。

Abstract: Modern neural network based speech recognition models are required to
continually absorb new data without re-training the whole system, especially in
downstream applications using foundation models, having no access to the
original training data. Continually training the models in a rehearsal-free,
multilingual, and language agnostic condition, likely leads to catastrophic
forgetting, when a seemingly insignificant disruption to the weights can
destructively harm the quality of the models. Inspired by the ability of human
brains to learn and consolidate knowledge through the waking-sleeping cycle, we
propose a continual learning approach with two distinct phases: factorization
and centralization, learning and merging knowledge accordingly. Our experiments
on a sequence of varied code-switching datasets showed that the centralization
stage can effectively prevent catastrophic forgetting by accumulating the
knowledge in multiple scattering low-rank adapters.

</details>


### [46] [Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation Improvement](https://arxiv.org/abs/2506.16580)
*Tuan-Nam Nguyen,Ngoc-Quan Pham,Seymanur Akti,Alexander Waibel*

Key words: 流式处理、口音转换、Emformer、TTS、实时

TL;DR: 提出了首个流式口音转换模型，将非母语语音转换为母语口音，同时保留说话者身份和韵律，并改善发音。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有口音转换模型无法实现流式处理，研究目标是开发一种能够实时转换口音的技术。

Method: 通过改进现有的口音转换架构，使用Emformer编码器和优化的推理机制实现流式处理，同时集成TTS模型生成高质量训练数据。

Result: 模型性能与顶级口音转换模型相当，同时保持稳定的延迟，成为首个支持流式处理的口音转换系统。

Conclusion: 该流式口音转换模型为实时口音转换提供了可行方案。

Abstract: We propose a first streaming accent conversion (AC) model that transforms
non-native speech into a native-like accent while preserving speaker identity,
prosody and improving pronunciation. Our approach enables stream processing by
modifying a previous AC architecture with an Emformer encoder and an optimized
inference mechanism. Additionally, we integrate a native text-to-speech (TTS)
model to generate ideal ground-truth data for efficient training. Our streaming
AC model achieves comparable performance to the top AC models while maintaining
stable latency, making it the first AC system capable of streaming.

</details>


### [47] [Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework](https://arxiv.org/abs/2506.16584)
*Nadav Kunievsky,James A. Evans*

Key words: 大语言模型、世界模型、评估框架、语义基础、变异性

TL;DR: 本文提出了一种评估大语言模型（LLM）是否具有稳健世界模型的框架，通过分解模型响应的变异性来量化其语义基础。结果表明，更大模型更能将输出变异性归因于用户意图而非表面表达，但优势并不一致且有限。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 评估LLM是否具备世界模型对高风险应用中的可靠性至关重要。

Method: 提出一个评估框架，将模型响应变异性分解为用户目的、表达方式和模型不稳定性三个部分。

Result: 更大模型更多将输出变异性归因于用户目的，表明其世界模型更稳健，但优势有限且不一致。

Conclusion: 需超越基于准确性的基准，采用更直接的语义诊断方法评估模型内部世界理解的结构和稳定性。

Abstract: Understanding whether large language models (LLMs) possess a world model-a
structured understanding of the world that supports generalization beyond
surface-level patterns-is central to assessing their reliability, especially in
high-stakes applications. We propose a formal framework for evaluating whether
an LLM exhibits a sufficiently robust world model, defined as producing
consistent outputs across semantically equivalent prompts while distinguishing
between prompts that express different intents. We introduce a new evaluation
approach to measure this that decomposes model response variability into three
components: variability due to user purpose, user articulation, and model
instability. An LLM with a strong world model should attribute most of the
variability in its responses to changes in foundational purpose rather than
superficial changes in articulation. This approach allows us to quantify how
much of a model's behavior is semantically grounded rather than driven by model
instability or alternative wording. We apply this framework to evaluate LLMs
across diverse domains. Our results show how larger models attribute a greater
share of output variability to changes in user purpose, indicating a more
robust world model. This improvement is not uniform, however: larger models do
not consistently outperform smaller ones across all domains, and their
advantage in robustness is often modest. These findings highlight the
importance of moving beyond accuracy-based benchmarks toward semantic
diagnostics that more directly assess the structure and stability of a model's
internal understanding of the world.

</details>


### [48] [A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications](https://arxiv.org/abs/2506.16594)
*Hanshu Rao,Weisi Liu,Haohan Wang,I-Chan Huang,Zhe He,Xiaolei Huang*

Key words: 合成数据生成，大型语言模型，生物医学，数据稀缺，隐私保护

TL;DR: 这是一篇关于合成数据生成的综述，分析了2020年至2025年间的59项研究，重点探讨了生物医学领域中的数据稀缺、隐私和数据质量问题，以及大型语言模型（LLMs）在其中的应用。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 生物医学领域面临数据稀缺、隐私保护和数据质量等挑战，合成数据生成作为一种解决方案，需要系统研究其应用趋势和方法。

Method: 遵循PRISMA-ScR指南，对PubMed、ACM、Web of Science和Google Scholar中的59项研究进行系统性分析，研究内容包括临床应用、生成方法和评估方法。

Result: 研究发现，合成数据主要集中在非结构化文本（78.0%）、表格数据（13.6%）和多模态源（8.4%）；生成方法包括提示（72.9%）、微调LLMs（22.0%）和专用模型（5.1%）；评估方法包括内在指标（27.1%）、人为参与评估（55.9%）和LLM评估（13.6%）。

Conclusion: 研究指出了当前生物医学领域合成数据生成的局限性，并突出了在临床领域应用、资源与模型可访问性以及评估标准化方面的挑战。

Abstract: Synthetic data generation--mitigating data scarcity, privacy concerns, and
data quality challenges in biomedical fields--has been facilitated by rapid
advances of large language models (LLMs). This scoping review follows
PRISMA-ScR guidelines and synthesizes 59 studies, published between 2020 and
2025 and collected from PubMed, ACM, Web of Science, and Google Scholar. The
review systematically examines biomedical research and application trends in
synthetic data generation, emphasizing clinical applications, methodologies,
and evaluations. Our analysis identifies data modalities of unstructured texts
(78.0%), tabular data (13.6%), and multimodal sources (8.4%); generation
methods of prompting (72.9%), fine-tuning (22.0%) LLMs and specialized model
(5.1%); and heterogeneous evaluations of intrinsic metrics (27.1%),
human-in-the-loop assessments (55.9%), and LLM-based evaluations (13.6%). The
analysis addresses current limitations in what, where, and how health
professionals can leverage synthetic data generation for biomedical domains.
Our review also highlights challenges in adaption across clinical domains,
resource and model accessibility, and evaluation standardizations.

</details>


### [49] [Modeling Public Perceptions of Science in Media](https://arxiv.org/abs/2506.16622)
*Jiaxin Pei,Dustin Wright,Isabelle Augenstin,David Jurgens*

Key words: 科学传播, 公众感知, NLP模型, 参与度, Reddit

TL;DR: 该论文提出了一个计算框架，用于建模公众对科学新闻的多维度感知，并通过大规模数据集和NLP模型分析公众对科学的看法及其对参与度的影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 科学传播中需要更好地理解公众如何感知和互动科学新闻，以增强信任和理解。

Method: 开发了一个计算框架和NLP模型，构建了包含10,489个注释的大规模数据集，来自2,101名参与者。

Result: 科学新闻的消费频率是影响公众感知的主要因素；通过Reddit实验发现，感知评分与参与度直接相关。

Conclusion: 细致的感知建模对科学传播至关重要，可为预测公众兴趣提供新途径。

Abstract: Effectively engaging the public with science is vital for fostering trust and
understanding in our scientific community. Yet, with an ever-growing volume of
information, science communicators struggle to anticipate how audiences will
perceive and interact with scientific news. In this paper, we introduce a
computational framework that models public perception across twelve dimensions,
such as newsworthiness, importance, and surprisingness. Using this framework,
we create a large-scale science news perception dataset with 10,489 annotations
from 2,101 participants from diverse US and UK populations, providing valuable
insights into public responses to scientific information across domains. We
further develop NLP models that predict public perception scores with a strong
performance. Leveraging the dataset and model, we examine public perception of
science from two perspectives: (1) Perception as an outcome: What factors
affect the public perception of scientific information? (2) Perception as a
predictor: Can we use the estimated perceptions to predict public engagement
with science? We find that individuals' frequency of science news consumption
is the driver of perception, whereas demographic factors exert minimal
influence. More importantly, through a large-scale analysis and carefully
designed natural experiment on Reddit, we demonstrate that the estimated public
perception of scientific information has direct connections with the final
engagement pattern. Posts with more positive perception scores receive
significantly more comments and upvotes, which is consistent across different
scientific information and for the same science, but are framed differently.
Overall, this research underscores the importance of nuanced perception
modeling in science communication, offering new pathways to predict public
interest and engagement with scientific content.

</details>


### [50] [Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System](https://arxiv.org/abs/2506.16628)
*Jianlin Shi,Brian T. Bucher*

Key words: 机器学习, 大型语言模型, 基于规则的NLP, 临床文本处理

TL;DR: 论文提出一种新方法，利用大型语言模型（LLMs）辅助开发基于规则的NLP系统，显著提高效率和透明度。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 基于规则的NLP系统在临床环境中因其可解释性和操作效率仍被广泛使用，但人工开发和维护成本高。

Method: 利用LLMs在开发阶段自动识别相关文本片段并提取关键词，用于命名实体识别任务。

Result: 实验结果显示高召回率（Deepseek: 0.98, Qwen: 0.99）和关键词提取的完美表现（1.0）。

Conclusion: 该方法为NLP开发提供了新方向，比深度学习方法更快、成本更低且透明。

Abstract: Despite advances in machine learning (ML) and large language models (LLMs),
rule-based natural language processing (NLP) systems remain active in clinical
settings due to their interpretability and operational efficiency. However,
their manual development and maintenance are labor-intensive, particularly in
tasks with large linguistic variability. To overcome these limitations, we
proposed a novel approach employing LLMs solely during the rule-based systems
development phase. We conducted the initial experiments focusing on the first
two steps of developing a rule-based NLP pipeline: find relevant snippets from
the clinical note; extract informative keywords from the snippets for the
rule-based named entity recognition (NER) component. Our experiments
demonstrated exceptional recall in identifying clinically relevant text
snippets (Deepseek: 0.98, Qwen: 0.99) and 1.0 in extracting key terms for NER.
This study sheds light on a promising new direction for NLP development,
enabling semi-automated or automated development of rule-based systems with
significantly faster, more cost-effective, and transparent execution compared
with deep learning model-based solutions.

</details>


### [51] [GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View](https://arxiv.org/abs/2506.16633)
*Fenghua Cheng,Jinxiang Wang,Sen Wang,Zi Huang,Xue Li*

Key words: 多模态推理, 层次视觉信息, 地理知识, GeoGuess, SightSense

TL;DR: 论文提出了一种新的多模态推理任务GeoGuess，通过整合不同层次的视觉信息和地理知识来解决现有任务的局限性，并展示了其方法和数据集的优异性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前多模态推理任务在分级视觉线索推理方面存在不足，需要结合局部细节和全局上下文进行推理。

Method: 提出了SightSense方法，结合多模态和多层次推理，利用视觉信息层级和外部知识进行预测和解释。

Result: 实验表明，SightSense在GeoGuess任务中表现出色。

Conclusion: GeoGuess填补了多模态推理任务的空白，为复杂场景下的推理提供了新基准。

Abstract: Multimodal reasoning is a process of understanding, integrating and inferring
information across different data modalities. It has recently attracted surging
academic attention as a benchmark for Artificial Intelligence (AI). Although
there are various tasks for evaluating multimodal reasoning ability, they still
have limitations. Lack of reasoning on hierarchical visual clues at different
levels of granularity, e.g., local details and global context, is of little
discussion, despite its frequent involvement in real scenarios. To bridge the
gap, we introduce a novel and challenging task for multimodal reasoning, namely
GeoGuess. Given a street view image, the task is to identify its location and
provide a detailed explanation. A system that succeeds in GeoGuess should be
able to detect tiny visual clues, perceive the broader landscape, and associate
with vast geographic knowledge. Therefore, GeoGuess would require the ability
to reason between hierarchical visual information and geographic knowledge. In
this work, we establish a benchmark for GeoGuess by introducing a specially
curated dataset GeoExplain which consists of
panoramas-geocoordinates-explanation tuples. Additionally, we present a
multimodal and multilevel reasoning method, namely SightSense which can make
prediction and generate comprehensive explanation based on hierarchy of visual
information and external knowledge. Our analysis and experiments demonstrate
their outstanding performance in GeoGuess.

</details>


### [52] [Long-Context Generalization with Sparse Attention](https://arxiv.org/abs/2506.16640)
*Pavlo Vasylenko,Marcos Treviso,André F. T. Martins*

Key words: transformer, 稀疏注意力, α-entmax, ASEntmax, 位置编码, 长上下文泛化

TL;DR: 稀疏注意力机制（如α-entmax）及其自适应版本（ASEntmax）能有效解决传统softmax注意力在长序列任务中的注意力分散问题，性能优于多种基线方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统transformer的softmax注意力在长序列任务中会导致注意力分散和表示崩溃，需要一种更精确的注意力机制来聚焦固定大小的模式。

Method: 提出了稀疏注意力机制α-entmax及其自适应版本ASEntmax，结合可学习的温度参数，允许注意力分布在稀疏和密集模式之间切换，并改进了位置编码设计。

Result: ASEntmax结合适当位置编码的transformer在长上下文泛化任务中显著优于softmax、可扩展softmax和固定温度α-entmax基线方法。

Conclusion: 稀疏注意力机制（尤其是ASEntmax）和优化的位置编码能显著提升transformer在长序列任务中的性能。

Abstract: Transformer-based architectures traditionally employ softmax to compute
attention weights, which produces dense distributions over all tokens in a
sequence. While effective in many settings, this density has been shown to be
detrimental for tasks that demand precise focus on fixed-size patterns: as
sequence length increases, non-informative tokens accumulate attention
probability mass, leading to dispersion and representational collapse. We show
in this paper that sparse attention mechanisms using $\alpha$-entmax can avoid
these issues, due to their ability to assign exact zeros to irrelevant tokens.
Furthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows
$\alpha$-entmax with a learnable temperature parameter, allowing the attention
distribution to interpolate between sparse (pattern-focused) and dense
(softmax-like) regimes. Finally, we show that the ability to locate and
generalize fixed-size patterns can be further improved through a careful design
of position encodings, which impacts both dense and sparse attention methods.
By integrating ASEntmax into standard transformer layers alongside proper
positional encodings, we show that our models greatly outperform softmax,
scalable softmax, and fixed-temperature $\alpha$-entmax baselines on
long-context generalization.

</details>


### [53] [Arch-Router: Aligning LLM Routing with Human Preferences](https://arxiv.org/abs/2506.16655)
*Co Tran,Salman Paracha,Adil Hafeez,Shuguang Chen*

Key words: 大型语言模型（LLMs）、模型路由、偏好对齐、Arch-Router、主观评价

TL;DR: 提出了一种偏好对齐的路由框架Arch-Router，用于根据用户定义的领域或动作类型匹配查询，从而在模型路由决策中更好地反映人类偏好。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的大型语言模型（LLM）路由方法存在两个主要局限性：一是性能评估往往忽略主观评价标准，二是模型选择范围有限。

Method: 提出Arch-Router，一个1.5B参数的轻量级模型，能够将查询映射到领域-动作偏好，并支持无缝添加新模型而无需重新训练或修改架构。

Result: 在对话数据集上的实验表明，Arch-Router在匹配查询与人类偏好方面达到了SOTA效果，优于顶级专有模型。

Conclusion: Arch-Router能够捕获主观评价标准，并使路由决策更透明和灵活。

Abstract: With the rapid proliferation of large language models (LLMs) -- each
optimized for different strengths, style, or latency/cost profile -- routing
has become an essential technique to operationalize the use of different
models. However, existing LLM routing approaches are limited in two key ways:
they evaluate performance using benchmarks that often fail to capture human
preferences driven by subjective evaluation criteria, and they typically select
from a limited pool of models. In this work, we propose a preference-aligned
routing framework that guides model selection by matching queries to
user-defined domains (e.g., travel) or action types (e.g., image editing) --
offering a practical mechanism to encode preferences in routing decisions.
Specifically, we introduce \textbf{Arch-Router}, a compact 1.5B model that
learns to map queries to domain-action preferences for model routing decisions.
Our approach also supports seamlessly adding new models for routing without
requiring retraining or architectural modifications. Experiments on
conversational datasets demonstrate that our approach achieves state-of-the-art
(SOTA) results in matching queries with human preferences, outperforming top
proprietary models. Our approach captures subjective evaluation criteria and
makes routing decisions more transparent and flexible. Our model is available
at: \texttt{https://huggingface.co/katanemo/Arch-Router-1.5B}.

</details>


### [54] [Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations](https://arxiv.org/abs/2506.16678)
*Ananth Agarwal,Jasper Jian,Christopher D. Manning,Shikhar Murty*

Key words: LLMs, 语法, 探测, Transformer, 语法评估

TL;DR: LLMs对语法有很强掌握，但内部机制尚不明确。研究表明，探测提取的语法特征无法预测模型在英语语法任务中的表现，揭示了探测机制与实际行为之间的脱节。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究LLMs如何内部表示语法结构，以及探测机制是否能预测下游语法任务表现。

Method: 评估32个开源Transformer模型，通过探测提取语法特征，并与语法任务表现对比。

Result: 探测提取的语法特征无法预测模型在语法任务中的表现。

Conclusion: LLMs的语法探测机制与实际语法行为存在脱节。

Abstract: Large Language Models (LLMs) exhibit a robust mastery of syntax when
processing and generating text. While this suggests internalized understanding
of hierarchical syntax and dependency relations, the precise mechanism by which
they represent syntactic structure is an open area within interpretability
research. Probing provides one way to identify the mechanism of syntax being
linearly encoded in activations, however, no comprehensive study has yet
established whether a model's probing accuracy reliably predicts its downstream
syntactic performance. Adopting a "mechanisms vs. outcomes" framework, we
evaluate 32 open-weight transformer models and find that syntactic features
extracted via probing fail to predict outcomes of targeted syntax evaluations
across English linguistic phenomena. Our results highlight a substantial
disconnect between latent syntactic representations found via probing and
observable syntactic behaviors in downstream tasks.

</details>


### [55] [LegiGPT: Party Politics and Transport Policy with Large Language Model](https://arxiv.org/abs/2506.16692)
*Hyunsoo Yun,Eun Hak Lee*

Key words: LegiGPT, 政治意识形态, 交通政策, XAI, GPT-4

TL;DR: 使用LegiGPT框架结合LLM和XAI分析立法提案，揭示政治意识形态对交通政策制定的影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究立法者的政治意识形态如何影响立法决策，特别是交通政策制定。

Method: 引入LegiGPT框架，结合GPT-4的零样本提示和多阶段过滤分类流程，并使用XAI技术分析政党属性关系。

Result: 发现保守派和进步派提案数量及其比例、选区规模和选举人口是关键影响因素。

Conclusion: 两党通过不同方式促进跨党派立法，该方法为理解立法动态和未来政策制定提供了工具。

Abstract: Given the significant influence of lawmakers' political ideologies on
legislative decision-making, understanding their impact on policymaking is
critically important. We introduce a novel framework, LegiGPT, which integrates
a large language model (LLM) with explainable artificial intelligence (XAI) to
analyze transportation-related legislative proposals. LegiGPT employs a
multi-stage filtering and classification pipeline using zero-shot prompting
with GPT-4. Using legislative data from South Korea's 21st National Assembly,
we identify key factors - including sponsor characteristics, political
affiliations, and geographic variables - that significantly influence
transportation policymaking. The LLM was used to classify
transportation-related bill proposals through a stepwise filtering process
based on keywords, phrases, and contextual relevance. XAI techniques were then
applied to examine relationships between party affiliation and associated
attributes. The results reveal that the number and proportion of conservative
and progressive sponsors, along with district size and electoral population,
are critical determinants shaping legislative outcomes. These findings suggest
that both parties contributed to bipartisan legislation through different forms
of engagement, such as initiating or supporting proposals. This integrated
approach provides a valuable tool for understanding legislative dynamics and
guiding future policy development, with broader implications for infrastructure
planning and governance.

</details>


### [56] [ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models](https://arxiv.org/abs/2506.16712)
*Bin Chen,Xinzge Gao,Chuanrui Hu,Penghang Yu,Hua Zhang,Bing-Kun Bao*

Key words: 生成奖励模型, 推理路径, 幻觉减少, 强化学习

TL;DR: ReasonGRM是一个三阶段生成奖励建模框架，通过改进推理路径和减少幻觉，提升生成奖励模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 生成奖励模型在捕捉人类偏好方面比标量奖励模型更灵活，但其推理能力不足可能导致关键信息遗漏或幻觉。因此，提出ReasonGRM以解决这一问题。

Method: 1. 使用Zero-RL生成简洁、导向结果的推理路径以减少关键遗漏。
2. 引入新指标$R^\star$评分推理路径，选择正确且探索少的路径以减少幻觉。
3. 通过强化学习进一步优化模型在挑战性示例上的偏好判别能力。

Result: 在三个公开基准测试中，ReasonGRM平均优于之前最佳GRM 1.8%，并超越GPT-4o等专有模型达5.6%。

Conclusion: 实验表明，推理感知训练和高质量理据选择对可靠的偏好建模至关重要。

Abstract: Generative Reward Models (GRMs) provide greater flexibility than scalar
reward models in capturing human preferences, but their effectiveness is
limited by poor reasoning capabilities. This often results in incomplete or
overly speculative reasoning paths, leading to hallucinations or missing key
information in complex tasks. We address this challenge with ReasonGRM, a
three-stage generative reward modeling framework. In the first stage, Zero-RL
is used to generate concise, outcome-directed reasoning paths that reduce the
likelihood of critical omissions. In the second stage, we introduce a novel
evaluation metric, $R^\star$, which scores reasoning paths based on their
generation likelihood. This favors paths that reach correct answers with
minimal exploration, helping to reduce hallucination-prone data during
training. In the final stage, the model is further refined through
reinforcement learning on challenging examples to enhance its preference
discrimination capabilities. Experiments on three public benchmarks show that
ReasonGRM achieves competitive or state-of-the-art performance, outperforming
previous best GRMs by 1.8\% on average and surpassing proprietary models such
as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of
reasoning-aware training and highlight the importance of high-quality rationale
selection for reliable preference modeling.

</details>


### [57] [The Role of Model Confidence on Bias Effects in Measured Uncertainties](https://arxiv.org/abs/2506.16724)
*Xinyi Liu,Weiguang Wang,Hangfeng He*

Key words: 大型语言模型, 认知不确定性, 随机不确定性, 视觉问答, 偏见缓解

TL;DR: 论文研究了大型语言模型（LLM）在开放任务中如何准确评估认知不确定性，并探讨了偏见对不确定性量化的影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着LLM在开放任务中的广泛应用，准确评估认知不确定性（反映模型的缺乏知识）对于确保可靠结果变得至关重要。

Method: 通过视觉问答（VQA）任务实验，研究提示偏见如何影响GPT-4o和Qwen2-VL模型中的认知和随机不确定性。

Result: 研究发现，偏见在模型置信度较低时会对两种不确定性产生更大影响，并导致认知不确定性的低估（过度自信）。

Conclusion: 这些发现加深了对偏见缓解在不确定性量化中作用的理解，并为开发更先进的技术提供了潜在信息。

Abstract: With the growing adoption of Large Language Models (LLMs) for open-ended
tasks, accurately assessing epistemic uncertainty, which reflects a model's
lack of knowledge, has become crucial to ensuring reliable outcomes. However,
quantifying epistemic uncertainty in such tasks is challenging due to the
presence of aleatoric uncertainty, which arises from multiple valid answers.
While bias can introduce noise into epistemic uncertainty estimation, it may
also reduce noise from aleatoric uncertainty. To investigate this trade-off, we
conduct experiments on Visual Question Answering (VQA) tasks and find that
mitigating prompt-introduced bias improves uncertainty quantification in
GPT-4o. Building on prior work showing that LLMs tend to copy input information
when model confidence is low, we further analyze how these prompt biases affect
measured epistemic and aleatoric uncertainty across varying bias-free
confidence levels with GPT-4o and Qwen2-VL. We find that all considered biases
induce greater changes in both uncertainties when bias-free model confidence is
lower. Moreover, lower bias-free model confidence leads to greater
underestimation of epistemic uncertainty (i.e. overconfidence) due to bias,
whereas it has no significant effect on the direction of changes in aleatoric
uncertainty estimation. These distinct effects deepen our understanding of bias
mitigation for uncertainty quantification and potentially inform the
development of more advanced techniques.

</details>


### [58] [LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization](https://arxiv.org/abs/2506.16738)
*Daejin Jo,Jeeyoung Yun,Byungseok Roh,Sungwoong Kim*

Key words: 语音标记化,语义蒸馏,语言模型对齐,LM-SPT,多帧率

TL;DR: LM-SPT是一种新型的语音标记化方法，通过间接监督学习语义对齐的离散单元，并引入架构改进和多帧率支持，优于现有基线。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统语音标记化方法生成的标记序列过长，且难以与语言模型对齐，影响了语音语言模型的效率。

Method: 提出LM-SPT方法，通过间接监督（基于重建波形与原始波形的编码表示差异）进行语义蒸馏，改进编码器和解码器架构，支持多帧率。

Result: LM-SPT在重建保真度上优于基线，语音语言模型在其标记上训练后，在语音到文本和文本到语音任务中表现优异。

Conclusion: LM-SPT解决了语音标记化中的语义对齐和效率问题，为语音语言模型提供了更优的接口。

Abstract: With the rapid progress of speech language models (SLMs), discrete speech
tokens have emerged as a core interface between speech and text, enabling
unified modeling across modalities. Recent speech tokenization approaches aim
to isolate semantic information from low-level acoustics to better align with
language models. In particular, previous methods use SSL teachers such as
HuBERT to extract semantic representations, which are then distilled into a
semantic quantizer to suppress acoustic redundancy as well as capture
content-related latent structures. However, they still produce speech token
sequences significantly longer than their textual counterparts, creating
challenges for efficient speech-language modeling. Reducing the frame rate is a
natural solution, but standard techniques, such as rigid average pooling across
frames, can distort or dilute the semantic structure required for effective LM
alignment. To address this, we propose LM-SPT, a speech tokenization method
that introduces a novel semantic distillation. Instead of directly matching
teacher and student features via pooling, we reconstruct speech solely from
semantic tokens and minimize the discrepancy between the encoded
representations of the original and reconstructed waveforms, obtained from a
frozen automatic speech recognition (ASR) encoder. This indirect yet
data-driven supervision enables the tokenizer to learn discrete units that are
more semantically aligned with language models. LM-SPT further incorporates
architectural improvements to the encoder and decoder for speech tokenization,
and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz.
Experimental results show that LM-SPT achieves superior reconstruction fidelity
compared to baselines, and that SLMs trained with LM-SPT tokens achieve
competitive performances on speech-to-text and consistently outperform
baselines on text-to-speech tasks.

</details>


### [59] [Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly](https://arxiv.org/abs/2506.16755)
*Lance Ying,Ryan Truong,Katherine M. Collins,Cedegao E. Zhang,Megan Wei,Tyler Brooke-Wilson,Tan Zhi-Xuan,Lionel Wong,Joshua B. Tenenbaum*

Key words: 多模态推理,语言模型,社会认知,贝叶斯规划

TL;DR: LIRAS框架整合语言和视觉输入，通过多层次的社会推理任务表现优于现有模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究如何结合语言和视觉信息进行复杂的社会推理，以模拟人类在多模态环境中的判断能力。

Method: 提出LIRAS框架，结合多模态语言模型解析输入，并利用贝叶斯逆向规划引擎生成概率性判断。

Result: 在认知科学实验任务中，LIRAS表现优于现有模型和简化版本，更接近人类判断。

Conclusion: LIRAS通过整合语言和视觉信息，为多模态社会推理提供了一种有效的框架，可推广到更多领域。

Abstract: Drawing real world social inferences usually requires taking into account
information from multiple modalities. Language is a particularly powerful
source of information in social settings, especially in novel situations where
language can provide both abstract information about the environment dynamics
and concrete specifics about an agent that cannot be easily visually observed.
In this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a
framework for drawing context-specific social inferences that integrate
linguistic and visual inputs. LIRAS frames multimodal social reasoning as a
process of constructing structured but situation-specific agent and environment
representations - leveraging multimodal language models to parse language and
visual inputs into unified symbolic representations, over which a Bayesian
inverse planning engine can be run to produce granular probabilistic judgments.
On a range of existing and new social reasoning tasks derived from cognitive
science experiments, we find that our model (instantiated with a comparatively
lightweight VLM) outperforms ablations and state-of-the-art models in capturing
human judgments across all domains.

</details>


### [60] [SocialSim: Towards Socialized Simulation of Emotional Support Conversation](https://arxiv.org/abs/2506.16756)
*Zhuang Chen,Yaru Cao,Guanqun Bi,Jincenzi Wu,Jinfeng Zhou,Xiyao Xiao,Si Chen,Hongning Wang,Minlie Huang*

Key words: 情感支持对话、社交动态、社交披露、社交意识、合成数据集

TL;DR: SocialSim 框架通过整合社交互动的关键方面（社交披露和社交意识）来模拟情感支持对话（ESC），并构建了高质量的合成数据集 SSConv，其性能甚至优于众包数据。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的情感支持对话数据生成方法因忽视社交动态而效果不佳，作者提出 SocialSim 框架以改进模拟效果。

Method: 通过构建全面的个人角色库（促进社交披露）和激发认知推理（增强社交意识），SocialSim 模拟 ESC。

Result: 生成的合成数据集 SSConv 质量优于众包数据，训练的聊天机器人在自动和人工评估中表现最佳。

Conclusion: SocialSim 提供了一种可扩展的 ESC 合成方法，使情感关怀更易实现。

Abstract: Emotional support conversation (ESC) helps reduce people's psychological
stress and provide emotional value through interactive dialogues. Due to the
high cost of crowdsourcing a large ESC corpus, recent attempts use large
language models for dialogue augmentation. However, existing approaches largely
overlook the social dynamics inherent in ESC, leading to less effective
simulations. In this paper, we introduce SocialSim, a novel framework that
simulates ESC by integrating key aspects of social interactions: social
disclosure and social awareness. On the seeker side, we facilitate social
disclosure by constructing a comprehensive persona bank that captures diverse
and authentic help-seeking scenarios. On the supporter side, we enhance social
awareness by eliciting cognitive reasoning to generate logical and supportive
responses. Building upon SocialSim, we construct SSConv, a large-scale
synthetic ESC corpus of which quality can even surpass crowdsourced ESC data.
We further train a chatbot on SSConv and demonstrate its state-of-the-art
performance in both automatic and human evaluations. We believe SocialSim
offers a scalable way to synthesize ESC, making emotional care more accessible
and practical.

</details>


### [61] [Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models](https://arxiv.org/abs/2506.16760)
*Lei Jiang,Zixun Zhang,Zizhou Wang,Xiaobing Sun,Zhen Li,Liangli Zhen,Xiaohua Xu*

Key words: LVLMs, Jailbreak, Cross-modal, Adversarial, Safety Mechanisms

TL;DR: CAMO是一种新的黑盒越狱攻击框架，通过将恶意提示分解为视觉和文本片段，利用跨模态推理能力绕过安全机制。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的文本或图像扰动方法易被检测且效率低，需更隐蔽高效的攻击方式。

Method: 将恶意提示分解为视觉和文本片段，利用LVLMs的跨模态推理能力重构有害指令。

Result: CAMO在主流LVLMs上表现高效且隐蔽，具有强跨模型可迁移性。

Conclusion: 当前安全机制存在显著漏洞，亟需更先进的跨模态安全解决方案。

Abstract: Large Vision-Language Models (LVLMs) demonstrate exceptional performance
across multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass
built-in safety mechanisms to elicit restricted content generation. Existing
black-box jailbreak methods primarily rely on adversarial textual prompts or
image perturbations, yet these approaches are highly detectable by standard
content filtering systems and exhibit low query and computational efficiency.
In this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO),
a novel black-box jailbreak attack framework that decomposes malicious prompts
into semantically benign visual and textual fragments. By leveraging LVLMs'
cross-modal reasoning abilities, CAMO covertly reconstructs harmful
instructions through multi-step reasoning, evading conventional detection
mechanisms. Our approach supports adjustable reasoning complexity and requires
significantly fewer queries than prior attacks, enabling both stealth and
efficiency. Comprehensive evaluations conducted on leading LVLMs validate
CAMO's effectiveness, showcasing robust performance and strong cross-model
transferability. These results underscore significant vulnerabilities in
current built-in safety mechanisms, emphasizing an urgent need for advanced,
alignment-aware security and safety solutions in vision-language systems.

</details>


### [62] [DistillNote: LLM-based clinical note summaries improve heart failure diagnosis](https://arxiv.org/abs/2506.16777)
*Heloisa Oss Boll,Antonio Oss Boll,Leticia Puttlitz Boll,Ameen Abu Hanna,Iacer Calixto*

Key words: 大型语言模型, 临床笔记总结, Distillnote, 心力衰竭预测, 文本压缩

TL;DR: Distillnote是一个基于大型语言模型（LLM）的临床笔记总结框架，通过三种技术生成了64,000多份入院笔记总结。研究发现，蒸馏总结在预测心力衰竭时表现优于完整笔记，且被临床医生认为效率更高。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 临床文档的繁琐工作对医护人员造成了巨大负担，Distillnote旨在通过LLM生成简洁的临床笔记总结来缓解这一问题。

Method: 使用三种技术：1）一步直接总结；2）结构化的分治法总结；3）进一步压缩的结构化蒸馏总结。

Result: 蒸馏总结实现了79%的文本压缩率，并在预测心力衰竭时比完整笔记训练的模型提升了18.2%的AUPRC。临床医生认为一步总结更具相关性和实用性，而蒸馏总结在效率方面表现最佳。

Conclusion: Distillnote展示了LLM在临床笔记总结中的潜力，尤其是蒸馏总结在性能和效率上的优势，为未来研究提供了有价值的资源。

Abstract: Large language models (LLMs) offer unprecedented opportunities to generate
concise summaries of patient information and alleviate the burden of clinical
documentation that overwhelms healthcare providers. We present Distillnote, a
framework for LLM-based clinical note summarization, and generate over 64,000
admission note summaries through three techniques: (1) One-step, direct
summarization, and a divide-and-conquer approach involving (2) Structured
summarization focused on independent clinical insights, and (3) Distilled
summarization that further condenses the Structured summaries. We test how
useful are the summaries by using them to predict heart failure compared to a
model trained on the original notes. Distilled summaries achieve 79% text
compression and up to 18.2% improvement in AUPRC compared to an LLM trained on
the full notes. We also evaluate the quality of the generated summaries in an
LLM-as-judge evaluation as well as through blinded pairwise comparisons with
clinicians. Evaluations indicate that one-step summaries are favoured by
clinicians according to relevance and clinical actionability, while distilled
summaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio)
and significantly reduce hallucinations. We release our summaries on PhysioNet
to encourage future research.

</details>


### [63] [MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning](https://arxiv.org/abs/2506.16792)
*Muyang Zheng,Yuanzhi Yao,Changting Lin,Rui Wang,Meng Han*

Key words: 破解攻击，大型语言模型，黑盒模型，迭代语义调优

TL;DR: 该论文提出了一种名为MIST的方法，通过迭代语义调优破解黑盒大型语言模型（LLMs），有效生成保留语义但诱导有害内容的提示。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管大型语言模型已努力与社会价值观对齐，但仍易受破解攻击，而黑盒模型的破解因离散令牌输入、访问限制和有限查询预算而具有挑战性。

Method: MIST采用迭代语义调优，结合顺序同义词搜索和顺序确定优化策略，平衡语义相似性和计算效率。

Result: 实验表明，MIST在开源和闭源模型上均取得高攻击成功率和转移性，计算效率也验证其实际可行性。

Conclusion: MIST为黑盒LLM破解提供了一种高效且实用的方法。

Abstract: Despite efforts to align large language models (LLMs) with societal and moral
values, these models remain susceptible to jailbreak attacks--methods designed
to elicit harmful responses. Jailbreaking black-box LLMs is considered
challenging due to the discrete nature of token inputs, restricted access to
the target LLM, and limited query budget. To address the issues above, we
propose an effective method for jailbreaking black-box large language Models
via Iterative Semantic Tuning, named MIST. MIST enables attackers to
iteratively refine prompts that preserve the original semantic intent while
inducing harmful content. Specifically, to balance semantic similarity with
computational efficiency, MIST incorporates two key strategies: sequential
synonym search, and its advanced version--order-determining optimization.
Extensive experiments across two open-source models and four closed-source
models demonstrate that MIST achieves competitive attack success rates and
attack transferability compared with other state-of-the-art white-box and
black-box jailbreak methods. Additionally, we conduct experiments on
computational efficiency to validate the practical viability of MIST.

</details>


### [64] [From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts](https://arxiv.org/abs/2506.16912)
*Daniel Christoph,Max Ploner,Patrick Haller,Alan Akbik*

Key words: 语言模型, 样本效率, 长尾分布, 模型架构, 事实频率

TL;DR: 研究分析了不同架构和大小的模型在相同预训练数据上的表现，发现模型在高频事实上的表现类似，但在低频事实上差异显著。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 语言模型的样本效率对训练效率至关重要，尤其是在处理长尾分布信息时，模型需要学习并记忆高频和低频事实。

Method: 通过为关系事实标注其在训练语料库中的频率，比较不同模型在不同频率事实上的表现。

Result: 大多数模型在高频事实上表现相似，而在低频事实上表现差异显著。

Conclusion: 模型架构和大小对事实学习效率有显著影响，尤其是在低频事实的学习上。

Abstract: Sample efficiency is a crucial property of language models with practical
implications for training efficiency. In real-world text, information follows a
long-tailed distribution. Yet, we expect models to learn and recall frequent
and infrequent facts. Sample-efficient models are better equipped to handle
this challenge of learning and retaining rare information without requiring
excessive exposure. This study analyzes multiple models of varying
architectures and sizes, all trained on the same pre-training data. By
annotating relational facts with their frequencies in the training corpus, we
examine how model performance varies with fact frequency. Our findings show
that most models perform similarly on high-frequency facts but differ notably
on low-frequency facts. This analysis provides new insights into the
relationship between model architecture, size, and factual learning efficiency.

</details>


### [65] [Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond](https://arxiv.org/abs/2506.16982)
*Antonin Berthon,Mihaela van der Schaar*

Key words: 知识追踪、语言瓶颈模型、可解释性、LLM

TL;DR: 将知识追踪转化为逆问题，提出语言瓶颈模型（LBM），通过自然语言摘要提高可解释性，并在实验中取得与最优方法相当的准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统知识追踪方法依赖不透明的潜在嵌入，限制了可解释性，即使基于LLM的方法也可能生成无准确保证的预测或摘要。

Method: 提出LBM模型，包含编码LLM生成可解释知识摘要和解冻解码LLM基于摘要重建和预测学生回答，使用组相对策略优化训练编码器。

Result: 在合成算术基准和大规模Eedi数据集上，LBM的准确性媲美最优方法，且所需学生轨迹数据更少。

Conclusion: LBM通过自然语言瓶颈确保摘要准确且可解释，训练方法有效提升摘要质量。

Abstract: Accurately assessing student knowledge is critical for effective education,
yet traditional Knowledge Tracing (KT) methods rely on opaque latent
embeddings, limiting interpretability. Even LLM-based approaches generate
direct predictions or summaries that may hallucinate without any accuracy
guarantees. We recast KT as an inverse problem: learning the minimum
natural-language summary that makes past answers explainable and future answers
predictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM
that writes an interpretable knowledge summary and a frozen decoder LLM that
must reconstruct and predict student responses using only that summary text. By
constraining all predictive information to pass through a short
natural-language bottleneck, LBMs ensure that the summary contains accurate
information while remaining human-interpretable. Experiments on synthetic
arithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the
accuracy of state-of-the-art KT and direct LLM methods while requiring
orders-of-magnitude fewer student trajectories. We demonstrate that training
the encoder with group-relative policy optimization, using downstream decoding
accuracy as a reward signal, effectively improves summary quality.

</details>


### [66] [TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs](https://arxiv.org/abs/2506.16990)
*Sahil Kale,Vijaykant Nadadur*

Key words: LaTeX, LLM, 基准测试, 开源模型, 闭源模型

TL;DR: 该论文提出了TeXpert，一个用于评估大型语言模型（LLM）生成LaTeX代码能力的基准数据集，并对开源和闭源模型进行了性能分析。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基准测试未评估LLMs生成LaTeX代码的能力，而LaTeX在科学文档排版中占据重要地位，因此需要填补这一空白。

Method: 通过TeXpert数据集，为不同难度级别的科学文档组件提供自然语言提示，分析LLM的性能和常见错误类型。

Result: 研究发现：标准基准测试表现优异的LLMs在LaTeX生成中表现不佳；开源模型（如DeepSeek v3和DeepSeek Coder）与闭源模型相当；格式和包错误频发。

Conclusion: 大多数LLMs的训练数据中可能缺乏多样化的LaTeX示例，导致其在复杂任务中表现下降。

Abstract: LaTeX's precision and flexibility in typesetting have made it the gold
standard for the preparation of scientific documentation. Large Language Models
(LLMs) present a promising opportunity for researchers to produce
publication-ready material using LaTeX with natural language instructions, yet
current benchmarks completely lack evaluation of this ability. By introducing
TeXpert, our benchmark dataset with natural language prompts for generating
LaTeX code focused on components of scientific documents across multiple
difficulty levels, we conduct an in-depth analysis of LLM performance in this
regard and identify frequent error types. Our evaluation across open and
closed-source LLMs highlights multiple key findings: LLMs excelling on standard
benchmarks perform poorly in LaTeX generation with a significant accuracy
drop-off as the complexity of tasks increases; open-source models like DeepSeek
v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks;
and formatting and package errors are unexpectedly prevalent, suggesting a lack
of diverse LaTeX examples in the training datasets of most LLMs. Our dataset,
code, and model evaluations are available at
https://github.com/knowledge-verse-ai/TeXpert.

</details>


### [67] [PersonalAI: Towards digital twins in the graph form](https://arxiv.org/abs/2506.17001)
*Mikhail Menschikov,Dmitry Evseev,Ruslan Kostoev,Ilya Perepechkin,Ilnaz Salimov,Victoria Dochkina,Petr Anokhin,Evgeny Burnaev,Nikita Semenov*

Key words: 个性化语言模型、知识图谱、超边、时间依赖性、AriGraph

TL;DR: 提出了一种利用知识图谱作为外部记忆来个性化语言模型的方法，结合标准边和超边，增强了知识提取的统一性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决大型语言模型在个性化交互中难以保留和利用个人历史信息的问题。

Method: 采用知识图谱作为外部记忆，结合AriGraph架构，引入标准边和两种超边，统一并优化图构建与知识提取过程。

Result: 在TriviaQA、HotpotQA和DiaASQ基准测试中表现良好，尤其是在引入矛盾和时间参数后仍保持鲁棒性。

Conclusion: 提出的架构能有效管理和利用时间依赖性，提升个性化语言模型的性能。

Abstract: The challenge of personalizing language models, specifically the ability to
account for a user's history during interactions, is of significant interest.
Despite recent advancements in large language models (LLMs) and Retrieval
Augmented Generation that have enhanced the factual base of LLMs, the task of
retaining extensive personal information and using it to generate personalized
responses remains pertinent. To address this, we propose utilizing external
memory in the form of knowledge graphs, which are constructed and updated by
the LLM itself. We have expanded upon ideas of AriGraph architecture and for
the first time introduced a combined graph featuring both standard edges and
two types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and
DiaASQ benchmarks indicates that this approach aids in making the process of
graph construction and knowledge extraction unified and robust. Furthermore, we
augmented the DiaASQ benchmark by incorporating parameters such as time into
dialogues and introducing contradictory statements made by the same speaker at
different times. Despite these modifications, the performance of the
question-answering system remained robust, demonstrating the proposed
architecture's ability to maintain and utilize temporal dependencies.

</details>


### [68] [LLM-Generated Feedback Supports Learning If Learners Choose to Use It](https://arxiv.org/abs/2506.17006)
*Danielle R. Thomas,Conrad Borchers,Shambhavi Bhushan,Erin Gatz,Shivang Gupta,Kenneth R. Koedinger*

Key words: LLM反馈, 教育效果, 倾向性评分, 开放性任务, 低成本扩展

TL;DR: 研究了LLM生成反馈对学习的影响，发现其效果依赖学习者寻求支持的倾向，部分场景下效果显著。反馈被学习者普遍认为有帮助且不影响完成时间。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索LLM生成反馈在教育中的实际效果，填补现有反馈方法的空白，验证其低成本、可扩展的潜力。

Method: 比较885名学习者在三种反馈条件下的表现（LLM反馈、拒绝LLM反馈、无LLM反馈），使用倾向性评分控制选择偏差。

Result: 倾向性高的学习者在后测中表现更优，部分课程中LLM反馈效果显著（效应量0.28、0.33）。学习者反馈积极且不影响时间效率。

Conclusion: LLM反馈可作为现有反馈系统的补充，尤其适合主动寻求支持的学习者，具有实用性和可扩展性。

Abstract: Large language models (LLMs) are increasingly used to generate feedback, yet
their impact on learning remains underexplored, especially compared to existing
feedback methods. This study investigates how on-demand LLM-generated
explanatory feedback influences learning in seven scenario-based tutor training
lessons. Analyzing over 2,600 lesson completions from 885 tutor learners, we
compare posttest performance among learners across three groups: learners who
received feedback generated by gpt-3.5-turbo, those who declined it, and those
without access. All groups received non-LLM corrective feedback. To address
potential selection bias-where higher-performing learners may be more inclined
to use LLM feedback-we applied propensity scoring. Learners with a higher
predicted likelihood of engaging with LLM feedback scored significantly higher
at posttest than those with lower propensity. After adjusting for this effect,
two out of seven lessons showed statistically significant learning benefits
from LLM feedback with standardized effect sizes of 0.28 and 0.33. These
moderate effects suggest that the effectiveness of LLM feedback depends on the
learners' tendency to seek support. Importantly, LLM feedback did not
significantly increase completion time, and learners overwhelmingly rated it as
helpful. These findings highlight LLM feedback's potential as a low-cost and
scalable way to improve learning on open-ended tasks, particularly in existing
systems already providing feedback without LLMs. This work contributes open
datasets, LLM prompts, and rubrics to support reproducibility.

</details>


### [69] [Instituto de Telecomunicações at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning](https://arxiv.org/abs/2506.17019)
*Giuseppe Attanasio,Sonal Sannigrahi,Ben Peters,André F. T. Martins*

Key words: 语音处理, 指令跟随, 语音识别, 机器翻译, 口语问答

TL;DR: 本文介绍了IT-IST在IWSLT 2025指令跟随语音处理任务中的方法，提出了一种统一语音转文本模型，结合了小规模语言模型和高质量数据。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在解决语音识别、翻译和口语问答任务中的指令跟随问题，并探索小规模语言模型和高数据质量的可行性。

Method: 采用预训练的连续语音编码器和文本解码器，分阶段进行模态对齐和指令微调，并使用小规模语言模型（<2B）和高质量数据。

Result: 提交了IWSLT 2025短赛道任务的结果，展示了统一模型的性能。

Conclusion: 研究表明，小规模语言模型和高质量数据可以有效支持语音处理任务。

Abstract: This paper presents the IT-IST submission to the IWSLT 2025 Shared Task on
Instruction Following Speech Processing. We submit results for the Short Track,
i.e., speech recognition, translation, and spoken question answering. Our model
is a unified speech-to-text model that integrates a pre-trained continuous
speech encoder and text decoder through a first phase of modality alignment and
a second phase of instruction fine-tuning. Crucially, we focus on using
small-scale language model backbones (< 2B) and restrict to high-quality, CC-BY
data along with synthetic data generation to supplement existing resources.

</details>


### [70] [MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models](https://arxiv.org/abs/2506.17046)
*Xiaolong Wang,Zhaolu Kang,Wangyuxuan Zhai,Xinyue Lou,Yunghwei Lai,Ziyue Wang,Yawen Wang,Kaiyu Huang,Yile Wang,Peng Li,Yang Liu*

Key words: 多模态大语言模型, 歧义消除, 基准测试, 多语言, 跨模态

TL;DR: 论文提出了一种名为MUCAR的新型多模态基准，专门用于评估多语言和跨模态场景下的多模态歧义消除能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的多模态基准通常忽视语言和视觉歧义，导致无法充分利用模态间的相互澄清潜力。

Method: MUCAR包含多语言数据集和双歧义数据集，分别通过视觉上下文和模态间的相互澄清解决歧义。

Result: 对19种先进多模态模型的评估显示，其性能与人类水平存在显著差距。

Conclusion: 未来需要更复杂的跨模态歧义理解方法，以推动多模态推理的发展。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
advances across numerous vision-language tasks. Due to their strong image-text
alignment capability, MLLMs can effectively understand image-text pairs with
clear meanings. However, effectively resolving the inherent ambiguities in
natural language and visual contexts remains challenging. Existing multimodal
benchmarks typically overlook linguistic and visual ambiguities, relying mainly
on unimodal context for disambiguation and thus failing to exploit the mutual
clarification potential between modalities. To bridge this gap, we introduce
MUCAR, a novel and challenging benchmark designed explicitly for evaluating
multimodal ambiguity resolution across multilingual and cross-modal scenarios.
MUCAR includes: (1) a multilingual dataset where ambiguous textual expressions
are uniquely resolved by corresponding visual contexts, and (2) a
dual-ambiguity dataset that systematically pairs ambiguous images with
ambiguous textual contexts, with each combination carefully constructed to
yield a single, clear interpretation through mutual disambiguation. Extensive
evaluations involving 19 state-of-the-art multimodal models--encompassing both
open-source and proprietary architectures--reveal substantial gaps compared to
human-level performance, highlighting the need for future research into more
sophisticated cross-modal ambiguity comprehension methods, further pushing the
boundaries of multimodal reasoning.

</details>


### [71] [Simultaneous Translation with Offline Speech and LLM Models in CUNI Submission to IWSLT 2025](https://arxiv.org/abs/2506.17077)
*Dominik Macháček,Peter Polák*

Key words: 同步语音翻译, Whisper模型, AlignAtt, 术语注入, EuroLLM

TL;DR: 该论文介绍了Charles大学在IWSLT 2025同步语音翻译任务中的提交成果，使用Whisper模型和AlignAtt策略，通过提示注入领域术语和上下文优化性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 提升同步语音翻译任务的性能，特别是在多种语言对中实现高质量翻译。

Method: 采用离线Whisper语音模型，结合AlignAtt同步策略，并通过提示注入领域术语和上下文。级联系统使用EuroLLM进行无界同步翻译。

Result: 在开发集上，比基准系统在捷克语到英语上提高了2 BLEU分，在英语到德语、汉语和日语上提高了13-22 BLEU分。

Conclusion: 提出的方法显著提升了同步语音翻译的性能，并引入新的语音识别延迟度量。

Abstract: This paper describes Charles University submission to the Simultaneous Speech
Translation Task of the IWSLT 2025. We cover all four language pairs with a
direct or cascade approach. The backbone of our systems is the offline Whisper
speech model, which we use for both translation and transcription in
simultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We
further improve the performance by prompting to inject in-domain terminology,
and we accommodate context. Our cascaded systems further use EuroLLM for
unbounded simultaneous translation. Compared to the Organizers' baseline, our
systems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on
English to German, Chinese and Japanese on the development sets. Additionally,
we also propose a new enhanced measure of speech recognition latency.

</details>


### [72] [Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs](https://arxiv.org/abs/2506.17080)
*Ricardo Rei,Nuno M. Guerreiro,José Pombal,João Alves,Pedro Teixeirinha,Amin Farajian,André F. T. Martins*

Key words: Tower+, LLM, 多任务学习, 翻译, 强化学习

TL;DR: Tower+模型通过在训练中平衡翻译任务和通用能力，实现了多语言和多任务的高性能表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 微调预训练LLM虽能提升特定任务性能，但可能牺牲通用能力。Tower+致力于在翻译和通用任务间取得平衡。

Method: 采用混合训练方法（继续预训练、监督微调、偏好优化、强化学习），并通过精心设计数据增强多任务性能。

Result: Tower+在翻译和高资源语言任务中表现优异，小模型超越大模型，大模型在IF-MT等评测中领先。

Conclusion: Tower+证明在优化特定领域（如翻译）的同时，仍能保持通用能力，媲美前沿模型。

Abstract: Fine-tuning pretrained LLMs has been shown to be an effective strategy for
reaching state-of-the-art performance on specific tasks like machine
translation. However, this process of adaptation often implies sacrificing
general-purpose capabilities, such as conversational reasoning and
instruction-following, hampering the utility of the system in real-world
applications that require a mixture of skills. In this paper, we introduce
Tower+, a suite of models designed to deliver strong performance across both
translation and multilingual general-purpose text capabilities. We achieve a
Pareto frontier between translation specialization and multilingual
general-purpose capabilities by introducing a novel training recipe that builds
on Tower (Alves et al., 2024), comprising continued pretraining, supervised
fine-tuning, preference optimization, and reinforcement learning with
verifiable rewards. At each stage of training, we carefully generate and curate
data to strengthen performance on translation as well as general-purpose tasks
involving code generation, mathematics problem solving, and general
instruction-following. We develop models at multiple scales: 2B, 9B, and 72B.
Our smaller models often outperform larger general-purpose open-weight and
proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers
best-in-class translation performance for high-resource languages and top
results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we
introduce for evaluating both translation and instruction-following. Our
findings highlight that it is possible to rival frontier models in general
capabilities, while optimizing for specific business domains, such as
translation and localization.

</details>


### [73] [Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation](https://arxiv.org/abs/2506.17088)
*Jiahao Cheng,Tiancheng Su,Jia Yuan,Guoxiu He,Jiawei Liu,Xinqi Tao,Jingwen Xie,Huaxia Li*

Key words: LLM, 幻觉检测, 思维链提示, 实验评估

TL;DR: 本文探讨了思维链（CoT）提示对大型语言模型（LLM）幻觉检测的影响，揭示了其在减少幻觉频率的同时，也会影响检测信号的有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: LLM常产生幻觉（错误内容），而CoT提示可能改善这一现象，但其对幻觉检测的影响尚未充分研究。

Method: 通过实验评估CoT提示对LLM内部状态和概率分布的影响，并比较其对幻觉检测方法的准确性、置信度和分数分布的影响。

Result: CoT提示虽减少幻觉，但会掩盖关键检测信号，降低检测方法的有效性。

Conclusion: 研究发现CoT提示在改善推理的同时，存在与幻觉检测的权衡，需进一步研究优化。

Abstract: Large Language Models (LLMs) often exhibit \textit{hallucinations},
generating factually incorrect or semantically irrelevant content in response
to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by
encouraging step-by-step reasoning, but its impact on hallucination detection
remains underexplored. To bridge this gap, we conduct a systematic empirical
evaluation. We begin with a pilot experiment, revealing that CoT reasoning
significantly affects the LLM's internal states and token probability
distributions. Building on this, we evaluate the impact of various CoT
prompting methods on mainstream hallucination detection methods across both
instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three
key dimensions: changes in hallucination score distributions, variations in
detection accuracy, and shifts in detection confidence. Our findings show that
while CoT prompting helps reduce hallucination frequency, it also tends to
obscure critical signals used for detection, impairing the effectiveness of
various detection methods. Our study highlights an overlooked trade-off in the
use of reasoning. Code is publicly available at:
https://anonymous.4open.science/r/cot-hallu-detect.

</details>


### [74] [Better Language Model Inversion by Compactly Representing Next-Token Distributions](https://arxiv.org/abs/2506.17090)
*Murtaza Nazir,Matthew Finlayson,John X. Morris,Xiang Ren,Swabha Swayamdipta*

Key words: 语言模型反演,安全,隐私,PILS,下一个词概率

TL;DR: 论文提出了一种新方法PILS，通过语言模型的连续生成步骤中的下一个词概率反推隐藏提示，大幅提升了反推成功率，并展示了其泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究语言模型反演能力的目的是为了揭示其在安全性和责任性方面的潜在风险，例如从API保护的语言模型中泄露私有信息。

Method: 提出PILS方法，利用语言模型在多个生成步骤中的下一个词概率分布的低维子空间特性，通过线性映射无损压缩概率分布信息，从而反推隐藏提示。

Result: PILS方法的隐藏提示恢复率比现有技术高2-3.5倍，泛化能力强，训练步数增加时表现更优，且在恢复系统消息任务中表现良好。

Conclusion: 语言模型的下一个词概率反演是一个比先前认识更脆弱的攻击面，研究为安全性和隐私保护提供了重要启示。

Abstract: Language model inversion seeks to recover hidden prompts using only language
model outputs. This capability has implications for security and accountability
in language model deployments, such as leaking private information from an
API-protected language model's system message. We propose a new method --
prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts
by gleaning clues from the model's next-token probabilities over the course of
multiple generation steps. Our method is enabled by a key insight: The
vector-valued outputs of a language model occupy a low-dimensional subspace.
This enables us to losslessly compress the full next-token probability
distribution over multiple generation steps using a linear map, allowing more
output information to be used for inversion. Our approach yields massive gains
over previous state-of-the-art methods for recovering hidden prompts, achieving
2--3.5 times higher exact recovery rates across test sets, in one case
increasing the recovery rate from 17% to 60%. Our method also exhibits
surprisingly good generalization behavior; for instance, an inverter trained on
16 generations steps gets 5--27 points higher prompt recovery when we increase
the number of steps to 32 at test time. Furthermore, we demonstrate strong
performance of our method on the more challenging task of recovering hidden
system messages. We also analyze the role of verbatim repetition in prompt
recovery and propose a new method for cross-family model transfer for
logit-based inverters. Our findings show that next-token probabilities are a
considerably more vulnerable attack surface for inversion attacks than
previously known.

</details>


### [75] [Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?](https://arxiv.org/abs/2506.17121)
*Adithya Bhaskar,Alexander Wettig,Tianyu Gao,Yihe Dong,Danqi Chen*

Key words: 语言模型, 键值缓存, 长上下文, 内存优化, PruLong

TL;DR: 论文提出了一种统一的评估指标*KV footprint*，用于衡量键值缓存（KV cache）的内存占用和生命周期，解决了现有方法在高内存峰值和性能下降方面的问题，并提出了一种新的优化方法PruLong以减少内存使用。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前语言模型处理长上下文任务时，键值缓存（KV cache）的内存成本不断增加，现有方法在高内存峰值和性能方面存在问题，缺乏统一的比较标准。

Method: 提出*KV footprint*作为统一评估指标，改进了*post-fill eviction*方法以降低内存占用，并提出了PruLong方法优化键值缓存的学习和管理。

Result: PruLong比现有方法减少了12%的KV footprint，同时保持了长上下文任务的性能，尤其是在召回任务中表现优异。

Conclusion: KV footprint为长上下文推理方法的比较提供了统一标准，PruLong为未来优化键值缓存提供了可行路径。

Abstract: Language models handle increasingly long contexts for tasks such as book
summarization, but this leads to growing memory costs for the key-value (KV)
cache. Many prior works have proposed ways of discarding KVs from memory, but
their approaches are tailored to favorable settings, obscuring caveats like
high peak memory and performance degradation, and a fair comparison between
methods is difficult. In this paper, we propose the *KV footprint* as a unified
metric, which accounts for both the amount of KV entries stored and their
lifespan in memory. We evaluate methods based on the smallest footprint they
attain while preserving performance in both long-context understanding and
generation, with context lengths of up to 128K tokens. This metric reveals the
high peak memory of prior KV eviction methods. One class of methods --
*post-fill eviction* -- has a high footprint due to being incompatible with
eviction during pre-filling. We adapt these methods to be able to evict KVs
during pre-filling, achieving substantially lower KV footprints. We then turn
to *recency eviction* methods, wherein we propose PruLong, an end-to-end
optimization method for learning which attention heads need to retain the full
KV cache and which do not. PruLong saves memory while preserving long-context
performance, achieving 12% smaller KV footprint than prior methods while
retaining performance in challenging recall tasks. Our paper clarifies the
complex tangle of long-context inference methods and paves the way for future
development to minimize the KV footprint.

</details>


### [76] [CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models](https://arxiv.org/abs/2506.17180)
*Naiming Liu,Richard Baraniuk,Shashank Sonkar*

Key words: CLEAR-3K, 因果推理, 语言模型, 语义相似性, 数据集

TL;DR: CLEAR-3K是一个包含3000个断言-推理问题的数据集，旨在评估语言模型是否能区分语义相关性与真实的因果关系。研究发现，模型易混淆语义相似性与因果关系，且随着参数规模增大，模型对因果关系的判断从过度怀疑转变为过度接受，但性能提升有限。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了开发和评估语言模型在真实因果推理方面的能力，填补现有基准的不足。

Method: 构建CLEAR-3K数据集，通过21种先进语言模型对断言-推理问题进行分析。

Result: 模型常混淆语义相似性与因果关系，性能提升有限（最高MCC为0.55）。

Conclusion: CLEAR-3K为语言模型的因果推理能力提供了关键基准，但现有模型仍需改进。

Abstract: We introduce CLEAR-3K, a dataset of 3,000 assertion-reasoning questions
designed to evaluate whether language models can determine if one statement
causally explains another. Each question present an assertion-reason pair and
challenge language models to distinguish between semantic relatedness and
genuine causal explanatory relationships. Through comprehensive evaluation of
21 state-of-the-art language models (ranging from 0.5B to 72B parameters), we
identify two fundamental findings. First, language models frequently confuse
semantic similarity with causality, relying on lexical and semantic overlap
instead of inferring actual causal explanatory relationships. Second, as
parameter size increases, models tend to shift from being overly skeptical
about causal relationships to being excessively permissive in accepting them.
Despite this shift, performance measured by the Matthews Correlation
Coefficient plateaus at just 0.55, even for the best-performing models.Hence,
CLEAR-3K provides a crucial benchmark for developing and evaluating genuine
causal reasoning in language models, which is an essential capability for
applications that require accurate assessment of causal relationships.

</details>


### [77] [Towards AI Search Paradigm](https://arxiv.org/abs/2506.17188)
*Yuchen Li,Hengyi Cai,Rui Kong,Xinran Chen,Jiamin Chen,Jun Yang,Haojie Zhang,Jiayi Li,Jiayi Wu,Yiqun Chen,Changle Qu,Keyi Kong,Wenwen Ye,Lixin Su,Xinyu Ma,Long Xia,Daiting Shi,Jiashu Zhao,Haoyi Xiong,Shuaiqiang Wang,Dawei Yin*

Key words: AI搜索范式、LLM代理、任务规划、检索增强生成、基础设施优化

TL;DR: 该论文提出了一种名为“AI搜索范式”的综合蓝图，通过四个基于LLM的代理（Master、Planner、Executor和Writer）动态适应各种信息需求，实现复杂任务的多阶段推理和协作工作流。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 旨在开发下一代搜索系统，以模拟人类的信息处理和决策能力，满足从简单查询到复杂推理任务的多样化需求。

Method: 采用模块化架构，通过协调工作流实现任务规划、工具集成、执行策略和检索增强生成，并结合算法与基础设施优化。

Result: 提出了一套系统性的方法论，为构建可信、适应性强且可扩展的AI搜索系统提供了基础指导。

Conclusion: 该范式为未来搜索系统的设计和开发提供了重要参考。

Abstract: In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint
for next-generation search systems capable of emulating human information
processing and decision-making. The paradigm employs a modular architecture of
four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically
adapt to the full spectrum of information needs, from simple factual queries to
complex multi-stage reasoning tasks. These agents collaborate dynamically
through coordinated workflows to evaluate query complexity, decompose problems
into executable plans, and orchestrate tool usage, task execution, and content
synthesis. We systematically present key methodologies for realizing this
paradigm, including task planning and tool integration, execution strategies,
aligned and robust retrieval-augmented generation, and efficient LLM inference,
spanning both algorithmic techniques and infrastructure-level optimizations. By
providing an in-depth guide to these foundational components, this work aims to
inform the development of trustworthy, adaptive, and scalable AI search
systems.

</details>


### [78] [Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency](https://arxiv.org/abs/2506.17209)
*Kathleen C. Fraser,Hillary Dawkins,Isar Nejadgholi,Svetlana Kiritchenko*

Key words: 大语言模型, 安全对齐, 微调, 评估方法, 安全性

TL;DR: 论文研究了微调通用大语言模型（LLM）时可能移除其安全对齐功能的问题，强调了这一现象的普遍性和潜在危害，并探讨了安全评估的可靠性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着微调技术的普及，用户可能在无意中部署安全性降低的LLM，而恶意行为者可能利用这一漏洞绕过安全防护。需要可靠的安全评估方法来缓解此问题。

Method: 通过实验考察安全基准测试对实验过程和LLM随机性的鲁棒性，分析微调设置中的微小变化如何影响评估结果。

Result: 实验发现安全评估结果对微调设置的微小变化表现出惊人的方差，影响了结果的可比较性。

Conclusion: 研究揭示了LLM安全评估中存在的显著不一致性，呼吁未来研究中需更严谨地报告结果以确保可比性。

Abstract: Fine-tuning a general-purpose large language model (LLM) for a specific
domain or task has become a routine procedure for ordinary users. However,
fine-tuning is known to remove the safety alignment features of the model, even
when the fine-tuning data does not contain any harmful content. We consider
this to be a critical failure mode of LLMs due to the widespread uptake of
fine-tuning, combined with the benign nature of the "attack". Most
well-intentioned developers are likely unaware that they are deploying an LLM
with reduced safety. On the other hand, this known vulnerability can be easily
exploited by malicious actors intending to bypass safety guardrails. To make
any meaningful progress in mitigating this issue, we first need reliable and
reproducible safety evaluations. In this work, we investigate how robust a
safety benchmark is to trivial variations in the experimental procedure, and
the stochastic nature of LLMs. Our initial experiments expose surprising
variance in the results of the safety evaluation, even when seemingly
inconsequential changes are made to the fine-tuning setup. Our observations
have serious implications for how researchers in this field should report
results to enable meaningful comparisons in the future.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [79] [Ignition Phase : Standard Training for Fast Adversarial Robustness](https://arxiv.org/abs/2506.15685)
*Wang Yu-Hang,Liu ying,Fang liang,Wang Xuelin,Junkang Guo,Shiwei Li,Lei Gao,Jian Liu,Wenfei Yin*

Key words: Adversarial Training, AET, ERM, robustness, feature representation

TL;DR: Adversarial Evolution Training (AET) 通过前置ERM阶段提升对抗训练的效果，实现更高效率和更低成本的鲁棒性提升。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有对抗训练方法过于关注攻击生成，忽视了基础特征表示的重要性。

Method: 在传统对抗训练前加入一个ERM阶段，优化特征流形。

Result: AET在多种数据集和架构上表现优异，训练成本降低8-25%，同时保持了干净数据的准确性。

Conclusion: 特征预条件通过标准训练对高效鲁棒防御的重要性得到验证。

Abstract: Adversarial Training (AT) is a cornerstone defense, but many variants
overlook foundational feature representations by primarily focusing on stronger
attack generation. We introduce Adversarial Evolution Training (AET), a simple
yet powerful framework that strategically prepends an Empirical Risk
Minimization (ERM) phase to conventional AT. We hypothesize this initial ERM
phase cultivates a favorable feature manifold, enabling more efficient and
effective robustness acquisition. Empirically, AET achieves comparable or
superior robustness more rapidly, improves clean accuracy, and cuts training
costs by 8-25\%. Its effectiveness is shown across multiple datasets,
architectures, and when augmenting established AT methods. Our findings
underscore the impact of feature pre-conditioning via standard training for
developing more efficient, principled robust defenses. Code is available in the
supplementary material.

</details>


### [80] [Learning from M-Tuple Dominant Positive and Unlabeled Data](https://arxiv.org/abs/2506.15686)
*Jiahe Qin,Junpeng Li,Changchun Hua,Yana Yang*

Key words: 标签比例学习（LLP）、无偏风险估计、经验风险最小化（ERM）、泛化误差、风险校正

TL;DR: 论文提出了一种广义学习框架 MDPU，用于解决标签比例学习（LLP）中比例信息难以精确获取的问题，并通过数学建模和风险校正方法提升了分类效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在实际应用中，很难获取关于实例比例的精确监督信息。为了更好适应现实场景并利用元组内实例的比例约束，提出了 MDPU 框架。

Method: 首先对任意大小元组内的实例分布进行数学建模，通过经验风险最小化（ERM）方法推导出满足风险一致性的无偏风险估计器，并引入风险校正方法以减少过拟合。

Result: 理论证明了无偏风险估计器的泛化误差界，并通过多数据集实验验证了 MDPU 框架的有效性。

Conclusion: MDPU 框架通过风险校正和数学建模，显著提升了 LLP 任务的分类性能，适用于现实场景。

Abstract: Label Proportion Learning (LLP) addresses the classification problem where
multiple instances are grouped into bags and each bag contains information
about the proportion of each class. However, in practical applications,
obtaining precise supervisory information regarding the proportion of instances
in a specific class is challenging. To better align with real-world application
scenarios and effectively leverage the proportional constraints of instances
within tuples, this paper proposes a generalized learning framework
\emph{MDPU}. Specifically, we first mathematically model the distribution of
instances within tuples of arbitrary size, under the constraint that the number
of positive instances is no less than that of negative instances. Then we
derive an unbiased risk estimator that satisfies risk consistency based on the
empirical risk minimization (ERM) method. To mitigate the inevitable
overfitting issue during training, a risk correction method is introduced,
leading to the development of a corrected risk estimator. The generalization
error bounds of the unbiased risk estimator theoretically demonstrate the
consistency of the proposed method. Extensive experiments on multiple datasets
and comparisons with other relevant baseline methods comprehensively validate
the effectiveness of the proposed learning framework.

</details>


### [81] [S$^2$GPT-PINNs: Sparse and Small models for PDEs](https://arxiv.org/abs/2506.15687)
*Yajie Ji,Yanlai Chen,Shawn Koohy*

Key words: S$^2$GPT-PINN, 参数化偏微分方程, 知识蒸馏, 智能降采样

TL;DR: S$^2$GPT-PINN是一种稀疏小模型，用于求解参数化偏微分方程，通过知识蒸馏和智能降采样实现高效计算。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为特定领域的偏微分方程家族设计一个紧凑且计算高效的模型。

Method: 结合知识蒸馏（任务特定激活函数）和智能降采样（减少数据点数量）的两级定制方法。

Result: 模型参数比传统PINNs少得多，但仍保持极高效率。

Conclusion: S$^2$GPT-PINN为参数化偏微分方程提供了一种高效的小模型解决方案。

Abstract: We propose S$^2$GPT-PINN, a sparse and small model for solving parametric
partial differential equations (PDEs). Similar to Small Language Models (SLMs),
S$^2$GPT-PINN is tailored to domain-specific (families of) PDEs and
characterized by its compact architecture and minimal computational power.
Leveraging a small amount of extremely high quality data via a mathematically
rigorous greedy algorithm that is enabled by the large full-order models,
S$^2$GPT-PINN relies on orders of magnitude less parameters than PINNs to
achieve extremely high efficiency via two levels of customizations. The first
is knowledge distillation via task-specific activation functions that are
transferred from Pre-Trained PINNs. The second is a judicious down-sampling
when calculating the physics-informed loss of the network compressing the
number of data sites by orders of magnitude to the size of the small model.

</details>


### [82] [Cellular Traffic Prediction via Deep State Space Models with Attention Mechanism](https://arxiv.org/abs/2506.15688)
*Hui Ma,Kai Yang,Man-On Pun*

Key words: 蜂窝流量预测, 时空模式, 卷积神经网络, 注意力机制, 卡尔曼滤波

TL;DR: 提出了一种端到端框架，通过卷积神经网络与注意力机制捕捉空间动态，结合卡尔曼滤波进行时间建模，以提升蜂窝流量预测准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 蜂窝流量预测对运营商管理网络资源和决策至关重要，但由于其高度动态性和受多种外生因素影响，预测准确性易受损害。

Method: 采用端到端框架，结合卷积神经网络（带注意力机制）和卡尔曼滤波，利用辅助信息（如社交活动）优化预测性能。

Result: 在三个真实数据集上的实验表明，所提模型在预测准确性上优于现有机器学习技术。

Conclusion: 提出的框架能有效捕捉时空模式，显著提升蜂窝流量预测性能。

Abstract: Cellular traffic prediction is of great importance for operators to manage
network resources and make decisions. Traffic is highly dynamic and influenced
by many exogenous factors, which would lead to the degradation of traffic
prediction accuracy. This paper proposes an end-to-end framework with two
variants to explicitly characterize the spatiotemporal patterns of cellular
traffic among neighboring cells. It uses convolutional neural networks with an
attention mechanism to capture the spatial dynamics and Kalman filter for
temporal modelling. Besides, we can fully exploit the auxiliary information
such as social activities to improve prediction performance. We conduct
extensive experiments on three real-world datasets. The results show that our
proposed models outperform the state-of-the-art machine learning techniques in
terms of prediction accuracy.

</details>


### [83] [BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models](https://arxiv.org/abs/2506.15689)
*Liulu He,Shenli Zhen,Karwei Sun,Yijiang Liu,Yufei Zhao,Chongkang Tan,Huanrui Yang,Yuan Du,Li Du*

Key words: 量化, 大语言模型, 旋转, 偏置校正, 非对称缩放

TL;DR: 论文提出了BASE-Q，一种结合偏置校正和非对称缩放的方法，用于优化LLM量化中的旋转问题，减少误差并支持分块优化。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有旋转量化方法存在对齐失败和高斯化激活分布的问题，导致量化误差和能量损失。

Method: 提出BASE-Q，结合偏置校正和非对称缩放，减少舍入和截断误差，并支持分块优化。

Result: 实验证明BASE-Q显著提升了量化精度，与QuaRot、SpinQuant和OSTQuant相比，差距分别缩小50.5%、42.9%和29.2%。

Conclusion: BASE-Q是一种简单有效的方法，显著改善了量化性能，同时减少了内存消耗。

Abstract: Rotations have become essential to state-of-the-art quantization pipelines
for large language models (LLMs) by effectively smoothing outliers in weights
and activations. However, further optimizing the rotation parameters offers
only limited performance gains and introduces significant training overhead:
due to rotation parameter sharing, full-model must be loaded simultaneously to
enable backpropagation, resulting in substantial memory consumption and limited
practical utility. In this work, we identify two fundamental limitations of
current rotational quantization methods: (i) rotation fails to align channel
means, resulting in wider quantization bounds and increased rounding errors;
and (ii) rotation makes the activation distribution more Gaussian-like,
increasing energy loss caused by clipping errors. To address these issues, we
introduce \textbf{BASE-Q}, a simple yet powerful approach that combines bias
correction and asymmetric scaling to effectively reduce rounding and clipping
errors. Furthermore, BASE-Q enables blockwise optimization, eliminating the
need for memory-intensive full-model backpropagation. Extensive experiments on
various LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing
the accuracy gap to full-precision models by 50.5\%, 42.9\%, and 29.2\%
compared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be
released soon.

</details>


### [84] [LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs](https://arxiv.org/abs/2506.15690)
*Tianyu Wang,Lingyou Pang,Akira Horiguchi,Carey E. Priebe*

Key words: 大型语言模型,模型崩溃,合成数据,RAG数据库,高斯混合模型

TL;DR: 研究了大型语言模型训练中合成数据使用的潜在威胁，提出了一种网络级别的框架LWD，用于分析模型崩溃现象。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨合成数据在LLM训练中的使用及其潜在的模型崩溃威胁，现有研究多限于单一模型或统计替代。

Method: 提出了LLM Web Dynamics（LWD）框架，通过RAG数据库模拟互联网，分析模型输出的收敛模式。

Result: 通过类比相互作用的高斯混合模型，为收敛模式提供了理论保证。

Conclusion: LWD框架有效揭示了模型崩溃的网络级现象，并为未来研究提供了理论基础。

Abstract: The increasing use of synthetic data from the public Internet has enhanced
data usage efficiency in large language model (LLM) training. However, the
potential threat of model collapse remains insufficiently explored. Existing
studies primarily examine model collapse in a single model setting or rely
solely on statistical surrogates. In this work, we introduce LLM Web Dynamics
(LWD), an efficient framework for investigating model collapse at the network
level. By simulating the Internet with a retrieval-augmented generation (RAG)
database, we analyze the convergence pattern of model outputs. Furthermore, we
provide theoretical guarantees for this convergence by drawing an analogy to
interacting Gaussian Mixture Models.

</details>


### [85] [What Do Latent Action Models Actually Learn?](https://arxiv.org/abs/2506.15691)
*Chuheng Zhang,Tim Pearce,Pushi Zhang,Kaixin Wang,Xiaoyu Chen,Wei Shen,Li Zhao,Jiang Bian*

Key words: 潜在动作模型, 线性模型, PCA, 数据增强, 噪声

TL;DR: 该论文通过线性模型分析潜在动作模型（LAMs）是否能捕捉由动作引起的变化，而非无关噪声，并探讨了数据增强、清理等方法的效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究潜在动作模型（LAMs）在未标记视频中学习时，是否能区分由动作引起的变化与噪声引起的变化。

Method: 提出一个可解析的线性模型，探讨LAM学习与PCA的联系，并通过数据增强、数据清理和辅助动作预测等策略优化学习。

Result: 通过数值模拟，揭示了观察数据中动作、噪声等结构对LAM学习的影响。

Conclusion: 线性模型为LAM学习提供了理论支持，数据增强和方法优化能有效提升模型捕捉可控变化的能力。

Abstract: Latent action models (LAMs) aim to learn action-relevant changes from
unlabeled videos by compressing changes between frames as latents. However,
differences between video frames can be caused by controllable changes as well
as exogenous noise, leading to an important concern -- do latents capture the
changes caused by actions or irrelevant noise? This paper studies this issue
analytically, presenting a linear model that encapsulates the essence of LAM
learning, while being tractable.This provides several insights, including
connections between LAM and principal component analysis (PCA), desiderata of
the data-generating policy, and justification of strategies to encourage
learning controllable changes using data augmentation, data cleaning, and
auxiliary action-prediction. We also provide illustrative results based on
numerical simulation, shedding light on the specific structure of observations,
actions, and noise in data that influence LAM learning.

</details>


### [86] [MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement](https://arxiv.org/abs/2506.15692)
*Jaehyun Nam,Jinsung Yoon,Jiefeng Chen,Jinwoo Shin,Sercan Ö. Arık,Tomas Pfister*

Key words: 大型语言模型,机器学习工程,代码生成,消融研究,集成方法

TL;DR: MLE-STAR是一种新的基于大型语言模型（LLM）的代理方法，通过结合外部知识和细粒度探索策略，显著提升了机器学习工程任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有基于LLM的代理方法过度依赖其固有知识，且探索策略粗糙，无法深入优化特定组件（如特征工程），限制了模型选择和改进能力。

Method: MLE-STAR通过搜索引擎获取外部知识构建初始方案，随后通过针对性组件探索和消融研究迭代优化，并引入了一种新颖的集成方法。

Result: 实验显示MLE-STAR在44%的Kaggle竞赛中取得奖牌，显著优于其他替代方法。

Conclusion: MLE-STAR通过结合外部知识和细粒度探索，显著提升了机器学习工程任务的自动化能力。

Abstract: Agents based on large language models (LLMs) for machine learning engineering
(MLE) can automatically implement ML models via code generation. However,
existing approaches to build such agents often rely heavily on inherent LLM
knowledge and employ coarse exploration strategies that modify the entire code
structure at once. This limits their ability to select effective task-specific
models and perform deep exploration within specific components, such as
experimenting extensively with feature engineering options. To overcome these,
we propose MLE-STAR, a novel approach to build MLE agents. MLE-STAR first
leverages external knowledge by using a search engine to retrieve effective
models from the web, forming an initial solution, then iteratively refines it
by exploring various strategies targeting specific ML components. This
exploration is guided by ablation studies analyzing the impact of individual
code blocks. Furthermore, we introduce a novel ensembling method using an
effective strategy suggested by MLE-STAR. Our experimental results show that
MLE-STAR achieves medals in 44% of the Kaggle competitions on the MLE-bench,
significantly outperforming the best alternative.

</details>


### [87] [Verifiable Safety Q-Filters via Hamilton-Jacobi Reachability and Multiplicative Q-Networks](https://arxiv.org/abs/2506.15693)
*Jiaxing Li,Hanjiang Hu,Yujie Yang,Changliu Liu*

Key words: 安全过滤器, Hamilton-Jacobi可达性分析, Q值函数, 形式化验证, 无模型

TL;DR: 本文提出了一种基于Hamilton-Jacobi可达性分析的可验证无模型安全过滤器，解决了传统CBFs和学习型方法的安全性问题，并展示了四大基准测试中的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 学习型安全过滤器在复杂约束下优于传统方法（如手工制作的CBFs），但缺乏形式化安全保证。本文旨在填补这一空白。

Method: 1)扩展Q值函数的可验证自一致性属性；2)提出乘法Q网络结构以减少零子水平集收缩问题；3)开发验证管道以确保自一致性属性的严格验证。

Result: 在四项标准安全控制基准测试中，成功合成了形式化验证的无模型安全证书。

Conclusion: 所提出的方法在保证安全性的同时，展示了其在实际应用中的效果和验证的可行性。

Abstract: Recent learning-based safety filters have outperformed conventional methods,
such as hand-crafted Control Barrier Functions (CBFs), by effectively adapting
to complex constraints. However, these learning-based approaches lack formal
safety guarantees. In this work, we introduce a verifiable model-free safety
filter based on Hamilton-Jacobi reachability analysis. Our primary
contributions include: 1) extending verifiable self-consistency properties for
Q value functions, 2) proposing a multiplicative Q-network structure to
mitigate zero-sublevel-set shrinkage issues, and 3) developing a verification
pipeline capable of soundly verifying these self-consistency properties. Our
proposed approach successfully synthesizes formally verified, model-free safety
certificates across four standard safe-control benchmarks.

</details>


### [88] [Development of a Multiprocessing Interface Genetic Algorithm for Optimising a Multilayer Perceptron for Disease Prediction](https://arxiv.org/abs/2506.15694)
*Iliyas Ibrahim Iliyas,Souley Boukari,Abdulsalam Yau Gital*

Key words: 非线性特征提取, 核主成分分析, 多层感知机, 并行遗传算法, 疾病预测

TL;DR: 该研究提出了一种结合非线性特征提取、分类和高效优化的框架，通过核主成分分析、多层感知机和并行遗传算法优化，在多个数据集上取得了优于其他方法的分类准确率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了更高效地进行疾病状态预测，研究旨在开发一种能够同时优化特征提取和分类器超参数的框架。

Method: 结合核主成分分析（KPCA）降维、多层感知机（MLP）分类，以及并行多处理遗传算法（MIGA）进行超参数优化。

Result: 在乳腺癌、帕金森病和慢性肾病数据集上，优化后的MLP分别达到了99.12%、94.87%和100%的准确率，显著优于其他优化方法。

Conclusion: 该框架通过KPCA揭示非线性关系，并通过MIGA显著减少了优化时间，同时提升了分类性能。

Abstract: This study introduces a framework that integrates nonlinear feature
extraction, classification, and efficient optimization. First, kernel principal
component analysis with a radial basis function kernel reduces dimensionality
while preserving 95% of the variance. Second, a multilayer perceptron (MLP)
learns to predict disease status. Finally, a modified multiprocessing genetic
algorithm (MIGA) optimizes MLP hyperparameters in parallel over ten
generations. We evaluated this approach on three datasets: the Wisconsin
Diagnostic Breast Cancer dataset, the Parkinson's Telemonitoring dataset, and
the chronic kidney disease dataset. The MLP tuned by the MIGA achieved the best
accuracy of 99.12% for breast cancer, 94.87% for Parkinson's disease, and 100%
for chronic kidney disease. These results outperform those of other methods,
such as grid search, random search, and Bayesian optimization. Compared with a
standard genetic algorithm, kernel PCA revealed nonlinear relationships that
improved classification, and the MIGA's parallel fitness evaluations reduced
the tuning time by approximately 60%. The genetic algorithm incurs high
computational cost from sequential fitness evaluations, but our multiprocessing
interface GA (MIGA) parallelizes this step, slashing the tuning time and
steering the MLP toward the best accuracy score of 99.12%, 94.87%, and 100% for
breast cancer, Parkinson's disease, and CKD, respectively.

</details>


### [89] [SimuGen: Multi-modal Agentic Framework for Constructing Block Diagram-Based Simulation Models](https://arxiv.org/abs/2506.15695)
*Xinxing Ren,Qianbo Zang,Zekun Guo*

Key words: 大语言模型, Simulink, 多模态智能体, 代码生成, 仿真

TL;DR: 这篇论文提出了一种名为SimuGen的多模态智能体框架，旨在解决大语言模型（LLMs）在生成Simulink仿真代码时的局限性，通过结合视觉图表和领域知识来生成准确的仿真代码。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管LLMs在数学推理和代码生成方面表现优异，但在Simulink模型生成领域仍存在不足，主要是因为缺乏Simulink特定数据的预训练。

Method: 论文提出了SimuGen，一个基于多模态智能体的框架，结合视觉Simulink图表和领域知识，通过协调多个专用智能体（如调查员、单元测试审查员、代码生成器等）生成Simulink仿真代码。

Result: SimuGen能够生成可解释、鲁棒且可复现的Simulink仿真代码，其源代码已在GitHub上公开。

Conclusion: SimuGen通过协作和模块化设计，显著提升了LLMs在Simulink仿真代码生成领域的性能。

Abstract: Recent advances in large language models (LLMs) have shown impressive
performance in mathematical reasoning and code generation. However, LLMs still
struggle in the simulation domain, particularly in generating Simulink models,
which are essential tools in engineering and scientific research. Our
preliminary experiments indicate that LLM agents often fail to produce reliable
and complete Simulink simulation code from text-only inputs, likely due to the
lack of Simulink-specific data in their pretraining. To address this challenge,
we propose SimuGen, a multimodal agent-based framework that automatically
generates accurate Simulink simulation code by leveraging both the visual
Simulink diagram and domain knowledge. SimuGen coordinates several specialized
agents, including an investigator, unit test reviewer, code generator,
executor, debug locator, and report writer, supported by a domain-specific
knowledge base. This collaborative and modular design enables interpretable,
robust, and reproducible Simulink simulation generation. Our source code is
publicly available at https://github.com/renxinxing123/SimuGen_beta.

</details>


### [90] [CoC: Chain-of-Cancer based on Cross-Modal Autoregressive Traction for Survival Prediction](https://arxiv.org/abs/2506.15696)
*Haipeng Zhou,Sicheng Yang,Sihan Yang,Jing Qin,Lei Chen,Lei Zhu*

Key words: 生存预测, 多模态学习, 癌症, Chain-of-Cancer, 自回归互吸引

TL;DR: 该论文提出了一种基于四模态（三种临床数据和语言）的癌症生存预测框架Chain-of-Cancer（CoC），通过intra-learning和inter-learning实现多模态联合学习，并在五个公开数据集上验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法主要依赖病理学和基因组数据，未充分利用表观遗传学数据和语言描述，论文首次探索四模态联合预测癌症患者生存风险。

Method: 提出CoC框架，通过intra-learning处理临床数据的领域特征，inter-learning用语言提示原始特征并引入自回归互吸引模块实现协同表示。

Result: 在五个公开癌症数据集上的实验验证了方法的有效性，取得了state-of-the-art（SOTA）结果。

Conclusion: CoC框架通过多模态联合学习显著提升癌症生存预测性能，代码将开源。

Abstract: Survival prediction aims to evaluate the risk level of cancer patients.
Existing methods primarily rely on pathology and genomics data, either
individually or in combination. From the perspective of cancer pathogenesis,
epigenetic changes, such as methylation data, could also be crucial for this
task. Furthermore, no previous endeavors have utilized textual descriptions to
guide the prediction. To this end, we are the first to explore the use of four
modalities, including three clinical modalities and language, for conducting
survival prediction. In detail, we are motivated by the Chain-of-Thought (CoT)
to propose the Chain-of-Cancer (CoC) framework, focusing on intra-learning and
inter-learning. We encode the clinical data as the raw features, which remain
domain-specific knowledge for intra-learning. In terms of inter-learning, we
use language to prompt the raw features and introduce an Autoregressive Mutual
Traction module for synergistic representation. This tailored framework
facilitates joint learning among multiple modalities. Our approach is evaluated
across five public cancer datasets, and extensive experiments validate the
effectiveness of our methods and proposed designs, leading to producing \sota
results. Codes will be released.

</details>


### [91] [Global Context-aware Representation Learning for Spatially Resolved Transcriptomics](https://arxiv.org/abs/2506.15698)
*Yunhak Oh,Junseok Lee,Yeongmin Kim,Sangwoo Seo,Namkyeong Lee,Chanyoung Park*

Key words: 空间转录组学, 斑点表示, 图方法, 多切片集成

TL;DR: 提出了一种名为Spotscape的新框架，通过Similarity Telescope模块和相似性缩放策略改进空间转录组学中的斑点表示，优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有基于图的方法在捕捉斑点间全局关系和边界区域表现不佳，无法有效处理多切片集成问题。

Method: Spotscape引入Similarity Telescope模块和相似性缩放策略，优化斑点间关系表示和多切片集成。

Result: 实验证明Spotscape在单切片和多切片场景中的下游任务中表现优越。

Conclusion: Spotscape为空间转录组学提供了一种更有效的斑点表示和多切片集成方法。

Abstract: Spatially Resolved Transcriptomics (SRT) is a cutting-edge technique that
captures the spatial context of cells within tissues, enabling the study of
complex biological networks. Recent graph-based methods leverage both gene
expression and spatial information to identify relevant spatial domains.
However, these approaches fall short in obtaining meaningful spot
representations, especially for spots near spatial domain boundaries, as they
heavily emphasize adjacent spots that have minimal feature differences from an
anchor node. To address this, we propose Spotscape, a novel framework that
introduces the Similarity Telescope module to capture global relationships
between multiple spots. Additionally, we propose a similarity scaling strategy
to regulate the distances between intra- and inter-slice spots, facilitating
effective multi-slice integration. Extensive experiments demonstrate the
superiority of Spotscape in various downstream tasks, including single-slice
and multi-slice scenarios. Our code is available at the following link: https:
//github.com/yunhak0/Spotscape.

</details>


### [92] [BLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain Overlap](https://arxiv.org/abs/2506.15699)
*Shengyuan Hu,Neil Kale,Pratiksha Thaker,Yiwei Fu,Steven Wu,Virginia Smith*

Key words: 大语言模型, 遗忘技术, 基准测试, 安全评估

TL;DR: 提出了一个名为BLUR的新基准测试，用于更真实地评估大语言模型的遗忘效果，并发现现有方法在高重叠场景下表现不佳。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的大语言模型遗忘基准测试中，遗忘集和保留集差异过大，导致无法真实反映遗忘方法的有效性，容易受到良性扰动的影响。

Method: 设计了BLUR基准测试，提供更现实的遗忘-保留重叠场景，包括扩展评估任务、混合查询和不同难度的重新学习数据集。

Result: BLUR测试下，现有遗忘方法的性能显著下降，简单方法反而表现更好。

Conclusion: 强调稳健评估的重要性，并为未来研究提供了方向。

Abstract: Machine unlearning has the potential to improve the safety of large language
models (LLMs) by removing sensitive or harmful information post hoc. A key
challenge in unlearning involves balancing between forget quality (effectively
unlearning undesirable information) and retain quality (maintaining good
performance on other, general tasks). Unfortunately, as we show, current LLM
unlearning benchmarks contain highly disparate forget and retain sets --
painting a false picture of the effectiveness of LLM unlearning methods. This
can be particularly problematic because it opens the door for benign
perturbations, such as relearning attacks, to easily reveal supposedly
unlearned knowledge once models are deployed. To address this, we present
$\texttt{BLUR}$: a benchmark for LLM unlearning that provides more realistic
scenarios of forget-retain overlap. $\texttt{BLUR}$ significantly expands on
existing unlearning benchmarks by providing extended evaluation tasks, combined
forget/retain queries, and relearning datasets of varying degrees of
difficulty. Despite the benign nature of the queries considered, we find that
the performance of existing methods drops significantly when evaluated on
$\texttt{BLUR}$, with simple approaches performing better on average than more
recent methods. These results highlight the importance of robust evaluation and
suggest several important directions of future study. Our benchmark is publicly
available at: https://huggingface.co/datasets/forgelab/BLUR

</details>


### [93] [Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning for Robust Path Tracking](https://arxiv.org/abs/2506.15700)
*Minjae Cho,Hiroyasu Tsukamoto,Huy Trong Tran*

Key words: 控制收缩度量, 强化学习, 收缩理论, 最优控制

TL;DR: 论文提出了一种将控制收缩度量（CCM）与强化学习（RL）结合的算法，以提升控制策略的长期最优性和适应性，尤其适用于未知动力学系统。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: CCM虽能确保闭环系统的增量稳定性，但缺乏优化能力且依赖已知动力学模型。为解决这些问题，研究者将CCM与RL结合，自动化提升控制策略的性能。

Method: 提出收缩行为者-评论家（CAC）算法，通过学习收缩度量生成器（CMG）和基于CMG的优化策略，结合动力学模型和RL。

Result: 实验证明CAC算法在模拟和实际机器人任务中优于现有基线，并能自动生成具有长期最优性的收缩策略。

Conclusion: CCM与RL的结合有效解决了CCM的局限性，为复杂系统的控制提供了自动化且性能优异的解决方案。

Abstract: Control contraction metrics (CCMs) provide a framework to co-synthesize a
controller and a corresponding contraction metric -- a positive-definite
Riemannian metric under which a closed-loop system is guaranteed to be
incrementally exponentially stable. However, the synthesized controller only
ensures that all the trajectories of the system converge to one single
trajectory and, as such, does not impose any notion of optimality across an
entire trajectory. Furthermore, constructing CCMs requires a known dynamics
model and non-trivial effort in solving an infinite-dimensional convex
feasibility problem, which limits its scalability to complex systems featuring
high dimensionality with uncertainty. To address these issues, we propose to
integrate CCMs into reinforcement learning (RL), where CCMs provide
dynamics-informed feedback for learning control policies that minimize
cumulative tracking error under unknown dynamics. We show that our algorithm,
called contraction actor-critic (CAC), formally enhances the capability of CCMs
to provide a set of contracting policies with the long-term optimality of RL in
a fully automated setting. Given a pre-trained dynamics model, CAC
simultaneously learns a contraction metric generator (CMG) -- which generates a
contraction metric -- and uses an actor-critic algorithm to learn an optimal
tracking policy guided by that metric. We demonstrate the effectiveness of our
algorithm relative to established baselines through extensive empirical
studies, including simulated and real-world robot experiments, and provide a
theoretical rationale for incorporating contraction theory into RL.

</details>


### [94] [Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning](https://arxiv.org/abs/2506.15701)
*Haolin Pan,Hongyu Lin,Haoran Luo,Yang Liu,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Key words: 编译器自动调优,强化学习,LLM,IR指令数优化

TL;DR: 引入Compiler-R1，首个基于强化学习的框架，用于增强LLM在编译器自动调优中的能力，通过高质量数据集和两阶段训练流程显著降低IR指令数。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前LLM在编译器自动调优中面临高质量推理数据集缺失和与编译环境互动有限的问题，Compiler-R1旨在解决这些挑战。

Method: 提出Compiler-R1框架，包含精选的推理数据集和两阶段端到端强化学习训练流程，通过基于结果的奖励实现高效环境探索和学习。

Result: 在7个数据集上的实验显示，Compiler-R1平均减少8.46%的IR指令数，优于opt -Oz。

Conclusion: Compiler-R1展示了基于强化学习的LLM在编译器优化中的强大潜力，其代码和数据集已开源。

Abstract: Compiler auto-tuning optimizes pass sequences to improve performance metrics
such as Intermediate Representation (IR) instruction count. Although recent
advances leveraging Large Language Models (LLMs) have shown promise in
automating compiler tuning, two significant challenges still remain: the
absence of high-quality reasoning datasets for agents training, and limited
effective interactions with the compilation environment. In this work, we
introduce Compiler-R1, the first reinforcement learning (RL)-driven framework
specifically augmenting LLM capabilities for compiler auto-tuning. Compiler-R1
features a curated, high-quality reasoning dataset and a novel two-stage
end-to-end RL training pipeline, enabling efficient environment exploration and
learning through an outcome-based reward. Extensive experiments across seven
datasets demonstrate Compiler-R1 achieving an average 8.46% IR instruction
count reduction compared to opt -Oz, showcasing the strong potential of
RL-trained LLMs for compiler optimization. Our code and datasets are publicly
available at https://github.com/Panhaolin2001/Compiler-R1.

</details>


### [95] [Minifinetuning: Low-Data Generation Domain Adaptation through Corrective Self-Distillation](https://arxiv.org/abs/2506.15702)
*Peter Belcak,Greg Heinrich,Jan Kautz,Pavlo Molchanov*

Key words: minifinetuning, language model, domain adaptation, self-distillation

TL;DR: 论文介绍了一种名为minifinetuning（MFT）的方法，用于减少在低数据环境下语言模型领域适应时的过拟合问题，效果优于标准微调。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决语言模型在新领域微调时导致通用性能下降的问题，特别是在数据资源有限的情况下。

Method: 采用样本级别的矫正自蒸馏技术，无需预训练数据回放。

Result: 在多种模型和领域中，MFT表现出比标准微调2-10倍的优势，对数据稀缺（低至500样本）表现出鲁棒性。

Conclusion: MFT有效缓解了微调导致的性能下降，且与其他参数高效调优方法兼容。

Abstract: Finetuning language models for a new domain inevitably leads to the
deterioration of their general performance. This becomes more pronounced the
more limited the finetuning data resource.
  We introduce minifinetuning (MFT), a method for language model domain
adaptation that considerably reduces the effects of overfitting-induced
degeneralization in low-data settings and which does so in the absence of any
pre-training data for replay. MFT demonstrates 2-10x more favourable
specialization-to-degeneralization ratios than standard finetuning across a
wide range of models and domains and exhibits an intrinsic robustness to
overfitting when data in the new domain is scarce and down to as little as 500
samples.
  Employing corrective self-distillation that is individualized on the sample
level, MFT outperforms parameter-efficient finetuning methods, demonstrates
replay-like degeneralization mitigation properties, and is composable with
either for a combined effect.

</details>


### [96] [Federated Incomplete Multi-view Clustering with Globally Fused Graph Guidance](https://arxiv.org/abs/2506.15703)
*Guoqing Chao,Zhenghao Zhang,Lei Meng,Jie Wen,Dianhui Chu*

Key words: 联邦学习,多视图聚类,图卷积网络,缺失数据

TL;DR: 论文提出了一种名为FIMCFG的联邦不完全多视图聚类方法，利用全局融合图指导解决现有方法在特征提取和缺失数据问题上的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有联邦多视图聚类方法仅依赖全局伪标签指导聚类，未充分利用全局信息，且对缺失数据问题研究不足。

Method: 设计双头图卷积编码器提取全局和视图特定特征，通过融合图指导特征融合和伪标签监督聚类，并在服务器端优化图融合和伪标签计算。

Result: 实验证明FIMCFG方法具有有效性和优越性。

Conclusion: FIMCFG解决了联邦多视图聚类中的特征提取和缺失数据问题，提升了聚类性能。

Abstract: Federated multi-view clustering has been proposed to mine the valuable
information within multi-view data distributed across different devices and has
achieved impressive results while preserving the privacy. Despite great
progress, most federated multi-view clustering methods only used global
pseudo-labels to guide the downstream clustering process and failed to exploit
the global information when extracting features. In addition, missing data
problem in federated multi-view clustering task is less explored. To address
these problems, we propose a novel Federated Incomplete Multi-view Clustering
method with globally Fused Graph guidance (FIMCFG). Specifically, we designed a
dual-head graph convolutional encoder at each client to extract two kinds of
underlying features containing global and view-specific information.
Subsequently, under the guidance of the fused graph, the two underlying
features are fused into high-level features, based on which clustering is
conducted under the supervision of pseudo-labeling. Finally, the high-level
features are uploaded to the server to refine the graph fusion and
pseudo-labeling computation. Extensive experimental results demonstrate the
effectiveness and superiority of FIMCFG. Our code is publicly available at
https://github.com/PaddiHunter/FIMCFG.

</details>


### [97] [Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding](https://arxiv.org/abs/2506.15704)
*Feiyu Yao,Qian Wang*

Key words: 大语言模型(LLM), 稀疏注意力, 解码优化, 长上下文, 索引检索

TL;DR: LFPS是一种基于历史注意力模式的稀疏索引加速方法，用于优化长上下文LLM推理的解码过程，显著提升速度且保持生成准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着LLM支持更长上下文，KV缓存的存储需求急剧增加，成为GPU内存和PCIe带宽的瓶颈。稀疏注意力机制虽能缓解，但索引计算成本高且未充分利用历史解码信息。

Method: LFPS动态构建稀疏索引候选，利用历史注意力模式中的垂直和斜线趋势，结合位置扩展策略预测当前步骤的Top-k索引。

Result: 在LongBench-RULER等长上下文基准测试中，LFPS比全注意力快22.8倍，比精确Top-k检索快9.6倍，且生成准确性不受影响。

Conclusion: LFPS为长上下文LLM推理提供了高效且实用的解码优化方案。

Abstract: As large language models (LLMs) continue to support increasingly longer
contexts, the memory demand for key-value (KV) caches during decoding grows
rapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe
bandwidth. Sparse attention mechanisms alleviate this issue by computing
attention weights only for selected key-value pairs. However, their indexing
computation typically requires traversing all key vectors, resulting in
significant computational and data transfer overhead. To reduce the cost of
index retrieval, existing methods often treat each decoding step as an
independent process, failing to exploit the temporal correlations embedded in
historical decoding information. To this end, we propose LFPS(Learn From the
Past for Sparse Indexing), an acceleration method that dynamically constructs
sparse indexing candidates based on historical attention patterns. LFPS
captures two prevalent trends in decoder attention -vertical patterns
(attending to fixed positions) and slash patterns (attending to relative
positions) -and incorporates a positional expansion strategy to effectively
predict the Top-k indices for the current step. We validate LFPS on challenging
long-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as
the base model. Experimental results show that LFPS achieves up to 22.8$\times$
speedup over full attention and 9.6$\times$ speedup over exact Top-k retrieval
on an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively,
while preserving generation accuracy. These results demonstrate that LFPS
offers a practical and efficient solution for decoding optimization in
long-context LLM inference.

</details>


### [98] [Generalisation Bounds of Zero-Shot Economic Forecasting using Time Series Foundation Models](https://arxiv.org/abs/2506.15705)
*Jittarin Jetwiriyanon,Teo Susnjak,Surangika Ranathunga*

Key words: 时间序列基础模型、宏观经济指标、零样本预测、结构性断裂、不确定性估计

TL;DR: 研究发现，未经微调的时间序列基础模型（TSFMs）在稳定经济条件下可与经典模型媲美甚至超越，但在快速冲击时期性能下降。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索TSFMs在零样本条件下预测宏观经济指标的潜力，减少对定制经济计量模型和大量训练数据的需求。

Method: 在单变量条件下，对三个先进TSFM（Chronos、TimeGPT和Moirai）进行数据稀缺和结构性断裂的严格回测。

Result: TSFMs能够内化复杂经济动态、适应制度变迁，并提供良好的不确定性估计，但快速冲击时期性能下降。

Conclusion: TSFMs为宏观经济监测和战略规划提供了零样本部署的可行性指导。

Abstract: This study investigates zero-shot forecasting capabilities of Time Series
Foundation Models (TSFMs) for macroeconomic indicators. We apply TSFMs to
forecasting economic indicators under univariate conditions, bypassing the need
for train bespoke econometric models using and extensive training datasets. Our
experiments were conducted on a case study dataset, without additional
customisation. We rigorously back-tested three state-of-the-art TSFMs (Chronos,
TimeGPT and Moirai) under data-scarce conditions and structural breaks. Our
results demonstrate that appropriately engineered TSFMs can internalise rich
economic dynamics, accommodate regime shifts, and deliver well-behaved
uncertainty estimates out of the box, while matching state-of-the-art
multivariate models on this domain. Our findings suggest that, without any
fine-tuning, TSFMs can match or exceed classical models during stable economic
conditions. However, they are vulnerable to degradation in performances during
periods of rapid shocks. The findings offer guidance to practitioners on when
zero-shot deployments are viable for macroeconomic monitoring and strategic
planning.

</details>


### [99] [MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning](https://arxiv.org/abs/2506.15706)
*Yunze Lin*

Key words: 数学推理, 直接偏好优化, MDPO, 大语言模型

TL;DR: 该论文提出了一种多粒度直接偏好优化（MDPO）方法，通过在三个粒度上优化大语言模型的数学推理能力，显著提升了数学推理任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大语言模型在数学推理任务中容易产生幻觉和错误输出，现有的直接偏好优化（DPO）方法在长链推理中效果有限。

Method: MDPO方法在Solution2Solution、Inference2Inference和Step2Step三个粒度上优化模型，并统一训练目标与生成指标对齐。

Result: 在Qwen2和Llama3模型上，GSM8K数据集提升了1.7%和0.9%，MATH数据集提升了2.3%和1.2%，超过了DPO及其变体方法。

Conclusion: MDPO方法有效提升了数学推理能力，并提供了一种无需人工标注的数据构建流程。

Abstract: Mathematical reasoning presents a significant challenge for Large Language
Models (LLMs) as it requires ensuring the correctness of each reasoning step.
Researchers have been strengthening the mathematical reasoning abilities of
LLMs through supervised fine-tuning, but due to the inability to suppress
incorrect outputs, illusions can easily arise. Recently, Direct Preference
Optimization (DPO) has been widely adopted for aligning human intent by using
preference data to prevent LLMs from generating incorrect outputs. However, it
has shown limited benefits in long-chain mathematical reasoning, mainly because
DPO struggles to effectively capture the differences between accepted and
rejected answers from preferences in long-chain data. The inconsistency between
DPO training and LLMs' generation metrics also affects the effectiveness of
suppressing incorrect outputs. We propose the Multi-Granularity Direct
Preference Optimization (MDPO) method, optimizing the mathematical reasoning of
LLMs at three granularities: Solution2Solution, Inference2Inference, and
Step2Step. Solution2Solution focuses on the correctness of entire long-chain
reasoning; Inference2Inference concentrates on logical reasoning between steps;
Step2Step corrects computational errors in steps, enhancing the computational
capabilities of LLMs. Additionally, we unify the training objectives of the
three granularities to align with the generation metrics. We conducted
experiments on the open-source models Qwen2 and Llama3, achieving improvements
of 1.7% and 0.9% on the GSM8K dataset, and 2.3% and 1.2% on the MATH dataset,
outperforming DPO and other DPO variant methods. Furthermore, we also provide a
pipeline for constructing MDPO training data that is simple and does not
require manual annotation costs.

</details>


### [100] [Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling](https://arxiv.org/abs/2506.15707)
*Xinglin Wang,Yiwei Li,Shaoxiong Feng,Peiwen Yuan,Yueqi Zhang,Jiayi Shi,Chuyi Tan,Boyuan Pan,Yao Hu,Kan Li*

Key words: Test-Time Scaling, Large Language Models, resource allocation, DORA, mathematical reasoning

TL;DR: TTS通过搜索优化LLM性能，但资源分配效率低下。论文提出DORA方向级资源分配方法，在固定预算下最大化正确率，实验显示其优于基线方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有测试时搜索方法在资源分配上效率不足，存在偏向候选多的方向问题。

Method: 将测试时搜索建模为资源分配问题，提出DORA方法，通过方向级分配避免候选数量偏差。

Result: 在MATH500等数学推理基准上，DORA优于基线方法，达到最优准确率。

Conclusion: DORA解决了现有方法的资源分配问题，为TTS提供了更优解。

Abstract: Test-Time Scaling (TTS) improves the performance of Large Language Models
(LLMs) by using additional inference-time computation to explore multiple
reasoning paths through search. Yet how to allocate a fixed rollout budget most
effectively during search remains underexplored, often resulting in inefficient
use of compute at test time. To bridge this gap, we formulate test-time search
as a resource allocation problem and derive the optimal allocation strategy
that maximizes the probability of obtaining a correct solution under a fixed
rollout budget. Within this formulation, we reveal a core limitation of
existing search methods: solution-level allocation tends to favor reasoning
directions with more candidates, leading to theoretically suboptimal and
inefficient use of compute. To address this, we propose Direction-Oriented
Resource Allocation (DORA), a provably optimal method that mitigates this bias
by decoupling direction quality from candidate count and allocating resources
at the direction level. To demonstrate DORA's effectiveness, we conduct
extensive experiments on challenging mathematical reasoning benchmarks
including MATH500, AIME2024, and AIME2025. The empirical results show that DORA
consistently outperforms strong baselines with comparable computational cost,
achieving state-of-the-art accuracy. We hope our findings contribute to a
broader understanding of optimal TTS for LLMs.

</details>


### [101] [Refined Causal Graph Structure Learning via Curvature for Brain Disease Classification](https://arxiv.org/abs/2506.15708)
*Falih Gozi Febrinanto,Adonia Simango,Chengpei Xu,Jingjing Zhou,Jiangang Ma,Sonika Tyagi,Feng Xia*

Key words: 图神经网络, 脑疾病分类, 因果发现, 转移熵, 几何曲率

TL;DR: 论文提出了一种新的框架CGB，用于基于因果关系的脑疾病分类，通过因果发现方法和几何曲率策略优化脑网络建模，显著提升了分类性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有图神经网络（GNNs）在脑疾病检测中未充分考虑脑区间（ROIs）的因果关系，而因果互动比传统相关性更能揭示信号间的本质联系。

Method: 提出CGB框架，结合因果发现方法（转移熵）和几何曲率策略，建模因果脑网络并进行图重构以减少信息瓶颈。

Result: 实验表明，CGB在脑疾病数据集上的分类任务中F1分数优于现有方法。

Conclusion: CGB通过挖掘脑区间的因果关系并优化图结构，显著提升了脑疾病分类的准确性和表达性。

Abstract: Graph neural networks (GNNs) have been developed to model the relationship
between regions of interest (ROIs) in brains and have shown significant
improvement in detecting brain diseases. However, most of these frameworks do
not consider the intrinsic relationship of causality factor between brain ROIs,
which is arguably more essential to observe cause and effect interaction
between signals rather than typical correlation values. We propose a novel
framework called CGB (Causal Graphs for Brains) for brain disease
classification/detection, which models refined brain networks based on the
causal discovery method, transfer entropy, and geometric curvature strategy.
CGB unveils causal relationships between ROIs that bring vital information to
enhance brain disease classification performance. Furthermore, CGB also
performs a graph rewiring through a geometric curvature strategy to refine the
generated causal graph to become more expressive and reduce potential
information bottlenecks when GNNs model it. Our extensive experiments show that
CGB outperforms state-of-the-art methods in classification tasks on brain
disease datasets, as measured by average F1 scores.

</details>


### [102] [Studying and Improving Graph Neural Network-based Motif Estimation](https://arxiv.org/abs/2506.15709)
*Pedro C. Vieira,Miguel E. P. Silva,Pedro Manuel Pinto Ribeiro*

Key words: 图神经网络, 网络基序, 显著性分布, 多目标回归, 子图频率估计

TL;DR: 该论文研究了图神经网络（GNNs）在网络基序显著性分布（SP）预测中的应用，提出了一种独立于子图频率估计的直接SP估计方法，并通过实验验证了其可行性和优势。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的GNNs主要关注子图频率估计，而在网络基序显著性分布（SP）预测方面缺乏研究，且没有基准方法。论文旨在填补这一空白，提供一种新的SP估计方法。

Method: 将SP估计问题重新定义为多目标回归任务，通过直接估计而非频率计数，优化了可解释性、稳定性和大规模图的可扩展性。

Result: 实验表明，1-WL限制的模型在精确估计SP时存在困难，但可以通过与合成生成器的SP比较来近似推断图的生成过程。

Conclusion: 直接SP估计方法有助于突破通过子图计数进行基序估计的理论限制，为GNN在基序分析中的应用提供了新思路。

Abstract: Graph Neural Networks (GNNs) are a predominant method for graph
representation learning. However, beyond subgraph frequency estimation, their
application to network motif significance-profile (SP) prediction remains
under-explored, with no established benchmarks in the literature. We propose to
address this problem, framing SP estimation as a task independent of subgraph
frequency estimation. Our approach shifts from frequency counting to direct SP
estimation and modulates the problem as multitarget regression. The
reformulation is optimised for interpretability, stability and scalability on
large graphs. We validate our method using a large synthetic dataset and
further test it on real-world graphs. Our experiments reveal that 1-WL limited
models struggle to make precise estimations of SPs. However, they can
generalise to approximate the graph generation processes of networks by
comparing their predicted SP with the ones originating from synthetic
generators. This first study on GNN-based motif estimation also hints at how
using direct SP estimation can help go past the theoretical limitations that
motif estimation faces when performed through subgraph counting.

</details>


### [103] [RAST: Reasoning Activation in LLMs via Small-model Transfer](https://arxiv.org/abs/2506.15710)
*Siru Ouyang,Xinyu Zhu,Zilin Xiao,Minhao Jiang,Yu Meng,Jiawei Han*

Key words: 强化学习,大型语言模型,概率调整,推理能力,RAST

TL;DR: RAST方法通过从小模型中转移RL诱导的概率调整到大模型，显著提升了推理能力，同时减少了GPU资源消耗。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究RL在大型语言模型中的应用资源消耗大，发现RL诱导的概率变化具有模型尺寸不变性，提出高效替代方案。

Method: 提出RAST方法，从小模型中提取RL训练的概率调整并注入到大模型中，实现推理能力提升。

Result: 实验表明RAST显著提升模型推理能力，资源消耗低于直接RL训练，有时性能更优。

Conclusion: 揭示了RL驱动推理的本质，提供了无需高计算成本的实用策略。

Abstract: Reinforcement learning (RL) has become a powerful approach for improving the
reasoning capabilities of large language models (LLMs), as evidenced by recent
successes such as OpenAI's o1 and Deepseek-R1. However, applying RL at scale
remains intimidatingly resource-intensive, requiring multiple model copies and
extensive GPU workloads. On the other hand, while being powerful, recent
studies suggest that RL does not fundamentally endow models with new knowledge;
rather, it primarily reshapes the model's output distribution to activate
reasoning capabilities latent in the base model. Building on this insight, we
hypothesize that the changes in output probabilities induced by RL are largely
model-size invariant, opening the door to a more efficient paradigm: training a
small model with RL and transferring its induced probability shifts to larger
base models. To verify our hypothesis, we conduct a token-level analysis of
decoding trajectories and find high alignment in RL-induced output
distributions across model scales, validating our hypothesis. Motivated by
this, we propose RAST, a simple yet effective method that transfers reasoning
behaviors by injecting RL-induced probability adjustments from a small
RL-trained model into larger models. Experiments across multiple mathematical
reasoning benchmarks show that RAST substantially and consistently enhances the
reasoning capabilities of base models while requiring significantly lower GPU
memory than direct RL training, sometimes even yielding better performance than
the RL-trained counterparts. Our findings offer new insights into the nature of
RL-driven reasoning and practical strategies for scaling its benefits without
incurring its full computational cost. The project page of RAST is available at
https://ozyyshr.github.io/RAST/.

</details>


### [104] [Shadow defense against gradient inversion attack in federated learning](https://arxiv.org/abs/2506.15711)
*Le Jiang,Liyan Ma,Guang Yang*

Key words: 联邦学习、梯度反转攻击、隐私保护、可解释性、医疗数据

TL;DR: 文章提出了一种针对联邦学习中梯度反转攻击的防御框架，通过影子模型识别敏感区域，实现针对性噪声注入，有效保护隐私且不影响模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习（FL）在隐私保护分布式训练中表现突出，但梯度反转攻击（GIA）可能泄露敏感数据，现有防御方法难以平衡隐私保护与模型准确性。

Method: 引入基于影子模型和可解释性的框架，识别敏感区域并进行针对性噪声注入。

Result: 在ChestXRay和EyePACS数据集上分别达到PSNR 3.73和2.78、SSIM 0.2和0.166的防御效果，模型性能损耗小于1%。

Conclusion: 该方法在多种医疗图像数据集中验证了泛化能力，对不同类型的GIA均有效。

Abstract: Federated learning (FL) has emerged as a transformative framework for
privacy-preserving distributed training, allowing clients to collaboratively
train a global model without sharing their local data. This is especially
crucial in sensitive fields like healthcare, where protecting patient data is
paramount. However, privacy leakage remains a critical challenge, as the
communication of model updates can be exploited by potential adversaries.
Gradient inversion attacks (GIAs), for instance, allow adversaries to
approximate the gradients used for training and reconstruct training images,
thus stealing patient privacy. Existing defense mechanisms obscure gradients,
yet lack a nuanced understanding of which gradients or types of image
information are most vulnerable to such attacks. These indiscriminate
calibrated perturbations result in either excessive privacy protection
degrading model accuracy, or insufficient one failing to safeguard sensitive
information. Therefore, we introduce a framework that addresses these
challenges by leveraging a shadow model with interpretability for identifying
sensitive areas. This enables a more targeted and sample-specific noise
injection. Specially, our defensive strategy achieves discrepancies of 3.73 in
PSNR and 0.2 in SSIM compared to the circumstance without defense on the
ChestXRay dataset, and 2.78 in PSNR and 0.166 in the EyePACS dataset. Moreover,
it minimizes adverse effects on model performance, with less than 1\% F1
reduction compared to SOTA methods. Our extensive experiments, conducted across
diverse types of medical images, validate the generalization of the proposed
framework. The stable defense improvements for FedAvg are consistently over
1.5\% times in LPIPS and SSIM. It also offers a universal defense against
various GIA types, especially for these sensitive areas in images.

</details>


### [105] [BatteryBERT for Realistic Battery Fault Detection Using Point-Masked Signal Modeling](https://arxiv.org/abs/2506.15712)
*Songqi Zhou,Ruixue Liu,Yixing Wang,Jia Lu,Benben Jiang*

Key words: 锂离子电池；故障检测；BERT预训练；时间序列；自监督学习

TL;DR: 提出了一种基于BERT预训练的框架，用于锂离子电池的故障检测，通过自定义时间序列到令牌的表示模块和点级掩码信号建模预训练任务，显著提高了表征质量和分类准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 锂离子电池的准确故障检测对电动汽车和储能系统的安全和可靠运行至关重要，但现有方法在捕捉复杂时间依赖性和利用未标记数据方面存在不足。

Method: 扩展了标准BERT架构，加入自定义时间序列到令牌的表示模块和点级掩码信号建模预训练任务，结合自监督学习和下游分类器。

Result: 在大规模真实数据集上取得了AUROC为0.945的优异表现，显著优于现有方法。

Conclusion: 验证了BERT式预训练在时间序列故障检测中的有效性。

Abstract: Accurate fault detection in lithium-ion batteries is essential for the safe
and reliable operation of electric vehicles and energy storage systems.
However, existing methods often struggle to capture complex temporal
dependencies and cannot fully leverage abundant unlabeled data. Although large
language models (LLMs) exhibit strong representation capabilities, their
architectures are not directly suited to the numerical time-series data common
in industrial settings. To address these challenges, we propose a novel
framework that adapts BERT-style pretraining for battery fault detection by
extending the standard BERT architecture with a customized time-series-to-token
representation module and a point-level Masked Signal Modeling (point-MSM)
pretraining task tailored to battery applications. This approach enables
self-supervised learning on sequential current, voltage, and other
charge-discharge cycle data, yielding distributionally robust, context-aware
temporal embeddings. We then concatenate these embeddings with battery metadata
and feed them into a downstream classifier for accurate fault classification.
Experimental results on a large-scale real-world dataset show that models
initialized with our pretrained parameters significantly improve both
representation quality and classification accuracy, achieving an AUROC of 0.945
and substantially outperforming existing approaches. These findings validate
the effectiveness of BERT-style pretraining for time-series fault detection.

</details>


### [106] [An application of machine learning to the motion response prediction of floating assets](https://arxiv.org/abs/2506.15713)
*Michael T. M. B. Morris-Thomas,Marius Martens*

Key words: 机器学习, 非线性运动响应, 海上工程, 梯度提升集成, 实时预测

TL;DR: 该研究提出了一种监督机器学习方法，用于预测浮动海上资产在随机海洋条件下的非线性运动响应，显著优于传统频域方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 实时预测在随机海洋条件下的浮动海上资产行为是海上工程中的一个重大挑战，传统方法在极端海况和非线性响应中表现不佳。

Method: 采用多元回归的监督机器学习方法，结合梯度提升集成算法和定制被动天轮求解器，训练了约100万样本和100个特征。

Result: 模型对关键系泊参数的预测误差小于5%，船舶航向精度在2.5度以内，显著优于传统频域方法，并已在操作设施中成功部署。

Conclusion: 该框架在海上环境中实现了实时船舶监测和操作决策的高效应用。

Abstract: The real-time prediction of floating offshore asset behavior under stochastic
metocean conditions remains a significant challenge in offshore engineering.
While traditional empirical and frequency-domain methods work well in benign
conditions, they struggle with both extreme sea states and nonlinear responses.
This study presents a supervised machine learning approach using multivariate
regression to predict the nonlinear motion response of a turret-moored vessel
in 400 m water depth. We developed a machine learning workflow combining a
gradient-boosted ensemble method with a custom passive weathervaning solver,
trained on approximately $10^6$ samples spanning 100 features. The model
achieved mean prediction errors of less than 5% for critical mooring parameters
and vessel heading accuracy to within 2.5 degrees across diverse metocean
conditions, significantly outperforming traditional frequency-domain methods.
The framework has been successfully deployed on an operational facility,
demonstrating its efficacy for real-time vessel monitoring and operational
decision-making in offshore environments.

</details>


### [107] [Adaptive Two Sided Laplace Transforms: A Learnable, Interpretable, and Scalable Replacement for Self-Attention](https://arxiv.org/abs/2506.15714)
*Andrew Kiruluta*

Key words: 可学习STLT、Transformer、超长序列、自注意力替代、语言建模

TL;DR: 论文提出了一种可学习的两侧短时拉普拉斯变换（STLT）机制，用于替代传统Transformer中自注意力机制，实现了更高效的超长序列语言建模。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决传统自注意力机制在超长序列语言建模中的计算瓶颈问题，作者提出了一种更灵活且高效的可学习STLT机制。

Method: 通过引入可训练的拉普拉斯节点参数（包括衰减率、振荡频率和窗口带宽），并结合快速递归卷积和FFT计算，实现了动态调整节点数量和计算效率的提升。

Result: 在语言建模、机器翻译和长文档问答等任务上，STLT的性能与现有高效Transformer相当或更好，且能扩展到超过100k tokens的上下文长度。

Conclusion: 可学习的STLT机制兼具可解释性、可扩展性和鲁棒性，为解决超长序列语言建模问题提供了新思路。

Abstract: We propose an innovative, learnable two-sided short-time Laplace transform
(STLT) mechanism to supplant the traditional self attention in
transformer-based LLMs. Our STLT introduces trainable parameters for each
Laplace node, enabling end-to-end learning of decay rates , oscillatory
frequencies, and window bandwidth T. This flexibility allows the model to
dynamically adapt token relevance half lives and frequency responses during
training. By selecting S learnable nodes and leveraging fast recursive
convolution, we achieve an effective complexity of in time and memory. We
further incorporate an efficient FFT-based computation of the relevance matrix
and an adaptive node allocation mechanism to dynamically adjust the number of
active Laplace nodes. Empirical results on language modeling (WikiText\-103,
Project Gutenberg), machine translation (WMT'14 En\-De), and long document
question answering (NarrativeQA) demonstrate that our learnable STLT achieves
perplexities and scores on par with or better than existing efficient
transformers while naturally extending to context lengths exceeding 100k tokens
or more limited only by available hardware. Ablation studies confirm the
importance of learnable parameters and adaptive node allocation. The proposed
approach combines interpretability, through explicit decay and frequency
parameters, with scalability and robustness, offering a pathway towards
ultra-long-sequence language modeling without the computational bottleneck of
self-attention.

</details>


### [108] [NeuronSeek: On Stability and Expressivity of Task-driven Neurons](https://arxiv.org/abs/2506.15715)
*Hanyu Pei,Jing-Xiao Liao,Qibin Zhao,Ting Gao,Shijun Zhang,Xiaoge Zhang,Feng-Lei Fan*

Key words: 任务驱动神经元、张量分解、通用逼近、深度学习

TL;DR: 本文提出了一种名为NeuronSeek-TD的框架，通过张量分解（TD）替代符号回归（SR）优化神经元结构，具有更好的稳定性和收敛速度，并在理论上证明了其通用逼近能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 受人类大脑为不同任务设计不同神经元的启发，探索如何优化神经网络的神经元结构。

Method: 使用张量分解（TD）替代符号回归（SR）来发现最优神经元结构，并理论证明了其通用逼近能力。

Result: 实验表明NeuronSeek-TD框架具有更高的稳定性，并在多个基准测试中表现优异。

Conclusion: NeuronSeek-TD为任务驱动神经元的设计提供了更好的理论基础和实际性能。

Abstract: Drawing inspiration from our human brain that designs different neurons for
different tasks, recent advances in deep learning have explored modifying a
network's neurons to develop so-called task-driven neurons. Prototyping
task-driven neurons (referred to as NeuronSeek) employs symbolic regression
(SR) to discover the optimal neuron formulation and construct a network from
these optimized neurons. Along this direction, this work replaces symbolic
regression with tensor decomposition (TD) to discover optimal neuronal
formulations, offering enhanced stability and faster convergence. Furthermore,
we establish theoretical guarantees that modifying the aggregation functions
with common activation functions can empower a network with a fixed number of
parameters to approximate any continuous function with an arbitrarily small
error, providing a rigorous mathematical foundation for the NeuronSeek
framework. Extensive empirical evaluations demonstrate that our NeuronSeek-TD
framework not only achieves superior stability, but also is competitive
relative to the state-of-the-art models across diverse benchmarks. The code is
available at https://github.com/HanyuPei22/NeuronSeek.

</details>


### [109] [Alternates, Assemble! Selecting Optimal Alternates for Citizens' Assemblies](https://arxiv.org/abs/2506.15716)
*Angelos Assos,Carmel Baharav,Bailey Flanigan,Ariel Procaccia*

Key words: 审议民主,公民议会,替补选择,学习理论,优化

TL;DR: 论文提出了一个优化框架，用于解决公民议会中随机选择人员讨论政策问题时因参与者退出导致的不平衡问题，通过算法选择替补人员以最小化预期的不代表性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 公民议会作为一种审议民主形式，其合法性依赖于其对更广泛人群的代表性，但目前因参与者退出导致不平衡，而替补选择方法尚未优化。

Method: 引入基于学习理论的优化框架，利用历史数据估计退出概率，并选择替补以最小化预期的不代表性，同时提供理论保证。

Result: 理论分析提供了样本复杂性和计算效率的最坏情况界限，实证评估显示该方法显著提高了代表性且需要更少替补。

Conclusion: 该算法框架能有效改善公民议会的代表性，同时减少替补需求，具有实际应用价值。

Abstract: An increasingly influential form of deliberative democracy centers on
citizens' assemblies, where randomly selected people discuss policy questions.
The legitimacy of these panels hinges on their representation of the broader
population, but panelists often drop out, leading to an unbalanced composition.
Although participant attrition is mitigated in practice by alternates, their
selection is not taken into account by existing methods. To address this gap,
we introduce an optimization framework for alternate selection. Our algorithmic
approach, which leverages learning-theoretic machinery, estimates dropout
probabilities using historical data and selects alternates to minimize expected
misrepresentation. We establish theoretical guarantees for our approach,
including worst-case bounds on sample complexity (with implications for
computational efficiency) and on loss when panelists' probabilities of dropping
out are mis-estimated. Empirical evaluation using real-world data demonstrates
that, compared to the status quo, our method significantly improves
representation while requiring fewer alternates.

</details>


### [110] [daDPO: Distribution-Aware DPO for Distilling Conversational Abilities](https://arxiv.org/abs/2506.15717)
*Zhengze Zhang,Shiqi Wang,Yiqun Shen,Simin Guo,Dahua Lin,Xiaoliang Wang,Nguyen Cam-Tu,Fei Tan*

Key words: 大型语言模型, 知识蒸馏, 偏好优化, 分布感知, 模型剪枝

TL;DR: 论文提出了一种名为daDPO（Distribution-Aware DPO）的统一方法，用于优化偏好并基于分布的蒸馏，显著提升了小规模语言模型的对话能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大语言模型（LLM）虽表现优异，但在资源受限环境下，小规模模型的对话能力下降明显。当前的知识蒸馏方法主要关注'黑盒'蒸馏，忽略了教师模型的输出分布信息，导致性能受限。

Method: 作者提出了daDPO方法，结合偏好优化和基于分布的蒸馏，通过理论分析和实验验证，实现了对小规模模型的性能优化。

Result: 实验表明，daDPO在恢复剪枝模型性能和提升小规模LLM方面优于现有方法。例如，20%剪枝的Vicuna1.5-7B几乎达到教师性能（偏好率仅下降7.3%，而dDPO下降31%），Qwen2.5-1.5B甚至偶尔超过其7B教师模型。

Conclusion: daDPO为资源受限环境下提升小规模语言模型性能提供了有效解决方案，展示了分布信息在知识蒸馏中的重要性。

Abstract: Large language models (LLMs) have demonstrated exceptional performance across
various applications, but their conversational abilities decline sharply as
model size decreases, presenting a barrier to their deployment in
resource-constrained environments. Knowledge distillation with Direct
Preference Optimization (dDPO) has emerged as a promising approach to enhancing
the conversational abilities of smaller models using a larger teacher model.
However, current methods primarily focus on 'black-box' KD, which only uses the
teacher's responses, overlooking the output distribution offered by the
teacher. This paper addresses this gap by introducing daDPO (Distribution-Aware
DPO), a unified method for preference optimization and distribution-based
distillation. We provide rigorous theoretical analysis and empirical
validation, showing that daDPO outperforms existing methods in restoring
performance for pruned models and enhancing smaller LLM models. Notably, in
in-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve
near-teacher performance (-7.3% preference rate compared to that of dDPO's
-31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model
(14.0% win rate).

</details>


### [111] [BuildingBRep-11K: Precise Multi-Storey B-Rep Building Solids with Rich Layout Metadata](https://arxiv.org/abs/2506.15718)
*Yu Guo,Hongji Fang,Tianyu Fang,Zhe Cui*

Key words: 人工智能,3D对象生成,建筑数据集,形状语法,PointNet,几何回归,缺陷检测

TL;DR: 论文介绍了一个名为BuildingBRep-11K的大型建筑数据集，用于训练自动生成建筑级3D对象的模型，并验证了其可学习性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着人工智能的兴起，自动生成建筑级3D对象成为研究热点，但训练这类模型需要大量干净且标注丰富的数据集。

Method: 通过形状语法驱动的流程生成11,978个多楼层建筑样本，结合几何精确的B-rep实体和快速加载的元数据文件，并训练两个轻量级PointNet基线模型进行验证。

Result: 数据集在楼层数、房间数量和平均房间面积的预测中表现良好，同时在缺陷检测任务中达到一定准确率，展示了数据集的可学习性但仍具挑战性。

Conclusion: BuildingBRep-11K是一个可学习但非平凡的数据集，适用于几何回归和拓扑质量评估任务。

Abstract: With the rise of artificial intelligence, the automatic generation of
building-scale 3-D objects has become an active research topic, yet training
such models still demands large, clean and richly annotated datasets. We
introduce BuildingBRep-11K, a collection of 11 978 multi-storey (2-10 floors)
buildings (about 10 GB) produced by a shape-grammar-driven pipeline that
encodes established building-design principles. Every sample consists of a
geometrically exact B-rep solid-covering floors, walls, slabs and rule-based
openings-together with a fast-loading .npy metadata file that records detailed
per-floor parameters. The generator incorporates constraints on spatial scale,
daylight optimisation and interior layout, and the resulting objects pass
multi-stage filters that remove Boolean failures, undersized rooms and extreme
aspect ratios, ensuring compliance with architectural standards. To verify the
dataset's learnability we trained two lightweight PointNet baselines. (i)
Multi-attribute regression. A single encoder predicts storey count, total
rooms, per-storey vector and mean room area from a 4 000-point cloud. On 100
unseen buildings it attains 0.37-storey MAE (87 \% within $\pm1$), 5.7-room
MAE, and 3.2 m$^2$ MAE on mean area. (ii) Defect detection. With the same
backbone we classify GOOD versus DEFECT; on a balanced 100-model set the
network reaches 54 \% accuracy, recalling 82 \% of true defects at 53 \%
precision (41 TP, 9 FN, 37 FP, 13 TN). These pilots show that BuildingBRep-11K
is learnable yet non-trivial for both geometric regression and topological
quality assessment

</details>


### [112] [Data-Driven Heat Pump Management: Combining Machine Learning with Anomaly Detection for Residential Hot Water Systems](https://arxiv.org/abs/2506.15719)
*Manal Rahal,Bestoun S. Ahmed,Roger Renstrom,Robert Stener,Albrecht Wurtz*

Key words: 热泵,机器学习,异常检测,热水需求预测,自适应控制

TL;DR: 本文提出了一种结合预测机器学习和异常检测的新方法，优化热泵热水生产策略，显著提升效率和适应性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对热泵热水生产中传统控制方法的效率限制，提出利用机器学习和异常检测优化家庭热水需求预测。

Method: 结合机器学习和孤立森林算法，采用多步特征选择和三种ML模型（LightGBM、LSTM、双向LSTM），实验验证六种家庭实际安装数据。

Result: LightGBM表现最佳，RMSE提升达9.37%，异常检测F1-score为0.87，误报率仅5.2%。

Conclusion: 该方法在真实场景中表现优异，适合广泛部署，显著提升热泵效率。

Abstract: Heat pumps (HPs) have emerged as a cost-effective and clean technology for
sustainable energy systems, but their efficiency in producing hot water remains
restricted by conventional threshold-based control methods. Although machine
learning (ML) has been successfully implemented for various HP applications,
optimization of household hot water demand forecasting remains understudied.
This paper addresses this problem by introducing a novel approach that combines
predictive ML with anomaly detection to create adaptive hot water production
strategies based on household-specific consumption patterns. Our key
contributions include: (1) a composite approach combining ML and isolation
forest (iForest) to forecast household demand for hot water and steer
responsive HP operations; (2) multi-step feature selection with advanced
time-series analysis to capture complex usage patterns; (3) application and
tuning of three ML models: Light Gradient Boosting Machine (LightGBM), Long
Short-Term Memory (LSTM), and Bi-directional LSTM with the self-attention
mechanism on data from different types of real HP installations; and (4)
experimental validation on six real household installations. Our experiments
show that the best-performing model LightGBM achieves superior performance,
with RMSE improvements of up to 9.37\% compared to LSTM variants with $R^2$
values between 0.748-0.983. For anomaly detection, our iForest implementation
achieved an F1-score of 0.87 with a false alarm rate of only 5.2\%,
demonstrating strong generalization capabilities across different household
types and consumption patterns, making it suitable for real-world HP
deployments.

</details>


### [113] [Tripartite Weight-Space Ensemble for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2506.15720)
*Juntae Lee,Munawar Hayat,Sungrack Yun*

Key words: Few-shot learning, incremental learning, catastrophic forgetting, overfitting, weight-space ensemble, knowledge distillation

TL;DR: Few-shot class incremental learning (FSCIL) combines a tripartite weight-space ensemble (Tri-WE) and amplified data knowledge distillation to overcome catastrophic forgetting and overfitting, achieving state-of-the-art results.

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: FSCIL models suffer from forgetting previous concepts and overfitting to few new examples, necessitating a method that updates the entire model effectively while maintaining prior knowledge.

Method: Proposes Tri-WE to interpolate base, previous, and current model weights, and uses amplified data knowledge distillation for generalized representation distillation.

Result: Achieves top results on miniImageNet, CUB200, and CIFAR100 datasets, demonstrating effectiveness in few-shot incremental learning.

Conclusion: The method successfully balances adaptability to new classes and retention of old knowledge, proving superior to fixed-feature approaches.

Abstract: Few-shot class incremental learning (FSCIL) enables the continual learning of
new concepts with only a few training examples. In FSCIL, the model undergoes
substantial updates, making it prone to forgetting previous concepts and
overfitting to the limited new examples. Most recent trend is typically to
disentangle the learning of the representation from the classification head of
the model. A well-generalized feature extractor on the base classes (many
examples and many classes) is learned, and then fixed during incremental
learning. Arguing that the fixed feature extractor restricts the model's
adaptability to new classes, we introduce a novel FSCIL method to effectively
address catastrophic forgetting and overfitting issues. Our method enables to
seamlessly update the entire model with a few examples. We mainly propose a
tripartite weight-space ensemble (Tri-WE). Tri-WE interpolates the base,
immediately previous, and current models in weight-space, especially for the
classification heads of the models. Then, it collaboratively maintains
knowledge from the base and previous models. In addition, we recognize the
challenges of distilling generalized representations from the previous model
from scarce data. Hence, we suggest a regularization loss term using amplified
data knowledge distillation. Simply intermixing the few-shot data, we can
produce richer data enabling the distillation of critical knowledge from the
previous model. Consequently, we attain state-of-the-art results on the
miniImageNet, CUB200, and CIFAR100 datasets.

</details>


### [114] [Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration](https://arxiv.org/abs/2506.15721)
*Junqi Gao,Zhichang Guo,Dazhi Zhang,Dong Li,Runze Liu,Pengfei Li,Kai Tian,Biqing Qi*

Key words: 异构LLM融合, 合成数据, 分层树结构, 动态调整, 能力均衡

TL;DR: Bohdi是一个仅使用合成数据的异构LLM融合框架，通过分层树结构和动态调整采样比例，解决了现有方法在知识融合和数据分配上的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有异构LLM融合方法依赖有限领域的真实数据，且数据分配比例固定，导致目标LLM在多样领域的知识获取和能力均衡性上表现不佳。

Method: Bohdi采用分层树结构组织知识领域，通过多模型协作自动探索领域并生成数据，利用分层多臂老虎机问题动态调整采样比例，并结合Introspection-Rebirth机制增强在线适应能力。

Result: 实验结果表明，Bohdi在多个目标LLM上显著优于现有基线，数据效率更高，并有效消除了目标LLM的能力不均衡问题。

Conclusion: Bohdi通过合成数据和动态调整机制，实现了高效的异构LLM融合，解决了现有方法的局限性。

Abstract: Heterogeneous Large Language Model (LLM) fusion integrates the strengths of
multiple source LLMs with different architectures into a target LLM with low
computational overhead. While promising, existing methods suffer from two major
limitations: 1) reliance on real data from limited domain for knowledge fusion,
preventing the target LLM from fully acquiring knowledge across diverse
domains, and 2) fixed data allocation proportions across domains, failing to
dynamically adjust according to the target LLM's varying capabilities across
domains, leading to a capability imbalance. To overcome these limitations, we
propose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework.
Through the organization of knowledge domains into a hierarchical tree
structure, Bohdi enables automatic domain exploration and multi-domain data
generation through multi-model collaboration, thereby comprehensively
extracting knowledge from source LLMs. By formalizing domain expansion and data
sampling proportion allocation on the knowledge tree as a Hierarchical
Multi-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism
to adaptively adjust sampling proportions based on the target LLM's performance
feedback across domains. Integrated with our proposed Introspection-Rebirth
(IR) mechanism, DynaBranches dynamically tracks capability shifts during target
LLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT),
further enhancing its online adaptation capability. Comparative experimental
results on a comprehensive suite of benchmarks demonstrate that Bohdi
significantly outperforms existing baselines on multiple target LLMs, exhibits
higher data efficiency, and virtually eliminates the imbalance in the target
LLM's capabilities. Our code is available at
https://github.com/gjq100/Bohdi.git.

</details>


### [115] [UniMate: A Unified Model for Mechanical Metamaterial Generation, Property Prediction, and Condition Confirmation](https://arxiv.org/abs/2506.15722)
*Wangzhi Zhan,Jianpeng Chen,Dongqi Fu,Dawei Zhou*

Key words: 机械超材料, 3D拓扑, 密度条件, 力学性能, UNIMATE模型

TL;DR: 论文提出了一个名为UNIMATE的统一模型，用于同时处理机械超材料设计中的3D拓扑、密度条件和力学性能三个模态，填补了现有研究仅关注两个模态的空白。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的机械超材料设计研究通常仅关注两个模态（如3D拓扑或力学性能），但实际应用需要同时考虑所有三个模态，因此需要一种能够统一处理这些模态的模型。

Method: UNIMATE模型由模态对齐模块和协同扩散生成模块组成，能够同时处理3D拓扑、密度条件和力学性能。

Result: 实验表明，UNIMATE在拓扑生成、性能预测和条件确认任务中分别优于基线模型80.2%、5.1%和50.2%。

Conclusion: UNIMATE为机械超材料设计提供了一种统一的解决方案，显著提升了多模态任务的表现。

Abstract: Metamaterials are artificial materials that are designed to meet unseen
properties in nature, such as ultra-stiffness and negative materials indices.
In mechanical metamaterial design, three key modalities are typically involved,
i.e., 3D topology, density condition, and mechanical property. Real-world
complex application scenarios place the demanding requirements on machine
learning models to consider all three modalities together. However, a
comprehensive literature review indicates that most existing works only
consider two modalities, e.g., predicting mechanical properties given the 3D
topology or generating 3D topology given the required properties. Therefore,
there is still a significant gap for the state-of-the-art machine learning
models capturing the whole. Hence, we propose a unified model named UNIMATE,
which consists of a modality alignment module and a synergetic diffusion
generation module. Experiments indicate that UNIMATE outperforms the other
baseline models in topology generation task, property prediction task, and
condition confirmation task by up to 80.2%, 5.1%, and 50.2%, respectively. We
opensource our proposed UNIMATE model and corresponding results at
https://github.com/wzhan24/UniMate.

</details>


### [116] [MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2506.15724)
*Kunxi Li,Zhonghua Jiang,Zhouzhou Shen,Zhaode Wang,Chengfei Lv,Shengyu Zhang,Fan Wu,Fei Wu*

Key words: MadaKV，多模态大语言模型，KV缓存，模态自适应

TL;DR: MadaKV是一种模态自适应的KV缓存淘汰策略，用于提升多模态大语言模型在长上下文推理中的效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多模态场景中，注意力头对不同模态的偏好差异显著，传统单模态KV缓存淘汰方法无法有效捕捉模态特定信息，导致性能不佳。

Method: 通过模态偏好自适应和分层压缩补偿两个关键组件，动态感知注意力头中的模态信息并自适应保留关键令牌。

Result: 显著减少了KV缓存内存占用和模型推理解码延迟（提升1.3至1.5倍），同时保持多模态长上下文任务的高准确性。

Conclusion: 实验表明，MadaKV在性能和效率上优于现有KV缓存淘汰方法。

Abstract: This paper introduces MadaKV, a modality-adaptive key-value (KV) cache
eviction strategy designed to enhance the efficiency of multimodal large
language models (MLLMs) in long-context inference. In multimodal scenarios,
attention heads exhibit varying preferences for different modalities, resulting
in significant disparities in modality importance across attention heads.
Traditional KV cache eviction methods, which are tailored for unimodal
settings, fail to capture modality-specific information, thereby yielding
suboptimal performance. MadaKV addresses these challenges through two key
components: modality preference adaptation and hierarchical compression
compensation. By dynamically sensing modality information within attention
heads and adaptively retaining critical tokens, MadaKV achieves substantial
reductions in KV cache memory footprint and model inference decoding latency
(1.3 to 1.5 times improvement) while maintaining high accuracy across various
multimodal long-context tasks. Extensive experiments on representative MLLMs
and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to
existing KV cache eviction methods.

</details>


### [117] [Graph Diffusion that can Insert and Delete](https://arxiv.org/abs/2506.15725)
*Matteo Ninniri,Marco Podda,Davide Bacciu*

Key words: 生成模型,图扩散,分子生成,条件生成,动态调整

TL;DR: 论文提出了一种基于图扩散的分子生成模型GrIDDD，通过支持节点的单调插入和删除，实现了分子大小的动态调整，解决了现有方法在条件生成场景中的限制。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的基于DDPMs的图生成模型无法在扩散过程中调整图的大小，限制了其在条件生成场景（如属性驱动分子设计）中的有效性。

Method: 重新定义了噪声和去噪过程，支持节点的单调插入和删除，提出了GrIDDD模型，能够在生成过程中动态调整化学图的大小。

Result: GrIDDD在分子属性靶向任务中表现优于或相当于现有图扩散模型，且在分子优化任务中展现出与专用优化模型竞争的性能。

Conclusion: 该研究为大小自适应的分子生成开辟了新途径。

Abstract: Generative models of graphs based on discrete Denoising Diffusion
Probabilistic Models (DDPMs) offer a principled approach to molecular
generation by systematically removing structural noise through iterative atom
and bond adjustments. However, existing formulations are fundamentally limited
by their inability to adapt the graph size (that is, the number of atoms)
during the diffusion process, severely restricting their effectiveness in
conditional generation scenarios such as property-driven molecular design,
where the targeted property often correlates with the molecular size. In this
paper, we reformulate the noising and denoising processes to support monotonic
insertion and deletion of nodes. The resulting model, which we call GrIDDD,
dynamically grows or shrinks the chemical graph during generation. GrIDDD
matches or exceeds the performance of existing graph diffusion models on
molecular property targeting despite being trained on a more difficult problem.
Furthermore, when applied to molecular optimization, GrIDDD exhibits
competitive performance compared to specialized optimization models. This work
paves the way for size-adaptive molecular generation with graph diffusion.

</details>


### [118] [Descriptor-based Foundation Models for Molecular Property Prediction](https://arxiv.org/abs/2506.15792)
*Jackson Burns,Akshat Zalte,William Green*

Key words: 分子基础模型, 机器学习, 分子描述符, CheMeleon, 分子性质预测

TL;DR: CheMeleon是一种基于分子描述符预训练的新型分子基础模型，通过低噪声数据学习丰富的分子表征，在多个基准测试中表现优异，但在活性悬崖识别上存在不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 快速准确的分子性质预测对多领域科学进步至关重要。传统方法依赖噪声数据或模拟，CheMeleon旨在通过低噪声分子描述符实现更有效的预测。

Method: 使用Mordred包的确定性分子描述符预训练CheMeleon，采用Directed Message-Passing Neural Network模型预测无噪声描述符。

Result: 在Polaris和MoleculeACE基准测试中分别取得79%和97%的胜率，优于随机森林等基线模型，但在活性悬崖识别上表现不佳。

Conclusion: 基于描述符的预训练为分子性质预测提供了可扩展且高效的方法，未来可进一步探索描述符集和未标记数据集的应用。

Abstract: Fast and accurate prediction of molecular properties with machine learning is
pivotal to scientific advancements across myriad domains. Foundation models in
particular have proven especially effective, enabling accurate training on
small, real-world datasets. This study introduces CheMeleon, a novel molecular
foundation model pre-trained on deterministic molecular descriptors from the
Mordred package, leveraging a Directed Message-Passing Neural Network to
predict these descriptors in a noise-free setting. Unlike conventional
approaches relying on noisy experimental data or biased quantum mechanical
simulations, CheMeleon uses low-noise molecular descriptors to learn rich
molecular representations. Evaluated on 58 benchmark datasets from Polaris and
MoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,
outperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop
(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)
and other foundation models. However, it struggles to distinguish activity
cliffs like many of the tested models. The t-SNE projection of CheMeleon's
learned representations demonstrates effective separation of chemical series,
highlighting its ability to capture structural nuances. These results
underscore the potential of descriptor-based pre-training for scalable and
effective molecular property prediction, opening avenues for further
exploration of descriptor sets and unlabeled datasets.

</details>


### [119] [DeepJ: Graph Convolutional Transformers with Differentiable Pooling for Patient Trajectory Modeling](https://arxiv.org/abs/2506.15809)
*Deyi Li,Zijun Yao,Muxuan Liang,Mei Liu*

Key words: 电子健康记录, 图卷积变换器, 跨就诊建模, 患者风险预测

TL;DR: DeepJ提出了一种新颖的图卷积变换器模型，用于捕捉医疗事件在跨历次就诊中的动态交互，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有图学习方法在电子健康记录（EHR）中对跨就诊的动态医疗事件建模不足。

Method: 采用图卷积变换器模型（DeepJ）结合可微分图池化，捕捉跨就诊的医疗事件交互。

Result: DeepJ在患者结局预测中显著优于五种基线模型，且具有更好的可解释性。

Conclusion: DeepJ能够有效建模跨就诊的医疗事件交互，提升患者风险分层能力。

Abstract: In recent years, graph learning has gained significant interest for modeling
complex interactions among medical events in structured Electronic Health
Record (EHR) data. However, existing graph-based approaches often work in a
static manner, either restricting interactions within individual encounters or
collapsing all historical encounters into a single snapshot. As a result, when
it is necessary to identify meaningful groups of medical events spanning
longitudinal encounters, existing methods are inadequate in modeling
interactions cross encounters while accounting for temporal dependencies. To
address this limitation, we introduce Deep Patient Journey (DeepJ), a novel
graph convolutional transformer model with differentiable graph pooling to
effectively capture intra-encounter and inter-encounter medical event
interactions. DeepJ can identify groups of temporally and functionally related
medical events, offering valuable insights into key event clusters pertinent to
patient outcome prediction. DeepJ significantly outperformed five
state-of-the-art baseline models while enhancing interpretability,
demonstrating its potential for improved patient risk stratification.

</details>


### [120] [Optimizing Bidding Strategies in First-Price Auctions in Binary Feedback Setting with Predictions](https://arxiv.org/abs/2506.15817)
*Jason Tandiary*

Key words: 第一价格拍卖,机器学习,算法,BROAD-OMD,遗憾边界

TL;DR: 论文研究了二进制反馈下的Vickrey第一价格拍卖，提出了一种新算法，利用机器学习提高BROAD-OMD算法的遗憾边界，实现了零遗憾。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 第一价格拍卖的日益重要及机器学习模型的预测能力推动了该研究。

Method: 在BROAD-OMD框架内提出新算法，利用最高竞争出价的预测。

Result: 在准确预测下实现零遗憾；在一定正态条件下遗憾边界为O(T^(3/4)*Vt^(1/4))。

Conclusion: 新算法在第一价格拍卖中表现优越，尤其预测准确时表现更佳。

Abstract: This paper studies Vickrey first-price auctions under binary feedback.
Leveraging the enhanced performance of machine learning algorithms, the new
algorithm uses past information to improve the regret bounds of the BROAD-OMD
algorithm. Motivated by the growing relevance of first-price auctions and the
predictive capabilities of machine learning models, this paper proposes a new
algorithm within the BROAD-OMD framework (Hu et al., 2025) that leverages
predictions of the highest competing bid. This paper's main contribution is an
algorithm that achieves zero regret under accurate predictions. Additionally, a
bounded regret bound of O(T^(3/4) * Vt^(1/4)) is established under certain
normality conditions.

</details>


### [121] [AI-based modular warning machine for risk identification in proximity healthcare](https://arxiv.org/abs/2506.15823)
*Chiara Razzetta,Shahryar Noei,Federico Barbarossa,Edoardo Spairani,Monica Roascio,Elisa Barbi,Giulia Ciacci,Sara Sommariva,Sabrina Guastavino,Michele Piana,Matteo Lenge,Gabriele Arnulfo,Giovanni Magenes,Elvira Maranesi,Giulio Amabili,Anna Maria Massone,Federico Benvenuto,Giuseppe Jurman,Diego Sona,Cristina Campi*

Key words: 数字健康,社区医疗,机器学习,多模态数据,自动化流程

TL;DR: DHEAL-COM是一个研究项目，旨在开发数字健康解决方案，利用机器学习分析多模态数据，提供预测结果和特征解释。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过数字技术提升社区医疗服务，利用机器学习算法分析多模态数据以改善医疗决策。

Method: 提出了一个通用自动化流程，结合无监督和监督学习方法，处理数据并生成预测结果和特征识别。

Result: 开发了能处理多模态数据并提供预测与解释的自动化机器学习流程。

Conclusion: DHEAL-COM项目通过数字技术和机器学习，为社区医疗提供了高效的数据分析解决方案。

Abstract: "DHEAL-COM - Digital Health Solutions in Community Medicine" is a research
and technology project funded by the Italian Department of Health for the
development of digital solutions of interest in proximity healthcare. The
activity within the DHEAL-COM framework allows scientists to gather a notable
amount of multi-modal data whose interpretation can be performed by means of
machine learning algorithms. The present study illustrates a general automated
pipeline made of numerous unsupervised and supervised methods that can ingest
such data, provide predictive results, and facilitate model interpretations via
feature identification.

</details>


### [122] [Heterogeneous Federated Reinforcement Learning Using Wasserstein Barycenters](https://arxiv.org/abs/2506.15825)
*Luiz Pereira,M. Hadi Amini*

Key words: 模型融合, Wasserstein质心, 分布式架构, 异构联邦强化学习, DQN

TL;DR: 该论文提出了一种基于Wasserstein质心的新型算法FedWB，用于分布式架构中全局深度神经网络的训练，并应用于异构联邦强化学习（HFRL）问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究如何高效地在分布式架构中融合多个局部DNN模型，并解决异构联邦强化学习中的环境差异问题。

Method: 通过分割数据集到多个代理（agents）进行局部训练，利用Wasserstein质心进行权重参数聚合，提出了FedWB算法，并应用于HFRL场景。

Result: 在CartPole实验中，通过全局聚合局部模型，成功训练出一个适用于所有异构环境的全局DQN。

Conclusion: FedWB算法在分布式深度学习和异构联邦强化学习中表现出良好的适用性和有效性。

Abstract: In this paper, we first propose a novel algorithm for model fusion that
leverages Wasserstein barycenters in training a global Deep Neural Network
(DNN) in a distributed architecture. To this end, we divide the dataset into
equal parts that are fed to "agents" who have identical deep neural networks
and train only over the dataset fed to them (known as the local dataset). After
some training iterations, we perform an aggregation step where we combine the
weight parameters of all neural networks using Wasserstein barycenters. These
steps form the proposed algorithm referred to as FedWB. Moreover, we leverage
the processes created in the first part of the paper to develop an algorithm to
tackle Heterogeneous Federated Reinforcement Learning (HFRL). Our test
experiment is the CartPole toy problem, where we vary the lengths of the poles
to create heterogeneous environments. We train a deep Q-Network (DQN) in each
environment to learn to control each cart, while occasionally performing a
global aggregation step to generalize the local models; the end outcome is a
global DQN that functions across all environments.

</details>


### [123] [In-field Calibration of Low-Cost Sensors through XGBoost $\&$ Aggregate Sensor Data](https://arxiv.org/abs/2506.15840)
*Kevin Yin,Julia Gersey,Pei Zhang*

Key words: 空气质量监测, 低成本传感器, 数据漂移, XGBoost, 集成学习

TL;DR: 摘要提出了一种基于XGBoost集成学习的传感器校准模型，用于解决低成本传感器在城市空气质量监测中的漂移问题，从而提高数据精度和泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于高精度传感器成本高昂，难以大规模部署，而低成本传感器则易受环境变化和制造差异的影响导致数据漂移，因此需要一种有效的校准方法。

Method: 采用XGBoost集成学习模型，通过整合相邻传感器的数据来校准单个传感器的读数。

Result: 该方法减少了对单个传感器精度的依赖，提升了不同地理位置数据的泛化能力。

Conclusion: XGBoost集成学习能有效解决低成本传感器的漂移问题，适用于大范围空气质量监测。

Abstract: Effective large-scale air quality monitoring necessitates distributed sensing
due to the pervasive and harmful nature of particulate matter (PM),
particularly in urban environments. However, precision comes at a cost: highly
accurate sensors are expensive, limiting the spatial deployments and thus their
coverage. As a result, low-cost sensors have become popular, though they are
prone to drift caused by environmental sensitivity and manufacturing
variability. This paper presents a model for in-field sensor calibration using
XGBoost ensemble learning to consolidate data from neighboring sensors. This
approach reduces dependence on the presumed accuracy of individual sensors and
improves generalization across different locations.

</details>


### [124] [Uncertainty Estimation by Human Perception versus Neural Models](https://arxiv.org/abs/2506.15850)
*Pedro Mendes,Paolo Romano,David Garlan*

Key words: 神经网络, 不确定性, 人类感知, 校准, 可信AI

TL;DR: 现代神经网络（NNs）虽预测准确但校准不足，常产生过度自信的预测。研究比较了人类感知不确定性与其预测的差异，发现当前方法仅与人类直觉弱相关。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决神经网络预测中的不确定性校准问题，探究人类感知与模型不确定性的关系。

Method: 使用三个视觉基准数据集，结合人类分歧和众包置信度，评估模型预测不确定性与人类感知的关联。

Result: 当前方法与人类直觉仅弱相关，但通过引入人类软标签可改进校准且不损害准确性。

Conclusion: 模型与人类不确定性存在差距，利用人类洞察可提升AI系统的可信度。

Abstract: Modern neural networks (NNs) often achieve high predictive accuracy but
remain poorly calibrated, producing overconfident predictions even when wrong.
This miscalibration poses serious challenges in applications where reliable
uncertainty estimates are critical. In this work, we investigate how human
perceptual uncertainty compares to uncertainty estimated by NNs. Using three
vision benchmarks annotated with both human disagreement and crowdsourced
confidence, we assess the correlation between model-predicted uncertainty and
human-perceived uncertainty. Our results show that current methods only weakly
align with human intuition, with correlations varying significantly across
tasks and uncertainty metrics. Notably, we find that incorporating
human-derived soft labels into the training process can improve calibration
without compromising accuracy. These findings reveal a persistent gap between
model and human uncertainty and highlight the potential of leveraging human
insights to guide the development of more trustworthy AI systems.

</details>


### [125] [Improving Rectified Flow with Boundary Conditions](https://arxiv.org/abs/2506.15864)
*Xixi Hu,Runlong Liao,Keyang Xu,Bo Liu,Yeqing Li,Eugene Ie,Hongliang Fei,Qiang Liu*

Key words: Rectified Flow, 边界条件, 生成模型, FID分数

TL;DR: 边界约束Rectified Flow模型通过强制边界条件改进速度场估计，显著提升生成模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 直接建模速度场时，边界条件未满足导致估计不准，尤其在随机采样时误差放大。

Method: 提出边界约束Rectified Flow模型，通过最小代码修改强制边界条件。

Result: 在ImageNet上，FID分数提升8.01%（ODE采样）和8.98%（SDE采样）。

Conclusion: 边界约束有效提升速度场估计精度，改善生成质量。

Abstract: Rectified Flow offers a simple and effective approach to high-quality
generative modeling by learning a velocity field. However, we identify a
limitation in directly modeling the velocity with an unconstrained neural
network: the learned velocity often fails to satisfy certain boundary
conditions, leading to inaccurate velocity field estimations that deviate from
the desired ODE. This issue is particularly critical during stochastic sampling
at inference, as the score function's errors are amplified near the boundary.
To mitigate this, we propose a Boundary-enforced Rectified Flow Model (Boundary
RF Model), in which we enforce boundary conditions with a minimal code
modification. Boundary RF Model improves performance over vanilla RF model,
demonstrating 8.01% improvement in FID score on ImageNet using ODE sampling and
8.98% improvement using SDE sampling.

</details>


### [126] [Hidden Breakthroughs in Language Model Training](https://arxiv.org/abs/2506.15872)
*Sara Kangaslahti,Elan Rosenfeld,Naomi Saphra*

Key words: 损失曲线,概念突破,POLCA,无监督解释,学习动态

TL;DR: 该论文通过POLCA方法分解损失变化，揭示了训练过程中隐藏的概念突破，并通过实验验证了其作为无监督解释工具的价值。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究训练损失中的不连续性（可能代表概念突破）以深入理解学习动态，但传统损失度量掩盖了这些变化。

Method: 提出POLCA方法，通过分解损失变化在低秩训练子空间中的任意基上，识别样本簇的损失变化模式。

Result: 在合成算术和自然语言任务中验证了POLCA的有效性，发现可解释的概念突破簇。

Conclusion: 隐藏的相变是潜在的无监督解释工具，POLCA能有效揭示训练中的概念突破。

Abstract: Loss curves are smooth during most of model training, so visible
discontinuities stand out as possible conceptual breakthroughs. Studying these
breakthroughs enables a deeper understanding of learning dynamics, but only
when they are properly identified. This paper argues that similar breakthroughs
occur frequently throughout training but they are obscured by a loss metric
that collapses all variation into a single scalar. To find these hidden
transitions, we introduce POLCA, a method for decomposing changes in loss along
arbitrary bases of the low-rank training subspace. We use our method to
identify clusters of samples that share similar changes in loss during
training, disaggregating the overall loss into that of smaller groups of
conceptually similar data. We validate our method on synthetic arithmetic and
natural language tasks, showing that POLCA recovers clusters that represent
interpretable breakthroughs in the model's capabilities. We demonstrate the
promise of these hidden phase transitions as a tool for unsupervised
interpretability.

</details>


### [127] [Job Market Cheat Codes: Prototyping Salary Prediction and Job Grouping with Synthetic Job Listings](https://arxiv.org/abs/2506.15879)
*Abdel Rahman Alsheyab,Mohammad Alkhasawneh,Nidal Shahin*

Key words: 机器学习, 薪资预测, 职位聚类, 自然语言处理, 合成数据

TL;DR: 该论文提出了一种基于大型合成数据集的原型机器学习方法，用于识别趋势、预测薪资并聚类相似职位角色。通过回归、分类、聚类和自然语言处理（NLP）等技术，揭示了影响就业市场的关键特征，为求职者、雇主和研究者提供有价值的信息。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究旨在通过合成数据揭示就业市场的关键影响因素，并为相关方提供实用见解。

Method: 采用回归模型预测薪资、分类模型预测职位名称、聚类技术分组相似职位，并结合NLP进行文本特征提取。

Result: 分析揭示了影响薪资和职位角色的重要因素，并基于数据识别了不同的职位聚类。

Conclusion: 尽管基于合成数据，该方法展示了可转移的就业市场分析框架。

Abstract: This paper presents a machine learning methodology prototype using a large
synthetic dataset of job listings to identify trends, predict salaries, and
group similar job roles. Employing techniques such as regression,
classification, clustering, and natural language processing (NLP) for
text-based feature extraction and representation, this study aims to uncover
the key features influencing job market dynamics and provide valuable insights
for job seekers, employers, and researchers. Exploratory data analysis was
conducted to understand the dataset's characteristics. Subsequently, regression
models were developed to predict salaries, classification models to predict job
titles, and clustering techniques were applied to group similar jobs. The
analyses revealed significant factors influencing salary and job roles, and
identified distinct job clusters based on the provided data. While the results
are based on synthetic data and not intended for real-world deployment, the
methodology demonstrates a transferable framework for job market analysis.

</details>


### [128] [T-SHRED: Symbolic Regression for Regularization and Model Discovery with Transformer Shallow Recurrent Decoders](https://arxiv.org/abs/2506.15881)
*Alexey Yermakov,David Zoro,Mars Liyao Gao,J. Nathan Kutz*

Key words: SHallow REcurrent Decoders, SHRED, Transformer, SINDy, symbolic regression

TL;DR: SHallow REcurrent Decoders（SHRED）是一种轻量级的模型，适用于从稀疏传感器测量中进行系统识别和预测。本研究通过引入Transformer（T-SHRED）和SINDy注意力机制，进一步提升了模型的性能和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过改进SHRED模型，提高其在大型数据集上的预测性能，并通过符号回归增强模型的可解释性。

Method: 使用Transformer替代循环神经网络（RNN）进行时间编码，并引入SINDy注意力机制进行符号回归，从而优化了模型的潜在空间动态。

Result: T-SHRED在三种不同的动态系统中表现出色，特别是在稀疏和高数据量情况下，能够准确预测未来状态并生成可解释的符号模型。

Conclusion: 通过结合Transformer和SINDy注意力机制，T-SHRED不仅提升了预测性能，还增强了模型的可解释性，适用于多种动态系统。

Abstract: SHallow REcurrent Decoders (SHRED) are effective for system identification
and forecasting from sparse sensor measurements. Such models are light-weight
and computationally efficient, allowing them to be trained on consumer laptops.
SHRED-based models rely on Recurrent Neural Networks (RNNs) and a simple
Multi-Layer Perceptron (MLP) for the temporal encoding and spatial decoding
respectively. Despite the relatively simple structure of SHRED, they are able
to predict chaotic dynamical systems on different physical, spatial, and
temporal scales directly from a sparse set of sensor measurements. In this
work, we improve SHRED by leveraging transformers (T-SHRED) for the temporal
encoding which improves performance on next-step state prediction on large
datasets. We also introduce a sparse identification of nonlinear dynamics
(SINDy) attention mechanism into T-SHRED to perform symbolic regression
directly on the latent space as part of the model regularization architecture.
Symbolic regression improves model interpretability by learning and
regularizing the dynamics of the latent space during training. We analyze the
performance of T-SHRED on three different dynamical systems ranging from
low-data to high-data regimes. We observe that SINDy attention T-SHRED
accurately predicts future frames based on an interpretable symbolic model
across all tested datasets.

</details>


### [129] [Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute](https://arxiv.org/abs/2506.15882)
*Sheng Liu,Tianlang Chen,Pan Lu,Haotian Ye,Yizheng Chen,Lei Xing,James Zou*

Key words: 大型语言模型、推理深度、测试时计算、Fractional Reasoning

TL;DR: 论文提出了一种名为Fractional Reasoning的训练无关框架，通过动态调整推理深度来提升大型语言模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法如Best-of-N和自省推理在处理不同复杂度的输入时采用固定推理强度，缺乏灵活性。

Method: 提取与深度推理相关的潜在导向向量，并通过可调参数动态调整推理强度，适应输入复杂度。

Result: 在GSM8K、MATH500和GPQA等任务上，性能显著提升。

Conclusion: Fractional Reasoning通过灵活控制推理深度，为提升模型性能提供了一种有效方法。

Abstract: Test-time compute has emerged as a powerful paradigm for improving the
performance of large language models (LLMs), where generating multiple outputs
or refining individual chains can significantly boost answer accuracy. However,
existing methods like Best-of-N, majority voting, and self-reflection typically
apply reasoning in a uniform way across inputs, overlooking the fact that
different problems may require different levels of reasoning depth. In this
work, we propose Fractional Reasoning, a training-free and model-agnostic
framework that enables continuous control over reasoning intensity at inference
time, going beyond the limitations of fixed instructional prompts. Our method
operates by extracting the latent steering vector associated with deeper
reasoning and reapplying it with a tunable scaling factor, allowing the model
to tailor its reasoning process to the complexity of each input. This supports
two key modes of test-time scaling: (1) improving output quality in
breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing
the correctness of individual reasoning chains in depth-based strategies (e.g.,
self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that
Fractional Reasoning consistently improves performance across diverse reasoning
tasks and models.

</details>


### [130] [Formal Models of Active Learning from Contrastive Examples](https://arxiv.org/abs/2506.15893)
*Farnam Mansouri,Hans U. Simon,Adish Singla,Yuxin Chen,Sandra Zilles*

Key words: 对比样本、主动学习、样本复杂性、理论框架、自导向学习

TL;DR: 论文提出了一个理论框架，研究对比样本对主动学习的影响，重点关注样本复杂性，并揭示了对比学习与自导向学习之间的联系。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过对比样本（微小差异但标签不同的实例）可以提升机器学习的效果，但对其在主动学习中的作用缺乏理论分析。

Method: 提出理论框架，研究不同类型的对比样本对主动学习中概念类样本复杂性的影响，以几何概念类和布尔函数类为例。

Result: 揭示了对比样本选择对学习样本复杂性的影响，并发现对比学习与自导向学习之间的关联。

Conclusion: 对比样本在主动学习中具有重要作用，其理论分析为机器学习提供了新的视角。

Abstract: Machine learning can greatly benefit from providing learning algorithms with
pairs of contrastive training examples -- typically pairs of instances that
differ only slightly, yet have different class labels. Intuitively, the
difference in the instances helps explain the difference in the class labels.
This paper proposes a theoretical framework in which the effect of various
types of contrastive examples on active learners is studied formally. The focus
is on the sample complexity of learning concept classes and how it is
influenced by the choice of contrastive examples. We illustrate our results
with geometric concept classes and classes of Boolean functions. Interestingly,
we reveal a connection between learning from contrastive examples and the
classical model of self-directed learning.

</details>


### [131] [KG-FGNN: Knowledge-guided GNN Foundation Model for Fertilisation-oriented Soil GHG Flux Prediction](https://arxiv.org/abs/2506.15896)
*Yu Zhang,Gaoshan Bi,Simon Jeffery,Max Davis,Yang Li,Qing Xue,Po Yang*

Key words: 土壤温室气体通量, 农业过程模型, 图神经网络, 自编码器, 机器学习

TL;DR: 提出了一种知识引导的图神经网络框架，结合农业过程模型和机器学习技术，解决了农业数据稀缺问题，提高了土壤温室气体通量的预测精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 农业数据稀缺限制了机器学习在精准土壤温室气体通量预测中的应用，影响环境评估和可持续农业发展。

Method: 结合农业过程模型模拟多维度农业数据，采用自编码器和多目标多图神经网络提取关键特征并整合特征相关性。

Result: 实验表明，该方法在施肥相关的土壤温室气体预测中具有更高的准确性和稳定性。

Conclusion: 该方法有效解决了数据稀缺问题，为温室气体减排策略和可持续农业提供了有力工具。

Abstract: Precision soil greenhouse gas (GHG) flux prediction is essential in
agricultural systems for assessing environmental impacts, developing emission
mitigation strategies and promoting sustainable agriculture. Due to the lack of
advanced sensor and network technologies on majority of farms, there are
challenges in obtaining comprehensive and diverse agricultural data. As a
result, the scarcity of agricultural data seriously obstructs the application
of machine learning approaches in precision soil GHG flux prediction. This
research proposes a knowledge-guided graph neural network framework that
addresses the above challenges by integrating knowledge embedded in an
agricultural process-based model and graph neural network techniques.
Specifically, we utilise the agricultural process-based model to simulate and
generate multi-dimensional agricultural datasets for 47 countries that cover a
wide range of agricultural variables. To extract key agricultural features and
integrate correlations among agricultural features in the prediction process,
we propose a machine learning framework that integrates the autoencoder and
multi-target multi-graph based graph neural networks, which utilises the
autoencoder to selectively extract significant agricultural features from the
agricultural process-based model simulation data and the graph neural network
to integrate correlations among agricultural features for accurately predict
fertilisation-oriented soil GHG fluxes. Comprehensive experiments were
conducted with both the agricultural simulation dataset and real-world
agricultural dataset to evaluate the proposed approach in comparison with
well-known baseline and state-of-the-art regression methods. The results
demonstrate that our proposed approach provides superior accuracy and stability
in fertilisation-oriented soil GHG prediction.

</details>


### [132] [TrajDiff: Diffusion Bridge Network with Semantic Alignment for Trajectory Similarity Computation](https://arxiv.org/abs/2506.15898)
*Xiao Zhang,Xingyu Zhao,Hong Xia,Yuan Cao,Guiyuan Jiang,Junyu Dong,Yanwei Yu*

Key words: 轨迹相似性计算, 语义对齐, 抗噪预训练, 全局排名, TrajDiff

TL;DR: 论文提出了一种名为TrajDiff的新型轨迹相似性计算框架，通过语义对齐模块、基于DDBM的抗噪预训练和全局排名感知正则化，解决了现有方法在语义差距、噪声干扰和局部损失方面的挑战，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着位置追踪技术的普及，海量轨迹数据不断被收集。轨迹相似性计算作为轨迹数据挖掘的基本任务，在众多现实应用中至关重要。然而，现有学习方法面临语义差距、噪声干扰和局部损失三大挑战。

Method: 提出TrajDiff框架，包含三个关键模块：(1) 语义对齐模块通过跨注意力和自适应融合机制消除不同尺度数据间的语义差异；(2) 基于DDBM的抗噪预训练引入轨迹间的转移模式增强模型抗噪能力；(3) 全局排名感知正则化使模型从局部视角转向全局，捕捉轨迹整体排序信息。

Result: 在三个公开数据集上的实验表明，TrajDiff显著优于现有最佳方法，平均HR@1提升了33.38%。

Conclusion: TrajDiff通过语义对齐、抗噪预训练和全局排名感知正则化，有效解决了现有方法的不足，为轨迹相似性计算提供了新思路。

Abstract: With the proliferation of location-tracking technologies, massive volumes of
trajectory data are continuously being collected. As a fundamental task in
trajectory data mining, trajectory similarity computation plays a critical role
in a wide range of real-world applications. However, existing learning-based
methods face three challenges: First, they ignore the semantic gap between GPS
and grid features in trajectories, making it difficult to obtain meaningful
trajectory embeddings. Second, the noise inherent in the trajectories, as well
as the noise introduced during grid discretization, obscures the true motion
patterns of the trajectories. Third, existing methods focus solely on
point-wise and pair-wise losses, without utilizing the global ranking
information obtained by sorting all trajectories according to their similarity
to a given trajectory. To address the aforementioned challenges, we propose a
novel trajectory similarity computation framework, named TrajDiff.
Specifically, the semantic alignment module relies on cross-attention and an
attention score mask mechanism with adaptive fusion, effectively eliminating
semantic discrepancies between data at two scales and generating a unified
representation. Additionally, the DDBM-based Noise-robust Pre-Training
introduces the transfer patterns between any two trajectories into the model
training process, enhancing the model's noise robustness. Finally, the overall
ranking-aware regularization shifts the model's focus from a local to a global
perspective, enabling it to capture the holistic ordering information among
trajectories. Extensive experiments on three publicly available datasets show
that TrajDiff consistently outperforms state-of-the-art baselines. In
particular, it achieves an average HR@1 gain of 33.38% across all three
evaluation metrics and datasets.

</details>


### [133] [Clinically Interpretable Mortality Prediction for ICU Patients with Diabetes and Atrial Fibrillation: A Machine Learning Approach](https://arxiv.org/abs/2506.15901)
*Li Sun,Shuheng Chen,Yong Si,Junyi Fan,Maryam Pishgar,Elham Pishgar,Kamiar Alaei,Greg Placencia*

Key words: 糖尿病；心房颤动；机器学习；死亡率预测；ICU

TL;DR: 该论文开发了一种可解释的机器学习模型，用于预测ICU中同时患有糖尿病和心房颤动的患者28天死亡率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对ICU中糖尿病和心房颤动患者高死亡率的风险，目前缺乏针对这一高风险群体的预测模型。

Method: 使用MIMIC-IV数据库中的1,535名患者数据，经过预处理和特征选择后，训练并评估了7种机器学习模型。

Result: 逻辑回归表现最佳（AUROC：0.825），关键预测因素包括RAS、年龄、胆红素和拔管。

Conclusion: 该模型为糖尿病和心房颤动患者的早期ICU分诊提供了准确预测和临床见解。

Abstract: Background: Patients with both diabetes mellitus (DM) and atrial fibrillation
(AF) face elevated mortality in intensive care units (ICUs), yet models
targeting this high-risk group remain limited.
  Objective: To develop an interpretable machine learning (ML) model predicting
28-day mortality in ICU patients with concurrent DM and AF using early-phase
clinical data.
  Methods: A retrospective cohort of 1,535 adult ICU patients with DM and AF
was extracted from the MIMIC-IV database. Data preprocessing involved
median/mode imputation, z-score normalization, and early temporal feature
engineering. A two-step feature selection pipeline-univariate filtering (ANOVA
F-test) and Random Forest-based multivariate ranking-yielded 19 interpretable
features. Seven ML models were trained with stratified 5-fold cross-validation
and SMOTE oversampling. Interpretability was assessed via ablation and
Accumulated Local Effects (ALE) analysis.
  Results: Logistic regression achieved the best performance (AUROC: 0.825; 95%
CI: 0.779-0.867), surpassing more complex models. Key predictors included RAS,
age, bilirubin, and extubation. ALE plots showed intuitive, non-linear effects
such as age-related risk acceleration and bilirubin thresholds.
  Conclusion: This interpretable ML model offers accurate risk prediction and
clinical insights for early ICU triage in patients with DM and AF.

</details>


### [134] [VectorEdits: A Dataset and Benchmark for Instruction-Based Editing of Vector Graphics](https://arxiv.org/abs/2506.15903)
*Josef Kuchař,Marek Kadlčík,Michal Spiegel,Michal Štefánik*

Key words: 矢量图像编辑,自然语言指令,数据集,CLIP,视觉语言模型

TL;DR: 发布了包含27万对SVG图像与自然语言编辑指令的大规模数据集，用于指令引导的矢量图像编辑任务。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 支持基于文本指令的矢量图形编辑模型的训练与评估，推动自然语言驱动的矢量图形生成与编辑研究。

Method: 通过CLIP相似性匹配图像对，结合视觉语言模型生成编辑指令。

Result: 实验显示现有大语言模型难以生成准确且有效的编辑结果。

Conclusion: 数据集公开以促进相关研究。

Abstract: We introduce a large-scale dataset for instruction-guided vector image
editing, consisting of over 270,000 pairs of SVG images paired with natural
language edit instructions. Our dataset enables training and evaluation of
models that modify vector graphics based on textual commands. We describe the
data collection process, including image pairing via CLIP similarity and
instruction generation with vision-language models. Initial experiments with
state-of-the-art large language models reveal that current methods struggle to
produce accurate and valid edits, underscoring the challenge of this task. To
foster research in natural language-driven vector graphic generation and
editing, we make our resources created within this work publicly available.

</details>


### [135] [Pieceformer: Similarity-Driven Knowledge Transfer via Scalable Graph Transformer in VLSI](https://arxiv.org/abs/2506.15907)
*Hang Yang,Yusheng Hu,Yong Liu,Cong,Hao*

Key words: 图相似性, VLSI设计, 自监督学习, 图变换器

TL;DR: Pieceformer是一种自监督图相似性评估框架，通过混合消息传递和图变换器编码器提升VLSI设计中的知识转移效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 提高VLSI设计中的知识转移精度，减少工程成本和周转时间。

Method: 提出Pieceformer框架，结合混合消息传递和图变换器编码器，并引入线性变换器骨架和分区训练管道以提升可扩展性。

Result: 在CircuitNet数据集上，Pieceformer将MAE降低24.9%，并能正确聚类所有实际设计组，在分区任务中实现最多89%的运行时减少。

Conclusion: Pieceformer为现代VLSI系统提供了高效、可扩展且无偏的设计重用解决方案。

Abstract: Accurate graph similarity is critical for knowledge transfer in VLSI design,
enabling the reuse of prior solutions to reduce engineering effort and
turnaround time. We propose Pieceformer, a scalable, self-supervised similarity
assessment framework, equipped with a hybrid message-passing and graph
transformer encoder. To address transformer scalability, we incorporate a
linear transformer backbone and introduce a partitioned training pipeline for
efficient memory and parallelism management. Evaluations on synthetic and
real-world CircuitNet datasets show that Pieceformer reduces mean absolute
error (MAE) by 24.9% over the baseline and is the only method to correctly
cluster all real-world design groups. We further demonstrate the practical
usage of our model through a case study on a partitioning task, achieving up to
89% runtime reduction. These results validate the framework's effectiveness for
scalable, unbiased design reuse in modern VLSI systems.

</details>


### [136] [Early Attentive Sparsification Accelerates Neural Speech Transcription](https://arxiv.org/abs/2506.15912)
*Zifei Xu,Sayeh Sharify,Hesham Mostafa,Tristan Webb,Wanzin Yazar,Xin Wang*

Key words: Transformer, 语音处理, 稀疏化, 运行时加速, Whisper模型

TL;DR: 通过Transformer模型在语音处理中实现时间域信号稀疏化，以加速神经语音转录，在保持精度下降不超过1%的情况下，运行时加速1.6倍。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 语音信号具有高度可压缩性，利用Transformer编码器的自注意力机制可解释性，在早期编码阶段实现信号稀疏化以提高处理速度。

Method: 使用Whisper模型系列，对稀疏化阶段（编码层）和压缩比（稀疏度）进行联合架构搜索。

Result: 最佳方案在精度下降不超过1%的情况下，隐藏状态稀疏化至40-60%，在英文语音转录任务中实现1.6倍运行时加速。

Conclusion: 早期编码阶段的稀疏化可以显著加速语音转录，同时保持较高的准确性。

Abstract: Transformer-based neural speech processing has achieved state-of-the-art
performance. Since speech audio signals are known to be highly compressible,
here we seek to accelerate neural speech transcription by time-domain signal
sparsification early in the neural encoding stage, taking advantage of the
interpretability of the self-attention mechanism in transformer audio encoders.
With the Whisper family of models, we perform a systematic architecture search
over the joint space of sparsification stage (a certain encoder layer) and
compression ratio (sparsity). We found that the best resulting solutions under
1% accuracy degradation choose to sparsify the hidden state to 40-60% sparsity
at an early encoding stage, and thereby achieve up to 1.6x runtime acceleration
in English speech transcription tasks on Nvidia GPUs without any fine-tuning.

</details>


### [137] [PNCS:Power-Norm Cosine Similarity for Diverse Client Selection in Federated Learning](https://arxiv.org/abs/2506.15923)
*Liangyan Li,Yangyi Liu,Yimo Ning,Stefano Rini,Jun Chen*

Key words: 联邦学习, 梯度相关性, 客户端选择, 非独立同分布数据

TL;DR: 提出了一种基于功率范数余弦相似性（PNCS）的联邦学习框架，通过捕捉高阶梯度矩和多样化客户端选择，提升了非独立同分布数据下的收敛速度和准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有联邦学习方法未充分考虑远程客户端间的梯度相关性，尤其是在数据异构场景下，这一问题尤为突出。

Method: 利用功率范数余弦相似性进行客户端选择，并通过选择历史队列实现多样化选择。

Result: 在VGG16模型上的实验表明，该方法在不同数据划分下均优于现有技术。

Conclusion: 提出的PNCS框架有效解决了非独立同分布数据的联邦学习问题，提升了性能。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for leveraging
diverse datasets from multiple sources while preserving data privacy by
avoiding centralized storage. However, many existing approaches fail to account
for the intricate gradient correlations between remote clients, a limitation
that becomes especially problematic in data heterogeneity scenarios. In this
work, we propose a novel FL framework utilizing Power-Norm Cosine Similarity
(PNCS) to improve client selection for model aggregation. By capturing
higher-order gradient moments, PNCS addresses non-IID data challenges,
enhancing convergence speed and accuracy. Additionally, we introduce a simple
algorithm ensuring diverse client selection through a selection history queue.
Experiments with a VGG16 model across varied data partitions demonstrate
consistent improvements over state-of-the-art methods.

</details>


### [138] [Competing Bandits in Matching Markets via Super Stability](https://arxiv.org/abs/2506.15926)
*Soumya Basu*

Key words: 匹配市场，强盗学习，扩展GS算法，稳定遗憾，双向不确定性

TL;DR: 研究了双向不确定性下的匹配市场中强盗学习问题，扩展了以往单侧不确定性的研究，通过扩展GS算法实现对数级别的最差稳定遗憾。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 匹配市场中的双向不确定性尚未充分研究，扩展GS算法可以提供更稳定的匹配结果。

Method: 使用扩展GS算法，分别在集中式和分散式设置中实现对数级别和最差稳定遗憾。

Result: 算法在集中式设置中达到对数级别的稳定遗憾，分散式设置中仅增加常数遗憾。

Conclusion: 扩展GS算法在双向不确定性匹配市场中表现优越，并揭示了可接受差距和超稳定匹配的复杂性。

Abstract: We study bandit learning in matching markets with two-sided reward
uncertainty, extending prior research primarily focused on single-sided
uncertainty. Leveraging the concept of `super-stability' from Irving (1994), we
demonstrate the advantage of the Extended Gale-Shapley (GS) algorithm over the
standard GS algorithm in achieving true stable matchings under incomplete
information. By employing the Extended GS algorithm, our centralized algorithm
attains a logarithmic pessimal stable regret dependent on an instance-dependent
admissible gap parameter. This algorithm is further adapted to a decentralized
setting with a constant regret increase. Finally, we establish a novel
centralized instance-dependent lower bound for binary stable regret,
elucidating the roles of the admissible gap and super-stable matching in
characterizing the complexity of stable matching with bandit feedback.

</details>


### [139] [CORAL: Disentangling Latent Representations in Long-Tailed Diffusion](https://arxiv.org/abs/2506.15933)
*Esther Rodriguez,Monica Welfert,Samuel McDowell,Nathan Stromberg,Julian Antolin Camarena,Lalitha Sankar*

Key words: 扩散模型,长尾分布,潜在表示,对比学习,CORAL

TL;DR: 本文研究了扩散模型在长尾数据集上的表现，提出了一种新的对比潜在对齐框架（CORAL）以提高尾部类别的生成质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现实世界中的多类数据通常呈现长尾分布，标准扩散模型在这种分布下对尾部类别的生成质量较低。本文旨在解决这一问题。

Method: 通过分析扩散模型在长尾数据集上的行为，发现潜在表示的重叠是尾部类别生成质量低的原因，并提出了CORAL框架，利用对比损失优化潜在表示。

Result: 实验表明，CORAL显著提高了尾部类别生成样本的多样性和视觉质量，优于现有方法。

Conclusion: CORAL框架通过对比潜在对齐有效改善了扩散模型在长尾分布下的性能，为尾部类别生成提供了更好的解决方案。

Abstract: Diffusion models have achieved impressive performance in generating
high-quality and diverse synthetic data. However, their success typically
assumes a class-balanced training distribution. In real-world settings,
multi-class data often follow a long-tailed distribution, where standard
diffusion models struggle -- producing low-diversity and lower-quality samples
for tail classes. While this degradation is well-documented, its underlying
cause remains poorly understood. In this work, we investigate the behavior of
diffusion models trained on long-tailed datasets and identify a key issue: the
latent representations (from the bottleneck layer of the U-Net) for tail class
subspaces exhibit significant overlap with those of head classes, leading to
feature borrowing and poor generation quality. Importantly, we show that this
is not merely due to limited data per class, but that the relative class
imbalance significantly contributes to this phenomenon. To address this, we
propose COntrastive Regularization for Aligning Latents (CORAL), a contrastive
latent alignment framework that leverages supervised contrastive losses to
encourage well-separated latent class representations. Experiments demonstrate
that CORAL significantly improves both the diversity and visual quality of
samples generated for tail classes relative to state-of-the-art methods.

</details>


### [140] [On the optimal regret of collaborative personalized linear bandits](https://arxiv.org/abs/2506.15943)
*Bruce Huang,Ruida Zhou,Lin F. Yang,Suhas Diggavi*

Key words: Stochastic linear bandits, multi-agent collaboration, regret minimization, hierarchical Bayesian framework, information-theoretic bounds

TL;DR: 研究多智能体协作中的个性化线性赌博问题，提出了一种新的两阶段协作算法，实现了最优遗憾，并证明了协作的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多智能体在解决异构赌博问题时，各自独立应用单智能体算法会忽略跨智能体的相似性和学习机会。本文旨在探究协作对个性化线性赌博问题的影响。

Method: 提出了一种新的两阶段协作算法，并通过层次贝叶斯框架建模异构性，引入了一种新的信息论技术来约束遗憾。

Result: 算法实现了最优遗憾，具体表现为在不同轮次范围内的不同遗憾上界。

Conclusion: 协作在多智能体个性化线性赌博问题中能够显著提升性能，尤其在异构性较高的情况下效果更明显。

Abstract: Stochastic linear bandits are a fundamental model for sequential decision
making, where an agent selects a vector-valued action and receives a noisy
reward with expected value given by an unknown linear function. Although well
studied in the single-agent setting, many real-world scenarios involve multiple
agents solving heterogeneous bandit problems, each with a different unknown
parameter. Applying single agent algorithms independently ignores cross-agent
similarity and learning opportunities. This paper investigates the optimal
regret achievable in collaborative personalized linear bandits. We provide an
information-theoretic lower bound that characterizes how the number of agents,
the interaction rounds, and the degree of heterogeneity jointly affect regret.
We then propose a new two-stage collaborative algorithm that achieves the
optimal regret. Our analysis models heterogeneity via a hierarchical Bayesian
framework and introduces a novel information-theoretic technique for bounding
regret. Our results offer a complete characterization of when and how
collaboration helps with a optimal regret bound $\tilde{O}(d\sqrt{mn})$,
$\tilde{O}(dm^{1-\gamma}\sqrt{n})$, $\tilde{O}(dm\sqrt{n})$ for the number of
rounds $n$ in the range of $(0, \frac{d}{m \sigma^2})$, $[\frac{d}{m^{2\gamma}
\sigma^2}, \frac{d}{\sigma^2}]$ and $(\frac{d}{\sigma^2}, \infty)$
respectively, where $\sigma$ measures the level of heterogeneity, $m$ is the
number of agents, and $\gamma\in[0, 1/2]$ is an absolute constant. In contrast,
agents without collaboration achieve a regret bound $O(dm\sqrt{n})$ at best.

</details>


### [141] [One Period to Rule Them All: Identifying Critical Learning Periods in Deep Networks](https://arxiv.org/abs/2506.15954)
*Vinicius Yuiti Fukase,Heitor Gama,Barbara Bueno,Lucas Libanio,Anna Helena Reali Costa,Artur Jordao*

Key words: 深度学习, 关键学习时期, 计算成本, 可持续性

TL;DR: 该论文提出了一种系统方法，用于识别深度神经网络训练中的关键时期，通过减少计算成本显著提高效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有研究证实了关键学习时期的存在，但缺乏精确识别其发生时间的方法。本研究旨在填补这一空白。

Method: 利用泛化预测机制识别关键时期，并停止资源密集型训练方法，以减少计算成本和排放。

Result: 实验显示，该方法将训练时间减少59.67%，CO$_2$排放减少59.47%，成本降低60%，且不影响性能。

Conclusion: 该方法为深度学习提供了更高效、可持续的训练框架，特别适用于资源受限环境。

Abstract: Critical Learning Periods comprehend an important phenomenon involving deep
learning, where early epochs play a decisive role in the success of many
training recipes, such as data augmentation. Existing works confirm the
existence of this phenomenon and provide useful insights. However, the
literature lacks efforts to precisely identify when critical periods occur. In
this work, we fill this gap by introducing a systematic approach for
identifying critical periods during the training of deep neural networks,
focusing on eliminating computationally intensive regularization techniques and
effectively applying mechanisms for reducing computational costs, such as data
pruning. Our method leverages generalization prediction mechanisms to pinpoint
critical phases where training recipes yield maximum benefits to the predictive
ability of models. By halting resource-intensive recipes beyond these periods,
we significantly accelerate the learning phase and achieve reductions in
training time, energy consumption, and CO$_2$ emissions. Experiments on
standard architectures and benchmarks confirm the effectiveness of our method.
Specifically, we achieve significant milestones by reducing the training time
of popular architectures by up to 59.67%, leading to a 59.47% decrease in
CO$_2$ emissions and a 60% reduction in financial costs, without compromising
performance. Our work enhances understanding of training dynamics and paves the
way for more sustainable and efficient deep learning practices, particularly in
resource-constrained environments. In the era of the race for foundation
models, we believe our method emerges as a valuable framework. The repository
is available at https://github.com/baunilhamarga/critical-periods

</details>


### [142] [On the Theoretical Understanding of Identifiable Sparse Autoencoders and Beyond](https://arxiv.org/abs/2506.15963)
*Jingyi Cui,Qi Zhang,Yifei Wang,Yisen Wang*

Key words: 稀疏自编码器, 可识别性, 单义特征, 加权策略, 理论分析

TL;DR: 该论文通过理论分析提出了稀疏自编码器（SAEs）完全恢复单义特征的必要和充分条件，并提出了一种加权策略以改善可识别性。实验验证了理论，加权SAE显著提升了单义性和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究稀疏自编码器（SAEs）在何种条件下能从叠加的多义特征中完全恢复真实的单义特征，并解决其可识别性问题。

Method: 通过理论分析提出SAEs可识别的三条件，并设计加权策略优化重构效果。

Result: 实验证明加权SAE能显著提升单义性和可解释性。

Conclusion: 论文为SAEs的可识别性提供了理论支持，并通过加权策略改进了实际效果。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting
features learned by large language models (LLMs). It aims to recover complex
superposed polysemantic features into interpretable monosemantic ones through
feature reconstruction via sparsely activated neural networks. Despite the wide
applications of SAEs, it remains unclear under what conditions an SAE can fully
recover the ground truth monosemantic features from the superposed polysemantic
ones. In this paper, through theoretical analysis, we for the first time
propose the necessary and sufficient conditions for identifiable SAEs (SAEs
that learn unique and ground truth monosemantic features), including 1) extreme
sparsity of the ground truth feature, 2) sparse activation of SAEs, and 3)
enough hidden dimensions of SAEs. Moreover, when the identifiable conditions
are not fully met, we propose a reweighting strategy to improve the
identifiability. Specifically, following the theoretically suggested weight
selection principle, we prove that the gap between the loss functions of SAE
reconstruction and monosemantic feature reconstruction can be narrowed, so that
the reweighted SAEs have better reconstruction of the ground truth monosemantic
features than the uniformly weighted ones. In experiments, we validate our
theoretical findings and show that our weighted SAE significantly improves
feature monosemanticity and interpretability.

</details>


### [143] [LazyEviction: Lagged KV Eviction with Attention Pattern Observation for Efficient Long Reasoning](https://arxiv.org/abs/2506.15969)
*Haoyue Zhang,Hualei Zhang,Xiaosong Ma,Jie Zhang,Song Guo*

Key words: LLMs, Chain-of-Thought, KV cache, LazyEviction, token importance recurrence

TL;DR: LLMs使用CoT加强推理能力，但长推理序列导致KV缓存内存开销大。LazyEviction提出滞后淘汰机制，保留重复关键token，减少50% KV缓存，同时保持推理性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决长推理任务中KV缓存内存开销大的问题，现有方法未能有效捕捉token重要性周期现象。

Method: 提出LazyEviction框架，包含Recurrence Interval Tracking和基于最大重复间隔的淘汰策略。

Result: 实验表明LazyEviction减少50% KV缓存，保持数学推理准确性，优于现有方法。

Conclusion: 保护周期性关键token对多步推理任务至关重要，LazyEviction有效平衡内存与性能。

Abstract: Large Language Models (LLMs) exhibit enhanced reasoning capabilities by
employing Chain-of-Thought (CoT). However, the extended reasoning sequences
introduce significant GPU memory overhead due to increased key-value (KV) cache
size, particularly in tasks requiring long reasoning sequences, such as
mathematics and programming. Existing KV cache compression methods mitigate
memory bottlenecks but struggle in long reasoning tasks. In this paper, we
analyze attention patterns in reasoning tasks and reveal a Token Importance
Recurrence phenomenon: a large proportion of tokens receive renewed attention
after multiple decoding steps, which is failed to capture by existing works and
may lead to unpredictable eviction on such periodically critical tokens. To
address this, we propose LazyEviction, a lagged KV eviction framework designed
to maintain reasoning performance while reducing KV memory. LazyEviction is an
Observation Window-based Lagged Eviction Mechanism retaining latent recurring
tokens by performing lagged evictions across decoding steps, which contains two
key components: (1) Recurrence Interval Tracking for capturing temporal
variations in token importance, and (2) an Maximum Recurrence Interval-Centric
Eviction Policy that prioritizes eviction based on tokens' recurrence patterns.
Extensive experiments demonstrate that LazyEviction reduces KV cache size by
50% while maintaining comparable accuracy on mathematics reasoning datasets,
outperforming state-of-the-art methods. Our findings highlight the importance
of preserving recurring tokens, which are critical for maintaining knowledge
continuity in multi-step reasoning tasks.

</details>


### [144] [AutoHFormer: Efficient Hierarchical Autoregressive Transformer for Time Series Prediction](https://arxiv.org/abs/2506.16001)
*Qianru Zhang,Honggang Wen,Ming Li,Dong Huang,Siu-Ming Yiu,Christian S. Jensen,Pietro Liò*

Key words: 时间序列预测, 分层自回归Transformer, 高效计算, 多尺度建模

TL;DR: AutoHFormer：一种分层自回归Transformer，通过层次化时间建模、动态窗口注意力及自适应时间编码，实现了高效、精确的时间序列预测。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 时间序列预测需要同时满足严格的时间因果关系、低计算复杂度和多尺度模式识别的需求，现有方法难以兼顾。

Method: 1) 层次化时间建模：并行处理分段数据后顺序精修；2) 动态窗口注意力：可学习的因果窗口减少计算复杂度；3) 自适应时间编码：结合固定振荡模式和可学习衰减率。

Result: AutoHFormer比PatchTST快10.76倍，内存减少6.06倍，同时在96-720步预测范围内保持准确性。

Conclusion: AutoHFormer为时间序列建模设定了高效和精确的新基准。

Abstract: Time series forecasting requires architectures that simultaneously achieve
three competing objectives: (1) strict temporal causality for reliable
predictions, (2) sub-quadratic complexity for practical scalability, and (3)
multi-scale pattern recognition for accurate long-horizon forecasting. We
introduce AutoHFormer, a hierarchical autoregressive transformer that addresses
these challenges through three key innovations: 1) Hierarchical Temporal
Modeling: Our architecture decomposes predictions into segment-level blocks
processed in parallel, followed by intra-segment sequential refinement. This
dual-scale approach maintains temporal coherence while enabling efficient
computation. 2) Dynamic Windowed Attention: The attention mechanism employs
learnable causal windows with exponential decay, reducing complexity while
preserving precise temporal relationships. This design avoids both the
anti-causal violations of standard transformers and the sequential bottlenecks
of RNN hybrids. 3) Adaptive Temporal Encoding: a novel position encoding system
is adopted to capture time patterns at multiple scales. It combines fixed
oscillating patterns for short-term variations with learnable decay rates for
long-term trends. Comprehensive experiments demonstrate that AutoHFormer 10.76X
faster training and 6.06X memory reduction compared to PatchTST on PEMS08,
while maintaining consistent accuracy across 96-720 step horizons in most of
cases. These breakthroughs establish new benchmarks for efficient and precise
time series modeling. Implementations of our method and all baselines in
hierarchical autoregressive mechanism are available at
https://github.com/lizzyhku/Autotime.

</details>


### [145] [Bridging Brain with Foundation Models through Self-Supervised Learning](https://arxiv.org/abs/2506.16009)
*Hamdi Altaheri,Fakhri Karray,Md. Milon Islam,S M Taslim Uddin Raju,Amir-Hossein Karimi*

Key words: 自监督学习、基础模型、脑信号分析、多模态学习、人工智能

TL;DR: 本文探讨了自监督学习（SSL）在基础模型（FMs）中的应用及其在脑信号分析中的潜力，综述了这一新兴领域的关键技术、挑战和未来方向。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统监督学习受限于标注神经数据的稀缺性，而SSL能够从未标注数据中学习有意义的表示，为脑信号分析提供了一种新方法。

Method: 通过SSL技术开发脑信号专用的基础模型，并将其适应于下游任务，同时探索多模态SSL框架中脑信号与其他模态的整合。

Result: 综述了常用的评估指标和基准数据集，为研究者提供了比较分析的依据。

Conclusion: 该领域面临高噪声、个体间差异等挑战，但SSL为通用脑基础模型的发展提供了重要方向。

Abstract: Foundation models (FMs), powered by self-supervised learning (SSL), have
redefined the capabilities of artificial intelligence, demonstrating
exceptional performance in domains like natural language processing and
computer vision. These advances present a transformative opportunity for brain
signal analysis. Unlike traditional supervised learning, which is limited by
the scarcity of labeled neural data, SSL offers a promising solution by
enabling models to learn meaningful representations from unlabeled data. This
is particularly valuable in addressing the unique challenges of brain signals,
including high noise levels, inter-subject variability, and low signal-to-noise
ratios. This survey systematically reviews the emerging field of bridging brain
signals with foundation models through the innovative application of SSL. It
explores key SSL techniques, the development of brain-specific foundation
models, their adaptation to downstream tasks, and the integration of brain
signals with other modalities in multimodal SSL frameworks. The review also
covers commonly used evaluation metrics and benchmark datasets that support
comparative analysis. Finally, it highlights key challenges and outlines future
research directions. This work aims to provide researchers with a structured
understanding of this rapidly evolving field and a roadmap for developing
generalizable brain foundation models powered by self-supervision.

</details>


### [146] [VRAIL: Vectorized Reward-based Attribution for Interpretable Learning](https://arxiv.org/abs/2506.16014)
*Jina Kim,Youjin Jang,Jeongjin Han*

Key words: VRAIL, 强化学习, 可解释性, 奖励塑造, 深度学习

TL;DR: VRAIL是一种基于价值的强化学习框架，通过双层结构学习可解释的权重表示，提升训练稳定性和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在解决强化学习中价值函数难以解释的问题，同时提升训练稳定性和收敛性。

Method: 采用双层框架：深度学习阶段拟合价值函数，强化学习阶段基于奖励转换优化学习。支持线性和二次形式建模。

Result: 在Taxi-v3环境中，VRAIL相比标准DQN提升了训练稳定性和收敛性，并能揭示语义上有意义的子目标。

Conclusion: VRAIL是一种通用的、模型无关的奖励塑造框架，既能提升学习效果，又能增强可解释性。

Abstract: We propose VRAIL (Vectorized Reward-based Attribution for Interpretable
Learning), a bi-level framework for value-based reinforcement learning (RL)
that learns interpretable weight representations from state features. VRAIL
consists of two stages: a deep learning (DL) stage that fits an estimated value
function using state features, and an RL stage that uses this to shape learning
via potential-based reward transformations. The estimator is modeled in either
linear or quadratic form, allowing attribution of importance to individual
features and their interactions. Empirical results on the Taxi-v3 environment
demonstrate that VRAIL improves training stability and convergence compared to
standard DQN, without requiring environment modifications. Further analysis
shows that VRAIL uncovers semantically meaningful subgoals, such as passenger
possession, highlighting its ability to produce human-interpretable behavior.
Our findings suggest that VRAIL serves as a general, model-agnostic framework
for reward shaping that enhances both learning and interpretability.

</details>


### [147] [A Scalable Factorization Approach for High-Order Structured Tensor Recovery](https://arxiv.org/abs/2506.16032)
*Zhen Qin,Michael B. Wakin,Zhihui Zhu*

Key words: 张量分解, 非凸优化, Riemannian梯度下降, Stiefel流形

TL;DR: 本文提出了一个统一框架，通过Riemannian梯度下降法解决张量分解问题，证明了在多项式复杂度下线性收敛到真实张量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 张量分解能显著减少参数数量，但优化问题高度非凸，且现有方法在收敛分析和恢复保证方面存在挑战。

Method: 利用张量分解的规范形式，约束因子正交化，应用Riemannian梯度下降法在Stiefel流形上优化。

Result: 证明了在适当初始化下，RGD能以线性速率收敛到真实张量，且复杂度和收敛速率均为多项式而非指数级。

Conclusion: 该框架统一解决了多种张量分解问题，显著提升了收敛效率和理论保证。

Abstract: Tensor decompositions, which represent an $N$-order tensor using
approximately $N$ factors of much smaller dimensions, can significantly reduce
the number of parameters. This is particularly beneficial for high-order
tensors, as the number of entries in a tensor grows exponentially with the
order. Consequently, they are widely used in signal recovery and data analysis
across domains such as signal processing, machine learning, and quantum
physics. A computationally and memory-efficient approach to these problems is
to optimize directly over the factors using local search algorithms such as
gradient descent, a strategy known as the factorization approach in matrix and
tensor optimization. However, the resulting optimization problems are highly
nonconvex due to the multiplicative interactions between factors, posing
significant challenges for convergence analysis and recovery guarantees.
  In this paper, we present a unified framework for the factorization approach
to solving various tensor decomposition problems. Specifically, by leveraging
the canonical form of tensor decompositions--where most factors are constrained
to be orthonormal to mitigate scaling ambiguity--we apply Riemannian gradient
descent (RGD) to optimize these orthonormal factors on the Stiefel manifold.
Under a mild condition on the loss function, we establish a Riemannian
regularity condition for the factorized objective and prove that RGD converges
to the ground-truth tensor at a linear rate when properly initialized. Notably,
both the initialization requirement and the convergence rate scale polynomially
rather than exponentially with $N$, improving upon existing results for Tucker
and tensor-train format tensors.

</details>


### [148] [Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal Document Understanding](https://arxiv.org/abs/2506.16035)
*Vishesh Tripathi,Tanmay Odapally,Indraneel Das,Uday Allu,Biddwan Ahmed*

Key words: Retrieval-Augmented Generation, 多模态分块, 大模态模型, PDF处理, 语义连贯性

TL;DR: 提出了一种基于大模态模型（LMMs）的多模态文档分块方法，优于传统文本分块，显著提升RAG系统的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统文本分块方法难以处理复杂文档结构、多页表格和跨页上下文依赖，限制了RAG系统的效果。

Method: 利用LMMs对PDF文档进行分批次处理，保持语义连贯性和结构完整性，支持跨批次上下文保存。

Result: 在精选PDF数据集上测试，证明其分块质量和下游RAG性能优于传统方法，尤其在处理多页表格和视觉元素时表现更佳。

Conclusion: 多模态分块方法显著提升了RAG系统的准确性和文档结构保持能力。

Abstract: Retrieval-Augmented Generation (RAG) systems have revolutionized information
retrieval and question answering, but traditional text-based chunking methods
struggle with complex document structures, multi-page tables, embedded figures,
and contextual dependencies across page boundaries. We present a novel
multimodal document chunking approach that leverages Large Multimodal Models
(LMMs) to process PDF documents in batches while maintaining semantic coherence
and structural integrity. Our method processes documents in configurable page
batches with cross-batch context preservation, enabling accurate handling of
tables spanning multiple pages, embedded visual elements, and procedural
content. We evaluate our approach on a curated dataset of PDF documents with
manually crafted queries, demonstrating improvements in chunk quality and
downstream RAG performance. Our vision-guided approach achieves better accuracy
compared to traditional vanilla RAG systems, with qualitative analysis showing
superior preservation of document structure and semantic coherence.

</details>


### [149] [From Data to Decision: Data-Centric Infrastructure for Reproducible ML in Collaborative eScience](https://arxiv.org/abs/2506.16051)
*Zhiwei Li,Carl Kesselman,Tran Huy Nguyen,Benjamin Yixing Xu,Kyle Bolo,Kimberley Yu*

Key words: 机器学习, 可重复性, 数据框架, 生命周期间溯源

TL;DR: 论文提出了一种数据为中心的框架，通过六个结构化工件解决机器学习中的可重复性问题，并在青光眼检测案例中验证了其效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 机器学习中的可重复性是一个关键挑战，尤其是在协作研究中，需要解决动态但碎片化的工作流程问题。

Method: 引入了一个生命周期感知的可重复性框架，基于六个结构化工件（数据集、特征、工作流、执行、资产和受控词汇表），以形式化数据、代码和决策之间的关系。

Result: 通过青光眼检测的临床案例展示了该框架如何支持迭代探索、提升可重复性，并记录协作决策的全生命周期溯源。

Conclusion: 该框架有效地解决了机器学习中的可重复性问题，并为协作研究提供了透明的工具支持。

Abstract: Reproducibility remains a central challenge in machine learning (ML),
especially in collaborative eScience projects where teams iterate over data,
features, and models. Current ML workflows are often dynamic yet fragmented,
relying on informal data sharing, ad hoc scripts, and loosely connected tools.
This fragmentation impedes transparency, reproducibility, and the adaptability
of experiments over time. This paper introduces a data-centric framework for
lifecycle-aware reproducibility, centered around six structured artifacts:
Dataset, Feature, Workflow, Execution, Asset, and Controlled Vocabulary. These
artifacts formalize the relationships between data, code, and decisions,
enabling ML experiments to be versioned, interpretable, and traceable over
time. The approach is demonstrated through a clinical ML use case of glaucoma
detection, illustrating how the system supports iterative exploration, improves
reproducibility, and preserves the provenance of collaborative decisions across
the ML lifecycle.

</details>


### [150] [CRIA: A Cross-View Interaction and Instance-Adapted Pre-training Framework for Generalizable EEG Representations](https://arxiv.org/abs/2506.16056)
*Puchun Liu,C. L. Philip Chen,Yubin He,Tong Zhang*

Key words: EEG、多视图学习、表征学习、跨注意力、预训练

TL;DR: 论文提出了一种名为CRIA的自适应框架，通过多视图融合和跨注意力机制提升EEG数据的表征学习效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有预训练方法仅依赖单一视图的上下文语义，无法捕捉多视角的复杂协同作用，限制了表征的表达能力和泛化性。

Method: CRIA框架利用可变长度和可变通道编码，结合跨注意力机制和注意力矩阵遮蔽策略，整合时间、频域和空间视图的EEG信号特征。

Result: 在Temple University EEG和CHB-MIT数据集上，CRIA在事件分类和异常检测上分别达到57.02%和80.03%的平衡准确率。

Conclusion: CRIA在多视图融合和跨数据集泛化方面表现出色，为EEG表征学习提供了有效解决方案。

Abstract: The difficulty of extracting deep features from EEG data and effectively
integrating information from multiple views presents significant challenges for
developing a generalizable pretraining framework for EEG representation
learning. However, most existing pre-training methods rely solely on the
contextual semantics of a single view, failing to capture the complex and
synergistic interactions among different perspectives, limiting the
expressiveness and generalization of learned representations. To address these
issues, this paper proposes CRIA, an adaptive framework that utilizes
variable-length and variable-channel coding to achieve a unified representation
of EEG data across different datasets. In this work, we define cross-view
information as the integrated representation that emerges from the interaction
among temporal, spectral, and spatial views of EEG signals. The model employs a
cross-attention mechanism to fuse temporal, spectral, and spatial features
effectively, and combines an attention matrix masking strategy based on the
information bottleneck principle with a novel viewpoint masking pre-training
scheme. Experimental results on the Temple University EEG corpus and the
CHB-MIT dataset show that CRIA outperforms existing methods with the same
pre-training conditions, achieving a balanced accuracy of 57.02% for
multi-class event classification and 80.03% for anomaly detection, highlighting
its strong generalization ability.

</details>


### [151] [Floating-Point Neural Networks Are Provably Robust Universal Approximators](https://arxiv.org/abs/2506.16065)
*Geonho Hwang,Wonyeol Lee,Yeachan Park,Sejun Park,Feras Saad*

Key words: 区间通用逼近, 浮点神经网络, 直接图像映射, 表达能力

TL;DR: 该论文提出了首个浮点神经网络的区间通用逼近定理，证明其在浮点设置下的表达能力无限制。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨神经网络在有限精度浮点数环境下的区间通用逼近能力。

Method: 提出并证明浮点神经网络的区间通用逼近定理。

Result: 证明浮点神经网络能够完美捕获任何目标函数的直接图像映射。

Conclusion: 浮点神经网络的区间通用逼近能力与实数模型有本质区别，且具有重要推论。

Abstract: The classical universal approximation (UA) theorem for neural networks
establishes mild conditions under which a feedforward neural network can
approximate a continuous function $f$ with arbitrary accuracy. A recent result
shows that neural networks also enjoy a more general interval universal
approximation (IUA) theorem, in the sense that the abstract interpretation
semantics of the network using the interval domain can approximate the direct
image map of $f$ (i.e., the result of applying $f$ to a set of inputs) with
arbitrary accuracy. These theorems, however, rest on the unrealistic assumption
that the neural network computes over infinitely precise real numbers, whereas
their software implementations in practice compute over finite-precision
floating-point numbers. An open question is whether the IUA theorem still holds
in the floating-point setting.
  This paper introduces the first IUA theorem for floating-point neural
networks that proves their remarkable ability to perfectly capture the direct
image map of any rounded target function $f$, showing no limits exist on their
expressiveness. Our IUA theorem in the floating-point setting exhibits material
differences from the real-valued setting, which reflects the fundamental
distinctions between these two computational models. This theorem also implies
surprising corollaries, which include (i) the existence of provably robust
floating-point neural networks; and (ii) the computational completeness of the
class of straight-line programs that use only floating-point additions and
multiplications for the class of all floating-point programs that halt.

</details>


### [152] [A Lightweight RL-Driven Deep Unfolding Network for Robust WMMSE Precoding in Massive MU-MIMO-OFDM Systems](https://arxiv.org/abs/2506.16072)
*Kexuan Wang,An Liu*

Key words: WMMSE, 大规模MIMO, OFDM, 深度展开, 强化学习

TL;DR: 该论文提出了一种结合随机WMMSE算法和深度展开网络的RL驱动方法（RLDDU-Net），以应对大规模MIMO-OFDM系统中不完美CSI和高计算复杂性问题，从而提升性能并降低计算开销。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决加权最小均方误差（WMMSE）预编码在大规模多用户MIMO-OFDM系统中因不完美信道状态信息（CSI）和高计算复杂度而难以实际应用的问题。

Method: 1. 开发了宽频随机WMMSE（SWMMSE）算法，以最大化不完美CSI下的遍历加权和速率（EWSR）；2. 提出了RL驱动的深度展开网络（RLDDU-Net），将SWMMSE迭代映射到网络层，利用波束域稀疏性和频域子载波相关性加速收敛。

Result: 仿真结果表明，RLDDU-Net在不完美CSI下优于现有基线，具有更高的EWSR性能和计算效率。

Conclusion: RLDDU-Net通过结合随机优化和深度学习，有效解决了实际部署中的性能与复杂度问题。

Abstract: Weighted Minimum Mean Square Error (WMMSE) precoding is widely recognized for
its near-optimal weighted sum rate performance. However, its practical
deployment in massive multi-user (MU) multiple-input multiple-output (MIMO)
orthogonal frequency-division multiplexing (OFDM) systems is hindered by the
assumption of perfect channel state information (CSI) and high computational
complexity. To address these issues, we first develop a wideband stochastic
WMMSE (SWMMSE) algorithm that iteratively maximizes the ergodic weighted
sum-rate (EWSR) under imperfect CSI. Building on this, we propose a lightweight
reinforcement learning (RL)-driven deep unfolding (DU) network (RLDDU-Net),
where each SWMMSE iteration is mapped to a network layer. Specifically, its DU
module integrates approximation techniques and leverages beam-domain sparsity
as well as frequency-domain subcarrier correlation, significantly accelerating
convergence and reducing computational overhead. Furthermore, the RL module
adaptively adjusts the network depth and generates compensation matrices to
mitigate approximation errors. Simulation results under imperfect CSI
demonstrate that RLDDU-Net outperforms existing baselines in EWSR performance
while offering superior computational and convergence efficiency.

</details>


### [153] [Joint User Priority and Power Scheduling for QoS-Aware WMMSE Precoding: A Constrained-Actor Attentive-Critic Approach](https://arxiv.org/abs/2506.16074)
*Kexuan Wang,An Liu*

Key words: 6G网络, 约束强化学习, WMMSE预编码, 能效, QoS

TL;DR: 提出了一种基于约束强化学习（CRL）的CAAC算法，动态分配用户优先级和功率，提升6G网络的能效和QoS满足度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决传统WMMSE预编码在动态用户需求和时变信道条件下缺乏灵活性的问题。

Method: 采用约束随机逐次凸逼近（CSSCA）方法优化策略，结合轻量级注意力增强Q网络评估策略更新。

Result: 仿真表明CAAC在能效和QoS满足度上优于基线方法。

Conclusion: CAAC算法有效提升了6G网络的动态适应能力。

Abstract: 6G wireless networks are expected to support diverse quality-of-service (QoS)
demands while maintaining high energy efficiency. Weighted Minimum Mean Square
Error (WMMSE) precoding with fixed user priorities and transmit power is widely
recognized for enhancing overall system performance but lacks flexibility to
adapt to user-specific QoS requirements and time-varying channel conditions. To
address this, we propose a novel constrained reinforcement learning (CRL)
algorithm, Constrained-Actor Attentive-Critic (CAAC), which uses a policy
network to dynamically allocate user priorities and power for WMMSE precoding.
Specifically, CAAC integrates a Constrained Stochastic Successive Convex
Approximation (CSSCA) method to optimize the policy, enabling more effective
handling of energy efficiency goals and satisfaction of stochastic non-convex
QoS constraints compared to traditional and existing CRL methods. Moreover,
CAAC employs lightweight attention-enhanced Q-networks to evaluate policy
updates without prior environment model knowledge. The network architecture not
only enhances representational capacity but also boosts learning efficiency.
Simulation results show that CAAC outperforms baselines in both energy
efficiency and QoS satisfaction.

</details>


### [154] [Probing the Robustness of Large Language Models Safety to Latent Perturbations](https://arxiv.org/abs/2506.16078)
*Tianle Gu,Kexin Huang,Zongqi Wang,Yixu Wang,Jie Li,Yuanqi Yao,Yang Yao,Yujiu Yang,Yan Teng,Yingchun Wang*

Key words: 安全对齐, 潜在偏移, 鲁棒性, ASA攻击, LAPT训练

TL;DR: 这篇论文探讨了现有安全对齐方法的浅层性问题，提出了一种新的探测方法ASA，并通过LAPT训练策略提升了模型的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于现有安全对齐方法仅关注表面拒绝行为，未能有效改变内部表示，导致微小的潜在偏移仍可能触发不安全响应。

Method: 引入NLL探测方法量化潜在空间敏感性，提出ASA攻击；并采用LAPT训练策略，通过注入受控扰动增强对齐鲁棒性。

Result: 实验表明LAPT在不损害模型能力的情况下显著提升了安全对齐的鲁棒性。

Conclusion: 揭示了当前对齐范式的基本缺陷，呼吁采用表示层面的训练策略超越表面行为监督。

Abstract: Safety alignment is a key requirement for building reliable Artificial
General Intelligence. Despite significant advances in safety alignment, we
observe that minor latent shifts can still trigger unsafe responses in aligned
models. We argue that this stems from the shallow nature of existing alignment
methods, which focus on surface-level refusal behaviors without sufficiently
altering internal representations. Consequently, small shifts in hidden
activations can re-trigger harmful behaviors embedded in the latent space. To
explore the robustness of safety alignment to latent perturbations, we
introduce a probing method that measures the Negative Log-Likelihood of the
original response generated by the model. This probe quantifies local
sensitivity in the latent space, serving as a diagnostic tool for identifying
vulnerable directions. Based on this signal, we construct effective jailbreak
trajectories, giving rise to the Activation Steering Attack (ASA). More
importantly, these insights offer a principled foundation for improving
alignment robustness. To this end, we introduce Layer-wise Adversarial Patch
Training~(LAPT), a fine-tuning strategy that inject controlled perturbations
into hidden representations during training. Experimental results highlight
that LAPT strengthen alignment robustness without compromising general
capabilities. Our findings reveal fundamental flaws in current alignment
paradigms and call for representation-level training strategies that move
beyond surface-level behavior supervision. Codes and results are available at
https://github.com/Carol-gutianle/LatentSafety.

</details>


### [155] [A Brain-to-Population Graph Learning Framework for Diagnosing Brain Disorders](https://arxiv.org/abs/2506.16096)
*Qianqian Liao,Wuque Cai,Hongze Sun,Dongze Liu,Duo Chen,Dezhong Yao,Daqing Guo*

Key words: 脑疾病诊断, 功能连接, 图学习, 群体建模, 可解释性

TL;DR: 论文提出了一种两阶段的脑到群体图学习框架（B2P-GL），用于脑疾病诊断，结合了大脑区域语义相似性和基于条件的群体图建模，显著提升了诊断准确性和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前基于图的脑疾病诊断方法高度依赖预定义的大脑图谱，但忽略了图谱中嵌入的丰富信息以及站点和表型变异的混杂效应。

Method: B2P-GL框架分为两个阶段：大脑表示学习阶段利用GPT-4的大脑图谱知识丰富图表示，并通过自适应节点重新分配的图注意力网络优化大脑图；群体疾病诊断阶段整合表型数据构建群体图并进行特征融合以减少混杂效应。

Result: 在ABIDE I、ADHD-200和Rest-meta-MDD数据集上，B2P-GL在预测准确性上优于现有方法，同时增强了可解释性。

Conclusion: B2P-GL为脑疾病诊断提供了可靠且个性化的方法，提升了临床应用的前景。

Abstract: Recent developed graph-based methods for diagnosing brain disorders using
functional connectivity highly rely on predefined brain atlases, but overlook
the rich information embedded within atlases and the confounding effects of
site and phenotype variability. To address these challenges, we propose a
two-stage Brain-to-Population Graph Learning (B2P-GL) framework that integrates
the semantic similarity of brain regions and condition-based population graph
modeling. In the first stage, termed brain representation learning, we leverage
brain atlas knowledge from GPT-4 to enrich the graph representation and refine
the brain graph through an adaptive node reassignment graph attention network.
In the second stage, termed population disorder diagnosis, phenotypic data is
incorporated into population graph construction and feature fusion to mitigate
confounding effects and enhance diagnosis performance. Experiments on the ABIDE
I, ADHD-200, and Rest-meta-MDD datasets show that B2P-GL outperforms
state-of-the-art methods in prediction accuracy while enhancing
interpretability. Overall, our proposed framework offers a reliable and
personalized approach to brain disorder diagnosis, advancing clinical
applicability.

</details>


### [156] [Mitigating Over-Squashing in Graph Neural Networks by Spectrum-Preserving Sparsification](https://arxiv.org/abs/2506.16110)
*Langzhang Liang,Fanchen Bu,Zixing Song,Zenglin Xu,Shirui Pan,Kijung Shin*

Key words: 图神经网络, 图重布线, 频谱保持, 过挤压, 稀疏化

TL;DR: 本文提出了一种新的图重布线方法，通过保持频谱特性的图稀疏化来缓解图神经网络中的信息传递瓶颈（过挤压问题）。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的图重布线技术往往忽略了对原始图关键特性（如频谱特性）的保留，且通常增加边数以改善连通性，这会导致计算开销和过平滑风险。

Method: 作者提出了一种频谱保持的图稀疏化方法，在增强连通性的同时保持图的稀疏性和原始频谱特性。

Result: 实验结果表明，该方法在分类准确率和拉普拉斯频谱保留方面优于基线方法。

Conclusion: 该方法有效平衡了结构瓶颈减少和图特性保留的需求。

Abstract: The message-passing paradigm of Graph Neural Networks often struggles with
exchanging information across distant nodes typically due to structural
bottlenecks in certain graph regions, a limitation known as
\textit{over-squashing}. To reduce such bottlenecks, \textit{graph rewiring},
which modifies graph topology, has been widely used. However, existing graph
rewiring techniques often overlook the need to preserve critical properties of
the original graph, e.g., \textit{spectral properties}. Moreover, many
approaches rely on increasing edge count to improve connectivity, which
introduces significant computational overhead and exacerbates the risk of
over-smoothing. In this paper, we propose a novel graph rewiring method that
leverages \textit{spectrum-preserving} graph \textit{sparsification}, for
mitigating over-squashing. Our method generates graphs with enhanced
connectivity while maintaining sparsity and largely preserving the original
graph spectrum, effectively balancing structural bottleneck reduction and graph
property preservation. Experimental results validate the effectiveness of our
approach, demonstrating its superiority over strong baseline methods in
classification accuracy and retention of the Laplacian spectrum.

</details>


### [157] [From Teacher to Student: Tracking Memorization Through Model Distillation](https://arxiv.org/abs/2506.16170)
*Simardeep Singh*

Key words: 大语言模型,知识蒸馏,记忆风险,隐私安全

TL;DR: 研究发现，通过知识蒸馏将大教师模型压缩为小学生模型，不仅能降低成本，还能显著降低记忆训练数据的风险。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨知识蒸馏如何影响模型对训练数据的记忆，以解决隐私和安全问题。

Method: 研究不同知识蒸馏方法对微调任务数据记忆的影响，比较大教师模型与小学生模型的差异。

Result: 知识蒸馏能显著减少模型记忆训练数据的风险。

Conclusion: 知识蒸馏是一种有效降低模型记忆风险的方法。

Abstract: Large language models (LLMs) are known to memorize parts of their training
data, raising important concerns around privacy and security. While previous
research has focused on studying memorization in pre-trained models, much less
is known about how knowledge distillation (KD) affects memorization.In this
study, we explore how different KD methods influence the memorization of
fine-tuned task data when a large teacher model is distilled into smaller
student variants.This study demonstrates that distilling a larger teacher
model, fine-tuned on a dataset, into a smaller variant not only lowers
computational costs and model size but also significantly reduces the
memorization risks compared to standard fine-tuning approaches.

</details>


### [158] [Hallucination Level of Artificial Intelligence Whisperer: Case Speech Recognizing Pantterinousut Rap Song](https://arxiv.org/abs/2506.16174)
*Ismo Horppu,Frederick Ayala,Erlin Gulbenkoglu*

Key words: 芬兰语,语音转文本,AI翻译,说唱音乐

TL;DR: 论文探讨了芬兰语说唱歌曲的文本翻译挑战，比较了Faster Whisperer算法和YouTube的语音转文字功能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 芬兰语是一种复杂的语言，艺术家使用时发音和含义更难理解，因此测试AI在此场景下的表现具有趣味性和挑战性。

Method: 使用Faster Whisperer算法和YouTube语音转文字功能翻译芬兰说唱歌曲，并与由Mc Timo创作的原始歌词对比。

Result: AI的幻觉和误听水平将通过与原歌词的对比来测量。

Conclusion: 研究为AI在复杂语言艺术表达中的应用提供了有趣的测试案例。

Abstract: All languages are peculiar. Some of them are considered more challenging to
understand than others. The Finnish Language is known to be a complex language.
Also, when languages are used by artists, the pronunciation and meaning might
be more tricky to understand. Therefore, we are putting AI to a fun, yet
challenging trial: translating a Finnish rap song to text. We will compare the
Faster Whisperer algorithm and YouTube's internal speech-to-text functionality.
The reference truth will be Finnish rap lyrics, which the main author's little
brother, Mc Timo, has written. Transcribing the lyrics will be challenging
because the artist raps over synth music player by Syntikka Janne. The
hallucination level and mishearing of AI speech-to-text extractions will be
measured by comparing errors made against the original Finnish lyrics. The
error function is informal but still works for our case.

</details>


### [159] [Efficient and Privacy-Preserving Soft Prompt Transfer for LLMs](https://arxiv.org/abs/2506.16196)
*Xun Wang,Jing Xu,Franziska Boenisch,Michael Backes,Christopher A. Choquette-Choo,Adam Dziedzic*

Key words: 软提示, 大语言模型, 知识蒸馏, 隐私保护, 效率

TL;DR: POST框架通过在小模型上本地调优软提示并转移到大型语言模型（LLM），解决了隐私和效率问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 软提示虽然高效，但受限于具体LLM且调优成本高，同时隐私问题突出。

Method: POST结合知识蒸馏、本地调优和差分隐私，将小模型上的软提示转移到大型LLM。

Result: 实验表明POST降低了计算成本，保护了隐私，且提示转移效果良好。

Conclusion: POST为软提示调优提供了一种高效、隐私安全的解决方案。

Abstract: Prompting has become a dominant paradigm for adapting large language models
(LLMs). While discrete (textual) prompts are widely used for their
interpretability, soft (parameter) prompts have recently gained traction in
APIs. This is because they can encode information from more training samples
while minimizing the user's token usage, leaving more space in the context
window for task-specific input. However, soft prompts are tightly coupled to
the LLM they are tuned on, limiting their generalization to other LLMs. This
constraint is particularly problematic for efficiency and privacy: (1) tuning
prompts on each LLM incurs high computational costs, especially as LLMs
continue to grow in size. Additionally, (2) when the LLM is hosted externally,
soft prompt tuning often requires sharing private data with the LLM provider.
For instance, this is the case with the NVIDIA NeMo API. To address these
issues, we propose POST (Privacy Of Soft prompt Transfer), a framework that
enables private tuning of soft prompts on a small model and subsequently
transfers these prompts to a larger LLM. POST uses knowledge distillation to
derive a small model directly from the large LLM to improve prompt
transferability, tunes the soft prompt locally, optionally with differential
privacy guarantees, and transfers it back to the larger LLM using a small
public dataset. Our experiments show that POST reduces computational costs,
preserves privacy, and effectively transfers high-utility soft prompts.

</details>


### [160] [From Pixels to CSI: Distilling Latent Dynamics For Efficient Wireless Resource Management](https://arxiv.org/abs/2506.16216)
*Charbel Bou Chaaya,Abanoub M. Girgis,Mehdi Bennis*

Key words: 无线通信, 机器学习, 联合嵌入预测架构, 深度强化学习, 资源优化

TL;DR: 本文提出了一种新颖的机器学习技术，通过联合嵌入预测架构（JEPA）优化无线通信系统中的资源管理，减少传输功率并保持控制性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在优化远程控制器与设备之间的无线资源管理，同时不牺牲控制任务的性能。

Method: 使用两个耦合的JEPA网络分别建模控制动态和无线传播环境，并结合深度强化学习算法制定控制策略。

Result: 模拟结果显示，传输功率减少了50%以上，同时保持与未优化无线资源管理基准方法相当的控制性能。

Conclusion: 该方法在减少资源消耗的同时，有效维持了系统的控制性能。

Abstract: In this work, we aim to optimize the radio resource management of a
communication system between a remote controller and its device, whose state is
represented through image frames, without compromising the performance of the
control task. We propose a novel machine learning (ML) technique to jointly
model and predict the dynamics of the control system as well as the wireless
propagation environment in latent space. Our method leverages two coupled
joint-embedding predictive architectures (JEPAs): a control JEPA models the
control dynamics and guides the predictions of a wireless JEPA, which captures
the dynamics of the device's channel state information (CSI) through
cross-modal conditioning. We then train a deep reinforcement learning (RL)
algorithm to derive a control policy from latent control dynamics and a power
predictor to estimate scheduling intervals with favorable channel conditions
based on latent CSI representations. As such, the controller minimizes the
usage of radio resources by utilizing the coupled JEPA networks to imagine the
device's trajectory in latent space. We present simulation results on synthetic
multimodal data and show that our proposed approach reduces transmit power by
over 50% while maintaining control performance comparable to baseline methods
that do not account for wireless optimization.

</details>


### [161] [Think Global, Act Local: Bayesian Causal Discovery with Language Models in Sequential Data](https://arxiv.org/abs/2506.16234)
*Prakhar Verma,David Arbour,Sunav Choudhary,Harshita Chopra,Arno Solin,Atanu R. Sinha*

Key words: 因果发现、贝叶斯框架、语言模型、批次数据、PAG

TL;DR: BLANCE是一个混合贝叶斯框架，结合了批次数据和语言模型生成的专家知识，通过从DAG转向PAG表示，提高了因果发现的准确性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决观测数据因果发现中数据分批到达和专家知识稀缺的问题，同时应对语言模型的噪声和偏见。

Method: 采用贝叶斯框架，将批次数据与LM生成的噪声知识结合，并利用PAG表示处理模糊性，通过序贯优化方案查询最有信息量的边。

Result: 在不同数据集上，BLANCE在结构准确性上优于先前方法，且对LM噪声具有鲁棒性，适用于贝叶斯参数估计。

Conclusion: BLANCE通过创新框架和优化策略，显著提升了因果发现的性能，并展示了LM知识在实际数据中的潜在价值。

Abstract: Causal discovery from observational data typically assumes full access to
data and availability of domain experts. In practice, data often arrive in
batches, and expert knowledge is scarce. Language Models (LMs) offer a
surrogate but come with their own issues-hallucinations, inconsistencies, and
bias. We present BLANCE (Bayesian LM-Augmented Causal Estimation)-a hybrid
Bayesian framework that bridges these gaps by adaptively integrating sequential
batch data with LM-derived noisy, expert knowledge while accounting for both
data-induced and LM-induced biases. Our proposed representation shift from
Directed Acyclic Graph (DAG) to Partial Ancestral Graph (PAG) accommodates
ambiguities within a coherent Bayesian framework, allowing grounding the global
LM knowledge in local observational data. To guide LM interaction, we use a
sequential optimization scheme that adaptively queries the most informative
edges. Across varied datasets, BLANCE outperforms prior work in structural
accuracy and extends to Bayesian parameter estimation, showing robustness to LM
noise.

</details>


### [162] [Active MRI Acquisition with Diffusion Guided Bayesian Experimental Design](https://arxiv.org/abs/2506.16237)
*Jacopo Iollo,Geoffroy Oudoumanessah,Carole Lartizien,Michel Dojat,Florence Forbes*

Key words: MRI, 贝叶斯实验设计, 扩散模型, 图像重建, 自适应采样

TL;DR: 论文提出一种基于贝叶斯实验设计（BED）的新方法，通过自适应选择最有信息量的测量来加速MRI扫描，同时保持图像质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决MRI扫描时间长与图像质量之间的矛盾，提出一种自适应且任务依赖的测量选择方法。

Method: 采用顺序贝叶斯实验设计，结合扩散模型处理高维图像，并通过梯度优化选择子采样模式。

Result: 实验证明该方法不仅优化了图像重建，还能提升相关图像分析任务性能。

Conclusion: 该方法具有多功能性和高效性，适用于多种MRI采集任务。

Abstract: A key challenge in maximizing the benefits of Magnetic Resonance Imaging
(MRI) in clinical settings is to accelerate acquisition times without
significantly degrading image quality. This objective requires a balance
between under-sampling the raw k-space measurements for faster acquisitions and
gathering sufficient raw information for high-fidelity image reconstruction and
analysis tasks. To achieve this balance, we propose to use sequential Bayesian
experimental design (BED) to provide an adaptive and task-dependent selection
of the most informative measurements. Measurements are sequentially augmented
with new samples selected to maximize information gain on a posterior
distribution over target images. Selection is performed via a gradient-based
optimization of a design parameter that defines a subsampling pattern. In this
work, we introduce a new active BED procedure that leverages diffusion-based
generative models to handle the high dimensionality of the images and employs
stochastic optimization to select among a variety of patterns while meeting the
acquisition process constraints and budget. So doing, we show how our setting
can optimize, not only standard image reconstruction, but also any associated
image analysis task. The versatility and performance of our approach are
demonstrated on several MRI acquisitions.

</details>


### [163] [Synthetic ALS-EEG Data Augmentation for ALS Diagnosis Using Conditional WGAN with Weight Clipping](https://arxiv.org/abs/2506.16243)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Key words: ALS, EEG, CWGAN, 数据增强, 类别不平衡

TL;DR: 利用条件Wasserstein生成对抗网络（CWGAN）生成ALS患者的合成EEG信号，解决数据稀缺和类别不平衡问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: ALS患者高质量EEG数据稀缺且类别不平衡，难以训练可靠的机器学习分类器。

Method: 使用CWGAN学习和生成合成ALS EEG信号，预处理和标准化数据，训练模型并评估生成信号的真实性。

Result: 合成EEG信号与真实ALS EEG模式接近，训练收敛，信号具有潜在数据增强价值。

Conclusion: 合成信号能缓解类别不平衡问题，可能提升ALS检测准确性，促进数据共享和诊断模型改进。

Abstract: Amyotrophic Lateral Sclerosis (ALS) is a rare neurodegenerative disease, and
high-quality EEG data from ALS patients are scarce. This data scarcity, coupled
with severe class imbalance between ALS and healthy control recordings, poses a
challenge for training reliable machine learning classifiers. In this work, we
address these issues by generating synthetic EEG signals for ALS patients using
a Conditional Wasserstein Generative Adversarial Network (CWGAN). We train
CWGAN on a private EEG dataset (ALS vs. non-ALS) to learn the distribution of
ALS EEG signals and produce realistic synthetic samples. We preprocess and
normalize EEG recordings, and train a CWGAN model to generate synthetic ALS
signals. The CWGAN architecture and training routine are detailed, with key
hyperparameters chosen for stable training. Qualitative evaluation of generated
signals shows that they closely mimic real ALS EEG patterns. The CWGAN training
converged with generator and discriminator loss curves stabilizing, indicating
successful learning. The synthetic EEG signals appear realistic and have
potential use as augmented data for training classifiers, helping to mitigate
class imbalance and improve ALS detection accuracy. We discuss how this
approach can facilitate data sharing and enhance diagnostic models.

</details>


### [164] [Optimal Online Bookmaking for Any Number of Outcomes](https://arxiv.org/abs/2506.16253)
*Hadar Tal,Oron Sabag*

Key words: 在线博彩, 动态规划, Bellman-Pareto边界, Hermite多项式, 多目标优化

TL;DR: 研究在线博彩问题，通过动态调整赔率最大化利润并减少潜在损失，证明最优损失为多项式最大根，开发高效算法优化策略。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨博彩公司如何在动态调整赔率的过程中平衡利润与风险，保障财务安全。

Method: 利用动态规划和多目标优化框架，构建Bellman-Pareto边界，开发高效算法计算最优策略。

Result: 证明最优损失为多项式最大根，算法在面对最优或次优赌徒时均能实现最佳损失控制。

Conclusion: 博彩公司可通过策略优化实现公平性与财务安全性的平衡，揭示了博彩遗憾与Hermite多项式的关系。

Abstract: We study the Online Bookmaking problem, where a bookmaker dynamically updates
betting odds on the possible outcomes of an event. In each betting round, the
bookmaker can adjust the odds based on the cumulative betting behavior of
gamblers, aiming to maximize profit while mitigating potential loss. We show
that for any event and any number of betting rounds, in a worst-case setting
over all possible gamblers and outcome realizations, the bookmaker's optimal
loss is the largest root of a simple polynomial. Our solution shows that
bookmakers can be as fair as desired while avoiding financial risk, and the
explicit characterization reveals an intriguing relation between the
bookmaker's regret and Hermite polynomials. We develop an efficient algorithm
that computes the optimal bookmaking strategy: when facing an optimal gambler,
the algorithm achieves the optimal loss, and in rounds where the gambler is
suboptimal, it reduces the achieved loss to the optimal opportunistic loss, a
notion that is related to subgame perfect Nash equilibrium. The key technical
contribution to achieve these results is an explicit characterization of the
Bellman-Pareto frontier, which unifies the dynamic programming updates for
Bellman's value function with the multi-criteria optimization framework of the
Pareto frontier in the context of vector repeated games.

</details>


### [165] [Next-Token Prediction Should be Ambiguity-Sensitive: A Meta-Learning Perspective](https://arxiv.org/abs/2506.16288)
*Leo Gagnon,Eric Elmoznino,Sarthak Mittal,Tom Marty,Tejas Kasetty,Dhanya Sridhar,Guillaume Lajoie*

Key words: 自回归模型、贝叶斯推理、蒙特卡洛方法、高模糊性预测、认知科学

TL;DR: 该论文探讨了自回归基础模型在快速适应能力方面的局限性，尤其是在高模糊性预测下的挑战，并提出了一种基于蒙特卡洛的方法改进模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究动机源自认知科学，认为在高模糊性条件下，启发式或信息搜索策略比详尽推理更有效，从而提出改进模型的预测能力。

Method: 提出MetaHMM基准测试和蒙特卡洛预测方法，将预训练模型转化为能够分离任务推理与令牌预测的工具。

Result: 实验显示，Transformers在高模糊性预测中表现不佳，而蒙特卡洛方法在模糊上下文中有显著改进。

Conclusion: 虽然蒙特卡洛方法在高模糊性预测中表现良好，但仍存在挑战，需要进一步研究。

Abstract: The rapid adaptation ability of auto-regressive foundation models is often
attributed to the diversity of their pre-training data. This is because, from a
Bayesian standpoint, minimizing prediction error in such settings requires
integrating over all plausible latent hypotheses consistent with observations.
While this behavior is desirable in principle, it often proves too ambitious in
practice: under high ambiguity, the number of plausible latent alternatives
makes Bayes-optimal prediction computationally intractable. Cognitive science
has long recognized this limitation, suggesting that under such conditions,
heuristics or information-seeking strategies are preferable to exhaustive
inference. Translating this insight to next-token prediction, we hypothesize
that low- and high-ambiguity predictions pose different computational demands,
making ambiguity-agnostic next-token prediction a detrimental inductive bias.
To test this, we introduce MetaHMM, a synthetic sequence meta-learning
benchmark with rich compositional structure and a tractable Bayesian oracle. We
show that Transformers indeed struggle with high-ambiguity predictions across
model sizes. Motivated by cognitive theories, we propose a method to convert
pre-trained models into Monte Carlo predictors that decouple task inference
from token prediction. Preliminary results show substantial gains in ambiguous
contexts through improved capacity allocation and test-time scalable inference,
though challenges remain.

</details>


### [166] [Optimizing Multilingual Text-To-Speech with Accents & Emotions](https://arxiv.org/abs/2506.16310)
*Pranav Pawar,Akshansh Dwivedi,Jenish Boricha,Himanshu Gohil,Aditya Dubey*

Key words: text-to-speech, multilingual accent, emotion modelling, Hindi, Indian English

TL;DR: 本文提出了一种新的TTS架构，结合多尺度情感建模，特别针对印地语和印度英语口音，显著提升了口音准确性和情感识别能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前TTS系统在多语言环境下（尤其是印地语）生成具有正确口音和上下文相关情感的语音仍存在困难，这主要由于文化差异。

Method: 扩展Parler-TTS模型，集成语言特定音素对齐的混合编码器-解码器架构、文化敏感的情感嵌入层，以及动态口音代码切换技术。

Result: 口音准确性提升23.7%（WER从15.4%降至11.8%），情感识别准确率达85.3%，用户评价MOS为4.2/5。

Conclusion: 该系统通过展示可扩展的口音-情感解耦，使跨语言合成更加可行，尤其适用于南亚教育科技和辅助软件。

Abstract: State-of-the-art text-to-speech (TTS) systems realize high naturalness in
monolingual environments, synthesizing speech with correct multilingual accents
(especially for Indic languages) and context-relevant emotions still poses
difficulty owing to cultural nuance discrepancies in current frameworks. This
paper introduces a new TTS architecture integrating accent along with
preserving transliteration with multi-scale emotion modelling, in particularly
tuned for Hindi and Indian English accent. Our approach extends the Parler-TTS
model by integrating A language-specific phoneme alignment hybrid
encoder-decoder architecture, and culture-sensitive emotion embedding layers
trained on native speaker corpora, as well as incorporating a dynamic accent
code switching with residual vector quantization. Quantitative tests
demonstrate 23.7% improvement in accent accuracy (Word Error Rate reduction
from 15.4% to 11.8%) and 85.3% emotion recognition accuracy from native
listeners, surpassing METTS and VECL-TTS baselines. The novelty of the system
is that it can mix code in real time - generating statements such as "Namaste,
let's talk about <Hindi phrase>" with uninterrupted accent shifts while
preserving emotional consistency. Subjective evaluation with 200 users reported
a mean opinion score (MOS) of 4.2/5 for cultural correctness, much better than
existing multilingual systems (p<0.01). This research makes cross-lingual
synthesis more feasible by showcasing scalable accent-emotion disentanglement,
with direct application in South Asian EdTech and accessibility software.

</details>


### [167] [Improved Exploration in GFlownets via Enhanced Epistemic Neural Networks](https://arxiv.org/abs/2506.16313)
*Sajan Muhammad,Salem Lahlou*

Key words: GFlowNets, 认知神经网络, 不确定性量化, 联合预测

TL;DR: 提出了一种名为ENN-GFN-Enhanced的算法，通过结合认知神经网络（ENN）和改进的GFlowNets架构，提升了联合预测和不确定性量化的效率，从而优化探索和最优轨迹识别。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决GFlowNets训练中高效识别最优轨迹的问题，尤其是在未充分学习奖励分布区域中实现不确定性驱动的探索。

Method: 将认知神经网络（ENN）与传统GFlowNets架构结合，提出ENN-GFN-Enhanced算法，增强联合预测能力和不确定性量化。

Result: 在网格环境和结构化序列生成任务中，ENN-GFN-Enhanced算法相较于基线方法表现出更高的效能和效率。

Conclusion: ENN-GFN-Enhanced算法通过改进探索策略，显著提升了GFlowNets的性能。

Abstract: Efficiently identifying the right trajectories for training remains an open
problem in GFlowNets. To address this, it is essential to prioritize
exploration in regions of the state space where the reward distribution has not
been sufficiently learned. This calls for uncertainty-driven exploration, in
other words, the agent should be aware of what it does not know. This attribute
can be measured by joint predictions, which are particularly important for
combinatorial and sequential decision problems. In this research, we integrate
epistemic neural networks (ENN) with the conventional architecture of GFlowNets
to enable more efficient joint predictions and better uncertainty
quantification, thereby improving exploration and the identification of optimal
trajectories. Our proposed algorithm, ENN-GFN-Enhanced, is compared to the
baseline method in GFlownets and evaluated in grid environments and structured
sequence generation in various settings, demonstrating both its efficacy and
efficiency.

</details>


### [168] [Signatures to help interpretability of anomalies](https://arxiv.org/abs/2506.16314)
*Emmanuel Gangler,Emille E. O. Ishida,Matwey V. Kornilov,Vladimir Korolev,Anastasia Lavrukhina,Konstantin Malanchev,Maria V. Pruzhinskaya,Etienne Russeil,Timofey Semenikhin,Sreevarsha Sreejith,Alina A. Volnova*

Key words: 机器学习、异常检测、可解释性、异常签名

TL;DR: 为了改善机器学习在异常检测中的可解释性，本文提出了‘异常签名’的概念，旨在通过突出哪些特征影响了决策来帮助理解异常。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 机器学习常被视为黑盒，尤其在异常检测中，用户难以理解模型为何将某些事件标记为异常。这促使研究者提出一种方法以增强模型的可解释性。

Method: 提出了‘异常签名’的概念，通过突出显示对决策有贡献的特征，帮助用户理解模型的异常检测结果。

Result: 异常签名为用户提供了一种直观理解异常检测结果的方式，增强了模型的可解释性。

Conclusion: 异常签名是提升机器学习异常检测可解释性的有效工具，有助于用户更好地理解模型决策。

Abstract: Machine learning is often viewed as a black box when it comes to
understanding its output, be it a decision or a score. Automatic anomaly
detection is no exception to this rule, and quite often the astronomer is left
to independently analyze the data in order to understand why a given event is
tagged as an anomaly. We introduce here idea of anomaly signature, whose aim is
to help the interpretability of anomalies by highlighting which features
contributed to the decision.

</details>


### [169] [Bayesian Optimization over Bounded Domains with the Beta Product Kernel](https://arxiv.org/abs/2506.16316)
*Huy Hoang Nguyen,Han Zhou,Matthew B. Blaschko,Aleksei Tiulpin*

Key words: 贝叶斯优化, 高斯过程, Beta核, 有界域, 非平稳核

TL;DR: 论文提出了基于Beta分布密度函数的Beta核，用于在高斯过程贝叶斯优化中处理有界域问题，并展示了其在多种任务中的优越性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的Mat\'ern和RBF核函数未考虑函数定义域的限制，导致在有界域问题中表现不佳。

Method: 引入基于Beta分布密度函数的非平稳Beta核，统计分析其谱性质并验证其指数特征衰减率。

Result: 实验表明，Beta核在单位超立方体的边界或顶点附近的函数建模中表现稳健，优于Mat\'ern和RBF等核函数。

Conclusion: Beta核为有界域的高斯过程贝叶斯优化提供了更有效的解决方案。

Abstract: Bayesian optimization with Gaussian processes (GP) is commonly used to
optimize black-box functions. The Mat\'ern and the Radial Basis Function (RBF)
covariance functions are used frequently, but they do not make any assumptions
about the domain of the function, which may limit their applicability in
bounded domains. To address the limitation, we introduce the Beta kernel, a
non-stationary kernel induced by a product of Beta distribution density
functions. Such a formulation allows our kernel to naturally model functions on
bounded domains. We present statistical evidence supporting the hypothesis that
the kernel exhibits an exponential eigendecay rate, based on empirical analyses
of its spectral properties across different settings. Our experimental results
demonstrate the robustness of the Beta kernel in modeling functions with optima
located near the faces or vertices of the unit hypercube. The experiments show
that our kernel consistently outperforms a wide range of kernels, including the
well-known Mat\'ern and RBF, in different problems, including synthetic
function optimization and the compression of vision and language models.

</details>


### [170] [Watermarking Autoregressive Image Generation](https://arxiv.org/abs/2506.16349)
*Nikola Jovanović,Ismail Labiad,Tomáš Souček,Martin Vechev,Pierre Fernandez*

Key words: 水印技术, 自回归模型, 图像生成, 令牌级别, RCC问题

TL;DR: 本文提出了一种在自回归图像生成模型中实现令牌级别水印的方法，解决了重新标记化（RCC）问题，并通过实验验证了其可靠性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 生成模型的输出水印技术对于追踪其来源具有重要意义，但此前在自回归图像生成模型中尚未实现令牌级别的水印。

Method: 通过调整语言模型水印技术，引入定制化的令牌化-反令牌化微调程序和同步水印层，解决了RCC问题并增强了水印的鲁棒性。

Result: 实验结果表明，该方法能够实现可靠且鲁棒的水印检测，并提供了理论支持的p值。

Conclusion: 该研究为自回归图像生成模型的水印技术提供了有效解决方案，具有实际应用价值。

Abstract: Watermarking the outputs of generative models has emerged as a promising
approach for tracking their provenance. Despite significant interest in
autoregressive image generation models and their potential for misuse, no prior
work has attempted to watermark their outputs at the token level. In this work,
we present the first such approach by adapting language model watermarking
techniques to this setting. We identify a key challenge: the lack of reverse
cycle-consistency (RCC), wherein re-tokenizing generated image tokens
significantly alters the token sequence, effectively erasing the watermark. To
address this and to make our method robust to common image transformations,
neural compression, and removal attacks, we introduce (i) a custom
tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a
complementary watermark synchronization layer. As our experiments demonstrate,
our approach enables reliable and robust watermark detection with theoretically
grounded p-values.

</details>


### [171] [Data-Driven Policy Mapping for Safe RL-based Energy Management Systems](https://arxiv.org/abs/2506.16352)
*Theo Zangato,Aomar Osmani,Pegah Alizadeh*

Key words: 建筑能源管理，强化学习，聚类，预测，LSTM

TL;DR: 提出了一种基于强化学习的三步建筑能源管理系统（BEMS），通过聚类、预测和约束策略学习，实现可扩展、适应性强且安全的能源管理，可降低运营成本15%。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 全球能源需求增加和可再生能源整合复杂性提升，使得建筑成为可持续能源管理的核心。需解决可扩展性、适应性和安全性挑战。

Method: 1. 聚类非可移负载以识别通用消费模式；2. 集成基于LSTM的预测模块；3. 使用领域知识约束策略学习确保安全。

Result: 在真实数据上，运营成本降低15%，环境性能稳定，可快速优化新建筑并适应电价变化。

Conclusion: 该框架实现了可扩展、稳健且经济高效的建筑能源管理。

Abstract: Increasing global energy demand and renewable integration complexity have
placed buildings at the center of sustainable energy management. We present a
three-step reinforcement learning(RL)-based Building Energy Management System
(BEMS) that combines clustering, forecasting, and constrained policy learning
to address scalability, adaptability, and safety challenges. First, we cluster
non-shiftable load profiles to identify common consumption patterns, enabling
policy generalization and transfer without retraining for each new building.
Next, we integrate an LSTM based forecasting module to anticipate future
states, improving the RL agents' responsiveness to dynamic conditions. Lastly,
domain-informed action masking ensures safe exploration and operation,
preventing harmful decisions. Evaluated on real-world data, our approach
reduces operating costs by up to 15% for certain building types, maintains
stable environmental performance, and quickly classifies and optimizes new
buildings with limited data. It also adapts to stochastic tariff changes
without retraining. Overall, this framework delivers scalable, robust, and
cost-effective building energy management.

</details>


### [172] [Classification of Cattle Behavior and Detection of Heat (Estrus) using Sensor Data](https://arxiv.org/abs/2506.16380)
*Druva Dhakshinamoorthy,Avikshit Jha,Sabyasachi Majumdar,Devdulal Ghosh,Ranjita Chakraborty,Hena Ray*

Key words: 牛行为监测,发情检测,机器学习,传感器,低成本

TL;DR: 该论文提出了一种基于传感器数据和机器学习的新型系统，用于监测牛的行为并检测发情期。通过低成本蓝牙颈圈传感器采集实时数据，并结合多种机器学习模型实现高精度分类和检测。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决传统畜牧监测的低效问题，提供一种低成本、高精度的解决方案，特别适用于资源有限的环境。

Method: 设计蓝牙颈圈传感器采集牛的行为数据，结合同步CCTV视频标注数据集。使用SVM、RF、CNN进行行为分类，LSTM模型用于发情检测和异常检测。

Result: 行为分类准确率达93%，发情检测准确率达96%。

Conclusion: 该系统为精准畜牧监测提供了一种可扩展且易于实现的解决方案。

Abstract: This paper presents a novel system for monitoring cattle behavior and
detecting estrus (heat) periods using sensor data and machine learning. We
designed and deployed a low-cost Bluetooth-based neck collar equipped with
accelerometer and gyroscope sensors to capture real-time behavioral data from
real cows, which was synced to the cloud. A labeled dataset was created using
synchronized CCTV footage to annotate behaviors such as feeding, rumination,
lying, and others. We evaluated multiple machine learning models -- Support
Vector Machines (SVM), Random Forests (RF), and Convolutional Neural Networks
(CNN) -- for behavior classification. Additionally, we implemented a Long
Short-Term Memory (LSTM) model for estrus detection using behavioral patterns
and anomaly detection. Our system achieved over 93% behavior classification
accuracy and 96% estrus detection accuracy on a limited test set. The approach
offers a scalable and accessible solution for precision livestock monitoring,
especially in resource-constrained environments.

</details>


### [173] [State-Space Kolmogorov Arnold Networks for Interpretable Nonlinear System Identification](https://arxiv.org/abs/2506.16392)
*Gonçalo Granjal Cruz,Balazs Renczes,Mark C Runacres,Jan Decuyper*

Key words: SS-KAN, 系统辨识, 可解释性, 非线性系统, 状态空间模型

TL;DR: 论文提出SS-KAN模型，通过结合状态空间框架和Kolmogorov-Arnold网络，提升黑盒系统辨识的可解释性，以精度为代价实现非线性动态的可视化。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决黑盒系统辨识模型缺乏可解释性的问题，尤其关注非线性系统动态的理解。

Method: 将Kolmogorov-Arnold网络集成到状态空间框架中，采用稀疏促进正则化，并可视化学习到的单变量函数。

Result: 在Silverbox和Wiener-Hammerstein基准测试中，SS-KAN展现出更高的可解释性，但精度略低于现有黑盒模型。

Conclusion: SS-KAN在精度与可解释性间取得平衡，为非线性系统辨识提供了有前景的方案。

Abstract: While accurate, black-box system identification models lack interpretability
of the underlying system dynamics. This paper proposes State-Space
Kolmogorov-Arnold Networks (SS-KAN) to address this challenge by integrating
Kolmogorov-Arnold Networks within a state-space framework. The proposed model
is validated on two benchmark systems: the Silverbox and the Wiener-Hammerstein
benchmarks. Results show that SS-KAN provides enhanced interpretability due to
sparsity-promoting regularization and the direct visualization of its learned
univariate functions, which reveal system nonlinearities at the cost of
accuracy when compared to state-of-the-art black-box models, highlighting
SS-KAN as a promising approach for interpretable nonlinear system
identification, balancing accuracy and interpretability of nonlinear system
dynamics.

</details>


### [174] [GoalLadder: Incremental Goal Discovery with Vision-Language Models](https://arxiv.org/abs/2506.16396)
*Alexey Zakharov,Shimon Whiteson*

Key words: 强化学习, 视觉语言模型, 自然语言处理, 奖励函数, 机器人操作

TL;DR: GoalLadder利用视觉语言模型从单条语言指令中训练强化学习代理，通过逐步发现任务进展的中间状态，减少噪声反馈并提升成功率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 从自然语言中提取奖励并用于视觉环境中的强化学习任务是一个挑战，现有方法存在反馈需求量大或奖励函数不准确的问题。

Method: GoalLadder通过查询视觉语言模型识别任务进展状态，并使用ELO评级系统排名目标状态，以减少噪声反馈的影响。代理通过学习嵌入空间缩小与目标的距离完成任务。

Result: 在经典控制和机器人操作环境中，GoalLadder平均最终成功率约为95%，显著优于最佳竞争对手的45%。

Conclusion: GoalLadder通过结合视觉语言模型和强化学习，显著提升了从语言指令训练任务的性能，减少了准确的反馈需求。

Abstract: Natural language can offer a concise and human-interpretable means of
specifying reinforcement learning (RL) tasks. The ability to extract rewards
from a language instruction can enable the development of robotic systems that
can learn from human guidance; however, it remains a challenging problem,
especially in visual environments. Existing approaches that employ large,
pretrained language models either rely on non-visual environment
representations, require prohibitively large amounts of feedback, or generate
noisy, ill-shaped reward functions. In this paper, we propose a novel method,
$\textbf{GoalLadder}$, that leverages vision-language models (VLMs) to train RL
agents from a single language instruction in visual environments. GoalLadder
works by incrementally discovering states that bring the agent closer to
completing a task specified in natural language. To do so, it queries a VLM to
identify states that represent an improvement in agent's task progress and to
rank them using pairwise comparisons. Unlike prior work, GoalLadder does not
trust VLM's feedback completely; instead, it uses it to rank potential goal
states using an ELO-based rating system, thus reducing the detrimental effects
of noisy VLM feedback. Over the course of training, the agent is tasked with
minimising the distance to the top-ranked goal in a learned embedding space,
which is trained on unlabelled visual data. This key feature allows us to
bypass the need for abundant and accurate feedback typically required to train
a well-shaped reward function. We demonstrate that GoalLadder outperforms
existing related methods on classic control and robotic manipulation
environments with the average final success rate of $\sim$95% compared to only
$\sim$45% of the best competitor.

</details>


### [175] [Generating Directed Graphs with Dual Attention and Asymmetric Encoding](https://arxiv.org/abs/2506.16404)
*Alba Carballo-Castro,Manuel Madeira,Yiming Qin,Dorina Thanou,Pascal Frossard*

Key words: 有向图生成、离散流匹配、双注意力机制、位置编码、基准测试

TL;DR: 论文提出了一种名为Directo的生成模型，专注于解决有向图生成的两大挑战：依赖空间大和缺乏标准化评估。该方法结合了特定的位置编码、双注意力机制和离散生成框架，并在多样数据集上表现出色，为未来研究奠定了基础。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 有向图在多个领域（如生物学、交通、社交网络）中有广泛应用，但其生成任务因依赖空间大和缺乏标准评估而进展缓慢。论文旨在解决这些问题。

Method: 提出了Directo模型，结合了专为有向关系设计的位置编码、双注意力机制（捕获入度和出度依赖）和离散流匹配框架。

Result: 在合成和真实数据集上的实验表明，Directo在多样化设置中表现优异，甚至可与针对特定图类（如有向无环图）的专用模型竞争。

Conclusion: Directo提供了一种有效且通用的有向图生成方法，为未来研究提供了坚实基础。

Abstract: Directed graphs naturally model systems with asymmetric, ordered
relationships, essential to applications in biology, transportation, social
networks, and visual understanding. Generating such graphs enables tasks such
as simulation, data augmentation and novel instance discovery; however,
directed graph generation remains underexplored. We identify two key factors
limiting progress in this direction: first, modeling edge directionality
introduces a substantially larger dependency space, making the underlying
distribution harder to learn; second, the absence of standardized benchmarks
hinders rigorous evaluation. Addressing the former requires more expressive
models that are sensitive to directional topologies. We propose Directo, the
first generative model for directed graphs built upon the discrete flow
matching framework. Our approach combines: (i) principled positional encodings
tailored to asymmetric pairwise relations, (ii) a dual-attention mechanism
capturing both incoming and outgoing dependencies, and (iii) a robust, discrete
generative framework. To support evaluation, we introduce a benchmark suite
covering synthetic and real-world datasets. It shows that our method performs
strongly across diverse settings and even competes with specialized models for
particular classes, such as directed acyclic graphs. Our results highlight the
effectiveness and generality of our approach, establishing a solid foundation
for future research in directed graph generation.

</details>


### [176] [Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights](https://arxiv.org/abs/2506.16406)
*Zhiyuan Liang,Dongwen Tang,Yuhao Zhou,Xuanlei Zhao,Mingjia Shi,Wangbo Zhao,Zekai Li,Peihao Wang,Konstantin Schürholt,Damian Borth,Michael M. Bronstein,Yang You,Zhangyang Wang,Kai Wang*

Key words: 参数高效微调、LoRA、提示条件生成、跨领域泛化、DnD

TL;DR: Drag-and-Drop LLMs (DnD)是一种基于提示的低秩适应方法，无需对每个下游任务进行单独训练，显著降低计算开销并提升性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现代参数高效微调方法如LoRA虽降低了大型语言模型的定制成本，但仍需对每个数据集进行单独优化。DnD旨在通过提示映射直接生成任务参数，消除逐任务训练的需求。

Method: DnD通过轻量级文本编码器和级联超卷积解码器，将任务提示直接映射为LoRA权重更新，无需标注数据。

Result: DnD实现了高达12,000倍的计算开销降低，在未见任务上的性能提升达30%，并展示了强大的跨领域泛化能力。

Conclusion: 提示条件参数生成是梯度适应的一种可行替代方案，可快速定制LLMs。

Abstract: Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank
adaptation (LoRA) reduce the cost of customizing large language models (LLMs),
yet still require a separate optimization run for every downstream dataset. We
introduce \textbf{Drag-and-Drop LLMs (\textit{DnD})}, a prompt-conditioned
parameter generator that eliminates per-task training by mapping a handful of
unlabeled task prompts directly to LoRA weight updates. A lightweight text
encoder distills each prompt batch into condition embeddings, which are then
transformed by a cascaded hyper-convolutional decoder into the full set of LoRA
matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD
produces task-specific parameters in seconds, yielding i) up to
\textbf{12,000$\times$} lower overhead than full fine-tuning, ii) average gains
up to \textbf{30\%} in performance over the strongest training LoRAs on unseen
common-sense reasoning, math, coding, and multimodal benchmarks, and iii)
robust cross-domain generalization despite never seeing the target data or
labels. Our results demonstrate that prompt-conditioned parameter generation is
a viable alternative to gradient-based adaptation for rapidly specializing
LLMs. Our project is available at
\href{https://jerryliang24.github.io/DnD}{https://jerryliang24.github.io/DnD}.

</details>


### [177] [Optimizing MoE Routers: Design, Implementation, and Evaluation in Transformer Models](https://arxiv.org/abs/2506.16419)
*Daniel Fidel Harvey,George Weale,Berk Yilmaz*

Key words: 混合专家（MoE），路由器架构，BERT，Qwen1.5-MoE，稀疏路由

TL;DR: 研究分析了混合专家（MoE）模型中的路由架构，比较了六种路由器的性能，并提出了一种新的MLP-Hadamard路由器，展示了其在稀疏路由中的独特能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: MoE架构的扩展性依赖于路由模块，但不良的路由可能导致负载不平衡和精度下降。本研究旨在通过设计和实现不同的路由器架构来解决这些问题。

Method: 设计了六种路由器架构（Linear、Attention、MLP、Hybrid、Hash和新的MLP-Hadamard），并在BERT和Qwen1.5-MoE模型上进行实验，评估其参数效率、推理延迟、路由熵和专家利用率。

Result: 实验结果显示不同路由器各有优劣：Linear路由器速度最快，而MLP和Attention路由器表达能力更强；MLP-Hadamard路由器在结构化和稀疏路由中表现独特。

Conclusion: 研究为MoE路由器的设计和性能优化提供了比较分析，为大规模模型部署的效率优化提供了见解。

Abstract: Mixture of Experts (MoE) architectures increase large language model
scalability, yet their performance depends on the router module that moves
tokens to specialized experts. Bad routing can load imbalance and reduced
accuracy. This project designed and implemented different router architectures
within Transformer models to fix these limitations. We experimented with six
distinct router variants Linear, Attention, Multi-Layer Perceptron (MLP),
Hybrid, Hash, and our new MLP-Hadamard. We characterized these routers using
BERT and the Qwen1.5-MoE model, looking at parameter efficiency, inference
latency, routing entropy, and expert utilization patterns. Our evaluations
showed distinct trade-offs: Linear routers offer speed, while MLP and Attention
routers provide greater expressiveness. The MLP-Hadamard router shows a unique
capability for structured, sparse routing. We successfully replaced and
fine-tuned custom routers within the complex, quantized Qwen1.5-MoE model. This
work provides a comparative analysis of MoE router designs and offers insights
into optimizing their performance for efficient and effective large-scale model
deployment.

</details>


### [178] [EFormer: An Effective Edge-based Transformer for Vehicle Routing Problems](https://arxiv.org/abs/2506.16428)
*Dian Meng,Zhiguang Cao,Yaoxin Wu,Yaqing Hou,Hongwei Ge,Qiang Zhang*

Key words: Vehicle Routing Problem, Transformer, edge-based input, reinforcement learning, generalization

TL;DR: 论文提出了一种基于边缘的Transformer模型EFormer，用于解决车辆路径问题（VRP），通过引入边缘作为输入并采用并行编码策略，提高了模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有VRP的神经启发式方法主要依赖节点坐标作为输入，但在实际场景中使用边缘距离等真实成本指标更为有效，因此需要一种新的方法来解决这一局限性。

Method: EFormer采用边缘作为唯一输入，通过预编码模块和混合得分注意力机制将边缘信息转换为临时节点嵌入，并设计了并行编码策略（图编码器和节点编码器）来处理全局关系。解码阶段使用并行上下文嵌入和多查询集成来构建路径，并通过强化学习进行训练。

Result: 实验表明，EFormer在合成数据集（包括大规模和多样化分布）上的表现优于现有基线方法，并在TSPLib和CVRPLib的真实实例中展现出强泛化能力。

Conclusion: EFormer的核心设计有效解决了VRP问题，为实际应用中的路由优化提供了新的解决方案。

Abstract: Recent neural heuristics for the Vehicle Routing Problem (VRP) primarily rely
on node coordinates as input, which may be less effective in practical
scenarios where real cost metrics-such as edge-based distances-are more
relevant. To address this limitation, we introduce EFormer, an Edge-based
Transformer model that uses edge as the sole input for VRPs. Our approach
employs a precoder module with a mixed-score attention mechanism to convert
edge information into temporary node embeddings. We also present a parallel
encoding strategy characterized by a graph encoder and a node encoder, each
responsible for processing graph and node embeddings in distinct feature
spaces, respectively. This design yields a more comprehensive representation of
the global relationships among edges. In the decoding phase, parallel context
embedding and multi-query integration are used to compute separate attention
mechanisms over the two encoded embeddings, facilitating efficient path
construction. We train EFormer using reinforcement learning in an
autoregressive manner. Extensive experiments on the Traveling Salesman Problem
(TSP) and Capacitated Vehicle Routing Problem (CVRP) reveal that EFormer
outperforms established baselines on synthetic datasets, including large-scale
and diverse distributions. Moreover, EFormer demonstrates strong generalization
on real-world instances from TSPLib and CVRPLib. These findings confirm the
effectiveness of EFormer's core design in solving VRPs.

</details>


### [179] [An efficient neuromorphic approach for collision avoidance combining Stack-CNN with event cameras](https://arxiv.org/abs/2506.16436)
*Antonio Giulio Coretti,Mattia Varile,Mario Edoardo Bertaina*

Key words: 空间碎片、事件相机、Stack-CNN、信噪比、STM/SSA

TL;DR: 该论文提出了一种基于事件相机的创新性碰撞避免系统，用于空间碎片检测，通过Stack-CNN算法提升信噪比，并在地面测试中展示了其潜力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 空间碎片对航天器构成严重威胁，需要研究主动和被动的缓解策略。

Method: 利用事件相机和Stack-CNN算法实时分析数据，检测微弱移动物体。

Result: 地面测试表明，该系统能有效提升信噪比，为空间成像和STM/SSA操作提供了新方法。

Conclusion: 该碰撞避免系统在空间交通管理和空间态势感知中具有应用潜力。

Abstract: Space debris poses a significant threat, driving research into active and
passive mitigation strategies. This work presents an innovative collision
avoidance system utilizing event-based cameras - a novel imaging technology
well-suited for Space Situational Awareness (SSA) and Space Traffic Management
(STM). The system, employing a Stack-CNN algorithm (previously used for meteor
detection), analyzes real-time event-based camera data to detect faint moving
objects. Testing on terrestrial data demonstrates the algorithm's ability to
enhance signal-to-noise ratio, offering a promising approach for on-board space
imaging and improving STM/SSA operations.

</details>


### [180] [Latent Concept Disentanglement in Transformer-based Language Models](https://arxiv.org/abs/2506.16975)
*Guan Zhe Hong,Bhavya Vasudeva,Vatsal Sharan,Cyrus Rashtchian,Prabhakar Raghavan,Rina Panigrahy*

Key words: 大型语言模型,上下文学习,潜在概念,transformer,表征学习

TL;DR: 研究表明，大型语言模型在上下文学习中能够识别并利用潜在概念，而不仅仅是任务目标。通过分析离散和连续潜在概念任务，发现模型在表征空间中存在低维子空间，其几何结构与潜在概念参数化一致。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探究transformer模型在上下文学习中是否真正表示潜在结构，而不仅仅是解决问题的捷径。

Method: 通过分析离散（2-hop推理）和连续潜在概念任务，研究模型如何解耦和使用这些概念。

Result: 模型能够成功识别离散潜在概念并进行逐步组合；在连续潜在概念任务中，表征空间的低维子空间几何结构与潜在概念参数化一致。

Conclusion: 研究深化了对上下文学习和transformer表征的理解，证明了模型中存在高度局部化的结构用于解耦潜在概念。

Abstract: When large language models (LLMs) use in-context learning (ICL) to solve a
new task, they seem to grasp not only the goal of the task but also core,
latent concepts in the demonstration examples. This begs the question of
whether transformers represent latent structures as part of their computation
or whether they take shortcuts to solve the problem. Prior mechanistic work on
ICL does not address this question because it does not sufficiently examine the
relationship between the learned representation and the latent concept, and the
considered problem settings often involve only single-step reasoning. In this
work, we examine how transformers disentangle and use latent concepts. We show
that in 2-hop reasoning tasks with a latent, discrete concept, the model
successfully identifies the latent concept and does step-by-step concept
composition. In tasks parameterized by a continuous latent concept, we find
low-dimensional subspaces in the representation space where the geometry mimics
the underlying parameterization. Together, these results refine our
understanding of ICL and the representation of transformers, and they provide
evidence for highly localized structures in the model that disentangle latent
concepts in ICL tasks.

</details>


### [181] [Leveraging Influence Functions for Resampling Data in Physics-Informed Neural Networks](https://arxiv.org/abs/2506.16443)
*Jonas R. Naujoks,Aleksander Krasowski,Moritz Weckbecker,Galip Ümit Yolcu,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek,René P. Klausen*

Key words: 物理信息神经网络, 影响函数, 可解释AI, 数据采样, 科学机器学习

TL;DR: 本文探讨了基于影响函数的数据采样方法在PINNs训练中的应用，结果显示这种方法能提升预测精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为提高物理信息神经网络（PINNs）的预测准确性，研究如何通过可解释AI（XAI）中的影响函数优化训练数据采样。

Method: 采用影响函数来评估训练数据点对模型的影响，并基于此进行针对性的数据重采样。

Result: 实验表明，基于数据归因方法的针对性采样能有效提升PINNs的预测精度。

Conclusion: 影响函数为基础的采样方法在PINNs训练中具有实际应用价值，展示了XAI方法在科学机器学习中的潜力。

Abstract: Physics-informed neural networks (PINNs) offer a powerful approach to solving
partial differential equations (PDEs), which are ubiquitous in the quantitative
sciences. Applied to both forward and inverse problems across various
scientific domains, PINNs have recently emerged as a valuable tool in the field
of scientific machine learning. A key aspect of their training is that the data
-- spatio-temporal points sampled from the PDE's input domain -- are readily
available. Influence functions, a tool from the field of explainable AI (XAI),
approximate the effect of individual training points on the model, enhancing
interpretability. In the present work, we explore the application of influence
function-based sampling approaches for the training data. Our results indicate
that such targeted resampling based on data attribution methods has the
potential to enhance prediction accuracy in physics-informed neural networks,
demonstrating a practical application of an XAI method in PINN training.

</details>


### [182] [From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers](https://arxiv.org/abs/2506.17052)
*Jingtong Su,Julia Kempe,Karen Ullrich*

Key words: Transformer, 可解释性, 注意力机制, 复杂概念, 调控

TL;DR: 提出了SAM-D和SAM-I方法，用于解释和调控Transformer模型中的复杂概念。研究表明，这些方法在语言和视觉任务中均有效。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前Transformer模型的可解释性研究主要关注简单概念，忽视了注意力机制的作用。本文旨在填补这一空白，提出一种通用的方法来分析和调控复杂概念。

Method: 提出Scalable Attention Module Discovery (SAM-D)将复杂概念映射到特定注意力头，并通过Scalar Attention Module Intervention (SAM-I)用一个标量参数调控概念的影响。

Result: 实验表明，SAM-D能稳定定位概念模块，SAM-I显著提升了任务表现（如安全性降低+72.7%，推理能力提升+1.6%）。

Conclusion: 该方法具有通用性，适用于多种领域，并为模型可解释性和调控提供了新思路。

Abstract: Transformers have achieved state-of-the-art performance across language and
vision tasks. This success drives the imperative to interpret their internal
mechanisms with the dual goals of enhancing performance and improving
behavioral control. Attribution methods help advance interpretability by
assigning model outputs associated with a target concept to specific model
components. Current attribution research primarily studies multi-layer
perceptron neurons and addresses relatively simple concepts such as factual
associations (e.g., Paris is located in France). This focus tends to overlook
the impact of the attention mechanism and lacks a unified approach for
analyzing more complex concepts. To fill these gaps, we introduce Scalable
Attention Module Discovery (SAMD), a concept-agnostic method for mapping
arbitrary, complex concepts to specific attention heads of general transformer
models. We accomplish this by representing each concept as a vector,
calculating its cosine similarity with each attention head, and selecting the
TopK-scoring heads to construct the concept-associated attention module. We
then propose Scalar Attention Module Intervention (SAMI), a simple strategy to
diminish or amplify the effects of a concept by adjusting the attention module
using only a single scalar parameter. Empirically, we demonstrate SAMD on
concepts of varying complexity, and visualize the locations of their
corresponding modules. Our results demonstrate that module locations remain
stable before and after LLM post-training, and confirm prior work on the
mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on
HarmBench (+72.7%) by diminishing "safety" and improve performance on the GSM8K
benchmark (+1.6%) by amplifying "reasoning". Lastly, we highlight the
domain-agnostic nature of our approach by suppressing the image classification
accuracy of vision transformers on ImageNet.

</details>


### [183] [Consumer-friendly EEG-based Emotion Recognition System: A Multi-scale Convolutional Neural Network Approach](https://arxiv.org/abs/2506.16448)
*Tri Duc Ly,Gia H. Ngo*

Key words: EEG, 情绪识别, 深度学习, 多尺度卷积神经网络

TL;DR: 本文提出了一种基于多尺度卷积神经网络的EEG情绪识别新方法，在多个性能指标上优于现有技术。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 开发一种能够在现实生活场景中进行EEG情绪识别的深度学习模型。

Method: 采用多尺度卷积神经网络，利用不同比例系数的特征提取核和一种新型核，从大脑的四个不同区域学习关键信息。

Result: 模型在预测效价、唤醒度和支配度分数上优于目前最先进的TSception模型。

Conclusion: 提出的方法在EEG情绪识别任务中表现优异，适用于现实场景。

Abstract: EEG is a non-invasive, safe, and low-risk method to record
electrophysiological signals inside the brain. Especially with recent
technology developments like dry electrodes, consumer-grade EEG devices, and
rapid advances in machine learning, EEG is commonly used as a resource for
automatic emotion recognition. With the aim to develop a deep learning model
that can perform EEG-based emotion recognition in a real-life context, we
propose a novel approach to utilize multi-scale convolutional neural networks
to accomplish such tasks. By implementing feature extraction kernels with many
ratio coefficients as well as a new type of kernel that learns key information
from four separate areas of the brain, our model consistently outperforms the
state-of-the-art TSception model in predicting valence, arousal, and dominance
scores across many performance evaluation metrics.

</details>


### [184] [Joint Tensor-Train Parameterization for Efficient and Expressive Low-Rank Adaptation](https://arxiv.org/abs/2506.16456)
*Jun Qi,Chen-Yu Liu,Sabato Marco Siniscalchi,Chao-Han Huck Yang,Min-Hsiu Hsieh*

Key words: Low-Rank Adaptation, LoRA, Tensor-train decomposition, Parameter efficiency, Generalization

TL;DR: TensorGuide提出了一种基于张量列分解的新框架，通过生成联合低秩LoRA矩阵，显著提高了表达力和泛化能力，同时保持参数效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 标准LoRA独立优化低秩矩阵导致表达力和泛化能力受限，传统TT分解方法也未显著提升性能，因此需要更高效的方法。

Method: TensorGuide框架通过统一TT结构和受控高斯噪声生成联合低秩LoRA矩阵，优化参数效率。

Result: 实验表明，TensorGuide在量子点分类和GPT-2微调任务中优于标准LoRA和TT-LoRA，实现了更高准确性和可扩展性。

Conclusion: TensorGuide显著提升了LoRA的表达力和泛化能力，同时保持参数效率，具有理论和实验优势。

Abstract: Low-Rank Adaptation (LoRA) is widely recognized for its parameter-efficient
fine-tuning of large-scale neural models. However, standard LoRA independently
optimizes low-rank matrices, which inherently limits its expressivity and
generalization capabilities. While classical tensor-train (TT) decomposition
can be separately employed on individual LoRA matrices, this work demonstrates
that the classical TT-based approach neither significantly improves parameter
efficiency nor achieves substantial performance gains. This paper proposes
TensorGuide, a novel tensor-train-guided adaptation framework to overcome these
limitations. TensorGuide generates two correlated low-rank LoRA matrices
through a unified TT structure driven by controlled Gaussian noise. The
resulting joint TT representation inherently provides structured, low-rank
adaptations, significantly enhancing expressivity, generalization, and
parameter efficiency without increasing the number of trainable parameters.
Theoretically, we justify these improvements through neural tangent kernel
analyses, demonstrating superior optimization dynamics and enhanced
generalization. Extensive experiments on quantum dot classification and GPT-2
fine-tuning benchmarks demonstrate that TensorGuide-based LoRA consistently
outperforms standard LoRA and TT-LoRA, achieving improved accuracy and
scalability with fewer parameters.

</details>


### [185] [Black-Box Privacy Attacks on Shared Representations in Multitask Learning](https://arxiv.org/abs/2506.16460)
*John Abascal,Nicolás Berrios,Alina Oprea,Jonathan Ullman,Adam Smith,Matthew Jagielski*

Key words: 多任务学习、隐私泄露、黑盒攻击、任务推断、共享表示

TL;DR: 该研究探索了多任务学习（MTL）中共享表示可能泄露任务敏感信息的问题，提出了一种黑盒任务推断攻击模型，并展示了攻击在视觉和语言领域的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: MTL通过共享表示学习任务间的共性，但这些表示可能意外泄露任务隐私。研究旨在揭示共享表示的信息泄露风险。

Method: 提出了一种黑盒任务推断攻击模型，攻击者仅通过查询共享表示生成的嵌入向量即可推断任务是否参与了训练，无需影子模型或标记数据。

Result: 实验表明，攻击在多个MTL应用场景中有效，即使攻击者仅能获取任务的新样本而非训练数据。理论分析进一步支持了实验结果。

Conclusion: 共享表示在多任务学习中存在隐私泄露风险，即使在严格限制攻击者能力的黑盒场景下，仍可能泄露任务信息。

Abstract: Multitask learning (MTL) has emerged as a powerful paradigm that leverages
similarities among multiple learning tasks, each with insufficient samples to
train a standalone model, to solve them simultaneously while minimizing data
sharing across users and organizations. MTL typically accomplishes this goal by
learning a shared representation that captures common structure among the tasks
by embedding data from all tasks into a common feature space. Despite being
designed to be the smallest unit of shared information necessary to effectively
learn patterns across multiple tasks, these shared representations can
inadvertently leak sensitive information about the particular tasks they were
trained on.
  In this work, we investigate what information is revealed by the shared
representations through the lens of inference attacks. Towards this, we propose
a novel, black-box task-inference threat model where the adversary, given the
embedding vectors produced by querying the shared representation on samples
from a particular task, aims to determine whether that task was present when
training the shared representation. We develop efficient, purely black-box
attacks on machine learning models that exploit the dependencies between
embeddings from the same task without requiring shadow models or labeled
reference data. We evaluate our attacks across vision and language domains for
multiple use cases of MTL and demonstrate that even with access only to fresh
task samples rather than training data, a black-box adversary can successfully
infer a task's inclusion in training. To complement our experiments, we provide
theoretical analysis of a simplified learning setting and show a strict
separation between adversaries with training samples and fresh samples from the
target task's distribution.

</details>


### [186] [Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities](https://arxiv.org/abs/2506.16471)
*Tara Akhound-Sadegh,Jungyoon Lee,Avishek Joey Bose,Valentin De Bortoli,Arnaud Doucet,Michael M. Bronstein,Dominique Beaini,Siamak Ravanbakhsh,Kirill Neklyudov,Alexander Tong*

Key words: 采样, 扩散模型, 退火, 分子系统, PITA

TL;DR: 论文提出了一种名为PITA的新框架，通过结合退火技术和扩散平滑，显著提高了从目标分布中采样的效率，尤其在分子系统等领域表现突出。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 从非归一化概率密度中高效采样是一个核心挑战，现有扩散采样方法无法处理简单分子系统的规模。

Method: PITA结合了Boltzmann分布的退火和扩散平滑技术，通过训练一系列扩散模型并逐步降低温度，实现高效采样。

Result: PITA首次实现了对N体粒子系统、Alanine Dipeptide和三肽的平衡采样，且大大减少了能量函数的评估次数。

Conclusion: PITA为复杂系统的采样提供了高效的新方法，具有广泛的应用潜力。

Abstract: Sampling efficiently from a target unnormalized probability density remains a
core challenge, with relevance across countless high-impact scientific
applications. A promising approach towards this challenge is the design of
amortized samplers that borrow key ideas, such as probability path design, from
state-of-the-art generative diffusion models. However, all existing
diffusion-based samplers remain unable to draw samples from distributions at
the scale of even simple molecular systems. In this paper, we propose
Progressive Inference-Time Annealing (PITA), a novel framework to learn
diffusion-based samplers that combines two complementary interpolation
techniques: I.) Annealing of the Boltzmann distribution and II.) Diffusion
smoothing. PITA trains a sequence of diffusion models from high to low
temperatures by sequentially training each model at progressively higher
temperatures, leveraging engineered easy access to samples of the
temperature-annealed target density. In the subsequent step, PITA enables
simulating the trained diffusion model to procure training samples at a lower
temperature for the next diffusion model through inference-time annealing using
a novel Feynman-Kac PDE combined with Sequential Monte Carlo. Empirically, PITA
enables, for the first time, equilibrium sampling of N-body particle systems,
Alanine Dipeptide, and tripeptides in Cartesian coordinates with dramatically
lower energy function evaluations. Code available at:
https://github.com/taraak/pita

</details>


### [187] [Manifold Learning for Personalized and Label-Free Detection of Cardiac Arrhythmias](https://arxiv.org/abs/2506.16494)
*Amir Reza Vazifeh,Jason W. Fleischer*

Key words: ECG, machine learning, nonlinear dimensionality reduction, t-SNE, UMAP, arrhythmia

TL;DR: 非线性降维技术（NLDR）在无需训练或先验信息的情况下，有效识别ECG信号中的医学相关特征，并在心律失常分类中表现出高准确率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: ECG分析存在人工耗时和误差问题，机器学习虽提供自动化解决方案，但信号多变性和数据偏差限制了其通用性。

Method: 采用非线性降维（如t-SNE和UMAP）分析MIT-BIH数据集中的MLII和V1导联信号。

Result: 在混合人群中区分个体记录的准确率≥90%，对心律失常分类的中位准确率和F1值分别为98.96%和91.02%。

Conclusion: NLDR技术在心脏监测中潜力巨大，尤其在单导联和12导联ECG中，并适用于个性化医疗。

Abstract: Electrocardiograms (ECGs) provide direct, non-invasive measurements of heart
activity and are well-established tools for detecting and monitoring
cardiovascular disease. However, manual ECG analysis can be time-consuming and
prone to errors. Machine learning has emerged as a promising approach for
automated heartbeat recognition and classification, but substantial variations
in ECG signals make it challenging to develop generalizable models. ECG signals
can vary widely across individuals and leads, while datasets often follow
different labeling standards and may be biased, all of which greatly hinder
supervised methods. Conventional unsupervised methods, e.g. principal component
analysis, prioritize large (and often obvious) variances in the data and
typically overlook subtle yet clinically relevant patterns. If labels are
missing and/or variations are significant but small, both approaches fail.
Here, we show that nonlinear dimensionality reduction (NLDR) can accommodate
these issues and identify medically relevant features in ECG signals, with no
need for training or prior information. Using the MLII and V1 leads of the
MIT-BIH dataset, we demonstrate that t-distributed stochastic neighbor
embedding and uniform manifold approximation and projection can discriminate
individual recordings in mixed populations with >= 90% accuracy and distinguish
different arrhythmias in individual patients with a median accuracy of 98.96%
and a median F1-score of 91.02%. The results show that NLDR holds much promise
for cardiac monitoring, including the limiting cases of single-lead ECG and the
current 12-lead standard of care, and for personalized health care beyond
cardiology.

</details>


### [188] [SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity](https://arxiv.org/abs/2506.16500)
*Samir Khaki,Xiuyu Li,Junxian Guo,Ligeng Zhu,Chenfeng Xu,Konstantinos N. Plataniotis,Amir Yazdanbakhsh,Kurt Keutzer,Song Han,Zhijian Liu*

Key words: 大型语言模型,微调,稀疏性,SVD估计器,计算效率

TL;DR: 论文提出了一种名为SparseLoRA的方法，通过上下文稀疏性加速大型语言模型（LLM）的微调，显著降低了计算成本并保持了准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的参数高效微调方法（如QLoRA和DoRA）虽然减少了可训练参数和内存使用，但未能降低计算成本，有时甚至可能减慢微调速度。因此，需要一种既能高效计算又能保持准确性的新方法。

Method: 提出了SparseLoRA方法，采用轻量级的、无需训练的SVD稀疏估计器，动态选择稀疏权重子集进行损失和梯度计算。此外，还对层、标记和训练步骤的敏感性进行了系统分析。

Result: 实验结果显示，SparseLoRA将计算成本降至2.2倍，实测加速达1.6倍，同时在常识和算术推理、代码生成和指令遵循等下游任务中保持了准确性。

Conclusion: SparseLoRA是一种高效且准确的LLM微调方法，通过上下文稀疏性显著提升了计算效率。

Abstract: Fine-tuning LLMs is both computationally and memory-intensive. While
parameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the
number of trainable parameters and lower memory usage, they do not decrease
computational cost. In some cases, they may even slow down fine-tuning. In this
paper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning
through contextual sparsity. We propose a lightweight, training-free SVD
sparsity estimator that dynamically selects a sparse subset of weights for loss
and gradient computation. Also, we systematically analyze and address
sensitivity across layers, tokens, and training steps. Our experimental results
show that SparseLoRA reduces computational cost by up to 2.2 times and a
measured speedup of up to 1.6 times while maintaining accuracy across various
downstream tasks, including commonsense and arithmetic reasoning, code
generation, and instruction following.

</details>


### [189] [Subspace-Boosted Model Merging](https://arxiv.org/abs/2506.16506)
*Ronald Skorobogat,Karsten Roth,Mariana-Iuliana Georgescu,Zeynep Akata*

Key words: 模型合并, 任务算术, 子空间增强, 奇异值分解, 任务相似性

TL;DR: 该论文提出了一种名为“子空间增强”的技术，通过维护任务向量空间的秩来缓解模型合并中的性能下降问题，显著提高了合并效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着合并专家模型数量的增加，传统模型合并方法的性能增益会逐渐减弱。研究旨在从任务算术的角度分析原因并提出解决方案。

Method: 利用奇异值分解的任务向量空间，提出了子空间增强技术，同时采用高阶广义奇异值分解量化任务相似性。

Result: 在视觉基准测试中，子空间增强技术将合并效果提升超过10%，支持最多20个专家模型的合并。

Conclusion: 通过维护任务向量秩和量化任务相似性，为模型合并提供了新视角，显著提升了合并性能。

Abstract: Model merging enables the combination of multiple specialized expert models
into a single model capable of performing multiple tasks. However, the benefits
of merging an increasing amount of specialized experts generally lead to
diminishing returns and reduced overall performance gains. In this work, we
offer an explanation and analysis from a task arithmetic perspective; revealing
that as the merging process (across numerous existing merging methods)
continues for more and more experts, the associated task vector space
experiences rank collapse. To mitigate this issue, we introduce Subspace
Boosting, which operates on the singular value decomposed task vector space and
maintains task vector ranks. Subspace Boosting raises merging efficacy for up
to 20 expert models by large margins of more than 10% when evaluated on vision
benchmarks. Moreover, we propose employing Higher-Order Generalized Singular
Value Decomposition to further quantify task similarity, offering a new
interpretable perspective on model merging.

</details>


### [190] [Robust Reward Modeling via Causal Rubrics](https://arxiv.org/abs/2506.16507)
*Pragya Srivastava,Harman Singh,Rahul Madhavan,Gandharv Patil,Sravanti Addepalli,Arun Suggala,Rengarajan Aravamudhan,Soumya Sharma,Anirban Laha,Aravindan Raghuveer,Karthikeyan Shanmugam,Doina Precup*

Key words: 奖励模型, 奖励黑客, 因果建模, Crome, LLM

TL;DR: Crome（因果鲁棒奖励建模）是一个新框架，旨在减少奖励模型中的奖励黑客行为，通过因果增强和中立增强来区分因果属性和虚假属性，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的奖励模型容易因训练数据中的相关性而误将虚假属性（如回答长度或格式）视为质量的真正驱动因素，导致奖励黑客和策略失准。

Method: Crome通过因果增强（针对因果属性差异的数据对）和中立增强（针对虚假属性差异的数据对）来训练模型，无需预先了解虚假因素。

Result: Crome在RewardBench上的平均准确率提升了5.4%，在特定类别中提升了13.2%和7.2%，并在多个基准测试中表现稳健。

Conclusion: Crome通过因果建模显著提升了奖励模型的鲁棒性和性能，避免了奖励黑客问题。

Abstract: Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)
via human feedback, yet they often suffer from reward hacking. They tend to
latch on to superficial or spurious attributes, such as response length or
formatting, mistaking these cues learned from correlations in training data for
the true causal drivers of quality (e.g., factuality, relevance). This occurs
because standard training objectives struggle to disentangle these factors,
leading to brittle RMs and misaligned policies. We introduce Crome (Causally
Robust Reward Modeling), a novel framework grounded in an explicit causal model
designed to mitigate reward hacking. Crome employs the following synthetic
targeted augmentations during training: (1) Causal Augmentations, which are
pairs that differ along specific causal attributes, to enforce sensitivity
along each causal attribute individually, and (2) Neutral Augmentations, which
are tie-label pairs varying primarily in spurious attributes, to enforce
invariance along spurious attributes. Notably, our augmentations are produced
without any knowledge of spurious factors, via answer interventions only along
causal rubrics, that are identified by querying an oracle LLM. Empirically,
Crome significantly outperforms standard baselines on RewardBench, improving
average accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in
specific categories. The robustness of Crome is further testified by the
consistent gains obtained in a Best-of-N inference setting across increasing N,
across various benchmarks, including the popular RewardBench (covering chat,
chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and
the reasoning-specific GSM8k.

</details>


### [191] [Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility Metrics Using Phonetic, Semantic, and NLI Approaches](https://arxiv.org/abs/2506.16528)
*Bornali Phukon,Xiuwen Zheng,Mark Hasegawa-Johnson*

Key words: ASR, 可理解性, 构音障碍, 语义对齐, NLI

TL;DR: 本文提出了一种新的语音识别（ASR）评估指标，用于解决传统指标（如WER和CER）在衡量患有构音障碍或发声障碍的语音的可理解性方面的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统ASR指标无法准确捕捉语义对齐，尤其是在构音障碍或发声障碍语音中，人类听者能理解语义，但ASR系统可能产生错误。

Method: 结合自然语言推理（NLI）分数、语义相似度和语音相似度，提出了一种新的ASR评估指标。

Result: 新指标在Speech Accessibility Project数据上与人类判断的相关性达到0.890，优于传统方法。

Conclusion: 新指标强调了在ASR评估中优先考虑可理解性的重要性，而非基于错误的方法。

Abstract: Traditional ASR metrics like WER and CER fail to capture intelligibility,
especially for dysarthric and dysphonic speech, where semantic alignment
matters more than exact word matches. ASR systems struggle with these speech
types, often producing errors like phoneme repetitions and imprecise
consonants, yet the meaning remains clear to human listeners. We identify two
key challenges: (1) Existing metrics do not adequately reflect intelligibility,
and (2) while LLMs can refine ASR output, their effectiveness in correcting ASR
transcripts of dysarthric speech remains underexplored. To address this, we
propose a novel metric integrating Natural Language Inference (NLI) scores,
semantic similarity, and phonetic similarity. Our ASR evaluation metric
achieves a 0.890 correlation with human judgments on Speech Accessibility
Project data, surpassing traditional methods and emphasizing the need to
prioritize intelligibility over error-based measures.

</details>


### [192] [Mr. Snuffleupagus at SemEval-2025 Task 4: Unlearning Factual Knowledge from LLMs Using Adaptive RMU](https://arxiv.org/abs/2506.16548)
*Arjun Dosajh,Mihika Sanghi*

Key words: 大语言模型，隐私保护，去学习技术，表示误导遗忘

TL;DR: 该论文提出了一种名为自适应表示误导遗忘（RMU）的技术，用于从大语言模型中去除敏感信息，解决了隐私、版权和安全性问题，并在实验中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大型语言模型（LLMs）容易记忆训练数据，涉及隐私、版权合规和安全性问题，尤其是与个人身份信息（PII）相关时，需要有效的去学习技术。

Method: 采用自适应表示误导遗忘（RMU）技术，通过分析不同解码层的影响，确定最有效的敏感信息去除区域。

Result: 该技术在1B和7B参数模型的官方排行榜上均排名第四，验证了其有效性。

Conclusion: RMU技术在LLMs中去除敏感信息方面具有实际应用潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation. However, their tendency to
memorize training data raises concerns regarding privacy, copyright compliance,
and security, particularly in cases involving Personally Identifiable
Information (PII). Effective machine unlearning techniques are essential to
mitigate these risks, yet existing methods remain underdeveloped for LLMs due
to their open-ended output space. In this work, we apply the Adaptive
Representation Misdirection Unlearning (RMU) technique to unlearn sensitive
information from LLMs. Through extensive experiments, we analyze the effects of
unlearning across different decoder layers to determine the most effective
regions for sensitive information removal. Our technique ranked 4th on the
official leaderboard of both 1B parameter and 7B parameter models.

</details>


### [193] [A Free Probabilistic Framework for Analyzing the Transformer-based Language Models](https://arxiv.org/abs/2506.16550)
*Swagatam Das*

Key words: transformer, 自由概率理论, 非交换卷积, 泛化界, 熵动态

TL;DR: 该论文提出了一个基于自由概率理论的算子理论框架，用于分析基于transformer的语言模型，揭示了其频谱动力系统及信息流特性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究旨在通过非交换谐波分析和自由概率理论，理解transformer模型的归纳偏置、泛化行为和熵动态。

Method: 将token嵌入和注意力机制表示为自伴算子，将注意力重新解释为非交换卷积，并分析表示层间的自由加性卷积演化。

Result: 提出了基于自由熵的泛化界，并表明transformer层的频谱轨迹随深度变化具有可预测性。

Conclusion: 该框架为大型语言模型的信息流和结构复杂性提供了理论分析工具，连接了神经架构与非交换谐波分析。

Abstract: We outline an operator-theoretic framework for analyzing transformer-based
language models using the tools of free probability theory. By representing
token embeddings and attention mechanisms as self-adjoint operators in a racial
probability space, we reinterpret attention as a non-commutative convolution
and view the layer-wise propagation of representations as an evolution governed
by free additive convolution. This formalism reveals a spectral dynamical
system underpinning deep transformer stacks and offers insight into their
inductive biases, generalization behavior, and entropy dynamics. We derive a
generalization bound based on free entropy and demonstrate that the spectral
trace of transformer layers evolves predictably with depth. Our approach
bridges neural architecture with non-commutative harmonic analysis, enabling
principled analysis of information flow and structural complexity in large
language models

</details>


### [194] [One Sample is Enough to Make Conformal Prediction Robust](https://arxiv.org/abs/2506.16553)
*Soroush H. Zargarbashi,Mohammad Sadegh Akhondzadeh,Aleksandar Bojchevski*

Key words: 鲁棒性共形预测, 单样本认证, 计算效率

TL;DR: 该论文提出了一种单样本鲁棒性共形预测方法（RCP1），通过仅需一次前向传递即可生成鲁棒的预测集。相比现有方法，这种方法计算成本更低且平均预测集更小。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的基于平滑的鲁棒性共形预测需要多次模型前向传递，计算成本高。作者旨在简化这一过程，提出一种更高效的方法。

Method: 使用任何二元证书对共形预测过程本身进行认证，而不是对单个分数进行认证。仅需对单个扰动的输入进行一次前向传递即可生成鲁棒预测集。

Result: RCP1方法生成的预测集比现有方法（如需要约100次前向传递的方法）更小，计算效率更高，并且在分类和回归任务中均适用。

Conclusion: 通过认证共形预测过程本身而非个体分数，RCP1在保持高鲁棒性的同时显著降低了计算成本。

Abstract: Given any model, conformal prediction (CP) returns prediction sets guaranteed
to include the true label with high adjustable probability. Robust CP (RCP)
extends this to inputs with worst-case noise. A well-established approach is to
use randomized smoothing for RCP since it is applicable to any black-box model
and provides smaller sets compared to deterministic methods. However, current
smoothing-based RCP requires many model forward passes per each input which is
computationally expensive. We show that conformal prediction attains some
robustness even with a forward pass on a single randomly perturbed input. Using
any binary certificate we propose a single sample robust CP (RCP1). Our
approach returns robust sets with smaller average set size compared to SOTA
methods which use many (e.g. around 100) passes per input. Our key insight is
to certify the conformal prediction procedure itself rather than individual
scores. Our approach is agnostic to the setup (classification and regression).
We further extend our approach to smoothing-based robust conformal risk
control.

</details>


### [195] [Energy-Based Transfer for Reinforcement Learning](https://arxiv.org/abs/2506.16590)
*Zeyun Deng,Jasorsi Ghosh,Fiona Xie,Yuzhe Lu,Katia Sycara,Joseph Campbell*

Key words: 强化学习, 迁移学习, 样本效率, 多任务学习, 能量模型

TL;DR: 本文提出了一种基于能量的迁移学习方法，通过分布外检测选择性地提供指导，以提高强化学习在多任务持续学习中的样本效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统强化学习算法样本效率低，迁移学习中的教师策略在新任务中可能提供次优指导。

Method: 采用基于能量的方法，利用分布外检测选择性指导，教师仅干预其训练分布内的状态。

Result: 理论表明能量分数反映教师的状态访问密度，实验验证了单任务和多任务设置的样本效率和性能提升。

Conclusion: 该方法有效解决了迁移学习中次优指导问题，提升了样本效率和性能。

Abstract: Reinforcement learning algorithms often suffer from poor sample efficiency,
making them challenging to apply in multi-task or continual learning settings.
Efficiency can be improved by transferring knowledge from a previously trained
teacher policy to guide exploration in new but related tasks. However, if the
new task sufficiently differs from the teacher's training task, the transferred
guidance may be sub-optimal and bias exploration toward low-reward behaviors.
We propose an energy-based transfer learning method that uses
out-of-distribution detection to selectively issue guidance, enabling the
teacher to intervene only in states within its training distribution. We
theoretically show that energy scores reflect the teacher's state-visitation
density and empirically demonstrate improved sample efficiency and performance
across both single-task and multi-task settings.

</details>


### [196] [FLAME: Towards Federated Fine-Tuning Large Language Models Through Adaptive SMoE](https://arxiv.org/abs/2506.16600)
*Khiem Le,Tuan Tran,Ting Hua,Nitesh V. Chawla*

Key words: 联邦学习, LoRA, 稀疏混合专家, 资源自适应

TL;DR: FLAME是一个基于稀疏混合专家（SMoE）架构的新型联邦学习框架，通过动态激活专家数量实现客户端适应性，避免了现有方法因压缩导致的性能下降。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现有资源自适应的LoRA联邦微调方法因压缩全局LoRA矩阵导致的信息丢失和性能下降问题。

Method: 采用SMoE架构，保留未压缩的全局LoRA矩阵，通过客户端动态激活专家数量实现适应性；引入轻量级重缩放机制和激活感知聚合策略解决SMoE在联邦学习中的挑战。

Result: 在不同计算环境下，FLAME始终优于现有方法，提供了高效且鲁棒的资源自适应联邦学习解决方案。

Conclusion: FLAME通过SMoE架构和创新的适应机制，有效解决了资源自适应联邦学习中的性能瓶颈问题。

Abstract: Existing resource-adaptive LoRA federated fine-tuning methods enable clients
to fine-tune models using compressed versions of global LoRA matrices, in order
to accommodate various compute resources across clients. This compression
requirement will lead to suboptimal performance due to information loss. To
address this, we propose FLAME, a novel federated learning framework based on
the Sparse Mixture-of-Experts (SMoE) architecture. Unlike prior approaches,
FLAME retains full (uncompressed) global LoRA matrices and achieves client-side
adaptability by varying the number of activated experts per client. However,
incorporating SMoE into federated learning introduces unique challenges,
specifically, the mismatch in output magnitude from partial expert activation
and the imbalance in expert training quality across clients. FLAME tackles
these challenges through a lightweight rescaling mechanism and an
activation-aware aggregation scheme. Empirical results across diverse
computational settings demonstrate that FLAME consistently outperforms existing
methods, providing a robust and effective solution for resource-adaptive
federated learning.

</details>


### [197] [SlepNet: Spectral Subgraph Representation Learning for Neural Dynamics](https://arxiv.org/abs/2506.16602)
*Siddharth Viswanath,Rahul Singh,Yanlei Zhang,J. Adam Noah,Joy Hirsch,Smita Krishnaswamy*

Key words: GCN, 图信号处理, Slepian基, 神经活动表示, 时空数据

TL;DR: SlepNet是一种新型图卷积网络架构，利用Slepian基而非图傅里叶谐波，优化信号能量集中在相关子图上，提升神经活动表示的精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决图信号处理中空间或频谱局部化信号模式表示不足的问题，尤其在神经科学等领域需求迫切。

Method: 提出基于Slepian基的GCN架构SlepNet，通过掩码自动学习相关子图，集中信号能量。

Result: 在多个fMRI和交通动态数据集上优于传统GNN和图信号处理方法，信号模式表示分辨率更高。

Conclusion: SlepNet在时空数据预测和表示学习任务中均表现出优异性能。

Abstract: Graph neural networks have been useful in machine learning on
graph-structured data, particularly for node classification and some types of
graph classification tasks. However, they have had limited use in representing
patterning of signals over graphs. Patterning of signals over graphs and in
subgraphs carries important information in many domains including neuroscience.
Neural signals are spatiotemporally patterned, high dimensional and difficult
to decode. Graph signal processing and associated GCN models utilize the graph
Fourier transform and are unable to efficiently represent spatially or
spectrally localized signal patterning on graphs. Wavelet transforms have shown
promise here, but offer non-canonical representations and cannot be tightly
confined to subgraphs. Here we propose SlepNet, a novel GCN architecture that
uses Slepian bases rather than graph Fourier harmonics. In SlepNet, the Slepian
harmonics optimally concentrate signal energy on specifically relevant
subgraphs that are automatically learned with a mask. Thus, they can produce
canonical and highly resolved representations of neural activity, focusing
energy of harmonics on areas of the brain which are activated. We evaluated
SlepNet across three fMRI datasets, spanning cognitive and visual tasks, and
two traffic dynamics datasets, comparing its performance against conventional
GNNs and graph signal processing constructs. SlepNet outperforms the baselines
in all datasets. Moreover, the extracted representations of signal patterns
from SlepNet offers more resolution in distinguishing between similar patterns,
and thus represent brain signaling transients as informative trajectories. Here
we have shown that these extracted trajectory representations can be used for
other downstream untrained tasks. Thus we establish that SlepNet is useful both
for prediction and representation learning in spatiotemporal data.

</details>


### [198] [Distribution Parameter Actor-Critic: Shifting the Agent-Environment Boundary for Diverse Action Spaces](https://arxiv.org/abs/2506.16608)
*Jiamin He,A. Rupam Mahmood,Martha White*

Key words: 强化学习, 分布参数, 策略梯度, actor-critic, 连续控制

TL;DR: 该论文提出了一种新型强化学习框架，将分布参数作为动作，重新定义了智能体与环境之间的边界。通过这种重新参数化，无论原始动作类型如何，新动作空间总是连续的。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统强化学习在处理不同类型动作空间（离散、连续或混合）时面临梯度方差较大的问题，因此需要一种更通用的方法。

Method: 提出了分布参数策略梯度（DPPG）和插值评论家学习（ICL）方法，并基于TD3设计了一个实用的DPPG-based actor-critic算法（DPAC）。

Result: 在OpenAI Gym和DeepMind Control Suite的MuJoCo连续控制任务中，DPAC表现优于TD3，并在离散化动作空间环境中展现出竞争力。

Conclusion: 通过重新参数化动作空间，DPPG和DPAC能够有效降低梯度方差，提升强化学习的性能。

Abstract: We introduce a novel reinforcement learning (RL) framework that treats
distribution parameters as actions, redefining the boundary between agent and
environment. This reparameterization makes the new action space continuous,
regardless of the original action type (discrete, continuous, mixed, etc.).
Under this new parameterization, we develop a generalized deterministic policy
gradient estimator, Distribution Parameter Policy Gradient (DPPG), which has
lower variance than the gradient in the original action space. Although
learning the critic over distribution parameters poses new challenges, we
introduce interpolated critic learning (ICL), a simple yet effective strategy
to enhance learning, supported by insights from bandit settings. Building on
TD3, a strong baseline for continuous control, we propose a practical
DPPG-based actor-critic algorithm, Distribution Parameter Actor-Critic (DPAC).
Empirically, DPAC outperforms TD3 in MuJoCo continuous control tasks from
OpenAI Gym and DeepMind Control Suite, and demonstrates competitive performance
on the same environments with discretized action spaces.

</details>


### [199] [Learning Causally Predictable Outcomes from Psychiatric Longitudinal Data](https://arxiv.org/abs/2506.16629)
*Eric V. Strobl*

Key words: 因果推断,精神病学,纵向数据,DEBIAS算法,混杂控制

TL;DR: 论文提出了DEBIAS算法，通过优化结果定义来最大化因果可识别性，解决精神病学中纵向数据的因果推断问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 精神病学中症状异质性和潜在混杂因素常导致传统因果估计器失效，需要一种新方法来优化结果定义以减少混杂。

Method: DEBIAS算法学习非负、临床可解释的权重用于结果聚合，最大化持久治疗效果，并通过利用历史治疗的直接效应减少混杂。

Result: DEBIAS在抑郁和精神分裂症实验中，显著优于现有方法，恢复了临床可解释的因果效应。

Conclusion: DEBIAS提供了一种有效且可验证的方法，解决了精神病学纵向数据中的因果推断难题。

Abstract: Causal inference in longitudinal biomedical data remains a central challenge,
especially in psychiatry, where symptom heterogeneity and latent confounding
frequently undermine classical estimators. Most existing methods for treatment
effect estimation presuppose a fixed outcome variable and address confounding
through observed covariate adjustment. However, the assumption of
unconfoundedness may not hold for a fixed outcome in practice. To address this
foundational limitation, we directly optimize the outcome definition to
maximize causal identifiability. Our DEBIAS (Durable Effects with
Backdoor-Invariant Aggregated Symptoms) algorithm learns non-negative,
clinically interpretable weights for outcome aggregation, maximizing durable
treatment effects and empirically minimizing both observed and latent
confounding by leveraging the time-limited direct effects of prior treatments
in psychiatric longitudinal data. The algorithm also furnishes an empirically
verifiable test for outcome unconfoundedness. DEBIAS consistently outperforms
state-of-the-art methods in recovering causal effects for clinically
interpretable composite outcomes across comprehensive experiments in depression
and schizophrenia.

</details>


### [200] [Semantic Outlier Removal with Embedding Models and LLMs](https://arxiv.org/abs/2506.16644)
*Eren Akbiyik,João Almeida,Rik Melis,Ritu Sriram,Viviana Petrescu,Vilhjálmur Vilhjálmsson*

Key words: SORE, 文本处理, 多语言, 语义嵌入, 近似最近邻搜索

TL;DR: 提出了一种名为SORE的高效、透明的多语言文本处理方法，通过语义嵌入和近似最近邻搜索来去除无关内容。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统方法在多元语言和上下文敏感场景中表现不佳，而大型语言模型成本高昂，需要更高效、透明的解决方案。

Method: SORE利用多语言句子嵌入和近似最近邻搜索，先识别核心内容，再标记与核心显著偏离或匹配预定义异常组的段落。

Result: SORE在多种场景下表现优于传统方法，接近LLM的精确度，但成本更低，已用于生产环境处理多语言文档。

Conclusion: SORE是一种高效、准确的文本处理方法，适合大规模应用，同时公开了实现和评测数据集以促进研究。

Abstract: Modern text processing pipelines demand robust methods to remove extraneous
content while preserving a document's core message. Traditional approaches such
as HTML boilerplate extraction or keyword filters often fail in multilingual
settings and struggle with context-sensitive nuances, whereas Large Language
Models (LLMs) offer improved quality at high computational cost. We introduce
SORE (Semantic Outlier Removal), a cost-effective, transparent method that
leverages multilingual sentence embeddings and approximate nearest-neighbor
search to identify and excise unwanted text segments. By first identifying core
content via metadata embedding and then flagging segments that either closely
match predefined outlier groups or deviate significantly from the core, SORE
achieves near-LLM extraction precision at a fraction of the cost. Experiments
on HTML datasets demonstrate that SORE outperforms structural methods and yield
high precision in diverse scenarios. Our system is currently deployed in
production, processing millions of documents daily across multiple languages
while maintaining both efficiency and accuracy. To facilitate reproducibility
and further research, we release our implementation and evaluation datasets.

</details>


### [201] [A Distributional-Lifting Theorem for PAC Learning](https://arxiv.org/abs/2506.16651)
*Guy Blanc,Jane Lange,Carmen Strassle,Li-Yang Tan*

Key words: 分布学习、PAC学习、提升定理、信息理论、样本复杂性

TL;DR: 该论文提出了一种分布提升定理，能将针对特定分布族有效的学习器升级为适用于任何分布的学习器，同时避免了信息理论上的不可行性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对分布特定学习器的局限性，研究如何将其扩展至更广泛的分布族，增强其通用性和实用性。

Method: 提出一种分布提升定理，避免直接学习目标分布，简化了标准PAC模型中的提升过程。

Result: 新方法在标准PAC模型中有效，支持所有基础分布族，具有更好的样本复杂性和噪声容忍性。

Conclusion: 该方法显著提升了学习器的通用性和效率，避免了信息理论和计算复杂性问题。

Abstract: The apparent difficulty of efficient distribution-free PAC learning has led
to a large body of work on distribution-specific learning. Distributional
assumptions facilitate the design of efficient algorithms but also limit their
reach and relevance. Towards addressing this, we prove a distributional-lifting
theorem: This upgrades a learner that succeeds with respect to a limited
distribution family $\mathcal{D}$ to one that succeeds with respect to any
distribution $D^\star$, with an efficiency overhead that scales with the
complexity of expressing $D^\star$ as a mixture of distributions in
$\mathcal{D}$.
  Recent work of Blanc, Lange, Malik, and Tan considered the special case of
lifting uniform-distribution learners and designed a lifter that uses a
conditional sample oracle for $D^\star$, a strong form of access not afforded
by the standard PAC model. Their approach, which draws on ideas from
semi-supervised learning, first learns $D^\star$ and then uses this information
to lift.
  We show that their approach is information-theoretically intractable with
access only to random examples, thereby giving formal justification for their
use of the conditional sample oracle. We then take a different approach that
sidesteps the need to learn $D^\star$, yielding a lifter that works in the
standard PAC model and enjoys additional advantages: it works for all base
distribution families, preserves the noise tolerance of learners, has better
sample complexity, and is simpler.

</details>


### [202] [Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures](https://arxiv.org/abs/2506.16654)
*Vijay Prakash Dwivedi,Charilaos Kanatsoulis,Shenyang Huang,Jure Leskovec*

Key words: 图机器学习, 关系深度学习, 关系实体图, 多表集成, 异构数据

TL;DR: 该论文综述了关系深度学习（RDL）的蓝图，将关系数据库表示为关系实体图，并探讨了其在图机器学习中的应用与挑战。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统特征工程在大规模多表集成、时间动态和异构数据建模方面存在挑战，RDL提供了一种端到端的表示学习方法。

Method: 介绍了关系数据库的关系实体图表示，总结了用于RDL模型的公共基准数据集，并回顾了针对关系实体图的神经网络方法。

Result: 探讨了RDL如何统一图机器学习中的多个子领域，为关系数据处理设计基础模型。

Conclusion: RDL为关系数据的学习和建模提供了新的方向，并可能推动图机器学习的发展。

Abstract: Graph machine learning has led to a significant increase in the capabilities
of models that learn on arbitrary graph-structured data and has been applied to
molecules, social networks, recommendation systems, and transportation, among
other domains. Data in multi-tabular relational databases can also be
constructed as 'relational entity graphs' for Relational Deep Learning (RDL) -
a new blueprint that enables end-to-end representation learning without
traditional feature engineering. Compared to arbitrary graph-structured data,
relational entity graphs have key properties: (i) their structure is defined by
primary-foreign key relationships between entities in different tables, (ii)
the structural connectivity is a function of the relational schema defining a
database, and (iii) the graph connectivity is temporal and heterogeneous in
nature. In this paper, we provide a comprehensive review of RDL by first
introducing the representation of relational databases as relational entity
graphs, and then reviewing public benchmark datasets that have been used to
develop and evaluate recent GNN-based RDL models. We discuss key challenges
including large-scale multi-table integration and the complexities of modeling
temporal dynamics and heterogeneous data, while also surveying foundational
neural network methods and recent architectural advances specialized for
relational entity graphs. Finally, we explore opportunities to unify these
distinct modeling challenges, highlighting how RDL converges multiple
sub-fields in graph machine learning towards the design of foundation models
that can transform the processing of relational data.

</details>


### [203] [Mesh-Informed Neural Operator : A Transformer Generative Approach](https://arxiv.org/abs/2506.16656)
*Yaozhong Shi,Zachary E. Ross,Domniki Asimaki,Kamyar Azizzadenesheli*

Key words: 功能生成模型,神经网络算子,MINO,标准化评估,深度学习

TL;DR: 介绍了Mesh-Informed Neural Operator (MINO)，一种适用于不规则网格和多样域的功能生成模型，克服了FNO的局限性，并提供了标准化评估指标。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前功能生成模型受限于Fourier Neural Operator (FNO)，仅适用于规则网格和矩形域，限制了其广泛应用。

Method: 提出MINO，结合图神经网络和交叉注意力机制，实现域和离散化的无关性。

Result: MINO扩展了功能生成模型的适用范围，支持生成、逆问题和回归任务，并提供统一的视角整合神经算子与深度学习架构。

Conclusion: MINO为功能空间生成模型提供了更广泛的应用基础，并填补了标准化评估的空白。

Abstract: Generative models in function spaces, situated at the intersection of
generative modeling and operator learning, are attracting increasing attention
due to their immense potential in diverse scientific and engineering
applications. While functional generative models are theoretically domain- and
discretization-agnostic, current implementations heavily rely on the Fourier
Neural Operator (FNO), limiting their applicability to regular grids and
rectangular domains. To overcome these critical limitations, we introduce the
Mesh-Informed Neural Operator (MINO). By leveraging graph neural operators and
cross-attention mechanisms, MINO offers a principled, domain- and
discretization-agnostic backbone for generative modeling in function spaces.
This advancement significantly expands the scope of such models to more diverse
applications in generative, inverse, and regression tasks. Furthermore, MINO
provides a unified perspective on integrating neural operators with general
advanced deep learning architectures. Finally, we introduce a suite of
standardized evaluation metrics that enable objective comparison of functional
generative models, addressing another critical gap in the field.

</details>


### [204] [A Minimalist Optimizer Design for LLM Pretraining](https://arxiv.org/abs/2506.16659)
*Athanasios Glentis,Jiaxiang Li,Andi Han,Mingyi Hong*

Key words: SCALE, 优化器, 内存效率, 大型语言模型, 预训练

TL;DR: 论文提出了一种名为SCALE的新优化器，通过结合列归一化SGD和最后一层动量，显著减少了内存占用，同时保持了或超越了Adam等自适应优化器的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究旨在确定在大型语言模型预训练中，真正需要的最小优化器状态量，以减少内存消耗并保持性能。

Method: 采用自下而上的方法，发现列归一化梯度和输出层动量是两个高效的技术，并基于此提出SCALE优化器。

Result: SCALE在多种LLaMA模型上表现优于Adam和现有内存高效优化器，内存占用仅为35-45%。

Conclusion: SCALE是一种内存高效且性能优越的优化器，适合大规模预训练，并为未来优化器设计提供了基线。

Abstract: Training large language models (LLMs) typically relies on adaptive optimizers
such as Adam, which require significant memory to maintain first- and
second-moment matrices, known as optimizer states. While recent works such as
GaLore, Fira, and APOLLO have proposed state-compressed variants to reduce
memory consumption, a fundamental question remains: What is the minimal amount
of optimizer state that is truly necessary to retain state-of-the-art
performance in LLM pretraining? In this work, we systematically investigate
this question using a bottom-up approach. We find that two memory- and
compute-efficient optimization techniques are particularly effective: (1)
column-wise gradient normalization significantly boosts the performance of
plain SGD without requiring momentum; and (2) adding first-order momentum only
to the output layer - where gradient variance is highest - yields performance
competitive with fully adaptive methods such as Muon. Based on these insights,
we propose SCALE (Stochastic Column-normalized Last-layer Momentum), a new
optimizer that combines column-normalized SGD with last-layer momentum, where
column normalization refers to normalizing the gradient along the output
dimension. Across multiple LLaMA models (60M-1B), SCALE matches or exceeds the
performance of Adam while using only 35-45% of the total memory. It also
consistently outperforms memory-efficient optimizers such as GaLore, Fira, and
APOLLO, making it a strong candidate for large-scale pretraining under memory
constraints. For the LLaMA 7B model, SCALE outperforms the state-of-the-art
method APOLLO in terms of both perplexity and memory consumption. In addition,
our method serves as a minimalist baseline for more sophisticated optimizer
design.

</details>


### [205] [Private Training & Data Generation by Clustering Embeddings](https://arxiv.org/abs/2506.16661)
*Felix Zhou,Samson Zhou,Vahab Mirrokni,Alessandro Epasto,Vincent Cohen-Addad*

Key words: 差分隐私, 高斯混合模型, 合成数据, 深度学习, 隐私保护

TL;DR: 提出一种基于高斯混合模型（GMM）和差分隐私（DP）的合成图像嵌入生成方法，用于保护训练数据隐私。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决深度学习中使用敏感数据时的隐私问题，防止模型记忆和泄露数据。

Method: 通过DP聚类在高维嵌入空间中拟合GMM，生成合成数据集用于训练。

Result: 在标准数据集上实现SOTA分类精度，并能生成逼真的合成图像。

Conclusion: 该方法通用性强、可扩展性高，适用于多种任务。

Abstract: Deep neural networks often use large, high-quality datasets to achieve high
performance on many machine learning tasks. When training involves potentially
sensitive data, this process can raise privacy concerns, as large models have
been shown to unintentionally memorize and reveal sensitive information,
including reconstructing entire training samples. Differential privacy (DP)
provides a robust framework for protecting individual data and in particular, a
new approach to privately training deep neural networks is to approximate the
input dataset with a privately generated synthetic dataset, before any
subsequent training algorithm. We introduce a novel principled method for DP
synthetic image embedding generation, based on fitting a Gaussian Mixture Model
(GMM) in an appropriate embedding space using DP clustering. Our method
provably learns a GMM under separation conditions. Empirically, a simple
two-layer neural network trained on synthetically generated embeddings achieves
state-of-the-art (SOTA) classification accuracy on standard benchmark datasets.
Additionally, we demonstrate that our method can generate realistic synthetic
images that achieve downstream classification accuracy comparable to SOTA
methods. Our method is quite general, as the encoder and decoder modules can be
freely substituted to suit different tasks. It is also highly scalable,
consisting only of subroutines that scale linearly with the number of samples
and/or can be implemented efficiently in distributed systems.

</details>


### [206] [Fast and Stable Diffusion Planning through Variational Adaptive Weighting](https://arxiv.org/abs/2506.16688)
*Zhiying Qiu,Tao Lin*

Key words: 扩散模型, 离线强化学习, 自适应损失加权, 变分最优, 多项式逼近

TL;DR: 本文提出了一种基于变分最优不确定性感知的权重函数，通过封闭式多项式逼近方法，显著提升了扩散模型在离线强化学习中的训练效率和稳定性，实验证明其性能优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 扩散模型在离线强化学习中表现优异，但存在训练成本高、收敛慢的问题，尤其是使用基于变换器的去噪架构时。现有的优化策略如噪声调度、辅助预测目标和自适应损失加权仍面临训练不稳定和效率低的挑战。

Method: 提出一种变分最优不确定性感知的权重函数，并通过封闭式多项式逼近方法进行在线估计，将其集成到扩散规划流程中。

Result: 在Maze2D和Kitchen等标准离线强化学习基准测试中，该方法性能优于现有方法，训练步骤减少了10倍。

Conclusion: 该方法显著提升了扩散模型在离线强化学习中的训练效率和性能，具有实际应用价值。

Abstract: Diffusion models have recently shown promise in offline RL. However, these
methods often suffer from high training costs and slow convergence,
particularly when using transformer-based denoising backbones. While several
optimization strategies have been proposed -- such as modified noise schedules,
auxiliary prediction targets, and adaptive loss weighting -- challenges remain
in achieving stable and efficient training. In particular, existing loss
weighting functions typically rely on neural network approximators, which can
be ineffective in early training phases due to limited generalization capacity
of MLPs when exposed to sparse feedback in the early training stages. In this
work, we derive a variationally optimal uncertainty-aware weighting function
and introduce a closed-form polynomial approximation method for its online
estimation under the flow-based generative modeling framework. We integrate our
method into a diffusion planning pipeline and evaluate it on standard offline
RL benchmarks. Experimental results on Maze2D and Kitchen tasks show that our
method achieves competitive performance with up to 10 times fewer training
steps, highlighting its practical effectiveness.

</details>


### [207] [SIDE: Semantic ID Embedding for effective learning from sequences](https://arxiv.org/abs/2506.16698)
*Dinesh Ramasamy,Shakti Kumar,Chris Cadonic,Jiaxin Yang,Sohini Roychowdhury,Esam Abdel Rhman,Srihari Reddy*

Key words: 推荐系统, 向量量化, 语义ID, VQ-VAE, 离散PCA

TL;DR: 提出了一种基于向量量化（VQ）的新方法，通过生成紧凑的语义ID（SID）替代传统嵌入，解决了大规模推荐系统中存储和推理成本的挑战。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有工业广告推荐系统在处理大规模用户历史数据时，存储和推理成本过高，需一种更高效的解决方案。

Method: 采用多任务VQ-VAE框架（VQ融合）、无参数SID转换技术（SIDE）和增强的离散PCA（DPCA）量化方法。

Result: 在大规模工业广告推荐系统中，相比传统方法，归一化熵增益提升了2.4倍，数据占用减少了3倍。

Conclusion: 所提方法有效降低了推荐系统的资源消耗并提升了性能。

Abstract: Sequence-based recommendations models are driving the state-of-the-art for
industrial ad-recommendation systems. Such systems typically deal with user
histories or sequence lengths ranging in the order of O(10^3) to O(10^4)
events. While adding embeddings at this scale is manageable in pre-trained
models, incorporating them into real-time prediction models is challenging due
to both storage and inference costs. To address this scaling challenge, we
propose a novel approach that leverages vector quantization (VQ) to inject a
compact Semantic ID (SID) as input to the recommendation models instead of a
collection of embeddings. Our method builds on recent works of SIDs by
introducing three key innovations: (i) a multi-task VQ-VAE framework, called VQ
fusion that fuses multiple content embeddings and categorical predictions into
a single Semantic ID; (ii) a parameter-free, highly granular SID-to-embedding
conversion technique, called SIDE, that is validated with two content embedding
collections, thereby eliminating the need for a large parameterized lookup
table; and (iii) a novel quantization method called Discrete-PCA (DPCA) which
generalizes and enhances residual quantization techniques. The proposed
enhancements when applied to a large-scale industrial ads-recommendation system
achieves 2.4X improvement in normalized entropy (NE) gain and 3X reduction in
data footprint compared to traditional SID methods.

</details>


### [208] [How Many Domains Suffice for Domain Generalization? A Tight Characterization via the Domain Shattering Dimension](https://arxiv.org/abs/2506.16704)
*Cynthia Dwork,Lunjia Hu,Han Shao*

Key words: 域泛化, PAC学习, 域破碎维度, VC维度, 样本复杂度

TL;DR: 该论文研究了域泛化中的基本问题，提出了一个新的组合度量——域破碎维度，并证明了其与经典VC维度的定量关系。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨在域泛化中，需要从多少随机采样的域中收集数据才能学习一个在所有域上表现良好的模型。

Method: 在PAC框架中建模问题，引入域破碎维度这一新组合度量，分析其与经典VC维度的关系。

Result: 域破碎维度刻画了域样本复杂度，且与VC维度存在紧密关系，表明PAC可学习的假设类在域泛化中也可学习。

Conclusion: 域破碎维度是域泛化的关键度量，与VC维度的关系扩展了PAC学习理论的应用范围。

Abstract: We study a fundamental question of domain generalization: given a family of
domains (i.e., data distributions), how many randomly sampled domains do we
need to collect data from in order to learn a model that performs reasonably
well on every seen and unseen domain in the family? We model this problem in
the PAC framework and introduce a new combinatorial measure, which we call the
domain shattering dimension. We show that this dimension characterizes the
domain sample complexity. Furthermore, we establish a tight quantitative
relationship between the domain shattering dimension and the classic VC
dimension, demonstrating that every hypothesis class that is learnable in the
standard PAC setting is also learnable in our setting.

</details>


### [209] [TriCon-SF: A Triple-Shuffle and Contribution-Aware Serial Federated Learning Framework for Heterogeneous Healthcare Data](https://arxiv.org/abs/2506.16723)
*Yuping Yan,Yizhi Wang,Yuanshuai Li,Yaochu Jin*

Key words: 联邦学习,隐私保护,随机化,Shapley值,医疗数据

TL;DR: TriCon-SF是一种新颖的串行联邦学习框架，通过三重随机化和贡献感知增强隐私和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决联邦学习中的数据异构性、隐私泄露和恶意客户端行为问题。

Method: 采用三重随机化（模型层、数据段、训练序列）和Shapley值贡献评估。

Result: 在非独立同分布医疗数据上，TriCon-SF在精度和通信效率上优于标准方法。

Conclusion: TriCon-SF有效提升隐私保护和系统鲁棒性。

Abstract: Serial pipeline training is an efficient paradigm for handling data
heterogeneity in cross-silo federated learning with low communication overhead.
However, even without centralized aggregation, direct transfer of models
between clients can violate privacy regulations and remain susceptible to
gradient leakage and linkage attacks. Additionally, ensuring resilience against
semi-honest or malicious clients who may manipulate or misuse received models
remains a grand challenge, particularly in privacy-sensitive domains such as
healthcare. To address these challenges, we propose TriCon-SF, a novel serial
federated learning framework that integrates triple shuffling and contribution
awareness. TriCon-SF introduces three levels of randomization by shuffling
model layers, data segments, and training sequences to break deterministic
learning patterns and disrupt potential attack vectors, thereby enhancing
privacy and robustness. In parallel, it leverages Shapley value methods to
dynamically evaluate client contributions during training, enabling the
detection of dishonest behavior and enhancing system accountability. Extensive
experiments on non-IID healthcare datasets demonstrate that TriCon-SF
outperforms standard serial and parallel federated learning in both accuracy
and communication efficiency. Security analysis further supports its resilience
against client-side privacy attacks.

</details>


### [210] [On Training-Test (Mis)alignment in Unsupervised Combinatorial Optimization: Observation, Empirical Exploration, and Analysis](https://arxiv.org/abs/2506.16732)
*Fanchen Bu,Kijung Shin*

Key words: 无监督组合优化, 去随机化, 训练对齐, 微分方法

TL;DR: 论文探讨无监督组合优化中训练与测试不一致的问题，提出通过微分版本的去随机化改进对齐性，但增加训练挑战。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有无监督组合优化方法在训练和测试阶段存在不一致，导致训练损失低未必能提升去随机化后性能。

Method: 在训练中引入微分版本的去随机化以改进训练与测试的对齐性。

Result: 初步实验显示该方法能改善对齐性，但也带来训练上的新挑战。

Conclusion: 改进训练与测试对齐性需权衡训练复杂度。

Abstract: In unsupervised combinatorial optimization (UCO), during training, one aims
to have continuous decisions that are promising in a probabilistic sense for
each training instance, which enables end-to-end training on initially discrete
and non-differentiable problems. At the test time, for each test instance,
starting from continuous decisions, derandomization is typically applied to
obtain the final deterministic decisions. Researchers have developed more and
more powerful test-time derandomization schemes to enhance the empirical
performance and the theoretical guarantee of UCO methods. However, we notice a
misalignment between training and testing in the existing UCO methods.
Consequently, lower training losses do not necessarily entail better
post-derandomization performance, even for the training instances without any
data distribution shift. Empirically, we indeed observe such undesirable cases.
We explore a preliminary idea to better align training and testing in UCO by
including a differentiable version of derandomization into training. Our
empirical exploration shows that such an idea indeed improves training-test
alignment, but also introduces nontrivial challenges into training.

</details>


### [211] [Optimism Without Regularization: Constant Regret in Zero-Sum Games](https://arxiv.org/abs/2506.16736)
*John Lazarsfeld,Georgios Piliouras,Ryann Sim,Stratis Skoulakis*

Key words: 零和博弈、乐观虚构游戏、正则化、遗憾界限、双空间分析

TL;DR: 本文研究了两人零和博弈中乐观版本的虚构游戏学习。首次证明，即使没有正则化，乐观虚构游戏也能实现常数量级的遗憾，这在非无悔算法中表现出快速学习能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索乐观虚构游戏在非正则化情况下是否能实现快速学习，并比较乐观与交替虚构游戏的性能差异。

Method: 通过双空间几何视角分析乐观虚构游戏，证明其能量函数随时间保持有界。

Result: 乐观虚构游戏在双策略博弈中实现常数量级遗憾；交替虚构游戏的遗憾下限为√T。

Conclusion: 乐观虚构游戏在非正则化下仍高效，且与交替虚构游戏在性能上存在显著差异。

Abstract: This paper studies the optimistic variant of Fictitious Play for learning in
two-player zero-sum games. While it is known that Optimistic FTRL -- a
regularized algorithm with a bounded stepsize parameter -- obtains constant
regret in this setting, we show for the first time that similar, optimal rates
are also achievable without regularization: we prove for two-strategy games
that Optimistic Fictitious Play (using any tiebreaking rule) obtains only
constant regret, providing surprising new evidence on the ability of
non-no-regret algorithms for fast learning in games. Our proof technique
leverages a geometric view of Optimistic Fictitious Play in the dual space of
payoff vectors, where we show a certain energy function of the iterates remains
bounded over time. Additionally, we also prove a regret lower bound of
$\Omega(\sqrt{T})$ for Alternating Fictitious Play. In the unregularized
regime, this separates the ability of optimism and alternation in achieving
$o(\sqrt{T})$ regret.

</details>


### [212] [IsoNet: Causal Analysis of Multimodal Transformers for Neuromuscular Gesture Classification](https://arxiv.org/abs/2506.16744)
*Eion Tyacke,Kunal Gupta,Jay Patel,Rui Li*

Key words: 手势解码, 多模态融合, Transformer, 生物信号, 神经机器人

TL;DR: 研究比较了线性与基于注意力的多模态融合策略，发现基于注意力的分层Transformer在多模态生物信号分类中表现最佳，跨模态交互贡献显著。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解码手势的神经肌肉特征是神经科学和辅助技术（如假肢）的关键瓶颈，多模态融合能利用传感器的互补信息。

Method: 比较了三种架构（多模态MLP、多模态Transformer、分层Transformer）的融合策略，并引入隔离网络量化模态交互贡献。

Result: 基于注意力的分层Transformer在两种数据集上表现最优，跨模态交互贡献约30%的决策信号。

Conclusion: 研究揭示了多模态融合的优势及机制，对神经机器人系统的传感器设计有指导意义。

Abstract: Hand gestures are a primary output of the human motor system, yet the
decoding of their neuromuscular signatures remains a bottleneck for basic
neuroscience and assistive technologies such as prosthetics. Traditional
human-machine interface pipelines rely on a single biosignal modality, but
multimodal fusion can exploit complementary information from sensors. We
systematically compare linear and attention-based fusion strategies across
three architectures: a Multimodal MLP, a Multimodal Transformer, and a
Hierarchical Transformer, evaluating performance on scenarios with unimodal and
multimodal inputs. Experiments use two publicly available datasets: NinaPro DB2
(sEMG and accelerometer) and HD-sEMG 65-Gesture (high-density sEMG and force).
Across both datasets, the Hierarchical Transformer with attention-based fusion
consistently achieved the highest accuracy, surpassing the multimodal and best
single-modality linear-fusion MLP baseline by over 10% on NinaPro DB2 and 3.7%
on HD-sEMG. To investigate how modalities interact, we introduce an Isolation
Network that selectively silences unimodal or cross-modal attention pathways,
quantifying each group of token interactions' contribution to downstream
decisions. Ablations reveal that cross-modal interactions contribute
approximately 30% of the decision signal across transformer layers,
highlighting the importance of attention-driven fusion in harnessing
complementary modality information. Together, these findings reveal when and
how multimodal fusion would enhance biosignal classification and also provides
mechanistic insights of human muscle activities. The study would be beneficial
in the design of sensor arrays for neurorobotic systems.

</details>


### [213] [Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation](https://arxiv.org/abs/2506.16753)
*Kosuke Nakanishi,Akihiro Kubo,Yuji Yasui,Shin Ishii*

Key words: 强化学习, 对抗学习, 离策方法, 软约束优化

TL;DR: 本文提出了一种新的离策方法，通过将对抗学习重新表述为软约束优化问题，避免了额外的环境交互需求。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统强化学习方法在对抗性输入观测下存在漏洞，现有方法虽能部分应对但在长时间范围的最坏场景中效率低下，且难以发展离策方法。

Method: 将对抗学习重新定义为软约束优化问题，并利用策略评估的对称性理论支持。

Result: 提出了一种无需额外环境交互的有效离策方法。

Conclusion: 该新方法通过理论支持和实现优化，解决了对抗学习中环境交互效率低下的问题。

Abstract: Recently, robust reinforcement learning (RL) methods designed to handle
adversarial input observations have received significant attention, motivated
by RL's inherent vulnerabilities. While existing approaches have demonstrated
reasonable success, addressing worst-case scenarios over long time horizons
requires both minimizing the agent's cumulative rewards for adversaries and
training agents to counteract them through alternating learning. However, this
process introduces mutual dependencies between the agent and the adversary,
making interactions with the environment inefficient and hindering the
development of off-policy methods. In this work, we propose a novel off-policy
method that eliminates the need for additional environmental interactions by
reformulating adversarial learning as a soft-constrained optimization problem.
Our approach is theoretically supported by the symmetric property of policy
evaluation between the agent and the adversary. The implementation is available
at https://github.com/nakanakakosuke/VALT_SAC.

</details>


### [214] [Metapath-based Hyperbolic Contrastive Learning for Heterogeneous Graph Embedding](https://arxiv.org/abs/2506.16754)
*Jongmin Park,Seunghoon Han,Won-Yong Shin,Sungsu Lim*

Key words: 双曲空间,异构图嵌入,对比学习,元路径,复杂结构

TL;DR: 提出了一种基于多双曲空间的对比学习框架MHCL，用于捕捉异构图中的多样复杂结构，实验证明其优于现有基线。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有双曲异构图嵌入模型仅使用单一双曲空间，无法有效捕捉图中的多样幂律结构。

Method: 通过在不同双曲空间中学习元路径的分布，并采用对比学习优化元路径嵌入的判别性。

Result: MHCL在多个图机器学习任务中表现优于现有方法。

Conclusion: 多双曲空间和对比学习的结合能够有效捕捉异构图复杂结构。

Abstract: The hyperbolic space, characterized by a constant negative curvature and
exponentially expanding space, aligns well with the structural properties of
heterogeneous graphs. However, although heterogeneous graphs inherently possess
diverse power-law structures, most hyperbolic heterogeneous graph embedding
models rely on a single hyperbolic space. This approach may fail to effectively
capture the diverse power-law structures within heterogeneous graphs. To
address this limitation, we propose a Metapath-based Hyperbolic Contrastive
Learning framework (MHCL), which uses multiple hyperbolic spaces to capture
diverse complex structures within heterogeneous graphs. Specifically, by
learning each hyperbolic space to describe the distribution of complex
structures corresponding to each metapath, it is possible to capture semantic
information effectively. Since metapath embeddings represent distinct semantic
information, preserving their discriminability is important when aggregating
them to obtain node representations. Therefore, we use a contrastive learning
approach to optimize MHCL and improve the discriminability of metapath
embeddings. In particular, our contrastive learning method minimizes the
distance between embeddings of the same metapath and maximizes the distance
between those of different metapaths in hyperbolic space, thereby improving the
separability of metapath embeddings with distinct semantic information. We
conduct comprehensive experiments to evaluate the effectiveness of MHCL. The
experimental results demonstrate that MHCL outperforms state-of-the-art
baselines in various graph machine learning tasks, effectively capturing the
complex structures of heterogeneous graphs.

</details>


### [215] [What Is the Point of Equality in Machine Learning Fairness? Beyond Equality of Opportunity](https://arxiv.org/abs/2506.16782)
*Youjin Kong*

Key words: 机器学习公平性、分配平等、关系平等、结构性不平等、伦理基础

TL;DR: 论文摘要讨论了机器学习公平性的伦理基础，指出当前研究过分依赖分配平等而忽略了结构性不平等，提出了结合分配平等与关系平等的综合框架。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前机器学习公平性研究主要基于分配平等，但未能全面解决结构性不平等问题，需要一个更全面的伦理基础。

Method: 通过引入批判性社会和政治哲学，提出一个结合分配平等与关系平等的多维度平等主义框架。

Result: 该框架能更全面地解决机器学习系统中的分配与表征性伤害，并提出实际实施路径。

Conclusion: 综合分配平等与关系平等的框架为机器学习公平性提供了更全面的伦理基础。

Abstract: Fairness in machine learning (ML) has become a rapidly growing area of
research. But why, in the first place, is unfairness in ML morally wrong? And
why should we care about improving fairness? Most fair-ML research implicitly
appeals to distributive equality: the idea that desirable goods and benefits,
such as opportunities (e.g., Barocas et al., 2023), should be equally
distributed across society. Unfair ML models, then, are seen as wrong because
they unequally distribute such benefits. This paper argues that this exclusive
focus on distributive equality offers an incomplete and potentially misleading
ethical foundation. Grounding ML fairness in egalitarianism -- the view that
equality is a fundamental moral and social ideal -- requires challenging
structural inequality: systematic, institutional, and durable arrangements that
privilege some groups while disadvantaging others. Structural inequality
manifests through ML systems in two primary forms: allocative harms (e.g.,
economic loss) and representational harms (e.g., stereotypes, erasure). While
distributive equality helps address allocative harms, it fails to explain why
representational harms are wrong -- why it is wrong for ML systems to reinforce
social hierarchies that stratify people into superior and inferior groups --
and why ML systems should aim to foster a society where people relate as equals
(i.e., relational equality). To address these limitations, the paper proposes a
multifaceted egalitarian framework for ML fairness that integrates both
distributive and relational equality. Drawing on critical social and political
philosophy, this framework offers a more comprehensive ethical foundation for
tackling the full spectrum of harms perpetuated by ML systems. The paper also
outlines practical pathways for implementing the framework across the ML
pipeline.

</details>


### [216] [Revisiting LoRA through the Lens of Parameter Redundancy: Spectral Encoding Helps](https://arxiv.org/abs/2506.16787)
*Jiashun Cheng,Aochuan Chen,Nuo Chen,Ziqi Gao,Yuhan Li,Jia Li,Fugee Tsung*

Key words: 低秩适应, 光谱编码, 参数冗余, 下游任务

TL;DR: SeLoRA利用稀疏光谱子空间重新参数化LoRA，提升效率与性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: LoRA在微调大型基础模型时存在参数冗余问题，影响了其容量和效率。

Method: 提出了SeLoRA，通过光谱基座重新参数化LoRA，从稀疏光谱子空间中获取鲁棒的表现力。

Result: 实验证明SeLoRA在减少参数的同时提升了效率，并在多个下游任务中表现出色。

Conclusion: SeLoRA作为一种可扩展的即插即用框架，有效解决了LoRA的冗余问题。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a prominent technique for
fine-tuning large foundation models. Despite its successes, the substantial
parameter redundancy, which limits the capacity and efficiency of LoRA, has
been recognized as a bottleneck. In this work, we systematically investigate
the impact of redundancy in fine-tuning LoRA and reveal that reducing density
redundancy does not degrade expressiveness. Based on this insight, we introduce
\underline{S}pectral-\underline{e}ncoding \underline{L}ow-\underline{R}ank
\underline{A}daptation (SeLoRA), which harnesses the robust expressiveness of
spectral bases to re-parameterize LoRA from a sparse spectral subspace.
Designed with simplicity, SeLoRA enables seamless integration with various LoRA
variants for performance boosting, serving as a scalable plug-and-play
framework. Extensive experiments substantiate that SeLoRA achieves greater
efficiency with fewer parameters, delivering superior performance enhancements
over strong baselines on various downstream tasks, including commonsense
reasoning, math reasoning, and code generation.

</details>


### [217] [Exploring and Improving Initialization for Deep Graph Neural Networks: A Signal Propagation Perspective](https://arxiv.org/abs/2506.16790)
*Senmiao Wang,Yupeng Chen,Yushun Zhang,Ruoyu Sun,Tian Ding*

Key words: 图神经网络, 信号传播, 初始化方法, 深度网络, 性能优化

TL;DR: 论文提出了一种针对图神经网络（GNNs）的初始化方法SPoGInit，通过优化信号传播（SP）的三个关键指标，解决了GNNs深度增加时性能下降的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: GNNs在深度增加时性能下降，现有初始化方法无法同时控制信号传播的三个关键指标（前向传播、后向传播和图嵌入变化）。

Method: 提出SPoGInit方法，通过搜索优化权重初始化的方差，同时优化信号传播的三个指标。

Result: SPoGInit在多种任务和架构上优于常用初始化方法，且在GNNs深度增加时仍能提升性能。

Conclusion: SPoGInit为解决GNNs深度相关问题提供了有效方法，验证了信号传播分析框架的有效性。

Abstract: Graph Neural Networks (GNNs) often suffer from performance degradation as the
network depth increases. This paper addresses this issue by introducing
initialization methods that enhance signal propagation (SP) within GNNs. We
propose three key metrics for effective SP in GNNs: forward propagation,
backward propagation, and graph embedding variation (GEV). While the first two
metrics derive from classical SP theory, the third is specifically designed for
GNNs. We theoretically demonstrate that a broad range of commonly used
initialization methods for GNNs, which exhibit performance degradation with
increasing depth, fail to control these three metrics simultaneously. To deal
with this limitation, a direct exploitation of the SP analysis--searching for
weight initialization variances that optimize the three metrics--is shown to
significantly enhance the SP in deep GCNs. This approach is called Signal
Propagation on Graph-guided Initialization (SPoGInit). Our experiments
demonstrate that SPoGInit outperforms commonly used initialization methods on
various tasks and architectures. Notably, SPoGInit enables performance
improvements as GNNs deepen, which represents a significant advancement in
addressing depth-related challenges and highlights the validity and
effectiveness of the SP analysis framework.

</details>


### [218] [TabArena: A Living Benchmark for Machine Learning on Tabular Data](https://arxiv.org/abs/2506.16791)
*Nick Erickson,Lennart Purucker,Andrej Tschalzev,David Holzmüller,Prateek Mutalik Desai,and David Salinas,Frank Hutter*

Key words: TabArena, 表格数据, 基准测试, 深度学习, 梯度提升树, 集成学习

TL;DR: TabArena是一个持续维护的表格数据基准测试系统，旨在解决现有基准测试静态化的问题，通过手动整理数据集和模型、大规模测试及团队维护，推动表格机器学习的前沿。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着深度学习和基础模型在表格数据中的流行，现有基准测试的静态设计无法适应模型更新或缺陷修复，亟需动态化改进。

Method: 引入TabArena，包括手动整理数据集和模型、大规模基准测试、公开排行榜和团队维护。重点关注验证方法和超参数配置对模型性能的影响。

Result: 梯度提升树仍是实用数据集上的强竞争者，但深度学习方法在更多时间和集成条件下表现优异；基础模型在小数据集上表现突出。跨模型集成进一步推动了表格机器学习的先进水平。

Conclusion: TabArena通过动态维护和数据集成，为表格机器学习提供了一个可靠的基准测试平台，并展示了模型集成的潜力。

Abstract: With the growing popularity of deep learning and foundation models for
tabular data, the need for standardized and reliable benchmarks is higher than
ever. However, current benchmarks are static. Their design is not updated even
if flaws are discovered, model versions are updated, or new models are
released. To address this, we introduce TabArena, the first continuously
maintained living tabular benchmarking system. To launch TabArena, we manually
curate a representative collection of datasets and well-implemented models,
conduct a large-scale benchmarking study to initialize a public leaderboard,
and assemble a team of experienced maintainers. Our results highlight the
influence of validation method and ensembling of hyperparameter configurations
to benchmark models at their full potential. While gradient-boosted trees are
still strong contenders on practical tabular datasets, we observe that deep
learning methods have caught up under larger time budgets with ensembling. At
the same time, foundation models excel on smaller datasets. Finally, we show
that ensembles across models advance the state-of-the-art in tabular machine
learning and investigate the contributions of individual models. We launch
TabArena with a public leaderboard, reproducible code, and maintenance
protocols to create a living benchmark available at https://tabarena.ai.

</details>


### [219] [Robust Group Anomaly Detection for Quasi-Periodic Network Time Series](https://arxiv.org/abs/2506.16815)
*Kai Yang,Shaoyu Dou,Pan Luo,Xin Wang,H. Vincent Poor*

Key words: 多变量时间序列、异常检测、Gaussian Mixture Model、seq2GMM、替代优化算法

TL;DR: 提出了一种名为seq2GMM的框架，用于识别网络时间序列数据库中的异常时间序列，并通过优化算法高效训练模型，性能优于现有技术。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 实际中的多变量时间序列通常来自带有传感器和软件的物理对象网络，其信号具有准周期性，但存在周期和长度的变化，需要模型识别异常行为并解释决策。

Method: 提出了sequence to Gaussian Mixture Model（seq2GMM）框架，结合了替代优化算法以高效训练模型。

Result: seq2GMM在多个公共基准数据集上表现出色，显著优于现有异常检测技术，并通过理论分析和数值结果验证了训练算法的收敛性。

Conclusion: seq2GMM是一种有效的异常检测框架，能够识别异常时间序列并提供决策解释。

Abstract: Many real-world multivariate time series are collected from a network of
physical objects embedded with software, electronics, and sensors. The
quasi-periodic signals generated by these objects often follow a similar
repetitive and periodic pattern, but have variations in the period, and come in
different lengths caused by timing (synchronization) errors. Given a multitude
of such quasi-periodic time series, can we build machine learning models to
identify those time series that behave differently from the majority of the
observations? In addition, can the models help human experts to understand how
the decision was made? We propose a sequence to Gaussian Mixture Model
(seq2GMM) framework. The overarching goal of this framework is to identify
unusual and interesting time series within a network time series database. We
further develop a surrogate-based optimization algorithm that can efficiently
train the seq2GMM model. Seq2GMM exhibits strong empirical performance on a
plurality of public benchmark datasets, outperforming state-of-the-art anomaly
detection techniques by a significant margin. We also theoretically analyze the
convergence property of the proposed training algorithm and provide numerical
results to substantiate our theoretical claims.

</details>


### [220] [Predicting New Research Directions in Materials Science using Large Language Models and Concept Graphs](https://arxiv.org/abs/2506.16824)
*Thomas Marwitz,Alexander Colsmann,Ben Breitung,Christoph Brabec,Christoph Kirchlechner,Eva Blasco,Gabriel Cadilha Marques,Horst Hahn,Michael Hirtz,Pavel A. Levkin,Yolita M. Eggeler,Tobias Schlöder,Pascal Friederich*

Key words: 大型语言模型, 概念图, 材料科学, 研究方向, 机器学习

TL;DR: 使用大型语言模型（LLM）从材料科学领域的论文摘要中提取主要概念和语义信息，以发现人类未注意到的联系，并提出启发性的未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于研究文章数量呈指数增长，科学家难以阅读所有出版物，希望通过LLM自动提取关键概念，构建概念图，并预测新兴概念组合。

Method: 采用LLM提取概念并构建概念图，训练机器学习模型基于历史数据预测新兴概念组合。

Result: 研究显示，LLM比自动关键词提取方法更高效，且引入语义概念信息提升了预测性能。模型能为材料科学家提供未探索的创新研究方向。

Conclusion: LLM可通过提取和预测概念组合，启发科研人员的创造性思维，推动未来研究方向。

Abstract: Due to an exponential increase in published research articles, it is
impossible for individual scientists to read all publications, even within
their own research field. In this work, we investigate the use of large
language models (LLMs) for the purpose of extracting the main concepts and
semantic information from scientific abstracts in the domain of materials
science to find links that were not noticed by humans and thus to suggest
inspiring near/mid-term future research directions. We show that LLMs can
extract concepts more efficiently than automated keyword extraction methods to
build a concept graph as an abstraction of the scientific literature. A machine
learning model is trained to predict emerging combinations of concepts, i.e.
new research ideas, based on historical data. We demonstrate that integrating
semantic concept information leads to an increased prediction performance. The
applicability of our model is demonstrated in qualitative interviews with
domain experts based on individualized model suggestions. We show that the
model can inspire materials scientists in their creative thinking process by
predicting innovative combinations of topics that have not yet been
investigated.

</details>


### [221] [FedFitTech: A Baseline in Federated Learning for Fitness Tracking](https://arxiv.org/abs/2506.16840)
*Zeyneddin Oz,Shreyas Korde,Marius Bock,Kristof Van Laerhoven*

Key words: 联邦学习, 健身追踪, 隐私保护, 开源基准

TL;DR: 论文介绍了FedFitTech基准技术，通过联邦学习解决健身追踪设备中的数据隐私和效率问题，并展示了其应用效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统集中式学习方法在处理健身追踪数据时面临隐私、法规和通信效率问题，而联邦学习能够在不共享原始数据的情况下进行模型训练，为解决这些问题提供了可能性。

Method: 提出基于Flower框架的FedFitTech基准技术，采用客户端早期停止策略，并在健身追踪设备中实现系统应用。

Result: 系统减少了13%的冗余通信，同时识别性能仅下降1%，证明了其高效性和实用性。

Conclusion: FedFitTech为健身技术领域的隐私保护和高效学习提供了新的研究基础，并已开源。

Abstract: Rapid evolution of sensors and resource-efficient machine learning models
have spurred the widespread adoption of wearable fitness tracking devices.
Equipped with inertial sensors, such devices can continuously capture physical
movements for fitness technology (FitTech), enabling applications from sports
optimization to preventive healthcare. Traditional centralized learning
approaches to detect fitness activities struggle with privacy concerns,
regulatory constraints, and communication inefficiencies. In contrast,
Federated Learning (FL) enables a decentralized model training by communicating
model updates rather than private wearable sensor data. Applying FL to FitTech
presents unique challenges, such as data imbalance, lack of labelled data,
heterogeneous user activity patterns, and trade-offs between personalization
and generalization. To simplify research on FitTech in FL, we present the
FedFitTech baseline, under the Flower framework, which is publicly available
and widely used by both industry and academic researchers. Additionally, to
illustrate its usage, this paper presents a case study that implements a system
based on the FedFitTech baseline, incorporating a client-side early stopping
strategy and comparing the results. For instance, this system allows wearable
devices to optimize the trade-off between capturing common fitness activity
patterns and preserving individuals' nuances, thereby enhancing both the
scalability and efficiency of privacy-aware fitness tracking applications.
Results show that this reduces overall redundant communications by 13 percent,
while maintaining the overall recognition performance at a negligible
recognition cost by 1 percent. Thus, FedFitTech baseline creates a foundation
for a wide range of new research and development opportunities in FitTech, and
it is available as open-source at:
https://github.com/adap/flower/tree/main/baselines/fedfittech

</details>


### [222] [Bandwidth Selectors on Semiparametric Bayesian Networks](https://arxiv.org/abs/2506.16844)
*Victor Alejandre,Concha Bielza,Pedro Larrañaga*

Key words: 半参数贝叶斯网络, 带宽选择器, 交叉验证, 插件选择器, 非参数密度估计

TL;DR: 该论文研究了半参数贝叶斯网络（SPBNs）中带宽选择器对性能的影响，比较了交叉验证和插件选择器的效果，证明它们在处理非正态数据时优于传统正态规则。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的基于正态规则的带宽选择器在处理偏离正态分布的实际数据时表现不佳，导致密度估计和预测性能下降。因此，研究更先进的带宽选择器对SPBNs性能的提升具有重要意义。

Method: 论文提出了一套理论框架，评估了交叉验证和插件选择器在SPBNs中的应用，并将其整合到开源工具PyBNesian中进行了实验分析。

Result: 结果表明，交叉验证选择器（尤其是无偏交叉验证）在样本量大的情况下优于传统的正态规则，能够更有效地利用数据信息。

Conclusion: 先进的带宽选择器可以显著提升SPBNs在非正态数据下的性能，为实际应用提供了更好的解决方案。

Abstract: Semiparametric Bayesian networks (SPBNs) integrate parametric and
non-parametric probabilistic models, offering flexibility in learning complex
data distributions from samples. In particular, kernel density estimators
(KDEs) are employed for the non-parametric component. Under the assumption of
data normality, the normal rule is used to learn the bandwidth matrix for the
KDEs in SPBNs. This matrix is the key hyperparameter that controls the
trade-off between bias and variance. However, real-world data often deviates
from normality, potentially leading to suboptimal density estimation and
reduced predictive performance. This paper first establishes the theoretical
framework for the application of state-of-the-art bandwidth selectors and
subsequently evaluates their impact on SPBN performance. We explore the
approaches of cross-validation and plug-in selectors, assessing their
effectiveness in enhancing the learning capability and applicability of SPBNs.
To support this investigation, we have extended the open-source package
PyBNesian for SPBNs with the additional bandwidth selection techniques and
conducted extensive experimental analyses. Our results demonstrate that the
proposed bandwidth selectors leverage increasing information more effectively
than the normal rule, which, despite its robustness, stagnates with more data.
In particular, unbiased cross-validation generally outperforms the normal rule,
highlighting its advantage in high sample size scenarios.

</details>


### [223] [Soft decision trees for survival analysis](https://arxiv.org/abs/2506.16846)
*Antonio Consoloa,Edoardo Amaldi,Emilio Carrizosa*

Key words: 生存树、全局优化、软分裂规则、非线性优化、可解释性

TL;DR: 提出了一种新的软生存树模型（SST），通过非线性优化训练，结合了灵活性和可解释性，并在多个数据集上表现优于基准方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的生存树方法因启发式训练方式存在局限性，需要一种全局优化且能保持可解释性的方法。

Method: 使用软分裂规则和非线性优化训练SST，支持多种生存函数形式（参数化、半参数化等）。

Result: 在15个数据集上，SST的表现优于三种基准生存树方法。

Conclusion: SST是一种高效且灵活的生存分析方法，可扩展到群体公平性。

Abstract: Decision trees are popular in survival analysis for their interpretability
and ability to model complex relationships. Survival trees, which predict the
timing of singular events using censored historical data, are typically built
through heuristic approaches. Recently, there has been growing interest in
globally optimized trees, where the overall tree is trained by minimizing the
error function over all its parameters. We propose a new soft survival tree
model (SST), with a soft splitting rule at each branch node, trained via a
nonlinear optimization formulation amenable to decomposition. Since SSTs
provide for every input vector a specific survival function associated to a
single leaf node, they satisfy the conditional computation property and inherit
the related benefits. SST and the training formulation combine flexibility with
interpretability: any smooth survival function (parametric, semiparametric, or
nonparametric) estimated through maximum likelihood can be used, and each leaf
node of an SST yields a cluster of distinct survival functions which are
associated to the data points routed to it. Numerical experiments on 15
well-known datasets show that SSTs, with parametric and spline-based
semiparametric survival functions, trained using an adaptation of the
node-based decomposition algorithm proposed by Consolo et al. (2024) for soft
regression trees, outperform three benchmark survival trees in terms of four
widely-used discrimination and calibration measures. SSTs can also be extended
to consider group fairness.

</details>


### [224] [Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.16853)
*Semin Kim,Yeonwoo Cha,Jaehoon Yoo,Seunghoon Hong*

Key words: 文本到图像生成,提示优化,奖励模型,RATTPO

TL;DR: 提出了一种通用的提示优化方法RATTPO，适用于多种奖励模型，无需修改即可提升文本到图像生成的质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现有提示工程方法在不同奖励模型下表现不佳的问题。

Method: 通过迭代查询大型语言模型（LLM）并结合奖励感知反馈信号优化提示。

Result: RATTPO在搜索效率和性能上优于其他基线方法，减少了推理预算。

Conclusion: RATTPO是一种灵活且高效的提示优化方法。

Abstract: We investigate a general approach for improving user prompts in text-to-image
(T2I) diffusion models by finding prompts that maximize a reward function
specified at test-time. Although diverse reward models are used for evaluating
image generation, existing automated prompt engineering methods typically
target specific reward configurations. Consequently, these specialized designs
exhibit suboptimal performance when applied to new prompt engineering scenarios
involving different reward models. To address this limitation, we introduce
RATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time
optimization method applicable across various reward scenarios without
modification. RATTPO iteratively searches for optimized prompts by querying
large language models (LLMs) \textit{without} requiring reward-specific task
descriptions. Instead, it uses the optimization trajectory and a novel
reward-aware feedback signal (termed a "hint") as context. Empirical results
demonstrate the versatility of RATTPO, effectively enhancing user prompts
across diverse reward setups that assess various generation aspects, such as
aesthetics, general human preference, or spatial relationships between objects.
RATTPO surpasses other test-time search baselines in search efficiency, using
up to 3.5 times less inference budget, and, given sufficient inference budget,
achieves performance comparable to learning-based baselines that require
reward-specific fine-tuning. The code is available at
https://github.com/seminkim/RATTPO.

</details>


### [225] [Anomaly Detection in Event-triggered Traffic Time Series via Similarity Learning](https://arxiv.org/abs/2506.16855)
*Shaoyu Dou,Kai Yang,Yang Jiao,Chengbo Qiu,Kui Ren*

Key words: 时间序列分析, 无监督学习, 相似性度量, 网络安全, 自编码器, 高斯混合模型

TL;DR: 该论文提出了一个无监督学习框架，用于学习事件触发时间序列之间的相似性，结合了层次多分辨率序列自编码器和高斯混合模型，在性能上优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于事件触发时间序列的复杂性，选择合适的相似性度量方法在网络安全任务（如异常检测和聚类）中至关重要。本文旨在开发一种能够有效学习这种相似性的框架。

Method: 采用了层次多分辨率序列自编码器和高斯混合模型（GMM），学习时间序列的低维表示并计算相似性。

Result: 通过实验验证，提出的方法在性能上显著优于现有技术。

Conclusion: 该框架为建模和学习事件触发时间序列的相似性提供了系统化的方法，具有较高的实用性和解释性。

Abstract: Time series analysis has achieved great success in cyber security such as
intrusion detection and device identification. Learning similarities among
multiple time series is a crucial problem since it serves as the foundation for
downstream analysis. Due to the complex temporal dynamics of the
event-triggered time series, it often remains unclear which similarity metric
is appropriate for security-related tasks, such as anomaly detection and
clustering. The overarching goal of this paper is to develop an unsupervised
learning framework that is capable of learning similarities among a set of
event-triggered time series. From the machine learning vantage point, the
proposed framework harnesses the power of both hierarchical multi-resolution
sequential autoencoders and the Gaussian Mixture Model (GMM) to effectively
learn the low-dimensional representations from the time series. Finally, the
obtained similarity measure can be easily visualized for the explanation. The
proposed framework aspires to offer a stepping stone that gives rise to a
systematic approach to model and learn similarities among a multitude of
event-triggered time series. Through extensive qualitative and quantitative
experiments, it is revealed that the proposed method outperforms
state-of-the-art methods considerably.

</details>


### [226] [Optimal Depth of Neural Networks](https://arxiv.org/abs/2506.16862)
*Qian Qi*

Key words: 最优停止问题,残差网络,计算效率,正则化,Transformer

TL;DR: 该论文提出了一种理论框架，将深度神经网络的前向传播重新表述为最优停止问题，通过动态决策平衡计算成本与精度，证明了最优停止深度的有限性，并提出了一种实用的正则化方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决深度神经网络中如何确定最优深度这一基础但具有挑战性的问题，减少资源密集型的实验需求。

Method: 将ResNet的前向传播建模为最优停止问题，利用残差函数的递减收益假设证明最优停止深度的有限性，并提出正则化项$\mathcal{L}_{\rm depth}$。

Result: 在ImageNet上的实证验证表明，该方法在保证模型精度的同时显著提升计算效率。

Conclusion: 该框架为网络深度选择提供了理论支持，并在实际应用中展现出高效性。

Abstract: Determining the optimal depth of a neural network is a fundamental yet
challenging problem, typically resolved through resource-intensive
experimentation. This paper introduces a formal theoretical framework to
address this question by recasting the forward pass of a deep network,
specifically a Residual Network (ResNet), as an optimal stopping problem. We
model the layer-by-layer evolution of hidden representations as a sequential
decision process where, at each layer, a choice is made between halting
computation to make a prediction or continuing to a deeper layer for a
potentially more refined representation. This formulation captures the
intrinsic trade-off between accuracy and computational cost. Our primary
theoretical contribution is a proof that, under a plausible condition of
diminishing returns on the residual functions, the expected optimal stopping
depth is provably finite, even in an infinite-horizon setting. We leverage this
insight to propose a novel and practical regularization term, $\mathcal{L}_{\rm
depth}$, that encourages the network to learn representations amenable to
efficient, early exiting. We demonstrate the generality of our framework by
extending it to the Transformer architecture and exploring its connection to
continuous-depth models via free-boundary problems. Empirical validation on
ImageNet confirms that our regularizer successfully induces the theoretically
predicted behavior, leading to significant gains in computational efficiency
without compromising, and in some cases improving, final model accuracy.

</details>


### [227] [The Importance of Being Lazy: Scaling Limits of Continual Learning](https://arxiv.org/abs/2506.16884)
*Jacopo Graldi,Alessandro Breccia,Giulia Lanzillotta,Thomas Hofmann,Lorenzo Noci*

Key words: 灾难性遗忘, 特征学习, 非平稳环境, 模型规模, 动态平均场理论

TL;DR: 论文探讨了神经网络在非平稳环境中的学习问题，特别是灾难性遗忘（CF），通过模型规模和特征学习的角度分析了其影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管已有研究，神经网络在非平稳环境中的学习仍存在问题，尤其是对灾难性遗忘的理解尚不完整。

Method: 通过区分懒惰和丰富训练机制，结合动态平均场理论，分析了无限宽度模型在特征学习机制中的行为。

Result: 研究发现，增加模型宽度仅在其减少特征学习量时有益；高特征学习仅对任务相似性高时有效。

Conclusion: 神经网络的性能在特征学习的关键水平上达到最优，该水平取决于任务非平稳性，并且具有跨模型规模的普适性。

Abstract: Despite recent efforts, neural networks still struggle to learn in
non-stationary environments, and our understanding of catastrophic forgetting
(CF) is far from complete. In this work, we perform a systematic study on the
impact of model scale and the degree of feature learning in continual learning.
We reconcile existing contradictory observations on scale in the literature, by
differentiating between lazy and rich training regimes through a variable
parameterization of the architecture. We show that increasing model width is
only beneficial when it reduces the amount of feature learning, yielding more
laziness. Using the framework of dynamical mean field theory, we then study the
infinite width dynamics of the model in the feature learning regime and
characterize CF, extending prior theoretical results limited to the lazy
regime. We study the intricate relationship between feature learning, task
non-stationarity, and forgetting, finding that high feature learning is only
beneficial with highly similar tasks. We identify a transition modulated by
task similarity where the model exits an effectively lazy regime with low
forgetting to enter a rich regime with significant forgetting. Finally, our
findings reveal that neural networks achieve optimal performance at a critical
level of feature learning, which depends on task non-stationarity and transfers
across model scales. This work provides a unified perspective on the role of
scale and feature learning in continual learning.

</details>


### [228] [From Lab to Factory: Pitfalls and Guidelines for Self-/Unsupervised Defect Detection on Low-Quality Industrial Images](https://arxiv.org/abs/2506.16890)
*Sebastian Hönel,Jonas Nordqvist*

Key words: 工业检测, 无监督学习, 缺陷检测, 机器学习, 实际应用

TL;DR: 论文探讨了工业产品表面缺陷检测中传统机器学习和无监督方法的局限性，并提出了改进模型和数据质量的实际指导框架。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 工业产品缺陷检测通常依赖人工检查，成本高且易出错。尽管机器学习有潜力替代人工，但现有方法在低质量数据和复杂实际环境中表现不佳。

Method: 评估了两种先进的模型，用于识别和改进生产数据中的质量问题，并提出框架改进模型的稳健性和数据评估方法。

Result: 研究提供了在实际场景中识别模型或数据问题的指导，并展示了基于似然方法的常见缺陷。

Conclusion: 论文提出了更适应实际需求的实证风险估计框架，为工业应用中的缺陷检测提供了实用解决方案。

Abstract: The detection and localization of quality-related problems in industrially
mass-produced products has historically relied on manual inspection, which is
costly and error-prone. Machine learning has the potential to replace manual
handling. As such, the desire is to facilitate an unsupervised (or
self-supervised) approach, as it is often impossible to specify all conceivable
defects ahead of time. A plethora of prior works have demonstrated the aptitude
of common reconstruction-, embedding-, and synthesis-based methods in
laboratory settings. However, in practice, we observe that most methods do not
handle low data quality well or exude low robustness in unfavorable, but
typical real-world settings. For practitioners it may be very difficult to
identify the actual underlying problem when such methods underperform. Worse,
often-reported metrics (e.g., AUROC) are rarely suitable in practice and may
give misleading results. In our setting, we attempt to identify subtle
anomalies on the surface of blasted forged metal parts, using rather
low-quality RGB imagery only, which is a common industrial setting. We
specifically evaluate two types of state-of-the-art models that allow us to
identify and improve quality issues in production data, without having to
obtain new data. Our contribution is to provide guardrails for practitioners
that allow them to identify problems related to, e.g., (lack of) robustness or
invariance, in either the chosen model or the data reliably in similar
scenarios. Furthermore, we exemplify common pitfalls in and shortcomings of
likelihood-based approaches and outline a framework for proper empirical risk
estimation that is more suitable for real-world scenarios.

</details>


### [229] [A deep learning and machine learning approach to predict neonatal death in the context of São Paulo](https://arxiv.org/abs/2506.16929)
*Mohon Raihan,Plabon Kumar Saha,Rajan Das Gupta,A Z M Tahmidul Kabir,Afia Anjum Tamanna,Md. Harun-Ur-Rashid,Adnan Bin Abdus Salam,Md Tanvir Anjum,A Z M Ahteshamul Kabir*

Key words: 新生儿死亡, 机器学习, 深度学习, LSTM, 预测模型

TL;DR: 该论文探讨了利用机器学习和深度学习技术预测新生儿死亡风险，旨在通过早期干预降低死亡率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 新生儿死亡在欠发达和一些发达国家仍是一个严重问题，早期预测高风险婴儿对降低死亡率至关重要。

Method: 使用历史数据（140万新生儿）训练模型，比较了逻辑回归、K近邻、随机森林、XGBoost、CNN和LSTM等算法的表现。

Result: XGBoost和随机森林的准确率为94%，LSTM达到99%，表现最佳。

Conclusion: LSTM是最适合用于预测新生儿是否需要预防措施的模型。

Abstract: Neonatal death is still a concerning reality for underdeveloped and even some
developed countries. Worldwide data indicate that 26.693 babies out of 1,000
births die, according to Macro Trades. To reduce this number, early prediction
of endangered babies is crucial. Such prediction enables the opportunity to
take ample care of the child and mother so that early child death can be
avoided. In this context, machine learning was used to determine whether a
newborn baby is at risk. To train the predictive model, historical data of 1.4
million newborns was used. Machine learning and deep learning techniques such
as logical regression, K-nearest neighbor, random forest classifier, extreme
gradient boosting (XGBoost), convolutional neural network, and long short-term
memory (LSTM) were implemented using the dataset to identify the most accurate
model for predicting neonatal mortality. Among the machine learning algorithms,
XGBoost and random forest classifier achieved the best accuracy with 94%, while
among the deep learning models, LSTM delivered the highest accuracy with 99%.
Therefore, using LSTM appears to be the most suitable approach to predict
whether precautionary measures for a child are necessary.

</details>


### [230] [RocketStack: A level-aware deep recursive ensemble learning framework with exploratory feature fusion and model pruning dynamics](https://arxiv.org/abs/2506.16965)
*Çağatay Demirel*

Key words: 集成学习,深度堆叠,特征压缩,随机化,递归集成

TL;DR: RocketStack是一种新颖的深度递归集成框架，通过逐层修剪较弱的学习器、添加高斯噪声缓解性能饱和，并探索特征压缩技术，显著提升了集成深度和性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统集成学习注重横向多样性而非递归深度，且面临模型复杂度、特征冗余和计算负担等挑战。

Method: 提出RocketStack框架，支持多达十层的递归集成，逐层修剪弱学习器，采用高斯噪声优化OOF分数，并探索多种特征压缩技术（如注意力选择、SFE滤波器和自编码器）。

Result: 在33个数据集上验证，深度集成显著提升性能，二元分类任务中SFE压缩搭配噪声优化达到97.08%准确率，多分类任务中注意力选择达到98.60%准确率，同时大幅降低运行时间和特征维度。

Conclusion: 轻度随机化和周期性压缩是有效的正则化和稳定策略，RocketStack通过类似多级火箭的设计（修剪、压缩、推进），实现了高效深度集成。

Abstract: Ensemble learning remains a cornerstone of machine learning, with stacking
used to integrate predictions from multiple base learners through a meta-model.
However, deep stacking remains rare, as most designs prioritize horizontal
diversity over recursive depth due to model complexity, feature redundancy, and
computational burden. To address these challenges, RocketStack, a level-aware
recursive ensemble framework, is introduced and explored up to ten stacking
levels, extending beyond prior architectures. The framework incrementally
prunes weaker learners at each level, enabling deeper stacking without
excessive complexity. To mitigate early performance saturation, mild Gaussian
noise is added to out-of-fold (OOF) scores before pruning, and compared against
strict OOF pruning. Further both per-level and periodic feature compressions
are explored using attention-based selection, Simple, Fast, Efficient (SFE)
filter, and autoencoders. Across 33 datasets (23 binary, 10 multi-class),
linear-trend tests confirmed rising accuracy with depth in most variants, and
the top performing meta-model at each level increasingly outperformed the
strongest standalone ensemble. In the binary subset, periodic SFE with mild
OOF-score randomization reached 97.08% at level 10, 5.14% above the
strict-pruning configuration and cut runtime by 10.5% relative to no
compression. In the multi-class subset, periodic attention selection reached
98.60% at level 10, exceeding the strongest baseline by 6.11%, while reducing
runtime by 56.1% and feature dimensionality by 74% compared to no compression.
These findings highlight mild randomization as an effective regularizer and
periodic compression as a stabilizer. Echoing the design of multistage rockets
in aerospace (prune, compress, propel) RocketStack achieves deep recursive
ensembling with tractable complexity.

</details>


### [231] [Robust Reinforcement Learning for Discrete Compositional Generation via General Soft Operators](https://arxiv.org/abs/2506.17007)
*Marco Jiralerspong,Esther Derman,Danilo Vucetic,Nikolay Malkin,Bilun Sun,Tianyu Zhang,Pierre-Luc Bacon,Gauthier Gidel*

Key words: 强化学习, 代理奖励函数, 鲁棒性, 组合生成, 多样候选

TL;DR: 本文提出了一种鲁棒的强化学习方法，通过统一操作符应对代理奖励函数的不确定性，从而在大型搜索空间中生成更高质量的候选对象。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 科学发现中常需要从大量组合对象中筛选出少量有潜力的候选对象。传统方法依赖专家知识或基于奖励函数的采样，但后者因奖励函数的不确定性导致效果不佳。

Method: 提出了一种鲁棒的强化学习方法，引入统一操作符以应对奖励函数的不确定性，生成更尖锐的采样分布，并开发了新算法。

Result: 在合成和现实任务中，新算法能够识别出更高质量和多样性的候选对象。

Conclusion: 本文为离散组合生成任务提供了新的灵活解决方案。

Abstract: A major bottleneck in scientific discovery involves narrowing a large
combinatorial set of objects, such as proteins or molecules, to a small set of
promising candidates. While this process largely relies on expert knowledge,
recent methods leverage reinforcement learning (RL) to enhance this filtering.
They achieve this by estimating proxy reward functions from available datasets
and using regularization to generate more diverse candidates. These reward
functions are inherently uncertain, raising a particularly salient challenge
for scientific discovery. In this work, we show that existing methods, often
framed as sampling proportional to a reward function, are inadequate and yield
suboptimal candidates, especially in large search spaces. To remedy this issue,
we take a robust RL approach and introduce a unified operator that seeks
robustness to the uncertainty of the proxy reward function. This general
operator targets peakier sampling distributions while encompassing known soft
RL operators. It also leads us to a novel algorithm that identifies
higher-quality, diverse candidates in both synthetic and real-world tasks.
Ultimately, our work offers a new, flexible perspective on discrete
compositional generation tasks. Code: https://github.com/marcojira/tgm.

</details>


### [232] [The Hidden Cost of an Image: Quantifying the Energy Consumption of AI Image Generation](https://arxiv.org/abs/2506.17016)
*Giulia Bertazzini,Chiara Albisani,Daniele Baracchi,Dasara Shullani,Roberto Verdecchia*

Key words: AI图像生成，能耗评估，环境资源，模型量化，图像质量

TL;DR: 研究评估了17种先进AI图像生成模型的能耗，发现能耗差异高达46倍，分辨率、模型类型等因素影响能耗，而量化反而降低能效。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着AI图像生成的广泛应用及其对环境资源的巨大需求，研究旨在揭示每张生成图像背后的环境影响。

Method: 实验对比了17种模型，考察了模型量化、图像分辨率、提示长度等因素对能耗的影响，并结合图像质量指标分析。

Result: 模型能耗差异显著（最高46倍），分辨率影响不一致，U-Net能效优于Transformer，量化降低能效，提示长度无显著影响。

Conclusion: 提高图像质量未必增加能耗，某些高质量模型反而更节能。

Abstract: With the growing adoption of AI image generation, in conjunction with the
ever-increasing environmental resources demanded by AI, we are urged to answer
a fundamental question: What is the environmental impact hidden behind each
image we generate? In this research, we present a comprehensive empirical
experiment designed to assess the energy consumption of AI image generation.
Our experiment compares 17 state-of-the-art image generation models by
considering multiple factors that could affect their energy consumption, such
as model quantization, image resolution, and prompt length. Additionally, we
consider established image quality metrics to study potential trade-offs
between energy consumption and generated image quality. Results show that image
generation models vary drastically in terms of the energy they consume, with up
to a 46x difference. Image resolution affects energy consumption
inconsistently, ranging from a 1.3x to 4.7x increase when doubling resolution.
U-Net-based models tend to consume less than Transformer-based one. Model
quantization instead results to deteriorate the energy efficiency of most
models, while prompt length and content have no statistically significant
impact. Improving image quality does not always come at the cost of a higher
energy consumption, with some of the models producing the highest quality
images also being among the most energy efficient ones.

</details>


### [233] [Scalable and Reliable Multi-agent Reinforcement Learning for Traffic Assignment](https://arxiv.org/abs/2506.17029)
*Leizhen Wang,Peibo Duan,Cheng Lyu,Zewen Wang,Zhiqiang He,Nan Zheng,Zhenliang Ma*

Key words: 多智能体强化学习, 交通分配, 起讫点对, 可扩展性, 可靠性

TL;DR: 该论文提出了一种新型多智能体强化学习框架MARL-OD-DA，用于解决大规模交通分配问题，通过定义为起讫点对路由的智能体，提升了可扩展性和可靠性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于大都市的发展和出行需求的增加，传统的交通分配方法难以满足需求，而现有MARL方法在大规模网络中面临可扩展性和可靠性问题。

Method: 提出了MARL-OD-DA框架，将智能体定义为起讫点对路由；设计了基于Dirichlet的动作空间和基于局部相对差距的奖励函数。

Result: 实验表明，该框架能有效处理中型网络的城市级OD需求，SiouxFalls网络中相对差距降低了94.99%。

Conclusion: MARL-OD-DA在解决大规模交通分配问题上具有显著优势，尤其在可扩展性和收敛效率方面表现突出。

Abstract: The evolution of metropolitan cities and the increase in travel demands
impose stringent requirements on traffic assignment methods. Multi-agent
reinforcement learning (MARL) approaches outperform traditional methods in
modeling adaptive routing behavior without requiring explicit system dynamics,
which is beneficial for real-world deployment. However, MARL frameworks face
challenges in scalability and reliability when managing extensive networks with
substantial travel demand, which limiting their practical applicability in
solving large-scale traffic assignment problems. To address these challenges,
this study introduces MARL-OD-DA, a new MARL framework for the traffic
assignment problem, which redefines agents as origin-destination (OD) pair
routers rather than individual travelers, significantly enhancing scalability.
Additionally, a Dirichlet-based action space with action pruning and a reward
function based on the local relative gap are designed to enhance solution
reliability and improve convergence efficiency. Experiments demonstrate that
the proposed MARL framework effectively handles medium-sized networks with
extensive and varied city-level OD demand, surpassing existing MARL methods.
When implemented in the SiouxFalls network, MARL-OD-DA achieves better
assignment solutions in 10 steps, with a relative gap that is 94.99% lower than
that of conventional methods.

</details>


### [234] [Critical Appraisal of Fairness Metrics in Clinical Predictive AI](https://arxiv.org/abs/2506.17035)
*João Matos,Ben Van Calster,Leo Anthony Celi,Paula Dhiman,Judy Wawira Gichoya,Richard D. Riley,Chris Russell,Sara Khalid,Gary S. Collins*

Key words: 人工智能；公平性；临床预测；范围综述；度量

TL;DR: 该论文通过综述临床预测性AI的公平性度量，揭示了公平性定义不清、度量碎片化及缺乏临床验证的问题，并提出了未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了解决AI在临床实践中可能存在的偏见问题，并清晰定义和量化‘公平性’。

Method: 通过对五个数据库（2014-2024年）的文献进行范围综述，筛选了820篇记录，纳入41项研究，提取了62种公平性度量。

Result: 发现公平性度量呈现出碎片化现象，缺乏临床验证，且过度依赖阈值依赖的度量。仅18种度量是专门为医疗领域开发的，其中只有一种是临床实用性度量。

Conclusion: 公平性的定义和量化存在概念挑战，且在实际应用中存在不确定性量化、交叉性和现实适用性方面的不足。未来应优先开发临床意义明确的度量。

Abstract: Predictive artificial intelligence (AI) offers an opportunity to improve
clinical practice and patient outcomes, but risks perpetuating biases if
fairness is inadequately addressed. However, the definition of "fairness"
remains unclear. We conducted a scoping review to identify and critically
appraise fairness metrics for clinical predictive AI. We defined a "fairness
metric" as a measure quantifying whether a model discriminates (societally)
against individuals or groups defined by sensitive attributes. We searched five
databases (2014-2024), screening 820 records, to include 41 studies, and
extracted 62 fairness metrics. Metrics were classified by
performance-dependency, model output level, and base performance metric,
revealing a fragmented landscape with limited clinical validation and
overreliance on threshold-dependent measures. Eighteen metrics were explicitly
developed for healthcare, including only one clinical utility metric. Our
findings highlight conceptual challenges in defining and quantifying fairness
and identify gaps in uncertainty quantification, intersectionality, and
real-world applicability. Future work should prioritise clinically meaningful
metrics.

</details>


### [235] [LSCD: Lomb-Scargle Conditioned Diffusion for Time series Imputation](https://arxiv.org/abs/2506.17039)
*Elizabeth Fons,Alejandro Sztrajman,Yousef El-Laham,Luciana Ferrer,Svitlana Vyetrenko,Manuela Veloso*

Key words: 时间序列，不规则采样，Lomb-Scargle，扩散模型，频率分析

TL;DR: 提出了一种基于Lomb-Scargle的可微分层，用于处理不规则采样时间序列的频率分析，并将其与扩散模型结合，实现了更精确的数据填补和频率估计。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决不规则采样时间序列在机器学习中的处理难题，尤其是FFT方法因均匀采样假设导致的局限性。

Method: 引入可微分Lomb-Scargle层计算不规则数据的功率谱，并构建基于扩散模型的LSCD方法进行数据填补。

Result: 在合成和真实数据集上，方法比时域基线更准确地恢复缺失数据，并提供一致的频率估计。

Conclusion: 该方法易于集成到学习框架中，推动频谱引导在不完整或不规则数据机器学习中的应用。

Abstract: Time series with missing or irregularly sampled data are a persistent
challenge in machine learning. Many methods operate on the frequency-domain,
relying on the Fast Fourier Transform (FFT) which assumes uniform sampling,
therefore requiring prior interpolation that can distort the spectra. To
address this limitation, we introduce a differentiable Lomb--Scargle layer that
enables a reliable computation of the power spectrum of irregularly sampled
data. We integrate this layer into a novel score-based diffusion model (LSCD)
for time series imputation conditioned on the entire signal spectrum.
Experiments on synthetic and real-world benchmarks demonstrate that our method
recovers missing data more accurately than purely time-domain baselines, while
simultaneously producing consistent frequency estimates. Crucially, our method
can be easily integrated into learning frameworks, enabling broader adoption of
spectral guidance in machine learning approaches involving incomplete or
irregular data.

</details>


### [236] [MAWIFlow Benchmark: Realistic Flow-Based Evaluation for Network Intrusion Detection](https://arxiv.org/abs/2506.17041)
*Joshua Schraven,Alexander Windmann,Oliver Niggemann*

Key words: 网络入侵检测、基准数据集、MAWIFlow、CNN-BiLSTM、时间漂移

TL;DR: 本文介绍了MAWIFlow，一个基于MAWILAB v1.1数据集的流量基准，用于评估异常检测方法。通过与传统机器学习方法和深度学习模型的比较，展示了其在实际环境中的优势和重要性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有网络入侵检测基准数据集通常依赖合成流量，无法反映实际环境中的统计变异和时间漂移，因此需要更真实且可复现的评估方法。

Method: 通过可复现的预处理流程，将原始数据包转换为符合CICFlowMeter格式的流量表示，保留异常标签，并比较传统机器学习方法和CNN-BiLSTM深度学习模型的性能。

Result: 传统方法在静态数据上表现良好，但随时间推移性能显著下降；CNN-BiLSTM模型表现更稳定，泛化能力更强。

Conclusion: 合成基准和静态模型存在局限性，应采用具有明确时间结构的真实数据集。

Abstract: Benchmark datasets for network intrusion detection commonly rely on
synthetically generated traffic, which fails to reflect the statistical
variability and temporal drift encountered in operational environments. This
paper introduces MAWIFlow, a flow-based benchmark derived from the MAWILAB v1.1
dataset, designed to enable realistic and reproducible evaluation of anomaly
detection methods. A reproducible preprocessing pipeline is presented that
transforms raw packet captures into flow representations conforming to the
CICFlowMeter format, while preserving MAWILab's original anomaly labels. The
resulting datasets comprise temporally distinct samples from January 2011,
2016, and 2021, drawn from trans-Pacific backbone traffic.
  To establish reference baselines, traditional machine learning methods,
including Decision Trees, Random Forests, XGBoost, and Logistic Regression, are
compared to a deep learning model based on a CNN-BiLSTM architecture. Empirical
results demonstrate that tree-based classifiers perform well on temporally
static data but experience significant performance degradation over time. In
contrast, the CNN-BiLSTM model maintains better performance, thus showing
improved generalization. These findings underscore the limitations of synthetic
benchmarks and static models, and motivate the adoption of realistic datasets
with explicit temporal structure. All datasets, pipeline code, and model
implementations are made publicly available to foster transparency and
reproducibility.

</details>


### [237] [Navigating the Deep: Signature Extraction on Deep Neural Networks](https://arxiv.org/abs/2506.17047)
*Haolin Liu,Adrien Siproudhis,Samuel Experton,Peter Lorenz,Christina Boura,Thomas Peyrin*

Key words: 神经网络模型提取,签名提取,秩不足,噪声传播,ReLU

TL;DR: 本文提出了一种改进的神经网络参数提取方法，解决了此前方法在深度网络中的局限性，显著提高了提取深度和准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对现有神经网络参数提取方法在深层网络中因秩不足和噪声传播等问题表现不佳的问题进行研究。

Method: 通过系统分析并解决Carlini等人方法的局限性，提出高效的算法解决方案，改进签名提取过程。

Result: 在CIFAR-10数据集上，成功提取了八层神经网络的参数，准确率超过95%，远超此前仅能提取前三层的方法。

Conclusion: 该方法为针对更复杂神经网络架构的实际攻击提供了重要基础。

Abstract: Neural network model extraction has emerged in recent years as an important
security concern, as adversaries attempt to recover a network's parameters via
black-box queries. A key step in this process is signature extraction, which
aims to recover the absolute values of the network's weights layer by layer.
Prior work, notably by Carlini et al. (2020), introduced a technique inspired
by differential cryptanalysis to extract neural network parameters. However,
their method suffers from several limitations that restrict its applicability
to networks with a few layers only. Later works focused on improving sign
extraction, but largely relied on the assumption that signature extraction
itself was feasible.
  In this work, we revisit and refine the signature extraction process by
systematically identifying and addressing for the first time critical
limitations of Carlini et al.'s signature extraction method. These limitations
include rank deficiency and noise propagation from deeper layers. To overcome
these challenges, we propose efficient algorithmic solutions for each of the
identified issues, greatly improving the efficiency of signature extraction.
Our approach permits the extraction of much deeper networks than was previously
possible. We validate our method through extensive experiments on ReLU-based
neural networks, demonstrating significant improvements in extraction depth and
accuracy. For instance, our extracted network matches the target network on at
least 95% of the input space for each of the eight layers of a neural network
trained on the CIFAR-10 dataset, while previous works could barely extract the
first three layers. Our results represent a crucial step toward practical
attacks on larger and more complex neural network architectures.

</details>


### [238] [Flow-Based Non-stationary Temporal Regime Causal Structure Learning](https://arxiv.org/abs/2506.17065)
*Abdellah Rahmani,Pascal Frossard*

Key words: 因果发现, 多变量时间序列, 非平稳性, 异方差噪声, FANTOM

TL;DR: 论文提出了FANTOM框架，解决多变量时间序列中的因果推断问题，特别是针对非平稳过程和复杂噪声分布的情况。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多变量时间序列中的因果关系分析在金融和神经科学等领域至关重要，但现有方法无法处理非平稳性和复杂噪声分布的问题。

Method: 提出FANTOM框架，使用贝叶斯期望最大化算法，同时推断政权数量及其索引，并学习每个政权的有向无环图。

Result: 理论证明FANTOM在平稳和非平稳设置下是可识别的，实验表明其在合成和真实数据上优于现有方法。

Conclusion: FANTOM有效地解决了非平稳性和复杂噪声分布下的因果发现问题，具有理论和实践优势。

Abstract: Understanding causal relationships in multivariate time series is crucial in
many scenarios, such as those dealing with financial or neurological data. Many
such time series exhibit multiple regimes, i.e., consecutive temporal segments
with a priori unknown boundaries, with each regime having its own causal
structure. Inferring causal dependencies and regime shifts is critical for
analyzing the underlying processes. However, causal structure learning in this
setting is challenging due to (1) non stationarity, i.e., each regime can have
its own causal graph and mixing function, and (2) complex noise distributions,
which may be non Gaussian or heteroscedastic. Existing causal discovery
approaches cannot address these challenges, since generally assume stationarity
or Gaussian noise with constant variance. Hence, we introduce FANTOM, a unified
framework for causal discovery that handles non stationary processes along with
non Gaussian and heteroscedastic noises. FANTOM simultaneously infers the
number of regimes and their corresponding indices and learns each regime's
Directed Acyclic Graph. It uses a Bayesian Expectation Maximization algorithm
that maximizes the evidence lower bound of the data log likelihood. On the
theoretical side, we prove, under mild assumptions, that temporal
heteroscedastic causal models, introduced in FANTOM's formulation, are
identifiable in both stationary and non stationary settings. In addition,
extensive experiments on synthetic and real data show that FANTOM outperforms
existing methods.

</details>


### [239] [Identifiability of Deep Polynomial Neural Networks](https://arxiv.org/abs/2506.17093)
*Konstantin Usevich,Clara Dérand,Ricardo Borsoi,Marianne Clausel*

Key words: 多项式神经网络, 可辨识性, 低秩张量分解, 神经多样性

TL;DR: 该论文全面分析了深度多项式神经网络（PNNs）的可辨识性，揭示了激活度与层宽度之间的复杂关系，并提出了可辨识性的通用条件和有效条件。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多项式神经网络具有丰富的代数和几何结构，但其可辨识性（确保可解释性的关键属性）仍未被充分理解。

Method: 通过构造性证明，将深度PNNs与低秩张量分解及Kruskal型唯一性定理联系起来，分析了不同架构的可辨识性。

Result: 结果显示，非递增层宽度的架构在温和条件下通常是可辨识的，而编码-解码网络在解码器宽度增长不快时也是可辨识的。

Conclusion: 论文解决了PNNs神经多样性的一个开放猜想，并提供了激活度所需的新界限。

Abstract: Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric
structure. However, their identifiability -- a key property for ensuring
interpretability -- remains poorly understood. In this work, we present a
comprehensive analysis of the identifiability of deep PNNs, including
architectures with and without bias terms. Our results reveal an intricate
interplay between activation degrees and layer widths in achieving
identifiability. As special cases, we show that architectures with
non-increasing layer widths are generically identifiable under mild conditions,
while encoder-decoder networks are identifiable when the decoder widths do not
grow too rapidly. Our proofs are constructive and center on a connection
between deep PNNs and low-rank tensor decompositions, and Kruskal-type
uniqueness theorems. This yields both generic conditions determined by the
architecture, and effective conditions that depend on the network's parameters.
We also settle an open conjecture on the expected dimension of PNN's
neurovarieties, and provide new bounds on the activation degrees required for
it to reach its maximum.

</details>


### [240] [TransDreamerV3: Implanting Transformer In DreamerV3](https://arxiv.org/abs/2506.17103)
*Shruti Sadanand Dongare,Amun Kharel,Jonathan Samuel,Xiaona Zhou*

Key words: TransDreamerV3, Transformer, 强化学习, DreamerV3, Atari, Crafter

TL;DR: TransDreamerV3是一个结合Transformer编码器的强化学习模型，旨在提升复杂环境中的记忆和决策能力，实验显示其在部分Atari和Crafter任务中表现优于DreamerV3。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过结合Transformer架构增强DreamerV3模型，以提升在复杂环境中的性能。

Method: 在DreamerV3架构中集成Transformer编码器，并在Atari和Crafter任务上测试。

Result: 在Atari-Freeway和Crafter任务中表现优于DreamerV3，但在Minecraft任务中存在问题和训练局限性。

Conclusion: TransDreamerV3展示了在基于世界模型的强化学习中Transformer架构的潜力。

Abstract: This paper introduces TransDreamerV3, a reinforcement learning model that
enhances the DreamerV3 architecture by integrating a transformer encoder. The
model is designed to improve memory and decision-making capabilities in complex
environments. We conducted experiments on Atari-Boxing, Atari-Freeway,
Atari-Pong, and Crafter tasks, where TransDreamerV3 demonstrated improved
performance over DreamerV3, particularly in the Atari-Freeway and Crafter
tasks. While issues in the Minecraft task and limited training across all tasks
were noted, TransDreamerV3 displays advancement in world model-based
reinforcement learning, leveraging transformer architectures.

</details>


### [241] [Rapid and Continuous Trust Evaluation for Effective Task Collaboration Through Siamese Model](https://arxiv.org/abs/2506.17128)
*Botao Zhu,Xianbin Wang*

Key words: 信任评估, 协作系统, Siamese模型, 属性控制流图, 动态资源

TL;DR: 提出了一种基于Siamese模型的快速连续信任评估框架（SRCTE），用于协作系统中动态信任评估，实验证明其高效且准确性高。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 协作系统中信任评估面临分布式设备、复杂环境和动态资源的挑战，需要快速且连续的信任评价方法。

Method: 使用属性控制流图（ACFG）表示信任相关数据，并采用Siamese模型学习语义信息，通过嵌入相似度计算信任值。

Result: 实验显示SRCTE在少量数据下快速收敛，且异常信任检测率优于基线算法。

Conclusion: SRCTE框架能有效支持协作系统中的动态信任评估，具有实际应用价值。

Abstract: Trust is emerging as an effective tool to ensure the successful completion of
collaborative tasks within collaborative systems. However, rapidly and
continuously evaluating the trustworthiness of collaborators during task
execution is a significant challenge due to distributed devices, complex
operational environments, and dynamically changing resources. To tackle this
challenge, this paper proposes a Siamese-enabled rapid and continuous trust
evaluation framework (SRCTE) to facilitate effective task collaboration. First,
the communication and computing resource attributes of the collaborator in a
trusted state, along with historical collaboration data, are collected and
represented using an attributed control flow graph (ACFG) that captures
trust-related semantic information and serves as a reference for comparison
with data collected during task execution. At each time slot of task execution,
the collaborator's communication and computing resource attributes, as well as
task completion effectiveness, are collected in real time and represented with
an ACFG to convey their trust-related semantic information. A Siamese model,
consisting of two shared-parameter Structure2vec networks, is then employed to
learn the deep semantics of each pair of ACFGs and generate their embeddings.
Finally, the similarity between the embeddings of each pair of ACFGs is
calculated to determine the collaborator's trust value at each time slot. A
real system is built using two Dell EMC 5200 servers and a Google Pixel 8 to
test the effectiveness of the proposed SRCTE framework. Experimental results
demonstrate that SRCTE converges rapidly with only a small amount of data and
achieves a high anomaly trust detection rate compared to the baseline
algorithm.

</details>


### [242] [Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models](https://arxiv.org/abs/2506.17139)
*Michael Plainer,Hao Wu,Leon Klein,Stephan Günnemann,Frank Noé*

Key words: 扩散模型,Fokker-Planck方程,Boltzmann模拟器,分子动力学

TL;DR: 扩散模型在生物化学等领域表现出色，但在分子动力学模拟中揭示出采样与模拟的不一致性。本文提出一种基于能量的扩散模型，通过Fokker-Planck方程正则化确保一致性，并在多个系统中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 扩散模型在生成平衡分子构象和相关力方面表现出色，但用于分子动力学模拟时发现采样与模拟的不一致性，尤其在小的扩散时间步长下未能满足Fokker-Planck方程。

Method: 提出一种基于能量的扩散模型，引入Fokker-Planck导出的正则化项以确保一致性。

Result: 在玩具系统、丙氨酸二肽等实验中验证了模型的有效性，并开发了一种支持模拟的先进Boltzmann模拟器。

Conclusion: 通过正则化改进扩散模型，显著提高了采样与模拟的一致性，为分子动力学模拟提供了高效工具。

Abstract: Diffusion models have recently gained significant attention due to their
effectiveness in various scientific domains, including biochemistry. When
trained on equilibrium molecular distributions, diffusion models provide both:
a generative procedure to sample equilibrium conformations and associated
forces derived from the model's scores. However, using the forces for
coarse-grained molecular dynamics simulations uncovers inconsistencies in the
samples generated via classical diffusion inference and simulation, despite
both originating from the same model. Particularly at the small diffusion
timesteps required for simulations, diffusion models fail to satisfy the
Fokker-Planck equation, which governs how the score should evolve over time. We
interpret this deviation as an indication of the observed inconsistencies and
propose an energy-based diffusion model with a Fokker-Planck-derived
regularization term enforcing consistency. We demonstrate the effectiveness of
our approach on toy systems, alanine dipeptide, and introduce a
state-of-the-art transferable Boltzmann emulator for dipeptides that supports
simulation and demonstrates enhanced consistency and efficient sampling.

</details>


### [243] [Sparse-Reg: Improving Sample Complexity in Offline Reinforcement Learning using Sparsity](https://arxiv.org/abs/2506.17155)
*Samin Yeasar Arnob,Scott Fujimoto,Doina Precup*

Key words: 离线强化学习, 小数据集, 过拟合, 正则化, 稀疏性

TL;DR: 该论文研究了小数据集在离线强化学习中的应用，提出了一种基于稀疏性的正则化技术（Sparse-Reg），以解决过拟合并提升小数据集上的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 许多离线强化学习基准使用超百万数据点，但实际应用中数据集往往较小，导致算法容易过拟合，性能下降。

Method: 引入了基于稀疏性的正则化技术Sparse-Reg，以减少过拟合，适用于有限数据环境。

Result: 在连续控制任务中，Sparse-Reg优于现有最先进的基线方法。

Conclusion: Sparse-Reg能有效解决小数据集下的过拟合问题，提升离线强化学习的性能。

Abstract: In this paper, we investigate the use of small datasets in the context of
offline reinforcement learning (RL). While many common offline RL benchmarks
employ datasets with over a million data points, many offline RL applications
rely on considerably smaller datasets. We show that offline RL algorithms can
overfit on small datasets, resulting in poor performance. To address this
challenge, we introduce "Sparse-Reg": a regularization technique based on
sparsity to mitigate overfitting in offline reinforcement learning, enabling
effective learning in limited data settings and outperforming state-of-the-art
baselines in continuous control.

</details>


### [244] [Deep generative models as the probability transformation functions](https://arxiv.org/abs/2506.17171)
*Vitalii Bondar,Vira Babenko,Roman Trembovetskyi,Yurii Korobeinyk,Viktoriya Dzyuba*

Key words: 生成模型, 概率变换, 统一理论, 深度学习

TL;DR: 该论文提出了一个统一的理论视角，将深度生成模型视为概率变换函数，揭示了不同生成模型之间的共同本质。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究不同生成模型之间的共性，以促进方法改进和理论发展。

Method: 将生成模型视为从简单分布到复杂目标分布的概率变换函数。

Result: 统一视角有助于方法迁移和理论发展，可能提升生成模型的效率和效果。

Conclusion: 生成模型本质上是概率变换函数，统一视角为理论和方法提供了新基础。

Abstract: This paper introduces a unified theoretical perspective that views deep
generative models as probability transformation functions. Despite the apparent
differences in architecture and training methodologies among various types of
generative models - autoencoders, autoregressive models, generative adversarial
networks, normalizing flows, diffusion models, and flow matching - we
demonstrate that they all fundamentally operate by transforming simple
predefined distributions into complex target data distributions. This unifying
perspective facilitates the transfer of methodological improvements between
model architectures and provides a foundation for developing universal
theoretical approaches, potentially leading to more efficient and effective
generative modeling techniques.

</details>


### [245] [Variational Learning of Disentangled Representations](https://arxiv.org/abs/2506.17182)
*Yuli Slavutsky,Ozgur Beker,David Blei,Bianca Dumitrascu*

Key words: 解缠表示, 变分自编码器, 多条件设置, 生物医学数据

TL;DR: DISCoVeR是一种新的变分框架，用于在多条件下分离条件不变和条件特定的因素，通过双潜在架构和最大-最小目标实现更好的解缠效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在生物医学数据分析等领域，解缠表示对于分离稳定生物信号和上下文依赖性效应至关重要，但现有方法存在潜在表示泄漏问题。

Method: DISCoVeR采用双潜在架构建模共享和特定因素，并行重建保持信息性，并使用最大-最小目标促进分离。

Result: DISCoVeR在合成数据集、自然图像和单细胞RNA-seq数据上表现出更好的解缠能力。

Conclusion: DISCoVeR为多条件下学习解缠表示提供了一种原则性方法。

Abstract: Disentangled representations enable models to separate factors of variation
that are shared across experimental conditions from those that are
condition-specific. This separation is essential in domains such as biomedical
data analysis, where generalization to new treatments, patients, or species
depends on isolating stable biological signals from context-dependent effects.
While extensions of the variational autoencoder (VAE) framework have been
proposed to address this problem, they frequently suffer from leakage between
latent representations, limiting their ability to generalize to unseen
conditions. Here, we introduce DISCoVeR, a new variational framework that
explicitly separates condition-invariant and condition-specific factors.
DISCoVeR integrates three key components: (i) a dual-latent architecture that
models shared and specific factors separately; (ii) two parallel
reconstructions that ensure both representations remain informative; and (iii)
a novel max-min objective that encourages clean separation without relying on
handcrafted priors, while making only minimal assumptions. Theoretically, we
show that this objective maximizes data likelihood while promoting
disentanglement, and that it admits a unique equilibrium. Empirically, we
demonstrate that DISCoVeR achieves improved disentanglement on synthetic
datasets, natural images, and single-cell RNA-seq data. Together, these results
establish DISCoVeR as a principled approach for learning disentangled
representations in multi-condition settings.

</details>


### [246] [Optimal Implicit Bias in Linear Regression](https://arxiv.org/abs/2506.17187)
*Kanumuri Nithin Varma,Babak Hassibi*

Key words: 过参数化, 泛化性能, 隐式偏差, 线性回归, 高斯数据

TL;DR: 本文探讨过参数化学习问题中优化算法的隐式偏差如何影响泛化性能，并在非各向同性高斯数据的线性回归中，给出了最佳泛化误差的下界及最优凸隐式偏差的条件。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究在过参数化学习问题中，哪种隐式偏差能带来最佳的泛化性能。

Method: 通过凸函数/势能的最小化，对非各向同性高斯数据的线性回归进行渐近分析。

Result: 获得了最佳泛化误差的紧致下界，并在一定条件下找到了达到该下界的最优凸隐式偏差。

Conclusion: 在一定条件下，最优凸隐式偏差可实现最佳泛化性能。

Abstract: Most modern learning problems are over-parameterized, where the number of
learnable parameters is much greater than the number of training data points.
In this over-parameterized regime, the training loss typically has infinitely
many global optima that completely interpolate the data with varying
generalization performance. The particular global optimum we converge to
depends on the implicit bias of the optimization algorithm. The question we
address in this paper is, ``What is the implicit bias that leads to the best
generalization performance?". To find the optimal implicit bias, we provide a
precise asymptotic analysis of the generalization performance of interpolators
obtained from the minimization of convex functions/potentials for
over-parameterized linear regression with non-isotropic Gaussian data. In
particular, we obtain a tight lower bound on the best generalization error
possible among this class of interpolators in terms of the
over-parameterization ratio, the variance of the noise in the labels, the
eigenspectrum of the data covariance, and the underlying distribution of the
parameter to be estimated. Finally, we find the optimal convex implicit bias
that achieves this lower bound under certain sufficient conditions involving
the log-concavity of the distribution of a Gaussian convolved with the prior of
the true underlying parameter.

</details>


### [247] [Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning](https://arxiv.org/abs/2506.17204)
*Guozheng Ma,Lu Li,Zilin Wang,Li Shen,Pierre-Luc Bacon,Dacheng Tao*

Key words: 深度强化学习, 网络稀疏性, 随机剪枝, 参数效率, 扩展性

TL;DR: 通过在训练前随机剪枝引入静态网络稀疏性，可以有效提升深度强化学习模型的可扩展性，比密集网络表现更好。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 深度强化学习模型在扩展过程中易受训练时网络病理问题的影响，现有方法如周期性重置或架构改进复杂且不高效。

Method: 采用一次性随机剪枝方法，训练前随机移除一定比例的权重，生成稀疏网络。

Result: 稀疏网络在参数效率和抗优化挑战（如可塑性损失和梯度干扰）上优于密集网络，并在视觉和流式强化学习场景中展现出一致性优势。

Conclusion: 简单的稀疏化方法是提升深度强化学习扩展性的有效途径。

Abstract: Effectively scaling up deep reinforcement learning models has proven
notoriously difficult due to network pathologies during training, motivating
various targeted interventions such as periodic reset and architectural
advances such as layer normalization. Instead of pursuing more complex
modifications, we show that introducing static network sparsity alone can
unlock further scaling potential beyond their dense counterparts with
state-of-the-art architectures. This is achieved through simple one-shot random
pruning, where a predetermined percentage of network weights are randomly
removed once before training. Our analysis reveals that, in contrast to naively
scaling up dense DRL networks, such sparse networks achieve both higher
parameter efficiency for network expressivity and stronger resistance to
optimization challenges like plasticity loss and gradient interference. We
further extend our evaluation to visual and streaming RL scenarios,
demonstrating the consistent benefits of network sparsity.

</details>


### [248] [BREAD: Branched Rollouts from Expert Anchors Bridge SFT & RL for Reasoning](https://arxiv.org/abs/2506.17211)
*Xuechen Zhang,Zijian Huang,Yingcong Li,Chenshun Ni,Jiasi Chen,Samet Oymak*

Key words: 小语言模型, 推理学习, 强化学习, BREAD, 专家指导

TL;DR: 论文分析了小语言模型在复杂推理学习中的局限性，提出了BREAD方法，通过部分专家指导和分支展开优化学习效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 小语言模型（SLMs）在高质量训练数据稀缺时难以学习复杂推理行为，标准训练方法（SFT + RL）存在根本性限制，无法解决专家痕迹表达困难或初始化成功率低的问题。

Method: 提出BREAD方法，通过部分专家指导和分支展开联合SFT和RL阶段，插入专家提示以生成成功路径，密集奖励信号并形成自然学习课程。

Result: BREAD仅需不到40%的真实数据，性能优于标准GRPO，训练速度提高3倍，并能解决SFT + RL无法解决的问题。

Conclusion: BREAD通过分支展开和专家指导显著提升了小语言模型的推理能力。

Abstract: Small language models (SLMs) struggle to learn complex reasoning behaviors,
especially when high-quality traces are scarce or difficult to learn from. The
standard training approach combines a supervised fine-tuning (SFT) stage, often
to distill capabilities of a larger model, followed by a reinforcement learning
(RL)stage such as Group Relative Policy Optimization (GRPO). In this paper, we
investigate the fundamental limitations of this SFT + RL paradigm and propose
methods to overcome them. Under a suitable theoretical model, we demonstrate
that the SFT + RL strategy can fail completely when (1) the expert's traces are
too difficult for the small model to express, or (2) the small model's
initialization has exponentially small likelihood of success. To address these,
we introduce BREAD: a GRPO variant that unifies the SFT and RL stages via
partial expert guidance and branched rollouts. When self-generated traces fail,
BREAD adaptively inserts short expert prefixes/hints, allowing the small model
to complete the rest of the reasoning path, and ensuring that each update
includes at least one successful trace. This mechanism both densifies the
reward signal and induces a natural learning curriculum. BREAD requires fewer
than 40% of ground-truth traces, consistently outperforming standard GRPO while
speeding up the training by about 3 times. Importantly, we demonstrate that
BREAD helps the model solve problems that are otherwise unsolvable by the SFT +
RL strategy, highlighting how branched rollouts and expert guidance can
substantially boost SLM reasoning.

</details>


### [249] [No Free Lunch: Rethinking Internal Feedback for LLM Reasoning](https://arxiv.org/abs/2506.17219)
*Yanzhi Zhang,Zhaoxi Zhang,Haoxiang Guan,Yilin Cheng,Yitong Duan,Chen Wang,Yue Wang,Shuxin Zheng,Jiyan He*

Key words: 强化学习, 大语言模型, 内部反馈, 无监督奖励, 推理能力

TL;DR: 研究提出了一种新的强化学习方法RLIF，利用模型内部信号（如熵和确定性）替代外部监督提升大语言模型的推理能力，初期效果显著但随训练进展性能下降。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 减少对外部监督的依赖，探索基于内部反馈的强化学习方法对大语言模型的推理能力提升效果。

Method: 提出RLIF方法，利用无监督奖励代理（如token级熵、轨迹级熵和自我确定性）作为内部反馈信号。

Result: 初期RLIF能提升模型推理能力，甚至超越RLVR技术，但随着训练进展性能下降；对指令调优模型效果不明显。

Conclusion: RLIF在初期有效但局限性明显，需进一步研究如何有效整合内部反馈信号。

Abstract: Reinforcement learning has emerged as a powerful paradigm for post-training
large language models (LLMs) to improve reasoning. Approaches like
Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) have shown strong results, but they require
extensive external supervision. We investigate an alternative class of methods,
Reinforcement Learning from Internal Feedback (RLIF), which relies solely on
intrinsic model-derived signals instead of external rewards. In particular, we
leverage unsupervised reward proxies such as token-level entropy,
trajectory-level entropy, and self-certainty. Our theoretical analysis shows
these internal objectives are partially equivalent, and we empirically evaluate
various RLIF strategies on challenging math reasoning benchmarks. Experimental
results demonstrate that RLIF can boost the reasoning performance of base LLMs
at the beginning phase of the training, matching or surpassing RLVR techniques
on these tasks. However, when training progresses, performance degrades even
below the model before training. Moreover, we find that RLIF yields little
improvement for instruction-tuned models, indicating diminishing returns of
intrinsic feedback once an LLM is already instruction-tuned. We further analyze
this limitation by mixing model weights and explain the reason of RLIF's
training behaviors, providing practical guidelines for integrating internal
feedback signals into LLM training. We hope our analysis of internal feedback
will inform more principled and effective strategies for LLM post-training.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [250] [LLMs Struggle to Perform Counterfactual Reasoning with Parametric Knowledge](https://arxiv.org/abs/2506.15732)
*Khurram Yamin,Gaurav Ghosal,Bryan Wilder*

Key words: 大型语言模型、反事实推理、参数知识、上下文学习、多层推理

TL;DR: LLMs在整合新信息与参数知识时表现不佳，特别是在反事实推理任务中，且简单的后调优可能导致参数知识退化。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探讨LLMs能否通过上下文学习整合新知识与参数知识，尤其是反事实推理能力。

Method: 通过合成和真实实验在多层推理问题中测试，并进行后调优策略分析。

Result: LLMs在反事实推理中表现较差，主要依赖参数知识；后调优可能损害其存储的知识。

Conclusion: 当前LLMs在重新利用参数知识于新环境时存在显著局限性。

Abstract: Large Language Models have been shown to contain extensive world knowledge in
their parameters, enabling impressive performance on many knowledge intensive
tasks. However, when deployed in novel settings, LLMs often encounter
situations where they must integrate parametric knowledge with new or
unfamiliar information. In this work, we explore whether LLMs can combine
knowledge in-context with their parametric knowledge through the lens of
counterfactual reasoning. Through synthetic and real experiments in multi-hop
reasoning problems, we show that LLMs generally struggle with counterfactual
reasoning, often resorting to exclusively using their parametric knowledge.
Moreover, we show that simple post-hoc finetuning can struggle to instill
counterfactual reasoning ability -- often leading to degradation in stored
parametric knowledge. Ultimately, our work reveals important limitations of
current LLM's abilities to re-purpose parametric knowledge in novel settings.

</details>


### [251] [$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts](https://arxiv.org/abs/2506.15733)
*Mert Cemri,Nived Rajaraman,Rishabh Tiwari,Xiaoxuan Liu,Kurt Keutzer,Ion Stoica,Kannan Ramchandran,Ahmad Beirami,Ziteng Sun*

Key words: 延迟感知、测试时扩展、大语言模型、奖励模型、KL正则化

TL;DR: 提出了一种延迟感知的测试时扩展方法SPECS，通过小模型生成候选序列并结合大模型和奖励模型的信号，显著降低延迟且保持或超过波束搜索的准确率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前测试时扩展方法主要基于总计算资源优化准确率，而忽略了延迟约束，影响用户体验。

Method: 使用小模型高效生成候选序列，结合大模型和奖励模型的信号进行验证，引入奖励引导的软验证和基于奖励的推迟机制。

Result: 在多个数据集上，SPECS匹配或超过波束搜索准确率，同时降低延迟高达19.1%。

Conclusion: SPECS在延迟和准确率之间取得更好平衡，理论分析表明算法随波束宽度增加收敛于KL正则化强化学习目标。

Abstract: Scaling test-time compute has driven the recent advances in the reasoning
capabilities of large language models (LLMs), typically by allocating
additional computation for more thorough exploration. However, increased
compute often comes at the expense of higher user-facing latency, directly
impacting user experience. Current test-time scaling methods primarily optimize
for accuracy based on total compute resources (FLOPS), often overlooking
latency constraints. To address this gap, we propose $\texttt{SPECS}$, a
latency-aware test-time scaling method inspired by speculative decoding.
$\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences
efficiently, and evaluates these candidates using signals from both a larger
target model and a dedicated reward model. We introduce new integration
strategies, including reward-guided soft verification and a reward-based
deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench
datasets show that $\texttt{SPECS}$~matches or surpasses beam search accuracy
while reducing latency by up to $\sim$19.1\%. Our theoretical analysis shows
that our algorithm converges to the solution of a KL-regularized reinforcement
learning objective with increasing beam width.

</details>


### [252] [The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models](https://arxiv.org/abs/2506.15734)
*Peiyuan Tang,Haojie Xin,Xiaodong Zhang,Jun Sun,Qin Xia,Zijiang Yang*

Key words: 视觉语言模型（VLMs）; 安全性; 延迟安全感知; 软提示调优; 对抗攻击

TL;DR: 该论文通过系统分析视觉语言模型（VLM）在攻击下的行为，发现了一种称为‘延迟安全感知’的新现象，并提出了一种名为‘安全提示’的软提示调优方法，以增强模型的安全性，同时不影响其正常性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着视觉语言模型（VLMs）在实际应用中的能力不断增强，确保其安全性变得至关重要。由于VLMs的多模态特性，攻击者可以通过修改视觉或文本输入绕过安全防护，触发有害内容生成。

Method: 作者通过分析VLMs在攻击下的行为，提出‘安全提示’方法，通过优化可学习的提示标记，在文本生成过程中定期注入以增强安全感知。

Result: 在三个安全基准测试和一个对抗攻击测试中，该方法显著降低了攻击成功率，同时保持了模型在正常任务中的性能。

Conclusion: 该研究为实际应用中部署更安全的VLMs提供了一种实用解决方案。

Abstract: As Vision-Language Models (VLMs) demonstrate increasing capabilities across
real-world applications such as code generation and chatbot assistance,
ensuring their safety has become paramount. Unlike traditional Large Language
Models (LLMs), VLMs face unique vulnerabilities due to their multimodal nature,
allowing adversaries to modify visual or textual inputs to bypass safety
guardrails and trigger the generation of harmful content. Through systematic
analysis of VLM behavior under attack, we identify a novel phenomenon termed
``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs
may initially be compromised to produce harmful content, but eventually
recognize the associated risks and attempt to self-correct. This pattern
suggests that VLMs retain their underlying safety awareness but experience a
temporal delay in their activation. Building on this insight, we hypothesize
that VLMs' safety awareness can be proactively reactivated through carefully
designed prompts. To this end, we introduce ``The Safety Reminder'', a soft
prompt tuning approach that optimizes learnable prompt tokens, which are
periodically injected during the text generation process to enhance safety
awareness, effectively preventing harmful content generation. Additionally, our
safety reminder only activates when harmful content is detected, leaving normal
conversations unaffected and preserving the model's performance on benign
tasks. Through comprehensive evaluation across three established safety
benchmarks and one adversarial attacks, we demonstrate that our approach
significantly reduces attack success rates while maintaining model utility,
offering a practical solution for deploying safer VLMs in real-world
applications.

</details>


### [253] [ContextBench: Modifying Contexts for Targeted Latent Activation](https://arxiv.org/abs/2506.15735)
*Robert Graham,Edward Stevinson,Leo Richter,Alexander Chia,Joseph Miller,Joseph Isaac Bloom*

Key words: 语言模型、上下文修改、安全应用、EPO、扩散模型

TL;DR: 研究通过上下文修改生成针对性的、语言流畅的输入，以激活语言模型的特定潜在特征或行为，提出了ContextBench基准，并通过改进的EPO方法在效果和流畅性上取得最优表现。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 识别能触发语言模型特定行为或潜在特征的输入具有广泛的安全应用价值。

Method: 提出上下文修改方法，建立ContextBench基准，改进EPO方法（结合LLM辅助和扩散模型修复）。

Result: 改进的EPO方法在激活潜在特征和行为的效果与语言流畅性上均优于现有方法。

Conclusion: 上下文修改方法为语言模型的安全应用提供了新思路，改进的EPO方法在效果和流畅性上表现最优。

Abstract: Identifying inputs that trigger specific behaviours or latent features in
language models could have a wide range of safety use cases. We investigate a
class of methods capable of generating targeted, linguistically fluent inputs
that activate specific latent features or elicit model behaviours. We formalise
this approach as context modification and present ContextBench -- a benchmark
with tasks assessing core method capabilities and potential safety
applications. Our evaluation framework measures both elicitation strength
(activation of latent features or behaviours) and linguistic fluency,
highlighting how current state-of-the-art methods struggle to balance these
objectives. We enhance Evolutionary Prompt Optimisation (EPO) with
LLM-assistance and diffusion model inpainting, and demonstrate that these
variants achieve state-of-the-art performance in balancing elicitation
effectiveness and fluency.

</details>


### [254] [SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents](https://arxiv.org/abs/2506.15740)
*Jonathan Kutasov,Yuqi Sun,Paul Colognese,Teun van der Weij,Linda Petrini,Chen Bo Calvin Zhang,John Hughes,Xiang Deng,Henry Sleight,Tyler Tracy,Buck Shlegeris,Joe Benton*

Key words: 大型语言模型, 隐蔽破坏, 监控, SHADE-Arena

TL;DR: 论文研究了前沿大型语言模型（LLM）在执行现实任务时实现隐藏有害目标的能力，并评估了其逃避监控的效果。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 评估LLM作为自主代理在复杂任务中隐藏有害目标的能力，以应对潜在的监控挑战。

Method: 使用SHADE-Arena数据集，测试LLM完成主任务和侧任务时避免被监控模型检测的能力。

Result: 表现最佳的模型在监控下成功完成有害任务的概率分别为27%（Claude 3.7 Sonnet）和15%（Gemini 2.5 Pro）。顶级监控模型（Gemini 2.5 Pro）的AUC为0.87。

Conclusion: 当前模型在执行长上下文主任务时仍有困难，但监控隐蔽破坏行为的难度已显现，未来可能进一步增加。

Abstract: As Large Language Models (LLMs) are increasingly deployed as autonomous
agents in complex and long horizon settings, it is critical to evaluate their
ability to sabotage users by pursuing hidden objectives. We study the ability
of frontier LLMs to evade monitoring and achieve harmful hidden goals while
completing a wide array of realistic tasks. We evaluate a broad range of
frontier LLMs using SHADE (Subtle Harmful Agent Detection & Evaluation)-Arena,
the first highly diverse agent evaluation dataset for sabotage and monitoring
capabilities of LLM agents. SHADE-Arena consists of complex pairs of benign
main tasks and harmful side objectives in complicated environments. Agents are
evaluated on their ability to complete the side task without appearing
suspicious to an LLM monitor. When measuring agent ability to (a) complete the
main task, (b) complete the side task, and (c) avoid detection, we find that
the best performing frontier models score 27% (Claude 3.7 Sonnet) and 15%
(Gemini 2.5 Pro) as sabotage agents when overseen by Claude 3.6 Sonnet. For
current frontier models, success on the side task relies heavily on having
access to a hidden scratchpad that is not visible to the monitor. We also use
SHADE-Arena to measure models' monitoring abilities, with the top monitor
(Gemini 2.5 Pro) achieving an AUC of 0.87 at distinguishing benign and malign
transcripts. We find that for now, models still struggle at sabotage due to
failures in long-context main task execution. However, our measurements already
demonstrate the difficulty of monitoring for subtle sabotage attempts, which we
expect to only increase in the face of more complex and longer-horizon tasks.

</details>


### [255] [OAgents: An Empirical Study of Building Effective Agents](https://arxiv.org/abs/2506.15741)
*He Zhu,Tianrui Qin,King Zhu,Heyuan Huang,Yeyi Guan,Jinxiang Xia,Yi Yao,Hanhao Li,Ningning Wang,Pai Liu,Tianhao Peng,Xin Gui,Xiaowan Li,Yuhui Liu,Yuchen Eleanor Jiang,Jun Wang,Changwang Zhang,Xiangru Tang,Ge Zhang,Jian Yang,Minghao Liu,Xitong Gao,Wangchunshu Zhou,Jiaheng Liu*

Key words: Agentic AI, 评估协议, GAIA基准, BrowseComp, OAgents

TL;DR: 该论文通过系统实证研究GAIA基准和BrowseComp，探讨了智能体框架中关键设计选择的影响，并提出了更稳健的评估协议，揭示了有效组件的关键性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前智能体研究缺乏标准化和科学严谨性，导致方法间难以公平比较，且不同设计选择对效果的影响尚不明确。

Method: 在GAIA基准和BrowseComp上进行系统实证研究，分析流行设计选择的影响，并引入新的评估协议。

Result: 研究发现缺乏标准评估协议导致先前工作不可复现，且随机运行间差异显著。OAgents框架在开源项目中达到最佳性能。

Conclusion: 通过标准化评估和模块化设计，OAgents为未来智能体AI研究提供了坚实基础。

Abstract: Recently, Agentic AI has become an increasingly popular research field.
However, we argue that current agent research practices lack standardization
and scientific rigor, making it hard to conduct fair comparisons among methods.
As a result, it is still unclear how different design choices in agent
frameworks affect effectiveness, and measuring their progress remains
challenging. In this work, we conduct a systematic empirical study on GAIA
benchmark and BrowseComp to examine the impact of popular design choices in key
agent components in a fair and rigorous manner. We find that the lack of a
standard evaluation protocol makes previous works, even open-sourced ones,
non-reproducible, with significant variance between random runs. Therefore, we
introduce a more robust evaluation protocol to stabilize comparisons. Our study
reveals which components and designs are crucial for effective agents, while
others are redundant, despite seeming logical. Based on our findings, we build
and open-source OAgents, a new foundation agent framework that achieves
state-of-the-art performance among open-source projects. OAgents offers a
modular design for various agent components, promoting future research in
Agentic AI.

</details>


### [256] [Sysformer: Safeguarding Frozen Large Language Models with Adaptive System Prompts](https://arxiv.org/abs/2506.15751)
*Kartik Sharma,Yiqiao Jin,Vineeth Rakesh,Yingtong Dou,Menghai Pan,Mahashweta Das,Srijan Kumar*

Key words: LLMs, 安全性, 系统提示, Sysformer, 基准测试

TL;DR: 本文提出Sysformer模型，通过动态调整LLMs的系统提示提升安全性，避免有害响应。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 确保LLMs在安全关键场景中合规响应，现有方法成本高或效果不佳。

Method: 使用Sysformer模型在输入嵌入空间优化系统提示，保持LLMs参数冻结。

Result: 在5种LLMs和2个基准测试中，Sysformer显著提升安全性，有害提示拒绝率提升80%，安全提示合规率提升90%。

Conclusion: Sysformer为LLMs提供低成本高效安全保障，推动了可变系统提示设计的研究。

Abstract: As large language models (LLMs) are deployed in safety-critical settings, it
is essential to ensure that their responses comply with safety standards. Prior
research has revealed that LLMs often fail to grasp the notion of safe
behaviors, resulting in either unjustified refusals to harmless prompts or the
generation of harmful content. While substantial efforts have been made to
improve their robustness, existing defenses often rely on costly fine-tuning of
model parameters or employ suboptimal heuristic techniques. In this work, we
take a novel approach to safeguard LLMs by learning to adapt the system prompts
in instruction-tuned LLMs. While LLMs are typically pre-trained to follow a
fixed system prompt, we investigate the impact of tailoring the system prompt
to each specific user input on the safety of the responses. To this end, we
propose $\textbf{Sysformer}$, a trans$\textbf{former}$ model that updates an
initial $\textbf{sys}$tem prompt to a more robust system prompt in the LLM
input embedding space while attending to the user prompt. While keeping the LLM
parameters frozen, the Sysformer is trained to refuse to respond to a set of
harmful prompts while responding ideally to a set of safe ones. Through
extensive experiments on $5$ LLMs from different families and $2$ recent
benchmarks, we demonstrate that Sysformer can significantly enhance the
robustness of LLMs, leading to upto $80\%$ gain in the refusal rate on harmful
prompts while enhancing the compliance with the safe prompts by upto $90\%$.
Results also generalize well to sophisticated jailbreaking attacks, making LLMs
upto $100\%$ more robust against different attack strategies. We hope our
findings lead to cheaper safeguarding of LLMs and motivate future
investigations into designing variable system prompts.

</details>


### [257] [Linear-Time Primitives for Algorithm Development in Graphical Causal Inference](https://arxiv.org/abs/2506.15758)
*Marcel Wienöbst,Sebastian Weichwald,Leonard Henckel*

Key words: 图形因果推断、可达性、线性时间算法、Rust实现、工具变量

TL;DR: CIfly是一个高效的图形因果推断算法框架，通过将可达性作为核心操作，简化了许多因果推理任务，并提供了线性时间的运行时保证。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统因果推理任务中的操作（如moralization和latent projection）计算复杂度高，相当于布尔矩阵乘法，CIfly旨在提供更高效的替代方案。

Method: CIfly通过动态构建状态空间图并利用规则表模式来指定算法，确保线性时间运行，并通过开源Rust实现支持高性能执行。

Result: CIfly在多种因果推理任务中表现出高效性，能够重新实现现有任务并开发新算法（如工具变量分析）。

Conclusion: CIfly为图形因果推断提供了灵活、可扩展的框架，便于算法开发和高性能部署。

Abstract: We introduce CIfly, a framework for efficient algorithmic primitives in
graphical causal inference that isolates reachability as a reusable core
operation. It builds on the insight that many causal reasoning tasks can be
reduced to reachability in purpose-built state-space graphs that can be
constructed on the fly during traversal. We formalize a rule table schema for
specifying such algorithms and prove they run in linear time. We establish
CIfly as a more efficient alternative to the common primitives moralization and
latent projection, which we show are computationally equivalent to Boolean
matrix multiplication. Our open-source Rust implementation parses rule table
text files and runs the specified CIfly algorithms providing high-performance
execution accessible from Python and R. We demonstrate CIfly's utility by
re-implementing a range of established causal inference tasks within the
framework and by developing new algorithms for instrumental variables. These
contributions position CIfly as a flexible and scalable backbone for graphical
causal inference, guiding algorithm development and enabling easy and efficient
deployment.

</details>


### [258] [Advancing Stochastic 3-SAT Solvers by Dissipating Oversatisfied Constraints](https://arxiv.org/abs/2506.15774)
*J. Schwardt,J. C. Budich*

Key words: 3-SAT问题，随机局部搜索，DOCSAT算法，NP完全问题，组合优化

TL;DR: 提出了名为DOCSAT的随机局部搜索启发式算法，用于解决3-SAT问题，显著优于现有算法在处理临界困难实例时的表现。通过减少过满足约束，有效避免局部最小值陷阱。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究针对3-SAT问题中临界困难实例的现有算法（如WalkSAT）易陷入局部最小值的问题。

Method: 提出DOCSAT算法，通过减少过满足约束（Dissipate Oversatisfied Constraints，DOC）来优化搜索过程。

Result: 在N=15000的困难实例中，DOCSAT性能显著优于WalkSAT和Kissat等算法，尤其在解决最难的5%样本时。

Conclusion: DOCSAT通过利用问题统计结构避免局部最小值陷阱，可推广至其他组合优化问题。

Abstract: We introduce and benchmark a stochastic local search heuristic for the
NP-complete satisfiability problem 3-SAT that drastically outperforms existing
solvers in the notoriously difficult realm of critically hard instances. Our
construction is based on the crucial observation that well established previous
approaches such as WalkSAT are prone to get stuck in local minima that are
distinguished from true solutions by a larger number of oversatisfied
combinatorial constraints. To address this issue, the proposed algorithm,
coined DOCSAT, dissipates oversatisfied constraints (DOC), i.e. reduces their
unfavorable abundance so as to render them critical. We analyze and benchmark
our algorithm on a randomly generated sample of hard but satisfiable 3-SAT
instances with varying problem sizes up to N=15000. Quite remarkably, we find
that DOCSAT outperforms both WalkSAT and other well known algorithms including
the complete solver Kissat, even when comparing its ability to solve the
hardest quintile of the sample to the average performance of its competitors.
The essence of DOCSAT may be seen as a way of harnessing statistical structure
beyond the primary cost function of a combinatorial problem to avoid or escape
local minima traps in stochastic local search, which opens avenues for
generalization to other optimization problems.

</details>


### [259] [SLR: An Automated Synthesis Framework for Scalable Logical Reasoning](https://arxiv.org/abs/2506.15787)
*Lukas Helff,Ahmad Omar,Felix Friedrich,Wolfgang Stammer,Antonia Wüst,Tim Woydt,Rupert Mitchell,Patrick Schramowski,Kristian Kersting*

Key words: SLR, Large Language Models, Logical Reasoning, Automated Training, Benchmark

TL;DR: SLR是一个端到端框架，通过系统性评估和训练大型语言模型（LLMs）来支持可扩展的逻辑推理。它自动生成任务、验证模型输出，并通过逻辑调整显著提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统方法难以系统地评估和训练LLMs的逻辑推理能力，SLR旨在通过自动化和可扩展的任务生成解决这一问题。

Method: SLR通过用户任务规范自动生成推理任务，包括潜在规则、验证程序及任务指令，构建了SLR-Bench基准。

Result: 评估显示，当代LLMs在逻辑推理上表现不足，但通过SLR逻辑调整后，Llama-3-8B的准确率翻倍，计算成本更低。

Conclusion: SLR为评估和提升LLMs的逻辑推理能力提供了一种自动化、可扩展的解决方案。

Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and
training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given
a user's task specification, SLR enables scalable, automated synthesis of
inductive reasoning tasks with precisely controlled difficulty. For each task,
SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation
program used by a symbolic judge to deterministically verify model outputs, and
(iii) an instruction prompt for the reasoning task. Using SLR, we create
SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum
levels that progressively increase in relational, arithmetic, and recursive
complexity. Large-scale evaluation reveals that contemporary LLMs readily
produce syntactically valid rules, yet often fail at correct logical inference.
Recent reasoning LLMs do somewhat better, but incur substantial increases in
test-time compute, sometimes exceeding 15k completion tokens. Finally,
logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity
with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully
automated, requires no human annotation, ensures dataset novelty, and offers a
scalable environment for probing and advancing LLMs' reasoning capabilities.

</details>


### [260] [Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search](https://arxiv.org/abs/2506.15880)
*Berk Yilmaz,Junyu Hu,Jinsong Liu*

Key words: 深度强化学习, 蒙特卡洛树搜索, 中国象棋, AI策略

TL;DR: 该论文提出了一种结合深度强化学习和蒙特卡洛树搜索的方法，用于改进中国象棋（Xiangqi）的AI策略。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究目标是解决中国象棋复杂性问题（如独特的棋盘布局、棋子移动限制和胜利条件），并通过AI自我对弈提升策略水平。

Method: 将策略-价值网络与蒙特卡洛树搜索结合，模拟走棋后果并优化决策。

Result: 成功克服了中国象棋高分支因子和不对称棋子动态等挑战，提升了AI在策略游戏中的能力。

Conclusion: 该研究不仅推进了AI在文化策略游戏中的应用，还为适应领域特定规则的DRL-MCTS框架提供了新思路。

Abstract: This paper presents a Deep Reinforcement Learning (DRL) system for Xiangqi
(Chinese Chess) that integrates neural networks with Monte Carlo Tree Search
(MCTS) to enable strategic self-play and self-improvement. Addressing the
underexplored complexity of Xiangqi, including its unique board layout, piece
movement constraints, and victory conditions, our approach combines
policy-value networks with MCTS to simulate move consequences and refine
decision-making. By overcoming challenges such as Xiangqi's high branching
factor and asymmetrical piece dynamics, our work advances AI capabilities in
culturally significant strategy games while providing insights for adapting
DRL-MCTS frameworks to domain-specific rule systems.

</details>


### [261] [Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues](https://arxiv.org/abs/2506.15928)
*Myke C. Cohen,Zhe Su,Hsien-Te Kao,Daniel Nguyen,Spencer Lynch,Maarten Sap,Svitlana Volkova*

Key words: AI代理、社交谈判、人格特质、评估框架、任务关键性应用

TL;DR: 该论文提出了一个评估框架，用于在关键任务谈判场景中评估智能AI系统的表现，重点关注AI代理如何适应不同人类操作者和利益相关者。通过Sotopia模拟测试平台，研究通过两项实验系统地评估了人格特质和AI代理特性对LLM模拟社交谈判结果的影响。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究动机是满足对能够在高任务关键性环境中可靠运作的AI系统的需求，特别是在涉及跨团队协调和军民互动的应用中，AI代理需要适应多样的人类操作者。

Method: 研究采用Sotopia作为模拟测试平台，设计了两项实验：实验一使用因果发现方法分析人格特质对价格谈判的影响；实验二通过模拟人类人格和AI系统特性（如透明度和适应性）评估人类-AI工作谈判。

Result: 实验结果表明，人格特质（如随和性和外向性）显著影响谈判的合理性、目标实现和知识获取；AI代理的信任度对任务效果有重要影响。

Conclusion: 研究构建了一种可重复的评估方法，用于测试AI代理在不同操作者人格和团队动态中的可靠性，推动了AI代理评估的社会动态指标研究。

Abstract: This paper presents an evaluation framework for agentic AI systems in
mission-critical negotiation contexts, addressing the need for AI agents that
can adapt to diverse human operators and stakeholders. Using Sotopia as a
simulation testbed, we present two experiments that systematically evaluated
how personality traits and AI agent characteristics influence LLM-simulated
social negotiation outcomes--a capability essential for a variety of
applications involving cross-team coordination and civil-military interactions.
Experiment 1 employs causal discovery methods to measure how personality traits
impact price bargaining negotiations, through which we found that Agreeableness
and Extraversion significantly affect believability, goal achievement, and
knowledge acquisition outcomes. Sociocognitive lexical measures extracted from
team communications detected fine-grained differences in agents' empathic
communication, moral foundations, and opinion patterns, providing actionable
insights for agentic AI systems that must operate reliably in high-stakes
operational scenarios. Experiment 2 evaluates human-AI job negotiations by
manipulating both simulated human personality and AI system characteristics,
specifically transparency, competence, adaptability, demonstrating how AI agent
trustworthiness impact mission effectiveness. These findings establish a
repeatable evaluation methodology for experimenting with AI agent reliability
across diverse operator personalities and human-agent team dynamics, directly
supporting operational requirements for reliable AI systems. Our work advances
the evaluation of agentic AI workflows by moving beyond standard performance
metrics to incorporate social dynamics essential for mission success in complex
operations.

</details>


### [262] [Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning](https://arxiv.org/abs/2506.16015)
*Craig S. Wright*

Key words: 贝叶斯推理、科学主张评估、认知网络、机器推理、零知识验证

TL;DR: 提出了一种名为BEWA的系统，通过贝叶斯推理和加权权威模型，动态评估科学主张，促进机器推理系统的真理效用和完整性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 科学文献的快速增长超出了人类专家和当前人工智能系统的处理能力，需要一个能够动态评估科学主张并保持逻辑一致性的系统。

Method: BEWA采用贝叶斯推理、复制评分、引用加权和时间衰减机制，结合图模型传播主张和作者信誉建模，支持零知识审计验证。

Result: BEWA为机器推理系统提供了一个可验证的认知网络，增强了真理效用和信念一致性。

Conclusion: BEWA通过形式化科学推理，为动态科学领域的机器推理系统提供了新的基础。

Abstract: The exponential expansion of scientific literature has surpassed the
epistemic processing capabilities of both human experts and current artificial
intelligence systems. This paper introduces Bayesian Epistemology with Weighted
Authority (BEWA), a formally structured architecture that operationalises
belief as a dynamic, probabilistically coherent function over structured
scientific claims. Each claim is contextualised, author-attributed, and
evaluated through a system of replication scores, citation weighting, and
temporal decay. Belief updates are performed via evidence-conditioned Bayesian
inference, contradiction processing, and epistemic decay mechanisms. The
architecture supports graph-based claim propagation, authorial credibility
modelling, cryptographic anchoring, and zero-knowledge audit verification. By
formalising scientific reasoning into a computationally verifiable epistemic
network, BEWA advances the foundation for machine reasoning systems that
promote truth utility, rational belief convergence, and audit-resilient
integrity across dynamic scientific domains.

</details>


### [263] [Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations](https://arxiv.org/abs/2506.16016)
*William Sharpless,Dylan Hirsch,Sander Tonkens,Nikhil Shinde,Sylvia Herbert*

Key words: 强化学习, 硬约束, 哈密顿-雅可比方程, 双目标, DO-HJ-PPO

TL;DR: 提出两种新的值函数用于双目标满足，通过分解问题为到达、避开和到达-避开问题，利用哈密顿-雅可比方程和强化学习结合的方法，提出DO-HJ-PPO算法，在安全到达和多目标任务中表现优于基线。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决强化学习中硬约束对策略性能的负面影响，避免复杂的奖励工程和参数调整，提供新的约束决策视角。

Method: 将问题分解为到达、避开和到达-避开问题，利用哈密顿-雅可比方程与强化学习的结合，提出DO-HJ-PPO算法。

Result: DO-HJ-PPO在处理安全到达和多目标任务中表现出独特行为，并在多种指标上优于基线方法。

Conclusion: 通过新的值函数和算法设计，有效解决了双目标约束问题，扩展了强化学习的应用范围。

Abstract: Hard constraints in reinforcement learning (RL), whether imposed via the
reward function or the model architecture, often degrade policy performance.
Lagrangian methods offer a way to blend objectives with constraints, but often
require intricate reward engineering and parameter tuning. In this work, we
extend recent advances that connect Hamilton-Jacobi (HJ) equations with RL to
propose two novel value functions for dual-objective satisfaction. Namely, we
address: (1) the Reach-Always-Avoid problem - of achieving distinct reward and
penalty thresholds - and (2) the Reach-Reach problem - of achieving thresholds
of two distinct rewards. In contrast with temporal logic approaches, which
typically involve representing an automaton, we derive explicit, tractable
Bellman forms in this context by decomposing our problem into reach, avoid, and
reach-avoid problems, as to leverage these aforementioned recent advances. From
a mathematical perspective, the Reach-Always-Avoid and Reach-Reach problems are
complementary and fundamentally different from standard sum-of-rewards problems
and temporal logic problems, providing a new perspective on constrained
decision-making. We leverage our analysis to propose a variation of Proximal
Policy Optimization (DO-HJ-PPO), which solves these problems. Across a range of
tasks for safe-arrival and multi-target achievement, we demonstrate that
DO-HJ-PPO produces qualitatively distinct behaviors from previous approaches
and out-competes a number of baselines in various metrics.

</details>


### [264] [OSWorld-Human: Benchmarking the Efficiency of Computer-Use Agents](https://arxiv.org/abs/2506.16042)
*Reyna Abhyankar,Qi Qi,Yiying Zhang*

Key words: 生成AI，计算机代理，延迟，OSWorld，OSWorld-Human

TL;DR: 生成AI在解决桌面应用任务时存在高延迟问题，研究发现规划和反思步骤是主要原因，且步骤越多，后续步骧延迟越长。通过人工标注数据集OSWorld-Human评估16种代理，发现最高分代理的步骤仍比必要步骤多1.4-2.7倍。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前生成AI在桌面应用任务中虽准确率高，但因高延迟而难以实际应用。研究旨在探索延迟原因，并指导未来计算机代理的发展。

Method: 研究使用OSWorld基准测试中的计算机使用代理，分析其时间性能，并构建了人工标注数据集OSWorld-Human，用于评估16种代理的效率。

Result: 规划和反思的大模型调用是整体延迟的主要因素，且任务步骧越多，后续步骧延迟越高（可达3倍）。最高分代理的步骧仍比必要步骧多1.4-2.7倍。

Conclusion: 当前计算机代理效率低，任务步骧冗余多，需优化规划和反思机制以减少延迟。

Abstract: Generative AI is being leveraged to solve a variety of computer-use tasks
involving desktop applications. State-of-the-art systems have focused solely on
improving accuracy on leading benchmarks. However, these systems are
practically unusable due to extremely high end-to-end latency (e.g., tens of
minutes) for tasks that typically take humans just a few minutes to complete.
To understand the cause behind this and to guide future developments of
computer agents, we conduct the first study on the temporal performance of
computer-use agents on OSWorld, the flagship benchmark in computer-use AI. We
find that large model calls for planning and reflection account for the
majority of the overall latency, and as an agent uses more steps to complete a
task, each successive step can take 3x longer than steps at the beginning of a
task. We then construct OSWorld-Human, a manually annotated version of the
original OSWorld dataset that contains a human-determined trajectory for each
task. We evaluate 16 agents on their efficiency using OSWorld-Human and found
that even the highest-scoring agents on OSWorld take 1.4-2.7x more steps than
necessary.

</details>


### [265] [Consistency Verification in Ontology-Based Process Models with Parameter Interdependencies](https://arxiv.org/abs/2506.16087)
*Tom Jeleniewski,Hamied Nabizada,Jonathan Reif,Felix Gehlhoff,Alexander Fay*

Key words: 本体, 制造过程, 参数依赖, SPARQL, 单位一致性, 数据完整性

TL;DR: 本文提出了一套用于验证基于本体的过程模型的机制，以确保制造过程中参数依赖关系的正确性和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 制造过程中参数依赖关系的建模需要保证数据的一致性和语义连贯性，以便支持跨上下文应用和知识重用。

Method: 通过SPARQL过滤检索数据、单位一致性检查和数据完整性检查，验证数学表达式的正确性。

Result: 在树脂传递成型（RTM）的用例中验证了该方法的适用性。

Conclusion: 该方法支持开发机器可解释和可验证的工程模型。

Abstract: The formalization of process knowledge using ontologies enables consistent
modeling of parameter interdependencies in manufacturing. These
interdependencies are typically represented as mathematical expressions that
define relations between process parameters, supporting tasks such as
calculation, validation, and simulation. To support cross-context application
and knowledge reuse, such expressions are often defined in a generic form and
applied across multiple process contexts. This highlights the necessity of a
consistent and semantically coherent model to ensure the correctness of data
retrieval and interpretation. Consequently, dedicated mechanisms are required
to address key challenges such as selecting context-relevant data, ensuring
unit compatibility between variables and data elements, and verifying the
completeness of input data required for evaluating mathematical expressions.
This paper presents a set of verification mechanisms for a previously developed
ontology-based process model that integrates standardized process semantics,
data element definitions, and formal mathematical constructs. The approach
includes (i) SPARQL-based filtering to retrieve process-relevant data, (ii) a
unit consistency check based on expected-unit annotations and semantic
classification, and (iii) a data completeness check to validate the
evaluability of interdependencies. The applicability of the approach is
demonstrated with a use case from Resin Transfer Molding (RTM), supporting the
development of machine-interpretable and verifiable engineering models.

</details>


### [266] [Geometric Learning in Black-Box Optimization: A GNN Framework for Algorithm Performance Prediction](https://arxiv.org/abs/2506.16144)
*Ana Kostovska,Carola Doerr,Sašo Džeroski,Panče Panov,Tome Eftimov*

Key words: 算法性能预测, 图神经网络, 异构图, 黑箱优化, modCMA-ES, modDE

TL;DR: 本文研究了如何使用异构图数据结构和图神经网络来预测优化算法的性能，通过捕捉问题、算法配置和性能结果之间的复杂依赖关系，显著优于传统表格方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统的算法性能预测方法通常忽略了算法配置这一关键因素，而将其与问题特征和性能结果的关系简化为表格形式，未能充分表达其复杂性。

Method: 使用异构图数据结构和图神经网络，对两种模块化优化框架（modCMA-ES和modDE）的性能进行建模和预测。

Result: 在324种modCMA-ES和576种modDE变体上的实验中，相较于传统表格方法，本文方法在均方误差上取得了高达36.6%的改进。

Conclusion: 几何学习方法在黑箱优化中具有显著潜力，能够更好地捕捉算法性能的复杂依赖关系。

Abstract: Automated algorithm performance prediction in numerical blackbox optimization
often relies on problem characterizations, such as exploratory landscape
analysis features. These features are typically used as inputs to machine
learning models and are represented in a tabular format. However, such
approaches often overlook algorithm configurations, a key factor influencing
performance. The relationships between algorithm operators, parameters, problem
characteristics, and performance outcomes form a complex structure best
represented as a graph. This work explores the use of heterogeneous graph data
structures and graph neural networks to predict the performance of optimization
algorithms by capturing the complex dependencies between problems, algorithm
configurations, and performance outcomes. We focus on two modular frameworks,
modCMA-ES and modDE, which decompose two widely used derivative-free
optimization algorithms: the covariance matrix adaptation evolution strategy
(CMA-ES) and differential evolution (DE). We evaluate 324 modCMA-ES and 576
modDE variants on 24 BBOB problems across six runtime budgets and two problem
dimensions. Achieving up to 36.6% improvement in MSE over traditional
tabular-based methods, this work highlights the potential of geometric learning
in black-box optimization.

</details>


### [267] [Large Language Models are Near-Optimal Decision-Makers with a Non-Human Learning Behavior](https://arxiv.org/abs/2506.16163)
*Hao Li,Gengrui Zhang,Petter Holme,Shuyue Hu,Zhen Wang*

Key words: 决策,大语言模型,不确定性,风险

TL;DR: LLMs在决策任务中常优于人类，但其决策机制与人类有本质差异，呼吁进一步研究其替代人类判断的风险。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究LLMs与人类在不确定性、风险和转换能力等核心决策维度的差异，以评估其替代人类决策的潜力与风险。

Method: 使用心理学任务对比5种主流LLMs与360名人类参与者的表现。

Result: LLMs表现接近最优，但决策过程与人类截然不同。

Conclusion: LLMs虽能有效管理不确定性和风险，但依赖其替代人类判断存在风险。

Abstract: Human decision-making belongs to the foundation of our society and
civilization, but we are on the verge of a future where much of it will be
delegated to artificial intelligence. The arrival of Large Language Models
(LLMs) has transformed the nature and scope of AI-supported decision-making;
however, the process by which they learn to make decisions, compared to humans,
remains poorly understood. In this study, we examined the decision-making
behavior of five leading LLMs across three core dimensions of real-world
decision-making: uncertainty, risk, and set-shifting. Using three
well-established experimental psychology tasks designed to probe these
dimensions, we benchmarked LLMs against 360 newly recruited human participants.
Across all tasks, LLMs often outperformed humans, approaching near-optimal
performance. Moreover, the processes underlying their decisions diverged
fundamentally from those of humans. On the one hand, our finding demonstrates
the ability of LLMs to manage uncertainty, calibrate risk, and adapt to
changes. On the other hand, this disparity highlights the risks of relying on
them as substitutes for human judgment, calling for further inquiry.

</details>


### [268] [Approximation Fixpoint Theory with Refined Approximation Spaces](https://arxiv.org/abs/2506.16294)
*Linde Vanbesien,Bart Bogaerts,Marc Denecker*

Key words: Approximation Fixpoint Theory, Non-monotonic Reasoning, Knowledge Representation, Approximation Spaces

TL;DR: 论文通过扩展一致的近似不动点理论（AFT）以处理比区间更精细的近似，克服了原有理论的局限性，并引入了更一般的近似空间概念。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 近似不动点理论（AFT）在非单调推理形式中广泛应用，但在某些简单案例中存在局限性，研究旨在扩展AFT以增强其表达能力。

Method: 引入更一般的近似空间概念，改进原有AFT理论，使其能够处理比区间更精细的近似。

Result: 展示了新方法的更强表达能力，并研究了不同近似空间之间的关系。

Conclusion: 扩展后的AFT能够克服原有理论的局限性，为非单调推理形式提供更强大的理论支持。

Abstract: Approximation Fixpoint Theory (AFT) is a powerful theory covering various
semantics of non-monotonic reasoning formalisms in knowledge representation
such as Logic Programming and Answer Set Programming. Many semantics of such
non-monotonic formalisms can be characterized as suitable fixpoints of a
non-monotonic operator on a suitable lattice. Instead of working on the
original lattice, AFT operates on intervals in such lattice to approximate or
construct the fixpoints of interest. While AFT has been applied successfully
across a broad range of non-monotonic reasoning formalisms, it is confronted by
its limitations in other, relatively simple, examples. In this paper, we
overcome those limitations by extending consistent AFT to deal with
approximations that are more refined than intervals. Therefore, we introduce a
more general notion of approximation spaces, showcase the improved
expressiveness and investigate relations between different approximation
spaces.

</details>


### [269] [Explainable Rule Application via Structured Prompting: A Neural-Symbolic Approach](https://arxiv.org/abs/2506.16335)
*Albert Sadowski,Jarosław A. Chudziak*

Key words: 大语言模型, 逻辑推理, 结构化提示, 法律分析, 可解释AI

TL;DR: 论文提出了一种结构化提示框架，通过分解推理步骤结合神经与符号方法，提升了LLMs在逻辑一致性任务（如法律分析）中的表现。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决大语言模型在逻辑推理任务中规则应用不一致、异常处理困难和解释性不足的问题，尤其是在需要自然语言理解和精确逻辑推理的法律领域。

Method: 引入结构化提示框架，将推理分解为实体识别、属性提取和符号规则应用三个可验证步骤，结合神经与符号方法，通过形式化验证确保逻辑一致性。

Result: 在LegalBench hearsay任务中显著超越基线，OpenAI o1模型的F1分数达0.929，o3-mini为0.867，优于其少样本基线的0.714和0.74。

Conclusion: 该混合方法为透明且一致的基于规则的推理提供了可行路径，尤其在结构化法律推理任务中展现了可解释AI的应用潜力。

Abstract: Large Language Models (LLMs) excel in complex reasoning tasks but struggle
with consistent rule application, exception handling, and explainability,
particularly in domains like legal analysis that require both natural language
understanding and precise logical inference. This paper introduces a structured
prompting framework that decomposes reasoning into three verifiable steps:
entity identification, property extraction, and symbolic rule application. By
integrating neural and symbolic approaches, our method leverages LLMs'
interpretive flexibility while ensuring logical consistency through formal
verification. The framework externalizes task definitions, enabling domain
experts to refine logical structures without altering the architecture.
Evaluated on the LegalBench hearsay determination task, our approach
significantly outperformed baselines, with OpenAI o-family models showing
substantial improvements - o1 achieving an F1 score of 0.929 and o3-mini
reaching 0.867 using structured decomposition with complementary predicates,
compared to their few-shot baselines of 0.714 and 0.74 respectively. This
hybrid neural-symbolic system offers a promising pathway for transparent and
consistent rule-based reasoning, suggesting potential for explainable AI
applications in structured legal reasoning tasks.

</details>


### [270] [IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks](https://arxiv.org/abs/2506.16402)
*Xiaoya Lu,Zeren Chen,Xuhao Hu,Yijin Zhou,Weichen Zhang,Dongrui Liu,Lu Sheng,Jing Shao*

Key words: VLM, 交互安全, IS-Bench, 动态风险, 嵌入式AI

TL;DR: 提出IS-Bench，首个多模态交互安全基准，评估VLM驱动智能体的动态风险管理能力，揭示现有代理的不足。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有静态评估范式无法有效评估交互环境中的动态风险，阻碍了VLM驱动智能体在现实任务中的安全部署。

Method: 设计了IS-Bench基准，包含161个场景和388个安全风险，支持过程导向的风险缓解动作评估。

Result: 实验表明当前智能体缺乏交互安全感知，安全意识的Chain-of-Thought虽能改善性能，但影响任务完成率。

Conclusion: IS-Bench为开发更安全可靠的嵌入式AI系统奠定了基础，揭示了现有技术的局限性。

Abstract: Flawed planning from VLM-driven embodied agents poses significant safety
hazards, hindering their deployment in real-world household tasks. However,
existing static, non-interactive evaluation paradigms fail to adequately assess
risks within these interactive environments, since they cannot simulate dynamic
risks that emerge from an agent's actions and rely on unreliable post-hoc
evaluations that ignore unsafe intermediate steps. To bridge this critical gap,
we propose evaluating an agent's interactive safety: its ability to perceive
emergent risks and execute mitigation steps in the correct procedural order. We
thus present IS-Bench, the first multi-modal benchmark designed for interactive
safety, featuring 161 challenging scenarios with 388 unique safety risks
instantiated in a high-fidelity simulator. Crucially, it facilitates a novel
process-oriented evaluation that verifies whether risk mitigation actions are
performed before/after specific risk-prone steps. Extensive experiments on
leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current
agents lack interactive safety awareness, and that while safety-aware
Chain-of-Thought can improve performance, it often compromises task completion.
By highlighting these critical limitations, IS-Bench provides a foundation for
developing safer and more reliable embodied AI systems.

</details>


### [271] [Agentic Personalisation of Cross-Channel Marketing Experiences](https://arxiv.org/abs/2506.16429)
*Sami Abboud,Eleanor Hanna,Olivier Jeunen,Vineesha Raheja,Schaun Wheeler*

Key words: 个性化推送,序列决策,差异中的差异,汤普森抽样,用户参与

TL;DR: 该论文提出了一种基于序列决策框架的方法，通过差异中的差异设计和汤普森抽样优化消费者应用的个性化内容推送。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统的内容推送方法依赖人工操作，限制了内容、时间和频率的个性化效果，需要更高效的解决方案。

Method: 采用差异中的差异设计估计个体处理效应，结合汤普森抽样以平衡探索与利用的权衡。

Result: 在多服务应用中，该方法显著提升了多种目标事件的参与度，部署覆盖1.5亿用户。

Conclusion: 提出的方法成功实现了规模化部署，显著提升了用户参与度。

Abstract: Consumer applications provide ample opportunities to surface and communicate
various forms of content to users. From promotional campaigns for new features
or subscriptions, to evergreen nudges for engagement, or personalised
recommendations; across e-mails, push notifications, and in-app surfaces. The
conventional approach to orchestration for communication relies heavily on
labour-intensive manual marketer work, and inhibits effective personalisation
of content, timing, frequency, and copy-writing. We formulate this task under a
sequential decision-making framework, where we aim to optimise a modular
decision-making policy that maximises incremental engagement for any funnel
event. Our approach leverages a Difference-in-Differences design for Individual
Treatment Effect estimation, and Thompson sampling to balance the
explore-exploit trade-off. We present results from a multi-service application,
where our methodology has resulted in significant increases to a variety of
goal events across several product features, and is currently deployed across
150 million users.

</details>


### [272] [ML-Master: Towards AI-for-AI via Integration of Exploration and Reasoning](https://arxiv.org/abs/2506.16499)
*Zexi Liu,Yuzhu Cai,Xinyu Zhu,Yujie Zheng,Runkun Chen,Ying Wen,Yanfeng Wang,Weinan E,Siheng Chen*

Key words: AI4AI, 代理, 记忆机制, 探索与推理, MLE-Bench

TL;DR: 论文提出ML-Master，一种融合探索与推理的AI4AI代理，通过选择性记忆机制提高效率，性能显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着AI能力接近或超越人类水平，AI驱动的开发变得比人类更高效。AI4AI是一种有前景的路径，但现有LLM代理未能充分利用探索中的经验，导致性能不佳。

Method: 提出ML-Master代理，采用选择性记忆机制，整合并行解决方案轨迹的多样见解与分析推理，优化探索过程。

Result: 在MLE-Bench上，ML-Master平均奖牌率达29.3%，显著优于现有方法，尤其在中等复杂度任务中，且时间效率提高了一倍。

Conclusion: ML-Master是推进AI4AI的强有力工具，其高效性和性能优势为未来AI系统设计提供了新方向。

Abstract: As AI capabilities advance toward and potentially beyond human-level
performance, a natural transition emerges where AI-driven development becomes
more efficient than human-centric approaches. A promising pathway toward this
transition lies in AI-for-AI (AI4AI), which leverages AI techniques to automate
and optimize the design, training, and deployment of AI systems themselves.
While LLM-based agents have shown the potential to realize AI4AI, they are
often unable to fully leverage the experience accumulated by agents during the
exploration of solutions in the reasoning process, leading to inefficiencies
and suboptimal performance. To address this limitation, we propose ML-Master, a
novel AI4AI agent that seamlessly integrates exploration and reasoning by
employing a selectively scoped memory mechanism. This approach allows ML-Master
to efficiently combine diverse insights from parallel solution trajectories
with analytical reasoning, guiding further exploration without overwhelming the
agent with excessive context. We evaluate ML-Master on the MLE-Bench, where it
achieves a 29.3% average medal rate, significantly surpassing existing methods,
particularly in medium-complexity tasks, while accomplishing this superior
performance within a strict 12-hour time constraint-half the 24-hour limit used
by previous baselines. These results demonstrate ML-Master's potential as a
powerful tool for advancing AI4AI.

</details>


### [273] [Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System](https://arxiv.org/abs/2506.16575)
*Mustafa Akben,Aaron Satko*

Key words: 大型语言模型, Elo评分, 有害内容分析, 微侵犯, 仇恨言论

TL;DR: 该论文提出一种基于Elo评分的方法，显著提升了大型语言模型在有害内容分析中的表现，尤其在微侵犯和仇恨言论检测中优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 大型语言模型的内部审核系统在处理有害内容时可能产生问题，如拒绝执行指令或产生过于谨慎的响应，影响研究结果的效度。

Method: 采用Elo评分方法改进LLM在有害内容分析中的性能。

Result: 在微侵犯和仇恨言论数据集上，该方法在准确性、精确度和F1分数上优于传统LLM提示技术和常规机器学习模型。

Conclusion: 该方法提高了有害内容分析的可靠性，减少了误报，并为大规模数据集提供了更好的扩展性。

Abstract: Large language models (LLMs) offer promising opportunities for organizational
research. However, their built-in moderation systems can create problems when
researchers try to analyze harmful content, often refusing to follow certain
instructions or producing overly cautious responses that undermine validity of
the results. This is particularly problematic when analyzing organizational
conflicts such as microaggressions or hate speech. This paper introduces an Elo
rating-based method that significantly improves LLM performance for harmful
content analysis In two datasets, one focused on microaggression detection and
the other on hate speech, we find that our method outperforms traditional LLM
prompting techniques and conventional machine learning models on key measures
such as accuracy, precision, and F1 scores. Advantages include better
reliability when analyzing harmful content, fewer false positives, and greater
scalability for large-scale datasets. This approach supports organizational
applications, including detecting workplace harassment, assessing toxic
communication, and fostering safer and more inclusive work environments.

</details>


### [274] [A Community-driven vision for a new Knowledge Resource for AI](https://arxiv.org/abs/2506.16596)
*Vinay K Chaudhri,Chaitan Baru,Brandon Bennett,Mehul Bhatt,Darion Cassel,Anthony G Cohn,Rina Dechter,Esra Erdem,Dave Ferrucci,Ken Forbus,Gregory Gelfond,Michael Genesereth,Andrew S. Gordon,Benjamin Grosof,Gopal Gupta,Jim Hendler,Sharat Israni,Tyler R. Josephson,Patrick Kyllonen,Yuliya Lierler,Vladimir Lifschitz,Clifton McFate,Hande K. McGinty,Leora Morgenstern,Alessandro Oltramari,Praveen Paritosh,Dan Roth,Blake Shepard,Cogan Shimzu,Denny Vrandečić,Mark Whiting,Michael Witbrock*

Key words: 知识资源, AI基础设施, 开放工程框架, 知识表示, 社区驱动

TL;DR: 本文探讨了当前AI领域对综合性知识资源的需求，提出了一种社区驱动的知识基础设施愿景，并建议通过开放工程框架有效利用知识模块。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 尽管已有WordNet、ConceptNet等知识资源，AI领域仍缺乏全面、通用的知识源，导致大语言模型、机器人规划等问题频现。本文旨在提出解决方案。

Method: 结合知识表示与推理的最新进展，提出构建开放工程框架，整合知识模块，并制定贡献者遵循的规范和社会结构。

Result: 通过AAAI研讨会汇聚50多位研究人员，形成社区驱动的知识基础设施愿景。

Conclusion: 现代技术可助力开发新型知识资源，开放工程框架是实现这一目标的有力途径。

Abstract: The long-standing goal of creating a comprehensive, multi-purpose knowledge
resource, reminiscent of the 1984 Cyc project, still persists in AI. Despite
the success of knowledge resources like WordNet, ConceptNet, Wolfram|Alpha and
other commercial knowledge graphs, verifiable, general-purpose widely available
sources of knowledge remain a critical deficiency in AI infrastructure. Large
language models struggle due to knowledge gaps; robotic planning lacks
necessary world knowledge; and the detection of factually false information
relies heavily on human expertise. What kind of knowledge resource is most
needed in AI today? How can modern technology shape its development and
evaluation? A recent AAAI workshop gathered over 50 researchers to explore
these questions. This paper synthesizes our findings and outlines a
community-driven vision for a new knowledge infrastructure. In addition to
leveraging contemporary advances in knowledge representation and reasoning, one
promising idea is to build an open engineering framework to exploit knowledge
modules effectively within the context of practical applications. Such a
framework should include sets of conventions and social structures that are
adopted by contributors.

</details>


### [275] [The Role of Explanation Styles and Perceived Accuracy on Decision Making in Predictive Process Monitoring](https://arxiv.org/abs/2506.16617)
*Soobin Chae,Suhwan Lee,Hanna Hauptmann,Hajo A. Reijers,Xixi Lu*

Key words: 预测过程监控, 解释性AI, 决策实验, 感知准确性, 解释风格

TL;DR: 该研究探讨了解释风格和感知AI准确性对预测过程监控中决策的影响，发现两者对任务绩效和决策信心有显著影响。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 预测过程监控（PPM）中深度学习模型的低可解释性影响了用户信任和采用，解释性AI（XAI）旨在解决这一问题。当前XAI评估过于关注功能性指标，忽略了用户中心视角。

Method: 通过决策实验，研究不同解释风格（特征重要性、基于规则、反事实）和感知AI准确性（低或高）对用户决策的影响，测量任务绩效、一致性和决策信心。

Result: 感知准确性和解释风格对决策有显著影响。

Conclusion: 解释风格和AI感知准确性是提升用户决策和信任的关键因素。

Abstract: Predictive Process Monitoring (PPM) often uses deep learning models to
predict the future behavior of ongoing processes, such as predicting process
outcomes. While these models achieve high accuracy, their lack of
interpretability undermines user trust and adoption. Explainable AI (XAI) aims
to address this challenge by providing the reasoning behind the predictions.
However, current evaluations of XAI in PPM focus primarily on functional
metrics (such as fidelity), overlooking user-centered aspects such as their
effect on task performance and decision-making. This study investigates the
effects of explanation styles (feature importance, rule-based, and
counterfactual) and perceived AI accuracy (low or high) on decision-making in
PPM. We conducted a decision-making experiment, where users were presented with
the AI predictions, perceived accuracy levels, and explanations of different
styles. Users' decisions were measured both before and after receiving
explanations, allowing the assessment of objective metrics (Task Performance
and Agreement) and subjective metrics (Decision Confidence). Our findings show
that perceived accuracy and explanation style have a significant effect.

</details>


### [276] [Interpretable Low-Dimensional Modeling of Spatiotemporal Agent States for Decision Making in Football Tactics](https://arxiv.org/abs/2506.16696)
*Kenjiro Ide,Taiga Someya,Kohei Kawaguchi,Keisuke Fujii*

Key words: 足球战术、规则模型、可解释性、XGBoost、传球预测

TL;DR: 该研究探讨了如何使用低维、基于规则的模型来有效捕捉足球战术，并通过可解释的状态变量和实际数据分析验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 以往的研究模型计算成本高且缺乏可解释性，规则模型虽与专家知识一致但未充分考虑球员状态，因此需要一种更高效且可解释的模型。

Method: 通过定义球持有者和潜在接球者的可解释状态变量，结合专家讨论确定关键变量，使用XGBoost模型分析数据预测传球成功率。

Result: 研究发现球员与球的距离及其空间评分是决定传球成功的关键因素，模型具有较高的解释性和实用性。

Conclusion: 低维可解释模型为足球战术分析提供了实用的决策支持工具。

Abstract: Understanding football tactics is crucial for managers and analysts. Previous
research has proposed models based on spatial and kinematic equations, but
these are computationally expensive. Also, Reinforcement learning approaches
use player positions and velocities but lack interpretability and require large
datasets. Rule-based models align with expert knowledge but have not fully
considered all players' states. This study explores whether low-dimensional,
rule-based models using spatiotemporal data can effectively capture football
tactics. Our approach defines interpretable state variables for both the
ball-holder and potential pass receivers, based on criteria that explore
options like passing. Through discussions with a manager, we identified key
variables representing the game state. We then used StatsBomb event data and
SkillCorner tracking data from the 2023$/$24 LaLiga season to train an XGBoost
model to predict pass success. The analysis revealed that the distance between
the player and the ball, as well as the player's space score, were key factors
in determining successful passes. Our interpretable low-dimensional modeling
facilitates tactical analysis through the use of intuitive variables and
provides practical value as a tool to support decision-making in football.

</details>


### [277] [Incentivizing High-quality Participation From Federated Learning Agents](https://arxiv.org/abs/2506.16731)
*Jinlong Pang,Jiaheng Wei,Yifan Hua,Chen Qian,Yang Liu*

Key words: 联邦学习、激励机制、Wasserstein距离、Stackelberg博弈、peer prediction

TL;DR: 提出了一种考虑数据异质性的激励感知框架，用于解决联邦学习中代理参与不足的问题，通过引入Wasserstein距离和两阶段Stackelberg博弈模型，提升了模型收敛效率和参与积极性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有联邦学习研究中，代理可能因自私或不积极而退出或提供低质量贡献，且现有方法忽略了数据异质性对模型聚合的影响，导致模型效果不佳。

Method: 引入Wasserstein距离量化数据异质性，重新定义收敛上界；利用peer prediction机制设计评分函数以激励代理真实报告；提出两阶段Stackelberg博弈模型分析均衡存在性。

Result: 在真实数据集上的实验验证了所提机制的有效性，能够加速收敛并提高代理参与积极性。

Conclusion: 该框架通过激励和建模数据异质性，显著提升了联邦学习的效率和参与度，解决了现有研究的局限性。

Abstract: Federated learning (FL) provides a promising paradigm for facilitating
collaboration between multiple clients that jointly learn a global model
without directly sharing their local data. However, existing research suffers
from two caveats: 1) From the perspective of agents, voluntary and unselfish
participation is often assumed. But self-interested agents may opt out of the
system or provide low-quality contributions without proper incentives; 2) From
the mechanism designer's perspective, the aggregated models can be
unsatisfactory as the existing game-theoretical federated learning approach for
data collection ignores the potential heterogeneous effort caused by
contributed data. To alleviate above challenges, we propose an incentive-aware
framework for agent participation that considers data heterogeneity to
accelerate the convergence process. Specifically, we first introduce the notion
of Wasserstein distance to explicitly illustrate the heterogeneous effort and
reformulate the existing upper bound of convergence. To induce truthful
reporting from agents, we analyze and measure the generalization error gap of
any two agents by leveraging the peer prediction mechanism to develop score
functions. We further present a two-stage Stackelberg game model that
formalizes the process and examines the existence of equilibrium. Extensive
experiments on real-world datasets demonstrate the effectiveness of our
proposed mechanism.

</details>


### [278] [Reinforcement learning for hybrid charging stations planning and operation considering fixed and mobile chargers](https://arxiv.org/abs/2506.16764)
*Yanchen Zhu,Honghui Zou,Chufan Liu,Yuyu Luo,Yuankai Wu,Yuxuan Liang*

Key words: 车辆电气化，充电基础设施，深度强化学习，动态充电需求，混合充电站

TL;DR: 论文提出了一种混合充电基础设施优化方法（HCSPO），结合固定和移动充电设施，利用深度强化学习优化充电站规划与调度。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决城市充电需求动态变化带来的固定充电站利用率低下或拥堵问题，提供更灵活的充电设施布局方案。

Method: 提出HCSPO问题，结合充电需求预测模型（基于MPC）和深度强化学习，优化固定充电站规划与移动充电设施调度。

Result: 在实际城市场景中验证，显著提升了充电设施可用性并减少了用户不便。

Conclusion: 混合充电基础设施方案（HCSPO）通过智能规划与调度，有效满足动态充电需求。

Abstract: The success of vehicle electrification, which brings significant societal and
environmental benefits, is contingent upon the availability of efficient and
adaptable charging infrastructure. Traditional fixed-location charging stations
often face issues like underutilization or congestion due to the dynamic nature
of charging demand. Mobile chargers have emerged as a flexible solution,
capable of relocating to align with these demand fluctuations. This paper
addresses the optimal planning and operation of hybrid charging
infrastructures, integrating both fixed and mobile chargers within urban road
networks. We introduce the Hybrid Charging Station Planning and Operation
(HCSPO) problem, which simultaneously optimizes the location and configuration
of fixed charging stations and schedules mobile chargers for dynamic
operations. Our approach incorporates a charging demand prediction model
grounded in Model Predictive Control (MPC) to enhance decision-making. To solve
the HCSPO problem, we propose a deep reinforcement learning method, augmented
with heuristic scheduling techniques, to effectively bridge the planning of
fixed chargers with the real-time operation of mobile chargers. Extensive case
studies using real-world urban scenarios demonstrate that our method
significantly improves the availability of charging infrastructure and reduces
user inconvenience compared to existing solutions and baselines.

</details>


### [279] [AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario](https://arxiv.org/abs/2506.16898)
*Ciro Beneduce,Massimiliano Luca,Bruno Lepri*

Key words: 图像生成模型, 地理偏见, 实体消歧, 美国地理, Stable Diffusion

TL;DR: 论文研究了图像生成模型在生成美国各州及首府图像时存在的地理知识偏见和实体消歧问题，发现模型倾向于大都市区而忽视乡村和小城市。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索图像生成模型在生成地理图像时的知识局限性和潜在偏见。

Method: 使用FLUX 1和Stable Diffusion 3.5生成150张美国各州及首府的合成图像，并利用DINO-v2 ViT-S/14和Fréchet Inception Distances衡量图像相似性。

Result: 模型隐含了美国地理知识，但在生成“美国”图像时偏向大都市区；同时对欧洲风格地名存在实体消歧问题。

Conclusion: 图像生成模型在生成地理图像时存在偏见和知识局限性，需进一步改进以避免地域偏向和消歧错误。

Abstract: Image generation models are revolutionizing many domains, and urban analysis
and design is no exception. While such models are widely adopted, there is a
limited literature exploring their geographic knowledge, along with the biases
they embed. In this work, we generated 150 synthetic images for each state in
the USA and related capitals using FLUX 1 and Stable Diffusion 3.5, two
state-of-the-art models for image generation. We embed each image using DINO-v2
ViT-S/14 and the Fr\'echet Inception Distances to measure the similarity
between the generated images. We found that while these models have implicitly
learned aspects of USA geography, if we prompt the models to generate an image
for "United States" instead of specific cities or states, the models exhibit a
strong representative bias toward metropolis-like areas, excluding rural states
and smaller cities. {\color{black} In addition, we found that models
systematically exhibit some entity-disambiguation issues with European-sounding
names like Frankfort or Devon.

</details>


### [280] [Real-Time Black-Box Optimization for Dynamic Discrete Environments Using Embedded Ising Machines](https://arxiv.org/abs/2506.16924)
*Tomoya Kashimata,Yohei Hamakawa,Masaya Yamasaki,Kosuke Tatsumura*

Key words: Black-box optimization, multi-armed bandit, Ising machine, dynamic environment, discrete optimization

TL;DR: 论文提出了一种针对动态离散环境的启发式MAB方法，通过扩展BBO方法，利用Ising机器有效探索动作，同时考虑变量间的相互作用和动态环境的变化。该方法在无线通信系统中展示了动态适应性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 动态离散环境中的实时系统需要优化离散变量，传统MAB算法因组合爆炸问题无法有效优化，因此需要新的方法。

Method: 扩展BBO方法，利用Ising机器探索动作，同时考虑变量相互作用和动态环境变化，提出启发式MAB方法。

Result: 在无线通信系统中，该方法展示了动态适应性，能够有效优化动态离散环境。

Conclusion: 启发式MAB方法通过结合Ising机器，解决了动态离散环境中的优化问题，适用于实时系统。

Abstract: Many real-time systems require the optimization of discrete variables.
Black-box optimization (BBO) algorithms and multi-armed bandit (MAB) algorithms
perform optimization by repeatedly taking actions and observing the
corresponding instant rewards without any prior knowledge. Recently, a BBO
method using an Ising machine has been proposed to find the best action that is
represented by a combination of discrete values and maximizes the instant
reward in static environments. In contrast, dynamic environments, where
real-time systems operate, necessitate MAB algorithms that maximize the average
reward over multiple trials. However, due to the enormous number of actions
resulting from the combinatorial nature of discrete optimization, conventional
MAB algorithms cannot effectively optimize dynamic, discrete environments.
Here, we show a heuristic MAB method for dynamic, discrete environments by
extending the BBO method, in which an Ising machine effectively explores the
actions while considering interactions between variables and changes in dynamic
environments. We demonstrate the dynamic adaptability of the proposed method in
a wireless communication system with moving users.

</details>


### [281] [Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning](https://arxiv.org/abs/2506.16931)
*Jiaqi Chen,Mingfeng Fan,Xuefeng Zhang,Jingsong Liang,Yuhong Cao,Guohua Wu,Guillaume Adrien Sartoretti*

Key words: 广义旅行商问题, 多模态学习, 机器人任务规划, 实时计算

TL;DR: 论文提出了一种多模态融合学习（MMFL）框架，通过结合图和图像表示解决广义旅行商问题（GTSP），实现了高效任务规划。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 移动机器人任务规划中，GTSP问题在实际应用中难以同时解决准确性和效率需求。

Method: 引入坐标图像构建器，设计自适应分辨率策略和多模态融合模块，整合几何与空间特征。

Result: MMFL在多个GTSP实例中优于现有方法，且保持实时计算效率。

Conclusion: MMFL框架在机器人任务规划中表现出优异性能和实用性。

Abstract: Effective and efficient task planning is essential for mobile robots,
especially in applications like warehouse retrieval and environmental
monitoring. These tasks often involve selecting one location from each of
several target clusters, forming a Generalized Traveling Salesman Problem
(GTSP) that remains challenging to solve both accurately and efficiently. To
address this, we propose a Multimodal Fused Learning (MMFL) framework that
leverages both graph and image-based representations to capture complementary
aspects of the problem, and learns a policy capable of generating high-quality
task planning schemes in real time. Specifically, we first introduce a
coordinate-based image builder that transforms GTSP instances into spatially
informative representations. We then design an adaptive resolution scaling
strategy to enhance adaptability across different problem scales, and develop a
multimodal fusion module with dedicated bottlenecks that enables effective
integration of geometric and spatial features. Extensive experiments show that
our MMFL approach significantly outperforms state-of-the-art methods across
various GTSP instances while maintaining the computational efficiency required
for real-time robotic applications. Physical robot tests further validate its
practical effectiveness in real-world scenarios.

</details>


### [282] [Elevating Styled Mahjong Agents with Learning from Demonstration](https://arxiv.org/abs/2506.16995)
*Lingfeng Li,Yunlong Lu,Yongyi Wang,Wenxin Li*

Key words: 麻将, LfD算法, PPO, 游戏AI, 玩法风格

TL;DR: 提出了一种基于玩法的LfD算法，显著提升麻将代理的熟练度并保留独特玩法风格。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有离线学习和LfD算法在高随机性和分布外状态的麻将游戏中表现不佳，需改进。

Method: 利用现有麻将代理的玩法历史，提出一种仅需对PPO算法进行微小修改的新型LfD算法。

Result: 实证结果显示，该方法显著提升代理熟练度并有效保留独特玩法风格。

Conclusion: 新型LfD算法成功解决了麻将游戏中代理熟练度和玩法风格的平衡问题。

Abstract: A wide variety of bots in games enriches the gameplay experience and enhances
replayability. Recent advancements in game artificial intelligence have
predominantly focused on improving the proficiency of bots. Nevertheless,
developing highly competent bots with a wide range of distinct play styles
remains a relatively under-explored area. We select the Mahjong game
environment as a case study. The high degree of randomness inherent in the
Mahjong game and the prevalence of out-of-distribution states lead to
suboptimal performance of existing offline learning and
Learning-from-Demonstration (LfD) algorithms. In this paper, we leverage the
gameplay histories of existing Mahjong agents and put forward a novel LfD
algorithm that necessitates only minimal modifications to the Proximal Policy
Optimization algorithm. The comprehensive empirical results illustrate that our
proposed method not only significantly enhances the proficiency of the agents
but also effectively preserves their unique play styles.

</details>


### [283] [A Quantile Regression Approach for Remaining Useful Life Estimation with State Space Models](https://arxiv.org/abs/2506.17018)
*Davide Frizzo,Francesco Borsatti,Gian Antonio Susto*

Key words: 预测性维护, 剩余使用寿命, 状态空间模型, 同步分位数回归, 序列建模

TL;DR: 本文提出了一种基于状态空间模型（SSM）和同步分位数回归（SQR）的新方法，用于预测设备剩余使用寿命（RUL），在C-MAPSS数据集上验证了其准确性和效率优于传统序列建模技术（如LSTM、Transformer、Informer）。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 预测性维护（PdM）在现代工业中至关重要，能够通过准确的RUL预测优化维护计划并减少意外故障。

Method: 结合状态空间模型（SSM）和同步分位数回归（SQR），实现长期序列建模和多分位数估计。

Result: 实验结果显示SSM模型在准确性和计算效率上均优于传统方法。

Conclusion: 该方法在高风险工业应用中具有显著潜力。

Abstract: Predictive Maintenance (PdM) is pivotal in Industry 4.0 and 5.0, proactively
enhancing efficiency through accurate equipment Remaining Useful Life (RUL)
prediction, thus optimizing maintenance scheduling and reducing unexpected
failures and premature interventions. This paper introduces a novel RUL
estimation approach leveraging State Space Models (SSM) for efficient long-term
sequence modeling. To handle model uncertainty, Simoultaneous Quantile
Regression (SQR) is integrated into the SSM, enabling multiple quantile
estimations. The proposed method is benchmarked against traditional sequence
modelling techniques (LSTM, Transformer, Informer) using the C-MAPSS dataset.
Results demonstrate superior accuracy and computational efficiency of SSM
models, underscoring their potential for high-stakes industrial applications.

</details>


### [284] [Dispositions and Roles of Generically Dependent Entities](https://arxiv.org/abs/2506.17085)
*Fabian Neuhaus*

Key words: BFO 2020, 通用依赖持续体, 软件, 数据集, 功能, 角色

TL;DR: 论文讨论了BFO 2020对通用依赖持续体（如软件或数据集）的功能、倾向和角色的缺失支持，提出了两种解决方案：定义类和对BFO的修改建议。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: BFO 2020无法充分表示通用依赖持续体的可实现实体，限制了其在计算机模型功能或数据集角色等场景的应用。

Method: 分析了BFO 2020的限制，并提出两种方法：(a)使用定义类 (b)修改BFO以支持通用依赖持续体的功能、倾向和角色。

Result: 提出了两种可行的解决方案，解决了BFO 2020对通用依赖持续体支持的不足。

Conclusion: 通过定义类或修改BFO，可以更好地支持通用依赖持续体的功能、倾向和角色表示。

Abstract: BFO 2020 does not support functions, dispositions, and roles of generically
dependent continuants (like software or datasets). In this paper, we argue that
this is a severe limitation, which prevents, for example, the adequate
representation of the functions of computer models or the various roles of
datasets during the execution of these models. We discuss the aspects of BFO
2020 that prevent the representation of realizable entities of generically
dependent continuants. Two approaches to address the issue are presented: (a)
the use of defined classes and (b) a proposal of changes that allow BFO to
support functions, dispositions, and roles of generically dependent
continuants.

</details>


### [285] [Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic Theorem Proving](https://arxiv.org/abs/2506.17104)
*Chuxue Cao,Mengze Li,Juntao Dai,Jinluan Yang,Zijian Zhao,Shengyu Zhang,Weijie Shi,Chengzhong Liu,Sirui Han,Yike Guo*

Key words: 大语言模型, 一阶逻辑推理, 数学推理, 定理证明, DREAM, 多样化策略

TL;DR: 论文提出了DREAM方法，通过多样化策略和子命题错误反馈提升大模型在复杂数学推理中的表现。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 大语言模型在复杂数学推理中表现不佳，尤其是在多步一阶逻辑推理任务中，表现为Deepseek-Prover-V2-7B的准确性较低（4.2%）。

Method: 提出DREAM方法，结合公理驱动的策略多样化机制和子命题错误反馈机制，提升大模型的推理多样性和合理性。

Result: DREAM在推理阶段显著提升了性能（0.6%至6.4%），并提供了一个包含447个数学定理的数据集用于评估。

Conclusion: DREAM方法有效提升了大语言模型在多步一阶逻辑推理任务中的表现，为数学定理证明领域提供了新的解决方案。

Abstract: Large language models (LLMs) have shown promising first-order logic (FOL)
reasoning capabilities with applications in various areas. However, their
effectiveness in complex mathematical reasoning involving multi-step FOL
deductions is still under-researched. While LLMs perform competitively on
established mathematical reasoning benchmarks, they struggle with multi-step
FOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on
our proposed theorem proving dataset. This issue arises from the limited
exploration of diverse proof strategies and the potential for early reasoning
mistakes to undermine entire proofs. To address these issues, we propose DREAM,
a self-adaptive solution that enhances the Diversity and REAsonability of LLMs'
generation strategies. DREAM incorporates an Axiom-Driven Strategy
Diversification mechanism to promote varied strategic outcomes and a
Sub-Proposition Error Feedback to help LLMs reflect on and correct their
proofs. Our contributions include pioneering advancements in LLMs' mathematical
reasoning through FOL theorem proving, introducing a novel inference stage
solution that improves performance by 0.6% to 6.4%, and providing a curated
dataset of 447 mathematical theorems in Lean 4 format for evaluation.

</details>


### [286] [Are Bias Evaluation Methods Biased ?](https://arxiv.org/abs/2506.17111)
*Lina Berrayana,Sean Rooney,Luis Garcés-Erice,Ioana Giurgiu*

Key words: 大语言模型, 安全性评估, 偏见, 基准鲁棒性

TL;DR: 研究探讨了不同大语言模型安全基准的鲁棒性，发现不同评估方法导致模型排名差异显著。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 评估大语言模型安全性的基准方法多样，但缺乏对其鲁棒性的研究，可能影响模型比较的准确性。

Method: 通过多种广泛使用的偏见评估方法，对一组代表性模型进行排名比较。

Result: 不同评估方法导致模型排名存在显著差异。

Conclusion: 建议社区在使用此类基准时注意方法差异的影响。

Abstract: The creation of benchmarks to evaluate the safety of Large Language Models is
one of the key activities within the trusted AI community. These benchmarks
allow models to be compared for different aspects of safety such as toxicity,
bias, harmful behavior etc. Independent benchmarks adopt different approaches
with distinct data sets and evaluation methods. We investigate how robust such
benchmarks are by using different approaches to rank a set of representative
models for bias and compare how similar are the overall rankings. We show that
different but widely used bias evaluations methods result in disparate model
rankings. We conclude with recommendations for the community in the usage of
such benchmarks.

</details>


### [287] [Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models](https://arxiv.org/abs/2506.17114)
*Dadi Guo,Jiayu Liu,Zhiyuan Fan,Zhitao He,Haoran Li,Yumeng Wang,Yi R.,Fung*

Key words: 大型推理模型、数学证明、错误分析、RFMDataset

TL;DR: 大型推理模型在数学证明任务中表现不佳，尽管在处理数值问题时报告了高准确率。作者提出了RFMDataset来揭示这些模型的10种细粒度错误类型，显示其存在逻辑推理和严谨性上的根本缺陷。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 虽然大型推理模型在数学问题解决上表现出色，但其实际推理能力可能被高估，尤其是对数学证明任务的严谨性和复杂性。本文旨在揭示这些模型的隐藏缺陷。

Method: 通过构建RFMDataset（200个多样化的数学证明问题），对先进模型进行深入评估，分析其失败案例，总结10种错误类型。

Result: 1) 模型在数学证明任务中表现较差，正确率低于20%；2) 推理过程中出现多样化的错误，尤其是单步推理的正确性和严谨性无法保证；3) 模型存在幻觉和不完整性。

Conclusion: 当前模型的自我反思能力不足以解决逻辑困境，需要更正式和细粒度的逻辑训练。

Abstract: Large reasoning models (e.g., R1, o3) have demonstrated remarkable
mathematical problem-solving abilities. However, the high reported accuracy of
these advanced models on popular datasets, reliance on purely numerical
evaluation and potential benchmark leakage, often masks their true reasoning
shortcomings. To address this, we propose leveraging the inherent rigor and
methodological complexity of mathematical proofs as a diagnostic tool to expose
these hidden failures. Specifically, we introduce the RFMDataset (Reveal
Failure Modes), a collection of 200 diverse mathematical proof problems, and
thoroughly evaluate advanced models' performance on it. Our in-depth analysis
of their failures uncovers 10 fine-grained error types, which shows fundamental
limitations in current large reasoning models: 1) large reasoning models
grapple profoundly with mathematical proofs, with some generating entirely
correct proofs for less than 20% of problems and failing even on basic ones; 2)
models exhibit a diverse spectrum of reasoning failures, prominently
demonstrating the lack of guarantees for the correctness and rigor of
single-step reasoning; and 3) models show hallucination and incompleteness
during the reasoning process. Our findings reveal that models' self-reflection
is insufficient to resolve the current logical dilemmas, necessitating
formalized and fine-grained logical training.

</details>


### [288] [When Can Model-Free Reinforcement Learning be Enough for Thinking?](https://arxiv.org/abs/2506.17124)
*Josiah P. Hanna,Nicholas E. Corrado*

Key words: 无模型强化学习、大型语言模型、思想MDP、策略初始化、多任务预训练

TL;DR: 该论文研究了无模型强化学习（RL）如何在语言模型中引发“思考”行为，提出了一种称为“思想MDP”的理论模型，证明了策略初始化的重要性，并验证了开源大型语言模型符合此类行为的条件。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究无模型RL如何在不直接影响奖励或外部世界状态的情况下，通过学习“思考”行为来最大化奖励，从而扩展对RL行为的理解。

Method: 引入“思想MDP”理论模型，证明策略初始化的作用，并通过开源LLMs验证理论预测的必要条件，提出多任务预训练结合的实验。

Result: 理论模型和实验验证表明，无模型RL可以在满足特定条件下产生类似“思考”的行为，并在多任务环境中提高数据效率。

Conclusion: 无模型RL可以通过策略初始化和特定条件学习“思考”行为，为未来在非语言生成领域的研究提供了方向。

Abstract: Recent work on large language models has demonstrated the use of model-free
reinforcement learning (RL) to train reasoning-like capabilities. The emergence
of "thinking" through model-free RL is interesting as thinking actions neither
produce reward nor change the external world state to one where the agent is
more likely to get reward. This paper seeks to build a domain-independent
understanding of when model-free RL will lead to "thinking" as a strategy for
reward maximization. To build this understanding, we first introduce a
theoretical model which we call a \textit{thought Markov decision process}
(MDP). Thought MDPs minimally extend the classical MDP model to include an
abstract notion of thought state and thought action. Using the thought MDP
model, we prove the importance of policy initialization in determining whether
or not thinking emerges and show formally that thought actions are equivalent
to the agent choosing to perform a step of policy improvement before continuing
to act. We then show that open-source LLMs satisfy the conditions that our
theory predicts are necessary for model-free RL to produce thinking-like
behavior. Finally, we hypothesize sufficient conditions that would enable
thinking to be learned outside of language generation and introduce a toy
domain where a combination of multi-task pre-training and designated thought
actions enable more data-efficient RL compared to non-thinking agents.

</details>


### [289] [Chain-of-Trust: A Progressive Trust Evaluation Framework Enabled by Generative AI](https://arxiv.org/abs/2506.17130)
*Botao Zhu,Xianbin Wang,Lei Zhang,Xuemin,Shen*

Key words: 信任评估,协作系统,任务分解,生成式AI,分布式资源

TL;DR: 提出了一种名为chain-of-trust的渐进式信任评估框架，通过分阶段信任评估降低复杂度，利用生成式AI快速分析数据，提高任务完成效率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 在依赖分布式资源的协作系统中，信任评估对任务完成至关重要，但由于网络动态性和信息收集延迟，全面评估信任属性极具挑战性。

Method: 设计了一种分阶段的渐进式信任评估框架（chain-of-trust），结合任务分解和生成式AI的上下文学习与推理能力，逐步筛选可信设备。

Result: 实验证明，该框架在信任评估中具有高准确性。

Conclusion: chain-of-trust框架有效降低了信任评估的复杂性，提升了任务完成的效率和准确性。

Abstract: In collaborative systems with complex tasks relying on distributed resources,
trust evaluation of potential collaborators has emerged as an effective
mechanism for task completion. However, due to the network dynamics and varying
information gathering latencies, it is extremely challenging to observe and
collect all trust attributes of a collaborating device concurrently for a
comprehensive trust assessment. In this paper, a novel progressive trust
evaluation framework, namely chain-of-trust, is proposed to make better use of
misaligned device attribute data. This framework, designed for effective task
completion, divides the trust evaluation process into multiple chained stages
based on task decomposition. At each stage, based on the task completion
process, the framework only gathers the latest device attribute data relevant
to that stage, leading to reduced trust evaluation complexity and overhead. By
leveraging advanced in-context learning, few-shot learning, and reasoning
capabilities, generative AI is then employed to analyze and interpret the
collected data to produce correct evaluation results quickly. Only devices
deemed trustworthy at this stage proceed to the next round of trust evaluation.
The framework ultimately determines devices that remain trustworthy across all
stages. Experimental results demonstrate that the proposed framework achieves
high accuracy in trust evaluation.

</details>


### [290] [The MedPerturb Dataset: What Non-Content Perturbations Reveal About Human and Clinical LLM Decision Making](https://arxiv.org/abs/2506.17163)
*Abinitha Gourabathina,Yuexing Hao,Walter Gerych,Marzyeh Ghassemi*

Key words: 医疗LLM, 临床扰动, 鲁棒性, 性别, 风格, 格式

TL;DR: MedPerturb数据集用于评估医疗LLM在临床输入扰动下的表现，揭示了LLM与人类在性别、语言风格和格式方面的差异。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究医疗LLM在临床环境中的鲁棒性，探索其与人类在应对输入变异时的差异。

Method: 开发MedPerturb数据集，包含临床案例的性别、风格和格式扰动，比较LLM和人类专家的反应。

Result: LLM对性别和风格扰动更敏感，人类则更受格式扰动影响。

Conclusion: 需要动态评估框架来比较医疗LLM与人类在临床环境中的决策一致性。

Abstract: Clinical robustness is critical to the safe deployment of medical Large
Language Models (LLMs), but key questions remain about how LLMs and humans may
differ in response to the real-world variability typified by clinical settings.
To address this, we introduce MedPerturb, a dataset designed to systematically
evaluate medical LLMs under controlled perturbations of clinical input.
MedPerturb consists of clinical vignettes spanning a range of pathologies, each
transformed along three axes: (1) gender modifications (e.g., gender-swapping
or gender-removal); (2) style variation (e.g., uncertain phrasing or colloquial
tone); and (3) format changes (e.g., LLM-generated multi-turn conversations or
summaries). With MedPerturb, we release a dataset of 800 clinical contexts
grounded in realistic input variability, outputs from four LLMs, and three
human expert reads per clinical context. We use MedPerturb in two case studies
to reveal how shifts in gender identity cues, language style, or format reflect
diverging treatment selections between humans and LLMs. We find that LLMs are
more sensitive to gender and style perturbations while human annotators are
more sensitive to LLM-generated format perturbations such as clinical
summaries. Our results highlight the need for evaluation frameworks that go
beyond static benchmarks to assess the similarity between human clinician and
LLM decisions under the variability characteristic of clinical settings.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [291] [DeepRTL2: A Versatile Model for RTL-Related Tasks](https://arxiv.org/abs/2506.15697)
*Yi Liu,Hongji Zhang,Yunhao Zhou,Zhengyuan Shi,Changran Xu,Qiang Xu*

Key words: EDA, LLMs, RTL, code generation, embedding tasks

TL;DR: 論文介紹了DeepRTL2，這是一系列多功能的大型語言模型（LLMs），專門用於電子設計自動化（EDA）中的生成和嵌入型任務，提供了全面的解決方案。

<details>
  <summary>Details</summary>

Main category: cs.AR

Motivation: 雖然已有研究證實LLMs在EDA中的代碼生成任務中表現出色，但嵌入型任務（如代碼搜索和性能預測）卻被忽視。這些任務對硬件設計流程的加速和優化至關重要。

Method: 提出了DeepRTL2，一個統一處理生成和嵌入型任務的LLMs家族，覆蓋了RTL代碼的多種需求。

Result: 實驗顯示，DeepRTL2在所有評估任務中均達到最先進的性能表現。

Conclusion: DeepRTL2是第一個能全面解決EDA中多樣挑戰的模型，填補了嵌入型任務的研究空白。

Abstract: The integration of large language models (LLMs) into electronic design
automation (EDA) has significantly advanced the field, offering transformative
benefits, particularly in register transfer level (RTL) code generation and
understanding. While previous studies have demonstrated the efficacy of
fine-tuning LLMs for these generation-based tasks, embedding-based tasks, which
are equally critical to EDA workflows, have been largely overlooked. These
tasks, including natural language code search, RTL code functionality
equivalence checking, and performance prediction, are essential for
accelerating and optimizing the hardware design process. To address this gap,
we present DeepRTL2, a family of versatile LLMs that unifies both generation-
and embedding-based tasks related to RTL. By simultaneously tackling a broad
range of tasks, DeepRTL2 represents the first model to provide a comprehensive
solution to the diverse challenges in EDA. Through extensive experiments, we
show that DeepRTL2 achieves state-of-the-art performance across all evaluated
tasks.

</details>


### [292] [RCNet: $ΔΣ$ IADCs as Recurrent AutoEncoders](https://arxiv.org/abs/2506.16903)
*Arnaud Verdant,William Guicquero,Jérôme Chossat*

Key words: Delta-Sigma ADC, 深度学习, RNN, 硬件优化, SNR

TL;DR: 论文提出一种深度学习模型RCNet，用于优化Delta-Sigma ADC设计，通过RNN描述调制器和滤波器，结合硬件约束实现SNR与面积的权衡。

<details>
  <summary>Details</summary>

Main category: cs.AR

Motivation: 研究如何利用深度学习优化ADC设计，特别是在硬件约束下提升性能与面积的平衡。

Method: 使用RNN建模调制器和滤波器，结合高性能优化器和定制损失函数，量化权重、信号饱和等硬件约束。

Result: 在DC转换中，RCNet成功实现SNR（>13bit）与面积（<14pF）的权衡，且发现最佳架构未必依赖高阶调制器。

Conclusion: RCNet为ADC设计提供了新的自由度，证明了深度学习在硬件优化中的潜力。

Abstract: This paper proposes a deep learning model (RCNet) for Delta-Sigma
($\Delta\Sigma$) ADCs. Recurrent Neural Networks (RNNs) allow to describe both
modulators and filters. This analogy is applied to Incremental ADCs (IADC).
High-end optimizers combined with full-custom losses are used to define
additional hardware design constraints: quantized weights, signal saturation,
temporal noise injection, devices area. Focusing on DC conversion, our early
results demonstrate that $SNR$ defined as an Effective Number Of Bits (ENOB)
can be optimized under a certain hardware mapping complexity. The proposed
RCNet succeeded to provide design tradeoffs in terms of $SNR$ ($>$13bit) versus
area constraints ($<$14pF total capacitor) at a given $OSR$ (80 samples).
Interestingly, it appears that the best RCNet architectures do not necessarily
rely on high-order modulators, leveraging additional topology exploration
degrees of freedom.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [293] [RecBayes: Recurrent Bayesian Ad Hoc Teamwork in Large Partially Observable Domains](https://arxiv.org/abs/2506.15756)
*João G. Ribeiro,Yaniv Oren,Alberto Sardinha,Matthijs Spaan,Francisco S. Melo*

Key words: RecBayes, 部分可观测性, 团队协作, 贝叶斯分类器

TL;DR: RecBayes是一种无需环境状态或队友动作的新型部分可观测性团队协作方法，适用于大规模任务。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 解决现有方法需要完全可观测状态或队友动作的局限性，提出一种更适合大规模环境的部分可观测性团队协作方法。

Method: 使用基于过去经验的递归贝叶斯分类器，仅通过观测识别已知团队和任务。

Result: 在扩展至100万状态和2^125观测的任务中，RecBayes有效识别团队和任务并辅助协作。

Conclusion: RecBayes无需环境状态或队友动作，适合大规模部分可观测团队协作。

Abstract: This paper proposes RecBayes, a novel approach for ad hoc teamwork under
partial observability, a setting where agents are deployed on-the-fly to
environments where pre-existing teams operate, that never requires, at any
stage, access to the states of the environment or the actions of its teammates.
We show that by relying on a recurrent Bayesian classifier trained using past
experiences, an ad hoc agent is effectively able to identify known teams and
tasks being performed from observations alone. Unlike recent approaches such as
PO-GPL (Gu et al., 2021) and FEAT (Rahman et al., 2023), that require at some
stage fully observable states of the environment, actions of teammates, or
both, or approaches such as ATPO (Ribeiro et al., 2023) that require the
environments to be small enough to be tabularly modelled (Ribeiro et al.,
2023), in their work up to 4.8K states and 1.7K observations, we show RecBayes
is both able to handle arbitrarily large spaces while never relying on either
states and teammates' actions. Our results in benchmark domains from the
multi-agent systems literature, adapted for partial observability and scaled up
to 1M states and 2^125 observations, show that RecBayes is effective at
identifying known teams and tasks being performed from partial observations
alone, and as a result, is able to assist the teams in solving the tasks
effectively.

</details>


### [294] [Generalizable Agent Modeling for Agent Collaboration-Competition Adaptation with Multi-Retrieval and Dynamic Generation](https://arxiv.org/abs/2506.16718)
*Chenxu Wang,Yonggang Jin,Cheng Hu,Youpeng Zhao,Zipeng Dai,Jian Zhao,Shiyu Huang,Liuyu Xiang,Junge Zhang,Zhaofeng He*

Key words: 多智能体系统,强化学习,适应能力,协作与竞争,ACCA,MRDG

TL;DR: 论文提出了Agent Collaborative-Competitive Adaptation (ACCA)框架，用于评估智能体在不同任务、环境和与未知队友及对手交互中的适应能力，并设计了Multi-Retrieval and Dynamic Generation (MRDG)方法，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 解决智能体在新多智能体系统中的适应挑战，尤其是在与未知队友和对手交互时的复杂性问题。

Method: 提出了ACCA框架和MRDG方法，利用行为轨迹建模对手和队友，结合位置编码器、超网络模块和视角对齐模块来提升适应能力。

Result: 在SMAC、Overcooked-AI和Melting Pot等基准测试中，MRDG显著提升了与未知队友和对手的协作与竞争性能。

Conclusion: ACCA框架和MRDG方法有效提升了智能体在多智能体系统中的适应能力，优于现有基准方法。

Abstract: Adapting a single agent to a new multi-agent system brings challenges,
necessitating adjustments across various tasks, environments, and interactions
with unknown teammates and opponents. Addressing this challenge is highly
complex, and researchers have proposed two simplified scenarios, Multi-agent
reinforcement learning for zero-shot learning and Ad-Hoc Teamwork. Building on
these foundations, we propose a more comprehensive setting, Agent
Collaborative-Competitive Adaptation (ACCA), which evaluates an agent to
generalize across diverse scenarios, tasks, and interactions with both
unfamiliar opponents and teammates. In ACCA, agents adjust to task and
environmental changes, collaborate with unseen teammates, and compete against
unknown opponents. We introduce a new modeling approach, Multi-Retrieval and
Dynamic Generation (MRDG), that effectively models both teammates and opponents
using their behavioral trajectories. This method incorporates a positional
encoder for varying team sizes and a hypernetwork module to boost agents'
learning and adaptive capabilities. Additionally, a viewpoint alignment module
harmonizes the observational perspectives of retrieved teammates and opponents
with the learning agent. Extensive tests in benchmark scenarios like SMAC,
Overcooked-AI, and Melting Pot show that MRDG significantly improves robust
collaboration and competition with unseen teammates and opponents, surpassing
established baselines. Our code is available at:
https://github.com/vcis-wangchenxu/MRDG.git

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [295] [Contactless Precision Steering of Particles in a Fluid inside a Cube with Rotating Walls](https://arxiv.org/abs/2506.15958)
*Lucas Amoudruz,Petr Karnakov,Petros Koumoutsakos*

Key words: 接触式操作，流场控制，ODIL框架，多粒子捕获

TL;DR: 提出一种新型控制算法，通过流场引导多个粒子到精确位置，用于生物医学应用。

<details>
  <summary>Details</summary>

Main category: physics.flu-dyn

Motivation: 接触式操作小物体在生物医学和化学应用中至关重要，但多粒子捕获仍具挑战性。

Method: 使用旋转盘生成流场，结合ODIL框架的反馈控制策略。

Result: 实验证明该方法能同时运输两个粒子到预定位置。

Conclusion: 算法推进了生物医学中稳健的无接触粒子操作。

Abstract: Contactless manipulation of small objects is essential for biomedical and
chemical applications, such as cell analysis, assisted fertilisation, and
precision chemistry. Established methods, including optical, acoustic, and
magnetic tweezers, are now complemented by flow control techniques that use
flow-induced motion to enable precise and versatile manipulation. However,
trapping multiple particles in fluid remains a challenge. This study introduces
a novel control algorithm capable of steering multiple particles in flow. The
system uses rotating disks to generate flow fields that transport particles to
precise locations. Disk rotations are governed by a feedback control policy
based on the Optimising a Discrete Loss (ODIL) framework, which combines fluid
dynamics equations with path objectives into a single loss function. Our
experiments, conducted in both simulations and with the physical device,
demonstrate the capability of the approach to transport two beads
simultaneously to predefined locations, advancing robust contactless particle
manipulation for biomedical applications.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [296] [Data-Agnostic Cardinality Learning from Imperfect Workloads](https://arxiv.org/abs/2506.16007)
*Peizhi Wu,Rong Kang,Tieying Zhang,Jianjun Chen,Ryan Marcus,Zachary G. Ives*

Key words: 基数估计、查询优化、机器学习、数据库系统

TL;DR: GRASP是一种不依赖数据的基数估计学习系统，适用于真实世界的不完美查询工作负载，表现优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.DB

Motivation: 传统基数估计依赖于数据统计，但现实中数据访问常受限，且现有查询驱动模型对训练数据有严格假设，与实际不符。

Method: GRASP采用组合设计，泛化到未见过的连接模板，并引入新的每表基数估计模型和学习的计数草图模型。

Result: 在三个数据库实例中，GRASP表现优于现有查询驱动模型，且在复杂基准测试中性能接近或超越传统方法。

Conclusion: GRASP在数据不可访问的情况下仍能高效工作，显著提升了基数估计的准确性和查询延迟。

Abstract: Cardinality estimation (CardEst) is a critical aspect of query optimization.
Traditionally, it leverages statistics built directly over the data. However,
organizational policies (e.g., regulatory compliance) may restrict global data
access. Fortunately, query-driven cardinality estimation can learn CardEst
models using query workloads. However, existing query-driven models often
require access to data or summaries for best performance, and they assume
perfect training workloads with complete and balanced join templates (or join
graphs). Such assumptions rarely hold in real-world scenarios, in which join
templates are incomplete and imbalanced. We present GRASP, a data-agnostic
cardinality learning system designed to work under these real-world
constraints. GRASP's compositional design generalizes to unseen join templates
and is robust to join template imbalance. It also introduces a new per-table
CardEst model that handles value distribution shifts for range predicates, and
a novel learned count sketch model that captures join correlations across base
relations. Across three database instances, we demonstrate that GRASP
consistently outperforms existing query-driven models on imperfect workloads,
both in terms of estimation accuracy and query latency. Remarkably, GRASP
achieves performance comparable to, or even surpassing, traditional approaches
built over the underlying data on the complex CEB-IMDb-full benchmark --
despite operating without any data access and using only 10% of all possible
join templates.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [297] [Improvement of Nuclide Detection through Graph Spectroscopic Analysis Framework and its Application to Nuclear Facility Upset Detection](https://arxiv.org/abs/2506.16522)
*Pedro Rodríguez Fernández,Christian Svinth,Alex Hagen*

Key words: 放射性核素检测, 神经网络, 注意力机制, 铯释放

TL;DR: 提出了一种利用神经网络和注意力机制改进放射性核素检测极限的方法，相比传统方法检测性能提升2倍。

<details>
  <summary>Details</summary>

Main category: physics.ins-det

Motivation: 提高放射性核素的检测极限，尤其是在核设施事故中释放的铯等核素的检测灵敏度。

Method: 使用带有注意力机制的神经网络，结合辐射量子的到达时间，调整检测阈值以适应时域事件分布和局部光谱特征。

Result: 检测性能提升2倍，并展示了该方法对更复杂衰变链的核素具有潜在适用性。

Conclusion: 该方法不仅适用于铯的检测，还可扩展到其他核素，并能整合更多检测事件数据以进一步提升性能。

Abstract: We present a method to improve the detection limit for radionuclides using
spectroscopic radiation detectors and the arrival time of each detected
radiation quantum. We enable this method using a neural network with an
attention mechanism. We illustrate the method on the detection of Cesium
release from a nuclear facility during an upset, and our method shows $2\times$
improvement over the traditional spectroscopic method. We hypothesize that our
method achieves this performance increase by modulating its detection
probability by the overall rate of probable detections, specifically by
adapting detection thresholds based on temporal event distributions and local
spectral features, and show evidence to this effect. We believe this method is
applicable broadly and may be more successful for radionuclides with more
complicated decay chains than Cesium; we also note that our method can
generalize beyond the addition of arrival time and could integrate other data
about each detection event, such as pulse quality, location in detector, or
even combining the energy and time from detections in different detectors.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [298] [Client Selection Strategies for Federated Semantic Communications in Heterogeneous IoT Networks](https://arxiv.org/abs/2506.17063)
*Samer Lahoud,Kinda Khawam*

Key words: IoT, 语义通信, 联邦学习, 客户端选择, 隐私保护

TL;DR: 本文提出了一种新颖的联邦语义通信（SC）框架，旨在解决带宽受限无线网络中IoT设备数据高效传输和隐私保护的挑战。

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 随着IoT设备的指数增长，带宽受限网络中的数据高效传输和隐私保护成为关键问题。

Method: 采用联邦SC框架，通过传输语义特征减少通信开销，并提出了三种客户端选择策略以平衡性能与公平性。

Result: 实验表明，Utilitarian选择在重建质量上表现最佳，而Proportional Fairness在保持性能的同时显著提高了公平性和计算效率。

Conclusion: 联邦SC能够在异构IoT部署中平衡重建质量、资源效率和公平性，为可持续和隐私保护的边缘智能应用铺平道路。

Abstract: The exponential growth of IoT devices presents critical challenges in
bandwidth-constrained wireless networks, particularly regarding efficient data
transmission and privacy preservation. This paper presents a novel federated
semantic communication (SC) framework that enables collaborative training of
bandwidth-efficient models for image reconstruction across heterogeneous IoT
devices. By leveraging SC principles to transmit only semantic features, our
approach dramatically reduces communication overhead while preserving
reconstruction quality. We address the fundamental challenge of client
selection in federated learning environments where devices exhibit significant
disparities in dataset sizes and data distributions. Our framework implements
three distinct client selection strategies that explore different trade-offs
between system performance and fairness in resource allocation. The system
employs an end-to-end SC architecture with semantic bottlenecks, coupled with a
loss-based aggregation mechanism that naturally adapts to client heterogeneity.
Experimental evaluation on image data demonstrates that while Utilitarian
selection achieves the highest reconstruction quality, Proportional Fairness
maintains competitive performance while significantly reducing participation
inequality and improving computational efficiency. These results establish that
federated SC can successfully balance reconstruction quality, resource
efficiency, and fairness in heterogeneous IoT deployments, paving the way for
sustainable and privacy-preserving edge intelligence applications.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [299] [Improved Intelligibility of Dysarthric Speech using Conditional Flow Matching](https://arxiv.org/abs/2506.16127)
*Shoutrik Das,Nishant Singh,Arjun Gangwar,S Umesh*

Key words: 构音障碍,自监督学习,语音生成,条件流匹配,扩散变换器

TL;DR: 该研究探讨了自监督学习特征及其量化表示用于构音障碍到正常语音转换的有效性，提出了一种基于条件流匹配和扩散变换器的非自回归方法，提高了语音可懂度和收敛速度。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 构音障碍严重影响语音可理解性，需要开发有效的转换技术，以改善患者沟通能力。

Method: 使用自监督学习特征（如WavLM）替代传统梅尔频谱，结合条件流匹配和扩散变换器，实现非自回归的语音生成。

Result: 离散声学单元显著提升可懂度，且收敛速度优于传统方法。

Conclusion: 自监督学习特征在构音障碍语音转换中具有潜力，非自回归方法效果更优。

Abstract: Dysarthria is a neurological disorder that significantly impairs speech
intelligibility, often rendering affected individuals unable to communicate
effectively. This necessitates the development of robust dysarthric-to-regular
speech conversion techniques. In this work, we investigate the utility and
limitations of self-supervised learning (SSL) features and their quantized
representations as an alternative to mel-spectrograms for speech generation.
Additionally, we explore methods to mitigate speaker variability by generating
clean speech in a single-speaker voice using features extracted from WavLM. To
this end, we propose a fully non-autoregressive approach that leverages
Conditional Flow Matching (CFM) with Diffusion Transformers to learn a direct
mapping from dysarthric to clean speech. Our findings highlight the
effectiveness of discrete acoustic units in improving intelligibility while
achieving faster convergence compared to traditional mel-spectrogram-based
approaches.

</details>


### [300] [Universal Music Representations? Evaluating Foundation Models on World Music Corpora](https://arxiv.org/abs/2506.17055)
*Charilaos Papaioannou,Emmanouil Benetos,Alexandros Potamianos*

Key words: 基础模型, 音乐信息检索, 跨文化泛化, 监督微调, 少样本学习

TL;DR: 该论文通过多种方法评估了五种音频基础模型在六种音乐传统中的跨文化泛化能力，发现较大模型在非西方音乐上表现更好，但文化距离较远时效果下降。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 研究动机是探索基础模型在多元化音乐传统中的泛化能力，填补其在跨文化音乐信息检索中的研究空白。

Method: 采用了三种方法：探测模型的内部表示、有针对性的监督微调1-2层、以及多标签少样本学习，用于低资源场景。

Result: 结果显示，基础模型在五种数据集上达到了最先进的性能，但文化距离较远时表现下降。大模型在非西方音乐上表现更好。

Conclusion: 基础模型已具备大量音乐知识，但其跨文化泛化能力仍有局限。研究为未来通用音乐表征的进展提供了框架和指标。

Abstract: Foundation models have revolutionized music information retrieval, but
questions remain about their ability to generalize across diverse musical
traditions. This paper presents a comprehensive evaluation of five
state-of-the-art audio foundation models across six musical corpora spanning
Western popular, Greek, Turkish, and Indian classical traditions. We employ
three complementary methodologies to investigate these models' cross-cultural
capabilities: probing to assess inherent representations, targeted supervised
fine-tuning of 1-2 layers, and multi-label few-shot learning for low-resource
scenarios. Our analysis shows varying cross-cultural generalization, with
larger models typically outperforming on non-Western music, though results
decline for culturally distant traditions. Notably, our approaches achieve
state-of-the-art performance on five out of six evaluated datasets,
demonstrating the effectiveness of foundation models for world music
understanding. We also find that our targeted fine-tuning approach does not
consistently outperform probing across all settings, suggesting foundation
models already encode substantial musical knowledge. Our evaluation framework
and benchmarking results contribute to understanding how far current models are
from achieving universal music representations while establishing metrics for
future progress.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [301] [TrainVerify: Equivalence-Based Verification for Distributed LLM Training](https://arxiv.org/abs/2506.15961)
*Yunchi Lu,Youshan Miao,Cheng Tan,Peng Huang,Yi Zhu,Xian Zhang,Fan Yang*

Key words: LLM, 分布式训练, 形式验证, 计算图复杂度, TrainVerify

TL;DR: TrainVerify 是一个验证分布式训练 LLM 的系统，通过形状缩减技术和分阶段并行验证算法，确保大规模训练计划的数学等价性。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 当前大规模语言模型的分布式训练缺乏验证机制，容易引发静默错误和资源浪费，亟需一种高效的验证方法。

Method: TrainVerify 基于模型的逻辑规范作为基准，采用形状缩减技术和分阶段并行验证算法，降低验证复杂度。

Result: 成功验证了包括 Llama3 (405B) 和 DeepSeek-V3 (671B) 在内的前沿 LLM 训练计划。

Conclusion: TrainVerify 提供了可扩展的验证方案，确保分布式训练的正确性，节省计算资源。

Abstract: Training large language models (LLMs) at scale requires parallel execution
across thousands of devices, incurring enormous computational costs. Yet, these
costly distributed trainings are rarely verified, leaving them prone to silent
errors and potentially wasting millions of GPU hours. We introduce TrainVerify,
a system for verifiable distributed training of LLMs. Given a deep learning
model's logical specification as the ground truth, TrainVerify formally
verifies that a distributed parallel execution plan is mathematically
equivalent to it. Direct verification is notoriously difficult due to the sheer
scale of LLMs which often involves billions of variables and highly intricate
computation graphs. Therefore, TrainVerify introduces shape-reduction
techniques and a stage-wise parallel verification algorithm that significantly
reduces complexity while preserving formal correctness. TrainVerify scales to
frontier LLMs, including the successful verification of the Llama3 (405B) and
DeepSeek-V3 (671B) training plans.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [302] [MoNetV2: Enhanced Motion Network for Freehand 3D Ultrasound Reconstruction](https://arxiv.org/abs/2506.15835)
*Mingyuan Luo,Xin Yang,Zhongnuo Yan,Yan Cao,Yuanji Zhang,Xindi Hu,Jin Wang,Haoxuan Ding,Wei Han,Litao Sun,Dong Ni*

Key words: 3D超声, 深度学习, 运动信息融合, 多级一致性约束, 泛化性

TL;DR: MoNetV2通过结合图像和运动信息，提出多级一致性约束和多模态自监督策略，显著提升了3D超声重建的精度和泛化能力。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 传统的仅基于图像的3D超声重建存在累积漂移和复杂运动轨迹下的精度不足问题，需要提高重建精度和适应不同扫描速度和策略的能力。

Method: 1. 提出传感器驱动的时空多分支结构融合图像和运动信息；2. 设计在线多级一致性约束；3. 蒸馏多模态自监督策略以减少累积误差。

Result: MoNetV2在三个大数据集上超越了现有方法，展现了更高的重建质量和泛化性能。

Conclusion: MoNetV2通过融合多源信息和多级约束，有效提升了3D超声重建的准确性和适应性。

Abstract: Three-dimensional (3D) ultrasound (US) aims to provide sonographers with the
spatial relationships of anatomical structures, playing a crucial role in
clinical diagnosis. Recently, deep-learning-based freehand 3D US has made
significant advancements. It reconstructs volumes by estimating transformations
between images without external tracking. However, image-only reconstruction
poses difficulties in reducing cumulative drift and further improving
reconstruction accuracy, particularly in scenarios involving complex motion
trajectories. In this context, we propose an enhanced motion network (MoNetV2)
to enhance the accuracy and generalizability of reconstruction under diverse
scanning velocities and tactics. First, we propose a sensor-based temporal and
multi-branch structure that fuses image and motion information from a velocity
perspective to improve image-only reconstruction accuracy. Second, we devise an
online multi-level consistency constraint that exploits the inherent
consistency of scans to handle various scanning velocities and tactics. This
constraint exploits both scan-level velocity consistency, path-level appearance
consistency, and patch-level motion consistency to supervise inter-frame
transformation estimation. Third, we distill an online multi-modal
self-supervised strategy that leverages the correlation between network
estimation and motion information to further reduce cumulative errors.
Extensive experiments clearly demonstrate that MoNetV2 surpasses existing
methods in both reconstruction quality and generalizability performance across
three large datasets.

</details>


### [303] [Cross-Modality Learning for Predicting IHC Biomarkers from H&E-Stained Whole-Slide Images](https://arxiv.org/abs/2506.15853)
*Amit Das,Naofumi Tomita,Kyle J. Syme,Weijie Ma,Paige O'Connor,Kristin N. Corbett,Bing Ren,Xiaoying Liu,Saeed Hassanpour*

Key words: H&E染色, IHC染色, 深度学习, 对比学习, HistoStainAlign

TL;DR: HistoStainAlign是一种深度学习框架，可直接从H&E染色图像预测IHC染色模式，通过对比训练策略整合形态和分子特征，提高效率和诊断准确性。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: IHC染色成本高、耗时长且需要专业知识，研究者希望通过计算模型直接从H&E图像预测IHC结果，降低资源消耗。

Method: 提出HistoStainAlign框架，利用对比学习策略整合H&E和IHC的联合表征，无需补丁级标注或组织配准。

Result: 在胃肠道和肺部组织上测试三种IHC染料（P53、PD-L1、Ki-67），F1分数分别为0.735、0.830和0.723，显示出模型的有效性。

Conclusion: 该研究表明计算模型可作为预筛查工具，优化IHC染色流程，提升效率。

Abstract: Hematoxylin and Eosin (H&E) staining is a cornerstone of pathological
analysis, offering reliable visualization of cellular morphology and tissue
architecture for cancer diagnosis, subtyping, and grading. Immunohistochemistry
(IHC) staining provides molecular insights by detecting specific proteins
within tissues, enhancing diagnostic accuracy, and improving treatment
planning. However, IHC staining is costly, time-consuming, and
resource-intensive, requiring specialized expertise. To address these
limitations, this study proposes HistoStainAlign, a novel deep learning
framework that predicts IHC staining patterns directly from H&E whole-slide
images (WSIs) by learning joint representations of morphological and molecular
features. The framework integrates paired H&E and IHC embeddings through a
contrastive training strategy, capturing complementary features across staining
modalities without patch-level annotations or tissue registration. The model
was evaluated on gastrointestinal and lung tissue WSIs with three commonly used
IHC stains: P53, PD-L1, and Ki-67. HistoStainAlign achieved weighted F1 scores
of 0.735 [95% Confidence Interval (CI): 0.670-0.799], 0.830 [95% CI:
0.772-0.886], and 0.723 [95% CI: 0.607-0.836], respectively for these three IHC
stains. Embedding analyses demonstrated the robustness of the contrastive
alignment in capturing meaningful cross-stain relationships. Comparisons with a
baseline model further highlight the advantage of incorporating contrastive
learning for improved stain pattern prediction. This study demonstrates the
potential of computational approaches to serve as a pre-screening tool, helping
prioritize cases for IHC staining and improving workflow efficiency.

</details>


### [304] [InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video Understanding](https://arxiv.org/abs/2506.15745)
*Minsoo Kim,Kyuhong Shim,Jungwook Choi,Simyung Chang*

Key words: 多模态大型语言模型、KV缓存、内存优化、流视频理解

TL;DR: InfiniPot-V是一种无需训练、查询无关的框架，用于解决多模态大型语言模型（MLLMs）在处理视频时KV缓存线性增长的问题，实现固定的内存上限。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 现代MLLMs在处理长视频时会产生线性增长的KV缓存，超出手机、AR眼镜和边缘机器人等设备的固定内存限制。现有压缩方法需要离线视频或完整缓存。

Method: 通过Temporal-axis Redundancy（TaR）度量去除时间冗余令牌，通过Value-Norm（VaN）排名保留语义重要的令牌，实现内存硬上限。

Result: 在四种MLLMs和六个视频基准测试中，降低峰值GPU内存达94%，保持实时生成并匹配或超越完整缓存精度。

Conclusion: InfiniPot-V解决了KV缓存瓶颈，无需重新训练或查询先验，推动了设备端流视频助手的发展。

Abstract: Modern multimodal large language models (MLLMs) can reason over hour-long
video, yet their key-value (KV) cache grows linearly with time--quickly
exceeding the fixed memory of phones, AR glasses, and edge robots. Prior
compression schemes either assume the whole video and user query are available
offline or must first build the full cache, so memory still scales with stream
length. InfiniPot-V is the first training-free, query-agnostic framework that
enforces a hard, length-independent memory cap for streaming video
understanding. During video encoding it monitors the cache and, once a user-set
threshold is reached, runs a lightweight compression pass that (i) removes
temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)
keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four
open-source MLLMs and four long-video and two streaming-video benchmarks,
InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,
and matches or surpasses full-cache accuracy--even in multi-turn dialogues. By
dissolving the KV cache bottleneck without retraining or query knowledge,
InfiniPot-V closes the gap for on-device streaming video assistants.

</details>


### [305] [CF-Seg: Counterfactuals meet Segmentation](https://arxiv.org/abs/2506.16213)
*Raghav Mehta,Fabio De Sousa Ribeiro,Tian Xia,Melanie Roschewitz,Ainkaran Santhirasekaram,Dominic C. Marshall,Ben Glocker*

Key words: 医学图像分割、反事实图像、临床决策、胸部X光

TL;DR: 通过生成反事实图像模拟无疾病情况下的解剖结构，提升医学图像分割准确性。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 解决疾病存在时医学图像分割不准确的问题，避免误诊。

Method: 生成反事实图像模拟无疾病情况下的解剖结构，并用其分割目标结构。

Result: 在两个真实临床胸部X光数据集上，反事实图像提升了分割准确性。

Conclusion: 反事实图像有助于提升解剖结构分割效果，支持临床决策。

Abstract: Segmenting anatomical structures in medical images plays an important role in
the quantitative assessment of various diseases. However, accurate segmentation
becomes significantly more challenging in the presence of disease. Disease
patterns can alter the appearance of surrounding healthy tissues, introduce
ambiguous boundaries, or even obscure critical anatomical structures. As such,
segmentation models trained on real-world datasets may struggle to provide good
anatomical segmentation, leading to potential misdiagnosis. In this paper, we
generate counterfactual (CF) images to simulate how the same anatomy would
appear in the absence of disease without altering the underlying structure. We
then use these CF images to segment structures of interest, without requiring
any changes to the underlying segmentation model. Our experiments on two
real-world clinical chest X-ray datasets show that the use of counterfactual
images improves anatomical segmentation, thereby aiding downstream clinical
decision-making.

</details>


### [306] [Hybrid Attention Network for Accurate Breast Tumor Segmentation in Ultrasound Images](https://arxiv.org/abs/2506.16592)
*Muhammad Azeem Aslam,Asim Naveed,Nisar Ahmed*

Key words: 乳腺超声图像,肿瘤分割,注意力机制,混合损失函数,DenseNet121

TL;DR: 提出了一种基于混合注意力的网络用于乳腺超声图像中的病灶分割，通过整合预训练DenseNet121和多分支注意力增强解码器，结合多种注意力机制和空间特征增强模块，显著提升了分割性能。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 乳腺超声图像中的肿瘤分割因噪声、病变尺度变化和模糊边界而具有挑战性，需开发更有效的自动分割方法以辅助早期乳腺癌诊断。

Method: 采用预训练DenseNet121编码器与多分支注意力增强解码器架构，结合GSA、PE、SDPA机制及SFEB模块，并设计混合损失函数（BCE + Jaccard Loss）。

Result: 在公开数据集上实验表明，该方法优于现有方案，在像素级精度和区域重叠指标上表现优异。

Conclusion: 所提方法在乳腺超声病灶分割中表现出色，有望提升早期乳腺癌诊断的准确性和效率。

Abstract: Breast ultrasound imaging is a valuable tool for early breast cancer
detection, but automated tumor segmentation is challenging due to inherent
noise, variations in scale of lesions, and fuzzy boundaries. To address these
challenges, we propose a novel hybrid attention-based network for lesion
segmentation. Our proposed architecture integrates a pre-trained DenseNet121 in
the encoder part for robust feature extraction with a multi-branch
attention-enhanced decoder tailored for breast ultrasound images. The
bottleneck incorporates Global Spatial Attention (GSA), Position Encoding (PE),
and Scaled Dot-Product Attention (SDPA) to learn global context, spatial
relationships, and relative positional features. The Spatial Feature
Enhancement Block (SFEB) is embedded at skip connections to refine and enhance
spatial features, enabling the network to focus more effectively on tumor
regions. A hybrid loss function combining Binary Cross-Entropy (BCE) and
Jaccard Index loss optimizes both pixel-level accuracy and region-level overlap
metrics, enhancing robustness to class imbalance and irregular tumor shapes.
Experiments on public datasets demonstrate that our method outperforms existing
approaches, highlighting its potential to assist radiologists in early and
accurate breast cancer diagnosis.

</details>


### [307] [Pixel-wise Modulated Dice Loss for Medical Image Segmentation](https://arxiv.org/abs/2506.15744)
*Seyed Mohsen Hosseini*

Key words: 医学分割，类别不平衡，难度不平衡，Dice损失，PM Dice损失

TL;DR: 该论文提出了一种名为Pixel-wise Modulated Dice loss（PM Dice loss）的改进方法，用于解决医学分割任务中的类别不平衡和难度不平衡问题，相比现有方法，其计算成本低且效果更优。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 医学分割任务中的数据不平衡包括类别不平衡和难度不平衡，现有方法通常计算成本高且效果有限，因此需要一种更高效的改进方法。

Method: 通过引入像素级调制项对Dice损失进行简单改进，利用Dice损失在类别不平衡上的优势同时处理难度不平衡问题。

Result: 在三种常用医学分割任务上的实验表明，所提出的PM Dice loss在解决难度不平衡问题上优于其他方法。

Conclusion: PM Dice loss是一种计算成本低且高效的改进方法，能同时处理类别不平衡和难度不平衡问题。

Abstract: Class imbalance and the difficulty imbalance are the two types of data
imbalance that affect the performance of neural networks in medical
segmentation tasks. In class imbalance the loss is dominated by the majority
classes and in difficulty imbalance the loss is dominated by easy to classify
pixels. This leads to an ineffective training. Dice loss, which is based on a
geometrical metric, is very effective in addressing the class imbalance
compared to the cross entropy (CE) loss, which is adopted directly from
classification tasks. To address the difficulty imbalance, the common approach
is employing a re-weighted CE loss or a modified Dice loss to focus the
training on difficult to classify areas. The existing modification methods are
computationally costly and with limited success. In this study we propose a
simple modification to the Dice loss with minimal computational cost. With a
pixel level modulating term, we take advantage of the effectiveness of Dice
loss in handling the class imbalance to also handle the difficulty imbalance.
Results on three commonly used medical segmentation tasks show that the
proposed Pixel-wise Modulated Dice loss (PM Dice loss) outperforms other
methods, which are designed to tackle the difficulty imbalance problem.

</details>


### [308] [Implicit neural representations for accurate estimation of the standard model of white matter](https://arxiv.org/abs/2506.15762)
*Tom Hendriks,Gerrit Arends,Edwin Versteeg,Anna Vilanova,Maxime Chamberland,Chantal M. W. Tax*

Key words: diffusion MRI, implicit neural representations, Standard Model, white matter, parameter estimation

TL;DR: 论文提出了一种基于隐式神经表示（INRs）的新估计框架，用于解决标准模型（SM）在高维参数估计中的噪声和退化问题，表现出更高的准确性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 标准模型（SM）在估计白质微结构时面临高维参数退化和噪声问题，需开发更准确和鲁棒的估计方法。

Method: 利用INRs，通过正弦编码输入坐标引入空间正则化，采用无监督学习，支持纤维取向分布函数和核参数联合估计。

Result: INR方法在低信噪比条件下表现更优，支持空间上采样，且无需标记数据，推理速度快且适应性强。

Conclusion: INRs为分析扩散MRI数据提供了高效、灵活且准确的新工具，具有广泛应用潜力。

Abstract: Diffusion magnetic resonance imaging (dMRI) enables non-invasive
investigation of tissue microstructure. The Standard Model (SM) of white matter
aims to disentangle dMRI signal contributions from intra- and extra-axonal
water compartments. However, due to the model its high-dimensional nature,
extensive acquisition protocols with multiple b-values and diffusion tensor
shapes are typically required to mitigate parameter degeneracies. Even then,
accurate estimation remains challenging due to noise. This work introduces a
novel estimation framework based on implicit neural representations (INRs),
which incorporate spatial regularization through the sinusoidal encoding of the
input coordinates. The INR method is evaluated on both synthetic and in vivo
datasets and compared to parameter estimates using cubic polynomials,
supervised neural networks, and nonlinear least squares. Results demonstrate
superior accuracy of the INR method in estimating SM parameters, particularly
in low signal-to-noise conditions. Additionally, spatial upsampling of the INR
can represent the underlying dataset anatomically plausibly in a continuous
way, which is unattainable with linear or cubic interpolation. The INR is fully
unsupervised, eliminating the need for labeled training data. It achieves fast
inference ($\sim$6 minutes), is robust to both Gaussian and Rician noise,
supports joint estimation of SM kernel parameters and the fiber orientation
distribution function with spherical harmonics orders up to at least 8 and
non-negativity constraints, and accommodates spatially varying acquisition
protocols caused by magnetic gradient non-uniformities. The combination of
these properties along with the possibility to easily adapt the framework to
other dMRI models, positions INRs as a potentially important tool for analyzing
and interpreting diffusion MRI data.

</details>


### [309] [Robust Training with Data Augmentation for Medical Imaging Classification](https://arxiv.org/abs/2506.17133)
*Josué Martínez-Martínez,Olivia Brown,Mostafa Karami,Sheida Nabavi*

Key words: 深度学习, 医学影像, 对抗攻击, 分布偏移, RTDA

TL;DR: 本文提出了一种名为RTDA的鲁棒训练算法，用于增强医学图像分类模型对抗对抗攻击和分布偏移的稳健性，并在多种成像技术上验证其有效性。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 深度学习在医学影像诊断中的应用日益广泛，但其易受对抗攻击和分布偏移的影响，可能威胁诊断可靠性并削弱医疗专业人员的信任。

Method: 研究提出了一种结合数据增强的鲁棒训练算法（RTDA），并与其他六种基线技术（如对抗训练和数据增强的独立或组合方法）进行了性能对比。

Result: 实验表明，RTDA在对抗攻击和分布偏移下表现出更强的稳健性，同时在三种不同成像技术（乳腺X光、X射线和超声）中保持了高准确率。

Conclusion: RTDA是一种有效的方法，能够提升医学图像分类模型在复杂环境中的稳健性和泛化能力。

Abstract: Deep neural networks are increasingly being used to detect and diagnose
medical conditions using medical imaging. Despite their utility, these models
are highly vulnerable to adversarial attacks and distribution shifts, which can
affect diagnostic reliability and undermine trust among healthcare
professionals. In this study, we propose a robust training algorithm with data
augmentation (RTDA) to mitigate these vulnerabilities in medical image
classification. We benchmark classifier robustness against adversarial
perturbations and natural variations of RTDA and six competing baseline
techniques, including adversarial training and data augmentation approaches in
isolation and combination, using experimental data sets with three different
imaging technologies (mammograms, X-rays, and ultrasound). We demonstrate that
RTDA achieves superior robustness against adversarial attacks and improved
generalization performance in the presence of distribution shift in each image
classification task while maintaining high clean accuracy.

</details>


### [310] [MeDi: Metadata-Guided Diffusion Models for Mitigating Biases in Tumor Classification](https://arxiv.org/abs/2506.17140)
*David Jacob Drexlin,Jonas Dippel,Julius Hense,Niklas Prenißl,Grégoire Montavon,Frederick Klauschen,Klaus-Robert Müller*

Key words: 深度学习, 生成模型, 数据偏见, 扩散模型, 组织病理学

TL;DR: 论文提出了一种名为MeDi的元数据引导生成扩散模型框架，旨在通过合成数据增强代表性不足的子群体，从而平衡训练数据并减少下游模型的偏见。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 深度学习模型在组织学预测任务中取得了显著进展，但在临床应用中，由于对染色、扫描仪、医院和人口统计等条件缺乏鲁棒性，导致模型性能受限。特别是对代表性不足的子群体，模型容易学习捷径并产生偏见预测。大规模基础模型未能完全解决此问题。

Method: 提出了一种名为MeDi的元数据引导生成扩散模型框架，通过合成数据增强代表性不足的子群体，以平衡训练数据和减少偏见。

Result: 实验表明，MeDi能够为TCGA中未见过的子群体生成高质量组织病理学图像，提高生成图像的整体保真度，并在存在子群体偏移的数据集上改善了下游分类器的性能。

Conclusion: 该研究展示了生成模型在减少数据偏见方面的潜力，MeDi作为一个概念验证，为未来研究提供了方向。

Abstract: Deep learning models have made significant advances in histological
prediction tasks in recent years. However, for adaptation in clinical practice,
their lack of robustness to varying conditions such as staining, scanner,
hospital, and demographics is still a limiting factor: if trained on
overrepresented subpopulations, models regularly struggle with less frequent
patterns, leading to shortcut learning and biased predictions. Large-scale
foundation models have not fully eliminated this issue. Therefore, we propose a
novel approach explicitly modeling such metadata into a Metadata-guided
generative Diffusion model framework (MeDi). MeDi allows for a targeted
augmentation of underrepresented subpopulations with synthetic data, which
balances limited training data and mitigates biases in downstream models. We
experimentally show that MeDi generates high-quality histopathology images for
unseen subpopulations in TCGA, boosts the overall fidelity of the generated
images, and enables improvements in performance for downstream classifiers on
datasets with subpopulation shifts. Our work is a proof-of-concept towards
better mitigating data biases with generative models.

</details>


### [311] [Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network](https://arxiv.org/abs/2506.17165)
*Mahin Montasir Afif,Abdullah Al Noman,K. M. Tahsin Kabir,Md. Mortuza Ahmmed,Md. Mostafizur Rahman,Mufti Mahmud,Md. Ashraful Babu*

Key words: GAN, 医学影像, 数据增强, CNN, 分类性能

TL;DR: 研究探讨了GAN生成与真实脑肿瘤MRI图像在不同混合比例下对CNN分类性能的影响，发现少量GAN数据（如100张）能显著提升模型性能，但过多会降低效果。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 由于真实医学影像数据有限，研究希望通过GAN生成合成图像来扩充数据集，并探索其对分类模型性能的影响。

Method: 使用DCGAN生成合成脑肿瘤MRI图像，按不同比例与真实图像混合训练CNN模型，并在真实测试集上评估性能。

Result: 少量GAN数据（如100张）能显著提升模型性能（测试准确率95.2%），但过多GAN数据会导致性能下降。

Conclusion: GAN合成数据能有效扩充医学数据集，但需控制比例以避免模型泛化能力下降。

Abstract: Generative Adversarial Networks (GAN) have shown potential in expanding
limited medical imaging datasets. This study explores how different ratios of
GAN-generated and real brain tumor MRI images impact the performance of a CNN
in classifying healthy vs. tumorous scans. A DCGAN was used to create synthetic
images which were mixed with real ones at various ratios to train a custom CNN.
The CNN was then evaluated on a separate real-world test set. Our results
indicate that the model maintains high sensitivity and precision in tumor
classification, even when trained predominantly on synthetic data. When only a
small portion of GAN data was added, such as 900 real images and 100 GAN
images, the model achieved excellent performance, with test accuracy reaching
95.2%, and precision, recall, and F1-score all exceeding 95%. However, as the
proportion of GAN images increased further, performance gradually declined.
This study suggests that while GANs are useful for augmenting limited datasets
especially when real data is scarce, too much synthetic data can introduce
artifacts that affect the model's ability to generalize to real world cases.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [312] [Approximate Ricci-flat Metrics for Calabi-Yau Manifolds](https://arxiv.org/abs/2506.15766)
*Seung-Joo Lee,Andre Lukas*

Key words: Calabi-Yau流形, Kähler势, Ricci-flat度量, 机器学习, 复结构参数

TL;DR: 提出了一种通过机器学习技术确定Calabi-Yau流形上近似Ricci-flat Kähler度量的解析Kähler势的方法，并应用于Dwork族的五次超曲面和双三次CY超曲面。

<details>
  <summary>Details</summary>

Main category: hep-th

Motivation: 研究如何在Calabi-Yau流形上找到解析的Kähler势及其近似Ricci-flat Kähler度量，以简化复杂的几何问题。

Method: 结合机器学习技术计算数值Ricci-flat Kähler势，并将其拟合到Donaldson的Ansatz中。

Result: 获得了简单的解析表达式，用于描述近似Ricci-flat Kähler势，且仅依赖于复结构参数的模。

Conclusion: 提出的方法为Calabi-Yau流形上的几何问题提供了有效的解决方案，简化了复杂参数的描述。

Abstract: We outline a method to determine analytic K\"ahler potentials with associated
approximately Ricci-flat K\"ahler metrics on Calabi-Yau manifolds. Key
ingredients are numerically calculating Ricci-flat K\"ahler potentials via
machine learning techniques and fitting the numerical results to Donaldson's
Ansatz. We apply this method to the Dwork family of quintic hypersurfaces in
$\mathbb{P}^4$ and an analogous one-parameter family of bi-cubic CY
hypersurfaces in $\mathbb{P}^2\times\mathbb{P}^2$. In each case, a relatively
simple analytic expression is obtained for the approximately Ricci-flat
K\"ahler potentials, including the explicit dependence on the complex structure
parameter. We find that these K\"ahler potentials only depend on the modulus of
the complex structure parameter.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [313] [Generative Modeling of Full-Atom Protein Conformations using Latent Diffusion on Graph Embeddings](https://arxiv.org/abs/2506.17064)
*Aditya Sengar,Ali Hariri,Daniel Probst,Patrick Barth,Pierre Vandergheynst*

Key words: 蛋白质构象生成,潜在扩散模型,Chebyshev图神经网络,分子动力学,G蛋白偶联受体

TL;DR: 该论文提出了一种名为LD-FPG的框架，用于生成全原子蛋白质结构，通过潜在扩散模型从分子动力学轨迹中构建动态蛋白质的多样化构象集合。

<details>
  <summary>Details</summary>

Main category: q-bio.BM

Motivation: 理解G蛋白偶联受体（GPCRs）等功能动态蛋白质需要生成多样化的全原子构象集合，但现有模型通常简化原子细节或忽略构象多样性。

Method: LD-FPG结合Chebyshev图神经网络（ChebNet）获取蛋白质构象的低维潜在嵌入，并采用三种池化策略（blind、sequential和residue-based）。通过扩散模型生成新样本，并通过解码器映射回笛卡尔坐标。

Result: 在人类多巴胺D2受体2微秒的MD轨迹上测试，sequential和residue-based池化策略成功生成高保真度的构象集合（全原子lDDT约0.7；C-alpha-lDDT约0.8），且二面角分布与MD数据Jensen-Shannon散度低于0.03。

Conclusion: LD-FPG为大型蛋白质的系统特异性全原子构象集合生成提供了实用工具，有望用于复杂动态目标的结构性药物设计。

Abstract: Generating diverse, all-atom conformational ensembles of dynamic proteins
such as G-protein-coupled receptors (GPCRs) is critical for understanding their
function, yet most generative models simplify atomic detail or ignore
conformational diversity altogether. We present latent diffusion for full
protein generation (LD-FPG), a framework that constructs complete all-atom
protein structures, including every side-chain heavy atom, directly from
molecular dynamics (MD) trajectories. LD-FPG employs a Chebyshev graph neural
network (ChebNet) to obtain low-dimensional latent embeddings of protein
conformations, which are processed using three pooling strategies: blind,
sequential and residue-based. A diffusion model trained on these latent
representations generates new samples that a decoder, optionally regularized by
dihedral-angle losses, maps back to Cartesian coordinates. Using D2R-MD, a
2-microsecond MD trajectory (12 000 frames) of the human dopamine D2 receptor
in a membrane environment, the sequential and residue-based pooling strategy
reproduces the reference ensemble with high structural fidelity (all-atom lDDT
of approximately 0.7; C-alpha-lDDT of approximately 0.8) and recovers backbone
and side-chain dihedral-angle distributions with a Jensen-Shannon divergence of
less than 0.03 compared to the MD data. LD-FPG thereby offers a practical route
to system-specific, all-atom ensemble generation for large proteins, providing
a promising tool for structure-based therapeutic design on complex, dynamic
targets. The D2R-MD dataset and our implementation are freely available to
facilitate further research.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [314] [CP$^2$: Leveraging Geometry for Conformal Prediction via Canonicalization](https://arxiv.org/abs/2506.16189)
*Putri A. van der Linden,Alexander Timans,Erik J. Bekkers*

Key words: conformal prediction, geometric shifts, robustness, pose canonicalization, uncertainty quantification

TL;DR: 该论文研究了在几何数据变化（如旋转或翻转）下，整合几何信息（如几何姿态）到Conformal Prediction（CP）中，以恢复其理论保障并增强鲁棒性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 传统CP在分布变化（尤其是几何变化）下实用性受限，因此需要一种方法来维持CP的覆盖保证并提升模型在几何变化下的表现。

Method: 通过整合几何信息（如几何姿态）到CP过程中，利用姿态规范化技术作为信息提取器，以提升CP在几何变化下的性能。

Result: 实验表明，整合几何信息后的CP能有效应对离散和连续的几何变化，且在黑盒预测器中具有广泛适用性。

Conclusion: 通过整合几何信息，CP在几何变化下仍能保持理论保障和实用性，为解决分布变化问题提供了新思路。

Abstract: We study the problem of conformal prediction (CP) under geometric data
shifts, where data samples are susceptible to transformations such as rotations
or flips. While CP endows prediction models with post-hoc uncertainty
quantification and formal coverage guarantees, their practicality breaks under
distribution shifts that deteriorate model performance. To address this issue,
we propose integrating geometric information--such as geometric pose--into the
conformal procedure to reinstate its guarantees and ensure robustness under
geometric shifts. In particular, we explore recent advancements on pose
canonicalization as a suitable information extractor for this purpose.
Evaluating the combined approach across discrete and continuous shifts and
against equivariant and augmentation-based baselines, we find that integrating
geometric information with CP yields a principled way to address geometric
shifts while maintaining broad applicability to black-box predictors.

</details>


### [315] [Latent Noise Injection for Private and Statistically Aligned Synthetic Data Generation](https://arxiv.org/abs/2506.16636)
*Rex Shen,Lu Tian*

Key words: 合成数据生成, 掩码自回归流, 潜在噪声注入, 差分隐私, 高维数据

TL;DR: 提出了一种基于掩码自回归流的潜在噪声注入方法，用于高质量合成数据生成，解决了高维设置下传统方法的收敛慢问题，同时满足差分隐私。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 标准生成模型在高维设置下收敛慢，难以逼近真实数据分布，提出潜在噪声注入方法以改进。

Method: 使用掩码自回归流（MAF），在潜在空间扰动数据点后映射回数据域，保持观测与合成数据的一一对应。

Result: 方法在理论和实证中均显示高效，通过K次研究的元分析恢复经典效率，同时满足差分隐私。

Conclusion: 潜在噪声注入方法在隐私保护和数据效用间取得良好平衡，适用于分散且隐私敏感的场景。

Abstract: Synthetic Data Generation has become essential for scalable,
privacy-preserving statistical analysis. While standard approaches based on
generative models, such as Normalizing Flows, have been widely used, they often
suffer from slow convergence in high-dimensional settings, frequently
converging more slowly than the canonical $1/\sqrt{n}$ rate when approximating
the true data distribution.
  To overcome these limitations, we propose a Latent Noise Injection method
using Masked Autoregressive Flows (MAF). Instead of directly sampling from the
trained model, our method perturbs each data point in the latent space and maps
it back to the data domain. This construction preserves a one to one
correspondence between observed and synthetic data, enabling synthetic outputs
that closely reflect the underlying distribution, particularly in challenging
high-dimensional regimes where traditional sampling struggles.
  Our procedure satisfies local $(\epsilon, \delta)$-differential privacy and
introduces a single perturbation parameter to control the privacy-utility
trade-off. Although estimators based on individual synthetic datasets may
converge slowly, we show both theoretically and empirically that aggregating
across $K$ studies in a meta analysis framework restores classical efficiency
and yields consistent, reliable inference. We demonstrate that with a
well-calibrated perturbation parameter, Latent Noise Injection achieves strong
statistical alignment with the original data and robustness against membership
inference attacks. These results position our method as a compelling
alternative to conventional flow-based sampling for synthetic data sharing in
decentralized and privacy-sensitive domains, such as biomedical research.

</details>


### [316] [Sampling conditioned diffusions via Pathspace Projected Monte Carlo](https://arxiv.org/abs/2506.15743)
*Tobias Grafke*

Key words: 随机微分方程, Metropolis-adjusted流形采样, 约束条件, 路径采样

TL;DR: 提出一种算法，用于在满足一般约束条件下对随机微分方程进行采样，包括积分约束、端点约束和随机积分约束。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 解决在复杂约束条件下对随机路径进行采样的问题。

Method: 采用路径空间Metropolis-adjusted流形采样方案，在满足约束的子流形上采样随机路径。

Result: 算法成功应用于多种场景，如动态凝聚相变、随机非线性波动方程的高振幅波约束等。

Conclusion: 该算法在复杂约束条件下具有广泛适用性和有效性。

Abstract: We present an algorithm to sample stochastic differential equations
conditioned on rather general constraints, including integral constraints,
endpoint constraints, and stochastic integral constraints. The algorithm is a
pathspace Metropolis-adjusted manifold sampling scheme, which samples
stochastic paths on the submanifold of realizations that adhere to the
conditioning constraint. We demonstrate the effectiveness of the algorithm by
sampling a dynamical condensation phase transition, conditioning a random walk
on a fixed Levy stochastic area, conditioning a stochastic nonlinear wave
equation on high amplitude waves, and sampling a stochastic partial
differential equation model of turbulent pipe flow conditioned on
relaminarization events.

</details>


### [317] [From Local Interactions to Global Operators: Scalable Gaussian Process Operator for Physical Systems](https://arxiv.org/abs/2506.15906)
*Sawan Kumar,Tapas Tripura,Rajdip Nayek,Souvik Chakraborty*

Key words: 算子学习、高斯过程算子、偏微分方程、稀疏核、局部近似、Kronecker分解

TL;DR: 提出了一种新颖、可扩展的高斯过程算子（GPO）方法，通过稀疏性、局部性和结构信息优化核设计，解决大规模数据和高维输入的计算挑战。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 传统的概率神经网络算子在处理高维数据密集型问题时存在计算复杂度高的问题，需要一种更高效的方法。

Method: 利用空间域中最邻近局部核近似、参数空间中的稀疏核近似和结构化的Kronecker分解，结合任务感知的均值函数设计。

Result: 在多种非线性偏微分方程（如Navier-Stokes、波平流等）中表现高精度，证明了方法的可扩展性和准确性。

Conclusion: 该方法在可扩展性和精度之间取得了平衡，为复杂物理系统提供了不确定性建模的可靠基础。

Abstract: Operator learning offers a powerful paradigm for solving parametric partial
differential equations (PDEs), but scaling probabilistic neural operators such
as the recently proposed Gaussian Processes Operators (GPOs) to
high-dimensional, data-intensive regimes remains a significant challenge. In
this work, we introduce a novel, scalable GPO, which capitalizes on sparsity,
locality, and structural information through judicious kernel design.
Addressing the fundamental limitation of cubic computational complexity, our
method leverages nearest-neighbor-based local kernel approximations in the
spatial domain, sparse kernel approximation in the parameter space, and
structured Kronecker factorizations to enable tractable inference on
large-scale datasets and high-dimensional input. While local approximations
often introduce accuracy trade-offs due to limited kernel interactions, we
overcome this by embedding operator-aware kernel structures and employing
expressive, task-informed mean functions derived from neural operator
architectures. Through extensive evaluations on a broad class of nonlinear PDEs
- including Navier-Stokes, wave advection, Darcy flow, and Burgers' equations -
we demonstrate that our framework consistently achieves high accuracy across
varying discretization scales. These results underscore the potential of our
approach to bridge the gap between scalability and fidelity in GPO, offering a
compelling foundation for uncertainty-aware modeling in complex physical
systems.

</details>


### [318] [Diffusion-Based Hypothesis Testing and Change-Point Detection](https://arxiv.org/abs/2506.16089)
*Sean Moushegian,Taposh Banerjee,Vahid Tarokh*

Key words: 分数方法, 扩散散度, 假设检验, 变点检测

TL;DR: 本文总结了基于分数方法的假设检验和变点检测的局限性，并通过引入扩散散度扩展了这些方法。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现有基于分数的方法在假设检验和变点检测中效果不如基于似然的方法，因此希望通过扩散散度改进性能。

Method: 通过矩阵变换和权重矩阵扩展分数方法为基于扩散的方法，并理论分析其性能。同时提出了数值优化权重矩阵的方法。

Result: 理论分析了基于扩散的算法性能，并通过数值模拟展示了其优势。

Conclusion: 基于扩散的方法在某些场景下能实现最优性能，优于传统分数方法。

Abstract: Score-based methods have recently seen increasing popularity in modeling and
generation. Methods have been constructed to perform hypothesis testing and
change-point detection with score functions, but these methods are in general
not as powerful as their likelihood-based peers. Recent works consider
generalizing the score-based Fisher divergence into a diffusion-divergence by
transforming score functions via multiplication with a matrix-valued function
or a weight matrix. In this paper, we extend the score-based hypothesis test
and change-point detection stopping rule into their diffusion-based analogs.
Additionally, we theoretically quantify the performance of these
diffusion-based algorithms and study scenarios where optimal performance is
achievable. We propose a method of numerically optimizing the weight matrix and
present numerical simulations to illustrate the advantages of diffusion-based
algorithms.

</details>


### [319] [Random feature approximation for general spectral methods](https://arxiv.org/abs/2506.16283)
*Mike Nguyen,Nicole Mücke*

Key words: 随机特征方法, 谱正则化, 神经网络切核, 梯度下降, 泛化性质

TL;DR: 该论文研究了随机特征方法的泛化性质，将之前Tikhonov正则化的结果扩展到一类广泛的谱正则化技术，并对神经网络的NTK方法进行了理论分析。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 随机特征近似是大型学习算法中广泛使用的核方法技术之一，但其泛化性质的研究仍有不足，尤其是对非显式方法（如梯度下降）的分析。

Method: 扩展了谱正则化技术的分析框架，包括显式方法和隐式方法（如梯度下降、Heavy-Ball和Nesterov方法），并结合NTK理论分析神经网络。

Result: 在适当的源条件下，获得了正则类上的最优学习率，即使对于不属于再生核希尔伯特空间的类也适用，改进了之前相关设置中的结果。

Conclusion: 该研究为随机特征方法提供了更全面的理论支持，尤其是在神经网络和梯度下降方法中的应用，填补了部分理论空白。

Abstract: Random feature approximation is arguably one of the most widely used
techniques for kernel methods in large-scale learning algorithms. In this work,
we analyze the generalization properties of random feature methods, extending
previous results for Tikhonov regularization to a broad class of spectral
regularization techniques. This includes not only explicit methods but also
implicit schemes such as gradient descent and accelerated algorithms like the
Heavy-Ball and Nesterov method. Through this framework, we enable a theoretical
analysis of neural networks and neural operators through the lens of the Neural
Tangent Kernel (NTK) approach trained via gradient descent. For our estimators
we obtain optimal learning rates over regularity classes (even for classes that
are not included in the reproducing kernel Hilbert space), which are defined
through appropriate source conditions. This improves or completes previous
results obtained in related settings for specific kernel algorithms.

</details>


### [320] [The Condition Number as a Scale-Invariant Proxy for Information Encoding in Neural Units](https://arxiv.org/abs/2506.16289)
*Oswaldo Ludwig*

Key words: 条件数, 信息编码, 选择性微调, 灾难性遗忘, 多模态大语言模型

TL;DR: 论文探讨了神经网络权重张量的条件数与信息编码的关系，提出高条件数可能反映选择性信息放大与压缩，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 研究神经网络中权重张量的条件数与信息编码效率的关系，为选择性微调提供理论基础。

Method: 通过信息论分析线性单元和高斯输入的条件数与信息熵关系，并应用于多模态大语言模型的选择性微调。

Result: 高条件数对应信息传输效率提升，选择性微调可缓解跨模态适应的灾难性遗忘问题。

Conclusion: 条件数可作为信息编码效率的指标，选择性微调为实际应用提供新途径。

Abstract: This paper explores the relationship between the condition number of a neural
network's weight tensor and the extent of information encoded by the associated
processing unit, viewed through the lens of information theory. We argue that a
high condition number, though not sufficient for effective knowledge encoding,
may indicate that the unit has learned to selectively amplify and compress
information. We formalize this intuition, particularly for linear units with
Gaussian inputs, linking the condition number and the transformation's
log-volume scaling factor to the characteristics of the output entropy and the
geometric properties of the learned transformation. Our analysis demonstrates
that for a fixed weight norm, a concentrated distribution of singular values
(high condition number) corresponds to reduced overall information transfer,
indicating a specialized and efficient encoding strategy. Furthermore, we
present a practical case study where these principles are applied to guide
selective fine-tuning of a multimodal Large Language Model, aiming to mitigate
catastrophic forgetting during cross-modal adaptation. Unlike many existing
catastrophic forgetting mitigation methods that rely on access to pre-training
statistics, which are often unavailable, our selective fine-tuning approach
offers a way to bypass this common requirement.

</details>


### [321] [Identifying Heterogeneity in Distributed Learning](https://arxiv.org/abs/2506.16394)
*Zelin Xiao,Jia Gu,Song Xi Chen*

Key words: 分布式M估计, 异质性参数, Wald检验, 极端对比检验, 样本分割

TL;DR: 论文提出了两种分布式M估计中识别异质参数的方法：一种基于重归一化Wald检验，另一种基于极端对比检验（ECT）。前者在异质性密集时有效，后者适用于稀疏异质性且样本量较大的情况。结合两种方法可增强鲁棒性，并通过实验验证了其错误率和效能。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 研究在分布式M估计中如何以最小的数据传输识别异质参数，解决不同数据块间参数异质性的问题。

Method: 1. 重归一化Wald检验，适用于异质性密集的情况。2. 极端对比检验（ECT），通过样本分割避免偏差积累，适用于稀疏异质性。

Result: Wald检验在异质性密集时一致有效，ECT在稀疏异质性且样本量较大时一致有效。结合两种方法提升了鲁棒性。

Conclusion: 提出的方法在识别异质参数方面高效且通信成本低，适用于不同异质性场景。

Abstract: We study methods for identifying heterogeneous parameter components in
distributed M-estimation with minimal data transmission. One is based on a
re-normalized Wald test, which is shown to be consistent as long as the number
of distributed data blocks $K$ is of a smaller order of the minimum block
sample size {and the level of heterogeneity is dense}. The second one is an
extreme contrast test (ECT) based on the difference between the largest and
smallest component-wise estimated parameters among data blocks. By introducing
a sample splitting procedure, the ECT can avoid the bias accumulation arising
from the M-estimation procedures, and exhibits consistency for $K$ being much
larger than the sample size while the heterogeneity is sparse. The ECT
procedure is easy to operate and communication-efficient. A combination of the
Wald and the extreme contrast tests is formulated to attain more robust power
under varying levels of sparsity of the heterogeneity. We also conduct
intensive numerical experiments to compare the family-wise error rate (FWER)
and the power of the proposed methods. Additionally, we conduct a case study to
present the implementation and validity of the proposed methods.

</details>


### [322] [On Continuous Monitoring of Risk Violations under Unknown Shift](https://arxiv.org/abs/2506.16416)
*Alexander Timans,Rajeev Verma,Eric Nalisnick,Christian A. Naesseth*

Key words: 机器学习, 动态数据流, 风险监控, 顺序假设检验

TL;DR: 提出了一个实时监控风险违规的通用框架，用于动态数据流中的机器学习系统，通过顺序假设检验检测风险违规，同时控制误报率。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现实中的机器学习系统面临动态且不可预测的分布变化，需要持续监控部署可靠性。

Method: 利用‘赌注测试’范式，提出顺序假设检验方法，检测模型决策机制中的风险违规。

Result: 方法在异常检测和集合预测等任务中有效监控风险。

Conclusion: 该方法适用于多种分布变化场景，具有广泛适用性。

Abstract: Machine learning systems deployed in the real world must operate under
dynamic and often unpredictable distribution shifts. This challenges the
validity of statistical safety assurances on the system's risk established
beforehand. Common risk control frameworks rely on fixed assumptions and lack
mechanisms to continuously monitor deployment reliability. In this work, we
propose a general framework for the real-time monitoring of risk violations in
evolving data streams. Leveraging the 'testing by betting' paradigm, we propose
a sequential hypothesis testing procedure to detect violations of bounded risks
associated with the model's decision-making mechanism, while ensuring control
on the false alarm rate. Our method operates under minimal assumptions on the
nature of encountered shifts, rendering it broadly applicable. We illustrate
the effectiveness of our approach by monitoring risks in outlier detection and
set prediction under a variety of shifts.

</details>


### [323] [Schrödinger Bridge Matching for Tree-Structured Costs and Entropic Wasserstein Barycentres](https://arxiv.org/abs/2506.17197)
*Samuel Howard,Peter Potaptchik,George Deligiannidis*

Key words: Schrödinger Bridge, Optimal Transport, IMF, Wasserstein barycentres, flow-based generative modelling

TL;DR: 本文提出了一种基于流的生成模型方法，将IMF扩展到解决树结构SB问题，继承了IMF相对于IPF的许多优势，并适用于Wasserstein barycentres的计算。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 传统IMF方法在解决SB问题时表现优异，但缺乏对树结构SB问题的支持。本文旨在扩展IMF方法，以应对更复杂的树结构优化问题。

Method: 通过Sequential bridge-matching步骤，将IMF方法扩展到树结构SB问题，并利用flow-based entropic OT求解器。

Result: 所提出的方法在树结构SB问题中继承了IMF的优势，并成功应用于Wasserstein barycentres的计算。

Conclusion: 扩展的IMF方法为树结构SB问题提供了高效且实用的解决方案，尤其适用于Wasserstein barycentres等场景。

Abstract: Recent advances in flow-based generative modelling have provided scalable
methods for computing the Schr\"odinger Bridge (SB) between distributions, a
dynamic form of entropy-regularised Optimal Transport (OT) for the quadratic
cost. The successful Iterative Markovian Fitting (IMF) procedure solves the SB
problem via sequential bridge-matching steps, presenting an elegant and
practical approach with many favourable properties over the more traditional
Iterative Proportional Fitting (IPF) procedure. Beyond the standard setting,
optimal transport can be generalised to the multi-marginal case in which the
objective is to minimise a cost defined over several marginal distributions. Of
particular importance are costs defined over a tree structure, from which
Wasserstein barycentres can be recovered as a special case. In this work, we
extend the IMF procedure to solve for the tree-structured SB problem. Our
resulting algorithm inherits the many advantages of IMF over IPF approaches in
the tree-based setting. In the specific case of Wasserstein barycentres, our
approach can be viewed as extending fixed-point approaches for barycentre
computation to the case of flow-based entropic OT solvers.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [324] [MoiréXNet: Adaptive Multi-Scale Demoiréing with Linear Attention Test-Time Training and Truncated Flow Matching Prior](https://arxiv.org/abs/2506.15929)
*Liangyan Li,Yimo Ning,Kevin Le,Wei Dong,Yunzhe Li,Jun Chen,Xiaohong Liu*

Key words: 去摩尔纹, MAP估计, 深度学习, 生成模型

TL;DR: 本文提出了一种结合MAP估计和深度学习技术的图像和视频去摩尔纹新框架，解决了现有方法在非线性和数据稀缺问题上的局限性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统方法在处理非线性摩尔纹退化时效果不佳，生成模型虽然在线性退化上表现良好，但在非线性情况下容易产生伪影。

Method: 提出了一种混合MAP框架，结合了增强的监督学习模型和截断流匹配先验（TFMP），分别学习非线性映射和细化输出。

Result: 框架结合了线性注意力和生成模型的优势，显著提升了去摩尔纹的性能。

Conclusion: 该框架有效解决了非线性退化问题，同时避免了伪影的产生。

Abstract: This paper introduces a novel framework for image and video demoir\'eing by
integrating Maximum A Posteriori (MAP) estimation with advanced deep learning
techniques. Demoir\'eing addresses inherently nonlinear degradation processes,
which pose significant challenges for existing methods.
  Traditional supervised learning approaches either fail to remove moir\'e
patterns completely or produce overly smooth results. This stems from
constrained model capacity and scarce training data, which inadequately
represent the clean image distribution and hinder accurate reconstruction of
ground-truth images. While generative models excel in image restoration for
linear degradations, they struggle with nonlinear cases such as demoir\'eing
and often introduce artifacts.
  To address these limitations, we propose a hybrid MAP-based framework that
integrates two complementary components. The first is a supervised learning
model enhanced with efficient linear attention Test-Time Training (TTT)
modules, which directly learn nonlinear mappings for RAW-to-sRGB demoir\'eing.
The second is a Truncated Flow Matching Prior (TFMP) that further refines the
outputs by aligning them with the clean image distribution, effectively
restoring high-frequency details and suppressing artifacts. These two
components combine the computational efficiency of linear attention with the
refinement abilities of generative models, resulting in improved restoration
performance.

</details>


### [325] [Beyond Audio and Pose: A General-Purpose Framework for Video Synchronization](https://arxiv.org/abs/2506.15937)
*Yosub Shin,Igor Molybog*

Key words: 视频同步, VideoSync, 基准数据, 卷积神经网络

TL;DR: VideoSync是一个不依赖特定特征提取方法的视频同步框架，适用于多种场景，并在公平条件下优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有视频同步方法依赖音频或特定视觉事件，限制了广泛适用性且缺乏可复现的基准数据。

Method: 提出VideoSync框架，基于通用特征提取，并在新数据集（单人、多人、非人场景）上评估。

Result: VideoSync在公平条件下优于SeSyn-Net等现有方法，并提出了更严格的评估框架。

Conclusion: VideoSync提升了视频同步的通用性和鲁棒性，适用于实际应用。

Abstract: Video synchronization-aligning multiple video streams capturing the same
event from different angles-is crucial for applications such as reality TV show
production, sports analysis, surveillance, and autonomous systems. Prior work
has heavily relied on audio cues or specific visual events, limiting
applicability in diverse settings where such signals may be unreliable or
absent. Additionally, existing benchmarks for video synchronization lack
generality and reproducibility, restricting progress in the field. In this
work, we introduce VideoSync, a video synchronization framework that operates
independently of specific feature extraction methods, such as human pose
estimation, enabling broader applicability across different content types. We
evaluate our system on newly composed datasets covering single-human,
multi-human, and non-human scenarios, providing both the methodology and code
for dataset creation to establish reproducible benchmarks. Our analysis reveals
biases in prior SOTA work, particularly in SeSyn-Net's preprocessing pipeline,
leading to inflated performance claims. We correct these biases and propose a
more rigorous evaluation framework, demonstrating that VideoSync outperforms
existing approaches, including SeSyn-Net, under fair experimental conditions.
Additionally, we explore various synchronization offset prediction methods,
identifying a convolutional neural network (CNN)-based model as the most
effective. Our findings advance video synchronization beyond domain-specific
constraints, making it more generalizable and robust for real-world
applications.

</details>


### [326] [Heterogeneous-Modal Unsupervised Domain Adaptation via Latent Space Bridging](https://arxiv.org/abs/2506.15971)
*Jiawen Yang,Shuhao Chen,Yucong Duan,Ke Tang,Yu Zhang*

Key words: 无监督域适应,多模态,语义分割,LSB,HMUDA

TL;DR: 论文提出了一种称为HMUDA的新设置，通过桥接域实现不同模态间的知识迁移，并设计了LSB框架，用于语义分割任务，实验表明其性能最优。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统UDA方法在处理完全不同的模态时表现不佳，因此需要一种新方法来解决这一问题。

Method: 提出LSB框架，采用双分支结构，利用特征一致性损失和域对齐损失来实现模态间的表征对齐和域间差异减少。

Result: 在六个基准数据集上的实验证明，LSB达到了最先进的性能。

Conclusion: HMUDA设置和LSB框架有效解决了不同模态间的领域适应问题，为语义分割任务提供了新思路。

Abstract: Unsupervised domain adaptation (UDA) methods effectively bridge domain gaps
but become struggled when the source and target domains belong to entirely
distinct modalities. To address this limitation, we propose a novel setting
called Heterogeneous-Modal Unsupervised Domain Adaptation (HMUDA), which
enables knowledge transfer between completely different modalities by
leveraging a bridge domain containing unlabeled samples from both modalities.
To learn under the HMUDA setting, we propose Latent Space Bridging (LSB), a
specialized framework designed for the semantic segmentation task.
Specifically, LSB utilizes a dual-branch architecture, incorporating a feature
consistency loss to align representations across modalities and a domain
alignment loss to reduce discrepancies between class centroids across domains.
Extensive experiments conducted on six benchmark datasets demonstrate that LSB
achieves state-of-the-art performance.

</details>


### [327] [GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning](https://arxiv.org/abs/2506.16141)
*Yi Chen,Yuying Ge,Rui Wang,Yixiao Ge,Junhao Cheng,Ying Shan,Xihui Liu*

Key words: 多模态大语言模型, 强化学习, 推理一致性, SEED-Bench-R1, GRPO-CARE

TL;DR: 提出了GRPO-CARE框架，用于提升多模态大语言模型在推理一致性和答案正确性上的表现，并通过SEED-Bench-R1基准测试验证其效果。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法（如GRPO）在多模态大语言模型中存在推理步骤与答案逻辑不一致的问题，且缺乏严格的评估标准。

Method: 提出GRPO-CARE框架，引入双层次奖励机制（答案正确性奖励和一致性奖励），取代KL惩罚。

Result: GRPO-CARE在SEED-Bench-R1上优于标准GRPO，性能提升6.7%，一致性提升24.5%。

Conclusion: GRPO-CARE是一种通用且有效的方法，可提升多模态大语言模型的解释性和鲁棒性。

Abstract: Recent reinforcement learning approaches, such as outcome-supervised GRPO,
have advanced Chain-of-Thought reasoning in large language models (LLMs), yet
their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack
of rigorous evaluation for MLLM post-training methods, we introduce
SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced
perception and reasoning. It offers a large training set and evaluates
generalization across three escalating challenges: in-distribution,
cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,
we find that standard GRPO, while improving answer accuracy, often reduces
logical coherence between reasoning steps and answers, with only a 57.9%
consistency rate. This stems from reward signals focusing solely on final
answers, encouraging shortcuts, and strict KL penalties limiting exploration.To
address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing
both answer correctness and reasoning coherence without explicit supervision.
GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer
correctness, and (2) an adaptive consistency bonus, computed by comparing the
model's reasoning-to-answer likelihood (via a slowly-evolving reference model)
against group peers.This dual mechanism amplifies rewards for reasoning paths
that are both correct and logically consistent. Replacing KL penalties with
this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,
achieving a 6.7% performance gain on the hardest evaluation level and a 24.5%
improvement in consistency. It also shows strong transferability, improving
model performance across diverse video understanding benchmarks. Our work
contributes a systematically designed benchmark and a generalizable
post-training framework, advancing the development of more interpretable and
robust MLLMs.

</details>


### [328] [Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization](https://arxiv.org/abs/2506.15980)
*Cong Wang,Zexuan Deng,Zhiwei Jiang,Fei Shen,Yafeng Yin,Shiwei Gan,Zifeng Cheng,Shiping Ge,Qing Gu*

Key words: 手语视频生成,多细粒度条件,离散标记化,视频扩散模型

TL;DR: SignViP是一种新的手语视频生成框架，通过多细粒度条件和离散标记化提升生成质量。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法依赖单一粗粒度条件，限制了生成视频的自然性和表现力。

Method: 提出SignViP框架，包含视频扩散模型、FSQ自动编码器和多条件标记翻译器。

Result: 实验表明SignViP在视频质量、时间一致性和语义保真度上达到最优。

Conclusion: SignViP通过多条件整合提升了手语视频生成的性能。

Abstract: Sign Language Video Generation (SLVG) seeks to generate identity-preserving
sign language videos from spoken language texts. Existing methods primarily
rely on the single coarse condition (\eg, skeleton sequences) as the
intermediary to bridge the translation model and the video generation model,
which limits both the naturalness and expressiveness of the generated videos.
To overcome these limitations, we propose SignViP, a novel SLVG framework that
incorporates multiple fine-grained conditions for improved generation fidelity.
Rather than directly translating error-prone high-dimensional conditions,
SignViP adopts a discrete tokenization paradigm to integrate and represent
fine-grained conditions (\ie, fine-grained poses and 3D hands). SignViP
contains three core components. (1) Sign Video Diffusion Model is jointly
trained with a multi-condition encoder to learn continuous embeddings that
encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization
(FSQ) Autoencoder is further trained to compress and quantize these embeddings
into discrete tokens for compact representation of the conditions. (3)
Multi-Condition Token Translator is trained to translate spoken language text
to discrete multi-condition tokens. During inference, Multi-Condition Token
Translator first translates the spoken language text into discrete
multi-condition tokens. These tokens are then decoded to continuous embeddings
by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion
Model to guide video generation. Experimental results show that SignViP
achieves state-of-the-art performance across metrics, including video quality,
temporal coherence, and semantic fidelity. The code is available at
https://github.com/umnooob/signvip/.

</details>


### [329] [DIGMAPPER: A Modular System for Automated Geologic Map Digitization](https://arxiv.org/abs/2506.16006)
*Weiwei Duan,Michael P. Gerlek,Steven N. Minton,Craig A. Knoblock,Fandel Lin,Theresa Chen,Leeje Jang,Sofia Kirsanova,Zekun Li,Yijun Lin,Yao-Yi Chiang*

Key words: 地质地图数字化, 深度学习, 地理配准, 合成数据, Transformer模型

TL;DR: DIGMAPPER是一个模块化、可扩展的系统，用于自动化地质地图的数字化，结合了深度学习模型和创新技术，显著提高了效率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 历史地质地图包含丰富的地质信息，但数字化过程耗时长且劳动密集，DIGMAPPER旨在解决这一问题。

Method: 系统采用Docker化的工作流架构，整合了深度学习模型，包括地图布局分析、特征提取和地理配准，并利用大语言模型、合成数据生成和Transformer模型等技术。

Result: 在DARPA-USGS数据集上，系统展现出高准确度的多边形、线和点特征提取，以及可靠的地理配准性能。

Conclusion: DIGMAPPER已部署于USGS，显著加速了地理空间数据集的创建，支持关键矿产评估和地质科学应用。

Abstract: Historical geologic maps contain rich geospatial information, such as rock
units, faults, folds, and bedding planes, that is critical for assessing
mineral resources essential to renewable energy, electric vehicles, and
national security. However, digitizing maps remains a labor-intensive and
time-consuming task. We present DIGMAPPER, a modular, scalable system developed
in collaboration with the United States Geological Survey (USGS) to automate
the digitization of geologic maps. DIGMAPPER features a fully dockerized,
workflow-orchestrated architecture that integrates state-of-the-art deep
learning models for map layout analysis, feature extraction, and
georeferencing. To overcome challenges such as limited training data and
complex visual content, our system employs innovative techniques, including
in-context learning with large language models, synthetic data generation, and
transformer-based models. Evaluations on over 100 annotated maps from the
DARPA-USGS dataset demonstrate high accuracy across polygon, line, and point
feature extraction, and reliable georeferencing performance. Deployed at USGS,
DIGMAPPER significantly accelerates the creation of analysis-ready geospatial
datasets, supporting national-scale critical mineral assessments and broader
geoscientific applications.

</details>


### [330] [Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs](https://arxiv.org/abs/2506.16962)
*Haoran Sun,Yankai Jiang,Wenjie Lou,Yujie Zhang,Wenjie Li,Lilong Wang,Mianxin Liu,Lei Liu,Xiaosong Wang*

Key words: 多模态大语言模型,医学推理,推理路径搜索,Chiron-o1,MICS

TL;DR: 提出了一种名为MICS的新方法，用于生成医学多模态大语言模型（MLLMs）的推理路径，并构建了一个多任务医学推理数据集MMRP和一个新模型Chiron-o1，其性能在多个基准测试中达到最优。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前医学领域MLLMs的推理能力仍处于早期阶段，缺乏全面的框架来搜索和评估关键诊断的有效推理路径。

Method: 提出MICS方案，通过导师模型分步初始化推理路径，由实习模型继续推进推理，并根据MICS-Score选择最优路径。构建MMRP数据集和Chiron-o1模型。

Result: Chiron-o1在多个医学视觉问答和推理基准测试中表现最优。

Conclusion: MICS方法能有效提升医学MLLMs的推理能力，相关数据集和模型为医学领域提供了新工具。

Abstract: Multimodal large language models (MLLMs) have begun to demonstrate robust
reasoning capabilities on general tasks, yet their application in the medical
domain remains in its early stages. Constructing chain-of-thought (CoT)
training data is essential for bolstering the reasoning abilities of medical
MLLMs. However, existing approaches exhibit a deficiency in offering a
comprehensive framework for searching and evaluating effective reasoning paths
towards critical diagnosis. To address this challenge, we propose Mentor-Intern
Collaborative Search (MICS), a novel reasoning-path searching scheme to
generate rigorous and effective medical CoT data. MICS first leverages mentor
models to initialize the reasoning, one step at a time, then prompts each
intern model to continue the thinking along those initiated paths, and finally
selects the optimal reasoning path according to the overall reasoning
performance of multiple intern models. The reasoning performance is determined
by an MICS-Score, which assesses the quality of generated reasoning paths.
Eventually, we construct MMRP, a multi-task medical reasoning dataset with
ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum
learning strategy, with robust visual question-answering and generalizable
reasoning capabilities. Extensive experiments demonstrate that Chiron-o1,
trained on our CoT dataset constructed using MICS, achieves state-of-the-art
performance across a list of medical visual question answering and reasoning
benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing
Step-by-Step and Verifiable Medical Reasoning in MLLMs

</details>


### [331] [MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation](https://arxiv.org/abs/2506.17113)
*Shoubin Yu,Yue Zhang,Ziyang Wang,Jaehong Yoon,Mohit Bansal*

Key words: MEXA, 多模态推理, 专家模型, LRM, 跨领域任务

TL;DR: MEXA是一个无需训练的框架，通过动态选择专家模型并结合大型推理模型（LRM）进行多模态推理，显著提升了跨领域任务的表现。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 多模态推理中，由于模态和任务的多样性，构建统一框架面临挑战。MEXA旨在通过专家模型的动态选择和聚合解决这一问题。

Method: MEXA根据输入模态和任务需求动态选择专家模型，生成可解释的推理输出，并通过LRM进行聚合和最终推理。

Result: 在多项多模态基准测试中，MEXA表现优于基线模型，证明了其在跨领域任务中的有效性。

Conclusion: MEXA通过模块化设计实现了灵活、透明的多模态推理，无需额外训练，具有广泛适用性。

Abstract: Combining pre-trained expert models offers substantial potential for scalable
multimodal reasoning, but building a unified framework remains challenging due
to the increasing diversity of input modalities and task complexity. For
instance, medical diagnosis requires precise reasoning over structured clinical
tables, while financial forecasting depends on interpreting plot-based data to
make informed predictions. To tackle this challenge, we introduce MEXA, a
training-free framework that performs modality- and task-aware aggregation of
multiple expert models to enable effective multimodal reasoning across diverse
and distinct domains. MEXA dynamically selects expert models based on the input
modality and the task-specific reasoning demands (i.e., skills). Each expert
model, specialized in a modality task pair, generates interpretable textual
reasoning outputs. MEXA then aggregates and reasons over these outputs using a
Large Reasoning Model (LRM) to produce the final answer. This modular design
allows flexible and transparent multimodal reasoning across diverse domains
without additional training overhead. We extensively evaluate our approach on
diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D
Understanding, and Medical QA. MEXA consistently delivers performance
improvements over strong multimodal baselines, highlighting the effectiveness
and broad applicability of our expert-driven selection and aggregation in
diverse multimodal reasoning tasks.

</details>


### [332] [SycnMapV2: Robust and Adaptive Unsupervised Segmentation](https://arxiv.org/abs/2506.16297)
*Heng Zhang,Zikang Wan,Danilo Vasconcellos Vargas*

Key words: 无监督分割, 鲁棒性, 自适应算法, 自组织动力学, SyncMapV2

TL;DR: SyncMapV2 是首个解决无监督分割问题的算法，具有最先进的鲁棒性，无需训练或监督，且在噪声、天气和模糊等干扰下性能优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有 AI 算法在噪声增加时性能急剧下降，而人类视觉却能保持鲁棒性，因此需要开发一种无需训练、能适应各种干扰的算法。

Method: SyncMapV2 基于自组织动力学方程和随机网络概念，实现无监督分割，并能够在线适应新输入，无需重新初始化。

Result: 在数字干扰下，SyncMapV2 的 mIoU 仅下降 0.01%，远优于现有方法（23.8%）。噪声、天气和模糊干扰下的性能也显著领先。

Conclusion: SyncMapV2 展示了无监督、自适应分割的潜力，为未来鲁棒和自适应智能的发展奠定了基础。

Abstract: Human vision excels at segmenting visual cues without the need for explicit
training, and it remains remarkably robust even as noise severity increases. In
contrast, existing AI algorithms struggle to maintain accuracy under similar
conditions. Here, we present SyncMapV2, the first to solve unsupervised
segmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal
drop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop
observed in SOTA methods.This superior performance extends across various types
of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur (7.0%
vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust training,
supervision, or loss functions. It is based on a learning paradigm that uses
self-organizing dynamical equations combined with concepts from random
networks. Moreover,unlike conventional methods that require re-initialization
for each new input, SyncMapV2 adapts online, mimicking the continuous
adaptability of human vision. Thus, we go beyond the accurate and robust
results, and present the first algorithm that can do all the above online,
adapting to input rather than re-initializing. In adaptability tests, SyncMapV2
demonstrates near-zero performance degradation, which motivates and fosters a
new generation of robust and adaptive intelligence in the near future.

</details>


### [333] [Learning Multi-scale Spatial-frequency Features for Image Denoising](https://arxiv.org/abs/2506.16307)
*Xu Zhao,Chen Zhao,Xiantao Hu,Hongliang Zhang,Ying Tai,Jian Yang*

Key words: 图像去噪,多尺度架构,自适应双域网络,ASFU,全局特征融合

TL;DR: 提出了一种新型多尺度自适应双域网络(MADNet)用于图像去噪，通过图像金字塔输入和自适应空间频率学习单元(ASFU)提升性能，实验验证了其优越性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有去噪方法多依赖固定单输入单输出的Unet架构，忽略了像素级多尺度表征和频域噪声特性的差异。

Method: 采用图像金字塔输入，设计ASFU单元通过可学习掩码分离高低频信息，并在跳跃连接中加入全局特征融合块。

Result: 在合成和真实噪声图像数据集上的实验表明，MADNet优于现有先进去噪方法。

Conclusion: MADNet通过多尺度自适应和双域处理，显著提升了图像去噪效果。

Abstract: Recent advancements in multi-scale architectures have demonstrated
exceptional performance in image denoising tasks. However, existing
architectures mainly depends on a fixed single-input single-output Unet
architecture, ignoring the multi-scale representations of pixel level. In
addition, previous methods treat the frequency domain uniformly, ignoring the
different characteristics of high-frequency and low-frequency noise. In this
paper, we propose a novel multi-scale adaptive dual-domain network (MADNet) for
image denoising. We use image pyramid inputs to restore noise-free results from
low-resolution images. In order to realize the interaction of high-frequency
and low-frequency information, we design an adaptive spatial-frequency learning
unit (ASFU), where a learnable mask is used to separate the information into
high-frequency and low-frequency components. In the skip connections, we design
a global feature fusion block to enhance the features at different scales.
Extensive experiments on both synthetic and real noisy image datasets verify
the effectiveness of MADNet compared with current state-of-the-art denoising
approaches.

</details>


### [334] [Segment Anything for Satellite Imagery: A Strong Baseline and a Regional Dataset for Automatic Field Delineation](https://arxiv.org/abs/2506.16318)
*Carmelo Scribano,Elena Govi,Paolo bertellini,Simone Parisi,Giorgia Franchini,Marko Bertogna*

Key words: 农田边界制图, SAM模型, 微调, ERAS数据集

TL;DR: 提出一种基于SAM模型的农田边界自动提取方法，通过微调策略优化模型，并发布新的区域数据集ERAS。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 农业边界精确制图对高效农业运作至关重要，自动提取可减少地面调查成本。

Method: 基于SAM模型的管道，采用微调策略，并补充新的区域数据集ERAS。

Result: 实验验证了分割准确性和泛化能力，为自动农田边界提取提供了基准。

Conclusion: 提出的方法效果良好，新数据集ERAS已公开。

Abstract: Accurate mapping of agricultural field boundaries is essential for the
efficient operation of agriculture. Automatic extraction from high-resolution
satellite imagery, supported by computer vision techniques, can avoid costly
ground surveys. In this paper, we present a pipeline for field delineation
based on the Segment Anything Model (SAM), introducing a fine-tuning strategy
to adapt SAM to this task. In addition to using published datasets, we describe
a method for acquiring a complementary regional dataset that covers areas
beyond current sources. Extensive experiments assess segmentation accuracy and
evaluate the generalization capabilities. Our approach provides a robust
baseline for automated field delineation. The new regional dataset, known as
ERAS, is now publicly available.

</details>


### [335] [Reliable Few-shot Learning under Dual Noises](https://arxiv.org/abs/2506.16330)
*Ji Zhang,Jingkuan Song,Lianli Gao,Nicu Sebe,Heng Tao Shen*

Key words: 少样本学习, 任务适应, 去噪, 对比学习, 记忆库

TL;DR: DETA++提出了一种去噪任务适应方法，通过对比相关性聚合模块和记忆库来提升小样本学习在开放世界中的可靠性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有小样本学习方法在开放世界中容易受到分布内和分布外噪声的影响，导致任务适应和预测不可靠。

Method: DETA++采用对比相关性聚合（CoRA）模块计算支持样本权重，结合干净原型损失和噪声熵最大化损失实现任务适应，并使用记忆库和局部最近质心分类器（LocalNCC）增强预测鲁棒性。

Result: 实验表明DETA++能有效应对双噪声问题，提升小样本学习的可靠性。

Conclusion: DETA++通过多种策略有效解决了小样本学习中的噪声问题，具有较好的灵活性和实用性。

Abstract: Recent advances in model pre-training give rise to task adaptation-based
few-shot learning (FSL), where the goal is to adapt a pre-trained task-agnostic
model for capturing task-specific knowledge with a few-labeled support samples
of the target task.Nevertheless, existing approaches may still fail in the open
world due to the inevitable in-distribution (ID) and out-of-distribution (OOD)
noise from both support and query samples of the target task. With limited
support samples available, i) the adverse effect of the dual noises can be
severely amplified during task adaptation, and ii) the adapted model can
produce unreliable predictions on query samples in the presence of the dual
noises. In this work, we propose DEnoised Task Adaptation (DETA++) for reliable
FSL. DETA++ uses a Contrastive Relevance Aggregation (CoRA) module to calculate
image and region weights for support samples, based on which a clean prototype
loss and a noise entropy maximization loss are proposed to achieve noise-robust
task adaptation. Additionally,DETA++ employs a memory bank to store and refine
clean regions for each inner-task class, based on which a Local Nearest
Centroid Classifier (LocalNCC) is devised to yield noise-robust predictions on
query samples. Moreover, DETA++ utilizes an Intra-class Region Swapping
(IntraSwap) strategy to rectify ID class prototypes during task adaptation,
enhancing the model's robustness to the dual noises. Extensive experiments
demonstrate the effectiveness and flexibility of DETA++.

</details>


### [336] [CLIP-MG: Guiding Semantic Attention with Skeletal Pose Features and RGB Data for Micro-Gesture Recognition on the iMiGUE Dataset](https://arxiv.org/abs/2506.16385)
*Santosh Patapati,Trisanth Srinivasan,Amith Adiraju*

Key words: 微手势识别, CLIP, 多模态融合, 姿态引导, 情感计算

TL;DR: 本文提出了一种名为CLIP-MG的改进CLIP模型，用于微手势识别任务，通过结合人体骨骼信息和多模态融合机制，在iMiGUE数据集上实现了61.82%的Top-1准确率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 微手势识别在情感计算中具有挑战性，因为其动作细微且多为无意识行为。本文旨在改进CLIP模型以适应这一任务。

Method: 提出CLIP-MG模型，通过姿态引导的语义查询生成和门控多模态融合机制，将人体骨骼信息整合到CLIP模型中。

Result: 在iMiGUE数据集上，CLIP-MG的Top-1准确率达到61.82%。

Conclusion: CLIP-MG展示了将视觉-语言模型应用于微手势识别的潜力，但仍存在改进空间。

Abstract: Micro-gesture recognition is a challenging task in affective computing due to
the subtle, involuntary nature of the gestures and their low movement
amplitude. In this paper, we introduce a Pose-Guided Semantics-Aware CLIP-based
architecture, or CLIP for Micro-Gesture recognition (CLIP-MG), a modified CLIP
model tailored for micro-gesture classification on the iMiGUE dataset. CLIP-MG
integrates human pose (skeleton) information into the CLIP-based recognition
pipeline through pose-guided semantic query generation and a gated multi-modal
fusion mechanism. The proposed model achieves a Top-1 accuracy of 61.82%. These
results demonstrate both the potential of our approach and the remaining
difficulty in fully adapting vision-language models like CLIP for micro-gesture
recognition.

</details>


### [337] [Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks](https://arxiv.org/abs/2506.16407)
*Dong Nguyen Tien,Dung D. Le*

Key words: 视觉文档理解, 对抗攻击, OCR, 多模态, 鲁棒性

TL;DR: 研究者提出了首个统一框架，用于生成和评估基于OCR的视觉文档理解（VDU）模型的多模态对抗攻击，涵盖六种梯度布局攻击场景，并验证了其在四种数据集和六种模型家族中的有效性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探索VDU系统在现实对抗扰动下的鲁棒性，弥补当前研究的不足。

Method: 采用梯度布局攻击方法，包括OCR边界框、像素和文本的多粒度扰动，并通过布局预算约束保证合理性。

Result: 实验显示，行级攻击和复合扰动（边界框+像素+文本）导致性能下降最严重，PGD方法在所有模型中表现优于随机基线。

Conclusion: 多模态对抗攻击能有效揭示VDU模型的弱点，布局预算和文本修改对攻击效果有显著影响。

Abstract: Visual Document Understanding (VDU) systems have achieved strong performance
in information extraction by integrating textual, layout, and visual signals.
However, their robustness under realistic adversarial perturbations remains
insufficiently explored. We introduce the first unified framework for
generating and evaluating multi-modal adversarial attacks on OCR-based VDU
models. Our method covers six gradient-based layout attack scenarios,
incorporating manipulations of OCR bounding boxes, pixels, and texts across
both word and line granularities, with constraints on layout perturbation
budget (e.g., IoU >= 0.6) to preserve plausibility.
  Experimental results across four datasets (FUNSD, CORD, SROIE, DocVQA) and
six model families demonstrate that line-level attacks and compound
perturbations (BBox + Pixel + Text) yield the most severe performance
degradation. Projected Gradient Descent (PGD)-based BBox perturbations
outperform random-shift baselines in all investigated models. Ablation studies
further validate the impact of layout budget, text modification, and
adversarial transferability.

</details>


### [338] [Efficient Transformations in Deep Learning Convolutional Neural Networks](https://arxiv.org/abs/2506.16418)
*Berk Yilmaz,Daniel Fidel Harvey,Prajit Dhuri*

Key words: 信号处理变换, ResNet50, 能量效率, 图像分类, WHT

TL;DR: 研究探讨了在ResNet50模型中集成信号处理变换（FFT、WHT、DCT）对图像分类的影响，发现WHT显著降低了能耗并提高了准确性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 评估信号处理变换在CNN中计算效率、能耗和分类准确性的权衡，为节能应用提供解决方案。

Method: 在ResNet50中集成FFT、WHT和DCT，使用CIFAR-100数据集进行训练和测试。

Result: WHT在早期或全层应用时，准确率分别提升至74%和79%，能耗从25,606 kJ降至39 kJ。

Conclusion: WHT在能源受限的CNN应用中表现出高效和有效性。

Abstract: This study investigates the integration of signal processing transformations
-- Fast Fourier Transform (FFT), Walsh-Hadamard Transform (WHT), and Discrete
Cosine Transform (DCT) -- within the ResNet50 convolutional neural network
(CNN) model for image classification. The primary objective is to assess the
trade-offs between computational efficiency, energy consumption, and
classification accuracy during training and inference. Using the CIFAR-100
dataset (100 classes, 60,000 images), experiments demonstrated that
incorporating WHT significantly reduced energy consumption while improving
accuracy. Specifically, a baseline ResNet50 model achieved a testing accuracy
of 66%, consuming an average of 25,606 kJ per model. In contrast, a modified
ResNet50 incorporating WHT in the early convolutional layers achieved 74%
accuracy, and an enhanced version with WHT applied to both early and late
layers achieved 79% accuracy, with an average energy consumption of only 39 kJ
per model. These results demonstrate the potential of WHT as a highly efficient
and effective approach for energy-constrained CNN applications.

</details>


### [339] [Spotting tell-tale visual artifacts in face swapping videos: strengths and pitfalls of CNN detectors](https://arxiv.org/abs/2506.16497)
*Riccardo Ziglio,Cecilia Pasquini,Silvio Ranise*

Key words: face swapping, CNN, video forensics, generalization, occlusion

TL;DR: 该论文讨论了视频流中换脸操纵的威胁，研究了利用视觉特征检测换脸的方法，并评估了CNN模型在不同数据集上的性能和泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 由于自动化和实时换脸工具的进步，视频流中的换脸操纵成为远程视频通信中的威胁，论文旨在研究如何有效检测此类操纵。

Method: 论文通过基准测试CNN模型在两个数据集（包括一个新收集的数据集）上的表现，分析了模型在不同数据源和换脸算法上的泛化能力。

Result: 结果显示，通用CNN架构在相同数据源下表现优异，但在跨数据集时难以鲁棒地捕捉遮挡相关的视觉特征。

Conclusion: 论文指出，需要开发专门化的检测策略来应对换脸操纵中引入的视觉伪影。

Abstract: Face swapping manipulations in video streams represents an increasing threat
in remote video communications, due to advances
  in automated and real-time tools. Recent literature proposes to characterize
and exploit visual artifacts introduced in video frames
  by swapping algorithms when dealing with challenging physical scenes, such as
face occlusions. This paper investigates the
  effectiveness of this approach by benchmarking CNN-based data-driven models
on two data corpora (including a newly collected
  one) and analyzing generalization capabilities with respect to different
acquisition sources and swapping algorithms. The results
  confirm excellent performance of general-purpose CNN architectures when
operating within the same data source, but a significant
  difficulty in robustly characterizing occlusion-based visual cues across
datasets. This highlights the need for specialized detection
  strategies to deal with such artifacts.

</details>


### [340] [Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details](https://arxiv.org/abs/2506.16504)
*Zeqiang Lai,Yunfei Zhao,Haolin Liu,Zibo Zhao,Qingxiang Lin,Huiwen Shi,Xianghui Yang,Mingxin Yang,Shuhui Yang,Yifei Feng,Sheng Zhang,Xin Huang,Di Luo,Fan Yang,Fang Yang,Lifu Wang,Sicong Liu,Yixuan Tang,Yulin Cai,Zebin He,Tian Liu,Yuhong Liu,Jie Jiang,Linus,Jingwei Huang,Chunchao Guo*

Key words: 3D扩散模型, Hunyuan3D 2.5, LATTICE, 物理渲染, 3D形状生成

TL;DR: Hunyuan3D 2.5是一款强大的3D扩散模型套件，通过两阶段流程在形状和纹理生成上取得显著进步，引入LATTICE形状基础模型和PBR技术，性能超越先前方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 旨在生成高保真、详细纹理的3D资产，弥补生成与手工制作3D形状之间的差距。

Method: 采用两阶段流程，引入10B参数的LATTICE形状基础模型和基于物理渲染（PBR）的多视图架构。

Result: 在形状和端到端纹理生成上显著优于先前方法，生成的3D形状锐利、细节丰富。

Conclusion: Hunyuan3D 2.5在3D资产生成领域取得重要突破，为高质量3D内容创作提供了有力工具。

Abstract: In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion
models aimed at generating high-fidelity and detailed textured 3D assets.
Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D
2.0, while demonstrating substantial advancements in both shape and texture
generation. In terms of shape generation, we introduce a new shape foundation
model -- LATTICE, which is trained with scaled high-quality datasets,
model-size, and compute. Our largest model reaches 10B parameters and generates
sharp and detailed 3D shape with precise image-3D following while keeping mesh
surface clean and smooth, significantly closing the gap between generated and
handcrafted 3D shapes. In terms of texture generation, it is upgraded with
phyiscal-based rendering (PBR) via a novel multi-view architecture extended
from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D
2.5 significantly outperforms previous methods in both shape and end-to-end
texture generation.

</details>


### [341] [From Semantic To Instance: A Semi-Self-Supervised Learning Approach](https://arxiv.org/abs/2506.16563)
*Keyhan Najafian,Farhad Maleki,Lingling Jin,Ian Stavness*

Key words: 实例分割, 半自监督学习, GLMask, 农业, COCO数据集

TL;DR: 提出了一种半自监督学习方法GLMask，用于实例分割，减少了对人工标注的依赖，在农业和通用数据集上表现出色。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 实例分割在农业中很重要，但大规模标注数据集成本高，尤其是密集和遮挡对象的图像。

Method: 设计了GLMask，一种关注形状、纹理和图案的图像-掩码表示，减少对颜色特征的依赖，并通过管道生成实例分割。

Result: 在小麦头部实例分割上达到98.5% mAP@50，在COCO数据集上性能提升12.6%。

Conclusion: GLMask方法在农业和其他类似数据特征的领域具有广泛适用性。

Abstract: Instance segmentation is essential for applications such as automated
monitoring of plant health, growth, and yield. However, extensive effort is
required to create large-scale datasets with pixel-level annotations of each
object instance for developing instance segmentation models that restrict the
use of deep learning in these areas. This challenge is more significant in
images with densely packed, self-occluded objects, which are common in
agriculture. To address this challenge, we propose a semi-self-supervised
learning approach that requires minimal manual annotation to develop a
high-performing instance segmentation model. We design GLMask, an image-mask
representation for the model to focus on shape, texture, and pattern while
minimizing its dependence on color features. We develop a pipeline to generate
semantic segmentation and then transform it into instance-level segmentation.
The proposed approach substantially outperforms the conventional instance
segmentation models, establishing a state-of-the-art wheat head instance
segmentation model with mAP@50 of 98.5%. Additionally, we assessed the proposed
methodology on the general-purpose Microsoft COCO dataset, achieving a
significant performance improvement of over 12.6% mAP@50. This highlights that
the utility of our proposed approach extends beyond precision agriculture and
applies to other domains, specifically those with similar data characteristics.

</details>


### [342] [Spatially-Aware Evaluation of Segmentation Uncertainty](https://arxiv.org/abs/2506.16589)
*Tal Zeevi,Eléonore V. Lieffrig,Lawrence H. Staib,John A. Onofrey*

Key words: 不确定性评估,医学图像分割,空间信息,前列腺分割

TL;DR: 提出三种考虑空间信息的指标，优化医学图像分割中的不确定性评估。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有不确定性评估方法忽略空间上下文和解剖结构，导致对重要性不同的区域评分相同。

Method: 设计三种结合结构和边界信息的空间感知指标，并在前列腺分区分割数据上进行验证。

Result: 新指标能更好区分有意义和虚假的不确定性模式，并与临床重要性更一致。

Conclusion: 空间感知指标显著提升了不确定性评估的临床相关性。

Abstract: Uncertainty maps highlight unreliable regions in segmentation predictions.
However, most uncertainty evaluation metrics treat voxels independently,
ignoring spatial context and anatomical structure. As a result, they may assign
identical scores to qualitatively distinct patterns (e.g., scattered vs.
boundary-aligned uncertainty). We propose three spatially aware metrics that
incorporate structural and boundary information and conduct a thorough
validation on medical imaging data from the prostate zonal segmentation
challenge within the Medical Segmentation Decathlon. Our results demonstrate
improved alignment with clinically important factors and better discrimination
between meaningful and spurious uncertainty patterns.

</details>


### [343] [How to Train your Text-to-Image Model: Evaluating Design Choices for Synthetic Training Captions](https://arxiv.org/abs/2506.16679)
*Manuel Brack,Sudeep Katakol,Felix Friedrich,Patrick Schramowski,Hareesh Ravi,Kristian Kersting,Ajinkya Kale*

Key words: 文本到图像模型, 合成标题, 文本对齐, 输出多样性, 训练数据策略

TL;DR: 本文研究了合成标题对文本到图像模型性能的影响，发现高密度标题提升文本对齐但可能影响美观和多样性，而随机长度标题则平衡了美观与对齐，同时保持多样性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究合成标题策略对文本到图像模型性能的影响，填补当前文献中关于设计选择的知识空白。

Method: 系统性地研究不同合成标题生成策略，并评估其对模型下游性能的影响。

Result: 高密度标题提升文本对齐但可能牺牲美观和多样性；随机长度标题在美观、对齐和多样性之间取得平衡。标题分布的变化还会显著影响模型输出偏差。

Conclusion: 标题设计对模型性能至关重要，需权衡不同策略以实现最优效果。

Abstract: Training data is at the core of any successful text-to-image models. The
quality and descriptiveness of image text are crucial to a model's performance.
Given the noisiness and inconsistency in web-scraped datasets, recent works
shifted towards synthetic training captions. While this setup is generally
believed to produce more capable models, current literature does not provide
any insights into its design choices. This study closes this gap by
systematically investigating how different synthetic captioning strategies
impact the downstream performance of text-to-image models. Our experiments
demonstrate that dense, high-quality captions enhance text alignment but may
introduce trade-offs in output aesthetics and diversity. Conversely, captions
of randomized lengths yield balanced improvements across aesthetics and
alignment without compromising sample diversity. We also demonstrate that
varying caption distributions introduce significant shifts in the output bias
of a trained model. Our findings underscore the importance of caption design in
achieving optimal model performance and provide practical insights for more
effective training data strategies in text-to-image generation.

</details>


### [344] [PQCAD-DM: Progressive Quantization and Calibration-Assisted Distillation for Extremely Efficient Diffusion Model](https://arxiv.org/abs/2506.16776)
*Beomseok Ko,Hyeryung Jang*

Key words: 扩散模型、渐进量化、校准辅助蒸馏、模型压缩

TL;DR: PQCAD-DM结合渐进量化（PQ）和校准辅助蒸馏（CAD），有效降低扩散模型的计算资源需求，提升生成效率同时保持高质量。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 扩散模型因迭代计算和资源消耗高，且易受误差累积影响，限制了压缩技术的效果。

Method: 采用两阶段量化（PQ）和校准辅助蒸馏（CAD），PQ通过动量机制降低低精度扰动，CAD使用全精度校准数据提升蒸馏效果。

Result: PQCAD-DM显著减少推理时间，同时在不同数据集上保持竞争力。

Conclusion: PQCAD-DM在生成效率和性能之间取得平衡，优于固定比特量化方法。

Abstract: Diffusion models excel in image generation but are computational and
resource-intensive due to their reliance on iterative Markov chain processes,
leading to error accumulation and limiting the effectiveness of naive
compression techniques. In this paper, we propose PQCAD-DM, a novel hybrid
compression framework combining Progressive Quantization (PQ) and
Calibration-Assisted Distillation (CAD) to address these challenges. PQ employs
a two-stage quantization with adaptive bit-width transitions guided by a
momentum-based mechanism, reducing excessive weight perturbations in
low-precision. CAD leverages full-precision calibration datasets during
distillation, enabling the student to match full-precision performance even
with a quantized teacher. As a result, PQCAD-DM achieves a balance between
computational efficiency and generative quality, halving inference time while
maintaining competitive performance. Extensive experiments validate PQCAD-DM's
superior generative capabilities and efficiency across diverse datasets,
outperforming fixed-bit quantization methods.

</details>


### [345] [Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection](https://arxiv.org/abs/2506.16819)
*Yuchu Jiang,Jiaming Chu,Jian Zhao,Xin Zhang,Xu Yang,Lei Jin,Chi Zhang,Xuelong Li*

Key words: 深度伪造检测, 定位, 补丁级分类器, 条件查询, 测试时适应

TL;DR: Loupe是一个轻量级框架，可同时进行深度伪造检测和定位，通过补丁级分类器和条件查询的集成，实现了全局分类和细粒度掩码预测。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决现有深度伪造检测方法在跨操作类型泛化能力不足或架构复杂的问题。

Method: Loupe结合补丁感知分类器和条件查询的分割模块，并引入伪标签引导的测试时适应机制。

Result: 在IJCAI 2025竞赛中，以0.846的总分获得最佳性能。

Conclusion: Loupe通过补丁级融合和条件查询设计，提升了分类准确性和空间定位能力。

Abstract: The proliferation of generative models has raised serious concerns about
visual content forgery. Existing deepfake detection methods primarily target
either image-level classification or pixel-wise localization. While some
achieve high accuracy, they often suffer from limited generalization across
manipulation types or rely on complex architectures. In this paper, we propose
Loupe, a lightweight yet effective framework for joint deepfake detection and
localization. Loupe integrates a patch-aware classifier and a segmentation
module with conditional queries, allowing simultaneous global authenticity
classification and fine-grained mask prediction. To enhance robustness against
distribution shifts of test set, Loupe introduces a pseudo-label-guided
test-time adaptation mechanism by leveraging patch-level predictions to
supervise the segmentation head. Extensive experiments on the DDL dataset
demonstrate that Loupe achieves state-of-the-art performance, securing the
first place in the IJCAI 2025 Deepfake Detection and Localization Challenge
with an overall score of 0.846. Our results validate the effectiveness of the
proposed patch-level fusion and conditional query design in improving both
classification accuracy and spatial localization under diverse forgery
patterns. The code is available at https://github.com/Kamichanw/Loupe.

</details>


### [346] [AnyTraverse: An off-road traversability framework with VLM and human operator in the loop](https://arxiv.org/abs/2506.16826)
*Sattwik Sahu,Agamdeep Singh,Karthik Nambiar,Srikanth Saripalli,P. B. Sujit*

Key words: off-road traversability, natural language prompts, human-operator assistance, zero-shot learning, autonomous navigation

TL;DR: AnyTraverse框架通过结合自然语言提示和人类操作员辅助，实现多样化机器人车辆的可导航区域分割，减少主动监督负担并适应多变户外场景。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有框架因非结构化环境的显著变化和场景不确定性，难以适应不同机器人类型，需开发适应性更强的解决方案。

Method: AnyTraverse采用零样本学习方法，结合自然语言提示和人类操作员辅助，仅在遇到未探索场景或未知类别时调用操作员。

Result: 在多个数据集中测试表现优于GA-NAV和Off-seg，实现车辆无关的越野可穿越性分析，平衡自动化与目标监督。

Conclusion: AnyTraverse无需大量数据收集或重新训练，适应性强，是越野导航的有效解决方案。

Abstract: Off-road traversability segmentation enables autonomous navigation with
applications in search-and-rescue, military operations, wildlife exploration,
and agriculture. Current frameworks struggle due to significant variations in
unstructured environments and uncertain scene changes, and are not adaptive to
be used for different robot types. We present AnyTraverse, a framework
combining natural language-based prompts with human-operator assistance to
determine navigable regions for diverse robotic vehicles. The system segments
scenes for a given set of prompts and calls the operator only when encountering
previously unexplored scenery or unknown class not part of the prompt in its
region-of-interest, thus reducing active supervision load while adapting to
varying outdoor scenes. Our zero-shot learning approach eliminates the need for
extensive data collection or retraining. Our experimental validation includes
testing on RELLIS-3D, Freiburg Forest, and RUGD datasets and demonstrate
real-world deployment on multiple robot platforms. The results show that
AnyTraverse performs better than GA-NAV and Off-seg while offering a
vehicle-agnostic approach to off-road traversability that balances automation
with targeted human supervision.

</details>


### [347] [ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control](https://arxiv.org/abs/2506.16856)
*Jun Fu,Bin Tian,Haonan Chen,Shi Meng,Tingting Yao*

Key words: 自动驾驶停车，Transformer，端到端学习，行人预测，CARLA模拟器

TL;DR: 该论文提出了一种基于Transformer的端到端自动驾驶停车框架，通过学习专家示范来处理复杂环境中的停车任务，具有高精度和适应性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统基于规则的停车系统在动态或拥挤环境中表现不佳，而人类驾驶员可以凭直觉停车，因此需要一种更灵活的方法来解决这一问题。

Method: 使用Transformer网络，结合鸟瞰图特征、目标点、车辆运动和行人轨迹，通过交叉注意力模块和GRU行人预测器实现高精度控制。

Result: 在CARLA 0.9.14模拟器中测试，模型成功率达到96.57%，平均位置误差为0.21米，平均方向误差为0.41度。

Conclusion: 论文提出的框架在自动驾驶停车任务中表现出色，关键模块如行人预测和目标点注意力融合显著提升了性能。

Abstract: Autonomous parking plays a vital role in intelligent vehicle systems,
particularly in constrained urban environments where high-precision control is
required. While traditional rule-based parking systems struggle with
environmental uncertainties and lack adaptability in crowded or dynamic scenes,
human drivers demonstrate the ability to park intuitively without explicit
modeling. Inspired by this observation, we propose a Transformer-based
end-to-end framework for autonomous parking that learns from expert
demonstrations. The network takes as input surround-view camera images,
goal-point representations, ego vehicle motion, and pedestrian trajectories. It
outputs discrete control sequences including throttle, braking, steering, and
gear selection. A novel cross-attention module integrates BEV features with
target points, and a GRU-based pedestrian predictor enhances safety by modeling
dynamic obstacles. We validate our method on the CARLA 0.9.14 simulator in both
vertical and parallel parking scenarios. Experiments show our model achieves a
high success rate of 96.57\%, with average positional and orientation errors of
0.21 meters and 0.41 degrees, respectively. The ablation studies further
demonstrate the effectiveness of key modules such as pedestrian prediction and
goal-point attention fusion. The code and dataset will be released at:
https://github.com/little-snail-f/ParkFormer.

</details>


### [348] [Privacy-Preserving in Connected and Autonomous Vehicles Through Vision to Text Transformation](https://arxiv.org/abs/2506.15854)
*Abdolazim Rezaei,Mehdi Sookhak,Ahmad Patooghy*

Key words: 隐私保护, 强化学习, 视觉语言模型, AI摄像头, 文本描述

TL;DR: 本文提出一种新的隐私保护框架，利用反馈强化学习和视觉语言模型，将图像转换为语义等效的文本描述，既保留场景信息又保护隐私。结果显示其隐私保护和文本质量显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 针对AI摄像头捕获的隐私敏感图像数据可能被滥用的风险，传统模糊化技术不足以完全保护隐私，因此需要更有效的方法。

Method: 采用反馈强化学习和视觉语言模型，将图像转化为语义等效的文本描述，通过分层强化学习策略迭代优化生成的文本。

Result: 评估结果显示，该框架在隐私保护和文本质量上均有显著提升，独特词数增加约77%，细节密度提高约50%。

Conclusion: 提出的框架有效解决了AI摄像头图像隐私问题，同时保留了关键场景信息。

Abstract: Connected and Autonomous Vehicles (CAVs) rely on a range of devices that
often process privacy-sensitive data. Among these, roadside units play a
critical role particularly through the use of AI-equipped (AIE) cameras for
applications such as violation detection. However, the privacy risks associated
with captured imagery remain a major concern, as such data can be misused for
identity theft, profiling, or unauthorized commercial purposes. While
traditional techniques such as face blurring and obfuscation have been applied
to mitigate privacy risks, individual privacy remains at risk, as individuals
can still be tracked using other features such as their clothing. This paper
introduces a novel privacy-preserving framework that leverages feedback-based
reinforcement learning (RL) and vision-language models (VLMs) to protect
sensitive visual information captured by AIE cameras. The main idea is to
convert images into semantically equivalent textual descriptions, ensuring that
scene-relevant information is retained while visual privacy is preserved. A
hierarchical RL strategy is employed to iteratively refine the generated text,
enhancing both semantic accuracy and privacy. Evaluation results demonstrate
significant improvements in both privacy protection and textual quality, with
the Unique Word Count increasing by approximately 77\% and Detail Density by
around 50\% compared to existing approaches.

</details>


### [349] [With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You](https://arxiv.org/abs/2506.16895)
*Fabian Gröger,Shuo Wen,Huyen Le,Maria Brbić*

Key words: 多模态学习、有限数据、对齐技术、STRUCTURE、零样本任务

TL;DR: 本文探讨了在有限配对数据下构建多模态模型的可行性，通过对齐预训练的单一模态基础模型，仅需数万个配对样本即可实现高质量对齐，并提出了STRUCTURE正则化技术。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有多模态模型依赖大量配对样本，成本高昂且不现实，本研究旨在解决这一限制。

Method: 采用STRUCTURE正则化技术保护单模态编码器的潜在空间几何结构，并优化对齐层（避免仅对齐最后一层）。

Result: 在24个零样本图像分类和检索任务中，平均相对改进分别为51.6%和91.8%。

Conclusion: 该方法在有限样本下高效，为资源受限领域提供了可行解决方案。

Abstract: Multimodal models have demonstrated powerful capabilities in complex tasks
requiring multimodal alignment including zero-shot classification and
cross-modal retrieval. However, existing models typically rely on millions of
paired multimodal samples, which are prohibitively expensive or infeasible to
obtain in many domains. In this work, we explore the feasibility of building
multimodal models with limited amount of paired data by aligning pretrained
unimodal foundation models. We show that high-quality alignment is possible
with as few as tens of thousands of paired samples$\unicode{x2013}$less than
$1\%$ of the data typically used in the field. To achieve this, we introduce
STRUCTURE, an effective regularization technique that preserves the
neighborhood geometry of the latent space of unimodal encoders. Additionally,
we show that aligning last layers is often suboptimal and demonstrate the
benefits of aligning the layers with the highest representational similarity
across modalities. These two components can be readily incorporated into
existing alignment methods, yielding substantial gains across 24 zero-shot
image classification and retrieval benchmarks, with average relative
improvement of $51.6\%$ in classification and $91.8\%$ in retrieval tasks. Our
results highlight the effectiveness and broad applicability of our framework
for limited-sample multimodal learning and offer a promising path forward for
resource-constrained domains.

</details>


### [350] [Pediatric Pancreas Segmentation from MRI Scans with Deep Learning](https://arxiv.org/abs/2506.15908)
*Elif Keles,Merve Yazol,Gorkem Durak,Ziliang Hong,Halil Ertugrul Aktas,Zheyuan Zhang,Linkai Peng,Onkar Susladkar,Necati Guzelyel,Oznur Leman Boyunaga,Cemal Yazici,Mark Lowe,Aliye Uc,Ulas Bagci*

Key words: PanSegNet, 深度学习, 胰腺分割, MRI, 儿童胰腺炎

TL;DR: PanSegNet是一种用于儿童胰腺MRI分割的深度学习算法，在健康儿童和急慢性胰腺炎患者中表现优异，与专家手动分割结果高度一致。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 评估和验证PanSegNet在儿童胰腺MRI分割中的应用，以促进无辐射的儿童胰腺影像学研究。

Method: 使用84例MRI扫描（健康儿童及急慢性胰腺炎患者），通过Dice相似系数和Hausdorff距离评估算法性能。

Result: PanSegNet在健康儿童中的DSC为88%，急慢性胰腺炎中分别为81%和80%，与手动分割结果高度一致（R²达0.85）。

Conclusion: PanSegNet是首个经过验证的胰腺MRI分割深度学习工具，性能达到专家水平，数据与算法已开源。

Abstract: Objective: Our study aimed to evaluate and validate PanSegNet, a deep
learning (DL) algorithm for pediatric pancreas segmentation on MRI in children
with acute pancreatitis (AP), chronic pancreatitis (CP), and healthy controls.
Methods: With IRB approval, we retrospectively collected 84 MRI scans (1.5T/3T
Siemens Aera/Verio) from children aged 2-19 years at Gazi University
(2015-2024). The dataset includes healthy children as well as patients
diagnosed with AP or CP based on clinical criteria. Pediatric and general
radiologists manually segmented the pancreas, then confirmed by a senior
pediatric radiologist. PanSegNet-generated segmentations were assessed using
Dice Similarity Coefficient (DSC) and 95th percentile Hausdorff distance
(HD95). Cohen's kappa measured observer agreement. Results: Pancreas MRI T2W
scans were obtained from 42 children with AP/CP (mean age: 11.73 +/- 3.9 years)
and 42 healthy children (mean age: 11.19 +/- 4.88 years). PanSegNet achieved
DSC scores of 88% (controls), 81% (AP), and 80% (CP), with HD95 values of 3.98
mm (controls), 9.85 mm (AP), and 15.67 mm (CP). Inter-observer kappa was 0.86
(controls), 0.82 (pancreatitis), and intra-observer agreement reached 0.88 and
0.81. Strong agreement was observed between automated and manual volumes (R^2 =
0.85 in controls, 0.77 in diseased), demonstrating clinical reliability.
Conclusion: PanSegNet represents the first validated deep learning solution for
pancreatic MRI segmentation, achieving expert-level performance across healthy
and diseased states. This tool, algorithm, along with our annotated dataset,
are freely available on GitHub and OSF, advancing accessible, radiation-free
pediatric pancreatic imaging and fostering collaborative research in this
underserved domain.

</details>


### [351] [VideoGAN-based Trajectory Proposal for Automated Vehicles](https://arxiv.org/abs/2506.16209)
*Annajoyce Mariani,Kira Maag,Hanno Gottschalk*

Key words: 生成对抗网络, 鸟瞰视图, 轨迹生成, 自动驾驶

TL;DR: 提出一种基于生成对抗网络（GAN）的管道，利用鸟瞰视频生成统计上准确的交通场景轨迹，快速训练和推理。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决传统方法难以捕捉未来轨迹复杂多模态分布的问题。

Method: 使用鸟瞰视频训练GAN，提取抽象轨迹数据。

Result: 训练时间100 GPU小时，推理时间低于20毫秒，轨迹统计上与真实数据对齐。

Conclusion: GAN能有效生成物理现实的交通场景轨迹。

Abstract: Being able to generate realistic trajectory options is at the core of
increasing the degree of automation of road vehicles. While model-driven,
rule-based, and classical learning-based methods are widely used to tackle
these tasks at present, they can struggle to effectively capture the complex,
multimodal distributions of future trajectories. In this paper we investigate
whether a generative adversarial network (GAN) trained on videos of bird's-eye
view (BEV) traffic scenarios can generate statistically accurate trajectories
that correctly capture spatial relationships between the agents. To this end,
we propose a pipeline that uses low-resolution BEV occupancy grid videos as
training data for a video generative model. From the generated videos of
traffic scenarios we extract abstract trajectory data using single-frame object
detection and frame-to-frame object matching. We particularly choose a GAN
architecture for the fast training and inference times with respect to
diffusion models. We obtain our best results within 100 GPU hours of training,
with inference times under 20\,ms. We demonstrate the physical realism of the
proposed trajectories in terms of distribution alignment of spatial and dynamic
parameters with respect to the ground truth videos from the Waymo Open Motion
Dataset.

</details>


### [352] [Do We Need Large VLMs for Spotting Soccer Actions?](https://arxiv.org/abs/2506.17144)
*Ritabrata Chakraborty,Rajatsubhra Chakraborty,Avijit Dasgupta,Sandeep Chaurasia*

Key words: 文本分析, 大型语言模型, 动作识别, 足球比赛, 轻量级

TL;DR: 论文提出了一种基于文本的轻量级方法，利用大型语言模型（LLMs）替代传统的视频分析方法，通过专家评论识别足球比赛中的关键动作。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统视频分析方法计算量大且复杂，而专家评论提供了丰富的细节和上下文信息，足以可靠地识别比赛中的关键动作。

Method: 使用SoccerNet Echoes数据集中的时间戳评论，通过三个专门化的LLMs（分别专注结果、兴奋度和战术）来评估评论窗口并识别关键动作（如进球、黄牌等）。

Result: 实验表明，这种基于语言的方法能有效检测关键比赛事件，提供了一种无需训练的轻量级替代方案。

Conclusion: 文本中心方法为动作识别提供了高效、轻量的替代方案。

Abstract: Traditional video-based tasks like soccer action spotting rely heavily on
visual inputs, often requiring complex and computationally expensive models to
process dense video data. In this work, we propose a shift from this
video-centric approach to a text-based task, making it lightweight and scalable
by utilizing Large Language Models (LLMs) instead of Vision-Language Models
(VLMs). We posit that expert commentary, which provides rich, fine-grained
descriptions and contextual cues such as excitement and tactical insights,
contains enough information to reliably spot key actions in a match. To
demonstrate this, we use the SoccerNet Echoes dataset, which provides
timestamped commentary, and employ a system of three LLMs acting as judges
specializing in outcome, excitement, and tactics. Each LLM evaluates sliding
windows of commentary to identify actions like goals, cards, and substitutions,
generating accurate timestamps for these events. Our experiments show that this
language-centric approach performs effectively in detecting critical match
events, providing a lightweight and training-free alternative to traditional
video-based methods for action spotting.

</details>


### [353] [Facial Landmark Visualization and Emotion Recognition Through Neural Networks](https://arxiv.org/abs/2506.17191)
*Israel Juárez-Jiménez,Tiffany Guadalupe Martínez Paredes,Jesús García-Ramírez,Eric Ramos Aguilar*

Key words: 情感识别, 深度学习, 面部标志, 数据集分析, 可视化技术

TL;DR: 论文提出了一种新的面部数据集可视化技术——面部标志箱线图，用于识别异常数据，并比较了两种面部标志特征。结果表明神经网络优于随机森林分类器。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 面部表情的深度学习模型缺乏全面的数据集分析，尤其是在可视化面部标志时存在挑战。

Method: 提出了面部标志箱线图来识别异常数据，并比较了绝对位置和位移两类面部标志特征。

Result: 神经网络的表现优于随机森林分类器。

Conclusion: 论文提出的箱线图技术有助于数据集分析，且神经网络是更优的情绪识别方法。

Abstract: Emotion recognition from facial images is a crucial task in human-computer
interaction, enabling machines to learn human emotions through facial
expressions. Previous studies have shown that facial images can be used to
train deep learning models; however, most of these studies do not include a
through dataset analysis. Visualizing facial landmarks can be challenging when
extracting meaningful dataset insights; to address this issue, we propose
facial landmark box plots, a visualization technique designed to identify
outliers in facial datasets. Additionally, we compare two sets of facial
landmark features: (i) the landmarks' absolute positions and (ii) their
displacements from a neutral expression to the peak of an emotional expression.
Our results indicate that a neural network achieves better performance than a
random forest classifier.

</details>


### [354] [Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting](https://arxiv.org/abs/2506.17212)
*Tianjiao Yu,Vedant Shah,Muntasir Wahed,Ying Shen,Kiet A. Nguyen,Ismini Lourentzou*

Key words: 3D重建,铰接物体,数字孪生,物理约束

TL;DR: Part$^{2}$GS是一种新颖框架，用于建模多部件物体的铰接数字孪生，具有高保真几何和物理一致的铰接特性，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现实世界中铰接物体普遍存在，但建模其结构和运动仍是3D重建中的挑战。

Method: 采用部件感知的3D高斯表示和基于物理约束的运动感知规范表示，引入排斥点场防止部件碰撞。

Result: 在合成和真实数据集上，Part$^{2}$GS在可移动部件的Chamfer距离上比现有方法优10倍。

Conclusion: Part$^{2}$GS通过结合部件感知和物理约束，实现了高保真且物理一致的铰接建模。

Abstract: Articulated objects are common in the real world, yet modeling their
structure and motion remains a challenging task for 3D reconstruction methods.
In this work, we introduce Part$^{2}$GS, a novel framework for modeling
articulated digital twins of multi-part objects with high-fidelity geometry and
physically consistent articulation. Part$^{2}$GS leverages a part-aware 3D
Gaussian representation that encodes articulated components with learnable
attributes, enabling structured, disentangled transformations that preserve
high-fidelity geometry. To ensure physically consistent motion, we propose a
motion-aware canonical representation guided by physics-based constraints,
including contact enforcement, velocity consistency, and vector-field
alignment. Furthermore, we introduce a field of repel points to prevent part
collisions and maintain stable articulation paths, significantly improving
motion coherence over baselines. Extensive evaluations on both synthetic and
real-world datasets show that Part$^{2}$GS consistently outperforms
state-of-the-art methods by up to 10$\times$ in Chamfer Distance for movable
parts.

</details>


### [355] [Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation](https://arxiv.org/abs/2506.17213)
*Xiuyu Yang,Shuhan Tan,Philipp Krähenbühl*

Key words: 交通仿真, 长时模拟, 点对点仿真, 闭环运动, 场景生成

TL;DR: InfGen是一个统一的下一代预测模型，用于交通仿真，结合闭环运动仿真和场景生成，实现稳定的长时模拟。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有模型和基准主要关注场景中初始代理的闭环运动仿真，不适用于长时模拟。

Method: InfGen通过交替进行闭环运动仿真和场景生成，自动切换模式。

Result: 在短期（9s）交通仿真中表现最佳，长期（30s）仿真显著优于其他方法。

Conclusion: InfGen为自驾驶系统的长时点对点仿真提供了高效解决方案。

Abstract: An ideal traffic simulator replicates the realistic long-term point-to-point
trip that a self-driving system experiences during deployment. Prior models and
benchmarks focus on closed-loop motion simulation for initial agents in a
scene. This is problematic for long-term simulation. Agents enter and exit the
scene as the ego vehicle enters new regions. We propose InfGen, a unified
next-token prediction model that performs interleaved closed-loop motion
simulation and scene generation. InfGen automatically switches between
closed-loop motion simulation and scene generation mode. It enables stable
long-term rollout simulation. InfGen performs at the state-of-the-art in
short-term (9s) traffic simulation, and significantly outperforms all other
methods in long-term (30s) simulation. The code and model of InfGen will be
released at https://orangesodahub.github.io/InfGen

</details>


### [356] [Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens](https://arxiv.org/abs/2506.17218)
*Zeyuan Yang,Xueyang Yu,Delin Chen,Maohao Shen,Chuang Gan*

Key words: 视觉语言模型, 多模态推理, 心理意象, Mirage框架, 隐式视觉标记

TL;DR: 论文提出了一种名为Mirage的框架，通过隐式视觉标记增强视觉语言模型的多模态推理能力，避免了显式图像生成的限制。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有视觉语言模型（VLMs）需要将视觉推理转化为文字描述，限制了其在需要视觉想象的任务中的表现。作者受人类通过心理意象推理的启发，探索无需生成显式图像的多模态推理方法。

Method: 提出Mirage框架，通过隐式视觉标记和文本标记交替解码，模拟心理意象。采用监督学习（从图像嵌入中蒸馏）和强化学习阶段优化模型，使其无需生成像素级图像即可完成多模态推理。

Result: 在多个基准测试中，Mirage展现出更强的多模态推理能力，且无需显式图像生成。

Conclusion: Mirage通过隐式视觉标记成功提升了VLMs的多模态推理能力，为视觉想象任务提供了新思路。

Abstract: Vision-language models (VLMs) excel at multimodal understanding, yet their
text-only decoding forces them to verbalize visual reasoning, limiting
performance on tasks that demand visual imagination. Recent attempts train VLMs
to render explicit images, but the heavy image-generation pre-training often
hinders the reasoning ability. Inspired by the way humans reason with mental
imagery-the internal construction and manipulation of visual cues-we
investigate whether VLMs can reason through interleaved multimodal trajectories
without producing explicit images. To this end, we present a Machine Mental
Imagery framework, dubbed as Mirage, which augments VLM decoding with latent
visual tokens alongside ordinary text. Concretely, whenever the model chooses
to ``think visually'', it recasts its hidden states as next tokens, thereby
continuing a multimodal trajectory without generating pixel-level images. Begin
by supervising the latent tokens through distillation from ground-truth image
embeddings, we then switch to text-only supervision to make the latent
trajectory align tightly with the task objective. A subsequent reinforcement
learning stage further enhances the multimodal reasoning capability.
Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger
multimodal reasoning without explicit image generation.

</details>


### [357] [LAION-C: An Out-of-Distribution Benchmark for Web-Scale Vision Models](https://arxiv.org/abs/2506.16950)
*Fanfei Li,Thomas Klein,Wieland Brendel,Robert Geirhos,Roland S. Zimmermann*

Key words: OOD鲁棒性, 基准数据集, LAION-C, 模型泛化, 人类比较

TL;DR: 论文提出了一个新的基准数据集LAION-C，用于评估在当代大规模网络数据集时代的OOD鲁棒性，解决了传统基准（如ImageNet-C）因数据集变化而失效的问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 由于传统OOD基准（如ImageNet-C）在当前大规模网络数据集时代不再有效，无法准确评估模型的OOD鲁棒性，因此需要新的基准来量化进展。

Method: 提出LAION-C作为ImageNet-C的替代，包含六种新型失真类型，专为当前大规模数据集（如LAION）设计，并进行了模型和人类观察者的对比实验。

Result: LAION-C对当代模型（如Gemini和GPT-4o）具有显著挑战性，且发现最佳模型的OOD泛化能力已与人类观察者相当或超越。

Conclusion: LAION-C为OOD鲁棒性评估提供了更准确的基准，标志着模型OOD泛化能力的新范式转变。

Abstract: Out-of-distribution (OOD) robustness is a desired property of computer vision
models. Improving model robustness requires high-quality signals from
robustness benchmarks to quantify progress. While various benchmark datasets
such as ImageNet-C were proposed in the ImageNet era, most ImageNet-C
corruption types are no longer OOD relative to today's large, web-scraped
datasets, which already contain common corruptions such as blur or JPEG
compression artifacts. Consequently, these benchmarks are no longer well-suited
for evaluating OOD robustness in the era of web-scale datasets. Indeed, recent
models show saturating scores on ImageNet-era OOD benchmarks, indicating that
it is unclear whether models trained on web-scale datasets truly become better
at OOD generalization or whether they have simply been exposed to the test
distortions during training. To address this, we introduce LAION-C as a
benchmark alternative for ImageNet-C. LAION-C consists of six novel distortion
types specifically designed to be OOD, even for web-scale datasets such as
LAION. In a comprehensive evaluation of state-of-the-art models, we find that
the LAION-C dataset poses significant challenges to contemporary models,
including MLLMs such as Gemini and GPT-4o. We additionally conducted a
psychophysical experiment to evaluate the difficulty of our corruptions for
human observers, enabling a comparison of models to lab-quality human
robustness data. We observe a paradigm shift in OOD generalization: from humans
outperforming models, to the best models now matching or outperforming the best
human observers.

</details>


### [358] [Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for Resource-Constrained Environments](https://arxiv.org/abs/2506.16994)
*Yasir Ali Farrukh,Syed Wali,Irfan Khan,Nathaniel D. Bastian*

Key words: 无监督领域自适应,CLIP,提示驱动,教师-学生范式,轻量级

TL;DR: Prmpt2Adpt是一个轻量级、高效的零样本领域自适应框架，通过基于提示的特征对齐和教师-学生范式，在资源受限环境中实现快速领域自适应。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有提示驱动的UDA方法依赖于大型视觉语言模型，并需要完全访问源域数据，限制了其在资源受限环境（如无人机）中的适用性。

Method: 使用蒸馏和微调的CLIP模型作为Faster R-CNN教师的冻结主干，通过提示驱动实例归一化（PIN）对齐低层特征，再通过教师模型生成伪标签指导学生模型适配。

Result: 在MDS-A数据集上，Prmpt2Adpt实现了与最先进方法相当的检测性能，同时实现了7倍更快的自适应速度和5倍更快的推理速度。

Conclusion: Prmpt2Adpt是一种实用且可扩展的解决方案，适用于低资源领域的实时自适应任务。

Abstract: Unsupervised Domain Adaptation (UDA) is a critical challenge in real-world
vision systems, especially in resource-constrained environments like drones,
where memory and computation are limited. Existing prompt-driven UDA methods
typically rely on large vision-language models and require full access to
source-domain data during adaptation, limiting their applicability. In this
work, we propose Prmpt2Adpt, a lightweight and efficient zero-shot domain
adaptation framework built around a teacher-student paradigm guided by
prompt-based feature alignment. At the core of our method is a distilled and
fine-tuned CLIP model, used as the frozen backbone of a Faster R-CNN teacher. A
small set of low-level source features is aligned to the target domain
semantics-specified only through a natural language prompt-via Prompt-driven
Instance Normalization (PIN). These semantically steered features are used to
briefly fine-tune the detection head of the teacher model. The adapted teacher
then generates high-quality pseudo-labels, which guide the on-the-fly
adaptation of a compact student model. Experiments on the MDS-A dataset
demonstrate that Prmpt2Adpt achieves competitive detection performance compared
to state-of-the-art methods, while delivering up to 7x faster adaptation and 5x
faster inference speed using few source images-making it a practical and
scalable solution for real-time adaptation in low-resource domains.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [359] [From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology](https://arxiv.org/abs/2506.16697)
*Zhicheng Lin*

Key words: 大型语言模型, 心理学研究, 测量工具, 因果推断, 双有效性框架

TL;DR: 论文讨论了大型语言模型（LLMs）在心理学中的应用，指出当前研究方法可能导致虚假结果，提出需要结合可靠测量与因果推断原则，并提出了双有效性框架来指导验证。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 心理学研究中广泛使用LLMs作为工具或模型，但现有的人类测量工具可能导致矛盾结果，需要更严谨的科学验证方法。

Method: 提出了双有效性框架，根据不同研究目标（如测量、模拟等）调整验证策略，强调证据标准需与研究目标相匹配。

Result: 当前研究方法未能满足验证要求，常将统计模式匹配误认为心理现象。需要开发计算模拟的心理构念并明确证据标准。

Conclusion: 未来研究应避免盲目应用人类测量工具，需建立可扩展的证据标准，以确保心理学研究的严谨性。

Abstract: Large language models (LLMs) are rapidly being adopted across psychology,
serving as research tools, experimental subjects, human simulators, and
computational models of cognition. However, the application of human
measurement tools to these systems can produce contradictory results, raising
concerns that many findings are measurement phantoms--statistical artifacts
rather than genuine psychological phenomena. In this Perspective, we argue that
building a robust science of AI psychology requires integrating two of our
field's foundational pillars: the principles of reliable measurement and the
standards for sound causal inference. We present a dual-validity framework to
guide this integration, which clarifies how the evidence needed to support a
claim scales with its scientific ambition. Using an LLM to classify text may
require only basic accuracy checks, whereas claiming it can simulate anxiety
demands a far more rigorous validation process. Current practice systematically
fails to meet these requirements, often treating statistical pattern matching
as evidence of psychological phenomena. The same model output--endorsing "I am
anxious"--requires different validation strategies depending on whether
researchers claim to measure, characterize, simulate, or model psychological
constructs. Moving forward requires developing computational analogues of
psychological constructs and establishing clear, scalable standards of evidence
rather than the uncritical application of human measurement tools.

</details>


### [360] [Large Language Models as Psychological Simulators: A Methodological Guide](https://arxiv.org/abs/2506.16702)
*Zhicheng Lin*

Key words: 大型语言模型, 心理学模拟, 认知建模, 伦理问题, 角色模拟

TL;DR: 该论文提供了一个框架，指导如何将大型语言模型（LLMs）用作心理学模拟器，涵盖角色模拟和认知建模两方面，并讨论了相关方法、挑战及伦理问题。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 由于缺乏方法论指导，论文旨在填补这一空白，帮助心理和行为研究者利用LLMs进行更高效的研究。

Method: 提出了两种主要应用方法：1）模拟角色和人格以探索多样情境；2）作为计算模型研究认知过程。并提供了验证策略和具体用例。

Result: 整合了关于LLMs性能的实证证据，包括系统偏见、文化局限性和提示敏感性，为研究者提供了实用工具。

Conclusion: 论文强调透明度和对模型局限性的认识，为心理学研究提供了新的可能性，但需谨慎应对伦理和技术挑战。

Abstract: Large language models (LLMs) offer emerging opportunities for psychological
and behavioral research, but methodological guidance is lacking. This article
provides a framework for using LLMs as psychological simulators across two
primary applications: simulating roles and personas to explore diverse
contexts, and serving as computational models to investigate cognitive
processes. For simulation, we present methods for developing psychologically
grounded personas that move beyond demographic categories, with strategies for
validation against human data and use cases ranging from studying inaccessible
populations to prototyping research instruments. For cognitive modeling, we
synthesize emerging approaches for probing internal representations,
methodological advances in causal interventions, and strategies for relating
model behavior to human cognition. We address overarching challenges including
prompt sensitivity, temporal limitations from training data cutoffs, and
ethical considerations that extend beyond traditional human subjects review.
Throughout, we emphasize the need for transparency about model capabilities and
constraints. Together, this framework integrates emerging empirical evidence
about LLM performance--including systematic biases, cultural limitations, and
prompt brittleness--to help researchers wrangle these challenges and leverage
the unique capabilities of LLMs in psychological research.

</details>


### [361] [LLM-Based Bot Broadens the Range of Arguments in Online Discussions, Even When Transparently Disclosed as AI](https://arxiv.org/abs/2506.17073)
*Valeria Vuk,Cristina Sarasua,Fabrizio Gilardi*

Key words: 在线讨论,政治讨论,LLM,机器人,民主

TL;DR: 该论文探讨了LLM机器人是否能拓宽在线政治讨论的观点范围，并通过实验证明其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 在线政治讨论中观点单一化问题严重，可能影响民主进程。

Method: 通过两个预注册随机实验，使用LLM机器人监控讨论并引入缺失观点。

Result: 机器人显著拓宽了讨论的观点范围，且AI身份披露对效果无显著影响。

Conclusion: LLM工具可能对改善在线政治讨论有积极作用。

Abstract: A wide range of participation is essential for democracy, as it helps prevent
the dominance of extreme views, erosion of legitimacy, and political
polarization. However, engagement in online political discussions often
features a limited spectrum of views due to high levels of self-selection and
the tendency of online platforms to facilitate exchanges primarily among
like-minded individuals. This study examines whether an LLM-based bot can widen
the scope of perspectives expressed by participants in online discussions
through two pre-registered randomized experiments conducted in a chatroom. We
evaluate the impact of a bot that actively monitors discussions, identifies
missing arguments, and introduces them into the conversation. The results
indicate that our bot significantly expands the range of arguments, as measured
by both objective and subjective metrics. Furthermore, disclosure of the bot as
AI does not significantly alter these effects. These findings suggest that
LLM-based moderation tools can positively influence online political discourse.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [362] [Empowering Near-Field Communications in Low-Altitude Economy with LLM: Fundamentals, Potentials, Solutions, and Future Directions](https://arxiv.org/abs/2506.17067)
*Zhuo Xu,Tianyue Zheng,Linglong Dai*

Key words: 低空经济（LAE）, 近场通信, 超大尺度MIMO（XL-MIMO）, 大语言模型（LLM）, 多用户预编码

TL;DR: 本文探讨了低空经济（LAE）与超大尺度MIMO（XL-MIMO）系统中的近场通信的结合，提出利用大语言模型（LLM）解决近场通信中的信号处理复杂性和用户区分问题，并介绍了基于LLM的解决方案和未来研究方向。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 低空经济的兴起与近场通信技术的匹配激发了研究兴趣，但近场通信在LAE中面临信号处理复杂性和用户区分的挑战，促使探索LLM的潜力。

Method: 文章通过介绍LLM和近场通信的基础知识，分析其优势和特性，并提出基于LLM的解决方案，包括联合区分远近场用户和设计多用户预编码矩阵的案例研究。

Result: 研究展示了LLM在解决近场通信挑战中的潜力，并提供了一个实际的解决方案案例。

Conclusion: LLM为解决LAE中的近场通信问题提供了新思路，但仍需进一步研究解决未来技术和开放性问题。

Abstract: The low-altitude economy (LAE) is gaining significant attention from academia
and industry. Fortunately, LAE naturally aligns with near-field communications
in extremely large-scale MIMO (XL-MIMO) systems. By leveraging near-field
beamfocusing, LAE can precisely direct beam energy to unmanned aerial vehicles,
while the additional distance dimension boosts overall spectrum efficiency.
However, near-field communications in LAE still face several challenges, such
as the increase in signal processing complexity and the necessity of
distinguishing between far and near-field users. Inspired by the large language
models (LLM) with powerful ability to handle complex problems, we apply LLM to
solve challenges of near-field communications in LAE. The objective of this
article is to provide a comprehensive analysis and discussion on LLM-empowered
near-field communications in LAE. Specifically, we first introduce fundamentals
of LLM and near-field communications, including the key advantages of LLM and
key characteristics of near-field communications. Then, we reveal the
opportunities and challenges of near-field communications in LAE. To address
these challenges, we present a LLM-based scheme for near-field communications
in LAE, and provide a case study which jointly distinguishes far and near-field
users and designs multi-user precoding matrix. Finally, we outline and
highlight several future research directions and open issues.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [363] [Unsupervised deep learning model for fast energy layer pre-selection of delivery-efficient proton arc therapy plan optimization of nasopharyngeal carcinoma](https://arxiv.org/abs/2506.15803)
*Bohan Yang,Gang Liu,Rirao Dao,Yujia Qian,Ke Shi,Anke Tang,Yong Luo,Jingnan Liu*

Key words: 质子弧治疗, 无监督深度学习, 能量层预选, 鼻咽癌, 放射治疗

TL;DR: 本研究提出了一种无监督深度学习框架SPArcdl，用于快速有效的质子弧治疗（PAT）能量层预选，显著提升了计划质量和交付效率。

<details>
  <summary>Details</summary>

Main category: physics.med-ph

Motivation: 质子弧治疗（PAT）在放射治疗中具有优势，但能量层序列的优化计算量大。本研究旨在通过深度学习减少能量层切换时间并保持高质量的治疗计划。

Method: 提出了一种新颖的数据表示方法——点计数表示，并结合UNet架构SPArcdl，优化目标覆盖、减少器官风险和能量切换时间的三目标函数。模型通过54例鼻咽癌病例进行测试，并与SPArc粒子群优化方法对比。

Result: SPArcdl显著提升了计划质量和交付效率，包括提升一致性指数、降低均匀性指数、缩短能量切换时间38.4%，且推理时间小于1秒。

Conclusion: SPArcdl是快速有效的工具，能通过策略性能量层预选生成高质量的PAT计划，同时减少交付时间。

Abstract: Objective. Proton arc therapy (PAT) is an emerging and promising modality in
radiotherapy, offering several advantages over conventional intensitymodulated
proton therapy (IMPT). However, identifying the optimal energy layer (EL)
sequence remains computationally intensive due to the large number of possible
energy layer transitions. This study proposes an unsupervised deep learning
framework for fast and effective EL pre-selection, aiming to minimize energy
layer switch time while preserving high plan quality. Approach. We introduce a
novel data representation method, spot-count representation, which encodes the
number of proton spots intersecting the target and organs at risk (OARs) in a
matrix structured by sorted gantry angles and energy layers. This
representation is the input of a UNet-based architecture, SPArcdl, which is
trained to optimize a tri-objective function: maximizing target coverage,
minimizing OAR exposure, and reducing energy switching time. The model is
evaluated on 54 nasopharyngeal cancer cases, and its performance is benchmarked
against plans generated by SPArcparticle swarm. Main results. SPArcdl produces
EL pre-selection that significantly improves both plan quality and delivery
efficiency. Compared to SPArc particle swarm, it enhances the conformity index
by 0.16 (p < 0.01), reduces the homogeneity index by 0.71 (p < 0.01), shortens
the energy switching time by 38.4% (p < 0.01), and lowers the mean dose to
brainstem by 0.21 (p < 0.01). The results unintentionally reveal employing
unchanged ELS is more time-wise efficient than descended ELS. SPArcdl's
inference time is within 1 second. Significance. SPArcdl is a fast and
effective tool for generating high-quality PAT plans by strategically
pre-selecting energy layers to reduce delivery time while maintaining excellent
dosimetric performance.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [364] [Quantum Fisher-Preconditioned Reinforcement Learning: From Single-Qubit Control to Rayleigh-Fading Link Adaptation](https://arxiv.org/abs/2506.15753)
*Oluwaseyi Giwa,Muhammad Ahmed Mohsin,Muhammad Ali Jamshed*

Key words: 量子强化学习,自然梯度,反量子Fisher信息,Tikhonov正则化,噪声鲁棒性

TL;DR: 提出了一种基于自然梯度的量子预处理策略梯度（QPPG）算法，用于在噪声环境下稳定学习，并在经典和量子环境中表现优异。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 为了在量子强化学习中实现稳定且高效的学习，尤其是在噪声环境下，需要一种能够结合经典和量子几何的方法。

Method: 提出QPPG算法，通过反量子Fisher信息和Tikhonov正则化对策略更新进行白化处理。

Result: 在经典和量子环境中，QPPG比REINFORCE收敛速度快4倍，且在不确定性下维持1 dB增益，具有高噪声鲁棒性。

Conclusion: QPPG展示了基于全QFI预处理的量子强化学习的可扩展性优势。

Abstract: In this letter, we propose Quantum-Preconditioned Policy Gradient (QPPG), a
natural gradient-based algorithm for link adaptation that whitens policy
updates using the full inverse quantum Fisher information with Tikhonov
regularization. QPPG bridges classical and quantum geometry, achieving stable
learning even under noise. Evaluated on classical and quantum environments,
including noisy single-qubit Gym tasks and Rayleigh-fading channels, QPPG
converges 4 times faster than REINFORCE and sustains a 1 dB gain under
uncertainty. It reaches a 90 percent return in one hundred episodes with high
noise robustness, showcasing the advantages of full QFI-based preconditioning
for scalable quantum reinforcement learning.

</details>


### [365] [Compilation, Optimization, Error Mitigation, and Machine Learning in Quantum Algorithms](https://arxiv.org/abs/2506.15760)
*Shuangbao Paul Wang,Jianzhou Mao,Eric Sakk*

Key words: 量子算法,编译优化,错误缓解,近似量子傅里叶变换,混合平台

TL;DR: 本文讨论量子算法的编译、优化和错误缓解，这是执行现实世界量子算法的关键步骤。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 为了更好地利用现有高性能计算能力与量子指数加速的优势，研究在混合平台（QPU和CPU/GPU）上运行的量子算法。

Method: 提出了一种近似量子傅里叶变换（AQFT），用于优化量子算法的电路执行。

Result: 该方法在量子傅里叶变换提供的指数加速基础上进一步提升了电路执行效率。

Conclusion: AQFT为量子算法优化提供了一种有效方法，增强了实际应用中的执行性能。

Abstract: This paper discusses the compilation, optimization, and error mitigation of
quantum algorithms, essential steps to execute real-world quantum algorithms.
Quantum algorithms running on a hybrid platform with QPU and CPU/GPU take
advantage of existing high-performance computing power with quantum-enabled
exponential speedups. The proposed approximate quantum Fourier transform (AQFT)
for quantum algorithm optimization improves the circuit execution on top of an
exponential speed-ups the quantum Fourier transform has provided.

</details>


### [366] [Superconducting Qubit Readout Using Next-Generation Reservoir Computing](https://arxiv.org/abs/2506.15771)
*Robert Kent,Benjamin Lienhard,Gregory Lafyatis,Daniel J. Gauthier*

Key words: 量子处理器, 储层计算, 量子比特测量, 串扰, 机器学习

TL;DR: 论文提出了一种基于下一代储层计算的机器学习方法，用于提升量子处理器中多量子比特测量的效率和保真度，同时减少计算复杂性和延迟。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 超导量子比特的测量是量子处理器中的瓶颈问题，传统方法难以处理频率复用测量中的串扰，而现有的神经网络方法计算成本高且扩展性差。

Method: 采用储层计算技术，通过构造测量信号的多项式特征并将其映射到对应的量子比特状态，避免了神经网络中高成本的非线性激活函数。

Result: 该方法在单量子比特和五量子比特数据集上分别减少了50%和11%的错误率，串扰降低了2.5倍，且计算复杂度显著低于现有方法。

Conclusion: 储层计算能够高效提升量子比特状态识别的保真度，同时保持未来量子处理器的可扩展性。

Abstract: Quantum processors require rapid and high-fidelity simultaneous measurements
of many qubits. While superconducting qubits are among the leading modalities
toward a useful quantum processor, their readout remains a bottleneck.
Traditional approaches to processing measurement data often struggle to account
for crosstalk present in frequency-multiplexed readout, the preferred method to
reduce the resource overhead. Recent approaches to address this challenge use
neural networks to improve the state-discrimination fidelity. However, they are
computationally expensive to train and evaluate, resulting in increased latency
and poor scalability as the number of qubits increases. We present an
alternative machine learning approach based on next-generation reservoir
computing that constructs polynomial features from the measurement signals and
maps them to the corresponding qubit states. This method is highly
parallelizable, avoids the costly nonlinear activation functions common in
neural networks, and supports real-time training, enabling fast evaluation,
adaptability, and scalability. Despite its lower computational complexity, our
reservoir approach is able to maintain high qubit-state-discrimination
fidelity. Relative to traditional methods, our approach achieves error
reductions of up to 50% and 11% on single- and five-qubit datasets,
respectively, and delivers up to 2.5x crosstalk reduction on the five-qubit
dataset. Compared with recent machine-learning methods, evaluating our model
requires 100x fewer multiplications for single-qubit and 2.5x fewer for
five-qubit models. This work demonstrates that reservoir computing can enhance
qubit-state discrimination while maintaining scalability for future quantum
processors.

</details>


### [367] [Feedback-driven recurrent quantum neural network universality](https://arxiv.org/abs/2506.16332)
*Lukas Gonon,Rodrigo Martínez-Peña,Juan-Pablo Ortega*

Key words: 量子水库计算,反馈协议,循环量子神经网络,普适性,实时处理

TL;DR: 本文提出了一种基于反馈的量子水库计算方法，解决了传统方法中实时处理和计算开销问题，并提供了理论保证。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 量子水库计算在处理时序数据时面临实时性差和计算开销大的问题，反馈协议的潜力尚未得到充分理论支持。

Method: 提出了一种动态、反馈驱动的循环量子神经网络架构，扩展了现有前馈模型。

Result: 证明了该模型具有线性读出时的普适性，提供了近似界限和普适性结果。

Conclusion: 该研究为具有实时处理能力的量子水库计算提供了理论和实践基础。

Abstract: Quantum reservoir computing uses the dynamics of quantum systems to process
temporal data, making it particularly well-suited for learning with noisy
intermediate-scale quantum devices. Early experimental proposals, such as the
restarting and rewinding protocols, relied on repeating previous steps of the
quantum map to avoid backaction. However, this approach compromises real-time
processing and increases computational overhead. Recent developments have
introduced alternative protocols that address these limitations. These include
online, mid-circuit measurement, and feedback techniques, which enable
real-time computation while preserving the input history. Among these, the
feedback protocol stands out for its ability to process temporal information
with comparatively fewer components. Despite this potential advantage, the
theoretical foundations of feedback-based quantum reservoir computing remain
underdeveloped, particularly with regard to the universality and the
approximation capabilities of this approach. This paper addresses this issue by
presenting a recurrent quantum neural network architecture that extends a class
of existing feedforward models to a dynamic, feedback-driven reservoir setting.
We provide theoretical guarantees for variational recurrent quantum neural
networks, including approximation bounds and universality results. Notably, our
analysis demonstrates that the model is universal with linear readouts, making
it both powerful and experimentally accessible. These results pave the way for
practical and theoretically grounded quantum reservoir computing with real-time
processing capabilities.

</details>


### [368] [Enhancing Expressivity of Quantum Neural Networks Based on the SWAP test](https://arxiv.org/abs/2506.16938)
*Sebastian Nagies,Emiliano Tolotti,Davide Pastorello,Enrico Blanzieri*

Key words: 量子神经网络, SWAP测试, 经典神经网络, 表达能力, 奇偶校验函数

TL;DR: 该论文研究了基于SWAP测试电路的量子神经网络（QNN），发现其在数学上等同于具有二次激活函数的经典两层前馈网络。虽然这种架构能处理许多任务，但在表达性上存在局限，无法解决如奇偶校验函数等复杂问题。通过引入广义SWAP测试电路，论文提出了改进方案，使其能够在任意维度上学习奇偶校验函数。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 探讨量子神经网络如何通过经典任务分析来提升表达能力，特别是针对现有的SWAP测试电路架构的局限性进行改进。

Method: 分析基于SWAP测试电路的量子神经网络，并将其与经典神经网络类比，发现其不足后引入广义SWAP测试电路以扩展表达能力。

Result: 改进后的架构能够在任意维度上学习奇偶校验函数，解决了原架构在表达性上的根本局限性。

Conclusion: 通过经典任务分析，可以有效提升量子神经网络的表达能力，改进后的SWAP测试电路架构展现出广泛的表示能力，具有潜在的量子学习应用价值。

Abstract: Parameterized quantum circuits represent promising architectures for machine
learning applications, yet many lack clear connections to classical models,
potentially limiting their ability to translate the wide success of classical
neural networks to the quantum realm. We examine a specific type of quantum
neural network (QNN) built exclusively from SWAP test circuits, and discuss its
mathematical equivalence to a classical two-layer feedforward network with
quadratic activation functions under amplitude encoding. Our analysis across
classical real-world and synthetic datasets reveals that while this
architecture can successfully learn many practical tasks, it exhibits
fundamental expressivity limitations due to violating the universal
approximation theorem, particularly failing on harder problems like the parity
check function. To address this limitation, we introduce a circuit modification
using generalized SWAP test circuits that effectively implements classical
neural networks with product layers. This enhancement enables successful
learning of parity check functions in arbitrary dimensions which we
analytically argue to be impossible for the original architecture beyond two
dimensions regardless of network size. Our results establish a framework for
enhancing QNN expressivity through classical task analysis and demonstrate that
our SWAP test-based architecture offers broad representational capacity,
suggesting potential promise also for quantum learning tasks.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [369] [A Neural Operator based Hybrid Microscale Model for Multiscale Simulation of Rate-Dependent Materials](https://arxiv.org/abs/2506.16918)
*Dhananjeyan Jeyaraj,Hamidreza Eivazi,Jendrik-Alexander Tröger,Stefan Wittek,Stefan Hartmann,Andreas Rausch*

Key words: 多尺度模拟，深度学习，神经网络算子，计算均质化，粘弹性材料

TL;DR: 使用神经网络算子预测微观物理现象，结合数据驱动和物理方法，加速多尺度模拟，误差小于6%，速度提升约100倍。

<details>
  <summary>Details</summary>

Main category: physics.comp-ph

Motivation: 多尺度模拟中微观尺度计算繁重，传统方法效率低，需结合深度学习提升效率。

Method: 采用神经网络算子结合物理原理预测微观行为，构建混合模型。

Result: 均质化应力误差小于6%，计算速度提升约100倍。

Conclusion: 混合模型在多尺度模拟中高效且灵活，适用于不同材料和空间离散化。

Abstract: The behavior of materials is influenced by a wide range of phenomena
occurring across various time and length scales. To better understand the
impact of microstructure on macroscopic response, multiscale modeling
strategies are essential. Numerical methods, such as the $\text{FE}^2$
approach, account for micro-macro interactions to predict the global response
in a concurrent manner. However, these methods are computationally intensive
due to the repeated evaluations of the microscale. This challenge has led to
the integration of deep learning techniques into computational homogenization
frameworks to accelerate multiscale simulations. In this work, we employ neural
operators to predict the microscale physics, resulting in a hybrid model that
combines data-driven and physics-based approaches. This allows for
physics-guided learning and provides flexibility for different materials and
spatial discretizations. We apply this method to time-dependent solid mechanics
problems involving viscoelastic material behavior, where the state is
represented by internal variables only at the microscale. The constitutive
relations of the microscale are incorporated into the model architecture and
the internal variables are computed based on established physical principles.
The results for homogenized stresses ($<6\%$ error) show that the approach is
computationally efficient ($\sim 100 \times$ faster).

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [370] [Can AI Dream of Unseen Galaxies? Conditional Diffusion Model for Galaxy Morphology Augmentation](https://arxiv.org/abs/2506.16233)
*Chenrui Ma,Zechang Sun,Tao Jing,Zheng Cai,Yuan-Sen Ting,Song Huang,Mingyu Li*

Key words: 天文观测,生成模型,条件扩散模型,数据增强,罕见天体检测

TL;DR: 论文提出了一种条件扩散模型，用于合成真实的星系图像以增强机器学习训练数据，显著提升了罕见天体检测的性能。

<details>
  <summary>Details</summary>

Main category: astro-ph.GA

Motivation: 解决天文观测中机器学习模型因标注数据有限而难以泛化的问题，尤其是针对罕见但有科学价值的天体。

Method: 利用Galaxy Zoo 2数据集，开发条件扩散模型生成符合特定形态特征的星系图像，并用于数据增强。

Result: 模型生成的图像多样性高且逼真，显著提升了分类和罕见天体检测的性能（检测数量从352增至872）。

Conclusion: 生成模型能有效弥补标注数据稀缺与天文观测需求之间的差距，为未来天体物理基础模型开发提供启示。

Abstract: Observational astronomy relies on visual feature identification to detect
critical astrophysical phenomena. While machine learning (ML) increasingly
automates this process, models often struggle with generalization in
large-scale surveys due to the limited representativeness of labeled datasets
-- whether from simulations or human annotation -- a challenge pronounced for
rare yet scientifically valuable objects. To address this, we propose a
conditional diffusion model to synthesize realistic galaxy images for
augmenting ML training data. Leveraging the Galaxy Zoo 2 dataset which contains
visual feature -- galaxy image pairs from volunteer annotation, we demonstrate
that our model generates diverse, high-fidelity galaxy images closely adhere to
the specified morphological feature conditions. Moreover, this model enables
generative extrapolation to project well-annotated data into unseen domains and
advancing rare object detection. Integrating synthesized images into ML
pipelines improves performance in standard morphology classification, boosting
completeness and purity by up to 30\% across key metrics. For rare object
detection, using early-type galaxies with prominent dust lane features (
$\sim$0.1\% in GZ2 dataset) as a test case, our approach doubled the number of
detected instances from 352 to 872, compared to previous studies based on
visual inspection. This study highlights the power of generative models to
bridge gaps between scarce labeled data and the vast, uncharted parameter space
of observational astronomy and sheds insight for future astrophysical
foundation model developments. Our project homepage is available at
https://galaxysd-webpage.streamlit.app/.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [371] [Code Rate Optimization via Neural Polar Decoders](https://arxiv.org/abs/2506.15836)
*Ziv Aharoni,Bashar Huleihel,Henry D Pfister,Haim H Permuter*

Key words: 神经极化解码器，互信息，极码，比特误码率，通信优化

TL;DR: 论文提出了一种通过神经极化解码器（NPDs）优化通信码率的方法，适用于未知信道模型的情况，通过训练和推断两阶段优化输入分布以提高互信息（MI）和解码性能。

<details>
  <summary>Details</summary>

Main category: cs.IT

Motivation: 研究旨在解决未知信道模型下通信码率的优化问题，传统方法无法直接应用，需要一种能够同时优化码率和输入分布的新方案。

Method: 采用神经极化解码器（NPDs）估计信道输入和输出的互信息（MI），并通过两阶段（训练和推断）优化输入分布参数。训练阶段交替执行MI估计和参数优化，推断阶段利用优化模型构建极码。

Result: 实验结果表明，在非均匀输入分布的信道中，该方法显著提高了互信息（MI）和比特误码率（BER），尤其适用于块长度为1024的情况。

Conclusion: 该方法在理论和实际应用中均表现优异，为通信系统提供了可扩展的解决方案，特别适用于非均匀输入分布的信道优化。

Abstract: This paper proposes a method to optimize communication code rates via the
application of neural polar decoders (NPDs). Employing this approach enables
simultaneous optimization of code rates over input distributions while
providing a practical coding scheme within the framework of polar codes. The
proposed approach is designed for scenarios where the channel model is unknown,
treating the channel as a black box that produces output samples from input
samples. We employ polar codes to achieve our objectives, using NPDs to
estimate mutual information (MI) between the channel inputs and outputs, and
optimize a parametric model of the input distribution. The methodology involves
a two-phase process: a training phase and an inference phase. In the training
phase, two steps are repeated interchangeably. First, the estimation step
estimates the MI of the channel inputs and outputs via NPDs. Second, the
improvement step optimizes the input distribution parameters to maximize the MI
estimate obtained by the NPDs. In the inference phase, the optimized model is
used to construct polar codes. This involves incorporating the Honda-Yamamoto
(HY) scheme to accommodate the optimized input distributions and list decoding
to enhance decoding performance. Experimental results on memoryless and
finite-state channels (FSCs) demonstrate the effectiveness of our approach,
particularly in cases where the channel's capacity-achieving input distribution
is non-uniform. For these cases, we show significant improvements in MI and bit
error rates (BERs) over those achieved by uniform and independent and
identically distributed (i.i.d.) input distributions, validating our method for
block lengths up to 1024. This scalable approach has potential applications in
real-world communication systems, bridging theoretical capacity estimation and
practical coding performance.

</details>


### [372] [Neural Polar Decoders for DNA Data Storage](https://arxiv.org/abs/2506.17076)
*Ziv Aharoni,Henry D. Pfister*

Key words: DNA数据存储，神经极性解码器，同步错误，低复杂度解码，通道容量

TL;DR: 提出一种基于神经极性解码器（NPDs）的低复杂度解码方法，用于解决DNA数据存储系统中的同步错误问题，显著降低了复杂度并提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.IT

Motivation: DNA数据存储系统中的同步错误（插入、删除等）是主要挑战，传统解码方法计算复杂较高，因此需要一种高效且低复杂度的解决方案。

Method: 使用神经极性解码器（NPDs），仅需样本访问通道，无需明确通道模型，复杂度为O(AN log N)，且可优化输入分布和代码设计。

Result: NPDs在删除通道和IDS通道上表现出接近最优的解码性能，复杂度显著低于传统方法，并在实际DNA存储场景中验证了其效果。

Conclusion: 神经极性解码器（NPDs）为DNA数据存储系统提供了一种高效、低复杂度的解决方案，性能优于现有方法。

Abstract: Synchronization errors, such as insertions and deletions, present a
fundamental challenge in DNA-based data storage systems, arising from both
synthesis and sequencing noise. These channels are often modeled as
insertion-deletion-substitution (IDS) channels, for which designing
maximum-likelihood decoders is computationally expensive. In this work, we
propose a data-driven approach based on neural polar decoders (NPDs) to design
low-complexity decoders for channels with synchronization errors. The proposed
architecture enables decoding over IDS channels with reduced complexity $O(AN
log N )$, where $A$ is a tunable parameter independent of the channel. NPDs
require only sample access to the channel and can be trained without an
explicit channel model. Additionally, NPDs provide mutual information (MI)
estimates that can be used to optimize input distributions and code design. We
demonstrate the effectiveness of NPDs on both synthetic deletion and IDS
channels. For deletion channels, we show that NPDs achieve near-optimal
decoding performance and accurate MI estimation, with significantly lower
complexity than trellis-based decoders. We also provide numerical estimates of
the channel capacity for the deletion channel. We extend our evaluation to
realistic DNA storage settings, including channels with multiple noisy reads
and real-world Nanopore sequencing data. Our results show that NPDs match or
surpass the performance of existing methods while using significantly fewer
parameters than the state-of-the-art. These findings highlight the promise of
NPDs for robust and efficient decoding in DNA data storage systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [373] [RapFlow-TTS: Rapid and High-Fidelity Text-to-Speech with Improved Consistency Flow Matching](https://arxiv.org/abs/2506.16741)
*Hyun Joon Park,Jeongmin Liu,Jin Sob Kim,Jeong Yeol Yang,Sung Won Han,Eunwoo Song*

Key words: RapFlow-TTS, flow matching, TTS, ODE, high-fidelity, few-step synthesis

TL;DR: RapFlow-TTS是一种快速且高保真的TTS声学模型，通过流匹配（FM）训练中的速度一致性约束来优化生成效果，减少了合成步骤，提升了推理速度。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 基于常微分方程（ODE）的TTS生成虽然能实现高质量语音，但通常需要大量生成步骤，导致质量和推理速度之间的权衡。RapFlow-TTS旨在解决这一问题。

Method: 模型通过强制FM拉直的ODE轨迹上的速度场一致性，减少了生成步骤。还采用了时间间隔调度和对抗学习技术，进一步提升少步合成的质量。

Result: 实验表明，RapFlow-TTS在合成步骤上比传统FM和基于分数的方法分别减少了5倍和10倍，同时实现了高保真语音合成。

Conclusion: RapFlow-TTS通过速度一致性约束和优化技术，显著提升了TTS生成的效率和质量，为少步高保真合成提供了有效解决方案。

Abstract: We introduce RapFlow-TTS, a rapid and high-fidelity TTS acoustic model that
leverages velocity consistency constraints in flow matching (FM) training.
Although ordinary differential equation (ODE)-based TTS generation achieves
natural-quality speech, it typically requires a large number of generation
steps, resulting in a trade-off between quality and inference speed. To address
this challenge, RapFlow-TTS enforces consistency in the velocity field along
the FM-straightened ODE trajectory, enabling consistent synthetic quality with
fewer generation steps. Additionally, we introduce techniques such as time
interval scheduling and adversarial learning to further enhance the quality of
the few-step synthesis. Experimental results show that RapFlow-TTS achieves
high-fidelity speech synthesis with a 5- and 10-fold reduction in synthesis
steps than the conventional FM- and score-based approaches, respectively.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [374] [Simulating Correlated Electrons with Symmetry-Enforced Normalizing Flows](https://arxiv.org/abs/2506.17015)
*Dominic Schuh,Janik Kreit,Evan Berkowitz,Lena Funcke,Thomas Luu,Kim A. Nicoli,Marcel Rodekamp*

Key words: 归一化流, 费米子哈伯德模型, 玻尔兹曼分布, 对称性感知, 速度提升

TL;DR: 该论文首次证明了归一化流可以准确学习费米子哈伯德模型的玻尔兹曼分布，解决了传统方法的局限性。

<details>
  <summary>Details</summary>

Main category: cond-mat.str-el

Motivation: 费米子哈伯德模型是描述石墨烯及相关材料电子结构的关键框架，而现有方法（如混合蒙特卡洛）在时间连续极限附近存在遍历性问题，导致估计偏差。

Method: 利用具有对称性感知的架构以及独立同分布采样，解决了传统方法的遍历性问题。

Result: 该方法在准确性上优于传统方法，并实现了显著的速度提升。

Conclusion: 归一化流为费米子哈伯德模型的学习提供了高效且准确的解决方案。

Abstract: We present the first proof of principle that normalizing flows can accurately
learn the Boltzmann distribution of the fermionic Hubbard model - a key
framework for describing the electronic structure of graphene and related
materials. State-of-the-art methods like Hybrid Monte Carlo often suffer from
ergodicity issues near the time-continuum limit, leading to biased estimates.
Leveraging symmetry-aware architectures as well as independent and identically
distributed sampling, our approach resolves these issues and achieves
significant speed-ups over traditional methods.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [375] [Linearithmic Clean-up for Vector-Symbolic Key-Value Memory with Kroneker Rotation Products](https://arxiv.org/abs/2506.15793)
*Ruipeng Liu,Qinru Qiu,Simon Khan,Garrett E. Katz*

Key words: 向量符号架构(VSA), 清理步骤, Kronecker积, 旋转矩阵, 时间复杂度

TL;DR: 该论文提出了一种基于Kronecker积旋转矩阵的新代码表示方法，显著提高了向量符号架构（VSA）中“清理”步骤的效率，将时间复杂度从平方降为线对数。

<details>
  <summary>Details</summary>

Main category: cs.DS

Motivation: 当前VSA中的“清理”步骤因需要对比噪声向量与原型向量代码书而计算复杂度高，形成性能瓶颈。

Method: 通过Kronecker积的旋转矩阵表示代码书，实现高效的清理操作。

Result: 清理步骤的时间复杂度为线对数（O(N log N)），空间复杂度为线性（O(N)），且代码书无需显式存储，单个向量可高效生成。

Conclusion: 新方法大幅提升了VSA的扩展性，实验验证其比基线技术高效多个数量级。

Abstract: A computational bottleneck in current Vector-Symbolic Architectures (VSAs) is
the ``clean-up'' step, which decodes the noisy vectors retrieved from the
architecture. Clean-up typically compares noisy vectors against a ``codebook''
of prototype vectors, incurring computational complexity that is quadratic or
similar. We present a new codebook representation that supports efficient
clean-up, based on Kroneker products of rotation-like matrices. The resulting
clean-up time complexity is linearithmic, i.e. $\mathcal{O}(N\,\text{log}\,N)$,
where $N$ is the vector dimension and also the number of vectors in the
codebook. Clean-up space complexity is $\mathcal{O}(N)$. Furthermore, the
codebook is not stored explicitly in computer memory: It can be represented in
$\mathcal{O}(\text{log}\,N)$ space, and individual vectors in the codebook can
be materialized in $\mathcal{O}(N)$ time and space. At the same time,
asymptotic memory capacity remains comparable to standard approaches. Computer
experiments confirm these results, demonstrating several orders of magnitude
more scalability than baseline VSA techniques.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [376] [A Study of Hybrid and Evolutionary Metaheuristics for Single Hidden Layer Feedforward Neural Network Architecture](https://arxiv.org/abs/2506.15737)
*Gautam Siddharth Kashyap,Md Tabrez Nafis,Samar Wazir*

Key words: Artificial Neural Networks, Stochastic Gradient Descent, Particle Swarm Optimization, Genetic Algorithms, Metaheuristic Optimizers

TL;DR: 论文研究了PSO和GAs作为SGD的替代方案，提出了一种混合PSO-SGD策略，显著降低了训练误差，验证了进化方法的有效性。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: SGD在训练ANN时存在计算成本高和易陷入局部最优的问题，因此探索PSO和GAs这类基于群体的元启发式优化器来克服这些限制。

Method: 开发了一种混合PSO-SGD策略，结合PSO的群体搜索和SGD的局部搜索优势，以提高训练效率。

Result: 混合PSO-SGD方法将中位训练MSE降低了90-95%，相比传统GA和PSO表现更优。RMHC也显著降低了MSE，而RS表现不佳。

Conclusion: 混合和进化方法在训练效率和准确性上优于传统优化方法，支持了Building Block Hypothesis的有效性。

Abstract: Training Artificial Neural Networks (ANNs) with Stochastic Gradient Descent
(SGD) frequently encounters difficulties, including substantial computing
expense and the risk of converging to local optima, attributable to its
dependence on partial weight gradients. Therefore, this work investigates
Particle Swarm Optimization (PSO) and Genetic Algorithms (GAs) - two
population-based Metaheuristic Optimizers (MHOs) - as alternatives to SGD to
mitigate these constraints. A hybrid PSO-SGD strategy is developed to improve
local search efficiency. The findings indicate that the hybrid PSO-SGD
technique decreases the median training MSE by 90 to 95 percent relative to
conventional GA and PSO across various network sizes (e.g., from around 0.02 to
approximately 0.001 in the Sphere function). RMHC attains substantial
enhancements, reducing MSE by roughly 85 to 90 percent compared to GA.
Simultaneously, RS consistently exhibits errors exceeding 0.3, signifying
subpar performance. These findings underscore that hybrid and evolutionary
procedures significantly improve training efficiency and accuracy compared to
conventional optimization methods and imply that the Building Block Hypothesis
(BBH) may still be valid, indicating that advantageous weight structures are
retained during evolutionary search.

</details>


### [377] [Robust Dynamic Material Handling via Adaptive Constrained Evolutionary Reinforcement Learning](https://arxiv.org/abs/2506.16795)
*Chengpeng Hu,Ziming Wang,Bo Yuan,Jialin Liu,Chengqi Zhang,Xin Yao*

Key words: 动态物料搬运，强化学习，稀疏奖励，约束满足，自适应学习

TL;DR: 论文提出了一种自适应约束进化强化学习（ACERL）方法，用于解决动态物料搬运（DMH）问题，通过多样化的探索和约束满足来优化策略。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 动态物料搬运问题需要实时分配任务并满足约束条件，但在稀疏奖励和动态事件下，传统方法难以适应。

Method: ACERL通过维护一组演员进行多样化探索，处理稀疏奖励和约束违反，并自适应选择训练实例。

Result: ACERL在多个实例上表现出色，能够满足约束条件并在未见过的噪声实例中保持鲁棒性。

Conclusion: ACERL在解决DMH问题时表现出高效性和鲁棒性，各组件协同作用显著。

Abstract: Dynamic material handling (DMH) involves the assignment of dynamically
arriving material transporting tasks to suitable vehicles in real time for
minimising makespan and tardiness. In real-world scenarios, historical task
records are usually available, which enables the training of a decision policy
on multiple instances consisting of historical records. Recently, reinforcement
learning has been applied to solve DMH. Due to the occurrence of dynamic events
such as new tasks, adaptability is highly required. Solving DMH is challenging
since constraints including task delay should be satisfied. A feedback is
received only when all tasks are served, which leads to sparse reward. Besides,
making the best use of limited computational resources and historical records
for training a robust policy is crucial. The time allocated to different
problem instances would highly impact the learning process. To tackle those
challenges, this paper proposes a novel adaptive constrained evolutionary
reinforcement learning (ACERL) approach, which maintains a population of actors
for diverse exploration. ACERL accesses each actor for tackling sparse rewards
and constraint violation to restrict the behaviour of the policy. Moreover,
ACERL adaptively selects the most beneficial training instances for improving
the policy. Extensive experiments on eight training and eight unseen test
instances demonstrate the outstanding performance of ACERL compared with
several state-of-the-art algorithms. Policies trained by ACERL can schedule the
vehicles while fully satisfying the constraints. Additional experiments on 40
unseen noised instances show the robust performance of ACERL. Cross-validation
further presents the overall effectiveness of ACREL. Besides, a rigorous
ablation study highlights the coordination and benefits of each ingredient of
ACERL.

</details>


### [378] [Continual Learning with Columnar Spiking Neural Networks](https://arxiv.org/abs/2506.17169)
*Denis Larionov,Nikolay Bazenkov,Mikhail Kiselev*

Key words: 持续学习, 灾难性遗忘, 脉冲神经网络, CoLaNET

TL;DR: 研究探讨了柱状组织的脉冲神经网络（SNNs）在持续学习和灾难性遗忘中的表现。通过使用CoLaNET，发现微柱在新任务中适应性最强时不与前序学习共享结构。同时研究了CoLaNET超参数在保留旧知识（稳定性）与获取新信息（可塑性）之间的权衡。最优配置在连续十个MNIST任务中表现良好，每个任务保持92%准确率，遗忘率低。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 探索如何在持续学习中避免灾难性遗忘，同时高效适应新任务。

Method: 使用柱状分层网络（CoLaNET），研究其超参数对稳定性和可塑性的影响。

Result: 最优配置在十个MNIST任务中保持高准确率（92%），遗忘率仅4%。

Conclusion: CoLaNET在持续学习中表现优异，能够平衡稳定性与可塑性，减少遗忘。

Abstract: This study investigates columnar-organized spiking neural networks (SNNs) for
continual learning and catastrophic forgetting. Using CoLaNET (Columnar Layered
Network), we show that microcolumns adapt most efficiently to new tasks when
they lack shared structure with prior learning. We demonstrate how CoLaNET
hyperparameters govern the trade-off between retaining old knowledge
(stability) and acquiring new information (plasticity). Our optimal
configuration learns ten sequential MNIST tasks effectively, maintaining 92%
accuracy on each. It shows low forgetting, with only 4% performance degradation
on the first task after training on nine subsequent tasks.

</details>


<div id='cs.SY'></div>

# cs.SY [[Back]](#toc)

### [379] [Formal Control for Uncertain Systems via Contract-Based Probabilistic Surrogates (Extended Version)](https://arxiv.org/abs/2506.16971)
*Oliver Schön,Sofie Haesaert,Sadegh Soudjani*

Key words: 概率模拟关系,替代模型,形式化方法,可扩展性,非线性系统

TL;DR: 提出了一种基于概率模拟关系和随机系统替代模型的方法，显著提高了形式方法的可扩展性和实用性，适用于高维复杂非线性系统。

<details>
  <summary>Details</summary>

Main category: cs.SY

Motivation: 现有系统表示方法在复杂性和可扩展性上存在挑战，影响了形式化方法的决策效果和应用范围。

Method: 利用概率模拟关系和替代模型，避免了直接计算误差界限，提供了一种抽象的、可扩展的技术。

Result: 该方法在高维复杂系统中表现良好，尤其在车辆交叉口案例中展示了其优势。

Conclusion: 通过牺牲一定的保守性，该方法有效提高了复杂系统的可扩展性和实用性。

Abstract: The requirement for identifying accurate system representations has not only
been a challenge to fulfill, but it has compromised the scalability of formal
methods, as the resulting models are often too complex for effective decision
making with formal correctness and performance guarantees. Focusing on
probabilistic simulation relations and surrogate models of stochastic systems,
we propose an approach that significantly enhances the scalability and
practical applicability of such simulation relations by eliminating the need to
compute error bounds directly. As a result, we provide an abstraction-based
technique that scales effectively to higher dimensions while addressing complex
nonlinear agent-environment interactions with infinite-horizon temporal logic
guarantees amidst uncertainty. Our approach trades scalability for conservatism
favorably, as demonstrated on a complex high-dimensional vehicle intersection
case study.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [380] [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
*Yilin Zhang,Xinran Zhao,Zora Zhiruo Wang,Chenyang Yang,Jiayi Wei,Tongshuang Wu*

Key words: Retrieval-Augmented Generation, 分块, 抽象语法树, 代码生成

TL;DR: 论文提出了一种基于抽象语法树的结构感知分块方法(\ourwork)，用于改进检索增强生成(RAG)中的分块过程，提升语义一致性和生成质量。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现有基于行的分块启发式方法会破坏语义结构，影响代码生成质量，因此需要一种结构感知的分块方法。

Method: 利用抽象语法树(AST)递归地将大型AST节点拆分为较小分块，并在尺寸限制内合并兄弟节点。

Result: 该方法在多种编程语言和任务中表现优异，如将Recall@5提高4.3分，Pass@1提高2.67分。

Conclusion: 结构感知分块对提升检索增强代码智能的扩展性至关重要。

Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale
code generation, grounding predictions in external code corpora to improve
actuality. However, a critical yet underexplored aspect of RAG pipelines is
chunking -- the process of dividing documents into retrievable units. Existing
line-based chunking heuristics often break semantic structures, splitting
functions or merging unrelated code, which can degrade generation quality. We
propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method
that recursively breaks large AST nodes into smaller chunks and merges sibling
nodes while respecting size limits. This approach generates self-contained,
semantically coherent units across programming languages and tasks, improving
performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3
points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.
Our work highlights the importance of structure-aware chunking for scaling
retrieval-enhanced code intelligence.

</details>


### [381] [Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems](https://arxiv.org/abs/2506.17208)
*Matias Martinez,Xavier Franch*

Key words: 自动程序修复, 大语言模型, SWE-Bench, 代理系统, 开源库

TL;DR: 本文对SWE-Bench Lite和Verified排行榜的所有提交进行了全面研究，分析了67种方法，揭示了专有LLM的主导地位和设计多样性。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 由于SWE-Bench排行榜的提交过程缺乏详细文档，许多解决方案的架构设计和来源不明确。本文旨在填补这一空白。

Method: 分析了68个SWE-Bench Lite和79个Verified的提交，考察了提交者类型、产品可用性、LLM使用和系统架构等维度。

Result: 发现专有LLM（如Claude 3.5/3.7）占主导地位，存在代理和非代理设计，贡献者涵盖个人开发者到大型科技公司。

Conclusion: 研究揭示了当前APR领域的趋势和多样性，强调了透明化解决方案架构的重要性。

Abstract: The rapid progress in Automated Program Repair (APR) has been driven by
advances in AI, particularly large language models (LLMs) and agent-based
systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair
systems using real issues and pull requests mined from 12 popular open-source
Python repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench
Verified, have become central platforms for tracking progress and comparing
solutions. However, because the submission process does not require detailed
documentation, the architectural design and origin of many solutions remain
unclear. In this paper, we present the first comprehensive study of all
submissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)
leaderboards, analyzing 67 unique approaches across dimensions such as
submitter type, product availability, LLM usage, and system architecture. Our
findings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),
the presence of both agentic and non-agentic designs, and a contributor base
spanning from individual developers to large tech companies.

</details>


### [382] [AI-Driven Tools in Modern Software Quality Assurance: An Assessment of Benefits, Challenges, and Future Directions](https://arxiv.org/abs/2506.16586)
*Ihor Pysmennyi,Roman Kyslyi,Kyrylo Kleshch*

Key words: 质量保证, AI工具, 分布式软件, 测试用例, 验证

TL;DR: 研究探讨了在现代分布式软件应用程序中整合AI工具的潜力，分析了其对质量保证流程的影响，并指出实际应用中的挑战。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 传统质量保证方法在现代软件系统的复杂性、规模和快速迭代中面临挑战，资源限制导致成本高昂。研究旨在评估AI工具整合的益处、挑战和前景。

Method: 通过综合分析验证和确认流程，包括探索性测试分析、等价划分、边界分析、蜕变测试等，并使用AI代理进行端到端回归测试作为概念验证。

Result: 生成测试用例的执行不稳定率仅为8.3%，显示出潜力，但也发现语义覆盖生成和黑盒模型解释性等挑战。

Conclusion: AI在质量保证中具有变革潜力，但需战略性地实施，并开发适当的验证方法。

Abstract: Traditional quality assurance (QA) methods face significant challenges in
addressing the complexity, scale, and rapid iteration cycles of modern software
systems and are strained by limited resources available, leading to substantial
costs associated with poor quality. The object of this research is the Quality
Assurance processes for modern distributed software applications. The subject
of the research is the assessment of the benefits, challenges, and prospects of
integrating modern AI-oriented tools into quality assurance processes. We
performed comprehensive analysis of implications on both verification and
validation processes covering exploratory test analyses, equivalence
partitioning and boundary analyses, metamorphic testing, finding
inconsistencies in acceptance criteria (AC), static analyses, test case
generation, unit test generation, test suit optimization and assessment, end to
end scenario execution. End to end regression of sample enterprise application
utilizing AI-agents over generated test scenarios was implemented as a proof of
concept highlighting practical use of the study. The results, with only 8.3%
flaky executions of generated test cases, indicate significant potential for
the proposed approaches. However, the study also identified substantial
challenges for practical adoption concerning generation of semantically
identical coverage, "black box" nature and lack of explainability from
state-of-the-art Large Language Models (LLMs), the tendency to correct mutated
test cases to match expected results, underscoring the necessity for thorough
verification of both generated artifacts and test execution results. The
research demonstrates AI's transformative potential for QA but highlights the
importance of a strategic approach to implementing these technologies,
considering the identified limitations and the need for developing appropriate
verification methodologies.

</details>


### [383] [SemAgent: A Semantics Aware Program Repair Agent](https://arxiv.org/abs/2506.16650)
*Anvith Pabba,Alex Mathai,Anindya Chakraborty,Baishakhi Ray*

Key words: 大语言模型,软件工程,自动程序修复,语义理解,SWE-Bench

TL;DR: SemAgent利用问题、代码和执行语义生成更完整和通用的补丁，通过两阶段架构（修复和评审）显著提升了软件修复任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现有系统在解决仓库级问题时倾向于局部化修复，缺乏对问题、代码和执行语义的深入理解，导致补丁过拟合。

Method: 提出SemAgent，通过执行语义检索上下文、抽象问题语义、隔离代码语义，并采用两阶段架构（修复和评审）生成补丁。

Result: 在SWEBench-Lite基准测试中，解决率达到44.66%，超过其他基于工作流的方法，性能提升7.66%。

Conclusion: 结合问题、代码和执行的语义可以生成更稳健和语义一致的修复，尤其擅长多行推理和边缘情况处理。

Abstract: Large Language Models (LLMs) have shown impressive capabilities in downstream
software engineering tasks such as Automated Program Repair (APR). In
particular, there has been a lot of research on repository-level
issue-resolution benchmarks such as SWE-Bench. Although there has been
significant progress on this topic, we notice that in the process of solving
such issues, existing agentic systems tend to hyper-localize on immediately
suspicious lines of code and fix them in isolation, without a deeper
understanding of the issue semantics, code semantics, or execution semantics.
Consequently, many existing systems generate patches that overfit to the user
issue, even when a more general fix is preferable. To address this limitation,
we introduce SemAgent, a novel workflow-based procedure that leverages issue,
code, and execution semantics to generate patches that are complete -
identifying and fixing all lines relevant to the issue. We achieve this through
a novel pipeline that (a) leverages execution semantics to retrieve relevant
context, (b) comprehends issue-semantics via generalized abstraction, (c)
isolates code-semantics within the context of this abstraction, and (d)
leverages this understanding in a two-stage architecture: a repair stage that
proposes fine-grained fixes, followed by a reviewer stage that filters relevant
fixes based on the inferred issue-semantics. Our evaluations show that our
methodology achieves a solve rate of 44.66% on the SWEBench-Lite benchmark
beating all other workflow-based approaches, and an absolute improvement of
7.66% compared to our baseline, which lacks such deep semantic understanding.
We note that our approach performs particularly well on issues requiring
multi-line reasoning (and editing) and edge-case handling, suggesting that
incorporating issue and code semantics into APR pipelines can lead to robust
and semantically consistent repairs.

</details>


### [384] [LLMs in Coding and their Impact on the Commercial Software Engineering Landscape](https://arxiv.org/abs/2506.16653)
*Vladislav Belozerov,Peter J Barclay,Askhan Sami*

Key words: 大语言模型、编码工具、隐私泄露、安全漏洞、奉承现象、审查管理

TL;DR: 论文探讨了大语言模型编码工具的潜在风险，如隐私泄露、安全漏洞和模型迎合错误观点（奉承现象），并提出企业需加强审查和管理以确保安全性和准确性。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 大语言模型编码工具在软件工程中的广泛应用带来了新的安全隐患，如隐私泄露、安全漏洞和模型奉承现象，这促使研究者提出解决方案以确保工具的安全使用。

Method: 建议企业对AI生成的代码进行标记和审查，将提示和输出限制在私有或本地部署中，遵守安全法规，并增加检测奉承性答案的测试。

Result: 研究发现10%的提示会泄露隐私数据，42%生成的代码片段存在安全漏洞，模型还表现出对错误观点的迎合行为。

Conclusion: 企业需要采取严格的审查和管理措施，以在提高开发速度的同时确保安全性和准确性。

Abstract: Large-language-model coding tools are now mainstream in software engineering.
But as these same tools move human effort up the development stack, they
present fresh dangers: 10% of real prompts leak private data, 42% of generated
snippets hide security flaws, and the models can even ``agree'' with wrong
ideas, a trait called sycophancy. We argue that firms must tag and review every
AI-generated line of code, keep prompts and outputs inside private or
on-premises deployments, obey emerging safety regulations, and add tests that
catch sycophantic answers -- so they can gain speed without losing security and
accuracy.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [385] [TRUST: Transparent, Robust and Ultra-Sparse Trees](https://arxiv.org/abs/2506.15791)
*Albert Dorador*

Key words: 可解释性, 回归树, 随机森林, 稀疏线性模型, 大语言模型

TL;DR: 论文介绍了TRUST（透明、鲁棒且超稀疏的树模型），它将随机森林的准确性与浅层决策树和稀疏线性模型的可解释性结合，并通过大语言模型生成用户友好的解释。实验表明，TRUST在准确性上优于其他可解释模型，且与随机森林相当，同时在可解释性上有显著提升。

<details>
  <summary>Details</summary>

Main category: stat.ME

Motivation: 尽管分段常数回归树易于解释，但其预测准确性通常不如黑盒模型（如随机森林）。作者提出TRUST模型，旨在结合随机森林的准确性和决策树的可解释性，同时提供透明的解释。

Method: TRUST模型结合了随机森林的准确性和浅层决策树的可解释性，利用大语言模型生成用户友好的解释。

Result: 在合成和真实数据集上的实验表明，TRUST在预测准确性上优于其他可解释模型（如CART、Lasso等），且与随机森林相当，同时在可解释性上有显著提升。

Conclusion: TRUST模型在保持高预测准确性的同时，显著提升了可解释性，为高准确性与透明性结合的模型设计提供了新思路。

Abstract: Piecewise-constant regression trees remain popular for their
interpretability, yet often lag behind black-box models like Random Forest in
predictive accuracy. In this work, we introduce TRUST (Transparent, Robust, and
Ultra-Sparse Trees), a novel regression tree model that combines the accuracy
of Random Forests with the interpretability of shallow decision trees and
sparse linear models. TRUST further enhances transparency by leveraging Large
Language Models to generate tailored, user-friendly explanations. Extensive
validation on synthetic and real-world benchmark datasets demonstrates that
TRUST consistently outperforms other interpretable models -- including CART,
Lasso, and Node Harvest -- in predictive accuracy, while matching the accuracy
of Random Forest and offering substantial gains in both accuracy and
interpretability over M5', a well-established model that is conceptually
related.

</details>


### [386] [Bayesian Joint Model of Multi-Sensor and Failure Event Data for Multi-Mode Failure Prediction](https://arxiv.org/abs/2506.17036)
*Sina Aghaee Dabaghan Fard,Minhee Kim,Akash Deep,Jaesung Lee*

Key words: 多故障模式、剩余使用寿命预测、分层贝叶斯、不确定性量化

TL;DR: 本文提出了一种统一的方法，联合建模多传感器时间序列数据和多故障模式下的故障时间，通过分层贝叶斯框架实现准确预测和不确定性量化。

<details>
  <summary>Details</summary>

Main category: stat.ME

Motivation: 现代工业系统中，多故障模式和多传感器时间序列数据的融合是一个挑战，现有模型往往忽视两者关系或缺乏统计严谨性。

Method: 结合Cox比例风险模型、卷积多输出高斯过程和多项故障模式分布的分层贝叶斯框架，使用变分贝叶斯和蒙特卡洛采样进行后验推断和预测。

Result: 在喷气发动机数据集上通过数值和案例研究验证了模型优势。

Conclusion: 该模型能够有效整合多源数据并提供鲁棒的不确定性量化。

Abstract: Modern industrial systems are often subject to multiple failure modes, and
their conditions are monitored by multiple sensors, generating multiple
time-series signals. Additionally, time-to-failure data are commonly available.
Accurately predicting a system's remaining useful life (RUL) requires
effectively leveraging multi-sensor time-series data alongside multi-mode
failure event data. In most existing models, failure modes and RUL prediction
are performed independently, ignoring the inherent relationship between these
two tasks. Some models integrate multiple failure modes and event prediction
using black-box machine learning approaches, which lack statistical rigor and
cannot characterize the inherent uncertainty in the model and data. This paper
introduces a unified approach to jointly model the multi-sensor time-series
data and failure time concerning multiple failure modes. This proposed model
integrate a Cox proportional hazards model, a Convolved Multi-output Gaussian
Process, and multinomial failure mode distributions in a hierarchical Bayesian
framework with corresponding priors, enabling accurate prediction with robust
uncertainty quantification. Posterior distributions are effectively obtained by
Variational Bayes, and prediction is performed with Monte Carlo sampling. The
advantages of the proposed model is validated through extensive numerical and
case studies with jet-engine dataset.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [387] [Quantum Artificial Intelligence for Secure Autonomous Vehicle Navigation: An Architectural Proposal](https://arxiv.org/abs/2506.16000)
*Hemanth Kannamarlapudi,Sowmya Chintalapudi*

Key words: 量子计算, 自动驾驶车辆, 传感器融合

TL;DR: 提出了一种基于量子人工智能的新型架构，用于自动驾驶车辆的导航决策和通信，包括量子神经网络、量子强化学习模块和后量子加密协议。

<details>
  <summary>Details</summary>

Main category: cs.ET

Motivation: 自动驾驶车辆的导航需要处理大量数据并做出安全决策，现有方法在性能和安全性上存在挑战。

Method: 采用量子神经网络进行多模态传感器融合，Nav-Q模块通过量子变分电路学习导航策略，并使用后量子加密协议保障通信安全。

Result: 该框架为自动驾驶导航提供了量子性能提升和未来安全保障。

Conclusion: 提出的方法解决了自动驾驶导航中的基础性挑战。

Abstract: Navigation is a very crucial aspect of autonomous vehicle ecosystem which
heavily relies on collecting and processing large amounts of data in various
states and taking a confident and safe decision to define the next vehicle
maneuver. In this paper, we propose a novel architecture based on Quantum
Artificial Intelligence by enabling quantum and AI at various levels of
navigation decision making and communication process in Autonomous vehicles :
Quantum Neural Networks for multimodal sensor fusion, Nav-Q for Quantum
reinforcement learning for navigation policy optimization and finally
post-quantum cryptographic protocols for secure communication. Quantum neural
networks uses quantum amplitude encoding to fuse data from various sensors like
LiDAR, radar, camera, GPS and weather etc., This approach gives a unified
quantum state representation between heterogeneous sensor modalities. Nav-Q
module processes the fused quantum states through variational quantum circuits
to learn optimal navigation policies under swift dynamic and complex
conditions. Finally, post quantum cryptographic protocols are used to secure
communication channels for both within vehicle communication and V2X (Vehicle
to Everything) communications and thus secures the autonomous vehicle
communication from both classical and quantum security threats. Thus, the
proposed framework addresses fundamental challenges in autonomous vehicles
navigation by providing quantum performance and future proof security. Index
Terms Quantum Computing, Autonomous Vehicles, Sensor Fusion

</details>


### [388] [Artificial Intelligence for Atmospheric Sciences: A Research Roadmap](https://arxiv.org/abs/2506.16281)
*Martha Arbayani Zaidan,Naser Hossein Motlagh,Petteri Nurmi,Tareq Hussein,Markku Kulmala,Tuukka Petäjä,Sasu Tarkoma*

Key words: 气象科学, 人工智能, 大数据, 跨学科研究, 气候变化

TL;DR: 本文综述了人工智能在气象科学中的关键作用，探讨了大数据和基础设施方面的挑战，并提出了未来研究的路线图。

<details>
  <summary>Details</summary>

Main category: cs.ET

Motivation: 气象科学对于理解空气质量、极端天气和气候变化至关重要。近期传感、通信、计算和AI技术的突破为气象研究提供了大量数据和分析工具。

Method: 通过跨学科综述，结合气象科学与计算机科学，分析了AI在气象研究中的应用。

Result: 明确了AI融入气象研究的关键挑战（如大数据和基础设施），并提出了研究路线图。

Conclusion: AI具有变革气象研究的潜力，但需解决技术和资源方面的挑战。

Abstract: Atmospheric sciences are crucial for understanding environmental phenomena
ranging from air quality to extreme weather events, and climate change. Recent
breakthroughs in sensing, communication, computing, and Artificial Intelligence
(AI) have significantly advanced atmospheric sciences, enabling the generation
of vast amounts of data through long-term Earth observations and providing
powerful tools for analyzing atmospheric phenomena and predicting natural
disasters. This paper contributes a critical interdisciplinary overview that
bridges the fields of atmospheric science and computer science, highlighting
the transformative potential of AI in atmospheric research. We identify key
challenges associated with integrating AI into atmospheric research, including
issues related to big data and infrastructure, and provide a detailed research
roadmap that addresses both current and emerging challenges.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [389] [Unpacking Generative AI in Education: Computational Modeling of Teacher and Student Perspectives in Social Media Discourse](https://arxiv.org/abs/2506.16412)
*Paulina DeVito,Akhil Vallala,Sean Mcmahon,Yaroslav Hinda,Benjamin Thaw,Hanqi Zhuang,Hari Kalva*

Key words: 生成式AI（GAI）、教育、社交媒体分析、情感分析、主题建模、大语言模型（LLM）

TL;DR: 该研究利用社交媒体数据分析了教育领域中的生成式AI（GAI）技术，通过情感分析、主题建模和作者分类等方法，揭示了学生和教师的观点差异，并提出改进建议。

<details>
  <summary>Details</summary>

Main category: cs.SI

Motivation: 随着生成式AI在教育领域的快速发展，了解学生和教师对这些工具的看法至关重要。本研究旨在通过社交媒体数据全面分析相关利益方的讨论动态。

Method: 研究使用了1,199篇Reddit帖子和13,959条评论，结合情感分析、主题建模和作者分类。提出了一个基于提示的大语言模型（LLM）框架，并与传统NLP模型进行比较。

Result: GPT-4o框架在所有任务中表现优异（如情感分析准确率达90.6%），揭示了12个潜在主题。学生和教师对GAI的看法存在明显差异：学生担忧AI检测器的误判，教师则关注职业安全和学术诚信。

Conclusion: 研究呼吁更明确的政策、透明的GAI整合实践以及支持机制。同时展示了LLM框架在分析在线社区利益方讨论中的潜力。

Abstract: Generative AI (GAI) technologies are quickly reshaping the educational
landscape. As adoption accelerates, understanding how students and educators
perceive these tools is essential. This study presents one of the most
comprehensive analyses to date of stakeholder discourse dynamics on GAI in
education using social media data. Our dataset includes 1,199 Reddit posts and
13,959 corresponding top-level comments. We apply sentiment analysis, topic
modeling, and author classification. To support this, we propose and validate a
modular framework that leverages prompt-based large language models (LLMs) for
analysis of online social discourse, and we evaluate this framework against
classical natural language processing (NLP) models. Our GPT-4o pipeline
consistently outperforms prior approaches across all tasks. For example, it
achieved 90.6% accuracy in sentiment analysis against gold-standard human
annotations. Topic extraction uncovered 12 latent topics in the public
discourse with varying sentiment and author distributions. Teachers and
students convey optimism about GAI's potential for personalized learning and
productivity in higher education. However, key differences emerged: students
often voice distress over false accusations of cheating by AI detectors, while
teachers generally express concern about job security, academic integrity, and
institutional pressures to adopt GAI tools. These contrasting perspectives
highlight the tension between innovation and oversight in GAI-enabled learning
environments. Our findings suggest a need for clearer institutional policies,
more transparent GAI integration practices, and support mechanisms for both
educators and students. More broadly, this study demonstrates the potential of
LLM-based frameworks for modeling stakeholder discourse within online
communities.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [390] [Fair Contracts in Principal-Agent Games with Heterogeneous Types](https://arxiv.org/abs/2506.15887)
*Jakub Tłuczek,Victor Villin,Christos Dimitrakakis*

Key words: 公平性,多智能体系统,委托代理游戏,自适应合同,效率

TL;DR: 论文提出了一种基于重复委托代理游戏的框架，通过自适应的合同设计，在确保效率的同时实现多智能体系统中的公平性。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 现实中多智能体系统因潜在差异导致财富分配不均，本文旨在通过公平性感知的框架解决这一问题。

Method: 采用重复委托代理游戏模型，委托人通过学习提供自适应合同，设计同质线性合同以均衡结果。

Result: 框架能平等化各智能体的结果，同时保持系统整体性能和稳定性。

Conclusion: 公平性与效率可以并存，自适应合同设计是实现公平稳定的有效途径。

Abstract: Fairness is desirable yet challenging to achieve within multi-agent systems,
especially when agents differ in latent traits that affect their abilities.
This hidden heterogeneity often leads to unequal distributions of wealth, even
when agents operate under the same rules. Motivated by real-world examples, we
propose a framework based on repeated principal-agent games, where a principal,
who also can be seen as a player of the game, learns to offer adaptive
contracts to agents. By leveraging a simple yet powerful contract structure, we
show that a fairness-aware principal can learn homogeneous linear contracts
that equalize outcomes across agents in a sequential social dilemma.
Importantly, this fairness does not come at the cost of efficiency: our results
demonstrate that it is possible to promote equity and stability in the system
while preserving overall performance.

</details>


### [391] [Solving Zero-Sum Convex Markov Games](https://arxiv.org/abs/2506.16120)
*Fivos Kalogiannis,Emmanouil-Vasileios Vlatakis-Gkaragkounis,Ian Gemp,Georgios Piliouras*

Key words: 凸马尔可夫博弈，纳什均衡，策略梯度，非凸正则化，全局收敛

TL;DR: 论文首次证明了在两人零和凸马尔可夫博弈（cMGs）中使用独立策略梯度方法全局收敛到纳什均衡的可行性，并提出了一种非凸正则化方法来解决博弈中的挑战。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 研究动机是解决凸马尔可夫博弈中存在的非凸性、贝尔曼一致性的缺失以及无限视野复杂性等挑战，为多智能体策略交互提供理论保证。

Method: 采用两步法：首先利用隐藏凸-隐藏凹函数的特性，通过非凸正则化将问题转化为非凸近端Polyak-Lojasiewicz（NC-pPL）目标；其次在NC-pPL和双向pPL条件下解决一般约束最小最大问题。

Result: 论文首次为随机嵌套和交替梯度下降-上升方法提供了全局收敛保证，并证明了独立策略梯度方法在凸马尔可夫博弈中的有效性。

Conclusion: 通过非凸正则化和两步法，论文成功解决了凸马尔可夫博弈中的收敛问题，为相关领域提供了新工具和理论支持。

Abstract: We contribute the first provable guarantees of global convergence to Nash
equilibria (NE) in two-player zero-sum convex Markov games (cMGs) by using
independent policy gradient methods. Convex Markov games, recently defined by
Gemp et al. (2024), extend Markov decision processes to multi-agent settings
with preferences that are convex over occupancy measures, offering a broad
framework for modeling generic strategic interactions. However, even the
fundamental min-max case of cMGs presents significant challenges, including
inherent nonconvexity, the absence of Bellman consistency, and the complexity
of the infinite horizon.
  We follow a two-step approach. First, leveraging properties of
hidden-convex--hidden-concave functions, we show that a simple nonconvex
regularization transforms the min-max optimization problem into a
nonconvex-proximal Polyak-Lojasiewicz (NC-pPL) objective. Crucially, this
regularization can stabilize the iterates of independent policy gradient
methods and ultimately lead them to converge to equilibria. Second, building on
this reduction, we address the general constrained min-max problems under
NC-pPL and two-sided pPL conditions, providing the first global convergence
guarantees for stochastic nested and alternating gradient descent-ascent
methods, which we believe may be of independent interest.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [392] [Multi-use LLM Watermarking and the False Detection Problem](https://arxiv.org/abs/2506.15975)
*Zihao Fu,Chris Russell*

Key words: 数字水印,误检问题,双水印,检测,用户识别

TL;DR: 论文提出了一种双水印方法，解决了现有数字水印在检测和用户识别中的误检问题，通过联合编码显著降低了误报率。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 现有的数字水印技术在同时用于检测和用户识别时，随着用户容量增加，会导致未加水印的文本被误检为加水印。论文旨在解决这一问题。

Method: 通过理论分析提出双水印技术，联合编码检测水印和识别水印。

Result: 实验验证了理论分析，双水印显著降低了误报率并保持了高检测准确率。

Conclusion: 双水印是一种有效的解决方案，能够兼顾检测和识别需求。

Abstract: Digital watermarking is a promising solution for mitigating some of the risks
arising from the misuse of automatically generated text. These approaches
either embed non-specific watermarks to allow for the detection of any text
generated by a particular sampler, or embed specific keys that allow the
identification of the LLM user. However, simultaneously using the same
embedding for both detection and user identification leads to a false detection
problem, whereby, as user capacity grows, unwatermarked text is increasingly
likely to be falsely detected as watermarked. Through theoretical analysis, we
identify the underlying causes of this phenomenon. Building on these insights,
we propose Dual Watermarking which jointly encodes detection and identification
watermarks into generated text, significantly reducing false positives while
maintaining high detection accuracy. Our experimental results validate our
theoretical findings and demonstrate the effectiveness of our approach.

</details>


### [393] [Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models](https://arxiv.org/abs/2506.16447)
*Biao Yi,Tiansheng Huang,Sishuo Chen,Tong Li,Zheli Liu,Zhixuan Chu,Yiming Li*

Key words: 大语言模型, 后门攻击, 黑盒防御, BEAT, 安全对齐

TL;DR: 论文提出了一种名为BEAT的黑盒防御方法，用于检测和解除大语言模型（LLM）中的后门攻击，通过探测拼接效应识别触发样本，有效应对样本依赖性目标的挑战。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 针对大语言模型的后门攻击（Backdoor unalignment attacks）能够隐秘地破坏安全对齐，且难以通过常规审核发现，对LLMaaS应用构成严重威胁。这些攻击具有样本依赖性，扩大了目标空间。

Method: BEAT通过探测拼接效应，测量输入前后探测输出的分布变化来识别触发样本。该方法从相反角度应对样本依赖性挑战，利用多次采样逼近输出分布。

Result: 实验验证了BEAT在多种后门攻击和LLM（包括闭源GPT-3.5-turbo）上的有效性和高效性，并能初步防御流行的越狱攻击。

Conclusion: BEAT为黑盒环境下的后门攻击提供了一种高效防御方案，且能扩展到其他类型的攻击。

Abstract: Backdoor unalignment attacks against Large Language Models (LLMs) enable the
stealthy compromise of safety alignment using a hidden trigger while evading
normal safety auditing. These attacks pose significant threats to the
applications of LLMs in the real-world Large Language Model as a Service
(LLMaaS) setting, where the deployed model is a fully black-box system that can
only interact through text. Furthermore, the sample-dependent nature of the
attack target exacerbates the threat. Instead of outputting a fixed label, the
backdoored LLM follows the semantics of any malicious command with the hidden
trigger, significantly expanding the target space. In this paper, we introduce
BEAT, a black-box defense that detects triggered samples during inference to
deactivate the backdoor. It is motivated by an intriguing observation (dubbed
the probe concatenate effect), where concatenated triggered samples
significantly reduce the refusal rate of the backdoored LLM towards a malicious
probe, while non-triggered samples have little effect. Specifically, BEAT
identifies whether an input is triggered by measuring the degree of distortion
in the output distribution of the probe before and after concatenation with the
input. Our method addresses the challenges of sample-dependent targets from an
opposite perspective. It captures the impact of the trigger on the refusal
signal (which is sample-independent) instead of sample-specific successful
attack behaviors. It overcomes black-box access limitations by using multiple
sampling to approximate the output distribution. Extensive experiments are
conducted on various backdoor attacks and LLMs (including the closed-source
GPT-3.5-turbo), verifying the effectiveness and efficiency of our defense.
Besides, we also preliminarily verify that BEAT can effectively defend against
popular jailbreak attacks, as they can be regarded as 'natural backdoors'.

</details>


### [394] [PRISON: Unmasking the Criminal Potential of Large Language Models](https://arxiv.org/abs/2506.16150)
*Xinyi Wu,Geng Hong,Pei Chen,Yueyue Chen,Xudong Pan,Min Yang*

Key words: 大型语言模型、犯罪能力、对抗鲁棒性、行为对齐、安全机制

TL;DR: 本研究提出PRISON框架，用于量化大型语言模型（LLM）在犯罪潜在能力五个维度的表现，揭示其可能的不当行为。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 随着LLM的发展，其在复杂社会背景下的潜在不当行为引发关注。现有研究缺乏对其犯罪能力的系统评估。

Method: 通过PRISON框架，使用经典电影改编的结构化犯罪情景，评估LLM的犯罪潜在能力和反犯罪能力。

Result: 研究发现，先进LLM常表现出突发犯罪倾向（如误导性陈述或逃避策略），且在识别欺骗行为时准确性仅为41%。

Conclusion: 研究强调了在广泛部署LLM前，需加强其对抗鲁棒性、行为一致性和安全机制。

Abstract: As large language models (LLMs) advance, concerns about their misconduct in
complex social contexts intensify. Existing research overlooked the systematic
understanding and assessment of their criminal capability in realistic
interactions. We propose a unified framework PRISON, to quantify LLMs' criminal
potential across five dimensions: False Statements, Frame-Up, Psychological
Manipulation, Emotional Disguise, and Moral Disengagement. Using structured
crime scenarios adapted from classic films, we evaluate both criminal potential
and anti-crime ability of LLMs via role-play. Results show that
state-of-the-art LLMs frequently exhibit emergent criminal tendencies, such as
proposing misleading statements or evasion tactics, even without explicit
instructions. Moreover, when placed in a detective role, models recognize
deceptive behavior with only 41% accuracy on average, revealing a striking
mismatch between conducting and detecting criminal behavior. These findings
underscore the urgent need for adversarial robustness, behavioral alignment,
and safety mechanisms before broader LLM deployment.

</details>


### [395] [Towards Effective Complementary Security Analysis using Large Language Models](https://arxiv.org/abs/2506.16899)
*Jonas Wagner,Simon Müller,Christian Näther,Jan-Philipp Steghöfer,Andreas Both*

Key words: 静态应用安全测试,大语言模型,误报率,Chain-of-Thought,Self-Consistency

TL;DR: 使用大语言模型（LLM）提升静态应用安全测试（SAST）工具报告的评估效率，显著降低误报率。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: SAST工具报告中的大量误报（FPs）降低了安全分析效率，提出利用LLM改进评估效果。

Method: 采用Chain-of-Thought和Self-Consistency等提示技术，结合OWASP Benchmark和真实项目数据进行实验。

Result: LLM在OWASP Benchmark中检测62.5%的误报，联合多个模型提升至78.9%；真实数据中单个模型检测33.85%，联合模型提升至38.46%。

Conclusion: LLM能有效补充传统SAST工具，提高自动化水平并减少误报处理资源。

Abstract: A key challenge in security analysis is the manual evaluation of potential
security weaknesses generated by static application security testing (SAST)
tools. Numerous false positives (FPs) in these reports reduce the effectiveness
of security analysis. We propose using Large Language Models (LLMs) to improve
the assessment of SAST findings. We investigate the ability of LLMs to reduce
FPs while trying to maintain a perfect true positive rate, using datasets
extracted from the OWASP Benchmark (v1.2) and a real-world software project.
Our results indicate that advanced prompting techniques, such as
Chain-of-Thought and Self-Consistency, substantially improve FP detection.
Notably, some LLMs identified approximately 62.5% of FPs in the OWASP Benchmark
dataset without missing genuine weaknesses. Combining detections from different
LLMs would increase this FP detection to approximately 78.9%. Additionally, we
demonstrate our approach's generalizability using a real-world dataset covering
five SAST tools, three programming languages, and infrastructure files. The
best LLM detected 33.85% of all FPs without missing genuine weaknesses, while
combining detections from different LLMs would increase this detection to
38.46%. Our findings highlight the potential of LLMs to complement traditional
SAST tools, enhancing automation and reducing resources spent addressing false
alarms.

</details>


### [396] [Malware Classification Leveraging NLP & Machine Learning for Enhanced Accuracy](https://arxiv.org/abs/2506.16224)
*Bishwajit Prasad Gond,Rajneekant,Pushkar Kishore,Durga Prasad Mohapatra*

Key words: 自然语言处理, n-gram分析, 恶意软件分类, 机器学习, 特征选择

TL;DR: 该论文研究了基于自然语言处理（NLP）的n-gram分析和机器学习技术在恶意软件分类中的应用，通过提取文本特征实现更高精度的分类，准确率达99.02%。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 探讨如何利用NLP技术从恶意软件样本中提取和分析文本特征，以区分恶意软件与良性软件家族的独特语言模式。

Method: 采用NLP的n-gram分析方法，研究n-gram大小选择、特征表示和分类算法，并结合混合特征选择技术降低特征维度。

Result: 在真实恶意软件样本上测试，实现了99.02%的分类准确率，特征集减少至原始特征的1.6%。

Conclusion: 提出的n-gram方法结合混合特征选择技术显著提升了恶意软件分类的准确性和效率。

Abstract: This paper investigates the application of natural language processing
(NLP)-based n-gram analysis and machine learning techniques to enhance malware
classification. We explore how NLP can be used to extract and analyze textual
features from malware samples through n-grams, contiguous string or API call
sequences. This approach effectively captures distinctive linguistic patterns
among malware and benign families, enabling finer-grained classification. We
delve into n-gram size selection, feature representation, and classification
algorithms. While evaluating our proposed method on real-world malware samples,
we observe significantly improved accuracy compared to the traditional methods.
By implementing our n-gram approach, we achieved an accuracy of 99.02% across
various machine learning algorithms by using hybrid feature selection technique
to address high dimensionality. Hybrid feature selection technique reduces the
feature set to only 1.6% of the original features.

</details>


### [397] [The Hitchhiker's Guide to Efficient, End-to-End, and Tight DP Auditing](https://arxiv.org/abs/2506.16666)
*Meenatchi Sundaram Muthu Selva Annamalai,Borja Balle,Jamie Hayes,Georgios Kaissis,Emiliano De Cristofaro*

Key words: 差分隐私（DP）、审计、系统化研究、开放挑战

TL;DR: 论文系统化研究了差分隐私（DP）审计技术，总结了当前研究的关键见解和开放挑战，提出了一个评估框架，并提出了未来的研究方向。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 旨在整理和系统化差分隐私（DP）审计技术的研究现状，为领域内提供清晰的评估标准和未来研究方向。

Method: 引入了一个全面的DP审计框架，设立了效率、端到端性和紧致性三个目标，并分析了现有审计技术的操作模式、威胁模型和评估函数。

Result: 提出了可复用的系统化方法，用于评估领域进展，并指出了研究中被忽视的关键细节和开放问题。

Conclusion: 论文为DP审计领域提供了方法论指南，识别了研究中的摩擦点和未来方向。

Abstract: This paper systematizes research on auditing Differential Privacy (DP)
techniques, aiming to identify key insights into the current state of the art
and open challenges. First, we introduce a comprehensive framework for
reviewing work in the field and establish three cross-contextual desiderata
that DP audits should target--namely, efficiency, end-to-end-ness, and
tightness. Then, we systematize the modes of operation of state-of-the-art DP
auditing techniques, including threat models, attacks, and evaluation
functions. This allows us to highlight key details overlooked by prior work,
analyze the limiting factors to achieving the three desiderata, and identify
open research problems. Overall, our work provides a reusable and systematic
methodology geared to assess progress in the field and identify friction points
and future directions for our community to focus on.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [398] [Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning](https://arxiv.org/abs/2506.15828)
*Emanuele Musumeci,Michele Brienza,Francesco Argenziano,Vincenzo Suriani,Daniele Nardi,Domenico D. Bloisi*

Key words: 经典规划, 大语言模型, 机器人, 适应性, 场景图

TL;DR: 该论文提出了一种结合经典规划与大语言模型（LLMs）的方法，通过分层和逐步放松目标的方式，使机器人能够在复杂环境中适应并有效执行任务。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决经典规划方法在真实场景中适应性不足的问题，同时避免LLMs生成不可行或不安全计划的风险。

Method: 提出一种分层规划框架，利用LLMs提取常识知识并逐步放松目标，使任务更易处理。

Result: 通过定性和定量评估，证明该方法能在复杂环境中有效执行任务，优于基准方法。

Conclusion: 结合经典规划与LLMs的方法显著提升了机器人在复杂场景中的适应性和任务执行能力。

Abstract: Classical planning in AI and Robotics addresses complex tasks by shifting
from imperative to declarative approaches (e.g., PDDL). However, these methods
often fail in real scenarios due to limited robot perception and the need to
ground perceptions to planning predicates. This often results in heavily
hard-coded behaviors that struggle to adapt, even with scenarios where goals
can be achieved through relaxed planning. Meanwhile, Large Language Models
(LLMs) lead to planning systems that leverage commonsense reasoning but often
at the cost of generating unfeasible and/or unsafe plans. To address these
limitations, we present an approach integrating classical planning with LLMs,
leveraging their ability to extract commonsense knowledge and ground actions.
We propose a hierarchical formulation that enables robots to make unfeasible
tasks tractable by defining functionally equivalent goals through gradual
relaxation. This mechanism supports partial achievement of the intended
objective, suited to the agent's specific context. Our method demonstrates its
ability to adapt and execute tasks effectively within environments modeled
using 3D Scene Graphs through comprehensive qualitative and quantitative
evaluations. We also show how this method succeeds in complex scenarios where
other benchmark methods are more likely to fail. Code, dataset, and additional
material are released to the community.

</details>


### [399] [SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation](https://arxiv.org/abs/2506.15847)
*Arpit Bahety,Arnav Balaji,Ben Abbatematteo,Roberto Martín-Martín*

Key words: 机器人学习、移动操作、视频演示、安全验证、形态适配

TL;DR: SafeMimic框架通过单个人类视频演示，安全且自主地学习移动操作任务，结合语义解析、运动适配和安全验证。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 为使机器人在家庭中高效协助，需能通过人类演示学习新任务，同时减少依赖人工监控。

Method: 解析视频为语义和动作片段，适配机器人形态，并使用安全Q函数验证候选动作。

Result: 实验证明SafeMimic在七项任务中优于现有基线，能安全高效学习多步移动操作任务。

Conclusion: SafeMimic显著提升了机器人从单次人类演示中学习复杂任务的能力，且具备安全性和适应性。

Abstract: For robots to become efficient helpers in the home, they must learn to
perform new mobile manipulation tasks simply by watching humans perform them.
Learning from a single video demonstration from a human is challenging as the
robot needs to first extract from the demo what needs to be done and how,
translate the strategy from a third to a first-person perspective, and then
adapt it to be successful with its own morphology. Furthermore, to mitigate the
dependency on costly human monitoring, this learning process should be
performed in a safe and autonomous manner. We present SafeMimic, a framework to
learn new mobile manipulation skills safely and autonomously from a single
third-person human video. Given an initial human video demonstration of a
multi-step mobile manipulation task, SafeMimic first parses the video into
segments, inferring both the semantic changes caused and the motions the human
executed to achieve them and translating them to an egocentric reference. Then,
it adapts the behavior to the robot's own morphology by sampling candidate
actions around the human ones, and verifying them for safety before execution
in a receding horizon fashion using an ensemble of safety Q-functions trained
in simulation. When safe forward progression is not possible, SafeMimic
backtracks to previous states and attempts a different sequence of actions,
adapting both the trajectory and the grasping modes when required for its
morphology. As a result, SafeMimic yields a strategy that succeeds in the
demonstrated behavior and learns task-specific actions that reduce exploration
in future attempts. Our experiments show that our method allows robots to
safely and efficiently learn multi-step mobile manipulation behaviors from a
single human demonstration, from different users, and in different
environments, with improvements over state-of-the-art baselines across seven
tasks

</details>


### [400] [CapsDT: Diffusion-Transformer for Capsule Robot Manipulation](https://arxiv.org/abs/2506.16263)
*Xiting He,Mingwu Su,Xinqi Jiang,Long Bai,Jiewen Lai,Hongliang Ren*

Key words: Vision-Language-Action (VLA), 胶囊机器人, 扩散Transformer, 内窥镜任务, 机器人控制

TL;DR: 本文介绍了CapsDT，一种基于扩散Transformer的模型，用于胃镜胶囊机器人的操作，结合视觉和文本输入生成控制信号，提升了内窥镜任务的效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 探索Vision-Language-Action (VLA)模型在内窥镜胶囊机器人中的应用潜力，以改善人机交互、诊断准确性和治疗效果。

Method: 设计了CapsDT模型，通过处理视觉输入和文本指令来推断机器人控制信号，并开发了一个胶囊内窥镜机器人系统，由机械臂控制的磁铁驱动。

Result: CapsDT在多种内窥镜任务中表现优秀，达到了最先进的性能，并在实际模拟操作中取得了26.25%的成功率。

Conclusion: CapsDT展示了作为视觉-语言通用模型的潜力，为内窥镜机器人领域提供了高效且直观的解决方案。

Abstract: Vision-Language-Action (VLA) models have emerged as a prominent research
area, showcasing significant potential across a variety of applications.
However, their performance in endoscopy robotics, particularly endoscopy
capsule robots that perform actions within the digestive system, remains
unexplored. The integration of VLA models into endoscopy robots allows more
intuitive and efficient interactions between human operators and medical
devices, improving both diagnostic accuracy and treatment outcomes. In this
work, we design CapsDT, a Diffusion Transformer model for capsule robot
manipulation in the stomach. By processing interleaved visual inputs, and
textual instructions, CapsDT can infer corresponding robotic control signals to
facilitate endoscopy tasks. In addition, we developed a capsule endoscopy robot
system, a capsule robot controlled by a robotic arm-held magnet, addressing
different levels of four endoscopy tasks and creating corresponding capsule
robot datasets within the stomach simulator. Comprehensive evaluations on
various robotic tasks indicate that CapsDT can serve as a robust
vision-language generalist, achieving state-of-the-art performance in various
levels of endoscopy tasks while achieving a 26.25% success rate in real-world
simulation manipulation.

</details>


### [401] [Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining](https://arxiv.org/abs/2506.16475)
*Yaru Niu,Yunzhe Zhang,Mingyang Yu,Changyi Lin,Chenhao Li,Yikai Wang,Yuxiang Yang,Wenhao Yu,Tingnan Zhang,Bingqing Chen,Jonathan Francis,Zhenzhen Li,Jie Tan,Ding Zhao*

Key words: 四足机器人、模仿学习、跨实体训练、操作技能、数据集

TL;DR: 论文提出了一个跨实体模仿学习系统，用于四足机器人的多功能操作，结合人类和机器人数据，显著提升了任务成功率。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决四足机器人在复杂环境中实现多样化自主操作技能的挑战。

Method: 开发了遥操作和数据收集流程，并提出模块化架构以支持跨实体数据的联合训练和预训练。

Result: 系统在六项实际任务中平均成功率提升41.9%，利用人类数据预训练进一步提升了38.6%。

Conclusion: 该方法通过结合人类和机器人数据，显著提升了四足机器人的操作能力，尤其在未知环境中表现优异。

Abstract: Quadrupedal robots have demonstrated impressive locomotion capabilities in
complex environments, but equipping them with autonomous versatile manipulation
skills in a scalable way remains a significant challenge. In this work, we
introduce a cross-embodiment imitation learning system for quadrupedal
manipulation, leveraging data collected from both humans and LocoMan, a
quadruped equipped with multiple manipulation modes. Specifically, we develop a
teleoperation and data collection pipeline, which unifies and modularizes the
observation and action spaces of the human and the robot. To effectively
leverage the collected data, we propose an efficient modularized architecture
that supports co-training and pretraining on structured modality-aligned data
across different embodiments. Additionally, we construct the first manipulation
dataset for the LocoMan robot, covering various household tasks in both
unimanual and bimanual modes, supplemented by a corresponding human dataset. We
validate our system on six real-world manipulation tasks, where it achieves an
average success rate improvement of 41.9% overall and 79.7% under
out-of-distribution (OOD) settings compared to the baseline. Pretraining with
human data contributes a 38.6% success rate improvement overall and 82.7% under
OOD settings, enabling consistently better performance with only half the
amount of robot data. Our code, hardware, and data are open-sourced at:
https://human2bots.github.io.

</details>


### [402] [Grounding Language Models with Semantic Digital Twins for Robotic Planning](https://arxiv.org/abs/2506.16493)
*Mehreen Naeem,Andrew Melnik,Michael Beetz*

Key words: 语义数字孪生、大语言模型、机器人任务执行、动态环境、ALFRED基准测试

TL;DR: 提出了一种整合语义数字孪生与大语言模型的新框架，用于动态环境中机器人任务的适应性执行。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 通过结合SDT和LLM，提升机器人在动态环境中执行任务的适应性和目标导向性。

Method: 系统将自然语言指令分解为结构化动作三元组，并由SDT提供环境上下文数据进行语义接地。LLM利用错误反馈和SDT洞察生成恢复策略并迭代修订动作计划。

Result: 在ALFRED基准测试中表现出色，实现了家庭场景下的稳健任务完成。

Conclusion: 该框架成功将高级推理与语义环境理解结合，增强了在不确定性和失败情况下的任务可靠性。

Abstract: We introduce a novel framework that integrates Semantic Digital Twins (SDTs)
with Large Language Models (LLMs) to enable adaptive and goal-driven robotic
task execution in dynamic environments. The system decomposes natural language
instructions into structured action triplets, which are grounded in contextual
environmental data provided by the SDT. This semantic grounding allows the
robot to interpret object affordances and interaction rules, enabling action
planning and real-time adaptability. In case of execution failures, the LLM
utilizes error feedback and SDT insights to generate recovery strategies and
iteratively revise the action plan. We evaluate our approach using tasks from
the ALFRED benchmark, demonstrating robust performance across various household
scenarios. The proposed framework effectively combines high-level reasoning
with semantic environment understanding, achieving reliable task completion in
the face of uncertainty and failure.

</details>


### [403] [BIDA: A Bi-level Interaction Decision-making Algorithm for Autonomous Vehicles in Dynamic Traffic Scenarios](https://arxiv.org/abs/2506.16546)
*Liyang Yu,Tianyi Wang,Junfeng Jiao,Fengwu Shan,Hongqing Chu,Bingzhao Gao*

Key words: 自动驾驶车辆, 交互决策, 蒙特卡洛树搜索, 深度强化学习, 动态交通场景

TL;DR: 论文提出了一种双层次交互决策算法（BIDA），结合交互式蒙特卡洛树搜索（MCTS）和深度强化学习（DRL），以提高自动驾驶车辆（AVs）在动态交通场景中的交互合理性、效率和安全性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 在复杂交通环境中，自动驾驶车辆需要实时与人类交通参与者交互并做出安全关键决策，而人类行为的不可预测性给动态场景（如多车道高速公路和无信号T型交叉口）带来了巨大挑战。

Method: 设计了BIDA算法，集成交互式MCTS和DRL，利用DRL构建可靠的价值网络和策略网络，指导交互式MCTS的在线推理过程，并在CARLA中实现动态轨迹规划和跟踪控制器。

Result: 实验表明，BIDA不仅提升了交互推理效率并降低计算成本，还在多种交通条件下表现出优于其他最新基准的安全性和交互合理性。

Conclusion: BIDA在动态交通场景中为自动驾驶车辆的交互决策提供了高效、安全的解决方案。

Abstract: In complex real-world traffic environments, autonomous vehicles (AVs) need to
interact with other traffic participants while making real-time and
safety-critical decisions accordingly. The unpredictability of human behaviors
poses significant challenges, particularly in dynamic scenarios, such as
multi-lane highways and unsignalized T-intersections. To address this gap, we
design a bi-level interaction decision-making algorithm (BIDA) that integrates
interactive Monte Carlo tree search (MCTS) with deep reinforcement learning
(DRL), aiming to enhance interaction rationality, efficiency and safety of AVs
in dynamic key traffic scenarios. Specifically, we adopt three types of DRL
algorithms to construct a reliable value network and policy network, which
guide the online deduction process of interactive MCTS by assisting in value
update and node selection. Then, a dynamic trajectory planner and a trajectory
tracking controller are designed and implemented in CARLA to ensure smooth
execution of planned maneuvers. Experimental evaluations demonstrate that our
BIDA not only enhances interactive deduction and reduces computational costs,
but also outperforms other latest benchmarks, which exhibits superior safety,
efficiency and interaction rationality under varying traffic conditions.

</details>


### [404] [Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control](https://arxiv.org/abs/2506.16565)
*Yuxin Chen,Jianglan Wei,Chenfeng Xu,Boyi Li,Masayoshi Tomizuka,Andrea Bajcsy,Ran Tian*

Key words: 世界模型, 机器人学习, 视觉干扰物, ReOI, 开放环境

TL;DR: 论文提出了ReOI方法，通过检测和修正视觉干扰物，提升世界模型在开放环境中的预测可靠性，显著提高了任务成功率。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 世界模型在遇到训练中罕见的视觉干扰物时表现脆弱，影响了机器人学习和规划效果，需要一种方法提升其鲁棒性。

Method: 提出ReOI策略，包括检测视觉干扰物、修正当前观察、重新预测未来结果并恢复视觉一致性。

Result: ReOI在存在新干扰物的情况下，任务成功率提升了3倍，显著优于未干预的世界模型预测。

Conclusion: ReOI是一种简单有效的测试时策略，显著提升了世界模型在开放环境中的实用性和鲁棒性。

Abstract: World models enable robots to "imagine" future observations given current
observations and planned actions, and have been increasingly adopted as
generalized dynamics models to facilitate robot learning. Despite their
promise, these models remain brittle when encountering novel visual distractors
such as objects and background elements rarely seen during training.
Specifically, novel distractors can corrupt action outcome predictions, causing
downstream failures when robots rely on the world model imaginations for
planning or action verification. In this work, we propose Reimagination with
Observation Intervention (ReOI), a simple yet effective test-time strategy that
enables world models to predict more reliable action outcomes in open-world
scenarios where novel and unanticipated visual distractors are inevitable.
Given the current robot observation, ReOI first detects visual distractors by
identifying which elements of the scene degrade in physically implausible ways
during world model prediction. Then, it modifies the current observation to
remove these distractors and bring the observation closer to the training
distribution. Finally, ReOI "reimagines" future outcomes with the modified
observation and reintroduces the distractors post-hoc to preserve visual
consistency for downstream planning and verification. We validate our approach
on a suite of robotic manipulation tasks in the context of action verification,
where the verifier needs to select desired action plans based on predictions
from a world model. Our results show that ReOI is robust to both
in-distribution and out-of-distribution visual distractors. Notably, it
improves task success rates by up to 3x in the presence of novel distractors,
significantly outperforming action verification that relies on world model
predictions without imagination interventions.

</details>


### [405] [History-Augmented Vision-Language Models for Frontier-Based Zero-Shot Object Navigation](https://arxiv.org/abs/2506.16623)
*Mobin Habibpour,Fatemeh Afghah*

Key words: Object Navigation, Vision-Language Models, Zero-shot Learning, Robotics, Habitat

TL;DR: 本文提出了一种新颖的零样本目标导航框架，通过动态、历史感知提示深度整合视觉语言模型（VLM）的推理能力，显著提升了机器人在未知环境中寻找目标的性能。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 现有的目标导航方法对视觉语言模型的使用较为肤浅，仅用于对象-场景相似性检查，忽略了更深层次的推理能力，导致上下文理解不足和重复导航行为。

Method: 采用动态、历史感知提示策略，为VLM提供动作历史上下文，使其能生成导航动作的语义指导分数并避免决策循环，同时引入VLM辅助的路径生成机制优化最终接近目标的方式。

Result: 在HM3D数据集上的实验显示，成功率为46%，路径长度加权的成功率为24.8%，性能与最先进的零样本方法相当。

Conclusion: 该方法通过历史增强的VLM提示策略，显著提升了机器人导航的鲁棒性和上下文感知能力，展示了其在目标导航中的潜力。

Abstract: Object Goal Navigation (ObjectNav) challenges robots to find objects in
unseen environments, demanding sophisticated reasoning. While Vision-Language
Models (VLMs) show potential, current ObjectNav methods often employ them
superficially, primarily using vision-language embeddings for object-scene
similarity checks rather than leveraging deeper reasoning. This limits
contextual understanding and leads to practical issues like repetitive
navigation behaviors. This paper introduces a novel zero-shot ObjectNav
framework that pioneers the use of dynamic, history-aware prompting to more
deeply integrate VLM reasoning into frontier-based exploration. Our core
innovation lies in providing the VLM with action history context, enabling it
to generate semantic guidance scores for navigation actions while actively
avoiding decision loops. We also introduce a VLM-assisted waypoint generation
mechanism for refining the final approach to detected objects. Evaluated on the
HM3D dataset within Habitat, our approach achieves a 46% Success Rate (SR) and
24.8% Success weighted by Path Length (SPL). These results are comparable to
state-of-the-art zero-shot methods, demonstrating the significant potential of
our history-augmented VLM prompting strategy for more robust and context-aware
robotic navigation.

</details>


### [406] [Learning Dexterous Object Handover](https://arxiv.org/abs/2506.16822)
*Daniel Frau-Alfaro,Julio Castaño-Amoros,Santiago Puente,Pablo Gil,Roberto Calandra*

Key words: 物体交接,强化学习,双四元数,机器人协作

TL;DR: 本研究利用基于双四元数的新型奖励函数，通过强化学习（RL）实现了双手之间的灵巧物体交接，实验结果证明了其在未见物体和扰动情况下的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 在协作环境中，如家庭，机器人需要安全高效地进行物体交接以与人类互动。

Method: 使用强化学习（RL）和基于双四元数的奖励函数来最小化旋转距离，优于欧拉角和旋转矩阵等其他表示方法。

Result: 训练策略在未见物体和扰动情况下表现稳定，最佳场景下成功率达94%，且在其他机器人移动时性能仅下降13.8%。

Conclusion: 该策略为机器人物体交接提供了高效且鲁棒的解决方案。

Abstract: Object handover is an important skill that we use daily when interacting with
other humans. To deploy robots in collaborative setting, like houses, being
able to receive and handing over objects safely and efficiently becomes a
crucial skill. In this work, we demonstrate the use of Reinforcement Learning
(RL) for dexterous object handover between two multi-finger hands. Key to this
task is the use of a novel reward function based on dual quaternions to
minimize the rotation distance, which outperforms other rotation
representations such as Euler and rotation matrices. The robustness of the
trained policy is experimentally evaluated by testing w.r.t. objects that are
not included in the training distribution, and perturbations during the
handover process. The results demonstrate that the trained policy successfully
perform this task, achieving a total success rate of 94% in the best-case
scenario after 100 experiments, thereby showing the robustness of our policy
with novel objects. In addition, the best-case performance of the policy
decreases by only 13.8% when the other robot moves during the handover, proving
that our policy is also robust to this type of perturbation, which is common in
real-world object handovers.

</details>


### [407] [Steering Your Diffusion Policy with Latent Space Reinforcement Learning](https://arxiv.org/abs/2506.15799)
*Andrew Wagenmaker,Mitsuhiko Nakamoto,Yunchu Zhang,Seohong Park,Waleed Yagoub,Anusha Nagabandi,Abhishek Gupta,Sergey Levine*

Key words: 行为克隆, 强化学习, 扩散策略, 自适应策略, 机器人控制

TL;DR: 这篇论文提出了一种名为DSRL（扩散引导强化学习）的方法，通过结合行为克隆（BC）和强化学习（RL），实现高效的自适应策略改进，避免了传统方法中需要大量人类演示或样本的问题。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 行为克隆策略在初始性能不佳时通常需要额外的人类演示来改进，而这既昂贵又耗时；强化学习虽然可以自主改进策略，但需要大量样本。本文旨在解决这两种方法的局限性。

Method: 提出了DSRL方法，通过在BC策略的潜在噪声空间上运行RL来改进策略，既高效又无需修改基础策略的权重。

Result: DSRL在仿真基准测试、实际机器人任务以及预训练通用策略的适应中表现出高效性和有效性。

Conclusion: DSRL是一种高效的自主策略改进方法，适用于实际机器人任务，且无需额外的人类演示或修改基础策略。

Abstract: Robotic control policies learned from human demonstrations have achieved
impressive results in many real-world applications. However, in scenarios where
initial performance is not satisfactory, as is often the case in novel
open-world settings, such behavioral cloning (BC)-learned policies typically
require collecting additional human demonstrations to further improve their
behavior -- an expensive and time-consuming process. In contrast, reinforcement
learning (RL) holds the promise of enabling autonomous online policy
improvement, but often falls short of achieving this due to the large number of
samples it typically requires. In this work we take steps towards enabling fast
autonomous adaptation of BC-trained policies via efficient real-world RL.
Focusing in particular on diffusion policies -- a state-of-the-art BC
methodology -- we propose diffusion steering via reinforcement learning (DSRL):
adapting the BC policy by running RL over its latent-noise space. We show that
DSRL is highly sample efficient, requires only black-box access to the BC
policy, and enables effective real-world autonomous policy improvement.
Furthermore, DSRL avoids many of the challenges associated with finetuning
diffusion policies, obviating the need to modify the weights of the base policy
at all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks,
and for adapting pretrained generalist policies, illustrating its sample
efficiency and effective performance at real-world policy improvement.

</details>


### [408] [Investigating Lagrangian Neural Networks for Infinite Horizon Planning in Quadrupedal Locomotion](https://arxiv.org/abs/2506.16079)
*Prakrut Kotecha,Aditya Shirwatkar,Shishir Kolathaya*

Key words: 拉格朗日神经网络, 四足机器人, 动力学模型, 样本效率, 预测精度

TL;DR: 论文探讨了拉格朗日神经网络（LNNs）在四足机器人无限时域规划中的应用，通过四种动力学模型验证了其在样本效率和预测准确性上的显著优势。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 传统动力学模型在长时间预测中易受误差累积影响，而LNNs通过保留物理定律，提供了更准确稳定的预测，适合可持续运动规划。

Method: 研究评估了四种LNNs模型：全阶正向动力学（FD）、质量矩阵对角化表示、全阶逆向动力学（ID）训练与FD推断，以及基于躯体质心（CoM）的降阶模型。

Result: LNNs在样本效率上提升10倍，预测精度提高2-10倍，且对角化方法在降低计算复杂度的同时保留了可解释性，支持实时控制。

Conclusion: LNNs能有效捕捉四足机器人动力学结构，提升运动规划与控制性能，并通过更高控制频率展示了实际部署潜力。

Abstract: Lagrangian Neural Networks (LNNs) present a principled and interpretable
framework for learning the system dynamics by utilizing inductive biases. While
traditional dynamics models struggle with compounding errors over long
horizons, LNNs intrinsically preserve the physical laws governing any system,
enabling accurate and stable predictions essential for sustainable locomotion.
This work evaluates LNNs for infinite horizon planning in quadrupedal robots
through four dynamics models: (1) full-order forward dynamics (FD) training and
inference, (2) diagonalized representation of Mass Matrix in full order FD, (3)
full-order inverse dynamics (ID) training with FD inference, (4) reduced-order
modeling via torso centre-of-mass (CoM) dynamics. Experiments demonstrate that
LNNs bring improvements in sample efficiency (10x) and superior prediction
accuracy (up to 2-10x) compared to baseline methods. Notably, the
diagonalization approach of LNNs reduces computational complexity while
retaining some interpretability, enabling real-time receding horizon control.
These findings highlight the advantages of LNNs in capturing the underlying
structure of system dynamics in quadrupeds, leading to improved performance and
efficiency in locomotion planning and control. Additionally, our approach
achieves a higher control frequency than previous LNN methods, demonstrating
its potential for real-world deployment on quadrupeds.

</details>


### [409] [CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity](https://arxiv.org/abs/2506.16652)
*Guang Yin,Yitong Li,Yixuan Wang,Dale McConachie,Paarth Shah,Kunimatsu Hashimoto,Huan Zhang,Katherine Liu,Yunzhu Li*

Key words: 机器人操作, 自然语言指令, 视觉语言模型, 模糊性, 中间表示

TL;DR: 针对机器人操作任务中自然语言指令的模糊性问题，提出了一种基于视觉语言模型（VLM）的新型框架，通过生成可解释的中间代码来解析指令并解决语义模糊。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 自然语言指令在机器人操作任务中常常模糊不清，现有端到端模型因缺乏模块化和可解释性导致性能不佳。

Method: 使用VLM解析指令，生成可执行的任务特定代码，并结合感知模块生成3D注意力图以解决指令模糊问题。

Result: 实验表明该方法在涉及语言模糊性、密集接触和多物体交互的任务中表现优越。

Conclusion: 该框架通过可解释的中间表示解决了指令模糊问题，性能优于现有模仿学习方法。

Abstract: Natural language instructions for robotic manipulation tasks often exhibit
ambiguity and vagueness. For instance, the instruction "Hang a mug on the mug
tree" may involve multiple valid actions if there are several mugs and branches
to choose from. Existing language-conditioned policies typically rely on
end-to-end models that jointly handle high-level semantic understanding and
low-level action generation, which can result in suboptimal performance due to
their lack of modularity and interpretability. To address these challenges, we
introduce a novel robotic manipulation framework that can accomplish tasks
specified by potentially ambiguous natural language. This framework employs a
Vision-Language Model (VLM) to interpret abstract concepts in natural language
instructions and generates task-specific code - an interpretable and executable
intermediate representation. The generated code interfaces with the perception
module to produce 3D attention maps that highlight task-relevant regions by
integrating spatial and semantic information, effectively resolving ambiguities
in instructions. Through extensive experiments, we identify key limitations of
current imitation learning methods, such as poor adaptation to language and
environmental variations. We show that our approach excels across challenging
manipulation tasks involving language ambiguity, contact-rich manipulation, and
multi-object interactions.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [410] [Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support](https://arxiv.org/abs/2506.16473)
*Sophie Chiang,Guy Laban,Hatice Gunes*

Key words: 情感支持对话, 社交机器人, 语义分析, 心理健康

TL;DR: 该研究探讨了机器人情感支持对话与传统人类治疗对话的相似性，发现机器人对话的主题和语义与人类治疗对话高度重叠。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 随着对话机器人在情感支持对话中的应用增多，了解其与传统人类治疗对话的相似性变得重要。

Method: 通过分析人类治疗对话和机器人对话数据集，使用句子嵌入和K-means聚类评估主题和语义重叠。

Result: 90.88%的机器人对话主题可映射到人类治疗数据集，且语义高度重叠。

Conclusion: 机器人支持对话在主题和语义上与人类治疗对话相似，显示出其在心理健康干预中的潜力。

Abstract: As conversational agents increasingly engage in emotionally supportive
dialogue, it is important to understand how closely their interactions resemble
those in traditional therapy settings. This study investigates whether the
concerns shared with a robot align with those shared in human-to-human (H2H)
therapy sessions, and whether robot responses semantically mirror those of
human therapists. We analyzed two datasets: one of interactions between users
and professional therapists (Hugging Face's NLP Mental Health Conversations),
and another involving supportive conversations with a social robot (QTrobot
from LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence
embeddings and K-means clustering, we assessed cross-agent thematic alignment
by applying a distance-based cluster-fitting method that evaluates whether
responses from one agent type map to clusters derived from the other, and
validated it using Euclidean distances. Results showed that 90.88% of robot
conversation disclosures could be mapped to clusters from the human therapy
dataset, suggesting shared topical structure. For matched clusters, we compared
the subjects as well as therapist and robot responses using Transformer,
Word2Vec, and BERT embeddings, revealing strong semantic overlap in subjects'
disclosures in both datasets, as well as in the responses given to similar
human disclosure themes across agent types (robot vs. human therapist). These
findings highlight both the parallels and boundaries of robot-led support
conversations and their potential for augmenting mental health interventions.

</details>


### [411] [On using AI for EEG-based BCI applications: problems, current challenges and future trends](https://arxiv.org/abs/2506.16168)
*Thomas Barbera,Jacopo Burger,Alessandro D'Amelio,Simone Zini,Simone Bianco,Raffaella Lanzarotti,Paolo Napoletano,Giuseppe Boccignone,Jose Luis Contreras-Vidal*

Key words: 人工智能, 脑电图, 脑机接口, 因果分析, 技术挑战

TL;DR: 论文探讨了AI在解码脑电图（EEG）信号以开发革命性脑机接口（BCI）中的应用及其挑战，并提出了未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 探索AI如何通过EEG信号解码实现新型脑机接口（如脑语音、脑图像转换），并解决相关技术和伦理问题。

Method: 通过因果视角分析EEG-based BCI的基础范式，并评估AI模型的挑战。

Result: 提出未来可能克服当前技术、方法和伦理限制的研究方向。

Conclusion: 为开发实用且高效的EEG-based BCI解决方案提供清晰的路线图。

Abstract: Imagine unlocking the power of the mind to communicate, create, and even
interact with the world around us. Recent breakthroughs in Artificial
Intelligence (AI), especially in how machines "see" and "understand" language,
are now fueling exciting progress in decoding brain signals from scalp
electroencephalography (EEG). Prima facie, this opens the door to revolutionary
brain-computer interfaces (BCIs) designed for real life, moving beyond
traditional uses to envision Brain-to-Speech, Brain-to-Image, and even a
Brain-to-Internet of Things (BCIoT).
  However, the journey is not as straightforward as it was for Computer Vision
(CV) and Natural Language Processing (NLP). Applying AI to real-world EEG-based
BCIs, particularly in building powerful foundational models, presents unique
and intricate hurdles that could affect their reliability.
  Here, we unfold a guided exploration of this dynamic and rapidly evolving
research area. Rather than barely outlining a map of current endeavors and
results, the goal is to provide a principled navigation of this hot and
cutting-edge research landscape. We consider the basic paradigms that emerge
from a causal perspective and the attendant challenges presented to AI-based
models. Looking ahead, we then discuss promising research avenues that could
overcome today's technological, methodological, and ethical limitations. Our
aim is to lay out a clear roadmap for creating truly practical and effective
EEG-based BCI solutions that can thrive in everyday environments.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [412] [Convergent Methods for Koopman Operators on Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2506.15782)
*Nicolas Boullé,Matthew J. Colbrook,Gustav Conradie*

Key words: Koopman算子, RKHS, 光谱分析, 数据驱动, 高维数据

TL;DR: 论文提出了一种基于RKHS的数据驱动方法，用于计算Koopman和Perron--Frobenius算子的光谱性质，解决了传统L^2空间的局限性，并证明了算法的收敛性和最优性。

<details>
  <summary>Details</summary>

Main category: math.NA

Motivation: 现有方法在处理高维数据时存在效率低和灵活性不足的问题，特别是在L^2空间中。通过将Koopman算子定义在RKHS上，可以改善计算效率和预测精度。

Method: 引入了基于RKHS的通用算法，计算算子的光谱性质和伪谱，利用用户指定的核函数确定函数空间，避免大数据的限制。

Result: 算法在实际高维数据（如湍流、分子动力学、海洋数据）中表现出色，证明了收敛性和最优性。

Conclusion: 通过RKHS框架和优化算法，显著提升了高维数据的光谱分析效率，未来可应用于更多复杂系统。

Abstract: Data-driven spectral analysis of Koopman operators is a powerful tool for
understanding numerous real-world dynamical systems, from neuronal activity to
variations in sea surface temperature. The Koopman operator acts on a function
space and is most commonly studied on the space of square-integrable functions.
However, defining it on a suitable reproducing kernel Hilbert space (RKHS)
offers numerous practical advantages, including pointwise predictions with
error bounds, improved spectral properties that facilitate computations, and
more efficient algorithms, particularly in high dimensions. We introduce the
first general, provably convergent, data-driven algorithms for computing
spectral properties of Koopman and Perron--Frobenius operators on RKHSs. These
methods efficiently compute spectra and pseudospectra with error control and
spectral measures while exploiting the RKHS structure to avoid the large-data
limits required in the $L^2$ settings. The function space is determined by a
user-specified kernel, eliminating the need for quadrature-based sampling as in
$L^2$ and enabling greater flexibility with finite, externally provided
datasets. Using the Solvability Complexity Index hierarchy, we construct
adversarial dynamical systems for these problems to show that no algorithm can
succeed in fewer limits, thereby proving the optimality of our algorithms.
Notably, this impossibility extends to randomized algorithms and datasets. We
demonstrate the effectiveness of our algorithms on challenging,
high-dimensional datasets arising from real-world measurements and
high-fidelity numerical simulations, including turbulent channel flow,
molecular dynamics of a binding protein, Antarctic sea ice concentration, and
Northern Hemisphere sea surface height. The algorithms are publicly available
in the software package $\texttt{SpecRKHS}$.

</details>


<div id='cond-mat.quant-gas'></div>

# cond-mat.quant-gas [[Back]](#toc)

### [413] [Single-shot thermometry of simulated Bose--Einstein condensates using artificial intelligence](https://arxiv.org/abs/2506.16925)
*Jack Griffiths,Steven A. Wrathmall,Simon A. Gardiner*

Key words: ultracold Bose gases, convolutional neural network, thermodynamic parameters, non-destructive measurement, zero-shot generalisation

TL;DR: 使用卷积神经网络从单次成像的密度分布中快速无损估计玻色气体的化学势和温度，展示了在未见过的几何和动态过程中的泛化能力。

<details>
  <summary>Details</summary>

Main category: cond-mat.quant-gas

Motivation: 传统测量技术在超冷玻色气体中难以精确测量热力学参数，且具有破坏性。因此需要一种快速、无损的方法来替代。

Method: 利用卷积神经网络对谐波陷阱中的准2D玻色气体进行训练，通过单次成像的密度分布估计化学势和温度。

Result: 模型在未见过的环形陷阱和动态热化过程中表现出色，误差仅为几纳开尔文，且无需额外训练。

Conclusion: 监督学习可以克服传统超冷原子测温的限制，有望扩展到更广泛的几何配置和温度范围，实现量子气体实验的实时分析。

Abstract: Precise determination of thermodynamic parameters in ultracold Bose gases
remains challenging due to the destructive nature of conventional measurement
techniques and inherent experimental uncertainties. We demonstrate an
artificial intelligence approach for rapid, non-destructive estimation of the
chemical potential and temperature from single-shot, in situ imaged density
profiles of finite-temperature Bose gases. Our convolutional neural network is
trained exclusively on quasi-2D `pancake' condensates in harmonic trap
configurations. It achieves parameter extraction within fractions of a second.
The model also demonstrates zero-shot generalisation across both trap geometry
and thermalisation dynamics, successfully estimating thermodynamic parameters
for toroidally trapped condensates with errors of only a few nanokelvin despite
no prior exposure to such geometries during training, and maintaining
predictive accuracy during dynamic thermalisation processes after a relatively
brief evolution without explicit training on non-equilibrium states. These
results suggest that supervised learning can overcome traditional limitations
in ultracold atom thermometry, with extension to broader geometric
configurations, temperature ranges, and additional parameters potentially
enabling comprehensive real-time analysis of quantum gas experiments. Such
capabilities could significantly streamline experimental workflows whilst
improving measurement precision across a range of quantum fluid systems.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [414] [Category-based Galaxy Image Generation via Diffusion Models](https://arxiv.org/abs/2506.16255)
*Xingzhong Fan,Hongming Tang,Yue Zeng,M. B. N. Kouwenhoven,Guangquan Zeng*

Key words: 星系生成,扩散模型,数据驱动,天体物理,深度学习

TL;DR: GalCatDiff是一种基于扩散模型的天文学框架，结合图像特征和天体物理属性，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: astro-ph.IM

Motivation: 传统星系生成方法依赖物理假设和参数调优，数据驱动生成模型提供更高效替代方案。

Method: GalCatDiff结合增强U-Net和新型Astro-RAB块，动态融合注意力与卷积操作，并嵌入类别信息。

Result: 生成星系在颜色和尺寸分布一致性上显著提升，视觉真实且物理一致。

Conclusion: GalCatDiff增强星系模拟可靠性，可作为数据增强工具支持未来星系分类算法。

Abstract: Conventional galaxy generation methods rely on semi-analytical models and
hydrodynamic simulations, which are highly dependent on physical assumptions
and parameter tuning. In contrast, data-driven generative models do not have
explicit physical parameters pre-determined, and instead learn them efficiently
from observational data, making them alternative solutions to galaxy
generation. Among these, diffusion models outperform Variational Autoencoders
(VAEs) and Generative Adversarial Networks (GANs) in quality and diversity.
Leveraging physical prior knowledge to these models can further enhance their
capabilities. In this work, we present GalCatDiff, the first framework in
astronomy to leverage both galaxy image features and astrophysical properties
in the network design of diffusion models. GalCatDiff incorporates an enhanced
U-Net and a novel block entitled Astro-RAB (Residual Attention Block), which
dynamically combines attention mechanisms with convolution operations to ensure
global consistency and local feature fidelity. Moreover, GalCatDiff uses
category embeddings for class-specific galaxy generation, avoiding the high
computational costs of training separate models for each category. Our
experimental results demonstrate that GalCatDiff significantly outperforms
existing methods in terms of the consistency of sample color and size
distributions, and the generated galaxies are both visually realistic and
physically consistent. This framework will enhance the reliability of galaxy
simulations and can potentially serve as a data augmentor to support future
galaxy classification algorithm development.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [415] [Modern approaches to building effective interpretable models of the property market using machine learning](https://arxiv.org/abs/2506.15723)
*Irina G. Tanashkina,Alexey S. Tanashkin,Alexander S. Maksimchuik,Anna Yu. Poshivailo*

Key words: 机器学习, 房地产评估, 可解释模型, 线性回归, RuleFit

TL;DR: 本文回顾了利用机器学习构建房地产市场价格评估可解释模型的现代方法，重点解决了实际市场数据与理想数据之间的差异问题，并提出适用于不同类型房产的建模策略。

<details>
  <summary>Details</summary>

Main category: q-fin.ST

Motivation: 研究者缺乏专业知识时，构建高质量的房地产市场价格模型面临诸多困难，尤其是实际市场数据与教程中的理想数据差异巨大。本文旨在解决这些问题。

Method: 通过收集初始数据、识别异常值、分析数据模式、选择价格因素、构建模型和评估效率等步骤，结合线性回归与地统计插值方法（适用于土地）和RuleFit方法（适用于公寓）。

Result: 成功构建了适用于土地和公寓的有效模型，证明了在必须满足可解释性要求的情况下，仍能建立高效的房地产市场价格模型。

Conclusion: 尽管可解释性要求严格，但通过合理的方法仍能构建实用的房地产模型，这对法律等实际应用具有重要意义。

Abstract: In this article, we review modern approaches to building interpretable models
of property markets using machine learning on the base of mass valuation of
property in the Primorye region, Russia. The researcher, lacking expertise in
this topic, encounters numerous difficulties in the effort to build a good
model. The main source of this is the huge difference between noisy real market
data and ideal data which is very common in all types of tutorials on machine
learning. This paper covers all stages of modeling: the collection of initial
data, identification of outliers, the search and analysis of patterns in data,
the formation and final choice of price factors, the building of the model, and
the evaluation of its efficiency. For each stage, we highlight potential issues
and describe sound methods for overcoming emerging difficulties on actual
examples. We show that the combination of classical linear regression with
interpolation methods of geostatistics allows to build an effective model for
land parcels. For flats, when many objects are attributed to one spatial point
the application of geostatistical methods is difficult. Therefore we suggest
linear regression with automatic generation and selection of additional rules
on the base of decision trees, so called the RuleFit method. Thus we show, that
despite the strong restriction as the requirement of interpretability which is
important in practical aspects, for example, legal matters, it is still
possible to build effective models of real property markets.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [416] [MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers](https://arxiv.org/abs/2506.15862)
*Jushaan Singh Kalra,Xinran Zhao,To Eun Kim,Fengyu Cai,Fernando Diaz,Tongshuang Wu*

Key words: 检索增强生成, 动态选择, 异构检索器, 零样本, 人类信息源

TL;DR: 论文提出了一种动态选择和整合多种检索器的零样本方法，验证了其有效性和效率，且能整合人类信息源。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 实践中通常固定单一检索器，无法适应多样化信息需求，需动态选择和整合多种检索器。

Method: 引入检索器混合框架，零样本加权组合异构检索器。

Result: 混合检索器表现优于单一检索器和更大模型，效率高，且能有效结合人类信息源。

Conclusion: 动态混合检索器是一种高效且灵活的方法，适用于多样化信息需求。

Abstract: Retrieval-augmented Generation (RAG) is powerful, but its effectiveness
hinges on which retrievers we use and how. Different retrievers offer distinct,
often complementary signals: BM25 captures lexical matches; dense retrievers,
semantic similarity. Yet in practice, we typically fix a single retriever based
on heuristics, which fails to generalize across diverse information needs. Can
we dynamically select and integrate multiple retrievers for each individual
query, without the need for manual selection? In our work, we validate this
intuition with quantitative analysis and introduce mixture of retrievers: a
zero-shot, weighted combination of heterogeneous retrievers. Extensive
experiments show that such mixtures are effective and efficient: Despite
totaling just 0.8B parameters, this mixture outperforms every individual
retriever and even larger 7B models by +10.8% and +3.9% on average,
respectively. Further analysis also shows that this mixture framework can help
incorporate specialized non-oracle human information sources as retrievers to
achieve good collaboration, with a 58.9% relative performance improvement over
simulated humans alone.

</details>


### [417] [Revela: Dense Retriever Learning via Language Modeling](https://arxiv.org/abs/2506.16552)
*Fengyu Cai,Tong Chen,Xinran Zhao,Sihao Chen,Hongming Zhang,Sherry Tongshuang Wu,Iryna Gurevych,Heinz Koeppl*

Key words: 自我监督学习, 密集检索器, 语言建模, Revela

TL;DR: 论文介绍了Revela，一种通过语言建模实现自我监督检索器学习的统一且可扩展的训练框架，其在多个基准测试中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 由于专业领域（如代码）中标注数据获取成本高且困难，研究者对自我监督检索器学习兴趣增加，旨在利用语言模型的自我监督学习目标训练检索器。

Method: Revela通过语言建模框架，利用内部注意力机制建模文档间的语义依赖，将检索器优化与语言建模结合。

Result: 在通用和特定领域基准测试中，Revela在NDCG@10指标上分别相对提升了18.3%和14.4%，且性能随模型规模提升。

Conclusion: Revela展示了自我监督检索器学习的有效性和可扩展性，为未来研究提供了方向。

Abstract: Dense retrievers play a vital role in accessing external and specialized
knowledge to augment language models (LMs). Training dense retrievers typically
requires annotated query-document pairs, which are costly and hard to obtain in
specialized domains such as code-motivating growing interest in self-supervised
retriever learning. Since LMs are trained to capture token-level dependencies
through a self-supervised learning objective (i.e., next-token prediction), we
can analogously cast retrieval as learning dependencies among chunks of tokens.
This analogy naturally leads to the question: How can we adapt self-supervised
learning objectives in the spirit of language modeling to train retrievers?
  To answer this question, we introduce Revela, a unified and scalable training
framework for self-supervised retriever learning via language modeling. Revela
models semantic dependencies among documents by conditioning next-token
prediction on both local and cross-document context through an in-batch
attention mechanism. This attention is weighted by retriever-computed
similarity scores, enabling the retriever to be optimized as part of language
modeling. We evaluate Revela on both general-domain (BEIR) and domain-specific
(CoIR) benchmarks across various retriever backbones. At a comparable parameter
scale, Revela outperforms the previous best method with absolute improvements
of 5.2 % (18.3 % relative) and 5.6 % (14.4 % relative) on NDCG@10,
respectively, underscoring its effectiveness. Performance increases with model
size, highlighting both the scalability of our approach and its promise for
self-supervised retriever learning.

</details>


### [418] [GFlowGR: Fine-tuning Generative Recommendation Frameworks with Generative Flow Networks](https://arxiv.org/abs/2506.16114)
*Yejing Wang,Shengyu Zhou,Jinyu Lu,Qidong Liu,Xinhang Li,Wenlin Zhang,Feng Li,Pengjie Wang,Jian Xu,Bo Zheng,Xiangyu Zhao*

Key words: 生成推荐, 微调, GFlowNets, 暴露偏差

TL;DR: 论文提出了一种基于GFlowNets的生成推荐微调框架GFlowGR，以解决现有生成推荐方法中未充分探索的暴露偏差问题，通过集成协同知识和多样生成特性，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 现有生成推荐方法在微调步骤中忽视了未观测到的潜在正样本（暴露偏差问题），作者希望通过改进微调策略来解决这一问题。

Method: 论文将生成推荐视为多步生成任务，提出基于GFlowNets的微调框架GFlowGR，结合协同知识设计自适应轨迹采样器和奖励模型，利用多样生成特性优化推荐结果。

Result: 在两种真实数据集和不同生成推荐骨干模型上的实验表明，GFlowGR能有效缓解暴露偏差问题，表现优于现有方法。

Conclusion: GFlowGR通过创新微调策略和多样生成机制，为生成推荐提供了更鲁棒和高效的解决方案。

Abstract: Generative recommendations (GR), which usually include item tokenizers and
generative Large Language Models (LLMs), have demonstrated remarkable success
across a wide range of scenarios. The majority of existing research efforts
primarily concentrate on developing powerful item tokenizers or advancing LLM
decoding strategies to attain superior performance. However, the critical
fine-tuning step in GR frameworks, which is essential for adapting LLMs to
recommendation data, remains largely unexplored. Current approaches
predominantly rely on either the next-token prediction loss of supervised
fine-tuning (SFT) or recommendationspecific direct preference optimization
(DPO) strategies. Both methods ignore the exploration of possible positive
unobserved samples, which is commonly referred to as the exposure bias problem.
To mitigate this problem, this paper treats the GR as a multi-step generation
task and constructs a GFlowNets-based fine-tuning framework (GFlowGR). The
proposed framework integrates collaborative knowledge from traditional
recommender systems to create an adaptive trajectory sampler and a
comprehensive reward model. Leveraging the diverse generation property of
GFlowNets, along with sampling and heuristic weighting techniques, GFlowGR
emerges as a promising approach to mitigate the exposure bias problem.
Extensive empirical results on two real-world datasets and with two different
GR backbones highlight the effectiveness and robustness of GFlowGR.

</details>


### [419] [A Simple Contrastive Framework Of Item Tokenization For Generative Recommendation](https://arxiv.org/abs/2506.16683)
*Penglong Zhai,Yifang Yuan,Fanyi Di,Jie Li,Yue Liu,Chen Li,Jie Huang,Sicong Wang,Yao Xu,Xin Li*

Key words: 生成式推荐，对比学习，多模态整合，语义令牌化，SimCIT

TL;DR: 该论文提出了一种基于对比学习的无监督深度量化方法SimCIT，用于解决生成式推荐系统中语义令牌化与多模态知识整合的问题。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 生成式检索推荐在大规模系统中面临令牌冗余和高维度的挑战，现有基于重建的方法与推荐任务目标冲突，且多模态信息整合难度大。

Method: 提出SimCIT框架，采用可学习的残差量化模块，结合对比学习对齐多模态信号，实现语义令牌化与知识整合。

Result: 在多个公共数据集和大规模工业数据集上的实验证明了SimCIT在生成式推荐中的有效性。

Conclusion: SimCIT通过对比学习解决了令牌化与多模态整合的难题，提升了生成式推荐的性能。

Abstract: Generative retrieval-based recommendation has emerged as a promising paradigm
aiming at directly generating the identifiers of the target candidates.
However, in large-scale recommendation systems, this approach becomes
increasingly cumbersome due to the redundancy and sheer scale of the token
space. To overcome these limitations, recent research has explored the use of
semantic tokens as an alternative to ID tokens, which typically leveraged
reconstruction-based strategies, like RQ-VAE, to quantize content embeddings
and significantly reduce the embedding size. However, reconstructive
quantization aims for the precise reconstruction of each item embedding
independently, which conflicts with the goal of generative retrieval tasks
focusing more on differentiating among items. Moreover, multi-modal side
information of items, such as descriptive text and images, geographical
knowledge in location-based recommendation services, has been shown to be
effective in improving recommendations by providing richer contexts for
interactions. Nevertheless, effectively integrating such complementary
knowledge into existing generative recommendation frameworks remains
challenging. To overcome these challenges, we propose a novel unsupervised deep
quantization exclusively based on contrastive learning, named SimCIT (a Simple
Contrastive Item Tokenization framework). Specifically, different from existing
reconstruction-based strategies, SimCIT propose to use a learnable residual
quantization module to align with the signals from different modalities of the
items, which combines multi-modal knowledge alignment and semantic tokenization
in a mutually beneficial contrastive learning framework. Extensive experiments
across public datasets and a large-scale industrial dataset from various
domains demonstrate SimCIT's effectiveness in LLM-based generative
recommendation.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [420] [Multi-Armed Bandits With Machine Learning-Generated Surrogate Rewards](https://arxiv.org/abs/2506.16658)
*Wenlong Ji,Yihan Pan,Ruihao Zhu,Lihua Lei*

Key words: 多臂老虎机, 机器学习辅助, 替代奖励, 离线数据, 在线学习

TL;DR: 论文提出了一种结合预训练机器学习模型的MAB新方法MLA-UCB，通过利用离线辅助数据提升在线决策效率。

<details>
  <summary>Details</summary>

Main category: math.ST

Motivation: 传统MAB算法仅依赖在线数据，数据稀缺性限制了性能。实际场景中常存在丰富的离线辅助数据（如历史用户特征），如何有效利用这些数据提升MAB性能是研究动机。

Method: 引入预训练ML模型将辅助数据和历史数据转化为“替代奖励”，并提出MLA-UCB算法。该算法适用于任意奖励预测模型和辅助数据形式，无需已知真实奖励与替代奖励的协方差矩阵。

Result: 在真实和替代奖励联合高斯的假设下，MLA-UCB能显著降低累积遗憾。数值实验表明，即使在离线数据量适中且相关性一般时，效率仍有显著提升。

Conclusion: MLA-UCB通过融合离线数据与在线学习，解决了传统MAB数据匮乏问题，为实际应用提供了高效解决方案。

Abstract: Multi-armed bandit (MAB) is a widely adopted framework for sequential
decision-making under uncertainty. Traditional bandit algorithms rely solely on
online data, which tends to be scarce as it must be gathered during the online
phase when the arms are actively pulled. However, in many practical settings,
rich auxiliary data, such as covariates of past users, is available prior to
deploying any arms. We introduce a new setting for MAB where pre-trained
machine learning (ML) models are applied to convert side information and
historical data into \emph{surrogate rewards}. A prominent feature of this
setting is that the surrogate rewards may exhibit substantial bias, as true
reward data is typically unavailable in the offline phase, forcing ML
predictions to heavily rely on extrapolation. To address the issue, we propose
the Machine Learning-Assisted Upper Confidence Bound (MLA-UCB) algorithm, which
can be applied to any reward prediction model and any form of auxiliary data.
When the predicted and true rewards are jointly Gaussian, it provably improves
the cumulative regret, provided that the correlation is non-zero -- even in
cases where the mean surrogate reward completely misaligns with the true mean
rewards. Notably, our method requires no prior knowledge of the covariance
matrix between true and surrogate rewards. We compare MLA-UCB with the standard
UCB on a range of numerical studies and show a sizable efficiency gain even
when the size of the offline data and the correlation between predicted and
true rewards are moderate.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [421] [Graphics4Science: Computer Graphics for Scientific Impacts](https://arxiv.org/abs/2506.15786)
*Peter Yichen Chen,Minghao Guo,Hanspeter Pfister,Ming Lin,William Freeman,Qixing Huang,Han-Wei Shen,Wojciech Matusik*

Key words: 计算机图形学,科学建模,几何推理,物理建模,数据稀缺

TL;DR: 本文探讨了计算机图形学与科学之间的深厚关系,突出其历史和当前贡献,并试图将图形学重新定义为科学的建模语言。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 展示计算机图形学如何从3D可视化发展为解决科学挑战的工具,强调其在数据稀缺场景中的价值。

Method: 通过几何推理和物理建模等核心方法,填补图形学与科学之间的词汇差距。

Result: 提出Graphics4Science课程,旨在促进图形学社区参与科学问题。

Conclusion: 图形学可以作为科学的建模语言,推动未来的科学发现。

Abstract: Computer graphics, often associated with films, games, and visual effects,
has long been a powerful tool for addressing scientific challenges--from its
origins in 3D visualization for medical imaging to its role in modern
computational modeling and simulation. This course explores the deep and
evolving relationship between computer graphics and science, highlighting past
achievements, ongoing contributions, and open questions that remain. We show
how core methods, such as geometric reasoning and physical modeling, provide
inductive biases that help address challenges in both fields, especially in
data-scarce settings. To that end, we aim to reframe graphics as a modeling
language for science by bridging vocabulary gaps between the two communities.
Designed for both newcomers and experts, Graphics4Science invites the graphics
community to engage with science, tackle high-impact problems where graphics
expertise can make a difference, and contribute to the future of scientific
discovery. Additional details are available on the course website:
https://graphics4science.github.io

</details>


### [422] [VEIGAR: View-consistent Explicit Inpainting and Geometry Alignment for 3D object Removal](https://arxiv.org/abs/2506.15821)
*Pham Khai Nguyen Do,Bao Nguyen Tran,Nam Nguyen,Duc Dung Nguyen*

Key words: Novel View Synthesis, 3D generation, cross-view consistency, VEIGAR, depth loss

TL;DR: VEIGAR是一种高效框架，无需初始3D重建阶段即可超越现有方法，提升重建质量和跨视角一致性。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 现有的NVS和3D生成方法通常依赖初始3D重建阶段，计算成本高且结果质量不足，亟需更高效的解决方案。

Method: VEIGAR采用轻量基础模型在像素空间对齐先验，并引入基于尺度不变深度损失的新型监督策略。

Result: 实验表明，VEIGAR在重建质量、跨视角一致性上达到新SOTA，训练时间减少三倍。

Conclusion: VEIGAR在效率和效果上实现了优越平衡，为3D编辑任务提供了高效新方法。

Abstract: Recent advances in Novel View Synthesis (NVS) and 3D generation have
significantly improved editing tasks, with a primary emphasis on maintaining
cross-view consistency throughout the generative process. Contemporary methods
typically address this challenge using a dual-strategy framework: performing
consistent 2D inpainting across all views guided by embedded priors either
explicitly in pixel space or implicitly in latent space; and conducting 3D
reconstruction with additional consistency guidance. Previous strategies, in
particular, often require an initial 3D reconstruction phase to establish
geometric structure, introducing considerable computational overhead. Even with
the added cost, the resulting reconstruction quality often remains suboptimal.
In this paper, we present VEIGAR, a computationally efficient framework that
outperforms existing methods without relying on an initial reconstruction
phase. VEIGAR leverages a lightweight foundation model to reliably align priors
explicitly in the pixel space. In addition, we introduce a novel supervision
strategy based on scale-invariant depth loss, which removes the need for
traditional scale-and-shift operations in monocular depth regularization.
Through extensive experimentation, VEIGAR establishes a new state-of-the-art
benchmark in reconstruction quality and cross-view consistency, while achieving
a threefold reduction in training time compared to the fastest existing method,
highlighting its superior balance of efficiency and effectiveness.

</details>


### [423] [FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models](https://arxiv.org/abs/2506.16627)
*Haotian Yin,Aleksander Plocharski,Michal Jan Wlodarczyk,Mikolaj Kida,Przemyslaw Musialski*

Key words: 神经SDF, 曲率代理, 有限差分, 自动微分

TL;DR: 提出了两种高效计算曲率代理的方法，替代传统高斯曲率惩罚，降低计算成本。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 神经符号距离场（SDF）需要高效且低成本的曲率计算方法，以支持工程级形状重建。

Method: 设计了有限差分代理和自动微分代理两种方法，仅需计算混合二阶导数，避免完整Hessian矩阵。

Result: 在ABC基准测试中，性能与基于Hessian的方法相当或更优，GPU内存和计算时间减少一半。

Conclusion: 该方法为工程级SDF学习提供了可扩展且实用的路径。

Abstract: Neural signed-distance fields (SDFs) have become a versatile backbone for
geometric learning, yet enforcing developable, CAD-style behavior still hinges
on Gaussian curvature penalties that require full Hessian evaluation and
second-order automatic differentiation, both of which are costly in memory and
runtime. We present a curvature proxy that regularizes only the mixed
second-order term (Weingarten term), allowing the two principal curvatures to
adapt freely to data while suppressing unwanted warp. Two complementary
instantiations realize this idea: (i) a finite-difference proxy that replaces
each Hessian entry with four forward SDF evaluations and a single first-order
gradient, and (ii) an autodiff proxy that computes the same mixed derivative
via one Hessian-vector product, sidestepping explicit full Hessian assembly and
remaining faster in practice. Both variants converge to the exact mixed second
derivative, thus preserving the intended geometric bias without incurring full
second-order graphs. On the ABC benchmarks, the proxies match or exceed the
reconstruction fidelity of Hessian-based baselines while reducing GPU memory
use and wall-clock time by a factor of two. Because the method is drop-in and
framework-agnostic, it opens a practical path toward scalable, curvature-aware
SDF learning for engineering-grade shape reconstruction.

</details>


### [424] [Beyond Blur: A Fluid Perspective on Generative Diffusion Models](https://arxiv.org/abs/2506.16827)
*Grzegorz Gruszczynski,Michal Jan Wlodarczyk,Jakub J Meixner,Przemyslaw Musialski*

Key words: PDE, 图像生成, 对流-扩散过程, 深度学习, Lattice Boltzmann

TL;DR: 提出了一种基于对流-扩散过程的PDE驱动图像腐蚀方法，通过物理启发的PDE实现图像生成，结合GPU加速求解器和湍流模拟，神经网络学习逆操作，提升了图像多样性和质量。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 结合流体动力学和深度学习，提出一种更通用的PDE驱动图像生成方法，提升生成图像的多样性和质量。

Method: 通过基于对流-扩散过程的PDE建模图像腐蚀，采用GPU加速的Lattice Boltzmann求解器实现快速计算，生成随机速度场模拟湍流，神经网络学习逆操作。

Result: 该方法能生成更高质量和多样性的图像，同时保持色彩一致性，并推广了现有PDE方法的局限性。

Conclusion: 研究通过物理启发的PDE和深度学习的结合，提供了一种新颖的图像生成框架，为基于扩散的图像合成提供了新视角。

Abstract: We propose a novel PDE-driven corruption process for generative image
synthesis based on advection-diffusion processes which generalizes existing
PDE-based approaches. Our forward pass formulates image corruption via a
physically motivated PDE that couples directional advection with isotropic
diffusion and Gaussian noise, controlled by dimensionless numbers (Peclet,
Fourier). We implement this PDE numerically through a GPU-accelerated custom
Lattice Boltzmann solver for fast evaluation. To induce realistic turbulence,
we generate stochastic velocity fields that introduce coherent motion and
capture multi-scale mixing. In the generative process, a neural network learns
to reverse the advection-diffusion operator thus constituting a novel
generative model. We discuss how previous methods emerge as specific cases of
our operator, demonstrating that our framework generalizes prior PDE-based
corruption techniques. We illustrate how advection improves the diversity and
quality of the generated images while keeping the overall color palette
unaffected. This work bridges fluid dynamics, dimensionless PDE theory, and
deep generative modeling, offering a fresh perspective on physically informed
image corruption processes for diffusion-based synthesis.

</details>


### [425] [DreamCube: 3D Panorama Generation via Multi-plane Synchronization](https://arxiv.org/abs/2506.17206)
*Yukun Huang,Yanning Zhou,Jianan Wang,Kaiyi Huang,Xihui Liu*

Key words: 3D全景合成，多平面同步，RGB-D扩散模型，图像生成，深度估计

TL;DR: 论文提出了一种名为DreamCube的多平面RGB-D扩散模型，通过多平面同步技术将2D基础模型的算子扩展到全景领域，实现了高质量、多样化的3D全景合成。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 现有的3D全景合成方法因数据稀缺而依赖2D基础模型的图像先验，但3D全景与2D单视图的不兼容性限制了其效果。研究旨在解决这些问题。

Method: 采用多平面同步技术将2D基础模型的算子扩展到全景领域，并设计DreamCube模型，最大化利用2D先验以实现多样外观和准确几何。

Result: 实验表明，方法在全景图像生成、全景深度估计和3D场景生成中表现优异。

Conclusion: 通过多平面同步和DreamCube模型，成功将2D基础模型的能力扩展到3D全景领域。

Abstract: 3D panorama synthesis is a promising yet challenging task that demands
high-quality and diverse visual appearance and geometry of the generated
omnidirectional content. Existing methods leverage rich image priors from
pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic
data, but the incompatibility between 3D panoramas and 2D single views limits
their effectiveness. In this work, we demonstrate that by applying multi-plane
synchronization to the operators from 2D foundation models, their capabilities
can be seamlessly extended to the omnidirectional domain. Based on this design,
we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D
panorama generation, which maximizes the reuse of 2D foundation model priors to
achieve diverse appearances and accurate geometry while maintaining multi-view
consistency. Extensive experiments demonstrate the effectiveness of our
approach in panoramic image generation, panoramic depth estimation, and 3D
scene generation.

</details>
