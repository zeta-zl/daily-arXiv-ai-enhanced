{"id": "2507.00002", "pdf": "https://arxiv.org/pdf/2507.00002", "abs": "https://arxiv.org/abs/2507.00002", "authors": ["Christopher James Augeri"], "title": "Hypertokens: Holographic Associative Memory in Tokenized LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "preprint as accepted to https://qnlp.ai/ - Quantum AI and NLP\n  Conference 2025", "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but suffer from\napparent precision loss, reframed here as information spreading. This reframing\nshifts the problem from computational precision to an information-theoretic\ncommunication issue. We address the K:V and V:K memory problem in LLMs by\nintroducing HDRAM (Holographically Defined Random Access Memory), a symbolic\nmemory framework treating transformer latent space as a spread-spectrum\nchannel. Built upon hypertokens, structured symbolic codes integrating\nclassical error-correcting codes (ECC), holographic computing, and\nquantum-inspired search, HDRAM recovers distributed information through\nprincipled despreading. These phase-coherent memory addresses enable efficient\nkey-value operations and Grover-style search in latent space. By combining ECC\ngrammar with compressed sensing and Krylov subspace alignment, HDRAM\nsignificantly improves associative retrieval without architectural changes,\ndemonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can\nfortify transformer architectures.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "keywords": "Error", "conclusion": "Error"}}
{"id": "2507.00003", "pdf": "https://arxiv.org/pdf/2507.00003", "abs": "https://arxiv.org/abs/2507.00003", "authors": ["Eyhab Al-Masri"], "title": "Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.NI"], "comment": null, "summary": "This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework\nfor interpretable intrusion detection in IoT environments. By integrating\nRandom Forest, XGBoost, and Logistic Regression with neutrosophic logic, the\nsystem decomposes prediction confidence into truth (T), falsity (F), and\nindeterminacy (I) components, enabling uncertainty quantification and\nabstention. Predictions with high indeterminacy are flagged for review using\nboth global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD\ndataset, NeutroSENSE achieved 97% accuracy, while demonstrating that\nmisclassified samples exhibit significantly higher indeterminacy (I = 0.62)\nthan correct ones (I = 0.24). The use of indeterminacy as a proxy for\nuncertainty enables informed abstention and targeted review-particularly\nvaluable in edge deployments. Figures and tables validate the correlation\nbetween I-scores and error likelihood, supporting more trustworthy,\nhuman-in-the-loop AI decisions. This work shows that neutrosophic logic\nenhances both accuracy and explainability, providing a practical foundation for\ntrust-aware AI in edge and fog-based IoT security systems.", "AI": {"tldr": "NeutroSENSE\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u968f\u673a\u68ee\u6797\u3001XGBoost\u548c\u903b\u8f91\u56de\u5f52\u7684\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4e2d\u6027\u903b\u8f91\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728IoT\u5165\u4fb5\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4e3aIoT\u73af\u5883\u63d0\u4f9b\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u9ad8\u51c6\u786e\u7387\u7684\u5165\u4fb5\u68c0\u6d4b\u65b9\u6cd5\uff0c\u540c\u65f6\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4ee5\u652f\u6301\u4eba\u5de5\u5ba1\u67e5\u3002", "method": "\u96c6\u6210\u968f\u673a\u68ee\u6797\u3001XGBoost\u548c\u903b\u8f91\u56de\u5f52\uff0c\u7ed3\u5408\u4e2d\u6027\u903b\u8f91\u5206\u89e3\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4e3a\u771f\u503c\u3001\u5047\u503c\u548c\u4e0d\u786e\u5b9a\u6027\u6210\u5206\uff0c\u5e76\u8bbe\u7f6e\u5168\u5c40\u548c\u7c7b\u522b\u7279\u5b9a\u9608\u503c\u8fdb\u884c\u9884\u6d4b\u5ba1\u67e5\u3002", "result": "\u5728IoT-CAD\u6570\u636e\u96c6\u4e0a\u8fbe\u523097%\u51c6\u786e\u7387\uff0c\u4e14\u9519\u8bef\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u663e\u8457\u9ad8\u4e8e\u6b63\u786e\u9884\u6d4b\uff080.62 vs. 0.24\uff09\u3002", "conclusion": "\u4e2d\u6027\u903b\u8f91\u63d0\u5347\u4e86\u5165\u4fb5\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u53ef\u4fe1AI\u51b3\u7b56\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\u3002", "keywords": "NeutroSENSE, \u5165\u4fb5\u68c0\u6d4b, \u4e2d\u6027\u903b\u8f91, IoT, \u4e0d\u786e\u5b9a\u6027\u91cf\u5316"}}
{"id": "2507.00004", "pdf": "https://arxiv.org/pdf/2507.00004", "abs": "https://arxiv.org/abs/2507.00004", "authors": ["Austin R. Ellis-Mohr", "Anuj K. Nayak", "Lav R. Varshney"], "title": "A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.PF"], "comment": null, "summary": "Large language models (LLMs) demand considerable computational, energy, and\nfinancial resources during both training and deployment. While scaling laws for\ntraining have guided much of the field's recent progress, inference costs now\nrepresent a significant and growing component of the overall resource burden,\nparticularly for reasoning-focused models. Existing characterizations of\ncompute-optimality that consider model size, dataset size, and inference tokens\nin isolation or in fixed combinations risk overlooking more efficient operating\npoints. We introduce directed stochastic skill search (DS3), a general\nframework that represents inference as stochastic traversal over a learned\nskill graph. From a simplified yet expressive instantiation, we derive\nclosed-form expressions for task success and compute cost across a wide range\nof inference strategies -- including chain-of-thought (CoT) and tree-of-thought\n(ToT) -- enabling comparative analysis as a function of task difficulty and\nmodel capability. To that end, we extend a prior first-principles tripartite\ngraph framework of LLM training to incorporate inference, and separately bridge\nDS3 with empirical methods that characterize LLM scaling behavior. We\ntheoretically recover empirically observed patterns, including: linear accuracy\nscaling with logarithmic compute; variation in preferred inference strategies\nas a function of task difficulty and model capability; emergent behavior\nelicited by reasoning even when performance plateaus under parameter scaling;\nand both best-of-N (BoN) and majority voting behavior captured within a unified\nanalytical framework. By explicitly characterizing training-inference\ninterdependencies, our framework deepens theoretical understanding and supports\nprincipled algorithmic design and resource allocation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86DS3\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bad\u7ec3\u4e0e\u63a8\u7406\u7684\u8d44\u6e90\u5206\u914d\uff0c\u901a\u8fc7\u6280\u80fd\u56fe\u968f\u673a\u904d\u5386\u7684\u65b9\u5f0f\u5206\u6790\u4efb\u52a1\u6210\u529f\u7387\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u63ed\u793a\u4e86\u4efb\u52a1\u96be\u5ea6\u4e0e\u6a21\u578b\u80fd\u529b\u5bf9\u63a8\u7406\u7b56\u7565\u7684\u5f71\u54cd\u3002", "motivation": "LLMs\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u6d88\u8017\u5927\u91cf\u8d44\u6e90\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u8ba1\u7b97\u6700\u4f18\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8d44\u6e90\u5206\u914d\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86DS3\u6846\u67b6\uff0c\u901a\u8fc7\u6280\u80fd\u56fe\u7684\u968f\u673a\u904d\u5386\u5206\u6790\u63a8\u7406\u7b56\u7565\uff08\u5982CoT\u548cToT\uff09\uff0c\u5e76\u7ed3\u5408\u8bad\u7ec3\u4e0e\u63a8\u7406\u7684\u4e09\u65b9\u56fe\u6846\u67b6\u8fdb\u884c\u7406\u8bba\u63a8\u5bfc\u3002", "result": "\u7406\u8bba\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5b9e\u8bc1\u89c2\u6d4b\u73b0\u8c61\uff0c\u5982\u7ebf\u6027\u7cbe\u5ea6\u4e0e\u5bf9\u6570\u8ba1\u7b97\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u63a8\u7406\u7b56\u7565\u968f\u4efb\u52a1\u96be\u5ea6\u548c\u6a21\u578b\u80fd\u529b\u7684\u53d8\u5316\u3002", "conclusion": "\u6846\u67b6\u6df1\u5316\u4e86\u5bf9LLM\u8bad\u7ec3\u4e0e\u63a8\u7406\u5173\u7cfb\u7684\u7406\u89e3\uff0c\u4e3a\u7b97\u6cd5\u8bbe\u8ba1\u548c\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3001\u63a8\u7406\u4f18\u5316\u3001\u6280\u80fd\u56fe\u3001\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u3001\u6811\u72b6\u601d\u7ef4\uff08ToT\uff09"}}
{"id": "2507.00152", "pdf": "https://arxiv.org/pdf/2507.00152", "abs": "https://arxiv.org/abs/2507.00152", "authors": ["Ekaterina Borisova", "Fabio Barth", "Nils Feldhus", "Raia Abu Ahmad", "Malte Ostendorff", "Pedro Ortiz Suarez", "Georg Rehm", "Sebastian M\u00f6ller"], "title": "Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data", "categories": ["cs.CL"], "comment": "TRL@ACL 2025, camera-ready version", "summary": "Tables are among the most widely used tools for representing structured data\nin research, business, medicine, and education. Although LLMs demonstrate\nstrong performance in downstream tasks, their efficiency in processing tabular\ndata remains underexplored. In this paper, we investigate the effectiveness of\nboth text-based and multimodal LLMs on table understanding tasks through a\ncross-domain and cross-modality evaluation. Specifically, we compare their\nperformance on tables from scientific vs. non-scientific contexts and examine\ntheir robustness on tables represented as images vs. text. Additionally, we\nconduct an interpretability analysis to measure context usage and input\nrelevance. We also introduce the TableEval benchmark, comprising 3017 tables\nfrom scholarly publications, Wikipedia, and financial reports, where each table\nis provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.\nOur findings indicate that while LLMs maintain robustness across table\nmodalities, they face significant challenges when processing scientific tables.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "keywords": "Error", "conclusion": "Error"}}
