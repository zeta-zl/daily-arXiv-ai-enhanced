<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 17]
- [cs.LG](#cs.LG) [Total: 14]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.SD](#cs.SD) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Towards Probabilistic Question Answering Over Tabular Data](https://arxiv.org/abs/2506.20747)
*Chen Shen,Sajjadur Rahman,Estevam Hruschka*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Current approaches for question answering (QA) over tabular data, such as
NL2SQL systems, perform well for factual questions where answers are directly
retrieved from tables. However, they fall short on probabilistic questions
requiring reasoning under uncertainty. In this paper, we introduce a new
benchmark LUCARIO and a framework for probabilistic QA over large tabular data.
Our method induces Bayesian Networks from tables, translates natural language
queries into probabilistic queries, and uses large language models (LLMs) to
generate final answers. Empirical results demonstrate significant improvements
over baselines, highlighting the benefits of hybrid symbolic-neural reasoning.

</details>


### [2] [Multi-lingual Functional Evaluation for Large Language Models](https://arxiv.org/abs/2506.20793)
*Victor Ojewale,Inioluwa Deborah Raji,Suresh Venkatasubramanian*

Key words: 多语言基准测试、功能性评估、大语言模型、跨语言表现、鲁棒性

TL;DR: 该论文提出了一种新的多语言功能基准测试方法（CL-GSM Symbolic和CL-IFEval），通过翻译现有英语基准测试模板到多语言（法语、西班牙语、印地语、阿拉伯语和约鲁巴语），揭示了静态多语言基准测试在实际性能评估中的局限性，并分析了模型在不同语言中的表现和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的静态多语言基准测试（如Belebele、M-MMLU和M-GSM）无法充分评估大语言模型在多语言环境中的实际性能和鲁棒性。因此，作者创建了功能性的多语言基准测试，填补了这一空白。

Method: 通过将现有的英语功能基准测试模板翻译成五种其他语言（法语、西班牙语、印地语、阿拉伯语和约鲁巴语），创建了CL-GSM Symbolic和CL-IFEval两个新的多语言功能基准测试。

Result: 研究发现，某些静态多语言基准测试（如M-GSM和Belebele）与功能性性能评估之间的性能差距较大（下降15%-24%），而M-MMLU与CL-IFEval之间的性能差距较小（0.5%-3%）。此外，模型在不同语言中的鲁棒性差异显著，阿拉伯语和英语表现最稳定。

Conclusion: 功能性的多语言基准测试能更准确地评估模型的实际性能，而静态基准测试可能掩盖性能差距。模型在不同语言中的鲁棒性存在显著差异。

Abstract: Multi-lingual competence in large language models is often evaluated via
static data benchmarks such as Belebele, M-MMLU and M-GSM. However, these
evaluations often fail to provide an adequate understanding of the practical
performance and robustness of models across multi-lingual settings. In
response, we create multi-lingual functional benchmarks -- Cross-Lingual Grade
School Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following
Eval (CL-IFEval)-- by translating existing functional benchmark templates from
English to five additional languages that span the range of resources available
for NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that
some static multi-lingual benchmarks capture functional performance much more
closely than others (i.e. across models, there is a 24%, 17% and 18% decrease
in performance between M-GSM and CL-GSM Symbolic in English, French and Spanish
respectively; similarly there's a 15 - 24% performance drop across languages
between Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between
M-MMLU and CL-IFEval). Similarly, we find that model robustness across
languages varies significantly, with certain languages (eg. Arabic, English)
being the most consistently well performing across evaluation iterations.

</details>


### [3] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [4] [MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](https://arxiv.org/abs/2506.20821)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, quantized open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.

</details>


### [5] [Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes](https://arxiv.org/abs/2506.20822)
*Quintin Myers,Yanjun Gao*

Key words: 大型语言模型,暴力内容,道德模糊,人口统计学偏见

TL;DR: 该论文研究了大型语言模型（LLMs）在检测和处理暴力内容时对道德模糊场景的推理能力，发现其表面生成与内部暴力倾向存在差异，并存在跨人口统计学的偏见。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: LLMs被广泛应用于检测和应对在线暴力内容，但其在现实道德模糊场景中的推理能力尚不明确，本研究旨在填补这一空白。

Method: 通过验证的社会科学工具（VBVQ）和基于身份的提示（如种族、年龄、地理），在零样本设置下评估六个LLMs。

Result: LLMs的表层文本生成与内部暴力倾向不一致，且其暴力倾向在不同人口统计学中存在偏见，与社会科学的发现相矛盾。

Conclusion: LLMs在暴力内容处理中存在潜在偏见和推理缺陷，需进一步改进和评估。

Abstract: Large language models (LLMs) are increasingly proposed for detecting and
responding to violent content online, yet their ability to reason about morally
ambiguous, real-world scenarios remains underexamined. We present the first
study to evaluate LLMs using a validated social science instrument designed to
measure human response to everyday conflict, namely the Violent Behavior
Vignette Questionnaire (VBVQ). To assess potential bias, we introduce
persona-based prompting that varies race, age, and geographic identity within
the United States. Six LLMs developed across different geopolitical and
organizational contexts are evaluated under a unified zero-shot setting. Our
study reveals two key findings: (1) LLMs surface-level text generation often
diverges from their internal preference for violent responses; (2) their
violent tendencies vary across demographics, frequently contradicting
established findings in criminology, social science, and psychology.

</details>


### [6] [Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine](https://arxiv.org/abs/2506.20876)
*Sebastian Joseph,Lily Chen,Barry Wei,Michael Mackert,Iain J. Marshall,Paul Pu Liang,Ramez Kouzy,Byron C. Wallace,Junyi Jessy Li*

Key words: 医疗事实核查、自动化系统、证据医学、交互沟通

TL;DR: 论文研究了医疗领域自动事实核查系统的挑战，指出当前系统未普及的原因，并提出应将事实核查视为交互式沟通问题而非端到端流程。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 医疗决策的高风险性和医学文献的复杂性导致对自动事实核查系统的需求增长，但现有系统仍未广泛应用，研究旨在探索其原因。

Method: 通过研究临床专家如何验证社交媒体上的医疗声明并综合医学证据，揭示医疗事实核查的难点。

Result: 发现医疗事实核查面临的挑战包括：无法将实际声明与临床试验证据关联、声明模糊性及意图不匹配、主观真实性标签问题。

Conclusion: 事实核查应被视为交互式沟通问题，而非端到端流程。

Abstract: Technological progress has led to concrete advancements in tasks that were
regarded as challenging, such as automatic fact-checking. Interest in adopting
these systems for public health and medicine has grown due to the high-stakes
nature of medical decisions and challenges in critically appraising a vast and
diverse medical literature. Evidence-based medicine connects to every
individual, and yet the nature of it is highly technical, rendering the medical
literacy of majority users inadequate to sufficiently navigate the domain. Such
problems with medical communication ripens the ground for end-to-end
fact-checking agents: check a claim against current medical literature and
return with an evidence-backed verdict. And yet, such systems remain largely
unused. To understand this, we present the first study examining how clinical
experts verify real claims from social media by synthesizing medical evidence.
In searching for this upper-bound, we reveal fundamental challenges in
end-to-end fact-checking when applied to medicine: Difficulties connecting
claims in the wild to scientific evidence in the form of clinical trials;
ambiguities in underspecified claims mixed with mismatched intentions; and
inherently subjective veracity labels. We argue that fact-checking should be
approached and evaluated as an interactive communication problem, rather than
an end-to-end process.

</details>


### [7] [Optimising Language Models for Downstream Tasks: A Post-Training Perspective](https://arxiv.org/abs/2506.20917)
*Zhengyan Shi*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Language models (LMs) have demonstrated remarkable capabilities in NLP, yet
adapting them efficiently and robustly to specific tasks remains challenging.
As their scale and complexity grow, fine-tuning LMs on labelled data often
underutilizes available unlabelled data, leads to overfitting on small
task-specific sets, and imposes significant computational costs. These
limitations hamper their application to the open-ended landscape of real-world
language tasks.
  This thesis proposes a series of methods to better adapt LMs to downstream
applications. First, we explore strategies for extracting task-relevant
knowledge from unlabelled data, introducing a novel continued pre-training
technique that outperforms state-of-the-art semi-supervised approaches. Next,
we present a parameter-efficient fine-tuning method that substantially reduces
memory and compute costs while maintaining competitive performance. We also
introduce improved supervised fine-tuning methods that enable LMs to better
follow instructions, especially when labelled data is scarce, enhancing their
performance across a range of NLP tasks, including open-ended generation.
Finally, we develop new evaluation methods and benchmarks, such as multi-hop
spatial reasoning tasks, to assess LM capabilities and adaptation more
comprehensively.
  Through extensive empirical studies across diverse NLP tasks, our results
demonstrate that these approaches substantially improve LM robustness,
efficiency, and generalization, making them more adaptable to a broad range of
applications. These advances mark a significant step towards more robust and
efficient LMs, bringing us closer to the goal of artificial general
intelligence.

</details>


### [8] [FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language](https://arxiv.org/abs/2506.20920)
*Guilherme Penedo,Hynek Kydlíček,Vinko Sabolčec,Bettina Messmer,Negar Foroutan,Amir Hossein Kargaran,Colin Raffel,Martin Jaggi,Leandro Von Werra,Thomas Wolf*

Key words: 多语言预训练,数据管道,FineWeb2,语言模型

TL;DR: 本文提出了一种新的多语言预训练数据集FineWeb2，通过自动化的数据管道支持多种语言，提升了模型的性能，并引入了基于重复计数和质量的数据重新平衡方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 预训练大规模语言模型需要大量高质量的多语言数据，但目前多语言数据的过滤和去重仍面临挑战。

Method: 设计了基于FineWeb的自动化数据管道，支持多种语言，并通过实验优化设计选择；引入了数据重新平衡方法。

Result: FineWeb2数据集（20TB，50亿文档）在多语言任务中表现优于之前的数据集。

Conclusion: 该数据管道和数据集为多语言模型训练提供了高效解决方案，并公开了相关代码。

Abstract: Pre-training state-of-the-art large language models (LLMs) requires vast
amounts of clean and diverse text data. While the open development of large
high-quality English pre-training datasets has seen substantial recent
progress, training performant multilingual LLMs remains a challenge, in large
part due to the inherent difficulty of tailoring filtering and deduplication
pipelines to a large number of languages. In this work, we introduce a new
pre-training dataset curation pipeline based on FineWeb that can be
automatically adapted to support any language. We extensively ablate our
pipeline design choices on a set of nine diverse languages, guided by a set of
meaningful and informative evaluation tasks that were chosen through a novel
selection process based on measurable criteria. Ultimately, we show that our
pipeline can be used to create non-English corpora that produce more performant
models than prior datasets. We additionally introduce a straightforward and
principled approach to rebalance datasets that takes into consideration both
duplication count and quality, providing an additional performance uplift.
Finally, we scale our pipeline to over 1000 languages using almost 100 Common
Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)
multilingual dataset which we release along with our pipeline, training, and
evaluation codebases.

</details>


### [9] [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
*Xinping Zhao,Xinshuo Hu,Zifei Shan,Shouzheng Huang,Yao Zhou,Zetian Sun,Zhenyu Liu,Dongfang Li,Xinyuan Wei,Qian Chen,Youcheng Pan,Yang Xiang,Meishan Zhang,Haofen Wang,Jun Yu,Baotian Hu,Min Zhang*

Key words: 文本嵌入, Transformer, 多阶段训练, MTEB

TL;DR: 提出KaLM-Embedding-V2，一种多功能且紧凑的嵌入模型，通过改进的训练技术和数据在通用文本嵌入任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决现有嵌入模型在性能和通用性上的不足，提出更高效的训练方法和数据策略。

Method: 1. 采用双向Transformer和均值池化生成固定长度嵌入；2. 多阶段训练流程；3. 引入焦点式重加权和在线难负样本混合策略。

Result: 在MTEB中显著优于同类模型，甚至与大模型竞争。

Conclusion: KaLM-Embedding-V2为紧凑型嵌入模型设定了新标准。

Abstract: In this paper, we propose KaLM-Embedding-V2, a versatile and compact
embedding model, which achieves impressive performance in general-purpose text
embedding tasks by leveraging superior training techniques and data. Our key
innovations include: (1) To better align the architecture with representation
learning, we remove the causal attention mask and adopt a fully bidirectional
transformer with simple yet effective mean-pooling to produce fixed-length
embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on
large-scale weakly supervised open-source corpora; (ii) fine-tuning on
high-quality retrieval and non-retrieval datasets; and (iii) model-soup
parameter averaging for robust generalization. Besides, we introduce a
focal-style reweighting mechanism that concentrates learning on difficult
samples and an online hard-negative mixing strategy to continuously enrich hard
negatives without expensive offline mining; (3) We collect over 20 categories
of data for pre-training and 100 categories of data for fine-tuning, to boost
both the performance and generalization of the embedding model. Extensive
evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English
show that our model significantly outperforms others of comparable size, and
competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new
standard for a versatile and compact embedding model with less than 1B
parameters.

</details>


### [10] [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
*Eric Zhang,Leshem Choshen,Jacob Andreas*

Key words: 语言模型, 梯度下降, 元学习, 提示, 泛化

TL;DR: 论文探讨了通过梯度下降微调语言模型（LM）以模拟提示效果的方法，证明了在适当初始化下，梯度下降可以提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在验证是否可以通过微调使语言模型模拟提示的效果，以结合提示的强泛化能力和参数更新的低成本优势。

Method: 采用基于梯度的元学习方法，利用模型自身的提示预测作为目标，无需真实标签，通过梯度下降训练模拟提示效果。

Result: 实验显示梯度下降能部分或完全恢复提示模型的性能，尤其在“反转诅咒”任务和文本问答任务中表现显著。

Conclusion: 研究结果表明梯度下降在适当条件下具有强大表达能力，为长上下文建模和梯度学习泛化能力提供了新思路。

Abstract: There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.

</details>


### [11] [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
*Adithya Chittem,Aishna Shrivastava,Sai Tarun Pendela,Jagat Sesh Challa,Dhruv Kumar*

Key words: LLM, 16PF, SAC, personality modelling, human-like interaction

TL;DR: 论文提出了一种改进的LLM个性建模方法，通过扩展16PF模型和引入SAC框架，实现了对16种特质的表达强度控制。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有LLM个性模型受限于Big Five框架的粗粒度和缺乏强度控制机制。

Method: 扩展MPI至16PF模型，开发SAC框架，通过形容词锚定和五维强度因素动态控制特质强度。

Result: 连续强度谱比二元切换更一致可控，特质强度变化对相关特质有系统性影响。

Conclusion: 方法为医疗、教育等领域提供了更精细的人机交互途径。

Abstract: Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.

</details>


### [12] [Large Language Models Acing Chartered Accountancy](https://arxiv.org/abs/2506.21031)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Mohammad Adnan,Sakshi Deo,Ali Imam Abidi,Keshav Gupta*

Key words: 大型语言模型, 金融知识, CA-Ben基准, 印度特许会计师, 法律推理

TL;DR: 论文提出了CA-Ben基准，用于评估大型语言模型在金融、法律和定量推理方面的能力，并在印度财务背景下测试了多种模型的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在填补大型语言模型在特定金融领域知识应用方面的空白，特别是在印度财务环境中。

Method: 通过CA-Ben基准（基于印度特许会计师考试题库）评估六种主流LLM，采用标准化协议进行分析。

Result: Claude 3.5 Sonnet和GPT-4o表现最佳，尤其在概念和法律推理方面，但数值计算和法律解释仍是挑战。

Conclusion: 当前LLM在金融领域存在局限性，未来需结合混合推理和检索增强生成方法改进。

Abstract: Advanced intelligent systems, particularly Large Language Models (LLMs), are
significantly reshaping financial practices through advancements in Natural
Language Processing (NLP). However, the extent to which these models
effectively capture and apply domain-specific financial knowledge remains
uncertain. Addressing a critical gap in the expansive Indian financial context,
this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically
designed to evaluate the financial, legal, and quantitative reasoning
capabilities of LLMs. CA-Ben comprises structured question-answer datasets
derived from the rigorous examinations conducted by the Institute of Chartered
Accountants of India (ICAI), spanning foundational, intermediate, and advanced
CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1
405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated
using standardized protocols. Results indicate variations in performance, with
Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and
legal reasoning. Notable challenges emerged in numerical computations and legal
interpretations. The findings emphasize the strengths and limitations of
current LLMs, suggesting future improvements through hybrid reasoning and
retrieval-augmented generation methods, particularly for quantitative analysis
and accurate legal interpretation.

</details>


### [13] [A Semi-supervised Scalable Unified Framework for E-commerce Query Classification](https://arxiv.org/abs/2506.21049)
*Chunyuan Yuan,Chong Zhang,Zheng Fang,Ming Pang,Xue Jiang,Changping Peng,Zhangang Lin,Ching Law*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Query classification, including multiple subtasks such as intent and category
prediction, is vital to e-commerce applications. E-commerce queries are usually
short and lack context, and the information between labels cannot be used,
resulting in insufficient prior information for modeling. Most existing
industrial query classification methods rely on users' posterior click behavior
to construct training samples, resulting in a Matthew vicious cycle.
Furthermore, the subtasks of query classification lack a unified framework,
leading to low efficiency for algorithm optimization.
  In this paper, we propose a novel Semi-supervised Scalable Unified Framework
(SSUF), containing multiple enhanced modules to unify the query classification
tasks. The knowledge-enhanced module uses world knowledge to enhance query
representations and solve the problem of insufficient query information. The
label-enhanced module uses label semantics and semi-supervised signals to
reduce the dependence on posterior labels. The structure-enhanced module
enhances the label representation based on the complex label relations. Each
module is highly pluggable, and input features can be added or removed as
needed according to each subtask. We conduct extensive offline and online A/B
experiments, and the results show that SSUF significantly outperforms the
state-of-the-art models.

</details>


### [14] [MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection](https://arxiv.org/abs/2506.21053)
*Fuqiang Niu,Genan Dai,Yisha Lu,Jiayu Liao,Xiang Li,Hu Huang,Bowen Zhang*

Key words: 姿态检测, 社交媒体, 多轮对话, 数据集, 大语言模型

TL;DR: 这篇论文介绍了MT2-CSD数据集和LLM-CRAN模型，用于解决社交媒体多目标多轮对话姿态检测的挑战。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统姿态检测研究多针对单实例，难以建模真实社交媒体中的多轮动态交互，缺乏合适的数据集是主要限制。

Method: 提出MT2-CSD数据集（24,457标注实例）和LLM-CRAN模型，利用大语言模型的推理能力提升对话理解。

Result: 实验表明LLM-CRAN在多轮对话姿态检测任务中显著优于基线模型。

Conclusion: MT2-CSD和LLM-CRAN为社交媒体多目标多轮对话姿态检测提供了新工具和解决方案。

Abstract: In the realm of contemporary social media, automatic stance detection is
pivotal for opinion mining, as it synthesizes and examines user perspectives on
contentious topics to uncover prevailing trends and sentiments. Traditional
stance detection research often targets individual instances, thereby limiting
its capacity to model multi-party discussions typical in real social media
scenarios. This shortcoming largely stems from the scarcity of datasets that
authentically capture the dynamics of social media interactions, hindering
advancements in conversational stance detection. In this paper, we introduce
MT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational
stance detection. To the best of our knowledge, MT2-CSD is the largest dataset
available for this purpose, comprising 24,457 annotated instances and
exhibiting the greatest conversational depth, thereby presenting new challenges
for stance detection. To address these challenges, we propose the Large
Language model enhanced Conversational Relational Attention Network (LLM-CRAN),
which exploits the reasoning capabilities of LLMs to improve conversational
understanding. We conduct extensive experiments to evaluate the efficacy of
LLM-CRAN on the MT2-CSD dataset. The experimental results indicate that
LLM-CRAN significantly outperforms strong baseline models in the task of
conversational stance detection.

</details>


### [15] [DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning](https://arxiv.org/abs/2506.21096)
*Kang He,Yuzhe Ding. Haining Wang,Fei Li,Chong Teng,Donghong Ji*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Previous multimodal sentence representation learning methods have achieved
impressive performance. However, most approaches focus on aligning images and
text at a coarse level, facing two critical challenges:cross-modal misalignment
bias and intra-modal semantic divergence, which significantly degrade sentence
representation quality. To address these challenges, we propose DALR
(Dual-level Alignment Learning for Multimodal Sentence Representation). For
cross-modal alignment, we propose a consistency learning module that softens
negative samples and utilizes semantic similarity from an auxiliary task to
achieve fine-grained cross-modal alignment. Additionally, we contend that
sentence relationships go beyond binary positive-negative labels, exhibiting a
more intricate ranking structure. To better capture these relationships and
enhance representation quality, we integrate ranking distillation with global
intra-modal alignment learning. Comprehensive experiments on semantic textual
similarity (STS) and transfer (TR) tasks validate the effectiveness of our
approach, consistently demonstrating its superiority over state-of-the-art
baselines.

</details>


### [16] [ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry](https://arxiv.org/abs/2506.21098)
*Qinwen Chen,Wenbiao Tao,Zhiwei Zhu,Mingfan Xi,Liangzhong Guo,Yuan Wang,Wei Wang,Yunshi Lan*

Key words: 社区问答, 检索增强生成, 实时工业部署, 记忆机制, 性能提升

TL;DR: ComRAG 是一个基于检索增强生成的框架，用于实时工业社区问答，通过基于质心的记忆机制整合静态知识和动态历史问答对，显著提升了性能和效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 社区问答（CQA）平台是重要的知识库，但现有方法未能充分利用外部知识，缺乏动态历史上下文或适合工业部署的记忆机制。

Method: 提出 ComRAG 框架，结合静态知识和动态历史问答对，采用基于质心的记忆机制，支持检索、生成和高效存储。

Result: 在三个工业 CQA 数据集上，ComRAG 显著优于基线方法，向量相似度提升 25.9%，延迟降低 8.7%-23.3%，迭代中块增长从 20.23% 降至 2.06%。

Conclusion: ComRAG 是一个高效且可扩展的解决方案，适用于工业 CQA 任务。

Abstract: Community Question Answering (CQA) platforms can be deemed as important
knowledge bases in community, but effectively leveraging historical
interactions and domain knowledge in real-time remains a challenge. Existing
methods often underutilize external knowledge, fail to incorporate dynamic
historical QA context, or lack memory mechanisms suited for industrial
deployment. We propose ComRAG, a retrieval-augmented generation framework for
real-time industrial CQA that integrates static knowledge with dynamic
historical QA pairs via a centroid-based memory mechanism designed for
retrieval, generation, and efficient storage. Evaluated on three industrial CQA
datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%
improvement in vector similarity, reducing latency by 8.7% to 23.3%, and
lowering chunk growth from 20.23% to 2.06% over iterations.

</details>


### [17] [Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models](https://arxiv.org/abs/2506.21119)
*Xiaoshuang Ji,Zhendong Zhao,Xiaojun Chen,Xin Zhao,Zeyao Liu*

Key words: 微调, Transformer, 资源分配, 渐进式学习, 参数高效

TL;DR: Progtuning是一种结合渐进式学习的新型微调框架，通过逐步减少更新的Transformer块数量来优化资源分配，同时保持性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着模型规模增长，全参数微调成本高昂，现有参数高效微调方法未考虑Transformer块贡献不均，导致资源分配低效。

Method: Progtuning通过渐进式学习逐步减少更新的Transformer块数量，基于贡献优化资源分配。

Result: Progtuning减少约25%的更新参数，同时保持竞争力，且与参数高效微调方法适配性好。

Conclusion: Progtuning有效提升资源利用率，适用于多种适配场景。

Abstract: Fine-tuning is a promising technique for leveraging Transformer-based
language models in downstream tasks. As model sizes continue to grow, updating
all model parameters becomes increasingly costly. Parameter-efficient
fine-tuning methods effectively address this issue by selectively updating a
small subset of parameters. However, fine-tuning and most existing
parameter-efficient fine-tuning methods require updating the same number of
parameters as the initial size, ignoring the unequal contribution across
Transformer blocks and leading to extremely inefficient allocation of computing
resources. In this paper, we propose Progtuning, the novel fine-tuning
framework combined with progressive learning for Transformer-based language
models. Specifically, Progtuning progressively reduces the number of updated
transformer blocks based on the contribution. Remarkably, Progtuning optimizes
resource allocation and reduces the number of updated parameters by
approximately 25\%, while still maintaining competitive performance. And it
also exhibits high adaptability with parameter-efficient fine-tuning methods,
demonstrating excellent performance across various adaptation scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [Progressive Size-Adaptive Federated Learning: A Comprehensive Framework for Heterogeneous Multi-Modal Data Systems](https://arxiv.org/abs/2506.20685)
*Sajid Hussain,Muhammad Sohail,Nauman Ali Khan,Naima Iltaf,Ihtesham ul Islam*

Key words: 联邦学习, 数据集大小, 多模态数据, 通信效率, SAFL

TL;DR: 本文提出了一种基于数据集大小特性的自适应联邦学习框架SAFL，揭示了数据集大小和模态对联邦学习效果的显著影响，并展示了其在多模态数据上的高效性和优越性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有联邦学习方法主要关注模型异质性和聚合技术，而忽视了数据集大小特性对训练动态的根本影响。本文旨在填补这一研究空白。

Method: 提出Size-Based Adaptive Federated Learning (SAFL)框架，根据数据集大小特性系统组织联邦学习，并通过13个多模态数据集进行实验评估。

Result: 发现数据集大小在1000-1500样本时效果最佳，结构化数据性能显著优于非结构化数据，且大数据集性能下降。SAFL平均准确率达87.68%，通信效率高。

Conclusion: SAFL为联邦学习策略提供了理论和实践指导，填补了数据特性驱动联邦学习方法的研究空白。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm for
distributed machine learning while preserving data privacy. However, existing
approaches predominantly focus on model heterogeneity and aggregation
techniques, largely overlooking the fundamental impact of dataset size
characteristics on federated training dynamics. This paper introduces
Size-Based Adaptive Federated Learning (SAFL), a novel progressive training
framework that systematically organizes federated learning based on dataset
size characteristics across heterogeneous multi-modal data. Our comprehensive
experimental evaluation across 13 diverse datasets spanning 7 modalities
(vision, text, time series, audio, sensor, medical vision, and multimodal)
reveals critical insights: 1) an optimal dataset size range of 1000-1500
samples for federated learning effectiveness; 2) a clear modality performance
hierarchy with structured data (time series, sensor) significantly
outperforming unstructured data (text, multimodal); and 3) systematic
performance degradation for large datasets exceeding 2000 samples. SAFL
achieves an average accuracy of 87.68% across all datasets, with structured
data modalities reaching 99%+ accuracy. The framework demonstrates superior
communication efficiency, reducing total data transfer to 7.38 GB across 558
communications while maintaining high performance. Our real-time monitoring
framework provides unprecedented insights into system resource utilization,
network efficiency, and training dynamics. This work fills critical gaps in
understanding how data characteristics should drive federated learning
strategies, providing both theoretical insights and practical guidance for
real-world FL deployments in neural network and learning systems.

</details>


### [19] [E-ABIN: an Explainable module for Anomaly detection in BIological Networks](https://arxiv.org/abs/2506.20693)
*Ugo Lomoio,Tommaso Mazza,Pierangelo Veltri,Pietro Hiram Guzzi*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The increasing availability of large-scale omics data calls for robust
analytical frameworks capable of handling complex gene expression datasets
while offering interpretable results. Recent advances in artificial
intelligence have enabled the identification of aberrant molecular patterns
distinguishing disease states from healthy controls. Coupled with improvements
in model interpretability, these tools now support the identification of genes
potentially driving disease phenotypes. However, current approaches to gene
anomaly detection often remain limited to single datasets and lack accessible
graphical interfaces. Here, we introduce E-ABIN, a general-purpose, explainable
framework for Anomaly detection in Biological Networks. E-ABIN combines
classical machine learning and graph-based deep learning techniques within a
unified, user-friendly platform, enabling the detection and interpretation of
anomalies from gene expression or methylation-derived networks. By integrating
algorithms such as Support Vector Machines, Random Forests, Graph Autoencoders
(GAEs), and Graph Adversarial Attributed Networks (GAANs), E-ABIN ensures a
high predictive accuracy while maintaining interpretability. We demonstrate the
utility of E-ABIN through case studies of bladder cancer and coeliac disease,
where it effectively uncovers biologically relevant anomalies and offers
insights into disease mechanisms.

</details>


### [20] [On Context-Content Uncertainty Principle](https://arxiv.org/abs/2506.20699)
*Xin Li*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Context-Content Uncertainty Principle (CCUP) proposes that inference
under uncertainty is governed by an entropy asymmetry between context and
content: high-entropy contexts must be interpreted through alignment with
low-entropy, structured content. In this paper, we develop a layered
computational framework that derives operational principles from this
foundational asymmetry. At the base level, CCUP formalizes inference as
directional entropy minimization, establishing a variational gradient that
favors content-first structuring. Building upon this, we identify four
hierarchical layers of operational principles: (\textbf{L1}) \emph{Core
Inference Constraints}, including structure-before-specificity, asymmetric
inference flow, cycle-consistent bootstrapping, and conditional compression,
all shown to be mutually reducible; (\textbf{L2}) \emph{Resource Allocation
Principles}, such as precision-weighted attention, asymmetric learning rates,
and attractor-based memory encoding; (\textbf{L3}) \emph{Temporal Bootstrapping
Dynamics}, which organize learning over time via structure-guided curricula;
and (\textbf{L4}) \emph{Spatial Hierarchical Composition}, which integrates
these mechanisms into self-organizing cycles of memory, inference, and
planning. We present formal equivalence theorems, a dependency lattice among
principles, and computational simulations demonstrating the efficiency gains of
CCUP-aligned inference. This work provides a unified theoretical foundation for
understanding how brains and machines minimize uncertainty through recursive
structure-specificity alignment. The brain is not just an inference machine. It
is a cycle-consistent entropy gradient resolver, aligning structure and
specificity via path-dependent, content-seeded simulation.

</details>


### [21] [Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models](https://arxiv.org/abs/2506.20701)
*Vineet Jain,Kusha Sareen,Mohammad Pedramfar,Siamak Ravanbakhsh*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Adapting a pretrained diffusion model to new objectives at inference time
remains an open problem in generative modeling. Existing steering methods
suffer from inaccurate value estimation, especially at high noise levels, which
biases guidance. Moreover, information from past runs is not reused to improve
sample quality, resulting in inefficient use of compute. Inspired by the
success of Monte Carlo Tree Search, we address these limitations by casting
inference-time alignment as a search problem that reuses past computations. We
introduce a tree-based approach that samples from the reward-aligned target
density by propagating terminal rewards back through the diffusion chain and
iteratively refining value estimates with each additional generation. Our
proposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact
samples from the target distribution in the limit of infinite rollouts, and its
greedy variant, Diffusion Tree Search (DTS$^\star$), performs a global search
for high reward samples. On MNIST and CIFAR-10 class-conditional generation,
DTS matches the FID of the best-performing baseline with up to $10\times$ less
compute. In text-to-image generation and language completion tasks, DTS$^\star$
effectively searches for high reward samples that match best-of-N with up to
$5\times$ less compute. By reusing information from previous generations, we
get an anytime algorithm that turns additional compute into steadily better
samples, providing a scalable approach for inference-time alignment of
diffusion models.

</details>


### [22] [On Convolutions, Intrinsic Dimension, and Diffusion Models](https://arxiv.org/abs/2506.20705)
*Kin Kwan Leung,Rasa Hosseinzadeh,Gabriel Loaiza-Ganem*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The manifold hypothesis asserts that data of interest in high-dimensional
ambient spaces, such as image data, lies on unknown low-dimensional
submanifolds. Diffusion models (DMs) -- which operate by convolving data with
progressively larger amounts of Gaussian noise and then learning to revert this
process -- have risen to prominence as the most performant generative models,
and are known to be able to learn distributions with low-dimensional support.
For a given datum in one of these submanifolds, we should thus intuitively
expect DMs to have implicitly learned its corresponding local intrinsic
dimension (LID), i.e. the dimension of the submanifold it belongs to. Kamkari
et al. (2024b) recently showed that this is indeed the case by linking this LID
to the rate of change of the log marginal densities of the DM with respect to
the amount of added noise, resulting in an LID estimator known as FLIPD. LID
estimators such as FLIPD have a plethora of uses, among others they quantify
the complexity of a given datum, and can be used to detect outliers,
adversarial examples and AI-generated text. FLIPD achieves state-of-the-art
performance at LID estimation, yet its theoretical underpinnings are incomplete
since Kamkari et al. (2024b) only proved its correctness under the highly
unrealistic assumption of affine submanifolds. In this work we bridge this gap
by formally proving the correctness of FLIPD under realistic assumptions.
Additionally, we show that an analogous result holds when Gaussian convolutions
are replaced with uniform ones, and discuss the relevance of this result.

</details>


### [23] [Test-time Scaling Techniques in Theoretical Physics -- A Comparison of Methods on the TPBench Dataset](https://arxiv.org/abs/2506.20729)
*Zhiqi Gao,Tianyi Li,Yurii Kvasiuk,Sai Chaitanya Tadepalli,Maja Rudolph,Daniel J. H. Chung,Frederic Sala,Moritz Münchmeyer*

Key words: 大型语言模型, 理论物理, 测试时扩展, 符号验证

TL;DR: 本文研究了大型语言模型（LLMs）在高级理论物理领域的表现，并开发了一种新的符号弱验证框架，显著提升了在TPBench物理数据集上的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨从数学推理基准（如AIME）中学到的测试时扩展方法是否能推广到高级理论物理领域。

Method: 开发了一种符号弱验证框架，利用物理问题的结构提升并行扩展效果。

Result: 新方法在TPBench上显著优于现有测试时扩展方法，并在AIME上也表现出色。

Conclusion: 研究表明，分步符号验证是解决复杂科学问题的有效工具。

Abstract: Large language models (LLMs) have shown strong capabilities in complex
reasoning, and test-time scaling techniques can enhance their performance with
comparably low cost. Many of these methods have been developed and evaluated on
mathematical reasoning benchmarks such as AIME. This paper investigates whether
the lessons learned from these benchmarks generalize to the domain of advanced
theoretical physics. We evaluate a range of common test-time scaling methods on
the TPBench physics dataset and compare their effectiveness with results on
AIME. To better leverage the structure of physics problems, we develop a novel,
symbolic weak-verifier framework to improve parallel scaling results. Our
empirical results demonstrate that this method significantly outperforms
existing test-time scaling approaches on TPBench. We also evaluate our method
on AIME, confirming its effectiveness in solving advanced mathematical
problems. Our findings highlight the power of step-wise symbolic verification
for tackling complex scientific problems.

</details>


### [24] [A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools](https://arxiv.org/abs/2506.20743)
*Minh-Hao Van,Prateek Verma,Chen Zhao,Xintao Wu*

Key words: 基础模型；材料科学；多模态AI；数据治理；持续学习

TL;DR: 这篇论文综述了基础模型（FM）在材料科学（MatSci）中的应用，涵盖了其优势、应用领域及挑战。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 材料科学研究面临多样化的数据类型和尺度挑战，传统机器学习模型难以胜任。FM因其跨领域泛化能力和多模态特性，为材料科学提供了新的解决方案。

Method: 论文提出了一种任务驱动的分类法，涵盖数据提取、原子模拟、性质预测等六大应用领域，并综述了单模态和多模态FM、LLM代理以及相关工具和平台。

Result: FM在材料科学中已取得初步成功，但仍面临泛化性、可解释性、数据不平衡等局限性。

Conclusion: 未来研究方向包括扩展预训练、持续学习、数据治理和可信度提升。

Abstract: Foundation models (FMs) are catalyzing a transformative shift in materials
science (MatSci) by enabling scalable, general-purpose, and multimodal AI
systems for scientific discovery. Unlike traditional machine learning models,
which are typically narrow in scope and require task-specific engineering, FMs
offer cross-domain generalization and exhibit emergent capabilities. Their
versatility is especially well-suited to materials science, where research
challenges span diverse data types and scales. This survey provides a
comprehensive overview of foundation models, agentic systems, datasets, and
computational tools supporting this growing field. We introduce a task-driven
taxonomy encompassing six broad application areas: data extraction,
interpretation and Q\&A; atomistic simulation; property prediction; materials
structure, design and discovery; process planning, discovery, and optimization;
and multiscale modeling. We discuss recent advances in both unimodal and
multimodal FMs, as well as emerging large language model (LLM) agents.
Furthermore, we review standardized datasets, open-source tools, and autonomous
experimental platforms that collectively fuel the development and integration
of FMs into research workflows. We assess the early successes of foundation
models and identify persistent limitations, including challenges in
generalizability, interpretability, data imbalance, safety concerns, and
limited multimodal fusion. Finally, we articulate future research directions
centered on scalable pretraining, continual learning, data governance, and
trustworthiness.

</details>


### [25] [Multiple Streams of Relation Extraction: Enriching and Recalling in Transformers](https://arxiv.org/abs/2506.20746)
*Todd Nief,David Reber,Sean Richardson,Ari Holtzman*

Key words: LLM, fine-tuning, relation information, dynamic weight-grafting, localization approaches

TL;DR: 论文探讨了LLM在微调时学习的关系信息的存储和提取方式，提出了动态权重嫁接方法，揭示了信息在模型中的两种路径：提取和“回忆”。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究目标是理解LLM在微调过程中学习的关系信息如何在模型中存储和提取，填补现有局部化方法的不足。

Method: 采用动态权重嫁接技术，对比微调模型和预训练模型，分析关系信息的提取和“回忆”路径。

Result: 研究发现微调信息通过两种路径处理：实体处理时的提取和预测时的“回忆”，且路径的必要性和充分性因情况而异。

Conclusion: LLM通过特定注意力机制和关系提取步骤实现信息的“回忆”，揭示了微调信息处理的多路径机制。

Abstract: When an LLM learns a relation during finetuning (e.g., new movie releases,
corporate mergers, etc.), where does this information go? Is it extracted when
the model processes an entity, recalled just-in-time before a prediction, or
are there multiple separate heuristics? Existing localization approaches (e.g.
activation patching) are ill-suited for this analysis because they tend to
replace parts of the residual stream, potentially deleting information. To fill
this gap, we propose dynamic weight-grafting between fine-tuned and pre-trained
language models to show that fine-tuned language models both (1) extract
relation information learned during finetuning while processing entities and
(2) ``recall" this information in later layers while generating predictions. In
some cases, models need both of these pathways to correctly generate finetuned
information while, in other cases, a single ``enrichment" or ``recall" pathway
alone is sufficient. We examine the necessity and sufficiency of these
information pathways, examining what layers they occur at, how much redundancy
they exhibit, and which model components are involved -- finding that the
``recall" pathway occurs via both task-specific attention mechanisms and a
relation extraction step in the output of the attention and the feedforward
networks at the final layers before next token prediction.

</details>


### [26] [Characterization and Mitigation of Training Instabilities in Microscaling Formats](https://arxiv.org/abs/2506.20752)
*Huangyuan Su,Mujin Kwun,Stephanie Gil,Sham Kakade,Nikhil Anand*

Key words: 大型语言模型、低精度算术、MX格式、训练不稳定、量化

TL;DR: 论文研究了在低精度算术格式（如MX格式）下训练大型语言模型时出现的损失不稳定问题，并通过实验提出了一些稳定策略。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了降低训练大型语言模型的成本和计算需求，研究者在硬件加速器中引入了低精度算术格式（如MX格式）。然而，这些格式在训练过程中会导致损失的不稳定，尤其是在大规模计算时。

Method: 通过在一千多个语言模型中进行实验，研究者发现MX格式训练时会出现损失不稳定现象。随后，他们通过控制实验和模型分析，发现这种不稳定是由梯度偏差引起的。

Result: 研究发现，通过调整量化策略中的精度方案，可以避免或延迟不稳定的发生，并在某些混合配置下恢复与全精度训练相当的性能。

Conclusion: MX格式虽然能提高效率，但会引入训练不稳定。通过调整精度方案，可以在一定程度上缓解这一问题。

Abstract: Training large language models is an expensive, compute-bound process that
must be repeated as models scale, algorithms improve, and new data is
collected. To address this, next-generation hardware accelerators increasingly
support lower-precision arithmetic formats, such as the Microscaling (MX)
formats introduced in NVIDIA's Blackwell architecture. These formats use a
shared scale within blocks of parameters to extend representable range and
perform forward/backward GEMM operations in reduced precision for efficiency
gains. In this work, we investigate the challenges and viability of
block-scaled precision formats during model training. Across nearly one
thousand language models trained from scratch -- spanning compute budgets from
$2 \times 10^{17}$ to $4.8 \times 10^{19}$ FLOPs and sweeping over a broad
range of weight-activation precision combinations -- we consistently observe
that training in MX formats exhibits sharp, stochastic instabilities in the
loss, particularly at larger compute scales. To explain this phenomenon, we
conduct controlled experiments and ablations on a smaller proxy model that
exhibits similar behavior as the language model, sweeping across architectural
settings, hyperparameters, and precision formats. These experiments motivate a
simple model in which multiplicative gradient bias introduced by the
quantization of layer-norm affine parameters and a small fraction of
activations can trigger runaway divergence. Through \emph{in situ} intervention
experiments on our proxy model, we demonstrate that instabilities can be
averted or delayed by modifying precision schemes mid-training. Guided by these
findings, we evaluate stabilization strategies in the LLM setting and show that
certain hybrid configurations recover performance competitive with
full-precision training. We release our code at
https://github.com/Hither1/systems-scaling.

</details>


### [27] [Stochastic and Non-local Closure Modeling for Nonlinear Dynamical Systems via Latent Score-based Generative Models](https://arxiv.org/abs/2506.20771)
*Xinghao Dong,Huchen Yang,Jin-Long Wu*

Key words: 生成式AI；随机模型；多尺度动力系统；潜在空间；扩散模型

TL;DR: 提出了一种基于潜在评分的生成式AI框架，用于学习计算力学中非线性动力系统的随机、非局部闭合模型和本构定律。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决复杂多尺度动力系统建模的挑战，尤其是没有明确尺度分离的情况，传统方法因计算成本高而受限。

Method: 联合训练卷积自编码器与条件扩散模型，在潜在空间中降维并保留物理特性。

Result: 数值结果表明，该方法能发现合适的潜在空间，确保重构误差小且扩散模型性能好，计算加速同时保持预测精度。

Conclusion: 提出的随机建模框架在潜在条件扩散模型下，显著提高了计算效率，同时保持了与物理空间标准扩散模型相当的预测准确性。

Abstract: We propose a latent score-based generative AI framework for learning
stochastic, non-local closure models and constitutive laws in nonlinear
dynamical systems of computational mechanics. This work addresses a key
challenge of modeling complex multiscale dynamical systems without a clear
scale separation, for which numerically resolving all scales is prohibitively
expensive, e.g., for engineering turbulent flows. While classical closure
modeling methods leverage domain knowledge to approximate subgrid-scale
phenomena, their deterministic and local assumptions can be too restrictive in
regimes lacking a clear scale separation. Recent developments of
diffusion-based stochastic models have shown promise in the context of closure
modeling, but their prohibitive computational inference cost limits practical
applications for many real-world applications. This work addresses this
limitation by jointly training convolutional autoencoders with conditional
diffusion models in the latent spaces, significantly reducing the
dimensionality of the sampling process while preserving essential physical
characteristics. Numerical results demonstrate that the joint training approach
helps discover a proper latent space that not only guarantees small
reconstruction errors but also ensures good performance of the diffusion model
in the latent space. When integrated into numerical simulations, the proposed
stochastic modeling framework via latent conditional diffusion models achieves
significant computational acceleration while maintaining comparable predictive
accuracy to standard diffusion models in physical spaces.

</details>


### [28] [Stochastic Parameter Decomposition](https://arxiv.org/abs/2506.20790)
*Lucius Bushnaq,Dan Braun,Lee Sharkey*

Key words: 神经网络分解,线性参数分解,SPD,APD,机械可解释性

TL;DR: 介绍了一种名为随机参数分解（SPD）的新方法，解决了现有APD方法的计算成本高和超参数敏感性问题，并能更好地识别真实机制。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前线性参数分解方法中的APD存在计算成本高和超参数敏感性问题，限制了其在大模型中的应用。

Method: 提出SPD方法，相比APD更具可扩展性和超参数鲁棒性，通过随机化改进分解性能。

Result: SPD能分解更大更复杂模型，避免参数萎缩问题，并在玩具模型中更好识别真实机制。

Conclusion: SPD克服了现有方法的障碍，为机械可解释性研究提供了新的可能性。

Abstract: A key step in reverse engineering neural networks is to decompose them into
simpler parts that can be studied in relative isolation. Linear parameter
decomposition -- a framework that has been proposed to resolve several issues
with current decomposition methods -- decomposes neural network parameters into
a sum of sparsely used vectors in parameter space. However, the current main
method in this framework, Attribution-based Parameter Decomposition (APD), is
impractical on account of its computational cost and sensitivity to
hyperparameters. In this work, we introduce \textit{Stochastic Parameter
Decomposition} (SPD), a method that is more scalable and robust to
hyperparameters than APD, which we demonstrate by decomposing models that are
slightly larger and more complex than was possible to decompose with APD. We
also show that SPD avoids other issues, such as shrinkage of the learned
parameters, and better identifies ground truth mechanisms in toy models. By
bridging causal mediation analysis and network decomposition methods, this
demonstration opens up new research possibilities in mechanistic
interpretability by removing barriers to scaling linear parameter decomposition
methods to larger models. We release a library for running SPD and reproducing
our experiments at https://github.com/goodfire-ai/spd.

</details>


### [29] [GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization](https://arxiv.org/abs/2506.20807)
*Martin Andrews,Sam Witteveen*

Key words: GPU优化, LLM, 自动化, AMD MI300

TL;DR: 论文提出了一种基于LLM的自动化方法，通过多阶段进化过程优化GPU内核，尤其针对缺乏文档的新架构。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对GPU内核优化的复杂性，尤其是在新或文档不足的架构上，传统方法效率低下。

Method: 使用LLM进行多阶段进化优化：选择基础代码、生成优化假设、自动实施实验。

Result: 由于性能数据尚未公开，论文重点介绍了架构设计和定性分析。

Conclusion: LLM驱动的工具有潜力在资源受限或快速演进的硬件环境中简化GPU内核优化。

Abstract: Optimizing GPU kernels for high performance is a complex task, often
demanding deep architectural knowledge, extensive profiling, and iterative
experimentation. This challenge is amplified when targeting newer or
less-documented GPU architectures where traditional development aids are
scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an
automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)
strategically selecting promising prior code versions as a basis for new
iterations; (b) generating hypotheses for optimization experiments, based on
existing code and assimilated knowledge from general GPU literature; and (c)
autonomously implementing these experiments through code modification and
subsequent submission to an external evaluation system, using only observed
timing data as performance feedback. We detail how this approach navigates the
challenges of the AMD MI300 target architecture and leverages LLMs to
compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were
embargoed on paper submission date, we present the architectural design,
operational workflow, and qualitative insights, highlighting the potential of
LLM-driven agents to democratise and accelerate GPU kernel optimization,
especially in resource-constrained or rapidly evolving hardware environments.

</details>


### [30] [FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs](https://arxiv.org/abs/2506.20810)
*Shashwat Khandelwal,Jakoba Petri-Koenig,Thomas B. Preußer,Michaela Blott,Shreejith Shanker*

Key words: LSTM, FPGA, 量化, FINN框架, ONNX

TL;DR: 该论文提出了一种基于FINN框架的通用方法，用于在FPGA上部署LSTM网络，解决了现有工具主要针对前馈网络的问题，并通过量化技术和硬件映射优化实现了性能和资源的平衡。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前RNN（尤其是LSTM）在时间序列任务中表现优异，但其计算复杂度在资源受限环境中的实时部署存在挑战。FPGA是一种高效的AI加速平台，但现有工具缺乏对LSTM的支持，通常需要完全自定义实现。

Method: 利用开源的FINN框架和ONNX的Scan操作符建模LSTM的计算特性，支持混合量化和功能验证；通过自定义转换将量化ONNX计算图映射到硬件块，生成高效的硬件IP。

Result: 在股票价格预测任务中，生成的量化ConvLSTM加速器在性能和资源消耗之间取得了平衡，且推理精度与现有最佳模型相当或更优。

Conclusion: 提出的通用流程为FPGA上资源高效的RNN加速器设计铺平了道路。

Abstract: Recurrent neural networks (RNNs), particularly LSTMs, are effective for
time-series tasks like sentiment analysis and short-term stock prediction.
However, their computational complexity poses challenges for real-time
deployment in resource constrained environments. While FPGAs offer a promising
platform for energy-efficient AI acceleration, existing tools mainly target
feed-forward networks, and LSTM acceleration typically requires full custom
implementation. In this paper, we address this gap by leveraging the
open-source and extensible FINN framework to enable the generalized deployment
of LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open
Neural Network Exchange (ONNX) specification to model the recurrent nature of
LSTM computations, enabling support for mixed quantisation within them and
functional verification of LSTM-based models. Furthermore, we introduce custom
transformations within the FINN compiler to map the quantised ONNX computation
graph to hardware blocks from the HLS kernel library of the FINN compiler and
Vitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM
model for a mid-price stock prediction task using the widely used dataset and
generating a corresponding hardware IP of the model using our flow, targeting
the XCZU7EV device. We show that the generated quantised ConvLSTM accelerator
through our flow achieves a balance between performance (latency) and resource
consumption, while matching (or bettering) inference accuracy of
state-of-the-art models with reduced precision. We believe that the
generalisable nature of the proposed flow will pave the way for
resource-efficient RNN accelerator designs on FPGAs.

</details>


### [31] [Divide, Specialize, and Route: A New Approach to Efficient Ensemble Learning](https://arxiv.org/abs/2506.20814)
*Jakub Piwko,Jędrzej Ruciński,Dawid Płudowski,Antoni Zajko,Patryzja Żak,Mateusz Zacharecki,Anna Kozak,Katarzyna Woźnica*

Key words: 集成学习、二进制分类、动态分配、可解释性

TL;DR: 提出了一种名为Hellsemble的新型集成学习框架，通过根据样本难度增量训练和动态分配，提升了分类性能同时保持了计算效率和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统集成学习方法（如bagging、boosting和DES）存在计算成本高和适应异构数据分布能力不足的问题。

Method: Hellsemble通过将数据集逐级划分为难度递增的子集，训练专门化的基学习器，并利用路由模型动态分配新样本。

Result: 在OpenML-CC18和Tabzilla基准测试中表现优于传统集成方法。

Conclusion: 基于样本难度构建高效、鲁棒的集成系统是一个有前景的研究方向。

Abstract: Ensemble learning has proven effective in boosting predictive performance,
but traditional methods such as bagging, boosting, and dynamic ensemble
selection (DES) suffer from high computational cost and limited adaptability to
heterogeneous data distributions. To address these limitations, we propose
Hellsemble, a novel and interpretable ensemble framework for binary
classification that leverages dataset complexity during both training and
inference. Hellsemble incrementally partitions the dataset into circles of
difficulty by iteratively passing misclassified instances from simpler models
to subsequent ones, forming a committee of specialised base learners. Each
model is trained on increasingly challenging subsets, while a separate router
model learns to assign new instances to the most suitable base model based on
inferred difficulty. Hellsemble achieves strong classification accuracy while
maintaining computational efficiency and interpretability. Experimental results
on OpenML-CC18 and Tabzilla benchmarks demonstrate that Hellsemble often
outperforms classical ensemble methods. Our findings suggest that embracing
instance-level difficulty offers a promising direction for constructing
efficient and robust ensemble systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [32] [The Singapore Consensus on Global AI Safety Research Priorities](https://arxiv.org/abs/2506.20702)
*Yoshua Bengio,Tegan Maharaj,Luke Ong,Stuart Russell,Dawn Song,Max Tegmark,Lan Xue,Ya-Qin Zhang,Stephen Casper,Wan Sie Lee,Sören Mindermann,Vanessa Wilfred,Vidhisha Balachandran,Fazl Barez,Michael Belinsky,Imane Bello,Malo Bourgon,Mark Brakel,Siméon Campos,Duncan Cass-Beggs,Jiahao Chen,Rumman Chowdhury,Kuan Chua Seah,Jeff Clune,Juntao Dai,Agnes Delaborde,Nouha Dziri,Francisco Eiras,Joshua Engels,Jinyu Fan,Adam Gleave,Noah Goodman,Fynn Heide,Dan Hendrycks,Cyrus Hodes,Bryan Low Kian Hsiang,Minlie Huang,Sami Jawhar,Wang Jingyu,Adam Tauman Kalai,Meindert Kamphuis,Mohan Kankanhalli,Subhash Kantamneni,Mathias Bonde Kirk,Thomas Kwa,Jeffrey Ladish,Kwok-Yan Lam,Wan Lee Sie,Taewhi Lee,Xiaojian Li,Jiajun Liu,Chaochao Lu,Yifan Mai,Richard Mallah,Julian Michael,Nick Moës,Simon Möller,Kihyuk Nam,Kwan Yee Ng,Mark Nitzberg,Besmira Nushi,Seán O hÉigeartaigh,Alejandro Ortega,Pierre Peigné,James Petrie,Benjamin Prud'Homme,Reihaneh Rabbany,Nayat Sanchez-Pi,Sarah Schwettmann,Buck Shlegeris,Saad Siddiqui,Aradhana Sinha,Martín Soto,Cheston Tan,Dong Ting,Robert Trager,Brian Tse,Anthony Tung K. H.,Vanessa Wilfred,John Willes,Denise Wong,Wei Xu,Rongwu Xu,Yi Zeng,HongJiang Zhang,Djordje Žikelić*

Key words: AI安全，可信生态系统，深度防御模型，开发，评估，控制

TL;DR: 该论文讨论了AI安全性研究的重要性，并提出了基于深度防御模型的三大研究领域。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着AI能力的快速提升和自主性的增强，确保其安全、可信赖、可靠和安全的必要性引发了广泛讨论。构建可信生态系统至关重要，以帮助人们自信地接受AI并最大限度地促进创新。

Method: 通过2025年新加坡AI安全国际科学交流会议，汇集全球AI科学家，识别并综合AI安全研究的优先事项。报告基于33个政府支持的国际AI安全报告，采用深度防御模型，将AI安全研究分为开发、评估和控制三大领域。

Result: 报告提出了三大AI安全研究领域：开发（创建可信AI系统的挑战）、评估（评估风险的挑战）和控制（部署后监控和干预的挑战）。

Conclusion: 构建可信的AI生态系统需要从开发、评估和控制三个方面入手，以确保AI的安全性和可靠性，同时促进创新。

Abstract: Rapidly improving AI capabilities and autonomy hold significant promise of
transformation, but are also driving vigorous debate on how to ensure that AI
is safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem
is therefore essential -- it helps people embrace AI with confidence and gives
maximal space for innovation while avoiding backlash.
  The "2025 Singapore Conference on AI (SCAI): International Scientific
Exchange on AI Safety" aimed to support research in this space by bringing
together AI scientists across geographies to identify and synthesise research
priorities in AI safety. This resulting report builds on the International AI
Safety Report chaired by Yoshua Bengio and backed by 33 governments. By
adopting a defence-in-depth model, this report organises AI safety research
domains into three types: challenges with creating trustworthy AI systems
(Development), challenges with evaluating their risks (Assessment), and
challenges with monitoring and intervening after deployment (Control).

</details>


### [33] [MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2506.20737)
*Gurusha Juneja,Alon Albalak,Wenyue Hua,William Yang Wang*

Key words: LLM代理, 隐私保护, MAGPIE基准, 多轮对话, 协作任务

TL;DR: 该论文研究了LLM代理在上下文隐私理解方面的表现，发现当前模型在处理高复杂度任务时隐私保护不足。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着LLM代理在协作任务中的广泛应用，隐私保护变得至关重要，尤其在涉及专有工具和数据库的场景中。

Method: 论文提出了MAGPIE基准测试，包含158个高风险场景，评估LLM代理在上下文隐私理解和协作任务中的表现。

Result: 实验表明，包括GPT-4o和Claude-2.7-Sonnet在内的模型对隐私数据的分类错误率较高，多轮对话中仍会泄露隐私信息。

Conclusion: 当前LLM代理在上下文隐私保护和协作任务完成方面尚不完善。

Abstract: The proliferation of LLM-based agents has led to increasing deployment of
inter-agent collaboration for tasks like scheduling, negotiation, resource
allocation etc. In such systems, privacy is critical, as agents often access
proprietary tools and domain-specific databases requiring strict
confidentiality. This paper examines whether LLM-based agents demonstrate an
understanding of contextual privacy. And, if instructed, do these systems
preserve inference time user privacy in non-adversarial multi-turn
conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents
primarily assess single-turn, low-complexity tasks where private information
can be easily excluded. We first present a benchmark - MAGPIE comprising 158
real-life high-stakes scenarios across 15 domains. These scenarios are designed
such that complete exclusion of private data impedes task completion yet
unrestricted information sharing could lead to substantial losses. We then
evaluate the current state-of-the-art LLMs on (a) their understanding of
contextually private data and (b) their ability to collaborate without
violating user privacy. Empirical experiments demonstrate that current models,
including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual
privacy, misclassifying private data as shareable 25.2\% and 43.6\% of the
time. In multi-turn conversations, these models disclose private information in
59.9\% and 50.5\% of cases even under explicit privacy instructions.
Furthermore, multi-agent systems fail to complete tasks in 71\% of scenarios.
These results underscore that current models are not aligned towards both
contextual privacy preservation and collaborative task-solving.

</details>


### [34] [Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI Applications](https://arxiv.org/abs/2506.20815)
*Xinye Tang,Haijun Zhai,Chaitanya Belwal,Vineeth Thayanithi,Philip Baumann,Yogesh K Roy*

Key words: LLM, 提示推荐, 上下文感知, 领域特定应用, 自适应技能排名

TL;DR: 本文提出了一种针对领域特定AI应用的动态上下文感知提示推荐系统，通过结合上下文查询分析和自适应技能排名等方法，显著提高了提示的实用性和相关性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 由于LLM驱动的应用对用户提示质量高度敏感，而高质量提示的生成在领域特定应用中尤为困难，因此需要一种动态且上下文感知的提示推荐系统。

Method: 系统结合了上下文查询分析、检索增强知识基础、分层技能组织和自适应技能排名，通过行为遥测和两阶段分层推理动态生成提示建议。

Result: 在真实数据集上的实验表明，系统生成的提示具有高度的实用性和相关性，并通过自动和专家评估验证。

Conclusion: 该系统有效解决了领域特定应用中高质量提示生成的挑战，为LLM应用提供了动态且上下文感知的推荐解决方案。

Abstract: LLM-powered applications are highly susceptible to the quality of user
prompts, and crafting high-quality prompts can often be challenging especially
for domain-specific applications. This paper presents a novel dynamic
context-aware prompt recommendation system for domain-specific AI applications.
Our solution combines contextual query analysis, retrieval-augmented knowledge
grounding, hierarchical skill organization, and adaptive skill ranking to
generate relevant and actionable prompt suggestions.
  The system leverages behavioral telemetry and a two-stage hierarchical
reasoning process to dynamically select and rank relevant skills, and
synthesizes prompts using both predefined and adaptive templates enhanced with
few-shot learning. Experiments on real-world datasets demonstrate that our
approach achieves high usefulness and relevance, as validated by both automated
and expert evaluations.

</details>


### [35] [Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation](https://arxiv.org/abs/2506.20949)
*Chenkai Sun,Denghui Zhang,ChengXiang Zhai,Heng Ji*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given the growing influence of language model-based agents on high-stakes
societal decisions, from public policy to healthcare, ensuring their beneficial
impact requires understanding the far-reaching implications of their
suggestions. We propose a proof-of-concept framework that projects how
model-generated advice could propagate through societal systems on a
macroscopic scale over time, enabling more robust alignment. To assess the
long-term safety awareness of language models, we also introduce a dataset of
100 indirect harm scenarios, testing models' ability to foresee adverse,
non-obvious outcomes from seemingly harmless user prompts. Our approach
achieves not only over 20% improvement on the new dataset but also an average
win rate exceeding 70% against strong baselines on existing safety benchmarks
(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer
agents.

</details>


### [36] [Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?](https://arxiv.org/abs/2506.21215)
*Haoang Chi,He Li,Wenjing Yang,Feng Liu,Long Lan,Xiaoguang Ren,Tongliang Liu,Bo Han*

Key words: 因果推理,大语言模型,G²-Reasoner,CausalProbe-2024

TL;DR: 该论文探讨大语言模型（LLMs）的真实因果推理能力，发现当前模型仅能进行浅层（level-1）推理，缺乏人类水平（level-2）的因果推理能力。为此，作者提出新方法G²-Reasoner，结合通用知识和目标导向提示，显著提升了LLMs的因果推理能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究LLMs是否具备与人类相似的因果推理能力，揭示其局限性并提出改进方法。

Method: 分析transformer LLMs的自回归机制，提出CausalProbe-2024基准测试，并开发G²-Reasoner方法，结合通用知识和目标导向提示。

Result: 研究发现LLMs仅能进行level-1因果推理，而G²-Reasoner显著提升了其在新鲜和反事实上下文中的因果推理能力。

Conclusion: G²-Reasoner为LLMs实现level-2因果推理提供了新方向，揭示了超越浅层推理的可能性。

Abstract: Causal reasoning capability is critical in advancing large language models
(LLMs) toward strong artificial intelligence. While versatile LLMs appear to
have demonstrated capabilities in understanding contextual causality and
providing responses that obey the laws of causality, it remains unclear whether
they perform genuine causal reasoning akin to humans. However, current evidence
indicates the contrary. Specifically, LLMs are only capable of performing
shallow (level-1) causal reasoning, primarily attributed to the causal
knowledge embedded in their parameters, but they lack the capacity for genuine
human-like (level-2) causal reasoning. To support this hypothesis,
methodologically, we delve into the autoregression mechanism of
transformer-based LLMs, revealing that it is not inherently causal.
Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,
whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs
exhibit a significant performance drop on CausalProbe-2024 compared to earlier
benchmarks, indicating the fact that they primarily engage in level-1 causal
reasoning. To bridge the gap towards level-2 causal reasoning, we draw
inspiration from the fact that human reasoning is usually facilitated by
general knowledge and intended goals. We propose G^2-Reasoner, a method that
incorporates general knowledge and goal-oriented prompts into LLMs' causal
reasoning processes. Experiments demonstrate that G^2-Reasoner significantly
enhances LLMs' causal reasoning capability, particularly in fresh and
counterfactual contexts. This work sheds light on a new path for LLMs to
advance towards genuine causal reasoning, going beyond level-1 and making
strides towards level-2.

</details>


### [37] [World-aware Planning Narratives Enhance Large Vision-Language Model Planner](https://arxiv.org/abs/2506.21230)
*Junhao Shi,Zhaoye Fei,Siyin Wang,Qipeng Guo,Jingjing Gong,Xipeng QIu*

Key words: 视觉语言模型, 环境感知, 课程学习, 认知能力, EB-ALFRED基准

TL;DR: 论文提出了一种名为WAP的框架，通过四种认知能力提升大型视觉语言模型在环境感知规划任务中的表现，显著提高了任务成功率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前大型视觉语言模型在复杂场景和长时程规划任务中表现不佳，主要因为其缺乏对环境上下文的理解。

Method: 提出了World-Aware Planning Narrative Enhancement (WAP)框架，通过四种认知能力（视觉外观建模、空间推理、功能抽象和语法基础）和环境感知的课程学习，提升模型的视觉推理能力。

Result: 在EB-ALFRED基准测试中，任务成功率大幅提升，特别是在常识推理和长时程规划任务中表现突出。

Conclusion: WAP框架有效提升了模型的环境感知能力，其开源模型表现优于专有系统。

Abstract: Large Vision-Language Models (LVLMs) show promise for embodied planning tasks
but struggle with complex scenarios involving unfamiliar environments and
multi-step goals. Current approaches rely on environment-agnostic imitation
learning that disconnects instructions from environmental contexts, causing
models to struggle with context-sensitive instructions and rely on
supplementary cues rather than visual reasoning during long-horizon
interactions. In this work, we propose World-Aware Planning Narrative
Enhancement (WAP), a framework that infuses LVLMs with comprehensive
environmental understanding through four cognitive capabilities (visual
appearance modeling, spatial reasoning, functional abstraction, and syntactic
grounding) while developing and evaluating models using only raw visual
observations through curriculum learning. Evaluations on the EB-ALFRED
benchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a
60.7 absolute improvement in task success rates, particularly in commonsense
reasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced
open-source models outperform proprietary systems like GPT-4o and
Claude-3.5-Sonnet by a large margin.

</details>


### [38] [IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems](https://arxiv.org/abs/2506.21310)
*Pauline Speckmann,Mario Nadj,Christian Janiesch*

Key words: 可解释AI, 交互式系统, 用户视角, 透明度, LIME, SHAP

TL;DR: IXAII是一个交互式的可解释AI系统，结合了多种解释方法（LIME、SHAP、Anchors、DiCE），并针对不同用户群体提供定制化视图和解释内容。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的AI可解释性方法多为静态且忽视了用户视角，限制了其实际效果，因此开发了IXAII以提升透明度和用户体验。

Method: 开发了交互式系统IXAII，整合了LIME、SHAP、Anchors和DiCE四种解释方法，提供定制化视图和用户可控的解释格式。

Result: 通过专家和普通用户的访谈评估，IXAII因其多样化的解释和可视化选项被认为有助于增强透明度。

Conclusion: IXAII通过结合多种解释方法和交互性，为AI解释实践和人机交互提供了新视角。

Abstract: Although several post-hoc methods for explainable AI have been developed,
most are static and neglect the user perspective, limiting their effectiveness
for the target audience. In response, we developed the interactive explainable
intelligent system called IXAII that offers explanations from four explainable
AI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored
views for five user groups and gives users agency over the explanations'
content and their format. We evaluated IXAII through interviews with experts
and lay users. Our results indicate that IXAII, which provides different
explanations with multiple visualization options, is perceived as helpful to
increase transparency. By bridging the gaps between explainable AI methods,
interactivity, and practical implementation, we provide a novel perspective on
AI explanation practices and human-AI interaction.

</details>


### [39] [Active Inference AI Systems for Scientific Discovery](https://arxiv.org/abs/2506.21329)
*Karthik Duraisamy*

Key words: AI驱动科学, 主动推理, 因果模型, 知识图谱, 实验闭环

TL;DR: 本文探讨了AI驱动科学发展所需解决的三大核心问题：抽象、推理与现实间的差距，并提出了一种基于主动推理的AI系统架构。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前AI系统在科学发现中存在操作架构、脆弱的推理机制及与实验现实的分离问题，阻碍了其颠覆性潜力。

Method: 提出一种主动推理AI系统，结合因果自监督基础模型、贝叶斯规划器、持久知识图谱及闭环实验验证。

Result: 该系统通过内部模型与外部验证的交互实现科学发现，并强调了人类判断在其中的持久重要性。

Conclusion: 未来AI驱动科学依赖于填补三大核心差距，并构建结合内部推理与外部验证的体系，同时人类角色不可或缺。

Abstract: The rapid evolution of artificial intelligence has led to expectations of
transformative scientific discovery, yet current systems remain fundamentally
limited by their operational architectures, brittle reasoning mechanisms, and
their separation from experimental reality. Building on earlier work, we
contend that progress in AI-driven science now depends on closing three
fundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap
-- rather than on model size/data/test time compute. Scientific reasoning
demands internal representations that support simulation of actions and
response, causal structures that distinguish correlation from mechanism, and
continuous calibration. We define active inference AI systems for scientific
discovery as those that (i) maintain long-lived research memories grounded in
causal self-supervised foundation models, (ii) symbolic or neuro-symbolic
planners equipped with Bayesian guardrails, (iii) grow persistent knowledge
graphs where thinking generates novel conceptual nodes, reasoning establishes
causal edges, and real-world interaction prunes false connections while
strengthening verified pathways, and (iv) refine their internal representations
through closed-loop interaction with both high-fidelity simulators and
automated laboratories - an operational loop where mental simulation guides
action and empirical surprise reshapes understanding. In essence, we outline an
architecture where discovery arises from the interplay between internal models
that enable counterfactual reasoning and external validation that grounds
hypotheses in reality. It is also argued that the inherent ambiguity in
feedback from simulations and experiments, and underlying uncertainties makes
human judgment indispensable, not as a temporary scaffold but as a permanent
architectural component.

</details>


### [40] [TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding](https://arxiv.org/abs/2506.21393)
*Junwen Zhang,Pu Chen,Yin Zhang*

Key words: 多模态理解、表格处理、神经符号推理、Mixture-of-Experts、WildStruct

TL;DR: TableMoE是一种专为多模态表格数据设计的神经符号混合连接专家架构，通过创新的神经符号路由机制显著提升模型在真实复杂条件下的表现。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决现有多模态大语言模型在处理复杂表格结构、符号密度高及视觉退化等问题时的性能不足。

Method: 提出神经符号混合连接专家架构（TableMoE），引入神经符号路由机制，动态分配表格元素至专门专家模块，并利用大规模数据集TableMoE-Align进行预训练。

Result: TableMoE在四个WildStruct基准测试中显著超越现有最先进模型，并通过消融研究验证了其核心组件的有效性。

Conclusion: 通过神经符号推理的集成，TableMoE在多模态表格理解中展现出卓越的稳健性和可解释性。

Abstract: Multimodal understanding of tables in real-world contexts is challenging due
to the complexity of structure, symbolic density, and visual degradation (blur,
skew, watermarking, incomplete structures or fonts, multi-span or
hierarchically nested layouts). Existing multimodal large language models
(MLLMs) struggle with such WildStruct conditions, resulting in limited
performance and poor generalization. To address these challenges, we propose
TableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture
specifically designed for robust, structured reasoning over multimodal table
data. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which
predicts latent semantic token roles (e.g., header, data cell, axis, formula)
and dynamically routes table elements to specialized experts (Table-to-HTML,
Table-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed
by symbolic reasoning graphs. To facilitate effective alignment-driven
pretraining, we introduce the large-scale TableMoE-Align dataset, consisting of
1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and
industry, utilized exclusively for model pretraining. For evaluation, we curate
and release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,
WMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models
under real-world multimodal degradation and structural complexity. Experimental
results demonstrate that TableMoE significantly surpasses existing
state-of-the-art models. Extensive ablation studies validate each core
component, emphasizing the critical role of Neuro-Symbolic Routing and
structured expert alignment. Through qualitative analyses, we further showcase
TableMoE's interpretability and enhanced robustness, underscoring the
effectiveness of integrating neuro-symbolic reasoning for multimodal table
understanding.

</details>


### [41] [Spatial Mental Modeling from Limited Views](https://arxiv.org/abs/2506.21458)
*Baiqiao Yin,Qineng Wang,Pingyue Zhang,Jianshu Zhang,Kangrui Wang,Zihan Wang,Jieyu Zhang,Keshigeyan Chandrasegaran,Han Liu,Ranjay Krishna,Saining Xie,Manling Li,Jiajun Wu,Li Fei-Fei*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Can Vision Language Models (VLMs) imagine the full scene from just a few
views, like humans do? Humans form spatial mental models, internal
representations of unseen space, to reason about layout, perspective, and
motion. Our new MindCube benchmark with 21,154 questions across 3,268 images
exposes this critical gap, where existing VLMs exhibit near-random performance.
Using MindCube, we systematically evaluate how well VLMs build robust spatial
mental models through representing positions (cognitive mapping), orientations
(perspective-taking), and dynamics (mental simulation for "what-if" movements).
We then explore three approaches to help VLMs approximate spatial mental
models, including unseen intermediate views, natural language reasoning chains,
and cognitive maps. The significant improvement comes from a synergistic
approach, "map-then-reason", that jointly trains the model to first generate a
cognitive map and then reason upon it. By training models to reason over these
internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding
reinforcement learning pushed performance even further to 70.7% (+32.9%). Our
key insight is that such scaffolding of spatial mental models, actively
constructing and utilizing internal structured spatial representations with
flexible reasoning processes, significantly improves understanding of
unobservable space.

</details>


### [42] [Ad-Hoc Human-AI Coordination Challenge](https://arxiv.org/abs/2506.21490)
*Tin Dizdarević,Ravi Hammond,Tobias Gessler,Anisoara Calinescu,Jonathan Cook,Matteo Gallici,Andrei Lupu,Jakob Nicolaus Foerster*

Key words: 人机协调，Hanabi，代理评估，开源数据集

TL;DR: 该论文提出了Ad-Hoc Human-AI Coordination Challenge（AH2AC2），通过开发人机代理并开源数据集，解决人机协调评估中的成本和可重复性问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决人机协调在现实应用中的挑战，特别是在合作游戏Hanabi中的人机交互评估问题。

Method: 开发人机代理作为评估伙伴，并开源有限的游戏数据集。

Result: 提出了人机代理和数据集，并提供了两个和三玩家场景的基线结果。

Conclusion: AH2AC2为人机协调研究提供了低成本、可重复的评估方法。

Abstract: Achieving seamless coordination between AI agents and humans is crucial for
real-world applications, yet it remains a significant open challenge. Hanabi is
a cooperative card game featuring imperfect information, constrained
communication, theory of mind requirements, and coordinated action -- making it
an ideal testbed for human-AI coordination. However, its use for human-AI
interaction has been limited by the challenges of human evaluation. In this
work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to
overcome the constraints of costly and difficult-to-reproduce human
evaluations. We develop \textit{human proxy agents} on a large-scale human
dataset that serve as robust, cheap, and reproducible human-like evaluation
partners in AH2AC2. To encourage the development of data-efficient methods, we
open-source a dataset of 3,079 games, deliberately limiting the amount of
available human gameplay data. We present baseline results for both two- and
three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy
agents through a controlled evaluation system rather than releasing them
publicly. The code is available at
\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.

</details>


### [43] [Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge](https://arxiv.org/abs/2506.21506)
*Boyu Gou,Zanming Huang,Yuting Ning,Yu Gu,Michael Lin,Weijian Qi,Andrei Kopanev,Botao Yu,Bernal Jiménez Gutiérrez,Yiheng Shu,Chan Hee Song,Jiaman Wu,Shijie Chen,Hanane Nour Moussa,Tianshu Zhang,Jian Xie,Yifei Li,Tianci Xue,Zeyi Liao,Kai Zhang,Boyuan Zheng,Zhaowei Cai,Viktor Rozgic,Morteza Ziyadi,Huan Sun,Yu Su*

Key words: Error

TL;DR: Error

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Agentic search such as Deep Research systems, where large language models
autonomously browse the web, synthesize information, and return comprehensive
citation-backed answers, represents a major shift in how users interact with
web-scale information. While promising greater efficiency and cognitive
offloading, the growing complexity and open-endedness of agentic search have
outpaced existing evaluation benchmarks and methodologies, which largely assume
short search horizons and static answers. In this paper, we introduce Mind2Web
2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that
require real-time web browsing and extensive information synthesis, constructed
with over 1,000 hours of human labor. To address the challenge of evaluating
time-varying and complex answers, we propose a novel Agent-as-a-Judge
framework. Our method constructs task-specific judge agents based on a
tree-structured rubric design to automatically assess both answer correctness
and source attribution. We conduct a comprehensive evaluation of nine frontier
agentic search systems and human performance, along with a detailed error
analysis to draw insights for future development. The best-performing system,
OpenAI Deep Research, can already achieve 50-70% of human performance while
spending half the time, showing a great potential. Altogether, Mind2Web 2
provides a rigorous foundation for developing and benchmarking the next
generation of agentic search systems.

</details>


### [44] [PsyLite Technical Report](https://arxiv.org/abs/2506.21536)
*Fangjun Ding,Renyu Zhang,Xinyu Feng,Chengye Xie,Zheng Zhang,Yanting Zhang*

Key words: 心理咨询, AI对话, 轻量化模型, 条件RAG, ORPO训练

TL;DR: 研究提出PsyLite，一种轻量级心理咨询大型语言模型，通过两阶段训练和条件RAG设计，显著提升心理咨询专业性（47.6%）和对话安全性（2.4%），并在资源有限环境下实现部署（仅需5GB内存）。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有AI心理咨询模型在对话安全、场景处理及轻量化部署方面存在不足，需改进。

Method: 基于InternLM2.5-7B-chat，采用混合蒸馏数据微调和ORPO偏好优化的两阶段训练策略，结合条件RAG设计。

Result: 在CEval、CPsyCounE和SafeDialBench评估中表现优异，心理咨询专业性提升47.6%，对话安全性提升2.4%。

Conclusion: PsyLite为资源受限环境提供高效、安全的心理咨询解决方案。

Abstract: With the rapid development of digital technology, AI-driven psychological
counseling has gradually become an important research direction in the field of
mental health. However, existing models still have deficiencies in dialogue
safety, detailed scenario handling, and lightweight deployment. To address
these issues, this study proposes PsyLite, a lightweight psychological
counseling large language model agent developed based on the base model
InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation
data fine-tuning and ORPO preference optimization), PsyLite enhances the
model's deep-reasoning ability, psychological counseling ability, and safe
dialogue ability. After deployment using Ollama and Open WebUI, a custom
workflow is created with Pipelines. An innovative conditional RAG is designed
to introduce crosstalk humor elements at appropriate times during psychological
counseling to enhance user experience and decline dangerous requests to
strengthen dialogue safety. Evaluations show that PsyLite outperforms the
baseline models in the Chinese general evaluation (CEval), psychological
counseling professional evaluation (CPsyCounE), and dialogue safety evaluation
(SafeDialBench), particularly in psychological counseling professionalism
(CPsyCounE score improvement of 47.6\%) and dialogue safety (\safe{} score
improvement of 2.4\%). Additionally, the model uses quantization technology
(GGUF q4\_k\_m) to achieve low hardware deployment (5GB memory is sufficient
for operation), providing a feasible solution for psychological counseling
applications in resource-constrained environments.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [45] [DRAGON: Distributional Rewards Optimize Diffusion Generative Models](https://arxiv.org/abs/2504.15217)
*Yatong Bai,Jonah Casebeer,Somayeh Sojoudi,Nicholas J. Bryan*

Key words: DRAGON, 奖励函数, 生成模型优化, 分布级奖励, 音频生成

TL;DR: DRAGON是一种灵活的生成模型优化框架，可用于优化多种奖励函数，包括实例级和分布级奖励，显著优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 传统RLHF或DPO方法灵活性不足，无法应对多种奖励函数（如实例级和分布级），DRAGON旨在解决这一问题。

Method: 通过选择编码器和参考集构建奖励函数，利用正负对比集优化奖励。在音频领域测试了20种奖励函数。

Result: DRAGON在所有20种奖励上平均胜率为81.45%，且基于范例集的奖励效果与模型奖励相当。

Conclusion: DRAGON为优化人类感知质量提供了新方法，无需依赖人类偏好标注。

Abstract: We present Distributional RewArds for Generative OptimizatioN (DRAGON), a
versatile framework for fine-tuning media generation models towards a desired
outcome. Compared with traditional reinforcement learning with human feedback
(RLHF) or pairwise preference approaches such as direct preference optimization
(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate
either individual examples or distributions of them, making it compatible with
a broad spectrum of instance-wise, instance-to-distribution, and
distribution-to-distribution rewards. Leveraging this versatility, we construct
novel reward functions by selecting an encoder and a set of reference examples
to create an exemplar distribution. When cross-modality encoders such as CLAP
are used, the reference examples may be of a different modality (e.g., text
versus audio). Then, DRAGON gathers online and on-policy generations, scores
them to construct a positive demonstration set and a negative set, and
leverages the contrast between the two sets to maximize the reward. For
evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20
different reward functions, including a custom music aesthetics model, CLAP
score, Vendi diversity, and Frechet audio distance (FAD). We further compare
instance-wise (per-song) and full-dataset FAD settings while ablating multiple
FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an
81.45% average win rate. Moreover, reward functions based on exemplar sets
indeed enhance generations and are comparable to model-based rewards. With an
appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality
win rate without training on human preference annotations. As such, DRAGON
exhibits a new approach to designing and optimizing reward functions for
improving human-perceived quality. Sound examples at
https://ml-dragon.github.io/web.

</details>
