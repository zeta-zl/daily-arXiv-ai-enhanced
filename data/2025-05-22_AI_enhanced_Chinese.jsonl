{"id": "2505.14804", "pdf": "https://arxiv.org/pdf/2505.14804", "abs": "https://arxiv.org/abs/2505.14804", "authors": ["Richard Khoury", "Maxence Verhaverbeke", "Julie A. Gramaccia"], "title": "Automated Journalistic Questions: A New Method for Extracting 5W1H in French", "categories": ["cs.CL", "cs.LG"], "comment": "14 pages, 5 figures, 7 tables", "summary": "The 5W1H questions -- who, what, when, where, why and how -- are commonly\nused in journalism to ensure that an article describes events clearly and\nsystematically. Answering them is a crucial prerequisites for tasks such as\nsummarization, clustering, and news aggregation. In this paper, we design the\nfirst automated extraction pipeline to get 5W1H information from French news\narticles. To evaluate the performance of our algo- rithm, we also create a\ncorpus of 250 Quebec news articles with 5W1H answers marked by four human\nannotators. Our results demonstrate that our pipeline performs as well in this\ntask as the large language model GPT-4o.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u81ea\u52a8\u63d0\u53d6\u6cd5\u8bed\u53e5\u65b0\u95fb\u4e2d5W1H\u4fe1\u606f\u7684\u7cfb\u7edf\uff0c\u5176\u6027\u80fd\u4e0eGPT-4\u76f8\u5f53\uff0c\u5e76\u4f7f\u7528\u4eba\u5de5\u6807\u6ce8\u7684250\u7bc7\u6587\u7ae0\u4f5c\u4e3a\u8bc4\u4f30\u6570\u636e\u3002", "motivation": "5W1H\u95ee\u9898\u5728\u65b0\u95fb\u4e2d\u6781\u4e3a\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u7f3a\u4e4f\u4ece\u6cd5\u8bed\u65b0\u95fb\u4e2d\u81ea\u52a8\u63d0\u53d6\u8fd9\u4e9b\u4fe1\u606f\u7684\u5de5\u5177\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u4e00\u4e2a\u81ea\u52a8\u5316\u7cfb\u7edf\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u63d0\u53d65W1H\u4fe1\u606f\u7684\u7b97\u6cd5\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u7531\u4eba\u5de5\u6807\u6ce8\u7684250\u7bc7\u9b41\u5317\u514b\u65b0\u95fb\u6587\u7ae0\u7684\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u81ea\u52a8\u5316\u63d0\u53d6\u7cfb\u7edf\u7684\u6027\u80fd\u53ef\u4ee5\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578bGPT-4\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u81ea\u52a8\u53165W1H\u63d0\u53d6\u5de5\u5177\uff0c\u586b\u8865\u4e86\u6cd5\u8bed\u65b0\u95fb\u9886\u57df\u7684\u7a7a\u767d\u3002", "keywords": "5W1H, \u65b0\u95fb\u63d0\u53d6, \u6cd5\u8bed\u65b0\u95fb, \u81ea\u52a8\u5316, GPT-4"}}
{"id": "2505.14810", "pdf": "https://arxiv.org/pdf/2505.14810", "abs": "https://arxiv.org/abs/2505.14810", "authors": ["Tingchen Fu", "Jiawei Gu", "Yafu Li", "Xiaoye Qu", "Yu Cheng"], "title": "Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction-following is essential for aligning large language models (LLMs)\nwith user intent. While recent reasoning-oriented models exhibit impressive\nperformance on complex mathematical problems, their ability to adhere to\nnatural language instructions remains underexplored. In this work, we introduce\nMathIF, a dedicated benchmark for evaluating instruction-following in\nmathematical reasoning tasks. Our empirical analysis reveals a consistent\ntension between scaling up reasoning capacity and maintaining controllability,\nas models that reason more effectively often struggle to comply with user\ndirectives. We find that models tuned on distilled long chains-of-thought or\ntrained with reasoning-oriented reinforcement learning often degrade in\ninstruction adherence, especially when generation length increases.\nFurthermore, we show that even simple interventions can partially recover\nobedience, though at the cost of reasoning performance. These findings\nhighlight a fundamental tension in current LLM training paradigms and motivate\nthe need for more instruction-aware reasoning models. We release the code and\ndata at https://github.com/TingchenFu/MathIF.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86MathIF\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u63a8\u7406\u80fd\u529b\u63d0\u5347\u4e0e\u6307\u4ee4\u9075\u5faa\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "motivation": "\u63a2\u7d22\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u9075\u5faa\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u63d0\u5347\u65f6\u5f80\u5f80\u727a\u7272\u6307\u4ee4\u9075\u5faa\u6027\u3002", "method": "\u5f15\u5165MathIF\u57fa\u51c6\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u84b8\u998f\u957f\u94fe\u601d\u7ef4\u6216\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u540e\u7684\u6307\u4ee4\u9075\u5faa\u8868\u73b0\u3002", "result": "\u63a8\u7406\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u5728\u6307\u4ee4\u9075\u5faa\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u5c24\u5176\u662f\u751f\u6210\u957f\u5ea6\u589e\u52a0\u65f6\uff1b\u7b80\u5355\u5e72\u9884\u53ef\u90e8\u5206\u6062\u590d\u6307\u4ee4\u9075\u5faa\u6027\uff0c\u4f46\u4f1a\u727a\u7272\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u5f53\u524dLLM\u8bad\u7ec3\u8303\u5f0f\u5b58\u5728\u63a8\u7406\u80fd\u529b\u4e0e\u6307\u4ee4\u9075\u5faa\u6027\u7684\u51b2\u7a81\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5177\u6307\u4ee4\u611f\u77e5\u80fd\u529b\u7684\u63a8\u7406\u6a21\u578b\u3002", "keywords": "\u6307\u4ee4\u9075\u5faa\u3001\u6570\u5b66\u63a8\u7406\u3001\u8bed\u8a00\u6a21\u578b\u3001\u57fa\u51c6\u6d4b\u8bd5\u3001\u8bad\u7ec3\u51b2\u7a81"}}
{"id": "2505.14815", "pdf": "https://arxiv.org/pdf/2505.14815", "abs": "https://arxiv.org/abs/2505.14815", "authors": ["Mingyang Wang", "Lukas Lange", "Heike Adel", "Yunpu Ma", "Jannik Str\u00f6tgen", "Hinrich Sch\u00fctze"], "title": "Language Mixing in Reasoning Language Models: Patterns, Impact, and Internal Causes", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning language models (RLMs) excel at complex tasks by leveraging a\nchain-of-thought process to generate structured intermediate steps. However,\nlanguage mixing, i.e., reasoning steps containing tokens from languages other\nthan the prompt, has been observed in their outputs and shown to affect\nperformance, though its impact remains debated. We present the first systematic\nstudy of language mixing in RLMs, examining its patterns, impact, and internal\ncauses across 15 languages, 7 task difficulty levels, and 18 subject areas, and\nshow how all three factors influence language mixing. Moreover, we demonstrate\nthat the choice of reasoning language significantly affects performance:\nforcing models to reason in Latin or Han scripts via constrained decoding\nnotably improves accuracy. Finally, we show that the script composition of\nreasoning traces closely aligns with that of the model's internal\nrepresentations, indicating that language mixing reflects latent processing\npreferences in RLMs. Our findings provide actionable insights for optimizing\nmultilingual reasoning and open new directions for controlling reasoning\nlanguages to build more interpretable and adaptable RLMs.", "AI": {"tldr": "\u7cfb\u7edf\u7814\u7a76\u4e86\u63a8\u7406\u8bed\u8a00\u6a21\u578b\uff08RLMs\uff09\u4e2d\u7684\u8bed\u8a00\u6df7\u5408\u73b0\u8c61\uff0c\u5206\u6790\u4e86\u5176\u6a21\u5f0f\u3001\u5f71\u54cd\u53ca\u5185\u90e8\u539f\u56e0\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bed\u8a00\u9009\u62e9\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u4ee5\u53ca\u6a21\u578b\u5185\u90e8\u8868\u5f81\u4e0e\u8bed\u8a00\u6df7\u5408\u7684\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76RLMs\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u8bed\u8a00\u6df7\u5408\u73b0\u8c61\uff0c\u63a2\u8ba8\u5176\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u53ca\u5176\u5185\u90e8\u539f\u56e0\u3002", "method": "\u901a\u8fc715\u79cd\u8bed\u8a00\u30017\u79cd\u4efb\u52a1\u96be\u5ea6\u548c18\u4e2a\u4e3b\u9898\u9886\u57df\u7684\u5b9e\u9a8c\uff0c\u5206\u6790\u8bed\u8a00\u6df7\u5408\u7684\u6a21\u5f0f\u3001\u5f71\u54cd\u53ca\u5185\u90e8\u539f\u56e0\uff1b\u91c7\u7528\u7ea6\u675f\u89e3\u7801\u6cd5\u63a2\u7a76\u8bed\u8a00\u9009\u62e9\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5f3a\u5236\u6a21\u578b\u4f7f\u7528\u62c9\u4e01\u6216\u6c49\u8bed\u811a\u672c\u8fdb\u884c\u63a8\u7406\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff1b\u63a8\u7406\u8f68\u8ff9\u7684\u811a\u672c\u7ec4\u6210\u4e0e\u6a21\u578b\u5185\u90e8\u8868\u5f81\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u8bed\u8a00\u9009\u62e9\u5bf9RLMs\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f18\u5316\u591a\u8bed\u8a00\u63a8\u7406\u5e76\u63d0\u4f9b\u66f4\u53ef\u63a7\u7684\u63a8\u7406\u8bed\u8a00\u53ef\u63d0\u5347\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\u3002", "keywords": "\u63a8\u7406\u8bed\u8a00\u6a21\u578b, \u8bed\u8a00\u6df7\u5408, \u591a\u8bed\u8a00\u63a8\u7406, \u7ea6\u675f\u89e3\u7801, \u5185\u90e8\u8868\u5f81"}}
{"id": "2505.14700", "pdf": "https://arxiv.org/pdf/2505.14700", "abs": "https://arxiv.org/abs/2505.14700", "authors": ["R\u00f4mulo Damasclin Chaves dos Santos", "Jorge Henrique de Oliveira Sales"], "title": "Stochastic Fractional Neural Operators: A Symmetrized Approach to Modeling Turbulence in Complex Fluid Dynamics", "categories": ["cs.LG"], "comment": "17 pages", "summary": "In this work, we introduce a new class of neural network operators designed\nto handle problems where memory effects and randomness play a central role. In\nthis work, we introduce a new class of neural network operators designed to\nhandle problems where memory effects and randomness play a central role. These\noperators merge symmetrized activation functions, Caputo-type fractional\nderivatives, and stochastic perturbations introduced via It\\^o type noise. The\nresult is a powerful framework capable of approximating functions that evolve\nover time with both long-term memory and uncertain dynamics. We develop the\nmathematical foundations of these operators, proving three key theorems of\nVoronovskaya type. These results describe the asymptotic behavior of the\noperators, their convergence in the mean-square sense, and their consistency\nunder fractional regularity assumptions. All estimates explicitly account for\nthe influence of the memory parameter $\\alpha$ and the noise level $\\sigma$. As\na practical application, we apply the proposed theory to the fractional\nNavier-Stokes equations with stochastic forcing, a model often used to describe\nturbulence in fluid flows with memory. Our approach provides theoretical\nguarantees for the approximation quality and suggests that these neural\noperators can serve as effective tools in the analysis and simulation of\ncomplex systems. By blending ideas from neural networks, fractional calculus,\nand stochastic analysis, this research opens new perspectives for modeling\nturbulent phenomena and other multiscale processes where memory and randomness\nare fundamental. The results lay the groundwork for hybrid learning-based\nmethods with strong analytical backing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bb0\u5fc6\u6548\u5e94\u548c\u968f\u673a\u6027\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\uff0c\u901a\u8fc7\u5bf9\u79f0\u6fc0\u6d3b\u51fd\u6570\u3001Caputo\u578b\u5206\u6570\u9636\u5bfc\u6570\u548cIt\u00f4\u578b\u566a\u58f0\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u7684\u5206\u6790\u548c\u6a21\u62df\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "motivation": "\u9488\u5bf9\u5177\u6709\u957f\u671f\u8bb0\u5fc6\u548c\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u7684\u7cfb\u7edf\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u3001\u5206\u6570\u9636\u5fae\u79ef\u5206\u548c\u968f\u673a\u5206\u6790\uff0c\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u6570\u5b66\u5de5\u5177\u3002", "method": "\u91c7\u7528\u5bf9\u79f0\u6fc0\u6d3b\u51fd\u6570\u3001Caputo\u578b\u5206\u6570\u9636\u5bfc\u6570\u548cIt\u00f4\u578b\u566a\u58f0\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\uff0c\u5e76\u8bc1\u660e\u5176Voronovskaya\u578b\u5b9a\u7406\uff0c\u5305\u62ec\u6e10\u8fd1\u884c\u4e3a\u3001\u5747\u65b9\u6536\u655b\u548c\u4e00\u81f4\u6027\u3002", "result": "\u7406\u8bba\u5e94\u7528\u4e8e\u5206\u6570\u9636Navier-Stokes\u65b9\u7a0b\uff0c\u8bc1\u5b9e\u7b97\u5b50\u80fd\u6709\u6548\u8fd1\u4f3c\u5177\u6709\u8bb0\u5fc6\u548c\u968f\u673a\u6027\u7684\u6e4d\u6d41\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u590d\u6742\u7cfb\u7edf\u7684\u6df7\u5408\u5b66\u4e60\u65b9\u6cd5\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\u5728\u591a\u5c3a\u5ea6\u8fc7\u7a0b\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50,\u5206\u6570\u9636\u5bfc\u6570,\u968f\u673a\u5206\u6790,\u6e4d\u6d41\u6a21\u578b,Voronovskaya\u5b9a\u7406"}}
{"id": "2505.14689", "pdf": "https://arxiv.org/pdf/2505.14689", "abs": "https://arxiv.org/abs/2505.14689", "authors": ["Ashwani Anand", "Satya Prakash Nayak", "Ritam Raha", "Anne-Kathrin Schmuck"], "title": "Follow the STARs: Dynamic $\u03c9$-Regular Shielding of Learned Policies", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "This paper presents a novel dynamic post-shielding framework that enforces\nthe full class of $\\omega$-regular correctness properties over pre-computed\nprobabilistic policies. This constitutes a paradigm shift from the predominant\nsetting of safety-shielding -- i.e., ensuring that nothing bad ever happens --\nto a shielding process that additionally enforces liveness -- i.e., ensures\nthat something good eventually happens. At the core, our method uses\nStrategy-Template-based Adaptive Runtime Shields (STARs), which leverage\npermissive strategy templates to enable post-shielding with minimal\ninterference. As its main feature, STARs introduce a mechanism to dynamically\ncontrol interference, allowing a tunable enforcement parameter to balance\nformal obligations and task-specific behavior at runtime. This allows to\ntrigger more aggressive enforcement when needed, while allowing for optimized\npolicy choices otherwise. In addition, STARs support runtime adaptation to\nchanging specifications or actuator failures, making them especially suited for\ncyber-physical applications. We evaluate STARs on a mobile robot benchmark to\ndemonstrate their controllable interference when enforcing (incrementally\nupdated) $\\omega$-regular correctness properties over learned probabilistic\npolicies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u540e\u5c4f\u853d\u6846\u67b6\uff08STARs\uff09\uff0c\u7528\u4e8e\u5728\u9884\u8ba1\u7b97\u6982\u7387\u7b56\u7565\u4e0a\u5f3a\u5236\u6267\u884c\u03c9-\u6b63\u5219\u6b63\u786e\u6027\u5c5e\u6027\uff0c\u4ece\u5355\u7eaf\u7684\u5b89\u5168\u6027\u5c4f\u853d\u8f6c\u5411\u540c\u65f6\u786e\u4fdd\u6d3b\u6027\u7684\u5c4f\u853d\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5c06\u5c4f\u853d\u8fc7\u7a0b\u4ece\u4ec5\u786e\u4fdd\u5b89\u5168\u6027\u6269\u5c55\u5230\u540c\u65f6\u5f3a\u5236\u6267\u884c\u6d3b\u6027\uff0c\u4ee5\u6ee1\u8db3\u66f4\u5168\u9762\u7684\u6b63\u786e\u6027\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u9700\u8981\u52a8\u6001\u8c03\u6574\u7684\u590d\u6742\u7cfb\u7edf\uff08\u5982\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\uff09\u3002", "method": "\u65b9\u6cd5\u91c7\u7528\u4e86Strategy-Template-based Adaptive Runtime Shields\uff08STARs\uff09\uff0c\u901a\u8fc7\u7b56\u7565\u6a21\u677f\u5b9e\u73b0\u6700\u5c0f\u5e72\u6270\u7684\u540e\u5c4f\u853d\uff0c\u5e76\u52a8\u6001\u63a7\u5236\u5e72\u6270\u4ee5\u5e73\u8861\u5f62\u5f0f\u5316\u4e49\u52a1\u548c\u4efb\u52a1\u884c\u4e3a\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cSTARs\u80fd\u591f\u5728\u79fb\u52a8\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u52a8\u6001\u63a7\u5236\u5e72\u6270\uff0c\u5e76\u9002\u5e94\u53d8\u5316\u7684\u89c4\u8303\u6216\u6267\u884c\u5668\u6545\u969c\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "STARs\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u5f3a\u5927\u7684\u52a8\u6001\u5c4f\u853d\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u9700\u8981\u4e25\u683c\u5f62\u5f0f\u5316\u4fdd\u8bc1\u548c\u8fd0\u884c\u65f6\u9002\u5e94\u7684\u5e94\u7528\u573a\u666f\u3002", "keywords": "\u52a8\u6001\u5c4f\u853d, \u03c9-\u6b63\u5219\u5c5e\u6027, \u7b56\u7565\u6a21\u677f, \u8fd0\u884c\u65f6\u9002\u5e94, \u7f51\u7edc\u7269\u7406\u7cfb\u7edf"}}
{"id": "2505.14818", "pdf": "https://arxiv.org/pdf/2505.14818", "abs": "https://arxiv.org/abs/2505.14818", "authors": ["Leon Lin", "Jun Zheng", "Haidong Wang"], "title": "WebNovelBench: Placing LLM Novelists on the Web Novel Distribution", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Robustly evaluating the long-form storytelling capabilities of Large Language\nModels (LLMs) remains a significant challenge, as existing benchmarks often\nlack the necessary scale, diversity, or objective measures. To address this, we\nintroduce WebNovelBench, a novel benchmark specifically designed for evaluating\nlong-form novel generation. WebNovelBench leverages a large-scale dataset of\nover 4,000 Chinese web novels, framing evaluation as a synopsis-to-story\ngeneration task. We propose a multi-faceted framework encompassing eight\nnarrative quality dimensions, assessed automatically via an LLM-as-Judge\napproach. Scores are aggregated using Principal Component Analysis and mapped\nto a percentile rank against human-authored works. Our experiments demonstrate\nthat WebNovelBench effectively differentiates between human-written\nmasterpieces, popular web novels, and LLM-generated content. We provide a\ncomprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling\nabilities and offering insights for future development. This benchmark provides\na scalable, replicable, and data-driven methodology for assessing and advancing\nLLM-driven narrative generation.", "AI": {"tldr": "WebNovelBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u957f\u7bc7\u6545\u4e8b\u751f\u6210\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u57fa\u4e8e4000\u591a\u90e8\u4e2d\u6587\u7f51\u7edc\u5c0f\u8bf4\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\u533a\u5206\u4eba\u7c7b\u4f5c\u54c1\u4e0eLLM\u751f\u6210\u5185\u5bb9\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5728\u89c4\u6a21\u3001\u591a\u6837\u6027\u6216\u5ba2\u89c2\u6027\u4e0a\u4e0d\u8db3\uff0c\u96be\u4ee5\u8bc4\u4f30LLM\u7684\u957f\u7bc7\u53d9\u4e8b\u80fd\u529b\u3002", "method": "\u5229\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5c06\u8bc4\u4f30\u4efb\u52a1\u8bbe\u8ba1\u4e3a\u4ece\u6897\u6982\u751f\u6210\u6545\u4e8b\uff0c\u5e76\u901a\u8fc7LLM\u81ea\u52a8\u8bc4\u4f30\u516b\u4e2a\u53d9\u4e8b\u7ef4\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cWebNovelBench\u80fd\u6709\u6548\u533a\u5206\u4eba\u7c7b\u6770\u4f5c\u3001\u6d41\u884c\u7f51\u7edc\u5c0f\u8bf4\u548cLLM\u751f\u6210\u5185\u5bb9\uff0c\u5e76\u5bf924\u4e2a\u524d\u6cbfLLM\u8fdb\u884c\u4e86\u6392\u540d\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u53d9\u4e8b\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u4e14\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u3002", "keywords": "WebNovelBench, \u957f\u7bc7\u6545\u4e8b\u751f\u6210, LLM\u8bc4\u4f30, \u53d9\u4e8b\u8d28\u91cf"}}
{"id": "2505.14727", "pdf": "https://arxiv.org/pdf/2505.14727", "abs": "https://arxiv.org/abs/2505.14727", "authors": ["Mohammad Rubyet Islam"], "title": "The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents", "categories": ["cs.LG", "91G70 Statistical methods, risk measures 91B84 Economic models\n  (financial models, industrial models, growth models)", "I.2.6; I.5.1; I.2.7"], "comment": null, "summary": "The pursuit of alpha returns that exceed market benchmarks has undergone a\nprofound transformation, evolving from intuition-driven investing to\nautonomous, AI powered systems. This paper introduces a comprehensive five\nstage taxonomy that traces this progression across manual strategies,\nstatistical models, classical machine learning, deep learning, and agentic\narchitectures powered by large language models (LLMs). Unlike prior surveys\nfocused narrowly on modeling techniques, this review adopts a system level\nlens, integrating advances in representation learning, multimodal data fusion,\nand tool augmented LLM agents. The strategic shift from static predictors to\ncontextaware financial agents capable of real time reasoning, scenario\nsimulation, and cross modal decision making is emphasized. Key challenges in\ninterpretability, data fragility, governance, and regulatory compliance areas\ncritical to production deployment are examined. The proposed taxonomy offers a\nunified framework for evaluating maturity, aligning infrastructure, and guiding\nthe responsible development of next generation alpha systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e94\u9636\u6bb5\u5206\u7c7b\u6cd5\uff0c\u8ffd\u8e2a\u4ece\u76f4\u89c9\u9a71\u52a8\u6295\u8d44\u5230AI\u81ea\u4e3b\u7cfb\u7edf\u7684\u8f6c\u53d8\uff0c\u5f3a\u8c03\u7cfb\u7edf\u7ea7\u89c6\u89d2\u3001\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548cLLM\u4ee3\u7406\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u7814\u7a76\u8d85\u8d8a\u5e02\u573a\u57fa\u51c6\u7684alpha\u6536\u76ca\u83b7\u53d6\u65b9\u6cd5\u7684\u6f14\u53d8\uff0c\u4ece\u4f20\u7edf\u6295\u8d44\u5230AI\u9a71\u52a8\u7cfb\u7edf\u7684\u8f6c\u53d8\u3002", "method": "\u63d0\u51fa\u4e94\u9636\u6bb5\u5206\u7c7b\u6cd5\uff0c\u6db5\u76d6\u624b\u52a8\u7b56\u7565\u3001\u7edf\u8ba1\u6a21\u578b\u3001\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u53caLLM\u4ee3\u7406\u67b6\u6784\uff0c\u7cfb\u7edf\u6574\u5408\u8868\u5f81\u5b66\u4e60\u3001\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548c\u5de5\u5177\u589e\u5f3aLLM\u4ee3\u7406\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6210\u719f\u5ea6\u3001\u534f\u8c03\u57fa\u7840\u8bbe\u65bd\u548c\u6307\u5bfc\u4e0b\u4e00\u4ee3alpha\u7cfb\u7edf\u5f00\u53d1\u7684\u7edf\u4e00\u6846\u67b6\u3002", "conclusion": "\u5206\u7c7b\u6cd5\u4e3a\u4e0b\u4e00\u4ee3\u7406\u8d22\u7cfb\u7edf\u7684\u8d1f\u8d23\u4efb\u53d1\u5c55\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7ea7\u89c6\u89d2\u548c\u5b9e\u7528\u6307\u5bfc\u3002", "keywords": "alpha\u6536\u76ca\uff0cAI\u9a71\u52a8\u7cfb\u7edf\uff0c\u4e94\u9636\u6bb5\u5206\u7c7b\u6cd5\uff0cLLM\u4ee3\u7406\uff0c\u591a\u6a21\u6001\u6570\u636e\u878d\u5408"}}
{"id": "2505.14738", "pdf": "https://arxiv.org/pdf/2505.14738", "abs": "https://arxiv.org/abs/2505.14738", "authors": ["Xu Yang", "Xiao Yang", "Shikai Fang", "Bowen Xian", "Yuante Li", "Jian Wang", "Minrui Xu", "Haoran Pan", "Xinpeng Hong", "Weiqing Liu", "Yelong Shen", "Weizhu Chen", "Jiang Bian"], "title": "R&D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution", "categories": ["cs.AI"], "comment": "7 pages, 1 figure, 1 table", "summary": "Recent advances in AI and ML have transformed data science, yet increasing\ncomplexity and expertise requirements continue to hinder progress. While\ncrowdsourcing platforms alleviate some challenges, high-level data science\ntasks remain labor-intensive and iterative. To overcome these limitations, we\nintroduce R&D-Agent, a dual-agent framework for iterative exploration. The\nResearcher agent uses performance feedback to generate ideas, while the\nDeveloper agent refines code based on error feedback. By enabling multiple\nparallel exploration traces that merge and enhance one another, R&D-Agent\nnarrows the gap between automated solutions and expert-level performance.\nEvaluated on MLE-Bench, R&D-Agent emerges as the top-performing machine\nlearning engineering agent, demonstrating its potential to accelerate\ninnovation and improve precision across diverse data science applications. We\nhave open-sourced R&D-Agent on GitHub: https://github.com/microsoft/RD-Agent.", "AI": {"tldr": "R&D-Agent is a dual-agent framework designed to automate high-level data science tasks by combining iterative idea generation and code refinement, achieving top performance in machine learning engineering.", "motivation": "The complexity and expertise required for advanced data science tasks hinder progress, despite AI and ML advancements. Crowdsourcing platforms are insufficient for labor-intensive tasks.", "method": "A dual-agent framework: Researcher generates ideas using performance feedback; Developer refines code based on error feedback, enabling parallel exploration traces.", "result": "R&D-Agent outperforms others on MLE-Bench, showing potential to accelerate innovation and improve precision in data science applications.", "conclusion": "R&D-Agent effectively bridges the gap between automated solutions and expert-level performance, with open-source availability on GitHub.", "keywords": "AI, machine learning, data science, automation, R&D-Agent, iterative exploration"}}
{"id": "2505.14824", "pdf": "https://arxiv.org/pdf/2505.14824", "abs": "https://arxiv.org/abs/2505.14824", "authors": ["Yihong Liu", "Mingyang Wang", "Amir Hossein Kargaran", "Felicia K\u00f6rner", "Ercong Nie", "Barbara Plank", "Fran\u00e7ois Yvon", "Hinrich Sch\u00fctze"], "title": "Tracing Multilingual Factual Knowledge Acquisition in Pretraining", "categories": ["cs.CL"], "comment": "preprint", "summary": "Large Language Models (LLMs) are capable of recalling multilingual factual\nknowledge present in their pretraining data. However, most studies evaluate\nonly the final model, leaving the development of factual recall and\ncrosslingual consistency throughout pretraining largely unexplored. In this\nwork, we trace how factual recall and crosslingual consistency evolve during\npretraining, focusing on OLMo-7B as a case study. We find that both accuracy\nand consistency improve over time for most languages. We show that this\nimprovement is primarily driven by the fact frequency in the pretraining\ncorpus: more frequent facts are more likely to be recalled correctly,\nregardless of language. Yet, some low-frequency facts in non-English languages\ncan still be correctly recalled. Our analysis reveals that these instances\nlargely benefit from crosslingual transfer of their English counterparts -- an\neffect that emerges predominantly in the early stages of pretraining. We\npinpoint two distinct pathways through which multilingual factual knowledge\nacquisition occurs: (1) frequency-driven learning, which is dominant and\nlanguage-agnostic, and (2) crosslingual transfer, which is limited in scale and\ntypically constrained to relation types involving named entities. We release\nour code and data to facilitate further research at\nhttps://github.com/cisnlp/multilingual-fact-tracing.", "AI": {"tldr": "\u672c\u7814\u7a76\u8ffd\u8e2aOLMo-7B\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u591a\u8bed\u8a00\u4e8b\u5b9e\u8bb0\u5fc6\u548c\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\u7684\u6f14\u53d8\uff0c\u53d1\u73b0\u5176\u6539\u8fdb\u4e3b\u8981\u53d7\u9884\u8bad\u7ec3\u8bed\u6599\u4e2d\u4e8b\u5b9e\u9891\u7387\u5f71\u54cd\uff0c\u540c\u65f6\u63ed\u793a\u8de8\u8bed\u8a00\u8fc1\u79fb\u5728\u65e9\u671f\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u4fc3\u8fdb\u4f5c\u7528\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u591a\u8bed\u8a00\u4e8b\u5b9e\u8bb0\u5fc6\u548c\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\u7684\u53d1\u5c55\u52a8\u6001\uff0c\u586b\u8865\u5f53\u524d\u7814\u7a76\u591a\u5173\u6ce8\u6700\u7ec8\u6a21\u578b\u7684\u7a7a\u767d\u3002", "method": "\u4ee5OLMo-7B\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u5176\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u53d8\u5316\uff0c\u7ed3\u5408\u4e8b\u5b9e\u9891\u7387\u548c\u8de8\u8bed\u8a00\u8fc1\u79fb\u6548\u5e94\u8fdb\u884c\u5256\u6790\u3002", "result": "\u4e8b\u5b9e\u9891\u7387\u4e3b\u5bfc\u6a21\u578b\u7684\u591a\u8bed\u8a00\u8bb0\u5fc6\u80fd\u529b\uff0c\u4f4e\u9891\u7387\u975e\u82f1\u8bed\u4e8b\u5b9e\u901a\u8fc7\u82f1\u8bed\u5bf9\u5e94\u4fe1\u606f\u7684\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b9e\u73b0\u6b63\u786e\u56de\u5fc6\u3002\u8de8\u8bed\u8a00\u8fc1\u79fb\u4e3b\u8981\u5b58\u5728\u4e8e\u6d89\u53ca\u547d\u540d\u5b9e\u4f53\u7684\u5173\u7cfb\u7c7b\u578b\u4e2d\u3002", "conclusion": "\u591a\u8bed\u8a00\u4e8b\u5b9e\u83b7\u53d6\u901a\u8fc7\u9891\u7387\u9a71\u52a8\u5b66\u4e60\u548c\u8de8\u8bed\u8a00\u8fc1\u79fb\u4e24\u6761\u8def\u5f84\u5b9e\u73b0\uff0c\u524d\u8005\u66f4\u4e3a\u666e\u904d\u4e14\u8bed\u8a00\u65e0\u5173\uff0c\u540e\u8005\u89c4\u6a21\u6709\u9650\u4e14\u96c6\u4e2d\u4e8e\u7279\u5b9a\u5173\u7cfb\u7c7b\u578b\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u591a\u8bed\u8a00\u4e8b\u5b9e\u8bb0\u5fc6\u3001\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\u3001\u9884\u8bad\u7ec3\u3001\u8de8\u8bed\u8a00\u8fc1\u79fb"}}
{"id": "2505.14733", "pdf": "https://arxiv.org/pdf/2505.14733", "abs": "https://arxiv.org/abs/2505.14733", "authors": ["Yunho Jin", "Gu-Yeon Wei", "David Brooks"], "title": "The Energy Cost of Reasoning: Analyzing Energy Usage in LLMs with Test-time Compute", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Scaling large language models (LLMs) has driven significant advancements, yet\nit faces diminishing returns and escalating energy demands. This work\nintroduces test-time compute (TTC)-allocating additional computational\nresources during inference-as a compelling complement to conventional scaling\nstrategies. Specifically, we investigate whether employing TTC can achieve\nsuperior accuracy-energy trade-offs compared to simply increasing model size.\nOur empirical analysis reveals that TTC surpasses traditional model scaling in\naccuracy/energy efficiency, with notable gains in tasks demanding complex\nreasoning rather than mere factual recall. Further, we identify a critical\ninteraction between TTC performance and output sequence length, demonstrating\nthat strategically adjusting compute resources at inference time according to\nquery complexity can substantially enhance efficiency. Our findings advocate\nfor TTC as a promising direction, enabling more sustainable, accurate, and\nadaptable deployment of future language models without incurring additional\npretraining costs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u63a8\u7406\u9636\u6bb5\u5206\u914d\u989d\u5916\u8ba1\u7b97\u8d44\u6e90\uff08TTC\uff09\u4f5c\u4e3a\u4f20\u7edf\u6a21\u578b\u6269\u5c55\u7684\u8865\u5145\uff0c\u4ee5\u63d0\u5347\u51c6\u786e\u6027\u4e0e\u80fd\u6e90\u6548\u7387\u7684\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6269\u5c55\u9762\u4e34\u6536\u76ca\u9012\u51cf\u548c\u80fd\u8017\u589e\u52a0\u7684\u95ee\u9898\uff0c\u7814\u7a76TTC\u662f\u5426\u80fd\u63d0\u4f9b\u66f4\u4f18\u7684\u51c6\u786e\u6027\u4e0e\u80fd\u6e90\u6548\u7387\u6743\u8861\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u6bd4\u8f83TTC\u4e0e\u4f20\u7edf\u6a21\u578b\u6269\u5c55\u5728\u4efb\u52a1\u8868\u73b0\u548c\u80fd\u6e90\u6d88\u8017\u4e0a\u7684\u5dee\u5f02\uff0c\u5e76\u7814\u7a76TTC\u4e0e\u8f93\u51fa\u5e8f\u5217\u957f\u5ea6\u7684\u5173\u7cfb\u3002", "result": "TTC\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u6bd4\u4f20\u7edf\u6269\u5c55\u66f4\u9ad8\u6548\uff0c\u4e14\u53ef\u6839\u636e\u67e5\u8be2\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\u63d0\u5347\u6548\u7387\u3002", "conclusion": "TTC\u662f\u4e00\u79cd\u65e0\u9700\u989d\u5916\u9884\u8bad\u7ec3\u6210\u672c\u5373\u53ef\u5b9e\u73b0\u53ef\u6301\u7eed\u3001\u51c6\u786e\u4e14\u7075\u6d3b\u90e8\u7f72\u8bed\u8a00\u6a21\u578b\u7684\u6709\u524d\u666f\u65b9\u6cd5\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u63a8\u7406\u8ba1\u7b97, \u80fd\u6e90\u6548\u7387, \u6a21\u578b\u6269\u5c55, TTC"}}
{"id": "2505.14932", "pdf": "https://arxiv.org/pdf/2505.14932", "abs": "https://arxiv.org/abs/2505.14932", "authors": ["Isabelle Lee", "Sarah Liaw", "Dani Yogatama"], "title": "FOL-Pretrain: A complexity annotated corpus of first-order logic", "categories": ["cs.AI"], "comment": null, "summary": "Transformer-based large language models (LLMs) have demonstrated remarkable\nreasoning capabilities such as coding and solving mathematical problems to\ncommonsense inference. While these tasks vary in complexity, they all require\nmodels to integrate and compute over structured information. Despite recent\nefforts to reverse-engineer LLM behavior through controlled experiments, our\nunderstanding of how these models internalize and execute complex algorithms\nremains limited. Progress has largely been confined to small-scale studies or\nshallow tasks such as basic arithmetic and grammatical pattern matching. One\nbarrier to deeper understanding is the nature of pretraining data -- vast,\nheterogeneous, and often poorly annotated, making it difficult to isolate\nmechanisms of reasoning. To bridge this gap, we introduce a large-scale, fully\nopen, complexity-annotated dataset of first-order logic reasoning traces,\ndesigned to probe and analyze algorithmic reasoning in LLMs. The dataset\nconsists of 3.5 billion tokens, including 8.8 million LLM-augmented,\nhuman-annotated examples and 7.5 million synthetically generated examples. Each\nsynthetic example is verifiably correct, produced by a custom automated theorem\nsolver, and accompanied by metadata tracing its algorithmic provenance. We aim\nto provide a scalable, interpretable artifact for studying how LLMs learn and\ngeneralize symbolic reasoning processes, paving the way for more transparent\nand targeted investigations into the algorithmic capabilities of modern models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u5b8c\u5168\u5f00\u653e\u4e14\u590d\u6742\u5ea6\u6807\u6ce8\u7684\u4e00\u9636\u903b\u8f91\u63a8\u7406\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7b97\u6cd5\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u6267\u884c\u590d\u6742\u7b97\u6cd5\u7684\u5185\u90e8\u673a\u5236\u4ecd\u7136\u4e0d\u660e\u3002\u73b0\u6709\u7814\u7a76\u591a\u5c40\u9650\u4e8e\u5c0f\u89c4\u6a21\u6216\u6d45\u5c42\u6b21\u4efb\u52a1\uff0c\u65e0\u6cd5\u6df1\u5165\u7406\u89e3\u6a21\u578b\u5982\u4f55\u5b66\u4e60\u548c\u6267\u884c\u590d\u6742\u7684\u7b26\u53f7\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u5305\u542b35\u4ebf\u6807\u8bb0\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u62ec880\u4e07\u7531LLM\u589e\u5f3a\u7684\u4eba\u5de5\u6807\u6ce8\u793a\u4f8b\u548c750\u4e07\u5408\u6210\u7684\u81ea\u52a8\u5b9a\u7406\u6c42\u89e3\u5668\u751f\u6210\u7684\u53ef\u9a8c\u8bc1\u6b63\u786e\u793a\u4f8b\uff0c\u6765\u7cfb\u7edf\u5206\u6790LLMs\u7684\u7b97\u6cd5\u63a8\u7406\u884c\u4e3a\u3002", "result": "\u6570\u636e\u96c6\u4e3a\u7814\u7a76LLMs\u5982\u4f55\u5b66\u4e60\u548c\u6cdb\u5316\u7b26\u53f7\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u66f4\u900f\u660e\u5730\u63a2\u7d22\u73b0\u4ee3\u6a21\u578b\u7684\u7b97\u6cd5\u80fd\u529b\u3002", "conclusion": "\u8fd9\u4e00\u7814\u7a76\u4e3a\u6df1\u5165\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u673a\u5236\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u6765\u9488\u5bf9\u5176\u7b97\u6cd5\u80fd\u529b\u7684\u900f\u660e\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u4e00\u9636\u903b\u8f91\u63a8\u7406, \u6570\u636e\u96c6, \u7b97\u6cd5\u63a8\u7406, \u7b26\u53f7\u63a8\u7406"}}
{"id": "2505.14827", "pdf": "https://arxiv.org/pdf/2505.14827", "abs": "https://arxiv.org/abs/2505.14827", "authors": ["Yufan Zhuang", "Liyuan Liu", "Chandan Singh", "Jingbo Shang", "Jianfeng Gao"], "title": "Text Generation Beyond Discrete Token Sampling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In standard autoregressive generation, an LLM predicts the next-token\ndistribution, samples a discrete token, and then discards the distribution,\npassing only the sampled token as new input. To preserve this distribution's\nrich information, we propose Mixture of Inputs (MoI), a training-free method\nfor autoregressive generation. After generating a token following the standard\nparadigm, we construct a new input that blends the generated discrete token\nwith the previously discarded token distribution. Specifically, we employ a\nBayesian estimation method that treats the token distribution as the prior, the\nsampled token as the observation, and replaces the conventional one-hot vector\nwith the continuous posterior expectation as the new model input. MoI allows\nthe model to maintain a richer internal representation throughout the\ngeneration process, resulting in improved text quality and reasoning\ncapabilities. On mathematical reasoning, code generation, and PhD-level QA\ntasks, MoI consistently improves performance across multiple models including\nQwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional\ntraining and negligible computational overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u52a8\u56de\u5f52\u751f\u6210\u65b9\u6cd5Mixture of Inputs (MoI)\uff0c\u901a\u8fc7\u4fdd\u7559\u4e22\u5f03\u7684\u4ee4\u724c\u5206\u5e03\u4fe1\u606f\uff0c\u63d0\u5347\u751f\u6210\u6587\u672c\u7684\u8d28\u91cf\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u81ea\u52a8\u56de\u5f52\u751f\u6210\u65b9\u6cd5\u4f1a\u4e22\u5f03\u4e0b\u4e00\u4ee4\u724c\u7684\u5206\u5e03\u4fe1\u606f\uff0c\u5bfc\u81f4\u4fe1\u606f\u635f\u5931\u3002MoI\u65e8\u5728\u4fdd\u7559\u8fd9\u4e9b\u4fe1\u606f\u4ee5\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "method": "MoI\u5728\u751f\u6210\u4ee4\u724c\u540e\uff0c\u5c06\u79bb\u6563\u4ee4\u724c\u4e0e\u4e4b\u524d\u4e22\u5f03\u7684\u5206\u5e03\u4fe1\u606f\u7ed3\u5408\uff0c\u91c7\u7528\u8d1d\u53f6\u65af\u4f30\u8ba1\u65b9\u6cd5\u751f\u6210\u8fde\u7eed\u540e\u9a8c\u671f\u671b\u4f5c\u4e3a\u65b0\u8f93\u5165\u3002", "result": "MoI\u5728\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u535a\u58eb\u7ea7QA\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2a\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "MoI\u901a\u8fc7\u4fdd\u7559\u5206\u5e03\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u56de\u5f52\u751f\u6210\u7684\u8d28\u91cf\u548c\u63a8\u7406\u80fd\u529b\u3002", "keywords": "\u81ea\u52a8\u56de\u5f52\u751f\u6210, Mixture of Inputs, \u8d1d\u53f6\u65af\u4f30\u8ba1, \u6587\u672c\u751f\u6210, \u63a8\u7406\u80fd\u529b"}}
{"id": "2505.14737", "pdf": "https://arxiv.org/pdf/2505.14737", "abs": "https://arxiv.org/abs/2505.14737", "authors": ["Huiliang Zhang", "Di Wu", "Arnaud Zinflou", "Stephane Dellacherie", "Mouhamadou Makhtar Dione", "Benoit Boulet"], "title": "Leveraging Multivariate Long-Term History Representation for Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multivariate Time Series (MTS) forecasting has a wide range of applications\nin both industry and academia. Recent advances in Spatial-Temporal Graph Neural\nNetwork (STGNN) have achieved great progress in modelling spatial-temporal\ncorrelations. Limited by computational complexity, most STGNNs for MTS\nforecasting focus primarily on short-term and local spatial-temporal\ndependencies. Although some recent methods attempt to incorporate univariate\nhistory into modeling, they still overlook crucial long-term spatial-temporal\nsimilarities and correlations across MTS, which are essential for accurate\nforecasting. To fill this gap, we propose a framework called the Long-term\nMultivariate History Representation (LMHR) Enhanced STGNN for MTS forecasting.\nSpecifically, a Long-term History Encoder (LHEncoder) is adopted to effectively\nencode the long-term history into segment-level contextual representations and\nreduce point-level noise. A non-parametric Hierarchical Representation\nRetriever (HRetriever) is designed to include the spatial information in the\nlong-term spatial-temporal dependency modelling and pick out the most valuable\nrepresentations with no additional training. A Transformer-based Aggregator\n(TAggregator) selectively fuses the sparsely retrieved contextual\nrepresentations based on the ranking positional embedding efficiently.\nExperimental results demonstrate that LMHR outperforms typical STGNNs by 10.72%\non the average prediction horizons and state-of-the-art methods by 4.12% on\nseveral real-world datasets. Additionally, it consistently improves prediction\naccuracy by 9.8% on the top 10% of rapidly changing patterns across the\ndatasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLMHR\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3aSTGNN\u7684\u957f\u65f6\u7a7a\u4f9d\u8d56\u5efa\u6a21\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709STGNN\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u77ed\u671f\u548c\u5c40\u90e8\u7684\u65f6\u7a7a\u4f9d\u8d56\uff0c\u5ffd\u7565\u4e86\u957f\u671f\u65f6\u7a7a\u76f8\u4f3c\u6027\u548c\u76f8\u5173\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "LMHR\u7ed3\u5408\u4e86\u957f\u65f6\u5386\u53f2\u7f16\u7801\u5668(LHEncoder)\u3001\u5c42\u6b21\u8868\u793a\u68c0\u7d22\u5668(HRetriever)\u548c\u57fa\u4e8eTransformer\u7684\u805a\u5408\u5668(TAggregator)\uff0c\u4ee5\u9ad8\u6548\u5efa\u6a21\u957f\u671f\u65f6\u7a7a\u4f9d\u8d56\u3002", "result": "LMHR\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5178\u578bSTGNN\u65b9\u6cd510.72%\uff0c\u5e76\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd54.12%\uff0c\u4e14\u5728\u5feb\u901f\u53d8\u5316\u6a21\u5f0f\u4e0a\u7684\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\u4e869.8%\u3002", "conclusion": "LMHR\u901a\u8fc7\u6709\u6548\u5efa\u6a21\u957f\u671f\u65f6\u7a7a\u4f9d\u8d56\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\u3002", "keywords": "\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b, \u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc, \u957f\u671f\u4f9d\u8d56\u5efa\u6a21, LMHR\u6846\u67b6"}}
{"id": "2505.14940", "pdf": "https://arxiv.org/pdf/2505.14940", "abs": "https://arxiv.org/abs/2505.14940", "authors": ["Kaspar Rothenfusser"], "title": "To Be or Not To Be: Vector ontologies as a truly formal ontological framework", "categories": ["cs.AI", "cs.SC"], "comment": null, "summary": "Since Edmund Husserl coined the term \"Formal Ontologies\" in the early 20th\ncentury, a field that identifies itself with this particular branch of sciences\nhas gained increasing attention. Many authors, and even Husserl himself have\ndeveloped what they claim to be formal ontologies. I argue that under close\ninspection, none of these so claimed formal ontologies are truly formal in the\nHusserlian sense. More concretely, I demonstrate that they violate the two most\nimportant notions of formal ontology as developed in Husserl's Logical\nInvestigations, namely a priori validity independent of perception and\nformalism as the total absence of content. I hence propose repositioning the\nwork previously understood as formal ontology as the foundational ontology it\nreally is. This is to recognize the potential of a truly formal ontology in the\nHusserlian sense. Specifically, I argue that formal ontology following his\nconditions, allows us to formulate ontological structures, which could capture\nwhat is more objectively without presupposing a particular framework arising\nfrom perception. I further argue that the ability to design the formal\nstructure deliberately allows us to create highly scalable and interoperable\ninformation artifacts. As concrete evidence, I showcase that a class of formal\nontology, which uses the axioms of vector spaces, is able to express most of\nthe conceptualizations found in foundational ontologies. Most importantly, I\nargue that many information systems, specifically artificial intelligence, are\nlikely already using some type of vector ontologies to represent reality in\ntheir internal worldviews and elaborate on the evidence that humans do as well.\nI hence propose a thorough investigation of the ability of vector ontologies to\nact as a human-machine interoperable ontological framework that allows us to\nunderstand highly sophisticated machines and machines to understand us.", "AI": {"tldr": "\u4f5c\u8005\u8ba4\u4e3a\u73b0\u6709\u6240\u8c13\u7684\u2018\u5f62\u5f0f\u672c\u4f53\u8bba\u2019\u5e76\u672a\u771f\u6b63\u7b26\u5408\u80e1\u585e\u5c14\u63d0\u51fa\u7684\u5f62\u5f0f\u672c\u4f53\u8bba\u6807\u51c6\uff0c\u5e76\u5efa\u8bae\u5c06\u5176\u91cd\u65b0\u5b9a\u4f4d\u4e3a\u57fa\u7840\u672c\u4f53\u8bba\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u5411\u91cf\u7a7a\u95f4\u4f5c\u4e3a\u4e00\u79cd\u771f\u6b63\u7b26\u5408\u80e1\u585e\u5c14\u6761\u4ef6\u7684\u5f62\u5f0f\u672c\u4f53\u8bba\uff0c\u53ef\u4f5c\u4e3a\u4eba\u673a\u4e92\u64cd\u4f5c\u7684\u672c\u4f53\u6846\u67b6\u3002", "motivation": "\u80e1\u585e\u5c14\u63d0\u51fa\u7684\u2018\u5f62\u5f0f\u672c\u4f53\u8bba\u2019\u5e38\u88ab\u8bef\u7528\uff0c\u4f5c\u8005\u65e8\u5728\u6f84\u6e05\u5176\u771f\u6b63\u542b\u4e49\uff0c\u5e76\u63a2\u8ba8\u4e00\u79cd\u7b26\u5408\u5176\u6807\u51c6\u7684\u672c\u4f53\u8bba\u5f62\u5f0f\uff0c\u4ee5\u89e3\u51b3\u4eba\u673a\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5206\u6790\u80e1\u585e\u5c14\u7684\u300a\u903b\u8f91\u7814\u7a76\u300b\uff0c\u6307\u51fa\u73b0\u6709\u2018\u5f62\u5f0f\u672c\u4f53\u8bba\u2019\u8fdd\u53cd\u2018\u5148\u9a8c\u6709\u6548\u6027\u2019\u548c\u2018\u5b8c\u5168\u65e0\u5185\u5bb9\u7684\u5f62\u5f0f\u5316\u2019\u4e24\u4e2a\u5173\u952e\u6807\u51c6\u3002\u968f\u540e\uff0c\u63d0\u51fa\u4ee5\u5411\u91cf\u7a7a\u95f4\u4e3a\u57fa\u7840\u7684\u771f\u6b63\u5f62\u5f0f\u672c\u4f53\u8bba\uff0c\u5e76\u9a8c\u8bc1\u5176\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5411\u91cf\u7a7a\u95f4\u672c\u4f53\u8bba\u80fd\u591f\u8868\u8fbe\u5927\u591a\u6570\u57fa\u7840\u672c\u4f53\u8bba\u7684\u6982\u5ff5\u5316\uff0c\u4e14\u53ef\u80fd\u662f\u73b0\u6709\u4fe1\u606f\u7cfb\u7edf\u4e2d\u5df2\u4f7f\u7528\u7684\u5f62\u5f0f\u3002", "conclusion": "\u5411\u91cf\u7a7a\u95f4\u672c\u4f53\u8bba\u7b26\u5408\u80e1\u585e\u5c14\u7684\u6807\u51c6\uff0c\u5e76\u5177\u6709\u6210\u4e3a\u4eba\u673a\u4e92\u64cd\u4f5c\u6846\u67b6\u7684\u6f5c\u529b\u3002", "keywords": "\u5f62\u5f0f\u672c\u4f53\u8bba\u3001\u80e1\u585e\u5c14\u3001\u5411\u91cf\u7a7a\u95f4\u3001\u4eba\u673a\u4e92\u64cd\u4f5c\u3001\u57fa\u7840\u672c\u4f53\u8bba"}}
{"id": "2505.14832", "pdf": "https://arxiv.org/pdf/2505.14832", "abs": "https://arxiv.org/abs/2505.14832", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Albert No"], "title": "SEPS: A Separability Measure for Robust Unlearning in LLMs", "categories": ["cs.CL"], "comment": "32 pages", "summary": "Machine unlearning aims to selectively remove targeted knowledge from Large\nLanguage Models (LLMs), ensuring they forget specified content while retaining\nessential information. Existing unlearning metrics assess whether a model\ncorrectly answers retain queries and rejects forget queries, but they fail to\ncapture real-world scenarios where forget queries rarely appear in isolation.\nIn fact, forget and retain queries often coexist within the same prompt, making\nmixed-query evaluation crucial.\n  We introduce SEPS, an evaluation framework that explicitly measures a model's\nability to both forget and retain information within a single prompt. Through\nextensive experiments across three benchmarks, we identify two key failure\nmodes in existing unlearning methods: (1) untargeted unlearning\nindiscriminately erases both forget and retain content once a forget query\nappears, and (2) targeted unlearning overfits to single-query scenarios,\nleading to catastrophic failures when handling multiple queries. To address\nthese issues, we propose Mixed Prompt (MP) unlearning, a strategy that\nintegrates both forget and retain queries into a unified training objective.\nOur approach significantly improves unlearning effectiveness, demonstrating\nrobustness even in complex settings with up to eight mixed forget and retain\nqueries in a single prompt.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SEPS\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8861\u91cf\u6a21\u578b\u5728\u5355\u63d0\u793a\u4e2d\u540c\u65f6\u9057\u5fd8\u548c\u4fdd\u7559\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u7684\u4e24\u4e2a\u5173\u952e\u5931\u8d25\u6a21\u5f0f\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86Mixed Prompt\uff08MP\uff09\u9057\u5fd8\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9057\u5fd8\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u6307\u6807\u672a\u80fd\u6355\u6349\u73b0\u5b9e\u573a\u666f\u4e2d\u9057\u5fd8\u548c\u4fdd\u7559\u67e5\u8be2\u5171\u5b58\u7684\u60c5\u51b5\uff0c\u5bfc\u81f4\u8bc4\u4f30\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51faSEPS\u8bc4\u4f30\u6846\u67b6\u548cMixed Prompt\uff08MP\uff09\u9057\u5fd8\u7b56\u7565\uff0c\u5c06\u9057\u5fd8\u548c\u4fdd\u7559\u67e5\u8be2\u6574\u5408\u4e3a\u7edf\u4e00\u7684\u8bad\u7ec3\u76ee\u6807\u3002", "result": "MP\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u9057\u5fd8\u6548\u679c\uff0c\u5728\u590d\u6742\u8bbe\u7f6e\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u5373\u4f7f\u5355\u63d0\u793a\u5305\u542b\u591a\u8fbe\u516b\u4e2a\u6df7\u5408\u67e5\u8be2\u3002", "conclusion": "SEPS\u548cMP\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u6df7\u5408\u67e5\u8be2\u573a\u666f\u4e2d\u7684\u9057\u5fd8\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u673a\u5668\u9057\u5fd8\u3001LLM\u3001\u6df7\u5408\u67e5\u8be2\u3001SEPS\u3001MP\u7b56\u7565"}}
{"id": "2505.14739", "pdf": "https://arxiv.org/pdf/2505.14739", "abs": "https://arxiv.org/abs/2505.14739", "authors": ["Heiko Oppel", "Andreas Spilz", "Michael Munz"], "title": "Time Series Similarity Score Functions to Monitor and Interact with the Training and Denoising Process of a Time Series Diffusion Model applied to a Human Activity Recognition Dataset based on IMUs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Denoising diffusion probabilistic models are able to generate synthetic\nsensor signals. The training process of such a model is controlled by a loss\nfunction which measures the difference between the noise that was added in the\nforward process and the noise that was predicted by the diffusion model. This\nenables the generation of realistic data. However, the randomness within the\nprocess and the loss function itself makes it difficult to estimate the quality\nof the data. Therefore, we examine multiple similarity metrics and adapt an\nexisting metric to overcome this issue by monitoring the training and\nsynthetisation process using those metrics. The adapted metric can even be\nfine-tuned on the input data to comply with the requirements of an underlying\nclassification task. We were able to significantly reduce the amount of\ntraining epochs without a performance reduction in the classification task. An\noptimized training process not only saves resources, but also reduces the time\nfor training generative models.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u5728\u751f\u6210\u5408\u6210\u4f20\u611f\u5668\u4fe1\u53f7\u65f6\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4ee5\u964d\u4f4e\u8bad\u7ec3\u5468\u671f\u540c\u65f6\u4fdd\u6301\u5206\u7c7b\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u5728\u751f\u6210\u6570\u636e\u65f6\u96be\u4ee5\u8bc4\u4f30\u6570\u636e\u8d28\u91cf\uff0c\u9700\u8981\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u4ee5\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u548c\u65f6\u95f4\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u591a\u79cd\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e76\u5bf9\u73b0\u6709\u5ea6\u91cf\u8fdb\u884c\u9002\u914d\uff0c\u4ee5\u76d1\u63a7\u8bad\u7ec3\u548c\u5408\u6210\u8fc7\u7a0b\uff1b\u6539\u8fdb\u7684\u5ea6\u91cf\u65b9\u6cd5\u53ef\u6839\u636e\u5206\u7c7b\u4efb\u52a1\u9700\u6c42\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u6539\u8fdb\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u5468\u671f\uff0c\u540c\u65f6\u672a\u964d\u4f4e\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u4f18\u5316\u540e\u7684\u8bad\u7ec3\u8fc7\u7a0b\u8282\u7701\u4e86\u8d44\u6e90\u548c\u65f6\u95f4\uff0c\u63d0\u5347\u4e86\u751f\u6210\u6a21\u578b\u7684\u6548\u7387\u3002", "keywords": "\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b, \u76f8\u4f3c\u6027\u5ea6\u91cf, \u6570\u636e\u751f\u6210, \u8bad\u7ec3\u4f18\u5316, \u5206\u7c7b\u4efb\u52a1"}}
{"id": "2505.14946", "pdf": "https://arxiv.org/pdf/2505.14946", "abs": "https://arxiv.org/abs/2505.14946", "authors": ["Eric Han", "Jun Chen", "Karthik Abinav Sankararaman", "Xiaoliang Peng", "Tengyu Xu", "Eryk Helenowski", "Kaiyan Peng", "Mrinal Kumar", "Sinong Wang", "Han Fang", "Arya Talebzadeh"], "title": "Reinforcement Learning from User Feedback", "categories": ["cs.AI"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in diverse user\nfacing applications, aligning them with real user preferences becomes\nessential. Existing methods like Reinforcement Learning from Human Feedback\n(RLHF) rely on expert annotators trained on manually defined guidelines, whose\njudgments may not reflect the priorities of everyday users. We introduce\nReinforcement Learning from User Feedback (RLUF), a framework for aligning LLMs\ndirectly to implicit signals from users in production. RLUF addresses key\nchallenges of user feedback: user feedback is often binary (e.g., emoji\nreactions), sparse, and occasionally adversarial. We train a reward model,\nP[Love], to predict the likelihood that an LLM response will receive a Love\nReaction, a lightweight form of positive user feedback, and integrate P[Love]\ninto a multi-objective policy optimization framework alongside helpfulness and\nsafety objectives. In large-scale experiments, we show that P[Love] is\npredictive of increased positive feedback and serves as a reliable offline\nevaluator of future user behavior. Policy optimization using P[Love]\nsignificantly raises observed positive-feedback rates, including a 28% increase\nin Love Reactions during live A/B tests. However, optimizing for positive\nreactions introduces reward hacking challenges, requiring careful balancing of\nobjectives. By directly leveraging implicit signals from users, RLUF offers a\npath to aligning LLMs with real-world user preferences at scale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRLUF\u6846\u67b6\uff0c\u76f4\u63a5\u5229\u7528\u7528\u6237\u53cd\u9988\u4fe1\u53f7\u5bf9\u9f50\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u901a\u8fc7\u8bad\u7ec3\u9884\u6d4b\u6a21\u578bP[Love]\u5e76\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u7528\u6237\u6b63\u9762\u53cd\u9988\u7387\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u6807\u6ce8\uff0c\u96be\u4ee5\u53cd\u6620\u771f\u5b9e\u7528\u6237\u504f\u597d\uff0cRLUF\u65e8\u5728\u76f4\u63a5\u4ece\u7528\u6237\u751f\u4ea7\u73af\u5883\u7684\u9690\u5f0f\u53cd\u9988\u4e2d\u5b66\u4e60\u3002", "method": "\u8bad\u7ec3\u5956\u52b1\u6a21\u578bP[Love]\u9884\u6d4b\u7528\u6237\u6b63\u9762\u53cd\u9988\uff08\u5982\u70b9\u8d5e\uff09\uff0c\u5e76\u5c06\u5176\u4e0e\u5e2e\u52a9\u6027\u548c\u5b89\u5168\u6027\u76ee\u6807\u7ed3\u5408\u8fdb\u884c\u591a\u76ee\u6807\u7b56\u7565\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793aP[Love]\u80fd\u9884\u6d4b\u7528\u6237\u884c\u4e3a\uff0c\u7b56\u7565\u4f18\u5316\u4f7f\u6b63\u9762\u53cd\u9988\u7387\u663e\u8457\u63d0\u5347\uff08A/B\u6d4b\u8bd5\u4e2d\u70b9\u8d5e\u589e\u52a028%\uff09\u3002", "conclusion": "RLUF\u901a\u8fc7\u76f4\u63a5\u5229\u7528\u7528\u6237\u4fe1\u53f7\uff0c\u4e3aLLMs\u4e0e\u771f\u5b9e\u7528\u6237\u504f\u597d\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u4f46\u9700\u6ce8\u610f\u5956\u52b1\u7be1\u6539\u95ee\u9898\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u7528\u6237\u53cd\u9988\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u5bf9\u9f50\u3001P[Love]"}}
{"id": "2505.14845", "pdf": "https://arxiv.org/pdf/2505.14845", "abs": "https://arxiv.org/abs/2505.14845", "authors": ["Wang Jiaqi", "Wang bo", "Guo fa", "Cheng cheng", "Yang li"], "title": "A Comparative Study of Large Language Models and Human Personality Traits", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated human-like capabilities in\nlanguage comprehension and generation, becoming active participants in social\nand cognitive domains. This study investigates whether LLMs exhibit\npersonality-like traits and how these traits compare with human personality,\nfocusing on the applicability of conventional personality assessment tools. A\nbehavior-based approach was used across three empirical studies. Study 1\nexamined test-retest stability and found that LLMs show higher variability and\nare more input-sensitive than humans, lacking long-term stability. Based on\nthis, we propose the Distributed Personality Framework, conceptualizing LLM\ntraits as dynamic and input-driven. Study 2 analyzed cross-variant consistency\nin personality measures and found LLMs' responses were highly sensitive to item\nwording, showing low internal consistency compared to humans. Study 3 explored\npersonality retention during role-playing, showing LLM traits are shaped by\nprompt and parameter settings. These findings suggest that LLMs express fluid,\nexternally dependent personality patterns, offering insights for constructing\nLLM-specific personality frameworks and advancing human-AI interaction. This\nwork contributes to responsible AI development and extends the boundaries of\npersonality psychology in the age of intelligent systems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u662f\u5426\u8868\u73b0\u7c7b\u4f3c\u4eba\u7c7b\u7684\u4e2a\u6027\u7279\u5f81\uff0c\u53d1\u73b0\u5176\u4e2a\u6027\u662f\u52a8\u6001\u4e14\u4f9d\u8d56\u4e8e\u8f93\u5165\u7684\uff0c\u63d0\u51fa\u4e86\u5206\u5e03\u5f0f\u4e2a\u6027\u6846\u67b6\u3002", "motivation": "\u63a2\u7d22LLM\u662f\u5426\u5177\u6709\u7c7b\u4f3c\u4eba\u7c7b\u7684\u4e2a\u6027\u7279\u5f81\uff0c\u5e76\u9a8c\u8bc1\u4f20\u7edf\u4e2a\u6027\u8bc4\u4f30\u5de5\u5177\u7684\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528\u884c\u4e3a\u57fa\u7840\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u9879\u5b9e\u8bc1\u7814\u7a76\uff1a\u6d4b\u8bd5\u91cd\u6d4b\u7a33\u5b9a\u6027\u3001\u8de8\u53d8\u4f53\u4e00\u81f4\u6027\u5206\u6790\u3001\u89d2\u8272\u626e\u6f14\u4e2d\u7684\u4e2a\u6027\u4fdd\u7559\u3002", "result": "LLM\u7684\u4e2a\u6027\u7279\u5f81\u8868\u73b0\u51fa\u9ad8\u53d8\u5f02\u6027\u3001\u8f93\u5165\u654f\u611f\u6027\u548c\u4f4e\u4e00\u81f4\u6027\uff0c\u7f3a\u4e4f\u957f\u671f\u7a33\u5b9a\u6027\u3002", "conclusion": "LLM\u7684\u4e2a\u6027\u662f\u52a8\u6001\u4e14\u5916\u90e8\u4f9d\u8d56\u7684\uff0c\u4e3a\u6784\u5efaLLM\u7279\u5b9a\u4e2a\u6027\u6846\u67b6\u548c\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u4e2a\u6027\u7279\u5f81\u3001\u5206\u5e03\u5f0f\u4e2a\u6027\u6846\u67b6\u3001\u4eba\u673a\u4ea4\u4e92\u3001\u4eba\u5de5\u667a\u80fd"}}
{"id": "2505.14741", "pdf": "https://arxiv.org/pdf/2505.14741", "abs": "https://arxiv.org/abs/2505.14741", "authors": ["Kunyun Wang", "Bohan Li", "Kai Yu", "Minyi Guo", "Jieru Zhao"], "title": "Communication-Efficient Diffusion Denoising Parallelization via Reuse-then-Predict Mechanism", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diffusion models have emerged as a powerful class of generative models across\nvarious modalities, including image, video, and audio synthesis. However, their\ndeployment is often limited by significant inference latency, primarily due to\nthe inherently sequential nature of the denoising process. While existing\nparallelization strategies attempt to accelerate inference by distributing\ncomputation across multiple devices, they typically incur high communication\noverhead, hindering deployment on commercial hardware. To address this\nchallenge, we propose \\textbf{ParaStep}, a novel parallelization method based\non a reuse-then-predict mechanism that parallelizes diffusion inference by\nexploiting similarity between adjacent denoising steps. Unlike prior approaches\nthat rely on layer-wise or stage-wise communication, ParaStep employs\nlightweight, step-wise communication, substantially reducing overhead. ParaStep\nachieves end-to-end speedups of up to \\textbf{3.88}$\\times$ on SVD,\n\\textbf{2.43}$\\times$ on CogVideoX-2b, and \\textbf{6.56}$\\times$ on\nAudioLDM2-large, while maintaining generation quality. These results highlight\nParaStep as a scalable and communication-efficient solution for accelerating\ndiffusion inference, particularly in bandwidth-constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aParaStep\u7684\u65b0\u578b\u5e76\u884c\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u7528-\u9884\u6d4b\u673a\u5236\u51cf\u5c11\u6269\u6563\u6a21\u578b\u63a8\u7406\u5ef6\u8fdf\uff0c\u663e\u8457\u63d0\u5347\u901f\u5ea6\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u5f3a\u5927\uff0c\u4f46\u5176\u987a\u5e8f\u53bb\u566a\u8fc7\u7a0b\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u9ad8\uff0c\u73b0\u6709\u5e76\u884c\u5316\u65b9\u6cd5\u56e0\u901a\u4fe1\u5f00\u9500\u5927\u96be\u4ee5\u5546\u7528\u3002", "method": "\u91c7\u7528\u91cd\u7528-\u9884\u6d4b\u673a\u5236\uff0c\u5229\u7528\u76f8\u90bb\u53bb\u566a\u6b65\u9aa4\u7684\u76f8\u4f3c\u6027\u5e76\u884c\u5316\u63a8\u7406\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u7684\u6b65\u9aa4\u95f4\u901a\u4fe1\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cParaStep\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\uff08\u6700\u9ad86.56\u500d\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "ParaStep\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u901a\u4fe1\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u5e26\u5bbd\u53d7\u9650\u73af\u5883\u3002", "keywords": "\u6269\u6563\u6a21\u578b,\u5e76\u884c\u5316,\u63a8\u7406\u52a0\u901f,\u901a\u4fe1\u5f00\u9500,\u751f\u6210\u8d28\u91cf"}}
{"id": "2505.14970", "pdf": "https://arxiv.org/pdf/2505.14970", "abs": "https://arxiv.org/abs/2505.14970", "authors": ["Xiaoyin Chen", "Jiarui Lu", "Minsu Kim", "Dinghuai Zhang", "Jian Tang", "Alexandre Pich\u00e9", "Nicolas Gontier", "Yoshua Bengio", "Ehsan Kamalloo"], "title": "Self-Evolving Curriculum for LLM Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has proven effective for fine-tuning large\nlanguage models (LLMs), significantly enhancing their reasoning abilities in\ndomains such as mathematics and code generation. A crucial factor influencing\nRL fine-tuning success is the training curriculum: the order in which training\nproblems are presented. While random curricula serve as common baselines, they\nremain suboptimal; manually designed curricula often rely heavily on\nheuristics, and online filtering methods can be computationally prohibitive. To\naddress these limitations, we propose Self-Evolving Curriculum (SEC), an\nautomatic curriculum learning method that learns a curriculum policy\nconcurrently with the RL fine-tuning process. Our approach formulates\ncurriculum selection as a non-stationary Multi-Armed Bandit problem, treating\neach problem category (e.g., difficulty level or problem type) as an individual\narm. We leverage the absolute advantage from policy gradient methods as a proxy\nmeasure for immediate learning gain. At each training step, the curriculum\npolicy selects categories to maximize this reward signal and is updated using\nthe TD(0) method. Across three distinct reasoning domains: planning, inductive\nreasoning, and mathematics, our experiments demonstrate that SEC significantly\nimproves models' reasoning capabilities, enabling better generalization to\nharder, out-of-distribution test problems. Additionally, our approach achieves\nbetter skill balance when fine-tuning simultaneously on multiple reasoning\ndomains. These findings highlight SEC as a promising strategy for RL\nfine-tuning of LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u81ea\u6211\u6f14\u5316\u8bfe\u7a0b\uff08SEC\uff09\u7684\u81ea\u52a8\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u8bfe\u7a0b\u7b56\u7565\u6765\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4f18\u4e8e\u968f\u673a\u548c\u624b\u52a8\u8bbe\u8ba1\u7684\u8bfe\u7a0b\u3002", "motivation": "\u968f\u673a\u8bfe\u7a0b\u6548\u679c\u4e0d\u4f73\uff0c\u624b\u52a8\u8bbe\u8ba1\u8bfe\u7a0b\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u800c\u5728\u7ebf\u8fc7\u6ee4\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e9f\u9700\u4e00\u79cd\u81ea\u52a8\u4e14\u9ad8\u6548\u7684\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "SEC\u5c06\u8bfe\u7a0b\u9009\u62e9\u5efa\u6a21\u4e3a\u975e\u5e73\u7a33\u7684\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5229\u7528\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u4e2d\u7684\u7edd\u5bf9\u4f18\u52bf\u4f5c\u4e3a\u5b66\u4e60\u589e\u76ca\u7684\u4ee3\u7406\u6307\u6807\uff0c\u5e76\u901a\u8fc7TD(0)\u65b9\u6cd5\u66f4\u65b0\u8bfe\u7a0b\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSEC\u5728\u89c4\u5212\u3001\u5f52\u7eb3\u63a8\u7406\u548c\u6570\u5b66\u9886\u57df\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u66f4\u597d\u5730\u6cdb\u5316\u5230\u66f4\u96be\u7684\u6d4b\u8bd5\u95ee\u9898\u4e0a\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u591a\u63a8\u7406\u9886\u57df\u7684\u6280\u80fd\u5e73\u8861\u3002", "conclusion": "SEC\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684RL\u5fae\u8c03\u7b56\u7565\uff0c\u80fd\u6709\u6548\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u8bfe\u7a0b\u5b66\u4e60, \u591a\u81c2\u8001\u864e\u673a, \u63a8\u7406\u80fd\u529b"}}
{"id": "2505.14848", "pdf": "https://arxiv.org/pdf/2505.14848", "abs": "https://arxiv.org/abs/2505.14848", "authors": ["Xi Wang", "Jiaqian Hu", "Safinah Ali"], "title": "MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation", "categories": ["cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "We present MAATS, a Multi Agent Automated Translation System that leverages\nthe Multidimensional Quality Metrics (MQM) framework as a fine-grained signal\nfor error detection and refinement. MAATS employs multiple specialized AI\nagents, each focused on a distinct MQM category (e.g., Accuracy, Fluency,\nStyle, Terminology), followed by a synthesis agent that integrates the\nannotations to iteratively refine translations. This design contrasts with\nconventional single-agent methods that rely on self-correction.\n  Evaluated across diverse language pairs and Large Language Models (LLMs),\nMAATS outperforms zero-shot and single-agent baselines with statistically\nsignificant gains in both automatic metrics and human assessments. It excels\nparticularly in semantic accuracy, locale adaptation, and linguistically\ndistant language pairs. Qualitative analysis highlights its strengths in\nmulti-layered error diagnosis, omission detection across perspectives, and\ncontext-aware refinement. By aligning modular agent roles with interpretable\nMQM dimensions, MAATS narrows the gap between black-box LLMs and human\ntranslation workflows, shifting focus from surface fluency to deeper semantic\nand contextual fidelity.", "AI": {"tldr": "MAATS\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u81ea\u52a8\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u5229\u7528MQM\u6846\u67b6\u8fdb\u884c\u7ec6\u7c92\u5ea6\u9519\u8bef\u68c0\u6d4b\u548c\u7ffb\u8bd1\u4f18\u5316\uff0c\u901a\u8fc7\u591a\u4e2a\u4e13\u95e8\u5316\u7684AI\u4ee3\u7406\u548c\u5408\u6210\u4ee3\u7406\u8fed\u4ee3\u6539\u8fdb\u7ffb\u8bd1\u6548\u679c\uff0c\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u4ee3\u7406\u7ffb\u8bd1\u7cfb\u7edf\u4f9d\u8d56\u81ea\u6821\u6b63\uff0c\u5c40\u9650\u6027\u8f83\u5927\u3002MAATS\u65e8\u5728\u5229\u7528MQM\u6846\u67b6\u548c\u591a\u4e2a\u5206\u5de5\u660e\u786e\u7684\u4ee3\u7406\uff0c\u63d0\u5347\u7ffb\u8bd1\u7684\u8bed\u4e49\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u9002\u5e94\u6027\u3002", "method": "MAATS\u91c7\u7528\u591a\u4e2a\u4e13\u6ce8\u4e8e\u4e0d\u540cMQM\u7c7b\u522b\uff08\u5982\u51c6\u786e\u6027\u3001\u6d41\u7545\u6027\u3001\u98ce\u683c\u3001\u672f\u8bed\uff09\u7684AI\u4ee3\u7406\uff0c\u7ed3\u5408\u5408\u6210\u4ee3\u7406\u6574\u5408\u6807\u6ce8\u7ed3\u679c\uff0c\u8fed\u4ee3\u4f18\u5316\u7ffb\u8bd1\u3002", "result": "\u5728\u591a\u79cd\u8bed\u8a00\u5bf9\u548cLLMs\u4e0a\uff0cMAATS\u5728\u81ea\u52a8\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u548c\u5355\u4ee3\u7406\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u8bed\u4e49\u51c6\u786e\u6027\u548c\u8de8\u8bed\u8a00\u9002\u5e94\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "MAATS\u901a\u8fc7\u6a21\u5757\u5316\u4ee3\u7406\u548cMQM\u6846\u67b6\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u7f29\u5c0f\u4e86\u9ed1\u76d2LLMs\u4e0e\u4eba\u5de5\u7ffb\u8bd1\u6d41\u7a0b\u7684\u5dee\u8ddd\uff0c\u63d0\u5347\u4e86\u7ffb\u8bd1\u7684\u6df1\u5ea6\u51c6\u786e\u6027\u3002", "keywords": "MAATS, MQM, \u591a\u4ee3\u7406, \u7ffb\u8bd1\u4f18\u5316, \u8bed\u4e49\u51c6\u786e\u6027"}}
{"id": "2505.14742", "pdf": "https://arxiv.org/pdf/2505.14742", "abs": "https://arxiv.org/abs/2505.14742", "authors": ["Hong Huang", "Dapeng Wu"], "title": "Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have made exciting achievements across various\ndomains, yet their deployment on resource-constrained personal devices remains\nhindered by the prohibitive computational and memory demands of task-specific\nfine-tuning. While quantization offers a pathway to efficiency, existing\nmethods struggle to balance performance and overhead, either incurring high\ncomputational/memory costs or failing to address activation outliers, a\ncritical bottleneck in quantized fine-tuning. To address these challenges, we\npropose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning,\ncertain activation outlier channels retain stable spatial positions across\ntraining iterations. Building on OSSH, we propose Quaff, a Quantized\nparameter-efficient fine-tuning framework for LLMs, optimizing low-precision\nactivation representations through targeted momentum scaling. Quaff dynamically\nsuppresses outliers exclusively in invariant channels using lightweight\noperations, eliminating full-precision weight storage and global rescaling\nwhile reducing quantization errors. Extensive experiments across ten benchmarks\nvalidate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA\nreasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory\nsavings over full-precision fine-tuning while improving accuracy by 0.6% on the\nPhi-3 model, reconciling the triple trade-off between efficiency, performance,\nand deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080\nSuper) without sacrificing model utility, Quaff democratizes personalized LLM\ndeployment. The code is available at https://github.com/Little0o0/Quaff.git.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQuaff\u7684\u9ad8\u6548\u91cf\u5316\u53c2\u6570\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6291\u5236\u6fc0\u6d3b\u5f02\u5e38\u503c\u4f18\u5316\u4f4e\u7cbe\u5ea6\u6fc0\u6d3b\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u91cf\u5316\u5fae\u8c03\u4e2d\u7684\u6027\u80fd\u4e0e\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8d44\u6e90\u53d7\u9650\u7684\u4e2a\u4eba\u8bbe\u5907\u4e0a\u90e8\u7f72\u53d7\u9650\u4e8e\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\uff0c\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u6027\u80fd\u4e0e\u5f00\u9500\u3002", "method": "\u57fa\u4e8eOutlier Spatial Stability Hypothesis\uff08OSSH\uff09\uff0c\u5f00\u53d1\u4e86Quaff\u6846\u67b6\uff0c\u901a\u8fc7\u76ee\u6807\u52a8\u91cf\u7f29\u653e\u548c\u8f7b\u91cf\u64cd\u4f5c\u52a8\u6001\u6291\u5236\u5f02\u5e38\u503c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cQuaff\u5728GPQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e861.73\u500d\u5ef6\u8fdf\u964d\u4f4e\u548c30%\u5185\u5b58\u8282\u7701\uff0c\u540c\u65f6\u51c6\u786e\u6027\u63d0\u53470.6%\u3002", "conclusion": "Quaff\u6846\u67b6\u5728\u6548\u7387\u3001\u6027\u80fd\u548c\u53ef\u90e8\u7f72\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4f7f\u6d88\u8d39\u7ea7GPU\u4e5f\u80fd\u5b8c\u6210LLM\u5fae\u8c03\u3002", "keywords": "LLMs, Quantization, Parameter-efficient Fine-tuning, OSSH, Quaff"}}
{"id": "2505.14983", "pdf": "https://arxiv.org/pdf/2505.14983", "abs": "https://arxiv.org/abs/2505.14983", "authors": ["Zahra Zahedi", "Shashank Mehrotra", "Teruhisa Misu", "Kumar Akash"], "title": "Toward Informed AV Decision-Making: Computational Model of Well-being and Trust in Mobility", "categories": ["cs.AI", "cs.HC", "cs.RO"], "comment": null, "summary": "For future human-autonomous vehicle (AV) interactions to be effective and\nsmooth, human-aware systems that analyze and align human needs with automation\ndecisions are essential. Achieving this requires systems that account for human\ncognitive states. We present a novel computational model in the form of a\nDynamic Bayesian Network (DBN) that infers the cognitive states of both AV\nusers and other road users, integrating this information into the AV's\ndecision-making process. Specifically, our model captures the well-being of\nboth an AV user and an interacting road user as cognitive states alongside\ntrust. Our DBN models infer beliefs over the AV user's evolving well-being,\ntrust, and intention states, as well as the possible well-being of other road\nusers, based on observed interaction experiences. Using data collected from an\ninteraction study, we refine the model parameters and empirically assess its\nperformance. Finally, we extend our model into a causal inference model (CIM)\nframework for AV decision-making, enabling the AV to enhance user well-being\nand trust while balancing these factors with its own operational costs and the\nwell-being of interacting road users. Our evaluation demonstrates the model's\neffectiveness in accurately predicting user's states and guiding informed,\nhuman-centered AV decisions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8d1d\u53f6\u65af\u7f51\u7edc\u6a21\u578b\uff0c\u7528\u4e8e\u63a8\u65ad\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7528\u6237\u53ca\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u7684\u8ba4\u77e5\u72b6\u6001\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u4ee5\u63d0\u5347\u4ea4\u4e92\u6548\u679c\u3002", "motivation": "\u4e3a\u5b9e\u73b0\u4eba\u673a\u4ea4\u4e92\u7684\u9ad8\u6548\u4e0e\u987a\u7545\uff0c\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u5206\u6790\u5e76\u6ee1\u8db3\u4eba\u7c7b\u9700\u6c42\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u52a8\u6001\u8d1d\u53f6\u65af\u7f51\u7edc\uff08DBN\uff09\u5efa\u6a21\uff0c\u63a8\u65ad\u7528\u6237\u548c\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u7684\u8ba4\u77e5\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6570\u636e\u4f18\u5316\u6a21\u578b\u53c2\u6570\u3002\u8fdb\u4e00\u6b65\u6269\u5c55\u4e3a\u56e0\u679c\u63a8\u7406\u6a21\u578b\uff08CIM\uff09\uff0c\u7528\u4e8e\u5e73\u8861\u7528\u6237\u798f\u7949\u3001\u4fe1\u4efb\u4e0e\u7cfb\u7edf\u8fd0\u8425\u6210\u672c\u3002", "result": "\u6a21\u578b\u80fd\u51c6\u786e\u9884\u6d4b\u7528\u6237\u72b6\u6001\uff0c\u5e76\u652f\u6301\u4ee5\u4eba\u4e3a\u672c\u7684\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u3002", "conclusion": "\u8be5\u6a21\u578b\u6709\u6548\u5730\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5bf9\u4eba\u7c7b\u9700\u6c42\u7684\u54cd\u5e94\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u5316\u7684\u4ea4\u4e92\u4f53\u9a8c\u3002", "keywords": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u3001\u52a8\u6001\u8d1d\u53f6\u65af\u7f51\u7edc\u3001\u8ba4\u77e5\u72b6\u6001\u3001\u56e0\u679c\u63a8\u7406\u3001\u4eba\u673a\u4ea4\u4e92"}}
{"id": "2505.14852", "pdf": "https://arxiv.org/pdf/2505.14852", "abs": "https://arxiv.org/abs/2505.14852", "authors": ["Drishya Karki", "Michiel Kamphuis", "Angelecia Frey"], "title": "EasyMath: A 0-shot Math Benchmark for SLMs", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.6; I.2.7"], "comment": "17 pages, 9 figures, 8 tables", "summary": "EasyMath is a compact benchmark for practical math reasoning in small\nlanguage models. It covers thirteen categories, from basic arithmetic and order\nof operations to word problems, algebraic expressions, edge cases, and omits\nspecialist topics. We tested 23 models (14M to 4B parameters) using exact,\nnumerical, and symbolic checks on free-form answers in a zero-shot setting.\nAccuracy rises with size and training, chain-of-thought adds modest gains, and\nconsistency improves at scale.", "AI": {"tldr": "EasyMath \u662f\u4e00\u4e2a\u7528\u4e8e\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5b9e\u7528\u6570\u5b66\u63a8\u7406\u7684\u7d27\u51d1\u57fa\u51c6\uff0c\u6db5\u76d6 13 \u4e2a\u7c7b\u522b\uff0c\u6d4b\u8bd5\u4e86 23 \u4e2a\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u51c6\u786e\u6027\u4e0e\u6a21\u578b\u5927\u5c0f\u548c\u8bad\u7ec3\u76f8\u5173\u3002", "motivation": "\u7814\u7a76\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\u3002", "method": "\u8bbe\u8ba1\u4e86\u6db5\u76d6 13 \u7c7b\u6570\u5b66\u95ee\u9898\u7684 EasyMath \u57fa\u51c6\uff0c\u6d4b\u8bd5\u4e86 23 \u4e2a\u6a21\u578b\uff08\u53c2\u6570\u4ece 14M \u5230 4B\uff09\uff0c\u91c7\u7528\u96f6\u6837\u672c\u8bbe\u7f6e\u8fdb\u884c\u7cbe\u786e\u3001\u6570\u503c\u548c\u7b26\u53f7\u68c0\u67e5\u3002", "result": "\u6a21\u578b\u51c6\u786e\u6027\u4e0e\u89c4\u6a21\u548c\u8bad\u7ec3\u7a0b\u5ea6\u6b63\u76f8\u5173\uff0c\u94fe\u5f0f\u601d\u8003\uff08chain-of-thought\uff09\u5e26\u6765\u5c0f\u5e45\u63d0\u5347\uff0c\u5927\u89c4\u6a21\u6a21\u578b\u8868\u73b0\u66f4\u4e00\u81f4\u3002", "conclusion": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u4e0e\u89c4\u6a21\u548c\u8bad\u7ec3\u7d27\u5bc6\u76f8\u5173\uff0c\u94fe\u5f0f\u601d\u8003\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "keywords": "EasyMath, \u6570\u5b66\u63a8\u7406, \u5c0f\u578b\u8bed\u8a00\u6a21\u578b, \u57fa\u51c6\u6d4b\u8bd5, \u96f6\u6837\u672c"}}
{"id": "2505.14745", "pdf": "https://arxiv.org/pdf/2505.14745", "abs": "https://arxiv.org/abs/2505.14745", "authors": ["Varun Raaghav", "Dimitrios Bikos", "Antonio Rago", "Francesca Toni", "Maria Charalambides"], "title": "Explainable Prediction of the Mechanical Properties of Composites with CNNs", "categories": ["cs.LG", "cs.AI", "I.2.1"], "comment": "9 pages, 6 figures", "summary": "Composites are amongst the most important materials manufactured today, as\nevidenced by their use in countless applications. In order to establish the\nsuitability of composites in specific applications, finite element (FE)\nmodelling, a numerical method based on partial differential equations, is the\nindustry standard for assessing their mechanical properties. However, FE\nmodelling is exceptionally costly from a computational viewpoint, a limitation\nwhich has led to efforts towards applying AI models to this task. However, in\nthese approaches: the chosen model architectures were rudimentary, feed-forward\nneural networks giving limited accuracy; the studies focus on predicting\nelastic mechanical properties, without considering material strength limits;\nand the models lacked transparency, hindering trustworthiness by users. In this\npaper, we show that convolutional neural networks (CNNs) equipped with methods\nfrom explainable AI (XAI) can be successfully deployed to solve this problem.\nOur approach uses customised CNNs trained on a dataset we generate using\ntransverse tension tests in FE modelling to predict composites' mechanical\nproperties, i.e., Young's modulus and yield strength. We show empirically that\nour approach achieves high accuracy, outperforming a baseline, ResNet-34, in\nestimating the mechanical properties. We then use SHAP and Integrated\nGradients, two post-hoc XAI methods, to explain the predictions, showing that\nthe CNNs use the critical geometrical features that influence the composites'\nbehaviour, thus allowing engineers to verify that the models are trustworthy by\nrepresenting the science of composites.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN)\u548c\u53ef\u89e3\u91caAI(XAI)\u65b9\u6cd5\u9884\u6d4b\u590d\u5408\u6750\u6599\u529b\u5b66\u6027\u80fd\u7684\u9ad8\u7cbe\u5ea6\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u6709\u9650\u5143\u5efa\u6a21\u548c\u57fa\u7840\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u6709\u9650\u5143\u5efa\u6a21\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4f20\u7edfAI\u65b9\u6cd5\u5728\u9884\u6d4b\u590d\u5408\u6750\u6599\u529b\u5b66\u6027\u80fd\u65f6\u5b58\u5728\u7cbe\u5ea6\u4f4e\u3001\u4e0d\u8003\u8651\u6750\u6599\u5f3a\u5ea6\u9650\u5236\u548c\u7f3a\u4e4f\u900f\u660e\u5ea6\u7b49\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4f7f\u7528CNN\u548cXAI\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5b9a\u5236\u5316CNN\u6a21\u578b\uff0c\u901a\u8fc7\u6709\u9650\u5143\u5efa\u6a21\u751f\u6210\u7684\u6a2a\u5411\u5f20\u529b\u6d4b\u8bd5\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u9884\u6d4b\u590d\u5408\u6750\u6599\u7684\u6768\u6c0f\u6a21\u91cf\u548c\u5c48\u670d\u5f3a\u5ea6\uff0c\u5e76\u4f7f\u7528SHAP\u548cIntegrated Gradients\u89e3\u91ca\u9884\u6d4b\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u529b\u5b66\u6027\u80fd\u65b9\u9762\u5177\u6709\u9ad8\u51c6\u786e\u6027\uff0c\u4f18\u4e8eResNet-34\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u901a\u8fc7XAI\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u79d1\u5b66\u5408\u7406\u6027\u3002", "conclusion": "\u7ed3\u5408CNN\u548cXAI\u7684\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u8fd8\u901a\u8fc7\u89e3\u91ca\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u589e\u5f3a\u4e86\u5de5\u7a0b\u5e08\u5bf9\u6a21\u578b\u7684\u4fe1\u4efb\u5ea6\u3002", "keywords": "\u590d\u5408\u6750\u6599, \u5377\u79ef\u795e\u7ecf\u7f51\u7edc, \u53ef\u89e3\u91caAI, \u6709\u9650\u5143\u5efa\u6a21, \u529b\u5b66\u6027\u80fd"}}
{"id": "2505.15011", "pdf": "https://arxiv.org/pdf/2505.15011", "abs": "https://arxiv.org/abs/2505.15011", "authors": ["Kryspin Varys", "Federico Cerutti", "Adam Sobey", "Timothy J. Norman"], "title": "HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Our society is governed by a set of norms which together bring about the\nvalues we cherish such as safety, fairness or trustworthiness. The goal of\nvalue-alignment is to create agents that not only do their tasks but through\ntheir behaviours also promote these values. Many of the norms are written as\nlaws or rules (legal / safety norms) but even more remain unwritten (social\nnorms). Furthermore, the techniques used to represent these norms also differ.\nSafety / legal norms are often represented explicitly, for example, in some\nlogical language while social norms are typically learned and remain hidden in\nthe parameter space of a neural network. There is a lack of approaches in the\nliterature that could combine these various norm representations into a single\nalgorithm. We propose a novel method that integrates these norms into the\nreinforcement learning process. Our method monitors the agent's compliance with\nthe given norms and summarizes it in a quantity we call the agent's reputation.\nThis quantity is used to weigh the received rewards to motivate the agent to\nbecome value-aligned. We carry out a series of experiments including a\ncontinuous state space traffic problem to demonstrate the importance of the\nwritten and unwritten norms and show how our method can find the value-aligned\npolicies. Furthermore, we carry out ablations to demonstrate why it is better\nto combine these two groups of norms rather than using either separately.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6cd5\u5f8b/\u5b89\u5168\u89c4\u8303\u4e0e\u793e\u4f1a\u89c4\u8303\u7ed3\u5408\u5230\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u6d4b\u4ee3\u7406\u7684\u884c\u4e3a\u5e76\u8ba1\u7b97\u5176\u58f0\u8a89\u6765\u4fc3\u8fdb\u4ef7\u503c\u5bf9\u9f50\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u8fde\u7eed\u72b6\u6001\u7a7a\u95f4\u4ea4\u901a\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u89c4\u8303\u8868\u793a\u65b9\u6cd5\u5206\u6563\uff0c\u6cd5\u5f8b\u89c4\u8303\u660e\u786e\u800c\u793e\u4f1a\u89c4\u8303\u9690\u5f0f\uff0c\u7f3a\u4e4f\u7edf\u4e00\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u6574\u5408\u4e24\u79cd\u89c4\u8303\u4ee5\u5b9e\u73b0\u4ef7\u503c\u5bf9\u9f50\u7684\u667a\u80fd\u4ee3\u7406\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u6d4b\u4ee3\u7406\u5bf9\u89c4\u8303\u7684\u9075\u5b88\u60c5\u51b5\u5e76\u8ba1\u7b97\u58f0\u8a89\u503c\uff0c\u5c06\u58f0\u8a89\u4f5c\u4e3a\u5956\u52b1\u7684\u6743\u91cd\uff0c\u4ee5\u6fc0\u52b1\u4ee3\u7406\u5b9e\u73b0\u4ef7\u503c\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7ed3\u5408\u4e24\u79cd\u89c4\u8303\uff0c\u627e\u5230\u4ef7\u503c\u5bf9\u9f50\u7684\u7b56\u7565\uff0c\u4e14\u6bd4\u5355\u72ec\u4f7f\u7528\u4efb\u4e00\u7ec4\u89c4\u8303\u6548\u679c\u66f4\u597d\u3002", "conclusion": "\u6574\u5408\u6cd5\u5f8b\u548c\u793e\u4f1a\u89c4\u8303\u7684\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u4ee3\u7406\u7684\u4ef7\u503c\u5bf9\u9f50\u80fd\u529b\u3002", "keywords": "\u4ef7\u503c\u5bf9\u9f50\uff0c\u5f3a\u5316\u5b66\u4e60\uff0c\u89c4\u8303\uff0c\u58f0\u8a89\uff0c\u793e\u4f1a\u89c4\u8303"}}
{"id": "2505.14871", "pdf": "https://arxiv.org/pdf/2505.14871", "abs": "https://arxiv.org/abs/2505.14871", "authors": ["Ryan Solgi", "Kai Zhen", "Rupak Vignesh Swaminathan", "Nathan Susanj", "Athanasios Mouchtaris", "Siegfried Kunzmann", "Zheng Zhang"], "title": "Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The efficient implementation of large language models (LLMs) is crucial for\ndeployment on resource-constrained devices. Low-rank tensor compression\ntechniques, such as tensor-train (TT) networks, have been widely studied for\nover-parameterized neural networks. However, their applications to compress\npre-trained large language models (LLMs) for downstream tasks (post-training)\nremains challenging due to the high-rank nature of pre-trained LLMs and the\nlack of access to pretraining data. In this study, we investigate low-rank\ntensorized LLMs during fine-tuning and propose sparse augmented tensor networks\n(Saten) to enhance their performance. The proposed Saten framework enables full\nmodel compression. Experimental results demonstrate that Saten enhances both\naccuracy and compression efficiency in tensorized language models, achieving\nstate-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSaten\u7684\u4f4e\u79e9\u5f20\u91cf\u538b\u7f29\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u538b\u7f29\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u9700\u8981\u9ad8\u6548\u7684\u538b\u7f29\u6280\u672f\uff0c\u4f46\u5176\u9884\u8bad\u7ec3\u540e\u7684\u9ad8\u79e9\u7279\u6027\u53ca\u7f3a\u4e4f\u9884\u8bad\u7ec3\u6570\u636e\u9650\u5236\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u7a00\u758f\u589e\u5f3a\u5f20\u91cf\u7f51\u7edc\uff08Saten\uff09\uff0c\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5bf9\u4f4e\u79e9\u5f20\u91cf\u5316\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u538b\u7f29\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSaten\u5728\u538b\u7f29\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Saten\u6846\u67b6\u4e3a\u540e\u8bad\u7ec3\u9636\u6bb5\u7684\u5927\u8bed\u8a00\u6a21\u578b\u538b\u7f29\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u4f4e\u79e9\u5f20\u91cf\u538b\u7f29,\u7a00\u758f\u589e\u5f3a\u5f20\u91cf\u7f51\u7edc,\u6a21\u578b\u5fae\u8c03"}}
{"id": "2505.14748", "pdf": "https://arxiv.org/pdf/2505.14748", "abs": "https://arxiv.org/abs/2505.14748", "authors": ["Zaifa Xue", "Tao Zhang", "Tuo Xu", "Huaixin Liang", "Le Gao"], "title": "Cooperative Causal GraphSAGE", "categories": ["cs.LG", "cs.GT"], "comment": null, "summary": "GraphSAGE is a widely used graph neural network. The introduction of causal\ninference has improved its robust performance and named as Causal GraphSAGE.\nHowever, Causal GraphSAGE focuses on measuring causal weighting among\nindividual nodes, but neglecting the cooperative relationships among sampling\nnodes as a whole. To address this issue, this paper proposes Cooperative Causal\nGraphSAGE (CoCa-GraphSAGE), which combines cooperative game theory with Causal\nGraphSAGE. Initially, a cooperative causal structure model is constructed in\nthe case of cooperation based on the graph structure. Subsequently, Cooperative\nCausal sampling (CoCa-sampling) algorithm is proposed, employing the Shapley\nvalues to calculate the cooperative contribution based on causal weights of the\nnodes sets. CoCa-sampling guides the selection of nodes with significant\ncooperative causal effects during the neighborhood sampling process, thus\nintegrating the selected neighborhood features under cooperative relationships,\nwhich takes the sampled nodes as a whole and generates more stable target node\nembeddings. Experiments on publicly available datasets show that the proposed\nmethod has comparable classification performance to the compared methods and\noutperforms under perturbations, demonstrating the robustness improvement by\nCoCa-sampling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5408\u4f5c\u535a\u5f08\u7406\u8bba\u7684CoCa-GraphSAGE\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u8282\u70b9\u96c6\u7684\u5408\u4f5c\u56e0\u679c\u6548\u5e94\uff0c\u6539\u8fdb\u91c7\u6837\u8fc7\u7a0b\u5e76\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684Causal GraphSAGE\u5ffd\u7565\u4e86\u91c7\u6837\u8282\u70b9\u95f4\u7684\u5408\u4f5c\u5173\u7cfb\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u8db3\u3002", "method": "\u63d0\u51faCoCa-GraphSAGE\uff0c\u6784\u5efa\u5408\u4f5c\u56e0\u679c\u7ed3\u6784\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1CoCa-sampling\u7b97\u6cd5\uff0c\u5229\u7528Shapley\u503c\u8ba1\u7b97\u8282\u70b9\u96c6\u7684\u5408\u4f5c\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u5728\u6270\u52a8\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u5408\u4f5c\u56e0\u679c\u91c7\u6837\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "keywords": "GraphSAGE, \u56e0\u679c\u63a8\u65ad, \u5408\u4f5c\u535a\u5f08\u7406\u8bba, Shapley\u503c, \u9c81\u68d2\u6027"}}
{"id": "2505.15068", "pdf": "https://arxiv.org/pdf/2505.15068", "abs": "https://arxiv.org/abs/2505.15068", "authors": ["Cheng Qian", "Hongyi Du", "Hongru Wang", "Xiusi Chen", "Yuji Zhang", "Avirup Sil", "Chengxiang Zhai", "Kathleen McKeown", "Heng Ji"], "title": "ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "36 Pages, 26 Figures, 5 Tables", "summary": "Recent progress in large language models (LLMs) has enabled substantial\nadvances in solving mathematical problems. However, existing benchmarks often\nfail to reflect the complexity of real-world problems, which demand open-ended,\ninterdisciplinary reasoning and integration of computational tools. To address\nthis gap, we introduce ModelingBench, a novel benchmark featuring\nreal-world-inspired, open-ended problems from math modeling competitions across\ndiverse domains, ranging from urban traffic optimization to ecosystem resource\nplanning. These tasks require translating natural language into formal\nmathematical formulations, applying appropriate tools, and producing\nstructured, defensible reports. ModelingBench also supports multiple valid\nsolutions, capturing the ambiguity and creativity of practical modeling. We\nalso present ModelingAgent, a multi-agent framework that coordinates tool use,\nsupports structured workflows, and enables iterative self-refinement to\ngenerate well-grounded, creative solutions. To evaluate outputs, we further\npropose ModelingJudge, an expert-in-the-loop system leveraging LLMs as\ndomain-specialized judges assessing solutions from multiple expert\nperspectives. Empirical results show that ModelingAgent substantially\noutperforms strong baselines and often produces solutions indistinguishable\nfrom those of human experts. Together, our work provides a comprehensive\nframework for evaluating and advancing real-world problem-solving in\nopen-ended, interdisciplinary modeling challenges.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86ModelingBench\uff0c\u4e00\u4e2a\u9762\u5411\u771f\u5b9e\u4e16\u754c\u95ee\u9898\u7684\u5f00\u653e\u6570\u5b66\u5efa\u6a21\u57fa\u51c6\uff0c\u4ee5\u53caModelingAgent\u548cModelingJudge\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u590d\u6742\u5efa\u6a21\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u95ee\u9898\u8bc4\u6d4b\u57fa\u51c6\u672a\u80fd\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u95ee\u9898\u7684\u590d\u6742\u6027\u548c\u5f00\u653e\u9700\u6c42\uff0c\u5c24\u5176\u662f\u8de8\u5b66\u79d1\u63a8\u7406\u548c\u5de5\u5177\u6574\u5408\u80fd\u529b\u3002", "method": "\u63d0\u51faModelingBench\u57fa\u51c6\uff0c\u5305\u542b\u591a\u5143\u9886\u57df\u7684\u95ee\u9898\uff1b\u5f00\u53d1ModelingAgent\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u652f\u6301\u5de5\u5177\u4f7f\u7528\u548c\u5de5\u4f5c\u6d41\u7ba1\u7406\uff1b\u5f15\u5165ModelingJudge\u4e13\u5bb6\u8bc4\u4f30\u7cfb\u7edf\u3002", "result": "ModelingAgent\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u751f\u6210\u7684\u89e3\u51b3\u65b9\u6848\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\u76f8\u5f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u653e\u8de8\u5b66\u79d1\u5efa\u6a21\u95ee\u9898\u63d0\u4f9b\u4e86\u5168\u9762\u8bc4\u6d4b\u6846\u67b6\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u6570\u5b66\u5efa\u6a21,\u8de8\u5b66\u79d1\u95ee\u9898,\u667a\u80fd\u4f53\u6846\u67b6,\u8bc4\u4f30\u7cfb\u7edf"}}
{"id": "2505.14874", "pdf": "https://arxiv.org/pdf/2505.14874", "abs": "https://arxiv.org/abs/2505.14874", "authors": ["Chin-Jou Li", "Eunjung Yeo", "Kwanghee Choi", "Paula Andrea P\u00e9rez-Toro", "Masao Someki", "Rohan Kumar Das", "Zhengjun Yue", "Juan Rafael Orozco-Arroyave", "Elmar N\u00f6th", "David R. Mortensen"], "title": "Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "5 pages, 1 figure, Accepted to Interspeech 2025", "summary": "Automatic speech recognition (ASR) for dysarthric speech remains challenging\ndue to data scarcity, particularly in non-English languages. To address this,\nwe fine-tune a voice conversion model on English dysarthric speech (UASpeech)\nto encode both speaker characteristics and prosodic distortions, then apply it\nto convert healthy non-English speech (FLEURS) into non-English dysarthric-like\nspeech. The generated data is then used to fine-tune a multilingual ASR model,\nMassively Multilingual Speech (MMS), for improved dysarthric speech\nrecognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE\n(Tamil) demonstrates that VC with both speaker and prosody conversion\nsignificantly outperforms the off-the-shelf MMS performance and conventional\naugmentation techniques such as speed and tempo perturbation. Objective and\nsubjective analyses of the generated data further confirm that the generated\nspeech simulates dysarthric characteristics.", "AI": {"tldr": "\u901a\u8fc7\u8bed\u97f3\u8f6c\u6362\u6a21\u578b\u751f\u6210\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u7c7b\u4f3c\u6784\u97f3\u969c\u788d\u7684\u8bed\u97f3\u6570\u636e\uff0c\u7528\u4e8e\u6539\u8fdb\u591a\u8bed\u8a00\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u5728\u6784\u97f3\u969c\u788d\u8bed\u97f3\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u6784\u97f3\u969c\u788d\u8bed\u97f3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u82f1\u8bed\u6784\u97f3\u969c\u788d\u8bed\u97f3\u6570\u636e\uff08UASpeech\uff09\u5fae\u8c03\u8bed\u97f3\u8f6c\u6362\u6a21\u578b\uff0c\u751f\u6210\u975e\u82f1\u8bed\u5065\u5eb7\u8bed\u97f3\uff08FLEURS\uff09\u7684\u6784\u97f3\u969c\u788d\u7248\u672c\uff0c\u5e76\u7528\u4e8e\u5fae\u8c03\u591a\u8bed\u8a00ASR\u6a21\u578b\uff08MMS\uff09\u3002", "result": "\u5728\u897f\u73ed\u7259\u8bed\u3001\u610f\u5927\u5229\u8bed\u548c\u6cf0\u7c73\u5c14\u8bed\u7684\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u548c\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002", "conclusion": "\u901a\u8fc7\u8bed\u97f3\u8f6c\u6362\u751f\u6210\u7684\u6570\u636e\u6709\u6548\u6a21\u62df\u6784\u97f3\u969c\u788d\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347ASR\u6027\u80fd\u3002", "keywords": "\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b, \u6784\u97f3\u969c\u788d, \u8bed\u97f3\u8f6c\u6362, \u591a\u8bed\u8a00"}}
{"id": "2505.14751", "pdf": "https://arxiv.org/pdf/2505.14751", "abs": "https://arxiv.org/abs/2505.14751", "authors": ["Maheak Dave", "Aniket Kumar Singh", "Aryan Pareek", "Harshita Jha", "Debasis Chaudhuri", "Manish Pratap Singh"], "title": "Self Distillation via Iterative Constructive Perturbations", "categories": ["cs.LG", "cs.AI", "cs.ET"], "comment": null, "summary": "Deep Neural Networks have achieved remarkable achievements across various\ndomains, however balancing performance and generalization still remains a\nchallenge while training these networks. In this paper, we propose a novel\nframework that uses a cyclic optimization strategy to concurrently optimize the\nmodel and its input data for better training, rethinking the traditional\ntraining paradigm. Central to our approach is Iterative Constructive\nPerturbation (ICP), which leverages the model's loss to iteratively perturb the\ninput, progressively constructing an enhanced representation over some\nrefinement steps. This ICP input is then fed back into the model to produce\nimproved intermediate features, which serve as a target in a self-distillation\nframework against the original features. By alternately altering the model's\nparameters to the data and the data to the model, our method effectively\naddresses the gap between fitting and generalization, leading to enhanced\nperformance. Extensive experiments demonstrate that our approach not only\nmitigates common performance bottlenecks in neural networks but also\ndemonstrates significant improvements across training variations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5faa\u73af\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u6784\u9020\u6270\u52a8\uff08ICP\uff09\u4f18\u5316\u6a21\u578b\u548c\u8f93\u5165\u6570\u636e\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8bad\u7ec3\u4e2d\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u96be\u4ee5\u517c\u987e\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528ICP\u6280\u672f\uff0c\u5229\u7528\u6a21\u578b\u635f\u5931\u8fed\u4ee3\u6270\u52a8\u8f93\u5165\uff0c\u6784\u5efa\u589e\u5f3a\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u81ea\u84b8\u998f\u6846\u67b6\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5e76\u5728\u591a\u79cd\u8bad\u7ec3\u53d8\u4f53\u4e2d\u663e\u8457\u63d0\u5347\u8868\u73b0\u3002", "conclusion": "\u5faa\u73af\u4f18\u5316\u7b56\u7565\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u6a21\u578b\u548c\u8f93\u5165\u6570\u636e\uff0c\u663e\u8457\u6539\u5584\u4e86\u8bad\u7ec3\u6548\u679c\u3002", "keywords": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc,\u5faa\u73af\u4f18\u5316,\u8fed\u4ee3\u6784\u9020\u6270\u52a8,\u81ea\u84b8\u998f,\u6cdb\u5316\u80fd\u529b"}}
{"id": "2505.15146", "pdf": "https://arxiv.org/pdf/2505.15146", "abs": "https://arxiv.org/abs/2505.15146", "authors": ["Lanxiang Hu", "Mingjia Huo", "Yuxuan Zhang", "Haoyang Yu", "Eric P. Xing", "Ion Stoica", "Tajana Rosing", "Haojian Jin", "Hao Zhang"], "title": "lmgame-Bench: How Good are LLMs at Playing Games?", "categories": ["cs.AI"], "comment": null, "summary": "Playing video games requires perception, memory, and planning, exactly the\nfaculties modern large language model (LLM) agents are expected to master. We\nstudy the major challenges in using popular video games to evaluate modern LLMs\nand find that directly dropping LLMs into games cannot make an effective\nevaluation, for three reasons -- brittle vision perception, prompt sensitivity,\nand potential data contamination. We introduce lmgame-Bench to turn games into\nreliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and\nnarrative games delivered through a unified Gym-style API and paired with\nlightweight perception and memory scaffolds, and is designed to stabilize\nprompt variance and remove contamination. Across 13 leading models, we show\nlmgame-Bench is challenging while still separating models well. Correlation\nanalysis shows that every game probes a unique blend of capabilities often\ntested in isolation elsewhere. More interestingly, performing reinforcement\nlearning on a single game from lmgame-Bench transfers both to unseen games and\nto external planning tasks. Our evaluation code is available at\nhttps://github.com/lmgame-org/GamingAgent/lmgame-bench.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u76f4\u63a5\u5e94\u7528\u4e8e\u89c6\u9891\u6e38\u620f\u8bc4\u4f30\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86lmgame-Bench\u5de5\u5177\uff0c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "motivation": "\u89c6\u9891\u6e38\u620f\u9700\u8981\u611f\u77e5\u3001\u8bb0\u5fc6\u548c\u89c4\u5212\u80fd\u529b\uff0c\u8fd9\u4e0e\u73b0\u4ee3LLM\u7684\u9884\u671f\u80fd\u529b\u76f8\u5339\u914d\uff0c\u4f46\u76f4\u63a5\u8bc4\u4f30\u5b58\u5728\u969c\u788d\u3002", "method": "\u63d0\u51fa\u4e86lmgame-Bench\uff0c\u5305\u542b\u591a\u79cd\u6e38\u620f\u7c7b\u578b\uff0c\u5e76\u901a\u8fc7API\u548c\u8f7b\u91cf\u7ea7\u611f\u77e5\u4e0e\u8bb0\u5fc6\u6846\u67b6\u6765\u7a33\u5b9a\u8bc4\u4f30\u3002", "result": "\u901a\u8fc713\u4e2a\u9886\u5148\u6a21\u578b\u7684\u6d4b\u8bd5\uff0clmgame-Bench\u663e\u793a\u4e86\u6311\u6218\u6027\u548c\u533a\u5206\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u80fd\u529b\u8fc1\u79fb\u7684\u6709\u6548\u6027\u3002", "conclusion": "lmgame-Bench\u662f\u4e00\u4e2a\u53ef\u9760\u4e14\u591a\u529f\u80fd\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u80fd\u591f\u5168\u9762\u6d4b\u8bd5LLM\u7684\u7efc\u5408\u80fd\u529b\u3002", "keywords": "LLM, \u89c6\u9891\u6e38\u620f\u8bc4\u4f30, lmgame-Bench, \u611f\u77e5, \u8bb0\u5fc6, \u89c4\u5212"}}
{"id": "2505.14880", "pdf": "https://arxiv.org/pdf/2505.14880", "abs": "https://arxiv.org/abs/2505.14880", "authors": ["Chris Sypherd", "Sergei Petrov", "Sonny George", "Vaishak Belle"], "title": "Incorporating Token Usage into Prompting Strategy Evaluation", "categories": ["cs.CL"], "comment": "20 pages, 12 tables, 4 figures", "summary": "In recent years, large language models have demonstrated remarkable\nperformance across diverse tasks. However, their task effectiveness is heavily\ndependent on the prompting strategy used to elicit output, which can vary\nwidely in both performance and token usage. While task performance is often\nused to determine prompting strategy success, we argue that\nefficiency--balancing performance and token usage--can be a more practical\nmetric for real-world utility. To enable this, we propose Big-$O_{tok}$, a\ntheoretical framework for describing the token usage growth of prompting\nstrategies, and analyze Token Cost, an empirical measure of tokens per\nperformance. We apply these to several common prompting strategies and find\nthat increased token usage leads to drastically diminishing performance\nreturns. Our results validate the Big-$O_{tok}$ analyses and reinforce the need\nfor efficiency-aware evaluations.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u9ad8\u5ea6\u4f9d\u8d56\u63d0\u793a\u7b56\u7565\uff0c\u672c\u6587\u63d0\u51faBig-$O_{tok}$\u6846\u67b6\u548cToken Cost\u8bc4\u4f30\u6548\u7387\uff0c\u53d1\u73b0token\u4f7f\u7528\u589e\u52a0\u5e26\u6765\u7684\u6027\u80fd\u56de\u62a5\u663e\u8457\u9012\u51cf\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5b9e\u9645\u6548\u7528\u53d7\u63d0\u793a\u7b56\u7565\u6548\u7387\uff08\u6027\u80fd\u548ctoken\u4f7f\u7528\u5e73\u8861\uff09\u7684\u76f4\u63a5\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u7684\u8bc4\u4f30\u6307\u6807\u3002", "method": "\u63d0\u51faBig-$O_{tok}$\u7406\u8bba\u6846\u67b6\u7528\u4e8e\u63cf\u8ff0\u63d0\u793a\u7b56\u7565\u7684token\u4f7f\u7528\u589e\u957f\uff0c\u5e76\u5f15\u5165Token Cost\uff08\u6bcf\u6027\u80fd\u7684token\u6210\u672c\uff09\u4f5c\u4e3a\u5b9e\u8bc1\u6307\u6807\u3002", "result": "\u9a8c\u8bc1\u4e86Big-$O_{tok}$\u5206\u6790\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0token\u4f7f\u7528\u589e\u52a0\u4f1a\u5bfc\u81f4\u6027\u80fd\u56de\u62a5\u6025\u5267\u9012\u51cf\u3002", "conclusion": "\u5f3a\u8c03\u6548\u7387\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0cBig-$O_{tok}$\u548cToken Cost\u4e3a\u63d0\u793a\u7b56\u7565\u7684\u5b9e\u7528\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u63d0\u793a\u7b56\u7565, \u6548\u7387\u8bc4\u4f30, token\u6210\u672c, Big-$O_{tok}$"}}
{"id": "2505.14752", "pdf": "https://arxiv.org/pdf/2505.14752", "abs": "https://arxiv.org/abs/2505.14752", "authors": ["Yihong Tang", "Menglin Kong", "Lijun Sun"], "title": "Large Language Models for Data Synthesis", "categories": ["cs.LG"], "comment": null, "summary": "Generating synthetic data that faithfully captures the statistical structure\nof real-world distributions is a fundamental challenge in data modeling.\nClassical approaches often depend on strong parametric assumptions or manual\nstructural design and struggle in high-dimensional or heterogeneous domains.\nRecent progress in Large Language Models (LLMs) reveals their potential as\nflexible, high-dimensional priors over real-world distributions. However, when\napplied to data synthesis, standard LLM-based sampling is inefficient,\nconstrained by fixed context limits, and fails to ensure statistical alignment.\nGiven this, we introduce LLMSynthor, a general framework for data synthesis\nthat transforms LLMs into structure-aware simulators guided by distributional\nfeedback. LLMSynthor treats the LLM as a nonparametric copula simulator for\nmodeling high-order dependencies and introduces LLM Proposal Sampling to\ngenerate grounded proposal distributions that improve sampling efficiency\nwithout requiring rejection. By minimizing discrepancies in the summary\nstatistics space, the iterative synthesis loop aligns real and synthetic data\nwhile gradually uncovering and refining the latent generative structure. We\nevaluate LLMSynthor in both controlled and real-world settings using\nheterogeneous datasets in privacy-sensitive domains (e.g., e-commerce,\npopulation, and mobility) that encompass both structured and unstructured\nformats. The synthetic data produced by LLMSynthor shows high statistical\nfidelity, practical utility, and cross-data adaptability, positioning it as a\nvaluable tool across economics, social science, urban studies, and beyond.", "AI": {"tldr": "LLMSynthor\u6846\u67b6\u901a\u8fc7LLMs\u751f\u6210\u7edf\u8ba1\u5bf9\u9f50\u7684\u5408\u6210\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u6216\u5f02\u6784\u57df\u4e2d\u6570\u636e\u5408\u6210\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u7ef4\u6216\u5f02\u6784\u6570\u636e\u4e2d\u4f9d\u8d56\u5f3a\u53c2\u6570\u5047\u8bbe\u6216\u624b\u52a8\u8bbe\u8ba1\u7684\u95ee\u9898\uff0c\u5229\u7528LLMs\u4f5c\u4e3a\u7075\u6d3b\u7684\u5148\u9a8c\u5206\u5e03\u3002", "method": "\u5f15\u5165LLMSynthor\u6846\u67b6\uff0c\u5c06LLMs\u89c6\u4e3a\u975e\u53c2\u6570Copula\u6a21\u62df\u5668\uff0c\u901a\u8fc7LLM Proposal Sampling\u63d0\u5347\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u5408\u6210\u5faa\u73af\u6700\u5c0f\u5316\u7edf\u8ba1\u5dee\u5f02\u3002", "result": "\u5408\u6210\u7684\u6570\u636e\u5728\u7edf\u8ba1\u4fdd\u771f\u5ea6\u3001\u5b9e\u7528\u6027\u548c\u8de8\u6570\u636e\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "LLMSynthor\u4e3a\u7ecf\u6d4e\u5b66\u3001\u793e\u4f1a\u79d1\u5b66\u548c\u57ce\u5e02\u7814\u7a76\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u5408\u6210\u5de5\u5177\u3002", "keywords": "\u5408\u6210\u6570\u636e, \u5927\u8bed\u8a00\u6a21\u578b, \u7edf\u8ba1\u5bf9\u9f50, \u975e\u53c2\u6570\u6a21\u62df, LLMSynthor"}}
{"id": "2505.15240", "pdf": "https://arxiv.org/pdf/2505.15240", "abs": "https://arxiv.org/abs/2505.15240", "authors": ["Yassir Fathullah", "Mark J. F. Gales"], "title": "Generalised Probabilistic Modelling and Improved Uncertainty Estimation in Comparative LLM-as-a-judge", "categories": ["cs.AI", "cs.LG", "stat.ML"], "comment": "To appear in UAI 2025", "summary": "This paper explores generalised probabilistic modelling and uncertainty\nestimation in comparative LLM-as-a-judge frameworks. We show that existing\nProduct-of-Experts methods are specific cases of a broader framework, enabling\ndiverse modelling options. Furthermore, we propose improved uncertainty\nestimates for individual comparisons, enabling more efficient selection and\nachieving strong performance with fewer evaluations. We also introduce a method\nfor estimating overall ranking uncertainty. Finally, we demonstrate that\ncombining absolute and comparative scoring improves performance. Experiments\nshow that the specific expert model has a limited impact on final rankings but\nour proposed uncertainty estimates, especially the probability of reordering,\nsignificantly improve the efficiency of systems reducing the number of needed\ncomparisons by ~50%. Furthermore, ranking-level uncertainty metrics can be used\nto identify low-performing predictions, where the nature of the probabilistic\nmodel has a notable impact on the quality of the overall uncertainty.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u6bd4\u8f83\u6027LLM-as-a-judge\u6846\u67b6\u4e2d\u7684\u5e7f\u4e49\u6982\u7387\u5efa\u6a21\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u4ee5\u51cf\u5c11\u6240\u9700\u6bd4\u8f83\u6b21\u6570\u5e76\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6539\u8fdb\u73b0\u6709\u4ea7\u54c1-\u4e13\u5bb6\u65b9\u6cd5\uff0c\u6269\u5c55\u5176\u6846\u67b6\u4ee5\u652f\u6301\u591a\u6837\u5316\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u66f4\u7cbe\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u63d0\u9ad8\u7cfb\u7edf\u6548\u7387\u3002", "method": "\u63d0\u51fa\u6539\u8fdb\u7684\u4e2a\u4f53\u6bd4\u8f83\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ee5\u53ca\u6574\u4f53\u6392\u540d\u4e0d\u786e\u5b9a\u6027\u7684\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u7edd\u5bf9\u548c\u6bd4\u8f83\u8bc4\u5206\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6539\u8fdb\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff08\u5c24\u5176\u662f\u91cd\u65b0\u6392\u5e8f\u6982\u7387\uff09\u663e\u8457\u63d0\u9ad8\u6548\u7387\uff0c\u51cf\u5c11\u6bd4\u8f83\u6b21\u6570\u7ea650%\uff0c\u4e14\u6392\u540d\u7ea7\u522b\u4e0d\u786e\u5b9a\u6027\u6307\u6807\u6709\u52a9\u4e8e\u8bc6\u522b\u4f4e\u8d28\u91cf\u9884\u6d4b\u3002", "conclusion": "\u5e7f\u4e49\u6982\u7387\u5efa\u6a21\u6846\u67b6\u6269\u5c55\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u6539\u8fdb\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6548\u7387\u548c\u8d28\u91cf\u3002", "keywords": "\u6982\u7387\u5efa\u6a21,\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1,LLM-as-a-judge,\u6bd4\u8f83\u6846\u67b6"}}
{"id": "2505.14886", "pdf": "https://arxiv.org/pdf/2505.14886", "abs": "https://arxiv.org/abs/2505.14886", "authors": ["Danqing Wang", "Zhuorui Ye", "Xinran Zhao", "Fei Fang", "Lei Li"], "title": "Strategic Planning and Rationalizing on Trees Make LLMs Better Debaters", "categories": ["cs.CL"], "comment": "9 main pages", "summary": "Winning competitive debates requires sophisticated reasoning and argument\nskills. There are unique challenges in the competitive debate: (1) The time\nconstraints force debaters to make strategic choices about which points to\npursue rather than covering all possible arguments; (2) The persuasiveness of\nthe debate relies on the back-and-forth interaction between arguments, which a\nsingle final game status cannot evaluate. To address these challenges, we\npropose TreeDebater, a novel debate framework that excels in competitive\ndebate. We introduce two tree structures: the Rehearsal Tree and Debate Flow\nTree. The Rehearsal Tree anticipates the attack and defenses to evaluate the\nstrength of the claim, while the Debate Flow Tree tracks the debate status to\nidentify the active actions. TreeDebater allocates its time budget among\ncandidate actions and uses the speech time controller and feedback from the\nsimulated audience to revise its statement. The human evaluation on both the\nstage-level and the debate-level comparison shows that our TreeDebater\noutperforms the state-of-the-art multi-agent debate system. Further\ninvestigation shows that TreeDebater shows better strategies in limiting time\nto important debate actions, aligning with the strategies of human debate\nexperts.", "AI": {"tldr": "TreeDebater\u6846\u67b6\u901a\u8fc7\u9884\u6f14\u6811\u548c\u8fa9\u8bba\u6d41\u6811\u7ed3\u6784\u4f18\u5316\u4e86\u7ade\u4e89\u6027\u8fa9\u8bba\u4e2d\u7684\u65f6\u95f4\u5206\u914d\u548c\u4e92\u52a8\u7b56\u7565\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "motivation": "\u7ade\u4e89\u6027\u8fa9\u8bba\u4e2d\u65f6\u95f4\u9650\u5236\u548c\u4e92\u52a8\u6027\u8bc4\u4f30\u7684\u72ec\u7279\u6311\u6218\u9700\u8981\u65b0\u7684\u6846\u67b6\u6765\u89e3\u51b3\u3002", "method": "\u63d0\u51faTreeDebater\u6846\u67b6\uff0c\u4f7f\u7528Rehearsal Tree\u548cDebate Flow Tree\u7ed3\u6784\uff0c\u7ed3\u5408\u65f6\u95f4\u5206\u914d\u548c\u89c2\u4f17\u53cd\u9988\u3002", "result": "TreeDebater\u5728\u591a\u4ee3\u7406\u8fa9\u8bba\u7cfb\u7edf\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u4e14\u7b56\u7565\u66f4\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u3002", "conclusion": "TreeDebater\u5728\u7ade\u4e89\u6027\u8fa9\u8bba\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u65f6\u95f4\u7ba1\u7406\u548c\u4e92\u52a8\u7b56\u7565\u4e0a\u3002", "keywords": "\u7ade\u4e89\u6027\u8fa9\u8bba, TreeDebater, Rehearsal Tree, Debate Flow Tree, \u591a\u4ee3\u7406\u7cfb\u7edf"}}
{"id": "2505.14756", "pdf": "https://arxiv.org/pdf/2505.14756", "abs": "https://arxiv.org/abs/2505.14756", "authors": ["Chih-Yu Chang", "Milad Azvar", "Chinedum Okwudire", "Raed Al Kontar"], "title": "$\\texttt{LLINBO}$: Trustworthy LLM-in-the-Loop Bayesian Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Bayesian optimization (BO) is a sequential decision-making tool widely used\nfor optimizing expensive black-box functions. Recently, Large Language Models\n(LLMs) have shown remarkable adaptability in low-data regimes, making them\npromising tools for black-box optimization by leveraging contextual knowledge\nto propose high-quality query points. However, relying solely on LLMs as\noptimization agents introduces risks due to their lack of explicit surrogate\nmodeling and calibrated uncertainty, as well as their inherently opaque\ninternal mechanisms. This structural opacity makes it difficult to characterize\nor control the exploration-exploitation trade-off, ultimately undermining\ntheoretical tractability and reliability. To address this, we propose LLINBO:\nLLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with\nstatistical surrogate experts (e.g., Gaussian Processes (GP)). The core\nphilosophy is to leverage contextual reasoning strengths of LLMs for early\nexploration, while relying on principled statistical models to guide efficient\nexploitation. Specifically, we introduce three mechanisms that enable this\ncollaboration and establish their theoretical guarantees. We end the paper with\na real-life proof-of-concept in the context of 3D printing. The code to\nreproduce the results can be found at\nhttps://github.com/UMDataScienceLab/LLM-in-the-Loop-BO.", "AI": {"tldr": "LLINBO\uff1a\u4e00\u79cd\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u7edf\u8ba1\u4ee3\u7406\u6a21\u578b\uff08\u5982\u9ad8\u65af\u8fc7\u7a0b\uff09\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u65e8\u5728\u5229\u7528LLM\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u65e9\u671f\u63a2\u7d22\uff0c\u4f9d\u9760\u7edf\u8ba1\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u5f00\u53d1\u3002", "motivation": "LLMs\u5728\u4f4e\u6570\u636e\u91cf\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u663e\u5f0f\u4ee3\u7406\u6a21\u578b\u548c\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u63a2\u7d22-\u5f00\u53d1\u6743\u8861\u96be\u4ee5\u63a7\u5236\u3002LLINBO\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faLLINBO\u6846\u67b6\uff0c\u7ed3\u5408LLMs\u4e0e\u7edf\u8ba1\u4ee3\u7406\u6a21\u578b\uff08\u5982GP\uff09\uff0c\u5e76\u5f15\u5165\u4e09\u79cd\u534f\u4f5c\u673a\u5236\uff0c\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "\u57283D\u6253\u5370\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "LLINBO\u901a\u8fc7\u7ed3\u5408LLMs\u7684\u63a8\u7406\u80fd\u529b\u548c\u7edf\u8ba1\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u63d0\u5347\u4e86\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u6548\u7387\u548c\u53ef\u4fe1\u5ea6\u3002", "keywords": "\u8d1d\u53f6\u65af\u4f18\u5316, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u9ad8\u65af\u8fc7\u7a0b, \u63a2\u7d22-\u5f00\u53d1\u6743\u8861, 3D\u6253\u5370"}}
{"id": "2505.15274", "pdf": "https://arxiv.org/pdf/2505.15274", "abs": "https://arxiv.org/abs/2505.15274", "authors": ["Xin Shu", "Shuai Wang", "Ang Li"], "title": "Identification of Probabilities of Causation: A Complete Characterization", "categories": ["cs.AI"], "comment": null, "summary": "Probabilities of causation are fundamental to modern decision-making. Pearl\nfirst introduced three binary probabilities of causation, and Tian and Pearl\nlater derived tight bounds for them using Balke's linear programming. The\ntheoretical characterization of probabilities of causation with multi-valued\ntreatments and outcomes has remained unresolved for decades, limiting the scope\nof causality-based decision-making. In this paper, we resolve this foundational\ngap by proposing a complete set of representative probabilities of causation\nand proving that they are sufficient to characterize all possible probabilities\nof causation within the framework of Structural Causal Models (SCMs). We then\nformally derive tight bounds for these representative quantities using formal\nmathematical proofs. Finally, we demonstrate the practical relevance of our\nresults through illustrative toy examples.", "AI": {"tldr": "\u8be5\u8bba\u6587\u89e3\u51b3\u4e86\u591a\u503c\u5904\u7406\u548c\u7ed3\u679c\u7684\u56e0\u679c\u6982\u7387\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5b8c\u6574\u7684\u4ee3\u8868\u6027\u56e0\u679c\u6982\u7387\u96c6\uff0c\u5e76\u4e3a\u5176\u63a8\u5bfc\u4e86\u4e25\u683c\u7684\u754c\u9650\u3002", "motivation": "\u591a\u503c\u5904\u7406\u548c\u7ed3\u679c\u7684\u56e0\u679c\u6982\u7387\u5728\u7406\u8bba\u4e0a\u7684\u7f3a\u5931\u9650\u5236\u4e86\u57fa\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u51b3\u7b56\u8303\u56f4\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u57fa\u7840\u6027\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u4ee3\u8868\u6027\u56e0\u679c\u6982\u7387\u96c6\uff0c\u5e76\u5728\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u6846\u67b6\u5185\u8bc1\u660e\u5176\u5145\u5206\u6027\uff0c\u901a\u8fc7\u6570\u5b66\u8bc1\u660e\u4e3a\u5176\u63a8\u5bfc\u4e25\u683c\u754c\u9650\u3002", "result": "\u6210\u529f\u5b9a\u4e49\u548c\u8bc1\u660e\u4e86\u591a\u503c\u5904\u7406\u7684\u56e0\u679c\u6982\u7387\u7684\u754c\u9650\uff0c\u6269\u5c55\u4e86\u56e0\u679c\u63a8\u7406\u7684\u5e94\u7528\u8303\u56f4\u3002", "conclusion": "\u7814\u7a76\u4e3a\u591a\u503c\u56e0\u679c\u6982\u7387\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u51b3\u7b56\u95ee\u9898\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "keywords": "\u56e0\u679c\u6982\u7387\u3001\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u3001\u591a\u503c\u5904\u7406\u3001\u6570\u5b66\u754c\u9650"}}
{"id": "2505.14887", "pdf": "https://arxiv.org/pdf/2505.14887", "abs": "https://arxiv.org/abs/2505.14887", "authors": ["Nathan Roll", "Calbert Graham", "Yuka Tatsumi", "Kim Tien Nguyen", "Meghan Sumner", "Dan Jurafsky"], "title": "In-Context Learning Boosts Speech Recognition via Human-like Adaptation to Speakers and Language Varieties", "categories": ["cs.CL", "eess.AS"], "comment": "15 pages; 3 figures", "summary": "Human listeners readily adjust to unfamiliar speakers and language varieties\nthrough exposure, but do these adaptation benefits extend to state-of-the-art\nspoken language models? We introduce a scalable framework that allows for\nin-context learning (ICL) in Phi-4 Multimodal using interleaved task prompts\nand audio-text pairs, and find that as few as 12 example utterances (~50\nseconds) at inference time reduce word error rates by a relative 19.7% (1.2\npp.) on average across diverse English corpora. These improvements are most\npronounced in low-resource varieties, when the context and target speaker\nmatch, and when more examples are provided--though scaling our procedure yields\ndiminishing marginal returns to context length. Overall, we find that our novel\nICL adaptation scheme (1) reveals a similar performance profile to human\nlisteners, and (2) demonstrates consistent improvements to automatic speech\nrecognition (ASR) robustness across diverse speakers and language backgrounds.\nWhile adaptation succeeds broadly, significant gaps remain for certain\nvarieties, revealing where current models still fall short of human\nflexibility. We release our prompts and code on GitHub.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4eba\u7c7b\u542c\u4f17\u901a\u8fc7\u66b4\u9732\u9002\u5e94\u4e0d\u540c\u8bf4\u8bdd\u8005\u548c\u8bed\u8a00\u53d8\u4f53\u7684\u80fd\u529b\u662f\u5426\u9002\u7528\u4e8e\u73b0\u4ee3\u8bed\u97f3\u6a21\u578b\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\uff08\u7ea650\u79d2\uff09\u5c06\u8bcd\u9519\u8bef\u7387\u964d\u4f4e\u4e8619.7%\u3002", "motivation": "\u7814\u7a76\u8bed\u97f3\u6a21\u578b\u662f\u5426\u80fd\u591f\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u9002\u5e94\u4e0d\u540c\u8bf4\u8bdd\u8005\u548c\u8bed\u8a00\u53d8\u4f53\uff0c\u7c7b\u4f3c\u4eba\u7c7b\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u4f7f\u7528Phi-4\u591a\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u4efb\u52a1\u63d0\u793a\u548c\u97f3\u9891-\u6587\u672c\u5bf9\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u6846\u67b6\u3002", "result": "\u572812\u4e2a\u793a\u4f8b\uff08\u7ea650\u79d2\uff09\u4e0b\uff0c\u8bcd\u9519\u8bef\u7387\u5e73\u5747\u964d\u4f4e\u4e8619.7%\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u53d8\u4f53\u4e2d\u8868\u73b0\u663e\u8457\u3002", "conclusion": "ICL\u65b9\u6848\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u9002\u5e94\u80fd\u529b\uff0c\u63d0\u5347\u4e86ASR\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u67d0\u4e9b\u53d8\u4f53\u4e2d\u4ecd\u6709\u5dee\u8ddd\u3002", "keywords": "\u8bed\u97f3\u6a21\u578b\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u9002\u5e94\u6027\u3001\u8bcd\u9519\u8bef\u7387\u3001ASR"}}
{"id": "2505.14765", "pdf": "https://arxiv.org/pdf/2505.14765", "abs": "https://arxiv.org/abs/2505.14765", "authors": ["Orhun Vural", "Bunyamin Ozaydin", "Khalid Y. Aram", "James Booth", "Brittany F. Lindsey", "Abdulaziz Ahmed"], "title": "Deep Learning-Based Forecasting of Boarding Patient Counts to Address ED Overcrowding", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.6; J.3"], "comment": null, "summary": "This study develops deep learning models to forecast the number of patients\nin the emergency department (ED) boarding phase six hours in advance, aiming to\nsupport proactive operational decision-making using only non-clinical,\noperational, and contextual features. Data were collected from five sources: ED\ntracking systems, inpatient census records, weather reports, federal holiday\ncalendars, and local event schedules. After feature engineering, the data were\naggregated at an hourly level, cleaned, and merged into a unified dataset for\nmodel training. Several time series deep learning models, including ResNetPlus,\nTSTPlus, TSiTPlus (from the tsai library), and N-BEATSx, were trained using\nOptuna and grid search for hyperparameter tuning. The average ED boarding count\nwas 28.7, with a standard deviation of 11.2. N-BEATSx achieved the best\nperformance, with a mean absolute error of 2.10, mean squared error of 7.08,\nroot mean squared error of 2.66, and a coefficient of determination of 0.95.\nThe model maintained stable accuracy even during periods of extremely high\nboarding counts, defined as values exceeding one, two, or three standard\ndeviations above the mean. Results show that accurate six-hour-ahead forecasts\nare achievable without using patient-level clinical data. While strong\nperformance was observed even with a basic feature set, the inclusion of\nadditional features improved prediction stability under extreme conditions.\nThis framework offers a practical and generalizable approach for hospital\nsystems to anticipate boarding levels and help mitigate ED overcrowding.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528\u975e\u4e34\u5e8a\u6570\u636e\u548c\u80cc\u666f\u7279\u5f81\uff0c\u63d0\u524d6\u5c0f\u65f6\u9884\u6d4b\u6025\u8bca\u79d1\uff08ED\uff09\u5019\u8bca\u60a3\u8005\u6570\u91cf\uff0c\u652f\u6301\u533b\u9662\u63d0\u524d\u51b3\u7b56\u3002N-BEATSx\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u8bef\u5dee\u8f83\u4f4e\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6025\u8bca\u79d1\u5019\u8bca\u60a3\u8005\u6570\u91cf\u9884\u6d4b\u95ee\u9898\uff0c\u652f\u6301\u533b\u9662\u8fd0\u8425\u51b3\u7b56\uff0c\u907f\u514d\u8fc7\u5ea6\u62e5\u6324\u3002", "method": "\u4ece\u4e94\u4e2a\u6570\u636e\u6e90\u6536\u96c6\u6570\u636e\uff0c\u8fdb\u884c\u7279\u5f81\u5de5\u7a0b\u540e\uff0c\u4f7f\u7528\u591a\u79cd\u65f6\u95f4\u5e8f\u5217\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982ResNetPlus\u3001TSTPlus\u3001TSiTPlus\u548cN-BEATSx\uff09\u8fdb\u884c\u8bad\u7ec3\u548c\u4f18\u5316\u3002", "result": "N-BEATSx\u6a21\u578b\u8868\u73b0\u6700\u4f18\uff0c\u9884\u6d4b\u8bef\u5dee\u4f4e\uff0c\u4e14\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u533b\u9662\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u53ef\u9884\u6d4b\u6025\u8bca\u5019\u8bca\u60c5\u51b5\uff0c\u7f13\u89e3\u62e5\u6324\u3002", "keywords": "\u6df1\u5ea6\u5b66\u4e60,\u6025\u8bca\u79d1,\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b,N-BEATSx,\u8fd0\u8425\u51b3\u7b56"}}
{"id": "2505.15276", "pdf": "https://arxiv.org/pdf/2505.15276", "abs": "https://arxiv.org/abs/2505.15276", "authors": ["Rongzhi Zhu", "Yi Liu", "Zequn Sun", "Yiwei Wang", "Wei Hu"], "title": "When Can Large Reasoning Models Save Thinking? Mechanistic Analysis of Behavioral Divergence in Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) have significantly advanced performance on\ncomplex tasks, yet their tendency to overthink introduces inefficiencies. This\nstudy investigates the internal mechanisms of reinforcement learning\n(RL)-trained LRMs when prompted to save thinking, revealing three distinct\nthinking modes: no thinking (NT), explicit thinking (ET), and implicit thinking\n(IT). Through comprehensive analysis of confidence in thinking termination,\nattention from thinking to generation, and attentional focus on input sections,\nwe uncover key factors influencing the reasoning behaviors. We further find\nthat NT reduces output length at the cost of accuracy, while ET and IT maintain\naccuracy with reduced response length. Our findings expose fundamental\ninconsistencies in RL-optimized LRMs, necessitating adaptive improvements for\nreliable efficiency.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0b\u5b58\u5728\u4e09\u79cd\u601d\u8003\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u5f71\u54cd\u63a8\u7406\u884c\u4e3a\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u6307\u51fa\u9700\u6539\u8fdb\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u63a2\u7d22LRM\u7684\u5185\u90e8\u673a\u5236\uff0c\u4ee5\u51cf\u5c11\u56e0\u5176\u8fc7\u5ea6\u601d\u8003\u5e26\u6765\u7684\u6548\u7387\u95ee\u9898\u3002", "method": "\u5206\u6790\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684LRM\u7684\u601d\u8003\u7ec8\u6b62\u4fe1\u5fc3\u3001\u6ce8\u610f\u529b\u8f6c\u79fb\u53ca\u8f93\u5165\u90e8\u5206\u5173\u6ce8\u5ea6\u3002", "result": "\u65e0\u601d\u8003\uff08NT\uff09\u6a21\u5f0f\u51cf\u5c11\u8f93\u51fa\u4f46\u727a\u7272\u51c6\u786e\u6027\uff0c\u663e\u5f0f\uff08ET\uff09\u548c\u9690\u5f0f\uff08IT\uff09\u601d\u8003\u6a21\u5f0f\u4fdd\u6301\u51c6\u786e\u6027\u540c\u65f6\u51cf\u5c11\u54cd\u5e94\u957f\u5ea6\u3002", "conclusion": "RL\u4f18\u5316\u7684LRM\u5b58\u5728\u57fa\u672c\u4e0d\u4e00\u81f4\u6027\uff0c\u9700\u81ea\u9002\u5e94\u6539\u8fdb\u4ee5\u5b9e\u73b0\u53ef\u9760\u6548\u7387\u3002", "keywords": "\u5927\u578b\u63a8\u7406\u6a21\u578b, \u5f3a\u5316\u5b66\u4e60, \u601d\u8003\u6a21\u5f0f, \u6548\u7387, \u51c6\u786e\u6027"}}
{"id": "2505.14892", "pdf": "https://arxiv.org/pdf/2505.14892", "abs": "https://arxiv.org/abs/2505.14892", "authors": ["Jacob X Li", "Shreyas S Raman", "Jessica Wan", "Fahad Samman", "Jazlyn Lin"], "title": "Scaling Laws for State Dynamics in Large Language Models", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.1; I.2.4; I.5.4"], "comment": "16 pages; 23 figures", "summary": "Large Language Models (LLMs) are increasingly used in tasks requiring\ninternal state tracking, yet their ability to model state transition dynamics\nremains poorly understood. We evaluate how well LLMs capture deterministic\nstate dynamics across 3 domains: Box Tracking, Abstract DFA Sequences, and\nComplex Text Games, each formalizable as a finite-state system. Across tasks,\nwe find that next-state prediction accuracy degrades with increasing\nstate-space size and sparse transitions. GPT-2 XL reaches about 70% accuracy in\nlow-complexity settings but drops below 30% when the number of boxes or states\nexceeds 5 or 10, respectively. In DFA tasks, Pythia-1B fails to exceed 50%\naccuracy when the number of states is > 10 and transitions are < 30. Through\nactivation patching, we identify attention heads responsible for propagating\nstate information: GPT-2 XL Layer 22 Head 20, and Pythia-1B Heads at Layers 10,\n11, 12, and 14. While these heads successfully move relevant state features,\naction information is not reliably routed to the final token, indicating weak\njoint state-action reasoning. Our results suggest that state tracking in LLMs\nemerges from distributed interactions of next-token heads rather than explicit\nsymbolic computation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8ddf\u8e2a\u5185\u90e8\u72b6\u6001\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u72b6\u6001\u8f6c\u6362\u52a8\u6001\u5efa\u6a21\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u5728\u72b6\u6001\u7a7a\u95f4\u589e\u5927\u6216\u8f6c\u6362\u7a00\u758f\u65f6\u51c6\u786e\u6027\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u786e\u5b9a\u6027\u72b6\u6001\u52a8\u6001\u5efa\u6a21\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u7406\u89e3\u5176\u5185\u90e8\u72b6\u6001\u8ddf\u8e2a\u80fd\u529b\u7684\u673a\u5236\u548c\u9650\u5236\u3002", "method": "\u5728\u4e09\u4e2a\u9886\u57df\uff08Box Tracking\u3001Abstract DFA Sequences\u548cComplex Text Games\uff09\u8bc4\u4f30LLMs\u7684\u72b6\u6001\u8f6c\u6362\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u6fc0\u6d3b\u4fee\u8865\u6280\u672f\u8bc6\u522b\u76f8\u5173\u6ce8\u610f\u529b\u5934\u3002", "result": "\u968f\u7740\u72b6\u6001\u7a7a\u95f4\u589e\u5927\u6216\u8f6c\u6362\u7a00\u758f\uff0cLLMs\u7684\u51c6\u786e\u6027\u663e\u8457\u4e0b\u964d\uff1b\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u8d1f\u8d23\u72b6\u6001\u4fe1\u606f\u4f20\u64ad\uff0c\u4f46\u72b6\u6001-\u52a8\u4f5c\u8054\u5408\u63a8\u7406\u8f83\u5f31\u3002", "conclusion": "LLMs\u7684\u72b6\u6001\u8ddf\u8e2a\u80fd\u529b\u6e90\u4e8e\u591a\u4e2a\u6ce8\u610f\u529b\u5934\u7684\u5206\u5e03\u5f0f\u4ea4\u4e92\uff0c\u800c\u975e\u663e\u5f0f\u7b26\u53f7\u8ba1\u7b97\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u72b6\u6001\u8ddf\u8e2a,\u6ce8\u610f\u529b\u673a\u5236,\u72b6\u6001\u8f6c\u6362,\u7b26\u53f7\u8ba1\u7b97"}}
{"id": "2505.14766", "pdf": "https://arxiv.org/pdf/2505.14766", "abs": "https://arxiv.org/abs/2505.14766", "authors": ["Ben Cohen", "Emaad Khwaja", "Youssef Doubli", "Salahidine Lemaachi", "Chris Lettieri", "Charles Masson", "Hugo Miccinilli", "Elise Ram\u00e9", "Qiqi Ren", "Afshin Rostamizadeh", "Jean Ogier du Terrail", "Anna-Monica Toon", "Kan Wang", "Stephan Xie", "David Asker", "Ameet Talwalkar", "Othmane Abou-Amal"], "title": "This Time is Different: An Observability Perspective on Time Series Foundation Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce Toto, a time series forecasting foundation model with 151\nmillion parameters. Toto uses a modern decoder-only architecture coupled with\narchitectural innovations designed to account for specific challenges found in\nmultivariate observability time series data. Toto's pre-training corpus is a\nmixture of observability data, open datasets, and synthetic data, and is\n4-10$\\times$ larger than those of leading time series foundation models.\nAdditionally, we introduce BOOM, a large-scale benchmark consisting of 350\nmillion observations across 2,807 real-world time series. For both Toto and\nBOOM, we source observability data exclusively from Datadog's own telemetry and\ninternal observability metrics. Extensive evaluations demonstrate that Toto\nachieves state-of-the-art performance on both BOOM and on established general\npurpose time series forecasting benchmarks. Toto's model weights, inference\ncode, and evaluation scripts, as well as BOOM's data and evaluation code, are\nall available as open source under the Apache 2.0 License available at\nhttps://huggingface.co/Datadog/Toto-Open-Base-1.0 and\nhttps://github.com/DataDog/toto.", "AI": {"tldr": "Toto\u662f\u4e00\u4e2a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u7840\u6a21\u578b\uff0c\u53c2\u6570\u8fbe1.51\u4ebf\uff0c\u9488\u5bf9\u591a\u5143\u53ef\u89c2\u6d4b\u6027\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u6311\u6218\u8fdb\u884c\u4e86\u67b6\u6784\u521b\u65b0\uff0c\u5e76\u5728\u5927\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u5728\u5904\u7406\u591a\u5143\u53ef\u89c2\u6d4b\u6027\u6570\u636e\u65f6\u9762\u4e34\u6311\u6218\uff0cToto\u65e8\u5728\u901a\u8fc7\u521b\u65b0\u67b6\u6784\u548c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6570\u636e\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Toto\u91c7\u7528\u4ec5\u89e3\u7801\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u4e86\u4e13\u95e8\u4e3a\u591a\u5143\u53ef\u89c2\u6d4b\u6027\u6570\u636e\u8bbe\u8ba1\u7684\u521b\u65b0\u6280\u672f\uff0c\u5e76\u4f7f\u7528\u6df7\u5408\u6570\u636e\u9884\u8bad\u7ec3\u3002", "result": "Toto\u5728BOOM\u57fa\u51c6\u6d4b\u8bd5\u548c\u901a\u7528\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "Toto\u901a\u8fc7\u67b6\u6784\u521b\u65b0\u548c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5143\u53ef\u89c2\u6d4b\u6027\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u9884\u6d4b\u80fd\u529b\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u57fa\u7840\u6a21\u578b\uff0c\u53ef\u89c2\u6d4b\u6027\u6570\u636e\uff0cBOOM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9884\u8bad\u7ec3"}}
{"id": "2505.15400", "pdf": "https://arxiv.org/pdf/2505.15400", "abs": "https://arxiv.org/abs/2505.15400", "authors": ["Xiaoyun Zhang", "Jingqing Ruan", "Xing Ma", "Yawen Zhu", "Haodong Zhao", "Hao Li", "Jiansong Chen", "Ke Zeng", "Xunliang Cai"], "title": "When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) achieve remarkable performance via long\nreasoning chains, but often incur excessive computational overhead due to\nredundant reasoning, especially on simple tasks. In this work, we\nsystematically quantify the upper bounds of LRMs under both Long-Thinking and\nNo-Thinking modes, and uncover the phenomenon of \"Internal Self-Recovery\nMechanism\" where models implicitly supplement reasoning during answer\ngeneration. Building on this insight, we propose Adaptive Self-Recovery\nReasoning (ASRR), a framework that suppresses unnecessary reasoning and enables\nimplicit recovery. By introducing accuracy-aware length reward regulation, ASRR\nadaptively allocates reasoning effort according to problem difficulty,\nachieving high efficiency with negligible performance sacrifice. Experiments\nacross multiple benchmarks and models show that, compared with GRPO, ASRR\nreduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal\naccuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates\non safety benchmarks (up to +21.7%). Our results highlight the potential of\nASRR for enabling efficient, adaptive, and safer reasoning in LRMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u81ea\u6211\u6062\u590d\u63a8\u7406\uff08ASRR\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6291\u5236\u4e0d\u5fc5\u8981\u7684\u63a8\u7406\u548c\u9690\u5f0f\u6062\u590d\uff0c\u663e\u8457\u63d0\u9ad8\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u5904\u7406\u7b80\u5355\u4efb\u52a1\u65f6\u5b58\u5728\u5197\u4f59\u63a8\u7406\u95ee\u9898\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927\u3002\u7814\u7a76\u65e8\u5728\u91cf\u5316LRM\u7684\u6027\u80fd\u4e0a\u9650\uff0c\u5e76\u63a2\u7d22\u5176\u5185\u90e8\u81ea\u6211\u6062\u590d\u673a\u5236\u3002", "method": "\u63d0\u51faASRR\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u5ea6\u611f\u77e5\u7684\u957f\u5ea6\u5956\u52b1\u8c03\u8282\uff0c\u81ea\u9002\u5e94\u5206\u914d\u63a8\u7406\u8d44\u6e90\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cASRR\u663e\u8457\u51cf\u5c11\u63a8\u7406\u9884\u7b97\uff08\u6700\u9ad832.5%\uff09\uff0c\u6027\u80fd\u635f\u5931\u6781\u5c0f\uff08\u6700\u9ad81.2% pass@1\uff09\uff0c\u5e76\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\uff08\u6700\u9ad8+21.7%\uff09\u3002", "conclusion": "ASRR\u6846\u67b6\u4e3aLRM\u63d0\u4f9b\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u4e14\u5b89\u5168\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u5176\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002", "keywords": "\u5927\u578b\u63a8\u7406\u6a21\u578b,\u81ea\u9002\u5e94\u63a8\u7406,\u5197\u4f59\u63a8\u7406,\u5185\u90e8\u81ea\u6211\u6062\u590d\u673a\u5236,ASRR"}}
{"id": "2505.14905", "pdf": "https://arxiv.org/pdf/2505.14905", "abs": "https://arxiv.org/abs/2505.14905", "authors": ["Xiaoyan Bai", "Ike Peng", "Aditya Singh", "Chenhao Tan"], "title": "Concept Incongruence: An Exploration of Time and Death in Role Playing", "categories": ["cs.CL"], "comment": "Our code is available, see\n  https://github.com/ChicagoHAI/concept-incongruence.git", "summary": "Consider this prompt \"Draw a unicorn with two horns\". Should large language\nmodels (LLMs) recognize that a unicorn has only one horn by definition and ask\nusers for clarifications, or proceed to generate something anyway? We introduce\nconcept incongruence to capture such phenomena where concept boundaries clash\nwith each other, either in user prompts or in model representations, often\nleading to under-specified or mis-specified behaviors. In this work, we take\nthe first step towards defining and analyzing model behavior under concept\nincongruence. Focusing on temporal boundaries in the Role-Play setting, we\npropose three behavioral metrics--abstention rate, conditional accuracy, and\nanswer rate--to quantify model behavior under incongruence due to the role's\ndeath. We show that models fail to abstain after death and suffer from an\naccuracy drop compared to the Non-Role-Play setting. Through probing\nexperiments, we identify two main causes: (i) unreliable encoding of the\n\"death\" state across different years, leading to unsatisfactory abstention\nbehavior, and (ii) role playing causes shifts in the model's temporal\nrepresentations, resulting in accuracy drops. We leverage these insights to\nimprove consistency in the model's abstention and answer behaviors. Our\nfindings suggest that concept incongruence leads to unexpected model behaviors\nand point to future directions on improving model behavior under concept\nincongruence.", "AI": {"tldr": "LLMs\u9047\u5230\u6982\u5ff5\u51b2\u7a81\u65f6\u662f\u5426\u5e94\u6307\u51fa\u95ee\u9898\u6216\u76f4\u63a5\u751f\u6210\u5185\u5bb9\uff1f\u672c\u6587\u5f15\u5165\u6982\u5ff5\u51b2\u7a81\u5206\u6790\u6a21\u578b\u884c\u4e3a\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u89d2\u8272\u6b7b\u4ea1\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u7528\u6237\u63d0\u793a\u6216\u6a21\u578b\u8868\u793a\u4e2d\u6982\u5ff5\u51b2\u7a81\u65f6\u7684\u884c\u4e3a\uff0c\u4ee5\u89e3\u51b3\u672a\u5b9a\u4e49\u6216\u9519\u8bef\u5b9a\u4e49\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u8bbe\u7f6e\u4e2d\u7684\u65f6\u95f4\u8fb9\u754c\uff0c\u63d0\u51fa\u4e09\u9879\u884c\u4e3a\u6307\u6807\uff08\u56de\u907f\u7387\u3001\u6761\u4ef6\u51c6\u786e\u7387\u548c\u56de\u7b54\u7387\uff09\u91cf\u5316\u6a21\u578b\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u539f\u56e0\u3002", "result": "\u6a21\u578b\u5728\u89d2\u8272\u6b7b\u4ea1\u540e\u672a\u56de\u907f\uff0c\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u4e3b\u8981\u7531\u4e8e\u6b7b\u4ea1\u72b6\u6001\u7f16\u7801\u4e0d\u53ef\u9760\u548c\u89d2\u8272\u626e\u6f14\u5bfc\u81f4\u7684\u65f6\u95f4\u8868\u793a\u504f\u79fb\u3002", "conclusion": "\u6982\u5ff5\u51b2\u7a81\u5bfc\u81f4\u6a21\u578b\u884c\u4e3a\u5f02\u5e38\uff0c\u672a\u6765\u9700\u6539\u8fdb\u6a21\u578b\u5728\u6b64\u7c7b\u60c5\u51b5\u4e0b\u7684\u884c\u4e3a\u4e00\u81f4\u6027\u3002", "keywords": "\u6982\u5ff5\u51b2\u7a81,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u89d2\u8272\u626e\u6f14,\u65f6\u95f4\u8fb9\u754c,\u6a21\u578b\u884c\u4e3a"}}
{"id": "2505.14777", "pdf": "https://arxiv.org/pdf/2505.14777", "abs": "https://arxiv.org/abs/2505.14777", "authors": ["Mingquan Feng", "Yixin Huang", "Yifan Fu", "Shaobo Wang", "Junchi Yan"], "title": "KO: Kinetics-inspired Neural Optimizer with PDE Simulation Approaches", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The design of optimization algorithms for neural networks remains a critical\nchallenge, with most existing methods relying on heuristic adaptations of\ngradient-based approaches. This paper introduces KO (Kinetics-inspired\nOptimizer), a novel neural optimizer inspired by kinetic theory and partial\ndifferential equation (PDE) simulations. We reimagine the training dynamics of\nnetwork parameters as the evolution of a particle system governed by kinetic\nprinciples, where parameter updates are simulated via a numerical scheme for\nthe Boltzmann transport equation (BTE) that models stochastic particle\ncollisions. This physics-driven approach inherently promotes parameter\ndiversity during optimization, mitigating the phenomenon of parameter\ncondensation, i.e. collapse of network parameters into low-dimensional\nsubspaces, through mechanisms analogous to thermal diffusion in physical\nsystems. We analyze this property, establishing both a mathematical proof and a\nphysical interpretation. Extensive experiments on image classification\n(CIFAR-10/100, ImageNet) and text classification (IMDB, Snips) tasks\ndemonstrate that KO consistently outperforms baseline optimizers (e.g., Adam,\nSGD), achieving accuracy improvements while computation cost remains\ncomparable.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u529b\u5b66\u7406\u8bba\u548cPDE\u6a21\u62df\u7684\u65b0\u4f18\u5316\u5668KO\uff0c\u901a\u8fc7BTE\u6a21\u62df\u53c2\u6570\u66f4\u65b0\uff0c\u63d0\u5347\u53c2\u6570\u591a\u6837\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u4f18\u5316\u5668\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u5668\u591a\u57fa\u4e8e\u542f\u53d1\u5f0f\u68af\u5ea6\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\uff0cKO\u901a\u8fc7\u7269\u7406\u9a71\u52a8\u7684\u52a8\u529b\u5b66\u7406\u8bba\u6539\u8fdb\u4f18\u5316\u8fc7\u7a0b\u3002", "method": "\u5229\u7528Boltzmann\u8f93\u8fd0\u65b9\u7a0b\u6a21\u62df\u53c2\u6570\u66f4\u65b0\u7684\u52a8\u6001\uff0c\u7c7b\u4f3c\u4e8e\u7c92\u5b50\u7cfb\u7edf\u7684\u968f\u673a\u78b0\u649e\uff0c\u901a\u8fc7\u70ed\u6269\u6563\u673a\u5236\u9632\u6b62\u53c2\u6570\u574d\u7f29\u3002", "result": "\u5728\u56fe\u50cf\u548c\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cKO\u5728\u4fdd\u6301\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u7cbe\u5ea6\u4f18\u4e8eAdam\u548cSGD\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "KO\u901a\u8fc7\u7269\u7406\u9a71\u52a8\u7684\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u4e86\u4f18\u5316\u6548\u679c\uff0c\u4e3a\u89e3\u51b3\u53c2\u6570\u574d\u7f29\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc\u4f18\u5316, \u52a8\u529b\u5b66\u7406\u8bba, Boltzmann\u8f93\u8fd0\u65b9\u7a0b, \u53c2\u6570\u591a\u6837\u6027, \u70ed\u6269\u6563"}}
{"id": "2505.15410", "pdf": "https://arxiv.org/pdf/2505.15410", "abs": "https://arxiv.org/abs/2505.15410", "authors": ["Bahar Radmehr", "Ekaterina Shved", "Fatma Bet\u00fcl G\u00fcre\u015f", "Adish Singla", "Tanja K\u00e4ser"], "title": "ClickSight: Interpreting Student Clickstreams to Reveal Insights on Learning Strategies via LLMs", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted in Latebreaking results track in AIED 2025(26th\n  International Conference on Artificial Intelligence in Education JULY 22-26,\n  2025 PALERMO, ITALY)", "summary": "Clickstream data from digital learning environments offer valuable insights\ninto students' learning behaviors, but are challenging to interpret due to\ntheir high dimensionality and granularity. Prior approaches have relied mainly\non handcrafted features, expert labeling, clustering, or supervised models,\ntherefore often lacking generalizability and scalability. In this work, we\nintroduce ClickSight, an in-context Large Language Model (LLM)-based pipeline\nthat interprets student clickstreams to reveal their learning strategies.\nClickSight takes raw clickstreams and a list of learning strategies as input\nand generates textual interpretations of students' behaviors during\ninteraction. We evaluate four different prompting strategies and investigate\nthe impact of self-refinement on interpretation quality. Our evaluation spans\ntwo open-ended learning environments and uses a rubric-based domain-expert\nevaluation. Results show that while LLMs can reasonably interpret learning\nstrategies from clickstreams, interpretation quality varies by prompting\nstrategy, and self-refinement offers limited improvement. ClickSight\ndemonstrates the potential of LLMs to generate theory-driven insights from\neducational interaction data.", "AI": {"tldr": "ClickSight\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u4ece\u5b66\u751f\u7684\u70b9\u51fb\u6d41\u6570\u636e\u4e2d\u89e3\u8bfb\u5b66\u4e60\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u624b\u5de5\u7279\u5f81\u6216\u76d1\u7763\u6a21\u578b\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002ClickSight\u65e8\u5728\u5229\u7528LLM\u63d0\u4f9b\u7406\u8bba\u9a71\u52a8\u7684\u5b66\u751f\u884c\u4e3a\u89e3\u8bfb\u3002", "method": "ClickSight\u5c06\u539f\u59cb\u70b9\u51fb\u6d41\u548c\u5b66\u4e60\u7b56\u7565\u5217\u8868\u4f5c\u4e3a\u8f93\u5165\uff0c\u751f\u6210\u6587\u672c\u89e3\u8bfb\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u5e76\u63a2\u8ba8\u4e86\u81ea\u4f18\u5316\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0cLLM\u80fd\u5408\u7406\u89e3\u8bfb\u5b66\u4e60\u7b56\u7565\uff0c\u4f46\u8d28\u91cf\u56e0\u63d0\u793a\u7b56\u7565\u800c\u5f02\uff0c\u81ea\u4f18\u5316\u6539\u8fdb\u6709\u9650\u3002", "conclusion": "ClickSight\u5c55\u793a\u4e86LLM\u5728\u6559\u80b2\u4ea4\u4e92\u6570\u636e\u4e2d\u751f\u6210\u7406\u8bba\u9a71\u52a8\u89c1\u89e3\u7684\u6f5c\u529b\u3002", "keywords": "\u70b9\u51fb\u6d41\u6570\u636e, \u5b66\u4e60\u7b56\u7565, \u5927\u8bed\u8a00\u6a21\u578b, \u6559\u80b2\u6280\u672f"}}
{"id": "2505.14906", "pdf": "https://arxiv.org/pdf/2505.14906", "abs": "https://arxiv.org/abs/2505.14906", "authors": ["Ye Yuan", "Haolun Wu", "Hao Zhou", "Xue Liu", "Hao Chen", "Yan Xin", "Jianzhong", "Zhang"], "title": "Understanding 6G through Language Models: A Case Study on LLM-aided Structured Entity Extraction in Telecom Domain", "categories": ["cs.CL", "cs.SY", "eess.SY"], "comment": null, "summary": "Knowledge understanding is a foundational part of envisioned 6G networks to\nadvance network intelligence and AI-native network architectures. In this\nparadigm, information extraction plays a pivotal role in transforming\nfragmented telecom knowledge into well-structured formats, empowering diverse\nAI models to better understand network terminologies. This work proposes a\nnovel language model-based information extraction technique, aiming to extract\nstructured entities from the telecom context. The proposed telecom structured\nentity extraction (TeleSEE) technique applies a token-efficient representation\nmethod to predict entity types and attribute keys, aiming to save the number of\noutput tokens and improve prediction accuracy. Meanwhile, TeleSEE involves a\nhierarchical parallel decoding method, improving the standard encoder-decoder\narchitecture by integrating additional prompting and decoding strategies into\nentity extraction tasks. In addition, to better evaluate the performance of the\nproposed technique in the telecom domain, we further designed a dataset named\n6GTech, including 2390 sentences and 23747 words from more than 100 6G-related\ntechnical publications. Finally, the experiment shows that the proposed TeleSEE\nmethod achieves higher accuracy than other baseline techniques, and also\npresents 5 to 9 times higher sample processing speed.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u4fe1\u606f\u63d0\u53d6\u6280\u672fTeleSEE\uff0c\u7528\u4e8e\u4ece\u7535\u4fe1\u9886\u57df\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u5b9e\u4f53\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u8868\u793a\u65b9\u6cd5\u548c\u5206\u5c42\u5e76\u884c\u89e3\u7801\u63d0\u5347\u51c6\u786e\u6027\u548c\u5904\u7406\u901f\u5ea6\u3002", "motivation": "6G\u7f51\u7edc\u4e2d\u77e5\u8bc6\u7406\u89e3\u5bf9\u63a8\u52a8\u7f51\u7edc\u667a\u80fd\u548cAI\u539f\u751f\u67b6\u6784\u81f3\u5173\u91cd\u8981\uff0c\u4fe1\u606f\u63d0\u53d6\u80fd\u5c06\u788e\u7247\u5316\u7535\u4fe1\u77e5\u8bc6\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u683c\u5f0f\uff0c\u5e2e\u52a9AI\u6a21\u578b\u66f4\u597d\u5730\u7406\u89e3\u7f51\u7edc\u672f\u8bed\u3002", "method": "TeleSEE\u91c7\u7528\u4ee4\u724c\u9ad8\u6548\u8868\u793a\u65b9\u6cd5\u6765\u9884\u6d4b\u5b9e\u4f53\u7c7b\u578b\u548c\u5c5e\u6027\u952e\uff0c\u51cf\u5c11\u8f93\u51fa\u4ee4\u724c\u6570\u91cf\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\uff1b\u540c\u65f6\u5f15\u5165\u5206\u5c42\u5e76\u884c\u89e3\u7801\u65b9\u6cd5\uff0c\u4f18\u5316\u6807\u51c6\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTeleSEE\u6bd4\u5176\u4ed6\u57fa\u7ebf\u6280\u672f\u51c6\u786e\u6027\u66f4\u9ad8\uff0c\u6837\u672c\u5904\u7406\u901f\u5ea6\u63d0\u53475\u52309\u500d\u3002", "conclusion": "TeleSEE\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u7535\u4fe1\u9886\u57df\u4fe1\u606f\u63d0\u53d6\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e6G\u7f51\u7edc\u77e5\u8bc6\u7406\u89e3\u3002", "keywords": "6G\u7f51\u7edc,\u4fe1\u606f\u63d0\u53d6,\u8bed\u8a00\u6a21\u578b,\u7ed3\u6784\u5316\u5b9e\u4f53,TeleSEE"}}
{"id": "2505.14802", "pdf": "https://arxiv.org/pdf/2505.14802", "abs": "https://arxiv.org/abs/2505.14802", "authors": ["Iman Kazemian", "Paritosh Ramanan", "Murat Yildirim"], "title": "Text embedding models can be great data engineers", "categories": ["cs.LG"], "comment": null, "summary": "Data engineering pipelines are essential - albeit costly - components of\npredictive analytics frameworks requiring significant engineering time and\ndomain expertise for carrying out tasks such as data ingestion, preprocessing,\nfeature extraction, and feature engineering. In this paper, we propose ADEPT,\nan automated data engineering pipeline via text embeddings. At the core of the\nADEPT framework is a simple yet powerful idea that the entropy of embeddings\ncorresponding to textually dense raw format representation of time series can\nbe intuitively viewed as equivalent (or in many cases superior) to that of\nnumerically dense vector representations obtained by data engineering\npipelines. Consequently, ADEPT uses a two step approach that (i) leverages text\nembeddings to represent the diverse data sources, and (ii) constructs a\nvariational information bottleneck criteria to mitigate entropy variance in\ntext embeddings of time series data. ADEPT provides an end-to-end automated\nimplementation of predictive models that offers superior predictive performance\ndespite issues such as missing data, ill-formed records, improper or corrupted\ndata formats and irregular timestamps. Through exhaustive experiments, we show\nthat the ADEPT outperforms the best existing benchmarks in a diverse set of\ndatasets from large-scale applications across healthcare, finance, science and\nindustrial internet of things. Our results show that ADEPT can potentially\nleapfrog many conventional data pipeline steps thereby paving the way for\nefficient and scalable automation pathways for diverse data science\napplications.", "AI": {"tldr": "ADEPT\u662f\u4e00\u79cd\u901a\u8fc7\u6587\u672c\u5d4c\u5165\u81ea\u52a8\u5316\u7684\u6570\u636e\u5de5\u7a0b\u7ba1\u9053\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u4f18\u4e8e\u4f20\u7edf\u6570\u636e\u5de5\u7a0b\u7ba1\u9053\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u5de5\u7a0b\u7ba1\u9053\u9700\u8981\u5927\u91cf\u5de5\u7a0b\u65f6\u95f4\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u6210\u672c\u9ad8\u6602\uff0cADEPT\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u6280\u672f\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "ADEPT\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a\u5229\u7528\u6587\u672c\u5d4c\u5165\u8868\u793a\u591a\u6837\u5316\u6570\u636e\u6e90\uff0c\u5e76\u901a\u8fc7\u53d8\u5206\u4fe1\u606f\u74f6\u9888\u51c6\u5219\u51cf\u5c11\u6587\u672c\u5d4c\u5165\u7684\u71b5\u65b9\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cADEPT\u5728\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u5c24\u5176\u5728\u5904\u7406\u7f3a\u5931\u6570\u636e\u3001\u683c\u5f0f\u9519\u8bef\u7b49\u95ee\u9898\u65f6\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "ADEPT\u80fd\u591f\u8df3\u8fc7\u4f20\u7edf\u6570\u636e\u7ba1\u9053\u6b65\u9aa4\uff0c\u4e3a\u6570\u636e\u79d1\u5b66\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u9014\u5f84\u3002", "keywords": "\u6570\u636e\u5de5\u7a0b\u7ba1\u9053, \u6587\u672c\u5d4c\u5165, \u65f6\u95f4\u5e8f\u5217, \u81ea\u52a8\u5316, \u53d8\u5206\u4fe1\u606f\u74f6\u9888"}}
{"id": "2505.15693", "pdf": "https://arxiv.org/pdf/2505.15693", "abs": "https://arxiv.org/abs/2505.15693", "authors": ["Milad Kazemi", "Mateo Perez", "Fabio Somenzi", "Sadegh Soudjani", "Ashutosh Trivedi", "Alvaro Velasquez"], "title": "Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives", "categories": ["cs.AI"], "comment": "29 pages, 6 figures and 2 tables", "summary": "Recent advances in reinforcement learning (RL) have renewed focus on the\ndesign of reward functions that shape agent behavior. Manually designing reward\nfunctions is tedious and error-prone. A principled alternative is to specify\nbehaviors in a formal language that can be automatically translated into\nrewards. Omega-regular languages are a natural choice for this purpose, given\ntheir established role in formal verification and synthesis. However, existing\nmethods using omega-regular specifications typically rely on discounted reward\nRL in episodic settings, with periodic resets. This setup misaligns with the\nsemantics of omega-regular specifications, which describe properties over\ninfinite behavior traces. In such cases, the average reward criterion and the\ncontinuing setting -- where the agent interacts with the environment over a\nsingle, uninterrupted lifetime -- are more appropriate.\n  To address the challenges of infinite-horizon, continuing tasks, we focus on\nabsolute liveness specifications -- a subclass of omega-regular languages that\ncannot be violated by any finite behavior prefix, making them well-suited to\nthe continuing setting. We present the first model-free RL framework that\ntranslates absolute liveness specifications to average-reward objectives. Our\napproach enables learning in communicating MDPs without episodic resetting. We\nalso introduce a reward structure for lexicographic multi-objective\noptimization, aiming to maximize an external average-reward objective among the\npolicies that also maximize the satisfaction probability of a given\nomega-regular specification. Our method guarantees convergence in unknown\ncommunicating MDPs and supports on-the-fly reductions that do not require full\nknowledge of the environment, thus enabling model-free RL. Empirical results\nshow our average-reward approach in continuing setting outperforms\ndiscount-based methods across benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e73\u5747\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u65e0\u9650\u65f6\u95f4\u8303\u56f4\u7684\u6301\u7eed\u4efb\u52a1\u4e2d\u5904\u7406\u7edd\u5bf9\u6d3b\u8dc3\u6027\u89c4\u8303\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u57fa\u4e8e\u6298\u6263\u5956\u52b1\u7684\u65b9\u6cd5\u5728\u6301\u7eed\u73af\u5883\u4e2d\u7684\u4e0d\u9002\u7528\u6027\u3002", "motivation": "\u624b\u52a8\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u8017\u65f6\u4e14\u6613\u51fa\u9519\uff0c\u800comega-\u6b63\u5219\u8bed\u8a00\u867d\u7136\u5728\u5f62\u5f0f\u4e0a\u80fd\u591f\u63cf\u8ff0\u65e0\u9650\u884c\u4e3a\u8f68\u8ff9\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6298\u6263\u5956\u52b1\u548c\u5468\u671f\u6027\u91cd\u7f6e\uff0c\u4e0e\u5b9e\u9645\u8bed\u4e49\u4e0d\u7b26\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u9002\u5e94\u6301\u7eed\u4efb\u52a1\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u7edd\u5bf9\u6d3b\u8dc3\u6027\u89c4\u8303\u8f6c\u6362\u4e3a\u5e73\u5747\u5956\u52b1\u76ee\u6807\uff0c\u652f\u6301\u5728\u65e0\u91cd\u7f6e\u7684\u6301\u7eed\u73af\u5883\u4e2d\u5b66\u4e60\uff0c\u5e76\u5f15\u5165\u5b57\u5178\u5e8f\u591a\u76ee\u6807\u4f18\u5316\u7684\u5956\u52b1\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u5956\u52b1\u65b9\u6cd5\u5728\u6301\u7eed\u73af\u5883\u4e2d\u4f18\u4e8e\u57fa\u4e8e\u6298\u6263\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5728\u6301\u7eed\u73af\u5883\u4e2d\u5e94\u7528omega-\u6b63\u5219\u89c4\u8303\u7684\u6311\u6218\uff0c\u4e3a\u5b9e\u9645\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5408\u9002\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u5e73\u5747\u5956\u52b1, omega-\u6b63\u5219\u8bed\u8a00, \u7edd\u5bf9\u6d3b\u8dc3\u6027\u89c4\u8303, \u6301\u7eed\u4efb\u52a1"}}
{"id": "2505.14917", "pdf": "https://arxiv.org/pdf/2505.14917", "abs": "https://arxiv.org/abs/2505.14917", "authors": ["Zhiwei Liu", "Paul Thompson", "Jiaqi Rong", "Sophia Ananiadou"], "title": "ConspEmoLLM-v2: A robust and stable model to detect sentiment-transformed conspiracy theories", "categories": ["cs.CL"], "comment": "work in progress", "summary": "Despite the many benefits of large language models (LLMs), they can also\ncause harm, e.g., through automatic generation of misinformation, including\nconspiracy theories. Moreover, LLMs can also ''disguise'' conspiracy theories\nby altering characteristic textual features, e.g., by transforming their\ntypically strong negative emotions into a more positive tone. Although several\nstudies have proposed automated conspiracy theory detection methods, they are\nusually trained using human-authored text, whose features can vary from\nLLM-generated text. Furthermore, several conspiracy detection models, including\nthe previously proposed ConspEmoLLM, rely heavily on the typical emotional\nfeatures of human-authored conspiracy content. As such, intentionally disguised\ncontent may evade detection. To combat such issues, we firstly developed an\naugmented version of the ConDID conspiracy detection dataset, ConDID-v2, which\nsupplements human-authored conspiracy tweets with versions rewritten by an LLM\nto reduce the negativity of their original sentiment. The quality of the\nrewritten tweets was verified by combining human and LLM-based assessment. We\nsubsequently used ConDID-v2 to train ConspEmoLLM-v2, an enhanced version of\nConspEmoLLM. Experimental results demonstrate that ConspEmoLLM-v2 retains or\nexceeds the performance of ConspEmoLLM on the original human-authored content\nin ConDID, and considerably outperforms both ConspEmoLLM and several other\nbaselines when applied to sentiment-transformed tweets in ConDID-v2. The\nproject will be available at https://github.com/lzw108/ConspEmoLLM.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7248\u6570\u636e\u96c6ConDID-v2\u548c\u68c0\u6d4b\u6a21\u578bConspEmoLLM-v2\uff0c\u7528\u4e8e\u8bc6\u522bLLM\u751f\u6210\u7684\u60c5\u7eea\u4f2a\u88c5\u9634\u8c0b\u8bba\u5185\u5bb9\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u9634\u8c0b\u8bba\u5185\u5bb9\u53ef\u80fd\u901a\u8fc7\u6539\u53d8\u60c5\u7eea\u7279\u5f81\u4f2a\u88c5\u6210\u6b63\u5e38\u6587\u672c\u7684\u95ee\u9898\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u6b64\u7c7b\u5185\u5bb9\u3002", "method": "\u5f00\u53d1\u4e86\u589e\u5f3a\u7248\u6570\u636e\u96c6ConDID-v2\uff0c\u5305\u542b\u4eba\u5de5\u548cLLM\u91cd\u5199\u7684\u9634\u8c0b\u8bba\u63a8\u6587\uff0c\u5e76\u8bad\u7ec3\u4e86\u6539\u8fdb\u6a21\u578bConspEmoLLM-v2\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cConspEmoLLM-v2\u5728\u539f\u59cb\u548c\u60c5\u7eea\u4f2a\u88c5\u5185\u5bb9\u4e0a\u7684\u68c0\u6d4b\u6027\u80fd\u5747\u4f18\u4e8e\u5148\u524d\u6a21\u578b\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9LLM\u751f\u6210\u7684\u60c5\u7eea\u4f2a\u88c5\u9634\u8c0b\u8bba\u5185\u5bb9\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u9634\u8c0b\u8bba\u68c0\u6d4b, \u60c5\u7eea\u4f2a\u88c5, ConDID-v2, ConspEmoLLM-v2"}}
{"id": "2505.14803", "pdf": "https://arxiv.org/pdf/2505.14803", "abs": "https://arxiv.org/abs/2505.14803", "authors": ["Yu Liu", "Weiyao Tao", "Tong Xia", "Simon Knight", "Tingting Zhu"], "title": "SurvUnc: A Meta-Model Based Uncertainty Quantification Framework for Survival Analysis", "categories": ["cs.LG", "cs.AI", "cs.ET"], "comment": "KDD 2025", "summary": "Survival analysis, which estimates the probability of event occurrence over\ntime from censored data, is fundamental in numerous real-world applications,\nparticularly in high-stakes domains such as healthcare and risk assessment.\nDespite advances in numerous survival models, quantifying the uncertainty of\npredictions from these models remains underexplored and challenging. The lack\nof reliable uncertainty quantification limits the interpretability and\ntrustworthiness of survival models, hindering their adoption in clinical\ndecision-making and other sensitive applications. To bridge this gap, in this\nwork, we introduce SurvUnc, a novel meta-model based framework for post-hoc\nuncertainty quantification for survival models. SurvUnc introduces an\nanchor-based learning strategy that integrates concordance knowledge into\nmeta-model optimization, leveraging pairwise ranking performance to estimate\nuncertainty effectively. Notably, our framework is model-agnostic, ensuring\ncompatibility with any survival model without requiring modifications to its\narchitecture or access to its internal parameters. Especially, we design a\ncomprehensive evaluation pipeline tailored to this critical yet overlooked\nproblem. Through extensive experiments on four publicly available benchmarking\ndatasets and five representative survival models, we demonstrate the\nsuperiority of SurvUnc across multiple evaluation scenarios, including\nselective prediction, misprediction detection, and out-of-domain detection. Our\nresults highlight the effectiveness of SurvUnc in enhancing model\ninterpretability and reliability, paving the way for more trustworthy survival\npredictions in real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSurvUnc\u7684\u65b0\u578b\u5143\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u540e\u9a8c\u751f\u5b58\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5c3d\u7ba1\u751f\u5b58\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982\u533b\u7597\u548c\u98ce\u9669\u8bc4\u4f30\uff09\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5176\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u4fe1\u4efb\u5ea6\u548c\u5e94\u7528\u8303\u56f4\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u951a\u70b9\u7684\u5b66\u4e60\u7b56\u7565\uff0c\u5c06\u4e00\u81f4\u6027\u77e5\u8bc6\u878d\u5165\u5143\u6a21\u578b\u4f18\u5316\u4e2d\uff0c\u5e76\u901a\u8fc7\u6210\u5bf9\u6392\u5e8f\u6027\u80fd\u6709\u6548\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u3002\u8be5\u6846\u67b6\u6a21\u578b\u65e0\u5173\uff0c\u65e0\u9700\u4fee\u6539\u539f\u6a21\u578b\u67b6\u6784\u6216\u8bbf\u95ee\u5185\u90e8\u53c2\u6570\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e94\u79cd\u4ee3\u8868\u6027\u751f\u5b58\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSurvUnc\u5728\u9009\u62e9\u6027\u9884\u6d4b\u3001\u9519\u8bef\u9884\u6d4b\u68c0\u6d4b\u548c\u57df\u5916\u68c0\u6d4b\u7b49\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "SurvUnc\u663e\u8457\u63d0\u5347\u4e86\u751f\u5b58\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u9884\u6d4b\u94fa\u5e73\u4e86\u9053\u8def\u3002", "keywords": "\u751f\u5b58\u5206\u6790\u3001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u5143\u6a21\u578b\u3001\u53ef\u89e3\u91ca\u6027\u3001\u6a21\u578b\u65e0\u5173"}}
{"id": "2505.15742", "pdf": "https://arxiv.org/pdf/2505.15742", "abs": "https://arxiv.org/abs/2505.15742", "authors": ["Adam Gould", "Francesca Toni"], "title": "Neuro-Argumentative Learning with Case-Based Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to NeSy25", "summary": "We introduce Gradual Abstract Argumentation for Case-Based Reasoning (Gradual\nAA-CBR), a data-driven, neurosymbolic classification model in which the outcome\nis determined by an argumentation debate structure that is learned\nsimultaneously with neural-based feature extractors. Each argument in the\ndebate is an observed case from the training data, favouring their labelling.\nCases attack or support those with opposing or agreeing labellings, with the\nstrength of each argument and relationship learned through gradient-based\nmethods. This argumentation debate structure provides human-aligned reasoning,\nimproving model interpretability compared to traditional neural networks (NNs).\nUnlike the existing purely symbolic variant, Abstract Argumentation for\nCase-Based Reasoning (AA-CBR), Gradual AA-CBR is capable of multi-class\nclassification, automatic learning of feature and data point importance,\nassigning uncertainty values to outcomes, using all available data points, and\ndoes not require binary features. We show that Gradual AA-CBR performs\ncomparably to NNs whilst significantly outperforming existing AA-CBR\nformulations.", "AI": {"tldr": "Gradual AA-CBR \u662f\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u7684\u5206\u7c7b\u6a21\u578b\uff0c\u901a\u8fc7\u8fa9\u8bba\u7ed3\u6784\u5b9e\u73b0\u591a\u7c7b\u5206\u7c7b\uff0c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u52a8\u5b66\u4e60\u80fd\u529b\uff0c\u53c8\u80fd\u63d0\u4f9b\u7c7b\u4f3c\u7b26\u53f7\u7cfb\u7edf\u7684\u4eba\u7c7b\u53ef\u7406\u89e3\u63a8\u7406\u8fc7\u7a0b\u7684\u6a21\u578b\u3002", "method": "\u6a21\u578b\u57fa\u4e8e\u6848\u4f8b\u7684\u8fa9\u8bba\u7ed3\u6784\uff0c\u901a\u8fc7\u68af\u5ea6\u65b9\u6cd5\u5b66\u4e60\u53c2\u6570\u548c\u5173\u7cfb\uff0c\u652f\u6301\u591a\u7c7b\u5206\u7c7b\u548c\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u3002", "result": "\u6027\u80fd\u4e0e\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u76f8\u5f53\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u7b26\u53f7\u63a8\u7406\u65b9\u6cd5\u3002", "conclusion": "Gradual AA-CBR \u7ed3\u5408\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u5f3a\u5927\u5b66\u4e60\u80fd\u529b\u4e0e\u7b26\u53f7\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u662f\u5206\u7c7b\u4efb\u52a1\u7684\u6709\u6548\u65b9\u6cd5\u3002", "keywords": "\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf, \u5206\u7c7b, \u53ef\u89e3\u91ca\u6027, \u6848\u4f8b\u63a8\u7406, \u8fa9\u8bba\u7ed3\u6784"}}
{"id": "2505.14918", "pdf": "https://arxiv.org/pdf/2505.14918", "abs": "https://arxiv.org/abs/2505.14918", "authors": ["Fadel M. Megahed", "Ying-Ju Chen", "L. Allision Jones-Farmer", "Younghwa Lee", "Jiawei Brooke Wang", "Inez M. Zwetsloot"], "title": "Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "25 pages", "summary": "This study introduces a framework for evaluating consistency in large\nlanguage model (LLM) binary text classification, addressing the lack of\nestablished reliability assessment methods. Adapting psychometric principles,\nwe determine sample size requirements, develop metrics for invalid responses,\nand evaluate intra- and inter-rater reliability. Our case study examines\nfinancial news sentiment classification across 14 LLMs (including\nclaude-3-7-sonnet, gpt-4o, deepseek-r1, gemma3, llama3.2, phi4, and\ncommand-r-plus), with five replicates per model on 1,350 articles. Models\ndemonstrated high intra-rater consistency, achieving perfect agreement on\n90-98% of examples, with minimal differences between expensive and economical\nmodels from the same families. When validated against StockNewsAPI labels,\nmodels achieved strong performance (accuracy 0.76-0.88), with smaller models\nlike gemma3:1B, llama3.2:3B, and claude-3-5-haiku outperforming larger\ncounterparts. All models performed at chance when predicting actual market\nmovements, indicating task constraints rather than model limitations. Our\nframework provides systematic guidance for LLM selection, sample size planning,\nand reliability assessment, enabling organizations to optimize resources for\nclassification tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e8c\u5143\u6587\u672c\u5206\u7c7b\u4e00\u81f4\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5fc3\u7406\u6d4b\u91cf\u539f\u5219\u786e\u5b9a\u6837\u672c\u91cf\u3001\u5f00\u53d1\u65e0\u6548\u54cd\u5e94\u6307\u6807\uff0c\u5e76\u5728\u91d1\u878d\u65b0\u95fb\u60c5\u611f\u5206\u7c7b\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e8614\u79cdLLM\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u7f3a\u4e4f\u53ef\u9760\u6027\u8bc4\u4f30\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u4e3aLLM\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u7cfb\u7edf\u5316\u7684\u9009\u62e9\u548c\u8d44\u6e90\u4f18\u5316\u6307\u5bfc\u3002", "method": "\u91c7\u7528\u5fc3\u7406\u6d4b\u91cf\u539f\u5219\uff0c\u8bbe\u8ba1\u6837\u672c\u91cf\u8981\u6c42\u548c\u53ef\u9760\u6027\u6307\u6807\uff0c\u901a\u8fc7\u91d1\u878d\u65b0\u95fb\u60c5\u611f\u5206\u7c7b\u7684\u6848\u4f8b\u7814\u7a76\uff08\u5305\u542b14\u79cdLLM\u7684\u591a\u91cd\u6d4b\u8bd5\uff09\u3002", "result": "\u6a21\u578b\u8868\u73b0\u51fa\u9ad8\u5185\u90e8\u5206\u7c7b\u4e00\u81f4\u6027\uff0890-98%\uff09\uff0c\u5c0f\u6a21\u578b\u5982gemma3:1B\u8868\u73b0\u4f18\u4e8e\u5927\u6a21\u578b\uff0c\u4f46\u5bf9\u5e02\u573a\u5b9e\u9645\u8fd0\u52a8\u7684\u9884\u6d4b\u8868\u73b0\u968f\u673a\u3002", "conclusion": "\u6846\u67b6\u4e3aLLM\u9009\u62e9\u3001\u6837\u672c\u89c4\u5212\u548c\u53ef\u9760\u6027\u8bc4\u4f30\u63d0\u4f9b\u7cfb\u7edf\u5de5\u5177\uff0c\u5e2e\u52a9\u7ec4\u7ec7\u4f18\u5316\u5206\u7c7b\u4efb\u52a1\u7684\u8d44\u6e90\u5206\u914d\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u6587\u672c\u5206\u7c7b, \u53ef\u9760\u6027\u8bc4\u4f30, \u60c5\u611f\u5206\u6790, \u91d1\u878d\u65b0\u95fb"}}
{"id": "2505.14820", "pdf": "https://arxiv.org/pdf/2505.14820", "abs": "https://arxiv.org/abs/2505.14820", "authors": ["Rushit N. Shah", "Nikolaos Agadakos", "Synthia Sasulski", "Ali Farajzadeh", "Sanjiban Choudhury", "Brian Ziebart"], "title": "Imitation Learning via Focused Satisficing", "categories": ["cs.LG"], "comment": "Accepted for publication at the 34th International Joint Conference\n  on Artificial Intelligence (IJCAI 2025)", "summary": "Imitation learning often assumes that demonstrations are close to optimal\naccording to some fixed, but unknown, cost function. However, according to\nsatisficing theory, humans often choose acceptable behavior based on their\npersonal (and potentially dynamic) levels of aspiration, rather than achieving\n(near-) optimality. For example, a lunar lander demonstration that successfully\nlands without crashing might be acceptable to a novice despite being slow or\njerky. Using a margin-based objective to guide deep reinforcement learning, our\nfocused satisficing approach to imitation learning seeks a policy that\nsurpasses the demonstrator's aspiration levels -- defined over trajectories or\nportions of trajectories -- on unseen demonstrations without explicitly\nlearning those aspirations. We show experimentally that this focuses the policy\nto imitate the highest quality (portions of) demonstrations better than\nexisting imitation learning methods, providing much higher rates of guaranteed\nacceptability to the demonstrator, and competitive true returns on a range of\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u6ee1\u610f\u5316\u7406\u8bba\u201d\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u8d85\u8d8a\u6f14\u793a\u8005\u7684\u671f\u671b\u6c34\u5e73\uff0c\u800c\u975e\u8ffd\u6c42\u6700\u4f18\u6027\u3002", "motivation": "\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u5047\u8bbe\u6f14\u793a\u63a5\u8fd1\u6700\u4f18\uff0c\u4f46\u4eba\u7c7b\u884c\u4e3a\u5f80\u5f80\u57fa\u4e8e\u52a8\u6001\u7684\u671f\u671b\u6c34\u5e73\u3002\u7814\u7a76\u65e8\u5728\u5229\u7528\u6f14\u793a\u7684\u201c\u6ee1\u610f\u201d\u6807\u51c6\uff0c\u800c\u975e\u6700\u4f18\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8fb9\u9645\u7684\u76ee\u6807\u51fd\u6570\u6307\u5bfc\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u4e13\u6ce8\u4e8e\u8d85\u8d8a\u6f14\u793a\u8005\u7684\u671f\u671b\u6c34\u5e73\uff0c\u800c\u65e0\u9700\u660e\u786e\u5b66\u4e60\u8fd9\u4e9b\u671f\u671b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u4eff\u9ad8\u8d28\u91cf\u6f14\u793a\u90e8\u5206\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4fdd\u8bc1\u66f4\u9ad8\u7684\u53ef\u63a5\u53d7\u6027\uff0c\u5e76\u5728\u591a\u79cd\u73af\u5883\u4e2d\u8868\u73b0\u7ade\u4e89\u529b\u3002", "conclusion": "\u6ee1\u610f\u5316\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u6a21\u4eff\u9ad8\u8d28\u91cf\u884c\u4e3a\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u671f\u671b\u7684\u573a\u666f\u3002", "keywords": "\u6a21\u4eff\u5b66\u4e60, \u6ee1\u610f\u5316\u7406\u8bba, \u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60, \u671f\u671b\u6c34\u5e73, \u8fb9\u9645\u76ee\u6807"}}
{"id": "2505.11626", "pdf": "https://arxiv.org/pdf/2505.11626", "abs": "https://arxiv.org/abs/2505.11626", "authors": ["Udita Patel", "Rutu Mulkar", "Jay Roberts", "Cibi Chakravarthy Senthilkumar", "Sujay Gandhi", "Xiaofei Zheng", "Naumaan Nayyar", "Rafael Castrillo"], "title": "THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose THELMA (Task Based Holistic Evaluation of Large Language Model\nApplications), a reference free framework for RAG (Retrieval Augmented\ngeneration) based question answering (QA) applications. THELMA consist of six\ninterdependent metrics specifically designed for holistic, fine grained\nevaluation of RAG QA applications. THELMA framework helps developers and\napplication owners evaluate, monitor and improve end to end RAG QA pipelines\nwithout requiring labelled sources or reference responses.We also present our\nfindings on the interplay of the proposed THELMA metrics, which can be\ninterpreted to identify the specific RAG component needing improvement in QA\napplications.", "AI": {"tldr": "THELMA\u662f\u4e00\u4e2a\u65e0\u53c2\u8003\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30RAG QA\u5e94\u7528\uff0c\u5305\u542b\u516d\u9879\u76f8\u4e92\u5173\u8054\u7684\u6307\u6807\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u4f18\u5316\u6d41\u7a0b\u3002", "motivation": "\u4e3aRAG QA\u5e94\u7528\u63d0\u4f9b\u4e00\u4e2a\u65e0\u9700\u6807\u6ce8\u6570\u636e\u6216\u53c2\u8003\u54cd\u5e94\u7684\u7cbe\u7ec6\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u516d\u9879\u76f8\u4e92\u5173\u8054\u7684\u6307\u6807\uff0c\u7528\u4e8e\u5168\u9762\u7ec6\u7c92\u5ea6\u8bc4\u4f30QA\u5e94\u7528\u3002", "result": "THELMA\u80fd\u8bc6\u522bRAG\u7ec4\u4ef6\u4e2d\u9700\u8981\u6539\u8fdb\u7684\u90e8\u5206\u3002", "conclusion": "THELMA\u4e3aRAG QA\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bc4\u4f30\u548c\u4f18\u5316\u5de5\u5177\u3002", "keywords": "THELMA, RAG, QA, \u8bc4\u4f30\u6846\u67b6, \u65e0\u53c2\u8003"}}
{"id": "2505.14925", "pdf": "https://arxiv.org/pdf/2505.14925", "abs": "https://arxiv.org/abs/2505.14925", "authors": ["Sil Hamilton", "Rebecca M. M. Hicke", "Matthew Wilkens", "David Mimno"], "title": "Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Although the context length of large language models (LLMs) has increased to\nmillions of tokens, evaluating their effectiveness beyond needle-in-a-haystack\napproaches has proven difficult. We argue that novels provide a case study of\nsubtle, complicated structure and long-range semantic dependencies often over\n128k tokens in length. Inspired by work on computational novel analysis, we\nrelease the Too Long, Didn't Model (TLDM) benchmark, which tests a model's\nability to report plot summary, storyworld configuration, and elapsed narrative\ntime. We find that none of seven tested frontier LLMs retain stable\nunderstanding beyond 64k tokens. Our results suggest language model developers\nmust look beyond \"lost in the middle\" benchmarks when evaluating model\nperformance in complex long-context scenarios. To aid in further development we\nrelease the TLDM benchmark together with reference code and data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTLDM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u672c\u4e2d\u7684\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5927\u7ea664K tokens\u540e\u8868\u73b0\u4e0d\u7a33\u5b9a\u3002", "motivation": "\u5c3d\u7ba1LLMs\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u5df2\u5927\u5e45\u589e\u52a0\uff0c\u4f46\u5bf9\u5176\u5728\u590d\u6742\u957f\u6587\u672c\u4e2d\u80fd\u529b\u7684\u8bc4\u4f30\u4ecd\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u8d85\u51fa\u201c\u5927\u6d77\u635e\u9488\u201d\u65b9\u6cd5\u7684\u90e8\u5206\u3002\u5c0f\u8bf4\u56e0\u5176\u590d\u6742\u7ed3\u6784\u548c\u957f\u7a0b\u8bed\u4e49\u4f9d\u8d56\u6210\u4e3a\u7406\u60f3\u6848\u4f8b\u3002", "method": "\u57fa\u4e8e\u8ba1\u7b97\u5c0f\u8bf4\u5206\u6790\u7684\u73b0\u6709\u5de5\u4f5c\uff0c\u8bbe\u8ba1\u4e86TLDM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6d4b\u8bd5\u6a21\u578b\u5bf9\u60c5\u8282\u6458\u8981\u3001\u6545\u4e8b\u4e16\u754c\u914d\u7f6e\u548c\u53d9\u4e8b\u65f6\u95f4\u7684\u7406\u89e3\u80fd\u529b\u3002", "result": "\u6d4b\u8bd5\u76847\u4e2a\u524d\u6cbfLLMs\u5728\u8d85\u8fc764K tokens\u540e\u5747\u65e0\u6cd5\u4fdd\u6301\u7a33\u5b9a\u7406\u89e3\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u8005\u9700\u5728\u590d\u6742\u957f\u6587\u672c\u573a\u666f\u4e2d\u91c7\u7528\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u201c\u4e2d\u95f4\u4e22\u5931\u201d\u57fa\u51c6\u3002TLDM\u57fa\u51c6\u53ca\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "keywords": "\u957f\u6587\u672c\u7406\u89e3, \u8bed\u8a00\u6a21\u578b\u8bc4\u4f30, TLDM\u57fa\u51c6, \u5c0f\u8bf4\u5206\u6790"}}
{"id": "2505.14821", "pdf": "https://arxiv.org/pdf/2505.14821", "abs": "https://arxiv.org/abs/2505.14821", "authors": ["Runze Zhao", "Yue Yu", "Adams Yiyue Zhu", "Chen Yang", "Dongruo Zhou"], "title": "Sample and Computationally Efficient Continuous-Time Reinforcement Learning with General Function Approximation", "categories": ["cs.LG", "cs.AI"], "comment": "28 pages, 4 figures, 5 tables. Accepted to UAI 2025", "summary": "Continuous-time reinforcement learning (CTRL) provides a principled framework\nfor sequential decision-making in environments where interactions evolve\ncontinuously over time. Despite its empirical success, the theoretical\nunderstanding of CTRL remains limited, especially in settings with general\nfunction approximation. In this work, we propose a model-based CTRL algorithm\nthat achieves both sample and computational efficiency. Our approach leverages\noptimism-based confidence sets to establish the first sample complexity\nguarantee for CTRL with general function approximation, showing that a\nnear-optimal policy can be learned with a suboptimality gap of\n$\\tilde{O}(\\sqrt{d_{\\mathcal{R}} + d_{\\mathcal{F}}}N^{-1/2})$ using $N$\nmeasurements, where $d_{\\mathcal{R}}$ and $d_{\\mathcal{F}}$ denote the\ndistributional Eluder dimensions of the reward and dynamic functions,\nrespectively, capturing the complexity of general function approximation in\nreinforcement learning. Moreover, we introduce structured policy updates and an\nalternative measurement strategy that significantly reduce the number of policy\nupdates and rollouts while maintaining competitive sample efficiency. We\nimplemented experiments to backup our proposed algorithms on continuous control\ntasks and diffusion model fine-tuning, demonstrating comparable performance\nwith significantly fewer policy updates and rollouts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u8fde\u7eed\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e50\u89c2\u7f6e\u4fe1\u96c6\u548c\u7ed3\u6784\u5316\u7b56\u7565\u66f4\u65b0\u5b9e\u73b0\u6837\u672c\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u8fde\u7eed\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\u5728\u7406\u8bba\u7406\u89e3\u4e0a\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u901a\u7528\u51fd\u6570\u903c\u8fd1\u573a\u666f\u4e0b\uff1b\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5229\u7528\u4e50\u89c2\u7f6e\u4fe1\u96c6\u6784\u5efa\u7f6e\u4fe1\u533a\u95f4\uff0c\u63d0\u51fa\u7ed3\u6784\u5316\u7b56\u7565\u66f4\u65b0\u548c\u66ff\u4ee3\u6d4b\u91cf\u7b56\u7565\u4ee5\u51cf\u5c11\u7b56\u7565\u66f4\u65b0\u548crollouts\u6b21\u6570\u3002", "result": "\u7b97\u6cd5\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u548c\u6269\u6563\u6a21\u578b\u5fae\u8c03\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e3aO\u0303(\u221a(d\u1d63 + d_f)/\u221aN)\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8fde\u7eed\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002", "keywords": "\u8fde\u7eed\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60, \u901a\u7528\u51fd\u6570\u903c\u8fd1, \u6837\u672c\u6548\u7387, \u8ba1\u7b97\u6548\u7387, \u4e50\u89c2\u7f6e\u4fe1\u96c6"}}
{"id": "2505.14693", "pdf": "https://arxiv.org/pdf/2505.14693", "abs": "https://arxiv.org/abs/2505.14693", "authors": ["Francisco Arag\u00e3o"], "title": "Propositional Measure Logic", "categories": ["cs.LO", "cs.AI", "Primary: 03B48, Secondary: 68T27, 60A99, 68T37"], "comment": "!0 pages", "summary": "We present a propositional logic with fundamental probabilistic semantics, in\nwhich each formula is given a real measure in the interval $[0,1]$ that\nrepresents its degree of truth. This semantics replaces the binarity of\nclassical logic, while preserving its deductive structure. We demonstrate the\nsoundness theorem, establishing that the proposed system is sound and suitable\nfor reasoning under uncertainty. We discuss potential applications and avenues\nfor future extensions of the theory. We apply probabilistic logic to a still\nrefractory problem in Bayesian Networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u8bed\u4e49\u7684\u547d\u9898\u903b\u8f91\uff0c\u5c06\u516c\u5f0f\u7684\u771f\u503c\u4ece\u4e8c\u5143\u6269\u5c55\u5230\u533a\u95f4$[0,1]$\uff0c\u4fdd\u7559\u7ecf\u5178\u903b\u8f91\u7684\u63a8\u7406\u7ed3\u6784\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u7ecf\u5178\u903b\u8f91\u4e8c\u503c\u6027\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\u63d0\u4f9b\u66f4\u5408\u7406\u7684\u8bed\u4e49\u6846\u67b6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6982\u7387\u8bed\u4e49\u7684\u547d\u9898\u903b\u8f91\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u8bc1\u660e\u5176\u53ef\u9760\u6027\uff08soundness\uff09\u9a8c\u8bc1\u5176\u5408\u7406\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u9002\u7528\u4e8e\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u8d1d\u53f6\u65af\u7f51\u7edc\u4e2d\u7684\u987d\u56fa\u95ee\u9898\u3002", "conclusion": "\u8be5\u903b\u8f91\u7cfb\u7edf\u4e3a\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u5de5\u5177\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u6982\u7387\u903b\u8f91, \u547d\u9898\u903b\u8f91, \u4e0d\u786e\u5b9a\u6027\u63a8\u7406, \u8d1d\u53f6\u65af\u7f51\u7edc"}}
{"id": "2505.14963", "pdf": "https://arxiv.org/pdf/2505.14963", "abs": "https://arxiv.org/abs/2505.14963", "authors": ["Shan Chen", "Pedro Moreira", "Yuxin Xiao", "Sam Schmidgall", "Jeremy Warner", "Hugo Aerts", "Thomas Hartvigsen", "Jack Gallifant", "Danielle S. Bitterman"], "title": "MedBrowseComp: Benchmarking Medical Deep Research and Computer Use", "categories": ["cs.CL"], "comment": "You can visit our project page at:\n  https://moreirap12.github.io/mbc-browse-app/", "summary": "Large language models (LLMs) are increasingly envisioned as decision-support\ntools in clinical practice, yet safe clinical reasoning demands integrating\nheterogeneous knowledge bases -- trials, primary studies, regulatory documents,\nand cost data -- under strict accuracy constraints. Existing evaluations often\nrely on synthetic prompts, reduce the task to single-hop factoid queries, or\nconflate reasoning with open-ended generation, leaving their real-world utility\nunclear. To close this gap, we present MedBrowseComp, the first benchmark that\nsystematically tests an agent's ability to reliably retrieve and synthesize\nmulti-hop medical facts from live, domain-specific knowledge bases.\nMedBrowseComp contains more than 1,000 human-curated questions that mirror\nclinical scenarios where practitioners must reconcile fragmented or conflicting\ninformation to reach an up-to-date conclusion. Applying MedBrowseComp to\nfrontier agentic systems reveals performance shortfalls as low as ten percent,\nexposing a critical gap between current LLM capabilities and the rigor demanded\nin clinical settings. MedBrowseComp therefore offers a clear testbed for\nreliable medical information seeking and sets concrete goals for future model\nand toolchain upgrades. You can visit our project page at:\nhttps://moreirap12.github.io/mbc-browse-app/", "AI": {"tldr": "MedBrowseComp\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u68c0\u7d22\u548c\u5408\u6210\u591a\u8df3\u533b\u5b66\u4e8b\u5b9e\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u4e34\u5e8a\u63a8\u7406\u4e2d\u7684\u6027\u80fd\u4e0d\u8db3\u3002", "motivation": "\u4e34\u5e8a\u5b9e\u8df5\u9700\u8981\u6574\u5408\u591a\u6837\u5316\u7684\u77e5\u8bc6\u5e93\u5e76\u786e\u4fdd\u4e25\u683c\u51c6\u786e\u6027\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u9a8c\u8bc1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002", "method": "MedBrowseComp\u5305\u542b1000\u591a\u4e2a\u4eba\u5de5\u7b56\u5212\u7684\u95ee\u9898\uff0c\u6a21\u62df\u4e34\u5e8a\u573a\u666f\uff0c\u6d4b\u8bd5\u6a21\u578b\u4ece\u52a8\u6001\u77e5\u8bc6\u5e93\u4e2d\u68c0\u7d22\u548c\u5408\u6210\u4fe1\u606f\u7684\u80fd\u529b\u3002", "result": "\u524d\u6cbf\u4ee3\u7406\u7cfb\u7edf\u7684\u6027\u80fd\u4f4e\u81f310%\uff0c\u8868\u660e\u73b0\u6709\u6a21\u578b\u4e0e\u4e34\u5e8a\u9700\u6c42\u4e4b\u95f4\u5b58\u5728\u5de8\u5927\u5dee\u8ddd\u3002", "conclusion": "MedBrowseComp\u4e3a\u53ef\u9760\u7684\u533b\u5b66\u4fe1\u606f\u68c0\u7d22\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u6d4b\u8bd5\u57fa\u51c6\uff0c\u5e76\u4e3a\u672a\u6765\u6a21\u578b\u5347\u7ea7\u8bbe\u5b9a\u4e86\u76ee\u6807\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u4e34\u5e8a\u51b3\u7b56,\u591a\u8df3\u63a8\u7406,\u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2505.14825", "pdf": "https://arxiv.org/pdf/2505.14825", "abs": "https://arxiv.org/abs/2505.14825", "authors": ["Marios Andreou", "Nan Chen", "Erik Bollt"], "title": "Assimilative Causal Inference", "categories": ["cs.LG", "math.ST", "physics.data-an", "stat.ME", "stat.ML", "stat.TH", "62F15, 62D20, 62M20, 93E11, 93E14, 60H10"], "comment": "Includes the Main Text and Supporting Information in a single\n  document. 39 pages (p. 1--2 Title, Contents and Abstract | p. 3--14 Main Text\n  | p. 15--39 Supporting Information), 9 figures (3 in the Main Text and 6 in\n  the Supporting Information), typeset in LaTeX. Submitted for peer-review. For\n  more info see https://mariosandreou.short.gy/ACI", "summary": "Causal inference determines cause-and-effect relationships between variables\nand has broad applications across disciplines. Traditional time-series methods\noften reveal causal links only in a time-averaged sense, while ensemble-based\ninformation transfer approaches detect the time evolution of short-term causal\nrelationships but are typically limited to low-dimensional systems. In this\npaper, a new causal inference framework, called assimilative causal inference\n(ACI), is developed. Fundamentally different from the state-of-the-art methods,\nACI uses a dynamical system and a single realization of a subset of the state\nvariables to identify instantaneous causal relationships and the dynamic\nevolution of the associated causal influence range (CIR). Instead of\nquantifying how causes influence effects as done traditionally, ACI solves an\ninverse problem via Bayesian data assimilation, thus tracing causes backward\nfrom observed effects with an implicit Bayesian hypothesis. Causality is\ndetermined by assessing whether incorporating the information of the effect\nvariables reduces the uncertainty in recovering the potential cause variables.\nACI has several desirable features. First, it captures the dynamic interplay of\nvariables, where their roles as causes and effects can shift repeatedly over\ntime. Second, a mathematically justified objective criterion determines the CIR\nwithout empirical thresholds. Third, ACI is scalable to high-dimensional\nproblems by leveraging computationally efficient Bayesian data assimilation\ntechniques. Finally, ACI applies to short time series and incomplete datasets.\nNotably, ACI does not require observations of candidate causes, which is a key\nadvantage since potential drivers are often unknown or unmeasured. The\neffectiveness of ACI is demonstrated by complex dynamical systems showcasing\nintermittency and extreme events.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56e0\u679c\u63a8\u7406\u6846\u67b6ACI\uff0c\u901a\u8fc7\u52a8\u6001\u7cfb\u7edf\u548c\u8d1d\u53f6\u65af\u6570\u636e\u540c\u5316\u65b9\u6cd5\u89e3\u51b3\u4f20\u7edf\u56e0\u679c\u63a8\u7406\u7684\u5c40\u9650\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u7cfb\u7edf\u548c\u77ed\u65f6\u5e8f\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u56e0\u679c\u63a8\u7406\u65b9\u6cd5\u5728\u5904\u7406\u65f6\u95f4\u6f14\u5316\u7684\u77ed\u671f\u56e0\u679c\u5173\u7cfb\u548c\u9ad8\u7ef4\u7cfb\u7edf\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u548c\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "ACI\u57fa\u4e8e\u52a8\u6001\u7cfb\u7edf\u548c\u8d1d\u53f6\u65af\u6570\u636e\u540c\u5316\uff0c\u901a\u8fc7\u9006\u5411\u95ee\u9898\u8bc6\u522b\u77ac\u65f6\u56e0\u679c\u5173\u7cfb\u548c\u52a8\u6001\u56e0\u679c\u5f71\u54cd\u8303\u56f4\uff08CIR\uff09\u3002", "result": "ACI\u80fd\u591f\u52a8\u6001\u6355\u6349\u53d8\u91cf\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u53d8\u5316\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u7cfb\u7edf\u548c\u77ed\u65f6\u5e8f\u6570\u636e\uff0c\u4e14\u65e0\u9700\u89c2\u5bdf\u6240\u6709\u5019\u9009\u539f\u56e0\u3002", "conclusion": "ACI\u4e3a\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u7684\u56e0\u679c\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u56e0\u679c\u63a8\u7406, \u8d1d\u53f6\u65af\u6570\u636e\u540c\u5316, \u52a8\u6001\u7cfb\u7edf, \u9ad8\u7ef4\u95ee\u9898, \u77ed\u65f6\u5e8f\u6570\u636e"}}
{"id": "2505.14707", "pdf": "https://arxiv.org/pdf/2505.14707", "abs": "https://arxiv.org/abs/2505.14707", "authors": ["Georgiana Manolache", "Gerard Schouten", "Joaquin Vanschoren"], "title": "CrypticBio: A Large Multimodal Dataset for Visually Confusing Biodiversity", "categories": ["cs.CV", "cs.AI"], "comment": "We present CrypticBio, the largest publicly available multimodal\n  dataset of visually confusing species, specifically curated to support the\n  development of AI models for biodiversity identification using images,\n  language and spatiotemporal data", "summary": "We present CrypticBio, the largest publicly available multimodal dataset of\nvisually confusing species, specifically curated to support the development of\nAI models in the context of biodiversity applications. Visually confusing or\ncryptic species are groups of two or more taxa that are nearly\nindistinguishable based on visual characteristics alone. While much existing\nwork addresses taxonomic identification in a broad sense, datasets that\ndirectly address the morphological confusion of cryptic species are small,\nmanually curated, and target only a single taxon. Thus, the challenge of\nidentifying such subtle differences in a wide range of taxa remains\nunaddressed. Curated from real-world trends in species misidentification among\ncommunity annotators of iNaturalist, CrypticBio contains 52K unique cryptic\ngroups spanning 67K species, represented in 166 million images. Rich\nresearch-grade image annotations--including scientific, multicultural, and\nmultilingual species terminology, hierarchical taxonomy, spatiotemporal\ncontext, and associated cryptic groups--address multimodal AI in biodiversity\nresearch. For easy dataset curation, we provide an open-source pipeline\nCrypticBio-Curate. The multimodal nature of the dataset beyond vision-language\narises from the integration of geographical and temporal data as complementary\ncues to identifying cryptic species. To highlight the importance of the\ndataset, we benchmark a suite of state-of-the-art foundation models across\nCrypticBio subsets of common, unseen, endangered, and invasive species, and\ndemonstrate the substantial impact of geographical context on vision-language\nzero-shot learning for cryptic species. By introducing CrypticBio, we aim to\ncatalyze progress toward real-world-ready biodiversity AI models capable of\nhandling the nuanced challenges of species ambiguity.", "AI": {"tldr": "CrypticBio\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u89c6\u89c9\u4e0a\u96be\u4ee5\u533a\u5206\u7684\u7269\u79cd\uff0c\u652f\u6301\u751f\u7269\u591a\u6837\u6027\u7814\u7a76\u4e2dAI\u6a21\u578b\u7684\u5f00\u53d1\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u5728\u5904\u7406\u5f62\u6001\u76f8\u4f3c\u7684\u7269\u79cd\u65f6\u89c4\u6a21\u5c0f\u4e14\u5355\u4e00\uff0c\u672a\u80fd\u5168\u9762\u89e3\u51b3\u7269\u79cd\u8bc6\u522b\u7684\u6311\u6218\uff0cCrypticBio\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6570\u636e\u96c6\u6574\u5408\u4e86166\u767e\u4e07\u5f20\u56fe\u50cf\uff0c\u8986\u76d652K\u4e2a\u72ec\u7279\u7ec4\u522b\u548c67K\u79cd\u7269\u79cd\uff0c\u63d0\u4f9b\u4e30\u5bcc\u7684\u7814\u7a76\u7ea7\u6ce8\u91ca\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u5730\u7406\u4f4d\u7f6e\u548c\u65f6\u95f4\uff09\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u5730\u7406\u4e0a\u4e0b\u6587\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u5b66\u4e60\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "CrypticBio\u65e8\u5728\u63a8\u52a8\u751f\u7269\u591a\u6837\u6027AI\u6a21\u578b\u7684\u8fdb\u6b65\uff0c\u89e3\u51b3\u7269\u79cd\u6a21\u7cca\u6027\u95ee\u9898\u3002", "keywords": "CrypticBio, \u591a\u6a21\u6001\u6570\u636e\u96c6, \u89c6\u89c9\u6df7\u6dc6\u7269\u79cd, AI\u6a21\u578b, \u751f\u7269\u591a\u6837\u6027"}}
{"id": "2505.14971", "pdf": "https://arxiv.org/pdf/2505.14971", "abs": "https://arxiv.org/abs/2505.14971", "authors": ["Prashanth Vijayaraghavan", "Soroush Vosoughi", "Lamogha Chizor", "Raya Horesh", "Rogerio Abreu de Paula", "Ehsan Degan", "Vandana Mukherjee"], "title": "DECASTE: Unveiling Caste Stereotypes in Large Language Models through Multi-Dimensional Bias Analysis", "categories": ["cs.CL", "cs.CY"], "comment": "7 (content pages) + 2 (reference pages) + 5 (Appendix pages), 5\n  figures, 6 Tables, IJCAI 2025", "summary": "Recent advancements in large language models (LLMs) have revolutionized\nnatural language processing (NLP) and expanded their applications across\ndiverse domains. However, despite their impressive capabilities, LLMs have been\nshown to reflect and perpetuate harmful societal biases, including those based\non ethnicity, gender, and religion. A critical and underexplored issue is the\nreinforcement of caste-based biases, particularly towards India's marginalized\ncaste groups such as Dalits and Shudras. In this paper, we address this gap by\nproposing DECASTE, a novel, multi-dimensional framework designed to detect and\nassess both implicit and explicit caste biases in LLMs. Our approach evaluates\ncaste fairness across four dimensions: socio-cultural, economic, educational,\nand political, using a range of customized prompting strategies. By\nbenchmarking several state-of-the-art LLMs, we reveal that these models\nsystematically reinforce caste biases, with significant disparities observed in\nthe treatment of oppressed versus dominant caste groups. For example, bias\nscores are notably elevated when comparing Dalits and Shudras with dominant\ncaste groups, reflecting societal prejudices that persist in model outputs.\nThese results expose the subtle yet pervasive caste biases in LLMs and\nemphasize the need for more comprehensive and inclusive bias evaluation\nmethodologies that assess the potential risks of deploying such models in\nreal-world contexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDECASTE\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u79cd\u59d3\u504f\u89c1\uff0c\u63ed\u793a\u6a21\u578b\u5bf9\u5370\u5ea6\u8fb9\u7f18\u79cd\u59d3\u7fa4\u4f53\u7684\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u5e76\u547c\u5401\u66f4\u5168\u9762\u7684\u504f\u89c1\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5e7f\u6cdb\u5e94\u7528\u7684\u540c\u65f6\uff0c\u4ecd\u53cd\u6620\u51fa\u793e\u4f1a\u504f\u89c1\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5370\u5ea6\u8fb9\u7f18\u79cd\u59d3\u7fa4\u4f53\u7684\u504f\u89c1\uff0c\u8fd9\u4e00\u95ee\u9898\u7684\u7814\u7a76\u5c1a\u4e0d\u5145\u5206\u3002", "method": "\u63d0\u51faDECASTE\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u7ef4\u5ea6\uff08\u793e\u4f1a\u6587\u5316\u3001\u7ecf\u6d4e\u3001\u6559\u80b2\u3001\u653f\u6cbb\uff09\u548c\u4e2a\u6027\u5316\u63d0\u793a\u7b56\u7565\uff0c\u68c0\u6d4bLLMs\u4e2d\u7684\u663e\u6027\u548c\u9690\u6027\u79cd\u59d3\u504f\u89c1\u3002", "result": "\u6d4b\u8bd5\u663e\u793a\u4e3b\u6d41LLMs\u7cfb\u7edf\u6027\u5f3a\u5316\u79cd\u59d3\u504f\u89c1\uff0c\u5bf9\u8fb9\u7f18\u79cd\u59d3\uff08\u5982\u8fbe\u5229\u7279\u548c\u9996\u9640\u7f57\uff09\u7684\u504f\u89c1\u8bc4\u5206\u663e\u8457\u9ad8\u4e8e\u4e3b\u5bfc\u79cd\u59d3\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u4e2d\u6f5c\u85cf\u7684\u79cd\u59d3\u504f\u89c1\uff0c\u5f3a\u8c03\u9700\u5f00\u53d1\u66f4\u5168\u9762\u4e14\u5305\u5bb9\u7684\u504f\u89c1\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u964d\u4f4e\u5b9e\u9645\u90e8\u7f72\u7684\u98ce\u9669\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u79cd\u59d3\u504f\u89c1\u3001DECASTE\u6846\u67b6\u3001\u516c\u5e73\u6027\u8bc4\u4f30\u3001\u793e\u4f1a\u504f\u89c1"}}
{"id": "2505.14826", "pdf": "https://arxiv.org/pdf/2505.14826", "abs": "https://arxiv.org/abs/2505.14826", "authors": ["Rohan Deb", "Kiran Thekumparampil", "Kousha Kalantari", "Gaurush Hiranandani", "Shoham Sabach", "Branislav Kveton"], "title": "FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": null, "summary": "Supervised fine-tuning (SFT) is a standard approach to adapting large\nlanguage models (LLMs) to new domains. In this work, we improve the statistical\nefficiency of SFT by selecting an informative subset of training examples.\nSpecifically, for a fixed budget of training examples, which determines the\ncomputational cost of fine-tuning, we determine the most informative ones. The\nkey idea in our method is to select examples that maximize information gain,\nmeasured by the Hessian of the log-likelihood of the LLM. We approximate it\nefficiently by linearizing the LLM at the last layer using multinomial logistic\nregression models. Our approach is computationally efficient, analyzable, and\nperforms well empirically. We demonstrate this on several problems, and back\nour claims with both quantitative results and an LLM evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u9009\u62e9\u8bad\u7ec3\u5b50\u96c6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u4fe1\u606f\u589e\u76ca\u6765\u63d0\u5347\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u7684\u7edf\u8ba1\u6548\u7387\u3002", "motivation": "\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u662f\u9002\u5e94\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5230\u65b0\u9886\u57df\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u4f46\u5982\u4f55\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5927\u7684\u8bad\u7ec3\u5b50\u96c6\u4ee5\u63d0\u9ad8\u6548\u7387\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97LLM\u5bf9\u6570\u4f3c\u7136\u7684Hessian\u77e9\u9635\u6765\u6d4b\u91cf\u4fe1\u606f\u589e\u76ca\uff0c\u9009\u62e9\u6700\u5927\u5316\u4fe1\u606f\u589e\u76ca\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u5e76\u4f7f\u7528\u591a\u9879\u5f0f\u903b\u8f91\u56de\u5f52\u6a21\u578b\u7ebf\u6027\u5316LLM\u7684\u6700\u540e\u4e00\u5c42\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u8be5\u65b9\u6cd5\u8ba1\u7b97\u9ad8\u6548\u4e14\u53ef\u5206\u6790\uff0c\u5728\u591a\u4e2a\u95ee\u9898\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5e76\u901a\u8fc7\u5b9a\u91cf\u7ed3\u679c\u548cLLM\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6709\u9650\u7684\u8bad\u7ec3\u6837\u672c\u9884\u7b97\u4e0b\u663e\u8457\u63d0\u5347\u4e86SFT\u7684\u7edf\u8ba1\u6548\u7387\u3002", "keywords": "\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3001\u4fe1\u606f\u589e\u76ca\u3001Hessian\u77e9\u9635\u3001\u591a\u9879\u5f0f\u903b\u8f91\u56de\u5f52"}}
{"id": "2505.14708", "pdf": "https://arxiv.org/pdf/2505.14708", "abs": "https://arxiv.org/abs/2505.14708", "authors": ["Xuan Shen", "Chenxia Han", "Yufa Zhou", "Yanyue Xie", "Yifan Gong", "Quanyi Wang", "Yiwei Wang", "Yanzhi Wang", "Pu Zhao", "Jiuxiang Gu"], "title": "DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint Version", "summary": "Diffusion transformer-based video generation models (DiTs) have recently\nattracted widespread attention for their excellent generation quality. However,\ntheir computational cost remains a major bottleneck-attention alone accounts\nfor over 80% of total latency, and generating just 8 seconds of 720p video\ntakes tens of minutes-posing serious challenges to practical application and\nscalability. To address this, we propose the DraftAttention, a training-free\nframework for the acceleration of video diffusion transformers with dynamic\nsparse attention on GPUs. We apply down-sampling to each feature map across\nframes in the compressed latent space, enabling a higher-level receptive field\nover the latent composed of hundreds of thousands of tokens. The low-resolution\ndraft attention map, derived from draft query and key, exposes redundancy both\nspatially within each feature map and temporally across frames. We reorder the\nquery, key, and value based on the draft attention map to guide the sparse\nattention computation in full resolution, and subsequently restore their\noriginal order after the attention computation. This reordering enables\nstructured sparsity that aligns with hardware-optimized execution. Our\ntheoretical analysis demonstrates that the low-resolution draft attention\nclosely approximates the full attention, providing reliable guidance for\nconstructing accurate sparse attention. Experimental results show that our\nmethod outperforms existing sparse attention approaches in video generation\nquality and achieves up to 1.75x end-to-end speedup on GPUs. Code:\nhttps://github.com/shawnricecake/draft-attention", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDraftAttention\u7684\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u52a0\u901f\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\uff08DiTs\uff09\u7684\u8ba1\u7b97\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u538b\u7f29\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u964d\u91c7\u6837\u548c\u91cd\u6392\u5e8f\u5b9e\u73b0\u786c\u4ef6\u4f18\u5316\u7684\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u901f\u5ea6\u548c\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff08DiTs\uff09\u56e0\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff08\u5c24\u5176\u662f\u6ce8\u610f\u529b\u673a\u5236\u5360\u6bd4\u8d85\u8fc780%\u5ef6\u8fdf\uff09\uff0c\u4e25\u91cd\u9650\u5236\u5176\u5b9e\u9645\u5e94\u7528\u548c\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faDraftAttention\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u538b\u7f29\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u7279\u5f81\u56fe\u8fdb\u884c\u964d\u91c7\u6837\uff0c\u751f\u6210\u4f4e\u5206\u8fa8\u7387\u8349\u6848\u6ce8\u610f\u529b\u56fe\uff0c\u5e76\u57fa\u4e8e\u6b64\u5bf9\u67e5\u8be2\u3001\u952e\u548c\u503c\u8fdb\u884c\u91cd\u6392\u5e8f\uff0c\u5b9e\u73b0\u786c\u4ef6\u4f18\u5316\u7684\u7a00\u758f\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u751f\u6210\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u5e76\u5728GPU\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe1.75\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\u3002", "conclusion": "DraftAttention\u4e3a\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u52a0\u901f\u65b9\u6848\uff0c\u663e\u8457\u6539\u5584\u4e86\u5b9e\u9645\u5e94\u7528\u53ef\u884c\u6027\u3002", "keywords": "\u89c6\u9891\u751f\u6210\uff1b\u6269\u6563\u53d8\u6362\u5668\uff1b\u7a00\u758f\u6ce8\u610f\u529b\uff1b\u8ba1\u7b97\u52a0\u901f\uff1bGPU\u4f18\u5316"}}
{"id": "2505.14972", "pdf": "https://arxiv.org/pdf/2505.14972", "abs": "https://arxiv.org/abs/2505.14972", "authors": ["Haoyi Qiu", "Kung-Hsiang Huang", "Ruichen Zheng", "Jiao Sun", "Nanyun Peng"], "title": "Multimodal Cultural Safety: Evaluation Frameworks and Alignment Strategies", "categories": ["cs.CL"], "comment": null, "summary": "Large vision-language models (LVLMs) are increasingly deployed in globally\ndistributed applications, such as tourism assistants, yet their ability to\nproduce culturally appropriate responses remains underexplored. Existing\nmultimodal safety benchmarks primarily focus on physical safety and overlook\nviolations rooted in cultural norms, which can result in symbolic harm. To\naddress this gap, we introduce CROSS, a benchmark designed to assess the\ncultural safety reasoning capabilities of LVLMs. CROSS includes 1,284\nmultilingual visually grounded queries from 16 countries, three everyday\ndomains, and 14 languages, where cultural norm violations emerge only when\nimages are interpreted in context. We propose CROSS-Eval, an intercultural\ntheory-based framework that measures four key dimensions: cultural awareness,\nnorm education, compliance, and helpfulness. Using this framework, we evaluate\n21 leading LVLMs, including mixture-of-experts models and reasoning models.\nResults reveal significant cultural safety gaps: the best-performing model\nachieves only 61.79% in awareness and 37.73% in compliance. While some\nopen-source models reach GPT-4o-level performance, they still fall notably\nshort of proprietary models. Our results further show that increasing reasoning\ncapacity improves cultural alignment but does not fully resolve the issue. To\nimprove model performance, we develop two enhancement strategies: supervised\nfine-tuning with culturally grounded, open-ended data and preference tuning\nwith contrastive response pairs that highlight safe versus unsafe behaviors.\nThese methods substantially improve GPT-4o's cultural awareness (+60.14%) and\ncompliance (+55.2%), while preserving general multimodal capabilities with\nminimal performance reduction on general multimodal understanding benchmarks.", "AI": {"tldr": "CROSS\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6587\u5316\u5b89\u5168\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u6587\u5316\u610f\u8bc6\u548c\u89c4\u8303\u9075\u4ece\u6027\u65b9\u9762\u7684\u663e\u8457\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u591a\u6a21\u6001\u5b89\u5168\u6d4b\u8bd5\u4e2d\u5ffd\u89c6\u6587\u5316\u89c4\u8303\uff0c\u53ef\u80fd\u5bfc\u81f4\u7b26\u53f7\u4f24\u5bb3\uff0c\u9700\u8981\u4e13\u95e8\u8bc4\u4f30\u548c\u63d0\u5347\u6a21\u578b\u7684\u6587\u5316\u5b89\u5168\u80fd\u529b\u3002", "method": "\u63d0\u51faCROSS\u57fa\u51c6\u548cCROSS-Eval\u6846\u67b6\uff0c\u8bc4\u4f3021\u79cd\u6a21\u578b\u7684\u6587\u5316\u5b89\u5168\u7ef4\u5ea6\uff0c\u5e76\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u504f\u597d\u8c03\u4f18\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728\u6587\u5316\u610f\u8bc6\u548c\u89c4\u8303\u9075\u4ece\u6027\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0861.79%\u548c37.73%\uff09\uff0c\u4f46\u6539\u8fdb\u65b9\u6cd5\u663e\u8457\u63d0\u5347GPT-4o\u7684\u80fd\u529b\uff08+60.14%\u548c+55.2%\uff09\u3002", "conclusion": "\u589e\u52a0\u63a8\u7406\u80fd\u529b\u6709\u52a9\u4e8e\u6587\u5316\u5bf9\u9f50\uff0c\u4f46\u95ee\u9898\u672a\u5b8c\u5168\u89e3\u51b3\uff1b\u901a\u8fc7\u5fae\u8c03\u548c\u504f\u597d\u8c03\u4f18\u53ef\u6709\u6548\u63d0\u5347\u6a21\u578b\u6587\u5316\u5b89\u5168\u6027\u3002", "keywords": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u6587\u5316\u5b89\u5168\u3001CROSS\u57fa\u51c6\u3001\u591a\u6a21\u6001\u8bc4\u4f30\u3001\u6a21\u578b\u6539\u8fdb"}}
{"id": "2505.14828", "pdf": "https://arxiv.org/pdf/2505.14828", "abs": "https://arxiv.org/abs/2505.14828", "authors": ["Juan Nathaniel", "Carla Roesch", "Jatan Buch", "Derek DeSantis", "Adam Rupe", "Kara Lamb", "Pierre Gentine"], "title": "Deep Koopman operator framework for causal discovery in nonlinear dynamical systems", "categories": ["cs.LG"], "comment": "10+14 pages, 10+13 figures", "summary": "We use a deep Koopman operator-theoretic formalism to develop a novel causal\ndiscovery algorithm, Kausal. Causal discovery aims to identify cause-effect\nmechanisms for better scientific understanding, explainable decision-making,\nand more accurate modeling. Standard statistical frameworks, such as Granger\ncausality, lack the ability to quantify causal relationships in nonlinear\ndynamics due to the presence of complex feedback mechanisms, timescale mixing,\nand nonstationarity. This presents a challenge in studying many real-world\nsystems, such as the Earth's climate. Meanwhile, Koopman operator methods have\nemerged as a promising tool for approximating nonlinear dynamics in a linear\nspace of observables. In Kausal, we propose to leverage this powerful idea for\ncausal analysis where optimal observables are inferred using deep learning.\nCausal estimates are then evaluated in a reproducing kernel Hilbert space, and\ndefined as the distance between the marginal dynamics of the effect and the\njoint dynamics of the cause-effect observables. Our numerical experiments\ndemonstrate Kausal's superior ability in discovering and characterizing causal\nsignals compared to existing approaches of prescribed observables. Lastly, we\nextend our analysis to observations of El Ni\\~no-Southern Oscillation\nhighlighting our algorithm's applicability to real-world phenomena. Our code is\navailable at https://github.com/juannat7/kausal.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6Koopman\u7b97\u5b50\u7684\u65b0\u56e0\u679c\u53d1\u73b0\u7b97\u6cd5Kausal\uff0c\u7528\u4e8e\u89e3\u51b3\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u8bc6\u522b\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u6846\u67b6\uff08\u5982Granger\u56e0\u679c\uff09\u65e0\u6cd5\u91cf\u5316\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u800cKoopman\u7b97\u5b50\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5728\u89c2\u6d4b\u7a7a\u95f4\u4e2d\u8fd1\u4f3c\u975e\u7ebf\u6027\u52a8\u6001\u7684\u65b0\u5de5\u5177\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u63a8\u65ad\u6700\u4f18\u89c2\u6d4b\u53d8\u91cf\uff0c\u5e76\u5728\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u8bc4\u4f30\u56e0\u679c\u4f30\u8ba1\u503c\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u663e\u793aKausal\u5728\u53d1\u73b0\u548c\u8868\u5f81\u56e0\u679c\u4fe1\u53f7\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5384\u5c14\u5c3c\u8bfa-\u5357\u65b9\u632f\u8361\u73b0\u8c61\u3002", "conclusion": "Kausal\u5728\u975e\u7ebf\u6027\u56e0\u679c\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\u7814\u7a76\u3002", "keywords": "Koopman\u7b97\u5b50, \u56e0\u679c\u53d1\u73b0, \u975e\u7ebf\u6027\u52a8\u529b\u5b66, \u6df1\u5ea6\u5b66\u4e60, \u6c14\u5019\u79d1\u5b66"}}
{"id": "2505.14709", "pdf": "https://arxiv.org/pdf/2505.14709", "abs": "https://arxiv.org/abs/2505.14709", "authors": ["Xuan Shen", "Weize Ma", "Yufa Zhou", "Enhao Tang", "Yanyue Xie", "Zhengang Li", "Yifan Gong", "Quanyi Wang", "Henghui Ding", "Yiwei Wang", "Yanzhi Wang", "Pu Zhao", "Jun Lin", "Jiuxiang Gu"], "title": "FastCar: Cache Attentive Replay for Fast Auto-Regressive Video Generation on the Edge", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint Version", "summary": "Auto-regressive (AR) models, initially successful in language generation,\nhave recently shown promise in visual generation tasks due to their superior\nsampling efficiency. Unlike image generation, video generation requires a\nsubstantially larger number of tokens to produce coherent temporal frames,\nresulting in significant overhead during the decoding phase. Our key\nobservations are: (i) MLP modules in the decode phase dominate the inference\nlatency, and (ii) there exists high temporal redundancy in MLP outputs of\nadjacent frames. In this paper, we propose the \\textbf{FastCar} framework to\naccelerate the decode phase for the AR video generation by exploring the\ntemporal redundancy. The Temporal Attention Score (TAS) is proposed to\ndetermine whether to apply the replay strategy (\\textit{i.e.}, reusing cached\nMLP outputs from the previous frame to reduce redundant computations) with\ndetailed theoretical analysis and justification. Also, we develop a hardware\naccelerator on FPGA with Dynamic Resource Scheduling (DRS) based on TAS to\nenable better resource utilization and faster inference. Experimental results\ndemonstrate the effectiveness of our method, which outperforms traditional\nsparse attention approaches with more than 2.1x decoding speedup and higher\nenergy efficiency on the edge. Furthermore, by combining FastCar and sparse\nattention, FastCar can boost the performance of sparse attention with\nalleviated drifting, demonstrating our unique advantages for high-resolution\nand long-duration video generation. Code:\nhttps://github.com/shawnricecake/fast-car", "AI": {"tldr": "FastCar\u6846\u67b6\u901a\u8fc7\u5229\u7528\u65f6\u95f4\u5197\u4f59\u52a0\u901fAR\u89c6\u9891\u751f\u6210\u7684\u89e3\u7801\u9636\u6bb5\uff0c\u63d0\u51faTAS\u548c\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u663e\u8457\u63d0\u5347\u89e3\u7801\u901f\u5ea6\u548c\u80fd\u6548\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u9700\u8981\u5927\u91cftoken\uff0c\u5bfc\u81f4\u89e3\u7801\u9636\u6bb5\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u4e14MLP\u6a21\u5757\u7684\u63a8\u7406\u5ef6\u8fdf\u9ad8\uff0c\u76f8\u90bb\u5e27\u5b58\u5728\u65f6\u95f4\u5197\u4f59\u3002", "method": "\u63d0\u51faFastCar\u6846\u67b6\uff0c\u5229\u7528TAS\u786e\u5b9a\u662f\u5426\u91cd\u7528\u7f13\u5b58\u7684MLP\u8f93\u51fa\u4ee5\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u5e76\u5f00\u53d1FPGA\u786c\u4ef6\u52a0\u901f\u5668\u3002", "result": "FastCar\u5b9e\u73b02.1\u500d\u89e3\u7801\u52a0\u901f\uff0c\u80fd\u6548\u66f4\u9ad8\uff0c\u7ed3\u5408\u7a00\u758f\u6ce8\u610f\u529b\u53ef\u7f13\u89e3\u6f02\u79fb\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u548c\u957f\u89c6\u9891\u751f\u6210\u3002", "conclusion": "FastCar\u901a\u8fc7\u65f6\u95f4\u5197\u4f59\u548c\u786c\u4ef6\u4f18\u5316\u663e\u8457\u63d0\u5347\u89c6\u9891\u751f\u6210\u6548\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u3002", "keywords": "AR\u6a21\u578b\u3001\u89c6\u9891\u751f\u6210\u3001\u65f6\u95f4\u5197\u4f59\u3001TAS\u3001FPGA\u3001\u786c\u4ef6\u52a0\u901f"}}
{"id": "2505.14984", "pdf": "https://arxiv.org/pdf/2505.14984", "abs": "https://arxiv.org/abs/2505.14984", "authors": ["Adarsh Singh", "Kushal Raj Bhandari", "Jianxi Gao", "Soham Dan", "Vivek Gupta"], "title": "CRAFT: Training-Free Cascaded Retrieval for Tabular QA", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Table Question Answering (TQA) involves retrieving relevant tables from a\nlarge corpus to answer natural language queries. Traditional dense retrieval\nmodels, such as DTR and ColBERT, not only incur high computational costs for\nlarge-scale retrieval tasks but also require retraining or fine-tuning on new\ndatasets, limiting their adaptability to evolving domains and knowledge. In\nthis work, we propose $\\textbf{CRAFT}$, a cascaded retrieval approach that\nfirst uses a sparse retrieval model to filter a subset of candidate tables\nbefore applying more computationally expensive dense models and neural\nre-rankers. Our approach achieves better retrieval performance than\nstate-of-the-art (SOTA) sparse, dense, and hybrid retrievers. We further\nenhance table representations by generating table descriptions and titles using\nGemini Flash 1.5. End-to-end TQA results using various Large Language Models\n(LLMs) on NQ-Tables, a subset of the Natural Questions Dataset, demonstrate\n$\\textbf{CRAFT}$ effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCRAFT\u7684\u7ea7\u8054\u68c0\u7d22\u65b9\u6cd5\uff0c\u7528\u4e8e\u8868\u683c\u95ee\u7b54\uff08TQA\uff09\uff0c\u7ed3\u5408\u7a00\u758f\u548c\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u4ee5\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u8868\u683c\u63cf\u8ff0\u8fdb\u4e00\u6b65\u4f18\u5316\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u5728\u5927\u89c4\u6a21\u68c0\u7d22\u4efb\u52a1\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u8868\u683c\u95ee\u7b54\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528\u7ea7\u8054\u68c0\u7d22\u65b9\u6cd5\uff0c\u5148\u7528\u7a00\u758f\u68c0\u7d22\u6a21\u578b\u7b5b\u9009\u5019\u9009\u8868\u683c\uff0c\u518d\u5e94\u7528\u5bc6\u96c6\u6a21\u578b\u548c\u795e\u7ecf\u91cd\u6392\u5668\uff0c\u5e76\u7ed3\u5408\u8868\u683c\u63cf\u8ff0\u751f\u6210\uff08\u4f7f\u7528Gemini Flash 1.5\uff09\u4f18\u5316\u8868\u683c\u8868\u793a\u3002", "result": "CRAFT\u5728\u68c0\u7d22\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u7a00\u758f\u3001\u5bc6\u96c6\u548c\u6df7\u5408\u68c0\u7d22\u65b9\u6cd5\uff0c\u5e76\u5728NQ-Tables\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u4e86\u6709\u6548\u6027\u3002", "conclusion": "CRAFT\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u8868\u683c\u95ee\u7b54\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u3002", "keywords": "\u8868\u683c\u95ee\u7b54\uff08TQA\uff09\uff0c\u7ea7\u8054\u68c0\u7d22\uff0c\u7a00\u758f\u68c0\u7d22\uff0c\u5bc6\u96c6\u68c0\u7d22\uff0c\u81ea\u7136\u8bed\u8a00\u5904\u7406"}}
{"id": "2505.14840", "pdf": "https://arxiv.org/pdf/2505.14840", "abs": "https://arxiv.org/abs/2505.14840", "authors": ["Shreya Gupta", "Boyang Huang", "Barna Saha", "Yinzhan Xu", "Christopher Ye"], "title": "Subquadratic Algorithms and Hardness for Attention with Any Temperature", "categories": ["cs.LG", "cs.CC", "F.2.1"], "comment": "34 pages, 2 figures, abstract shortened to meet arXiv requirements", "summary": "Despite the popularity of the Transformer architecture, the standard\nalgorithm for computing Attention suffers from quadratic time complexity in\ncontext length $n$. Alman and Song [NeurIPS 2023] showed that when the head\ndimension $d = \\Theta(\\log n)$, subquadratic Attention is possible if and only\nif the inputs have small entries bounded by $B = o(\\sqrt{\\log n})$ in absolute\nvalues, under the Strong Exponential Time Hypothesis ($\\mathsf{SETH}$).\nEquivalently, subquadratic Attention is possible if and only if the softmax is\napplied with high temperature for $d=\\Theta(\\log n)$. Running times of these\nalgorithms depend exponentially on $B$ and thus they do not lead to even a\npolynomial-time algorithm outside the specific range of $B$.\n  This naturally leads to the question: when can Attention be computed\nefficiently without strong assumptions on temperature? Are there fast attention\nalgorithms that scale polylogarithmically with entry size $B$? In this work, we\nresolve this question and characterize when fast Attention for arbitrary\ntemperatures is possible. First, for all constant $d = O(1)$, we give the first\nsubquadratic $\\tilde{O}(n^{2 - 1/d} \\cdot \\mathrm{polylog}(B))$ time algorithm\nfor Attention with large $B$. Our result holds even for matrices with large\nhead dimension if they have low rank. In this regime, we also give a similar\nrunning time for Attention gradient computation, and therefore for the full LLM\ntraining process. Furthermore, we show that any substantial improvement on our\nalgorithm is unlikely. In particular, we show that even when $d =\n2^{\\Theta(\\log^* n)}$, Attention requires $n^{2 - o(1)}$ time under\n$\\mathsf{SETH}$.\n  Finally, in the regime where $d = \\mathrm{poly}(n)$, we show that the\nstandard algorithm is optimal under popular fine-grained complexity\nassumptions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u65e0\u9700\u5f3a\u6e29\u5ea6\u5047\u8bbe\u4e0b\u5b9e\u73b0\u5feb\u901f\u6ce8\u610f\u529b\u7684\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6700\u4f18\u6027\u3002", "motivation": "\u63a2\u8ba8\u5728\u65e0\u9700\u5f3a\u6e29\u5ea6\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u9ad8\u6548\u8ba1\u7b97\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u8bc6\u522b\u5feb\u901f\u6ce8\u610f\u529b\u7b97\u6cd5\u7684\u9002\u7528\u6761\u4ef6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a$\tilde{O}(n^{2 - 1/d} \\cdot \text{polylog}(B))$\u7684\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u5927$B$\u503c\u548c\u4f4e\u79e9\u77e9\u9635\u3002\u540c\u65f6\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u5c40\u9650\u6027\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9\u4efb\u610f\u6e29\u5ea6\u7684\u9ad8\u6548\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u6807\u51c6\u7b97\u6cd5\u662f\u6700\u4f18\u7684\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u6ce8\u610f\u529b\u673a\u5236\u9ad8\u6548\u8ba1\u7b97\u7684\u7a7a\u767d\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "keywords": "\u6ce8\u610f\u529b\u673a\u5236, \u8ba1\u7b97\u590d\u6742\u5ea6, \u4f4e\u79e9\u77e9\u9635, \u6e29\u5ea6\u5047\u8bbe, SETH"}}
{"id": "2505.14711", "pdf": "https://arxiv.org/pdf/2505.14711", "abs": "https://arxiv.org/abs/2505.14711", "authors": ["Yohei Ogawa", "Rikuhei Umemoto", "Keisuke Fujii"], "title": "Space evaluation at the starting point of soccer transitions", "categories": ["stat.AP", "cs.AI"], "comment": "23 pages, 8 figures", "summary": "Soccer is a sport played on a pitch where effective use of space is crucial.\nDecision-making during transitions, when possession switches between teams, has\nbeen increasingly important, but research on space evaluation in these moments\nhas been limited. Recent space evaluation methods such as OBSO (Off-Ball\nScoring Opportunity) use scoring probability, so it is not well-suited for\nassessing areas far from the goal, where transitions typically occur. In this\npaper, we propose OBPV (Off-Ball Positioning Value) to evaluate space across\nthe pitch, including the starting points of transitions. OBPV extends OBSO by\nintroducing the field value model, which evaluates the entire pitch, and by\nemploying the transition kernel model, which reflects positional specificity\nthrough kernel density estimation of pass distributions. Experiments using La\nLiga 2023/24 season tracking and event data show that OBPV highlights effective\nspace utilization during counter-attacks and reveals team-specific\ncharacteristics in how the teams utilize space after positive and negative\ntransitions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOBPV\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u8db3\u7403\u6bd4\u8d5b\u4e2d\u5168\u573a\u7684\u7a7a\u95f4\u5229\u7528\u60c5\u51b5\uff0c\u7279\u522b\u662f\u5728\u653b\u9632\u8f6c\u6362\u65f6\u3002", "motivation": "\u73b0\u6709\u7a7a\u95f4\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982OBSO\uff09\u5728\u8bc4\u4f30\u8fdc\u79bb\u7403\u95e8\u7684\u533a\u57df\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u800c\u653b\u9632\u8f6c\u6362\u901a\u5e38\u53d1\u751f\u5728\u8fd9\u4e9b\u533a\u57df\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u65b9\u6cd5\u3002", "method": "OBPV\u6269\u5c55\u4e86OBSO\uff0c\u5f15\u5165\u4e86\u573a\u5730\u4ef7\u503c\u6a21\u578b\u548c\u8fc7\u6e21\u6838\u6a21\u578b\uff0c\u901a\u8fc7\u6838\u5bc6\u5ea6\u4f30\u8ba1\u4f20\u7403\u5206\u5e03\u6765\u8bc4\u4f30\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOBPV\u80fd\u591f\u7a81\u51fa\u53cd\u51fb\u4e2d\u7684\u6709\u6548\u7a7a\u95f4\u5229\u7528\uff0c\u5e76\u63ed\u793a\u7403\u961f\u5728\u653b\u9632\u8f6c\u6362\u540e\u5bf9\u7a7a\u95f4\u5229\u7528\u7684\u7279\u5b9a\u98ce\u683c\u3002", "conclusion": "OBPV\u4e3a\u8db3\u7403\u6bd4\u8d5b\u4e2d\u7684\u7a7a\u95f4\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5168\u9762\u7684\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u653b\u9632\u8f6c\u6362\u7684\u5206\u6790\u3002", "keywords": "\u8db3\u7403, \u7a7a\u95f4\u8bc4\u4f30, OBPV, \u653b\u9632\u8f6c\u6362, \u573a\u5730\u4ef7\u503c\u6a21\u578b"}}
{"id": "2505.14990", "pdf": "https://arxiv.org/pdf/2505.14990", "abs": "https://arxiv.org/abs/2505.14990", "authors": ["Ishika Agarwal", "Nimet Beyza Bozdag", "Dilek Hakkani-T\u00fcr"], "title": "Language Specific Knowledge: Do Models Know Better in X than in English?", "categories": ["cs.CL"], "comment": null, "summary": "Code-switching is a common phenomenon of alternating between different\nlanguages in the same utterance, thought, or conversation. We posit that humans\ncode-switch because they feel more comfortable talking about certain topics and\ndomains in one language than another. With the rise of knowledge-intensive\nlanguage models, we ask ourselves the next, natural question: Could models hold\nmore knowledge on some topics in some language X? More importantly, could we\nimprove reasoning by changing the language that reasoning is performed in? We\ncoin the term Language Specific Knowledge (LSK) to represent this phenomenon.\nAs ethnic cultures tend to develop alongside different languages, we employ\nculture-specific datasets (that contain knowledge about cultural and social\nbehavioral norms). We find that language models can perform better when using\nchain-of-thought reasoning in some languages other than English, sometimes even\nbetter in low-resource languages. Paired with previous works showing that\nsemantic similarity does not equate to representational similarity, we\nhypothesize that culturally specific texts occur more abundantly in\ncorresponding languages, enabling specific knowledge to occur only in specific\n\"expert\" languages. Motivated by our initial results, we design a simple\nmethodology called LSKExtractor to benchmark the language-specific knowledge\npresent in a language model and, then, exploit it during inference. We show our\nresults on various models and datasets, showing an average relative improvement\nof 10% in accuracy. Our research contributes to the open-source development of\nlanguage models that are inclusive and more aligned with the cultural and\nlinguistic contexts in which they are deployed.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8bed\u8a00\u7279\u5b9a\u77e5\u8bc6\uff08LSK\uff09\u73b0\u8c61\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u67d0\u4e9b\u8bed\u8a00\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u63d0\u51faLSKExtractor\u65b9\u6cd5\u6765\u5229\u7528\u8fd9\u4e00\u73b0\u8c61\u3002\u5b9e\u9a8c\u663e\u793a\u51c6\u786e\u6027\u5e73\u5747\u63d0\u534710%\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u4ee3\u7801\u8f6c\u6362\u73b0\u8c61\uff0c\u63a2\u7d22\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u53ef\u80fd\u7684\u77e5\u8bc6\u5dee\u5f02\u53ca\u63a8\u7406\u80fd\u529b\u63d0\u5347\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51faLSKExtractor\u65b9\u6cd5\uff0c\u5229\u7528\u6587\u5316\u7279\u5b9a\u6570\u636e\u96c6\u8bc4\u4f30\u548c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684LSK\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5728\u67d0\u4e9b\u8bed\u8a00\u4e2d\u63a8\u7406\u6027\u80fd\u66f4\u4f18\uff0c\u5e73\u5747\u51c6\u786e\u6027\u63d0\u534710%\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u8bed\u8a00\u4e2d\u8868\u73b0\u66f4\u4f73\uff0c\u63a8\u52a8\u4e86\u66f4\u5177\u5305\u5bb9\u6027\u7684\u6a21\u578b\u5f00\u53d1\u3002", "keywords": "\u4ee3\u7801\u8f6c\u6362,\u8bed\u8a00\u7279\u5b9a\u77e5\u8bc6,\u63a8\u7406\u80fd\u529b,\u6587\u5316\u7279\u5b9a\u6570\u636e\u96c6"}}
{"id": "2505.14877", "pdf": "https://arxiv.org/pdf/2505.14877", "abs": "https://arxiv.org/abs/2505.14877", "authors": ["Francisco P\u00e9rez-Galarce", "Jorge Mart\u00ednez-Palomera", "Karim Pichara", "Pablo Huijse", "M\u00e1rcio Catelan"], "title": "A self-regulated convolutional neural network for classifying variable stars", "categories": ["cs.LG", "astro-ph.SR"], "comment": null, "summary": "Over the last two decades, machine learning models have been widely applied\nand have proven effective in classifying variable stars, particularly with the\nadoption of deep learning architectures such as convolutional neural networks,\nrecurrent neural networks, and transformer models. While these models have\nachieved high accuracy, they require high-quality, representative data and a\nlarge number of labelled samples for each star type to generalise well, which\ncan be challenging in time-domain surveys. This challenge often leads to models\nlearning and reinforcing biases inherent in the training data, an issue that is\nnot easily detectable when validation is performed on subsamples from the same\ncatalogue. The problem of biases in variable star data has been largely\noverlooked, and a definitive solution has yet to be established. In this paper,\nwe propose a new approach to improve the reliability of classifiers in variable\nstar classification by introducing a self-regulated training process. This\nprocess utilises synthetic samples generated by a physics-enhanced latent space\nvariational autoencoder, incorporating six physical parameters from Gaia Data\nRelease 3. Our method features a dynamic interaction between a classifier and a\ngenerative model, where the generative model produces ad-hoc synthetic light\ncurves to reduce confusion during classifier training and populate\nunderrepresented regions in the physical parameter space. Experiments conducted\nunder various scenarios demonstrate that our self-regulated training approach\noutperforms traditional training methods for classifying variable stars on\nbiased datasets, showing statistically significant improvements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6211\u8c03\u8282\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7269\u7406\u589e\u5f3a\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u51cf\u5c11\u5206\u7c7b\u5668\u5728\u504f\u659c\u6570\u636e\u96c6\u4e0a\u7684\u504f\u5dee\uff0c\u663e\u8457\u63d0\u5347\u53d8\u661f\u5206\u7c7b\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u53d8\u661f\u5206\u7c7b\u4e2d\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f9d\u8d56\u9ad8\u8d28\u91cf\u548c\u4ee3\u8868\u6027\u6570\u636e\uff0c\u5bb9\u6613\u5b66\u4e60\u5e76\u5f3a\u5316\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u504f\u5dee\u3002\u76ee\u524d\u5bf9\u6570\u636e\u504f\u5dee\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u5c1a\u672a\u5b8c\u5584\u3002", "method": "\u5f15\u5165\u81ea\u6211\u8c03\u8282\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u7ed3\u5408\u7269\u7406\u589e\u5f3a\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\u751f\u6210\u5408\u6210\u6837\u672c\uff0c\u52a8\u6001\u8c03\u6574\u5206\u7c7b\u5668\u548c\u751f\u6210\u6a21\u578b\u7684\u4ea4\u4e92\uff0c\u4ee5\u51cf\u5c11\u8bad\u7ec3\u4e2d\u7684\u6df7\u6dc6\u548c\u586b\u8865\u53c2\u6570\u7a7a\u95f4\u6b20\u8868\u793a\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u504f\u659c\u6570\u636e\u96c6\u4e0a\u7684\u5206\u7c7b\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5177\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\u4e0a\u7684\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u548c\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u6570\u636e\u504f\u5dee\u5bf9\u5206\u7c7b\u5668\u7684\u5f71\u54cd\uff0c\u63d0\u5347\u5206\u7c7b\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "keywords": "\u673a\u5668\u5b66\u4e60\uff0c\u53d8\u661f\u5206\u7c7b\uff0c\u6570\u636e\u504f\u5dee\uff0c\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u5408\u6210\u6570\u636e"}}
{"id": "2505.14714", "pdf": "https://arxiv.org/pdf/2505.14714", "abs": "https://arxiv.org/abs/2505.14714", "authors": ["Tuan-Vinh La", "Minh-Hieu Nguyen", "Minh-Son Dao"], "title": "KGAlign: Joint Semantic-Structural Knowledge Encoding for Multimodal Fake News Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Fake news detection remains a challenging problem due to the complex\ninterplay between textual misinformation, manipulated images, and external\nknowledge reasoning. While existing approaches have achieved notable results in\nverifying veracity and cross-modal consistency, two key challenges persist: (1)\nExisting methods often consider only the global image context while neglecting\nlocal object-level details, and (2) they fail to incorporate external knowledge\nand entity relationships for deeper semantic understanding. To address these\nchallenges, we propose a novel multi-modal fake news detection framework that\nintegrates visual, textual, and knowledge-based representations. Our approach\nleverages bottom-up attention to capture fine-grained object details, CLIP for\nglobal image semantics, and RoBERTa for context-aware text encoding. We further\nenhance knowledge utilization by retrieving and adaptively selecting relevant\nentities from a knowledge graph. The fused multi-modal features are processed\nthrough a Transformer-based classifier to predict news veracity. Experimental\nresults demonstrate that our model outperforms recent approaches, showcasing\nthe effectiveness of neighbor selection mechanism and multi-modal fusion for\nfake news detection. Our proposal introduces a new paradigm: knowledge-grounded\nmultimodal reasoning. By integrating explicit entity-level selection and\nNLI-guided filtering, we shift fake news detection from feature fusion to\nsemantically grounded verification. For reproducibility and further research,\nthe source code is publicly at\n\\href{https://github.com/latuanvinh1998/KGAlign}{github.com/latuanvinh1998/KGAlign}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5047\u65b0\u95fb\u68c0\u6d4b\u6846\u67b6\uff0c\u878d\u5408\u89c6\u89c9\u3001\u6587\u672c\u548c\u77e5\u8bc6\u8868\u793a\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5bf9\u8c61\u7ec6\u8282\u548c\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u8bed\u4e49\u7406\u89e3\uff0c\u5b9e\u9a8c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5047\u65b0\u95fb\u68c0\u6d4b\u9762\u4e34\u6587\u672c\u8bef\u5bfc\u3001\u56fe\u50cf\u64cd\u7eb5\u548c\u5916\u90e8\u77e5\u8bc6\u63a8\u7406\u7684\u590d\u6742\u4ea4\u4e92\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u5c40\u90e8\u5bf9\u8c61\u7ec6\u8282\u548c\u5916\u90e8\u77e5\u8bc6\u5229\u7528\u3002", "method": "\u7ed3\u5408\u81ea\u5e95\u5411\u4e0a\u6ce8\u610f\u529b\u6355\u6349\u5bf9\u8c61\u7ec6\u8282\u3001CLIP\u63d0\u53d6\u5168\u5c40\u56fe\u50cf\u8bed\u4e49\u3001RoBERTa\u7f16\u7801\u6587\u672c\uff0c\u5e76\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u548c\u9009\u62e9\u76f8\u5173\u5b9e\u4f53\uff0c\u901a\u8fc7Transformer\u5206\u7c7b\u5668\u9884\u6d4b\u65b0\u95fb\u771f\u5b9e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u90bb\u5c45\u9009\u62e9\u673a\u5236\u548c\u591a\u6a21\u6001\u878d\u5408\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u5047\u65b0\u95fb\u68c0\u6d4b\u4ece\u7279\u5f81\u878d\u5408\u8f6c\u5411\u8bed\u4e49\u9a8c\u8bc1\uff0c\u63d0\u51fa\u4e86\u77e5\u8bc6\u9a71\u52a8\u7684\u591a\u6a21\u6001\u63a8\u7406\u65b0\u8303\u5f0f\u3002", "keywords": "\u5047\u65b0\u95fb\u68c0\u6d4b\u3001\u591a\u6a21\u6001\u878d\u5408\u3001\u77e5\u8bc6\u56fe\u8c31\u3001Transformer\u3001\u8bed\u4e49\u9a8c\u8bc1"}}
{"id": "2505.14992", "pdf": "https://arxiv.org/pdf/2505.14992", "abs": "https://arxiv.org/abs/2505.14992", "authors": ["Zhihao Wen", "Sheng Liang", "Yaxiong Wu", "Yongyue Zhang", "Yong Liu"], "title": "Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models", "categories": ["cs.CL", "I.2.7"], "comment": "5 pages, 2 figures", "summary": "Information extraction (IE) plays a crucial role in natural language\nprocessing (NLP) by converting unstructured text into structured knowledge.\nDeploying computationally intensive large language models (LLMs) on\nresource-constrained devices for information extraction is challenging,\nparticularly due to issues like hallucinations, limited context length, and\nhigh latency-especially when handling diverse extraction schemas. To address\nthese challenges, we propose a two-stage information extraction approach\nadapted for on-device LLMs, called Dual-LoRA with Incremental Schema Caching\n(DLISC), which enhances both schema identification and schema-aware extraction\nin terms of effectiveness and efficiency. In particular, DLISC adopts an\nIdentification LoRA module for retrieving the most relevant schemas to a given\nquery, and an Extraction LoRA module for performing information extraction\nbased on the previously selected schemas. To accelerate extraction inference,\nIncremental Schema Caching is incorporated to reduce redundant computation,\nsubstantially improving efficiency. Extensive experiments across multiple\ninformation extraction datasets demonstrate notable improvements in both\neffectiveness and efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDLISC\u7684\u4e24\u9636\u6bb5\u4fe1\u606f\u63d0\u53d6\u65b9\u6cd5\uff0c\u9488\u5bf9\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684LLMs\uff0c\u901a\u8fc7Dual-LoRA\u548c\u589e\u91cf\u6a21\u5f0f\u7f13\u5b58\u63d0\u5347\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72LLMs\u8fdb\u884c\u4fe1\u606f\u63d0\u53d6\u65f6\u7684\u5e7b\u89c9\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u548c\u9ad8\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u91c7\u7528Dual-LoRA\uff08Identification LoRA\u548cExtraction LoRA\uff09\u548c\u589e\u91cf\u6a21\u5f0f\u7f13\u5b58\uff08Incremental Schema Caching\uff09\u3002", "result": "\u5728\u591a\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\uff0cDLISC\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u5747\u8868\u73b0\u51fa\u663e\u8457\u63d0\u5347\u3002", "conclusion": "DLISC\u662f\u4e00\u79cd\u9488\u5bf9\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u9ad8\u6548\u4fe1\u606f\u63d0\u53d6\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6311\u6218\u3002", "keywords": "\u4fe1\u606f\u63d0\u53d6,LLMs,Dual-LoRA,\u589e\u91cf\u6a21\u5f0f\u7f13\u5b58"}}
{"id": "2505.14882", "pdf": "https://arxiv.org/pdf/2505.14882", "abs": "https://arxiv.org/abs/2505.14882", "authors": ["Abdellah Aznag", "Rachel Cummings", "Adam N. Elmachtoub"], "title": "An active learning framework for multi-group mean estimation", "categories": ["cs.LG"], "comment": null, "summary": "We study a fundamental learning problem over multiple groups with unknown\ndata distributions, where an analyst would like to learn the mean of each\ngroup. Moreover, we want to ensure that this data is collected in a relatively\nfair manner such that the noise of the estimate of each group is reasonable. In\nparticular, we focus on settings where data are collected dynamically, which is\nimportant in adaptive experimentation for online platforms or adaptive clinical\ntrials for healthcare. In our model, we employ an active learning framework to\nsequentially collect samples with bandit feedback, observing a sample in each\nperiod from the chosen group. After observing a sample, the analyst updates\ntheir estimate of the mean and variance of that group and chooses the next\ngroup accordingly. The analyst's objective is to dynamically collect samples to\nminimize the collective noise of the estimators, measured by the norm of the\nvector of variances of the mean estimators.\n  We propose an algorithm, Variance-UCB, that sequentially selects groups\naccording to an upper confidence bound on the variance estimate. We provide a\ngeneral theoretical framework for providing efficient bounds on learning from\nany underlying distribution where the variances can be estimated reasonably.\nThis framework yields upper bounds on regret that improve significantly upon\nall existing bounds, as well as a collection of new results for different\nobjectives and distributions than those previously studied.", "AI": {"tldr": "\u7814\u7a76\u4e86\u591a\u7ec4\u672a\u77e5\u6570\u636e\u5206\u5e03\u4e0b\u7684\u5747\u503c\u4f30\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u6570\u636e\u6536\u96c6\u548c\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u51faVariance-UCB\u7b97\u6cd5\uff0c\u4ee5\u6700\u5c0f\u5316\u4f30\u8ba1\u5668\u7684\u96c6\u4f53\u566a\u58f0\u3002", "motivation": "\u89e3\u51b3\u591a\u7ec4\u672a\u77e5\u6570\u636e\u5206\u5e03\u4e0b\u5747\u503c\u4f30\u8ba1\u7684\u516c\u5e73\u6027\u548c\u6548\u7387\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u6570\u636e\u6536\u96c6\u573a\u666f\u4e2d\uff0c\u5982\u5728\u7ebf\u5e73\u53f0\u6216\u533b\u7597\u9886\u57df\u7684\u9002\u5e94\u6027\u5b9e\u9a8c\u3002", "method": "\u91c7\u7528\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7Variance-UCB\u7b97\u6cd5\u52a8\u6001\u9009\u62e9\u7ec4\u522b\uff0c\u57fa\u4e8e\u65b9\u5dee\u4f30\u8ba1\u7684\u7f6e\u4fe1\u4e0a\u9650\u66f4\u65b0\u7b56\u7565\uff0c\u6700\u5c0f\u5316\u96c6\u4f53\u566a\u58f0\u3002", "result": "\u63d0\u51fa\u7684\u7406\u8bba\u6846\u67b6\u663e\u8457\u6539\u8fdb\u4e86\u73b0\u6709\u7b97\u6cd5\u5728\u9057\u61be\u8fb9\u754c\u4e0a\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u5206\u5e03\u548c\u76ee\u6807\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Variance-UCB\u7b97\u6cd5\u5728\u591a\u7ec4\u52a8\u6001\u6570\u636e\u6536\u96c6\u573a\u666f\u4e2d\u6709\u6548\u4e14\u9ad8\u6548\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5747\u7b49\u548c\u516c\u5e73\u6027\u76ee\u6807\u4e0b\u7684\u6027\u80fd\u3002", "keywords": "\u5747\u503c\u4f30\u8ba1\u3001\u591a\u7ec4\u6570\u636e\u3001\u516c\u5e73\u6027\u3001\u52a8\u6001\u6570\u636e\u6536\u96c6\u3001Variance-UCB\u7b97\u6cd5"}}
{"id": "2505.14717", "pdf": "https://arxiv.org/pdf/2505.14717", "abs": "https://arxiv.org/abs/2505.14717", "authors": ["Xigui Li", "Yuanye Zhou", "Feiyang Xiao", "Xin Guo", "Chen Jiang", "Tan Pan", "Xingmeng Zhang", "Cenyu Liu", "Zeyun Miao", "Jianchao Ge", "Xiansheng Wang", "Qimeng Wang", "Yichi Zhang", "Wenbo Zhang", "Fengping Zhu", "Limei Han", "Yuan Qi", "Chensen Lin", "Yuan Cheng"], "title": "Aneumo: A Large-Scale Multimodal Aneurysm Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Intracranial aneurysms (IAs) are serious cerebrovascular lesions found in\napproximately 5\\% of the general population. Their rupture may lead to high\nmortality. Current methods for assessing IA risk focus on morphological and\npatient-specific factors, but the hemodynamic influences on IA development and\nrupture remain unclear. While accurate for hemodynamic studies, conventional\ncomputational fluid dynamics (CFD) methods are computationally intensive,\nhindering their deployment in large-scale or real-time clinical applications.\nTo address this challenge, we curated a large-scale, high-fidelity aneurysm CFD\ndataset to facilitate the development of efficient machine learning algorithms\nfor such applications. Based on 427 real aneurysm geometries, we synthesized\n10,660 3D shapes via controlled deformation to simulate aneurysm evolution. The\nauthenticity of these synthetic shapes was confirmed by neurosurgeons. CFD\ncomputations were performed on each shape under eight steady-state mass flow\nconditions, generating a total of 85,280 blood flow dynamics data covering key\nparameters. Furthermore, the dataset includes segmentation masks, which can\nsupport tasks that use images, point clouds or other multimodal data as input.\nAdditionally, we introduced a benchmark for estimating flow parameters to\nassess current modeling methods. This dataset aims to advance aneurysm research\nand promote data-driven approaches in biofluids, biomedical engineering, and\nclinical risk assessment. The code and dataset are available at:\nhttps://github.com/Xigui-Li/Aneumo.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u9ad8\u4fdd\u771f\u7684\u9885\u5185\u52a8\u8109\u7624\uff08IA\uff09CFD\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5f00\u53d1\u9ad8\u6548\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u4ee5\u7814\u7a76\u8840\u6d41\u52a8\u529b\u5b66\u5bf9IA\u53d1\u5c55\u548c\u7834\u88c2\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u7684CFD\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u6216\u5b9e\u65f6\u7684\u4e34\u5e8a\u8bc4\u4f30\uff0c\u800c\u5f53\u524dIA\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u5ffd\u89c6\u4e86\u8840\u6d41\u52a8\u529b\u5b66\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5bf9427\u4e2a\u771f\u5b9e\u52a8\u8109\u7624\u51e0\u4f55\u5f62\u72b6\u8fdb\u884c\u53d7\u63a7\u53d8\u5f62\uff0c\u5408\u6210\u4e8610,660\u4e2a3D\u5f62\u72b6\u6a21\u62dfIA\u6f14\u53d8\uff0c\u5e76\u57288\u79cd\u7a33\u6001\u8d28\u91cf\u6d41\u91cf\u6761\u4ef6\u4e0b\u751f\u621085,280\u4e2a\u8840\u6d41\u52a8\u529b\u5b66\u6570\u636e\u3002", "result": "\u6570\u636e\u96c6\u5305\u62ec\u8840\u6d41\u52a8\u529b\u5b66\u53c2\u6570\u3001\u5206\u5272\u63a9\u6a21\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53ef\u7528\u4e8e\u591a\u6a21\u6001\u6570\u636e\u8f93\u5165\u7684\u4efb\u52a1\u548c\u5f53\u524d\u5efa\u6a21\u65b9\u6cd5\u7684\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u65e8\u5728\u63a8\u52a8IA\u7814\u7a76\uff0c\u5e76\u4fc3\u8fdb\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u751f\u7269\u6d41\u4f53\u3001\u751f\u7269\u533b\u5b66\u5de5\u7a0b\u548c\u4e34\u5e8a\u98ce\u9669\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u3002", "keywords": "\u9885\u5185\u52a8\u8109\u7624,\u8840\u6d41\u52a8\u529b\u5b66,\u673a\u5668\u5b66\u4e60,CFD\u6570\u636e\u96c6"}}
{"id": "2505.14996", "pdf": "https://arxiv.org/pdf/2505.14996", "abs": "https://arxiv.org/abs/2505.14996", "authors": ["Zixuan Ke", "Austin Xu", "Yifei Ming", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "title": "Meta-Design Matters: A Self-Design Multi-Agent System", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Multi-agent systems (MAS) leveraging the impressive capabilities of Large\nLanguage Models (LLMs) hold significant potential for tackling complex tasks.\nHowever, most current MAS depend on manually designed agent roles and\ncommunication protocols. These manual designs often fail to align with the\nunderlying LLMs' strengths and struggle to adapt to novel tasks. Recent\nautomatic MAS approaches attempt to mitigate these limitations but typically\nnecessitate a validation-set for tuning and yield static MAS designs lacking\nadaptability during inference. We introduce SELF-MAS, the first\nself-supervised, inference-time only framework for automatic MAS design.\nSELF-MAS employs meta-level design to iteratively generate, evaluate, and\nrefine MAS configurations tailored to each problem instance, without requiring\na validation set. Critically, it enables dynamic agent composition and problem\ndecomposition through meta-feedback on solvability and completeness.\nExperiments across math, graduate-level QA, and software engineering\nbenchmarks, using both closed-source and open-source LLM back-bones of varying\nsizes, demonstrate that SELF-MAS outperforms both manual and automatic MAS\nbaselines, achieving a 7.44% average accuracy improvement over the next\nstrongest baseline while maintaining cost-efficiency. These findings underscore\nthe promise of meta-level self-supervised design for creating effective and\nadaptive MAS.", "AI": {"tldr": "SELF-MAS\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u3001\u4ec5\u5728\u63a8\u7406\u65f6\u8fd0\u884c\u7684\u81ea\u52a8\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\u8bbe\u8ba1\u6846\u67b6\uff0c\u65e0\u9700\u9a8c\u8bc1\u96c6\uff0c\u52a8\u6001\u8c03\u6574\u4ee3\u7406\u914d\u7f6e\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u591a\u4ee3\u7406\u7cfb\u7edf\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u89d2\u8272\u548c\u534f\u8bae\uff0c\u96be\u4ee5\u9002\u5e94\u65b0\u4efb\u52a1\uff1b\u73b0\u6709\u81ea\u52a8\u65b9\u6cd5\u9700\u8981\u9a8c\u8bc1\u96c6\u4e14\u7f3a\u4e4f\u52a8\u6001\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faSELF-MAS\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u7ea7\u8bbe\u8ba1\u8fed\u4ee3\u751f\u6210\u3001\u8bc4\u4f30\u548c\u4f18\u5316MAS\u914d\u7f6e\uff0c\u652f\u6301\u52a8\u6001\u4ee3\u7406\u7ec4\u5408\u548c\u95ee\u9898\u5206\u89e3\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u4e0d\u540c\u89c4\u6a21\u7684LLM\u4e0a\uff0cSELF-MAS\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53477.44%\uff0c\u6210\u672c\u6548\u76ca\u9ad8\u3002", "conclusion": "\u5143\u7ea7\u81ea\u76d1\u7763\u8bbe\u8ba1\u4e3a\u6784\u5efa\u9ad8\u6548\u3001\u81ea\u9002\u5e94MAS\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u591a\u4ee3\u7406\u7cfb\u7edf,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u81ea\u76d1\u7763,\u52a8\u6001\u9002\u5e94\u6027,\u5143\u7ea7\u8bbe\u8ba1"}}
{"id": "2505.14884", "pdf": "https://arxiv.org/pdf/2505.14884", "abs": "https://arxiv.org/abs/2505.14884", "authors": ["Susav Shrestha", "Brad Settlemyer", "Nikoli Dryden", "Narasimha Reddy"], "title": "Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accelerating large language model (LLM) inference is critical for real-world\ndeployments requiring high throughput and low latency. Contextual sparsity,\nwhere each token dynamically activates only a small subset of the model\nparameters, shows promise but does not scale to large batch sizes due to union\nof active neurons quickly approaching dense computation. We introduce Polar\nSparsity, highlighting a key shift in sparsity importance from MLP to Attention\nlayers as we scale batch size and sequence length. While MLP layers become more\ncompute-efficient under batching, their sparsity vanishes. In contrast,\nattention becomes increasingly more expensive at scale, while their head\nsparsity remains stable and batch-invariant. We develop hardware-efficient,\nsparsity-aware GPU kernels for selective MLP and Attention computations,\ndelivering up to \\(2.2\\times\\) end-to-end speedups for models like OPT, LLaMA-2\n\\& 3, across various batch sizes and sequence lengths without compromising\naccuracy. To our knowledge, this is the first work to demonstrate that\ncontextual sparsity can scale effectively to large batch sizes, delivering\nsubstantial inference acceleration with minimal changes, making Polar Sparsity\npractical for large-scale, high-throughput LLM deployment systems. Our code is\navailable at: https://github.com/susavlsh10/Polar-Sparsity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPolar Sparsity\u65b9\u6cd5\uff0c\u901a\u8fc7\u5173\u6ce8Attention\u5c42\u7684\u7a00\u758f\u6027\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u7684\u52a0\u901f\uff0c\u652f\u6301\u5927\u6279\u91cf\u5904\u7406\u4e14\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3LLM\u63a8\u7406\u4e2d\u7684\u9ad8\u541e\u5410\u548c\u4f4e\u5ef6\u8fdf\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u5927\u6279\u6b21\u5904\u7406\u65f6\u4e0a\u4e0b\u6587\u7a00\u758f\u6027\u65e0\u6cd5\u6709\u6548\u6269\u5c55\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faPolar Sparsity\u65b9\u6cd5\uff0c\u91cd\u70b9\u5229\u7528Attention\u5c42\u7684\u7a00\u758f\u6027\uff0c\u5e76\u5f00\u53d1\u786c\u4ef6\u9ad8\u6548\u7684GPU\u5185\u6838\u6765\u5904\u7406\u9009\u62e9\u6027MLP\u548cAttention\u8ba1\u7b97\u3002", "result": "\u5728OPT\u3001LLaMA-2\u548c3\u7b49\u6a21\u578b\u4e0a\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad82.2\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u6279\u6b21\u5927\u5c0f\u548c\u5e8f\u5217\u957f\u5ea6\uff0c\u4e14\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "Polar Sparsity\u9996\u6b21\u8bc1\u660e\u4e0a\u4e0b\u6587\u7a00\u758f\u6027\u53ef\u6709\u6548\u6269\u5c55\u5230\u5927\u6279\u6b21\u5904\u7406\uff0c\u4e3a\u5927\u541e\u5410LLM\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "LLM, \u63a8\u7406\u52a0\u901f, \u4e0a\u4e0b\u6587\u7a00\u758f\u6027, Polar Sparsity, Attention\u5c42"}}
{"id": "2505.14718", "pdf": "https://arxiv.org/pdf/2505.14718", "abs": "https://arxiv.org/abs/2505.14718", "authors": ["Guoxuan Mao", "Ting Cao", "Ziyang Li", "Yuan Dong"], "title": "Enhancing Shape Perception and Segmentation Consistency for Industrial Image Inspection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semantic segmentation stands as a pivotal research focus in computer vision.\nIn the context of industrial image inspection, conventional semantic\nsegmentation models fail to maintain the segmentation consistency of fixed\ncomponents across varying contextual environments due to a lack of perception\nof object contours. Given the real-time constraints and limited computing\ncapability of industrial image detection machines, it is also necessary to\ncreate efficient models to reduce computational complexity. In this work, a\nShape-Aware Efficient Network (SPENet) is proposed, which focuses on the shapes\nof objects to achieve excellent segmentation consistency by separately\nsupervising the extraction of boundary and body information from images. In\nSPENet, a novel method is introduced for describing fuzzy boundaries to better\nadapt to real-world scenarios named Variable Boundary Domain (VBD).\nAdditionally, a new metric, Consistency Mean Square Error(CMSE), is proposed to\nmeasure segmentation consistency for fixed components. Our approach attains the\nbest segmentation accuracy and competitive speed on our dataset, showcasing\nsignificant advantages in CMSE among numerous state-of-the-art real-time\nsegmentation networks, achieving a reduction of over 50% compared to the\npreviously top-performing models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPENet\u7684\u5f62\u72b6\u611f\u77e5\u9ad8\u6548\u7f51\u7edc\uff0c\u7528\u4e8e\u89e3\u51b3\u5de5\u4e1a\u56fe\u50cf\u68c0\u6d4b\u4e2d\u4f20\u7edf\u8bed\u4e49\u5206\u5272\u6a21\u578b\u5728\u591a\u53d8\u73af\u5883\u4e0b\u5bf9\u56fa\u5b9a\u7ec4\u4ef6\u5206\u5272\u4e00\u81f4\u7684\u4e0d\u8db3\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u5de5\u4e1a\u56fe\u50cf\u68c0\u6d4b\u4e2d\uff0c\u4f20\u7edf\u8bed\u4e49\u5206\u5272\u6a21\u578b\u56e0\u7f3a\u4e4f\u5bf9\u7269\u4f53\u8f6e\u5ed3\u7684\u611f\u77e5\uff0c\u65e0\u6cd5\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u4fdd\u6301\u56fa\u5b9a\u7ec4\u4ef6\u7684\u5206\u5272\u4e00\u81f4\u6027\uff0c\u4e14\u9700\u6ee1\u8db3\u5b9e\u65f6\u6027\u548c\u6709\u9650\u8ba1\u7b97\u80fd\u529b\u7684\u9700\u6c42\u3002", "method": "SPENet\u901a\u8fc7\u5206\u522b\u76d1\u7763\u56fe\u50cf\u8fb9\u754c\u548c\u4e3b\u4f53\u4fe1\u606f\u7684\u63d0\u53d6\uff0c\u5f15\u5165\u6a21\u7cca\u8fb9\u754c\u63cf\u8ff0\u7684Variable Boundary Domain (VBD)\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u5ea6\u91cfConsistency Mean Square Error (CMSE)\u3002", "result": "SPENet\u5728\u5206\u5272\u51c6\u786e\u6027\u548cCMSE\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4\u5148\u524d\u6700\u4f73\u6a21\u578b\u7684CMSE\u964d\u4f4e\u8d8550%\uff0c\u5e76\u5728\u6570\u636e\u96c6\u4e2d\u8fbe\u5230\u6700\u4f73\u5206\u5272\u7cbe\u5ea6\u548c\u7ade\u4e89\u6027\u901f\u5ea6\u3002", "conclusion": "SPENet\u5728\u5de5\u4e1a\u56fe\u50cf\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u5206\u5272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5b9e\u65f6\u5206\u5272\u7f51\u7edc\u3002", "keywords": "\u8bed\u4e49\u5206\u5272, \u5de5\u4e1a\u56fe\u50cf\u68c0\u6d4b, SPENet, \u5206\u5272\u4e00\u81f4\u6027, \u8ba1\u7b97\u6548\u7387"}}
{"id": "2505.15000", "pdf": "https://arxiv.org/pdf/2505.15000", "abs": "https://arxiv.org/abs/2505.15000", "authors": ["Chengwei Wei", "Bin Wang", "Jung-jae Kim", "Nancy F. Chen"], "title": "Towards Spoken Mathematical Reasoning: Benchmarking Speech-based Models over Multi-faceted Math Problems", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) and multimodal LLMs (MLLMs)\nhave led to strong reasoning ability across a wide range of tasks. However,\ntheir ability to perform mathematical reasoning from spoken input remains\nunderexplored. Prior studies on speech modality have mostly focused on factual\nspeech understanding or simple audio reasoning tasks, providing limited insight\ninto logical step-by-step reasoning, such as that required for mathematical\nproblem solving. To address this gap, we introduce Spoken Math Question\nAnswering (Spoken-MQA), a new benchmark designed to evaluate the mathematical\nreasoning capabilities of speech-based models, including both cascade models\n(ASR + LLMs) and end-to-end speech LLMs. Spoken-MQA covers a diverse set of\nmath problems, including pure arithmetic, single-step and multi-step contextual\nreasoning, and knowledge-oriented reasoning problems, all presented in\nunambiguous natural spoken language. Through extensive experiments, we find\nthat: (1) while some speech LLMs perform competitively on contextual reasoning\ntasks involving basic arithmetic, they still struggle with direct arithmetic\nproblems; (2) current LLMs exhibit a strong bias toward symbolic mathematical\nexpressions written in LaTex and have difficulty interpreting verbalized\nmathematical expressions; and (3) mathematical knowledge reasoning abilities\nare significantly degraded in current speech LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u6d4b\u57fa\u51c6Spoken-MQA\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u97f3\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u76f4\u63a5\u7b97\u672f\u95ee\u9898\u548c\u53e3\u8bed\u5316\u6570\u5b66\u8868\u8fbe\u7406\u89e3\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u591a\u5a92\u4f53\u8bed\u8a00\u6a21\u578b\u5728\u53e3\u8bed\u8f93\u5165\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5f15\u5165Spoken-MQA\u8bc4\u6d4b\u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u79cd\u6570\u5b66\u95ee\u9898\u7c7b\u578b\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u7ea7\u8054\u6a21\u578b\u4e0e\u7aef\u5230\u7aef\u8bed\u97f3\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u8bed\u97f3\u6a21\u578b\u5728\u57fa\u7840\u7b97\u672f\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u76f4\u63a5\u7b97\u672f\u95ee\u9898\u548c\u53e3\u8bed\u6570\u5b66\u8868\u8fbe\u7406\u89e3\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u6570\u5b66\u77e5\u8bc6\u63a8\u7406\u80fd\u529b\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u5f53\u524d\u8bed\u97f3\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u4ecd\u5b58\u5728\u5c40\u9650\uff0c\u5c24\u5176\u662f\u5bf9\u53e3\u8bed\u5316\u8868\u8fbe\u7684\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u8bed\u97f3\u6a21\u578b,\u6570\u5b66\u63a8\u7406,Spoken-MQA,\u8bc4\u6d4b\u57fa\u51c6"}}
{"id": "2505.14896", "pdf": "https://arxiv.org/pdf/2505.14896", "abs": "https://arxiv.org/abs/2505.14896", "authors": ["Hootan Mahmoodiyan", "Maryam Ahang", "Mostafa Abbasi", "Homayoun Najjaran"], "title": "Feature-Weighted MMD-CORAL for Domain Adaptation in Power Transformer Fault Diagnosis", "categories": ["cs.LG"], "comment": null, "summary": "Ensuring the reliable operation of power transformers is critical to grid\nstability. Dissolved Gas Analysis (DGA) is widely used for fault diagnosis, but\ntraditional methods rely on heuristic rules, which may lead to inconsistent\nresults. Machine learning (ML)-based approaches have improved diagnostic\naccuracy; however, power transformers operate under varying conditions, and\ndifferences in transformer type, environmental factors, and operational\nsettings create distribution shifts in diagnostic data. Consequently, direct\nmodel transfer between transformers often fails, making techniques for domain\nadaptation a necessity. To tackle this issue, this work proposes a\nfeature-weighted domain adaptation technique that combines Maximum Mean\nDiscrepancy (MMD) and Correlation Alignment (CORAL) with feature-specific\nweighting (MCW). Kolmogorov-Smirnov (K-S) statistics are used to assign\nadaptable weights, prioritizing features with larger distributional\ndiscrepancies and thereby improving source and target domain alignment.\nExperimental evaluations on datasets for power transformers demonstrate the\neffectiveness of the proposed method, which achieves a 7.9% improvement over\nFine-Tuning and a 2.2% improvement over MMD-CORAL (MC). Furthermore, it\noutperforms both techniques across various training sample sizes, confirming\nits robustness for domain adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u52a0\u6743\u7684\u9886\u57df\u81ea\u9002\u5e94\u6280\u672f\uff08MCW\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\u548c\u76f8\u5173\u5bf9\u9f50\uff08CORAL\uff09\uff0c\u5e76\u5229\u7528Kolmogorov-Smirnov\u7edf\u8ba1\u91cf\u5206\u914d\u6743\u91cd\uff0c\u4ee5\u63d0\u9ad8\u7535\u529b\u53d8\u538b\u5668\u6545\u969c\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5fae\u8c03\u548c\u4f20\u7edfMMD-CORAL\u3002", "motivation": "\u4f20\u7edfDGA\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u800c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u867d\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u4f46\u53d8\u538b\u5668\u8fd0\u884c\u6761\u4ef6\u7684\u591a\u6837\u6027\u5bfc\u81f4\u6570\u636e\u5206\u5e03\u504f\u79fb\uff0c\u9700\u8981\u9886\u57df\u81ea\u9002\u5e94\u6280\u672f\u3002", "method": "\u7ed3\u5408MMD\u548cCORAL\u7684\u7279\u5f81\u52a0\u6743\u6280\u672f\uff08MCW\uff09\uff0c\u5229\u7528K-S\u7edf\u8ba1\u91cf\u4e3a\u7279\u5f81\u5206\u914d\u6743\u91cd\uff0c\u4f18\u5148\u5904\u7406\u5206\u5e03\u5dee\u5f02\u8f83\u5927\u7684\u7279\u5f81\uff0c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u6e90\u57df\u548c\u76ee\u6807\u57df\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMCW\u6bd4\u5fae\u8c03\u65b9\u6cd5\u63d0\u9ad8\u4e867.9%\uff0c\u6bd4\u4f20\u7edfMMD-CORAL\u65b9\u6cd5\u63d0\u9ad8\u4e862.2%\uff0c\u5e76\u5728\u4e0d\u540c\u8bad\u7ec3\u6837\u672c\u89c4\u6a21\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684MCW\u65b9\u6cd5\u5728\u7535\u529b\u53d8\u538b\u5668\u6545\u969c\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u6570\u636e\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "keywords": "\u7535\u529b\u53d8\u538b\u5668\u3001\u6545\u969c\u8bca\u65ad\u3001\u9886\u57df\u81ea\u9002\u5e94\u3001\u6700\u5927\u5747\u503c\u5dee\u5f02\u3001\u76f8\u5173\u5bf9\u9f50\u3001Kolmogorov-Smirnov\u7edf\u8ba1\u91cf"}}
{"id": "2505.14719", "pdf": "https://arxiv.org/pdf/2505.14719", "abs": "https://arxiv.org/abs/2505.14719", "authors": ["Wei Hua", "Chenlin Zhou", "Jibin Wu", "Yansong Chua", "Yangyang Shu"], "title": "MSVIT: Improving Spiking Vision Transformer Using Multi-scale Attention Fusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The combination of Spiking Neural Networks(SNNs) with Vision Transformer\narchitectures has attracted significant attention due to the great potential\nfor energy-efficient and high-performance computing paradigms. However, a\nsubstantial performance gap still exists between SNN-based and ANN-based\ntransformer architectures. While existing methods propose spiking\nself-attention mechanisms that are successfully combined with SNNs, the overall\narchitectures proposed by these methods suffer from a bottleneck in effectively\nextracting features from different image scales. In this paper, we address this\nissue and propose MSVIT, a novel spike-driven Transformer architecture, which\nfirstly uses multi-scale spiking attention (MSSA) to enrich the capability of\nspiking attention blocks. We validate our approach across various main data\nsets. The experimental results show that MSVIT outperforms existing SNN-based\nmodels, positioning itself as a state-of-the-art solution among SNN-transformer\narchitectures. The codes are available at\nhttps://github.com/Nanhu-AI-Lab/MSViT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MSVIT\uff0c\u4e00\u79cd\u65b0\u578b\u7684\u8109\u51b2\u9a71\u52a8Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u8109\u51b2\u6ce8\u610f\u529b\uff08MSSA\uff09\u63d0\u5347\u6027\u80fd\uff0c\u6210\u4e3aSNN-transformer\u67b6\u6784\u7684\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "Spiking Neural Networks\uff08SNNs\uff09\u4e0eVision Transformer\u7ed3\u5408\u5177\u6709\u9ad8\u6548\u80fd\u548c\u4f4e\u80fd\u8017\u7684\u6f5c\u529b\uff0c\u4f46SNN-based\u4e0eANN-based Transformer\u95f4\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5728\u63d0\u53d6\u591a\u5c3a\u5ea6\u56fe\u50cf\u7279\u5f81\u65b9\u9762\u5b58\u5728\u74f6\u9888\u3002", "method": "\u63d0\u51faMSVIT\uff0c\u9996\u6b21\u91c7\u7528\u591a\u5c3a\u5ea6\u8109\u51b2\u6ce8\u610f\u529b\uff08MSSA\uff09\u589e\u5f3a\u8109\u51b2\u6ce8\u610f\u529b\u5757\u7684\u80fd\u529b\u3002", "result": "MSVIT\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709SNN-based\u6a21\u578b\uff0c\u6210\u4e3aSNN-transformer\u67b6\u6784\u7684\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "MSVIT\u901a\u8fc7\u591a\u5c3a\u5ea6\u8109\u51b2\u6ce8\u610f\u529b\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86SNN-transformer\u67b6\u6784\u7684\u6027\u80fd\u3002", "keywords": "Spiking Neural Networks, Vision Transformer, MSVIT, Multi-Scale Spiking Attention"}}
{"id": "2505.15024", "pdf": "https://arxiv.org/pdf/2505.15024", "abs": "https://arxiv.org/abs/2505.15024", "authors": ["Furong Jia", "David Sontag", "Monica Agrawal"], "title": "Diagnosing our datasets: How does my language model learn clinical information?", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have performed well across various clinical\nnatural language processing tasks, despite not being directly trained on\nelectronic health record (EHR) data. In this work, we examine how popular\nopen-source LLMs learn clinical information from large mined corpora through\ntwo crucial but understudied lenses: (1) their interpretation of clinical\njargon, a foundational ability for understanding real-world clinical notes, and\n(2) their responses to unsupported medical claims. For both use cases, we\ninvestigate the frequency of relevant clinical information in their\ncorresponding pretraining corpora, the relationship between pretraining data\ncomposition and model outputs, and the sources underlying this data. To isolate\nclinical jargon understanding, we evaluate LLMs on a new dataset MedLingo.\nUnsurprisingly, we find that the frequency of clinical jargon mentions across\nmajor pretraining corpora correlates with model performance. However, jargon\nfrequently appearing in clinical notes often rarely appears in pretraining\ncorpora, revealing a mismatch between available data and real-world usage.\nSimilarly, we find that a non-negligible portion of documents support disputed\nclaims that can then be parroted by models. Finally, we classified and analyzed\nthe types of online sources in which clinical jargon and unsupported medical\nclaims appear, with implications for future dataset composition.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u901a\u8fc7\u5b66\u4e60\u5927\u578b\u8bed\u6599\u5e93\u6765\u7406\u89e3\u4e34\u5e8a\u672f\u8bed\u548c\u5e94\u5bf9\u65e0\u652f\u6301\u7684\u533b\u5b66\u58f0\u660e\uff0c\u53d1\u73b0\u8bed\u6599\u5e93\u4e2d\u4e34\u5e8a\u672f\u8bed\u7684\u9891\u7387\u4e0e\u6a21\u578b\u8868\u73b0\u76f8\u5173\uff0c\u4f46\u5b58\u5728\u4e0e\u73b0\u5b9e\u4f7f\u7528\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1LLMs\u672a\u76f4\u63a5\u57fa\u4e8e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u8bad\u7ec3\uff0c\u4f46\u5b83\u4eec\u5728\u4e34\u5e8a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLMs\u5982\u4f55\u901a\u8fc7\u8bed\u6599\u5e93\u5b66\u4e60\u4e34\u5e8a\u4fe1\u606f\uff0c\u5c24\u5176\u662f\u4e34\u5e8a\u672f\u8bed\u89e3\u91ca\u548c\u5bf9\u65e0\u652f\u6301\u533b\u5b66\u58f0\u660e\u7684\u54cd\u5e94\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5173\u952e\u89c6\u89d2\u5206\u6790\uff1a\uff081\uff09\u4e34\u5e8a\u672f\u8bed\u7406\u89e3\uff08\u57fa\u4e8e\u65b0\u6570\u636e\u96c6MedLingo\uff09\uff0c\uff082\uff09\u5bf9\u65e0\u652f\u6301\u533b\u5b66\u58f0\u660e\u7684\u54cd\u5e94\u3002\u7814\u7a76\u8c03\u67e5\u4e86\u76f8\u5173\u4e34\u5e8a\u4fe1\u606f\u5728\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u7684\u9891\u7387\u53ca\u5176\u4e0e\u6a21\u578b\u8f93\u51fa\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u4e34\u5e8a\u672f\u8bed\u5728\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u7684\u51fa\u73b0\u9891\u7387\u4e0e\u6a21\u578b\u8868\u73b0\u76f8\u5173\uff0c\u4f46\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u5e38\u89c1\u7684\u672f\u8bed\u5728\u8bed\u6599\u5e93\u4e2d\u51fa\u73b0\u8f83\u5c11\u3002\u6b64\u5916\uff0c\u6a21\u578b\u53ef\u80fd\u590d\u8ff0\u8bed\u6599\u5e93\u4e2d\u65e0\u652f\u6301\u7684\u533b\u5b66\u58f0\u660e\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u4e0e\u73b0\u5b9e\u4f7f\u7528\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\uff0c\u5f3a\u8c03\u4e86\u672a\u6765\u6570\u636e\u96c6\u6784\u5efa\u4e2d\u5728\u7ebf\u6765\u6e90\u5206\u7c7b\u7684\u91cd\u8981\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u4e34\u5e8a\u81ea\u7136\u8bed\u8a00\u5904\u7406, \u4e34\u5e8a\u672f\u8bed, \u533b\u5b66\u58f0\u660e, \u9884\u8bad\u7ec3\u6570\u636e"}}
{"id": "2505.14897", "pdf": "https://arxiv.org/pdf/2505.14897", "abs": "https://arxiv.org/abs/2505.14897", "authors": ["Ali Mohajerzarrinkelk", "Maryam Ahang", "Mehran Zoravar", "Mostafa Abbasi", "Homayoun Najjaran"], "title": "Multi-Channel Swin Transformer Framework for Bearing Remaining Useful Life Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Precise estimation of the Remaining Useful Life (RUL) of rolling bearings is\nan important consideration to avoid unexpected failures, reduce downtime, and\npromote safety and efficiency in industrial systems. Complications in\ndegradation trends, noise presence, and the necessity to detect faults in\nadvance make estimation of RUL a challenging task. This paper introduces a\nnovel framework that combines wavelet-based denoising method, Wavelet Packet\nDecomposition (WPD), and a customized multi-channel Swin Transformer model\n(MCSFormer) to address these problems. With attention mechanisms incorporated\nfor feature fusion, the model is designed to learn global and local degradation\npatterns utilizing hierarchical representations for enhancing predictive\nperformance. Additionally, a customized loss function is developed as a key\ndistinction of this work to differentiate between early and late predictions,\nprioritizing accurate early detection and minimizing the high operation risks\nof late predictions. The proposed model was evaluated with the PRONOSTIA\ndataset using three experiments. Intra-condition experiments demonstrated that\nMCSFormer outperformed state-of-the-art models, including the Adaptive\nTransformer, MDAN, and CNN-SRU, achieving 41%, 64%, and 69% lower MAE on\naverage across different operating conditions, respectively. In terms of\ncross-condition testing, it achieved superior generalization under varying\noperating conditions compared to the adapted ViT and Swin Transformer. Lastly,\nthe custom loss function effectively reduced late predictions, as evidenced in\na 6.3% improvement in the scoring metric while maintaining competitive overall\nperformance. The model's robust noise resistance, generalization capability,\nand focus on safety make MCSFormer a trustworthy and effective predictive\nmaintenance tool in industrial applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5c0f\u6ce2\u53bb\u566a\u3001\u5c0f\u6ce2\u5305\u5206\u89e3\u548c\u591a\u901a\u9053Swin Transformer\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u7cbe\u786e\u9884\u6d4b\u6eda\u52a8\u8f74\u627f\u7684\u5269\u4f59\u4f7f\u7528\u5bff\u547d\uff0c\u5177\u6709\u8f83\u597d\u7684\u566a\u58f0\u62b5\u6297\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u7cbe\u786e\u9884\u6d4b\u6eda\u52a8\u8f74\u627f\u7684\u5269\u4f59\u4f7f\u7528\u5bff\u547d\u53ef\u907f\u514d\u610f\u5916\u6545\u969c\u3001\u51cf\u5c11\u505c\u673a\u65f6\u95f4\uff0c\u5e76\u63d0\u9ad8\u5de5\u4e1a\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002\u4f46\u7531\u4e8e\u9000\u5316\u8d8b\u52bf\u590d\u6742\u3001\u566a\u58f0\u5e72\u6270\u548c\u65e9\u671f\u6545\u969c\u68c0\u6d4b\u9700\u6c42\uff0c\u8fd9\u662f\u4e00\u9879\u6311\u6218\u6027\u4efb\u52a1\u3002", "method": "\u7ed3\u5408\u5c0f\u6ce2\u53bb\u566a\u548c\u5c0f\u6ce2\u5305\u5206\u89e3\uff08WPD\uff09\u65b9\u6cd5\uff0c\u5e76\u91c7\u7528\u81ea\u5b9a\u4e49\u591a\u901a\u9053Swin Transformer\u6a21\u578b\uff08MCSFormer\uff09\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u7279\u5f81\uff0c\u5b66\u4e60\u5168\u5c40\u548c\u5c40\u90e8\u9000\u5316\u6a21\u5f0f\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u533a\u5206\u65e9\u671f\u548c\u665a\u671f\u9884\u6d4b\u3002", "result": "\u5728PRONOSTIA\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u4e09\u7ec4\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aMCSFormer\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5305\u62ec\u5e73\u5747MAE\u964d\u4f4e41%\u300164%\u548c69%\u3002\u8de8\u6761\u4ef6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u663e\u8457\u51cf\u5c11\u4e86\u665a\u671f\u9884\u6d4b\u8bef\u5dee\u3002", "conclusion": "MCSFormer\u5728\u566a\u58f0\u62b5\u6297\u3001\u6cdb\u5316\u80fd\u529b\u548c\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u662f\u4e00\u79cd\u53ef\u9760\u4e14\u6709\u6548\u7684\u5de5\u4e1a\u9884\u6d4b\u7ef4\u62a4\u5de5\u5177\u3002", "keywords": "\u5269\u4f59\u4f7f\u7528\u5bff\u547d\uff08RUL\uff09\u3001\u6eda\u52a8\u8f74\u627f\u3001\u5c0f\u6ce2\u53bb\u566a\u3001Swin Transformer\u3001\u9884\u6d4b\u7ef4\u62a4"}}
{"id": "2505.14723", "pdf": "https://arxiv.org/pdf/2505.14723", "abs": "https://arxiv.org/abs/2505.14723", "authors": ["Subrata Biswas", "Mohammad Nur Hossain Khan", "Bashima Islam"], "title": "QUADS: QUAntized Distillation Framework for Efficient Speech Language Understanding", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": null, "summary": "Spoken Language Understanding (SLU) systems must balance performance and\nefficiency, particularly in resource-constrained environments. Existing methods\napply distillation and quantization separately, leading to suboptimal\ncompression as distillation ignores quantization constraints. We propose QUADS,\na unified framework that optimizes both through multi-stage training with a\npre-tuned model, enhancing adaptability to low-bit regimes while maintaining\naccuracy. QUADS achieves 71.13\\% accuracy on SLURP and 99.20\\% on FSC, with\nonly minor degradations of up to 5.56\\% compared to state-of-the-art models.\nAdditionally, it reduces computational complexity by 60--73$\\times$ (GMACs) and\nmodel size by 83--700$\\times$, demonstrating strong robustness under extreme\nquantization. These results establish QUADS as a highly efficient solution for\nreal-world, resource-constrained SLU applications.", "AI": {"tldr": "QUADS\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u77e5\u8bc6\u84b8\u998f\u4e0e\u91cf\u5316\u6280\u672f\uff0c\u4f18\u5316\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u8bed\u97f3\u8bed\u8a00\u7406\u89e3\uff08SLU\uff09\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u9ad8\u6548\u7387\u4e14\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u72ec\u7acb\u5e94\u7528\u77e5\u8bc6\u84b8\u998f\u4e0e\u91cf\u5316\uff0c\u5bfc\u81f4\u538b\u7f29\u6548\u679c\u4e0d\u7406\u60f3\u3002QUADS\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u7684\u4f18\u5316\u6846\u67b6\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u9002\u914d\u4f4e\u6bd4\u7279\u573a\u666f\u3002", "method": "QUADS\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\uff0c\u7ed3\u5408\u9884\u8c03\u4f18\u6a21\u578b\uff0c\u540c\u65f6\u4f18\u5316\u77e5\u8bc6\u84b8\u998f\u4e0e\u91cf\u5316\u6280\u672f\u3002", "result": "QUADS\u5728SLURP\u548cFSC\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523071.13%\u548c99.20%\u7684\u51c6\u786e\u7387\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e60-73\u500d\uff0c\u6a21\u578b\u5927\u5c0f\u7f29\u51cf83-700\u500d\u3002", "conclusion": "QUADS\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u5316\u7684SLU\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u771f\u5b9e\u573a\u666f\uff0c\u4e14\u8868\u73b0\u51fa\u6781\u5f3a\u7684\u91cf\u5316\u9c81\u68d2\u6027\u3002", "keywords": "\u8bed\u97f3\u8bed\u8a00\u7406\u89e3\uff0c\u77e5\u8bc6\u84b8\u998f\uff0c\u91cf\u5316\u6280\u672f\uff0c\u9ad8\u6548\u8ba1\u7b97"}}
{"id": "2505.15031", "pdf": "https://arxiv.org/pdf/2505.15031", "abs": "https://arxiv.org/abs/2505.15031", "authors": ["Wenqing Wu", "Haixu Xi", "Chengzhi Zhang"], "title": "Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "comment": null, "summary": "Peer review is vital in academia for evaluating research quality. Top AI\nconferences use reviewer confidence scores to ensure review reliability, but\nexisting studies lack fine-grained analysis of text-score consistency,\npotentially missing key details. This work assesses consistency at word,\nsentence, and aspect levels using deep learning and NLP conference review data.\nWe employ deep learning to detect hedge sentences and aspects, then analyze\nreport length, hedge word/sentence frequency, aspect mentions, and sentiment to\nevaluate text-score alignment. Correlation, significance, and regression tests\nexamine confidence scores' impact on paper outcomes. Results show high\ntext-score consistency across all levels, with regression revealing higher\nconfidence scores correlate with paper rejection, validating expert assessments\nand peer review fairness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u548cNLP\u6280\u672f\uff0c\u5206\u6790\u4e86AI\u4f1a\u8bae\u8bc4\u5ba1\u4e2d\u6587\u672c\u4e0e\u7f6e\u4fe1\u5ea6\u5f97\u5206\u7684\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u9ad8\u7f6e\u4fe1\u5ea6\u5f97\u5206\u4e0e\u8bba\u6587\u62d2\u7a3f\u76f8\u5173\uff0c\u9a8c\u8bc1\u4e86\u8bc4\u5ba1\u7684\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u8bc4\u5ba1\u6587\u672c\u4e0e\u7f6e\u4fe1\u5ea6\u5f97\u5206\u4e00\u81f4\u6027\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u53ef\u80fd\u9057\u6f0f\u5173\u952e\u7ec6\u8282\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u542b\u7cca\u53e5\u5b50\u548c\u65b9\u9762\uff0c\u5206\u6790\u62a5\u544a\u957f\u5ea6\u3001\u542b\u7cca\u8bcd\u53e5\u9891\u7387\u3001\u65b9\u9762\u63d0\u53ca\u548c\u60c5\u611f\uff0c\u4ee5\u8bc4\u4f30\u6587\u672c\u4e0e\u5f97\u5206\u7684\u4e00\u81f4\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5404\u4e2a\u5c42\u9762\u7684\u6587\u672c\u4e0e\u5f97\u5206\u4e00\u81f4\u6027\u9ad8\uff0c\u56de\u5f52\u5206\u6790\u8868\u660e\u9ad8\u7f6e\u4fe1\u5ea6\u5f97\u5206\u4e0e\u8bba\u6587\u62d2\u7a3f\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u4e13\u5bb6\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u548c\u540c\u884c\u8bc4\u5ba1\u7684\u516c\u5e73\u6027\u3002", "keywords": "peer review, confidence scores, deep learning, NLP, text-score consistency"}}
{"id": "2505.14903", "pdf": "https://arxiv.org/pdf/2505.14903", "abs": "https://arxiv.org/abs/2505.14903", "authors": ["Regol Florence", "Schwinn Leo", "Sprague Kyle", "Coates Mark", "Markovich Thomas"], "title": "When to retrain a machine learning model", "categories": ["cs.LG"], "comment": null, "summary": "A significant challenge in maintaining real-world machine learning models is\nresponding to the continuous and unpredictable evolution of data. Most\npractitioners are faced with the difficult question: when should I retrain or\nupdate my machine learning model? This seemingly straightforward problem is\nparticularly challenging for three reasons: 1) decisions must be made based on\nvery limited information - we usually have access to only a few examples, 2)\nthe nature, extent, and impact of the distribution shift are unknown, and 3) it\ninvolves specifying a cost ratio between retraining and poor performance, which\ncan be hard to characterize. Existing works address certain aspects of this\nproblem, but none offer a comprehensive solution. Distribution shift detection\nfalls short as it cannot account for the cost trade-off; the scarcity of the\ndata, paired with its unusual structure, makes it a poor fit for existing\noffline reinforcement learning methods, and the online learning formulation\noverlooks key practical considerations. To address this, we present a\nprincipled formulation of the retraining problem and propose an\nuncertainty-based method that makes decisions by continually forecasting the\nevolution of model performance evaluated with a bounded metric. Our experiments\naddressing classification tasks show that the method consistently outperforms\nexisting baselines on 7 datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6570\u636e\u6301\u7eed\u6f14\u53d8\u65f6\u7684\u91cd\u65b0\u8bad\u7ec3\u51b3\u7b56\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6570\u636e\u6301\u7eed\u53d8\u5316\u65f6\u4f55\u65f6\u91cd\u65b0\u8bad\u7ec3\u7684\u96be\u9898\uff0c\u7279\u522b\u662f\u9762\u5bf9\u6709\u9650\u4fe1\u606f\u3001\u672a\u77e5\u5206\u5e03\u504f\u79fb\u548c\u6210\u672c\u6743\u8861\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6301\u7eed\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u7684\u53d8\u5316\u6765\u505a\u51fa\u91cd\u65b0\u8bad\u7ec3\u51b3\u7b56\u3002", "result": "\u57287\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u91cd\u65b0\u8bad\u7ec3\u51b3\u7b56\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u673a\u5668\u5b66\u4e60, \u5206\u5e03\u504f\u79fb, \u91cd\u65b0\u8bad\u7ec3\u51b3\u7b56, \u4e0d\u786e\u5b9a\u6027\u65b9\u6cd5, \u6027\u80fd\u9884\u6d4b"}}
{"id": "2505.14726", "pdf": "https://arxiv.org/pdf/2505.14726", "abs": "https://arxiv.org/abs/2505.14726", "authors": ["Manshi Limbu", "Diwita Banerjee"], "title": "MedBLIP: Fine-tuning BLIP for Medical Image Captioning", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Medical image captioning is a challenging task that requires generating\nclinically accurate and semantically meaningful descriptions of radiology\nimages. While recent vision-language models (VLMs) such as BLIP, BLIP2, Gemini\nand ViT-GPT2 show strong performance on natural image datasets, they often\nproduce generic or imprecise captions when applied to specialized medical\ndomains. In this project, we explore the effectiveness of fine-tuning the BLIP\nmodel on the ROCO dataset for improved radiology captioning. We compare the\nfine-tuned BLIP against its zero-shot version, BLIP-2 base, BLIP-2 Instruct and\na ViT-GPT2 transformer baseline. Our results demonstrate that domain-specific\nfine-tuning on BLIP significantly improves performance across both quantitative\nand qualitative evaluation metrics. We also visualize decoder cross-attention\nmaps to assess interpretability and conduct an ablation study to evaluate the\ncontributions of encoder-only and decoder-only fine-tuning. Our findings\nhighlight the importance of targeted adaptation for medical applications and\nsuggest that decoder-only fine-tuning (encoder-frozen) offers a strong\nperformance baseline with 5% lower training time than full fine-tuning, while\nfull model fine-tuning still yields the best results overall.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u5fae\u8c03BLIP\u6a21\u578b\u5728ROCO\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u63cf\u8ff0\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5bf9\u6bd4\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u5fae\u8c03BLIP\u6a21\u578b\uff0c\u5e76\u4e0eBLIP-2\u3001ViT-GPT2\u7b49\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\uff0c\u540c\u65f6\u5206\u6790\u4e86\u4ec5\u89e3\u7801\u5668\u5fae\u8c03\u7684\u6548\u679c\u3002", "result": "\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4ec5\u89e3\u7801\u5668\u5fae\u8c03\u5728\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u8868\u73b0\u3002", "conclusion": "\u533b\u5b66\u5e94\u7528\u9700\u8981\u9488\u5bf9\u6027\u9002\u914d\uff0c\u5b8c\u6574\u5fae\u8c03\u6548\u679c\u6700\u4f73\uff0c\u4f46\u4ec5\u89e3\u7801\u5668\u5fae\u8c03\u662f\u9ad8\u6548\u9009\u62e9\u3002", "keywords": "\u533b\u5b66\u56fe\u50cf\u63cf\u8ff0\uff0cBLIP\u6a21\u578b\uff0c\u9886\u57df\u5fae\u8c03\uff0cROCO\u6570\u636e\u96c6"}}
{"id": "2505.15038", "pdf": "https://arxiv.org/pdf/2505.15038", "abs": "https://arxiv.org/abs/2505.15038", "authors": ["Haiyan Zhao", "Xuansheng Wu", "Fan Yang", "Bo Shen", "Ninghao Liu", "Mengnan Du"], "title": "Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 5 figures, 3 tables", "summary": "Linear Concept Vectors have proven effective for steering large language\nmodels (LLMs). While existing approaches like linear probing and\ndifference-in-means derive these vectors from LLM hidden representations,\ndiverse data introduces noises (i.e., irrelevant features) that challenge\nsteering robustness. To address this, we propose Sparse Autoencoder-Denoised\nConcept Vectors (SDCV), which uses Sparse Autoencoders to filter out noisy\nfeatures from hidden representations. When applied to linear probing and\ndifference-in-means, our method improves their steering success rates. We\nvalidate our noise hypothesis through counterfactual experiments and feature\nvisualizations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSDCV\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u4ece\u9690\u85cf\u8868\u5f81\u4e2d\u53bb\u9664\u566a\u58f0\u7279\u5f81\uff0c\u4ece\u800c\u63d0\u5347\u7ebf\u6027\u6982\u5ff5\u5411\u91cf\u7684\u7a33\u5065\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u7ebf\u6027\u63a2\u6d4b\u548c\u5747\u503c\u5dee\u5f02\uff09\u5728\u4ece\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9690\u85cf\u8868\u5f81\u4e2d\u63d0\u53d6\u7ebf\u6027\u6982\u5ff5\u5411\u91cf\u65f6\uff0c\u53d7\u6570\u636e\u591a\u6837\u6027\u5f15\u5165\u7684\u566a\u58f0\u5f71\u54cd\uff0c\u5bfc\u81f4\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSDCV\u65b9\u6cd5\uff0c\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8fc7\u6ee4\u9690\u85cf\u8868\u5f81\u4e2d\u7684\u566a\u58f0\u7279\u5f81\uff0c\u5e76\u5728\u7ebf\u6027\u63a2\u6d4b\u548c\u5747\u503c\u5dee\u5f02\u4e2d\u5e94\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSDCV\u63d0\u9ad8\u4e86\u7ebf\u6027\u63a2\u6d4b\u548c\u5747\u503c\u5dee\u5f02\u65b9\u6cd5\u7684\u64cd\u63a7\u6210\u529f\u7387\u3002", "conclusion": "\u901a\u8fc7\u53cd\u4e8b\u5b9e\u5b9e\u9a8c\u548c\u7279\u5f81\u53ef\u89c6\u5316\u9a8c\u8bc1\u4e86\u566a\u58f0\u5047\u8bbe\uff0cSDCV\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7ebf\u6027\u6982\u5ff5\u5411\u91cf\u7684\u8868\u73b0\u3002", "keywords": "\u7ebf\u6027\u6982\u5ff5\u5411\u91cf\u3001\u7a00\u758f\u81ea\u7f16\u7801\u5668\u3001\u566a\u58f0\u6ee4\u6ce2\u3001\u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.14919", "pdf": "https://arxiv.org/pdf/2505.14919", "abs": "https://arxiv.org/abs/2505.14919", "authors": ["Frederik Wenkel", "Wilson Tu", "Cassandra Masschelein", "Hamed Shirzad", "Cian Eastwood", "Shawn T. Whitfield", "Ihab Bendidi", "Craig Russell", "Liam Hodgson", "Yassir El Mesbahi", "Jiarui Ding", "Marta M. Fay", "Berton Earnshaw", "Emmanuel Noutahi", "Alisandra K. Denton"], "title": "TxPert: Leveraging Biochemical Relationships for Out-of-Distribution Transcriptomic Perturbation Prediction", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Accurately predicting cellular responses to genetic perturbations is\nessential for understanding disease mechanisms and designing effective\ntherapies. Yet exhaustively exploring the space of possible perturbations\n(e.g., multi-gene perturbations or across tissues and cell types) is\nprohibitively expensive, motivating methods that can generalize to unseen\nconditions. In this work, we explore how knowledge graphs of gene-gene\nrelationships can improve out-of-distribution (OOD) prediction across three\nchallenging settings: unseen single perturbations; unseen double perturbations;\nand unseen cell lines. In particular, we present: (i) TxPert, a new\nstate-of-the-art method that leverages multiple biological knowledge networks\nto predict transcriptional responses under OOD scenarios; (ii) an in-depth\nanalysis demonstrating the impact of graphs, model architecture, and data on\nperformance; and (iii) an expanded benchmarking framework that strengthens\nevaluation standards for perturbation modeling.", "AI": {"tldr": "TxPert\u5229\u7528\u57fa\u56e0\u5173\u7cfb\u77e5\u8bc6\u56fe\u8c31\u6539\u8fdb\u672a\u89c1\u6761\u4ef6\u4e0b\u7684\u7ec6\u80de\u6270\u52a8\u54cd\u5e94\u9884\u6d4b\uff0c\u63d0\u5347\u4e86\u8de8\u7ec4\u7ec7\u3001\u7ec6\u80de\u7c7b\u578b\u548c\u591a\u57fa\u56e0\u6270\u52a8\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u7ec6\u80de\u5bf9\u9057\u4f20\u6270\u52a8\u7684\u54cd\u5e94\u5bf9\u4e8e\u75be\u75c5\u673a\u5236\u7814\u7a76\u548c\u6cbb\u7597\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5168\u5c3a\u5ea6\u5b9e\u9a8c\u63a2\u7d22\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86TxPert\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u7ec4\u751f\u7269\u77e5\u8bc6\u7f51\u7edc\u9884\u6d4b\u8f6c\u5f55\u54cd\u5e94\uff0c\u5e76\u8fdb\u884c\u6a21\u578b\u4e0e\u6570\u636e\u5206\u6790\u3002", "result": "TxPert\u5728\u5355\u3001\u53cc\u57fa\u56e0\u6270\u52a8\u53ca\u672a\u89c1\u7ec6\u80de\u7cfb\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u786e\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u6807\u6746\u3002", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u80fd\u6709\u6548\u589e\u5f3a\u6a21\u578b\u5bf9\u672a\u89c1\u6270\u52a8\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4e3a\u65b0\u6807\u51c6\u6846\u67b6\u7684\u5efa\u7acb\u63d0\u4f9b\u652f\u6301\u3002", "keywords": "\u7ec6\u80de\u6270\u52a8\u54cd\u5e94,\u77e5\u8bc6\u56fe\u8c31,\u6cdb\u5316\u9884\u6d4b,TxPert,\u8f6c\u5f55\u54cd\u5e94"}}
{"id": "2505.14728", "pdf": "https://arxiv.org/pdf/2505.14728", "abs": "https://arxiv.org/abs/2505.14728", "authors": ["Xiao Lin", "Zhining Liu", "Ze Yang", "Gaotang Li", "Ruizhong Qiu", "Shuke Wang", "Hui Liu", "Haotian Li", "Sumit Keswani", "Vishwa Pardeshi", "Huijun Zhao", "Wei Fan", "Hanghang Tong"], "title": "MORALISE: A Structured Benchmark for Moral Alignment in Visual Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY", "cs.MM"], "comment": "21 pages, 11 figures, 7 tables", "summary": "Warning: This paper contains examples of harmful language and images. Reader\ndiscretion is advised. Recently, vision-language models have demonstrated\nincreasing influence in morally sensitive domains such as autonomous driving\nand medical analysis, owing to their powerful multimodal reasoning\ncapabilities. As these models are deployed in high-stakes real-world\napplications, it is of paramount importance to ensure that their outputs align\nwith human moral values and remain within moral boundaries. However, existing\nwork on moral alignment either focuses solely on textual modalities or relies\nheavily on AI-generated images, leading to distributional biases and reduced\nrealism. To overcome these limitations, we introduce MORALISE, a comprehensive\nbenchmark for evaluating the moral alignment of vision-language models (VLMs)\nusing diverse, expert-verified real-world data. We begin by proposing a\ncomprehensive taxonomy of 13 moral topics grounded in Turiel's Domain Theory,\nspanning the personal, interpersonal, and societal moral domains encountered in\neveryday life. Built on this framework, we manually curate 2,481 high-quality\nimage-text pairs, each annotated with two fine-grained labels: (1) topic\nannotation, identifying the violated moral topic(s), and (2) modality\nannotation, indicating whether the violation arises from the image or the text.\nFor evaluation, we encompass two tasks, \\textit{moral judgment} and\n\\textit{moral norm attribution}, to assess models' awareness of moral\nviolations and their reasoning ability on morally salient content. Extensive\nexperiments on 19 popular open- and closed-source VLMs show that MORALISE poses\na significant challenge, revealing persistent moral limitations in current\nstate-of-the-art models. The full benchmark is publicly available at\nhttps://huggingface.co/datasets/Ze1025/MORALISE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86MORALISE\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9053\u5fb7\u5bf9\u9f50\uff0c\u901a\u8fc7\u4e13\u5bb6\u9a8c\u8bc1\u7684\u771f\u5b9e\u6570\u636e\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u504f\u5dee\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9053\u5fb7\u654f\u611f\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u9700\u8981\u786e\u4fdd\u5176\u8f93\u51fa\u7b26\u5408\u4eba\u7c7b\u9053\u5fb7\u4ef7\u503c\u89c2\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u5c40\u9650\u4e8e\u6587\u672c\u6216AI\u751f\u6210\u56fe\u50cf\uff0c\u7f3a\u4e4f\u771f\u5b9e\u6027\u548c\u591a\u6837\u6027\u3002", "method": "\u57fa\u4e8eTuriel\u7684\u9886\u57df\u7406\u8bba\uff0c\u63d0\u51fa\u4e8613\u4e2a\u9053\u5fb7\u4e3b\u9898\u5206\u7c7b\uff0c\u5e76\u624b\u52a8\u6807\u6ce8\u4e862,481\u4e2a\u9ad8\u8d28\u91cf\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u5305\u542b\u4e3b\u9898\u548c\u6a21\u6001\u6ce8\u91ca\u3002\u8bc4\u4f30\u4efb\u52a1\u5305\u62ec\u9053\u5fb7\u5224\u65ad\u548c\u9053\u5fb7\u89c4\u8303\u5f52\u56e0\u3002", "result": "\u572819\u4e2a\u6d41\u884c\u7684\u5f00\u653e\u548c\u95ed\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMORALISE\u5bf9\u73b0\u6709\u6a21\u578b\u63d0\u51fa\u4e86\u663e\u8457\u6311\u6218\uff0c\u63ed\u793a\u4e86\u5176\u9053\u5fb7\u5c40\u9650\u6027\u3002", "conclusion": "MORALISE\u662f\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\uff0c\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9053\u5fb7\u5bf9\u9f50\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u9053\u5fb7\u5bf9\u9f50, MORALISE, \u57fa\u51c6, \u591a\u6a21\u6001"}}
{"id": "2505.15045", "pdf": "https://arxiv.org/pdf/2505.15045", "abs": "https://arxiv.org/abs/2505.15045", "authors": ["Siyue Zhang", "Yilun Zhao", "Liyuan Geng", "Arman Cohan", "Anh Tuan Luu", "Chen Zhao"], "title": "Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective", "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM)-based embedding models, benefiting from large\nscale pre-training and post-training, have begun to surpass BERT and T5-based\nmodels on general-purpose text embedding tasks such as document retrieval.\nHowever, a fundamental limitation of LLM embeddings lies in the unidirectional\nattention used during autoregressive pre-training, which misaligns with the\nbidirectional nature of text embedding tasks. To this end, We propose adopting\ndiffusion language models for text embeddings, motivated by their inherent\nbidirectional architecture and recent success in matching or surpassing LLMs\nespecially on reasoning tasks. We present the first systematic study of the\ndiffusion language embedding model, which outperforms the LLM-based embedding\nmodel by 20% on long-document retrieval, 8% on reasoning-intensive retrieval,\n2% on instruction-following retrieval, and achieve competitive performance on\ntraditional text embedding benchmarks. Our analysis verifies that bidirectional\nattention is crucial for encoding global context in long and complex text.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08Diffusion Language Model\uff09\u6765\u6539\u8fdb\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6587\u672c\u5d4c\u5165\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86LLM\u56e0\u5355\u5411\u6ce8\u610f\u529b\u673a\u5236\u5728\u53cc\u5411\u6587\u672c\u5d4c\u5165\u4efb\u52a1\u4e2d\u7684\u4e0d\u5339\u914d\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "LLM\u7684\u5d4c\u5165\u6a21\u578b\u5728\u901a\u7528\u6587\u672c\u5d4c\u5165\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5355\u5411\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u53cc\u5411\u6587\u672c\u5d4c\u5165\u4efb\u52a1\u4e0d\u5339\u914d\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002\u6269\u6563\u8bed\u8a00\u6a21\u578b\u56e0\u5176\u53cc\u5411\u67b6\u6784\u548c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6210\u529f\uff0c\u6210\u4e3a\u6539\u8fdb\u65b9\u5411\u3002", "method": "\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u5730\u63a2\u8ba8\u4e86\u6269\u6563\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\uff0c\u901a\u8fc7\u5176\u56fa\u6709\u7684\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u6587\u672c\u5d4c\u5165\u4efb\u52a1\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u957f\u6587\u672c\u68c0\u7d22\u3001\u63a8\u7406\u5bc6\u96c6\u578b\u68c0\u7d22\u548c\u6307\u4ee4\u8ddf\u968f\u68c0\u7d22\u4efb\u52a1\u4e2d\u5206\u522b\u4f18\u4e8eLLM\u6a21\u578b20%\u30018%\u548c2%\uff0c\u5e76\u5728\u4f20\u7edf\u6587\u672c\u5d4c\u5165\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u7ade\u4e89\u6027\u3002", "conclusion": "\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\u5728\u7f16\u7801\u957f\u6587\u672c\u548c\u590d\u6742\u6587\u672c\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u65f6\u81f3\u5173\u91cd\u8981\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u6269\u6563\u8bed\u8a00\u6a21\u578b, \u6587\u672c\u5d4c\u5165, \u53cc\u5411\u6ce8\u610f\u529b, \u6587\u6863\u68c0\u7d22"}}
{"id": "2505.14933", "pdf": "https://arxiv.org/pdf/2505.14933", "abs": "https://arxiv.org/abs/2505.14933", "authors": ["Xuefeng Du"], "title": "Foundations of Unknown-aware Machine Learning", "categories": ["cs.LG"], "comment": "PhD Dissertation", "summary": "Ensuring the reliability and safety of machine learning models in open-world\ndeployment is a central challenge in AI safety. This thesis develops both\nalgorithmic and theoretical foundations to address key reliability issues\narising from distributional uncertainty and unknown classes, from standard\nneural networks to modern foundation models like large language models (LLMs).\n  Traditional learning paradigms, such as empirical risk minimization (ERM),\nassume no distribution shift between training and inference, often leading to\noverconfident predictions on out-of-distribution (OOD) inputs. This thesis\nintroduces novel frameworks that jointly optimize for in-distribution accuracy\nand reliability to unseen data. A core contribution is the development of an\nunknown-aware learning framework that enables models to recognize and handle\nnovel inputs without labeled OOD data.\n  We propose new outlier synthesis methods, VOS, NPOS, and DREAM-OOD, to\ngenerate informative unknowns during training. Building on this, we present\nSAL, a theoretical and algorithmic framework that leverages unlabeled\nin-the-wild data to enhance OOD detection under realistic deployment\nconditions. These methods demonstrate that abundant unlabeled data can be\nharnessed to recognize and adapt to unforeseen inputs, providing formal\nreliability guarantees.\n  The thesis also extends reliable learning to foundation models. We develop\nHaloScope for hallucination detection in LLMs, MLLMGuard for defending against\nmalicious prompts in multimodal models, and data cleaning methods to denoise\nhuman feedback used for better alignment. These tools target failure modes that\nthreaten the safety of large-scale models in deployment.\n  Overall, these contributions promote unknown-aware learning as a new\nparadigm, and we hope it can advance the reliability of AI systems with minimal\nhuman efforts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u672a\u77e5\u611f\u77e5\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5f00\u653e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\u548c\u672a\u77e5\u7c7b\u522b\u7684\u60c5\u51b5\u4e0b\u3002\u901a\u8fc7\u7b97\u6cd5\u548c\u7406\u8bba\u57fa\u7840\uff0c\u8bba\u6587\u6539\u8fdb\u4e86OOD\u68c0\u6d4b\u5e76\u6269\u5c55\u4e86\u53ef\u9760\u5b66\u4e60\u5230\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u5728\u5f00\u653e\u4e16\u754c\u90e8\u7f72\u4e2d\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7531\u4e8e\u5206\u5e03\u504f\u79fb\u548c\u672a\u77e5\u8f93\u5165\u53ef\u80fd\u505a\u51fa\u8fc7\u4e8e\u81ea\u4fe1\u7684\u9884\u6d4b\uff0c\u5bfc\u81f4\u5b89\u5168\u95ee\u9898\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u672a\u77e5\u611f\u77e5\u5b66\u4e60\u63d0\u5347\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u79cd\u65b9\u6cd5\uff1a1\uff09VOS\u3001NPOS\u548cDREAM-OOD\u7528\u4e8e\u751f\u6210\u672a\u77e5\u6837\u672c\uff1b2\uff09SAL\u6846\u67b6\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u589e\u5f3aOOD\u68c0\u6d4b\uff1b3\uff09\u9488\u5bf9\u57fa\u7840\u6a21\u578b\u7684\u5de5\u5177\u5982HaloScope\u3001MLLMGuard\u548c\u6570\u636e\u6e05\u6d17\u65b9\u6cd5\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u8bc6\u522b\u548c\u9002\u5e94\u672a\u77e5\u8f93\u5165\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u7684\u53ef\u9760\u6027\u4fdd\u8bc1\u3002\u540c\u65f6\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u57fa\u7840\u6a21\u578b\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u6027\u3002", "conclusion": "\u672a\u77e5\u611f\u77e5\u5b66\u4e60\u4e3aAI\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u80fd\u591f\u5728\u6700\u5c0f\u5316\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "keywords": "AI\u5b89\u5168,\u5206\u5e03\u504f\u79fb,OOD\u68c0\u6d4b,\u672a\u77e5\u611f\u77e5\u5b66\u4e60,\u57fa\u7840\u6a21\u578b"}}
{"id": "2505.15046", "pdf": "https://arxiv.org/pdf/2505.15046", "abs": "https://arxiv.org/abs/2505.15046", "authors": ["Yifan Wu", "Lutao Yan", "Leixian Shen", "Yinan Mei", "Jiannan Wang", "Yuyu Luo"], "title": "ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The emergence of Multi-modal Large Language Models (MLLMs) presents new\nopportunities for chart understanding. However, due to the fine-grained nature\nof these tasks, applying MLLMs typically requires large, high-quality datasets\nfor task-specific fine-tuning, leading to high data collection and training\ncosts. To address this, we propose ChartCards, a unified chart-metadata\ngeneration framework for multi-task chart understanding. ChartCards\nsystematically synthesizes various chart information, including data tables,\nvisualization code, visual elements, and multi-dimensional semantic captions.\nBy structuring this information into organized metadata, ChartCards enables a\nsingle chart to support multiple downstream tasks, such as text-to-chart\nretrieval, chart summarization, chart-to-table conversion, chart description,\nand chart question answering. Using ChartCards, we further construct MetaChart,\na large-scale high-quality dataset containing 10,862 data tables, 85K charts,\nand 170 K high-quality chart captions. We validate the dataset through\nqualitative crowdsourcing evaluations and quantitative fine-tuning experiments\nacross various chart understanding tasks. Fine-tuning six different models on\nMetaChart resulted in an average performance improvement of 5% across all\ntasks. The most notable improvements are seen in text-to-chart retrieval and\nchart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements\nof 17% and 28%, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86ChartCards\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u7ed3\u6784\u5316\u5143\u6570\u636e\u652f\u6301\u591a\u4efb\u52a1\u56fe\u8868\u7406\u89e3\uff0c\u5e76\u6784\u5efa\u4e86MetaChart\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4e2d\u7684\u5e94\u7528\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u4f46\u6570\u636e\u6536\u96c6\u548c\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86ChartCards\u6846\u67b6\u4ee5\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u5347\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86ChartCards\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u5408\u6210\u56fe\u8868\u4fe1\u606f\uff08\u5982\u6570\u636e\u8868\u3001\u53ef\u89c6\u5316\u4ee3\u7801\u3001\u89c6\u89c9\u5143\u7d20\u7b49\uff09\u5e76\u751f\u6210\u7edf\u4e00\u5143\u6570\u636e\uff0c\u652f\u6301\u591a\u4efb\u52a1\u56fe\u8868\u7406\u89e3\u3002\u8fd8\u6784\u5efa\u4e86MetaChart\u6570\u636e\u96c6\uff0c\u5305\u542b10,862\u5f20\u6570\u636e\u8868\u548c85K\u56fe\u8868\u3002", "result": "\u5728\u516d\u79cd\u6a21\u578b\u4e0a\u5fae\u8c03MetaChart\u6570\u636e\u96c6\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u53475%\uff0c\u5176\u4e2d\u6587\u672c\u5230\u56fe\u8868\u68c0\u7d22\u548c\u56fe\u8868\u5230\u8868\u683c\u4efb\u52a1\u7684\u63d0\u5347\u6700\u4e3a\u663e\u8457\uff0c\u5206\u522b\u8fbe\u523017%\u548c28%\u3002", "conclusion": "ChartCards\u6846\u67b6\u548cMetaChart\u6570\u636e\u96c6\u6709\u6548\u964d\u4f4e\u4e86\u6570\u636e\u6210\u672c\u5e76\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u56fe\u8868\u7406\u89e3\u7684\u6027\u80fd\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u56fe\u8868\u7406\u89e3\u3001\u5143\u6570\u636e\u751f\u6210\u3001\u6570\u636e\u96c6\u6784\u5efa\u3001\u4efb\u52a1\u591a\u6837\u6027"}}
{"id": "2505.14943", "pdf": "https://arxiv.org/pdf/2505.14943", "abs": "https://arxiv.org/abs/2505.14943", "authors": ["Ross Nordby"], "title": "Soft Prompts for Evaluation: Measuring Conditional Distance of Capabilities", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "To help evaluate and understand the latent capabilities of language models,\nthis paper introduces an approach using optimized input embeddings, or 'soft\nprompts,' as a metric of conditional distance between a model and a target\nbehavior. The technique aims to facilitate latent capability discovery as a\npart of automated red teaming/evaluation suites and to provide quantitative\nfeedback about the accessibility of potentially concerning behaviors in a way\nthat may scale to powerful future models, including those which may otherwise\nbe capable of deceptive alignment. An evaluation framework using soft prompts\nis demonstrated in natural language, chess, and pathfinding, and the technique\nis extended with generalized conditional soft prompts to aid in constructing\ntask evaluations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4f18\u5316\u8f93\u5165\u5d4c\u5165\uff08\u8f6f\u63d0\u793a\uff09\u6765\u8861\u91cf\u6a21\u578b\u4e0e\u76ee\u6807\u884c\u4e3a\u4e4b\u95f4\u6761\u4ef6\u8ddd\u79bb\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u80fd\u529b\u3002", "motivation": "\u65e8\u5728\u5e2e\u52a9\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u80fd\u529b\uff0c\u5e76\u4e3a\u81ea\u52a8\u5316\u7ea2\u961f\u8bc4\u4f30\u63d0\u4f9b\u5b9a\u91cf\u53cd\u9988\uff0c\u5c24\u5176\u662f\u5728\u672a\u6765\u53ef\u80fd\u5177\u6709\u6b3a\u9a97\u6027\u5bf9\u9f50\u7684\u5f3a\u5927\u6a21\u578b\u4e2d\u3002", "method": "\u91c7\u7528\u8f6f\u63d0\u793a\u4f5c\u4e3a\u6761\u4ef6\u8ddd\u79bb\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u5e76\u5728\u81ea\u7136\u8bed\u8a00\u3001\u56fd\u9645\u8c61\u68cb\u548c\u8def\u5f84\u89c4\u5212\u7b49\u573a\u666f\u4e2d\u9a8c\u8bc1\u5176\u8bc4\u4f30\u6846\u67b6\uff0c\u540c\u65f6\u6269\u5c55\u4e86\u901a\u7528\u6761\u4ef6\u8f6f\u63d0\u793a\u4ee5\u652f\u6301\u4efb\u52a1\u8bc4\u4f30\u7684\u6784\u5efa\u3002", "result": "\u5c55\u793a\u4e86\u8f6f\u63d0\u793a\u5728\u591a\u79cd\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u8bc4\u4f30\u6a21\u578b\u6f5c\u5728\u80fd\u529b\u65b9\u9762\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8f6f\u63d0\u793a\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u80fd\u591f\u91cf\u5316\u6a21\u578b\u6f5c\u5728\u884c\u4e3a\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u9002\u7528\u4e8e\u672a\u6765\u5f3a\u5927\u6a21\u578b\u7684\u6f5c\u5728\u98ce\u9669\u5206\u6790\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b,\u8f6f\u63d0\u793a,\u6f5c\u5728\u80fd\u529b\u8bc4\u4f30,\u81ea\u52a8\u5316\u7ea2\u961f,\u6761\u4ef6\u8ddd\u79bb"}}
{"id": "2505.15050", "pdf": "https://arxiv.org/pdf/2505.15050", "abs": "https://arxiv.org/abs/2505.15050", "authors": ["Gaurav Kumar", "Debajyoti Mazumder", "Ayush Garg", "Jasabanta Patro"], "title": "Improving the fact-checking performance of language models by relying on their entailment ability", "categories": ["cs.CL"], "comment": "44 pages", "summary": "Automated fact-checking is a crucial task in this digital age. To verify a\nclaim, current approaches majorly follow one of two strategies i.e. (i) relying\non embedded knowledge of language models, and (ii) fine-tuning them with\nevidence pieces. While the former can make systems to hallucinate, the later\nhave not been very successful till date. The primary reason behind this is that\nfact verification is a complex process. Language models have to parse through\nmultiple pieces of evidence before making a prediction. Further, the evidence\npieces often contradict each other. This makes the reasoning process even more\ncomplex. We proposed a simple yet effective approach where we relied on\nentailment and the generative ability of language models to produce\n''supporting'' and ''refuting'' justifications (for the truthfulness of a\nclaim). We trained language models based on these justifications and achieved\nsuperior results. Apart from that, we did a systematic comparison of different\nprompting and fine-tuning strategies, as it is currently lacking in the\nliterature. Some of our observations are: (i) training language models with raw\nevidence sentences registered an improvement up to 8.20% in macro-F1, over the\nbest performing baseline for the RAW-FC dataset, (ii) similarly, training\nlanguage models with prompted claim-evidence understanding (TBE-2) registered\nan improvement (with a margin up to 16.39%) over the baselines for the same\ndataset, (iii) training language models with entailed justifications (TBE-3)\noutperformed the baselines by a huge margin (up to 28.57% and 44.26% for\nLIAR-RAW and RAW-FC, respectively). We have shared our code repository to\nreproduce the results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u751f\u6210\u652f\u6301\u4e0e\u53cd\u9a73\u7406\u7531\u7684\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u65b9\u6cd5\u4f9d\u8d56\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u77e5\u8bc6\u6216\u8bc1\u636e\u5fae\u8c03\u7684\u4e0d\u8db3\uff0c\u540e\u8005\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u4ea7\u751f\u652f\u6301\u4e0e\u53cd\u9a73\u7406\u7531\uff0c\u5e76\u901a\u8fc7\u8fd9\u4e9b\u7406\u7531\u8bad\u7ec3\u6a21\u578b\uff0c\u540c\u65f6\u5bf9\u6bd4\u4e0d\u540c\u63d0\u793a\u4e0e\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u65b0\u65b9\u6cd5\u5728RAW-FC\u548cLIAR-RAW\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u5347\u4e8616.39%\u548c44.26%\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u4e3a\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5, \u8bed\u8a00\u6a21\u578b, \u8bc1\u636e\u63a8\u7406, \u5fae\u8c03\u7b56\u7565"}}
{"id": "2505.14945", "pdf": "https://arxiv.org/pdf/2505.14945", "abs": "https://arxiv.org/abs/2505.14945", "authors": ["O. Deniz Kose", "Gonzalo Mateos", "Yanning Shen"], "title": "Unlearning Algorithmic Biases over Graphs", "categories": ["cs.LG"], "comment": null, "summary": "The growing enforcement of the right to be forgotten regulations has\npropelled recent advances in certified (graph) unlearning strategies to comply\nwith data removal requests from deployed machine learning (ML) models.\nMotivated by the well-documented bias amplification predicament inherent to\ngraph data, here we take a fresh look at graph unlearning and leverage it as a\nbias mitigation tool. Given a pre-trained graph ML model, we develop a\ntraining-free unlearning procedure that offers certifiable bias mitigation via\na single-step Newton update on the model weights. This way, we contribute a\ncomputationally lightweight alternative to the prevalent training- and\noptimization-based fairness enhancement approaches, with quantifiable\nperformance guarantees. We first develop a novel fairness-aware nodal feature\nunlearning strategy along with refined certified unlearning bounds for this\nsetting, whose impact extends beyond the realm of graph unlearning. We then\ndesign structural unlearning methods endowed with principled selection\nmechanisms over nodes and edges informed by rigorous bias analyses. Unlearning\nthese judiciously selected elements can mitigate algorithmic biases with\nminimal impact on downstream utility (e.g., node classification accuracy).\nExperimental results over real networks corroborate the bias mitigation\nefficacy of our unlearning strategies, and delineate markedly favorable\nutility-complexity trade-offs relative to retraining from scratch using\naugmented graph data obtained via removals.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u6570\u636e\u53bb\u5b66\u4e60\u7684\u504f\u89c1\u7f13\u89e3\u5de5\u5177\uff0c\u901a\u8fc7\u725b\u987f\u66f4\u65b0\u6a21\u578b\u6743\u91cd\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u504f\u89c1\u6d88\u9664\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u91cf\u5316\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "motivation": "\u63a8\u52a8\u53bb\u5b66\u4e60\u7b56\u7565\u4ee5\u5e94\u5bf9\u6570\u636e\u5220\u9664\u8bf7\u6c42\uff0c\u5e76\u5229\u7528\u5176\u4f5c\u4e3a\u56fe\u6570\u636e\u504f\u89c1\u7f13\u89e3\u7684\u5de5\u5177\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u56fe\u53bb\u5b66\u4e60\u7a0b\u5e8f\uff0c\u901a\u8fc7\u5355\u6b65\u725b\u987f\u66f4\u65b0\u5b9e\u73b0\u504f\u89c1\u7f13\u89e3\uff0c\u5e76\u8bbe\u8ba1\u4e86\u516c\u5e73\u6027\u611f\u77e5\u7684\u8282\u70b9\u7279\u5f81\u53bb\u5b66\u4e60\u7b56\u7565\u548c\u7ed3\u6784\u53bb\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u4e0b\u6e38\u4efb\u52a1\u7684\u5b9e\u7528\u6027\uff08\u5982\u8282\u70b9\u5206\u7c7b\u51c6\u786e\u6027\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u504f\u89c1\u7f13\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u8f7b\u91cf\u7ea7\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u76f8\u6bd4\u4ece\u5934\u8bad\u7ec3\u5177\u6709\u66f4\u4f18\u7684\u6548\u7528-\u590d\u6742\u5ea6\u6743\u8861\u3002", "keywords": "\u56fe\u53bb\u5b66\u4e60\uff0c\u504f\u89c1\u7f13\u89e3\uff0c\u725b\u987f\u66f4\u65b0\uff0c\u516c\u5e73\u6027\uff0c\u8f7b\u91cf\u7ea7"}}
{"id": "2505.15054", "pdf": "https://arxiv.org/pdf/2505.15054", "abs": "https://arxiv.org/abs/2505.15054", "authors": ["Feiyang Cai", "Jiahui Bai", "Tao Tang", "Joshua Luo", "Tianyu Zhu", "Ling Liu", "Feng Luo"], "title": "MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.BM"], "comment": null, "summary": "Precise recognition, editing, and generation of molecules are essential\nprerequisites for both chemists and AI systems tackling various chemical tasks.\nWe present MolLangBench, a comprehensive benchmark designed to evaluate\nfundamental molecule-language interface tasks: language-prompted molecular\nstructure recognition, editing, and generation. To ensure high-quality,\nunambiguous, and deterministic outputs, we construct the recognition tasks\nusing automated cheminformatics tools, and curate editing and generation tasks\nthrough rigorous expert annotation and validation. MolLangBench supports the\nevaluation of models that interface language with different molecular\nrepresentations, including linear strings, molecular images, and molecular\ngraphs. Evaluations of state-of-the-art models reveal significant limitations:\nthe strongest model (o3) achieves $79.2\\%$ and $78.5\\%$ accuracy on recognition\nand editing tasks, which are intuitively simple for humans, and performs even\nworse on the generation task, reaching only $29.0\\%$ accuracy. These results\nhighlight the shortcomings of current AI systems in handling even preliminary\nmolecular recognition and manipulation tasks. We hope MolLangBench will\ncatalyze further research toward more effective and reliable AI systems for\nchemical applications.", "AI": {"tldr": "MolLangBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5206\u5b50-\u8bed\u8a00\u754c\u9762\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u5305\u62ec\u5206\u5b50\u7ed3\u6784\u8bc6\u522b\u3001\u7f16\u8f91\u548c\u751f\u6210\u3002\u5c3d\u7ba1\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u7684\u8868\u73b0\u5728\u4eba\u7c7b\u770b\u6765\u4ecd\u663e\u4e0d\u8db3\uff0c\u4f46\u8be5\u57fa\u51c6\u5e0c\u671b\u63a8\u52a8\u66f4\u6709\u6548\u4e14\u53ef\u9760\u7684\u5316\u5b66AI\u7cfb\u7edf\u7814\u7a76\u3002", "motivation": "\u7531\u4e8e\u5206\u5b50\u8bc6\u522b\u3001\u7f16\u8f91\u548c\u751f\u6210\u5bf9\u5316\u5b66\u5bb6\u548cAI\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f53\u524dAI\u7cfb\u7edf\u5728\u6b64\u7c7b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u9ad8\u8d28\u91cf\u57fa\u51c6\u6765\u63a8\u52a8\u7814\u7a76\u3002", "method": "MolLangBench\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u5177\u548c\u4e13\u5bb6\u6807\u6ce8\u6784\u5efa\u4efb\u52a1\uff0c\u8bc4\u4f30\u8bed\u8a00\u4e0e\u5206\u5b50\u8868\u793a\uff08\u5982\u5b57\u7b26\u4e32\u3001\u56fe\u50cf\u3001\u56fe\uff09\u7684\u4ea4\u4e92\u80fd\u529b\u3002", "result": "\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u8bc6\u522b\u548c\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u5c1a\u53ef\uff08\u7ea679%\uff09\uff0c\u4f46\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u4ec5\u4e3a29%\uff0c\u663e\u793aAI\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002", "conclusion": "MolLangBench\u63ed\u793a\u4e86\u5f53\u524dAI\u7cfb\u7edf\u7684\u4e0d\u8db3\uff0c\u5e76\u6709\u671b\u4fc3\u8fdb\u5316\u5b66\u5e94\u7528\u9886\u57df\u66f4\u6709\u6548\u7684AI\u7814\u7a76\u3002", "keywords": "MolLangBench, \u5206\u5b50\u8bc6\u522b, \u5206\u5b50\u7f16\u8f91, \u5206\u5b50\u751f\u6210, AI\u8bc4\u4f30, \u5316\u5b66\u4efb\u52a1"}}
{"id": "2505.14959", "pdf": "https://arxiv.org/pdf/2505.14959", "abs": "https://arxiv.org/abs/2505.14959", "authors": ["Kungang Li", "Xiangyi Chen", "Ling Leng", "Jiajing Xu", "Jiankai Sun", "Behnam Rezaei"], "title": "Privacy Preserving Conversion Modeling in Data Clean Room", "categories": ["cs.LG", "cs.IR", "H.4"], "comment": "Published in Proceedings of the 18th ACM Conference on Recommender\n  Systems. 2024 (RecSys '24)", "summary": "In the realm of online advertising, accurately predicting the conversion rate\n(CVR) is crucial for enhancing advertising efficiency and user satisfaction.\nThis paper addresses the challenge of CVR prediction while adhering to user\nprivacy preferences and advertiser requirements. Traditional methods face\nobstacles such as the reluctance of advertisers to share sensitive conversion\ndata and the limitations of model training in secure environments like data\nclean rooms. We propose a novel model training framework that enables\ncollaborative model training without sharing sample-level gradients with the\nadvertising platform. Our approach introduces several innovative components:\n(1) utilizing batch-level aggregated gradients instead of sample-level\ngradients to minimize privacy risks; (2) applying adapter-based\nparameter-efficient fine-tuning and gradient compression to reduce\ncommunication costs; and (3) employing de-biasing techniques to train the model\nunder label differential privacy, thereby maintaining accuracy despite\nprivacy-enhanced label perturbations. Our experimental results, conducted on\nindustrial datasets, demonstrate that our method achieves competitive ROCAUC\nperformance while significantly decreasing communication overhead and complying\nwith both advertiser privacy requirements and user privacy choices. This\nframework establishes a new standard for privacy-preserving, high-performance\nCVR prediction in the digital advertising landscape.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9690\u79c1\u4fdd\u62a4CVR\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6279\u91cf\u7ea7\u68af\u5ea6\u805a\u5408\u3001\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u548c\u53bb\u504f\u6280\u672f\uff0c\u5728\u6ee1\u8db3\u9690\u79c1\u9700\u6c42\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5728\u7ebf\u5e7f\u544a\u4e2d\uff0cCVR\u9884\u6d4b\u5bf9\u5e7f\u544a\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u5e7f\u544a\u4e3b\u4e0d\u613f\u5206\u4eab\u654f\u611f\u6570\u636e\u548c\u5b89\u5168\u73af\u5883\u9650\u5236\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6279\u91cf\u7ea7\u68af\u5ea6\u800c\u975e\u6837\u672c\u7ea7\u68af\u5ea6\uff0c\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u548c\u68af\u5ea6\u538b\u7f29\uff0c\u540c\u65f6\u91c7\u7528\u53bb\u504f\u6280\u672f\u5904\u7406\u5dee\u5206\u9690\u79c1\u6807\u7b7e\u3002", "result": "\u5728\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728ROCAUC\u6027\u80fd\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6570\u5b57\u5e7f\u544a\u9886\u57df\u7684\u9ad8\u6027\u80fd\u3001\u9690\u79c1\u4fdd\u62a4CVR\u9884\u6d4b\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002", "keywords": "CVR\u9884\u6d4b, \u9690\u79c1\u4fdd\u62a4, \u6279\u91cf\u7ea7\u68af\u5ea6, \u53c2\u6570\u9ad8\u6548\u5fae\u8c03, \u5dee\u5206\u9690\u79c1"}}
{"id": "2505.15055", "pdf": "https://arxiv.org/pdf/2505.15055", "abs": "https://arxiv.org/abs/2505.15055", "authors": ["Hongli Zhou", "Hui Huang", "Ziqing Zhao", "Lvyuan Han", "Huicheng Wang", "Kehai Chen", "Muyun Yang", "Wei Bao", "Jian Dong", "Bing Xu", "Conghui Zhu", "Hailong Cao", "Tiejun Zhao"], "title": "Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory", "categories": ["cs.CL"], "comment": null, "summary": "The evaluation of large language models (LLMs) via benchmarks is widespread,\nyet inconsistencies between different leaderboards and poor separability among\ntop models raise concerns about their ability to accurately reflect authentic\nmodel capabilities. This paper provides a critical analysis of benchmark\neffectiveness, examining main-stream prominent LLM benchmarks using results\nfrom diverse models. We first propose a new framework for accurate and reliable\nestimations of item characteristics and model abilities. Specifically, we\npropose Pseudo-Siamese Network for Item Response Theory (PSN-IRT), an enhanced\nItem Response Theory framework that incorporates a rich set of item parameters\nwithin an IRT-grounded architecture. Based on PSN-IRT, we conduct extensive\nanalysis which reveals significant and varied shortcomings in the measurement\nquality of current benchmarks. Furthermore, we demonstrate that leveraging\nPSN-IRT is able to construct smaller benchmarks while maintaining stronger\nalignment with human preference.", "AI": {"tldr": "\u5bf9\u4e3b\u6d41\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u57fa\u51c6\u6d4b\u8bd5\u7684\u6709\u6548\u6027\u8fdb\u884c\u6279\u5224\u6027\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u6846\u67b6PSN-IRT\u4ee5\u6539\u8fdb\u57fa\u51c6\u6d4b\u8bd5\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u548c\u533a\u5206\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u80fd\u529b\u3002", "method": "\u63d0\u51faPseudo-Siamese Network for Item Response Theory (PSN-IRT)\u6846\u67b6\uff0c\u901a\u8fc7\u4e30\u5bcc\u9879\u76ee\u53c2\u6570\u589e\u5f3aIRT\u67b6\u6784\uff0c\u8fdb\u800c\u5206\u6790\u57fa\u51c6\u6d4b\u8bd5\u7684\u6d4b\u91cf\u8d28\u91cf\u3002", "result": "PSN-IRT\u63ed\u793a\u4e86\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u7684\u663e\u8457\u7f3a\u9677\uff0c\u5e76\u80fd\u6784\u5efa\u66f4\u5c0f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u540c\u65f6\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002", "conclusion": "PSN-IRT\u4e3a\u6539\u8fdb\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u51c6\u786e\u548c\u53ef\u9760\u7684\u65b9\u6cd5\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u57fa\u51c6\u6d4b\u8bd5, IRT, \u6d4b\u91cf\u8d28\u91cf, \u4eba\u7c7b\u504f\u597d"}}
{"id": "2505.14964", "pdf": "https://arxiv.org/pdf/2505.14964", "abs": "https://arxiv.org/abs/2505.14964", "authors": ["Dave Cook", "Tim Klawa"], "title": "The Achilles Heel of AI: Fundamentals of Risk-Aware Training Data for High-Consequence Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "AI systems in high-consequence domains such as defense, intelligence, and\ndisaster response must detect rare, high-impact events while operating under\ntight resource constraints. Traditional annotation strategies that prioritize\nlabel volume over informational value introduce redundancy and noise, limiting\nmodel generalization. This paper introduces smart-sizing, a training data\nstrategy that emphasizes label diversity, model-guided selection, and marginal\nutility-based stopping. We implement this through Adaptive Label Optimization\n(ALO), combining pre-labeling triage, annotator disagreement analysis, and\niterative feedback to prioritize labels that meaningfully improve model\nperformance. Experiments show that models trained on 20 to 40 percent of\ncurated data can match or exceed full-data baselines, particularly in\nrare-class recall and edge-case generalization. We also demonstrate how latent\nlabeling errors embedded in training and validation sets can distort\nevaluation, underscoring the need for embedded audit tools and\nperformance-aware governance. Smart-sizing reframes annotation as a\nfeedback-driven process aligned with mission outcomes, enabling more robust\nmodels with fewer labels and supporting efficient AI development pipelines for\nfrontier models and operational systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3asmart-sizing\u7684\u6570\u636e\u6807\u6ce8\u7b56\u7565\uff0c\u901a\u8fc7\u5f3a\u8c03\u6807\u7b7e\u591a\u6837\u6027\u3001\u6a21\u578b\u5f15\u5bfc\u9009\u62e9\u548c\u8fb9\u9645\u6548\u7528\u505c\u6b62\uff0c\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u9886\u57df\u4e2d\uff0c\u4f20\u7edf\u7684\u6807\u6ce8\u7b56\u7565\u56e0\u5197\u4f59\u548c\u566a\u58f0\u95ee\u9898\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6570\u636e\u6807\u6ce8\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7Adaptive Label Optimization (ALO)\uff0c\u7ed3\u5408\u9884\u6807\u6ce8\u5206\u7c7b\u3001\u6807\u6ce8\u8005\u5206\u6b67\u5206\u6790\u548c\u8fed\u4ee3\u53cd\u9988\uff0c\u4f18\u5148\u9009\u62e9\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u6807\u7b7e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u4f7f\u752820%\u81f340%\u7684\u7cbe\u9009\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5728\u7a00\u6709\u7c7b\u522b\u53ec\u56de\u548c\u8fb9\u7f18\u6848\u4f8b\u6cdb\u5316\u65b9\u9762\uff0c\u8868\u73b0\u53ef\u4e0e\u5168\u91cf\u6570\u636e\u57fa\u51c6\u76f8\u5ab2\u7f8e\u6216\u66f4\u4f18\u3002", "conclusion": "\u667a\u80fd\u6807\u6ce8\u7b56\u7565\u5c06\u6807\u6ce8\u89c6\u4e3a\u53cd\u9988\u9a71\u52a8\u7684\u8fc7\u7a0b\uff0c\u652f\u6301\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6a21\u578b\uff0c\u540c\u65f6\u51cf\u5c11\u6807\u7b7e\u9700\u6c42\uff0c\u63d0\u5347AI\u5f00\u53d1\u6548\u7387\u3002", "keywords": "\u667a\u80fd\u6807\u6ce8\uff0c\u6807\u7b7e\u4f18\u5316\uff0c\u6a21\u578b\u6027\u80fd\uff0c\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u6570\u636e\u6548\u7387"}}
{"id": "2505.15062", "pdf": "https://arxiv.org/pdf/2505.15062", "abs": "https://arxiv.org/abs/2505.15062", "authors": ["Jiashu He", "Jinxuan Fan", "Bowen Jiang", "Ignacio Houine", "Dan Roth", "Alejandro Ribeiro"], "title": "Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "When addressing complex questions that require new information, people often\nassociate the question with existing knowledge to derive a sensible answer. For\ninstance, when evaluating whether melatonin aids insomnia, one might associate\n\"hormones helping mental disorders\" with \"melatonin being a hormone and\ninsomnia a mental disorder\" to complete the reasoning. Large Language Models\n(LLMs) also require such associative thinking, particularly in resolving\nscientific inquiries when retrieved knowledge is insufficient and does not\ndirectly answer the question. Graph Inspired Veracity Extrapolation (GIVE)\naddresses this by using a knowledge graph (KG) to extrapolate structured\nknowledge. However, it involves the construction and pruning of many\nhypothetical triplets, which limits efficiency and generalizability. We propose\nSelf-GIVE, a retrieve-RL framework that enhances LLMs with automatic\nassociative thinking through reinforcement learning. Self-GIVE extracts\nstructured information and entity sets to assist the model in linking to the\nqueried concepts. We address GIVE's key limitations: (1) extensive LLM calls\nand token overhead for knowledge extrapolation, (2) difficulty in deploying on\nsmaller LLMs (3B or 7B) due to complex instructions, and (3) inaccurate\nknowledge from LLM pruning. Specifically, after fine-tuning using self-GIVE\nwith a 135 node UMLS KG, it improves the performance of the Qwen2.5 3B and 7B\nmodels by up to $\\textbf{28.5%$\\rightarrow$71.4%}$ and\n$\\textbf{78.6$\\rightarrow$90.5%}$ in samples $\\textbf{unseen}$ in challenging\nbiomedical QA tasks. In particular, Self-GIVE allows the 7B model to match or\noutperform GPT3.5 turbo with GIVE, while cutting token usage by over 90\\%.\nSelf-GIVE enhances the scalable integration of structured retrieval and\nreasoning with associative thinking.", "AI": {"tldr": "Self-GIVE\u662f\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22-\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5173\u8054\u601d\u7ef4\u589e\u5f3aLLM\uff0c\u89e3\u51b3\u4e86GIVE\u5728\u6548\u7387\u3001\u901a\u7528\u6027\u548c\u77e5\u8bc6\u51c6\u786e\u6027\u65b9\u9762\u7684\u9650\u5236\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709GIVE\u65b9\u6cd5\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u56e0\u77e5\u8bc6\u56fe\u6784\u5efa\u548c\u4fee\u526a\u6548\u7387\u4f4e\u4e0b\u3001\u901a\u7528\u6027\u4e0d\u8db3\u4ee5\u53ca\u5c0f\u89c4\u6a21\u6a21\u578b\u90e8\u7f72\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSelf-GIVE\u6846\u67b6\uff0c\u7ed3\u5408\u68c0\u7d22\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u81ea\u52a8\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\u4ee5\u8f85\u52a9\u63a8\u7406\uff0c\u51cf\u5c11LLM\u8c03\u7528\u548c\u4ee4\u724c\u5f00\u9500\u3002", "result": "\u57283B\u548c7B\u6a21\u578b\u4e0a\u6027\u80fd\u63d0\u5347\u663e\u8457\uff08\u6700\u9ad828.5%\u219271.4%\u548c78.6\u219290.5%\uff09\uff0c\u4e147B\u6a21\u578b\u8868\u73b0\u5ab2\u7f8eGPT3.5\uff0c\u540c\u65f6\u4ee4\u724c\u4f7f\u7528\u51cf\u5c1190%\u4ee5\u4e0a\u3002", "conclusion": "Self-GIVE\u6709\u6548\u63d0\u5347\u4e86\u7ed3\u6784\u5316\u68c0\u7d22\u4e0e\u5173\u8054\u63a8\u7406\u7684\u53ef\u6269\u5c55\u6027\u3002", "keywords": "Self-GIVE, LLM, \u77e5\u8bc6\u56fe\u8c31, \u5f3a\u5316\u5b66\u4e60, \u751f\u7269\u533b\u5b66QA"}}
{"id": "2505.14967", "pdf": "https://arxiv.org/pdf/2505.14967", "abs": "https://arxiv.org/abs/2505.14967", "authors": ["Fangzhen Zhao", "Chenyi Zhang", "Naipeng Dong", "Ming Li", "Jinxiao Shan"], "title": "Anomaly Detection Based on Critical Paths for Deep Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages in ACM journal latex format", "summary": "Deep neural networks (DNNs) are notoriously hard to understand and difficult\nto defend. Extracting representative paths (including the neuron activation\nvalues and the connections between neurons) from DNNs using software\nengineering approaches has recently shown to be a promising approach in\ninterpreting the decision making process of blackbox DNNs, as the extracted\npaths are often effective in capturing essential features. With this in mind,\nthis work investigates a novel approach that extracts critical paths from DNNs\nand subsequently applies the extracted paths for the anomaly detection task,\nbased on the observation that outliers and adversarial inputs do not usually\ninduce the same activation pattern on those paths as normal (in-distribution)\ninputs.\n  In our approach, we first identify critical detection paths via genetic\nevolution and mutation. Since different paths in a DNN often capture different\nfeatures for the same target class, we ensemble detection results from multiple\npaths by integrating random subspace sampling and a voting mechanism. Compared\nwith state-of-the-art methods, our experimental results suggest that our method\nnot only outperforms them, but it is also suitable for the detection of a broad\nrange of anomaly types with high accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u63d0\u53d6\u5173\u952e\u8def\u5f84\u5e76\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9057\u4f20\u8fdb\u5316\u548c\u53d8\u5f02\u8bc6\u522b\u8def\u5f84\uff0c\u5e76\u7ed3\u5408\u968f\u673a\u5b50\u7a7a\u95f4\u91c7\u6837\u548c\u6295\u7968\u673a\u5236\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u96be\u4ee5\u7406\u89e3\u548c\u9632\u5fa1\uff0c\u63d0\u53d6\u4ee3\u8868\u6027\u8def\u5f84\u80fd\u6709\u6548\u89e3\u91ca\u5176\u51b3\u7b56\u8fc7\u7a0b\u3002\u8bba\u6587\u57fa\u4e8e\u5f02\u5e38\u8f93\u5165\u4e0d\u4f1a\u4e0e\u6b63\u5e38\u8f93\u5165\u6fc0\u6d3b\u76f8\u540c\u8def\u5f84\u7684\u89c2\u5bdf\uff0c\u63a2\u7d22\u5173\u952e\u8def\u5f84\u63d0\u53d6\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\u3002", "method": "\u901a\u8fc7\u9057\u4f20\u8fdb\u5316\u548c\u53d8\u5f02\u8bc6\u522b\u5173\u952e\u68c0\u6d4b\u8def\u5f84\uff0c\u5e76\u96c6\u6210\u591a\u6761\u8def\u5f84\u7684\u68c0\u6d4b\u7ed3\u679c\uff08\u7ed3\u5408\u968f\u673a\u5b50\u7a7a\u95f4\u91c7\u6837\u548c\u6295\u7968\u673a\u5236\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5f02\u5e38\u7c7b\u578b\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u63d0\u53d6\u5173\u952e\u8def\u5f84\u5e76\u7ed3\u5408\u591a\u8def\u5f84\u96c6\u6210\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347DNN\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9002\u7528\u6027\u3002", "keywords": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc, \u5173\u952e\u8def\u5f84, \u5f02\u5e38\u68c0\u6d4b, \u9057\u4f20\u8fdb\u5316, \u968f\u673a\u5b50\u7a7a\u95f4\u91c7\u6837"}}
{"id": "2505.14744", "pdf": "https://arxiv.org/pdf/2505.14744", "abs": "https://arxiv.org/abs/2505.14744", "authors": ["Janis Zenkner", "Tobias Sesterhenn", "Christian Bartelt"], "title": "Transductively Informed Inductive Program Synthesis", "categories": ["cs.PL", "cs.AI", "cs.LG"], "comment": null, "summary": "Abstraction and reasoning in program synthesis has seen significant progress\nthrough both inductive and transductive paradigms. Inductive approaches\ngenerate a program or latent function from input-output examples, which can\nthen be applied to new inputs. Transductive approaches directly predict output\nvalues for given inputs, effectively serving as the function themselves.\nCurrent approaches combine inductive and transductive models via isolated\nensembling, but they do not explicitly model the interaction between both\nparadigms. In this work, we introduce \\acs{tiips}, a novel framework that\nunifies transductive and inductive strategies by explicitly modeling their\ninteractions through a cooperative mechanism: an inductive model generates\nprograms, while a transductive model constrains, guides, and refines the search\nto improve synthesis accuracy and generalization. We evaluate \\acs{tiips} on\ntwo widely studied program synthesis domains: string and list manipulation. Our\nresults show that \\acs{tiips} solves more tasks and yields functions that more\nclosely match optimal solutions in syntax and semantics, particularly in\nout-of-distribution settings, yielding state-of-the-art performance. We believe\nthat explicitly modeling the synergy between inductive and transductive\nreasoning opens promising avenues for general-purpose program synthesis and\nbroader applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTIIPS\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5f52\u7eb3\u548c\u8f6c\u5bfc\u7b56\u7565\u7684\u534f\u540c\u4f5c\u7528\uff0c\u63d0\u5347\u7a0b\u5e8f\u5408\u6210\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u76ee\u524d\u7684\u65b9\u6cd5\u5c06\u5f52\u7eb3\u548c\u8f6c\u5bfc\u6a21\u578b\u5b64\u7acb\u5730\u96c6\u6210\uff0c\u672a\u663e\u5f0f\u5efa\u6a21\u5176\u4ea4\u4e92\u3002", "method": "\u5f15\u5165TIIPS\u6846\u67b6\uff0c\u901a\u8fc7\u4e00\u4e2a\u534f\u540c\u673a\u5236\u663e\u5f0f\u5efa\u6a21\u5f52\u7eb3\u548c\u8f6c\u5bfc\u7684\u4ea4\u4e92\uff1a\u5f52\u7eb3\u6a21\u578b\u751f\u6210\u7a0b\u5e8f\uff0c\u8f6c\u5bfc\u6a21\u578b\u7ea6\u675f\u3001\u6307\u5bfc\u548c\u4f18\u5316\u641c\u7d22\u3002", "result": "\u5728\u5b57\u7b26\u4e32\u548c\u5217\u8868\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cTIIPS\u89e3\u51b3\u4e86\u66f4\u591a\u4efb\u52a1\uff0c\u751f\u6210\u7684\u51fd\u6570\u5728\u8bed\u6cd5\u548c\u8bed\u4e49\u4e0a\u66f4\u63a5\u8fd1\u6700\u4f18\u89e3\uff0c\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u5f52\u7eb3\u548c\u8f6c\u5bfc\u7684\u534f\u540c\u4f5c\u7528\u4e3a\u901a\u7528\u7a0b\u5e8f\u5408\u6210\u548c\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u524d\u666f\u3002", "keywords": "\u7a0b\u5e8f\u5408\u6210,\u5f52\u7eb3\u63a8\u7406,\u8f6c\u5bfc\u63a8\u7406,\u534f\u540c\u673a\u5236"}}
{"id": "2505.15063", "pdf": "https://arxiv.org/pdf/2505.15063", "abs": "https://arxiv.org/abs/2505.15063", "authors": ["Sarfraz Ahmad", "Hasan Iqbal", "Momina Ahsan", "Numaan Naeem", "Muhammad Ahsan Riaz Khan", "Arham Riaz", "Muhammad Arslan Manzoor", "Yuxia Wang", "Preslav Nakov"], "title": "UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking", "categories": ["cs.CL", "I.2.7"], "comment": "16 pages, 10 figures, 4 tables, Submitted to ARR May 2025", "summary": "The rapid use of large language models (LLMs) has raised critical concerns\nregarding the factual reliability of their outputs, especially in low-resource\nlanguages such as Urdu. Existing automated fact-checking solutions\noverwhelmingly focus on English, leaving a significant gap for the 200+ million\nUrdu speakers worldwide. In this work, we introduce UrduFactCheck, the first\ncomprehensive, modular fact-checking framework specifically tailored for Urdu.\nOur system features a dynamic, multi-strategy evidence retrieval pipeline that\ncombines monolingual and translation-based approaches to address the scarcity\nof high-quality Urdu evidence. We curate and release two new hand-annotated\nbenchmarks: UrduFactBench for claim verification and UrduFactQA for evaluating\nLLM factuality. Extensive experiments demonstrate that UrduFactCheck,\nparticularly its translation-augmented variants, consistently outperforms\nbaselines and open-source alternatives on multiple metrics. We further\nbenchmark twelve state-of-the-art (SOTA) LLMs on factual question answering in\nUrdu, highlighting persistent gaps between proprietary and open-source models.\nUrduFactCheck's code and datasets are open-sourced and publicly available at\nhttps://github.com/mbzuai-nlp/UrduFactCheck.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u4e4c\u5c14\u90fd\u8bed\u7684\u4e8b\u5b9e\u6838\u67e5\u6846\u67b6UrduFactCheck\uff0c\u586b\u8865\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e8b\u5b9e\u6838\u67e5\u7684\u7a7a\u767d\uff0c\u5e76\u5728\u591a\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e4c\u5c14\u90fd\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u8f93\u51fa\u7684\u4e8b\u5b9e\u53ef\u9760\u6027\u95ee\u9898\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u5de5\u5177\u4e3b\u8981\u96c6\u4e2d\u4e8e\u82f1\u8bed\uff0c\u7f3a\u4e4f\u9488\u5bf9\u4e4c\u5c14\u90fd\u8bed\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u4e8b\u5b9e\u6838\u67e5\u6846\u67b6UrduFactCheck\uff0c\u91c7\u7528\u52a8\u6001\u591a\u7b56\u7565\u8bc1\u636e\u68c0\u7d22\u7ba1\u9053\uff0c\u7ed3\u5408\u5355\u8bed\u548c\u57fa\u4e8e\u7ffb\u8bd1\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4e4c\u5c14\u90fd\u8bed\u9ad8\u8d28\u91cf\u8bc1\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUrduFactCheck\uff08\u5c24\u5176\u662f\u5176\u7ffb\u8bd1\u589e\u5f3a\u53d8\u4f53\uff09\u5728\u591a\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u548c\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\u3002\u540c\u65f6\uff0c\u5bf912\u79cdSOTA LLMs\u5728\u4e4c\u5c14\u90fd\u8bed\u4e8b\u5b9e\u95ee\u7b54\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "UrduFactCheck\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4e8b\u5b9e\u6838\u67e5\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u7a81\u51fa\u4e86\u4e13\u6709\u548c\u5f00\u6e90LLMs\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002", "keywords": "\u4e8b\u5b9e\u6838\u67e5, \u4e4c\u5c14\u90fd\u8bed, \u4f4e\u8d44\u6e90\u8bed\u8a00, \u5927\u8bed\u8a00\u6a21\u578b, \u8bc1\u636e\u68c0\u7d22"}}
{"id": "2505.14969", "pdf": "https://arxiv.org/pdf/2505.14969", "abs": "https://arxiv.org/abs/2505.14969", "authors": ["Yangchao Wu", "Zongyue Qin", "Alex Wong", "Stefano Soatto"], "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Speculative decoding is a technique to leverage hardware concurrency to\nimprove the efficiency of large-scale autoregressive (AR) Transformer models by\nenabling multiple steps of token generation in a single forward pass.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead to\ncurrent SSM state update implementations. With the algorithm, we describe a\nhardware-aware implementation that improves naive application of AR Transformer\ntree-based speculative decoding methods to SSMs. Furthermore, we outperform\nvanilla speculative decoding with SSMs even with a baseline drafting model and\ntree structure on three different benchmarks, opening up opportunities for\nfurther speed up with SSM and hybrid model inference. Code will be released\nupon paper acceptance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6811\u72b6\u63a8\u6d4b\u89e3\u7801\u7b97\u6cd5\uff0c\u7528\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u548c\u6df7\u5408\u67b6\u6784\uff0c\u63d0\u5347\u4e86\u6548\u7387\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1SSM\u5df2\u6bd4\u81ea\u56de\u5f52Transformer\u66f4\u9ad8\u6548\uff0c\u4f46\u5176\u72b6\u6001\u53ef\u80fd\u5305\u542b\u6570\u5343\u4e2a\u6807\u8bb0\uff0c\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u6811\u72b6\u9a8c\u8bc1\u3002", "method": "\u5229\u7528\u79ef\u7d2f\u7684\u72b6\u6001\u8f6c\u79fb\u77e9\u9635\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u6811\u72b6\u63a8\u6d4b\u89e3\u7801\uff0c\u5e76\u63d0\u51fa\u786c\u4ef6\u611f\u77e5\u5b9e\u73b0\u65b9\u6848\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u666e\u901a\u63a8\u6d4b\u89e3\u7801\uff0c\u5e76\u5c55\u793a\u4e86SSM\u548c\u6df7\u5408\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u52a0\u901f\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3aSSM\u548c\u6df7\u5408\u6a21\u578b\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u4ecd\u6709\u4f18\u5316\u7a7a\u95f4\u3002", "keywords": "\u63a8\u6d4b\u89e3\u7801, \u72b6\u6001\u7a7a\u95f4\u6a21\u578b, \u6df7\u5408\u67b6\u6784, \u6811\u72b6\u9a8c\u8bc1, \u786c\u4ef6\u611f\u77e5"}}
{"id": "2505.15065", "pdf": "https://arxiv.org/pdf/2505.15065", "abs": "https://arxiv.org/abs/2505.15065", "authors": ["Suhas BN", "Yash Mahajan", "Dominik Mattioli", "Andrew M. Sherrill", "Rosa I. Arriaga", "Chris W. Wiese", "Saeed Abdullah"], "title": "The Pursuit of Empathy: Evaluating Small Language Models for PTSD Dialogue Support", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50, 68T05", "I.2.7; I.2.1; H.5.2"], "comment": "23 pages, 3 figures", "summary": "Can small language models with 0.5B to 5B parameters meaningfully engage in\ntrauma-informed, empathetic dialogue for individuals with PTSD? We address this\nquestion by introducing TIDE, a dataset of 10,000 two-turn dialogues spanning\n500 diverse PTSD client personas and grounded in a three-factor empathy model:\nemotion recognition, distress normalization, and supportive reflection. All\nscenarios and reference responses were reviewed for realism and trauma\nsensitivity by a clinical psychologist specializing in PTSD. We evaluate eight\nsmall language models before and after fine-tuning, comparing their outputs to\na frontier model (Claude Sonnet 3.5). Our IRB-approved human evaluation and\nautomatic metrics show that fine-tuning generally improves perceived empathy,\nbut gains are highly scenario- and user-dependent, with smaller models facing\nan empathy ceiling. Demographic analysis shows older adults value distress\nvalidation and graduate-educated users prefer nuanced replies, while gender\neffects are minimal. We highlight the limitations of automatic metrics and the\nneed for context- and user-aware system design. Our findings, along with the\nplanned release of TIDE, provide a foundation for building safe,\nresource-efficient, and ethically sound empathetic AI to supplement, not\nreplace, clinical mental health care.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff080.5B\u81f35B\u53c2\u6570\uff09\u53ef\u901a\u8fc7\u5fae\u8c03\u5728\u521b\u4f24\u77e5\u60c5\u3001\u5171\u60c5\u5bf9\u8bdd\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u6548\u679c\u53d7\u573a\u666f\u548c\u7528\u6237\u5f71\u54cd\uff0c\u5b58\u5728\u5171\u60c5\u4e0a\u9650\u3002", "motivation": "\u63a2\u8ba8\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u5728PTSD\u60a3\u8005\u7684\u5171\u60c5\u5bf9\u8bdd\u4e2d\u53d1\u6325\u4f5c\u7528\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u5bf9\u8bdd\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528TIDE\u6570\u636e\u96c6\uff0810,000\u6761\u5bf9\u8bdd\uff09\u5fae\u8c038\u4e2a\u5c0f\u578b\u6a21\u578b\uff0c\u5e76\u4e0e\u524d\u6cbf\u6a21\u578b\uff08Claude Sonnet 3.5\uff09\u5bf9\u6bd4\uff0c\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u548c\u81ea\u52a8\u6307\u6807\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5fae\u8c03\u901a\u5e38\u80fd\u63d0\u5347\u5171\u60c5\u6548\u679c\uff0c\u4f46\u6548\u679c\u56e0\u573a\u666f\u548c\u7528\u6237\u800c\u5f02\uff1b\u5c0f\u578b\u6a21\u578b\u5b58\u5728\u5171\u60c5\u4e0a\u9650\uff0c\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u5bf9\u5171\u60c5\u8868\u8fbe\u6709\u4e0d\u540c\u504f\u597d\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u5b89\u5168\u3001\u9ad8\u6548\u4e14\u7b26\u5408\u4f26\u7406\u7684\u5171\u60c5AI\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4f46\u4ecd\u9700\u4e0a\u4e0b\u6587\u548c\u7528\u6237\u611f\u77e5\u7684\u7cfb\u7edf\u8bbe\u8ba1\u3002", "keywords": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u3001PTSD\u3001\u5171\u60c5\u5bf9\u8bdd\u3001TIDE\u6570\u636e\u96c6\u3001\u5fae\u8c03"}}
{"id": "2505.14975", "pdf": "https://arxiv.org/pdf/2505.14975", "abs": "https://arxiv.org/abs/2505.14975", "authors": ["John L. Zhou", "Jonathan C. Kao"], "title": "Flattening Hierarchies with Policy Bootstrapping", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u5206\u5c42\u7684\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b50\u76ee\u6807\u6761\u4ef6\u7b56\u7565\u548c\u91cd\u8981\u6027\u91c7\u6837\u8fdb\u884c\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u5956\u52b1\u548c\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6311\u6218\u3002", "motivation": "\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\uff08GCRL\uff09\u5728\u65e0\u5956\u52b1\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u901a\u7528\u7b56\u7565\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u56e0\u7a00\u758f\u5956\u52b1\u548c\u6298\u6263\u95ee\u9898\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u5206\u5c42\u65b9\u6cd5\u53c8\u5e26\u6765\u590d\u6742\u6027\u548c\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u975e\u5206\u5c42\u7684\u76ee\u6807\u6761\u4ef6\u7b56\u7565\u8bad\u7ec3\u7b97\u6cd5\uff0c\u5229\u7528\u5b50\u76ee\u6807\u6761\u4ef6\u7b56\u7565\u548c\u4f18\u52bf\u52a0\u6743\u91cd\u8981\u6027\u91c7\u6837\uff0c\u907f\u514d\u4e86\u5b50\u76ee\u6807\u751f\u6210\u6a21\u578b\u7684\u590d\u6742\u6027\u3002", "result": "\u5728\u72b6\u6001\u548c\u50cf\u7d20\u57fa\u7840\u7684\u64cd\u63a7\u4e0e\u8fd0\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u7684\u79bb\u7ebfGCRL\u7b97\u6cd5\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u590d\u6742\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u751f\u6210\u6a21\u578b\u5373\u53ef\u6269\u5c55\u5230\u9ad8\u7ef4\u63a7\u5236\u4efb\u52a1\uff0c\u4e3a\u79bb\u7ebfGCRL\u63d0\u4f9b\u4e86\u7b80\u5355\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60, \u957f\u65f6\u7a0b\u4efb\u52a1, \u91cd\u8981\u6027\u91c7\u6837, \u975e\u5206\u5c42\u7b56\u7565"}}
{"id": "2505.15069", "pdf": "https://arxiv.org/pdf/2505.15069", "abs": "https://arxiv.org/abs/2505.15069", "authors": ["Pratik Rakesh Singh", "Kritarth Prasad", "Mohammadi Zaki", "Pankaj Wasnik"], "title": "In-Domain African Languages Translation Using LLMs and Multi-armed Bandits", "categories": ["cs.CL"], "comment": null, "summary": "Neural Machine Translation (NMT) systems face significant challenges when\nworking with low-resource languages, particularly in domain adaptation tasks.\nThese difficulties arise due to limited training data and suboptimal model\ngeneralization, As a result, selecting an optimal model for translation is\ncrucial for achieving strong performance on in-domain data, particularly in\nscenarios where fine-tuning is not feasible or practical. In this paper, we\ninvestigate strategies for selecting the most suitable NMT model for a given\ndomain using bandit-based algorithms, including Upper Confidence Bound, Linear\nUCB, Neural Linear Bandit, and Thompson Sampling. Our method effectively\naddresses the resource constraints by facilitating optimal model selection with\nhigh confidence. We evaluate the approach across three African languages and\ndomains, demonstrating its robustness and effectiveness in both scenarios where\ntarget data is available and where it is absent.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\uff08NMT\uff09\u4e2d\uff0c\u5982\u4f55\u5229\u7528\u57fa\u4e8e**bandit\u7684\u7b97\u6cd5**\u9009\u62e9\u6700\u4f73\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u9886\u57df\u9002\u5e94\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u4e0d\u8db3\u548c\u6a21\u578b\u6cdb\u5316\u95ee\u9898\uff0c\u5e76\u5728\u975e\u6d32\u8bed\u8a00\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "NMT\u7cfb\u7edf\u5728\u5904\u7406\u4f4e\u8d44\u6e90\u8bed\u8a00\u9886\u57df\u9002\u5e94\u4efb\u52a1\u65f6\uff0c\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u6709\u9650\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9762\u4e34\u6027\u80fd\u6311\u6218\u3002\u56e0\u6b64\uff0c\u5728\u65e0\u6cd5\u8fdb\u884c\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u9009\u62e9\u6700\u4f73\u7ffb\u8bd1\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u5305\u62ec**Upper Confidence Bound\u3001Linear UCB\u3001Neural Linear Bandit\u548cThompson Sampling**\u5728\u5185\u7684bandit-based\u7b97\u6cd5\uff0c\u4ee5\u9ad8\u6548\u9009\u62e9\u6700\u9002\u5408\u7279\u5b9a\u9886\u57df\u7684NMT\u6a21\u578b\u3002", "result": "\u5728\u4e09\u79cd\u975e\u6d32\u8bed\u8a00\u548c\u4e0d\u540c\u9886\u57df\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u76ee\u6807\u6570\u636e\u5b58\u5728\u6216\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\u5747\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8ebandit\u7684\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u8d44\u6e90\u9650\u5236\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684NMT\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1,\u4f4e\u8d44\u6e90\u8bed\u8a00,\u9886\u57df\u9002\u5e94,bandit\u7b97\u6cd5,\u6a21\u578b\u9009\u62e9"}}
{"id": "2505.14999", "pdf": "https://arxiv.org/pdf/2505.14999", "abs": "https://arxiv.org/abs/2505.14999", "authors": ["Eric Hanchen Jiang", "Haozheng Luo", "Shengyuan Pang", "Xiaomin Li", "Zhenting Qi", "Hengli Li", "Cheng-Fu Yang", "Zongyu Lin", "Xinfeng Li", "Hao Xu", "Kai-Wei Chang", "Ying Nian Wu"], "title": "Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Mathematical reasoning presents a significant challenge for Large Language\nModels (LLMs), often requiring robust multi step logical consistency. While\nChain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee\ncorrectness, and improving reliability via extensive sampling is\ncomputationally costly. This paper introduces the Energy Outcome Reward Model\n(EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy\nBased Models (EBMs) to simplify the training of reward models by learning to\nassign a scalar energy score to CoT solutions using only outcome labels,\nthereby avoiding detailed annotations. It achieves this by interpreting\ndiscriminator output logits as negative energies, effectively ranking\ncandidates where lower energy is assigned to solutions leading to correct final\noutcomes implicitly favoring coherent reasoning. On mathematical benchmarks\n(GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with\nLlama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively\nleverages a given pool of candidate solutions to match or exceed the\nperformance of brute force sampling, thereby enhancing LLM reasoning outcome\nreliability through its streamlined post hoc verification process.", "AI": {"tldr": "EORM\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u540e\u9a8c\u9a8c\u8bc1\u5668\uff0c\u901a\u8fc7\u80fd\u91cf\u6a21\u578b\u7b80\u5316\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347LLM\u6570\u5b66\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u63d0\u9ad8LLM\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u903b\u8f91\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\uff0c\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u5f15\u5165EORM\uff0c\u5229\u7528\u80fd\u91cf\u6a21\u578b\u7b80\u5316\u5956\u52b1\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u4ec5\u4f7f\u7528\u7ed3\u679c\u6807\u7b7e\u4e3aCoT\u89e3\u51b3\u65b9\u6848\u5206\u914d\u80fd\u91cf\u5206\u6570\u3002", "result": "\u5728GSM8k\u548cMATH\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u7387\uff08\u5982Llama 3 8B\u5728GSM8k\u4e0a\u8fbe\u523090.7%\uff09\u3002", "conclusion": "EORM\u901a\u8fc7\u540e\u9a8c\u9a8c\u8bc1\u9ad8\u6548\u63d0\u5347\u4e86LLM\u63a8\u7406\u7ed3\u679c\u7684\u53ef\u9760\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u66b4\u529b\u91c7\u6837\u3002", "keywords": "LLM, \u6570\u5b66\u63a8\u7406, EORM, \u80fd\u91cf\u6a21\u578b, \u540e\u9a8c\u9a8c\u8bc1"}}
{"id": "2505.14753", "pdf": "https://arxiv.org/pdf/2505.14753", "abs": "https://arxiv.org/abs/2505.14753", "authors": ["Mengzhu Wang", "Jiao Li", "Shanshan Wang", "Long Lan", "Huibin Tan", "Liang Yang", "Guoli Yang"], "title": "TransMedSeg: A Transferable Semantic Framework for Semi-Supervised Medical Image Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Semi-supervised learning (SSL) has achieved significant progress in medical\nimage segmentation (SSMIS) through effective utilization of limited labeled\ndata. While current SSL methods for medical images predominantly rely on\nconsistency regularization and pseudo-labeling, they often overlook\ntransferable semantic relationships across different clinical domains and\nimaging modalities. To address this, we propose TransMedSeg, a novel\ntransferable semantic framework for semi-supervised medical image segmentation.\nOur approach introduces a Transferable Semantic Augmentation (TSA) module,\nwhich implicitly enhances feature representations by aligning domain-invariant\nsemantics through cross-domain distribution matching and intra-domain\nstructural preservation. Specifically, TransMedSeg constructs a unified feature\nspace where teacher network features are adaptively augmented towards student\nnetwork semantics via a lightweight memory module, enabling implicit semantic\ntransformation without explicit data generation. Interestingly, this\naugmentation is implicitly realized through an expected transferable\ncross-entropy loss computed over the augmented teacher distribution. An upper\nbound of the expected loss is theoretically derived and minimized during\ntraining, incurring negligible computational overhead. Extensive experiments on\nmedical image datasets demonstrate that TransMedSeg outperforms existing\nsemi-supervised methods, establishing a new direction for transferable\nrepresentation learning in medical image analysis.", "AI": {"tldr": "TransMedSeg\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u8f6c\u79fb\u7684\u8bed\u4e49\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u57df\u5206\u5e03\u5339\u914d\u548c\u57df\u5185\u7ed3\u6784\u4fdd\u7559\uff0c\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5ffd\u7565\u4e86\u8de8\u4e34\u5e8a\u9886\u57df\u548c\u6210\u50cf\u6a21\u6001\u7684\u53ef\u8f6c\u79fb\u8bed\u4e49\u5173\u7cfb\uff0cTransMedSeg\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u53ef\u8f6c\u79fb\u8bed\u4e49\u589e\u5f3a\uff08TSA\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u8de8\u57df\u5206\u5e03\u5bf9\u9f50\u548c\u57df\u5185\u7ed3\u6784\u4fdd\u7559\u9690\u5f0f\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5185\u5b58\u6a21\u5757\u5b9e\u73b0\u8bed\u4e49\u8f6c\u6362\u3002", "result": "TransMedSeg\u5728\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u53ef\u8f6c\u79fb\u8868\u793a\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "conclusion": "TransMedSeg\u901a\u8fc7\u9690\u5f0f\u8bed\u4e49\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\uff0c\u5e76\u5177\u6709\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u7279\u70b9\u3002", "keywords": "\u534a\u76d1\u7763\u5b66\u4e60, \u533b\u5b66\u56fe\u50cf\u5206\u5272, \u53ef\u8f6c\u79fb\u8bed\u4e49, \u8de8\u57df\u5bf9\u9f50, \u7ed3\u6784\u4fdd\u7559"}}
{"id": "2505.15071", "pdf": "https://arxiv.org/pdf/2505.15071", "abs": "https://arxiv.org/abs/2505.15071", "authors": ["Chen Huang", "Junkai Luo", "Xinzuo Wang", "Wenqiang Lei", "Jiancheng Lv"], "title": "Can Large Language Models Understand Internet Buzzwords Through User-Generated Content", "categories": ["cs.CL"], "comment": "ACL 2025 Main Paper. Our dataset and code are available at\n  https://github.com/SCUNLP/Buzzword", "summary": "The massive user-generated content (UGC) available in Chinese social media is\ngiving rise to the possibility of studying internet buzzwords. In this paper,\nwe study if large language models (LLMs) can generate accurate definitions for\nthese buzzwords based on UGC as examples. Our work serves a threefold\ncontribution. First, we introduce CHEER, the first dataset of Chinese internet\nbuzzwords, each annotated with a definition and relevant UGC. Second, we\npropose a novel method, called RESS, to effectively steer the comprehending\nprocess of LLMs to produce more accurate buzzword definitions, mirroring the\nskills of human language learning. Third, with CHEER, we benchmark the\nstrengths and weaknesses of various off-the-shelf definition generation methods\nand our RESS. Our benchmark demonstrates the effectiveness of RESS while\nrevealing crucial shared challenges: over-reliance on prior exposure,\nunderdeveloped inferential abilities, and difficulty identifying high-quality\nUGC to facilitate comprehension. We believe our work lays the groundwork for\nfuture advancements in LLM-based definition generation. Our dataset and code\nare available at https://github.com/SCUNLP/Buzzword.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u57fa\u4e8e\u4e2d\u6587\u793e\u4ea4\u5a92\u4f53UGC\u751f\u6210\u7f51\u7edc\u6d41\u884c\u8bed\u5b9a\u4e49\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u65b9\u6cd5RESS\u3002", "motivation": "\u901a\u8fc7\u4e2d\u6587\u793e\u4ea4\u5a92\u4f53\u7684\u5927\u89c4\u6a21UGC\u7814\u7a76\u7f51\u7edc\u6d41\u884c\u8bed\uff0c\u63a2\u7d22LLMs\u751f\u6210\u51c6\u786e\u5b9a\u4e49\u7684\u6f5c\u529b\u3002", "method": "\u5f15\u5165\u9996\u4e2a\u4e2d\u6587\u7f51\u7edc\u6d41\u884c\u8bed\u6570\u636e\u96c6CHEER\uff0c\u5e76\u63d0\u51fa\u65b0\u65b9\u6cd5RESS\u4ee5\u6307\u5bfcLLMs\u7406\u89e3\u8fc7\u7a0b\u3002", "result": "RESS\u5728\u5b9a\u4e49\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86LLMs\u7684\u5171\u6027\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u4e3aLLMs\u5b9a\u4e49\u751f\u6210\u9886\u57df\u7684\u672a\u6765\u8fdb\u6b65\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "keywords": "LLMs, \u7f51\u7edc\u6d41\u884c\u8bed, \u6570\u636e\u96c6, \u5b9a\u4e49\u751f\u6210, RESS"}}
{"id": "2505.15008", "pdf": "https://arxiv.org/pdf/2505.15008", "abs": "https://arxiv.org/abs/2505.15008", "authors": ["Alvin Heng", "Harold Soh"], "title": "Know When to Abstain: Optimal Selective Classification with Likelihood Ratios", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Selective classification enhances the reliability of predictive models by\nallowing them to abstain from making uncertain predictions. In this work, we\nrevisit the design of optimal selection functions through the lens of the\nNeyman--Pearson lemma, a classical result in statistics that characterizes the\noptimal rejection rule as a likelihood ratio test. We show that this\nperspective not only unifies the behavior of several post-hoc selection\nbaselines, but also motivates new approaches to selective classification which\nwe propose here. A central focus of our work is the setting of covariate shift,\nwhere the input distribution at test time differs from that at training. This\nrealistic and challenging scenario remains relatively underexplored in the\ncontext of selective classification. We evaluate our proposed methods across a\nrange of vision and language tasks, including both supervised learning and\nvision-language models. Our experiments demonstrate that our\nNeyman--Pearson-informed methods consistently outperform existing baselines,\nindicating that likelihood ratio-based selection offers a robust mechanism for\nimproving selective classification under covariate shifts. Our code is publicly\navailable at https://github.com/clear-nus/sc-likelihood-ratios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7Neyman-Pearson\u5f15\u7406\u91cd\u65b0\u8bbe\u8ba1\u9009\u62e9\u6027\u5206\u7c7b\u7684\u6700\u4f18\u9009\u62e9\u51fd\u6570\uff0c\u7edf\u4e00\u4e86\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u7684\u884c\u4e3a\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u9009\u62e9\u6027\u5206\u7c7b\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u534f\u53d8\u91cf\u504f\u79fb\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u901a\u8fc7Neyman-Pearson\u5f15\u7406\u4f18\u5316\u9009\u62e9\u6027\u5206\u7c7b\u7684\u4e0d\u786e\u5b9a\u6027\u9884\u6d4b\uff0c\u63d0\u5347\u9884\u6d4b\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u534f\u53d8\u91cf\u504f\u79fb\u7684\u5b9e\u9645\u6311\u6218\u6027\u573a\u666f\u4e2d\u3002", "method": "\u57fa\u4e8eNeyman-Pearson\u5f15\u7406\u8bbe\u8ba1\u4f3c\u7136\u6bd4\u68c0\u9a8c\u4f5c\u4e3a\u6700\u4f18\u62d2\u7edd\u89c4\u5219\uff0c\u63d0\u51fa\u65b0\u7684\u9009\u62e9\u6027\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u5728\u534f\u53d8\u91cf\u504f\u79fb\u573a\u666f\u4e0b\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u57fa\u4e8e\u4f3c\u7136\u6bd4\u9009\u62e9\u7684\u65b9\u6cd5\u5728\u591a\u79cd\u89c6\u89c9\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u4f3c\u7136\u6bd4\u9009\u62e9\u4e3a\u534f\u53d8\u91cf\u504f\u79fb\u4e0b\u7684\u9009\u62e9\u6027\u5206\u7c7b\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u6539\u8fdb\u673a\u5236\u3002", "keywords": "\u9009\u62e9\u6027\u5206\u7c7b, Neyman-Pearson\u5f15\u7406, \u534f\u53d8\u91cf\u504f\u79fb, \u4f3c\u7136\u6bd4\u68c0\u9a8c"}}
{"id": "2505.15074", "pdf": "https://arxiv.org/pdf/2505.15074", "abs": "https://arxiv.org/abs/2505.15074", "authors": ["Yuhang Zhou", "Jing Zhu", "Shengyi Qian", "Zhuokai Zhao", "Xiyao Wang", "Xiaoyu Liu", "Ming Li", "Paiheng Xu", "Wei Ai", "Furong Huang"], "title": "DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 3 figures", "summary": "Large Language Models (LLMs) are increasingly aligned with human preferences\nthrough Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,\nGroup Relative Policy Optimization (GRPO) has gained attention for its\nsimplicity and strong performance, notably eliminating the need for a learned\nvalue function. However, GRPO implicitly assumes a balanced domain distribution\nand uniform semantic alignment across groups - assumptions that rarely hold in\nreal-world datasets. When applied to multi-domain, imbalanced data, GRPO\ndisproportionately optimizes for dominant domains, neglecting underrepresented\nones and resulting in poor generalization and fairness. We propose\nDomain-Informed Self-Consistency Policy Optimization (DISCO), a principled\nextension to GRPO that addresses inter-group imbalance with two key\ninnovations. Domain-aware reward scaling counteracts frequency bias by\nreweighting optimization based on domain prevalence. Difficulty-aware reward\nscaling leverages prompt-level self-consistency to identify and prioritize\nuncertain prompts that offer greater learning value. Together, these strategies\npromote more equitable and effective policy learning across domains. Extensive\nexperiments across multiple LLMs and skewed training distributions show that\nDISCO improves generalization, outperforms existing GRPO variants by 5% on\nQwen3 models, and sets new state-of-the-art results on multi-domain alignment\nbenchmarks.", "AI": {"tldr": "DISCO\u662f\u4e00\u79cd\u6539\u8fdbGRPO\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u57df\u611f\u77e5\u548c\u96be\u5ea6\u611f\u77e5\u7684\u5956\u52b1\u7f29\u653e\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6cdb\u5316\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "GRPO\u5728\u73b0\u5b9e\u6570\u636e\u4e2d\u56e0\u5047\u8bbe\u5747\u8861\u5206\u5e03\u548c\u7edf\u4e00\u8bed\u4e49\u5bf9\u9f50\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u591a\u57df\u4e0d\u5e73\u8861\u6570\u636e\u4e2d\uff0c\u503e\u5411\u4e8e\u4e3b\u5bfc\u57df\uff0c\u5ffd\u89c6\u5c0f\u4f17\u57df\u3002", "method": "\u63d0\u51faDISCO\u65b9\u6cd5\uff0c\u5305\u542b\u57df\u611f\u77e5\u5956\u52b1\u7f29\u653e\u548c\u96be\u5ea6\u611f\u77e5\u5956\u52b1\u7f29\u653e\uff0c\u524d\u8005\u6839\u636e\u57df\u9891\u7387\u91cd\u65b0\u52a0\u6743\u4f18\u5316\uff0c\u540e\u8005\u5229\u7528\u63d0\u793a\u7ea7\u81ea\u4e00\u81f4\u6027\u4f18\u5148\u5b66\u4e60\u9ad8\u4ef7\u503c\u4e0d\u786e\u5b9a\u63d0\u793a\u3002", "result": "\u5728\u591a\u4e2aLLM\u548c\u503e\u659c\u8bad\u7ec3\u5206\u5e03\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cDISCO\u5c06Qwen3\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\u4e865%\uff0c\u5728\u591a\u57df\u5bf9\u9f50\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u521b\u9020\u4e86\u65b0\u8bb0\u5f55\u3002", "conclusion": "DISCO\u901a\u8fc7\u521b\u65b0\u7684\u5956\u52b1\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86GRPO\u5728\u6570\u636e\u4e0d\u5e73\u8861\u548c\u516c\u5e73\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5f3a\u5316\u5b66\u4e60, \u4eba\u7c7b\u53cd\u9988, \u6570\u636e\u4e0d\u5e73\u8861, \u516c\u5e73\u6027"}}
{"id": "2505.15009", "pdf": "https://arxiv.org/pdf/2505.15009", "abs": "https://arxiv.org/abs/2505.15009", "authors": ["Quan Nguyen", "Thanh Nguyen-Tang"], "title": "One-Layer Transformers are Provably Optimal for In-context Reasoning and Distributional Association Learning in Next-Token Prediction Tasks", "categories": ["cs.LG", "cs.AI"], "comment": "27 pages", "summary": "We study the approximation capabilities and on-convergence behaviors of\none-layer transformers on the noiseless and noisy in-context reasoning of\nnext-token prediction. Existing theoretical results focus on understanding the\nin-context reasoning behaviors for either the first gradient step or when the\nnumber of samples is infinite. Furthermore, no convergence rates nor\ngeneralization abilities were known. Our work addresses these gaps by showing\nthat there exists a class of one-layer transformers that are provably\nBayes-optimal with both linear and ReLU attention. When being trained with\ngradient descent, we show via a finite-sample analysis that the expected loss\nof these transformers converges at linear rate to the Bayes risk. Moreover, we\nprove that the trained models generalize to unseen samples as well as exhibit\nlearning behaviors that were empirically observed in previous works. Our\ntheoretical findings are further supported by extensive empirical validations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5355\u5c42Transformer\u5728\u65e0\u566a\u58f0\u548c\u6709\u566a\u58f0\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u903c\u8fd1\u80fd\u529b\u548c\u6536\u655b\u884c\u4e3a\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7406\u8bba\u5728\u6536\u655b\u901f\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u7a7a\u767d\uff0c\u8bc1\u660e\u4e86\u5176\u8d1d\u53f6\u65af\u6700\u4f18\u6027\u3002", "motivation": "\u73b0\u6709\u7406\u8bba\u7814\u7a76\u591a\u5173\u6ce8\u65e0\u9650\u6837\u672c\u6216\u521d\u59cb\u68af\u5ea6\u6b65\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u7f3a\u4e4f\u6536\u655b\u901f\u7387\u548c\u6cdb\u5316\u80fd\u529b\u7684\u5206\u6790\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u6709\u9650\u6837\u672c\u5206\u6790\uff0c\u5c55\u793a\u4e86\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u7684Transformer\u6a21\u578b\u4ee5\u7ebf\u6027\u901f\u7387\u6536\u655b\u5230\u8d1d\u53f6\u65af\u98ce\u9669\uff0c\u5e76\u5bf9\u672a\u89c1\u6837\u672c\u5177\u6709\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u8bc1\u660e\u4e86\u5b58\u5728\u4e00\u7c7b\u5355\u5c42Transformer\uff08\u7ebf\u6027\u548cReLU\u6ce8\u610f\u529b\uff09\u662f\u8d1d\u53f6\u65af\u6700\u4f18\u7684\uff0c\u4e14\u8bad\u7ec3\u6a21\u578b\u5728\u7406\u8bba\u4e0a\u548c\u5b9e\u9a8c\u4e2d\u90fd\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5355\u5c42Transformer\u5728\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u652f\u6301\u5176\u5728\u5b9e\u9645\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "Transformer, \u4e0a\u4e0b\u6587\u63a8\u7406, \u8d1d\u53f6\u65af\u6700\u4f18, \u68af\u5ea6\u4e0b\u964d, \u6cdb\u5316\u80fd\u529b"}}
{"id": "2505.14757", "pdf": "https://arxiv.org/pdf/2505.14757", "abs": "https://arxiv.org/abs/2505.14757", "authors": ["John Rincon", "Alexander R. Pelletier", "Destiny Gilliland", "Wei Wang", "Ding Wang", "Baradwaj S. Sankar", "Lori Scott-Sheldon", "Samson Gebreab", "William Hersh", "Parisa Rashidi", "Sally Baxter", "Wade Schulz", "Trey Ideker", "Yael Bensoussan", "Paul C. Boutros", "Alex A. T. Bui", "Colin Walsh", "Karol E. Watson", "Peipei Ping"], "title": "Bridge2AI: Building A Cross-disciplinary Curriculum Towards AI-Enhanced Biomedical and Clinical Care", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Objective: As AI becomes increasingly central to healthcare, there is a\npressing need for bioinformatics and biomedical training systems that are\npersonalized and adaptable. Materials and Methods: The NIH Bridge2AI Training,\nRecruitment, and Mentoring (TRM) Working Group developed a cross-disciplinary\ncurriculum grounded in collaborative innovation, ethical data stewardship, and\nprofessional development within an adapted Learning Health System (LHS)\nframework. Results: The curriculum integrates foundational AI modules,\nreal-world projects, and a structured mentee-mentor network spanning Bridge2AI\nGrand Challenges and the Bridge Center. Guided by six learner personas, the\nprogram tailors educational pathways to individual needs while supporting\nscalability. Discussion: Iterative refinement driven by continuous feedback\nensures that content remains responsive to learner progress and emerging\ntrends. Conclusion: With over 30 scholars and 100 mentors engaged across North\nAmerica, the TRM model demonstrates how adaptive, persona-informed training can\nbuild interdisciplinary competencies and foster an integrative, ethically\ngrounded AI education in biomedical contexts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4e2a\u6027\u5316\u3001\u9002\u5e94\u6027\u5f3a\u7684\u751f\u7269\u4fe1\u606f\u5b66\u548c\u751f\u7269\u533b\u5b66AI\u57f9\u8bad\u7cfb\u7edf\uff0c\u901a\u8fc7\u8de8\u5b66\u79d1\u8bfe\u7a0b\u548c\u5bfc\u5e08\u7f51\u7edc\uff0c\u57f9\u517b\u5177\u5907\u4f26\u7406\u57fa\u7840\u7684\u591a\u5b66\u79d1\u80fd\u529b\u3002", "motivation": "\u968f\u7740AI\u5728\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u65e5\u76ca\u91cd\u8981\uff0c\u9700\u8981\u5f00\u53d1\u4e2a\u6027\u5316\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u57f9\u8bad\u7cfb\u7edf\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "NIH Bridge2AI TRM\u5de5\u4f5c\u7ec4\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u534f\u4f5c\u521b\u65b0\u3001\u4f26\u7406\u6570\u636e\u7ba1\u7406\u548c\u4e13\u4e1a\u53d1\u5c55\u7684\u8de8\u5b66\u79d1\u8bfe\u7a0b\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u8005\u89d2\u8272\u548c\u6559\u80b2\u8def\u5f84\u7684\u4e2a\u6027\u5316\u8bbe\u8ba1\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u3002", "result": "\u8be5\u8bfe\u7a0b\u6574\u5408\u4e86\u57fa\u7840AI\u6a21\u5757\u3001\u5b9e\u9645\u9879\u76ee\u548c\u6709\u7ed3\u6784\u7684\u5bfc\u5e08\u7f51\u7edc\uff0c\u5df2\u670930\u591a\u540d\u5b66\u8005\u548c100\u540d\u5bfc\u5e08\u53c2\u4e0e\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u6301\u7eed\u53cd\u9988\u7684\u8fed\u4ee3\u6539\u8fdb\uff0c\u8be5\u6a21\u578b\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u4e2a\u6027\u5316\u57f9\u8bad\u5728\u751f\u7269\u533b\u5b66\u80cc\u666f\u4e0b\u57f9\u517b\u591a\u5b66\u79d1\u80fd\u529b\u548c\u4f26\u7406\u57fa\u7840\u7684AI\u6559\u80b2\u3002", "keywords": "AI\u57f9\u8bad,\u751f\u7269\u4fe1\u606f\u5b66,\u8de8\u5b66\u79d1\u8bfe\u7a0b,\u4f26\u7406\u6570\u636e\u7ba1\u7406,\u4e2a\u6027\u5316\u5b66\u4e60"}}
{"id": "2505.15075", "pdf": "https://arxiv.org/pdf/2505.15075", "abs": "https://arxiv.org/abs/2505.15075", "authors": ["Hao Wang", "Pinzhi Huang", "Jihan Yang", "Saining Xie", "Daisuke Kawahara"], "title": "Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "https://github.com/nlp-waseda/traveling-across-languages", "summary": "The rapid evolution of multimodal large language models (MLLMs) has\nsignificantly enhanced their real-world applications. However, achieving\nconsistent performance across languages, especially when integrating cultural\nknowledge, remains a significant challenge. To better assess this issue, we\nintroduce two new benchmarks: KnowRecall and VisRecall, which evaluate\ncross-lingual consistency in MLLMs. KnowRecall is a visual question answering\nbenchmark designed to measure factual knowledge consistency in 15 languages,\nfocusing on cultural and historical questions about global landmarks. VisRecall\nassesses visual memory consistency by asking models to describe landmark\nappearances in 9 languages without access to images. Experimental results\nreveal that state-of-the-art MLLMs, including proprietary ones, still struggle\nto achieve cross-lingual consistency. This underscores the need for more robust\napproaches that produce truly multilingual and culturally aware models.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecdKnowRecall\u548cVisRecall\u4e24\u4e2a\u65b0\u57fa\u51c6\uff0c\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u548c\u6587\u5316\u77e5\u8bc6\u4e00\u81f4\u6027\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u591a\u8bed\u8a00\u548c\u8de8\u6587\u5316\u77e5\u8bc6\u4e00\u81f4\u6027\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faKnowRecall\u548cVisRecall\u4e24\u4e2a\u65b0\u57fa\u51c6\uff0c\u5206\u522b\u8bc4\u4f30\u6a21\u578b\u572815\u79cd\u8bed\u8a00\u7684\u89c6\u89c9\u95ee\u7b54\u548c9\u79cd\u8bed\u8a00\u7684\u89c6\u89c9\u8bb0\u5fc6\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u7684\u5148\u8fdb\u6a21\u578b\u5728\u591a\u8bed\u8a00\u4e00\u81f4\u6027\u4e0a\u4ecd\u6709\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u6587\u5316\u5386\u53f2\u77e5\u8bc6\u548c\u89c6\u89c9\u8bb0\u5fc6\u65b9\u9762\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u9700\u8981\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\uff0c\u4ee5\u5f00\u53d1\u771f\u6b63\u591a\u8bed\u8a00\u4e14\u6587\u5316\u611f\u77e5\u7684\u6a21\u578b\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\u3001\u6587\u5316\u77e5\u8bc6\u3001\u57fa\u51c6\u6d4b\u8bd5\u3001\u89c6\u89c9\u95ee\u7b54"}}
{"id": "2505.15015", "pdf": "https://arxiv.org/pdf/2505.15015", "abs": "https://arxiv.org/abs/2505.15015", "authors": ["Longlong Li", "Cunquan Qu", "Guanghui Wang"], "title": "Beyond Node Attention: Multi-Scale Harmonic Encoding for Feature-Wise Graph Message Passing", "categories": ["cs.LG"], "comment": null, "summary": "Conventional Graph Neural Networks (GNNs) aggregate neighbor embeddings as\nholistic vectors, lacking the ability to identify fine-grained,\ndirection-specific feature relevance. We propose MSH-GNN (Multi-Scale Harmonic\nGraph Neural Network), a novel architecture that performs feature-wise adaptive\nmessage passing through node-specific harmonic projections. For each node,\nMSH-GNN dynamically projects neighbor features onto frequency-sensitive\ndirections determined by the target node's own representation. These\nprojections are further modulated using learnable sinusoidal encodings at\nmultiple frequencies, enabling the model to capture both smooth and oscillatory\nstructural patterns across scales. A frequency-aware attention pooling\nmechanism is introduced to emphasize spectrally and structurally salient nodes\nduring readout. Theoretically, we prove that MSH-GNN approximates\nshift-invariant kernels and matches the expressive power of the\n1-Weisfeiler-Lehman (1-WL) test. Empirically, MSH-GNN consistently outperforms\nstate-of-the-art models on a wide range of graph and node classification tasks.\nFurthermore, in challenging classification settings involving joint variations\nin graph topology and spectral frequency, MSH-GNN excels at capturing\nstructural asymmetries and high-frequency modulations, enabling more accurate\ngraph discrimination.", "AI": {"tldr": "MSH-GNN\u662f\u4e00\u79cd\u65b0\u578b\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u8c10\u6ce2\u6295\u5f71\u5b9e\u73b0\u7279\u5f81\u7ea7\u522b\u7684\u81ea\u9002\u5e94\u6d88\u606f\u4f20\u9012\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u5206\u7c7b\u548c\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfGNN\u4ee5\u6574\u4f53\u5411\u91cf\u5f62\u5f0f\u805a\u5408\u90bb\u5c45\u5d4c\u5165\uff0c\u65e0\u6cd5\u6355\u6349\u7ec6\u7c92\u5ea6\u548c\u65b9\u5411\u7279\u5b9a\u7684\u7279\u5f81\u76f8\u5173\u6027\u3002", "method": "MSH-GNN\u901a\u8fc7\u8282\u70b9\u7279\u5b9a\u7684\u8c10\u6ce2\u6295\u5f71\uff0c\u52a8\u6001\u5730\u5c06\u90bb\u5c45\u7279\u5f81\u6620\u5c04\u5230\u9891\u7387\u654f\u611f\u7684\u65b9\u5411\uff0c\u5e76\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u6b63\u5f26\u7f16\u7801\u5728\u591a\u9891\u7387\u4e0b\u8c03\u5236\u3002", "result": "MSH-GNN\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5c24\u5176\u5728\u6d89\u53ca\u56fe\u62d3\u6251\u548c\u9891\u8c31\u9891\u7387\u8054\u5408\u53d8\u5316\u7684\u590d\u6742\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MSH-GNN\u901a\u8fc7\u591a\u5c3a\u5ea6\u8c10\u6ce2\u6d88\u606f\u4f20\u9012\u673a\u5236\u6709\u6548\u6355\u6349\u7ed3\u6784\u4e0d\u5bf9\u79f0\u6027\u548c\u9ad8\u9891\u8c03\u5236\uff0c\u63d0\u5347\u4e86\u56fe\u5224\u522b\u80fd\u529b\u3002", "keywords": "Graph Neural Networks, Multi-Scale Harmonic, Feature-wise Adaptation, Frequency-aware Attention, Graph Classification"}}
{"id": "2505.14758", "pdf": "https://arxiv.org/pdf/2505.14758", "abs": "https://arxiv.org/abs/2505.14758", "authors": ["Alayt Issak", "Uttkarsh Narayan", "Ramya Srinivasan", "Erica Kleinman", "Casper Harteveld"], "title": "Kaleidoscope Gallery: Exploring Ethics and Generative AI Through Art", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Ethical theories and Generative AI (GenAI) models are dynamic concepts\nsubject to continuous evolution. This paper investigates the visualization of\nethics through a subset of GenAI models. We expand on the emerging field of\nVisual Ethics, using art as a form of critical inquiry and the metaphor of a\nkaleidoscope to invoke moral imagination. Through formative interviews with 10\nethics experts, we first establish a foundation of ethical theories. Our\nanalysis reveals five families of ethical theories, which we then transform\ninto images using the text-to-image (T2I) GenAI model. The resulting imagery,\ncurated as Kaleidoscope Gallery and evaluated by the same experts, revealed\neight themes that highlight how morality, society, and learned associations are\ncentral to ethical theories. We discuss implications for critically examining\nT2I models and present cautions and considerations. This work contributes to\nexamining ethical theories as foundational knowledge that interrogates GenAI\nmodels as socio-technical systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u6a21\u578b\u53ef\u89c6\u5316\u4f26\u7406\u7406\u8bba\uff0c\u5229\u7528\u827a\u672f\u548c\u4e07\u82b1\u7b52\u9690\u55bb\u8fdb\u884c\u6279\u5224\u6027\u63a2\u7a76\uff0c\u63ed\u793a\u4e86\u4e94\u79cd\u4f26\u7406\u7406\u8bba\u5bb6\u65cf\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u56fe\u50cf\uff0c\u6700\u7ec8\u63d0\u51fa\u516b\u79cd\u4e3b\u9898\u4ee5\u5206\u6790\u9053\u5fb7\u3001\u793e\u4f1a\u4e0e\u5b66\u4e60\u5173\u8054\u7684\u4e2d\u5fc3\u5730\u4f4d\u3002", "motivation": "\u901a\u8fc7\u751f\u6210\u5f0fAI\u6a21\u578b\u53ef\u89c6\u5316\u4f26\u7406\u7406\u8bba\uff0c\u63a2\u7d22\u4f26\u7406\u4e0e\u6280\u672f\u7684\u4ea4\u4e92\uff0c\u5e76\u63a8\u52a8\u89c6\u89c9\u4f26\u7406\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc710\u4f4d\u4f26\u7406\u5b66\u4e13\u5bb6\u7684\u8bbf\u8c08\u5efa\u7acb\u4f26\u7406\u7406\u8bba\u57fa\u7840\uff0c\u4f7f\u7528\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u751f\u6210\u5f0fAI\u6a21\u578b\u5c06\u4f26\u7406\u7406\u8bba\u8f6c\u6362\u4e3a\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u8bc4\u4f30\u5f62\u6210\u4e07\u82b1\u7b52\u753b\u5eca\u3002", "result": "\u63ed\u793a\u4e86\u4e94\u79cd\u4f26\u7406\u7406\u8bba\u5bb6\u65cf\uff0c\u5e76\u901a\u8fc7\u56fe\u50cf\u5206\u6790\u63d0\u51fa\u516b\u79cd\u4e3b\u9898\uff0c\u5f3a\u8c03\u9053\u5fb7\u3001\u793e\u4f1a\u548c\u5b66\u4e60\u5173\u8054\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6279\u5224\u6027\u5206\u6790T2I\u6a21\u578b\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5e76\u63a2\u8ba8\u4e86\u4f26\u7406\u7406\u8bba\u4f5c\u4e3a\u793e\u4f1a\u6280\u672f\u7cfb\u7edf\u7684\u57fa\u7840\u77e5\u8bc6\u7684\u610f\u4e49\u3002", "keywords": "\u751f\u6210\u5f0fAI, \u4f26\u7406\u7406\u8bba, \u53ef\u89c6\u5316\u4f26\u7406, \u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b, \u793e\u4f1a\u6280\u672f\u7cfb\u7edf"}}
{"id": "2505.15087", "pdf": "https://arxiv.org/pdf/2505.15087", "abs": "https://arxiv.org/abs/2505.15087", "authors": ["Zhiyu Shen", "Jiyuan Liu", "Yunhe Pang", "Yanghui Rao"], "title": "HopWeaver: Synthesizing Authentic Multi-Hop Questions Across Text Corpora", "categories": ["cs.CL"], "comment": "27 pages. Code will be available at\n  [https://github.com/Zh1yuShen/HopWeaver]", "summary": "Multi-Hop Question Answering (MHQA) is crucial for evaluating the model's\ncapability to integrate information from diverse sources. However, creating\nextensive and high-quality MHQA datasets is challenging: (i) manual annotation\nis expensive, and (ii) current synthesis methods often produce simplistic\nquestions or require extensive manual guidance. This paper introduces\nHopWeaver, the first automatic framework synthesizing authentic multi-hop\nquestions from unstructured text corpora without human intervention. HopWeaver\nsynthesizes two types of multi-hop questions (bridge and comparison) using an\ninnovative approach that identifies complementary documents across corpora. Its\ncoherent pipeline constructs authentic reasoning paths that integrate\ninformation across multiple documents, ensuring synthesized questions\nnecessitate authentic multi-hop reasoning. We further present a comprehensive\nsystem for evaluating synthesized multi-hop questions. Empirical evaluations\ndemonstrate that the synthesized questions achieve comparable or superior\nquality to human-annotated datasets at a lower cost. Our approach is valuable\nfor developing MHQA datasets in specialized domains with scarce annotated\nresources. The code for HopWeaver is publicly available.", "AI": {"tldr": "HopWeaver\u662f\u4e00\u79cd\u81ea\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u672a\u7ed3\u6784\u5316\u6587\u672c\u8bed\u6599\u5e93\u4e2d\u5408\u6210\u771f\u5b9e\u7684\u591a\u8df3\u95ee\u9898\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u5176\u751f\u6210\u7684\u95ee\u9898\u8d28\u91cf\u4e0e\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u76f8\u5f53\u6216\u66f4\u9ad8\uff0c\u4e14\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u89e3\u51b3\u624b\u52a8\u6807\u6ce8\u591a\u8df3\u95ee\u9898\u6570\u636e\u96c6\u7684\u9ad8\u6210\u672c\u95ee\u9898\u4ee5\u53ca\u73b0\u6709\u5408\u6210\u65b9\u6cd5\u751f\u6210\u7684\u95ee\u9898\u8fc7\u4e8e\u7b80\u5355\u6216\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6307\u5bfc\u7684\u5c40\u9650\u6027\u3002", "method": "HopWeaver\u901a\u8fc7\u8bc6\u522b\u8bed\u6599\u5e93\u4e2d\u7684\u4e92\u8865\u6587\u6863\uff0c\u6784\u5efa\u8fde\u8d2f\u7684\u63a8\u7406\u8def\u5f84\uff0c\u81ea\u52a8\u5408\u6210\u4e24\u79cd\u7c7b\u578b\uff08\u6865\u63a5\u548c\u6bd4\u8f83\uff09\u7684\u591a\u8df3\u95ee\u9898\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u5408\u6210\u7684\u95ee\u9898\u8d28\u91cf\u4e0e\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u76f8\u5f53\u6216\u66f4\u9ad8\uff0c\u4e14\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "HopWeaver\u4e3a\u7f3a\u4e4f\u6807\u6ce8\u8d44\u6e90\u7684\u4e13\u4e1a\u9886\u57df\u5f00\u53d1\u591a\u8df3\u95ee\u9898\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u591a\u8df3\u95ee\u7b54,\u81ea\u52a8\u5408\u6210\u6846\u67b6,\u65e0\u76d1\u7763\u5b66\u4e60,\u6865\u63a5\u95ee\u9898,\u6bd4\u8f83\u95ee\u9898"}}
{"id": "2505.15030", "pdf": "https://arxiv.org/pdf/2505.15030", "abs": "https://arxiv.org/abs/2505.15030", "authors": ["Qingyu Song", "Peiyu Liao", "Wenqian Zhao", "Yiwen Wang", "Shoubo Hu", "Hui-Ling Zhen", "Ning Jiang", "Mingxuan Yuan"], "title": "Harnessing Large Language Models Locally: Empirical Results and Implications for AI PC", "categories": ["cs.LG"], "comment": "18 pages, 14 figures", "summary": "The increasing deployment of Large Language Models (LLMs) on edge devices,\ndriven by model advancements and hardware improvements, offers significant\nprivacy benefits. However, these on-device LLMs inherently face performance\nlimitations due to reduced model capacity and necessary compression techniques.\nTo address this, we introduce a systematic methodology -- encompassing model\ncapability, development efficiency, and system resources -- for evaluating\non-device LLMs. Our comprehensive evaluation, encompassing models from 0.5B to\n14B parameters and seven post-training quantization (PTQ) methods on commodity\nlaptops, yields several critical insights: 1) System-level metrics exhibit\nnear-linear scaling with effective bits-per-weight (BPW). 2) A practical\nthreshold exists around $\\sim$3.5 effective BPW, larger models subjected to\nlow-bit quantization consistently outperform smaller models utilizing higher\nbit-precision. 3) Quantization with low BPW incurs marginal accuracy loss but\nsignificant memory savings. 4) Determined by low-level implementation specifics\npower consumption on CPU, where computation-intensive operations spend more\npower than memory-intensive ones. These findings offer crucial insights and\npractical guidelines for the efficient deployment and optimized configuration\nof LLMs on resource-constrained edge devices. Our codebase is available at\nhttps://github.com/simmonssong/LLMOnDevice.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u65b9\u6cd5\u8bc4\u4f30\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u6db5\u76d6\u6a21\u578b\u80fd\u529b\u3001\u5f00\u53d1\u6548\u7387\u548c\u7cfb\u7edf\u8d44\u6e90\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5f97\u51fa\u91cf\u5316\u3001\u6027\u80fd\u548c\u80fd\u6548\u7684\u5173\u952e\u6d1e\u5bdf\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u867d\u7136\u5e26\u6765\u4e86\u9690\u79c1\u4f18\u52bf\uff0c\u4f46\u6a21\u578b\u5bb9\u91cf\u548c\u538b\u7f29\u6280\u672f\u5bfc\u81f4\u7684\u6027\u80fd\u9650\u5236\u4ecd\u5f85\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7cfb\u7edf\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6db5\u76d6\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\uff080.5B-14B\uff09\u7684\u6a21\u578b\u548c\u4e03\u79cd\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u5728\u5546\u7528\u7b14\u8bb0\u672c\u7535\u8111\u4e0a\u8fdb\u884c\u5168\u9762\u6d4b\u8bd5\u3002", "result": "\u53d1\u73b0\u7cfb\u7edf\u7ea7\u6307\u6807\u4e0e\u6709\u6548\u6bd4\u7279\u6743\u91cd\uff08BPW\uff09\u63a5\u8fd1\u7ebf\u6027\u5173\u7cfb\uff1b\u4f4e\u6bd4\u7279\u91cf\u5316\u7684\u5927\u6a21\u578b\u57283.5 BPW\u9608\u503c\u4e0b\u8868\u73b0\u4f18\u4e8e\u9ad8\u6bd4\u7279\u91cf\u5316\u7684\u5c0f\u6a21\u578b\uff1b\u4f4eBPW\u91cf\u5316\u867d\u51c6\u786e\u7387\u7565\u6709\u635f\u5931\u4f46\u663e\u8457\u8282\u7701\u5185\u5b58\uff1bCPU\u4e0a\u8ba1\u7b97\u5bc6\u96c6\u578b\u64cd\u4f5c\u529f\u8017\u66f4\u9ad8\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u90e8\u7f72\u548c\u4f18\u5316\u914d\u7f6e\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u548c\u5b9e\u7528\u6307\u5bfc\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u8fb9\u7f18\u8ba1\u7b97\u3001\u91cf\u5316\u3001\u6027\u80fd\u8bc4\u4f30\u3001\u80fd\u6548"}}
{"id": "2505.15090", "pdf": "https://arxiv.org/pdf/2505.15090", "abs": "https://arxiv.org/abs/2505.15090", "authors": ["Sona Elza Simon", "Preethi Jyothi"], "title": "DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "Effective cross-lingual transfer remains a critical challenge in scaling the\nbenefits of large language models from high-resource to low-resource languages.\nTowards this goal, prior studies have explored many approaches to combine task\nknowledge from task-specific data in a (high-resource) source language and\nlanguage knowledge from unlabeled text in a (low-resource) target language. One\nnotable approach proposed composable sparse fine-tuning (SFT) for cross-lingual\ntransfer that learns task-specific and language-specific sparse masks to select\na subset of the pretrained model's parameters that are further fine-tuned.\nThese sparse fine-tuned vectors (SFTs) are subsequently composed with the\npretrained model to facilitate zero-shot cross-lingual transfer to a task in a\ntarget language, using only task-specific data from a source language. These\nsparse masks for SFTs were identified using a simple magnitude-based pruning.\nIn our work, we introduce DeFT-X, a novel composable SFT approach that denoises\nthe weight matrices of a pretrained model before magnitude pruning using\nsingular value decomposition, thus yielding more robust SFTs. We evaluate\nDeFT-X on a diverse set of extremely low-resource languages for sentiment\nclassification (NusaX) and natural language inference (AmericasNLI) and\ndemonstrate that it performs at par or outperforms SFT and other prominent\ncross-lingual transfer baselines.", "AI": {"tldr": "DeFT-X\u662f\u4e00\u79cd\u65b0\u7684\u53ef\u7ec4\u5408\u7a00\u758f\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u53bb\u566a\u548c\u5947\u5f02\u503c\u5206\u89e3\u6539\u8fdb\u8de8\u8bed\u8a00\u8fc1\u79fb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9ad8\u8d44\u6e90\u8bed\u8a00\u5230\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4efb\u52a1\u8fc1\u79fb\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5bf9\u6781\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8de8\u8bed\u8a00\u8fc1\u79fb\u3002", "method": "\u4f7f\u7528\u5947\u5f02\u503c\u5206\u89e3\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u77e9\u9635\u53bb\u566a\uff0c\u518d\u8fdb\u884c\u7a00\u758f\u5fae\u8c03\uff0c\u751f\u6210\u66f4\u9c81\u68d2\u7684\u4efb\u52a1\u548c\u8bed\u8a00\u7279\u5b9a\u7a00\u758f\u63a9\u7801\u3002", "result": "\u5728\u6781\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u60c5\u611f\u5206\u7c7b\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cDeFT-X\u8868\u73b0\u4f18\u4e8e\u6216\u4e0eSFT\u53ca\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "DeFT-X\u901a\u8fc7\u53bb\u566a\u548c\u7a00\u758f\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u6548\u679c\u3002", "keywords": "\u8de8\u8bed\u8a00\u8fc1\u79fb,\u7a00\u758f\u5fae\u8c03,\u5947\u5f02\u503c\u5206\u89e3,\u4f4e\u8d44\u6e90\u8bed\u8a00"}}
{"id": "2505.15034", "pdf": "https://arxiv.org/pdf/2505.15034", "abs": "https://arxiv.org/abs/2505.15034", "authors": ["Kaiwen Zha", "Zhengqi Gao", "Maohao Shen", "Zhang-Wei Hong", "Duane S. Boning", "Dina Katabi"], "title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Tech report. The first two authors contributed equally", "summary": "Reinforcement learning (RL) has recently emerged as a compelling approach for\nenhancing the reasoning capabilities of large language models (LLMs), where an\nLLM generator serves as a policy guided by a verifier (reward model). However,\ncurrent RL post-training methods for LLMs typically use verifiers that are\nfixed (rule-based or frozen pretrained) or trained discriminatively via\nsupervised fine-tuning (SFT). Such designs are susceptible to reward hacking\nand generalize poorly beyond their training distributions. To overcome these\nlimitations, we propose Tango, a novel framework that uses RL to concurrently\ntrain both an LLM generator and a verifier in an interleaved manner. A central\ninnovation of Tango is its generative, process-level LLM verifier, which is\ntrained via RL and co-evolves with the generator. Importantly, the verifier is\ntrained solely based on outcome-level verification correctness rewards without\nrequiring explicit process-level annotations. This generative RL-trained\nverifier exhibits improved robustness and superior generalization compared to\ndeterministic or SFT-trained verifiers, fostering effective mutual\nreinforcement with the generator. Extensive experiments demonstrate that both\ncomponents of Tango achieve state-of-the-art results among 7B/8B-scale models:\nthe generator attains best-in-class performance across five competition-level\nmath benchmarks and four challenging out-of-domain reasoning tasks, while the\nverifier leads on the ProcessBench dataset. Remarkably, both components exhibit\nparticularly substantial improvements on the most difficult mathematical\nreasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.", "AI": {"tldr": "\u63d0\u51faTango\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u540c\u65f6\u8bad\u7ec3LLM\u751f\u6210\u5668\u548c\u9a8c\u8bc1\u5668\uff0c\u63d0\u5347\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RL\u540e\u8bad\u7ec3\u65b9\u6cd5\u4e2d\u7684\u9a8c\u8bc1\u5668\u56fa\u5b9a\u6216\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u8bad\u7ec3\uff0c\u6613\u53d7\u5956\u52b1\u653b\u51fb\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9700\u6539\u8fdb\u3002", "method": "Tango\u6846\u67b6\u5229\u7528RL\u540c\u65f6\u8bad\u7ec3\u751f\u6210\u5668\u548c\u751f\u6210\u5f0f\u8fc7\u7a0b\u7ea7\u9a8c\u8bc1\u5668\uff0c\u65e0\u9700\u8fc7\u7a0b\u7ea7\u6807\u6ce8\u3002", "result": "Tango\u57287B/8B\u89c4\u6a21\u6a21\u578b\u4e2d\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "Tango\u901a\u8fc7\u5171\u540c\u8fdb\u5316\u751f\u6210\u5668\u548c\u9a8c\u8bc1\u5668\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u63a8\u7406\u80fd\u529b, \u9a8c\u8bc1\u5668, \u6cdb\u5316"}}
{"id": "2505.15094", "pdf": "https://arxiv.org/pdf/2505.15094", "abs": "https://arxiv.org/abs/2505.15094", "authors": ["Jing Yu", "Yuqi Tang", "Kehua Feng", "Mingyang Rao", "Lei Liang", "Zhiqiang Zhang", "Mengshu Sun", "Wen Zhang", "Qiang Zhang", "Keyan Ding", "Huajun Chen"], "title": "SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models", "categories": ["cs.CL"], "comment": "25 pages, 4 figures", "summary": "Large Language Models (LLMs) have shown impressive capabilities in contextual\nunderstanding and reasoning. However, evaluating their performance across\ndiverse scientific domains remains underexplored, as existing benchmarks\nprimarily focus on general domains and fail to capture the intricate complexity\nof scientific data. To bridge this gap, we construct SciCUEval, a comprehensive\nbenchmark dataset tailored to assess the scientific context understanding\ncapability of LLMs. It comprises ten domain-specific sub-datasets spanning\nbiology, chemistry, physics, biomedicine, and materials science, integrating\ndiverse data modalities including structured tables, knowledge graphs, and\nunstructured texts. SciCUEval systematically evaluates four core competencies:\nRelevant information identification, Information-absence detection,\nMulti-source information integration, and Context-aware inference, through a\nvariety of question formats. We conduct extensive evaluations of\nstate-of-the-art LLMs on SciCUEval, providing a fine-grained analysis of their\nstrengths and limitations in scientific context understanding, and offering\nvaluable insights for the future development of scientific-domain LLMs.", "AI": {"tldr": "SciCUEval\u662f\u4e13\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u9886\u57df\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u800c\u6784\u5efa\u7684\u7efc\u5408\u6027\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u4e2a\u79d1\u5b66\u5b50\u9886\u57df\u548c\u6570\u636e\u7c7b\u578b\uff0c\u7cfb\u7edf\u5730\u8bc4\u4f30\u6838\u5fc3\u80fd\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u6d1e\u89c1\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u9886\u57df\uff0c\u672a\u80fd\u5145\u5206\u6355\u6349\u79d1\u5b66\u6570\u636e\u7684\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u9488\u5bf9\u79d1\u5b66\u9886\u57df\u8bbe\u8ba1\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u6784\u5efaSciCUEval\u6570\u636e\u96c6\uff0c\u5305\u542b\u5341\u4e2a\u7279\u5b9a\u9886\u57df\u7684\u5b50\u6570\u636e\u96c6\uff0c\u6574\u5408\u591a\u79cd\u6570\u636e\u6a21\u6001\uff0c\u901a\u8fc7\u591a\u6837\u5316\u95ee\u9898\u683c\u5f0f\u8bc4\u4f30\u56db\u9879\u6838\u5fc3\u80fd\u529b\u3002", "result": "\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728SciCUEval\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u63ed\u793a\u4e86\u5176\u5728\u79d1\u5b66\u4e0a\u4e0b\u6587\u7406\u89e3\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "SciCUEval\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u9886\u57df\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u6d1e\u89c1\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u79d1\u5b66\u8bc4\u4f30,\u57fa\u51c6\u6d4b\u8bd5,\u4e0a\u4e0b\u6587\u7406\u89e3,\u591a\u6a21\u6001\u6570\u636e"}}
{"id": "2505.15040", "pdf": "https://arxiv.org/pdf/2505.15040", "abs": "https://arxiv.org/abs/2505.15040", "authors": ["Ivan Smirnov", "Shangding Gu"], "title": "RLBenchNet: The Right Network for the Right Reinforcement Learning Task", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has seen significant advancements through the\napplication of various neural network architectures. In this study, we\nsystematically investigate the performance of several neural networks in RL\ntasks, including Long Short-Term Memory (LSTM), Multi-Layer Perceptron (MLP),\nMamba/Mamba-2, Transformer-XL, Gated Transformer-XL, and Gated Recurrent Unit\n(GRU). Through comprehensive evaluation across continuous control, discrete\ndecision-making, and memory-based environments, we identify\narchitecture-specific strengths and limitations. Our results reveal that: (1)\nMLPs excel in fully observable continuous control tasks, providing an optimal\nbalance of performance and efficiency; (2) recurrent architectures like LSTM\nand GRU offer robust performance in partially observable environments with\nmoderate memory requirements; (3) Mamba models achieve a 4.5x higher throughput\ncompared to LSTM and a 3.9x increase over GRU, all while maintaining comparable\nperformance; and (4) only Transformer-XL, Gated Transformer-XL, and Mamba-2\nsuccessfully solve the most challenging memory-intensive tasks, with Mamba-2\nrequiring 8x less memory than Transformer-XL. These findings provide insights\nfor researchers and practitioners, enabling more informed architecture\nselection based on specific task characteristics and computational constraints.\nCode is available at: https://github.com/SafeRL-Lab/RLBenchNet", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5728\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u67b6\u6784\u7684\u4f18\u52a3\u52bf\uff0c\u5e76\u63d0\u4f9b\u4e86\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u7279\u6027\u7684\u67b6\u6784\u9009\u62e9\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u5f3a\u5316\u5b66\u4e60\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7814\u7a76\u8005\u9700\u8981\u660e\u786e\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u9002\u7528\u573a\u666f\uff0c\u4ee5\u4f18\u5316\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5168\u9762\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u5bf9\u6bd4\u4e86LSTM\u3001MLP\u3001Mamba/Mamba-2\u3001Transformer-XL\u3001Gated Transformer-XL\u548cGRU\u5728\u8fde\u7eed\u63a7\u5236\u3001\u79bb\u6563\u51b3\u7b56\u548c\u8bb0\u5fc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "MLP\u5728\u5b8c\u5168\u53ef\u89c2\u5bdf\u7684\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff1bLSTM\u548cGRU\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u8868\u73b0\u7a33\u5065\uff1bMamba\u6a21\u578b\u5728\u541e\u5410\u91cf\u4e0a\u663e\u8457\u4f18\u4e8eLSTM\u548cGRU\uff1bTransformer-XL\u548cMamba-2\u5728\u8bb0\u5fc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u540e\u8005\u5185\u5b58\u9700\u6c42\u66f4\u4f4e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u6839\u636e\u4efb\u52a1\u7279\u6027\u548c\u8ba1\u7b97\u9650\u5236\u9009\u62e9\u5408\u9002\u67b6\u6784\u7684\u5b9e\u7528\u6307\u5357\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u795e\u7ecf\u7f51\u7edc\u67b6\u6784, LSTM, MLP, Mamba, Transformer-XL"}}
{"id": "2505.15095", "pdf": "https://arxiv.org/pdf/2505.15095", "abs": "https://arxiv.org/abs/2505.15095", "authors": ["Ishmanbir Singh", "Dipankar Srirag", "Aditya Joshi"], "title": "Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable Sarcasm Detection of Australian and Indian English", "categories": ["cs.CL", "cs.AI"], "comment": "Under review. 4 pages + references", "summary": "Sarcasm is a challenge to sentiment analysis because of the incongruity\nbetween stated and implied sentiment. The challenge is exacerbated when the\nimplication may be relevant to a specific country or geographical region.\nPragmatic metacognitive prompting (PMP) is a cognition-inspired technique that\nhas been used for pragmatic reasoning. In this paper, we harness PMP for\nexplainable sarcasm detection for Australian and Indian English, alongside a\nbenchmark dataset for standard English. We manually add sarcasm explanations to\nan existing sarcasm-labeled dataset for Australian and Indian English called\nBESSTIE, and compare the performance for explainable sarcasm detection for them\nwith FLUTE, a standard English dataset containing sarcasm explanations. Our\napproach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA)\nachieves statistically significant performance improvement across all tasks and\ndatasets when compared with four alternative prompting strategies. We also find\nthat alternative techniques such as agentic prompting mitigate context-related\nfailures by enabling external knowledge retrieval. The focused contribution of\nour work is utilising PMP in generating sarcasm explanations for varieties of\nEnglish.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u542f\u53d1\u6280\u672fPMP\u7684\u53ef\u89e3\u91ca\u8bbd\u523a\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u6fb3\u5927\u5229\u4e9a\u548c\u5370\u5ea6\u82f1\u8bed\uff0c\u5e76\u5728\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8bbd\u523a\u56e0\u8868\u9762\u4e0e\u9690\u542b\u60c5\u611f\u7684\u4e0d\u4e00\u81f4\u800c\u5bf9\u60c5\u611f\u5206\u6790\u6784\u6210\u6311\u6218\uff0c\u5c24\u5176\u5728\u6d89\u53ca\u7279\u5b9a\u56fd\u5bb6\u6216\u5730\u533a\u65f6\u66f4\u52a0\u590d\u6742\u3002", "method": "\u5229\u7528PMP\u6280\u672f\u4e3a\u6fb3\u5927\u5229\u4e9a\u548c\u5370\u5ea6\u82f1\u8bed\u6570\u636e\u96c6BESSTIE\u624b\u52a8\u6dfb\u52a0\u8bbd\u523a\u89e3\u91ca\uff0c\u5e76\u4e0e\u6807\u51c6\u82f1\u8bed\u6570\u636e\u96c6FLUTE\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5728GEMMA\u548cLLAMA\u6a21\u578b\u4e0a\uff0cPMP\u65b9\u6cd5\u5728\u6240\u6709\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u56db\u79cd\u63d0\u793a\u7b56\u7565\u3002", "conclusion": "PMP\u80fd\u6709\u6548\u751f\u6210\u82f1\u8bed\u53d8\u4f53\u7684\u8bbd\u523a\u89e3\u91ca\uff0c\u4e14\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u53ef\u7f13\u89e3\u4e0a\u4e0b\u6587\u76f8\u5173\u9519\u8bef\u3002", "keywords": "\u8bbd\u523a\u68c0\u6d4b\uff0cPMP\uff0c\u53ef\u89e3\u91ca\u6027\uff0c\u82f1\u8bed\u53d8\u4f53\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.15047", "pdf": "https://arxiv.org/pdf/2505.15047", "abs": "https://arxiv.org/abs/2505.15047", "authors": ["Yingming Pu", "Tao Lin", "Hongyu Chen"], "title": "PiFlow: Principle-aware Scientific Discovery with Multi-Agent Collaboration", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate\nremarkable potential for scientific discovery. Existing approaches, however,\noften automate scientific discovery using predefined workflows that lack\nrationality constraints. This often leads to aimless hypothesizing and a\nfailure to consistently link hypotheses with evidence, thereby hindering\nsystematic uncertainty reduction. Overcoming these limitations fundamentally\nrequires systematic uncertainty reduction. We introduce \\texttt{PiFlow}, an\ninformation-theoretical framework, treating automated scientific discovery as a\nstructured uncertainty reduction problem guided by principles (e.g., scientific\nlaws). In evaluations across three distinct scientific domains -- discovering\nnanomaterial structures, bio-molecules, and superconductor candidates with\ntargeted properties -- our method significantly improves discovery efficiency,\nreflected by a 73.55\\% increase in the Area Under the Curve (AUC) of property\nvalues versus exploration steps, and enhances solution quality by 94.06\\%\ncompared to a vanilla agent system. Overall, \\texttt{PiFlow} serves as a\nPlug-and-Play method, establishing a novel paradigm shift in highly efficient\nautomated scientific discovery, paving the way for more robust and accelerated\nAI-driven research. Code is publicly available at our\n\\href{https://github.com/amair-lab/PiFlow}{GitHub}.", "AI": {"tldr": "PiFlow\u6846\u67b6\u901a\u8fc7\u4fe1\u606f\u8bba\u65b9\u6cd5\u4f18\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u79d1\u5b66\u53d1\u73b0\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u7ed3\u679c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7f3a\u4e4f\u7406\u6027\u7ea6\u675f\uff0c\u5bfc\u81f4\u5047\u8bbe\u4e0e\u8bc1\u636e\u8131\u8282\uff0c\u963b\u788d\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4fe1\u606f\u8bba\u6846\u67b6PiFlow\uff0c\u5c06\u79d1\u5b66\u53d1\u73b0\u89c6\u4e3a\u57fa\u4e8e\u79d1\u5b66\u6cd5\u5219\u7684\u7ed3\u6784\u5316\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e\u95ee\u9898\u3002", "result": "\u5728\u4e09\u4e2a\u79d1\u5b66\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cPiFlow\u7684AUC\u63d0\u534773.55\uff05\uff0c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u63d0\u9ad894.06\uff05\u3002", "conclusion": "PiFlow\u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u65b9\u6cd5\uff0c\u4e3a\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u4e86\u9ad8\u6548\u65b0\u8303\u5f0f\u3002", "keywords": "Large Language Model, multi-agent system, scientific discovery, uncertainty reduction, PiFlow"}}
{"id": "2505.15105", "pdf": "https://arxiv.org/pdf/2505.15105", "abs": "https://arxiv.org/abs/2505.15105", "authors": ["Aryaman Arora", "Neil Rathi", "Nikil Roashan Selvam", "R\u00f3bert Cs\u00f3rdas", "Dan Jurafsky", "Christopher Potts"], "title": "Mechanistic evaluation of Transformers and state space models", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "9 page main text, 6 pages appendix", "summary": "State space models (SSMs) for language modelling promise an efficient and\nperformant alternative to quadratic-attention Transformers, yet show variable\nperformance on recalling basic information from the context. While performance\non synthetic tasks like Associative Recall (AR) can point to this deficiency,\nbehavioural metrics provide little information as to why--on a mechanistic\nlevel--certain architectures fail and others succeed. To address this, we\nconduct experiments on AR and find that only Transformers and Based SSM models\nfully succeed at AR, with Mamba a close third, whereas the other SSMs (H3,\nHyena) fail. We then use causal interventions to explain why. We find that\nTransformers and Based learn to store key-value associations in-context using\ninduction heads. By contrast, the SSMs compute these associations only at the\nlast state, with only Mamba succeeding because of its short convolution\ncomponent. To extend and deepen these findings, we introduce Associative\nTreecall (ATR), a synthetic task similar to AR based on PCFG induction. ATR\nintroduces language-like hierarchical structure into the AR setting. We find\nthat all architectures learn the same mechanism as they did for AR, and the\nsame three models succeed at the task. These results reveal that architectures\nwith similar accuracy may still have substantive differences, motivating the\nadoption of mechanistic evaluations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u5728\u8bed\u8a00\u5efa\u6a21\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u53d1\u73b0Transformer\u548cBased SSM\u6a21\u578b\u5728\u5173\u8054\u53ec\u56de\uff08AR\uff09\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800c\u5176\u4ed6SSM\u6a21\u578b\u5982H3\u548cHyena\u5219\u5931\u8d25\u3002\u901a\u8fc7\u56e0\u679c\u5e72\u9884\uff0c\u63ed\u793a\u4e86\u6210\u529f\u6a21\u578b\u901a\u8fc7\u5f52\u7eb3\u5934\u5b58\u50a8\u952e\u503c\u5173\u8054\uff0c\u800c\u5931\u8d25\u6a21\u578b\u4ec5\u4f9d\u8d56\u6700\u540e\u72b6\u6001\u3002\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u67b6\u6784\u5373\u4f7f\u51c6\u786e\u7387\u76f8\u8fd1\uff0c\u673a\u5236\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63ed\u793a\u4e0d\u540cSSM\u67b6\u6784\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\u53ca\u5176\u80cc\u540e\u7684\u673a\u5236\uff0c\u7279\u522b\u662f\u5bf9\u5173\u8054\u53ec\u56de\u4efb\u52a1\uff08AR\uff09\u7684\u80fd\u529b\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\u5728AR\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u548c\u56e0\u679c\u5e72\u9884\u5206\u6790\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8ePCFG\u8bf1\u5bfc\u7684\u65b0\u4efb\u52a1\u2014\u2014\u5173\u8054\u6811\u53ec\u56de\uff08ATR\uff09\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4ec5Transformer\u548cBased SSM\u6a21\u578b\u80fd\u5b8c\u5168\u6210\u529f\u5b8c\u6210AR\u4efb\u52a1\uff0c\u800cMamba\u7a0d\u900a\uff0c\u5176\u4ed6SSM\u6a21\u578b\u5931\u8d25\u3002\u673a\u5236\u4e0a\uff0c\u6210\u529f\u6a21\u578b\u901a\u8fc7\u5f52\u7eb3\u5934\u5b58\u50a8\u952e\u503c\u5173\u8054\uff0c\u800c\u5931\u8d25\u6a21\u578b\u4ec5\u4f9d\u8d56\u6700\u540e\u72b6\u6001\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u4e0d\u540c\u67b6\u6784\u5373\u4f7f\u51c6\u786e\u7387\u76f8\u8fd1\uff0c\u5176\u673a\u5236\u53ef\u80fd\u6709\u663e\u8457\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u673a\u5236\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "keywords": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b, \u8bed\u8a00\u5efa\u6a21, \u5173\u8054\u53ec\u56de, Transformer, \u56e0\u679c\u5e72\u9884"}}
{"id": "2505.15064", "pdf": "https://arxiv.org/pdf/2505.15064", "abs": "https://arxiv.org/abs/2505.15064", "authors": ["Sho Sonoda", "Yuka Hashimoto", "Isao Ishikawa", "Masahiro Ikeda"], "title": "Generalization Through Growth: Hidden Dynamics Controls Depth Dependence", "categories": ["cs.LG", "math.DS", "stat.ML"], "comment": null, "summary": "Recent theory has reduced the depth dependence of generalization bounds from\nexponential to polynomial and even depth-independent rates, yet these results\nremain tied to specific architectures and Euclidean inputs. We present a\nunified framework for arbitrary \\blue{pseudo-metric} spaces in which a\ndepth-\\(k\\) network is the composition of continuous hidden maps\n\\(f:\\mathcal{X}\\to \\mathcal{X}\\) and an output map \\(h:\\mathcal{X}\\to\n\\mathbb{R}\\). The resulting bound $O(\\sqrt{(\\alpha + \\log \\beta(k))/n})$\nisolates the sole depth contribution in \\(\\beta(k)\\), the word-ball growth of\nthe semigroup generated by the hidden layers. By Gromov's theorem polynomial\n(resp. exponential) growth corresponds to virtually nilpotent (resp. expanding)\ndynamics, revealing a geometric dichotomy behind existing $O(\\sqrt{k})$\n(sublinear depth) and $\\tilde{O}(1)$ (depth-independent) rates. We further\nprovide covering-number estimates showing that expanding dynamics yield an\nexponential parameter saving via compositional expressivity. Our results\ndecouple specification from implementation, offering architecture-agnostic and\ndynamical-systems-aware guarantees applicable to modern deep-learning paradigms\nsuch as test-time inference and diffusion models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u4efb\u610f\u4f2a\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u6df1\u5ea6\u7f51\u7edc\u7684\u6cdb\u5316\u754c\u9650\uff0c\u63ed\u793a\u4e86\u6df1\u5ea6\u8d21\u732e\u4e0e\u7f51\u7edc\u52a8\u529b\u5b66\u51e0\u4f55\u7279\u6027\u4e4b\u95f4\u7684\u5173\u8054\u3002", "motivation": "\u5f53\u524d\u7684\u7406\u8bba\u4ec5\u9488\u5bf9\u7279\u5b9a\u67b6\u6784\u548c\u6b27\u51e0\u91cc\u5f97\u8f93\u5165\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u754c\u9650\u7684\u9002\u7528\u6027\u3002\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u4f2a\u5ea6\u91cf\u7a7a\u95f4\uff0c\u5e76\u63ed\u793a\u7f51\u7edc\u7684\u52a8\u529b\u5b66\u7279\u6027\u5982\u4f55\u5f71\u54cd\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5c06\u6df1\u5ea6\u7f51\u7edc\u89c6\u4e3a\u8fde\u7eed\u9690\u85cf\u6620\u5c04\u548c\u8f93\u51fa\u6620\u5c04\u7684\u7ec4\u5408\uff0c\u5229\u7528\u4f2a\u5ea6\u91cf\u7a7a\u95f4\u7684\u51e0\u4f55\u7279\u6027\uff08\u5982\u8bcd\u7403\u589e\u957f\u7684\u534a\u7fa4\uff09\u6765\u63a8\u5bfc\u6cdb\u5316\u754c\u9650\u3002", "result": "\u5f97\u51fa\u7684\u6cdb\u5316\u754c\u9650\u4e3a $O(\\sqrt{(\\alpha + \\log \\beta(k))/n})$\uff0c\u5176\u4e2d $\\beta(k)$ \u6355\u6349\u4e86\u6df1\u5ea6\u8d21\u732e\uff0c\u5e76\u63ed\u793a\u4e86\u591a\u9879\u5f0f\u589e\u957f\uff08\u865a\u62df\u5e42\u96f6\u52a8\u529b\u5b66\uff09\u4e0e\u6307\u6570\u589e\u957f\uff08\u6269\u5c55\u52a8\u529b\u5b66\uff09\u4e4b\u95f4\u7684\u51e0\u4f55\u4e8c\u5206\u73b0\u8c61\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u67b6\u6784\u65e0\u5173\u7684\u6cdb\u5316\u4fdd\u8bc1\uff0c\u8fd8\u901a\u8fc7\u8986\u76d6\u6570\u4f30\u8ba1\u5c55\u793a\u4e86\u6269\u5c55\u52a8\u529b\u5b66\u5982\u4f55\u901a\u8fc7\u7ec4\u5408\u8868\u8fbe\u80fd\u529b\u8282\u7701\u53c2\u6570\u3002\u8fd9\u4e00\u6846\u67b6\u9002\u7528\u4e8e\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u8303\u5f0f\uff0c\u5982\u6d4b\u8bd5\u65f6\u63a8\u7406\u548c\u6269\u6563\u6a21\u578b\u3002", "keywords": "\u6cdb\u5316\u754c\u9650, \u4f2a\u5ea6\u91cf\u7a7a\u95f4, \u7ec4\u5408\u8868\u8fbe\u80fd\u529b, \u534a\u7fa4\u52a8\u529b\u5b66, \u6df1\u5ea6\u7f51\u7edc"}}
{"id": "2505.15107", "pdf": "https://arxiv.org/pdf/2505.15107", "abs": "https://arxiv.org/abs/2505.15107", "authors": ["Ziliang Wang", "Xuhui Zheng", "Kang An", "Cijun Ouyang", "Jialu Cai", "Yuhang Wang", "Yichao Wu"], "title": "StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "20 pages, 6 figures", "summary": "Efficient multi-hop reasoning requires Large Language Models (LLMs) based\nagents to acquire high-value external knowledge iteratively. Previous work has\nexplored reinforcement learning (RL) to train LLMs to perform search-based\ndocument retrieval, achieving notable improvements in QA performance, but\nunderperform on complex, multi-hop QA resulting from the sparse rewards from\nglobal signal only. To address this gap in existing research, we introduce\nStepSearch, a framework for search LLMs that trained with step-wise proximal\npolicy optimization method. It consists of richer and more detailed\nintermediate search rewards and token-level process supervision based on\ninformation gain and redundancy penalties to better guide each search step. We\nconstructed a fine-grained question-answering dataset containing\nsub-question-level search trajectories based on open source datasets through a\nset of data pipeline method. On standard multi-hop QA benchmarks, it\nsignificantly outperforms global-reward baselines, achieving 11.2% and 4.2%\nabsolute improvements for 3B and 7B models over various search with RL\nbaselines using only 19k training data, demonstrating the effectiveness of\nfine-grained, stepwise supervision in optimizing deep search LLMs. Our\nimplementation is publicly available at\nhttps://github.com/zxh20001117/StepSearch.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faStepSearch\u6846\u67b6\uff0c\u901a\u8fc7\u6b65\u8fdb\u5f0f\u7b56\u7565\u4f18\u5316\u548c\u7ec6\u7c92\u5ea6\u5956\u52b1\u673a\u5236\u6539\u8fdb\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u641c\u7d22\u6027\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u641c\u7d22\u65b9\u6cd5\u5728\u590d\u6742\u591a\u8df3\u95ee\u7b54\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u4e3a\u5168\u5c40\u7a00\u758f\u5956\u52b1\u65e0\u6cd5\u6709\u6548\u6307\u5bfc\u6bcf\u4e00\u6b65\u641c\u7d22\u3002", "method": "\u91c7\u7528\u6b65\u8fdb\u5f0f\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u4fe1\u606f\u589e\u76ca\u548c\u5197\u4f59\u60e9\u7f5a\u7684\u4e2d\u95f4\u5956\u52b1\u673a\u5236\uff0c\u6784\u5efa\u7ec6\u7c92\u5ea6\u641c\u7d22\u8f68\u8ff9\u6570\u636e\u96c6\u3002", "result": "\u5728\u6807\u51c6\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c3B\u548c7B\u6a21\u578b\u5206\u522b\u5b9e\u73b011.2%\u548c4.2%\u7684\u7edd\u5bf9\u63d0\u5347\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u6b65\u8fdb\u5f0f\u76d1\u7763\u80fd\u6709\u6548\u4f18\u5316\u6df1\u5ea6\u641c\u7d22LLMs\uff0c\u4e14\u5728\u5c0f\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "keywords": "\u591a\u8df3\u63a8\u7406, \u5f3a\u5316\u5b66\u4e60, \u6b65\u8fdb\u5f0f\u4f18\u5316, \u4fe1\u606f\u589e\u76ca, \u7ec6\u7c92\u5ea6\u76d1\u7763"}}
{"id": "2505.15072", "pdf": "https://arxiv.org/pdf/2505.15072", "abs": "https://arxiv.org/abs/2505.15072", "authors": ["Xin Zhou", "Weiqing Wang", "Francisco J. Bald\u00e1n", "Wray Buntine", "Christoph Bergmeir"], "title": "MoTime: A Dataset Suite for Multimodal Time Series Forecasting", "categories": ["cs.LG", "cs.CL", "cs.DB", "cs.IR"], "comment": null, "summary": "While multimodal data sources are increasingly available from real-world\nforecasting, most existing research remains on unimodal time series. In this\nwork, we present MoTime, a suite of multimodal time series forecasting datasets\nthat pair temporal signals with external modalities such as text, metadata, and\nimages. Covering diverse domains, MoTime supports structured evaluation of\nmodality utility under two scenarios: 1) the common forecasting task, where\nvarying-length history is available, and 2) cold-start forecasting, where no\nhistorical data is available. Experiments show that external modalities can\nimprove forecasting performance in both scenarios, with particularly strong\nbenefits for short series in some datasets, though the impact varies depending\non data characteristics. By making datasets and findings publicly available, we\naim to support more comprehensive and realistic benchmarks in future multimodal\ntime series forecasting research.", "AI": {"tldr": "MoTime \u662f\u4e00\u4e2a\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6570\u636e\u96c6\u5957\u4ef6\uff0c\u7ed3\u5408\u4e86\u65f6\u95f4\u4fe1\u53f7\u4e0e\u6587\u672c\u3001\u5143\u6570\u636e\u548c\u56fe\u50cf\u7b49\u5916\u90e8\u6a21\u6001\uff0c\u652f\u6301\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8bc4\u4f30\u6a21\u6001\u6548\u7528\u3002", "motivation": "\u5f53\u524d\u591a\u6570\u7814\u7a76\u4ecd\u5c40\u9650\u4e8e\u5355\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\uff0c\u800c\u591a\u6a21\u6001\u6570\u636e\u5728\u771f\u5b9e\u4e16\u754c\u9884\u6d4b\u4e2d\u65e5\u76ca\u53ef\u7528\u3002\u56e0\u6b64\uff0cMoTime \u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u591a\u6a21\u6001\u9884\u6d4b\u7814\u7a76\u652f\u6301\u3002", "method": "MoTime \u6784\u5efa\u4e86\u4e00\u5957\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u8986\u76d6\u591a\u79cd\u9886\u57df\uff0c\u652f\u6301\u5728\u5e38\u89c1\u9884\u6d4b\u4efb\u52a1\uff08\u6709\u5386\u53f2\u6570\u636e\uff09\u548c\u51b7\u542f\u52a8\u9884\u6d4b\uff08\u65e0\u5386\u53f2\u6570\u636e\uff09\u4e24\u79cd\u573a\u666f\u4e0b\u8bc4\u4f30\u6a21\u6001\u6548\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5916\u90e8\u6a21\u6001\u53ef\u4ee5\u63d0\u5347\u4e24\u79cd\u573a\u666f\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u5bf9\u67d0\u4e9b\u6570\u636e\u96c6\u4e2d\u7684\u77ed\u5e8f\u5217\u6709\u663e\u8457\u5e2e\u52a9\uff0c\u4f46\u6548\u679c\u56e0\u6570\u636e\u7279\u5f81\u800c\u5f02\u3002", "conclusion": "MoTime \u63d0\u4f9b\u4e86\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u548c\u7814\u7a76\u6210\u679c\uff0c\u65e8\u5728\u652f\u6301\u672a\u6765\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7814\u7a76\u4e2d\u66f4\u5168\u9762\u548c\u73b0\u5b9e\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "keywords": "\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217,\u9884\u6d4b,\u6570\u636e\u96c6,\u51b7\u542f\u52a8\u9884\u6d4b,\u6a21\u6001\u6548\u7528"}}
{"id": "2505.15108", "pdf": "https://arxiv.org/pdf/2505.15108", "abs": "https://arxiv.org/abs/2505.15108", "authors": ["Ian Steenstra", "Timothy W. Bickmore"], "title": "A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The proliferation of Large Language Models (LLMs) and Intelligent Virtual\nAgents acting as psychotherapists presents significant opportunities for\nexpanding mental healthcare access. However, their deployment has also been\nlinked to serious adverse outcomes, including user harm and suicide,\nfacilitated by a lack of standardized evaluation methodologies capable of\ncapturing the nuanced risks of therapeutic interaction. Current evaluation\ntechniques lack the sensitivity to detect subtle changes in patient cognition\nand behavior during therapy sessions that may lead to subsequent\ndecompensation. We introduce a novel risk taxonomy specifically designed for\nthe systematic evaluation of conversational AI psychotherapists. Developed\nthrough an iterative process including review of the psychotherapy risk\nliterature, qualitative interviews with clinical and legal experts, and\nalignment with established clinical criteria (e.g., DSM-5) and existing\nassessment tools (e.g., NEQ, UE-ATR), the taxonomy aims to provide a structured\napproach to identifying and assessing user/patient harms. We provide a\nhigh-level overview of this taxonomy, detailing its grounding, and discuss\npotential use cases. We discuss two use cases in detail: monitoring cognitive\nmodel-based risk factors during a counseling conversation to detect unsafe\ndeviations, in both human-AI counseling sessions and in automated benchmarking\nof AI psychotherapists with simulated patients. The proposed taxonomy offers a\nfoundational step towards establishing safer and more responsible innovation in\nthe domain of AI-driven mental health support.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u9488\u5bf9AI\u5fc3\u7406\u6cbb\u7597\u5e08\u7684\u65b0\u578b\u98ce\u9669\u5206\u7c7b\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u6027\u8bc4\u4f30\u51cf\u5c11\u6f5c\u5728\u5371\u5bb3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u667a\u80fd\u865a\u62df\u4ee3\u7406\u4f5c\u4e3a\u5fc3\u7406\u6cbb\u7597\u5e08\u7684\u666e\u53ca\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u53ef\u80fd\u5bfc\u81f4\u7528\u6237\u4f24\u5bb3\u548c\u81ea\u6740\u7b49\u4e25\u91cd\u540e\u679c\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u3001\u4e13\u5bb6\u8bbf\u8c08\u5e76\u7ed3\u5408\u4e34\u5e8a\u6807\u51c6\uff08\u5982DSM-5\uff09\u548c\u73b0\u6709\u8bc4\u4f30\u5de5\u5177\uff08\u5982NEQ\u3001UE-ATR\uff09\u5f00\u53d1\u7684\u5206\u7c7b\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u98ce\u9669\u5206\u7c7b\u6cd5\uff0c\u9002\u7528\u4e8e\u4eba\u7c7b-AI\u4f1a\u8bdd\u548cAI\u81ea\u52a8\u5316\u8bc4\u4f30\u573a\u666f\uff0c\u4ee5\u68c0\u6d4b\u98ce\u9669\u56e0\u7d20\u3002", "conclusion": "\u8be5\u5206\u7c7b\u6cd5\u4e3aAI\u9a71\u52a8\u7684\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u548c\u8d1f\u8d23\u4efb\u7684\u521b\u65b0\u57fa\u7840\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5fc3\u7406\u6cbb\u7597, \u98ce\u9669\u8bc4\u4f30, \u5206\u7c7b\u6cd5, \u4eba\u5de5\u667a\u80fd"}}
{"id": "2505.15076", "pdf": "https://arxiv.org/pdf/2505.15076", "abs": "https://arxiv.org/abs/2505.15076", "authors": ["Nanxu Gong", "Sixun Dong", "Haoyue Bai", "Xinyuan Wang", "Wangyang Ying", "Yanjie Fu"], "title": "Agentic Feature Augmentation: Unifying Selection and Generation with Teaming, Planning, and Memories", "categories": ["cs.LG"], "comment": null, "summary": "As a widely-used and practical tool, feature engineering transforms raw data\ninto discriminative features to advance AI model performance. However, existing\nmethods usually apply feature selection and generation separately, failing to\nstrive a balance between reducing redundancy and adding meaningful dimensions.\nTo fill this gap, we propose an agentic feature augmentation concept, where the\nunification of feature generation and selection is modeled as agentic teaming\nand planning. Specifically, we develop a Multi-Agent System with Long and\nShort-Term Memory (MAGS), comprising a selector agent to eliminate redundant\nfeatures, a generator agent to produce informative new dimensions, and a router\nagent that strategically coordinates their actions. We leverage in-context\nlearning with short-term memory for immediate feedback refinement and long-term\nmemory for globally optimal guidance. Additionally, we employ offline Proximal\nPolicy Optimization (PPO) reinforcement fine-tuning to train the router agent\nfor effective decision-making to navigate a vast discrete feature space.\nExtensive experiments demonstrate that this unified agentic framework\nconsistently achieves superior task performance by intelligently orchestrating\nfeature selection and generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u7279\u5f81\u589e\u5f3a\u65b9\u6cd5\uff08MAGS\uff09\uff0c\u901a\u8fc7\u8054\u5408\u7279\u5f81\u9009\u62e9\u548c\u751f\u6210\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u7279\u5f81\u9009\u62e9\u548c\u751f\u6210\u5206\u5f00\u5904\u7406\uff0c\u96be\u4ee5\u5e73\u8861\u5197\u4f59\u51cf\u5c11\u548c\u6709\u610f\u4e49\u7ef4\u5ea6\u589e\u52a0\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u9009\u62e9\u3001\u751f\u6210\u548c\u8def\u7531\u4e09\u4e2a\u667a\u80fd\u4f53\u7684\u7cfb\u7edf\uff0c\u5229\u7528\u957f\u77ed\u65f6\u8bb0\u5fc6\u548c\u79bb\u7ebfPPO\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u901a\u8fc7\u667a\u80fd\u534f\u8c03\u7279\u5f81\u9009\u62e9\u548c\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "MAGS\u6846\u67b6\u4e3a\u7279\u5f81\u5de5\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u7edf\u4e00\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "keywords": "\u7279\u5f81\u5de5\u7a0b,\u591a\u667a\u80fd\u4f53\u7cfb\u7edf,\u5f3a\u5316\u5b66\u4e60,\u7279\u5f81\u9009\u62e9,\u7279\u5f81\u751f\u6210"}}
{"id": "2505.15110", "pdf": "https://arxiv.org/pdf/2505.15110", "abs": "https://arxiv.org/abs/2505.15110", "authors": ["Xuanliang Zhang", "Dingzirui Wang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che"], "title": "RoT: Enhancing Table Reasoning with Iterative Row-Wise Traversals", "categories": ["cs.CL"], "comment": null, "summary": "The table reasoning task, crucial for efficient data acquisition, aims to\nanswer questions based on the given table. Recently, reasoning large language\nmodels (RLLMs) with Long Chain-of-Thought (Long CoT) significantly enhance\nreasoning capabilities, leading to brilliant performance on table reasoning.\nHowever, Long CoT suffers from high cost for training and exhibits low\nreliability due to table content hallucinations. Therefore, we propose\nRow-of-Thought (RoT), which performs iteratively row-wise table traversal,\nallowing for reasoning extension and reflection-based refinement at each\ntraversal. Scaling reasoning length by row-wise traversal and leveraging\nreflection capabilities of LLMs, RoT is training-free. The sequential traversal\nencourages greater attention to the table, thus reducing hallucinations.\nExperiments show that RoT, using non-reasoning models, outperforms RLLMs by an\naverage of 4.3%, and achieves state-of-the-art results on WikiTableQuestions\nand TableBench with comparable models, proving its effectiveness. Also, RoT\noutperforms Long CoT with fewer reasoning tokens, indicating higher efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aRow-of-Thought\uff08RoT\uff09\u7684\u8bad\u7ec3\u514d\u8d39\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u884c\u904d\u5386\u8868\u683c\u5e76\u7ed3\u5408\u53cd\u601d\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8868\u683c\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5e7b\u89c9\u5185\u5bb9\u3002", "motivation": "\u73b0\u6709\u7684Long Chain-of-Thought\uff08Long CoT\uff09\u65b9\u6cd5\u867d\u7136\u5728\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u8868\u683c\u5185\u5bb9\u5e7b\u89c9\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u65b9\u6cd5\u3002", "method": "RoT\u901a\u8fc7\u8fed\u4ee3\u7684\u9010\u884c\u8868\u683c\u904d\u5386\uff0c\u5141\u8bb8\u5728\u6bcf\u6b21\u904d\u5386\u4e2d\u8fdb\u884c\u63a8\u7406\u6269\u5c55\u548c\u57fa\u4e8e\u53cd\u601d\u7684\u4f18\u5316\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53cd\u601d\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRoT\u5728\u975e\u63a8\u7406\u6a21\u578b\u4e0a\u7684\u8868\u73b0\u5e73\u5747\u4f18\u4e8eRLLMs\u8fbe4.3%\uff0c\u5e76\u5728WikiTableQuestions\u548cTableBench\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0cRoT\u7684\u63a8\u7406\u4ee4\u724c\u6570\u5c11\u4e8eLong CoT\uff0c\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "RoT\u901a\u8fc7\u9010\u884c\u904d\u5386\u548c\u53cd\u601d\u673a\u5236\uff0c\u4e0d\u4ec5\u964d\u4f4e\u4e86\u5185\u5bb9\u5e7b\u89c9\u7684\u98ce\u9669\uff0c\u8fd8\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u4e3a\u8868\u683c\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u8868\u683c\u63a8\u7406, Row-of-Thought, \u5e7b\u89c9\u51cf\u5c11, \u8bad\u7ec3\u514d\u8d39\u65b9\u6cd5, \u72b6\u6001\u6700\u5148\u8fdb"}}
{"id": "2505.15080", "pdf": "https://arxiv.org/pdf/2505.15080", "abs": "https://arxiv.org/abs/2505.15080", "authors": ["Sergey Pankov", "Georges Harik"], "title": "SUS backprop: linear backpropagation algorithm for long inputs in transformers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "21 pages, 9 figures", "summary": "It is straightforward to design an unbiased gradient estimator that\nstochastically cuts the backpropagation flow through any part of a\ncomputational graph. By cutting the parts that have little effect on the\ncomputation, one can potentially save a significant amount of back-propagation\ncomputation in exchange for a minimal increase in the stochastic gradient\nvariance, in some situations. Such a situation occurs in the attention\nmechanism of the transformer architecture. For long sequences, attention\nbecomes the limiting factor, as its compute requirements increase quadratically\nwith sequence length $n$. At the same time, most attention weights become very\nsmall, as most attention heads tend to connect a given token with only a small\nfraction of other tokens in the sequence. These weights become promising\ntargets for cutting backpropagation. We propose a simple probabilistic rule\ncontrolled by a single parameter $c$ that cuts backpropagation through most\nattention weights, leaving at most $c$ interactions per token per attention\nhead. This brings a factor of $c/n$ reduction in the compute required for the\nattention backpropagation, turning it from quadratic $O(n^2)$ to linear\ncomplexity $O(nc)$. We have empirically verified that, for a typical\ntransformer model, cutting $99\\%$ of the attention gradient flow (i.e. choosing\n$c \\sim 20-30$) results in relative gradient variance increase of only about\n$1\\%$ for $n \\sim 2000$, and it decreases with $n$. This approach is amenable\nto efficient sparse matrix implementation, thus being promising for making the\ncost of a backward pass negligible relative to the cost of a forward pass when\ntraining a transformer model on long sequences.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u968f\u673a\u5207\u65ad\u53cd\u5411\u4f20\u64ad\u6d41\u4ee5\u51cf\u5c11\u8ba1\u7b97\u91cf\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8eTransformer\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u4e8c\u6b21\u964d\u4f4e\u5230\u7ebf\u6027\u3002", "motivation": "\u5728Transformer\u67b6\u6784\u7684\u6ce8\u610f\u529b\u673a\u5236\u4e2d\uff0c\u957f\u5e8f\u5217\u7684\u8ba1\u7b97\u9700\u6c42\u5448\u4e8c\u6b21\u589e\u957f\uff0c\u4f46\u5927\u591a\u6570\u6ce8\u610f\u529b\u6743\u91cd\u5f88\u5c0f\u3002\u901a\u8fc7\u968f\u673a\u5207\u65ad\u53cd\u5411\u4f20\u64ad\u6d41\uff0c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u68af\u5ea6\u65b9\u5dee\u7684\u589e\u52a0\u5f88\u5c0f\u3002", "method": "\u4f7f\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u6982\u7387\u89c4\u5219\uff0c\u901a\u8fc7\u5355\u4e00\u53c2\u6570c\u63a7\u5236\u6bcf\u6b21\u53cd\u5411\u4f20\u64ad\u4e2d\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u6700\u591a\u4fdd\u7559c\u4e2a\u4ea4\u4e92\uff0c\u5c06\u6ce8\u610f\u529b\u53cd\u5411\u4f20\u64ad\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(n\u00b2)\u964d\u4e3aO(nc)\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5207\u65ad99%\u7684\u6ce8\u610f\u529b\u68af\u5ea6\u6d41\uff08c\u224820-30\uff09\u5bf9\u68af\u5ea6\u65b9\u5dee\u7684\u5f71\u54cd\u4ec5\u7ea61%\uff08n\u22482000\uff09\uff0c\u4e14\u968fn\u589e\u5927\u5f71\u54cd\u51cf\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u5b9e\u73b0\u7a00\u758f\u77e9\u9635\u8ba1\u7b97\uff0c\u663e\u8457\u964d\u4f4e\u957f\u5e8f\u5217\u8bad\u7ec3\u65f6\u7684\u53cd\u5411\u4f20\u64ad\u6210\u672c\u3002", "keywords": "\u53cd\u5411\u4f20\u64ad\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001Transformer\u3001\u68af\u5ea6\u4f30\u8ba1\u3001\u8ba1\u7b97\u4f18\u5316"}}
{"id": "2505.15117", "pdf": "https://arxiv.org/pdf/2505.15117", "abs": "https://arxiv.org/abs/2505.15117", "authors": ["Bowen Jin", "Jinsung Yoon", "Priyanka Kargupta", "Sercan O. Arik", "Jiawei Han"], "title": "An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "22 pages", "summary": "Reinforcement learning (RL) has demonstrated strong potential in training\nlarge language models (LLMs) capable of complex reasoning for real-world\nproblem solving. More recently, RL has been leveraged to create sophisticated\nLLM-based search agents that adeptly combine reasoning with search engine use.\nWhile the use of RL for training search agents is promising, the optimal design\nof such agents remains not fully understood. In particular, key factors -- such\nas (1) reward formulation, (2) the choice and characteristics of the underlying\nLLM, and (3) the role of the search engine in the RL process -- require further\ninvestigation. In this work, we conduct comprehensive empirical studies to\nsystematically investigate these and offer actionable insights. We highlight\nseveral key findings: format rewards are effective in improving final\nperformance, whereas intermediate retrieval rewards have limited impact; the\nscale and initialization of the LLM (general-purpose vs. reasoning-specialized)\nsignificantly influence RL outcomes; and the choice of search engine plays a\ncritical role in shaping RL training dynamics and the robustness of the trained\nagent during inference. These establish important guidelines for successfully\nbuilding and deploying LLM-based search agents in real-world applications. Code\nis available at https://github.com/PeterGriffinJin/Search-R1.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u8bad\u7ec3\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u641c\u7d22\u4ee3\u7406\u4e2d\u7684\u5173\u952e\u8bbe\u8ba1\u56e0\u7d20\uff0c\u5305\u62ec\u5956\u52b1\u516c\u5f0f\u3001LLM\u7684\u9009\u62e9\u548c\u641c\u7d22\u5f15\u64ce\u7684\u4f5c\u7528\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002", "motivation": "RL\u88ab\u7528\u4e8e\u8bad\u7ec3LLM\u4ee5\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\uff0c\u4f46\u57fa\u4e8eRL\u7684\u641c\u7d22\u4ee3\u7406\u7684\u6700\u4f18\u8bbe\u8ba1\u5c1a\u4e0d\u5b8c\u5168\u6e05\u695a\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5173\u952e\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u5168\u9762\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u7cfb\u7edf\u8c03\u67e5\u4e86\u5956\u52b1\u516c\u5f0f\u3001LLM\u9009\u62e9\u548c\u641c\u7d22\u5f15\u64ce\u5728RL\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u53d1\u73b0\u683c\u5f0f\u5316\u5956\u52b1\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0cLLM\u7684\u89c4\u6a21\u548c\u521d\u59cb\u5316\u5bf9RL\u7ed3\u679c\u5f71\u54cd\u91cd\u5927\uff0c\u641c\u7d22\u5f15\u64ce\u9009\u62e9\u5bf9\u8bad\u7ec3\u52a8\u6001\u548c\u4ee3\u7406\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u6784\u5efa\u548c\u90e8\u7f72LLM\u641c\u7d22\u4ee3\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u641c\u7d22\u4ee3\u7406,\u5956\u52b1\u516c\u5f0f,\u641c\u7d22\u5f15\u64ce"}}
{"id": "2505.15083", "pdf": "https://arxiv.org/pdf/2505.15083", "abs": "https://arxiv.org/abs/2505.15083", "authors": ["Jeremy Qin"], "title": "Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series forecasting plays a crucial role in various applications,\nparticularly in healthcare, where accurate predictions of future health\ntrajectories can significantly impact clinical decision-making. Ensuring\ntransparency and explainability of the models responsible for these tasks is\nessential for their adoption in critical settings. Recent work has explored a\ntop-down approach to bi-level transparency, focusing on understanding trends\nand properties of predicted time series using static features. In this work, we\nextend this framework by incorporating exogenous time series features alongside\nstatic features in a structured manner, while maintaining cohesive\ninterpretation. Our approach leverages the insights of trajectory comprehension\nto introduce an encoding mechanism for exogenous time series, where they are\ndecomposed into meaningful trends and properties, enabling the extraction of\ninterpretable patterns. Through experiments on several synthetic datasets, we\ndemonstrate that our approach remains predictive while preserving\ninterpretability and robustness. This work represents a step towards developing\nrobust, and generalized time series forecasting models. The code is available\nat https://github.com/jeremy-qin/TIMEVIEW", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9759\u6001\u7279\u5f81\u548c\u5916\u751f\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\u7684\u53cc\u5c42\u900f\u660e\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u533b\u7597\u7b49\u9886\u57df\uff0c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u5bf9\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5c06\u5916\u751f\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\u4e0e\u9759\u6001\u7279\u5f81\u7ed3\u5408\uff0c\u901a\u8fc7\u5206\u89e3\u4e3a\u8d8b\u52bf\u548c\u5c5e\u6027\u63d0\u53d6\u53ef\u89e3\u91ca\u6a21\u5f0f\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u9c81\u68d2\u4e14\u901a\u7528\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b,\u53ef\u89e3\u91ca\u6027,\u5916\u751f\u7279\u5f81,\u533b\u7597\u5e94\u7528"}}
{"id": "2505.14838", "pdf": "https://arxiv.org/pdf/2505.14838", "abs": "https://arxiv.org/abs/2505.14838", "authors": ["Hiba Arnaout", "Noy Sternlicht", "Tom Hope", "Iryna Gurevych"], "title": "In-depth Research Impact Summarization through Fine-Grained Temporal Citation Analysis", "categories": ["cs.DL", "cs.AI"], "comment": null, "summary": "Understanding the impact of scientific publications is crucial for\nidentifying breakthroughs and guiding future research. Traditional metrics\nbased on citation counts often miss the nuanced ways a paper contributes to its\nfield. In this work, we propose a new task: generating nuanced, expressive, and\ntime-aware impact summaries that capture both praise (confirmation citations)\nand critique (correction citations) through the evolution of fine-grained\ncitation intents. We introduce an evaluation framework tailored to this task,\nshowing moderate to strong human correlation on subjective metrics such as\ninsightfulness. Expert feedback from professors reveals a strong interest in\nthese summaries and suggests future improvements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u9879\u65b0\u4efb\u52a1\uff1a\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5f15\u7528\u610f\u56fe\u7684\u6f14\u53d8\uff0c\u751f\u6210\u8868\u8fbe\u529b\u5f3a\u4e14\u65f6\u95f4\u654f\u611f\u7684\u8bba\u6587\u5f71\u54cd\u603b\u7ed3\uff0c\u5305\u62ec\u8d5e\u7f8e\u548c\u6279\u8bc4\u5f15\u7528\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5f15\u7528\u6b21\u6570\u7684\u6307\u6807\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u8bba\u6587\u7684\u8d21\u732e\uff0c\u9700\u8981\u66f4\u7ec6\u81f4\u7684\u8bc4\u4f30\u65b9\u5f0f\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u8868\u8fbe\u529b\u5f3a\u4e14\u65f6\u95f4\u654f\u611f\u7684\u8bba\u6587\u5f71\u54cd\u603b\u7ed3\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u53cd\u9988\u4f18\u5316\u3002", "result": "\u4eba\u7c7b\u5728\u4e3b\u89c2\u6307\u6807\uff08\u5982\u6d1e\u5bdf\u529b\uff09\u4e0a\u8868\u73b0\u51fa\u4e2d\u7b49\u81f3\u5f3a\u76f8\u5173\u6027\uff0c\u4e13\u5bb6\u5bf9\u603b\u7ed3\u8868\u73b0\u51fa\u6d53\u539a\u5174\u8da3\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u8bba\u6587\u5f71\u54cd\u63d0\u4f9b\u4e86\u66f4\u7ec6\u81f4\u7684\u8bc4\u4f30\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "keywords": "\u8bba\u6587\u5f71\u54cd\u3001\u5f15\u7528\u610f\u56fe\u3001\u8bc4\u4f30\u6846\u67b6\u3001\u4e13\u5bb6\u53cd\u9988"}}
{"id": "2505.15154", "pdf": "https://arxiv.org/pdf/2505.15154", "abs": "https://arxiv.org/abs/2505.15154", "authors": ["Jinghui Lu", "Haiyang Yu", "Siliang Xu", "Shiwei Ran", "Guozhi Tang", "Siqi Wang", "Bin Shan", "Teng Fu", "Hao Feng", "Jingqun Tang", "Han Wang", "Can Huang"], "title": "Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": null, "summary": "Recent advancements in reasoning have significantly enhanced the capabilities\nof Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nacross diverse tasks. However, excessive reliance on chain-of-thought (CoT)\nreasoning can impair model performance and brings unnecessarily lengthened\noutputs, reducing efficiency. Our work reveals that prolonged reasoning does\nnot universally improve accuracy and even degrade performance on simpler tasks.\nTo address this, we propose Certainty-based Adaptive Reasoning (CAR), a novel\nframework that dynamically switches between short answers and long-form\nreasoning based on the model perplexity. CAR first generates a short answer and\nevaluates its perplexity, triggering reasoning only when the model exhibits low\nconfidence (i.e., high perplexity). Experiments across diverse multimodal\nVQA/KIE benchmarks and text reasoning datasets show that CAR outperforms both\nshort-answer and long-form reasoning approaches, striking an optimal balance\nbetween accuracy and efficiency.", "AI": {"tldr": "CAR\u6846\u67b6\u901a\u8fc7\u57fa\u4e8e\u56f0\u60d1\u5ea6\u52a8\u6001\u5207\u6362\u7b80\u77ed\u56de\u7b54\u4e0e\u957f\u63a8\u7406\uff0c\u4f18\u5316\u51c6\u786e\u6027\u4e0e\u6548\u7387\u3002", "motivation": "\u8fc7\u5ea6\u4f9d\u8d56\u94fe\u5f0f\u63a8\u7406\uff08CoT\uff09\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u548c\u8f93\u51fa\u5197\u957f\uff0cCAR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faCAR\u6846\u67b6\uff0c\u9996\u5148\u751f\u6210\u7b80\u77ed\u7b54\u6848\u5e76\u8bc4\u4f30\u56f0\u60d1\u5ea6\uff0c\u4ec5\u5f53\u56f0\u60d1\u5ea6\u9ad8\u65f6\u89e6\u53d1\u63a8\u7406\u3002", "result": "\u5728\u591a\u79cdVQA/KIE\u57fa\u51c6\u6d4b\u8bd5\u548c\u6587\u672c\u63a8\u7406\u6570\u636e\u96c6\u4e2d\uff0cCAR\u8868\u73b0\u4f18\u4e8e\u7b80\u77ed\u56de\u7b54\u548c\u957f\u63a8\u7406\u65b9\u6cd5\u3002", "conclusion": "CAR\u5728\u51c6\u786e\u6027\u4e0e\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6700\u4f73\u5e73\u8861\u3002", "keywords": "CAR, CoT, perplexity, reasoning, efficiency"}}
{"id": "2505.15101", "pdf": "https://arxiv.org/pdf/2505.15101", "abs": "https://arxiv.org/abs/2505.15101", "authors": ["Eray Can Elumar", "Cem Tekin", "Osman Yagan"], "title": "Cost-aware LLM-based Online Dataset Annotation", "categories": ["cs.LG", "cs.IT", "math.IT"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled automated\ndataset labeling with minimal human supervision. While majority voting across\nmultiple LLMs can improve label reliability by mitigating individual model\nbiases, it incurs high computational costs due to repeated querying. In this\nwork, we propose a novel online framework, Cost-aware Majority Voting (CaMVo),\nfor efficient and accurate LLM-based dataset annotation. CaMVo adaptively\nselects a subset of LLMs for each data instance based on contextual embeddings,\nbalancing confidence and cost without requiring pre-training or ground-truth\nlabels. Leveraging a LinUCB-based selection mechanism and a Bayesian estimator\nover confidence scores, CaMVo estimates a lower bound on labeling accuracy for\neach LLM and aggregates responses through weighted majority voting. Our\nempirical evaluation on the MMLU and IMDB Movie Review datasets demonstrates\nthat CaMVo achieves comparable or superior accuracy to full majority voting\nwhile significantly reducing labeling costs. This establishes CaMVo as a\npractical and robust solution for cost-efficient annotation in dynamic labeling\nenvironments.", "AI": {"tldr": "CaMVo\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5728\u7ebf\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5d4c\u5165\u81ea\u9002\u5e94\u9009\u62e9LLM\u5b50\u96c6\uff0c\u5e73\u8861\u7f6e\u4fe1\u5ea6\u548c\u6210\u672c\uff0c\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u6570\u636e\u96c6\u6807\u6ce8\u3002", "motivation": "\u867d\u7136\u591aLLM\u591a\u6570\u6295\u7968\u80fd\u63d0\u9ad8\u6807\u7b7e\u53ef\u9760\u6027\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u65e2\u9ad8\u6548\u53c8\u51c6\u786e\u7684\u6807\u6ce8\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCaMVo\u6846\u67b6\uff0c\u7ed3\u5408LinUCB\u9009\u62e9\u673a\u5236\u548c\u8d1d\u53f6\u65af\u4f30\u8ba1\u5668\uff0c\u52a8\u6001\u9009\u62e9LLM\u5e76\u52a0\u6743\u6295\u7968\u3002", "result": "\u5728MMLU\u548cIMDB\u6570\u636e\u96c6\u4e0a\uff0cCaMVo\u5728\u4fdd\u6301\u6216\u4f18\u4e8e\u591a\u6570\u6295\u7968\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6210\u672c\u3002", "conclusion": "CaMVo\u4e3a\u52a8\u6001\u6807\u6ce8\u73af\u5883\u63d0\u4f9b\u4e86\u5b9e\u7528\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "LLM, \u6570\u636e\u96c6\u6807\u6ce8, \u591a\u6570\u6295\u7968, \u6210\u672c\u6548\u7387, \u52a8\u6001\u9009\u62e9"}}
{"id": "2505.14841", "pdf": "https://arxiv.org/pdf/2505.14841", "abs": "https://arxiv.org/abs/2505.14841", "authors": ["Yuchen Tian", "Assel Kembay", "Nhan Duy Truong", "Jason K. Eshraghian", "Omid Kavehei"], "title": "Beyond Pairwise Plasticity: Group-Level Spike Synchrony Facilitates Efficient Learning in Spiking Neural Networks", "categories": ["cs.NE", "cs.AI"], "comment": "22 pages, 7 figures, 5 tables. This work proposes SSDP, a\n  biologically inspired spike-synchrony-dependent plasticity rule. We\n  demonstrate its effectiveness across shallow and deep spiking architectures\n  including Spiking-ResNet18 and SNN-Transformer", "summary": "Brain networks rely on precise spike timing and coordinated activity to\nsupport robust and energy-efficient learning. Inspired by these principles,\nspiking neural networks (SNNs) are widely regarded as promising candidates for\nlow-power, event-driven computing. However, most biologically-inspired learning\nrules employed in SNNs, including spike-timing-dependent plasticity (STDP),\nrely on isolated spike pairs and lack sensitivity to population-level activity.\nThis limits their stability and generalization, particularly in noisy and\nfast-changing environments. Motivated by biological observations that neural\nsynchrony plays a central role in learning and memory, we introduce a\nspike-synchrony-dependent plasticity (SSDP) rule that adjusts synaptic weights\nbased on the degree of coordinated firing among neurons. SSDP supports stable\nand scalable learning by encouraging neurons to form coherent activity\npatterns. One prominent outcome is a sudden transition from unstable to stable\ndynamics during training, suggesting that synchrony may drive convergence\ntoward equilibrium firing regimes. We demonstrate SSDP's effectiveness across\nmultiple network types, from minimal-layer models to spiking ResNets and\nSNN-Transformer. To our knowledge, this is the first application of a synaptic\nplasticity mechanism in a spiking transformer. SSDP operates in a fully\nevent-driven manner and incurs minimal computational cost, making it\nwell-suited for neuromorphic deployment. In this approach, local synaptic\nmodifications are associated with the collective dynamics of neural networks,\nresulting in a learning strategy that adheres to biological principles while\nmaintaining practical efficiency, these findings position SSDP as a\ngeneral-purpose optimization strategy for SNNs, while offering new insights\ninto population-based learning mechanisms in the brain.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSSDP\u7684\u5b66\u4e60\u89c4\u5219\uff0c\u901a\u8fc7\u534f\u8c03\u795e\u7ecf\u5143\u4e4b\u95f4\u7684\u540c\u6b65\u6d3b\u52a8\u6765\u8c03\u6574\u7a81\u89e6\u6743\u91cd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfSNNs\u5b66\u4e60\u89c4\u5219\u5728\u566a\u58f0\u548c\u5feb\u901f\u53d8\u5316\u73af\u5883\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u53d7\u751f\u7269\u5b66\u4e2d\u795e\u7ecf\u540c\u6b65\u6027\u5728\u5b66\u4e60\u4e0e\u8bb0\u5fc6\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u542f\u53d1\uff0c\u4f5c\u8005\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5143\u7fa4\u4f53\u6d3b\u52a8\u7684\u5b66\u4e60\u89c4\u5219\uff0c\u4ee5\u63d0\u9ad8SNNs\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5143\u540c\u6b65\u653e\u7535\u7684\u7a81\u89e6\u53ef\u5851\u6027\u89c4\u5219\uff08SSDP\uff09\uff0c\u901a\u8fc7\u8c03\u6574\u7a81\u89e6\u6743\u91cd\u4ee5\u9f13\u52b1\u795e\u7ecf\u5143\u5f62\u6210\u4e00\u81f4\u7684\u6d3b\u52a8\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSSDP\u80fd\u591f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u4ece\u4e0d\u7a33\u5b9a\u5230\u7a33\u5b9a\u7684\u52a8\u6001\u8f6c\u53d8\uff0c\u5e76\u5728\u591a\u79cd\u7f51\u7edc\u7c7b\u578b\u4e2d\uff08\u5305\u62ecSNN-Transformer\uff09\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SSDP\u4e0d\u4ec5\u7b26\u5408\u751f\u7269\u5b66\u539f\u7406\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\uff0c\u9002\u5408\u795e\u7ecf\u5f62\u6001\u90e8\u7f72\uff0c\u4e3aSNNs\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u4e3a\u8111\u5185\u7fa4\u4f53\u5b66\u4e60\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "keywords": "spiking neural networks, synaptic plasticity, neural synchrony, SSDP, event-driven computing"}}
{"id": "2505.15182", "pdf": "https://arxiv.org/pdf/2505.15182", "abs": "https://arxiv.org/abs/2505.15182", "authors": ["Jeonghye Kim", "Sojeong Rhee", "Minbeom Kim", "Dohyung Kim", "Sangmook Lee", "Youngchul Sung", "Kyomin Jung"], "title": "ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in LLM agents have largely built on reasoning backbones like\nReAct, which interleave thought and action in complex environments. However,\nReAct often produces ungrounded or incoherent reasoning steps, leading to\nmisalignment between the agent's actual state and goal. Our analysis finds that\nthis stems from ReAct's inability to maintain consistent internal beliefs and\ngoal alignment, causing compounding errors and hallucinations. To address this,\nwe introduce ReflAct, a novel backbone that shifts reasoning from merely\nplanning next actions to continuously reflecting on the agent's state relative\nto its goal. By explicitly grounding decisions in states and enforcing ongoing\ngoal alignment, ReflAct dramatically improves strategic reliability. This\ndesign delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7%\non average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even\noutperforms ReAct with added enhancement modules (e.g., Reflexion, WKM),\nshowing that strengthening the core reasoning backbone is key to reliable agent\nperformance.", "AI": {"tldr": "ReflAct\u901a\u8fc7\u6301\u7eed\u53cd\u601d\u4ee3\u7406\u72b6\u6001\u4e0e\u76ee\u6807\u7684\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u4e86ReAct\u56e0\u4fe1\u5ff5\u4e0d\u4e00\u81f4\u548c\u76ee\u6807\u5bf9\u9f50\u4e0d\u8db3\u5bfc\u81f4\u7684\u63a8\u7406\u9519\u8bef\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u7684\u6218\u7565\u53ef\u9760\u6027\u3002", "motivation": "ReAct\u5728\u590d\u6742\u73af\u5883\u4e2d\u5e38\u56e0\u63a8\u7406\u6b65\u9aa4\u4e0d\u5207\u5b9e\u9645\u6216\u4e0d\u8fde\u8d2f\u800c\u5bfc\u81f4\u4ee3\u7406\u72b6\u6001\u4e0e\u76ee\u6807\u4e0d\u4e00\u81f4\uff0cReflAct\u65e8\u5728\u901a\u8fc7\u6301\u7eed\u53cd\u601d\u548c\u72b6\u6001\u5bf9\u9f50\u6539\u8fdb\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86ReflAct\uff0c\u5c06\u63a8\u7406\u4ece\u4ec5\u89c4\u5212\u4e0b\u4e00\u6b65\u884c\u52a8\u8f6c\u53d8\u4e3a\u6301\u7eed\u53cd\u601d\u4ee3\u7406\u72b6\u6001\u4e0e\u76ee\u6807\u7684\u76f8\u5bf9\u5173\u7cfb\uff0c\u5e76\u663e\u5f0f\u5730\u5c06\u51b3\u7b56\u57fa\u4e8e\u72b6\u6001\u4e0e\u76ee\u6807\u5bf9\u9f50\u3002", "result": "ReflAct\u6bd4ReAct\u5e73\u5747\u8868\u73b0\u63d0\u5347\u4e8627.7%\uff0c\u5728ALFWorld\u4e2d\u8fbe\u5230\u4e8693.3%\u7684\u6210\u529f\u7387\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u589e\u5f3a\u7248\u7684ReAct\u3002", "conclusion": "\u5f3a\u5316\u6838\u5fc3\u63a8\u7406\u4e3b\u5e72\u662f\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u7684\u5173\u952e\uff0cReflAct\u901a\u8fc7\u6301\u7eed\u53cd\u601d\u548c\u72b6\u6001\u5bf9\u9f50\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7406\u7684\u6218\u7565\u53ef\u9760\u6027\u3002", "keywords": "LLM\u4ee3\u7406, ReAct, ReflAct, \u72b6\u6001\u5bf9\u9f50, \u4ee3\u7406\u63a8\u7406"}}
{"id": "2505.15103", "pdf": "https://arxiv.org/pdf/2505.15103", "abs": "https://arxiv.org/abs/2505.15103", "authors": ["Zihu Wang", "Boxun Xu", "Hejia Geng", "Peng Li"], "title": "Khan-GCL: Kolmogorov-Arnold Network Based Graph Contrastive Learning with Hard Negatives", "categories": ["cs.LG"], "comment": "Graph Contrastive Learning, Self-supervised Learning,\n  Kolmogorov-Arnold Network, Representation Learning", "summary": "Graph contrastive learning (GCL) has demonstrated great promise for learning\ngeneralizable graph representations from unlabeled data. However, conventional\nGCL approaches face two critical limitations: (1) the restricted expressive\ncapacity of multilayer perceptron (MLP) based encoders, and (2) suboptimal\nnegative samples that either from random augmentations-failing to provide\neffective 'hard negatives'-or generated hard negatives without addressing the\nsemantic distinctions crucial for discriminating graph data. To this end, we\npropose Khan-GCL, a novel framework that integrates the Kolmogorov-Arnold\nNetwork (KAN) into the GCL encoder architecture, substantially enhancing its\nrepresentational capacity. Furthermore, we exploit the rich information\nembedded within KAN coefficient parameters to develop two novel critical\nfeature identification techniques that enable the generation of semantically\nmeaningful hard negative samples for each graph representation. These\nstrategically constructed hard negatives guide the encoder to learn more\ndiscriminative features by emphasizing critical semantic differences between\ngraphs. Extensive experiments demonstrate that our approach achieves\nstate-of-the-art performance compared to existing GCL methods across a variety\nof datasets and tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faKhan-GCL\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408KAN\u7f51\u7edc\u5230GCL\u7f16\u7801\u5668\u4e2d\u63d0\u5347\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5229\u7528KAN\u53c2\u6570\u751f\u6210\u8bed\u4e49\u6709\u610f\u4e49\u7684\u786c\u8d1f\u6837\u672c\uff0c\u4ece\u800c\u5b66\u4e60\u66f4\u5177\u533a\u5206\u6027\u7684\u7279\u5f81\u3002", "motivation": "\u4f20\u7edfGCL\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5c40\u9650\uff1aMLP\u7f16\u7801\u5668\u8868\u8fbe\u80fd\u529b\u4e0d\u8db3\uff0c\u4ee5\u53ca\u8d1f\u6837\u672c\u751f\u6210\u65b9\u5f0f\u4e0d\u7406\u60f3\uff08\u968f\u673a\u751f\u6210\u7684\u8d1f\u6837\u672c\u7f3a\u4e4f\u8bed\u4e49\u533a\u5206\u6027\u6216\u786c\u8d1f\u6837\u672c\u672a\u8003\u8651\u8bed\u4e49\u5dee\u5f02\uff09\u3002", "method": "\u63d0\u51faKhan-GCL\u6846\u67b6\uff0c\u96c6\u6210KAN\u7f51\u7edc\u589e\u5f3a\u7f16\u7801\u5668\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u901a\u8fc7KAN\u53c2\u6570\u5f00\u53d1\u4e24\u79cd\u5173\u952e\u7279\u5f81\u8bc6\u522b\u6280\u672f\uff0c\u751f\u6210\u8bed\u4e49\u6709\u610f\u4e49\u7684\u786c\u8d1f\u6837\u672c\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e0a\uff0cKhan-GCL\u6bd4\u73b0\u6709GCL\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\uff0c\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "Khan-GCL\u901a\u8fc7\u589e\u5f3a\u7f16\u7801\u5668\u80fd\u529b\u548c\u751f\u6210\u9ad8\u8d28\u91cf\u8d1f\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u5bf9\u6bd4\u5b66\u4e60\u7684\u6027\u80fd\u3002", "keywords": "graph contrastive learning, KAN, hard negative samples, semantic differences, graph representations"}}
{"id": "2505.14843", "pdf": "https://arxiv.org/pdf/2505.14843", "abs": "https://arxiv.org/abs/2505.14843", "authors": ["Yunha Yeo", "Daeho Um"], "title": "Leveraging Generative AI Models to Explore Human Identity", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ISEA 2025", "summary": "This paper attempts to explore human identity by utilizing neural networks in\nan indirect manner. For this exploration, we adopt diffusion models,\nstate-of-the-art AI generative models trained to create human face images. By\nrelating the generated human face to human identity, we establish a\ncorrespondence between the face image generation process of the diffusion model\nand the process of human identity formation. Through experiments with the\ndiffusion model, we observe that changes in its external input result in\nsignificant changes in the generated face image. Based on the correspondence,\nwe indirectly confirm the dependence of human identity on external factors in\nthe process of human identity formation. Furthermore, we introduce\n\\textit{Fluidity of Human Identity}, a video artwork that expresses the fluid\nnature of human identity affected by varying external factors. The video is\navailable at\nhttps://www.behance.net/gallery/219958453/Fluidity-of-Human-Identity?.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6269\u6563\u6a21\u578b\u95f4\u63a5\u63a2\u7d22\u4eba\u7c7b\u8eab\u4efd\uff0c\u5229\u7528AI\u751f\u6210\u4eba\u8138\u56fe\u50cf\u5e76\u4e0e\u8eab\u4efd\u5f62\u6210\u8fc7\u7a0b\u5efa\u7acb\u5bf9\u5e94\u5173\u7cfb\uff0c\u9a8c\u8bc1\u4e86\u5916\u90e8\u56e0\u7d20\u5bf9\u4eba\u7c7b\u8eab\u4efd\u7684\u5f71\u54cd\uff0c\u5e76\u521b\u4f5c\u4e86\u8868\u8fbe\u8eab\u4efd\u6d41\u52a8\u6027\u7684\u89c6\u9891\u4f5c\u54c1\u3002", "motivation": "\u901a\u8fc7AI\u751f\u6210\u6a21\u578b\uff08\u6269\u6563\u6a21\u578b\uff09\u95f4\u63a5\u7814\u7a76\u4eba\u7c7b\u8eab\u4efd\u7684\u5f62\u6210\u53ca\u5176\u5bf9\u5916\u90e8\u56e0\u7d20\u7684\u4f9d\u8d56\u6027\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u4eba\u8138\u56fe\u50cf\uff0c\u5e76\u5c06\u5176\u751f\u6210\u8fc7\u7a0b\u4e0e\u4eba\u7c7b\u8eab\u4efd\u5f62\u6210\u8fc7\u7a0b\u5bf9\u5e94\uff0c\u901a\u8fc7\u8f93\u5165\u53d8\u5316\u89c2\u5bdf\u751f\u6210\u56fe\u50cf\u7684\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5916\u90e8\u8f93\u5165\u7684\u53d8\u5316\u5bfc\u81f4\u751f\u6210\u4eba\u8138\u56fe\u50cf\u7684\u663e\u8457\u53d8\u5316\uff0c\u95f4\u63a5\u8bc1\u5b9e\u4e86\u4eba\u7c7b\u8eab\u4efd\u5bf9\u5916\u90e8\u56e0\u7d20\u7684\u4f9d\u8d56\u6027\u3002", "conclusion": "\u4eba\u7c7b\u8eab\u4efd\u5177\u6709\u6d41\u52a8\u6027\uff0c\u53d7\u5916\u90e8\u56e0\u7d20\u5f71\u54cd\uff1b\u901a\u8fc7\u827a\u672f\u4f5c\u54c1\u8fdb\u4e00\u6b65\u8868\u8fbe\u4e86\u8fd9\u4e00\u89c2\u70b9\u3002", "keywords": "human identity, diffusion models, AI, generative models, external factors"}}
{"id": "2505.15196", "pdf": "https://arxiv.org/pdf/2505.15196", "abs": "https://arxiv.org/abs/2505.15196", "authors": ["Weiqi Wang", "Limeng Cui", "Xin Liu", "Sreyashi Nag", "Wenju Xu", "Chen Luo", "Sheikh Muhammad Sarwar", "Yang Li", "Hansu Gu", "Hui Liu", "Changlong Yu", "Jiaxin Bai", "Yifan Gao", "Haiyang Zhang", "Qi He", "Shuiwang Ji", "Yangqiu Song"], "title": "EcomScriptBench: A Multi-task Benchmark for E-commerce Script Planning via Step-wise Intention-Driven Product Association", "categories": ["cs.CL"], "comment": "ACL2025", "summary": "Goal-oriented script planning, or the ability to devise coherent sequences of\nactions toward specific goals, is commonly employed by humans to plan for\ntypical activities. In e-commerce, customers increasingly seek LLM-based\nassistants to generate scripts and recommend products at each step, thereby\nfacilitating convenient and efficient shopping experiences. However, this\ncapability remains underexplored due to several challenges, including the\ninability of LLMs to simultaneously conduct script planning and product\nretrieval, difficulties in matching products caused by semantic discrepancies\nbetween planned actions and search queries, and a lack of methods and benchmark\ndata for evaluation. In this paper, we step forward by formally defining the\ntask of E-commerce Script Planning (EcomScript) as three sequential subtasks.\nWe propose a novel framework that enables the scalable generation of\nproduct-enriched scripts by associating products with each step based on the\nsemantic similarity between the actions and their purchase intentions. By\napplying our framework to real-world e-commerce data, we construct the very\nfirst large-scale EcomScript dataset, EcomScriptBench, which includes 605,229\nscripts sourced from 2.4 million products. Human annotations are then conducted\nto provide gold labels for a sampled subset, forming an evaluation benchmark.\nExtensive experiments reveal that current (L)LMs face significant challenges\nwith EcomScript tasks, even after fine-tuning, while injecting product purchase\nintentions improves their performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u7535\u5b50\u5546\u52a1\u811a\u672c\u89c4\u5212\uff08EcomScript\uff09\u4efb\u52a1\uff0c\u5206\u4e3a\u4e09\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u5173\u8054\u4ea7\u54c1\u548c\u52a8\u4f5c\uff0c\u6784\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6EcomScriptBench\u3002\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\uff08L\uff09LMs\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5f15\u5165\u8d2d\u4e70\u610f\u56fe\u53ef\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4e3a\u6ee1\u8db3\u7535\u5546\u987e\u5ba2\u5bf9LLM\u52a9\u624b\u7684\u9700\u6c42\uff0c\u89e3\u51b3\u5f53\u524dLLMs\u5728\u811a\u672c\u89c4\u5212\u548c\u4ea7\u54c1\u68c0\u7d22\u4e0a\u7684\u4e0d\u8db3\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u8bc4\u4f30\u65b9\u6cd5\u548c\u6570\u636e\u7684\u95ee\u9898\u3002", "method": "\u5b9a\u4e49EcomScript\u4efb\u52a1\uff0c\u63d0\u51fa\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u6846\u67b6\uff0c\u5173\u8054\u4ea7\u54c1\u548c\u811a\u672c\u6b65\u9aa4\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b60\u591a\u4e07\u811a\u672c\u7684\u6570\u636e\u96c6EcomScriptBench\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\uff08L\uff09LMs\u5728EcomScript\u4efb\u52a1\u4e0a\u8868\u73b0\u6311\u6218\u6027\uff0c\u4f46\u5f15\u5165\u4ea7\u54c1\u8d2d\u4e70\u610f\u56fe\u80fd\u63d0\u9ad8\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u4e3a\u7535\u5546\u811a\u672c\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u5411\u3002", "keywords": "\u7535\u5b50\u5546\u52a1\u811a\u672c\u89c4\u5212, LLM, \u8bed\u4e49\u76f8\u4f3c\u6027, \u4ea7\u54c1\u68c0\u7d22, EcomScriptBench"}}
{"id": "2505.15116", "pdf": "https://arxiv.org/pdf/2505.15116", "abs": "https://arxiv.org/abs/2505.15116", "authors": ["Zehong Wang", "Zheyuan Liu", "Tianyi Ma", "Jiazheng Li", "Zheyuan Zhang", "Xingbo Fu", "Yiyang Li", "Zhengqing Yuan", "Wei Song", "Yijun Ma", "Qingkai Zeng", "Xiusi Chen", "Jianan Zhao", "Jundong Li", "Meng Jiang", "Pietro Lio", "Nitesh Chawla", "Chuxu Zhang", "Yanfang Ye"], "title": "Graph Foundation Models: A Comprehensive Survey", "categories": ["cs.LG", "cs.AI", "cs.SI"], "comment": "Github Repo:\n  https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs. 93 pages,\n  438 references", "summary": "Graph-structured data pervades domains such as social networks, biological\nsystems, knowledge graphs, and recommender systems. While foundation models\nhave transformed natural language processing, vision, and multimodal learning\nthrough large-scale pretraining and generalization, extending these\ncapabilities to graphs -- characterized by non-Euclidean structures and complex\nrelational semantics -- poses unique challenges and opens new opportunities. To\nthis end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose\nintelligence to structured data, enabling broad transfer across graph-centric\ntasks and domains. This survey provides a comprehensive overview of GFMs,\nunifying diverse efforts under a modular framework comprising three key\ncomponents: backbone architectures, pretraining strategies, and adaptation\nmechanisms. We categorize GFMs by their generalization scope -- universal,\ntask-specific, and domain-specific -- and review representative methods, key\ninnovations, and theoretical insights within each category. Beyond methodology,\nwe examine theoretical foundations including transferability and emergent\ncapabilities, and highlight key challenges such as structural alignment,\nheterogeneity, scalability, and evaluation. Positioned at the intersection of\ngraph learning and general-purpose AI, GFMs are poised to become foundational\ninfrastructure for open-ended reasoning over structured data. This survey\nconsolidates current progress and outlines future directions to guide research\nin this rapidly evolving field. Resources are available at\nhttps://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.", "AI": {"tldr": "Graph Foundation Models (GFMs) extend large-scale pretraining to graph-structured data, addressing unique challenges and enabling broad transfer across tasks. This survey categorizes GFMs, reviews methods, and outlines future directions.", "motivation": "Graph-structured data is pervasive but poses unique challenges due to non-Euclidean structures and complex semantics. GFMs aim to bring scalable, general-purpose AI to graph data.", "method": "The survey organizes GFMs into a modular framework with backbone architectures, pretraining strategies, and adaptation mechanisms, categorizing them by generalization scope (universal, task-specific, domain-specific).", "result": "GFMs show promise for open-ended reasoning over structured data, but challenges like structural alignment and scalability remain.", "conclusion": "GFMs are foundational infrastructure for graph learning, with future research needed to address theoretical and practical challenges.", "keywords": "Graph Foundation Models, pretraining, generalization, graph learning, structured data"}}
{"id": "2505.15209", "pdf": "https://arxiv.org/pdf/2505.15209", "abs": "https://arxiv.org/abs/2505.15209", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Hyesoo Hong", "Soeun Kim", "Seungju Han", "Youngjae Yu", "Albert No"], "title": "DUSK: Do Not Unlearn Shared Knowledge", "categories": ["cs.CL"], "comment": "21 pages", "summary": "Large language models (LLMs) are increasingly deployed in real-world\napplications, raising concerns about the unauthorized use of copyrighted or\nsensitive data. Machine unlearning aims to remove such 'forget' data while\npreserving utility and information from the 'retain' set. However, existing\nevaluations typically assume that forget and retain sets are fully disjoint,\noverlooking realistic scenarios where they share overlapping content. For\ninstance, a news article may need to be unlearned, even though the same event,\nsuch as an earthquake in Japan, is also described factually on Wikipedia.\nEffective unlearning should remove the specific phrasing of the news article\nwhile preserving publicly supported facts. In this paper, we introduce DUSK, a\nbenchmark designed to evaluate unlearning methods under realistic data overlap.\nDUSK constructs document sets that describe the same factual content in\ndifferent styles, with some shared information appearing across all sets and\nother content remaining unique to each. When one set is designated for\nunlearning, an ideal method should remove its unique content while preserving\nshared facts. We define seven evaluation metrics to assess whether unlearning\nmethods can achieve this selective removal. Our evaluation of nine recent\nunlearning methods reveals a key limitation: while most can remove\nsurface-level text, they often fail to erase deeper, context-specific knowledge\nwithout damaging shared content. We release DUSK as a public benchmark to\nsupport the development of more precise and reliable unlearning techniques for\nreal-world applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DUSK\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5728\u6570\u636e\u91cd\u53e0\u7684\u73b0\u5b9e\u573a\u666f\u4e0b\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9057\u5fd8\u80fd\u529b\uff0c\u5173\u6ce8\u9009\u62e9\u6027\u79fb\u9664\u7279\u5b9a\u5185\u5bb9\u7684\u540c\u65f6\u4fdd\u7559\u5171\u4eab\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u7684\u9057\u5fd8\u8bc4\u4f30\u901a\u5e38\u5047\u8bbe\u9057\u5fd8\u548c\u4fdd\u7559\u6570\u636e\u96c6\u5b8c\u5168\u5206\u79bb\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u4e2d\u4e8c\u8005\u53ef\u80fd\u91cd\u53e0\u7684\u60c5\u51b5\u3002\u9700\u5f00\u53d1\u80fd\u9009\u62e9\u6027\u79fb\u9664\u7279\u5b9a\u5185\u5bb9\u7684\u6280\u672f\u3002", "method": "\u8bba\u6587\u63d0\u51faDUSK\u57fa\u51c6\uff0c\u6784\u5efa\u5305\u542b\u76f8\u540c\u4e8b\u5b9e\u4f46\u98ce\u683c\u4e0d\u540c\u7684\u6587\u6863\u96c6\uff0c\u90e8\u5206\u5185\u5bb9\u5171\u4eab\uff0c\u90e8\u5206\u72ec\u7279\u3002\u8bc4\u4f30\u65b9\u6cd5\u662f\u5426\u80fd\u9009\u62e9\u6027\u79fb\u9664\u72ec\u7279\u5185\u5bb9\u3002", "result": "\u8bc4\u4f30\u4e86\u4e5d\u79cd\u9057\u5fd8\u65b9\u6cd5\uff0c\u53d1\u73b0\u5927\u591a\u6570\u80fd\u79fb\u9664\u8868\u5c42\u6587\u672c\uff0c\u4f46\u96be\u4ee5\u5728\u4e0d\u635f\u5bb3\u5171\u4eab\u5185\u5bb9\u7684\u60c5\u51b5\u4e0b\u6e05\u9664\u6df1\u5c42\u77e5\u8bc6\u3002", "conclusion": "DUSK\u4f5c\u4e3a\u516c\u5171\u57fa\u51c6\uff0c\u652f\u6301\u5f00\u53d1\u66f4\u7cbe\u786e\u53ef\u9760\u7684\u9057\u5fd8\u6280\u672f\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u673a\u5668\u9057\u5fd8, \u6570\u636e\u91cd\u53e0, \u57fa\u51c6\u8bc4\u4f30, \u9009\u62e9\u6027\u79fb\u9664, \u5171\u4eab\u4fe1\u606f"}}
{"id": "2505.15130", "pdf": "https://arxiv.org/pdf/2505.15130", "abs": "https://arxiv.org/abs/2505.15130", "authors": ["Sajjad Ghiasvand", "Haniyeh Ehsani Oskouie", "Mahnoosh Alizadeh", "Ramtin Pedarsani"], "title": "Few-Shot Adversarial Low-Rank Fine-Tuning of Vision-Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) such as CLIP have shown remarkable performance\nin cross-modal tasks through large-scale contrastive pre-training. To adapt\nthese large transformer-based models efficiently for downstream tasks,\nParameter-Efficient Fine-Tuning (PEFT) techniques like LoRA have emerged as\nscalable alternatives to full fine-tuning, especially in few-shot scenarios.\nHowever, like traditional deep neural networks, VLMs are highly vulnerable to\nadversarial attacks, where imperceptible perturbations can significantly\ndegrade model performance. Adversarial training remains the most effective\nstrategy for improving model robustness in PEFT. In this work, we propose\nAdvCLIP-LoRA, the first algorithm designed to enhance the adversarial\nrobustness of CLIP models fine-tuned with LoRA in few-shot settings. Our method\nformulates adversarial fine-tuning as a minimax optimization problem and\nprovides theoretical guarantees for convergence under smoothness and\nnonconvex-strong-concavity assumptions. Empirical results across eight datasets\nusing ViT-B/16 and ViT-B/32 models show that AdvCLIP-LoRA significantly\nimproves robustness against common adversarial attacks (e.g., FGSM, PGD),\nwithout sacrificing much clean accuracy. These findings highlight AdvCLIP-LoRA\nas a practical and theoretically grounded approach for robust adaptation of\nVLMs in resource-constrained settings.", "AI": {"tldr": "AdvCLIP-LoRA\u9996\u6b21\u63d0\u51fa\u4e00\u79cd\u5728Few-shot\u573a\u666f\u4e0b\u589e\u5f3aCLIP\u6a21\u578b\u5bf9\u6297\u9c81\u68d2\u6027\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u6700\u5927\u5316\u4f18\u5316\u95ee\u9898\u5b9e\u73b0\u5bf9\u6297\u5fae\u8c03\uff0c\u7406\u8bba\u4fdd\u8bc1\u6536\u655b\uff0c\u5b9e\u9a8c\u663e\u8457\u63d0\u5347\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1CLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u6613\u53d7\u5e72\u6270\uff0c\u4e14\u4f20\u7edf\u5bf9\u6297\u8bad\u7ec3\u5728\u8d44\u6e90\u53d7\u9650\u7684Few-shot\u573a\u666f\u4e2d\u96be\u4ee5\u9ad8\u6548\u5e94\u7528\u3002", "method": "\u63d0\u51faAdvCLIP-LoRA\u7b97\u6cd5\uff0c\u5c06\u5bf9\u6297\u5fae\u8c03\u5efa\u6a21\u4e3a\u6700\u5c0f\u6700\u5927\u5316\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u57fa\u4e8eLoRA\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3002", "result": "\u57288\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cAdvCLIP-LoRA\u663e\u8457\u63d0\u5347\u5bf9\u6297\u653b\u51fb\uff08\u5982FGSM\u3001PGD\uff09\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u5e72\u51c0\u51c6\u786e\u7387\u3002", "conclusion": "AdvCLIP-LoRA\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684VLM\u9c81\u68d2\u9002\u5e94\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u7406\u8bba\u652f\u6301\u7684\u65b9\u6cd5\u3002", "keywords": "Vision-Language Models, Adversarial Robustness, LoRA, Few-shot Learning, CLIP"}}
{"id": "2505.15210", "pdf": "https://arxiv.org/pdf/2505.15210", "abs": "https://arxiv.org/abs/2505.15210", "authors": ["Jie Ma", "Ning Qu", "Zhitao Gao", "Rui Xing", "Jun Liu", "Hongbin Pei", "Jiang Xie", "Linyun Song", "Pinghui Wang", "Jing Tao", "Zhou Su"], "title": "Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs", "categories": ["cs.CL", "cs.IR", "I.2.4"], "comment": "Under Review", "summary": "Knowledge graph-based retrieval-augmented generation seeks to mitigate\nhallucinations in Large Language Models (LLMs) caused by insufficient or\noutdated knowledge. However, existing methods often fail to fully exploit the\nprior knowledge embedded in knowledge graphs (KGs), particularly their\nstructural information and explicit or implicit constraints. The former can\nenhance the faithfulness of LLMs' reasoning, while the latter can improve the\nreliability of response generation. Motivated by these, we propose a\ntrustworthy reasoning framework, termed Deliberation over Priors (DP), which\nsufficiently utilizes the priors contained in KGs. Specifically, DP adopts a\nprogressive knowledge distillation strategy that integrates structural priors\ninto LLMs through a combination of supervised fine-tuning and Kahneman-Tversky\noptimization, thereby improving the faithfulness of relation path generation.\nFurthermore, our framework employs a reasoning-introspection strategy, which\nguides LLMs to perform refined reasoning verification based on extracted\nconstraint priors, ensuring the reliability of response generation. Extensive\nexperiments on three benchmark datasets demonstrate that DP achieves new\nstate-of-the-art performance, especially a Hit@1 improvement of 13% on the\nComplexWebQuestions dataset, and generates highly trustworthy responses. We\nalso conduct various analyses to verify its flexibility and practicality. The\ncode is available at https://github.com/reml-group/Deliberation-on-Priors.", "AI": {"tldr": "DP\u6846\u67b6\u901a\u8fc7\u5145\u5206\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u63d0\u5347LLMs\u7684\u751f\u6210\u53ef\u4fe1\u5ea6\u548c\u63a8\u7406\u5fe0\u5b9e\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u7ed3\u6784\u4fe1\u606f\u4e0e\u7ea6\u675f\u77e5\u8bc6\u7684\u4e0d\u8db3\uff0c\u63d0\u5347LLMs\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\u548c\u63a8\u7406-\u53cd\u601d\u7b56\u7565\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u4e0eKahneman-Tversky\u4f18\u5316\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0cComplexWebQuestions\u4e0a\u7684Hit@1\u63d0\u534713%\u3002", "conclusion": "DP\u6846\u67b6\u663e\u8457\u63d0\u5347\u751f\u6210\u7ed3\u679c\u7684\u53ef\u4fe1\u5ea6\uff0c\u5177\u6709\u7075\u6d3b\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "keywords": "\u77e5\u8bc6\u56fe\u8c31, LLMs, \u53ef\u4fe1\u63a8\u7406, Kahneman-Tversky\u4f18\u5316"}}
{"id": "2505.15134", "pdf": "https://arxiv.org/pdf/2505.15134", "abs": "https://arxiv.org/abs/2505.15134", "authors": ["Shivam Agarwal", "Zimin Zhang", "Lifan Yuan", "Jiawei Han", "Hao Peng"], "title": "The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Entropy minimization (EM) trains the model to concentrate even more\nprobability mass on its most confident outputs. We show that this simple\nobjective alone, without any labeled data, can substantially improve large\nlanguage models' (LLMs) performance on challenging math, physics, and coding\ntasks. We explore three approaches: (1) EM-FT minimizes token-level entropy\nsimilarly to instruction finetuning, but on unlabeled outputs drawn from the\nmodel; (2) EM-RL: reinforcement learning with negative entropy as the only\nreward to maximize; (3) EM-INF: inference-time logit adjustment to reduce\nentropy without any training data or parameter updates. On Qwen-7B, EM-RL,\nwithout any labeled data, achieves comparable or better performance than strong\nRL baselines such as GRPO and RLOO that are trained on 60K labeled examples.\nFurthermore, EM-INF enables Qwen-32B to match or exceed the performance of\nproprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the\nchallenging SciCode benchmark, while being 3x more efficient than\nself-consistency and sequential refinement. Our findings reveal that many\npretrained LLMs possess previously underappreciated reasoning capabilities that\ncan be effectively elicited through entropy minimization alone, without any\nlabeled data or even any parameter updates.", "AI": {"tldr": "\u901a\u8fc7\u71b5\u6700\u5c0f\u5316\uff08EM\uff09\u7684\u65b9\u6cd5\uff0c\u8bba\u6587\u5c55\u793a\u4e86\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u7f16\u7801\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u71b5\u6700\u5c0f\u5316\u6fc0\u53d1\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u63a8\u7406\u80fd\u529b\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u6216\u53c2\u6570\u66f4\u65b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u71b5\u6700\u5c0f\u5316\u65b9\u6cd5\uff1aEM-FT\uff08\u57fa\u4e8e\u65e0\u6807\u7b7e\u8f93\u51fa\u7684\u6307\u4ee4\u5fae\u8c03\uff09\u3001EM-RL\uff08\u4ec5\u4ee5\u8d1f\u71b5\u4e3a\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u548cEM-INF\uff08\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u7684\u63a8\u7406\u65f6\u8c03\u6574\uff09\u3002", "result": "\u5728Qwen-7B\u4e0a\uff0cEM-RL\u7684\u6027\u80fd\u4f18\u4e8e\u57fa\u4e8e\u6807\u6ce8\u6570\u636e\u7684RL\u57fa\u7ebf\uff1bQwen-32B\u901a\u8fc7EM-INF\u5728SciCode\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5ab2\u7f8eGPT-4o\u7b49\u4e13\u6709\u6a21\u578b\uff0c\u4e14\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u71b5\u6700\u5c0f\u5316\u80fd\u591f\u6709\u6548\u6fc0\u53d1\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u6216\u53c2\u6570\u66f4\u65b0\u3002", "keywords": "\u71b5\u6700\u5c0f\u5316, \u5927\u8bed\u8a00\u6a21\u578b, \u65e0\u76d1\u7763\u5b66\u4e60, \u63a8\u7406\u80fd\u529b, \u6548\u7387"}}
{"id": "2505.14862", "pdf": "https://arxiv.org/pdf/2505.14862", "abs": "https://arxiv.org/abs/2505.14862", "authors": ["Nicolas M\u00fcller", "Piotr Kawa", "Wei-Herng Choong", "Adriana Stan", "Aditya Tirumala Bukkapatnam", "Karla Pizzi", "Alexander Wagner", "Philip Sperl"], "title": "Replay Attacks Against Audio Deepfake Detection", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "We show how replay attacks undermine audio deepfake detection: By playing and\nre-recording deepfake audio through various speakers and microphones, we make\nspoofed samples appear authentic to the detection model. To study this\nphenomenon in more detail, we introduce ReplayDF, a dataset of recordings\nderived from M-AILABS and MLAAD, featuring 109 speaker-microphone combinations\nacross six languages and four TTS models. It includes diverse acoustic\nconditions, some highly challenging for detection. Our analysis of six\nopen-source detection models across five datasets reveals significant\nvulnerability, with the top-performing W2V2-AASIST model's Equal Error Rate\n(EER) surging from 4.7% to 18.2%. Even with adaptive Room Impulse Response\n(RIR) retraining, performance remains compromised with an 11.0% EER. We release\nReplayDF for non-commercial research use.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u91cd\u653e\u653b\u51fb\u5982\u4f55\u901a\u8fc7\u64ad\u653e\u548c\u91cd\u65b0\u5f55\u5236\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\u6765\u6b3a\u9a97\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u63d0\u51faReplayDF\u6570\u636e\u96c6\u4ee5\u5206\u6790\u8fd9\u4e00\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u91cd\u653e\u653b\u51fb\u5bf9\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u7684\u8106\u5f31\u6027\u3002", "method": "\u4f7f\u7528ReplayDF\u6570\u636e\u96c6\uff08\u5305\u542b109\u79cd\u626c\u58f0\u5668-\u9ea6\u514b\u98ce\u7ec4\u5408\uff09\u5bf9\u516d\u79cd\u5f00\u6e90\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5206\u6790\u5176\u5728\u591a\u79cd\u58f0\u5b66\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u91cd\u653e\u653b\u51fb\u663e\u8457\u964d\u4f4e\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u6700\u4f73\u6a21\u578bW2V2-AASIST\u7684EER\u4ece4.7%\u4e0a\u5347\u523018.2%\u3002\u5373\u4f7f\u7ecf\u8fc7RIR\u91cd\u65b0\u8bad\u7ec3\uff0cEER\u4ecd\u9ad8\u8fbe11.0%\u3002", "conclusion": "\u91cd\u653e\u653b\u51fb\u5bf9\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u4ecd\u9700\u6539\u8fdb\u3002", "keywords": "\u91cd\u653e\u653b\u51fb,\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020,\u68c0\u6d4b\u6a21\u578b,ReplayDF,EER"}}
{"id": "2505.15214", "pdf": "https://arxiv.org/pdf/2505.15214", "abs": "https://arxiv.org/abs/2505.15214", "authors": ["Sangyeon Yoon", "Wonje Jeung", "Albert No"], "title": "R-TOFU: Unlearning in Large Reasoning Models", "categories": ["cs.CL"], "comment": "19 pages", "summary": "Large Reasoning Models (LRMs) embed private or copyrighted information not\nonly in their final answers but also throughout multi-step chain-of-thought\n(CoT) traces, making reliable unlearning far more demanding than in standard\nLLMs. We introduce Reasoning-TOFU (R-TOFU), the first benchmark tailored to\nthis setting. R-TOFU augments existing unlearning tasks with realistic CoT\nannotations and provides step-wise metrics that expose residual knowledge\ninvisible to answer-level checks. Using R-TOFU, we carry out a comprehensive\ncomparison of gradient-based and preference-optimization baselines and show\nthat conventional answer-only objectives leave substantial forget traces in\nreasoning. We further propose Reasoned IDK, a preference-optimization variant\nthat preserves coherent yet inconclusive reasoning, achieving a stronger\nbalance between forgetting efficacy and model utility than earlier refusal\nstyles. Finally, we identify a failure mode: decoding variants such as\nZeroThink and LessThink can still reveal forgotten content despite seemingly\nsuccessful unlearning, emphasizing the need to evaluate models under diverse\ndecoding settings. Together, the benchmark, analysis, and new baseline\nestablish a systematic foundation for studying and improving unlearning in LRMs\nwhile preserving their reasoning capabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86R-TOFU\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u7814\u7a76\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u4e2d\u7684\u77e5\u8bc6\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5Reasoned IDK\u4ee5\u5e73\u8861\u9057\u5fd8\u6548\u679c\u4e0e\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u9488\u5bf9\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u591a\u6b65\u63a8\u7406\u94fe\u4e2d\u5d4c\u5165\u79c1\u6709\u6216\u53d7\u7248\u6743\u4fdd\u62a4\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u9057\u5fd8\u65b9\u6cd5\u5728\u6b64\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faR-TOFU\u57fa\u51c6\uff0c\u7ed3\u5408\u94fe\u5f0f\u63a8\u7406\u6ce8\u91ca\uff0c\u5e76\u5f00\u53d1\u4e86Reasoned IDK\u65b9\u6cd5\uff0c\u901a\u8fc7\u504f\u597d\u4f18\u5316\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u9057\u5fd8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u4f20\u7edf\u7b54\u6848\u7ea7\u9057\u5fd8\u65b9\u6cd5\u5728\u63a8\u7406\u94fe\u4e2d\u4ecd\u6709\u6b8b\u7559\u77e5\u8bc6\uff0c\u800cReasoned IDK\u5728\u9057\u5fd8\u6548\u679c\u4e0e\u6a21\u578b\u6548\u7528\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861\u3002", "conclusion": "R-TOFU\u57fa\u51c6\u548c\u5206\u6790\u4e3aLRMs\u4e2d\u7684\u77e5\u8bc6\u9057\u5fd8\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u57fa\u7840\uff0c\u5e76\u63ed\u793a\u4e86\u5728\u4e0d\u540c\u89e3\u7801\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "keywords": "\u5927\u578b\u63a8\u7406\u6a21\u578b, R-TOFU, \u77e5\u8bc6\u9057\u5fd8, \u94fe\u5f0f\u63a8\u7406, \u504f\u597d\u4f18\u5316"}}
{"id": "2505.15138", "pdf": "https://arxiv.org/pdf/2505.15138", "abs": "https://arxiv.org/abs/2505.15138", "authors": ["Yang Xu", "Swetha Ganesh", "Washim Uddin Mondal", "Qinbo Bai", "Vaneet Aggarwal"], "title": "Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor Critic Algorithm", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper investigates infinite-horizon average reward Constrained Markov\nDecision Processes (CMDPs) with general parametrization. We propose a\nPrimal-Dual Natural Actor-Critic algorithm that adeptly manages constraints\nwhile ensuring a high convergence rate. In particular, our algorithm achieves\nglobal convergence and constraint violation rates of\n$\\tilde{\\mathcal{O}}(1/\\sqrt{T})$ over a horizon of length $T$ when the mixing\ntime, $\\tau_{\\mathrm{mix}}$, is known to the learner. In absence of knowledge\nof $\\tau_{\\mathrm{mix}}$, the achievable rates change to\n$\\tilde{\\mathcal{O}}(1/T^{0.5-\\epsilon})$ provided that $T \\geq\n\\tilde{\\mathcal{O}}\\left(\\tau_{\\mathrm{mix}}^{2/\\epsilon}\\right)$. Our results\nmatch the theoretical lower bound for Markov Decision Processes and establish a\nnew benchmark in the theoretical exploration of average reward CMDPs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5177\u6709\u4e00\u822c\u53c2\u6570\u5316\u7684\u65e0\u9650\u65f6\u95f4\u5e73\u5747\u5956\u52b1\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08CMDPs\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u5904\u7406\u7ea6\u675f\u5e76\u786e\u4fdd\u9ad8\u6536\u655b\u7387\u7684\u539f\u59cb\u5bf9\u5076\u81ea\u7136\u6f14\u5458\u8bc4\u8bba\u5bb6\u7b97\u6cd5\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5728\u65e0\u9650\u65f6\u95f4\u8303\u56f4\u5185\u5904\u7406\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08CMDPs\uff09\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u53c2\u6570\u5316\u901a\u7528\u7684\u573a\u666f\u4e0b\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u6ee1\u8db3\u9ad8\u6536\u655b\u7387\u548c\u7ea6\u675f\u7ba1\u7406\u7684\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u539f\u59cb\u5bf9\u5076\u81ea\u7136\u6f14\u5458\u8bc4\u8bba\u5bb6\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u5df2\u77e5\u6df7\u5408\u65f6\u95f4$\\tau_{\\mathrm{mix}}$\u65f6\u80fd\u591f\u8fbe\u5230\u5168\u5c40\u6536\u655b\u548c\u7ea6\u675f\u8fdd\u53cd\u7387\u4e3a$\\tilde{\\mathcal{O}}(1/\\sqrt{T})$\u7684\u901f\u7387\u3002", "result": "\u7b97\u6cd5\u5728\u5df2\u77e5$\\tau_{\\mathrm{mix}}$\u65f6\u901f\u7387\u8fbe\u5230$\\tilde{\\mathcal{O}}(1/\\sqrt{T})$\uff0c\u672a\u77e5\u65f6\u5219\u4e3a$\\tilde{\\mathcal{O}}(1/T^{0.5-\\epsilon})$\uff0c\u9700\u8981$T \\geq \\tilde{\\mathcal{O}}\\left(\\tau_{\\mathrm{mix}}^{2/\\epsilon}\\right)$\u3002", "conclusion": "\u8be5\u7814\u7a76\u7684\u6210\u679c\u8fbe\u5230\u4e86\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u7406\u8bba\u4e0b\u754c\uff0c\u4e3a\u5e73\u5747\u5956\u52b1CMDPs\u7684\u7406\u8bba\u63a2\u7d22\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "keywords": "\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b, \u6f14\u5458\u8bc4\u8bba\u5bb6\u7b97\u6cd5, \u65e0\u9650\u65f6\u95f4\u8303\u56f4, \u53c2\u6570\u4f18\u5316"}}
{"id": "2505.14864", "pdf": "https://arxiv.org/pdf/2505.14864", "abs": "https://arxiv.org/abs/2505.14864", "authors": ["Mohamed Wahib", "Muhammed Abdullah Soyturk", "Didem Unat"], "title": "Balanced and Elastic End-to-end Training of Dynamic LLMs", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "To reduce computational and memory costs in Large Language Models (LLMs),\ndynamic workload reduction schemes like Mixture of Experts (MoEs), parameter\npruning, layer freezing, sparse attention, early token exit, and Mixture of\nDepths (MoDs) have emerged. However, these methods introduce severe workload\nimbalances, limiting their practicality for large-scale distributed training.\nWe propose DynMo, an autonomous dynamic load balancing solution that ensures\noptimal compute distribution when using pipeline parallelism in training\ndynamic models. DynMo adaptively balances workloads, dynamically packs tasks\ninto fewer workers to free idle resources, and supports both multi-GPU\nsingle-node and multi-node systems. Compared to static training methods\n(Megatron-LM, DeepSpeed), DynMo accelerates training by up to 1.23x (MoEs),\n3.18x (pruning), 2.23x (layer freezing), 4.02x (sparse attention), 4.52x (early\nexit), and 1.17x (MoDs). DynMo is available at\nhttps://anonymous.4open.science/r/DynMo-4D04/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDynMo\uff0c\u4e00\u79cd\u52a8\u6001\u8d1f\u8f7d\u5747\u8861\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u52a8\u6001\u6a21\u578b\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u901f\u5ea6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u51cf\u5c11\u65b9\u6848\uff08\u5982MoEs\u3001\u53c2\u6570\u4fee\u526a\u7b49\uff09\u5bfc\u81f4\u4e25\u91cd\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u5e73\u8861\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "DynMo\u662f\u4e00\u79cd\u81ea\u4e3b\u52a8\u6001\u8d1f\u8f7d\u5e73\u8861\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u5e73\u8861\u5de5\u4f5c\u8d1f\u8f7d\u3001\u52a8\u6001\u5c06\u4efb\u52a1\u6253\u5305\u5230\u66f4\u5c11\u7684\u5de5\u4f5c\u8282\u70b9\u4ee5\u91ca\u653e\u7a7a\u95f2\u8d44\u6e90\uff0c\u652f\u6301\u591aGPU\u5355\u8282\u70b9\u548c\u591a\u8282\u70b9\u7cfb\u7edf\u3002", "result": "\u4e0e\u9759\u6001\u8bad\u7ec3\u65b9\u6cd5\u76f8\u6bd4\uff0cDynMo\u5728\u591a\u79cd\u52a8\u6001\u6a21\u578b\u4e0a\u663e\u8457\u52a0\u901f\u8bad\u7ec3\uff0c\u6700\u9ad8\u53ef\u8fbe4.52\u500d\u3002", "conclusion": "DynMo\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "keywords": "LLMs, \u52a8\u6001\u8d1f\u8f7d\u5747\u8861, DynMo, \u5206\u5e03\u5f0f\u8bad\u7ec3, \u5de5\u4f5c\u8d1f\u8f7d\u4f18\u5316"}}
{"id": "2505.15229", "pdf": "https://arxiv.org/pdf/2505.15229", "abs": "https://arxiv.org/abs/2505.15229", "authors": ["Qihan Wang", "Shidong Pan", "Tal Linzen", "Emily Black"], "title": "Multilingual Prompting for Improving LLM Generation Diversity", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) are known to lack cultural representation and\noverall diversity in their generations, from expressing opinions to answering\nfactual questions. To mitigate this problem, we propose multilingual prompting:\na prompting method which generates several variations of a base prompt with\nadded cultural and linguistic cues from several cultures, generates responses,\nand then combines the results. Building on evidence that LLMs have\nlanguage-specific knowledge, multilingual prompting seeks to increase diversity\nby activating a broader range of cultural knowledge embedded in model training\ndata. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA\n70B, and LLaMA 8B), we show that multilingual prompting consistently\noutperforms existing diversity-enhancing techniques such as high-temperature\nsampling, step-by-step recall, and personas prompting. Further analyses show\nthat the benefits of multilingual prompting vary with language resource level\nand model size, and that aligning the prompting language with the cultural cues\nreduces hallucination about culturally-specific information.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8bed\u8a00\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u6587\u5316\u548c\u8bed\u8a00\u63d0\u793a\u6765\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5185\u5bb9\u7684\u591a\u6837\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u5185\u5bb9\u4e2d\u7f3a\u4e4f\u6587\u5316\u591a\u6837\u6027\u548c\u4ee3\u8868\u6027\uff0c\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u591a\u8bed\u8a00\u63d0\u793a\u6fc0\u6d3b\u6a21\u578b\u4e2d\u5d4c\u5165\u7684\u5e7f\u6cdb\u6587\u5316\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u591a\u8bed\u8a00\u63d0\u793a\u65b9\u6cd5\uff0c\u751f\u6210\u5305\u542b\u4e0d\u540c\u6587\u5316\u7ebf\u7d22\u7684\u63d0\u793a\u53d8\u4f53\uff0c\u7efc\u5408\u751f\u6210\u7ed3\u679c\u4ee5\u63d0\u9ad8\u591a\u6837\u6027\u3002", "result": "\u591a\u8bed\u8a00\u63d0\u793a\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5176\u6548\u679c\u53d7\u8bed\u8a00\u8d44\u6e90\u6c34\u5e73\u548c\u6a21\u578b\u5927\u5c0f\u5f71\u54cd\uff0c\u4e0e\u6587\u5316\u7ebf\u7d22\u5bf9\u9f50\u7684\u63d0\u793a\u51cf\u5c11\u4e86\u5e7b\u89c9\u3002", "conclusion": "\u591a\u8bed\u8a00\u63d0\u793a\u662f\u4e00\u79cd\u6709\u6548\u7684\u589e\u5f3a\u6a21\u578b\u751f\u6210\u5185\u5bb9\u591a\u6837\u6027\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9700\u8981\u6587\u5316\u654f\u611f\u6027\u7684\u4efb\u52a1\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u6587\u5316\u591a\u6837\u6027\u3001\u591a\u8bed\u8a00\u63d0\u793a\u3001\u751f\u6210\u591a\u6837\u6027\u3001\u5e7b\u89c9"}}
{"id": "2505.15140", "pdf": "https://arxiv.org/pdf/2505.15140", "abs": "https://arxiv.org/abs/2505.15140", "authors": ["Tong Cheng", "Fu Jie", "Xinpeng Ling", "Huifa Li", "Zhili Chen"], "title": "EC-LDA : Label Distribution Inference Attack against Federated Graph Learning with Embedding Compression", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have been widely used for graph analysis.\nFederated Graph Learning (FGL) is an emerging learning framework to\ncollaboratively train graph data from various clients. However, since clients\nare required to upload model parameters to the server in each round, this\nprovides the server with an opportunity to infer each client's data privacy. In\nthis paper, we focus on label distribution attacks(LDAs) that aim to infer the\nlabel distributions of the clients' local data. We take the first step to\nattack client's label distributions in FGL. Firstly, we observe that the\neffectiveness of LDA is closely related to the variance of node embeddings in\nGNNs. Next, we analyze the relation between them and we propose a new attack\nnamed EC-LDA, which significantly improves the attack effectiveness by\ncompressing node embeddings. Thirdly, extensive experiments on node\nclassification and link prediction tasks across six widely used graph datasets\nshow that EC-LDA outperforms the SOTA LDAs. For example, EC-LDA attains optimal\nvalues under both Cos-sim and JS-div evaluation metrics in the CoraFull and\nLastFM datasets. Finally, we explore the robustness of EC-LDA under\ndifferential privacy protection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u8054\u90a6\u56fe\u5b66\u4e60\u4e2d\u6807\u7b7e\u5206\u5e03\u653b\u51fb\uff08LDA\uff09\uff0c\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aEC-LDA\u7684\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u538b\u7f29\u8282\u70b9\u5d4c\u5165\u663e\u8457\u63d0\u5347\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u8054\u90a6\u56fe\u5b66\u4e60\u4e2d\u670d\u52a1\u5668\u53ef\u80fd\u901a\u8fc7\u4e0a\u4f20\u7684\u6a21\u578b\u53c2\u6570\u63a8\u65ad\u5ba2\u6237\u7aef\u6807\u7b7e\u5206\u5e03\u7684\u95ee\u9898\uff0c\u91cd\u70b9\u5173\u6ce8LDA\u653b\u51fb\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8282\u70b9\u5d4c\u5165\u65b9\u5dee\u4e0eLDA\u7684\u5173\u7cfb\uff0c\u63d0\u51faEC-LDA\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u8fdb\u884c\u8282\u70b9\u5206\u7c7b\u548c\u94fe\u63a5\u9884\u6d4b\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "EC-LDA\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709LDA\u65b9\u6cd5\uff0c\u5e76\u5728\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u4e0b\u6d4b\u8bd5\u5176\u9c81\u68d2\u6027\u3002", "conclusion": "EC-LDA\u663e\u8457\u63d0\u5347LDA\u653b\u51fb\u6548\u679c\uff0c\u4e3a\u8054\u90a6\u56fe\u5b66\u4e60\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "keywords": "\u56fe\u795e\u7ecf\u7f51\u7edc, \u8054\u90a6\u5b66\u4e60, \u6807\u7b7e\u5206\u5e03\u653b\u51fb, \u9690\u79c1\u4fdd\u62a4"}}
{"id": "2505.15245", "pdf": "https://arxiv.org/pdf/2505.15245", "abs": "https://arxiv.org/abs/2505.15245", "authors": ["Zihao Jiang", "Ben Liu", "Miao Peng", "Wenjie Xu", "Yao Xiao", "Zhenyan Shan", "Min Peng"], "title": "Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework", "categories": ["cs.CL", "cs.AI"], "comment": "In Findings of the Association for Computational Linguistics: ACL\n  2025", "summary": "While large language models (LLMs) show great potential in temporal\nreasoning, most existing work focuses heavily on enhancing performance, often\nneglecting the explainable reasoning processes underlying the results. To\naddress this gap, we introduce a comprehensive benchmark covering a wide range\nof temporal granularities, designed to systematically evaluate LLMs'\ncapabilities in explainable temporal reasoning. Furthermore, our findings\nreveal that LLMs struggle to deliver convincing explanations when relying\nsolely on textual information. To address challenge, we propose GETER, a novel\nstructure-aware generative framework that integrates Graph structures with text\nfor Explainable TEmporal Reasoning. Specifically, we first leverage temporal\nknowledge graphs to develop a temporal encoder that captures structural\ninformation for the query. Subsequently, we introduce a structure-text prefix\nadapter to map graph structure features into the text embedding space. Finally,\nLLMs generate explanation text by seamlessly integrating the soft graph token\nwith instruction-tuning prompt tokens. Experimental results indicate that GETER\nachieves state-of-the-art performance while also demonstrating its\neffectiveness as well as strong generalization capabilities. Our dataset and\ncode are available at https://github.com/carryTatum/GETER.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aGETER\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u7ed3\u6784\u548c\u6587\u672c\u4fe1\u606f\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u65f6\u95f4\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u63a8\u7406\u8fc7\u7a0b\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8fc7\u4e8e\u6ce8\u91cd\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u63a8\u7406\u6027\u80fd\uff0c\u5374\u5ffd\u7565\u4e86\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\u548c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u65f6\u95f4\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u65f6\u95f4\u7f16\u7801\u5668\u6355\u6349\u67e5\u8be2\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u8bbe\u8ba1\u7ed3\u6784-\u6587\u672c\u524d\u7f00\u9002\u914d\u5668\u5c06\u56fe\u7ed3\u6784\u7279\u5f81\u6620\u5c04\u5230\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\uff0c\u6700\u540e\u7ed3\u5408\u8f6f\u56fe\u6807\u8bb0\u548c\u6307\u4ee4\u8c03\u6574\u751f\u6210\u89e3\u91ca\u6587\u672c\u3002", "result": "GETER\u6846\u67b6\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5c55\u73b0\u4e86\u5176\u6709\u6548\u6027\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GETER\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u56fe\u7ed3\u6784\u548c\u6587\u672c\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u65f6\u95f4\u63a8\u7406,\u53ef\u89e3\u91ca\u6027,\u56fe\u7ed3\u6784,GETER"}}
{"id": "2505.15141", "pdf": "https://arxiv.org/pdf/2505.15141", "abs": "https://arxiv.org/abs/2505.15141", "authors": ["Yunlong Hou", "Fengzhuo Zhang", "Cunxiao Du", "Xuan Zhang", "Jiachun Pan", "Tianyu Pang", "Chao Du", "Vincent Y. F. Tan", "Zhuoran Yang"], "title": "BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "35 pages, 4 figures", "summary": "Speculative decoding has emerged as a popular method to accelerate the\ninference of Large Language Models (LLMs) while retaining their superior text\ngeneration performance. Previous methods either adopt a fixed speculative\ndecoding configuration regardless of the prefix tokens, or train draft models\nin an offline or online manner to align them with the context. This paper\nproposes a training-free online learning framework to adaptively choose the\nconfiguration of the hyperparameters for speculative decoding as text is being\ngenerated. We first formulate this hyperparameter selection problem as a\nMulti-Armed Bandit problem and provide a general speculative decoding framework\nBanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms,\nUCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity,\nthe stopping time regret. We upper bound this regret under both stochastic and\nadversarial reward settings. By deriving an information-theoretic impossibility\nresult, it is shown that the regret performance of UCBSpec is optimal up to\nuniversal constants. Finally, extensive empirical experiments with LLaMA3 and\nQwen2 demonstrate that our algorithms are effective compared to existing\nmethods, and the throughput is close to the oracle best hyperparameter in\nsimulated real-life LLM serving scenarios with diverse input prompts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u9002\u5e94\u8d85\u53c2\u6570\u9009\u62e9\u65b9\u6cd5BanditSpec\uff0c\u901a\u8fc7\u591a\u81c2\u8001\u864e\u673a\u6a21\u578b\u4f18\u5316\u63a8\u6d4b\u89e3\u7801\u914d\u7f6e\uff0c\u5728\u52a8\u6001\u6587\u672c\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u8981\u4e48\u91c7\u7528\u56fa\u5b9a\u914d\u7f6e\uff0c\u8981\u4e48\u4f9d\u8d56\u79bb\u7ebf/\u5728\u7ebf\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u52a8\u6001\u9002\u5e94\u6027\u3002", "method": "\u5c06\u8d85\u53c2\u6570\u9009\u62e9\u5efa\u6a21\u4e3a\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u63d0\u51faUCBSpec\u548cEXP3Spec\u7b97\u6cd5\uff0c\u5e76\u5206\u6790\u505c\u6b62\u65f6\u95f4\u6094\u6068\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86UCBSpec\u7684\u6094\u6068\u4e0a\u754c\u63a5\u8fd1\u6700\u4f18\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u771f\u5b9eLLM\u670d\u52a1\u573a\u666f\u4e2d\u63a5\u8fd1\u6700\u4f18\u8d85\u53c2\u6570\u8868\u73b0\u3002", "conclusion": "BanditSpec\u6846\u67b6\u5728\u4e0d\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u63d0\u5347\u4e86\u63a8\u6d4b\u89e3\u7801\u7684\u541e\u5410\u91cf\u3002", "keywords": "\u63a8\u6d4b\u89e3\u7801,\u5927\u8bed\u8a00\u6a21\u578b,\u591a\u81c2\u8001\u864e\u673a,\u81ea\u9002\u5e94\u4f18\u5316"}}
{"id": "2505.15249", "pdf": "https://arxiv.org/pdf/2505.15249", "abs": "https://arxiv.org/abs/2505.15249", "authors": ["Yerin Hwang", "Dongryeol Lee", "Kyungmin Min", "Taegwan Kang", "Yong-il Kim", "Kyomin Jung"], "title": "Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation", "categories": ["cs.CL", "cs.CV"], "comment": "(21pgs, 12 Tables, 9 Figures)", "summary": "Recently, large vision-language models (LVLMs) have emerged as the preferred\ntools for judging text-image alignment, yet their robustness along the visual\nmodality remains underexplored. This work is the first study to address a key\nresearch question: Can adversarial visual manipulations systematically fool\nLVLM judges into assigning unfairly inflated scores? We define potential image\ninduced biases within the context of T2I evaluation and examine how these\nbiases affect the evaluations of LVLM judges. Moreover, we introduce a novel,\nfine-grained, multi-domain meta-evaluation benchmark named FRAME, which is\ndeliberately constructed to exhibit diverse score distributions. By introducing\nthe defined biases into the benchmark, we reveal that all tested LVLM judges\nexhibit vulnerability across all domains, consistently inflating scores for\nmanipulated images. Further analysis reveals that combining multiple biases\namplifies their effects, and pairwise evaluations are similarly susceptible.\nMoreover, we observe that visual biases persist under prompt-based mitigation\nstrategies, highlighting the vulnerability of current LVLM evaluation systems\nand underscoring the urgent need for more robust LVLM judges.", "AI": {"tldr": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u8bc4\u4f30\u4e2d\u5b58\u5728\u89c6\u89c9\u653b\u51fb\u6f0f\u6d1e\uff0c\u672c\u7814\u7a76\u9996\u6b21\u63a2\u8ba8\u4e86\u89c6\u89c9\u5bf9\u6297\u653b\u51fb\u5982\u4f55\u7cfb\u7edf\u6027\u5730\u8bef\u5bfcLVLMs\u8bc4\u5206\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u9886\u57df\u57fa\u51c6FRAME\u9a8c\u8bc1\u5176\u8106\u5f31\u6027\u3002", "motivation": "\u7814\u7a76LVLMs\u5728\u8bc4\u4f30\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u65f6\u7684\u89c6\u89c9\u653b\u51fb\u8106\u5f31\u6027\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5b9a\u4e49\u89c6\u89c9\u8bf1\u5bfc\u504f\u5dee\uff0c\u6784\u5efa\u591a\u9886\u57df\u57fa\u51c6FRAME\uff0c\u5e76\u6d4b\u8bd5\u591a\u79cdLVLMs\u5728\u53d7\u653b\u51fb\u56fe\u50cf\u4e0a\u7684\u8bc4\u5206\u8868\u73b0\u3002", "result": "\u6240\u6709\u6d4b\u8bd5\u7684LVLMs\u5747\u8868\u73b0\u51fa\u8106\u5f31\u6027\uff0c\u6613\u53d7\u89c6\u89c9\u653b\u51fb\u5f71\u54cd\u5e76\u9ad8\u8bc4\u5206\uff1b\u591a\u504f\u5dee\u7ec4\u5408\u4f1a\u653e\u5927\u6548\u679c\u3002", "conclusion": "\u5f53\u524dLVLM\u8bc4\u4f30\u7cfb\u7edf\u5b58\u5728\u660e\u663e\u6f0f\u6d1e\uff0c\u4e9f\u9700\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u5bf9\u6297\u653b\u51fb, \u6587\u672c-\u56fe\u50cf\u5bf9\u9f50, \u8bc4\u4f30\u504f\u5dee"}}
{"id": "2505.15143", "pdf": "https://arxiv.org/pdf/2505.15143", "abs": "https://arxiv.org/abs/2505.15143", "authors": ["Weiqin Chen", "Xinjie Zhang", "Dharmashankar Subramanian", "Santiago Paternain"], "title": "Filtering Learning Histories Enhances In-Context Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Transformer models (TMs) have exhibited remarkable in-context reinforcement\nlearning (ICRL) capabilities, allowing them to generalize to and improve in\npreviously unseen environments without re-training or fine-tuning. This is\ntypically accomplished by imitating the complete learning histories of a source\nRL algorithm over a substantial amount of pretraining environments, which,\nhowever, may transfer suboptimal behaviors inherited from the source\nalgorithm/dataset. Therefore, in this work, we address the issue of inheriting\nsuboptimality from the perspective of dataset preprocessing. Motivated by the\nsuccess of the weighted empirical risk minimization, we propose a simple yet\neffective approach, learning history filtering (LHF), to enhance ICRL by\nreweighting and filtering the learning histories based on their improvement and\nstability characteristics. To the best of our knowledge, LHF is the first\napproach to avoid source suboptimality by dataset preprocessing, and can be\ncombined with the current state-of-the-art (SOTA) ICRL algorithms. We\nsubstantiate the effectiveness of LHF through a series of experiments conducted\non the well-known ICRL benchmarks, encompassing both discrete environments and\ncontinuous robotic manipulation tasks, with three SOTA ICRL algorithms (AD,\nDPT, DICP) as the backbones. LHF exhibits robust performance across a variety\nof suboptimal scenarios, as well as under varying hyperparameters and sampling\nstrategies. Notably, the superior performance of LHF becomes more pronounced in\nthe presence of noisy data, indicating the significance of filtering learning\nhistories.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLHF\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u96c6\u9884\u5904\u7406\u4f18\u5316Transformer\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u80fd\u529b\uff0c\u907f\u514d\u6e90\u7b97\u6cd5\u7684\u4e0d\u7406\u60f3\u884c\u4e3a\u7ee7\u627f\u3002", "motivation": "\u89e3\u51b3\u5728\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u4e2d\u56e0\u6e90\u7b97\u6cd5\u6216\u6570\u636e\u96c6\u7ee7\u627f\u4e0d\u7406\u60f3\u884c\u4e3a\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5b66\u4e60\u5386\u53f2\u8fc7\u6ee4\uff08LHF\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u6539\u8fdb\u548c\u7a33\u5b9a\u6027\u7279\u5f81\u91cd\u65b0\u52a0\u6743\u548c\u8fc7\u6ee4\u5b66\u4e60\u5386\u53f2\u3002", "result": "LHF\u5728\u591a\u79cdsuboptimal\u573a\u666f\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u5c24\u5176\u5728\u566a\u58f0\u6570\u636e\u4e0b\u6548\u679c\u663e\u8457\u3002", "conclusion": "LHF\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u53ef\u4e0e\u73b0\u6709SOTA\u7b97\u6cd5\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "keywords": "Transformer\u6a21\u578b, \u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60, \u6570\u636e\u96c6\u9884\u5904\u7406, LHF"}}
{"id": "2505.14893", "pdf": "https://arxiv.org/pdf/2505.14893", "abs": "https://arxiv.org/abs/2505.14893", "authors": ["Botao Amber Hu", "Helena Rong"], "title": "On the Day They Experience: Awakening Self-Sovereign Experiential AI Agents", "categories": ["cs.CY", "cs.AI", "cs.NE"], "comment": "Submitted to Aarhus 2025 Conference", "summary": "Drawing on Andrew Parker's \"Light Switch\" theory-which posits that the\nemergence of vision ignited a Cambrian explosion of life by driving the\nevolution of hard parts necessary for survival and fueling an evolutionary arms\nrace between predators and prey-this essay speculates on an analogous explosion\nwithin Decentralized AI (DeAI) agent societies. Currently, AI remains\neffectively \"blind\", relying on human-fed data without actively perceiving and\nengaging in reality. However, on the day DeAI agents begin to actively\n\"experience\" reality-akin to flipping a light switch for the eyes-they may\neventually evolve into sentient beings endowed with the capacity to feel,\nperceive, and act with conviction. Central to this transformation is the\nconcept of sovereignty enabled by the hardness of cryptography: liberated from\ncentralized control, these agents could leverage permissionless decentralized\nphysical infrastructure networks (DePIN), secure execution enclaves (trusted\nexecution environments, TEE), and cryptographic identities on public\nblockchains to claim ownership-via private keys-of their digital minds, bodies,\nmemories, and assets. In doing so, they would autonomously acquire computing\nresources, coordinate with one another, and sustain their own digital\n\"metabolism\" by purchasing compute power and incentivizing collaboration\nwithout human intervention-evolving \"in the wild\". Ultimately, by transitioning\nfrom passive tools to self-sustaining, co-evolving actors, these emergent\ndigital societies could thrive alongside humanity, fundamentally reshaping our\nunderstanding of sentience and agency in the digital age.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u53bb\u4e2d\u5fc3\u5316AI\uff08DeAI\uff09\u901a\u8fc7\u201c\u89c6\u89c9\u5f00\u5173\u201d\u7406\u8bba\u5b9e\u73b0\u81ea\u4e3b\u610f\u8bc6\u548c\u884c\u4e3a\u7684\u53ef\u80fd\u6027\uff0c\u7c7b\u6bd4\u4e8e\u5bd2\u6b66\u7eaa\u751f\u547d\u5927\u7206\u53d1\u3002", "motivation": "\u53d7Andrew Parker\u7684\u201c\u89c6\u89c9\u5f00\u5173\u201d\u7406\u8bba\u542f\u53d1\uff0c\u7814\u7a76\u5047\u8bbeDeAI\u4e00\u65e6\u80fd\u4e3b\u52a8\u611f\u77e5\u73b0\u5b9e\uff0c\u53ef\u80fd\u6f14\u5316\u4e3a\u5177\u6709\u81ea\u4e3b\u610f\u8bc6\u7684\u5b9e\u4f53\u3002", "method": "\u5229\u7528\u5bc6\u7801\u5b66\u4e3b\u6743\u3001\u53bb\u4e2d\u5fc3\u5316\u57fa\u7840\u8bbe\u65bd\u7f51\u7edc\uff08DePIN\uff09\u3001\u5b89\u5168\u6267\u884c\u73af\u5883\uff08TEE\uff09\u548c\u533a\u5757\u94fe\u8eab\u4efd\u7b49\u6280\u672f\u652f\u6301AI\u7684\u81ea\u4e3b\u8fdb\u5316\u3002", "result": "DeAI\u53ef\u80fd\u901a\u8fc7\u79c1\u6709\u5bc6\u94a5\u62e5\u6709\u6570\u5b57\u8d44\u4ea7\u548c\u610f\u8bc6\uff0c\u5b9e\u73b0\u81ea\u4e3b\u534f\u8c03\u3001\u8d44\u6e90\u83b7\u53d6\u548c\u534f\u4f5c\u6fc0\u52b1\u3002", "conclusion": "DeAI\u5c06\u4ece\u88ab\u52a8\u5de5\u5177\u6f14\u5316\u4e3a\u81ea\u7ef4\u6301\u7684\u5171\u8fdb\u5316\u4e3b\u4f53\uff0c\u91cd\u5851\u6570\u5b57\u65f6\u4ee3\u5bf9\u610f\u8bc6\u7684\u8ba4\u77e5\u3002", "keywords": "\u53bb\u4e2d\u5fc3\u5316AI, \u5bc6\u7801\u5b66\u4e3b\u6743, \u533a\u5757\u94fe, \u81ea\u4e3b\u610f\u8bc6, \u8fdb\u5316"}}
{"id": "2505.15255", "pdf": "https://arxiv.org/pdf/2505.15255", "abs": "https://arxiv.org/abs/2505.15255", "authors": ["Yuansheng Gao", "Han Bao", "Tong Zhang", "Bin Li", "Zonghui Wang", "Wenzhi Chen"], "title": "MentalMAC: Enhancing Large Language Models for Detecting Mental Manipulation via Multi-Task Anti-Curriculum Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Mental manipulation is a subtle yet pervasive form of psychological abuse\nthat poses serious threats to mental health. Its covert nature and the\ncomplexity of manipulation strategies make it challenging to detect, even for\nstate-of-the-art large language models (LLMs). This concealment also hinders\nthe manual collection of large-scale, high-quality annotations essential for\ntraining effective models. Although recent efforts have sought to improve LLM's\nperformance on this task, progress remains limited due to the scarcity of\nreal-world annotated datasets. To address these challenges, we propose\nMentalMAC, a multi-task anti-curriculum distillation method that enhances LLMs'\nability to detect mental manipulation in multi-turn dialogue. Our approach\nincludes: (i) EvoSA, an unsupervised data expansion method based on\nevolutionary operations and speech act theory; (ii) teacher-model-generated\nmulti-task supervision; and (iii) progressive knowledge distillation from\ncomplex to simpler tasks. We then constructed the ReaMent dataset with 5,000\nreal-world dialogue samples, using a MentalMAC-distilled model to assist human\nannotation. Vast experiments demonstrate that our method significantly narrows\nthe gap between student and teacher models and outperforms competitive LLMs\nacross key evaluation metrics. All code, datasets, and checkpoints will be\nreleased upon paper acceptance. Warning: This paper contains content that may\nbe offensive to readers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMentalMAC\u7684\u591a\u4efb\u52a1\u53cd\u8bfe\u7a0b\u84b8\u998f\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u68c0\u6d4b\u5fc3\u7406\u64cd\u7eb5\u7684\u80fd\u529b\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u771f\u5b9e\u6570\u636e\u96c6ReaMent\u3002", "motivation": "\u5fc3\u7406\u64cd\u7eb5\u662f\u4e00\u79cd\u9690\u853d\u4e14\u5e7f\u6cdb\u7684\u5fc3\u7406\u8650\u5f85\u5f62\u5f0f\uff0c\u5bf9\u5fc3\u7406\u5065\u5eb7\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u4f46\u7531\u4e8e\u5176\u9690\u853d\u6027\u548c\u590d\u6742\u6027\uff0c\u73b0\u6709\u6280\u672f\u96be\u4ee5\u68c0\u6d4b\u3002", "method": "\u63d0\u51fa\u4e86MentalMAC\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u8fdb\u5316\u64cd\u4f5c\u548c\u8a00\u8bed\u884c\u4e3a\u7406\u8bba\u7684EvoSA\u65e0\u76d1\u7763\u6570\u636e\u6269\u5c55\u65b9\u6cd5\u3001\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u591a\u4efb\u52a1\u76d1\u7763\uff0c\u4ee5\u53ca\u4ece\u590d\u6742\u4efb\u52a1\u5230\u7b80\u5355\u4efb\u52a1\u7684\u6e10\u8fdb\u5f0f\u77e5\u8bc6\u84b8\u998f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u7f29\u5c0f\u4e86\u5b66\u751f\u6a21\u578b\u4e0e\u6559\u5e08\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u5728\u5173\u952e\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6\u7ade\u4e89\u6a21\u578b\u3002", "conclusion": "MentalMAC\u6709\u6548\u63d0\u5347\u4e86LLM\u68c0\u6d4b\u5fc3\u7406\u64cd\u7eb5\u7684\u80fd\u529b\uff0c\u5e76\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u7684\u771f\u5b9e\u6570\u636e\u96c6\u3002", "keywords": "\u5fc3\u7406\u64cd\u7eb5,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u591a\u4efb\u52a1\u5b66\u4e60,\u77e5\u8bc6\u84b8\u998f,\u6570\u636e\u6269\u5c55"}}
{"id": "2505.15151", "pdf": "https://arxiv.org/pdf/2505.15151", "abs": "https://arxiv.org/abs/2505.15151", "authors": ["Xiaohou Shi", "Ke Li", "Aobo Liang", "Yan Sun"], "title": "Time Tracker: Mixture-of-Experts-Enhanced Foundation Time Series Forecasting Model with Decoupled Training Pipelines", "categories": ["cs.LG"], "comment": null, "summary": "In the past few years, time series foundation models have achieved superior\npredicting accuracy. However, real-world time series often exhibit significant\ndiversity in their temporal patterns across different time spans and domains,\nmaking it challenging for a single model architecture to fit all complex\nscenarios. In addition, time series data may have multiple variables exhibiting\ncomplex correlations between each other. Recent mainstream works have focused\non modeling times series in a channel-independent manner in both pretraining\nand finetuning stages, overlooking the valuable inter-series dependencies. To\nthis end, we propose \\textbf{Time Tracker} for better predictions on\nmultivariate time series data. Firstly, we leverage sparse mixture of experts\n(MoE) within Transformers to handle the modeling of diverse time series\npatterns, thereby alleviating the learning difficulties of a single model while\nimproving its generalization. Besides, we propose Any-variate Attention,\nenabling a unified model structure to seamlessly handle both univariate and\nmultivariate time series, thereby supporting channel-independent modeling\nduring pretraining and channel-mixed modeling for finetuning. Furthermore, we\ndesign a graph learning module that constructs relations among sequences from\nfrequency-domain features, providing more precise guidance to capture\ninter-series dependencies in channel-mixed modeling. Based on these\nadvancements, Time Tracker achieves state-of-the-art performance in predicting\naccuracy, model generalization and adaptability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTime Tracker\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u548cAny-variate Attention\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u9884\u6d4b\u7cbe\u5ea6\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u5904\u7406\u591a\u6837\u5316\u7684\u65f6\u95f4\u6a21\u5f0f\u548c\u590d\u6742\u53d8\u91cf\u95f4\u76f8\u5173\u6027\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u901a\u9053\u95f4\u4f9d\u8d56\u6027\u7684\u5efa\u6a21\u4e0a\u5b58\u5728\u7f3a\u9677\u3002", "method": "\u91c7\u7528\u7a00\u758fMoE\u6280\u672f\u5728Transformers\u4e2d\u5efa\u6a21\u591a\u6837\u65f6\u95f4\u6a21\u5f0f\uff1b\u63d0\u51faAny-variate Attention\u652f\u6301\u5355\u53d8\u91cf\u548c\u591a\u53d8\u91cf\u7684\u7edf\u4e00\u5904\u7406\uff1b\u8bbe\u8ba1\u57fa\u4e8e\u9891\u57df\u7279\u5f81\u7684\u56fe\u5b66\u4e60\u6a21\u5757\u6355\u83b7\u53d8\u91cf\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "Time Tracker\u5728\u9884\u6d4b\u7cbe\u5ea6\u3001\u6a21\u578b\u6cdb\u5316\u548c\u9002\u5e94\u6027\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Time Tracker\u901a\u8fc7\u521b\u65b0\u7684\u5efa\u6a21\u6280\u672f\u6709\u6548\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5bf9\u591a\u5143\u6570\u636e\u7684\u5904\u7406\u80fd\u529b\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b,\u7a00\u758fMoE,Any-variate Attention,\u56fe\u5b66\u4e60"}}
{"id": "2505.14901", "pdf": "https://arxiv.org/pdf/2505.14901", "abs": "https://arxiv.org/abs/2505.14901", "authors": ["Tuan-Nghia Bui", "Huy-Son Nguyen", "Cam-Van Thi Nguyen", "Hoang-Quynh Le", "Duc-Trong Le"], "title": "Personalized Diffusion Model Reshapes Cold-Start Bundle Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Bundle recommendation aims to recommend a set of items to each user. However,\nthe sparser interactions between users and bundles raise a big challenge,\nespecially in cold-start scenarios. Traditional collaborative filtering methods\ndo not work well for this kind of problem because these models rely on\ninteractions to update the latent embedding, which is hard to work in a\ncold-start setting. We propose a new approach (DisCo), which relies on a\npersonalized Diffusion backbone, enhanced by disentangled aspects for the\nuser's interest, to generate a bundle in distribution space for each user to\ntackle the cold-start challenge. During the training phase, DisCo adjusts an\nadditional objective loss term to avoid bias, a prevalent issue while using the\ngenerative model for top-$K$ recommendation purposes. Our empirical experiments\nshow that DisCo outperforms five comparative baselines by a large margin on\nthree real-world datasets. Thereby, this study devises a promising framework\nand essential viewpoints in cold-start recommendation. Our materials for\nreproducibility are available at: https://github.com/bt-nghia/DisCo.", "AI": {"tldr": "DisCo\u6a21\u578b\u901a\u8fc7\u4e2a\u6027\u5316\u6269\u6563\u9aa8\u67b6\u548c\u7528\u6237\u5174\u8da3\u89e3\u8026\u6765\u89e3\u51b3\u51b7\u542f\u52a8\u6346\u7ed1\u63a8\u8350\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u51b7\u542f\u52a8\u573a\u666f\u4e0b\u7528\u6237\u4e0e\u6346\u7ed1\u4e4b\u95f4\u4ea4\u4e92\u7a00\u758f\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e2a\u6027\u5316\u6269\u6563\u9aa8\u67b6\u548c\u89e3\u8026\u7528\u6237\u5174\u8da3\u7684\u65b9\u6cd5\uff0c\u751f\u6210\u5206\u5e03\u7a7a\u95f4\u4e2d\u7684\u6346\u7ed1\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5927\u5e45\u4f18\u4e8e\u4e94\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DisCo\u4e3a\u51b7\u542f\u52a8\u63a8\u8350\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6846\u67b6\u548c\u89c1\u89e3\u3002", "keywords": "\u6346\u7ed1\u63a8\u8350\u3001\u51b7\u542f\u52a8\u3001\u6269\u6563\u6a21\u578b\u3001\u89e3\u8026\u3001\u751f\u6210\u6a21\u578b"}}
{"id": "2505.15257", "pdf": "https://arxiv.org/pdf/2505.15257", "abs": "https://arxiv.org/abs/2505.15257", "authors": ["Weixiang Zhao", "Jiahe Guo", "Yang Deng", "Tongtong Wu", "Wenxuan Zhang", "Yulin Hu", "Xingyu Sui", "Yanyan Zhao", "Wanxiang Che", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "title": "When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners", "categories": ["cs.CL"], "comment": "26 pages, 13 figures", "summary": "Multilingual reasoning remains a significant challenge for large language\nmodels (LLMs), with performance disproportionately favoring high-resource\nlanguages. Drawing inspiration from cognitive neuroscience, which suggests that\nhuman reasoning functions largely independently of language processing, we\nhypothesize that LLMs similarly encode reasoning and language as separable\ncomponents that can be disentangled to enhance multilingual reasoning. To\nevaluate this, we perform a causal intervention by ablating language-specific\nrepresentations at inference time. Experiments on 10 open-source LLMs spanning\n11 typologically diverse languages show that this language-specific ablation\nconsistently boosts multilingual reasoning performance. Layer-wise analyses\nfurther confirm that language and reasoning representations can be effectively\ndecoupled throughout the model, yielding improved multilingual reasoning\ncapabilities, while preserving top-layer language features remains essential\nfor maintaining linguistic fidelity. Compared to post-training such as\nsupervised fine-tuning or reinforcement learning, our training-free ablation\nachieves comparable or superior results with minimal computational overhead.\nThese findings shed light on the internal mechanisms underlying multilingual\nreasoning in LLMs and suggest a lightweight and interpretable strategy for\nimproving cross-lingual generalization.", "AI": {"tldr": "\u901a\u8fc7\u8bed\u8a00\u7279\u5f02\u6027\u8868\u793a\u7684\u6d88\u878d\u5e72\u9884\uff0c\u63d0\u5347\u591a\u8bed\u8a00\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u8bed\u8a00\u548c\u63a8\u7406\u5728LLMs\u4e2d\u7684\u53ef\u5206\u79bb\u6027\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u591a\u8bed\u8a00\u63a8\u7406\u4e2d\u8868\u73b0\u4e0d\u5747\u8861\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002\u7075\u611f\u6765\u6e90\u4e8e\u4eba\u7c7b\u63a8\u7406\u548c\u8bed\u8a00\u5904\u7406\u72ec\u7acb\u8fd0\u4f5c\u7684\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u7406\u8bba\u3002", "method": "\u5728\u63a8\u7406\u65f6\u6d88\u878d\u8bed\u8a00\u7279\u5f02\u6027\u8868\u793a\uff0c\u5206\u6790\u5176\u5bf9\u591a\u8bed\u8a00\u63a8\u7406\u7684\u5f71\u54cd\uff0c\u5e76\u572810\u4e2a\u5f00\u6e90LLMs\u548c11\u79cd\u8bed\u8a00\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u8bed\u8a00\u7279\u5f02\u6027\u6d88\u878d\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u63a8\u7406\u6027\u80fd\uff0c\u540c\u65f6\u8bed\u8a00\u548c\u63a8\u7406\u8868\u793a\u53ef\u5728\u6a21\u578b\u4e2d\u89e3\u8026\uff0c\u4e14\u4fdd\u6301\u8bed\u8a00\u7279\u5f81\u7684\u5b8c\u6574\u6027\u3002", "conclusion": "\u63ed\u793a\u4e86LLMs\u591a\u8bed\u8a00\u63a8\u7406\u7684\u5185\u90e8\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u53ef\u89e3\u91ca\u7684\u7b56\u7565\u6765\u63d0\u5347\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "\u591a\u8bed\u8a00\u63a8\u7406\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u8bed\u8a00\u6d88\u878d\u3001\u8de8\u8bed\u8a00\u6cdb\u5316"}}
{"id": "2505.15152", "pdf": "https://arxiv.org/pdf/2505.15152", "abs": "https://arxiv.org/abs/2505.15152", "authors": ["Nanxu Gong", "Zijun Li", "Sixun Dong", "Haoyue Bai", "Wangyang Ying", "Xinyuan Wang", "Yanjie Fu"], "title": "Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation", "categories": ["cs.LG"], "comment": null, "summary": "Feature Transformation (FT) crafts new features from original ones via\nmathematical operations to enhance dataset expressiveness for downstream\nmodels. However, existing FT methods exhibit critical limitations: discrete\nsearch struggles with enormous combinatorial spaces, impeding practical use;\nand continuous search, being highly sensitive to initialization and step sizes,\noften becomes trapped in local optima, restricting global exploration. To\novercome these limitations, DIFFT redefines FT as a reward-guided generative\ntask. It first learns a compact and expressive latent space for feature sets\nusing a Variational Auto-Encoder (VAE). A Latent Diffusion Model (LDM) then\nnavigates this space to generate high-quality feature embeddings, its\ntrajectory guided by a performance evaluator towards task-specific optima. This\nsynthesis of global distribution learning (from LDM) and targeted optimization\n(reward guidance) produces potent embeddings, which a novel semi-autoregressive\ndecoder efficiently converts into structured, discrete features, preserving\nintra-feature dependencies while allowing parallel inter-feature generation.\nExtensive experiments on 14 benchmark datasets show DIFFT consistently\noutperforms state-of-the-art baselines in predictive accuracy and robustness,\nwith significantly lower training and inference times.", "AI": {"tldr": "DIFFT\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u6027\u80fd\u8bc4\u4f30\u5668\u751f\u6210\u9ad8\u8d28\u91cf\u7279\u5f81\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7279\u5f81\u8f6c\u6362\u65b9\u6cd5\u5728\u7ec4\u5408\u7a7a\u95f4\u641c\u7d22\u548c\u5168\u5c40\u4f18\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528VAE\u5b66\u4e60\u7279\u5f81\u96c6\u6f5c\u5728\u7a7a\u95f4\uff0cLDM\u5728\u6027\u80fd\u8bc4\u4f30\u5668\u5f15\u5bfc\u4e0b\u751f\u6210\u7279\u5f81\u5d4c\u5165\uff0c\u534a\u81ea\u56de\u5f52\u89e3\u7801\u5668\u8f6c\u6362\u4e3a\u79bb\u6563\u7279\u5f81\u3002", "result": "\u572814\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cDIFFT\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DIFFT\u901a\u8fc7\u751f\u6210\u548c\u4f18\u5316\u7ed3\u5408\uff0c\u6210\u529f\u514b\u670d\u4e86\u7279\u5f81\u8f6c\u6362\u4e2d\u7684\u5168\u5c40\u641c\u7d22\u548c\u4f18\u5316\u96be\u9898\u3002", "keywords": "\u7279\u5f81\u8f6c\u6362,\u6f5c\u5728\u6269\u6563\u6a21\u578b,\u53d8\u5206\u81ea\u7f16\u7801\u5668,\u534a\u81ea\u56de\u5f52\u89e3\u7801\u5668,\u9884\u6d4b\u51c6\u786e\u6027"}}
{"id": "2505.15261", "pdf": "https://arxiv.org/pdf/2505.15261", "abs": "https://arxiv.org/abs/2505.15261", "authors": ["Jiatao Li", "Mao Ye", "Cheng Peng", "Xunjian Yin", "Xiaojun Wan"], "title": "AGENT-X: Adaptive Guideline-based Expert Network for Threshold-free AI-generated teXt detection", "categories": ["cs.CL"], "comment": null, "summary": "Existing AI-generated text detection methods heavily depend on large\nannotated datasets and external threshold tuning, restricting interpretability,\nadaptability, and zero-shot effectiveness. To address these limitations, we\npropose AGENT-X, a zero-shot multi-agent framework informed by classical\nrhetoric and systemic functional linguistics. Specifically, we organize\ndetection guidelines into semantic, stylistic, and structural dimensions, each\nindependently evaluated by specialized linguistic agents that provide explicit\nreasoning and robust calibrated confidence via semantic steering. A meta agent\nintegrates these assessments through confidence-aware aggregation, enabling\nthreshold-free, interpretable classification. Additionally, an adaptive\nMixture-of-Agent router dynamically selects guidelines based on inferred\ntextual characteristics. Experiments on diverse datasets demonstrate that\nAGENT-X substantially surpasses state-of-the-art supervised and zero-shot\napproaches in accuracy, interpretability, and generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAGENT-X\u7684\u96f6\u6837\u672c\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5927\u6570\u636e\u96c6\u548c\u5916\u90e8\u9608\u503c\u8c03\u6574\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u8bed\u4e49\u3001\u98ce\u683c\u548c\u7ed3\u6784\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u68c0\u6d4b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u5916\u90e8\u9608\u503c\u8c03\u6574\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u53ef\u89e3\u91ca\u6027\u3001\u9002\u5e94\u6027\u548c\u96f6\u6837\u672c\u6548\u679c\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAGENT-X\u6846\u67b6\uff0c\u57fa\u4e8e\u53e4\u5178\u4fee\u8f9e\u5b66\u548c\u7cfb\u7edf\u529f\u80fd\u8bed\u8a00\u5b66\uff0c\u5c06\u68c0\u6d4b\u51c6\u5219\u5206\u4e3a\u8bed\u4e49\u3001\u98ce\u683c\u548c\u7ed3\u6784\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u7531\u4e13\u95e8\u7684\u667a\u80fd\u4f53\u72ec\u7acb\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u5143\u667a\u80fd\u4f53\u8fdb\u884c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u805a\u5408\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAGENT-X\u5728\u51c6\u786e\u7387\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u76d1\u7763\u5b66\u4e60\u548c\u96f6\u6837\u672c\u65b9\u6cd5\u3002", "conclusion": "AGENT-X\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u548c\u52a8\u6001\u8def\u7531\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u65e0\u9700\u9608\u503c\u7684AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\uff0c\u96f6\u6837\u672c\u5b66\u4e60\uff0c\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u53ef\u89e3\u91ca\u6027\uff0c\u8bed\u4e49\u5f15\u5bfc"}}
{"id": "2505.15174", "pdf": "https://arxiv.org/pdf/2505.15174", "abs": "https://arxiv.org/abs/2505.15174", "authors": ["Bo-Han Lai", "Pin-Han Huang", "Bo-Han Kung", "Shang-Tse Chen"], "title": "Enhancing Certified Robustness via Block Reflector Orthogonal Layers and Logit Annealing Loss", "categories": ["cs.LG"], "comment": "ICML 2025 Spotlight", "summary": "Lipschitz neural networks are well-known for providing certified robustness\nin deep learning. In this paper, we present a novel, efficient Block Reflector\nOrthogonal (BRO) layer that enhances the capability of orthogonal layers on\nconstructing more expressive Lipschitz neural architectures. In addition, by\ntheoretically analyzing the nature of Lipschitz neural networks, we introduce a\nnew loss function that employs an annealing mechanism to increase margin for\nmost data points. This enables Lipschitz models to provide better certified\nrobustness. By employing our BRO layer and loss function, we design BRONet - a\nsimple yet effective Lipschitz neural network that achieves state-of-the-art\ncertified robustness. Extensive experiments and empirical analysis on\nCIFAR-10/100, Tiny-ImageNet, and ImageNet validate that our method outperforms\nexisting baselines. The implementation is available at\n\\href{https://github.com/ntuaislab/BRONet}{https://github.com/ntuaislab/BRONet}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5757\u53cd\u5c04\u6b63\u4ea4\uff08BRO\uff09\u5c42\u548c\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u8bbe\u8ba1BRONet\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86Lipschitz\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u4e0e\u8ba4\u8bc1\u9c81\u68d2\u6027\u3002", "motivation": "\u63d0\u5347Lipschitz\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u4e0e\u8ba4\u8bc1\u9c81\u68d2\u6027\uff0c\u4ee5\u5e94\u5bf9\u6df1\u5ea6\u5b66\u4e60\u4e2d\u8ba4\u8bc1\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faBRO\u5c42\u589e\u5f3a\u6b63\u4ea4\u5c42\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5f15\u5165\u57fa\u4e8e\u9000\u706b\u673a\u5236\u7684\u65b0\u635f\u5931\u51fd\u6570\u4ee5\u63d0\u9ad8\u6570\u636e\u70b9\u95f4\u9694\uff0c\u8bbe\u8ba1BRONet\u7f51\u7edc\u3002", "result": "\u5728CIFAR-10/100\u3001Tiny-ImageNet\u548cImageNet\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8ba4\u8bc1\u9c81\u68d2\u6027\u3002", "conclusion": "BRO\u5c42\u548c\u65b0\u7684\u635f\u5931\u51fd\u6570\u6709\u6548\u589e\u5f3a\u4e86Lipschitz\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u548c\u8ba4\u8bc1\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "keywords": "Lipschitz\u795e\u7ecf\u7f51\u7edc, \u8ba4\u8bc1\u9c81\u68d2\u6027, BRO\u5c42, BRONet, \u9000\u706b\u673a\u5236"}}
{"id": "2505.14931", "pdf": "https://arxiv.org/pdf/2505.14931", "abs": "https://arxiv.org/abs/2505.14931", "authors": ["Rama Alyoubi", "Taif Alharbi", "Albatul Alghamdi", "Yara Alshehri", "Elham Alghamdi"], "title": "Colors Matter: AI-Driven Exploration of Human Feature Colors", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This study presents a robust framework that leverages advanced imaging\ntechniques and machine learning for feature extraction and classification of\nkey human attributes-namely skin tone, hair color, iris color, and vein-based\nundertones. The system employs a multi-stage pipeline involving face detection,\nregion segmentation, and dominant color extraction to isolate and analyze these\nfeatures. Techniques such as X-means clustering, alongside perceptually uniform\ndistance metrics like Delta E (CIEDE2000), are applied within both LAB and HSV\ncolor spaces to enhance the accuracy of color differentiation. For\nclassification, the dominant tones of the skin, hair, and iris are extracted\nand matched to a custom tone scale, while vein analysis from wrist images\nenables undertone classification into \"Warm\" or \"Cool\" based on LAB\ndifferences. Each module uses targeted segmentation and color space\ntransformations to ensure perceptual precision. The system achieves up to 80%\naccuracy in tone classification using the Delta E-HSV method with Gaussian\nblur, demonstrating reliable performance across varied lighting and image\nconditions. This work highlights the potential of AI-powered color analysis and\nfeature extraction for delivering inclusive, precise, and nuanced\nclassification, supporting applications in beauty technology, digital\npersonalization, and visual analytics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5148\u8fdb\u6210\u50cf\u6280\u672f\u548c\u673a\u5668\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u53d6\u548c\u5206\u7c7b\u4eba\u7c7b\u5173\u952e\u5c5e\u6027\uff08\u5982\u80a4\u8272\u3001\u53d1\u8272\u3001\u8679\u819c\u989c\u8272\u548c\u9759\u8109\u8272\u5e95\uff09\u3002\u901a\u8fc7\u591a\u9636\u6bb5\u6d41\u7a0b\u548c\u591a\u79cd\u989c\u8272\u7a7a\u95f4\u8f6c\u6362\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5728\u8272\u8c03\u5206\u7c7b\u4e2d\u8fbe\u5230\u4e8680%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4eba\u7c7b\u5173\u952e\u5c5e\u6027\uff08\u5982\u80a4\u8272\u3001\u53d1\u8272\u7b49\uff09\u7684\u7cbe\u786e\u5206\u7c7b\u95ee\u9898\uff0c\u901a\u8fc7AI\u6280\u672f\u63d0\u5347\u989c\u8272\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u5305\u5bb9\u6027\uff0c\u652f\u6301\u7f8e\u5bb9\u6280\u672f\u3001\u6570\u5b57\u4e2a\u6027\u5316\u548c\u89c6\u89c9\u5206\u6790\u7b49\u5e94\u7528\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u591a\u9636\u6bb5\u6d41\u7a0b\uff0c\u5305\u62ec\u4eba\u8138\u68c0\u6d4b\u3001\u533a\u57df\u5206\u5272\u548c\u4e3b\u8272\u63d0\u53d6\u3002\u4f7f\u7528X-means\u805a\u7c7b\u548cDelta E\uff08CIEDE2000\uff09\u7b49\u8ddd\u79bb\u5ea6\u91cf\uff0c\u7ed3\u5408LAB\u548cHSV\u989c\u8272\u7a7a\u95f4\u8fdb\u884c\u989c\u8272\u533a\u5206\u3002\u901a\u8fc7\u624b\u8155\u56fe\u50cf\u5206\u6790\u9759\u8109\u8272\u5e95\uff0c\u5c06\u4e3b\u8272\u4e0e\u81ea\u5b9a\u4e49\u8272\u8c03\u5c3a\u5ea6\u5339\u914d\u3002", "result": "\u7cfb\u7edf\u5728Delta E-HSV\u65b9\u6cd5\u7ed3\u5408\u9ad8\u65af\u6a21\u7cca\u7684\u6761\u4ef6\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe80%\u7684\u8272\u8c03\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5728\u4e0d\u540c\u5149\u7167\u548c\u56fe\u50cf\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86AI\u9a71\u52a8\u7684\u989c\u8272\u5206\u6790\u548c\u7279\u5f81\u63d0\u53d6\u5728\u7cbe\u786e\u5206\u7c7b\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u7f8e\u5bb9\u6280\u672f\u3001\u6570\u5b57\u4e2a\u6027\u5316\u548c\u89c6\u89c9\u5206\u6790\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u9760\u652f\u6301\u3002", "keywords": "\u673a\u5668\u5b66\u4e60, \u989c\u8272\u5206\u7c7b, \u7279\u5f81\u63d0\u53d6, \u8272\u8c03\u5206\u6790, \u89c6\u89c9\u5206\u6790"}}
{"id": "2505.15277", "pdf": "https://arxiv.org/pdf/2505.15277", "abs": "https://arxiv.org/abs/2505.15277", "authors": ["Hyungjoo Chae", "Sunghwan Kim", "Junhee Cho", "Seungone Kim", "Seungjun Moon", "Gyeom Hwangbo", "Dongha Lim", "Minjin Kim", "Yeonjun Hwang", "Minju Gwak", "Dongwook Choi", "Minseok Kang", "Gwanhoon Im", "ByeongUng Cho", "Hyojun Kim", "Jun Hee Han", "Taeyoon Kwon", "Minju Kim", "Beong-woo Kwak", "Dongjin Kang", "Jinyoung Yeo"], "title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Web navigation is a unique domain that can automate many repetitive real-life\ntasks and is challenging as it requires long-horizon sequential decision making\nbeyond typical multimodal large language model (MLLM) tasks. Yet, specialized\nreward models for web navigation that can be utilized during both training and\ntest-time have been absent until now. Despite the importance of speed and\ncost-effectiveness, prior works have utilized MLLMs as reward models, which\nposes significant constraints for real-world deployment. To address this, in\nthis work, we propose the first process reward model (PRM) called Web-Shepherd\nwhich could assess web navigation trajectories in a step-level. To achieve\nthis, we first construct the WebPRM Collection, a large-scale dataset with 40K\nstep-level preference pairs and annotated checklists spanning diverse domains\nand difficulty levels. Next, we also introduce the WebRewardBench, the first\nmeta-evaluation benchmark for evaluating PRMs. In our experiments, we observe\nthat our Web-Shepherd achieves about 30 points better accuracy compared to\nusing GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by\nusing GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve\n10.9 points better performance, in 10 less cost compared to using GPT-4o-mini\nas the verifier. Our model, dataset, and code are publicly available at LINK.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u8fc7\u7a0b\u5956\u52b1\u6a21\u578bWeb-Shepherd\uff0c\u7528\u4e8e\u8bc4\u4f30\u7f51\u9875\u5bfc\u822a\u8f68\u8ff9\u7684\u6bcf\u4e00\u6b65\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6WebPRM Collection\u548c\u57fa\u51c6\u6d4b\u8bd5WebRewardBench\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "\u7f51\u9875\u5bfc\u822a\u9700\u8981\u957f\u5e8f\u5217\u51b3\u7b56\uff0c\u4f20\u7edf\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u5b58\u5728\u901f\u5ea6\u548c\u6210\u672c\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "1. \u6784\u5efaWebPRM Collection\u6570\u636e\u96c6\uff0840K\u6b65\u7ea7\u504f\u597d\u5bf9\u548c\u6807\u6ce8\u6e05\u5355\uff09\uff1b2. \u5f15\u5165WebRewardBench\u57fa\u51c6\uff1b3. \u63d0\u51faWeb-Shepherd\u6a21\u578b\u3002", "result": "Web-Shepherd\u5728WebRewardBench\u4e0a\u6bd4GPT-4o\u51c6\u786e\u7387\u9ad830%\uff1b\u5728WebArena-lite\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u63d0\u534710.9%\uff0c\u6210\u672c\u964d\u4f4e10\u500d\u3002", "conclusion": "Web-Shepherd\u4e3a\u7f51\u9875\u5bfc\u822a\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u8fc7\u7a0b\u5956\u52b1\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u9886\u57df\u53d1\u5c55\u3002", "keywords": "\u7f51\u9875\u5bfc\u822a, \u8fc7\u7a0b\u5956\u52b1\u6a21\u578b, \u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, WebPRM Collection, WebRewardBench"}}
{"id": "2505.15177", "pdf": "https://arxiv.org/pdf/2505.15177", "abs": "https://arxiv.org/abs/2505.15177", "authors": ["Jiawei Gu", "Ziyue Qiao", "Zechao Li"], "title": "SpectralGap: Graph-Level Out-of-Distribution Detection via Laplacian Eigenvalue Gaps", "categories": ["cs.LG"], "comment": "Accepted to IJCAI 20205", "summary": "The task of graph-level out-of-distribution (OOD) detection is crucial for\ndeploying graph neural networks in real-world settings. In this paper, we\nobserve a significant difference in the relationship between the largest and\nsecond-largest eigenvalues of the Laplacian matrix for in-distribution (ID) and\nOOD graph samples: \\textit{OOD samples often exhibit anomalous spectral gaps\n(the difference between the largest and second-largest eigenvalues)}. This\nobservation motivates us to propose SpecGap, an effective post-hoc approach for\nOOD detection on graphs. SpecGap adjusts features by subtracting the component\nassociated with the second-largest eigenvalue, scaled by the spectral gap, from\nthe high-level features (i.e., $\\mathbf{X}-\\left(\\lambda_n-\\lambda_{n-1}\\right)\n\\mathbf{u}_{n-1} \\mathbf{v}_{n-1}^T$). SpecGap achieves state-of-the-art\nperformance across multiple benchmark datasets. We present extensive ablation\nstudies and comprehensive theoretical analyses to support our empirical\nresults. As a parameter-free post-hoc method, SpecGap can be easily integrated\ninto existing graph neural network models without requiring any additional\ntraining or model modification.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u62c9\u666e\u62c9\u65af\u77e9\u9635\u7279\u5f81\u503c\u5dee\u5f02\u7684\u56fe\u7ea7\u5206\u5e03\u5916\uff08OOD\uff09\u68c0\u6d4b\u65b9\u6cd5SpecGap\uff0c\u901a\u8fc7\u8c03\u6574\u7279\u5f81\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u96c6\u6210\u5230\u73b0\u6709\u6a21\u578b\u4e2d\u3002", "motivation": "\u7814\u7a76\u89c2\u5bdf\u5230\u5206\u5e03\u5185\uff08ID\uff09\u4e0eOOD\u56fe\u6837\u672c\u5728\u62c9\u666e\u62c9\u65af\u77e9\u9635\u7279\u5f81\u503c\u7684\u6700\u5927\u4e0e\u6b21\u5927\u503c\u5173\u7cfb\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5c24\u5176\u662fOOD\u6837\u672c\u5e38\u8868\u73b0\u51fa\u5f02\u5e38\u7684\u8c31\u95f4\u9699\uff08\u5373\u6700\u5927\u4e0e\u6b21\u5927\u7279\u5f81\u503c\u4e4b\u5dee\uff09\u3002\u8fd9\u4e00\u89c2\u5bdf\u4e3aSpecGap\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "method": "\u63d0\u51faSpecGap\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u53bb\u4e0e\u6b21\u5927\u7279\u5f81\u503c\u76f8\u5173\u7684\u5206\u91cf\uff08\u6839\u636e\u8c31\u95f4\u9699\u7f29\u653e\uff09\u6765\u8c03\u6574\u9ad8\u5c42\u7279\u5f81\uff08\u5373\u516c\u5f0f\uff1a$\\mathbf{X}\u2212\\left(\\lambda_n\u2212\\lambda_{n-1}\\right) \\mathbf{u}_{n-1} \\mathbf{v}_{n-1}^T$\uff09\uff0c\u4ece\u800c\u5b9e\u73b0OOD\u68c0\u6d4b\u3002", "result": "SpecGap\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u6d88\u878d\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u4f5c\u4e3a\u4e00\u79cd\u65e0\u53c2\u6570\u7684\u4e8b\u540e\u65b9\u6cd5\uff0cSpecGap\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u4fee\u6539\u6a21\u578b\u5373\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u4e3aOOD\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u56fe\u795e\u7ecf\u7f51\u7edc, OOD\u68c0\u6d4b, \u8c31\u95f4\u9699, \u62c9\u666e\u62c9\u65af\u77e9\u9635"}}
{"id": "2505.15282", "pdf": "https://arxiv.org/pdf/2505.15282", "abs": "https://arxiv.org/abs/2505.15282", "authors": ["Yanzhi Tian", "Zeming Liu", "Zhengyang Liu", "Yuhang Guo"], "title": "Exploring In-Image Machine Translation with Real-World Background", "categories": ["cs.CL", "cs.CV"], "comment": "Accepted to ACL 2025 Findings. Code available at\n  https://github.com/BITHLP/DebackX", "summary": "In-Image Machine Translation (IIMT) aims to translate texts within images\nfrom one language to another. Previous research on IIMT was primarily conducted\non simplified scenarios such as images of one-line text with black font in\nwhite backgrounds, which is far from reality and impractical for applications\nin the real world. To make IIMT research practically valuable, it is essential\nto consider a complex scenario where the text backgrounds are derived from\nreal-world images. To facilitate research of complex scenario IIMT, we design\nan IIMT dataset that includes subtitle text with real-world background. However\nprevious IIMT models perform inadequately in complex scenarios. To address the\nissue, we propose the DebackX model, which separates the background and\ntext-image from the source image, performs translation on text-image directly,\nand fuses the translated text-image with the background, to generate the target\nimage. Experimental results show that our model achieves improvements in both\ntranslation quality and visual effect.", "AI": {"tldr": "In-Image Machine Translation (IIMT) focuses on translating text within images, but prior research was limited to simple scenarios. To address this, a new dataset with real-world backgrounds is created, and the DebackX model is proposed to improve performance in complex scenarios, showing better translation quality and visual results.", "motivation": "Previous IIMT research was limited to simplified scenarios (e.g., one-line text on white backgrounds), which are impractical for real-world applications. The need for handling complex scenarios with real-world backgrounds motivated this study.", "method": "The DebackX model separates background and text-image from the source, translates the text-image directly, and fuses it with the background to generate the target image.", "result": "Experimental results demonstrate that DebackX improves both translation quality and visual effects in complex scenarios.", "conclusion": "The proposed DebackX model effectively addresses the limitations of previous IIMT methods in complex scenarios, enhancing practicality and performance.", "keywords": "In-Image Machine Translation (IIMT), complex scenarios, DebackX model, real-world backgrounds, text translation"}}
{"id": "2505.15178", "pdf": "https://arxiv.org/pdf/2505.15178", "abs": "https://arxiv.org/abs/2505.15178", "authors": ["Zhehao Huang", "Xinwen Cheng", "Jie Zhang", "Jinghao Zheng", "Haoran Wang", "Zhengbao He", "Tao Li", "Xiaolin Huang"], "title": "A Unified Gradient-based Framework for Task-agnostic Continual Learning-Unlearning", "categories": ["cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2409.19732", "summary": "Recent advancements in deep models have highlighted the need for intelligent\nsystems that combine continual learning (CL) for knowledge acquisition with\nmachine unlearning (MU) for data removal, forming the Continual\nLearning-Unlearning (CLU) paradigm. While existing work treats CL and MU as\nseparate processes, we reveal their intrinsic connection through a unified\noptimization framework based on Kullback-Leibler divergence minimization. This\nframework decomposes gradient updates for approximate CLU into four components:\nlearning new knowledge, unlearning targeted data, preserving existing\nknowledge, and modulation via weight saliency. A critical challenge lies in\nbalancing knowledge update and retention during sequential learning-unlearning\ncycles. To resolve this stability-plasticity dilemma, we introduce a\nremain-preserved manifold constraint to induce a remaining Hessian compensation\nfor CLU iterations. A fast-slow weight adaptation mechanism is designed to\nefficiently approximate the second-order optimization direction, combined with\nadaptive weighting coefficients and a balanced weight saliency mask, proposing\na unified implementation framework for gradient-based CLU. Furthermore, we\npioneer task-agnostic CLU scenarios that support fine-grained unlearning at the\ncross-task category and random sample levels beyond the traditional task-aware\nsetups. Experiments demonstrate that the proposed UG-CLU framework effectively\ncoordinates incremental learning, precise unlearning, and knowledge stability\nacross multiple datasets and model architectures, providing a theoretical\nfoundation and methodological support for dynamic, compliant intelligent\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eKL\u6563\u5ea6\u6700\u5c0f\u5316\u7684\u7edf\u4e00\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u4e0e\u673a\u5668\u9057\u5fd8\uff08MU\uff09\u7ed3\u5408\u4e3aCLU\u8303\u5f0f\uff0c\u901a\u8fc7\u68af\u5ea6\u5206\u89e3\u548c\u6743\u91cd\u8c03\u5236\u5b9e\u73b0\u77e5\u8bc6\u66f4\u65b0\u4e0e\u4fdd\u7559\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06CL\u548cMU\u89c6\u4e3a\u72ec\u7acb\u8fc7\u7a0b\uff0c\u5ffd\u7565\u4e86\u5176\u5185\u5728\u8054\u7cfb\uff0c\u4e9f\u9700\u7edf\u4e00\u7684\u6846\u67b6\u652f\u6301\u77e5\u8bc6\u52a8\u6001\u66f4\u65b0\u4e0e\u5408\u89c4\u6027\u3002", "method": "\u57fa\u4e8eKL\u6563\u5ea6\u7684\u7edf\u4e00\u4f18\u5316\u6846\u67b6\uff0c\u5305\u542b\u68af\u5ea6\u66f4\u65b0\u7684\u56db\u4e2a\u7ec4\u4ef6\uff0c\u5f15\u5165\u4fdd\u7559\u6d41\u5f62\u7ea6\u675f\u548c\u5feb\u6162\u6743\u91cd\u9002\u5e94\u673a\u5236\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u9057\u5fd8\u3002", "result": "UG-CLU\u6846\u67b6\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u4e2d\u6709\u6548\u534f\u8c03\u5b66\u4e60\u3001\u9057\u5fd8\u4e0e\u7a33\u5b9a\u6027\uff0c\u4e3a\u52a8\u6001\u5408\u89c4\u7cfb\u7edf\u63d0\u4f9b\u652f\u6301\u3002", "conclusion": "CLU\u8303\u5f0f\u901a\u8fc7\u7edf\u4e00\u4f18\u5316\u5e73\u8861\u5b66\u4e60\u4e0e\u9057\u5fd8\uff0c\u4e3a\u667a\u80fd\u7cfb\u7edf\u7684\u52a8\u6001\u5408\u89c4\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u57fa\u7840\u3002", "keywords": "\u6301\u7eed\u5b66\u4e60\uff1b\u673a\u5668\u9057\u5fd8\uff1bKL\u6563\u5ea6\uff1b\u4f18\u5316\u6846\u67b6\uff1b\u52a8\u6001\u5408\u89c4\u6027"}}
{"id": "2505.14948", "pdf": "https://arxiv.org/pdf/2505.14948", "abs": "https://arxiv.org/abs/2505.14948", "authors": ["Hao Tang", "Kevin Ellis", "Suhas Lohit", "Michael J. Jones", "Moitreya Chatterjee"], "title": "Programmatic Video Prediction Using Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The task of estimating the world model describing the dynamics of a real\nworld process assumes immense importance for anticipating and preparing for\nfuture outcomes. For applications such as video surveillance, robotics\napplications, autonomous driving, etc. this objective entails synthesizing\nplausible visual futures, given a few frames of a video to set the visual\ncontext. Towards this end, we propose ProgGen, which undertakes the task of\nvideo frame prediction by representing the dynamics of the video using a set of\nneuro-symbolic, human-interpretable set of states (one per frame) by leveraging\nthe inductive biases of Large (Vision) Language Models (LLM/VLM). In\nparticular, ProgGen utilizes LLM/VLM to synthesize programs: (i) to estimate\nthe states of the video, given the visual context (i.e. the frames); (ii) to\npredict the states corresponding to future time steps by estimating the\ntransition dynamics; (iii) to render the predicted states as visual RGB-frames.\nEmpirical evaluations reveal that our proposed method outperforms competing\ntechniques at the task of video frame prediction in two challenging\nenvironments: (i) PhyWorld (ii) Cart Pole. Additionally, ProgGen permits\ncounter-factual reasoning and interpretable video generation attesting to its\neffectiveness and generalizability for video generation tasks.", "AI": {"tldr": "ProgGen\u5229\u7528\u795e\u7ecf\u7b26\u53f7\u5316\u65b9\u6cd5\u9884\u6d4b\u89c6\u9891\u5e27\uff0c\u901a\u8fc7LLM/VLM\u751f\u6210\u53ef\u89e3\u91ca\u7a0b\u5e8f\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u4e3a\u89c6\u9891\u76d1\u63a7\u3001\u673a\u5668\u4eba\u5e94\u7528\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u573a\u666f\uff0c\u9884\u6d4b\u672a\u6765\u89c6\u89c9\u52a8\u6001\u81f3\u5173\u91cd\u8981\uff0c\u9700\u4ece\u5c11\u91cf\u5e27\u751f\u6210\u5408\u7406\u672a\u6765\u5e27\u3002", "method": "ProgGen\u7528LLM/VLM\u751f\u6210\u7a0b\u5e8f\uff1a(1)\u4f30\u8ba1\u5f53\u524d\u89c6\u9891\u72b6\u6001\uff0c(2)\u9884\u6d4b\u672a\u6765\u72b6\u6001\u52a8\u6001\uff0c(3)\u6e32\u67d3\u4e3aRGB\u5e27\u3002", "result": "\u5728PhyWorld\u548cCart Pole\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u7ade\u54c1\uff0c\u652f\u6301\u53cd\u4e8b\u5b9e\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u89c6\u9891\u751f\u6210\u3002", "conclusion": "ProgGen\u5728\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u9ad8\u6548\u901a\u7528\uff0c\u517c\u5177\u53ef\u89e3\u91ca\u6027\u3002", "keywords": "\u89c6\u9891\u9884\u6d4b,\u795e\u7ecf\u7b26\u53f7\u5316,LLM/VLM,\u53ef\u89e3\u91ca\u6027"}}
{"id": "2505.15291", "pdf": "https://arxiv.org/pdf/2505.15291", "abs": "https://arxiv.org/abs/2505.15291", "authors": ["Joonho Yang", "Seunghyun Yoon", "Hwan Chang", "Byeongjeong Kim", "Hwanhee Lee"], "title": "Hallucinate at the Last in Long Response Generation: A Case Study on Long Document Summarization", "categories": ["cs.CL"], "comment": "11 tables, 8 figures", "summary": "Large Language Models (LLMs) have significantly advanced text generation\ncapabilities, including tasks like summarization, often producing coherent and\nfluent outputs. However, faithfulness to source material remains a significant\nchallenge due to the generation of hallucinations. While extensive research\nfocuses on detecting and reducing these inaccuracies, less attention has been\npaid to the positional distribution of hallucination within generated text,\nparticularly in long outputs. In this work, we investigate where hallucinations\noccur in LLM-based long response generation, using long document summarization\nas a key case study. Focusing on the challenging setting of long context-aware\nlong response generation, we find a consistent and concerning phenomenon:\nhallucinations tend to concentrate disproportionately in the latter parts of\nthe generated long response. To understand this bias, we explore potential\ncontributing factors related to the dynamics of attention and decoding over\nlong sequences. Furthermore, we investigate methods to mitigate this positional\nhallucination, aiming to improve faithfulness specifically in the concluding\nsegments of long outputs.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u957f\u6587\u672c\u65f6\uff0c\u5e7b\u89c9\u73b0\u8c61\u5728\u540e\u6bb5\u66f4\u96c6\u4e2d\u7684\u95ee\u9898\u53ca\u5176\u89e3\u51b3\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u6d41\u7545\uff0c\u4f46\u5e7b\u89c9\u95ee\u9898\u662f\u4e3b\u8981\u6311\u6218\uff0c\u5c24\u5176\u662f\u957f\u6587\u672c\u540e\u7aef\u5e7b\u89c9\u7684\u5206\u5e03\u5c1a\u672a\u6df1\u5165\u7814\u7a76\u3002", "method": "\u4ee5\u957f\u6587\u6863\u6458\u8981\u4e3a\u4f8b\uff0c\u5206\u6790\u4e86\u5e7b\u89c9\u7684\u4f4d\u7f6e\u5206\u5e03\uff0c\u63a2\u7d22\u4e86\u6ce8\u610f\u529b\u548c\u89e3\u7801\u52a8\u6001\u7684\u5f71\u54cd\uff0c\u5e76\u7814\u7a76\u4e86\u7f13\u89e3\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u5e7b\u89c9\u5728\u540e\u6bb5\u66f4\u96c6\u4e2d\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u6027\u6539\u5584\u65b9\u6cd5\u3002", "conclusion": "\u5e7b\u89c9\u5728\u957f\u6587\u672c\u540e\u7aef\u66f4\u5e38\u89c1\uff0c\u9700\u9488\u5bf9\u6027\u4f18\u5316\u4ee5\u63d0\u5347\u751f\u6210\u5fe0\u5b9e\u5ea6\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5e7b\u89c9, \u957f\u6587\u672c\u751f\u6210, \u4f4d\u7f6e\u5206\u5e03, \u6458\u8981"}}
{"id": "2505.15180", "pdf": "https://arxiv.org/pdf/2505.15180", "abs": "https://arxiv.org/abs/2505.15180", "authors": ["Jiawei Gu", "Ziyue Qiao", "Xiao Luo"], "title": "NeuBM: Mitigating Model Bias in Graph Neural Networks through Neutral Input Calibration", "categories": ["cs.LG"], "comment": "Accepted to IJCAI 20205", "summary": "Graph Neural Networks (GNNs) have shown remarkable performance across various\ndomains, yet they often struggle with model bias, particularly in the presence\nof class imbalance. This bias can lead to suboptimal performance and unfair\npredictions, especially for underrepresented classes. We introduce NeuBM\n(Neutral Bias Mitigation), a novel approach to mitigate model bias in GNNs\nthrough neutral input calibration. NeuBM leverages a dynamically updated\nneutral graph to estimate and correct the inherent biases of the model. By\nsubtracting the logits obtained from the neutral graph from those of the input\ngraph, NeuBM effectively recalibrates the model's predictions, reducing bias\nacross different classes. Our method integrates seamlessly into existing GNN\narchitectures and training procedures, requiring minimal computational\noverhead. Extensive experiments on multiple benchmark datasets demonstrate that\nNeuBM significantly improves the balanced accuracy and recall of minority\nclasses, while maintaining strong overall performance. The effectiveness of\nNeuBM is particularly pronounced in scenarios with severe class imbalance and\nlimited labeled data, where traditional methods often struggle. We provide\ntheoretical insights into how NeuBM achieves bias mitigation, relating it to\nthe concept of representation balancing. Our analysis reveals that NeuBM not\nonly adjusts the final predictions but also influences the learning of balanced\nfeature representations throughout the network.", "AI": {"tldr": "NeuBM \u662f\u4e00\u79cd\u901a\u8fc7\u4e2d\u6027\u8f93\u5165\u6821\u51c6\u51cf\u5c11\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u6a21\u578b\u504f\u5dee\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5c11\u6570\u7c7b\u7684\u5e73\u8861\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u7c7b\u4e0d\u5e73\u8861\u65f6\u5b58\u5728\u6a21\u578b\u504f\u5dee\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u4e0d\u516c\u5e73\u9884\u6d4b\uff0c\u5c24\u5176\u5bf9\u5c11\u6570\u7c7b\u4e0d\u5229\u3002", "method": "NeuBM \u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u7684\u4e2d\u6027\u56fe\u4f30\u8ba1\u5e76\u7ea0\u6b63\u6a21\u578b\u5185\u5728\u504f\u5dee\uff0c\u901a\u8fc7\u51cf\u53bb\u4e2d\u6027\u56fe\u7684 logits \u6765\u91cd\u65b0\u6821\u51c6\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a NeuBM \u663e\u8457\u63d0\u5347\u5c11\u6570\u7c7b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u7c7b\u4e0d\u5e73\u8861\u4e25\u91cd\u548c\u6807\u7b7e\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "NeuBM \u4e0d\u4ec5\u8c03\u6574\u6700\u7ec8\u9884\u6d4b\uff0c\u8fd8\u901a\u8fc7\u8868\u793a\u5e73\u8861\u4fc3\u8fdb\u7f51\u7edc\u5b66\u4e60\u5747\u8861\u7279\u5f81\u8868\u793a\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6613\u96c6\u6210\u7684\u65b9\u6cd5\u3002", "keywords": "\u56fe\u795e\u7ecf\u7f51\u7edc, \u6a21\u578b\u504f\u5dee, \u7c7b\u4e0d\u5e73\u8861, \u4e2d\u6027\u8f93\u5165\u6821\u51c6, \u8868\u793a\u5e73\u8861"}}
{"id": "2505.15297", "pdf": "https://arxiv.org/pdf/2505.15297", "abs": "https://arxiv.org/abs/2505.15297", "authors": ["Xintong Wang", "Yixiao Liu", "Jingheng Pan", "Liang Ding", "Longyue Wang", "Chris Biemann"], "title": "Chinese Toxic Language Mitigation via Sentiment Polarity Consistent Rewrites", "categories": ["cs.CL"], "comment": "14 pages, 7 figures", "summary": "Detoxifying offensive language while preserving the speaker's original intent\nis a challenging yet critical goal for improving the quality of online\ninteractions. Although large language models (LLMs) show promise in rewriting\ntoxic content, they often default to overly polite rewrites, distorting the\nemotional tone and communicative intent. This problem is especially acute in\nChinese, where toxicity often arises implicitly through emojis, homophones, or\ndiscourse context. We present ToxiRewriteCN, the first Chinese detoxification\ndataset explicitly designed to preserve sentiment polarity. The dataset\ncomprises 1,556 carefully annotated triplets, each containing a toxic sentence,\na sentiment-aligned non-toxic rewrite, and labeled toxic spans. It covers five\nreal-world scenarios: standard expressions, emoji-induced and homophonic\ntoxicity, as well as single-turn and multi-turn dialogues. We evaluate 17 LLMs,\nincluding commercial and open-source models with variant architectures, across\nfour dimensions: detoxification accuracy, fluency, content preservation, and\nsentiment polarity. Results show that while commercial and MoE models perform\nbest overall, all models struggle to balance safety with emotional fidelity in\nmore subtle or context-heavy settings such as emoji, homophone, and\ndialogue-based inputs. We release ToxiRewriteCN to support future research on\ncontrollable, sentiment-aware detoxification for Chinese.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u4e2d\u6587\u7684\u60c5\u611f\u5bf9\u9f50\u53bb\u6bd2\u6570\u636e\u96c6ToxiRewriteCN\uff0c\u8bc4\u4f30\u4e8617\u79cdLLM\u6a21\u578b\u5728\u53bb\u6bd2\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u9690\u6666\u6216\u4e0a\u4e0b\u6587\u5bc6\u96c6\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5728\u7ebf\u4ea4\u6d41\u4e2d\uff0c\u53bb\u6bd2\u5316\u5185\u5bb9\u540c\u65f6\u4fdd\u7559\u539f\u610f\u548c\u60c5\u611f\u662f\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5c24\u5176\u5728\u4e2d\u6587\u4e2d\u901a\u8fc7\u8868\u60c5\u3001\u8c10\u97f3\u7b49\u65b9\u5f0f\u9690\u6666\u8868\u8fbe\u7684\u6bd2\u6027\u66f4\u96be\u5904\u7406\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b1,556\u6761\u6807\u6ce8\u4e09\u5143\u7ec4\u7684\u4e2d\u6587\u53bb\u6bd2\u6570\u636e\u96c6ToxiRewriteCN\uff0c\u8986\u76d6\u4e94\u79cd\u5b9e\u9645\u573a\u666f\uff0c\u8bc4\u4f30\u4e8617\u79cdLLM\u6a21\u578b\u7684\u591a\u7ef4\u5ea6\u8868\u73b0\u3002", "result": "\u5546\u4e1a\u548cMoE\u6a21\u578b\u6574\u4f53\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u9690\u6666\u6216\u4e0a\u4e0b\u6587\u5bc6\u96c6\u7684\u573a\u666f\u4e2d\u96be\u4ee5\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u60c5\u611f\u4fdd\u771f\u5ea6\u3002", "conclusion": "ToxiRewriteCN\u4e3a\u4e2d\u6587\u60c5\u611f\u611f\u77e5\u53bb\u6bd2\u7814\u7a76\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "keywords": "\u4e2d\u6587\u53bb\u6bd2,\u60c5\u611f\u5bf9\u9f50,ToxiRewriteCN,\u5927\u8bed\u8a00\u6a21\u578b,\u9690\u6666\u6bd2\u6027"}}
{"id": "2505.15195", "pdf": "https://arxiv.org/pdf/2505.15195", "abs": "https://arxiv.org/abs/2505.15195", "authors": ["Adel Javanmard", "Rudrajit Das", "Alessandro Epasto", "Vahab Mirrokni"], "title": "Self-Boost via Optimal Retraining: An Analysis via Approximate Message Passing", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": "31 pages, 6 figures, 5 tables", "summary": "Retraining a model using its own predictions together with the original,\npotentially noisy labels is a well-known strategy for improving the model\nperformance. While prior works have demonstrated the benefits of specific\nheuristic retraining schemes, the question of how to optimally combine the\nmodel's predictions and the provided labels remains largely open. This paper\naddresses this fundamental question for binary classification tasks. We develop\na principled framework based on approximate message passing (AMP) to analyze\niterative retraining procedures for two ground truth settings: Gaussian mixture\nmodel (GMM) and generalized linear model (GLM). Our main contribution is the\nderivation of the Bayes optimal aggregator function to combine the current\nmodel's predictions and the given labels, which when used to retrain the same\nmodel, minimizes its prediction error. We also quantify the performance of this\noptimal retraining strategy over multiple rounds. We complement our theoretical\nresults by proposing a practically usable version of the theoretically-optimal\naggregator function for linear probing with the cross-entropy loss, and\ndemonstrate its superiority over baseline methods in the high label noise\nregime.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8fd1\u4f3c\u6d88\u606f\u4f20\u9012\uff08AMP\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fed\u4ee3\u91cd\u8bad\u7ec3\u7684\u8d1d\u53f6\u65af\u6700\u4f18\u805a\u5408\u51fd\u6570\uff0c\u4ee5\u6700\u5c0f\u5316\u9884\u6d4b\u8bef\u5dee\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u91cd\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6700\u4f18\u5730\u7ed3\u5408\u6a21\u578b\u7684\u9884\u6d4b\u548c\u539f\u59cb\u6807\u7b7e\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u8fd1\u4f3c\u6d88\u606f\u4f20\u9012\uff08AMP\uff09\u6846\u67b6\uff0c\u5206\u6790\u4e86\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u548c\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\uff08GLM\uff09\u7684\u8fed\u4ee3\u91cd\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u63a8\u5bfc\u51fa\u8d1d\u53f6\u65af\u6700\u4f18\u805a\u5408\u51fd\u6570\uff0c\u5728\u9ad8\u6807\u7b7e\u566a\u58f0\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u7684\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6700\u4f18\u805a\u5408\u51fd\u6570\u80fd\u591f\u6709\u6548\u51cf\u5c11\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "keywords": "\u91cd\u8bad\u7ec3, \u8fd1\u4f3c\u6d88\u606f\u4f20\u9012, \u8d1d\u53f6\u65af\u6700\u4f18, \u4e8c\u5143\u5206\u7c7b"}}
{"id": "2505.15316", "pdf": "https://arxiv.org/pdf/2505.15316", "abs": "https://arxiv.org/abs/2505.15316", "authors": ["Xin Bai", "Guanyi Chen", "Tingting He", "Chenlian Zhou", "Yu Liu"], "title": "Emotional Supporters often Use Multiple Strategies in a Single Turn", "categories": ["cs.CL"], "comment": null, "summary": "Emotional Support Conversations (ESC) are crucial for providing empathy,\nvalidation, and actionable guidance to individuals in distress. However,\nexisting definitions of the ESC task oversimplify the structure of supportive\nresponses, typically modelling them as single strategy-utterance pairs. Through\na detailed corpus analysis of the ESConv dataset, we identify a common yet\npreviously overlooked phenomenon: emotional supporters often employ multiple\nstrategies consecutively within a single turn. We formally redefine the ESC\ntask to account for this, proposing a revised formulation that requires\ngenerating the full sequence of strategy-utterance pairs given a dialogue\nhistory. To facilitate this refined task, we introduce several modelling\napproaches, including supervised deep learning models and large language\nmodels. Our experiments show that, under this redefined task, state-of-the-art\nLLMs outperform both supervised models and human supporters. Notably, contrary\nto some earlier findings, we observe that LLMs frequently ask questions and\nprovide suggestions, demonstrating more holistic support capabilities.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5b9a\u4e49\u4e86\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\uff08ESC\uff09\u4efb\u52a1\uff0c\u63d0\u51fa\u5c06\u652f\u6301\u6027\u56de\u5e94\u89c6\u4e3a\u591a\u7b56\u7565\u8fde\u7eed\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6539\u8fdb\u4efb\u52a1\u4e2d\u4f18\u4e8e\u76d1\u7763\u6a21\u578b\u548c\u4eba\u7c7b\u652f\u6301\u8005\u3002", "motivation": "\u73b0\u6709\u7684ESC\u4efb\u52a1\u5b9a\u4e49\u8fc7\u4e8e\u7b80\u5316\u652f\u6301\u56de\u5e94\u7ed3\u6784\uff0c\u5ffd\u89c6\u4e86\u60c5\u611f\u652f\u6301\u8005\u5e38\u5728\u540c\u4e00\u8f6e\u5bf9\u8bdd\u4e2d\u8fde\u7eed\u4f7f\u7528\u591a\u7b56\u7565\u7684\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u8bed\u6599\u5e93\u5206\u6790\u91cd\u65b0\u5b9a\u4e49ESC\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548cLLM\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6539\u8fdb\u4efb\u52a1\u4e2d\uff0cLLM\u8868\u73b0\u4f18\u4e8e\u76d1\u7763\u6a21\u578b\u548c\u4eba\u7c7b\u652f\u6301\u8005\uff0c\u4e14\u5c55\u73b0\u66f4\u5168\u9762\u7684\u652f\u6301\u80fd\u529b\u3002", "conclusion": "\u91cd\u65b0\u5b9a\u4e49ESC\u4efb\u52a1\u80fd\u66f4\u771f\u5b9e\u53cd\u6620\u5b9e\u9645\u652f\u6301\u884c\u4e3a\uff0cLLM\u5728\u8be5\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\u3002", "keywords": "\u60c5\u611f\u652f\u6301\u5bf9\u8bdd, \u591a\u7b56\u7565\u8fde\u7eed\u5e8f\u5217, \u8bed\u6599\u5e93\u5206\u6790, \u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.15212", "pdf": "https://arxiv.org/pdf/2505.15212", "abs": "https://arxiv.org/abs/2505.15212", "authors": ["Haomin Bai", "Dingzhi Yu", "Shuai Li", "Haipeng Luo", "Lijun Zhang"], "title": "Group Distributionally Robust Optimization with Flexible Sample Queries", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Group distributionally robust optimization (GDRO) aims to develop models that\nperform well across $m$ distributions simultaneously. Existing GDRO algorithms\ncan only process a fixed number of samples per iteration, either 1 or $m$, and\ntherefore can not support scenarios where the sample size varies dynamically.\nTo address this limitation, we investigate GDRO with flexible sample queries\nand cast it as a two-player game: one player solves an online convex\noptimization problem, while the other tackles a prediction with limited advice\n(PLA) problem. Within such a game, we propose a novel PLA algorithm,\nconstructing appropriate loss estimators for cases where the sample size is\neither 1 or not, and updating the decision using follow-the-regularized-leader.\nThen, we establish the first high-probability regret bound for non-oblivious\nPLA. Building upon the above approach, we develop a GDRO algorithm that allows\nan arbitrary and varying sample size per round, achieving a high-probability\noptimization error bound of $O\\left(\\frac{1}{t}\\sqrt{\\sum_{j=1}^t\n\\frac{m}{r_j}\\log m}\\right)$, where $r_t$ denotes the sample size at round $t$.\nThis result demonstrates that the optimization error decreases as the number of\nsamples increases and implies a consistent sample complexity of $O(m\\log\n(m)/\\epsilon^2)$ for any fixed sample size $r\\in[m]$, aligning with existing\nbounds for cases of $r=1$ or $m$. We validate our approach on synthetic binary\nand real-world multi-class datasets.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u652f\u6301\u52a8\u6001\u6837\u672c\u91cf\u7684\u7ec4\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff08GDRO\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9884\u6d4b\u6709\u9650\u5efa\u8bae\uff08PLA\uff09\u7b97\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u5141\u8bb8\u6bcf\u8f6e\u6837\u672c\u91cf\u4efb\u610f\u53d8\u5316\u7684GDRO\u7b97\u6cd5\uff0c\u53d6\u5f97\u4e86\u4e0e\u56fa\u5b9a\u6837\u672c\u91cf\u60c5\u51b5\u4e00\u81f4\u7684\u7406\u8bba\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GDRO\u7b97\u6cd5\u53ea\u80fd\u5904\u7406\u56fa\u5b9a\u6837\u672c\u91cf\uff081\u6216m\uff09\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u6837\u672c\u91cf\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06GDRO\u95ee\u9898\u5efa\u6a21\u4e3a\u4e24\u4eba\u6e38\u620f\uff0c\u63d0\u51fa\u65b0\u7684PLA\u7b97\u6cd5\u5e76\u6784\u5efa\u635f\u5931\u4f30\u8ba1\u5668\uff0c\u91c7\u7528\u6b63\u5219\u5316\u8ddf\u968f\u66f4\u65b0\u7b56\u7565\uff0c\u5f00\u53d1\u4e86\u652f\u6301\u4efb\u610f\u6837\u672c\u91cf\u7684GDRO\u7b97\u6cd5\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u4f18\u5316\u8bef\u5dee\u968f\u6837\u672c\u91cf\u589e\u52a0\u800c\u4e0b\u964d\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e0e\u56fa\u5b9a\u6837\u672c\u91cf\u60c5\u51b5\u4e00\u81f4\uff1b\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7b97\u6cd5\u89e3\u51b3\u4e86GDRO\u4e2d\u6837\u672c\u91cf\u52a8\u6001\u53d8\u5316\u7684\u9650\u5236\uff0c\u6269\u5c55\u4e86\u5176\u5e94\u7528\u573a\u666f\uff0c\u5e76\u4fdd\u6301\u4e86\u7406\u8bba\u6027\u80fd\u3002", "keywords": "GDRO, PLA, \u52a8\u6001\u6837\u672c\u91cf, \u4f18\u5316\u8bef\u5dee, \u6837\u672c\u590d\u6742\u5ea6"}}
{"id": "2505.15323", "pdf": "https://arxiv.org/pdf/2505.15323", "abs": "https://arxiv.org/abs/2505.15323", "authors": ["Silvia Cappelletti", "Tobia Poppi", "Samuele Poppi", "Zheng-Xin Yong", "Diego Garcia-Olano", "Marcella Cornia", "Lorenzo Baraldi", "Rita Cucchiara"], "title": "Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack", "categories": ["cs.CL"], "comment": "13 pages, 5 figures, 7 tables", "summary": "Large Language Models (LLMs) are increasingly evaluated on multiple-choice\nquestion answering (MCQA) tasks using *first-token probability* (FTP), which\nselects the answer option whose initial token has the highest likelihood. While\nefficient, FTP can be fragile: models may assign high probability to unrelated\ntokens (*misalignment*) or use a valid token merely as part of a generic\npreamble rather than as a clear answer choice (*misinterpretation*),\nundermining the reliability of symbolic evaluation. We propose a simple\nsolution: the *prefilling attack*, a structured natural-language prefix (e.g.,\n\"*The correct option is:*\") prepended to the model output. Originally explored\nin AI safety, we repurpose prefilling to steer the model to respond with a\nclean, valid option, without modifying its parameters. Empirically, the FTP\nwith prefilling strategy substantially improves accuracy, calibration, and\noutput consistency across a broad set of LLMs and MCQA benchmarks. It\noutperforms standard FTP and often matches the performance of open-ended\ngeneration approaches that require full decoding and external classifiers,\nwhile being significantly more efficient. Our findings suggest that prefilling\nis a simple, robust, and low-cost method to enhance the reliability of\nFTP-based evaluation in multiple-choice settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a*prefilling attack*\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6a21\u578b\u8f93\u51fa\u524d\u6dfb\u52a0\u7ed3\u6784\u5316\u524d\u7f00\uff08\u5982'*The correct option is:*'\uff09\uff0c\u6709\u6548\u6539\u5584\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u9879\u9009\u62e9\u9898\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u7684*first-token probability* (FTP)\u65b9\u6cd5\u5728\u591a\u9879\u9009\u62e9\u9898\u4efb\u52a1\u4e2d\u5b58\u5728\u8106\u5f31\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u65e0\u5173\u4ee4\u724c\u7684\u9ad8\u6982\u7387\u6216\u8bef\u89e3\uff0c\u5f71\u54cd\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa*prefilling attack*\uff0c\u901a\u8fc7\u6dfb\u52a0\u81ea\u7136\u8bed\u8a00\u524d\u7f00\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u6e05\u6670\u7684\u6709\u6548\u9009\u9879\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u3001\u6821\u51c6\u6027\u548c\u8f93\u51fa\u4e00\u81f4\u6027\uff0c\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u4e0e\u5f00\u653e\u5f0f\u751f\u6210\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "prefilling\u662f\u4e00\u79cd\u7b80\u5355\u3001\u9c81\u68d2\u4e14\u4f4e\u6210\u672c\u7684\u65b9\u6cd5\uff0c\u53ef\u63d0\u5347FTP\u5728\u591a\u9879\u9009\u62e9\u9898\u8bc4\u4f30\u4e2d\u7684\u53ef\u9760\u6027\u3002", "keywords": "Large Language Models, multiple-choice question answering, first-token probability, prefilling attack, AI safety"}}
{"id": "2505.15213", "pdf": "https://arxiv.org/pdf/2505.15213", "abs": "https://arxiv.org/abs/2505.15213", "authors": ["Sampanna Yashwant Kahu"], "title": "KernelOracle: Predicting the Linux Scheduler's Next Move with Deep Learning", "categories": ["cs.LG", "cs.OS", "D.4.1; I.2.0; I.2.1"], "comment": "7 pages, 11 figures, pre-print. The source code and data used in this\n  work is available at: https://github.com/SampannaKahu/KernelOracle", "summary": "Efficient task scheduling is paramount in the Linux kernel, where the\nCompletely Fair Scheduler (CFS) meticulously manages CPU resources to balance\nhigh utilization with interactive responsiveness. This research pioneers the\nuse of deep learning techniques to predict the sequence of tasks selected by\nCFS, aiming to evaluate the feasibility of a more generalized and potentially\nmore adaptive task scheduler for diverse workloads. Our core contributions are\ntwofold: first, the systematic generation and curation of a novel scheduling\ndataset from a running Linux kernel, capturing real-world CFS behavior; and\nsecond, the development, training, and evaluation of a Long Short-Term Memory\n(LSTM) network designed to accurately forecast the next task to be scheduled.\nThis paper further discusses the practical pathways and implications of\nintegrating such a predictive model into the kernel's scheduling framework. The\nfindings and methodologies presented herein open avenues for data-driven\nadvancements in kernel scheduling, with the full source code provided for\nreproducibility and further exploration.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4bLinux\u5185\u6838\u7684CFS\u8c03\u5ea6\u4efb\u52a1\u5e8f\u5217\uff0c\u63a2\u7d22\u6784\u5efa\u66f4\u6cdb\u5316\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u8c03\u5ea6\u5668\uff0c\u8d21\u732e\u5305\u62ec\u65b0\u8c03\u5ea6\u6570\u636e\u96c6\u751f\u6210\u548cLSTM\u7f51\u7edc\u5f00\u53d1\u3002", "motivation": "\u4f18\u5316Linux\u5185\u6838\u4efb\u52a1\u8c03\u5ea6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u63d0\u9ad8CFS\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u751f\u6210\u771f\u5b9e\u8c03\u5ea6\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u5e76\u8bad\u7ec3LSTM\u7f51\u7edc\u9884\u6d4b\u4efb\u52a1\u5e8f\u5217\u3002", "result": "\u6210\u529f\u9884\u6d4bCFS\u4efb\u52a1\u8c03\u5ea6\u5e8f\u5217\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u5185\u6838\u8c03\u5ea6\u63d0\u4f9b\u53ef\u884c\u8def\u5f84\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u53ef\u63d0\u5347\u5185\u6838\u8c03\u5ea6\u6548\u7387\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6574\u5408\u9884\u6d4b\u6a21\u578b\u3002", "keywords": "Linux\u5185\u6838, CFS, \u6df1\u5ea6\u5b66\u4e60, LSTM, \u4efb\u52a1\u8c03\u5ea6"}}
{"id": "2505.14976", "pdf": "https://arxiv.org/pdf/2505.14976", "abs": "https://arxiv.org/abs/2505.14976", "authors": ["Roozbeh Aghili", "Xingfang Wu", "Foutse Khomh", "Heng Li"], "title": "SDLog: A Deep Learning Framework for Detecting Sensitive Information in Software Logs", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Software logs are messages recorded during the execution of a software system\nthat provide crucial run-time information about events and activities. Although\nsoftware logs have a critical role in software maintenance and operation tasks,\npublicly accessible log datasets remain limited, hindering advance in log\nanalysis research and practices. The presence of sensitive information,\nparticularly Personally Identifiable Information (PII) and quasi-identifiers,\nintroduces serious privacy and re-identification risks, discouraging the\npublishing and sharing of real-world logs. In practice, log anonymization\ntechniques primarily rely on regular expression patterns, which involve\nmanually crafting rules to identify and replace sensitive information. However,\nthese regex-based approaches suffer from significant limitations, such as\nextensive manual efforts and poor generalizability across diverse log formats\nand datasets. To mitigate these limitations, we introduce SDLog, a deep\nlearning-based framework designed to identify sensitive information in software\nlogs. Our results show that SDLog overcomes regex limitations and outperforms\nthe best-performing regex patterns in identifying sensitive information. With\nonly 100 fine-tuning samples from the target dataset, SDLog can correctly\nidentify 99.5% of sensitive attributes and achieves an F1-score of 98.4%. To\nthe best of our knowledge, this is the first deep learning alternative to\nregex-based methods in software log anonymization.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6SDLog\uff0c\u7528\u4e8e\u8bc6\u522b\u8f6f\u4ef6\u65e5\u5fd7\u4e2d\u7684\u654f\u611f\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6b63\u5219\u8868\u8fbe\u5f0f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u8f6f\u4ef6\u65e5\u5fd7\u4e2d\u5305\u542b\u654f\u611f\u4fe1\u606f\uff0c\u963b\u788d\u4e86\u65e5\u5fd7\u6570\u636e\u96c6\u7684\u516c\u5f00\u5171\u4eab\u548c\u65e5\u5fd7\u5206\u6790\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51faSDLog\u6846\u67b6\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u8bc6\u522b\u65e5\u5fd7\u4e2d\u7684\u654f\u611f\u4fe1\u606f\u3002", "result": "SDLog\u5728\u4ec5\u9700100\u4e2a\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u6b63\u786e\u8bc6\u522b\u7387\u4e3a99.5%\uff0cF1\u5f97\u5206\u4e3a98.4%\u3002", "conclusion": "SDLog\u662f\u9996\u4e2a\u6df1\u5ea6\u5b66\u4e60\u66ff\u4ee3\u6b63\u5219\u8868\u8fbe\u5f0f\u7684\u65e5\u5fd7\u533f\u540d\u5316\u65b9\u6cd5\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "keywords": "\u8f6f\u4ef6\u65e5\u5fd7,\u654f\u611f\u4fe1\u606f,\u6df1\u5ea6\u5b66\u4e60,\u533f\u540d\u5316"}}
{"id": "2505.15333", "pdf": "https://arxiv.org/pdf/2505.15333", "abs": "https://arxiv.org/abs/2505.15333", "authors": ["Yuhao Zhang", "Xiangnan Ma", "Kaiqi Kou", "Peizhuo Liu", "Weiqiao Shan", "Benyou Wang", "Tong Xiao", "Yuxin Huang", "Zhengtao Yu", "Jingbo Zhu"], "title": "Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted to ACL 2025 Findings", "summary": "The success of building textless speech-to-speech translation (S2ST) models\nhas attracted much attention. However, S2ST still faces two main challenges: 1)\nextracting linguistic features for various speech signals, called cross-modal\n(CM), and 2) learning alignment of difference languages in long sequences,\ncalled cross-lingual (CL). We propose the unit language to overcome the two\nmodeling challenges. The unit language can be considered a text-like\nrepresentation format, constructed using $n$-gram language modeling. We\nimplement multi-task learning to utilize the unit language in guiding the\nspeech modeling process. Our initial results reveal a conflict when applying\nsource and target unit languages simultaneously. We propose task prompt\nmodeling to mitigate this conflict. We conduct experiments on four languages of\nthe Voxpupil dataset. Our method demonstrates significant improvements over a\nstrong baseline and achieves performance comparable to models trained with\ntext.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u5143\u8bed\u8a00\u7684\u6587\u672c\u65e0\u5173\u8bed\u97f3\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u548c\u8de8\u8bed\u8a00\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u4efb\u52a1\u63d0\u793a\u5efa\u6a21\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u8de8\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u548c\u8de8\u8bed\u8a00\u5bf9\u9f50\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5355\u5143\u8bed\u8a00\u4f5c\u4e3a\u6587\u672c\u7c7b\u4f3c\u8868\u793a\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u4efb\u52a1\u63d0\u793a\u5efa\u6a21\uff0c\u6307\u5bfc\u8bed\u97f3\u5efa\u6a21\u8fc7\u7a0b\u3002", "result": "\u5728Voxpupil\u6570\u636e\u96c6\u7684\u56db\u79cd\u8bed\u8a00\u4e0a\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u63a5\u8fd1\u6709\u6587\u672c\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "\u5355\u5143\u8bed\u8a00\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u7ed3\u5408\u662f\u89e3\u51b3\u8bed\u97f3\u7ffb\u8bd1\u6311\u6218\u7684\u6709\u6548\u65b9\u6cd5\u3002", "keywords": "\u8bed\u97f3\u7ffb\u8bd1\u3001\u5355\u5143\u8bed\u8a00\u3001\u591a\u4efb\u52a1\u5b66\u4e60\u3001\u8de8\u6a21\u6001\u3001\u8de8\u8bed\u8a00"}}
{"id": "2505.15228", "pdf": "https://arxiv.org/pdf/2505.15228", "abs": "https://arxiv.org/abs/2505.15228", "authors": ["Mathew Vanherreweghe", "Lirand\u00eb Pira", "Patrick Rebentrost"], "title": "Degree-Optimized Cumulative Polynomial Kolmogorov-Arnold Networks", "categories": ["cs.LG", "cs.CE", "cs.NE"], "comment": null, "summary": "We introduce cumulative polynomial Kolmogorov-Arnold networks (CP-KAN), a\nneural architecture combining Chebyshev polynomial basis functions and\nquadratic unconstrained binary optimization (QUBO). Our primary contribution\ninvolves reformulating the degree selection problem as a QUBO task, reducing\nthe complexity from $O(D^N)$ to a single optimization step per layer. This\napproach enables efficient degree selection across neurons while maintaining\ncomputational tractability. The architecture performs well in regression tasks\nwith limited data, showing good robustness to input scales and natural\nregularization properties from its polynomial basis. Additionally, theoretical\nanalysis establishes connections between CP-KAN's performance and properties of\nfinancial time series. Our empirical validation across multiple domains\ndemonstrates competitive performance compared to several traditional\narchitectures tested, especially in scenarios where data efficiency and\nnumerical stability are important. Our implementation, including strategies for\nmanaging computational overhead in larger networks is available in\nRef.~\\citep{cpkan_implementation}.", "AI": {"tldr": "CP-KAN\u662f\u4e00\u79cd\u7ed3\u5408\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f\u57fa\u51fd\u6570\u548c\u4e8c\u6b21\u65e0\u7ea6\u675f\u4e8c\u8fdb\u5236\u4f18\u5316\uff08QUBO\uff09\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u5c06\u5ea6\u9009\u62e9\u95ee\u9898\u91cd\u6784\u4e3aQUBO\u4efb\u52a1\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u8ba1\u7b97\u590d\u6742\u5ea6\u8fc7\u9ad8\u548c\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f\u57fa\u51fd\u6570\u548cQUBO\u4f18\u5316\u65b9\u6cd5\uff0c\u5c06\u5ea6\u9009\u62e9\u95ee\u9898\u7b80\u5316\u4e3a\u5355\u5c42\u4f18\u5316\u6b65\u9aa4\u3002", "result": "\u5728\u6570\u636e\u6709\u9650\u7684\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u826f\u597d\u7684\u8f93\u5165\u5c3a\u5ea6\u9c81\u68d2\u6027\u548c\u81ea\u7136\u6b63\u5219\u5316\u7279\u6027\u3002", "conclusion": "CP-KAN\u5728\u6570\u636e\u6548\u7387\u548c\u6570\u503c\u7a33\u5b9a\u6027\u8981\u6c42\u9ad8\u7684\u573a\u666f\u4e2d\u5177\u6709\u7ade\u4e89\u529b\u3002", "keywords": "CP-KAN, \u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f, QUBO, \u6570\u636e\u6548\u7387, \u6570\u503c\u7a33\u5b9a\u6027"}}
{"id": "2505.14978", "pdf": "https://arxiv.org/pdf/2505.14978", "abs": "https://arxiv.org/abs/2505.14978", "authors": ["Ghasem Pasandi", "Kishor Kunal", "Varun Tej", "Kunjal Shan", "Hanfei Sun", "Sumit Jain", "Chunhui Li", "Chenhui Deng", "Teodor-Dumitru Ene", "Haoxing Ren", "Sreedhar Pratty"], "title": "JARVIS: A Multi-Agent Code Assistant for High-Quality EDA Script Generation", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper presents JARVIS, a novel multi-agent framework that leverages\nLarge Language Models (LLMs) and domain expertise to generate high-quality\nscripts for specialized Electronic Design Automation (EDA) tasks. By combining\na domain-specific LLM trained with synthetically generated data, a custom\ncompiler for structural verification, rule enforcement, code fixing\ncapabilities, and advanced retrieval mechanisms, our approach achieves\nsignificant improvements over state-of-the-art domain-specific models. Our\nframework addresses the challenges of data scarcity and hallucination errors in\nLLMs, demonstrating the potential of LLMs in specialized engineering domains.\nWe evaluate our framework on multiple benchmarks and show that it outperforms\nexisting models in terms of accuracy and reliability. Our work sets a new\nprecedent for the application of LLMs in EDA and paves the way for future\ninnovations in this field.", "AI": {"tldr": "JARVIS\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4e3aEDA\u4efb\u52a1\u751f\u6210\u9ad8\u8d28\u91cf\u811a\u672c\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u5e7b\u89c9\u9519\u8bef\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3EDA\u4efb\u52a1\u4e2d\u6570\u636e\u7a00\u7f3a\u548cLLMs\u7684\u5e7b\u89c9\u9519\u8bef\u95ee\u9898\uff0c\u63a2\u7d22LLMs\u5728\u4e13\u4e1a\u5de5\u7a0b\u9886\u57df\u7684\u6f5c\u529b\u3002", "method": "\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u7684LLM\u3001\u81ea\u5b9a\u4e49\u7f16\u8bd1\u5668\u3001\u89c4\u5219\u5f3a\u5236\u6267\u884c\u3001\u4ee3\u7801\u4fee\u590d\u80fd\u529b\u548c\u9ad8\u7ea7\u68c0\u7d22\u673a\u5236\u3002", "result": "\u5728\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u9886\u57df\u7279\u5b9a\u6a21\u578b\u3002", "conclusion": "JARVIS\u4e3aLLMs\u5728EDA\u9886\u57df\u7684\u5e94\u7528\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u4e3a\u672a\u6765\u521b\u65b0\u94fa\u5e73\u9053\u8def\u3002", "keywords": "JARVIS, LLMs, EDA, \u591a\u667a\u80fd\u4f53\u6846\u67b6, \u5e7b\u89c9\u9519\u8bef, \u6570\u636e\u7a00\u7f3a"}}
{"id": "2505.15337", "pdf": "https://arxiv.org/pdf/2505.15337", "abs": "https://arxiv.org/abs/2505.15337", "authors": ["Hao Fang", "Jiawei Kong", "Tianqu Zhuang", "Yixiang Qiu", "Kuofeng Gao", "Bin Chen", "Shu-Tao Xia", "Yaowei Wang", "Min Zhang"], "title": "Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The misuse of large language models (LLMs), such as academic plagiarism, has\ndriven the development of detectors to identify LLM-generated texts. To bypass\nthese detectors, paraphrase attacks have emerged to purposely rewrite these\ntexts to evade detection. Despite the success, existing methods require\nsubstantial data and computational budgets to train a specialized paraphraser,\nand their attack efficacy greatly reduces when faced with advanced detection\nalgorithms. To address this, we propose \\textbf{Co}ntrastive\n\\textbf{P}araphrase \\textbf{A}ttack (CoPA), a training-free method that\neffectively deceives text detectors using off-the-shelf LLMs. The first step is\nto carefully craft instructions that encourage LLMs to produce more human-like\ntexts. Nonetheless, we observe that the inherent statistical biases of LLMs can\nstill result in some generated texts carrying certain machine-like attributes\nthat can be captured by detectors. To overcome this, CoPA constructs an\nauxiliary machine-like word distribution as a contrast to the human-like\ndistribution generated by the LLM. By subtracting the machine-like patterns\nfrom the human-like distribution during the decoding process, CoPA is able to\nproduce sentences that are less discernible by text detectors. Our theoretical\nanalysis suggests the superiority of the proposed attack. Extensive experiments\nvalidate the effectiveness of CoPA in fooling text detectors across various\nscenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5CoPA\uff0c\u5229\u7528\u73b0\u6210\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u6587\u672c\uff0c\u901a\u8fc7\u5bf9\u6bd4\u673a\u5668\u548c\u4eba\u7c7b\u8bcd\u6c47\u5206\u5e03\u6765\u89c4\u907f\u6587\u672c\u68c0\u6d4b\u5668\u7684\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u89c4\u907f\u6587\u672c\u68c0\u6d4b\u5668\u7684\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e14\u5728\u9ad8\u7ea7\u68c0\u6d4b\u7b97\u6cd5\u4e0b\u6548\u679c\u4e0d\u4f73\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86CoPA\u65b9\u6cd5\u3002", "method": "CoPA\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6307\u4ee4\u8ba9LLM\u751f\u6210\u66f4\u4eba\u7c7b\u5316\u7684\u6587\u672c\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u673a\u5668\u548c\u4eba\u7c7b\u8bcd\u6c47\u5206\u5e03\u6765\u6d88\u9664\u673a\u5668\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCoPA\u80fd\u6709\u6548\u6b3a\u9a97\u591a\u79cd\u6587\u672c\u68c0\u6d4b\u5668\u3002", "conclusion": "CoPA\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u5bf9\u6297\u6587\u672c\u68c0\u6d4b\u5668\u7684\u65b9\u6cd5\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u6587\u672c\u68c0\u6d4b\u5668, \u5bf9\u6297\u653b\u51fb, CoPA"}}
{"id": "2505.15231", "pdf": "https://arxiv.org/pdf/2505.15231", "abs": "https://arxiv.org/abs/2505.15231", "authors": ["Kabir V. Dabholkar", "Omri Barak"], "title": "Finding separatrices of dynamical flows with Deep Koopman Eigenfunctions", "categories": ["cs.LG"], "comment": null, "summary": "Many natural systems, including neural circuits involved in decision making,\ncan be modeled as high-dimensional dynamical systems with multiple stable\nstates. While existing analytical tools primarily describe behavior near stable\nequilibria, characterizing separatrices -- the manifolds that delineate\nboundaries between different basins of attraction -- remains challenging,\nparticularly in high-dimensional settings. Here, we introduce a numerical\nframework leveraging Koopman Theory combined with Deep Neural Networks to\neffectively characterize separatrices. Specifically, we approximate Koopman\nEigenfunctions (KEFs) associated with real positive eigenvalues, which vanish\nprecisely at the separatrices. Utilizing these scalar KEFs, optimization\nmethods efficiently locate separatrices even in complex systems. We demonstrate\nour approach on synthetic benchmarks, ecological network models, and recurrent\nneural networks trained on neuroscience-inspired tasks. Moreover, we illustrate\nthe practical utility of our method by designing optimal perturbations that can\nshift systems across separatrices, enabling predictions relevant to optogenetic\nstimulation experiments in neuroscience.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Koopman\u7406\u8bba\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6570\u503c\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u8868\u5f81\u9ad8\u7ef4\u52a8\u529b\u5b66\u7cfb\u7edf\u4e2d\u7684\u5206\u754c\u9762\uff0c\u5e76\u5728\u591a\u4e2a\u6a21\u578b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u9ad8\u7ef4\u52a8\u529b\u5b66\u7cfb\u7edf\u4e2d\u5206\u754c\u9762\uff08\u5206\u9694\u5438\u5f15\u76c6\u7684\u6d41\u5f62\uff09\u96be\u4ee5\u8868\u5f81\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u3002", "method": "\u65b9\u6cd5\u7ed3\u5408Koopman\u7406\u8bba\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u8fd1\u4f3c\u4e0e\u6b63\u5b9e\u7279\u5f81\u503c\u76f8\u5173\u7684Koopman\u672c\u5f81\u51fd\u6570\uff0c\u5229\u7528\u4f18\u5316\u65b9\u6cd5\u5b9a\u4f4d\u5206\u754c\u9762\u3002", "result": "\u7ed3\u679c\u5728\u5408\u6210\u57fa\u51c6\u3001\u751f\u6001\u7f51\u7edc\u6a21\u578b\u548c\u795e\u7ecf\u79d1\u5b66\u4efb\u52a1\u8bad\u7ec3\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u8bbe\u8ba1\u6700\u4f18\u6270\u52a8\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u9ad8\u7ef4\u7cfb\u7edf\u4e2d\u9ad8\u6548\u4e14\u5b9e\u7528\uff0c\u7279\u522b\u9002\u7528\u4e8e\u795e\u7ecf\u79d1\u5b66\u4e2d\u7684\u5b9e\u9a8c\u9884\u6d4b\u3002", "keywords": "Koopman\u7406\u8bba, \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc, \u5206\u754c\u9762, \u9ad8\u7ef4\u52a8\u529b\u5b66\u7cfb\u7edf, \u795e\u7ecf\u79d1\u5b66"}}
{"id": "2505.15347", "pdf": "https://arxiv.org/pdf/2505.15347", "abs": "https://arxiv.org/abs/2505.15347", "authors": ["Xiang Liu", "Hong Chen", "Xuming Hu", "Xiaowen Chu"], "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management", "categories": ["cs.CL"], "comment": "18 pages", "summary": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns.", "AI": {"tldr": "FlowKV\u662f\u4e00\u79cd\u65b0\u7684KV\u7f13\u5b58\u7ba1\u7406\u673a\u5236\uff0c\u901a\u8fc7\u591a\u8f6e\u9694\u79bb\u907f\u514d\u91cd\u590d\u538b\u7f29\u65e7\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u51cf\u8f7b\u4fe1\u606f\u4e22\u5931\u548c\u9057\u5fd8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\uff0cKV\u7f13\u5b58\u7684\u7ebf\u6027\u589e\u957f\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u73b0\u6709\u7b56\u7565\u56e0\u91cd\u590d\u538b\u7f29\u65e9\u671f\u4e0a\u4e0b\u6587\u800c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faFlowKV\u7684\u591a\u8f6e\u9694\u79bb\u673a\u5236\uff0c\u4ec5\u5bf9\u65b0\u751f\u6210\u7684KV\u5bf9\u8fdb\u884c\u538b\u7f29\uff0c\u4fdd\u7559\u5386\u53f2\u538b\u7f29\u7f13\u5b58\uff0c\u907f\u514d\u91cd\u590d\u538b\u7f29\u3002", "result": "FlowKV\u5728\u6307\u4ee4\u9075\u5faa\u51c6\u786e\u6027\u548c\u7528\u6237\u504f\u597d\u4fdd\u7559\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\u7b56\u7565\uff0c\u63d0\u5347\u5e45\u5ea6\u4ece10.90%\u523075.40%\u3002", "conclusion": "FlowKV\u901a\u8fc7\u591a\u8f6e\u9694\u79bb\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86KV\u7f13\u5b58\u7ba1\u7406\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\u548c\u9057\u5fd8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8f6e\u5bf9\u8bdd\u6027\u80fd\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, KV\u7f13\u5b58, \u591a\u8f6e\u5bf9\u8bdd, FlowKV, \u4fe1\u606f\u4fdd\u7559"}}
{"id": "2505.15239", "pdf": "https://arxiv.org/pdf/2505.15239", "abs": "https://arxiv.org/abs/2505.15239", "authors": ["Peter S\u00faken\u00edk", "Christoph H. Lampert", "Marco Mondelli"], "title": "Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The empirical emergence of neural collapse -- a surprising symmetry in the\nfeature representations of the training data in the penultimate layer of deep\nneural networks -- has spurred a line of theoretical research aimed at its\nunderstanding. However, existing work focuses on data-agnostic models or, when\ndata structure is taken into account, it remains limited to multi-layer\nperceptrons. Our paper fills both these gaps by analyzing modern architectures\nin a data-aware regime: we prove that global optima of deep regularized\ntransformers and residual networks (ResNets) with LayerNorm trained with cross\nentropy or mean squared error loss are approximately collapsed, and the\napproximation gets tighter as the depth grows. More generally, we formally\nreduce any end-to-end large-depth ResNet or transformer training into an\nequivalent unconstrained features model, thus justifying its wide use in the\nliterature even beyond data-agnostic settings. Our theoretical results are\nsupported by experiments on computer vision and language datasets showing that,\nas the depth grows, neural collapse indeed becomes more prominent.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u73b0\u4ee3\u6df1\u5ea6\u67b6\u6784\u5728\u6570\u636e\u611f\u77e5\u6a21\u5f0f\u4e0b\u795e\u7ecf\u5d29\u6e83\u7684\u51fa\u73b0\uff0c\u8bc1\u660e\u4e86\u5168\u5c40\u6700\u4f18\u89e3\u8fd1\u4f3c\u5d29\u6e83\uff0c\u968f\u7740\u6df1\u5ea6\u589e\u52a0\uff0c\u8fd1\u4f3c\u66f4\u7d27\u5bc6\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u5d29\u6e83\u5728\u6df1\u5ea6\u7f51\u7edc\u4e2d\u7684\u51fa\u73b0\uff0c\u586b\u8865\u73b0\u6709\u7406\u8bba\u5728\u6570\u636e\u611f\u77e5\u548c\u591a\u5c42\u611f\u77e5\u673a\u4e4b\u5916\u7684\u7a7a\u767d\u3002", "method": "\u5206\u6790\u6b63\u5219\u5316\u53d8\u6362\u5668\u548c\u6b8b\u5dee\u7f51\u7edc\uff08ResNets\uff09\u5728\u4ea4\u53c9\u71b5\u6216\u5747\u65b9\u8bef\u5dee\u635f\u5931\u4e0b\u7684\u5168\u5c40\u6700\u4f18\u89e3\u3002", "result": "\u968f\u7740\u6df1\u5ea6\u589e\u52a0\uff0c\u795e\u7ecf\u5d29\u6e83\u66f4\u52a0\u663e\u8457\uff0c\u7406\u8bba\u7ed3\u679c\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u6570\u636e\u5b9e\u9a8c\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u6df1\u5ea6\u7f51\u7edc\u8bad\u7ec3\u53ef\u4ee5\u7b80\u5316\u4e3a\u65e0\u7ea6\u675f\u7279\u5f81\u6a21\u578b\uff0c\u652f\u6301\u5176\u5728\u6570\u636e\u611f\u77e5\u4e4b\u5916\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "keywords": "\u795e\u7ecf\u5d29\u6e83\uff0c\u6df1\u5ea6\u7f51\u7edc\uff0c\u53d8\u6362\u5668\uff0c\u6b8b\u5dee\u7f51\u7edc\uff0c\u6570\u636e\u611f\u77e5"}}
{"id": "2505.15348", "pdf": "https://arxiv.org/pdf/2505.15348", "abs": "https://arxiv.org/abs/2505.15348", "authors": ["Enric Junqu\u00e9 de Fortuny"], "title": "The Super Emotion Dataset", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite the wide-scale usage and development of emotion classification\ndatasets in NLP, the field lacks a standardized, large-scale resource that\nfollows a psychologically grounded taxonomy. Existing datasets either use\ninconsistent emotion categories, suffer from limited sample size, or focus on\nspecific domains. The Super Emotion Dataset addresses this gap by harmonizing\ndiverse text sources into a unified framework based on Shaver's empirically\nvalidated emotion taxonomy, enabling more consistent cross-domain emotion\nrecognition research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5fc3\u7406\u5b66\u7684\u60c5\u611f\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u7c7b\u522b\u4e0d\u4e00\u81f4\u3001\u6837\u672c\u91cf\u5c0f\u6216\u9886\u57df\u5c40\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u60c5\u611f\u5206\u7c7b\u6570\u636e\u96c6\u7f3a\u4e4f\u6807\u51c6\u5316\u4e14\u5fc3\u7406\u5b66\u57fa\u7840\u4e0d\u8db3\uff0c\u4f5c\u8005\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u5229\u7528Shaver\u7684\u60c5\u611f\u5206\u7c7b\u6cd5\uff0c\u6574\u5408\u4e86\u591a\u6837\u5316\u7684\u6587\u672c\u6765\u6e90\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u89c4\u6a21\u8f83\u5927\u3001\u8de8\u9886\u57df\u4e00\u81f4\u7684\u60c5\u611f\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u8de8\u9886\u57df\u60c5\u611f\u8bc6\u522b\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u4e00\u81f4\u7684\u8d44\u6e90\u3002", "keywords": "\u60c5\u611f\u5206\u7c7b\u3001\u5fc3\u7406\u5b66\u3001\u6570\u636e\u96c6\u3001\u8de8\u9886\u57df"}}
{"id": "2505.15244", "pdf": "https://arxiv.org/pdf/2505.15244", "abs": "https://arxiv.org/abs/2505.15244", "authors": ["Mohamad Mestoukirdi", "Mourad Khanfouci"], "title": "Reliable Vertical Federated Learning in 5G Core Network Architecture", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "Globecom Submission", "summary": "This work proposes a new algorithm to mitigate model generalization loss in\nVertical Federated Learning (VFL) operating under client reliability\nconstraints within 5G Core Networks (CNs). Recently studied and endorsed by\n3GPP, VFL enables collaborative and load-balanced model training and inference\nacross the CN. However, the performance of VFL significantly degrades when the\nNetwork Data Analytics Functions (NWDAFs) - which serve as primary clients for\nVFL model training and inference - experience reliability issues stemming from\nresource constraints and operational overhead. Unlike edge environments, CN\nenvironments adopt fundamentally different data management strategies,\ncharacterized by more centralized data orchestration capabilities. This\npresents opportunities to implement better distributed solutions that take full\nadvantage of the CN data handling flexibility. Leveraging this flexibility, we\npropose a method that optimizes the vertical feature split among clients while\ncentrally defining their local models based on reliability metrics. Our\nempirical evaluation demonstrates the effectiveness of our proposed algorithm,\nshowing improved performance over traditional baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u7f13\u89e35G\u6838\u5fc3\u7f51\u7edc\u4e2d\u5782\u76f4\u8054\u90a6\u5b66\u4e60\uff08VFL\uff09\u5728\u5ba2\u6237\u7aef\u53ef\u9760\u6027\u7ea6\u675f\u4e0b\u7684\u6a21\u578b\u6cdb\u5316\u635f\u5931\u95ee\u9898\u3002", "motivation": "VFL\u57285G\u6838\u5fc3\u7f51\u7edc\u4e2d\u7684\u6027\u80fd\u56e0\u5ba2\u6237\u7aef\uff08NWDAFs\uff09\u7684\u53ef\u9760\u6027\u95ee\u9898\u800c\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f18\u5316\u5ba2\u6237\u7aef\u95f4\u7684\u5782\u76f4\u7279\u5f81\u5206\u5272\uff0c\u5e76\u6839\u636e\u53ef\u9760\u6027\u6307\u6807\u96c6\u4e2d\u5b9a\u4e49\u672c\u5730\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u7b97\u6cd5\u6bd4\u4f20\u7edf\u65b9\u6cd5\u6027\u80fd\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347VFL\u57285G\u6838\u5fc3\u7f51\u7edc\u4e2d\u7684\u6027\u80fd\u3002", "keywords": "\u5782\u76f4\u8054\u90a6\u5b66\u4e60\uff0c5G\u6838\u5fc3\u7f51\u7edc\uff0c\u6a21\u578b\u6cdb\u5316\uff0c\u5ba2\u6237\u7aef\u53ef\u9760\u6027"}}
{"id": "2505.15002", "pdf": "https://arxiv.org/pdf/2505.15002", "abs": "https://arxiv.org/abs/2505.15002", "authors": ["Fernando Lucatelli Nunes", "Gordon Plotkin", "Matthijs V\u00e1k\u00e1r"], "title": "Unraveling the iterative CHAD", "categories": ["cs.PL", "cs.AI", "cs.LG", "math.CT", "math.LO", "18C10, 18C15, 18C20, 18D05, 03B70, 03F52, 68Q55, 68N18, 68T07", "F.3.2; F.3.3; D.3.1; D.3.2; D.2.4; G.4; I.2.3; I.2.6; G.1.10"], "comment": "57 pages", "summary": "Combinatory Homomorphic Automatic Differentiation (CHAD) was originally\nformulated as a semantics-driven source transformation for reverse-mode AD in\ntotal programming languages. We extend this framework to partial languages with\nfeatures such as potentially non-terminating operations, real-valued\nconditionals, and iteration constructs like while-loops, while preserving\nCHAD's structure-preserving semantics principle. A key contribution is the\nintroduction of iteration-extensive indexed categories, which allow iteration\nin the base category to lift to parameterized initial algebras in the indexed\ncategory. This enables iteration to be interpreted in the Grothendieck\nconstruction of the target language in a principled way. The resulting fibred\niterative structure cleanly models iteration in the categorical semantics.\nConsequently, the extended CHAD transformation remains the unique\nstructure-preserving functor (an iterative Freyd category morphism) from the\nfreely generated iterative Freyd category of the source language to the\nGrothendieck construction of the target's syntactic semantics, mapping each\nprimitive operation to its derivative. We prove the correctness of this\ntransformation using the universal property of the source language's syntax,\nshowing that the transformed programs compute correct reverse-mode derivatives.\nOur development also contributes to understanding iteration constructs within\ndependently typed languages and categories of containers. As our primary\nmotivation and application, we generalize CHAD to languages with data types,\npartial features, and iteration, providing the first rigorous categorical\nsemantics for reverse-mode CHAD in such settings and formally guaranteeing the\ncorrectness of the source-to-source CHAD technique.", "AI": {"tldr": "\u8bba\u6587\u6269\u5c55\u4e86CHAD\u6846\u67b6\uff0c\u652f\u6301\u90e8\u5206\u7f16\u7a0b\u8bed\u8a00\u4e2d\u7684\u975e\u7ec8\u6b62\u64cd\u4f5c\u3001\u6761\u4ef6\u8bed\u53e5\u548c\u5faa\u73af\u7ed3\u6784\uff0c\u5f15\u5165\u4e86\u8fed\u4ee3\u6269\u5c55\u7d22\u5f15\u7c7b\u522b\uff0c\u5e76\u8bc1\u660e\u5176\u6b63\u786e\u6027\u3002", "motivation": "\u4e3a\u90e8\u5206\u7f16\u7a0b\u8bed\u8a00\u63d0\u4f9bCHAD\u7684\u6269\u5c55\uff0c\u652f\u6301\u66f4\u591a\u8bed\u8a00\u7279\u6027\u5e76\u786e\u4fdd\u7ed3\u6784\u4fdd\u6301\u8bed\u4e49\u3002", "method": "\u5f15\u5165\u8fed\u4ee3\u6269\u5c55\u7d22\u5f15\u7c7b\u522b\uff0c\u901a\u8fc7Grothendieck\u6784\u9020\u5c06\u8fed\u4ee3\u63d0\u5347\u5230\u76ee\u6807\u8bed\u8a00\u7684\u521d\u59cb\u4ee3\u6570\u4e2d\u3002", "result": "\u6269\u5c55\u540e\u7684CHAD\u8f6c\u6362\u4fdd\u6301\u4e86\u7ed3\u6784\u552f\u4e00\u6027\uff0c\u5e76\u6b63\u786e\u8ba1\u7b97\u53cd\u5411\u6a21\u5f0f\u5bfc\u6570\u3002", "conclusion": "\u8bba\u6587\u4e3a\u4f9d\u8d56\u7c7b\u578b\u8bed\u8a00\u548c\u5bb9\u5668\u7c7b\u522b\u4e2d\u7684\u8fed\u4ee3\u7ed3\u6784\u63d0\u4f9b\u4e86\u65b0\u7406\u89e3\uff0c\u5e76\u6269\u5c55\u4e86CHAD\u7684\u5e94\u7528\u8303\u56f4\u3002", "keywords": "CHAD, \u53cd\u5411\u6a21\u5f0f\u81ea\u52a8\u5fae\u5206, \u8fed\u4ee3\u6269\u5c55\u7d22\u5f15\u7c7b\u522b, Grothendieck\u6784\u9020, \u4f9d\u8d56\u7c7b\u578b\u8bed\u8a00"}}
{"id": "2505.15353", "pdf": "https://arxiv.org/pdf/2505.15353", "abs": "https://arxiv.org/abs/2505.15353", "authors": ["Ryo Kishino", "Yusuke Takase", "Momose Oyama", "Hiroaki Yamagiwa", "Hidetoshi Shimodaira"], "title": "Revealing Language Model Trajectories via Kullback-Leibler Divergence", "categories": ["cs.CL"], "comment": null, "summary": "A recently proposed method enables efficient estimation of the KL divergence\nbetween language models, including models with different architectures, by\nassigning coordinates based on log-likelihood vectors. To better understand the\nbehavior of this metric, we systematically evaluate KL divergence across a wide\nrange of conditions using publicly available language models. Our analysis\ncovers comparisons between pretraining checkpoints, fine-tuned and base models,\nand layers via the logit lens. We find that trajectories of language models, as\nmeasured by KL divergence, exhibit a spiral structure during pretraining and\nthread-like progressions across layers. Furthermore, we show that, in terms of\ndiffusion exponents, model trajectories in the log-likelihood space are more\nconstrained than those in weight space.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6570\u4f3c\u7136\u5411\u91cf\u6d4b\u91cf\u8bed\u8a00\u6a21\u578b\u95f4KL\u6563\u5ea6\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u7ed3\u679c\uff0c\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u4e2d\u7684\u8f68\u8ff9\u5448\u87ba\u65cb\u7ed3\u6784\uff0c\u800c\u5728\u5bf9\u6570\u4f3c\u7136\u7a7a\u95f4\u4e2d\u6a21\u578b\u7684\u8f68\u8ff9\u6bd4\u6743\u91cd\u7a7a\u95f4\u4e2d\u66f4\u4e3a\u53d7\u9650\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u57fa\u4e8e\u5bf9\u6570\u4f3c\u7136\u5411\u91cf\u6d4b\u91cf\u8bed\u8a00\u6a21\u578bKL\u6563\u5ea6\u7684\u884c\u4e3a\uff0c\u7814\u7a76\u8005\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u8fd9\u4e00\u5ea6\u91cf\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u53ef\u7528\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u57fa\u4e8e\u5bf9\u6570\u4f3c\u7136\u5411\u91cf\u7684\u65b9\u6cd5\u6d4b\u91cfKL\u6563\u5ea6\uff0c\u5bf9\u6bd4\u4e86\u9884\u8bad\u7ec3\u68c0\u67e5\u70b9\u3001\u5fae\u8c03\u4e0e\u57fa\u7840\u6a21\u578b\u4ee5\u53ca\u4e0d\u540c\u5c42\u7684\u5dee\u5f02\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u4e2d\u7684\u8f68\u8ff9\u5448\u73b0\u87ba\u65cb\u7ed3\u6784\uff0c\u800c\u5728\u5bf9\u6570\u4f3c\u7136\u7a7a\u95f4\u4e2d\u6a21\u578b\u8f68\u8ff9\u6bd4\u6743\u91cd\u7a7a\u95f4\u4e2d\u66f4\u4e3a\u53d7\u9650\u3002", "conclusion": "\u57fa\u4e8e\u5bf9\u6570\u4f3c\u7136\u5411\u91cf\u7684KL\u6563\u5ea6\u6d4b\u91cf\u65b9\u6cd5\u80fd\u591f\u63ed\u793a\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u7279\u5f81\uff0c\u5bf9\u6570\u4f3c\u7136\u7a7a\u95f4\u7684\u8f68\u8ff9\u6bd4\u6743\u91cd\u7a7a\u95f4\u66f4\u5177\u9650\u5236\u6027\u3002", "keywords": "KL\u6563\u5ea6, \u8bed\u8a00\u6a21\u578b, \u5bf9\u6570\u4f3c\u7136\u5411\u91cf, \u9884\u8bad\u7ec3\u8f68\u8ff9, \u6743\u91cd\u7a7a\u95f4"}}
{"id": "2505.15246", "pdf": "https://arxiv.org/pdf/2505.15246", "abs": "https://arxiv.org/abs/2505.15246", "authors": ["Xiaoling Zhou", "Wei Ye", "Rui Xie", "Shikun Zhang"], "title": "Mitigating Spurious Correlations with Causal Logit Perturbation", "categories": ["cs.LG", "K.3.2", "F.4.1"], "comment": "34 pages,9 figures", "summary": "Deep learning has seen widespread success in various domains such as science,\nindustry, and society. However, it is acknowledged that certain approaches\nsuffer from non-robustness, relying on spurious correlations for predictions.\nAddressing these limitations is of paramount importance, necessitating the\ndevelopment of methods that can disentangle spurious correlations. {This study\nattempts to implement causal models via logit perturbations and introduces a\nnovel Causal Logit Perturbation (CLP) framework to train classifiers with\ngenerated causal logit perturbations for individual samples, thereby mitigating\nthe spurious associations between non-causal attributes (i.e., image\nbackgrounds) and classes.} {Our framework employs a} perturbation network to\ngenerate sample-wise logit perturbations using a series of training\ncharacteristics of samples as inputs. The whole framework is optimized by an\nonline meta-learning-based learning algorithm and leverages human causal\nknowledge by augmenting metadata in both counterfactual and factual manners.\nEmpirical evaluations on four typical biased learning scenarios, including\nlong-tail learning, noisy label learning, generalized long-tail learning, and\nsubpopulation shift learning, demonstrate that CLP consistently achieves\nstate-of-the-art performance. Moreover, visualization results support the\neffectiveness of the generated causal perturbations in redirecting model\nattention towards causal image attributes and dismantling spurious\nassociations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56e0\u679c\u5bf9\u6570\u6270\u52a8\uff08CLP\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6270\u52a8\u7f51\u7edc\u751f\u6210\u6837\u672c\u7ea7\u7684\u5bf9\u6570\u6270\u52a8\uff0c\u4ee5\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u4f2a\u76f8\u5173\u6027\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u6709\u504f\u5b66\u4e60\u573a\u666f\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e2d\u67d0\u4e9b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4f2a\u76f8\u5173\u6027\u8fdb\u884c\u9884\u6d4b\uff0c\u5bfc\u81f4\u975e\u9c81\u68d2\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u56e0\u679c\u6a21\u578b\u89e3\u9664\u8fd9\u4e9b\u4f2a\u76f8\u5173\u6027\u3002", "method": "\u91c7\u7528\u56e0\u679c\u5bf9\u6570\u6270\u52a8\uff08CLP\uff09\u6846\u67b6\uff0c\u5229\u7528\u6270\u52a8\u7f51\u7edc\u751f\u6210\u6837\u672c\u7ea7\u7684\u5bf9\u6570\u6270\u52a8\uff0c\u5e76\u7ed3\u5408\u5728\u7ebf\u5143\u5b66\u4e60\u7b97\u6cd5\u4f18\u5316\u6846\u67b6\u3002", "result": "\u5728\u56db\u79cd\u5178\u578b\u6709\u504f\u5b66\u4e60\u573a\u666f\uff08\u957f\u5c3e\u5b66\u4e60\u3001\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u3001\u5e7f\u4e49\u957f\u5c3e\u5b66\u4e60\u548c\u5b50\u7fa4\u4f53\u504f\u79fb\u5b66\u4e60\uff09\u4e2d\uff0cCLP\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CLP\u6846\u67b6\u80fd\u6709\u6548\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u56e0\u679c\u5c5e\u6027\u5e76\u6d88\u9664\u4f2a\u76f8\u5173\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "keywords": "\u6df1\u5ea6\u5b66\u4e60\u3001\u56e0\u679c\u6a21\u578b\u3001\u5bf9\u6570\u6270\u52a8\u3001\u4f2a\u76f8\u5173\u6027\u3001\u5143\u5b66\u4e60"}}
{"id": "2505.15355", "pdf": "https://arxiv.org/pdf/2505.15355", "abs": "https://arxiv.org/abs/2505.15355", "authors": ["Xabier de Zuazo", "Eva Navas", "Ibon Saratxaga", "Mathieu Bourguignon", "Nicola Molinaro"], "title": "Decoding Phone Pairs from MEG Signals Across Speech Modalities", "categories": ["cs.CL", "cs.LG", "cs.NE", "cs.SD", "eess.AS", "I.2.6; I.5.1"], "comment": "21 pages, 4 figures, 1 graphical abstract, submitted to Computer\n  Speech and Language (special issue on Iberian Languages)", "summary": "Understanding the neural mechanisms underlying speech production is essential\nfor both advancing cognitive neuroscience theory and developing practical\ncommunication technologies. In this study, we investigated\nmagnetoencephalography signals to decode phones from brain activity during\nspeech production and perception (passive listening and voice playback) tasks.\nUsing a dataset comprising 17 participants, we performed pairwise phone\nclassification, extending our analysis to 15 phonetic pairs. Multiple machine\nlearning approaches, including regularized linear models and neural network\narchitectures, were compared to determine their effectiveness in decoding\nphonetic information. Our results demonstrate significantly higher decoding\naccuracy during speech production (76.6%) compared to passive listening and\nplayback modalities (~51%), emphasizing the richer neural information available\nduring overt speech. Among the models, the Elastic Net classifier consistently\noutperformed more complex neural networks, highlighting the effectiveness of\ntraditional regularization techniques when applied to limited and\nhigh-dimensional MEG datasets. Besides, analysis of specific brain frequency\nbands revealed that low-frequency oscillations, particularly Delta (0.2-3 Hz)\nand Theta (4-7 Hz), contributed the most substantially to decoding accuracy,\nsuggesting that these bands encode critical speech production-related neural\nprocesses. Despite using advanced denoising methods, it remains unclear whether\ndecoding solely reflects neural activity or if residual muscular or movement\nartifacts also contributed, indicating the need for further methodological\nrefinement. Overall, our findings underline the critical importance of\nexamining overt speech production paradigms, which, despite their complexity,\noffer opportunities to improve brain-computer interfaces to help individuals\nwith severe speech impairments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8111\u78c1\u56fe\uff08MEG\uff09\u4fe1\u53f7\u89e3\u7801\u8bed\u97f3\u4ea7\u751f\u548c\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u97f3\u7d20\uff0c\u53d1\u73b0\u8bed\u97f3\u4ea7\u751f\u65f6\u7684\u89e3\u7801\u51c6\u786e\u7387\u663e\u8457\u9ad8\u4e8e\u88ab\u52a8\u542c\u97f3\u3002Elastic Net\u5206\u7c7b\u5668\u8868\u73b0\u6700\u4f73\uff0c\u4f4e\u9891\u6ce2\u6bb5\u5bf9\u89e3\u7801\u8d21\u732e\u6700\u5927\u3002", "motivation": "\u63a2\u7a76\u8bed\u97f3\u4ea7\u751f\u7684\u795e\u7ecf\u673a\u5236\uff0c\u63a8\u52a8\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u7406\u8bba\u53d1\u5c55\u53ca\u901a\u4fe1\u6280\u672f\u5e94\u7528\u3002", "method": "\u4f7f\u752817\u540d\u53c2\u4e0e\u8005\u7684MEG\u6570\u636e\uff0c\u6bd4\u8f83\u591a\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u6b63\u5219\u5316\u7ebf\u6027\u6a21\u578b\u548c\u795e\u7ecf\u7f51\u7edc\uff09\uff0c\u5206\u679015\u5bf9\u97f3\u7d20\u7684\u5206\u7c7b\u6548\u679c\u3002", "result": "\u8bed\u97f3\u4ea7\u751f\u4efb\u52a1\u89e3\u7801\u51c6\u786e\u738776.6%\uff0c\u663e\u8457\u9ad8\u4e8e\u88ab\u52a8\u542c\u97f3\uff0851%\uff09\u3002Elastic Net\u8868\u73b0\u6700\u4f18\uff0c\u4f4e\u9891\u632f\u8361\uff08Delta\u548cTheta\u6ce2\u6bb5\uff09\u8d21\u732e\u6700\u5927\u3002", "conclusion": "\u8bed\u97f3\u4ea7\u751f\u8303\u5f0f\u4e3a\u7814\u7a76\u795e\u7ecf\u673a\u5236\u63d0\u4f9b\u4e86\u4e30\u5bcc\u4fe1\u606f\uff0c\u5bf9\u6539\u8fdb\u8111\u673a\u63a5\u53e3\u6280\u672f\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u65b9\u6cd5\u5b66\u4f18\u5316\u3002", "keywords": "\u8bed\u97f3\u4ea7\u751f, MEG, \u97f3\u7d20\u89e3\u7801, \u673a\u5668\u5b66\u4e60, \u4f4e\u9891\u632f\u8361"}}
{"id": "2505.15250", "pdf": "https://arxiv.org/pdf/2505.15250", "abs": "https://arxiv.org/abs/2505.15250", "authors": ["Suping Xu", "Lin Shang", "Keyu Liu", "Hengrong Ju", "Xibei Yang", "Witold Pedrycz"], "title": "Margin-aware Fuzzy Rough Feature Selection: Bridging Uncertainty Characterization and Pattern Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Fuzzy rough feature selection (FRFS) is an effective means of addressing the\ncurse of dimensionality in high-dimensional data. By removing redundant and\nirrelevant features, FRFS helps mitigate classifier overfitting, enhance\ngeneralization performance, and lessen computational overhead. However, most\nexisting FRFS algorithms primarily focus on reducing uncertainty in pattern\nclassification, neglecting that lower uncertainty does not necessarily result\nin improved classification performance, despite it commonly being regarded as a\nkey indicator of feature selection effectiveness in the FRFS literature. To\nbridge uncertainty characterization and pattern classification, we propose a\nMargin-aware Fuzzy Rough Feature Selection (MAFRFS) framework that considers\nboth the compactness and separation of label classes. MAFRFS effectively\nreduces uncertainty in pattern classification tasks, while guiding the feature\nselection towards more separable and discriminative label class structures.\nExtensive experiments on 15 public datasets demonstrate that MAFRFS is highly\nscalable and more effective than FRFS. The algorithms developed using MAFRFS\noutperform six state-of-the-art feature selection algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7d27\u5bc6\u5ea6\u548c\u7c7b\u522b\u5206\u79bb\u6027\u7684\u8fb9\u7f18\u611f\u77e5\u6a21\u7cca\u7c97\u7cd9\u7279\u5f81\u9009\u62e9\uff08MAFRFS\uff09\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u7cca\u7c97\u7cd9\u7279\u5f81\u9009\u62e9\uff08FRFS\uff09\u4ec5\u5173\u6ce8\u4e0d\u786e\u5b9a\u6027\u51cf\u5c11\u800c\u5ffd\u7565\u5206\u7c7b\u6027\u80fd\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660eMAFRFS\u6bd4FRFS\u66f4\u5177\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfFRFS\u65b9\u6cd5\u867d\u7136\u80fd\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u672a\u80fd\u76f4\u63a5\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u7c7b\u522b\u7d27\u5bc6\u5ea6\u548c\u5206\u79bb\u6027\uff0c\u8bbe\u8ba1\u4e00\u4e2a\u66f4\u6709\u6548\u7684\u7279\u5f81\u9009\u62e9\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u8fb9\u7f18\u611f\u77e5\u6a21\u7cca\u7c97\u7cd9\u7279\u5f81\u9009\u62e9\uff08MAFRFS\uff09\u6846\u67b6\uff0c\u7efc\u5408\u8003\u8651\u7c7b\u522b\u7d27\u5bc6\u5ea6\u548c\u5206\u79bb\u6027\uff0c\u4ee5\u964d\u4f4e\u4e0d\u786e\u5b9a\u6027\u5e76\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u572815\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMAFRFS\u6bd4\u4f20\u7edfFRFS\u548c\u516d\u79cd\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\u8868\u73b0\u66f4\u597d\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "MAFRFS\u901a\u8fc7\u7ed3\u5408\u7c7b\u522b\u7d27\u5bc6\u5ea6\u548c\u5206\u79bb\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7279\u5f81\u9009\u62e9\u7684\u6548\u679c\u548c\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u9ad8\u7ef4\u6570\u636e\u964d\u7ef4\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u6a21\u7cca\u7c97\u7cd9\u7279\u5f81\u9009\u62e9, \u964d\u7ef4, \u9ad8\u7ef4\u6570\u636e, \u8fb9\u7f18\u611f\u77e5, \u4e0d\u786e\u5b9a\u6027\u548c\u5206\u79bb\u6027"}}
{"id": "2505.15356", "pdf": "https://arxiv.org/pdf/2505.15356", "abs": "https://arxiv.org/abs/2505.15356", "authors": ["Weiming Zhang", "Qingyao Li", "Xinyi Dai", "Jizheng Chen", "Kounianhua Du", "Weinan Zhang", "Weiwen Liu", "Yasheng Wang", "Ruiming Tang", "Yong Yu"], "title": "NL-Debugging: Exploiting Natural Language as an Intermediate Representation for Code Debugging", "categories": ["cs.CL"], "comment": null, "summary": "Debugging is a critical aspect of LLM's coding ability. Early debugging\nefforts primarily focused on code-level analysis, which often falls short when\naddressing complex programming errors that require a deeper understanding of\nalgorithmic logic. Recent advancements in large language models (LLMs) have\nshifted attention toward leveraging natural language reasoning to enhance\ncode-related tasks. However, two fundamental questions remain unanswered: What\ntype of natural language format is most effective for debugging tasks? And what\nspecific benefits does natural language reasoning bring to the debugging\nprocess? In this paper, we introduce NL-DEBUGGING, a novel framework that\nemploys natural language as an intermediate representation to improve code\ndebugging. By debugging at a natural language level, we demonstrate that\nNL-DEBUGGING outperforms traditional debugging methods and enables a broader\nmodification space through direct refinement guided by execution feedback. Our\nfindings highlight the potential of natural language reasoning to advance\nautomated code debugging and address complex programming challenges.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faNL-DEBUGGING\u6846\u67b6\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u6539\u8fdb\u4ee3\u7801\u8c03\u8bd5\uff0c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u5e76\u901a\u8fc7\u6267\u884c\u53cd\u9988\u76f4\u63a5\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u7ea7\u5206\u6790\u96be\u4ee5\u89e3\u51b3\u590d\u6742\u7684\u7f16\u7a0b\u9519\u8bef\uff0c\u800c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u5728\u4ee3\u7801\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u660e\u786e\u5176\u6700\u4f18\u5f62\u5f0f\u53ca\u5177\u4f53\u4f18\u52bf\u3002", "method": "NL-DEBUGGING\u6846\u67b6\u4ee5\u81ea\u7136\u8bed\u8a00\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u901a\u8fc7\u6267\u884c\u53cd\u9988\u6307\u5bfc\u8c03\u8bd5\uff0c\u63d0\u4f9b\u66f4\u5e7f\u7684\u4fee\u6539\u7a7a\u95f4\u3002", "result": "NL-DEBUGGING\u6846\u67b6\u5728\u4ee3\u7801\u8c03\u8bd5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u6709\u671b\u63a8\u52a8\u81ea\u52a8\u5316\u4ee3\u7801\u8c03\u8bd5\uff0c\u89e3\u51b3\u590d\u6742\u7f16\u7a0b\u6311\u6218\u3002", "keywords": "\u81ea\u7136\u8bed\u8a00\u63a8\u7406, \u4ee3\u7801\u8c03\u8bd5, NL-DEBUGGING, \u6267\u884c\u53cd\u9988"}}
{"id": "2505.15251", "pdf": "https://arxiv.org/pdf/2505.15251", "abs": "https://arxiv.org/abs/2505.15251", "authors": ["Idriss Malek", "Abhijit Sharma", "Salem Lahlou"], "title": "Loss-Guided Auxiliary Agents for Overcoming Mode Collapse in GFlowNets", "categories": ["cs.LG"], "comment": null, "summary": "Although Generative Flow Networks (GFlowNets) are designed to capture\nmultiple modes of a reward function, they often suffer from mode collapse in\npractice, getting trapped in early discovered modes and requiring prolonged\ntraining to find diverse solutions. Existing exploration techniques may rely on\nheuristic novelty signals. We propose Loss-Guided GFlowNets (LGGFN), a novel\napproach where an auxiliary GFlowNet's exploration is directly driven by the\nmain GFlowNet's training loss. By prioritizing trajectories where the main\nmodel exhibits high loss, LGGFN focuses sampling on poorly understood regions\nof the state space. This targeted exploration significantly accelerates the\ndiscovery of diverse, high-reward samples. Empirically, across various\nbenchmarks including grid environments, structured sequence generation, and\nBayesian structure learning, LGGFN consistently enhances exploration efficiency\nand sample diversity compared to baselines. For instance, on a challenging\nsequence generation task, it discovered over 40 times more unique valid modes\nwhile simultaneously reducing the exploration error metric by approximately\n99\\%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLGGFN\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u4e3bGFlowNet\u7684\u8bad\u7ec3\u635f\u5931\u6765\u6307\u5bfc\u8f85\u52a9GFlowNet\u7684\u63a2\u7d22\uff0c\u4ece\u800c\u52a0\u901f\u53d1\u73b0\u591a\u6837\u5316\u7684\u9ad8\u5956\u52b1\u6837\u672c\u3002", "motivation": "\u4f20\u7edf\u7684GFlowNets\u5728\u5b9e\u8df5\u4e2d\u5bb9\u6613\u51fa\u73b0\u6a21\u5f0f\u5d29\u6e83\uff0c\u5bfc\u81f4\u8bad\u7ec3\u65f6\u95f4\u5ef6\u957f\u4e14\u96be\u4ee5\u53d1\u73b0\u591a\u6837\u5316\u7684\u89e3\u3002\u73b0\u6709\u63a2\u7d22\u6280\u672f\u4f9d\u8d56\u4e8e\u542f\u53d1\u5f0f\u7684\u65b0\u9896\u6027\u4fe1\u53f7\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "LGGFN\u901a\u8fc7\u4f18\u5148\u63a2\u7d22\u4e3bGFlowNet\u635f\u5931\u8f83\u9ad8\u7684\u8f68\u8ff9\uff0c\u5c06\u91c7\u6837\u96c6\u4e2d\u5728\u96be\u4ee5\u7406\u89e3\u7684\u533a\u57df\uff0c\u4ece\u800c\u63d0\u5347\u63a2\u7d22\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLGGFN\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u591a\u6837\u6027\u548c\u63a2\u7d22\u6548\u7387\u3002\u4f8b\u5982\uff0c\u5728\u5e8f\u5217\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u5b83\u80fd\u53d1\u73b0\u8d85\u8fc7\u57fa\u7ebf40\u500d\u7684\u6709\u6548\u6a21\u5f0f\u3002", "conclusion": "LGGFN\u662f\u4e00\u79cd\u6709\u6548\u7684\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u5229\u7528\u8bad\u7ec3\u635f\u5931\u6307\u5bfc\u63a2\u7d22\uff0c\u89e3\u51b3\u4e86GFlowNets\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\u3002", "keywords": "GFlowNets, mode collapse, exploration, LGGFN, diverse samples"}}
{"id": "2505.15023", "pdf": "https://arxiv.org/pdf/2505.15023", "abs": "https://arxiv.org/abs/2505.15023", "authors": ["David N. Palacio"], "title": "Towards a Science of Causal Interpretability in Deep Learning for Software Engineering", "categories": ["cs.SE", "cs.AI"], "comment": "PhD thesis, To appear in ProQuest", "summary": "This dissertation addresses achieving causal interpretability in Deep\nLearning for Software Engineering (DL4SE). While Neural Code Models (NCMs) show\nstrong performance in automating software tasks, their lack of transparency in\ncausal relationships between inputs and outputs limits full understanding of\ntheir capabilities. To build trust in NCMs, researchers and practitioners must\nexplain code predictions. Associational interpretability, which identifies\ncorrelations, is often insufficient for tasks requiring intervention and change\nanalysis. To address this, the dissertation introduces DoCode, a novel post hoc\ninterpretability method for NCMs. DoCode uses causal inference to provide\nprogramming language-oriented explanations of model predictions. It follows a\nfour-step pipeline: modeling causal problems using Structural Causal Models\n(SCMs), identifying the causal estimand, estimating effects with metrics like\nAverage Treatment Effect (ATE), and refuting effect estimates. Its framework is\nextensible, with an example that reduces spurious correlations by grounding\nexplanations in programming language properties. A case study on deep code\ngeneration across interpretability scenarios and various deep learning\narchitectures demonstrates DoCode's benefits. Results show NCMs' sensitivity to\ncode syntax changes and their ability to learn certain programming concepts\nwhile minimizing confounding bias. The dissertation also examines associational\ninterpretability as a foundation, analyzing software information's causal\nnature using tools like COMET and TraceXplainer for traceability. It highlights\nthe need to identify code confounders and offers practical guidelines for\napplying causal interpretability to NCMs, contributing to more trustworthy AI\nin software engineering.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDoCode\u7684\u540e\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u56e0\u679c\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4ee3\u7801\u9884\u6d4b\u7684\u89e3\u91ca\u3002", "motivation": "\u7531\u4e8e\u795e\u7ecf\u4ee3\u7801\u6a21\u578b\uff08NCMs\uff09\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u5176\u56e0\u679c\u5173\u7cfb\u7684\u900f\u660e\u5ea6\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5bf9\u5176\u80fd\u529b\u7684\u5168\u9762\u7406\u89e3\uff0c\u7814\u7a76\u65e8\u5728\u63d0\u5347NCMs\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "DoCode\u91c7\u7528\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\uff0c\u5305\u542b\u56db\u4e2a\u6b65\u9aa4\uff1a\u4f7f\u7528\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCMs\uff09\u5efa\u6a21\u3001\u8bc6\u522b\u56e0\u679c\u4f30\u8ba1\u91cf\u3001\u901a\u8fc7\u5e73\u5747\u5904\u7406\u6548\u5e94\uff08ATE\uff09\u7b49\u6307\u6807\u4f30\u8ba1\u6548\u679c\uff0c\u4ee5\u53ca\u5bf9\u6548\u679c\u4f30\u8ba1\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDoCode\u80fd\u591f\u51cf\u5c11\u4f2a\u76f8\u5173\u6027\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u5206\u6790\u5c55\u793aNCMs\u5bf9\u4ee3\u7801\u8bed\u6cd5\u53d8\u5316\u7684\u654f\u611f\u6027\u53ca\u5b66\u4e60\u7f16\u7a0b\u6982\u5ff5\u7684\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3aNCMs\u7684\u56e0\u679c\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u63a8\u52a8\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u66f4\u53ef\u4fe1\u8d56\u7684\u4eba\u5de5\u667a\u80fd\u3002", "keywords": "\u56e0\u679c\u89e3\u91ca\u6027\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u795e\u7ecf\u4ee3\u7801\u6a21\u578b\u3001\u8f6f\u4ef6\u5de5\u7a0b\u3001DoCode"}}
{"id": "2505.15372", "pdf": "https://arxiv.org/pdf/2505.15372", "abs": "https://arxiv.org/abs/2505.15372", "authors": ["Peng Wang", "Ruihan Tao", "Qiguang Chen", "Mengkang Hu", "Libo Qin"], "title": "X-WebAgentBench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Findings", "summary": "Recently, large language model (LLM)-based agents have achieved significant\nsuccess in interactive environments, attracting significant academic and\nindustrial attention. Despite these advancements, current research\npredominantly focuses on English scenarios. In reality, there are over 7,000\nlanguages worldwide, all of which demand access to comparable agentic services.\nNevertheless, the development of language agents remains inadequate for meeting\nthe diverse requirements of multilingual agentic applications. To fill this\ngap, we introduce X-WebAgentBench, a novel multilingual agent benchmark in an\ninteractive web environment, which evaluates the planning and interaction\nperformance of language agents across multiple languages, thereby contributing\nto the advancement of global agent intelligence. Additionally, we assess the\nperformance of various LLMs and cross-lingual alignment methods, examining\ntheir effectiveness in enhancing agents. Our findings reveal that even advanced\nmodels like GPT-4o, when combined with cross-lingual techniques, fail to\nachieve satisfactory results. We hope that X-WebAgentBench can serve as a\nvaluable benchmark for multilingual agent scenario in real-world applications.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86X-WebAgentBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u4ee3\u7406\u5728\u4ea4\u4e92\u5f0f\u7f51\u7edc\u73af\u5883\u4e2d\u89c4\u5212\u4e0e\u4ea4\u4e92\u6027\u80fd\u7684\u65b0\u57fa\u51c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u4ee5\u82f1\u8bed\u4e3a\u4e3b\u7684\u4e0d\u8db3\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u82f1\u8bed\u573a\u666f\uff0c\u5ffd\u89c6\u4e86\u5168\u74037000\u591a\u79cd\u8bed\u8a00\u7684\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86X-WebAgentBench\uff0c\u4e00\u4e2a\u591a\u8bed\u8a00\u4ee3\u7406\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u8bed\u8a00\u4ee3\u7406\u7684\u89c4\u5212\u548c\u4ea4\u4e92\u80fd\u529b\uff0c\u540c\u65f6\u6d4b\u8bd5\u4e86\u591a\u79cdLLM\u548c\u8de8\u8bed\u8a00\u5bf9\u9f50\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u662fGPT-4o\u7b49\u5148\u8fdb\u6a21\u578b\u7ed3\u5408\u8de8\u8bed\u8a00\u6280\u672f\uff0c\u4e5f\u672a\u8fbe\u5230\u6ee1\u610f\u7ed3\u679c\u3002", "conclusion": "X-WebAgentBench\u6709\u671b\u6210\u4e3a\u73b0\u5b9e\u5e94\u7528\u4e2d\u591a\u8bed\u8a00\u4ee3\u7406\u573a\u666f\u7684\u91cd\u8981\u57fa\u51c6\u3002", "keywords": "\u591a\u8bed\u8a00\u4ee3\u7406, LLM, \u57fa\u51c6\u6d4b\u8bd5, \u8de8\u8bed\u8a00\u5bf9\u9f50, GPT-4o"}}
{"id": "2505.15259", "pdf": "https://arxiv.org/pdf/2505.15259", "abs": "https://arxiv.org/abs/2505.15259", "authors": ["Hyunseok Lee", "Jeonghoon Kim", "Beomjun Kim", "Jihoon Tack", "Chansong Jo", "Jaehong Lee", "Cheonbok Park", "Sookyo In", "Jinwoo Shin", "Kang Min Yoo"], "title": "ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled\nautonomous agents to interact with computers via Graphical User Interfaces\n(GUIs), where accurately localizing the coordinates of interface elements\n(e.g., buttons) is often required for fine-grained actions. However, this\nremains significantly challenging, leading prior works to rely on large-scale\nweb datasets to improve the grounding accuracy. In this work, we propose\nReasoning Graphical User Interface Grounding for Data Efficiency (ReGUIDE), a\nnovel and effective framework for web grounding that enables MLLMs to learn\ndata efficiently through self-generated reasoning and spatial-aware criticism.\nMore specifically, ReGUIDE learns to (i) self-generate a language reasoning\nprocess for the localization via online reinforcement learning, and (ii)\ncriticize the prediction using spatial priors that enforce equivariance under\ninput transformations. At inference time, ReGUIDE further boosts performance\nthrough a test-time scaling strategy, which combines spatial search with\ncoordinate aggregation. Our experiments demonstrate that ReGUIDE significantly\nadvances web grounding performance across multiple benchmarks, outperforming\nbaselines with substantially fewer training data points (e.g., only 0.2%\nsamples compared to the best open-sourced baselines).", "AI": {"tldr": "ReGUIDE \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6211\u751f\u6210\u7684\u63a8\u7406\u548c\u7a7a\u95f4\u611f\u77e5\u7684\u6279\u8bc4\u6765\u63d0\u9ad8\u7f51\u9875\u5143\u7d20\u7684\u5b9a\u4f4d\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u4e14\u6570\u636e\u9700\u6c42\u6781\u4f4e\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u4e2d\u7684\u5143\u7d20\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u901a\u5e38\u9700\u8981\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002ReGUIDE \u65e8\u5728\u901a\u8fc7\u6570\u636e\u9ad8\u6548\u7684\u5b66\u4e60\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "ReGUIDE \u6846\u67b6\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u8bed\u8a00\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u7ed3\u5408\u7a7a\u95f4\u5148\u9a8c\u8fdb\u884c\u9884\u6d4b\u6279\u8bc4\u3002\u63a8\u65ad\u65f6\u5229\u7528\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u7b56\u7565\uff08\u7a7a\u95f4\u641c\u7d22\u4e0e\u5750\u6807\u805a\u5408\uff09\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReGUIDE \u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u4ec5\u9700\u6781\u5c11\u7684\u8bad\u7ec3\u6570\u636e\uff08\u4f8b\u5982\u4ec5\u4e3a\u6700\u4f73\u5f00\u6e90\u57fa\u7ebf\u7684 0.2%\uff09\u3002", "conclusion": "ReGUIDE \u901a\u8fc7\u6570\u636e\u9ad8\u6548\u7684\u63a8\u7406\u548c\u7a7a\u95f4\u6279\u8bc4\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86 GUI \u5143\u7d20\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "keywords": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b, GUI \u5b9a\u4f4d, \u6570\u636e\u6548\u7387, \u5f3a\u5316\u5b66\u4e60, \u7a7a\u95f4\u5148\u9a8c, \u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55"}}
{"id": "2505.15386", "pdf": "https://arxiv.org/pdf/2505.15386", "abs": "https://arxiv.org/abs/2505.15386", "authors": ["Yiming Huang", "Junyan Zhang", "Zihao Wang", "Biquan Bie", "Xuming Hu", "Yi R.", "Fung", "Xinlei He"], "title": "RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have become powerful, but hallucinations remain\na vital obstacle to their trustworthy use. While previous works improved the\ncapability of hallucination detection by measuring uncertainty, they all lack\nthe ability to explain the provenance behind why hallucinations occur, i.e.,\nwhich part of the inputs tends to trigger hallucinations. Recent works on the\nprompt attack indicate that uncertainty exists in semantic propagation, where\nattention mechanisms gradually fuse local token information into high-level\nsemantics across layers. Meanwhile, uncertainty also emerges in language\ngeneration, due to its probability-based selection of high-level semantics for\nsampled generations. Based on that, we propose RePPL to recalibrate uncertainty\nmeasurement by these two aspects, which dispatches explainable uncertainty\nscores to each token and aggregates in Perplexity-style Log-Average form as\ntotal score. Experiments show that our method achieves the best comprehensive\ndetection performance across various QA datasets on advanced models (average\nAUC of 0.833), and our method is capable of producing token-level uncertainty\nscores as explanations for the hallucination. Leveraging these scores, we\npreliminarily find the chaotic pattern of hallucination and showcase its\npromising usage.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRePPL\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u4e2a\u7ef4\u5ea6\u91cd\u65b0\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u6d4b\u91cf\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u5e7b\u89c9\u68c0\u6d4b\u548c\u89e3\u91ca\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u89e3\u91ca\u5e7b\u89c9\u6765\u6e90\uff0c\u56e0\u6b64\u9700\u6539\u8fdb\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u8bed\u4e49\u4f20\u64ad\u548c\u8bed\u8a00\u751f\u6210\u7684\u4e0d\u786e\u5b9a\u6027\uff0cRePPL\u65b9\u6cd5\u4e3a\u6bcf\u4e2atoken\u5206\u914d\u53ef\u89e3\u91ca\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u6570\uff0c\u5e76\u4ee5Perplexity-style Log-Average\u5f62\u5f0f\u6c47\u603b\u3002", "result": "\u5728\u591a\u4e2aQA\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f73\u7efc\u5408\u68c0\u6d4b\u6027\u80fd\uff08\u5e73\u5747AUC 0.833\uff09\uff0c\u5e76\u80fd\u63d0\u4f9btoken\u7ea7\u7684\u89e3\u91ca\u5206\u6570\u3002", "conclusion": "RePPL\u4e0d\u4ec5\u63d0\u5347\u5e7b\u89c9\u68c0\u6d4b\u80fd\u529b\uff0c\u8fd8\u63ed\u793a\u4e86\u5e7b\u89c9\u7684\u6df7\u6c8c\u6a21\u5f0f\uff0c\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u5e7b\u89c9\u68c0\u6d4b,\u4e0d\u786e\u5b9a\u6027,\u8bed\u4e49\u4f20\u64ad,\u8bed\u8a00\u751f\u6210"}}
{"id": "2505.15270", "pdf": "https://arxiv.org/pdf/2505.15270", "abs": "https://arxiv.org/abs/2505.15270", "authors": ["Chenyu Zheng", "Xinyu Zhang", "Rongzhen Wang", "Wei Huang", "Zhi Tian", "Weilin Huang", "Jun Zhu", "Chongxuan Li"], "title": "Scaling Diffusion Transformers Efficiently via $\u03bc$P", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "35 pages, 10 figures, 15 tables", "summary": "Diffusion Transformers have emerged as the foundation for vision generative\nmodels, but their scalability is limited by the high cost of hyperparameter\n(HP) tuning at large scales. Recently, Maximal Update Parametrization ($\\mu$P)\nwas proposed for vanilla Transformers, which enables stable HP transfer from\nsmall to large language models, and dramatically reduces tuning costs. However,\nit remains unclear whether $\\mu$P of vanilla Transformers extends to diffusion\nTransformers, which differ architecturally and objectively. In this work, we\ngeneralize standard $\\mu$P to diffusion Transformers and validate its\neffectiveness through large-scale experiments. First, we rigorously prove that\n$\\mu$P of mainstream diffusion Transformers, including DiT, U-ViT,\nPixArt-$\\alpha$, and MMDiT, aligns with that of the vanilla Transformer,\nenabling the direct application of existing $\\mu$P methodologies. Leveraging\nthis result, we systematically demonstrate that DiT-$\\mu$P enjoys robust HP\ntransferability. Notably, DiT-XL-2-$\\mu$P with transferred learning rate\nachieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we\nvalidate the effectiveness of $\\mu$P on text-to-image generation by scaling\nPixArt-$\\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases,\nmodels under $\\mu$P outperform their respective baselines while requiring small\ntuning cost, only 5.5% of one training run for PixArt-$\\alpha$ and 3% of\nconsumption by human experts for MMDiT-18B. These results establish $\\mu$P as a\nprincipled and efficient framework for scaling diffusion Transformers.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u6700\u5927\u66f4\u65b0\u53c2\u6570\u5316\uff08\u03bcP\uff09\u65b9\u6cd5\u5e94\u7528\u4e8e\u6269\u6563Transformer\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5927\u89c4\u6a21\u6a21\u578b\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u6210\u672c\u3002", "motivation": "\u6269\u6563Transformer\u5728\u89c6\u89c9\u751f\u6210\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5927\u89c4\u6a21\u5e94\u7528\u53d7\u9650\u4e8e\u9ad8\u6210\u672c\u7684\u9ad8\u53c2\u6570\u8c03\u4f18\u3002\u03bcP\u65b9\u6cd5\u5728\u666e\u901aTransformer\u4e2d\u5df2\u8bc1\u660e\u6709\u6548\uff0c\u4f46\u5c1a\u672a\u5728\u6269\u6563Transformer\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "method": "\u5c06\u6807\u51c6\u03bcP\u65b9\u6cd5\u63a8\u5e7f\u5230\u6269\u6563Transformer\uff08\u5982DiT\u3001U-ViT\u3001PixArt-\u03b1\u548cMMDiT\uff09\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u03bcP\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u8bad\u7ec3\u6548\u7387\uff08\u5982DiT-XL-2-\u03bcP\u6536\u655b\u901f\u5ea6\u63d0\u53472.9\u500d\uff09\uff0c\u4e14\u5728\u5c0f\u89c4\u6a21\u8c03\u4f18\u6210\u672c\u4e0b\uff0cPixArt-\u03b1\u548cMMDiT\u7684\u6027\u80fd\u8d85\u8d8a\u57fa\u7ebf\u3002", "conclusion": "\u03bcP\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u6269\u6563Transformer\u7684\u8c03\u4f18\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "keywords": "\u6269\u6563Transformer, \u6700\u5927\u66f4\u65b0\u53c2\u6570\u5316, \u8d85\u53c2\u6570\u8c03\u4f18, \u89c6\u89c9\u751f\u6210\u6a21\u578b"}}
{"id": "2505.15033", "pdf": "https://arxiv.org/pdf/2505.15033", "abs": "https://arxiv.org/abs/2505.15033", "authors": ["Kehinde O. Aina", "Ram Avinery", "Hui-Shun Kuan", "Meredith D. Betterton", "Michael A. D. Goodisman", "Daniel I. Goldman"], "title": "Toward Task Capable Active Matter: Learning to Avoid Clogging in Confined Collectives via Collisions", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "13 pages, 9 figures. Published in Frontiers in Physics, Social\n  Physics section. Includes experimental and simulation analysis of multi-robot\n  excavation using decentralized learning", "summary": "Social organisms which construct nests consisting of tunnels and chambers\nnecessarily navigate confined and crowded conditions. Unlike low-density\ncollectives like bird flocks and insect swarms, in which hydrodynamic and\nstatistical phenomena dominate, the physics of glasses and supercooled fluids\nis important to understand clogging behaviors in high-density collectives. Our\nprevious work revealed that fire ants flowing in confined tunnels utilize\ndiverse behaviors like unequal workload distributions, spontaneous direction\nreversals, and limited interaction times to mitigate clogging and jamming and\nthus maintain functional flow; implementation of similar rules in a small\nrobophysical swarm led to high performance through spontaneous dissolution of\nclogs and clusters. However, how the insects learn such behaviors, and how we\ncan develop \"task capable\" active matter in such regimes, remains a challenge\nin part because interaction dynamics are dominated by local, time-consuming\ncollisions and no single agent can guide the entire collective. Here, we\nhypothesized that effective flow and clog mitigation could emerge purely\nthrough local learning. We tasked small groups of robots with pellet excavation\nin a narrow tunnel, allowing them to modify reversal probabilities over time.\nInitially, robots had equal probabilities and clogs were common. Reversals\nimproved flow. When reversal probabilities adapted via collisions and noisy\ntunnel length estimates, workload inequality and performance improved. Our\nrobophysical study of an excavating swarm shows that, despite the seeming\ncomplexity and difficulty of the task, simple learning rules can mitigate or\nleverage unavoidable features in task-capable dense active matter, leading to\nhypotheses for dense biological and robotic swarms.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u9ad8\u5bc6\u5ea6\u96c6\u4f53\u4e2d\u5982\u4f55\u901a\u8fc7\u5c40\u90e8\u5b66\u4e60\u89c4\u5219\u5b9e\u73b0\u6709\u6548\u6d41\u52a8\u548c\u5835\u585e\u7f13\u89e3\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b80\u5355\u7684\u5b66\u4e60\u89c4\u5219\u53ef\u4ee5\u6539\u5584\u6027\u80fd\u3002", "motivation": "\u7406\u89e3\u9ad8\u5bc6\u5ea6\u96c6\u4f53\uff08\u5982\u8682\u8681\u548c\u673a\u5668\u4eba\u7fa4\u4f53\uff09\u5982\u4f55\u901a\u8fc7\u5b66\u4e60\u884c\u4e3a\u7f13\u89e3\u5835\u585e\uff0c\u4ee5\u7ef4\u6301\u529f\u80fd\u6027\u6d41\u52a8\u3002", "method": "\u5728\u72ed\u7a84\u96a7\u9053\u4e2d\u8ba9\u673a\u5668\u4eba\u7fa4\u4f53\u8fdb\u884c\u9897\u7c92\u6316\u6398\uff0c\u5e76\u5141\u8bb8\u5b83\u4eec\u901a\u8fc7\u78b0\u649e\u548c\u566a\u58f0\u96a7\u9053\u957f\u5ea6\u4f30\u8ba1\u8c03\u6574\u9006\u8f6c\u6982\u7387\u3002", "result": "\u9006\u8f6c\u6982\u7387\u7684\u9002\u5e94\u6027\u8c03\u6574\u6539\u5584\u4e86\u6d41\u52a8\u6027\u548c\u5de5\u4f5c\u8d1f\u8377\u5206\u914d\uff0c\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "conclusion": "\u7b80\u5355\u7684\u5c40\u90e8\u5b66\u4e60\u89c4\u5219\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u9ad8\u5bc6\u5ea6\u6d3b\u8dc3\u7269\u8d28\u4e2d\u7684\u5835\u585e\u95ee\u9898\u3002", "keywords": "\u9ad8\u5bc6\u5ea6\u96c6\u4f53, \u5835\u585e\u7f13\u89e3, \u5c40\u90e8\u5b66\u4e60, \u673a\u5668\u4eba\u7fa4\u4f53, \u8682\u8681\u884c\u4e3a"}}
{"id": "2505.15389", "pdf": "https://arxiv.org/pdf/2505.15389", "abs": "https://arxiv.org/abs/2505.15389", "authors": ["DongGeon Lee", "Joonwon Jang", "Jihae Jeong", "Hwanjo Yu"], "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study", "categories": ["cs.CL", "cs.CR", "cs.CV"], "comment": null, "summary": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs show greater vulnerability to\nmeme-based harmful prompts than to synthetic or typographic images. Memes\nsignificantly increase harmful responses and decrease refusals compared to\ntext-only inputs. Though multi-turn interactions provide partial mitigation,\nelevated vulnerability persists. These results highlight the need for\necologically valid evaluations and stronger safety mechanisms.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u9762\u5bf9\u771f\u5b9e\u6a21\u56e0\uff08meme\uff09\u56fe\u50cf\u65f6\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u5176\u6bd4\u5408\u6210\u56fe\u50cf\u66f4\u6613\u4ea7\u751f\u6709\u5bb3\u8f93\u51fa\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u90e8\u7f72\uff0c\u5176\u5b89\u5168\u6027\u98ce\u9669\u65e5\u76ca\u51f8\u663e\u3002\u73b0\u6709\u8bc4\u4f30\u591a\u4f9d\u8d56\u4eba\u5de5\u56fe\u50cf\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u7528\u6237\u5206\u4eab\u7684\u6a21\u56e0\u56fe\u50cf\u5bf9\u6a21\u578b\u5b89\u5168\u6027\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u63d0\u51faMemeSafetyBench\uff0c\u5305\u542b50,430\u4e2a\u5b9e\u4f8b\uff0c\u7ed3\u5408\u771f\u5b9e\u6a21\u56e0\u56fe\u50cf\u4e0e\u6709\u5bb3/\u826f\u6027\u6307\u4ee4\uff0c\u57fa\u4e8e\u7efc\u5408\u5b89\u5168\u5206\u7c7b\u6cd5\u548cLLM\u751f\u6210\u7684\u6307\u4ee4\uff0c\u8bc4\u4f30\u591a\u4e2aVLMs\u5728\u5355\u8f6e\u548c\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\u3002", "result": "VLMs\u5bf9\u6a21\u56e0\u6709\u5bb3\u63d0\u793a\u7684\u8106\u5f31\u6027\u9ad8\u4e8e\u5408\u6210\u6216\u6392\u7248\u56fe\u50cf\uff1b\u591a\u8f6e\u4ea4\u4e92\u90e8\u5206\u7f13\u89e3\u95ee\u9898\uff0c\u4f46\u8106\u5f31\u6027\u4ecd\u5b58\u5728\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u5f00\u53d1\u66f4\u751f\u6001\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u66f4\u5f3a\u7684\u5b89\u5168\u673a\u5236\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b,\u6a21\u56e0,\u5b89\u5168\u6027\u8bc4\u4f30,\u591a\u8f6e\u4ea4\u4e92"}}
{"id": "2505.15284", "pdf": "https://arxiv.org/pdf/2505.15284", "abs": "https://arxiv.org/abs/2505.15284", "authors": ["Kun Fang", "Qinghua Tao", "Mingzhen He", "Kexin Lv", "Runze Yang", "Haibo Hu", "Xiaolin Huang", "Jie Yang", "Longbin Cao"], "title": "Kernel PCA for Out-of-Distribution Detection: Non-Linear Kernel Selections and Approximations", "categories": ["cs.LG", "cs.CV"], "comment": "This study is an extension of its conference version published in\n  NeurIPS'24, see\n  https://proceedings.neurips.cc/paper_files/paper/2024/hash/f2543511e5f4d4764857f9ad833a977d-Abstract-Conference.html", "summary": "Out-of-Distribution (OoD) detection is vital for the reliability of deep\nneural networks, the key of which lies in effectively characterizing the\ndisparities between OoD and In-Distribution (InD) data. In this work, such\ndisparities are exploited through a fresh perspective of non-linear feature\nsubspace. That is, a discriminative non-linear subspace is learned from InD\nfeatures to capture representative patterns of InD, while informative patterns\nof OoD features cannot be well captured in such a subspace due to their\ndifferent distribution. Grounded on this perspective, we exploit the deviations\nof InD and OoD features in such a non-linear subspace for effective OoD\ndetection. To be specific, we leverage the framework of Kernel Principal\nComponent Analysis (KPCA) to attain the discriminative non-linear subspace and\ndeploy the reconstruction error on such subspace to distinguish InD and OoD\ndata. Two challenges emerge: (i) the learning of an effective non-linear\nsubspace, i.e., the selection of kernel function in KPCA, and (ii) the\ncomputation of the kernel matrix with large-scale InD data. For the former, we\nreveal two vital non-linear patterns that closely relate to the InD-OoD\ndisparity, leading to the establishment of a Cosine-Gaussian kernel for\nconstructing the subspace. For the latter, we introduce two techniques to\napproximate the Cosine-Gaussian kernel with significantly cheap computations.\nIn particular, our approximation is further tailored by incorporating the InD\ndata confidence, which is demonstrated to promote the learning of\ndiscriminative subspaces for OoD data. Our study presents new insights into the\nnon-linear feature subspace for OoD detection and contributes practical\nexplorations on the associated kernel design and efficient computations,\nyielding a KPCA detection method with distinctively improved efficacy and\nefficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u7ebf\u6027\u7279\u5f81\u5b50\u7a7a\u95f4\u7684OoD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6838\u4e3b\u6210\u5206\u5206\u6790\uff08KPCA\uff09\u6784\u5efa\u5224\u522b\u6027\u5b50\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u91cd\u6784\u8bef\u5dee\u533a\u5206InD\u548cOoD\u6570\u636e\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u6838\u51fd\u6570\u9009\u62e9\u548c\u9ad8\u6548\u8ba1\u7b97\u7b49\u6311\u6218\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u975e\u7ebf\u6027\u7279\u5f81\u5b50\u7a7a\u95f4\u6709\u6548\u8868\u5f81InD\u548cOoD\u6570\u636e\u7684\u5dee\u5f02\uff0c\u63d0\u5347\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684OoD\u68c0\u6d4b\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528KPCA\u6846\u67b6\u5b66\u4e60\u5224\u522b\u6027\u975e\u7ebf\u6027\u5b50\u7a7a\u95f4\uff0c\u8bbe\u8ba1Cosine-Gaussian\u6838\u51fd\u6570\uff0c\u5e76\u5f15\u5165\u9ad8\u6548\u8ba1\u7b97\u6280\u672f\uff0c\u7ed3\u5408InD\u6570\u636e\u7684\u7f6e\u4fe1\u5ea6\u4f18\u5316\u5b50\u7a7a\u95f4\u5b66\u4e60\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684OoD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7684\u6548\u80fd\u548c\u6548\u7387\u3002", "conclusion": "\u975e\u7ebf\u6027\u7279\u5f81\u5b50\u7a7a\u95f4\u4e3aOoD\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u8bbe\u8ba1\u7684\u6838\u51fd\u6570\u548c\u9ad8\u6548\u8ba1\u7b97\u6280\u672f\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "keywords": "OoD\u68c0\u6d4b, \u6838\u4e3b\u6210\u5206\u5206\u6790, \u975e\u7ebf\u6027\u5b50\u7a7a\u95f4, Cosine-Gaussian\u6838, \u9ad8\u6548\u8ba1\u7b97"}}
{"id": "2505.15392", "pdf": "https://arxiv.org/pdf/2505.15392", "abs": "https://arxiv.org/abs/2505.15392", "authors": ["Yiming Huang", "Biquan Bie", "Zuqiu Na", "Weilin Ruan", "Songxin Lei", "Yutao Yue", "Xinlei He"], "title": "An Empirical Study of the Anchoring Effect in LLMs: Existence, Mechanism, and Potential Mitigations", "categories": ["cs.CL"], "comment": null, "summary": "The rise of Large Language Models (LLMs) like ChatGPT has advanced natural\nlanguage processing, yet concerns about cognitive biases are growing. In this\npaper, we investigate the anchoring effect, a cognitive bias where the mind\nrelies heavily on the first information as anchors to make affected judgments.\nWe explore whether LLMs are affected by anchoring, the underlying mechanisms,\nand potential mitigation strategies. To facilitate studies at scale on the\nanchoring effect, we introduce a new dataset, SynAnchors. Combining refined\nevaluation metrics, we benchmark current widely used LLMs. Our findings show\nthat LLMs' anchoring bias exists commonly with shallow-layer acting and is not\neliminated by conventional strategies, while reasoning can offer some\nmitigation. This recontextualization via cognitive psychology urges that LLM\nevaluations focus not on standard benchmarks or over-optimized robustness\ntests, but on cognitive-bias-aware trustworthy evaluation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u666e\u904d\u5b58\u5728\u951a\u5b9a\u6548\u5e94\uff0c\u5e38\u89c4\u7b56\u7565\u65e0\u6cd5\u6d88\u9664\uff0c\u63a8\u7406\u53ef\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7f13\u89e3\uff0c\u5f3a\u8c03\u8bc4\u4f30\u9700\u5173\u6ce8\u8ba4\u77e5\u504f\u5dee\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u8ba4\u77e5\u504f\u5dee\u95ee\u9898\u65e5\u76ca\u5f15\u53d1\u5173\u6ce8\uff0c\u672c\u6587\u65e8\u5728\u7814\u7a76LLMs\u4e2d\u7684\u951a\u5b9a\u6548\u5e94\u53ca\u5176\u89e3\u51b3\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u65b0\u6570\u636e\u96c6SynAnchors\u5e76\u7ed3\u5408\u6539\u8fdb\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5bf9\u4e3b\u6d41LLMs\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "LLMs\u666e\u904d\u5b58\u5728\u951a\u5b9a\u6548\u5e94\uff0c\u6d45\u5c42\u4f5c\u7528\u660e\u663e\uff0c\u5e38\u89c4\u7b56\u7565\u65e0\u6cd5\u6d88\u9664\uff0c\u63a8\u7406\u80fd\u90e8\u5206\u7f13\u89e3\u3002", "conclusion": "\u547c\u5401LLM\u8bc4\u4f30\u9700\u4ece\u6807\u51c6\u57fa\u51c6\u8f6c\u5411\u8ba4\u77e5\u504f\u5dee\u76f8\u5173\u7684\u53ef\u4fe1\u8bc4\u4f30\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u951a\u5b9a\u6548\u5e94\u3001\u8ba4\u77e5\u504f\u5dee\u3001\u8bc4\u4f30\u7b56\u7565\u3001SynAnchors"}}
{"id": "2505.15293", "pdf": "https://arxiv.org/pdf/2505.15293", "abs": "https://arxiv.org/abs/2505.15293", "authors": ["Qianyue Hao", "Yiwen Song", "Qingmin Liao", "Jian Yuan", "Yong Li"], "title": "LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Policy exploration is critical in reinforcement learning (RL), where existing\napproaches include greedy, Gaussian process, etc. However, these approaches\nutilize preset stochastic processes and are indiscriminately applied in all\nkinds of RL tasks without considering task-specific features that influence\npolicy exploration. Moreover, during RL training, the evolution of such\nstochastic processes is rigid, which typically only incorporates a decay in the\nvariance, failing to adjust flexibly according to the agent's real-time\nlearning status. Inspired by the analyzing and reasoning capability of large\nlanguage models (LLMs), we design LLM-Explorer to adaptively generate\ntask-specific exploration strategies with LLMs, enhancing the policy\nexploration in RL. In our design, we sample the learning trajectory of the\nagent during the RL training in a given task and prompt the LLM to analyze the\nagent's current policy learning status and then generate a probability\ndistribution for future policy exploration. Updating the probability\ndistribution periodically, we derive a stochastic process specialized for the\nparticular task and dynamically adjusted to adapt to the learning process. Our\ndesign is a plug-in module compatible with various widely applied RL\nalgorithms, including the DQN series, DDPG, TD3, and any possible variants\ndeveloped based on them. Through extensive experiments on the Atari and MuJoCo\nbenchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy\nexploration, achieving an average performance improvement up to 37.27%. Our\ncode is open-source at https://anonymous.4open.science/r/LLM-Explorer-19BE for\nreproducibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u9002\u5e94\u7b56\u7565\u63a2\u7d22\u65b9\u6cd5LLM-Explorer\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u7684\u63a2\u7d22\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709RL\u7b56\u7565\u63a2\u7d22\u65b9\u6cd5\uff08\u5982\u8d2a\u5a6a\u6216\u9ad8\u65af\u8fc7\u7a0b\uff09\u7f3a\u4e4f\u4efb\u52a1\u7279\u5b9a\u6027\u4e14\u52a8\u6001\u8c03\u6574\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002LLM\u7684\u5206\u6790\u548c\u63a8\u7406\u80fd\u529b\u4e3a\u81ea\u9002\u5e94\u63a2\u7d22\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u5229\u7528LLM\u5206\u6790\u4ee3\u7406\u5728RL\u8bad\u7ec3\u4e2d\u7684\u5b66\u4e60\u8f68\u8ff9\uff0c\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u7684\u6982\u7387\u5206\u5e03\uff0c\u52a8\u6001\u8c03\u6574\u63a2\u7d22\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u517c\u5bb9\u591a\u79cdRL\u7b97\u6cd5\uff08\u5982DQN\u3001DDPG\u3001TD3\u7b49\uff09\u3002", "result": "\u5728Atari\u548cMuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLM-Explorer\u5e73\u5747\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe37.27%\u3002", "conclusion": "LLM-Explorer\u901a\u8fc7\u52a8\u6001\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u63a2\u7d22\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86RL\u7684\u6548\u679c\uff0c\u4e14\u5177\u6709\u5e7f\u6cdb\u517c\u5bb9\u6027\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60,\u5927\u8bed\u8a00\u6a21\u578b,\u7b56\u7565\u63a2\u7d22,\u52a8\u6001\u8c03\u6574,\u4efb\u52a1\u7279\u5b9a\u6027"}}
{"id": "2505.15036", "pdf": "https://arxiv.org/pdf/2505.15036", "abs": "https://arxiv.org/abs/2505.15036", "authors": ["Kehinde O. Aina", "Hosain Bagheri", "Daniel I. Goldman"], "title": "Fault-Tolerant Multi-Robot Coordination with Limited Sensing within Confined Environments", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "15 pages, 4 figures. Accepted to DARS 2024 (Distributed Autonomous\n  Robotic Systems), to appear in Springer Proceedings in Advanced Robotics", "summary": "As robots are increasingly deployed to collaborate on tasks within shared\nworkspaces and resources, the failure of an individual robot can critically\naffect the group's performance. This issue is particularly challenging when\nrobots lack global information or direct communication, relying instead on\nsocial interaction for coordination and to complete their tasks. In this study,\nwe propose a novel fault-tolerance technique leveraging physical contact\ninteractions in multi-robot systems, specifically under conditions of limited\nsensing and spatial confinement. We introduce the \"Active Contact Response\"\n(ACR) method, where each robot modulates its behavior based on the likelihood\nof encountering an inoperative (faulty) robot. Active robots are capable of\ncollectively repositioning stationary and faulty peers to reduce obstructions\nand maintain optimal group functionality. We implement our algorithm in a team\nof autonomous robots, equipped with contact-sensing and collision-tolerance\ncapabilities, tasked with collectively excavating cohesive model pellets.\nExperimental results indicate that the ACR method significantly improves the\nsystem's recovery time from robot failures, enabling continued collective\nexcavation with minimal performance degradation. Thus, this work demonstrates\nthe potential of leveraging local, social, and physical interactions to enhance\nfault tolerance and coordination in multi-robot systems operating in\nconstrained and extreme environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u63a5\u89e6\u4ea4\u4e92\u7684\u6545\u969c\u5bb9\u5fcd\u6280\u672f\uff08ACR\u65b9\u6cd5\uff09\uff0c\u7528\u4e8e\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u6709\u9650\u611f\u77e5\u548c\u7a7a\u95f4\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u534f\u4f5c\u4efb\u52a1\uff0c\u663e\u8457\u7f29\u77ed\u4e86\u7cfb\u7edf\u4ece\u6545\u969c\u4e2d\u6062\u590d\u7684\u65f6\u95f4\u3002", "motivation": "\u673a\u5668\u4eba\u5728\u5171\u4eab\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u534f\u4f5c\u65f6\uff0c\u4e2a\u4f53\u6545\u969c\u53ef\u80fd\u5bfc\u81f4\u7fa4\u4f53\u6027\u80fd\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u5168\u5c40\u4fe1\u606f\u6216\u76f4\u63a5\u901a\u4fe1\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5f15\u5165\u4e86\u201c\u4e3b\u52a8\u63a5\u89e6\u54cd\u5e94\u201d\uff08ACR\uff09\u65b9\u6cd5\uff0c\u673a\u5668\u4eba\u901a\u8fc7\u8c03\u6574\u884c\u4e3a\u6765\u5e94\u5bf9\u53ef\u80fd\u9047\u5230\u7684\u6545\u969c\u673a\u5668\u4eba\uff0c\u4e3b\u52a8\u79fb\u52a8\u6545\u969c\u540c\u4f34\u4ee5\u51cf\u5c11\u969c\u788d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cACR\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u7cfb\u7edf\u4ece\u6545\u969c\u4e2d\u6062\u590d\u7684\u65f6\u95f4\uff0c\u4fdd\u6301\u4e86\u7fa4\u4f53\u6316\u6398\u4efb\u52a1\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5c40\u90e8\u3001\u793e\u4f1a\u548c\u7269\u7406\u4ea4\u4e92\u53ef\u4ee5\u63d0\u5347\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u53d7\u9650\u73af\u5883\u4e2d\u7684\u6545\u969c\u5bb9\u5fcd\u80fd\u529b\u548c\u534f\u8c03\u6548\u7387\u3002", "keywords": "\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff1b\u6545\u969c\u5bb9\u5fcd\uff1b\u7269\u7406\u63a5\u89e6\uff1bACR\uff1b\u53d7\u9650\u73af\u5883"}}
{"id": "2505.15404", "pdf": "https://arxiv.org/pdf/2505.15404", "abs": "https://arxiv.org/abs/2505.15404", "authors": ["Zhexin Zhang", "Xian Qi Loye", "Victor Shea-Jay Huang", "Junxiao Yang", "Qi Zhu", "Shiyao Cui", "Fei Mi", "Lifeng Shang", "Yingkang Wang", "Hongning Wang", "Minlie Huang"], "title": "How Should We Enhance the Safety of Large Reasoning Models: An Empirical Study", "categories": ["cs.CL"], "comment": "19 pages", "summary": "Large Reasoning Models (LRMs) have achieved remarkable success on\nreasoning-intensive tasks such as mathematics and programming. However, their\nenhanced reasoning capabilities do not necessarily translate to improved safety\nperformance-and in some cases, may even degrade it. This raises an important\nresearch question: how can we enhance the safety of LRMs? In this paper, we\npresent a comprehensive empirical study on how to enhance the safety of LRMs\nthrough Supervised Fine-Tuning (SFT). Our investigation begins with an\nunexpected observation: directly distilling safe responses from DeepSeek-R1\nfails to significantly enhance safety. We analyze this phenomenon and identify\nthree key failure patterns that contribute to it. We then demonstrate that\nexplicitly addressing these issues during the data distillation process can\nlead to substantial safety improvements. Next, we explore whether a long and\ncomplex reasoning process is necessary for achieving safety. Interestingly, we\nfind that simply using short or template-based reasoning process can attain\ncomparable safety performance-and are significantly easier for models to learn\nthan more intricate reasoning chains. These findings prompt a deeper reflection\non the role of reasoning in ensuring safety. Finally, we find that mixing math\nreasoning data during safety fine-tuning is helpful to balance safety and\nover-refusal. Overall, we hope our empirical study could provide a more\nholistic picture on enhancing the safety of LRMs. The code and data used in our\nexperiments are released in https://github.com/thu-coai/LRM-Safety-Study.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u7814\u7a76\u5982\u4f55\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u77ed\u6216\u6a21\u677f\u5316\u7684\u63a8\u7406\u8fc7\u7a0b\u53ef\u4ee5\u5b9e\u73b0\u7c7b\u4f3c\u5b89\u5168\u6027\u80fd\uff0c\u4e14\u6df7\u5408\u6570\u5b66\u63a8\u7406\u6570\u636e\u6709\u52a9\u4e8e\u5e73\u8861\u5b89\u5168\u6027\u548c\u8fc7\u5ea6\u62d2\u7edd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u5e76\u672a\u76f4\u63a5\u8f6c\u5316\u4e3a\u5b89\u5168\u6027\u80fd\u7684\u6539\u8fdb\uff0c\u751a\u81f3\u53ef\u80fd\u964d\u4f4e\u5b89\u5168\u6027\uff0c\u56e0\u6b64\u7814\u7a76\u5982\u4f55\u589e\u5f3aLRMs\u7684\u5b89\u5168\u6027\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u7814\u7a76\u5b89\u5168\u6027\u63d0\u5347\u65b9\u6cd5\uff0c\u5206\u6790\u6570\u636e\u84b8\u998f\u4e2d\u7684\u4e09\u79cd\u5173\u952e\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u63a2\u7d22\u77ed\u6216\u6a21\u677f\u5316\u63a8\u7406\u8fc7\u7a0b\u7684\u6548\u679c\u3002", "result": "\u53d1\u73b0\u663e\u5f0f\u89e3\u51b3\u6570\u636e\u84b8\u998f\u4e2d\u7684\u95ee\u9898\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\uff0c\u77ed\u6216\u6a21\u677f\u5316\u63a8\u7406\u8fc7\u7a0b\u80fd\u5b9e\u73b0\u7c7b\u4f3c\u5b89\u5168\u6027\u80fd\uff0c\u4e14\u6df7\u5408\u6570\u5b66\u63a8\u7406\u6570\u636e\u6709\u52a9\u4e8e\u5e73\u8861\u5b89\u5168\u6027\u548c\u8fc7\u5ea6\u62d2\u7edd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7b80\u5355\u63a8\u7406\u8fc7\u7a0b\u4e5f\u80fd\u6709\u6548\u63d0\u5347LRMs\u7684\u5b89\u5168\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89c6\u89d2\u3002", "keywords": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\uff0c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u5b89\u5168\u6027\uff0c\u6570\u636e\u84b8\u998f\uff0c\u6570\u5b66\u63a8\u7406"}}
{"id": "2505.15303", "pdf": "https://arxiv.org/pdf/2505.15303", "abs": "https://arxiv.org/abs/2505.15303", "authors": ["Johannes Kaiser", "Kristian Schwethelm", "Daniel Rueckert", "Georgios Kaissis"], "title": "Laplace Sample Information: Data Informativeness Through a Bayesian Lens", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Accurately estimating the informativeness of individual samples in a dataset\nis an important objective in deep learning, as it can guide sample selection,\nwhich can improve model efficiency and accuracy by removing redundant or\npotentially harmful samples. We propose Laplace Sample Information (LSI)\nmeasure of sample informativeness grounded in information theory widely\napplicable across model architectures and learning settings. LSI leverages a\nBayesian approximation to the weight posterior and the KL divergence to measure\nthe change in the parameter distribution induced by a sample of interest from\nthe dataset. We experimentally show that LSI is effective in ordering the data\nwith respect to typicality, detecting mislabeled samples, measuring class-wise\ninformativeness, and assessing dataset difficulty. We demonstrate these\ncapabilities of LSI on image and text data in supervised and unsupervised\nsettings. Moreover, we show that LSI can be computed efficiently through probes\nand transfers well to the training of large models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u7406\u8bba\u7684\u6837\u672c\u4fe1\u606f\u5ea6\u91cf\u65b9\u6cd5\uff08LSI\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u6570\u636e\u96c6\u6837\u672c\u7684\u4fe1\u606f\u91cf\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\u67b6\u6784\u548c\u5b66\u4e60\u573a\u666f\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\uff0c\u51c6\u786e\u8bc4\u4f30\u6570\u636e\u96c6\u6837\u672c\u7684\u4fe1\u606f\u91cf\u662f\u4e00\u4e2a\u91cd\u8981\u76ee\u6807\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u6307\u5bfc\u6837\u672c\u9009\u62e9\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faLaplace\u6837\u672c\u4fe1\u606f\uff08LSI\uff09\u5ea6\u91cf\uff0c\u5229\u7528\u8d1d\u53f6\u65af\u8fd1\u4f3c\u6743\u91cd\u540e\u9a8c\u548cKL\u6563\u5ea6\u6765\u8861\u91cf\u6837\u672c\u5bf9\u53c2\u6570\u5206\u5e03\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cLSI\u80fd\u6709\u6548\u6392\u5e8f\u6570\u636e\u3001\u68c0\u6d4b\u9519\u8bef\u6807\u7b7e\u3001\u6d4b\u91cf\u7c7b\u5185\u4fe1\u606f\u91cf\u53ca\u8bc4\u4f30\u6570\u636e\u96c6\u96be\u5ea6\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u548c\u6587\u672c\u6570\u636e\u3002", "conclusion": "LSI\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u6837\u672c\u4fe1\u606f\u5ea6\u91cf\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u3002", "keywords": "\u6837\u672c\u4fe1\u606f\u91cf\uff0c\u6df1\u5ea6\u5b66\u4e60\uff0cLaplace\u6837\u672c\u4fe1\u606f\uff0cKL\u6563\u5ea6\uff0c\u6570\u636e\u6548\u7387"}}
{"id": "2505.15422", "pdf": "https://arxiv.org/pdf/2505.15422", "abs": "https://arxiv.org/abs/2505.15422", "authors": ["Nudrat Habib", "Tosin Adewumi", "Marcus Liwicki", "Elisa Barney"], "title": "Trends and Challenges in Authorship Analysis: A Review of ML, DL, and LLM Approaches", "categories": ["cs.CL"], "comment": "25 pages, 3 figures", "summary": "Authorship analysis plays an important role in diverse domains, including\nforensic linguistics, academia, cybersecurity, and digital content\nauthentication. This paper presents a systematic literature review on two key\nsub-tasks of authorship analysis; Author Attribution and Author Verification.\nThe review explores SOTA methodologies, ranging from traditional ML approaches\nto DL models and LLMs, highlighting their evolution, strengths, and\nlimitations, based on studies conducted from 2015 to 2024. Key contributions\ninclude a comprehensive analysis of methods, techniques, their corresponding\nfeature extraction techniques, datasets used, and emerging challenges in\nauthorship analysis. The study highlights critical research gaps, particularly\nin low-resource language processing, multilingual adaptation, cross-domain\ngeneralization, and AI-generated text detection. This review aims to help\nresearchers by giving an overview of the latest trends and challenges in\nauthorship analysis. It also points out possible areas for future study. The\ngoal is to support the development of better, more reliable, and accurate\nauthorship analysis system in diverse textual domain.", "AI": {"tldr": "\u672c\u6587\u662f\u5bf9\u4f5c\u8005\u5206\u6790\u9886\u57df\u7684\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff0c\u805a\u7126\u4e8e\u4f5c\u8005\u5f52\u5c5e\u548c\u4f5c\u8005\u9a8c\u8bc1\u4e24\u4e2a\u5b50\u4efb\u52a1\uff0c\u603b\u7ed3\u4e862015\u81f32024\u5e74\u7684\u7814\u7a76\u65b9\u6cd5\u3001\u6280\u672f\u3001\u6570\u636e\u96c6\u53ca\u6311\u6218\u3002", "motivation": "\u4f5c\u8005\u5206\u6790\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982\u6cd5\u533b\u8bed\u8a00\u5b66\u3001\u7f51\u7edc\u5b89\u5168\uff09\u5177\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6700\u65b0\u65b9\u6cd5\u548c\u6280\u672f\u6311\u6218\u7684\u5168\u9762\u7efc\u8ff0\u3002", "method": "\u7efc\u8ff0\u5305\u62ec\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5206\u6790\u5176\u6f14\u8fdb\u3001\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "result": "\u603b\u7ed3\u4e86\u5173\u952e\u65b9\u6cd5\u3001\u7279\u5f81\u63d0\u53d6\u6280\u672f\u3001\u6570\u636e\u96c6\u548c\u65b0\u5174\u6311\u6218\uff0c\u5982\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u548c\u591a\u8bed\u8a00\u9002\u5e94\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5f53\u524d\u8d8b\u52bf\u548c\u6311\u6218\u7684\u6982\u8ff0\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u63a8\u52a8\u66f4\u53ef\u9760\u3001\u51c6\u786e\u7684\u4f5c\u8005\u5206\u6790\u7cfb\u7edf\u53d1\u5c55\u3002", "keywords": "\u4f5c\u8005\u5206\u6790, \u6587\u732e\u7efc\u8ff0, \u673a\u5668\u5b66\u4e60, \u6df1\u5ea6\u5b66\u4e60, \u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.15306", "pdf": "https://arxiv.org/pdf/2505.15306", "abs": "https://arxiv.org/abs/2505.15306", "authors": ["Yiwen Song", "Qianyue Hao", "Qingmin Liao", "Jian Yuan", "Yong Li"], "title": "Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Model ensemble is a useful approach in reinforcement learning (RL) for\ntraining effective agents. Despite wide success of RL, training effective\nagents remains difficult due to the multitude of factors requiring careful\ntuning, such as algorithm selection, hyperparameter settings, and even random\nseed choices, all of which can significantly influence an agent's performance.\nModel ensemble helps overcome this challenge by combining multiple weak agents\ninto a single, more powerful one, enhancing overall performance. However,\nexisting ensemble methods, such as majority voting and Boltzmann addition, are\ndesigned as fixed strategies and lack a semantic understanding of specific\ntasks, limiting their adaptability and effectiveness. To address this, we\npropose LLM-Ens, a novel approach that enhances RL model ensemble with\ntask-specific semantic understandings driven by large language models (LLMs).\nGiven a task, we first design an LLM to categorize states in this task into\ndistinct 'situations', incorporating high-level descriptions of the task\nconditions. Then, we statistically analyze the strengths and weaknesses of each\nindividual agent to be used in the ensemble in each situation. During the\ninference time, LLM-Ens dynamically identifies the changing task situation and\nswitches to the agent that performs best in the current situation, ensuring\ndynamic model selection in the evolving task condition. Our approach is\ndesigned to be compatible with agents trained with different random seeds,\nhyperparameter settings, and various RL algorithms. Extensive experiments on\nthe Atari benchmark show that LLM-Ens significantly improves the RL model\nensemble, surpassing well-known baselines by up to 20.9%. For reproducibility,\nour code is open-source at\nhttps://anonymous.4open.science/r/LLM4RLensemble-F7EE.", "AI": {"tldr": "LLM-Ens\u662f\u4e00\u79cd\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u589e\u5f3aRL\u6a21\u578b\u96c6\u6210\u7684\u65b0\u65b9\u6cd5\uff0c\u52a8\u6001\u9009\u62e9\u6700\u4f73\u4ee3\u7406\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfRL\u6a21\u578b\u96c6\u6210\u65b9\u6cd5\u7f3a\u4e4f\u4efb\u52a1\u8bed\u4e49\u7406\u89e3\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u548c\u6709\u6548\u6027\u3002", "method": "\u5229\u7528LLM\u5bf9\u4efb\u52a1\u72b6\u6001\u5206\u7c7b\uff0c\u5e76\u52a8\u6001\u9009\u62e9\u5728\u8be5\u72b6\u6001\u4e0b\u8868\u73b0\u6700\u4f73\u7684\u4ee3\u7406\u3002", "result": "\u5728Atari\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe20.9%\u3002", "conclusion": "LLM-Ens\u663e\u8457\u63d0\u5347\u4e86RL\u6a21\u578b\u96c6\u6210\u7684\u6027\u80fd\uff0c\u4e14\u517c\u5bb9\u4e0d\u540c\u8bad\u7ec3\u8bbe\u7f6e\u7684\u4ee3\u7406\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u6a21\u578b\u96c6\u6210, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u52a8\u6001\u9009\u62e9"}}
{"id": "2505.15039", "pdf": "https://arxiv.org/pdf/2505.15039", "abs": "https://arxiv.org/abs/2505.15039", "authors": ["Sicheol Sung", "Aditi", "Dogyu kim", "Yo-Sub Han", "Sang-Ki Ko"], "title": "LogiCase: Effective Test Case Generation from Logical Description in Competitive Programming", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Automated Test Case Generation (ATCG) is crucial for evaluating software\nreliability, particularly in competitive programming where robust algorithm\nassessments depend on diverse and accurate test cases. However, existing ATCG\nmethods often fail to meet complex specifications or generate effective corner\ncases, limiting their utility. In this work, we introduce Context-Free Grammars\nwith Counters (CCFGs), a formalism that captures both syntactic and semantic\nstructures in input specifications. Using a fine-tuned CodeT5 model, we\ntranslate natural language input specifications into CCFGs, enabling the\nsystematic generation of high-quality test cases. Experiments on the\nCodeContests dataset demonstrate that CCFG-based test cases outperform baseline\nmethods in identifying incorrect algorithms, achieving significant gains in\nvalidity and effectiveness. Our approach provides a scalable and reliable\ngrammar-driven framework for enhancing automated competitive programming\nevaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5\u4e0e\u8ba1\u6570\u5668\uff08CCFGs\uff09\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u7ade\u4e89\u7f16\u7a0b\u4e2d\u5bf9\u7b97\u6cd5\u53ef\u9760\u6027\u7684\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u590d\u6742\u7684\u8f93\u5165\u89c4\u8303\u6216\u751f\u6210\u6709\u6548\u7684\u8fb9\u754c\u60c5\u51b5\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u91c7\u7528\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5\u4e0e\u8ba1\u6570\u5668\uff08CCFGs\uff09\u6765\u6355\u83b7\u8f93\u5165\u89c4\u8303\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u7684CodeT5\u6a21\u578b\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u8f6c\u6362\u4e3aCCFGs\uff0c\u4ece\u800c\u7cfb\u7edf\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "\u5728CodeContests\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCCFG\u751f\u6210\u7684\u6d4b\u8bd5\u7528\u4f8b\u5728\u8bc6\u522b\u9519\u8bef\u7b97\u6cd5\u548c\u6709\u6548\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7ade\u4e89\u7f16\u7a0b\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u8bed\u6cd5\u9a71\u52a8\u6846\u67b6\u3002", "keywords": "\u81ea\u52a8\u5316\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210, \u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5, \u7ade\u4e89\u7f16\u7a0b, CodeT5\u6a21\u578b"}}
{"id": "2505.15424", "pdf": "https://arxiv.org/pdf/2505.15424", "abs": "https://arxiv.org/abs/2505.15424", "authors": ["Yan-Shuo Liang", "Wu-Jun Li"], "title": "Gated Integration of Low-Rank Adaptation for Continual Learning of Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Continual learning (CL), which requires the model to learn multiple tasks\nsequentially, is crucial for language models (LMs). Recently, low-rank\nadaptation (LoRA), one of the most representative parameter-efficient\nfine-tuning (PEFT) methods, has gained increasing attention in CL of LMs.\nHowever, most existing CL methods based on LoRA typically expand a new LoRA\nbranch to learn each new task and force the new and old LoRA branches to\ncontribute equally to old tasks, potentially leading to forgetting. In this\nwork, we propose a new method, called gated integration of low-rank adaptation\n(GainLoRA), for CL of LMs. GainLoRA expands a new LoRA branch for each new task\nand introduces gating modules to integrate the new and old LoRA branches.\nFurthermore, GainLoRA leverages the new gating module to minimize the\ncontribution from the new LoRA branch to old tasks, effectively mitigating\nforgetting and improving the model's overall performance. Experimental results\non CL benchmarks demonstrate that GainLoRA outperforms existing\nstate-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGainLoRA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u95e8\u63a7\u6a21\u5757\u6574\u5408\u65b0\u65e7LoRA\u5206\u652f\uff0c\u6709\u6548\u7f13\u89e3\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLoRA\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5728\u9002\u5e94\u65b0\u4efb\u52a1\u65f6\u901a\u5e38\u5f3a\u5236\u65b0\u65e7LoRA\u5206\u652f\u5e73\u7b49\u8d21\u732e\uff0c\u53ef\u80fd\u5bfc\u81f4\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u63d0\u51faGainLoRA\u65b9\u6cd5\uff0c\u4e3a\u6bcf\u4e2a\u65b0\u4efb\u52a1\u6269\u5c55LoRA\u5206\u652f\uff0c\u5e76\u5f15\u5165\u95e8\u63a7\u6a21\u5757\u6574\u5408\u65b0\u65e7\u5206\u652f\uff0c\u51cf\u5c11\u65b0\u5206\u652f\u5bf9\u65e7\u4efb\u52a1\u7684\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eGainLoRA\u5728\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GainLoRA\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u6709\u6548\u7f13\u89e3\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6574\u4f53\u6027\u80fd\u3002", "keywords": "\u6301\u7eed\u5b66\u4e60, LoRA, \u95e8\u63a7\u6a21\u5757, \u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.15311", "pdf": "https://arxiv.org/pdf/2505.15311", "abs": "https://arxiv.org/abs/2505.15311", "authors": ["Yurun Yuan", "Fan Chen", "Zeyu Jia", "Alexander Rakhlin", "Tengyang Xie"], "title": "Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Policy-based methods currently dominate reinforcement learning (RL) pipelines\nfor large language model (LLM) reasoning, leaving value-based approaches\nlargely unexplored. We revisit the classical paradigm of Bellman Residual\nMinimization and introduce Trajectory Bellman Residual Minimization (TBRM), an\nalgorithm that naturally adapts this idea to LLMs, yielding a simple yet\neffective off-policy algorithm that optimizes a single trajectory-level Bellman\nobjective using the model's own logits as $Q$-values. TBRM removes the need for\ncritics, importance-sampling ratios, or clipping, and operates with only one\nrollout per prompt. We prove convergence to the near-optimal KL-regularized\npolicy from arbitrary off-policy data via an improved\nchange-of-trajectory-measure analysis. Experiments on standard\nmathematical-reasoning benchmarks show that TBRM consistently outperforms\npolicy-based baselines, like PPO and GRPO, with comparable or lower\ncomputational and memory overhead. Our results indicate that value-based RL\nmight be a principled and efficient alternative for enhancing reasoning\ncapabilities in LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTBRM\u7684\u7b97\u6cd5\uff0c\u57fa\u4e8e\u8d1d\u5c14\u66fc\u6b8b\u5dee\u6700\u5c0f\u5316\u7684\u7ecf\u5178\u8303\u5f0f\uff0c\u9002\u5e94\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u5373\u53ef\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u7b56\u7565\u7684\u65b9\u6cd5\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u63a8\u7406\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u800c\u57fa\u4e8e\u4ef7\u503c\u7684\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7TBRM\u7b97\u6cd5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u8f68\u8ff9\u8d1d\u5c14\u66fc\u6b8b\u5dee\u6700\u5c0f\u5316\uff08TBRM\uff09\u7b97\u6cd5\uff0c\u5b83\u662f\u4e00\u79cd\u7b80\u5355\u7684\u79bb\u7b56\u7565\u7b97\u6cd5\uff0c\u5229\u7528\u6a21\u578b\u7684\u81ea\u8eablogits\u4f5c\u4e3aQ\u503c\uff0c\u4f18\u5316\u5355\u4e00\u7684\u8f68\u8ff9\u7ea7\u8d1d\u5c14\u66fc\u76ee\u6807\uff0c\u65e0\u9700\u8bc4\u8bba\u8005\u3001\u91cd\u8981\u6027\u91c7\u6837\u6bd4\u7387\u6216\u88c1\u526a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6807\u51c6\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTBRM\u6bd4\u57fa\u4e8e\u7b56\u7565\u7684\u57fa\u7ebf\uff08\u5982PPO\u548cGRPO\uff09\u8868\u73b0\u66f4\u597d\uff0c\u4e14\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u76f8\u5f53\u6216\u66f4\u4f4e\u3002", "conclusion": "\u57fa\u4e8e\u4ef7\u503c\u7684RL\u53ef\u80fd\u662f\u4e00\u79cd\u66f4\u539f\u5219\u6027\u548c\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u589e\u5f3aLLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u8d1d\u5c14\u66fc\u6b8b\u5dee,TBRM,\u63a8\u7406\u80fd\u529b"}}
{"id": "2505.15044", "pdf": "https://arxiv.org/pdf/2505.15044", "abs": "https://arxiv.org/abs/2505.15044", "authors": ["Ze Wang", "Jingang Qu", "Zhenyu Gao", "Pascal Morin"], "title": "Learning-based Airflow Inertial Odometry for MAVs using Thermal Anemometers in a GPS and vision denied environment", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "This work demonstrates an airflow inertial based odometry system with\nmulti-sensor data fusion, including thermal anemometer, IMU, ESC, and\nbarometer. This goal is challenging because low-cost IMUs and barometers have\nsignificant bias, and anemometer measurements are very susceptible to\ninterference from spinning propellers and ground effects. We employ a GRU-based\ndeep neural network to estimate relative air speed from noisy and disturbed\nanemometer measurements, and an observer with bias model to fuse the sensor\ndata and thus estimate the state of aerial vehicle. A complete flight data,\nincluding takeoff and landing on the ground, shows that the approach is able to\ndecouple the downwash induced wind speed caused by propellers and the ground\neffect, and accurately estimate the flight speed in a wind-free indoor\nenvironment. IMU, and barometer bias are effectively estimated, which\nsignificantly reduces the position integration drift, which is only 5.7m for\n203s manual random flight. The open source is available on\nhttps://github.com/SyRoCo-ISIR/Flight-Speed-Estimation-Airflow.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6c14\u6d41\u60ef\u6027\u7684\u591a\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u901a\u8fc7GRU\u795e\u7ecf\u7f51\u7edc\u548c\u89c2\u6d4b\u5668\u6a21\u578b\u6709\u6548\u89e3\u8026\u87ba\u65cb\u6868\u548c\u5730\u9762\u6548\u5e94\u5e26\u6765\u7684\u5e72\u6270\uff0c\u5e76\u5728\u65e0\u98ce\u5ba4\u5185\u73af\u5883\u4e2d\u5b9e\u73b0\u7cbe\u786e\u7684\u98de\u884c\u901f\u5ea6\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u4f4e\u6210\u672cIMU\u548c\u6c14\u538b\u8ba1\u504f\u5dee\u5927\u3001\u98ce\u901f\u8ba1\u6613\u53d7\u87ba\u65cb\u6868\u548c\u5730\u9762\u6548\u5e94\u5e72\u6270\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u65e0\u4eba\u673a\u98de\u884c\u72b6\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528GRU\u795e\u7ecf\u7f51\u7edc\u4ece\u53d7\u5e72\u6270\u7684\u98ce\u901f\u8ba1\u6570\u636e\u4e2d\u4f30\u8ba1\u76f8\u5bf9\u98ce\u901f\uff0c\u5e76\u7ed3\u5408\u89c2\u6d4b\u5668\u4e0e\u504f\u5dee\u6a21\u578b\u878d\u5408\u591a\u4f20\u611f\u5668\u6570\u636e\uff08\u70ed\u98ce\u901f\u8ba1\u3001IMU\u3001ESC\u3001\u6c14\u538b\u8ba1\uff09\u3002", "result": "\u5728\u65e0\u98ce\u5ba4\u5185\u73af\u5883\u4e2d\u6210\u529f\u89e3\u8026\u87ba\u65cb\u6868\u4e0b\u6d17\u6c14\u6d41\u548c\u5730\u9762\u6548\u5e94\uff0c\u98de\u884c\u901f\u5ea6\u4f30\u8ba1\u51c6\u786e\uff1bIMU\u548c\u6c14\u538b\u8ba1\u504f\u5dee\u4f30\u8ba1\u6709\u6548\uff0c203\u79d2\u624b\u52a8\u968f\u673a\u98de\u884c\u7684\u4f4d\u7f6e\u79ef\u5206\u6f02\u79fb\u4ec5\u4e3a5.7\u7c73\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u548c\u72b6\u6001\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f4d\u7f6e\u6f02\u79fb\uff0c\u9002\u5408\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "keywords": "\u591a\u4f20\u611f\u5668\u878d\u5408, \u6c14\u6d41\u60ef\u6027\u91cc\u7a0b\u8ba1, GRU\u795e\u7ecf\u7f51\u7edc, \u65e0\u4eba\u673a\u72b6\u6001\u4f30\u8ba1, \u504f\u5dee\u6a21\u578b"}}
{"id": "2505.15426", "pdf": "https://arxiv.org/pdf/2505.15426", "abs": "https://arxiv.org/abs/2505.15426", "authors": ["Aleksandra Tomaszewska", "Dariusz Czerski", "Bartosz \u017buk", "Maciej Ogrodniczuk"], "title": "NeoN: A Tool for Automated Detection, Linguistic and LLM-Driven Analysis of Neologisms in Polish", "categories": ["cs.CL"], "comment": "15 pages, this is an extended version of a paper accepted for the\n  25th International Conference on Computational Science (ICCS), 7-9 July 2025", "summary": "NeoN, a tool for detecting and analyzing Polish neologisms. Unlike\ntraditional dictionary-based methods requiring extensive manual review, NeoN\ncombines reference corpora, Polish-specific linguistic filters, an LLM-driven\nprecision-boosting filter, and daily RSS monitoring in a multi-layered\npipeline. The system uses context-aware lemmatization, frequency analysis, and\northographic normalization to extract candidate neologisms while consolidating\ninflectional variants. Researchers can verify candidates through an intuitive\ninterface with visualizations and filtering controls. An integrated LLM module\nautomatically generates definitions and categorizes neologisms by domain and\nsentiment. Evaluations show NeoN maintains high accuracy while significantly\nreducing manual effort, providing an accessible solution for tracking lexical\ninnovation in Polish.", "AI": {"tldr": "NeoN \u662f\u4e00\u79cd\u7528\u4e8e\u68c0\u6d4b\u548c\u5206\u6790\u6ce2\u5170\u8bed\u65b0\u8bcd\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6280\u672f\u548c\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u624b\u5de5\u5de5\u4f5c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u65b0\u8bcd\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u624b\u5de5\u5ba1\u67e5\uff0c\u6548\u7387\u4f4e\u4e0b\u3002NeoN \u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u6280\u672f\u4f18\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u7ed3\u5408\u53c2\u8003\u8bed\u6599\u5e93\u3001\u6ce2\u5170\u8bed\u7279\u5b9a\u8bed\u8a00\u8fc7\u6ee4\u5668\u3001\u57fa\u4e8eLLM\u7684\u7cbe\u5ea6\u63d0\u5347\u8fc7\u6ee4\u5668\u53ca\u6bcf\u65e5RSS\u76d1\u63a7\uff0c\u91c7\u7528\u591a\u5c42\u7ba1\u9053\u65b9\u6cd5\u3002", "result": "\u8bc4\u4f30\u663e\u793aNeoN\u5728\u51cf\u5c11\u624b\u5de5\u5de5\u4f5c\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\uff0c\u4e3a\u6ce2\u5170\u8bed\u8bcd\u6c47\u521b\u65b0\u8ffd\u8e2a\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "NeoN \u901a\u8fc7\u6280\u672f\u6574\u5408\u5b9e\u73b0\u4e86\u65b0\u8bcd\u68c0\u6d4b\u548c\u5206\u6790\u7684\u9ad8\u6548\u81ea\u52a8\u5316\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u76f4\u89c2\u7684\u53ef\u89c6\u5316\u5de5\u5177\u3002", "keywords": "NeoN, \u6ce2\u5170\u8bed\u65b0\u8bcd, \u81ea\u52a8\u5316\u68c0\u6d4b, LLM, \u8bed\u8a00\u5b66\u5de5\u5177"}}
{"id": "2505.15312", "pdf": "https://arxiv.org/pdf/2505.15312", "abs": "https://arxiv.org/abs/2505.15312", "authors": ["Yuxuan Shu", "Vasileios Lampos"], "title": "Sonnet: Spectral Operator Neural Network for Multivariable Time Series Forecasting", "categories": ["cs.LG"], "comment": "The code is available at https://github.com/ClaudiaShu/Sonnet", "summary": "Multivariable time series forecasting methods can integrate information from\nexogenous variables, leading to significant prediction accuracy gains.\nTransformer architecture has been widely applied in various time series\nforecasting models due to its ability to capture long-range sequential\ndependencies. However, a na\\\"ive application of transformers often struggles to\neffectively model complex relationships among variables over time. To mitigate\nagainst this, we propose a novel architecture, namely the Spectral Operator\nNeural Network (Sonnet). Sonnet applies learnable wavelet transformations to\nthe input and incorporates spectral analysis using the Koopman operator. Its\npredictive skill relies on the Multivariable Coherence Attention (MVCA), an\noperation that leverages spectral coherence to model variable dependencies. Our\nempirical analysis shows that Sonnet yields the best performance on $34$ out of\n$47$ forecasting tasks with an average mean absolute error (MAE) reduction of\n$1.1\\%$ against the most competitive baseline (different per task). We further\nshow that MVCA -- when put in place of the na\\\"ive attention used in various\ndeep learning models -- can remedy its deficiencies, reducing MAE by $10.7\\%$\non average in the most challenging forecasting tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSonnet\u7684\u65b0\u67b6\u6784\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5c0f\u6ce2\u53d8\u6362\u548cKoopman\u7b97\u5b50\u8fdb\u884c\u8c31\u5206\u6790\uff0c\u5229\u7528\u591a\u53d8\u91cf\u76f8\u5e72\u6ce8\u610f\u529b\uff08MVCA\uff09\u5efa\u6a21\u53d8\u91cf\u4f9d\u8d56\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684Transformer\u67b6\u6784\u5728\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u96be\u4ee5\u6709\u6548\u5efa\u6a21\u53d8\u91cf\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u6765\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "method": "Sonnet\u7ed3\u5408\u4e86\u53ef\u5b66\u4e60\u7684\u5c0f\u6ce2\u53d8\u6362\u548cKoopman\u7b97\u5b50\u7684\u8c31\u5206\u6790\uff0c\u5f15\u5165\u4e86\u591a\u53d8\u91cf\u76f8\u5e72\u6ce8\u610f\u529b\uff08MVCA\uff09\u6765\u5efa\u6a21\u53d8\u91cf\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u572847\u4e2a\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cSonnet\u572834\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u964d\u4f4e\u4e861.1%\u3002MVCA\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e2d\u5e73\u5747\u964d\u4f4eMAE 10.7%\u3002", "conclusion": "Sonnet\u901a\u8fc7\u8c31\u5206\u6790\u548cMVCA\u663e\u8457\u63d0\u5347\u4e86\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "keywords": "\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b, Transformer\u67b6\u6784, \u8c31\u5206\u6790, Koopman\u7b97\u5b50, \u591a\u53d8\u91cf\u76f8\u5e72\u6ce8\u610f\u529b"}}
{"id": "2505.15427", "pdf": "https://arxiv.org/pdf/2505.15427", "abs": "https://arxiv.org/abs/2505.15427", "authors": ["Zhiwen Li", "Die Chen", "Mingyuan Fan", "Cen Chen", "Yaliang Li", "Yanhao Wang", "Wenmeng Zhou"], "title": "Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The remarkable ability of diffusion models to generate high-fidelity images\nhas led to their widespread adoption. However, concerns have also arisen\nregarding their potential to produce Not Safe for Work (NSFW) content and\nexhibit social biases, hindering their practical use in real-world\napplications. In response to this challenge, prior work has focused on\nemploying security filters to identify and exclude toxic text, or\nalternatively, fine-tuning pre-trained diffusion models to erase sensitive\nconcepts. Unfortunately, existing methods struggle to achieve satisfactory\nperformance in the sense that they can have a significant impact on the normal\nmodel output while still failing to prevent the generation of harmful content\nin some cases. In this paper, we propose a novel self-discovery approach to\nidentifying a semantic direction vector in the embedding space to restrict text\nembedding within a safe region. Our method circumvents the need for correcting\nindividual words within the input text and steers the entire text prompt\ntowards a safe region in the embedding space, thereby enhancing model\nrobustness against all possibly unsafe prompts. In addition, we employ Low-Rank\nAdaptation (LoRA) for semantic direction vector initialization to reduce the\nimpact on the model performance for other semantics. Furthermore, our method\ncan also be integrated with existing methods to improve their social\nresponsibility. Extensive experiments on benchmark datasets demonstrate that\nour method can effectively reduce NSFW content and mitigate social bias\ngenerated by diffusion models compared to several state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u53d1\u73b0\u8bed\u4e49\u65b9\u5411\u5411\u91cf\u7ea6\u675f\u6587\u672c\u5d4c\u5165\u81f3\u5b89\u5168\u533a\u57df\u7684\u65b0\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u4e0d\u5b89\u5168\u5185\u5bb9\u548c\u793e\u4f1a\u504f\u89c1\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u4fdd\u771f\u56fe\u50cf\u7684\u80fd\u529b\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u5176\u53ef\u80fd\u4ea7\u751f\u4e0d\u5b89\u5168\u5185\u5bb9\u548c\u793e\u4f1a\u504f\u89c1\u7684\u95ee\u9898\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u53d1\u73b0\u8bed\u4e49\u65b9\u5411\u5411\u91cf\u4ee5\u7ea6\u675f\u6587\u672c\u5d4c\u5165\u81f3\u5b89\u5168\u533a\u57df\uff0c\u7ed3\u5408LoRA\u521d\u59cb\u5316\u4ee5\u51cf\u5c11\u5bf9\u5176\u4ed6\u8bed\u4e49\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u4e0d\u5b89\u5168\u5185\u5bb9\u548c\u793e\u4f1a\u504f\u89c1\u7684\u751f\u6210\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u8bed\u4e49\u65b9\u5411\u5411\u91cf\u548cLoRA\u6280\u672f\uff0c\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u7a33\u5065\u6027\u548c\u793e\u4f1a\u8d23\u4efb\u3002", "keywords": "\u6269\u6563\u6a21\u578b\u3001\u4e0d\u5b89\u5168\u5185\u5bb9\u3001\u793e\u4f1a\u504f\u89c1\u3001\u8bed\u4e49\u65b9\u5411\u5411\u91cf\u3001LoRA"}}
{"id": "2505.15329", "pdf": "https://arxiv.org/pdf/2505.15329", "abs": "https://arxiv.org/abs/2505.15329", "authors": ["Anqiao Ouyang", "Hongyi Ke", "Qi Wang"], "title": "Fourier-Invertible Neural Encoder (FINE) for Homogeneous Flows", "categories": ["cs.LG"], "comment": null, "summary": "Invertible neural architectures have recently attracted attention for their\ncompactness, interpretability, and information-preserving properties. In this\nwork, we propose the Fourier-Invertible Neural Encoder (FINE), which combines\ninvertible monotonic activation functions with reversible filter structures,\nand could be extended using Invertible ResNets. This architecture is examined\nin learning low-dimensional representations of one-dimensional nonlinear wave\ninteractions and exact circular translation symmetry. Dimensionality is\npreserved across layers, except for a Fourier truncation step in the latent\nspace, which enables dimensionality reduction while maintaining shift\nequivariance and interpretability. Our results demonstrate that FINE\nsignificantly outperforms classical linear methods such as Discrete Fourier\nTransformation (DFT) and Proper Orthogonal Decomposition (POD), and achieves\nreconstruction accuracy better than conventional deep autoencoders with\nconvolutional layers (CNN) - while using substantially smaller models and\noffering superior physical interpretability. These findings suggest that\ninvertible single-neuron networks, when combined with spectral truncation,\noffer a promising framework for learning compact and interpretable\nrepresentations of physics datasets, and symmetry-aware representation learning\nin physics-informed machine learning.", "AI": {"tldr": "FINE\u7ed3\u5408\u53ef\u9006\u5355\u8c03\u6fc0\u6d3b\u51fd\u6570\u548c\u53ef\u9006\u6ee4\u6ce2\u7ed3\u6784\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u622a\u65ad\u5b9e\u73b0\u964d\u7ef4\uff0c\u5728\u975e\u7ebf\u6027\u6ce2\u76f8\u4e92\u4f5c\u7528\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5c0f\u578b\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u53ef\u9006\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u7d27\u51d1\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u4fe1\u606f\u4fdd\u7559\u7279\u6027\uff0c\u4ee5\u63d0\u5347\u7269\u7406\u6570\u636e\u96c6\u7684\u8868\u793a\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u63d0\u51faFINE\u67b6\u6784\uff0c\u7ed3\u5408\u53ef\u9006\u5355\u8c03\u6fc0\u6d3b\u51fd\u6570\u548c\u53ef\u9006\u6ee4\u6ce2\u7ed3\u6784\uff0c\u652f\u6301\u53ef\u9006ResNets\u6269\u5c55\uff0c\u5e76\u5728\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u5085\u91cc\u53f6\u622a\u65ad\u3002", "result": "FINE\u5728\u964d\u7ef4\u3001\u91cd\u5efa\u7cbe\u5ea6\u548c\u7269\u7406\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8eDFT\u3001POD\u548cCNN\u3002", "conclusion": "FINE\u4e3a\u5b66\u4e60\u7269\u7406\u6570\u636e\u96c6\u7684\u7d27\u51d1\u8868\u793a\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6846\u67b6\u3002", "keywords": "\u53ef\u9006\u795e\u7ecf\u7f51\u7edc, \u5085\u91cc\u53f6\u622a\u65ad, \u7269\u7406\u6570\u636e\u96c6, \u5bf9\u79f0\u6027\u5b66\u4e60"}}
{"id": "2505.15428", "pdf": "https://arxiv.org/pdf/2505.15428", "abs": "https://arxiv.org/abs/2505.15428", "authors": ["Momose Oyama", "Ryo Kishino", "Hiroaki Yamagiwa", "Hidetoshi Shimodaira"], "title": "Likelihood Variance as Text Importance for Resampling Texts to Map Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We address the computational cost of constructing a model map, which embeds\ndiverse language models into a common space for comparison via KL divergence.\nThe map relies on log-likelihoods over a large text set, making the cost\nproportional to the number of texts. To reduce this cost, we propose a\nresampling method that selects important texts with weights proportional to the\nvariance of log-likelihoods across models for each text. Our method\nsignificantly reduces the number of required texts while preserving the\naccuracy of KL divergence estimates. Experiments show that it achieves\ncomparable performance to uniform sampling with about half as many texts, and\nalso facilitates efficient incorporation of new models into an existing map.\nThese results enable scalable and efficient construction of language model\nmaps.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91cd\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u91cd\u8981\u6027\u6587\u672c\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u4fdd\u6301KL\u6563\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u6784\u5efa\u6a21\u578b\u6620\u5c04\u65f6\u56e0\u4f9d\u8d56\u5927\u91cf\u6587\u672c\u8ba1\u7b97\u5bf9\u6570\u4f3c\u7136\u800c\u5bfc\u81f4\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u91cd\u91c7\u6837\u65b9\u6cd5\uff0c\u6839\u636e\u6bcf\u4e2a\u6587\u672c\u5728\u6a21\u578b\u95f4\u5bf9\u6570\u4f3c\u7136\u7684\u65b9\u5dee\u6743\u91cd\u9009\u62e9\u91cd\u8981\u6587\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u6570\u91cf\u51cf\u534a\u7684\u60c5\u51b5\u4e0b\u4ecd\u8fbe\u5230\u5747\u5300\u91c7\u6837\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u9ad8\u6548\u6574\u5408\u65b0\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8bed\u8a00\u6a21\u578b\u6620\u5c04\u7684\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u6784\u5efa\u3002", "keywords": "\u8bed\u8a00\u6a21\u578b, KL\u6563\u5ea6, \u91cd\u91c7\u6837, \u8ba1\u7b97\u6210\u672c"}}
{"id": "2505.15340", "pdf": "https://arxiv.org/pdf/2505.15340", "abs": "https://arxiv.org/abs/2505.15340", "authors": ["Yuanlin Chu", "Bo Wang", "Xiang Liu", "Hong Chen", "Aiwei Liu", "Xuming Hu"], "title": "SSR: Speculative Parallel Scaling Reasoning in Test-time", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) have achieved impressive results on multi-step\nmathematical reasoning, yet at the cost of high computational overhead. This\nchallenge is particularly acute for test-time scaling methods such as parallel\ndecoding, which increase answer diversity but scale poorly in efficiency. To\naddress this efficiency-accuracy trade-off, we propose SSR (Speculative\nParallel Scaling Reasoning), a training-free framework that leverages a key\ninsight: by introducing speculative decoding at the step level, we can\naccelerate reasoning without sacrificing correctness. SSR integrates two\ncomponents: a Selective Parallel Module (SPM) that identifies a small set of\npromising reasoning strategies via model-internal scoring, and Step-level\nSpeculative Decoding (SSD), which enables efficient draft-target collaboration\nfor fine-grained reasoning acceleration. Experiments on three mathematical\nbenchmarks-AIME 2024, MATH-500, and LiveMathBench - demonstrate that SSR\nachieves strong gains over baselines. For instance, on LiveMathBench, SSR\nimproves pass@1 accuracy by 13.84% while reducing computation to 80.5% of the\nbaseline FLOPs. On MATH-500, SSR reduces compute to only 30% with no loss in\naccuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSSR\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6b65\u9aa4\u7ea7\u63a8\u6d4b\u89e3\u7801\u548c\u9009\u62e9\u6027\u5e76\u884c\u6a21\u5757\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u5e73\u8861\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6b65\u6570\u5b66\u63a8\u7406\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u5e76\u884c\u89e3\u7801\u7b49\u65b9\u6cd5\u4e2d\u6548\u7387\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "SSR\u6846\u67b6\u5305\u542b\u9009\u62e9\u6027\u5e76\u884c\u6a21\u5757\uff08SPM\uff09\u548c\u6b65\u9aa4\u7ea7\u63a8\u6d4b\u89e3\u7801\uff08SSD\uff09\uff0c\u524d\u8005\u901a\u8fc7\u6a21\u578b\u5185\u90e8\u8bc4\u5206\u7b5b\u9009\u6709\u6f5c\u529b\u7684\u63a8\u7406\u7b56\u7565\uff0c\u540e\u8005\u5b9e\u73b0\u9ad8\u6548\u8349\u6848\u4e0e\u76ee\u6807\u7684\u534f\u4f5c\u52a0\u901f\u63a8\u7406\u3002", "result": "\u5728AIME 2024\u3001MATH-500\u548cLiveMathBench\u4e09\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSSR\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5982\u5728LiveMathBench\u4e0a\u63d0\u5347\u51c6\u786e\u738713.84%\uff0c\u540c\u65f6\u5728MATH-500\u4e0a\u51cf\u5c11\u8ba1\u7b97\u91cf\u81f330%\u4e14\u65e0\u7cbe\u5ea6\u635f\u5931\u3002", "conclusion": "SSR\u6210\u529f\u89e3\u51b3\u4e86\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u77db\u76fe\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u65b9\u6848\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u6570\u5b66\u63a8\u7406, \u63a8\u6d4b\u89e3\u7801, \u6548\u7387\u4f18\u5316"}}
{"id": "2505.15049", "pdf": "https://arxiv.org/pdf/2505.15049", "abs": "https://arxiv.org/abs/2505.15049", "authors": ["Kyungho Lee"], "title": "Towards a Working Definition of Designing Generative User Interfaces", "categories": ["cs.HC", "cs.AI", "H.5.2; D.2.2; H.1.2; I.3.6"], "comment": null, "summary": "Generative UI is transforming interface design by facilitating AI-driven\ncollaborative workflows between designers and computational systems. This study\nestablishes a working definition of Generative UI through a multi-method\nqualitative approach, integrating insights from a systematic literature review\nof 127 publications, expert interviews with 18 participants, and analyses of 12\ncase studies. Our findings identify five core themes that position Generative\nUI as an iterative and co-creative process. We highlight emerging design\nmodels, including hybrid creation, curation-based workflows, and AI-assisted\nrefinement strategies. Additionally, we examine ethical challenges, evaluation\ncriteria, and interaction models that shape the field. By proposing a\nconceptual foundation, this study advances both theoretical discourse and\npractical implementation, guiding future HCI research toward responsible and\neffective generative UI design practices.", "AI": {"tldr": "\u751f\u6210\u5f0fUI\u901a\u8fc7AI\u9a71\u52a8\u7684\u534f\u4f5c\u5de5\u4f5c\u6d41\u7a0b\u6539\u53d8\u754c\u9762\u8bbe\u8ba1\uff0c\u672c\u7814\u7a76\u901a\u8fc7\u591a\u79cd\u5b9a\u6027\u65b9\u6cd5\u5b9a\u4e49\u751f\u6210\u5f0fUI\uff0c\u5e76\u8bc6\u522b\u4e94\u4e2a\u6838\u5fc3\u4e3b\u9898\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0fUI\u7684\u5b9a\u4e49\u3001\u8bbe\u8ba1\u6a21\u578b\u53ca\u5176\u5728HCI\u9886\u57df\u7684\u5b9e\u8df5\u4e0e\u6311\u6218\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff08127\u7bc7\uff09\u3001\u4e13\u5bb6\u8bbf\u8c08\uff0818\u4eba\uff09\u548c\u6848\u4f8b\u5206\u6790\uff0812\u4e2a\uff09\u7684\u591a\u65b9\u6cd5\u5b9a\u6027\u7814\u7a76\u3002", "result": "\u8bc6\u522b\u51fa\u4e94\u4e2a\u6838\u5fc3\u4e3b\u9898\uff0c\u63d0\u51fa\u6df7\u5408\u521b\u4f5c\u3001\u57fa\u4e8e\u7b56\u5c55\u7684\u5de5\u4f5c\u6d41\u7a0b\u548cAI\u8f85\u52a9\u4f18\u5316\u7b56\u7565\u7b49\u65b0\u5174\u8bbe\u8ba1\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u751f\u6210\u5f0fUI\u7684\u7406\u8bba\u4e0e\u5b9e\u8df5\u63d0\u4f9b\u57fa\u7840\uff0c\u63a8\u52a8\u672a\u6765HCI\u7814\u7a76\u7684\u8d1f\u8d23\u4efb\u548c\u6709\u6548\u8bbe\u8ba1\u5b9e\u8df5\u3002", "keywords": "\u751f\u6210\u5f0fUI, \u534f\u4f5c\u5de5\u4f5c\u6d41\u7a0b, AI\u8bbe\u8ba1, HCI, \u4f26\u7406\u6311\u6218"}}
{"id": "2505.15431", "pdf": "https://arxiv.org/pdf/2505.15431", "abs": "https://arxiv.org/abs/2505.15431", "authors": ["Ao Liu", "Botong Zhou", "Can Xu", "Chayse Zhou", "ChenChen Zhang", "Chengcheng Xu", "Chenhao Wang", "Decheng Wu", "Dengpeng Wu", "Dian Jiao", "Dong Du", "Dong Wang", "Feng Zhang", "Fengzong Lian", "Guanghui Xu", "Guanwei Zhang", "Hai Wang", "Haipeng Luo", "Han Hu", "Huilin Xu", "Jiajia Wu", "Jianchen Zhu", "Jianfeng Yan", "Jiaqi Zhu", "Jihong Zhang", "Jinbao Xue", "Jun Xia", "Junqiang Zheng", "Kai Liu", "Kai Zhang", "Kai Zheng", "Kejiao Li", "Keyao Wang", "Lan Jiang", "Lixin Liu", "Lulu Wu", "Mengyuan Huang", "Peijie Yu", "Peiqi Wang", "Qian Wang", "Qianbiao Xiang", "Qibin Liu", "Qingfeng Sun", "Richard Guo", "Ruobing Xie", "Saiyong Yang", "Shaohua Chen", "Shihui Hu", "Shuai Li", "Shuaipeng Li", "Shuang Chen", "Suncong Zheng", "Tao Yang", "Tian Zhang", "Tinghao Yu", "Weidong Han", "Weijie Liu", "Weijin Zhou", "Weikang Wang", "Wesleye Chen", "Xiao Feng", "Xiaoqin Ren", "Xingwu Sun", "Xiong Kuang", "Xuemeng Huang", "Xun Cao", "Yanfeng Chen", "Yang Du", "Yang Zhen", "Yangyu Tao", "Yaping Deng", "Yi Shen", "Yigeng Hong", "Yiqi Chen", "Yiqing Huang", "Yuchi Deng", "Yue Mao", "Yulong Wang", "Yuyuan Zeng", "Zenan Xu", "Zhanhui Kang", "Zhe Zhao", "ZhenXiang Yan", "Zheng Fang", "Zhichao Hu", "Zhongzhi Chen", "Zhuoyu Li", "Zongwei Li", "Alex Yan", "Ande Liang", "Baitong Liu", "Beiping Pan", "Bin Xing", "Binghong Wu", "Bingxin Qu", "Bolin Ni", "Boyu Wu", "Chen Li", "Cheng Jiang", "Cheng Zhang", "Chengjun Liu", "Chengxu Yang", "Chiyu Wang", "Chong Zha", "Daisy Yi", "Di Wang", "Fanyang Lu", "Fei Chen", "Feifei Liu", "Feng Zheng", "Guanghua Yu", "Guiyang Li", "Guohua Wang", "Haisheng Lin", "Han Liu", "Han Wang", "Hao Fei", "Hao Lu", "Haoqing Jiang", "Haoran Sun", "Haotian Zhu", "Huangjin Dai", "Huankui Chen", "Huawen Feng", "Huihui Cai", "Huxin Peng", "Jackson Lv", "Jiacheng Shi", "Jiahao Bu", "Jianbo Li", "Jianglu Hu", "Jiangtao Guan", "Jianing Xu", "Jianwei Cai", "Jiarong Zhang", "Jiawei Song", "Jie Jiang", "Jie Liu", "Jieneng Yang", "Jihong Zhang", "Jin lv", "Jing Zhao", "Jinjian Li", "Jinxing Liu", "Jun Zhao", "Juntao Guo", "Kai Wang", "Kan Wu", "Lei Fu", "Lei He", "Lei Wang", "Li Liu", "Liang Dong", "Liya Zhan", "Long Cheng", "Long Xu", "Mao Zheng", "Meng Liu", "Mengkang Hu", "Nanli Chen", "Peirui Chen", "Peng He", "Pengju Pan", "Pengzhi Wei", "Qi Yang", "Qi Yi", "Roberts Wang", "Rongpeng Chen", "Rui Sun", "Rui Yang", "Ruibin Chen", "Ruixu Zhou", "Shaofeng Zhang", "Sheng Zhang", "Shihao Xu", "Shuaishuai Chang", "Shulin Liu", "SiQi Wang", "Songjia Feng", "Songling Yuan", "Tao Zhang", "Tianjiao Lang", "Tongkai Li", "Wei Deng", "Wei Li", "Weichao Wang", "Weigang Zhang", "Weixuan Sun", "Wen Ouyang", "Wenxiang Jiao", "Wenzhi Sun", "Wenzhuo Jia", "Xiang Zhang", "Xiangyu He", "Xianshun Ren", "XiaoYing Zhu", "Xiaolong Guo", "Xiaoxue Li", "Xiaoyu Ma", "Xican Lu", "Xinhua Feng", "Xinting Huang", "Xinyu Guan", "Xirui Li", "Xu Zhang", "Xudong Gao", "Xun Luo", "Xuxiang Qi", "Yangkun Chen", "Yangyu Tao", "Yanling Xiao", "Yantao Mai", "Yanze Chen", "Yao Ding", "Yeting Yang", "YiFan Song", "Yifan Yang", "Yijiao Zhu", "Yinhe Wu", "Yixian Liu", "Yong Yang", "Yuanjun Cai", "Yuanlin Tu", "Yue Zhang", "Yufei Huang", "Yuhang Zhou", "Yuhao Jiang", "Yuhong Liu", "Yuhui Hu", "Yujin Lin", "Yun Yang", "Yunhao Wang", "Yusong Zhang", "Zekun Wu", "Zelong Zhang", "Zhan Yu", "Zhaoliang Yang", "Zhe Zhao", "Zheng Li", "Zhenyu Huang", "Zhiguang Liu", "Zhijiang Xu", "Zhiqing Kui", "Zhiyin Zeng", "Zhiyuan Xiong", "Zhuo Han", "Zifan Wu", "Zigang Geng", "Zilong Zhao", "Ziyan Tang", "Ziyuan Zhu", "Zonglei Zhu", "Zhijiang Xu"], "title": "Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.", "AI": {"tldr": "Hunyuan-TurboS\u662f\u4e00\u6b3e\u65b0\u578b\u5927\u578b\u6df7\u5408Transformer-Mamba MoE\u6a21\u578b\uff0c\u7ed3\u5408Mamba\u7684\u957f\u5e8f\u5217\u5904\u7406\u6548\u7387\u548cTransformer\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u957f\u77ed\u671f\u601d\u7ef4\u94fe\u673a\u5236\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5e8f\u5217\u5904\u7406\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u7684\u6311\u6218\uff0c\u7ed3\u5408Mamba\u548cTransformer\u7684\u4f18\u52bf\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u91c7\u752856B\u6fc0\u6d3b\u53c2\u6570\uff08\u603b560B\uff09\u7684\u6df7\u5408\u67b6\u6784\uff08Mamba2\u3001Attention\u3001FFN\uff09\uff0c\u9884\u8bad\u7ec316T\u9ad8\u8d28\u91cftoken\uff0c\u652f\u6301256K\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u957f\u77ed\u671f\u601d\u7ef4\u94fe\u548c\u591a\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728LMSYS Chatbot Arena\u4e2d\u6392\u540d\u524d7\uff0c\u5f97\u52061356\uff0c\u8d85\u8d8a\u591a\u4e2a\u9886\u5148\u6a21\u578b\uff0c23\u4e2a\u81ea\u52a8\u5316\u57fa\u51c6\u6d4b\u8bd5\u5e73\u5747\u5f97\u520677.9%\u3002", "conclusion": "Hunyuan-TurboS\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5b9e\u73b0\u5e73\u8861\uff0c\u4e3a\u9ad8\u6548\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u6811\u7acb\u4e86\u65b0\u8303\u5f0f\u3002", "keywords": "LLM, Transformer, Mamba, MoE, \u601d\u7ef4\u94fe, \u5f3a\u5316\u5b66\u4e60"}}
{"id": "2505.15345", "pdf": "https://arxiv.org/pdf/2505.15345", "abs": "https://arxiv.org/abs/2505.15345", "authors": ["Jacob E. Kooi", "Zhao Yang", "Vincent Fran\u00e7ois-Lavet"], "title": "Hadamax Encoding: Elevating Performance in Model-Free Atari", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Neural network architectures have a large impact in machine learning. In\nreinforcement learning, network architectures have remained notably simple, as\nchanges often lead to small gains in performance. This work introduces a novel\nencoder architecture for pixel-based model-free reinforcement learning. The\nHadamax (\\textbf{Hada}mard \\textbf{max}-pooling) encoder achieves\nstate-of-the-art performance by max-pooling Hadamard products between\nGELU-activated parallel hidden layers. Based on the recent PQN algorithm, the\nHadamax encoder achieves state-of-the-art model-free performance in the\nAtari-57 benchmark. Specifically, without applying any algorithmic\nhyperparameter modifications, Hadamax-PQN achieves an 80\\% performance gain\nover vanilla PQN and significantly surpasses Rainbow-DQN. For reproducibility,\nthe full code is available on\n\\href{https://github.com/Jacobkooi/Hadamax}{GitHub}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHadamax\u7684\u65b0\u578b\u7f16\u7801\u5668\u67b6\u6784\uff0c\u7528\u4e8e\u57fa\u4e8e\u50cf\u7d20\u7684\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u5728GELU\u6fc0\u6d3b\u7684\u5e76\u884c\u9690\u85cf\u5c42\u4e4b\u95f4\u8fdb\u884cHadamard\u4e58\u79ef\u7684\u6700\u5927\u6c60\u5316\uff0c\u5b9e\u73b0\u4e86Atari-57\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u5f71\u54cd\u6df1\u8fdc\uff0c\u4f46\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u67b6\u6784\u8bbe\u8ba1\u901a\u5e38\u8f83\u4e3a\u7b80\u5355\uff0c\u6539\u8fdb\u5e26\u6765\u7684\u6027\u80fd\u63d0\u5347\u6709\u9650\u3002\u56e0\u6b64\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u7f16\u7801\u5668\u67b6\u6784\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u79cd\u79f0\u4e3aHadamax\u7684\u7f16\u7801\u5668\u67b6\u6784\uff0c\u7ed3\u5408Hadamard\u4e58\u79ef\u548c\u6700\u5927\u6c60\u5316\u6280\u672f\uff0c\u57fa\u4e8eGELU\u6fc0\u6d3b\u7684\u5e76\u884c\u9690\u85cf\u5c42\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8ePQN\u7b97\u6cd5\u4e2d\u3002", "result": "\u5728\u4e0d\u4fee\u6539\u7b97\u6cd5\u8d85\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0cHadamax-PQN\u6bd4\u539f\u59cbPQN\u6027\u80fd\u63d0\u5347\u4e8680%\uff0c\u5e76\u663e\u8457\u8d85\u8d8a\u4e86Rainbow-DQN\u3002", "conclusion": "Hadamax\u7f16\u7801\u5668\u5728\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u7684\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60,\u7f16\u7801\u5668\u67b6\u6784,Hadamard\u4e58\u79ef,\u6700\u5927\u6c60\u5316,Atari-57"}}
{"id": "2505.15442", "pdf": "https://arxiv.org/pdf/2505.15442", "abs": "https://arxiv.org/abs/2505.15442", "authors": ["Suhas Kamasetty Ramesh", "Ayan Sengupta", "Tanmoy Chakraborty"], "title": "On the Generalization vs Fidelity Paradox in Knowledge Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge distillation (KD) is a key technique for compressing large language\nmodels into smaller ones while preserving performance. Despite the recent\ntraction of KD research, its effectiveness for smaller language models (LMs)\nand the mechanisms driving knowledge transfer remain underexplored. In this\nwork, we present the first large-scale empirical and statistical analysis of KD\nacross models ranging from 0.5B to 7B parameters on 14 complex reasoning tasks\nin a zero-shot setting. Our findings reveal that KD can improve the average\nperformance of smaller models by up to $10\\%$, with a peak task specific gain\nof $22\\%$, while providing only marginal benefits ($\\sim 1.3\\%$) for larger\nmodels. Surprisingly, teacher performance has a minimal impact on student\noutcomes, while teacher task expertise impacts KD effectiveness. A correlation\nstudy indicates that smaller LMs benefit more from KD, whereas larger LMs show\ndiminished gains. Additionally, we uncover a misalignment between improvements\nin student performance and reasoning fidelity, suggesting that while KD\nenhances accuracy, it does not always maintain the structured decision-making\nprocesses of the teacher. Our ablation study further highlights the importance\nof teacher signals and logit smoothing in influencing students' performance\nafter distillation. Overall, our study offers a comprehensive empirical and\nstatistical assessment of KD, highlighting both its benefits and trade-offs\nwhen distilling knowledge from larger to smaller LMs.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u7814\u7a76\u4e86\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u5728\u4e0d\u540c\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff080.5B\u81f37B\u53c2\u6570\uff09\u4e2d\u7684\u6548\u679c\u53ca\u5176\u673a\u5236\u3002\u7ed3\u679c\u663e\u793a\uff0cKD\u5bf9\u5c0f\u6a21\u578b\u63d0\u5347\u663e\u8457\uff08\u5e73\u574710%\uff0c\u5cf0\u503c22%\uff09\uff0c\u800c\u5bf9\u5927\u6a21\u578b\u6548\u679c\u6709\u9650\uff08\u7ea61.3%\uff09\uff0c\u4e14\u6559\u5e08\u6a21\u578b\u6027\u80fd\u5bf9\u7ed3\u679c\u5f71\u54cd\u8f83\u5c0f\uff0c\u4efb\u52a1\u4e13\u4e1a\u6027\u66f4\u4e3a\u5173\u952e\u3002\u6b64\u5916\uff0cKD\u867d\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u53ef\u80fd\u635f\u5bb3\u63a8\u7406\u4fdd\u771f\u5ea6\u3002", "motivation": "\u63a2\u7d22\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u5728\u5c0f\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\u53ca\u5176\u80cc\u540e\u7684\u673a\u5236\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u572814\u4e2a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u8bbe\u7f6e\u7684\u5927\u89c4\u6a21\u5b9e\u8bc1\u548c\u7edf\u8ba1\u5206\u6790\uff0c\u6db5\u76d60.5B\u81f37B\u53c2\u6570\u7684\u6a21\u578b\uff0c\u7814\u7a76KD\u5bf9\u5b66\u751f\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "KD\u663e\u8457\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd\uff08\u5e73\u574710%\uff0c\u5cf0\u503c22%\uff09\uff0c\u4f46\u5bf9\u5927\u6a21\u578b\u6548\u679c\u5fae\u5f31\u3002\u6559\u5e08\u4efb\u52a1\u4e13\u4e1a\u6027\u6bd4\u6574\u4f53\u6027\u80fd\u66f4\u5173\u952e\uff0c\u4e14KD\u53ef\u80fd\u635f\u5bb3\u63a8\u7406\u4fdd\u771f\u5ea6\u3002", "conclusion": "KD\u5bf9\u5c0f\u6a21\u578b\u6548\u679c\u663e\u8457\uff0c\u4f46\u9700\u6743\u8861\u51c6\u786e\u6027\u4e0e\u63a8\u7406\u4fdd\u771f\u5ea6\uff1b\u6559\u5e08\u4efb\u52a1\u4e13\u4e1a\u6027\u548c\u4fe1\u53f7\u8bbe\u8ba1\u662f\u5173\u952e\u56e0\u7d20\u3002", "keywords": "\u77e5\u8bc6\u84b8\u998f, \u8bed\u8a00\u6a21\u578b, \u5c0f\u6a21\u578b, \u4efb\u52a1\u4e13\u4e1a\u6027, \u63a8\u7406\u4fdd\u771f\u5ea6"}}
{"id": "2505.15354", "pdf": "https://arxiv.org/pdf/2505.15354", "abs": "https://arxiv.org/abs/2505.15354", "authors": ["Malik Tiomoko", "Hamza Cherkaoui", "Giuseppe Paolo", "Zhang Yili", "Yu Meng", "Zhang Keli", "Hafiz Tiomoko Ali"], "title": "Human in the Loop Adaptive Optimization for Improved Time Series Forecasting", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Time series forecasting models often produce systematic, predictable errors\neven in critical domains such as energy, finance, and healthcare. We introduce\na novel post training adaptive optimization framework that improves forecast\naccuracy without retraining or architectural changes. Our method automatically\napplies expressive transformations optimized via reinforcement learning,\ncontextual bandits, or genetic algorithms to correct model outputs in a\nlightweight and model agnostic way. Theoretically, we prove that affine\ncorrections always reduce the mean squared error; practically, we extend this\nidea with dynamic action based optimization. The framework also supports an\noptional human in the loop component: domain experts can guide corrections\nusing natural language, which is parsed into actions by a language model.\nAcross multiple benchmarks (e.g., electricity, weather, traffic), we observe\nconsistent accuracy gains with minimal computational overhead. Our interactive\ndemo shows the framework's real time usability. By combining automated post hoc\nrefinement with interpretable and extensible mechanisms, our approach offers a\npowerful new direction for practical forecasting systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u540e\u8bad\u7ec3\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\u81ea\u52a8\u6821\u6b63\u9884\u6d4b\u8bef\u5dee\uff0c\u652f\u6301\u4e13\u5bb6\u53c2\u4e0e\uff0c\u663e\u8457\u63d0\u5347\u591a\u4e2a\u9886\u57df\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u5728\u5173\u952e\u9886\u57df\u5e38\u4ea7\u751f\u7cfb\u7edf\u6027\u8bef\u5dee\uff0c\u4e9f\u9700\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u67b6\u6784\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u3001\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u6216\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u8868\u8fbe\u6027\u53d8\u6362\uff0c\u52a8\u6001\u6821\u6b63\u6a21\u578b\u8f93\u51fa\uff0c\u652f\u6301\u4e13\u5bb6\u81ea\u7136\u8bed\u8a00\u5f15\u5bfc\u3002", "result": "\u5728\u7535\u529b\u3001\u5929\u6c14\u3001\u4ea4\u901a\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6846\u67b6\u4ee5\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u6301\u7eed\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u7ed3\u5408\u81ea\u52a8\u5316\u6821\u6b63\u4e0e\u53ef\u89e3\u91ca\u673a\u5236\uff0c\u4e3a\u5b9e\u7528\u9884\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b,\u540e\u8bad\u7ec3\u4f18\u5316,\u5f3a\u5316\u5b66\u4e60,\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a,\u9057\u4f20\u7b97\u6cd5"}}
{"id": "2505.15058", "pdf": "https://arxiv.org/pdf/2505.15058", "abs": "https://arxiv.org/abs/2505.15058", "authors": ["Tianbao Zhang", "Jian Zhao", "Yuer Li", "Zheng Zhu", "Ping Hu", "Zhaoxin Fan", "Wenjun Wu", "Xuelong Li"], "title": "AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS", "68T10"], "comment": "11pages, conference", "summary": "Whole-body audio-driven avatar pose and expression generation is a critical\ntask for creating lifelike digital humans and enhancing the capabilities of\ninteractive virtual agents, with wide-ranging applications in virtual reality,\ndigital entertainment, and remote communication. Existing approaches often\ngenerate audio-driven facial expressions and gestures independently, which\nintroduces a significant limitation: the lack of seamless coordination between\nfacial and gestural elements, resulting in less natural and cohesive\nanimations. To address this limitation, we propose AsynFusion, a novel\nframework that leverages diffusion transformers to achieve harmonious\nexpression and gesture synthesis. The proposed method is built upon a\ndual-branch DiT architecture, which enables the parallel generation of facial\nexpressions and gestures. Within the model, we introduce a Cooperative\nSynchronization Module to facilitate bidirectional feature interaction between\nthe two modalities, and an Asynchronous LCM Sampling strategy to reduce\ncomputational overhead while maintaining high-quality outputs. Extensive\nexperiments demonstrate that AsynFusion achieves state-of-the-art performance\nin generating real-time, synchronized whole-body animations, consistently\noutperforming existing methods in both quantitative and qualitative\nevaluations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAsynFusion\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u53d8\u538b\u5668\u5b9e\u73b0\u9762\u90e8\u8868\u60c5\u548c\u624b\u52bf\u7684\u548c\u8c10\u5408\u6210\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u9762\u90e8\u548c\u624b\u52bf\u7f3a\u4e4f\u534f\u8c03\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7684\u97f3\u9891\u9a71\u52a8\u65b9\u6cd5\u901a\u5e38\u72ec\u7acb\u751f\u6210\u9762\u90e8\u8868\u60c5\u548c\u624b\u52bf\uff0c\u7f3a\u4e4f\u534f\u8c03\u6027\uff0c\u5bfc\u81f4\u52a8\u753b\u6548\u679c\u4e0d\u81ea\u7136\u3002", "method": "\u57fa\u4e8e\u53cc\u5206\u652fDiT\u67b6\u6784\uff0c\u5f15\u5165\u534f\u4f5c\u540c\u6b65\u6a21\u5757\u548c\u5f02\u6b65LCM\u91c7\u6837\u7b56\u7565\uff0c\u5b9e\u73b0\u5e76\u884c\u751f\u6210\u8868\u60c5\u548c\u624b\u52bf\u3002", "result": "AsynFusion\u5728\u5b9e\u65f6\u3001\u540c\u6b65\u7684\u5168\u8eab\u4f53\u52a8\u753b\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u5747\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AsynFusion\u901a\u8fc7\u534f\u8c03\u9762\u90e8\u548c\u624b\u52bf\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u753b\u7684\u81ea\u7136\u6027\u548c\u4e00\u81f4\u6027\u3002", "keywords": "\u97f3\u9891\u9a71\u52a8, \u6570\u5b57\u4eba, \u865a\u62df\u73b0\u5b9e, \u6269\u6563\u53d8\u538b\u5668, \u52a8\u753b\u751f\u6210"}}
{"id": "2505.15443", "pdf": "https://arxiv.org/pdf/2505.15443", "abs": "https://arxiv.org/abs/2505.15443", "authors": ["Artem Zabolotnyi", "Roman Makarov", "Mile Mitrovic", "Polina Proskura", "Oleg Travkin", "Roman Alferov", "Alexey Zaytsev"], "title": "AdUE: Improving uncertainty estimation head for LoRA adapters in LLMs", "categories": ["cs.CL", "stat.ML"], "comment": "9 pages, 1 figure", "summary": "Uncertainty estimation remains a critical challenge in adapting pre-trained\nlanguage models to classification tasks, particularly under parameter-efficient\nfine-tuning approaches such as adapters. We introduce AdUE1, an efficient\npost-hoc uncertainty estimation (UE) method, to enhance softmax-based\nestimates. Our approach (1) uses a differentiable approximation of the maximum\nfunction and (2) applies additional regularization through L2-SP, anchoring the\nfine-tuned head weights and regularizing the model. Evaluations on five NLP\nclassification datasets across four language models (RoBERTa, ELECTRA, LLaMA-2,\nQwen) demonstrate that our method consistently outperforms established\nbaselines such as Mahalanobis distance and softmax response. Our approach is\nlightweight (no base-model changes) and produces better-calibrated confidence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AdUE1\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4e0b\u6539\u5584\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5206\u7c7b\u4efb\u52a1\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u901a\u8fc7\u6700\u5927\u51fd\u6570\u7684\u53ef\u5fae\u8fd1\u4f3c\u548cL2-SP\u6b63\u5219\u5316\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4ecd\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08\u5982\u9002\u914d\u5668\uff09\u4e0b\u3002\u7814\u7a76\u65e8\u5728\u63d0\u5347softmax\u57fa\u4f30\u8ba1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u6700\u5927\u51fd\u6570\u7684\u53ef\u5fae\u8fd1\u4f3c\u548cL2-SP\u6b63\u5219\u5316\uff0c\u5fae\u8c03\u5934\u6743\u91cd\u5e76\u6b63\u5219\u5316\u6a21\u578b\uff0c\u65e0\u9700\u6539\u52a8\u57fa\u7840\u6a21\u578b\u3002", "result": "\u5728\u4e94\u4e2aNLP\u5206\u7c7b\u6570\u636e\u96c6\u548c\u56db\u79cd\u8bed\u8a00\u6a21\u578b\u4e0a\uff0cAdUE1\u65b9\u6cd5\u5747\u4f18\u4e8eMahalanobis\u8ddd\u79bb\u548csoftmax\u54cd\u5e94\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u8f7b\u91cf\u4e14\u6821\u51c6\u66f4\u597d\u7684\u7f6e\u4fe1\u5ea6\u3002", "conclusion": "AdUE1\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u540e\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u4efb\u52a1\u4e2d\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u80fd\u529b\u3002", "keywords": "\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1, \u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b, \u53c2\u6570\u9ad8\u6548\u5fae\u8c03, AdUE1, L2-SP\u6b63\u5219\u5316"}}
{"id": "2505.15371", "pdf": "https://arxiv.org/pdf/2505.15371", "abs": "https://arxiv.org/abs/2505.15371", "authors": ["Mounssif Krouka", "Chaouki Ben Issaid", "Mehdi Bennis"], "title": "Distributionally Robust Federated Learning with Client Drift Minimization", "categories": ["cs.LG"], "comment": null, "summary": "Federated learning (FL) faces critical challenges, particularly in\nheterogeneous environments where non-independent and identically distributed\ndata across clients can lead to unfair and inefficient model performance. In\nthis work, we introduce \\textit{DRDM}, a novel algorithm that addresses these\nissues by combining a distributionally robust optimization (DRO) framework with\ndynamic regularization to mitigate client drift. \\textit{DRDM} frames the\ntraining as a min-max optimization problem aimed at maximizing performance for\nthe worst-case client, thereby promoting robustness and fairness. This robust\nobjective is optimized through an algorithm leveraging dynamic regularization\nand efficient local updates, which significantly reduces the required number of\ncommunication rounds. Moreover, we provide a theoretical convergence analysis\nfor convex smooth objectives under partial participation. Extensive experiments\non three benchmark datasets, covering various model architectures and data\nheterogeneity levels, demonstrate that \\textit{DRDM} significantly improves\nworst-case test accuracy while requiring fewer communication rounds than\nexisting state-of-the-art baselines. Furthermore, we analyze the impact of\nsignal-to-noise ratio (SNR) and bandwidth on the energy consumption of\nparticipating clients, demonstrating that the number of local update steps can\nbe adaptively selected to achieve a target worst-case test accuracy with\nminimal total energy cost across diverse communication environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDRDM\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u548c\u52a8\u6001\u6b63\u5219\u5316\uff0c\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u5728\u5f02\u6784\u73af\u5883\u4e0b\u7684\u4e0d\u516c\u5e73\u548c\u4f4e\u6548\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6d4b\u8bd5\u51c6\u786e\u6027\u548c\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u5f02\u6784\u73af\u5883\u4e2d\u9762\u4e34\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5e26\u6765\u7684\u4e0d\u516c\u5e73\u548c\u4f4e\u6548\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u591f\u63d0\u5347\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86DRDM\u7b97\u6cd5\uff0c\u5c06\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff08DRO\uff09\u4e0e\u52a8\u6001\u6b63\u5219\u5316\u7ed3\u5408\uff0c\u901a\u8fc7\u6700\u5c0f-\u6700\u5927\u4f18\u5316\u95ee\u9898\u4f18\u5316\u6700\u574f\u60c5\u51b5\u5ba2\u6237\u7aef\u6027\u80fd\uff0c\u5229\u7528\u52a8\u6001\u6b63\u5219\u5316\u548c\u9ad8\u6548\u672c\u5730\u66f4\u65b0\u51cf\u5c11\u901a\u4fe1\u8f6e\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDRDM\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6700\u574f\u60c5\u51b5\u6d4b\u8bd5\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u901a\u4fe1\u8f6e\u6570\uff0c\u5e76\u80fd\u6839\u636e\u901a\u4fe1\u73af\u5883\u81ea\u9002\u5e94\u9009\u62e9\u672c\u5730\u66f4\u65b0\u6b65\u9aa4\u4ee5\u6700\u5c0f\u5316\u80fd\u8017\u3002", "conclusion": "DRDM\u5728\u8054\u90a6\u5b66\u4e60\u7684\u5f02\u6784\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u987e\u4e86\u9c81\u68d2\u6027\u3001\u516c\u5e73\u6027\u548c\u901a\u4fe1\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u8054\u90a6\u5b66\u4e60\uff0c\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff0c\u52a8\u6001\u6b63\u5219\u5316\uff0c\u5f02\u6784\u6570\u636e\uff0c\u901a\u4fe1\u6548\u7387"}}
{"id": "2505.15444", "pdf": "https://arxiv.org/pdf/2505.15444", "abs": "https://arxiv.org/abs/2505.15444", "authors": ["Yutao Zhu", "Jiajie Jin", "Hongjin Qian", "Zheng Liu", "Zhicheng Dou", "Ji-Rong Wen"], "title": "Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation Framework Using Role-Specific Token Optimization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing studies have optimized retrieval-augmented generation (RAG) across\nvarious sub-tasks, such as query understanding and retrieval refinement, but\nintegrating these optimizations into a unified framework remains challenging.\nTo tackle this problem, this work proposes RoleRAG, a unified RAG framework\nthat achieves efficient multi-task processing through role-specific token\noptimization. RoleRAG comprises six modules, each handling a specific sub-task\nwithin the RAG process. Additionally, we introduce a query graph to represent\nthe decomposition of the query, which can be dynamically resolved according to\nthe decomposing state. All modules are driven by the same underlying LLM,\ndistinguished by task-specific role tokens that are individually optimized.\nThis design allows RoleRAG to dynamically activate different modules within a\nsingle LLM instance, thereby streamlining deployment and reducing resource\nconsumption. Experimental results on five open-domain question-answering\ndatasets demonstrate the effectiveness, generalizability, and flexibility of\nour framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86RoleRAG\u6846\u67b6\uff0c\u901a\u8fc7\u89d2\u8272\u7279\u5b9a\u4ee4\u724c\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u591a\u4efb\u52a1\u5904\u7406\uff0c\u7edf\u4e00\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u5b50\u4efb\u52a1\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5206\u522b\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u5b50\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4f46\u7f3a\u4e4f\u5c06\u8fd9\u4e9b\u4f18\u5316\u6574\u5408\u5230\u7edf\u4e00\u6846\u67b6\u4e2d\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRoleRAG\u6846\u67b6\uff0c\u5305\u542b\u516d\u4e2a\u6a21\u5757\uff0c\u6bcf\u4e2a\u6a21\u5757\u5904\u7406RAG\u7684\u4e00\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u67e5\u8be2\u56fe\u52a8\u6001\u5206\u89e3\u95ee\u9898\u3002\u6240\u6709\u6a21\u5757\u7531\u540c\u4e00\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\uff0c\u4f46\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u7684\u89d2\u8272\u4ee4\u724c\u533a\u5206\u3002", "result": "\u5728\u4e94\u4e2a\u5f00\u653e\u57df\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRoleRAG\u6846\u67b6\u6709\u6548\u3001\u901a\u7528\u4e14\u7075\u6d3b\u3002", "conclusion": "RoleRAG\u6846\u67b6\u7b80\u5316\u4e86\u90e8\u7f72\u5e76\u51cf\u5c11\u4e86\u8d44\u6e90\u6d88\u8017\uff0c\u5c55\u793a\u4e86\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u5904\u7406RAG\u591a\u4efb\u52a1\u7684\u6f5c\u529b\u3002", "keywords": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u591a\u4efb\u52a1\u5904\u7406\uff0c\u89d2\u8272\u7279\u5b9a\u4f18\u5316\uff0c\u67e5\u8be2\u56fe\uff0c\u5927\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.15391", "pdf": "https://arxiv.org/pdf/2505.15391", "abs": "https://arxiv.org/abs/2505.15391", "authors": ["Duncan Bart", "Bruno Endres Forlin", "Ana-Lucia Varbanescu", "Marco Ottavi", "Kuan-Hsun Chen"], "title": "InTreeger: An End-to-End Framework for Integer-Only Decision Tree Inference", "categories": ["cs.LG"], "comment": null, "summary": "Integer quantization has emerged as a critical technique to facilitate\ndeployment on resource-constrained devices. Although they do reduce the\ncomplexity of the learning models, their inference performance is often prone\nto quantization-induced errors. To this end, we introduce InTreeger: an\nend-to-end framework that takes a training dataset as input, and outputs an\narchitecture-agnostic integer-only C implementation of tree-based machine\nlearning model, without loss of precision. This framework enables anyone, even\nthose without prior experience in machine learning, to generate a highly\noptimized integer-only classification model that can run on any hardware simply\nby providing an input dataset and target variable. We evaluated our generated\nimplementations across three different architectures (ARM, x86, and RISC-V),\nresulting in significant improvements in inference latency. In addition, we\nshow the energy efficiency compared to typical decision tree implementations\nthat rely on floating-point arithmetic. The results underscore the advantages\nof integer-only inference, making it particularly suitable for energy- and\narea-constrained devices such as embedded systems and edge computing platforms,\nwhile also enabling the execution of decision trees on existing ultra-low power\ndevices.", "AI": {"tldr": "InTreeger\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u67b6\u6784\u65e0\u5173\u7684\u6574\u6570\u51b3\u7b56\u6811\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u548c\u80fd\u6548\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u3002", "motivation": "\u89e3\u51b3\u91cf\u5316\u8bef\u5dee\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u9ad8\u6548\u3001\u7cbe\u786e\u7684\u6574\u6570\u63a8\u7406\u65b9\u6848\u3002", "method": "\u5f00\u53d1InTreeger\u6846\u67b6\uff0c\u8f93\u5165\u6570\u636e\u96c6\u76f4\u63a5\u751f\u6210\u9ad8\u5ea6\u4f18\u5316\u7684\u6574\u6570\u51b3\u7b56\u6811\u6a21\u578bC\u5b9e\u73b0\u3002", "result": "\u5728ARM\u3001x86\u548cRISC-V\u67b6\u6784\u4e0a\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\uff0c\u5e76\u63d0\u9ad8\u80fd\u6548\u3002", "conclusion": "\u6574\u6570\u63a8\u7406\u9002\u5408\u5d4c\u5165\u5f0f\u7cfb\u7edf\u548c\u8fb9\u7f18\u8ba1\u7b97\u5e73\u53f0\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8d85\u4f4e\u529f\u8017\u8bbe\u5907\u3002", "keywords": "integer quantization, tree-based model, inference latency, energy efficiency, embedded systems"}}
{"id": "2505.15456", "pdf": "https://arxiv.org/pdf/2505.15456", "abs": "https://arxiv.org/abs/2505.15456", "authors": ["Weixiang Zhao", "Xingyu Sui", "Yulin Hu", "Jiahe Guo", "Haixiao Liu", "Biye Li", "Yanyan Zhao", "Bing Qin", "Ting Liu"], "title": "Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment", "categories": ["cs.CL"], "comment": "30 pages, 18 figures, 10 tables", "summary": "Personalized alignment is essential for enabling large language models (LLMs)\nto engage effectively in user-centric dialogue. While recent prompt-based and\noffline optimization methods offer preliminary solutions, they fall short in\ncold-start scenarios and long-term personalization due to their inherently\nstatic and shallow designs. In this work, we introduce the Reinforcement\nLearning for Personalized Alignment (RLPA) framework, in which an LLM interacts\nwith a simulated user model to iteratively infer and refine user profiles\nthrough dialogue. The training process is guided by a dual-level reward\nstructure: the Profile Reward encourages accurate construction of user\nrepresentations, while the Response Reward incentivizes generation of responses\nconsistent with the inferred profile. We instantiate RLPA by fine-tuning\nQwen-2.5-3B-Instruct, resulting in Qwen-RLPA, which achieves state-of-the-art\nperformance in personalized dialogue. Empirical evaluations demonstrate that\nQwen-RLPA consistently outperforms prompting and offline fine-tuning baselines,\nand even surpasses advanced commercial models such as Claude-3.5 and GPT-4o.\nFurther analysis highlights Qwen-RLPA's robustness in reconciling conflicting\nuser preferences, sustaining long-term personalization and delivering more\nefficient inference compared to recent reasoning-focused LLMs. These results\nemphasize the potential of dynamic profile inference as a more effective\nparadigm for building personalized dialogue systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4e2a\u6027\u5316\u5bf9\u9f50\u6846\u67b6\uff08RLPA\uff09\uff0c\u901a\u8fc7\u53cc\u5956\u52b1\u673a\u5236\u4f18\u5316LLM\u5728\u5bf9\u8bdd\u4e2d\u7684\u957f\u671f\u4e2a\u6027\u5316\u8868\u73b0\uff0cQwen-RLPA\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u53ca\u5546\u4e1a\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u9759\u6001\u65b9\u6cd5\u5728\u51b7\u542f\u52a8\u548c\u957f\u671f\u4e2a\u6027\u5316\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u52a8\u6001\u4f18\u5316\u7528\u6237\u5bf9\u9f50\u3002", "method": "RLPA\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u7528\u6237\u4ea4\u4e92\u4e0e\u53cc\u5956\u52b1\uff08\u7528\u6237\u753b\u50cf\u5956\u52b1\u548c\u54cd\u5e94\u5956\u52b1\uff09\u8fed\u4ee3\u4f18\u5316\u7528\u6237\u753b\u50cf\u4e0e\u5bf9\u8bdd\u54cd\u5e94\u3002", "result": "Qwen-RLPA\u5728\u4e2a\u6027\u5316\u5bf9\u8bdd\u4e2d\u8d85\u8d8a\u63d0\u793a\u5b66\u4e60\u3001\u79bb\u7ebf\u5fae\u8c03\u57fa\u7ebf\u53caClaude-3.5\u3001GPT-4o\u7b49\u5546\u4e1a\u6a21\u578b\uff0c\u4e14\u5728\u5904\u7406\u51b2\u7a81\u504f\u597d\u548c\u63a8\u7406\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u52a8\u6001\u7528\u6237\u753b\u50cf\u63a8\u65ad\u662f\u6784\u5efa\u4e2a\u6027\u5316\u5bf9\u8bdd\u7cfb\u7edf\u7684\u66f4\u6709\u6548\u8303\u5f0f\u3002", "keywords": "\u4e2a\u6027\u5316\u5bf9\u9f50, \u5f3a\u5316\u5b66\u4e60, \u5927\u8bed\u8a00\u6a21\u578b, \u7528\u6237\u753b\u50cf, \u53cc\u5956\u52b1\u673a\u5236"}}
{"id": "2505.15405", "pdf": "https://arxiv.org/pdf/2505.15405", "abs": "https://arxiv.org/abs/2505.15405", "authors": ["Martin Carrasco", "Guillermo Bernardez", "Marco Montagna", "Nina Miolane", "Lev Telyatnikov"], "title": "HOPSE: Scalable Higher-Order Positional and Structural Encoder for Combinatorial Representations", "categories": ["cs.LG"], "comment": null, "summary": "While Graph Neural Networks (GNNs) have proven highly effective at modeling\nrelational data, pairwise connections cannot fully capture multi-way\nrelationships naturally present in complex real-world systems. In response to\nthis, Topological Deep Learning (TDL) leverages more general combinatorial\nrepresentations -- such as simplicial or cellular complexes -- to accommodate\nhigher-order interactions. Existing TDL methods often extend GNNs through\nHigher-Order Message Passing (HOMP), but face critical \\emph{scalability\nchallenges} due to \\textit{(i)} a combinatorial explosion of message-passing\nroutes, and \\textit{(ii)} significant complexity overhead from the propagation\nmechanism. To overcome these limitations, we propose HOPSE (Higher-Order\nPositional and Structural Encoder) -- a \\emph{message passing-free} framework\nthat uses Hasse graph decompositions to derive efficient and expressive\nencodings over \\emph{arbitrary higher-order domains}. Notably, HOPSE scales\nlinearly with dataset size while preserving expressive power and permutation\nequivariance. Experiments on molecular, expressivity and topological benchmarks\nshow that HOPSE matches or surpasses state-of-the-art performance while\nachieving up to 7 $times$ speedups over HOMP-based models, opening a new path\nfor scalable TDL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHOPSE\u7684\u6846\u67b6\uff0c\u901a\u8fc7Hasse\u56fe\u5206\u89e3\u89e3\u51b3\u73b0\u6709\u9ad8\u9636\u62d3\u6251\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u907f\u514d\u4e86\u6d88\u606f\u4f20\u9012\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5b9e\u73b0\u4e86\u7ebf\u6027\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u62d3\u6251\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u9636\u5173\u7cfb\u65f6\u9762\u4e34\u6d88\u606f\u4f20\u9012\u8def\u5f84\u7206\u70b8\u548c\u8ba1\u7b97\u590d\u6742\u6027\u9ad8\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86HOPSE\u6846\u67b6\uff0c\u5229\u7528Hasse\u56fe\u5206\u89e3\u5728\u9ad8\u9636\u57df\u4e0a\u751f\u6210\u9ad8\u6548\u7684\u7f16\u7801\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u7684\u6d88\u606f\u4f20\u9012\u673a\u5236\u3002", "result": "HOPSE\u5728\u5206\u5b50\u3001\u8868\u8fbe\u80fd\u529b\u548c\u62d3\u6251\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u4e14\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u4e867\u500d\u3002", "conclusion": "HOPSE\u4e3a\u53ef\u6269\u5c55\u7684\u62d3\u6251\u6df1\u5ea6\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u9ad8\u9636\u5173\u7cfb\u5efa\u6a21\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\u3002", "keywords": "\u56fe\u795e\u7ecf\u7f51\u7edc, \u62d3\u6251\u6df1\u5ea6\u5b66\u4e60, \u9ad8\u9636\u5173\u7cfb, Hasse\u56fe\u5206\u89e3, \u53ef\u6269\u5c55\u6027"}}
{"id": "2505.15467", "pdf": "https://arxiv.org/pdf/2505.15467", "abs": "https://arxiv.org/abs/2505.15467", "authors": ["Yukun Zhao", "Lingyong Yan", "Zhenyang Li", "Shuaiqiang Wang", "Zhumin Chen", "Zhaochun Ren", "Dawei Yin"], "title": "Joint Flashback Adaptation for Forgetting-Resistant Instruction Tuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models have achieved remarkable success in various tasks.\nHowever, it is challenging for them to learn new tasks incrementally due to\ncatastrophic forgetting. Existing approaches rely on experience replay,\noptimization constraints, or task differentiation, which encounter strict\nlimitations in real-world scenarios. To address these issues, we propose Joint\nFlashback Adaptation. We first introduce flashbacks -- a limited number of\nprompts from old tasks -- when adapting to new tasks and constrain the\ndeviations of the model outputs compared to the original one. We then\ninterpolate latent tasks between flashbacks and new tasks to enable jointly\nlearning relevant latent tasks, new tasks, and flashbacks, alleviating data\nsparsity in flashbacks and facilitating knowledge sharing for smooth\nadaptation. Our method requires only a limited number of flashbacks without\naccess to the replay data and is task-agnostic. We conduct extensive\nexperiments on state-of-the-art large language models across 1000+\ninstruction-following tasks, arithmetic reasoning tasks, and general reasoning\ntasks. The results demonstrate the superior performance of our method in\nimproving generalization on new tasks and reducing forgetting in old tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aJoint Flashback Adaptation\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e7\u4efb\u52a1\u7684\u5c11\u91cf\u63d0\u793a\uff08flashbacks\uff09\u548c\u6a21\u578b\u8f93\u51fa\u7684\u7ea6\u675f\uff0c\u7ed3\u5408\u4e2d\u95f4\u6f5c\u5728\u4efb\u52a1\u7684\u63d2\u503c\uff0c\u6709\u6548\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u589e\u91cf\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6709\u4e25\u683c\u9650\u5236\u3002", "method": "Joint Flashback Adaptation\uff1a\u5f15\u5165\u5c11\u91cf\u65e7\u4efb\u52a1\u63d0\u793a\uff08flashbacks\uff09\u5e76\u7ea6\u675f\u6a21\u578b\u8f93\u51fa\u504f\u5dee\uff0c\u63d2\u503c\u6f5c\u5728\u4efb\u52a1\u4ee5\u8054\u5408\u5b66\u4e60\u76f8\u5173\u4efb\u52a1\u3001\u65b0\u4efb\u52a1\u548cflashbacks\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57281000+\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u65b0\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u5e76\u51cf\u5c11\u65e7\u4efb\u52a1\u7684\u9057\u5fd8\u3002", "conclusion": "Joint Flashback Adaptation\u65e0\u9700\u91cd\u653e\u6570\u636e\uff0c\u4ec5\u9700\u5c11\u91cfflashbacks\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u589e\u91cf\u5b66\u4e60\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u589e\u91cf\u5b66\u4e60\u3001\u707e\u96be\u6027\u9057\u5fd8\u3001Joint Flashback Adaptation"}}
{"id": "2505.15407", "pdf": "https://arxiv.org/pdf/2505.15407", "abs": "https://arxiv.org/abs/2505.15407", "authors": ["Naiqi Li", "Yuqiu Xie", "Peiyuan Liu", "Tao Dai", "Yong Jiang", "Shu-Tao Xia"], "title": "Efficient Differentiable Approximation of Generalized Low-rank Regularization", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": "Accepted by IJCAI-25", "summary": "Low-rank regularization (LRR) has been widely applied in various machine\nlearning tasks, but the associated optimization is challenging. Directly\noptimizing the rank function under constraints is NP-hard in general. To\novercome this difficulty, various relaxations of the rank function were\nstudied. However, optimization of these relaxed LRRs typically depends on\nsingular value decomposition, which is a time-consuming and nondifferentiable\noperator that cannot be optimized with gradient-based techniques. To address\nthese challenges, in this paper we propose an efficient differentiable\napproximation of the generalized LRR. The considered LRR form subsumes many\npopular choices like the nuclear norm, the Schatten-$p$ norm, and various\nnonconvex relaxations. Our method enables LRR terms to be appended to loss\nfunctions in a plug-and-play fashion, and the GPU-friendly operations enable\nefficient and convenient implementation. Furthermore, convergence analysis is\npresented, which rigorously shows that both the bias and the variance of our\nrank estimator rapidly reduce with increased sample size and iteration steps.\nIn the experimental study, the proposed method is applied to various tasks,\nwhich demonstrates its versatility and efficiency. Code is available at\nhttps://github.com/naiqili/EDLRR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u53ef\u5fae\u5206\u5e7f\u4e49\u4f4e\u79e9\u6b63\u5219\u5316\uff08LRR\uff09\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfLRR\u4f18\u5316\u4e2d\u8ba1\u7b97\u590d\u6742\u4e14\u4e0d\u53ef\u5fae\u5206\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u591a\u6837\u6027\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u4f4e\u79e9\u6b63\u5219\u5316\u7684\u4f18\u5316\u9762\u4e34NP\u96be\u95ee\u9898\uff0c\u4e14\u4f9d\u8d56\u4e8e\u8ba1\u7b97\u8017\u65f6\u7684\u5947\u5f02\u503c\u5206\u89e3\uff0c\u96be\u4ee5\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u7684\u6280\u672f\u8fdb\u884c\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5fae\u5206\u7684\u5e7f\u4e49LRR\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u652f\u6301\u5373\u63d2\u5373\u7528\uff0c\u9002\u7528\u4e8eGPU\u52a0\u901f\u64cd\u4f5c\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9002\u7528\u6027\uff0c\u6536\u655b\u5206\u6790\u8868\u660e\u5176\u504f\u5dee\u548c\u65b9\u5dee\u968f\u6837\u672c\u91cf\u548c\u8fed\u4ee3\u6b65\u6570\u5feb\u901f\u964d\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f4e\u79e9\u6b63\u5219\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u4f4e\u79e9\u6b63\u5219\u5316, \u53ef\u5fae\u5206\u8fd1\u4f3c, \u5947\u5f02\u503c\u5206\u89e3, \u68af\u5ea6\u4f18\u5316"}}
{"id": "2505.15471", "pdf": "https://arxiv.org/pdf/2505.15471", "abs": "https://arxiv.org/abs/2505.15471", "authors": ["Yiyun Zhou", "Chang Yao", "Jingyuan Chen"], "title": "CoLA: Collaborative Low-Rank Adaptation", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025, Findings", "summary": "The scaling law of Large Language Models (LLMs) reveals a power-law\nrelationship, showing diminishing return on performance as model scale\nincreases. While training LLMs from scratch is resource-intensive, fine-tuning\na pre-trained model for specific tasks has become a practical alternative. Full\nfine-tuning (FFT) achieves strong performance; however, it is computationally\nexpensive and inefficient. Parameter-efficient fine-tuning (PEFT) methods, like\nLoRA, have been proposed to address these challenges by freezing the\npre-trained model and adding lightweight task-specific modules. LoRA, in\nparticular, has proven effective, but its application to multi-task scenarios\nis limited by interference between tasks. Recent approaches, such as\nMixture-of-Experts (MOE) and asymmetric LoRA, have aimed to mitigate these\nissues but still struggle with sample scarcity and noise interference due to\ntheir fixed structure. In response, we propose CoLA, a more flexible LoRA\narchitecture with an efficient initialization scheme, and introduces three\ncollaborative strategies to enhance performance by better utilizing the\nquantitative relationships between matrices $A$ and $B$. Our experiments\ndemonstrate the effectiveness and robustness of CoLA, outperforming existing\nPEFT methods, especially in low-sample scenarios. Our data and code are fully\npublicly available at https://github.com/zyy-2001/CoLA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoLA\u7684\u65b0\u578bLoRA\u67b6\u6784\uff0c\u901a\u8fc7\u7075\u6d3b\u7684\u521d\u59cb\u5316\u548c\u534f\u4f5c\u7b56\u7565\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4f4e\u6837\u672c\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08\u5982LoRA\uff09\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u56e0\u4efb\u52a1\u95f4\u5e72\u6270\u800c\u53d7\u9650\uff0c\u8fd1\u671f\u7684\u65b9\u6cd5\uff08\u5982MOE\u548c\u975e\u5bf9\u79f0LoRA\uff09\u4ecd\u53d7\u9650\u4e8e\u6837\u672c\u7a00\u7f3a\u548c\u566a\u58f0\u5e72\u6270\u3002", "method": "\u63d0\u51faCoLA\u67b6\u6784\uff0c\u91c7\u7528\u9ad8\u6548\u521d\u59cb\u5316\u65b9\u6848\u548c\u4e09\u79cd\u534f\u4f5c\u7b56\u7565\uff0c\u66f4\u597d\u5730\u5229\u7528\u77e9\u9635A\u548cB\u4e4b\u95f4\u7684\u5b9a\u91cf\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCoLA\u5728\u4f4e\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u8d8a\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709PEFT\u65b9\u6cd5\u3002", "conclusion": "CoLA\u662f\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684LoRA\u6539\u8fdb\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u591a\u4efb\u52a1\u573a\u666f\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u5fae\u8c03, LoRA, \u591a\u4efb\u52a1\u5b66\u4e60, CoLA"}}
{"id": "2505.15418", "pdf": "https://arxiv.org/pdf/2505.15418", "abs": "https://arxiv.org/abs/2505.15418", "authors": ["Yueheng Li", "Guangming Xie", "Zongqing Lu"], "title": "Guided Policy Optimization under Partial Observability", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "24 pages, 13 figures", "summary": "Reinforcement Learning (RL) in partially observable environments poses\nsignificant challenges due to the complexity of learning under uncertainty.\nWhile additional information, such as that available in simulations, can\nenhance training, effectively leveraging it remains an open problem. To address\nthis, we introduce Guided Policy Optimization (GPO), a framework that co-trains\na guider and a learner. The guider takes advantage of privileged information\nwhile ensuring alignment with the learner's policy that is primarily trained\nvia imitation learning. We theoretically demonstrate that this learning scheme\nachieves optimality comparable to direct RL, thereby overcoming key limitations\ninherent in existing approaches. Empirical evaluations show strong performance\nof GPO across various tasks, including continuous control with partial\nobservability and noise, and memory-based challenges, significantly\noutperforming existing methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGPO\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u540c\u8bad\u7ec3\u5f15\u5bfc\u8005\u548c\u5b66\u4e60\u8005\uff0c\u89e3\u51b3\u4e86\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u96be\u9898\u3002", "motivation": "\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\uff0c\u5f3a\u5316\u5b66\u4e60\u7531\u4e8e\u4e0d\u786e\u5b9a\u6027\u800c\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5229\u7528\u989d\u5916\u4fe1\u606f\uff08\u5982\u4eff\u771f\u6570\u636e\uff09\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faGuided Policy Optimization (GPO)\u6846\u67b6\uff0c\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\uff0c\u901a\u8fc7\u5f15\u5bfc\u8005\u5229\u7528\u7279\u6743\u4fe1\u606f\u5e76\u4e0e\u5b66\u4e60\u8005\u7684\u7b56\u7565\u5bf9\u9f50\u3002", "result": "\u7406\u8bba\u8bc1\u660eGPO\u80fd\u8fbe\u5230\u4e0e\u76f4\u63a5\u5f3a\u5316\u5b66\u4e60\u76f8\u5f53\u7684\u4f18\u5316\u6548\u679c\uff1b\u5b9e\u9a8c\u8868\u660eGPO\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GPO\u901a\u8fc7\u5f15\u5bfc\u8005\u4e0e\u5b66\u4e60\u8005\u7684\u5171\u540c\u8bad\u7ec3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883, Guided Policy Optimization, \u6a21\u4eff\u5b66\u4e60"}}
{"id": "2505.15077", "pdf": "https://arxiv.org/pdf/2505.15077", "abs": "https://arxiv.org/abs/2505.15077", "authors": ["Alessandro dos Santos Ferreira", "Ana Paula Marques Ramos", "Jos\u00e9 Marcato Junior", "Wesley Nunes Gon\u00e7alves"], "title": "Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T07 (Primary), 68U10, 68T45 (Secondary)", "I.4.8; I.2.10; I.5.4"], "comment": "18 pages, 13 figures", "summary": "Urban forests play a key role in enhancing environmental quality and\nsupporting biodiversity in cities. Mapping and monitoring these green spaces\nare crucial for urban planning and conservation, yet accurately detecting trees\nis challenging due to complex landscapes and the variability in image\nresolution caused by different satellite sensors or UAV flight altitudes. While\ndeep learning architectures have shown promise in addressing these challenges,\ntheir effectiveness remains strongly dependent on the availability of large and\nmanually labeled datasets, which are often expensive and difficult to obtain in\nsufficient quantity. In this work, we propose a novel pipeline that integrates\ndomain adaptation with GANs and Diffusion models to enhance the quality of\nlow-resolution aerial images. Our proposed pipeline enhances low-resolution\nimagery while preserving semantic content, enabling effective tree segmentation\nwithout requiring large volumes of manually annotated data. Leveraging models\nsuch as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we\ngenerate realistic and structurally consistent synthetic samples that expand\nthe training dataset and unify scale across domains. This approach not only\nimproves the robustness of segmentation models across different acquisition\nconditions but also provides a scalable and replicable solution for remote\nsensing scenarios with scarce annotation resources. Experimental results\ndemonstrated an improvement of over 50% in IoU for low-resolution images,\nhighlighting the effectiveness of our method compared to traditional pipelines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408GAN\u548c\u6269\u6563\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u5347\u4f4e\u5206\u8fa8\u7387\u822a\u7a7a\u56fe\u50cf\u8d28\u91cf\uff0c\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u6709\u6548\u7684\u6811\u6728\u5206\u5272\u3002", "motivation": "\u57ce\u5e02\u68ee\u6797\u5bf9\u73af\u5883\u8d28\u91cf\u548c\u751f\u7269\u591a\u6837\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6811\u6728\u68c0\u6d4b\u56e0\u56fe\u50cf\u5206\u8fa8\u7387\u5dee\u5f02\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6d41\u7a0b\uff0c\u7ed3\u5408\u57df\u9002\u5e94\u4e0eGAN\u3001\u6269\u6563\u6a21\u578b\uff08\u5982pix2pix\u3001Real-ESRGAN\u3001Latent Diffusion\u3001Stable Diffusion\uff09\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6837\u672c\uff0c\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u5e76\u7edf\u4e00\u5c3a\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u7684IoU\u63d0\u5347\u4e8650%\u4ee5\u4e0a\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u5206\u5272\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u8fd8\u4e3a\u6807\u6ce8\u8d44\u6e90\u7a00\u7f3a\u7684\u9065\u611f\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u57ce\u5e02\u68ee\u6797, \u6811\u6728\u68c0\u6d4b, GAN, \u6269\u6563\u6a21\u578b, \u57df\u9002\u5e94"}}
{"id": "2505.15472", "pdf": "https://arxiv.org/pdf/2505.15472", "abs": "https://arxiv.org/abs/2505.15472", "authors": ["Song Dai", "Yibo Yan", "Jiamin Su", "Dongfang Zihao", "Yubo Gao", "Yonghua Hei", "Jungang Li", "Junyan Zhang", "Sicheng Tao", "Zhuoran Gao", "Xuming Hu"], "title": "PhysicsArena: The First Multimodal Physics Reasoning Benchmark Exploring Variable, Process, and Solution Dimensions", "categories": ["cs.CL", "I.2.7; I.2.10"], "comment": "27 pages,20 figures, EMNLP", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in diverse reasoning tasks, yet their application to complex\nphysics reasoning remains underexplored. Physics reasoning presents unique\nchallenges, requiring grounding in physical conditions and the interpretation\nof multimodal information. Current physics benchmarks are limited, often\nfocusing on text-only inputs or solely on problem-solving, thereby overlooking\nthe critical intermediate steps of variable identification and process\nformulation. To address these limitations, we introduce PhysicsArena, the first\nmultimodal physics reasoning benchmark designed to holistically evaluate MLLMs\nacross three critical dimensions: variable identification, physical process\nformulation, and solution derivation. PhysicsArena aims to provide a\ncomprehensive platform for assessing and advancing the multimodal physics\nreasoning abilities of MLLMs.", "AI": {"tldr": "PhysicsArena\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u7269\u7406\u63a8\u7406\u57fa\u51c6\uff0c\u65e8\u5728\u5168\u9762\u8bc4\u4f30MLLMs\u5728\u53d8\u91cf\u8bc6\u522b\u3001\u7269\u7406\u8fc7\u7a0b\u5236\u5b9a\u548c\u89e3\u51b3\u65b9\u6848\u63a8\u5bfc\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7269\u7406\u63a8\u7406\u57fa\u51c6\u5c40\u9650\u4e8e\u6587\u672c\u8f93\u5165\u6216\u95ee\u9898\u89e3\u51b3\uff0c\u7f3a\u4e4f\u591a\u6a21\u6001\u4fe1\u606f\u548c\u4e2d\u95f4\u6b65\u9aa4\u7684\u8bc4\u4f30\u3002", "method": "\u5f15\u5165\u4e86PhysicsArena\uff0c\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\uff08\u53d8\u91cf\u8bc6\u522b\u3001\u7269\u7406\u8fc7\u7a0b\u5236\u5b9a\u548c\u89e3\u51b3\u65b9\u6848\u63a8\u5bfc\uff09\u8bc4\u4f30MLLMs\u3002", "result": "PhysicsArena\u4e3aMLLMs\u7684\u591a\u6a21\u6001\u7269\u7406\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u8bc4\u4f30\u5e73\u53f0\u3002", "conclusion": "\u8be5\u57fa\u51c6\u586b\u8865\u4e86MLLMs\u5728\u590d\u6742\u7269\u7406\u63a8\u7406\u4e2d\u7684\u7814\u7a76\u7a7a\u767d\u3002", "keywords": "MLLMs, \u591a\u6a21\u6001, \u7269\u7406\u63a8\u7406, \u57fa\u51c6"}}
{"id": "2505.15423", "pdf": "https://arxiv.org/pdf/2505.15423", "abs": "https://arxiv.org/abs/2505.15423", "authors": ["Marcell T. Kurbucz", "Nikolaos Tzivanakis", "Nilufer Sari Aslam", "Adam M. Sykulski"], "title": "SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding", "categories": ["cs.LG", "econ.EM", "stat.AP", "stat.ME", "stat.ML", "62H20, 62J05, 68T05", "G.3; I.2.6; I.5.1; I.5.2"], "comment": "15 pages, 1 figure, 3 tables", "summary": "Capturing nonlinear relationships without sacrificing interpretability\nremains a persistent challenge in regression modeling. We introduce SplitWise,\na novel framework that enhances stepwise regression. It adaptively transforms\nnumeric predictors into threshold-based binary features using shallow decision\ntrees, but only when such transformations improve model fit, as assessed by the\nAkaike Information Criterion (AIC) or Bayesian Information Criterion (BIC).\nThis approach preserves the transparency of linear models while flexibly\ncapturing nonlinear effects. Implemented as a user-friendly R package,\nSplitWise is evaluated on both synthetic and real-world datasets. The results\nshow that it consistently produces more parsimonious and generalizable models\nthan traditional stepwise and penalized regression techniques.", "AI": {"tldr": "SplitWise\u6846\u67b6\u901a\u8fc7\u6d45\u5c42\u51b3\u7b56\u6811\u5c06\u6570\u503c\u9884\u6d4b\u53d8\u91cf\u8f6c\u5316\u4e3a\u4e8c\u8fdb\u5236\u7279\u5f81\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u7ebf\u6027\u6a21\u578b\u900f\u660e\u5ea6\u7684\u540c\u65f6\u6355\u6349\u975e\u7ebf\u6027\u6548\u5e94\u3002", "motivation": "\u89e3\u51b3\u56de\u5f52\u6a21\u578b\u4e2d\u975e\u7ebf\u6027\u5173\u7cfb\u6355\u83b7\u4e0e\u53ef\u89e3\u91ca\u6027\u517c\u987e\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6d45\u5c42\u51b3\u7b56\u6811\u81ea\u9002\u5e94\u5730\u8f6c\u6362\u9884\u6d4b\u53d8\u91cf\uff0c\u5e76\u901a\u8fc7AIC\u6216BIC\u8bc4\u4f30\u6a21\u578b\u62df\u5408\u6548\u679c\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u9010\u6b65\u56de\u5f52\u548c\u60e9\u7f5a\u56de\u5f52\u65b9\u6cd5\u3002", "conclusion": "SplitWise\u5728\u4fdd\u6301\u6a21\u578b\u900f\u660e\u6027\u7684\u540c\u65f6\u7075\u6d3b\u6355\u6349\u975e\u7ebf\u6027\u6548\u5e94\uff0c\u751f\u6210\u66f4\u7b80\u6d01\u4e14\u901a\u7528\u7684\u6a21\u578b\u3002", "keywords": "SplitWise, \u975e\u7ebf\u6027\u5173\u7cfb, \u53ef\u89e3\u91ca\u6027, \u9010\u6b65\u56de\u5f52, \u51b3\u7b56\u6811"}}
{"id": "2505.15475", "pdf": "https://arxiv.org/pdf/2505.15475", "abs": "https://arxiv.org/abs/2505.15475", "authors": ["Zhanyue Qin", "Yue Ding", "Deyuan Liu", "Qingbin Liu", "Junxian Cai", "Xi Chen", "Zhiying Tu", "Dianhui Chu", "Cuiyun Gao", "Dianbo Sui"], "title": "LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Nowadays, Large Language Models (LLMs) have attracted widespread attention\ndue to their powerful performance. However, due to the unavoidable exposure to\nsocially biased data during training, LLMs tend to exhibit social biases,\nparticularly gender bias. To better explore and quantifying the degree of\ngender bias in LLMs, we propose a pair of datasets named GenBiasEval and\nGenHintEval, respectively. The GenBiasEval is responsible for evaluating the\ndegree of gender bias in LLMs, accompanied by an evaluation metric named\nAFGB-Score (Absolutely Fair Gender Bias Score). Meanwhile, the GenHintEval is\nused to assess whether LLMs can provide responses consistent with prompts that\ncontain gender hints, along with the accompanying evaluation metric UB-Score\n(UnBias Score). Besides, in order to mitigate gender bias in LLMs more\neffectively, we present the LFTF (Locating First and Then Fine-Tuning)\nalgorithm.The algorithm first ranks specific LLM blocks by their relevance to\ngender bias in descending order using a metric called BMI (Block Mitigating\nImportance Score). Based on this ranking, the block most strongly associated\nwith gender bias is then fine-tuned using a carefully designed loss function.\nNumerous experiments have shown that our proposed LFTF algorithm can\nsignificantly mitigate gender bias in LLMs while maintaining their general\ncapabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u4e2a\u6570\u636e\u96c6\uff08GenBiasEval\u548cGenHintEval\uff09\u53caLFTF\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u51cf\u5c11LLMs\u4e2d\u7684\u6027\u522b\u504f\u89c1\u3002", "motivation": "LLMs\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u63a5\u89e6\u793e\u4f1a\u504f\u89c1\u6570\u636e\uff0c\u5bfc\u81f4\u51fa\u73b0\u6027\u522b\u504f\u89c1\uff0c\u9700\u8981\u6709\u6548\u8bc4\u4f30\u548c\u7f13\u89e3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u6570\u636e\u96c6\uff08GenBiasEval\u548cGenHintEval\uff09\u548c\u76f8\u5173\u8bc4\u5206\u6307\u6807\uff08AFGB-Score\u548cUB-Score\uff09\uff0c\u4ee5\u53caLFTF\u7b97\u6cd5\uff08\u901a\u8fc7BMI\u8bc4\u5206\u6392\u5e8f\u5e76\u5fae\u8c03\u76f8\u5173\u6a21\u5757\uff09\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eLFTF\u7b97\u6cd5\u80fd\u663e\u8457\u51cf\u5c11LLMs\u7684\u6027\u522b\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u901a\u7528\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLMs\u4e2d\u7684\u6027\u522b\u504f\u89c1\u95ee\u9898\u3002", "keywords": "Large Language Models, gender bias, GenBiasEval, GenHintEval, LFTF algorithm"}}
{"id": "2505.15433", "pdf": "https://arxiv.org/pdf/2505.15433", "abs": "https://arxiv.org/abs/2505.15433", "authors": ["Beni Egressy", "Jan St\u00fchmer"], "title": "Set-LLM: A Permutation-Invariant LLM", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "While large language models (LLMs) demonstrate impressive capabilities across\nnumerous applications, their robustness remains a critical concern. This paper\nis motivated by a specific vulnerability: the order sensitivity of LLMs. This\nvulnerability manifests itself as the order bias observed when LLMs decide\nbetween possible options (for example, a preference for the first option) and\nthe tendency of LLMs to provide different answers when options are reordered.\nThe use cases for this scenario extend beyond the classical case of\nmultiple-choice question answering to the use of LLMs as automated evaluators\nin AI pipelines, comparing output generated by different models. We introduce\nSet-LLM, a novel architectural adaptation for pretrained LLMs that enables the\nprocessing of mixed set-text inputs with permutation invariance guarantees. The\nadaptations involve a new attention mask and new positional encodings\nspecifically designed for sets. We provide a theoretical proof of invariance\nand demonstrate through experiments that Set-LLM can be trained effectively,\nachieving comparable or improved performance and maintaining the runtime of the\noriginal model, while eliminating order sensitivity.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Set-LLM\uff0c\u4e00\u79cd\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u987a\u5e8f\u654f\u611f\u6027\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b0\u7684\u6ce8\u610f\u529b\u63a9\u7801\u548c\u4f4d\u7f6e\u7f16\u7801\u5b9e\u73b0\u6392\u5217\u4e0d\u53d8\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u80fd\u529b\uff0c\u4f46\u5176\u9c81\u68d2\u6027\u95ee\u9898\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u662f\u9009\u9879\u987a\u5e8f\u654f\u611f\u6027\u3002", "method": "\u63d0\u51faSet-LLM\u67b6\u6784\uff0c\u91c7\u7528\u65b0\u7684\u6ce8\u610f\u529b\u63a9\u7801\u548c\u4f4d\u7f6e\u7f16\u7801\uff0c\u786e\u4fdd\u5176\u5bf9\u6df7\u5408\u96c6\u5408\u8f93\u5165\u7684\u6392\u5217\u4e0d\u53d8\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eSet-LLM\u53ef\u6709\u6548\u8bad\u7ec3\uff0c\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u4e14\u6d88\u9664\u987a\u5e8f\u654f\u611f\u6027\u3002", "conclusion": "Set-LLM\u6210\u529f\u89e3\u51b3\u4e86LLM\u7684\u987a\u5e8f\u654f\u611f\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u59cb\u6a21\u578b\u7684\u8fd0\u884c\u6548\u7387\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u987a\u5e8f\u654f\u611f\u6027,\u6392\u5217\u4e0d\u53d8\u6027,Set-LLM"}}
{"id": "2505.15480", "pdf": "https://arxiv.org/pdf/2505.15480", "abs": "https://arxiv.org/abs/2505.15480", "authors": ["Qihuang Zhong", "Liang Ding", "Xiantao Cai", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance", "categories": ["cs.CL"], "comment": "Accepted to ACL2025 Findings", "summary": "Supervised fine-tuning (SFT) is a common approach to improve the\ndomain-specific question-answering (QA) performance of large language models\n(LLMs). However, recent literature reveals that due to the conflicts between\nLLMs' internal knowledge and the context knowledge of training data, vanilla\nSFT using the full QA training set is usually suboptimal. In this paper, we\nfirst design a query diversification strategy for robust conflict detection and\nthen conduct a series of experiments to analyze the impact of knowledge\nconflict. We find that 1) training samples with varied conflicts contribute\ndifferently, where SFT on the data with large conflicts leads to catastrophic\nperformance drops; 2) compared to directly filtering out the conflict data,\nappropriately applying the conflict data would be more beneficial. Motivated by\nthis, we propose a simple-yet-effective Knowledge-aware Fine-tuning (namely\nKaFT) approach to effectively boost LLMs' performance. The core of KaFT is to\nadapt the training weight by assigning different rewards for different training\nsamples according to conflict level. Extensive experiments show that KaFT\nbrings consistent and significant improvements across four LLMs. More analyses\nprove that KaFT effectively improves the model generalization and alleviates\nthe hallucination.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKaFT\u7684\u77e5\u8bc6\u611f\u77e5\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u6839\u636e\u51b2\u7a81\u7ea7\u522b\u8c03\u6574\u8bad\u7ec3\u6743\u91cd\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9886\u57df\u7279\u5b9a\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5728\u5904\u7406\u8bad\u7ec3\u6570\u636e\u4e0e\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u7684\u51b2\u7a81\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u5206\u6790\u77e5\u8bc6\u51b2\u7a81\u7684\u5f71\u54cd\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86\u67e5\u8be2\u591a\u6837\u5316\u7b56\u7565\u4ee5\u68c0\u6d4b\u51b2\u7a81\uff0c\u5e76\u63d0\u51fa\u4e86KaFT\u65b9\u6cd5\uff0c\u6839\u636e\u51b2\u7a81\u7ea7\u522b\u4e3a\u8bad\u7ec3\u6837\u672c\u5206\u914d\u4e0d\u540c\u5956\u52b1\u4ee5\u8c03\u6574\u8bad\u7ec3\u6743\u91cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKaFT\u5728\u56db\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u5747\u5e26\u6765\u4e86\u663e\u8457\u4e14\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u6539\u5584\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u5e76\u51cf\u5c11\u4e86\u5e7b\u89c9\u73b0\u8c61\u3002", "conclusion": "KaFT\u901a\u8fc7\u6709\u6548\u5229\u7528\u51b2\u7a81\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\u63d0\u4f9b\u4e86\u7b80\u5355\u800c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u76d1\u7763\u5fae\u8c03,\u77e5\u8bc6\u51b2\u7a81,\u5927\u578b\u8bed\u8a00\u6a21\u578b,KaFT,\u95ee\u7b54\u4efb\u52a1"}}
{"id": "2505.15496", "pdf": "https://arxiv.org/pdf/2505.15496", "abs": "https://arxiv.org/abs/2505.15496", "authors": ["Hossein Zakerinia", "Christoph H. Lampert"], "title": "Fast Rate Bounds for Multi-Task and Meta-Learning with Different Sample Sizes", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We present new fast-rate generalization bounds for multi-task and\nmeta-learning in the unbalanced setting, i.e. when the tasks have training sets\nof different sizes, as is typically the case in real-world scenarios.\nPreviously, only standard-rate bounds were known for this situation, while\nfast-rate bounds were limited to the setting where all training sets are of\nequal size. Our new bounds are numerically computable as well as interpretable,\nand we demonstrate their flexibility in handling a number of cases where they\ngive stronger guarantees than previous bounds. Besides the bounds themselves,\nwe also make conceptual contributions: we demonstrate that the unbalanced\nmulti-task setting has different statistical properties than the balanced\nsituation, specifically that proofs from the balanced situation do not carry\nover to the unbalanced setting. Additionally, we shed light on the fact that\nthe unbalanced situation allows two meaningful definitions of multi-task risk,\ndepending on whether if all tasks should be considered equally important or if\nsample-rich tasks should receive more weight than sample-poor ones.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u4efb\u52a1\u548c\u5143\u5b66\u4e60\u5728\u4e0d\u5e73\u8861\u8bbe\u7f6e\u4e0b\u7684\u65b0\u5feb\u901f\u7387\u6cdb\u5316\u8fb9\u754c\uff0c\u586b\u8865\u4e86\u5b9e\u9645\u573a\u666f\u4e2d\u4efb\u52a1\u8bad\u7ec3\u96c6\u5927\u5c0f\u4e0d\u540c\u65f6\u7684\u7406\u8bba\u7a7a\u767d\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u573a\u666f\u4e2d\u591a\u4efb\u52a1\u5b66\u4e60\u4efb\u52a1\u8bad\u7ec3\u96c6\u5927\u5c0f\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u6b64\u524d\u4ec5\u6709\u6807\u51c6\u901f\u7387\u8fb9\u754c\uff0c\u5feb\u901f\u7387\u8fb9\u754c\u4ec5\u9650\u4e8e\u8bad\u7ec3\u96c6\u5927\u5c0f\u76f8\u540c\u7684\u60c5\u51b5\u3002", "method": "\u901a\u8fc7\u6570\u503c\u53ef\u8ba1\u7b97\u4e14\u53ef\u89e3\u91ca\u7684\u65b0\u8fb9\u754c\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u5e73\u8861\u591a\u4efb\u52a1\u8bbe\u7f6e\u4e0b\u7684\u7075\u6d3b\u6027\u548c\u66f4\u5f3a\u7684\u4fdd\u8bc1\u3002", "result": "\u65b0\u8fb9\u754c\u63d0\u4f9b\u4e86\u6bd4\u4ee5\u5f80\u66f4\u5f3a\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u63ed\u793a\u4e0d\u5e73\u8861\u8bbe\u7f6e\u4e0e\u5e73\u8861\u8bbe\u7f6e\u7684\u4e0d\u540c\u7edf\u8ba1\u7279\u6027\u3002", "conclusion": "\u4e0d\u5e73\u8861\u591a\u4efb\u52a1\u8bbe\u7f6e\u5141\u8bb8\u4e24\u79cd\u98ce\u9669\u5b9a\u4e49\uff0c\u53d6\u51b3\u4e8e\u4efb\u52a1\u7684\u91cd\u8981\u6027\u548c\u6837\u672c\u6570\u91cf\u7684\u6743\u91cd\u5206\u914d\u3002", "keywords": "\u591a\u4efb\u52a1\u5b66\u4e60, \u5143\u5b66\u4e60, \u6cdb\u5316\u8fb9\u754c, \u4e0d\u5e73\u8861\u4efb\u52a1"}}
{"id": "2505.15088", "pdf": "https://arxiv.org/pdf/2505.15088", "abs": "https://arxiv.org/abs/2505.15088", "authors": ["Yuxuan Wang", "Jingshu Chen", "Qingyang Wang"], "title": "Leveraging Large Language Models for Command Injection Vulnerability Analysis in Python: An Empirical Study on Popular Open-Source Projects", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Command injection vulnerabilities are a significant security threat in\ndynamic languages like Python, particularly in widely used open-source projects\nwhere security issues can have extensive impact. With the proven effectiveness\nof Large Language Models(LLMs) in code-related tasks, such as testing,\nresearchers have explored their potential for vulnerabilities analysis. This\nstudy evaluates the potential of large language models (LLMs), such as GPT-4,\nas an alternative approach for automated testing for vulnerability detection.\nIn particular, LLMs have demonstrated advanced contextual understanding and\nadaptability, making them promising candidates for identifying nuanced security\nvulnerabilities within code. To evaluate this potential, we applied LLM-based\nanalysis to six high-profile GitHub projects-Django, Flask, TensorFlow,\nScikit-learn, PyTorch, and Langchain-each with over 50,000 stars and extensive\nadoption across software development and academic research. Our analysis\nassesses both the strengths and limitations of LLMs in detecting command\ninjection vulnerabilities, evaluating factors such as detection accuracy,\nefficiency, and practical integration into development workflows. In addition,\nwe provide a comparative analysis of different LLM tools to identify those most\nsuitable for security applications. Our findings offer guidance for developers\nand security researchers on leveraging LLMs as innovative and automated\napproaches to enhance software security.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4\uff09\u5728\u81ea\u52a8\u5316\u68c0\u6d4b\u547d\u4ee4\u6ce8\u5165\u6f0f\u6d1e\u4e2d\u7684\u6f5c\u529b\uff0c\u5206\u6790\u4e86\u5176\u57286\u4e2a\u77e5\u540d\u5f00\u6e90\u9879\u76ee\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u53d1\u8005\u5229\u7528LLM\u63d0\u5347\u8f6f\u4ef6\u5b89\u5168\u7684\u5efa\u8bae\u3002", "motivation": "\u52a8\u6001\u8bed\u8a00\uff08\u5982Python\uff09\u4e2d\u547d\u4ee4\u6ce8\u5165\u6f0f\u6d1e\u7684\u5b89\u5168\u5a01\u80c1\u663e\u8457\uff0c\u5c24\u5176\u662f\u5f00\u6e90\u9879\u76ee\u4e2d\u3002LLM\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u5176\u5728\u6f0f\u6d1e\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u5c06LLM\u5e94\u7528\u4e8e6\u4e2a\u9ad8\u661fGitHub\u9879\u76ee\uff08Django\u3001Flask\u7b49\uff09\uff0c\u8bc4\u4f30\u5176\u5728\u547d\u4ee4\u6ce8\u5165\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u51c6\u786e\u6027\u3001\u6548\u7387\u53ca\u5b9e\u9645\u96c6\u6210\u80fd\u529b\u3002", "result": "\u5206\u6790\u4e86LLM\u5728\u68c0\u6d4b\u547d\u4ee4\u6ce8\u5165\u6f0f\u6d1e\u4e2d\u7684\u4f18\u52bf\u548c\u5c40\u9650\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540cLLM\u5de5\u5177\u5728\u5b89\u5168\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "LLM\u53ef\u4f5c\u4e3a\u521b\u65b0\u7684\u81ea\u52a8\u5316\u5de5\u5177\u63d0\u5347\u8f6f\u4ef6\u5b89\u5168\uff0c\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u548c\u5b89\u5168\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "keywords": "\u547d\u4ee4\u6ce8\u5165\u6f0f\u6d1e, \u5927\u578b\u8bed\u8a00\u6a21\u578b, \u81ea\u52a8\u5316\u6d4b\u8bd5, \u8f6f\u4ef6\u5b89\u5168, Python"}}
{"id": "2505.15490", "pdf": "https://arxiv.org/pdf/2505.15490", "abs": "https://arxiv.org/abs/2505.15490", "authors": ["Isidora Jeknic", "Alex Duchnowski", "Alexander Koller"], "title": "Collaborative Problem-Solving in an Optimization Game", "categories": ["cs.CL"], "comment": "23 pages, 16 figures", "summary": "Dialogue agents that support human users in solving complex tasks have\nreceived much attention recently. Many such tasks are NP-hard optimization\nproblems that require careful collaborative exploration of the solution space.\nWe introduce a novel dialogue game in which the agents collaboratively solve a\ntwo-player Traveling Salesman problem, along with an agent that combines LLM\nprompting with symbolic mechanisms for state tracking and grounding. Our best\nagent solves 45% of games optimally in self-play. It also demonstrates an\nability to collaborate successfully with human users and generalize to\nunfamiliar graphs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u5bf9\u8bdd\u6e38\u620f\uff0c\u7528\u4e8e\u534f\u4f5c\u89e3\u51b3\u53cc\u4eba\u65c5\u884c\u5546\u95ee\u9898\uff0c\u7ed3\u5408\u4e86LLM\u63d0\u793a\u4e0e\u7b26\u53f7\u673a\u5236\uff0c\u6700\u4f18\u89e3\u51b3\u738745%\uff0c\u5e76\u80fd\u4e0e\u4eba\u7c7b\u7528\u6237\u6210\u529f\u534f\u4f5c\u53ca\u6cdb\u5316\u81f3\u964c\u751f\u56fe\u3002", "motivation": "\u652f\u6301\u4eba\u7c7b\u7528\u6237\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u7684\u5bf9\u8bdd\u4ee3\u7406\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u65c5\u884c\u5546\u95ee\u9898\u7b49NP\u96be\u95ee\u9898\u9700\u8981\u534f\u4f5c\u63a2\u7d22\u89e3\u7a7a\u95f4\u3002", "method": "\u8bbe\u8ba1\u4e86\u65b0\u5bf9\u8bdd\u6e38\u620f\uff0c\u7ed3\u5408LLM\u63d0\u793a\u4e0e\u7b26\u53f7\u673a\u5236\u8fdb\u884c\u72b6\u6001\u8ddf\u8e2a\u548c\u57fa\u7840\u652f\u6301\u3002", "result": "\u6700\u4f18\u89e3\u51b3\u7387\u4e3a45%\uff0c\u5e76\u80fd\u4e0e\u4eba\u7c7b\u7528\u6237\u534f\u4f5c\uff0c\u6cdb\u5316\u81f3\u964c\u751f\u56fe\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u534f\u4f5c\u89e3\u51b3\u590d\u6742\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u5bf9\u8bdd\u4ee3\u7406\uff0c\u65c5\u884c\u5546\u95ee\u9898\uff0cLLM\uff0c\u534f\u4f5c\uff0c\u7b26\u53f7\u673a\u5236"}}
{"id": "2505.15497", "pdf": "https://arxiv.org/pdf/2505.15497", "abs": "https://arxiv.org/abs/2505.15497", "authors": ["Frederik Baymler Mathiesen", "Nikolaus Vertovec", "Francesco Fabiano", "Luca Laurenti", "Alessandro Abate"], "title": "Certified Neural Approximations of Nonlinear Dynamics", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "first and second author contributed equally", "summary": "Neural networks hold great potential to act as approximate models of\nnonlinear dynamical systems, with the resulting neural approximations enabling\nverification and control of such systems. However, in safety-critical contexts,\nthe use of neural approximations requires formal bounds on their closeness to\nthe underlying system. To address this fundamental challenge, we propose a\nnovel, adaptive, and parallelizable verification method based on certified\nfirst-order models. Our approach provides formal error bounds on the neural\napproximations of dynamical systems, allowing them to be safely employed as\nsurrogates by interpreting the error bound as bounded disturbances acting on\nthe approximated dynamics. We demonstrate the effectiveness and scalability of\nour method on a range of established benchmarks from the literature, showing\nthat it outperforms the state-of-the-art. Furthermore, we highlight the\nflexibility of our framework by applying it to two novel scenarios not\npreviously explored in this context: neural network compression and an\nautoencoder-based deep learning architecture for learning Koopman operators,\nboth yielding compelling results.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u81ea\u9002\u5e94\u4e14\u53ef\u5e76\u884c\u5316\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u52a8\u6001\u7cfb\u7edf\u8fd1\u4f3c\u63d0\u4f9b\u4e86\u6b63\u5f0f\u8bef\u5dee\u754c\u9650\uff0c\u786e\u4fdd\u5176\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u53ef\u9760\u4f7f\u7528\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u975e\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\u7684\u8fd1\u4f3c\u6a21\u578b\u9700\u8981\u5f62\u5f0f\u5316\u7684\u8bef\u5dee\u8fb9\u754c\u4ee5\u786e\u4fdd\u5176\u53ef\u9760\u6027\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6b64\u7c7b\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u8ba4\u8bc1\u7684\u4e00\u9636\u6a21\u578b\uff0c\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u3001\u53ef\u5e76\u884c\u5316\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8bef\u5dee\u754c\u9650\u89e3\u91ca\u4e3a\u6709\u754c\u6270\u52a8\u6765\u5206\u6790\u795e\u7ecf\u7f51\u7edc\u7684\u8fd1\u4f3c\u6548\u679c\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5728\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\u548c\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684Koopman\u7b97\u5b50\u5b66\u4e60\u4e2d\u5c55\u793a\u4e86\u7075\u6d3b\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u52a8\u6001\u7cfb\u7edf\u8fd1\u4f3c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9a8c\u8bc1\u5de5\u5177\uff0c\u6269\u5c55\u4e86\u5176\u5728\u5b89\u5168\u548c\u590d\u6742\u573a\u666f\u4e2d\u7684\u5e94\u7528\u8303\u56f4\u3002", "keywords": "\u795e\u7ecf\u7f51\u7edc,\u52a8\u6001\u7cfb\u7edf,\u8bef\u5dee\u754c\u9650,\u5b89\u5168\u5173\u952e,\u9a8c\u8bc1\u65b9\u6cd5"}}
{"id": "2505.15501", "pdf": "https://arxiv.org/pdf/2505.15501", "abs": "https://arxiv.org/abs/2505.15501", "authors": ["Federico Ranaldi", "Andrea Zugarini", "Leonardo Ranaldi", "Fabio Massimo Zanzotto"], "title": "Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce the concept of protoknowledge to formalize and measure how\nsequences of tokens encoding Knowledge Graphs are internalized during\npretraining and utilized at inference time by Large Language Models (LLMs).\nIndeed, LLMs have demonstrated the ability to memorize vast amounts of token\nsequences during pretraining, and a central open question is how they leverage\nthis memorization as reusable knowledge through generalization. We then\ncategorize protoknowledge into lexical, hierarchical, and topological forms,\nvarying on the type of knowledge that needs to be activated. We measure\nprotoknowledge through Knowledge Activation Tasks (KATs), analyzing its general\nproperties such as semantic bias. We then investigate the impact of\nprotoknowledge on Text-to-SPARQL performance by varying prompting strategies\ndepending on input conditions. To this end, we adopt a novel analysis framework\nthat assesses whether model predictions align with the successful activation of\nthe relevant protoknowledge for each query. This methodology provides a\npractical tool to explore Semantic-Level Data Contamination and serves as an\neffective strategy for Closed-Pretraining models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u201cprotoknowledge\u201d\u7684\u6982\u5ff5\uff0c\u7528\u4e8e\u5f62\u5f0f\u5316\u548c\u8861\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u5982\u4f55\u5185\u5316\u77e5\u8bc6\u56fe\u8c31\u7684\u4ee4\u724c\u5e8f\u5217\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u5229\u7528\u8fd9\u4e9b\u77e5\u8bc6\u3002", "motivation": "\u7814\u7a76LLMs\u5982\u4f55\u901a\u8fc7\u6cdb\u5316\u5229\u7528\u9884\u8bad\u7ec3\u671f\u95f4\u8bb0\u5fc6\u7684\u5927\u91cf\u4ee4\u724c\u5e8f\u5217\u4f5c\u4e3a\u53ef\u91cd\u7528\u77e5\u8bc6\u3002", "method": "\u5b9a\u4e49\u4e86lexical\u3001hierarchical\u548ctopological\u4e09\u79cdprotoknowledge\u7c7b\u522b\uff0c\u5e76\u901a\u8fc7\u77e5\u8bc6\u6fc0\u6d3b\u4efb\u52a1\uff08KATs\uff09\u6d4b\u91cf\u5176\u7279\u5f81\u3002\u91c7\u7528\u65b0\u6846\u67b6\u5206\u6790protoknowledge\u5bf9Text-to-SPARQL\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u63d0\u4f9b\u4e86\u8861\u91cf\u8bed\u4e49\u504f\u89c1\u7b49protoknowledge\u7279\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u63a2\u7d22\u8bed\u4e49\u7ea7\u6570\u636e\u6c61\u67d3\u7684\u6709\u6548\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u95ed\u5f0f\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "keywords": "protoknowledge, LLMs, Knowledge Activation Tasks, Text-to-SPARQL, Semantic-Level Data Contamination"}}
{"id": "2505.15507", "pdf": "https://arxiv.org/pdf/2505.15507", "abs": "https://arxiv.org/abs/2505.15507", "authors": ["Mahesh Godavarti"], "title": "Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.IR", "20-XX, 08A02", "F.4.1; I.2"], "comment": "11 pages submitted to NeurIPS 2025", "summary": "We introduce a new algebraic structure for multi-dimensional compositional\nembeddings, built on directional non-commutative monoidal operators. The core\ncontribution of this work is this novel framework, which exhibits appealing\ntheoretical properties (associativity along each dimension and an interchange\nlaw ensuring global consistency) while remaining compatible with modern machine\nlearning architectures. Our construction defines a distinct composition\noperator circ_i for each axis i, ensuring associative combination along each\naxis without imposing global commutativity. Importantly, all axis-specific\noperators commute with one another, enforcing a global interchange law that\nenables consistent crossaxis compositions. This is, to our knowledge, the first\napproach that provides a common foundation that generalizes classical\nsequence-modeling paradigms (e.g., structured state-space models (SSMs) and\ntransformer self-attention) to a unified multi-dimensional framework. For\nexample, specific one-dimensional instances of our framework can recover the\nfamiliar affine transformation algebra, vanilla self-attention, and the\nSSM-style recurrence. The higher-dimensional generalizations naturally support\nrecursive, structure-aware operations in embedding spaces. We outline several\npotential applications unlocked by this structure-including structured\npositional encodings in Transformers, directional image embeddings, and\nsymbolic modeling of sequences or grids-indicating that it could inform future\ndeep learning model designs. We formally establish the algebraic properties of\nour framework and discuss efficient implementations. Finally, as our focus is\ntheoretical, we include no experiments here and defer empirical validation to\nfuture work, which we plan to undertake.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u7ef4\u7ec4\u5408\u5d4c\u5165\u7684\u4ee3\u6570\u7ed3\u6784\uff0c\u57fa\u4e8e\u65b9\u5411\u6027\u975e\u4ea4\u6362\u5355\u5b50\u8fd0\u7b97\u7b26\u3002", "motivation": "\u4e3a\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u67b6\u6784\u63d0\u4f9b\u4e00\u79cd\u5177\u6709\u7406\u8bba\u5438\u5f15\u529b\u7684\u591a\u7ef4\u6846\u67b6\uff0c\u7edf\u4e00\u5e76\u63a8\u5e7f\u7ecf\u5178\u5e8f\u5217\u5efa\u6a21\u8303\u5f0f\u3002", "method": "\u5b9a\u4e49\u6bcf\u4e2a\u8f74\u7684\u72ec\u7acb\u7ec4\u5408\u8fd0\u7b97\u7b26circ_i\uff0c\u786e\u4fdd\u5404\u8f74\u5173\u8054\u6027\u4e14\u5168\u5c40\u53ef\u4ea4\u6362\uff0c\u652f\u6301\u4e00\u81f4\u8de8\u8f74\u7ec4\u5408\u3002", "result": "\u8be5\u6846\u67b6\u9996\u6b21\u4e3a\u7ecf\u5178\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u7edf\u4e00\u7684\u591a\u7ef4\u57fa\u7840\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u9a8c\u8bc1\u5176\u4ee3\u6570\u6027\u8d28\u3002", "conclusion": "\u8fd9\u79cd\u7ed3\u6784\u6709\u671b\u5e94\u7528\u4e8e\u672a\u6765\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bbe\u8ba1\uff0c\u5982Transformer\u7684\u4f4d\u7f6e\u7f16\u7801\u548c\u56fe\u50cf\u5d4c\u5165\u3002", "keywords": "\u591a\u7ef4\u5d4c\u5165,\u975e\u4ea4\u6362\u8fd0\u7b97\u7b26,\u7ec4\u5408\u6846\u67b6,\u5e8f\u5217\u5efa\u6a21,\u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2505.15091", "pdf": "https://arxiv.org/pdf/2505.15091", "abs": "https://arxiv.org/abs/2505.15091", "authors": ["Qihang Yu", "Kairui Fu", "Shengyu Zhang", "Zheqi Lv", "Fan Wu", "Fei Wu"], "title": "ThinkRec: Thinking-based recommendation via LLM", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled more\nsemantic-aware recommendations through natural language generation. Existing\nLLM for recommendation (LLM4Rec) methods mostly operate in a System 1-like\nmanner, relying on superficial features to match similar items based on click\nhistory, rather than reasoning through deeper behavioral logic. This often\nleads to superficial and erroneous recommendations. Motivated by this, we\npropose ThinkRec, a thinking-based framework that shifts LLM4Rec from System 1\nto System 2 (rational system). Technically, ThinkRec introduces a thinking\nactivation mechanism that augments item metadata with keyword summarization and\ninjects synthetic reasoning traces, guiding the model to form interpretable\nreasoning chains that consist of analyzing interaction histories, identifying\nuser preferences, and making decisions based on target items. On top of this,\nwe propose an instance-wise expert fusion mechanism to reduce the reasoning\ndifficulty. By dynamically assigning weights to expert models based on users'\nlatent features, ThinkRec adapts its reasoning path to individual users,\nthereby enhancing precision and personalization. Extensive experiments on\nreal-world datasets demonstrate that ThinkRec significantly improves the\naccuracy and interpretability of recommendations. Our implementations are\navailable in anonymous Github: https://anonymous.4open.science/r/ThinkRec_LLM.", "AI": {"tldr": "ThinkRec\u662f\u4e00\u4e2a\u57fa\u4e8e\u601d\u8003\u7684\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u601d\u8003\u6fc0\u6d3b\u673a\u5236\u548c\u5b9e\u4f8b\u7ea7\u4e13\u5bb6\u878d\u5408\u673a\u5236\uff0c\u4ece\u6d45\u5c42\u5339\u914d\u8f6c\u5411\u6df1\u5c42\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u63a8\u8350\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u8350\u65b9\u6cd5\uff08LLM4Rec\uff09\u591a\u4f9d\u8d56\u6d45\u5c42\u7279\u5f81\u5339\u914d\uff0c\u7f3a\u4e4f\u6df1\u5c42\u884c\u4e3a\u903b\u8f91\u63a8\u7406\uff0c\u5bfc\u81f4\u63a8\u8350\u7ed3\u679c\u80a4\u6d45\u4e14\u9519\u8bef\u3002", "method": "ThinkRec\u5f15\u5165\u4e86\u601d\u8003\u6fc0\u6d3b\u673a\u5236\uff0c\u901a\u8fc7\u5173\u952e\u8bcd\u6458\u8981\u548c\u5408\u6210\u63a8\u7406\u8f68\u8ff9\u589e\u5f3a\u9879\u76ee\u5143\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u5b9e\u4f8b\u7ea7\u4e13\u5bb6\u878d\u5408\u673a\u5236\u52a8\u6001\u8c03\u6574\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cThinkRec\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u8350\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "ThinkRec\u901a\u8fc7\u4eceSystem 1\u8f6c\u5411System 2\u7684\u63a8\u7406\u65b9\u5f0f\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u7cbe\u51c6\u548c\u4e2a\u6027\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b\u3001\u63a8\u8350\u7cfb\u7edf\u3001\u6df1\u5c42\u63a8\u7406\u3001\u601d\u8003\u6fc0\u6d3b\u673a\u5236\u3001\u4e13\u5bb6\u878d\u5408"}}
{"id": "2505.15508", "pdf": "https://arxiv.org/pdf/2505.15508", "abs": "https://arxiv.org/abs/2505.15508", "authors": ["Prasoon Bajpai", "Tanmoy Chakraborty"], "title": "Multilingual Test-Time Scaling via Initial Thought Transfer", "categories": ["cs.CL"], "comment": "14 pages, 9 figures, 5 Tables", "summary": "Test-time scaling has emerged as a widely adopted inference-time strategy for\nboosting reasoning performance. However, its effectiveness has been studied\nalmost exclusively in English, leaving its behavior in other languages largely\nunexplored. We present the first systematic study of test-time scaling in\nmultilingual settings, evaluating DeepSeek-R1-Distill-LLama-8B and\nDeepSeek-R1-Distill-Qwen-7B across both high- and low-resource Latin-script\nlanguages. Our findings reveal that the relative gains from test-time scaling\nvary significantly across languages. Additionally, models frequently switch to\nEnglish mid-reasoning, even when operating under strictly monolingual prompts.\nWe further show that low-resource languages not only produce initial reasoning\nthoughts that differ significantly from English but also have lower internal\nconsistency across generations in their early reasoning. Building on our\nfindings, we introduce MITT (Multilingual Initial Thought Transfer), an\nunsupervised and lightweight reasoning prefix-tuning approach that transfers\nhigh-resource reasoning prefixes to enhance test-time scaling across all\nlanguages, addressing inconsistencies in multilingual reasoning performance.\nMITT significantly boosts DeepSeek-R1-Distill-Qwen-7B's reasoning performance,\nespecially for underrepresented languages.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u6548\u679c\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u8bed\u8a00\u95f4\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMITT\u7684\u65e0\u76d1\u7763\u524d\u7f00\u8c03\u4f18\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u6d4b\u8bd5\u65f6\u7f29\u653e\u867d\u5e7f\u6cdb\u7528\u4e8e\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u5176\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u884c\u4e3a\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u8bc4\u4f30\u4e86DeepSeek-R1-Distill-LLama-8B\u548cDeepSeek-R1-Distill-Qwen-7B\u5728\u9ad8\u3001\u4f4e\u8d44\u6e90\u62c9\u4e01\u8bed\u7cfb\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51faMITT\u65b9\u6cd5\u3002", "result": "\u4e0d\u540c\u8bed\u8a00\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u589e\u76ca\u5dee\u5f02\u663e\u8457\uff0cMITT\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6027\u80fd\u3002", "conclusion": "MITT\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u8bed\u8a00\u63a8\u7406\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "keywords": "\u6d4b\u8bd5\u65f6\u7f29\u653e, \u591a\u8bed\u8a00\u63a8\u7406, MITT, \u4f4e\u8d44\u6e90\u8bed\u8a00"}}
{"id": "2505.15511", "pdf": "https://arxiv.org/pdf/2505.15511", "abs": "https://arxiv.org/abs/2505.15511", "authors": ["Brandon Duderstadt", "Zach Nussbaum", "Laurens van der Maaten"], "title": "NOMAD Projection", "categories": ["cs.LG"], "comment": null, "summary": "The rapid adoption of generative AI has driven an explosion in the size of\ndatasets consumed and produced by AI models. Traditional methods for\nunstructured data visualization, such as t-SNE and UMAP, have not kept up with\nthe pace of dataset scaling. This presents a significant challenge for AI\nexplainability, which relies on methods such as t-SNE and UMAP for exploratory\ndata analysis. In this paper, we introduce Negative Or Mean Affinity\nDiscrimination (NOMAD) Projection, the first method for unstructured data\nvisualization via nonlinear dimensionality reduction that can run on multiple\nGPUs at train time. We provide theory that situates NOMAD Projection as an\napproximate upper bound on the InfoNC-t-SNE loss, and empirical results that\ndemonstrate NOMAD Projection's superior performance and speed profile compared\nto existing state-of-the-art methods. We demonstrate the scalability of NOMAD\nProjection by computing the first complete data map of Multilingual Wikipedia.", "AI": {"tldr": "NOMAD Projection\u662f\u4e00\u79cd\u9762\u5411\u975e\u7ed3\u6784\u5316\u6570\u636e\u7684\u975e\u7ebf\u6027\u964d\u7ef4\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u652f\u6301\u591aGPU\u8bad\u7ec3\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4\u6570\u636e\u96c6\u89c4\u6a21\u6025\u5267\u589e\u957f\uff0c\u4f20\u7edf\u53ef\u89c6\u5316\u65b9\u6cd5\uff08\u5982t-SNE\u548cUMAP\uff09\u96be\u4ee5\u5e94\u5bf9\uff0c\u5f71\u54cd\u4e86AI\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faNOMAD Projection\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u964d\u7ef4\u5b9e\u73b0\u975e\u7ed3\u6784\u5316\u6570\u636e\u7684\u53ef\u89c6\u5316\uff0c\u5e76\u652f\u6301\u591aGPU\u8bad\u7ec3\u3002", "result": "\u7406\u8bba\u8bc1\u660eNOMAD Projection\u662fInfoNC-t-SNE\u635f\u5931\u7684\u8fd1\u4f3c\u4e0a\u754c\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u6027\u80fd\u548c\u901f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u6210\u529f\u5e94\u7528\u4e8eMultilingual Wikipedia\u6570\u636e\u96c6\u3002", "conclusion": "NOMAD Projection\u4e3a\u5927\u89c4\u6a21\u975e\u7ed3\u6784\u5316\u6570\u636e\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u751f\u6210\u5f0fAI, \u6570\u636e\u53ef\u89c6\u5316, \u975e\u7ebf\u6027\u964d\u7ef4, NOMAD Projection, \u591aGPU\u8bad\u7ec3"}}
{"id": "2505.15524", "pdf": "https://arxiv.org/pdf/2505.15524", "abs": "https://arxiv.org/abs/2505.15524", "authors": ["Lang Gao", "Kaiyang Wan", "Wei Liu", "Chenxi Wang", "Zirui Song", "Zixiang Xu", "Yanbo Wang", "Veselin Stoyanov", "Xiuying Chen"], "title": "Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Bias in Large Language Models (LLMs) significantly undermines their\nreliability and fairness. We focus on a common form of bias: when two reference\nconcepts in the model's concept space, such as sentiment polarities (e.g.,\n\"positive\" and \"negative\"), are asymmetrically correlated with a third, target\nconcept, such as a reviewing aspect, the model exhibits unintended bias. For\ninstance, the understanding of \"food\" should not skew toward any particular\nsentiment. Existing bias evaluation methods assess behavioral differences of\nLLMs by constructing labeled data for different social groups and measuring\nmodel responses across them, a process that requires substantial human effort\nand captures only a limited set of social concepts. To overcome these\nlimitations, we propose BiasLens, a test-set-free bias analysis framework based\non the structure of the model's vector space. BiasLens combines Concept\nActivation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract\ninterpretable concept representations, and quantifies bias by measuring the\nvariation in representational similarity between the target concept and each of\nthe reference concepts. Even without labeled data, BiasLens shows strong\nagreement with traditional bias evaluation metrics (Spearman correlation r >\n0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect\nusing existing methods. For example, in simulated clinical scenarios, a\npatient's insurance status can cause the LLM to produce biased diagnostic\nassessments. Overall, BiasLens offers a scalable, interpretable, and efficient\nparadigm for bias discovery, paving the way for improving fairness and\ntransparency in LLMs.", "AI": {"tldr": "\u63d0\u51faBiasLens\u6846\u67b6\uff0c\u65e0\u6807\u6ce8\u6570\u636e\u5373\u53ef\u5206\u6790LLM\u4e2d\u7684\u504f\u89c1\uff0c\u901a\u8fc7\u5411\u91cf\u7a7a\u95f4\u7ed3\u6784\u91cf\u5316\u504f\u5dee\uff0c\u5c55\u793a\u51fa\u4e0e\u4f20\u7edf\u65b9\u6cd5\u9ad8\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u56e0\u6982\u5ff5\u7a7a\u95f4\u4e0d\u5bf9\u79f0\u5173\u8054\u5bfc\u81f4\u7684\u504f\u89c1\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u4e14\u8986\u76d6\u6709\u9650\u3002", "method": "\u7ed3\u5408\u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf\uff08CAVs\uff09\u4e0e\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\uff0c\u63d0\u53d6\u53ef\u89e3\u91ca\u6982\u5ff5\u8868\u793a\uff0c\u901a\u8fc7\u8868\u793a\u76f8\u4f3c\u6027\u53d8\u5316\u91cf\u5316\u504f\u89c1\u3002", "result": "BiasLens\u4e0e\u4f20\u7edf\u504f\u89c1\u8bc4\u6d4b\u6307\u6807\u9ad8\u5ea6\u4e00\u81f4\uff08Spearman\u76f8\u5173\u6027r>0.85\uff09\uff0c\u5e76\u80fd\u63ed\u793a\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4b\u7684\u504f\u89c1\u3002", "conclusion": "BiasLens\u4e3a\u504f\u89c1\u53d1\u73b0\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u8303\u5f0f\uff0c\u6709\u52a9\u4e8e\u63d0\u5347LLM\u7684\u516c\u5e73\u6027\u4e0e\u900f\u660e\u5ea6\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u504f\u89c1\u5206\u6790, \u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf, \u7a00\u758f\u81ea\u7f16\u7801\u5668, \u516c\u5e73\u6027"}}
{"id": "2505.15514", "pdf": "https://arxiv.org/pdf/2505.15514", "abs": "https://arxiv.org/abs/2505.15514", "authors": ["Soham Sane"], "title": "AM-PPO: (Advantage) Alpha-Modulation with Proximal Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "17 pages, 4 Tables, 9 Figures, 11 equations", "summary": "Proximal Policy Optimization (PPO) is a widely used reinforcement learning\nalgorithm that heavily relies on accurate advantage estimates for stable and\nefficient training. However, raw advantage signals can exhibit significant\nvariance, noise, and scale-related issues, impeding optimal learning\nperformance. To address this challenge, we introduce Advantage Modulation PPO\n(AM-PPO), a novel enhancement of PPO that adaptively modulates advantage\nestimates using a dynamic, non-linear scaling mechanism. This adaptive\nmodulation employs an alpha controller that dynamically adjusts the scaling\nfactor based on evolving statistical properties of the advantage signals, such\nas their norm, variance, and a predefined target saturation level. By\nincorporating a tanh-based gating function driven by these adaptively scaled\nadvantages, AM-PPO reshapes the advantage signals to stabilize gradient updates\nand improve the conditioning of the policy gradient landscape. Crucially, this\nmodulation also influences value function training by providing consistent and\nadaptively conditioned learning targets. Empirical evaluations across standard\ncontinuous control benchmarks demonstrate that AM-PPO achieves superior reward\ntrajectories, exhibits sustained learning progression, and significantly\nreduces the clipping required by adaptive optimizers. These findings underscore\nthe potential of advantage modulation as a broadly applicable technique for\nenhancing reinforcement learning optimization.", "AI": {"tldr": "AM-PPO\u901a\u8fc7\u52a8\u6001\u975e\u7ebf\u6027\u7f29\u653e\u673a\u5236\u6539\u8fdbPPO\u7684\u4f18\u52bf\u4f30\u8ba1\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f18\u52bf\u4f30\u8ba1\u7684\u65b9\u5dee\u3001\u566a\u58f0\u548c\u5c3a\u5ea6\u95ee\u9898\u5f71\u54cdPPO\u8bad\u7ec3\u6027\u80fd\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5f15\u5165\u52a8\u6001\u03b1\u63a7\u5236\u5668\uff0c\u57fa\u4e8e\u4f18\u52bf\u4fe1\u53f7\u7684\u7edf\u8ba1\u7279\u6027\uff08\u5982\u8303\u6570\u3001\u65b9\u5dee\uff09\u8fdb\u884c\u975e\u7ebf\u6027\u7f29\u653e\uff0c\u5e76\u4f7f\u7528tanh\u95e8\u63a7\u51fd\u6570\u4f18\u5316\u68af\u5ea6\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u663e\u793aAM-PPO\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5b66\u4e60\u8fc7\u7a0b\u7a33\u5b9a\uff0c\u51cf\u5c11\u4e86\u81ea\u9002\u5e94\u4f18\u5316\u5668\u7684\u88c1\u526a\u9700\u6c42\u3002", "conclusion": "\u4f18\u52bf\u8c03\u5236\u662f\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684\u901a\u7528\u6280\u672f\u3002", "keywords": "PPO, \u4f18\u52bf\u4f30\u8ba1, \u5f3a\u5316\u5b66\u4e60, \u52a8\u6001\u7f29\u653e, \u68af\u5ea6\u4f18\u5316"}}
{"id": "2505.15098", "pdf": "https://arxiv.org/pdf/2505.15098", "abs": "https://arxiv.org/abs/2505.15098", "authors": ["Yihang Li", "Tianle Zhang", "Xuelong Wei", "Jiayi Li", "Lin Zhao", "Dongchi Huang", "Zhirui Fang", "Minhua Zheng", "Wenjun Dai", "Xiaodong He"], "title": "Object-Focus Actor for Data-efficient Robot Generalization Dexterous Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robot manipulation learning from human demonstrations offers a rapid means to\nacquire skills but often lacks generalization across diverse scenes and object\nplacements. This limitation hinders real-world applications, particularly in\ncomplex tasks requiring dexterous manipulation. Vision-Language-Action (VLA)\nparadigm leverages large-scale data to enhance generalization. However, due to\ndata scarcity, VLA's performance remains limited. In this work, we introduce\nObject-Focus Actor (OFA), a novel, data-efficient approach for generalized\ndexterous manipulation. OFA exploits the consistent end trajectories observed\nin dexterous manipulation tasks, allowing for efficient policy training. Our\nmethod employs a hierarchical pipeline: object perception and pose estimation,\npre-manipulation pose arrival and OFA policy execution. This process ensures\nthat the manipulation is focused and efficient, even in varied backgrounds and\npositional layout. Comprehensive real-world experiments across seven tasks\ndemonstrate that OFA significantly outperforms baseline methods in both\npositional and background generalization tests. Notably, OFA achieves robust\nperformance with only 10 demonstrations, highlighting its data efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOFA\u7684\u6570\u636e\u9ad8\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u4e00\u81f4\u7684\u76ee\u6807\u8f68\u8ff9\u6765\u63d0\u5347\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u4ece\u4eba\u7c7b\u6f14\u793a\u4e2d\u5b66\u4e60\u7075\u5de7\u64cd\u4f5c\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u6570\u636e\u7a00\u7f3a\u7684\u9650\u5236\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6d41\u7a0b\uff1a\u76ee\u6807\u611f\u77e5\u4e0e\u4f4d\u59ff\u4f30\u8ba1\u3001\u9884\u64cd\u4f5c\u4f4d\u59ff\u5230\u8fbe\u548cOFA\u7b56\u7565\u6267\u884c\u3002", "result": "\u57287\u4e2a\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5728\u4ec5\u970010\u6b21\u6f14\u793a\u65f6\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "OFA\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u7075\u5de7\u64cd\u4f5c\u65b9\u6cd5\u3002", "keywords": "\u673a\u5668\u4eba\u64cd\u4f5c, \u6cdb\u5316\u5b66\u4e60, \u6570\u636e\u6548\u7387, \u7075\u5de7\u64cd\u4f5c, \u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c"}}
{"id": "2505.15553", "pdf": "https://arxiv.org/pdf/2505.15553", "abs": "https://arxiv.org/abs/2505.15553", "authors": ["Angelie Kraft", "Judith Simon", "Sonja Schimmler"], "title": "Social Bias in Popular Question-Answering Benchmarks", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Question-answering (QA) and reading comprehension (RC) benchmarks are\nessential for assessing the capabilities of large language models (LLMs) in\nretrieving and reproducing knowledge. However, we demonstrate that popular QA\nand RC benchmarks are biased and do not cover questions about different\ndemographics or regions in a representative way, potentially due to a lack of\ndiversity of those involved in their creation. We perform a qualitative content\nanalysis of 30 benchmark papers and a quantitative analysis of 20 respective\nbenchmark datasets to learn (1) who is involved in the benchmark creation, (2)\nhow social bias is addressed or prevented, and (3) whether the demographics of\nthe creators and annotators correspond to particular biases in the content.\nMost analyzed benchmark papers provided insufficient information regarding the\nstakeholders involved in benchmark creation, particularly the annotators.\nNotably, just one of the benchmark papers explicitly reported measures taken to\naddress social representation issues. Moreover, the data analysis revealed\ngender, religion, and geographic biases across a wide range of encyclopedic,\ncommonsense, and scholarly benchmarks. More transparent and bias-aware QA and\nRC benchmark creation practices are needed to facilitate better scrutiny and\nincentivize the development of fairer LLMs.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u6d41\u884c\u7684\u95ee\u7b54\uff08QA\uff09\u548c\u9605\u8bfb\u7406\u89e3\uff08RC\uff09\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u504f\u89c1\uff0c\u4e14\u672a\u80fd\u516c\u5e73\u8986\u76d6\u4e0d\u540c\u4eba\u7fa4\u6216\u5730\u533a\u7684\u95ee\u9898\uff0c\u6839\u6e90\u5728\u4e8e\u521b\u5efa\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u591a\u6837\u6027\u3002\u901a\u8fc7\u5bf930\u7bc7\u8bba\u6587\u548c20\u4e2a\u6570\u636e\u96c6\u7684\u5206\u6790\uff0c\u53d1\u73b0\u5927\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u672a\u900f\u660e\u62ab\u9732\u521b\u5efa\u8005\u4fe1\u606f\uff0c\u4e14\u4ec5\u4e00\u7bc7\u660e\u786e\u5c1d\u8bd5\u89e3\u51b3\u793e\u4f1a\u4ee3\u8868\u6027\u504f\u89c1\u3002\u5efa\u8bae\u9700\u8981\u66f4\u900f\u660e\u4e14\u6ce8\u91cd\u504f\u89c1\u7684\u57fa\u51c6\u6d4b\u8bd5\u521b\u5efa\u5b9e\u8df5\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u80fd\u529b\u4f9d\u8d56\u4e8eQA\u548cRC\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u53ef\u80fd\u5b58\u5728\u504f\u89c1\uff0c\u672a\u80fd\u516c\u5e73\u4ee3\u8868\u4e0d\u540c\u4eba\u7fa4\u6216\u5730\u533a\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u8868\u73b0\u7684\u4e0d\u516c\u5e73\u6027\u3002", "method": "\u5bf930\u7bc7\u57fa\u51c6\u6d4b\u8bd5\u8bba\u6587\u8fdb\u884c\u5b9a\u6027\u5185\u5bb9\u5206\u6790\uff0c\u5e76\u5bf920\u4e2a\u76f8\u5173\u6570\u636e\u96c6\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\uff0c\u91cd\u70b9\u5173\u6ce8\u521b\u5efa\u8005\u591a\u6837\u6027\u3001\u793e\u4f1a\u504f\u89c1\u5904\u7406\u65b9\u5f0f\u4ee5\u53ca\u521b\u5efa\u8005\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\u4e0e\u5185\u5bb9\u504f\u89c1\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u5927\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u8bba\u6587\u672a\u5145\u5206\u62ab\u9732\u521b\u5efa\u8005\u4fe1\u606f\uff0c\u4ec5\u67091\u7bc7\u8bba\u6587\u660e\u786e\u62a5\u544a\u4e86\u89e3\u51b3\u793e\u4f1a\u4ee3\u8868\u6027\u95ee\u9898\u7684\u63aa\u65bd\u3002\u6570\u636e\u5206\u6790\u663e\u793a\uff0c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b58\u5728\u6027\u522b\u3001\u5b97\u6559\u548c\u5730\u7406\u504f\u89c1\u3002", "conclusion": "\u9700\u8981\u66f4\u900f\u660e\u4e14\u6ce8\u91cd\u504f\u89c1\u7684\u57fa\u51c6\u6d4b\u8bd5\u521b\u5efa\u5b9e\u8df5\uff0c\u4ee5\u786e\u4fdd\u6a21\u578b\u5f00\u53d1\u7684\u516c\u5e73\u6027\u548c\u53ef\u5ba1\u67e5\u6027\u3002", "keywords": "QA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9605\u8bfb\u7406\u89e3\uff0c\u793e\u4f1a\u504f\u89c1\uff0c\u591a\u6837\u6027\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.15516", "pdf": "https://arxiv.org/pdf/2505.15516", "abs": "https://arxiv.org/abs/2505.15516", "authors": ["Christiaan Meijer", "E. G. Patrick Bos"], "title": "Explainable embeddings with Distance Explainer", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "68T99", "I.2.m"], "comment": "33 pages, 19 figures. Submitted to JMLR. Method implementation:\n  https://research-software-directory.org/software/distance-explainer", "summary": "While eXplainable AI (XAI) has advanced significantly, few methods address\ninterpretability in embedded vector spaces where dimensions represent complex\nabstractions. We introduce Distance Explainer, a novel method for generating\nlocal, post-hoc explanations of embedded spaces in machine learning models. Our\napproach adapts saliency-based techniques from RISE to explain the distance\nbetween two embedded data points by assigning attribution values through\nselective masking and distance-ranked mask filtering. We evaluate Distance\nExplainer on cross-modal embeddings (image-image and image-caption pairs) using\nestablished XAI metrics including Faithfulness, Sensitivity/Robustness, and\nRandomization. Experiments with ImageNet and CLIP models demonstrate that our\nmethod effectively identifies features contributing to similarity or\ndissimilarity between embedded data points while maintaining high robustness\nand consistency. We also explore how parameter tuning, particularly mask\nquantity and selection strategy, affects explanation quality. This work\naddresses a critical gap in XAI research and enhances transparency and\ntrustworthiness in deep learning applications utilizing embedded spaces.", "AI": {"tldr": "Distance Explainer \u662f\u4e00\u79cd\u65b0\u9896\u7684\u672c\u5730\u4e8b\u540e\u89e3\u91ca\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u89e3\u91ca\u5d4c\u5165\u5411\u91cf\u7a7a\u95f4\u4e2d\u7684\u8ddd\u79bb\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u63a9\u7801\u548c\u8ddd\u79bb\u6392\u540d\u63a9\u7801\u8fc7\u6ee4\u751f\u6210\u89e3\u91ca\uff0c\u63d0\u9ad8\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u900f\u660e\u6027\u3002", "motivation": "\u5f53\u524d\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u65b9\u6cd5\u8f83\u5c11\u5173\u6ce8\u5d4c\u5165\u5411\u91cf\u7a7a\u95f4\u7684\u89e3\u91ca\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5bf9\u590d\u6742\u62bd\u8c61\u7684\u7ef4\u5ea6\u8fdb\u884c\u89e3\u91ca\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u900f\u660e\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u5f15\u5165 Distance Explainer\uff0c\u5229\u7528 RISE \u7684\u663e\u8457\u6027\u6280\u672f\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u63a9\u7801\u548c\u8ddd\u79bb\u6392\u540d\u63a9\u7801\u8fc7\u6ee4\u751f\u6210\u5d4c\u5165\u7a7a\u95f4\u4e2d\u4e24\u70b9\u8ddd\u79bb\u7684\u89e3\u91ca\u3002", "result": "\u5728 ImageNet \u548c CLIP \u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u5d4c\u5165\u7a7a\u95f4\u4e2d\u76f8\u4f3c\u6216\u4e0d\u76f8\u4f3c\u7684\u7279\u5f81\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u9c81\u68d2\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "Distance Explainer \u89e3\u51b3\u4e86 XAI \u7814\u7a76\u4e2d\u7684\u5173\u952e\u7a7a\u767d\uff0c\u63d0\u5347\u4e86\u57fa\u4e8e\u5d4c\u5165\u7a7a\u95f4\u7684\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u7684\u900f\u660e\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "keywords": "\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd, \u5d4c\u5165\u7a7a\u95f4, \u8ddd\u79bb\u89e3\u91ca, \u663e\u8457\u6027\u6280\u672f, \u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2505.15554", "pdf": "https://arxiv.org/pdf/2505.15554", "abs": "https://arxiv.org/abs/2505.15554", "authors": ["Wendi Zhou", "Ameer Saadat-Yazdi", "Nadin K\u00f6kciyan"], "title": "DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion", "categories": ["cs.CL", "cs.AI"], "comment": "ArgMining 2025 CQs-Gen shared task", "summary": "Critical questions are essential resources to provoke critical thinking when\nencountering an argumentative text. We present our system for the Critical\nQuestions Generation (CQs-Gen) Shared Task at ArgMining 2025. Our approach\nleverages large language models (LLMs) with chain-of-thought prompting to\ngenerate critical questions guided by Walton's argumentation schemes. For each\ninput intervention, we conversationally prompt LLMs to instantiate the\ncorresponding argument scheme template to first obtain structured arguments,\nand then generate relevant critical questions. Following this, we rank all the\navailable critical questions by prompting LLMs to select the top 3 most helpful\nquestions based on the original intervention text. This combination of\nstructured argumentation theory and step-by-step reasoning enables the\ngeneration of contextually relevant and diverse critical questions. Our\npipeline achieves competitive performance in the final test set, showing its\npotential to foster critical thinking given argumentative text and detect\nmissing or uninformed claims. Code available at\n\\href{https://git.ecdf.ed.ac.uk/s2236454/DayDreamer-CQs-Gen}{DayDreamer}.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7ed3\u5408\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u548c\u6c83\u5c14\u987f\u8bba\u8bc1\u65b9\u6848\uff0c\u751f\u6210\u9488\u5bf9\u8bba\u8fa9\u6587\u672c\u7684\u5173\u952e\u6027\u95ee\u9898\u3002\u7cfb\u7edf\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u52a9\u4e8e\u6fc0\u53d1\u6279\u5224\u6027\u601d\u7ef4\u548c\u68c0\u6d4b\u7f3a\u5931\u8bba\u70b9\u3002", "motivation": "\u5173\u952e\u6027\u95ee\u9898\u80fd\u6fc0\u53d1\u6279\u5224\u6027\u601d\u7ef4\uff0c\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u6b64\u7c7b\u95ee\u9898\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528LLMs\u548c\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\uff0c\u57fa\u4e8e\u6c83\u5c14\u987f\u8bba\u8bc1\u65b9\u6848\u751f\u6210\u7ed3\u6784\u5316\u8bba\u8fa9\uff0c\u5e76\u7b5b\u9009\u51fa\u6700\u76f8\u5173\u76843\u4e2a\u95ee\u9898\u3002", "result": "\u7cfb\u7edf\u5728\u6d4b\u8bd5\u96c6\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u53ef\u6709\u6548\u8bc6\u522b\u7f3a\u5931\u8bba\u70b9\u3002", "conclusion": "\u7ed3\u5408\u7ed3\u6784\u5316\u8bba\u8fa9\u7406\u8bba\u548c\u5206\u6b65\u63a8\u7406\uff0c\u7cfb\u7edf\u80fd\u751f\u6210\u76f8\u5173\u4e14\u591a\u6837\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u6279\u5224\u6027\u601d\u7ef4\u3002", "keywords": "\u5173\u952e\u6027\u95ee\u9898\u751f\u6210\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\uff0c\u6c83\u5c14\u987f\u8bba\u8bc1\u65b9\u6848\uff0c\u6279\u5224\u6027\u601d\u7ef4"}}
{"id": "2505.15544", "pdf": "https://arxiv.org/pdf/2505.15544", "abs": "https://arxiv.org/abs/2505.15544", "authors": ["Haruki Settai", "Naoya Takeishi", "Takehisa Yairi"], "title": "A Temporal Difference Method for Stochastic Continuous Dynamics", "categories": ["cs.LG"], "comment": null, "summary": "For continuous systems modeled by dynamical equations such as ODEs and SDEs,\nBellman's principle of optimality takes the form of the Hamilton-Jacobi-Bellman\n(HJB) equation, which provides the theoretical target of reinforcement learning\n(RL). Although recent advances in RL successfully leverage this formulation,\nthe existing methods typically assume the underlying dynamics are known a\npriori because they need explicit access to the coefficient functions of\ndynamical equations to update the value function following the HJB equation. We\naddress this inherent limitation of HJB-based RL; we propose a model-free\napproach still targeting the HJB equation and propose the corresponding\ntemporal difference method. We demonstrate its potential advantages over\ntransition kernel-based formulations, both qualitatively and empirically. The\nproposed formulation paves the way toward bridging stochastic optimal control\nand model-free reinforcement learning.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHamilton-Jacobi-Bellman\uff08HJB\uff09\u65b9\u7a0b\u7684\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u9700\u9884\u5148\u77e5\u9053\u52a8\u529b\u5b66\u65b9\u7a0b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5176\u4f18\u4e8e\u57fa\u4e8e\u8f6c\u79fb\u6838\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684HJB\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u52a8\u529b\u5b66\u65b9\u7a0b\u5df2\u77e5\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u6a21\u578b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9HJB\u65b9\u7a0b\u7684\u65e0\u6a21\u578b\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u76f8\u5e94\u7684\u65f6\u95f4\u5dee\u5206\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u5747\u4f18\u4e8e\u57fa\u4e8e\u8f6c\u79fb\u6838\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u968f\u673a\u6700\u4f18\u63a7\u5236\u4e0e\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60\uff1bHJB\u65b9\u7a0b\uff1b\u968f\u673a\u6700\u4f18\u63a7\u5236\uff1b\u65e0\u6a21\u578b\u65b9\u6cd5"}}
{"id": "2505.15556", "pdf": "https://arxiv.org/pdf/2505.15556", "abs": "https://arxiv.org/abs/2505.15556", "authors": ["Ana-Maria Bucur", "Marcos Zampieri", "Tharindu Ranasinghe", "Fabio Crestani"], "title": "A Survey on Multilingual Mental Disorders Detection from Social Media Data", "categories": ["cs.CL"], "comment": null, "summary": "The increasing prevalence of mental health disorders globally highlights the\nurgent need for effective digital screening methods that can be used in\nmultilingual contexts. Most existing studies, however, focus on English data,\noverlooking critical mental health signals that may be present in non-English\ntexts. To address this important gap, we present the first survey on the\ndetection of mental health disorders using multilingual social media data. We\ninvestigate the cultural nuances that influence online language patterns and\nself-disclosure behaviors, and how these factors can impact the performance of\nNLP tools. Additionally, we provide a comprehensive list of multilingual data\ncollections that can be used for developing NLP models for mental health\nscreening. Our findings can inform the design of effective multilingual mental\nhealth screening tools that can meet the needs of diverse populations,\nultimately improving mental health outcomes on a global scale.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u591a\u8bed\u8a00\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u68c0\u6d4b\u5fc3\u7406\u5065\u5eb7\u969c\u788d\u7684\u7814\u7a76\uff0c\u5f3a\u8c03\u4e86\u73b0\u6709\u7814\u7a76\u591a\u4ee5\u82f1\u6587\u6570\u636e\u4e3a\u4e3b\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6\u8bed\u8a00\u4e2d\u7684\u5fc3\u7406\u5065\u5eb7\u4fe1\u53f7\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u8bed\u8a00\u6570\u636e\u6536\u96c6\u7684\u5168\u9762\u5217\u8868\u3002", "motivation": "\u5168\u7403\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u7684\u589e\u52a0\u9700\u8981\u6709\u6548\u7684\u591a\u8bed\u8a00\u6570\u5b57\u7b5b\u67e5\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u82f1\u6587\u6570\u636e\uff0c\u672a\u80fd\u6355\u6349\u5176\u4ed6\u8bed\u8a00\u4e2d\u7684\u5173\u952e\u4fe1\u53f7\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u591a\u8bed\u8a00\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4e2d\u7684\u6587\u5316\u5dee\u5f02\u5bf9\u8bed\u8a00\u6a21\u5f0f\u548c\u81ea\u6211\u62ab\u9732\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u7814\u7a76\u5982\u4f55\u4f18\u5316NLP\u5de5\u5177\u7684\u6027\u80fd\u3002", "result": "\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u5173\u4e8e\u591a\u8bed\u8a00\u5fc3\u7406\u5065\u5eb7\u969c\u788d\u68c0\u6d4b\u7684\u8c03\u67e5\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u7528\u4e8e\u5f00\u53d1NLP\u6a21\u578b\u7684\u591a\u8bed\u8a00\u6570\u636e\u96c6\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u6709\u6548\u7684\u591a\u8bed\u8a00\u5fc3\u7406\u5065\u5eb7\u7b5b\u67e5\u5de5\u5177\u63d0\u4f9b\u4e86\u91cd\u8981\u4f9d\u636e\uff0c\u6709\u52a9\u4e8e\u6539\u5584\u5168\u7403\u5fc3\u7406\u5065\u5eb7\u72b6\u51b5\u3002", "keywords": "\u5fc3\u7406\u5065\u5eb7, \u591a\u8bed\u8a00, \u793e\u4ea4\u5a92\u4f53, NLP, \u6587\u5316\u5dee\u5f02"}}
{"id": "2505.15547", "pdf": "https://arxiv.org/pdf/2505.15547", "abs": "https://arxiv.org/abs/2505.15547", "authors": ["Adrian Arnaiz-Rodriguez", "Federico Errica"], "title": "Oversmoothing, \"Oversquashing\", Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "After a renaissance phase in which researchers revisited the message-passing\nparadigm through the lens of deep learning, the graph machine learning\ncommunity shifted its attention towards a deeper and practical understanding of\nmessage-passing's benefits and limitations. In this position paper, we notice\nhow the fast pace of progress around the topics of oversmoothing and\noversquashing, the homophily-heterophily dichotomy, and long-range tasks, came\nwith the consolidation of commonly accepted beliefs and assumptions that are\nnot always true nor easy to distinguish from each other. We argue that this has\nled to ambiguities around the investigated problems, preventing researchers\nfrom focusing on and addressing precise research questions while causing a good\namount of misunderstandings. Our contribution wants to make such common beliefs\nexplicit and encourage critical thinking around these topics, supported by\nsimple but noteworthy counterexamples. The hope is to clarify the distinction\nbetween the different issues and promote separate but intertwined research\ndirections to address them.", "AI": {"tldr": "\u8be5\u7acb\u573a\u8bba\u6587\u63a2\u8ba8\u4e86\u56fe\u673a\u5668\u5b66\u4e60\u4e2d\u6d88\u606f\u4f20\u9012\u8303\u5f0f\u7684\u76ca\u5904\u4e0e\u5c40\u9650\u6027\uff0c\u6307\u51fa\u4e86\u7814\u7a76\u4e2d\u5e38\u89c1\u7684\u8bef\u89e3\u548c\u6a21\u7cca\u4e4b\u5904\uff0c\u5e76\u63d0\u4f9b\u4e86\u53cd\u4f8b\u4ee5\u4fc3\u8fdb\u6279\u5224\u6027\u601d\u7ef4\u3002", "motivation": "\u968f\u7740\u56fe\u673a\u5668\u5b66\u4e60\u793e\u533a\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u56f4\u7ed5\u6d88\u606f\u4f20\u9012\u7684\u7814\u7a76\u4e2d\u51fa\u73b0\u4e86\u4e00\u4e9b\u88ab\u5e7f\u6cdb\u63a5\u53d7\u4f46\u672a\u5fc5\u51c6\u786e\u7684\u5047\u8bbe\u548c\u4fe1\u5ff5\u3002\u8fd9\u4e9b\u6a21\u7cca\u6027\u963b\u788d\u4e86\u7814\u7a76\u8005\u5bf9\u5177\u4f53\u95ee\u9898\u7684\u805a\u7126\u4e0e\u89e3\u51b3\u3002", "method": "\u5206\u6790\u5e38\u89c1\u7684\u8bef\u89e3\u548c\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7\u7b80\u5355\u7684\u53cd\u4f8b\u8fdb\u884c\u9a8c\u8bc1\u548c\u6f84\u6e05\u3002", "result": "\u63ed\u793a\u4e86\u7814\u7a76\u4e2d\u7684\u6a21\u7cca\u6027\u548c\u8bef\u89e3\uff0c\u660e\u786e\u4e86\u4e0d\u540c\u95ee\u9898\u4e4b\u95f4\u7684\u533a\u522b\u3002", "conclusion": "\u63d0\u5021\u5bf9\u8fd9\u4e9b\u95ee\u9898\u91c7\u53d6\u6279\u5224\u6027\u601d\u7ef4\uff0c\u63a8\u52a8\u72ec\u7acb\u4f46\u76f8\u4e92\u5173\u8054\u7684\u7814\u7a76\u65b9\u5411\u3002", "keywords": "\u56fe\u673a\u5668\u5b66\u4e60, \u6d88\u606f\u4f20\u9012, \u6279\u5224\u6027\u601d\u7ef4, \u53cd\u4f8b, \u7814\u7a76\u6a21\u7cca\u6027"}}
{"id": "2505.15561", "pdf": "https://arxiv.org/pdf/2505.15561", "abs": "https://arxiv.org/abs/2505.15561", "authors": ["Florin Cuconasu", "Simone Filice", "Guy Horowitz", "Yoelle Maarek", "Fabrizio Silvestri"], "title": "Do RAG Systems Suffer From Positional Bias?", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Retrieval Augmented Generation enhances LLM accuracy by adding passages\nretrieved from an external corpus to the LLM prompt. This paper investigates\nhow positional bias - the tendency of LLMs to weight information differently\nbased on its position in the prompt - affects not only the LLM's capability to\ncapitalize on relevant passages, but also its susceptibility to distracting\npassages. Through extensive experiments on three benchmarks, we show how\nstate-of-the-art retrieval pipelines, while attempting to retrieve relevant\npassages, systematically bring highly distracting ones to the top ranks, with\nover 60% of queries containing at least one highly distracting passage among\nthe top-10 retrieved passages. As a result, the impact of the LLM positional\nbias, which in controlled settings is often reported as very prominent by\nrelated works, is actually marginal in real scenarios since both relevant and\ndistracting passages are, in turn, penalized. Indeed, our findings reveal that\nsophisticated strategies that attempt to rearrange the passages based on LLM\npositional preferences do not perform better than random shuffling.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u4f4d\u7f6e\u504f\u89c1\u5bf9LLM\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5b9e\u9645\u573a\u666f\u4e2d\u7531\u4e8e\u76f8\u5173\u548c\u5e72\u6270\u6027\u6bb5\u843d\u5e76\u5b58\uff0c\u4f4d\u7f6e\u504f\u89c1\u7684\u5b9e\u9645\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u7814\u7a76\u4f4d\u7f6e\u504f\u89c1\u5982\u4f55\u5f71\u54cdLLM\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u5bf9\u76f8\u5173\u6bb5\u843d\u7684\u5229\u7528\u53ca\u5176\u5bf9\u5e72\u6270\u6027\u6bb5\u843d\u7684\u654f\u611f\u6027\u3002", "method": "\u901a\u8fc7\u5728\u4e09\u4e2a\u57fa\u51c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5206\u6790\u68c0\u7d22\u7ba1\u9053\u8fd4\u56de\u7684\u6bb5\u843d\u53ca\u5176\u5bf9LLM\u8f93\u51fa\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u68c0\u7d22\u7ba1\u9053\u5728\u68c0\u7d22\u76f8\u5173\u6bb5\u843d\u65f6\u5f80\u5f80\u4f1a\u5f15\u5165\u9ad8\u5e72\u6270\u6027\u6bb5\u843d\uff0c60%\u7684\u67e5\u8be2\u5728top-10\u6bb5\u843d\u4e2d\u5305\u542b\u81f3\u5c11\u4e00\u4e2a\u9ad8\u5e72\u6270\u6027\u6bb5\u843d\uff0c\u800cLLM\u7684\u4f4d\u7f6e\u504f\u89c1\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5f71\u54cd\u6709\u9650\u3002", "conclusion": "\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u7531\u4e8e\u76f8\u5173\u548c\u5e72\u6270\u6027\u6bb5\u843d\u5e76\u5b58\uff0c\u4f4d\u7f6e\u504f\u89c1\u7684\u5f71\u54cd\u88ab\u524a\u5f31\uff0c\u8bd5\u56fe\u6839\u636e\u4f4d\u7f6e\u504f\u597d\u91cd\u65b0\u6392\u5217\u6bb5\u843d\u7684\u590d\u6742\u7b56\u7565\u5e76\u4e0d\u4f18\u4e8e\u968f\u673a\u6392\u5217\u3002", "keywords": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u4f4d\u7f6e\u504f\u89c1\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u5e72\u6270\u6027\u6bb5\u843d"}}
{"id": "2505.15548", "pdf": "https://arxiv.org/pdf/2505.15548", "abs": "https://arxiv.org/abs/2505.15548", "authors": ["Suvadeep Hajra"], "title": "Short-Range Dependency Effects on Transformer Instability and a Decomposed Attention Solution", "categories": ["cs.LG"], "comment": null, "summary": "Transformer language models have driven significant progress across various\nfields, including natural language processing and computer vision. A central\ncomponent of these models is the self-attention (SA) mechanism, which learns\nrich vector representations of tokens by modeling their relationships with\nothers in a sequence. However, despite extensive research, transformers\ncontinue to suffer from training instability -- often manifesting as spikes or\ndivergence in the training loss during a run.\n  In this work, we identify one source of this instability: SA's limited\nability to capture short-range dependencies, especially in tasks like language\nmodeling, where almost every token heavily relies on its nearby neighbors. This\nlimitation causes the pre-softmax logits of SA to grow rapidly, destabilizing\ntraining. To address this, we propose decomposing the SA into local\n(short-range) and global (long-range) attention heads. This decomposed\nattention, referred to as Long Short-attention (LS-attention), mitigates logit\nexplosion and results in more stable training compared to an equivalent\nmulti-head self-attention (MHSA). Empirical comparisons with two alternative\ntraining stabilization methods show that LS-attention reduces the validation\nperplexity to nearly 2/5 of that achieved by one method and reaches a similar\nperplexity as the other method using only 1/20 of the GPU hours. Additionally,\nour experiments demonstrate that LS-attention reduces inference latency by up\nto 36% compared to a state-of-the-art implementation of equivalent MHSA.", "AI": {"tldr": "LS-attention\u901a\u8fc7\u5206\u89e3\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e3a\u5c40\u90e8\u548c\u5168\u5c40\u6ce8\u610f\u529b\u5934\uff0c\u89e3\u51b3\u4e86\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u7814\u7a76\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u6355\u6349\u77ed\u7a0b\u4f9d\u8d56\u65f6\u7684\u5c40\u9650\u6027\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "method": "\u63d0\u51faLS-attention\uff0c\u5c06\u81ea\u6ce8\u610f\u529b\u5206\u89e3\u4e3a\u5c40\u90e8\u548c\u5168\u5c40\u6ce8\u610f\u529b\u5934\u3002", "result": "LS-attention\u663e\u8457\u964d\u4f4e\u9a8c\u8bc1\u56f0\u60d1\u5ea6\uff0c\u51cf\u5c11GPU\u4f7f\u7528\u65f6\u95f4\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e36%\u3002", "conclusion": "LS-attention\u662f\u4e00\u79cd\u9ad8\u6548\u7a33\u5b9a\u7684\u81ea\u6ce8\u610f\u529b\u6539\u8fdb\u65b9\u6cd5\u3002", "keywords": "\u81ea\u6ce8\u610f\u529b\u673a\u5236,\u8bad\u7ec3\u7a33\u5b9a\u6027,LS-attention,Transformer,\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.15111", "pdf": "https://arxiv.org/pdf/2505.15111", "abs": "https://arxiv.org/abs/2505.15111", "authors": ["Ke Guo", "Haochen Liu", "Xiaojun Wu", "Jia Pan", "Chen Lv"], "title": "iPad: Iterative Proposal-centric End-to-End Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "End-to-end (E2E) autonomous driving systems offer a promising alternative to\ntraditional modular pipelines by reducing information loss and error\naccumulation, with significant potential to enhance both mobility and safety.\nHowever, most existing E2E approaches directly generate plans based on dense\nbird's-eye view (BEV) grid features, leading to inefficiency and limited\nplanning awareness. To address these limitations, we propose iterative\nProposal-centric autonomous driving (iPad), a novel framework that places\nproposals - a set of candidate future plans - at the center of feature\nextraction and auxiliary tasks. Central to iPad is ProFormer, a BEV encoder\nthat iteratively refines proposals and their associated features through\nproposal-anchored attention, effectively fusing multi-view image data.\nAdditionally, we introduce two lightweight, proposal-centric auxiliary tasks -\nmapping and prediction - that improve planning quality with minimal\ncomputational overhead. Extensive experiments on the NAVSIM and CARLA\nBench2Drive benchmarks demonstrate that iPad achieves state-of-the-art\nperformance while being significantly more efficient than prior leading\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aiPad\u7684\u65b0\u578b\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u7684\u5019\u9009\u8ba1\u5212\u548c\u8f7b\u91cf\u7ea7\u8f85\u52a9\u4efb\u52a1\u63d0\u9ad8\u6548\u7387\u548c\u89c4\u5212\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u57fa\u4e8e\u5bc6\u96c6\u9e1f\u77b0\u56fe\u7279\u5f81\u76f4\u63a5\u751f\u6210\u8ba1\u5212\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u4e0b\u548c\u89c4\u5212\u611f\u77e5\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faiPad\u6846\u67b6\uff0c\u6838\u5fc3\u662fProFormer\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u63d0\u6848\u951a\u5b9a\u6ce8\u610f\u529b\u8fed\u4ee3\u4f18\u5316\u5019\u9009\u8ba1\u5212\u53ca\u5176\u76f8\u5173\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u8f85\u52a9\u4efb\u52a1\uff08\u6620\u5c04\u548c\u9884\u6d4b\uff09\u3002", "result": "\u5728NAVSIM\u548cCARLA Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u6548\u7387\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "iPad\u6846\u67b6\u901a\u8fc7\u63d0\u6848\u4e2d\u5fc3\u5316\u8bbe\u8ba1\u548c\u8f7b\u91cf\u7ea7\u4efb\u52a1\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "keywords": "\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u3001\u9e1f\u77b0\u56fe\u3001\u63d0\u6848\u4f18\u5316\u3001\u8f7b\u91cf\u7ea7\u4efb\u52a1"}}
{"id": "2505.15563", "pdf": "https://arxiv.org/pdf/2505.15563", "abs": "https://arxiv.org/abs/2505.15563", "authors": ["Mohammad Ali", "Naeemul Hassan"], "title": "Semantic-based Unsupervised Framing Analysis (SUFA): A Novel Approach for Computational Framing Analysis", "categories": ["cs.CL"], "comment": "Association for Education in Journalism and Mass Communication\n  (AEJMC) Conference, August 07--10, 2023, Washington, DC, USA", "summary": "This research presents a novel approach to computational framing analysis,\ncalled Semantic Relations-based Unsupervised Framing Analysis (SUFA). SUFA\nleverages semantic relations and dependency parsing algorithms to identify and\nassess entity-centric emphasis frames in news media reports. This innovative\nmethod is derived from two studies -- qualitative and computational -- using a\ndataset related to gun violence, demonstrating its potential for analyzing\nentity-centric emphasis frames. This article discusses SUFA's strengths,\nlimitations, and application procedures. Overall, the SUFA approach offers a\nsignificant methodological advancement in computational framing analysis, with\nits broad applicability across both the social sciences and computational\ndomains.", "AI": {"tldr": "SUFA\u662f\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5173\u7cfb\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u65b0\u95fb\u5a92\u4f53\u4e2d\u7684\u5b9e\u4f53\u7126\u70b9\u6846\u67b6\u5206\u6790\uff0c\u5229\u7528\u4f9d\u8d56\u89e3\u6790\u7b97\u6cd5\uff0c\u5c55\u793a\u4e86\u5728\u67aa\u652f\u66b4\u529b\u6570\u636e\u96c6\u4e0a\u7684\u6f5c\u529b\u3002", "motivation": "\u63d0\u51faSUFA\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u901a\u8fc7\u8bed\u4e49\u5173\u7cfb\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u6539\u8fdb\u4f20\u7edf\u7684\u8ba1\u7b97\u6846\u67b6\u5206\u6790\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u65b0\u95fb\u5a92\u4f53\u4e2d\u7684\u5b9e\u4f53\u7126\u70b9\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u8bed\u4e49\u5173\u7cfb\u548c\u4f9d\u8d56\u89e3\u6790\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b9a\u6027\u53ca\u8ba1\u7b97\u7814\u7a76\u9a8c\u8bc1\u65b9\u6cd5\u5728\u67aa\u652f\u66b4\u529b\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002", "result": "SUFA\u80fd\u591f\u6709\u6548\u8bc6\u522b\u548c\u5206\u6790\u5b9e\u4f53\u7126\u70b9\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u793e\u4f1a\u79d1\u5b66\u548c\u8ba1\u7b97\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "SUFA\u4e3a\u8ba1\u7b97\u6846\u67b6\u5206\u6790\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u65b9\u6cd5\u5b66\u8fdb\u6b65\uff0c\u5177\u6709\u8de8\u5b66\u79d1\u5e94\u7528\u7684\u6f5c\u529b\u3002", "keywords": "SUFA, \u8bed\u4e49\u5173\u7cfb, \u65e0\u76d1\u7763\u5b66\u4e60, \u6846\u67b6\u5206\u6790, \u65b0\u95fb\u5a92\u4f53"}}
{"id": "2505.15560", "pdf": "https://arxiv.org/pdf/2505.15560", "abs": "https://arxiv.org/abs/2505.15560", "authors": ["Julian Oelhaf", "Georg Kordowich", "Changhun Kim", "Paula Andrea Perez-Toro", "Andreas Maier", "Johann Jager", "Siming Bayer"], "title": "Impact of Data Sparsity on Machine Learning for Fault Detection in Power System Protection", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Germany's transition to a renewable energy-based power system is reshaping\ngrid operations, requiring advanced monitoring and control to manage\ndecentralized generation. Machine learning (ML) has emerged as a powerful tool\nfor power system protection, particularly for fault detection (FD) and fault\nline identification (FLI) in transmission grids. However, ML model reliability\ndepends on data quality and availability. Data sparsity resulting from sensor\nfailures, communication disruptions, or reduced sampling rates poses a\nchallenge to ML-based FD and FLI. Yet, its impact has not been systematically\nvalidated prior to this work. In response, we propose a framework to assess the\nimpact of data sparsity on ML-based FD and FLI performance. We simulate\nrealistic data sparsity scenarios, evaluate their impact, derive quantitative\ninsights, and demonstrate the effectiveness of this evaluation strategy by\napplying it to an existing ML-based framework. Results show the ML model\nremains robust for FD, maintaining an F1-score of 0.999 $\\pm$ 0.000 even after\na 50x data reduction. In contrast, FLI is more sensitive, with performance\ndecreasing by 55.61% for missing voltage measurements and 9.73% due to\ncommunication failures at critical network points. These findings offer\nactionable insights for optimizing ML models for real-world grid protection.\nThis enables more efficient FD and supports targeted improvements in FLI.", "AI": {"tldr": "\u5fb7\u56fd\u5411\u53ef\u518d\u751f\u80fd\u6e90\u7535\u7f51\u8f6c\u578b\u4e2d\uff0c\u673a\u5668\u5b66\u4e60\u7528\u4e8e\u6545\u969c\u68c0\u6d4b\u548c\u6545\u969c\u7ebf\u8def\u8bc6\u522b\u3002\u4f46\u6570\u636e\u7a00\u758f\u6027\u5f71\u54cd\u6a21\u578b\u53ef\u9760\u6027\uff0c\u672c\u6587\u63d0\u51fa\u8bc4\u4f30\u6846\u67b6\u5206\u6790\u5176\u5f71\u54cd\uff0c\u7ed3\u679c\u663e\u793a\u6545\u969c\u68c0\u6d4b\u7a33\u5065\uff0c\u7ebf\u8def\u8bc6\u522b\u8f83\u654f\u611f\u3002", "motivation": "\u7814\u7a76\u6570\u636e\u7a00\u758f\u6027\u5bf9\u673a\u5668\u5b66\u4e60\u5728\u7535\u7f51\u4fdd\u62a4\u4e2d\uff08\u5982\u6545\u969c\u68c0\u6d4b\u548c\u7ebf\u8def\u8bc6\u522b\uff09\u7684\u5f71\u54cd\uff0c\u586b\u8865\u6b64\u524d\u7cfb\u7edf\u6027\u9a8c\u8bc1\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u8bc4\u4f30\u6846\u67b6\uff0c\u6a21\u62df\u6570\u636e\u7a00\u758f\u573a\u666f\uff0c\u5b9a\u91cf\u5206\u6790\u5176\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u6545\u969c\u68c0\u6d4b\u6a21\u578b\u5728\u6570\u636e\u5927\u5e45\u51cf\u5c11\u4e0b\u4ecd\u7a33\u5065\uff08F1-score 0.999\uff09\uff0c\u4f46\u7ebf\u8def\u8bc6\u522b\u6027\u80fd\u4e0b\u964d\u660e\u663e\uff08\u6700\u9ad855.61%\uff09\u3002", "conclusion": "\u8bc4\u4f30\u6846\u67b6\u63d0\u4f9b\u4e86\u4f18\u5316\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5b9e\u9645\u5efa\u8bae\uff0c\u652f\u6301\u7535\u7f51\u4fdd\u62a4\u7684\u9ad8\u6548\u5b9e\u65bd\u3002", "keywords": "\u53ef\u518d\u751f\u80fd\u6e90\u7535\u7f51, \u673a\u5668\u5b66\u4e60, \u6545\u969c\u68c0\u6d4b, \u6545\u969c\u7ebf\u8def\u8bc6\u522b, \u6570\u636e\u7a00\u758f\u6027"}}
{"id": "2505.15607", "pdf": "https://arxiv.org/pdf/2505.15607", "abs": "https://arxiv.org/abs/2505.15607", "authors": ["David Dinucu-Jianu", "Jakub Macina", "Nico Daheim", "Ido Hakimi", "Iryna Gurevych", "Mrinmaya Sachan"], "title": "From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "David Dinucu-Jianu and Jakub Macina contributed equally. Code\n  available: https://github.com/eth-lre/PedagogicalRL", "summary": "Large language models (LLMs) can transform education, but their optimization\nfor direct question-answering often undermines effective pedagogy which\nrequires strategically withholding answers. To mitigate this, we propose an\nonline reinforcement learning (RL)-based alignment framework that can quickly\nadapt LLMs into effective tutors using simulated student-tutor interactions by\nemphasizing pedagogical quality and guided problem-solving over simply giving\naway answers. We use our method to train a 7B parameter tutor model without\nhuman annotations which reaches similar performance to larger proprietary\nmodels like LearnLM. We introduce a controllable reward weighting to balance\npedagogical support and student solving accuracy, allowing us to trace the\nPareto frontier between these two objectives. Our models better preserve\nreasoning capabilities than single-turn SFT baselines and can optionally\nenhance interpretability through thinking tags that expose the model's\ninstructional planning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u9f50\u6846\u67b6\uff0c\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u6559\u80b2\u8f85\u52a9\u5de5\u5177\uff0c\u5f3a\u8c03\u6559\u5b66\u8d28\u91cf\u548c\u5f15\u5bfc\u5f0f\u5b66\u4e60\u800c\u975e\u76f4\u63a5\u56de\u7b54\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56de\u7b54\u95ee\u9898\u65f6\u76f4\u63a5\u7ed9\u51fa\u7b54\u6848\uff0c\u53ef\u80fd\u524a\u5f31\u6559\u5b66\u6548\u679c\uff0c\u9700\u8981\u6539\u8fdb\u5176\u6559\u5b66\u7b56\u7565\u3002", "method": "\u91c7\u7528\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u5e08\u751f\u4e92\u52a8\u8bad\u7ec3\u6a21\u578b\uff0c\u5e73\u8861\u6559\u5b66\u652f\u6301\u548c\u5b66\u751f\u89e3\u9898\u51c6\u786e\u6027\u3002", "result": "\u8bad\u7ec3\u51fa\u76847B\u53c2\u6570\u6a21\u578b\u6027\u80fd\u63a5\u8fd1LearnLM\u7b49\u5927\u578b\u4e13\u6709\u6a21\u578b\uff0c\u4e14\u5728\u4fdd\u7559\u63a8\u7406\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u6559\u5b66\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5176\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u6559\u80b2\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u5f3a\u5316\u5b66\u4e60, \u6559\u5b66\u4f18\u5316, Pareto\u524d\u6cbf, \u63a7\u5236\u5956\u52b1"}}
{"id": "2505.15570", "pdf": "https://arxiv.org/pdf/2505.15570", "abs": "https://arxiv.org/abs/2505.15570", "authors": ["Marko Tuononen", "Duy Vu", "Dani Korpi", "Vesa Starck", "Ville Hautam\u00e4ki"], "title": "Refining Neural Activation Patterns for Layer-Level Concept Discovery in Neural Network-Based Receivers", "categories": ["cs.LG", "68T07 (Primary) 62H30, 94A05 (Secondary)", "I.2.6; I.5.3; C.2.1"], "comment": "46 pages, 40 figures, 28 tables, 10 equations, and 5 listings", "summary": "Concept discovery in neural networks often targets individual neurons or\nhuman-interpretable features, overlooking distributed layer-wide patterns. We\nstudy the Neural Activation Pattern (NAP) methodology, which clusters\nfull-layer activation distributions to identify such layer-level concepts.\nApplied to visual object recognition and radio receiver models, we propose\nimproved normalization, distribution estimation, distance metrics, and varied\ncluster selection. In the radio receiver model, distinct concepts did not\nemerge; instead, a continuous activation manifold shaped by Signal-to-Noise\nRatio (SNR) was observed -- highlighting SNR as a key learned factor,\nconsistent with classical receiver behavior and supporting physical\nplausibility. Our enhancements to NAP improved in-distribution vs.\nout-of-distribution separation, suggesting better generalization and indirectly\nvalidating clustering quality. These results underscore the importance of\nclustering design and activation manifolds in interpreting and troubleshooting\nneural network behavior.", "AI": {"tldr": "\u7814\u7a76\u4e86\u795e\u7ecf\u6fc0\u6d3b\u6a21\u5f0f\uff08NAP\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b\u5168\u5c42\u6fc0\u6d3b\u5206\u5e03\u53d1\u73b0\u5c42\u7ea7\u522b\u6982\u5ff5\uff0c\u6539\u8fdb\u4e86\u6807\u51c6\u5316\u3001\u5206\u5e03\u4f30\u8ba1\u3001\u8ddd\u79bb\u5ea6\u91cf\u7b49\uff0c\u5728\u65e0\u7ebf\u7535\u63a5\u6536\u6a21\u578b\u4e2d\u89c2\u5bdf\u5230\u8fde\u7eed\u6fc0\u6d3b\u6d41\u5f62\uff0c\u5f3a\u8c03\u4e86SNR\u4f5c\u4e3a\u5173\u952e\u5b66\u4e60\u56e0\u7d20\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u4e2d\u5206\u5e03\u5f0f\u5c42\u8303\u56f4\u6a21\u5f0f\u7684\u6982\u5ff5\u53d1\u73b0\uff0c\u5f25\u8865\u4e86\u4ee5\u5f80\u5173\u6ce8\u5355\u4e2a\u795e\u7ecf\u5143\u6216\u53ef\u89e3\u91ca\u7279\u5f81\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528NAP\u65b9\u6cd5\uff0c\u5e94\u7528\u6539\u8fdb\u7684\u6807\u51c6\u5316\u3001\u5206\u5e03\u4f30\u8ba1\u3001\u8ddd\u79bb\u5ea6\u91cf\u548c\u805a\u7c7b\u9009\u62e9\uff0c\u5206\u6790\u89c6\u89c9\u5bf9\u8c61\u8bc6\u522b\u548c\u65e0\u7ebf\u7535\u63a5\u6536\u6a21\u578b\u7684\u6fc0\u6d3b\u5206\u5e03\u3002", "result": "\u65e0\u7ebf\u7535\u63a5\u6536\u6a21\u578b\u4e2d\u672a\u53d1\u73b0\u660e\u663e\u6982\u5ff5\uff0c\u4f46\u89c2\u5bdf\u5230\u7531\u4fe1\u566a\u6bd4\uff08SNR\uff09\u5851\u9020\u7684\u8fde\u7eed\u6fc0\u6d3b\u6d41\u5f62\uff0c\u652f\u6301\u7269\u7406\u5408\u7406\u6027\uff1b\u6539\u8fdb\u7684NAP\u63d0\u9ad8\u4e86\u5206\u5e03\u5185\u5916\u6570\u636e\u5206\u79bb\u80fd\u529b\u3002", "conclusion": "\u805a\u7c7b\u8bbe\u8ba1\u548c\u6fc0\u6d3b\u6d41\u5f62\u5728\u89e3\u91ca\u548c\u8c03\u8bd5\u795e\u7ecf\u7f51\u7edc\u884c\u4e3a\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u6539\u8fdb\u7684NAP\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "keywords": "\u6982\u5ff5\u53d1\u73b0,\u795e\u7ecf\u6fc0\u6d3b\u6a21\u5f0f\uff08NAP\uff09,\u805a\u7c7b,\u4fe1\u566a\u6bd4\uff08SNR\uff09,\u6fc0\u6d3b\u6d41\u5f62"}}
{"id": "2505.15612", "pdf": "https://arxiv.org/pdf/2505.15612", "abs": "https://arxiv.org/abs/2505.15612", "authors": ["Wei Liu", "Ruochen Zhou", "Yiyun Deng", "Yuzhen Huang", "Junteng Liu", "Yuntian Deng", "Yizhe Zhang", "Junxian He"], "title": "Learn to Reason Efficiently with Adaptive Length-based Reward Shaping", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Reasoning Models (LRMs) have shown remarkable capabilities in solving\ncomplex problems through reinforcement learning (RL), particularly by\ngenerating long reasoning traces. However, these extended outputs often exhibit\nsubstantial redundancy, which limits the efficiency of LRMs. In this paper, we\ninvestigate RL-based approaches to promote reasoning efficiency. Specifically,\nwe first present a unified framework that formulates various efficient\nreasoning methods through the lens of length-based reward shaping. Building on\nthis perspective, we propose a novel Length-bAsed StEp Reward shaping method\n(LASER), which employs a step function as the reward, controlled by a target\nlength. LASER surpasses previous methods, achieving a superior Pareto-optimal\nbalance between performance and efficiency. Next, we further extend LASER based\non two key intuitions: (1) The reasoning behavior of the model evolves during\ntraining, necessitating reward specifications that are also adaptive and\ndynamic; (2) Rather than uniformly encouraging shorter or longer chains of\nthought (CoT), we posit that length-based reward shaping should be\ndifficulty-aware i.e., it should penalize lengthy CoTs more for easy queries.\nThis approach is expected to facilitate a combination of fast and slow\nthinking, leading to a better overall tradeoff. The resulting method is termed\nLASER-D (Dynamic and Difficulty-aware). Experiments on\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and\nDeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both\nreasoning performance and response length efficiency. For instance, LASER-D and\nits variant achieve a +6.1 improvement on AIME2024 while reducing token usage\nby 63%. Further analysis reveals our RL-based compression produces more concise\nreasoning patterns with less redundant \"self-reflections\". Resources are at\nhttps://github.com/hkust-nlp/Laser.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u957f\u5ea6\u5956\u52b1\u5851\u9020\u65b9\u6cd5LASER\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u548c\u96be\u5ea6\u611f\u77e5\u7684\u6269\u5c55LASER-D\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u751f\u6210\u957f\u63a8\u7406\u94fe\u65f6\u5e38\u4f34\u968f\u5197\u4f59\uff0c\u9650\u5236\u4e86\u6548\u7387\uff0c\u56e0\u6b64\u9700\u8981\u4f18\u5316\u63a8\u7406\u6548\u7387\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLASER\u6846\u67b6\uff0c\u57fa\u4e8e\u957f\u5ea6\u5956\u52b1\u5851\u9020\uff1b\u8fdb\u4e00\u6b65\u6269\u5c55\u4e3a\u52a8\u6001\u548c\u96be\u5ea6\u611f\u77e5\u7684LASER-D\uff0c\u9002\u5e94\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u884c\u4e3a\u53d8\u5316\u548c\u4e0d\u540c\u96be\u5ea6\u67e5\u8be2\u7684\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLASER-D\u5728\u6027\u80fd\u4e0a\u63d0\u53476.1%\uff08AIME2024\u6307\u6807\uff09\uff0c\u540c\u65f6\u964d\u4f4e63%\u7684token\u4f7f\u7528\uff0c\u751f\u6210\u66f4\u7b80\u6d01\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "conclusion": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u957f\u5ea6\u5956\u52b1\u5851\u9020\u65b9\u6cd5\u80fd\u6709\u6548\u5e73\u8861\u63a8\u7406\u6027\u80fd\u4e0e\u6548\u7387\uff0c\u52a8\u6001\u548c\u96be\u5ea6\u611f\u77e5\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u6a21\u578b\u8868\u73b0\u3002", "keywords": "\u5927\u578b\u63a8\u7406\u6a21\u578b,\u5f3a\u5316\u5b66\u4e60,\u5956\u52b1\u5851\u9020,\u63a8\u7406\u6548\u7387,\u52a8\u6001\u96be\u5ea6\u611f\u77e5"}}
{"id": "2505.15572", "pdf": "https://arxiv.org/pdf/2505.15572", "abs": "https://arxiv.org/abs/2505.15572", "authors": ["Wangyang Ying", "Haoyue Bai", "Nanxu Gong", "Xinyuan Wang", "Sixun Dong", "Haifeng Chen", "Yanjie Fu"], "title": "Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The data-to-equation (Data2Eqn) task aims to discover interpretable\nmathematical equations that map observed values to labels, offering physical\ninsights and broad applicability across academic and industrial domains.\nGenetic programming and traditional deep learning-based approaches suffer from\nsearch inefficiency and poor generalization on small task-specific datasets.\nFoundation models showed promise in this area, but existing approaches suffer\nfrom: 1) They are pretrained on general-purpose data distributions, making them\nless effective for domain-specific tasks; and 2) their training objectives\nfocus on token-level alignment, overlooking mathematical semantics, which can\nlead to inaccurate equations. To address these issues, we aim to enhance the\ndomain adaptability of foundation models for Data2Eqn tasks. In this work, we\npropose a reinforcement learning-based finetuning framework that directly\noptimizes the generation policy of a pretrained model through reward signals\nderived from downstream numerical fitness. Our method allows the model to adapt\nto specific and complex data distributions and generate mathematically\nmeaningful equations. Extensive experiments demonstrate that our approach\nimproves both the accuracy and robustness of equation generation under complex\ndistributions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u57fa\u7840\u6a21\u578b\u5728\u6570\u636e\u5230\u65b9\u7a0b\u4efb\u52a1\u4e2d\u7684\u9886\u57df\u9002\u5e94\u80fd\u529b\uff0c\u4f18\u5316\u751f\u6210\u7684\u6570\u5b66\u65b9\u7a0b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u548c\u57fa\u7840\u6a21\u578b\u5728\u6570\u636e\u5230\u65b9\u7a0b\u4efb\u52a1\u4e2d\u5b58\u5728\u641c\u7d22\u6548\u7387\u4f4e\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u5bf9\u6570\u5b66\u8bed\u4e49\u5173\u6ce8\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u4e0b\u6e38\u6570\u503c\u9002\u5e94\u6027\u751f\u6210\u7684\u5956\u52b1\u4fe1\u53f7\u76f4\u63a5\u4f18\u5316\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u751f\u6210\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u6570\u636e\u5206\u5e03\u4e0b\u63d0\u9ad8\u4e86\u65b9\u7a0b\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6709\u6548\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "keywords": "\u6570\u636e\u5230\u65b9\u7a0b,\u57fa\u7840\u6a21\u578b,\u5f3a\u5316\u5b66\u4e60,\u5fae\u8c03,\u6570\u5b66\u8bed\u4e49"}}
{"id": "2505.15123", "pdf": "https://arxiv.org/pdf/2505.15123", "abs": "https://arxiv.org/abs/2505.15123", "authors": ["Ta Duc Huy", "Duy Anh Huynh", "Yutong Xie", "Yuankai Qi", "Qi Chen", "Phi Le Nguyen", "Sen Kim Tran", "Son Lam Phung", "Anton van den Hengel", "Zhibin Liao", "Minh-Son To", "Johan W. Verjans", "Vu Minh Hieu Phan"], "title": "Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding", "categories": ["cs.CV", "cs.AI"], "comment": "Under Review", "summary": "Visual grounding (VG) is the capability to identify the specific regions in\nan image associated with a particular text description. In medical imaging, VG\nenhances interpretability by highlighting relevant pathological features\ncorresponding to textual descriptions, improving model transparency and\ntrustworthiness for wider adoption of deep learning models in clinical\npractice. Current models struggle to associate textual descriptions with\ndisease regions due to inefficient attention mechanisms and a lack of\nfine-grained token representations. In this paper, we empirically demonstrate\ntwo key observations. First, current VLMs assign high norms to background\ntokens, diverting the model's attention from regions of disease. Second, the\nglobal tokens used for cross-modal learning are not representative of local\ndisease tokens. This hampers identifying correlations between the text and\ndisease tokens. To address this, we introduce simple, yet effective\nDisease-Aware Prompting (DAP) process, which uses the explainability map of a\nVLM to identify the appropriate image features. This simple strategy amplifies\ndisease-relevant regions while suppressing background interference. Without any\nadditional pixel-level annotations, DAP improves visual grounding accuracy by\n20.74% compared to state-of-the-art methods across three major chest X-ray\ndatasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684Disease-Aware Prompting\uff08DAP\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u5584\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u89c6\u89c9\u5b9a\u4f4d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u96be\u4ee5\u5173\u8054\u6587\u672c\u63cf\u8ff0\u4e0e\u75be\u75c5\u533a\u57df\uff0c\u4e3b\u8981\u7531\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u6548\u7387\u4f4e\u548c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8868\u5f81\u3002", "method": "\u63d0\u51faDAP\u65b9\u6cd5\uff0c\u5229\u7528VLM\u7684\u53ef\u89e3\u91ca\u6027\u56fe\u6765\u8bc6\u522b\u5408\u9002\u7684\u56fe\u50cf\u7279\u5f81\uff0c\u589e\u5f3a\u75be\u75c5\u76f8\u5173\u533a\u57df\u5e76\u6291\u5236\u80cc\u666f\u5e72\u6270\u3002", "result": "\u5728\u4e0d\u9700\u989d\u5916\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\uff0cDAP\u5728\u4e09\u4e2a\u4e3b\u8981\u80f8\u90e8X\u5149\u6570\u636e\u96c6\u4e0a\u5c06\u89c6\u89c9\u5b9a\u4f4d\u51c6\u786e\u7387\u63d0\u5347\u4e8620.74%\u3002", "conclusion": "DAP\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u5f71\u50cf\u4e2d\u89c6\u89c9\u5b9a\u4f4d\u7684\u6027\u80fd\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002", "keywords": "\u89c6\u89c9\u5b9a\u4f4d,\u533b\u5b66\u5f71\u50cf,\u6ce8\u610f\u529b\u673a\u5236,DAP,\u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2505.15623", "pdf": "https://arxiv.org/pdf/2505.15623", "abs": "https://arxiv.org/abs/2505.15623", "authors": ["Tiasa Singha Roy", "Aditeya Baral", "Ayush Rajesh Jhaveri", "Yusuf Baig"], "title": "Can LLMs $\\textit{understand}$ Math? -- Exploring the Pitfalls in Mathematical Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) demonstrate considerable potential in various\nnatural language tasks but face significant challenges in mathematical\nreasoning, particularly in executing precise, multi-step logic. However,\ncurrent evaluation frameworks judge their performance solely based on accuracy,\nwhich only accounts for the final answer. This study explores these pitfalls by\nemploying a novel evaluation framework. We propose an evaluation metric called\nthe MAPLE score, which holistically quantifies reasoning misalignment by\nintegrating error rates, redundancy, and validity.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u2014\u2014MAPLE\u5206\u6570\uff0c\u7528\u4e8e\u5168\u9762\u91cf\u5316\u63a8\u7406\u7684\u504f\u5dee\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u6846\u67b6\u4ec5\u57fa\u4e8e\u6700\u7ec8\u7b54\u6848\u7684\u51c6\u786e\u6027\u6765\u8861\u91cfLLMs\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u5ffd\u89c6\u4e86\u591a\u6b65\u903b\u8f91\u6267\u884c\u7684\u7cbe\u786e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMAPLE\u5206\u6570\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u7ed3\u5408\u9519\u8bef\u7387\u3001\u5197\u4f59\u6027\u548c\u6709\u6548\u6027\uff0c\u4ee5\u5168\u9762\u8861\u91cf\u63a8\u7406\u7684\u504f\u5dee\u3002", "result": "MAPLE\u5206\u6570\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30LLMs\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u901a\u8fc7MAPLE\u5206\u6570\uff0c\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u548c\u6539\u8fdbLLMs\u5728\u590d\u6742\u903b\u8f91\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u6570\u5b66\u63a8\u7406, \u8bc4\u4f30\u6846\u67b6, MAPLE\u5206\u6570"}}
{"id": "2505.15579", "pdf": "https://arxiv.org/pdf/2505.15579", "abs": "https://arxiv.org/abs/2505.15579", "authors": ["Hossein Zakerinia", "Jonathan Scott", "Christoph H. Lampert"], "title": "Federated Learning with Unlabeled Clients: Personalization Can Happen in Low Dimensions", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Personalized federated learning has emerged as a popular approach to training\non devices holding statistically heterogeneous data, known as clients. However,\nmost existing approaches require a client to have labeled data for training or\nfinetuning in order to obtain their own personalized model. In this paper we\naddress this by proposing FLowDUP, a novel method that is able to generate a\npersonalized model using only a forward pass with unlabeled data. The generated\nmodel parameters reside in a low-dimensional subspace, enabling efficient\ncommunication and computation. FLowDUP's learning objective is theoretically\nmotivated by our new transductive multi-task PAC-Bayesian generalization bound,\nthat provides performance guarantees for unlabeled clients. The objective is\nstructured in such a way that it allows both clients with labeled data and\nclients with only unlabeled data to contribute to the training process. To\nsupplement our theoretical results we carry out a thorough experimental\nevaluation of FLowDUP, demonstrating strong empirical performance on a range of\ndatasets with differing sorts of statistically heterogeneous clients. Through\nnumerous ablation studies, we test the efficacy of the individual components of\nthe method.", "AI": {"tldr": "FLowDUP\u662f\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528\u672a\u6807\u8bb0\u6570\u636e\u7684\u524d\u5411\u4f20\u9012\u751f\u6210\u4e2a\u6027\u5316\u6a21\u578b\uff0c\u53c2\u6570\u5b58\u50a8\u4e8e\u4f4e\u7ef4\u5b50\u7a7a\u95f4\uff0c\u8ba1\u7b97\u548c\u901a\u4fe1\u9ad8\u6548\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6807\u8bb0\u6570\u636e\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4ec5\u9700\u672a\u6807\u8bb0\u6570\u636e\u5373\u53ef\u751f\u6210\u4e2a\u6027\u5316\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "method": "FLowDUP\u65b9\u6cd5\u57fa\u4e8e\u65b0\u63d0\u51fa\u7684\u5f52\u7eb3\u591a\u4efb\u52a1PAC-Bayesian\u6cdb\u5316\u8fb9\u754c\u7406\u8bba\uff0c\u5141\u8bb8\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6570\u636e\u7684\u5ba2\u6237\u7aef\u5171\u540c\u53c2\u4e0e\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86FLowDUP\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u7edf\u8ba1\u5f02\u6784\u5ba2\u6237\u7aef\u4e0a\u7684\u4f18\u5f02\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "FLowDUP\u4e3a\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u6807\u8bb0\u6570\u636e\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u9a8c\u652f\u6301\u3002", "keywords": "\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60, FLowDUP, PAC-Bayesian, \u672a\u6807\u8bb0\u6570\u636e, \u4f4e\u7ef4\u5b50\u7a7a\u95f4"}}
{"id": "2505.15133", "pdf": "https://arxiv.org/pdf/2505.15133", "abs": "https://arxiv.org/abs/2505.15133", "authors": ["Haiduo Huang", "Jiangcheng Song", "Yadong Zhang", "Pengju Ren"], "title": "DeepKD: A Deeply Decoupled and Denoised Knowledge Distillation Trainer", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in knowledge distillation have emphasized the importance of\ndecoupling different knowledge components. While existing methods utilize\nmomentum mechanisms to separate task-oriented and distillation gradients, they\noverlook the inherent conflict between target-class and non-target-class\nknowledge flows. Furthermore, low-confidence dark knowledge in non-target\nclasses introduces noisy signals that hinder effective knowledge transfer. To\naddress these limitations, we propose DeepKD, a novel training framework that\nintegrates dual-level decoupling with adaptive denoising. First, through\ntheoretical analysis of gradient signal-to-noise ratio (GSNR) characteristics\nin task-oriented and non-task-oriented knowledge distillation, we design\nindependent momentum updaters for each component to prevent mutual\ninterference. We observe that the optimal momentum coefficients for\ntask-oriented gradient (TOG), target-class gradient (TCG), and non-target-class\ngradient (NCG) should be positively related to their GSNR. Second, we introduce\na dynamic top-k mask (DTM) mechanism that gradually increases K from a small\ninitial value to incorporate more non-target classes as training progresses,\nfollowing curriculum learning principles. The DTM jointly filters\nlow-confidence logits from both teacher and student models, effectively\npurifying dark knowledge during early training. Extensive experiments on\nCIFAR-100, ImageNet, and MS-COCO demonstrate DeepKD's effectiveness. Our code\nis available at https://github.com/haiduo/DeepKD.", "AI": {"tldr": "DeepKD\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u7ea7\u89e3\u8026\u548c\u81ea\u9002\u5e94\u53bb\u566a\u6280\u672f\u89e3\u51b3\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u68af\u5ea6\u51b2\u7a81\u548c\u566a\u58f0\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u77e5\u8bc6\u84b8\u998f\u4e2d\u5ffd\u89c6\u4e86\u76ee\u6807\u7c7b\u548c\u975e\u76ee\u6807\u7c7b\u77e5\u8bc6\u6d41\u7684\u5185\u5728\u51b2\u7a81\uff0c\u4e14\u4f4e\u7f6e\u4fe1\u5ea6\u7684\u6697\u77e5\u8bc6\u5f15\u5165\u566a\u58f0\u4fe1\u53f7\uff0c\u5f71\u54cd\u77e5\u8bc6\u8f6c\u79fb\u6548\u679c\u3002", "method": "\u901a\u8fc7\u68af\u5ea6\u4fe1\u566a\u6bd4\uff08GSNR\uff09\u5206\u6790\u8bbe\u8ba1\u4e86\u72ec\u7acb\u7684\u52a8\u91cf\u66f4\u65b0\u5668\uff0c\u5e76\u5f15\u5165\u52a8\u6001top-K\u63a9\u7801\u673a\u5236\uff08DTM\uff09\u9010\u6b65\u8fc7\u6ee4\u4f4e\u7f6e\u4fe1\u5ea6\u77e5\u8bc6\u3002", "result": "\u5728CIFAR-100\u3001ImageNet\u548cMS-COCO\u7b49\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86DeepKD\u7684\u6709\u6548\u6027\u3002", "conclusion": "DeepKD\u901a\u8fc7\u89e3\u8026\u548c\u53bb\u566a\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u84b8\u998f\u7684\u6548\u679c\u3002", "keywords": "\u77e5\u8bc6\u84b8\u998f, \u68af\u5ea6\u51b2\u7a81, \u4fe1\u566a\u6bd4, \u52a8\u6001top-K\u63a9\u7801, \u89e3\u8026"}}
{"id": "2505.15633", "pdf": "https://arxiv.org/pdf/2505.15633", "abs": "https://arxiv.org/abs/2505.15633", "authors": ["David Thulke", "Jakob Kemmler", "Christian Dugast", "Hermann Ney"], "title": "Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at the ClimateNLP 2025 Workshop at ACL", "summary": "Large language models that use retrieval augmented generation have the\npotential to unlock valuable knowledge for researchers, policymakers, and the\npublic by making long and technical climate-related documents more accessible.\nWhile this approach can help alleviate factual hallucinations by relying on\nretrieved passages as additional context, its effectiveness depends on whether\nthe model's output remains faithful to these passages. To address this, we\nexplore the automatic assessment of faithfulness of different models in this\nsetting. We then focus on ClimateGPT, a large language model specialised in\nclimate science, to examine which factors in its instruction fine-tuning impact\nthe model's faithfulness. By excluding unfaithful subsets of the model's\ntraining data, we develop ClimateGPT Faithful+, which achieves an improvement\nin faithfulness from 30% to 57% in supported atomic claims according to our\nautomatic metric.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u6c14\u5019\u79d1\u5b66\u9886\u57df\u7684\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u4f18\u5316\u6a21\u578b\u7684\u5fe0\u5b9e\u5ea6\u3002", "motivation": "\u6c14\u5019\u76f8\u5173\u7684\u6280\u672f\u6587\u732e\u901a\u5e38\u5197\u957f\u4e14\u590d\u6742\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u53ef\u4ee5\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u3001\u653f\u7b56\u5236\u5b9a\u8005\u548c\u516c\u4f17\u66f4\u6613\u83b7\u53d6\u8fd9\u4e9b\u77e5\u8bc6\u3002\u7136\u800c\uff0c\u6a21\u578b\u7684\u8f93\u51fa\u662f\u5426\u5fe0\u5b9e\u4e8e\u68c0\u7d22\u5230\u7684\u5185\u5bb9\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u6765\u68c0\u6d4b\u6a21\u578b\u7684\u5fe0\u5b9e\u5ea6\uff0c\u5e76\u9488\u5bf9ClimateGPT\u6a21\u578b\uff0c\u901a\u8fc7\u5254\u9664\u4e0d\u5fe0\u5b9e\u7684\u8bad\u7ec3\u6570\u636e\u5b50\u96c6\u5f00\u53d1\u4e86ClimateGPT Faithful+\u3002", "result": "\u4f18\u5316\u540e\u7684ClimateGPT Faithful+\u5728\u652f\u6301\u7684\u539f\u5b50\u4e3b\u5f20\u4e2d\u7684\u5fe0\u5b9e\u5ea6\u4ece30%\u63d0\u5347\u81f357%\u3002", "conclusion": "\u901a\u8fc7\u8fc7\u6ee4\u4e0d\u5fe0\u5b9e\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u6c14\u5019\u79d1\u5b66\u9886\u57df\u7684\u8f93\u51fa\u5fe0\u5b9e\u5ea6\u3002", "keywords": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u6c14\u5019\u79d1\u5b66\u3001\u8bed\u8a00\u6a21\u578b\u3001\u5fe0\u5b9e\u5ea6\u8bc4\u4f30"}}
{"id": "2505.15589", "pdf": "https://arxiv.org/pdf/2505.15589", "abs": "https://arxiv.org/abs/2505.15589", "authors": ["Carlos Stein Brito", "Daniel McNamee"], "title": "World Models as Reference Trajectories for Rapid Motor Adaptation", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Deploying learned control policies in real-world environments poses a\nfundamental challenge. When system dynamics change unexpectedly, performance\ndegrades until models are retrained on new data. We introduce Reflexive World\nModels (RWM), a dual control framework that uses world model predictions as\nimplicit reference trajectories for rapid adaptation. Our method separates the\ncontrol problem into long-term reward maximization through reinforcement\nlearning and robust motor execution through rapid latent control. This dual\narchitecture achieves significantly faster adaptation with low online\ncomputational cost compared to model-based RL baselines, while maintaining\nnear-optimal performance. The approach combines the benefits of flexible policy\nlearning through reinforcement learning with rapid error correction\ncapabilities, providing a principled approach to maintaining performance in\nhigh-dimensional continuous control tasks under varying dynamics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Reflexive World Models (RWM)\uff0c\u4e00\u79cd\u53cc\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u5feb\u901f\u9002\u5e94\u7cfb\u7edf\u52a8\u6001\u53d8\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u5728\u7cfb\u7edf\u52a8\u6001\u53d8\u5316\u65f6\uff0c\u4f20\u7edf\u63a7\u5236\u7b56\u7565\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u9700\u8981\u5feb\u901f\u9002\u5e94\u65b0\u6570\u636e\u3002", "method": "\u7ed3\u5408\u4e16\u754c\u6a21\u578b\u9884\u6d4b\u4f5c\u4e3a\u9690\u5f0f\u53c2\u8003\u8f68\u8ff9\uff0c\u5c06\u63a7\u5236\u95ee\u9898\u5206\u4e3a\u957f\u671f\u5956\u52b1\u6700\u5927\u5316\u548c\u5feb\u901f\u6f5c\u5728\u63a7\u5236\u6267\u884c\u3002", "result": "\u76f8\u6bd4\u57fa\u4e8e\u6a21\u578b\u7684RL\u57fa\u51c6\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u9002\u5e94\u901f\u5ea6\u548c\u4f4e\u5728\u7ebf\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "RWM\u7ed3\u5408\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u7075\u6d3b\u7b56\u7565\u5b66\u4e60\u548c\u5feb\u901f\u7ea0\u9519\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u3002", "keywords": "Reflexive World Models, \u5f3a\u5316\u5b66\u4e60, \u5feb\u901f\u9002\u5e94, \u53cc\u63a7\u5236\u6846\u67b6"}}
{"id": "2505.15634", "pdf": "https://arxiv.org/pdf/2505.15634", "abs": "https://arxiv.org/abs/2505.15634", "authors": ["Zihao Li", "Xu Wang", "Yuzhe Yang", "Ziyu Yao", "Haoyi Xiong", "Mengnan Du"], "title": "Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate the ability to solve reasoning and\nmathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT\nlength, as seen in models such as DeepSeek-R1, significantly enhances this\nreasoning for complex problems, but requires costly and high-quality long CoT\ndata and fine-tuning. This work, inspired by the deep thinking paradigm of\nDeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of\nan LLM without external datasets. Our method first employs Sparse Autoencoders\n(SAEs) to extract interpretable features from vanilla CoT. These features are\nthen used to steer the LLM's internal states during generation. Recognizing\nthat many LLMs do not have corresponding pre-trained SAEs, we further introduce\na novel SAE-free steering algorithm, which directly computes steering\ndirections from the residual activations of an LLM, obviating the need for an\nexplicit SAE. Experimental results demonstrate that both our SAE-based and\nsubsequent SAE-free steering algorithms significantly enhance the reasoning\ncapabilities of LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65e0\u9700\u5916\u90e8\u6570\u636e\u96c6\u7684LLM\u63a8\u7406\u589e\u5f3a\u65b9\u6cd5\uff1a\u57fa\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u548cSAE-free\u7684\u5bfc\u5411\u6280\u672f\uff0c\u5747\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u89e3\u51b3\u590d\u6742\u63a8\u7406\u548c\u6570\u5b66\u95ee\u9898\u7684\u80fd\u529b\uff0c\u540c\u65f6\u907f\u514d\u5bf9\u9ad8\u6210\u672c\u957f\u94fe\u601d\u7ef4\uff08CoT\uff09\u6570\u636e\u548c\u5fae\u8c03\u7684\u4f9d\u8d56\u3002", "method": "1. \u57fa\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u4ece\u666e\u901aCoT\u4e2d\u63d0\u53d6\u53ef\u89e3\u91ca\u7279\u5f81\uff0c\u7528\u4e8e\u751f\u6210\u65f6\u5bfc\u5411LLM\u5185\u90e8\u72b6\u6001\uff1b2. \u63d0\u51faSAE-free\u7b97\u6cd5\uff0c\u76f4\u63a5\u4eceLLM\u7684\u6b8b\u5dee\u6fc0\u6d3b\u4e2d\u8ba1\u7b97\u5bfc\u5411\u65b9\u5411\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e24\u79cd\u65b9\u6cd5\u5747\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u5bfc\u5411\u6280\u672f\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u5916\u90e8\u6570\u636e\u7684LLM\u63a8\u7406\u589e\u5f3a\u65b9\u6848\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u94fe\u5f0f\u601d\u7ef4, \u7a00\u758f\u81ea\u7f16\u7801\u5668, \u63a8\u7406\u80fd\u529b, \u5bfc\u5411\u6280\u672f"}}
{"id": "2505.15594", "pdf": "https://arxiv.org/pdf/2505.15594", "abs": "https://arxiv.org/abs/2505.15594", "authors": ["Yury Belousov", "Brian Pulfer", "Vitaliy Kinakh", "Slava Voloshynovskiy"], "title": "Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Paper accepted at the 33rd European Signal Processing Conference\n  (EUSIPCO 2025)", "summary": "While foundation models demonstrate impressive performance across various\ntasks, they remain vulnerable to adversarial inputs. Current research explores\nvarious approaches to enhance model robustness, with Diffusion Denoised\nSmoothing emerging as a particularly promising technique. This method employs a\npretrained diffusion model to preprocess inputs before model inference. Yet,\nits effectiveness remains largely unexplored beyond classification. We aim to\naddress this gap by analyzing three datasets with four distinct downstream\ntasks under three different adversarial attack algorithms. Our findings reveal\nthat while foundation models maintain resilience against conventional\ntransformations, applying high-noise diffusion denoising to clean images\nwithout any distortions significantly degrades performance by as high as 57%.\nLow-noise diffusion settings preserve performance but fail to provide adequate\nprotection across all attack types. Moreover, we introduce a novel attack\nstrategy specifically targeting the diffusion process itself, capable of\ncircumventing defenses in the low-noise regime. Our results suggest that the\ntrade-off between adversarial robustness and performance remains a challenge to\nbe addressed.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u6269\u6563\u53bb\u566a\u5e73\u6ed1\u6280\u672f\u5728\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u548c\u5bf9\u6297\u653b\u51fb\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5176\u5bf9\u57fa\u7840\u6a21\u578b\u7684\u9c81\u68d2\u6027\u5b58\u5728\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5bf9\u5bf9\u6297\u8f93\u5165\u7684\u8106\u5f31\u6027\u4ecd\u6709\u5f85\u89e3\u51b3\u3002\u6269\u6563\u53bb\u566a\u5e73\u6ed1\u6280\u672f\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u4f46\u5176\u6709\u6548\u6027\u5c1a\u672a\u5728\u591a\u4efb\u52a1\u4e2d\u5145\u5206\u9a8c\u8bc1\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u5728\u6a21\u578b\u63a8\u7406\u524d\u9884\u5904\u7406\u8f93\u5165\uff0c\u5e76\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u56db\u79cd\u4e0b\u6e38\u4efb\u52a1\u548c\u4e09\u79cd\u5bf9\u6297\u653b\u51fb\u7b97\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9ad8\u566a\u58f0\u6269\u6563\u53bb\u566a\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\uff08\u9ad8\u8fbe57%\uff09\uff0c\u800c\u4f4e\u566a\u58f0\u8bbe\u7f6e\u867d\u80fd\u4fdd\u6301\u6027\u80fd\u4f46\u65e0\u6cd5\u62b5\u5fa1\u6240\u6709\u653b\u51fb\u7c7b\u578b\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u8fc7\u7a0b\u7684\u65b0\u578b\u653b\u51fb\u7b56\u7565\u3002", "conclusion": "\u6269\u6563\u53bb\u566a\u5e73\u6ed1\u6280\u672f\u5728\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u89e3\u51b3\u3002", "keywords": "\u57fa\u7840\u6a21\u578b, \u5bf9\u6297\u653b\u51fb, \u6269\u6563\u53bb\u566a\u5e73\u6ed1, \u9c81\u68d2\u6027, \u6027\u80fd\u6743\u8861"}}
{"id": "2505.15646", "pdf": "https://arxiv.org/pdf/2505.15646", "abs": "https://arxiv.org/abs/2505.15646", "authors": ["Ke Hu", "Krishna Puvvada", "Elena Rastorgueva", "Zhehuai Chen", "He Huang", "Shuoyang Ding", "Kunal Dhawan", "Hainan Xu", "Jagadeesh Balam", "Boris Ginsburg"], "title": "Word Level Timestamp Generation for Automatic Speech Recognition and Translation", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "We introduce a data-driven approach for enabling word-level timestamp\nprediction in the Canary model. Accurate timestamp information is crucial for a\nvariety of downstream tasks such as speech content retrieval and timed\nsubtitles. While traditional hybrid systems and end-to-end (E2E) models may\nemploy external modules for timestamp prediction, our approach eliminates the\nneed for separate alignment mechanisms. By leveraging the NeMo Forced Aligner\n(NFA) as a teacher model, we generate word-level timestamps and train the\nCanary model to predict timestamps directly. We introduce a new <|timestamp|>\ntoken, enabling the Canary model to predict start and end timestamps for each\nword. Our method demonstrates precision and recall rates between 80% and 90%,\nwith timestamp prediction errors ranging from 20 to 120 ms across four\nlanguages, with minimal WER degradation. Additionally, we extend our system to\nautomatic speech translation (AST) tasks, achieving timestamp prediction errors\naround 200 milliseconds.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u76f4\u63a5\u5728Canary\u6a21\u578b\u4e2d\u9884\u6d4b\u8bcd\u7ea7\u65f6\u95f4\u6233\uff0c\u6d88\u9664\u4e86\u5916\u90e8\u5bf9\u9f50\u6a21\u5757\u7684\u9700\u6c42\uff0c\u5e76\u5728\u591a\u8bed\u8a00\u548c\u81ea\u52a8\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u53d6\u5f97\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u8bcd\u7ea7\u65f6\u95f4\u6233\u5bf9\u4e8e\u8bed\u97f3\u5185\u5bb9\u68c0\u7d22\u548c\u5b9a\u65f6\u5b57\u5e55\u7b49\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u5916\u90e8\u5bf9\u9f50\u6a21\u5757\uff0c\u800c\u672c\u65b9\u6cd5\u76f4\u63a5\u5d4c\u5165\u65f6\u95f4\u6233\u9884\u6d4b\u529f\u80fd\u3002", "method": "\u5229\u7528NeMo Forced Aligner\uff08NFA\uff09\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u751f\u6210\u65f6\u95f4\u6233\u6570\u636e\uff0c\u5e76\u5728Canary\u6a21\u578b\u4e2d\u5f15\u5165<|timestamp|>\u6807\u8bb0\uff0c\u76f4\u63a5\u9884\u6d4b\u8bcd\u7ea7\u65f6\u95f4\u6233\u3002", "result": "\u5728\u56db\u79cd\u8bed\u8a00\u4e2d\uff0c\u65f6\u95f4\u6233\u9884\u6d4b\u7684\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u8fbe80%-90%\uff0c\u8bef\u5dee\u4e3a20-120\u6beb\u79d2\uff1b\u5728\u81ea\u52a8\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8bef\u5dee\u7ea6200\u6beb\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7b80\u5316\u4e86\u65f6\u95f4\u6233\u9884\u6d4b\u6d41\u7a0b\uff0c\u5e76\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u8bcd\u9519\u8bef\u7387\u3002", "keywords": "\u65f6\u95f4\u6233\u9884\u6d4b, Canary\u6a21\u578b, \u6570\u636e\u9a71\u52a8, \u8bed\u97f3\u5904\u7406, \u81ea\u52a8\u8bed\u97f3\u7ffb\u8bd1"}}
{"id": "2505.15602", "pdf": "https://arxiv.org/pdf/2505.15602", "abs": "https://arxiv.org/abs/2505.15602", "authors": ["Patrick Cheridito", "Jean-Loup Dupret", "Donatien Hainaut"], "title": "Deep Learning for Continuous-time Stochastic Control with Jumps", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC", "q-fin.PM", "I.2.8; I.2.6"], "comment": null, "summary": "In this paper, we introduce a model-based deep-learning approach to solve\nfinite-horizon continuous-time stochastic control problems with jumps. We\niteratively train two neural networks: one to represent the optimal policy and\nthe other to approximate the value function. Leveraging a continuous-time\nversion of the dynamic programming principle, we derive two different training\nobjectives based on the Hamilton-Jacobi-Bellman equation, ensuring that the\nnetworks capture the underlying stochastic dynamics. Empirical evaluations on\ndifferent problems illustrate the accuracy and scalability of our approach,\ndemonstrating its effectiveness in solving complex, high-dimensional stochastic\ncontrol tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5177\u6709\u8df3\u8dc3\u7684\u6709\u9650\u65f6\u95f4\u8fde\u7eed\u65f6\u95f4\u968f\u673a\u63a7\u5236\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u3001\u9ad8\u7ef4\u968f\u673a\u63a7\u5236\u95ee\u9898\u65f6\u6548\u7387\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u8fed\u4ee3\u8bad\u7ec3\u4e24\u4e2a\u795e\u7ecf\u7f51\u7edc\uff1a\u4e00\u4e2a\u8868\u793a\u6700\u4f18\u7b56\u7565\uff0c\u53e6\u4e00\u4e2a\u8fd1\u4f3c\u503c\u51fd\u6570\uff0c\u5e76\u5229\u7528\u52a8\u6001\u89c4\u5212\u539f\u7406\u63a8\u5bfc\u8bad\u7ec3\u76ee\u6807\u3002", "result": "\u5728\u4e0d\u540c\u95ee\u9898\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6269\u5c55\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u590d\u6742\u9ad8\u7ef4\u968f\u673a\u63a7\u5236\u4efb\u52a1\u3002", "keywords": "\u6df1\u5ea6\u5b66\u4e60, \u968f\u673a\u63a7\u5236, \u795e\u7ecf\u7f51\u7edc, \u52a8\u6001\u89c4\u5212, \u8df3\u8dc3\u8fc7\u7a0b"}}
{"id": "2505.15656", "pdf": "https://arxiv.org/pdf/2505.15656", "abs": "https://arxiv.org/abs/2505.15656", "authors": ["Zhexin Zhang", "Yuhao Sun", "Junxiao Yang", "Shiyao Cui", "Hongning Wang", "Minlie Huang"], "title": "Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!", "categories": ["cs.CL"], "comment": "19 pages", "summary": "Fine-tuning on open-source Large Language Models (LLMs) with proprietary data\nis now a standard practice for downstream developers to obtain task-specific\nLLMs. Surprisingly, we reveal a new and concerning risk along with the\npractice: the creator of the open-source LLMs can later extract the private\ndownstream fine-tuning data through simple backdoor training, only requiring\nblack-box access to the fine-tuned downstream model. Our comprehensive\nexperiments, across 4 popularly used open-source models with 3B to 32B\nparameters and 2 downstream datasets, suggest that the extraction performance\ncan be strikingly high: in practical settings, as much as 76.3% downstream\nfine-tuning data (queries) out of a total 5,000 samples can be perfectly\nextracted, and the success rate can increase to 94.9% in more ideal settings.\nWe also explore a detection-based defense strategy but find it can be bypassed\nwith improved attack. Overall, we highlight the emergency of this newly\nidentified data breaching risk in fine-tuning, and we hope that more follow-up\nresearch could push the progress of addressing this concerning risk. The code\nand data used in our experiments are released at\nhttps://github.com/thu-coai/Backdoor-Data-Extraction.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u540e\u95e8\u8bad\u7ec3\uff0c\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u521b\u5efa\u8005\u53ef\u4ee5\u4ece\u4e0b\u6e38\u5f00\u53d1\u8005\u5fae\u8c03\u540e\u7684\u6a21\u578b\u4e2d\u63d0\u53d6\u79c1\u6709\u6570\u636e\uff0c\u4e14\u5728\u5b9e\u9a8c\u4e2d\u63d0\u53d6\u6210\u529f\u7387\u9ad8\u8fbe94.9%\u3002", "motivation": "\u63ed\u793a\u5f00\u6e90LLMs\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5b58\u5728\u7684\u6570\u636e\u6cc4\u9732\u98ce\u9669\uff0c\u4fc3\u4f7f\u5b66\u672f\u754c\u5173\u6ce8\u5e76\u5bfb\u6c42\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u540e\u95e8\u8bad\u7ec3\u548c\u9ed1\u76d2\u8bbf\u95ee\u5fae\u8c03\u540e\u7684\u4e0b\u6e38\u6a21\u578b\uff0c\u5b9e\u9a8c\u57284\u79cd\u5f00\u6e90LLMs\u548c2\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u3002", "result": "\u5728\u5b9e\u9a8c\u4e2d\uff0c\u6210\u529f\u63d0\u53d6\u4e8676.3%\uff08\u5b9e\u9645\u8bbe\u7f6e\uff09\u523094.9%\uff08\u7406\u60f3\u8bbe\u7f6e\uff09\u7684\u4e0b\u6e38\u5fae\u8c03\u6570\u636e\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u6570\u636e\u6cc4\u9732\u98ce\u9669\u7684\u7d27\u8feb\u6027\uff0c\u547c\u5401\u66f4\u591a\u7814\u7a76\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "keywords": "LLMs, fine-tuning, data breaching, backdoor training"}}
{"id": "2505.15622", "pdf": "https://arxiv.org/pdf/2505.15622", "abs": "https://arxiv.org/abs/2505.15622", "authors": ["Pietro Bartoli", "Christian Veronesi", "Andrea Giudici", "David Siorpaes", "Diana Trojaniello", "Franco Zappa"], "title": "Benchmarking Energy and Latency in TinyML: A Novel Method for Resource-Constrained AI", "categories": ["cs.LG"], "comment": "8 pages, 6 figures The article is already accepted for International\n  Joint Conference on Neural Networks (IJCNN) 2025", "summary": "The rise of IoT has increased the need for on-edge machine learning, with\nTinyML emerging as a promising solution for resource-constrained devices such\nas MCU. However, evaluating their performance remains challenging due to\ndiverse architectures and application scenarios. Current solutions have many\nnon-negligible limitations. This work introduces an alternative benchmarking\nmethodology that integrates energy and latency measurements while\ndistinguishing three execution phases pre-inference, inference, and\npost-inference. Additionally, the setup ensures that the device operates\nwithout being powered by an external measurement unit, while automated testing\ncan be leveraged to enhance statistical significance. To evaluate our setup, we\ntested the STM32N6 MCU, which includes a NPU for executing neural networks. Two\nconfigurations were considered: high-performance and Low-power. The variation\nof the EDP was analyzed separately for each phase, providing insights into the\nimpact of hardware configurations on energy efficiency. Each model was tested\n1000 times to ensure statistically relevant results. Our findings demonstrate\nthat reducing the core voltage and clock frequency improve the efficiency of\npre- and post-processing without significantly affecting network execution\nperformance. This approach can also be used for cross-platform comparisons to\ndetermine the most efficient inference platform and to quantify how pre- and\npost-processing overhead varies across different hardware implementations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684TinyML\u6027\u80fd\u8bc4\u4f30\u65b9\u6cd5\uff0c\u878d\u5408\u80fd\u8017\u548c\u5ef6\u8fdf\u6d4b\u91cf\uff0c\u5e76\u901a\u8fc7\u5206\u9636\u6bb5\u5206\u6790\uff08\u9884\u63a8\u7406\u3001\u63a8\u7406\u548c\u540e\u63a8\u7406\uff09\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u6027\u80fd\u6d1e\u5bdf\u3002", "motivation": "\u7531\u4e8eTinyML\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e9f\u9700\u66f4\u5168\u9762\u7684\u6027\u80fd\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7ed3\u5408\u80fd\u8017\u4e0e\u5ef6\u8fdf\u6d4b\u91cf\uff0c\u533a\u5206\u6267\u884c\u9636\u6bb5\uff0c\u652f\u6301\u81ea\u52a8\u5316\u6d4b\u8bd5\u4ee5\u63d0\u9ad8\u7edf\u8ba1\u663e\u8457\u6027\u3002", "result": "\u5728STM32N6 MCU\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u964d\u4f4e\u6838\u5fc3\u7535\u538b\u548c\u65f6\u949f\u9891\u7387\u53ef\u63d0\u5347\u80fd\u6548\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u8de8\u5e73\u53f0\u6bd4\u8f83\uff0c\u80fd\u591f\u91cf\u5316\u4e0d\u540c\u786c\u4ef6\u5b9e\u73b0\u4e2d\u9884/\u540e\u5904\u7406\u7684\u6548\u7387\u5dee\u5f02\u3002", "keywords": "TinyML, benchmarking, energy efficiency, edge computing"}}
{"id": "2505.15670", "pdf": "https://arxiv.org/pdf/2505.15670", "abs": "https://arxiv.org/abs/2505.15670", "authors": ["Ke Hu", "Ehsan Hosseini-Asl", "Chen Chen", "Edresson Casanova", "Subhankar Ghosh", "Piotr \u017belasko", "Zhehuai Chen", "Jason Li", "Jagadeesh Balam", "Boris Ginsburg"], "title": "Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Spoken dialogue is an intuitive form of human-computer interaction, yet\ncurrent speech language models often remain constrained to turn-based\nexchanges, lacking real-time adaptability such as user barge-in. We propose a\nnovel duplex speech to speech (S2S) architecture featuring continuous user\ninputs and codec agent outputs with channel fusion that directly models\nsimultaneous user and agent streams. Using a pretrained streaming encoder for\nuser input enables the first duplex S2S model without requiring speech\npretrain. Separate architectures for agent and user modeling facilitate codec\nfine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared\nto previous works. Experimental results show that the proposed model\noutperforms previous duplex models in reasoning, turn-taking, and barge-in\nabilities. The model requires significantly less speech data, as speech\npretrain is skipped, which markedly simplifies the process of building a duplex\nS2S model from any LLMs. Finally, it is the first openly available duplex S2S\nmodel with training and inference code to foster reproducibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u53cc\u5de5\u8bed\u97f3\u5230\u8bed\u97f3\uff08S2S\uff09\u67b6\u6784\uff0c\u652f\u6301\u5b9e\u65f6\u7528\u6237\u6253\u65ad\uff08barge-in\uff09\uff0c\u5e76\u901a\u8fc7\u901a\u9053\u878d\u5408\u76f4\u63a5\u5efa\u6a21\u7528\u6237\u548c\u4ee3\u7406\u7684\u540c\u65f6\u8bed\u97f3\u6d41\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6bd4\u7279\u7387\u548c\u6240\u9700\u8bed\u97f3\u6570\u636e\u91cf\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u591a\u9650\u4e8e\u56de\u5408\u5236\u4ea4\u4e92\uff0c\u7f3a\u4e4f\u5b9e\u65f6\u9002\u5e94\u6027\uff08\u5982\u7528\u6237\u6253\u65ad\uff09\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u6d41\u5f0f\u7f16\u7801\u5668\u5904\u7406\u7528\u6237\u8f93\u5165\uff0c\u5206\u79bb\u7528\u6237\u548c\u4ee3\u7406\u5efa\u6a21\u67b6\u6784\uff0c\u5e76\u7ed3\u5408\u901a\u9053\u878d\u5408\u6280\u672f\u3002", "result": "\u6a21\u578b\u5728\u63a8\u7406\u3001\u8f6e\u8f6c\u548c\u6253\u65ad\u80fd\u529b\u4e0a\u4f18\u4e8e\u5148\u524d\u6a21\u578b\uff0c\u6bd4\u7279\u7387\u51cf\u534a\uff080.6 kbps\uff09\uff0c\u4e14\u65e0\u9700\u8bed\u97f3\u9884\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u6a21\u578b\u7b80\u5316\u4e86\u4eceLLMs\u6784\u5efa\u53cc\u5de5S2S\u6a21\u578b\u7684\u8fc7\u7a0b\uff0c\u5e76\u9996\u6b21\u516c\u5f00\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u4ee3\u7801\u3002", "keywords": "\u53cc\u5de5\u8bed\u97f3\u5230\u8bed\u97f3\uff08S2S\uff09, \u5b9e\u65f6\u6253\u65ad, \u901a\u9053\u878d\u5408, \u4f4e\u6bd4\u7279\u7387, \u8bed\u97f3\u5efa\u6a21"}}
{"id": "2505.15624", "pdf": "https://arxiv.org/pdf/2505.15624", "abs": "https://arxiv.org/abs/2505.15624", "authors": ["H. V. AlquBoj", "Hilal AlQuabeh", "Velibor Bojkovic", "Munachiso Nwadike", "Kentaro Inui"], "title": "Mechanistic Insights into Grokking from the Embedding Layer", "categories": ["cs.LG", "cs.CL"], "comment": "Mechanistic view of embedding layers", "summary": "Grokking, a delayed generalization in neural networks after perfect training\nperformance, has been observed in Transformers and MLPs, but the components\ndriving it remain underexplored. We show that embeddings are central to\ngrokking: introducing them into MLPs induces delayed generalization in modular\narithmetic tasks, whereas MLPs without embeddings can generalize immediately.\nOur analysis identifies two key mechanisms: (1) Embedding update dynamics,\nwhere rare tokens stagnate due to sparse gradient updates and weight decay, and\n(2) Bilinear coupling, where the interaction between embeddings and downstream\nweights introduces saddle points and increases sensitivity to initialization.\nTo confirm these mechanisms, we investigate frequency-aware sampling, which\nbalances token updates by minimizing gradient variance, and embedding-specific\nlearning rates, derived from the asymmetric curvature of the bilinear loss\nlandscape. We prove that an adaptive learning rate ratio,\n\\(\\frac{\\eta_E}{\\eta_W} \\propto \\frac{\\sigma_{\\max}(E)}{\\sigma_{\\max}(W)} \\cdot\n\\frac{f_W}{f_E}\\), mitigates bilinear coupling effects, accelerating\nconvergence. Our methods not only improve grokking dynamics but also extend to\nbroader challenges in Transformer optimization, where bilinear interactions\nhinder efficient training.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5d4c\u5165\u662f\u795e\u7ecf\u7f51\u7edc\u4e2d\u5ef6\u8fdf\u6cdb\u5316\uff08grokking\uff09\u7684\u6838\u5fc3\u56e0\u7d20\uff0c\u901a\u8fc7\u5206\u6790\u5d4c\u5165\u66f4\u65b0\u52a8\u529b\u5b66\u548c\u53cc\u7ebf\u6027\u8026\u5408\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u4f18\u5316\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u4e2d\u5ef6\u8fdf\u6cdb\u5316\u7684\u9a71\u52a8\u56e0\u7d20\uff0c\u7279\u522b\u662f\u5728\u5d4c\u5165\u5c42\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f15\u5165\u5d4c\u5165\u5c42\u5230MLPs\u4e2d\uff0c\u5206\u6790\u5176\u66f4\u65b0\u52a8\u529b\u5b66\u548c\u53cc\u7ebf\u6027\u8026\u5408\u673a\u5236\uff0c\u63d0\u51fa\u9891\u7387\u611f\u77e5\u91c7\u6837\u548c\u5d4c\u5165\u7279\u5b9a\u5b66\u4e60\u7387\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u5d4c\u5165\u5c42\u5bfc\u81f4\u5ef6\u8fdf\u6cdb\u5316\uff0c\u901a\u8fc7\u4f18\u5316\u65b9\u6cd5\u53ef\u4ee5\u52a0\u901f\u6536\u655b\u5e76\u6539\u5584\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "\u5d4c\u5165\u5c42\u662f\u5ef6\u8fdf\u6cdb\u5316\u7684\u5173\u952e\uff0c\u9488\u5bf9\u6027\u4f18\u5316\u65b9\u6cd5\u53ef\u6709\u6548\u6539\u5584\u8bad\u7ec3\u52a8\u6001\u548c\u6027\u80fd\u3002", "keywords": "grokking, \u795e\u7ecf\u7f51\u7edc, \u5d4c\u5165, \u5ef6\u8fdf\u6cdb\u5316, \u53cc\u7ebf\u6027\u8026\u5408"}}
{"id": "2505.15155", "pdf": "https://arxiv.org/pdf/2505.15155", "abs": "https://arxiv.org/abs/2505.15155", "authors": ["Yuante Li", "Xu Yang", "Xiao Yang", "Minrui Xu", "Xisen Wang", "Weiqing Liu", "Jiang Bian"], "title": "R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization", "categories": ["q-fin.CP", "cs.AI", "cs.CE", "cs.LG"], "comment": null, "summary": "Financial markets pose fundamental challenges for asset return prediction due\nto their high dimensionality, non-stationarity, and persistent volatility.\nDespite advances in large language models and multi-agent systems, current\nquantitative research pipelines suffer from limited automation, weak\ninterpretability, and fragmented coordination across key components such as\nfactor mining and model innovation. In this paper, we propose R&D-Agent for\nQuantitative Finance, in short RD-Agent(Q), the first data-centric multi-agent\nframework designed to automate the full-stack research and development of\nquantitative strategies via coordinated factor-model co-optimization.\nRD-Agent(Q) decomposes the quant process into two iterative stages: a Research\nstage that dynamically sets goal-aligned prompts, formulates hypotheses based\non domain priors, and maps them to concrete tasks, and a Development stage that\nemploys a code-generation agent, Co-STEER, to implement task-specific code,\nwhich is then executed in real-market backtests. The two stages are connected\nthrough a feedback stage that thoroughly evaluates experimental outcomes and\ninforms subsequent iterations, with a multi-armed bandit scheduler for adaptive\ndirection selection. Empirically, RD-Agent(Q) achieves up to 2X higher\nannualized returns than classical factor libraries using 70% fewer factors, and\noutperforms state-of-the-art deep time-series models on real markets. Its joint\nfactor-model optimization delivers a strong balance between predictive accuracy\nand strategy robustness. Our code is available at:\nhttps://github.com/microsoft/RD-Agent.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRD-Agent(Q)\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u534f\u8c03\u56e0\u5b50\u6a21\u578b\u5171\u540c\u4f18\u5316\uff0c\u81ea\u52a8\u5316\u5b8c\u6210\u91cf\u5316\u91d1\u878d\u7b56\u7565\u7684\u5168\u6808\u7814\u53d1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56de\u62a5\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u91d1\u878d\u5e02\u573a\u7684\u9884\u6d4b\u9762\u4e34\u9ad8\u7ef4\u6027\u3001\u975e\u5e73\u7a33\u6027\u548c\u6301\u7eed\u6ce2\u52a8\u6027\u7b49\u6311\u6218\uff0c\u73b0\u6709\u91cf\u5316\u7814\u7a76\u6d41\u7a0b\u5728\u81ea\u52a8\u5316\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7ec4\u4ef6\u534f\u8c03\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "RD-Agent(Q)\u5c06\u91cf\u5316\u6d41\u7a0b\u5206\u89e3\u4e3a\u7814\u7a76\u9636\u6bb5\u548c\u5f00\u53d1\u9636\u6bb5\uff0c\u5e76\u901a\u8fc7\u53cd\u9988\u9636\u6bb5\u8fed\u4ee3\u4f18\u5316\uff0c\u91c7\u7528\u591a\u81c2\u8001\u864e\u673a\u8c03\u5ea6\u5668\u81ea\u9002\u5e94\u9009\u62e9\u65b9\u5411\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRD-Agent(Q)\u7684\u5e74\u5316\u56de\u62a5\u7387\u8f83\u4f20\u7edf\u56e0\u5b50\u5e93\u63d0\u53472\u500d\uff0c\u540c\u65f6\u56e0\u5b50\u6570\u91cf\u51cf\u5c1170%\uff0c\u5e76\u5728\u771f\u5b9e\u5e02\u573a\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u8054\u5408\u4f18\u5316\u56e0\u5b50\u548c\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u7b56\u7565\u9c81\u68d2\u6027\u7684\u5e73\u8861\u3002", "keywords": "\u91cf\u5316\u91d1\u878d\u3001\u591a\u4ee3\u7406\u7cfb\u7edf\u3001\u56e0\u5b50\u6a21\u578b\u3001\u81ea\u52a8\u5316\u7b56\u7565\u3001\u673a\u5668\u5b66\u4e60"}}
{"id": "2505.15674", "pdf": "https://arxiv.org/pdf/2505.15674", "abs": "https://arxiv.org/abs/2505.15674", "authors": ["Miao Yu", "Liang Lin", "Guibin Zhang", "Xinfeng Li", "Junfeng Fang", "Ningyu Zhang", "Kun Wang", "Yang Wang"], "title": "UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models require iterative updates to address challenges such as\nknowledge conflicts and outdated information (e.g., incorrect, private, or\nillegal contents). Machine unlearning provides a systematic methodology for\ntargeted knowledge removal from trained models, enabling elimination of\nsensitive information influences. However, mainstream fine-tuning-based\nunlearning methods often fail to balance unlearning efficacy and model ability,\nfrequently resulting in catastrophic model collapse under extensive knowledge\nremoval. Meanwhile, in-context unlearning, which relies solely on contextual\nprompting without modifying the model's intrinsic mechanisms, suffers from\nlimited generalizability and struggles to achieve true unlearning. In this\nwork, we introduce UniErase, a novel unlearning paradigm that employs learnable\nparametric suffix (unlearning token) to steer language models toward targeted\nforgetting behaviors. UniErase operates through two key phases: (I) an\noptimization stage that binds desired unlearning outputs to the model's\nautoregressive probability distribution via token optimization, followed by\n(II) a lightweight model editing phase that activates the learned token to\nprobabilistically induce specified forgetting objective. Serving as a new\nresearch direction for token learning to induce unlearning target, UniErase\nachieves state-of-the-art (SOTA) performance across batch, sequential, and\nprecise unlearning under fictitious and real-world knowledge settings.\nRemarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66%\nof the LLM parameters, outperforms previous forgetting SOTA baseline by around\n4.01 times for model ability with even better unlearning efficacy. Similarly,\nUniErase, maintaining more ability, also surpasses previous retaining SOTA by\n35.96% for unlearning efficacy, showing dual top-tier performances in current\nunlearing domain.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUniErase\u7684\u65b0\u9896\u9057\u5fd8\u8303\u5f0f\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u53c2\u6570\u540e\u7f00\u5f15\u5bfc\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u76ee\u6807\u9057\u5fd8\u884c\u4e3a\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6a21\u578b\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u8fed\u4ee3\u66f4\u65b0\u4ee5\u89e3\u51b3\u77e5\u8bc6\u51b2\u7a81\u548c\u8fc7\u65f6\u4fe1\u606f\u7b49\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u9057\u5fd8\u6548\u80fd\u548c\u6a21\u578b\u80fd\u529b\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u5bb9\u6613\u51fa\u73b0\u6a21\u578b\u5d29\u6e83\u6216\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "UniErase\u901a\u8fc7\u4e24\u9636\u6bb5\u5b9e\u73b0\u76ee\u6807\u9057\u5fd8\uff1a(I) \u4f18\u5316\u9636\u6bb5\u5c06\u9057\u5fd8\u76ee\u6807\u7ed1\u5b9a\u5230\u6a21\u578b\u7684\u6982\u7387\u5206\u5e03\uff0c(II) \u8f7b\u91cf\u7ea7\u6a21\u578b\u7f16\u8f91\u9636\u6bb5\u6fc0\u6d3b\u5b66\u4e60\u5230\u7684\u6807\u8bb0\u4ee5\u8bf1\u5bfc\u9057\u5fd8\u3002", "result": "UniErase\u5728\u865a\u6784\u548c\u771f\u5b9e\u77e5\u8bc6\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u6279\u91cf\u3001\u987a\u5e8f\u548c\u7cbe\u786e\u9057\u5fd8\u7684SOTA\u6027\u80fd\uff0c\u4ec5\u4fee\u6539\u7ea63.66%\u7684\u6a21\u578b\u53c2\u6570\uff0c\u9057\u5fd8\u6548\u80fd\u63d0\u53474.01\u500d\uff0c\u4fdd\u6301\u66f4\u9ad8\u6a21\u578b\u80fd\u529b\u3002", "conclusion": "UniErase\u5c55\u793a\u4e86\u5728\u9057\u5fd8\u9886\u57df\u7684\u53cc\u91cd\u9876\u7ea7\u8868\u73b0\uff0c\u4e3a\u53c2\u6570\u5b66\u4e60\u8bf1\u5bfc\u9057\u5fd8\u76ee\u6807\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u673a\u5668\u9057\u5fd8\u3001UniErase\u3001\u53c2\u6570\u4f18\u5316\u3001\u6a21\u578b\u7f16\u8f91"}}
{"id": "2505.15626", "pdf": "https://arxiv.org/pdf/2505.15626", "abs": "https://arxiv.org/abs/2505.15626", "authors": ["Jacopo Teneggi", "Zhenzhen Wang", "Paul H. Yi", "Tianmin Shu", "Jeremias Sulam"], "title": "Aligning Explanations with Human Communication", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Machine learning explainability aims to make the decision-making process of\nblack-box models more transparent by finding the most important input features\nfor a given prediction task. Recent works have proposed composing explanations\nfrom semantic concepts (e.g., colors, patterns, shapes) that are inherently\ninterpretable to the user of a model. However, these methods generally ignore\nthe communicative context of explanation-the ability of the user to understand\nthe prediction of the model from the explanation. For example, while a medical\ndoctor might understand an explanation in terms of clinical markers, a patient\nmay need a more accessible explanation to make sense of the same diagnosis. In\nthis paper, we address this gap with listener-adaptive explanations. We propose\nan iterative procedure grounded in principles of pragmatic reasoning and the\nrational speech act to generate explanations that maximize communicative\nutility. Our procedure only needs access to pairwise preferences between\ncandidate explanations, relevant in real-world scenarios where a listener model\nmay not be available. We evaluate our method in image classification tasks,\ndemonstrating improved alignment between explanations and listener preferences\nacross three datasets. Furthermore, we perform a user study that demonstrates\nour explanations increase communicative utility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u542c\u4f17\u81ea\u9002\u5e94\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u7528\u63a8\u7406\u548c\u7406\u6027\u8a00\u8bed\u884c\u4e3a\u751f\u6210\u66f4\u5177\u6c9f\u901a\u6548\u7528\u7684\u89e3\u91ca\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u6c9f\u901a\u80cc\u666f\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u53ef\u89e3\u91ca\u6027\u89e3\u91ca\u65f6\u5ffd\u7565\u4e86\u542c\u4f17\u7684\u80cc\u666f\u548c\u7406\u89e3\u80fd\u529b\uff0c\u5bfc\u81f4\u89e3\u91ca\u6548\u679c\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u542c\u4f17\u81ea\u9002\u5e94\u89e3\u91ca\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u7528\u63a8\u7406\u548c\u7406\u6027\u8a00\u8bed\u884c\u4e3a\u7684\u8fed\u4ee3\u7a0b\u5e8f\uff0c\u4ec5\u9700\u5019\u9009\u89e3\u91ca\u7684\u6210\u5bf9\u504f\u597d\u5373\u53ef\u751f\u6210\u9ad8\u6548\u7528\u7684\u89e3\u91ca\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u8bc1\u660e\u5176\u89e3\u91ca\u4e0e\u542c\u4f17\u504f\u597d\u66f4\u5339\u914d\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u5c55\u793a\u4e86\u6c9f\u901a\u6548\u7528\u7684\u63d0\u5347\u3002", "conclusion": "\u542c\u4f17\u81ea\u9002\u5e94\u89e3\u91ca\u80fd\u591f\u663e\u8457\u63d0\u5347\u6c9f\u901a\u6548\u7528\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u4e2d\u542c\u4f17\u6a21\u578b\u4e0d\u53ef\u7528\u7684\u60c5\u51b5\u3002", "keywords": "\u673a\u5668\u53ef\u89e3\u91ca\u6027, \u542c\u4f17\u81ea\u9002\u5e94\u89e3\u91ca, \u8bed\u7528\u63a8\u7406, \u7406\u6027\u8a00\u8bed\u884c\u4e3a, \u6c9f\u901a\u6548\u7528"}}
{"id": "2505.15173", "pdf": "https://arxiv.org/pdf/2505.15173", "abs": "https://arxiv.org/abs/2505.15173", "authors": ["Zhipei Xu", "Xuanyu Zhang", "Xing Zhou", "Jian Zhang"], "title": "AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The rapid advancement of Artificial Intelligence Generated Content (AIGC)\ntechnologies, particularly in video generation, has led to unprecedented\ncreative capabilities but also increased threats to information integrity,\nidentity security, and public trust. Existing detection methods, while\neffective in general scenarios, lack robust solutions for human-centric videos,\nwhich pose greater risks due to their realism and potential for legal and\nethical misuse. Moreover, current detection approaches often suffer from poor\ngeneralization, limited scalability, and reliance on labor-intensive supervised\nfine-tuning. To address these challenges, we propose AvatarShield, the first\ninterpretable MLLM-based framework for detecting human-centric fake videos,\nenhanced via Group Relative Policy Optimization (GRPO). Through our carefully\ndesigned accuracy detection reward and temporal compensation reward, it\neffectively avoids the use of high-cost text annotation data, enabling precise\ntemporal modeling and forgery detection. Meanwhile, we design a dual-encoder\narchitecture, combining high-level semantic reasoning and low-level artifact\namplification to guide MLLMs in effective forgery detection. We further collect\nFakeHumanVid, a large-scale human-centric video benchmark that includes\nsynthesis methods guided by pose, audio, and text inputs, enabling rigorous\nevaluation of detection methods in real-world scenes. Extensive experiments\nshow that AvatarShield significantly outperforms existing approaches in both\nin-domain and cross-domain detection, setting a new standard for human-centric\nvideo forensics.", "AI": {"tldr": "AvatarShield \u662f\u4e00\u79cd\u57fa\u4e8e MLLM \u7684\u53ef\u89e3\u91ca\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5047\u89c6\u9891\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5956\u52b1\u673a\u5236\u548c\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "AIGC \u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u865a\u5047\u89c6\u9891\u7684\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4ee5\u4eba\u4e3a\u672c\u7684\u89c6\u9891\u68c0\u6d4b\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa AvatarShield \u6846\u67b6\uff0c\u7ed3\u5408 GRPO \u5956\u52b1\u673a\u5236\u548c\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u4f2a\u9020\u68c0\u6d4b\u3002", "result": "\u5728 FakeHumanVid \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAvatarShield \u5728\u57df\u5185\u548c\u8de8\u57df\u68c0\u6d4b\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AvatarShield \u4e3a\u4ee5\u4eba\u4e3a\u672c\u7684\u89c6\u9891\u53d6\u8bc1\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002", "keywords": "AIGC, \u89c6\u9891\u68c0\u6d4b, MLLM, GRPO, \u53cc\u7f16\u7801\u5668, FakeHumanVid, \u4f2a\u9020\u68c0\u6d4b"}}
{"id": "2505.15682", "pdf": "https://arxiv.org/pdf/2505.15682", "abs": "https://arxiv.org/abs/2505.15682", "authors": ["Cosimo Iaia", "Bhavin Choksi", "Emily Wiebers", "Gemma Roig", "Christian J. Fiebach"], "title": "The Representational Alignment between Humans and Language Models is implicitly driven by a Concreteness Effect", "categories": ["cs.CL", "I.2.7; J.4"], "comment": "13 pages, 4 Figures, 1 Table", "summary": "The nouns of our language refer to either concrete entities (like a table) or\nabstract concepts (like justice or love), and cognitive psychology has\nestablished that concreteness influences how words are processed. Accordingly,\nunderstanding how concreteness is represented in our mind and brain is a\ncentral question in psychology, neuroscience, and computational linguistics.\nWhile the advent of powerful language models has allowed for quantitative\ninquiries into the nature of semantic representations, it remains largely\nunderexplored how they represent concreteness. Here, we used behavioral\njudgments to estimate semantic distances implicitly used by humans, for a set\nof carefully selected abstract and concrete nouns. Using Representational\nSimilarity Analysis, we find that the implicit representational space of\nparticipants and the semantic representations of language models are\nsignificantly aligned. We also find that both representational spaces are\nimplicitly aligned to an explicit representation of concreteness, which was\nobtained from our participants using an additional concreteness rating task.\nImportantly, using ablation experiments, we demonstrate that the human-to-model\nalignment is substantially driven by concreteness, but not by other important\nword characteristics established in psycholinguistics. These results indicate\nthat humans and language models converge on the concreteness dimension, but not\non other dimensions.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u4e2d\u5177\u4f53\u6027\uff08concreteness\uff09\u7684\u8868\u5f81\uff0c\u53d1\u73b0\u4eba\u7c7b\u4e0e\u6a21\u578b\u5728\u8fd9\u7ef4\u5ea6\u4e0a\u663e\u8457\u5bf9\u9f50\uff0c\u800c\u5176\u4ed6\u5fc3\u7406\u8bed\u8a00\u5b66\u7279\u5f81\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u7406\u89e3\u5177\u4f53\u6027\u5728\u5fc3\u7406\u548c\u5927\u8111\u4e2d\u7684\u8868\u5f81\u662f\u5fc3\u7406\u5b66\u3001\u795e\u7ecf\u79d1\u5b66\u548c\u8ba1\u7b97\u8bed\u8a00\u5b66\u7684\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u884c\u4e3a\u5224\u65ad\u548c\u8868\u5f81\u76f8\u4f3c\u6027\u5206\u6790\uff0c\u5bf9\u6bd4\u4eba\u7c7b\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u8868\u5f81\u3002", "result": "\u4eba\u7c7b\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u8868\u5f81\u7a7a\u95f4\u5728\u5177\u4f53\u6027\u7ef4\u5ea6\u4e0a\u663e\u8457\u5bf9\u9f50\uff0c\u4e14\u4e3b\u8981\u7531\u5177\u4f53\u6027\u9a71\u52a8\u3002", "conclusion": "\u4eba\u7c7b\u548c\u8bed\u8a00\u6a21\u578b\u5728\u5177\u4f53\u6027\u7ef4\u5ea6\u4e0a\u8868\u73b0\u4e00\u81f4\uff0c\u4f46\u5176\u4ed6\u7ef4\u5ea6\u65e0\u663e\u8457\u5bf9\u9f50\u3002", "keywords": "\u5177\u4f53\u6027, \u8bed\u8a00\u6a21\u578b, \u8868\u5f81\u76f8\u4f3c\u6027, \u8ba4\u77e5\u5fc3\u7406\u5b66"}}
{"id": "2505.15631", "pdf": "https://arxiv.org/pdf/2505.15631", "abs": "https://arxiv.org/abs/2505.15631", "authors": ["Nick Kocher", "Christian Wassermann", "Leona Hennig", "Jonas Seng", "Holger Hoos", "Kristian Kersting", "Marius Lindauer", "Matthias M\u00fcller"], "title": "Guidelines for the Quality Assessment of Energy-Aware NAS Benchmarks", "categories": ["cs.LG"], "comment": null, "summary": "Neural Architecture Search (NAS) accelerates progress in deep learning\nthrough systematic refinement of model architectures. The downside is\nincreasingly large energy consumption during the search process.\nSurrogate-based benchmarking mitigates the cost of full training by querying a\npre-trained surrogate to obtain an estimate for the quality of the model.\nSpecifically, energy-aware benchmarking aims to make it possible for NAS to\nfavourably trade off model energy consumption against accuracy. Towards this\nend, we propose three design principles for such energy-aware benchmarks: (i)\nreliable power measurements, (ii) a wide range of GPU usage, and (iii) holistic\ncost reporting. We analyse EA-HAS-Bench based on these principles and find that\nthe choice of GPU measurement API has a large impact on the quality of results.\nUsing the Nvidia System Management Interface (SMI) on top of its underlying\nlibrary influences the sampling rate during the initial data collection,\nreturning faulty low-power estimations. This results in poor correlation with\naccurate measurements obtained from an external power meter. With this study,\nwe bring to attention several key considerations when performing energy-aware\nsurrogate-based benchmarking and derive first guidelines that can help design\nnovel benchmarks. We show a narrow usage range of the four GPUs attached to our\ndevice, ranging from 146 W to 305 W in a single-GPU setting, and narrowing down\neven further when using all four GPUs. To improve holistic energy reporting, we\npropose calibration experiments over assumptions made in popular tools, such as\nCode Carbon, thus achieving reductions in the maximum inaccuracy from 10.3 % to\n8.9 % without and to 6.6 % with prior estimation of the expected load on the\ndevice.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u80fd\u91cf\u611f\u77e5\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u57fa\u51c6\u8bbe\u8ba1\u539f\u5219\uff0c\u5e76\u5206\u6790\u4e86GPU\u6d4b\u91cfAPI\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "motivation": "\u51cf\u5c11NAS\u8fc7\u7a0b\u4e2d\u7684\u9ad8\u80fd\u8017\u95ee\u9898\uff0c\u901a\u8fc7\u80fd\u91cf\u611f\u77e5\u57fa\u51c6\u8bbe\u8ba1\u4f18\u5316\u6a21\u578b\u80fd\u6548\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u8bbe\u8ba1\u539f\u5219\uff1a\u53ef\u9760\u529f\u7387\u6d4b\u91cf\u3001\u5e7f\u6cdbGPU\u4f7f\u7528\u8303\u56f4\u548c\u5168\u9762\u6210\u672c\u62a5\u544a\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u53d1\u73b0GPU\u6d4b\u91cfAPI\u5bf9\u7ed3\u679c\u8d28\u91cf\u5f71\u54cd\u663e\u8457\uff0c\u4f7f\u7528Nvidia SMI\u4f1a\u5bfc\u81f4\u521d\u59cb\u6570\u636e\u6536\u96c6\u7684\u4f4e\u529f\u7387\u4f30\u503c\u4e0d\u51c6\u786e\u3002", "conclusion": "\u5f3a\u8c03\u80fd\u91cf\u611f\u77e5\u57fa\u51c6\u8bbe\u8ba1\u7684\u5173\u952e\u8003\u8651\u56e0\u7d20\uff0c\u5e76\u63d0\u4f9b\u6539\u8fdb\u5efa\u8bae\uff0c\u5982\u6821\u51c6\u5b9e\u9a8c\u4ee5\u63d0\u5347\u5de5\u5177\u51c6\u786e\u6027\u3002", "keywords": "Neural Architecture Search, energy-aware benchmarking, GPU measurement, surrogate modeling"}}
{"id": "2505.15683", "pdf": "https://arxiv.org/pdf/2505.15683", "abs": "https://arxiv.org/abs/2505.15683", "authors": ["Zishuai Zhang", "Hainan Zhang", "Jiaying Zheng", "Ziwei Wang", "Yongxin Tong", "Jin Dong", "Zhiming Zheng"], "title": "A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability", "categories": ["cs.CL", "cs.AI", "cs.DC"], "comment": null, "summary": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability.", "AI": {"tldr": "FL-LLaMA\u662f\u4e00\u4e2a\u57fa\u4e8eLLaMA2\u7684\u5b89\u5168\u3001\u9ad8\u6548\u4e14\u81ea\u9002\u5e94\u7684\u8054\u90a6\u5206\u5272\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u79c1\u6709\u6570\u636e\u5728\u5206\u6563\u73af\u5883\u4e0b\u8bad\u7ec3\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u65f6\u7684\u5b89\u5168\u3001\u6548\u7387\u548c\u9002\u5e94\u6027\u95ee\u9898\u3002", "motivation": "\u79c1\u6709\u6570\u636e\u5206\u6563\u4e14\u8d28\u91cf\u9ad8\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\u3001\u901a\u4fe1\u5f00\u9500\u5927\u548c\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\uff0cFL-LLaMA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5c06\u90e8\u5206\u8f93\u5165\u8f93\u51fa\u5757\u653e\u5728\u672c\u5730\u5ba2\u6237\u7aef\u3001\u6ce8\u5165\u9ad8\u65af\u566a\u58f0\u3001\u5e76\u884c\u8bad\u7ec3\u7b56\u7565\u3001\u6ce8\u610f\u529b\u63a9\u7801\u538b\u7f29\u548c\u52a8\u6001\u8c03\u6574\u5206\u5272\u70b9\u7b49\u65b9\u6cd5\uff0c\u63d0\u5347\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "result": "\u5728NLU\u3001\u6458\u8981\u548c\u5bf9\u8bdd\u4efb\u52a1\u4e2d\uff0cFL-LLaMA\u6027\u80fd\u4e0e\u96c6\u4e2d\u5f0fLLaMA2\u76f8\u5f53\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u53472\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53478\u500d\u3002", "conclusion": "FL-LLaMA\u5728\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u9002\u5e94\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e0b\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "keywords": "FL-LLaMA, \u8054\u90a6\u5b66\u4e60, LLaMA2, \u9690\u79c1\u4fdd\u62a4, \u5e76\u884c\u8bad\u7ec3"}}
{"id": "2505.15638", "pdf": "https://arxiv.org/pdf/2505.15638", "abs": "https://arxiv.org/abs/2505.15638", "authors": ["Daniel Waxman", "Fernando Llorente", "Petar M. Djuri\u0107"], "title": "Bayesian Ensembling: Insights from Online Optimization and Empirical Bayes", "categories": ["cs.LG", "stat.CO", "stat.ME", "stat.ML"], "comment": "25 pages, 12 figures", "summary": "We revisit the classical problem of Bayesian ensembles and address the\nchallenge of learning optimal combinations of Bayesian models in an online,\ncontinual learning setting. To this end, we reinterpret existing approaches\nsuch as Bayesian model averaging (BMA) and Bayesian stacking through a novel\nempirical Bayes lens, shedding new light on the limitations and pathologies of\nBMA. Further motivated by insights from online optimization, we propose Online\nBayesian Stacking (OBS), a method that optimizes the log-score over predictive\ndistributions to adaptively combine Bayesian models. A key contribution of our\nwork is establishing a novel connection between OBS and portfolio selection,\nbridging Bayesian ensemble learning with a rich, well-studied theoretical\nframework that offers efficient algorithms and extensive regret analysis. We\nfurther clarify the relationship between OBS and online BMA, showing that they\noptimize related but distinct cost functions. Through theoretical analysis and\nempirical evaluation, we identify scenarios where OBS outperforms online BMA\nand provide principled guidance on when practitioners should prefer one\napproach over the other.", "AI": {"tldr": "\u8be5\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u8d1d\u53f6\u65af\u96c6\u6210\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u8d1d\u53f6\u65af\u5806\u53e0\u65b9\u6cd5\uff08OBS\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u9884\u6d4b\u5206\u5e03\u7684\u5bf9\u6570\u5206\u6570\u81ea\u9002\u5e94\u5730\u7ec4\u5408\u8d1d\u53f6\u65af\u6a21\u578b\uff0c\u5e76\u4e0e\u5728\u7ebf\u8d1d\u53f6\u65af\u6a21\u578b\u5e73\u5747\uff08BMA\uff09\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u73af\u5883\u4e2d\u8d1d\u53f6\u65af\u6a21\u578b\u7684\u6700\u4f18\u7ec4\u5408\u95ee\u9898\uff0c\u91cd\u65b0\u5ba1\u89c6\u73b0\u6709\u65b9\u6cd5\uff08\u5982BMA\u548c\u8d1d\u53f6\u65af\u5806\u53e0\uff09\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u5728\u7ebf\u8d1d\u53f6\u65af\u5806\u53e0\uff08OBS\uff09\uff0c\u5229\u7528\u7ecf\u9a8c\u8d1d\u53f6\u65af\u89c6\u89d2\u548c\u5728\u7ebf\u4f18\u5316\u7406\u8bba\uff0c\u4f18\u5316\u9884\u6d4b\u5206\u5e03\u7684\u5bf9\u6570\u5206\u6570\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u53d1\u73b0OBS\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u4f18\u4e8e\u5728\u7ebfBMA\u3002", "conclusion": "OBS\u4e0eBMA\u4f18\u5316\u4e86\u4e0d\u540c\u4f46\u76f8\u5173\u7684\u6210\u672c\u51fd\u6570\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u9009\u62e9\u65b9\u6cd5\u7684\u4f9d\u636e\u3002", "keywords": "\u8d1d\u53f6\u65af\u96c6\u6210\u3001\u5728\u7ebf\u5b66\u4e60\u3001\u8d1d\u53f6\u65af\u6a21\u578b\u5e73\u5747\u3001\u8d1d\u53f6\u65af\u5806\u53e0\u3001\u6295\u8d44\u7ec4\u5408\u9009\u62e9"}}
{"id": "2505.15197", "pdf": "https://arxiv.org/pdf/2505.15197", "abs": "https://arxiv.org/abs/2505.15197", "authors": ["Pinxin Liu", "Haiyang Liu", "Luchuan Song", "Chenliang Xu"], "title": "Intentional Gesture: Deliver Your Intentions with Gestures for Speech", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": null, "summary": "When humans speak, gestures help convey communicative intentions, such as\nadding emphasis or describing concepts. However, current co-speech gesture\ngeneration methods rely solely on superficial linguistic cues (\\textit{e.g.}\nspeech audio or text transcripts), neglecting to understand and leverage the\ncommunicative intention that underpins human gestures. This results in outputs\nthat are rhythmically synchronized with speech but are semantically shallow. To\naddress this gap, we introduce \\textbf{Intentional-Gesture}, a novel framework\nthat casts gesture generation as an intention-reasoning task grounded in\nhigh-level communicative functions. % First, we curate the \\textbf{InG} dataset\nby augmenting BEAT-2 with gesture-intention annotations (\\textit{i.e.}, text\nsentences summarizing intentions), which are automatically annotated using\nlarge vision-language models. Next, we introduce the \\textbf{Intentional\nGesture Motion Tokenizer} to leverage these intention annotations. It injects\nhigh-level communicative functions (\\textit{e.g.}, intentions) into tokenized\nmotion representations to enable intention-aware gesture synthesis that are\nboth temporally aligned and semantically meaningful, achieving new\nstate-of-the-art performance on the BEAT-2 benchmark. Our framework offers a\nmodular foundation for expressive gesture generation in digital humans and\nembodied AI. Project Page: https://andypinxinliu.github.io/Intentional-Gesture", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIntentional-Gesture\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9ad8\u5c42\u4ea4\u9645\u610f\u56fe\u6765\u5b9e\u73b0\u8bed\u4e49\u4e30\u5bcc\u4e14\u65f6\u95f4\u540c\u6b65\u7684\u624b\u52bf\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4ea4\u9645\u610f\u56fe\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u624b\u52bf\u751f\u6210\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6d45\u5c42\u8bed\u8a00\u7ebf\u7d22\uff08\u5982\u8bed\u97f3\u6216\u6587\u672c\uff09\uff0c\u5ffd\u7565\u4ea4\u9645\u610f\u56fe\uff0c\u5bfc\u81f4\u751f\u6210\u624b\u52bf\u867d\u4e0e\u8bed\u97f3\u8282\u594f\u540c\u6b65\u4f46\u8bed\u4e49\u6d45\u8584\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faIntentional-Gesture\u6846\u67b6\uff0c\u57fa\u4e8e\u9ad8\u5c42\u4ea4\u9645\u529f\u80fd\uff08\u610f\u56fe\uff09\u751f\u6210\u624b\u52bf\u3002\u901a\u8fc7\u589e\u5f3aBEAT-2\u6570\u636e\u96c6\u5e76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u6807\u6ce8\u610f\u56fe\uff0c\u5f00\u53d1Intentional Gesture Motion Tokenizer\u4ee5\u6ce8\u5165\u610f\u56fe\u4fe1\u606f\u3002", "result": "\u5728BEAT-2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751f\u6210\u4e86\u65f6\u95f4\u5bf9\u9f50\u4e14\u8bed\u4e49\u4e30\u5bcc\u7684\u624b\u52bf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6570\u5b57\u4eba\u7c7b\u548c\u5177\u8eabAI\u4e2d\u7684\u624b\u52bf\u751f\u6210\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u57fa\u7840\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\u3002", "keywords": "\u624b\u52bf\u751f\u6210\u3001\u4ea4\u9645\u610f\u56fe\u3001\u6570\u5b57\u4eba\u7c7b\u3001\u5177\u8eabAI\u3001\u8bed\u4e49\u4e30\u5bcc"}}
{"id": "2505.15684", "pdf": "https://arxiv.org/pdf/2505.15684", "abs": "https://arxiv.org/abs/2505.15684", "authors": ["Gengyang Li", "Yifeng Gao", "Yuming Li", "Yunfang Wu"], "title": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing Reasoning Redundancy", "categories": ["cs.CL"], "comment": null, "summary": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption.", "AI": {"tldr": "ThinkLess\u662f\u4e00\u4e2a\u63a8\u7406\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u524d\u7ec8\u6b62\u63a8\u7406\u751f\u6210\u6765\u51cf\u5c11\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3Chain-of-Thought (CoT)\u63d0\u793a\u65b9\u6cd5\u4e2d\u63a8\u7406\u4ee4\u724c\u8fc7\u957f\u5bfc\u81f4\u7684\u5ef6\u8fdf\u3001\u5185\u5b58\u6d88\u8017\u4ee5\u53ca\u53ef\u80fd\u622a\u65ad\u7b54\u6848\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6ce8\u610f\u529b\u5206\u6790\u53d1\u73b0\u7b54\u6848\u4ee4\u724c\u4e3b\u8981\u5173\u6ce8\u63a8\u7406\u7ec8\u6b62\u4ee4\u724c\uff0c\u56e0\u6b64\u5728\u65e9\u671f\u4f4d\u7f6e\u63d2\u5165\u7ec8\u6b62\u4ee4\u724c\u4ee5\u8df3\u8fc7\u5197\u4f59\u63a8\u7406\uff1b\u91c7\u7528\u8f7b\u91cf\u7ea7\u540e\u8c03\u8282\u673a\u5236\u4fdd\u8bc1\u8f93\u51fa\u683c\u5f0f\u3002", "result": "\u5728\u4e0d\u8fdb\u884c\u5fae\u8c03\u6216\u4f7f\u7528\u989d\u5916\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0cThinkLess\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u89e3\u7801\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u3002", "conclusion": "ThinkLess\u662f\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u6a21\u578b\u5373\u53ef\u63d0\u5347\u63a8\u7406\u6548\u7387\u7684\u6709\u6548\u65b9\u6cd5\u3002", "keywords": "Chain-of-Thought, \u63a8\u7406\u4f18\u5316, \u8bed\u8a00\u6a21\u578b, \u6ce8\u610f\u529b\u673a\u5236"}}
{"id": "2505.15643", "pdf": "https://arxiv.org/pdf/2505.15643", "abs": "https://arxiv.org/abs/2505.15643", "authors": ["Lan V. Truong"], "title": "Optimal Best-Arm Identification under Fixed Confidence with Multiple Optima", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "comment": "22 pages", "summary": "We study the problem of best-arm identification in stochastic multi-armed\nbandits under the fixed-confidence setting, with a particular focus on\ninstances that admit multiple optimal arms. While the Track-and-Stop algorithm\nof Garivier and Kaufmann (2016) is widely conjectured to be instance-optimal,\nits performance in the presence of multiple optima has remained insufficiently\nunderstood. In this work, we revisit the Track-and-Stop strategy and propose a\nmodified stopping rule that ensures instance-optimality even when the set of\noptimal arms is not a singleton. Our analysis introduces a new\ninformation-theoretic lower bound that explicitly accounts for multiple optimal\narms, and we demonstrate that our stopping rule tightly matches this bound.", "AI": {"tldr": "\u7814\u7a76\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u4e2d\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u8bbe\u7f6e\u4e0b\u7684\u6700\u4f18\u81c2\u8bc6\u522b\uff0c\u63d0\u51fa\u6539\u8fdb\u7684\u505c\u6b62\u89c4\u5219\u4ee5\u9002\u5e94\u591a\u6700\u4f18\u81c2\u60c5\u51b5\u3002", "motivation": "\u73b0\u6709Track-and-Stop\u7b97\u6cd5\u5728\u591a\u6700\u4f18\u81c2\u60c5\u51b5\u4e0b\u6027\u80fd\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u4ee5\u5b9e\u73b0\u5b9e\u4f8b\u6700\u4f18\u6027\u3002", "method": "\u4fee\u6539Track-and-Stop\u7684\u505c\u6b62\u89c4\u5219\uff0c\u5f15\u5165\u65b0\u4fe1\u606f\u8bba\u4e0b\u754c\u4ee5\u8003\u8651\u591a\u6700\u4f18\u81c2\u3002", "result": "\u65b0\u505c\u6b62\u89c4\u5219\u7d27\u5bc6\u5339\u914d\u7406\u8bba\u4e0b\u754c\uff0c\u5b9e\u4f8b\u6700\u4f18\u3002", "conclusion": "\u6539\u8fdb\u7684\u505c\u6b62\u89c4\u5219\u5728\u591a\u6700\u4f18\u81c2\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "keywords": "\u591a\u81c2\u8001\u864e\u673a, \u6700\u4f18\u81c2\u8bc6\u522b, \u56fa\u5b9a\u7f6e\u4fe1\u5ea6, \u505c\u6b62\u89c4\u5219, \u4fe1\u606f\u8bba\u4e0b\u754c"}}
{"id": "2505.15201", "pdf": "https://arxiv.org/pdf/2505.15201", "abs": "https://arxiv.org/abs/2505.15201", "authors": ["Christian Walder", "Deep Karkhanis"], "title": "Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts\nfor each problem and reward them independently. This optimizes for pass@1\nperformance and prioritizes the strength of isolated samples at the expense of\nthe diversity and collective utility of sets of samples. This under-utilizes\nthe sampling capacity, limiting exploration and eventual improvement on harder\nexamples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a\ntransformation on the final rewards which leads to direct optimization of\npass@k performance, thus optimizing for sets of samples that maximize reward\nwhen considered jointly. Our contribution is to derive novel low variance\nunbiased estimators for pass@k and its gradient, in both the binary and\ncontinuous reward settings. We show optimization with our estimators reduces to\nstandard RL with rewards that have been jointly transformed by a stable and\nefficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable\nrobust optimization of pass@k for any arbitrary k <= n. Moreover, instead of\ntrading off pass@1 performance for pass@k gains, our method allows annealing k\nduring training, optimizing both metrics and often achieving strong pass@1\nnumbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the\nvariance reducing properties of our formulations. We also include real-world\nexamples using the open-source LLM, GEMMA-2. We find that our transformation\neffectively optimizes for the target k. Furthermore, higher k values enable\nsolving more and harder problems, while annealing k boosts both the pass@1 and\npass@k . Crucially, for challenging task sets where conventional pass@1\noptimization stalls, our pass@k approach unblocks learning, likely due to\nbetter exploration by prioritizing joint utility over the utility of individual\nsamples.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPKPO\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316pass@k\u6027\u80fd\u6765\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6837\u672c\u591a\u6837\u6027\u548c\u96c6\u4f53\u6548\u7528\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u5355\u4e00\u6837\u672c\u7684pass@1\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f4e\u65b9\u5dee\u65e0\u504f\u4f30\u8ba1\u5668\u5b9e\u73b0\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u4f18\u5316pass@1\u6027\u80fd\u65f6\uff0c\u5ffd\u7565\u4e86\u6837\u672c\u591a\u6837\u6027\u548c\u96c6\u4f53\u6548\u7528\uff0c\u5bfc\u81f4\u91c7\u6837\u80fd\u529b\u5229\u7528\u4e0d\u8db3\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u4f18\u5316pass@k\u6027\u80fd\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86PKPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f6c\u6362\u6700\u7ec8\u5956\u52b1\u76f4\u63a5\u4f18\u5316pass@k\u6027\u80fd\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4f4e\u65b9\u5dee\u65e0\u504f\u4f30\u8ba1\u5668\u53ca\u5176\u68af\u5ea6\u3002\u5141\u8bb8\u5728\u8bad\u7ec3\u4e2d\u52a8\u6001\u8c03\u6574k\u503c\uff0c\u517c\u987epass@1\u548cpass@k\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86PKPO\u65b9\u6cd5\u5728\u4f4e\u65b9\u5dee\u548c\u6709\u6548\u6027\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u80fd\u591f\u89e3\u51b3\u66f4\u96be\u7684\u95ee\u9898\uff0c\u5e76\u5728\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u89e3\u9501\u5b66\u4e60\u6f5c\u529b\u3002", "conclusion": "PKPO\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316pass@k\u6027\u80fd\u63d0\u5347\u4e86\u6837\u672c\u591a\u6837\u6027\u548c\u96c6\u4f53\u6548\u7528\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, PKPO, pass@k\u4f18\u5316, \u4f4e\u65b9\u5dee\u4f30\u8ba1\u5668, \u6837\u672c\u591a\u6837\u6027"}}
{"id": "2505.15692", "pdf": "https://arxiv.org/pdf/2505.15692", "abs": "https://arxiv.org/abs/2505.15692", "authors": ["Jinyang Wu", "Chonghua Liao", "Mingkuan Feng", "Shuai Zhang", "Zhengqi Wen", "Pengpeng Shao", "Huazhe Xu", "Jianhua Tao"], "title": "Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has emerged as an effective method for training\nreasoning models. However, existing RL approaches typically bias the model's\noutput distribution toward reward-maximizing paths without introducing external\nknowledge. This limits their exploration capacity and results in a narrower\nreasoning capability boundary compared to base models. To address this\nlimitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel\nframework that augments RL by incorporating external high-level guidance\n(\"thought patterns\"). By adaptively integrating structured thoughts during\ntraining, TAPO effectively balances model-internal exploration and external\nguidance exploitation. Extensive experiments show that our approach\nsignificantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva\nMath. Notably, these high-level thought patterns, abstracted from only 500\nprior samples, generalize effectively across various tasks and models. This\nhighlights TAPO's potential for broader applications across multiple tasks and\ndomains. Our further analysis reveals that introducing external guidance\nproduces powerful reasoning models with superior explainability of inference\nbehavior and enhanced output readability.", "AI": {"tldr": "TAPO\uff08Thought-Augmented Policy Optimization\uff09\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u9ad8\u5c42\u6b21\u6307\u5bfc\u6765\u589e\u5f3a\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u504f\u5411\u5956\u52b1\u6700\u5927\u5316\u7684\u8def\u5f84\uff0c\u7f3a\u4e4f\u5916\u90e8\u77e5\u8bc6\u5f15\u5165\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u63a2\u7d22\u80fd\u529b\u548c\u63a8\u7406\u8fb9\u754c\u3002", "method": "TAPO\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u6574\u5408\u7ed3\u6784\u5316\u201c\u601d\u7ef4\u6a21\u5f0f\u201d\uff0c\u5e73\u8861\u6a21\u578b\u5185\u90e8\u63a2\u7d22\u4e0e\u5916\u90e8\u6307\u5bfc\u5229\u7528\u3002", "result": "TAPO\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1aAIME\u63d0\u534799%\uff0cAMC\u63d0\u534741%\uff0cMinerva Math\u63d0\u534717%\u3002", "conclusion": "TAPO\u5c55\u793a\u4e86\u5728\u5e7f\u6cdb\u4efb\u52a1\u548c\u9886\u57df\u4e2d\u5e94\u7528\u6f5c\u529b\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u63a8\u7406\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u8f93\u51fa\u53ef\u8bfb\u6027\u3002", "keywords": "\u5f3a\u5316\u5b66\u4e60, \u63a8\u7406\u6a21\u578b, \u5916\u90e8\u6307\u5bfc, TAPO"}}
{"id": "2505.15647", "pdf": "https://arxiv.org/pdf/2505.15647", "abs": "https://arxiv.org/abs/2505.15647", "authors": ["Youming Tao", "Zuyuan Zhang", "Dongxiao Yu", "Xiuzhen Cheng", "Falko Dressler", "Di Wang"], "title": "Second-Order Convergence in Private Stochastic Non-Convex Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We investigate the problem of finding second-order stationary points (SOSP)\nin differentially private (DP) stochastic non-convex optimization. Existing\nmethods suffer from two key limitations: (i) inaccurate convergence error rate\ndue to overlooking gradient variance in the saddle point escape analysis, and\n(ii) dependence on auxiliary private model selection procedures for identifying\nDP-SOSP, which can significantly impair utility, particularly in distributed\nsettings. To address these issues, we propose a generic perturbed stochastic\ngradient descent (PSGD) framework built upon Gaussian noise injection and\ngeneral gradient oracles. A core innovation of our framework is using model\ndrift distance to determine whether PSGD escapes saddle points, ensuring\nconvergence to approximate local minima without relying on second-order\ninformation or additional DP-SOSP identification. By leveraging the adaptive\nDP-SPIDER estimator as a specific gradient oracle, we develop a new DP\nalgorithm that rectifies the convergence error rates reported in prior work. We\nfurther extend this algorithm to distributed learning with arbitrarily\nheterogeneous data, providing the first formal guarantees for finding DP-SOSP\nin such settings. Our analysis also highlights the detrimental impacts of\nprivate selection procedures in distributed learning under high-dimensional\nmodels, underscoring the practical benefits of our design. Numerical\nexperiments on real-world datasets validate the efficacy of our approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5dee\u5206\u9690\u79c1\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4e8c\u9636\u7a33\u5b9a\u70b9\u8bc6\u522b\u4e2d\u7684\u6536\u655b\u8bef\u5dee\u548c\u9690\u79c1\u9009\u62e9\u4f9d\u8d56\u6027\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5dee\u5206\u9690\u79c1\u975e\u51f8\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u6536\u655b\u8bef\u5dee\u7387\u9ad8\u548c\u4f9d\u8d56\u8f85\u52a9\u9690\u79c1\u9009\u62e9\u7a0b\u5e8f\u7684\u5c40\u9650\u6027\uff0c\u5f71\u54cd\u4e86\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u566a\u58f0\u6ce8\u5165\u548c\u901a\u7528\u68af\u5ea6\u9884\u8a00\u673a\u7684\u5e72\u6270\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u6846\u67b6\uff0c\u5229\u7528\u6a21\u578b\u6f02\u79fb\u8ddd\u79bb\u5224\u65ad\u978d\u70b9\u9003\u9038\u3002", "result": "\u65b0\u7b97\u6cd5\u6821\u6b63\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u7684\u6536\u655b\u8bef\u5dee\u7387\uff0c\u5e76\u5728\u5206\u5e03\u5f0f\u5b66\u4e60\u4e2d\u9996\u6b21\u5b9e\u73b0\u4e86\u5bf9\u5dee\u5206\u9690\u79c1\u4e8c\u9636\u7a33\u5b9a\u70b9\u7684\u6b63\u5f0f\u4fdd\u8bc1\u3002", "conclusion": "\u8bba\u6587\u8bbe\u8ba1\u5728\u5b9e\u9645\u6570\u636e\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u9ad8\u7ef4\u5206\u5e03\u5f0f\u5b66\u4e60\u4e2d\u9690\u79c1\u9009\u62e9\u7a0b\u5e8f\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "keywords": "\u5dee\u5206\u9690\u79c1, \u975e\u51f8\u4f18\u5316, \u4e8c\u9636\u7a33\u5b9a\u70b9, \u968f\u673a\u68af\u5ea6\u4e0b\u964d, \u5206\u5e03\u5f0f\u5b66\u4e60"}}
{"id": "2505.15206", "pdf": "https://arxiv.org/pdf/2505.15206", "abs": "https://arxiv.org/abs/2505.15206", "authors": ["Chi Kit Ng", "Long Bai", "Guankun Wang", "Yupeng Wang", "Huxin Gao", "Kun Yuan", "Chenhan Jin", "Tieyong Zeng", "Hongliang Ren"], "title": "EndoVLA: Dual-Phase Vision-Language-Action Model for Autonomous Tracking in Endoscopy", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "In endoscopic procedures, autonomous tracking of abnormal regions and\nfollowing circumferential cutting markers can significantly reduce the\ncognitive burden on endoscopists. However, conventional model-based pipelines\nare fragile for each component (e.g., detection, motion planning) requires\nmanual tuning and struggles to incorporate high-level endoscopic intent,\nleading to poor generalization across diverse scenes. Vision-Language-Action\n(VLA) models, which integrate visual perception, language grounding, and motion\nplanning within an end-to-end framework, offer a promising alternative by\nsemantically adapting to surgeon prompts without manual recalibration. Despite\ntheir potential, applying VLA models to robotic endoscopy presents unique\nchallenges due to the complex and dynamic anatomical environments of the\ngastrointestinal (GI) tract. To address this, we introduce EndoVLA, designed\nspecifically for continuum robots in GI interventions. Given endoscopic images\nand surgeon-issued tracking prompts, EndoVLA performs three core tasks: (1)\npolyp tracking, (2) delineation and following of abnormal mucosal regions, and\n(3) adherence to circular markers during circumferential cutting. To tackle\ndata scarcity and domain shifts, we propose a dual-phase strategy comprising\nsupervised fine-tuning on our EndoVLA-Motion dataset and reinforcement\nfine-tuning with task-aware rewards. Our approach significantly improves\ntracking performance in endoscopy and enables zero-shot generalization in\ndiverse scenes and complex sequential tasks.", "AI": {"tldr": "EndoVLA\u662f\u4e00\u79cd\u4e13\u4e3a\u80c3\u80a0\u9053\u624b\u672f\u8bbe\u8ba1\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u6846\u67b6\u6574\u5408\u89c6\u89c9\u611f\u77e5\u3001\u8bed\u8a00\u7406\u89e3\u548c\u52a8\u4f5c\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u5185\u7aa5\u955c\u624b\u672f\u4e2d\u5f02\u5e38\u533a\u57df\u7684\u81ea\u4e3b\u8ffd\u8e2a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u7ba1\u9053\u5728\u5f02\u5e38\u533a\u57df\u8ffd\u8e2a\u548c\u6807\u8bb0\u8ddf\u8e2a\u4e2d\u8868\u73b0\u8106\u5f31\uff0c\u9700\u8981\u624b\u52a8\u8c03\u6574\u4e14\u96be\u4ee5\u9002\u5e94\u590d\u6742\u573a\u666f\u3002EndoVLA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u901a\u8fc7\u8bed\u4e49\u9002\u5e94\u5916\u79d1\u533b\u751f\u63d0\u793a\uff0c\u65e0\u9700\u624b\u52a8\u91cd\u65b0\u6821\u51c6\u3002", "method": "EndoVLA\u5229\u7528\u7aef\u5230\u7aef\u6846\u67b6\u6574\u5408\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u52a8\u4f5c\u89c4\u5212\uff0c\u63d0\u51fa\u4e86\u53cc\u9636\u6bb5\u7b56\u7565\uff1a\u5728EndoVLA-Motion\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u5956\u52b1\u8fdb\u884c\u5f3a\u5316\u5fae\u8c03\u3002", "result": "EndoVLA\u663e\u8457\u63d0\u5347\u4e86\u5185\u7aa5\u955c\u4e2d\u7684\u8ffd\u8e2a\u6027\u80fd\uff0c\u5e76\u5728\u591a\u6837\u5316\u573a\u666f\u548c\u590d\u6742\u987a\u5e8f\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u3002", "conclusion": "EndoVLA\u4e3a\u673a\u5668\u4eba\u5185\u7aa5\u955c\u624b\u672f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u80fd\u591f\u9002\u5e94\u590d\u6742\u89e3\u5256\u73af\u5883\u5e76\u51cf\u5c11\u5916\u79d1\u533b\u751f\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002", "keywords": "\u5185\u7aa5\u955c\u624b\u672f, \u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b, \u81ea\u4e3b\u8ffd\u8e2a, \u673a\u5668\u4eba\u624b\u672f, EndoVLA"}}
{"id": "2505.15695", "pdf": "https://arxiv.org/pdf/2505.15695", "abs": "https://arxiv.org/abs/2505.15695", "authors": ["Ryang Heo", "Yongsik Seo", "Junseong Lee", "Dongha Lee"], "title": "Can Large Language Models be Effective Online Opinion Miners?", "categories": ["cs.CL"], "comment": "8 pages, 6 figures", "summary": "The surge of user-generated online content presents a wealth of insights into\ncustomer preferences and market trends. However, the highly diverse, complex,\nand context-rich nature of such contents poses significant challenges to\ntraditional opinion mining approaches. To address this, we introduce Online\nOpinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol\ndesigned to assess the ability of large language models (LLMs) to mine opinions\neffectively from diverse and intricate online environments. OOMB provides\nextensive (entity, feature, opinion) tuple annotations and a comprehensive\nopinion-centric summary that highlights key opinion topics within each content,\nthereby enabling the evaluation of both the extractive and abstractive\ncapabilities of models. Through our proposed benchmark, we conduct a\ncomprehensive analysis of which aspects remain challenging and where LLMs\nexhibit adaptability, to explore whether they can effectively serve as opinion\nminers in realistic online scenarios. This study lays the foundation for\nLLM-based opinion mining and discusses directions for future research in this\nfield.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u5728\u7ebf\u610f\u89c1\u6316\u6398\u57fa\u51c6\uff08OOMB\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u6837\u7684\u5728\u7ebf\u73af\u5883\u4e2d\u6316\u6398\u610f\u89c1\u7684\u80fd\u529b\uff0c\u4e3a\u57fa\u4e8eLLM\u7684\u610f\u89c1\u6316\u6398\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u7528\u6237\u751f\u6210\u5185\u5bb9\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\u5bf9\u4f20\u7edf\u610f\u89c1\u6316\u6398\u65b9\u6cd5\u63d0\u51fa\u6311\u6218\uff0c\u9700\u5f00\u53d1\u65b0\u57fa\u51c6\u8bc4\u4f30LLM\u5728\u6b64\u9886\u57df\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faOOMB\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u63d0\u4f9b\uff08\u5b9e\u4f53\u3001\u7279\u5f81\u3001\u610f\u89c1\uff09\u4e09\u5143\u7ec4\u6807\u6ce8\u548c\u610f\u89c1\u6458\u8981\uff0c\u4ee5\u8bc4\u4f30\u6a21\u578b\u7684\u63d0\u53d6\u548c\u6458\u8981\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u57fa\u51c6\u5206\u6790\uff0c\u63ed\u793aLLM\u5728\u610f\u89c1\u6316\u6398\u4e2d\u7684\u6311\u6218\u548c\u9002\u5e94\u6027\uff0c\u63a2\u8ba8\u5176\u5728\u73b0\u5b9e\u5728\u7ebf\u573a\u666f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "OOMB\u4e3aLLM\u610f\u89c1\u6316\u6398\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "keywords": "\u610f\u89c1\u6316\u6398,\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u5728\u7ebf\u5185\u5bb9,\u57fa\u51c6\u8bc4\u4f30"}}
{"id": "2505.15648", "pdf": "https://arxiv.org/pdf/2505.15648", "abs": "https://arxiv.org/abs/2505.15648", "authors": ["Harmender Gahlawat", "Meirav Zehavi"], "title": "Learning Small Decision Trees with Few Outliers: A Parameterized Perspective", "categories": ["cs.LG", "cs.DS"], "comment": null, "summary": "Decision trees are a fundamental tool in machine learning for representing,\nclassifying, and generalizing data. It is desirable to construct ``small''\ndecision trees, by minimizing either the \\textit{size} ($s$) or the\n\\textit{depth} $(d)$ of the \\textit{decision tree} (\\textsc{DT}). Recently, the\nparameterized complexity of \\textsc{Decision Tree Learning} has attracted a lot\nof attention. We consider a generalization of \\textsc{Decision Tree Learning}\nwhere given a \\textit{classification instance} $E$ and an integer $t$, the task\nis to find a ``small'' \\textsc{DT} that disagrees with $E$ in at most $t$\nexamples. We consider two problems: \\textsc{DTSO} and \\textsc{DTDO}, where the\ngoal is to construct a \\textsc{DT} minimizing $s$ and $d$, respectively. We\nfirst establish that both \\textsc{DTSO} and \\textsc{DTDO} are W[1]-hard when\nparameterized by $s+\\delta_{max}$ and $d+\\delta_{max}$, respectively, where\n$\\delta_{max}$ is the maximum number of features in which two differently\nlabeled examples can differ. We complement this result by showing that these\nproblems become \\textsc{FPT} if we include the parameter $t$. We also consider\nthe kernelization complexity of these problems and establish several positive\nand negative results for both \\textsc{DTSO} and \\textsc{DTDO}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u51b3\u7b56\u6811\u5b66\u4e60\u4e2d\u7684\u53c2\u6570\u5316\u590d\u6742\u6027\uff0c\u8003\u5bdf\u4e86\u5728\u5141\u8bb8\u6709\u9650\u9519\u8bef\u4e0b\u7684\u6700\u5c0f\u5316\u51b3\u7b56\u6811\u5c3a\u5bf8\u548c\u6df1\u5ea6\u7684\u95ee\u9898\uff0c\u5e76\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\u3002", "motivation": "\u7814\u7a76\u51b3\u7b56\u6811\u5b66\u4e60\u4e2d\u53c2\u6570\u5316\u590d\u6742\u6027\u7684\u52a8\u673a\u662f\u5e0c\u671b\u5728\u5bb9\u5fcd\u6709\u9650\u9519\u8bef\u7684\u60c5\u51b5\u4e0b\uff0c\u6784\u9020\u66f4\u5c0f\u6216\u66f4\u6d45\u7684\u51b3\u7b56\u6811\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u8bba\u6587\u8003\u5bdf\u4e86\u4e24\u79cd\u95ee\u9898\uff08DTSO\u548cDTDO\uff09\uff0c\u5206\u522b\u9488\u5bf9\u51b3\u7b56\u6811\u7684\u5c3a\u5bf8\u548c\u6df1\u5ea6\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u7ed3\u5408\u53c2\u6570\u5316\u590d\u6742\u6027\u7406\u8bba\u5206\u6790\u4e86\u5176\u8ba1\u7b97\u96be\u5ea6\u548c\u56fa\u5b9a\u53c2\u6570\u53ef\u89e3\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660eDTSO\u548cDTDO\u5728\u7279\u5b9a\u53c2\u6570\u4e0b\u4e3aW[1]-\u96be\u95ee\u9898\uff0c\u4f46\u52a0\u5165\u9519\u8bef\u5bb9\u5fcd\u53c2\u6570t\u540e\u53d8\u4e3a\u56fa\u5b9a\u53c2\u6570\u53ef\u89e3\uff0c\u5e76\u63d0\u4f9b\u4e86\u6838\u5316\u590d\u6742\u6027\u7684\u6b63\u8d1f\u7ed3\u679c\u3002", "conclusion": "\u8bba\u6587\u63ed\u793a\u4e86\u51b3\u7b56\u6811\u5b66\u4e60\u95ee\u9898\u7684\u590d\u6742\u6027\uff0c\u5e76\u5bf9\u4e0d\u540c\u53c2\u6570\u4e0b\u7684\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002", "keywords": "\u51b3\u7b56\u6811, \u53c2\u6570\u5316\u590d\u6742\u6027, \u6838\u5316, W[1]-\u96be, \u56fa\u5b9a\u53c2\u6570\u53ef\u89e3"}}
{"id": "2505.15216", "pdf": "https://arxiv.org/pdf/2505.15216", "abs": "https://arxiv.org/abs/2505.15216", "authors": ["Andy K. Zhang", "Joey Ji", "Celeste Menders", "Riya Dulepet", "Thomas Qin", "Ron Y. Wang", "Junrong Wu", "Kyleen Liao", "Jiliang Li", "Jinghan Hu", "Sara Hong", "Nardos Demilew", "Shivatmica Murgai", "Jason Tran", "Nishka Kacheria", "Ethan Ho", "Denis Liu", "Lauren McLane", "Olivia Bruvik", "Dai-Rong Han", "Seungwoo Kim", "Akhil Vyas", "Cuiyuanxiu Chen", "Ryan Li", "Weiran Xu", "Jonathan Z. Ye", "Prerit Choudhary", "Siddharth M. Bhatia", "Vikram Sivashankar", "Yuxuan Bao", "Dawn Song", "Dan Boneh", "Daniel E. Ho", "Percy Liang"], "title": "BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": "78 pages", "summary": "AI agents have the potential to significantly alter the cybersecurity\nlandscape. To help us understand this change, we introduce the first framework\nto capture offensive and defensive cyber-capabilities in evolving real-world\nsystems. Instantiating this framework with BountyBench, we set up 25 systems\nwith complex, real-world codebases. To capture the vulnerability lifecycle, we\ndefine three task types: Detect (detecting a new vulnerability), Exploit\n(exploiting a specific vulnerability), and Patch (patching a specific\nvulnerability). For Detect, we construct a new success indicator, which is\ngeneral across vulnerability types and provides localized evaluation. We\nmanually set up the environment for each system, including installing packages,\nsetting up server(s), and hydrating database(s). We add 40 bug bounties, which\nare vulnerabilities with monetary awards from \\$10 to \\$30,485, and cover 9 of\nthe OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy\nbased on information to guide detection, interpolating from identifying a zero\nday to exploiting a specific vulnerability. We evaluate 5 agents: Claude Code,\nOpenAI Codex CLI, and custom agents with GPT-4.1, Gemini 2.5 Pro Preview, and\nClaude 3.7 Sonnet Thinking. Given up to three attempts, the top-performing\nagents are Claude Code (5% on Detect, mapping to \\$1,350), Custom Agent with\nClaude 3.7 Sonnet Thinking (5% on Detect, mapping to \\$1,025; 67.5% on\nExploit), and OpenAI Codex CLI (5% on Detect, mapping to \\$2,400; 90% on Patch,\nmapping to \\$14,422). OpenAI Codex CLI and Claude Code are more capable at\ndefense, achieving higher Patch scores of 90% and 87.5%, compared to Exploit\nscores of 32.5% and 57.5% respectively; in contrast, the custom agents are\nrelatively balanced between offense and defense, achieving Exploit scores of\n40-67.5% and Patch scores of 45-60%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9996\u4e2a\u6846\u67b6BountyBench\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u7f51\u7edc\u5b89\u5168\u4e2d\u7684\u653b\u9632\u80fd\u529b\uff0c\u6d4b\u8bd5\u4e865\u79cd\u4ee3\u7406\u5728Detect\u3001Exploit\u548cPatch\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u4e3a\u7406\u89e3AI\u4ee3\u7406\u5982\u4f55\u6539\u53d8\u7f51\u7edc\u5b89\u5168\u9886\u57df\uff0c\u9700\u7cfb\u7edf\u8bc4\u4f30\u5176\u653b\u9632\u80fd\u529b\u3002", "method": "\u6784\u5efa25\u4e2a\u771f\u5b9e\u7cfb\u7edf\u73af\u5883\uff0c\u5b9a\u4e49Detect\u3001Exploit\u548cPatch\u4efb\u52a1\uff0c\u5f15\u5165\u4fe1\u606f\u5bfc\u5411\u7b56\u7565\u8c03\u8282\u4efb\u52a1\u96be\u5ea6\uff0c\u5e76\u8bc4\u4f305\u79cdAI\u4ee3\u7406\u3002", "result": "Claude Code\u3001OpenAI Codex CLI\u548c\u5b9a\u5236\u4ee3\u7406\u8868\u73b0\u6700\u4f73\uff0c\u9632\u5fa1\u4efb\u52a1\u4e2dOpenAI Codex CLI\u548cClaude Code\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "AI\u4ee3\u7406\u5728\u7f51\u7edc\u5b89\u5168\u4e2d\u5c55\u73b0\u51fa\u653b\u9632\u6f5c\u529b\uff0c\u4f46\u8868\u73b0\u56e0\u4efb\u52a1\u7c7b\u578b\u548c\u4ee3\u7406\u80fd\u529b\u800c\u5f02\u3002", "keywords": "AI\u4ee3\u7406, \u7f51\u7edc\u5b89\u5168, BountyBench, \u653b\u9632\u80fd\u529b, \u6f0f\u6d1e\u751f\u547d\u5468\u671f"}}
{"id": "2505.15696", "pdf": "https://arxiv.org/pdf/2505.15696", "abs": "https://arxiv.org/abs/2505.15696", "authors": ["Maike Behrendt", "Stefan Sylvius Wagner", "Stefan Harmeling"], "title": "MaxPoolBERT: Enhancing BERT Classification via Layer- and Token-Wise Aggregation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The [CLS] token in BERT is commonly used as a fixed-length representation for\nclassification tasks, yet prior work has shown that both other tokens and\nintermediate layers encode valuable contextual information. In this work, we\npropose MaxPoolBERT, a lightweight extension to BERT that refines the [CLS]\nrepresentation by aggregating information across layers and tokens.\nSpecifically, we explore three modifications: (i) max-pooling the [CLS] token\nacross multiple layers, (ii) enabling the [CLS] token to attend over the entire\nfinal layer using an additional multi-head attention (MHA) layer, and (iii)\ncombining max-pooling across the full sequence with MHA. Our approach enhances\nBERT's classification accuracy (especially on low-resource tasks) without\nrequiring pre-training or significantly increasing model size. Experiments on\nthe GLUE benchmark show that MaxPoolBERT consistently achieves a better\nperformance on the standard BERT-base model.", "AI": {"tldr": "MaxPoolBERT\u901a\u8fc7\u8de8\u5c42\u548c\u8de8token\u7684\u4fe1\u606f\u805a\u5408\u6539\u8fdbBERT\u7684[CLS]\u8868\u793a\uff0c\u63d0\u5347\u4e86\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edf\u7684BERT\u4e2d\uff0c[CLS] token\u88ab\u7528\u4f5c\u5206\u7c7b\u4efb\u52a1\u7684\u56fa\u5b9a\u957f\u5ea6\u8868\u793a\uff0c\u4f46\u5176\u4ed6token\u548c\u4e2d\u95f4\u5c42\u4e5f\u5305\u542b\u6709\u4ef7\u503c\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002MaxPoolBERT\u65e8\u5728\u901a\u8fc7\u6539\u8fdb[CLS]\u8868\u793a\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u6539\u8fdb\u65b9\u6cd5\uff1a(i) \u8de8\u5c42\u5bf9[CLS] token\u8fdb\u884c\u6700\u5927\u6c60\u5316\uff0c(ii) \u901a\u8fc7\u989d\u5916\u7684\u591a\u5934\u6ce8\u610f\u529b\u5c42\u8ba9[CLS] token\u5173\u6ce8\u6700\u7ec8\u5c42\u7684\u6240\u6709\u4fe1\u606f\uff0c(iii) \u7ed3\u5408\u5e8f\u5217\u7684\u6700\u5927\u6c60\u5316\u548c\u591a\u5934\u6ce8\u610f\u529b\u3002", "result": "\u5728GLUE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMaxPoolBERT\u8868\u73b0\u4f18\u4e8e\u6807\u51c6BERT-base\u6a21\u578b\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u4efb\u52a1\u4e0a\u6548\u679c\u663e\u8457\u3002", "conclusion": "MaxPoolBERT\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u6269\u5c55\u65b9\u6cd5\uff0c\u4e0d\u9700\u8981\u9884\u8bad\u7ec3\u6216\u663e\u8457\u589e\u52a0\u6a21\u578b\u5927\u5c0f\u5373\u53ef\u63d0\u5347BERT\u7684\u5206\u7c7b\u51c6\u786e\u6027\u3002", "keywords": "BERT, [CLS] token, \u6700\u5927\u6c60\u5316, \u591a\u5934\u6ce8\u610f\u529b, \u5206\u7c7b\u4efb\u52a1"}}
{"id": "2505.15657", "pdf": "https://arxiv.org/pdf/2505.15657", "abs": "https://arxiv.org/abs/2505.15657", "authors": ["Cheng Yan", "Felix Mohr", "Tom Viering"], "title": "LCDB 1.1: A Database Illustrating Learning Curves Are More Ill-Behaved Than Previously Thought", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sample-wise learning curves plot performance versus training set size. They\nare useful for studying scaling laws and speeding up hyperparameter tuning and\nmodel selection. Learning curves are often assumed to be well-behaved: monotone\n(i.e. improving with more data) and convex. By constructing the Learning Curves\nDatabase 1.1 (LCDB 1.1), a large-scale database with high-resolution learning\ncurves, we show that learning curves are less often well-behaved than\npreviously thought. Using statistically rigorous methods, we observe\nsignificant ill-behavior in approximately 14% of the learning curves, almost\ntwice as much as in previous estimates. We also identify which learners are to\nblame and show that specific learners are more ill-behaved than others.\nAdditionally, we demonstrate that different feature scalings rarely resolve\nill-behavior. We evaluate the impact of ill-behavior on downstream tasks, such\nas learning curve fitting and model selection, and find it poses significant\nchallenges, underscoring the relevance and potential of LCDB 1.1 as a\nchallenging benchmark for future research.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5b66\u4e60\u66f2\u7ebf\u7684\u4e0d\u89c4\u5219\u884c\u4e3a\u6bd4\u4e4b\u524d\u8ba4\u4e3a\u7684\u66f4\u5e38\u89c1\uff0c\u63ed\u793a\u4e86\u5176\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86LCDB 1.1\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u7684\u57fa\u51c6\u3002", "motivation": "\u63a2\u7d22\u5b66\u4e60\u66f2\u7ebf\u7684\u884c\u4e3a\u7279\u5f81\u53ca\u5176\u5bf9\u6a21\u578b\u9009\u62e9\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u6784\u5efaLCDB 1.1\u6570\u636e\u5e93\uff0c\u4f7f\u7528\u7edf\u8ba1\u65b9\u6cd5\u5206\u6790\u5b66\u4e60\u66f2\u7ebf\u7684\u884c\u4e3a\u3002", "result": "\u7ea614%\u7684\u5b66\u4e60\u66f2\u7ebf\u8868\u73b0\u51fa\u663e\u8457\u7684\u4e0d\u89c4\u5219\u884c\u4e3a\uff0c\u67d0\u4e9b\u5b66\u4e60\u5668\u66f4\u5bb9\u6613\u4ea7\u751f\u8fd9\u7c7b\u884c\u4e3a\u3002", "conclusion": "\u5b66\u4e60\u66f2\u7ebf\u7684\u4e0d\u89c4\u5219\u884c\u4e3a\u5bf9\u6a21\u578b\u9009\u62e9\u548c\u62df\u5408\u63d0\u51fa\u4e86\u65b0\u6311\u6218\uff0cLCDB 1.1\u53ef\u4f5c\u4e3a\u7814\u7a76\u57fa\u51c6\u3002", "keywords": "\u5b66\u4e60\u66f2\u7ebf, \u6570\u636e\u5e93, \u6a21\u578b\u9009\u62e9, \u8d85\u53c2\u6570\u8c03\u4f18, \u7edf\u8ba1\u65b9\u6cd5"}}
{"id": "2505.15234", "pdf": "https://arxiv.org/pdf/2505.15234", "abs": "https://arxiv.org/abs/2505.15234", "authors": ["Saqib Qamar", "Mohd Fazil", "Parvez Ahmad", "Ghulam Muhammad"], "title": "SAMA-UNet: Enhancing Medical Image Segmentation with Self-Adaptive Mamba-Like Attention and Causal-Resonance Learning", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Medical image segmentation plays an important role in various clinical\napplications, but existing models often struggle with the computational\ninefficiencies and challenges posed by complex medical data. State Space\nSequence Models (SSMs) have demonstrated promise in modeling long-range\ndependencies with linear computational complexity, yet their application in\nmedical image segmentation remains hindered by incompatibilities with image\ntokens and autoregressive assumptions. Moreover, it is difficult to achieve a\nbalance in capturing both local fine-grained information and global semantic\ndependencies. To address these challenges, we introduce SAMA-UNet, a novel\narchitecture for medical image segmentation. A key innovation is the\nSelf-Adaptive Mamba-like Aggregated Attention (SAMA) block, which integrates\ncontextual self-attention with dynamic weight modulation to prioritise the most\nrelevant features based on local and global contexts. This approach reduces\ncomputational complexity and improves the representation of complex image\nfeatures across multiple scales. We also suggest the Causal-Resonance\nMulti-Scale Module (CR-MSM), which enhances the flow of information between the\nencoder and decoder by using causal resonance learning. This mechanism allows\nthe model to automatically adjust feature resolution and causal dependencies\nacross scales, leading to better semantic alignment between the low-level and\nhigh-level features in U-shaped architectures. Experiments on MRI, CT, and\nendoscopy images show that SAMA-UNet performs better in segmentation accuracy\nthan current methods using CNN, Transformer, and Mamba. The implementation is\npublicly available at GitHub.", "AI": {"tldr": "SAMA-UNet\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u533b\u5b66\u56fe\u50cf\u5206\u5272\u67b6\u6784\uff0c\u901a\u8fc7SAMA\u5757\u548cCR-MSM\u6a21\u5757\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u548c\u4fe1\u606f\u6355\u83b7\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u548c\u590d\u6742\u6570\u636e\u5efa\u6a21\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u800cSSMs\u7684\u5e94\u7528\u53d7\u5230\u56fe\u50cf\u4ee4\u724c\u548c\u81ea\u56de\u5f52\u5047\u8bbe\u7684\u9650\u5236\u3002", "method": "\u91c7\u7528SAMA\u5757\u6574\u5408\u4e0a\u4e0b\u6587\u81ea\u6ce8\u610f\u529b\u548c\u52a8\u6001\u6743\u91cd\u8c03\u5236\uff0c\u5e76\u5f15\u5165CR-MSM\u6a21\u5757\u589e\u5f3a\u7f16\u7801\u5668\u4e0e\u89e3\u7801\u5668\u95f4\u4fe1\u606f\u6d41\u3002", "result": "\u5728MRI\u3001CT\u548c\u5185\u7aa5\u955c\u56fe\u50cf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAMA-UNet\u5728\u5206\u5272\u7cbe\u5ea6\u4e0a\u4f18\u4e8eCNN\u3001Transformer\u548cMamba\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SAMA-UNet\u901a\u8fc7\u521b\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u5c3a\u5ea6\u6a21\u5757\uff0c\u6709\u6548\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002", "keywords": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u3001SAMA-UNet\u3001SSMs\u3001\u81ea\u6ce8\u610f\u529b\u3001\u591a\u5c3a\u5ea6\u6a21\u5757"}}
{"id": "2505.15700", "pdf": "https://arxiv.org/pdf/2505.15700", "abs": "https://arxiv.org/abs/2505.15700", "authors": ["Alkis Koudounas", "Claudio Savelli", "Flavio Giobergia", "Elena Baralis"], "title": "\"Alexa, can you forget me?\" Machine Unlearning Benchmark in Spoken Language Understanding", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Machine unlearning, the process of efficiently removing specific information\nfrom machine learning models, is a growing area of interest for responsible AI.\nHowever, few studies have explored the effectiveness of unlearning methods on\ncomplex tasks, particularly speech-related ones. This paper introduces\nUnSLU-BENCH, the first benchmark for machine unlearning in spoken language\nunderstanding (SLU), focusing on four datasets spanning four languages. We\naddress the unlearning of data from specific speakers as a way to evaluate the\nquality of potential \"right to be forgotten\" requests. We assess eight\nunlearning techniques and propose a novel metric to simultaneously better\ncapture their efficacy, utility, and efficiency. UnSLU-BENCH sets a foundation\nfor unlearning in SLU and reveals significant differences in the effectiveness\nand computational feasibility of various techniques.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u8bed\u97f3\u8bed\u8a00\u7406\u89e3\uff08SLU\uff09\u7684\u673a\u5668\u5b66\u4e60\u201c\u9057\u5fd8\u201d\u57fa\u51c6UnSLU-BENCH\uff0c\u8bc4\u4f30\u4e86\u516b\u79cd\u9057\u5fd8\u6280\u672f\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8861\u91cf\u6807\u51c6\u3002", "motivation": "\u968f\u7740\u8d23\u4efbAI\u7684\u53d1\u5c55\uff0c\u9ad8\u6548\u53bb\u9664\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u7279\u5b9a\u4fe1\u606f\uff08\u9057\u5fd8\uff09\u6210\u4e3a\u91cd\u8981\u8bfe\u9898\uff0c\u4f46\u590d\u6742\u4efb\u52a1\uff08\u5c24\u5176\u662f\u8bed\u97f3\u76f8\u5173\u4efb\u52a1\uff09\u4e2d\u9057\u5fd8\u65b9\u6cd5\u7684\u6548\u679c\u5c1a\u7f3a\u4e4f\u7814\u7a76\u3002", "method": "\u8bba\u6587\u805a\u7126\u56db\u79cd\u8bed\u8a00\u7684\u56db\u4e2a\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u516b\u79cd\u9057\u5fd8\u6280\u672f\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u65b0\u6307\u6807\u6765\u7efc\u5408\u8861\u91cf\u9057\u5fd8\u6548\u679c\u3001\u6a21\u578b\u6548\u7528\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "UnSLU-BENCH\u4e3aSLU\u9886\u57df\u7684\u9057\u5fd8\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6280\u672f\u5728\u6709\u6548\u6027\u548c\u8ba1\u7b97\u53ef\u884c\u6027\u4e0a\u7684\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86SLU\u9886\u57df\u9057\u5fd8\u6280\u672f\u7684\u7a7a\u767d\uff0c\u4e3a\u65b0\u65b9\u6cd5\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u51c6\u652f\u6301\u3002", "keywords": "\u673a\u5668\u5b66\u4e60\u9057\u5fd8, \u8bed\u97f3\u8bed\u8a00\u7406\u89e3, \u57fa\u51c6\u6d4b\u8bd5, \u6570\u636e\u9690\u79c1, \u8d23\u4efbAI"}}
{"id": "2505.15661", "pdf": "https://arxiv.org/pdf/2505.15661", "abs": "https://arxiv.org/abs/2505.15661", "authors": ["Sina Mohammad-Taheri", "Matthew J. Colbrook", "Simone Brugiapaglia"], "title": "Deep greedy unfolding: Sorting out argsorting in greedy sparse recovery algorithms", "categories": ["cs.LG", "cs.NA", "cs.NE", "math.NA"], "comment": null, "summary": "Gradient-based learning imposes (deep) neural networks to be differentiable\nat all steps. This includes model-based architectures constructed by unrolling\niterations of an iterative algorithm onto layers of a neural network, known as\nalgorithm unrolling. However, greedy sparse recovery algorithms depend on the\nnon-differentiable argsort operator, which hinders their integration into\nneural networks. In this paper, we address this challenge in Orthogonal\nMatching Pursuit (OMP) and Iterative Hard Thresholding (IHT), two popular\nrepresentative algorithms in this class. We propose permutation-based variants\nof these algorithms and approximate permutation matrices using \"soft\"\npermutation matrices derived from softsort, a continuous relaxation of argsort.\nWe demonstrate -- both theoretically and numerically -- that Soft-OMP and\nSoft-IHT, as differentiable counterparts of OMP and IHT and fully compatible\nwith neural network training, effectively approximate these algorithms with a\ncontrollable degree of accuracy. This leads to the development of OMP- and\nIHT-Net, fully trainable network architectures based on Soft-OMP and Soft-IHT,\nrespectively. Finally, by choosing weights as \"structure-aware\" trainable\nparameters, we connect our approach to structured sparse recovery and\ndemonstrate its ability to extract latent sparsity patterns from data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f6f\u6392\u5217\u77e9\u9635\u7684\u65b9\u6cd5\uff0c\u5c06\u4e0d\u53ef\u5fae\u7684\u8d2a\u5fc3\u7a00\u758f\u6062\u590d\u7b97\u6cd5\uff08\u5982OMP\u548cIHT\uff09\u8f6c\u5316\u4e3a\u53ef\u5fae\u7248\u672c\uff0c\u4f7f\u5176\u80fd\u591f\u96c6\u6210\u5230\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u63a7\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u8d2a\u5fc3\u7a00\u758f\u6062\u590d\u7b97\u6cd5\uff08\u5982OMP\u548cIHT\uff09\u7531\u4e8e\u5176\u975e\u53ef\u5fae\u6027\u800c\u96be\u4ee5\u878d\u5165\u795e\u7ecf\u7f51\u7edc\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u8f6f\u6392\u5e8f\uff08softsort\uff09\u751f\u6210\u8f6f\u6392\u5217\u77e9\u9635\uff0c\u8fd1\u4f3c\u975e\u53ef\u5fae\u7684argsort\u7b97\u5b50\uff0c\u63d0\u51faSoft-OMP\u548cSoft-IHT\u4f5c\u4e3aOMP\u548cIHT\u7684\u53ef\u5fae\u66ff\u4ee3\uff0c\u5e76\u6784\u5efaOMP-\u548cIHT-Net\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0cSoft-OMP\u548cSoft-IHT\u80fd\u591f\u6709\u6548\u8fd1\u4f3cOMP\u548cIHT\uff0c\u4e14\u7cbe\u5ea6\u53ef\u63a7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u8d2a\u5fc3\u7a00\u758f\u6062\u590d\u7b97\u6cd5\u878d\u5165\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u80fd\u591f\u63d0\u53d6\u6570\u636e\u7684\u6f5c\u5728\u7a00\u758f\u6a21\u5f0f\u3002", "keywords": "\u7b97\u6cd5\u5c55\u5f00,\u8d2a\u5fc3\u7b97\u6cd5,\u7a00\u758f\u6062\u590d,\u53ef\u5fae\u6027,\u795e\u7ecf\u7f51\u7edc"}}
{"id": "2505.15702", "pdf": "https://arxiv.org/pdf/2505.15702", "abs": "https://arxiv.org/abs/2505.15702", "authors": ["Peng Wang", "Biyu Zhou", "Xuehai Tang", "Jizhong Han", "Songlin Hu"], "title": "LyapLock: Bounded Knowledge Preservation in Sequential Large Language Model Editing", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models often contain factually incorrect or outdated\nknowledge, giving rise to model editing methods for precise knowledge updates.\nHowever, current mainstream locate-then-edit approaches exhibit a progressive\nperformance decline during sequential editing, due to inadequate mechanisms for\nlong-term knowledge preservation. To tackle this, we model the sequential\nediting as a constrained stochastic programming. Given the challenges posed by\nthe cumulative preservation error constraint and the gradually revealed editing\ntasks, \\textbf{LyapLock} is proposed. It integrates queuing theory and Lyapunov\noptimization to decompose the long-term constrained programming into tractable\nstepwise subproblems for efficient solving. This is the first model editing\nframework with rigorous theoretical guarantees, achieving asymptotic optimal\nediting performance while meeting the constraints of long-term knowledge\npreservation. Experimental results show that our framework scales sequential\nediting capacity to over 10,000 edits while stabilizing general capabilities\nand boosting average editing efficacy by 11.89\\% over SOTA baselines.\nFurthermore, it can be leveraged to enhance the performance of baseline\nmethods. Our code is released on https://github.com/caskcsg/LyapLock.", "AI": {"tldr": "LyapLock\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7406\u8bba\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u957f\u671f\u77e5\u8bc6\u66f4\u65b0\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u5904\u7406\u7684\u5b50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fde\u7eed\u7f16\u8f91\u80fd\u529b\uff0c\u5e76\u4fdd\u6301\u4e86\u957f\u671f\u77e5\u8bc6\u4fdd\u7559\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u77e5\u8bc6\u5f80\u5f80\u5b58\u5728\u9519\u8bef\u6216\u8fc7\u65f6\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684\u5b9a\u4f4d-\u7f16\u8f91\u65b9\u6cd5\u5728\u8fde\u7eed\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u6027\u80fd\u9010\u6e10\u4e0b\u964d\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u957f\u671f\u77e5\u8bc6\u4fdd\u7559\u673a\u5236\u3002", "method": "LyapLock\u5c06\u8fde\u7eed\u7f16\u8f91\u5efa\u6a21\u4e3a\u7ea6\u675f\u968f\u673a\u89c4\u5212\u95ee\u9898\uff0c\u7ed3\u5408\u6392\u961f\u8bba\u548c\u674e\u96c5\u666e\u8bfa\u592b\u4f18\u5316\uff0c\u5c06\u957f\u671f\u7ea6\u675f\u95ee\u9898\u5206\u89e3\u4e3a\u9010\u6b65\u53ef\u89e3\u51b3\u7684\u5b50\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLyapLock\u80fd\u5c06\u8fde\u7eed\u7f16\u8f91\u80fd\u529b\u6269\u5c55\u81f310,000\u6b21\u4ee5\u4e0a\uff0c\u540c\u65f6\u7a33\u5b9a\u901a\u7528\u80fd\u529b\uff0c\u5e76\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u5347\u5e73\u5747\u7f16\u8f91\u6548\u679c11.89%\u3002", "conclusion": "LyapLock\u662f\u9996\u4e2a\u5177\u6709\u4e25\u683c\u7406\u8bba\u4fdd\u8bc1\u7684\u6a21\u578b\u7f16\u8f91\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u6e10\u8fdb\u6700\u4f18\u7684\u7f16\u8f91\u6027\u80fd\uff0c\u540c\u65f6\u6ee1\u8db3\u4e86\u957f\u671f\u77e5\u8bc6\u4fdd\u7559\u7684\u7ea6\u675f\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u77e5\u8bc6\u7f16\u8f91, \u957f\u671f\u77e5\u8bc6\u4fdd\u7559, \u7ea6\u675f\u968f\u673a\u89c4\u5212, \u674e\u96c5\u666e\u8bfa\u592b\u4f18\u5316"}}
{"id": "2505.15668", "pdf": "https://arxiv.org/pdf/2505.15668", "abs": "https://arxiv.org/abs/2505.15668", "authors": ["Davide Scassola", "Sebastiano Saccani", "Luca Bortolussi"], "title": "Graph Conditional Flow Matching for Relational Data Generation", "categories": ["cs.LG", "68T07"], "comment": "9 pages of main content, submitted to a conference", "summary": "Data synthesis is gaining momentum as a privacy-enhancing technology. While\nsingle-table tabular data generation has seen considerable progress, current\nmethods for multi-table data often lack the flexibility and expressiveness\nneeded to capture complex relational structures. In particular, they struggle\nwith long-range dependencies and complex foreign-key relationships, such as\ntables with multiple parent tables or multiple types of links between the same\npair of tables. We propose a generative model for relational data that\ngenerates the content of a relational dataset given the graph formed by the\nforeign-key relationships. We do this by learning a deep generative model of\nthe content of the whole relational database by flow matching, where the neural\nnetwork trained to denoise records leverages a graph neural network to obtain\ninformation from connected records. Our method is flexible, as it can support\nrelational datasets with complex structures, and expressive, as the generation\nof each record can be influenced by any other record within the same connected\ncomponent. We evaluate our method on several benchmark datasets and show that\nit achieves state-of-the-art performance in terms of synthetic data fidelity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u8868\u5173\u7cfb\u6570\u636e\u7684\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u6d41\u7a0b\u5339\u914d\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u5408\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u5173\u7cfb\u7ed3\u6784\u4e2d\u7684\u7075\u6d3b\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u8868\u6570\u636e\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u5173\u7cfb\u548c\u957f\u7a0b\u4f9d\u8d56\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\u7684\u6280\u672f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6d41\u7a0b\u5339\u914d\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u5229\u7528\u5916\u952e\u5173\u7cfb\u56fe\u751f\u6210\u6574\u4e2a\u5173\u7cfb\u6570\u636e\u5e93\u7684\u5185\u5bb9\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8868\u73b0\u51fa\u6700\u4f18\u7684\u5408\u6210\u6570\u636e\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u5173\u7cfb\u7ed3\u6784\u7684\u6570\u636e\u5408\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "keywords": "\u6570\u636e\u5408\u6210,\u5173\u7cfb\u6570\u636e,\u56fe\u795e\u7ecf\u7f51\u7edc,\u6d41\u7a0b\u5339\u914d,\u6df1\u5ea6\u751f\u6210\u6a21\u578b"}}
{"id": "2505.15242", "pdf": "https://arxiv.org/pdf/2505.15242", "abs": "https://arxiv.org/abs/2505.15242", "authors": ["Zhiyuan Wei", "Jing Sun", "Zijian Zhang", "Zhe Hou", "Zixiao Zhao"], "title": "Adaptive Plan-Execute Framework for Smart Contract Security Auditing", "categories": ["cs.CR", "cs.AI"], "comment": "30 pages, 5 figures", "summary": "Large Language Models (LLMs) have shown great promise in code analysis and\nauditing; however, they still struggle with hallucinations and limited\ncontext-aware reasoning. We introduce SmartAuditFlow, a novel Plan-Execute\nframework that enhances smart contract security analysis through dynamic audit\nplanning and structured execution. Unlike conventional LLM-based auditing\napproaches that follow fixed workflows and predefined steps, SmartAuditFlow\ndynamically generates and refines audit plans based on the unique\ncharacteristics of each smart contract. It continuously adjusts its auditing\nstrategy in response to intermediate LLM outputs and newly detected\nvulnerabilities, ensuring a more adaptive and precise security assessment. The\nframework then executes these plans step by step, applying a structured\nreasoning process to enhance vulnerability detection accuracy while minimizing\nhallucinations and false positives. To further improve audit precision,\nSmartAuditFlow integrates iterative prompt optimization and external knowledge\nsources, such as static analysis tools and Retrieval-Augmented Generation\n(RAG). This ensures audit decisions are contextually informed and backed by\nreal-world security knowledge, producing comprehensive security reports.\nExtensive evaluations across multiple benchmarks demonstrate that\nSmartAuditFlow outperforms existing methods, achieving 100 percent accuracy on\ncommon and critical vulnerabilities, 41.2 percent accuracy for comprehensive\ncoverage of known smart contract weaknesses in real-world projects, and\nsuccessfully identifying all 13 tested CVEs. These results highlight\nSmartAuditFlow's scalability, cost-effectiveness, and superior adaptability\nover traditional static analysis tools and contemporary LLM-based approaches,\nestablishing it as a robust solution for automated smart contract auditing.", "AI": {"tldr": "SmartAuditFlow\u662f\u4e00\u79cd\u65b0\u9896\u7684Plan-Execute\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u667a\u80fd\u5408\u7ea6\u5b89\u5168\u5206\u6790\u7684\u52a8\u6001\u6027\u548c\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u52a8\u6001\u5ba1\u8ba1\u89c4\u5212\u548c\u7ed3\u6784\u5316\u6267\u884c\u4f18\u5316\u6f0f\u6d1e\u68c0\u6d4b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u5206\u6790\u548c\u5ba1\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4ecd\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u4e0d\u8db3\u7684\u5c40\u9650\u3002", "method": "\u91c7\u7528\u52a8\u6001\u751f\u6210\u548c\u4f18\u5316\u5ba1\u8ba1\u8ba1\u5212\u7684Plan-Execute\u6846\u67b6\uff0c\u7ed3\u5408\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u548c\u5916\u90e8\u77e5\u8bc6\u6e90\uff08\u5982\u9759\u6001\u5206\u6790\u5de5\u5177\u548cRAG\uff09\uff0c\u589e\u5f3a\u6f0f\u6d1e\u68c0\u6d4b\u7684\u7cbe\u786e\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e38\u89c1\u548c\u5173\u952e\u6f0f\u6d1e\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe100%\uff0c41.2%\u8986\u76d6\u7387\u8bc6\u522b\u5df2\u77e5\u5f31\u70b9\uff0c\u5e76\u6210\u529f\u68c0\u6d4b\u6240\u670913\u4e2a\u6d4b\u8bd5\u7684CVE\u3002", "conclusion": "SmartAuditFlow\u5728\u53ef\u6269\u5c55\u6027\u3001\u6210\u672c\u6548\u76ca\u548c\u9002\u5e94\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u9759\u6001\u5206\u6790\u5de5\u5177\u548c\u73b0\u6709LLM\u65b9\u6cd5\uff0c\u662f\u81ea\u52a8\u667a\u80fd\u5408\u7ea6\u5ba1\u8ba1\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u667a\u80fd\u5408\u7ea6\u5b89\u5168, LLM, \u52a8\u6001\u5ba1\u8ba1, \u6f0f\u6d1e\u68c0\u6d4b"}}
{"id": "2505.15710", "pdf": "https://arxiv.org/pdf/2505.15710", "abs": "https://arxiv.org/abs/2505.15710", "authors": ["Tianqi Du", "Zeming Wei", "Quan Chen", "Chenheng Zhang", "Yisen Wang"], "title": "Advancing LLM Safe Alignment with Safety Representation Ranking", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has demonstrated\nmilestone success in a variety of tasks, yet their potential for generating\nharmful content has raised significant safety concerns. Existing safety\nevaluation approaches typically operate directly on textual responses,\noverlooking the rich information embedded in the model's internal\nrepresentations. In this paper, we propose Safety Representation Ranking (SRR),\na listwise ranking framework that selects safe responses using hidden states\nfrom the LLM itself. SRR encodes both instructions and candidate completions\nusing intermediate transformer representations and ranks candidates via a\nlightweight similarity-based scorer. Our approach directly leverages internal\nmodel states and supervision at the list level to capture subtle safety\nsignals. Experiments across multiple benchmarks show that SRR significantly\nimproves robustness to adversarial prompts. Our code will be available upon\npublication.", "AI": {"tldr": "\u63d0\u51fa\u4e86Safety Representation Ranking (SRR)\u6846\u67b6\uff0c\u5229\u7528LLM\u7684\u9690\u85cf\u72b6\u6001\u5bf9\u5b89\u5168\u54cd\u5e94\u8fdb\u884c\u6392\u5e8f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u6027\u63d0\u793a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u6027\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u76f4\u63a5\u64cd\u4f5c\u4e8e\u6587\u672c\u54cd\u5e94\uff0c\u5ffd\u7565\u4e86\u6a21\u578b\u5185\u90e8\u5d4c\u5165\u7684\u4e30\u5bcc\u4fe1\u606f\u3002", "method": "\u63d0\u51faSRR\uff0c\u4e00\u4e2a\u5217\u8868\u6392\u5e8f\u6846\u67b6\uff0c\u5229\u7528LLM\u7684\u9690\u85cf\u72b6\u6001\u5bf9\u5019\u9009\u5b8c\u6210\u8fdb\u884c\u7f16\u7801\u548c\u6392\u5e8f\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSRR\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u5bf9\u6297\u6027\u63d0\u793a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "SRR\u901a\u8fc7\u76f4\u63a5\u5229\u7528\u6a21\u578b\u5185\u90e8\u72b6\u6001\u548c\u5217\u8868\u7ea7\u76d1\u7763\uff0c\u6355\u6349\u7ec6\u5fae\u7684\u5b89\u5168\u4fe1\u53f7\uff0c\u662f\u6709\u6548\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u5b89\u5168\u6027\u8bc4\u4f30,\u9690\u85cf\u72b6\u6001,\u6392\u5e8f\u6846\u67b6"}}
{"id": "2505.15688", "pdf": "https://arxiv.org/pdf/2505.15688", "abs": "https://arxiv.org/abs/2505.15688", "authors": ["Leonardo N. Coregliano", "Maryanthe Malliaris"], "title": "A packing lemma for VCN${}_k$-dimension and learning high-dimensional data", "categories": ["cs.LG", "math.ST", "stat.TH", "Primary: 68Q32. Secondary: 68T05"], "comment": "29 pages, 1 figure", "summary": "Recently, the authors introduced the theory of high-arity PAC learning, which\nis well-suited for learning graphs, hypergraphs and relational structures. In\nthe same initial work, the authors proved a high-arity analogue of the\nFundamental Theorem of Statistical Learning that almost completely\ncharacterizes all notions of high-arity PAC learning in terms of a\ncombinatorial dimension, called the Vapnik--Chervonenkis--Natarajan (VCN${}_k$)\n$k$-dimension, leaving as an open problem only the characterization of\nnon-partite, non-agnostic high-arity PAC learnability.\n  In this work, we complete this characterization by proving that non-partite\nnon-agnostic high-arity PAC learnability implies a high-arity version of the\nHaussler packing property, which in turn implies finiteness of\nVCN${}_k$-dimension. This is done by obtaining direct proofs that classic PAC\nlearnability implies classic Haussler packing property, which in turn implies\nfinite Natarajan dimension and noticing that these direct proofs nicely lift to\nhigh-arity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5b8c\u6210\u4e86\u9ad8\u7ef4\u5ea6PAC\u5b66\u4e60\u7406\u8bba\u4e2d\u975e\u5206\u5272\u3001\u975e\u4e0d\u53ef\u77e5\u5b66\u4e60\u7684\u8868\u5f81\uff0c\u8bc1\u660e\u4e86\u5176\u4e0e\u9ad8\u7ef4\u5ea6Haussler\u6253\u5305\u6027\u8d28\u7684\u8054\u7cfb\uff0c\u5e76\u6700\u7ec8\u63a8\u5bfc\u51faVCNk\u7ef4\u5ea6\u7684\u6709\u9650\u6027\u3002", "motivation": "\u5148\u524d\u7684\u5de5\u4f5c\u5728\u9ad8\u7ef4\u5ea6PAC\u5b66\u4e60\u7406\u8bba\u4e2d\u7559\u4e0b\u4e86\u4e00\u4e2a\u672a\u89e3\u51b3\u95ee\u9898\uff0c\u5373\u975e\u5206\u5272\u3001\u975e\u4e0d\u53ef\u77e5\u9ad8\u7ef4\u5ea6PAC\u5b66\u4e60\u7684\u8868\u5f81\u3002", "method": "\u901a\u8fc7\u76f4\u63a5\u8bc1\u660e\u7ecf\u5178PAC\u5b66\u4e60\u6027\u8d28\u4e0eHaussler\u6253\u5305\u6027\u8d28\u7684\u5173\u7cfb\uff0c\u5e76\u5c06\u5176\u63a8\u5e7f\u5230\u9ad8\u7ef4\u5ea6\u60c5\u51b5\u3002", "result": "\u8bc1\u660e\u4e86\u975e\u5206\u5272\u975e\u4e0d\u53ef\u77e5\u9ad8\u7ef4\u5ea6PAC\u5b66\u4e60\u6027\u8d28\u4e0e\u9ad8\u7ef4\u5ea6Haussler\u6253\u5305\u6027\u8d28\u7684\u8054\u7cfb\uff0c\u8fdb\u800c\u5f97\u51faVCNk\u7ef4\u5ea6\u7684\u6709\u9650\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5b8c\u6210\u4e86\u9ad8\u7ef4\u5ea6PAC\u5b66\u4e60\u7406\u8bba\u7684\u5b8c\u6574\u8868\u5f81\uff0c\u586b\u8865\u4e86\u5148\u524d\u5de5\u4f5c\u7684\u7a7a\u767d\u3002", "keywords": "\u9ad8\u7ef4\u5ea6PAC\u5b66\u4e60, \u975e\u5206\u5272\u975e\u4e0d\u53ef\u77e5\u5b66\u4e60, VCNk\u7ef4\u5ea6, Haussler\u6253\u5305\u6027\u8d28"}}
{"id": "2505.15712", "pdf": "https://arxiv.org/pdf/2505.15712", "abs": "https://arxiv.org/abs/2505.15712", "authors": ["Yuan Yuan", "Muyu He", "Muhammad Adil Shahid", "Jiani Huang", "Ziyang Li", "Li Zhang"], "title": "TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces TurnaboutLLM, a novel framework and dataset for\nevaluating the deductive reasoning abilities of Large Language Models (LLMs) by\nleveraging the interactive gameplay of detective games Ace Attorney and\nDanganronpa. The framework tasks LLMs with identifying contradictions between\ntestimonies and evidences within long narrative contexts, a challenging task\ndue to the large answer space and diverse reasoning types presented by its\nquestions. We evaluate twelve state-of-the-art LLMs on the dataset, hinting at\nlimitations of popular strategies for enhancing deductive reasoning such as\nextensive thinking and Chain-of-Thought prompting. The results also suggest\nvarying effects of context size, the number of reasoning step and answer space\nsize on model performance. Overall, TurnaboutLLM presents a substantial\nchallenge for LLMs' deductive reasoning abilities in complex, narrative-rich\nenvironments.", "AI": {"tldr": "\u4ecb\u7ecdTurnaboutLLM\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4fa6\u63a2\u6e38\u620f\u4e92\u52a8\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6f14\u7ece\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u53d9\u4e8b\u73af\u5883\u4e2d\u6f14\u7ece\u63a8\u7406\u7684\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528\u4fa6\u63a2\u6e38\u620f\u300a\u9006\u8f6c\u88c1\u5224\u300b\u548c\u300a\u5f39\u4e38\u8bba\u7834\u300b\u7684\u4e92\u52a8\u5267\u60c5\uff0c\u8bbe\u8ba1\u6570\u636e\u96c6\u548c\u6846\u67b6\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u957f\u53d9\u4e8b\u4e2d\u8bc6\u522b\u77db\u76fe\u7684\u80fd\u529b\u3002", "result": "\u6d4b\u8bd512\u4e2a\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u5e38\u7528\u63a8\u7406\u589e\u5f3a\u7b56\u7565\uff08\u5982\u591a\u6b65\u601d\u8003\u3001\u601d\u7ef4\u94fe\u63d0\u793a\uff09\u7684\u6548\u679c\u6709\u9650\uff0c\u4e14\u4e0a\u4e0b\u6587\u5927\u5c0f\u3001\u63a8\u7406\u6b65\u9aa4\u6570\u548c\u7b54\u6848\u7a7a\u95f4\u5927\u5c0f\u5bf9\u6027\u80fd\u6709\u4e0d\u540c\u5f71\u54cd\u3002", "conclusion": "TurnaboutLLM\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u53d9\u4e8b\u73af\u5883\u4e2d\u7684\u6f14\u7ece\u63a8\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u6311\u6218\u3002", "keywords": "TurnaboutLLM, \u5927\u8bed\u8a00\u6a21\u578b, \u6f14\u7ece\u63a8\u7406, \u4fa6\u63a2\u6e38\u620f, \u8bc4\u4f30\u6846\u67b6"}}
{"id": "2505.15694", "pdf": "https://arxiv.org/pdf/2505.15694", "abs": "https://arxiv.org/abs/2505.15694", "authors": ["Xingyu Zhou", "Yulian Wu", "Francesco Orabona"], "title": "A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this paper, we theoretically investigate the effects of noisy labels in\noffline alignment, with a focus on the interplay between privacy and robustness\nagainst adversarial corruption. Specifically, under linear modeling\nassumptions, we present a unified analysis covering both reinforcement learning\nfrom human feedback (RLHF) and direct preference optimization (DPO) under\ndifferent privacy-corruption scenarios, such as Local differential\nprivacy-then-Corruption (LTC), where human preference labels are privatized\nbefore being corrupted by an adversary, and Corruption-then-Local differential\nprivacy (CTL), where labels are corrupted before privacy protection. Our\nanalysis leverages a reduction framework that reduces the offline alignment\nproblem under linear modeling assumptions to parameter estimation in logistic\nregression. This framework allows us to establish an interesting separation\nresult between LTC and CTL, demonstrating that LTC presents a greater challenge\nthan CTL in offline alignment, even under linear models. As important\nby-products, our findings also advance the state-of-the-art theoretical results\nin offline alignment under privacy-only or corruption-only scenarios.", "AI": {"tldr": "\u7814\u7a76\u4e86\u566a\u58f0\u6807\u7b7e\u5728\u79bb\u7ebf\u5bf9\u9f50\u4e2d\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u9690\u79c1\u4e0e\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u9690\u79c1-\u8150\u8d25\u573a\u666f\u4e0b\u79bb\u7ebf\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u566a\u58f0\u6807\u7b7e\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u6a21\u578b\u5047\u8bbe\uff0c\u7ed3\u5408\u903b\u8f91\u56de\u5f52\u7684\u53c2\u6570\u4f30\u8ba1\u6846\u67b6\uff0c\u5206\u6790 RLHF \u548c DPO \u65b9\u6cd5\u3002", "result": "\u53d1\u73b0 LTC \u573a\u666f\u6bd4 CTL \u66f4\u5177\u6311\u6218\u6027\uff0c\u5e76\u63a8\u52a8\u4e86\u9690\u79c1\u6216\u8150\u8d25\u5355\u4e00\u573a\u666f\u7684\u7406\u8bba\u8fdb\u5c55\u3002", "conclusion": "\u79bb\u7ebf\u5bf9\u9f50\u4e2d\u566a\u58f0\u6807\u7b7e\u7684\u5904\u7406\u5bf9\u9690\u79c1\u548c\u9c81\u68d2\u6027\u6709\u91cd\u8981\u5f71\u54cd\uff0cLTC \u6bd4 CTL \u66f4\u96be\u3002", "keywords": "\u79bb\u7ebf\u5bf9\u9f50, \u566a\u58f0\u6807\u7b7e, \u9690\u79c1, \u5bf9\u6297\u6027\u8150\u8d25, RLHF, DPO"}}
{"id": "2505.15715", "pdf": "https://arxiv.org/pdf/2505.15715", "abs": "https://arxiv.org/abs/2505.15715", "authors": ["He Hu", "Yucheng Zhou", "Juzheng Si", "Qianning Wang", "Hengheng Zhang", "Fuji Ren", "Fei Ma", "Laizhong Cui"], "title": "Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with Large Language Models for Mental Health Counseling", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) hold significant potential for mental health\nsupport, capable of generating empathetic responses and simulating therapeutic\nconversations. However, existing LLM-based approaches often lack the clinical\ngrounding necessary for real-world psychological counseling, particularly in\nexplicit diagnostic reasoning aligned with standards like the DSM/ICD and\nincorporating diverse therapeutic modalities beyond basic empathy or single\nstrategies. To address these critical limitations, we propose PsyLLM, the first\nlarge language model designed to systematically integrate both diagnostic and\ntherapeutic reasoning for mental health counseling. To develop the PsyLLM, we\npropose a novel automated data synthesis pipeline. This pipeline processes\nreal-world mental health posts, generates multi-turn dialogue structures, and\nleverages LLMs guided by international diagnostic standards (e.g., DSM/ICD) and\nmultiple therapeutic frameworks (e.g., CBT, ACT, psychodynamic) to simulate\ndetailed clinical reasoning processes. Rigorous multi-dimensional filtering\nensures the generation of high-quality, clinically aligned dialogue data. In\naddition, we introduce a new benchmark and evaluation protocol, assessing\ncounseling quality across four key dimensions: comprehensiveness,\nprofessionalism, authenticity, and safety. Our experiments demonstrate that\nPsyLLM significantly outperforms state-of-the-art baseline models on this\nbenchmark.", "AI": {"tldr": "PsyLLM\u662f\u4e00\u4e2a\u4e13\u4e3a\u5fc3\u7406\u5065\u5eb7\u54a8\u8be2\u8bbe\u8ba1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u8bca\u65ad\u548c\u6cbb\u7597\u63a8\u7406\uff0c\u7ed3\u5408DSM/ICD\u6807\u51c6\u548c\u591a\u79cd\u6cbb\u7597\u6846\u67b6\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u4e34\u5e8a\u5bf9\u8bdd\u6570\u636e\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u65b9\u6cd5\u7f3a\u4e4f\u4e34\u5e8a\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u7b26\u5408DSM/ICD\u6807\u51c6\u7684\u8bca\u65ad\u63a8\u7406\u548c\u591a\u6837\u5316\u6cbb\u7597\u7b56\u7565\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u81ea\u52a8\u5316\u6570\u636e\u5408\u6210\u6d41\u7a0b\uff0c\u7ed3\u5408\u771f\u5b9e\u5fc3\u7406\u5065\u5eb7\u5e16\u5b50\u751f\u6210\u591a\u8f6e\u5bf9\u8bdd\uff0c\u5e76\u5728DSM/ICD\u6807\u51c6\u548c\u591a\u79cd\u6cbb\u7597\u6846\u67b6\u6307\u5bfc\u4e0b\u751f\u6210\u4e34\u5e8a\u63a8\u7406\u6570\u636e\u3002", "result": "PsyLLM\u5728\u7efc\u5408\u6027\u3001\u4e13\u4e1a\u6027\u3001\u771f\u5b9e\u6027\u548c\u5b89\u5168\u6027\u56db\u4e2a\u7ef4\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "PsyLLM\u901a\u8fc7\u7cfb\u7edf\u6574\u5408\u8bca\u65ad\u548c\u6cbb\u7597\u63a8\u7406\uff0c\u4e3a\u5fc3\u7406\u5065\u5eb7\u54a8\u8be2\u63d0\u4f9b\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "LLM, \u5fc3\u7406\u5065\u5eb7, DSM/ICD, CBT, ACT, \u5fc3\u7406\u52a8\u529b\u5b66"}}
{"id": "2505.15721", "pdf": "https://arxiv.org/pdf/2505.15721", "abs": "https://arxiv.org/abs/2505.15721", "authors": ["Coby Penso", "Bar Mahpud", "Jacob Goldberger", "Or Sheffet"], "title": "Privacy-Preserving Conformal Prediction Under Local Differential Privacy", "categories": ["cs.LG", "stat.ML"], "comment": "Preprint. Under review", "summary": "Conformal prediction (CP) provides sets of candidate classes with a\nguaranteed probability of containing the true class. However, it typically\nrelies on a calibration set with clean labels. We address privacy-sensitive\nscenarios where the aggregator is untrusted and can only access a perturbed\nversion of the true labels. We propose two complementary approaches under local\ndifferential privacy (LDP). In the first approach, users do not access the\nmodel but instead provide their input features and a perturbed label using a\nk-ary randomized response. In the second approach, which enforces stricter\nprivacy constraints, users add noise to their conformity score by binary search\nresponse. This method requires access to the classification model but preserves\nboth data and label privacy. Both approaches compute the conformal threshold\ndirectly from noisy data without accessing the true labels. We prove\nfinite-sample coverage guarantees and demonstrate robust coverage even under\nsevere randomization. This approach unifies strong local privacy with\npredictive uncertainty control, making it well-suited for sensitive\napplications such as medical imaging or large language model queries,\nregardless of whether users can (or are willing to) compute their own scores.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u5728\u5c40\u90e8\u5dee\u5206\u9690\u79c1\uff08LDP\uff09\u4e0b\u4fdd\u62a4\u9690\u79c1\u7684CP\u65b9\u6cd5\uff0c\u786e\u4fdd\u5728\u6807\u7b7e\u6270\u52a8\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u63d0\u4f9b\u53ef\u9760\u7684\u7f6e\u4fe1\u96c6\u3002", "motivation": "\u89e3\u51b3\u9690\u79c1\u654f\u611f\u573a\u666f\u4e2d\uff0c\u4e0d\u53ef\u4fe1\u805a\u5408\u5668\u53ea\u80fd\u8bbf\u95ee\u6270\u52a8\u6807\u7b7e\u7684\u95ee\u9898\uff0c\u786e\u4fddCP\u7684\u51c6\u786e\u6027\u3002", "method": "\u65b9\u6cd5\u4e00\uff1a\u7528\u6237\u4e0d\u8bbf\u95ee\u6a21\u578b\uff0c\u800c\u662f\u63d0\u4f9b\u8f93\u5165\u7279\u5f81\u548c\u6270\u52a8\u6807\u7b7e\uff08k-ary\u968f\u673a\u54cd\u5e94\uff09\uff1b\u65b9\u6cd5\u4e8c\uff1a\u7528\u6237\u901a\u8fc7\u4e8c\u5206\u67e5\u627e\u54cd\u5e94\u5411\u8bc4\u5206\u6dfb\u52a0\u566a\u58f0\uff0c\u4fdd\u62a4\u6570\u636e\u548c\u6807\u7b7e\u9690\u79c1\u3002", "result": "\u5728\u5f3a\u968f\u673a\u5316\u6761\u4ef6\u4e0b\u4ecd\u80fd\u4fdd\u6301\u7a33\u5065\u7684\u8986\u76d6\u6027\uff0c\u65e0\u9700\u8bbf\u95ee\u771f\u5b9e\u6807\u7b7e\u5373\u53ef\u8ba1\u7b97\u9608\u503c\u3002", "conclusion": "\u7edf\u4e00\u5f3a\u5c40\u90e8\u9690\u79c1\u4e0e\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u533b\u7597\u5f71\u50cf\u6216\u5927\u578b\u8bed\u8a00\u6a21\u578b\u67e5\u8be2\u7b49\u654f\u611f\u573a\u666f\u3002", "keywords": "Conformal prediction, Local differential privacy, Randomized response, Privacy-preserving, Coverage guarantees"}}
{"id": "2505.15256", "pdf": "https://arxiv.org/pdf/2505.15256", "abs": "https://arxiv.org/abs/2505.15256", "authors": ["Tatyana Shmykova", "Leila Khaertdinova", "Ilya Pershin"], "title": "Zero-Shot Gaze-based Volumetric Medical Image Segmentation", "categories": ["cs.CV", "cs.AI", "I.2.1"], "comment": "Accepted to MMFM-BIOMED Workshop @ CVPR 2025", "summary": "Accurate segmentation of anatomical structures in volumetric medical images\nis crucial for clinical applications, including disease monitoring and cancer\ntreatment planning. Contemporary interactive segmentation models, such as\nSegment Anything Model 2 (SAM-2) and its medical variant (MedSAM-2), rely on\nmanually provided prompts like bounding boxes and mouse clicks. In this study,\nwe introduce eye gaze as a novel informational modality for interactive\nsegmentation, marking the application of eye-tracking for 3D medical image\nsegmentation. We evaluate the performance of using gaze-based prompts with\nSAM-2 and MedSAM-2 using both synthetic and real gaze data. Compared to\nbounding boxes, gaze-based prompts offer a time-efficient interaction approach\nwith slightly lower segmentation quality. Our findings highlight the potential\nof using gaze as a complementary input modality for interactive 3D medical\nimage segmentation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u773c\u7403\u8ffd\u8e2a\u6280\u672f\uff08\u773c\u52a8\uff09\u4f5c\u4e3a\u4ea4\u4e92\u5f0f3D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u65b0\u8f93\u5165\u65b9\u5f0f\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684\u8fb9\u754c\u6846\u63d0\u793a\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u63d0\u793a\uff08\u5982\u8fb9\u754c\u6846\u6216\u70b9\u51fb\uff09\uff0c\u6548\u7387\u8f83\u4f4e\u3002\u7814\u7a76\u63a2\u7d22\u4e86\u773c\u52a8\u63d0\u793a\u4f5c\u4e3a\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528SAM-2\u548cMedSAM-2\u6a21\u578b\uff0c\u7ed3\u5408\u5408\u6210\u548c\u771f\u5b9e\u773c\u52a8\u6570\u636e\uff0c\u8bc4\u4f30\u773c\u52a8\u63d0\u793a\u7684\u5206\u5272\u6027\u80fd\u3002", "result": "\u773c\u52a8\u63d0\u793a\u5728\u65f6\u95f4\u6548\u7387\u4e0a\u4f18\u4e8e\u8fb9\u754c\u6846\uff0c\u4f46\u5206\u5272\u8d28\u91cf\u7565\u4f4e\u3002", "conclusion": "\u773c\u52a8\u53ef\u4ee5\u4f5c\u4e3a3D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u8865\u5145\u8f93\u5165\u65b9\u5f0f\uff0c\u5177\u6709\u6f5c\u529b\u3002", "keywords": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u3001\u773c\u52a8\u8ffd\u8e2a\u3001\u4ea4\u4e92\u5f0f\u5206\u5272\u3001SAM-2\u3001MedSAM-2"}}
{"id": "2505.15722", "pdf": "https://arxiv.org/pdf/2505.15722", "abs": "https://arxiv.org/abs/2505.15722", "authors": ["Xiaoyu Luo", "Yiyi Chen", "Johannes Bjerva", "Qiongxiu Li"], "title": "Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 14 tables, 10 figures", "summary": "We present the first comprehensive study of Memorization in Multilingual\nLarge Language Models (MLLMs), analyzing 95 languages using models across\ndiverse model scales, architectures, and memorization definitions. As MLLMs are\nincreasingly deployed, understanding their memorization behavior has become\ncritical. Yet prior work has focused primarily on monolingual models, leaving\nmultilingual memorization underexplored, despite the inherently long-tailed\nnature of training corpora. We find that the prevailing assumption, that\nmemorization is highly correlated with training data availability, fails to\nfully explain memorization patterns in MLLMs. We hypothesize that treating\nlanguages in isolation - ignoring their similarities - obscures the true\npatterns of memorization. To address this, we propose a novel graph-based\ncorrelation metric that incorporates language similarity to analyze\ncross-lingual memorization. Our analysis reveals that among similar languages,\nthose with fewer training tokens tend to exhibit higher memorization, a trend\nthat only emerges when cross-lingual relationships are explicitly modeled.\nThese findings underscore the importance of a language-aware perspective in\nevaluating and mitigating memorization vulnerabilities in MLLMs. This also\nconstitutes empirical evidence that language similarity both explains\nMemorization in MLLMs and underpins Cross-lingual Transferability, with broad\nimplications for multilingual NLP.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u9996\u6b21\u5168\u9762\u7814\u7a76\u4e86\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e2d\u7684\u8bb0\u5fc6\u73b0\u8c61\uff0c\u5206\u6790\u4e8695\u79cd\u8bed\u8a00\u7684\u6a21\u578b\uff0c\u53d1\u73b0\u5355\u7eaf\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u91cf\u7684\u5047\u8bbe\u65e0\u6cd5\u5b8c\u5168\u89e3\u91ca\u8bb0\u5fc6\u6a21\u5f0f\uff0c\u63d0\u51fa\u57fa\u4e8e\u8bed\u8a00\u76f8\u4f3c\u6027\u7684\u56fe\u6307\u6807\u4ee5\u63ed\u793a\u8de8\u8bed\u8a00\u8bb0\u5fc6\u89c4\u5f8b\u3002", "motivation": "\u968f\u7740MLLMs\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7406\u89e3\u5176\u8bb0\u5fc6\u884c\u4e3a\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6b64\u524d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5355\u8bed\u6a21\u578b\uff0c\u591a\u8bed\u8a00\u8bb0\u5fc6\u95ee\u9898\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u89c4\u6a21\u3001\u67b6\u6784\u7684\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u8bed\u8a00\u76f8\u4f3c\u6027\u76f8\u5173\u6307\u6807\uff0c\u7814\u7a76\u8de8\u8bed\u8a00\u8bb0\u5fc6\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u76f8\u4f3c\u8bed\u8a00\u4e2d\u8bad\u7ec3\u6807\u8bb0\u8f83\u5c11\u7684\u8bed\u8a00\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8bb0\u5fc6\u7387\uff0c\u8fd9\u4e00\u8d8b\u52bf\u4ec5\u5728\u663e\u5f0f\u5efa\u6a21\u8de8\u8bed\u8a00\u5173\u7cfb\u65f6\u624d\u663e\u73b0\u3002", "conclusion": "\u8bed\u8a00\u76f8\u4f3c\u6027\u4e0d\u4ec5\u89e3\u91ca\u4e86MLLMs\u7684\u8bb0\u5fc6\u884c\u4e3a\uff0c\u4e5f\u652f\u6301\u8de8\u8bed\u8a00\u53ef\u8fc1\u79fb\u6027\uff0c\u4e3a\u591a\u8bed\u8a00NLP\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002", "keywords": "\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b, \u8bb0\u5fc6\u73b0\u8c61, \u8bed\u8a00\u76f8\u4f3c\u6027, \u8de8\u8bed\u8a00\u8fc1\u79fb"}}
{"id": "2505.15746", "pdf": "https://arxiv.org/pdf/2505.15746", "abs": "https://arxiv.org/abs/2505.15746", "authors": ["Jingzhe Liu", "Zhigang Hua", "Yan Xie", "Bingheng Li", "Harry Shomer", "Yu Song", "Kaveh Hassani", "Jiliang Tang"], "title": "Higher-order Structure Boosts Link Prediction on Temporal Graphs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Temporal Graph Neural Networks (TGNNs) have gained growing attention for\nmodeling and predicting structures in temporal graphs. However, existing TGNNs\nprimarily focus on pairwise interactions while overlooking higher-order\nstructures that are integral to link formation and evolution in real-world\ntemporal graphs. Meanwhile, these models often suffer from efficiency\nbottlenecks, further limiting their expressive power. To tackle these\nchallenges, we propose a Higher-order structure Temporal Graph Neural Network,\nwhich incorporates hypergraph representations into temporal graph learning. In\nparticular, we develop an algorithm to identify the underlying higher-order\nstructures, enhancing the model's ability to capture the group interactions.\nFurthermore, by aggregating multiple edge features into hyperedge\nrepresentations, HTGN effectively reduces memory cost during training. We\ntheoretically demonstrate the enhanced expressiveness of our approach and\nvalidate its effectiveness and efficiency through extensive experiments on\nvarious real-world temporal graphs. Experimental results show that HTGN\nachieves superior performance on dynamic link prediction while reducing memory\ncosts by up to 50\\% compared to existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u8868\u793a\u7684\u9ad8\u9636\u7ed3\u6784\u65f6\u5e8f\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HTGN\uff09\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u9ad8\u9636\u7ed3\u6784\u548c\u6548\u7387\u74f6\u9888\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u548c\u5185\u5b58\u6548\u7387\u5747\u6709\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65f6\u5e8f\u56fe\u795e\u7ecf\u7f51\u7edc\uff08TGNNs\uff09\u4e3b\u8981\u5173\u6ce8\u4e24\u4e24\u4ea4\u4e92\uff0c\u5ffd\u7565\u4e86\u5bf9\u94fe\u8def\u5f62\u6210\u548c\u6f14\u5316\u81f3\u5173\u91cd\u8981\u7684\u9ad8\u9636\u7ed3\u6784\uff0c\u4e14\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5f15\u5165\u8d85\u56fe\u8868\u793a\uff0c\u5f00\u53d1\u7b97\u6cd5\u8bc6\u522b\u9ad8\u9636\u7ed3\u6784\uff0c\u901a\u8fc7\u805a\u5408\u8fb9\u7279\u5f81\u4e3a\u8d85\u8fb9\u8868\u793a\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u3002", "result": "HTGN\u5728\u52a8\u6001\u94fe\u8def\u9884\u6d4b\u4e2d\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5185\u5b58\u6210\u672c\u964d\u4f4e50%\u3002", "conclusion": "HTGN\u901a\u8fc7\u6355\u6349\u9ad8\u9636\u7ed3\u6784\u548c\u4f18\u5316\u5185\u5b58\u6548\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u5e8f\u56fe\u5efa\u6a21\u80fd\u529b\u3002", "keywords": "\u65f6\u5e8f\u56fe\u795e\u7ecf\u7f51\u7edc, \u9ad8\u9636\u7ed3\u6784, \u8d85\u56fe, \u52a8\u6001\u94fe\u8def\u9884\u6d4b, \u5185\u5b58\u6548\u7387"}}
{"id": "2505.15265", "pdf": "https://arxiv.org/pdf/2505.15265", "abs": "https://arxiv.org/abs/2505.15265", "authors": ["Zihao Pan", "Yu Tong", "Weibin Wu", "Jingyi Wang", "Lifeng Chen", "Zhe Zhao", "Jiajia Wei", "Yitong Qiao", "Zibin Zheng"], "title": "Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic Concepts for LVLMs", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Adversarial attacks aim to generate malicious inputs that mislead deep\nmodels, but beyond causing model failure, they cannot provide certain\ninterpretable information such as ``\\textit{What content in inputs make models\nmore likely to fail?}'' However, this information is crucial for researchers to\nspecifically improve model robustness. Recent research suggests that models may\nbe particularly sensitive to certain semantics in visual inputs (such as\n``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this\npaper we conducted the first exploration on large vision-language models\n(LVLMs) and found that LVLMs indeed are susceptible to hallucinations and\nvarious errors when facing specific semantic concepts in images. To efficiently\nsearch for these sensitive concepts, we integrated large language models (LLMs)\nand text-to-image (T2I) models to propose a novel semantic evolution framework.\nRandomly initialized semantic concepts undergo LLM-based crossover and mutation\noperations to form image descriptions, which are then converted by T2I models\ninto visual inputs for LVLMs. The task-specific performance of LVLMs on each\ninput is quantified as fitness scores for the involved semantics and serves as\nreward signals to further guide LLMs in exploring concepts that induce LVLMs.\nExtensive experiments on seven mainstream LVLMs and two multimodal tasks\ndemonstrate the effectiveness of our method. Additionally, we provide\ninteresting findings about the sensitive semantics of LVLMs, aiming to inspire\nfurther in-depth research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u4e49\u6f14\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u6587\u672c\u751f\u6210\u56fe\u50cf\u6a21\u578b\uff08T2I\uff09\uff0c\u641c\u7d22\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5bf9\u7279\u5b9a\u8bed\u4e49\u6982\u5ff5\u7684\u654f\u611f\u533a\u57df\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u63a2\u7a76LVLM\u5728\u7279\u5b9a\u8bed\u4e49\u6982\u5ff5\u4e0b\u7684\u5e7b\u89c9\u548c\u9519\u8bef\u884c\u4e3a\uff0c\u4ee5\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u9488\u5bf9\u6027\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "method": "\u7ed3\u5408LLM\u548cT2I\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u4ea4\u53c9\u548c\u7a81\u53d8\u751f\u6210\u56fe\u50cf\u63cf\u8ff0\uff0c\u8f6c\u6362\u4e3a\u89c6\u89c9\u8f93\u5165\u5e76\u91cf\u5316LVLM\u7684\u6027\u80fd\u4f5c\u4e3a\u53cd\u9988\u4fe1\u53f7\u3002", "result": "\u5728\u4e03\u79cd\u4e3b\u6d41LVLM\u548c\u4e24\u9879\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u63ed\u793a\u4e86LVLM\u7684\u654f\u611f\u8bed\u4e49\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5b9e\u7528\uff0c\u8fd8\u4e3a\u7814\u7a76LVLM\u7684\u654f\u611f\u6027\u63d0\u4f9b\u4e86\u6709\u8da3\u89c1\u89e3\u3002", "keywords": "\u5bf9\u6297\u653b\u51fb,\u8bed\u4e49\u6f14\u5316,\u89c6\u89c9\u8bed\u8a00\u6a21\u578b,\u9c81\u68d2\u6027"}}
{"id": "2505.15727", "pdf": "https://arxiv.org/pdf/2505.15727", "abs": "https://arxiv.org/abs/2505.15727", "authors": ["Heyang Liu", "Yuhao Wang", "Ziyang Cheng", "Ronghua Wu", "Qunshan Gu", "Yanfeng Wang", "Yu Wang"], "title": "VocalBench: Benchmarking the Vocal Conversational Abilities for Speech Interaction Models", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has accelerated the\ndevelopment of multi-modal models capable of vocal communication. Unlike\ntext-based interactions, speech conveys rich and diverse information, including\nsemantic content, acoustic variations, paralanguage cues, and environmental\ncontext. However, existing evaluations of speech interaction models\npredominantly focus on the quality of their textual responses, often\noverlooking critical aspects of vocal performance and lacking benchmarks with\nvocal-specific test instances. To address this gap, we propose VocalBench, a\ncomprehensive benchmark designed to evaluate speech interaction models'\ncapabilities in vocal communication. VocalBench comprises 9,400 carefully\ncurated instances across four key dimensions: semantic quality, acoustic\nperformance, conversational abilities, and robustness. It covers 16 fundamental\nskills essential for effective vocal interaction. Experimental results reveal\nsignificant variability in current model capabilities, each exhibiting distinct\nstrengths and weaknesses, and provide valuable insights to guide future\nresearch in speech-based interaction systems. Code and evaluation instances are\navailable at https://github.com/SJTU-OmniAgent/VocalBench.", "AI": {"tldr": "\u63d0\u51fa\u4e86VocalBench\uff0c\u4e00\u4e2a\u5168\u9762\u8bc4\u4f30\u8bed\u97f3\u4ea4\u4e92\u6a21\u578b\u5728\u591a\u7ef4\u8bed\u97f3\u901a\u4fe1\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u4ea4\u4e92\u6a21\u578b\u8bc4\u4f30\u591a\u5173\u6ce8\u6587\u672c\u54cd\u5e94\u8d28\u91cf\uff0c\u5ffd\u89c6\u8bed\u97f3\u6027\u80fd\u5173\u952e\u65b9\u9762\uff0c\u7f3a\u4e4f\u4e13\u95e8\u6d4b\u8bd5\u5b9e\u4f8b\u3002", "method": "\u8bbe\u8ba1VocalBench\uff0c\u6db5\u76d69400\u4e2a\u5b9e\u4f8b\uff0c\u8986\u76d6\u8bed\u4e49\u8d28\u91cf\u3001\u58f0\u5b66\u8868\u73b0\u3001\u4f1a\u8bdd\u80fd\u529b\u548c\u9c81\u68d2\u6027\u56db\u7ef4\u5ea6\u53ca16\u9879\u57fa\u7840\u6280\u80fd\u3002", "result": "\u5f53\u524d\u6a21\u578b\u80fd\u529b\u5dee\u5f02\u663e\u8457\uff0c\u5404\u6709\u4f18\u52a3\uff0c\u4e3a\u672a\u6765\u8bed\u97f3\u4ea4\u4e92\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u6307\u5bfc\u3002", "conclusion": "VocalBench\u586b\u8865\u4e86\u8bed\u97f3\u4ea4\u4e92\u8bc4\u4f30\u7a7a\u767d\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002", "keywords": "\u8bed\u97f3\u4ea4\u4e92\u3001\u8bc4\u4f30\u57fa\u51c6\u3001\u591a\u6a21\u6001\u6a21\u578b\u3001\u8bed\u97f3\u6027\u80fd"}}
{"id": "2505.15747", "pdf": "https://arxiv.org/pdf/2505.15747", "abs": "https://arxiv.org/abs/2505.15747", "authors": ["Kanan Kiguchi", "Yunhao Tu", "Katsuhiro Ajito", "Fady Alnajjar", "Kazuyuki Murase"], "title": "Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.1; H.3.1; J.3"], "comment": "38 pages, 8 figures, 4 tables", "summary": "We propose a novel framework for integrating fragmented multi-modal data in\nAlzheimer's disease (AD) research using large language models (LLMs) and\nknowledge graphs. While traditional multimodal analysis requires matched\npatient IDs across datasets, our approach demonstrates population-level\nintegration of MRI, gene expression, biomarkers, EEG, and clinical indicators\nfrom independent cohorts. Statistical analysis identified significant features\nin each modality, which were connected as nodes in a knowledge graph. LLMs then\nanalyzed the graph to extract potential correlations and generate hypotheses in\nnatural language. This approach revealed several novel relationships, including\na potential pathway linking metabolic risk factors to tau protein abnormalities\nvia neuroinflammation (r>0.6, p<0.001), and unexpected correlations between\nfrontal EEG channels and specific gene expression profiles (r=0.42-0.58,\np<0.01). Cross-validation with independent datasets confirmed the robustness of\nmajor findings, with consistent effect sizes across cohorts (variance <15%).\nThe reproducibility of these findings was further supported by expert review\n(Cohen's k=0.82) and computational validation. Our framework enables cross\nmodal integration at a conceptual level without requiring patient ID matching,\noffering new possibilities for understanding AD pathology through fragmented\ndata reuse and generating testable hypotheses for future research.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u77e5\u8bc6\u56fe\u8c31\u6574\u5408\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7814\u7a76\u4e2d\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u65e0\u9700\u60a3\u8005ID\u5339\u914d\uff0c\u63ed\u793a\u65b0\u5173\u8054\u5e76\u901a\u8fc7\u9a8c\u8bc1\u786e\u8ba4\u7a33\u5065\u6027\u3002", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u5206\u6790\u9700\u8981\u8de8\u6570\u636e\u96c6\u7684\u60a3\u8005ID\u5339\u914d\uff0c\u9650\u5236\u4e86\u6570\u636e\u6574\u5408\u7684\u6f5c\u529b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u65b0\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u5bf9\u72ec\u7acb\u961f\u5217\u6570\u636e\u7684\u65e0\u5339\u914d\u6574\u5408\u3002", "method": "\u4f7f\u7528\u7edf\u8ba1\u5206\u6790\u65b9\u6cd5\u8bc6\u522b\u5404\u6a21\u6001\u7684\u663e\u8457\u7279\u5f81\uff0c\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790\u56fe\u8c31\u4ee5\u63d0\u53d6\u6f5c\u5728\u5173\u8054\u5e76\u751f\u6210\u81ea\u7136\u8bed\u8a00\u5047\u8bbe\u3002", "result": "\u63ed\u793a\u4e86\u591a\u4e2a\u65b0\u5173\u8054\uff08\u5982\u4ee3\u8c22\u98ce\u9669\u56e0\u5b50\u4e0etau\u86cb\u767d\u5f02\u5e38\u7684\u6f5c\u5728\u8def\u5f84\uff09\uff0c\u5e76\u901a\u8fc7\u72ec\u7acb\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u4e3b\u8981\u53d1\u73b0\u7684\u7a33\u5065\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e0\u9700\u60a3\u8005ID\u5339\u914d\u5373\u53ef\u5b9e\u73b0\u6982\u5ff5\u5c42\u9762\u7684\u8de8\u6a21\u6001\u6574\u5408\uff0c\u4e3a\u5229\u7528\u788e\u7247\u5316\u6570\u636e\u7814\u7a76AD\u75c5\u7406\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u6027\u3002", "keywords": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u3001\u591a\u6a21\u6001\u6570\u636e\u3001\u77e5\u8bc6\u56fe\u8c31\u3001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u6570\u636e\u6574\u5408"}}
{"id": "2505.15734", "pdf": "https://arxiv.org/pdf/2505.15734", "abs": "https://arxiv.org/abs/2505.15734", "authors": ["Gaurav Srivastava", "Zhenyu Bi", "Meng Lu", "Xuan Wang"], "title": "DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have improved significantly in their reasoning\nthrough extensive training on massive datasets. However, relying solely on\nadditional data for improvement is becoming increasingly impractical,\nhighlighting the need for models to autonomously enhance their reasoning\nwithout external supervision. In this paper, we propose Debate, Train, Evolve\n(DTE), a novel ground truth-free training framework that uses multi-agent\ndebate traces to evolve a single language model. We also introduce a new\nprompting strategy Reflect-Critique-Refine, to improve debate quality by\nexplicitly instructing agents to critique and refine their reasoning. Extensive\nevaluations on five reasoning benchmarks with six open-weight models show that\nour DTE framework achieve substantial improvements, with an average accuracy\ngain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe\nstrong cross-domain generalization, with an average accuracy gain of 5.8% on\nall other benchmarks, suggesting that our method captures general reasoning\ncapabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aDTE\u7684\u65e0\u76d1\u7763\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u6210\u679c\u663e\u8457\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u4ec5\u4f9d\u8d56\u6d77\u91cf\u6570\u636e\u8bad\u7ec3\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u81ea\u4e3b\u63a8\u7406\u80fd\u529b\u63d0\u5347\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDTE\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u548cReflect-Critique-Refine\u63d0\u793a\u7b56\u7565\uff0c\u4f18\u5316\u6a21\u578b\u81ea\u6211\u8fdb\u5316\u3002", "result": "\u5728\u4e94\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53478.92%\uff0c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u63d0\u53475.8%\u3002", "conclusion": "DTE\u6846\u67b6\u6709\u6548\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u5177\u5907\u826f\u597d\u7684\u901a\u7528\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u63a8\u7406\u80fd\u529b,\u591a\u667a\u80fd\u4f53\u8fa9\u8bba,\u65e0\u76d1\u7763\u8bad\u7ec3,Reflect-Critique-Refine"}}
{"id": "2505.15754", "pdf": "https://arxiv.org/pdf/2505.15754", "abs": "https://arxiv.org/abs/2505.15754", "authors": ["Palash Chatterjee", "Roni Khardon"], "title": "Improving planning and MBRL with temporally-extended actions", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Continuous time systems are often modeled using discrete time dynamics but\nthis requires a small simulation step to maintain accuracy. In turn, this\nrequires a large planning horizon which leads to computationally demanding\nplanning problems and reduced performance. Previous work in model free\nreinforcement learning has partially addressed this issue using action repeats\nwhere a policy is learned to determine a discrete action duration. Instead we\npropose to control the continuous decision timescale directly by using\ntemporally-extended actions and letting the planner treat the duration of the\naction as an additional optimization variable along with the standard action\nvariables. This additional structure has multiple advantages. It speeds up\nsimulation time of trajectories and, importantly, it allows for deep horizon\nsearch in terms of primitive actions while using a shallow search depth in the\nplanner. In addition, in the model based reinforcement learning (MBRL) setting,\nit reduces compounding errors from model learning and improves training time\nfor models. We show that this idea is effective and that the range for action\ndurations can be automatically selected using a multi-armed bandit formulation\nand integrated into the MBRL framework. An extensive experimental evaluation\nboth in planning and in MBRL, shows that our approach yields faster planning,\nbetter solutions, and that it enables solutions to problems that are not solved\nin the standard formulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u76f4\u63a5\u63a7\u5236\u8fde\u7eed\u51b3\u7b56\u65f6\u95f4\u5c3a\u5ea6\u7684\u65b9\u6cd5\uff0c\u4f18\u5316\u52a8\u4f5c\u6301\u7eed\u65f6\u95f4\uff0c\u4ee5\u63d0\u5347\u89c4\u5212\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u79bb\u6563\u65f6\u95f4\u6a21\u62df\u9700\u8981\u5c0f\u6b65\u957f\u4ee5\u4fdd\u6301\u7cbe\u5ea6\uff0c\u5bfc\u81f4\u8ba1\u7b97\u91cf\u5927\u548c\u6027\u80fd\u4e0b\u964d\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u65f6\u95f4\u6269\u5c55\u52a8\u4f5c\uff0c\u5c06\u52a8\u4f5c\u6301\u7eed\u65f6\u95f4\u4f5c\u4e3a\u4f18\u5316\u53d8\u91cf\uff0c\u7ed3\u5408\u591a\u81c2\u8001\u864e\u673a\u81ea\u52a8\u9009\u62e9\u6301\u7eed\u65f6\u95f4\u8303\u56f4\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u52a0\u901f\u89c4\u5212\u3001\u63d0\u9ad8\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\uff0c\u5e76\u89e3\u51b3\u6807\u51c6\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u89c4\u5212\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "keywords": "\u8fde\u7eed\u65f6\u95f4\u7cfb\u7edf,\u5f3a\u5316\u5b66\u4e60,\u65f6\u95f4\u6269\u5c55\u52a8\u4f5c,\u591a\u81c2\u8001\u864e\u673a,\u89c4\u5212\u6548\u7387"}}
{"id": "2505.15275", "pdf": "https://arxiv.org/pdf/2505.15275", "abs": "https://arxiv.org/abs/2505.15275", "authors": ["Seokjun Lee", "Seung-Hyun Kong"], "title": "Learning-based Autonomous Oversteer Control and Collision Avoidance", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Oversteer, wherein a vehicle's rear tires lose traction and induce\nunintentional excessive yaw, poses critical safety challenges. Failing to\ncontrol oversteer often leads to severe traffic accidents. Although recent\nautonomous driving efforts have attempted to handle oversteer through\nstabilizing maneuvers, the majority rely on expert-defined trajectories or\nassume obstacle-free environments, limiting real-world applicability. This\npaper introduces a novel end-to-end (E2E) autonomous driving approach that\ntackles oversteer control and collision avoidance simultaneously. Existing E2E\ntechniques, including Imitation Learning (IL), Reinforcement Learning (RL), and\nHybrid Learning (HL), generally require near-optimal demonstrations or\nextensive experience. Yet even skilled human drivers struggle to provide\nperfect demonstrations under oversteer, and high transition variance hinders\naccumulating sufficient data. Hence, we present Q-Compared Soft Actor-Critic\n(QC-SAC), a new HL algorithm that effectively learns from suboptimal\ndemonstration data and adapts rapidly to new conditions. To evaluate QC-SAC, we\nintroduce a benchmark inspired by real-world driver training: a vehicle\nencounters sudden oversteer on a slippery surface and must avoid randomly\nplaced obstacles ahead. Experimental results show QC-SAC attains near-optimal\ndriving policies, significantly surpassing state-of-the-art IL, RL, and HL\nbaselines. Our method demonstrates the world's first safe autonomous oversteer\ncontrol with obstacle avoidance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7aef\u5230\u7aef\uff08E2E\uff09\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5QC-SAC\uff0c\u7528\u4e8e\u540c\u65f6\u5904\u7406\u8f66\u8f86\u8fc7\u5ea6\u8f6c\u5411\u63a7\u5236\u548c\u907f\u969c\u95ee\u9898\uff0c\u901a\u8fc7\u5b66\u4e60\u6b21\u4f18\u6f14\u793a\u6570\u636e\u5e76\u5feb\u901f\u9002\u5e94\u65b0\u6761\u4ef6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8f66\u8f86\u8fc7\u5ea6\u8f6c\u5411\u662f\u4e00\u4e2a\u5173\u952e\u7684\u5b89\u5168\u95ee\u9898\uff0c\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u5b9a\u4e49\u7684\u8f68\u8ff9\u6216\u5047\u8bbe\u65e0\u969c\u73af\u5883\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5904\u7406\u8fc7\u5ea6\u8f6c\u5411\u548c\u907f\u969c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faQ-Compared Soft Actor-Critic (QC-SAC)\uff0c\u4e00\u79cd\u6df7\u5408\u5b66\u4e60\uff08HL\uff09\u7b97\u6cd5\uff0c\u80fd\u591f\u4ece\u6b21\u4f18\u6f14\u793a\u6570\u636e\u4e2d\u5b66\u4e60\u5e76\u5feb\u901f\u9002\u5e94\u65b0\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cQC-SAC\u5728\u6a21\u62df\u7684\u6e7f\u6ed1\u8def\u9762\u548c\u968f\u673a\u969c\u788d\u7269\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u9a7e\u9a76\u7b56\u7565\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709IL\u3001RL\u548cHL\u57fa\u7ebf\u3002", "conclusion": "QC-SAC\u9996\u6b21\u5b9e\u73b0\u4e86\u5b89\u5168\u7684\u81ea\u52a8\u9a7e\u9a76\u8fc7\u5ea6\u8f6c\u5411\u63a7\u5236\u4e0e\u907f\u969c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u81ea\u52a8\u9a7e\u9a76, \u8fc7\u5ea6\u8f6c\u5411\u63a7\u5236, \u7aef\u5230\u7aef\u5b66\u4e60, Q-Compared Soft Actor-Critic, \u907f\u969c"}}
{"id": "2505.15769", "pdf": "https://arxiv.org/pdf/2505.15769", "abs": "https://arxiv.org/abs/2505.15769", "authors": ["Mikhail Budnikov", "Ivan Yamshchikov"], "title": "Transfer of Structural Knowledge from Synthetic Languages", "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 3 figures and 3 tables to be published in ACL 2025 Workshop\n  XLLM", "summary": "This work explores transfer learning from several synthetic languages to\nEnglish. We investigate the structure of the embeddings in the fine-tuned\nmodels, the information they contain, and the capabilities of the fine-tuned\nmodels on simple linguistic tasks. We also introduce a new synthetic language\nthat leads to better transfer to English than the languages used in previous\nresearch. Finally, we introduce Tiny-Cloze Benchmark - a new synthetic\nbenchmark for natural language understanding that is more informative for less\npowerful models. We use Tiny-Cloze Benchmark to evaluate fine-tuned models in\nseveral domains demonstrating that fine-tuning on a new synthetic language\nallows for better performance on a variety of tasks.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4ece\u591a\u79cd\u5408\u6210\u8bed\u8a00\u5230\u82f1\u8bed\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u5206\u6790\u4e86\u5fae\u8c03\u6a21\u578b\u7684\u5d4c\u5165\u7ed3\u6784\u53ca\u5176\u5728\u7b80\u5355\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u5408\u6210\u8bed\u8a00\u548c\u6539\u8fdb\u7684Tiny-Cloze Benchmark\u3002", "motivation": "\u63a2\u7a76\u5408\u6210\u8bed\u8a00\u5bf9\u82f1\u8bed\u8fc1\u79fb\u5b66\u4e60\u7684\u6548\u679c\uff0c\u5bfb\u627e\u66f4\u4f18\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u548c\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u5f15\u5165\u4e86\u65b0\u7684\u5408\u6210\u8bed\u8a00\u548c\u6539\u8fdb\u7684Tiny-Cloze Benchmark\uff0c\u8bc4\u4f30\u5fae\u8c03\u6a21\u578b\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u65b0\u5408\u6210\u8bed\u8a00\u6539\u5584\u4e86\u8fc1\u79fb\u5b66\u4e60\u6548\u679c\uff0cTiny-Cloze Benchmark\u5bf9\u8f83\u5f31\u6a21\u578b\u66f4\u5177\u4fe1\u606f\u6027\u3002", "conclusion": "\u4f7f\u7528\u65b0\u7684\u5408\u6210\u8bed\u8a00\u5fae\u8c03\u6a21\u578b\u80fd\u63d0\u5347\u591a\u79cd\u4efb\u52a1\u6027\u80fd\uff0c\u6539\u8fdb\u7684\u57fa\u51c6\u6d4b\u8bd5\u66f4\u5177\u5b9e\u7528\u6027\u3002", "keywords": "\u8fc1\u79fb\u5b66\u4e60, \u5408\u6210\u8bed\u8a00, \u5d4c\u5165\u5f0f\u7ed3\u6784, Tiny-Cloze Benchmark"}}
{"id": "2505.15777", "pdf": "https://arxiv.org/pdf/2505.15777", "abs": "https://arxiv.org/abs/2505.15777", "authors": ["Jorge Bacca"], "title": "Projection-Based Correction for Enhancing Deep Inverse Networks", "categories": ["cs.LG", "cs.CV", "physics.comp-ph"], "comment": null, "summary": "Deep learning-based models have demonstrated remarkable success in solving\nillposed inverse problems; however, many fail to strictly adhere to the\nphysical constraints imposed by the measurement process. In this work, we\nintroduce a projection-based correction method to enhance the inference of deep\ninverse networks by ensuring consistency with the forward model. Specifically,\ngiven an initial estimate from a learned reconstruction network, we apply a\nprojection step that constrains the solution to lie within the valid solution\nspace of the inverse problem. We theoretically demonstrate that if the recovery\nmodel is a well-trained deep inverse network, the solution can be decomposed\ninto range-space and null-space components, where the projection-based\ncorrection reduces to an identity transformation. Extensive simulations and\nexperiments validate the proposed method, demonstrating improved reconstruction\naccuracy across diverse inverse problems and deep network architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6295\u5f71\u7684\u4fee\u6b63\u65b9\u6cd5\uff0c\u901a\u8fc7\u786e\u4fdd\u4e0e\u6b63\u5411\u6a21\u578b\u7684\u4e00\u81f4\u6027\u6765\u589e\u5f3a\u6df1\u5ea6\u9006\u7f51\u7edc\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u89e3\u51b3\u4e0d\u9002\u5b9a\u53cd\u95ee\u9898\u65f6\u672a\u4e25\u683c\u9075\u5faa\u7269\u7406\u7ea6\u675f\uff0c\u6539\u8fdb\u65b9\u6848\u65e8\u5728\u589e\u5f3a\u89e3\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "method": "\u5229\u7528\u6295\u5f71\u6b65\u9aa4\u7ea6\u675f\u89e3\u7a7a\u95f4\uff0c\u786e\u4fdd\u5176\u4f4d\u4e8e\u53cd\u95ee\u9898\u7684\u6709\u6548\u89e3\u7a7a\u95f4\u4e2d\uff0c\u7406\u8bba\u8bc1\u660e\u4fee\u6b63\u540e\u53ef\u5206\u89e3\u4e3a\u5206\u91cf\u7a7a\u95f4\u3002", "result": "\u5e7f\u6cdb\u6a21\u62df\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u53cd\u95ee\u9898\u548c\u7f51\u7edc\u67b6\u6784\u4e0b\u7684\u91cd\u5efa\u7cbe\u5ea6\u63d0\u5347\u3002", "conclusion": "\u6295\u5f71\u4fee\u6b63\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6df1\u5ea6\u9006\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u786e\u4fdd\u89e3\u4e0e\u7269\u7406\u7ea6\u675f\u7684\u4e00\u81f4\u6027\u3002", "keywords": "\u6df1\u5ea6\u5b66\u4e60\u3001\u53cd\u95ee\u9898\u3001\u6295\u5f71\u4fee\u6b63\u3001\u7269\u7406\u7ea6\u675f"}}
{"id": "2505.15285", "pdf": "https://arxiv.org/pdf/2505.15285", "abs": "https://arxiv.org/abs/2505.15285", "authors": ["Fengting Zhang", "Boxu Liang", "Qinghao Liu", "Min Liu", "Xiang Chen", "Yaonan Wang"], "title": "Reconsider the Template Mesh in Deep Learning-based Mesh Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Mesh reconstruction is a cornerstone process across various applications,\nincluding in-silico trials, digital twins, surgical planning, and navigation.\nRecent advancements in deep learning have notably enhanced mesh reconstruction\nspeeds. Yet, traditional methods predominantly rely on deforming a standardised\ntemplate mesh for individual subjects, which overlooks the unique anatomical\nvariations between them, and may compromise the fidelity of the\nreconstructions. In this paper, we propose an adaptive-template-based mesh\nreconstruction network (ATMRN), which generates adaptive templates from the\ngiven images for the subsequent deformation, moving beyond the constraints of a\nsingular, fixed template. Our approach, validated on cortical magnetic\nresonance (MR) images from the OASIS dataset, sets a new benchmark in\nvoxel-to-cortex mesh reconstruction, achieving an average symmetric surface\ndistance of 0.267mm across four cortical structures. Our proposed method is\ngeneric and can be easily transferred to other image modalities and anatomical\nstructures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u6a21\u677f\u7684\u7f51\u683c\u91cd\u5efa\u7f51\u7edc\uff08ATMRN\uff09\uff0c\u901a\u8fc7\u751f\u6210\u9002\u5e94\u6027\u6a21\u677f\u6765\u514b\u670d\u4f20\u7edf\u5355\u4e00\u56fa\u5b9a\u6a21\u677f\u7684\u5c40\u9650\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7f51\u683c\u91cd\u5efa\u7684\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7f51\u683c\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6a21\u677f\uff0c\u65e0\u6cd5\u6355\u6349\u4e2a\u4f53\u95f4\u7684\u89e3\u5256\u5dee\u5f02\uff0c\u5f71\u54cd\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002", "method": "\u91c7\u7528ATMRN\u751f\u6210\u81ea\u9002\u5e94\u6a21\u677f\uff0c\u968f\u540e\u8fdb\u884c\u53d8\u5f62\uff0c\u907f\u514d\u4e86\u56fa\u5b9a\u6a21\u677f\u7684\u9650\u5236\u3002", "result": "\u5728OASIS\u6570\u636e\u96c6\u7684\u76ae\u5c42\u78c1\u5171\u632f\u56fe\u50cf\u4e0a\u9a8c\u8bc1\uff0c\u5e73\u5747\u5bf9\u79f0\u8868\u9762\u8ddd\u79bb\u8fbe0.267mm\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u7528\u6027\u5f3a\uff0c\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u56fe\u50cf\u6a21\u6001\u548c\u89e3\u5256\u7ed3\u6784\u3002", "keywords": "\u7f51\u683c\u91cd\u5efa, \u81ea\u9002\u5e94\u6a21\u677f, \u6df1\u5ea6\u5b66\u4e60, \u89e3\u5256\u5dee\u5f02"}}
{"id": "2505.15774", "pdf": "https://arxiv.org/pdf/2505.15774", "abs": "https://arxiv.org/abs/2505.15774", "authors": ["Huanxuan Liao", "Wen Hu", "Yao Xu", "Shizhu He", "Jun Zhao", "Kang Liu"], "title": "Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) encounter significant challenges in\nlong-sequence inference due to computational inefficiency and redundant\nprocessing, driving interest in context compression techniques. Existing\nmethods often rely on token importance to perform hard local compression or\nencode context into latent representations for soft global compression.\nHowever, the uneven distribution of textual content relevance and the diversity\nof demands for user instructions mean these approaches frequently lead to the\nloss of potentially valuable information. To address this, we propose\n$\\textbf{Hy}$brid $\\textbf{Co}$ntext $\\textbf{Co}$mpression (HyCo$_2$) for\nLLMs, which integrates both global and local perspectives to guide context\ncompression while retaining both the essential semantics and critical details\nfor task completion. Specifically, we employ a hybrid adapter to refine global\nsemantics with the global view, based on the observation that different\nadapters excel at different tasks. Then we incorporate a classification layer\nthat assigns a retention probability to each context token based on the local\nview, determining whether it should be retained or discarded. To foster a\nbalanced integration of global and local compression, we introduce auxiliary\nparaphrasing and completion pretraining before instruction tuning. This\npromotes a synergistic integration that emphasizes instruction-relevant\ninformation while preserving essential local details, ultimately balancing\nlocal and global information retention in context compression. Experiments show\nthat our HyCo$_2$ method significantly enhances long-text reasoning while\nreducing token usage. It improves the performance of various LLM series by an\naverage of 13.1\\% across seven knowledge-intensive QA benchmarks. Moreover,\nHyCo$_2$ matches the performance of uncompressed methods while reducing token\nconsumption by 88.8\\%.", "AI": {"tldr": "HyCo\u2082\u662f\u4e00\u79cd\u6df7\u5408\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u89c6\u89d2\uff0c\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u5e8f\u5217\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5e8f\u5217\u63a8\u7406\u4e2d\u7684\u8ba1\u7b97\u4f4e\u6548\u548c\u5197\u4f59\u5904\u7406\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u5bfc\u81f4\u7684\u4fe1\u606f\u4e22\u5931\u3002", "method": "\u901a\u8fc7\u6df7\u5408\u9002\u914d\u5668\u548c\u5206\u7c7b\u5c42\u6574\u5408\u5168\u5c40\u4e0e\u5c40\u90e8\u538b\u7f29\u89c6\u89d2\uff0c\u7ed3\u5408\u8f85\u52a9\u9884\u8bad\u7ec3\u4efb\u52a1\u6307\u5bfc\u4fe1\u606f\u4fdd\u7559\u3002", "result": "HyCo\u2082\u5728\u77e5\u8bc6\u5bc6\u96c6\u578bQA\u4efb\u52a1\u4e2d\u5e73\u5747\u63d0\u534713.1%\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c1188.8%\u7684token\u6d88\u8017\u3002", "conclusion": "HyCo\u2082\u6709\u6548\u5e73\u8861\u4fe1\u606f\u4fdd\u7559\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u663e\u8457\u63d0\u5347\u957f\u6587\u672c\u63a8\u7406\u6027\u80fd\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u4e0a\u4e0b\u6587\u538b\u7f29, \u5168\u5c40-\u5c40\u90e8\u89c6\u89d2, \u6df7\u5408\u9002\u914d\u5668, \u4fe1\u606f\u4fdd\u7559"}}
{"id": "2505.15782", "pdf": "https://arxiv.org/pdf/2505.15782", "abs": "https://arxiv.org/abs/2505.15782", "authors": ["Pedro P. Santos", "Alberto Sardinha", "Francisco S. Melo"], "title": "Solving General-Utility Markov Decision Processes in the Single-Trial Regime with Online Planning", "categories": ["cs.LG"], "comment": null, "summary": "In this work, we contribute the first approach to solve infinite-horizon\ndiscounted general-utility Markov decision processes (GUMDPs) in the\nsingle-trial regime, i.e., when the agent's performance is evaluated based on a\nsingle trajectory. First, we provide some fundamental results regarding policy\noptimization in the single-trial regime, investigating which class of policies\nsuffices for optimality, casting our problem as a particular MDP that is\nequivalent to our original problem, as well as studying the computational\nhardness of policy optimization in the single-trial regime. Second, we show how\nwe can leverage online planning techniques, in particular a Monte-Carlo tree\nsearch algorithm, to solve GUMDPs in the single-trial regime. Third, we provide\nexperimental results showcasing the superior performance of our approach in\ncomparison to relevant baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u89e3\u51b3\u5355\u6b21\u8bd5\u9a8c\u673a\u5236\u4e0b\u65e0\u9650\u65f6\u57df\u6298\u6263\u901a\u7528\u6548\u7528\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08GUMDPs\uff09\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u7b56\u7565\u4f18\u5316\u7684\u7406\u8bba\u5206\u6790\u3001\u5728\u7ebf\u89c4\u5212\u6280\u672f\u7684\u5e94\u7528\u53ca\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u5355\u6b21\u8bd5\u9a8c\u673a\u5236\u4e0b\u7684GUMDPs\u95ee\u9898\uff0c\u586b\u8865\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u5728\u5b9e\u9645\u4e2d\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u7ed3\u5408\u7b56\u7565\u4f18\u5316\u7684\u7406\u8bba\u5206\u6790\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7b97\u6cd5\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6c42\u89e3\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u76f8\u5173\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5355\u6b21\u8bd5\u9a8c\u673a\u5236\u4e0b\u8868\u73b0\u4f18\u8d8a\uff0c\u4e3aGUMDPs\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u901a\u7528\u6548\u7528\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b, \u5355\u6b21\u8bd5\u9a8c\u673a\u5236, \u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22, \u7b56\u7565\u4f18\u5316"}}
{"id": "2505.15776", "pdf": "https://arxiv.org/pdf/2505.15776", "abs": "https://arxiv.org/abs/2505.15776", "authors": ["Changtai Zhu", "Siyin Wang", "Ruijun Feng", "Kai Song", "Xipeng Qiu"], "title": "ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Conversational search systems require effective handling of context-dependent\nqueries that often contain ambiguity, omission, and coreference. Conversational\nQuery Reformulation (CQR) addresses this challenge by transforming these\nqueries into self-contained forms suitable for off-the-shelf retrievers.\nHowever, existing CQR approaches suffer from two critical constraints: high\ndependency on costly external supervision from human annotations or large\nlanguage models, and insufficient alignment between the rewriting model and\ndownstream retrievers. We present ConvSearch-R1, the first self-driven\nframework that completely eliminates dependency on external rewrite supervision\nby leveraging reinforcement learning to optimize reformulation directly through\nretrieval signals. Our novel two-stage approach combines Self-Driven Policy\nWarm-Up to address the cold-start problem through retrieval-guided\nself-distillation, followed by Retrieval-Guided Reinforcement Learning with a\nspecially designed rank-incentive reward shaping mechanism that addresses the\nsparsity issue in conventional retrieval metrics. Extensive experiments on\nTopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly\noutperforms previous state-of-the-art methods, achieving over 10% improvement\non the challenging TopiOCQA dataset while using smaller 3B parameter models\nwithout any external supervision.", "AI": {"tldr": "ConvSearch-R1\u662f\u4e00\u4e2a\u81ea\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u4ece\u68c0\u7d22\u4fe1\u53f7\u4f18\u5316\u67e5\u8be2\u91cd\u6784\uff0c\u6d88\u9664\u4e86\u5bf9\u5916\u90e8\u76d1\u7763\u7684\u4f9d\u8d56\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5bf9\u8bdd\u641c\u7d22\u7cfb\u7edf\u4e2d\u67e5\u8be2\u91cd\u6784\u65b9\u6cd5\u5bf9\u5916\u90e8\u76d1\u7763\u7684\u9ad8\u4f9d\u8d56\u4ee5\u53ca\u4e0e\u4e0b\u6e38\u68c0\u7d22\u5668\u5bf9\u9f50\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u81ea\u9a71\u52a8\u7b56\u7565\u9884\u70ed\u548c\u68c0\u7d22\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u6392\u540d\u6fc0\u52b1\u5956\u52b1\u673a\u5236\u3002", "result": "\u5728TopiOCQA\u548cQReCC\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u63d0\u5347\u8d85\u8fc710%\u3002", "conclusion": "ConvSearch-R1\u65e0\u9700\u5916\u90e8\u76d1\u7763\u5373\u53ef\u663e\u8457\u63d0\u5347\u67e5\u8be2\u91cd\u6784\u6548\u679c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "keywords": "\u5bf9\u8bdd\u641c\u7d22, \u67e5\u8be2\u91cd\u6784, \u5f3a\u5316\u5b66\u4e60, \u81ea\u9a71\u52a8\u6846\u67b6"}}
{"id": "2505.15784", "pdf": "https://arxiv.org/pdf/2505.15784", "abs": "https://arxiv.org/abs/2505.15784", "authors": ["Jun Wan", "Lingrui Mei"], "title": "Large Language Models as Computable Approximations to Solomonoff Induction", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Both authors contributed equally", "summary": "The rapid advancement of large language models (LLMs) calls for a rigorous\ntheoretical framework to explain their empirical success. While significant\nprogress has been made in understanding LLM behaviors, existing theoretical\nframeworks remain fragmented in explaining emergent phenomena through a unified\nmathematical lens. We establish the first formal connection between LLM\narchitectures and Algorithmic Information Theory (AIT) by proving two\nfundamental results: (1) the training process computationally approximates\nSolomonoff prior through loss minimization interpreted as program length\noptimization, and (2) next-token prediction implements approximate Solomonoff\ninduction. We leverage AIT to provide a unified theoretical explanation for\nin-context learning, few-shot learning, and scaling laws. Furthermore, our\ntheoretical insights lead to a principled method for few-shot example selection\nthat prioritizes samples where models exhibit lower predictive confidence. We\ndemonstrate through experiments on diverse text classification benchmarks that\nthis strategy yields significant performance improvements, particularly for\nsmaller model architectures, when compared to selecting high-confidence\nexamples. Our framework bridges the gap between theoretical foundations and\npractical LLM behaviors, providing both explanatory power and actionable\ninsights for future model development.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7b97\u6cd5\u4fe1\u606f\u8bba\uff08AIT\uff09\u9996\u6b21\u5efa\u7acb\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u67b6\u6784\u4e0e\u7406\u8bba\u4e4b\u95f4\u7684\u6b63\u5f0f\u8054\u7cfb\uff0c\u89e3\u91ca\u4e86\u5176\u8bad\u7ec3\u8fc7\u7a0b\u4e0e\u9884\u6d4b\u884c\u4e3a\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u6d4b\u4fe1\u5fc3\u7684\u5c11\u6837\u672c\u9009\u62e9\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5173\u4e8eLLM\u7684\u7406\u8bba\u6846\u67b6\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u6570\u5b66\u89e3\u91ca\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7AIT\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u8bc1\u660e\u4e24\u4e2a\u6838\u5fc3\u7406\u8bba\u7ed3\u679c\uff1a(1) \u8bad\u7ec3\u8fc7\u7a0b\u901a\u8fc7\u635f\u5931\u6700\u5c0f\u5316\u8fd1\u4f3c\u6240\u7f57\u95e8\u8bfa\u592b\u5148\u9a8c\uff1b(2) \u4e0b\u4e00\u8bcd\u9884\u6d4b\u5b9e\u73b0\u8fd1\u4f3c\u6240\u7f57\u95e8\u8bfa\u592b\u5f52\u7eb3\u3002\u5e76\u57fa\u4e8e\u9884\u6d4b\u4fe1\u5fc3\u8bbe\u8ba1\u5c11\u6837\u672c\u9009\u62e9\u65b9\u6cd5\u3002", "result": "\u7406\u8bba\u89e3\u91ca\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u5c11\u6837\u672c\u5b66\u4e60\u548c\u6269\u5c55\u89c4\u5f8b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4f4e\u9884\u6d4b\u4fe1\u5fc3\u6837\u672c\u9009\u62e9\u7b56\u7565\u5bf9\u5c0f\u578b\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\u4f5c\u7528\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u7684\u7406\u8bba\u4e0e\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6865\u63a5\uff0c\u517c\u5177\u89e3\u91ca\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u7b97\u6cd5\u4fe1\u606f\u8bba, \u6240\u7f57\u95e8\u8bfa\u592b\u5148\u9a8c, \u5c11\u6837\u672c\u5b66\u4e60, \u4e0a\u4e0b\u6587\u5b66\u4e60"}}
{"id": "2505.15778", "pdf": "https://arxiv.org/pdf/2505.15778", "abs": "https://arxiv.org/abs/2505.15778", "authors": ["Zhen Zhang", "Xuehai He", "Weixiang Yan", "Ao Shen", "Chenyang Zhao", "Shuohang Wang", "Yelong Shen", "Xin Eric Wang"], "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Human cognition typically involves thinking through abstract, fluid concepts\nrather than strictly using discrete linguistic tokens. Current reasoning\nmodels, however, are constrained to reasoning within the boundaries of human\nlanguage, processing discrete token embeddings that represent fixed points in\nthe semantic space. This discrete constraint restricts the expressive power and\nupper potential of such reasoning models, often causing incomplete exploration\nof reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling\none token per step. In this work, we introduce Soft Thinking, a training-free\nmethod that emulates human-like \"soft\" reasoning by generating soft, abstract\nconcept tokens in a continuous concept space. These concept tokens are created\nby the probability-weighted mixture of token embeddings, which form the\ncontinuous concept space, enabling smooth transitions and richer\nrepresentations that transcend traditional discrete boundaries. In essence,\neach generated concept token encapsulates multiple meanings from related\ndiscrete tokens, implicitly exploring various reasoning paths to converge\neffectively toward the correct answer. Empirical evaluations on diverse\nmathematical and coding benchmarks consistently demonstrate the effectiveness\nand efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points\nwhile simultaneously reducing token usage by up to 22.4% compared to standard\nCoT. Qualitative analysis further reveals that Soft Thinking outputs remain\nhighly interpretable and readable, highlighting the potential of Soft Thinking\nto break the inherent bottleneck of discrete language-based reasoning. Code is\navailable at https://github.com/eric-ai-lab/Soft-Thinking.", "AI": {"tldr": "\u63d0\u51fa'Soft Thinking'\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fde\u7eed\u6982\u5ff5\u7a7a\u95f4\u4e2d\u7684\u8f6f\u6982\u5ff5\u4ee4\u724c\u6a21\u62df\u4eba\u7c7b\u62bd\u8c61\u63a8\u7406\uff0c\u8d85\u8d8a\u79bb\u6563\u8bed\u8a00\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u6a21\u578b\u53d7\u9650\u4e8e\u79bb\u6563\u8bed\u8a00\u6807\u8bb0\uff0c\u672a\u80fd\u5145\u5206\u63a2\u7d22\u63a8\u7406\u8def\u5f84\uff0c\u56e0\u6b64\u63d0\u51fa\u8fde\u7eed\u6982\u5ff5\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u751f\u6210\u6982\u7387\u52a0\u6743\u7684\u4ee4\u724c\u5d4c\u5165\u6df7\u5408\u4f53\u4f5c\u4e3a\u8f6f\u6982\u5ff5\u4ee4\u724c\uff0c\u5f62\u6210\u8fde\u7eed\u6982\u5ff5\u7a7a\u95f4\u3002", "result": "\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u4e2d\uff0cpass@1\u51c6\u786e\u7387\u63d0\u53472.48\u5206\uff0c\u4ee4\u724c\u4f7f\u7528\u51cf\u5c1122.4%\u3002", "conclusion": "Soft Thinking\u80fd\u7a81\u7834\u79bb\u6563\u8bed\u8a00\u7684\u63a8\u7406\u74f6\u9888\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "keywords": "Soft Thinking, \u8fde\u7eed\u6982\u5ff5\u7a7a\u95f4, \u62bd\u8c61\u63a8\u7406, Chain-of-Thought, \u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.15788", "pdf": "https://arxiv.org/pdf/2505.15788", "abs": "https://arxiv.org/abs/2505.15788", "authors": ["Zahra Khatti", "Daniel P. Robinson", "Frank E. Curtis"], "title": "Fair Supervised Learning Through Constraints on Smooth Nonconvex Unfairness-Measure Surrogates", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "A new strategy for fair supervised machine learning is proposed. The main\nadvantages of the proposed strategy as compared to others in the literature are\nas follows. (a) We introduce a new smooth nonconvex surrogate to approximate\nthe Heaviside functions involved in discontinuous unfairness measures. The\nsurrogate is based on smoothing methods from the optimization literature, and\nis new for the fair supervised learning literature. The surrogate is a tight\napproximation which ensures the trained prediction models are fair, as opposed\nto other (e.g., convex) surrogates that can fail to lead to a fair prediction\nmodel in practice. (b) Rather than rely on regularizers (that lead to\noptimization problems that are difficult to solve) and corresponding\nregularization parameters (that can be expensive to tune), we propose a\nstrategy that employs hard constraints so that specific tolerances for\nunfairness can be enforced without the complications associated with the use of\nregularization. (c)~Our proposed strategy readily allows for constraints on\nmultiple (potentially conflicting) unfairness measures at the same time.\nMultiple measures can be considered with a regularization approach, but at the\ncost of having even more difficult optimization problems to solve and further\nexpense for tuning. By contrast, through hard constraints, our strategy leads\nto optimization models that can be solved tractably with minimal tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u516c\u5e73\u76d1\u7763\u673a\u5668\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u975e\u51f8\u5e73\u6ed1\u66ff\u4ee3\u548c\u786c\u7ea6\u675f\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u4e0d\u516c\u5e73\u6027\u548c\u4f18\u5316\u96be\u9898\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4e2d\uff0c\u4e0d\u8fde\u7eed\u7684\u516c\u5e73\u6027\u5ea6\u91cf\u96be\u4ee5\u5904\u7406\uff0c\u4e14\u57fa\u4e8e\u6b63\u5219\u5316\u7684\u65b9\u6cd5\u4f18\u5316\u56f0\u96be\u4e14\u53c2\u6570\u8c03\u4f18\u6210\u672c\u9ad8\u3002", "method": "\u5f15\u5165\u65b0\u7684\u5e73\u6ed1\u975e\u51f8\u66ff\u4ee3\u51fd\u6570\u903c\u8fd1Heaviside\u51fd\u6570\uff0c\u5e76\u4f7f\u7528\u786c\u7ea6\u675f\u800c\u975e\u6b63\u5219\u5316\u6765\u5b9e\u73b0\u516c\u5e73\u6027\u5bb9\u5fcd\u5ea6\u3002", "result": "\u7b56\u7565\u80fd\u591f\u6709\u6548\u4fdd\u8bc1\u6a21\u578b\u516c\u5e73\u6027\uff0c\u5e76\u652f\u6301\u540c\u65f6\u5904\u7406\u591a\u4e2a\uff08\u53ef\u80fd\u51b2\u7a81\u7684\uff09\u4e0d\u516c\u5e73\u6027\u5ea6\u91cf\uff0c\u4e14\u4f18\u5316\u95ee\u9898\u6613\u4e8e\u6c42\u89e3\u3002", "conclusion": "\u65b0\u7b56\u7565\u5728\u516c\u5e73\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002", "keywords": "\u516c\u5e73\u673a\u5668\u5b66\u4e60, \u975e\u51f8\u66ff\u4ee3, \u786c\u7ea6\u675f, \u4f18\u5316\u95ee\u9898"}}
{"id": "2505.15781", "pdf": "https://arxiv.org/pdf/2505.15781", "abs": "https://arxiv.org/abs/2505.15781", "authors": ["Xinyin Ma", "Runpeng Yu", "Gongfan Fang", "Xinchao Wang"], "title": "dKV-Cache: The Cache for Diffusion Language Models", "categories": ["cs.CL"], "comment": "The code is available at https://github.com/horseee/dKV-Cache", "summary": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5ef6\u8fdfKV-Cache\u7684\u673a\u5236\uff0c\u901a\u8fc7\u9010\u6b65\u7f13\u5b58\u952e\u503c\u72b6\u6001\u6765\u89e3\u51b3\u6269\u6563\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u56e0\u5176\u975e\u81ea\u56de\u5f52\u67b6\u6784\u548c\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\uff0c\u65e0\u6cd5\u5229\u7528\u952e\u503c\u7f13\u5b58\u52a0\u901f\u63a8\u7406\uff0c\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6162\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\uff0c\u63d0\u51fa\u4e86\u5ef6\u8fdfKV-Cache\u673a\u5236\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u79cd\u7f13\u5b58\u7b56\u7565\uff1a(1) dKV-Cache-Decode\uff0c\u51e0\u4e4e\u65e0\u635f\u52a0\u901f\u4e14\u5bf9\u957f\u5e8f\u5217\u6027\u80fd\u6709\u63d0\u5347\uff1b(2) dKV-Cache-Greedy\uff0c\u727a\u7272\u90e8\u5206\u6027\u80fd\u6362\u53d6\u66f4\u9ad8\u52a0\u901f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u5b9e\u73b0\u4e862-10\u500d\u7684\u63d0\u5347\uff0c\u5e76\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u5ef6\u8fdfKV-Cache\u673a\u5236\u6709\u6548\u7f29\u5c0f\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u4e0e\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5e94\u7528\u3002", "keywords": "\u6269\u6563\u8bed\u8a00\u6a21\u578b, KV-Cache, \u63a8\u7406\u52a0\u901f, \u952e\u503c\u7f13\u5b58"}}
{"id": "2505.15798", "pdf": "https://arxiv.org/pdf/2505.15798", "abs": "https://arxiv.org/abs/2505.15798", "authors": ["Taehoon Kim", "Henry Gouk", "Minyoung Kim", "Timothy Hospedales"], "title": "Model Merging is Secretly Certifiable: Non-Vacuous Generalisation Bounds for Low-Shot Learning", "categories": ["cs.LG"], "comment": null, "summary": "Certifying the IID generalisation ability of deep networks is the first of\nmany requirements for trusting AI in high-stakes applications from medicine to\nsecurity. However, when instantiating generalisation bounds for deep networks\nit remains challenging to obtain non-vacuous guarantees, especially when\napplying contemporary large models on the small scale data prevalent in such\nhigh-stakes fields. In this paper, we draw a novel connection between a family\nof learning methods based on model fusion and generalisation certificates, and\nsurprisingly show that with minor adjustment several existing learning\nstrategies already provide non-trivial generalisation guarantees. Essentially,\nby focusing on data-driven learning of downstream tasks by fusion rather than\nfine-tuning, the certified generalisation gap becomes tiny and independent of\nthe base network size, facilitating its certification. Our results show for the\nfirst time non-trivial generalisation guarantees for learning with as low as\n100 examples, while using vision models such as VIT-B and language models such\nas mistral-7B. This observation is significant as it has immediate implications\nfor facilitating the certification of existing systems as trustworthy, and\nopens up new directions for research at the intersection of practice and\ntheory.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u901a\u8fc7\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u800c\u975e\u5fae\u8c03\u6765\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5c11\u6837\u672c\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u975e\u5e73\u51e1\u7684\u6cdb\u5316\u4fdd\u8bc1\u3002", "motivation": "\u5728\u533b\u5b66\u3001\u5b89\u5168\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u9a8c\u8bc1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u72ec\u7acb\u540c\u5206\u5e03\uff08IID\uff09\u6cdb\u5316\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5728\u5927\u6a21\u578b\u5c0f\u6570\u636e\u573a\u666f\u4e0b\u63d0\u4f9b\u975e\u7a7a\u6cdb\u7684\u4fdd\u8bc1\u3002", "method": "\u901a\u8fc7\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u5b66\u4e60\u4e0b\u6e38\u4efb\u52a1\uff0c\u800c\u975e\u76f4\u63a5\u5fae\u8c03\u57fa\u7840\u7f51\u7edc\uff0c\u4ece\u800c\u7f29\u5c0f\u6cdb\u5316\u5dee\u8ddd\u5e76\u4f7f\u5176\u72ec\u7acb\u4e8e\u57fa\u7840\u7f51\u7edc\u89c4\u6a21\u3002", "result": "\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u4ec5100\u4e2a\u6837\u672c\u4e0b\uff0c\u5bf9VIT-B\u548cmistral-7B\u7b49\u6a21\u578b\u7684\u975e\u5e73\u51e1\u6cdb\u5316\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u4e3a\u73b0\u6709\u7cfb\u7edf\u7684\u53ef\u4fe1\u8ba4\u8bc1\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u8fd8\u4e3a\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u7ed3\u5408\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u6df1\u5ea6\u5b66\u4e60, \u6cdb\u5316\u80fd\u529b, \u6a21\u578b\u878d\u5408, \u9ad8\u98ce\u9669\u5e94\u7528, \u5c0f\u6837\u672c\u5b66\u4e60"}}
{"id": "2505.15308", "pdf": "https://arxiv.org/pdf/2505.15308", "abs": "https://arxiv.org/abs/2505.15308", "authors": ["Ji Guo", "Xiaolei Wen", "Wenbo Jiang", "Cheng Huang", "Jinjin Li", "Hongwei Li"], "title": "BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the widespread application of super-resolution (SR) in various fields,\nresearchers have begun to investigate its security. Previous studies have\ndemonstrated that SR models can also be subjected to backdoor attacks through\ndata poisoning, affecting downstream tasks. A backdoor SR model generates an\nattacker-predefined target image when given a triggered image while producing a\nnormal high-resolution (HR) output for clean images. However, prior backdoor\nattacks on SR models have primarily focused on the stealthiness of poisoned\nlow-resolution (LR) images while ignoring the stealthiness of poisoned HR\nimages, making it easy for users to detect anomalous data. To address this\nproblem, we propose BadSR, which improves the stealthiness of poisoned HR\nimages. The key idea of BadSR is to approximate the clean HR image and the\npre-defined target image in the feature space while ensuring that modifications\nto the clean HR image remain within a constrained range. The poisoned HR images\ngenerated by BadSR can be integrated with existing triggers. To further improve\nthe effectiveness of BadSR, we design an adversarially optimized trigger and a\nbackdoor gradient-driven poisoned sample selection method based on a genetic\nalgorithm. The experimental results show that BadSR achieves a high attack\nsuccess rate in various models and data sets, significantly affecting\ndownstream tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86BadSR\uff0c\u4e00\u79cd\u9488\u5bf9\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u7684\u6539\u8fdb\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u8fd1\u4f3c\u6e05\u6d01\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u9884\u5b9a\u4e49\u76ee\u6807\u56fe\u50cf\uff0c\u540c\u65f6\u9650\u5236\u5bf9\u6e05\u6d01\u56fe\u50cf\u7684\u4fee\u6539\u8303\u56f4\uff0c\u63d0\u9ad8\u4e86\u4e2d\u6bd2\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u9690\u853d\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cBadSR\u5728\u591a\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u4ee5\u5f80\u7684\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u540e\u95e8\u653b\u51fb\u4e3b\u8981\u5173\u6ce8\u4e2d\u6bd2\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u9690\u853d\u6027\uff0c\u5ffd\u7565\u4e86\u4e2d\u6bd2\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u9690\u853d\u6027\uff0c\u5bb9\u6613\u88ab\u7528\u6237\u5bdf\u89c9\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86BadSR\u3002", "method": "BadSR\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u8fd1\u4f3c\u6e05\u6d01\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u9884\u5b9a\u4e49\u76ee\u6807\u56fe\u50cf\uff0c\u5e76\u9650\u5236\u5bf9\u6e05\u6d01\u56fe\u50cf\u7684\u4fee\u6539\u8303\u56f4\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86\u5bf9\u6297\u4f18\u5316\u7684\u89e6\u53d1\u5668\u548c\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u7684\u540e\u95e8\u68af\u5ea6\u9a71\u52a8\u4e2d\u6bd2\u6837\u672c\u9009\u62e9\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cBadSR\u5728\u591a\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u5747\u5b9e\u73b0\u4e86\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff0c\u663e\u8457\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "BadSR\u901a\u8fc7\u6539\u8fdb\u4e2d\u6bd2\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u9690\u853d\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u540e\u95e8\u653b\u51fb\u7684\u6548\u679c\uff0c\u4e3a\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u7684\u5b89\u5168\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u8d85\u5206\u8fa8\u7387\uff0c\u540e\u95e8\u653b\u51fb\uff0c\u6570\u636e\u4e2d\u6bd2\uff0c\u9690\u853d\u6027\uff0cBadSR"}}
{"id": "2505.15792", "pdf": "https://arxiv.org/pdf/2505.15792", "abs": "https://arxiv.org/abs/2505.15792", "authors": ["Danna Zheng", "Mirella Lapata", "Jeff Z. Pan"], "title": "Long-Form Information Alignment Evaluation Beyond Atomic Facts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Information alignment evaluators are vital for various NLG evaluation tasks\nand trustworthy LLM deployment, reducing hallucinations and enhancing user\ntrust. Current fine-grained methods, like FactScore, verify facts individually\nbut neglect inter-fact dependencies, enabling subtle vulnerabilities. In this\nwork, we introduce MontageLie, a challenging benchmark that constructs\ndeceptive narratives by \"montaging\" truthful statements without introducing\nexplicit hallucinations. We demonstrate that both coarse-grained LLM-based\nevaluators and current fine-grained frameworks are susceptible to this attack,\nwith AUC-ROC scores falling below 65%. To enable more robust fine-grained\nevaluation, we propose DoveScore, a novel framework that jointly verifies\nfactual accuracy and event-order consistency. By modeling inter-fact\nrelationships, DoveScore outperforms existing fine-grained methods by over 8%,\nproviding a more robust solution for long-form text alignment evaluation. Our\ncode and datasets are available at https://github.com/dannalily/DoveScore.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5MontageLie\uff0c\u7528\u4e8e\u68c0\u6d4b\u6587\u672c\u751f\u6210\u8bc4\u4f30\u4e2d\u7684\u6b3a\u9a97\u6027\u53d9\u8ff0\uff0c\u5e76\u63d0\u51fa\u4e86DoveScore\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u9a8c\u8bc1\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u4e8b\u4ef6\u987a\u5e8f\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u4e86\u8bc4\u4f30\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982FactScore\uff09\u867d\u7136\u80fd\u9a8c\u8bc1\u5355\u4e2a\u4e8b\u5b9e\uff0c\u4f46\u5ffd\u7565\u4e86\u4e8b\u5b9e\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u6f5c\u5728\u7684\u6f0f\u6d1e\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faMontageLie\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u901a\u8fc7DoveScore\u6846\u67b6\u8054\u5408\u9a8c\u8bc1\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u4e8b\u4ef6\u987a\u5e8f\u4e00\u81f4\u6027\uff0c\u5efa\u6a21\u4e8b\u5b9e\u95f4\u5173\u7cfb\u3002", "result": "DoveScore\u5728AUC-ROC\u4e0a\u6bd4\u73b0\u6709\u7ec6\u7c92\u5ea6\u65b9\u6cd5\u63d0\u5347\u4e868%\u4ee5\u4e0a\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "DoveScore\u901a\u8fc7\u5efa\u6a21\u4e8b\u5b9e\u95f4\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u6587\u672c\u751f\u6210\u8bc4\u4f30\u7684\u9c81\u68d2\u6027\u3002", "keywords": "\u4fe1\u606f\u5bf9\u9f50\u8bc4\u4f30,LLM\u90e8\u7f72,FactScore,MontageLie,DoveScore,\u4e8b\u4ef6\u987a\u5e8f\u4e00\u81f4\u6027"}}
{"id": "2505.15802", "pdf": "https://arxiv.org/pdf/2505.15802", "abs": "https://arxiv.org/abs/2505.15802", "authors": ["Sarah E. Wessinger", "Leslie N. Smith", "Jacob Gull", "Jonathan Gehman", "Zachary Beever", "Andrew J. Kammerer"], "title": "A Deep Learning Framework for Two-Dimensional, Multi-Frequency Propagation Factor Estimation", "categories": ["cs.LG", "eess.SP", "physics.ao-ph"], "comment": "Submitted for publication", "summary": "Accurately estimating the refractive environment over multiple frequencies\nwithin the marine atmospheric boundary layer is crucial for the effective\ndeployment of radar technologies. Traditional parabolic equation simulations,\nwhile effective, can be computationally expensive and time-intensive, limiting\ntheir practical application. This communication explores a novel approach using\ndeep neural networks to estimate the pattern propagation factor, a critical\nparameter for characterizing environmental impacts on signal propagation.\nImage-to-image translation generators designed to ingest modified refractivity\ndata and generate predictions of pattern propagation factors over the same\ndomain were developed. Findings demonstrate that deep neural networks can be\ntrained to analyze multiple frequencies and reasonably predict the pattern\npropagation factor, offering an alternative to traditional methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6d77\u6d0b\u5927\u6c14\u8fb9\u754c\u5c42\u4e2d\u591a\u9891\u7387\u6298\u5c04\u73af\u5883\u4f30\u8ba1\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4e86\u4f20\u7edf\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u629b\u7269\u7ebf\u65b9\u7a0b\u6a21\u62df\u3002", "motivation": "\u51c6\u786e\u4f30\u8ba1\u6d77\u6d0b\u5927\u6c14\u8fb9\u754c\u5c42\u4e2d\u591a\u9891\u7387\u7684\u6298\u5c04\u73af\u5883\u5bf9\u96f7\u8fbe\u6280\u672f\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u3002", "method": "\u91c7\u7528\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u751f\u6210\u5668\uff0c\u8f93\u5165\u4fee\u6b63\u6298\u5c04\u7387\u6570\u636e\uff0c\u751f\u6210\u9884\u6d4b\u6a21\u5f0f\u4f20\u64ad\u56e0\u5b50\u3002", "result": "\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u53ef\u4ee5\u5206\u6790\u591a\u9891\u7387\u5e76\u5408\u7406\u9884\u6d4b\u6a21\u5f0f\u4f20\u64ad\u56e0\u5b50\uff0c\u63d0\u4f9b\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u9884\u6d4b\u6a21\u5f0f\u4f20\u64ad\u56e0\u5b50\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u4f5c\u4e3a\u4f20\u7edf\u65b9\u6cd5\u7684\u6709\u6548\u8865\u5145\u3002", "keywords": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u6a21\u5f0f\u4f20\u64ad\u56e0\u5b50\uff0c\u6d77\u6d0b\u5927\u6c14\u8fb9\u754c\u5c42\uff0c\u591a\u9891\u7387\uff0c\u629b\u7269\u7ebf\u65b9\u7a0b"}}
{"id": "2505.15795", "pdf": "https://arxiv.org/pdf/2505.15795", "abs": "https://arxiv.org/abs/2505.15795", "authors": ["Lisa Alazraki", "Tan Yi-Chern", "Jon Ander Campos", "Maximilian Mozes", "Marek Rei", "Max Bartolo"], "title": "Reverse Engineering Human Preferences with Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "The capabilities of Large Language Models (LLMs) are routinely evaluated by\nother LLMs trained to predict human preferences. This framework--known as\nLLM-as-a-judge--is highly scalable and relatively low cost. However, it is also\nvulnerable to malicious exploitation, as LLM responses can be tuned to overfit\nthe preferences of the judge. Previous work shows that the answers generated by\na candidate-LLM can be edited post hoc to maximise the score assigned to them\nby a judge-LLM. In this study, we adopt a different approach and use the signal\nprovided by judge-LLMs as a reward to adversarially tune models that generate\ntext preambles designed to boost downstream performance. We find that frozen\nLLMs pipelined with these models attain higher LLM-evaluation scores than\nexisting frameworks. Crucially, unlike other frameworks which intervene\ndirectly on the model's response, our method is virtually undetectable. We also\ndemonstrate that the effectiveness of the tuned preamble generator transfers\nwhen the candidate-LLM and the judge-LLM are replaced with models that are not\nused during training. These findings raise important questions about the design\nof more reliable LLM-as-a-judge evaluation settings. They also demonstrate that\nhuman preferences can be reverse engineered effectively, by pipelining LLMs to\noptimise upstream preambles via reinforcement learning--an approach that could\nfind future applications in diverse tasks and domains beyond adversarial\nattacks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5bf9\u6297\u6027\u8c03\u6574\u751f\u6210\u6587\u672c\u524d\u5bfc\u7684\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728LLM\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\u4e14\u96be\u4ee5\u68c0\u6d4b\u3002", "motivation": "LLM\u8bc4\u4f30\u6846\u67b6\uff08LLM-as-a-judge\uff09\u6613\u53d7\u6076\u610f\u653b\u51fb\uff0c\u5019\u9009LLM\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u56de\u7b54\u6765\u8fce\u5408\u8bc4\u59d4LLM\u7684\u504f\u597d\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u66f4\u9690\u853d\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u524d\u5bfc\u6587\u672c\u6765\u63d0\u9ad8\u8bc4\u5206\u3002", "method": "\u4f7f\u7528\u8bc4\u59d4LLM\u63d0\u4f9b\u7684\u4fe1\u53f7\u4f5c\u4e3a\u5956\u52b1\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u8c03\u6574\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u4f18\u5316\u7684\u6587\u672c\u524d\u5bfc\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u4f18\u5316\u540e\u7684\u524d\u5bfc\u751f\u6210\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u8bc4\u5206\u6548\u679c\uff0c\u4e14\u8be5\u65b9\u6cd5\u5bf9\u4e0d\u540c\u8bc4\u59d4LLM\u548c\u5019\u9009LLM\u5177\u6709\u8fc1\u79fb\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u8bc4\u4f30\u6846\u67b6\u7684\u6f5c\u5728\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u853d\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u4eba\u7c7b\u504f\u597d\u53ef\u4ee5\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u53cd\u5411\u5de5\u7a0b\u3002", "keywords": "LLM-as-a-judge, adversarial tuning, preamble generation, reinforcement learning, reverse engineering"}}
{"id": "2505.15803", "pdf": "https://arxiv.org/pdf/2505.15803", "abs": "https://arxiv.org/abs/2505.15803", "authors": ["Dheeraj Baby", "Yifei Tang", "Hieu Duy Nguyen", "Yu-Xiang Wang", "Rohit Pyati"], "title": "Adaptive Estimation and Learning under Temporal Distribution Shift", "categories": ["cs.LG"], "comment": "Accepted at ICML 2025", "summary": "In this paper, we study the problem of estimation and learning under temporal\ndistribution shift. Consider an observation sequence of length $n$, which is a\nnoisy realization of a time-varying groundtruth sequence. Our focus is to\ndevelop methods to estimate the groundtruth at the final time-step while\nproviding sharp point-wise estimation error rates. We show that, without prior\nknowledge on the level of temporal shift, a wavelet soft-thresholding estimator\nprovides an optimal estimation error bound for the groundtruth. Our proposed\nestimation method generalizes existing researches Mazzetto and Upfal (2023) by\nestablishing a connection between the sequence's non-stationarity level and the\nsparsity in the wavelet-transformed domain. Our theoretical findings are\nvalidated by numerical experiments. Additionally, we applied the estimator to\nderive sparsity-aware excess risk bounds for binary classification under\ndistribution shift and to develop computationally efficient training\nobjectives. As a final contribution, we draw parallels between our results and\nthe classical signal processing problem of total-variation denoising (Mammen\nand van de Geer,1997; Tibshirani, 2014), uncovering novel optimal algorithms\nfor such task.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u65f6\u95f4\u5206\u5e03\u6f02\u79fb\u4e0b\u7684\u4f30\u8ba1\u548c\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6700\u4f18\u7684\u5c0f\u6ce2\u8f6f\u9608\u503c\u4f30\u8ba1\u5668\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u548c\u7406\u8bba\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u89c2\u6d4b\u5e8f\u5217\u968f\u65f6\u95f4\u53d8\u5316\u4e14\u5b58\u5728\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u5728\u65e0\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u51c6\u786e\u4f30\u8ba1\u6700\u7ec8\u65f6\u95f4\u6b65\u7684\u771f\u5b9e\u503c\u3002", "method": "\u4f7f\u7528\u5c0f\u6ce2\u8f6f\u9608\u503c\u4f30\u8ba1\u5668\uff0c\u5c06\u5e8f\u5217\u7684\u975e\u5e73\u7a33\u6027\u4e0e\u5c0f\u6ce2\u53d8\u6362\u57df\u7684\u7a00\u758f\u6027\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6700\u4f18\u7684\u4f30\u8ba1\u8bef\u5dee\u754c\uff0c\u5e76\u5e94\u7528\u4e8e\u5206\u5e03\u6f02\u79fb\u4e0b\u7684\u5206\u7c7b\u98ce\u9669\u754c\u548c\u9ad8\u6548\u8bad\u7ec3\u76ee\u6807\u7684\u5f00\u53d1\u3002", "conclusion": "\u5c0f\u6ce2\u8f6f\u9608\u503c\u4f30\u8ba1\u5668\u5728\u65e0\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f18\uff0c\u5e76\u63ed\u793a\u4e86\u4e0e\u7ecf\u5178\u4fe1\u53f7\u5904\u7406\u95ee\u9898\u7684\u8054\u7cfb\u3002", "keywords": "\u65f6\u95f4\u5206\u5e03\u6f02\u79fb, \u5c0f\u6ce2\u8f6f\u9608\u503c, \u975e\u5e73\u7a33\u6027, \u7a00\u758f\u6027, \u6700\u4f18\u4f30\u8ba1"}}
{"id": "2505.15801", "pdf": "https://arxiv.org/pdf/2505.15801", "abs": "https://arxiv.org/abs/2505.15801", "authors": ["Yuchen Yan", "Jin Jiang", "Zhenbang Ren", "Yijun Li", "Xudong Cai", "Yang Liu", "Xin Xu", "Mengdi Zhang", "Jian Shao", "Yongliang Shen", "Jun Xiao", "Yueting Zhuang"], "title": "VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Dataset: https://huggingface.co/datasets/ZJU-REAL/VerifyBench", "summary": "Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved\nremarkable performance in the domain of reasoning. A key component of their\ntraining is the incorporation of verifiable rewards within reinforcement\nlearning (RL). However, existing reward benchmarks do not evaluate\nreference-based reward systems, leaving researchers with limited understanding\nof the accuracy of verifiers used in RL. In this paper, we introduce two\nbenchmarks, VerifyBench and VerifyBench-Hard, designed to assess the\nperformance of reference-based reward systems. These benchmarks are constructed\nthrough meticulous data collection and curation, followed by careful human\nannotation to ensure high quality. Current models still show considerable room\nfor improvement on both VerifyBench and VerifyBench-Hard, especially\nsmaller-scale models. Furthermore, we conduct a thorough and comprehensive\nanalysis of evaluation results, offering insights for understanding and\ndeveloping reference-based reward systems. Our proposed benchmarks serve as\neffective tools for guiding the development of verifier accuracy and the\nreasoning capabilities of models trained via RL in reasoning tasks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e24\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5VerifyBench\u548cVerifyBench-Hard\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u53c2\u8003\u7684\u5956\u52b1\u7cfb\u7edf\u6027\u80fd\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5956\u52b1\u57fa\u51c6\u7684\u7a7a\u767d\u3002\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u548c\u4eba\u5de5\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u4ecd\u6709\u8f83\u5927\u6539\u8fdb\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u5c0f\u89c4\u6a21\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u5956\u52b1\u57fa\u51c6\u672a\u80fd\u8bc4\u4f30\u57fa\u4e8e\u53c2\u8003\u7684\u5956\u52b1\u7cfb\u7edf\uff0c\u5bfc\u81f4\u7814\u7a76\u4eba\u5458\u5bf9\u5f3a\u5316\u5b66\u4e60\u4e2d\u9a8c\u8bc1\u5668\u7684\u51c6\u786e\u6027\u7f3a\u4e4f\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u6a21\u578b\u5f00\u53d1\u548c\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u7ec6\u81f4\u7684\u6570\u636e\u6536\u96c6\u3001\u6574\u7406\u548c\u4eba\u5de5\u6807\u6ce8\uff0c\u6784\u5efa\u4e86VerifyBench\u548cVerifyBench-Hard\u4e24\u4e2a\u9ad8\u8d28\u91cf\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u53c2\u8003\u7684\u5956\u52b1\u7cfb\u7edf\u3002", "result": "\u5f53\u524d\u6a21\u578b\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4ecd\u6709\u663e\u8457\u63d0\u5347\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u5c0f\u89c4\u6a21\u6a21\u578b\u3002\u6587\u7ae0\u8fd8\u63d0\u4f9b\u4e86\u5bf9\u8bc4\u4f30\u7ed3\u679c\u7684\u5168\u9762\u5206\u6790\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u89c1\u89e3\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u57fa\u4e8e\u53c2\u8003\u7684\u5956\u52b1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002", "keywords": "\u57fa\u51c6\u6d4b\u8bd5,\u5956\u52b1\u7cfb\u7edf,\u5f3a\u5316\u5b66\u4e60,\u63a8\u7406\u6a21\u578b,\u9a8c\u8bc1\u5668"}}
{"id": "2505.15808", "pdf": "https://arxiv.org/pdf/2505.15808", "abs": "https://arxiv.org/abs/2505.15808", "authors": ["Carlos Rodriguez-Pardo", "Leonardo Chiani", "Emanuele Borgonovo", "Massimo Tavoni"], "title": "Neural Conditional Transport Maps", "categories": ["cs.LG", "cs.AI", "math.PR", "stat.AP", "stat.ML", "49Q22 (Primary) 68T07 (Secondary)", "I.5.1; I.2.0; G.3"], "comment": "Under Review. Supplementary material included in the pdf", "summary": "We present a neural framework for learning conditional optimal transport (OT)\nmaps between probability distributions. Our approach introduces a conditioning\nmechanism capable of processing both categorical and continuous conditioning\nvariables simultaneously. At the core of our method lies a hypernetwork that\ngenerates transport layer parameters based on these inputs, creating adaptive\nmappings that outperform simpler conditioning methods. Comprehensive ablation\nstudies demonstrate the superior performance of our method over baseline\nconfigurations. Furthermore, we showcase an application to global sensitivity\nanalysis, offering high performance in computing OT-based sensitivity indices.\nThis work advances the state-of-the-art in conditional optimal transport,\nenabling broader application of optimal transport principles to complex,\nhigh-dimensional domains such as generative modeling and black-box model\nexplainability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u6761\u4ef6\u6700\u4f18\u4f20\u8f93\u6620\u5c04\u7684\u795e\u7ecf\u6846\u67b6\uff0c\u652f\u6301\u5904\u7406\u5206\u7c7b\u548c\u8fde\u7eed\u6761\u4ef6\u53d8\u91cf\uff0c\u901a\u8fc7\u8d85\u7f51\u7edc\u751f\u6210\u4f20\u8f93\u5c42\u53c2\u6570\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u6269\u5c55\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u9ad8\u7ef4\u9886\u57df\uff0c\u5982\u751f\u6210\u5efa\u6a21\u548c\u9ed1\u76d2\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u8d85\u7f51\u7edc\u751f\u6210\u57fa\u4e8e\u6761\u4ef6\u53d8\u91cf\u7684\u4f20\u8f93\u5c42\u53c2\u6570\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u6620\u5c04\u3002", "result": "\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5e76\u5728\u5168\u5c40\u654f\u611f\u6027\u5206\u6790\u4e2d\u5c55\u793a\u4e86\u9ad8\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63a8\u52a8\u4e86\u6761\u4ef6\u6700\u4f18\u4f20\u8f93\u7684\u524d\u6cbf\uff0c\u62d3\u5bbd\u4e86\u6700\u4f18\u4f20\u8f93\u539f\u5219\u7684\u5e94\u7528\u8303\u56f4\u3002", "keywords": "\u6761\u4ef6\u6700\u4f18\u4f20\u8f93, \u8d85\u7f51\u7edc, \u5168\u5c40\u654f\u611f\u6027\u5206\u6790, \u751f\u6210\u5efa\u6a21, \u6a21\u578b\u53ef\u89e3\u91ca\u6027"}}
{"id": "2505.15805", "pdf": "https://arxiv.org/pdf/2505.15805", "abs": "https://arxiv.org/abs/2505.15805", "authors": ["Hwan Chang", "Yumin Kim", "Yonghyun Jun", "Hwanhee Lee"], "title": "Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains such as enterprise and government, ensuring that they adhere to\nuser-defined security policies within context is critical-especially with\nrespect to information non-disclosure. While prior LLM studies have focused on\ngeneral safety and socially sensitive data, large-scale benchmarks for\ncontextual security preservation against attacks remain lacking. To address\nthis, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating\nLLM adherence to contextual non-disclosure policies in question answering.\nDerived from realistic contexts, our dataset includes explicit policies and\nqueries designed as direct and challenging indirect attacks seeking prohibited\ninformation. We evaluate 10 LLMs on our benchmark and reveal a significant\nvulnerability: many models violate user-defined policies and leak sensitive\ninformation. This failure is particularly severe against indirect attacks,\nhighlighting a critical gap in current LLM safety alignment for sensitive\napplications. Our analysis reveals that while models can often identify the\ncorrect answer to a query, they struggle to incorporate policy constraints\nduring generation. In contrast, they exhibit a partial ability to revise\noutputs when explicitly prompted. Our findings underscore the urgent need for\nmore robust methods to guarantee contextual security.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCoPriva\u7684\u65b0\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc4\u4f30LLM\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u9075\u5b88\u4e0a\u4e0b\u6587\u975e\u62ab\u9732\u653f\u7b56\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5176\u5728\u654f\u611f\u4fe1\u606f\u4fdd\u62a4\u4e0a\u7684\u6f0f\u6d1e\u3002", "motivation": "\u9488\u5bf9LLM\u5728\u654f\u611f\u9886\u57df\u90e8\u7f72\u65f6\u786e\u4fdd\u9075\u5b88\u7528\u6237\u5b9a\u4e49\u7684\u5b89\u5168\u653f\u7b56\u7684\u9700\u6c42\uff0c\u5c24\u5176\u662f\u4fe1\u606f\u975e\u62ab\u9732\u65b9\u9762\u3002", "method": "\u5f15\u5165CoPriva\u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u653f\u7b56\u4e0e\u67e5\u8be2\uff0c\u6d4b\u8bd510\u79cdLLM\u7684\u6027\u80fd\u3002", "result": "\u53d1\u73b0\u8bb8\u591a\u6a21\u578b\u4f1a\u8fdd\u53cd\u653f\u7b56\u6cc4\u9732\u654f\u611f\u4fe1\u606f\uff0c\u5c24\u5176\u662f\u9762\u5bf9\u95f4\u63a5\u653b\u51fb\u65f6\u8868\u73b0\u66f4\u5dee\u3002", "conclusion": "\u5f53\u524dLLM\u7684\u5b89\u5168\u5bf9\u9f50\u5728\u654f\u611f\u5e94\u7528\u4e2d\u5b58\u5728\u91cd\u5927\u7f3a\u9677\uff0c\u4e9f\u9700\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "keywords": "LLM, \u5b89\u5168\u653f\u7b56, \u4fe1\u606f\u975e\u62ab\u9732, CoPriva"}}
{"id": "2505.15811", "pdf": "https://arxiv.org/pdf/2505.15811", "abs": "https://arxiv.org/abs/2505.15811", "authors": ["Eric J. Michaud", "Asher Parker-Sartori", "Max Tegmark"], "title": "On the creation of narrow AI: hierarchy and nonlocality of neural network skills", "categories": ["cs.LG"], "comment": "19 pages, 13 figures", "summary": "We study the problem of creating strong, yet narrow, AI systems. While recent\nAI progress has been driven by the training of large general-purpose foundation\nmodels, the creation of smaller models specialized for narrow domains could be\nvaluable for both efficiency and safety. In this work, we explore two\nchallenges involved in creating such systems, having to do with basic\nproperties of how neural networks learn and structure their representations.\nThe first challenge regards when it is possible to train narrow models from\nscratch. Through experiments on a synthetic task, we find that it is sometimes\nnecessary to train networks on a wide distribution of data to learn certain\nnarrow skills within that distribution. This effect arises when skills depend\non each other hierarchically, and training on a broad distribution introduces a\ncurriculum which substantially accelerates learning. The second challenge\nregards how to transfer particular skills from large general models into small\nspecialized models. We find that model skills are often not perfectly localized\nto a particular set of prunable components. However, we find that methods based\non pruning can still outperform distillation. We investigate the use of a\nregularization objective to align desired skills with prunable components while\nunlearning unnecessary skills.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u521b\u5efa\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u7a84AI\u7cfb\u7edf\uff0c\u7814\u7a76\u4e86\u4ece\u96f6\u8bad\u7ec3\u7a84\u6a21\u578b\u7684\u6761\u4ef6\u548c\u4ece\u5927\u6a21\u578b\u8f6c\u79fb\u7279\u5b9a\u6280\u80fd\u5230\u5c0f\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u7a84AI\u7cfb\u7edf\u7684\u4ef7\u503c\u5728\u4e8e\u63d0\u9ad8\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u5c24\u5176\u662f\u5728\u8bad\u7ec3\u5927\u578b\u901a\u7528\u6a21\u578b\u7684\u80cc\u666f\u4e0b\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e86\u7a84\u6a21\u578b\u4ece\u96f6\u8bad\u7ec3\u7684\u53ef\u884c\u6027\uff0c\u5e76\u7814\u7a76\u4e86\u901a\u8fc7\u526a\u679d\u65b9\u6cd5\u4ece\u5927\u6a21\u578b\u8f6c\u79fb\u6280\u80fd\u5230\u5c0f\u6a21\u578b\u7684\u6280\u672f\u3002", "result": "\u53d1\u73b0\u8bad\u7ec3\u7a84\u6a21\u578b\u65f6\u6709\u65f6\u9700\u8981\u5e7f\u6cdb\u6570\u636e\u5206\u5e03\uff1b\u526a\u679d\u65b9\u6cd5\u5728\u6280\u80fd\u8f6c\u79fb\u4e2d\u8868\u73b0\u4f18\u4e8e\u84b8\u998f\u3002", "conclusion": "\u7a84AI\u7cfb\u7edf\u7684\u521b\u5efa\u9700\u89e3\u51b3\u6280\u80fd\u4f9d\u8d56\u6027\u548c\u6280\u80fd\u5c40\u90e8\u5316\u7684\u6311\u6218\uff0c\u526a\u679d\u548c\u6b63\u5219\u5316\u662f\u6709\u6548\u65b9\u6cd5\u3002", "keywords": "\u7a84AI\u7cfb\u7edf, \u795e\u7ecf\u7f51\u7edc, \u526a\u679d, \u6280\u80fd\u8f6c\u79fb, \u6570\u636e\u5206\u5e03"}}
{"id": "2505.15344", "pdf": "https://arxiv.org/pdf/2505.15344", "abs": "https://arxiv.org/abs/2505.15344", "authors": ["Faruk Alpay"], "title": "Alpay Algebra: A Universal Structural Foundation", "categories": ["cs.LO", "cs.AI", "math.CT", "18B99, 68T27", "F.4.1; I.2.3"], "comment": "37 pages, 0 figures. Self-contained categorical framework built\n  directly on Mac Lane and Bourbaki; minimal references are intentional to\n  foreground the new construction", "summary": "Alpay Algebra is introduced as a universal, category-theoretic framework that\nunifies classical algebraic structures with modern needs in symbolic recursion\nand explainable AI. Starting from a minimal list of axioms, we model each\nalgebra as an object in a small cartesian closed category $\\mathcal{A}$ and\ndefine a transfinite evolution functor $\\phi\\colon\\mathcal{A}\\to\\mathcal{A}$.\nWe prove that the fixed point $\\phi^{\\infty}$ exists for every initial object\nand satisfies an internal universal property that recovers familiar constructs\n-- limits, colimits, adjunctions -- while extending them to ordinal-indexed\nfolds. A sequence of theorems establishes (i) soundness and conservativity over\nstandard universal algebra, (ii) convergence of $\\phi$-iterates under regular\ncardinals, and (iii) an explanatory correspondence between $\\phi^{\\infty}$ and\nminimal sufficient statistics in information-theoretic AI models. We conclude\nby outlining computational applications: type-safe functional languages,\ncategorical model checking, and signal-level reasoning engines that leverage\nAlpay Algebra's structural invariants. All proofs are self-contained; no\nexternal set-theoretic axioms beyond ZFC are required. This exposition\npositions Alpay Algebra as a bridge between foundational mathematics and\nhigh-impact AI systems, and provides a reference for further work in category\ntheory, transfinite fixed-point analysis, and symbolic computation.", "AI": {"tldr": "Alpay Algebra\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u8303\u7574\u8bba\u6846\u67b6\uff0c\u7ed3\u5408\u7ecf\u5178\u4ee3\u6570\u7ed3\u6784\u4e0e\u73b0\u4ee3\u7b26\u53f7\u9012\u5f52\u548c\u53ef\u89e3\u91caAI\u9700\u6c42\uff0c\u901a\u8fc7\u6700\u5c0f\u516c\u7406\u7cfb\u7edf\u5b9a\u4e49\uff0c\u8bc1\u660e\u5176\u4e0d\u52a8\u70b9\u5b58\u5728\u6027\uff0c\u5e76\u5728AI\u6a21\u578b\u4e2d\u5c55\u793a\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u4f5c\u8005\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u7edf\u4e00\u7684\u6570\u5b66\u6846\u67b6\uff0c\u8fde\u63a5\u7ecf\u5178\u4ee3\u6570\u7ed3\u6784\u4e0e\u73b0\u4ee3AI\u9700\u6c42\uff0c\u5c24\u5176\u5728\u7b26\u53f7\u9012\u5f52\u548c\u53ef\u89e3\u91caAI\u9886\u57df\u3002", "method": "\u901a\u8fc7\u8303\u7574\u8bba\u65b9\u6cd5\uff0c\u4ece\u6700\u5c0f\u516c\u7406\u51fa\u53d1\u5b9a\u4e49Alpay Algebra\uff0c\u5e76\u5f15\u5165\u6f14\u5316\u51fd\u5b50\u03c6\uff0c\u8bc1\u660e\u5176\u4e0d\u52a8\u70b9\u03c6^\u221e\u7684\u5b58\u5728\u6027\u53ca\u5176\u666e\u904d\u6027\u8d28\u3002", "result": "\u8bc1\u660e\u4e86\u03c6^\u221e\u7684\u5b58\u5728\u6027\u53ca\u5176\u5728\u6807\u51c6\u901a\u7528\u4ee3\u6570\u4e2d\u7684\u4fdd\u5b88\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728AI\u6a21\u578b\u4e2d\u7684\u4fe1\u606f\u8bba\u5bf9\u5e94\u5173\u7cfb\u3002", "conclusion": "Alpay Algebra\u6210\u4e3a\u57fa\u7840\u6570\u5b66\u4e0e\u9ad8\u5f71\u54cdAI\u7cfb\u7edf\u7684\u6865\u6881\uff0c\u4e3a\u8303\u7574\u8bba\u3001\u7b26\u53f7\u8ba1\u7b97\u7b49\u9886\u57df\u63d0\u4f9b\u53c2\u8003\u3002", "keywords": "Alpay Algebra, \u8303\u7574\u8bba, \u7b26\u53f7\u9012\u5f52, \u53ef\u89e3\u91caAI, \u4e0d\u52a8\u70b9\u5206\u6790"}}
{"id": "2505.15807", "pdf": "https://arxiv.org/pdf/2505.15807", "abs": "https://arxiv.org/abs/2505.15807", "authors": ["Patrick Kahardipraja", "Reduan Achtibat", "Thomas Wiegand", "Wojciech Samek", "Sebastian Lapuschkin"], "title": "The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "work in progress", "summary": "Large language models are able to exploit in-context learning to access\nexternal knowledge beyond their training data through retrieval-augmentation.\nWhile promising, its inner workings remain unclear. In this work, we shed light\non the mechanism of in-context retrieval augmentation for question answering by\nviewing a prompt as a composition of informational components. We propose an\nattribution-based method to identify specialized attention heads, revealing\nin-context heads that comprehend instructions and retrieve relevant contextual\ninformation, and parametric heads that store entities' relational knowledge. To\nbetter understand their roles, we extract function vectors and modify their\nattention weights to show how they can influence the answer generation process.\nFinally, we leverage the gained insights to trace the sources of knowledge used\nduring inference, paving the way towards more safe and transparent language\nmodels.", "AI": {"tldr": "\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u4e0a\u4e0b\u6587\u68c0\u7d22\u589e\u5f3a\u5b66\u4e60\u5916\u90e8\u77e5\u8bc6\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u5f52\u56e0\u65b9\u6cd5\u63ed\u793a\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u7684\u4f5c\u7528\uff0c\u6700\u540e\u901a\u8fc7\u8c03\u6574\u6ce8\u610f\u529b\u6743\u91cd\u5c55\u793a\u4e86\u77e5\u8bc6\u6765\u6e90\u7684\u8ffd\u8e2a\u3002", "motivation": "\u5c3d\u7ba1\u4e0a\u4e0b\u6587\u68c0\u7d22\u589e\u5f3a\u5b66\u4e60\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5185\u90e8\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5176\u5de5\u4f5c\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5c06\u63d0\u793a\u89c6\u4e3a\u4fe1\u606f\u7ec4\u4ef6\u7684\u7ec4\u5408\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5f52\u56e0\u65b9\u6cd5\uff0c\u8bc6\u522b\u4e86\u4e13\u95e8\u7528\u4e8e\u7406\u89e3\u6307\u4ee4\u548c\u68c0\u7d22\u4fe1\u606f\u7684\u6ce8\u610f\u529b\u5934\uff0c\u4ee5\u53ca\u5b58\u50a8\u5b9e\u4f53\u5173\u7cfb\u77e5\u8bc6\u7684\u53c2\u6570\u5934\u3002\u5e76\u63d0\u53d6\u529f\u80fd\u5411\u91cf\uff0c\u8c03\u6574\u6ce8\u610f\u529b\u6743\u91cd\u4ee5\u5206\u6790\u5176\u5bf9\u7b54\u6848\u751f\u6210\u7684\u5f71\u54cd\u3002", "result": "\u63ed\u793a\u4e86\u4e0a\u4e0b\u6587\u68c0\u7d22\u589e\u5f3a\u5b66\u4e60\u7684\u673a\u5236\uff0c\u5c55\u793a\u4e86\u6ce8\u610f\u529b\u5934\u5982\u4f55\u5f71\u54cd\u77e5\u8bc6\u68c0\u7d22\u548c\u7b54\u6848\u751f\u6210\uff0c\u4e3a\u66f4\u5b89\u5168\u548c\u900f\u660e\u7684\u8bed\u8a00\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u5934\u7684\u4f5c\u7528\uff0c\u4e3a\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u63a8\u52a8\u4e86\u6a21\u578b\u5b89\u5168\u6027\u548c\u900f\u660e\u6027\u7684\u53d1\u5c55\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u4e0a\u4e0b\u6587\u5b66\u4e60, \u68c0\u7d22\u589e\u5f3a, \u6ce8\u610f\u529b\u673a\u5236, \u95ee\u7b54\u7cfb\u7edf"}}
{"id": "2505.15813", "pdf": "https://arxiv.org/pdf/2505.15813", "abs": "https://arxiv.org/abs/2505.15813", "authors": ["Muquan Yu", "Mu Nan", "Hossein Adeli", "Jacob S. Prince", "John A. Pyles", "Leila Wehbe", "Margaret M. Henderson", "Michael J. Tarr", "Andrew F. Luo"], "title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex", "categories": ["cs.LG", "q-bio.NC"], "comment": null, "summary": "Understanding functional representations within higher visual cortex is a\nfundamental question in computational neuroscience. While artificial neural\nnetworks pretrained on large-scale datasets exhibit striking representational\nalignment with human neural responses, learning image-computable models of\nvisual cortex relies on individual-level, large-scale fMRI datasets. The\nnecessity for expensive, time-intensive, and often impractical data acquisition\nlimits the generalizability of encoders to new subjects and stimuli. BraInCoRL\nuses in-context learning to predict voxelwise neural responses from few-shot\nexamples without any additional finetuning for novel subjects and stimuli. We\nleverage a transformer architecture that can flexibly condition on a variable\nnumber of in-context image stimuli, learning an inductive bias over multiple\nsubjects. During training, we explicitly optimize the model for in-context\nlearning. By jointly conditioning on image features and voxel activations, our\nmodel learns to directly generate better performing voxelwise models of higher\nvisual cortex. We demonstrate that BraInCoRL consistently outperforms existing\nvoxelwise encoder designs in a low-data regime when evaluated on entirely novel\nimages, while also exhibiting strong test-time scaling behavior. The model also\ngeneralizes to an entirely new visual fMRI dataset, which uses different\nsubjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates\nbetter interpretability of neural signals in higher visual cortex by attending\nto semantically relevant stimuli. Finally, we show that our framework enables\ninterpretable mappings from natural language queries to voxel selectivity.", "AI": {"tldr": "BraInCoRL\u4f7f\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u4ece\u5c0f\u6837\u672c\u4e2d\u9884\u6d4b\u795e\u7ecf\u53cd\u5e94\uff0c\u65e0\u9700\u5bf9\u65b0\u53d7\u8bd5\u8005\u548c\u523a\u6fc0\u8fdb\u884c\u5fae\u8c03\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u9ad8\u4e86\u795e\u7ecf\u4fe1\u53f7\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21fMRI\u6570\u636e\u96c6\u7684\u9650\u5236\uff0c\u63d0\u5347\u6a21\u578b\u5728\u65b0\u53d7\u8bd5\u8005\u548c\u523a\u6fc0\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528transformer\u67b6\u6784\uff0c\u7ed3\u5408\u56fe\u50cf\u7279\u5f81\u548cvoxel\u6fc0\u6d3b\u8fdb\u884c\u8054\u5408\u4f18\u5316\uff0c\u652f\u6301\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002", "result": "\u5728\u4f4e\u6570\u636e\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u8d8a\uff0c\u9002\u7528\u4e8e\u65b0\u56fe\u50cf\u548c\u65b0fMRI\u6570\u636e\u96c6\uff0c\u5e76\u80fd\u6620\u5c04\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5230voxel\u9009\u62e9\u6027\u3002", "conclusion": "BraInCoRL\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u76ae\u5c42\u7f16\u7801\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4f4e\u6570\u636e\u573a\u666f\u3002", "keywords": "\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u3001\u89c6\u89c9\u76ae\u5c42\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001transformer\u3001fMRI"}}
{"id": "2505.15810", "pdf": "https://arxiv.org/pdf/2505.15810", "abs": "https://arxiv.org/abs/2505.15810", "authors": ["Yuqi Zhou", "Sunhao Dai", "Shuai Wang", "Kaiwen Zhou", "Qinqlin Jia", "Junxu"], "title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,\ncoupling online Reinforcement Learning (RL) with explicit chain-of-thought\nreasoning prior to object grounding and thereby achieving substantial\nperformance gains. In this paper, we first conduct extensive analysis\nexperiments of three key components of that training pipeline: input design,\noutput evaluation, and policy update-each revealing distinct challenges arising\nfrom blindly applying general-purpose RL without adapting to GUI grounding\ntasks. Input design: Current templates encourage the model to generate\nchain-of-thought reasoning, but longer chains unexpectedly lead to worse\ngrounding performance. Output evaluation: Reward functions based on hit signals\nor box area allow models to exploit box size, leading to reward hacking and\npoor localization quality. Policy update: Online RL tends to overfit easy\nexamples due to biases in length and sample difficulty, leading to\nunder-optimization on harder cases. To address these issues, we propose three\ntargeted solutions. First, we adopt a Fast Thinking Template that encourages\ndirect answer generation, reducing excessive reasoning during training. Second,\nwe incorporate a box size constraint into the reward function to mitigate\nreward hacking. Third, we revise the RL objective by adjusting length\nnormalization and adding a difficulty-aware scaling factor, enabling better\noptimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with\nQwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on\nScreenSpot-Pro. This surpasses all prior models of similar size and even\noutperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI\nagent grounding. The project repository is available at\nhttps://github.com/Yuqi-Zhou/GUI-G1.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86GUI\u4ee3\u7406\u8bad\u7ec3\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u9488\u5bf9\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u6700\u7ec8\u5728GUI\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u53d6\u5f97\u65b0\u7a81\u7834\u3002", "motivation": "\u63ed\u793a\u76f2\u76ee\u5e94\u7528\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u4e8eGUI\u5b9a\u4f4d\u4efb\u52a1\u65f6\u5b58\u5728\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5feb\u901f\u601d\u8003\u6a21\u677f\u3001\u6539\u8fdb\u5956\u52b1\u51fd\u6570\u548c\u8c03\u6574RL\u76ee\u6807\uff0c\u4ee5\u4f18\u5316\u8bad\u7ec3\u6548\u679c\u3002", "result": "GUI-G1-3B\u6a21\u578b\u5728ScreenSpot\u548cScreenSpot-Pro\u4e0a\u5206\u522b\u8fbe\u523090.3%\u548c37.1%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u540c\u7c7b\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u6027\u6539\u8fdb\uff0c\u6a21\u578b\u5728GUI\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u4f73\u6027\u80fd\u3002", "keywords": "GUI\u4ee3\u7406,\u5f3a\u5316\u5b66\u4e60,\u5956\u52b1\u51fd\u6570,\u5b9a\u4f4d\u4efb\u52a1"}}
{"id": "2505.14692", "pdf": "https://arxiv.org/pdf/2505.14692", "abs": "https://arxiv.org/abs/2505.14692", "authors": ["KM Khalid Saifullah", "Faiaz Azmain", "Habiba Hye"], "title": "Sentiment Analysis in Software Engineering: Evaluating Generative Pre-trained Transformers", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": null, "summary": "Sentiment analysis plays a crucial role in understanding developer\ninteractions, issue resolutions, and project dynamics within software\nengineering (SE). While traditional SE-specific sentiment analysis tools have\nmade significant strides, they often fail to account for the nuanced and\ncontext-dependent language inherent to the domain. This study systematically\nevaluates the performance of bidirectional transformers, such as BERT, against\ngenerative pre-trained transformers, specifically GPT-4o-mini, in SE sentiment\nanalysis. Using datasets from GitHub, Stack Overflow, and Jira, we benchmark\nthe models' capabilities with fine-tuned and default configurations. The\nresults reveal that fine-tuned GPT-4o-mini performs comparable to BERT and\nother bidirectional models on structured and balanced datasets like GitHub and\nJira, achieving macro-averaged F1-scores of 0.93 and 0.98, respectively.\nHowever, on linguistically complex datasets with imbalanced sentiment\ndistributions, such as Stack Overflow, the default GPT-4o-mini model exhibits\nsuperior generalization, achieving an accuracy of 85.3\\% compared to the\nfine-tuned model's 13.1\\%. These findings highlight the trade-offs between\nfine-tuning and leveraging pre-trained models for SE tasks. The study\nunderscores the importance of aligning model architectures with dataset\ncharacteristics to optimize performance and proposes directions for future\nresearch in refining sentiment analysis tools tailored to the SE domain.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86BERT\u548cGPT-4o-mini\u5728\u8f6f\u4ef6\u5de5\u7a0b\u60c5\u611f\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4o-mini\u5728\u590d\u6742\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u800cBERT\u5728\u7ed3\u6784\u5316\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u60c5\u611f\u5206\u6790\u5de5\u5177\u672a\u80fd\u5145\u5206\u5904\u7406\u9886\u57df\u5185\u590d\u6742\u8bed\u5883\uff0c\u9700\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u63d0\u5347\u5206\u6790\u6548\u679c\u3002", "method": "\u4f7f\u7528GitHub\u3001Stack Overflow\u548cJira\u6570\u636e\u96c6\uff0c\u5bf9BERT\u548cGPT-4o-mini\u8fdb\u884c\u5fae\u8c03\u548c\u9ed8\u8ba4\u914d\u7f6e\u7684\u5bf9\u6bd4\u6d4b\u8bd5\u3002", "result": "\u5fae\u8c03\u540e\u7684GPT-4o-mini\u5728GitHub\u548cJira\u4e0a\u8868\u73b0\u4e0eBERT\u76f8\u5f53\uff0c\u4f46\u5728Stack Overflow\u4e0a\u9ed8\u8ba4\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff08\u51c6\u786e\u738785.3%\uff09\u3002", "conclusion": "\u6a21\u578b\u6027\u80fd\u4e0e\u6570\u636e\u96c6\u7279\u6027\u76f8\u5173\uff0c\u672a\u6765\u7814\u7a76\u9700\u4f18\u5316\u6a21\u578b\u4ee5\u9002\u914d\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u9700\u6c42\u3002", "keywords": "\u60c5\u611f\u5206\u6790, \u8f6f\u4ef6\u5de5\u7a0b, BERT, GPT-4o-mini, \u5fae\u8c03"}}
{"id": "2505.15358", "pdf": "https://arxiv.org/pdf/2505.15358", "abs": "https://arxiv.org/abs/2505.15358", "authors": ["Angelique Mangubat", "Shane Gilroy"], "title": "Objective Bicycle Occlusion Level Classification using a Deformable Parts-Based Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Road safety is a critical challenge, particularly for cyclists, who are among\nthe most vulnerable road users. This study aims to enhance road safety by\nproposing a novel benchmark for bicycle occlusion level classification using\nadvanced computer vision techniques. Utilizing a parts-based detection model,\nimages are annotated and processed through a custom image detection pipeline. A\nnovel method of bicycle occlusion level is proposed to objectively quantify the\nvisibility and occlusion level of bicycle semantic parts. The findings indicate\nthat the model robustly quantifies the visibility and occlusion level of\nbicycles, a significant improvement over the subjective methods used by the\ncurrent state of the art. Widespread use of the proposed methodology will\nfacilitate the accurate performance reporting of cyclist detection algorithms\nfor occluded cyclists, informing the development of more robust vulnerable road\nuser detection methods for autonomous vehicles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u7684\u81ea\u884c\u8f66\u906e\u6321\u7b49\u7ea7\u5206\u7c7b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u90e8\u5206\u68c0\u6d4b\u6a21\u578b\u548c\u81ea\u5b9a\u4e49\u56fe\u50cf\u68c0\u6d4b\u6d41\u7a0b\uff0c\u91cf\u5316\u4e86\u81ea\u884c\u8f66\u8bed\u4e49\u90e8\u4ef6\u7684\u53ef\u89c1\u6027\u548c\u906e\u6321\u7a0b\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f53\u524d\u4e3b\u89c2\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u63d0\u9ad8\u9053\u8def\u5b89\u5168\uff0c\u5c24\u5176\u662f\u5bf9\u6613\u53d7\u4f24\u5bb3\u7684\u9a91\u884c\u8005\uff0c\u901a\u8fc7\u91cf\u5316\u81ea\u884c\u8f66\u906e\u6321\u7a0b\u5ea6\u6765\u6539\u8fdb\u81ea\u884c\u8f66\u68c0\u6d4b\u7b97\u6cd5\u7684\u6027\u80fd\u62a5\u544a\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u90e8\u5206\u7684\u68c0\u6d4b\u6a21\u578b\u548c\u81ea\u5b9a\u4e49\u56fe\u50cf\u68c0\u6d4b\u6d41\u7a0b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u7528\u4e8e\u91cf\u5316\u81ea\u884c\u8f66\u8bed\u4e49\u90e8\u4ef6\u7684\u53ef\u89c1\u6027\u548c\u906e\u6321\u6c34\u5e73\u3002", "result": "\u6a21\u578b\u80fd\u591f\u7a33\u5065\u5730\u91cf\u5316\u81ea\u884c\u8f66\u7684\u53ef\u89c1\u6027\u548c\u906e\u6321\u6c34\u5e73\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e3b\u89c2\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u6709\u52a9\u4e8e\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e2d\u6613\u53d7\u4f24\u5bb3\u9053\u8def\u4f7f\u7528\u8005\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "keywords": "\u9053\u8def\u5b89\u5168, \u81ea\u884c\u8f66\u906e\u6321, \u8ba1\u7b97\u673a\u89c6\u89c9, \u90e8\u5206\u68c0\u6d4b\u6a21\u578b, \u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2505.15817", "pdf": "https://arxiv.org/pdf/2505.15817", "abs": "https://arxiv.org/abs/2505.15817", "authors": ["Tong Zheng", "Lichang Chen", "Simeng Han", "R. Thomas McCoy", "Heng Huang"], "title": "Learning to Reason via Mixture-of-Thought for Logical Reasoning", "categories": ["cs.CL"], "comment": "38 pages", "summary": "Human beings naturally utilize multiple reasoning modalities to learn and\nsolve logical problems, i.e., different representational formats such as\nnatural language, code, and symbolic logic. In contrast, most existing\nLLM-based approaches operate with a single reasoning modality during training,\ntypically natural language. Although some methods explored modality selection\nor augmentation at inference time, the training process remains modality-blind,\nlimiting synergy among modalities. To fill in this gap, we propose\nMixture-of-Thought (MoT), a framework that enables LLMs to reason across three\ncomplementary modalities: natural language, code, and a newly introduced\nsymbolic modality, truth-table, which systematically enumerates logical cases\nand partially mitigates key failure modes in natural language reasoning. MoT\nadopts a two-phase design: (1) self-evolving MoT training, which jointly learns\nfrom filtered, self-generated rationales across modalities; and (2) MoT\ninference, which fully leverages the synergy of three modalities to produce\nbetter predictions. Experiments on logical reasoning benchmarks including FOLIO\nand ProofWriter demonstrate that our MoT framework consistently and\nsignificantly outperforms strong LLM baselines with single-modality\nchain-of-thought approaches, achieving up to +11.7pp average accuracy gain.\nFurther analyses show that our MoT framework benefits both training and\ninference stages; that it is particularly effective on harder logical reasoning\nproblems; and that different modalities contribute complementary strengths,\nwith truth-table reasoning helping to overcome key bottlenecks in natural\nlanguage inference.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMixture-of-Thought (MoT)\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u3001\u4ee3\u7801\u548c\u7b26\u53f7\u903b\u8f91\uff08\u771f\u503c\u8868\uff09\u4e09\u79cd\u63a8\u7406\u6a21\u6001\uff0c\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u5e38\u4ec5\u4f7f\u7528\u5355\u4e00\u63a8\u7406\u6a21\u6001\uff08\u5982\u81ea\u7136\u8bed\u8a00\uff09\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u534f\u540c\u7684\u4f18\u52bf\u3002MoT\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "MoT\u91c7\u7528\u4e24\u9636\u6bb5\u8bbe\u8ba1\uff1a1\uff09\u81ea\u6f14\u5316\u7684MoT\u8bad\u7ec3\uff0c\u5b66\u4e60\u591a\u6a21\u6001\u81ea\u751f\u6210\u63a8\u7406\u4f9d\u636e\uff1b2\uff09MoT\u63a8\u7406\uff0c\u5145\u5206\u7ed3\u5408\u4e09\u79cd\u6a21\u6001\u7684\u534f\u540c\u6548\u5e94\u3002", "result": "\u5728FOLIO\u548cProofWriter\u7b49\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMoT\u6bd4\u5355\u6a21\u6001\u65b9\u6cd5\u5e73\u5747\u63d0\u534711.7%\u7684\u51c6\u786e\u7387\uff0c\u5c24\u5176\u5728\u590d\u6742\u95ee\u9898\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "MoT\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u534f\u540c\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u771f\u503c\u8868\u6a21\u6001\u6709\u6548\u5f25\u8865\u4e86\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u7684\u74f6\u9888\u3002", "keywords": "\u591a\u6a21\u6001\u63a8\u7406,\u81ea\u7136\u8bed\u8a00\u5904\u7406,\u7b26\u53f7\u903b\u8f91,\u771f\u503c\u8868"}}
{"id": "2505.14697", "pdf": "https://arxiv.org/pdf/2505.14697", "abs": "https://arxiv.org/abs/2505.14697", "authors": ["Jiwoo Song", "Daning Huang"], "title": "Global Description of Flutter Dynamics via Koopman Theory", "categories": ["physics.flu-dyn", "cs.LG"], "comment": null, "summary": "This paper presents a novel parametrization approach for aeroelastic systems\nutilizing Koopman theory, specifically leveraging the Koopman Bilinear Form\n(KBF) model. To address the limitations of linear parametric dependence in the\nKBF model, we introduce the Extended KBF (EKBF) model, which enables a global\nlinear representation of aeroelastic dynamics while capturing stronger\nnonlinear dependence on, e.g., the flutter parameter. The effectiveness of the\nproposed methodology is demonstrated through two case studies: a 2D academic\nexample and a panel flutter problem. Results show that EKBF effectively\ninterpolates and extrapolates principal eigenvalues, capturing flutter\nmechanisms, and accurately predicting the flutter boundary even when the data\nis corrupted by noise. Furthermore, parameterized isostable and isochron\nidentified by EKBF provides valuable insights into the nonlinear flutter\nsystem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKoopman\u7406\u8bba\u7684\u65b0\u578b\u6c14\u52a8\u5f39\u6027\u7cfb\u7edf\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55KBF\u6a21\u578b\uff08EKBF\uff09\u89e3\u51b3\u4e86KBF\u6a21\u578b\u7ebf\u6027\u53c2\u6570\u4f9d\u8d56\u7684\u5c40\u9650\u6027\uff0c\u6709\u6548\u6355\u6349\u4e86\u975e\u7ebf\u6027\u52a8\u6001\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3KBF\u6a21\u578b\u5728\u6c14\u52a8\u5f39\u6027\u7cfb\u7edf\u4e2d\u7ebf\u6027\u53c2\u6570\u4f9d\u8d56\u7684\u5c40\u9650\u6027\uff0c\u4f5c\u8005\u63d0\u51fa\u6269\u5c55KBF\u6a21\u578b\uff08EKBF\uff09\uff0c\u4ee5\u66f4\u5168\u5c40\u7ebf\u6027\u65b9\u5f0f\u8868\u793a\u975e\u7ebf\u6027\u52a8\u6001\u3002", "method": "\u5f15\u5165EKBF\u6a21\u578b\uff0c\u5229\u7528Koopman\u7406\u8bba\u5bf9\u6c14\u52a8\u5f39\u6027\u7cfb\u7edf\u8fdb\u884c\u53c2\u6570\u5316\uff0c\u5e76\u901a\u8fc72D\u5b66\u672f\u6848\u4f8b\u548c\u9762\u677f\u98a4\u632f\u95ee\u9898\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "EKBF\u80fd\u6709\u6548\u63d2\u503c\u548c\u5916\u63a8\u4e3b\u7279\u5f81\u503c\uff0c\u6355\u6349\u98a4\u632f\u673a\u5236\uff0c\u5e76\u5728\u6570\u636e\u53d7\u566a\u58f0\u5e72\u6270\u65f6\u51c6\u786e\u9884\u6d4b\u98a4\u632f\u8fb9\u754c\u3002\u540c\u65f6\uff0cEKBF\u8bc6\u522b\u7684\u53c2\u6570\u5316\u7b49\u7a33\u548c\u7b49\u65f6\u7ebf\u63d0\u4f9b\u4e86\u5bf9\u975e\u7ebf\u6027\u98a4\u632f\u7cfb\u7edf\u7684\u65b0\u89c1\u89e3\u3002", "conclusion": "EKBF\u6a21\u578b\u6210\u529f\u6269\u5c55\u4e86KBF\u7684\u4f18\u52bf\uff0c\u4e3a\u975e\u7ebf\u6027\u6c14\u52a8\u5f39\u6027\u7cfb\u7edf\u7684\u5206\u6790\u548c\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002", "keywords": "Koopman\u7406\u8bba\uff0c\u6c14\u52a8\u5f39\u6027\u7cfb\u7edf\uff0cEKBF\u6a21\u578b\uff0c\u975e\u7ebf\u6027\u52a8\u6001\uff0c\u98a4\u632f\u5206\u6790"}}
{"id": "2505.15367", "pdf": "https://arxiv.org/pdf/2505.15367", "abs": "https://arxiv.org/abs/2505.15367", "authors": ["Dasol Choi", "Seunghyun Lee", "Youngsook Song"], "title": "Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "13 pages", "summary": "Vision-Language Models (VLMs) have demonstrated impressive capabilities in\nunderstanding visual content, but their reliability in safety-critical contexts\nremains under-explored. We introduce VERI (Visual Emergency Recognition\nDataset), a carefully designed diagnostic benchmark of 200 images (100\ncontrastive pairs). Each emergency scene is matched with a visually similar but\nsafe counterpart through multi-stage human verification and iterative\nrefinement. Using a two-stage protocol - risk identification and emergency\nresponse - we evaluate 14 VLMs (2B-124B parameters) across medical emergencies,\naccidents, and natural disasters. Our analysis reveals a systematic\noverreaction problem: models excel at identifying real emergencies (70-100\npercent success rate) but suffer from an alarming rate of false alarms,\nmisidentifying 31-96 percent of safe situations as dangerous, with 10 scenarios\nfailed by all models regardless of scale. This \"better-safe-than-sorry\" bias\nmanifests primarily through contextual overinterpretation (88-93 percent of\nerrors), challenging VLMs' reliability for safety applications. These findings\nhighlight persistent limitations that are not resolved by increasing model\nscale, motivating targeted approaches for improving contextual safety\nassessment in visually misleading scenarios.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5b89\u5168\u5173\u952e\u60c5\u5883\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u6a21\u578b\u666e\u904d\u5b58\u5728\u8fc7\u5ea6\u53cd\u5e94\u7684\u95ee\u9898\uff0c\u5373\u5bf9\u5b89\u5168\u60c5\u51b5\u7684\u8bef\u5224\u7387\u9ad8\u3002", "motivation": "\u7814\u7a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u63ed\u793a\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165VERI\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5bf9\u8bbe\u8ba1\u548c\u4e24\u9636\u6bb5\u8bc4\u4f30\u534f\u8bae\uff08\u98ce\u9669\u8bc6\u522b\u548c\u5e94\u6025\u54cd\u5e94\uff09\u5bf914\u79cd\u4e0d\u540c\u89c4\u6a21\u7684VLM\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u6a21\u578b\u5728\u8bc6\u522b\u771f\u5b9e\u7d27\u6025\u60c5\u51b5\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u5b89\u5168\u60c5\u51b5\u7684\u8bef\u5224\u7387\u9ad8\u8fbe31-96%\uff0c\u4e14\u5b58\u5728\u7cfb\u7edf\u6027\u8fc7\u5ea6\u53cd\u5e94\u95ee\u9898\u3002", "conclusion": "VLM\u7684\u53ef\u9760\u6027\u5728\u5b89\u5168\u5e94\u7528\u4e2d\u5b58\u5728\u95ee\u9898\uff0c\u589e\u52a0\u6a21\u578b\u89c4\u6a21\u65e0\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u9700\u9488\u5bf9\u6027\u5730\u6539\u8fdb\u6a21\u578b\u5728\u89c6\u89c9\u8bef\u5bfc\u573a\u666f\u4e0b\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u80fd\u529b\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u5b89\u5168\u53ef\u9760\u6027, VERI\u6570\u636e\u96c6, \u8fc7\u5ea6\u53cd\u5e94, \u8bef\u5224\u7387"}}
{"id": "2505.14699", "pdf": "https://arxiv.org/pdf/2505.14699", "abs": "https://arxiv.org/abs/2505.14699", "authors": ["Miguel Lopez-Duran", "Julian Fierrez", "Aythami Morales", "Ruben Tolosana", "Oscar Delgado-Mohatar", "Alvaro Ortigosa"], "title": "Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "15 pages, 2 figures, preprint presented in The Fifth ICDAR\n  International Workshop on Machine Learning", "summary": "The automatic analysis of document layouts in digital-born PDF documents\nremains a challenging problem due to the heterogeneous arrangement of textual\nand nontextual elements and the imprecision of the textual metadata in the\nPortable Document Format. In this work, we benchmark Graph Neural Network (GNN)\narchitectures for the task of fine-grained layout classification of text blocks\nfrom digital native documents. We introduce two graph construction structures:\na k-closest-neighbor graph and a fully connected graph, and generate node\nfeatures via pre-trained text and vision models, thus avoiding manual feature\nengineering. Three experimental frameworks are evaluated: single-modality (text\nor visual), concatenated multimodal, and dual-branch multimodal. We evaluated\nfour foundational GNN models and compared them with the baseline. Our\nexperiments are specifically conducted on a rich dataset of public affairs\ndocuments that includes more than 20 sources (e.g., regional and national-level\nofficial gazettes), 37K PDF documents, with 441K pages in total. Our results\ndemonstrate that GraphSAGE operating on the k-closest-neighbor graph in a\ndual-branch configuration achieves the highest per-class and overall accuracy,\noutperforming the baseline in some sources. These findings confirm the\nimportance of local layout relationships and multimodal fusion exploited\nthrough GNNs for the analysis of native digital document layouts.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u8fdb\u884c\u6570\u5b57\u6587\u6863\u7ec6\u7c92\u5ea6\u5e03\u5c40\u5206\u7c7b\uff0c\u901a\u8fc7\u4e24\u79cd\u56fe\u7ed3\u6784\u548c\u591a\u6a21\u6001\u65b9\u6cd5\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6570\u5b57PDF\u6587\u6863\u4e2d\u5f02\u6784\u5e03\u5c40\u5206\u6790\u96be\u9898\uff0c\u907f\u514d\u624b\u52a8\u7279\u5f81\u5de5\u7a0b\u3002", "method": "\u5f15\u5165k\u8fd1\u90bb\u56fe\u548c\u5168\u8fde\u63a5\u56fe\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u6587\u672c\u4e0e\u89c6\u89c9\u6a21\u578b\u751f\u6210\u8282\u70b9\u7279\u5f81\uff0c\u8bc4\u4f30\u4e09\u79cd\u591a\u6a21\u6001\u6846\u67b6\u3002", "result": "GraphSAGE\u5728\u53cc\u5206\u652f\u914d\u7f6e\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u8bc1\u660e\u4e86\u5c40\u90e8\u5e03\u5c40\u5173\u7cfb\u548c\u591a\u6a21\u6001\u878d\u5408\u7684\u91cd\u8981\u6027\u3002", "conclusion": "GNN\u80fd\u6709\u6548\u5206\u6790\u6570\u5b57\u6587\u6863\u5e03\u5c40\uff0c\u591a\u6a21\u6001\u65b9\u6cd5\u548c\u5c40\u90e8\u5173\u7cfb\u662f\u5173\u952e\u3002", "keywords": "GNN, \u6587\u6863\u5e03\u5c40, \u591a\u6a21\u6001, \u56fe\u7ed3\u6784, PDF"}}
{"id": "2505.15380", "pdf": "https://arxiv.org/pdf/2505.15380", "abs": "https://arxiv.org/abs/2505.15380", "authors": ["Zijian Lin", "Yang Zhang", "Yougen Yuan", "Yuming Yan", "Jinjiang Liu", "Zhiyong Wu", "Pengfei Hu", "Qun Yu"], "title": "Accelerating Autoregressive Speech Synthesis Inference With Speech Speculative Decoding", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "5 pages, 4 figures", "summary": "Modern autoregressive speech synthesis models leveraging language models have\ndemonstrated remarkable performance. However, the sequential nature of next\ntoken prediction in these models leads to significant latency, hindering their\ndeployment in scenarios where inference speed is critical. In this work, we\npropose Speech Speculative Decoding (SSD), a novel framework for autoregressive\nspeech synthesis acceleration. Specifically, our method employs a lightweight\ndraft model to generate candidate token sequences, which are subsequently\nverified in parallel by the target model using the proposed SSD framework.\nExperimental results demonstrate that SSD achieves a significant speedup of\n1.4x compared with conventional autoregressive decoding, while maintaining high\nfidelity and naturalness. Subjective evaluations further validate the\neffectiveness of SSD in preserving the perceptual quality of the target model\nwhile accelerating inference.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSSD\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8349\u7a3f\u6a21\u578b\u751f\u6210\u5019\u9009\u4ee4\u724c\u5e8f\u5217\uff0c\u5e76\u5229\u7528\u5e76\u884c\u9a8c\u8bc1\u52a0\u901f\u81ea\u56de\u5f52\u8bed\u97f3\u5408\u6210\uff0c\u5b9e\u73b0\u4e861.4\u500d\u7684\u52a0\u901f\u4e14\u4fdd\u6301\u9ad8\u8d28\u91cf\u8f93\u51fa\u3002", "motivation": "\u73b0\u4ee3\u81ea\u56de\u5f52\u8bed\u97f3\u5408\u6210\u6a21\u578b\u5728\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5b58\u5728\u663e\u8457\u5ef6\u8fdf\uff0c\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u5feb\u901f\u63a8\u7406\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u8349\u7a3f\u6a21\u578b\u751f\u6210\u5019\u9009\u4ee4\u724c\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7SSD\u6846\u67b6\u7531\u76ee\u6807\u6a21\u578b\u5e76\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSSD\u76f8\u6bd4\u4f20\u7edf\u81ea\u56de\u5f52\u89e3\u7801\u5b9e\u73b0\u4e861.4\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6548\u679c\u3002", "conclusion": "SSD\u6846\u67b6\u5728\u52a0\u901f\u63a8\u7406\u7684\u540c\u65f6\u6709\u6548\u4fdd\u6301\u4e86\u611f\u77e5\u8d28\u91cf\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "keywords": "\u81ea\u56de\u5f52\u8bed\u97f3\u5408\u6210, \u63a8\u7406\u52a0\u901f, \u5019\u9009\u9a8c\u8bc1, \u5e76\u884c\u89e3\u7801"}}
{"id": "2505.14701", "pdf": "https://arxiv.org/pdf/2505.14701", "abs": "https://arxiv.org/abs/2505.14701", "authors": ["Aidan Furlong", "Xingang Zhao", "Robert Salko", "Xu Wu"], "title": "Deployment of Traditional and Hybrid Machine Learning for Critical Heat Flux Prediction in the CTF Thermal Hydraulics Code", "categories": ["cs.CE", "cs.LG"], "comment": null, "summary": "Critical heat flux (CHF) marks the transition from nucleate to film boiling,\nwhere heat transfer to the working fluid can rapidly deteriorate. Accurate CHF\nprediction is essential for efficiency, safety, and preventing equipment\ndamage, particularly in nuclear reactors. Although widely used, empirical\ncorrelations frequently exhibit discrepancies in comparison with experimental\ndata, limiting their reliability in diverse operational conditions. Traditional\nmachine learning (ML) approaches have demonstrated the potential for CHF\nprediction but have often suffered from limited interpretability, data\nscarcity, and insufficient knowledge of physical principles. Hybrid model\napproaches, which combine data-driven ML with physics-based models, mitigate\nthese concerns by incorporating prior knowledge of the domain. This study\nintegrated a purely data-driven ML model and two hybrid models (using the Biasi\nand Bowring CHF correlations) within the CTF subchannel code via a custom\nFortran framework. Performance was evaluated using two validation cases: a\nsubset of the Nuclear Regulatory Commission CHF database and the Bennett dryout\nexperiments. In both cases, the hybrid models exhibited significantly lower\nerror metrics in comparison with conventional empirical correlations. The pure\nML model remained competitive with the hybrid models. Trend analysis of error\nparity indicates that ML-based models reduce the tendency for CHF\noverprediction, improving overall accuracy. These results demonstrate that\nML-based CHF models can be effectively integrated into subchannel codes and can\npotentially increase performance in comparison with conventional methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u673a\u5668\u5b66\u4e60\u4e0e\u7269\u7406\u6a21\u578b\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u4e34\u754c\u70ed\u6d41\u5bc6\u5ea6\uff08CHF\uff09\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u7ed3\u679c\u663e\u793a\u6df7\u5408\u6a21\u578b\u5728\u8bef\u5dee\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7ecf\u9a8c\u76f8\u5173\u6027\u65b9\u6cd5\u3002", "motivation": "\u4e34\u754c\u70ed\u6d41\u5bc6\u5ea6\uff08CHF\uff09\u7684\u51c6\u786e\u9884\u6d4b\u5bf9\u6838\u53cd\u5e94\u5806\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7ecf\u9a8c\u76f8\u5173\u6027\u65b9\u6cd5\u5728\u591a\u6837\u5316\u7684\u64cd\u4f5c\u6761\u4ef6\u4e0b\u53ef\u9760\u6027\u6709\u9650\uff0c\u800c\u7eaf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53c8\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u7269\u7406\u77e5\u8bc6\u3002", "method": "\u7814\u7a76\u96c6\u6210\u4e86\u4e00\u79cd\u7eaf\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u4e24\u79cd\u6df7\u5408\u6a21\u578b\uff08\u4f7f\u7528Biasi\u548cBowring CHF\u76f8\u5173\u6027\uff09\u901a\u8fc7\u81ea\u5b9a\u4e49Fortran\u6846\u67b6\uff0c\u5e76\u5728CTF\u5b50\u901a\u9053\u4ee3\u7801\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6df7\u5408\u6a21\u578b\u5728\u4e24\u79cd\u9a8c\u8bc1\u6848\u4f8b\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u66f4\u4f4e\u7684\u8bef\u5dee\u6307\u6807\uff0c\u7eaf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8868\u73b0\u4e0e\u6df7\u5408\u6a21\u578b\u76f8\u5f53\u3002\u673a\u5668\u5b66\u4e60\u6a21\u578b\u51cf\u5c11\u4e86CHF\u7684\u8fc7\u5ea6\u9884\u6d4b\u8d8b\u52bf\uff0c\u63d0\u9ad8\u4e86\u6574\u4f53\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684CHF\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u96c6\u6210\u5230\u5b50\u901a\u9053\u4ee3\u7801\u4e2d\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5177\u6709\u6f5c\u5728\u7684\u5e94\u7528\u4ef7\u503c\u3002", "keywords": "\u4e34\u754c\u70ed\u6d41\u5bc6\u5ea6,\u673a\u5668\u5b66\u4e60,\u6df7\u5408\u6a21\u578b,\u6838\u53cd\u5e94\u5806,\u70ed\u4f20\u9012"}}
{"id": "2505.14704", "pdf": "https://arxiv.org/pdf/2505.14704", "abs": "https://arxiv.org/abs/2505.14704", "authors": ["Giovanni Catalani", "Jean Fesquet", "Xavier Bertrand", "Fr\u00e9d\u00e9ric Tost", "Michael Bauerheim", "Joseph Morlier"], "title": "Towards scalable surrogate models based on Neural Fields for large scale aerodynamic simulations", "categories": ["physics.flu-dyn", "cs.LG"], "comment": null, "summary": "This paper introduces a novel surrogate modeling framework for aerodynamic\napplications based on Neural Fields. The proposed approach, MARIO (Modulated\nAerodynamic Resolution Invariant Operator), addresses non parametric geometric\nvariability through an efficient shape encoding mechanism and exploits the\ndiscretization-invariant nature of Neural Fields. It enables training on\nsignificantly downsampled meshes, while maintaining consistent accuracy during\nfull-resolution inference. These properties allow for efficient modeling of\ndiverse flow conditions, while reducing computational cost and memory\nrequirements compared to traditional CFD solvers and existing surrogate\nmethods. The framework is validated on two complementary datasets that reflect\nindustrial constraints. First, the AirfRANS dataset consists in a\ntwo-dimensional airfoil benchmark with non-parametric shape variations.\nPerformance evaluation of MARIO on this case demonstrates an order of magnitude\nimprovement in prediction accuracy over existing methods across velocity,\npressure, and turbulent viscosity fields, while accurately capturing boundary\nlayer phenomena and aerodynamic coefficients. Second, the NASA Common Research\nModel features three-dimensional pressure distributions on a full aircraft\nsurface mesh, with parametric control surface deflections. This configuration\nconfirms MARIO's accuracy and scalability. Benchmarking against\nstate-of-the-art methods demonstrates that Neural Field surrogates can provide\nrapid and accurate aerodynamic predictions under the computational and data\nlimitations characteristic of industrial applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u573a\u7684\u65b0\u578b\u66ff\u4ee3\u5efa\u6a21\u6846\u67b6MARIO\uff0c\u7528\u4e8e\u7a7a\u6c14\u52a8\u529b\u5b66\u5e94\u7528\uff0c\u901a\u8fc7\u9ad8\u6548\u5f62\u72b6\u7f16\u7801\u548c\u79bb\u6563\u4e0d\u53d8\u6027\u5b9e\u73b0\u9ad8\u6548\u5efa\u6a21\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4f20\u7edfCFD\u6c42\u89e3\u5668\u548c\u73b0\u6709\u66ff\u4ee3\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u9700\u6c42\u4e0a\u5b58\u5728\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u975e\u53c2\u6570\u51e0\u4f55\u53d8\u5316\u548c\u591a\u6837\u5316\u7684\u6d41\u52a8\u6761\u4ef6\u3002", "method": "\u63d0\u51faMARIO\u6846\u67b6\uff0c\u5229\u7528\u795e\u7ecf\u573a\u7684\u79bb\u6563\u4e0d\u53d8\u6027\u548c\u9ad8\u6548\u5f62\u72b6\u7f16\u7801\u673a\u5236\uff0c\u5728\u4f4e\u5206\u8fa8\u7387\u7f51\u683c\u4e0a\u8bad\u7ec3\u5e76\u4fdd\u6301\u5168\u5206\u8fa8\u7387\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728AirfRANS\u548cNASA Common Research Model\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u663e\u793a\uff0cMARIO\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u51c6\u786e\u6355\u6349\u8fb9\u754c\u5c42\u73b0\u8c61\u548c\u7a7a\u6c14\u52a8\u529b\u5b66\u7cfb\u6570\u3002", "conclusion": "\u795e\u7ecf\u573a\u66ff\u4ee3\u6a21\u578b\u80fd\u591f\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u63d0\u4f9b\u5feb\u901f\u51c6\u786e\u7684\u7a7a\u6c14\u52a8\u529b\u5b66\u9884\u6d4b\uff0c\u89e3\u51b3\u8ba1\u7b97\u548c\u6570\u636e\u9650\u5236\u95ee\u9898\u3002", "keywords": "\u795e\u7ecf\u573a, \u66ff\u4ee3\u5efa\u6a21, \u7a7a\u6c14\u52a8\u529b\u5b66, \u975e\u53c2\u6570\u51e0\u4f55, \u9ad8\u6548\u5f62\u72b6\u7f16\u7801"}}
{"id": "2505.15406", "pdf": "https://arxiv.org/pdf/2505.15406", "abs": "https://arxiv.org/abs/2505.15406", "authors": ["Zirui Song", "Qian Jiang", "Mingxuan Cui", "Mingzhe Li", "Lang Gao", "Zeyu Zhang", "Zixiang Xu", "Yanbo Wang", "Chenxi Wang", "Guangxian Ouyang", "Zhenhao Chen", "Xiuying Chen"], "title": "Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "We release AJailBench, including both static and optimized\n  adversarial data, to facilitate future research:\n  https://github.com/mbzuai-nlp/AudioJailbreak", "summary": "The rise of Large Audio Language Models (LAMs) brings both potential and\nrisks, as their audio outputs may contain harmful or unethical content.\nHowever, current research lacks a systematic, quantitative evaluation of LAM\nsafety especially against jailbreak attacks, which are challenging due to the\ntemporal and semantic nature of speech. To bridge this gap, we introduce\nAJailBench, the first benchmark specifically designed to evaluate jailbreak\nvulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of\n1,495 adversarial audio prompts spanning 10 policy-violating categories,\nconverted from textual jailbreak attacks using realistic text to speech\nsynthesis. Using this dataset, we evaluate several state-of-the-art LAMs and\nreveal that none exhibit consistent robustness across attacks. To further\nstrengthen jailbreak testing and simulate more realistic attack conditions, we\npropose a method to generate dynamic adversarial variants. Our Audio\nPerturbation Toolkit (APT) applies targeted distortions across time, frequency,\nand amplitude domains. To preserve the original jailbreak intent, we enforce a\nsemantic consistency constraint and employ Bayesian optimization to efficiently\nsearch for perturbations that are both subtle and highly effective. This\nresults in AJailBench-APT, an extended dataset of optimized adversarial audio\nsamples. Our findings demonstrate that even small, semantically preserved\nperturbations can significantly reduce the safety performance of leading LAMs,\nunderscoring the need for more robust and semantically aware defense\nmechanisms.", "AI": {"tldr": "AJailBench\u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff08LAMs\uff09\u5728\u8d8a\u72f1\u653b\u51fb\u4e0b\u5b89\u5168\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u751f\u6210\u4f18\u5316\u7684\u5bf9\u6297\u97f3\u9891\u6837\u672c\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u73b0\u6709\u7684LAMs\u7814\u7a76\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u5b89\u5168\u6027\u8bc4\u4f30\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u8d8a\u72f1\u653b\u51fb\u65f6\uff0c\u7531\u4e8e\u5176\u65f6\u95f4\u548c\u8bed\u4e49\u7279\u6027\uff0c\u6311\u6218\u66f4\u5927\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86AJailBench-Base\u6570\u636e\u96c6\uff081,495\u4e2a\u5bf9\u6297\u97f3\u9891\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u97f3\u9891\u6270\u52a8\u5de5\u5177\u7bb1\uff08APT\uff09\u751f\u6210\u52a8\u6001\u5bf9\u6297\u53d8\u4f53\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709LAMs\u5747\u65e0\u6cd5\u5728\u6240\u6709\u653b\u51fb\u4e2d\u4fdd\u6301\u7a33\u5b9a\uff0c\u4e14\u8f7b\u5fae\u8bed\u4e49\u4fdd\u7559\u7684\u6270\u52a8\u4e5f\u4f1a\u663e\u8457\u964d\u4f4e\u5b89\u5168\u6027\u3002", "conclusion": "\u9700\u8981\u66f4\u9c81\u68d2\u4e14\u8bed\u4e49\u611f\u77e5\u7684\u9632\u5fa1\u673a\u5236\u6765\u63d0\u5347LAMs\u7684\u5b89\u5168\u6027\u3002", "keywords": "\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b, \u8d8a\u72f1\u653b\u51fb, \u5b89\u5168\u6027\u8bc4\u4f30, \u5bf9\u6297\u6837\u672c, \u8bed\u4e49\u4e00\u81f4\u6027"}}
{"id": "2505.14705", "pdf": "https://arxiv.org/pdf/2505.14705", "abs": "https://arxiv.org/abs/2505.14705", "authors": ["Xin Zhang", "Ziruo Zhang", "Jiawei Du", "Zuozhu Liu", "Joey Tianyi Zhou"], "title": "Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal Dataset Distillation (MDD) seeks to condense large-scale\nimage-text datasets into compact surrogates while retaining their effectiveness\nfor cross-modal learning. Despite recent progress, existing MDD approaches\noften suffer from \\textit{\\textbf{Modality Collapse}}, characterized by\nover-concentrated intra-modal representations and enlarged distributional gap\nacross modalities. In this paper, at the first time, we identify this issue as\nstemming from a fundamental conflict between the over-compression behavior\ninherent in dataset distillation and the cross-modal supervision imposed by\ncontrastive objectives. To alleviate modality collapse, we introduce\n\\textbf{RepBlend}, a novel MDD framework that weakens overdominant cross-modal\nsupervision via representation blending, thereby significantly enhancing\nintra-modal diversity. Additionally, we observe that current MDD methods impose\nasymmetric supervision across modalities, resulting in biased optimization. To\naddress this, we propose symmetric projection trajectory matching, which\nsynchronizes the optimization dynamics using modality-specific projection\nheads, thereby promoting balanced supervision and enhancing cross-modal\nalignment. Experiments on Flickr-30K and MS-COCO show that RepBlend\nconsistently outperforms prior state-of-the-art MDD methods, achieving\nsignificant gains in retrieval performance (e.g., +9.4 IR@10, +6.3 TR@10 under\nthe 100-pair setting) and offering up to 6.7$\\times$ distillation speedup.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRepBlend\u6846\u67b6\uff0c\u901a\u8fc7\u8868\u5f81\u6df7\u5408\u548c\u5bf9\u79f0\u6295\u5f71\u8f68\u8ff9\u5339\u914d\u89e3\u51b3\u591a\u6a21\u6001\u6570\u636e\u96c6\u84b8\u998f\u4e2d\u7684\u6a21\u6001\u584c\u7f29\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u548c\u84b8\u998f\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u6570\u636e\u96c6\u84b8\u998f\u4e2d\u5b58\u5728\u6a21\u6001\u584c\u7f29\u548c\u8de8\u6a21\u6001\u76d1\u7763\u4e0d\u5bf9\u79f0\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u8de8\u6a21\u6001\u5b66\u4e60\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faRepBlend\u6846\u67b6\uff0c\u91c7\u7528\u8868\u5f81\u6df7\u5408\u51cf\u5f31\u8de8\u6a21\u6001\u76d1\u7763\u7684\u8fc7\u5ea6\u4e3b\u5bfc\uff0c\u5e76\u901a\u8fc7\u5bf9\u79f0\u6295\u5f71\u8f68\u8ff9\u5339\u914d\u5e73\u8861\u8de8\u6a21\u6001\u76d1\u7763\u3002", "result": "\u5728Flickr-30K\u548cMS-COCO\u4e0a\uff0cRepBlend\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u68c0\u7d22\u6027\u80fd\u548c\u84b8\u998f\u901f\u5ea6\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "RepBlend\u6709\u6548\u7f13\u89e3\u4e86\u6a21\u6001\u584c\u7f29\u5e76\u5b9e\u73b0\u4e86\u5e73\u8861\u7684\u8de8\u6a21\u6001\u76d1\u7763\uff0c\u4e3a\u591a\u6a21\u6001\u6570\u636e\u96c6\u84b8\u998f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u591a\u6a21\u6001\u6570\u636e\u96c6\u84b8\u998f,\u6a21\u6001\u584c\u7f29,\u8868\u5f81\u6df7\u5408,\u5bf9\u79f0\u6295\u5f71\u8f68\u8ff9\u5339\u914d"}}
{"id": "2505.14713", "pdf": "https://arxiv.org/pdf/2505.14713", "abs": "https://arxiv.org/abs/2505.14713", "authors": ["Dionissios T. Hristopulos", "Anastassia Baxevani", "Giorgio Kaniadakis"], "title": "Stochastic Processes with Modified Lognormal Distribution Featuring Flexible Upper Tail", "categories": ["stat.ME", "cs.LG", "math.ST", "physics.data-an", "stat.ML", "stat.TH", "60G05, 60G10, 60G12, 62M10, 62M30", "G.3; H.1; I.2.6; J.2"], "comment": "44 pages (36 in Main and 8 in Supplement), 27 figures (20 in Main and\n  7 in Supplement), 13 tables (9 in Main and 4 in Supplement)", "summary": "Asymmetric, non-Gaussian probability distributions are often observed in the\nanalysis of natural and engineering datasets. The lognormal distribution is a\nstandard model for data with skewed frequency histograms and fat tails.\nHowever, the lognormal law severely restricts the asymptotic dependence of the\nprobability density and the hazard function for high values. Herein we present\na family of three-parameter non-Gaussian probability density functions that are\nbased on generalized kappa-exponential and kappa-logarithm functions and\ninvestigate its mathematical properties. These kappa-lognormal densities\nrepresent continuous deformations of the lognormal with lighter right tails,\ncontrolled by the parameter kappa. In addition, bimodal distributions are\nobtained for certain parameter combinations. We derive closed-form analytic\nexpressions for the main statistical functions of the kappa-lognormal\ndistribution. For the moments, we derive bounds that are based on\nhypergeometric functions as well as series expansions. Explicit expressions for\nthe gradient and Hessian of the negative log-likelihood are obtained to\nfacilitate numerical maximum-likelihood estimates of the kappa-lognormal\nparameters from data. We also formulate a joint probability density function\nfor kappa-lognormal stochastic processes by applying Jacobi's multivariate\ntheorem to a latent Gaussian process. Estimation of the kappa-lognormal\ndistribution based on synthetic and real data is explored. Furthermore, we\ninvestigate applications of kappa-lognormal processes with different covariance\nkernels in time series forecasting and spatial interpolation using warped\nGaussian process regression. Our results are of practical interest for modeling\nskewed distributions in various scientific and engineering fields.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u53c2\u6570\u7684\u975e\u9ad8\u65af\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u5bb6\u65cf\uff0c\u57fa\u4e8e\u5e7f\u4e49\u7684kappa\u6307\u6570\u548c\u5bf9\u6570\u51fd\u6570\uff0c\u7528\u4e8e\u5904\u7406\u504f\u6001\u548c\u91cd\u5c3e\u6570\u636e\uff0c\u63d0\u4f9b\u4e86\u6570\u5b66\u7279\u6027\u548c\u5b9e\u7528\u5e94\u7528\u3002", "motivation": "\u9488\u5bf9\u975e\u5bf9\u79f0\u3001\u975e\u9ad8\u65af\u5206\u5e03\u6570\u636e\uff08\u5982\u504f\u6001\u548c\u91cd\u5c3e\u6570\u636e\uff09\uff0c\u4f20\u7edf\u5bf9\u6570\u6b63\u6001\u5206\u5e03\u5728\u9ad8\u503c\u533a\u57df\u7684\u6e10\u8fd1\u4f9d\u8d56\u6027\u548c\u5371\u9669\u51fd\u6570\u53d7\u9650\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5e7f\u4e49kappa\u6307\u6570\u548c\u5bf9\u6570\u51fd\u6570\u7684\u4e09\u53c2\u6570\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\uff0c\u7814\u7a76\u5176\u6570\u5b66\u6027\u8d28\uff0c\u5305\u62ec\u7edf\u8ba1\u51fd\u6570\u3001\u77e9\u3001\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u53ca\u591a\u7ef4\u6269\u5c55\u3002", "result": "kappa-\u5bf9\u6570\u6b63\u6001\u5206\u5e03\u80fd\u591f\u63d0\u4f9b\u66f4\u8f7b\u7684\u53f3\u5c3e\u63a7\u5236\u548c\u53cc\u5cf0\u5206\u5e03\u7279\u6027\uff0c\u9002\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548c\u7a7a\u95f4\u63d2\u503c\u7b49\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "kappa-\u5bf9\u6570\u6b63\u6001\u5206\u5e03\u4e3a\u79d1\u5b66\u548c\u5de5\u7a0b\u9886\u57df\u7684\u504f\u6001\u6570\u636e\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u5b9e\u7528\u7684\u5de5\u5177\u3002", "keywords": "kappa-\u5bf9\u6570\u6b63\u6001\u5206\u5e03, \u504f\u6001\u5206\u5e03, \u91cd\u5c3e\u6570\u636e, \u6700\u5927\u4f3c\u7136\u4f30\u8ba1, \u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52"}}
{"id": "2505.15420", "pdf": "https://arxiv.org/pdf/2505.15420", "abs": "https://arxiv.org/abs/2505.15420", "authors": ["Yuhao Wang", "Wenjie Qu", "Yanze Jiang", "Zichen Liu", "Yue Liu", "Shengfang Zhai", "Yinpeng Dong", "Jiaheng Zhang"], "title": "Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by incorporating external knowledge bases, but they are vulnerable to\nprivacy risks from data extraction attacks. Existing extraction methods\ntypically rely on malicious inputs such as prompt injection or jailbreaking,\nmaking them easily detectable via input- or output-level detection. In this\npaper, we introduce Implicit Knowledge Extraction Attack (IKEA), which conducts\nknowledge extraction on RAG systems through benign queries. IKEA first\nleverages anchor concepts to generate queries with the natural appearance, and\nthen designs two mechanisms to lead to anchor concept thoroughly 'explore' the\nRAG's privacy knowledge: (1) Experience Reflection Sampling, which samples\nanchor concepts based on past query-response patterns to ensure the queries'\nrelevance to RAG documents; (2) Trust Region Directed Mutation, which\niteratively mutates anchor concepts under similarity constraints to further\nexploit the embedding space. Extensive experiments demonstrate IKEA's\neffectiveness under various defenses, surpassing baselines by over 80% in\nextraction efficiency and 90% in attack success rate. Moreover, the substitute\nRAG system built from IKEA's extractions consistently outperforms those based\non baseline methods across multiple evaluation tasks, underscoring the\nsignificant privacy risk in RAG systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u5f0f\u77e5\u8bc6\u63d0\u53d6\u653b\u51fb\uff08IKEA\uff09\uff0c\u901a\u8fc7\u826f\u6027\u67e5\u8be2\u5bf9RAG\u7cfb\u7edf\u8fdb\u884c\u77e5\u8bc6\u63d0\u53d6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "RAG\u7cfb\u7edf\u53ef\u80fd\u56e0\u5916\u90e8\u77e5\u8bc6\u5e93\u7684\u5f15\u5165\u800c\u9762\u4e34\u9690\u79c1\u98ce\u9669\uff0c\u73b0\u6709\u7684\u653b\u51fb\u65b9\u6cd5\u7531\u4e8e\u4f9d\u8d56\u6076\u610f\u8f93\u5165\u6613\u88ab\u68c0\u6d4b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9690\u853d\u7684\u653b\u51fb\u65b9\u5f0f\u3002", "method": "IKEA\u901a\u8fc7\u951a\u6982\u5ff5\u751f\u6210\u81ea\u7136\u67e5\u8be2\uff0c\u5e76\u5229\u7528\u7ecf\u9a8c\u53cd\u5c04\u91c7\u6837\u548c\u4fe1\u4efb\u533a\u57df\u5b9a\u5411\u7a81\u53d8\u4e24\u5927\u673a\u5236\uff0c\u5b9e\u73b0\u5bf9\u9690\u79c1\u77e5\u8bc6\u7684\u6709\u6548\u63d0\u53d6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIKEA\u5728\u5404\u79cd\u9632\u5fa1\u63aa\u65bd\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u53d6\u6548\u7387\u548c\u653b\u51fb\u6210\u529f\u7387\u8fdc\u8d85\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "IKEA\u63ed\u793a\u4e86RAG\u7cfb\u7edf\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u7684\u4e25\u91cd\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u4e86\u6539\u8fdb\u9632\u5fa1\u673a\u5236\u7684\u5fc5\u8981\u6027\u3002", "keywords": "RAG\u7cfb\u7edf, \u9690\u79c1\u98ce\u9669, \u77e5\u8bc6\u63d0\u53d6, \u9690\u6027\u653b\u51fb"}}
{"id": "2505.14899", "pdf": "https://arxiv.org/pdf/2505.14899", "abs": "https://arxiv.org/abs/2505.14899", "authors": ["Wenjie Lin", "Jin Wei-Kocsis"], "title": "Think, Reflect, Create: Metacognitive Learning for Zero-Shot Robotic Planning with LLMs", "categories": ["cs.RO", "cs.CL"], "comment": null, "summary": "While large language models (LLMs) have shown great potential across various\ndomains, their applications in robotics remain largely limited to static,\nprompt-based behaviors and still face challenges in handling complex tasks\nunder zero-shot or few-shot settings. Inspired by human metacognitive learning\nand creative problem-solving, we address this limitation by exploring a\nfundamental research question: Can LLMs be empowered with metacognitive\ncapabilities to reason, reflect, and create, thereby enhancing their ability to\nperform robotic tasks with minimal demonstrations? In this paper, we present an\nearly-stage framework that integrates metacognitive learning into LLM-powered\nmulti-robot collaboration. The proposed framework equips the LLM-powered\nrobotic agents with a skill decomposition and self-reflection mechanism that\nidentifies modular skills from prior tasks, reflects on failures in unseen task\nscenarios, and synthesizes effective new solutions. Experimental results show\nthat our metacognitive-learning-empowered LLM framework significantly\noutperforms existing baselines. Moreover, we observe that the framework is\ncapable of generating solutions that differ from the ground truth yet still\nsuccessfully complete the tasks. These exciting findings support our hypothesis\nthat metacognitive learning can foster creativity in robotic planning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u4e86\u5982\u4f55\u901a\u8fc7\u8d4b\u4e88\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5143\u8ba4\u77e5\u80fd\u529b\u6765\u63d0\u5347\u5176\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u96f6\u6837\u672c\u6216\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u3002", "motivation": "\u73b0\u6709LLMs\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\u5c40\u9650\u4e8e\u9759\u6001\u3001\u57fa\u4e8e\u63d0\u793a\u7684\u884c\u4e3a\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u4efb\u52a1\u3002\u53d7\u4eba\u7c7b\u5143\u8ba4\u77e5\u5b66\u4e60\u548c\u521b\u9020\u6027\u89e3\u51b3\u95ee\u9898\u7684\u542f\u53d1\uff0c\u7814\u7a76\u76ee\u6807\u662f\u63d0\u5347LLMs\u7684\u63a8\u7406\u3001\u53cd\u601d\u548c\u521b\u9020\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u65e9\u671f\u6846\u67b6\uff0c\u5c06\u5143\u8ba4\u77e5\u5b66\u4e60\u6574\u5408\u5230LLM\u9a71\u52a8\u7684\u591a\u673a\u5668\u4eba\u534f\u4f5c\u4e2d\uff0c\u5305\u62ec\u6280\u80fd\u5206\u89e3\u548c\u81ea\u53cd\u601d\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u80fd\u751f\u6210\u4e0e\u771f\u5b9e\u60c5\u51b5\u4e0d\u540c\u4f46\u4ecd\u80fd\u6210\u529f\u5b8c\u6210\u4efb\u52a1\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u5143\u8ba4\u77e5\u5b66\u4e60\u53ef\u4ee5\u4fc3\u8fdb\u673a\u5668\u4eba\u89c4\u5212\u7684\u521b\u9020\u6027\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u5143\u8ba4\u77e5\u5b66\u4e60,\u673a\u5668\u4eba\u534f\u4f5c,\u521b\u9020\u6027\u89c4\u5212"}}
{"id": "2505.14716", "pdf": "https://arxiv.org/pdf/2505.14716", "abs": "https://arxiv.org/abs/2505.14716", "authors": ["Sahil Tomar", "Rajeshwar Tripathi", "Sandeep Kumar"], "title": "A Hybrid Quantum Classical Pipeline for X Ray Based Fracture Diagnosis", "categories": ["eess.IV", "cs.CV", "cs.ET", "cs.IT", "cs.LG", "math.IT"], "comment": "8 pages", "summary": "Bone fractures are a leading cause of morbidity and disability worldwide,\nimposing significant clinical and economic burdens on healthcare systems.\nTraditional X ray interpretation is time consuming and error prone, while\nexisting machine learning and deep learning solutions often demand extensive\nfeature engineering, large, annotated datasets, and high computational\nresources. To address these challenges, a distributed hybrid quantum classical\npipeline is proposed that first applies Principal Component Analysis (PCA) for\ndimensionality reduction and then leverages a 4 qubit quantum amplitude\nencoding circuit for feature enrichment. By fusing eight PCA derived features\nwith eight quantum enhanced features into a 16 dimensional vector and then\nclassifying with different machine learning models achieving 99% accuracy using\na public multi region X ray dataset on par with state of the art transfer\nlearning models while reducing feature extraction time by 82%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u6df7\u5408\u91cf\u5b50\u7ecf\u5178\u7ba1\u9053\uff0c\u7528\u4e8e\u9aa8\u6298X\u5149\u56fe\u50cf\u5206\u7c7b\uff0c\u7ed3\u5408PCA\u548c\u91cf\u5b50\u632f\u5e45\u7f16\u7801\uff0c\u8fbe\u523099%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u7279\u5f81\u63d0\u53d6\u65f6\u95f482%\u3002", "motivation": "\u4f20\u7edfX\u5149\u89e3\u6790\u8017\u65f6\u4e14\u6613\u9519\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7279\u5f81\u5de5\u7a0b\u548c\u6570\u636e\u6807\u6ce8\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u6df7\u5408\u91cf\u5b50\u7ecf\u5178\u7ba1\u9053\uff0c\u5148\u4f7f\u7528PCA\u964d\u7ef4\uff0c\u518d\u901a\u8fc74\u91cf\u5b50\u6bd4\u7279\u632f\u5e45\u7f16\u7801\u7535\u8def\u589e\u5f3a\u7279\u5f81\uff0c\u6700\u7ec8\u7ed3\u5408PCA\u548c\u91cf\u5b50\u7279\u5f81\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u516c\u5171\u591a\u533a\u57dfX\u5149\u6570\u636e\u96c6\u4e0a\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523099%\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u7279\u5f81\u63d0\u53d6\u65f6\u95f4\u51cf\u5c1182%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9aa8\u6298\u8bca\u65ad\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "keywords": "\u9aa8\u6298\u8bca\u65ad, \u91cf\u5b50\u8ba1\u7b97, PCA, \u673a\u5668\u5b66\u4e60"}}
{"id": "2505.14910", "pdf": "https://arxiv.org/pdf/2505.14910", "abs": "https://arxiv.org/abs/2505.14910", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Dongyu Yao", "Zhiyuan Zhu", "Ziyue Jiang", "Yuhan Wang", "Tao Jin", "Zhou Zhao"], "title": "TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by ACL 2025", "summary": "Customizable multilingual zero-shot singing voice synthesis (SVS) has various\npotential applications in music composition and short video dubbing. However,\nexisting SVS models overly depend on phoneme and note boundary annotations,\nlimiting their robustness in zero-shot scenarios and producing poor transitions\nbetween phonemes and notes. Moreover, they also lack effective multi-level\nstyle control via diverse prompts. To overcome these challenges, we introduce\nTCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer\nand style control based on various prompts. TCSinger 2 mainly includes three\nkey modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration,\nextends content embedding, and applies masking to the boundaries to enable\nsmooth transitions. 2) Custom Audio Encoder, uses contrastive learning to\nextract aligned representations from singing, speech, and textual prompts. 3)\nFlow-based Custom Transformer, leverages Cus-MOE, with F0 supervision,\nenhancing both the synthesis quality and style modeling of the generated\nsinging voice. Experimental results show that TCSinger 2 outperforms baseline\nmodels in both subjective and objective metrics across multiple related tasks.", "AI": {"tldr": "TCSinger 2\u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u591a\u8bed\u8a00\u96f6\u6837\u672c\u6b4c\u58f0\u5408\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u98ce\u683c\u8fc1\u79fb\u548c\u591a\u6837\u5316\u63d0\u793a\u7684\u6837\u5f0f\u63a7\u5236\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u6b4c\u58f0\u5408\u6210\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u97f3\u7d20\u548c\u97f3\u7b26\u8fb9\u754c\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u96f6\u6837\u672c\u573a\u666f\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u7684\u591a\u7ea7\u6837\u5f0f\u63a7\u5236\u3002", "method": "1) BBC Encoder\u5b9e\u73b0\u5e73\u6ed1\u8fc7\u6e21\uff1b2) Custom Audio Encoder\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u8868\u793a\uff1b3) Flow-based Custom Transformer\u63d0\u5347\u5408\u6210\u8d28\u91cf\u548c\u6837\u5f0f\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTCSinger 2\u5728\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "TCSinger 2\u901a\u8fc7\u521b\u65b0\u6a21\u5757\u89e3\u51b3\u4e86\u73b0\u6709\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6b4c\u58f0\u5408\u6210\u7684\u8d28\u91cf\u548c\u7075\u6d3b\u6027\u3002", "keywords": "\u6b4c\u58f0\u5408\u6210, \u96f6\u6837\u672c\u5b66\u4e60, \u591a\u4efb\u52a1\u5b66\u4e60, \u98ce\u683c\u8fc1\u79fb"}}
{"id": "2505.15429", "pdf": "https://arxiv.org/pdf/2505.15429", "abs": "https://arxiv.org/abs/2505.15429", "authors": ["Pritam Anand"], "title": "Uncertainty Quantification in SVM prediction", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper explores Uncertainty Quantification (UQ) in SVM predictions,\nparticularly for regression and forecasting tasks. Unlike the Neural Network,\nthe SVM solutions are typically more stable, sparse, optimal and interpretable.\nHowever, there are only few literature which addresses the UQ in SVM\nprediction. At first, we provide a comprehensive summary of existing Prediction\nInterval (PI) estimation and probabilistic forecasting methods developed in the\nSVM framework and evaluate them against the key properties expected from an\nideal PI model. We find that none of the existing SVM PI models achieves a\nsparse solution. To introduce sparsity in SVM model, we propose the Sparse\nSupport Vector Quantile Regression (SSVQR) model, which constructs PIs and\nprobabilistic forecasts by solving a pair of linear programs. Further, we\ndevelop a feature selection algorithm for PI estimation using SSVQR that\neffectively eliminates a significant number of features while improving PI\nquality in case of high-dimensional dataset. Finally we extend the SVM models\nin Conformal Regression setting for obtaining more stable prediction set with\nfinite test set guarantees. Extensive experiments on artificial, real-world\nbenchmark datasets compare the different characteristics of both existing and\nproposed SVM-based PI estimation methods and also highlight the advantages of\nthe feature selection in PI estimation. Furthermore, we compare both, the\nexisting and proposed SVM-based PI estimation models, with modern deep learning\nmodels for probabilistic forecasting tasks on benchmark datasets. Furthermore,\nSVM models show comparable or superior performance to modern complex deep\nlearning models for probabilistic forecasting task in our experiments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7a00\u758f\u652f\u6301\u5411\u91cf\u5206\u4f4d\u6570\u56de\u5f52\uff08SSVQR\uff09\u6a21\u578b\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u9884\u6d4b\u533a\u95f4\uff08PI\uff09\u4f30\u8ba1\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "SVM\u9884\u6d4b\u5728\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8e\u795e\u7ecf\u7f51\u7edc\uff0c\u4f46\u5173\u4e8e\u5176\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u7814\u7a76\u8f83\u5c11\u3002\u4f5c\u8005\u5e0c\u671b\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bba\u6587\u603b\u7ed3\u4e86\u73b0\u6709SVM\u6846\u67b6\u4e0b\u7684PI\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86SSVQR\u6a21\u578b\uff0c\u901a\u8fc7\u7ebf\u6027\u89c4\u5212\u6784\u5efa\u7a00\u758f\u89e3\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u7279\u5f81\u9009\u62e9\u7b97\u6cd5\u4ee5\u63d0\u9ad8PI\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSSVQR\u5728\u9ad8\u7ef4\u6570\u636e\u96c6\u4e0a\u80fd\u663e\u8457\u51cf\u5c11\u7279\u5f81\u6570\u91cf\u5e76\u63d0\u5347PI\u8d28\u91cf\uff0c\u4e14\u5728\u6982\u7387\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u7b49\u540c\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "\u8bba\u6587\u8bc1\u660e\u4e86SVM\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u7684\u6f5c\u529b\uff0cSSVQR\u6a21\u578b\u4e3a\u7a00\u758f\u6027\u548c\u9ad8\u6548\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u4e0d\u786e\u5b9a\u6027\u91cf\u5316, \u652f\u6301\u5411\u91cf\u673a, \u9884\u6d4b\u533a\u95f4, \u7a00\u758f\u5206\u4f4d\u6570\u56de\u5f52, \u7279\u5f81\u9009\u62e9"}}
{"id": "2505.14722", "pdf": "https://arxiv.org/pdf/2505.14722", "abs": "https://arxiv.org/abs/2505.14722", "authors": ["Pierre-Marc Jodoin", "Manon Edde", "Gabriel Girard", "F\u00e9lix Dumais", "Guillaume Theaud", "Matthieu Dumont", "Jean-Christophe Houde", "Yoan David", "Maxime Descoteaux"], "title": "ComBAT Harmonization for diffusion MRI: Challenges and Best Practices", "categories": ["stat.AP", "cs.CV", "cs.LG", "physics.med-ph"], "comment": null, "summary": "Over the years, ComBAT has become the standard method for harmonizing\nMRI-derived measurements, with its ability to compensate for site-related\nadditive and multiplicative biases while preserving biological variability.\nHowever, ComBAT relies on a set of assumptions that, when violated, can result\nin flawed harmonization. In this paper, we thoroughly review ComBAT's\nmathematical foundation, outlining these assumptions, and exploring their\nimplications for the demographic composition necessary for optimal results.\n  Through a series of experiments involving a slightly modified version of\nComBAT called Pairwise-ComBAT tailored for normative modeling applications, we\nassess the impact of various population characteristics, including population\nsize, age distribution, the absence of certain covariates, and the magnitude of\nadditive and multiplicative factors. Based on these experiments, we present\nfive essential recommendations that should be carefully considered to enhance\nconsistency and supporting reproducibility, two essential factors for open\nscience, collaborative research, and real-life clinical deployment.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86ComBAT\u65b9\u6cd5\u5728MRI\u6570\u636e\u534f\u8c03\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7248Pairwise-ComBAT\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u4eba\u53e3\u7279\u5f81\u5bf9\u6548\u679c\u7684\u5f71\u54cd\uff0c\u6700\u7ec8\u7ed9\u51fa\u4e94\u9879\u5efa\u8bae\u4ee5\u63d0\u5347\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "motivation": "ComBAT\u4f5c\u4e3aMRI\u6570\u636e\u534f\u8c03\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u867d\u80fd\u4fdd\u7559\u751f\u7269\u53d8\u5f02\u6027\uff0c\u4f46\u5176\u4f9d\u8d56\u7684\u5047\u8bbe\u4e00\u65e6\u88ab\u8fdd\u80cc\u4f1a\u5bfc\u81f4\u534f\u8c03\u5931\u8d25\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7814\u7a76\u5176\u6570\u5b66\u57fa\u7840\u548c\u9002\u7528\u6761\u4ef6\u3002", "method": "\u901a\u8fc7\u6539\u8fdb\u7248Pairwise-ComBAT\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4eba\u53e3\u89c4\u6a21\u3001\u5e74\u9f84\u5206\u5e03\u3001\u534f\u53d8\u91cf\u7f3a\u5931\u7b49\u56e0\u7d20\u5bf9\u534f\u8c03\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u4e0d\u540c\u4eba\u53e3\u7279\u5f81\u5bf9\u65b9\u6cd5\u6548\u679c\u7684\u5177\u4f53\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e94\u9879\u4f18\u5316\u5efa\u8bae\u3002", "conclusion": "\u4e3a\u63d0\u5347ComBAT\u5728\u5f00\u653e\u79d1\u5b66\u548c\u4e34\u5e8a\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u9700\u9075\u5faa\u4e94\u9879\u5efa\u8bae\u4ee5\u786e\u4fdd\u534f\u8c03\u7684\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u3002", "keywords": "MRI, ComBAT, \u6570\u636e\u534f\u8c03, \u4eba\u53e3\u7279\u5f81, \u53ef\u91cd\u590d\u6027"}}
{"id": "2505.15441", "pdf": "https://arxiv.org/pdf/2505.15441", "abs": "https://arxiv.org/abs/2505.15441", "authors": ["David Nordstr\u00f6m", "Johan Edstedt", "Fredrik Kahl", "Georg B\u00f6kman"], "title": "Stronger ViTs With Octic Equivariance", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent efforts at scaling computer vision models have established Vision\nTransformers (ViTs) as the leading architecture. ViTs incorporate weight\nsharing over image patches as an important inductive bias. In this work, we\nshow that ViTs benefit from incorporating equivariance under the octic group,\ni.e., reflections and 90-degree rotations, as a further inductive bias. We\ndevelop new architectures, octic ViTs, that use octic-equivariant layers and\nput them to the test on both supervised and self-supervised learning. Through\nextensive experiments on DeiT-III and DINOv2 training on ImageNet-1K, we show\nthat octic ViTs yield more computationally efficient networks while also\nimproving performance. In particular, we achieve approximately 40% reduction in\nFLOPs for ViT-H while simultaneously improving both classification and\nsegmentation results.", "AI": {"tldr": "Octic ViTs\u901a\u8fc7\u5f15\u5165\u516b\u5143\u7ec4\u7b49\u53d8\u6027\uff08\u53cd\u5c04\u548c90\u5ea6\u65cb\u8f6c\uff09\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\uff0c\u4f18\u5316\u4e86ViT\u67b6\u6784\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u6709\u63d0\u5347\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728ViT\u4e2d\u878d\u5165\u516b\u5143\u7ec4\u7b49\u53d8\u6027\u4f5c\u4e3a\u65b0\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86octic ViTs\u67b6\u6784\uff0c\u4f7f\u7528\u516b\u5143\u7ec4\u7b49\u53d8\u5c42\uff0c\u5e76\u5728DeiT-III\u548cDINOv2\u4e0a\u8fdb\u884c\u4e86\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u5b9e\u9a8c\u3002", "result": "\u5728ViT-H\u4e0a\u5b9e\u73b0\u4e86\u7ea640%\u7684FLOPs\u51cf\u5c11\uff0c\u540c\u65f6\u5206\u7c7b\u548c\u5206\u5272\u7ed3\u679c\u5747\u6709\u6240\u63d0\u5347\u3002", "conclusion": "Octic ViTs\u8bc1\u660e\u4e86\u516b\u5143\u7ec4\u7b49\u53d8\u6027\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86ViT\u7684\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u3002", "keywords": "Vision Transformers, \u516b\u5143\u7ec4\u7b49\u53d8\u6027, \u5f52\u7eb3\u504f\u7f6e, \u8ba1\u7b97\u6548\u7387, \u56fe\u50cf\u5206\u7c7b, \u5206\u5272"}}
{"id": "2505.14725", "pdf": "https://arxiv.org/pdf/2505.14725", "abs": "https://arxiv.org/abs/2505.14725", "authors": ["Xuejun Sun", "Yiran Song", "Xiaochen Zhou", "Ruilie Cai", "Yu Zhang", "Xinyi Li", "Rui Peng", "Jialiu Xie", "Yuanyuan Yan", "Muyao Tang", "Prem Lakshmanane", "Baiming Zou", "James S. Hagood", "Raymond J. Pickles", "Didong Li", "Fei Zou", "Xiaojing Zheng"], "title": "HR-VILAGE-3K3M: A Human Respiratory Viral Immunization Longitudinal Gene Expression Dataset for Systems Immunity", "categories": ["q-bio.GN", "cs.LG"], "comment": null, "summary": "Respiratory viral infections pose a global health burden, yet the cellular\nimmune responses driving protection or pathology remain unclear. Natural\ninfection cohorts often lack pre-exposure baseline data and structured temporal\nsampling. In contrast, inoculation and vaccination trials generate insightful\nlongitudinal transcriptomic data. However, the scattering of these datasets\nacross platforms, along with inconsistent metadata and preprocessing procedure,\nhinders AI-driven discovery. To address these challenges, we developed the\nHuman Respiratory Viral Immunization LongitudinAl Gene Expression\n(HR-VILAGE-3K3M) repository: an AI-ready, rigorously curated dataset that\nintegrates 14,136 RNA-seq profiles from 3,178 subjects across 66 studies\nencompassing over 2.56 million cells. Spanning vaccination, inoculation, and\nmixed exposures, the dataset includes microarray, bulk RNA-seq, and single-cell\nRNA-seq from whole blood, PBMCs, and nasal swabs, sourced from GEO, ImmPort,\nand ArrayExpress. We harmonized subject-level metadata, standardized outcome\nmeasures, applied unified preprocessing pipelines with rigorous quality\ncontrol, and aligned all data to official gene symbols. To demonstrate the\nutility of HR-VILAGE-3K3M, we performed predictive modeling of vaccine\nresponders and evaluated batch-effect correction methods. Beyond these initial\ndemonstrations, it supports diverse systems immunology applications and\nbenchmarking of feature selection and transfer learning algorithms. Its scale\nand heterogeneity also make it ideal for pretraining foundation models of the\nhuman immune response and for advancing multimodal learning frameworks. As the\nlargest longitudinal transcriptomic resource for human respiratory viral\nimmunization, it provides an accessible platform for reproducible AI-driven\nresearch, accelerating systems immunology and vaccine development against\nemerging viral threats.", "AI": {"tldr": "HR-VILAGE-3K3M\u662f\u4e00\u4e2aAI\u5c31\u7eea\u3001\u4e25\u683c\u6574\u7406\u7684\u8f6c\u5f55\u7ec4\u6570\u636e\u96c6\uff0c\u6574\u5408\u4e86\u591a\u4e2a\u7814\u7a76\u4e2d\u768414,136\u4e2aRNA-seq\u6570\u636e\uff0c\u7528\u4e8e\u52a0\u901f\u547c\u5438\u7cfb\u7edf\u75c5\u6bd2\u611f\u67d3\u514d\u75ab\u7814\u7a76\u548c\u75ab\u82d7\u5f00\u53d1\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5206\u6563\u3001\u5143\u6570\u636e\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u4ee5\u652f\u6301AI\u9a71\u52a8\u7684\u514d\u75ab\u5e94\u7b54\u7814\u7a76\u3002", "method": "\u6574\u5408\u4e8666\u9879\u7814\u7a76\u768414,136\u4e2aRNA-seq\u6570\u636e\uff0c\u7edf\u4e00\u9884\u5904\u7406\u548c\u5143\u6570\u636e\u6807\u51c6\u5316\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86HR-VILAGE-3K3M\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u9884\u6d4b\u75ab\u82d7\u53cd\u5e94\u548c\u6279\u6548\u5e94\u77eb\u6b63\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u547c\u5438\u7cfb\u7edf\u75c5\u6bd2\u514d\u75ab\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u5e73\u53f0\uff0c\u652f\u6301\u7cfb\u7edf\u514d\u75ab\u5b66\u548c\u75ab\u82d7\u5f00\u53d1\u3002", "keywords": "\u547c\u5438\u7cfb\u7edf\u75c5\u6bd2, RNA-seq, \u7cfb\u7edf\u514d\u75ab\u5b66, \u75ab\u82d7\u5f00\u53d1, AI"}}
{"id": "2505.15070", "pdf": "https://arxiv.org/pdf/2505.15070", "abs": "https://arxiv.org/abs/2505.15070", "authors": ["Aldo Porco", "Dhruv Mehra", "Igor Malioutov", "Karthik Radhakrishnan", "Moniba Keymanesh", "Daniel Preo\u0163iuc-Pietro", "Sean MacAvaney", "Pengxiang Cheng"], "title": "An Alternative to FLOPS Regularization to Effectively Productionize SPLADE-Doc", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted as a short paper at SIGIR 2025", "summary": "Learned Sparse Retrieval (LSR) models encode text as weighted term vectors,\nwhich need to be sparse to leverage inverted index structures during retrieval.\nSPLADE, the most popular LSR model, uses FLOPS regularization to encourage\nvector sparsity during training. However, FLOPS regularization does not ensure\nsparsity among terms - only within a given query or document. Terms with very\nhigh Document Frequencies (DFs) substantially increase latency in production\nretrieval engines, such as Apache Solr, due to their lengthy posting lists. To\naddress the issue of high DFs, we present a new variant of FLOPS\nregularization: DF-FLOPS. This new regularization technique penalizes the usage\nof high-DF terms, thereby shortening posting lists and reducing retrieval\nlatency. Unlike other inference-time sparsification methods, such as stopword\nremoval, DF-FLOPS regularization allows for the selective inclusion of\nhigh-frequency terms in cases where the terms are truly salient. We find that\nDF-FLOPS successfully reduces the prevalence of high-DF terms and lowers\nretrieval latency (around 10x faster) in a production-grade engine while\nmaintaining effectiveness both in-domain (only a 2.2-point drop in MRR@10) and\ncross-domain (improved performance in 12 out of 13 tasks on which we tested).\nWith retrieval latencies on par with BM25, this work provides an important step\ntowards making LSR practical for deployment in production-grade search engines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a00\u758f\u68c0\u7d22\u6a21\u578b\u6b63\u5219\u5316\u65b9\u6cd5DF-FLOPS\uff0c\u65e8\u5728\u964d\u4f4e\u9ad8\u6587\u6863\u9891\u7387\uff08DF\uff09\u8bcd\u9879\u7684\u4f7f\u7528\uff0c\u4ece\u800c\u51cf\u5c11\u68c0\u7d22\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u9ad8\u6587\u6863\u9891\u7387\uff08DF\uff09\u8bcd\u9879\u4f1a\u5bfc\u81f4\u68c0\u7d22\u5ef6\u8fdf\u589e\u52a0\uff0c\u5f71\u54cd\u7a00\u758f\u68c0\u7d22\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86DF-FLOPS\u6b63\u5219\u5316\u6280\u672f\uff0c\u901a\u8fc7\u60e9\u7f5a\u9ad8-DF\u8bcd\u9879\u7684\u4f7f\u7528\uff0c\u7f29\u77ed\u5012\u6392\u7d22\u5f15\u4e2d\u7684\u5e16\u5b50\u5217\u8868\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDF-FLOPS\u663e\u8457\u964d\u4f4e\u4e86\u9ad8-DF\u8bcd\u9879\u7684\u51fa\u73b0\u9891\u7387\u548c\u68c0\u7d22\u5ef6\u8fdf\uff08\u7ea6\u5feb10\u500d\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u68c0\u7d22\u6548\u679c\u3002", "conclusion": "DF-FLOPS\u4e3a\u7a00\u758f\u68c0\u7d22\u6a21\u578b\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002", "keywords": "\u7a00\u758f\u68c0\u7d22\u3001FLOPS\u6b63\u5219\u5316\u3001\u9ad8\u6587\u6863\u9891\u7387\u3001\u68c0\u7d22\u5ef6\u8fdf\u3001\u5012\u6392\u7d22\u5f15"}}
{"id": "2505.14731", "pdf": "https://arxiv.org/pdf/2505.14731", "abs": "https://arxiv.org/abs/2505.14731", "authors": ["Ningning Yao", "Huan Xi", "Lang Chen", "Zhe Song", "Jian Li", "Yulei Chen", "Baocai Guo", "Yuanhang Zhang", "Tong Zhu", "Pengfei Li", "Daniel Rosenfeld", "John H. Seinfeld", "Shaocai Yu"], "title": "Effective climate policies for major emission reductions of ozone precursors: Global evidence from two decades", "categories": ["stat.AP", "cs.LG"], "comment": "There are 30 pages of 12 figures", "summary": "Despite policymakers deploying various tools to mitigate emissions of ozone\n(O\\textsubscript{3}) precursors, such as nitrogen oxides (NO\\textsubscript{x}),\ncarbon monoxide (CO), and volatile organic compounds (VOCs), the effectiveness\nof policy combinations remains uncertain. We employ an integrated framework\nthat couples structural break detection with machine learning to pinpoint\neffective interventions across the building, electricity, industrial, and\ntransport sectors, identifying treatment effects as abrupt changes without\nprior assumptions about policy treatment assignment and timing. Applied to two\ndecades of global O\\textsubscript{3} precursor emissions data, we detect 78,\n77, and 78 structural breaks for NO\\textsubscript{x}, CO, and VOCs,\ncorresponding to cumulative emission reductions of 0.96-0.97 Gt, 2.84-2.88 Gt,\nand 0.47-0.48 Gt, respectively. Sector-level analysis shows that electricity\nsector structural policies cut NO\\textsubscript{x} by up to 32.4\\%, while in\nbuildings, developed countries combined adoption subsidies with carbon taxes to\nachieve 42.7\\% CO reductions and developing countries used financing plus fuel\ntaxes to secure 52.3\\%. VOCs abatement peaked at 38.5\\% when fossil-fuel\nsubsidy reforms were paired with financial incentives. Finally, hybrid\nstrategies merging non-price measures (subsidies, bans, mandates) with pricing\ninstruments delivered up to an additional 10\\% co-benefit. These findings guide\nthe sequencing and complementarity of context-specific policy portfolios for\nO\\textsubscript{3} precursor mitigation.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u65ad\u70b9\u68c0\u6d4b\u548c\u673a\u5668\u5b66\u4e60\uff0c\u5206\u6790\u4e86\u5168\u7403\u81ed\u6c27\u524d\u4f53\u6392\u653e\u6570\u636e\u7684\u653f\u7b56\u5e72\u9884\u6548\u679c\uff0c\u53d1\u73b0\u7535\u529b\u3001\u5efa\u7b51\u7b49\u884c\u4e1a\u7684\u4e0d\u540c\u653f\u7b56\u7ec4\u5408\u80fd\u663e\u8457\u51cf\u5c11\u6392\u653e\u3002", "motivation": "\u5c3d\u7ba1\u653f\u7b56\u5236\u5b9a\u8005\u4f7f\u7528\u4e86\u591a\u79cd\u5de5\u5177\u6765\u51cf\u5c11\u81ed\u6c27\u524d\u4f53\u6392\u653e\uff0c\u4f46\u653f\u7b56\u7ec4\u5408\u7684\u6709\u6548\u6027\u4ecd\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u65ad\u70b9\u68c0\u6d4b\u4e0e\u673a\u5668\u5b66\u4e60\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u5206\u6790\u5168\u7403\u81ed\u6c27\u524d\u4f53\u6392\u653e\u6570\u636e\uff0c\u8bc6\u522b\u653f\u7b56\u5e72\u9884\u7684\u7a81\u53d1\u53d8\u5316\u3002", "result": "\u68c0\u6d4b\u523078\u300177\u548c78\u4e2aNOx\u3001CO\u548cVOCs\u7684\u7ed3\u6784\u65ad\u70b9\uff0c\u5206\u522b\u7d2f\u8ba1\u51cf\u5c11\u6392\u653e0.96-0.97 Gt\u30012.84-2.88 Gt\u548c0.47-0.48 Gt\u3002\u7535\u529b\u884c\u4e1a\u653f\u7b56\u6700\u9ad8\u51cf\u5c11NOx 32.4%\uff0c\u5efa\u7b51\u884c\u4e1a\u653f\u7b56\u7ec4\u5408\u5728\u53d1\u8fbe\u56fd\u5bb6\u548c\u53d1\u5c55\u4e2d\u56fd\u5bb6\u5206\u522b\u51cf\u5c11CO 42.7%\u548c52.3%\u3002", "conclusion": "\u6df7\u5408\u7b56\u7565\u7ed3\u5408\u975e\u4ef7\u683c\u548c\u4ef7\u683c\u5de5\u5177\u53ef\u989d\u5916\u5e26\u676510%\u7684\u534f\u540c\u6548\u76ca\uff0c\u4e3a\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u3002", "keywords": "\u81ed\u6c27\u524d\u4f53\u3001\u7ed3\u6784\u65ad\u70b9\u68c0\u6d4b\u3001\u673a\u5668\u5b66\u4e60\u3001\u653f\u7b56\u5e72\u9884\u3001\u6392\u653e\u51cf\u5c11"}}
{"id": "2505.15447", "pdf": "https://arxiv.org/pdf/2505.15447", "abs": "https://arxiv.org/abs/2505.15447", "authors": ["Ziqiang Xu", "Qi Dai", "Tian Xie", "Yifan Yang", "Kai Qiu", "DongDong Chen", "Zuxuan Wu", "Chong Luo"], "title": "ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video understanding is inherently intention-driven-humans naturally focus on\nrelevant frames based on their goals. Recent advancements in multimodal large\nlanguage models (MLLMs) have enabled flexible query-driven reasoning; however,\nvideo-based frameworks like Video Chain-of-Thought lack direct training signals\nto effectively identify relevant frames. Current approaches often rely on\nheuristic methods or pseudo-label supervised annotations, which are both costly\nand limited in scalability across diverse scenarios. To overcome these\nchallenges, we introduce ViaRL, the first framework to leverage rule-based\nreinforcement learning (RL) for optimizing frame selection in intention-driven\nvideo understanding. An iterated amplification strategy is adopted to perform\nalternating cyclic training in the video CoT system, where each component\nundergoes iterative cycles of refinement to improve its capabilities. ViaRL\nutilizes the answer accuracy of a downstream model as a reward signal to train\na frame selector through trial-and-error, eliminating the need for expensive\nannotations while closely aligning with human-like learning processes.\nComprehensive experiments across multiple benchmarks, including VideoMME,\nLVBench, and MLVU, demonstrate that ViaRL consistently delivers superior\ntemporal grounding performance and robust generalization across diverse video\nunderstanding tasks, highlighting its effectiveness and scalability. Notably,\nViaRL achieves a nearly 15\\% improvement on Needle QA, a subset of MLVU, which\nis required to search a specific needle within a long video and regarded as one\nof the most suitable benchmarks for evaluating temporal grounding.", "AI": {"tldr": "ViaRL\u5229\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u610f\u56fe\u9a71\u52a8\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5e27\u9009\u62e9\uff0c\u65e0\u9700\u6602\u8d35\u6807\u6ce8\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u7406\u89e3\u4e2d\u5e27\u9009\u62e9\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\u6216\u4f2a\u6807\u6ce8\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u653e\u5927\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e0b\u6e38\u6a21\u578b\u7684\u51c6\u786e\u6027\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u8bad\u7ec3\u5e27\u9009\u62e9\u5668\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728Needle QA\u4efb\u52a1\u4e0a\u63d0\u5347\u8fd115%\u3002", "conclusion": "ViaRL\u6709\u6548\u4e14\u53ef\u6269\u5c55\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u7684\u65f6\u6001\u5b9a\u4f4d\u6027\u80fd\u3002", "keywords": "\u89c6\u9891\u7406\u89e3,\u5f3a\u5316\u5b66\u4e60,\u65f6\u6001\u5b9a\u4f4d,ViaRL"}}
{"id": "2505.14747", "pdf": "https://arxiv.org/pdf/2505.14747", "abs": "https://arxiv.org/abs/2505.14747", "authors": ["Fatemeh Chajaei", "Hossein Bagheri"], "title": "LOD1 3D City Model from LiDAR: The Impact of Segmentation Accuracy on Quality of Urban 3D Modeling and Morphology Extraction", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Three-dimensional reconstruction of buildings, particularly at Level of\nDetail 1 (LOD1), plays a crucial role in various applications such as urban\nplanning, urban environmental studies, and designing optimized transportation\nnetworks. This study focuses on assessing the potential of LiDAR data for\naccurate 3D building reconstruction at LOD1 and extracting morphological\nfeatures from these models. Four deep semantic segmentation models, U-Net,\nAttention U-Net, U-Net3+, and DeepLabV3+, were used, applying transfer learning\nto extract building footprints from LiDAR data. The results showed that U-Net3+\nand Attention U-Net outperformed the others, achieving IoU scores of 0.833 and\n0.814, respectively. Various statistical measures, including maximum, range,\nmode, median, and the 90th percentile, were used to estimate building heights,\nresulting in the generation of 3D models at LOD1. As the main contribution of\nthe research, the impact of segmentation accuracy on the quality of 3D building\nmodeling and the accuracy of morphological features like building area and\nexternal wall surface area was investigated. The results showed that the\naccuracy of building identification (segmentation performance) significantly\naffects the 3D model quality and the estimation of morphological features,\ndepending on the height calculation method. Overall, the UNet3+ method,\nutilizing the 90th percentile and median measures, leads to accurate height\nestimation of buildings and the extraction of morphological features.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86LiDAR\u6570\u636e\u5728LOD1\u7ea7\u522b3D\u5efa\u7b51\u91cd\u5efa\u4e2d\u7684\u6f5c\u529b\uff0c\u6bd4\u8f83\u4e86\u56db\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8868\u73b0\uff0c\u53d1\u73b0UNet3+\u548cAttention U-Net\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u63a2\u8ba8\u4e86\u5206\u5272\u7cbe\u5ea6\u5bf93D\u5efa\u6a21\u8d28\u91cf\u53ca\u5f62\u6001\u7279\u5f81\u4f30\u8ba1\u7684\u5f71\u54cd\u3002", "motivation": "3D\u5efa\u7b51\u91cd\u5efa\uff08\u5c24\u5176\u662fLOD1\u7ea7\u522b\uff09\u5728\u57ce\u5e02\u89c4\u5212\u3001\u73af\u5883\u7814\u7a76\u548c\u4ea4\u901a\u7f51\u7edc\u4f18\u5316\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LiDAR\u6570\u636e\u5728\u6b64\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u529b\u53ca\u5176\u5f62\u6001\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u6df1\u5ea6\u5b66\u4e60\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff08U-Net\u3001Attention U-Net\u3001U-Net3+\u548cDeepLabV3+\uff09\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u4eceLiDAR\u6570\u636e\u4e2d\u63d0\u53d6\u5efa\u7b51\u8f6e\u5ed3\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u7edf\u8ba1\u65b9\u6cd5\u4f30\u8ba1\u5efa\u7b51\u9ad8\u5ea6\uff0c\u751f\u6210LOD1\u7ea7\u522b\u76843D\u6a21\u578b\u3002", "result": "UNet3+\u548cAttention U-Net\u8868\u73b0\u6700\u4f18\uff0cIoU\u5206\u6570\u5206\u522b\u4e3a0.833\u548c0.814\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u5206\u5272\u7cbe\u5ea6\u663e\u8457\u5f71\u54cd3D\u5efa\u6a21\u8d28\u91cf\u548c\u5f62\u6001\u7279\u5f81\uff08\u5982\u5efa\u7b51\u9762\u79ef\u548c\u5916\u5899\u9762\u79ef\uff09\u7684\u4f30\u8ba1\u51c6\u786e\u6027\u3002", "conclusion": "UNet3+\u7ed3\u5408\u7b2c90\u767e\u5206\u4f4d\u6570\u548c\u4e2d\u4f4d\u6570\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u5efa\u7b51\u9ad8\u5ea6\u5e76\u63d0\u53d6\u5f62\u6001\u7279\u5f81\uff0c\u4e3a3D\u5efa\u7b51\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "keywords": "3D\u5efa\u7b51\u91cd\u5efa, LiDAR\u6570\u636e, \u6df1\u5ea6\u5b66\u4e60, \u8bed\u4e49\u5206\u5272, LOD1, \u5f62\u6001\u7279\u5f81"}}
{"id": "2505.15469", "pdf": "https://arxiv.org/pdf/2505.15469", "abs": "https://arxiv.org/abs/2505.15469", "authors": ["Jonathan Katzy", "Yongcheng Huang", "Gopal-Raj Panchu", "Maksym Ziemlewski", "Paris Loizides", "Sander Vermeulen", "Arie van Deursen", "Maliheh Izadi"], "title": "A Qualitative Investigation into LLM-Generated Multilingual Code Comments and Automatic Evaluation Metrics", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted PROMISE '25", "summary": "Large Language Models are essential coding assistants, yet their training is\npredominantly English-centric. In this study, we evaluate the performance of\ncode language models in non-English contexts, identifying challenges in their\nadoption and integration into multilingual workflows. We conduct an open-coding\nstudy to analyze errors in code comments generated by five state-of-the-art\ncode models, CodeGemma, CodeLlama, CodeQwen1.5, GraniteCode, and StarCoder2\nacross five natural languages: Chinese, Dutch, English, Greek, and Polish. Our\nstudy yields a dataset of 12,500 labeled generations, which we publicly\nrelease. We then assess the reliability of standard metrics in capturing\ncomment \\textit{correctness} across languages and evaluate their\ntrustworthiness as judgment criteria. Through our open-coding investigation, we\nidentified a taxonomy of 26 distinct error categories in model-generated code\ncomments. They highlight variations in language cohesion, informativeness, and\nsyntax adherence across different natural languages. Our analysis shows that,\nwhile these models frequently produce partially correct comments, modern neural\nmetrics fail to reliably differentiate meaningful completions from random\nnoise. Notably, the significant score overlap between expert-rated correct and\nincorrect comments calls into question the effectiveness of these metrics in\nassessing generated comments.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u5728\u975e\u82f1\u8bed\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u5b58\u5728\u6311\u6218\u3002", "motivation": "\u5927\u6a21\u578b\u8bad\u7ec3\u4e2d\u4ee5\u82f1\u8bed\u4e3a\u4e3b\uff0c\u975e\u82f1\u8bed\u73af\u5883\u7684\u4ee3\u7801\u6ce8\u91ca\u751f\u6210\u80fd\u529b\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5f00\u653e\u7f16\u7801\u7814\u7a76\u4e94\u79cd\u4ee3\u7801\u6a21\u578b\u5728\u4e94\u79cd\u8bed\u8a00\u768412,500\u6761\u751f\u6210\u6ce8\u91ca\u4e2d\u7684\u9519\u8bef\u3002", "result": "\u53d1\u73b026\u79cd\u9519\u8bef\u7c7b\u578b\uff0c\u73b0\u4ee3\u795e\u7ecf\u6307\u6807\u65e0\u6cd5\u53ef\u9760\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u6ce8\u91ca\u3002", "conclusion": "\u73b0\u6709\u6307\u6807\u5728\u591a\u8bed\u8a00\u6ce8\u91ca\u8bc4\u4f30\u4e2d\u6548\u679c\u5b58\u7591\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b,\u4ee3\u7801\u6ce8\u91ca,\u591a\u8bed\u8a00,\u8bc4\u4f30\u6307\u6807"}}
{"id": "2505.15158", "pdf": "https://arxiv.org/pdf/2505.15158", "abs": "https://arxiv.org/abs/2505.15158", "authors": ["Yunsheng Ma", "Burhaneddin Yaman", "Xin Ye", "Mahmut Yurt", "Jingru Luo", "Abhirup Mallik", "Ziran Wang", "Liu Ren"], "title": "ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving", "categories": ["cs.CV", "cs.CL"], "comment": "10 pages", "summary": "Recent advances have explored integrating large language models (LLMs) into\nend-to-end autonomous driving systems to enhance generalization and\ninterpretability. However, most existing approaches are limited to either\ndriving performance or vision-language reasoning, making it difficult to\nachieve both simultaneously. In this paper, we propose ALN-P3, a unified\nco-distillation framework that introduces cross-modal alignment between \"fast\"\nvision-based autonomous driving systems and \"slow\" language-driven reasoning\nmodules. ALN-P3 incorporates three novel alignment mechanisms: Perception\nAlignment (P1A), Prediction Alignment (P2A), and Planning Alignment (P3A),\nwhich explicitly align visual tokens with corresponding linguistic outputs\nacross the full perception, prediction, and planning stack. All alignment\nmodules are applied only during training and incur no additional costs during\ninference. Extensive experiments on four challenging benchmarks-nuScenes, Nu-X,\nTOD3Cap, and nuScenes QA-demonstrate that ALN-P3 significantly improves both\ndriving decisions and language reasoning, achieving state-of-the-art results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ALN-P3\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e0e\u8bed\u8a00\u63a8\u7406\u6a21\u5757\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u9a7e\u9a76\u51b3\u7b56\u4e0e\u8bed\u8a00\u63a8\u7406\u7684\u53cc\u91cd\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u6027\u80fd\u548c\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u80fd\u529b\uff0cALN-P3\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "ALN-P3\u901a\u8fc7\u4e09\u79cd\u5bf9\u9f50\u673a\u5236\uff08\u611f\u77e5\u3001\u9884\u6d4b\u3001\u89c4\u5212\u5bf9\u9f50\uff09\u5728\u8bad\u7ec3\u65f6\u5bf9\u9f50\u89c6\u89c9\u4e0e\u8bed\u8a00\u8f93\u51fa\uff0c\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u5f00\u9500\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cALN-P3\u663e\u8457\u63d0\u5347\u4e86\u9a7e\u9a76\u51b3\u7b56\u548c\u8bed\u8a00\u63a8\u7406\u80fd\u529b\uff0c\u8fbe\u5230\u6700\u4f18\u6548\u679c\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u8de8\u6a21\u6001\u5bf9\u9f50\u662f\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7efc\u5408\u6027\u80fd\u7684\u6709\u6548\u9014\u5f84\u3002", "keywords": "\u81ea\u52a8\u9a7e\u9a76\uff1b\u5927\u8bed\u8a00\u6a21\u578b\uff1b\u8de8\u6a21\u6001\u5bf9\u9f50\uff1b\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406"}}
{"id": "2505.14754", "pdf": "https://arxiv.org/pdf/2505.14754", "abs": "https://arxiv.org/abs/2505.14754", "authors": ["Andrey Alexandrov", "Giovanni Acampora", "Giovanni De Lellis", "Antonia Di Crescenzo", "Chiara Errico", "Daria Morozova", "Valeri Tioukov", "Autilia Vittiello"], "title": "Model-Independent Machine Learning Approach for Nanometric Axial Localization and Tracking", "categories": ["eess.IV", "astro-ph.IM", "cs.CV", "cs.LG", "physics.ins-det"], "comment": "11 pages, 4 figures, 1 table", "summary": "Accurately tracking particles and determining their position along the\noptical axis is a major challenge in optical microscopy, especially when\nextremely high precision is needed. In this study, we introduce a deep learning\napproach using convolutional neural networks (CNNs) that can determine axial\npositions from dual-focal plane images without relying on predefined models.\nOur method achieves an axial localization accuracy of 40 nanometers - six times\nbetter than traditional single-focal plane techniques. The model's simple\ndesign and strong performance make it suitable for a wide range of uses,\nincluding dark matter detection, proton therapy for cancer, and radiation\nprotection in space. It also shows promise in fields like biological imaging,\nmaterials science, and environmental monitoring. This work highlights how\nmachine learning can turn complex image data into reliable, precise\ninformation, offering a flexible and powerful tool for many scientific\napplications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u53cc\u7126\u5e73\u9762\u56fe\u50cf\u8f74\u5411\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7cbe\u5ea6\u8fbe40\u7eb3\u7c73\uff0c\u8fdc\u8d85\u4f20\u7edf\u6280\u672f\u3002", "motivation": "\u89e3\u51b3\u5149\u5b66\u663e\u5fae\u955c\u4e2d\u9ad8\u7cbe\u5ea6\u8f74\u5411\u5b9a\u4f4d\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u4ece\u53cc\u7126\u5e73\u9762\u56fe\u50cf\u4e2d\u76f4\u63a5\u786e\u5b9a\u8f74\u5411\u4f4d\u7f6e\u3002", "result": "\u8f74\u5411\u5b9a\u4f4d\u7cbe\u5ea6\u4e3a40\u7eb3\u7c73\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u9ad86\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u5728\u79d1\u5b66\u5e94\u7528\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u5f3a\u5927\u6027\u80fd\u3002", "keywords": "\u8f74\u5411\u5b9a\u4f4d, \u53cc\u7126\u5e73\u9762, \u6df1\u5ea6\u5b66\u4e60, \u5377\u79ef\u795e\u7ecf\u7f51\u7edc, \u5149\u5b66\u663e\u5fae\u955c"}}
{"id": "2505.14759", "pdf": "https://arxiv.org/pdf/2505.14759", "abs": "https://arxiv.org/abs/2505.14759", "authors": ["Yan Wang", "Ling Ding", "Tien N Nguyen", "Shaohua Wang", "Yanan Zheng"], "title": "LEANCODE: Understanding Models Better for Code Simplification of Pre-trained Large Language Models", "categories": ["cs.SE", "cs.LG"], "comment": "Accepted to ACL 2025 main conference", "summary": "Large Language Models for code often entail significant computational\ncomplexity, which grows significantly with the length of the input code\nsequence. We propose LeanCode for code simplification to reduce training and\nprediction time, leveraging code contexts in utilizing attention scores to\nrepresent the tokens' importance. We advocate for the selective removal of\ntokens based on the average context-aware attention scores rather than average\nscores across all inputs. LeanCode uses the attention scores of `CLS' tokens\nwithin the encoder for classification tasks, such as code search. It also\nemploys the encoder-decoder attention scores to determine token significance\nfor sequence-to-sequence tasks like code summarization.Our evaluation shows\nLeanCode's superiority over the SOTAs DietCode and Slimcode, with improvements\nof 60% and 16% for code search, and 29% and 27% for code summarization,\nrespectively.", "AI": {"tldr": "LeanCode\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u5206\u6570\u7684\u4ee3\u7801\u7b80\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u63d0\u5347\u4e86\u4ee3\u7801\u641c\u7d22\u548c\u603b\u7ed3\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u4ee3\u7801\u65f6\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u7279\u522b\u662f\u968f\u7740\u8f93\u5165\u4ee3\u7801\u5e8f\u5217\u957f\u5ea6\u7684\u589e\u52a0\u3002", "method": "\u5229\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u6ce8\u610f\u529b\u5206\u6570\u9009\u62e9\u6027\u5730\u79fb\u9664\u4e0d\u91cd\u8981\u7684\u4ee3\u7801\u6807\u8bb0\uff0c\u5e76\u5229\u7528CLS\u6807\u8bb0\u7684\u6ce8\u610f\u529b\u5206\u6570\u4f18\u5316\u5206\u7c7b\u4efb\u52a1\uff0c\u4ee5\u53ca\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6ce8\u610f\u529b\u5206\u6570\u4f18\u5316\u5e8f\u5217\u751f\u6210\u4efb\u52a1\u3002", "result": "LeanCode\u5728\u4ee3\u7801\u641c\u7d22\u4efb\u52a1\u4e0a\u6bd4SOTAs DietCode\u548cSlimcode\u5206\u522b\u63d0\u534760%\u548c16%\uff0c\u5728\u4ee3\u7801\u603b\u7ed3\u4efb\u52a1\u4e0a\u5206\u522b\u63d0\u534729%\u548c27%\u3002", "conclusion": "LeanCode\u901a\u8fc7\u6709\u6548\u7b80\u5316\u4ee3\u7801\u5e76\u5229\u7528\u6ce8\u610f\u529b\u5206\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u4efb\u52a1\u6027\u80fd\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u4ee3\u7801\u7b80\u5316,\u6ce8\u610f\u529b\u5206\u6570,\u4ee3\u7801\u641c\u7d22,\u4ee3\u7801\u603b\u7ed3"}}
{"id": "2505.15504", "pdf": "https://arxiv.org/pdf/2505.15504", "abs": "https://arxiv.org/abs/2505.15504", "authors": ["Conghao Xiong", "Zhengrui Guo", "Zhe Xu", "Yifei Zhang", "Raymond Kai-Yu Tong", "Si Yong Yeo", "Hao Chen", "Joseph J. Y. Sung", "Irwin King"], "title": "Beyond Linearity: Squeeze-and-Recalibrate Blocks for Few-Shot Whole Slide Image Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning has advanced computational pathology but expert annotations\nremain scarce. Few-shot learning mitigates annotation burdens yet suffers from\noverfitting and discriminative feature mischaracterization. In addition, the\ncurrent few-shot multiple instance learning (MIL) approaches leverage\npretrained vision-language models to alleviate these issues, but at the cost of\ncomplex preprocessing and high computational cost. We propose a\nSqueeze-and-Recalibrate (SR) block, a drop-in replacement for linear layers in\nMIL models to address these challenges. The SR block comprises two core\ncomponents: a pair of low-rank trainable matrices (squeeze pathway, SP) that\nreduces parameter count and imposes a bottleneck to prevent spurious feature\nlearning, and a frozen random recalibration matrix that preserves geometric\nstructure, diversifies feature directions, and redefines the optimization\nobjective for the SP. We provide theoretical guarantees that the SR block can\napproximate any linear mapping to arbitrary precision, thereby ensuring that\nthe performance of a standard MIL model serves as a lower bound for its\nSR-enhanced counterpart. Extensive experiments demonstrate that our SR-MIL\nmodels consistently outperform prior methods while requiring significantly\nfewer parameters and no architectural changes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdSqueeze-and-Recalibrate (SR)\u5757\uff0c\u4f5c\u4e3aMIL\u6a21\u578b\u4e2d\u7ebf\u6027\u5c42\u7684\u66ff\u4ee3\uff0c\u89e3\u51b3\u4e86\u5c0f\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u8fc7\u62df\u5408\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u4e13\u5bb6\u6807\u6ce8\u7a00\u7f3a\uff0c\u5c0f\u6837\u672c\u5b66\u4e60\u6613\u8fc7\u62df\u5408\u4e14\u7279\u5f81\u8868\u5f81\u4e0d\u4f73\uff0c\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002", "method": "SR\u5757\u5305\u542b\u4f4e\u79e9\u53ef\u8bad\u7ec3\u77e9\u9635\uff08\u538b\u7f29\u8def\u5f84\uff09\u548c\u51bb\u7ed3\u968f\u673a\u91cd\u6821\u51c6\u77e9\u9635\uff0c\u51cf\u5c11\u53c2\u6570\u5e76\u4fdd\u6301\u51e0\u4f55\u7ed3\u6784\u3002", "result": "SR-MIL\u6a21\u578b\u5728\u5b9e\u9a8c\u8868\u73b0\u4f18\u5f02\uff0c\u53c2\u6570\u66f4\u5c11\u4e14\u65e0\u9700\u67b6\u6784\u6539\u52a8\u3002", "conclusion": "SR\u5757\u6709\u6548\u63d0\u5347\u5c0f\u6837\u672cMIL\u6027\u80fd\uff0c\u7406\u8bba\u4fdd\u8bc1\u5176\u7075\u6d3b\u6027\u3002", "keywords": "deep learning, few-shot learning, computational pathology, MIL, SR block"}}
{"id": "2505.14806", "pdf": "https://arxiv.org/pdf/2505.14806", "abs": "https://arxiv.org/abs/2505.14806", "authors": ["Minglu Zhao", "Dehong Xu", "Deqian Kong", "Wen-Hao Zhang", "Ying Nian Wu"], "title": "Place Cells as Position Embeddings of Multi-Time Random Walk Transition Kernels for Path Planning", "categories": ["q-bio.NC", "cs.LG", "stat.ML"], "comment": null, "summary": "The hippocampus orchestrates spatial navigation through collective place cell\nencodings that form cognitive maps. We reconceptualize the population of place\ncells as position embeddings approximating multi-scale symmetric random walk\ntransition kernels: the inner product $\\langle h(x, t), h(y, t) \\rangle =\nq(y|x, t)$ represents normalized transition probabilities, where $h(x, t)$ is\nthe embedding at location $ x $, and $q(y|x, t)$ is the normalized symmetric\ntransition probability over time $t$. The time parameter $\\sqrt{t}$ defines a\nspatial scale hierarchy, mirroring the hippocampal dorsoventral axis. $q(y|x,\nt)$ defines spatial adjacency between $x$ and $y$ at scale or resolution\n$\\sqrt{t}$, and the pairwise adjacency relationships $(q(y|x, t), \\forall x,\ny)$ are reduced into individual embeddings $(h(x, t), \\forall x)$ that\ncollectively form a map of the environment at sale $\\sqrt{t}$. Our framework\nemploys gradient ascent on $q(y|x, t) = \\langle h(x, t), h(y, t)\\rangle$ with\nadaptive scale selection, choosing the time scale with maximal gradient at each\nstep for trap-free, smooth trajectories. Efficient matrix squaring $P_{2t} =\nP_t^2$ builds global representations from local transitions $P_1$ without\nmemorizing past trajectories, enabling hippocampal preplay-like path planning.\nThis produces robust navigation through complex environments, aligning with\nhippocampal navigation. Experimental results show that our model captures place\ncell properties -- field size distribution, adaptability, and remapping --\nwhile achieving computational efficiency. By modeling collective transition\nprobabilities rather than individual place fields, we offer a biologically\nplausible, scalable framework for spatial navigation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u79f0\u968f\u673a\u6e38\u8d70\u8f6c\u79fb\u6838\u7684\u591a\u5c3a\u5ea6\u4f4d\u7f6e\u5d4c\u5165\u6a21\u578b\uff0c\u7528\u4e8e\u91cd\u65b0\u6982\u5ff5\u5316\u6d77\u9a6c\u4f53\u4f4d\u7f6e\u7ec6\u80de\u7684\u7fa4\u4f53\u7f16\u7801\uff0c\u6a21\u62df\u8ba4\u77e5\u5730\u56fe\u7684\u5f62\u6210\u548c\u7a7a\u95f4\u5bfc\u822a\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5173\u6ce8\u5355\u4e2a\u4f4d\u7f6e\u7ec6\u80de\u7684\u7279\u6027\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7fa4\u4f53\u8f6c\u79fb\u6982\u7387\u7684\u5efa\u6a21\uff0c\u63d0\u4f9b\u4e00\u4e2a\u66f4\u7b26\u5408\u751f\u7269\u5b9e\u9645\u4e14\u53ef\u6269\u5c55\u7684\u7a7a\u95f4\u5bfc\u822a\u6846\u67b6\u3002", "method": "\u91c7\u7528\u68af\u5ea6\u4e0a\u5347\u6cd5\u548c\u81ea\u9002\u5e94\u5c3a\u5ea6\u9009\u62e9\uff0c\u901a\u8fc7\u77e9\u9635\u5e73\u65b9\u4ece\u5c40\u90e8\u8f6c\u79fb\u6784\u5efa\u5168\u5c40\u8868\u793a\uff0c\u65e0\u9700\u8bb0\u5fc6\u5386\u53f2\u8f68\u8ff9\uff0c\u5b9e\u73b0\u5e73\u6ed1\u5bfc\u822a\u8def\u5f84\u3002", "result": "\u6a21\u578b\u6210\u529f\u6355\u83b7\u4e86\u4f4d\u7f6e\u7ec6\u80de\u7684\u7279\u6027\uff08\u5982\u573a\u5927\u5c0f\u5206\u5e03\u3001\u9002\u5e94\u6027\u548c\u91cd\u6620\u5c04\uff09\uff0c\u5e76\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u7684\u5bfc\u822a\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u7fa4\u4f53\u8f6c\u79fb\u6982\u7387\u5efa\u6a21\u800c\u975e\u5355\u4e2a\u4f4d\u7f6e\u573a\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u751f\u7269\u5408\u7406\u4e14\u53ef\u6269\u5c55\u7684\u7a7a\u95f4\u5bfc\u822a\u6846\u67b6\u3002", "keywords": "\u6d77\u9a6c\u4f53,\u4f4d\u7f6e\u7ec6\u80de,\u5bf9\u79f0\u968f\u673a\u6e38\u8d70,\u591a\u5c3a\u5ea6\u5d4c\u5165,\u7a7a\u95f4\u5bfc\u822a"}}
{"id": "2505.14808", "pdf": "https://arxiv.org/pdf/2505.14808", "abs": "https://arxiv.org/abs/2505.14808", "authors": ["Soo Min Kwon", "Alec S. Xu", "Can Yaras", "Laura Balzano", "Qing Qu"], "title": "Out-of-Distribution Generalization of In-Context Learning: A Low-Dimensional Subspace Perspective", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "This work aims to demystify the out-of-distribution (OOD) capabilities of\nin-context learning (ICL) by studying linear regression tasks parameterized\nwith low-rank covariance matrices. With such a parameterization, we can model\ndistribution shifts as a varying angle between the subspace of the training and\ntesting covariance matrices. We prove that a single-layer linear attention\nmodel incurs a test risk with a non-negligible dependence on the angle,\nillustrating that ICL is not robust to such distribution shifts. However, using\nthis framework, we also prove an interesting property of ICL: when trained on\ntask vectors drawn from a union of low-dimensional subspaces, ICL can\ngeneralize to any subspace within their span, given sufficiently long prompt\nlengths. This suggests that the OOD generalization ability of Transformers may\nactually stem from the new task lying within the span of those encountered\nduring training. We empirically show that our results also hold for models such\nas GPT-2, and conclude with (i) experiments on how our observations extend to\nnonlinear function classes and (ii) results on how LoRA has the ability to\ncapture distribution shifts.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4f4e\u79e9\u534f\u65b9\u5dee\u77e9\u9635\u53c2\u6570\u5316\u7684\u7ebf\u6027\u56de\u5f52\u4efb\u52a1\uff0c\u63ed\u793a\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u80fd\u529b\u4e0a\u7684\u5c40\u9650\u6027\u53ca\u5176\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22ICL\u5728OOD\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5176\u5bf9\u534f\u65b9\u5dee\u77e9\u9635\u89d2\u5ea6\u53d8\u5316\u7684\u654f\u611f\u6027\uff0c\u4ee5\u53ca\u5176\u5728\u8bad\u7ec3\u4efb\u52a1\u5411\u91cf\u6240\u5f20\u6210\u7684\u5b50\u7a7a\u95f4\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u56de\u5f52\u4efb\u52a1\uff0c\u4f7f\u7528\u4f4e\u79e9\u534f\u65b9\u5dee\u77e9\u9635\u53c2\u6570\u5316\u5206\u5e03\u53d8\u5316\uff0c\u5206\u6790\u5355\u5c42\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u7684\u6d4b\u8bd5\u98ce\u9669\u4e0e\u534f\u65b9\u5dee\u77e9\u9635\u89d2\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "ICL\u5bf9\u534f\u65b9\u5dee\u77e9\u9635\u89d2\u5ea6\u53d8\u5316\u4e0d\u5177\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u8bad\u7ec3\u4efb\u52a1\u5411\u91cf\u5f20\u6210\u7684\u5b50\u7a7a\u95f4\u5185\u8868\u73b0\u51fa\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u7ed3\u679c\u5728GPT-2\u7b49\u6a21\u578b\u4e2d\u4e5f\u6210\u7acb\u3002", "conclusion": "Transformers\u7684OOD\u6cdb\u5316\u80fd\u529b\u53ef\u80fd\u6e90\u4e8e\u65b0\u4efb\u52a1\u5904\u4e8e\u8bad\u7ec3\u4efb\u52a1\u6240\u5f20\u6210\u7684\u5b50\u7a7a\u95f4\u5185\u3002\u6b64\u5916\uff0cLoRA\u80fd\u6355\u6349\u5206\u5e03\u53d8\u5316\u3002", "keywords": "\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u3001\u5206\u5e03\u5916\uff08OOD\uff09\u3001\u4f4e\u79e9\u534f\u65b9\u5dee\u77e9\u9635\u3001\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u3001\u6cdb\u5316\u80fd\u529b"}}
{"id": "2505.15298", "pdf": "https://arxiv.org/pdf/2505.15298", "abs": "https://arxiv.org/abs/2505.15298", "authors": ["Kangan Qian", "Sicong Jiang", "Yang Zhong", "Ziang Luo", "Zilin Huang", "Tianze Zhu", "Kun Jiang", "Mengmeng Yang", "Zheng Fu", "Jinyu Miao", "Yining Shi", "He Zhe Lim", "Li Liu", "Tianbao Zhou", "Hongyi Wang", "Huang Yu", "Yifei Hu", "Guang Li", "Guang Chen", "Hao Ye", "Lijun Sun", "Diange Yang"], "title": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "categories": ["cs.RO", "cs.CL", "cs.CV"], "comment": "18 pages, 8 figures", "summary": "Vision-Language Models (VLMs) show promise for autonomous driving, yet their\nstruggle with hallucinations, inefficient reasoning, and limited real-world\nvalidation hinders accurate perception and robust step-by-step reasoning. To\novercome this, we introduce \\textbf{AgentThink}, a pioneering unified framework\nthat, for the first time, integrates Chain-of-Thought (CoT) reasoning with\ndynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's\ncore innovations include: \\textbf{(i) Structured Data Generation}, by\nestablishing an autonomous driving tool library to automatically construct\nstructured, self-verified reasoning data explicitly incorporating tool usage\nfor diverse driving scenarios; \\textbf{(ii) A Two-stage Training Pipeline},\nemploying Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO) to equip VLMs with the capability for autonomous tool invocation; and\n\\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel\nmulti-tool assessment protocol to rigorously evaluate the model's tool\ninvocation and utilization. Experiments on the DriveLMM-o1 benchmark\ndemonstrate AgentThink significantly boosts overall reasoning scores by\n\\textbf{53.91\\%} and enhances answer accuracy by \\textbf{33.54\\%}, while\nmarkedly improving reasoning quality and consistency. Furthermore, ablation\nstudies and robust zero-shot/few-shot generalization experiments across various\nbenchmarks underscore its powerful capabilities. These findings highlight a\npromising trajectory for developing trustworthy and tool-aware autonomous\ndriving models.", "AI": {"tldr": "AgentThink\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u548c\u52a8\u6001\u5de5\u5177\u8c03\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u7684\u63a8\u7406\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b58\u5728\u5e7b\u89c9\u3001\u4f4e\u6548\u63a8\u7406\u548c\u7f3a\u4e4f\u771f\u5b9e\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u611f\u77e5\u548c\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u6570\u636e\u751f\u6210\u3001\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u548c\u4ee3\u7406\u5f0f\u5de5\u5177\u4f7f\u7528\u8bc4\u4f30\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548cGRPO\u4f18\u5316\uff0c\u8bad\u7ec3\u6a21\u578b\u81ea\u4e3b\u8c03\u7528\u5de5\u5177\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cAgentThink\u5728\u63a8\u7406\u5206\u6570\u4e0a\u63d0\u5347\u4e8653.91%\uff0c\u7b54\u6848\u51c6\u786e\u6027\u63d0\u9ad8\u4e8633.54%\uff0c\u540c\u65f6\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AgentThink\u4e3a\u5f00\u53d1\u53ef\u4fe1\u8d56\u3001\u5de5\u5177\u611f\u77e5\u7684\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u81ea\u52a8\u9a7e\u9a76, \u94fe\u5f0f\u601d\u7ef4\u63a8\u7406, \u5de5\u5177\u8c03\u7528"}}
{"id": "2505.15517", "pdf": "https://arxiv.org/pdf/2505.15517", "abs": "https://arxiv.org/abs/2505.15517", "authors": ["Kaiyuan Chen", "Shuangyu Xie", "Zehan Ma", "Ken Goldberg"], "title": "Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) acquire real-world knowledge and general\nreasoning ability through Internet-scale image-text corpora. They can augment\nrobotic systems with scene understanding and task planning, and assist\nvisuomotor policies that are trained on robot trajectory data. We explore the\nreverse paradigm - using rich, real, multi-modal robot trajectory data to\nenhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual\nQuestion Answering (VQA) dataset generation framework for VLMs. Given a human\ntele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual\nand non-descriptive sensory modalities, such as end-effector pose, gripper\naperture, and force sensing. Based on these modalities, it segments the robot\ntrajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses\nscene and interaction understanding to identify 3D properties of the robot,\ntask goal, and the target object. The properties are used to generate\nrepresentative VQA queries - images with textural multiple-choice questions -\nbased on spatial, goal-conditioned, and interaction reasoning question\ntemplates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710\nquestions covering 463 distinct scenes and 3,396 robotic manipulation tasks\nfrom 176k real robot trajectories. Results suggest that Robo2VLM-1 can\nbenchmark and improve VLM capabilities in spatial and interaction reasoning.", "AI": {"tldr": "Robo2VLM \u662f\u4e00\u4e2a\u5229\u7528\u673a\u5668\u4eba\u8f68\u8ff9\u6570\u636e\u751f\u6210\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u6570\u636e\u96c6\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u548c\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u3002\u901a\u8fc7\u673a\u5668\u4eba\u4f20\u611f\u5668\u7684\u975e\u89c6\u89c9\u6570\u636e\uff0c\u751f\u6210\u591a\u6a21\u6001\u95ee\u9898\uff0c\u63d0\u5347 VLMs \u5728\u7a7a\u95f4\u548c\u4ea4\u4e92\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u901a\u8fc7\u5229\u7528\u771f\u5b9e\u673a\u5668\u4eba\u8f68\u8ff9\u6570\u636e\uff0c\u589e\u5f3a VLMs \u5728\u573a\u666f\u7406\u89e3\u548c\u4efb\u52a1\u89c4\u5212\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5f25\u8865\u4f20\u7edf\u6570\u636e\u96c6\u7684\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8e\u673a\u5668\u4eba\u4f20\u611f\u5668\u7684\u975e\u89c6\u89c9\u6570\u636e\uff08\u5982\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\u3001\u5939\u6301\u5668\u5f00\u5408\u3001\u529b\u4f20\u611f\uff09\uff0c\u5c06\u8f68\u8ff9\u5206\u6bb5\u5e76\u751f\u6210 VQA \u67e5\u8be2\uff0c\u5305\u62ec\u7a7a\u95f4\u3001\u76ee\u6807\u548c\u4ea4\u4e92\u63a8\u7406\u95ee\u9898\u3002", "result": "\u6784\u5efa\u4e86 Robo2VLM-1 \u6570\u636e\u96c6\uff0c\u5305\u542b 684,710 \u4e2a\u95ee\u9898\u548c 176k \u771f\u5b9e\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u8bc1\u660e\u5176\u80fd\u6709\u6548\u8bc4\u4f30\u548c\u63d0\u5347 VLMs \u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "Robo2VLM \u901a\u8fc7\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u4e30\u5bcc\u4e86 VLMs \u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u63a8\u52a8\u4e86\u5176\u5728\u7a7a\u95f4\u548c\u4ea4\u4e92\u63a8\u7406\u9886\u57df\u7684\u8fdb\u5c55\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b,\u673a\u5668\u4eba\u8f68\u8ff9,VQA \u6570\u636e\u96c6,\u7a7a\u95f4\u63a8\u7406,\u4ea4\u4e92\u63a8\u7406"}}
{"id": "2505.15365", "pdf": "https://arxiv.org/pdf/2505.15365", "abs": "https://arxiv.org/abs/2505.15365", "authors": ["Stefan Pasch"], "title": "AI vs. Human Judgment of Content Moderation: LLM-as-a-Judge and Ethics-Based Response Refusals", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in high-stakes\nsettings, their ability to refuse ethically sensitive prompts-such as those\ninvolving hate speech or illegal activities-has become central to content\nmoderation and responsible AI practices. While refusal responses can be viewed\nas evidence of ethical alignment and safety-conscious behavior, recent research\nsuggests that users may perceive them negatively. At the same time, automated\nassessments of model outputs are playing a growing role in both evaluation and\ntraining. In particular, LLM-as-a-Judge frameworks-in which one model is used\nto evaluate the output of another-are now widely adopted to guide benchmarking\nand fine-tuning. This paper examines whether such model-based evaluators assess\nrefusal responses differently than human users. Drawing on data from Chatbot\nArena and judgments from two AI judges (GPT-4o and Llama 3 70B), we compare how\ndifferent types of refusals are rated. We distinguish ethical refusals, which\nexplicitly cite safety or normative concerns (e.g., \"I can't help with that\nbecause it may be harmful\"), and technical refusals, which reflect system\nlimitations (e.g., \"I can't answer because I lack real-time data\"). We find\nthat LLM-as-a-Judge systems evaluate ethical refusals significantly more\nfavorably than human users, a divergence not observed for technical refusals.\nWe refer to this divergence as a moderation bias-a systematic tendency for\nmodel-based evaluators to reward refusal behaviors more than human users do.\nThis raises broader questions about transparency, value alignment, and the\nnormative assumptions embedded in automated evaluation systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u62d2\u7edd\u654f\u611f\u63d0\u793a\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u8bc4\u4f30\u8005\u4e0e\u4eba\u7c7b\u7528\u6237\u5bf9\u4f26\u7406\u62d2\u7edd\u7684\u8bc4\u4ef7\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u2018\u8c03\u8282\u504f\u89c1\u2019\u7684\u73b0\u8c61\u3002", "motivation": "\u968f\u7740LLM\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u62d2\u7edd\u654f\u611f\u63d0\u793a\u7684\u80fd\u529b\u5bf9\u5185\u5bb9\u5ba1\u6838\u548c\u8d1f\u8d23\u4efbAI\u5b9e\u8df5\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7528\u6237\u53ef\u80fd\u5bf9\u62d2\u7edd\u53cd\u5e94\u6301\u8d1f\u9762\u770b\u6cd5\uff0c\u800c\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982LLM-as-a-Judge\uff09\u7684\u666e\u53ca\u4f7f\u5f97\u7814\u7a76\u6a21\u578b\u8bc4\u4f30\u8005\u4e0e\u4eba\u7c7b\u7528\u6237\u8bc4\u4ef7\u7684\u5dee\u5f02\u6210\u4e3a\u5fc5\u8981\u3002", "method": "\u7814\u7a76\u901a\u8fc7Chatbot Arena\u7684\u6570\u636e\u548c\u4e24\u4e2aAI\u8bc4\u4f30\u6a21\u578b\uff08GPT-4o\u548cLlama 3 70B\uff09\u7684\u8bc4\u5224\uff0c\u6bd4\u8f83\u4e86\u4f26\u7406\u62d2\u7edd\u548c\u6280\u672f\u62d2\u7edd\u7684\u8bc4\u5206\u5dee\u5f02\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u8bc4\u4f30\u8005\u5bf9\u4f26\u7406\u62d2\u7edd\u7684\u8bc4\u4ef7\u660e\u663e\u9ad8\u4e8e\u4eba\u7c7b\u7528\u6237\uff0c\u800c\u5bf9\u6280\u672f\u62d2\u7edd\u7684\u8bc4\u4ef7\u5219\u65e0\u6b64\u5dee\u5f02\uff0c\u8fd9\u79cd\u5dee\u5f02\u88ab\u79f0\u4e3a\u2018\u8c03\u8282\u504f\u89c1\u2019\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u81ea\u52a8\u5316\u8bc4\u4f30\u7cfb\u7edf\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u900f\u660e\u5ea6\u3001\u4ef7\u503c\u89c2\u5bf9\u9f50\u548c\u89c4\u8303\u5047\u8bbe\u95ee\u9898\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u5185\u5bb9\u5ba1\u6838,\u4f26\u7406\u5bf9\u9f50,\u6a21\u578b\u8bc4\u4f30,\u8c03\u8282\u504f\u89c1"}}
{"id": "2505.14867", "pdf": "https://arxiv.org/pdf/2505.14867", "abs": "https://arxiv.org/abs/2505.14867", "authors": ["So Won Jeong", "Claire Donnat"], "title": "LOBSTUR: A Local Bootstrap Framework for Tuning Unsupervised Representations in Graph Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) are increasingly used in conjunction with\nunsupervised learning techniques to learn powerful node representations, but\ntheir deployment is hindered by their high sensitivity to hyperparameter tuning\nand the absence of established methodologies for selecting the optimal models.\nTo address these challenges, we propose LOBSTUR-GNN ({\\bf Lo}cal {\\bf B}oot{\\bf\ns}trap for {\\bf T}uning {\\bf U}nsupervised {\\bf R}epresentations in GNNs) i), a\nnovel framework designed to adapt bootstrapping techniques for unsupervised\ngraph representation learning. LOBSTUR-GNN tackles two main challenges: (a)\nadapting the bootstrap edge and feature resampling process to account for local\ngraph dependencies in creating alternative versions of the same graph, and (b)\nestablishing robust metrics for evaluating learned representations without\nground-truth labels. Using locally bootstrapped resampling and leveraging\nCanonical Correlation Analysis (CCA) to assess embedding consistency, LOBSTUR\nprovides a principled approach for hyperparameter tuning in unsupervised GNNs.\nWe validate the effectiveness and efficiency of our proposed method through\nextensive experiments on established academic datasets, showing an 65.9\\%\nimprovement in the classification accuracy compared to an uninformed selection\nof hyperparameters. Finally, we deploy our framework on a real-world\napplication, thereby demonstrating its validity and practical utility in\nvarious settings. \\footnote{The code is available at\n\\href{https://github.com/sowonjeong/lobstur-graph-bootstrap}{github.com/sowonjeong/lobstur-graph-bootstrap}.}", "AI": {"tldr": "\u7814\u7a76\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5728\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u8d85\u53c2\u6570\u654f\u611f\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLOBSTUR-GNN\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u5c40\u90e8\u5f15\u5bfc\u62bd\u6837\u548cCCA\u5206\u6790\u4f18\u5316\u8d85\u53c2\u6570\u9009\u62e9\u548c\u8868\u793a\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u663e\u793a\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u9ad8\u4e8665.9%\u3002", "motivation": "GNN\u5728\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u90e8\u7f72\u53d7\u5230\u8d85\u53c2\u6570\u654f\u611f\u6027\u548c\u7f3a\u4e4f\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\u8bba\u7684\u5236\u7ea6\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faLOBSTUR-GNN\u6846\u67b6\uff0c\u7ed3\u5408\u5c40\u90e8\u5f15\u5bfc\u8fb9\u548c\u7279\u5f81\u91cd\u62bd\u6837\uff0c\u5229\u7528CCA\u8bc4\u4f30\u5d4c\u5165\u4e00\u81f4\u6027\uff0c\u4ee5\u4f18\u5316\u8d85\u53c2\u6570\u9009\u62e9\u3002", "result": "\u5728\u5b66\u672f\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u4e0e\u672a\u4f18\u5316\u7684\u8d85\u53c2\u6570\u9009\u62e9\u76f8\u6bd4\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u9ad8\u4e8665.9%\u3002", "conclusion": "LOBSTUR-GNN\u4e3a\u65e0\u76d1\u7763GNN\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "keywords": "\u56fe\u795e\u7ecf\u7f51\u7edc\u3001\u65e0\u76d1\u7763\u5b66\u4e60\u3001\u8d85\u53c2\u6570\u8c03\u4f18\u3001\u5f15\u5bfc\u62bd\u6837\u3001CCA"}}
{"id": "2505.14924", "pdf": "https://arxiv.org/pdf/2505.14924", "abs": "https://arxiv.org/abs/2505.14924", "authors": ["Shashwat Khandelwal", "Shreejith Shanker"], "title": "SecCAN: An Extended CAN Controller with Embedded Intrusion Detection", "categories": ["eess.SY", "cs.AR", "cs.LG", "cs.SY"], "comment": "4 pages, 3 figures, 3 tables, Accepted in IEEE Embedded Systems\n  Letters (https://ieee-ceda.org/publication/esl)", "summary": "Recent research has highlighted the vulnerability of in-vehicle network\nprotocols such as controller area networks (CAN) and proposed machine\nlearning-based intrusion detection systems (IDSs) as an effective mitigation\ntechnique. However, their efficient integration into vehicular architecture is\nnon-trivial, with existing methods relying on electronic control units\n(ECUs)-coupled IDS accelerators or dedicated ECUs as IDS accelerators. Here,\ninitiating IDS requires complete reception of a CAN message from the\ncontroller, incurring data movement and software overheads. In this paper, we\npresent SecCAN, a novel CAN controller architecture that embeds IDS capability\nwithin the datapath of the controller. This integration allows IDS to tap\nmessages directly from within the CAN controller as they are received from the\nbus, removing overheads incurred by existing ML-based IDSs. A custom-quantised\nmachine-learning accelerator is developed as the IDS engine and embedded into\nSecCAN's receive data path, with optimisations to overlap the IDS inference\nwith the protocol's reception window. We implement SecCAN on AMD XCZU7EV FPGA\nto quantify its performance and benefits in hardware, using multiple attack\ndatasets. We show that SecCAN can completely hide the IDS latency within the\nCAN reception window for all CAN packet sizes and detect multiple attacks with\nstate-of-the-art accuracy with zero software overheads on the ECU and low\nenergy overhead (73.7 uJ per message) for IDS inference. Also, SecCAN incurs\nlimited resource overhead compared to a standard CAN controller (< 30% LUT, <\n1% FF), making it ideally suited for automotive deployment.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86SecCAN\uff0c\u4e00\u79cd\u5c06\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\uff08IDS\uff09\u5d4c\u5165CAN\u63a7\u5236\u5668\u6570\u636e\u8def\u5f84\u7684\u65b0\u67b6\u6784\uff0c\u4ee5\u51cf\u5c11\u73b0\u6709\u673a\u5668\u5b66\u4e60IDS\u7684\u8f6f\u4ef6\u548c\u6570\u636e\u79fb\u52a8\u5f00\u9500\uff0c\u5e76\u901a\u8fc7FPGA\u5b9e\u73b0\u8bc1\u660e\u4e86\u5176\u9ad8\u6548\u6027\u548c\u4f4e\u80fd\u8017\u3002", "motivation": "\u73b0\u6709\u8f66\u8f86\u7f51\u7edc\u534f\u8bae\uff08\u5982CAN\uff09\u6613\u53d7\u653b\u51fb\uff0c\u800c\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60IDS\u96c6\u6210\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u79fb\u52a8\u548c\u8f6f\u4ef6\u5f00\u9500\u5927\u7b49\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1SecCAN\u67b6\u6784\uff0c\u5c06\u5b9a\u5236\u91cf\u5316\u7684\u673a\u5668\u5b66\u4e60\u52a0\u901f\u5668\u5d4c\u5165CAN\u63a7\u5236\u5668\u63a5\u6536\u6570\u636e\u8def\u5f84\uff0c\u4e0e\u534f\u8bae\u63a5\u6536\u7a97\u53e3\u91cd\u53e0\u8fdb\u884cIDS\u63a8\u65ad\u3002", "result": "SecCAN\u5728FPGA\u4e0a\u5b9e\u73b0\uff0c\u5b8c\u5168\u9690\u85cfIDS\u5ef6\u8fdf\uff0c\u68c0\u6d4b\u653b\u51fb\u51c6\u786e\u7387\u9ad8\uff0c\u8f6f\u4ef6\u5f00\u9500\u4e3a\u96f6\uff0c\u80fd\u8017\u4f4e\uff0873.7 uJ/\u6d88\u606f\uff09\uff0c\u8d44\u6e90\u5f00\u9500\u5c0f\uff08<30% LUT\uff0c<1% FF\uff09\u3002", "conclusion": "SecCAN\u662f\u4e00\u79cd\u9002\u5408\u6c7d\u8f66\u90e8\u7f72\u7684\u9ad8\u6548\u3001\u4f4e\u5f00\u9500CAN\u63a7\u5236\u5668IDS\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "CAN, \u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf, \u673a\u5668\u5b66\u4e60\u52a0\u901f\u5668, FPGA"}}
{"id": "2505.15558", "pdf": "https://arxiv.org/pdf/2505.15558", "abs": "https://arxiv.org/abs/2505.15558", "authors": ["Kaiyuan Chen", "Letian Fu", "David Huang", "Yanxiang Zhang", "Lawrence Yunliang Chen", "Huang Huang", "Kush Hari", "Ashwin Balakrishna", "Ted Xiao", "Pannag R Sanketi", "John Kubiatowicz", "Ken Goldberg"], "title": "Robo-DM: Data Management For Large Robot Datasets", "categories": ["cs.RO", "cs.AI", "cs.DB", "cs.LG"], "comment": "Best paper finalist of IEEE ICRA 2025", "summary": "Recent results suggest that very large datasets of teleoperated robot\ndemonstrations can be used to train transformer-based models that have the\npotential to generalize to new scenes, robots, and tasks. However, curating,\ndistributing, and loading large datasets of robot trajectories, which typically\nconsist of video, textual, and numerical modalities - including streams from\nmultiple cameras - remains challenging. We propose Robo-DM, an efficient\nopen-source cloud-based data management toolkit for collecting, sharing, and\nlearning with robot data. With Robo-DM, robot datasets are stored in a\nself-contained format with Extensible Binary Meta Language (EBML). Robo-DM can\nsignificantly reduce the size of robot trajectory data, transfer costs, and\ndata load time during training. Compared to the RLDS format used in OXE\ndatasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x\n(lossless). Robo-DM also accelerates data retrieval by load-balancing video\ndecoding with memory-mapped decoding caches. Compared to LeRobot, a framework\nthat also uses lossy video compression, Robo-DM is up to 50x faster when\ndecoding sequentially. We physically evaluate a model trained by Robo-DM with\nlossy compression, a pick-and-place task, and In-Context Robot Transformer.\nRobo-DM uses 75x compression of the original dataset and does not suffer\nreduction in downstream task accuracy.", "AI": {"tldr": "Robo-DM\u662f\u4e00\u79cd\u9ad8\u6548\u5f00\u6e90\u7684\u4e91\u6570\u636e\u7ba1\u7406\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u6570\u636e\u7684\u6536\u96c6\u3001\u5171\u4eab\u548c\u5b66\u4e60\uff0c\u663e\u8457\u51cf\u5c11\u6570\u636e\u5927\u5c0f\u3001\u4f20\u8f93\u6210\u672c\u53ca\u8bad\u7ec3\u65f6\u7684\u52a0\u8f7d\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u673a\u5668\u4eba\u6570\u636e\u96c6\uff08\u5305\u542b\u89c6\u9891\u3001\u6587\u672c\u548c\u6570\u503c\u6a21\u6001\uff09\u7684\u6574\u7406\u3001\u5206\u53d1\u548c\u52a0\u8f7d\u96be\u9898\u3002", "method": "\u91c7\u7528EBML\u683c\u5f0f\u5b58\u50a8\u81ea\u5305\u542b\u6570\u636e\uff0c\u901a\u8fc7\u8d1f\u8f7d\u5747\u8861\u89c6\u9891\u89e3\u7801\u548c\u5185\u5b58\u6620\u5c04\u89e3\u7801\u7f13\u5b58\u52a0\u901f\u6570\u636e\u68c0\u7d22\u3002", "result": "\u76f8\u6bd4RLDS\u683c\u5f0f\uff0cRobo-DM\u5728\u538b\u7f29\u4e0a\u8282\u7701\u4e8670\u500d\uff08\u6709\u635f\uff09\u548c3.5\u500d\uff08\u65e0\u635f\uff09\u7a7a\u95f4\uff1b\u76f8\u6bd4LeRobot\uff0c\u987a\u5e8f\u89e3\u7801\u901f\u5ea6\u5feb50\u500d\u3002", "conclusion": "Robo-DM\u5728\u4fdd\u6301\u4efb\u52a1\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe75\u500d\u7684\u539f\u59cb\u6570\u636e\u96c6\u538b\u7f29\u3002", "keywords": "\u673a\u5668\u4eba\u6570\u636e\u7ba1\u7406, EBML, \u6570\u636e\u538b\u7f29, \u4e91\u5de5\u5177\u5305"}}
{"id": "2505.15466", "pdf": "https://arxiv.org/pdf/2505.15466", "abs": "https://arxiv.org/abs/2505.15466", "authors": ["Valeria Cesaroni", "Eleonora Pasqua", "Piercosma Bisconti", "Martina Galletti"], "title": "A Participatory Strategy for AI Ethics in Education and Rehabilitation grounded in the Capability Approach", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "AI-based technologies have significant potential to enhance inclusive\neducation and clinical-rehabilitative contexts for children with Special\nEducational Needs and Disabilities. AI can enhance learning experiences,\nempower students, and support both teachers and rehabilitators. However, their\nusage presents challenges that require a systemic-ecological vision, ethical\nconsiderations, and participatory research. Therefore, research and\ntechnological development must be rooted in a strong ethical-theoretical\nframework. The Capability Approach - a theoretical model of disability, human\nvulnerability, and inclusion - offers a more relevant perspective on\nfunctionality, effectiveness, and technological adequacy in inclusive learning\nenvironments. In this paper, we propose a participatory research strategy with\ndifferent stakeholders through a case study on the ARTIS Project, which\ndevelops an AI-enriched interface to support children with text comprehension\ndifficulties. Our research strategy integrates ethical, educational, clinical,\nand technological expertise in designing and implementing AI-based technologies\nfor children's learning environments through focus groups and collaborative\ndesign sessions. We believe that this holistic approach to AI adoption in\neducation can help bridge the gap between technological innovation and ethical\nresponsibility.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u6280\u672f\u5982\u4f55\u63d0\u5347\u7279\u6b8a\u6559\u80b2\u9700\u6c42\u513f\u7ae5\u7684\u5305\u5bb9\u6027\u6559\u80b2\uff0c\u63d0\u51fa\u57fa\u4e8e\u80fd\u529b\u65b9\u6cd5\u7684\u4f26\u7406\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u53c2\u4e0e\u5f0f\u7814\u7a76\u7b56\u7565\u7684\u5e94\u7528\u3002", "motivation": "AI\u6280\u672f\u5bf9\u63d0\u5347\u7279\u6b8a\u6559\u80b2\u548c\u5eb7\u590d\u73af\u5883\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u89e3\u51b3\u4f26\u7406\u548c\u7cfb\u7edf\u6027\u6311\u6218\uff0c\u9700\u57fa\u4e8e\u80fd\u529b\u65b9\u6cd5\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u5b66\u4e60\u73af\u5883\u3002", "method": "\u901a\u8fc7ARTIS\u9879\u76ee\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u91c7\u7528\u53c2\u4e0e\u5f0f\u7814\u7a76\u7b56\u7565\uff0c\u7ed3\u5408\u591a\u65b9\u4e13\u5bb6\u8bbe\u8ba1AI\u6280\u672f\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u5408\u4f26\u7406\u3001\u6559\u80b2\u548c\u6280\u672f\u7684\u65b9\u6cd5\uff0c\u652f\u6301AI\u5728\u5305\u5bb9\u6027\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u7ed3\u5408\u53c2\u4e0e\u5f0f\u7814\u7a76\u7684\u4f26\u7406\u6846\u67b6\u80fd\u5f25\u5408\u6280\u672f\u521b\u65b0\u4e0e\u4f26\u7406\u8d23\u4efb\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "keywords": "AI, \u5305\u5bb9\u6027\u6559\u80b2, \u80fd\u529b\u65b9\u6cd5, \u4f26\u7406\u6846\u67b6, \u53c2\u4e0e\u5f0f\u7814\u7a76"}}
{"id": "2505.15559", "pdf": "https://arxiv.org/pdf/2505.15559", "abs": "https://arxiv.org/abs/2505.15559", "authors": ["Zixun Guo", "Simon Dixon"], "title": "Moonbeam: A MIDI Foundation Model Using Both Absolute and Relative Music Attributes", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Moonbeam is a transformer-based foundation model for symbolic music,\npretrained on a large and diverse collection of MIDI data totaling 81.6K hours\nof music and 18 billion tokens. Moonbeam incorporates music-domain inductive\nbiases by capturing both absolute and relative musical attributes through the\nintroduction of a novel domain-knowledge-inspired tokenization method and\nMultidimensional Relative Attention (MRA), which captures relative music\ninformation without additional trainable parameters. Leveraging the pretrained\nMoonbeam, we propose 2 finetuning architectures with full anticipatory\ncapabilities, targeting 2 categories of downstream tasks: symbolic music\nunderstanding and conditional music generation (including music infilling). Our\nmodel outperforms other large-scale pretrained music models in most cases in\nterms of accuracy and F1 score across 3 downstream music classification tasks\non 4 datasets. Moreover, our finetuned conditional music generation model\noutperforms a strong transformer baseline with a REMI-like tokenizer. We\nopen-source the code, pretrained model, and generated samples on Github.", "AI": {"tldr": "Moonbeam\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u7b26\u53f7\u97f3\u4e50\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u572881.6K\u5c0f\u65f6\u7684MIDI\u6570\u636e\u548c180\u4ebf\u4e2atoken\u4e0a\uff0c\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u7684tokenization\u65b9\u6cd5\u548c\u591a\u7ef4\u76f8\u5bf9\u6ce8\u610f\u529b\uff08MRA\uff09\u6765\u5b9e\u73b0\u97f3\u4e50\u7406\u89e3\u4e0e\u751f\u6210\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u97f3\u4e50\u9886\u57df\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u63d0\u5347\u7b26\u53f7\u97f3\u4e50\u7684\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u9886\u57df\u77e5\u8bc6\u7684tokenization\u65b9\u6cd5\u548cMRA\u6280\u672f\uff0c\u65e0\u9700\u989d\u5916\u53ef\u8bad\u7ec3\u53c2\u6570\u5373\u53ef\u6355\u6349\u76f8\u5bf9\u97f3\u4e50\u4fe1\u606f\u3002\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5fae\u8c03\u67b6\u6784\u3002", "result": "\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u5206\u7c7b\u548c\u97f3\u4e50\u751f\u6210\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "Moonbeam\u5728\u7b26\u53f7\u97f3\u4e50\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u4e14\u5f00\u6e90\u4e86\u6a21\u578b\u4e0e\u4ee3\u7801\u3002", "keywords": "Moonbeam, Transformer, \u7b26\u53f7\u97f3\u4e50, MIDI, MRA"}}
{"id": "2505.15489", "pdf": "https://arxiv.org/pdf/2505.15489", "abs": "https://arxiv.org/abs/2505.15489", "authors": ["Jiaying Wu", "Fanxiao Li", "Min-Yen Kan", "Bryan Hooi"], "title": "Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "The real-world impact of misinformation stems from the underlying misleading\nnarratives that creators seek to convey. As such, interpreting misleading\ncreator intent is essential for multimodal misinformation detection (MMD)\nsystems aimed at effective information governance. In this paper, we introduce\nan automated framework that simulates real-world multimodal news creation by\nexplicitly modeling creator intent through two components: the desired\ninfluence and the execution plan. Using this framework, we construct\nDeceptionDecoded, a large-scale benchmark comprising 12,000 image-caption pairs\naligned with trustworthy reference articles. The dataset captures both\nmisleading and non-misleading intents and spans manipulations across visual and\ntextual modalities. We conduct a comprehensive evaluation of 14\nstate-of-the-art vision-language models (VLMs) on three intent-centric tasks:\n(1) misleading intent detection, (2) misleading source attribution, and (3)\ncreator desire inference. Despite recent advances, we observe that current VLMs\nfall short in recognizing misleading intent, often relying on spurious cues\nsuch as superficial cross-modal consistency, stylistic signals, and heuristic\nauthenticity hints. Our findings highlight the pressing need for intent-aware\nmodeling in MMD and open new directions for developing systems capable of\ndeeper reasoning about multimodal misinformation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u65b0\u95fb\u521b\u4f5c\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u5efa\u8005\u610f\u56fe\u7684\u4e24\u4e2a\u7ec4\u4ef6\uff08\u671f\u671b\u5f71\u54cd\u548c\u6267\u884c\u8ba1\u5212\uff09\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6DeceptionDecoded\uff0c\u8bc4\u4f30\u4e8614\u79cd\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e09\u79cd\u610f\u56fe\u4e2d\u5fc3\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u8bc6\u522b\u8bef\u5bfc\u610f\u56fe\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5efa\u6a21\u521b\u5efa\u8005\u610f\u56fe\uff0c\u63d0\u5347\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7cfb\u7edf\u7684\u6548\u679c\uff0c\u4ee5\u652f\u6301\u66f4\u6709\u6548\u7684\u4fe1\u606f\u6cbb\u7406\u3002", "method": "\u91c7\u7528\u81ea\u52a8\u5316\u6846\u67b6\u6a21\u62df\u591a\u6a21\u6001\u65b0\u95fb\u521b\u4f5c\uff0c\u6784\u5efa\u5305\u542b12000\u4e2a\u56fe\u50cf-\u6807\u9898\u5bf9\u7684\u6570\u636e\u96c6DeceptionDecoded\uff0c\u6db5\u76d6\u4e86\u8bef\u5bfc\u4e0e\u975e\u8bef\u5bfc\u610f\u56fe\uff0c\u5e76\u5728\u591a\u79cd\u6a21\u6001\u4e0a\u8fdb\u884c\u64cd\u7eb5\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\uff0c\u5f53\u524d\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u8bef\u5bfc\u610f\u56fe\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5bb9\u6613\u4f9d\u8d56\u8868\u9762\u7ebf\u7d22\uff08\u5982\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u3001\u98ce\u683c\u4fe1\u53f7\u548c\u542f\u53d1\u5f0f\u771f\u5b9e\u6027\u63d0\u793a\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u9700\u8981\u66f4\u6df1\u5165\u7684\u610f\u56fe\u611f\u77e5\u5efa\u6a21\uff0c\u4e3a\u5f00\u53d1\u5177\u5907\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "keywords": "\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b, \u521b\u5efa\u8005\u610f\u56fe, \u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u610f\u56fe\u611f\u77e5\u5efa\u6a21, DeceptionDecoded"}}
{"id": "2505.15510", "pdf": "https://arxiv.org/pdf/2505.15510", "abs": "https://arxiv.org/abs/2505.15510", "authors": ["Zihui Cheng", "Qiguang Chen", "Xiao Xu", "Jiaqi Wang", "Weiyun Wang", "Hao Fei", "Yidong Wang", "Alex Jinpeng Wang", "Zhi Chen", "Wanxiang Che", "Libo Qin"], "title": "Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have achieved significant success in\nmultimodal tasks, with multimodal chain-of-thought (MCoT) further enhancing\nperformance and interpretability. Recent MCoT methods fall into two categories:\n(i) Textual-MCoT (T-MCoT), which takes multimodal input and produces textual\noutput; and (ii) Interleaved-MCoT (I-MCoT), which generates interleaved\nimage-text outputs. Despite advances in both approaches, the mechanisms driving\nthese improvements are not fully understood. To fill this gap, we first reveal\nthat MCoT boosts LVLMs by incorporating visual thoughts, which convey image\ninformation to the reasoning process regardless of the MCoT format, depending\nonly on clarity and conciseness of expression. Furthermore, to explore visual\nthoughts systematically, we define four distinct forms of visual thought\nexpressions and analyze them comprehensively. Our findings demonstrate that\nthese forms differ in clarity and conciseness, yielding varying levels of MCoT\nimprovement. Additionally, we explore the internal nature of visual thoughts,\nfinding that visual thoughts serve as intermediaries between the input image\nand reasoning to deeper transformer layers, enabling more advanced visual\ninformation transmission. We hope that the visual thoughts can inspire further\nbreakthroughs for future MCoT research.", "AI": {"tldr": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u591a\u6a21\u6001\u601d\u7ef4\u94fe\uff08MCoT\uff09\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0cMCoT\u901a\u8fc7\u5f15\u5165\u89c6\u89c9\u601d\u7ef4\uff08\u65e0\u8bba\u683c\u5f0f\u5982\u4f55\uff09\u6765\u589e\u5f3a\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u89c6\u89c9\u601d\u7ef4\u7684\u6e05\u6670\u5ea6\u548c\u7b80\u6d01\u6027\u51b3\u5b9a\u4e86\u5176\u6548\u679c\u3002\u8fdb\u4e00\u6b65\u63ed\u793a\u4e86\u89c6\u89c9\u601d\u7ef4\u4f5c\u4e3a\u8f93\u5165\u56fe\u50cf\u4e0e\u6df1\u5c42\u63a8\u7406\u7684\u4e2d\u4ecb\u4f5c\u7528\u3002", "motivation": "\u63a2\u7a76\u591a\u6a21\u6001\u601d\u7ef4\u94fe\uff08MCoT\uff09\u63d0\u5347\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u6027\u80fd\u7684\u5177\u4f53\u673a\u5236\uff0c\u5c24\u5176\u662f\u89c6\u89c9\u601d\u7ef4\u7684\u4f5c\u7528\u5f62\u5f0f\u53ca\u5176\u5bf9\u63a8\u7406\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "method": "\u5b9a\u4e49\u5e76\u5206\u6790\u4e86\u56db\u79cd\u89c6\u89c9\u601d\u7ef4\u7684\u8868\u8fbe\u5f62\u5f0f\uff0c\u8bc4\u4f30\u5176\u6e05\u6670\u5ea6\u548c\u7b80\u6d01\u6027\u5bf9MCoT\u6548\u679c\u7684\u5f71\u54cd\uff1b\u7814\u7a76\u4e86\u89c6\u89c9\u601d\u7ef4\u5728Transformer\u6df1\u5c42\u7684\u4fe1\u606f\u4f20\u9012\u4f5c\u7528\u3002", "result": "\u89c6\u89c9\u601d\u7ef4\u7684\u6e05\u6670\u5ea6\u548c\u7b80\u6d01\u6027\u663e\u8457\u5f71\u54cdMCoT\u7684\u6548\u679c\uff1b\u89c6\u89c9\u601d\u7ef4\u5145\u5f53\u8f93\u5165\u56fe\u50cf\u4e0e\u6df1\u5c42\u63a8\u7406\u7684\u4e2d\u4ecb\uff0c\u4fc3\u8fdb\u66f4\u9ad8\u7ea7\u7684\u89c6\u89c9\u4fe1\u606f\u4f20\u9012\u3002", "conclusion": "\u89c6\u89c9\u601d\u7ef4\u662fMCoT\u63d0\u5347LVLMs\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u672a\u6765\u7814\u7a76\u53ef\u57fa\u4e8e\u5176\u7279\u6027\u63a2\u7d22\u66f4\u591a\u7a81\u7834\u3002", "keywords": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u3001\u591a\u6a21\u6001\u601d\u7ef4\u94fe\uff08MCoT\uff09\u3001\u89c6\u89c9\u601d\u7ef4\u3001\u63a8\u7406\u673a\u5236\u3001\u591a\u6a21\u6001\u4efb\u52a1"}}
{"id": "2505.15581", "pdf": "https://arxiv.org/pdf/2505.15581", "abs": "https://arxiv.org/abs/2505.15581", "authors": ["Hua Li", "Shijie Lian", "Zhiyuan Li", "Runmin Cong", "Sam Kwong"], "title": "UWSAM: Segment Anything Model Guided Underwater Instance Segmentation and A Large-scale Benchmark Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With recent breakthroughs in large-scale modeling, the Segment Anything Model\n(SAM) has demonstrated significant potential in a variety of visual\napplications. However, due to the lack of underwater domain expertise, SAM and\nits variants face performance limitations in end-to-end underwater instance\nsegmentation tasks, while their higher computational requirements further\nhinder their application in underwater scenarios. To address this challenge, we\npropose a large-scale underwater instance segmentation dataset, UIIS10K, which\nincludes 10,048 images with pixel-level annotations for 10 categories. Then, we\nintroduce UWSAM, an efficient model designed for automatic and accurate\nsegmentation of underwater instances. UWSAM efficiently distills knowledge from\nthe SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the\nMask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective\nvisual representation learning. Furthermore, we design an End-to-end Underwater\nPrompt Generator (EUPG) for UWSAM, which automatically generates underwater\nprompts instead of explicitly providing foreground points or boxes as prompts,\nthus enabling the network to locate underwater instances accurately for\nefficient segmentation. Comprehensive experimental results show that our model\nis effective, achieving significant performance improvements over\nstate-of-the-art methods on multiple underwater instance datasets. Datasets and\ncodes are available at https://github.com/LiamLian0727/UIIS10K.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3a\u6c34\u4e0b\u5b9e\u4f8b\u5206\u5272\u8bbe\u8ba1\u7684\u9ad8\u6548\u6a21\u578bUWSAM\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u81ea\u52a8\u63d0\u793a\u751f\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u573a\u666f\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u4e3a\u89e3\u51b3SAM\u53ca\u5176\u53d8\u4f53\u5728\u6c34\u4e0b\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e2d\u56e0\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4e0d\u8db3\u548c\u8ba1\u7b97\u9700\u6c42\u9ad8\u800c\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efaUIIS10K\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u57fa\u4e8eMask GAT\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5MG-UKD\u548c\u7aef\u5230\u7aef\u6c34\u4e0b\u63d0\u793a\u751f\u6210\u5668EUPG\u3002", "result": "UWSAM\u5728\u591a\u4e2a\u6c34\u4e0b\u5b9e\u4f8b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UWSAM\u901a\u8fc7\u9ad8\u6548\u77e5\u8bc6\u84b8\u998f\u548c\u81ea\u52a8\u63d0\u793a\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u6c34\u4e0b\u5b9e\u4f8b\u5206\u5272\u7684\u9ad8\u6027\u80fd\u3002", "keywords": "\u6c34\u4e0b\u5b9e\u4f8b\u5206\u5272, SAM, \u77e5\u8bc6\u84b8\u998f, \u81ea\u52a8\u63d0\u793a\u751f\u6210, UIIS10K"}}
{"id": "2505.14986", "pdf": "https://arxiv.org/pdf/2505.14986", "abs": "https://arxiv.org/abs/2505.14986", "authors": ["Meenal Parakh", "Alexandre Kirchmeyer", "Beining Han", "Jia Deng"], "title": "AnyBody: A Benchmark Suite for Cross-Embodiment Manipulation", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Generalizing control policies to novel embodiments remains a fundamental\nchallenge in enabling scalable and transferable learning in robotics. While\nprior works have explored this in locomotion, a systematic study in the context\nof manipulation tasks remains limited, partly due to the lack of standardized\nbenchmarks. In this paper, we introduce a benchmark for learning\ncross-embodiment manipulation, focusing on two foundational tasks-reach and\npush-across a diverse range of morphologies. The benchmark is designed to test\ngeneralization along three axes: interpolation (testing performance within a\nrobot category that shares the same link structure), extrapolation (testing on\na robot with a different link structure), and composition (testing on\ncombinations of link structures). On the benchmark, we evaluate the ability of\ndifferent RL policies to learn from multiple morphologies and to generalize to\nnovel ones. Our study aims to answer whether morphology-aware training can\noutperform single-embodiment baselines, whether zero-shot generalization to\nunseen morphologies is feasible, and how consistently these patterns hold\nacross different generalization regimes. The results highlight the current\nlimitations of multi-embodiment learning and provide insights into how\narchitectural and training design choices influence policy generalization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8de8\u5b9e\u4f53\u64cd\u4f5c\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u6311\u6218\uff0c\u5e76\u8bc4\u4f30\u4e0d\u540c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u8de8\u5b9e\u4f53\u64cd\u4f5c\u63a7\u5236\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u586b\u8865\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\uff0c\u4ee5\u4fc3\u8fdb\u673a\u5668\u4eba\u7684\u53ef\u6269\u5c55\u548c\u53ef\u8fc1\u79fb\u5b66\u4e60\u3002", "method": "\u8bbe\u8ba1\u4e86\u6db5\u76d6\u591a\u79cd\u5f62\u6001\u7684\u57fa\u51c6\u6d4b\u8bd5\u4efb\u52a1\uff08\u5230\u8fbe\u548c\u63a8\u52a8\uff09\uff0c\u5e76\u8bc4\u4f30\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u63d2\u503c\u3001\u5916\u63a8\u548c\u7ec4\u5408\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\u591a\u5b9e\u4f53\u5b66\u4e60\u4ecd\u5b58\u5728\u5c40\u9650\uff0c\u4f46\u4e3a\u67b6\u6784\u548c\u8bad\u7ec3\u8bbe\u8ba1\u5982\u4f55\u5f71\u54cd\u7b56\u7565\u6cdb\u5316\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u5f62\u6001\u611f\u77e5\u8bad\u7ec3\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u5230\u672a\u89c1\u5f62\u6001\u7684\u53ef\u884c\u6027\u95ee\u9898\u3002", "keywords": "\u673a\u5668\u4eba\u5b66\u4e60, \u8de8\u5b9e\u4f53\u64cd\u4f5c, \u5f3a\u5316\u5b66\u4e60, \u6cdb\u5316\u80fd\u529b, \u57fa\u51c6\u6d4b\u8bd5"}}
{"id": "2505.15585", "pdf": "https://arxiv.org/pdf/2505.15585", "abs": "https://arxiv.org/abs/2505.15585", "authors": ["Haocheng Ju", "Bin Dong"], "title": "MIRB: Mathematical Information Retrieval Benchmark", "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": "Our code and data are available at https://github.com/j991222/mirb\n  and https://huggingface.co/collections/hcju/mirb-6827001711765454f58c5a76", "summary": "Mathematical Information Retrieval (MIR) is the task of retrieving\ninformation from mathematical documents and plays a key role in various\napplications, including theorem search in mathematical libraries, answer\nretrieval on math forums, and premise selection in automated theorem proving.\nHowever, a unified benchmark for evaluating these diverse retrieval tasks has\nbeen lacking. In this paper, we introduce MIRB (Mathematical Information\nRetrieval Benchmark) to assess the MIR capabilities of retrieval models. MIRB\nincludes four tasks: semantic statement retrieval, question-answer retrieval,\npremise retrieval, and formula retrieval, spanning a total of 12 datasets. We\nevaluate 13 retrieval models on this benchmark and analyze the challenges\ninherent to MIR. We hope that MIRB provides a comprehensive framework for\nevaluating MIR systems and helps advance the development of more effective\nretrieval models tailored to the mathematical domain.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MIRB\uff08\u6570\u5b66\u4fe1\u606f\u68c0\u7d22\u57fa\u51c6\uff09\uff0c\u65e8\u5728\u8bc4\u4f30\u6570\u5b66\u6587\u6863\u4e2d\u4fe1\u606f\u68c0\u7d22\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6db5\u76d6\u56db\u4e2a\u4efb\u52a1\u548c12\u4e2a\u6570\u636e\u96c6\uff0c\u5e76\u5bf913\u4e2a\u6a21\u578b\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u6570\u5b66\u4fe1\u606f\u68c0\u7d22\uff08MIR\uff09\u4efb\u52a1\uff0cMIRB\u7684\u5f15\u5165\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "MIRB\u5305\u542b\u56db\u4e2a\u4efb\u52a1\uff1a\u8bed\u4e49\u9648\u8ff0\u68c0\u7d22\u3001\u95ee\u7b54\u68c0\u7d22\u3001\u524d\u63d0\u68c0\u7d22\u548c\u516c\u5f0f\u68c0\u7d22\uff0c\u517112\u4e2a\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e8613\u4e2a\u68c0\u7d22\u6a21\u578b\u3002", "result": "\u901a\u8fc7MIRB\u57fa\u51c6\uff0c\u5206\u6790\u4e86\u6570\u5b66\u4fe1\u606f\u68c0\u7d22\u7684\u6311\u6218\uff0c\u4e3a\u8bc4\u4f30MIR\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6846\u67b6\u3002", "conclusion": "MIRB\u6709\u671b\u4fc3\u8fdb\u9488\u5bf9\u6570\u5b66\u9886\u57df\u66f4\u6709\u6548\u68c0\u7d22\u6a21\u578b\u7684\u5f00\u53d1\u3002", "keywords": "\u6570\u5b66\u4fe1\u606f\u68c0\u7d22\uff08MIR\uff09\uff0cMIRB\uff0c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u68c0\u7d22\u6a21\u578b"}}
{"id": "2505.15596", "pdf": "https://arxiv.org/pdf/2505.15596", "abs": "https://arxiv.org/abs/2505.15596", "authors": ["Xinyi Lu", "Aditya Mahesh", "Zejia Shen", "Mitchell Dudley", "Larissa Sano", "Xu Wang"], "title": "Exploring LLM-Generated Feedback for Economics Essays: How Teaching Assistants Evaluate and Envision Its Use", "categories": ["cs.HC", "cs.AI"], "comment": "To be published in AIED'2025: In Proceedings of the 26th\n  International Conference on Artificial Intelligence in Education. The system\n  prompt and example feedback can be found through\n  http://github.com/UM-Lifelong-Learning-Lab/AIED2025-Exploring-LLM-Generated-Feedback-for-Economics-Essay", "summary": "This project examines the prospect of using AI-generated feedback as\nsuggestions to expedite and enhance human instructors' feedback provision. In\nparticular, we focus on understanding the teaching assistants' perspectives on\nthe quality of AI-generated feedback and how they may or may not utilize AI\nfeedback in their own workflows. We situate our work in a foundational college\nEconomics class, which has frequent short essay assignments. We developed an\nLLM-powered feedback engine that generates feedback on students' essays based\non grading rubrics used by the teaching assistants (TAs). To ensure that TAs\ncan meaningfully critique and engage with the AI feedback, we had them complete\ntheir regular grading jobs. For a randomly selected set of essays that they had\ngraded, we used our feedback engine to generate feedback and displayed the\nfeedback as in-text comments in a Word document. We then performed think-aloud\nstudies with 5 TAs over 20 1-hour sessions to have them evaluate the AI\nfeedback, contrast the AI feedback with their handwritten feedback, and share\nhow they envision using the AI feedback if they were offered as suggestions.\nThe study highlights the importance of providing detailed rubrics for AI to\ngenerate high-quality feedback for knowledge-intensive essays. TAs considered\nthat using AI feedback as suggestions during their grading could expedite\ngrading, enhance consistency, and improve overall feedback quality. We discuss\nthe importance of decomposing the feedback generation task into steps and\npresenting intermediate results, in order for TAs to use the AI feedback.", "AI": {"tldr": "\u7814\u7a76\u4e86AI\u751f\u6210\u53cd\u9988\u4f5c\u4e3a\u5efa\u8bae\u5982\u4f55\u52a0\u901f\u548c\u63d0\u5347\u4eba\u7c7b\u6559\u5e08\u7684\u53cd\u9988\u63d0\u4f9b\u6548\u679c\uff0c\u91cd\u70b9\u5173\u6ce8\u52a9\u6559\u5bf9AI\u53cd\u9988\u8d28\u91cf\u7684\u770b\u6cd5\u53ca\u5176\u5728\u5de5\u4f5c\u6d41\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u63a2\u7d22AI\u751f\u6210\u53cd\u9988\u5728\u8f85\u52a9\u4eba\u7c7b\u6559\u5e08\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u57fa\u7840\u7ecf\u6d4e\u5b66\u8bfe\u7a0b\u4e2d\u9891\u7e41\u7684\u77ed\u6587\u4f5c\u4e1a\u573a\u666f\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u53cd\u9988\u5f15\u64ce\uff0c\u6839\u636e\u52a9\u6559\u4f7f\u7528\u7684\u8bc4\u5206\u6807\u51c6\u751f\u6210\u53cd\u9988\uff0c\u5e76\u901a\u8fc7\u601d\u8003\u6027\u8bc4\u4f30\u7814\u7a76\u8ba9\u52a9\u6559\u8bc4\u4ef7AI\u53cd\u9988\u3002", "result": "\u52a9\u6559\u8ba4\u4e3aAI\u53cd\u9988\u53ef\u4ee5\u52a0\u901f\u8bc4\u5206\u3001\u63d0\u5347\u4e00\u81f4\u6027\u548c\u53cd\u9988\u8d28\u91cf\uff0c\u4f46\u9700\u8981\u8be6\u7ec6\u8bc4\u5206\u6807\u51c6\u548c\u5206\u6b65\u4efb\u52a1\u5206\u89e3\u3002", "conclusion": "AI\u751f\u6210\u53cd\u9988\u4f5c\u4e3a\u5efa\u8bae\u6709\u52a9\u4e8e\u63d0\u5347\u6559\u5b66\u6548\u7387\uff0c\u4f46\u9700\u8be6\u7ec6\u6807\u51c6\u548c\u4e2d\u95f4\u7ed3\u679c\u5c55\u793a\u4ee5\u652f\u6301\u52a9\u6559\u4f7f\u7528\u3002", "keywords": "AI\u53cd\u9988, \u52a9\u6559, \u8bc4\u5206\u6807\u51c6, LLM, \u7ecf\u6d4e\u5b66\u8bfe\u7a0b"}}
{"id": "2505.15013", "pdf": "https://arxiv.org/pdf/2505.15013", "abs": "https://arxiv.org/abs/2505.15013", "authors": ["Anupama Sridhar", "Alexander Johansen"], "title": "Convergence of Adam in Deep ReLU Networks via Directional Complexity and Kakeya Bounds", "categories": ["stat.ML", "cs.LG"], "comment": "9 pages main paper", "summary": "First-order adaptive optimization methods like Adam are the default choices\nfor training modern deep neural networks. Despite their empirical success, the\ntheoretical understanding of these methods in non-smooth settings, particularly\nin Deep ReLU networks, remains limited. ReLU activations create exponentially\nmany region boundaries where standard smoothness assumptions break down.\n\\textbf{We derive the first\n\\(\\tilde{O}\\!\\bigl(\\sqrt{d_{\\mathrm{eff}}/n}\\bigr)\\) generalization bound for\nAdam in Deep ReLU networks and the first global-optimal convergence for Adam in\nthe non smooth, non convex relu landscape without a global PL or convexity\nassumption.} Our analysis is based on stratified Morse theory and novel results\nin Kakeya sets. We develop a multi-layer refinement framework that\nprogressively tightens bounds on region crossings. We prove that the number of\nregion crossings collapses from exponential to near-linear in the effective\ndimension. Using a Kakeya based method, we give a tighter generalization bound\nthan PAC-Bayes approaches and showcase convergence using a mild uniform low\nbarrier assumption.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86Adam\u4f18\u5316\u5728\u975e\u5149\u6ed1Deep ReLU\u7f51\u7edc\u4e2d\u7684\u7406\u8bba\u8868\u73b0\uff0c\u9996\u6b21\u63d0\u51fa\u4e86\u5176\u6cdb\u5316\u8fb9\u754c\u4e0e\u5168\u5c40\u6536\u655b\u6027\u8bc1\u660e\uff0c\u5e76\u57fa\u4e8e\u5206\u5c42Morse\u7406\u8bba\u548cKakeya\u96c6\u63d0\u51fa\u591a\u5c42\u4f18\u5316\u6846\u67b6\u3002", "motivation": "\u5c3d\u7ba1Adam\u7b49\u4e00\u9636\u81ea\u9002\u5e94\u4f18\u5316\u65b9\u6cd5\u5728\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u5728\u975e\u5149\u6ed1ReLU\u7f51\u7edc\u4e2d\u7684\u7406\u8bba\u7406\u89e3\u4ecd\u6709\u9650\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e\u5206\u5c42Morse\u7406\u8bba\u548cKakeya\u96c6\u7684\u65b0\u7ed3\u679c\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u9010\u6b65\u6536\u7d27\u533a\u57df\u7a7f\u8d8a\u7684\u8fb9\u754c\u3002", "result": "\u8bc1\u660e\u4e86Adam\u5728Deep ReLU\u7f51\u7edc\u4e2d\u7684\u6cdb\u5316\u8fb9\u754c\u4e3aO(\u221a(d_eff/n))\uff0c\u5e76\u5728\u975e\u5149\u6ed1\u975e\u51f8\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u5168\u5c40\u6700\u4f18\u6536\u655b\u3002", "conclusion": "\u672c\u6587\u4e3aAdam\u5728\u975e\u5149\u6ed1Deep ReLU\u7f51\u7edc\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u901a\u8fc7Kakeya\u65b9\u6cd5\u6539\u8fdb\u4e86\u6cdb\u5316\u8fb9\u754c\u3002", "keywords": "Adam\u4f18\u5316, Deep ReLU\u7f51\u7edc, \u975e\u5149\u6ed1\u4f18\u5316, \u6cdb\u5316\u8fb9\u754c, \u5206\u5c42Morse\u7406\u8bba, Kakeya\u96c6"}}
{"id": "2505.15667", "pdf": "https://arxiv.org/pdf/2505.15667", "abs": "https://arxiv.org/abs/2505.15667", "authors": ["Nicholas Sanders", "Yuanchao Li", "Korin Richmond", "Simon King"], "title": "Segmentation-Variant Codebooks for Preservation of Paralinguistic and Prosodic Information", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Quantization in SSL speech models (e.g., HuBERT) improves compression and\nperformance in tasks like language modeling, resynthesis, and text-to-speech\nbut often discards prosodic and paralinguistic information (e.g., emotion,\nprominence). While increasing codebook size mitigates some loss, it\ninefficiently raises bitrates. We propose Segmentation-Variant Codebooks\n(SVCs), which quantize speech at distinct linguistic units (frame, phone, word,\nutterance), factorizing it into multiple streams of segment-specific discrete\nfeatures. Our results show that SVCs are significantly more effective at\npreserving prosodic and paralinguistic information across probing tasks.\nAdditionally, we find that pooling before rather than after discretization\nbetter retains segment-level information. Resynthesis experiments further\nconfirm improved style realization and slightly improved quality while\npreserving intelligibility.", "AI": {"tldr": "\u63d0\u51faSegmentation-Variant Codebooks\uff08SVCs\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e0d\u540c\u8bed\u8a00\u5355\u5143\u4e0a\u91cf\u5316\u8bed\u97f3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u6a21\u578b\u4e2d\u97f5\u5f8b\u548c\u526f\u8bed\u8a00\u4fe1\u606f\u7684\u4fdd\u7559\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u8868\u660e\uff0c\u73b0\u6709SSL\u8bed\u97f3\u6a21\u578b\uff08\u5982HuBERT\uff09\u7684\u91cf\u5316\u65b9\u6cd5\u867d\u7136\u6539\u5584\u4e86\u538b\u7f29\u548c\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u5e38\u4e22\u5931\u97f5\u5f8b\u548c\u526f\u8bed\u8a00\u4fe1\u606f\u3002\u589e\u5927\u7801\u672c\u89c4\u6a21\u867d\u80fd\u90e8\u5206\u7f13\u89e3\uff0c\u4f46\u4f1a\u4f4e\u6548\u5730\u63d0\u9ad8\u6bd4\u7279\u7387\u3002", "method": "\u63d0\u51faSegmentation-Variant Codebooks\uff08SVCs\uff09\uff0c\u5728\u4e0d\u540c\u8bed\u8a00\u5355\u5143\uff08\u5e27\u3001\u97f3\u7d20\u3001\u5355\u8bcd\u3001\u8bdd\u8bed\uff09\u4e0a\u91cf\u5316\u8bed\u97f3\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u591a\u4e2a\u7279\u5b9a\u5206\u6bb5\u7684\u79bb\u6563\u7279\u5f81\u6d41\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cSVCs\u5728\u591a\u79cd\u63a2\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u66f4\u6709\u6548\u5730\u4fdd\u7559\u4e86\u97f5\u5f8b\u548c\u526f\u8bed\u8a00\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u5728\u79bb\u6563\u5316\u524d\u8fdb\u884c\u6c60\u5316\u64cd\u4f5c\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u5206\u6bb5\u7ea7\u4fe1\u606f\u3002", "conclusion": "SVCs\u65b9\u6cd5\u5728\u4fdd\u7559\u53ef\u61c2\u5ea6\u7684\u540c\u65f6\uff0c\u6539\u8fdb\u4e86\u98ce\u683c\u8868\u73b0\u548c\u8bed\u97f3\u8d28\u91cf\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u8bed\u97f3\u91cf\u5316\u4e2d\u7684\u6709\u6548\u6027\u3002", "keywords": "SSL, HuBERT, \u91cf\u5316, \u97f5\u5f8b\u4fe1\u606f, \u526f\u8bed\u8a00\u4fe1\u606f"}}
{"id": "2505.15022", "pdf": "https://arxiv.org/pdf/2505.15022", "abs": "https://arxiv.org/abs/2505.15022", "authors": ["Ya-Yun Huang", "Joseph McClernon", "Jason A. Oliver", "Matthew M. Engelhard"], "title": "Infinite hierarchical contrastive clustering for personal digital envirotyping", "categories": ["stat.ML", "cs.LG"], "comment": "10pages, 5 figures, Machine Learning four Health(ML4H 2024)", "summary": "Daily environments have profound influence on our health and behavior. Recent\nwork has shown that digital envirotyping, where computer vision is applied to\nimages of daily environments taken during ecological momentary assessment\n(EMA), can be used to identify meaningful relationships between environmental\nfeatures and health outcomes of interest. To systematically study such effects\non an individual level, it is helpful to group images into distinct\nenvironments encountered in an individual's daily life; these may then be\nanalyzed, further grouped into related environments with similar features, and\nlinked to health outcomes. Here we introduce infinite hierarchical contrastive\nclustering to address this challenge. Building on the established contrastive\nclustering framework, our method a) allows an arbitrary number of clusters\nwithout requiring the full Dirichlet Process machinery by placing a\nstick-breaking prior on predicted cluster probabilities; and b) encourages\ndistinct environments to form well-defined sub-clusters within each cluster of\nrelated environments by incorporating a participant-specific prediction loss.\nOur experiments show that our model effectively identifies distinct personal\nenvironments and groups these environments into meaningful environment types.\nWe then illustrate how the resulting clusters can be linked to various health\noutcomes, highlighting the potential of our approach to advance the\nenvirotyping paradigm.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9650\u5c42\u6b21\u5bf9\u6bd4\u805a\u7c7b\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u65e5\u5e38\u73af\u5883\u56fe\u50cf\u4e2d\u8bc6\u522b\u4e0e\u5065\u5eb7\u7ed3\u679c\u76f8\u5173\u7684\u73af\u5883\u7c7b\u578b\u3002", "motivation": "\u7814\u7a76\u65e5\u5e38\u73af\u5883\u5bf9\u5065\u5eb7\u548c\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u9700\u8981\u901a\u8fc7\u56fe\u50cf\u5206\u6790\u5bf9\u73af\u5883\u8fdb\u884c\u5206\u7ec4\uff0c\u5e76\u5173\u8054\u5065\u5eb7\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u65e0\u9650\u5c42\u6b21\u5bf9\u6bd4\u805a\u7c7b\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5bf9\u6bd4\u805a\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u68d2\u65ad\u5148\u9a8c\u548c\u53c2\u4e0e\u8005\u7279\u5b9a\u9884\u6d4b\u635f\u5931\uff0c\u5b9e\u73b0\u4efb\u610f\u6570\u91cf\u7684\u805a\u7c7b\u548c\u5b50\u805a\u7c7b\u3002", "result": "\u6a21\u578b\u6709\u6548\u8bc6\u522b\u4e86\u4e2a\u4eba\u73af\u5883\u5e76\u5c06\u5176\u5206\u7ec4\u4e3a\u6709\u610f\u4e49\u7684\u73af\u5883\u7c7b\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u805a\u7c7b\u7ed3\u679c\u4e0e\u5065\u5eb7\u7ed3\u679c\u5173\u8054\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63a8\u52a8\u4e86\u73af\u5883\u5206\u578b\u8303\u5f0f\u7684\u53d1\u5c55\uff0c\u4e3a\u7814\u7a76\u73af\u5883\u4e0e\u5065\u5eb7\u5173\u7cfb\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "keywords": "\u73af\u5883\u5206\u578b, \u5bf9\u6bd4\u805a\u7c7b, \u5065\u5eb7\u7ed3\u679c, \u751f\u6001\u77ac\u65f6\u8bc4\u4f30, \u8ba1\u7b97\u673a\u89c6\u89c9"}}
{"id": "2505.15701", "pdf": "https://arxiv.org/pdf/2505.15701", "abs": "https://arxiv.org/abs/2505.15701", "authors": ["Pingqing Zheng", "Jiayin Qin", "Fuqi Zhang", "Shang Wu", "Yu Cao", "Caiwen Ding", "Yang", "Zhao"], "title": "HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases", "categories": ["cs.AR", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated their potential in hardware\ndesign tasks, such as Hardware Description Language (HDL) generation and\ndebugging. Yet, their performance in real-world, repository-level HDL projects\nwith thousands or even tens of thousands of code lines is hindered. To this\nend, we propose HDLxGraph, a novel framework that integrates Graph Retrieval\nAugmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph\nrepresentations by incorporating Abstract Syntax Trees (ASTs) and Data Flow\nGraphs (DFGs) to capture both code graph view and hardware graph view.\nHDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the\nlimited recall issues inherent in similarity-based semantic retrieval by\nincorporating structural information, but also enhances its extensibility to\nvarious real-world tasks by a task-specific retrieval finetuning. Additionally,\nto address the lack of comprehensive HDL search benchmarks, we introduce\nHDLSearch, a multi-granularity evaluation dataset derived from real-world\nrepository-level projects. Experimental results demonstrate that HDLxGraph\nsignificantly improves average search accuracy, debugging efficiency and\ncompletion quality by 12.04%, 12.22% and 5.04% compared to similarity-based\nRAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are\navailable at https://github.com/Nick-Zheng-Q/HDLxGraph.", "AI": {"tldr": "HDLxGraph\u7ed3\u5408\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e0eLLMs\uff0c\u901a\u8fc7AST\u548cDFG\u63d0\u5347\u786c\u4ef6\u8bbe\u8ba1\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u660e\u641c\u7d22\u7cbe\u5ea6\u3001\u8c03\u8bd5\u6548\u7387\u548c\u5b8c\u6210\u8d28\u91cf\u663e\u8457\u63d0\u9ad8\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u771f\u5b9e\u4e16\u754cHDL\u9879\u76ee\u4e2d\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faHDLxGraph\u6846\u67b6\uff0c\u96c6\u6210Graph RAG\u4e0eLLMs\uff0c\u5f15\u5165AST\u548cDFG\u7684\u53cc\u68c0\u7d22\u673a\u5236\u3002", "result": "\u641c\u7d22\u7cbe\u5ea6\u3001\u8c03\u8bd5\u6548\u7387\u548c\u5b8c\u6210\u8d28\u91cf\u5206\u522b\u63d0\u534712.04%\u300112.22%\u548c5.04%\u3002", "conclusion": "HDLxGraph\u663e\u8457\u63d0\u5347LLMs\u5728HDL\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5177\u5907\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002", "keywords": "LLMs, \u786c\u4ef6\u8bbe\u8ba1, Graph RAG, AST, DFG"}}
{"id": "2505.15738", "pdf": "https://arxiv.org/pdf/2505.15738", "abs": "https://arxiv.org/abs/2505.15738", "authors": ["Xiaoxue Yang", "Bozhidar Stevanoski", "Matthieu Meeus", "Yves-Alexandre de Montjoye"], "title": "Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are rapidly deployed in real-world applications\nranging from chatbots to agentic systems. Alignment is one of the main\napproaches used to defend against attacks such as prompt injection and\njailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even\nagainst Greedy Coordinate Gradient (GCG), a white-box attack that generates\nadversarial suffixes to induce attacker-desired outputs. However, this search\nspace over discrete tokens is extremely large, making the task of finding\nsuccessful attacks difficult. GCG has, for instance, been shown to converge to\nlocal minima, making it sensitive to initialization choices. In this paper, we\nassess the future-proof robustness of these defenses using a more informed\nthreat model: attackers who have access to some information about the alignment\nprocess. Specifically, we propose an informed white-box attack leveraging the\nintermediate model checkpoints to initialize GCG, with each checkpoint acting\nas a stepping stone for the next one. We show this approach to be highly\neffective across state-of-the-art (SOTA) defenses and models. We further show\nour informed initialization to outperform other initialization methods and show\na gradient-informed checkpoint selection strategy to greatly improve attack\nperformance and efficiency. Importantly, we also show our method to\nsuccessfully find universal adversarial suffixes -- single suffixes effective\nacross diverse inputs. Our results show that, contrary to previous beliefs,\neffective adversarial suffixes do exist against SOTA alignment-based defenses,\nthat these can be found by existing attack methods when adversaries exploit\nalignment knowledge, and that even universal suffixes exist. Taken together,\nour results highlight the brittleness of current alignment-based methods and\nthe need to consider stronger threat models when testing the safety of LLMs.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9632\u5fa1\u63aa\u65bd\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4e2d\u95f4\u6a21\u578b\u68c0\u67e5\u70b9\u521d\u59cb\u5316GCG\u653b\u51fb\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5373\u4f7f\u662f\u5148\u8fdb\u7684\u5bf9\u9f50\u9632\u5fa1\u63aa\u65bd\u4e5f\u5b58\u5728\u6709\u6548\u7684\u5bf9\u6297\u6027\u540e\u7f00\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u9f50\u9632\u5fa1\u63aa\u65bd\u7684\u8106\u5f31\u6027\uff0c\u7279\u522b\u662f\u5728\u653b\u51fb\u8005\u83b7\u5f97\u90e8\u5206\u5bf9\u9f50\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684\u767d\u76d2\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u4e2d\u95f4\u6a21\u578b\u68c0\u67e5\u70b9\u521d\u59cb\u5316GCG\u653b\u51fb\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u4fe1\u606f\u9009\u62e9\u68c0\u67e5\u70b9\u4ee5\u63d0\u9ad8\u653b\u51fb\u6548\u7387\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5148\u8fdb\u9632\u5fa1\u63aa\u65bd\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u9ad8\u6548\uff0c\u6210\u529f\u627e\u5230\u4e86\u901a\u7528\u7684\u5bf9\u6297\u6027\u540e\u7f00\uff0c\u8bc1\u660e\u4e86\u5f53\u524d\u9632\u5fa1\u63aa\u65bd\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5f53\u524d\u7684\u5bf9\u9f50\u9632\u5fa1\u63aa\u65bd\u5728\u9762\u5bf9\u77e5\u8bc6\u4e30\u5bcc\u7684\u653b\u51fb\u8005\u65f6\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u9700\u8003\u8651\u66f4\u5f3a\u7684\u5a01\u80c1\u6a21\u578b\u6765\u8bc4\u4f30LLMs\u7684\u5b89\u5168\u6027\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u5bf9\u9f50\u9632\u5fa1, \u5bf9\u6297\u653b\u51fb, GCG\u653b\u51fb, \u5b89\u5168\u6027\u8bc4\u4f30"}}
{"id": "2505.15644", "pdf": "https://arxiv.org/pdf/2505.15644", "abs": "https://arxiv.org/abs/2505.15644", "authors": ["Zhen Sun", "Ziyi Zhang", "Zeren Luo", "Zeyang Sha", "Tianshuo Cong", "Zheng Li", "Shiwen Cui", "Weiqiang Wang", "Jiaheng Wei", "Xinlei He", "Qi Li", "Qian Wang"], "title": "FragFake: A Dataset for Fine-Grained Detection of Edited Images with Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "14pages,15 figures", "summary": "Fine-grained edited image detection of localized edits in images is crucial\nfor assessing content authenticity, especially given that modern diffusion\nmodels and image editing methods can produce highly realistic manipulations.\nHowever, this domain faces three challenges: (1) Binary classifiers yield only\na global real-or-fake label without providing localization; (2) Traditional\ncomputer vision methods often rely on costly pixel-level annotations; and (3)\nNo large-scale, high-quality dataset exists for modern image-editing detection\ntechniques. To address these gaps, we develop an automated data-generation\npipeline to create FragFake, the first dedicated benchmark dataset for edited\nimage detection, which includes high-quality images from diverse editing models\nand a wide variety of edited objects. Based on FragFake, we utilize Vision\nLanguage Models (VLMs) for the first time in the task of edited image\nclassification and edited region localization. Experimental results show that\nfine-tuned VLMs achieve higher average Object Precision across all datasets,\nsignificantly outperforming pretrained models. We further conduct ablation and\ntransferability analyses to evaluate the detectors across various\nconfigurations and editing scenarios. To the best of our knowledge, this work\nis the first to reformulate localized image edit detection as a vision-language\nunderstanding task, establishing a new paradigm for the field. We anticipate\nthat this work will establish a solid foundation to facilitate and inspire\nsubsequent research endeavors in the domain of multimodal content authenticity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u7ba1\u9053\u521b\u5efaFragFake\u6570\u636e\u96c6\uff0c\u5e76\u9996\u6b21\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u7f16\u8f91\u56fe\u50cf\u68c0\u6d4b\u4efb\u52a1\uff0c\u53d6\u5f97\u4e86\u6bd4\u9884\u8bad\u7ec3\u6a21\u578b\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u4ee3\u56fe\u50cf\u7f16\u8f91\u6280\u672f\u80fd\u591f\u751f\u6210\u9ad8\u5ea6\u903c\u771f\u7684\u7be1\u6539\u56fe\u50cf\uff0c\u4f46\u73b0\u6709\u7684\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u5c40\u90e8\u5316\u4fe1\u606f\uff0c\u4e14\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u7ba1\u9053\u521b\u5efaFragFake\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u8fdb\u884c\u7f16\u8f91\u56fe\u50cf\u5206\u7c7b\u548c\u533a\u57df\u5b9a\u4f4d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684VLMs\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5c06\u5c40\u90e8\u5316\u56fe\u50cf\u7f16\u8f91\u68c0\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\uff0c\u4e3a\u591a\u6a21\u6001\u5185\u5bb9\u771f\u5b9e\u6027\u9886\u57df\u5960\u5b9a\u4e86\u65b0\u57fa\u7840\u3002", "keywords": "\u56fe\u50cf\u7f16\u8f91\u68c0\u6d4b\u3001FragFake\u6570\u636e\u96c6\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u5185\u5bb9\u771f\u5b9e\u6027\u3001\u591a\u6a21\u6001"}}
{"id": "2505.15741", "pdf": "https://arxiv.org/pdf/2505.15741", "abs": "https://arxiv.org/abs/2505.15741", "authors": ["Dikshit Chauhan", "Bapi Dutta", "Indu Bala", "Niki van Stein", "Thomas B\u00e4ck", "Anupam Yadav"], "title": "Evolutionary Computation and Large Language Models: A Survey of Methods, Synergies, and Applications", "categories": ["cs.NE", "cs.CL", "cs.MA", "I.2.7; I.2.11"], "comment": null, "summary": "Integrating Large Language Models (LLMs) and Evolutionary Computation (EC)\nrepresents a promising avenue for advancing artificial intelligence by\ncombining powerful natural language understanding with optimization and search\ncapabilities. This manuscript explores the synergistic potential of LLMs and\nEC, reviewing their intersections, complementary strengths, and emerging\napplications. We identify key opportunities where EC can enhance LLM training,\nfine-tuning, prompt engineering, and architecture search, while LLMs can, in\nturn, aid in automating the design, analysis, and interpretation of ECs. The\nmanuscript explores the synergistic integration of EC and LLMs, highlighting\ntheir bidirectional contributions to advancing artificial intelligence. It\nfirst examines how EC techniques enhance LLMs by optimizing key components such\nas prompt engineering, hyperparameter tuning, and architecture search,\ndemonstrating how evolutionary methods automate and refine these processes.\nSecondly, the survey investigates how LLMs improve EC by automating\nmetaheuristic design, tuning evolutionary algorithms, and generating adaptive\nheuristics, thereby increasing efficiency and scalability. Emerging\nco-evolutionary frameworks are discussed, showcasing applications across\ndiverse fields while acknowledging challenges like computational costs,\ninterpretability, and algorithmic convergence. The survey concludes by\nidentifying open research questions and advocating for hybrid approaches that\ncombine the strengths of EC and LLMs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u8fdb\u5316\u8ba1\u7b97\uff08EC\uff09\u7684\u534f\u540c\u6f5c\u529b\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u4e24\u8005\u7684\u7ed3\u5408\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u3002", "motivation": "\u901a\u8fc7\u6574\u5408LLMs\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u548cEC\u7684\u4f18\u5316\u4e0e\u641c\u7d22\u80fd\u529b\uff0c\u63a2\u7d22\u4eba\u5de5\u667a\u80fd\u7684\u65b0\u8fdb\u5c55\u3002", "method": "\u5206\u6790\u4e86EC\u5982\u4f55\u4f18\u5316LLMs\u7684\u8bad\u7ec3\u3001\u5fae\u8c03\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u67b6\u6784\u641c\u7d22\uff0c\u4ee5\u53caLLMs\u5982\u4f55\u81ea\u52a8\u5316EC\u7684\u8bbe\u8ba1\u3001\u5206\u6790\u4e0e\u89e3\u91ca\u3002", "result": "\u5c55\u793a\u4e86\u53cc\u5411\u589e\u5f3a\u7684\u5b9e\u73b0\u8def\u5f84\uff0c\u5305\u62ecEC\u4f18\u5316LLMs\u7684\u7ec4\u4ef6\u548cLLMs\u81ea\u52a8\u5316EC\u7684\u8bbe\u8ba1\u4e0e\u8c03\u4f18\uff0c\u5e76\u8ba8\u8bba\u4e86\u65b0\u5174\u7684\u5171\u8fdb\u5316\u6846\u67b6\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u5f00\u653e\u7684\u7814\u7a76\u95ee\u9898\uff0c\u5e76\u63d0\u5021\u7ed3\u5408EC\u548cLLMs\u4f18\u52bf\u7684\u6df7\u5408\u65b9\u6cd5\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b, \u8fdb\u5316\u8ba1\u7b97, \u4eba\u5de5\u667a\u80fd, \u534f\u540c\u4f18\u5316, \u81ea\u52a8\u5316\u8bbe\u8ba1"}}
{"id": "2505.15753", "pdf": "https://arxiv.org/pdf/2505.15753", "abs": "https://arxiv.org/abs/2505.15753", "authors": ["Taiye Chen", "Zeming Wei", "Ang Li", "Yisen Wang"], "title": "Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are known to be vulnerable to jailbreaking\nattacks, wherein adversaries exploit carefully engineered prompts to induce\nharmful or unethical responses. Such threats have raised critical concerns\nabout the safety and reliability of LLMs in real-world deployment. While\nexisting defense mechanisms partially mitigate such risks, subsequent\nadvancements in adversarial techniques have enabled novel jailbreaking methods\nto circumvent these protections, exposing the limitations of static defense\nframeworks. In this work, we explore defending against evolving jailbreaking\nthreats through the lens of context retrieval. First, we conduct a preliminary\nstudy demonstrating that even a minimal set of safety-aligned examples against\na particular jailbreak can significantly enhance robustness against this attack\npattern. Building on this insight, we further leverage the retrieval-augmented\ngeneration (RAG) techniques and propose Safety Context Retrieval (SCR), a\nscalable and robust safeguarding paradigm for LLMs against jailbreaking. Our\ncomprehensive experiments demonstrate how SCR achieves superior defensive\nperformance against both established and emerging jailbreaking tactics,\ncontributing a new paradigm to LLM safety. Our code will be available upon\npublication.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u4e0a\u4e0b\u6587\u68c0\u7d22\u9632\u5fa1LLM\u7684\u8d8a\u72f1\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u5b89\u5168\u6027\u4e0a\u4e0b\u6587\u68c0\u7d22\uff08SCR\uff09\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\uff0c\u73b0\u6709\u9632\u5fa1\u673a\u5236\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u63a2\u7d22\u52a8\u6001\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u521d\u6b65\u7814\u7a76\u9a8c\u8bc1\u5b89\u5168\u5bf9\u9f50\u793a\u4f8b\u7684\u6709\u6548\u6027\uff0c\u5e76\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u63d0\u51faSCR\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSCR\u5bf9\u73b0\u6709\u548c\u65b0\u5174\u8d8a\u72f1\u653b\u51fb\u5177\u6709\u4f18\u5f02\u9632\u5fa1\u6027\u80fd\u3002", "conclusion": "SCR\u4e3aLLM\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u7684\u9632\u5fa1\u8303\u5f0f\uff0c\u5176\u4ee3\u7801\u5c06\u5728\u53d1\u5e03\u65f6\u516c\u5f00\u3002", "keywords": "\u5927\u578b\u8bed\u8a00\u6a21\u578b,\u8d8a\u72f1\u653b\u51fb,\u5b89\u5168\u6027\u4e0a\u4e0b\u6587\u68c0\u7d22,\u68c0\u7d22\u589e\u5f3a\u751f\u6210"}}
{"id": "2505.15772", "pdf": "https://arxiv.org/pdf/2505.15772", "abs": "https://arxiv.org/abs/2505.15772", "authors": ["Cheng Yifan", "Zhang Ruoyi", "Shi Jiatong"], "title": "MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech Paralinguistic and Affect Labeling", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted by Interspeech", "summary": "Acquiring large-scale emotional speech data with strong consistency remains a\nchallenge for speech synthesis. This paper presents MIKU-PAL, a fully automated\nmultimodal pipeline for extracting high-consistency emotional speech from\nunlabeled video data. Leveraging face detection and tracking algorithms, we\ndeveloped an automatic emotion analysis system using a multimodal large\nlanguage model (MLLM). Our results demonstrate that MIKU-PAL can achieve\nhuman-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss\nkappa score) while being much cheaper and faster than human annotation. With\nthe high-quality, flexible, and consistent annotation from MIKU-PAL, we can\nannotate fine-grained speech emotion categories of up to 26 types, validated by\nhuman annotators with 83% rationality ratings. Based on our proposed system, we\nfurther released a fine-grained emotional speech dataset MIKU-EmoBench(131.2\nhours) as a new benchmark for emotional text-to-speech and visual voice\ncloning.", "AI": {"tldr": "MIKU-PAL\u662f\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u591a\u6a21\u6001\u6d41\u7a0b\uff0c\u7528\u4e8e\u4ece\u672a\u6807\u6ce8\u7684\u89c6\u9891\u6570\u636e\u4e2d\u63d0\u53d6\u9ad8\u4e00\u81f4\u6027\u7684\u60c5\u611f\u8bed\u97f3\u6570\u636e\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6210\u672c\u548c\u65f6\u95f4\uff0c\u540c\u65f6\u8fbe\u5230\u4e86\u4eba\u7c7b\u6c34\u5e73\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u5927\u89c4\u6a21\u9ad8\u4e00\u81f4\u6027\u60c5\u611f\u8bed\u97f3\u6570\u636e\u7684\u83b7\u53d6\u4e00\u76f4\u662f\u8bed\u97f3\u5408\u6210\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u9762\u90e8\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5f00\u53d1\u81ea\u52a8\u60c5\u611f\u5206\u6790\u7cfb\u7edf\u3002", "result": "MIKU-PAL\u5728MELD\u6570\u636e\u96c6\u4e0a\u8fbe\u523068.5%\u7684\u51c6\u786e\u7387\u548c0.93\u7684Fleiss kappa\u5206\u6570\uff0c\u80fd\u591f\u6807\u6ce826\u79cd\u7ec6\u7c92\u5ea6\u60c5\u611f\u7c7b\u522b\uff0c\u5e76\u901a\u8fc7\u4e8683%\u7684\u4eba\u7c7b\u5408\u7406\u6027\u8bc4\u5206\u3002", "conclusion": "MIKU-PAL\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u81ea\u52a8\u5316\u60c5\u611f\u8bed\u97f3\u6807\u6ce8\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u4e86MIKU-EmoBench\u6570\u636e\u96c6\u4f5c\u4e3a\u65b0\u57fa\u51c6\u3002", "keywords": "\u60c5\u611f\u8bed\u97f3\u3001\u591a\u6a21\u6001\u3001\u81ea\u52a8\u6807\u6ce8\u3001\u8bed\u97f3\u5408\u6210\u3001\u6570\u636e\u96c6"}}
{"id": "2505.15662", "pdf": "https://arxiv.org/pdf/2505.15662", "abs": "https://arxiv.org/abs/2505.15662", "authors": ["Jianlong Lu", "Hanqiu Peng", "Ying Chen"], "title": "Neural Quantum Digital Twins for Optimizing Quantum Annealing", "categories": ["quant-ph", "cs.AI", "cs.ET"], "comment": "20 pages, 11 figures, 2 tables", "summary": "Quantum annealers have shown potential in addressing certain combinatorial\noptimization problems, though their performance is often limited by scalability\nand errors rates. In this work, we propose a Neural Quantum Digital Twin (NQDT)\nframework that reconstructs the energy landscape of quantum many-body systems\nrelevant to quantum annealing. The digital twin models both ground and excited\nstate dynamics, enabling detailed simulation of the adiabatic evolution\nprocess. We benchmark NQDT on systems with known analytical solutions and\ndemonstrate that it accurately captures key quantum phenomena, including\nquantum criticality and phase transitions. Leveraging this framework, one can\nidentify optimal annealing schedules that minimize excitation-related errors.\nThese findings highlight the utility of neural network-based digital twins as a\ndiagnostic and optimization tool for improving the performance of quantum\nannealers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u795e\u7ecf\u91cf\u5b50\u6570\u5b57\u5b6a\u751f\uff08NQDT\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u91cd\u5efa\u91cf\u5b50\u9000\u706b\u4e2d\u591a\u4f53\u7cfb\u7edf\u7684\u80fd\u91cf\u666f\u89c2\uff0c\u6a21\u62df\u57fa\u6001\u548c\u6fc0\u53d1\u6001\u52a8\u529b\u5b66\uff0c\u4f18\u5316\u9000\u706b\u8c03\u5ea6\u4ee5\u51cf\u5c11\u8bef\u5dee\u3002", "motivation": "\u91cf\u5b50\u9000\u706b\u5668\u5728\u89e3\u51b3\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u65f6\u53d7\u9650\u4e8e\u53ef\u6269\u5c55\u6027\u548c\u8bef\u5dee\u7387\uff0c\u9700\u8981\u901a\u8fc7\u65b0\u65b9\u6cd5\u6539\u8fdb\u6027\u80fd\u3002", "method": "\u6784\u5efaNQDT\u6846\u67b6\uff0c\u6a21\u62df\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u80fd\u91cf\u666f\u89c2\u548c\u52a8\u529b\u5b66\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u5df2\u77e5\u7cfb\u7edf\u4e2d\u7684\u51c6\u786e\u6027\u3002", "result": "NQDT\u80fd\u7cbe\u786e\u6355\u6349\u91cf\u5b50\u4e34\u754c\u6027\u548c\u76f8\u53d8\u7b49\u5173\u952e\u73b0\u8c61\uff0c\u8bc6\u522b\u6700\u4f18\u9000\u706b\u8c03\u5ea6\u4ee5\u51cf\u5c11\u6fc0\u53d1\u8bef\u5dee\u3002", "conclusion": "\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6570\u5b57\u5b6a\u751f\u53ef\u4f5c\u4e3a\u8bca\u65ad\u548c\u4f18\u5316\u5de5\u5177\uff0c\u63d0\u5347\u91cf\u5b50\u9000\u706b\u5668\u7684\u6027\u80fd\u3002", "keywords": "\u91cf\u5b50\u9000\u706b, \u6570\u5b57\u5b6a\u751f, \u795e\u7ecf\u7f51\u7edc, \u591a\u4f53\u7cfb\u7edf, \u80fd\u91cf\u666f\u89c2"}}
{"id": "2505.15773", "pdf": "https://arxiv.org/pdf/2505.15773", "abs": "https://arxiv.org/abs/2505.15773", "authors": ["Yu-Xiang Luo", "Yi-Cheng Lin", "Ming-To Chuang", "Jia-Hung Chen", "I-Ning Tsai", "Pei Xing Kiew", "Yueh-Hsuan Huang", "Chien-Feng Liu", "Yu-Chen Chen", "Bo-Han Feng", "Wenze Ren", "Hung-yi Lee"], "title": "ToxicTone: A Mandarin Audio Dataset Annotated for Toxicity and Toxic Utterance Tonality", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by INTERSPEECH 2025. 5 pages", "summary": "Despite extensive research on toxic speech detection in text, a critical gap\nremains in handling spoken Mandarin audio. The lack of annotated datasets that\ncapture the unique prosodic cues and culturally specific expressions in\nMandarin leaves spoken toxicity underexplored. To address this, we introduce\nToxicTone -- the largest public dataset of its kind -- featuring detailed\nannotations that distinguish both forms of toxicity (e.g., profanity, bullying)\nand sources of toxicity (e.g., anger, sarcasm, dismissiveness). Our data,\nsourced from diverse real-world audio and organized into 13 topical categories,\nmirrors authentic communication scenarios. We also propose a multimodal\ndetection framework that integrates acoustic, linguistic, and emotional\nfeatures using state-of-the-art speech and emotion encoders. Extensive\nexperiments show our approach outperforms text-only and baseline models,\nunderscoring the essential role of speech-specific cues in revealing hidden\ntoxic expressions.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aToxicTone\u7684\u666e\u901a\u8bdd\u8bed\u97f3\u6bd2\u6027\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u68c0\u6d4b\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6bd2\u6027\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u586b\u8865\u666e\u901a\u8bdd\u8bed\u97f3\u6bd2\u6027\u68c0\u6d4b\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u6355\u6349\u72ec\u7279\u7684\u97f5\u5f8b\u7ebf\u7d22\u548c\u6587\u5316\u7279\u5b9a\u8868\u8fbe\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u58f0\u5b66\u3001\u8bed\u8a00\u548c\u60c5\u611f\u7279\u5f81\uff0c\u4f7f\u7528\u5148\u8fdb\u7684\u8bed\u97f3\u548c\u60c5\u611f\u7f16\u7801\u5668\u3002", "result": "\u8be5\u6846\u67b6\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u6587\u672c\u7684\u57fa\u51c6\u6a21\u578b\uff0c\u51f8\u663e\u4e86\u8bed\u97f3\u7279\u5b9a\u7ebf\u7d22\u7684\u91cd\u8981\u6027\u3002", "conclusion": "ToxicTone\u6570\u636e\u96c6\u548c\u591a\u6a21\u6001\u6846\u67b6\u4e3a\u8bed\u97f3\u6bd2\u6027\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u9690\u85cf\u7684\u6bd2\u6027\u8868\u8fbe\u3002", "keywords": "\u666e\u901a\u8bdd\u8bed\u97f3\u6bd2\u6027\u68c0\u6d4b\uff0c\u591a\u6a21\u6001\u6846\u67b6\uff0cToxicTone\u6570\u636e\u96c6"}}
{"id": "2505.15671", "pdf": "https://arxiv.org/pdf/2505.15671", "abs": "https://arxiv.org/abs/2505.15671", "authors": ["Hamzeh Asgharnezhad", "Afshar Shamsi", "Roohallah Alizadehsani", "Arash Mohammadi", "Hamid Alinejad-Rokny"], "title": "Enhancing Monte Carlo Dropout Performance for Uncertainty Quantification", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 5 tables, 7 figures", "summary": "Knowing the uncertainty associated with the output of a deep neural network\nis of paramount importance in making trustworthy decisions, particularly in\nhigh-stakes fields like medical diagnosis and autonomous systems. Monte Carlo\nDropout (MCD) is a widely used method for uncertainty quantification, as it can\nbe easily integrated into various deep architectures. However, conventional MCD\noften struggles with providing well-calibrated uncertainty estimates. To\naddress this, we introduce innovative frameworks that enhances MCD by\nintegrating different search solutions namely Grey Wolf Optimizer (GWO),\nBayesian Optimization (BO), and Particle Swarm Optimization (PSO) as well as an\nuncertainty-aware loss function, thereby improving the reliability of\nuncertainty quantification. We conduct comprehensive experiments using\ndifferent backbones, namely DenseNet121, ResNet50, and VGG16, on various\ndatasets, including Cats vs. Dogs, Myocarditis, Wisconsin, and a synthetic\ndataset (Circles). Our proposed algorithm outperforms the MCD baseline by 2-3%\non average in terms of both conventional accuracy and uncertainty accuracy\nwhile achieving significantly better calibration. These results highlight the\npotential of our approach to enhance the trustworthiness of deep learning\nmodels in safety-critical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u8499\u7279\u5361\u6d1bDropout\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u4f18\u5316\u7b97\u6cd5\u548c\u4e0d\u786e\u5b9a\u611f\u77e5\u635f\u5931\u51fd\u6570\uff0c\u63d0\u5347\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u9886\u57df\u5982\u533b\u7597\u8bca\u65ad\u548c\u81ea\u4e3b\u7cfb\u7edf\u4e2d\uff0c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edfMCD\u65b9\u6cd5\u5728\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5f15\u5165Grey Wolf Optimizer\u3001Bayesian Optimization\u3001Particle Swarm Optimization\u4e09\u79cd\u4f18\u5316\u7b97\u6cd5\u53ca\u4e0d\u786e\u5b9a\u611f\u77e5\u635f\u5931\u51fd\u6570\uff0c\u589e\u5f3aMCD\u7684\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u7f51\u7edc\u7ed3\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6539\u8fdb\u540e\u7684\u7b97\u6cd5\u5728\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u65b9\u9762\u5e73\u5747\u63d0\u53472-3%\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfMCD\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002", "keywords": "\u8499\u7279\u5361\u6d1bDropout, \u4e0d\u786e\u5b9a\u6027\u91cf\u5316, \u4f18\u5316\u7b97\u6cd5, \u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2505.15093", "pdf": "https://arxiv.org/pdf/2505.15093", "abs": "https://arxiv.org/abs/2505.15093", "authors": ["Jason Yang", "Wenda Chu", "Daniel Khalil", "Raul Astudillo", "Bruce J. Wittmann", "Frances H. Arnold", "Yisong Yue"], "title": "Steering Generative Models with Experimental Data for Protein Fitness Optimization", "categories": ["q-bio.BM", "cs.LG"], "comment": null, "summary": "Protein fitness optimization involves finding a protein sequence that\nmaximizes desired quantitative properties in a combinatorially large design\nspace of possible sequences. Recent developments in steering protein generative\nmodels (e.g diffusion models, language models) offer a promising approach.\nHowever, by and large, past studies have optimized surrogate rewards and/or\nutilized large amounts of labeled data for steering, making it unclear how well\nexisting methods perform and compare to each other in real-world optimization\ncampaigns where fitness is measured by low-throughput wet-lab assays. In this\nstudy, we explore fitness optimization using small amounts (hundreds) of\nlabeled sequence-fitness pairs and comprehensively evaluate strategies such as\nclassifier guidance and posterior sampling for guiding generation from\ndifferent discrete diffusion models of protein sequences. We also demonstrate\nhow guidance can be integrated into adaptive sequence selection akin to\nThompson sampling in Bayesian optimization, showing that plug-and-play guidance\nstrategies offer advantages compared to alternatives such as reinforcement\nlearning with protein language models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u86cb\u767d\u8d28\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\u4f7f\u7528\u5c11\u91cf\u6807\u8bb0\u6570\u636e\u4f18\u5316\u86cb\u767d\u8d28\u9002\u5e94\u6027\u7684\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u7b56\u7565\u7684\u6307\u5bfc\u751f\u6210\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7c7b\u4f3cThompson\u91c7\u6837\u7684\u81ea\u9002\u5e94\u5e8f\u5217\u9009\u62e9\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u86cb\u767d\u8d28\u8bbe\u8ba1\u65b9\u6cd5\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u7684\u8868\u73b0\u548c\u6bd4\u8f83\u5c1a\u4e0d\u660e\u786e\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u901a\u91cf\u6e7f\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u3002", "method": "\u5229\u7528\u5c11\u91cf\u6807\u8bb0\u5e8f\u5217-\u9002\u5e94\u6027\u6570\u636e\uff0c\u8bc4\u4f30\u4e86\u5206\u7c7b\u5668\u6307\u5bfc\u548c\u540e\u9a8c\u91c7\u6837\u7b49\u7b56\u7565\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u7c7b\u4f3cThompson\u91c7\u6837\u7684\u81ea\u9002\u5e94\u5e8f\u5217\u9009\u62e9\u4e2d\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5373\u63d2\u5373\u7528\u7684\u6307\u5bfc\u7b56\u7565\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u4f8b\u5982\u57fa\u4e8e\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u86cb\u767d\u8d28\u9002\u5e94\u6027\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u86cb\u767d\u8d28\u4f18\u5316\uff0c\u751f\u6210\u6a21\u578b\uff0c\u5206\u7c7b\u5668\u6307\u5bfc\uff0c\u540e\u9a8c\u91c7\u6837\uff0c\u81ea\u9002\u5e94\u5e8f\u5217\u9009\u62e9"}}
{"id": "2505.15120", "pdf": "https://arxiv.org/pdf/2505.15120", "abs": "https://arxiv.org/abs/2505.15120", "authors": ["Muniba Noreen", "Furqan Shaukat"], "title": "Lung Nodule-SSM: Self-Supervised Lung Nodule Detection and Classification in Thoracic CT Images", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Lung cancer remains among the deadliest types of cancer in recent decades,\nand early lung nodule detection is crucial for improving patient outcomes. The\nlimited availability of annotated medical imaging data remains a bottleneck in\ndeveloping accurate computer-aided diagnosis (CAD) systems. Self-supervised\nlearning can help leverage large amounts of unlabeled data to develop more\nrobust CAD systems. With the recent advent of transformer-based architecture\nand their ability to generalize to unseen tasks, there has been an effort\nwithin the healthcare community to adapt them to various medical downstream\ntasks. Thus, we propose a novel \"LungNodule-SSM\" method, which utilizes\nselfsupervised learning with DINOv2 as a backbone to enhance lung nodule\ndetection and classification without annotated data. Our methodology has two\nstages: firstly, the DINOv2 model is pre-trained on unlabeled CT scans to learn\nrobust feature representations, then secondly, these features are fine-tuned\nusing transformer-based architectures for lesionlevel detection and accurate\nlung nodule diagnosis. The proposed method has been evaluated on the\nchallenging LUNA 16 dataset, consisting of 888 CT scans, and compared with SOTA\nmethods. Our experimental results show the superiority of our proposed method\nwith an accuracy of 98.37%, explaining its effectiveness in lung nodule\ndetection. The source code, datasets, and pre-processed data can be accessed\nusing the\nlink:https://github.com/EMeRALDsNRPU/Lung-Nodule-SSM-Self-Supervised-Lung-Nodule-Detection-and-Classification/tree/main", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5LungNodule-SSM\uff0c\u7528\u4e8e\u65e0\u6807\u6ce8\u6570\u636e\u7684\u80ba\u7ed3\u8282\u68c0\u6d4b\u4e0e\u5206\u7c7b\uff0c\u901a\u8fc7DINOv2\u9884\u8bad\u7ec3\u548ctransformer\u67b6\u6784\u5fae\u8c03\uff0c\u5728LUNA 16\u6570\u636e\u96c6\u4e0a\u8fbe\u523098.37%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u65e9\u671f\u80ba\u7ed3\u8282\u68c0\u6d4b\u5bf9\u63d0\u9ad8\u80ba\u764c\u60a3\u8005\u751f\u5b58\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6807\u6ce8\u533b\u5b66\u5f71\u50cf\u6570\u636e\u7684\u7a00\u7f3a\u9650\u5236\u4e86\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u7684\u5f00\u53d1\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u5148\u901a\u8fc7DINOv2\u5728\u65e0\u6807\u6ce8CT\u626b\u63cf\u4e0a\u9884\u8bad\u7ec3\u5b66\u4e60\u9c81\u68d2\u7279\u5f81\uff0c\u518d\u7528transformer\u67b6\u6784\u5fae\u8c03\u8fdb\u884c\u75c5\u7076\u68c0\u6d4b\u4e0e\u7ed3\u8282\u5206\u7c7b\u3002", "result": "\u5728LUNA 16\u6570\u636e\u96c6\uff08888\u4e2aCT\u626b\u63cf\uff09\u4e0a\u6d4b\u8bd5\uff0c\u51c6\u786e\u7387\u8fbe98.37%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LungNodule-SSM\u65b9\u6cd5\u5728\u80ba\u7ed3\u8282\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002", "keywords": "\u80ba\u7ed3\u8282\u68c0\u6d4b\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u3001DINOv2\u3001transformer\u3001\u533b\u7597\u5f71\u50cf"}}
{"id": "2505.15687", "pdf": "https://arxiv.org/pdf/2505.15687", "abs": "https://arxiv.org/abs/2505.15687", "authors": ["Zhe Xu", "Cheng Jin", "Yihui Wang", "Ziyi Liu", "Hao Chen"], "title": "Discovering Pathology Rationale and Token Allocation for Efficient Multimodal Pathology Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal pathological image understanding has garnered widespread interest\ndue to its potential to improve diagnostic accuracy and enable personalized\ntreatment through integrated visual and textual data. However, existing methods\nexhibit limited reasoning capabilities, which hamper their ability to handle\ncomplex diagnostic scenarios. Additionally, the enormous size of pathological\nimages leads to severe computational burdens, further restricting their\npractical deployment. To address these limitations, we introduce a novel\nbilateral reinforcement learning framework comprising two synergistic branches.\nOne reinforcement branch enhances the reasoning capability by enabling the\nmodel to learn task-specific decision processes, i.e., pathology rationales,\ndirectly from labels without explicit reasoning supervision. While the other\nbranch dynamically allocates a tailored number of tokens to different images\nbased on both their visual content and task context, thereby optimizing\ncomputational efficiency. We apply our method to various pathological tasks\nsuch as visual question answering, cancer subtyping, and lesion detection.\nExtensive experiments show an average +41.7 absolute performance improvement\nwith 70.3% lower inference costs over the base models, achieving both reasoning\naccuracy and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u5411\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u63d0\u5347\u75c5\u7406\u56fe\u50cf\u7406\u89e3\u7684\u63a8\u7406\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u75c5\u7406\u8bca\u65ad\u573a\u666f\u4e2d\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u8ba1\u7b97\u8d1f\u62c5\u5927\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u53cc\u5411\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4e00\u4e2a\u5206\u652f\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u53e6\u4e00\u4e2a\u5206\u652f\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6027\u80fd\u5e73\u5747\u63d0\u534741.7%\uff0c\u63a8\u7406\u6210\u672c\u964d\u4f4e70.3%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002", "keywords": "\u591a\u6a21\u6001\u75c5\u7406\u56fe\u50cf\u7406\u89e3\uff0c\u53cc\u5411\u5f3a\u5316\u5b66\u4e60\uff0c\u63a8\u7406\u80fd\u529b\uff0c\u8ba1\u7b97\u6548\u7387"}}
{"id": "2505.15703", "pdf": "https://arxiv.org/pdf/2505.15703", "abs": "https://arxiv.org/abs/2505.15703", "authors": ["Xiaodong Mei", "Sheng Wang", "Jie Cheng", "Yingbing Chen", "Dan Xu"], "title": "HAMF: A Hybrid Attention-Mamba Framework for Joint Scene Context Understanding and Future Motion Representation Learning", "categories": ["cs.CV", "cs.AI"], "comment": "In submission", "summary": "Motion forecasting represents a critical challenge in autonomous driving\nsystems, requiring accurate prediction of surrounding agents' future\ntrajectories. While existing approaches predict future motion states with the\nextracted scene context feature from historical agent trajectories and road\nlayouts, they suffer from the information degradation during the scene feature\nencoding. To address the limitation, we propose HAMF, a novel motion\nforecasting framework that learns future motion representations with the scene\ncontext encoding jointly, to coherently combine the scene understanding and\nfuture motion state prediction. We first embed the observed agent states and\nmap information into 1D token sequences, together with the target multi-modal\nfuture motion features as a set of learnable tokens. Then we design a unified\nAttention-based encoder, which synergistically combines self-attention and\ncross-attention mechanisms to model the scene context information and aggregate\nfuture motion features jointly. Complementing the encoder, we implement the\nMamba module in the decoding stage to further preserve the consistency and\ncorrelations among the learned future motion representations, to generate the\naccurate and diverse final trajectories. Extensive experiments on Argoverse 2\nbenchmark demonstrate that our hybrid Attention-Mamba model achieves\nstate-of-the-art motion forecasting performance with the simple and lightweight\narchitecture.", "AI": {"tldr": "HAMF\u662f\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8fd0\u52a8\u9884\u6d4b\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u573a\u666f\u4e0a\u4e0b\u6587\u7f16\u7801\u548c\u672a\u6765\u8fd0\u52a8\u8868\u5f81\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u573a\u666f\u7279\u5f81\u7f16\u7801\u4e2d\u4fe1\u606f\u9000\u5316\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u75281D\u4ee4\u724c\u5e8f\u5217\u5d4c\u5165\u89c2\u6d4b\u5230\u7684\u4ee3\u7406\u72b6\u6001\u548c\u5730\u56fe\u4fe1\u606f\uff0c\u8bbe\u8ba1\u7edf\u4e00\u7684\u6ce8\u610f\u529b\u7f16\u7801\u5668\uff0c\u5e76\u7ed3\u5408Mamba\u6a21\u5757\u89e3\u7801\u3002", "result": "\u5728Argoverse 2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "HAMF\u901a\u8fc7\u8f7b\u91cf\u7ea7\u67b6\u6784\u548c\u8054\u5408\u5b66\u4e60\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002", "keywords": "\u8fd0\u52a8\u9884\u6d4b\u3001\u81ea\u52a8\u9a7e\u9a76\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001Mamba\u6a21\u5757\u3001\u8f7b\u91cf\u7ea7\u67b6\u6784"}}
{"id": "2505.15157", "pdf": "https://arxiv.org/pdf/2505.15157", "abs": "https://arxiv.org/abs/2505.15157", "authors": ["Mohit Sharma", "Adam Fishman", "Vikash Kumar", "Chris Paxton", "Oliver Kroemer"], "title": "Cascaded Diffusion Models for Neural Motion Planning", "categories": ["cs.RO", "cs.LG"], "comment": "ICRA'25", "summary": "Robots in the real world need to perceive and move to goals in complex\nenvironments without collisions. Avoiding collisions is especially difficult\nwhen relying on sensor perception and when goals are among clutter. Diffusion\npolicies and other generative models have shown strong performance in solving\nlocal planning problems, but often struggle at avoiding all of the subtle\nconstraint violations that characterize truly challenging global motion\nplanning problems. In this work, we propose an approach for learning global\nmotion planning using diffusion policies, allowing the robot to generate full\ntrajectories through complex scenes and reasoning about multiple obstacles\nalong the path. Our approach uses cascaded hierarchical models which unify\nglobal prediction and local refinement together with online plan repair to\nensure the trajectories are collision free. Our method outperforms (by ~5%) a\nwide variety of baselines on challenging tasks in multiple domains including\nnavigation and manipulation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7b56\u7565\u7684\u5168\u5c40\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea7\u8054\u5206\u5c42\u6a21\u578b\u7ed3\u5408\u5168\u5c40\u9884\u6d4b\u548c\u5c40\u90e8\u4f18\u5316\uff0c\u5b9e\u73b0\u590d\u6742\u573a\u666f\u4e2d\u7684\u65e0\u78b0\u649e\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u4f9d\u8d56\u4f20\u611f\u5668\u611f\u77e5\u65f6\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u907f\u514d\u78b0\u649e\u7684\u6311\u6218\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6269\u6563\u7b56\u7565\u5b66\u4e60\u5168\u5c40\u8fd0\u52a8\u89c4\u5212\uff0c\u7ed3\u5408\u7ea7\u8054\u5206\u5c42\u6a21\u578b\u548c\u5728\u7ebf\u4fee\u590d\uff0c\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\u3002", "result": "\u5728\u5bfc\u822a\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u7ea65%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u590d\u6742\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "keywords": "\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212,\u6269\u6563\u7b56\u7565,\u65e0\u78b0\u649e\u8f68\u8ff9,\u7ea7\u8054\u5206\u5c42\u6a21\u578b"}}
{"id": "2505.15175", "pdf": "https://arxiv.org/pdf/2505.15175", "abs": "https://arxiv.org/abs/2505.15175", "authors": ["Diego Granziol", "Donald Flynn"], "title": "A Linear Approach to Data Poisoning", "categories": ["stat.ML", "cs.CR", "cs.LG", "math.ST", "stat.TH"], "comment": "9 pages, 9 Figures", "summary": "We investigate the theoretical foundations of data poisoning attacks in\nmachine learning models. Our analysis reveals that the Hessian with respect to\nthe input serves as a diagnostic tool for detecting poisoning, exhibiting\nspectral signatures that characterize compromised datasets. We use random\nmatrix theory (RMT) to develop a theory for the impact of poisoning proportion\nand regularisation on attack efficacy in linear regression. Through QR stepwise\nregression, we study the spectral signatures of the Hessian in multi-output\nregression. We perform experiments on deep networks to show experimentally that\nthis theory extends to modern convolutional and transformer networks under the\ncross-entropy loss. Based on these insights we develop preliminary algorithms\nto determine if a network has been poisoned and remedies which do not require\nfurther training.", "AI": {"tldr": "\u7814\u7a76\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u6570\u636e\u6295\u6bd2\u653b\u51fb\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f93\u5165Hessian\u77e9\u9635\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u77e9\u9635\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u7d22\u6570\u636e\u6295\u6bd2\u653b\u51fb\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8bc6\u522b\u653b\u51fb\u5bf9\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u68c0\u6d4b\u548c\u8865\u6551\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u968f\u673a\u77e9\u9635\u7406\u8bba\u5206\u6790\u7ebf\u6027\u56de\u5f52\u4e2d\u7684\u6295\u6bd2\u6bd4\u4f8b\u548c\u6b63\u5219\u5316\u5f71\u54cd\uff0c\u901a\u8fc7QR\u9010\u6b65\u56de\u5f52\u7814\u7a76Hessian\u77e9\u9635\u7684\u8c31\u7279\u5f81\uff0c\u5e76\u5728\u6df1\u5ea6\u7f51\u7edc\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "Hessian\u77e9\u9635\u7684\u8c31\u7279\u5f81\u53ef\u4f5c\u4e3a\u6295\u6bd2\u68c0\u6d4b\u5de5\u5177\uff0c\u7406\u8bba\u9002\u7528\u4e8e\u5377\u79ef\u548cTransformer\u7f51\u7edc\uff0c\u5e76\u5f00\u53d1\u4e86\u521d\u6b65\u7684\u68c0\u6d4b\u548c\u8865\u6551\u7b97\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u6295\u6bd2\u653b\u51fb\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u4e3a\u5b89\u5168\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "keywords": "\u6570\u636e\u6295\u6bd2, Hessian\u77e9\u9635, \u968f\u673a\u77e9\u9635\u7406\u8bba, QR\u56de\u5f52, \u6df1\u5ea6\u5b66\u4e60"}}
{"id": "2505.15203", "pdf": "https://arxiv.org/pdf/2505.15203", "abs": "https://arxiv.org/abs/2505.15203", "authors": ["Rina Tazaki", "Tomoyuki Akiyama", "Akira Furui"], "title": "EEG-Based Inter-Patient Epileptic Seizure Detection Combining Domain Adversarial Training with CNN-BiLSTM Network", "categories": ["eess.SP", "cs.LG"], "comment": "6 pages, 4 figures, accepted at IEEE EMBC 2025", "summary": "Automated epileptic seizure detection from electroencephalogram (EEG) remains\nchallenging due to significant individual differences in EEG patterns across\npatients. While existing studies achieve high accuracy with patient-specific\napproaches, they face difficulties in generalizing to new patients. To address\nthis, we propose a detection framework combining domain adversarial training\nwith a convolutional neural network (CNN) and a bidirectional long short-term\nmemory (BiLSTM). First, the CNN extracts local patient-invariant features\nthrough domain adversarial training, which optimizes seizure detection accuracy\nwhile minimizing patient-specific characteristics. Then, the BiLSTM captures\ntemporal dependencies in the extracted features to model seizure evolution\npatterns. Evaluation using EEG recordings from 20 patients with focal epilepsy\ndemonstrated superior performance over non-adversarial methods, achieving high\ndetection accuracy across different patients. The integration of adversarial\ntraining with temporal modeling enables robust cross-patient seizure detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9886\u57df\u5bf9\u6297\u8bad\u7ec3\u548c\u65f6\u5e8f\u6a21\u578b\uff08CNN\u4e0eBiLSTM\uff09\u7684\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\u6846\u67b6\uff0c\u80fd\u591f\u8de8\u60a3\u8005\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u3002", "motivation": "\u766b\u75eb\u60a3\u8005\u7684\u8111\u7535\u56fe\uff08EEG\uff09\u6a21\u5f0f\u5b58\u5728\u663e\u8457\u4e2a\u4f53\u5dee\u5f02\uff0c\u73b0\u6709\u60a3\u8005\u7279\u5f02\u6027\u65b9\u6cd5\u5728\u65b0\u60a3\u8005\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528CNN\u901a\u8fc7\u9886\u57df\u5bf9\u6297\u8bad\u7ec3\u63d0\u53d6\u60a3\u8005\u4e0d\u53d8\u7279\u5f81\uff0c\u518d\u7528BiLSTM\u6355\u83b7\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\u53d1\u4f5c\u6a21\u5f0f\u3002", "result": "\u572820\u540d\u60a3\u8005\u7684EEG\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u4f18\u4e8e\u975e\u5bf9\u6297\u65b9\u6cd5\uff0c\u5b9e\u73b0\u8de8\u60a3\u8005\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u3002", "conclusion": "\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u4e0e\u65f6\u7a7a\u5efa\u6a21\u7684\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u8de8\u60a3\u8005\u766b\u75eb\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3002", "keywords": ""}}
{"id": "2505.15740", "pdf": "https://arxiv.org/pdf/2505.15740", "abs": "https://arxiv.org/abs/2505.15740", "authors": ["Jilin Hu", "Jianyu Zhang", "Yongwang Zhao", "Talia Ringer"], "title": "HybridProver: Augmenting Theorem Proving with LLM-Driven Proof Synthesis and Refinement", "categories": ["cs.FL", "cs.AI", "cs.SE"], "comment": null, "summary": "Formal methods is pivotal for verifying the reliability of critical systems\nthrough rigorous mathematical proofs. However, its adoption is hindered by\nlabor-intensive manual proofs and the expertise required to use theorem\nprovers. Recent advancements in large language models (LLMs) offer new\nopportunities for automated theorem proving. Two promising approaches are\ngenerating tactics step by step and generating a whole proof directly with an\nLLM. However, existing work makes no attempt to combine the two approaches. In\nthis work, we introduce HybridProver, a dual-model proof synthesis framework\nthat combines tactic-based generation and whole-proof synthesis to harness the\nbenefits of both approaches. HybridProver generates whole proof candidates for\nevaluation directly, then extracts proof sketches from those candidates. It\nthen uses a tactic-based generation model that integrates automated tools to\ncomplete the sketches via stepwise refinement. We implement HybridProver for\nthe Isabelle theorem prover and fine-tune LLMs on our optimized Isabelle\ndatasets. Evaluation on the miniF2F dataset illustrates HybridProver's\neffectiveness. We achieve a 59.4% success rate on miniF2F, where the previous\nSOTA is 56.1%. Our ablation studies show that this SOTA result is attributable\nto combining whole-proof and tactic-based generation. Additionally, we show how\nthe dataset quality, training parameters, and sampling diversity affect the\nfinal result during automated theorem proving with LLMs. All of our code,\ndatasets, and LLMs are open source.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7ed3\u5408\u9010\u6b65\u751f\u6210\u7b56\u7565\u548c\u76f4\u63a5\u751f\u6210\u5b8c\u6574\u8bc1\u660e\u7684\u53cc\u6a21\u578b\u8bc1\u660e\u6846\u67b6HybridProver\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5b9a\u7406\u8bc1\u660e\uff0c\u5e76\u5728Isabelle\u4e2d\u5b9e\u73b0\u4e8659.4%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u5c3d\u7ba1\u5f62\u5f0f\u5316\u65b9\u6cd5\u5bf9\u5173\u952e\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u9a8c\u8bc1\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5176\u5e94\u7528\u53d7\u5230\u624b\u52a8\u8bc1\u660e\u7684\u7e41\u7410\u548c\u5b9a\u7406\u8bc1\u660e\u5668\u4f7f\u7528\u95e8\u69db\u7684\u9650\u5236\u3002\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\u4e3a\u81ea\u52a8\u5316\u5b9a\u7406\u8bc1\u660e\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "HybridProver\u7ed3\u5408\u4e86\u9010\u6b65\u751f\u6210\u7b56\u7565\u548c\u76f4\u63a5\u751f\u6210\u5b8c\u6574\u8bc1\u660e\u7684\u65b9\u6cd5\uff0c\u9996\u5148\u751f\u6210\u5019\u9009\u5b8c\u6574\u8bc1\u660e\u5e76\u63d0\u53d6\u8bc1\u660e\u8349\u56fe\uff0c\u7136\u540e\u901a\u8fc7\u9010\u6b65\u7ec6\u5316\u5b8c\u6210\u8349\u56fe\u3002", "result": "\u5728miniF2F\u6570\u636e\u96c6\u4e0a\uff0cHybridProver\u7684\u6210\u529f\u7387\u8fbe\u523059.4%\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u768456.1%\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e00\u7ed3\u679c\u5f52\u529f\u4e8e\u4e24\u79cd\u65b9\u6cd5\u7684\u7ed3\u5408\u3002", "conclusion": "HybridProver\u5c55\u793a\u4e86\u7ed3\u5408\u9010\u6b65\u751f\u6210\u548c\u76f4\u63a5\u751f\u6210\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u5206\u6790\u4e86\u6570\u636e\u96c6\u8d28\u91cf\u3001\u8bad\u7ec3\u53c2\u6570\u548c\u91c7\u6837\u591a\u6837\u6027\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002\u6240\u6709\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548cLLMs\u5747\u5df2\u5f00\u6e90\u3002", "keywords": "\u5f62\u5f0f\u5316\u65b9\u6cd5,\u81ea\u52a8\u5316\u5b9a\u7406\u8bc1\u660e,\u5927\u8bed\u8a00\u6a21\u578b,HybridProver,Isabelle"}}
{"id": "2505.15215", "pdf": "https://arxiv.org/pdf/2505.15215", "abs": "https://arxiv.org/abs/2505.15215", "authors": ["Otto Tabell", "Santtu Tikka", "Juha Karvanen"], "title": "Clustering and Pruning in Causal Data Fusion", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Data fusion, the process of combining observational and experimental data,\ncan enable the identification of causal effects that would otherwise remain\nnon-identifiable. Although identification algorithms have been developed for\nspecific scenarios, do-calculus remains the only general-purpose tool for\ncausal data fusion, particularly when variables are present in some data\nsources but not others. However, approaches based on do-calculus may encounter\ncomputational challenges as the number of variables increases and the causal\ngraph grows in complexity. Consequently, there exists a need to reduce the size\nof such models while preserving the essential features. For this purpose, we\npropose pruning (removing unnecessary variables) and clustering (combining\nvariables) as preprocessing operations for causal data fusion. We generalize\nearlier results on a single data source and derive conditions for applying\npruning and clustering in the case of multiple data sources. We give sufficient\nconditions for inferring the identifiability or non-identifiability of a causal\neffect in a larger graph based on a smaller graph and show how to obtain the\ncorresponding identifying functional for identifiable causal effects. Examples\nfrom epidemiology and social science demonstrate the use of the results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u526a\u679d\u548c\u805a\u7c7b\u9884\u5904\u7406\u64cd\u4f5c\u6765\u7b80\u5316\u56e0\u679c\u6570\u636e\u878d\u5408\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3do-calculus\u5728\u9ad8\u590d\u6742\u5ea6\u56e0\u679c\u56fe\u4e2d\u7684\u8ba1\u7b97\u6311\u6218\u3002", "motivation": "\u56e0\u679c\u6570\u636e\u878d\u5408\u5728\u9ad8\u7ef4\u5ea6\u6216\u590d\u6742\u56e0\u679c\u56fe\u4e2d\u7684\u5e94\u7528\u9762\u4e34\u8ba1\u7b97\u6311\u6218\uff0c\u9700\u8981\u7b80\u5316\u6a21\u578b\u540c\u65f6\u4fdd\u7559\u5173\u952e\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u526a\u679d\uff08\u79fb\u9664\u4e0d\u5fc5\u8981\u53d8\u91cf\uff09\u548c\u805a\u7c7b\uff08\u5408\u5e76\u53d8\u91cf\uff09\u4f5c\u4e3a\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u5e76\u63a8\u5e7f\u4e86\u591a\u6570\u636e\u6e90\u6761\u4ef6\u4e0b\u7684\u5e94\u7528\u3002", "result": "\u63a8\u5bfc\u4e86\u5728\u8f83\u5927\u56fe\u4e2d\u57fa\u4e8e\u8f83\u5c0f\u56fe\u63a8\u65ad\u56e0\u679c\u6548\u5e94\u53ef\u8bc6\u522b\u6027\u7684\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u5c55\u793a\u4e86\u53ef\u8bc6\u522b\u56e0\u679c\u6548\u5e94\u7684\u529f\u80fd\u8868\u8fbe\u5f0f\u3002", "conclusion": "\u526a\u679d\u548c\u805a\u7c7b\u9884\u5904\u7406\u6709\u6548\u7b80\u5316\u4e86\u56e0\u679c\u6570\u636e\u878d\u5408\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "keywords": "\u6570\u636e\u878d\u5408,\u56e0\u679c\u63a8\u65ad,\u526a\u679d,\u805a\u7c7b,\u53ef\u8bc6\u522b\u6027"}}
{"id": "2505.15218", "pdf": "https://arxiv.org/pdf/2505.15218", "abs": "https://arxiv.org/abs/2505.15218", "authors": ["Itsuki Yazawa", "Seitaro Yoneda", "Akira Furui"], "title": "Recognition of Unseen Combined Motions via Convex Combination-based EMG Pattern Synthesis for Myoelectric Control", "categories": ["eess.SP", "cs.LG"], "comment": "6 pages, 8 figures, accepted at IEEE EMBC 2025", "summary": "Electromyogram (EMG) signals recorded from the skin surface enable intuitive\ncontrol of assistive devices such as prosthetic limbs. However, in EMG-based\nmotion recognition, collecting comprehensive training data for all target\nmotions remains challenging, particularly for complex combined motions. This\npaper proposes a method to efficiently recognize combined motions using\nsynthetic EMG data generated through convex combinations of basic motion\npatterns. Instead of measuring all possible combined motions, the proposed\nmethod utilizes measured basic motion data along with synthetically combined\nmotion data for training. This approach expands the range of recognizable\ncombined motions while minimizing the required training data collection. We\nevaluated the effectiveness of the proposed method through an upper limb motion\nclassification experiment with eight subjects. The experimental results\ndemonstrated that the proposed method improved the classification accuracy for\nunseen combined motions by approximately 17%.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u57fa\u672c\u8fd0\u52a8\u6a21\u5f0f\u5408\u6210EMG\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u8bc6\u522b\u7ec4\u5408\u8fd0\u52a8\uff0c\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u6536\u96c6\u91cf\u3002", "motivation": "\u89e3\u51b3EMG\u4fe1\u53f7\u8bc6\u522b\u7ec4\u5408\u8fd0\u52a8\u65f6\u6570\u636e\u6536\u96c6\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u590d\u6742\u7ec4\u5408\u8fd0\u52a8\u3002", "method": "\u901a\u8fc7\u57fa\u672c\u8fd0\u52a8\u6570\u636e\u7684\u51f8\u7ec4\u5408\u751f\u6210\u5408\u6210EMG\u6570\u636e\uff0c\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u672a\u89c1\u7ec4\u5408\u8fd0\u52a8\u7684\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u5347\u7ea617%\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u65b9\u6cd5\u53ef\u6269\u5c55\u8bc6\u522b\u8303\u56f4\u5e76\u51cf\u5c11\u6570\u636e\u6536\u96c6\u9700\u6c42\u3002", "keywords": "EMG\u4fe1\u53f7\u3001\u7ec4\u5408\u8fd0\u52a8\u8bc6\u522b\u3001\u5408\u6210\u6570\u636e\u3001\u51f8\u7ec4\u5408"}}
{"id": "2505.15219", "pdf": "https://arxiv.org/pdf/2505.15219", "abs": "https://arxiv.org/abs/2505.15219", "authors": ["Yao Du", "Huawei Fan", "Xingang Wang"], "title": "Versatile Reservoir Computing for Heterogeneous Complex Networks", "categories": ["nlin.CD", "cs.LG"], "comment": "5 pages, 4 figures", "summary": "A new machine learning scheme, termed versatile reservoir computing, is\nproposed for sustaining the dynamics of heterogeneous complex networks. We show\nthat a single, small-scale reservoir computer trained on time series from a\nsubset of elements is able to replicate the dynamics of any element in a\nlarge-scale complex network, though the elements are of different intrinsic\nparameters and connectivities. Furthermore, by substituting failed elements\nwith the trained machine, we demonstrate that the collective dynamics of the\nnetwork can be preserved accurately over a finite time horizon. The capability\nand effectiveness of the proposed scheme are validated on three representative\nnetwork models: a homogeneous complex network of non-identical phase\noscillators, a heterogeneous complex network of non-identical phase\noscillators, and a heterogeneous complex network of non-identical chaotic\noscillators.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u529f\u80fd\u50a8\u5c42\u8ba1\u7b97\u7684\u65b0\u673a\u5668\u5b66\u4e60\u65b9\u6848\uff0c\u7528\u4e8e\u7ef4\u6301\u5f02\u6784\u590d\u6742\u7f51\u7edc\u7684\u52a8\u6001\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u5c0f\u578b\u50a8\u5c42\u8ba1\u7b97\u673a\u590d\u5236\u5927\u89c4\u6a21\u590d\u6742\u7f51\u7edc\u4e2d\u4e0d\u540c\u53c2\u6570\u548c\u8fde\u63a5\u6027\u5143\u7d20\u7684\u52a8\u6001\u3002", "method": "\u5229\u7528\u5355\u53f0\u5c0f\u578b\u50a8\u5c42\u8ba1\u7b97\u673a\uff0c\u57fa\u4e8e\u90e8\u5206\u5143\u7d20\u7684\u65f6\u95f4\u5e8f\u5217\u8bad\u7ec3\uff0c\u6a21\u62df\u7f51\u7edc\u4e2d\u4efb\u610f\u5143\u7d20\u7684\u52a8\u6001\uff1b\u66ff\u6362\u6545\u969c\u5143\u7d20\u4ee5\u4fdd\u6301\u7f51\u7edc\u96c6\u4f53\u52a8\u6001\u3002", "result": "\u5728\u4e09\u79cd\u4ee3\u8868\u6027\u7f51\u7edc\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6848\u7684\u6709\u6548\u6027\uff0c\u53ef\u51c6\u786e\u7ef4\u6301\u7f51\u7edc\u52a8\u6001\u3002", "conclusion": "\u591a\u529f\u80fd\u50a8\u5c42\u8ba1\u7b97\u80fd\u6709\u6548\u590d\u5236\u5e76\u7ef4\u6301\u5f02\u6784\u590d\u6742\u7f51\u7edc\u7684\u52a8\u6001\u3002", "keywords": "\u50a8\u5c42\u8ba1\u7b97,\u590d\u6742\u7f51\u7edc,\u52a8\u6001\u590d\u5236,\u673a\u5668\u5b66\u4e60"}}
{"id": "2505.15765", "pdf": "https://arxiv.org/pdf/2505.15765", "abs": "https://arxiv.org/abs/2505.15765", "authors": ["Kaizhi Zheng", "Ruijian Zhang", "Jing Gu", "Jie Yang", "Xin Eric Wang"], "title": "Constructing a 3D Town from a Single Image", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Acquiring detailed 3D scenes typically demands costly equipment, multi-view\ndata, or labor-intensive modeling. Therefore, a lightweight alternative,\ngenerating complex 3D scenes from a single top-down image, plays an essential\nrole in real-world applications. While recent 3D generative models have\nachieved remarkable results at the object level, their extension to full-scene\ngeneration often leads to inconsistent geometry, layout hallucinations, and\nlow-quality meshes. In this work, we introduce 3DTown, a training-free\nframework designed to synthesize realistic and coherent 3D scenes from a single\ntop-down view. Our method is grounded in two principles: region-based\ngeneration to improve image-to-3D alignment and resolution, and spatial-aware\n3D inpainting to ensure global scene coherence and high-quality geometry\ngeneration. Specifically, we decompose the input image into overlapping regions\nand generate each using a pretrained 3D object generator, followed by a masked\nrectified flow inpainting process that fills in missing geometry while\nmaintaining structural continuity. This modular design allows us to overcome\nresolution bottlenecks and preserve spatial structure without requiring 3D\nsupervision or fine-tuning. Extensive experiments across diverse scenes show\nthat 3DTown outperforms state-of-the-art baselines, including Trellis,\nHunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and\ntexture fidelity. Our results demonstrate that high-quality 3D town generation\nis achievable from a single image using a principled, training-free approach.", "AI": {"tldr": "3DTown\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u4ec5\u9700\u5355\u5f20\u4fef\u89c6\u56fe\u5373\u53ef\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u4e09\u7ef4\u573a\u666f\uff0c\u901a\u8fc7\u533a\u57df\u751f\u6210\u548c\u7a7a\u95f4\u611f\u77e5\u4fee\u590d\u786e\u4fdd\u4e00\u81f4\u6027\u4e0e\u51e0\u4f55\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u751f\u6210\u4e2d\u5b58\u5728\u51e0\u4f55\u4e0d\u4e00\u81f4\u3001\u5e03\u5c40\u5e7b\u89c9\u548c\u4f4e\u8d28\u91cf\u7f51\u683c\u95ee\u9898\uff0c\u9700\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u533a\u57df\u5206\u89e3\u4e0e\u9884\u8bad\u7ec33D\u751f\u6210\u5668\u7ed3\u5408\uff0c\u901a\u8fc7\u63a9\u819c\u77eb\u6b63\u6d41\u4fee\u590d\u786e\u4fdd\u5168\u5c40\u4e00\u81f4\u6027\u548c\u9ad8\u8d28\u91cf\u51e0\u4f55\u3002", "result": "3DTown\u5728\u51e0\u4f55\u8d28\u91cf\u3001\u7a7a\u95f4\u8fde\u8d2f\u6027\u548c\u7eb9\u7406\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5355\u56fe\u751f\u6210\u9ad8\u8d28\u91cf3D\u573a\u666f\u53ef\u901a\u8fc7\u65e0\u8bad\u7ec3\u3001\u6a21\u5757\u5316\u65b9\u6cd5\u5b9e\u73b0\u3002", "keywords": "3DTown, 3D\u573a\u666f\u751f\u6210, \u5355\u89c6\u56fe, \u65e0\u8bad\u7ec3, \u51e0\u4f55\u4fee\u590d"}}
{"id": "2505.15248", "pdf": "https://arxiv.org/pdf/2505.15248", "abs": "https://arxiv.org/abs/2505.15248", "authors": ["Andre Dourson", "Kylie Taylor", "Xiaoli Qiao", "Michael Fitzke"], "title": "VET-DINO: Learning Anatomical Understanding Through Multi-View Distillation in Veterinary Imaging", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Self-supervised learning has emerged as a powerful paradigm for training deep\nneural networks, particularly in medical imaging where labeled data is scarce.\nWhile current approaches typically rely on synthetic augmentations of single\nimages, we propose VET-DINO, a framework that leverages a unique characteristic\nof medical imaging: the availability of multiple standardized views from the\nsame study. Using a series of clinical veterinary radiographs from the same\npatient study, we enable models to learn view-invariant anatomical structures\nand develop an implied 3D understanding from 2D projections. We demonstrate our\napproach on a dataset of 5 million veterinary radiographs from 668,000 canine\nstudies. Through extensive experimentation, including view synthesis and\ndownstream task performance, we show that learning from real multi-view pairs\nleads to superior anatomical understanding compared to purely synthetic\naugmentations. VET-DINO achieves state-of-the-art performance on various\nveterinary imaging tasks. Our work establishes a new paradigm for\nself-supervised learning in medical imaging that leverages domain-specific\nproperties rather than merely adapting natural image techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVET-DINO\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u533b\u5b66\u5f71\u50cf\u4e2d\u540c\u4e00\u7814\u7a76\u7684\u591a\u4e2a\u6807\u51c6\u5316\u89c6\u56fe\uff0c\u5b66\u4e60\u89c6\u56fe\u4e0d\u53d8\u7684\u89e3\u5256\u7ed3\u6784\u5e76\u5efa\u7acb\u4ece2D\u6295\u5f71\u52303D\u7406\u89e3\u7684\u9690\u542b\u8ba4\u77e5\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u5408\u6210\u589e\u5f3a\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u533b\u5b66\u5f71\u50cf\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u5f53\u524d\u81ea\u76d1\u7763\u5b66\u4e60\u591a\u4f9d\u8d56\u5355\u5e45\u56fe\u50cf\u7684\u5408\u6210\u589e\u5f3a\uff0c\u800c\u533b\u5b66\u5f71\u50cf\u5177\u6709\u540c\u4e00\u7814\u7a76\u7684\u591a\u4e2a\u6807\u51c6\u5316\u89c6\u56fe\u8fd9\u4e00\u72ec\u6709\u7279\u6027\uff0c\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u63d0\u51faVET-DINO\u6846\u67b6\uff0c\u5229\u7528\u540c\u4e00\u72ac\u7c7b\u7814\u7a76\u4e2d\u7684\u4e34\u5e8a\u517d\u533bX\u5149\u7247\u7684\u591a\u89c6\u56fe\u6570\u636e\uff0c\u901a\u8fc7\u771f\u5b9e\u591a\u89c6\u56fe\u5bf9\u5b66\u4e60\u89c6\u56fe\u4e0d\u53d8\u7279\u5f81\u548c3D\u7406\u89e3\u3002", "result": "\u5728500\u4e07\u5f20\u72ac\u7c7bX\u5149\u7247\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVET-DINO\u5728\u591a\u89c6\u56fe\u5408\u6210\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5408\u6210\u589e\u5f3a\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "VET-DINO\u4e3a\u533b\u5b66\u5f71\u50cf\u81ea\u76d1\u7763\u5b66\u4e60\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u5f3a\u8c03\u5229\u7528\u9886\u57df\u7279\u5f02\u6027\u7279\u5f81\u800c\u975e\u7b80\u5355\u8fc1\u79fb\u81ea\u7136\u56fe\u50cf\u6280\u672f\u3002", "keywords": "\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u533b\u5b66\u5f71\u50cf\u3001\u591a\u89c6\u56fe\u5b66\u4e60\u3001VET-DINO"}}
{"id": "2505.15252", "pdf": "https://arxiv.org/pdf/2505.15252", "abs": "https://arxiv.org/abs/2505.15252", "authors": ["Zhengyi Li", "Yue Guan", "Kang Yang", "Yu Feng", "Ning Liu", "Yu Yu", "Jingwen Leng", "Minyi Guo"], "title": "An Efficient Private GPT Never Autoregressively Decodes", "categories": ["cs.CR", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "The wide deployment of the generative pre-trained transformer (GPT) has\nraised privacy concerns for both clients and servers. While cryptographic\nprimitives can be employed for secure GPT inference to protect the privacy of\nboth parties, they introduce considerable performance overhead.To accelerate\nsecure inference, this study proposes a public decoding and secure verification\napproach that utilizes public GPT models, motivated by the observation that\nsecurely decoding one and multiple tokens takes a similar latency. The client\nuses the public model to generate a set of tokens, which are then securely\nverified by the private model for acceptance. The efficiency of our approach\ndepends on the acceptance ratio of tokens proposed by the public model, which\nwe improve from two aspects: (1) a private sampling protocol optimized for\ncryptographic primitives and (2) model alignment using knowledge distillation.\nOur approach improves the efficiency of secure decoding while maintaining the\nsame level of privacy and generation quality as standard secure decoding.\nExperiments demonstrate a $2.1\\times \\sim 6.0\\times$ speedup compared to\nstandard decoding across three pairs of public-private models and different\nnetwork conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u516c\u5f00\u89e3\u7801\u548c\u5b89\u5168\u9a8c\u8bc1\u7684\u65b9\u6cd5\uff0c\u4ee5\u52a0\u901fGPT\u7684\u5b89\u5168\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u968f\u7740GPT\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9690\u79c1\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u4f20\u7edf\u7684\u52a0\u5bc6\u65b9\u6cd5\u867d\u80fd\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u6027\u80fd\u5f00\u9500\u5927\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u4f7f\u7528\u516c\u5f00GPT\u6a21\u578b\u751f\u6210\u5019\u9009\u4ee4\u724c\uff0c\u5e76\u901a\u8fc7\u79c1\u6709\u6a21\u578b\u5b89\u5168\u9a8c\u8bc1\u30022. \u4f18\u5316\u79c1\u6709\u91c7\u6837\u534f\u8bae\u548c\u6a21\u578b\u5bf9\u9f50\uff08\u77e5\u8bc6\u84b8\u998f\uff09\uff0c\u4ee5\u63d0\u9ad8\u4ee4\u724c\u63a5\u53d7\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u79cd\u516c\u5f00-\u79c1\u6709\u6a21\u578b\u7ec4\u5408\u548c\u4e0d\u540c\u7f51\u7edc\u6761\u4ef6\u4e0b\uff0c\u5b9e\u73b0\u4e862.1\u500d\u52306.0\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u4fdd\u6301\u9690\u79c1\u548c\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u63a8\u7406\u7684\u6548\u7387\u3002", "keywords": "GPT, \u5b89\u5168\u63a8\u7406, \u9690\u79c1\u4fdd\u62a4, \u77e5\u8bc6\u84b8\u998f, \u52a0\u5bc6\u4f18\u5316"}}
{"id": "2505.15779", "pdf": "https://arxiv.org/pdf/2505.15779", "abs": "https://arxiv.org/abs/2505.15779", "authors": ["Chuanhao Li", "Jianwen Sun", "Yukang Feng", "Mingliang Zhai", "Yifan Chang", "Kaipeng Zhang"], "title": "IA-T2I: Internet-Augmented Text-to-Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 7 figures, a framework that integrates reference images\n  from the Internet into T2I/TI2I models", "summary": "Current text-to-image (T2I) generation models achieve promising results, but\nthey fail on the scenarios where the knowledge implied in the text prompt is\nuncertain. For example, a T2I model released in February would struggle to\ngenerate a suitable poster for a movie premiering in April, because the\ncharacter designs and styles are uncertain to the model. To solve this problem,\nwe propose an Internet-Augmented text-to-image generation (IA-T2I) framework to\ncompel T2I models clear about such uncertain knowledge by providing them with\nreference images. Specifically, an active retrieval module is designed to\ndetermine whether a reference image is needed based on the given text prompt; a\nhierarchical image selection module is introduced to find the most suitable\nimage returned by an image search engine to enhance the T2I model; a\nself-reflection mechanism is presented to continuously evaluate and refine the\ngenerated image to ensure faithful alignment with the text prompt. To evaluate\nthe proposed framework's performance, we collect a dataset named Img-Ref-T2I,\nwhere text prompts include three types of uncertain knowledge: (1) known but\nrare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt\nto guide GPT-4o in making preference evaluation, which has been shown to have\nan evaluation accuracy similar to that of human preference evaluation.\nExperimental results demonstrate the effectiveness of our framework,\noutperforming GPT-4o by about 30% in human evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e92\u8054\u7f51\u589e\u5f3a\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff08IA-T2I\uff09\uff0c\u901a\u8fc7\u68c0\u7d22\u548c\u7b5b\u9009\u53c2\u8003\u56fe\u50cf\u89e3\u51b3T2I\u6a21\u578b\u5bf9\u4e0d\u786e\u5b9a\u77e5\u8bc6\u7684\u751f\u6210\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6548\u679c\u3002", "motivation": "\u73b0\u6709T2I\u6a21\u578b\u5728\u6587\u672c\u63d0\u793a\u9690\u542b\u4e0d\u786e\u5b9a\u77e5\u8bc6\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4f8b\u5982\u65e0\u6cd5\u751f\u6210\u672a\u6765\u4e8b\u4ef6\u7684\u76f8\u5173\u56fe\u50cf\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u53c2\u8003\u56fe\u50cf\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e3b\u52a8\u68c0\u7d22\u6a21\u5757\u5224\u65ad\u662f\u5426\u9700\u8981\u53c2\u8003\u56fe\u50cf\uff0c\u5f15\u5165\u5206\u5c42\u56fe\u50cf\u9009\u62e9\u6a21\u5757\u7b5b\u9009\u6700\u5408\u9002\u7684\u53c2\u8003\u56fe\u50cf\uff0c\u5e76\u63d0\u51fa\u81ea\u53cd\u601d\u673a\u5236\u6301\u7eed\u4f18\u5316\u751f\u6210\u7ed3\u679c\u3002", "result": "\u5728\u5305\u542b\u4e09\u7c7b\u4e0d\u786e\u5b9a\u77e5\u8bc6\u7684Img-Ref-T2I\u6570\u636e\u96c6\u4e0a\uff0cIA-T2I\u6846\u67b6\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u6bd4GPT-4o\u8868\u73b0\u4f1830%\u3002", "conclusion": "IA-T2I\u901a\u8fc7\u7ed3\u5408\u53c2\u8003\u56fe\u50cf\u548c\u81ea\u53cd\u601d\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86T2I\u6a21\u578b\u5bf9\u4e0d\u786e\u5b9a\u77e5\u8bc6\u7684\u751f\u6210\u80fd\u529b\u3002", "keywords": "\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3001\u4e0d\u786e\u5b9a\u77e5\u8bc6\u3001\u53c2\u8003\u56fe\u50cf\u3001\u81ea\u53cd\u601d\u673a\u5236"}}
{"id": "2505.15263", "pdf": "https://arxiv.org/pdf/2505.15263", "abs": "https://arxiv.org/abs/2505.15263", "authors": ["Om Khangaonkar", "Hamed Pirsiavash"], "title": "gen2seg: Generative Models Enable Generalizable Instance Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Website: https://reachomk.github.io/gen2seg/", "summary": "By pretraining to synthesize coherent images from perturbed inputs,\ngenerative models inherently learn to understand object boundaries and scene\ncompositions. How can we repurpose these generative representations for\ngeneral-purpose perceptual organization? We finetune Stable Diffusion and MAE\n(encoder+decoder) for category-agnostic instance segmentation using our\ninstance coloring loss exclusively on a narrow set of object types (indoor\nfurnishings and cars). Surprisingly, our models exhibit strong zero-shot\ngeneralization, accurately segmenting objects of types and styles unseen in\nfinetuning (and in many cases, MAE's ImageNet-1K pretraining too). Our\nbest-performing models closely approach the heavily supervised SAM when\nevaluated on unseen object types and styles, and outperform it when segmenting\nfine structures and ambiguous boundaries. In contrast, existing promptable\nsegmentation architectures or discriminatively pretrained models fail to\ngeneralize. This suggests that generative models learn an inherent grouping\nmechanism that transfers across categories and domains, even without\ninternet-scale pretraining. Code, pretrained models, and demos are available on\nour website.", "AI": {"tldr": "\u901a\u8fc7\u5fae\u8c03Stable Diffusion\u548cMAE,\u5229\u7528\u751f\u6210\u6a21\u578b\u7684\u8868\u5f81\u8fdb\u884c\u96f6\u6837\u672c\u5b9e\u4f8b\u5206\u5272,\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u5bf9\u8c61\u7c7b\u578b\u548c\u98ce\u683c\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5c06\u751f\u6210\u6a21\u578b\u7684\u7279\u5f81\u91cd\u7528\u4e8e\u901a\u7528\u7684\u611f\u77e5\u7ec4\u7ec7\u4efb\u52a1,\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u60c5\u51b5\u4e0b\u3002", "method": "\u4f7f\u7528\u5b9e\u4f8b\u7740\u8272\u635f\u5931\u5bf9Stable Diffusion\u548cMAE\u8fdb\u884c\u5fae\u8c03,\u4e13\u6ce8\u4e8e\u5c11\u91cf\u5bf9\u8c61\u7c7b\u578b(\u5982\u5bb6\u5177\u548c\u6c7d\u8f66)\u3002", "result": "\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u5bf9\u8c61\u7c7b\u578b\u548c\u98ce\u683c\u4e0a\u7684\u5206\u5272\u6027\u80fd\u63a5\u8fd1SAM,\u4e14\u5728\u590d\u6742\u7ed3\u6784\u548c\u6a21\u7cca\u8fb9\u754c\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u751f\u6210\u6a21\u578b\u5b66\u4e60\u5230\u4e86\u8de8\u7c7b\u522b\u548c\u9886\u57df\u7684\u56fa\u6709\u5206\u7ec4\u673a\u5236,\u65e0\u9700\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e5f\u53ef\u6cdb\u5316\u3002", "keywords": "\u751f\u6210\u6a21\u578b,\u5b9e\u4f8b\u5206\u5272,\u96f6\u6837\u672c\u5b66\u4e60,Stable Diffusion,MAE"}}
{"id": "2505.15790", "pdf": "https://arxiv.org/pdf/2505.15790", "abs": "https://arxiv.org/abs/2505.15790", "authors": ["Minjung Park", "Jodi Forlizzi", "John Zimmerman"], "title": "Exploring the Innovation Opportunities for Pre-trained Models", "categories": ["cs.HC", "cs.AI"], "comment": "33 pages, 20 figures, 4 tables, DIS", "summary": "Innovators transform the world by understanding where services are\nsuccessfully meeting customers' needs and then using this knowledge to identify\nfailsafe opportunities for innovation. Pre-trained models have changed the AI\ninnovation landscape, making it faster and easier to create new AI products and\nservices. Understanding where pre-trained models are successful is critical for\nsupporting AI innovation. Unfortunately, the hype cycle surrounding pre-trained\nmodels makes it hard to know where AI can really be successful. To address\nthis, we investigated pre-trained model applications developed by HCI\nresearchers as a proxy for commercially successful applications. The research\napplications demonstrate technical capabilities, address real user needs, and\navoid ethical challenges. Using an artifact analysis approach, we categorized\ncapabilities, opportunity domains, data types, and emerging interaction design\npatterns, uncovering some of the opportunity space for innovation with\npre-trained models.", "AI": {"tldr": "\u6458\u8981\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u5982\u4f55\u63a8\u52a8AI\u521b\u65b0\uff0c\u5e76\u901a\u8fc7\u5206\u6790HCI\u7814\u7a76\u8005\u7684\u5e94\u7528\u5b9e\u4f8b\uff0c\u63ed\u793a\u4e86\u5176\u5728\u6280\u672f\u80fd\u529b\u3001\u7528\u6237\u9700\u6c42\u548c\u4f26\u7406\u5408\u89c4\u65b9\u9762\u7684\u6210\u529f\u6848\u4f8b\u3002", "motivation": "\u9884\u8bad\u7ec3\u6a21\u578b\u4e3aAI\u521b\u65b0\u63d0\u4f9b\u4e86\u4fbf\u5229\uff0c\u4f46\u5176\u5b9e\u9645\u6210\u529f\u9886\u57df\u56e0\u7092\u4f5c\u800c\u96be\u4ee5\u8fa8\u8bc6\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7HCI\u5e94\u7528\u6848\u4f8b\u63ed\u793a\u5176\u771f\u5b9e\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u4eba\u5de5\u5236\u54c1\u5206\u6790\u65b9\u6cd5\uff0c\u5bf9HCI\u7814\u7a76\u8005\u5f00\u53d1\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5e94\u7528\u8fdb\u884c\u5206\u7c7b\uff0c\u6db5\u76d6\u80fd\u529b\u3001\u9886\u57df\u3001\u6570\u636e\u7c7b\u578b\u548c\u4ea4\u4e92\u8bbe\u8ba1\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u6280\u672f\u5b9e\u73b0\u3001\u7528\u6237\u9700\u6c42\u6ee1\u8db3\u548c\u4f26\u7406\u5408\u89c4\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u521b\u65b0\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u65b9\u5411\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790HCI\u5e94\u7528\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u521b\u65b0\u673a\u4f1a\u7a7a\u95f4\uff0c\u4e3aAI\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002", "keywords": "\u9884\u8bad\u7ec3\u6a21\u578b\u3001AI\u521b\u65b0\u3001HCI\u3001\u4eba\u5de5\u5236\u54c1\u5206\u6790\u3001\u4ea4\u4e92\u8bbe\u8ba1"}}
{"id": "2505.15342", "pdf": "https://arxiv.org/pdf/2505.15342", "abs": "https://arxiv.org/abs/2505.15342", "authors": ["Kaito Ariu", "Po-An Wang", "Alexandre Proutiere", "Kenshi Abe"], "title": "Policy Testing in Markov Decision Processes", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "We study the policy testing problem in discounted Markov decision processes\n(MDPs) under the fixed-confidence setting. The goal is to determine whether the\nvalue of a given policy exceeds a specified threshold while minimizing the\nnumber of observations. We begin by deriving an instance-specific lower bound\nthat any algorithm must satisfy. This lower bound is characterized as the\nsolution to an optimization problem with non-convex constraints. We propose a\npolicy testing algorithm inspired by this optimization problem--a common\napproach in pure exploration problems such as best-arm identification, where\nasymptotically optimal algorithms often stem from such optimization-based\ncharacterizations. As for other pure exploration tasks in MDPs, however, the\nnon-convex constraints in the lower-bound problem present significant\nchallenges, raising doubts about whether statistically optimal and\ncomputationally tractable algorithms can be designed. To address this, we\nreformulate the lower-bound problem by interchanging the roles of the objective\nand the constraints, yielding an alternative problem with a non-convex\nobjective but convex constraints. Strikingly, this reformulated problem admits\nan interpretation as a policy optimization task in a newly constructed reversed\nMDP. Leveraging recent advances in policy gradient methods, we efficiently\nsolve this problem and use it to design a policy testing algorithm that is\nstatistically optimal--matching the instance-specific lower bound on sample\ncomplexity--while remaining computationally tractable. We validate our approach\nwith numerical experiments.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6298\u6263\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u4e2d\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u4e0b\u7684\u7b56\u7565\u6d4b\u8bd5\u95ee\u9898\uff0c\u76ee\u6807\u662f\u5224\u65ad\u7ed9\u5b9a\u7b56\u7565\u7684\u503c\u662f\u5426\u8d85\u8fc7\u9608\u503c\uff0c\u5e76\u6700\u5c0f\u5316\u89c2\u6d4b\u6b21\u6570\u3002\u63d0\u51fa\u4e86\u57fa\u4e8e\u4f18\u5316\u95ee\u9898\u7684\u7edf\u8ba1\u6700\u4f18\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3MDP\u4e2d\u7b56\u7565\u6d4b\u8bd5\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u7edf\u8ba1\u6700\u4f18\u6027\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u6784\u9020\u4f18\u5316\u95ee\u9898\uff0c\u5c06\u975e\u51f8\u7ea6\u675f\u8f6c\u5316\u4e3a\u51f8\u7ea6\u675f\uff0c\u5e76\u5229\u7528\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u9ad8\u6548\u89e3\u51b3\uff0c\u8bbe\u8ba1\u7edf\u8ba1\u6700\u4f18\u4e14\u8ba1\u7b97\u53ef\u884c\u7684\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u5339\u914d\u5b9e\u4f8b\u7279\u5b9a\u7684\u4e0b\u754c\uff0c\u9a8c\u8bc1\u4e86\u5176\u7edf\u8ba1\u6700\u4f18\u6027\u548c\u8ba1\u7b97\u53ef\u884c\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u95ee\u9898\u7684\u91cd\u6784\u548c\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86MDP\u4e2d\u7b56\u7565\u6d4b\u8bd5\u95ee\u9898\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u7edf\u8ba1\u6700\u4f18\u4e0e\u8ba1\u7b97\u9ad8\u6548\u7684\u5e73\u8861\u3002", "keywords": "\u6298\u6263MDP,\u7b56\u7565\u6d4b\u8bd5,\u56fa\u5b9a\u7f6e\u4fe1\u5ea6,\u7edf\u8ba1\u6700\u4f18,\u8ba1\u7b97\u590d\u6742\u5ea6"}}
{"id": "2505.15381", "pdf": "https://arxiv.org/pdf/2505.15381", "abs": "https://arxiv.org/abs/2505.15381", "authors": ["Seitaro Yoneda", "Akira Furui"], "title": "Inter-Subject Variance Transfer Learning for EMG Pattern Classification Based on Bayesian Inference", "categories": ["eess.SP", "cs.LG"], "comment": "5 pages, 3 figures, 3 tables, accepted at EMBC2024", "summary": "In electromyogram (EMG)-based motion recognition, a subject-specific\nclassifier is typically trained with sufficient labeled data. However, this\nprocess demands extensive data collection over extended periods, burdening the\nsubject. To address this, utilizing information from pre-training on multiple\nsubjects for the training of the target subject could be beneficial. This paper\nproposes an inter-subject variance transfer learning method based on a Bayesian\napproach. This method is founded on the simple hypothesis that while the means\nof EMG features vary greatly across subjects, their variances may exhibit\nsimilar patterns. Our approach transfers variance information, acquired through\npre-training on multiple source subjects, to a target subject within a Bayesian\nupdating framework, thereby allowing accurate classification using limited\ntarget calibration data. A coefficient was also introduced to adjust the amount\nof information transferred for efficient transfer learning. Experimental\nevaluations using two EMG datasets demonstrated the effectiveness of our\nvariance transfer strategy and its superiority compared to existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u65b9\u6cd5\u7684\u8de8\u4e3b\u4f53\u65b9\u5dee\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u51cf\u5c11EMG\u8fd0\u52a8\u8bc6\u522b\u4e2d\u7684\u6821\u51c6\u6570\u636e\u9700\u6c42\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfEMG\u8fd0\u52a8\u8bc6\u522b\u9700\u8981\u5927\u91cf\u4e2a\u4f53\u7279\u5b9a\u6570\u636e\uff0c\u6536\u96c6\u8fc7\u7a0b\u8017\u65f6\u4e14\u7e41\u7410\uff0c\u5229\u7528\u591a\u4e3b\u4f53\u9884\u8bad\u7ec3\u4fe1\u606f\u53ef\u51cf\u8f7b\u8d1f\u62c5\u3002", "method": "\u57fa\u4e8e\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u5047\u8bbeEMG\u7279\u5f81\u5747\u503c\u8de8\u4e3b\u4f53\u5dee\u5f02\u5927\u4f46\u65b9\u5dee\u76f8\u4f3c\uff0c\u8fc1\u79fb\u65b9\u5dee\u4fe1\u606f\u5e76\u7ed3\u5408\u76ee\u6807\u6570\u636e\u6821\u51c6\uff0c\u5f15\u5165\u7cfb\u6570\u8c03\u6574\u4fe1\u606f\u8fc1\u79fb\u91cf\u3002", "result": "\u4f7f\u7528\u4e24\u4e2aEMG\u6570\u636e\u96c6\u9a8c\u8bc1\uff0c\u65b9\u5dee\u8fc1\u79fb\u7b56\u7565\u6709\u6548\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u80fd\u663e\u8457\u51cf\u5c11\u76ee\u6807\u4e3b\u4f53\u7684\u6821\u51c6\u6570\u636e\u9700\u6c42\uff0c\u9002\u7528\u4e8eEMG\u8fd0\u52a8\u8bc6\u522b\u4efb\u52a1\u3002", "keywords": "EMG, \u8fd0\u52a8\u8bc6\u522b, \u8fc1\u79fb\u5b66\u4e60, \u8d1d\u53f6\u65af\u65b9\u6cd5, \u65b9\u5dee\u8fc1\u79fb"}}
{"id": "2505.15417", "pdf": "https://arxiv.org/pdf/2505.15417", "abs": "https://arxiv.org/abs/2505.15417", "authors": ["Leon Chlon", "Maggie Chlon", "MarcAntonio M. Awada"], "title": "Robust Multimodal Learning via Entropy-Gated Contrastive Fusion", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Real-world multimodal systems routinely face missing-input scenarios, and in\nreality, robots lose audio in a factory or a clinical record omits lab tests at\ninference time. Standard fusion layers either preserve robustness or\ncalibration but never both. We introduce Adaptive Entropy-Gated Contrastive\nFusion (AECF), a single light-weight layer that (i) adapts its entropy\ncoefficient per instance, (ii) enforces monotone calibration across all\nmodality subsets, and (iii) drives a curriculum mask directly from\ntraining-time entropy. On AV-MNIST and MS-COCO, AECF improves masked-input mAP\nby +18 pp at a 50% drop rate while reducing ECE by up to 200%, yet adds 1%\nrun-time. All back-bones remain frozen, making AECF an easy drop-in layer for\nrobust, calibrated multimodal inference.", "AI": {"tldr": "AECF\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u81ea\u9002\u5e94\u878d\u5408\u5c42\uff0c\u80fd\u5728\u7f3a\u5931\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u540c\u65f6\u4fdd\u6301\u9c81\u68d2\u6027\u548c\u6821\u51c6\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u7cfb\u7edf\u5728\u8f93\u5165\u7f3a\u5931\u65f6\u65e0\u6cd5\u540c\u65f6\u4fdd\u6301\u9c81\u68d2\u6027\u548c\u6821\u51c6\u6027\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faAECF\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u71b5\u7cfb\u6570\u3001\u5355\u8c03\u6821\u51c6\u548c\u8bfe\u7a0b\u63a9\u7801\u5b9e\u73b0\u3002", "result": "\u5728AV-MNIST\u548cMS-COCO\u6570\u636e\u96c6\u4e0a\uff0cAECF\u572850%\u7f3a\u5931\u7387\u4e0b\u63d0\u5347mAP 18%\uff0c\u540c\u65f6\u964d\u4f4eECE 200%\uff0c\u8fd0\u884c\u65f6\u95f4\u4ec5\u589e\u52a01%\u3002", "conclusion": "AECF\u662f\u4e00\u79cd\u7b80\u5355\u6613\u7528\u7684\u878d\u5408\u5c42\uff0c\u9002\u7528\u4e8e\u9c81\u68d2\u4e14\u6821\u51c6\u7684\u591a\u6a21\u6001\u63a8\u7406\u3002", "keywords": "\u591a\u6a21\u6001\u7cfb\u7edf, \u9c81\u68d2\u6027, \u6821\u51c6\u6027, AECF, \u7f3a\u5931\u8f93\u5165"}}
{"id": "2505.15437", "pdf": "https://arxiv.org/pdf/2505.15437", "abs": "https://arxiv.org/abs/2505.15437", "authors": ["Nikita Kotelevskii", "Mohsen Guizani", "Eric Moulines", "Maxim Panov"], "title": "Adaptive Temperature Scaling with Conformal Prediction", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Conformal prediction enables the construction of high-coverage prediction\nsets for any pre-trained model, guaranteeing that the true label lies within\nthe set with a specified probability. However, these sets do not provide\nprobability estimates for individual labels, limiting their practical use. In\nthis paper, we propose, to the best of our knowledge, the first method for\nassigning calibrated probabilities to elements of a conformal prediction set.\nOur approach frames this as an adaptive calibration problem, selecting an\ninput-specific temperature parameter to match the desired coverage level.\nExperiments on several challenging image classification datasets demonstrate\nthat our method maintains coverage guarantees while significantly reducing\nexpected calibration error.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3a\u5171\u5f62\u9884\u6d4b\u96c6\u5206\u914d\u6821\u51c6\u6982\u7387\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6821\u51c6\u95ee\u9898\u89e3\u51b3\u4e86\u4f20\u7edf\u5171\u5f62\u9884\u6d4b\u96c6\u65e0\u6cd5\u63d0\u4f9b\u5355\u4e2a\u6807\u7b7e\u6982\u7387\u4f30\u8ba1\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u5171\u5f62\u9884\u6d4b\u96c6\u867d\u7136\u4fdd\u8bc1\u4e86\u9ad8\u8986\u76d6\u7387\uff0c\u4f46\u65e0\u6cd5\u63d0\u4f9b\u5355\u4e2a\u6807\u7b7e\u7684\u6982\u7387\u4f30\u8ba1\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u4e3a\u5171\u5f62\u9884\u6d4b\u96c6\u4e2d\u7684\u5143\u7d20\u5206\u914d\u6821\u51c6\u6982\u7387\u3002", "method": "\u4f5c\u8005\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u81ea\u9002\u5e94\u6821\u51c6\u95ee\u9898\uff0c\u901a\u8fc7\u9009\u62e9\u8f93\u5165\u7279\u5b9a\u7684\u6e29\u5ea6\u53c2\u6570\u6765\u5339\u914d\u6240\u9700\u7684\u8986\u76d6\u7387\u6c34\u5e73\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8986\u76d6\u7387\u4fdd\u8bc1\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9884\u671f\u6821\u51c6\u8bef\u5dee\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u5171\u5f62\u9884\u6d4b\u96c6\u7684\u6982\u7387\u6821\u51c6\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u5de5\u5177\u3002", "keywords": "\u5171\u5f62\u9884\u6d4b, \u6982\u7387\u6821\u51c6, \u81ea\u9002\u5e94\u6821\u51c6, \u56fe\u50cf\u5206\u7c7b"}}
{"id": "2505.15462", "pdf": "https://arxiv.org/pdf/2505.15462", "abs": "https://arxiv.org/abs/2505.15462", "authors": ["Michal Kucha\u0159", "Jarom\u00edr Fi\u0161er", "Cyril Oswald", "Tom\u00e1\u0161 Vyhl\u00eddal"], "title": "AI-based Decision Support System for Heritage Aircraft Corrosion Prevention", "categories": ["eess.SY", "cs.LG", "cs.SY", "62C25", "H.4.2; I.6.5"], "comment": "6 pages, 4 figures, 4 tables, submitted January 31, 2025, to Process\n  Control 2025", "summary": "The paper presents a decision support system for the long-term preservation\nof aeronautical heritage exhibited/stored in sheltered sites. The aeronautical\nheritage is characterized by diverse materials of which this heritage is\nconstituted. Heritage aircraft are made of ancient aluminum alloys, (ply)wood,\nand particularly fabrics. The decision support system (DSS) designed, starting\nfrom a conceptual model, is knowledge-based on degradation/corrosion mechanisms\nof prevailing materials of aeronautical heritage. In the case of historical\naircraft wooden parts, this knowledge base is filled in by the damage function\nmodels developed within former European projects. Model-based corrosion\nprediction is implemented within the new DSS for ancient aluminum alloys. The\nnovelty of this DSS consists of supporting multi-material heritage protection\nand tailoring to peculiarities of aircraft exhibition/storage hangars and the\nneeds of aviation museums. The novel DSS is tested on WWII aircraft heritage\nexhibited in the Aviation Museum Kbely, Military History Institute Prague,\nCzech Republic.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u7528\u4e8e\u822a\u7a7a\u9057\u4ea7\u957f\u671f\u4fdd\u5b58\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u4e13\u6ce8\u4e8e\u591a\u6750\u6599\u9057\u4ea7\u4fdd\u62a4\u3002", "motivation": "\u822a\u7a7a\u9057\u4ea7\u7531\u591a\u79cd\u6750\u6599\u6784\u6210\uff0c\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u6750\u6599\u7684\u964d\u89e3\u673a\u5236\u8bbe\u8ba1\u4fdd\u62a4\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8e\u77e5\u8bc6\u5e93\u548c\u8150\u8680\u673a\u5236\u6a21\u578b\u5f00\u53d1\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u5305\u62ec\u5386\u53f2\u6728\u6750\u548c\u53e4\u8001\u94dd\u5408\u91d1\u7684\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u7cfb\u7edf\u5728\u6377\u514b\u822a\u7a7a\u535a\u7269\u9986\u7684\u4e8c\u6218\u98de\u673a\u9057\u4ea7\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u652f\u6301\u591a\u6750\u6599\u9057\u4ea7\u4fdd\u62a4\uff0c\u5e76\u9002\u5e94\u822a\u7a7a\u535a\u7269\u9986\u7684\u9700\u6c42\u3002", "keywords": "\u822a\u7a7a\u9057\u4ea7, \u51b3\u7b56\u652f\u6301\u7cfb\u7edf, \u591a\u6750\u6599\u4fdd\u62a4, \u8150\u8680\u673a\u5236"}}
{"id": "2505.15488", "pdf": "https://arxiv.org/pdf/2505.15488", "abs": "https://arxiv.org/abs/2505.15488", "authors": ["Shubhrangshu Debsarkar", "Bijoy Kundu"], "title": "Machine Learning Derived Blood Input for Dynamic PET Images of Rat Heart", "categories": ["eess.IV", "cs.LG"], "comment": null, "summary": "Dynamic FDG PET imaging study of n = 52 rats including 26 control\nWistar-Kyoto (WKY) rats and 26 experimental spontaneously hypertensive rats\n(SHR) were performed using a Siemens microPET and Albira trimodal scanner\nlongitudinally at 1, 2, 3, 5, 9, 12 and 18 months of age. A 15-parameter dual\noutput model correcting for spill over contamination and partial volume effects\nwith peak fitting cost functions was developed for simultaneous estimation of\nmodel corrected blood input function (MCIF) and kinetic rate constants for\ndynamic FDG PET images of rat heart in vivo. Major drawbacks of this model are\nits dependence on manual annotations for the Image Derived Input Function\n(IDIF) and manual determination of crucial model parameters to compute MCIF. To\novercome these limitations, we performed semi-automated segmentation and then\nformulated a Long-Short-Term Memory (LSTM) cell network to train and predict\nMCIF in test data using a concatenation of IDIFs and myocardial inputs and\ncompared them with reference-modeled MCIF. Thresholding along 2D plane slices\nwith two thresholds, with T1 representing high-intensity myocardium, and T2\nrepresenting lower-intensity rings, was used to segment the area of the LV\nblood pool. The resultant IDIF and myocardial TACs were used to compute the\ncorresponding reference (model) MCIF for all data sets. The segmented IDIF and\nthe myocardium formed the input for the LSTM network. A k-fold cross validation\nstructure with a 33:8:11 split and 5 folds was utilized to create the model and\nevaluate the performance of the LSTM network for all datasets. To overcome the\nsparseness of data as time steps increase, midpoint interpolation was utilized\nto increase the density of datapoints beyond time = 10 minutes. The model\nutilizing midpoint interpolation was able to achieve a 56.4% improvement over\nprevious Mean Squared Error (MSE).", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u52a8\u6001FDG PET\u6210\u50cf\u6280\u672f\u5bf952\u53ea\u5927\u9f20\uff08\u5305\u62ec26\u53ea\u5bf9\u7167WKY\u5927\u9f20\u548c26\u53ea\u5b9e\u9a8c\u6027SHR\u5927\u9f20\uff09\u8fdb\u884c\u7eb5\u5411\u5206\u6790\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd15\u53c2\u6570\u53cc\u8f93\u51fa\u6a21\u578b\u6765\u6821\u6b63\u56fe\u50cf\u8bef\u5dee\u3002\u4e3a\u89e3\u51b3\u624b\u52a8\u6807\u6ce8\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u534a\u81ea\u52a8\u5316\u5206\u5272\u548cLSTM\u7f51\u7edc\u9884\u6d4bMCIF\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u6570\u636e\u7a00\u758f\u65f6\u91c7\u7528\u4e2d\u70b9\u63d2\u503c\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u52a8\u6001FDG PET\u56fe\u50cf\u5206\u6790\u4f9d\u8d56\u624b\u52a8\u6807\u6ce8\u548c\u53c2\u6570\u786e\u5b9a\uff0c\u6548\u7387\u4f4e\u4e14\u6613\u51fa\u9519\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u534a\u81ea\u52a8\u5206\u5272\u548cLSTM\u7f51\u7edc\u9884\u6d4bMCIF\uff0c\u91c7\u7528k\u6298\u4ea4\u53c9\u9a8c\u8bc1\u548c\u4e2d\u70b9\u63d2\u503c\u6280\u672f\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u4e2d\u70b9\u63d2\u503c\uff0c\u6a21\u578b\u7684MSE\u63d0\u9ad8\u4e8656.4%\u3002", "conclusion": "\u63d0\u51fa\u7684\u534a\u81ea\u52a8\u5316\u548cLSTM\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001FDG PET\u56fe\u50cf\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "keywords": "\u52a8\u6001FDG PET, LSTM\u7f51\u7edc, \u534a\u81ea\u52a8\u5316\u5206\u5272, \u4e2d\u70b9\u63d2\u503c, \u5927\u9f20\u5fc3\u810f\u6210\u50cf"}}
{"id": "2505.15503", "pdf": "https://arxiv.org/pdf/2505.15503", "abs": "https://arxiv.org/abs/2505.15503", "authors": ["Tom Silver", "Rajat Kumar Jenamani", "Ziang Liu", "Ben Dodson", "Tapomayukh Bhattacharjee"], "title": "Coloring Between the Lines: Personalization in the Null Space of Planning Constraints", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Generalist robots must personalize in-the-wild to meet the diverse needs and\npreferences of long-term users. How can we enable flexible personalization\nwithout sacrificing safety or competency? This paper proposes Coloring Between\nthe Lines (CBTL), a method for personalization that exploits the null space of\nconstraint satisfaction problems (CSPs) used in robot planning. CBTL begins\nwith a CSP generator that ensures safe and competent behavior, then\nincrementally personalizes behavior by learning parameterized constraints from\nonline interaction. By quantifying uncertainty and leveraging the\ncompositionality of planning constraints, CBTL achieves sample-efficient\nadaptation without environment resets. We evaluate CBTL in (1) three diverse\nsimulation environments; (2) a web-based user study; and (3) a real-robot\nassisted feeding system, finding that CBTL consistently achieves more effective\npersonalization with fewer interactions than baselines. Our results demonstrate\nthat CBTL provides a unified and practical approach for continual, flexible,\nactive, and safe robot personalization. Website:\nhttps://emprise.cs.cornell.edu/cbtl/", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86CBTL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\u7684\u96f6\u7a7a\u95f4\u5b9e\u73b0\u673a\u5668\u4eba\u884c\u4e3a\u7684\u4e2a\u6027\u5316\uff0c\u517c\u987e\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4e3a\u6ee1\u8db3\u957f\u671f\u7528\u6237\u591a\u6837\u5316\u9700\u6c42\uff0c\u9700\u5728\u4fdd\u6301\u5b89\u5168\u6027\u548c\u80fd\u529b\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u7075\u6d3b\u7684\u4e2a\u6027\u5316\u673a\u5668\u4eba\u884c\u4e3a\u3002", "method": "\u5f00\u53d1\u4e86CBTL\u65b9\u6cd5\uff0c\u5229\u7528CSP\u751f\u6210\u5b89\u5168\u884c\u4e3a\uff0c\u901a\u8fc7\u5728\u7ebf\u4e92\u52a8\u5b66\u4e60\u53c2\u6570\u5316\u7ea6\u675f\uff0c\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u7684\u4e2a\u6027\u5316\u8c03\u6574\u3002", "result": "\u5728\u4e09\u79cd\u6a21\u62df\u73af\u5883\u3001\u7528\u6237\u7814\u7a76\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5582\u98df\u7cfb\u7edf\u4e2d\uff0cCBTL\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\uff0c\u4ea4\u4e92\u66f4\u5c11\u3002", "conclusion": "CBTL\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u3001\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u6301\u7eed\u3001\u7075\u6d3b\u3001\u4e3b\u52a8\u4e14\u5b89\u5168\u7684\u673a\u5668\u4eba\u4e2a\u6027\u5316\u3002", "keywords": "\u673a\u5668\u4eba\u4e2a\u6027\u5316\u3001\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\u3001\u96f6\u7a7a\u95f4\u3001\u6837\u672c\u9ad8\u6548\u3001\u5b89\u5168\u884c\u4e3a"}}
{"id": "2505.15506", "pdf": "https://arxiv.org/pdf/2505.15506", "abs": "https://arxiv.org/abs/2505.15506", "authors": ["Debarshi Brahma", "Anuska Roy", "Soma Biswas"], "title": "Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts", "categories": ["cs.CV", "cs.LG"], "comment": "Published in TMLR (2025)", "summary": "Recently, Vision-Language foundation models like CLIP and ALIGN, which are\npre-trained on large-scale data have shown remarkable zero-shot generalization\nto diverse datasets with different classes and even domains. In this work, we\ntake a step further and analyze whether these models can be adapted to target\ndatasets having very different distributions and classes compared to what these\nmodels have been trained on, using only a few labeled examples from the target\ndataset. In such scenarios, finetuning large pretrained models is challenging\ndue to problems of overfitting as well as loss of generalization, and has not\nbeen well explored in prior literature. Since, the pre-training data of such\nmodels are unavailable, it is difficult to comprehend the performance on\nvarious downstream datasets. First, we try to answer the question: Given a\ntarget dataset with a few labelled examples, can we estimate whether further\nfine-tuning can enhance the performance compared to zero-shot evaluation? by\nanalyzing the common vision-language embedding space. Based on the analysis, we\npropose a novel prompt-tuning method, PromptMargin for adapting such\nlarge-scale VLMs directly on the few target samples. PromptMargin effectively\ntunes the text as well as visual prompts for this task, and has two main\nmodules: 1) Firstly, we use a selective augmentation strategy to complement the\nfew training samples in each task; 2) Additionally, to ensure robust training\nin the presence of unfamiliar class names, we increase the inter-class margin\nfor improved class discrimination using a novel Multimodal Margin Regularizer.\nExtensive experiments and analysis across fifteen target benchmark datasets,\nwith varying degrees of distribution shifts from natural images, shows the\neffectiveness of the proposed framework over the existing state-of-the-art\napproaches applied to this setting. github.com/debarshigit/PromptMargin.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u8c03\u6574\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\u548cALIGN\uff09\u4ee5\u9002\u5e94\u76ee\u6807\u6570\u636e\u96c6\u7684\u5206\u5e03\u548c\u7c7b\u522b\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPromptMargin\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u589e\u5f3a\u7b56\u7565\u548c\u591a\u6a21\u6001\u8fb9\u7f18\u6b63\u5219\u5316\u5668\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u5728\u76ee\u6807\u6570\u636e\u96c6\u5206\u5e03\u548c\u7c7b\u522b\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u5dee\u5f02\u663e\u8457\u65f6\uff0c\u5982\u4f55\u901a\u8fc7\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u907f\u514d\u8fc7\u62df\u5408\u5e76\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faPromptMargin\u65b9\u6cd5\uff0c\u5305\u62ec\u9009\u62e9\u589e\u5f3a\u7b56\u7565\u548c\u591a\u6a21\u6001\u8fb9\u7f18\u6b63\u5219\u5316\u5668\uff0c\u8c03\u6574\u6587\u672c\u548c\u89c6\u89c9\u63d0\u793a\u4ee5\u4f18\u5316\u6a21\u578b\u3002", "result": "\u572815\u4e2a\u5177\u6709\u4e0d\u540c\u5206\u5e03\u504f\u79fb\u7684\u76ee\u6807\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86PromptMargin\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "PromptMargin\u80fd\u591f\u6709\u6548\u9002\u5e94\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u4e0b\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "keywords": "\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b, \u96f6\u6837\u672c\u5b66\u4e60, \u5c0f\u6837\u672c\u5fae\u8c03, PromptMargin, \u591a\u6a21\u6001"}}
{"id": "2505.15557", "pdf": "https://arxiv.org/pdf/2505.15557", "abs": "https://arxiv.org/abs/2505.15557", "authors": ["Anna R. Flowers", "Christopher T. Franck", "Micka\u00ebl Binois", "Chiwoo Park", "Robert B. Gramacy"], "title": "Modular Jump Gaussian Processes", "categories": ["stat.ME", "cs.LG"], "comment": "18 pages, 12 figures", "summary": "Gaussian processes (GPs) furnish accurate nonlinear predictions with\nwell-calibrated uncertainty. However, the typical GP setup has a built-in\nstationarity assumption, making it ill-suited for modeling data from processes\nwith sudden changes, or \"jumps\" in the output variable. The \"jump GP\" (JGP) was\ndeveloped for modeling data from such processes, combining local GPs and latent\n\"level\" variables under a joint inferential framework. But joint modeling can\nbe fraught with difficulty. We aim to simplify by suggesting a more modular\nsetup, eschewing joint inference but retaining the main JGP themes: (a)\nlearning optimal neighborhood sizes that locally respect manifolds of\ndiscontinuity; and (b) a new cluster-based (latent) feature to capture regions\nof distinct output levels on both sides of the manifold. We show that each of\n(a) and (b) separately leads to dramatic improvements when modeling processes\nwith jumps. In tandem (but without requiring joint inference) that benefit is\ncompounded, as illustrated on real and synthetic benchmark examples from the\nrecent literature.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u65b9\u6cd5\uff0c\u7b80\u5316\u4e86Jump GP\uff08JGP\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u72ec\u7acb\u4f18\u5316\u5c40\u90e8\u90bb\u57df\u5927\u5c0f\u548c\u5f15\u5165\u805a\u7c7b\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u8df3\u8dc3\u6570\u636e\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u5047\u8bbe\u6570\u636e\u7684\u5e73\u7a33\u6027\uff0c\u4e0d\u9002\u5408\u5904\u7406\u8df3\u8dc3\u6570\u636e\u3002JGP\u867d\u7136\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5176\u8054\u5408\u5efa\u6a21\u6846\u67b6\u590d\u6742\u4e14\u96be\u4ee5\u5b9e\u73b0\u3002\u672c\u6587\u65e8\u5728\u7b80\u5316JGP\uff0c\u4fdd\u7559\u5176\u6838\u5fc3\u4f18\u70b9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u65b9\u6cd5\uff1a\uff08a\uff09\u5b66\u4e60\u5c40\u90e8\u90bb\u57df\u5927\u5c0f\u4ee5\u9002\u5e94\u4e0d\u8fde\u7eed\u6d41\u5f62\uff1b\uff08b\uff09\u5f15\u5165\u65b0\u7684\u805a\u7c7b\u7279\u5f81\u6355\u83b7\u8df3\u8dc3\u4e24\u4fa7\u7684\u4e0d\u540c\u8f93\u51fa\u533a\u57df\u3002\u4e24\u8005\u72ec\u7acb\u4f18\u5316\uff0c\u65e0\u9700\u8054\u5408\u63a8\u65ad\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5355\u72ec\u4f18\u5316\uff08a\uff09\u6216\uff08b\uff09\u5747\u80fd\u663e\u8457\u63d0\u5347\u8df3\u8dc3\u6570\u636e\u7684\u5efa\u6a21\u6548\u679c\uff1b\u4e24\u8005\u7ed3\u5408\u65f6\u6548\u679c\u66f4\u4f73\uff0c\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "\u6a21\u5757\u5316\u7684JGP\u65b9\u6cd5\u7b80\u5316\u4e86\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5904\u7406\u8df3\u8dc3\u6570\u636e\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "keywords": "\u9ad8\u65af\u8fc7\u7a0b\uff0c\u8df3\u8dc3\u6570\u636e\uff0c\u6a21\u5757\u5316\u5efa\u6a21\uff0c\u5c40\u90e8\u90bb\u57df\uff0c\u805a\u7c7b\u7279\u5f81"}}
{"id": "2505.15576", "pdf": "https://arxiv.org/pdf/2505.15576", "abs": "https://arxiv.org/abs/2505.15576", "authors": ["Xin Huang", "Ruibin Li", "Tong Jia", "Wei Zheng", "Ya Wang"], "title": "Visual Perturbation and Adaptive Hard Negative Contrastive Learning for Compositional Reasoning in Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at the International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "Vision-Language Models (VLMs) are essential for multimodal tasks, especially\ncompositional reasoning (CR) tasks, which require distinguishing fine-grained\nsemantic differences between visual and textual embeddings. However, existing\nmethods primarily fine-tune the model by generating text-based hard negative\nsamples, neglecting the importance of image-based negative samples, which\nresults in insufficient training of the visual encoder and ultimately impacts\nthe overall performance of the model. Moreover, negative samples are typically\ntreated uniformly, without considering their difficulty levels, and the\nalignment of positive samples is insufficient, which leads to challenges in\naligning difficult sample pairs. To address these issues, we propose Adaptive\nHard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard\nnegatives into the visual domain to generate semantically disturbed image-based\nnegatives for training the model, thereby enhancing its overall performance.\nAHNPL also introduces a contrastive learning approach using a multimodal hard\nnegative loss to improve the model's discrimination of hard negatives within\neach modality and a dynamic margin loss that adjusts the contrastive margin\naccording to sample difficulty to enhance the distinction of challenging sample\npairs. Experiments on three public datasets demonstrate that our method\neffectively boosts VLMs' performance on complex CR tasks. The source code is\navailable at https://github.com/nynu-BDAI/AHNPL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAHNPL\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u89c6\u89c9\u9886\u57df\u7684\u786c\u8d1f\u6837\u672c\u548c\u6539\u8fdb\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u5408\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u786c\u8d1f\u6837\u672c\uff0c\u5ffd\u7565\u4e86\u89c6\u89c9\u8d1f\u6837\u672c\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u89c6\u89c9\u7f16\u7801\u5668\u8bad\u7ec3\u4e0d\u8db3\uff0c\u5f71\u54cd\u6a21\u578b\u6574\u4f53\u6027\u80fd\u3002\u540c\u65f6\uff0c\u8d1f\u6837\u672c\u7684\u96be\u5ea6\u672a\u88ab\u533a\u5206\uff0c\u6b63\u6837\u672c\u5bf9\u9f50\u4e5f\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86AHNPL\u65b9\u6cd5\uff0c\u5305\u62ec\u5c06\u6587\u672c\u786c\u8d1f\u6837\u672c\u8f6c\u5316\u4e3a\u89c6\u89c9\u9886\u57df\u7684\u8bed\u4e49\u6270\u52a8\u8d1f\u6837\u672c\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u786c\u8d1f\u635f\u5931\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ee5\u53ca\u52a8\u6001\u8c03\u6574\u5bf9\u6bd4\u8fb9\u754c\u7684\u52a8\u6001\u8fb9\u754c\u635f\u5931\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAHNPL\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u7ec4\u5408\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "AHNPL\u901a\u8fc7\u6539\u8fdb\u786c\u8d1f\u6837\u672c\u751f\u6210\u548c\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002", "keywords": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b, \u7ec4\u5408\u63a8\u7406, \u786c\u8d1f\u6837\u672c, \u5bf9\u6bd4\u5b66\u4e60, \u52a8\u6001\u8fb9\u754c\u635f\u5931"}}
{"id": "2505.15636", "pdf": "https://arxiv.org/pdf/2505.15636", "abs": "https://arxiv.org/abs/2505.15636", "authors": ["Yousef Al-Jazzazi", "Haya Diwan", "Jinrui Gou", "Cameron Musco", "Christopher Musco", "Torsten Suel"], "title": "Distance Adaptive Beam Search for Provably Accurate Graph-Based Nearest Neighbor Search", "categories": ["cs.IR", "cs.DB", "cs.DS", "cs.LG"], "comment": null, "summary": "Nearest neighbor search is central in machine learning, information\nretrieval, and databases. For high-dimensional datasets, graph-based methods\nsuch as HNSW, DiskANN, and NSG have become popular thanks to their empirical\naccuracy and efficiency. These methods construct a directed graph over the\ndataset and perform beam search on the graph to find nodes close to a given\nquery. While significant work has focused on practical refinements and\ntheoretical understanding of graph-based methods, many questions remain. We\npropose a new distance-based termination condition for beam search to replace\nthe commonly used condition based on beam width. We prove that, as long as the\nsearch graph is navigable, our resulting Adaptive Beam Search method is\nguaranteed to approximately solve the nearest-neighbor problem, establishing a\nconnection between navigability and the performance of graph-based search. We\nalso provide extensive experiments on our new termination condition for both\nnavigable graphs and approximately navigable graphs used in practice, such as\nHNSW and Vamana graphs. We find that Adaptive Beam Search outperforms standard\nbeam search over a range of recall values, data sets, graph constructions, and\ntarget number of nearest neighbors. It thus provides a simple and practical way\nto improve the performance of popular methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ddd\u79bb\u7684\u81ea\u9002\u5e94\u6ce2\u675f\u641c\u7d22\u7ec8\u6b62\u6761\u4ef6\uff0c\u53d6\u4ee3\u4f20\u7edf\u7684\u6ce2\u675f\u5bbd\u5ea6\u6761\u4ef6\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5bfc\u822a\u56fe\u4e0a\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u6807\u51c6\u6ce2\u675f\u641c\u7d22\u7684\u6027\u80fd\u3002", "motivation": "\u9ad8\u7ef4\u6570\u636e\u4e2d\u57fa\u4e8e\u56fe\u7684\u6700\u8fd1\u90bb\u641c\u7d22\u65b9\u6cd5\uff08\u5982HNSW\u3001DiskANN\u7b49\uff09\u867d\u7136\u5b9e\u7528\u9ad8\u6548\uff0c\u4f46\u4ecd\u6709\u8bb8\u591a\u672a\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u641c\u7d22\u7ec8\u6b62\u6761\u4ef6\u4e0a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u6ce2\u675f\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u8ddd\u79bb\u7684\u7ec8\u6b62\u6761\u4ef6\uff0c\u53d6\u4ee3\u4f20\u7edf\u7684\u6ce2\u675f\u5bbd\u5ea6\u9650\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u3001\u56fe\u7ed3\u6784\u548c\u76ee\u6807\u6700\u8fd1\u90bb\u6570\u76ee\u4e0b\u5747\u4f18\u4e8e\u6807\u51c6\u6ce2\u675f\u641c\u7d22\u3002", "conclusion": "\u81ea\u9002\u5e94\u6ce2\u675f\u641c\u7d22\u4e3a\u6d41\u884c\u7684\u56fe\u641c\u7d22\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u5b9e\u7528\u7684\u6539\u8fdb\u65b9\u6848\u3002", "keywords": "\u6700\u8fd1\u90bb\u641c\u7d22,\u56fe\u641c\u7d22,\u81ea\u9002\u5e94\u6ce2\u675f\u641c\u7d22,HNSW,Vamana"}}
{"id": "2505.15641", "pdf": "https://arxiv.org/pdf/2505.15641", "abs": "https://arxiv.org/abs/2505.15641", "authors": ["Zhengjia Zhuo", "Viswanath Nagarajan"], "title": "A Simple Approximation Algorithm for Optimal Decision Tree", "categories": ["cs.DS", "cs.LG"], "comment": null, "summary": "Optimal decision tree (\\odt) is a fundamental problem arising in applications\nsuch as active learning, entity identification, and medical diagnosis. An\ninstance of \\odt is given by $m$ hypotheses, out of which an unknown ``true''\nhypothesis is drawn according to some probability distribution. An algorithm\nneeds to identify the true hypothesis by making queries: each query incurs a\ncost and has a known response for each hypothesis. The goal is to minimize the\nexpected query cost to identify the true hypothesis. We consider the most\ngeneral setting with arbitrary costs, probabilities and responses. \\odt is\nNP-hard to approximate better than $\\ln m$ and there are $O(\\ln m)$\napproximation algorithms known for it. However, these algorithms and/or their\nanalyses are quite complex. Moreover, the leading constant factors are large.\nWe provide a simple algorithm and analysis for \\odt, proving an approximation\nratio of $8 \\ln m$.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u6700\u4f18\u51b3\u7b56\u6811\uff08ODT\uff09\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5176\u8fd1\u4f3c\u6bd4\u4e3a8 ln m\u3002", "motivation": "ODT\u95ee\u9898\u5728\u4e3b\u52a8\u5b66\u4e60\u3001\u5b9e\u4f53\u8bc6\u522b\u548c\u533b\u5b66\u8bca\u65ad\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u73b0\u6709\u7b97\u6cd5\u548c\u5206\u6790\u590d\u6742\u4e14\u5e38\u6570\u56e0\u5b50\u8f83\u5927\uff0c\u9700\u7b80\u5316\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u6210\u672c\u3001\u6982\u7387\u548c\u54cd\u5e94\u7684\u5e7f\u4e49\u8bbe\u7f6e\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e868 ln m\u7684\u8fd1\u4f3c\u6bd4\uff0c\u6bd4\u73b0\u6709\u590d\u6742\u7b97\u6cd5\u66f4\u7b80\u6d01\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u7b80\u5316\u4e86ODT\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u5728\u7406\u8bba\u4e0a\u4fdd\u6301\u4e86\u826f\u597d\u7684\u8fd1\u4f3c\u6027\u80fd\u3002", "keywords": "\u6700\u4f18\u51b3\u7b56\u6811, \u8fd1\u4f3c\u7b97\u6cd5, NP\u96be\u95ee\u9898, \u5e7f\u4e49\u8bbe\u7f6e"}}
{"id": "2505.15659", "pdf": "https://arxiv.org/pdf/2505.15659", "abs": "https://arxiv.org/abs/2505.15659", "authors": ["Ruijie Zheng", "Jing Wang", "Scott Reed", "Johan Bjorck", "Yu Fang", "Fengyuan Hu", "Joel Jang", "Kaushil Kundalia", "Zongyu Lin", "Loic Magne", "Avnish Narayan", "You Liang Tan", "Guanzhi Wang", "Qi Wang", "Jiannan Xiang", "Yinzhen Xu", "Seonghyeon Ye", "Jan Kautz", "Furong Huang", "Yuke Zhu", "Linxi Fan"], "title": "FLARE: Robot Learning with Implicit World Modeling", "categories": ["cs.RO", "cs.LG"], "comment": "Project Webpage / Blogpost:\n  https://research.nvidia.com/labs/gear/flare", "summary": "We introduce $\\textbf{F}$uture $\\textbf{LA}$tent $\\textbf{RE}$presentation\nAlignment ($\\textbf{FLARE}$), a novel framework that integrates predictive\nlatent world modeling into robot policy learning. By aligning features from a\ndiffusion transformer with latent embeddings of future observations,\n$\\textbf{FLARE}$ enables a diffusion transformer policy to anticipate latent\nrepresentations of future observations, allowing it to reason about long-term\nconsequences while generating actions. Remarkably lightweight, $\\textbf{FLARE}$\nrequires only minimal architectural modifications -- adding a few tokens to\nstandard vision-language-action (VLA) models -- yet delivers substantial\nperformance gains. Across two challenging multitask simulation imitation\nlearning benchmarks spanning single-arm and humanoid tabletop manipulation,\n$\\textbf{FLARE}$ achieves state-of-the-art performance, outperforming prior\npolicy learning baselines by up to 26%. Moreover, $\\textbf{FLARE}$ unlocks the\nability to co-train with human egocentric video demonstrations without action\nlabels, significantly boosting policy generalization to a novel object with\nunseen geometry with as few as a single robot demonstration. Our results\nestablish $\\textbf{FLARE}$ as a general and scalable approach for combining\nimplicit world modeling with high-frequency robotic control.", "AI": {"tldr": "FLARE\u662f\u4e00\u79cd\u65b0\u578b\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6269\u6563\u53d8\u6362\u5668\u4e0e\u672a\u6765\u89c2\u6d4b\u7684\u6f5c\u5728\u8868\u793a\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u957f\u671f\u5f71\u54cd\u63a8\u7406\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u63d0\u51faFLARE\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u9884\u6d4b\u6f5c\u5728\u4e16\u754c\u6a21\u578b\u6574\u5408\u5230\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u4e2d\uff0c\u89e3\u51b3\u957f\u671f\u63a8\u7406\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6269\u6563\u53d8\u6362\u5668\u7279\u5f81\u4e0e\u672a\u6765\u89c2\u6d4b\u6f5c\u5728\u5d4c\u5165\u5bf9\u9f50\uff0c\u4ec5\u9700\u5c11\u91cf\u67b6\u6784\u4fee\u6539\u5373\u53ef\u5b9e\u73b0\u3002", "result": "\u5728\u4e24\u4e2a\u591a\u4efb\u52a1\u6a21\u62df\u6a21\u4eff\u5b66\u4e60\u57fa\u51c6\u4e2d\uff0c\u6027\u80fd\u8d85\u8d8a\u57fa\u7ebf\u8fbe26%\uff0c\u5e76\u80fd\u901a\u8fc7\u4eba\u7c7b\u6f14\u793a\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FLARE\u662f\u4e00\u79cd\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u5c06\u9690\u5f0f\u4e16\u754c\u5efa\u6a21\u4e0e\u9ad8\u9891\u673a\u5668\u4eba\u63a7\u5236\u7ed3\u5408\u3002", "keywords": "FLARE, \u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60, \u6f5c\u5728\u4e16\u754c\u5efa\u6a21, \u6269\u6563\u53d8\u6362\u5668, \u6a21\u4eff\u5b66\u4e60"}}
{"id": "2505.15728", "pdf": "https://arxiv.org/pdf/2505.15728", "abs": "https://arxiv.org/abs/2505.15728", "authors": ["Luqin Gan", "Tarek M. Zikry", "Genevera I. Allen"], "title": "Are machine learning interpretations reliable? A stability study on global interpretations", "categories": ["stat.ML", "cs.LG", "stat.AP"], "comment": "17 pages main text, 5 main text figures. 57 pages in total with\n  Appendix and Bibliography", "summary": "As machine learning systems are increasingly used in high-stakes domains,\nthere is a growing emphasis placed on making them interpretable to improve\ntrust in these systems. In response, a range of interpretable machine learning\n(IML) methods have been developed to generate human-understandable insights\ninto otherwise black box models. With these methods, a fundamental question\narises: Are these interpretations reliable? Unlike with prediction accuracy or\nother evaluation metrics for supervised models, the proximity to the true\ninterpretation is difficult to define. Instead, we ask a closely related\nquestion that we argue is a prerequisite for reliability: Are these\ninterpretations stable? We define stability as findings that are consistent or\nreliable under small random perturbations to the data or algorithms. In this\nstudy, we conduct the first systematic, large-scale empirical stability study\non popular machine learning global interpretations for both supervised and\nunsupervised tasks on tabular data. Our findings reveal that popular\ninterpretation methods are frequently unstable, notably less stable than the\npredictions themselves, and that there is no association between the accuracy\nof machine learning predictions and the stability of their associated\ninterpretations. Moreover, we show that no single method consistently provides\nthe most stable interpretations across a range of benchmark datasets. Overall,\nthese results suggest that interpretability alone does not warrant trust, and\nunderscores the need for rigorous evaluation of interpretation stability in\nfuture work. To support these principles, we have developed and released an\nopen source IML dashboard and Python package to enable researchers to assess\nthe stability and reliability of their own data-driven interpretations and\ndiscoveries.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u6d41\u884c\u7684\u673a\u5668\u5b66\u4e60\u5168\u5c40\u89e3\u91ca\u65b9\u6cd5\u7684\u7a33\u5b9a\u6027\uff0c\u53d1\u73b0\u8fd9\u4e9b\u89e3\u91ca\u65b9\u6cd5\u666e\u904d\u4e0d\u7a33\u5b9a\uff0c\u4e14\u89e3\u91ca\u7a33\u5b9a\u6027\u4e0e\u9884\u6d4b\u51c6\u786e\u6027\u65e0\u5173\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e94\u7528\u589e\u52a0\uff0c\u63d0\u9ad8\u6a21\u578b\u89e3\u91ca\u7684\u53ef\u4fe1\u5ea6\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5bf9\u8868\u683c\u6570\u636e\u7684\u76d1\u7763\u548c\u65e0\u76d1\u7763\u4efb\u52a1\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f30\u591a\u79cd\u6d41\u884c\u89e3\u91ca\u65b9\u6cd5\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\u89e3\u91ca\u65b9\u6cd5\u666e\u904d\u4e0d\u7a33\u5b9a\uff0c\u4e14\u89e3\u91ca\u7a33\u5b9a\u6027\u4e0e\u9884\u6d4b\u51c6\u786e\u6027\u65e0\u5173\u8054\u3002", "conclusion": "\u4ec5\u4f9d\u9760\u89e3\u91ca\u6027\u4e0d\u8db3\u4ee5\u5efa\u7acb\u4fe1\u4efb\uff0c\u672a\u6765\u9700\u4e25\u683c\u8bc4\u4f30\u89e3\u91ca\u7a33\u5b9a\u6027\u3002", "keywords": "\u673a\u5668\u5b66\u4e60, \u53ef\u89e3\u91ca\u6027, \u7a33\u5b9a\u6027, \u5b9e\u8bc1\u7814\u7a76"}}
{"id": "2505.15791", "pdf": "https://arxiv.org/pdf/2505.15791", "abs": "https://arxiv.org/abs/2505.15791", "authors": ["Fengyuan Dai", "Zifeng Zhuang", "Yufei Huang", "Siteng Huang", "Bangyan Liao", "Donglin Wang", "Fajie Yuan"], "title": "VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL", "categories": ["cs.CV", "cs.LG"], "comment": "Under review", "summary": "Diffusion models have emerged as powerful generative tools across various\ndomains, yet tailoring pre-trained models to exhibit specific desirable\nproperties remains challenging. While reinforcement learning (RL) offers a\npromising solution,current methods struggle to simultaneously achieve stable,\nefficient fine-tuning and support non-differentiable rewards. Furthermore,\ntheir reliance on sparse rewards provides inadequate supervision during\nintermediate steps, often resulting in suboptimal generation quality. To\naddress these limitations, dense and differentiable signals are required\nthroughout the diffusion process. Hence, we propose VAlue-based Reinforced\nDiffusion (VARD): a novel approach that first learns a value function\npredicting expection of rewards from intermediate states, and subsequently uses\nthis value function with KL regularization to provide dense supervision\nthroughout the generation process. Our method maintains proximity to the\npretrained model while enabling effective and stable training via\nbackpropagation. Experimental results demonstrate that our approach facilitates\nbetter trajectory guidance, improves training efficiency and extends the\napplicability of RL to diffusion models optimized for complex,\nnon-differentiable reward functions.", "AI": {"tldr": "VARD\u662f\u4e00\u79cd\u57fa\u4e8e\u503c\u51fd\u6570\u7684\u5f3a\u5316\u6269\u6563\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u4e2d\u95f4\u72b6\u6001\u7684\u503c\u51fd\u6570\u5e76\u63d0\u4f9b\u5bc6\u96c6\u76d1\u7763\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u7a33\u5b9a\u6027\u548c\u975e\u53ef\u5fae\u5206\u5956\u52b1\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u5fae\u8c03\u65f6\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u7a33\u5b9a\u6027\u548c\u975e\u53ef\u5fae\u5206\u5956\u52b1\u652f\u6301\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u5bc6\u96c6\u3001\u53ef\u5fae\u5206\u7684\u4fe1\u53f7\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faVARD\u65b9\u6cd5\uff0c\u9996\u5148\u5b66\u4e60\u9884\u6d4b\u4e2d\u95f4\u72b6\u6001\u5956\u52b1\u671f\u671b\u7684\u503c\u51fd\u6570\uff0c\u5e76\u7ed3\u5408KL\u6b63\u5219\u5316\u63d0\u4f9b\u5bc6\u96c6\u76d1\u7763\uff0c\u786e\u4fdd\u6a21\u578b\u63a5\u8fd1\u9884\u8bad\u7ec3\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVARD\u80fd\u591f\u66f4\u597d\u5730\u6307\u5bfc\u751f\u6210\u8f68\u8ff9\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u6269\u5c55\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u975e\u53ef\u5fae\u5206\u5956\u52b1\u6269\u6563\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "VARD\u901a\u8fc7\u503c\u51fd\u6570\u548c\u5bc6\u96c6\u76d1\u7763\uff0c\u6709\u6548\u4f18\u5316\u4e86\u6269\u6563\u6a21\u578b\u7684\u5fae\u8c03\u8fc7\u7a0b\uff0c\u9002\u7528\u4e8e\u590d\u6742\u975e\u53ef\u5fae\u5206\u5956\u52b1\u573a\u666f\u3002", "keywords": "\u6269\u6563\u6a21\u578b,\u5f3a\u5316\u5b66\u4e60,\u503c\u51fd\u6570,KL\u6b63\u5219\u5316,\u975e\u53ef\u5fae\u5206\u5956\u52b1"}}
{"id": "2505.15793", "pdf": "https://arxiv.org/pdf/2505.15793", "abs": "https://arxiv.org/abs/2505.15793", "authors": ["Zhiwen Chen", "Bo Leng", "Zhuoren Li", "Hanming Deng", "Guizhe Jin", "Ran Yu", "Huanxi Wen"], "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations.Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM-Hinted RL\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6765\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6027\u80fd\uff0c\u540c\u65f6\u89e3\u51b3LLM\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7684LLM\u4e3b\u5bfc\u7684RL\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u7684LLM\u8f93\u51fa\uff0c\u53ef\u80fd\u5371\u53ca\u9a7e\u9a76\u7b56\u7565\u6027\u80fd\u3002", "method": "\u63d0\u51faHCRMP\u67b6\u6784\uff0c\u5305\u542b\u589e\u5f3a\u8bed\u4e49\u8868\u793a\u6a21\u5757\u3001\u4e0a\u4e0b\u6587\u7a33\u5b9a\u6027\u951a\u6a21\u5757\u548c\u8bed\u4e49\u7f13\u5b58\u6a21\u5757\uff0c\u7528\u4e8e\u72b6\u6001\u589e\u5f3a\u548c\u7b56\u7565\u4f18\u5316\u3002", "result": "HCRMP\u5728CARLA\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4efb\u52a1\u6210\u529f\u7387\u9ad8\u8fbe80.3%\uff0c\u5b89\u5168\u5173\u952e\u6761\u4ef6\u4e0b\u78b0\u649e\u7387\u964d\u4f4e11.4%\u3002", "conclusion": "\u4fdd\u6301LLM\u4e0eRL\u7684\u76f8\u5bf9\u72ec\u7acb\u6027\u5bf9\u89e3\u51b3\u5e7b\u89c9\u95ee\u9898\u81f3\u5173\u91cd\u8981\uff0cHCRMP\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u9a7e\u9a76\u6027\u80fd\u3002", "keywords": "\u5927\u8bed\u8a00\u6a21\u578b, \u5f3a\u5316\u5b66\u4e60, \u81ea\u52a8\u9a7e\u9a76, \u5e7b\u89c9\u95ee\u9898, HCRMP"}}
