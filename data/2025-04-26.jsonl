{"id": "2504.16956", "pdf": "https://arxiv.org/pdf/2504.16956", "abs": "https://arxiv.org/abs/2504.16956", "authors": ["Cong Qi", "Hanzhang Fang", "Tianxing Hu", "Siqi Jiang", "Wei Zhi"], "title": "Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity", "categories": ["cs.CL", "cs.LG", "q-bio.GN"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of\ncellular heterogeneity, but its complexity, which is marked by high\ndimensionality, sparsity, and batch effects, which poses major computational\nchallenges. Transformer-based models have made significant advances in this\ndomain but are often limited by their quadratic complexity and suboptimal\nhandling of long-range dependencies. In this work, we introduce GeneMamba, a\nscalable and efficient foundation model for single-cell transcriptomics built\non state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba\ncaptures bidirectional gene context with linear-time complexity, offering\nsubstantial computational gains over transformer baselines. The model is\npretrained on nearly 30 million cells and incorporates biologically informed\nobjectives, including pathway-aware contrastive loss and rank-based gene\nencoding. We evaluate GeneMamba across diverse tasks, including multi-batch\nintegration, cell type annotation, and gene-gene correlation, demonstrating\nstrong performance, interpretability, and robustness. These results position\nGeneMamba as a practical and powerful alternative to transformer-based methods,\nadvancing the development of biologically grounded, scalable tools for\nlarge-scale single-cell data analysis."}
{"id": "2504.16977", "pdf": "https://arxiv.org/pdf/2504.16977", "abs": "https://arxiv.org/abs/2504.16977", "authors": ["Priyaranjan Pattnayak", "Hitesh Laxmichand Patel", "Amit Agarwal"], "title": "Tokenization Matters: Improving Zero-Shot NER for Indic Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization is a critical component of Natural Language Processing (NLP),\nespecially for low resource languages, where subword segmentation influences\nvocabulary structure and downstream task accuracy. Although Byte Pair Encoding\n(BPE) is a standard tokenization method in multilingual language models, its\nsuitability for Named Entity Recognition (NER) in low resource Indic languages\nremains underexplored due to its limitations in handling morphological\ncomplexity. In this work, we systematically compare BPE, SentencePiece, and\nCharacter Level tokenization strategies using IndicBERT for NER tasks in low\nresource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as\nextremely low resource Indic languages like Santali, Manipuri, and Sindhi. We\nassess both intrinsic linguistic properties tokenization efficiency, out of\nvocabulary (OOV) rates, and morphological preservation as well as extrinsic\ndownstream performance, including fine tuning and zero shot cross lingual\ntransfer.\n  Our experiments show that SentencePiece is a consistently better performing\napproach than BPE for NER in low resource Indic Languages, particularly in zero\nshot cross lingual settings, as it better preserves entity consistency. While\nBPE provides the most compact tokenization form, it is not capable of\ngeneralization because it misclassifies or even fails to recognize entity\nlabels when tested on unseen languages. In contrast, SentencePiece constitutes\na better linguistic structural preservation model, benefiting extremely low\nresource and morphologically rich Indic languages, such as Santali and\nManipuri, for superior entity recognition, as well as high generalization\nacross scripts, such as Sindhi, written in Arabic. The results point to\nSentencePiece as the more effective tokenization strategy for NER within\nmultilingual and low resource Indic NLP applications."}
{"id": "2504.17025", "pdf": "https://arxiv.org/pdf/2504.17025", "abs": "https://arxiv.org/abs/2504.17025", "authors": ["Luca Moroni", "Giovanni Puccetti", "Pere-Lluis Huguet Cabot", "Andrei Stefan Bejgu", "Edoardo Barba", "Alessio Miaschi", "Felice Dell'Orletta", "Andrea Esuli", "Roberto Navigli"], "title": "Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "The number of pretrained Large Language Models (LLMs) is increasing steadily,\nthough the majority are designed predominantly for the English language. While\nstate-of-the-art LLMs can handle other languages, due to language contamination\nor some degree of multilingual pretraining data, they are not optimized for\nnon-English languages, leading to inefficient encoding (high token \"fertility\")\nand slower inference speed. In this work, we thoroughly compare a variety of\nvocabulary adaptation techniques for optimizing English LLMs for the Italian\nlanguage, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a\nnovel method that leverages neural mapping for vocabulary substitution. SAVA\nachieves competitive performance across multiple downstream tasks, enhancing\ngrounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing\ntoken fertility by 25\\%, and Llama-3.1-8B, optimizing the vocabulary and\nreducing the number of parameters by 1 billion. We show that, following the\nadaptation of the vocabulary, these models can recover their performance with a\nrelatively limited stage of continual training on the target language. Finally,\nwe test the capabilities of the adapted models on various multi-choice and\ngenerative tasks."}
{"id": "2504.17052", "pdf": "https://arxiv.org/pdf/2504.17052", "abs": "https://arxiv.org/abs/2504.17052", "authors": ["Shariar Kabir", "Kevin Esterling", "Yue Dong"], "title": "Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models", "categories": ["cs.CL"], "comment": "20 pages, 9 figures", "summary": "Large Language Models (LLMs) are increasingly shaping political discourse,\nyet their responses often display inconsistency when subjected to scrutiny.\nWhile prior research has primarily categorized LLM outputs as left- or\nright-leaning to assess their political stances, a critical question remains:\nDo these responses reflect genuine internal beliefs or merely surface-level\nalignment with training data? To address this, we propose a novel framework for\nevaluating belief depth by analyzing (1) argumentative consistency and (2)\nuncertainty quantification. We evaluate 12 LLMs on 19 economic policies from\nthe Political Compass Test, challenging their belief stability with both\nsupportive and opposing arguments. Our analysis reveals that LLMs exhibit\ntopic-specific belief stability rather than a uniform ideological stance.\nNotably, up to 95% of left-leaning models' responses and 89% of right-leaning\nmodels' responses remain consistent under the challenge, enabling semantic\nentropy to achieve high accuracy (AUROC=0.78), effectively distinguishing\nbetween surface-level alignment from genuine belief. These findings call into\nquestion the assumption that LLMs maintain stable, human-like political\nideologies, emphasizing the importance of conducting topic-specific reliability\nassessments for real-world applications."}
{"id": "2504.16937", "pdf": "https://arxiv.org/pdf/2504.16937", "abs": "https://arxiv.org/abs/2504.16937", "authors": ["Ariel S. Kapusta", "David Jin", "Peter M. Teague", "Robert A. Houston", "Jonathan B. Elliott", "Grace Y. Park", "Shelby S. Holdren"], "title": "A Framework for the Assurance of AI-Enabled Systems", "categories": ["cs.AI"], "comment": "12 pages, 2 figures, published in conference proceedings of SPIE\n  Defense and Commercial Sensing conference on Assurance and Security for\n  AI-enabled Systems 2025", "summary": "The United States Department of Defense (DOD) looks to accelerate the\ndevelopment and deployment of AI capabilities across a wide spectrum of defense\napplications to maintain strategic advantages. However, many common features of\nAI algorithms that make them powerful, such as capacity for learning,\nlarge-scale data ingestion, and problem-solving, raise new technical, security,\nand ethical challenges. These challenges may hinder adoption due to uncertainty\nin development, testing, assurance, processes, and requirements.\nTrustworthiness through assurance is essential to achieve the expected value\nfrom AI.\n  This paper proposes a claims-based framework for risk management and\nassurance of AI systems that addresses the competing needs for faster\ndeployment, successful adoption, and rigorous evaluation. This framework\nsupports programs across all acquisition pathways provide grounds for\nsufficient confidence that an AI-enabled system (AIES) meets its intended\nmission goals without introducing unacceptable risks throughout its lifecycle.\nThe paper's contributions are a framework process for AI assurance, a set of\nrelevant definitions to enable constructive conversations on the topic of AI\nassurance, and a discussion of important considerations in AI assurance. The\nframework aims to provide the DOD a robust yet efficient mechanism for swiftly\nfielding effective AI capabilities without overlooking critical risks or\nundermining stakeholder trust."}
{"id": "2504.16961", "pdf": "https://arxiv.org/pdf/2504.16961", "abs": "https://arxiv.org/abs/2504.16961", "authors": ["Binon Teji", "Swarup Roy"], "title": "A Novel Graph Transformer Framework for Gene Regulatory Network Inference", "categories": ["cs.LG", "cs.ET", "q-bio.GN", "q-bio.MN", "q-bio.QM"], "comment": null, "summary": "The inference of gene regulatory networks (GRNs) is a foundational stride\ntowards deciphering the fundamentals of complex biological systems. Inferring a\npossible regulatory link between two genes can be formulated as a link\nprediction problem. Inference of GRNs via gene coexpression profiling data may\nnot always reflect true biological interactions, as its susceptibility to noise\nand misrepresenting true biological regulatory relationships. Most GRN\ninference methods face several challenges in the network reconstruction phase.\nTherefore, it is important to encode gene expression values, leverege the prior\nknowledge gained from the available inferred network structures and positional\ninformations of the input network nodes towards inferring a better and more\nconfident GRN network reconstruction. In this paper, we explore the integration\nof multiple inferred networks to enhance the inference of Gene Regulatory\nNetworks (GRNs). Primarily, we employ autoencoder embeddings to capture gene\nexpression patterns directly from raw data, preserving intricate biological\nsignals. Then, we embed the prior knowledge from GRN structures transforming\nthem into a text-like representation using random walks, which are then encoded\nwith a masked language model, BERT, to generate global embeddings for each gene\nacross all networks. Additionally, we embed the positional encodings of the\ninput gene networks to better identify the position of each unique gene within\nthe graph. These embeddings are integrated into graph transformer-based model,\ntermed GT-GRN, for GRN inference. The GT-GRN model effectively utilizes the\ntopological structure of the ground truth network while incorporating the\nenriched encoded information. Experimental results demonstrate that GT-GRN\nsignificantly outperforms existing GRN inference methods, achieving superior\naccuracy and highlighting the robustness of our approach."}
{"id": "2504.17075", "pdf": "https://arxiv.org/pdf/2504.17075", "abs": "https://arxiv.org/abs/2504.17075", "authors": ["Arjun Subramonian", "Vagrant Gautam", "Preethi Seshadri", "Dietrich Klakow", "Kai-Wei Chang", "Yizhou Sun"], "title": "Agree to Disagree? A Meta-Evaluation of LLM Misgendering", "categories": ["cs.CL", "cs.CY"], "comment": "Work in progress", "summary": "Numerous methods have been proposed to measure LLM misgendering, including\nprobability-based evaluations (e.g., automatically with templatic sentences)\nand generation-based evaluations (e.g., with automatic heuristics or human\nvalidation). However, it has gone unexamined whether these evaluation methods\nhave convergent validity, that is, whether their results align. Therefore, we\nconduct a systematic meta-evaluation of these methods across three existing\ndatasets for LLM misgendering. We propose a method to transform each dataset to\nenable parallel probability- and generation-based evaluation. Then, by\nautomatically evaluating a suite of 6 models from 3 families, we find that\nthese methods can disagree with each other at the instance, dataset, and model\nlevels, conflicting on 20.2% of evaluation instances. Finally, with a human\nevaluation of 2400 LLM generations, we show that misgendering behaviour is\ncomplex and goes far beyond pronouns, which automatic evaluations are not\ncurrently designed to capture, suggesting essential disagreement with human\nevaluations. Based on our findings, we provide recommendations for future\nevaluations of LLM misgendering. Our results are also more widely relevant, as\nthey call into question broader methodological conventions in LLM evaluation,\nwhich often assume that different evaluation methods agree."}
{"id": "2504.16938", "pdf": "https://arxiv.org/pdf/2504.16938", "abs": "https://arxiv.org/abs/2504.16938", "authors": ["Lucas Carr", "Nicholas Leisegang", "Thomas Meyer", "Sergei Obiedkov"], "title": "Rational Inference in Formal Concept Analysis", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "Defeasible conditionals are a form of non-monotonic inference which enable\nthe expression of statements like \"if $\\phi$ then normally $\\psi$\". The KLM\nframework defines a semantics for the propositional case of defeasible\nconditionals by construction of a preference ordering over possible worlds. The\npattern of reasoning induced by these semantics is characterised by consequence\nrelations satisfying certain desirable properties of non-monotonic reasoning.\nIn FCA, implications are used to describe dependencies between attributes.\nHowever, these implications are unsuitable to reason with erroneous data or\ndata prone to exceptions. Until recently, the topic of non-monotonic inference\nin FCA has remained largely uninvestigated. In this paper, we provide a\nconstruction of the KLM framework for defeasible reasoning in FCA and show that\nthis construction remains faithful to the principle of non-monotonic inference\ndescribed in the original framework. We present an additional argument that,\nwhile remaining consistent with the original ideas around non-monotonic\nreasoning, the defeasible reasoning we propose in FCA offers a more contextual\nview on inference, providing the ability for more relevant conclusions to be\ndrawn when compared to the propositional case."}
{"id": "2504.16968", "pdf": "https://arxiv.org/pdf/2504.16968", "abs": "https://arxiv.org/abs/2504.16968", "authors": ["Jun Wu", "Jiangtao Wen", "Yuxing Han"], "title": "Backslash: Rate Constrained Optimized Training of Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rapid advancement of large-language models (LLMs) has driven extensive\nresearch into parameter compression after training has been completed, yet\ncompression during the training phase remains largely unexplored. In this work,\nwe introduce Rate-Constrained Training (Backslash), a novel training-time\ncompression approach based on rate-distortion optimization (RDO). Backslash\nenables a flexible trade-off between model accuracy and complexity,\nsignificantly reducing parameter redundancy while preserving performance.\nExperiments in various architectures and tasks demonstrate that Backslash can\nreduce memory usage by 60\\% - 90\\% without accuracy loss and provides\nsignificant compression gain compared to compression after training. Moreover,\nBackslash proves to be highly versatile: it enhances generalization with small\nLagrange multipliers, improves model robustness to pruning (maintaining\naccuracy even at 80\\% pruning rates), and enables network simplification for\naccelerated inference on edge devices."}
{"id": "2504.17083", "pdf": "https://arxiv.org/pdf/2504.17083", "abs": "https://arxiv.org/abs/2504.17083", "authors": ["Rendi Chevi", "Kentaro Inui", "Thamar Solorio", "Alham Fikri Aji"], "title": "How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study", "categories": ["cs.CL"], "comment": "Accepted at GenAICHI 2025 @ ACM CHI 2025", "summary": "What makes an interaction with the LLM more preferable for the user? While it\nis intuitive to assume that information accuracy in the LLM's responses would\nbe one of the influential variables, recent studies have found that inaccurate\nLLM's responses could still be preferable when they are perceived to be more\nauthoritative, certain, well-articulated, or simply verbose. These variables\ninterestingly fall under the broader category of language style, implying that\nthe style in the LLM's responses might meaningfully influence users'\npreferences. This hypothesized dynamic could have double-edged consequences:\nenhancing the overall user experience while simultaneously increasing their\nsusceptibility to risks such as LLM's misinformation or hallucinations. In this\nshort paper, we present our preliminary studies in exploring this subject.\nThrough a series of exploratory and experimental user studies, we found that\nLLM's language style does indeed influence user's preferences, but how and\nwhich language styles influence the preference varied across different user\npopulations, and more interestingly, moderated by the user's very own\nindividual traits. As a preliminary work, the findings in our studies should be\ninterpreted with caution, particularly given the limitations in our samples,\nwhich still need wider demographic diversity and larger sample sizes. Our\nfuture directions will first aim to address these limitations, which would\nenable a more comprehensive joint effect analysis between the language style,\nindividual traits, and preferences, and further investigate the potential\ncausal relationship between and beyond these variables."}
{"id": "2504.16939", "pdf": "https://arxiv.org/pdf/2504.16939", "abs": "https://arxiv.org/abs/2504.16939", "authors": ["Emre Can Acikgoz", "Cheng Qian", "Hongru Wang", "Vardhan Dongre", "Xiusi Chen", "Heng Ji", "Dilek Hakkani-TÃ¼r", "Gokhan Tur"], "title": "A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have propelled conversational\nAI from traditional dialogue systems into sophisticated agents capable of\nautonomous actions, contextual awareness, and multi-turn interactions with\nusers. Yet, fundamental questions about their capabilities, limitations, and\npaths forward remain open. This survey paper presents a desideratum for\nnext-generation Conversational Agents - what has been achieved, what challenges\npersist, and what must be done for more scalable systems that approach\nhuman-level intelligence. To that end, we systematically analyze LLM-driven\nConversational Agents by organizing their capabilities into three primary\ndimensions: (i) Reasoning - logical, systematic thinking inspired by human\nintelligence for decision making, (ii) Monitor - encompassing self-awareness\nand user interaction monitoring, and (iii) Control - focusing on tool\nutilization and policy following. Building upon this, we introduce a novel\ntaxonomy by classifying recent work on Conversational Agents around our\nproposed desideratum. We identify critical research gaps and outline key\ndirections, including realistic evaluations, long-term multi-turn reasoning\nskills, self-evolution capabilities, collaborative and multi-agent task\ncompletion, personalization, and proactivity. This work aims to provide a\nstructured foundation, highlight existing limitations, and offer insights into\npotential future research directions for Conversational Agents, ultimately\nadvancing progress toward Artificial General Intelligence (AGI). We maintain a\ncurated repository of papers at:\nhttps://github.com/emrecanacikgoz/awesome-conversational-agents."}
{"id": "2504.16970", "pdf": "https://arxiv.org/pdf/2504.16970", "abs": "https://arxiv.org/abs/2504.16970", "authors": ["Yin Wang", "Chunlin Gong", "Xiang Wu", "Hanleran Zhang"], "title": "STFM: A Spatio-Temporal Information Fusion Model Based on Phase Space Reconstruction for Sea Surface Temperature Prediction", "categories": ["cs.LG"], "comment": "19 pages, 14 figures", "summary": "The sea surface temperature (SST), a key environmental parameter, is crucial\nto optimizing production planning, making its accurate prediction a vital\nresearch topic. However, the inherent nonlinearity of the marine dynamic system\npresents significant challenges. Current forecasting methods mainly include\nphysics-based numerical simulations and data-driven machine learning\napproaches. The former, while describing SST evolution through differential\nequations, suffers from high computational complexity and limited\napplicability, whereas the latter, despite its computational benefits, requires\nlarge datasets and faces interpretability challenges. This study presents a\nprediction framework based solely on data-driven techniques. Using phase space\nreconstruction, we construct initial-delay attractor pairs with a mathematical\nhomeomorphism and design a Spatio-Temporal Fusion Mapping (STFM) to uncover\ntheir intrinsic connections. Unlike conventional models, our method captures\nSST dynamics efficiently through phase space reconstruction and achieves high\nprediction accuracy with minimal training data in comparative tests"}
{"id": "2504.17091", "pdf": "https://arxiv.org/pdf/2504.17091", "abs": "https://arxiv.org/abs/2504.17091", "authors": ["Seunghyun Yoo"], "title": "Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning", "categories": ["cs.CL", "68T05"], "comment": "5 page", "summary": "Due to the proliferation of short-form content and the rapid adoption of AI,\nopportunities for deep, reflective thinking have significantly diminished,\nundermining users' critical thinking and reducing engagement with the reasoning\nbehind AI-generated outputs. To address this issue, we propose an Interactive\nChain-of-Thought (CoT) Framework that enhances human-centered explainability\nand responsible AI usage by making the model's inference process transparent,\nmodular, and user-editable. The framework decomposes reasoning into clearly\ndefined blocks that users can inspect, modify, and re-execute, encouraging\nactive cognitive engagement rather than passive consumption. It further\nintegrates a lightweight edit-adaptation mechanism inspired by preference\nlearning, allowing the system to align with diverse cognitive styles and user\nintentions. Ethical transparency is ensured through explicit metadata\ndisclosure, built-in bias checkpoint functionality, and privacy-preserving\nsafeguards. This work outlines the design principles and architecture necessary\nto promote critical engagement, responsible interaction, and inclusive\nadaptation in AI systems aimed at addressing complex societal challenges."}
{"id": "2504.17006", "pdf": "https://arxiv.org/pdf/2504.17006", "abs": "https://arxiv.org/abs/2504.17006", "authors": ["Jalal Arabneydi", "Saiful Islam", "Srijita Das", "Sai Krishna Gottipati", "William Duguay", "Cloderic Mars", "Matthew E. Taylor", "Matthew Guzdial", "Antoine Fagette", "Younes Zerouali"], "title": "A Systematic Approach to Design Real-World Human-in-the-Loop Deep Reinforcement Learning: Salient Features, Challenges and Trade-offs", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": "This is a result of the collaboration by JACOBB, AMII(Alberta Machine\n  Intelligence Institute), Thales and AI Redefined (AIR) in 2021-2023", "summary": "With the growing popularity of deep reinforcement learning (DRL),\nhuman-in-the-loop (HITL) approach has the potential to revolutionize the way we\napproach decision-making problems and create new opportunities for human-AI\ncollaboration. In this article, we introduce a novel multi-layered hierarchical\nHITL DRL algorithm that comprises three types of learning: self learning,\nimitation learning and transfer learning. In addition, we consider three forms\nof human inputs: reward, action and demonstration. Furthermore, we discuss main\nchallenges, trade-offs and advantages of HITL in solving complex problems and\nhow human information can be integrated in the AI solution systematically. To\nverify our technical results, we present a real-world unmanned aerial vehicles\n(UAV) problem wherein a number of enemy drones attack a restricted area. The\nobjective is to design a scalable HITL DRL algorithm for ally drones to\nneutralize the enemy drones before they reach the area. To this end, we first\nimplement our solution using an award-winning open-source HITL software called\nCogment. We then demonstrate several interesting results such as (a) HITL leads\nto faster training and higher performance, (b) advice acts as a guiding\ndirection for gradient methods and lowers variance, and (c) the amount of\nadvice should neither be too large nor too small to avoid over-training and\nunder-training. Finally, we illustrate the role of human-AI cooperation in\nsolving two real-world complex scenarios, i.e., overloaded and decoy attacks."}
{"id": "2504.16972", "pdf": "https://arxiv.org/pdf/2504.16972", "abs": "https://arxiv.org/abs/2504.16972", "authors": ["Hossein Ahmadi", "Sajjad Emdadi Mahdimahalleh", "Arman Farahat", "Banafsheh Saffari"], "title": "Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "comment": null, "summary": "The rapid growth of unlabeled time-series data in domains such as wireless\ncommunications, radar, biomedical engineering, and the Internet of Things (IoT)\nhas driven advancements in unsupervised learning. This review synthesizes\nrecent progress in applying autoencoders and vision transformers for\nunsupervised signal analysis, focusing on their architectures, applications,\nand emerging trends. We explore how these models enable feature extraction,\nanomaly detection, and classification across diverse signal types, including\nelectrocardiograms, radar waveforms, and IoT sensor data. The review highlights\nthe strengths of hybrid architectures and self-supervised learning, while\nidentifying challenges in interpretability, scalability, and domain\ngeneralization. By bridging methodological innovations and practical\napplications, this work offers a roadmap for developing robust, adaptive models\nfor signal intelligence."}
{"id": "2504.17119", "pdf": "https://arxiv.org/pdf/2504.17119", "abs": "https://arxiv.org/abs/2504.17119", "authors": ["Muskan Garg", "Shaina Raza", "Shebuti Rayana", "Xingyi Liu", "Sunghwan Sohn"], "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey", "categories": ["cs.CL", "cs.AI"], "comment": "35 pages, 7 tables, 5 figures", "summary": "Despite substantial progress in healthcare applications driven by large\nlanguage models (LLMs), growing concerns around data privacy, and limited\nresources; the small language models (SLMs) offer a scalable and clinically\nviable solution for efficient performance in resource-constrained environments\nfor next-generation healthcare informatics. Our comprehensive survey presents a\ntaxonomic framework to identify and categorize them for healthcare\nprofessionals and informaticians. The timeline of healthcare SLM contributions\nestablishes a foundational framework for analyzing models across three\ndimensions: NLP tasks, stakeholder roles, and the continuum of care. We present\na taxonomic framework to identify the architectural foundations for building\nmodels from scratch; adapting SLMs to clinical precision through prompting,\ninstruction fine-tuning, and reasoning; and accessibility and sustainability\nthrough compression techniques. Our primary objective is to offer a\ncomprehensive survey for healthcare professionals, introducing recent\ninnovations in model optimization and equipping them with curated resources to\nsupport future research and development in the field. Aiming to showcase the\ngroundbreaking advancements in SLMs for healthcare, we present a comprehensive\ncompilation of experimental results across widely studied NLP tasks in\nhealthcare to highlight the transformative potential of SLMs in healthcare. The\nupdated repository is available at Github"}
{"id": "2504.17017", "pdf": "https://arxiv.org/pdf/2504.17017", "abs": "https://arxiv.org/abs/2504.17017", "authors": ["Balaji Rao", "William Eiers", "Carlo Lipizzi"], "title": "Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification", "categories": ["cs.AI", "cs.FL", "cs.LG", "cs.LO"], "comment": "Accepted to the Proceedings of the 19th Conference on Neurosymbolic\n  Learning and Reasoning (NeSy 2025)", "summary": "Formally verifying properties of software code has been a highly desirable\ntask, especially with the emergence of LLM-generated code. In the same vein,\nthey provide an interesting avenue for the exploration of formal verification\nand mechanistic interpretability. Since the introduction of code-specific\nmodels, despite their successes in generating code in Lean4 and Isabelle, the\ntask of generalized theorem proving still remains far from being fully solved\nand will be a benchmark for reasoning capability in LLMs. In this work, we\nintroduce a framework that generates whole proofs in a formal language to be\nused within systems that utilize the power of built-in tactics and\noff-the-shelf automated theorem provers. Our framework includes 3 components:\ngenerating natural language statements of the code to be verified, an LLM that\ngenerates formal proofs for the given statement, and a module employing\nheuristics for building the final proof. To train the LLM, we employ a 2-stage\nfine-tuning process, where we first use SFT-based training to enable the model\nto generate syntactically correct Isabelle code and then RL-based training that\nencourages the model to generate proofs verified by a theorem prover. We\nvalidate our framework using the miniF2F-test benchmark and the Isabelle proof\nassistant and design a use case to verify the correctness of the AWS S3 bucket\naccess policy code. We also curate a dataset based on the\nFVEL\\textsubscript{\\textnormal{ER}} dataset for future training tasks."}
{"id": "2504.16980", "pdf": "https://arxiv.org/pdf/2504.16980", "abs": "https://arxiv.org/abs/2504.16980", "authors": ["Pratyush Maini", "Sachin Goyal", "Dylan Sam", "Alex Robey", "Yash Savani", "Yiding Jiang", "Andy Zou", "Zacharcy C. Lipton", "J. Zico Kolter"], "title": "Safety Pretraining: Toward the Next Generation of Safe AI", "categories": ["cs.LG"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in high-stakes\nsettings, the risk of generating harmful or toxic content remains a central\nchallenge. Post-hoc alignment methods are brittle: once unsafe patterns are\nlearned during pretraining, they are hard to remove. We present a data-centric\npretraining framework that builds safety into the model from the start. Our\ncontributions include: (i) a safety classifier trained on 10,000 GPT-4 labeled\nexamples, used to filter 600B tokens; (ii) the largest synthetic safety dataset\nto date (100B tokens) generated via recontextualization of harmful web data;\n(iii) RefuseWeb and Moral Education datasets that convert harmful prompts into\nrefusal dialogues and web-style educational material; (iv) Harmfulness-Tag\nannotations injected during pretraining to flag unsafe content and steer away\ninference from harmful generations; and (v) safety evaluations measuring base\nmodel behavior before instruction tuning. Our safety-pretrained models reduce\nattack success rates from 38.8% to 8.4% with no performance degradation on\nstandard LLM safety benchmarks."}
{"id": "2504.17130", "pdf": "https://arxiv.org/pdf/2504.17130", "abs": "https://arxiv.org/abs/2504.17130", "authors": ["Hannah Cyberey", "David Evans"], "title": "Steering the CensorShip: Uncovering Representation Vectors for LLM \"Thought\" Control", "categories": ["cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) have transformed the way we access information.\nThese models are often tuned to refuse to comply with requests that are\nconsidered harmful and to produce responses that better align with the\npreferences of those who control the models. To understand how this\n\"censorship\" works. We use representation engineering techniques to study\nopen-weights safety-tuned models. We present a method for finding a\nrefusal--compliance vector that detects and controls the level of censorship in\nmodel outputs. We also analyze recent reasoning LLMs, distilled from\nDeepSeek-R1, and uncover an additional dimension of censorship through \"thought\nsuppression\". We show a similar approach can be used to find a vector that\nsuppresses the model's reasoning process, allowing us to remove censorship by\napplying the negative multiples of this vector"}
{"id": "2504.17087", "pdf": "https://arxiv.org/pdf/2504.17087", "abs": "https://arxiv.org/abs/2504.17087", "authors": ["Yuran Li", "Jama Hussein Mohamud", "Chongren Sun", "Di Wu", "Benoit Boulet"], "title": "Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments", "categories": ["cs.AI"], "comment": "12 pages, 5 figures, 6 tables", "summary": "Large language models (LLMs) are being widely applied across various fields,\nbut as tasks become more complex, evaluating their responses is increasingly\nchallenging. Compared to human evaluators, the use of LLMs to support\nperformance evaluation offers a more efficient alternative. However, most\nstudies focus mainly on aligning LLMs' judgments with human preferences,\noverlooking the existence of biases and mistakes in human judgment.\nFurthermore, how to select suitable LLM judgments given multiple potential LLM\nresponses remains underexplored. To address these two aforementioned issues, we\npropose a three-stage meta-judge selection pipeline: 1) developing a\ncomprehensive rubric with GPT-4 and human experts, 2) using three advanced LLM\nagents to score judgments, and 3) applying a threshold to filter out\nlow-scoring judgments. Compared to methods using a single LLM as both judge and\nmeta-judge, our pipeline introduces multi-agent collaboration and a more\ncomprehensive rubric. Experimental results on the JudgeBench dataset show about\n15.55\\% improvement compared to raw judgments and about 8.37\\% improvement over\nthe single-agent baseline. Our work demonstrates the potential of LLMs as\nmeta-judges and lays the foundation for future research on constructing\npreference datasets for LLM-as-a-judge reinforcement learning."}
{"id": "2504.17004", "pdf": "https://arxiv.org/pdf/2504.17004", "abs": "https://arxiv.org/abs/2504.17004", "authors": ["Amin Karbasi", "Omar Montasser", "John Sous", "Grigoris Velegkas"], "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment."}
{"id": "2504.17137", "pdf": "https://arxiv.org/pdf/2504.17137", "abs": "https://arxiv.org/abs/2504.17137", "authors": ["Chanhee Park", "Hyeonseok Moon", "Chanjun Park", "Heuiseok Lim"], "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL2025 Findings", "summary": "Retrieval-Augmented Generation (RAG) has gained prominence as an effective\nmethod for enhancing the generative capabilities of Large Language Models\n(LLMs) through the incorporation of external knowledge. However, the evaluation\nof RAG systems remains a challenge, due to the intricate interplay between\nretrieval and generation components. This limitation has resulted in a scarcity\nof benchmarks that facilitate a detailed, component-specific assessment. In\nthis work, we present MIRAGE, a Question Answering dataset specifically\ndesigned for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped\nto a retrieval pool of 37,800 entries, enabling an efficient and precise\nevaluation of both retrieval and generation tasks. We also introduce novel\nevaluation metrics aimed at measuring RAG adaptability, encompassing dimensions\nsuch as noise vulnerability, context acceptability, context insensitivity, and\ncontext misinterpretation. Through comprehensive experiments across various\nretriever-LLM configurations, we provide new insights into the optimal\nalignment of model pairs and the nuanced dynamics within RAG systems. The\ndataset and evaluation code are publicly available, allowing for seamless\nintegration and customization in diverse research settings\\footnote{The MIRAGE\ncode and data are available at https://github.com/nlpai-lab/MIRAGE."}
{"id": "2504.17179", "pdf": "https://arxiv.org/pdf/2504.17179", "abs": "https://arxiv.org/abs/2504.17179", "authors": ["Mohammad Zarei", "Melanie A Jutras", "Eliana Evans", "Mike Tan", "Omid Aaramoon"], "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO", "68T45, 68T05 68T45, 68T05 68T45, 68T05", "I.2.6; I.2.10; I.4.8"], "comment": "8 pages, 10 figures. Accepted to IEEE Conference on Artificial\n  Intelligence (CAI), 2025", "summary": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately\ndetect objects and interpret their surroundings. However, even when trained\nusing millions of miles of real-world data, AVs are often unable to detect rare\nfailure modes (RFMs). The problem of RFMs is commonly referred to as the\n\"long-tail challenge\", due to the distribution of data including many instances\nthat are very rarely seen. In this paper, we present a novel approach that\nutilizes advanced generative and explainable AI techniques to aid in\nunderstanding RFMs. Our methods can be used to enhance the robustness and\nreliability of AVs when combined with both downstream model training and\ntesting. We extract segmentation masks for objects of interest (e.g., cars) and\ninvert them to create environmental masks. These masks, combined with carefully\ncrafted text prompts, are fed into a custom diffusion model. We leverage the\nStable Diffusion inpainting model guided by adversarial noise optimization to\ngenerate images containing diverse environments designed to evade object\ndetection models and expose vulnerabilities in AI systems. Finally, we produce\nnatural language descriptions of the generated RFMs that can guide developers\nand policymakers to improve the safety and reliability of AV systems."}
{"id": "2504.17028", "pdf": "https://arxiv.org/pdf/2504.17028", "abs": "https://arxiv.org/abs/2504.17028", "authors": ["Iman Khadir", "Shane Stevenson", "Henry Li", "Kyle Krick", "Abram Burrows", "David Hall", "Stan Posey", "Samuel S. P. Shen"], "title": "Democracy of AI Numerical Weather Models: An Example of Global Forecasting with FourCastNetv2 Made by a University Research Lab Using GPU", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": "12 pages, 8 figures", "summary": "This paper demonstrates the feasibility of democratizing AI-driven global\nweather forecasting models among university research groups by leveraging\nGraphics Processing Units (GPUs) and freely available AI models, such as\nNVIDIA's FourCastNetv2. FourCastNetv2 is an NVIDIA's advanced neural network\nfor weather prediction and is trained on a 73-channel subset of the European\nCentre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset\nat single levels and different pressure levels. Although the training\nspecifications for FourCastNetv2 are not released to the public, the training\ndocumentation of the model's first generation, FourCastNet, is available to all\nusers. The training had 64 A100 GPUs and took 16 hours to complete. Although\nNVIDIA's models offer significant reductions in both time and cost compared to\ntraditional Numerical Weather Prediction (NWP), reproducing published\nforecasting results presents ongoing challenges for resource-constrained\nuniversity research groups with limited GPU availability. We demonstrate both\n(i) leveraging FourCastNetv2 to create predictions through the designated\napplication programming interface (API) and (ii) utilizing NVIDIA hardware to\ntrain the original FourCastNet model. Further, this paper demonstrates the\ncapabilities and limitations of NVIDIA A100's for resource-limited research\ngroups in universities. We also explore data management, training efficiency,\nand model validation, highlighting the advantages and challenges of using\nlimited high-performance computing resources. Consequently, this paper and its\ncorresponding GitHub materials may serve as an initial guide for other\nuniversity research groups and courses related to machine learning, climate\nscience, and data science to develop research and education programs on AI\nweather forecasting, and hence help democratize the AI NWP in the digital\neconomy."}
{"id": "2504.17192", "pdf": "https://arxiv.org/pdf/2504.17192", "abs": "https://arxiv.org/abs/2504.17192", "authors": ["Minju Seo", "Jinheon Baek", "Seongyun Lee", "Sung Ju Hwang"], "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning", "categories": ["cs.CL"], "comment": null, "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins."}
{"id": "2504.17282", "pdf": "https://arxiv.org/pdf/2504.17282", "abs": "https://arxiv.org/abs/2504.17282", "authors": ["Lynn Cherif", "Flemming Kondrup", "David Venuto", "Ankit Anand", "Doina Precup", "Khimya Khetarpal"], "title": "Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Agents that can autonomously navigate the web through a graphical user\ninterface (GUI) using a unified action space (e.g., mouse and keyboard actions)\ncan require very large amounts of domain-specific expert demonstrations to\nachieve good performance. Low sample efficiency is often exacerbated in\nsparse-reward and large-action-space environments, such as a web GUI, where\nonly a few actions are relevant in any given situation. In this work, we\nconsider the low-data regime, with limited or no access to expert behavior. To\nenable sample-efficient learning, we explore the effect of constraining the\naction space through $\\textit{intent-based affordances}$ -- i.e., considering\nin any situation only the subset of actions that achieve a desired outcome. We\npropose $\\textbf{Code as Generative Affordances}$ $(\\textbf{$\\texttt{CoGA}$})$,\na method that leverages pre-trained vision-language models (VLMs) to generate\ncode that determines affordable actions through implicit intent-completion\nfunctions and using a fully-automated program generation and verification\npipeline. These programs are then used in-the-loop of a reinforcement learning\nagent to return a set of affordances given a pixel observation. By greatly\nreducing the number of actions that an agent must consider, we demonstrate on a\nwide range of tasks in the MiniWob++ benchmark that: $\\textbf{1)}$\n$\\texttt{CoGA}$ is orders of magnitude more sample efficient than its RL agent,\n$\\textbf{2)}$ $\\texttt{CoGA}$'s programs can generalize within a family of\ntasks, and $\\textbf{3)}$ $\\texttt{CoGA}$ performs better or on par compared\nwith behavior cloning when a small number of expert demonstrations is\navailable."}
{"id": "2504.17058", "pdf": "https://arxiv.org/pdf/2504.17058", "abs": "https://arxiv.org/abs/2504.17058", "authors": ["Rahul Vishwakarma"], "title": "Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The generation of high-quality synthetic data presents significant challenges\nin machine learning research, particularly regarding statistical fidelity and\nuncertainty quantification. Existing generative models produce compelling\nsynthetic samples but lack rigorous statistical guarantees about their relation\nto the underlying data distribution, limiting their applicability in critical\ndomains requiring robust error bounds. We address this fundamental limitation\nby presenting a novel framework that incorporates conformal prediction\nmethodologies into Generative Adversarial Networks (GANs). By integrating\nmultiple conformal prediction paradigms including Inductive Conformal\nPrediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction,\nand Venn-Abers Predictors, we establish distribution-free uncertainty\nquantification in generated samples. This approach, termed Conformalized GAN\n(cGAN), demonstrates enhanced calibration properties while maintaining the\ngenerative power of traditional GANs, producing synthetic data with provable\nstatistical guarantees. We provide rigorous mathematical proofs establishing\nfinite-sample validity guarantees and asymptotic efficiency properties,\nenabling the reliable application of synthetic data in high-stakes domains\nincluding healthcare, finance, and autonomous systems."}
{"id": "2504.17200", "pdf": "https://arxiv.org/pdf/2504.17200", "abs": "https://arxiv.org/abs/2504.17200", "authors": ["Yangxinyu Xie", "Bowen Jiang", "Tanwi Mallick", "Joshua David Bergerson", "John K. Hutchison", "Duane R. Verner", "Jordan Branham", "M. Ross Alexander", "Robert B. Ross", "Yan Feng", "Leslie-Anne Levy", "Weijie Su", "Camillo J. Taylor"], "title": "A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are a transformational capability at the\nfrontier of artificial intelligence and machine learning that can support\ndecision-makers in addressing pressing societal challenges such as extreme\nnatural hazard events. As generalized models, LLMs often struggle to provide\ncontext-specific information, particularly in areas requiring specialized\nknowledge. In this work we propose a retrieval-augmented generation (RAG)-based\nmulti-agent LLM system to support analysis and decision-making in the context\nof natural hazards and extreme weather events. As a proof of concept, we\npresent WildfireGPT, a specialized system focused on wildfire hazards. The\narchitecture employs a user-centered, multi-agent design to deliver tailored\nrisk insights across diverse stakeholder groups. By integrating natural hazard\nand extreme weather projection data, observational datasets, and scientific\nliterature through an RAG framework, the system ensures both the accuracy and\ncontextual relevance of the information it provides. Evaluation across ten\nexpert-led case studies demonstrates that WildfireGPT significantly outperforms\nexisting LLM-based solutions for decision support."}
{"id": "2504.17295", "pdf": "https://arxiv.org/pdf/2504.17295", "abs": "https://arxiv.org/abs/2504.17295", "authors": ["Shahrzad Khayatbashi", "Viktor SjÃ¶lind", "Anders GranÃ¥ker", "Amin Jalali"], "title": "AI-Enhanced Business Process Automation: A Case Study in the Insurance Domain Using Object-Centric Process Mining", "categories": ["cs.AI"], "comment": null, "summary": "Recent advancements in Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), have enhanced organizations' ability to reengineer\nbusiness processes by automating knowledge-intensive tasks. This automation\ndrives digital transformation, often through gradual transitions that improve\nprocess efficiency and effectiveness. To fully assess the impact of such\nautomation, a data-driven analysis approach is needed - one that examines how\ntraditional and AI-enhanced process variants coexist during this transition.\nObject-Centric Process Mining (OCPM) has emerged as a valuable method that\nenables such analysis, yet real-world case studies are still needed to\ndemonstrate its applicability. This paper presents a case study from the\ninsurance sector, where an LLM was deployed in production to automate the\nidentification of claim parts, a task previously performed manually and\nidentified as a bottleneck for scalability. To evaluate this transformation, we\napply OCPM to assess the impact of AI-driven automation on process scalability.\nOur findings indicate that while LLMs significantly enhance operational\ncapacity, they also introduce new process dynamics that require further\nrefinement. This study also demonstrates the practical application of OCPM in a\nreal-world setting, highlighting its advantages and limitations."}
{"id": "2504.17065", "pdf": "https://arxiv.org/pdf/2504.17065", "abs": "https://arxiv.org/abs/2504.17065", "authors": ["Sahar Bagherkhani", "Jackson Christopher Earls", "Franco De Flaviis", "Pierre Baldi"], "title": "Antenna Near-Field Reconstruction from Far-Field Data Using Convolutional Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Electromagnetic field reconstruction is crucial in many applications,\nincluding antenna diagnostics, electromagnetic interference analysis, and\nsystem modeling. This paper presents a deep learning-based approach for\nFar-Field to Near-Field (FF-NF) transformation using Convolutional Neural\nNetworks (CNNs). The goal is to reconstruct near-field distributions from the\nfar-field data of an antenna without relying on explicit analytical\ntransformations. The CNNs are trained on paired far-field and near-field data\nand evaluated using mean squared error (MSE). The best model achieves a\ntraining error of 0.0199 and a test error of 0.3898. Moreover, visual\ncomparisons between the predicted and true near-field distributions demonstrate\nthe model's effectiveness in capturing complex electromagnetic field behavior,\nhighlighting the potential of deep learning in electromagnetic field\nreconstruction."}
{"id": "2504.17220", "pdf": "https://arxiv.org/pdf/2504.17220", "abs": "https://arxiv.org/abs/2504.17220", "authors": ["Kaidong Feng", "Zhu Sun", "Jie Yang", "Hui Fang", "Xinghua Qu", "Wenyuan Liu"], "title": "Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "LLMs are increasingly explored for bundle generation, thanks to their\nreasoning capabilities and knowledge. However, deploying large-scale LLMs\nintroduces significant efficiency challenges, primarily high computational\ncosts during fine-tuning and inference due to their massive parameterization.\nKnowledge distillation (KD) offers a promising solution, transferring expertise\nfrom large teacher models to compact student models. This study systematically\ninvestigates knowledge distillation approaches for bundle generation, aiming to\nminimize computational demands while preserving performance. We explore three\ncritical research questions: (1) how does the format of KD impact bundle\ngeneration performance? (2) to what extent does the quantity of distilled\nknowledge influence performance? and (3) how do different ways of utilizing the\ndistilled knowledge affect performance? We propose a comprehensive KD framework\nthat (i) progressively extracts knowledge (patterns, rules, deep thoughts);\n(ii) captures varying quantities of distilled knowledge through different\nstrategies; and (iii) exploits complementary LLM adaptation techniques\n(in-context learning, supervised fine-tuning, combination) to leverage\ndistilled knowledge in small student models for domain-specific adaptation and\nenhanced efficiency. Extensive experiments provide valuable insights into how\nknowledge format, quantity, and utilization methodologies collectively shape\nLLM-based bundle generation performance, exhibiting KD's significant potential\nfor more efficient yet effective LLM-based bundle generation."}
{"id": "2504.17356", "pdf": "https://arxiv.org/pdf/2504.17356", "abs": "https://arxiv.org/abs/2504.17356", "authors": ["Weiliang Zhang", "Xiaohan Huang", "Yi Du", "Ziyue Qiao", "Qingqing Long", "Zhen Meng", "Yuanchun Zhou", "Meng Xiao"], "title": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": "20 pages, keywords: Automated Feature Engineering, Tabular Dataset,\n  Multi-Agent Reinforcement Learning, Feature Selection", "summary": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved."}
{"id": "2504.17066", "pdf": "https://arxiv.org/pdf/2504.17066", "abs": "https://arxiv.org/abs/2504.17066", "authors": ["Kewen Peng", "Yicheng Yang", "Hao Zhuo", "Tim Menzies"], "title": "Whence Is A Model Fair? Fixing Fairness Bugs via Propensity Score Matching", "categories": ["cs.LG", "cs.CY", "cs.SE", "stat.ML"], "comment": null, "summary": "Fairness-aware learning aims to mitigate discrimination against specific\nprotected social groups (e.g., those categorized by gender, ethnicity, age)\nwhile minimizing predictive performance loss. Despite efforts to improve\nfairness in machine learning, prior studies have shown that many models remain\nunfair when measured against various fairness metrics. In this paper, we\nexamine whether the way training and testing data are sampled affects the\nreliability of reported fairness metrics. Since training and test sets are\noften randomly sampled from the same population, bias present in the training\ndata may still exist in the test data, potentially skewing fairness\nassessments. To address this, we propose FairMatch, a post-processing method\nthat applies propensity score matching to evaluate and mitigate bias. FairMatch\nidentifies control and treatment pairs with similar propensity scores in the\ntest set and adjusts decision thresholds for different subgroups accordingly.\nFor samples that cannot be matched, we perform probabilistic calibration using\nfairness-aware loss functions. Experimental results demonstrate that our\napproach can (a) precisely locate subsets of the test data where the model is\nunbiased, and (b) significantly reduce bias on the remaining data. Overall,\npropensity score matching offers a principled way to improve both fairness\nevaluation and mitigation, without sacrificing predictive performance."}
{"id": "2504.17238", "pdf": "https://arxiv.org/pdf/2504.17238", "abs": "https://arxiv.org/abs/2504.17238", "authors": ["Jinfeng Zhou", "Yuxuan Chen", "Jianing Yin", "Yongkang Huang", "Yihan Shi", "Xikun Zhang", "Libiao Peng", "Rongsheng Zhang", "Tangjie Lv", "Zhipeng Hu", "Hongning Wang", "Minlie Huang"], "title": "Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Cognitive Restructuring (CR) is a psychotherapeutic process aimed at\nidentifying and restructuring an individual's negative thoughts, arising from\nmental health challenges, into more helpful and positive ones via multi-turn\ndialogues. Clinician shortage and stigma urge the development of human-LLM\ninteractive psychotherapy for CR. Yet, existing efforts implement CR via simple\ntext rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to\nalign with the psychotherapeutic process for effective CR. To address this gap,\nwe propose CRDial, a novel framework for CR, which creates multi-turn dialogues\nwith specifically designed identification and restructuring stages of negative\nthoughts, integrates sentence-level supportive conversation strategies, and\nadopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we\ndistill Crisp, a large-scale and high-quality bilingual dialogue dataset, from\nLLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and\n14B scales. Extensive human studies show the superiority of Crispers in\npointwise, pairwise, and intervention evaluations."}
{"id": "2504.17402", "pdf": "https://arxiv.org/pdf/2504.17402", "abs": "https://arxiv.org/abs/2504.17402", "authors": ["Anna Sofia Lippolis", "Mohammad Javad Saeedizade", "Robin Keskisarkka", "Aldo Gangemi", "Eva Blomqvist", "Andrea Giovanni Nuzzolese"], "title": "Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown significant potential for ontology\nengineering. However, it is still unclear to what extent they are applicable to\nthe task of domain-specific ontology generation. In this study, we explore the\napplication of LLMs for automated ontology generation and evaluate their\nperformance across different domains. Specifically, we investigate the\ngeneralizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both\nequipped with reasoning capabilities, by generating ontologies from a set of\ncompetency questions (CQs) and related user stories. Our experimental setup\ncomprises six distinct domains carried out in existing ontology engineering\nprojects and a total of 95 curated CQs designed to test the models' reasoning\nfor ontology engineering. Our findings show that with both LLMs, the\nperformance of the experiments is remarkably consistent across all domains,\nindicating that these methods are capable of generalizing ontology generation\ntasks irrespective of the domain. These results highlight the potential of\nLLM-based approaches in achieving scalable and domain-agnostic ontology\nconstruction and lay the groundwork for further research into enhancing\nautomated reasoning and knowledge representation techniques."}
{"id": "2504.17068", "pdf": "https://arxiv.org/pdf/2504.17068", "abs": "https://arxiv.org/abs/2504.17068", "authors": ["Pranav Kantroo", "GÃ¼nter P. Wagner", "Benjamin B. Machta"], "title": "In-Context Learning can distort the relationship between sequence likelihoods and biological fitness", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Language models have emerged as powerful predictors of the viability of\nbiological sequences. During training these models learn the rules of the\ngrammar obeyed by sequences of amino acids or nucleotides. Once trained, these\nmodels can take a sequence as input and produce a likelihood score as an\noutput; a higher likelihood implies adherence to the learned grammar and\ncorrelates with experimental fitness measurements. Here we show that in-context\nlearning can distort the relationship between fitness and likelihood scores of\nsequences. This phenomenon most prominently manifests as anomalously high\nlikelihood scores for sequences that contain repeated motifs. We use protein\nlanguage models with different architectures trained on the masked language\nmodeling objective for our experiments, and find transformer-based models to be\nparticularly vulnerable to this effect. This behavior is mediated by a look-up\noperation where the model seeks the identity of the masked position by using\nthe other copy of the repeated motif as a reference. This retrieval behavior\ncan override the model's learned priors. This phenomenon persists for\nimperfectly repeated sequences, and extends to other kinds of biologically\nrelevant features such as reversed complement motifs in RNA sequences that fold\ninto hairpin structures."}
{"id": "2504.17252", "pdf": "https://arxiv.org/pdf/2504.17252", "abs": "https://arxiv.org/abs/2504.17252", "authors": ["Ocheme Anthony Ekle", "Biswarup Das"], "title": "Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo", "categories": ["cs.CL", "cs.LG", "68T50, 68T01", "I.2.7; I.2.1"], "comment": "25 pages, 14 combined figures (19 total), includes horizontal\n  layouts. Submitted to arXiv for open access", "summary": "In this study, we develop Neural Machine Translation (NMT) and\nTransformer-based transfer learning models for English-to-Igbo translation - a\nlow-resource African language spoken by over 40 million people across Nigeria\nand West Africa. Our models are trained on a curated and benchmarked dataset\ncompiled from Bible corpora, local news, Wikipedia articles, and Common Crawl,\nall verified by native language experts. We leverage Recurrent Neural Network\n(RNN) architectures, including Long Short-Term Memory (LSTM) and Gated\nRecurrent Units (GRU), enhanced with attention mechanisms to improve\ntranslation accuracy. To further enhance performance, we apply transfer\nlearning using MarianNMT pre-trained models within the SimpleTransformers\nframework. Our RNN-based system achieves competitive results, closely matching\nexisting English-Igbo benchmarks. With transfer learning, we observe a\nperformance gain of +4.83 BLEU points, reaching an estimated translation\naccuracy of 70%. These findings highlight the effectiveness of combining RNNs\nwith transfer learning to address the performance gap in low-resource language\ntranslation tasks."}
{"id": "2504.17404", "pdf": "https://arxiv.org/pdf/2504.17404", "abs": "https://arxiv.org/abs/2504.17404", "authors": ["Feifei Zhao", "Yuwei Wang", "Enmeng Lu", "Dongcheng Zhao", "Bing Han", "Haibo Tong", "Yao Liang", "Dongqi Liang", "Kang Sun", "Lei Wang", "Yitao Liang", "Chao Liu", "Yaodong Yang", "Yi Zeng"], "title": "Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society", "categories": ["cs.AI"], "comment": null, "summary": "Artificial Intelligence (AI) systems are becoming increasingly powerful and\nautonomous, and may progress to surpass human intelligence levels, namely\nArtificial Superintelligence (ASI). During the progression from AI to ASI, it\nmay exceed human control, violate human values, and even lead to irreversible\ncatastrophic consequences in extreme cases. This gives rise to a pressing issue\nthat needs to be addressed: superalignment, ensuring that AI systems much\nsmarter than humans, remain aligned with human (compatible) intentions and\nvalues. Existing scalable oversight and weak-to-strong generalization methods\nmay prove substantially infeasible and inadequate when facing ASI. We must\nexplore safer and more pluralistic frameworks and approaches for\nsuperalignment. In this paper, we redefine superalignment as the human-AI\nco-alignment towards a sustainable symbiotic society, and highlight a framework\nthat integrates external oversight and intrinsic proactive alignment. External\noversight superalignment should be grounded in human-centered ultimate\ndecision, supplemented by interpretable automated evaluation and correction, to\nachieve continuous alignment with humanity's evolving values. Intrinsic\nproactive superalignment is rooted in a profound understanding of the self,\nothers, and society, integrating self-awareness, self-reflection, and empathy\nto spontaneously infer human intentions, distinguishing good from evil and\nproactively considering human well-being, ultimately attaining human-AI\nco-alignment through iterative interaction. The integration of\nexternally-driven oversight with intrinsically-driven proactive alignment\nempowers sustainable symbiotic societies through human-AI co-alignment, paving\nthe way for achieving safe and beneficial AGI and ASI for good, for human, and\nfor a symbiotic ecology."}
{"id": "2504.17073", "pdf": "https://arxiv.org/pdf/2504.17073", "abs": "https://arxiv.org/abs/2504.17073", "authors": ["David Lu", "Lior Maman", "Jackson Earls", "Amir Boag", "Pierre Baldi"], "title": "Sparse Phased Array Optimization Using Deep Learning", "categories": ["cs.LG"], "comment": null, "summary": "Antenna arrays are widely used in wireless communication, radar systems,\nradio astronomy, and military defense to enhance signal strength, directivity,\nand interference suppression. We introduce a deep learning-based optimization\napproach that enhances the design of sparse phased arrays by reducing grating\nlobes. This approach begins by generating sparse array configurations to\naddress the non-convex challenges and extensive degrees of freedom inherent in\narray design. We use neural networks to approximate the non-convex cost\nfunction that estimates the energy ratio between the main and side lobes. This\ndifferentiable approximation facilitates cost function minimization through\ngradient descent, optimizing the antenna elements' coordinates and leading to\nan improved layout. Additionally, we incorporate a tailored penalty mechanism\nthat includes various physical and design constraints into the optimization\nprocess, enhancing its robustness and practical applicability. We demonstrate\nthe effectiveness of our method by applying it to the ten array configurations\nwith the lowest initial costs, achieving further cost reductions ranging from\n411% to 643%, with an impressive average improvement of 552%. By significantly\nreducing side lobe levels in antenna arrays, this breakthrough paves the way\nfor ultra-precise beamforming, enhanced interference mitigation, and\nnext-generation wireless and radar systems with unprecedented efficiency and\nclarity."}
{"id": "2504.17264", "pdf": "https://arxiv.org/pdf/2504.17264", "abs": "https://arxiv.org/abs/2504.17264", "authors": ["Zhaolu Kang", "Hongtian Cai", "Xiangyang Ji", "Jinzhe Li", "Nanfei Gu"], "title": "JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "In recent years, Unsupervised Domain Adaptation (UDA) has gained significant\nattention in the field of Natural Language Processing (NLP) owing to its\nability to enhance model generalization across diverse domains. However, its\napplication for knowledge transfer between distinct legal domains remains\nlargely unexplored. To address the challenges posed by lengthy and complex\nlegal texts and the limited availability of large-scale annotated datasets, we\npropose JurisCTC, a novel model designed to improve the accuracy of Legal\nJudgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC\nfacilitates effective knowledge transfer across various legal domains and\nemploys contrastive learning to distinguish samples from different domains.\nSpecifically, for the LJP task, we enable knowledge transfer between civil and\ncriminal law domains. Compared to other models and specific large language\nmodels (LLMs), JurisCTC demonstrates notable advancements, achieving peak\naccuracies of 76.59% and 78.83%, respectively."}
{"id": "2504.17531", "pdf": "https://arxiv.org/pdf/2504.17531", "abs": "https://arxiv.org/abs/2504.17531", "authors": ["Justus Flerlage", "Ilja Behnke", "Odej Kao"], "title": "Towards Machine-Generated Code for the Resolution of User Intentions", "categories": ["cs.AI"], "comment": null, "summary": "The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code, which is tantamount to the generation of workflows\ncomprising a multitude of interdependent steps. This development represents a\nsignificant progression in the realm of hybrid workflows, where human and\nartificial intelligence collaborate to address user intentions, with the former\nresponsible for defining these intentions and the latter for implementing the\nsolutions to address them. In this paper, we investigate the feasibility of\ngenerating and executing workflows through code generation that results from\nprompting an LLM with a concrete user intention, such as \\emph{Please send my\ncar title to my insurance company}, and a simplified application programming\ninterface for a GUI-less operating system. We provide in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate a general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions."}
{"id": "2504.17074", "pdf": "https://arxiv.org/pdf/2504.17074", "abs": "https://arxiv.org/abs/2504.17074", "authors": ["William R. Keely", "Otto LamminpÃ¤Ã¤", "Steffen Mauceri", "Sean M. R. Crowell", "Christopher W. O'Dell", "Gregory R. McGarragh"], "title": "Conditional Diffusion-Based Retrieval of Atmospheric CO2 from Earth Observing Spectroscopy", "categories": ["cs.LG", "astro-ph.IM"], "comment": "Published as a workshop paper in \"Tackling Climate Change with\n  Machine Learning\", ICLR 2025. https://www.climatechange.ai/papers/iclr2025/12", "summary": "Satellite-based estimates of greenhouse gas (GHG) properties from\nobservations of reflected solar spectra are integral for understanding and\nmonitoring complex terrestrial systems and their impact on the carbon cycle due\nto their near global coverage. Known as retrieval, making GHG concentration\nestimations from these observations is a non-linear Bayesian inverse problem,\nwhich is operationally solved using a computationally expensive algorithm\ncalled Optimal Estimation (OE), providing a Gaussian approximation to a\nnon-Gaussian posterior. This leads to issues in solver algorithm convergence,\nand to unrealistically confident uncertainty estimates for the retrieved\nquantities. Upcoming satellite missions will provide orders of magnitude more\ndata than the current constellation of GHG observers. Development of fast and\naccurate retrieval algorithms with robust uncertainty quantification is\ncritical. Doing so stands to provide substantial climate impact of moving\ntowards the goal of near continuous real-time global monitoring of carbon\nsources and sinks which is essential for policy making. To achieve this goal,\nwe propose a diffusion-based approach to flexibly retrieve a Gaussian or\nnon-Gaussian posterior, for NASA's Orbiting Carbon Observatory-2 spectrometer,\nwhile providing a substantial computational speed-up over the current\noperational state-of-the-art."}
{"id": "2504.17279", "pdf": "https://arxiv.org/pdf/2504.17279", "abs": "https://arxiv.org/abs/2504.17279", "authors": ["Xiuying Chen", "Tairan Wang", "Juexiao Zhou", "Zirui Song", "Xin Gao", "Xiangliang Zhang"], "title": "Evaluating and Mitigating Bias in AI-Based Medical Text Generation", "categories": ["cs.CL"], "comment": "12 pages, 8 figures, published in Nature Computational Science", "summary": "Artificial intelligence (AI) systems, particularly those based on deep\nlearning models, have increasingly achieved expert-level performance in medical\napplications. However, there is growing concern that such AI systems may\nreflect and amplify human bias, and reduce the quality of their performance in\nhistorically under-served populations. The fairness issue has attracted\nconsiderable research interest in the medical imaging classification field, yet\nit remains understudied in the text generation domain. In this study, we\ninvestigate the fairness problem in text generation within the medical field\nand observe significant performance discrepancies across different races,\nsexes, and age groups, including intersectional groups, various model scales,\nand different evaluation metrics. To mitigate this fairness issue, we propose\nan algorithm that selectively optimizes those underperformed groups to reduce\nbias. The selection rules take into account not only word-level accuracy but\nalso the pathology accuracy to the target reference, while ensuring that the\nentire process remains fully differentiable for effective model training. Our\nevaluations across multiple backbones, datasets, and modalities demonstrate\nthat our proposed algorithm enhances fairness in text generation without\ncompromising overall performance. Specifically, the disparities among various\ngroups across different metrics were diminished by more than 30% with our\nalgorithm, while the relative change in text generation accuracy was typically\nwithin 2%. By reducing the bias generated by deep learning models, our proposed\napproach can potentially alleviate concerns about the fairness and reliability\nof text generation diagnosis in medical domain.\n  Our code is publicly available to facilitate further research at\nhttps://github.com/iriscxy/GenFair."}
{"id": "2504.17544", "pdf": "https://arxiv.org/pdf/2504.17544", "abs": "https://arxiv.org/abs/2504.17544", "authors": ["W. Russell Neuman", "Chad Coleman", "Ali Dasdan", "Safinah Ali", "Manan Shah"], "title": "Auditing the Ethical Logic of Generative AI Models", "categories": ["cs.AI"], "comment": null, "summary": "As generative AI models become increasingly integrated into high-stakes\ndomains, the need for robust methods to evaluate their ethical reasoning\nbecomes increasingly important. This paper introduces a five-dimensional audit\nmodel -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth\nof Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic\nof leading large language models (LLMs). Drawing on traditions from applied\nethics and higher-order thinking, we present a multi-battery prompt approach,\nincluding novel ethical dilemmas, to probe the models' reasoning across diverse\ncontexts. We benchmark seven major LLMs finding that while models generally\nconverge on ethical decisions, they vary in explanatory rigor and moral\nprioritization. Chain-of-Thought prompting and reasoning-optimized models\nsignificantly enhance performance on our audit metrics. This study introduces a\nscalable methodology for ethical benchmarking of AI systems and highlights the\npotential for AI to complement human moral reasoning in complex decision-making\ncontexts."}
{"id": "2504.17079", "pdf": "https://arxiv.org/pdf/2504.17079", "abs": "https://arxiv.org/abs/2504.17079", "authors": ["Esam Mahdi", "C. Martin-Barreiro", "X. Cabezas"], "title": "A Novel Hybrid Approach Using an Attention-Based Transformer + GRU Model for Predicting Cryptocurrency Prices", "categories": ["cs.LG", "stat.AP"], "comment": null, "summary": "In this article, we introduce a novel deep learning hybrid model that\nintegrates attention Transformer and Gated Recurrent Unit (GRU) architectures\nto improve the accuracy of cryptocurrency price predictions. By combining the\nTransformer's strength in capturing long-range patterns with the GRU's ability\nto model short-term and sequential trends, the hybrid model provides a\nwell-rounded approach to time series forecasting. We apply the model to predict\nthe daily closing prices of Bitcoin and Ethereum based on historical data that\ninclude past prices, trading volumes, and the Fear and Greed index. We evaluate\nthe performance of our proposed model by comparing it with four other machine\nlearning models: two are non-sequential feedforward models: Radial Basis\nFunction Network (RBFN) and General Regression Neural Network (GRNN), and two\nare bidirectional sequential memory-based models: Bidirectional Long-Short-Term\nMemory (BiLSTM) and Bidirectional Gated Recurrent Unit (BiGRU). The performance\nof the model is assessed using several metrics, including Mean Squared Error\n(MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean\nAbsolute Percentage Error (MAPE), along with statistical validation through the\nnonparametric Friedman test followed by a post hoc Wilcoxon signed rank test.\nThe results demonstrate that our hybrid model consistently achieves superior\naccuracy, highlighting its effectiveness for financial prediction tasks. These\nfindings provide valuable insights for improving real-time decision making in\ncryptocurrency markets and support the growing use of hybrid deep learning\nmodels in financial analytics."}
{"id": "2504.17309", "pdf": "https://arxiv.org/pdf/2504.17309", "abs": "https://arxiv.org/abs/2504.17309", "authors": ["Junyan Zhang", "Shuliang Liu", "Aiwei Liu", "Yubo Gao", "Jungang Li", "Xiaojie Gu", "Xuming Hu"], "title": "CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality", "categories": ["cs.CL"], "comment": "Published at the 1st workshop on GenAI Watermarking, collocated with\n  ICLR 2025", "summary": "Watermarking technology is a method used to trace the usage of content\ngenerated by large language models. Sentence-level watermarking aids in\npreserving the semantic integrity within individual sentences while maintaining\ngreater robustness. However, many existing sentence-level watermarking\ntechniques depend on arbitrary segmentation or generation processes to embed\nwatermarks, which can limit the availability of appropriate sentences. This\nlimitation, in turn, compromises the quality of the generated response. To\naddress the challenge of balancing high text quality with robust watermark\ndetection, we propose CoheMark, an advanced sentence-level watermarking\ntechnique that exploits the cohesive relationships between sentences for better\nlogical fluency. The core methodology of CoheMark involves selecting sentences\nthrough trained fuzzy c-means clustering and applying specific next sentence\nselection criteria. Experimental evaluations demonstrate that CoheMark achieves\nstrong watermark strength while exerting minimal impact on text quality."}
{"id": "2504.16940", "pdf": "https://arxiv.org/pdf/2504.16940", "abs": "https://arxiv.org/abs/2504.16940", "authors": ["Drew Linsley", "Pinyuan Feng", "Thomas Serre"], "title": "Can deep neural networks learn biological vision?", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep neural networks (DNNs) once showed increasing alignment with primate\nneural responses as they improved on computer vision benchmarks. This trend\nraised the exciting possibility that better models of biological vision would\ncome as a byproduct of the deep learning revolution in artificial intelligence.\nHowever, the trend has reversed over recent years as DNNs have scaled to human\nor superhuman recognition accuracy, a divergence that may stem from modern DNNs\nlearning to rely on different visual features than primates to solve tasks.\nWhere will better computational models of biological vision come from? We\npropose that vision science must break from artificial intelligence to develop\nalgorithms that are designed with biological visual systems in mind instead of\ninternet data benchmarks. We predict that the next generation of deep learning\nmodels of biological vision will be trained with data diets, training routines,\nand objectives that are closer to those that shape human vision than those that\nare in use today."}
{"id": "2504.17099", "pdf": "https://arxiv.org/pdf/2504.17099", "abs": "https://arxiv.org/abs/2504.17099", "authors": ["Martin Boeckling", "Heiko Paulheim", "Sarah Detzler"], "title": "GeoRDF2Vec Learning Location-Aware Entity Representations in Knowledge Graphs", "categories": ["cs.LG", "cs.SI"], "comment": "18 pages, ESWC 2025", "summary": "Many knowledge graphs contain a substantial number of spatial entities, such\nas cities, buildings, and natural landmarks. For many of these entities, exact\ngeometries are stored within the knowledge graphs. However, most existing\napproaches for learning entity representations do not take these geometries\ninto account. In this paper, we introduce a variant of RDF2Vec that\nincorporates geometric information to learn location-aware embeddings of\nentities. Our approach expands different nodes by flooding the graph from\ngeographic nodes, ensuring that each reachable node is considered. Based on the\nresulting flooded graph, we apply a modified version of RDF2Vec that biases\ngraph walks using spatial weights. Through evaluations on multiple benchmark\ndatasets, we demonstrate that our approach outperforms both non-location-aware\nRDF2Vec and GeoTransE."}
{"id": "2504.17311", "pdf": "https://arxiv.org/pdf/2504.17311", "abs": "https://arxiv.org/abs/2504.17311", "authors": ["Yulia Otmakhova", "Hung Thinh Truong", "Rahmad Mahendra", "Zenan Zhai", "Rongxin Zhu", "Daniel Beck", "Jey Han Lau"], "title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors."}
{"id": "2504.16942", "pdf": "https://arxiv.org/pdf/2504.16942", "abs": "https://arxiv.org/abs/2504.16942", "authors": ["Shushman Choudhury", "Elad Aharoni", "Chandrakumari Suvarna", "Iveel Tsogsuren", "Abdul Rahman Kreidieh", "Chun-Ta Lu", "Neha Arora"], "title": "S2Vec: Self-Supervised Geospatial Embeddings", "categories": ["cs.SI", "cs.AI", "cs.CV"], "comment": "To be submitted to ACM Transactions on Spatial Algorithms and Systems", "summary": "Scalable general-purpose representations of the built environment are crucial\nfor geospatial artificial intelligence applications. This paper introduces\nS2Vec, a novel self-supervised framework for learning such geospatial\nembeddings. S2Vec uses the S2 Geometry library to partition large areas into\ndiscrete S2 cells, rasterizes built environment feature vectors within cells as\nimages, and applies masked autoencoding on these rasterized images to encode\nthe feature vectors. This approach yields task-agnostic embeddings that capture\nlocal feature characteristics and broader spatial relationships. We evaluate\nS2Vec on three large-scale socioeconomic prediction tasks, showing its\ncompetitive performance against state-of-the-art image-based embeddings. We\nalso explore the benefits of combining S2Vec embeddings with image-based\nembeddings downstream, showing that such multimodal fusion can often improve\nperformance. Our results highlight how S2Vec can learn effective\ngeneral-purpose geospatial representations and how it can complement other data\nmodalities in geospatial artificial intelligence."}
{"id": "2504.17109", "pdf": "https://arxiv.org/pdf/2504.17109", "abs": "https://arxiv.org/abs/2504.17109", "authors": ["Zhaobin Mo", "Xiangyi Liao", "Dominik A. Karbowski", "Yanbing Wang"], "title": "Discovering the Precursors of Traffic Breakdowns Using Spatiotemporal Graph Attribution Networks", "categories": ["cs.LG"], "comment": null, "summary": "Understanding and predicting the precursors of traffic breakdowns is critical\nfor improving road safety and traffic flow management. This paper presents a\nnovel approach combining spatiotemporal graph neural networks (ST-GNNs) with\nShapley values to identify and interpret traffic breakdown precursors. By\nextending Shapley explanation methods to a spatiotemporal setting, our proposed\nmethod bridges the gap between black-box neural network predictions and\ninterpretable causes. We demonstrate the method on the Interstate-24 data, and\nidentify that road topology and abrupt braking are major factors that lead to\ntraffic breakdowns."}
{"id": "2504.17332", "pdf": "https://arxiv.org/pdf/2504.17332", "abs": "https://arxiv.org/abs/2504.17332", "authors": ["Zihan Wang", "Lu Yuan", "Zhengxuan Zhang", "Qing Zhao"], "title": "Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection", "categories": ["cs.CL"], "comment": null, "summary": "In the digital era, social media has become a major conduit for information\ndissemination, yet it also facilitates the rapid spread of misinformation.\nTraditional misinformation detection methods primarily focus on surface-level\nfeatures, overlooking the crucial roles of human empathy in the propagation\nprocess. To address this gap, we propose the Dual-Aspect Empathy Framework\n(DAE), which integrates cognitive and emotional empathy to analyze\nmisinformation from both the creator and reader perspectives. By examining\ncreators' cognitive strategies and emotional appeals, as well as simulating\nreaders' cognitive judgments and emotional responses using Large Language\nModels (LLMs), DAE offers a more comprehensive and human-centric approach to\nmisinformation detection. Moreover, we further introduce an empathy-aware\nfiltering mechanism to enhance response authenticity and diversity.\nExperimental results on benchmark datasets demonstrate that DAE outperforms\nexisting methods, providing a novel paradigm for multimodal misinformation\ndetection."}
{"id": "2504.16946", "pdf": "https://arxiv.org/pdf/2504.16946", "abs": "https://arxiv.org/abs/2504.16946", "authors": ["Xiaotong Ye", "Nicolas Bougie", "Toshihiko Yamasaki", "Narimasa Watanabe"], "title": "MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "Generative agents offer promising capabilities for simulating realistic urban\nbehaviors. However, existing methods oversimplify transportation choices in\nmodern cities, and require prohibitive computational resources for large-scale\npopulation simulation. To address these limitations, we first present a virtual\ncity that features multiple functional buildings and transportation modes.\nThen, we conduct extensive surveys to model behavioral choices and mobility\npreferences among population groups. Building on these insights, we introduce a\nsimulation framework that captures the complexity of urban mobility while\nremaining scalable, enabling the simulation of over 4,000 agents. To assess the\nrealism of the generated behaviors, we perform a series of micro and\nmacro-level analyses. Beyond mere performance comparison, we explore insightful\nexperiments, such as predicting crowd density from movement patterns and\nidentifying trends in vehicle preferences across agent demographics."}
{"id": "2504.17140", "pdf": "https://arxiv.org/pdf/2504.17140", "abs": "https://arxiv.org/abs/2504.17140", "authors": ["Ashish Ranjan", "Ayush Agarwal", "Shalin Barot", "Sushant Kumar"], "title": "Scalable Permutation-Aware Modeling for Temporal Set Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Temporal set prediction involves forecasting the elements that will appear in\nthe next set, given a sequence of prior sets, each containing a variable number\nof elements. Existing methods often rely on intricate architectures with\nsubstantial computational overhead, which hampers their scalability. In this\nwork, we introduce a novel and scalable framework that leverages\npermutation-equivariant and permutation-invariant transformations to\nefficiently model set dynamics. Our approach significantly reduces both\ntraining and inference time while maintaining competitive performance.\nExtensive experiments on multiple public benchmarks show that our method\nachieves results on par with or superior to state-of-the-art models across\nseveral evaluation metrics. These results underscore the effectiveness of our\nmodel in enabling efficient and scalable temporal set prediction."}
{"id": "2504.17353", "pdf": "https://arxiv.org/pdf/2504.17353", "abs": "https://arxiv.org/abs/2504.17353", "authors": ["Chengguang Gan", "Sunbowen Lee", "Zhixi Cai", "Yanbin Wei", "Lei Zheng", "Yunhao Liang", "Shiwen Ni", "Tatsunori Mori"], "title": "M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction", "categories": ["cs.CL", "cs.CV", "cs.MM"], "comment": null, "summary": "Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection\nof information extraction and model interpretability. MRE aims to leverage the\nmutual understanding between tasks of different granularities, enhancing the\nperformance of both coarse-grained and fine-grained tasks through joint\nmodeling. While MRE has been explored and validated in the textual domain, its\napplicability to visual and multimodal domains remains unexplored. In this\nwork, we extend MRE to the multimodal information extraction domain for the\nfirst time. Specifically, we introduce a new task: Multimodal Mutual\nReinforcement Effect (M-MRE), and construct a corresponding dataset to support\nthis task. To address the challenges posed by M-MRE, we further propose a\nPrompt Format Adapter (PFA) that is fully compatible with various Large\nVision-Language Models (LVLMs). Experimental results demonstrate that MRE can\nalso be observed in the M-MRE task, a multimodal text-image understanding\nscenario. This provides strong evidence that MRE facilitates mutual gains\nacross three interrelated tasks, confirming its generalizability beyond the\ntextual domain."}
{"id": "2504.16947", "pdf": "https://arxiv.org/pdf/2504.16947", "abs": "https://arxiv.org/abs/2504.16947", "authors": ["Dachun Sun", "You Lyu", "Jinning Li", "Yizhuo Chen", "Tianshi Wang", "Tomoyoshi Kimura", "Tarek Abdelzaher"], "title": "SCRAG: Social Computing-Based Retrieval Augmented Generation for Community Response Forecasting in Social Media Environments", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "This paper introduces SCRAG, a prediction framework inspired by social\ncomputing, designed to forecast community responses to real or hypothetical\nsocial media posts. SCRAG can be used by public relations specialists (e.g., to\ncraft messaging in ways that avoid unintended misinterpretations) or public\nfigures and influencers (e.g., to anticipate social responses), among other\napplications related to public sentiment prediction, crisis management, and\nsocial what-if analysis. While large language models (LLMs) have achieved\nremarkable success in generating coherent and contextually rich text, their\nreliance on static training data and susceptibility to hallucinations limit\ntheir effectiveness at response forecasting in dynamic social media\nenvironments. SCRAG overcomes these challenges by integrating LLMs with a\nRetrieval-Augmented Generation (RAG) technique rooted in social computing.\nSpecifically, our framework retrieves (i) historical responses from the target\ncommunity to capture their ideological, semantic, and emotional makeup, and\n(ii) external knowledge from sources such as news articles to inject\ntime-sensitive context. This information is then jointly used to forecast the\nresponses of the target community to new posts or narratives. Extensive\nexperiments across six scenarios on the X platform (formerly Twitter), tested\nwith various embedding models and LLMs, demonstrate over 10% improvements on\naverage in key evaluation metrics. A concrete example further shows its\neffectiveness in capturing diverse ideologies and nuances. Our work provides a\nsocial computing tool for applications where accurate and concrete insights\ninto community responses are crucial."}
{"id": "2504.17160", "pdf": "https://arxiv.org/pdf/2504.17160", "abs": "https://arxiv.org/abs/2504.17160", "authors": ["Alberto FernÃ¡ndez-HernÃ¡ndez", "Jose I. Mestre", "Manuel F. Dolz", "Jose Duato", "Enrique S. Quintana-OrtÃ­"], "title": "OUI Need to Talk About Weight Decay: A New Perspective on Overfitting Detection", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "10 pages, 3 figures", "summary": "We introduce the Overfitting-Underfitting Indicator (OUI), a novel tool for\nmonitoring the training dynamics of Deep Neural Networks (DNNs) and identifying\noptimal regularization hyperparameters. Specifically, we validate that OUI can\neffectively guide the selection of the Weight Decay (WD) hyperparameter by\nindicating whether a model is overfitting or underfitting during training\nwithout requiring validation data. Through experiments on DenseNet-BC-100 with\nCIFAR- 100, EfficientNet-B0 with TinyImageNet and ResNet-34 with ImageNet-1K,\nwe show that maintaining OUI within a prescribed interval correlates strongly\nwith improved generalization and validation scores. Notably, OUI converges\nsignificantly faster than traditional metrics such as loss or accuracy,\nenabling practitioners to identify optimal WD (hyperparameter) values within\nthe early stages of training. By leveraging OUI as a reliable indicator, we can\ndetermine early in training whether the chosen WD value leads the model to\nunderfit the training data, overfit, or strike a well-balanced trade-off that\nmaximizes validation scores. This enables more precise WD tuning for optimal\nperformance on the tested datasets and DNNs. All code for reproducing these\nexperiments is available at https://github.com/AlbertoFdezHdez/OUI."}
{"id": "2504.17360", "pdf": "https://arxiv.org/pdf/2504.17360", "abs": "https://arxiv.org/abs/2504.17360", "authors": ["Jose G. Moreno", "Jesus Lovon", "M'Rick Robin-Charlet", "Christine Damase-Michel", "Lynda Tamine"], "title": "PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning of Large Language Models (LLMs) has become the default practice\nfor improving model performance on a given task. However, performance\nimprovement comes at the cost of training on vast amounts of annotated data\nwhich could be sensitive leading to significant data privacy concerns. In\nparticular, the healthcare domain is one of the most sensitive domains exposed\nto data privacy issues. In this paper, we present PatientDx, a framework of\nmodel merging that allows the design of effective LLMs for health-predictive\ntasks without requiring fine-tuning nor adaptation on patient data. Our\nproposal is based on recently proposed techniques known as merging of LLMs and\naims to optimize a building block merging strategy. PatientDx uses a pivotal\nmodel adapted to numerical reasoning and tunes hyperparameters on examples\nbased on a performance metric but without training of the LLM on these data.\nExperiments using the mortality tasks of the MIMIC-IV dataset show improvements\nup to 7% in terms of AUROC when compared to initial models. Additionally, we\nconfirm that when compared to fine-tuned models, our proposal is less prone to\ndata leak problems without hurting performance. Finally, we qualitatively show\nthe capabilities of our proposal through a case study. Our best model is\npublicly available at https://huggingface.co/ Jgmorenof/mistral\\_merged\\_0\\_4."}
{"id": "2504.16948", "pdf": "https://arxiv.org/pdf/2504.16948", "abs": "https://arxiv.org/abs/2504.16948", "authors": ["Zhen Tan", "Huan Liu"], "title": "Intrinsic Barriers to Explaining Deep Foundation Models", "categories": ["cs.CY", "cs.AI", "cs.ET"], "comment": null, "summary": "Deep Foundation Models (DFMs) offer unprecedented capabilities but their\nincreasing complexity presents profound challenges to understanding their\ninternal workings-a critical need for ensuring trust, safety, and\naccountability. As we grapple with explaining these systems, a fundamental\nquestion emerges: Are the difficulties we face merely temporary hurdles,\nawaiting more sophisticated analytical techniques, or do they stem from\n\\emph{intrinsic barriers} deeply rooted in the nature of these large-scale\nmodels themselves? This paper delves into this critical question by examining\nthe fundamental characteristics of DFMs and scrutinizing the limitations\nencountered by current explainability methods when confronted with this\ninherent challenge. We probe the feasibility of achieving satisfactory\nexplanations and consider the implications for how we must approach the\nverification and governance of these powerful technologies."}
{"id": "2504.17196", "pdf": "https://arxiv.org/pdf/2504.17196", "abs": "https://arxiv.org/abs/2504.17196", "authors": ["Jiawen Hou", "Hao Wu"], "title": "A Double-Norm Aggregated Tensor Latent Factorization Model for Temporal-Aware Traffic Speed Imputation", "categories": ["cs.LG"], "comment": "11pages,3figures", "summary": "In intelligent transportation systems (ITS), traffic management departments\nrely on sensors, cameras, and GPS devices to collect real-time traffic data.\nTraffic speed data is often incomplete due to sensor failures, data\ntransmission delays, or occlusions, resulting in missing speed data in certain\nroad segments. Currently, tensor decomposition based methods are extensively\nutilized, they mostly rely on the $L_2$-norm to construct their learning\nobjectives, which leads to reduced robustness in the algorithms. To address\nthis, we propose Temporal-Aware Traffic Speed Imputation (TATSI), which\ncombines the $L_2$-norm and smooth $L_1$ (${SL}_1$)-norm in its loss function,\nthereby achieving both high accuracy and robust performance in imputing missing\ntime-varying traffic speed data. TATSI adopts a single latent factor-dependent,\nnonnegative, and multiplicative update (SLF-NMU) approach, which serves as an\nefficient solver for performing nonnegative latent factor analysis (LFA) on a\ntensor. Empirical studies on three real-world time-varying traffic speed\ndatasets demonstrate that, compared with state-of-the-art traffic speed\npredictors, TATSI more precisely captures temporal patterns, thereby yielding\nthe most accurate imputations for missing traffic speed data."}
{"id": "2504.17366", "pdf": "https://arxiv.org/pdf/2504.17366", "abs": "https://arxiv.org/abs/2504.17366", "authors": ["Yongxuan Wu", "Runyu Chen", "Peiyu Liu", "Hongjin Qian"], "title": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench."}
{"id": "2504.16968", "pdf": "https://arxiv.org/pdf/2504.16968", "abs": "https://arxiv.org/abs/2504.16968", "authors": ["Jun Wu", "Jiangtao Wen", "Yuxing Han"], "title": "Backslash: Rate Constrained Optimized Training of Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rapid advancement of large-language models (LLMs) has driven extensive\nresearch into parameter compression after training has been completed, yet\ncompression during the training phase remains largely unexplored. In this work,\nwe introduce Rate-Constrained Training (Backslash), a novel training-time\ncompression approach based on rate-distortion optimization (RDO). Backslash\nenables a flexible trade-off between model accuracy and complexity,\nsignificantly reducing parameter redundancy while preserving performance.\nExperiments in various architectures and tasks demonstrate that Backslash can\nreduce memory usage by 60\\% - 90\\% without accuracy loss and provides\nsignificant compression gain compared to compression after training. Moreover,\nBackslash proves to be highly versatile: it enhances generalization with small\nLagrange multipliers, improves model robustness to pruning (maintaining\naccuracy even at 80\\% pruning rates), and enables network simplification for\naccelerated inference on edge devices."}
{"id": "2504.17210", "pdf": "https://arxiv.org/pdf/2504.17210", "abs": "https://arxiv.org/abs/2504.17210", "authors": ["Junfei Wang", "Darshana Upadhyay", "Marzia Zaman", "Pirathayini Srikantha"], "title": "Synthetic Power Flow Data Generation Using Physics-Informed Denoising Diffusion Probabilistic Models", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to IEEE SmartGridComm Conference 2025", "summary": "Many data-driven modules in smart grid rely on access to high-quality power\nflow data; however, real-world data are often limited due to privacy and\noperational constraints. This paper presents a physics-informed generative\nframework based on Denoising Diffusion Probabilistic Models (DDPMs) for\nsynthesizing feasible power flow data. By incorporating auxiliary training and\nphysics-informed loss functions, the proposed method ensures that the generated\ndata exhibit both statistical fidelity and adherence to power system\nfeasibility. We evaluate the approach on the IEEE 14-bus and 30-bus benchmark\nsystems, demonstrating its ability to capture key distributional properties and\ngeneralize to out-of-distribution scenarios. Comparative results show that the\nproposed model outperforms three baseline models in terms of feasibility,\ndiversity, and accuracy of statistical features. This work highlights the\npotential of integrating generative modelling into data-driven power system\napplications."}
{"id": "2504.17390", "pdf": "https://arxiv.org/pdf/2504.17390", "abs": "https://arxiv.org/abs/2504.17390", "authors": ["Jihyun Lee", "Yejin Jeon", "Seungyeon Seo", "Gary Geunbae Lee"], "title": "PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona", "categories": ["cs.CL"], "comment": "Accepted in NAACL 2025 main", "summary": "Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests\nthrough natural language interactions, yet existing systems often produce\ngeneric, monotonic responses that lack individuality and fail to adapt to\nusers' personal attributes. To address this, we introduce PicPersona-TOD, a\nnovel dataset that incorporates user images as part of the persona, enabling\npersonalized responses tailored to user-specific factors such as age or\nemotional context. This is facilitated by first impressions, dialogue\npolicy-guided prompting, and the use of external knowledge to reduce\nhallucinations. Human evaluations confirm that our dataset enhances user\nexperience, with personalized responses contributing to a more engaging\ninteraction. Additionally, we introduce a new NLG model, Pictor, which not only\npersonalizes responses, but also demonstrates robust performance across unseen\ndomains https://github.com/JihyunLee1/PicPersona."}
{"id": "2504.16972", "pdf": "https://arxiv.org/pdf/2504.16972", "abs": "https://arxiv.org/abs/2504.16972", "authors": ["Hossein Ahmadi", "Sajjad Emdadi Mahdimahalleh", "Arman Farahat", "Banafsheh Saffari"], "title": "Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "comment": null, "summary": "The rapid growth of unlabeled time-series data in domains such as wireless\ncommunications, radar, biomedical engineering, and the Internet of Things (IoT)\nhas driven advancements in unsupervised learning. This review synthesizes\nrecent progress in applying autoencoders and vision transformers for\nunsupervised signal analysis, focusing on their architectures, applications,\nand emerging trends. We explore how these models enable feature extraction,\nanomaly detection, and classification across diverse signal types, including\nelectrocardiograms, radar waveforms, and IoT sensor data. The review highlights\nthe strengths of hybrid architectures and self-supervised learning, while\nidentifying challenges in interpretability, scalability, and domain\ngeneralization. By bridging methodological innovations and practical\napplications, this work offers a roadmap for developing robust, adaptive models\nfor signal intelligence."}
{"id": "2504.17219", "pdf": "https://arxiv.org/pdf/2504.17219", "abs": "https://arxiv.org/abs/2504.17219", "authors": ["Hyomin Lee", "Minseon Kim", "Sangwon Jang", "Jongheon Jeong", "Sung Ju Hwang"], "title": "Enhancing Variational Autoencoders with Smooth Robust Latent Encoding", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Under review", "summary": "Variational Autoencoders (VAEs) have played a key role in scaling up\ndiffusion-based generative models, as in Stable Diffusion, yet questions\nregarding their robustness remain largely underexplored. Although adversarial\ntraining has been an established technique for enhancing robustness in\npredictive models, it has been overlooked for generative models due to concerns\nabout potential fidelity degradation by the nature of trade-offs between\nperformance and robustness. In this work, we challenge this presumption,\nintroducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training\nframework that boosts both generation quality and robustness. In contrast to\nconventional adversarial training, which focuses on robustness only, our\napproach smooths the latent space via adversarial perturbations, promoting more\ngeneralizable representations while regularizing with originality\nrepresentation to sustain original fidelity. Applied as a post-training step on\npre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal\ncomputational overhead. Experiments show that SRL-VAE improves both generation\nquality, in image reconstruction and text-guided image editing, and robustness,\nagainst Nightshade attacks and image editing attacks. These results establish a\nnew paradigm, showing that adversarial training, once thought to be detrimental\nto generative models, can instead enhance both fidelity and robustness."}
{"id": "2504.17445", "pdf": "https://arxiv.org/pdf/2504.17445", "abs": "https://arxiv.org/abs/2504.17445", "authors": ["Anna Lieb", "Maneesh Arora", "Eni Mustafaraj"], "title": "Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation", "categories": ["cs.CL"], "comment": "Presented at IC2S2 2024 in Philadelphia, USA", "summary": "Unsupervised machine learning techniques, such as topic modeling and\nclustering, are often used to identify latent patterns in unstructured text\ndata in fields such as political science and sociology. These methods overcome\ncommon concerns about reproducibility and costliness involved in the\nlabor-intensive process of human qualitative analysis. However, two major\nlimitations of topic models are their interpretability and their practicality\nfor answering targeted, domain-specific social science research questions. In\nthis work, we investigate opportunities for using LLM-generated text\naugmentation to improve the usefulness of topic modeling output. We use a\npolitical science case study to evaluate our results in a domain-specific\napplication, and find that topic modeling using GPT-4 augmentations creates\nhighly interpretable categories that can be used to investigate domain-specific\nresearch questions with minimal human guidance."}
{"id": "2504.16977", "pdf": "https://arxiv.org/pdf/2504.16977", "abs": "https://arxiv.org/abs/2504.16977", "authors": ["Priyaranjan Pattnayak", "Hitesh Laxmichand Patel", "Amit Agarwal"], "title": "Tokenization Matters: Improving Zero-Shot NER for Indic Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization is a critical component of Natural Language Processing (NLP),\nespecially for low resource languages, where subword segmentation influences\nvocabulary structure and downstream task accuracy. Although Byte Pair Encoding\n(BPE) is a standard tokenization method in multilingual language models, its\nsuitability for Named Entity Recognition (NER) in low resource Indic languages\nremains underexplored due to its limitations in handling morphological\ncomplexity. In this work, we systematically compare BPE, SentencePiece, and\nCharacter Level tokenization strategies using IndicBERT for NER tasks in low\nresource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as\nextremely low resource Indic languages like Santali, Manipuri, and Sindhi. We\nassess both intrinsic linguistic properties tokenization efficiency, out of\nvocabulary (OOV) rates, and morphological preservation as well as extrinsic\ndownstream performance, including fine tuning and zero shot cross lingual\ntransfer.\n  Our experiments show that SentencePiece is a consistently better performing\napproach than BPE for NER in low resource Indic Languages, particularly in zero\nshot cross lingual settings, as it better preserves entity consistency. While\nBPE provides the most compact tokenization form, it is not capable of\ngeneralization because it misclassifies or even fails to recognize entity\nlabels when tested on unseen languages. In contrast, SentencePiece constitutes\na better linguistic structural preservation model, benefiting extremely low\nresource and morphologically rich Indic languages, such as Santali and\nManipuri, for superior entity recognition, as well as high generalization\nacross scripts, such as Sindhi, written in Arabic. The results point to\nSentencePiece as the more effective tokenization strategy for NER within\nmultilingual and low resource Indic NLP applications."}
{"id": "2504.17232", "pdf": "https://arxiv.org/pdf/2504.17232", "abs": "https://arxiv.org/abs/2504.17232", "authors": ["Nivedita M", "Yasmeen Shajitha S"], "title": "Multi-Modal Traffic Analysis: Integrating Time-Series Forecasting, Accident Prediction, and Image Classification", "categories": ["cs.LG"], "comment": "5 pages,10 figures", "summary": "This study proposes an integrated machine learning framework for advanced\ntraffic analysis, combining time-series forecasting, classification, and\ncomputer vision techniques. The system utilizes an ARIMA(2,0,1) model for\ntraffic prediction (MAE: 2.1), an XGBoost classifier for accident severity\nclassification (100% accuracy on balanced data), and a Convolutional Neural\nNetwork (CNN) for traffic image classification (92% accuracy). Tested on\ndiverse datasets, the framework outperforms baseline models and identifies key\nfactors influencing accident severity, including weather and road\ninfrastructure. Its modular design supports deployment in smart city systems\nfor real-time monitoring, accident prevention, and resource optimization,\ncontributing to the evolution of intelligent transportation systems."}
{"id": "2504.17480", "pdf": "https://arxiv.org/pdf/2504.17480", "abs": "https://arxiv.org/abs/2504.17480", "authors": ["Xin Yi", "Shunfan Zhengc", "Linlin Wanga", "Xiaoling Wang", "Liang He"], "title": "Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable."}
{"id": "2504.16979", "pdf": "https://arxiv.org/pdf/2504.16979", "abs": "https://arxiv.org/abs/2504.16979", "authors": ["Masoud Tafavvoghi", "Lars Ailo Bongo", "AndrÃ© Berli Delgado", "Nikita Shvetsov", "Anders Sildnes", "Line Moi", "Lill-Tove Rasmussen Busund", "Kajsa MÃ¸llersen"], "title": "Automating tumor-infiltrating lymphocyte assessment in breast cancer histopathology images using QuPath: a transparent and accessible machine learning pipeline", "categories": ["q-bio.QM", "cs.AI", "cs.CV"], "comment": "16 Pages, 9 Figures, 3 tables", "summary": "In this study, we built an end-to-end tumor-infiltrating lymphocytes (TILs)\nassessment pipeline within QuPath, demonstrating the potential of easily\naccessible tools to perform complex tasks in a fully automatic fashion. First,\nwe trained a pixel classifier to segment tumor, tumor-associated stroma, and\nother tissue compartments in breast cancer H&E-stained whole-slide images (WSI)\nto isolate tumor-associated stroma for subsequent analysis. Next, we applied a\npre-trained StarDist deep learning model in QuPath for cell detection and used\nthe extracted cell features to train a binary classifier distinguishing TILs\nfrom other cells. To evaluate our TILs assessment pipeline, we calculated the\nTIL density in each WSI and categorized them as low, medium, or high TIL\nlevels. Our pipeline was evaluated against pathologist-assigned TIL scores,\nachieving a Cohen's kappa of 0.71 on the external test set, corroborating\nprevious research findings. These results confirm that existing software can\noffer a practical solution for the assessment of TILs in H&E-stained WSIs of\nbreast cancer."}
{"id": "2504.17243", "pdf": "https://arxiv.org/pdf/2504.17243", "abs": "https://arxiv.org/abs/2504.17243", "authors": ["Xinyu Zhou", "Simin Fan", "Martin Jaggi", "Jie Fu"], "title": "NeuralGrok: Accelerate Grokking by Neural Gradient Transformation", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint, 16 pages", "summary": "Grokking is proposed and widely studied as an intricate phenomenon in which\ngeneralization is achieved after a long-lasting period of overfitting. In this\nwork, we propose NeuralGrok, a novel gradient-based approach that learns an\noptimal gradient transformation to accelerate the generalization of\ntransformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary\nmodule (e.g., an MLP block) in conjunction with the base model. This module\ndynamically modulates the influence of individual gradient components based on\ntheir contribution to generalization, guided by a bilevel optimization\nalgorithm. Our extensive experiments demonstrate that NeuralGrok significantly\naccelerates generalization, particularly in challenging arithmetic tasks. We\nalso show that NeuralGrok promotes a more stable training paradigm, constantly\nreducing the model's complexity, while traditional regularization methods, such\nas weight decay, can introduce substantial instability and impede\ngeneralization. We further investigate the intrinsic model complexity\nleveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that\nNeuralGrok effectively facilitates generalization by reducing the model\ncomplexity. We offer valuable insights on the grokking phenomenon of\nTransformer models, which encourages a deeper understanding of the fundamental\nprinciples governing generalization ability."}
{"id": "2504.17550", "pdf": "https://arxiv.org/pdf/2504.17550", "abs": "https://arxiv.org/abs/2504.17550", "authors": ["Yejin Bang", "Ziwei Ji", "Alan Schelten", "Anthony Hartshorn", "Tara Fowler", "Cheng Zhang", "Nicola Cancedda", "Pascale Fung"], "title": "HalluLens: LLM Hallucination Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": "42 pages", "summary": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations."}
{"id": "2504.17004", "pdf": "https://arxiv.org/pdf/2504.17004", "abs": "https://arxiv.org/abs/2504.17004", "authors": ["Amin Karbasi", "Omar Montasser", "John Sous", "Grigoris Velegkas"], "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment."}
{"id": "2504.17247", "pdf": "https://arxiv.org/pdf/2504.17247", "abs": "https://arxiv.org/abs/2504.17247", "authors": ["Diogo Soares", "Leon Hetzel", "Paulina Szymczak", "Fabian Theis", "Stephan GÃ¼nnemann", "Ewa Szczurek"], "title": "Targeted AMP generation through controlled diffusion with efficient embeddings", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": null, "summary": "Deep learning-based antimicrobial peptide (AMP) discovery faces critical\nchallenges such as low experimental hit rates as well as the need for nuanced\ncontrollability and efficient modeling of peptide properties. To address these\nchallenges, we introduce OmegAMP, a framework that leverages a diffusion-based\ngenerative model with efficient low-dimensional embeddings, precise\ncontrollability mechanisms, and novel classifiers with drastically reduced\nfalse positive rates for candidate filtering. OmegAMP enables the targeted\ngeneration of AMPs with specific physicochemical properties, activity profiles,\nand species-specific effectiveness. Moreover, it maximizes sample diversity\nwhile ensuring faithfulness to the underlying data distribution during\ngeneration. We demonstrate that OmegAMP achieves state-of-the-art performance\nacross all stages of the AMP discovery pipeline, significantly advancing the\npotential of computational frameworks in combating antimicrobial resistance."}
{"id": "2504.17562", "pdf": "https://arxiv.org/pdf/2504.17562", "abs": "https://arxiv.org/abs/2504.17562", "authors": ["Rei Higuchi", "Ryotaro Kawata", "Naoki Nishikawa", "Kazusato Oko", "Shoichiro Yamaguchi", "Sosuke Kobayashi", "Seiya Tokui", "Kohei Hayashi", "Daisuke Okanohara", "Taiji Suzuki"], "title": "When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The ability to acquire latent semantics is one of the key properties that\ndetermines the performance of language models. One convenient approach to\ninvoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at\nthe beginning of texts in the pre-training data, making it easier for the model\nto access latent semantics before observing the entire text. Previous studies\nhave reported that this technique actually improves the performance of trained\nmodels in downstream tasks; however, this improvement has been observed only in\nspecific downstream tasks, without consistent enhancement in average next-token\nprediction loss. To understand this phenomenon, we closely investigate how\nprepending metadata during pre-training affects model performance by examining\nits behavior using artificial data. Interestingly, we found that this approach\nproduces both positive and negative effects on the downstream tasks. We\ndemonstrate that the effectiveness of the approach depends on whether latent\nsemantics can be inferred from the downstream task's prompt. Specifically,\nthrough investigations using data generated by probabilistic context-free\ngrammars, we show that training with metadata helps improve model's performance\nwhen the given context is long enough to infer the latent semantics. In\ncontrast, the technique negatively impacts performance when the context lacks\nthe necessary information to make an accurate posterior inference."}
{"id": "2504.17020", "pdf": "https://arxiv.org/pdf/2504.17020", "abs": "https://arxiv.org/abs/2504.17020", "authors": ["Kasper Engelen", "Guillermo A. PÃ©rez", "Shrisha Rao"], "title": "Analyzing Value Functions of States in Parametric Markov Chains", "categories": ["cs.LO", "cs.AI"], "comment": "Published as part of the book \"Principles of Verification: Cycling\n  the Probabilistic Landscape: Essays Dedicated to Joost-Pieter Katoen on the\n  Occasion of His 60th Birthday, Part II\"", "summary": "Parametric Markov chains (pMC) are used to model probabilistic systems with\nunknown or partially known probabilities. Although (universal) pMC verification\nfor reachability properties is known to be coETR-complete, there have been\nefforts to approach it using potentially easier-to-check properties such as\nasking whether the pMC is monotonic in certain parameters. In this paper, we\nfirst reduce monotonicity to asking whether the reachability probability from a\ngiven state is never less than that of another given state. Recent results for\nthe latter property imply an efficient algorithm to collapse same-value\nequivalence classes, which in turn preserves verification results and\nmonotonicity. We implement our algorithm to collapse \"trivial\" equivalence\nclasses in the pMC and show empirical evidence for the following: First, the\ncollapse gives reductions in size for some existing benchmarks and significant\nreductions on some custom benchmarks; Second, the collapse speeds up existing\nalgorithms to check monotonicity and parameter lifting, and hence can be used\nas a fast pre-processing step in practice."}
{"id": "2504.17258", "pdf": "https://arxiv.org/pdf/2504.17258", "abs": "https://arxiv.org/abs/2504.17258", "authors": ["Md Ashiqur Rahman", "Raymond A. Yeh"], "title": "Group Downsampling with Equivariant Anti-aliasing", "categories": ["cs.LG", "cs.CV", "math.GR"], "comment": null, "summary": "Downsampling layers are crucial building blocks in CNN architectures, which\nhelp to increase the receptive field for learning high-level features and\nreduce the amount of memory/computation in the model. In this work, we study\nthe generalization of the uniform downsampling layer for group equivariant\narchitectures, e.g., G-CNNs. That is, we aim to downsample signals (feature\nmaps) on general finite groups with anti-aliasing. This involves the following:\n(a) Given a finite group and a downsampling rate, we present an algorithm to\nform a suitable choice of subgroup. (b) Given a group and a subgroup, we study\nthe notion of bandlimited-ness and propose how to perform anti-aliasing.\nNotably, our method generalizes the notion of downsampling based on classical\nsampling theory. When the signal is on a cyclic group, i.e., periodic, our\nmethod recovers the standard downsampling of an ideal low-pass filter followed\nby a subsampling operation. Finally, we conducted experiments on image\nclassification tasks demonstrating that the proposed downsampling operation\nimproves accuracy, better preserves equivariance, and reduces model size when\nincorporated into G-equivariant networks"}
{"id": "2504.17565", "pdf": "https://arxiv.org/pdf/2504.17565", "abs": "https://arxiv.org/abs/2504.17565", "authors": ["Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Yunjie Ji", "Han Zhao", "Xiangang Li"], "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training", "categories": ["cs.CL"], "comment": null, "summary": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M"}
{"id": "2504.17023", "pdf": "https://arxiv.org/pdf/2504.17023", "abs": "https://arxiv.org/abs/2504.17023", "authors": ["Felix Kares", "Timo Speith", "Hanwei Zhang", "Markus Langer"], "title": "What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency Maps in Explainable AI (XAI)", "categories": ["cs.HC", "cs.AI"], "comment": "27 pages, 7 figures, 4 tables", "summary": "Saliency maps are a popular approach for explaining classifications of\n(convolutional) neural networks. However, it remains an open question as to how\nbest to evaluate salience maps, with three families of evaluation methods\ncommonly being used: subjective user measures, objective user measures, and\nmathematical metrics. We examine three of the most popular saliency map\napproaches (viz., LIME, Grad-CAM, and Guided Backpropagation) in a between\nsubject study (N=166) across these families of evaluation methods. We test 1)\nfor subjective measures, if the maps differ with respect to user trust and\nsatisfaction; 2) for objective measures, if the maps increase users' abilities\nand thus understanding of a model; 3) for mathematical metrics, which map\nachieves the best ratings across metrics; and 4) whether the mathematical\nmetrics can be associated with objective user measures. To our knowledge, our\nstudy is the first to compare several salience maps across all these evaluation\nmethods$-$with the finding that they do not agree in their assessment (i.e.,\nthere was no difference concerning trust and satisfaction, Grad-CAM improved\nusers' abilities best, and Guided Backpropagation had the most favorable\nmathematical metrics). Additionally, we show that some mathematical metrics\nwere associated with user understanding, although this relationship was often\ncounterintuitive. We discuss these findings in light of general debates\nconcerning the complementary use of user studies and mathematical metrics in\nthe evaluation of explainable AI (XAI) approaches."}
{"id": "2504.17261", "pdf": "https://arxiv.org/pdf/2504.17261", "abs": "https://arxiv.org/abs/2504.17261", "authors": ["Jiaqi Chen", "Xiaoye Zhu", "Yue Wang", "Tianyang Liu", "Xinhui Chen", "Ying Chen", "Chak Tou Leong", "Yifei Ke", "Joseph Liu", "Yiwen Yuan", "Julian McAuley", "Li-jia Li"], "title": "Symbolic Representation for Any-to-Any Generative Tasks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose a symbolic generative task description language and a\ncorresponding inference engine capable of representing arbitrary multimodal\ntasks as structured symbolic flows. Unlike conventional generative models that\nrely on large-scale training and implicit neural representations to learn\ncross-modal mappings, often at high computational cost and with limited\nflexibility, our framework introduces an explicit symbolic representation\ncomprising three core primitives: functions, parameters, and topological logic.\nLeveraging a pre-trained language model, our inference engine maps natural\nlanguage instructions directly to symbolic workflows in a training-free manner.\nOur framework successfully performs over 12 diverse multimodal generative\ntasks, demonstrating strong performance and flexibility without the need for\ntask-specific tuning. Experiments show that our method not only matches or\noutperforms existing state-of-the-art unified models in content quality, but\nalso offers greater efficiency, editability, and interruptibility. We believe\nthat symbolic task representations provide a cost-effective and extensible\nfoundation for advancing the capabilities of generative AI."}
{"id": "2504.17574", "pdf": "https://arxiv.org/pdf/2504.17574", "abs": "https://arxiv.org/abs/2504.17574", "authors": ["Zhenkai Qin", "Guifang Yang", "Dongze Wu"], "title": "RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "As false information continues to proliferate across social media platforms,\neffective rumor detection has emerged as a pressing challenge in natural\nlanguage processing. This paper proposes RAGAT-Mind, a multi-granular modeling\napproach for Chinese rumor detection, built upon the MindSpore deep learning\nframework. The model integrates TextCNN for local semantic extraction,\nbidirectional GRU for sequential context learning, Multi-Head Self-Attention\nfor global dependency focusing, and Bidirectional Graph Convolutional Networks\n(BiGCN) for structural representation of word co-occurrence graphs. Experiments\non the Weibo1-Rumor dataset demonstrate that RAGAT-Mind achieves superior\nclassification performance, attaining 99.2% accuracy and a macro-F1 score of\n0.9919. The results validate the effectiveness of combining hierarchical\nlinguistic features with graph-based semantic structures. Furthermore, the\nmodel exhibits strong generalization and interpretability, highlighting its\npractical value for real-world rumor detection applications."}
{"id": "2504.17028", "pdf": "https://arxiv.org/pdf/2504.17028", "abs": "https://arxiv.org/abs/2504.17028", "authors": ["Iman Khadir", "Shane Stevenson", "Henry Li", "Kyle Krick", "Abram Burrows", "David Hall", "Stan Posey", "Samuel S. P. Shen"], "title": "Democracy of AI Numerical Weather Models: An Example of Global Forecasting with FourCastNetv2 Made by a University Research Lab Using GPU", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": "12 pages, 8 figures", "summary": "This paper demonstrates the feasibility of democratizing AI-driven global\nweather forecasting models among university research groups by leveraging\nGraphics Processing Units (GPUs) and freely available AI models, such as\nNVIDIA's FourCastNetv2. FourCastNetv2 is an NVIDIA's advanced neural network\nfor weather prediction and is trained on a 73-channel subset of the European\nCentre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset\nat single levels and different pressure levels. Although the training\nspecifications for FourCastNetv2 are not released to the public, the training\ndocumentation of the model's first generation, FourCastNet, is available to all\nusers. The training had 64 A100 GPUs and took 16 hours to complete. Although\nNVIDIA's models offer significant reductions in both time and cost compared to\ntraditional Numerical Weather Prediction (NWP), reproducing published\nforecasting results presents ongoing challenges for resource-constrained\nuniversity research groups with limited GPU availability. We demonstrate both\n(i) leveraging FourCastNetv2 to create predictions through the designated\napplication programming interface (API) and (ii) utilizing NVIDIA hardware to\ntrain the original FourCastNet model. Further, this paper demonstrates the\ncapabilities and limitations of NVIDIA A100's for resource-limited research\ngroups in universities. We also explore data management, training efficiency,\nand model validation, highlighting the advantages and challenges of using\nlimited high-performance computing resources. Consequently, this paper and its\ncorresponding GitHub materials may serve as an initial guide for other\nuniversity research groups and courses related to machine learning, climate\nscience, and data science to develop research and education programs on AI\nweather forecasting, and hence help democratize the AI NWP in the digital\neconomy."}
{"id": "2504.17274", "pdf": "https://arxiv.org/pdf/2504.17274", "abs": "https://arxiv.org/abs/2504.17274", "authors": ["Siddharth Vishwanath", "Jonathan Hehir"], "title": "Signal Recovery from Random Dot-Product Graphs Under Local Differential Privacy", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH", "68P27, 62H22, 62C20, 62R07"], "comment": null, "summary": "We consider the problem of recovering latent information from graphs under\n$\\varepsilon$-edge local differential privacy where the presence of\nrelationships/edges between two users/vertices remains confidential, even from\nthe data curator. For the class of generalized random dot-product graphs, we\nshow that a standard local differential privacy mechanism induces a specific\ngeometric distortion in the latent positions. Leveraging this insight, we show\nthat consistent recovery of the latent positions is achievable by appropriately\nadjusting the statistical inference procedure for the privatized graph.\nFurthermore, we prove that our procedure is nearly minimax-optimal under local\nedge differential privacy constraints. Lastly, we show that this framework\nallows for consistent recovery of geometric and topological information\nunderlying the latent positions, as encoded in their persistence diagrams. Our\nresults extend previous work from the private community detection literature to\na substantially richer class of models and inferential tasks."}
{"id": "2504.17653", "pdf": "https://arxiv.org/pdf/2504.17653", "abs": "https://arxiv.org/abs/2504.17653", "authors": ["Samaneh Hosseini Moghaddam", "Kelly Lyons", "Cheryl Regehr", "Vivek Goel", "Kaitlyn Regehr"], "title": "Towards a comprehensive taxonomy of online abusive language informed by machine leaning", "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of abusive language in online communications has posed\nsignificant risks to the health and wellbeing of individuals and communities.\nThe growing concern regarding online abuse and its consequences necessitates\nmethods for identifying and mitigating harmful content and facilitating\ncontinuous monitoring, moderation, and early intervention. This paper presents\na taxonomy for distinguishing key characteristics of abusive language within\nonline text. Our approach uses a systematic method for taxonomy development,\nintegrating classification systems of 18 existing multi-label datasets to\ncapture key characteristics relevant to online abusive language classification.\nThe resulting taxonomy is hierarchical and faceted, comprising 5 categories and\n17 dimensions. It classifies various facets of online abuse, including context,\ntarget, intensity, directness, and theme of abuse. This shared understanding\ncan lead to more cohesive efforts, facilitate knowledge exchange, and\naccelerate progress in the field of online abuse detection and mitigation among\nresearchers, policy makers, online platform owners, and other stakeholders."}
{"id": "2504.17029", "pdf": "https://arxiv.org/pdf/2504.17029", "abs": "https://arxiv.org/abs/2504.17029", "authors": ["Jeffrey Smith", "Taisei Fujii", "Jesse Craney", "Charles Gretton"], "title": "Fried Parameter Estimation from Single Wavefront Sensor Image with Artificial Neural Networks", "categories": ["astro-ph.IM", "cs.AI"], "comment": null, "summary": "Atmospheric turbulence degrades the quality of astronomical observations in\nground-based telescopes, leading to distorted and blurry images. Adaptive\nOptics (AO) systems are designed to counteract these effects, using atmospheric\nmeasurements captured by a wavefront sensor to make real-time corrections to\nthe incoming wavefront. The Fried parameter, r0, characterises the strength of\natmospheric turbulence and is an essential control parameter for optimising the\nperformance of AO systems and more recently sky profiling for Free Space\nOptical (FSO) communication channels. In this paper, we develop a novel\ndata-driven approach, adapting machine learning methods from computer vision\nfor Fried parameter estimation from a single Shack-Hartmann or pyramid\nwavefront sensor image. Using these data-driven methods, we present a detailed\nsimulation-based evaluation of our approach using the open-source COMPASS AO\nsimulation tool to evaluate both the Shack-Hartmann and pyramid wavefront\nsensors. Our evaluation is over a range of guide star magnitudes, and realistic\nnoise, atmospheric and instrument conditions. Remarkably, we are able to\ndevelop a single network-based estimator that is accurate in both open and\nclosed-loop AO configurations. Our method accurately estimates the Fried\nparameter from a single WFS image directly from AO telemetry to a few\nmillimetres. Our approach is suitable for real time control, exhibiting 0.83ms\nr0 inference times on retail NVIDIA RTX 3090 GPU hardware, and thereby\ndemonstrating a compelling economic solution for use in real-time instrument\ncontrol."}
{"id": "2504.17276", "pdf": "https://arxiv.org/pdf/2504.17276", "abs": "https://arxiv.org/abs/2504.17276", "authors": ["Ke-Jia Chen", "Wenhui Mu", "Zheng Liu"], "title": "HeRB: Heterophily-Resolved Structure Balancer for Graph Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Recent research has witnessed the remarkable progress of Graph Neural\nNetworks (GNNs) in the realm of graph data representation. However, GNNs still\nencounter the challenge of structural imbalance. Prior solutions to this\nproblem did not take graph heterophily into account, namely that connected\nnodes process distinct labels or features, thus resulting in a deficiency in\neffectiveness. Upon verifying the impact of heterophily on solving the\nstructural imbalance problem, we propose to rectify the heterophily first and\nthen transfer homophilic knowledge. To the end, we devise a method named HeRB\n(Heterophily-Resolved Structure Balancer) for GNNs. HeRB consists of two\ninnovative components: 1) A heterophily-lessening augmentation module which\nserves to reduce inter-class edges and increase intra-class edges; 2) A\nhomophilic knowledge transfer mechanism to convey homophilic information from\nhead nodes to tail nodes. Experimental results demonstrate that HeRB achieves\nsuperior performance on two homophilic and six heterophilic benchmark datasets,\nand the ablation studies further validate the efficacy of two proposed\ncomponents."}
{"id": "2504.17665", "pdf": "https://arxiv.org/pdf/2504.17665", "abs": "https://arxiv.org/abs/2504.17665", "authors": ["Zena Al-Khalili", "Nick Howell", "Dietrich Klakow"], "title": "Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics", "categories": ["cs.CL"], "comment": null, "summary": "Assisting LLMs with code generation improved their performance on\nmathematical reasoning tasks. However, the evaluation of code-assisted LLMs is\ngenerally restricted to execution correctness, lacking a rigorous evaluation of\ntheir generated programs. In this work, we bridge this gap by conducting an\nin-depth analysis of code-assisted LLMs' generated programs in response to math\nreasoning tasks. Our evaluation focuses on the extent to which LLMs ground\ntheir programs to math rules, and how that affects their end performance. For\nthis purpose, we assess the generations of five different LLMs, on two\ndifferent math datasets, both manually and automatically. Our results reveal\nthat the distribution of grounding depends on LLMs' capabilities and the\ndifficulty of math problems. Furthermore, mathematical grounding is more\neffective for closed-source models, while open-source models fail to employ\nmath rules in their solutions correctly. On MATH500, the percentage of grounded\nprograms decreased to half, while the ungrounded generations doubled in\ncomparison to ASDiv grade-school problems. Our work highlights the need for\nin-depth evaluation beyond execution accuracy metrics, toward a better\nunderstanding of code-assisted LLMs' capabilities and limits in the math\ndomain."}
{"id": "2504.17040", "pdf": "https://arxiv.org/pdf/2504.17040", "abs": "https://arxiv.org/abs/2504.17040", "authors": ["Zhenhailong Wang", "Senthil Purushwalkam", "Caiming Xiong", "Silvio Savarese", "Heng Ji", "Ran Xu"], "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present DyMU, an efficient, training-free framework that dynamically\nreduces the computational burden of vision-language models (VLMs) while\nmaintaining high task performance. Our approach comprises two key components.\nFirst, Dynamic Token Merging (DToMe) reduces the number of visual token\nembeddings by merging similar tokens based on image complexity, addressing the\ninherent inefficiency of fixed-length outputs in vision transformers. Second,\nVirtual Token Unmerging (VTU) simulates the expected token sequence for large\nlanguage models (LLMs) by efficiently reconstructing the attention dynamics of\na full sequence, thus preserving the downstream performance without additional\nfine-tuning. Unlike previous approaches, our method dynamically adapts token\ncompression to the content of the image and operates completely training-free,\nmaking it readily applicable to most state-of-the-art VLM architectures.\nExtensive experiments on image and video understanding tasks demonstrate that\nDyMU can reduce the average visual token count by 32%-85% while achieving\ncomparable performance to full-length models across diverse VLM architectures,\nincluding the recently popularized AnyRes-based visual encoders. Furthermore,\nthrough qualitative analyses, we demonstrate that DToMe effectively adapts\ntoken reduction based on image complexity and, unlike existing systems,\nprovides users more control over computational costs. Project page:\nhttps://mikewangwzhl.github.io/dymu/."}
{"id": "2504.17277", "pdf": "https://arxiv.org/pdf/2504.17277", "abs": "https://arxiv.org/abs/2504.17277", "authors": ["Zongliang Ji", "Andre Carlos Kajdacsy-Balla Amaral", "Anna Goldenberg", "Rahul G. Krishnan"], "title": "ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to the Conference on Health, Inference, and Learning (CHIL)\n  2025", "summary": "Ordering a minimal subset of lab tests for patients in the intensive care\nunit (ICU) can be challenging. Care teams must balance between ensuring the\navailability of the right information and reducing the clinical burden and\ncosts associated with each lab test order. Most in-patient settings experience\nfrequent over-ordering of lab tests, but are now aiming to reduce this burden\non both hospital resources and the environment. This paper develops a novel\nmethod that combines off-policy learning with privileged information to\nidentify the optimal set of ICU lab tests to order. Our approach, EXplainable\nOff-policy learning with Side Information for ICU blood Test Orders (ExOSITO)\ncreates an interpretable assistive tool for clinicians to order lab tests by\nconsidering both the observed and predicted future status of each patient. We\npose this problem as a causal bandit trained using offline data and a reward\nfunction derived from clinically-approved rules; we introduce a novel learning\nframework that integrates clinical knowledge with observational data to bridge\nthe gap between the optimal and logging policies. The learned policy function\nprovides interpretable clinical information and reduces costs without omitting\nany vital lab orders, outperforming both a physician's policy and prior\napproaches to this practical problem."}
{"id": "2504.17671", "pdf": "https://arxiv.org/pdf/2504.17671", "abs": "https://arxiv.org/abs/2504.17671", "authors": ["Yuanchang Ye", "Weiyan Wen"], "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making."}
{"id": "2504.17044", "pdf": "https://arxiv.org/pdf/2504.17044", "abs": "https://arxiv.org/abs/2504.17044", "authors": ["Dhari Gandhi", "Himanshu Joshi", "Lucas Hartman", "Shabnam Hassani"], "title": "Approaches to Responsible Governance of GenAI in Organizations", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid evolution of Generative AI (GenAI) has introduced unprecedented\nopportunities while presenting complex challenges around ethics,\naccountability, and societal impact. This paper draws on a literature review,\nestablished governance frameworks, and industry roundtable discussions to\nidentify core principles for integrating responsible GenAI governance into\ndiverse organizational structures. Our objective is to provide actionable\nrecommendations for a balanced, risk-based governance approach that enables\nboth innovation and oversight. Findings emphasize the need for adaptable risk\nassessment tools, continuous monitoring practices, and cross-sector\ncollaboration to establish trustworthy GenAI. These insights provide a\nstructured foundation and Responsible GenAI Guide (ResAI) for organizations to\nalign GenAI initiatives with ethical, legal, and operational best practices."}
{"id": "2504.17300", "pdf": "https://arxiv.org/pdf/2504.17300", "abs": "https://arxiv.org/abs/2504.17300", "authors": ["Wencong You", "Daniel Lowd"], "title": "The Ultimate Cookbook for Invisible Poison: Crafting Subtle Clean-Label Text Backdoors with Style Attributes", "categories": ["cs.LG"], "comment": "Accepted at SaTML 2025", "summary": "Backdoor attacks on text classifiers can cause them to predict a predefined\nlabel when a particular \"trigger\" is present. Prior attacks often rely on\ntriggers that are ungrammatical or otherwise unusual, leading to conspicuous\nattacks. As a result, human annotators, who play a critical role in curating\ntraining data in practice, can easily detect and filter out these unnatural\ntexts during manual inspection, reducing the risk of such attacks. We argue\nthat a key criterion for a successful attack is for text with and without\ntriggers to be indistinguishable to humans. However, prior work neither\ndirectly nor comprehensively evaluated attack subtlety and invisibility with\nhuman involvement. We bridge the gap by conducting thorough human evaluations\nto assess attack subtlety. We also propose \\emph{AttrBkd}, consisting of three\nrecipes for crafting subtle yet effective trigger attributes, such as\nextracting fine-grained attributes from existing baseline backdoor attacks. Our\nhuman evaluations find that AttrBkd with these baseline-derived attributes is\noften more effective (higher attack success rate) and more subtle (fewer\ninstances detected by humans) than the original baseline backdoor attacks,\ndemonstrating that backdoor attacks can bypass detection by being inconspicuous\nand appearing natural even upon close inspection, while still remaining\neffective. Our human annotation also provides information not captured by\nautomated metrics used in prior work, and demonstrates the misalignment of\nthese metrics with human judgment."}
{"id": "2504.17674", "pdf": "https://arxiv.org/pdf/2504.17674", "abs": "https://arxiv.org/abs/2504.17674", "authors": ["Jared Fernandez", "Clara Na", "Vashisth Tiwari", "Yonatan Bisk", "Sasha Luccioni", "Emma Strubell"], "title": "Energy Considerations of Large Language Model Inference and Efficiency Optimizations", "categories": ["cs.CL", "cs.LG"], "comment": "16 pages", "summary": "As large language models (LLMs) scale in size and adoption, their\ncomputational and environmental costs continue to rise. Prior benchmarking\nefforts have primarily focused on latency reduction in idealized settings,\noften overlooking the diverse real-world inference workloads that shape energy\nuse. In this work, we systematically analyze the energy implications of common\ninference efficiency optimizations across diverse Natural Language Processing\n(NLP) and generative Artificial Intelligence (AI) workloads, including\nconversational AI and code generation. We introduce a modeling approach that\napproximates real-world LLM workflows through a binning strategy for\ninput-output token distributions and batch size variations. Our empirical\nanalysis spans software frameworks, decoding strategies, GPU architectures,\nonline and offline serving settings, and model parallelism configurations. We\nshow that the effectiveness of inference optimizations is highly sensitive to\nworkload geometry, software stack, and hardware accelerators, demonstrating\nthat naive energy estimates based on FLOPs or theoretical GPU utilization\nsignificantly underestimate real-world energy consumption. Our findings reveal\nthat the proper application of relevant inference efficiency optimizations can\nreduce total energy use by up to 73% from unoptimized baselines. These insights\nprovide a foundation for sustainable LLM deployment and inform energy-efficient\ndesign strategies for future AI infrastructure."}
{"id": "2504.17055", "pdf": "https://arxiv.org/pdf/2504.17055", "abs": "https://arxiv.org/abs/2504.17055", "authors": ["Ayushi Agrawal", "Aditya Kondai", "Kavita Vemuri"], "title": "Psychological Effect of AI driven marketing tools for beauty/facial feature enhancement", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "AI-powered facial assessment tools are reshaping how individuals evaluate\nappearance and internalize social judgments. This study examines the\npsychological impact of such tools on self-objectification, self-esteem, and\nemotional responses, with attention to gender differences. Two samples used\ndistinct versions of a facial analysis tool: one overtly critical (N=75; M=22.9\nyears), and another more neutral (N=51; M=19.9 years). Participants completed\nvalidated self-objectification and self-esteem scales and custom items\nmeasuring emotion, digital/physical appearance enhancement (DAE, PAEE), and\nperceived social emotion (PSE). Results revealed consistent links between high\nself-objectification, low self-esteem, and increased appearance enhancement\nbehaviors across both versions. Despite softer framing, the newer tool still\nevoked negative emotional responses (U=1466.5, p=0.013), indicating implicit\nfeedback may reinforce appearance-related insecurities. Gender differences\nemerged in DAE (p=0.025) and PSE (p<0.001), with females more prone to digital\nenhancement and less likely to perceive emotional impact in others. These\nfindings reveal how AI tools may unintentionally reinforce and amplify existing\nsocial biases and underscore the critical need for responsible AI design and\ndevelopment. Future research will investigate how human ideologies embedded in\nthe training data of such tools shape their evaluative outputs, and how these,\nin turn, influence user attitudes and decisions."}
{"id": "2504.17305", "pdf": "https://arxiv.org/pdf/2504.17305", "abs": "https://arxiv.org/abs/2504.17305", "authors": ["Dinan Li", "Panagiotis Kakosimos", "Luca Peretti"], "title": "Machine learning-based condition monitoring of powertrains in modern electric drives", "categories": ["cs.LG"], "comment": "2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "The recent technological advances in digitalization have revolutionized the\nindustrial sector. Leveraging data analytics has now enabled the collection of\ndeep insights into the performance and, as a result, the optimization of\nassets. Industrial drives, for example, already accumulate all the necessary\ninformation to control electric machines. These signals include but are not\nlimited to currents, frequency, and temperature. Integrating machine learning\n(ML) models responsible for predicting the evolution of those directly\ncollected or implicitly derived parameters enhances the smartness of industrial\nsystems even further. In this article, data already residing in most modern\nelectric drives has been used to develop a data-driven thermal model of a power\nmodule. A test bench has been designed and used specifically for training and\nvalidating the thermal digital twin undergoing various static and dynamic\noperating profiles. Different approaches, from traditional linear models to\ndeep neural networks, have been implemented to emanate the best ML model for\nestimating the case temperature of a power module. Several evaluation metrics\nwere then used to assess the investigated methods' performance and\nimplementation in industrial embedded systems."}
{"id": "2504.17685", "pdf": "https://arxiv.org/pdf/2504.17685", "abs": "https://arxiv.org/abs/2504.17685", "authors": ["Haru-Tada Sato", "Fuka Matsuzaki", "Jun-ichiro Takahashi"], "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 2 figures", "summary": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach."}
{"id": "2504.17058", "pdf": "https://arxiv.org/pdf/2504.17058", "abs": "https://arxiv.org/abs/2504.17058", "authors": ["Rahul Vishwakarma"], "title": "Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The generation of high-quality synthetic data presents significant challenges\nin machine learning research, particularly regarding statistical fidelity and\nuncertainty quantification. Existing generative models produce compelling\nsynthetic samples but lack rigorous statistical guarantees about their relation\nto the underlying data distribution, limiting their applicability in critical\ndomains requiring robust error bounds. We address this fundamental limitation\nby presenting a novel framework that incorporates conformal prediction\nmethodologies into Generative Adversarial Networks (GANs). By integrating\nmultiple conformal prediction paradigms including Inductive Conformal\nPrediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction,\nand Venn-Abers Predictors, we establish distribution-free uncertainty\nquantification in generated samples. This approach, termed Conformalized GAN\n(cGAN), demonstrates enhanced calibration properties while maintaining the\ngenerative power of traditional GANs, producing synthetic data with provable\nstatistical guarantees. We provide rigorous mathematical proofs establishing\nfinite-sample validity guarantees and asymptotic efficiency properties,\nenabling the reliable application of synthetic data in high-stakes domains\nincluding healthcare, finance, and autonomous systems."}
{"id": "2504.17314", "pdf": "https://arxiv.org/pdf/2504.17314", "abs": "https://arxiv.org/abs/2504.17314", "authors": ["Miaoyun Zhao", "Qiang Zhang", "Chenrong Li"], "title": "Class-Conditional Distribution Balancing for Group Robust Classification", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Spurious correlations that lead models to correct predictions for the wrong\nreasons pose a critical challenge for robust real-world generalization.\nExisting research attributes this issue to group imbalance and addresses it by\nmaximizing group-balanced or worst-group accuracy, which heavily relies on\nexpensive bias annotations. A compromise approach involves predicting bias\ninformation using extensively pretrained foundation models, which requires\nlarge-scale data and becomes impractical for resource-limited rare domains. To\naddress these challenges, we offer a novel perspective by reframing the\nspurious correlations as imbalances or mismatches in class-conditional\ndistributions, and propose a simple yet effective robust learning method that\neliminates the need for both bias annotations and predictions. With the goal of\nreducing the mutual information between spurious factors and label information,\nour method leverages a sample reweighting strategy to achieve class-conditional\ndistribution balancing, which automatically highlights minority groups and\nclasses, effectively dismantling spurious correlations and producing a debiased\ndata distribution for classification. Extensive experiments and analysis\ndemonstrate that our approach consistently delivers state-of-the-art\nperformance, rivaling methods that rely on bias supervision."}
{"id": "2504.17704", "pdf": "https://arxiv.org/pdf/2504.17704", "abs": "https://arxiv.org/abs/2504.17704", "authors": ["Cheng Wang", "Yue Liu", "Baolong Li", "Duzhen Zhang", "Zhongzhi Li", "Junfeng Fang"], "title": "Safety in Large Reasoning Models: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks\nlike mathematics and coding, leveraging their advanced reasoning capabilities.\nNevertheless, as these capabilities progress, significant concerns regarding\ntheir vulnerabilities and safety have arisen, which can pose challenges to\ntheir deployment and application in real-world settings. This paper presents a\ncomprehensive survey of LRMs, meticulously exploring and summarizing the newly\nemerged safety risks, attacks, and defense strategies. By organizing these\nelements into a detailed taxonomy, this work aims to offer a clear and\nstructured understanding of the current safety landscape of LRMs, facilitating\nfuture research and development to enhance the security and reliability of\nthese powerful models."}
{"id": "2504.17069", "pdf": "https://arxiv.org/pdf/2504.17069", "abs": "https://arxiv.org/abs/2504.17069", "authors": ["Rishav Pramanik", "Antoine Poupon", "Juan A. Rodriguez", "Masih Aminbeidokhti", "David Vazquez", "Christopher Pal", "Zhaozheng Yin", "Marco Pedersoli"], "title": "Distilling semantically aware orders for autoregressive image generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autoregressive patch-based image generation has recently shown competitive\nresults in terms of image quality and scalability. It can also be easily\nintegrated and scaled within Vision-Language models. Nevertheless,\nautoregressive models require a defined order for patch generation. While a\nnatural order based on the dictation of the words makes sense for text\ngeneration, there is no inherent generation order that exists for image\ngeneration. Traditionally, a raster-scan order (from top-left to bottom-right)\nguides autoregressive image generation models. In this paper, we argue that\nthis order is suboptimal, as it fails to respect the causality of the image\ncontent: for instance, when conditioned on a visual description of a sunset, an\nautoregressive model may generate clouds before the sun, even though the color\nof clouds should depend on the color of the sun and not the inverse. In this\nwork, we show that first by training a model to generate patches in\nany-given-order, we can infer both the content and the location (order) of each\npatch during generation. Secondly, we use these extracted orders to finetune\nthe any-given-order model to produce better-quality images. Through our\nexperiments, we show on two datasets that this new generation method produces\nbetter images than the traditional raster-scan approach, with similar training\ncosts and no extra annotations."}
{"id": "2504.17355", "pdf": "https://arxiv.org/pdf/2504.17355", "abs": "https://arxiv.org/abs/2504.17355", "authors": ["Xiaohan Huang", "Dongjie Wang", "Zhiyuan Ning", "Ziyue Qiao", "Qingqing Long", "Haowei Zhu", "Yi Du", "Min Wu", "Yuanchun Zhou", "Meng Xiao"], "title": "Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, Keywords: Automated Feature Transformation, Tabular\n  Dataset, Reinforcement Learning", "summary": "Feature transformation methods aim to find an optimal mathematical\nfeature-feature crossing process that generates high-value features and\nimproves the performance of downstream machine learning tasks. Existing\nframeworks, though designed to mitigate manual costs, often treat feature\ntransformations as isolated operations, ignoring dynamic dependencies between\ntransformation steps. To address the limitations, we propose TCTO, a\ncollaborative multi-agent reinforcement learning framework that automates\nfeature engineering through graph-driven path optimization. The framework's\ncore innovation lies in an evolving interaction graph that models features as\nnodes and transformations as edges. Through graph pruning and backtracking, it\ndynamically eliminates low-impact edges, reduces redundant operations, and\nenhances exploration stability. This graph also provides full traceability to\nempower TCTO to reuse high-utility subgraphs from historical transformations.\nTo demonstrate the efficacy and adaptability of our approach, we conduct\ncomprehensive experiments and case studies, which show superior performance\nacross a range of datasets."}
{"id": "2504.17720", "pdf": "https://arxiv.org/pdf/2504.17720", "abs": "https://arxiv.org/abs/2504.17720", "authors": ["Vansh Gupta", "Sankalan Pal Chowdhury", "VilÃ©m Zouhar", "Donya Rooein", "Mrinmaya Sachan"], "title": "Multilingual Performance Biases of Large Language Models in Education", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment."}
{"id": "2504.17070", "pdf": "https://arxiv.org/pdf/2504.17070", "abs": "https://arxiv.org/abs/2504.17070", "authors": ["Mohaiminul Al Nahian", "Zainab Altaweel", "David Reitano", "Sabbir Ahmed", "Saumitra Lohokare", "Shiqi Zhang", "Adnan Siraj Rakin"], "title": "Robo-Troj: Attacking LLM-based Task Planners", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robots need task planning methods to achieve goals that require more than\nindividual actions. Recently, large language models (LLMs) have demonstrated\nimpressive performance in task planning. LLMs can generate a step-by-step\nsolution using a description of actions and the goal. Despite the successes in\nLLM-based task planning, there is limited research studying the security\naspects of those systems. In this paper, we develop Robo-Troj, the first\nmulti-trigger backdoor attack for LLM-based task planners, which is the main\ncontribution of this work. As a multi-trigger attack, Robo-Troj is trained to\naccommodate the diversity of robot application domains. For instance, one can\nuse unique trigger words, e.g., \"herical\", to activate a specific malicious\nbehavior, e.g., cutting hand on a kitchen robot. In addition, we develop an\noptimization method for selecting the trigger words that are most effective.\nThrough demonstrating the vulnerability of LLM-based planners, we aim to\npromote the development of secured robot systems."}
{"id": "2504.17370", "pdf": "https://arxiv.org/pdf/2504.17370", "abs": "https://arxiv.org/abs/2504.17370", "authors": ["Marco Carpentiero", "Virginia Bordignon", "Vincenzo Matta", "Ali H. Sayed"], "title": "Doubly Adaptive Social Learning", "categories": ["cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In social learning, a network of agents assigns probability scores (beliefs)\nto some hypotheses of interest, which rule the generation of local streaming\ndata observed by each agent. Belief formation takes place by means of an\niterative two-step procedure where: i) the agents update locally their beliefs\nby using some likelihood model; and ii) the updated beliefs are combined with\nthe beliefs of the neighboring agents, using a pooling rule. This procedure can\nfail to perform well in the presence of dynamic drifts, leading the agents to\nincorrect decision making. Here, we focus on the fully online setting where\nboth the true hypothesis and the likelihood models can change over time. We\npropose the doubly adaptive social learning ($\\text{A}^2\\text{SL}$) strategy,\nwhich infuses social learning with the necessary adaptation capabilities. This\ngoal is achieved by exploiting two adaptation stages: i) a stochastic gradient\ndescent update to learn and track the drifts in the decision model; ii) and an\nadaptive belief update to track the true hypothesis changing over time. These\nstages are controlled by two adaptation parameters that govern the evolution of\nthe error probability for each agent. We show that all agents learn\nconsistently for sufficiently small adaptation parameters, in the sense that\nthey ultimately place all their belief mass on the true hypothesis. In\nparticular, the probability of choosing the wrong hypothesis converges to\nvalues on the order of the adaptation parameters. The theoretical analysis is\nillustrated both on synthetic data and by applying the $\\text{A}^2\\text{SL}$\nstrategy to a social learning problem in the online setting using real data."}
{"id": "2504.17753", "pdf": "https://arxiv.org/pdf/2504.17753", "abs": "https://arxiv.org/abs/2504.17753", "authors": ["Anuja Tayal", "Devika Salunke", "Barbara Di Eugenio", "Paula Allen-Meares", "Eulalia Puig Abril", "Olga Garcia", "Carolyn Dickens", "Andrew Boyd"], "title": "Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT", "categories": ["cs.CL"], "comment": null, "summary": "Conversational assistants are becoming more and more popular, including in\nhealthcare, partly because of the availability and capabilities of Large\nLanguage Models. There is a need for controlled, probing evaluations with real\nstakeholders which can highlight advantages and disadvantages of more\ntraditional architectures and those based on generative AI. We present a\nwithin-group user study to compare two versions of a conversational assistant\nthat allows heart failure patients to ask about salt content in food. One\nversion of the system was developed in-house with a neurosymbolic architecture,\nand one is based on ChatGPT. The evaluation shows that the in-house system is\nmore accurate, completes more tasks and is less verbose than the one based on\nChatGPT; on the other hand, the one based on ChatGPT makes fewer speech errors\nand requires fewer clarifications to complete the task. Patients show no\npreference for one over the other."}
{"id": "2504.17077", "pdf": "https://arxiv.org/pdf/2504.17077", "abs": "https://arxiv.org/abs/2504.17077", "authors": ["Dongjin Seo", "Soobin Um", "Sangbin Lee", "Jong Chul Ye", "Haejun Chung"], "title": "Physics-guided and fabrication-aware inverse design of photonic devices using diffusion models", "categories": ["physics.optics", "cs.AI", "physics.comp-ph"], "comment": "25 pages, 7 Figures", "summary": "Designing free-form photonic devices is fundamentally challenging due to the\nvast number of possible geometries and the complex requirements of fabrication\nconstraints. Traditional inverse-design approaches--whether driven by human\nintuition, global optimization, or adjoint-based gradient methods--often\ninvolve intricate binarization and filtering steps, while recent deep learning\nstrategies demand prohibitively large numbers of simulations (10^5 to 10^6). To\novercome these limitations, we present AdjointDiffusion, a physics-guided\nframework that integrates adjoint sensitivity gradients into the sampling\nprocess of diffusion models. AdjointDiffusion begins by training a diffusion\nnetwork on a synthetic, fabrication-aware dataset of binary masks. During\ninference, we compute the adjoint gradient of a candidate structure and inject\nthis physics-based guidance at each denoising step, steering the generative\nprocess toward high figure-of-merit (FoM) solutions without additional\npost-processing. We demonstrate our method on two canonical photonic design\nproblems--a bent waveguide and a CMOS image sensor color router--and show that\nour method consistently outperforms state-of-the-art nonlinear optimizers (such\nas MMA and SLSQP) in both efficiency and manufacturability, while using orders\nof magnitude fewer simulations (approximately 2 x 10^2) than pure deep learning\napproaches (approximately 10^5 to 10^6). By eliminating complex binarization\nschedules and minimizing simulation overhead, AdjointDiffusion offers a\nstreamlined, simulation-efficient, and fabrication-aware pipeline for\nnext-generation photonic device design. Our open-source implementation is\navailable at https://github.com/dongjin-seo2020/AdjointDiffusion."}
{"id": "2504.17403", "pdf": "https://arxiv.org/pdf/2504.17403", "abs": "https://arxiv.org/abs/2504.17403", "authors": ["Hans Rosenberger", "Rodrigo Fischer", "Johanna S. FrÃ¶hlich", "Ali Bereyhi", "Ralf R. MÃ¼ller"], "title": "Coding for Computation: Efficient Compression of Neural Networks for Reconfigurable Hardware", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT"], "comment": "Accepted at the 2025 IEEE Statistical Signal Processing (SSP)\n  Workshop, Edinburgh", "summary": "As state of the art neural networks (NNs) continue to grow in size, their\nresource-efficient implementation becomes ever more important. In this paper,\nwe introduce a compression scheme that reduces the number of computations\nrequired for NN inference on reconfigurable hardware such as FPGAs. This is\nachieved by combining pruning via regularized training, weight sharing and\nlinear computation coding (LCC). Contrary to common NN compression techniques,\nwhere the objective is to reduce the memory used for storing the weights of the\nNNs, our approach is optimized to reduce the number of additions required for\ninference in a hardware-friendly manner. The proposed scheme achieves\ncompetitive performance for simple multilayer perceptrons, as well as for large\nscale deep NNs such as ResNet-34."}
{"id": "2504.17768", "pdf": "https://arxiv.org/pdf/2504.17768", "abs": "https://arxiv.org/abs/2504.17768", "authors": ["Piotr Nawrot", "Robert Li", "Renjie Huang", "Sebastian Ruder", "Kelly Marchisio", "Edoardo M. Ponti"], "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications."}
{"id": "2504.17114", "pdf": "https://arxiv.org/pdf/2504.17114", "abs": "https://arxiv.org/abs/2504.17114", "authors": ["Valentin Langer", "Kartikay Tehlan", "Thomas Wendler"], "title": "Anatomy-constrained modelling of image-derived input functions in dynamic PET using multi-organ segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.med-ph"], "comment": "The code is available under\n  https://github.com/tinolan/curve_fit_multi_idif", "summary": "Accurate kinetic analysis of [$^{18}$F]FDG distribution in dynamic positron\nemission tomography (PET) requires anatomically constrained modelling of\nimage-derived input functions (IDIFs). Traditionally, IDIFs are obtained from\nthe aorta, neglecting anatomical variations and complex vascular contributions.\nThis study proposes a multi-organ segmentation-based approach that integrates\nIDIFs from the aorta, portal vein, pulmonary artery, and ureters. Using\nhigh-resolution CT segmentations of the liver, lungs, kidneys, and bladder, we\nincorporate organ-specific blood supply sources to improve kinetic modelling.\nOur method was evaluated on dynamic [$^{18}$F]FDG PET data from nine patients,\nresulting in a mean squared error (MSE) reduction of $13.39\\%$ for the liver\nand $10.42\\%$ for the lungs. These initial results highlight the potential of\nmultiple IDIFs in improving anatomical modelling and fully leveraging dynamic\nPET imaging. This approach could facilitate the integration of tracer kinetic\nmodelling into clinical routine."}
{"id": "2504.17421", "pdf": "https://arxiv.org/pdf/2504.17421", "abs": "https://arxiv.org/abs/2504.17421", "authors": ["Yang Liu", "Bingjie Yan", "Tianyuan Zou", "Jianqing Zhang", "Zixuan Gu", "Jianbing Ding", "Xidong Wang", "Jingyi Li", "Xiaozhou Ye", "Ye Ouyang", "Qiang Yang", "Ya-Qin Zhang"], "title": "Towards Harnessing the Collaborative Power of Large and Small Models for Domain Tasks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nthey require vast amounts of data and computational resources. In contrast,\nsmaller models (SMs), while less powerful, can be more efficient and tailored\nto specific domains. In this position paper, we argue that taking a\ncollaborative approach, where large and small models work synergistically, can\naccelerate the adaptation of LLMs to private domains and unlock new potential\nin AI. We explore various strategies for model collaboration and identify\npotential challenges and opportunities. Building upon this, we advocate for\nindustry-driven research that prioritizes multi-objective benchmarks on\nreal-world private datasets and applications."}
{"id": "2504.16939", "pdf": "https://arxiv.org/pdf/2504.16939", "abs": "https://arxiv.org/abs/2504.16939", "authors": ["Emre Can Acikgoz", "Cheng Qian", "Hongru Wang", "Vardhan Dongre", "Xiusi Chen", "Heng Ji", "Dilek Hakkani-TÃ¼r", "Gokhan Tur"], "title": "A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have propelled conversational\nAI from traditional dialogue systems into sophisticated agents capable of\nautonomous actions, contextual awareness, and multi-turn interactions with\nusers. Yet, fundamental questions about their capabilities, limitations, and\npaths forward remain open. This survey paper presents a desideratum for\nnext-generation Conversational Agents - what has been achieved, what challenges\npersist, and what must be done for more scalable systems that approach\nhuman-level intelligence. To that end, we systematically analyze LLM-driven\nConversational Agents by organizing their capabilities into three primary\ndimensions: (i) Reasoning - logical, systematic thinking inspired by human\nintelligence for decision making, (ii) Monitor - encompassing self-awareness\nand user interaction monitoring, and (iii) Control - focusing on tool\nutilization and policy following. Building upon this, we introduce a novel\ntaxonomy by classifying recent work on Conversational Agents around our\nproposed desideratum. We identify critical research gaps and outline key\ndirections, including realistic evaluations, long-term multi-turn reasoning\nskills, self-evolution capabilities, collaborative and multi-agent task\ncompletion, personalization, and proactivity. This work aims to provide a\nstructured foundation, highlight existing limitations, and offer insights into\npotential future research directions for Conversational Agents, ultimately\nadvancing progress toward Artificial General Intelligence (AGI). We maintain a\ncurated repository of papers at:\nhttps://github.com/emrecanacikgoz/awesome-conversational-agents."}
{"id": "2504.17119", "pdf": "https://arxiv.org/pdf/2504.17119", "abs": "https://arxiv.org/abs/2504.17119", "authors": ["Muskan Garg", "Shaina Raza", "Shebuti Rayana", "Xingyi Liu", "Sunghwan Sohn"], "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey", "categories": ["cs.CL", "cs.AI"], "comment": "35 pages, 7 tables, 5 figures", "summary": "Despite substantial progress in healthcare applications driven by large\nlanguage models (LLMs), growing concerns around data privacy, and limited\nresources; the small language models (SLMs) offer a scalable and clinically\nviable solution for efficient performance in resource-constrained environments\nfor next-generation healthcare informatics. Our comprehensive survey presents a\ntaxonomic framework to identify and categorize them for healthcare\nprofessionals and informaticians. The timeline of healthcare SLM contributions\nestablishes a foundational framework for analyzing models across three\ndimensions: NLP tasks, stakeholder roles, and the continuum of care. We present\na taxonomic framework to identify the architectural foundations for building\nmodels from scratch; adapting SLMs to clinical precision through prompting,\ninstruction fine-tuning, and reasoning; and accessibility and sustainability\nthrough compression techniques. Our primary objective is to offer a\ncomprehensive survey for healthcare professionals, introducing recent\ninnovations in model optimization and equipping them with curated resources to\nsupport future research and development in the field. Aiming to showcase the\ngroundbreaking advancements in SLMs for healthcare, we present a comprehensive\ncompilation of experimental results across widely studied NLP tasks in\nhealthcare to highlight the transformative potential of SLMs in healthcare. The\nupdated repository is available at Github"}
{"id": "2504.17448", "pdf": "https://arxiv.org/pdf/2504.17448", "abs": "https://arxiv.org/abs/2504.17448", "authors": ["Jun Zhang", "Jue Wang", "Huan Li", "Zhongle Xie", "Ke Chen", "Lidan Shou"], "title": "CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated Active Learning", "categories": ["cs.LG", "cs.DB", "cs.DC"], "comment": "Accepted by TKDE 2025", "summary": "Active learning (AL) reduces human annotation costs for machine learning\nsystems by strategically selecting the most informative unlabeled data for\nannotation, but performing it individually may still be insufficient due to\nrestricted data diversity and annotation budget. Federated Active Learning\n(FAL) addresses this by facilitating collaborative data selection and model\ntraining, while preserving the confidentiality of raw data samples. Yet,\nexisting FAL methods fail to account for the heterogeneity of data distribution\nacross clients and the associated fluctuations in global and local model\nparameters, adversely affecting model accuracy. To overcome these challenges,\nwe propose CHASe (Client Heterogeneity-Aware Data Selection), specifically\ndesigned for FAL. CHASe focuses on identifying those unlabeled samples with\nhigh epistemic variations (EVs), which notably oscillate around the decision\nboundaries during training. To achieve both effectiveness and efficiency,\n\\model{} encompasses techniques for 1) tracking EVs by analyzing inference\ninconsistencies across training epochs, 2) calibrating decision boundaries of\ninaccurate models with a new alignment loss, and 3) enhancing data selection\nefficiency via a data freeze and awaken mechanism with subset sampling.\nExperiments show that CHASe surpasses various established baselines in terms of\neffectiveness and efficiency, validated across diverse datasets, model\ncomplexities, and heterogeneous federation settings."}
{"id": "2504.17004", "pdf": "https://arxiv.org/pdf/2504.17004", "abs": "https://arxiv.org/abs/2504.17004", "authors": ["Amin Karbasi", "Omar Montasser", "John Sous", "Grigoris Velegkas"], "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment."}
{"id": "2504.17122", "pdf": "https://arxiv.org/pdf/2504.17122", "abs": "https://arxiv.org/abs/2504.17122", "authors": ["Kartikay Tehlan", "Thomas Wendler"], "title": "Physiological neural representation for personalised tracer kinetic parameter estimation from dynamic PET", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "The code is available at: https://github.com/tkartikay/PhysNRPET", "summary": "Dynamic positron emission tomography (PET) with [$^{18}$F]FDG enables\nnon-invasive quantification of glucose metabolism through kinetic analysis,\noften modelled by the two-tissue compartment model (TCKM). However, voxel-wise\nkinetic parameter estimation using conventional methods is computationally\nintensive and limited by spatial resolution. Deep neural networks (DNNs) offer\nan alternative but require large training datasets and significant\ncomputational resources. To address these limitations, we propose a\nphysiological neural representation based on implicit neural representations\n(INRs) for personalized kinetic parameter estimation. INRs, which learn\ncontinuous functions, allow for efficient, high-resolution parametric imaging\nwith reduced data requirements. Our method also integrates anatomical priors\nfrom a 3D CT foundation model to enhance robustness and precision in kinetic\nmodelling. We evaluate our approach on an [$^{18}$F]FDG dynamic PET/CT dataset\nand compare it to state-of-the-art DNNs. Results demonstrate superior spatial\nresolution, lower mean-squared error, and improved anatomical consistency,\nparticularly in tumour and highly vascularized regions. Our findings highlight\nthe potential of INRs for personalized, data-efficient tracer kinetic\nmodelling, enabling applications in tumour characterization, segmentation, and\nprognostic assessment."}
{"id": "2504.17449", "pdf": "https://arxiv.org/pdf/2504.17449", "abs": "https://arxiv.org/abs/2504.17449", "authors": ["Jun Zhang", "Jue Wang", "Huan Li", "Lidan Shou", "Ke Chen", "Gang Chen", "Qin Xie", "Guiming Xie", "Xuejian Gong"], "title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by VLDBJ 2025", "summary": "The significant computational demands of pretrained language models (PLMs),\nwhich often require dedicated hardware, present a substantial challenge in\nserving them efficiently, especially in multi-tenant environments. To address\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\nInference system, designed to manage tenants with distinct PLMs\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\nknowledge into general, domain-specific, and task-specific. Leveraging insights\non knowledge acquisition across different model layers, we construct\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\nestablish hierarchical knowledge management for hPLMs generated by various\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\nincreases by constructing and updating domain-specific knowledge trees based on\nfrequency. We manage task-specific knowledge within limited GPU memory through\nparameter swapping. Finally, we propose system optimizations to enhance\nresource utilization and inference throughput. These include fine-grained\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\noperations with GPU computations, and optimizing parallel implementations with\nbatched matrix multiplications. Our experimental results demonstrate that the\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\nsingle GPU, with only a negligible compromise in accuracy."}
{"id": "2504.17038", "pdf": "https://arxiv.org/pdf/2504.17038", "abs": "https://arxiv.org/abs/2504.17038", "authors": ["Christian D. Newman", "Brandon Scholten", "Sophia Testa", "Joshua A. C. Behler", "Syreen Banabilah", "Michael L. Collard", "Michael J. Decker", "Mohamed Wiem Mkaouer", "Marcos Zampieri", "Eman Abdullah AlOmar", "Reem Alsuhaibani", "Anthony Peruma", "Jonathan I. Maletic"], "title": "SCALAR: A Part-of-speech Tagger for Identifiers", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "The paper presents the Source Code Analysis and Lexical Annotation Runtime\n(SCALAR), a tool specialized for mapping (annotating) source code identifier\nnames to their corresponding part-of-speech tag sequence (grammar pattern).\nSCALAR's internal model is trained using scikit-learn's\nGradientBoostingClassifier in conjunction with a manually-curated oracle of\nidentifier names and their grammar patterns. This specializes the tagger to\nrecognize the unique structure of the natural language used by developers to\ncreate all types of identifiers (e.g., function names, variable names etc.).\nSCALAR's output is compared with a previous version of the tagger, as well as a\nmodern off-the-shelf part-of-speech tagger to show how it improves upon other\ntaggers' output for annotating identifiers. The code is available on Github"}
{"id": "2504.17124", "pdf": "https://arxiv.org/pdf/2504.17124", "abs": "https://arxiv.org/abs/2504.17124", "authors": ["Ming Du", "Mark Wolfman", "Chengjun Sun", "Shelly D. Kelly", "Mathew J. Cherukara"], "title": "Demonstration of an AI-driven workflow for dynamic x-ray spectroscopy", "categories": ["physics.app-ph", "cs.AI", "cs.CE", "cs.SY", "eess.SY"], "comment": null, "summary": "X-ray absorption near edge structure (XANES) spectroscopy is a powerful\ntechnique for characterizing the chemical state and symmetry of individual\nelements within materials, but requires collecting data at many energy points\nwhich can be time-consuming. While adaptive sampling methods exist for\nefficiently collecting spectroscopic data, they often lack domain-specific\nknowledge about XANES spectra structure. Here we demonstrate a\nknowledge-injected Bayesian optimization approach for adaptive XANES data\ncollection that incorporates understanding of spectral features like absorption\nedges and pre-edge peaks. We show this method accurately reconstructs the\nabsorption edge of XANES spectra using only 15-20% of the measurement points\ntypically needed for conventional sampling, while maintaining the ability to\ndetermine the x-ray energy of the sharp peak after absorption edge with errors\nless than 0.03 eV, the absorption edge with errors less than 0.1 eV; and\noverall root-mean-square errors less than 0.005 compared to compared to\ntraditionally sampled spectra. Our experiments on battery materials and\ncatalysts demonstrate the method's effectiveness for both static and dynamic\nXANES measurements, improving data collection efficiency and enabling better\ntime resolution for tracking chemical changes. This approach advances the\ndegree of automation in XANES experiments reducing the common errors of under-\nor over-sampling points in near the absorption edge and enabling dynamic\nexperiments that require high temporal resolution or limited measurement time."}
{"id": "2504.17461", "pdf": "https://arxiv.org/pdf/2504.17461", "abs": "https://arxiv.org/abs/2504.17461", "authors": ["Vipin Singh", "Tianheng Ling", "Teodor Chiaburu", "Felix Biessmann"], "title": "Evaluating Time Series Models for Urban Wastewater Management: Predictive Performance, Model Complexity and Resilience", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, 6 figures, accepted at 10th International Conference on\n  Smart and Sustainable Technologies (SpliTech) 2025, GitHub:\n  https://github.com/calgo-lab/resilient-timeseries-evaluation", "summary": "Climate change increases the frequency of extreme rainfall, placing a\nsignificant strain on urban infrastructures, especially Combined Sewer Systems\n(CSS). Overflows from overburdened CSS release untreated wastewater into\nsurface waters, posing environmental and public health risks. Although\ntraditional physics-based models are effective, they are costly to maintain and\ndifficult to adapt to evolving system dynamics. Machine Learning (ML)\napproaches offer cost-efficient alternatives with greater adaptability. To\nsystematically assess the potential of ML for modeling urban infrastructure\nsystems, we propose a protocol for evaluating Neural Network architectures for\nCSS time series forecasting with respect to predictive performance, model\ncomplexity, and robustness to perturbations. In addition, we assess model\nperformance on peak events and critical fluctuations, as these are the key\nregimes for urban wastewater management. To investigate the feasibility of\nlightweight models suitable for IoT deployment, we compare global models, which\nhave access to all information, with local models, which rely solely on nearby\nsensor readings. Additionally, to explore the security risks posed by network\noutages or adversarial attacks on urban infrastructure, we introduce error\nmodels that assess the resilience of models. Our results demonstrate that while\nglobal models achieve higher predictive performance, local models provide\nsufficient resilience in decentralized scenarios, ensuring robust modeling of\nurban infrastructure. Furthermore, models with longer native forecast horizons\nexhibit greater robustness to data perturbations. These findings contribute to\nthe development of interpretable and reliable ML solutions for sustainable\nurban wastewater management. The implementation is available in our GitHub\nrepository."}
{"id": "2504.17365", "pdf": "https://arxiv.org/pdf/2504.17365", "abs": "https://arxiv.org/abs/2504.17365", "authors": ["Ling You", "Wenxuan Huang", "Xinni Xie", "Xiangyi Wei", "Bangyan Li", "Shaohui Lin", "Yang Li", "Changbo Wang"], "title": "TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Soccer is a globally popular sporting event, typically characterized by long\nmatches and distinctive highlight moments. Recent advances in Multimodal Large\nLanguage Models (MLLMs) offer promising capabilities in temporal grounding and\nvideo understanding, soccer commentary generation often requires precise\ntemporal localization and semantically rich descriptions over long-form video.\nHowever, existing soccer MLLMs often rely on the temporal a priori for caption\ngeneration, so they cannot process the soccer video end-to-end. While some\ntraditional approaches follow a two-step paradigm that is complex and fails to\ncapture the global context to achieve suboptimal performance. To solve the\nabove issues, we present TimeSoccer, the first end-to-end soccer MLLM for\nSingle-anchor Dense Video Captioning (SDVC) in full-match soccer videos.\nTimeSoccer jointly predicts timestamps and generates captions in a single pass,\nenabling global context modeling across 45-minute matches. To support long\nvideo understanding of soccer matches, we introduce MoFA-Select, a\ntraining-free, motion-aware frame compression module that adaptively selects\nrepresentative frames via a coarse-to-fine strategy, and incorporates\ncomplementary training paradigms to strengthen the model's ability to handle\nlong temporal sequences. Extensive experiments demonstrate that our TimeSoccer\nachieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end\nform, generating high-quality commentary with accurate temporal alignment and\nstrong semantic relevance."}
{"id": "2504.17129", "pdf": "https://arxiv.org/pdf/2504.17129", "abs": "https://arxiv.org/abs/2504.17129", "authors": ["Seyed Yousef Soltanian", "Wenlong Zhang"], "title": "Peer-Aware Cost Estimation in Nonlinear General-Sum Dynamic Games for Mutual Learning and Intent Inference", "categories": ["eess.SY", "cs.AI", "cs.GT", "cs.RO", "cs.SY", "93C41, 49N70, 49N90, 91A27"], "comment": null, "summary": "Human-robot interactions can be modeled as incomplete-information general-sum\ndynamic games since the objective functions of both agents are not explicitly\nknown to each other. However, solving for equilibrium policies for such games\npresents a major challenge, especially if the games involve nonlinear\nunderlying dynamics. To simplify the problem, existing work often assumes that\none agent is an expert with complete information about its peer, which can lead\nto biased estimates and failures in coordination. To address this challenge, we\npropose a nonlinear peer-aware cost estimation (N-PACE) algorithm for\ngeneral-sum dynamic games. In N-PACE, using iterative linear quadratic (LQ)\napproximation of the nonlinear general-sum game, each agent explicitly models\nthe learning dynamics of its peer agent while inferring their objective\nfunctions, leading to unbiased fast learning in inferring the unknown objective\nfunction of the peer agent, which is critical for task completion and safety\nassurance. Additionally, we demonstrate how N-PACE enables \\textbf{intent\ncommunication} in such multi-agent systems by explicitly modeling the peer's\nlearning dynamics."}
{"id": "2504.17471", "pdf": "https://arxiv.org/pdf/2504.17471", "abs": "https://arxiv.org/abs/2504.17471", "authors": ["Yacine Belal", "Mohamed Maouche", "Sonia Ben Mokhtar", "Anthony Simonet-Boulogne"], "title": "GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Gossip Learning (GL) is a decentralized learning paradigm where users\niteratively exchange and aggregate models with a small set of neighboring\npeers. Recent GL approaches rely on dynamic communication graphs built and\nmaintained using Random Peer Sampling (RPS) protocols. Thanks to graph\ndynamics, GL can achieve fast convergence even over extremely sparse\ntopologies. However, the robustness of GL over dy- namic graphs to Byzantine\n(model poisoning) attacks remains unaddressed especially when Byzantine nodes\nattack the RPS protocol to scale up model poisoning. We address this issue by\nintroducing GRANITE, a framework for robust learning over sparse, dynamic\ngraphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two\nkey components (i) a History-aware Byzantine-resilient Peer Sampling protocol\n(HaPS), which tracks previously encountered identifiers to reduce adversarial\ninfluence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which\nleverages an estimate of Byzantine presence to set aggregation thresholds with\nformal guarantees. Empirical results confirm that GRANITE maintains convergence\nwith up to 30% Byzantine nodes, improves learning speed via adaptive filtering\nof poisoned models and obtains these results in up to 9 times sparser graphs\nthan dictated by current theory."}
{"id": "2504.17449", "pdf": "https://arxiv.org/pdf/2504.17449", "abs": "https://arxiv.org/abs/2504.17449", "authors": ["Jun Zhang", "Jue Wang", "Huan Li", "Lidan Shou", "Ke Chen", "Gang Chen", "Qin Xie", "Guiming Xie", "Xuejian Gong"], "title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by VLDBJ 2025", "summary": "The significant computational demands of pretrained language models (PLMs),\nwhich often require dedicated hardware, present a substantial challenge in\nserving them efficiently, especially in multi-tenant environments. To address\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\nInference system, designed to manage tenants with distinct PLMs\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\nknowledge into general, domain-specific, and task-specific. Leveraging insights\non knowledge acquisition across different model layers, we construct\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\nestablish hierarchical knowledge management for hPLMs generated by various\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\nincreases by constructing and updating domain-specific knowledge trees based on\nfrequency. We manage task-specific knowledge within limited GPU memory through\nparameter swapping. Finally, we propose system optimizations to enhance\nresource utilization and inference throughput. These include fine-grained\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\noperations with GPU computations, and optimizing parallel implementations with\nbatched matrix multiplications. Our experimental results demonstrate that the\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\nsingle GPU, with only a negligible compromise in accuracy."}
{"id": "2504.17137", "pdf": "https://arxiv.org/pdf/2504.17137", "abs": "https://arxiv.org/abs/2504.17137", "authors": ["Chanhee Park", "Hyeonseok Moon", "Chanjun Park", "Heuiseok Lim"], "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL2025 Findings", "summary": "Retrieval-Augmented Generation (RAG) has gained prominence as an effective\nmethod for enhancing the generative capabilities of Large Language Models\n(LLMs) through the incorporation of external knowledge. However, the evaluation\nof RAG systems remains a challenge, due to the intricate interplay between\nretrieval and generation components. This limitation has resulted in a scarcity\nof benchmarks that facilitate a detailed, component-specific assessment. In\nthis work, we present MIRAGE, a Question Answering dataset specifically\ndesigned for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped\nto a retrieval pool of 37,800 entries, enabling an efficient and precise\nevaluation of both retrieval and generation tasks. We also introduce novel\nevaluation metrics aimed at measuring RAG adaptability, encompassing dimensions\nsuch as noise vulnerability, context acceptability, context insensitivity, and\ncontext misinterpretation. Through comprehensive experiments across various\nretriever-LLM configurations, we provide new insights into the optimal\nalignment of model pairs and the nuanced dynamics within RAG systems. The\ndataset and evaluation code are publicly available, allowing for seamless\nintegration and customization in diverse research settings\\footnote{The MIRAGE\ncode and data are available at https://github.com/nlpai-lab/MIRAGE."}
{"id": "2504.17490", "pdf": "https://arxiv.org/pdf/2504.17490", "abs": "https://arxiv.org/abs/2504.17490", "authors": ["Mingqi Yuan", "Qi Wang", "Guozheng Ma", "Bo Li", "Xin Jin", "Yunbo Wang", "Xiaokang Yang", "Wenjun Zeng", "Dacheng Tao"], "title": "Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages", "summary": "Developing lifelong learning agents is crucial for artificial general\nintelligence. However, deep reinforcement learning (RL) systems often suffer\nfrom plasticity loss, where neural networks gradually lose their ability to\nadapt during training. Despite its significance, this field lacks unified\nbenchmarks and evaluation protocols. We introduce Plasticine, the first\nopen-source framework for benchmarking plasticity optimization in deep RL.\nPlasticine provides single-file implementations of over 13 mitigation methods,\n10 evaluation metrics, and learning scenarios with increasing non-stationarity\nlevels from standard to open-ended environments. This framework enables\nresearchers to systematically quantify plasticity loss, evaluate mitigation\nstrategies, and analyze plasticity dynamics across different contexts. Our\ndocumentation, examples, and source code are available at\nhttps://github.com/RLE-Foundation/Plasticine."}
{"id": "2504.17140", "pdf": "https://arxiv.org/pdf/2504.17140", "abs": "https://arxiv.org/abs/2504.17140", "authors": ["Ashish Ranjan", "Ayush Agarwal", "Shalin Barot", "Sushant Kumar"], "title": "Scalable Permutation-Aware Modeling for Temporal Set Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Temporal set prediction involves forecasting the elements that will appear in\nthe next set, given a sequence of prior sets, each containing a variable number\nof elements. Existing methods often rely on intricate architectures with\nsubstantial computational overhead, which hampers their scalability. In this\nwork, we introduce a novel and scalable framework that leverages\npermutation-equivariant and permutation-invariant transformations to\nefficiently model set dynamics. Our approach significantly reduces both\ntraining and inference time while maintaining competitive performance.\nExtensive experiments on multiple public benchmarks show that our method\nachieves results on par with or superior to state-of-the-art models across\nseveral evaluation metrics. These results underscore the effectiveness of our\nmodel in enabling efficient and scalable temporal set prediction."}
{"id": "2504.17492", "pdf": "https://arxiv.org/pdf/2504.17492", "abs": "https://arxiv.org/abs/2504.17492", "authors": ["Nawid Keshtmand", "Elena Fillola", "Jeffrey Nicholas Clark", "Raul Santos-Rodriguez", "Matthew Rigby"], "title": "Prototype-enhanced prediction in graph neural networks for climate applications", "categories": ["cs.LG"], "comment": null, "summary": "Data-driven emulators are increasingly being used to learn and emulate\nphysics-based simulations, reducing computational expense and run time. Here,\nwe present a structured way to improve the quality of these high-dimensional\nemulated outputs, through the use of prototypes: an approximation of the\nemulator's output passed as an input, which informs the model and leads to\nbetter predictions. We demonstrate our approach to emulate atmospheric\ndispersion, key for greenhouse gas emissions monitoring, by comparing a\nbaseline model to models trained using prototypes as an additional input. The\nprototype models achieve better performance, even with few prototypes and even\nif they are chosen at random, but we show that choosing the prototypes through\ndata-driven methods (k-means) can lead to almost 10\\% increased performance in\nsome metrics."}
{"id": "2504.17160", "pdf": "https://arxiv.org/pdf/2504.17160", "abs": "https://arxiv.org/abs/2504.17160", "authors": ["Alberto FernÃ¡ndez-HernÃ¡ndez", "Jose I. Mestre", "Manuel F. Dolz", "Jose Duato", "Enrique S. Quintana-OrtÃ­"], "title": "OUI Need to Talk About Weight Decay: A New Perspective on Overfitting Detection", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "10 pages, 3 figures", "summary": "We introduce the Overfitting-Underfitting Indicator (OUI), a novel tool for\nmonitoring the training dynamics of Deep Neural Networks (DNNs) and identifying\noptimal regularization hyperparameters. Specifically, we validate that OUI can\neffectively guide the selection of the Weight Decay (WD) hyperparameter by\nindicating whether a model is overfitting or underfitting during training\nwithout requiring validation data. Through experiments on DenseNet-BC-100 with\nCIFAR- 100, EfficientNet-B0 with TinyImageNet and ResNet-34 with ImageNet-1K,\nwe show that maintaining OUI within a prescribed interval correlates strongly\nwith improved generalization and validation scores. Notably, OUI converges\nsignificantly faster than traditional metrics such as loss or accuracy,\nenabling practitioners to identify optimal WD (hyperparameter) values within\nthe early stages of training. By leveraging OUI as a reliable indicator, we can\ndetermine early in training whether the chosen WD value leads the model to\nunderfit the training data, overfit, or strike a well-balanced trade-off that\nmaximizes validation scores. This enables more precise WD tuning for optimal\nperformance on the tested datasets and DNNs. All code for reproducing these\nexperiments is available at https://github.com/AlbertoFdezHdez/OUI."}
{"id": "2504.17493", "pdf": "https://arxiv.org/pdf/2504.17493", "abs": "https://arxiv.org/abs/2504.17493", "authors": ["Luca-Andrei Fechete", "Mohamed Sana", "Fadhel Ayed", "Nicola Piovesan", "Wenjie Li", "Antonio De Domenico", "Tareq Si Salem"], "title": "Goal-Oriented Time-Series Forecasting: Foundation Framework Design", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traditional time-series forecasting often focuses only on minimizing\nprediction errors, ignoring the specific requirements of real-world\napplications that employ them. This paper presents a new training methodology,\nwhich allows a forecasting model to dynamically adjust its focus based on the\nimportance of forecast ranges specified by the end application. Unlike previous\nmethods that fix these ranges beforehand, our training approach breaks down\npredictions over the entire signal range into smaller segments, which are then\ndynamically weighted and combined to produce accurate forecasts. We tested our\nmethod on standard datasets, including a new dataset from wireless\ncommunication, and found that not only it improves prediction accuracy but also\nimproves the performance of end application employing the forecasting model.\nThis research provides a basis for creating forecasting systems that better\nconnect prediction and decision-making in various practical applications."}
{"id": "2504.17162", "pdf": "https://arxiv.org/pdf/2504.17162", "abs": "https://arxiv.org/abs/2504.17162", "authors": ["Cece Zhang", "Xuehuan Zhu", "Nick Peterson", "Jieqiong Wang", "Shibiao Wan"], "title": "A Comprehensive Review on RNA Subcellular Localization Prediction", "categories": ["cs.CV", "cs.AI", "q-bio.GN", "q-bio.SC"], "comment": null, "summary": "The subcellular localization of RNAs, including long non-coding RNAs\n(lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs,\nplays a critical role in determining their biological functions. For instance,\nlncRNAs are predominantly associated with chromatin and act as regulators of\ngene transcription and chromatin structure, while mRNAs are distributed across\nthe nucleus and cytoplasm, facilitating the transport of genetic information\nfor protein synthesis. Understanding RNA localization sheds light on processes\nlike gene expression regulation with spatial and temporal precision. However,\ntraditional wet lab methods for determining RNA localization, such as in situ\nhybridization, are often time-consuming, resource-demanding, and costly. To\novercome these challenges, computational methods leveraging artificial\nintelligence (AI) and machine learning (ML) have emerged as powerful\nalternatives, enabling large-scale prediction of RNA subcellular localization.\nThis paper provides a comprehensive review of the latest advancements in\nAI-based approaches for RNA subcellular localization prediction, covering\nvarious RNA types and focusing on sequence-based, image-based, and hybrid\nmethodologies that combine both data types. We highlight the potential of these\nmethods to accelerate RNA research, uncover molecular pathways, and guide\ntargeted disease treatments. Furthermore, we critically discuss the challenges\nin AI/ML approaches for RNA subcellular localization, such as data scarcity and\nlack of benchmarks, and opportunities to address them. This review aims to\nserve as a valuable resource for researchers seeking to develop innovative\nsolutions in the field of RNA subcellular localization and beyond."}
{"id": "2504.17497", "pdf": "https://arxiv.org/pdf/2504.17497", "abs": "https://arxiv.org/abs/2504.17497", "authors": ["Radia Berreziga", "Mohammed Brahimi", "Khairedine Kraim", "Hamid Azzoune"], "title": "Combining GCN Structural Learning with LLM Chemical Knowledge for or Enhanced Virtual Screening", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Virtual screening plays a critical role in modern drug discovery by enabling\nthe identification of promising candidate molecules for experimental\nvalidation. Traditional machine learning methods such as support vector\nmachines (SVM) and XGBoost rely on predefined molecular representations, often\nleading to information loss and potential bias. In contrast, deep learning\napproaches-particularly Graph Convolutional Networks (GCNs)-offer a more\nexpressive and unbiased alternative by operating directly on molecular graphs.\nMeanwhile, Large Language Models (LLMs) have recently demonstrated\nstate-of-the-art performance in drug design, thanks to their capacity to\ncapture complex chemical patterns from large-scale data via attention\nmechanisms.\n  In this paper, we propose a hybrid architecture that integrates GCNs with\nLLM-derived embeddings to combine localized structural learning with global\nchemical knowledge. The LLM embeddings can be precomputed and stored in a\nmolecular feature library, removing the need to rerun the LLM during training\nor inference and thus maintaining computational efficiency. We found that\nconcatenating the LLM embeddings after each GCN layer-rather than only at the\nfinal layer-significantly improves performance, enabling deeper integration of\nglobal context throughout the network. The resulting model achieves superior\nresults, with an F1-score of (88.8%), outperforming standalone GCN (87.9%),\nXGBoost (85.5%), and SVM (85.4%) baselines."}
{"id": "2504.17170", "pdf": "https://arxiv.org/pdf/2504.17170", "abs": "https://arxiv.org/abs/2504.17170", "authors": ["Robert Kaufman"], "title": "Improving Human-Autonomous Vehicle Interaction in Complex Systems", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "PhD Dissertation from University of California, San Diego; 175 pages", "summary": "Unresolved questions about how autonomous vehicles (AVs) should meet the\ninformational needs of riders hinder real-world adoption. Complicating our\nability to satisfy rider needs is that different people, goals, and driving\ncontexts have different criteria for what constitutes interaction success.\nUnfortunately, most human-AV research and design today treats all people and\nsituations uniformly. It is crucial to understand how an AV should communicate\nto meet rider needs, and how communications should change when the human-AV\ncomplex system changes. I argue that understanding the relationships between\ndifferent aspects of the human-AV system can help us build improved and\nadaptable AV communications. I support this argument using three empirical\nstudies. First, I identify optimal communication strategies that enhance\ndriving performance, confidence, and trust for learning in extreme driving\nenvironments. Findings highlight the need for task-sensitive,\nmodality-appropriate communications tuned to learner cognitive limits and\ngoals. Next, I highlight the consequences of deploying faulty communication\nsystems and demonstrate the need for context-sensitive communications. Third, I\nuse machine learning (ML) to illuminate personal factors predicting trust in\nAVs, emphasizing the importance of tailoring designs to individual traits and\nconcerns. Together, this dissertation supports the necessity of transparent,\nadaptable, and personalized AV systems that cater to individual needs, goals,\nand contextual demands. By considering the complex system within which human-AV\ninteractions occur, we can deliver valuable insights for designers,\nresearchers, and policymakers. This dissertation also provides a concrete\ndomain to study theories of human-machine joint action and situational\nawareness, and can be used to guide future human-AI interaction research.\n[shortened for arxiv]"}
{"id": "2504.17503", "pdf": "https://arxiv.org/pdf/2504.17503", "abs": "https://arxiv.org/abs/2504.17503", "authors": ["Davide Prosperino", "Haochun Ma", "Christoph RÃ¤th"], "title": "Tailored minimal reservoir computing: on the bidirectional connection between nonlinearities in the reservoir and in data", "categories": ["cs.LG", "nlin.CD"], "comment": "13 pages, 11 figures", "summary": "We study how the degree of nonlinearity in the input data affects the optimal\ndesign of reservoir computers, focusing on how closely the model's nonlinearity\nshould align with that of the data. By reducing minimal RCs to a single tunable\nnonlinearity parameter, we explore how the predictive performance varies with\nthe degree of nonlinearity in the reservoir. To provide controlled testbeds, we\ngeneralize to the fractional Halvorsen system, a novel chaotic system with\nfractional exponents. Our experiments reveal that the prediction performance is\nmaximized when the reservoir's nonlinearity matches the nonlinearity present in\nthe data. In cases where multiple nonlinearities are present in the data, we\nfind that the correlation dimension of the predicted signal is reconstructed\ncorrectly when the smallest nonlinearity is matched. We use this observation to\npropose a method for estimating the minimal nonlinearity in unknown time series\nby sweeping the reservoir exponent and identifying the transition to a\nsuccessful reconstruction. Applying this method to both synthetic and\nreal-world datasets, including financial time series, we demonstrate its\npractical viability. Finally, we transfer these insights to classical RC by\naugmenting traditional architectures with fractional, generalized reservoir\nstates. This yields performance gains, particularly in resource-constrained\nscenarios such as physical reservoirs, where increasing reservoir size is\nimpractical or economically unviable. Our work provides a principled route\ntoward tailoring RCs to the intrinsic complexity of the systems they aim to\nmodel."}
{"id": "2504.17180", "pdf": "https://arxiv.org/pdf/2504.17180", "abs": "https://arxiv.org/abs/2504.17180", "authors": ["Minkyu Choi", "S P Sharan", "Harsh Goel", "Sahil Shah", "Sandeep Chinchali"], "title": "We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Current text-to-video (T2V) generation models are increasingly popular due to\ntheir ability to produce coherent videos from textual prompts. However, these\nmodels often struggle to generate semantically and temporally consistent videos\nwhen dealing with longer, more complex prompts involving multiple objects or\nsequential events. Additionally, the high computational costs associated with\ntraining or fine-tuning make direct improvements impractical. To overcome these\nlimitations, we introduce \\(\\projectname\\), a novel zero-training video\nrefinement pipeline that leverages neuro-symbolic feedback to automatically\nenhance video generation, achieving superior alignment with the prompts. Our\napproach first derives the neuro-symbolic feedback by analyzing a formal video\nrepresentation and pinpoints semantically inconsistent events, objects, and\ntheir corresponding frames. This feedback then guides targeted edits to the\noriginal video. Extensive empirical evaluations on both open-source and\nproprietary T2V models demonstrate that \\(\\projectname\\) significantly enhances\ntemporal and logical alignment across diverse prompts by almost $40\\%$."}
{"id": "2504.17520", "pdf": "https://arxiv.org/pdf/2504.17520", "abs": "https://arxiv.org/abs/2504.17520", "authors": ["Zhuojun Tian", "Zhaoyang Zhang", "Yiwei Li", "Mehdi Bennis"], "title": "Communication-Efficient Personalized Distributed Learning with Data and Node Heterogeneity", "categories": ["cs.LG", "cs.DC", "cs.MA"], "comment": "Accepcted by TCCN", "summary": "To jointly tackle the challenges of data and node heterogeneity in\ndecentralized learning, we propose a distributed strong lottery ticket\nhypothesis (DSLTH), based on which a communication-efficient personalized\nlearning algorithm is developed. In the proposed method, each local model is\nrepresented as the Hadamard product of global real-valued parameters and a\npersonalized binary mask for pruning. The local model is learned by updating\nand fusing the personalized binary masks while the real-valued parameters are\nfixed among different agents. To further reduce the complexity of hardware\nimplementation, we incorporate a group sparse regularization term in the loss\nfunction, enabling the learned local model to achieve structured sparsity.\nThen, a binary mask aggregation algorithm is designed by introducing an\nintermediate aggregation tensor and adding a personalized fine-tuning step in\neach iteration, which constrains model updates towards the local data\ndistribution. The proposed method effectively leverages the relativity among\nagents while meeting personalized requirements in heterogeneous node\nconditions. We also provide a theoretical proof for the DSLTH, establishing it\nas the foundation of the proposed method. Numerical simulations confirm the\nvalidity of the DSLTH and demonstrate the effectiveness of the proposed\nalgorithm."}
{"id": "2504.17198", "pdf": "https://arxiv.org/pdf/2504.17198", "abs": "https://arxiv.org/abs/2504.17198", "authors": ["XiangRui Zhang", "HaoYu Chen", "Yongzhong He", "Wenjia Niu", "Qiang Li"], "title": "Automatically Generating Rules of Malicious Software Packages via Large Language Model", "categories": ["cs.SE", "cs.AI", "cs.CR"], "comment": "14 pages, 11 figures", "summary": "Today's security tools predominantly rely on predefined rules crafted by\nexperts, making them poorly adapted to the emergence of software supply chain\nattacks. To tackle this limitation, we propose a novel tool, RuleLLM, which\nleverages large language models (LLMs) to automate rule generation for OSS\necosystems. RuleLLM extracts metadata and code snippets from malware as its\ninput, producing YARA and Semgrep rules that can be directly deployed in\nsoftware development. Specifically, the rule generation task involves three\nsubtasks: crafting rules, refining rules, and aligning rules. To validate\nRuleLLM's effectiveness, we implemented a prototype system and conducted\nexperiments on the dataset of 1,633 malicious packages. The results are\npromising that RuleLLM generated 763 rules (452 YARA and 311 Semgrep) with a\nprecision of 85.2\\% and a recall of 91.8\\%, outperforming state-of-the-art\n(SOTA) tools and scored-based approaches. We further analyzed generated rules\nand proposed a rule taxonomy: 11 categories and 38 subcategories."}
{"id": "2504.17526", "pdf": "https://arxiv.org/pdf/2504.17526", "abs": "https://arxiv.org/abs/2504.17526", "authors": ["Yuelin Liu", "Haiyuan Li", "Xenofon Vasilakos", "Rasheed Hussain", "Dimitra Simeonidou"], "title": "Cooperative Task Offloading through Asynchronous Deep Reinforcement Learning in Mobile Edge Computing for Future Networks", "categories": ["cs.LG"], "comment": null, "summary": "Future networks (including 6G) are poised to accelerate the realisation of\nInternet of Everything. However, it will result in a high demand for computing\nresources to support new services. Mobile Edge Computing (MEC) is a promising\nsolution, enabling to offload computation-intensive tasks to nearby edge\nservers from the end-user devices, thereby reducing latency and energy\nconsumption. However, relying solely on a single MEC server for task offloading\ncan lead to uneven resource utilisation and suboptimal performance in complex\nscenarios. Additionally, traditional task offloading strategies specialise in\ncentralised policy decisions, which unavoidably entail extreme transmission\nlatency and reach computational bottleneck. To fill the gaps, we propose a\nlatency and energy efficient Cooperative Task Offloading framework with\nTransformer-driven Prediction (CTO-TP), leveraging asynchronous multi-agent\ndeep reinforcement learning to address these challenges. This approach fosters\nedge-edge cooperation and decreases the synchronous waiting time by performing\nasynchronous training, optimising task offloading, and resource allocation\nacross distributed networks. The performance evaluation demonstrates that the\nproposed CTO-TP algorithm reduces up to 80% overall system latency and 87%\nenergy consumption compared to the baseline schemes."}
{"id": "2504.17210", "pdf": "https://arxiv.org/pdf/2504.17210", "abs": "https://arxiv.org/abs/2504.17210", "authors": ["Junfei Wang", "Darshana Upadhyay", "Marzia Zaman", "Pirathayini Srikantha"], "title": "Synthetic Power Flow Data Generation Using Physics-Informed Denoising Diffusion Probabilistic Models", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to IEEE SmartGridComm Conference 2025", "summary": "Many data-driven modules in smart grid rely on access to high-quality power\nflow data; however, real-world data are often limited due to privacy and\noperational constraints. This paper presents a physics-informed generative\nframework based on Denoising Diffusion Probabilistic Models (DDPMs) for\nsynthesizing feasible power flow data. By incorporating auxiliary training and\nphysics-informed loss functions, the proposed method ensures that the generated\ndata exhibit both statistical fidelity and adherence to power system\nfeasibility. We evaluate the approach on the IEEE 14-bus and 30-bus benchmark\nsystems, demonstrating its ability to capture key distributional properties and\ngeneralize to out-of-distribution scenarios. Comparative results show that the\nproposed model outperforms three baseline models in terms of feasibility,\ndiversity, and accuracy of statistical features. This work highlights the\npotential of integrating generative modelling into data-driven power system\napplications."}
{"id": "2504.17528", "pdf": "https://arxiv.org/pdf/2504.17528", "abs": "https://arxiv.org/abs/2504.17528", "authors": ["Weijie Liu", "Ziwei Zhan", "Carlee Joe-Wong", "Edith Ngai", "Jingpu Duan", "Deke Guo", "Xu Chen", "Xiaoxi Zhang"], "title": "TACO: Tackling Over-correction in Federated Learning with Tailored Adaptive Correction", "categories": ["cs.LG", "cs.AI", "I.2.6"], "comment": "11 pages, 7 figures, accepted by ICDCS 2025", "summary": "Non-independent and identically distributed (Non-IID) data across edge\nclients have long posed significant challenges to federated learning (FL)\ntraining in edge computing environments. Prior works have proposed various\nmethods to mitigate this statistical heterogeneity. While these works can\nachieve good theoretical performance, in this work we provide the first\ninvestigation into a hidden over-correction phenomenon brought by the uniform\nmodel correction coefficients across clients adopted by existing methods. Such\nover-correction could degrade model performance and even cause failures in\nmodel convergence. To address this, we propose TACO, a novel algorithm that\naddresses the non-IID nature of clients' data by implementing fine-grained,\nclient-specific gradient correction and model aggregation, steering local\nmodels towards a more accurate global optimum. Moreover, we verify that leading\nFL algorithms generally have better model accuracy in terms of communication\nrounds rather than wall-clock time, resulting from their extra computation\noverhead imposed on clients. To enhance the training efficiency, TACO deploys a\nlightweight model correction and tailored aggregation approach that requires\nminimum computation overhead and no extra information beyond the synchronized\nmodel parameters. To validate TACO's effectiveness, we present the first FL\nconvergence analysis that reveals the root cause of over-correction. Extensive\nexperiments across various datasets confirm TACO's superior and stable\nperformance in practice."}
{"id": "2504.17213", "pdf": "https://arxiv.org/pdf/2504.17213", "abs": "https://arxiv.org/abs/2504.17213", "authors": ["Shiwen Cao", "Zhaoxing Zhang", "Junming Jiao", "Juyi Qiao", "Guowen Song", "Rong Shen"], "title": "MCAF: Efficient Agent-based Video Understanding Framework through Multimodal Coarse-to-Fine Attention Focusing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Even in the era of rapid advances in large models, video understanding,\nparticularly long videos, remains highly challenging. Compared with textual or\nimage-based information, videos commonly contain more information with\nredundancy, requiring large models to strategically allocate attention at a\nglobal level for accurate comprehension. To address this, we propose MCAF, an\nagent-based, training-free framework perform video understanding through\nMultimodal Coarse-to-fine Attention Focusing. The key innovation lies in its\nability to sense and prioritize segments of the video that are highly relevant\nto the understanding task. First, MCAF hierarchically concentrates on highly\nrelevant frames through multimodal information, enhancing the correlation\nbetween the acquired contextual information and the query. Second, it employs a\ndilated temporal expansion mechanism to mitigate the risk of missing crucial\ndetails when extracting information from these concentrated frames. In\naddition, our framework incorporates a self-reflection mechanism utilizing the\nconfidence level of the model's responses as feedback. By iteratively applying\nthese two creative focusing strategies, it adaptively adjusts attention to\ncapture highly query-connected context and thus improves response accuracy.\nMCAF outperforms comparable state-of-the-art methods on average. On the\nEgoSchema dataset, it achieves a remarkable 5% performance gain over the\nleading approach. Meanwhile, on Next-QA and IntentQA datasets, it outperforms\nthe current state-of-the-art standard by 0.2% and 0.3% respectively. On the\nVideo-MME dataset, which features videos averaging nearly an hour in length,\nMCAF also outperforms other agent-based methods."}
{"id": "2504.17534", "pdf": "https://arxiv.org/pdf/2504.17534", "abs": "https://arxiv.org/abs/2504.17534", "authors": ["Juan Carlos Climent Pardo"], "title": "Learning Isometric Embeddings of Road Networks using Multidimensional Scaling", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.SC"], "comment": null, "summary": "The lack of generalization in learning-based autonomous driving applications\nis shown by the narrow range of road scenarios that vehicles can currently\ncover. A generalizable approach should capture many distinct road structures\nand topologies, as well as consider traffic participants, and dynamic changes\nin the environment, so that vehicles can navigate and perform motion planning\ntasks even in the most difficult situations. Designing suitable feature spaces\nfor neural network-based motion planers that encapsulate all kinds of road\nscenarios is still an open research challenge. This paper tackles this\nlearning-based generalization challenge and shows how graph representations of\nroad networks can be leveraged by using multidimensional scaling (MDS)\ntechniques in order to obtain such feature spaces. State-of-the-art graph\nrepresentations and MDS approaches are analyzed for the autonomous driving use\ncase. Finally, the option of embedding graph nodes is discussed in order to\nperform easier learning procedures and obtain dimensionality reduction."}
{"id": "2504.17219", "pdf": "https://arxiv.org/pdf/2504.17219", "abs": "https://arxiv.org/abs/2504.17219", "authors": ["Hyomin Lee", "Minseon Kim", "Sangwon Jang", "Jongheon Jeong", "Sung Ju Hwang"], "title": "Enhancing Variational Autoencoders with Smooth Robust Latent Encoding", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Under review", "summary": "Variational Autoencoders (VAEs) have played a key role in scaling up\ndiffusion-based generative models, as in Stable Diffusion, yet questions\nregarding their robustness remain largely underexplored. Although adversarial\ntraining has been an established technique for enhancing robustness in\npredictive models, it has been overlooked for generative models due to concerns\nabout potential fidelity degradation by the nature of trade-offs between\nperformance and robustness. In this work, we challenge this presumption,\nintroducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training\nframework that boosts both generation quality and robustness. In contrast to\nconventional adversarial training, which focuses on robustness only, our\napproach smooths the latent space via adversarial perturbations, promoting more\ngeneralizable representations while regularizing with originality\nrepresentation to sustain original fidelity. Applied as a post-training step on\npre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal\ncomputational overhead. Experiments show that SRL-VAE improves both generation\nquality, in image reconstruction and text-guided image editing, and robustness,\nagainst Nightshade attacks and image editing attacks. These results establish a\nnew paradigm, showing that adversarial training, once thought to be detrimental\nto generative models, can instead enhance both fidelity and robustness."}
{"id": "2504.17568", "pdf": "https://arxiv.org/pdf/2504.17568", "abs": "https://arxiv.org/abs/2504.17568", "authors": ["Ivan Rossi", "Flavio Sartori", "Cesare Rollo", "Giovanni Birolo", "Piero Fariselli", "Tiziana Sanavia"], "title": "Beyond Cox Models: Assessing the Performance of Machine-Learning Methods in Non-Proportional Hazards and Non-Linear Survival Analysis", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Survival analysis often relies on Cox models, assuming both linearity and\nproportional hazards (PH). This study evaluates machine and deep learning\nmethods that relax these constraints, comparing their performance with\npenalized Cox models on a benchmark of three synthetic and three real datasets.\nIn total, eight different models were tested, including six non-linear models\nof which four were also non-PH. Although Cox regression often yielded\nsatisfactory performance, we showed the conditions under which machine and deep\nlearning models can perform better. Indeed, the performance of these methods\nhas often been underestimated due to the improper use of Harrell's concordance\nindex (C-index) instead of more appropriate scores such as Antolini's\nconcordance index, which generalizes C-index in cases where the PH assumption\ndoes not hold. In addition, since occasionally high C-index models happen to be\nbadly calibrated, combining Antolini's C-index with Brier's score is useful to\nassess the overall performance of a survival method. Results on our benchmark\ndata showed that survival prediction should be approached by testing different\nmethods to select the most appropriate one according to sample size,\nnon-linearity and non-PH conditions. To allow an easy reproducibility of these\ntests on our benchmark data, code and documentation are freely available at\nhttps://github.com/compbiomed-unito/survhive."}
{"id": "2504.17243", "pdf": "https://arxiv.org/pdf/2504.17243", "abs": "https://arxiv.org/abs/2504.17243", "authors": ["Xinyu Zhou", "Simin Fan", "Martin Jaggi", "Jie Fu"], "title": "NeuralGrok: Accelerate Grokking by Neural Gradient Transformation", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint, 16 pages", "summary": "Grokking is proposed and widely studied as an intricate phenomenon in which\ngeneralization is achieved after a long-lasting period of overfitting. In this\nwork, we propose NeuralGrok, a novel gradient-based approach that learns an\noptimal gradient transformation to accelerate the generalization of\ntransformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary\nmodule (e.g., an MLP block) in conjunction with the base model. This module\ndynamically modulates the influence of individual gradient components based on\ntheir contribution to generalization, guided by a bilevel optimization\nalgorithm. Our extensive experiments demonstrate that NeuralGrok significantly\naccelerates generalization, particularly in challenging arithmetic tasks. We\nalso show that NeuralGrok promotes a more stable training paradigm, constantly\nreducing the model's complexity, while traditional regularization methods, such\nas weight decay, can introduce substantial instability and impede\ngeneralization. We further investigate the intrinsic model complexity\nleveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that\nNeuralGrok effectively facilitates generalization by reducing the model\ncomplexity. We offer valuable insights on the grokking phenomenon of\nTransformer models, which encourages a deeper understanding of the fundamental\nprinciples governing generalization ability."}
{"id": "2504.17577", "pdf": "https://arxiv.org/pdf/2504.17577", "abs": "https://arxiv.org/abs/2504.17577", "authors": ["Lei Wang", "Yu Cheng", "Yining Shi", "Zhengju Tang", "Zhiwen Mo", "Wenhao Xie", "Lingxiao Ma", "Yuqing Xia", "Jilong Xue", "Fan Yang", "Zhi Yang"], "title": "TileLang: A Composable Tiled Programming Model for AI Systems", "categories": ["cs.LG"], "comment": null, "summary": "Modern AI workloads rely heavily on optimized computing kernels for both\ntraining and inference. These AI kernels follow well-defined data-flow\npatterns, such as moving tiles between DRAM and SRAM and performing a sequence\nof computations on those tiles. However, writing high-performance kernels\nremains complex despite the clarity of these patterns. Achieving peak\nperformance requires careful, hardware-centric optimizations to fully leverage\nmodern accelerators. While domain-specific compilers attempt to reduce the\nburden of writing high-performance kernels, they often struggle with usability\nand expressiveness gaps. In this paper, we present TileLang, a generalized\ntiled programming model for more efficient AI Kernel programming. TileLang\ndecouples scheduling space (thread binding, layout, tensorize and pipeline)\nfrom dataflow, and encapsulated them as a set of customization annotations and\nprimitives. This approach allows users to focus on the kernel's data-flow\nitself, while leaving most other optimizations to compilers. We conduct\ncomprehensive experiments on commonly-used devices, across numerous\nexperiments, our evaluation shows that TileLang can achieve state-of-the-art\nperformance in key kernels, demonstrating that its unified block-and-thread\nparadigm and transparent scheduling capabilities deliver both the power and\nflexibility demanded by modern AI system development."}
{"id": "2504.17247", "pdf": "https://arxiv.org/pdf/2504.17247", "abs": "https://arxiv.org/abs/2504.17247", "authors": ["Diogo Soares", "Leon Hetzel", "Paulina Szymczak", "Fabian Theis", "Stephan GÃ¼nnemann", "Ewa Szczurek"], "title": "Targeted AMP generation through controlled diffusion with efficient embeddings", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": null, "summary": "Deep learning-based antimicrobial peptide (AMP) discovery faces critical\nchallenges such as low experimental hit rates as well as the need for nuanced\ncontrollability and efficient modeling of peptide properties. To address these\nchallenges, we introduce OmegAMP, a framework that leverages a diffusion-based\ngenerative model with efficient low-dimensional embeddings, precise\ncontrollability mechanisms, and novel classifiers with drastically reduced\nfalse positive rates for candidate filtering. OmegAMP enables the targeted\ngeneration of AMPs with specific physicochemical properties, activity profiles,\nand species-specific effectiveness. Moreover, it maximizes sample diversity\nwhile ensuring faithfulness to the underlying data distribution during\ngeneration. We demonstrate that OmegAMP achieves state-of-the-art performance\nacross all stages of the AMP discovery pipeline, significantly advancing the\npotential of computational frameworks in combating antimicrobial resistance."}
{"id": "2504.17578", "pdf": "https://arxiv.org/pdf/2504.17578", "abs": "https://arxiv.org/abs/2504.17578", "authors": ["Hongshu Guo", "Wenjie Qiu", "Zeyuan Ma", "Xinglin Zhang", "Jun Zhang", "Yue-Jiao Gong"], "title": "Advancing CMA-ES with Learning-Based Cooperative Coevolution for Scalable Optimization", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Recent research in Cooperative Coevolution~(CC) have achieved promising\nprogress in solving large-scale global optimization problems. However, existing\nCC paradigms have a primary limitation in that they require deep expertise for\nselecting or designing effective variable decomposition strategies. Inspired by\nadvancements in Meta-Black-Box Optimization, this paper introduces LCC, a\npioneering learning-based cooperative coevolution framework that dynamically\nschedules decomposition strategies during optimization processes. The\ndecomposition strategy selector is parameterized through a neural network,\nwhich processes a meticulously crafted set of optimization status features to\ndetermine the optimal strategy for each optimization step. The network is\ntrained via the Proximal Policy Optimization method in a reinforcement learning\nmanner across a collection of representative problems, aiming to maximize the\nexpected optimization performance. Extensive experimental results demonstrate\nthat LCC not only offers certain advantages over state-of-the-art baselines in\nterms of optimization effectiveness and resource consumption, but it also\nexhibits promising transferability towards unseen problems."}
{"id": "2504.17255", "pdf": "https://arxiv.org/pdf/2504.17255", "abs": "https://arxiv.org/abs/2504.17255", "authors": ["Shaoyu Pei", "Renxiong Wu", "Hao Zheng", "Lang Qin", "Shuaichen Lin", "Yuxing Gan", "Wenjing Huang", "Zhixuan Wang", "Mohan Qin", "Yong Liu", "Guangming Ni"], "title": "3D Deep-learning-based Segmentation of Human Skin Sweat Glands and Their 3D Morphological Response to Temperature Variations", "categories": ["eess.IV", "cs.AI", "physics.optics"], "comment": null, "summary": "Skin, the primary regulator of heat exchange, relies on sweat glands for\nthermoregulation. Alterations in sweat gland morphology play a crucial role in\nvarious pathological conditions and clinical diagnoses. Current methods for\nobserving sweat gland morphology are limited by their two-dimensional, in\nvitro, and destructive nature, underscoring the urgent need for real-time,\nnon-invasive, quantifiable technologies. We proposed a novel three-dimensional\n(3D) transformer-based multi-object segmentation framework, integrating a\nsliding window approach, joint spatial-channel attention mechanism, and\narchitectural heterogeneity between shallow and deep layers. Our proposed\nnetwork enables precise 3D sweat gland segmentation from skin volume data\ncaptured by optical coherence tomography (OCT). For the first time, subtle\nvariations of sweat gland 3D morphology in response to temperature changes,\nhave been visualized and quantified. Our approach establishes a benchmark for\nnormal sweat gland morphology and provides a real-time, non-invasive tool for\nquantifying 3D structural parameters. This enables the study of individual\nvariability and pathological changes in sweat gland structure, advancing\ndermatological research and clinical applications, including thermoregulation\nand bromhidrosis treatment."}
{"id": "2504.17601", "pdf": "https://arxiv.org/pdf/2504.17601", "abs": "https://arxiv.org/abs/2504.17601", "authors": ["Erik Bergh"], "title": "Interpretable non-linear dimensionality reduction using gaussian weighted linear transformation", "categories": ["cs.LG"], "comment": "11 pages, 5 figures", "summary": "Dimensionality reduction techniques are fundamental for analyzing and\nvisualizing high-dimensional data. With established methods like t-SNE and PCA\npresenting a trade-off between representational power and interpretability.\nThis paper introduces a novel approach that bridges this gap by combining the\ninterpretability of linear methods with the expressiveness of non-linear\ntransformations. The proposed algorithm constructs a non-linear mapping between\nhigh-dimensional and low-dimensional spaces through a combination of linear\ntransformations, each weighted by Gaussian functions. This architecture enables\ncomplex non-linear transformations while preserving the interpretability\nadvantages of linear methods, as each transformation can be analyzed\nindependently. The resulting model provides both powerful dimensionality\nreduction and transparent insights into the transformed space. Techniques for\ninterpreting the learned transformations are presented, including methods for\nidentifying suppressed dimensions and how space is expanded and contracted.\nThese tools enable practitioners to understand how the algorithm preserves and\nmodifies geometric relationships during dimensionality reduction. To ensure the\npractical utility of this algorithm, the creation of user-friendly software\npackages is emphasized, facilitating its adoption in both academia and\nindustry."}
{"id": "2504.17261", "pdf": "https://arxiv.org/pdf/2504.17261", "abs": "https://arxiv.org/abs/2504.17261", "authors": ["Jiaqi Chen", "Xiaoye Zhu", "Yue Wang", "Tianyang Liu", "Xinhui Chen", "Ying Chen", "Chak Tou Leong", "Yifei Ke", "Joseph Liu", "Yiwen Yuan", "Julian McAuley", "Li-jia Li"], "title": "Symbolic Representation for Any-to-Any Generative Tasks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose a symbolic generative task description language and a\ncorresponding inference engine capable of representing arbitrary multimodal\ntasks as structured symbolic flows. Unlike conventional generative models that\nrely on large-scale training and implicit neural representations to learn\ncross-modal mappings, often at high computational cost and with limited\nflexibility, our framework introduces an explicit symbolic representation\ncomprising three core primitives: functions, parameters, and topological logic.\nLeveraging a pre-trained language model, our inference engine maps natural\nlanguage instructions directly to symbolic workflows in a training-free manner.\nOur framework successfully performs over 12 diverse multimodal generative\ntasks, demonstrating strong performance and flexibility without the need for\ntask-specific tuning. Experiments show that our method not only matches or\noutperforms existing state-of-the-art unified models in content quality, but\nalso offers greater efficiency, editability, and interruptibility. We believe\nthat symbolic task representations provide a cost-effective and extensible\nfoundation for advancing the capabilities of generative AI."}
{"id": "2504.17613", "pdf": "https://arxiv.org/pdf/2504.17613", "abs": "https://arxiv.org/abs/2504.17613", "authors": ["Bowen Deng", "Chang Xu", "Hao Li", "Yuhao Huang", "Min Hou", "Jiang Bian"], "title": "TarDiff: Target-Oriented Diffusion Guidance for Synthetic Electronic Health Record Time Series Generation", "categories": ["cs.LG"], "comment": null, "summary": "Synthetic Electronic Health Record (EHR) time-series generation is crucial\nfor advancing clinical machine learning models, as it helps address data\nscarcity by providing more training data. However, most existing approaches\nfocus primarily on replicating statistical distributions and temporal\ndependencies of real-world data. We argue that fidelity to observed data alone\ndoes not guarantee better model performance, as common patterns may dominate,\nlimiting the representation of rare but important conditions. This highlights\nthe need for generate synthetic samples to improve performance of specific\nclinical models to fulfill their target outcomes. To address this, we propose\nTarDiff, a novel target-oriented diffusion framework that integrates\ntask-specific influence guidance into the synthetic data generation process.\nUnlike conventional approaches that mimic training data distributions, TarDiff\noptimizes synthetic samples by quantifying their expected contribution to\nimproving downstream model performance through influence functions.\nSpecifically, we measure the reduction in task-specific loss induced by\nsynthetic samples and embed this influence gradient into the reverse diffusion\nprocess, thereby steering the generation towards utility-optimized data.\nEvaluated on six publicly available EHR datasets, TarDiff achieves\nstate-of-the-art performance, outperforming existing methods by up to 20.4% in\nAUPRC and 18.4% in AUROC. Our results demonstrate that TarDiff not only\npreserves temporal fidelity but also enhances downstream model performance,\noffering a robust solution to data scarcity and class imbalance in healthcare\nanalytics."}
{"id": "2504.17264", "pdf": "https://arxiv.org/pdf/2504.17264", "abs": "https://arxiv.org/abs/2504.17264", "authors": ["Zhaolu Kang", "Hongtian Cai", "Xiangyang Ji", "Jinzhe Li", "Nanfei Gu"], "title": "JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "In recent years, Unsupervised Domain Adaptation (UDA) has gained significant\nattention in the field of Natural Language Processing (NLP) owing to its\nability to enhance model generalization across diverse domains. However, its\napplication for knowledge transfer between distinct legal domains remains\nlargely unexplored. To address the challenges posed by lengthy and complex\nlegal texts and the limited availability of large-scale annotated datasets, we\npropose JurisCTC, a novel model designed to improve the accuracy of Legal\nJudgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC\nfacilitates effective knowledge transfer across various legal domains and\nemploys contrastive learning to distinguish samples from different domains.\nSpecifically, for the LJP task, we enable knowledge transfer between civil and\ncriminal law domains. Compared to other models and specific large language\nmodels (LLMs), JurisCTC demonstrates notable advancements, achieving peak\naccuracies of 76.59% and 78.83%, respectively."}
{"id": "2504.17617", "pdf": "https://arxiv.org/pdf/2504.17617", "abs": "https://arxiv.org/abs/2504.17617", "authors": ["Bruno Casella", "Matthias Jakobs", "Marco Aldinucci", "Sebastian BuschjÃ¤ger"], "title": "Decentralized Time Series Classification with ROCKET Features", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.11; I.2.6"], "comment": "Submitted to Workshop on Federated Learning Advancements 2025, in\n  conjunction with ECML-PKDD, WAFL25", "summary": "Time series classification (TSC) is a critical task with applications in\nvarious domains, including healthcare, finance, and industrial monitoring. Due\nto privacy concerns and data regulations, Federated Learning has emerged as a\npromising approach for learning from distributed time series data without\ncentralizing raw information. However, most FL solutions rely on a\nclient-server architecture, which introduces robustness and confidentiality\nrisks related to the distinguished role of the server, which is a single point\nof failure and can observe knowledge extracted from clients. To address these\nchallenges, we propose DROCKS, a fully decentralized FL framework for TSC that\nleverages ROCKET (RandOm Convolutional KErnel Transform) features. In DROCKS,\nthe global model is trained by sequentially traversing a structured path across\nfederation nodes, where each node refines the model and selects the most\neffective local kernels before passing them to the successor. Extensive\nexperiments on the UCR archive demonstrate that DROCKS outperforms\nstate-of-the-art client-server FL approaches while being more resilient to node\nfailures and malicious attacks. Our code is available at\nhttps://anonymous.4open.science/r/DROCKS-7FF3/README.md."}
{"id": "2504.17277", "pdf": "https://arxiv.org/pdf/2504.17277", "abs": "https://arxiv.org/abs/2504.17277", "authors": ["Zongliang Ji", "Andre Carlos Kajdacsy-Balla Amaral", "Anna Goldenberg", "Rahul G. Krishnan"], "title": "ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to the Conference on Health, Inference, and Learning (CHIL)\n  2025", "summary": "Ordering a minimal subset of lab tests for patients in the intensive care\nunit (ICU) can be challenging. Care teams must balance between ensuring the\navailability of the right information and reducing the clinical burden and\ncosts associated with each lab test order. Most in-patient settings experience\nfrequent over-ordering of lab tests, but are now aiming to reduce this burden\non both hospital resources and the environment. This paper develops a novel\nmethod that combines off-policy learning with privileged information to\nidentify the optimal set of ICU lab tests to order. Our approach, EXplainable\nOff-policy learning with Side Information for ICU blood Test Orders (ExOSITO)\ncreates an interpretable assistive tool for clinicians to order lab tests by\nconsidering both the observed and predicted future status of each patient. We\npose this problem as a causal bandit trained using offline data and a reward\nfunction derived from clinically-approved rules; we introduce a novel learning\nframework that integrates clinical knowledge with observational data to bridge\nthe gap between the optimal and logging policies. The learned policy function\nprovides interpretable clinical information and reduces costs without omitting\nany vital lab orders, outperforming both a physician's policy and prior\napproaches to this practical problem."}
{"id": "2504.17618", "pdf": "https://arxiv.org/pdf/2504.17618", "abs": "https://arxiv.org/abs/2504.17618", "authors": ["Nikita Gabdullin"], "title": "The effects of Hessian eigenvalue spectral density type on the applicability of Hessian analysis to generalization capability assessment of neural networks", "categories": ["cs.LG", "cs.CV"], "comment": "11 pages, 10 figures, 4 tables, 4 equations", "summary": "Hessians of neural network (NN) contain essential information about the\ncurvature of NN loss landscapes which can be used to estimate NN generalization\ncapabilities. We have previously proposed generalization criteria that rely on\nthe observation that Hessian eigenvalue spectral density (HESD) behaves\nsimilarly for a wide class of NNs. This paper further studies their\napplicability by investigating factors that can result in different types of\nHESD. We conduct a wide range of experiments showing that HESD mainly has\npositive eigenvalues (MP-HESD) for NN training and fine-tuning with various\noptimizers on different datasets with different preprocessing and augmentation\nprocedures. We also show that mainly negative HESD (MN-HESD) is a consequence\nof external gradient manipulation, indicating that the previously proposed\nHessian analysis methodology cannot be applied in such cases. We also propose\ncriteria and corresponding conditions to determine HESD type and estimate NN\ngeneralization potential. These HESD types and previously proposed\ngeneralization criteria are combined into a unified HESD analysis methodology.\nFinally, we discuss how HESD changes during training, and show the occurrence\nof quasi-singular (QS) HESD and its influence on the proposed methodology and\non the conventional assumptions about the relation between Hessian eigenvalues\nand NN loss landscape curvature."}
{"id": "2504.17304", "pdf": "https://arxiv.org/pdf/2504.17304", "abs": "https://arxiv.org/abs/2504.17304", "authors": ["Yimin Shi", "Yang Fei", "Shiqi Zhang", "Haixun Wang", "Xiaokui Xiao"], "title": "You Are What You Bought: Generating Customer Personas for E-commerce Applications", "categories": ["cs.IR", "cs.AI"], "comment": "SIGIR 2025", "summary": "In e-commerce, user representations are essential for various applications.\nExisting methods often use deep learning techniques to convert customer\nbehaviors into implicit embeddings. However, these embeddings are difficult to\nunderstand and integrate with external knowledge, limiting the effectiveness of\napplications such as customer segmentation, search navigation, and product\nrecommendations. To address this, our paper introduces the concept of the\ncustomer persona. Condensed from a customer's numerous purchasing histories, a\ncustomer persona provides a multi-faceted and human-readable characterization\nof specific purchase behaviors and preferences, such as Busy Parents or Bargain\nHunters.\n  This work then focuses on representing each customer by multiple personas\nfrom a predefined set, achieving readable and informative explicit user\nrepresentations. To this end, we propose an effective and efficient solution\nGPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs to infer\npersonas for customers. To reduce overhead, GPLR applies LLM-based labeling to\nonly a fraction of users and utilizes a random walk technique to predict\npersonas for the remaining customers. We further propose RevAff, which provides\nan absolute error $\\epsilon$ guarantee while improving the time complexity of\nthe exact solution by a factor of at least\n$O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$, where $N$ represents the number of\ncustomers and products, and $E$ represents the interactions between them. We\nevaluate the performance of our persona-based representation in terms of\naccuracy and robustness for recommendation and customer segmentation tasks\nusing three real-world e-commerce datasets. Most notably, we find that\nintegrating customer persona representations improves the state-of-the-art\ngraph convolution-based recommendation model by up to 12% in terms of NDCG@K\nand F1-Score@K."}
{"id": "2504.17641", "pdf": "https://arxiv.org/pdf/2504.17641", "abs": "https://arxiv.org/abs/2504.17641", "authors": ["Shengtao Zhang", "Haokai Zhang", "Shiqi Lou", "Zicheng Wang", "Zinan Zeng", "Yilin Wang", "Minnan Luo"], "title": "PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Dynamic node classification is critical for modeling evolving systems like\nfinancial transactions and academic collaborations. In such systems,\ndynamically capturing node information changes is critical for dynamic node\nclassification, which usually requires all labels at every timestamp. However,\nit is difficult to collect all dynamic labels in real-world scenarios due to\nhigh annotation costs and label uncertainty (e.g., ambiguous or delayed labels\nin fraud detection). In contrast, final timestamp labels are easier to obtain\nas they rely on complete temporal patterns and are usually maintained as a\nunique label for each user in many open platforms, without tracking the history\ndata. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum\nLearning), a pioneering method addressing label-limited dynamic node\nclassification where only final labels are available. PTCL introduces: (1) a\ntemporal decoupling architecture separating the backbone (learning time-aware\nrepresentations) and decoder (strictly aligned with final labels), which\ngenerate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that\nprioritizes pseudo-labels closer to the final timestamp by assigning them\nhigher weights using an exponentially decaying function. We contribute a new\nacademic dataset (CoOAG), capturing long-range research interest in dynamic\ngraph. Experiments across real-world scenarios demonstrate PTCL's consistent\nsuperiority over other methods adapted to this task. Beyond methodology, we\npropose a unified framework FLiD (Framework for Label-Limited Dynamic Node\nClassification), consisting of a complete preparation workflow, training\npipeline, and evaluation standards, and supporting various models and datasets.\nThe code can be found at https://github.com/3205914485/FLiD."}
{"id": "2504.17306", "pdf": "https://arxiv.org/pdf/2504.17306", "abs": "https://arxiv.org/abs/2504.17306", "authors": ["Meher Boulaabi", "Takwa Ben AÃ¯cha Gader", "Afef Kacem Echi", "Sameh Mbarek"], "title": "Advanced Segmentation of Diabetic Retinopathy Lesions Using DeepLabv3+", "categories": ["cs.CV", "cs.AI"], "comment": "This work was accepted at the ACS/IEEE International Conference on\n  Computer Systems and Applications (AICCSA) 2024", "summary": "To improve the segmentation of diabetic retinopathy lesions (microaneurysms,\nhemorrhages, exudates, and soft exudates), we implemented a binary segmentation\nmethod specific to each type of lesion. As post-segmentation, we combined the\nindividual model outputs into a single image to better analyze the lesion\ntypes. This approach facilitated parameter optimization and improved accuracy,\neffectively overcoming challenges related to dataset limitations and annotation\ncomplexity. Specific preprocessing steps included cropping and applying\ncontrast-limited adaptive histogram equalization to the L channel of the LAB\nimage. Additionally, we employed targeted data augmentation techniques to\nfurther refine the model's efficacy. Our methodology utilized the DeepLabv3+\nmodel, achieving a segmentation accuracy of 99%. These findings highlight the\nefficacy of innovative strategies in advancing medical image analysis,\nparticularly in the precise segmentation of diabetic retinopathy lesions. The\nIDRID dataset was utilized to validate and demonstrate the robustness of our\napproach."}
{"id": "2504.17655", "pdf": "https://arxiv.org/pdf/2504.17655", "abs": "https://arxiv.org/abs/2504.17655", "authors": ["Farhad Pourkamali-Anaraki"], "title": "Aerial Image Classification in Scarce and Unconstrained Environments via Conformal Prediction", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "17 pages, 5 figures, and 2 tables", "summary": "This paper presents a comprehensive empirical analysis of conformal\nprediction methods on a challenging aerial image dataset featuring diverse\nevents in unconstrained environments. Conformal prediction is a powerful\npost-hoc technique that takes the output of any classifier and transforms it\ninto a set of likely labels, providing a statistical guarantee on the coverage\nof the true label. Unlike evaluations on standard benchmarks, our study\naddresses the complexities of data-scarce and highly variable real-world\nsettings. We investigate the effectiveness of leveraging pretrained models\n(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to\ngenerate informative prediction sets. To further evaluate the impact of\ncalibration, we consider two parallel pipelines (with and without temperature\nscaling) and assess performance using two key metrics: empirical coverage and\naverage prediction set size. This setup allows us to systematically examine how\ncalibration choices influence the trade-off between reliability and efficiency.\nOur findings demonstrate that even with relatively small labeled samples and\nsimple nonconformity scores, conformal prediction can yield valuable\nuncertainty estimates for complex tasks. Moreover, our analysis reveals that\nwhile temperature scaling is often employed for calibration, it does not\nconsistently lead to smaller prediction sets, underscoring the importance of\ncareful consideration in its application. Furthermore, our results highlight\nthe significant potential of model compression techniques within the conformal\nprediction pipeline for deployment in resource-constrained environments. Based\non our observations, we advocate for future research to delve into the impact\nof noisy or ambiguous labels on conformal prediction performance and to explore\neffective model reduction strategies."}
{"id": "2504.17311", "pdf": "https://arxiv.org/pdf/2504.17311", "abs": "https://arxiv.org/abs/2504.17311", "authors": ["Yulia Otmakhova", "Hung Thinh Truong", "Rahmad Mahendra", "Zenan Zhai", "Rongxin Zhu", "Daniel Beck", "Jey Han Lau"], "title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors."}
{"id": "2504.17660", "pdf": "https://arxiv.org/pdf/2504.17660", "abs": "https://arxiv.org/abs/2504.17660", "authors": ["Julius Vetter", "Manuel Gloeckler", "Daniel Gedon", "Jakob H. Macke"], "title": "Effortless, Simulation-Efficient Bayesian Inference using Tabular Foundation Models", "categories": ["cs.LG"], "comment": null, "summary": "Simulation-based inference (SBI) offers a flexible and general approach to\nperforming Bayesian inference: In SBI, a neural network is trained on synthetic\ndata simulated from a model and used to rapidly infer posterior distributions\nfor observed data. A key goal for SBI is to achieve accurate inference with as\nfew simulations as possible, especially for expensive simulators. In this work,\nwe address this challenge by repurposing recent probabilistic foundation models\nfor tabular data: We show how tabular foundation models -- specifically TabPFN\n-- can be used as pre-trained autoregressive conditional density estimators for\nSBI. We propose Neural Posterior Estimation with Prior-data Fitted Networks\n(NPE-PF) and show that it is competitive with current SBI approaches in terms\nof accuracy for both benchmark tasks and two complex scientific inverse\nproblems. Crucially, it often substantially outperforms them in terms of\nsimulation efficiency, sometimes requiring orders of magnitude fewer\nsimulations. NPE-PF eliminates the need for inference network selection,\ntraining, and hyperparameter tuning. We also show that it exhibits superior\nrobustness to model misspecification and can be scaled to simulation budgets\nthat exceed the context size limit of TabPFN. NPE-PF provides a new direction\nfor SBI, where training-free, general-purpose inference models offer efficient,\neasy-to-use, and flexible solutions for a wide range of stochastic inverse\nproblems."}
{"id": "2504.17315", "pdf": "https://arxiv.org/pdf/2504.17315", "abs": "https://arxiv.org/abs/2504.17315", "authors": ["Zhanglin Wu", "Tengfei Song", "Ning Xie", "Weidong Zhang", "Pengfei Li", "Shuang Wu", "Chong Li", "Junhao Zhu", "Hao Yang"], "title": "DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model", "categories": ["cs.CV", "cs.AI"], "comment": "7 pages, 1 figures, 2 tables", "summary": "This paper presents the technical solution proposed by Huawei Translation\nService Center (HW-TSC) for the \"End-to-End Document Image Machine Translation\nfor Complex Layouts\" competition at the 19th International Conference on\nDocument Analysis and Recognition (DIMT25@ICDAR2025). Leveraging\nstate-of-the-art open-source large vision-language model (LVLM), we introduce a\ntraining framework that combines multi-task learning with perceptual\nchain-of-thought to develop a comprehensive end-to-end document translation\nsystem. During the inference phase, we apply minimum Bayesian decoding and\npost-processing strategies to further enhance the system's translation\ncapabilities. Our solution uniquely addresses both OCR-based and OCR-free\ndocument image translation tasks within a unified framework. This paper\nsystematically details the training methods, inference strategies, LVLM base\nmodels, training data, experimental setups, and results, demonstrating an\neffective approach to document image machine translation."}
{"id": "2504.17664", "pdf": "https://arxiv.org/pdf/2504.17664", "abs": "https://arxiv.org/abs/2504.17664", "authors": ["GrÃ©gory Bournassenko"], "title": "On Multivariate Financial Time Series Classification", "categories": ["cs.LG"], "comment": null, "summary": "This article investigates the use of Machine Learning and Deep Learning\nmodels in multivariate time series analysis within financial markets. It\ncompares small and big data approaches, focusing on their distinct challenges\nand the benefits of scaling. Traditional methods such as SVMs are contrasted\nwith modern architectures like ConvTimeNet. The results show the importance of\nusing and understanding Big Data in depth in the analysis and prediction of\nfinancial time series."}
{"id": "2504.17331", "pdf": "https://arxiv.org/pdf/2504.17331", "abs": "https://arxiv.org/abs/2504.17331", "authors": ["SÃ¼leyman Ãzdel", "Kadir Burak Buldu", "Enkelejda Kasneci", "Efe Bozkir"], "title": "Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality", "categories": ["cs.HC", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Locomotion plays a crucial role in shaping the user experience within virtual\nreality environments. In particular, hands-free locomotion offers a valuable\nalternative by supporting accessibility and freeing users from reliance on\nhandheld controllers. To this end, traditional speech-based methods often\ndepend on rigid command sets, limiting the naturalness and flexibility of\ninteraction. In this study, we propose a novel locomotion technique powered by\nlarge language models (LLMs), which allows users to navigate virtual\nenvironments using natural language with contextual awareness. We evaluate\nthree locomotion methods: controller-based teleportation, voice-based steering,\nand our language model-driven approach. Our evaluation measures include\neye-tracking data analysis, including explainable machine learning through SHAP\nanalysis as well as standardized questionnaires for usability, presence,\ncybersickness, and cognitive load to examine user attention and engagement. Our\nfindings indicate that the LLM-driven locomotion possesses comparable\nusability, presence, and cybersickness scores to established methods like\nteleportation, demonstrating its novel potential as a comfortable, natural\nlanguage-based, hands-free alternative. In addition, it enhances user attention\nwithin the virtual environment, suggesting greater engagement. Complementary to\nthese findings, SHAP analysis revealed that fixation, saccade, and\npupil-related features vary across techniques, indicating distinct patterns of\nvisual attention and cognitive processing. Overall, we state that our method\ncan facilitate hands-free locomotion in virtual spaces, especially in\nsupporting accessibility."}
{"id": "2504.17703", "pdf": "https://arxiv.org/pdf/2504.17703", "abs": "https://arxiv.org/abs/2504.17703", "authors": ["Edward Collins", "Michel Wang"], "title": "Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) has emerged as a transformative paradigm in the field\nof distributed machine learning, enabling multiple clients such as mobile\ndevices, edge nodes, or organizations to collaboratively train a shared global\nmodel without the need to centralize sensitive data. This decentralized\napproach addresses growing concerns around data privacy, security, and\nregulatory compliance, making it particularly attractive in domains such as\nhealthcare, finance, and smart IoT systems. This survey provides a concise yet\ncomprehensive overview of Federated Learning, beginning with its core\narchitecture and communication protocol. We discuss the standard FL lifecycle,\nincluding local training, model aggregation, and global updates. A particular\nemphasis is placed on key technical challenges such as handling non-IID\n(non-independent and identically distributed) data, mitigating system and\nhardware heterogeneity, reducing communication overhead, and ensuring privacy\nthrough mechanisms like differential privacy and secure aggregation.\nFurthermore, we examine emerging trends in FL research, including personalized\nFL, cross-device versus cross-silo settings, and integration with other\nparadigms such as reinforcement learning and quantum computing. We also\nhighlight real-world applications and summarize benchmark datasets and\nevaluation metrics commonly used in FL research. Finally, we outline open\nresearch problems and future directions to guide the development of scalable,\nefficient, and trustworthy FL systems."}
{"id": "2504.17346", "pdf": "https://arxiv.org/pdf/2504.17346", "abs": "https://arxiv.org/abs/2504.17346", "authors": ["Tran Thuy Nga Truong", "Jooyong Kim"], "title": "Dual-Individual Genetic Algorithm: A Dual-Individual Approach for Efficient Training of Multi-Layer Neural Networks", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "This paper introduces an enhanced Genetic Algorithm technique called\nDual-Individual Genetic Algorithm (Dual-Individual GA), which optimizes neural\nnetworks for binary image classification tasks, such as cat vs. non-cat\nclassification. The proposed method employs only two individuals for crossover,\nrepresented by two parameter sets: Leader and Follower. The Leader focuses on\nexploitation, representing the primary optimal solution at even-indexed\npositions (0, 2, 4, ...), while the Follower promotes exploration by preserving\ndiversity and avoiding premature convergence, operating at odd-indexed\npositions (1, 3, 5, ...). Leader and Follower are modeled as two phases or\nroles. The key contributions of this work are threefold: (1) a self-adaptive\nlayer dimension mechanism that eliminates the need for manual tuning of layer\narchitectures; (2) generates two parameter sets, leader and follower parameter\nsets, with 10 layer architecture configurations (5 for each set), ranked by\nPareto dominance and cost. post-optimization; and (3) demonstrated superior\nperformance compared to traditional gradient-based methods. Experimental\nresults show that the Dual-Individual GA achieves 99.04% training accuracy and\n80% testing accuracy (cost = 0.034) on a three-layer network with architecture\n[12288, 17, 4, 1], outperforming a gradient-based approach that achieves 98%\ntraining accuracy and 80% testing accuracy (cost = 0.092) on a four-layer\nnetwork with architecture [12288, 20, 7, 5, 1]. These findings highlight the\nefficiency and effectiveness of the proposed method in optimizing neural\nnetworks."}
{"id": "2504.17709", "pdf": "https://arxiv.org/pdf/2504.17709", "abs": "https://arxiv.org/abs/2504.17709", "authors": ["Stefan Jonas", "Angela Meyer"], "title": "Fault Diagnosis in New Wind Turbines using Knowledge from Existing Turbines by Generative Domain Adaptation", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Intelligent condition monitoring of wind turbines is essential for reducing\ndowntimes. Machine learning models trained on wind turbine operation data are\ncommonly used to detect anomalies and, eventually, operation faults. However,\ndata-driven normal behavior models (NBMs) require a substantial amount of\ntraining data, as NBMs trained with scarce data may result in unreliable fault\ndiagnosis. To overcome this limitation, we present a novel generative deep\nlearning approach to make SCADA samples from one wind turbine lacking training\ndata resemble SCADA data from wind turbines with representative training data.\nThrough CycleGAN-based domain mapping, our method enables the application of an\nNBM trained on an existing wind turbine to one with severely limited data. We\ndemonstrate our approach on field data mapping SCADA samples across 7\nsubstantially different WTs. Our findings show significantly improved fault\ndiagnosis in wind turbines with scarce data. Our method achieves the most\nsimilar anomaly scores to an NBM trained with abundant data, outperforming NBMs\ntrained on scarce training data with improvements of +10.3% in F1-score when 1\nmonth of training data is available and +16.8% when 2 weeks are available. The\ndomain mapping approach outperforms conventional fine-tuning at all considered\ndegrees of data scarcity, ranging from 1 to 8 weeks of training data. The\nproposed technique enables earlier and more reliable fault diagnosis in newly\ninstalled wind farms, demonstrating a novel and promising research direction to\nimprove anomaly detection when faced with training data scarcity."}
{"id": "2504.17354", "pdf": "https://arxiv.org/pdf/2504.17354", "abs": "https://arxiv.org/abs/2504.17354", "authors": ["Tarik Sahin", "Jacopo Bonari", "Sebastian Brandstaeter", "Alexander Popp"], "title": "Data-Driven Surrogate Modeling Techniques to Predict the Effective Contact Area of Rough Surface Contact Problems", "categories": ["cs.CE", "cs.AI"], "comment": null, "summary": "The effective contact area in rough surface contact plays a critical role in\nmulti-physics phenomena such as wear, sealing, and thermal or electrical\nconduction. Although accurate numerical methods, like the Boundary Element\nMethod (BEM), are available to compute this quantity, their high computational\ncost limits their applicability in multi-query contexts, such as uncertainty\nquantification, parameter identification, and multi-scale algorithms, where\nmany repeated evaluations are required. This study proposes a surrogate\nmodeling framework for predicting the effective contact area using\nfast-to-evaluate data-driven techniques. Various machine learning algorithms\nare trained on a precomputed dataset, where the inputs are the imposed load and\nstatistical roughness parameters, and the output is the corresponding effective\ncontact area. All models undergo hyperparameter optimization to enable fair\ncomparisons in terms of predictive accuracy and computational efficiency,\nevaluated using established quantitative metrics. Among the models, the Kernel\nRidge Regressor demonstrates the best trade-off between accuracy and\nefficiency, achieving high predictive accuracy, low prediction time, and\nminimal training overhead-making it a strong candidate for general-purpose\nsurrogate modeling. The Gaussian Process Regressor provides an attractive\nalternative when uncertainty quantification is required, although it incurs\nadditional computational cost due to variance estimation. The generalization\ncapability of the Kernel Ridge model is validated on an unseen simulation\nscenario, confirming its ability to transfer to new configurations. Database\ngeneration constitutes the dominant cost in the surrogate modeling process.\nNevertheless, the approach proves practical and efficient for multi-query\ntasks, even when accounting for this initial expense."}
{"id": "2504.17717", "pdf": "https://arxiv.org/pdf/2504.17717", "abs": "https://arxiv.org/abs/2504.17717", "authors": ["Ãscar Escudero-Arnanz", "Antonio G. Marques", "Inmaculada Mora-JimÃ©nez", "JoaquÃ­n Ãlvarez-RodrÃ­guez", "Cristina Soguero-Ruiz"], "title": "Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis and Interpretable Patient-Similarity Representations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Background and Objectives: Multidrug Resistance (MDR) is a critical global\nhealth issue, causing increased hospital stays, healthcare costs, and\nmortality. This study proposes an interpretable Machine Learning (ML) framework\nfor MDR prediction, aiming for both accurate inference and enhanced\nexplainability.\n  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing\nclinical progression and patient-to-patient interactions. Similarity among\npatients is quantified using MTS-based methods: descriptive statistics, Dynamic\nTime Warping, and Time Cluster Kernel. These similarity measures serve as\ninputs for MDR classification via Logistic Regression, Random Forest, and\nSupport Vector Machines, with dimensionality reduction and kernel\ntransformations improving model performance. For explainability, patient\nsimilarity networks are constructed from these metrics. Spectral clustering and\nt-SNE are applied to identify MDR-related subgroups and visualize high-risk\nclusters, enabling insight into clinically relevant patterns.\n  Results: The framework was validated on ICU Electronic Health Records from\nthe University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms\nbaseline ML and deep learning models by leveraging graph-based patient\nsimilarity. The approach identifies key risk factors -- prolonged antibiotic\nuse, invasive procedures, co-infections, and extended ICU stays -- and reveals\nclinically meaningful clusters. Code and results are available at\n\\https://github.com/oscarescuderoarnanz/DM4MTS.\n  Conclusions: Patient similarity representations combined with graph-based\nanalysis provide accurate MDR prediction and interpretable insights. This\nmethod supports early detection, risk factor identification, and patient\nstratification, highlighting the potential of explainable ML in critical care."}
{"id": "2504.17355", "pdf": "https://arxiv.org/pdf/2504.17355", "abs": "https://arxiv.org/abs/2504.17355", "authors": ["Xiaohan Huang", "Dongjie Wang", "Zhiyuan Ning", "Ziyue Qiao", "Qingqing Long", "Haowei Zhu", "Yi Du", "Min Wu", "Yuanchun Zhou", "Meng Xiao"], "title": "Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, Keywords: Automated Feature Transformation, Tabular\n  Dataset, Reinforcement Learning", "summary": "Feature transformation methods aim to find an optimal mathematical\nfeature-feature crossing process that generates high-value features and\nimproves the performance of downstream machine learning tasks. Existing\nframeworks, though designed to mitigate manual costs, often treat feature\ntransformations as isolated operations, ignoring dynamic dependencies between\ntransformation steps. To address the limitations, we propose TCTO, a\ncollaborative multi-agent reinforcement learning framework that automates\nfeature engineering through graph-driven path optimization. The framework's\ncore innovation lies in an evolving interaction graph that models features as\nnodes and transformations as edges. Through graph pruning and backtracking, it\ndynamically eliminates low-impact edges, reduces redundant operations, and\nenhances exploration stability. This graph also provides full traceability to\nempower TCTO to reuse high-utility subgraphs from historical transformations.\nTo demonstrate the efficacy and adaptability of our approach, we conduct\ncomprehensive experiments and case studies, which show superior performance\nacross a range of datasets."}
{"id": "2504.17721", "pdf": "https://arxiv.org/pdf/2504.17721", "abs": "https://arxiv.org/abs/2504.17721", "authors": ["Cheng Shen", "Yuewei Liu"], "title": "Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees", "categories": ["cs.LG", "cs.AI"], "comment": "Under Review", "summary": "In industrial settings, surface defects on steel can significantly compromise\nits service life and elevate potential safety risks. Traditional defect\ndetection methods predominantly rely on manual inspection, which suffers from\nlow efficiency and high costs. Although automated defect detection approaches\nbased on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly,\ntheir reliability remains challenged due to data annotation uncertainties\nduring deep model training and overfitting issues. These limitations may lead\nto detection deviations when processing the given new test samples, rendering\nautomated detection processes unreliable. To address this challenge, we first\nevaluate the detection model's practical performance through calibration data\nthat satisfies the independent and identically distributed (i.i.d) condition\nwith test data. Specifically, we define a loss function for each calibration\nsample to quantify detection error rates, such as the complement of recall rate\nand false discovery rate. Subsequently, we derive a statistically rigorous\nthreshold based on a user-defined risk level to identify high-probability\ndefective pixels in test images, thereby constructing prediction sets (e.g.,\ndefect regions). This methodology ensures that the expected error rate (mean\nerror rate) on the test set remains strictly bounced by the predefined risk\nlevel. Additionally, we observe a negative correlation between the average\nprediction set size and the risk level on the test set, establishing a\nstatistically rigorous metric for assessing detection model uncertainty.\nFurthermore, our study demonstrates robust and efficient control over the\nexpected test set error rate across varying calibration-to-test partitioning\nratios, validating the method's adaptability and operational effectiveness."}
{"id": "2504.17366", "pdf": "https://arxiv.org/pdf/2504.17366", "abs": "https://arxiv.org/abs/2504.17366", "authors": ["Yongxuan Wu", "Runyu Chen", "Peiyu Liu", "Hongjin Qian"], "title": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench."}
{"id": "2504.17723", "pdf": "https://arxiv.org/pdf/2504.17723", "abs": "https://arxiv.org/abs/2504.17723", "authors": ["Natan Levy", "Adiel Ashrov", "Guy Katz"], "title": "Towards Robust LLMs: an Adversarial Robustness Measurement Framework", "categories": ["cs.LG"], "comment": "17 pages, 5 figures", "summary": "The rise of Large Language Models (LLMs) has revolutionized artificial\nintelligence, yet these models remain vulnerable to adversarial perturbations,\nundermining their reliability in high-stakes applications. While adversarial\nrobustness in vision-based neural networks has been extensively studied, LLM\nrobustness remains under-explored. We adapt the Robustness Measurement and\nAssessment (RoMA) framework to quantify LLM resilience against adversarial\ninputs without requiring access to model parameters. By comparing RoMA's\nestimates to those of formal verification methods, we demonstrate its accuracy\nwith minimal error margins while maintaining computational efficiency. Our\nempirical evaluation reveals that robustness varies significantly not only\nbetween different models but also across categories within the same task and\nbetween various types of perturbations. This non-uniformity underscores the\nneed for task-specific robustness evaluations, enabling practitioners to\ncompare and select models based on application-specific robustness\nrequirements. Our work provides a systematic methodology to assess LLM\nrobustness, advancing the development of more reliable language models for\nreal-world deployment."}
{"id": "2504.17384", "pdf": "https://arxiv.org/pdf/2504.17384", "abs": "https://arxiv.org/abs/2504.17384", "authors": ["Hanlin Sheng", "Xinming Wu", "Hang Gao", "Haibin Di", "Sergey Fomel", "Jintao Li", "Xu Si"], "title": "On the workflow, opportunities and challenges of developing foundation model in geophysics", "categories": ["physics.geo-ph", "cs.AI"], "comment": null, "summary": "Foundation models, as a mainstream technology in artificial intelligence,\nhave demonstrated immense potential across various domains in recent years,\nparticularly in handling complex tasks and multimodal data. In the field of\ngeophysics, although the application of foundation models is gradually\nexpanding, there is currently a lack of comprehensive reviews discussing the\nfull workflow of integrating foundation models with geophysical data. To\naddress this gap, this paper presents a complete framework that systematically\nexplores the entire process of developing foundation models in conjunction with\ngeophysical data. From data collection and preprocessing to model architecture\nselection, pre-training strategies, and model deployment, we provide a detailed\nanalysis of the key techniques and methodologies at each stage. In particular,\nconsidering the diversity, complexity, and physical consistency constraints of\ngeophysical data, we discuss targeted solutions to address these challenges.\nFurthermore, we discuss how to leverage the transfer learning capabilities of\nfoundation models to reduce reliance on labeled data, enhance computational\nefficiency, and incorporate physical constraints into model training, thereby\nimproving physical consistency and interpretability. Through a comprehensive\nsummary and analysis of the current technological landscape, this paper not\nonly fills the gap in the geophysics domain regarding a full-process review of\nfoundation models but also offers valuable practical guidance for their\napplication in geophysical data analysis, driving innovation and advancement in\nthe field."}
{"id": "2504.17739", "pdf": "https://arxiv.org/pdf/2504.17739", "abs": "https://arxiv.org/abs/2504.17739", "authors": ["Lorenzo Simone", "Mauro Giuseppe Camporeale", "Vito Marco Rubino", "Vincenzo Gervasi", "Giovanni Dimauro"], "title": "Interpretable Early Detection of Parkinson's Disease through Speech Analysis", "categories": ["cs.LG"], "comment": null, "summary": "Parkinson's disease is a progressive neurodegenerative disorder affecting\nmotor and non-motor functions, with speech impairments among its earliest\nsymptoms. Speech impairments offer a valuable diagnostic opportunity, with\nmachine learning advances providing promising tools for timely detection. In\nthis research, we propose a deep learning approach for early Parkinson's\ndisease detection from speech recordings, which also highlights the vocal\nsegments driving predictions to enhance interpretability. This approach seeks\nto associate predictive speech patterns with articulatory features, providing a\nbasis for interpreting underlying neuromuscular impairments. We evaluated our\napproach using the Italian Parkinson's Voice and Speech Database, containing\n831 audio recordings from 65 participants, including both healthy individuals\nand patients. Our approach showed competitive classification performance\ncompared to state-of-the-art methods, while providing enhanced interpretability\nby identifying key speech features influencing predictions."}
{"id": "2504.17393", "pdf": "https://arxiv.org/pdf/2504.17393", "abs": "https://arxiv.org/abs/2504.17393", "authors": ["Vesna Nowack", "Dalal Alrajeh", "Carolina Gutierrez MuÃ±oz", "Katie Thomas", "William Hobson", "Catherine Hamilton-Giachritsis", "Patrick Benjamin", "Tim Grant", "Juliane A. Kloess", "Jessica Woodhams"], "title": "Towards User-Centred Design of AI-Assisted Decision-Making in Law Enforcement", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "10 pages, 1 figure", "summary": "Artificial Intelligence (AI) has become an important part of our everyday\nlives, yet user requirements for designing AI-assisted systems in law\nenforcement remain unclear. To address this gap, we conducted qualitative\nresearch on decision-making within a law enforcement agency. Our study aimed to\nidentify limitations of existing practices, explore user requirements and\nunderstand the responsibilities that humans expect to undertake in these\nsystems.\n  Participants in our study highlighted the need for a system capable of\nprocessing and analysing large volumes of data efficiently to help in crime\ndetection and prevention. Additionally, the system should satisfy requirements\nfor scalability, accuracy, justification, trustworthiness and adaptability to\nbe adopted in this domain. Participants also emphasised the importance of\nhaving end users review the input data that might be challenging for AI to\ninterpret, and validate the generated output to ensure the system's accuracy.\nTo keep up with the evolving nature of the law enforcement domain, end users\nneed to help the system adapt to the changes in criminal behaviour and\ngovernment guidance, and technical experts need to regularly oversee and\nmonitor the system. Furthermore, user-friendly human interaction with the\nsystem is essential for its adoption and some of the participants confirmed\nthey would be happy to be in the loop and provide necessary feedback that the\nsystem can learn from. Finally, we argue that it is very unlikely that the\nsystem will ever achieve full automation due to the dynamic and complex nature\nof the law enforcement domain."}
{"id": "2504.17740", "pdf": "https://arxiv.org/pdf/2504.17740", "abs": "https://arxiv.org/abs/2504.17740", "authors": ["Mingchen Jiang", "Peng Xu", "Xichen Ye", "Xiaohui Chen", "Yun Yang", "Yifan Chen"], "title": "Embedding Empirical Distributions for Computing Optimal Transport Maps", "categories": ["cs.LG"], "comment": null, "summary": "Distributional data have become increasingly prominent in modern signal\nprocessing, highlighting the necessity of computing optimal transport (OT) maps\nacross multiple probability distributions. Nevertheless, recent studies on\nneural OT methods predominantly focused on the efficient computation of a\nsingle map between two distributions. To address this challenge, we introduce a\nnovel approach to learning transport maps for new empirical distributions.\nSpecifically, we employ the transformer architecture to produce embeddings from\ndistributional data of varying length; these embeddings are then fed into a\nhypernetwork to generate neural OT maps. Various numerical experiments were\nconducted to validate the embeddings and the generated OT maps. The model\nimplementation and the code are provided on\nhttps://github.com/jiangmingchen/HOTET."}
{"id": "2504.17401", "pdf": "https://arxiv.org/pdf/2504.17401", "abs": "https://arxiv.org/abs/2504.17401", "authors": ["Xu Wang", "Jialang Xu", "Shuai Zhang", "Baoru Huang", "Danail Stoyanov", "Evangelos B. Mazomenos"], "title": "StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Stereo disparity estimation is crucial for obtaining depth information in\nrobot-assisted minimally invasive surgery (RAMIS). While current deep learning\nmethods have made significant advancements, challenges remain in achieving an\noptimal balance between accuracy, robustness, and inference speed. To address\nthese challenges, we propose the StereoMamba architecture, which is\nspecifically designed for stereo disparity estimation in RAMIS. Our approach is\nbased on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances\nlong-range spatial dependencies both within and across stereo images. To\neffectively integrate multi-scale features from FE-Mamba, we then introduce a\nnovel Multidimensional Feature Fusion (MFF) module. Experiments against the\nstate-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba\nachieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the\nsecond-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining\nan inference speed of 21.28 FPS for a pair of high-resolution images\n(1280*1024), striking the optimum balance between accuracy, robustness, and\nefficiency. Furthermore, by comparing synthesized right images, generated from\nwarping left images using the generated disparity maps, with the actual right\nimage, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),\nexhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS\ndatasets."}
{"id": "2504.17749", "pdf": "https://arxiv.org/pdf/2504.17749", "abs": "https://arxiv.org/abs/2504.17749", "authors": ["Steven E. Wilson", "Sina Khanmohammadi"], "title": "MSGCN: Multiplex Spatial Graph Convolution Network for Interlayer Link Weight Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have been widely used for various learning\ntasks, ranging from node classification to link prediction. They have\ndemonstrated excellent performance in multiple domains involving\ngraph-structured data. However, an important category of learning tasks, namely\nlink weight prediction, has received less emphasis due to its increased\ncomplexity compared to binary link classification. Link weight prediction\nbecomes even more challenging when considering multilayer networks, where nodes\ncan be interconnected across multiple layers. To address these challenges, we\npropose a new method named Multiplex Spatial Graph Convolution Network (MSGCN),\nwhich spatially embeds information across multiple layers to predict interlayer\nlink weights. The MSGCN model generalizes spatial graph convolution to\nmultiplex networks and captures the geometric structure of nodes across\nmultiple layers. Extensive experiments using data with known interlayer link\ninformation show that the MSGCN model has robust, accurate, and generalizable\nlink weight prediction performance across a wide variety of multiplex network\nstructures."}
{"id": "2504.17421", "pdf": "https://arxiv.org/pdf/2504.17421", "abs": "https://arxiv.org/abs/2504.17421", "authors": ["Yang Liu", "Bingjie Yan", "Tianyuan Zou", "Jianqing Zhang", "Zixuan Gu", "Jianbing Ding", "Xidong Wang", "Jingyi Li", "Xiaozhou Ye", "Ye Ouyang", "Qiang Yang", "Ya-Qin Zhang"], "title": "Towards Harnessing the Collaborative Power of Large and Small Models for Domain Tasks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nthey require vast amounts of data and computational resources. In contrast,\nsmaller models (SMs), while less powerful, can be more efficient and tailored\nto specific domains. In this position paper, we argue that taking a\ncollaborative approach, where large and small models work synergistically, can\naccelerate the adaptation of LLMs to private domains and unlock new potential\nin AI. We explore various strategies for model collaboration and identify\npotential challenges and opportunities. Building upon this, we advocate for\nindustry-driven research that prioritizes multi-objective benchmarks on\nreal-world private datasets and applications."}
{"id": "2504.17752", "pdf": "https://arxiv.org/pdf/2504.17752", "abs": "https://arxiv.org/abs/2504.17752", "authors": ["Zhihui Gao", "Sri Krishna Vadlamani", "Kfir Sulimany", "Dirk Englund", "Tingjun Chen"], "title": "Disaggregated Deep Learning via In-Physics Computing at Radio Frequency", "categories": ["cs.LG", "cs.ET", "eess.SP", "physics.app-ph"], "comment": "11 pages, 4 figures. Supplementary Information: 54 pages, 20 figures,\n  1 table", "summary": "Modern edge devices, such as cameras, drones, and Internet-of-Things nodes,\nrely on deep learning to enable a wide range of intelligent applications,\nincluding object recognition, environment perception, and autonomous\nnavigation. However, deploying deep learning models directly on the often\nresource-constrained edge devices demands significant memory footprints and\ncomputational power for real-time inference using traditional digital computing\narchitectures. In this paper, we present WISE, a novel computing architecture\nfor wireless edge networks designed to overcome energy constraints in deep\nlearning inference. WISE achieves this goal through two key innovations:\ndisaggregated model access via wireless broadcasting and in-physics computation\nof general complex-valued matrix-vector multiplications directly at radio\nfrequency. Using a software-defined radio platform with wirelessly broadcast\nmodel weights over the air, we demonstrate that WISE achieves 95.7% image\nclassification accuracy with ultra-low operation power of 6.0 fJ/MAC per\nclient, corresponding to a computation efficiency of 165.8 TOPS/W. This\napproach enables energy-efficient deep learning inference on wirelessly\nconnected edge devices, achieving more than two orders of magnitude improvement\nin efficiency compared to traditional digital computing."}
{"id": "2504.17424", "pdf": "https://arxiv.org/pdf/2504.17424", "abs": "https://arxiv.org/abs/2504.17424", "authors": ["Tomoki Mizuno", "Kazuya Yabashi", "Tsuyoshi Tasaki"], "title": "Object Pose Estimation by Camera Arm Control Based on the Next Viewpoint Estimation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We have developed a new method to estimate a Next Viewpoint (NV) which is\neffective for pose estimation of simple-shaped products for product display\nrobots in retail stores. Pose estimation methods using Neural Networks (NN)\nbased on an RGBD camera are highly accurate, but their accuracy significantly\ndecreases when the camera acquires few texture and shape features at a current\nview point. However, it is difficult for previous mathematical model-based\nmethods to estimate effective NV which is because the simple shaped objects\nhave few shape features. Therefore, we focus on the relationship between the\npose estimation and NV estimation. When the pose estimation is more accurate,\nthe NV estimation is more accurate. Therefore, we develop a new pose estimation\nNN that estimates NV simultaneously. Experimental results showed that our NV\nestimation realized a pose estimation success rate 77.3\\%, which was 7.4pt\nhigher than the mathematical model-based NV calculation did. Moreover, we\nverified that the robot using our method displayed 84.2\\% of products."}
{"id": "2504.17780", "pdf": "https://arxiv.org/pdf/2504.17780", "abs": "https://arxiv.org/abs/2504.17780", "authors": ["Sneh Pillai"], "title": "Replay to Remember: Retaining Domain Knowledge in Streaming Language Models", "categories": ["cs.LG"], "comment": "8 pages 3 figures, 3 tables", "summary": "Continual learning in large language models (LLMs) typically encounters the\ncritical challenge of catastrophic forgetting, where previously acquired\nknowledge deteriorates upon exposure to new data. While techniques like replay\nbuffers and parameter-efficient tuning (e.g., Low-Rank Adaptation or LoRA) have\nbeen proposed, few studies investigate real-time domain adaptation under strict\ncomputational and data-stream constraints. In this paper, we demonstrate a\nlightweight method combining LoRA and a minimal replay mechanism in a realistic\nstreaming setting across three diverse knowledge domains: medical question\nanswering, genetics, and law. Using perplexity, semantic similarity, and\nGPT-based human-like evaluation metrics, we quantify the model's adaptation,\nforgetting, and recovery over time. Our experiments reveal that while\ncatastrophic forgetting naturally occurs, even minimal replay significantly\nstabilizes and partially restores domain-specific knowledge. This study\ncontributes practical insights for deploying adaptable LLMs in\nresource-constrained, real-world scenarios."}
{"id": "2504.17426", "pdf": "https://arxiv.org/pdf/2504.17426", "abs": "https://arxiv.org/abs/2504.17426", "authors": ["Michele Carissimi", "Martina Saletta", "Claudio Ferretti"], "title": "Towards Leveraging Large Language Model Summaries for Topic Modeling in Source Code", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Understanding source code is a topic of great interest in the software\nengineering community, since it can help programmers in various tasks such as\nsoftware maintenance and reuse. Recent advances in large language models (LLMs)\nhave demonstrated remarkable program comprehension capabilities, while\ntransformer-based topic modeling techniques offer effective ways to extract\nsemantic information from text. This paper proposes and explores a novel\napproach that combines these strengths to automatically identify meaningful\ntopics in a corpus of Python programs. Our method consists in applying topic\nmodeling on the descriptions obtained by asking an LLM to summarize the code.\nTo assess the internal consistency of the extracted topics, we compare them\nagainst topics inferred from function names alone, and those derived from\nexisting docstrings. Experimental results suggest that leveraging LLM-generated\nsummaries provides interpretable and semantically rich representation of code\nstructure. The promising results suggest that our approach can be fruitfully\napplied in various software engineering tasks such as automatic documentation\nand tagging, code search, software reorganization and knowledge discovery in\nlarge repositories."}
{"id": "2504.16941", "pdf": "https://arxiv.org/pdf/2504.16941", "abs": "https://arxiv.org/abs/2504.16941", "authors": ["Zakaria Lamine", "Abdelatif Hafid", "Mohamed Rahouti"], "title": "Mathematical Modeling of Protein Structures: A Cohomology-Based Approach to the Flagellar Motor", "categories": ["q-bio.BM", "cs.LG", "math.AT"], "comment": null, "summary": "This study presents a novel mathematical model derived from cohomology,\nleveraging the KEEL-proven theorem that establishes cohomology as tautological,\ngenerated by boundary classes of curves with fixed dual graphs. Simplicial\ncomplexes are constructed using skew-commutative graded algebra, and the\nstructure theorem is applied to connect distinct homologies, enabling precise\ninterpretations of the resulting geometric forms. The proposed model is\nutilized for protein structure analysis and prediction, with a specific\napplication to the Flagellar Motor structure. This approach offers new insights\ninto the geometric and algebraic foundations of biological macromolecular\nmodeling, highlighting its potential for advancement in structural biology."}
{"id": "2504.17428", "pdf": "https://arxiv.org/pdf/2504.17428", "abs": "https://arxiv.org/abs/2504.17428", "authors": ["Murali Sridharan", "Mika MÃ¤ntylÃ¤", "Leevi Rantala"], "title": "Detection, Classification and Prevalence of Self-Admitted Aging Debt", "categories": ["cs.SE", "cs.AI", "cs.CE", "cs.GL", "D.2.7; D.2.9"], "comment": "Draft", "summary": "Context: Previous research on software aging is limited with focus on dynamic\nruntime indicators like memory and performance, often neglecting evolutionary\nindicators like source code comments and narrowly examining legacy issues\nwithin the TD context. Objective: We introduce the concept of Aging Debt (AD),\nrepresenting the increased maintenance efforts and costs needed to keep\nsoftware updated. We study AD through Self-Admitted Aging Debt (SAAD) observed\nin source code comments left by software developers. Method: We employ a\nmixed-methods approach, combining qualitative and quantitative analyses to\ndetect and measure AD in software. This includes framing SAAD patterns from the\nsource code comments after analysing the source code context, then utilizing\nthe SAAD patterns to detect SAAD comments. In the process, we develop a\ntaxonomy for SAAD that reflects the temporal aging of software and its\nassociated debt. Then we utilize the taxonomy to quantify the different types\nof AD prevalent in OSS repositories. Results: Our proposed taxonomy categorizes\ntemporal software aging into Active and Dormant types. Our extensive analysis\nof over 9,000+ Open Source Software (OSS) repositories reveals that more than\n21% repositories exhibit signs of SAAD as observed from our gold standard SAAD\ndataset. Notably, Dormant AD emerges as the predominant category, highlighting\na critical but often overlooked aspect of software maintenance. Conclusion: As\nsoftware volume grows annually, so do evolutionary aging and maintenance\nchallenges; our proposed taxonomy can aid researchers in detailed software\naging studies and help practitioners develop improved and proactive maintenance\nstrategies."}
{"id": "2504.16943", "pdf": "https://arxiv.org/pdf/2504.16943", "abs": "https://arxiv.org/abs/2504.16943", "authors": ["Chiara Fusar Bassini", "Alice Lixuan Xu", "Jorge SÃ¡nchez Canales", "Lion Hirth", "Lynn H. Kaack"], "title": "Flexibility of German gas-fired generation: evidence from clustering empirical operation", "categories": ["cs.CY", "cs.LG"], "comment": "29 pages, 6 figures, 6 tables", "summary": "A key input to energy models are assumptions about the flexibility of power\ngeneration units, i.e., how quickly and often they can start up. These\nassumptions are usually calibrated on the technical characteristics of the\nunits, such as installed capacity or technology type. However, even if power\ngeneration units technically can dispatch flexibly, service obligations and\nmarket incentives may constrain their operation. Here, we cluster over 60% of\nGerman national gas generation (generation units of 100 MWp or above) based on\ntheir empirical flexibility. We process the hourly dispatch of sample units\nbetween 2019 and 2023 using a novel deep learning approach, that transforms\ntime series into easy-to-cluster representations. We identify two clusters of\npeaker units and two clusters of non-peaker units, whose different empirical\nflexibility is quantified by cluster-level ramp rates. Non-peaker units, around\nhalf of the sample, are empirically less flexible than peakers, and make up for\nmore than 83% of sample must-run generation. Regulatory changes addressing the\nlow market responsiveness of non-peakers are needed to unlock their\nflexibility."}
{"id": "2504.17447", "pdf": "https://arxiv.org/pdf/2504.17447", "abs": "https://arxiv.org/abs/2504.17447", "authors": ["De-An Huang", "Subhashree Radhakrishnan", "Zhiding Yu", "Jan Kautz"], "title": "FRAG: Frame Selection Augmented Generation for Long Video and Long Document Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "There has been impressive progress in Large Multimodal Models (LMMs). Recent\nworks extend these models to long inputs, including multi-page documents and\nlong videos. However, the model size and performance of these long context\nmodels are still limited due to the computational cost in both training and\ninference. In this work, we explore an orthogonal direction and process long\ninputs without long context LMMs. We propose Frame Selection Augmented\nGeneration (FRAG), where the model first selects relevant frames within the\ninput, and then only generates the final outputs based on the selected frames.\nThe core of the selection process is done by scoring each frame independently,\nwhich does not require long context processing. The frames with the highest\nscores are then selected by a simple Top-K selection. We show that this\nfrustratingly simple framework is applicable to both long videos and multi-page\ndocuments using existing LMMs without any fine-tuning. We consider two models,\nLLaVA-OneVision and InternVL2, in our experiments and show that FRAG\nconsistently improves the performance and achieves state-of-the-art\nperformances for both long video and long document understanding. For videos,\nFRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on\nVideo-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA\ncompared with recent LMMs specialized in long document understanding. Code is\navailable at: https://github.com/NVlabs/FRAG"}
{"id": "2504.16956", "pdf": "https://arxiv.org/pdf/2504.16956", "abs": "https://arxiv.org/abs/2504.16956", "authors": ["Cong Qi", "Hanzhang Fang", "Tianxing Hu", "Siqi Jiang", "Wei Zhi"], "title": "Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity", "categories": ["cs.CL", "cs.LG", "q-bio.GN"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of\ncellular heterogeneity, but its complexity, which is marked by high\ndimensionality, sparsity, and batch effects, which poses major computational\nchallenges. Transformer-based models have made significant advances in this\ndomain but are often limited by their quadratic complexity and suboptimal\nhandling of long-range dependencies. In this work, we introduce GeneMamba, a\nscalable and efficient foundation model for single-cell transcriptomics built\non state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba\ncaptures bidirectional gene context with linear-time complexity, offering\nsubstantial computational gains over transformer baselines. The model is\npretrained on nearly 30 million cells and incorporates biologically informed\nobjectives, including pathway-aware contrastive loss and rank-based gene\nencoding. We evaluate GeneMamba across diverse tasks, including multi-batch\nintegration, cell type annotation, and gene-gene correlation, demonstrating\nstrong performance, interpretability, and robustness. These results position\nGeneMamba as a practical and powerful alternative to transformer-based methods,\nadvancing the development of biologically grounded, scalable tools for\nlarge-scale single-cell data analysis."}
{"id": "2504.17449", "pdf": "https://arxiv.org/pdf/2504.17449", "abs": "https://arxiv.org/abs/2504.17449", "authors": ["Jun Zhang", "Jue Wang", "Huan Li", "Lidan Shou", "Ke Chen", "Gang Chen", "Qin Xie", "Guiming Xie", "Xuejian Gong"], "title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by VLDBJ 2025", "summary": "The significant computational demands of pretrained language models (PLMs),\nwhich often require dedicated hardware, present a substantial challenge in\nserving them efficiently, especially in multi-tenant environments. To address\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\nInference system, designed to manage tenants with distinct PLMs\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\nknowledge into general, domain-specific, and task-specific. Leveraging insights\non knowledge acquisition across different model layers, we construct\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\nestablish hierarchical knowledge management for hPLMs generated by various\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\nincreases by constructing and updating domain-specific knowledge trees based on\nfrequency. We manage task-specific knowledge within limited GPU memory through\nparameter swapping. Finally, we propose system optimizations to enhance\nresource utilization and inference throughput. These include fine-grained\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\noperations with GPU computations, and optimizing parallel implementations with\nbatched matrix multiplications. Our experimental results demonstrate that the\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\nsingle GPU, with only a negligible compromise in accuracy."}
{"id": "2504.16969", "pdf": "https://arxiv.org/pdf/2504.16969", "abs": "https://arxiv.org/abs/2504.16969", "authors": ["Mathias Hanson", "Gregory Lewkowicz", "Sam Verboven"], "title": "Engineering the Law-Machine Learning Translation Problem: Developing Legally Aligned Models", "categories": ["cs.CY", "cs.LG", "I.2.m; K.5.0"], "comment": "16 pages, 1 figure", "summary": "Organizations developing machine learning-based (ML) technologies face the\ncomplex challenge of achieving high predictive performance while respecting the\nlaw. This intersection between ML and the law creates new complexities. As ML\nmodel behavior is inferred from training data, legal obligations cannot be\noperationalized in source code directly. Rather, legal obligations require\n\"indirect\" operationalization. However, choosing context-appropriate\noperationalizations presents two compounding challenges: (1) laws often permit\nmultiple valid operationalizations for a given legal obligation-each with\nvarying degrees of legal adequacy; and, (2) each operationalization creates\nunpredictable trade-offs among the different legal obligations and with\npredictive performance. Evaluating these trade-offs requires metrics (or\nheuristics), which are in turn difficult to validate against legal obligations.\nCurrent methodologies fail to fully address these interwoven challenges as they\neither focus on legal compliance for traditional software or on ML model\ndevelopment without adequately considering legal complexities. In response, we\nintroduce a five-stage interdisciplinary framework that integrates legal and\nML-technical analysis during ML model development. This framework facilitates\ndesigning ML models in a legally aligned way and identifying high-performing\nmodels that are legally justifiable. Legal reasoning guides choices for\noperationalizations and evaluation metrics, while ML experts ensure technical\nfeasibility, performance optimization and an accurate interpretation of metric\nvalues. This framework bridges the gap between more conceptual analysis of law\nand ML models' need for deterministic specifications. We illustrate its\napplication using a case study in the context of anti-money laundering."}
{"id": "2504.17461", "pdf": "https://arxiv.org/pdf/2504.17461", "abs": "https://arxiv.org/abs/2504.17461", "authors": ["Vipin Singh", "Tianheng Ling", "Teodor Chiaburu", "Felix Biessmann"], "title": "Evaluating Time Series Models for Urban Wastewater Management: Predictive Performance, Model Complexity and Resilience", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, 6 figures, accepted at 10th International Conference on\n  Smart and Sustainable Technologies (SpliTech) 2025, GitHub:\n  https://github.com/calgo-lab/resilient-timeseries-evaluation", "summary": "Climate change increases the frequency of extreme rainfall, placing a\nsignificant strain on urban infrastructures, especially Combined Sewer Systems\n(CSS). Overflows from overburdened CSS release untreated wastewater into\nsurface waters, posing environmental and public health risks. Although\ntraditional physics-based models are effective, they are costly to maintain and\ndifficult to adapt to evolving system dynamics. Machine Learning (ML)\napproaches offer cost-efficient alternatives with greater adaptability. To\nsystematically assess the potential of ML for modeling urban infrastructure\nsystems, we propose a protocol for evaluating Neural Network architectures for\nCSS time series forecasting with respect to predictive performance, model\ncomplexity, and robustness to perturbations. In addition, we assess model\nperformance on peak events and critical fluctuations, as these are the key\nregimes for urban wastewater management. To investigate the feasibility of\nlightweight models suitable for IoT deployment, we compare global models, which\nhave access to all information, with local models, which rely solely on nearby\nsensor readings. Additionally, to explore the security risks posed by network\noutages or adversarial attacks on urban infrastructure, we introduce error\nmodels that assess the resilience of models. Our results demonstrate that while\nglobal models achieve higher predictive performance, local models provide\nsufficient resilience in decentralized scenarios, ensuring robust modeling of\nurban infrastructure. Furthermore, models with longer native forecast horizons\nexhibit greater robustness to data perturbations. These findings contribute to\nthe development of interpretable and reliable ML solutions for sustainable\nurban wastewater management. The implementation is available in our GitHub\nrepository."}
{"id": "2504.17006", "pdf": "https://arxiv.org/pdf/2504.17006", "abs": "https://arxiv.org/abs/2504.17006", "authors": ["Jalal Arabneydi", "Saiful Islam", "Srijita Das", "Sai Krishna Gottipati", "William Duguay", "Cloderic Mars", "Matthew E. Taylor", "Matthew Guzdial", "Antoine Fagette", "Younes Zerouali"], "title": "A Systematic Approach to Design Real-World Human-in-the-Loop Deep Reinforcement Learning: Salient Features, Challenges and Trade-offs", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": "This is a result of the collaboration by JACOBB, AMII(Alberta Machine\n  Intelligence Institute), Thales and AI Redefined (AIR) in 2021-2023", "summary": "With the growing popularity of deep reinforcement learning (DRL),\nhuman-in-the-loop (HITL) approach has the potential to revolutionize the way we\napproach decision-making problems and create new opportunities for human-AI\ncollaboration. In this article, we introduce a novel multi-layered hierarchical\nHITL DRL algorithm that comprises three types of learning: self learning,\nimitation learning and transfer learning. In addition, we consider three forms\nof human inputs: reward, action and demonstration. Furthermore, we discuss main\nchallenges, trade-offs and advantages of HITL in solving complex problems and\nhow human information can be integrated in the AI solution systematically. To\nverify our technical results, we present a real-world unmanned aerial vehicles\n(UAV) problem wherein a number of enemy drones attack a restricted area. The\nobjective is to design a scalable HITL DRL algorithm for ally drones to\nneutralize the enemy drones before they reach the area. To this end, we first\nimplement our solution using an award-winning open-source HITL software called\nCogment. We then demonstrate several interesting results such as (a) HITL leads\nto faster training and higher performance, (b) advice acts as a guiding\ndirection for gradient methods and lowers variance, and (c) the amount of\nadvice should neither be too large nor too small to avoid over-training and\nunder-training. Finally, we illustrate the role of human-AI cooperation in\nsolving two real-world complex scenarios, i.e., overloaded and decoy attacks."}
{"id": "2504.17471", "pdf": "https://arxiv.org/pdf/2504.17471", "abs": "https://arxiv.org/abs/2504.17471", "authors": ["Yacine Belal", "Mohamed Maouche", "Sonia Ben Mokhtar", "Anthony Simonet-Boulogne"], "title": "GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Gossip Learning (GL) is a decentralized learning paradigm where users\niteratively exchange and aggregate models with a small set of neighboring\npeers. Recent GL approaches rely on dynamic communication graphs built and\nmaintained using Random Peer Sampling (RPS) protocols. Thanks to graph\ndynamics, GL can achieve fast convergence even over extremely sparse\ntopologies. However, the robustness of GL over dy- namic graphs to Byzantine\n(model poisoning) attacks remains unaddressed especially when Byzantine nodes\nattack the RPS protocol to scale up model poisoning. We address this issue by\nintroducing GRANITE, a framework for robust learning over sparse, dynamic\ngraphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two\nkey components (i) a History-aware Byzantine-resilient Peer Sampling protocol\n(HaPS), which tracks previously encountered identifiers to reduce adversarial\ninfluence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which\nleverages an estimate of Byzantine presence to set aggregation thresholds with\nformal guarantees. Empirical results confirm that GRANITE maintains convergence\nwith up to 30% Byzantine nodes, improves learning speed via adaptive filtering\nof poisoned models and obtains these results in up to 9 times sparser graphs\nthan dictated by current theory."}
{"id": "2504.17017", "pdf": "https://arxiv.org/pdf/2504.17017", "abs": "https://arxiv.org/abs/2504.17017", "authors": ["Balaji Rao", "William Eiers", "Carlo Lipizzi"], "title": "Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification", "categories": ["cs.AI", "cs.FL", "cs.LG", "cs.LO"], "comment": "Accepted to the Proceedings of the 19th Conference on Neurosymbolic\n  Learning and Reasoning (NeSy 2025)", "summary": "Formally verifying properties of software code has been a highly desirable\ntask, especially with the emergence of LLM-generated code. In the same vein,\nthey provide an interesting avenue for the exploration of formal verification\nand mechanistic interpretability. Since the introduction of code-specific\nmodels, despite their successes in generating code in Lean4 and Isabelle, the\ntask of generalized theorem proving still remains far from being fully solved\nand will be a benchmark for reasoning capability in LLMs. In this work, we\nintroduce a framework that generates whole proofs in a formal language to be\nused within systems that utilize the power of built-in tactics and\noff-the-shelf automated theorem provers. Our framework includes 3 components:\ngenerating natural language statements of the code to be verified, an LLM that\ngenerates formal proofs for the given statement, and a module employing\nheuristics for building the final proof. To train the LLM, we employ a 2-stage\nfine-tuning process, where we first use SFT-based training to enable the model\nto generate syntactically correct Isabelle code and then RL-based training that\nencourages the model to generate proofs verified by a theorem prover. We\nvalidate our framework using the miniF2F-test benchmark and the Isabelle proof\nassistant and design a use case to verify the correctness of the AWS S3 bucket\naccess policy code. We also curate a dataset based on the\nFVEL\\textsubscript{\\textnormal{ER}} dataset for future training tasks."}
{"id": "2504.17474", "pdf": "https://arxiv.org/pdf/2504.17474", "abs": "https://arxiv.org/abs/2504.17474", "authors": ["Weiran Pan", "Wei Wei", "Feida Zhu", "Yong Deng"], "title": "Enhanced Sample Selection with Confidence Tracking: Identifying Correctly Labeled yet Hard-to-Learn Samples in Noisy Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a novel sample selection method for image classification in the\npresence of noisy labels. Existing methods typically consider small-loss\nsamples as correctly labeled. However, some correctly labeled samples are\ninherently difficult for the model to learn and can exhibit high loss similar\nto mislabeled samples in the early stages of training. Consequently, setting a\nthreshold on per-sample loss to select correct labels results in a trade-off\nbetween precision and recall in sample selection: a lower threshold may miss\nmany correctly labeled hard-to-learn samples (low recall), while a higher\nthreshold may include many mislabeled samples (low precision). To address this\nissue, our goal is to accurately distinguish correctly labeled yet\nhard-to-learn samples from mislabeled ones, thus alleviating the trade-off\ndilemma. We achieve this by considering the trends in model prediction\nconfidence rather than relying solely on loss values. Empirical observations\nshow that only for correctly labeled samples, the model's prediction confidence\nfor the annotated labels typically increases faster than for any other classes.\nBased on this insight, we propose tracking the confidence gaps between the\nannotated labels and other classes during training and evaluating their trends\nusing the Mann-Kendall Test. A sample is considered potentially correctly\nlabeled if all its confidence gaps tend to increase. Our method functions as a\nplug-and-play component that can be seamlessly integrated into existing sample\nselection techniques. Experiments on several standard benchmarks and real-world\ndatasets demonstrate that our method enhances the performance of existing\nmethods for learning with noisy labels."}
{"id": "2504.17044", "pdf": "https://arxiv.org/pdf/2504.17044", "abs": "https://arxiv.org/abs/2504.17044", "authors": ["Dhari Gandhi", "Himanshu Joshi", "Lucas Hartman", "Shabnam Hassani"], "title": "Approaches to Responsible Governance of GenAI in Organizations", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid evolution of Generative AI (GenAI) has introduced unprecedented\nopportunities while presenting complex challenges around ethics,\naccountability, and societal impact. This paper draws on a literature review,\nestablished governance frameworks, and industry roundtable discussions to\nidentify core principles for integrating responsible GenAI governance into\ndiverse organizational structures. Our objective is to provide actionable\nrecommendations for a balanced, risk-based governance approach that enables\nboth innovation and oversight. Findings emphasize the need for adaptable risk\nassessment tools, continuous monitoring practices, and cross-sector\ncollaboration to establish trustworthy GenAI. These insights provide a\nstructured foundation and Responsible GenAI Guide (ResAI) for organizations to\nalign GenAI initiatives with ethical, legal, and operational best practices."}
{"id": "2504.17490", "pdf": "https://arxiv.org/pdf/2504.17490", "abs": "https://arxiv.org/abs/2504.17490", "authors": ["Mingqi Yuan", "Qi Wang", "Guozheng Ma", "Bo Li", "Xin Jin", "Yunbo Wang", "Xiaokang Yang", "Wenjun Zeng", "Dacheng Tao"], "title": "Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages", "summary": "Developing lifelong learning agents is crucial for artificial general\nintelligence. However, deep reinforcement learning (RL) systems often suffer\nfrom plasticity loss, where neural networks gradually lose their ability to\nadapt during training. Despite its significance, this field lacks unified\nbenchmarks and evaluation protocols. We introduce Plasticine, the first\nopen-source framework for benchmarking plasticity optimization in deep RL.\nPlasticine provides single-file implementations of over 13 mitigation methods,\n10 evaluation metrics, and learning scenarios with increasing non-stationarity\nlevels from standard to open-ended environments. This framework enables\nresearchers to systematically quantify plasticity loss, evaluate mitigation\nstrategies, and analyze plasticity dynamics across different contexts. Our\ndocumentation, examples, and source code are available at\nhttps://github.com/RLE-Foundation/Plasticine."}
{"id": "2504.17102", "pdf": "https://arxiv.org/pdf/2504.17102", "abs": "https://arxiv.org/abs/2504.17102", "authors": ["Haoyu Li", "Xiangru Zhong", "Bin Hu", "Huan Zhang"], "title": "Neural Contraction Metrics with Formal Guarantees for Discrete-Time Nonlinear Dynamical Systems", "categories": ["math.OC", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted by L4DC 2025", "summary": "Contraction metrics are crucial in control theory because they provide a\npowerful framework for analyzing stability, robustness, and convergence of\nvarious dynamical systems. However, identifying these metrics for complex\nnonlinear systems remains an open challenge due to the lack of scalable and\neffective tools. This paper explores the approach of learning verifiable\ncontraction metrics parametrized as neural networks (NNs) for discrete-time\nnonlinear dynamical systems. While prior works on formal verification of\ncontraction metrics for general nonlinear systems have focused on convex\noptimization methods (e.g. linear matrix inequalities, etc) under the\nassumption of continuously differentiable dynamics, the growing prevalence of\nNN-based controllers, often utilizing ReLU activations, introduces challenges\ndue to the non-smooth nature of the resulting closed-loop dynamics. To bridge\nthis gap, we establish a new sufficient condition for establishing formal\nneural contraction metrics for general discrete-time nonlinear systems assuming\nonly the continuity of the dynamics. We show that from a computational\nperspective, our sufficient condition can be efficiently verified using the\nstate-of-the-art neural network verifier $\\alpha,\\!\\beta$-CROWN, which scales\nup non-convex neural network verification via novel integration of symbolic\nlinear bound propagation and branch-and-bound. Built upon our analysis tool, we\nfurther develop a learning method for synthesizing neural contraction metrics\nfrom sampled data. Finally, our approach is validated through the successful\nsynthesis and verification of NN contraction metrics for various nonlinear\nexamples."}
{"id": "2504.17493", "pdf": "https://arxiv.org/pdf/2504.17493", "abs": "https://arxiv.org/abs/2504.17493", "authors": ["Luca-Andrei Fechete", "Mohamed Sana", "Fadhel Ayed", "Nicola Piovesan", "Wenjie Li", "Antonio De Domenico", "Tareq Si Salem"], "title": "Goal-Oriented Time-Series Forecasting: Foundation Framework Design", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traditional time-series forecasting often focuses only on minimizing\nprediction errors, ignoring the specific requirements of real-world\napplications that employ them. This paper presents a new training methodology,\nwhich allows a forecasting model to dynamically adjust its focus based on the\nimportance of forecast ranges specified by the end application. Unlike previous\nmethods that fix these ranges beforehand, our training approach breaks down\npredictions over the entire signal range into smaller segments, which are then\ndynamically weighted and combined to produce accurate forecasts. We tested our\nmethod on standard datasets, including a new dataset from wireless\ncommunication, and found that not only it improves prediction accuracy but also\nimproves the performance of end application employing the forecasting model.\nThis research provides a basis for creating forecasting systems that better\nconnect prediction and decision-making in various practical applications."}
{"id": "2504.17112", "pdf": "https://arxiv.org/pdf/2504.17112", "abs": "https://arxiv.org/abs/2504.17112", "authors": ["Margherita Lampani", "Sabrina Guastavino", "Michele Piana", "Federico Benvenuto"], "title": "Physics-informed features in supervised machine learning", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "68Q32, 62J07, 65J20"], "comment": null, "summary": "Supervised machine learning involves approximating an unknown functional\nrelationship from a limited dataset of features and corresponding labels. The\nclassical approach to feature-based machine learning typically relies on\napplying linear regression to standardized features, without considering their\nphysical meaning. This may limit model explainability, particularly in\nscientific applications. This study proposes a physics-informed approach to\nfeature-based machine learning that constructs non-linear feature maps informed\nby physical laws and dimensional analysis. These maps enhance model\ninterpretability and, when physical laws are unknown, allow for the\nidentification of relevant mechanisms through feature ranking. The method aims\nto improve both predictive performance in regression tasks and classification\nskill scores by integrating domain knowledge into the learning process, while\nalso enabling the potential discovery of new physical equations within the\ncontext of explainable machine learning."}
{"id": "2504.17497", "pdf": "https://arxiv.org/pdf/2504.17497", "abs": "https://arxiv.org/abs/2504.17497", "authors": ["Radia Berreziga", "Mohammed Brahimi", "Khairedine Kraim", "Hamid Azzoune"], "title": "Combining GCN Structural Learning with LLM Chemical Knowledge for or Enhanced Virtual Screening", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Virtual screening plays a critical role in modern drug discovery by enabling\nthe identification of promising candidate molecules for experimental\nvalidation. Traditional machine learning methods such as support vector\nmachines (SVM) and XGBoost rely on predefined molecular representations, often\nleading to information loss and potential bias. In contrast, deep learning\napproaches-particularly Graph Convolutional Networks (GCNs)-offer a more\nexpressive and unbiased alternative by operating directly on molecular graphs.\nMeanwhile, Large Language Models (LLMs) have recently demonstrated\nstate-of-the-art performance in drug design, thanks to their capacity to\ncapture complex chemical patterns from large-scale data via attention\nmechanisms.\n  In this paper, we propose a hybrid architecture that integrates GCNs with\nLLM-derived embeddings to combine localized structural learning with global\nchemical knowledge. The LLM embeddings can be precomputed and stored in a\nmolecular feature library, removing the need to rerun the LLM during training\nor inference and thus maintaining computational efficiency. We found that\nconcatenating the LLM embeddings after each GCN layer-rather than only at the\nfinal layer-significantly improves performance, enabling deeper integration of\nglobal context throughout the network. The resulting model achieves superior\nresults, with an F1-score of (88.8%), outperforming standalone GCN (87.9%),\nXGBoost (85.5%), and SVM (85.4%) baselines."}
{"id": "2504.17128", "pdf": "https://arxiv.org/pdf/2504.17128", "abs": "https://arxiv.org/abs/2504.17128", "authors": ["Seyed Yousef Soltanian", "Wenlong Zhang"], "title": "PACE: A Framework for Learning and Control in Linear Incomplete-Information Differential Games", "categories": ["eess.SY", "cs.LG", "cs.MA", "cs.SY", "93C41, 49N70, 49N90, 91A27"], "comment": "Accepted to 7th Annual Conference on Learning for Dynamics and\n  Control (L4DC) 2025. Camera-ready version using the official PMLR template.\n  The full version including appendix and proofs", "summary": "In this paper, we address the problem of a two-player linear quadratic\ndifferential game with incomplete information, a scenario commonly encountered\nin multi-agent control, human-robot interaction (HRI), and approximation\nmethods for solving general-sum differential games. While solutions to such\nlinear differential games are typically obtained through coupled Riccati\nequations, the complexity increases when agents have incomplete information,\nparticularly when neither is aware of the other's cost function. To tackle this\nchallenge, we propose a model-based Peer-Aware Cost Estimation (PACE) framework\nfor learning the cost parameters of the other agent. In PACE, each agent treats\nits peer as a learning agent rather than a stationary optimal agent, models\ntheir learning dynamics, and leverages this dynamic to infer the cost function\nparameters of the other agent. This approach enables agents to infer each\nother's objective function in real time based solely on their previous state\nobservations and dynamically adapt their control policies. Furthermore, we\nprovide a theoretical guarantee for the convergence of parameter estimation and\nthe stability of system states in PACE. Additionally, in our numerical studies,\nwe demonstrate how modeling the learning dynamics of the other agent benefits\nPACE, compared to approaches that approximate the other agent as having\ncomplete information, particularly in terms of stability and convergence speed."}
{"id": "2504.17528", "pdf": "https://arxiv.org/pdf/2504.17528", "abs": "https://arxiv.org/abs/2504.17528", "authors": ["Weijie Liu", "Ziwei Zhan", "Carlee Joe-Wong", "Edith Ngai", "Jingpu Duan", "Deke Guo", "Xu Chen", "Xiaoxi Zhang"], "title": "TACO: Tackling Over-correction in Federated Learning with Tailored Adaptive Correction", "categories": ["cs.LG", "cs.AI", "I.2.6"], "comment": "11 pages, 7 figures, accepted by ICDCS 2025", "summary": "Non-independent and identically distributed (Non-IID) data across edge\nclients have long posed significant challenges to federated learning (FL)\ntraining in edge computing environments. Prior works have proposed various\nmethods to mitigate this statistical heterogeneity. While these works can\nachieve good theoretical performance, in this work we provide the first\ninvestigation into a hidden over-correction phenomenon brought by the uniform\nmodel correction coefficients across clients adopted by existing methods. Such\nover-correction could degrade model performance and even cause failures in\nmodel convergence. To address this, we propose TACO, a novel algorithm that\naddresses the non-IID nature of clients' data by implementing fine-grained,\nclient-specific gradient correction and model aggregation, steering local\nmodels towards a more accurate global optimum. Moreover, we verify that leading\nFL algorithms generally have better model accuracy in terms of communication\nrounds rather than wall-clock time, resulting from their extra computation\noverhead imposed on clients. To enhance the training efficiency, TACO deploys a\nlightweight model correction and tailored aggregation approach that requires\nminimum computation overhead and no extra information beyond the synchronized\nmodel parameters. To validate TACO's effectiveness, we present the first FL\nconvergence analysis that reveals the root cause of over-correction. Extensive\nexperiments across various datasets confirm TACO's superior and stable\nperformance in practice."}
{"id": "2504.17142", "pdf": "https://arxiv.org/pdf/2504.17142", "abs": "https://arxiv.org/abs/2504.17142", "authors": ["Siddharth Nair", "Timothy F. Walsh", "Greg Pickrell", "Fabio Semperlotti"], "title": "Reinforcement learning framework for the mechanical design of microelectronic components under multiphysics constraints", "categories": ["physics.comp-ph", "cs.CE", "cs.LG"], "comment": "27 pages of main text, 15 figures", "summary": "This study focuses on the development of reinforcement learning based\ntechniques for the design of microelectronic components under multiphysics\nconstraints. While traditional design approaches based on global optimization\napproaches are effective when dealing with a small number of design parameters,\nas the complexity of the solution space and of the constraints increases\ndifferent techniques are needed. This is an important reason that makes the\ndesign and optimization of microelectronic components (characterized by large\nsolution space and multiphysics constraints) very challenging for traditional\nmethods. By taking as prototypical elements an application-specific integrated\ncircuit (ASIC) and a heterogeneously integrated (HI) interposer, we develop and\nnumerically test an optimization framework based on reinforcement learning\n(RL). More specifically, we consider the optimization of the bonded\ninterconnect geometry for an ASIC chip as well as the placement of components\non a HI interposer while satisfying thermoelastic and design constraints. This\nplacement problem is particularly interesting because it features a\nhigh-dimensional solution space."}
{"id": "2504.17534", "pdf": "https://arxiv.org/pdf/2504.17534", "abs": "https://arxiv.org/abs/2504.17534", "authors": ["Juan Carlos Climent Pardo"], "title": "Learning Isometric Embeddings of Road Networks using Multidimensional Scaling", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.SC"], "comment": null, "summary": "The lack of generalization in learning-based autonomous driving applications\nis shown by the narrow range of road scenarios that vehicles can currently\ncover. A generalizable approach should capture many distinct road structures\nand topologies, as well as consider traffic participants, and dynamic changes\nin the environment, so that vehicles can navigate and perform motion planning\ntasks even in the most difficult situations. Designing suitable feature spaces\nfor neural network-based motion planers that encapsulate all kinds of road\nscenarios is still an open research challenge. This paper tackles this\nlearning-based generalization challenge and shows how graph representations of\nroad networks can be leveraged by using multidimensional scaling (MDS)\ntechniques in order to obtain such feature spaces. State-of-the-art graph\nrepresentations and MDS approaches are analyzed for the autonomous driving use\ncase. Finally, the option of embedding graph nodes is discussed in order to\nperform easier learning procedures and obtain dimensionality reduction."}
{"id": "2504.17166", "pdf": "https://arxiv.org/pdf/2504.17166", "abs": "https://arxiv.org/abs/2504.17166", "authors": ["Ke Wan", "Kensuke Tanioka", "Toshio Shimokawa"], "title": "Causal rule ensemble approach for multi-arm data", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Heterogeneous treatment effect (HTE) estimation is critical in medical\nresearch. It provides insights into how treatment effects vary among\nindividuals, which can provide statistical evidence for precision medicine.\nWhile most existing methods focus on binary treatment situations, real-world\napplications often involve multiple interventions. However, current HTE\nestimation methods are primarily designed for binary comparisons and often rely\non black-box models, which limit their applicability and interpretability in\nmulti-arm settings. To address these challenges, we propose an interpretable\nmachine learning framework for HTE estimation in multi-arm trials. Our method\nemploys a rule-based ensemble approach consisting of rule generation, rule\nensemble, and HTE estimation, ensuring both predictive accuracy and\ninterpretability. Through extensive simulation studies and real data\napplications, the performance of our method was evaluated against\nstate-of-the-art multi-arm HTE estimation approaches. The results indicate that\nour approach achieved lower bias and higher estimation accuracy compared with\nthose of existing methods. Furthermore, the interpretability of our framework\nallows clearer insights into how covariates influence treatment effects,\nfacilitating clinical decision making. By bridging the gap between accuracy and\ninterpretability, our study contributes a valuable tool for multi-arm HTE\nestimation, supporting precision medicine."}
{"id": "2504.17539", "pdf": "https://arxiv.org/pdf/2504.17539", "abs": "https://arxiv.org/abs/2504.17539", "authors": ["Zan-Kai Chong", "Hiroyuki Ohsaki", "Bryan Ng"], "title": "Proof of Useful Intelligence (PoUI): Blockchain Consensus Beyond Energy Waste", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Blockchain technology enables secure, transparent data management in\ndecentralized systems, supporting applications from cryptocurrencies like\nBitcoin to tokenizing real-world assets like property. Its scalability and\nsustainability hinge on consensus mechanisms balancing security and efficiency.\nProof of Work (PoW), used by Bitcoin, ensures security through energy-intensive\ncomputations but demands significant resources. Proof of Stake (PoS), as in\nEthereum post-Merge, selects validators based on staked cryptocurrency,\noffering energy efficiency but risking centralization from wealth\nconcentration. With AI models straining computational resources, we propose\nProof of Useful Intelligence (PoUI), a hybrid consensus mechanism. In PoUI,\nworkers perform AI tasks like language processing or image analysis to earn\ncoins, which are staked to secure the network, blending security with practical\nutility. Decentralized nodes--job posters, market coordinators, workers, and\nvalidators --collaborate via smart contracts to manage tasks and rewards."}
{"id": "2504.17173", "pdf": "https://arxiv.org/pdf/2504.17173", "abs": "https://arxiv.org/abs/2504.17173", "authors": ["Tianyu Zhang", "Dongheng Zhang", "Ruixu Geng", "Xuecheng Xie", "Shuai Yang", "Yan Chen"], "title": "Lessons from Deploying Learning-based CSI Localization on a Large-Scale ISAC Platform", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "In recent years, Channel State Information (CSI), recognized for its\nfine-grained spatial characteristics, has attracted increasing attention in\nWiFi-based indoor localization. However, despite its potential, CSI-based\napproaches have yet to achieve the same level of deployment scale and\ncommercialization as those based on Received Signal Strength Indicator (RSSI).\nA key limitation lies in the fact that most existing CSI-based systems are\ndeveloped and evaluated in controlled, small-scale environments, limiting their\ngeneralizability. To bridge this gap, we explore the deployment of a\nlarge-scale CSI-based localization system involving over 400 Access Points\n(APs) in a real-world building under the Integrated Sensing and Communication\n(ISAC) paradigm. We highlight two critical yet often overlooked factors: the\nunderutilization of unlabeled data and the inherent heterogeneity of CSI\nmeasurements. To address these challenges, we propose a novel CSI-based\nlearning framework for WiFi localization, tailored for large-scale ISAC\ndeployments on the server side. Specifically, we employ a novel graph-based\nstructure to model heterogeneous CSI data and reduce redundancy. We further\ndesign a pretext pretraining task that incorporates spatial and temporal priors\nto effectively leverage large-scale unlabeled CSI data. Complementarily, we\nintroduce a confidence-aware fine-tuning strategy to enhance the robustness of\nlocalization results. In a leave-one-smartphone-out experiment spanning five\nfloors and 25, 600 m2, we achieve a median localization error of 2.17 meters\nand a floor accuracy of 99.49%. This performance corresponds to an 18.7%\nreduction in mean absolute error (MAE) compared to the best-performing\nbaseline."}
{"id": "2504.17540", "pdf": "https://arxiv.org/pdf/2504.17540", "abs": "https://arxiv.org/abs/2504.17540", "authors": ["Ahmadreza Shateri", "Negar Nourani", "Morteza Dorrigiv", "Hamid Nasiri"], "title": "An Explainable Nature-Inspired Framework for Monkeypox Diagnosis: Xception Features Combined with NGBoost and African Vultures Optimization Algorithm", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "The recent global spread of monkeypox, particularly in regions where it has\nnot historically been prevalent, has raised significant public health concerns.\nEarly and accurate diagnosis is critical for effective disease management and\ncontrol. In response, this study proposes a novel deep learning-based framework\nfor the automated detection of monkeypox from skin lesion images, leveraging\nthe power of transfer learning, dimensionality reduction, and advanced machine\nlearning techniques. We utilize the newly developed Monkeypox Skin Lesion\nDataset (MSLD), which includes images of monkeypox, chickenpox, and measles, to\ntrain and evaluate our models. The proposed framework employs the Xception\narchitecture for deep feature extraction, followed by Principal Component\nAnalysis (PCA) for dimensionality reduction, and the Natural Gradient Boosting\n(NGBoost) algorithm for classification. To optimize the model's performance and\ngeneralization, we introduce the African Vultures Optimization Algorithm (AVOA)\nfor hyperparameter tuning, ensuring efficient exploration of the parameter\nspace. Our results demonstrate that the proposed AVOA-NGBoost model achieves\nstate-of-the-art performance, with an accuracy of 97.53%, F1-score of 97.72%\nand an AUC of 97.47%. Additionally, we enhance model interpretability using\nGrad-CAM and LIME techniques, providing insights into the decision-making\nprocess and highlighting key features influencing classification. This\nframework offers a highly precise and efficient diagnostic tool, potentially\naiding healthcare providers in early detection and diagnosis, particularly in\nresource-constrained environments."}
{"id": "2504.17177", "pdf": "https://arxiv.org/pdf/2504.17177", "abs": "https://arxiv.org/abs/2504.17177", "authors": ["Kevin Lane", "Morteza Karimzadeh"], "title": "A Genealogy of Multi-Sensor Foundation Models in Remote Sensing", "categories": ["cs.CV", "cs.LG", "I.4.7; I.4.8"], "comment": "20 pages, submitted to ACM SigSpatial, currently under peer review", "summary": "Foundation models have garnered increasing attention for representation\nlearning in remote sensing, primarily adopting approaches that have\ndemonstrated success in computer vision with minimal domain-specific\nmodification. However, the development and application of foundation models in\nthis field are still burgeoning, as there are a variety of competing approaches\nthat each come with significant benefits and drawbacks. This paper examines\nthese approaches along with their roots in the computer vision field in order\nto characterize potential advantages and pitfalls while outlining future\ndirections to further improve remote sensing-specific foundation models. We\ndiscuss the quality of the learned representations and methods to alleviate the\nneed for massive compute resources. We place emphasis on the multi-sensor\naspect of Earth observations, and the extent to which existing approaches\nleverage multiple sensors in training foundation models in relation to\nmulti-modal foundation models. Finally, we identify opportunities for further\nharnessing the vast amounts of unlabeled, seasonal, and multi-sensor remote\nsensing observations."}
{"id": "2504.17550", "pdf": "https://arxiv.org/pdf/2504.17550", "abs": "https://arxiv.org/abs/2504.17550", "authors": ["Yejin Bang", "Ziwei Ji", "Alan Schelten", "Anthony Hartshorn", "Tara Fowler", "Cheng Zhang", "Nicola Cancedda", "Pascale Fung"], "title": "HalluLens: LLM Hallucination Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": "42 pages", "summary": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations."}
{"id": "2504.17179", "pdf": "https://arxiv.org/pdf/2504.17179", "abs": "https://arxiv.org/abs/2504.17179", "authors": ["Mohammad Zarei", "Melanie A Jutras", "Eliana Evans", "Mike Tan", "Omid Aaramoon"], "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO", "68T45, 68T05 68T45, 68T05 68T45, 68T05", "I.2.6; I.2.10; I.4.8"], "comment": "8 pages, 10 figures. Accepted to IEEE Conference on Artificial\n  Intelligence (CAI), 2025", "summary": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately\ndetect objects and interpret their surroundings. However, even when trained\nusing millions of miles of real-world data, AVs are often unable to detect rare\nfailure modes (RFMs). The problem of RFMs is commonly referred to as the\n\"long-tail challenge\", due to the distribution of data including many instances\nthat are very rarely seen. In this paper, we present a novel approach that\nutilizes advanced generative and explainable AI techniques to aid in\nunderstanding RFMs. Our methods can be used to enhance the robustness and\nreliability of AVs when combined with both downstream model training and\ntesting. We extract segmentation masks for objects of interest (e.g., cars) and\ninvert them to create environmental masks. These masks, combined with carefully\ncrafted text prompts, are fed into a custom diffusion model. We leverage the\nStable Diffusion inpainting model guided by adversarial noise optimization to\ngenerate images containing diverse environments designed to evade object\ndetection models and expose vulnerabilities in AI systems. Finally, we produce\nnatural language descriptions of the generated RFMs that can guide developers\nand policymakers to improve the safety and reliability of AV systems."}
{"id": "2504.17551", "pdf": "https://arxiv.org/pdf/2504.17551", "abs": "https://arxiv.org/abs/2504.17551", "authors": ["Lin Che", "Yizi Chen", "Tanhua Jin", "Martin Raubal", "Konrad Schindler", "Peter Kiefer"], "title": "Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 7 figures, preprint version", "summary": "Urban land use classification and mapping are critical for urban planning,\nresource management, and environmental monitoring. Existing remote sensing\ntechniques often lack precision in complex urban environments due to the\nabsence of ground-level details. Unlike aerial perspectives, street view images\nprovide a ground-level view that captures more human and social activities\nrelevant to land use in complex urban scenes. Existing street view-based\nmethods primarily rely on supervised classification, which is challenged by the\nscarcity of high-quality labeled data and the difficulty of generalizing across\ndiverse urban landscapes. This study introduces an unsupervised contrastive\nclustering model for street view images with a built-in geographical prior, to\nenhance clustering performance. When combined with a simple visual assignment\nof the clusters, our approach offers a flexible and customizable solution to\nland use mapping, tailored to the specific needs of urban planners. We\nexperimentally show that our method can generate land use maps from geotagged\nstreet view image datasets of two cities. As our methodology relies on the\nuniversal spatial coherence of geospatial data (\"Tobler's law\"), it can be\nadapted to various settings where street view images are available, to enable\nscalable, unsupervised land use mapping and updating. The code will be\navailable at https://github.com/lin102/CCGP."}
{"id": "2504.17203", "pdf": "https://arxiv.org/pdf/2504.17203", "abs": "https://arxiv.org/abs/2504.17203", "authors": ["Shivasankari Kannan", "Yeounoh Chung", "Amita Gondi", "Tristan Swadell", "Fatma Ozcan"], "title": "High-Fidelity And Complex Test Data Generation For Real-World SQL Code Generation Services", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "The demand for high-fidelity test data is paramount in industrial settings\nwhere access to production data is largely restricted. Traditional data\ngeneration methods often fall short, struggling with low-fidelity and the\nability to model complex data structures and semantic relationships that are\ncritical for testing complex SQL code generation services like Natural Language\nto SQL (NL2SQL). In this paper, we address the critical need for generating\nsyntactically correct and semantically ``meaningful'' mock data for complex\nschema that includes columns with nested structures that we frequently\nencounter in Google SQL code generation workloads. We highlight the limitations\nof existing approaches used in production, particularly their inability to\nhandle large and complex schema, as well as the lack of semantically coherent\ntest data that lead to limited test coverage. We demonstrate that by leveraging\nLarge Language Models (LLMs) and incorporating strategic pre- and\npost-processing steps, we can generate realistic high-fidelity test data that\nadheres to complex structural constraints and maintains semantic integrity to\nthe test targets (SQL queries/functions). This approach supports comprehensive\ntesting of complex SQL queries involving joins, aggregations, and even deeply\nnested subqueries, ensuring robust evaluation of SQL code generation services,\nlike NL2SQL and SQL Code Assistant services. Our results demonstrate the\npractical utility of an out-of-the-box LLM (\\textit{gemini}) based test data\ngeneration for industrial SQL code generation services where generating\nrealistic test data is essential due to the frequent unavailability of\nproduction datasets."}
{"id": "2504.17609", "pdf": "https://arxiv.org/pdf/2504.17609", "abs": "https://arxiv.org/abs/2504.17609", "authors": ["Fengchun Liu", "Tong Zhang", "Chunying Zhang"], "title": "STCL:Curriculum learning Strategies for deep learning image steganography models", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Aiming at the problems of poor quality of steganographic images and slow\nnetwork convergence of image steganography models based on deep learning, this\npaper proposes a Steganography Curriculum Learning training strategy (STCL) for\ndeep learning image steganography models. So that only easy images are selected\nfor training when the model has poor fitting ability at the initial stage, and\ngradually expand to more difficult images, the strategy includes a difficulty\nevaluation strategy based on the teacher model and an knee point-based training\nscheduling strategy. Firstly, multiple teacher models are trained, and the\nconsistency of the quality of steganographic images under multiple teacher\nmodels is used as the difficulty score to construct the training subsets from\neasy to difficult. Secondly, a training control strategy based on knee points\nis proposed to reduce the possibility of overfitting on small training sets and\naccelerate the training process. Experimental results on three large public\ndatasets, ALASKA2, VOC2012 and ImageNet, show that the proposed image\nsteganography scheme is able to improve the model performance under multiple\nalgorithmic frameworks, which not only has a high PSNR, SSIM score, and\ndecoding accuracy, but also the steganographic images generated by the model\nunder the training of the STCL strategy have a low steganography analysis\nscores. You can find our code at\n\\href{https://github.com/chaos-boops/STCL}{https://github.com/chaos-boops/STCL}."}
{"id": "2504.17236", "pdf": "https://arxiv.org/pdf/2504.17236", "abs": "https://arxiv.org/abs/2504.17236", "authors": ["Xiqiang Qu", "Jun Chen", "Lei Yu", "Xiangyu Xu"], "title": "Rate-Distortion-Perception Theory for the Quadratic Wasserstein Space", "categories": ["cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "We establish a single-letter characterization of the fundamental\ndistortion-rate-perception tradeoff with limited common randomness under the\nsquared error distortion measure and the squared Wasserstein-2 perception\nmeasure. Moreover, it is shown that this single-letter characterization can be\nexplicitly evaluated for the Gaussian source. Various notions of universal\nrepresentation are also clarified."}
{"id": "2504.17617", "pdf": "https://arxiv.org/pdf/2504.17617", "abs": "https://arxiv.org/abs/2504.17617", "authors": ["Bruno Casella", "Matthias Jakobs", "Marco Aldinucci", "Sebastian BuschjÃ¤ger"], "title": "Decentralized Time Series Classification with ROCKET Features", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.11; I.2.6"], "comment": "Submitted to Workshop on Federated Learning Advancements 2025, in\n  conjunction with ECML-PKDD, WAFL25", "summary": "Time series classification (TSC) is a critical task with applications in\nvarious domains, including healthcare, finance, and industrial monitoring. Due\nto privacy concerns and data regulations, Federated Learning has emerged as a\npromising approach for learning from distributed time series data without\ncentralizing raw information. However, most FL solutions rely on a\nclient-server architecture, which introduces robustness and confidentiality\nrisks related to the distinguished role of the server, which is a single point\nof failure and can observe knowledge extracted from clients. To address these\nchallenges, we propose DROCKS, a fully decentralized FL framework for TSC that\nleverages ROCKET (RandOm Convolutional KErnel Transform) features. In DROCKS,\nthe global model is trained by sequentially traversing a structured path across\nfederation nodes, where each node refines the model and selects the most\neffective local kernels before passing them to the successor. Extensive\nexperiments on the UCR archive demonstrate that DROCKS outperforms\nstate-of-the-art client-server FL approaches while being more resilient to node\nfailures and malicious attacks. Our code is available at\nhttps://anonymous.4open.science/r/DROCKS-7FF3/README.md."}
{"id": "2504.17252", "pdf": "https://arxiv.org/pdf/2504.17252", "abs": "https://arxiv.org/abs/2504.17252", "authors": ["Ocheme Anthony Ekle", "Biswarup Das"], "title": "Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo", "categories": ["cs.CL", "cs.LG", "68T50, 68T01", "I.2.7; I.2.1"], "comment": "25 pages, 14 combined figures (19 total), includes horizontal\n  layouts. Submitted to arXiv for open access", "summary": "In this study, we develop Neural Machine Translation (NMT) and\nTransformer-based transfer learning models for English-to-Igbo translation - a\nlow-resource African language spoken by over 40 million people across Nigeria\nand West Africa. Our models are trained on a curated and benchmarked dataset\ncompiled from Bible corpora, local news, Wikipedia articles, and Common Crawl,\nall verified by native language experts. We leverage Recurrent Neural Network\n(RNN) architectures, including Long Short-Term Memory (LSTM) and Gated\nRecurrent Units (GRU), enhanced with attention mechanisms to improve\ntranslation accuracy. To further enhance performance, we apply transfer\nlearning using MarianNMT pre-trained models within the SimpleTransformers\nframework. Our RNN-based system achieves competitive results, closely matching\nexisting English-Igbo benchmarks. With transfer learning, we observe a\nperformance gain of +4.83 BLEU points, reaching an estimated translation\naccuracy of 70%. These findings highlight the effectiveness of combining RNNs\nwith transfer learning to address the performance gap in low-resource language\ntranslation tasks."}
{"id": "2504.17619", "pdf": "https://arxiv.org/pdf/2504.17619", "abs": "https://arxiv.org/abs/2504.17619", "authors": ["Catarina P. Coutinho", "Aneeqa Merhab", "Janko Petkovic", "Ferdinando Zanchetta", "Rita Fioresi"], "title": "Enhancing CNNs robustness to occlusions with bioinspired filters for border completion", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to the 7th International Conference on Geometric Science of\n  Information", "summary": "We exploit the mathematical modeling of the visual cortex mechanism for\nborder completion to define custom filters for CNNs. We see a consistent\nimprovement in performance, particularly in accuracy, when our modified LeNet 5\nis tested with occluded MNIST images."}
{"id": "2504.17321", "pdf": "https://arxiv.org/pdf/2504.17321", "abs": "https://arxiv.org/abs/2504.17321", "authors": ["Michael J. Smith", "Luke Fleming", "James E. Geach", "Ryan J. Roberts", "Freddie Kalaitzis", "James Banister"], "title": "Dargana: fine-tuning EarthPT for dynamic tree canopy mapping from space", "categories": ["physics.geo-ph", "cs.LG"], "comment": "9 pages, 6 figures, spotlight at `Tackling Climate Change with\n  Machine Learning', ICLR 2025", "summary": "We present Dargana, a fine-tuned variant of the EarthPT time-series\nfoundation model that achieves specialisation using <3% of its pre-training\ndata volume and 5% of its pre-training compute. Dargana is fine-tuned to\ngenerate regularly updated classification of tree canopy cover at 10m\nresolution, distinguishing conifer and broadleaved tree types. Using Cornwall,\nUK, as a test case, the model achieves a pixel-level ROC-AUC of 0.98 and a\nPR-AUC of 0.83 on unseen satellite imagery. Dargana can identify fine\nstructures like hedgerows and coppice below the training sample limit, and can\ntrack temporal changes to canopy cover such as new woodland establishment. Our\nresults demonstrate how pre-trained Large Observation Models like EarthPT can\nbe specialised for granular, dynamic land cover monitoring from space,\nproviding a valuable, scalable tool for natural capital management and\nconservation."}
{"id": "2504.17624", "pdf": "https://arxiv.org/pdf/2504.17624", "abs": "https://arxiv.org/abs/2504.17624", "authors": ["Jigang Fan", "Chunhao Zhu", "Xiaobing Lan", "Haiming Zhuang", "Mingyu Li", "Jian Zhang", "Shaoyong Lu"], "title": "Deciphering the unique dynamic activation pathway in a G protein-coupled receptor enables unveiling biased signaling and identifying cryptic allosteric sites in conformational intermediates", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "Neurotensin receptor 1 (NTSR1), a member of the Class A G protein-coupled\nreceptor superfamily, plays an important role in modulating dopaminergic\nneuronal activity and eliciting opioid-independent analgesia. Recent studies\nsuggest that promoting \\{beta}-arrestin-biased signaling in NTSR1 may diminish\ndrugs of abuse, such as psychostimulants, thereby offering a potential avenue\nfor treating human addiction-related disorders. In this study, we utilized a\nnovel computational and experimental approach that combined nudged elastic\nband-based molecular dynamics simulations, Markov state models, temporal\ncommunication network analysis, site-directed mutagenesis, and conformational\nbiosensors, to explore the intricate mechanisms underlying NTSR1 activation and\nbiased signaling. Our study reveals a dynamic stepwise transition mechanism and\nactivated transmission network associated with NTSR1 activation. It also yields\nvaluable insights into the complex interplay between the unique polar network,\nnon-conserved ion locks, and aromatic clusters in NTSR1 signaling. Moreover, we\nidentified a cryptic allosteric site located in the intracellular region of the\nreceptor that exists in an intermediate state within the activation pathway.\nCollectively, these findings contribute to a more profound understanding of\nNTSR1 activation and biased signaling at the atomic level, thereby providing a\npotential strategy for the development of NTSR1 allosteric modulators in the\nrealm of G protein-coupled receptor biology, biophysics, and medicine."}
{"id": "2504.17356", "pdf": "https://arxiv.org/pdf/2504.17356", "abs": "https://arxiv.org/abs/2504.17356", "authors": ["Weiliang Zhang", "Xiaohan Huang", "Yi Du", "Ziyue Qiao", "Qingqing Long", "Zhen Meng", "Yuanchun Zhou", "Meng Xiao"], "title": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": "20 pages, keywords: Automated Feature Engineering, Tabular Dataset,\n  Multi-Agent Reinforcement Learning, Feature Selection", "summary": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved."}
{"id": "2504.17641", "pdf": "https://arxiv.org/pdf/2504.17641", "abs": "https://arxiv.org/abs/2504.17641", "authors": ["Shengtao Zhang", "Haokai Zhang", "Shiqi Lou", "Zicheng Wang", "Zinan Zeng", "Yilin Wang", "Minnan Luo"], "title": "PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Dynamic node classification is critical for modeling evolving systems like\nfinancial transactions and academic collaborations. In such systems,\ndynamically capturing node information changes is critical for dynamic node\nclassification, which usually requires all labels at every timestamp. However,\nit is difficult to collect all dynamic labels in real-world scenarios due to\nhigh annotation costs and label uncertainty (e.g., ambiguous or delayed labels\nin fraud detection). In contrast, final timestamp labels are easier to obtain\nas they rely on complete temporal patterns and are usually maintained as a\nunique label for each user in many open platforms, without tracking the history\ndata. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum\nLearning), a pioneering method addressing label-limited dynamic node\nclassification where only final labels are available. PTCL introduces: (1) a\ntemporal decoupling architecture separating the backbone (learning time-aware\nrepresentations) and decoder (strictly aligned with final labels), which\ngenerate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that\nprioritizes pseudo-labels closer to the final timestamp by assigning them\nhigher weights using an exponentially decaying function. We contribute a new\nacademic dataset (CoOAG), capturing long-range research interest in dynamic\ngraph. Experiments across real-world scenarios demonstrate PTCL's consistent\nsuperiority over other methods adapted to this task. Beyond methodology, we\npropose a unified framework FLiD (Framework for Label-Limited Dynamic Node\nClassification), consisting of a complete preparation workflow, training\npipeline, and evaluation standards, and supporting various models and datasets.\nThe code can be found at https://github.com/3205914485/FLiD."}
{"id": "2504.17376", "pdf": "https://arxiv.org/pdf/2504.17376", "abs": "https://arxiv.org/abs/2504.17376", "authors": ["Maoyang Xiang", "Ramesh Fernando", "Bo Wang"], "title": "On-Device Qwen2.5: Efficient LLM Inference with Model Compression and Hardware Acceleration", "categories": ["cs.AR", "cs.LG"], "comment": null, "summary": "Transformer-based Large Language Models (LLMs) have significantly advanced AI\ncapabilities but pose considerable challenges for deployment on edge devices\ndue to high computational demands, memory bandwidth constraints, and energy\nconsumption. This paper addresses these challenges by presenting an efficient\nframework for deploying the Qwen2.5-0.5B model on the Xilinx Kria KV260 edge\nplatform, a heterogeneous system integrating an ARM Cortex-A53 CPU with\nreconfigurable FPGA logic. Leveraging Activation-aware Weight Quantization\n(AWQ) with FPGA-accelerated execution pipelines, the proposed approach enhances\nboth model compression rate and system throughput. Additionally, we propose a\nhybrid execution strategy that intelligently offloads compute-intensive\noperations to the FPGA while utilizing the CPU for lighter tasks, effectively\nbalancing the computational workload and maximizing overall performance. Our\nframework achieves a model compression rate of 55.08% compared to the original\nmodel and produces output at a rate of 5.1 tokens per second, outperforming the\nbaseline performance of 2.8 tokens per second."}
{"id": "2504.17655", "pdf": "https://arxiv.org/pdf/2504.17655", "abs": "https://arxiv.org/abs/2504.17655", "authors": ["Farhad Pourkamali-Anaraki"], "title": "Aerial Image Classification in Scarce and Unconstrained Environments via Conformal Prediction", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "17 pages, 5 figures, and 2 tables", "summary": "This paper presents a comprehensive empirical analysis of conformal\nprediction methods on a challenging aerial image dataset featuring diverse\nevents in unconstrained environments. Conformal prediction is a powerful\npost-hoc technique that takes the output of any classifier and transforms it\ninto a set of likely labels, providing a statistical guarantee on the coverage\nof the true label. Unlike evaluations on standard benchmarks, our study\naddresses the complexities of data-scarce and highly variable real-world\nsettings. We investigate the effectiveness of leveraging pretrained models\n(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to\ngenerate informative prediction sets. To further evaluate the impact of\ncalibration, we consider two parallel pipelines (with and without temperature\nscaling) and assess performance using two key metrics: empirical coverage and\naverage prediction set size. This setup allows us to systematically examine how\ncalibration choices influence the trade-off between reliability and efficiency.\nOur findings demonstrate that even with relatively small labeled samples and\nsimple nonconformity scores, conformal prediction can yield valuable\nuncertainty estimates for complex tasks. Moreover, our analysis reveals that\nwhile temperature scaling is often employed for calibration, it does not\nconsistently lead to smaller prediction sets, underscoring the importance of\ncareful consideration in its application. Furthermore, our results highlight\nthe significant potential of model compression techniques within the conformal\nprediction pipeline for deployment in resource-constrained environments. Based\non our observations, we advocate for future research to delve into the impact\nof noisy or ambiguous labels on conformal prediction performance and to explore\neffective model reduction strategies."}
{"id": "2504.17420", "pdf": "https://arxiv.org/pdf/2504.17420", "abs": "https://arxiv.org/abs/2504.17420", "authors": ["Louisa Pawusch", "Stefania Scheurer", "Wolfgang Nowak", "Reed Maxwell"], "title": "HydroStartML: A combined machine learning and physics-based approach to reduce hydrological model spin-up time", "categories": ["physics.geo-ph", "cs.LG"], "comment": "13 pages, 14 figures. To be published in Advances in Water Resources", "summary": "Finding the initial depth-to-water table (DTWT) configuration of a catchment\nis a critical challenge when simulating the hydrological cycle with integrated\nmodels, significantly impacting simulation outcomes. Traditionally, this\ninvolves iterative spin-up computations, where the model runs under constant\natmospheric settings until steady-state is achieved. These so-called model\nspin-ups are computationally expensive, often requiring many years of simulated\ntime, particularly when the initial DTWT configuration is far from steady\nstate.\n  To accelerate the model spin-up process we developed HydroStartML, a machine\nlearning emulator trained on steady-state DTWT configurations across the\ncontiguous United States. HydroStartML predicts, based on available data like\nconductivity and surface slopes, a DTWT configuration of the respective\nwatershed, which can be used as an initial DTWT.\n  Our results show that initializing spin-up computations with HydroStartML\npredictions leads to faster convergence than with other initial configurations\nlike spatially constant DTWTs. The emulator accurately predicts configurations\nclose to steady state, even for terrain configurations not seen in training,\nand allows especially significant reductions in computational spin-up effort in\nregions with deep DTWTs. This work opens the door for hybrid approaches that\nblend machine learning and traditional simulation, enhancing predictive\naccuracy and efficiency in hydrology for improving water resource management\nand understanding complex environmental interactions."}
{"id": "2504.17663", "pdf": "https://arxiv.org/pdf/2504.17663", "abs": "https://arxiv.org/abs/2504.17663", "authors": ["Michelle L. Ding", "Harini Suresh"], "title": "The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "In this paper, we adopt a survivor-centered approach to locate and dissect\nthe role of sociotechnical AI governance in preventing AI-Generated\nNon-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as\n\"deep fake pornography.\" We identify a \"malicious technical ecosystem\" or\n\"MTE,\" comprising of open-source face-swapping models and nearly 200\n\"nudifying\" software programs that allow non-technical users to create AIG-NCII\nwithin minutes. Then, using the National Institute of Standards and Technology\n(NIST) AI 100-4 report as a reflection of current synthetic content governance\nmethods, we show how the current landscape of practices fails to effectively\nregulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining\nthese gaps."}
{"id": "2504.17529", "pdf": "https://arxiv.org/pdf/2504.17529", "abs": "https://arxiv.org/abs/2504.17529", "authors": ["Youngjune Lee", "Haeyu Jeong", "Changgeon Lim", "Jeong Choi", "Hongjun Lim", "Hangon Kim", "Jiyoon Kwon", "Saehun Kim"], "title": "IRA: Adaptive Interest-aware Representation and Alignment for Personalized Multi-interest Retrieval", "categories": ["cs.IR", "cs.LG"], "comment": "Accepted to SIGIR 2025 Industry Track. First two authors contributed\n  equally", "summary": "Online community platforms require dynamic personalized retrieval and\nrecommendation that can continuously adapt to evolving user interests and new\ndocuments. However, optimizing models to handle such changes in real-time\nremains a major challenge in large-scale industrial settings. To address this,\nwe propose the Interest-aware Representation and Alignment (IRA) framework, an\nefficient and scalable approach that dynamically adapts to new interactions\nthrough a cumulative structure. IRA leverages two key mechanisms: (1) Interest\nUnits that capture diverse user interests as contextual texts, while\nreinforcing or fading over time through cumulative updates, and (2) a retrieval\nprocess that measures the relevance between Interest Units and documents based\nsolely on semantic relationships, eliminating dependence on click signals to\nmitigate temporal biases. By integrating cumulative Interest Unit updates with\nthe retrieval process, IRA continuously adapts to evolving user preferences,\nensuring robust and fine-grained personalization without being constrained by\npast training distributions. We validate the effectiveness of IRA through\nextensive experiments on real-world datasets, including its deployment in the\nHome Section of NAVER's CAFE, South Korea's leading community platform."}
{"id": "2504.17669", "pdf": "https://arxiv.org/pdf/2504.17669", "abs": "https://arxiv.org/abs/2504.17669", "authors": ["Subash Neupane", "Shaswata Mitra", "Sudip Mittal", "Shahram Rahimi"], "title": "Towards a HIPAA Compliant Agentic AI System in Healthcare", "categories": ["cs.MA", "cs.AI", "cs.ET"], "comment": null, "summary": "Agentic AI systems powered by Large Language Models (LLMs) as their\nfoundational reasoning engine, are transforming clinical workflows such as\nmedical report generation and clinical summarization by autonomously analyzing\nsensitive healthcare data and executing decisions with minimal human oversight.\nHowever, their adoption demands strict compliance with regulatory frameworks\nsuch as Health Insurance Portability and Accountability Act (HIPAA),\nparticularly when handling Protected Health Information (PHI). This\nwork-in-progress paper introduces a HIPAA-compliant Agentic AI framework that\nenforces regulatory compliance through dynamic, context-aware policy\nenforcement. Our framework integrates three core mechanisms: (1)\nAttribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid\nPHI sanitization pipeline combining regex patterns and BERT-based model to\nminimize leakage, and (3) immutable audit trails for compliance verification."}
{"id": "2504.17540", "pdf": "https://arxiv.org/pdf/2504.17540", "abs": "https://arxiv.org/abs/2504.17540", "authors": ["Ahmadreza Shateri", "Negar Nourani", "Morteza Dorrigiv", "Hamid Nasiri"], "title": "An Explainable Nature-Inspired Framework for Monkeypox Diagnosis: Xception Features Combined with NGBoost and African Vultures Optimization Algorithm", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "The recent global spread of monkeypox, particularly in regions where it has\nnot historically been prevalent, has raised significant public health concerns.\nEarly and accurate diagnosis is critical for effective disease management and\ncontrol. In response, this study proposes a novel deep learning-based framework\nfor the automated detection of monkeypox from skin lesion images, leveraging\nthe power of transfer learning, dimensionality reduction, and advanced machine\nlearning techniques. We utilize the newly developed Monkeypox Skin Lesion\nDataset (MSLD), which includes images of monkeypox, chickenpox, and measles, to\ntrain and evaluate our models. The proposed framework employs the Xception\narchitecture for deep feature extraction, followed by Principal Component\nAnalysis (PCA) for dimensionality reduction, and the Natural Gradient Boosting\n(NGBoost) algorithm for classification. To optimize the model's performance and\ngeneralization, we introduce the African Vultures Optimization Algorithm (AVOA)\nfor hyperparameter tuning, ensuring efficient exploration of the parameter\nspace. Our results demonstrate that the proposed AVOA-NGBoost model achieves\nstate-of-the-art performance, with an accuracy of 97.53%, F1-score of 97.72%\nand an AUC of 97.47%. Additionally, we enhance model interpretability using\nGrad-CAM and LIME techniques, providing insights into the decision-making\nprocess and highlighting key features influencing classification. This\nframework offers a highly precise and efficient diagnostic tool, potentially\naiding healthcare providers in early detection and diagnosis, particularly in\nresource-constrained environments."}
{"id": "2504.17671", "pdf": "https://arxiv.org/pdf/2504.17671", "abs": "https://arxiv.org/abs/2504.17671", "authors": ["Yuanchang Ye", "Weiyan Wen"], "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making."}
{"id": "2504.17546", "pdf": "https://arxiv.org/pdf/2504.17546", "abs": "https://arxiv.org/abs/2504.17546", "authors": ["Wouter van Loon"], "title": "An introduction to R package `mvs`", "categories": ["stat.CO", "cs.LG", "stat.ME", "stat.ML"], "comment": "15 pages, 4 figures. Package vignette corresponding to\n  https://doi.org/10.32614/CRAN.package.mvs", "summary": "In biomedical science, a set of objects or persons can often be described by\nmultiple distinct sets of features obtained from different data sources or\nmodalities (called \"multi-view data\"). Classical machine learning methods\nignore the multi-view structure of such data, limiting model interpretability\nand performance. The R package `mvs` provides methods that were designed\nspecifically for dealing with multi-view data, based on the multi-view stacking\n(MVS) framework. MVS is a form of supervised (machine) learning used to train\nmulti-view classification or prediction models. MVS works by training a\nlearning algorithm on each view separately, estimating the predictive power of\neach view-specific model through cross-validation, and then using another\nlearning algorithm to assign weights to the view-specific models based on their\nestimated predictions. MVS is a form of ensemble learning, dividing the large\nmulti-view learning problem into smaller sub-problems. Most of these\nsub-problems can be solved in parallel, making it computationally attractive.\nAdditionally, the number of features of the sub-problems is greatly reduced\ncompared with the full multi-view learning problem. This makes MVS especially\nuseful when the total number of features is larger than the number of\nobservations (i.e., high-dimensional data). MVS can still be applied even if\nthe sub-problems are themselves high-dimensional by adding suitable penalty\nterms to the learning algorithms. Furthermore, MVS can be used to automatically\nselect the views which are most important for prediction. The R package `mvs`\nmakes fitting MVS models, including such penalty terms, easily and openly\naccessible. `mvs` allows for the fitting of stacked models with any number of\nlevels, with different penalty terms, different outcome distributions, and\nprovides several options for missing data handling."}
{"id": "2504.17675", "pdf": "https://arxiv.org/pdf/2504.17675", "abs": "https://arxiv.org/abs/2504.17675", "authors": ["Caroline Panggabean", "Devaraj Verma C", "Bhagyashree Gogoi", "Ranju Limbu", "Rhythm Sarker"], "title": "Optimized Cloud Resource Allocation Using Genetic Algorithms for Energy Efficiency and QoS Assurance", "categories": ["cs.DC", "cs.AI"], "comment": "7 pages, 5 figures, accepted for publication (not yet published)", "summary": "Cloud computing environments demand dynamic and efficient resource management\nto ensure optimal performance, reduced energy consumption, and adherence to\nService Level Agreements (SLAs). This paper presents a Genetic Algorithm\n(GA)-based approach for Virtual Machine (VM) placement and consolidation,\naiming to minimize power usage while maintaining QoS constraints. The proposed\nmethod dynamically adjusts VM allocation based on real-time workload\nvariations, outperforming traditional heuristics such as First Fit Decreasing\n(FFD) and Best Fit Decreasing (BFD). Experimental results show notable\nreductions in energy consumption, VM migrations, SLA violation rates, and\nexecution time. A correlation heatmap further illustrates strong relationships\namong these key performance indicators, confirming the effectiveness of our\napproach in optimizing cloud resource utilization."}
{"id": "2504.17548", "pdf": "https://arxiv.org/pdf/2504.17548", "abs": "https://arxiv.org/abs/2504.17548", "authors": ["Kilian Tscharke", "Maximilian Wendlinger", "Afrae Ahouzi", "Pallavi Bhardwaj", "Kaweh Amoi-Taleghani", "Michael SchrÃ¶dl-Baumann", "Pascal Debus"], "title": "Quantum Autoencoder for Multivariate Time Series Anomaly Detection", "categories": ["quant-ph", "cs.CR", "cs.LG"], "comment": "Submitted to IEEE International Conference on Quantum Computing and\n  Engineering (QCE) 2025", "summary": "Anomaly Detection (AD) defines the task of identifying observations or events\nthat deviate from typical - or normal - patterns, a critical capability in IT\nsecurity for recognizing incidents such as system misconfigurations, malware\ninfections, or cyberattacks. In enterprise environments like SAP HANA Cloud\nsystems, this task often involves monitoring high-dimensional, multivariate\ntime series (MTS) derived from telemetry and log data. With the advent of\nquantum machine learning offering efficient calculations in high-dimensional\nlatent spaces, many avenues open for dealing with such complex data. One\napproach is the Quantum Autoencoder (QAE), an emerging and promising method\nwith potential for application in both data compression and AD. However, prior\napplications of QAEs to time series AD have been restricted to univariate data,\nlimiting their relevance for real-world enterprise systems. In this work, we\nintroduce a novel QAE-based framework designed specifically for MTS AD towards\nenterprise scale. We theoretically develop and experimentally validate the\narchitecture, demonstrating that our QAE achieves performance competitive with\nneural-network-based autoencoders while requiring fewer trainable parameters.\nWe evaluate our model on datasets that closely reflect SAP system telemetry and\nshow that the proposed QAE is a viable and efficient alternative for\nsemisupervised AD in real-world enterprise settings."}
{"id": "2504.17677", "pdf": "https://arxiv.org/pdf/2504.17677", "abs": "https://arxiv.org/abs/2504.17677", "authors": ["Jarne Thys", "Sebe Vanbrabant", "Davy Vanacken", "Gustavo Rovelo Ruiz"], "title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. This paper introduces INSIGHT, a proof of concept to combine\nvarious AI tools to assist teaching staff and students in the process of\nsolving exercises. INSIGHT has a modular design that allows it to be integrated\ninto various higher education courses. We analyze students' questions to an LLM\nby extracting keywords, which we use to dynamically build an FAQ from students'\nquestions and provide new insights for the teaching staff to use for more\npersonalized face-to-face support. Future work could build upon INSIGHT by\nusing the collected data to provide adaptive learning and adjust content based\non student progress and learning styles to offer a more interactive and\ninclusive learning experience."}
{"id": "2504.17562", "pdf": "https://arxiv.org/pdf/2504.17562", "abs": "https://arxiv.org/abs/2504.17562", "authors": ["Rei Higuchi", "Ryotaro Kawata", "Naoki Nishikawa", "Kazusato Oko", "Shoichiro Yamaguchi", "Sosuke Kobayashi", "Seiya Tokui", "Kohei Hayashi", "Daisuke Okanohara", "Taiji Suzuki"], "title": "When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The ability to acquire latent semantics is one of the key properties that\ndetermines the performance of language models. One convenient approach to\ninvoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at\nthe beginning of texts in the pre-training data, making it easier for the model\nto access latent semantics before observing the entire text. Previous studies\nhave reported that this technique actually improves the performance of trained\nmodels in downstream tasks; however, this improvement has been observed only in\nspecific downstream tasks, without consistent enhancement in average next-token\nprediction loss. To understand this phenomenon, we closely investigate how\nprepending metadata during pre-training affects model performance by examining\nits behavior using artificial data. Interestingly, we found that this approach\nproduces both positive and negative effects on the downstream tasks. We\ndemonstrate that the effectiveness of the approach depends on whether latent\nsemantics can be inferred from the downstream task's prompt. Specifically,\nthrough investigations using data generated by probabilistic context-free\ngrammars, we show that training with metadata helps improve model's performance\nwhen the given context is long enough to infer the latent semantics. In\ncontrast, the technique negatively impacts performance when the context lacks\nthe necessary information to make an accurate posterior inference."}
{"id": "2504.17685", "pdf": "https://arxiv.org/pdf/2504.17685", "abs": "https://arxiv.org/abs/2504.17685", "authors": ["Haru-Tada Sato", "Fuka Matsuzaki", "Jun-ichiro Takahashi"], "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 2 figures", "summary": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach."}
{"id": "2504.17584", "pdf": "https://arxiv.org/pdf/2504.17584", "abs": "https://arxiv.org/abs/2504.17584", "authors": ["Qingyuan Liu", "Liyan Chen", "Yanning Yang", "Haocheng Wang", "Dong Du", "Zhigang Mao", "Naifeng Jing", "Yubin Xia", "Haibo Chen"], "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference", "categories": ["cs.AR", "cs.LG"], "comment": "16 pages, 11 figures", "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."}
{"id": "2504.17696", "pdf": "https://arxiv.org/pdf/2504.17696", "abs": "https://arxiv.org/abs/2504.17696", "authors": ["Ghazal Kaviani", "Yavuz Yarici", "Seulgi Kim", "Mohit Prabhushankar", "Ghassan AlRegib", "Mashhour Solh", "Ameya Patil"], "title": "Hierarchical and Multimodal Data for Daily Activity Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Daily Activity Recordings for Artificial Intelligence (DARai, pronounced\n\"Dahr-ree\") is a multimodal, hierarchically annotated dataset constructed to\nunderstand human activities in real-world settings. DARai consists of\ncontinuous scripted and unscripted recordings of 50 participants in 10\ndifferent environments, totaling over 200 hours of data from 20 sensors\nincluding multiple camera views, depth and radar sensors, wearable inertial\nmeasurement units (IMUs), electromyography (EMG), insole pressure sensors,\nbiomonitor sensors, and gaze tracker.\n  To capture the complexity in human activities, DARai is annotated at three\nlevels of hierarchy: (i) high-level activities (L1) that are independent tasks,\n(ii) lower-level actions (L2) that are patterns shared between activities, and\n(iii) fine-grained procedures (L3) that detail the exact execution steps for\nactions. The dataset annotations and recordings are designed so that 22.7% of\nL2 actions are shared between L1 activities and 14.2% of L3 procedures are\nshared between L2 actions. The overlap and unscripted nature of DARai allows\ncounterfactual activities in the dataset.\n  Experiments with various machine learning models showcase the value of DARai\nin uncovering important challenges in human-centered applications.\nSpecifically, we conduct unimodal and multimodal sensor fusion experiments for\nrecognition, temporal localization, and future action anticipation across all\nhierarchical annotation levels. To highlight the limitations of individual\nsensors, we also conduct domain-variant experiments that are enabled by DARai's\nmulti-sensor and counterfactual activity design setup.\n  The code, documentation, and dataset are available at the dedicated DARai\nwebsite:\nhttps://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/"}
{"id": "2504.17586", "pdf": "https://arxiv.org/pdf/2504.17586", "abs": "https://arxiv.org/abs/2504.17586", "authors": ["Xuyi Hu", "Jian Li", "Lorenzo Picinali", "Aidan O. T. Hogg"], "title": "A Machine Learning Approach for Denoising and Upsampling HRTFs", "categories": ["cs.SD", "cs.LG"], "comment": null, "summary": "The demand for realistic virtual immersive audio continues to grow, with\nHead-Related Transfer Functions (HRTFs) playing a key role. HRTFs capture how\nsound reaches our ears, reflecting unique anatomical features and enhancing\nspatial perception. It has been shown that personalized HRTFs improve\nlocalization accuracy, but their measurement remains time-consuming and\nrequires a noise-free environment. Although machine learning has been shown to\nreduce the required measurement points and, thus, the measurement time, a\ncontrolled environment is still necessary. This paper proposes a method to\naddress this constraint by presenting a novel technique that can upsample\nsparse, noisy HRTF measurements. The proposed approach combines an HRTF Denoisy\nU-Net for denoising and an Autoencoding Generative Adversarial Network (AE-GAN)\nfor upsampling from three measurement points. The proposed method achieves a\nlog-spectral distortion (LSD) error of 5.41 dB and a cosine similarity loss of\n0.0070, demonstrating the method's effectiveness in HRTF upsampling."}
{"id": "2504.17703", "pdf": "https://arxiv.org/pdf/2504.17703", "abs": "https://arxiv.org/abs/2504.17703", "authors": ["Edward Collins", "Michel Wang"], "title": "Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) has emerged as a transformative paradigm in the field\nof distributed machine learning, enabling multiple clients such as mobile\ndevices, edge nodes, or organizations to collaboratively train a shared global\nmodel without the need to centralize sensitive data. This decentralized\napproach addresses growing concerns around data privacy, security, and\nregulatory compliance, making it particularly attractive in domains such as\nhealthcare, finance, and smart IoT systems. This survey provides a concise yet\ncomprehensive overview of Federated Learning, beginning with its core\narchitecture and communication protocol. We discuss the standard FL lifecycle,\nincluding local training, model aggregation, and global updates. A particular\nemphasis is placed on key technical challenges such as handling non-IID\n(non-independent and identically distributed) data, mitigating system and\nhardware heterogeneity, reducing communication overhead, and ensuring privacy\nthrough mechanisms like differential privacy and secure aggregation.\nFurthermore, we examine emerging trends in FL research, including personalized\nFL, cross-device versus cross-silo settings, and integration with other\nparadigms such as reinforcement learning and quantum computing. We also\nhighlight real-world applications and summarize benchmark datasets and\nevaluation metrics commonly used in FL research. Finally, we outline open\nresearch problems and future directions to guide the development of scalable,\nefficient, and trustworthy FL systems."}
{"id": "2504.17622", "pdf": "https://arxiv.org/pdf/2504.17622", "abs": "https://arxiv.org/abs/2504.17622", "authors": ["Chen Xu", "Qiang Wang", "Lijun Sun"], "title": "Likelihood-Free Variational Autoencoders", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Variational Autoencoders (VAEs) typically rely on a probabilistic decoder\nwith a predefined likelihood, most commonly an isotropic Gaussian, to model the\ndata conditional on latent variables. While convenient for optimization, this\nchoice often leads to likelihood misspecification, resulting in blurry\nreconstructions and poor data fidelity, especially for high-dimensional data\nsuch as images. In this work, we propose \\textit{EnVAE}, a novel\nlikelihood-free generative framework that has a deterministic decoder and\nemploys the energy score -- a proper scoring rule -- to build the\nreconstruction loss. This enables likelihood-free inference without requiring\nexplicit parametric density functions. To address the computational\ninefficiency of the energy score, we introduce a fast variant, \\textit{FEnVAE},\nbased on the local smoothness of the decoder and the sharpness of the posterior\ndistribution of latent variables. This yields an efficient single-sample\ntraining objective that integrates seamlessly into existing VAE pipelines with\nminimal overhead. Empirical results on standard benchmarks demonstrate that\n\\textit{EnVAE} achieves superior reconstruction and generation quality compared\nto likelihood-based baselines. Our framework offers a general, scalable, and\nstatistically principled alternative for flexible and nonparametric\ndistribution learning in generative modeling."}
{"id": "2504.17717", "pdf": "https://arxiv.org/pdf/2504.17717", "abs": "https://arxiv.org/abs/2504.17717", "authors": ["Ãscar Escudero-Arnanz", "Antonio G. Marques", "Inmaculada Mora-JimÃ©nez", "JoaquÃ­n Ãlvarez-RodrÃ­guez", "Cristina Soguero-Ruiz"], "title": "Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis and Interpretable Patient-Similarity Representations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Background and Objectives: Multidrug Resistance (MDR) is a critical global\nhealth issue, causing increased hospital stays, healthcare costs, and\nmortality. This study proposes an interpretable Machine Learning (ML) framework\nfor MDR prediction, aiming for both accurate inference and enhanced\nexplainability.\n  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing\nclinical progression and patient-to-patient interactions. Similarity among\npatients is quantified using MTS-based methods: descriptive statistics, Dynamic\nTime Warping, and Time Cluster Kernel. These similarity measures serve as\ninputs for MDR classification via Logistic Regression, Random Forest, and\nSupport Vector Machines, with dimensionality reduction and kernel\ntransformations improving model performance. For explainability, patient\nsimilarity networks are constructed from these metrics. Spectral clustering and\nt-SNE are applied to identify MDR-related subgroups and visualize high-risk\nclusters, enabling insight into clinically relevant patterns.\n  Results: The framework was validated on ICU Electronic Health Records from\nthe University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms\nbaseline ML and deep learning models by leveraging graph-based patient\nsimilarity. The approach identifies key risk factors -- prolonged antibiotic\nuse, invasive procedures, co-infections, and extended ICU stays -- and reveals\nclinically meaningful clusters. Code and results are available at\n\\https://github.com/oscarescuderoarnanz/DM4MTS.\n  Conclusions: Patient similarity representations combined with graph-based\nanalysis provide accurate MDR prediction and interpretable insights. This\nmethod supports early detection, risk factor identification, and patient\nstratification, highlighting the potential of explainable ML in critical care."}
{"id": "2504.17656", "pdf": "https://arxiv.org/pdf/2504.17656", "abs": "https://arxiv.org/abs/2504.17656", "authors": ["Ayush Jain", "Rampi Ramprasad"], "title": "polyGen: A Learning Framework for Atomic-level Polymer Structure Generation", "categories": ["cs.CE", "cond-mat.mtrl-sci", "cs.LG"], "comment": null, "summary": "Synthetic polymeric materials underpin fundamental technologies in the\nenergy, electronics, consumer goods, and medical sectors, yet their development\nstill suffers from prolonged design timelines. Although polymer informatics\ntools have supported speedup, polymer simulation protocols continue to face\nsignificant challenges: on-demand generation of realistic 3D atomic structures\nthat respect the conformational diversity of polymer structures. Generative\nalgorithms for 3D structures of inorganic crystals, bio-polymers, and small\nmolecules exist, but have not addressed synthetic polymers. In this work, we\nintroduce polyGen, the first latent diffusion model designed specifically to\ngenerate realistic polymer structures from minimal inputs such as the repeat\nunit chemistry alone, leveraging a molecular encoding that captures polymer\nconnectivity throughout the architecture. Due to a scarce dataset of only 3855\nDFT-optimized polymer structures, we augment our training with DFT-optimized\nmolecular structures, showing improvement in joint learning between similar\nchemical structures. We also establish structure matching criteria to benchmark\nour approach on this novel problem. polyGen effectively generates diverse\nconformations of both linear chains and complex branched structures, though its\nperformance decreases when handling repeat units with a high atom count. Given\nthese initial results, polyGen represents a paradigm shift in atomic-level\nstructure generation for polymer science-the first proof-of-concept for\npredicting realistic atomic-level polymer conformations while accounting for\ntheir intrinsic structural flexibility."}
{"id": "2504.17720", "pdf": "https://arxiv.org/pdf/2504.17720", "abs": "https://arxiv.org/abs/2504.17720", "authors": ["Vansh Gupta", "Sankalan Pal Chowdhury", "VilÃ©m Zouhar", "Donya Rooein", "Mrinmaya Sachan"], "title": "Multilingual Performance Biases of Large Language Models in Education", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment."}
{"id": "2504.17663", "pdf": "https://arxiv.org/pdf/2504.17663", "abs": "https://arxiv.org/abs/2504.17663", "authors": ["Michelle L. Ding", "Harini Suresh"], "title": "The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "In this paper, we adopt a survivor-centered approach to locate and dissect\nthe role of sociotechnical AI governance in preventing AI-Generated\nNon-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as\n\"deep fake pornography.\" We identify a \"malicious technical ecosystem\" or\n\"MTE,\" comprising of open-source face-swapping models and nearly 200\n\"nudifying\" software programs that allow non-technical users to create AIG-NCII\nwithin minutes. Then, using the National Institute of Standards and Technology\n(NIST) AI 100-4 report as a reflection of current synthetic content governance\nmethods, we show how the current landscape of practices fails to effectively\nregulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining\nthese gaps."}
{"id": "2504.17721", "pdf": "https://arxiv.org/pdf/2504.17721", "abs": "https://arxiv.org/abs/2504.17721", "authors": ["Cheng Shen", "Yuewei Liu"], "title": "Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees", "categories": ["cs.LG", "cs.AI"], "comment": "Under Review", "summary": "In industrial settings, surface defects on steel can significantly compromise\nits service life and elevate potential safety risks. Traditional defect\ndetection methods predominantly rely on manual inspection, which suffers from\nlow efficiency and high costs. Although automated defect detection approaches\nbased on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly,\ntheir reliability remains challenged due to data annotation uncertainties\nduring deep model training and overfitting issues. These limitations may lead\nto detection deviations when processing the given new test samples, rendering\nautomated detection processes unreliable. To address this challenge, we first\nevaluate the detection model's practical performance through calibration data\nthat satisfies the independent and identically distributed (i.i.d) condition\nwith test data. Specifically, we define a loss function for each calibration\nsample to quantify detection error rates, such as the complement of recall rate\nand false discovery rate. Subsequently, we derive a statistically rigorous\nthreshold based on a user-defined risk level to identify high-probability\ndefective pixels in test images, thereby constructing prediction sets (e.g.,\ndefect regions). This methodology ensures that the expected error rate (mean\nerror rate) on the test set remains strictly bounced by the predefined risk\nlevel. Additionally, we observe a negative correlation between the average\nprediction set size and the risk level on the test set, establishing a\nstatistically rigorous metric for assessing detection model uncertainty.\nFurthermore, our study demonstrates robust and efficient control over the\nexpected test set error rate across varying calibration-to-test partitioning\nratios, validating the method's adaptability and operational effectiveness."}
{"id": "2504.17671", "pdf": "https://arxiv.org/pdf/2504.17671", "abs": "https://arxiv.org/abs/2504.17671", "authors": ["Yuanchang Ye", "Weiyan Wen"], "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making."}
{"id": "2504.17751", "pdf": "https://arxiv.org/pdf/2504.17751", "abs": "https://arxiv.org/abs/2504.17751", "authors": ["Enqi Zhang"], "title": "Revisiting Reset Mechanisms in Spiking Neural Networks for Sequential Modeling: Specialized Discretization for Binary Activated RNN", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "In the field of image recognition, spiking neural networks (SNNs) have\nachieved performance comparable to conventional artificial neural networks\n(ANNs). In such applications, SNNs essentially function as traditional neural\nnetworks with quantized activation values. This article focuses on an another\nalternative perspective,viewing SNNs as binary-activated recurrent neural\nnetworks (RNNs) for sequential modeling tasks.From this viewpoint, current SNN\narchitectures face several fundamental challenges in sequence modeling: (1)\nTraditional models lack effective memory mechanisms for long-range sequence\nmodeling; (2) The biological-inspired components in SNNs (such as reset\nmechanisms and refractory period applications) remain theoretically\nunder-explored for sequence tasks; (3) The RNN-like computational paradigm in\nSNNs prevents parallel training across different timesteps.To address these\nchallenges, this study conducts a systematic analysis of the fundamental\nmechanisms underlying reset operations and refractory periods in\nbinary-activated RNN-based SNN sequence models. We re-examine whether such\nbiological mechanisms are strictly necessary for generating sparse spiking\npatterns, provide new theoretical explanations and insights, and ultimately\npropose the fixed-refractory-period SNN architecture for sequence modeling."}
{"id": "2504.17674", "pdf": "https://arxiv.org/pdf/2504.17674", "abs": "https://arxiv.org/abs/2504.17674", "authors": ["Jared Fernandez", "Clara Na", "Vashisth Tiwari", "Yonatan Bisk", "Sasha Luccioni", "Emma Strubell"], "title": "Energy Considerations of Large Language Model Inference and Efficiency Optimizations", "categories": ["cs.CL", "cs.LG"], "comment": "16 pages", "summary": "As large language models (LLMs) scale in size and adoption, their\ncomputational and environmental costs continue to rise. Prior benchmarking\nefforts have primarily focused on latency reduction in idealized settings,\noften overlooking the diverse real-world inference workloads that shape energy\nuse. In this work, we systematically analyze the energy implications of common\ninference efficiency optimizations across diverse Natural Language Processing\n(NLP) and generative Artificial Intelligence (AI) workloads, including\nconversational AI and code generation. We introduce a modeling approach that\napproximates real-world LLM workflows through a binning strategy for\ninput-output token distributions and batch size variations. Our empirical\nanalysis spans software frameworks, decoding strategies, GPU architectures,\nonline and offline serving settings, and model parallelism configurations. We\nshow that the effectiveness of inference optimizations is highly sensitive to\nworkload geometry, software stack, and hardware accelerators, demonstrating\nthat naive energy estimates based on FLOPs or theoretical GPU utilization\nsignificantly underestimate real-world energy consumption. Our findings reveal\nthat the proper application of relevant inference efficiency optimizations can\nreduce total energy use by up to 73% from unoptimized baselines. These insights\nprovide a foundation for sustainable LLM deployment and inform energy-efficient\ndesign strategies for future AI infrastructure."}
{"id": "2504.17771", "pdf": "https://arxiv.org/pdf/2504.17771", "abs": "https://arxiv.org/abs/2504.17771", "authors": ["Haochen Wang", "Zhiwei Shi", "Chengxi Zhu", "Yafei Qiao", "Cheng Zhang", "Fan Yang", "Pengjie Ren", "Lan Lu", "Dong Xuan"], "title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted to ICRA 2025. Project page:\n  https://dreamstarring.github.io/HAMLET/", "summary": "Learning-based methods, such as imitation learning (IL) and reinforcement\nlearning (RL), can produce excel control policies over challenging agile robot\ntasks, such as sports robot. However, no existing work has harmonized\nlearning-based policy with model-based methods to reduce training complexity\nand ensure the safety and stability for agile badminton robot control. In this\npaper, we introduce \\ourmethod, a novel hybrid control system for agile\nbadminton robots. Specifically, we propose a model-based strategy for chassis\nlocomotion which provides a base for arm policy. We introduce a\nphysics-informed ``IL+RL'' training framework for learning-based arm policy. In\nthis train framework, a model-based strategy with privileged information is\nused to guide arm policy training during both IL and RL phases. In addition, we\ntrain the critic model during IL phase to alleviate the performance drop issue\nwhen transitioning from IL to RL. We present results on our self-engineered\nbadminton robot, achieving 94.5% success rate against the serving machine and\n90.7% success rate against human players. Our system can be easily generalized\nto other agile mobile manipulation tasks such as agile catching and table\ntennis. Our project website: https://dreamstarring.github.io/HAMLET/."}
{"id": "2504.17690", "pdf": "https://arxiv.org/pdf/2504.17690", "abs": "https://arxiv.org/abs/2504.17690", "authors": ["Petros Georgiou", "Aaron Mark Thomas", "Sharu Theresa Jose", "Osvaldo Simeone"], "title": "On the Generalization of Adversarially Trained Quantum Classifiers", "categories": ["quant-ph", "cs.LG"], "comment": "22 pages, 6 figures", "summary": "Quantum classifiers are vulnerable to adversarial attacks that manipulate\ntheir input classical or quantum data. A promising countermeasure is\nadversarial training, where quantum classifiers are trained by using an\nattack-aware, adversarial loss function. This work establishes novel bounds on\nthe generalization error of adversarially trained quantum classifiers when\ntested in the presence of perturbation-constrained adversaries. The bounds\nquantify the excess generalization error incurred to ensure robustness to\nadversarial attacks as scaling with the training sample size $m$ as\n$1/\\sqrt{m}$, while yielding insights into the impact of the quantum embedding.\nFor quantum binary classifiers employing \\textit{rotation embedding}, we find\nthat, in the presence of adversarial attacks on classical inputs $\\mathbf{x}$,\nthe increase in sample complexity due to adversarial training over conventional\ntraining vanishes in the limit of high dimensional inputs $\\mathbf{x}$. In\ncontrast, when the adversary can directly attack the quantum state\n$\\rho(\\mathbf{x})$ encoding the input $\\mathbf{x}$, the excess generalization\nerror depends on the choice of embedding only through its Hilbert space\ndimension. The results are also extended to multi-class classifiers. We\nvalidate our theoretical findings with numerical experiments."}
{"id": "2504.17710", "pdf": "https://arxiv.org/pdf/2504.17710", "abs": "https://arxiv.org/abs/2504.17710", "authors": ["Yoeri Poels", "Alessandro Pau", "Christian Donner", "Giulio Romanelli", "Olivier Sauter", "Cristina Venturini", "Vlado Menkovski", "the TCV team", "the WPTE team"], "title": "Plasma State Monitoring and Disruption Characterization using Multimodal VAEs", "categories": ["physics.plasm-ph", "cs.CV", "cs.LG"], "comment": null, "summary": "When a plasma disrupts in a tokamak, significant heat and electromagnetic\nloads are deposited onto the surrounding device components. These forces scale\nwith plasma current and magnetic field strength, making disruptions one of the\nkey challenges for future devices. Unfortunately, disruptions are not fully\nunderstood, with many different underlying causes that are difficult to\nanticipate. Data-driven models have shown success in predicting them, but they\nonly provide limited interpretability. On the other hand, large-scale\nstatistical analyses have been a great asset to understanding disruptive\npatterns. In this paper, we leverage data-driven methods to find an\ninterpretable representation of the plasma state for disruption\ncharacterization. Specifically, we use a latent variable model to represent\ndiagnostic measurements as a low-dimensional, latent representation. We build\nupon the Variational Autoencoder (VAE) framework, and extend it for (1)\ncontinuous projections of plasma trajectories; (2) a multimodal structure to\nseparate operating regimes; and (3) separation with respect to disruptive\nregimes. Subsequently, we can identify continuous indicators for the disruption\nrate and the disruptivity based on statistical properties of measurement data.\nThe proposed method is demonstrated using a dataset of approximately 1600 TCV\ndischarges, selecting for flat-top disruptions or regular terminations. We\nevaluate the method with respect to (1) the identified disruption risk and its\ncorrelation with other plasma properties; (2) the ability to distinguish\ndifferent types of disruptions; and (3) downstream analyses. For the latter, we\nconduct a demonstrative study on identifying parameters connected to\ndisruptions using counterfactual-like analysis. Overall, the method can\nadequately identify distinct operating regimes characterized by varying\nproximity to disruptions in an interpretable manner."}
{"id": "2504.17719", "pdf": "https://arxiv.org/pdf/2504.17719", "abs": "https://arxiv.org/abs/2504.17719", "authors": ["Matthijs van der Lende", "Jeremias Lino Ferrao", "Niclas MÃ¼ller-Hof"], "title": "Evaluating Uncertainty in Deep Gaussian Processes", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Reliable uncertainty estimates are crucial in modern machine learning. Deep\nGaussian Processes (DGPs) and Deep Sigma Point Processes (DSPPs) extend GPs\nhierarchically, offering promising methods for uncertainty quantification\ngrounded in Bayesian principles. However, their empirical calibration and\nrobustness under distribution shift relative to baselines like Deep Ensembles\nremain understudied. This work evaluates these models on regression (CASP\ndataset) and classification (ESR dataset) tasks, assessing predictive\nperformance (MAE, Accu- racy), calibration using Negative Log-Likelihood (NLL)\nand Expected Calibration Error (ECE), alongside robustness under various\nsynthetic feature-level distribution shifts. Results indicate DSPPs provide\nstrong in-distribution calibration leveraging their sigma point approximations.\nHowever, compared to Deep Ensembles, which demonstrated superior robustness in\nboth per- formance and calibration under the tested shifts, the GP-based\nmethods showed vulnerabilities, exhibiting particular sensitivity in the\nobserved metrics. Our findings underscore ensembles as a robust baseline,\nsuggesting that while deep GP methods offer good in-distribution calibration,\ntheir practical robustness under distribution shift requires careful\nevaluation. To facilitate reproducibility, we make our code available at\nhttps://github.com/matthjs/xai-gp."}
{"id": "2504.17735", "pdf": "https://arxiv.org/pdf/2504.17735", "abs": "https://arxiv.org/abs/2504.17735", "authors": ["Akhil Padmanabha", "Saravanan Govindarajan", "Hwanmun Kim", "Sergio Ortiz", "Rahul Rajan", "Doruk Senkal", "Sneha Kadetotad"], "title": "EgoCHARM: Resource-Efficient Hierarchical Activity Recognition using an Egocentric IMU Sensor", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Human activity recognition (HAR) on smartglasses has various use cases,\nincluding health/fitness tracking and input for context-aware AI assistants.\nHowever, current approaches for egocentric activity recognition suffer from low\nperformance or are resource-intensive. In this work, we introduce a resource\n(memory, compute, power, sample) efficient machine learning algorithm,\nEgoCHARM, for recognizing both high level and low level activities using a\nsingle egocentric (head-mounted) Inertial Measurement Unit (IMU). Our\nhierarchical algorithm employs a semi-supervised learning strategy, requiring\nprimarily high level activity labels for training, to learn generalizable low\nlevel motion embeddings that can be effectively utilized for low level activity\nrecognition. We evaluate our method on 9 high level and 3 low level activities\nachieving 0.826 and 0.855 F1 scores on high level and low level activity\nrecognition respectively, with just 63k high level and 22k low level model\nparameters, allowing the low level encoder to be deployed directly on current\nIMU chips with compute. Lastly, we present results and insights from a\nsensitivity analysis and highlight the opportunities and limitations of\nactivity recognition using egocentric IMUs."}
{"id": "2504.17768", "pdf": "https://arxiv.org/pdf/2504.17768", "abs": "https://arxiv.org/abs/2504.17768", "authors": ["Piotr Nawrot", "Robert Li", "Renjie Huang", "Sebastian Ruder", "Kelly Marchisio", "Edoardo M. Ponti"], "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications."}
{"id": "2504.17771", "pdf": "https://arxiv.org/pdf/2504.17771", "abs": "https://arxiv.org/abs/2504.17771", "authors": ["Haochen Wang", "Zhiwei Shi", "Chengxi Zhu", "Yafei Qiao", "Cheng Zhang", "Fan Yang", "Pengjie Ren", "Lan Lu", "Dong Xuan"], "title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted to ICRA 2025. Project page:\n  https://dreamstarring.github.io/HAMLET/", "summary": "Learning-based methods, such as imitation learning (IL) and reinforcement\nlearning (RL), can produce excel control policies over challenging agile robot\ntasks, such as sports robot. However, no existing work has harmonized\nlearning-based policy with model-based methods to reduce training complexity\nand ensure the safety and stability for agile badminton robot control. In this\npaper, we introduce \\ourmethod, a novel hybrid control system for agile\nbadminton robots. Specifically, we propose a model-based strategy for chassis\nlocomotion which provides a base for arm policy. We introduce a\nphysics-informed ``IL+RL'' training framework for learning-based arm policy. In\nthis train framework, a model-based strategy with privileged information is\nused to guide arm policy training during both IL and RL phases. In addition, we\ntrain the critic model during IL phase to alleviate the performance drop issue\nwhen transitioning from IL to RL. We present results on our self-engineered\nbadminton robot, achieving 94.5% success rate against the serving machine and\n90.7% success rate against human players. Our system can be easily generalized\nto other agile mobile manipulation tasks such as agile catching and table\ntennis. Our project website: https://dreamstarring.github.io/HAMLET/."}
{"id": "2504.17782", "pdf": "https://arxiv.org/pdf/2504.17782", "abs": "https://arxiv.org/abs/2504.17782", "authors": ["Xize Cheng", "Slytherin Wang", "Zehan Wang", "Rongjie Huang", "Tao Jin", "Zhou Zhao"], "title": "Unleashing the Power of Natural Audio Featuring Multiple Sound Sources", "categories": ["cs.SD", "cs.LG"], "comment": "Work in Progress", "summary": "Universal sound separation aims to extract clean audio tracks corresponding\nto distinct events from mixed audio, which is critical for artificial auditory\nperception. However, current methods heavily rely on artificially mixed audio\nfor training, which limits their ability to generalize to naturally mixed audio\ncollected in real-world environments. To overcome this limitation, we propose\nClearSep, an innovative framework that employs a data engine to decompose\ncomplex naturally mixed audio into multiple independent tracks, thereby\nallowing effective sound separation in real-world scenarios. We introduce two\nremix-based evaluation metrics to quantitatively assess separation quality and\nuse these metrics as thresholds to iteratively apply the data engine alongside\nmodel training, progressively optimizing separation performance. In addition,\nwe propose a series of training strategies tailored to these separated\nindependent tracks to make the best use of them. Extensive experiments\ndemonstrate that ClearSep achieves state-of-the-art performance across multiple\nsound separation tasks, highlighting its potential for advancing sound\nseparation in natural audio scenarios. For more examples and detailed results,\nplease visit our demo page at https://clearsep.github.io."}
{"id": "2504.16956", "pdf": "https://arxiv.org/pdf/2504.16956", "abs": "https://arxiv.org/abs/2504.16956", "authors": ["Cong Qi", "Hanzhang Fang", "Tianxing Hu", "Siqi Jiang", "Wei Zhi"], "title": "Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity", "categories": ["cs.CL", "cs.LG", "q-bio.GN"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of\ncellular heterogeneity, but its complexity, which is marked by high\ndimensionality, sparsity, and batch effects, which poses major computational\nchallenges. Transformer-based models have made significant advances in this\ndomain but are often limited by their quadratic complexity and suboptimal\nhandling of long-range dependencies. In this work, we introduce GeneMamba, a\nscalable and efficient foundation model for single-cell transcriptomics built\non state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba\ncaptures bidirectional gene context with linear-time complexity, offering\nsubstantial computational gains over transformer baselines. The model is\npretrained on nearly 30 million cells and incorporates biologically informed\nobjectives, including pathway-aware contrastive loss and rank-based gene\nencoding. We evaluate GeneMamba across diverse tasks, including multi-batch\nintegration, cell type annotation, and gene-gene correlation, demonstrating\nstrong performance, interpretability, and robustness. These results position\nGeneMamba as a practical and powerful alternative to transformer-based methods,\nadvancing the development of biologically grounded, scalable tools for\nlarge-scale single-cell data analysis."}
{"id": "2504.16977", "pdf": "https://arxiv.org/pdf/2504.16977", "abs": "https://arxiv.org/abs/2504.16977", "authors": ["Priyaranjan Pattnayak", "Hitesh Laxmichand Patel", "Amit Agarwal"], "title": "Tokenization Matters: Improving Zero-Shot NER for Indic Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization is a critical component of Natural Language Processing (NLP),\nespecially for low resource languages, where subword segmentation influences\nvocabulary structure and downstream task accuracy. Although Byte Pair Encoding\n(BPE) is a standard tokenization method in multilingual language models, its\nsuitability for Named Entity Recognition (NER) in low resource Indic languages\nremains underexplored due to its limitations in handling morphological\ncomplexity. In this work, we systematically compare BPE, SentencePiece, and\nCharacter Level tokenization strategies using IndicBERT for NER tasks in low\nresource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as\nextremely low resource Indic languages like Santali, Manipuri, and Sindhi. We\nassess both intrinsic linguistic properties tokenization efficiency, out of\nvocabulary (OOV) rates, and morphological preservation as well as extrinsic\ndownstream performance, including fine tuning and zero shot cross lingual\ntransfer.\n  Our experiments show that SentencePiece is a consistently better performing\napproach than BPE for NER in low resource Indic Languages, particularly in zero\nshot cross lingual settings, as it better preserves entity consistency. While\nBPE provides the most compact tokenization form, it is not capable of\ngeneralization because it misclassifies or even fails to recognize entity\nlabels when tested on unseen languages. In contrast, SentencePiece constitutes\na better linguistic structural preservation model, benefiting extremely low\nresource and morphologically rich Indic languages, such as Santali and\nManipuri, for superior entity recognition, as well as high generalization\nacross scripts, such as Sindhi, written in Arabic. The results point to\nSentencePiece as the more effective tokenization strategy for NER within\nmultilingual and low resource Indic NLP applications."}
{"id": "2504.17025", "pdf": "https://arxiv.org/pdf/2504.17025", "abs": "https://arxiv.org/abs/2504.17025", "authors": ["Luca Moroni", "Giovanni Puccetti", "Pere-Lluis Huguet Cabot", "Andrei Stefan Bejgu", "Edoardo Barba", "Alessio Miaschi", "Felice Dell'Orletta", "Andrea Esuli", "Roberto Navigli"], "title": "Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "The number of pretrained Large Language Models (LLMs) is increasing steadily,\nthough the majority are designed predominantly for the English language. While\nstate-of-the-art LLMs can handle other languages, due to language contamination\nor some degree of multilingual pretraining data, they are not optimized for\nnon-English languages, leading to inefficient encoding (high token \"fertility\")\nand slower inference speed. In this work, we thoroughly compare a variety of\nvocabulary adaptation techniques for optimizing English LLMs for the Italian\nlanguage, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a\nnovel method that leverages neural mapping for vocabulary substitution. SAVA\nachieves competitive performance across multiple downstream tasks, enhancing\ngrounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing\ntoken fertility by 25\\%, and Llama-3.1-8B, optimizing the vocabulary and\nreducing the number of parameters by 1 billion. We show that, following the\nadaptation of the vocabulary, these models can recover their performance with a\nrelatively limited stage of continual training on the target language. Finally,\nwe test the capabilities of the adapted models on various multi-choice and\ngenerative tasks."}
{"id": "2504.17052", "pdf": "https://arxiv.org/pdf/2504.17052", "abs": "https://arxiv.org/abs/2504.17052", "authors": ["Shariar Kabir", "Kevin Esterling", "Yue Dong"], "title": "Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models", "categories": ["cs.CL"], "comment": "20 pages, 9 figures", "summary": "Large Language Models (LLMs) are increasingly shaping political discourse,\nyet their responses often display inconsistency when subjected to scrutiny.\nWhile prior research has primarily categorized LLM outputs as left- or\nright-leaning to assess their political stances, a critical question remains:\nDo these responses reflect genuine internal beliefs or merely surface-level\nalignment with training data? To address this, we propose a novel framework for\nevaluating belief depth by analyzing (1) argumentative consistency and (2)\nuncertainty quantification. We evaluate 12 LLMs on 19 economic policies from\nthe Political Compass Test, challenging their belief stability with both\nsupportive and opposing arguments. Our analysis reveals that LLMs exhibit\ntopic-specific belief stability rather than a uniform ideological stance.\nNotably, up to 95% of left-leaning models' responses and 89% of right-leaning\nmodels' responses remain consistent under the challenge, enabling semantic\nentropy to achieve high accuracy (AUROC=0.78), effectively distinguishing\nbetween surface-level alignment from genuine belief. These findings call into\nquestion the assumption that LLMs maintain stable, human-like political\nideologies, emphasizing the importance of conducting topic-specific reliability\nassessments for real-world applications."}
{"id": "2504.16961", "pdf": "https://arxiv.org/pdf/2504.16961", "abs": "https://arxiv.org/abs/2504.16961", "authors": ["Binon Teji", "Swarup Roy"], "title": "A Novel Graph Transformer Framework for Gene Regulatory Network Inference", "categories": ["cs.LG", "cs.ET", "q-bio.GN", "q-bio.MN", "q-bio.QM"], "comment": null, "summary": "The inference of gene regulatory networks (GRNs) is a foundational stride\ntowards deciphering the fundamentals of complex biological systems. Inferring a\npossible regulatory link between two genes can be formulated as a link\nprediction problem. Inference of GRNs via gene coexpression profiling data may\nnot always reflect true biological interactions, as its susceptibility to noise\nand misrepresenting true biological regulatory relationships. Most GRN\ninference methods face several challenges in the network reconstruction phase.\nTherefore, it is important to encode gene expression values, leverege the prior\nknowledge gained from the available inferred network structures and positional\ninformations of the input network nodes towards inferring a better and more\nconfident GRN network reconstruction. In this paper, we explore the integration\nof multiple inferred networks to enhance the inference of Gene Regulatory\nNetworks (GRNs). Primarily, we employ autoencoder embeddings to capture gene\nexpression patterns directly from raw data, preserving intricate biological\nsignals. Then, we embed the prior knowledge from GRN structures transforming\nthem into a text-like representation using random walks, which are then encoded\nwith a masked language model, BERT, to generate global embeddings for each gene\nacross all networks. Additionally, we embed the positional encodings of the\ninput gene networks to better identify the position of each unique gene within\nthe graph. These embeddings are integrated into graph transformer-based model,\ntermed GT-GRN, for GRN inference. The GT-GRN model effectively utilizes the\ntopological structure of the ground truth network while incorporating the\nenriched encoded information. Experimental results demonstrate that GT-GRN\nsignificantly outperforms existing GRN inference methods, achieving superior\naccuracy and highlighting the robustness of our approach."}
{"id": "2504.16937", "pdf": "https://arxiv.org/pdf/2504.16937", "abs": "https://arxiv.org/abs/2504.16937", "authors": ["Ariel S. Kapusta", "David Jin", "Peter M. Teague", "Robert A. Houston", "Jonathan B. Elliott", "Grace Y. Park", "Shelby S. Holdren"], "title": "A Framework for the Assurance of AI-Enabled Systems", "categories": ["cs.AI"], "comment": "12 pages, 2 figures, published in conference proceedings of SPIE\n  Defense and Commercial Sensing conference on Assurance and Security for\n  AI-enabled Systems 2025", "summary": "The United States Department of Defense (DOD) looks to accelerate the\ndevelopment and deployment of AI capabilities across a wide spectrum of defense\napplications to maintain strategic advantages. However, many common features of\nAI algorithms that make them powerful, such as capacity for learning,\nlarge-scale data ingestion, and problem-solving, raise new technical, security,\nand ethical challenges. These challenges may hinder adoption due to uncertainty\nin development, testing, assurance, processes, and requirements.\nTrustworthiness through assurance is essential to achieve the expected value\nfrom AI.\n  This paper proposes a claims-based framework for risk management and\nassurance of AI systems that addresses the competing needs for faster\ndeployment, successful adoption, and rigorous evaluation. This framework\nsupports programs across all acquisition pathways provide grounds for\nsufficient confidence that an AI-enabled system (AIES) meets its intended\nmission goals without introducing unacceptable risks throughout its lifecycle.\nThe paper's contributions are a framework process for AI assurance, a set of\nrelevant definitions to enable constructive conversations on the topic of AI\nassurance, and a discussion of important considerations in AI assurance. The\nframework aims to provide the DOD a robust yet efficient mechanism for swiftly\nfielding effective AI capabilities without overlooking critical risks or\nundermining stakeholder trust."}
{"id": "2504.17075", "pdf": "https://arxiv.org/pdf/2504.17075", "abs": "https://arxiv.org/abs/2504.17075", "authors": ["Arjun Subramonian", "Vagrant Gautam", "Preethi Seshadri", "Dietrich Klakow", "Kai-Wei Chang", "Yizhou Sun"], "title": "Agree to Disagree? A Meta-Evaluation of LLM Misgendering", "categories": ["cs.CL", "cs.CY"], "comment": "Work in progress", "summary": "Numerous methods have been proposed to measure LLM misgendering, including\nprobability-based evaluations (e.g., automatically with templatic sentences)\nand generation-based evaluations (e.g., with automatic heuristics or human\nvalidation). However, it has gone unexamined whether these evaluation methods\nhave convergent validity, that is, whether their results align. Therefore, we\nconduct a systematic meta-evaluation of these methods across three existing\ndatasets for LLM misgendering. We propose a method to transform each dataset to\nenable parallel probability- and generation-based evaluation. Then, by\nautomatically evaluating a suite of 6 models from 3 families, we find that\nthese methods can disagree with each other at the instance, dataset, and model\nlevels, conflicting on 20.2% of evaluation instances. Finally, with a human\nevaluation of 2400 LLM generations, we show that misgendering behaviour is\ncomplex and goes far beyond pronouns, which automatic evaluations are not\ncurrently designed to capture, suggesting essential disagreement with human\nevaluations. Based on our findings, we provide recommendations for future\nevaluations of LLM misgendering. Our results are also more widely relevant, as\nthey call into question broader methodological conventions in LLM evaluation,\nwhich often assume that different evaluation methods agree."}
{"id": "2504.16968", "pdf": "https://arxiv.org/pdf/2504.16968", "abs": "https://arxiv.org/abs/2504.16968", "authors": ["Jun Wu", "Jiangtao Wen", "Yuxing Han"], "title": "Backslash: Rate Constrained Optimized Training of Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rapid advancement of large-language models (LLMs) has driven extensive\nresearch into parameter compression after training has been completed, yet\ncompression during the training phase remains largely unexplored. In this work,\nwe introduce Rate-Constrained Training (Backslash), a novel training-time\ncompression approach based on rate-distortion optimization (RDO). Backslash\nenables a flexible trade-off between model accuracy and complexity,\nsignificantly reducing parameter redundancy while preserving performance.\nExperiments in various architectures and tasks demonstrate that Backslash can\nreduce memory usage by 60\\% - 90\\% without accuracy loss and provides\nsignificant compression gain compared to compression after training. Moreover,\nBackslash proves to be highly versatile: it enhances generalization with small\nLagrange multipliers, improves model robustness to pruning (maintaining\naccuracy even at 80\\% pruning rates), and enables network simplification for\naccelerated inference on edge devices."}
{"id": "2504.16938", "pdf": "https://arxiv.org/pdf/2504.16938", "abs": "https://arxiv.org/abs/2504.16938", "authors": ["Lucas Carr", "Nicholas Leisegang", "Thomas Meyer", "Sergei Obiedkov"], "title": "Rational Inference in Formal Concept Analysis", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "Defeasible conditionals are a form of non-monotonic inference which enable\nthe expression of statements like \"if $\\phi$ then normally $\\psi$\". The KLM\nframework defines a semantics for the propositional case of defeasible\nconditionals by construction of a preference ordering over possible worlds. The\npattern of reasoning induced by these semantics is characterised by consequence\nrelations satisfying certain desirable properties of non-monotonic reasoning.\nIn FCA, implications are used to describe dependencies between attributes.\nHowever, these implications are unsuitable to reason with erroneous data or\ndata prone to exceptions. Until recently, the topic of non-monotonic inference\nin FCA has remained largely uninvestigated. In this paper, we provide a\nconstruction of the KLM framework for defeasible reasoning in FCA and show that\nthis construction remains faithful to the principle of non-monotonic inference\ndescribed in the original framework. We present an additional argument that,\nwhile remaining consistent with the original ideas around non-monotonic\nreasoning, the defeasible reasoning we propose in FCA offers a more contextual\nview on inference, providing the ability for more relevant conclusions to be\ndrawn when compared to the propositional case."}
{"id": "2504.17083", "pdf": "https://arxiv.org/pdf/2504.17083", "abs": "https://arxiv.org/abs/2504.17083", "authors": ["Rendi Chevi", "Kentaro Inui", "Thamar Solorio", "Alham Fikri Aji"], "title": "How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study", "categories": ["cs.CL"], "comment": "Accepted at GenAICHI 2025 @ ACM CHI 2025", "summary": "What makes an interaction with the LLM more preferable for the user? While it\nis intuitive to assume that information accuracy in the LLM's responses would\nbe one of the influential variables, recent studies have found that inaccurate\nLLM's responses could still be preferable when they are perceived to be more\nauthoritative, certain, well-articulated, or simply verbose. These variables\ninterestingly fall under the broader category of language style, implying that\nthe style in the LLM's responses might meaningfully influence users'\npreferences. This hypothesized dynamic could have double-edged consequences:\nenhancing the overall user experience while simultaneously increasing their\nsusceptibility to risks such as LLM's misinformation or hallucinations. In this\nshort paper, we present our preliminary studies in exploring this subject.\nThrough a series of exploratory and experimental user studies, we found that\nLLM's language style does indeed influence user's preferences, but how and\nwhich language styles influence the preference varied across different user\npopulations, and more interestingly, moderated by the user's very own\nindividual traits. As a preliminary work, the findings in our studies should be\ninterpreted with caution, particularly given the limitations in our samples,\nwhich still need wider demographic diversity and larger sample sizes. Our\nfuture directions will first aim to address these limitations, which would\nenable a more comprehensive joint effect analysis between the language style,\nindividual traits, and preferences, and further investigate the potential\ncausal relationship between and beyond these variables."}
{"id": "2504.16970", "pdf": "https://arxiv.org/pdf/2504.16970", "abs": "https://arxiv.org/abs/2504.16970", "authors": ["Yin Wang", "Chunlin Gong", "Xiang Wu", "Hanleran Zhang"], "title": "STFM: A Spatio-Temporal Information Fusion Model Based on Phase Space Reconstruction for Sea Surface Temperature Prediction", "categories": ["cs.LG"], "comment": "19 pages, 14 figures", "summary": "The sea surface temperature (SST), a key environmental parameter, is crucial\nto optimizing production planning, making its accurate prediction a vital\nresearch topic. However, the inherent nonlinearity of the marine dynamic system\npresents significant challenges. Current forecasting methods mainly include\nphysics-based numerical simulations and data-driven machine learning\napproaches. The former, while describing SST evolution through differential\nequations, suffers from high computational complexity and limited\napplicability, whereas the latter, despite its computational benefits, requires\nlarge datasets and faces interpretability challenges. This study presents a\nprediction framework based solely on data-driven techniques. Using phase space\nreconstruction, we construct initial-delay attractor pairs with a mathematical\nhomeomorphism and design a Spatio-Temporal Fusion Mapping (STFM) to uncover\ntheir intrinsic connections. Unlike conventional models, our method captures\nSST dynamics efficiently through phase space reconstruction and achieves high\nprediction accuracy with minimal training data in comparative tests"}
{"id": "2504.16939", "pdf": "https://arxiv.org/pdf/2504.16939", "abs": "https://arxiv.org/abs/2504.16939", "authors": ["Emre Can Acikgoz", "Cheng Qian", "Hongru Wang", "Vardhan Dongre", "Xiusi Chen", "Heng Ji", "Dilek Hakkani-TÃ¼r", "Gokhan Tur"], "title": "A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have propelled conversational\nAI from traditional dialogue systems into sophisticated agents capable of\nautonomous actions, contextual awareness, and multi-turn interactions with\nusers. Yet, fundamental questions about their capabilities, limitations, and\npaths forward remain open. This survey paper presents a desideratum for\nnext-generation Conversational Agents - what has been achieved, what challenges\npersist, and what must be done for more scalable systems that approach\nhuman-level intelligence. To that end, we systematically analyze LLM-driven\nConversational Agents by organizing their capabilities into three primary\ndimensions: (i) Reasoning - logical, systematic thinking inspired by human\nintelligence for decision making, (ii) Monitor - encompassing self-awareness\nand user interaction monitoring, and (iii) Control - focusing on tool\nutilization and policy following. Building upon this, we introduce a novel\ntaxonomy by classifying recent work on Conversational Agents around our\nproposed desideratum. We identify critical research gaps and outline key\ndirections, including realistic evaluations, long-term multi-turn reasoning\nskills, self-evolution capabilities, collaborative and multi-agent task\ncompletion, personalization, and proactivity. This work aims to provide a\nstructured foundation, highlight existing limitations, and offer insights into\npotential future research directions for Conversational Agents, ultimately\nadvancing progress toward Artificial General Intelligence (AGI). We maintain a\ncurated repository of papers at:\nhttps://github.com/emrecanacikgoz/awesome-conversational-agents."}
{"id": "2504.17091", "pdf": "https://arxiv.org/pdf/2504.17091", "abs": "https://arxiv.org/abs/2504.17091", "authors": ["Seunghyun Yoo"], "title": "Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning", "categories": ["cs.CL", "68T05"], "comment": "5 page", "summary": "Due to the proliferation of short-form content and the rapid adoption of AI,\nopportunities for deep, reflective thinking have significantly diminished,\nundermining users' critical thinking and reducing engagement with the reasoning\nbehind AI-generated outputs. To address this issue, we propose an Interactive\nChain-of-Thought (CoT) Framework that enhances human-centered explainability\nand responsible AI usage by making the model's inference process transparent,\nmodular, and user-editable. The framework decomposes reasoning into clearly\ndefined blocks that users can inspect, modify, and re-execute, encouraging\nactive cognitive engagement rather than passive consumption. It further\nintegrates a lightweight edit-adaptation mechanism inspired by preference\nlearning, allowing the system to align with diverse cognitive styles and user\nintentions. Ethical transparency is ensured through explicit metadata\ndisclosure, built-in bias checkpoint functionality, and privacy-preserving\nsafeguards. This work outlines the design principles and architecture necessary\nto promote critical engagement, responsible interaction, and inclusive\nadaptation in AI systems aimed at addressing complex societal challenges."}
{"id": "2504.16972", "pdf": "https://arxiv.org/pdf/2504.16972", "abs": "https://arxiv.org/abs/2504.16972", "authors": ["Hossein Ahmadi", "Sajjad Emdadi Mahdimahalleh", "Arman Farahat", "Banafsheh Saffari"], "title": "Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "comment": null, "summary": "The rapid growth of unlabeled time-series data in domains such as wireless\ncommunications, radar, biomedical engineering, and the Internet of Things (IoT)\nhas driven advancements in unsupervised learning. This review synthesizes\nrecent progress in applying autoencoders and vision transformers for\nunsupervised signal analysis, focusing on their architectures, applications,\nand emerging trends. We explore how these models enable feature extraction,\nanomaly detection, and classification across diverse signal types, including\nelectrocardiograms, radar waveforms, and IoT sensor data. The review highlights\nthe strengths of hybrid architectures and self-supervised learning, while\nidentifying challenges in interpretability, scalability, and domain\ngeneralization. By bridging methodological innovations and practical\napplications, this work offers a roadmap for developing robust, adaptive models\nfor signal intelligence."}
{"id": "2504.17006", "pdf": "https://arxiv.org/pdf/2504.17006", "abs": "https://arxiv.org/abs/2504.17006", "authors": ["Jalal Arabneydi", "Saiful Islam", "Srijita Das", "Sai Krishna Gottipati", "William Duguay", "Cloderic Mars", "Matthew E. Taylor", "Matthew Guzdial", "Antoine Fagette", "Younes Zerouali"], "title": "A Systematic Approach to Design Real-World Human-in-the-Loop Deep Reinforcement Learning: Salient Features, Challenges and Trade-offs", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": "This is a result of the collaboration by JACOBB, AMII(Alberta Machine\n  Intelligence Institute), Thales and AI Redefined (AIR) in 2021-2023", "summary": "With the growing popularity of deep reinforcement learning (DRL),\nhuman-in-the-loop (HITL) approach has the potential to revolutionize the way we\napproach decision-making problems and create new opportunities for human-AI\ncollaboration. In this article, we introduce a novel multi-layered hierarchical\nHITL DRL algorithm that comprises three types of learning: self learning,\nimitation learning and transfer learning. In addition, we consider three forms\nof human inputs: reward, action and demonstration. Furthermore, we discuss main\nchallenges, trade-offs and advantages of HITL in solving complex problems and\nhow human information can be integrated in the AI solution systematically. To\nverify our technical results, we present a real-world unmanned aerial vehicles\n(UAV) problem wherein a number of enemy drones attack a restricted area. The\nobjective is to design a scalable HITL DRL algorithm for ally drones to\nneutralize the enemy drones before they reach the area. To this end, we first\nimplement our solution using an award-winning open-source HITL software called\nCogment. We then demonstrate several interesting results such as (a) HITL leads\nto faster training and higher performance, (b) advice acts as a guiding\ndirection for gradient methods and lowers variance, and (c) the amount of\nadvice should neither be too large nor too small to avoid over-training and\nunder-training. Finally, we illustrate the role of human-AI cooperation in\nsolving two real-world complex scenarios, i.e., overloaded and decoy attacks."}
{"id": "2504.17119", "pdf": "https://arxiv.org/pdf/2504.17119", "abs": "https://arxiv.org/abs/2504.17119", "authors": ["Muskan Garg", "Shaina Raza", "Shebuti Rayana", "Xingyi Liu", "Sunghwan Sohn"], "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey", "categories": ["cs.CL", "cs.AI"], "comment": "35 pages, 7 tables, 5 figures", "summary": "Despite substantial progress in healthcare applications driven by large\nlanguage models (LLMs), growing concerns around data privacy, and limited\nresources; the small language models (SLMs) offer a scalable and clinically\nviable solution for efficient performance in resource-constrained environments\nfor next-generation healthcare informatics. Our comprehensive survey presents a\ntaxonomic framework to identify and categorize them for healthcare\nprofessionals and informaticians. The timeline of healthcare SLM contributions\nestablishes a foundational framework for analyzing models across three\ndimensions: NLP tasks, stakeholder roles, and the continuum of care. We present\na taxonomic framework to identify the architectural foundations for building\nmodels from scratch; adapting SLMs to clinical precision through prompting,\ninstruction fine-tuning, and reasoning; and accessibility and sustainability\nthrough compression techniques. Our primary objective is to offer a\ncomprehensive survey for healthcare professionals, introducing recent\ninnovations in model optimization and equipping them with curated resources to\nsupport future research and development in the field. Aiming to showcase the\ngroundbreaking advancements in SLMs for healthcare, we present a comprehensive\ncompilation of experimental results across widely studied NLP tasks in\nhealthcare to highlight the transformative potential of SLMs in healthcare. The\nupdated repository is available at Github"}
{"id": "2504.16980", "pdf": "https://arxiv.org/pdf/2504.16980", "abs": "https://arxiv.org/abs/2504.16980", "authors": ["Pratyush Maini", "Sachin Goyal", "Dylan Sam", "Alex Robey", "Yash Savani", "Yiding Jiang", "Andy Zou", "Zacharcy C. Lipton", "J. Zico Kolter"], "title": "Safety Pretraining: Toward the Next Generation of Safe AI", "categories": ["cs.LG"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in high-stakes\nsettings, the risk of generating harmful or toxic content remains a central\nchallenge. Post-hoc alignment methods are brittle: once unsafe patterns are\nlearned during pretraining, they are hard to remove. We present a data-centric\npretraining framework that builds safety into the model from the start. Our\ncontributions include: (i) a safety classifier trained on 10,000 GPT-4 labeled\nexamples, used to filter 600B tokens; (ii) the largest synthetic safety dataset\nto date (100B tokens) generated via recontextualization of harmful web data;\n(iii) RefuseWeb and Moral Education datasets that convert harmful prompts into\nrefusal dialogues and web-style educational material; (iv) Harmfulness-Tag\nannotations injected during pretraining to flag unsafe content and steer away\ninference from harmful generations; and (v) safety evaluations measuring base\nmodel behavior before instruction tuning. Our safety-pretrained models reduce\nattack success rates from 38.8% to 8.4% with no performance degradation on\nstandard LLM safety benchmarks."}
{"id": "2504.17017", "pdf": "https://arxiv.org/pdf/2504.17017", "abs": "https://arxiv.org/abs/2504.17017", "authors": ["Balaji Rao", "William Eiers", "Carlo Lipizzi"], "title": "Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification", "categories": ["cs.AI", "cs.FL", "cs.LG", "cs.LO"], "comment": "Accepted to the Proceedings of the 19th Conference on Neurosymbolic\n  Learning and Reasoning (NeSy 2025)", "summary": "Formally verifying properties of software code has been a highly desirable\ntask, especially with the emergence of LLM-generated code. In the same vein,\nthey provide an interesting avenue for the exploration of formal verification\nand mechanistic interpretability. Since the introduction of code-specific\nmodels, despite their successes in generating code in Lean4 and Isabelle, the\ntask of generalized theorem proving still remains far from being fully solved\nand will be a benchmark for reasoning capability in LLMs. In this work, we\nintroduce a framework that generates whole proofs in a formal language to be\nused within systems that utilize the power of built-in tactics and\noff-the-shelf automated theorem provers. Our framework includes 3 components:\ngenerating natural language statements of the code to be verified, an LLM that\ngenerates formal proofs for the given statement, and a module employing\nheuristics for building the final proof. To train the LLM, we employ a 2-stage\nfine-tuning process, where we first use SFT-based training to enable the model\nto generate syntactically correct Isabelle code and then RL-based training that\nencourages the model to generate proofs verified by a theorem prover. We\nvalidate our framework using the miniF2F-test benchmark and the Isabelle proof\nassistant and design a use case to verify the correctness of the AWS S3 bucket\naccess policy code. We also curate a dataset based on the\nFVEL\\textsubscript{\\textnormal{ER}} dataset for future training tasks."}
{"id": "2504.17130", "pdf": "https://arxiv.org/pdf/2504.17130", "abs": "https://arxiv.org/abs/2504.17130", "authors": ["Hannah Cyberey", "David Evans"], "title": "Steering the CensorShip: Uncovering Representation Vectors for LLM \"Thought\" Control", "categories": ["cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) have transformed the way we access information.\nThese models are often tuned to refuse to comply with requests that are\nconsidered harmful and to produce responses that better align with the\npreferences of those who control the models. To understand how this\n\"censorship\" works. We use representation engineering techniques to study\nopen-weights safety-tuned models. We present a method for finding a\nrefusal--compliance vector that detects and controls the level of censorship in\nmodel outputs. We also analyze recent reasoning LLMs, distilled from\nDeepSeek-R1, and uncover an additional dimension of censorship through \"thought\nsuppression\". We show a similar approach can be used to find a vector that\nsuppresses the model's reasoning process, allowing us to remove censorship by\napplying the negative multiples of this vector"}
{"id": "2504.17004", "pdf": "https://arxiv.org/pdf/2504.17004", "abs": "https://arxiv.org/abs/2504.17004", "authors": ["Amin Karbasi", "Omar Montasser", "John Sous", "Grigoris Velegkas"], "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment."}
{"id": "2504.17087", "pdf": "https://arxiv.org/pdf/2504.17087", "abs": "https://arxiv.org/abs/2504.17087", "authors": ["Yuran Li", "Jama Hussein Mohamud", "Chongren Sun", "Di Wu", "Benoit Boulet"], "title": "Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments", "categories": ["cs.AI"], "comment": "12 pages, 5 figures, 6 tables", "summary": "Large language models (LLMs) are being widely applied across various fields,\nbut as tasks become more complex, evaluating their responses is increasingly\nchallenging. Compared to human evaluators, the use of LLMs to support\nperformance evaluation offers a more efficient alternative. However, most\nstudies focus mainly on aligning LLMs' judgments with human preferences,\noverlooking the existence of biases and mistakes in human judgment.\nFurthermore, how to select suitable LLM judgments given multiple potential LLM\nresponses remains underexplored. To address these two aforementioned issues, we\npropose a three-stage meta-judge selection pipeline: 1) developing a\ncomprehensive rubric with GPT-4 and human experts, 2) using three advanced LLM\nagents to score judgments, and 3) applying a threshold to filter out\nlow-scoring judgments. Compared to methods using a single LLM as both judge and\nmeta-judge, our pipeline introduces multi-agent collaboration and a more\ncomprehensive rubric. Experimental results on the JudgeBench dataset show about\n15.55\\% improvement compared to raw judgments and about 8.37\\% improvement over\nthe single-agent baseline. Our work demonstrates the potential of LLMs as\nmeta-judges and lays the foundation for future research on constructing\npreference datasets for LLM-as-a-judge reinforcement learning."}
{"id": "2504.17137", "pdf": "https://arxiv.org/pdf/2504.17137", "abs": "https://arxiv.org/abs/2504.17137", "authors": ["Chanhee Park", "Hyeonseok Moon", "Chanjun Park", "Heuiseok Lim"], "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL2025 Findings", "summary": "Retrieval-Augmented Generation (RAG) has gained prominence as an effective\nmethod for enhancing the generative capabilities of Large Language Models\n(LLMs) through the incorporation of external knowledge. However, the evaluation\nof RAG systems remains a challenge, due to the intricate interplay between\nretrieval and generation components. This limitation has resulted in a scarcity\nof benchmarks that facilitate a detailed, component-specific assessment. In\nthis work, we present MIRAGE, a Question Answering dataset specifically\ndesigned for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped\nto a retrieval pool of 37,800 entries, enabling an efficient and precise\nevaluation of both retrieval and generation tasks. We also introduce novel\nevaluation metrics aimed at measuring RAG adaptability, encompassing dimensions\nsuch as noise vulnerability, context acceptability, context insensitivity, and\ncontext misinterpretation. Through comprehensive experiments across various\nretriever-LLM configurations, we provide new insights into the optimal\nalignment of model pairs and the nuanced dynamics within RAG systems. The\ndataset and evaluation code are publicly available, allowing for seamless\nintegration and customization in diverse research settings\\footnote{The MIRAGE\ncode and data are available at https://github.com/nlpai-lab/MIRAGE."}
{"id": "2504.17028", "pdf": "https://arxiv.org/pdf/2504.17028", "abs": "https://arxiv.org/abs/2504.17028", "authors": ["Iman Khadir", "Shane Stevenson", "Henry Li", "Kyle Krick", "Abram Burrows", "David Hall", "Stan Posey", "Samuel S. P. Shen"], "title": "Democracy of AI Numerical Weather Models: An Example of Global Forecasting with FourCastNetv2 Made by a University Research Lab Using GPU", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": "12 pages, 8 figures", "summary": "This paper demonstrates the feasibility of democratizing AI-driven global\nweather forecasting models among university research groups by leveraging\nGraphics Processing Units (GPUs) and freely available AI models, such as\nNVIDIA's FourCastNetv2. FourCastNetv2 is an NVIDIA's advanced neural network\nfor weather prediction and is trained on a 73-channel subset of the European\nCentre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset\nat single levels and different pressure levels. Although the training\nspecifications for FourCastNetv2 are not released to the public, the training\ndocumentation of the model's first generation, FourCastNet, is available to all\nusers. The training had 64 A100 GPUs and took 16 hours to complete. Although\nNVIDIA's models offer significant reductions in both time and cost compared to\ntraditional Numerical Weather Prediction (NWP), reproducing published\nforecasting results presents ongoing challenges for resource-constrained\nuniversity research groups with limited GPU availability. We demonstrate both\n(i) leveraging FourCastNetv2 to create predictions through the designated\napplication programming interface (API) and (ii) utilizing NVIDIA hardware to\ntrain the original FourCastNet model. Further, this paper demonstrates the\ncapabilities and limitations of NVIDIA A100's for resource-limited research\ngroups in universities. We also explore data management, training efficiency,\nand model validation, highlighting the advantages and challenges of using\nlimited high-performance computing resources. Consequently, this paper and its\ncorresponding GitHub materials may serve as an initial guide for other\nuniversity research groups and courses related to machine learning, climate\nscience, and data science to develop research and education programs on AI\nweather forecasting, and hence help democratize the AI NWP in the digital\neconomy."}
{"id": "2504.17179", "pdf": "https://arxiv.org/pdf/2504.17179", "abs": "https://arxiv.org/abs/2504.17179", "authors": ["Mohammad Zarei", "Melanie A Jutras", "Eliana Evans", "Mike Tan", "Omid Aaramoon"], "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO", "68T45, 68T05 68T45, 68T05 68T45, 68T05", "I.2.6; I.2.10; I.4.8"], "comment": "8 pages, 10 figures. Accepted to IEEE Conference on Artificial\n  Intelligence (CAI), 2025", "summary": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately\ndetect objects and interpret their surroundings. However, even when trained\nusing millions of miles of real-world data, AVs are often unable to detect rare\nfailure modes (RFMs). The problem of RFMs is commonly referred to as the\n\"long-tail challenge\", due to the distribution of data including many instances\nthat are very rarely seen. In this paper, we present a novel approach that\nutilizes advanced generative and explainable AI techniques to aid in\nunderstanding RFMs. Our methods can be used to enhance the robustness and\nreliability of AVs when combined with both downstream model training and\ntesting. We extract segmentation masks for objects of interest (e.g., cars) and\ninvert them to create environmental masks. These masks, combined with carefully\ncrafted text prompts, are fed into a custom diffusion model. We leverage the\nStable Diffusion inpainting model guided by adversarial noise optimization to\ngenerate images containing diverse environments designed to evade object\ndetection models and expose vulnerabilities in AI systems. Finally, we produce\nnatural language descriptions of the generated RFMs that can guide developers\nand policymakers to improve the safety and reliability of AV systems."}
{"id": "2504.17192", "pdf": "https://arxiv.org/pdf/2504.17192", "abs": "https://arxiv.org/abs/2504.17192", "authors": ["Minju Seo", "Jinheon Baek", "Seongyun Lee", "Sung Ju Hwang"], "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning", "categories": ["cs.CL"], "comment": null, "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins."}
{"id": "2504.17058", "pdf": "https://arxiv.org/pdf/2504.17058", "abs": "https://arxiv.org/abs/2504.17058", "authors": ["Rahul Vishwakarma"], "title": "Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The generation of high-quality synthetic data presents significant challenges\nin machine learning research, particularly regarding statistical fidelity and\nuncertainty quantification. Existing generative models produce compelling\nsynthetic samples but lack rigorous statistical guarantees about their relation\nto the underlying data distribution, limiting their applicability in critical\ndomains requiring robust error bounds. We address this fundamental limitation\nby presenting a novel framework that incorporates conformal prediction\nmethodologies into Generative Adversarial Networks (GANs). By integrating\nmultiple conformal prediction paradigms including Inductive Conformal\nPrediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction,\nand Venn-Abers Predictors, we establish distribution-free uncertainty\nquantification in generated samples. This approach, termed Conformalized GAN\n(cGAN), demonstrates enhanced calibration properties while maintaining the\ngenerative power of traditional GANs, producing synthetic data with provable\nstatistical guarantees. We provide rigorous mathematical proofs establishing\nfinite-sample validity guarantees and asymptotic efficiency properties,\nenabling the reliable application of synthetic data in high-stakes domains\nincluding healthcare, finance, and autonomous systems."}
{"id": "2504.17282", "pdf": "https://arxiv.org/pdf/2504.17282", "abs": "https://arxiv.org/abs/2504.17282", "authors": ["Lynn Cherif", "Flemming Kondrup", "David Venuto", "Ankit Anand", "Doina Precup", "Khimya Khetarpal"], "title": "Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Agents that can autonomously navigate the web through a graphical user\ninterface (GUI) using a unified action space (e.g., mouse and keyboard actions)\ncan require very large amounts of domain-specific expert demonstrations to\nachieve good performance. Low sample efficiency is often exacerbated in\nsparse-reward and large-action-space environments, such as a web GUI, where\nonly a few actions are relevant in any given situation. In this work, we\nconsider the low-data regime, with limited or no access to expert behavior. To\nenable sample-efficient learning, we explore the effect of constraining the\naction space through $\\textit{intent-based affordances}$ -- i.e., considering\nin any situation only the subset of actions that achieve a desired outcome. We\npropose $\\textbf{Code as Generative Affordances}$ $(\\textbf{$\\texttt{CoGA}$})$,\na method that leverages pre-trained vision-language models (VLMs) to generate\ncode that determines affordable actions through implicit intent-completion\nfunctions and using a fully-automated program generation and verification\npipeline. These programs are then used in-the-loop of a reinforcement learning\nagent to return a set of affordances given a pixel observation. By greatly\nreducing the number of actions that an agent must consider, we demonstrate on a\nwide range of tasks in the MiniWob++ benchmark that: $\\textbf{1)}$\n$\\texttt{CoGA}$ is orders of magnitude more sample efficient than its RL agent,\n$\\textbf{2)}$ $\\texttt{CoGA}$'s programs can generalize within a family of\ntasks, and $\\textbf{3)}$ $\\texttt{CoGA}$ performs better or on par compared\nwith behavior cloning when a small number of expert demonstrations is\navailable."}
{"id": "2504.17200", "pdf": "https://arxiv.org/pdf/2504.17200", "abs": "https://arxiv.org/abs/2504.17200", "authors": ["Yangxinyu Xie", "Bowen Jiang", "Tanwi Mallick", "Joshua David Bergerson", "John K. Hutchison", "Duane R. Verner", "Jordan Branham", "M. Ross Alexander", "Robert B. Ross", "Yan Feng", "Leslie-Anne Levy", "Weijie Su", "Camillo J. Taylor"], "title": "A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are a transformational capability at the\nfrontier of artificial intelligence and machine learning that can support\ndecision-makers in addressing pressing societal challenges such as extreme\nnatural hazard events. As generalized models, LLMs often struggle to provide\ncontext-specific information, particularly in areas requiring specialized\nknowledge. In this work we propose a retrieval-augmented generation (RAG)-based\nmulti-agent LLM system to support analysis and decision-making in the context\nof natural hazards and extreme weather events. As a proof of concept, we\npresent WildfireGPT, a specialized system focused on wildfire hazards. The\narchitecture employs a user-centered, multi-agent design to deliver tailored\nrisk insights across diverse stakeholder groups. By integrating natural hazard\nand extreme weather projection data, observational datasets, and scientific\nliterature through an RAG framework, the system ensures both the accuracy and\ncontextual relevance of the information it provides. Evaluation across ten\nexpert-led case studies demonstrates that WildfireGPT significantly outperforms\nexisting LLM-based solutions for decision support."}
{"id": "2504.17065", "pdf": "https://arxiv.org/pdf/2504.17065", "abs": "https://arxiv.org/abs/2504.17065", "authors": ["Sahar Bagherkhani", "Jackson Christopher Earls", "Franco De Flaviis", "Pierre Baldi"], "title": "Antenna Near-Field Reconstruction from Far-Field Data Using Convolutional Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Electromagnetic field reconstruction is crucial in many applications,\nincluding antenna diagnostics, electromagnetic interference analysis, and\nsystem modeling. This paper presents a deep learning-based approach for\nFar-Field to Near-Field (FF-NF) transformation using Convolutional Neural\nNetworks (CNNs). The goal is to reconstruct near-field distributions from the\nfar-field data of an antenna without relying on explicit analytical\ntransformations. The CNNs are trained on paired far-field and near-field data\nand evaluated using mean squared error (MSE). The best model achieves a\ntraining error of 0.0199 and a test error of 0.3898. Moreover, visual\ncomparisons between the predicted and true near-field distributions demonstrate\nthe model's effectiveness in capturing complex electromagnetic field behavior,\nhighlighting the potential of deep learning in electromagnetic field\nreconstruction."}
{"id": "2504.17295", "pdf": "https://arxiv.org/pdf/2504.17295", "abs": "https://arxiv.org/abs/2504.17295", "authors": ["Shahrzad Khayatbashi", "Viktor SjÃ¶lind", "Anders GranÃ¥ker", "Amin Jalali"], "title": "AI-Enhanced Business Process Automation: A Case Study in the Insurance Domain Using Object-Centric Process Mining", "categories": ["cs.AI"], "comment": null, "summary": "Recent advancements in Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), have enhanced organizations' ability to reengineer\nbusiness processes by automating knowledge-intensive tasks. This automation\ndrives digital transformation, often through gradual transitions that improve\nprocess efficiency and effectiveness. To fully assess the impact of such\nautomation, a data-driven analysis approach is needed - one that examines how\ntraditional and AI-enhanced process variants coexist during this transition.\nObject-Centric Process Mining (OCPM) has emerged as a valuable method that\nenables such analysis, yet real-world case studies are still needed to\ndemonstrate its applicability. This paper presents a case study from the\ninsurance sector, where an LLM was deployed in production to automate the\nidentification of claim parts, a task previously performed manually and\nidentified as a bottleneck for scalability. To evaluate this transformation, we\napply OCPM to assess the impact of AI-driven automation on process scalability.\nOur findings indicate that while LLMs significantly enhance operational\ncapacity, they also introduce new process dynamics that require further\nrefinement. This study also demonstrates the practical application of OCPM in a\nreal-world setting, highlighting its advantages and limitations."}
{"id": "2504.17220", "pdf": "https://arxiv.org/pdf/2504.17220", "abs": "https://arxiv.org/abs/2504.17220", "authors": ["Kaidong Feng", "Zhu Sun", "Jie Yang", "Hui Fang", "Xinghua Qu", "Wenyuan Liu"], "title": "Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "LLMs are increasingly explored for bundle generation, thanks to their\nreasoning capabilities and knowledge. However, deploying large-scale LLMs\nintroduces significant efficiency challenges, primarily high computational\ncosts during fine-tuning and inference due to their massive parameterization.\nKnowledge distillation (KD) offers a promising solution, transferring expertise\nfrom large teacher models to compact student models. This study systematically\ninvestigates knowledge distillation approaches for bundle generation, aiming to\nminimize computational demands while preserving performance. We explore three\ncritical research questions: (1) how does the format of KD impact bundle\ngeneration performance? (2) to what extent does the quantity of distilled\nknowledge influence performance? and (3) how do different ways of utilizing the\ndistilled knowledge affect performance? We propose a comprehensive KD framework\nthat (i) progressively extracts knowledge (patterns, rules, deep thoughts);\n(ii) captures varying quantities of distilled knowledge through different\nstrategies; and (iii) exploits complementary LLM adaptation techniques\n(in-context learning, supervised fine-tuning, combination) to leverage\ndistilled knowledge in small student models for domain-specific adaptation and\nenhanced efficiency. Extensive experiments provide valuable insights into how\nknowledge format, quantity, and utilization methodologies collectively shape\nLLM-based bundle generation performance, exhibiting KD's significant potential\nfor more efficient yet effective LLM-based bundle generation."}
{"id": "2504.17066", "pdf": "https://arxiv.org/pdf/2504.17066", "abs": "https://arxiv.org/abs/2504.17066", "authors": ["Kewen Peng", "Yicheng Yang", "Hao Zhuo", "Tim Menzies"], "title": "Whence Is A Model Fair? Fixing Fairness Bugs via Propensity Score Matching", "categories": ["cs.LG", "cs.CY", "cs.SE", "stat.ML"], "comment": null, "summary": "Fairness-aware learning aims to mitigate discrimination against specific\nprotected social groups (e.g., those categorized by gender, ethnicity, age)\nwhile minimizing predictive performance loss. Despite efforts to improve\nfairness in machine learning, prior studies have shown that many models remain\nunfair when measured against various fairness metrics. In this paper, we\nexamine whether the way training and testing data are sampled affects the\nreliability of reported fairness metrics. Since training and test sets are\noften randomly sampled from the same population, bias present in the training\ndata may still exist in the test data, potentially skewing fairness\nassessments. To address this, we propose FairMatch, a post-processing method\nthat applies propensity score matching to evaluate and mitigate bias. FairMatch\nidentifies control and treatment pairs with similar propensity scores in the\ntest set and adjusts decision thresholds for different subgroups accordingly.\nFor samples that cannot be matched, we perform probabilistic calibration using\nfairness-aware loss functions. Experimental results demonstrate that our\napproach can (a) precisely locate subsets of the test data where the model is\nunbiased, and (b) significantly reduce bias on the remaining data. Overall,\npropensity score matching offers a principled way to improve both fairness\nevaluation and mitigation, without sacrificing predictive performance."}
{"id": "2504.17356", "pdf": "https://arxiv.org/pdf/2504.17356", "abs": "https://arxiv.org/abs/2504.17356", "authors": ["Weiliang Zhang", "Xiaohan Huang", "Yi Du", "Ziyue Qiao", "Qingqing Long", "Zhen Meng", "Yuanchun Zhou", "Meng Xiao"], "title": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": "20 pages, keywords: Automated Feature Engineering, Tabular Dataset,\n  Multi-Agent Reinforcement Learning, Feature Selection", "summary": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved."}
{"id": "2504.17238", "pdf": "https://arxiv.org/pdf/2504.17238", "abs": "https://arxiv.org/abs/2504.17238", "authors": ["Jinfeng Zhou", "Yuxuan Chen", "Jianing Yin", "Yongkang Huang", "Yihan Shi", "Xikun Zhang", "Libiao Peng", "Rongsheng Zhang", "Tangjie Lv", "Zhipeng Hu", "Hongning Wang", "Minlie Huang"], "title": "Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Cognitive Restructuring (CR) is a psychotherapeutic process aimed at\nidentifying and restructuring an individual's negative thoughts, arising from\nmental health challenges, into more helpful and positive ones via multi-turn\ndialogues. Clinician shortage and stigma urge the development of human-LLM\ninteractive psychotherapy for CR. Yet, existing efforts implement CR via simple\ntext rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to\nalign with the psychotherapeutic process for effective CR. To address this gap,\nwe propose CRDial, a novel framework for CR, which creates multi-turn dialogues\nwith specifically designed identification and restructuring stages of negative\nthoughts, integrates sentence-level supportive conversation strategies, and\nadopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we\ndistill Crisp, a large-scale and high-quality bilingual dialogue dataset, from\nLLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and\n14B scales. Extensive human studies show the superiority of Crispers in\npointwise, pairwise, and intervention evaluations."}
{"id": "2504.17068", "pdf": "https://arxiv.org/pdf/2504.17068", "abs": "https://arxiv.org/abs/2504.17068", "authors": ["Pranav Kantroo", "GÃ¼nter P. Wagner", "Benjamin B. Machta"], "title": "In-Context Learning can distort the relationship between sequence likelihoods and biological fitness", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Language models have emerged as powerful predictors of the viability of\nbiological sequences. During training these models learn the rules of the\ngrammar obeyed by sequences of amino acids or nucleotides. Once trained, these\nmodels can take a sequence as input and produce a likelihood score as an\noutput; a higher likelihood implies adherence to the learned grammar and\ncorrelates with experimental fitness measurements. Here we show that in-context\nlearning can distort the relationship between fitness and likelihood scores of\nsequences. This phenomenon most prominently manifests as anomalously high\nlikelihood scores for sequences that contain repeated motifs. We use protein\nlanguage models with different architectures trained on the masked language\nmodeling objective for our experiments, and find transformer-based models to be\nparticularly vulnerable to this effect. This behavior is mediated by a look-up\noperation where the model seeks the identity of the masked position by using\nthe other copy of the repeated motif as a reference. This retrieval behavior\ncan override the model's learned priors. This phenomenon persists for\nimperfectly repeated sequences, and extends to other kinds of biologically\nrelevant features such as reversed complement motifs in RNA sequences that fold\ninto hairpin structures."}
{"id": "2504.17402", "pdf": "https://arxiv.org/pdf/2504.17402", "abs": "https://arxiv.org/abs/2504.17402", "authors": ["Anna Sofia Lippolis", "Mohammad Javad Saeedizade", "Robin Keskisarkka", "Aldo Gangemi", "Eva Blomqvist", "Andrea Giovanni Nuzzolese"], "title": "Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown significant potential for ontology\nengineering. However, it is still unclear to what extent they are applicable to\nthe task of domain-specific ontology generation. In this study, we explore the\napplication of LLMs for automated ontology generation and evaluate their\nperformance across different domains. Specifically, we investigate the\ngeneralizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both\nequipped with reasoning capabilities, by generating ontologies from a set of\ncompetency questions (CQs) and related user stories. Our experimental setup\ncomprises six distinct domains carried out in existing ontology engineering\nprojects and a total of 95 curated CQs designed to test the models' reasoning\nfor ontology engineering. Our findings show that with both LLMs, the\nperformance of the experiments is remarkably consistent across all domains,\nindicating that these methods are capable of generalizing ontology generation\ntasks irrespective of the domain. These results highlight the potential of\nLLM-based approaches in achieving scalable and domain-agnostic ontology\nconstruction and lay the groundwork for further research into enhancing\nautomated reasoning and knowledge representation techniques."}
{"id": "2504.17252", "pdf": "https://arxiv.org/pdf/2504.17252", "abs": "https://arxiv.org/abs/2504.17252", "authors": ["Ocheme Anthony Ekle", "Biswarup Das"], "title": "Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo", "categories": ["cs.CL", "cs.LG", "68T50, 68T01", "I.2.7; I.2.1"], "comment": "25 pages, 14 combined figures (19 total), includes horizontal\n  layouts. Submitted to arXiv for open access", "summary": "In this study, we develop Neural Machine Translation (NMT) and\nTransformer-based transfer learning models for English-to-Igbo translation - a\nlow-resource African language spoken by over 40 million people across Nigeria\nand West Africa. Our models are trained on a curated and benchmarked dataset\ncompiled from Bible corpora, local news, Wikipedia articles, and Common Crawl,\nall verified by native language experts. We leverage Recurrent Neural Network\n(RNN) architectures, including Long Short-Term Memory (LSTM) and Gated\nRecurrent Units (GRU), enhanced with attention mechanisms to improve\ntranslation accuracy. To further enhance performance, we apply transfer\nlearning using MarianNMT pre-trained models within the SimpleTransformers\nframework. Our RNN-based system achieves competitive results, closely matching\nexisting English-Igbo benchmarks. With transfer learning, we observe a\nperformance gain of +4.83 BLEU points, reaching an estimated translation\naccuracy of 70%. These findings highlight the effectiveness of combining RNNs\nwith transfer learning to address the performance gap in low-resource language\ntranslation tasks."}
{"id": "2504.17073", "pdf": "https://arxiv.org/pdf/2504.17073", "abs": "https://arxiv.org/abs/2504.17073", "authors": ["David Lu", "Lior Maman", "Jackson Earls", "Amir Boag", "Pierre Baldi"], "title": "Sparse Phased Array Optimization Using Deep Learning", "categories": ["cs.LG"], "comment": null, "summary": "Antenna arrays are widely used in wireless communication, radar systems,\nradio astronomy, and military defense to enhance signal strength, directivity,\nand interference suppression. We introduce a deep learning-based optimization\napproach that enhances the design of sparse phased arrays by reducing grating\nlobes. This approach begins by generating sparse array configurations to\naddress the non-convex challenges and extensive degrees of freedom inherent in\narray design. We use neural networks to approximate the non-convex cost\nfunction that estimates the energy ratio between the main and side lobes. This\ndifferentiable approximation facilitates cost function minimization through\ngradient descent, optimizing the antenna elements' coordinates and leading to\nan improved layout. Additionally, we incorporate a tailored penalty mechanism\nthat includes various physical and design constraints into the optimization\nprocess, enhancing its robustness and practical applicability. We demonstrate\nthe effectiveness of our method by applying it to the ten array configurations\nwith the lowest initial costs, achieving further cost reductions ranging from\n411% to 643%, with an impressive average improvement of 552%. By significantly\nreducing side lobe levels in antenna arrays, this breakthrough paves the way\nfor ultra-precise beamforming, enhanced interference mitigation, and\nnext-generation wireless and radar systems with unprecedented efficiency and\nclarity."}
{"id": "2504.17404", "pdf": "https://arxiv.org/pdf/2504.17404", "abs": "https://arxiv.org/abs/2504.17404", "authors": ["Feifei Zhao", "Yuwei Wang", "Enmeng Lu", "Dongcheng Zhao", "Bing Han", "Haibo Tong", "Yao Liang", "Dongqi Liang", "Kang Sun", "Lei Wang", "Yitao Liang", "Chao Liu", "Yaodong Yang", "Yi Zeng"], "title": "Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society", "categories": ["cs.AI"], "comment": null, "summary": "Artificial Intelligence (AI) systems are becoming increasingly powerful and\nautonomous, and may progress to surpass human intelligence levels, namely\nArtificial Superintelligence (ASI). During the progression from AI to ASI, it\nmay exceed human control, violate human values, and even lead to irreversible\ncatastrophic consequences in extreme cases. This gives rise to a pressing issue\nthat needs to be addressed: superalignment, ensuring that AI systems much\nsmarter than humans, remain aligned with human (compatible) intentions and\nvalues. Existing scalable oversight and weak-to-strong generalization methods\nmay prove substantially infeasible and inadequate when facing ASI. We must\nexplore safer and more pluralistic frameworks and approaches for\nsuperalignment. In this paper, we redefine superalignment as the human-AI\nco-alignment towards a sustainable symbiotic society, and highlight a framework\nthat integrates external oversight and intrinsic proactive alignment. External\noversight superalignment should be grounded in human-centered ultimate\ndecision, supplemented by interpretable automated evaluation and correction, to\nachieve continuous alignment with humanity's evolving values. Intrinsic\nproactive superalignment is rooted in a profound understanding of the self,\nothers, and society, integrating self-awareness, self-reflection, and empathy\nto spontaneously infer human intentions, distinguishing good from evil and\nproactively considering human well-being, ultimately attaining human-AI\nco-alignment through iterative interaction. The integration of\nexternally-driven oversight with intrinsically-driven proactive alignment\nempowers sustainable symbiotic societies through human-AI co-alignment, paving\nthe way for achieving safe and beneficial AGI and ASI for good, for human, and\nfor a symbiotic ecology."}
{"id": "2504.17264", "pdf": "https://arxiv.org/pdf/2504.17264", "abs": "https://arxiv.org/abs/2504.17264", "authors": ["Zhaolu Kang", "Hongtian Cai", "Xiangyang Ji", "Jinzhe Li", "Nanfei Gu"], "title": "JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "In recent years, Unsupervised Domain Adaptation (UDA) has gained significant\nattention in the field of Natural Language Processing (NLP) owing to its\nability to enhance model generalization across diverse domains. However, its\napplication for knowledge transfer between distinct legal domains remains\nlargely unexplored. To address the challenges posed by lengthy and complex\nlegal texts and the limited availability of large-scale annotated datasets, we\npropose JurisCTC, a novel model designed to improve the accuracy of Legal\nJudgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC\nfacilitates effective knowledge transfer across various legal domains and\nemploys contrastive learning to distinguish samples from different domains.\nSpecifically, for the LJP task, we enable knowledge transfer between civil and\ncriminal law domains. Compared to other models and specific large language\nmodels (LLMs), JurisCTC demonstrates notable advancements, achieving peak\naccuracies of 76.59% and 78.83%, respectively."}
{"id": "2504.17074", "pdf": "https://arxiv.org/pdf/2504.17074", "abs": "https://arxiv.org/abs/2504.17074", "authors": ["William R. Keely", "Otto LamminpÃ¤Ã¤", "Steffen Mauceri", "Sean M. R. Crowell", "Christopher W. O'Dell", "Gregory R. McGarragh"], "title": "Conditional Diffusion-Based Retrieval of Atmospheric CO2 from Earth Observing Spectroscopy", "categories": ["cs.LG", "astro-ph.IM"], "comment": "Published as a workshop paper in \"Tackling Climate Change with\n  Machine Learning\", ICLR 2025. https://www.climatechange.ai/papers/iclr2025/12", "summary": "Satellite-based estimates of greenhouse gas (GHG) properties from\nobservations of reflected solar spectra are integral for understanding and\nmonitoring complex terrestrial systems and their impact on the carbon cycle due\nto their near global coverage. Known as retrieval, making GHG concentration\nestimations from these observations is a non-linear Bayesian inverse problem,\nwhich is operationally solved using a computationally expensive algorithm\ncalled Optimal Estimation (OE), providing a Gaussian approximation to a\nnon-Gaussian posterior. This leads to issues in solver algorithm convergence,\nand to unrealistically confident uncertainty estimates for the retrieved\nquantities. Upcoming satellite missions will provide orders of magnitude more\ndata than the current constellation of GHG observers. Development of fast and\naccurate retrieval algorithms with robust uncertainty quantification is\ncritical. Doing so stands to provide substantial climate impact of moving\ntowards the goal of near continuous real-time global monitoring of carbon\nsources and sinks which is essential for policy making. To achieve this goal,\nwe propose a diffusion-based approach to flexibly retrieve a Gaussian or\nnon-Gaussian posterior, for NASA's Orbiting Carbon Observatory-2 spectrometer,\nwhile providing a substantial computational speed-up over the current\noperational state-of-the-art."}
{"id": "2504.17531", "pdf": "https://arxiv.org/pdf/2504.17531", "abs": "https://arxiv.org/abs/2504.17531", "authors": ["Justus Flerlage", "Ilja Behnke", "Odej Kao"], "title": "Towards Machine-Generated Code for the Resolution of User Intentions", "categories": ["cs.AI"], "comment": null, "summary": "The growing capabilities of Artificial Intelligence (AI), particularly Large\nLanguage Models (LLMs), prompt a reassessment of the interaction mechanisms\nbetween users and their devices. Currently, users are required to use a set of\nhigh-level applications to achieve their desired results. However, the advent\nof AI may signal a shift in this regard, as its capabilities have generated\nnovel prospects for user-provided intent resolution through the deployment of\nmodel-generated code, which is tantamount to the generation of workflows\ncomprising a multitude of interdependent steps. This development represents a\nsignificant progression in the realm of hybrid workflows, where human and\nartificial intelligence collaborate to address user intentions, with the former\nresponsible for defining these intentions and the latter for implementing the\nsolutions to address them. In this paper, we investigate the feasibility of\ngenerating and executing workflows through code generation that results from\nprompting an LLM with a concrete user intention, such as \\emph{Please send my\ncar title to my insurance company}, and a simplified application programming\ninterface for a GUI-less operating system. We provide in-depth analysis and\ncomparison of various user intentions, the resulting code, and its execution.\nThe findings demonstrate a general feasibility of our approach and that the\nemployed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of\ncode-oriented workflows in accordance with provided user intentions."}
{"id": "2504.17279", "pdf": "https://arxiv.org/pdf/2504.17279", "abs": "https://arxiv.org/abs/2504.17279", "authors": ["Xiuying Chen", "Tairan Wang", "Juexiao Zhou", "Zirui Song", "Xin Gao", "Xiangliang Zhang"], "title": "Evaluating and Mitigating Bias in AI-Based Medical Text Generation", "categories": ["cs.CL"], "comment": "12 pages, 8 figures, published in Nature Computational Science", "summary": "Artificial intelligence (AI) systems, particularly those based on deep\nlearning models, have increasingly achieved expert-level performance in medical\napplications. However, there is growing concern that such AI systems may\nreflect and amplify human bias, and reduce the quality of their performance in\nhistorically under-served populations. The fairness issue has attracted\nconsiderable research interest in the medical imaging classification field, yet\nit remains understudied in the text generation domain. In this study, we\ninvestigate the fairness problem in text generation within the medical field\nand observe significant performance discrepancies across different races,\nsexes, and age groups, including intersectional groups, various model scales,\nand different evaluation metrics. To mitigate this fairness issue, we propose\nan algorithm that selectively optimizes those underperformed groups to reduce\nbias. The selection rules take into account not only word-level accuracy but\nalso the pathology accuracy to the target reference, while ensuring that the\nentire process remains fully differentiable for effective model training. Our\nevaluations across multiple backbones, datasets, and modalities demonstrate\nthat our proposed algorithm enhances fairness in text generation without\ncompromising overall performance. Specifically, the disparities among various\ngroups across different metrics were diminished by more than 30% with our\nalgorithm, while the relative change in text generation accuracy was typically\nwithin 2%. By reducing the bias generated by deep learning models, our proposed\napproach can potentially alleviate concerns about the fairness and reliability\nof text generation diagnosis in medical domain.\n  Our code is publicly available to facilitate further research at\nhttps://github.com/iriscxy/GenFair."}
{"id": "2504.17079", "pdf": "https://arxiv.org/pdf/2504.17079", "abs": "https://arxiv.org/abs/2504.17079", "authors": ["Esam Mahdi", "C. Martin-Barreiro", "X. Cabezas"], "title": "A Novel Hybrid Approach Using an Attention-Based Transformer + GRU Model for Predicting Cryptocurrency Prices", "categories": ["cs.LG", "stat.AP"], "comment": null, "summary": "In this article, we introduce a novel deep learning hybrid model that\nintegrates attention Transformer and Gated Recurrent Unit (GRU) architectures\nto improve the accuracy of cryptocurrency price predictions. By combining the\nTransformer's strength in capturing long-range patterns with the GRU's ability\nto model short-term and sequential trends, the hybrid model provides a\nwell-rounded approach to time series forecasting. We apply the model to predict\nthe daily closing prices of Bitcoin and Ethereum based on historical data that\ninclude past prices, trading volumes, and the Fear and Greed index. We evaluate\nthe performance of our proposed model by comparing it with four other machine\nlearning models: two are non-sequential feedforward models: Radial Basis\nFunction Network (RBFN) and General Regression Neural Network (GRNN), and two\nare bidirectional sequential memory-based models: Bidirectional Long-Short-Term\nMemory (BiLSTM) and Bidirectional Gated Recurrent Unit (BiGRU). The performance\nof the model is assessed using several metrics, including Mean Squared Error\n(MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean\nAbsolute Percentage Error (MAPE), along with statistical validation through the\nnonparametric Friedman test followed by a post hoc Wilcoxon signed rank test.\nThe results demonstrate that our hybrid model consistently achieves superior\naccuracy, highlighting its effectiveness for financial prediction tasks. These\nfindings provide valuable insights for improving real-time decision making in\ncryptocurrency markets and support the growing use of hybrid deep learning\nmodels in financial analytics."}
{"id": "2504.17544", "pdf": "https://arxiv.org/pdf/2504.17544", "abs": "https://arxiv.org/abs/2504.17544", "authors": ["W. Russell Neuman", "Chad Coleman", "Ali Dasdan", "Safinah Ali", "Manan Shah"], "title": "Auditing the Ethical Logic of Generative AI Models", "categories": ["cs.AI"], "comment": null, "summary": "As generative AI models become increasingly integrated into high-stakes\ndomains, the need for robust methods to evaluate their ethical reasoning\nbecomes increasingly important. This paper introduces a five-dimensional audit\nmodel -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth\nof Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic\nof leading large language models (LLMs). Drawing on traditions from applied\nethics and higher-order thinking, we present a multi-battery prompt approach,\nincluding novel ethical dilemmas, to probe the models' reasoning across diverse\ncontexts. We benchmark seven major LLMs finding that while models generally\nconverge on ethical decisions, they vary in explanatory rigor and moral\nprioritization. Chain-of-Thought prompting and reasoning-optimized models\nsignificantly enhance performance on our audit metrics. This study introduces a\nscalable methodology for ethical benchmarking of AI systems and highlights the\npotential for AI to complement human moral reasoning in complex decision-making\ncontexts."}
{"id": "2504.17309", "pdf": "https://arxiv.org/pdf/2504.17309", "abs": "https://arxiv.org/abs/2504.17309", "authors": ["Junyan Zhang", "Shuliang Liu", "Aiwei Liu", "Yubo Gao", "Jungang Li", "Xiaojie Gu", "Xuming Hu"], "title": "CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality", "categories": ["cs.CL"], "comment": "Published at the 1st workshop on GenAI Watermarking, collocated with\n  ICLR 2025", "summary": "Watermarking technology is a method used to trace the usage of content\ngenerated by large language models. Sentence-level watermarking aids in\npreserving the semantic integrity within individual sentences while maintaining\ngreater robustness. However, many existing sentence-level watermarking\ntechniques depend on arbitrary segmentation or generation processes to embed\nwatermarks, which can limit the availability of appropriate sentences. This\nlimitation, in turn, compromises the quality of the generated response. To\naddress the challenge of balancing high text quality with robust watermark\ndetection, we propose CoheMark, an advanced sentence-level watermarking\ntechnique that exploits the cohesive relationships between sentences for better\nlogical fluency. The core methodology of CoheMark involves selecting sentences\nthrough trained fuzzy c-means clustering and applying specific next sentence\nselection criteria. Experimental evaluations demonstrate that CoheMark achieves\nstrong watermark strength while exerting minimal impact on text quality."}
{"id": "2504.17099", "pdf": "https://arxiv.org/pdf/2504.17099", "abs": "https://arxiv.org/abs/2504.17099", "authors": ["Martin Boeckling", "Heiko Paulheim", "Sarah Detzler"], "title": "GeoRDF2Vec Learning Location-Aware Entity Representations in Knowledge Graphs", "categories": ["cs.LG", "cs.SI"], "comment": "18 pages, ESWC 2025", "summary": "Many knowledge graphs contain a substantial number of spatial entities, such\nas cities, buildings, and natural landmarks. For many of these entities, exact\ngeometries are stored within the knowledge graphs. However, most existing\napproaches for learning entity representations do not take these geometries\ninto account. In this paper, we introduce a variant of RDF2Vec that\nincorporates geometric information to learn location-aware embeddings of\nentities. Our approach expands different nodes by flooding the graph from\ngeographic nodes, ensuring that each reachable node is considered. Based on the\nresulting flooded graph, we apply a modified version of RDF2Vec that biases\ngraph walks using spatial weights. Through evaluations on multiple benchmark\ndatasets, we demonstrate that our approach outperforms both non-location-aware\nRDF2Vec and GeoTransE."}
{"id": "2504.16940", "pdf": "https://arxiv.org/pdf/2504.16940", "abs": "https://arxiv.org/abs/2504.16940", "authors": ["Drew Linsley", "Pinyuan Feng", "Thomas Serre"], "title": "Can deep neural networks learn biological vision?", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep neural networks (DNNs) once showed increasing alignment with primate\nneural responses as they improved on computer vision benchmarks. This trend\nraised the exciting possibility that better models of biological vision would\ncome as a byproduct of the deep learning revolution in artificial intelligence.\nHowever, the trend has reversed over recent years as DNNs have scaled to human\nor superhuman recognition accuracy, a divergence that may stem from modern DNNs\nlearning to rely on different visual features than primates to solve tasks.\nWhere will better computational models of biological vision come from? We\npropose that vision science must break from artificial intelligence to develop\nalgorithms that are designed with biological visual systems in mind instead of\ninternet data benchmarks. We predict that the next generation of deep learning\nmodels of biological vision will be trained with data diets, training routines,\nand objectives that are closer to those that shape human vision than those that\nare in use today."}
{"id": "2504.17311", "pdf": "https://arxiv.org/pdf/2504.17311", "abs": "https://arxiv.org/abs/2504.17311", "authors": ["Yulia Otmakhova", "Hung Thinh Truong", "Rahmad Mahendra", "Zenan Zhai", "Rongxin Zhu", "Daniel Beck", "Jey Han Lau"], "title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors."}
{"id": "2504.17109", "pdf": "https://arxiv.org/pdf/2504.17109", "abs": "https://arxiv.org/abs/2504.17109", "authors": ["Zhaobin Mo", "Xiangyi Liao", "Dominik A. Karbowski", "Yanbing Wang"], "title": "Discovering the Precursors of Traffic Breakdowns Using Spatiotemporal Graph Attribution Networks", "categories": ["cs.LG"], "comment": null, "summary": "Understanding and predicting the precursors of traffic breakdowns is critical\nfor improving road safety and traffic flow management. This paper presents a\nnovel approach combining spatiotemporal graph neural networks (ST-GNNs) with\nShapley values to identify and interpret traffic breakdown precursors. By\nextending Shapley explanation methods to a spatiotemporal setting, our proposed\nmethod bridges the gap between black-box neural network predictions and\ninterpretable causes. We demonstrate the method on the Interstate-24 data, and\nidentify that road topology and abrupt braking are major factors that lead to\ntraffic breakdowns."}
{"id": "2504.16942", "pdf": "https://arxiv.org/pdf/2504.16942", "abs": "https://arxiv.org/abs/2504.16942", "authors": ["Shushman Choudhury", "Elad Aharoni", "Chandrakumari Suvarna", "Iveel Tsogsuren", "Abdul Rahman Kreidieh", "Chun-Ta Lu", "Neha Arora"], "title": "S2Vec: Self-Supervised Geospatial Embeddings", "categories": ["cs.SI", "cs.AI", "cs.CV"], "comment": "To be submitted to ACM Transactions on Spatial Algorithms and Systems", "summary": "Scalable general-purpose representations of the built environment are crucial\nfor geospatial artificial intelligence applications. This paper introduces\nS2Vec, a novel self-supervised framework for learning such geospatial\nembeddings. S2Vec uses the S2 Geometry library to partition large areas into\ndiscrete S2 cells, rasterizes built environment feature vectors within cells as\nimages, and applies masked autoencoding on these rasterized images to encode\nthe feature vectors. This approach yields task-agnostic embeddings that capture\nlocal feature characteristics and broader spatial relationships. We evaluate\nS2Vec on three large-scale socioeconomic prediction tasks, showing its\ncompetitive performance against state-of-the-art image-based embeddings. We\nalso explore the benefits of combining S2Vec embeddings with image-based\nembeddings downstream, showing that such multimodal fusion can often improve\nperformance. Our results highlight how S2Vec can learn effective\ngeneral-purpose geospatial representations and how it can complement other data\nmodalities in geospatial artificial intelligence."}
{"id": "2504.17332", "pdf": "https://arxiv.org/pdf/2504.17332", "abs": "https://arxiv.org/abs/2504.17332", "authors": ["Zihan Wang", "Lu Yuan", "Zhengxuan Zhang", "Qing Zhao"], "title": "Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection", "categories": ["cs.CL"], "comment": null, "summary": "In the digital era, social media has become a major conduit for information\ndissemination, yet it also facilitates the rapid spread of misinformation.\nTraditional misinformation detection methods primarily focus on surface-level\nfeatures, overlooking the crucial roles of human empathy in the propagation\nprocess. To address this gap, we propose the Dual-Aspect Empathy Framework\n(DAE), which integrates cognitive and emotional empathy to analyze\nmisinformation from both the creator and reader perspectives. By examining\ncreators' cognitive strategies and emotional appeals, as well as simulating\nreaders' cognitive judgments and emotional responses using Large Language\nModels (LLMs), DAE offers a more comprehensive and human-centric approach to\nmisinformation detection. Moreover, we further introduce an empathy-aware\nfiltering mechanism to enhance response authenticity and diversity.\nExperimental results on benchmark datasets demonstrate that DAE outperforms\nexisting methods, providing a novel paradigm for multimodal misinformation\ndetection."}
{"id": "2504.17140", "pdf": "https://arxiv.org/pdf/2504.17140", "abs": "https://arxiv.org/abs/2504.17140", "authors": ["Ashish Ranjan", "Ayush Agarwal", "Shalin Barot", "Sushant Kumar"], "title": "Scalable Permutation-Aware Modeling for Temporal Set Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Temporal set prediction involves forecasting the elements that will appear in\nthe next set, given a sequence of prior sets, each containing a variable number\nof elements. Existing methods often rely on intricate architectures with\nsubstantial computational overhead, which hampers their scalability. In this\nwork, we introduce a novel and scalable framework that leverages\npermutation-equivariant and permutation-invariant transformations to\nefficiently model set dynamics. Our approach significantly reduces both\ntraining and inference time while maintaining competitive performance.\nExtensive experiments on multiple public benchmarks show that our method\nachieves results on par with or superior to state-of-the-art models across\nseveral evaluation metrics. These results underscore the effectiveness of our\nmodel in enabling efficient and scalable temporal set prediction."}
{"id": "2504.16946", "pdf": "https://arxiv.org/pdf/2504.16946", "abs": "https://arxiv.org/abs/2504.16946", "authors": ["Xiaotong Ye", "Nicolas Bougie", "Toshihiko Yamasaki", "Narimasa Watanabe"], "title": "MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "Generative agents offer promising capabilities for simulating realistic urban\nbehaviors. However, existing methods oversimplify transportation choices in\nmodern cities, and require prohibitive computational resources for large-scale\npopulation simulation. To address these limitations, we first present a virtual\ncity that features multiple functional buildings and transportation modes.\nThen, we conduct extensive surveys to model behavioral choices and mobility\npreferences among population groups. Building on these insights, we introduce a\nsimulation framework that captures the complexity of urban mobility while\nremaining scalable, enabling the simulation of over 4,000 agents. To assess the\nrealism of the generated behaviors, we perform a series of micro and\nmacro-level analyses. Beyond mere performance comparison, we explore insightful\nexperiments, such as predicting crowd density from movement patterns and\nidentifying trends in vehicle preferences across agent demographics."}
{"id": "2504.17353", "pdf": "https://arxiv.org/pdf/2504.17353", "abs": "https://arxiv.org/abs/2504.17353", "authors": ["Chengguang Gan", "Sunbowen Lee", "Zhixi Cai", "Yanbin Wei", "Lei Zheng", "Yunhao Liang", "Shiwen Ni", "Tatsunori Mori"], "title": "M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction", "categories": ["cs.CL", "cs.CV", "cs.MM"], "comment": null, "summary": "Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection\nof information extraction and model interpretability. MRE aims to leverage the\nmutual understanding between tasks of different granularities, enhancing the\nperformance of both coarse-grained and fine-grained tasks through joint\nmodeling. While MRE has been explored and validated in the textual domain, its\napplicability to visual and multimodal domains remains unexplored. In this\nwork, we extend MRE to the multimodal information extraction domain for the\nfirst time. Specifically, we introduce a new task: Multimodal Mutual\nReinforcement Effect (M-MRE), and construct a corresponding dataset to support\nthis task. To address the challenges posed by M-MRE, we further propose a\nPrompt Format Adapter (PFA) that is fully compatible with various Large\nVision-Language Models (LVLMs). Experimental results demonstrate that MRE can\nalso be observed in the M-MRE task, a multimodal text-image understanding\nscenario. This provides strong evidence that MRE facilitates mutual gains\nacross three interrelated tasks, confirming its generalizability beyond the\ntextual domain."}
{"id": "2504.17160", "pdf": "https://arxiv.org/pdf/2504.17160", "abs": "https://arxiv.org/abs/2504.17160", "authors": ["Alberto FernÃ¡ndez-HernÃ¡ndez", "Jose I. Mestre", "Manuel F. Dolz", "Jose Duato", "Enrique S. Quintana-OrtÃ­"], "title": "OUI Need to Talk About Weight Decay: A New Perspective on Overfitting Detection", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "10 pages, 3 figures", "summary": "We introduce the Overfitting-Underfitting Indicator (OUI), a novel tool for\nmonitoring the training dynamics of Deep Neural Networks (DNNs) and identifying\noptimal regularization hyperparameters. Specifically, we validate that OUI can\neffectively guide the selection of the Weight Decay (WD) hyperparameter by\nindicating whether a model is overfitting or underfitting during training\nwithout requiring validation data. Through experiments on DenseNet-BC-100 with\nCIFAR- 100, EfficientNet-B0 with TinyImageNet and ResNet-34 with ImageNet-1K,\nwe show that maintaining OUI within a prescribed interval correlates strongly\nwith improved generalization and validation scores. Notably, OUI converges\nsignificantly faster than traditional metrics such as loss or accuracy,\nenabling practitioners to identify optimal WD (hyperparameter) values within\nthe early stages of training. By leveraging OUI as a reliable indicator, we can\ndetermine early in training whether the chosen WD value leads the model to\nunderfit the training data, overfit, or strike a well-balanced trade-off that\nmaximizes validation scores. This enables more precise WD tuning for optimal\nperformance on the tested datasets and DNNs. All code for reproducing these\nexperiments is available at https://github.com/AlbertoFdezHdez/OUI."}
{"id": "2504.16947", "pdf": "https://arxiv.org/pdf/2504.16947", "abs": "https://arxiv.org/abs/2504.16947", "authors": ["Dachun Sun", "You Lyu", "Jinning Li", "Yizhuo Chen", "Tianshi Wang", "Tomoyoshi Kimura", "Tarek Abdelzaher"], "title": "SCRAG: Social Computing-Based Retrieval Augmented Generation for Community Response Forecasting in Social Media Environments", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "This paper introduces SCRAG, a prediction framework inspired by social\ncomputing, designed to forecast community responses to real or hypothetical\nsocial media posts. SCRAG can be used by public relations specialists (e.g., to\ncraft messaging in ways that avoid unintended misinterpretations) or public\nfigures and influencers (e.g., to anticipate social responses), among other\napplications related to public sentiment prediction, crisis management, and\nsocial what-if analysis. While large language models (LLMs) have achieved\nremarkable success in generating coherent and contextually rich text, their\nreliance on static training data and susceptibility to hallucinations limit\ntheir effectiveness at response forecasting in dynamic social media\nenvironments. SCRAG overcomes these challenges by integrating LLMs with a\nRetrieval-Augmented Generation (RAG) technique rooted in social computing.\nSpecifically, our framework retrieves (i) historical responses from the target\ncommunity to capture their ideological, semantic, and emotional makeup, and\n(ii) external knowledge from sources such as news articles to inject\ntime-sensitive context. This information is then jointly used to forecast the\nresponses of the target community to new posts or narratives. Extensive\nexperiments across six scenarios on the X platform (formerly Twitter), tested\nwith various embedding models and LLMs, demonstrate over 10% improvements on\naverage in key evaluation metrics. A concrete example further shows its\neffectiveness in capturing diverse ideologies and nuances. Our work provides a\nsocial computing tool for applications where accurate and concrete insights\ninto community responses are crucial."}
{"id": "2504.17360", "pdf": "https://arxiv.org/pdf/2504.17360", "abs": "https://arxiv.org/abs/2504.17360", "authors": ["Jose G. Moreno", "Jesus Lovon", "M'Rick Robin-Charlet", "Christine Damase-Michel", "Lynda Tamine"], "title": "PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning of Large Language Models (LLMs) has become the default practice\nfor improving model performance on a given task. However, performance\nimprovement comes at the cost of training on vast amounts of annotated data\nwhich could be sensitive leading to significant data privacy concerns. In\nparticular, the healthcare domain is one of the most sensitive domains exposed\nto data privacy issues. In this paper, we present PatientDx, a framework of\nmodel merging that allows the design of effective LLMs for health-predictive\ntasks without requiring fine-tuning nor adaptation on patient data. Our\nproposal is based on recently proposed techniques known as merging of LLMs and\naims to optimize a building block merging strategy. PatientDx uses a pivotal\nmodel adapted to numerical reasoning and tunes hyperparameters on examples\nbased on a performance metric but without training of the LLM on these data.\nExperiments using the mortality tasks of the MIMIC-IV dataset show improvements\nup to 7% in terms of AUROC when compared to initial models. Additionally, we\nconfirm that when compared to fine-tuned models, our proposal is less prone to\ndata leak problems without hurting performance. Finally, we qualitatively show\nthe capabilities of our proposal through a case study. Our best model is\npublicly available at https://huggingface.co/ Jgmorenof/mistral\\_merged\\_0\\_4."}
{"id": "2504.17196", "pdf": "https://arxiv.org/pdf/2504.17196", "abs": "https://arxiv.org/abs/2504.17196", "authors": ["Jiawen Hou", "Hao Wu"], "title": "A Double-Norm Aggregated Tensor Latent Factorization Model for Temporal-Aware Traffic Speed Imputation", "categories": ["cs.LG"], "comment": "11pages,3figures", "summary": "In intelligent transportation systems (ITS), traffic management departments\nrely on sensors, cameras, and GPS devices to collect real-time traffic data.\nTraffic speed data is often incomplete due to sensor failures, data\ntransmission delays, or occlusions, resulting in missing speed data in certain\nroad segments. Currently, tensor decomposition based methods are extensively\nutilized, they mostly rely on the $L_2$-norm to construct their learning\nobjectives, which leads to reduced robustness in the algorithms. To address\nthis, we propose Temporal-Aware Traffic Speed Imputation (TATSI), which\ncombines the $L_2$-norm and smooth $L_1$ (${SL}_1$)-norm in its loss function,\nthereby achieving both high accuracy and robust performance in imputing missing\ntime-varying traffic speed data. TATSI adopts a single latent factor-dependent,\nnonnegative, and multiplicative update (SLF-NMU) approach, which serves as an\nefficient solver for performing nonnegative latent factor analysis (LFA) on a\ntensor. Empirical studies on three real-world time-varying traffic speed\ndatasets demonstrate that, compared with state-of-the-art traffic speed\npredictors, TATSI more precisely captures temporal patterns, thereby yielding\nthe most accurate imputations for missing traffic speed data."}
{"id": "2504.16948", "pdf": "https://arxiv.org/pdf/2504.16948", "abs": "https://arxiv.org/abs/2504.16948", "authors": ["Zhen Tan", "Huan Liu"], "title": "Intrinsic Barriers to Explaining Deep Foundation Models", "categories": ["cs.CY", "cs.AI", "cs.ET"], "comment": null, "summary": "Deep Foundation Models (DFMs) offer unprecedented capabilities but their\nincreasing complexity presents profound challenges to understanding their\ninternal workings-a critical need for ensuring trust, safety, and\naccountability. As we grapple with explaining these systems, a fundamental\nquestion emerges: Are the difficulties we face merely temporary hurdles,\nawaiting more sophisticated analytical techniques, or do they stem from\n\\emph{intrinsic barriers} deeply rooted in the nature of these large-scale\nmodels themselves? This paper delves into this critical question by examining\nthe fundamental characteristics of DFMs and scrutinizing the limitations\nencountered by current explainability methods when confronted with this\ninherent challenge. We probe the feasibility of achieving satisfactory\nexplanations and consider the implications for how we must approach the\nverification and governance of these powerful technologies."}
{"id": "2504.17366", "pdf": "https://arxiv.org/pdf/2504.17366", "abs": "https://arxiv.org/abs/2504.17366", "authors": ["Yongxuan Wu", "Runyu Chen", "Peiyu Liu", "Hongjin Qian"], "title": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench."}
{"id": "2504.17210", "pdf": "https://arxiv.org/pdf/2504.17210", "abs": "https://arxiv.org/abs/2504.17210", "authors": ["Junfei Wang", "Darshana Upadhyay", "Marzia Zaman", "Pirathayini Srikantha"], "title": "Synthetic Power Flow Data Generation Using Physics-Informed Denoising Diffusion Probabilistic Models", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to IEEE SmartGridComm Conference 2025", "summary": "Many data-driven modules in smart grid rely on access to high-quality power\nflow data; however, real-world data are often limited due to privacy and\noperational constraints. This paper presents a physics-informed generative\nframework based on Denoising Diffusion Probabilistic Models (DDPMs) for\nsynthesizing feasible power flow data. By incorporating auxiliary training and\nphysics-informed loss functions, the proposed method ensures that the generated\ndata exhibit both statistical fidelity and adherence to power system\nfeasibility. We evaluate the approach on the IEEE 14-bus and 30-bus benchmark\nsystems, demonstrating its ability to capture key distributional properties and\ngeneralize to out-of-distribution scenarios. Comparative results show that the\nproposed model outperforms three baseline models in terms of feasibility,\ndiversity, and accuracy of statistical features. This work highlights the\npotential of integrating generative modelling into data-driven power system\napplications."}
{"id": "2504.16968", "pdf": "https://arxiv.org/pdf/2504.16968", "abs": "https://arxiv.org/abs/2504.16968", "authors": ["Jun Wu", "Jiangtao Wen", "Yuxing Han"], "title": "Backslash: Rate Constrained Optimized Training of Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rapid advancement of large-language models (LLMs) has driven extensive\nresearch into parameter compression after training has been completed, yet\ncompression during the training phase remains largely unexplored. In this work,\nwe introduce Rate-Constrained Training (Backslash), a novel training-time\ncompression approach based on rate-distortion optimization (RDO). Backslash\nenables a flexible trade-off between model accuracy and complexity,\nsignificantly reducing parameter redundancy while preserving performance.\nExperiments in various architectures and tasks demonstrate that Backslash can\nreduce memory usage by 60\\% - 90\\% without accuracy loss and provides\nsignificant compression gain compared to compression after training. Moreover,\nBackslash proves to be highly versatile: it enhances generalization with small\nLagrange multipliers, improves model robustness to pruning (maintaining\naccuracy even at 80\\% pruning rates), and enables network simplification for\naccelerated inference on edge devices."}
{"id": "2504.17390", "pdf": "https://arxiv.org/pdf/2504.17390", "abs": "https://arxiv.org/abs/2504.17390", "authors": ["Jihyun Lee", "Yejin Jeon", "Seungyeon Seo", "Gary Geunbae Lee"], "title": "PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona", "categories": ["cs.CL"], "comment": "Accepted in NAACL 2025 main", "summary": "Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests\nthrough natural language interactions, yet existing systems often produce\ngeneric, monotonic responses that lack individuality and fail to adapt to\nusers' personal attributes. To address this, we introduce PicPersona-TOD, a\nnovel dataset that incorporates user images as part of the persona, enabling\npersonalized responses tailored to user-specific factors such as age or\nemotional context. This is facilitated by first impressions, dialogue\npolicy-guided prompting, and the use of external knowledge to reduce\nhallucinations. Human evaluations confirm that our dataset enhances user\nexperience, with personalized responses contributing to a more engaging\ninteraction. Additionally, we introduce a new NLG model, Pictor, which not only\npersonalizes responses, but also demonstrates robust performance across unseen\ndomains https://github.com/JihyunLee1/PicPersona."}
{"id": "2504.17219", "pdf": "https://arxiv.org/pdf/2504.17219", "abs": "https://arxiv.org/abs/2504.17219", "authors": ["Hyomin Lee", "Minseon Kim", "Sangwon Jang", "Jongheon Jeong", "Sung Ju Hwang"], "title": "Enhancing Variational Autoencoders with Smooth Robust Latent Encoding", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Under review", "summary": "Variational Autoencoders (VAEs) have played a key role in scaling up\ndiffusion-based generative models, as in Stable Diffusion, yet questions\nregarding their robustness remain largely underexplored. Although adversarial\ntraining has been an established technique for enhancing robustness in\npredictive models, it has been overlooked for generative models due to concerns\nabout potential fidelity degradation by the nature of trade-offs between\nperformance and robustness. In this work, we challenge this presumption,\nintroducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training\nframework that boosts both generation quality and robustness. In contrast to\nconventional adversarial training, which focuses on robustness only, our\napproach smooths the latent space via adversarial perturbations, promoting more\ngeneralizable representations while regularizing with originality\nrepresentation to sustain original fidelity. Applied as a post-training step on\npre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal\ncomputational overhead. Experiments show that SRL-VAE improves both generation\nquality, in image reconstruction and text-guided image editing, and robustness,\nagainst Nightshade attacks and image editing attacks. These results establish a\nnew paradigm, showing that adversarial training, once thought to be detrimental\nto generative models, can instead enhance both fidelity and robustness."}
{"id": "2504.16972", "pdf": "https://arxiv.org/pdf/2504.16972", "abs": "https://arxiv.org/abs/2504.16972", "authors": ["Hossein Ahmadi", "Sajjad Emdadi Mahdimahalleh", "Arman Farahat", "Banafsheh Saffari"], "title": "Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "comment": null, "summary": "The rapid growth of unlabeled time-series data in domains such as wireless\ncommunications, radar, biomedical engineering, and the Internet of Things (IoT)\nhas driven advancements in unsupervised learning. This review synthesizes\nrecent progress in applying autoencoders and vision transformers for\nunsupervised signal analysis, focusing on their architectures, applications,\nand emerging trends. We explore how these models enable feature extraction,\nanomaly detection, and classification across diverse signal types, including\nelectrocardiograms, radar waveforms, and IoT sensor data. The review highlights\nthe strengths of hybrid architectures and self-supervised learning, while\nidentifying challenges in interpretability, scalability, and domain\ngeneralization. By bridging methodological innovations and practical\napplications, this work offers a roadmap for developing robust, adaptive models\nfor signal intelligence."}
{"id": "2504.17445", "pdf": "https://arxiv.org/pdf/2504.17445", "abs": "https://arxiv.org/abs/2504.17445", "authors": ["Anna Lieb", "Maneesh Arora", "Eni Mustafaraj"], "title": "Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation", "categories": ["cs.CL"], "comment": "Presented at IC2S2 2024 in Philadelphia, USA", "summary": "Unsupervised machine learning techniques, such as topic modeling and\nclustering, are often used to identify latent patterns in unstructured text\ndata in fields such as political science and sociology. These methods overcome\ncommon concerns about reproducibility and costliness involved in the\nlabor-intensive process of human qualitative analysis. However, two major\nlimitations of topic models are their interpretability and their practicality\nfor answering targeted, domain-specific social science research questions. In\nthis work, we investigate opportunities for using LLM-generated text\naugmentation to improve the usefulness of topic modeling output. We use a\npolitical science case study to evaluate our results in a domain-specific\napplication, and find that topic modeling using GPT-4 augmentations creates\nhighly interpretable categories that can be used to investigate domain-specific\nresearch questions with minimal human guidance."}
{"id": "2504.17232", "pdf": "https://arxiv.org/pdf/2504.17232", "abs": "https://arxiv.org/abs/2504.17232", "authors": ["Nivedita M", "Yasmeen Shajitha S"], "title": "Multi-Modal Traffic Analysis: Integrating Time-Series Forecasting, Accident Prediction, and Image Classification", "categories": ["cs.LG"], "comment": "5 pages,10 figures", "summary": "This study proposes an integrated machine learning framework for advanced\ntraffic analysis, combining time-series forecasting, classification, and\ncomputer vision techniques. The system utilizes an ARIMA(2,0,1) model for\ntraffic prediction (MAE: 2.1), an XGBoost classifier for accident severity\nclassification (100% accuracy on balanced data), and a Convolutional Neural\nNetwork (CNN) for traffic image classification (92% accuracy). Tested on\ndiverse datasets, the framework outperforms baseline models and identifies key\nfactors influencing accident severity, including weather and road\ninfrastructure. Its modular design supports deployment in smart city systems\nfor real-time monitoring, accident prevention, and resource optimization,\ncontributing to the evolution of intelligent transportation systems."}
{"id": "2504.16977", "pdf": "https://arxiv.org/pdf/2504.16977", "abs": "https://arxiv.org/abs/2504.16977", "authors": ["Priyaranjan Pattnayak", "Hitesh Laxmichand Patel", "Amit Agarwal"], "title": "Tokenization Matters: Improving Zero-Shot NER for Indic Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization is a critical component of Natural Language Processing (NLP),\nespecially for low resource languages, where subword segmentation influences\nvocabulary structure and downstream task accuracy. Although Byte Pair Encoding\n(BPE) is a standard tokenization method in multilingual language models, its\nsuitability for Named Entity Recognition (NER) in low resource Indic languages\nremains underexplored due to its limitations in handling morphological\ncomplexity. In this work, we systematically compare BPE, SentencePiece, and\nCharacter Level tokenization strategies using IndicBERT for NER tasks in low\nresource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as\nextremely low resource Indic languages like Santali, Manipuri, and Sindhi. We\nassess both intrinsic linguistic properties tokenization efficiency, out of\nvocabulary (OOV) rates, and morphological preservation as well as extrinsic\ndownstream performance, including fine tuning and zero shot cross lingual\ntransfer.\n  Our experiments show that SentencePiece is a consistently better performing\napproach than BPE for NER in low resource Indic Languages, particularly in zero\nshot cross lingual settings, as it better preserves entity consistency. While\nBPE provides the most compact tokenization form, it is not capable of\ngeneralization because it misclassifies or even fails to recognize entity\nlabels when tested on unseen languages. In contrast, SentencePiece constitutes\na better linguistic structural preservation model, benefiting extremely low\nresource and morphologically rich Indic languages, such as Santali and\nManipuri, for superior entity recognition, as well as high generalization\nacross scripts, such as Sindhi, written in Arabic. The results point to\nSentencePiece as the more effective tokenization strategy for NER within\nmultilingual and low resource Indic NLP applications."}
{"id": "2504.17480", "pdf": "https://arxiv.org/pdf/2504.17480", "abs": "https://arxiv.org/abs/2504.17480", "authors": ["Xin Yi", "Shunfan Zhengc", "Linlin Wanga", "Xiaoling Wang", "Liang He"], "title": "Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable."}
{"id": "2504.17243", "pdf": "https://arxiv.org/pdf/2504.17243", "abs": "https://arxiv.org/abs/2504.17243", "authors": ["Xinyu Zhou", "Simin Fan", "Martin Jaggi", "Jie Fu"], "title": "NeuralGrok: Accelerate Grokking by Neural Gradient Transformation", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint, 16 pages", "summary": "Grokking is proposed and widely studied as an intricate phenomenon in which\ngeneralization is achieved after a long-lasting period of overfitting. In this\nwork, we propose NeuralGrok, a novel gradient-based approach that learns an\noptimal gradient transformation to accelerate the generalization of\ntransformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary\nmodule (e.g., an MLP block) in conjunction with the base model. This module\ndynamically modulates the influence of individual gradient components based on\ntheir contribution to generalization, guided by a bilevel optimization\nalgorithm. Our extensive experiments demonstrate that NeuralGrok significantly\naccelerates generalization, particularly in challenging arithmetic tasks. We\nalso show that NeuralGrok promotes a more stable training paradigm, constantly\nreducing the model's complexity, while traditional regularization methods, such\nas weight decay, can introduce substantial instability and impede\ngeneralization. We further investigate the intrinsic model complexity\nleveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that\nNeuralGrok effectively facilitates generalization by reducing the model\ncomplexity. We offer valuable insights on the grokking phenomenon of\nTransformer models, which encourages a deeper understanding of the fundamental\nprinciples governing generalization ability."}
{"id": "2504.16979", "pdf": "https://arxiv.org/pdf/2504.16979", "abs": "https://arxiv.org/abs/2504.16979", "authors": ["Masoud Tafavvoghi", "Lars Ailo Bongo", "AndrÃ© Berli Delgado", "Nikita Shvetsov", "Anders Sildnes", "Line Moi", "Lill-Tove Rasmussen Busund", "Kajsa MÃ¸llersen"], "title": "Automating tumor-infiltrating lymphocyte assessment in breast cancer histopathology images using QuPath: a transparent and accessible machine learning pipeline", "categories": ["q-bio.QM", "cs.AI", "cs.CV"], "comment": "16 Pages, 9 Figures, 3 tables", "summary": "In this study, we built an end-to-end tumor-infiltrating lymphocytes (TILs)\nassessment pipeline within QuPath, demonstrating the potential of easily\naccessible tools to perform complex tasks in a fully automatic fashion. First,\nwe trained a pixel classifier to segment tumor, tumor-associated stroma, and\nother tissue compartments in breast cancer H&E-stained whole-slide images (WSI)\nto isolate tumor-associated stroma for subsequent analysis. Next, we applied a\npre-trained StarDist deep learning model in QuPath for cell detection and used\nthe extracted cell features to train a binary classifier distinguishing TILs\nfrom other cells. To evaluate our TILs assessment pipeline, we calculated the\nTIL density in each WSI and categorized them as low, medium, or high TIL\nlevels. Our pipeline was evaluated against pathologist-assigned TIL scores,\nachieving a Cohen's kappa of 0.71 on the external test set, corroborating\nprevious research findings. These results confirm that existing software can\noffer a practical solution for the assessment of TILs in H&E-stained WSIs of\nbreast cancer."}
{"id": "2504.17550", "pdf": "https://arxiv.org/pdf/2504.17550", "abs": "https://arxiv.org/abs/2504.17550", "authors": ["Yejin Bang", "Ziwei Ji", "Alan Schelten", "Anthony Hartshorn", "Tara Fowler", "Cheng Zhang", "Nicola Cancedda", "Pascale Fung"], "title": "HalluLens: LLM Hallucination Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": "42 pages", "summary": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations."}
{"id": "2504.17247", "pdf": "https://arxiv.org/pdf/2504.17247", "abs": "https://arxiv.org/abs/2504.17247", "authors": ["Diogo Soares", "Leon Hetzel", "Paulina Szymczak", "Fabian Theis", "Stephan GÃ¼nnemann", "Ewa Szczurek"], "title": "Targeted AMP generation through controlled diffusion with efficient embeddings", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": null, "summary": "Deep learning-based antimicrobial peptide (AMP) discovery faces critical\nchallenges such as low experimental hit rates as well as the need for nuanced\ncontrollability and efficient modeling of peptide properties. To address these\nchallenges, we introduce OmegAMP, a framework that leverages a diffusion-based\ngenerative model with efficient low-dimensional embeddings, precise\ncontrollability mechanisms, and novel classifiers with drastically reduced\nfalse positive rates for candidate filtering. OmegAMP enables the targeted\ngeneration of AMPs with specific physicochemical properties, activity profiles,\nand species-specific effectiveness. Moreover, it maximizes sample diversity\nwhile ensuring faithfulness to the underlying data distribution during\ngeneration. We demonstrate that OmegAMP achieves state-of-the-art performance\nacross all stages of the AMP discovery pipeline, significantly advancing the\npotential of computational frameworks in combating antimicrobial resistance."}
{"id": "2504.17004", "pdf": "https://arxiv.org/pdf/2504.17004", "abs": "https://arxiv.org/abs/2504.17004", "authors": ["Amin Karbasi", "Omar Montasser", "John Sous", "Grigoris Velegkas"], "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment."}
{"id": "2504.17562", "pdf": "https://arxiv.org/pdf/2504.17562", "abs": "https://arxiv.org/abs/2504.17562", "authors": ["Rei Higuchi", "Ryotaro Kawata", "Naoki Nishikawa", "Kazusato Oko", "Shoichiro Yamaguchi", "Sosuke Kobayashi", "Seiya Tokui", "Kohei Hayashi", "Daisuke Okanohara", "Taiji Suzuki"], "title": "When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The ability to acquire latent semantics is one of the key properties that\ndetermines the performance of language models. One convenient approach to\ninvoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at\nthe beginning of texts in the pre-training data, making it easier for the model\nto access latent semantics before observing the entire text. Previous studies\nhave reported that this technique actually improves the performance of trained\nmodels in downstream tasks; however, this improvement has been observed only in\nspecific downstream tasks, without consistent enhancement in average next-token\nprediction loss. To understand this phenomenon, we closely investigate how\nprepending metadata during pre-training affects model performance by examining\nits behavior using artificial data. Interestingly, we found that this approach\nproduces both positive and negative effects on the downstream tasks. We\ndemonstrate that the effectiveness of the approach depends on whether latent\nsemantics can be inferred from the downstream task's prompt. Specifically,\nthrough investigations using data generated by probabilistic context-free\ngrammars, we show that training with metadata helps improve model's performance\nwhen the given context is long enough to infer the latent semantics. In\ncontrast, the technique negatively impacts performance when the context lacks\nthe necessary information to make an accurate posterior inference."}
{"id": "2504.17258", "pdf": "https://arxiv.org/pdf/2504.17258", "abs": "https://arxiv.org/abs/2504.17258", "authors": ["Md Ashiqur Rahman", "Raymond A. Yeh"], "title": "Group Downsampling with Equivariant Anti-aliasing", "categories": ["cs.LG", "cs.CV", "math.GR"], "comment": null, "summary": "Downsampling layers are crucial building blocks in CNN architectures, which\nhelp to increase the receptive field for learning high-level features and\nreduce the amount of memory/computation in the model. In this work, we study\nthe generalization of the uniform downsampling layer for group equivariant\narchitectures, e.g., G-CNNs. That is, we aim to downsample signals (feature\nmaps) on general finite groups with anti-aliasing. This involves the following:\n(a) Given a finite group and a downsampling rate, we present an algorithm to\nform a suitable choice of subgroup. (b) Given a group and a subgroup, we study\nthe notion of bandlimited-ness and propose how to perform anti-aliasing.\nNotably, our method generalizes the notion of downsampling based on classical\nsampling theory. When the signal is on a cyclic group, i.e., periodic, our\nmethod recovers the standard downsampling of an ideal low-pass filter followed\nby a subsampling operation. Finally, we conducted experiments on image\nclassification tasks demonstrating that the proposed downsampling operation\nimproves accuracy, better preserves equivariance, and reduces model size when\nincorporated into G-equivariant networks"}
{"id": "2504.17020", "pdf": "https://arxiv.org/pdf/2504.17020", "abs": "https://arxiv.org/abs/2504.17020", "authors": ["Kasper Engelen", "Guillermo A. PÃ©rez", "Shrisha Rao"], "title": "Analyzing Value Functions of States in Parametric Markov Chains", "categories": ["cs.LO", "cs.AI"], "comment": "Published as part of the book \"Principles of Verification: Cycling\n  the Probabilistic Landscape: Essays Dedicated to Joost-Pieter Katoen on the\n  Occasion of His 60th Birthday, Part II\"", "summary": "Parametric Markov chains (pMC) are used to model probabilistic systems with\nunknown or partially known probabilities. Although (universal) pMC verification\nfor reachability properties is known to be coETR-complete, there have been\nefforts to approach it using potentially easier-to-check properties such as\nasking whether the pMC is monotonic in certain parameters. In this paper, we\nfirst reduce monotonicity to asking whether the reachability probability from a\ngiven state is never less than that of another given state. Recent results for\nthe latter property imply an efficient algorithm to collapse same-value\nequivalence classes, which in turn preserves verification results and\nmonotonicity. We implement our algorithm to collapse \"trivial\" equivalence\nclasses in the pMC and show empirical evidence for the following: First, the\ncollapse gives reductions in size for some existing benchmarks and significant\nreductions on some custom benchmarks; Second, the collapse speeds up existing\nalgorithms to check monotonicity and parameter lifting, and hence can be used\nas a fast pre-processing step in practice."}
{"id": "2504.17565", "pdf": "https://arxiv.org/pdf/2504.17565", "abs": "https://arxiv.org/abs/2504.17565", "authors": ["Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Yunjie Ji", "Han Zhao", "Xiangang Li"], "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training", "categories": ["cs.CL"], "comment": null, "summary": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M"}
{"id": "2504.17261", "pdf": "https://arxiv.org/pdf/2504.17261", "abs": "https://arxiv.org/abs/2504.17261", "authors": ["Jiaqi Chen", "Xiaoye Zhu", "Yue Wang", "Tianyang Liu", "Xinhui Chen", "Ying Chen", "Chak Tou Leong", "Yifei Ke", "Joseph Liu", "Yiwen Yuan", "Julian McAuley", "Li-jia Li"], "title": "Symbolic Representation for Any-to-Any Generative Tasks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose a symbolic generative task description language and a\ncorresponding inference engine capable of representing arbitrary multimodal\ntasks as structured symbolic flows. Unlike conventional generative models that\nrely on large-scale training and implicit neural representations to learn\ncross-modal mappings, often at high computational cost and with limited\nflexibility, our framework introduces an explicit symbolic representation\ncomprising three core primitives: functions, parameters, and topological logic.\nLeveraging a pre-trained language model, our inference engine maps natural\nlanguage instructions directly to symbolic workflows in a training-free manner.\nOur framework successfully performs over 12 diverse multimodal generative\ntasks, demonstrating strong performance and flexibility without the need for\ntask-specific tuning. Experiments show that our method not only matches or\noutperforms existing state-of-the-art unified models in content quality, but\nalso offers greater efficiency, editability, and interruptibility. We believe\nthat symbolic task representations provide a cost-effective and extensible\nfoundation for advancing the capabilities of generative AI."}
{"id": "2504.17023", "pdf": "https://arxiv.org/pdf/2504.17023", "abs": "https://arxiv.org/abs/2504.17023", "authors": ["Felix Kares", "Timo Speith", "Hanwei Zhang", "Markus Langer"], "title": "What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency Maps in Explainable AI (XAI)", "categories": ["cs.HC", "cs.AI"], "comment": "27 pages, 7 figures, 4 tables", "summary": "Saliency maps are a popular approach for explaining classifications of\n(convolutional) neural networks. However, it remains an open question as to how\nbest to evaluate salience maps, with three families of evaluation methods\ncommonly being used: subjective user measures, objective user measures, and\nmathematical metrics. We examine three of the most popular saliency map\napproaches (viz., LIME, Grad-CAM, and Guided Backpropagation) in a between\nsubject study (N=166) across these families of evaluation methods. We test 1)\nfor subjective measures, if the maps differ with respect to user trust and\nsatisfaction; 2) for objective measures, if the maps increase users' abilities\nand thus understanding of a model; 3) for mathematical metrics, which map\nachieves the best ratings across metrics; and 4) whether the mathematical\nmetrics can be associated with objective user measures. To our knowledge, our\nstudy is the first to compare several salience maps across all these evaluation\nmethods$-$with the finding that they do not agree in their assessment (i.e.,\nthere was no difference concerning trust and satisfaction, Grad-CAM improved\nusers' abilities best, and Guided Backpropagation had the most favorable\nmathematical metrics). Additionally, we show that some mathematical metrics\nwere associated with user understanding, although this relationship was often\ncounterintuitive. We discuss these findings in light of general debates\nconcerning the complementary use of user studies and mathematical metrics in\nthe evaluation of explainable AI (XAI) approaches."}
{"id": "2504.17574", "pdf": "https://arxiv.org/pdf/2504.17574", "abs": "https://arxiv.org/abs/2504.17574", "authors": ["Zhenkai Qin", "Guifang Yang", "Dongze Wu"], "title": "RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "As false information continues to proliferate across social media platforms,\neffective rumor detection has emerged as a pressing challenge in natural\nlanguage processing. This paper proposes RAGAT-Mind, a multi-granular modeling\napproach for Chinese rumor detection, built upon the MindSpore deep learning\nframework. The model integrates TextCNN for local semantic extraction,\nbidirectional GRU for sequential context learning, Multi-Head Self-Attention\nfor global dependency focusing, and Bidirectional Graph Convolutional Networks\n(BiGCN) for structural representation of word co-occurrence graphs. Experiments\non the Weibo1-Rumor dataset demonstrate that RAGAT-Mind achieves superior\nclassification performance, attaining 99.2% accuracy and a macro-F1 score of\n0.9919. The results validate the effectiveness of combining hierarchical\nlinguistic features with graph-based semantic structures. Furthermore, the\nmodel exhibits strong generalization and interpretability, highlighting its\npractical value for real-world rumor detection applications."}
{"id": "2504.17274", "pdf": "https://arxiv.org/pdf/2504.17274", "abs": "https://arxiv.org/abs/2504.17274", "authors": ["Siddharth Vishwanath", "Jonathan Hehir"], "title": "Signal Recovery from Random Dot-Product Graphs Under Local Differential Privacy", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH", "68P27, 62H22, 62C20, 62R07"], "comment": null, "summary": "We consider the problem of recovering latent information from graphs under\n$\\varepsilon$-edge local differential privacy where the presence of\nrelationships/edges between two users/vertices remains confidential, even from\nthe data curator. For the class of generalized random dot-product graphs, we\nshow that a standard local differential privacy mechanism induces a specific\ngeometric distortion in the latent positions. Leveraging this insight, we show\nthat consistent recovery of the latent positions is achievable by appropriately\nadjusting the statistical inference procedure for the privatized graph.\nFurthermore, we prove that our procedure is nearly minimax-optimal under local\nedge differential privacy constraints. Lastly, we show that this framework\nallows for consistent recovery of geometric and topological information\nunderlying the latent positions, as encoded in their persistence diagrams. Our\nresults extend previous work from the private community detection literature to\na substantially richer class of models and inferential tasks."}
{"id": "2504.17028", "pdf": "https://arxiv.org/pdf/2504.17028", "abs": "https://arxiv.org/abs/2504.17028", "authors": ["Iman Khadir", "Shane Stevenson", "Henry Li", "Kyle Krick", "Abram Burrows", "David Hall", "Stan Posey", "Samuel S. P. Shen"], "title": "Democracy of AI Numerical Weather Models: An Example of Global Forecasting with FourCastNetv2 Made by a University Research Lab Using GPU", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": "12 pages, 8 figures", "summary": "This paper demonstrates the feasibility of democratizing AI-driven global\nweather forecasting models among university research groups by leveraging\nGraphics Processing Units (GPUs) and freely available AI models, such as\nNVIDIA's FourCastNetv2. FourCastNetv2 is an NVIDIA's advanced neural network\nfor weather prediction and is trained on a 73-channel subset of the European\nCentre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset\nat single levels and different pressure levels. Although the training\nspecifications for FourCastNetv2 are not released to the public, the training\ndocumentation of the model's first generation, FourCastNet, is available to all\nusers. The training had 64 A100 GPUs and took 16 hours to complete. Although\nNVIDIA's models offer significant reductions in both time and cost compared to\ntraditional Numerical Weather Prediction (NWP), reproducing published\nforecasting results presents ongoing challenges for resource-constrained\nuniversity research groups with limited GPU availability. We demonstrate both\n(i) leveraging FourCastNetv2 to create predictions through the designated\napplication programming interface (API) and (ii) utilizing NVIDIA hardware to\ntrain the original FourCastNet model. Further, this paper demonstrates the\ncapabilities and limitations of NVIDIA A100's for resource-limited research\ngroups in universities. We also explore data management, training efficiency,\nand model validation, highlighting the advantages and challenges of using\nlimited high-performance computing resources. Consequently, this paper and its\ncorresponding GitHub materials may serve as an initial guide for other\nuniversity research groups and courses related to machine learning, climate\nscience, and data science to develop research and education programs on AI\nweather forecasting, and hence help democratize the AI NWP in the digital\neconomy."}
{"id": "2504.17653", "pdf": "https://arxiv.org/pdf/2504.17653", "abs": "https://arxiv.org/abs/2504.17653", "authors": ["Samaneh Hosseini Moghaddam", "Kelly Lyons", "Cheryl Regehr", "Vivek Goel", "Kaitlyn Regehr"], "title": "Towards a comprehensive taxonomy of online abusive language informed by machine leaning", "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of abusive language in online communications has posed\nsignificant risks to the health and wellbeing of individuals and communities.\nThe growing concern regarding online abuse and its consequences necessitates\nmethods for identifying and mitigating harmful content and facilitating\ncontinuous monitoring, moderation, and early intervention. This paper presents\na taxonomy for distinguishing key characteristics of abusive language within\nonline text. Our approach uses a systematic method for taxonomy development,\nintegrating classification systems of 18 existing multi-label datasets to\ncapture key characteristics relevant to online abusive language classification.\nThe resulting taxonomy is hierarchical and faceted, comprising 5 categories and\n17 dimensions. It classifies various facets of online abuse, including context,\ntarget, intensity, directness, and theme of abuse. This shared understanding\ncan lead to more cohesive efforts, facilitate knowledge exchange, and\naccelerate progress in the field of online abuse detection and mitigation among\nresearchers, policy makers, online platform owners, and other stakeholders."}
{"id": "2504.17276", "pdf": "https://arxiv.org/pdf/2504.17276", "abs": "https://arxiv.org/abs/2504.17276", "authors": ["Ke-Jia Chen", "Wenhui Mu", "Zheng Liu"], "title": "HeRB: Heterophily-Resolved Structure Balancer for Graph Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Recent research has witnessed the remarkable progress of Graph Neural\nNetworks (GNNs) in the realm of graph data representation. However, GNNs still\nencounter the challenge of structural imbalance. Prior solutions to this\nproblem did not take graph heterophily into account, namely that connected\nnodes process distinct labels or features, thus resulting in a deficiency in\neffectiveness. Upon verifying the impact of heterophily on solving the\nstructural imbalance problem, we propose to rectify the heterophily first and\nthen transfer homophilic knowledge. To the end, we devise a method named HeRB\n(Heterophily-Resolved Structure Balancer) for GNNs. HeRB consists of two\ninnovative components: 1) A heterophily-lessening augmentation module which\nserves to reduce inter-class edges and increase intra-class edges; 2) A\nhomophilic knowledge transfer mechanism to convey homophilic information from\nhead nodes to tail nodes. Experimental results demonstrate that HeRB achieves\nsuperior performance on two homophilic and six heterophilic benchmark datasets,\nand the ablation studies further validate the efficacy of two proposed\ncomponents."}
{"id": "2504.17029", "pdf": "https://arxiv.org/pdf/2504.17029", "abs": "https://arxiv.org/abs/2504.17029", "authors": ["Jeffrey Smith", "Taisei Fujii", "Jesse Craney", "Charles Gretton"], "title": "Fried Parameter Estimation from Single Wavefront Sensor Image with Artificial Neural Networks", "categories": ["astro-ph.IM", "cs.AI"], "comment": null, "summary": "Atmospheric turbulence degrades the quality of astronomical observations in\nground-based telescopes, leading to distorted and blurry images. Adaptive\nOptics (AO) systems are designed to counteract these effects, using atmospheric\nmeasurements captured by a wavefront sensor to make real-time corrections to\nthe incoming wavefront. The Fried parameter, r0, characterises the strength of\natmospheric turbulence and is an essential control parameter for optimising the\nperformance of AO systems and more recently sky profiling for Free Space\nOptical (FSO) communication channels. In this paper, we develop a novel\ndata-driven approach, adapting machine learning methods from computer vision\nfor Fried parameter estimation from a single Shack-Hartmann or pyramid\nwavefront sensor image. Using these data-driven methods, we present a detailed\nsimulation-based evaluation of our approach using the open-source COMPASS AO\nsimulation tool to evaluate both the Shack-Hartmann and pyramid wavefront\nsensors. Our evaluation is over a range of guide star magnitudes, and realistic\nnoise, atmospheric and instrument conditions. Remarkably, we are able to\ndevelop a single network-based estimator that is accurate in both open and\nclosed-loop AO configurations. Our method accurately estimates the Fried\nparameter from a single WFS image directly from AO telemetry to a few\nmillimetres. Our approach is suitable for real time control, exhibiting 0.83ms\nr0 inference times on retail NVIDIA RTX 3090 GPU hardware, and thereby\ndemonstrating a compelling economic solution for use in real-time instrument\ncontrol."}
{"id": "2504.17665", "pdf": "https://arxiv.org/pdf/2504.17665", "abs": "https://arxiv.org/abs/2504.17665", "authors": ["Zena Al-Khalili", "Nick Howell", "Dietrich Klakow"], "title": "Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics", "categories": ["cs.CL"], "comment": null, "summary": "Assisting LLMs with code generation improved their performance on\nmathematical reasoning tasks. However, the evaluation of code-assisted LLMs is\ngenerally restricted to execution correctness, lacking a rigorous evaluation of\ntheir generated programs. In this work, we bridge this gap by conducting an\nin-depth analysis of code-assisted LLMs' generated programs in response to math\nreasoning tasks. Our evaluation focuses on the extent to which LLMs ground\ntheir programs to math rules, and how that affects their end performance. For\nthis purpose, we assess the generations of five different LLMs, on two\ndifferent math datasets, both manually and automatically. Our results reveal\nthat the distribution of grounding depends on LLMs' capabilities and the\ndifficulty of math problems. Furthermore, mathematical grounding is more\neffective for closed-source models, while open-source models fail to employ\nmath rules in their solutions correctly. On MATH500, the percentage of grounded\nprograms decreased to half, while the ungrounded generations doubled in\ncomparison to ASDiv grade-school problems. Our work highlights the need for\nin-depth evaluation beyond execution accuracy metrics, toward a better\nunderstanding of code-assisted LLMs' capabilities and limits in the math\ndomain."}
{"id": "2504.17277", "pdf": "https://arxiv.org/pdf/2504.17277", "abs": "https://arxiv.org/abs/2504.17277", "authors": ["Zongliang Ji", "Andre Carlos Kajdacsy-Balla Amaral", "Anna Goldenberg", "Rahul G. Krishnan"], "title": "ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to the Conference on Health, Inference, and Learning (CHIL)\n  2025", "summary": "Ordering a minimal subset of lab tests for patients in the intensive care\nunit (ICU) can be challenging. Care teams must balance between ensuring the\navailability of the right information and reducing the clinical burden and\ncosts associated with each lab test order. Most in-patient settings experience\nfrequent over-ordering of lab tests, but are now aiming to reduce this burden\non both hospital resources and the environment. This paper develops a novel\nmethod that combines off-policy learning with privileged information to\nidentify the optimal set of ICU lab tests to order. Our approach, EXplainable\nOff-policy learning with Side Information for ICU blood Test Orders (ExOSITO)\ncreates an interpretable assistive tool for clinicians to order lab tests by\nconsidering both the observed and predicted future status of each patient. We\npose this problem as a causal bandit trained using offline data and a reward\nfunction derived from clinically-approved rules; we introduce a novel learning\nframework that integrates clinical knowledge with observational data to bridge\nthe gap between the optimal and logging policies. The learned policy function\nprovides interpretable clinical information and reduces costs without omitting\nany vital lab orders, outperforming both a physician's policy and prior\napproaches to this practical problem."}
{"id": "2504.17040", "pdf": "https://arxiv.org/pdf/2504.17040", "abs": "https://arxiv.org/abs/2504.17040", "authors": ["Zhenhailong Wang", "Senthil Purushwalkam", "Caiming Xiong", "Silvio Savarese", "Heng Ji", "Ran Xu"], "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present DyMU, an efficient, training-free framework that dynamically\nreduces the computational burden of vision-language models (VLMs) while\nmaintaining high task performance. Our approach comprises two key components.\nFirst, Dynamic Token Merging (DToMe) reduces the number of visual token\nembeddings by merging similar tokens based on image complexity, addressing the\ninherent inefficiency of fixed-length outputs in vision transformers. Second,\nVirtual Token Unmerging (VTU) simulates the expected token sequence for large\nlanguage models (LLMs) by efficiently reconstructing the attention dynamics of\na full sequence, thus preserving the downstream performance without additional\nfine-tuning. Unlike previous approaches, our method dynamically adapts token\ncompression to the content of the image and operates completely training-free,\nmaking it readily applicable to most state-of-the-art VLM architectures.\nExtensive experiments on image and video understanding tasks demonstrate that\nDyMU can reduce the average visual token count by 32%-85% while achieving\ncomparable performance to full-length models across diverse VLM architectures,\nincluding the recently popularized AnyRes-based visual encoders. Furthermore,\nthrough qualitative analyses, we demonstrate that DToMe effectively adapts\ntoken reduction based on image complexity and, unlike existing systems,\nprovides users more control over computational costs. Project page:\nhttps://mikewangwzhl.github.io/dymu/."}
{"id": "2504.17671", "pdf": "https://arxiv.org/pdf/2504.17671", "abs": "https://arxiv.org/abs/2504.17671", "authors": ["Yuanchang Ye", "Weiyan Wen"], "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making."}
{"id": "2504.17300", "pdf": "https://arxiv.org/pdf/2504.17300", "abs": "https://arxiv.org/abs/2504.17300", "authors": ["Wencong You", "Daniel Lowd"], "title": "The Ultimate Cookbook for Invisible Poison: Crafting Subtle Clean-Label Text Backdoors with Style Attributes", "categories": ["cs.LG"], "comment": "Accepted at SaTML 2025", "summary": "Backdoor attacks on text classifiers can cause them to predict a predefined\nlabel when a particular \"trigger\" is present. Prior attacks often rely on\ntriggers that are ungrammatical or otherwise unusual, leading to conspicuous\nattacks. As a result, human annotators, who play a critical role in curating\ntraining data in practice, can easily detect and filter out these unnatural\ntexts during manual inspection, reducing the risk of such attacks. We argue\nthat a key criterion for a successful attack is for text with and without\ntriggers to be indistinguishable to humans. However, prior work neither\ndirectly nor comprehensively evaluated attack subtlety and invisibility with\nhuman involvement. We bridge the gap by conducting thorough human evaluations\nto assess attack subtlety. We also propose \\emph{AttrBkd}, consisting of three\nrecipes for crafting subtle yet effective trigger attributes, such as\nextracting fine-grained attributes from existing baseline backdoor attacks. Our\nhuman evaluations find that AttrBkd with these baseline-derived attributes is\noften more effective (higher attack success rate) and more subtle (fewer\ninstances detected by humans) than the original baseline backdoor attacks,\ndemonstrating that backdoor attacks can bypass detection by being inconspicuous\nand appearing natural even upon close inspection, while still remaining\neffective. Our human annotation also provides information not captured by\nautomated metrics used in prior work, and demonstrates the misalignment of\nthese metrics with human judgment."}
{"id": "2504.17044", "pdf": "https://arxiv.org/pdf/2504.17044", "abs": "https://arxiv.org/abs/2504.17044", "authors": ["Dhari Gandhi", "Himanshu Joshi", "Lucas Hartman", "Shabnam Hassani"], "title": "Approaches to Responsible Governance of GenAI in Organizations", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid evolution of Generative AI (GenAI) has introduced unprecedented\nopportunities while presenting complex challenges around ethics,\naccountability, and societal impact. This paper draws on a literature review,\nestablished governance frameworks, and industry roundtable discussions to\nidentify core principles for integrating responsible GenAI governance into\ndiverse organizational structures. Our objective is to provide actionable\nrecommendations for a balanced, risk-based governance approach that enables\nboth innovation and oversight. Findings emphasize the need for adaptable risk\nassessment tools, continuous monitoring practices, and cross-sector\ncollaboration to establish trustworthy GenAI. These insights provide a\nstructured foundation and Responsible GenAI Guide (ResAI) for organizations to\nalign GenAI initiatives with ethical, legal, and operational best practices."}
{"id": "2504.17674", "pdf": "https://arxiv.org/pdf/2504.17674", "abs": "https://arxiv.org/abs/2504.17674", "authors": ["Jared Fernandez", "Clara Na", "Vashisth Tiwari", "Yonatan Bisk", "Sasha Luccioni", "Emma Strubell"], "title": "Energy Considerations of Large Language Model Inference and Efficiency Optimizations", "categories": ["cs.CL", "cs.LG"], "comment": "16 pages", "summary": "As large language models (LLMs) scale in size and adoption, their\ncomputational and environmental costs continue to rise. Prior benchmarking\nefforts have primarily focused on latency reduction in idealized settings,\noften overlooking the diverse real-world inference workloads that shape energy\nuse. In this work, we systematically analyze the energy implications of common\ninference efficiency optimizations across diverse Natural Language Processing\n(NLP) and generative Artificial Intelligence (AI) workloads, including\nconversational AI and code generation. We introduce a modeling approach that\napproximates real-world LLM workflows through a binning strategy for\ninput-output token distributions and batch size variations. Our empirical\nanalysis spans software frameworks, decoding strategies, GPU architectures,\nonline and offline serving settings, and model parallelism configurations. We\nshow that the effectiveness of inference optimizations is highly sensitive to\nworkload geometry, software stack, and hardware accelerators, demonstrating\nthat naive energy estimates based on FLOPs or theoretical GPU utilization\nsignificantly underestimate real-world energy consumption. Our findings reveal\nthat the proper application of relevant inference efficiency optimizations can\nreduce total energy use by up to 73% from unoptimized baselines. These insights\nprovide a foundation for sustainable LLM deployment and inform energy-efficient\ndesign strategies for future AI infrastructure."}
{"id": "2504.17305", "pdf": "https://arxiv.org/pdf/2504.17305", "abs": "https://arxiv.org/abs/2504.17305", "authors": ["Dinan Li", "Panagiotis Kakosimos", "Luca Peretti"], "title": "Machine learning-based condition monitoring of powertrains in modern electric drives", "categories": ["cs.LG"], "comment": "2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "The recent technological advances in digitalization have revolutionized the\nindustrial sector. Leveraging data analytics has now enabled the collection of\ndeep insights into the performance and, as a result, the optimization of\nassets. Industrial drives, for example, already accumulate all the necessary\ninformation to control electric machines. These signals include but are not\nlimited to currents, frequency, and temperature. Integrating machine learning\n(ML) models responsible for predicting the evolution of those directly\ncollected or implicitly derived parameters enhances the smartness of industrial\nsystems even further. In this article, data already residing in most modern\nelectric drives has been used to develop a data-driven thermal model of a power\nmodule. A test bench has been designed and used specifically for training and\nvalidating the thermal digital twin undergoing various static and dynamic\noperating profiles. Different approaches, from traditional linear models to\ndeep neural networks, have been implemented to emanate the best ML model for\nestimating the case temperature of a power module. Several evaluation metrics\nwere then used to assess the investigated methods' performance and\nimplementation in industrial embedded systems."}
{"id": "2504.17055", "pdf": "https://arxiv.org/pdf/2504.17055", "abs": "https://arxiv.org/abs/2504.17055", "authors": ["Ayushi Agrawal", "Aditya Kondai", "Kavita Vemuri"], "title": "Psychological Effect of AI driven marketing tools for beauty/facial feature enhancement", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "AI-powered facial assessment tools are reshaping how individuals evaluate\nappearance and internalize social judgments. This study examines the\npsychological impact of such tools on self-objectification, self-esteem, and\nemotional responses, with attention to gender differences. Two samples used\ndistinct versions of a facial analysis tool: one overtly critical (N=75; M=22.9\nyears), and another more neutral (N=51; M=19.9 years). Participants completed\nvalidated self-objectification and self-esteem scales and custom items\nmeasuring emotion, digital/physical appearance enhancement (DAE, PAEE), and\nperceived social emotion (PSE). Results revealed consistent links between high\nself-objectification, low self-esteem, and increased appearance enhancement\nbehaviors across both versions. Despite softer framing, the newer tool still\nevoked negative emotional responses (U=1466.5, p=0.013), indicating implicit\nfeedback may reinforce appearance-related insecurities. Gender differences\nemerged in DAE (p=0.025) and PSE (p<0.001), with females more prone to digital\nenhancement and less likely to perceive emotional impact in others. These\nfindings reveal how AI tools may unintentionally reinforce and amplify existing\nsocial biases and underscore the critical need for responsible AI design and\ndevelopment. Future research will investigate how human ideologies embedded in\nthe training data of such tools shape their evaluative outputs, and how these,\nin turn, influence user attitudes and decisions."}
{"id": "2504.17685", "pdf": "https://arxiv.org/pdf/2504.17685", "abs": "https://arxiv.org/abs/2504.17685", "authors": ["Haru-Tada Sato", "Fuka Matsuzaki", "Jun-ichiro Takahashi"], "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 2 figures", "summary": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach."}
{"id": "2504.17314", "pdf": "https://arxiv.org/pdf/2504.17314", "abs": "https://arxiv.org/abs/2504.17314", "authors": ["Miaoyun Zhao", "Qiang Zhang", "Chenrong Li"], "title": "Class-Conditional Distribution Balancing for Group Robust Classification", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Spurious correlations that lead models to correct predictions for the wrong\nreasons pose a critical challenge for robust real-world generalization.\nExisting research attributes this issue to group imbalance and addresses it by\nmaximizing group-balanced or worst-group accuracy, which heavily relies on\nexpensive bias annotations. A compromise approach involves predicting bias\ninformation using extensively pretrained foundation models, which requires\nlarge-scale data and becomes impractical for resource-limited rare domains. To\naddress these challenges, we offer a novel perspective by reframing the\nspurious correlations as imbalances or mismatches in class-conditional\ndistributions, and propose a simple yet effective robust learning method that\neliminates the need for both bias annotations and predictions. With the goal of\nreducing the mutual information between spurious factors and label information,\nour method leverages a sample reweighting strategy to achieve class-conditional\ndistribution balancing, which automatically highlights minority groups and\nclasses, effectively dismantling spurious correlations and producing a debiased\ndata distribution for classification. Extensive experiments and analysis\ndemonstrate that our approach consistently delivers state-of-the-art\nperformance, rivaling methods that rely on bias supervision."}
{"id": "2504.17058", "pdf": "https://arxiv.org/pdf/2504.17058", "abs": "https://arxiv.org/abs/2504.17058", "authors": ["Rahul Vishwakarma"], "title": "Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The generation of high-quality synthetic data presents significant challenges\nin machine learning research, particularly regarding statistical fidelity and\nuncertainty quantification. Existing generative models produce compelling\nsynthetic samples but lack rigorous statistical guarantees about their relation\nto the underlying data distribution, limiting their applicability in critical\ndomains requiring robust error bounds. We address this fundamental limitation\nby presenting a novel framework that incorporates conformal prediction\nmethodologies into Generative Adversarial Networks (GANs). By integrating\nmultiple conformal prediction paradigms including Inductive Conformal\nPrediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction,\nand Venn-Abers Predictors, we establish distribution-free uncertainty\nquantification in generated samples. This approach, termed Conformalized GAN\n(cGAN), demonstrates enhanced calibration properties while maintaining the\ngenerative power of traditional GANs, producing synthetic data with provable\nstatistical guarantees. We provide rigorous mathematical proofs establishing\nfinite-sample validity guarantees and asymptotic efficiency properties,\nenabling the reliable application of synthetic data in high-stakes domains\nincluding healthcare, finance, and autonomous systems."}
{"id": "2504.17704", "pdf": "https://arxiv.org/pdf/2504.17704", "abs": "https://arxiv.org/abs/2504.17704", "authors": ["Cheng Wang", "Yue Liu", "Baolong Li", "Duzhen Zhang", "Zhongzhi Li", "Junfeng Fang"], "title": "Safety in Large Reasoning Models: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks\nlike mathematics and coding, leveraging their advanced reasoning capabilities.\nNevertheless, as these capabilities progress, significant concerns regarding\ntheir vulnerabilities and safety have arisen, which can pose challenges to\ntheir deployment and application in real-world settings. This paper presents a\ncomprehensive survey of LRMs, meticulously exploring and summarizing the newly\nemerged safety risks, attacks, and defense strategies. By organizing these\nelements into a detailed taxonomy, this work aims to offer a clear and\nstructured understanding of the current safety landscape of LRMs, facilitating\nfuture research and development to enhance the security and reliability of\nthese powerful models."}
{"id": "2504.17355", "pdf": "https://arxiv.org/pdf/2504.17355", "abs": "https://arxiv.org/abs/2504.17355", "authors": ["Xiaohan Huang", "Dongjie Wang", "Zhiyuan Ning", "Ziyue Qiao", "Qingqing Long", "Haowei Zhu", "Yi Du", "Min Wu", "Yuanchun Zhou", "Meng Xiao"], "title": "Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, Keywords: Automated Feature Transformation, Tabular\n  Dataset, Reinforcement Learning", "summary": "Feature transformation methods aim to find an optimal mathematical\nfeature-feature crossing process that generates high-value features and\nimproves the performance of downstream machine learning tasks. Existing\nframeworks, though designed to mitigate manual costs, often treat feature\ntransformations as isolated operations, ignoring dynamic dependencies between\ntransformation steps. To address the limitations, we propose TCTO, a\ncollaborative multi-agent reinforcement learning framework that automates\nfeature engineering through graph-driven path optimization. The framework's\ncore innovation lies in an evolving interaction graph that models features as\nnodes and transformations as edges. Through graph pruning and backtracking, it\ndynamically eliminates low-impact edges, reduces redundant operations, and\nenhances exploration stability. This graph also provides full traceability to\nempower TCTO to reuse high-utility subgraphs from historical transformations.\nTo demonstrate the efficacy and adaptability of our approach, we conduct\ncomprehensive experiments and case studies, which show superior performance\nacross a range of datasets."}
{"id": "2504.17069", "pdf": "https://arxiv.org/pdf/2504.17069", "abs": "https://arxiv.org/abs/2504.17069", "authors": ["Rishav Pramanik", "Antoine Poupon", "Juan A. Rodriguez", "Masih Aminbeidokhti", "David Vazquez", "Christopher Pal", "Zhaozheng Yin", "Marco Pedersoli"], "title": "Distilling semantically aware orders for autoregressive image generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autoregressive patch-based image generation has recently shown competitive\nresults in terms of image quality and scalability. It can also be easily\nintegrated and scaled within Vision-Language models. Nevertheless,\nautoregressive models require a defined order for patch generation. While a\nnatural order based on the dictation of the words makes sense for text\ngeneration, there is no inherent generation order that exists for image\ngeneration. Traditionally, a raster-scan order (from top-left to bottom-right)\nguides autoregressive image generation models. In this paper, we argue that\nthis order is suboptimal, as it fails to respect the causality of the image\ncontent: for instance, when conditioned on a visual description of a sunset, an\nautoregressive model may generate clouds before the sun, even though the color\nof clouds should depend on the color of the sun and not the inverse. In this\nwork, we show that first by training a model to generate patches in\nany-given-order, we can infer both the content and the location (order) of each\npatch during generation. Secondly, we use these extracted orders to finetune\nthe any-given-order model to produce better-quality images. Through our\nexperiments, we show on two datasets that this new generation method produces\nbetter images than the traditional raster-scan approach, with similar training\ncosts and no extra annotations."}
{"id": "2504.17720", "pdf": "https://arxiv.org/pdf/2504.17720", "abs": "https://arxiv.org/abs/2504.17720", "authors": ["Vansh Gupta", "Sankalan Pal Chowdhury", "VilÃ©m Zouhar", "Donya Rooein", "Mrinmaya Sachan"], "title": "Multilingual Performance Biases of Large Language Models in Education", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment."}
{"id": "2504.17370", "pdf": "https://arxiv.org/pdf/2504.17370", "abs": "https://arxiv.org/abs/2504.17370", "authors": ["Marco Carpentiero", "Virginia Bordignon", "Vincenzo Matta", "Ali H. Sayed"], "title": "Doubly Adaptive Social Learning", "categories": ["cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In social learning, a network of agents assigns probability scores (beliefs)\nto some hypotheses of interest, which rule the generation of local streaming\ndata observed by each agent. Belief formation takes place by means of an\niterative two-step procedure where: i) the agents update locally their beliefs\nby using some likelihood model; and ii) the updated beliefs are combined with\nthe beliefs of the neighboring agents, using a pooling rule. This procedure can\nfail to perform well in the presence of dynamic drifts, leading the agents to\nincorrect decision making. Here, we focus on the fully online setting where\nboth the true hypothesis and the likelihood models can change over time. We\npropose the doubly adaptive social learning ($\\text{A}^2\\text{SL}$) strategy,\nwhich infuses social learning with the necessary adaptation capabilities. This\ngoal is achieved by exploiting two adaptation stages: i) a stochastic gradient\ndescent update to learn and track the drifts in the decision model; ii) and an\nadaptive belief update to track the true hypothesis changing over time. These\nstages are controlled by two adaptation parameters that govern the evolution of\nthe error probability for each agent. We show that all agents learn\nconsistently for sufficiently small adaptation parameters, in the sense that\nthey ultimately place all their belief mass on the true hypothesis. In\nparticular, the probability of choosing the wrong hypothesis converges to\nvalues on the order of the adaptation parameters. The theoretical analysis is\nillustrated both on synthetic data and by applying the $\\text{A}^2\\text{SL}$\nstrategy to a social learning problem in the online setting using real data."}
{"id": "2504.17070", "pdf": "https://arxiv.org/pdf/2504.17070", "abs": "https://arxiv.org/abs/2504.17070", "authors": ["Mohaiminul Al Nahian", "Zainab Altaweel", "David Reitano", "Sabbir Ahmed", "Saumitra Lohokare", "Shiqi Zhang", "Adnan Siraj Rakin"], "title": "Robo-Troj: Attacking LLM-based Task Planners", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robots need task planning methods to achieve goals that require more than\nindividual actions. Recently, large language models (LLMs) have demonstrated\nimpressive performance in task planning. LLMs can generate a step-by-step\nsolution using a description of actions and the goal. Despite the successes in\nLLM-based task planning, there is limited research studying the security\naspects of those systems. In this paper, we develop Robo-Troj, the first\nmulti-trigger backdoor attack for LLM-based task planners, which is the main\ncontribution of this work. As a multi-trigger attack, Robo-Troj is trained to\naccommodate the diversity of robot application domains. For instance, one can\nuse unique trigger words, e.g., \"herical\", to activate a specific malicious\nbehavior, e.g., cutting hand on a kitchen robot. In addition, we develop an\noptimization method for selecting the trigger words that are most effective.\nThrough demonstrating the vulnerability of LLM-based planners, we aim to\npromote the development of secured robot systems."}
{"id": "2504.17753", "pdf": "https://arxiv.org/pdf/2504.17753", "abs": "https://arxiv.org/abs/2504.17753", "authors": ["Anuja Tayal", "Devika Salunke", "Barbara Di Eugenio", "Paula Allen-Meares", "Eulalia Puig Abril", "Olga Garcia", "Carolyn Dickens", "Andrew Boyd"], "title": "Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT", "categories": ["cs.CL"], "comment": null, "summary": "Conversational assistants are becoming more and more popular, including in\nhealthcare, partly because of the availability and capabilities of Large\nLanguage Models. There is a need for controlled, probing evaluations with real\nstakeholders which can highlight advantages and disadvantages of more\ntraditional architectures and those based on generative AI. We present a\nwithin-group user study to compare two versions of a conversational assistant\nthat allows heart failure patients to ask about salt content in food. One\nversion of the system was developed in-house with a neurosymbolic architecture,\nand one is based on ChatGPT. The evaluation shows that the in-house system is\nmore accurate, completes more tasks and is less verbose than the one based on\nChatGPT; on the other hand, the one based on ChatGPT makes fewer speech errors\nand requires fewer clarifications to complete the task. Patients show no\npreference for one over the other."}
{"id": "2504.17403", "pdf": "https://arxiv.org/pdf/2504.17403", "abs": "https://arxiv.org/abs/2504.17403", "authors": ["Hans Rosenberger", "Rodrigo Fischer", "Johanna S. FrÃ¶hlich", "Ali Bereyhi", "Ralf R. MÃ¼ller"], "title": "Coding for Computation: Efficient Compression of Neural Networks for Reconfigurable Hardware", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT"], "comment": "Accepted at the 2025 IEEE Statistical Signal Processing (SSP)\n  Workshop, Edinburgh", "summary": "As state of the art neural networks (NNs) continue to grow in size, their\nresource-efficient implementation becomes ever more important. In this paper,\nwe introduce a compression scheme that reduces the number of computations\nrequired for NN inference on reconfigurable hardware such as FPGAs. This is\nachieved by combining pruning via regularized training, weight sharing and\nlinear computation coding (LCC). Contrary to common NN compression techniques,\nwhere the objective is to reduce the memory used for storing the weights of the\nNNs, our approach is optimized to reduce the number of additions required for\ninference in a hardware-friendly manner. The proposed scheme achieves\ncompetitive performance for simple multilayer perceptrons, as well as for large\nscale deep NNs such as ResNet-34."}
{"id": "2504.17077", "pdf": "https://arxiv.org/pdf/2504.17077", "abs": "https://arxiv.org/abs/2504.17077", "authors": ["Dongjin Seo", "Soobin Um", "Sangbin Lee", "Jong Chul Ye", "Haejun Chung"], "title": "Physics-guided and fabrication-aware inverse design of photonic devices using diffusion models", "categories": ["physics.optics", "cs.AI", "physics.comp-ph"], "comment": "25 pages, 7 Figures", "summary": "Designing free-form photonic devices is fundamentally challenging due to the\nvast number of possible geometries and the complex requirements of fabrication\nconstraints. Traditional inverse-design approaches--whether driven by human\nintuition, global optimization, or adjoint-based gradient methods--often\ninvolve intricate binarization and filtering steps, while recent deep learning\nstrategies demand prohibitively large numbers of simulations (10^5 to 10^6). To\novercome these limitations, we present AdjointDiffusion, a physics-guided\nframework that integrates adjoint sensitivity gradients into the sampling\nprocess of diffusion models. AdjointDiffusion begins by training a diffusion\nnetwork on a synthetic, fabrication-aware dataset of binary masks. During\ninference, we compute the adjoint gradient of a candidate structure and inject\nthis physics-based guidance at each denoising step, steering the generative\nprocess toward high figure-of-merit (FoM) solutions without additional\npost-processing. We demonstrate our method on two canonical photonic design\nproblems--a bent waveguide and a CMOS image sensor color router--and show that\nour method consistently outperforms state-of-the-art nonlinear optimizers (such\nas MMA and SLSQP) in both efficiency and manufacturability, while using orders\nof magnitude fewer simulations (approximately 2 x 10^2) than pure deep learning\napproaches (approximately 10^5 to 10^6). By eliminating complex binarization\nschedules and minimizing simulation overhead, AdjointDiffusion offers a\nstreamlined, simulation-efficient, and fabrication-aware pipeline for\nnext-generation photonic device design. Our open-source implementation is\navailable at https://github.com/dongjin-seo2020/AdjointDiffusion."}
{"id": "2504.17768", "pdf": "https://arxiv.org/pdf/2504.17768", "abs": "https://arxiv.org/abs/2504.17768", "authors": ["Piotr Nawrot", "Robert Li", "Renjie Huang", "Sebastian Ruder", "Kelly Marchisio", "Edoardo M. Ponti"], "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications."}
{"id": "2504.17421", "pdf": "https://arxiv.org/pdf/2504.17421", "abs": "https://arxiv.org/abs/2504.17421", "authors": ["Yang Liu", "Bingjie Yan", "Tianyuan Zou", "Jianqing Zhang", "Zixuan Gu", "Jianbing Ding", "Xidong Wang", "Jingyi Li", "Xiaozhou Ye", "Ye Ouyang", "Qiang Yang", "Ya-Qin Zhang"], "title": "Towards Harnessing the Collaborative Power of Large and Small Models for Domain Tasks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nthey require vast amounts of data and computational resources. In contrast,\nsmaller models (SMs), while less powerful, can be more efficient and tailored\nto specific domains. In this position paper, we argue that taking a\ncollaborative approach, where large and small models work synergistically, can\naccelerate the adaptation of LLMs to private domains and unlock new potential\nin AI. We explore various strategies for model collaboration and identify\npotential challenges and opportunities. Building upon this, we advocate for\nindustry-driven research that prioritizes multi-objective benchmarks on\nreal-world private datasets and applications."}
{"id": "2504.17114", "pdf": "https://arxiv.org/pdf/2504.17114", "abs": "https://arxiv.org/abs/2504.17114", "authors": ["Valentin Langer", "Kartikay Tehlan", "Thomas Wendler"], "title": "Anatomy-constrained modelling of image-derived input functions in dynamic PET using multi-organ segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.med-ph"], "comment": "The code is available under\n  https://github.com/tinolan/curve_fit_multi_idif", "summary": "Accurate kinetic analysis of [$^{18}$F]FDG distribution in dynamic positron\nemission tomography (PET) requires anatomically constrained modelling of\nimage-derived input functions (IDIFs). Traditionally, IDIFs are obtained from\nthe aorta, neglecting anatomical variations and complex vascular contributions.\nThis study proposes a multi-organ segmentation-based approach that integrates\nIDIFs from the aorta, portal vein, pulmonary artery, and ureters. Using\nhigh-resolution CT segmentations of the liver, lungs, kidneys, and bladder, we\nincorporate organ-specific blood supply sources to improve kinetic modelling.\nOur method was evaluated on dynamic [$^{18}$F]FDG PET data from nine patients,\nresulting in a mean squared error (MSE) reduction of $13.39\\%$ for the liver\nand $10.42\\%$ for the lungs. These initial results highlight the potential of\nmultiple IDIFs in improving anatomical modelling and fully leveraging dynamic\nPET imaging. This approach could facilitate the integration of tracer kinetic\nmodelling into clinical routine."}
{"id": "2504.16939", "pdf": "https://arxiv.org/pdf/2504.16939", "abs": "https://arxiv.org/abs/2504.16939", "authors": ["Emre Can Acikgoz", "Cheng Qian", "Hongru Wang", "Vardhan Dongre", "Xiusi Chen", "Heng Ji", "Dilek Hakkani-TÃ¼r", "Gokhan Tur"], "title": "A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have propelled conversational\nAI from traditional dialogue systems into sophisticated agents capable of\nautonomous actions, contextual awareness, and multi-turn interactions with\nusers. Yet, fundamental questions about their capabilities, limitations, and\npaths forward remain open. This survey paper presents a desideratum for\nnext-generation Conversational Agents - what has been achieved, what challenges\npersist, and what must be done for more scalable systems that approach\nhuman-level intelligence. To that end, we systematically analyze LLM-driven\nConversational Agents by organizing their capabilities into three primary\ndimensions: (i) Reasoning - logical, systematic thinking inspired by human\nintelligence for decision making, (ii) Monitor - encompassing self-awareness\nand user interaction monitoring, and (iii) Control - focusing on tool\nutilization and policy following. Building upon this, we introduce a novel\ntaxonomy by classifying recent work on Conversational Agents around our\nproposed desideratum. We identify critical research gaps and outline key\ndirections, including realistic evaluations, long-term multi-turn reasoning\nskills, self-evolution capabilities, collaborative and multi-agent task\ncompletion, personalization, and proactivity. This work aims to provide a\nstructured foundation, highlight existing limitations, and offer insights into\npotential future research directions for Conversational Agents, ultimately\nadvancing progress toward Artificial General Intelligence (AGI). We maintain a\ncurated repository of papers at:\nhttps://github.com/emrecanacikgoz/awesome-conversational-agents."}
{"id": "2504.17448", "pdf": "https://arxiv.org/pdf/2504.17448", "abs": "https://arxiv.org/abs/2504.17448", "authors": ["Jun Zhang", "Jue Wang", "Huan Li", "Zhongle Xie", "Ke Chen", "Lidan Shou"], "title": "CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated Active Learning", "categories": ["cs.LG", "cs.DB", "cs.DC"], "comment": "Accepted by TKDE 2025", "summary": "Active learning (AL) reduces human annotation costs for machine learning\nsystems by strategically selecting the most informative unlabeled data for\nannotation, but performing it individually may still be insufficient due to\nrestricted data diversity and annotation budget. Federated Active Learning\n(FAL) addresses this by facilitating collaborative data selection and model\ntraining, while preserving the confidentiality of raw data samples. Yet,\nexisting FAL methods fail to account for the heterogeneity of data distribution\nacross clients and the associated fluctuations in global and local model\nparameters, adversely affecting model accuracy. To overcome these challenges,\nwe propose CHASe (Client Heterogeneity-Aware Data Selection), specifically\ndesigned for FAL. CHASe focuses on identifying those unlabeled samples with\nhigh epistemic variations (EVs), which notably oscillate around the decision\nboundaries during training. To achieve both effectiveness and efficiency,\n\\model{} encompasses techniques for 1) tracking EVs by analyzing inference\ninconsistencies across training epochs, 2) calibrating decision boundaries of\ninaccurate models with a new alignment loss, and 3) enhancing data selection\nefficiency via a data freeze and awaken mechanism with subset sampling.\nExperiments show that CHASe surpasses various established baselines in terms of\neffectiveness and efficiency, validated across diverse datasets, model\ncomplexities, and heterogeneous federation settings."}
{"id": "2504.17119", "pdf": "https://arxiv.org/pdf/2504.17119", "abs": "https://arxiv.org/abs/2504.17119", "authors": ["Muskan Garg", "Shaina Raza", "Shebuti Rayana", "Xingyi Liu", "Sunghwan Sohn"], "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey", "categories": ["cs.CL", "cs.AI"], "comment": "35 pages, 7 tables, 5 figures", "summary": "Despite substantial progress in healthcare applications driven by large\nlanguage models (LLMs), growing concerns around data privacy, and limited\nresources; the small language models (SLMs) offer a scalable and clinically\nviable solution for efficient performance in resource-constrained environments\nfor next-generation healthcare informatics. Our comprehensive survey presents a\ntaxonomic framework to identify and categorize them for healthcare\nprofessionals and informaticians. The timeline of healthcare SLM contributions\nestablishes a foundational framework for analyzing models across three\ndimensions: NLP tasks, stakeholder roles, and the continuum of care. We present\na taxonomic framework to identify the architectural foundations for building\nmodels from scratch; adapting SLMs to clinical precision through prompting,\ninstruction fine-tuning, and reasoning; and accessibility and sustainability\nthrough compression techniques. Our primary objective is to offer a\ncomprehensive survey for healthcare professionals, introducing recent\ninnovations in model optimization and equipping them with curated resources to\nsupport future research and development in the field. Aiming to showcase the\ngroundbreaking advancements in SLMs for healthcare, we present a comprehensive\ncompilation of experimental results across widely studied NLP tasks in\nhealthcare to highlight the transformative potential of SLMs in healthcare. The\nupdated repository is available at Github"}
{"id": "2504.17004", "pdf": "https://arxiv.org/pdf/2504.17004", "abs": "https://arxiv.org/abs/2504.17004", "authors": ["Amin Karbasi", "Omar Montasser", "John Sous", "Grigoris Velegkas"], "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment."}
{"id": "2504.17449", "pdf": "https://arxiv.org/pdf/2504.17449", "abs": "https://arxiv.org/abs/2504.17449", "authors": ["Jun Zhang", "Jue Wang", "Huan Li", "Lidan Shou", "Ke Chen", "Gang Chen", "Qin Xie", "Guiming Xie", "Xuejian Gong"], "title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by VLDBJ 2025", "summary": "The significant computational demands of pretrained language models (PLMs),\nwhich often require dedicated hardware, present a substantial challenge in\nserving them efficiently, especially in multi-tenant environments. To address\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\nInference system, designed to manage tenants with distinct PLMs\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\nknowledge into general, domain-specific, and task-specific. Leveraging insights\non knowledge acquisition across different model layers, we construct\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\nestablish hierarchical knowledge management for hPLMs generated by various\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\nincreases by constructing and updating domain-specific knowledge trees based on\nfrequency. We manage task-specific knowledge within limited GPU memory through\nparameter swapping. Finally, we propose system optimizations to enhance\nresource utilization and inference throughput. These include fine-grained\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\noperations with GPU computations, and optimizing parallel implementations with\nbatched matrix multiplications. Our experimental results demonstrate that the\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\nsingle GPU, with only a negligible compromise in accuracy."}
{"id": "2504.17122", "pdf": "https://arxiv.org/pdf/2504.17122", "abs": "https://arxiv.org/abs/2504.17122", "authors": ["Kartikay Tehlan", "Thomas Wendler"], "title": "Physiological neural representation for personalised tracer kinetic parameter estimation from dynamic PET", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "The code is available at: https://github.com/tkartikay/PhysNRPET", "summary": "Dynamic positron emission tomography (PET) with [$^{18}$F]FDG enables\nnon-invasive quantification of glucose metabolism through kinetic analysis,\noften modelled by the two-tissue compartment model (TCKM). However, voxel-wise\nkinetic parameter estimation using conventional methods is computationally\nintensive and limited by spatial resolution. Deep neural networks (DNNs) offer\nan alternative but require large training datasets and significant\ncomputational resources. To address these limitations, we propose a\nphysiological neural representation based on implicit neural representations\n(INRs) for personalized kinetic parameter estimation. INRs, which learn\ncontinuous functions, allow for efficient, high-resolution parametric imaging\nwith reduced data requirements. Our method also integrates anatomical priors\nfrom a 3D CT foundation model to enhance robustness and precision in kinetic\nmodelling. We evaluate our approach on an [$^{18}$F]FDG dynamic PET/CT dataset\nand compare it to state-of-the-art DNNs. Results demonstrate superior spatial\nresolution, lower mean-squared error, and improved anatomical consistency,\nparticularly in tumour and highly vascularized regions. Our findings highlight\nthe potential of INRs for personalized, data-efficient tracer kinetic\nmodelling, enabling applications in tumour characterization, segmentation, and\nprognostic assessment."}
{"id": "2504.17038", "pdf": "https://arxiv.org/pdf/2504.17038", "abs": "https://arxiv.org/abs/2504.17038", "authors": ["Christian D. Newman", "Brandon Scholten", "Sophia Testa", "Joshua A. C. Behler", "Syreen Banabilah", "Michael L. Collard", "Michael J. Decker", "Mohamed Wiem Mkaouer", "Marcos Zampieri", "Eman Abdullah AlOmar", "Reem Alsuhaibani", "Anthony Peruma", "Jonathan I. Maletic"], "title": "SCALAR: A Part-of-speech Tagger for Identifiers", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "The paper presents the Source Code Analysis and Lexical Annotation Runtime\n(SCALAR), a tool specialized for mapping (annotating) source code identifier\nnames to their corresponding part-of-speech tag sequence (grammar pattern).\nSCALAR's internal model is trained using scikit-learn's\nGradientBoostingClassifier in conjunction with a manually-curated oracle of\nidentifier names and their grammar patterns. This specializes the tagger to\nrecognize the unique structure of the natural language used by developers to\ncreate all types of identifiers (e.g., function names, variable names etc.).\nSCALAR's output is compared with a previous version of the tagger, as well as a\nmodern off-the-shelf part-of-speech tagger to show how it improves upon other\ntaggers' output for annotating identifiers. The code is available on Github"}
{"id": "2504.17461", "pdf": "https://arxiv.org/pdf/2504.17461", "abs": "https://arxiv.org/abs/2504.17461", "authors": ["Vipin Singh", "Tianheng Ling", "Teodor Chiaburu", "Felix Biessmann"], "title": "Evaluating Time Series Models for Urban Wastewater Management: Predictive Performance, Model Complexity and Resilience", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, 6 figures, accepted at 10th International Conference on\n  Smart and Sustainable Technologies (SpliTech) 2025, GitHub:\n  https://github.com/calgo-lab/resilient-timeseries-evaluation", "summary": "Climate change increases the frequency of extreme rainfall, placing a\nsignificant strain on urban infrastructures, especially Combined Sewer Systems\n(CSS). Overflows from overburdened CSS release untreated wastewater into\nsurface waters, posing environmental and public health risks. Although\ntraditional physics-based models are effective, they are costly to maintain and\ndifficult to adapt to evolving system dynamics. Machine Learning (ML)\napproaches offer cost-efficient alternatives with greater adaptability. To\nsystematically assess the potential of ML for modeling urban infrastructure\nsystems, we propose a protocol for evaluating Neural Network architectures for\nCSS time series forecasting with respect to predictive performance, model\ncomplexity, and robustness to perturbations. In addition, we assess model\nperformance on peak events and critical fluctuations, as these are the key\nregimes for urban wastewater management. To investigate the feasibility of\nlightweight models suitable for IoT deployment, we compare global models, which\nhave access to all information, with local models, which rely solely on nearby\nsensor readings. Additionally, to explore the security risks posed by network\noutages or adversarial attacks on urban infrastructure, we introduce error\nmodels that assess the resilience of models. Our results demonstrate that while\nglobal models achieve higher predictive performance, local models provide\nsufficient resilience in decentralized scenarios, ensuring robust modeling of\nurban infrastructure. Furthermore, models with longer native forecast horizons\nexhibit greater robustness to data perturbations. These findings contribute to\nthe development of interpretable and reliable ML solutions for sustainable\nurban wastewater management. The implementation is available in our GitHub\nrepository."}
{"id": "2504.17124", "pdf": "https://arxiv.org/pdf/2504.17124", "abs": "https://arxiv.org/abs/2504.17124", "authors": ["Ming Du", "Mark Wolfman", "Chengjun Sun", "Shelly D. Kelly", "Mathew J. Cherukara"], "title": "Demonstration of an AI-driven workflow for dynamic x-ray spectroscopy", "categories": ["physics.app-ph", "cs.AI", "cs.CE", "cs.SY", "eess.SY"], "comment": null, "summary": "X-ray absorption near edge structure (XANES) spectroscopy is a powerful\ntechnique for characterizing the chemical state and symmetry of individual\nelements within materials, but requires collecting data at many energy points\nwhich can be time-consuming. While adaptive sampling methods exist for\nefficiently collecting spectroscopic data, they often lack domain-specific\nknowledge about XANES spectra structure. Here we demonstrate a\nknowledge-injected Bayesian optimization approach for adaptive XANES data\ncollection that incorporates understanding of spectral features like absorption\nedges and pre-edge peaks. We show this method accurately reconstructs the\nabsorption edge of XANES spectra using only 15-20% of the measurement points\ntypically needed for conventional sampling, while maintaining the ability to\ndetermine the x-ray energy of the sharp peak after absorption edge with errors\nless than 0.03 eV, the absorption edge with errors less than 0.1 eV; and\noverall root-mean-square errors less than 0.005 compared to compared to\ntraditionally sampled spectra. Our experiments on battery materials and\ncatalysts demonstrate the method's effectiveness for both static and dynamic\nXANES measurements, improving data collection efficiency and enabling better\ntime resolution for tracking chemical changes. This approach advances the\ndegree of automation in XANES experiments reducing the common errors of under-\nor over-sampling points in near the absorption edge and enabling dynamic\nexperiments that require high temporal resolution or limited measurement time."}
{"id": "2504.17365", "pdf": "https://arxiv.org/pdf/2504.17365", "abs": "https://arxiv.org/abs/2504.17365", "authors": ["Ling You", "Wenxuan Huang", "Xinni Xie", "Xiangyi Wei", "Bangyan Li", "Shaohui Lin", "Yang Li", "Changbo Wang"], "title": "TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Soccer is a globally popular sporting event, typically characterized by long\nmatches and distinctive highlight moments. Recent advances in Multimodal Large\nLanguage Models (MLLMs) offer promising capabilities in temporal grounding and\nvideo understanding, soccer commentary generation often requires precise\ntemporal localization and semantically rich descriptions over long-form video.\nHowever, existing soccer MLLMs often rely on the temporal a priori for caption\ngeneration, so they cannot process the soccer video end-to-end. While some\ntraditional approaches follow a two-step paradigm that is complex and fails to\ncapture the global context to achieve suboptimal performance. To solve the\nabove issues, we present TimeSoccer, the first end-to-end soccer MLLM for\nSingle-anchor Dense Video Captioning (SDVC) in full-match soccer videos.\nTimeSoccer jointly predicts timestamps and generates captions in a single pass,\nenabling global context modeling across 45-minute matches. To support long\nvideo understanding of soccer matches, we introduce MoFA-Select, a\ntraining-free, motion-aware frame compression module that adaptively selects\nrepresentative frames via a coarse-to-fine strategy, and incorporates\ncomplementary training paradigms to strengthen the model's ability to handle\nlong temporal sequences. Extensive experiments demonstrate that our TimeSoccer\nachieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end\nform, generating high-quality commentary with accurate temporal alignment and\nstrong semantic relevance."}
{"id": "2504.17471", "pdf": "https://arxiv.org/pdf/2504.17471", "abs": "https://arxiv.org/abs/2504.17471", "authors": ["Yacine Belal", "Mohamed Maouche", "Sonia Ben Mokhtar", "Anthony Simonet-Boulogne"], "title": "GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Gossip Learning (GL) is a decentralized learning paradigm where users\niteratively exchange and aggregate models with a small set of neighboring\npeers. Recent GL approaches rely on dynamic communication graphs built and\nmaintained using Random Peer Sampling (RPS) protocols. Thanks to graph\ndynamics, GL can achieve fast convergence even over extremely sparse\ntopologies. However, the robustness of GL over dy- namic graphs to Byzantine\n(model poisoning) attacks remains unaddressed especially when Byzantine nodes\nattack the RPS protocol to scale up model poisoning. We address this issue by\nintroducing GRANITE, a framework for robust learning over sparse, dynamic\ngraphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two\nkey components (i) a History-aware Byzantine-resilient Peer Sampling protocol\n(HaPS), which tracks previously encountered identifiers to reduce adversarial\ninfluence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which\nleverages an estimate of Byzantine presence to set aggregation thresholds with\nformal guarantees. Empirical results confirm that GRANITE maintains convergence\nwith up to 30% Byzantine nodes, improves learning speed via adaptive filtering\nof poisoned models and obtains these results in up to 9 times sparser graphs\nthan dictated by current theory."}
{"id": "2504.17129", "pdf": "https://arxiv.org/pdf/2504.17129", "abs": "https://arxiv.org/abs/2504.17129", "authors": ["Seyed Yousef Soltanian", "Wenlong Zhang"], "title": "Peer-Aware Cost Estimation in Nonlinear General-Sum Dynamic Games for Mutual Learning and Intent Inference", "categories": ["eess.SY", "cs.AI", "cs.GT", "cs.RO", "cs.SY", "93C41, 49N70, 49N90, 91A27"], "comment": null, "summary": "Human-robot interactions can be modeled as incomplete-information general-sum\ndynamic games since the objective functions of both agents are not explicitly\nknown to each other. However, solving for equilibrium policies for such games\npresents a major challenge, especially if the games involve nonlinear\nunderlying dynamics. To simplify the problem, existing work often assumes that\none agent is an expert with complete information about its peer, which can lead\nto biased estimates and failures in coordination. To address this challenge, we\npropose a nonlinear peer-aware cost estimation (N-PACE) algorithm for\ngeneral-sum dynamic games. In N-PACE, using iterative linear quadratic (LQ)\napproximation of the nonlinear general-sum game, each agent explicitly models\nthe learning dynamics of its peer agent while inferring their objective\nfunctions, leading to unbiased fast learning in inferring the unknown objective\nfunction of the peer agent, which is critical for task completion and safety\nassurance. Additionally, we demonstrate how N-PACE enables \\textbf{intent\ncommunication} in such multi-agent systems by explicitly modeling the peer's\nlearning dynamics."}
{"id": "2504.17449", "pdf": "https://arxiv.org/pdf/2504.17449", "abs": "https://arxiv.org/abs/2504.17449", "authors": ["Jun Zhang", "Jue Wang", "Huan Li", "Lidan Shou", "Ke Chen", "Gang Chen", "Qin Xie", "Guiming Xie", "Xuejian Gong"], "title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by VLDBJ 2025", "summary": "The significant computational demands of pretrained language models (PLMs),\nwhich often require dedicated hardware, present a substantial challenge in\nserving them efficiently, especially in multi-tenant environments. To address\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\nInference system, designed to manage tenants with distinct PLMs\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\nknowledge into general, domain-specific, and task-specific. Leveraging insights\non knowledge acquisition across different model layers, we construct\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\nestablish hierarchical knowledge management for hPLMs generated by various\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\nincreases by constructing and updating domain-specific knowledge trees based on\nfrequency. We manage task-specific knowledge within limited GPU memory through\nparameter swapping. Finally, we propose system optimizations to enhance\nresource utilization and inference throughput. These include fine-grained\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\noperations with GPU computations, and optimizing parallel implementations with\nbatched matrix multiplications. Our experimental results demonstrate that the\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\nsingle GPU, with only a negligible compromise in accuracy."}
{"id": "2504.17490", "pdf": "https://arxiv.org/pdf/2504.17490", "abs": "https://arxiv.org/abs/2504.17490", "authors": ["Mingqi Yuan", "Qi Wang", "Guozheng Ma", "Bo Li", "Xin Jin", "Yunbo Wang", "Xiaokang Yang", "Wenjun Zeng", "Dacheng Tao"], "title": "Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages", "summary": "Developing lifelong learning agents is crucial for artificial general\nintelligence. However, deep reinforcement learning (RL) systems often suffer\nfrom plasticity loss, where neural networks gradually lose their ability to\nadapt during training. Despite its significance, this field lacks unified\nbenchmarks and evaluation protocols. We introduce Plasticine, the first\nopen-source framework for benchmarking plasticity optimization in deep RL.\nPlasticine provides single-file implementations of over 13 mitigation methods,\n10 evaluation metrics, and learning scenarios with increasing non-stationarity\nlevels from standard to open-ended environments. This framework enables\nresearchers to systematically quantify plasticity loss, evaluate mitigation\nstrategies, and analyze plasticity dynamics across different contexts. Our\ndocumentation, examples, and source code are available at\nhttps://github.com/RLE-Foundation/Plasticine."}
{"id": "2504.17137", "pdf": "https://arxiv.org/pdf/2504.17137", "abs": "https://arxiv.org/abs/2504.17137", "authors": ["Chanhee Park", "Hyeonseok Moon", "Chanjun Park", "Heuiseok Lim"], "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL2025 Findings", "summary": "Retrieval-Augmented Generation (RAG) has gained prominence as an effective\nmethod for enhancing the generative capabilities of Large Language Models\n(LLMs) through the incorporation of external knowledge. However, the evaluation\nof RAG systems remains a challenge, due to the intricate interplay between\nretrieval and generation components. This limitation has resulted in a scarcity\nof benchmarks that facilitate a detailed, component-specific assessment. In\nthis work, we present MIRAGE, a Question Answering dataset specifically\ndesigned for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped\nto a retrieval pool of 37,800 entries, enabling an efficient and precise\nevaluation of both retrieval and generation tasks. We also introduce novel\nevaluation metrics aimed at measuring RAG adaptability, encompassing dimensions\nsuch as noise vulnerability, context acceptability, context insensitivity, and\ncontext misinterpretation. Through comprehensive experiments across various\nretriever-LLM configurations, we provide new insights into the optimal\nalignment of model pairs and the nuanced dynamics within RAG systems. The\ndataset and evaluation code are publicly available, allowing for seamless\nintegration and customization in diverse research settings\\footnote{The MIRAGE\ncode and data are available at https://github.com/nlpai-lab/MIRAGE."}
{"id": "2504.17492", "pdf": "https://arxiv.org/pdf/2504.17492", "abs": "https://arxiv.org/abs/2504.17492", "authors": ["Nawid Keshtmand", "Elena Fillola", "Jeffrey Nicholas Clark", "Raul Santos-Rodriguez", "Matthew Rigby"], "title": "Prototype-enhanced prediction in graph neural networks for climate applications", "categories": ["cs.LG"], "comment": null, "summary": "Data-driven emulators are increasingly being used to learn and emulate\nphysics-based simulations, reducing computational expense and run time. Here,\nwe present a structured way to improve the quality of these high-dimensional\nemulated outputs, through the use of prototypes: an approximation of the\nemulator's output passed as an input, which informs the model and leads to\nbetter predictions. We demonstrate our approach to emulate atmospheric\ndispersion, key for greenhouse gas emissions monitoring, by comparing a\nbaseline model to models trained using prototypes as an additional input. The\nprototype models achieve better performance, even with few prototypes and even\nif they are chosen at random, but we show that choosing the prototypes through\ndata-driven methods (k-means) can lead to almost 10\\% increased performance in\nsome metrics."}
{"id": "2504.17140", "pdf": "https://arxiv.org/pdf/2504.17140", "abs": "https://arxiv.org/abs/2504.17140", "authors": ["Ashish Ranjan", "Ayush Agarwal", "Shalin Barot", "Sushant Kumar"], "title": "Scalable Permutation-Aware Modeling for Temporal Set Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Temporal set prediction involves forecasting the elements that will appear in\nthe next set, given a sequence of prior sets, each containing a variable number\nof elements. Existing methods often rely on intricate architectures with\nsubstantial computational overhead, which hampers their scalability. In this\nwork, we introduce a novel and scalable framework that leverages\npermutation-equivariant and permutation-invariant transformations to\nefficiently model set dynamics. Our approach significantly reduces both\ntraining and inference time while maintaining competitive performance.\nExtensive experiments on multiple public benchmarks show that our method\nachieves results on par with or superior to state-of-the-art models across\nseveral evaluation metrics. These results underscore the effectiveness of our\nmodel in enabling efficient and scalable temporal set prediction."}
{"id": "2504.17493", "pdf": "https://arxiv.org/pdf/2504.17493", "abs": "https://arxiv.org/abs/2504.17493", "authors": ["Luca-Andrei Fechete", "Mohamed Sana", "Fadhel Ayed", "Nicola Piovesan", "Wenjie Li", "Antonio De Domenico", "Tareq Si Salem"], "title": "Goal-Oriented Time-Series Forecasting: Foundation Framework Design", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traditional time-series forecasting often focuses only on minimizing\nprediction errors, ignoring the specific requirements of real-world\napplications that employ them. This paper presents a new training methodology,\nwhich allows a forecasting model to dynamically adjust its focus based on the\nimportance of forecast ranges specified by the end application. Unlike previous\nmethods that fix these ranges beforehand, our training approach breaks down\npredictions over the entire signal range into smaller segments, which are then\ndynamically weighted and combined to produce accurate forecasts. We tested our\nmethod on standard datasets, including a new dataset from wireless\ncommunication, and found that not only it improves prediction accuracy but also\nimproves the performance of end application employing the forecasting model.\nThis research provides a basis for creating forecasting systems that better\nconnect prediction and decision-making in various practical applications."}
{"id": "2504.17160", "pdf": "https://arxiv.org/pdf/2504.17160", "abs": "https://arxiv.org/abs/2504.17160", "authors": ["Alberto FernÃ¡ndez-HernÃ¡ndez", "Jose I. Mestre", "Manuel F. Dolz", "Jose Duato", "Enrique S. Quintana-OrtÃ­"], "title": "OUI Need to Talk About Weight Decay: A New Perspective on Overfitting Detection", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "10 pages, 3 figures", "summary": "We introduce the Overfitting-Underfitting Indicator (OUI), a novel tool for\nmonitoring the training dynamics of Deep Neural Networks (DNNs) and identifying\noptimal regularization hyperparameters. Specifically, we validate that OUI can\neffectively guide the selection of the Weight Decay (WD) hyperparameter by\nindicating whether a model is overfitting or underfitting during training\nwithout requiring validation data. Through experiments on DenseNet-BC-100 with\nCIFAR- 100, EfficientNet-B0 with TinyImageNet and ResNet-34 with ImageNet-1K,\nwe show that maintaining OUI within a prescribed interval correlates strongly\nwith improved generalization and validation scores. Notably, OUI converges\nsignificantly faster than traditional metrics such as loss or accuracy,\nenabling practitioners to identify optimal WD (hyperparameter) values within\nthe early stages of training. By leveraging OUI as a reliable indicator, we can\ndetermine early in training whether the chosen WD value leads the model to\nunderfit the training data, overfit, or strike a well-balanced trade-off that\nmaximizes validation scores. This enables more precise WD tuning for optimal\nperformance on the tested datasets and DNNs. All code for reproducing these\nexperiments is available at https://github.com/AlbertoFdezHdez/OUI."}
{"id": "2504.17497", "pdf": "https://arxiv.org/pdf/2504.17497", "abs": "https://arxiv.org/abs/2504.17497", "authors": ["Radia Berreziga", "Mohammed Brahimi", "Khairedine Kraim", "Hamid Azzoune"], "title": "Combining GCN Structural Learning with LLM Chemical Knowledge for or Enhanced Virtual Screening", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Virtual screening plays a critical role in modern drug discovery by enabling\nthe identification of promising candidate molecules for experimental\nvalidation. Traditional machine learning methods such as support vector\nmachines (SVM) and XGBoost rely on predefined molecular representations, often\nleading to information loss and potential bias. In contrast, deep learning\napproaches-particularly Graph Convolutional Networks (GCNs)-offer a more\nexpressive and unbiased alternative by operating directly on molecular graphs.\nMeanwhile, Large Language Models (LLMs) have recently demonstrated\nstate-of-the-art performance in drug design, thanks to their capacity to\ncapture complex chemical patterns from large-scale data via attention\nmechanisms.\n  In this paper, we propose a hybrid architecture that integrates GCNs with\nLLM-derived embeddings to combine localized structural learning with global\nchemical knowledge. The LLM embeddings can be precomputed and stored in a\nmolecular feature library, removing the need to rerun the LLM during training\nor inference and thus maintaining computational efficiency. We found that\nconcatenating the LLM embeddings after each GCN layer-rather than only at the\nfinal layer-significantly improves performance, enabling deeper integration of\nglobal context throughout the network. The resulting model achieves superior\nresults, with an F1-score of (88.8%), outperforming standalone GCN (87.9%),\nXGBoost (85.5%), and SVM (85.4%) baselines."}
{"id": "2504.17162", "pdf": "https://arxiv.org/pdf/2504.17162", "abs": "https://arxiv.org/abs/2504.17162", "authors": ["Cece Zhang", "Xuehuan Zhu", "Nick Peterson", "Jieqiong Wang", "Shibiao Wan"], "title": "A Comprehensive Review on RNA Subcellular Localization Prediction", "categories": ["cs.CV", "cs.AI", "q-bio.GN", "q-bio.SC"], "comment": null, "summary": "The subcellular localization of RNAs, including long non-coding RNAs\n(lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs,\nplays a critical role in determining their biological functions. For instance,\nlncRNAs are predominantly associated with chromatin and act as regulators of\ngene transcription and chromatin structure, while mRNAs are distributed across\nthe nucleus and cytoplasm, facilitating the transport of genetic information\nfor protein synthesis. Understanding RNA localization sheds light on processes\nlike gene expression regulation with spatial and temporal precision. However,\ntraditional wet lab methods for determining RNA localization, such as in situ\nhybridization, are often time-consuming, resource-demanding, and costly. To\novercome these challenges, computational methods leveraging artificial\nintelligence (AI) and machine learning (ML) have emerged as powerful\nalternatives, enabling large-scale prediction of RNA subcellular localization.\nThis paper provides a comprehensive review of the latest advancements in\nAI-based approaches for RNA subcellular localization prediction, covering\nvarious RNA types and focusing on sequence-based, image-based, and hybrid\nmethodologies that combine both data types. We highlight the potential of these\nmethods to accelerate RNA research, uncover molecular pathways, and guide\ntargeted disease treatments. Furthermore, we critically discuss the challenges\nin AI/ML approaches for RNA subcellular localization, such as data scarcity and\nlack of benchmarks, and opportunities to address them. This review aims to\nserve as a valuable resource for researchers seeking to develop innovative\nsolutions in the field of RNA subcellular localization and beyond."}
{"id": "2504.17503", "pdf": "https://arxiv.org/pdf/2504.17503", "abs": "https://arxiv.org/abs/2504.17503", "authors": ["Davide Prosperino", "Haochun Ma", "Christoph RÃ¤th"], "title": "Tailored minimal reservoir computing: on the bidirectional connection between nonlinearities in the reservoir and in data", "categories": ["cs.LG", "nlin.CD"], "comment": "13 pages, 11 figures", "summary": "We study how the degree of nonlinearity in the input data affects the optimal\ndesign of reservoir computers, focusing on how closely the model's nonlinearity\nshould align with that of the data. By reducing minimal RCs to a single tunable\nnonlinearity parameter, we explore how the predictive performance varies with\nthe degree of nonlinearity in the reservoir. To provide controlled testbeds, we\ngeneralize to the fractional Halvorsen system, a novel chaotic system with\nfractional exponents. Our experiments reveal that the prediction performance is\nmaximized when the reservoir's nonlinearity matches the nonlinearity present in\nthe data. In cases where multiple nonlinearities are present in the data, we\nfind that the correlation dimension of the predicted signal is reconstructed\ncorrectly when the smallest nonlinearity is matched. We use this observation to\npropose a method for estimating the minimal nonlinearity in unknown time series\nby sweeping the reservoir exponent and identifying the transition to a\nsuccessful reconstruction. Applying this method to both synthetic and\nreal-world datasets, including financial time series, we demonstrate its\npractical viability. Finally, we transfer these insights to classical RC by\naugmenting traditional architectures with fractional, generalized reservoir\nstates. This yields performance gains, particularly in resource-constrained\nscenarios such as physical reservoirs, where increasing reservoir size is\nimpractical or economically unviable. Our work provides a principled route\ntoward tailoring RCs to the intrinsic complexity of the systems they aim to\nmodel."}
{"id": "2504.17170", "pdf": "https://arxiv.org/pdf/2504.17170", "abs": "https://arxiv.org/abs/2504.17170", "authors": ["Robert Kaufman"], "title": "Improving Human-Autonomous Vehicle Interaction in Complex Systems", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "PhD Dissertation from University of California, San Diego; 175 pages", "summary": "Unresolved questions about how autonomous vehicles (AVs) should meet the\ninformational needs of riders hinder real-world adoption. Complicating our\nability to satisfy rider needs is that different people, goals, and driving\ncontexts have different criteria for what constitutes interaction success.\nUnfortunately, most human-AV research and design today treats all people and\nsituations uniformly. It is crucial to understand how an AV should communicate\nto meet rider needs, and how communications should change when the human-AV\ncomplex system changes. I argue that understanding the relationships between\ndifferent aspects of the human-AV system can help us build improved and\nadaptable AV communications. I support this argument using three empirical\nstudies. First, I identify optimal communication strategies that enhance\ndriving performance, confidence, and trust for learning in extreme driving\nenvironments. Findings highlight the need for task-sensitive,\nmodality-appropriate communications tuned to learner cognitive limits and\ngoals. Next, I highlight the consequences of deploying faulty communication\nsystems and demonstrate the need for context-sensitive communications. Third, I\nuse machine learning (ML) to illuminate personal factors predicting trust in\nAVs, emphasizing the importance of tailoring designs to individual traits and\nconcerns. Together, this dissertation supports the necessity of transparent,\nadaptable, and personalized AV systems that cater to individual needs, goals,\nand contextual demands. By considering the complex system within which human-AV\ninteractions occur, we can deliver valuable insights for designers,\nresearchers, and policymakers. This dissertation also provides a concrete\ndomain to study theories of human-machine joint action and situational\nawareness, and can be used to guide future human-AI interaction research.\n[shortened for arxiv]"}
{"id": "2504.17520", "pdf": "https://arxiv.org/pdf/2504.17520", "abs": "https://arxiv.org/abs/2504.17520", "authors": ["Zhuojun Tian", "Zhaoyang Zhang", "Yiwei Li", "Mehdi Bennis"], "title": "Communication-Efficient Personalized Distributed Learning with Data and Node Heterogeneity", "categories": ["cs.LG", "cs.DC", "cs.MA"], "comment": "Accepcted by TCCN", "summary": "To jointly tackle the challenges of data and node heterogeneity in\ndecentralized learning, we propose a distributed strong lottery ticket\nhypothesis (DSLTH), based on which a communication-efficient personalized\nlearning algorithm is developed. In the proposed method, each local model is\nrepresented as the Hadamard product of global real-valued parameters and a\npersonalized binary mask for pruning. The local model is learned by updating\nand fusing the personalized binary masks while the real-valued parameters are\nfixed among different agents. To further reduce the complexity of hardware\nimplementation, we incorporate a group sparse regularization term in the loss\nfunction, enabling the learned local model to achieve structured sparsity.\nThen, a binary mask aggregation algorithm is designed by introducing an\nintermediate aggregation tensor and adding a personalized fine-tuning step in\neach iteration, which constrains model updates towards the local data\ndistribution. The proposed method effectively leverages the relativity among\nagents while meeting personalized requirements in heterogeneous node\nconditions. We also provide a theoretical proof for the DSLTH, establishing it\nas the foundation of the proposed method. Numerical simulations confirm the\nvalidity of the DSLTH and demonstrate the effectiveness of the proposed\nalgorithm."}
{"id": "2504.17180", "pdf": "https://arxiv.org/pdf/2504.17180", "abs": "https://arxiv.org/abs/2504.17180", "authors": ["Minkyu Choi", "S P Sharan", "Harsh Goel", "Sahil Shah", "Sandeep Chinchali"], "title": "We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Current text-to-video (T2V) generation models are increasingly popular due to\ntheir ability to produce coherent videos from textual prompts. However, these\nmodels often struggle to generate semantically and temporally consistent videos\nwhen dealing with longer, more complex prompts involving multiple objects or\nsequential events. Additionally, the high computational costs associated with\ntraining or fine-tuning make direct improvements impractical. To overcome these\nlimitations, we introduce \\(\\projectname\\), a novel zero-training video\nrefinement pipeline that leverages neuro-symbolic feedback to automatically\nenhance video generation, achieving superior alignment with the prompts. Our\napproach first derives the neuro-symbolic feedback by analyzing a formal video\nrepresentation and pinpoints semantically inconsistent events, objects, and\ntheir corresponding frames. This feedback then guides targeted edits to the\noriginal video. Extensive empirical evaluations on both open-source and\nproprietary T2V models demonstrate that \\(\\projectname\\) significantly enhances\ntemporal and logical alignment across diverse prompts by almost $40\\%$."}
{"id": "2504.17526", "pdf": "https://arxiv.org/pdf/2504.17526", "abs": "https://arxiv.org/abs/2504.17526", "authors": ["Yuelin Liu", "Haiyuan Li", "Xenofon Vasilakos", "Rasheed Hussain", "Dimitra Simeonidou"], "title": "Cooperative Task Offloading through Asynchronous Deep Reinforcement Learning in Mobile Edge Computing for Future Networks", "categories": ["cs.LG"], "comment": null, "summary": "Future networks (including 6G) are poised to accelerate the realisation of\nInternet of Everything. However, it will result in a high demand for computing\nresources to support new services. Mobile Edge Computing (MEC) is a promising\nsolution, enabling to offload computation-intensive tasks to nearby edge\nservers from the end-user devices, thereby reducing latency and energy\nconsumption. However, relying solely on a single MEC server for task offloading\ncan lead to uneven resource utilisation and suboptimal performance in complex\nscenarios. Additionally, traditional task offloading strategies specialise in\ncentralised policy decisions, which unavoidably entail extreme transmission\nlatency and reach computational bottleneck. To fill the gaps, we propose a\nlatency and energy efficient Cooperative Task Offloading framework with\nTransformer-driven Prediction (CTO-TP), leveraging asynchronous multi-agent\ndeep reinforcement learning to address these challenges. This approach fosters\nedge-edge cooperation and decreases the synchronous waiting time by performing\nasynchronous training, optimising task offloading, and resource allocation\nacross distributed networks. The performance evaluation demonstrates that the\nproposed CTO-TP algorithm reduces up to 80% overall system latency and 87%\nenergy consumption compared to the baseline schemes."}
{"id": "2504.17198", "pdf": "https://arxiv.org/pdf/2504.17198", "abs": "https://arxiv.org/abs/2504.17198", "authors": ["XiangRui Zhang", "HaoYu Chen", "Yongzhong He", "Wenjia Niu", "Qiang Li"], "title": "Automatically Generating Rules of Malicious Software Packages via Large Language Model", "categories": ["cs.SE", "cs.AI", "cs.CR"], "comment": "14 pages, 11 figures", "summary": "Today's security tools predominantly rely on predefined rules crafted by\nexperts, making them poorly adapted to the emergence of software supply chain\nattacks. To tackle this limitation, we propose a novel tool, RuleLLM, which\nleverages large language models (LLMs) to automate rule generation for OSS\necosystems. RuleLLM extracts metadata and code snippets from malware as its\ninput, producing YARA and Semgrep rules that can be directly deployed in\nsoftware development. Specifically, the rule generation task involves three\nsubtasks: crafting rules, refining rules, and aligning rules. To validate\nRuleLLM's effectiveness, we implemented a prototype system and conducted\nexperiments on the dataset of 1,633 malicious packages. The results are\npromising that RuleLLM generated 763 rules (452 YARA and 311 Semgrep) with a\nprecision of 85.2\\% and a recall of 91.8\\%, outperforming state-of-the-art\n(SOTA) tools and scored-based approaches. We further analyzed generated rules\nand proposed a rule taxonomy: 11 categories and 38 subcategories."}
{"id": "2504.17528", "pdf": "https://arxiv.org/pdf/2504.17528", "abs": "https://arxiv.org/abs/2504.17528", "authors": ["Weijie Liu", "Ziwei Zhan", "Carlee Joe-Wong", "Edith Ngai", "Jingpu Duan", "Deke Guo", "Xu Chen", "Xiaoxi Zhang"], "title": "TACO: Tackling Over-correction in Federated Learning with Tailored Adaptive Correction", "categories": ["cs.LG", "cs.AI", "I.2.6"], "comment": "11 pages, 7 figures, accepted by ICDCS 2025", "summary": "Non-independent and identically distributed (Non-IID) data across edge\nclients have long posed significant challenges to federated learning (FL)\ntraining in edge computing environments. Prior works have proposed various\nmethods to mitigate this statistical heterogeneity. While these works can\nachieve good theoretical performance, in this work we provide the first\ninvestigation into a hidden over-correction phenomenon brought by the uniform\nmodel correction coefficients across clients adopted by existing methods. Such\nover-correction could degrade model performance and even cause failures in\nmodel convergence. To address this, we propose TACO, a novel algorithm that\naddresses the non-IID nature of clients' data by implementing fine-grained,\nclient-specific gradient correction and model aggregation, steering local\nmodels towards a more accurate global optimum. Moreover, we verify that leading\nFL algorithms generally have better model accuracy in terms of communication\nrounds rather than wall-clock time, resulting from their extra computation\noverhead imposed on clients. To enhance the training efficiency, TACO deploys a\nlightweight model correction and tailored aggregation approach that requires\nminimum computation overhead and no extra information beyond the synchronized\nmodel parameters. To validate TACO's effectiveness, we present the first FL\nconvergence analysis that reveals the root cause of over-correction. Extensive\nexperiments across various datasets confirm TACO's superior and stable\nperformance in practice."}
{"id": "2504.17210", "pdf": "https://arxiv.org/pdf/2504.17210", "abs": "https://arxiv.org/abs/2504.17210", "authors": ["Junfei Wang", "Darshana Upadhyay", "Marzia Zaman", "Pirathayini Srikantha"], "title": "Synthetic Power Flow Data Generation Using Physics-Informed Denoising Diffusion Probabilistic Models", "categories": ["cs.LG", "cs.AI"], "comment": "Submitted to IEEE SmartGridComm Conference 2025", "summary": "Many data-driven modules in smart grid rely on access to high-quality power\nflow data; however, real-world data are often limited due to privacy and\noperational constraints. This paper presents a physics-informed generative\nframework based on Denoising Diffusion Probabilistic Models (DDPMs) for\nsynthesizing feasible power flow data. By incorporating auxiliary training and\nphysics-informed loss functions, the proposed method ensures that the generated\ndata exhibit both statistical fidelity and adherence to power system\nfeasibility. We evaluate the approach on the IEEE 14-bus and 30-bus benchmark\nsystems, demonstrating its ability to capture key distributional properties and\ngeneralize to out-of-distribution scenarios. Comparative results show that the\nproposed model outperforms three baseline models in terms of feasibility,\ndiversity, and accuracy of statistical features. This work highlights the\npotential of integrating generative modelling into data-driven power system\napplications."}
{"id": "2504.17534", "pdf": "https://arxiv.org/pdf/2504.17534", "abs": "https://arxiv.org/abs/2504.17534", "authors": ["Juan Carlos Climent Pardo"], "title": "Learning Isometric Embeddings of Road Networks using Multidimensional Scaling", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.SC"], "comment": null, "summary": "The lack of generalization in learning-based autonomous driving applications\nis shown by the narrow range of road scenarios that vehicles can currently\ncover. A generalizable approach should capture many distinct road structures\nand topologies, as well as consider traffic participants, and dynamic changes\nin the environment, so that vehicles can navigate and perform motion planning\ntasks even in the most difficult situations. Designing suitable feature spaces\nfor neural network-based motion planers that encapsulate all kinds of road\nscenarios is still an open research challenge. This paper tackles this\nlearning-based generalization challenge and shows how graph representations of\nroad networks can be leveraged by using multidimensional scaling (MDS)\ntechniques in order to obtain such feature spaces. State-of-the-art graph\nrepresentations and MDS approaches are analyzed for the autonomous driving use\ncase. Finally, the option of embedding graph nodes is discussed in order to\nperform easier learning procedures and obtain dimensionality reduction."}
{"id": "2504.17213", "pdf": "https://arxiv.org/pdf/2504.17213", "abs": "https://arxiv.org/abs/2504.17213", "authors": ["Shiwen Cao", "Zhaoxing Zhang", "Junming Jiao", "Juyi Qiao", "Guowen Song", "Rong Shen"], "title": "MCAF: Efficient Agent-based Video Understanding Framework through Multimodal Coarse-to-Fine Attention Focusing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Even in the era of rapid advances in large models, video understanding,\nparticularly long videos, remains highly challenging. Compared with textual or\nimage-based information, videos commonly contain more information with\nredundancy, requiring large models to strategically allocate attention at a\nglobal level for accurate comprehension. To address this, we propose MCAF, an\nagent-based, training-free framework perform video understanding through\nMultimodal Coarse-to-fine Attention Focusing. The key innovation lies in its\nability to sense and prioritize segments of the video that are highly relevant\nto the understanding task. First, MCAF hierarchically concentrates on highly\nrelevant frames through multimodal information, enhancing the correlation\nbetween the acquired contextual information and the query. Second, it employs a\ndilated temporal expansion mechanism to mitigate the risk of missing crucial\ndetails when extracting information from these concentrated frames. In\naddition, our framework incorporates a self-reflection mechanism utilizing the\nconfidence level of the model's responses as feedback. By iteratively applying\nthese two creative focusing strategies, it adaptively adjusts attention to\ncapture highly query-connected context and thus improves response accuracy.\nMCAF outperforms comparable state-of-the-art methods on average. On the\nEgoSchema dataset, it achieves a remarkable 5% performance gain over the\nleading approach. Meanwhile, on Next-QA and IntentQA datasets, it outperforms\nthe current state-of-the-art standard by 0.2% and 0.3% respectively. On the\nVideo-MME dataset, which features videos averaging nearly an hour in length,\nMCAF also outperforms other agent-based methods."}
{"id": "2504.17568", "pdf": "https://arxiv.org/pdf/2504.17568", "abs": "https://arxiv.org/abs/2504.17568", "authors": ["Ivan Rossi", "Flavio Sartori", "Cesare Rollo", "Giovanni Birolo", "Piero Fariselli", "Tiziana Sanavia"], "title": "Beyond Cox Models: Assessing the Performance of Machine-Learning Methods in Non-Proportional Hazards and Non-Linear Survival Analysis", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Survival analysis often relies on Cox models, assuming both linearity and\nproportional hazards (PH). This study evaluates machine and deep learning\nmethods that relax these constraints, comparing their performance with\npenalized Cox models on a benchmark of three synthetic and three real datasets.\nIn total, eight different models were tested, including six non-linear models\nof which four were also non-PH. Although Cox regression often yielded\nsatisfactory performance, we showed the conditions under which machine and deep\nlearning models can perform better. Indeed, the performance of these methods\nhas often been underestimated due to the improper use of Harrell's concordance\nindex (C-index) instead of more appropriate scores such as Antolini's\nconcordance index, which generalizes C-index in cases where the PH assumption\ndoes not hold. In addition, since occasionally high C-index models happen to be\nbadly calibrated, combining Antolini's C-index with Brier's score is useful to\nassess the overall performance of a survival method. Results on our benchmark\ndata showed that survival prediction should be approached by testing different\nmethods to select the most appropriate one according to sample size,\nnon-linearity and non-PH conditions. To allow an easy reproducibility of these\ntests on our benchmark data, code and documentation are freely available at\nhttps://github.com/compbiomed-unito/survhive."}
{"id": "2504.17219", "pdf": "https://arxiv.org/pdf/2504.17219", "abs": "https://arxiv.org/abs/2504.17219", "authors": ["Hyomin Lee", "Minseon Kim", "Sangwon Jang", "Jongheon Jeong", "Sung Ju Hwang"], "title": "Enhancing Variational Autoencoders with Smooth Robust Latent Encoding", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "Under review", "summary": "Variational Autoencoders (VAEs) have played a key role in scaling up\ndiffusion-based generative models, as in Stable Diffusion, yet questions\nregarding their robustness remain largely underexplored. Although adversarial\ntraining has been an established technique for enhancing robustness in\npredictive models, it has been overlooked for generative models due to concerns\nabout potential fidelity degradation by the nature of trade-offs between\nperformance and robustness. In this work, we challenge this presumption,\nintroducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training\nframework that boosts both generation quality and robustness. In contrast to\nconventional adversarial training, which focuses on robustness only, our\napproach smooths the latent space via adversarial perturbations, promoting more\ngeneralizable representations while regularizing with originality\nrepresentation to sustain original fidelity. Applied as a post-training step on\npre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal\ncomputational overhead. Experiments show that SRL-VAE improves both generation\nquality, in image reconstruction and text-guided image editing, and robustness,\nagainst Nightshade attacks and image editing attacks. These results establish a\nnew paradigm, showing that adversarial training, once thought to be detrimental\nto generative models, can instead enhance both fidelity and robustness."}
{"id": "2504.17577", "pdf": "https://arxiv.org/pdf/2504.17577", "abs": "https://arxiv.org/abs/2504.17577", "authors": ["Lei Wang", "Yu Cheng", "Yining Shi", "Zhengju Tang", "Zhiwen Mo", "Wenhao Xie", "Lingxiao Ma", "Yuqing Xia", "Jilong Xue", "Fan Yang", "Zhi Yang"], "title": "TileLang: A Composable Tiled Programming Model for AI Systems", "categories": ["cs.LG"], "comment": null, "summary": "Modern AI workloads rely heavily on optimized computing kernels for both\ntraining and inference. These AI kernels follow well-defined data-flow\npatterns, such as moving tiles between DRAM and SRAM and performing a sequence\nof computations on those tiles. However, writing high-performance kernels\nremains complex despite the clarity of these patterns. Achieving peak\nperformance requires careful, hardware-centric optimizations to fully leverage\nmodern accelerators. While domain-specific compilers attempt to reduce the\nburden of writing high-performance kernels, they often struggle with usability\nand expressiveness gaps. In this paper, we present TileLang, a generalized\ntiled programming model for more efficient AI Kernel programming. TileLang\ndecouples scheduling space (thread binding, layout, tensorize and pipeline)\nfrom dataflow, and encapsulated them as a set of customization annotations and\nprimitives. This approach allows users to focus on the kernel's data-flow\nitself, while leaving most other optimizations to compilers. We conduct\ncomprehensive experiments on commonly-used devices, across numerous\nexperiments, our evaluation shows that TileLang can achieve state-of-the-art\nperformance in key kernels, demonstrating that its unified block-and-thread\nparadigm and transparent scheduling capabilities deliver both the power and\nflexibility demanded by modern AI system development."}
{"id": "2504.17243", "pdf": "https://arxiv.org/pdf/2504.17243", "abs": "https://arxiv.org/abs/2504.17243", "authors": ["Xinyu Zhou", "Simin Fan", "Martin Jaggi", "Jie Fu"], "title": "NeuralGrok: Accelerate Grokking by Neural Gradient Transformation", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint, 16 pages", "summary": "Grokking is proposed and widely studied as an intricate phenomenon in which\ngeneralization is achieved after a long-lasting period of overfitting. In this\nwork, we propose NeuralGrok, a novel gradient-based approach that learns an\noptimal gradient transformation to accelerate the generalization of\ntransformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary\nmodule (e.g., an MLP block) in conjunction with the base model. This module\ndynamically modulates the influence of individual gradient components based on\ntheir contribution to generalization, guided by a bilevel optimization\nalgorithm. Our extensive experiments demonstrate that NeuralGrok significantly\naccelerates generalization, particularly in challenging arithmetic tasks. We\nalso show that NeuralGrok promotes a more stable training paradigm, constantly\nreducing the model's complexity, while traditional regularization methods, such\nas weight decay, can introduce substantial instability and impede\ngeneralization. We further investigate the intrinsic model complexity\nleveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that\nNeuralGrok effectively facilitates generalization by reducing the model\ncomplexity. We offer valuable insights on the grokking phenomenon of\nTransformer models, which encourages a deeper understanding of the fundamental\nprinciples governing generalization ability."}
{"id": "2504.17578", "pdf": "https://arxiv.org/pdf/2504.17578", "abs": "https://arxiv.org/abs/2504.17578", "authors": ["Hongshu Guo", "Wenjie Qiu", "Zeyuan Ma", "Xinglin Zhang", "Jun Zhang", "Yue-Jiao Gong"], "title": "Advancing CMA-ES with Learning-Based Cooperative Coevolution for Scalable Optimization", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Recent research in Cooperative Coevolution~(CC) have achieved promising\nprogress in solving large-scale global optimization problems. However, existing\nCC paradigms have a primary limitation in that they require deep expertise for\nselecting or designing effective variable decomposition strategies. Inspired by\nadvancements in Meta-Black-Box Optimization, this paper introduces LCC, a\npioneering learning-based cooperative coevolution framework that dynamically\nschedules decomposition strategies during optimization processes. The\ndecomposition strategy selector is parameterized through a neural network,\nwhich processes a meticulously crafted set of optimization status features to\ndetermine the optimal strategy for each optimization step. The network is\ntrained via the Proximal Policy Optimization method in a reinforcement learning\nmanner across a collection of representative problems, aiming to maximize the\nexpected optimization performance. Extensive experimental results demonstrate\nthat LCC not only offers certain advantages over state-of-the-art baselines in\nterms of optimization effectiveness and resource consumption, but it also\nexhibits promising transferability towards unseen problems."}
{"id": "2504.17247", "pdf": "https://arxiv.org/pdf/2504.17247", "abs": "https://arxiv.org/abs/2504.17247", "authors": ["Diogo Soares", "Leon Hetzel", "Paulina Szymczak", "Fabian Theis", "Stephan GÃ¼nnemann", "Ewa Szczurek"], "title": "Targeted AMP generation through controlled diffusion with efficient embeddings", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": null, "summary": "Deep learning-based antimicrobial peptide (AMP) discovery faces critical\nchallenges such as low experimental hit rates as well as the need for nuanced\ncontrollability and efficient modeling of peptide properties. To address these\nchallenges, we introduce OmegAMP, a framework that leverages a diffusion-based\ngenerative model with efficient low-dimensional embeddings, precise\ncontrollability mechanisms, and novel classifiers with drastically reduced\nfalse positive rates for candidate filtering. OmegAMP enables the targeted\ngeneration of AMPs with specific physicochemical properties, activity profiles,\nand species-specific effectiveness. Moreover, it maximizes sample diversity\nwhile ensuring faithfulness to the underlying data distribution during\ngeneration. We demonstrate that OmegAMP achieves state-of-the-art performance\nacross all stages of the AMP discovery pipeline, significantly advancing the\npotential of computational frameworks in combating antimicrobial resistance."}
{"id": "2504.17601", "pdf": "https://arxiv.org/pdf/2504.17601", "abs": "https://arxiv.org/abs/2504.17601", "authors": ["Erik Bergh"], "title": "Interpretable non-linear dimensionality reduction using gaussian weighted linear transformation", "categories": ["cs.LG"], "comment": "11 pages, 5 figures", "summary": "Dimensionality reduction techniques are fundamental for analyzing and\nvisualizing high-dimensional data. With established methods like t-SNE and PCA\npresenting a trade-off between representational power and interpretability.\nThis paper introduces a novel approach that bridges this gap by combining the\ninterpretability of linear methods with the expressiveness of non-linear\ntransformations. The proposed algorithm constructs a non-linear mapping between\nhigh-dimensional and low-dimensional spaces through a combination of linear\ntransformations, each weighted by Gaussian functions. This architecture enables\ncomplex non-linear transformations while preserving the interpretability\nadvantages of linear methods, as each transformation can be analyzed\nindependently. The resulting model provides both powerful dimensionality\nreduction and transparent insights into the transformed space. Techniques for\ninterpreting the learned transformations are presented, including methods for\nidentifying suppressed dimensions and how space is expanded and contracted.\nThese tools enable practitioners to understand how the algorithm preserves and\nmodifies geometric relationships during dimensionality reduction. To ensure the\npractical utility of this algorithm, the creation of user-friendly software\npackages is emphasized, facilitating its adoption in both academia and\nindustry."}
{"id": "2504.17255", "pdf": "https://arxiv.org/pdf/2504.17255", "abs": "https://arxiv.org/abs/2504.17255", "authors": ["Shaoyu Pei", "Renxiong Wu", "Hao Zheng", "Lang Qin", "Shuaichen Lin", "Yuxing Gan", "Wenjing Huang", "Zhixuan Wang", "Mohan Qin", "Yong Liu", "Guangming Ni"], "title": "3D Deep-learning-based Segmentation of Human Skin Sweat Glands and Their 3D Morphological Response to Temperature Variations", "categories": ["eess.IV", "cs.AI", "physics.optics"], "comment": null, "summary": "Skin, the primary regulator of heat exchange, relies on sweat glands for\nthermoregulation. Alterations in sweat gland morphology play a crucial role in\nvarious pathological conditions and clinical diagnoses. Current methods for\nobserving sweat gland morphology are limited by their two-dimensional, in\nvitro, and destructive nature, underscoring the urgent need for real-time,\nnon-invasive, quantifiable technologies. We proposed a novel three-dimensional\n(3D) transformer-based multi-object segmentation framework, integrating a\nsliding window approach, joint spatial-channel attention mechanism, and\narchitectural heterogeneity between shallow and deep layers. Our proposed\nnetwork enables precise 3D sweat gland segmentation from skin volume data\ncaptured by optical coherence tomography (OCT). For the first time, subtle\nvariations of sweat gland 3D morphology in response to temperature changes,\nhave been visualized and quantified. Our approach establishes a benchmark for\nnormal sweat gland morphology and provides a real-time, non-invasive tool for\nquantifying 3D structural parameters. This enables the study of individual\nvariability and pathological changes in sweat gland structure, advancing\ndermatological research and clinical applications, including thermoregulation\nand bromhidrosis treatment."}
{"id": "2504.17613", "pdf": "https://arxiv.org/pdf/2504.17613", "abs": "https://arxiv.org/abs/2504.17613", "authors": ["Bowen Deng", "Chang Xu", "Hao Li", "Yuhao Huang", "Min Hou", "Jiang Bian"], "title": "TarDiff: Target-Oriented Diffusion Guidance for Synthetic Electronic Health Record Time Series Generation", "categories": ["cs.LG"], "comment": null, "summary": "Synthetic Electronic Health Record (EHR) time-series generation is crucial\nfor advancing clinical machine learning models, as it helps address data\nscarcity by providing more training data. However, most existing approaches\nfocus primarily on replicating statistical distributions and temporal\ndependencies of real-world data. We argue that fidelity to observed data alone\ndoes not guarantee better model performance, as common patterns may dominate,\nlimiting the representation of rare but important conditions. This highlights\nthe need for generate synthetic samples to improve performance of specific\nclinical models to fulfill their target outcomes. To address this, we propose\nTarDiff, a novel target-oriented diffusion framework that integrates\ntask-specific influence guidance into the synthetic data generation process.\nUnlike conventional approaches that mimic training data distributions, TarDiff\noptimizes synthetic samples by quantifying their expected contribution to\nimproving downstream model performance through influence functions.\nSpecifically, we measure the reduction in task-specific loss induced by\nsynthetic samples and embed this influence gradient into the reverse diffusion\nprocess, thereby steering the generation towards utility-optimized data.\nEvaluated on six publicly available EHR datasets, TarDiff achieves\nstate-of-the-art performance, outperforming existing methods by up to 20.4% in\nAUPRC and 18.4% in AUROC. Our results demonstrate that TarDiff not only\npreserves temporal fidelity but also enhances downstream model performance,\noffering a robust solution to data scarcity and class imbalance in healthcare\nanalytics."}
{"id": "2504.17261", "pdf": "https://arxiv.org/pdf/2504.17261", "abs": "https://arxiv.org/abs/2504.17261", "authors": ["Jiaqi Chen", "Xiaoye Zhu", "Yue Wang", "Tianyang Liu", "Xinhui Chen", "Ying Chen", "Chak Tou Leong", "Yifei Ke", "Joseph Liu", "Yiwen Yuan", "Julian McAuley", "Li-jia Li"], "title": "Symbolic Representation for Any-to-Any Generative Tasks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose a symbolic generative task description language and a\ncorresponding inference engine capable of representing arbitrary multimodal\ntasks as structured symbolic flows. Unlike conventional generative models that\nrely on large-scale training and implicit neural representations to learn\ncross-modal mappings, often at high computational cost and with limited\nflexibility, our framework introduces an explicit symbolic representation\ncomprising three core primitives: functions, parameters, and topological logic.\nLeveraging a pre-trained language model, our inference engine maps natural\nlanguage instructions directly to symbolic workflows in a training-free manner.\nOur framework successfully performs over 12 diverse multimodal generative\ntasks, demonstrating strong performance and flexibility without the need for\ntask-specific tuning. Experiments show that our method not only matches or\noutperforms existing state-of-the-art unified models in content quality, but\nalso offers greater efficiency, editability, and interruptibility. We believe\nthat symbolic task representations provide a cost-effective and extensible\nfoundation for advancing the capabilities of generative AI."}
{"id": "2504.17617", "pdf": "https://arxiv.org/pdf/2504.17617", "abs": "https://arxiv.org/abs/2504.17617", "authors": ["Bruno Casella", "Matthias Jakobs", "Marco Aldinucci", "Sebastian BuschjÃ¤ger"], "title": "Decentralized Time Series Classification with ROCKET Features", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.11; I.2.6"], "comment": "Submitted to Workshop on Federated Learning Advancements 2025, in\n  conjunction with ECML-PKDD, WAFL25", "summary": "Time series classification (TSC) is a critical task with applications in\nvarious domains, including healthcare, finance, and industrial monitoring. Due\nto privacy concerns and data regulations, Federated Learning has emerged as a\npromising approach for learning from distributed time series data without\ncentralizing raw information. However, most FL solutions rely on a\nclient-server architecture, which introduces robustness and confidentiality\nrisks related to the distinguished role of the server, which is a single point\nof failure and can observe knowledge extracted from clients. To address these\nchallenges, we propose DROCKS, a fully decentralized FL framework for TSC that\nleverages ROCKET (RandOm Convolutional KErnel Transform) features. In DROCKS,\nthe global model is trained by sequentially traversing a structured path across\nfederation nodes, where each node refines the model and selects the most\neffective local kernels before passing them to the successor. Extensive\nexperiments on the UCR archive demonstrate that DROCKS outperforms\nstate-of-the-art client-server FL approaches while being more resilient to node\nfailures and malicious attacks. Our code is available at\nhttps://anonymous.4open.science/r/DROCKS-7FF3/README.md."}
{"id": "2504.17264", "pdf": "https://arxiv.org/pdf/2504.17264", "abs": "https://arxiv.org/abs/2504.17264", "authors": ["Zhaolu Kang", "Hongtian Cai", "Xiangyang Ji", "Jinzhe Li", "Nanfei Gu"], "title": "JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "In recent years, Unsupervised Domain Adaptation (UDA) has gained significant\nattention in the field of Natural Language Processing (NLP) owing to its\nability to enhance model generalization across diverse domains. However, its\napplication for knowledge transfer between distinct legal domains remains\nlargely unexplored. To address the challenges posed by lengthy and complex\nlegal texts and the limited availability of large-scale annotated datasets, we\npropose JurisCTC, a novel model designed to improve the accuracy of Legal\nJudgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC\nfacilitates effective knowledge transfer across various legal domains and\nemploys contrastive learning to distinguish samples from different domains.\nSpecifically, for the LJP task, we enable knowledge transfer between civil and\ncriminal law domains. Compared to other models and specific large language\nmodels (LLMs), JurisCTC demonstrates notable advancements, achieving peak\naccuracies of 76.59% and 78.83%, respectively."}
{"id": "2504.17618", "pdf": "https://arxiv.org/pdf/2504.17618", "abs": "https://arxiv.org/abs/2504.17618", "authors": ["Nikita Gabdullin"], "title": "The effects of Hessian eigenvalue spectral density type on the applicability of Hessian analysis to generalization capability assessment of neural networks", "categories": ["cs.LG", "cs.CV"], "comment": "11 pages, 10 figures, 4 tables, 4 equations", "summary": "Hessians of neural network (NN) contain essential information about the\ncurvature of NN loss landscapes which can be used to estimate NN generalization\ncapabilities. We have previously proposed generalization criteria that rely on\nthe observation that Hessian eigenvalue spectral density (HESD) behaves\nsimilarly for a wide class of NNs. This paper further studies their\napplicability by investigating factors that can result in different types of\nHESD. We conduct a wide range of experiments showing that HESD mainly has\npositive eigenvalues (MP-HESD) for NN training and fine-tuning with various\noptimizers on different datasets with different preprocessing and augmentation\nprocedures. We also show that mainly negative HESD (MN-HESD) is a consequence\nof external gradient manipulation, indicating that the previously proposed\nHessian analysis methodology cannot be applied in such cases. We also propose\ncriteria and corresponding conditions to determine HESD type and estimate NN\ngeneralization potential. These HESD types and previously proposed\ngeneralization criteria are combined into a unified HESD analysis methodology.\nFinally, we discuss how HESD changes during training, and show the occurrence\nof quasi-singular (QS) HESD and its influence on the proposed methodology and\non the conventional assumptions about the relation between Hessian eigenvalues\nand NN loss landscape curvature."}
{"id": "2504.17277", "pdf": "https://arxiv.org/pdf/2504.17277", "abs": "https://arxiv.org/abs/2504.17277", "authors": ["Zongliang Ji", "Andre Carlos Kajdacsy-Balla Amaral", "Anna Goldenberg", "Rahul G. Krishnan"], "title": "ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to the Conference on Health, Inference, and Learning (CHIL)\n  2025", "summary": "Ordering a minimal subset of lab tests for patients in the intensive care\nunit (ICU) can be challenging. Care teams must balance between ensuring the\navailability of the right information and reducing the clinical burden and\ncosts associated with each lab test order. Most in-patient settings experience\nfrequent over-ordering of lab tests, but are now aiming to reduce this burden\non both hospital resources and the environment. This paper develops a novel\nmethod that combines off-policy learning with privileged information to\nidentify the optimal set of ICU lab tests to order. Our approach, EXplainable\nOff-policy learning with Side Information for ICU blood Test Orders (ExOSITO)\ncreates an interpretable assistive tool for clinicians to order lab tests by\nconsidering both the observed and predicted future status of each patient. We\npose this problem as a causal bandit trained using offline data and a reward\nfunction derived from clinically-approved rules; we introduce a novel learning\nframework that integrates clinical knowledge with observational data to bridge\nthe gap between the optimal and logging policies. The learned policy function\nprovides interpretable clinical information and reduces costs without omitting\nany vital lab orders, outperforming both a physician's policy and prior\napproaches to this practical problem."}
{"id": "2504.17641", "pdf": "https://arxiv.org/pdf/2504.17641", "abs": "https://arxiv.org/abs/2504.17641", "authors": ["Shengtao Zhang", "Haokai Zhang", "Shiqi Lou", "Zicheng Wang", "Zinan Zeng", "Yilin Wang", "Minnan Luo"], "title": "PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Dynamic node classification is critical for modeling evolving systems like\nfinancial transactions and academic collaborations. In such systems,\ndynamically capturing node information changes is critical for dynamic node\nclassification, which usually requires all labels at every timestamp. However,\nit is difficult to collect all dynamic labels in real-world scenarios due to\nhigh annotation costs and label uncertainty (e.g., ambiguous or delayed labels\nin fraud detection). In contrast, final timestamp labels are easier to obtain\nas they rely on complete temporal patterns and are usually maintained as a\nunique label for each user in many open platforms, without tracking the history\ndata. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum\nLearning), a pioneering method addressing label-limited dynamic node\nclassification where only final labels are available. PTCL introduces: (1) a\ntemporal decoupling architecture separating the backbone (learning time-aware\nrepresentations) and decoder (strictly aligned with final labels), which\ngenerate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that\nprioritizes pseudo-labels closer to the final timestamp by assigning them\nhigher weights using an exponentially decaying function. We contribute a new\nacademic dataset (CoOAG), capturing long-range research interest in dynamic\ngraph. Experiments across real-world scenarios demonstrate PTCL's consistent\nsuperiority over other methods adapted to this task. Beyond methodology, we\npropose a unified framework FLiD (Framework for Label-Limited Dynamic Node\nClassification), consisting of a complete preparation workflow, training\npipeline, and evaluation standards, and supporting various models and datasets.\nThe code can be found at https://github.com/3205914485/FLiD."}
{"id": "2504.17304", "pdf": "https://arxiv.org/pdf/2504.17304", "abs": "https://arxiv.org/abs/2504.17304", "authors": ["Yimin Shi", "Yang Fei", "Shiqi Zhang", "Haixun Wang", "Xiaokui Xiao"], "title": "You Are What You Bought: Generating Customer Personas for E-commerce Applications", "categories": ["cs.IR", "cs.AI"], "comment": "SIGIR 2025", "summary": "In e-commerce, user representations are essential for various applications.\nExisting methods often use deep learning techniques to convert customer\nbehaviors into implicit embeddings. However, these embeddings are difficult to\nunderstand and integrate with external knowledge, limiting the effectiveness of\napplications such as customer segmentation, search navigation, and product\nrecommendations. To address this, our paper introduces the concept of the\ncustomer persona. Condensed from a customer's numerous purchasing histories, a\ncustomer persona provides a multi-faceted and human-readable characterization\nof specific purchase behaviors and preferences, such as Busy Parents or Bargain\nHunters.\n  This work then focuses on representing each customer by multiple personas\nfrom a predefined set, achieving readable and informative explicit user\nrepresentations. To this end, we propose an effective and efficient solution\nGPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs to infer\npersonas for customers. To reduce overhead, GPLR applies LLM-based labeling to\nonly a fraction of users and utilizes a random walk technique to predict\npersonas for the remaining customers. We further propose RevAff, which provides\nan absolute error $\\epsilon$ guarantee while improving the time complexity of\nthe exact solution by a factor of at least\n$O(\\frac{\\epsilon\\cdot|E|N}{|E|+N\\log N})$, where $N$ represents the number of\ncustomers and products, and $E$ represents the interactions between them. We\nevaluate the performance of our persona-based representation in terms of\naccuracy and robustness for recommendation and customer segmentation tasks\nusing three real-world e-commerce datasets. Most notably, we find that\nintegrating customer persona representations improves the state-of-the-art\ngraph convolution-based recommendation model by up to 12% in terms of NDCG@K\nand F1-Score@K."}
{"id": "2504.17655", "pdf": "https://arxiv.org/pdf/2504.17655", "abs": "https://arxiv.org/abs/2504.17655", "authors": ["Farhad Pourkamali-Anaraki"], "title": "Aerial Image Classification in Scarce and Unconstrained Environments via Conformal Prediction", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "17 pages, 5 figures, and 2 tables", "summary": "This paper presents a comprehensive empirical analysis of conformal\nprediction methods on a challenging aerial image dataset featuring diverse\nevents in unconstrained environments. Conformal prediction is a powerful\npost-hoc technique that takes the output of any classifier and transforms it\ninto a set of likely labels, providing a statistical guarantee on the coverage\nof the true label. Unlike evaluations on standard benchmarks, our study\naddresses the complexities of data-scarce and highly variable real-world\nsettings. We investigate the effectiveness of leveraging pretrained models\n(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to\ngenerate informative prediction sets. To further evaluate the impact of\ncalibration, we consider two parallel pipelines (with and without temperature\nscaling) and assess performance using two key metrics: empirical coverage and\naverage prediction set size. This setup allows us to systematically examine how\ncalibration choices influence the trade-off between reliability and efficiency.\nOur findings demonstrate that even with relatively small labeled samples and\nsimple nonconformity scores, conformal prediction can yield valuable\nuncertainty estimates for complex tasks. Moreover, our analysis reveals that\nwhile temperature scaling is often employed for calibration, it does not\nconsistently lead to smaller prediction sets, underscoring the importance of\ncareful consideration in its application. Furthermore, our results highlight\nthe significant potential of model compression techniques within the conformal\nprediction pipeline for deployment in resource-constrained environments. Based\non our observations, we advocate for future research to delve into the impact\nof noisy or ambiguous labels on conformal prediction performance and to explore\neffective model reduction strategies."}
{"id": "2504.17306", "pdf": "https://arxiv.org/pdf/2504.17306", "abs": "https://arxiv.org/abs/2504.17306", "authors": ["Meher Boulaabi", "Takwa Ben AÃ¯cha Gader", "Afef Kacem Echi", "Sameh Mbarek"], "title": "Advanced Segmentation of Diabetic Retinopathy Lesions Using DeepLabv3+", "categories": ["cs.CV", "cs.AI"], "comment": "This work was accepted at the ACS/IEEE International Conference on\n  Computer Systems and Applications (AICCSA) 2024", "summary": "To improve the segmentation of diabetic retinopathy lesions (microaneurysms,\nhemorrhages, exudates, and soft exudates), we implemented a binary segmentation\nmethod specific to each type of lesion. As post-segmentation, we combined the\nindividual model outputs into a single image to better analyze the lesion\ntypes. This approach facilitated parameter optimization and improved accuracy,\neffectively overcoming challenges related to dataset limitations and annotation\ncomplexity. Specific preprocessing steps included cropping and applying\ncontrast-limited adaptive histogram equalization to the L channel of the LAB\nimage. Additionally, we employed targeted data augmentation techniques to\nfurther refine the model's efficacy. Our methodology utilized the DeepLabv3+\nmodel, achieving a segmentation accuracy of 99%. These findings highlight the\nefficacy of innovative strategies in advancing medical image analysis,\nparticularly in the precise segmentation of diabetic retinopathy lesions. The\nIDRID dataset was utilized to validate and demonstrate the robustness of our\napproach."}
{"id": "2504.17660", "pdf": "https://arxiv.org/pdf/2504.17660", "abs": "https://arxiv.org/abs/2504.17660", "authors": ["Julius Vetter", "Manuel Gloeckler", "Daniel Gedon", "Jakob H. Macke"], "title": "Effortless, Simulation-Efficient Bayesian Inference using Tabular Foundation Models", "categories": ["cs.LG"], "comment": null, "summary": "Simulation-based inference (SBI) offers a flexible and general approach to\nperforming Bayesian inference: In SBI, a neural network is trained on synthetic\ndata simulated from a model and used to rapidly infer posterior distributions\nfor observed data. A key goal for SBI is to achieve accurate inference with as\nfew simulations as possible, especially for expensive simulators. In this work,\nwe address this challenge by repurposing recent probabilistic foundation models\nfor tabular data: We show how tabular foundation models -- specifically TabPFN\n-- can be used as pre-trained autoregressive conditional density estimators for\nSBI. We propose Neural Posterior Estimation with Prior-data Fitted Networks\n(NPE-PF) and show that it is competitive with current SBI approaches in terms\nof accuracy for both benchmark tasks and two complex scientific inverse\nproblems. Crucially, it often substantially outperforms them in terms of\nsimulation efficiency, sometimes requiring orders of magnitude fewer\nsimulations. NPE-PF eliminates the need for inference network selection,\ntraining, and hyperparameter tuning. We also show that it exhibits superior\nrobustness to model misspecification and can be scaled to simulation budgets\nthat exceed the context size limit of TabPFN. NPE-PF provides a new direction\nfor SBI, where training-free, general-purpose inference models offer efficient,\neasy-to-use, and flexible solutions for a wide range of stochastic inverse\nproblems."}
{"id": "2504.17311", "pdf": "https://arxiv.org/pdf/2504.17311", "abs": "https://arxiv.org/abs/2504.17311", "authors": ["Yulia Otmakhova", "Hung Thinh Truong", "Rahmad Mahendra", "Zenan Zhai", "Rongxin Zhu", "Daniel Beck", "Jey Han Lau"], "title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors."}
{"id": "2504.17664", "pdf": "https://arxiv.org/pdf/2504.17664", "abs": "https://arxiv.org/abs/2504.17664", "authors": ["GrÃ©gory Bournassenko"], "title": "On Multivariate Financial Time Series Classification", "categories": ["cs.LG"], "comment": null, "summary": "This article investigates the use of Machine Learning and Deep Learning\nmodels in multivariate time series analysis within financial markets. It\ncompares small and big data approaches, focusing on their distinct challenges\nand the benefits of scaling. Traditional methods such as SVMs are contrasted\nwith modern architectures like ConvTimeNet. The results show the importance of\nusing and understanding Big Data in depth in the analysis and prediction of\nfinancial time series."}
{"id": "2504.17315", "pdf": "https://arxiv.org/pdf/2504.17315", "abs": "https://arxiv.org/abs/2504.17315", "authors": ["Zhanglin Wu", "Tengfei Song", "Ning Xie", "Weidong Zhang", "Pengfei Li", "Shuang Wu", "Chong Li", "Junhao Zhu", "Hao Yang"], "title": "DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model", "categories": ["cs.CV", "cs.AI"], "comment": "7 pages, 1 figures, 2 tables", "summary": "This paper presents the technical solution proposed by Huawei Translation\nService Center (HW-TSC) for the \"End-to-End Document Image Machine Translation\nfor Complex Layouts\" competition at the 19th International Conference on\nDocument Analysis and Recognition (DIMT25@ICDAR2025). Leveraging\nstate-of-the-art open-source large vision-language model (LVLM), we introduce a\ntraining framework that combines multi-task learning with perceptual\nchain-of-thought to develop a comprehensive end-to-end document translation\nsystem. During the inference phase, we apply minimum Bayesian decoding and\npost-processing strategies to further enhance the system's translation\ncapabilities. Our solution uniquely addresses both OCR-based and OCR-free\ndocument image translation tasks within a unified framework. This paper\nsystematically details the training methods, inference strategies, LVLM base\nmodels, training data, experimental setups, and results, demonstrating an\neffective approach to document image machine translation."}
{"id": "2504.17703", "pdf": "https://arxiv.org/pdf/2504.17703", "abs": "https://arxiv.org/abs/2504.17703", "authors": ["Edward Collins", "Michel Wang"], "title": "Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) has emerged as a transformative paradigm in the field\nof distributed machine learning, enabling multiple clients such as mobile\ndevices, edge nodes, or organizations to collaboratively train a shared global\nmodel without the need to centralize sensitive data. This decentralized\napproach addresses growing concerns around data privacy, security, and\nregulatory compliance, making it particularly attractive in domains such as\nhealthcare, finance, and smart IoT systems. This survey provides a concise yet\ncomprehensive overview of Federated Learning, beginning with its core\narchitecture and communication protocol. We discuss the standard FL lifecycle,\nincluding local training, model aggregation, and global updates. A particular\nemphasis is placed on key technical challenges such as handling non-IID\n(non-independent and identically distributed) data, mitigating system and\nhardware heterogeneity, reducing communication overhead, and ensuring privacy\nthrough mechanisms like differential privacy and secure aggregation.\nFurthermore, we examine emerging trends in FL research, including personalized\nFL, cross-device versus cross-silo settings, and integration with other\nparadigms such as reinforcement learning and quantum computing. We also\nhighlight real-world applications and summarize benchmark datasets and\nevaluation metrics commonly used in FL research. Finally, we outline open\nresearch problems and future directions to guide the development of scalable,\nefficient, and trustworthy FL systems."}
{"id": "2504.17331", "pdf": "https://arxiv.org/pdf/2504.17331", "abs": "https://arxiv.org/abs/2504.17331", "authors": ["SÃ¼leyman Ãzdel", "Kadir Burak Buldu", "Enkelejda Kasneci", "Efe Bozkir"], "title": "Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality", "categories": ["cs.HC", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Locomotion plays a crucial role in shaping the user experience within virtual\nreality environments. In particular, hands-free locomotion offers a valuable\nalternative by supporting accessibility and freeing users from reliance on\nhandheld controllers. To this end, traditional speech-based methods often\ndepend on rigid command sets, limiting the naturalness and flexibility of\ninteraction. In this study, we propose a novel locomotion technique powered by\nlarge language models (LLMs), which allows users to navigate virtual\nenvironments using natural language with contextual awareness. We evaluate\nthree locomotion methods: controller-based teleportation, voice-based steering,\nand our language model-driven approach. Our evaluation measures include\neye-tracking data analysis, including explainable machine learning through SHAP\nanalysis as well as standardized questionnaires for usability, presence,\ncybersickness, and cognitive load to examine user attention and engagement. Our\nfindings indicate that the LLM-driven locomotion possesses comparable\nusability, presence, and cybersickness scores to established methods like\nteleportation, demonstrating its novel potential as a comfortable, natural\nlanguage-based, hands-free alternative. In addition, it enhances user attention\nwithin the virtual environment, suggesting greater engagement. Complementary to\nthese findings, SHAP analysis revealed that fixation, saccade, and\npupil-related features vary across techniques, indicating distinct patterns of\nvisual attention and cognitive processing. Overall, we state that our method\ncan facilitate hands-free locomotion in virtual spaces, especially in\nsupporting accessibility."}
{"id": "2504.17709", "pdf": "https://arxiv.org/pdf/2504.17709", "abs": "https://arxiv.org/abs/2504.17709", "authors": ["Stefan Jonas", "Angela Meyer"], "title": "Fault Diagnosis in New Wind Turbines using Knowledge from Existing Turbines by Generative Domain Adaptation", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Intelligent condition monitoring of wind turbines is essential for reducing\ndowntimes. Machine learning models trained on wind turbine operation data are\ncommonly used to detect anomalies and, eventually, operation faults. However,\ndata-driven normal behavior models (NBMs) require a substantial amount of\ntraining data, as NBMs trained with scarce data may result in unreliable fault\ndiagnosis. To overcome this limitation, we present a novel generative deep\nlearning approach to make SCADA samples from one wind turbine lacking training\ndata resemble SCADA data from wind turbines with representative training data.\nThrough CycleGAN-based domain mapping, our method enables the application of an\nNBM trained on an existing wind turbine to one with severely limited data. We\ndemonstrate our approach on field data mapping SCADA samples across 7\nsubstantially different WTs. Our findings show significantly improved fault\ndiagnosis in wind turbines with scarce data. Our method achieves the most\nsimilar anomaly scores to an NBM trained with abundant data, outperforming NBMs\ntrained on scarce training data with improvements of +10.3% in F1-score when 1\nmonth of training data is available and +16.8% when 2 weeks are available. The\ndomain mapping approach outperforms conventional fine-tuning at all considered\ndegrees of data scarcity, ranging from 1 to 8 weeks of training data. The\nproposed technique enables earlier and more reliable fault diagnosis in newly\ninstalled wind farms, demonstrating a novel and promising research direction to\nimprove anomaly detection when faced with training data scarcity."}
{"id": "2504.17346", "pdf": "https://arxiv.org/pdf/2504.17346", "abs": "https://arxiv.org/abs/2504.17346", "authors": ["Tran Thuy Nga Truong", "Jooyong Kim"], "title": "Dual-Individual Genetic Algorithm: A Dual-Individual Approach for Efficient Training of Multi-Layer Neural Networks", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "This paper introduces an enhanced Genetic Algorithm technique called\nDual-Individual Genetic Algorithm (Dual-Individual GA), which optimizes neural\nnetworks for binary image classification tasks, such as cat vs. non-cat\nclassification. The proposed method employs only two individuals for crossover,\nrepresented by two parameter sets: Leader and Follower. The Leader focuses on\nexploitation, representing the primary optimal solution at even-indexed\npositions (0, 2, 4, ...), while the Follower promotes exploration by preserving\ndiversity and avoiding premature convergence, operating at odd-indexed\npositions (1, 3, 5, ...). Leader and Follower are modeled as two phases or\nroles. The key contributions of this work are threefold: (1) a self-adaptive\nlayer dimension mechanism that eliminates the need for manual tuning of layer\narchitectures; (2) generates two parameter sets, leader and follower parameter\nsets, with 10 layer architecture configurations (5 for each set), ranked by\nPareto dominance and cost. post-optimization; and (3) demonstrated superior\nperformance compared to traditional gradient-based methods. Experimental\nresults show that the Dual-Individual GA achieves 99.04% training accuracy and\n80% testing accuracy (cost = 0.034) on a three-layer network with architecture\n[12288, 17, 4, 1], outperforming a gradient-based approach that achieves 98%\ntraining accuracy and 80% testing accuracy (cost = 0.092) on a four-layer\nnetwork with architecture [12288, 20, 7, 5, 1]. These findings highlight the\nefficiency and effectiveness of the proposed method in optimizing neural\nnetworks."}
{"id": "2504.17717", "pdf": "https://arxiv.org/pdf/2504.17717", "abs": "https://arxiv.org/abs/2504.17717", "authors": ["Ãscar Escudero-Arnanz", "Antonio G. Marques", "Inmaculada Mora-JimÃ©nez", "JoaquÃ­n Ãlvarez-RodrÃ­guez", "Cristina Soguero-Ruiz"], "title": "Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis and Interpretable Patient-Similarity Representations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Background and Objectives: Multidrug Resistance (MDR) is a critical global\nhealth issue, causing increased hospital stays, healthcare costs, and\nmortality. This study proposes an interpretable Machine Learning (ML) framework\nfor MDR prediction, aiming for both accurate inference and enhanced\nexplainability.\n  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing\nclinical progression and patient-to-patient interactions. Similarity among\npatients is quantified using MTS-based methods: descriptive statistics, Dynamic\nTime Warping, and Time Cluster Kernel. These similarity measures serve as\ninputs for MDR classification via Logistic Regression, Random Forest, and\nSupport Vector Machines, with dimensionality reduction and kernel\ntransformations improving model performance. For explainability, patient\nsimilarity networks are constructed from these metrics. Spectral clustering and\nt-SNE are applied to identify MDR-related subgroups and visualize high-risk\nclusters, enabling insight into clinically relevant patterns.\n  Results: The framework was validated on ICU Electronic Health Records from\nthe University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms\nbaseline ML and deep learning models by leveraging graph-based patient\nsimilarity. The approach identifies key risk factors -- prolonged antibiotic\nuse, invasive procedures, co-infections, and extended ICU stays -- and reveals\nclinically meaningful clusters. Code and results are available at\n\\https://github.com/oscarescuderoarnanz/DM4MTS.\n  Conclusions: Patient similarity representations combined with graph-based\nanalysis provide accurate MDR prediction and interpretable insights. This\nmethod supports early detection, risk factor identification, and patient\nstratification, highlighting the potential of explainable ML in critical care."}
{"id": "2504.17354", "pdf": "https://arxiv.org/pdf/2504.17354", "abs": "https://arxiv.org/abs/2504.17354", "authors": ["Tarik Sahin", "Jacopo Bonari", "Sebastian Brandstaeter", "Alexander Popp"], "title": "Data-Driven Surrogate Modeling Techniques to Predict the Effective Contact Area of Rough Surface Contact Problems", "categories": ["cs.CE", "cs.AI"], "comment": null, "summary": "The effective contact area in rough surface contact plays a critical role in\nmulti-physics phenomena such as wear, sealing, and thermal or electrical\nconduction. Although accurate numerical methods, like the Boundary Element\nMethod (BEM), are available to compute this quantity, their high computational\ncost limits their applicability in multi-query contexts, such as uncertainty\nquantification, parameter identification, and multi-scale algorithms, where\nmany repeated evaluations are required. This study proposes a surrogate\nmodeling framework for predicting the effective contact area using\nfast-to-evaluate data-driven techniques. Various machine learning algorithms\nare trained on a precomputed dataset, where the inputs are the imposed load and\nstatistical roughness parameters, and the output is the corresponding effective\ncontact area. All models undergo hyperparameter optimization to enable fair\ncomparisons in terms of predictive accuracy and computational efficiency,\nevaluated using established quantitative metrics. Among the models, the Kernel\nRidge Regressor demonstrates the best trade-off between accuracy and\nefficiency, achieving high predictive accuracy, low prediction time, and\nminimal training overhead-making it a strong candidate for general-purpose\nsurrogate modeling. The Gaussian Process Regressor provides an attractive\nalternative when uncertainty quantification is required, although it incurs\nadditional computational cost due to variance estimation. The generalization\ncapability of the Kernel Ridge model is validated on an unseen simulation\nscenario, confirming its ability to transfer to new configurations. Database\ngeneration constitutes the dominant cost in the surrogate modeling process.\nNevertheless, the approach proves practical and efficient for multi-query\ntasks, even when accounting for this initial expense."}
{"id": "2504.17721", "pdf": "https://arxiv.org/pdf/2504.17721", "abs": "https://arxiv.org/abs/2504.17721", "authors": ["Cheng Shen", "Yuewei Liu"], "title": "Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees", "categories": ["cs.LG", "cs.AI"], "comment": "Under Review", "summary": "In industrial settings, surface defects on steel can significantly compromise\nits service life and elevate potential safety risks. Traditional defect\ndetection methods predominantly rely on manual inspection, which suffers from\nlow efficiency and high costs. Although automated defect detection approaches\nbased on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly,\ntheir reliability remains challenged due to data annotation uncertainties\nduring deep model training and overfitting issues. These limitations may lead\nto detection deviations when processing the given new test samples, rendering\nautomated detection processes unreliable. To address this challenge, we first\nevaluate the detection model's practical performance through calibration data\nthat satisfies the independent and identically distributed (i.i.d) condition\nwith test data. Specifically, we define a loss function for each calibration\nsample to quantify detection error rates, such as the complement of recall rate\nand false discovery rate. Subsequently, we derive a statistically rigorous\nthreshold based on a user-defined risk level to identify high-probability\ndefective pixels in test images, thereby constructing prediction sets (e.g.,\ndefect regions). This methodology ensures that the expected error rate (mean\nerror rate) on the test set remains strictly bounced by the predefined risk\nlevel. Additionally, we observe a negative correlation between the average\nprediction set size and the risk level on the test set, establishing a\nstatistically rigorous metric for assessing detection model uncertainty.\nFurthermore, our study demonstrates robust and efficient control over the\nexpected test set error rate across varying calibration-to-test partitioning\nratios, validating the method's adaptability and operational effectiveness."}
{"id": "2504.17355", "pdf": "https://arxiv.org/pdf/2504.17355", "abs": "https://arxiv.org/abs/2504.17355", "authors": ["Xiaohan Huang", "Dongjie Wang", "Zhiyuan Ning", "Ziyue Qiao", "Qingqing Long", "Haowei Zhu", "Yi Du", "Min Wu", "Yuanchun Zhou", "Meng Xiao"], "title": "Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, Keywords: Automated Feature Transformation, Tabular\n  Dataset, Reinforcement Learning", "summary": "Feature transformation methods aim to find an optimal mathematical\nfeature-feature crossing process that generates high-value features and\nimproves the performance of downstream machine learning tasks. Existing\nframeworks, though designed to mitigate manual costs, often treat feature\ntransformations as isolated operations, ignoring dynamic dependencies between\ntransformation steps. To address the limitations, we propose TCTO, a\ncollaborative multi-agent reinforcement learning framework that automates\nfeature engineering through graph-driven path optimization. The framework's\ncore innovation lies in an evolving interaction graph that models features as\nnodes and transformations as edges. Through graph pruning and backtracking, it\ndynamically eliminates low-impact edges, reduces redundant operations, and\nenhances exploration stability. This graph also provides full traceability to\nempower TCTO to reuse high-utility subgraphs from historical transformations.\nTo demonstrate the efficacy and adaptability of our approach, we conduct\ncomprehensive experiments and case studies, which show superior performance\nacross a range of datasets."}
{"id": "2504.17723", "pdf": "https://arxiv.org/pdf/2504.17723", "abs": "https://arxiv.org/abs/2504.17723", "authors": ["Natan Levy", "Adiel Ashrov", "Guy Katz"], "title": "Towards Robust LLMs: an Adversarial Robustness Measurement Framework", "categories": ["cs.LG"], "comment": "17 pages, 5 figures", "summary": "The rise of Large Language Models (LLMs) has revolutionized artificial\nintelligence, yet these models remain vulnerable to adversarial perturbations,\nundermining their reliability in high-stakes applications. While adversarial\nrobustness in vision-based neural networks has been extensively studied, LLM\nrobustness remains under-explored. We adapt the Robustness Measurement and\nAssessment (RoMA) framework to quantify LLM resilience against adversarial\ninputs without requiring access to model parameters. By comparing RoMA's\nestimates to those of formal verification methods, we demonstrate its accuracy\nwith minimal error margins while maintaining computational efficiency. Our\nempirical evaluation reveals that robustness varies significantly not only\nbetween different models but also across categories within the same task and\nbetween various types of perturbations. This non-uniformity underscores the\nneed for task-specific robustness evaluations, enabling practitioners to\ncompare and select models based on application-specific robustness\nrequirements. Our work provides a systematic methodology to assess LLM\nrobustness, advancing the development of more reliable language models for\nreal-world deployment."}
{"id": "2504.17366", "pdf": "https://arxiv.org/pdf/2504.17366", "abs": "https://arxiv.org/abs/2504.17366", "authors": ["Yongxuan Wu", "Runyu Chen", "Peiyu Liu", "Hongjin Qian"], "title": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench."}
{"id": "2504.17739", "pdf": "https://arxiv.org/pdf/2504.17739", "abs": "https://arxiv.org/abs/2504.17739", "authors": ["Lorenzo Simone", "Mauro Giuseppe Camporeale", "Vito Marco Rubino", "Vincenzo Gervasi", "Giovanni Dimauro"], "title": "Interpretable Early Detection of Parkinson's Disease through Speech Analysis", "categories": ["cs.LG"], "comment": null, "summary": "Parkinson's disease is a progressive neurodegenerative disorder affecting\nmotor and non-motor functions, with speech impairments among its earliest\nsymptoms. Speech impairments offer a valuable diagnostic opportunity, with\nmachine learning advances providing promising tools for timely detection. In\nthis research, we propose a deep learning approach for early Parkinson's\ndisease detection from speech recordings, which also highlights the vocal\nsegments driving predictions to enhance interpretability. This approach seeks\nto associate predictive speech patterns with articulatory features, providing a\nbasis for interpreting underlying neuromuscular impairments. We evaluated our\napproach using the Italian Parkinson's Voice and Speech Database, containing\n831 audio recordings from 65 participants, including both healthy individuals\nand patients. Our approach showed competitive classification performance\ncompared to state-of-the-art methods, while providing enhanced interpretability\nby identifying key speech features influencing predictions."}
{"id": "2504.17384", "pdf": "https://arxiv.org/pdf/2504.17384", "abs": "https://arxiv.org/abs/2504.17384", "authors": ["Hanlin Sheng", "Xinming Wu", "Hang Gao", "Haibin Di", "Sergey Fomel", "Jintao Li", "Xu Si"], "title": "On the workflow, opportunities and challenges of developing foundation model in geophysics", "categories": ["physics.geo-ph", "cs.AI"], "comment": null, "summary": "Foundation models, as a mainstream technology in artificial intelligence,\nhave demonstrated immense potential across various domains in recent years,\nparticularly in handling complex tasks and multimodal data. In the field of\ngeophysics, although the application of foundation models is gradually\nexpanding, there is currently a lack of comprehensive reviews discussing the\nfull workflow of integrating foundation models with geophysical data. To\naddress this gap, this paper presents a complete framework that systematically\nexplores the entire process of developing foundation models in conjunction with\ngeophysical data. From data collection and preprocessing to model architecture\nselection, pre-training strategies, and model deployment, we provide a detailed\nanalysis of the key techniques and methodologies at each stage. In particular,\nconsidering the diversity, complexity, and physical consistency constraints of\ngeophysical data, we discuss targeted solutions to address these challenges.\nFurthermore, we discuss how to leverage the transfer learning capabilities of\nfoundation models to reduce reliance on labeled data, enhance computational\nefficiency, and incorporate physical constraints into model training, thereby\nimproving physical consistency and interpretability. Through a comprehensive\nsummary and analysis of the current technological landscape, this paper not\nonly fills the gap in the geophysics domain regarding a full-process review of\nfoundation models but also offers valuable practical guidance for their\napplication in geophysical data analysis, driving innovation and advancement in\nthe field."}
{"id": "2504.17740", "pdf": "https://arxiv.org/pdf/2504.17740", "abs": "https://arxiv.org/abs/2504.17740", "authors": ["Mingchen Jiang", "Peng Xu", "Xichen Ye", "Xiaohui Chen", "Yun Yang", "Yifan Chen"], "title": "Embedding Empirical Distributions for Computing Optimal Transport Maps", "categories": ["cs.LG"], "comment": null, "summary": "Distributional data have become increasingly prominent in modern signal\nprocessing, highlighting the necessity of computing optimal transport (OT) maps\nacross multiple probability distributions. Nevertheless, recent studies on\nneural OT methods predominantly focused on the efficient computation of a\nsingle map between two distributions. To address this challenge, we introduce a\nnovel approach to learning transport maps for new empirical distributions.\nSpecifically, we employ the transformer architecture to produce embeddings from\ndistributional data of varying length; these embeddings are then fed into a\nhypernetwork to generate neural OT maps. Various numerical experiments were\nconducted to validate the embeddings and the generated OT maps. The model\nimplementation and the code are provided on\nhttps://github.com/jiangmingchen/HOTET."}
{"id": "2504.17393", "pdf": "https://arxiv.org/pdf/2504.17393", "abs": "https://arxiv.org/abs/2504.17393", "authors": ["Vesna Nowack", "Dalal Alrajeh", "Carolina Gutierrez MuÃ±oz", "Katie Thomas", "William Hobson", "Catherine Hamilton-Giachritsis", "Patrick Benjamin", "Tim Grant", "Juliane A. Kloess", "Jessica Woodhams"], "title": "Towards User-Centred Design of AI-Assisted Decision-Making in Law Enforcement", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "10 pages, 1 figure", "summary": "Artificial Intelligence (AI) has become an important part of our everyday\nlives, yet user requirements for designing AI-assisted systems in law\nenforcement remain unclear. To address this gap, we conducted qualitative\nresearch on decision-making within a law enforcement agency. Our study aimed to\nidentify limitations of existing practices, explore user requirements and\nunderstand the responsibilities that humans expect to undertake in these\nsystems.\n  Participants in our study highlighted the need for a system capable of\nprocessing and analysing large volumes of data efficiently to help in crime\ndetection and prevention. Additionally, the system should satisfy requirements\nfor scalability, accuracy, justification, trustworthiness and adaptability to\nbe adopted in this domain. Participants also emphasised the importance of\nhaving end users review the input data that might be challenging for AI to\ninterpret, and validate the generated output to ensure the system's accuracy.\nTo keep up with the evolving nature of the law enforcement domain, end users\nneed to help the system adapt to the changes in criminal behaviour and\ngovernment guidance, and technical experts need to regularly oversee and\nmonitor the system. Furthermore, user-friendly human interaction with the\nsystem is essential for its adoption and some of the participants confirmed\nthey would be happy to be in the loop and provide necessary feedback that the\nsystem can learn from. Finally, we argue that it is very unlikely that the\nsystem will ever achieve full automation due to the dynamic and complex nature\nof the law enforcement domain."}
{"id": "2504.17749", "pdf": "https://arxiv.org/pdf/2504.17749", "abs": "https://arxiv.org/abs/2504.17749", "authors": ["Steven E. Wilson", "Sina Khanmohammadi"], "title": "MSGCN: Multiplex Spatial Graph Convolution Network for Interlayer Link Weight Prediction", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) have been widely used for various learning\ntasks, ranging from node classification to link prediction. They have\ndemonstrated excellent performance in multiple domains involving\ngraph-structured data. However, an important category of learning tasks, namely\nlink weight prediction, has received less emphasis due to its increased\ncomplexity compared to binary link classification. Link weight prediction\nbecomes even more challenging when considering multilayer networks, where nodes\ncan be interconnected across multiple layers. To address these challenges, we\npropose a new method named Multiplex Spatial Graph Convolution Network (MSGCN),\nwhich spatially embeds information across multiple layers to predict interlayer\nlink weights. The MSGCN model generalizes spatial graph convolution to\nmultiplex networks and captures the geometric structure of nodes across\nmultiple layers. Extensive experiments using data with known interlayer link\ninformation show that the MSGCN model has robust, accurate, and generalizable\nlink weight prediction performance across a wide variety of multiplex network\nstructures."}
{"id": "2504.17401", "pdf": "https://arxiv.org/pdf/2504.17401", "abs": "https://arxiv.org/abs/2504.17401", "authors": ["Xu Wang", "Jialang Xu", "Shuai Zhang", "Baoru Huang", "Danail Stoyanov", "Evangelos B. Mazomenos"], "title": "StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Stereo disparity estimation is crucial for obtaining depth information in\nrobot-assisted minimally invasive surgery (RAMIS). While current deep learning\nmethods have made significant advancements, challenges remain in achieving an\noptimal balance between accuracy, robustness, and inference speed. To address\nthese challenges, we propose the StereoMamba architecture, which is\nspecifically designed for stereo disparity estimation in RAMIS. Our approach is\nbased on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances\nlong-range spatial dependencies both within and across stereo images. To\neffectively integrate multi-scale features from FE-Mamba, we then introduce a\nnovel Multidimensional Feature Fusion (MFF) module. Experiments against the\nstate-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba\nachieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the\nsecond-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining\nan inference speed of 21.28 FPS for a pair of high-resolution images\n(1280*1024), striking the optimum balance between accuracy, robustness, and\nefficiency. Furthermore, by comparing synthesized right images, generated from\nwarping left images using the generated disparity maps, with the actual right\nimage, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),\nexhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS\ndatasets."}
{"id": "2504.17752", "pdf": "https://arxiv.org/pdf/2504.17752", "abs": "https://arxiv.org/abs/2504.17752", "authors": ["Zhihui Gao", "Sri Krishna Vadlamani", "Kfir Sulimany", "Dirk Englund", "Tingjun Chen"], "title": "Disaggregated Deep Learning via In-Physics Computing at Radio Frequency", "categories": ["cs.LG", "cs.ET", "eess.SP", "physics.app-ph"], "comment": "11 pages, 4 figures. Supplementary Information: 54 pages, 20 figures,\n  1 table", "summary": "Modern edge devices, such as cameras, drones, and Internet-of-Things nodes,\nrely on deep learning to enable a wide range of intelligent applications,\nincluding object recognition, environment perception, and autonomous\nnavigation. However, deploying deep learning models directly on the often\nresource-constrained edge devices demands significant memory footprints and\ncomputational power for real-time inference using traditional digital computing\narchitectures. In this paper, we present WISE, a novel computing architecture\nfor wireless edge networks designed to overcome energy constraints in deep\nlearning inference. WISE achieves this goal through two key innovations:\ndisaggregated model access via wireless broadcasting and in-physics computation\nof general complex-valued matrix-vector multiplications directly at radio\nfrequency. Using a software-defined radio platform with wirelessly broadcast\nmodel weights over the air, we demonstrate that WISE achieves 95.7% image\nclassification accuracy with ultra-low operation power of 6.0 fJ/MAC per\nclient, corresponding to a computation efficiency of 165.8 TOPS/W. This\napproach enables energy-efficient deep learning inference on wirelessly\nconnected edge devices, achieving more than two orders of magnitude improvement\nin efficiency compared to traditional digital computing."}
{"id": "2504.17421", "pdf": "https://arxiv.org/pdf/2504.17421", "abs": "https://arxiv.org/abs/2504.17421", "authors": ["Yang Liu", "Bingjie Yan", "Tianyuan Zou", "Jianqing Zhang", "Zixuan Gu", "Jianbing Ding", "Xidong Wang", "Jingyi Li", "Xiaozhou Ye", "Ye Ouyang", "Qiang Yang", "Ya-Qin Zhang"], "title": "Towards Harnessing the Collaborative Power of Large and Small Models for Domain Tasks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nthey require vast amounts of data and computational resources. In contrast,\nsmaller models (SMs), while less powerful, can be more efficient and tailored\nto specific domains. In this position paper, we argue that taking a\ncollaborative approach, where large and small models work synergistically, can\naccelerate the adaptation of LLMs to private domains and unlock new potential\nin AI. We explore various strategies for model collaboration and identify\npotential challenges and opportunities. Building upon this, we advocate for\nindustry-driven research that prioritizes multi-objective benchmarks on\nreal-world private datasets and applications."}
{"id": "2504.17780", "pdf": "https://arxiv.org/pdf/2504.17780", "abs": "https://arxiv.org/abs/2504.17780", "authors": ["Sneh Pillai"], "title": "Replay to Remember: Retaining Domain Knowledge in Streaming Language Models", "categories": ["cs.LG"], "comment": "8 pages 3 figures, 3 tables", "summary": "Continual learning in large language models (LLMs) typically encounters the\ncritical challenge of catastrophic forgetting, where previously acquired\nknowledge deteriorates upon exposure to new data. While techniques like replay\nbuffers and parameter-efficient tuning (e.g., Low-Rank Adaptation or LoRA) have\nbeen proposed, few studies investigate real-time domain adaptation under strict\ncomputational and data-stream constraints. In this paper, we demonstrate a\nlightweight method combining LoRA and a minimal replay mechanism in a realistic\nstreaming setting across three diverse knowledge domains: medical question\nanswering, genetics, and law. Using perplexity, semantic similarity, and\nGPT-based human-like evaluation metrics, we quantify the model's adaptation,\nforgetting, and recovery over time. Our experiments reveal that while\ncatastrophic forgetting naturally occurs, even minimal replay significantly\nstabilizes and partially restores domain-specific knowledge. This study\ncontributes practical insights for deploying adaptable LLMs in\nresource-constrained, real-world scenarios."}
{"id": "2504.17424", "pdf": "https://arxiv.org/pdf/2504.17424", "abs": "https://arxiv.org/abs/2504.17424", "authors": ["Tomoki Mizuno", "Kazuya Yabashi", "Tsuyoshi Tasaki"], "title": "Object Pose Estimation by Camera Arm Control Based on the Next Viewpoint Estimation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We have developed a new method to estimate a Next Viewpoint (NV) which is\neffective for pose estimation of simple-shaped products for product display\nrobots in retail stores. Pose estimation methods using Neural Networks (NN)\nbased on an RGBD camera are highly accurate, but their accuracy significantly\ndecreases when the camera acquires few texture and shape features at a current\nview point. However, it is difficult for previous mathematical model-based\nmethods to estimate effective NV which is because the simple shaped objects\nhave few shape features. Therefore, we focus on the relationship between the\npose estimation and NV estimation. When the pose estimation is more accurate,\nthe NV estimation is more accurate. Therefore, we develop a new pose estimation\nNN that estimates NV simultaneously. Experimental results showed that our NV\nestimation realized a pose estimation success rate 77.3\\%, which was 7.4pt\nhigher than the mathematical model-based NV calculation did. Moreover, we\nverified that the robot using our method displayed 84.2\\% of products."}
{"id": "2504.16941", "pdf": "https://arxiv.org/pdf/2504.16941", "abs": "https://arxiv.org/abs/2504.16941", "authors": ["Zakaria Lamine", "Abdelatif Hafid", "Mohamed Rahouti"], "title": "Mathematical Modeling of Protein Structures: A Cohomology-Based Approach to the Flagellar Motor", "categories": ["q-bio.BM", "cs.LG", "math.AT"], "comment": null, "summary": "This study presents a novel mathematical model derived from cohomology,\nleveraging the KEEL-proven theorem that establishes cohomology as tautological,\ngenerated by boundary classes of curves with fixed dual graphs. Simplicial\ncomplexes are constructed using skew-commutative graded algebra, and the\nstructure theorem is applied to connect distinct homologies, enabling precise\ninterpretations of the resulting geometric forms. The proposed model is\nutilized for protein structure analysis and prediction, with a specific\napplication to the Flagellar Motor structure. This approach offers new insights\ninto the geometric and algebraic foundations of biological macromolecular\nmodeling, highlighting its potential for advancement in structural biology."}
{"id": "2504.17426", "pdf": "https://arxiv.org/pdf/2504.17426", "abs": "https://arxiv.org/abs/2504.17426", "authors": ["Michele Carissimi", "Martina Saletta", "Claudio Ferretti"], "title": "Towards Leveraging Large Language Model Summaries for Topic Modeling in Source Code", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Understanding source code is a topic of great interest in the software\nengineering community, since it can help programmers in various tasks such as\nsoftware maintenance and reuse. Recent advances in large language models (LLMs)\nhave demonstrated remarkable program comprehension capabilities, while\ntransformer-based topic modeling techniques offer effective ways to extract\nsemantic information from text. This paper proposes and explores a novel\napproach that combines these strengths to automatically identify meaningful\ntopics in a corpus of Python programs. Our method consists in applying topic\nmodeling on the descriptions obtained by asking an LLM to summarize the code.\nTo assess the internal consistency of the extracted topics, we compare them\nagainst topics inferred from function names alone, and those derived from\nexisting docstrings. Experimental results suggest that leveraging LLM-generated\nsummaries provides interpretable and semantically rich representation of code\nstructure. The promising results suggest that our approach can be fruitfully\napplied in various software engineering tasks such as automatic documentation\nand tagging, code search, software reorganization and knowledge discovery in\nlarge repositories."}
{"id": "2504.16943", "pdf": "https://arxiv.org/pdf/2504.16943", "abs": "https://arxiv.org/abs/2504.16943", "authors": ["Chiara Fusar Bassini", "Alice Lixuan Xu", "Jorge SÃ¡nchez Canales", "Lion Hirth", "Lynn H. Kaack"], "title": "Flexibility of German gas-fired generation: evidence from clustering empirical operation", "categories": ["cs.CY", "cs.LG"], "comment": "29 pages, 6 figures, 6 tables", "summary": "A key input to energy models are assumptions about the flexibility of power\ngeneration units, i.e., how quickly and often they can start up. These\nassumptions are usually calibrated on the technical characteristics of the\nunits, such as installed capacity or technology type. However, even if power\ngeneration units technically can dispatch flexibly, service obligations and\nmarket incentives may constrain their operation. Here, we cluster over 60% of\nGerman national gas generation (generation units of 100 MWp or above) based on\ntheir empirical flexibility. We process the hourly dispatch of sample units\nbetween 2019 and 2023 using a novel deep learning approach, that transforms\ntime series into easy-to-cluster representations. We identify two clusters of\npeaker units and two clusters of non-peaker units, whose different empirical\nflexibility is quantified by cluster-level ramp rates. Non-peaker units, around\nhalf of the sample, are empirically less flexible than peakers, and make up for\nmore than 83% of sample must-run generation. Regulatory changes addressing the\nlow market responsiveness of non-peakers are needed to unlock their\nflexibility."}
{"id": "2504.17428", "pdf": "https://arxiv.org/pdf/2504.17428", "abs": "https://arxiv.org/abs/2504.17428", "authors": ["Murali Sridharan", "Mika MÃ¤ntylÃ¤", "Leevi Rantala"], "title": "Detection, Classification and Prevalence of Self-Admitted Aging Debt", "categories": ["cs.SE", "cs.AI", "cs.CE", "cs.GL", "D.2.7; D.2.9"], "comment": "Draft", "summary": "Context: Previous research on software aging is limited with focus on dynamic\nruntime indicators like memory and performance, often neglecting evolutionary\nindicators like source code comments and narrowly examining legacy issues\nwithin the TD context. Objective: We introduce the concept of Aging Debt (AD),\nrepresenting the increased maintenance efforts and costs needed to keep\nsoftware updated. We study AD through Self-Admitted Aging Debt (SAAD) observed\nin source code comments left by software developers. Method: We employ a\nmixed-methods approach, combining qualitative and quantitative analyses to\ndetect and measure AD in software. This includes framing SAAD patterns from the\nsource code comments after analysing the source code context, then utilizing\nthe SAAD patterns to detect SAAD comments. In the process, we develop a\ntaxonomy for SAAD that reflects the temporal aging of software and its\nassociated debt. Then we utilize the taxonomy to quantify the different types\nof AD prevalent in OSS repositories. Results: Our proposed taxonomy categorizes\ntemporal software aging into Active and Dormant types. Our extensive analysis\nof over 9,000+ Open Source Software (OSS) repositories reveals that more than\n21% repositories exhibit signs of SAAD as observed from our gold standard SAAD\ndataset. Notably, Dormant AD emerges as the predominant category, highlighting\na critical but often overlooked aspect of software maintenance. Conclusion: As\nsoftware volume grows annually, so do evolutionary aging and maintenance\nchallenges; our proposed taxonomy can aid researchers in detailed software\naging studies and help practitioners develop improved and proactive maintenance\nstrategies."}
{"id": "2504.16956", "pdf": "https://arxiv.org/pdf/2504.16956", "abs": "https://arxiv.org/abs/2504.16956", "authors": ["Cong Qi", "Hanzhang Fang", "Tianxing Hu", "Siqi Jiang", "Wei Zhi"], "title": "Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity", "categories": ["cs.CL", "cs.LG", "q-bio.GN"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of\ncellular heterogeneity, but its complexity, which is marked by high\ndimensionality, sparsity, and batch effects, which poses major computational\nchallenges. Transformer-based models have made significant advances in this\ndomain but are often limited by their quadratic complexity and suboptimal\nhandling of long-range dependencies. In this work, we introduce GeneMamba, a\nscalable and efficient foundation model for single-cell transcriptomics built\non state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba\ncaptures bidirectional gene context with linear-time complexity, offering\nsubstantial computational gains over transformer baselines. The model is\npretrained on nearly 30 million cells and incorporates biologically informed\nobjectives, including pathway-aware contrastive loss and rank-based gene\nencoding. We evaluate GeneMamba across diverse tasks, including multi-batch\nintegration, cell type annotation, and gene-gene correlation, demonstrating\nstrong performance, interpretability, and robustness. These results position\nGeneMamba as a practical and powerful alternative to transformer-based methods,\nadvancing the development of biologically grounded, scalable tools for\nlarge-scale single-cell data analysis."}
{"id": "2504.17447", "pdf": "https://arxiv.org/pdf/2504.17447", "abs": "https://arxiv.org/abs/2504.17447", "authors": ["De-An Huang", "Subhashree Radhakrishnan", "Zhiding Yu", "Jan Kautz"], "title": "FRAG: Frame Selection Augmented Generation for Long Video and Long Document Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "There has been impressive progress in Large Multimodal Models (LMMs). Recent\nworks extend these models to long inputs, including multi-page documents and\nlong videos. However, the model size and performance of these long context\nmodels are still limited due to the computational cost in both training and\ninference. In this work, we explore an orthogonal direction and process long\ninputs without long context LMMs. We propose Frame Selection Augmented\nGeneration (FRAG), where the model first selects relevant frames within the\ninput, and then only generates the final outputs based on the selected frames.\nThe core of the selection process is done by scoring each frame independently,\nwhich does not require long context processing. The frames with the highest\nscores are then selected by a simple Top-K selection. We show that this\nfrustratingly simple framework is applicable to both long videos and multi-page\ndocuments using existing LMMs without any fine-tuning. We consider two models,\nLLaVA-OneVision and InternVL2, in our experiments and show that FRAG\nconsistently improves the performance and achieves state-of-the-art\nperformances for both long video and long document understanding. For videos,\nFRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on\nVideo-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA\ncompared with recent LMMs specialized in long document understanding. Code is\navailable at: https://github.com/NVlabs/FRAG"}
{"id": "2504.16969", "pdf": "https://arxiv.org/pdf/2504.16969", "abs": "https://arxiv.org/abs/2504.16969", "authors": ["Mathias Hanson", "Gregory Lewkowicz", "Sam Verboven"], "title": "Engineering the Law-Machine Learning Translation Problem: Developing Legally Aligned Models", "categories": ["cs.CY", "cs.LG", "I.2.m; K.5.0"], "comment": "16 pages, 1 figure", "summary": "Organizations developing machine learning-based (ML) technologies face the\ncomplex challenge of achieving high predictive performance while respecting the\nlaw. This intersection between ML and the law creates new complexities. As ML\nmodel behavior is inferred from training data, legal obligations cannot be\noperationalized in source code directly. Rather, legal obligations require\n\"indirect\" operationalization. However, choosing context-appropriate\noperationalizations presents two compounding challenges: (1) laws often permit\nmultiple valid operationalizations for a given legal obligation-each with\nvarying degrees of legal adequacy; and, (2) each operationalization creates\nunpredictable trade-offs among the different legal obligations and with\npredictive performance. Evaluating these trade-offs requires metrics (or\nheuristics), which are in turn difficult to validate against legal obligations.\nCurrent methodologies fail to fully address these interwoven challenges as they\neither focus on legal compliance for traditional software or on ML model\ndevelopment without adequately considering legal complexities. In response, we\nintroduce a five-stage interdisciplinary framework that integrates legal and\nML-technical analysis during ML model development. This framework facilitates\ndesigning ML models in a legally aligned way and identifying high-performing\nmodels that are legally justifiable. Legal reasoning guides choices for\noperationalizations and evaluation metrics, while ML experts ensure technical\nfeasibility, performance optimization and an accurate interpretation of metric\nvalues. This framework bridges the gap between more conceptual analysis of law\nand ML models' need for deterministic specifications. We illustrate its\napplication using a case study in the context of anti-money laundering."}
{"id": "2504.17449", "pdf": "https://arxiv.org/pdf/2504.17449", "abs": "https://arxiv.org/abs/2504.17449", "authors": ["Jun Zhang", "Jue Wang", "Huan Li", "Lidan Shou", "Ke Chen", "Gang Chen", "Qin Xie", "Guiming Xie", "Xuejian Gong"], "title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by VLDBJ 2025", "summary": "The significant computational demands of pretrained language models (PLMs),\nwhich often require dedicated hardware, present a substantial challenge in\nserving them efficiently, especially in multi-tenant environments. To address\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\nInference system, designed to manage tenants with distinct PLMs\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\nknowledge into general, domain-specific, and task-specific. Leveraging insights\non knowledge acquisition across different model layers, we construct\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\nestablish hierarchical knowledge management for hPLMs generated by various\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\nincreases by constructing and updating domain-specific knowledge trees based on\nfrequency. We manage task-specific knowledge within limited GPU memory through\nparameter swapping. Finally, we propose system optimizations to enhance\nresource utilization and inference throughput. These include fine-grained\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\noperations with GPU computations, and optimizing parallel implementations with\nbatched matrix multiplications. Our experimental results demonstrate that the\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\nsingle GPU, with only a negligible compromise in accuracy."}
{"id": "2504.17006", "pdf": "https://arxiv.org/pdf/2504.17006", "abs": "https://arxiv.org/abs/2504.17006", "authors": ["Jalal Arabneydi", "Saiful Islam", "Srijita Das", "Sai Krishna Gottipati", "William Duguay", "Cloderic Mars", "Matthew E. Taylor", "Matthew Guzdial", "Antoine Fagette", "Younes Zerouali"], "title": "A Systematic Approach to Design Real-World Human-in-the-Loop Deep Reinforcement Learning: Salient Features, Challenges and Trade-offs", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": "This is a result of the collaboration by JACOBB, AMII(Alberta Machine\n  Intelligence Institute), Thales and AI Redefined (AIR) in 2021-2023", "summary": "With the growing popularity of deep reinforcement learning (DRL),\nhuman-in-the-loop (HITL) approach has the potential to revolutionize the way we\napproach decision-making problems and create new opportunities for human-AI\ncollaboration. In this article, we introduce a novel multi-layered hierarchical\nHITL DRL algorithm that comprises three types of learning: self learning,\nimitation learning and transfer learning. In addition, we consider three forms\nof human inputs: reward, action and demonstration. Furthermore, we discuss main\nchallenges, trade-offs and advantages of HITL in solving complex problems and\nhow human information can be integrated in the AI solution systematically. To\nverify our technical results, we present a real-world unmanned aerial vehicles\n(UAV) problem wherein a number of enemy drones attack a restricted area. The\nobjective is to design a scalable HITL DRL algorithm for ally drones to\nneutralize the enemy drones before they reach the area. To this end, we first\nimplement our solution using an award-winning open-source HITL software called\nCogment. We then demonstrate several interesting results such as (a) HITL leads\nto faster training and higher performance, (b) advice acts as a guiding\ndirection for gradient methods and lowers variance, and (c) the amount of\nadvice should neither be too large nor too small to avoid over-training and\nunder-training. Finally, we illustrate the role of human-AI cooperation in\nsolving two real-world complex scenarios, i.e., overloaded and decoy attacks."}
{"id": "2504.17461", "pdf": "https://arxiv.org/pdf/2504.17461", "abs": "https://arxiv.org/abs/2504.17461", "authors": ["Vipin Singh", "Tianheng Ling", "Teodor Chiaburu", "Felix Biessmann"], "title": "Evaluating Time Series Models for Urban Wastewater Management: Predictive Performance, Model Complexity and Resilience", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, 6 figures, accepted at 10th International Conference on\n  Smart and Sustainable Technologies (SpliTech) 2025, GitHub:\n  https://github.com/calgo-lab/resilient-timeseries-evaluation", "summary": "Climate change increases the frequency of extreme rainfall, placing a\nsignificant strain on urban infrastructures, especially Combined Sewer Systems\n(CSS). Overflows from overburdened CSS release untreated wastewater into\nsurface waters, posing environmental and public health risks. Although\ntraditional physics-based models are effective, they are costly to maintain and\ndifficult to adapt to evolving system dynamics. Machine Learning (ML)\napproaches offer cost-efficient alternatives with greater adaptability. To\nsystematically assess the potential of ML for modeling urban infrastructure\nsystems, we propose a protocol for evaluating Neural Network architectures for\nCSS time series forecasting with respect to predictive performance, model\ncomplexity, and robustness to perturbations. In addition, we assess model\nperformance on peak events and critical fluctuations, as these are the key\nregimes for urban wastewater management. To investigate the feasibility of\nlightweight models suitable for IoT deployment, we compare global models, which\nhave access to all information, with local models, which rely solely on nearby\nsensor readings. Additionally, to explore the security risks posed by network\noutages or adversarial attacks on urban infrastructure, we introduce error\nmodels that assess the resilience of models. Our results demonstrate that while\nglobal models achieve higher predictive performance, local models provide\nsufficient resilience in decentralized scenarios, ensuring robust modeling of\nurban infrastructure. Furthermore, models with longer native forecast horizons\nexhibit greater robustness to data perturbations. These findings contribute to\nthe development of interpretable and reliable ML solutions for sustainable\nurban wastewater management. The implementation is available in our GitHub\nrepository."}
{"id": "2504.17017", "pdf": "https://arxiv.org/pdf/2504.17017", "abs": "https://arxiv.org/abs/2504.17017", "authors": ["Balaji Rao", "William Eiers", "Carlo Lipizzi"], "title": "Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification", "categories": ["cs.AI", "cs.FL", "cs.LG", "cs.LO"], "comment": "Accepted to the Proceedings of the 19th Conference on Neurosymbolic\n  Learning and Reasoning (NeSy 2025)", "summary": "Formally verifying properties of software code has been a highly desirable\ntask, especially with the emergence of LLM-generated code. In the same vein,\nthey provide an interesting avenue for the exploration of formal verification\nand mechanistic interpretability. Since the introduction of code-specific\nmodels, despite their successes in generating code in Lean4 and Isabelle, the\ntask of generalized theorem proving still remains far from being fully solved\nand will be a benchmark for reasoning capability in LLMs. In this work, we\nintroduce a framework that generates whole proofs in a formal language to be\nused within systems that utilize the power of built-in tactics and\noff-the-shelf automated theorem provers. Our framework includes 3 components:\ngenerating natural language statements of the code to be verified, an LLM that\ngenerates formal proofs for the given statement, and a module employing\nheuristics for building the final proof. To train the LLM, we employ a 2-stage\nfine-tuning process, where we first use SFT-based training to enable the model\nto generate syntactically correct Isabelle code and then RL-based training that\nencourages the model to generate proofs verified by a theorem prover. We\nvalidate our framework using the miniF2F-test benchmark and the Isabelle proof\nassistant and design a use case to verify the correctness of the AWS S3 bucket\naccess policy code. We also curate a dataset based on the\nFVEL\\textsubscript{\\textnormal{ER}} dataset for future training tasks."}
{"id": "2504.17471", "pdf": "https://arxiv.org/pdf/2504.17471", "abs": "https://arxiv.org/abs/2504.17471", "authors": ["Yacine Belal", "Mohamed Maouche", "Sonia Ben Mokhtar", "Anthony Simonet-Boulogne"], "title": "GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Gossip Learning (GL) is a decentralized learning paradigm where users\niteratively exchange and aggregate models with a small set of neighboring\npeers. Recent GL approaches rely on dynamic communication graphs built and\nmaintained using Random Peer Sampling (RPS) protocols. Thanks to graph\ndynamics, GL can achieve fast convergence even over extremely sparse\ntopologies. However, the robustness of GL over dy- namic graphs to Byzantine\n(model poisoning) attacks remains unaddressed especially when Byzantine nodes\nattack the RPS protocol to scale up model poisoning. We address this issue by\nintroducing GRANITE, a framework for robust learning over sparse, dynamic\ngraphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two\nkey components (i) a History-aware Byzantine-resilient Peer Sampling protocol\n(HaPS), which tracks previously encountered identifiers to reduce adversarial\ninfluence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which\nleverages an estimate of Byzantine presence to set aggregation thresholds with\nformal guarantees. Empirical results confirm that GRANITE maintains convergence\nwith up to 30% Byzantine nodes, improves learning speed via adaptive filtering\nof poisoned models and obtains these results in up to 9 times sparser graphs\nthan dictated by current theory."}
{"id": "2504.17044", "pdf": "https://arxiv.org/pdf/2504.17044", "abs": "https://arxiv.org/abs/2504.17044", "authors": ["Dhari Gandhi", "Himanshu Joshi", "Lucas Hartman", "Shabnam Hassani"], "title": "Approaches to Responsible Governance of GenAI in Organizations", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid evolution of Generative AI (GenAI) has introduced unprecedented\nopportunities while presenting complex challenges around ethics,\naccountability, and societal impact. This paper draws on a literature review,\nestablished governance frameworks, and industry roundtable discussions to\nidentify core principles for integrating responsible GenAI governance into\ndiverse organizational structures. Our objective is to provide actionable\nrecommendations for a balanced, risk-based governance approach that enables\nboth innovation and oversight. Findings emphasize the need for adaptable risk\nassessment tools, continuous monitoring practices, and cross-sector\ncollaboration to establish trustworthy GenAI. These insights provide a\nstructured foundation and Responsible GenAI Guide (ResAI) for organizations to\nalign GenAI initiatives with ethical, legal, and operational best practices."}
{"id": "2504.17474", "pdf": "https://arxiv.org/pdf/2504.17474", "abs": "https://arxiv.org/abs/2504.17474", "authors": ["Weiran Pan", "Wei Wei", "Feida Zhu", "Yong Deng"], "title": "Enhanced Sample Selection with Confidence Tracking: Identifying Correctly Labeled yet Hard-to-Learn Samples in Noisy Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a novel sample selection method for image classification in the\npresence of noisy labels. Existing methods typically consider small-loss\nsamples as correctly labeled. However, some correctly labeled samples are\ninherently difficult for the model to learn and can exhibit high loss similar\nto mislabeled samples in the early stages of training. Consequently, setting a\nthreshold on per-sample loss to select correct labels results in a trade-off\nbetween precision and recall in sample selection: a lower threshold may miss\nmany correctly labeled hard-to-learn samples (low recall), while a higher\nthreshold may include many mislabeled samples (low precision). To address this\nissue, our goal is to accurately distinguish correctly labeled yet\nhard-to-learn samples from mislabeled ones, thus alleviating the trade-off\ndilemma. We achieve this by considering the trends in model prediction\nconfidence rather than relying solely on loss values. Empirical observations\nshow that only for correctly labeled samples, the model's prediction confidence\nfor the annotated labels typically increases faster than for any other classes.\nBased on this insight, we propose tracking the confidence gaps between the\nannotated labels and other classes during training and evaluating their trends\nusing the Mann-Kendall Test. A sample is considered potentially correctly\nlabeled if all its confidence gaps tend to increase. Our method functions as a\nplug-and-play component that can be seamlessly integrated into existing sample\nselection techniques. Experiments on several standard benchmarks and real-world\ndatasets demonstrate that our method enhances the performance of existing\nmethods for learning with noisy labels."}
{"id": "2504.17102", "pdf": "https://arxiv.org/pdf/2504.17102", "abs": "https://arxiv.org/abs/2504.17102", "authors": ["Haoyu Li", "Xiangru Zhong", "Bin Hu", "Huan Zhang"], "title": "Neural Contraction Metrics with Formal Guarantees for Discrete-Time Nonlinear Dynamical Systems", "categories": ["math.OC", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted by L4DC 2025", "summary": "Contraction metrics are crucial in control theory because they provide a\npowerful framework for analyzing stability, robustness, and convergence of\nvarious dynamical systems. However, identifying these metrics for complex\nnonlinear systems remains an open challenge due to the lack of scalable and\neffective tools. This paper explores the approach of learning verifiable\ncontraction metrics parametrized as neural networks (NNs) for discrete-time\nnonlinear dynamical systems. While prior works on formal verification of\ncontraction metrics for general nonlinear systems have focused on convex\noptimization methods (e.g. linear matrix inequalities, etc) under the\nassumption of continuously differentiable dynamics, the growing prevalence of\nNN-based controllers, often utilizing ReLU activations, introduces challenges\ndue to the non-smooth nature of the resulting closed-loop dynamics. To bridge\nthis gap, we establish a new sufficient condition for establishing formal\nneural contraction metrics for general discrete-time nonlinear systems assuming\nonly the continuity of the dynamics. We show that from a computational\nperspective, our sufficient condition can be efficiently verified using the\nstate-of-the-art neural network verifier $\\alpha,\\!\\beta$-CROWN, which scales\nup non-convex neural network verification via novel integration of symbolic\nlinear bound propagation and branch-and-bound. Built upon our analysis tool, we\nfurther develop a learning method for synthesizing neural contraction metrics\nfrom sampled data. Finally, our approach is validated through the successful\nsynthesis and verification of NN contraction metrics for various nonlinear\nexamples."}
{"id": "2504.17490", "pdf": "https://arxiv.org/pdf/2504.17490", "abs": "https://arxiv.org/abs/2504.17490", "authors": ["Mingqi Yuan", "Qi Wang", "Guozheng Ma", "Bo Li", "Xin Jin", "Yunbo Wang", "Xiaokang Yang", "Wenjun Zeng", "Dacheng Tao"], "title": "Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages", "summary": "Developing lifelong learning agents is crucial for artificial general\nintelligence. However, deep reinforcement learning (RL) systems often suffer\nfrom plasticity loss, where neural networks gradually lose their ability to\nadapt during training. Despite its significance, this field lacks unified\nbenchmarks and evaluation protocols. We introduce Plasticine, the first\nopen-source framework for benchmarking plasticity optimization in deep RL.\nPlasticine provides single-file implementations of over 13 mitigation methods,\n10 evaluation metrics, and learning scenarios with increasing non-stationarity\nlevels from standard to open-ended environments. This framework enables\nresearchers to systematically quantify plasticity loss, evaluate mitigation\nstrategies, and analyze plasticity dynamics across different contexts. Our\ndocumentation, examples, and source code are available at\nhttps://github.com/RLE-Foundation/Plasticine."}
{"id": "2504.17112", "pdf": "https://arxiv.org/pdf/2504.17112", "abs": "https://arxiv.org/abs/2504.17112", "authors": ["Margherita Lampani", "Sabrina Guastavino", "Michele Piana", "Federico Benvenuto"], "title": "Physics-informed features in supervised machine learning", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA", "68Q32, 62J07, 65J20"], "comment": null, "summary": "Supervised machine learning involves approximating an unknown functional\nrelationship from a limited dataset of features and corresponding labels. The\nclassical approach to feature-based machine learning typically relies on\napplying linear regression to standardized features, without considering their\nphysical meaning. This may limit model explainability, particularly in\nscientific applications. This study proposes a physics-informed approach to\nfeature-based machine learning that constructs non-linear feature maps informed\nby physical laws and dimensional analysis. These maps enhance model\ninterpretability and, when physical laws are unknown, allow for the\nidentification of relevant mechanisms through feature ranking. The method aims\nto improve both predictive performance in regression tasks and classification\nskill scores by integrating domain knowledge into the learning process, while\nalso enabling the potential discovery of new physical equations within the\ncontext of explainable machine learning."}
{"id": "2504.17493", "pdf": "https://arxiv.org/pdf/2504.17493", "abs": "https://arxiv.org/abs/2504.17493", "authors": ["Luca-Andrei Fechete", "Mohamed Sana", "Fadhel Ayed", "Nicola Piovesan", "Wenjie Li", "Antonio De Domenico", "Tareq Si Salem"], "title": "Goal-Oriented Time-Series Forecasting: Foundation Framework Design", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traditional time-series forecasting often focuses only on minimizing\nprediction errors, ignoring the specific requirements of real-world\napplications that employ them. This paper presents a new training methodology,\nwhich allows a forecasting model to dynamically adjust its focus based on the\nimportance of forecast ranges specified by the end application. Unlike previous\nmethods that fix these ranges beforehand, our training approach breaks down\npredictions over the entire signal range into smaller segments, which are then\ndynamically weighted and combined to produce accurate forecasts. We tested our\nmethod on standard datasets, including a new dataset from wireless\ncommunication, and found that not only it improves prediction accuracy but also\nimproves the performance of end application employing the forecasting model.\nThis research provides a basis for creating forecasting systems that better\nconnect prediction and decision-making in various practical applications."}
{"id": "2504.17128", "pdf": "https://arxiv.org/pdf/2504.17128", "abs": "https://arxiv.org/abs/2504.17128", "authors": ["Seyed Yousef Soltanian", "Wenlong Zhang"], "title": "PACE: A Framework for Learning and Control in Linear Incomplete-Information Differential Games", "categories": ["eess.SY", "cs.LG", "cs.MA", "cs.SY", "93C41, 49N70, 49N90, 91A27"], "comment": "Accepted to 7th Annual Conference on Learning for Dynamics and\n  Control (L4DC) 2025. Camera-ready version using the official PMLR template.\n  The full version including appendix and proofs", "summary": "In this paper, we address the problem of a two-player linear quadratic\ndifferential game with incomplete information, a scenario commonly encountered\nin multi-agent control, human-robot interaction (HRI), and approximation\nmethods for solving general-sum differential games. While solutions to such\nlinear differential games are typically obtained through coupled Riccati\nequations, the complexity increases when agents have incomplete information,\nparticularly when neither is aware of the other's cost function. To tackle this\nchallenge, we propose a model-based Peer-Aware Cost Estimation (PACE) framework\nfor learning the cost parameters of the other agent. In PACE, each agent treats\nits peer as a learning agent rather than a stationary optimal agent, models\ntheir learning dynamics, and leverages this dynamic to infer the cost function\nparameters of the other agent. This approach enables agents to infer each\nother's objective function in real time based solely on their previous state\nobservations and dynamically adapt their control policies. Furthermore, we\nprovide a theoretical guarantee for the convergence of parameter estimation and\nthe stability of system states in PACE. Additionally, in our numerical studies,\nwe demonstrate how modeling the learning dynamics of the other agent benefits\nPACE, compared to approaches that approximate the other agent as having\ncomplete information, particularly in terms of stability and convergence speed."}
{"id": "2504.17497", "pdf": "https://arxiv.org/pdf/2504.17497", "abs": "https://arxiv.org/abs/2504.17497", "authors": ["Radia Berreziga", "Mohammed Brahimi", "Khairedine Kraim", "Hamid Azzoune"], "title": "Combining GCN Structural Learning with LLM Chemical Knowledge for or Enhanced Virtual Screening", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Virtual screening plays a critical role in modern drug discovery by enabling\nthe identification of promising candidate molecules for experimental\nvalidation. Traditional machine learning methods such as support vector\nmachines (SVM) and XGBoost rely on predefined molecular representations, often\nleading to information loss and potential bias. In contrast, deep learning\napproaches-particularly Graph Convolutional Networks (GCNs)-offer a more\nexpressive and unbiased alternative by operating directly on molecular graphs.\nMeanwhile, Large Language Models (LLMs) have recently demonstrated\nstate-of-the-art performance in drug design, thanks to their capacity to\ncapture complex chemical patterns from large-scale data via attention\nmechanisms.\n  In this paper, we propose a hybrid architecture that integrates GCNs with\nLLM-derived embeddings to combine localized structural learning with global\nchemical knowledge. The LLM embeddings can be precomputed and stored in a\nmolecular feature library, removing the need to rerun the LLM during training\nor inference and thus maintaining computational efficiency. We found that\nconcatenating the LLM embeddings after each GCN layer-rather than only at the\nfinal layer-significantly improves performance, enabling deeper integration of\nglobal context throughout the network. The resulting model achieves superior\nresults, with an F1-score of (88.8%), outperforming standalone GCN (87.9%),\nXGBoost (85.5%), and SVM (85.4%) baselines."}
{"id": "2504.17142", "pdf": "https://arxiv.org/pdf/2504.17142", "abs": "https://arxiv.org/abs/2504.17142", "authors": ["Siddharth Nair", "Timothy F. Walsh", "Greg Pickrell", "Fabio Semperlotti"], "title": "Reinforcement learning framework for the mechanical design of microelectronic components under multiphysics constraints", "categories": ["physics.comp-ph", "cs.CE", "cs.LG"], "comment": "27 pages of main text, 15 figures", "summary": "This study focuses on the development of reinforcement learning based\ntechniques for the design of microelectronic components under multiphysics\nconstraints. While traditional design approaches based on global optimization\napproaches are effective when dealing with a small number of design parameters,\nas the complexity of the solution space and of the constraints increases\ndifferent techniques are needed. This is an important reason that makes the\ndesign and optimization of microelectronic components (characterized by large\nsolution space and multiphysics constraints) very challenging for traditional\nmethods. By taking as prototypical elements an application-specific integrated\ncircuit (ASIC) and a heterogeneously integrated (HI) interposer, we develop and\nnumerically test an optimization framework based on reinforcement learning\n(RL). More specifically, we consider the optimization of the bonded\ninterconnect geometry for an ASIC chip as well as the placement of components\non a HI interposer while satisfying thermoelastic and design constraints. This\nplacement problem is particularly interesting because it features a\nhigh-dimensional solution space."}
{"id": "2504.17528", "pdf": "https://arxiv.org/pdf/2504.17528", "abs": "https://arxiv.org/abs/2504.17528", "authors": ["Weijie Liu", "Ziwei Zhan", "Carlee Joe-Wong", "Edith Ngai", "Jingpu Duan", "Deke Guo", "Xu Chen", "Xiaoxi Zhang"], "title": "TACO: Tackling Over-correction in Federated Learning with Tailored Adaptive Correction", "categories": ["cs.LG", "cs.AI", "I.2.6"], "comment": "11 pages, 7 figures, accepted by ICDCS 2025", "summary": "Non-independent and identically distributed (Non-IID) data across edge\nclients have long posed significant challenges to federated learning (FL)\ntraining in edge computing environments. Prior works have proposed various\nmethods to mitigate this statistical heterogeneity. While these works can\nachieve good theoretical performance, in this work we provide the first\ninvestigation into a hidden over-correction phenomenon brought by the uniform\nmodel correction coefficients across clients adopted by existing methods. Such\nover-correction could degrade model performance and even cause failures in\nmodel convergence. To address this, we propose TACO, a novel algorithm that\naddresses the non-IID nature of clients' data by implementing fine-grained,\nclient-specific gradient correction and model aggregation, steering local\nmodels towards a more accurate global optimum. Moreover, we verify that leading\nFL algorithms generally have better model accuracy in terms of communication\nrounds rather than wall-clock time, resulting from their extra computation\noverhead imposed on clients. To enhance the training efficiency, TACO deploys a\nlightweight model correction and tailored aggregation approach that requires\nminimum computation overhead and no extra information beyond the synchronized\nmodel parameters. To validate TACO's effectiveness, we present the first FL\nconvergence analysis that reveals the root cause of over-correction. Extensive\nexperiments across various datasets confirm TACO's superior and stable\nperformance in practice."}
{"id": "2504.17166", "pdf": "https://arxiv.org/pdf/2504.17166", "abs": "https://arxiv.org/abs/2504.17166", "authors": ["Ke Wan", "Kensuke Tanioka", "Toshio Shimokawa"], "title": "Causal rule ensemble approach for multi-arm data", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Heterogeneous treatment effect (HTE) estimation is critical in medical\nresearch. It provides insights into how treatment effects vary among\nindividuals, which can provide statistical evidence for precision medicine.\nWhile most existing methods focus on binary treatment situations, real-world\napplications often involve multiple interventions. However, current HTE\nestimation methods are primarily designed for binary comparisons and often rely\non black-box models, which limit their applicability and interpretability in\nmulti-arm settings. To address these challenges, we propose an interpretable\nmachine learning framework for HTE estimation in multi-arm trials. Our method\nemploys a rule-based ensemble approach consisting of rule generation, rule\nensemble, and HTE estimation, ensuring both predictive accuracy and\ninterpretability. Through extensive simulation studies and real data\napplications, the performance of our method was evaluated against\nstate-of-the-art multi-arm HTE estimation approaches. The results indicate that\nour approach achieved lower bias and higher estimation accuracy compared with\nthose of existing methods. Furthermore, the interpretability of our framework\nallows clearer insights into how covariates influence treatment effects,\nfacilitating clinical decision making. By bridging the gap between accuracy and\ninterpretability, our study contributes a valuable tool for multi-arm HTE\nestimation, supporting precision medicine."}
{"id": "2504.17534", "pdf": "https://arxiv.org/pdf/2504.17534", "abs": "https://arxiv.org/abs/2504.17534", "authors": ["Juan Carlos Climent Pardo"], "title": "Learning Isometric Embeddings of Road Networks using Multidimensional Scaling", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.SC"], "comment": null, "summary": "The lack of generalization in learning-based autonomous driving applications\nis shown by the narrow range of road scenarios that vehicles can currently\ncover. A generalizable approach should capture many distinct road structures\nand topologies, as well as consider traffic participants, and dynamic changes\nin the environment, so that vehicles can navigate and perform motion planning\ntasks even in the most difficult situations. Designing suitable feature spaces\nfor neural network-based motion planers that encapsulate all kinds of road\nscenarios is still an open research challenge. This paper tackles this\nlearning-based generalization challenge and shows how graph representations of\nroad networks can be leveraged by using multidimensional scaling (MDS)\ntechniques in order to obtain such feature spaces. State-of-the-art graph\nrepresentations and MDS approaches are analyzed for the autonomous driving use\ncase. Finally, the option of embedding graph nodes is discussed in order to\nperform easier learning procedures and obtain dimensionality reduction."}
{"id": "2504.17173", "pdf": "https://arxiv.org/pdf/2504.17173", "abs": "https://arxiv.org/abs/2504.17173", "authors": ["Tianyu Zhang", "Dongheng Zhang", "Ruixu Geng", "Xuecheng Xie", "Shuai Yang", "Yan Chen"], "title": "Lessons from Deploying Learning-based CSI Localization on a Large-Scale ISAC Platform", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "In recent years, Channel State Information (CSI), recognized for its\nfine-grained spatial characteristics, has attracted increasing attention in\nWiFi-based indoor localization. However, despite its potential, CSI-based\napproaches have yet to achieve the same level of deployment scale and\ncommercialization as those based on Received Signal Strength Indicator (RSSI).\nA key limitation lies in the fact that most existing CSI-based systems are\ndeveloped and evaluated in controlled, small-scale environments, limiting their\ngeneralizability. To bridge this gap, we explore the deployment of a\nlarge-scale CSI-based localization system involving over 400 Access Points\n(APs) in a real-world building under the Integrated Sensing and Communication\n(ISAC) paradigm. We highlight two critical yet often overlooked factors: the\nunderutilization of unlabeled data and the inherent heterogeneity of CSI\nmeasurements. To address these challenges, we propose a novel CSI-based\nlearning framework for WiFi localization, tailored for large-scale ISAC\ndeployments on the server side. Specifically, we employ a novel graph-based\nstructure to model heterogeneous CSI data and reduce redundancy. We further\ndesign a pretext pretraining task that incorporates spatial and temporal priors\nto effectively leverage large-scale unlabeled CSI data. Complementarily, we\nintroduce a confidence-aware fine-tuning strategy to enhance the robustness of\nlocalization results. In a leave-one-smartphone-out experiment spanning five\nfloors and 25, 600 m2, we achieve a median localization error of 2.17 meters\nand a floor accuracy of 99.49%. This performance corresponds to an 18.7%\nreduction in mean absolute error (MAE) compared to the best-performing\nbaseline."}
{"id": "2504.17539", "pdf": "https://arxiv.org/pdf/2504.17539", "abs": "https://arxiv.org/abs/2504.17539", "authors": ["Zan-Kai Chong", "Hiroyuki Ohsaki", "Bryan Ng"], "title": "Proof of Useful Intelligence (PoUI): Blockchain Consensus Beyond Energy Waste", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Blockchain technology enables secure, transparent data management in\ndecentralized systems, supporting applications from cryptocurrencies like\nBitcoin to tokenizing real-world assets like property. Its scalability and\nsustainability hinge on consensus mechanisms balancing security and efficiency.\nProof of Work (PoW), used by Bitcoin, ensures security through energy-intensive\ncomputations but demands significant resources. Proof of Stake (PoS), as in\nEthereum post-Merge, selects validators based on staked cryptocurrency,\noffering energy efficiency but risking centralization from wealth\nconcentration. With AI models straining computational resources, we propose\nProof of Useful Intelligence (PoUI), a hybrid consensus mechanism. In PoUI,\nworkers perform AI tasks like language processing or image analysis to earn\ncoins, which are staked to secure the network, blending security with practical\nutility. Decentralized nodes--job posters, market coordinators, workers, and\nvalidators --collaborate via smart contracts to manage tasks and rewards."}
{"id": "2504.17177", "pdf": "https://arxiv.org/pdf/2504.17177", "abs": "https://arxiv.org/abs/2504.17177", "authors": ["Kevin Lane", "Morteza Karimzadeh"], "title": "A Genealogy of Multi-Sensor Foundation Models in Remote Sensing", "categories": ["cs.CV", "cs.LG", "I.4.7; I.4.8"], "comment": "20 pages, submitted to ACM SigSpatial, currently under peer review", "summary": "Foundation models have garnered increasing attention for representation\nlearning in remote sensing, primarily adopting approaches that have\ndemonstrated success in computer vision with minimal domain-specific\nmodification. However, the development and application of foundation models in\nthis field are still burgeoning, as there are a variety of competing approaches\nthat each come with significant benefits and drawbacks. This paper examines\nthese approaches along with their roots in the computer vision field in order\nto characterize potential advantages and pitfalls while outlining future\ndirections to further improve remote sensing-specific foundation models. We\ndiscuss the quality of the learned representations and methods to alleviate the\nneed for massive compute resources. We place emphasis on the multi-sensor\naspect of Earth observations, and the extent to which existing approaches\nleverage multiple sensors in training foundation models in relation to\nmulti-modal foundation models. Finally, we identify opportunities for further\nharnessing the vast amounts of unlabeled, seasonal, and multi-sensor remote\nsensing observations."}
{"id": "2504.17540", "pdf": "https://arxiv.org/pdf/2504.17540", "abs": "https://arxiv.org/abs/2504.17540", "authors": ["Ahmadreza Shateri", "Negar Nourani", "Morteza Dorrigiv", "Hamid Nasiri"], "title": "An Explainable Nature-Inspired Framework for Monkeypox Diagnosis: Xception Features Combined with NGBoost and African Vultures Optimization Algorithm", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "The recent global spread of monkeypox, particularly in regions where it has\nnot historically been prevalent, has raised significant public health concerns.\nEarly and accurate diagnosis is critical for effective disease management and\ncontrol. In response, this study proposes a novel deep learning-based framework\nfor the automated detection of monkeypox from skin lesion images, leveraging\nthe power of transfer learning, dimensionality reduction, and advanced machine\nlearning techniques. We utilize the newly developed Monkeypox Skin Lesion\nDataset (MSLD), which includes images of monkeypox, chickenpox, and measles, to\ntrain and evaluate our models. The proposed framework employs the Xception\narchitecture for deep feature extraction, followed by Principal Component\nAnalysis (PCA) for dimensionality reduction, and the Natural Gradient Boosting\n(NGBoost) algorithm for classification. To optimize the model's performance and\ngeneralization, we introduce the African Vultures Optimization Algorithm (AVOA)\nfor hyperparameter tuning, ensuring efficient exploration of the parameter\nspace. Our results demonstrate that the proposed AVOA-NGBoost model achieves\nstate-of-the-art performance, with an accuracy of 97.53%, F1-score of 97.72%\nand an AUC of 97.47%. Additionally, we enhance model interpretability using\nGrad-CAM and LIME techniques, providing insights into the decision-making\nprocess and highlighting key features influencing classification. This\nframework offers a highly precise and efficient diagnostic tool, potentially\naiding healthcare providers in early detection and diagnosis, particularly in\nresource-constrained environments."}
{"id": "2504.17179", "pdf": "https://arxiv.org/pdf/2504.17179", "abs": "https://arxiv.org/abs/2504.17179", "authors": ["Mohammad Zarei", "Melanie A Jutras", "Eliana Evans", "Mike Tan", "Omid Aaramoon"], "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO", "68T45, 68T05 68T45, 68T05 68T45, 68T05", "I.2.6; I.2.10; I.4.8"], "comment": "8 pages, 10 figures. Accepted to IEEE Conference on Artificial\n  Intelligence (CAI), 2025", "summary": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately\ndetect objects and interpret their surroundings. However, even when trained\nusing millions of miles of real-world data, AVs are often unable to detect rare\nfailure modes (RFMs). The problem of RFMs is commonly referred to as the\n\"long-tail challenge\", due to the distribution of data including many instances\nthat are very rarely seen. In this paper, we present a novel approach that\nutilizes advanced generative and explainable AI techniques to aid in\nunderstanding RFMs. Our methods can be used to enhance the robustness and\nreliability of AVs when combined with both downstream model training and\ntesting. We extract segmentation masks for objects of interest (e.g., cars) and\ninvert them to create environmental masks. These masks, combined with carefully\ncrafted text prompts, are fed into a custom diffusion model. We leverage the\nStable Diffusion inpainting model guided by adversarial noise optimization to\ngenerate images containing diverse environments designed to evade object\ndetection models and expose vulnerabilities in AI systems. Finally, we produce\nnatural language descriptions of the generated RFMs that can guide developers\nand policymakers to improve the safety and reliability of AV systems."}
{"id": "2504.17550", "pdf": "https://arxiv.org/pdf/2504.17550", "abs": "https://arxiv.org/abs/2504.17550", "authors": ["Yejin Bang", "Ziwei Ji", "Alan Schelten", "Anthony Hartshorn", "Tara Fowler", "Cheng Zhang", "Nicola Cancedda", "Pascale Fung"], "title": "HalluLens: LLM Hallucination Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": "42 pages", "summary": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations."}
{"id": "2504.17203", "pdf": "https://arxiv.org/pdf/2504.17203", "abs": "https://arxiv.org/abs/2504.17203", "authors": ["Shivasankari Kannan", "Yeounoh Chung", "Amita Gondi", "Tristan Swadell", "Fatma Ozcan"], "title": "High-Fidelity And Complex Test Data Generation For Real-World SQL Code Generation Services", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "The demand for high-fidelity test data is paramount in industrial settings\nwhere access to production data is largely restricted. Traditional data\ngeneration methods often fall short, struggling with low-fidelity and the\nability to model complex data structures and semantic relationships that are\ncritical for testing complex SQL code generation services like Natural Language\nto SQL (NL2SQL). In this paper, we address the critical need for generating\nsyntactically correct and semantically ``meaningful'' mock data for complex\nschema that includes columns with nested structures that we frequently\nencounter in Google SQL code generation workloads. We highlight the limitations\nof existing approaches used in production, particularly their inability to\nhandle large and complex schema, as well as the lack of semantically coherent\ntest data that lead to limited test coverage. We demonstrate that by leveraging\nLarge Language Models (LLMs) and incorporating strategic pre- and\npost-processing steps, we can generate realistic high-fidelity test data that\nadheres to complex structural constraints and maintains semantic integrity to\nthe test targets (SQL queries/functions). This approach supports comprehensive\ntesting of complex SQL queries involving joins, aggregations, and even deeply\nnested subqueries, ensuring robust evaluation of SQL code generation services,\nlike NL2SQL and SQL Code Assistant services. Our results demonstrate the\npractical utility of an out-of-the-box LLM (\\textit{gemini}) based test data\ngeneration for industrial SQL code generation services where generating\nrealistic test data is essential due to the frequent unavailability of\nproduction datasets."}
{"id": "2504.17551", "pdf": "https://arxiv.org/pdf/2504.17551", "abs": "https://arxiv.org/abs/2504.17551", "authors": ["Lin Che", "Yizi Chen", "Tanhua Jin", "Martin Raubal", "Konrad Schindler", "Peter Kiefer"], "title": "Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 7 figures, preprint version", "summary": "Urban land use classification and mapping are critical for urban planning,\nresource management, and environmental monitoring. Existing remote sensing\ntechniques often lack precision in complex urban environments due to the\nabsence of ground-level details. Unlike aerial perspectives, street view images\nprovide a ground-level view that captures more human and social activities\nrelevant to land use in complex urban scenes. Existing street view-based\nmethods primarily rely on supervised classification, which is challenged by the\nscarcity of high-quality labeled data and the difficulty of generalizing across\ndiverse urban landscapes. This study introduces an unsupervised contrastive\nclustering model for street view images with a built-in geographical prior, to\nenhance clustering performance. When combined with a simple visual assignment\nof the clusters, our approach offers a flexible and customizable solution to\nland use mapping, tailored to the specific needs of urban planners. We\nexperimentally show that our method can generate land use maps from geotagged\nstreet view image datasets of two cities. As our methodology relies on the\nuniversal spatial coherence of geospatial data (\"Tobler's law\"), it can be\nadapted to various settings where street view images are available, to enable\nscalable, unsupervised land use mapping and updating. The code will be\navailable at https://github.com/lin102/CCGP."}
{"id": "2504.17236", "pdf": "https://arxiv.org/pdf/2504.17236", "abs": "https://arxiv.org/abs/2504.17236", "authors": ["Xiqiang Qu", "Jun Chen", "Lei Yu", "Xiangyu Xu"], "title": "Rate-Distortion-Perception Theory for the Quadratic Wasserstein Space", "categories": ["cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "We establish a single-letter characterization of the fundamental\ndistortion-rate-perception tradeoff with limited common randomness under the\nsquared error distortion measure and the squared Wasserstein-2 perception\nmeasure. Moreover, it is shown that this single-letter characterization can be\nexplicitly evaluated for the Gaussian source. Various notions of universal\nrepresentation are also clarified."}
{"id": "2504.17609", "pdf": "https://arxiv.org/pdf/2504.17609", "abs": "https://arxiv.org/abs/2504.17609", "authors": ["Fengchun Liu", "Tong Zhang", "Chunying Zhang"], "title": "STCL:Curriculum learning Strategies for deep learning image steganography models", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Aiming at the problems of poor quality of steganographic images and slow\nnetwork convergence of image steganography models based on deep learning, this\npaper proposes a Steganography Curriculum Learning training strategy (STCL) for\ndeep learning image steganography models. So that only easy images are selected\nfor training when the model has poor fitting ability at the initial stage, and\ngradually expand to more difficult images, the strategy includes a difficulty\nevaluation strategy based on the teacher model and an knee point-based training\nscheduling strategy. Firstly, multiple teacher models are trained, and the\nconsistency of the quality of steganographic images under multiple teacher\nmodels is used as the difficulty score to construct the training subsets from\neasy to difficult. Secondly, a training control strategy based on knee points\nis proposed to reduce the possibility of overfitting on small training sets and\naccelerate the training process. Experimental results on three large public\ndatasets, ALASKA2, VOC2012 and ImageNet, show that the proposed image\nsteganography scheme is able to improve the model performance under multiple\nalgorithmic frameworks, which not only has a high PSNR, SSIM score, and\ndecoding accuracy, but also the steganographic images generated by the model\nunder the training of the STCL strategy have a low steganography analysis\nscores. You can find our code at\n\\href{https://github.com/chaos-boops/STCL}{https://github.com/chaos-boops/STCL}."}
{"id": "2504.17252", "pdf": "https://arxiv.org/pdf/2504.17252", "abs": "https://arxiv.org/abs/2504.17252", "authors": ["Ocheme Anthony Ekle", "Biswarup Das"], "title": "Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo", "categories": ["cs.CL", "cs.LG", "68T50, 68T01", "I.2.7; I.2.1"], "comment": "25 pages, 14 combined figures (19 total), includes horizontal\n  layouts. Submitted to arXiv for open access", "summary": "In this study, we develop Neural Machine Translation (NMT) and\nTransformer-based transfer learning models for English-to-Igbo translation - a\nlow-resource African language spoken by over 40 million people across Nigeria\nand West Africa. Our models are trained on a curated and benchmarked dataset\ncompiled from Bible corpora, local news, Wikipedia articles, and Common Crawl,\nall verified by native language experts. We leverage Recurrent Neural Network\n(RNN) architectures, including Long Short-Term Memory (LSTM) and Gated\nRecurrent Units (GRU), enhanced with attention mechanisms to improve\ntranslation accuracy. To further enhance performance, we apply transfer\nlearning using MarianNMT pre-trained models within the SimpleTransformers\nframework. Our RNN-based system achieves competitive results, closely matching\nexisting English-Igbo benchmarks. With transfer learning, we observe a\nperformance gain of +4.83 BLEU points, reaching an estimated translation\naccuracy of 70%. These findings highlight the effectiveness of combining RNNs\nwith transfer learning to address the performance gap in low-resource language\ntranslation tasks."}
{"id": "2504.17617", "pdf": "https://arxiv.org/pdf/2504.17617", "abs": "https://arxiv.org/abs/2504.17617", "authors": ["Bruno Casella", "Matthias Jakobs", "Marco Aldinucci", "Sebastian BuschjÃ¤ger"], "title": "Decentralized Time Series Classification with ROCKET Features", "categories": ["cs.LG", "cs.AI", "68T07", "I.2.11; I.2.6"], "comment": "Submitted to Workshop on Federated Learning Advancements 2025, in\n  conjunction with ECML-PKDD, WAFL25", "summary": "Time series classification (TSC) is a critical task with applications in\nvarious domains, including healthcare, finance, and industrial monitoring. Due\nto privacy concerns and data regulations, Federated Learning has emerged as a\npromising approach for learning from distributed time series data without\ncentralizing raw information. However, most FL solutions rely on a\nclient-server architecture, which introduces robustness and confidentiality\nrisks related to the distinguished role of the server, which is a single point\nof failure and can observe knowledge extracted from clients. To address these\nchallenges, we propose DROCKS, a fully decentralized FL framework for TSC that\nleverages ROCKET (RandOm Convolutional KErnel Transform) features. In DROCKS,\nthe global model is trained by sequentially traversing a structured path across\nfederation nodes, where each node refines the model and selects the most\neffective local kernels before passing them to the successor. Extensive\nexperiments on the UCR archive demonstrate that DROCKS outperforms\nstate-of-the-art client-server FL approaches while being more resilient to node\nfailures and malicious attacks. Our code is available at\nhttps://anonymous.4open.science/r/DROCKS-7FF3/README.md."}
{"id": "2504.17321", "pdf": "https://arxiv.org/pdf/2504.17321", "abs": "https://arxiv.org/abs/2504.17321", "authors": ["Michael J. Smith", "Luke Fleming", "James E. Geach", "Ryan J. Roberts", "Freddie Kalaitzis", "James Banister"], "title": "Dargana: fine-tuning EarthPT for dynamic tree canopy mapping from space", "categories": ["physics.geo-ph", "cs.LG"], "comment": "9 pages, 6 figures, spotlight at `Tackling Climate Change with\n  Machine Learning', ICLR 2025", "summary": "We present Dargana, a fine-tuned variant of the EarthPT time-series\nfoundation model that achieves specialisation using <3% of its pre-training\ndata volume and 5% of its pre-training compute. Dargana is fine-tuned to\ngenerate regularly updated classification of tree canopy cover at 10m\nresolution, distinguishing conifer and broadleaved tree types. Using Cornwall,\nUK, as a test case, the model achieves a pixel-level ROC-AUC of 0.98 and a\nPR-AUC of 0.83 on unseen satellite imagery. Dargana can identify fine\nstructures like hedgerows and coppice below the training sample limit, and can\ntrack temporal changes to canopy cover such as new woodland establishment. Our\nresults demonstrate how pre-trained Large Observation Models like EarthPT can\nbe specialised for granular, dynamic land cover monitoring from space,\nproviding a valuable, scalable tool for natural capital management and\nconservation."}
{"id": "2504.17619", "pdf": "https://arxiv.org/pdf/2504.17619", "abs": "https://arxiv.org/abs/2504.17619", "authors": ["Catarina P. Coutinho", "Aneeqa Merhab", "Janko Petkovic", "Ferdinando Zanchetta", "Rita Fioresi"], "title": "Enhancing CNNs robustness to occlusions with bioinspired filters for border completion", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to the 7th International Conference on Geometric Science of\n  Information", "summary": "We exploit the mathematical modeling of the visual cortex mechanism for\nborder completion to define custom filters for CNNs. We see a consistent\nimprovement in performance, particularly in accuracy, when our modified LeNet 5\nis tested with occluded MNIST images."}
{"id": "2504.17356", "pdf": "https://arxiv.org/pdf/2504.17356", "abs": "https://arxiv.org/abs/2504.17356", "authors": ["Weiliang Zhang", "Xiaohan Huang", "Yi Du", "Ziyue Qiao", "Qingqing Long", "Zhen Meng", "Yuanchun Zhou", "Meng Xiao"], "title": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": "20 pages, keywords: Automated Feature Engineering, Tabular Dataset,\n  Multi-Agent Reinforcement Learning, Feature Selection", "summary": "Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved."}
{"id": "2504.17624", "pdf": "https://arxiv.org/pdf/2504.17624", "abs": "https://arxiv.org/abs/2504.17624", "authors": ["Jigang Fan", "Chunhao Zhu", "Xiaobing Lan", "Haiming Zhuang", "Mingyu Li", "Jian Zhang", "Shaoyong Lu"], "title": "Deciphering the unique dynamic activation pathway in a G protein-coupled receptor enables unveiling biased signaling and identifying cryptic allosteric sites in conformational intermediates", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "Neurotensin receptor 1 (NTSR1), a member of the Class A G protein-coupled\nreceptor superfamily, plays an important role in modulating dopaminergic\nneuronal activity and eliciting opioid-independent analgesia. Recent studies\nsuggest that promoting \\{beta}-arrestin-biased signaling in NTSR1 may diminish\ndrugs of abuse, such as psychostimulants, thereby offering a potential avenue\nfor treating human addiction-related disorders. In this study, we utilized a\nnovel computational and experimental approach that combined nudged elastic\nband-based molecular dynamics simulations, Markov state models, temporal\ncommunication network analysis, site-directed mutagenesis, and conformational\nbiosensors, to explore the intricate mechanisms underlying NTSR1 activation and\nbiased signaling. Our study reveals a dynamic stepwise transition mechanism and\nactivated transmission network associated with NTSR1 activation. It also yields\nvaluable insights into the complex interplay between the unique polar network,\nnon-conserved ion locks, and aromatic clusters in NTSR1 signaling. Moreover, we\nidentified a cryptic allosteric site located in the intracellular region of the\nreceptor that exists in an intermediate state within the activation pathway.\nCollectively, these findings contribute to a more profound understanding of\nNTSR1 activation and biased signaling at the atomic level, thereby providing a\npotential strategy for the development of NTSR1 allosteric modulators in the\nrealm of G protein-coupled receptor biology, biophysics, and medicine."}
{"id": "2504.17376", "pdf": "https://arxiv.org/pdf/2504.17376", "abs": "https://arxiv.org/abs/2504.17376", "authors": ["Maoyang Xiang", "Ramesh Fernando", "Bo Wang"], "title": "On-Device Qwen2.5: Efficient LLM Inference with Model Compression and Hardware Acceleration", "categories": ["cs.AR", "cs.LG"], "comment": null, "summary": "Transformer-based Large Language Models (LLMs) have significantly advanced AI\ncapabilities but pose considerable challenges for deployment on edge devices\ndue to high computational demands, memory bandwidth constraints, and energy\nconsumption. This paper addresses these challenges by presenting an efficient\nframework for deploying the Qwen2.5-0.5B model on the Xilinx Kria KV260 edge\nplatform, a heterogeneous system integrating an ARM Cortex-A53 CPU with\nreconfigurable FPGA logic. Leveraging Activation-aware Weight Quantization\n(AWQ) with FPGA-accelerated execution pipelines, the proposed approach enhances\nboth model compression rate and system throughput. Additionally, we propose a\nhybrid execution strategy that intelligently offloads compute-intensive\noperations to the FPGA while utilizing the CPU for lighter tasks, effectively\nbalancing the computational workload and maximizing overall performance. Our\nframework achieves a model compression rate of 55.08% compared to the original\nmodel and produces output at a rate of 5.1 tokens per second, outperforming the\nbaseline performance of 2.8 tokens per second."}
{"id": "2504.17641", "pdf": "https://arxiv.org/pdf/2504.17641", "abs": "https://arxiv.org/abs/2504.17641", "authors": ["Shengtao Zhang", "Haokai Zhang", "Shiqi Lou", "Zicheng Wang", "Zinan Zeng", "Yilin Wang", "Minnan Luo"], "title": "PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Dynamic node classification is critical for modeling evolving systems like\nfinancial transactions and academic collaborations. In such systems,\ndynamically capturing node information changes is critical for dynamic node\nclassification, which usually requires all labels at every timestamp. However,\nit is difficult to collect all dynamic labels in real-world scenarios due to\nhigh annotation costs and label uncertainty (e.g., ambiguous or delayed labels\nin fraud detection). In contrast, final timestamp labels are easier to obtain\nas they rely on complete temporal patterns and are usually maintained as a\nunique label for each user in many open platforms, without tracking the history\ndata. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum\nLearning), a pioneering method addressing label-limited dynamic node\nclassification where only final labels are available. PTCL introduces: (1) a\ntemporal decoupling architecture separating the backbone (learning time-aware\nrepresentations) and decoder (strictly aligned with final labels), which\ngenerate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that\nprioritizes pseudo-labels closer to the final timestamp by assigning them\nhigher weights using an exponentially decaying function. We contribute a new\nacademic dataset (CoOAG), capturing long-range research interest in dynamic\ngraph. Experiments across real-world scenarios demonstrate PTCL's consistent\nsuperiority over other methods adapted to this task. Beyond methodology, we\npropose a unified framework FLiD (Framework for Label-Limited Dynamic Node\nClassification), consisting of a complete preparation workflow, training\npipeline, and evaluation standards, and supporting various models and datasets.\nThe code can be found at https://github.com/3205914485/FLiD."}
{"id": "2504.17420", "pdf": "https://arxiv.org/pdf/2504.17420", "abs": "https://arxiv.org/abs/2504.17420", "authors": ["Louisa Pawusch", "Stefania Scheurer", "Wolfgang Nowak", "Reed Maxwell"], "title": "HydroStartML: A combined machine learning and physics-based approach to reduce hydrological model spin-up time", "categories": ["physics.geo-ph", "cs.LG"], "comment": "13 pages, 14 figures. To be published in Advances in Water Resources", "summary": "Finding the initial depth-to-water table (DTWT) configuration of a catchment\nis a critical challenge when simulating the hydrological cycle with integrated\nmodels, significantly impacting simulation outcomes. Traditionally, this\ninvolves iterative spin-up computations, where the model runs under constant\natmospheric settings until steady-state is achieved. These so-called model\nspin-ups are computationally expensive, often requiring many years of simulated\ntime, particularly when the initial DTWT configuration is far from steady\nstate.\n  To accelerate the model spin-up process we developed HydroStartML, a machine\nlearning emulator trained on steady-state DTWT configurations across the\ncontiguous United States. HydroStartML predicts, based on available data like\nconductivity and surface slopes, a DTWT configuration of the respective\nwatershed, which can be used as an initial DTWT.\n  Our results show that initializing spin-up computations with HydroStartML\npredictions leads to faster convergence than with other initial configurations\nlike spatially constant DTWTs. The emulator accurately predicts configurations\nclose to steady state, even for terrain configurations not seen in training,\nand allows especially significant reductions in computational spin-up effort in\nregions with deep DTWTs. This work opens the door for hybrid approaches that\nblend machine learning and traditional simulation, enhancing predictive\naccuracy and efficiency in hydrology for improving water resource management\nand understanding complex environmental interactions."}
{"id": "2504.17655", "pdf": "https://arxiv.org/pdf/2504.17655", "abs": "https://arxiv.org/abs/2504.17655", "authors": ["Farhad Pourkamali-Anaraki"], "title": "Aerial Image Classification in Scarce and Unconstrained Environments via Conformal Prediction", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "17 pages, 5 figures, and 2 tables", "summary": "This paper presents a comprehensive empirical analysis of conformal\nprediction methods on a challenging aerial image dataset featuring diverse\nevents in unconstrained environments. Conformal prediction is a powerful\npost-hoc technique that takes the output of any classifier and transforms it\ninto a set of likely labels, providing a statistical guarantee on the coverage\nof the true label. Unlike evaluations on standard benchmarks, our study\naddresses the complexities of data-scarce and highly variable real-world\nsettings. We investigate the effectiveness of leveraging pretrained models\n(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to\ngenerate informative prediction sets. To further evaluate the impact of\ncalibration, we consider two parallel pipelines (with and without temperature\nscaling) and assess performance using two key metrics: empirical coverage and\naverage prediction set size. This setup allows us to systematically examine how\ncalibration choices influence the trade-off between reliability and efficiency.\nOur findings demonstrate that even with relatively small labeled samples and\nsimple nonconformity scores, conformal prediction can yield valuable\nuncertainty estimates for complex tasks. Moreover, our analysis reveals that\nwhile temperature scaling is often employed for calibration, it does not\nconsistently lead to smaller prediction sets, underscoring the importance of\ncareful consideration in its application. Furthermore, our results highlight\nthe significant potential of model compression techniques within the conformal\nprediction pipeline for deployment in resource-constrained environments. Based\non our observations, we advocate for future research to delve into the impact\nof noisy or ambiguous labels on conformal prediction performance and to explore\neffective model reduction strategies."}
{"id": "2504.17529", "pdf": "https://arxiv.org/pdf/2504.17529", "abs": "https://arxiv.org/abs/2504.17529", "authors": ["Youngjune Lee", "Haeyu Jeong", "Changgeon Lim", "Jeong Choi", "Hongjun Lim", "Hangon Kim", "Jiyoon Kwon", "Saehun Kim"], "title": "IRA: Adaptive Interest-aware Representation and Alignment for Personalized Multi-interest Retrieval", "categories": ["cs.IR", "cs.LG"], "comment": "Accepted to SIGIR 2025 Industry Track. First two authors contributed\n  equally", "summary": "Online community platforms require dynamic personalized retrieval and\nrecommendation that can continuously adapt to evolving user interests and new\ndocuments. However, optimizing models to handle such changes in real-time\nremains a major challenge in large-scale industrial settings. To address this,\nwe propose the Interest-aware Representation and Alignment (IRA) framework, an\nefficient and scalable approach that dynamically adapts to new interactions\nthrough a cumulative structure. IRA leverages two key mechanisms: (1) Interest\nUnits that capture diverse user interests as contextual texts, while\nreinforcing or fading over time through cumulative updates, and (2) a retrieval\nprocess that measures the relevance between Interest Units and documents based\nsolely on semantic relationships, eliminating dependence on click signals to\nmitigate temporal biases. By integrating cumulative Interest Unit updates with\nthe retrieval process, IRA continuously adapts to evolving user preferences,\nensuring robust and fine-grained personalization without being constrained by\npast training distributions. We validate the effectiveness of IRA through\nextensive experiments on real-world datasets, including its deployment in the\nHome Section of NAVER's CAFE, South Korea's leading community platform."}
{"id": "2504.17663", "pdf": "https://arxiv.org/pdf/2504.17663", "abs": "https://arxiv.org/abs/2504.17663", "authors": ["Michelle L. Ding", "Harini Suresh"], "title": "The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "In this paper, we adopt a survivor-centered approach to locate and dissect\nthe role of sociotechnical AI governance in preventing AI-Generated\nNon-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as\n\"deep fake pornography.\" We identify a \"malicious technical ecosystem\" or\n\"MTE,\" comprising of open-source face-swapping models and nearly 200\n\"nudifying\" software programs that allow non-technical users to create AIG-NCII\nwithin minutes. Then, using the National Institute of Standards and Technology\n(NIST) AI 100-4 report as a reflection of current synthetic content governance\nmethods, we show how the current landscape of practices fails to effectively\nregulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining\nthese gaps."}
{"id": "2504.17540", "pdf": "https://arxiv.org/pdf/2504.17540", "abs": "https://arxiv.org/abs/2504.17540", "authors": ["Ahmadreza Shateri", "Negar Nourani", "Morteza Dorrigiv", "Hamid Nasiri"], "title": "An Explainable Nature-Inspired Framework for Monkeypox Diagnosis: Xception Features Combined with NGBoost and African Vultures Optimization Algorithm", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "The recent global spread of monkeypox, particularly in regions where it has\nnot historically been prevalent, has raised significant public health concerns.\nEarly and accurate diagnosis is critical for effective disease management and\ncontrol. In response, this study proposes a novel deep learning-based framework\nfor the automated detection of monkeypox from skin lesion images, leveraging\nthe power of transfer learning, dimensionality reduction, and advanced machine\nlearning techniques. We utilize the newly developed Monkeypox Skin Lesion\nDataset (MSLD), which includes images of monkeypox, chickenpox, and measles, to\ntrain and evaluate our models. The proposed framework employs the Xception\narchitecture for deep feature extraction, followed by Principal Component\nAnalysis (PCA) for dimensionality reduction, and the Natural Gradient Boosting\n(NGBoost) algorithm for classification. To optimize the model's performance and\ngeneralization, we introduce the African Vultures Optimization Algorithm (AVOA)\nfor hyperparameter tuning, ensuring efficient exploration of the parameter\nspace. Our results demonstrate that the proposed AVOA-NGBoost model achieves\nstate-of-the-art performance, with an accuracy of 97.53%, F1-score of 97.72%\nand an AUC of 97.47%. Additionally, we enhance model interpretability using\nGrad-CAM and LIME techniques, providing insights into the decision-making\nprocess and highlighting key features influencing classification. This\nframework offers a highly precise and efficient diagnostic tool, potentially\naiding healthcare providers in early detection and diagnosis, particularly in\nresource-constrained environments."}
{"id": "2504.17669", "pdf": "https://arxiv.org/pdf/2504.17669", "abs": "https://arxiv.org/abs/2504.17669", "authors": ["Subash Neupane", "Shaswata Mitra", "Sudip Mittal", "Shahram Rahimi"], "title": "Towards a HIPAA Compliant Agentic AI System in Healthcare", "categories": ["cs.MA", "cs.AI", "cs.ET"], "comment": null, "summary": "Agentic AI systems powered by Large Language Models (LLMs) as their\nfoundational reasoning engine, are transforming clinical workflows such as\nmedical report generation and clinical summarization by autonomously analyzing\nsensitive healthcare data and executing decisions with minimal human oversight.\nHowever, their adoption demands strict compliance with regulatory frameworks\nsuch as Health Insurance Portability and Accountability Act (HIPAA),\nparticularly when handling Protected Health Information (PHI). This\nwork-in-progress paper introduces a HIPAA-compliant Agentic AI framework that\nenforces regulatory compliance through dynamic, context-aware policy\nenforcement. Our framework integrates three core mechanisms: (1)\nAttribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid\nPHI sanitization pipeline combining regex patterns and BERT-based model to\nminimize leakage, and (3) immutable audit trails for compliance verification."}
{"id": "2504.17546", "pdf": "https://arxiv.org/pdf/2504.17546", "abs": "https://arxiv.org/abs/2504.17546", "authors": ["Wouter van Loon"], "title": "An introduction to R package `mvs`", "categories": ["stat.CO", "cs.LG", "stat.ME", "stat.ML"], "comment": "15 pages, 4 figures. Package vignette corresponding to\n  https://doi.org/10.32614/CRAN.package.mvs", "summary": "In biomedical science, a set of objects or persons can often be described by\nmultiple distinct sets of features obtained from different data sources or\nmodalities (called \"multi-view data\"). Classical machine learning methods\nignore the multi-view structure of such data, limiting model interpretability\nand performance. The R package `mvs` provides methods that were designed\nspecifically for dealing with multi-view data, based on the multi-view stacking\n(MVS) framework. MVS is a form of supervised (machine) learning used to train\nmulti-view classification or prediction models. MVS works by training a\nlearning algorithm on each view separately, estimating the predictive power of\neach view-specific model through cross-validation, and then using another\nlearning algorithm to assign weights to the view-specific models based on their\nestimated predictions. MVS is a form of ensemble learning, dividing the large\nmulti-view learning problem into smaller sub-problems. Most of these\nsub-problems can be solved in parallel, making it computationally attractive.\nAdditionally, the number of features of the sub-problems is greatly reduced\ncompared with the full multi-view learning problem. This makes MVS especially\nuseful when the total number of features is larger than the number of\nobservations (i.e., high-dimensional data). MVS can still be applied even if\nthe sub-problems are themselves high-dimensional by adding suitable penalty\nterms to the learning algorithms. Furthermore, MVS can be used to automatically\nselect the views which are most important for prediction. The R package `mvs`\nmakes fitting MVS models, including such penalty terms, easily and openly\naccessible. `mvs` allows for the fitting of stacked models with any number of\nlevels, with different penalty terms, different outcome distributions, and\nprovides several options for missing data handling."}
{"id": "2504.17671", "pdf": "https://arxiv.org/pdf/2504.17671", "abs": "https://arxiv.org/abs/2504.17671", "authors": ["Yuanchang Ye", "Weiyan Wen"], "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making."}
{"id": "2504.17548", "pdf": "https://arxiv.org/pdf/2504.17548", "abs": "https://arxiv.org/abs/2504.17548", "authors": ["Kilian Tscharke", "Maximilian Wendlinger", "Afrae Ahouzi", "Pallavi Bhardwaj", "Kaweh Amoi-Taleghani", "Michael SchrÃ¶dl-Baumann", "Pascal Debus"], "title": "Quantum Autoencoder for Multivariate Time Series Anomaly Detection", "categories": ["quant-ph", "cs.CR", "cs.LG"], "comment": "Submitted to IEEE International Conference on Quantum Computing and\n  Engineering (QCE) 2025", "summary": "Anomaly Detection (AD) defines the task of identifying observations or events\nthat deviate from typical - or normal - patterns, a critical capability in IT\nsecurity for recognizing incidents such as system misconfigurations, malware\ninfections, or cyberattacks. In enterprise environments like SAP HANA Cloud\nsystems, this task often involves monitoring high-dimensional, multivariate\ntime series (MTS) derived from telemetry and log data. With the advent of\nquantum machine learning offering efficient calculations in high-dimensional\nlatent spaces, many avenues open for dealing with such complex data. One\napproach is the Quantum Autoencoder (QAE), an emerging and promising method\nwith potential for application in both data compression and AD. However, prior\napplications of QAEs to time series AD have been restricted to univariate data,\nlimiting their relevance for real-world enterprise systems. In this work, we\nintroduce a novel QAE-based framework designed specifically for MTS AD towards\nenterprise scale. We theoretically develop and experimentally validate the\narchitecture, demonstrating that our QAE achieves performance competitive with\nneural-network-based autoencoders while requiring fewer trainable parameters.\nWe evaluate our model on datasets that closely reflect SAP system telemetry and\nshow that the proposed QAE is a viable and efficient alternative for\nsemisupervised AD in real-world enterprise settings."}
{"id": "2504.17675", "pdf": "https://arxiv.org/pdf/2504.17675", "abs": "https://arxiv.org/abs/2504.17675", "authors": ["Caroline Panggabean", "Devaraj Verma C", "Bhagyashree Gogoi", "Ranju Limbu", "Rhythm Sarker"], "title": "Optimized Cloud Resource Allocation Using Genetic Algorithms for Energy Efficiency and QoS Assurance", "categories": ["cs.DC", "cs.AI"], "comment": "7 pages, 5 figures, accepted for publication (not yet published)", "summary": "Cloud computing environments demand dynamic and efficient resource management\nto ensure optimal performance, reduced energy consumption, and adherence to\nService Level Agreements (SLAs). This paper presents a Genetic Algorithm\n(GA)-based approach for Virtual Machine (VM) placement and consolidation,\naiming to minimize power usage while maintaining QoS constraints. The proposed\nmethod dynamically adjusts VM allocation based on real-time workload\nvariations, outperforming traditional heuristics such as First Fit Decreasing\n(FFD) and Best Fit Decreasing (BFD). Experimental results show notable\nreductions in energy consumption, VM migrations, SLA violation rates, and\nexecution time. A correlation heatmap further illustrates strong relationships\namong these key performance indicators, confirming the effectiveness of our\napproach in optimizing cloud resource utilization."}
{"id": "2504.17562", "pdf": "https://arxiv.org/pdf/2504.17562", "abs": "https://arxiv.org/abs/2504.17562", "authors": ["Rei Higuchi", "Ryotaro Kawata", "Naoki Nishikawa", "Kazusato Oko", "Shoichiro Yamaguchi", "Sosuke Kobayashi", "Seiya Tokui", "Kohei Hayashi", "Daisuke Okanohara", "Taiji Suzuki"], "title": "When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The ability to acquire latent semantics is one of the key properties that\ndetermines the performance of language models. One convenient approach to\ninvoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at\nthe beginning of texts in the pre-training data, making it easier for the model\nto access latent semantics before observing the entire text. Previous studies\nhave reported that this technique actually improves the performance of trained\nmodels in downstream tasks; however, this improvement has been observed only in\nspecific downstream tasks, without consistent enhancement in average next-token\nprediction loss. To understand this phenomenon, we closely investigate how\nprepending metadata during pre-training affects model performance by examining\nits behavior using artificial data. Interestingly, we found that this approach\nproduces both positive and negative effects on the downstream tasks. We\ndemonstrate that the effectiveness of the approach depends on whether latent\nsemantics can be inferred from the downstream task's prompt. Specifically,\nthrough investigations using data generated by probabilistic context-free\ngrammars, we show that training with metadata helps improve model's performance\nwhen the given context is long enough to infer the latent semantics. In\ncontrast, the technique negatively impacts performance when the context lacks\nthe necessary information to make an accurate posterior inference."}
{"id": "2504.17677", "pdf": "https://arxiv.org/pdf/2504.17677", "abs": "https://arxiv.org/abs/2504.17677", "authors": ["Jarne Thys", "Sebe Vanbrabant", "Davy Vanacken", "Gustavo Rovelo Ruiz"], "title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. This paper introduces INSIGHT, a proof of concept to combine\nvarious AI tools to assist teaching staff and students in the process of\nsolving exercises. INSIGHT has a modular design that allows it to be integrated\ninto various higher education courses. We analyze students' questions to an LLM\nby extracting keywords, which we use to dynamically build an FAQ from students'\nquestions and provide new insights for the teaching staff to use for more\npersonalized face-to-face support. Future work could build upon INSIGHT by\nusing the collected data to provide adaptive learning and adjust content based\non student progress and learning styles to offer a more interactive and\ninclusive learning experience."}
{"id": "2504.17584", "pdf": "https://arxiv.org/pdf/2504.17584", "abs": "https://arxiv.org/abs/2504.17584", "authors": ["Qingyuan Liu", "Liyan Chen", "Yanning Yang", "Haocheng Wang", "Dong Du", "Zhigang Mao", "Naifeng Jing", "Yubin Xia", "Haibo Chen"], "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference", "categories": ["cs.AR", "cs.LG"], "comment": "16 pages, 11 figures", "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."}
{"id": "2504.17685", "pdf": "https://arxiv.org/pdf/2504.17685", "abs": "https://arxiv.org/abs/2504.17685", "authors": ["Haru-Tada Sato", "Fuka Matsuzaki", "Jun-ichiro Takahashi"], "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 2 figures", "summary": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach."}
{"id": "2504.17586", "pdf": "https://arxiv.org/pdf/2504.17586", "abs": "https://arxiv.org/abs/2504.17586", "authors": ["Xuyi Hu", "Jian Li", "Lorenzo Picinali", "Aidan O. T. Hogg"], "title": "A Machine Learning Approach for Denoising and Upsampling HRTFs", "categories": ["cs.SD", "cs.LG"], "comment": null, "summary": "The demand for realistic virtual immersive audio continues to grow, with\nHead-Related Transfer Functions (HRTFs) playing a key role. HRTFs capture how\nsound reaches our ears, reflecting unique anatomical features and enhancing\nspatial perception. It has been shown that personalized HRTFs improve\nlocalization accuracy, but their measurement remains time-consuming and\nrequires a noise-free environment. Although machine learning has been shown to\nreduce the required measurement points and, thus, the measurement time, a\ncontrolled environment is still necessary. This paper proposes a method to\naddress this constraint by presenting a novel technique that can upsample\nsparse, noisy HRTF measurements. The proposed approach combines an HRTF Denoisy\nU-Net for denoising and an Autoencoding Generative Adversarial Network (AE-GAN)\nfor upsampling from three measurement points. The proposed method achieves a\nlog-spectral distortion (LSD) error of 5.41 dB and a cosine similarity loss of\n0.0070, demonstrating the method's effectiveness in HRTF upsampling."}
{"id": "2504.17696", "pdf": "https://arxiv.org/pdf/2504.17696", "abs": "https://arxiv.org/abs/2504.17696", "authors": ["Ghazal Kaviani", "Yavuz Yarici", "Seulgi Kim", "Mohit Prabhushankar", "Ghassan AlRegib", "Mashhour Solh", "Ameya Patil"], "title": "Hierarchical and Multimodal Data for Daily Activity Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Daily Activity Recordings for Artificial Intelligence (DARai, pronounced\n\"Dahr-ree\") is a multimodal, hierarchically annotated dataset constructed to\nunderstand human activities in real-world settings. DARai consists of\ncontinuous scripted and unscripted recordings of 50 participants in 10\ndifferent environments, totaling over 200 hours of data from 20 sensors\nincluding multiple camera views, depth and radar sensors, wearable inertial\nmeasurement units (IMUs), electromyography (EMG), insole pressure sensors,\nbiomonitor sensors, and gaze tracker.\n  To capture the complexity in human activities, DARai is annotated at three\nlevels of hierarchy: (i) high-level activities (L1) that are independent tasks,\n(ii) lower-level actions (L2) that are patterns shared between activities, and\n(iii) fine-grained procedures (L3) that detail the exact execution steps for\nactions. The dataset annotations and recordings are designed so that 22.7% of\nL2 actions are shared between L1 activities and 14.2% of L3 procedures are\nshared between L2 actions. The overlap and unscripted nature of DARai allows\ncounterfactual activities in the dataset.\n  Experiments with various machine learning models showcase the value of DARai\nin uncovering important challenges in human-centered applications.\nSpecifically, we conduct unimodal and multimodal sensor fusion experiments for\nrecognition, temporal localization, and future action anticipation across all\nhierarchical annotation levels. To highlight the limitations of individual\nsensors, we also conduct domain-variant experiments that are enabled by DARai's\nmulti-sensor and counterfactual activity design setup.\n  The code, documentation, and dataset are available at the dedicated DARai\nwebsite:\nhttps://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/"}
{"id": "2504.17622", "pdf": "https://arxiv.org/pdf/2504.17622", "abs": "https://arxiv.org/abs/2504.17622", "authors": ["Chen Xu", "Qiang Wang", "Lijun Sun"], "title": "Likelihood-Free Variational Autoencoders", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Variational Autoencoders (VAEs) typically rely on a probabilistic decoder\nwith a predefined likelihood, most commonly an isotropic Gaussian, to model the\ndata conditional on latent variables. While convenient for optimization, this\nchoice often leads to likelihood misspecification, resulting in blurry\nreconstructions and poor data fidelity, especially for high-dimensional data\nsuch as images. In this work, we propose \\textit{EnVAE}, a novel\nlikelihood-free generative framework that has a deterministic decoder and\nemploys the energy score -- a proper scoring rule -- to build the\nreconstruction loss. This enables likelihood-free inference without requiring\nexplicit parametric density functions. To address the computational\ninefficiency of the energy score, we introduce a fast variant, \\textit{FEnVAE},\nbased on the local smoothness of the decoder and the sharpness of the posterior\ndistribution of latent variables. This yields an efficient single-sample\ntraining objective that integrates seamlessly into existing VAE pipelines with\nminimal overhead. Empirical results on standard benchmarks demonstrate that\n\\textit{EnVAE} achieves superior reconstruction and generation quality compared\nto likelihood-based baselines. Our framework offers a general, scalable, and\nstatistically principled alternative for flexible and nonparametric\ndistribution learning in generative modeling."}
{"id": "2504.17703", "pdf": "https://arxiv.org/pdf/2504.17703", "abs": "https://arxiv.org/abs/2504.17703", "authors": ["Edward Collins", "Michel Wang"], "title": "Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) has emerged as a transformative paradigm in the field\nof distributed machine learning, enabling multiple clients such as mobile\ndevices, edge nodes, or organizations to collaboratively train a shared global\nmodel without the need to centralize sensitive data. This decentralized\napproach addresses growing concerns around data privacy, security, and\nregulatory compliance, making it particularly attractive in domains such as\nhealthcare, finance, and smart IoT systems. This survey provides a concise yet\ncomprehensive overview of Federated Learning, beginning with its core\narchitecture and communication protocol. We discuss the standard FL lifecycle,\nincluding local training, model aggregation, and global updates. A particular\nemphasis is placed on key technical challenges such as handling non-IID\n(non-independent and identically distributed) data, mitigating system and\nhardware heterogeneity, reducing communication overhead, and ensuring privacy\nthrough mechanisms like differential privacy and secure aggregation.\nFurthermore, we examine emerging trends in FL research, including personalized\nFL, cross-device versus cross-silo settings, and integration with other\nparadigms such as reinforcement learning and quantum computing. We also\nhighlight real-world applications and summarize benchmark datasets and\nevaluation metrics commonly used in FL research. Finally, we outline open\nresearch problems and future directions to guide the development of scalable,\nefficient, and trustworthy FL systems."}
{"id": "2504.17656", "pdf": "https://arxiv.org/pdf/2504.17656", "abs": "https://arxiv.org/abs/2504.17656", "authors": ["Ayush Jain", "Rampi Ramprasad"], "title": "polyGen: A Learning Framework for Atomic-level Polymer Structure Generation", "categories": ["cs.CE", "cond-mat.mtrl-sci", "cs.LG"], "comment": null, "summary": "Synthetic polymeric materials underpin fundamental technologies in the\nenergy, electronics, consumer goods, and medical sectors, yet their development\nstill suffers from prolonged design timelines. Although polymer informatics\ntools have supported speedup, polymer simulation protocols continue to face\nsignificant challenges: on-demand generation of realistic 3D atomic structures\nthat respect the conformational diversity of polymer structures. Generative\nalgorithms for 3D structures of inorganic crystals, bio-polymers, and small\nmolecules exist, but have not addressed synthetic polymers. In this work, we\nintroduce polyGen, the first latent diffusion model designed specifically to\ngenerate realistic polymer structures from minimal inputs such as the repeat\nunit chemistry alone, leveraging a molecular encoding that captures polymer\nconnectivity throughout the architecture. Due to a scarce dataset of only 3855\nDFT-optimized polymer structures, we augment our training with DFT-optimized\nmolecular structures, showing improvement in joint learning between similar\nchemical structures. We also establish structure matching criteria to benchmark\nour approach on this novel problem. polyGen effectively generates diverse\nconformations of both linear chains and complex branched structures, though its\nperformance decreases when handling repeat units with a high atom count. Given\nthese initial results, polyGen represents a paradigm shift in atomic-level\nstructure generation for polymer science-the first proof-of-concept for\npredicting realistic atomic-level polymer conformations while accounting for\ntheir intrinsic structural flexibility."}
{"id": "2504.17717", "pdf": "https://arxiv.org/pdf/2504.17717", "abs": "https://arxiv.org/abs/2504.17717", "authors": ["Ãscar Escudero-Arnanz", "Antonio G. Marques", "Inmaculada Mora-JimÃ©nez", "JoaquÃ­n Ãlvarez-RodrÃ­guez", "Cristina Soguero-Ruiz"], "title": "Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis and Interpretable Patient-Similarity Representations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Background and Objectives: Multidrug Resistance (MDR) is a critical global\nhealth issue, causing increased hospital stays, healthcare costs, and\nmortality. This study proposes an interpretable Machine Learning (ML) framework\nfor MDR prediction, aiming for both accurate inference and enhanced\nexplainability.\n  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing\nclinical progression and patient-to-patient interactions. Similarity among\npatients is quantified using MTS-based methods: descriptive statistics, Dynamic\nTime Warping, and Time Cluster Kernel. These similarity measures serve as\ninputs for MDR classification via Logistic Regression, Random Forest, and\nSupport Vector Machines, with dimensionality reduction and kernel\ntransformations improving model performance. For explainability, patient\nsimilarity networks are constructed from these metrics. Spectral clustering and\nt-SNE are applied to identify MDR-related subgroups and visualize high-risk\nclusters, enabling insight into clinically relevant patterns.\n  Results: The framework was validated on ICU Electronic Health Records from\nthe University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms\nbaseline ML and deep learning models by leveraging graph-based patient\nsimilarity. The approach identifies key risk factors -- prolonged antibiotic\nuse, invasive procedures, co-infections, and extended ICU stays -- and reveals\nclinically meaningful clusters. Code and results are available at\n\\https://github.com/oscarescuderoarnanz/DM4MTS.\n  Conclusions: Patient similarity representations combined with graph-based\nanalysis provide accurate MDR prediction and interpretable insights. This\nmethod supports early detection, risk factor identification, and patient\nstratification, highlighting the potential of explainable ML in critical care."}
{"id": "2504.17663", "pdf": "https://arxiv.org/pdf/2504.17663", "abs": "https://arxiv.org/abs/2504.17663", "authors": ["Michelle L. Ding", "Harini Suresh"], "title": "The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "In this paper, we adopt a survivor-centered approach to locate and dissect\nthe role of sociotechnical AI governance in preventing AI-Generated\nNon-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as\n\"deep fake pornography.\" We identify a \"malicious technical ecosystem\" or\n\"MTE,\" comprising of open-source face-swapping models and nearly 200\n\"nudifying\" software programs that allow non-technical users to create AIG-NCII\nwithin minutes. Then, using the National Institute of Standards and Technology\n(NIST) AI 100-4 report as a reflection of current synthetic content governance\nmethods, we show how the current landscape of practices fails to effectively\nregulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining\nthese gaps."}
{"id": "2504.17720", "pdf": "https://arxiv.org/pdf/2504.17720", "abs": "https://arxiv.org/abs/2504.17720", "authors": ["Vansh Gupta", "Sankalan Pal Chowdhury", "VilÃ©m Zouhar", "Donya Rooein", "Mrinmaya Sachan"], "title": "Multilingual Performance Biases of Large Language Models in Education", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment."}
{"id": "2504.17671", "pdf": "https://arxiv.org/pdf/2504.17671", "abs": "https://arxiv.org/abs/2504.17671", "authors": ["Yuanchang Ye", "Weiyan Wen"], "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making."}
{"id": "2504.17721", "pdf": "https://arxiv.org/pdf/2504.17721", "abs": "https://arxiv.org/abs/2504.17721", "authors": ["Cheng Shen", "Yuewei Liu"], "title": "Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees", "categories": ["cs.LG", "cs.AI"], "comment": "Under Review", "summary": "In industrial settings, surface defects on steel can significantly compromise\nits service life and elevate potential safety risks. Traditional defect\ndetection methods predominantly rely on manual inspection, which suffers from\nlow efficiency and high costs. Although automated defect detection approaches\nbased on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly,\ntheir reliability remains challenged due to data annotation uncertainties\nduring deep model training and overfitting issues. These limitations may lead\nto detection deviations when processing the given new test samples, rendering\nautomated detection processes unreliable. To address this challenge, we first\nevaluate the detection model's practical performance through calibration data\nthat satisfies the independent and identically distributed (i.i.d) condition\nwith test data. Specifically, we define a loss function for each calibration\nsample to quantify detection error rates, such as the complement of recall rate\nand false discovery rate. Subsequently, we derive a statistically rigorous\nthreshold based on a user-defined risk level to identify high-probability\ndefective pixels in test images, thereby constructing prediction sets (e.g.,\ndefect regions). This methodology ensures that the expected error rate (mean\nerror rate) on the test set remains strictly bounced by the predefined risk\nlevel. Additionally, we observe a negative correlation between the average\nprediction set size and the risk level on the test set, establishing a\nstatistically rigorous metric for assessing detection model uncertainty.\nFurthermore, our study demonstrates robust and efficient control over the\nexpected test set error rate across varying calibration-to-test partitioning\nratios, validating the method's adaptability and operational effectiveness."}
{"id": "2504.17674", "pdf": "https://arxiv.org/pdf/2504.17674", "abs": "https://arxiv.org/abs/2504.17674", "authors": ["Jared Fernandez", "Clara Na", "Vashisth Tiwari", "Yonatan Bisk", "Sasha Luccioni", "Emma Strubell"], "title": "Energy Considerations of Large Language Model Inference and Efficiency Optimizations", "categories": ["cs.CL", "cs.LG"], "comment": "16 pages", "summary": "As large language models (LLMs) scale in size and adoption, their\ncomputational and environmental costs continue to rise. Prior benchmarking\nefforts have primarily focused on latency reduction in idealized settings,\noften overlooking the diverse real-world inference workloads that shape energy\nuse. In this work, we systematically analyze the energy implications of common\ninference efficiency optimizations across diverse Natural Language Processing\n(NLP) and generative Artificial Intelligence (AI) workloads, including\nconversational AI and code generation. We introduce a modeling approach that\napproximates real-world LLM workflows through a binning strategy for\ninput-output token distributions and batch size variations. Our empirical\nanalysis spans software frameworks, decoding strategies, GPU architectures,\nonline and offline serving settings, and model parallelism configurations. We\nshow that the effectiveness of inference optimizations is highly sensitive to\nworkload geometry, software stack, and hardware accelerators, demonstrating\nthat naive energy estimates based on FLOPs or theoretical GPU utilization\nsignificantly underestimate real-world energy consumption. Our findings reveal\nthat the proper application of relevant inference efficiency optimizations can\nreduce total energy use by up to 73% from unoptimized baselines. These insights\nprovide a foundation for sustainable LLM deployment and inform energy-efficient\ndesign strategies for future AI infrastructure."}
{"id": "2504.17751", "pdf": "https://arxiv.org/pdf/2504.17751", "abs": "https://arxiv.org/abs/2504.17751", "authors": ["Enqi Zhang"], "title": "Revisiting Reset Mechanisms in Spiking Neural Networks for Sequential Modeling: Specialized Discretization for Binary Activated RNN", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "In the field of image recognition, spiking neural networks (SNNs) have\nachieved performance comparable to conventional artificial neural networks\n(ANNs). In such applications, SNNs essentially function as traditional neural\nnetworks with quantized activation values. This article focuses on an another\nalternative perspective,viewing SNNs as binary-activated recurrent neural\nnetworks (RNNs) for sequential modeling tasks.From this viewpoint, current SNN\narchitectures face several fundamental challenges in sequence modeling: (1)\nTraditional models lack effective memory mechanisms for long-range sequence\nmodeling; (2) The biological-inspired components in SNNs (such as reset\nmechanisms and refractory period applications) remain theoretically\nunder-explored for sequence tasks; (3) The RNN-like computational paradigm in\nSNNs prevents parallel training across different timesteps.To address these\nchallenges, this study conducts a systematic analysis of the fundamental\nmechanisms underlying reset operations and refractory periods in\nbinary-activated RNN-based SNN sequence models. We re-examine whether such\nbiological mechanisms are strictly necessary for generating sparse spiking\npatterns, provide new theoretical explanations and insights, and ultimately\npropose the fixed-refractory-period SNN architecture for sequence modeling."}
{"id": "2504.17690", "pdf": "https://arxiv.org/pdf/2504.17690", "abs": "https://arxiv.org/abs/2504.17690", "authors": ["Petros Georgiou", "Aaron Mark Thomas", "Sharu Theresa Jose", "Osvaldo Simeone"], "title": "On the Generalization of Adversarially Trained Quantum Classifiers", "categories": ["quant-ph", "cs.LG"], "comment": "22 pages, 6 figures", "summary": "Quantum classifiers are vulnerable to adversarial attacks that manipulate\ntheir input classical or quantum data. A promising countermeasure is\nadversarial training, where quantum classifiers are trained by using an\nattack-aware, adversarial loss function. This work establishes novel bounds on\nthe generalization error of adversarially trained quantum classifiers when\ntested in the presence of perturbation-constrained adversaries. The bounds\nquantify the excess generalization error incurred to ensure robustness to\nadversarial attacks as scaling with the training sample size $m$ as\n$1/\\sqrt{m}$, while yielding insights into the impact of the quantum embedding.\nFor quantum binary classifiers employing \\textit{rotation embedding}, we find\nthat, in the presence of adversarial attacks on classical inputs $\\mathbf{x}$,\nthe increase in sample complexity due to adversarial training over conventional\ntraining vanishes in the limit of high dimensional inputs $\\mathbf{x}$. In\ncontrast, when the adversary can directly attack the quantum state\n$\\rho(\\mathbf{x})$ encoding the input $\\mathbf{x}$, the excess generalization\nerror depends on the choice of embedding only through its Hilbert space\ndimension. The results are also extended to multi-class classifiers. We\nvalidate our theoretical findings with numerical experiments."}
{"id": "2504.17771", "pdf": "https://arxiv.org/pdf/2504.17771", "abs": "https://arxiv.org/abs/2504.17771", "authors": ["Haochen Wang", "Zhiwei Shi", "Chengxi Zhu", "Yafei Qiao", "Cheng Zhang", "Fan Yang", "Pengjie Ren", "Lan Lu", "Dong Xuan"], "title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted to ICRA 2025. Project page:\n  https://dreamstarring.github.io/HAMLET/", "summary": "Learning-based methods, such as imitation learning (IL) and reinforcement\nlearning (RL), can produce excel control policies over challenging agile robot\ntasks, such as sports robot. However, no existing work has harmonized\nlearning-based policy with model-based methods to reduce training complexity\nand ensure the safety and stability for agile badminton robot control. In this\npaper, we introduce \\ourmethod, a novel hybrid control system for agile\nbadminton robots. Specifically, we propose a model-based strategy for chassis\nlocomotion which provides a base for arm policy. We introduce a\nphysics-informed ``IL+RL'' training framework for learning-based arm policy. In\nthis train framework, a model-based strategy with privileged information is\nused to guide arm policy training during both IL and RL phases. In addition, we\ntrain the critic model during IL phase to alleviate the performance drop issue\nwhen transitioning from IL to RL. We present results on our self-engineered\nbadminton robot, achieving 94.5% success rate against the serving machine and\n90.7% success rate against human players. Our system can be easily generalized\nto other agile mobile manipulation tasks such as agile catching and table\ntennis. Our project website: https://dreamstarring.github.io/HAMLET/."}
{"id": "2504.17710", "pdf": "https://arxiv.org/pdf/2504.17710", "abs": "https://arxiv.org/abs/2504.17710", "authors": ["Yoeri Poels", "Alessandro Pau", "Christian Donner", "Giulio Romanelli", "Olivier Sauter", "Cristina Venturini", "Vlado Menkovski", "the TCV team", "the WPTE team"], "title": "Plasma State Monitoring and Disruption Characterization using Multimodal VAEs", "categories": ["physics.plasm-ph", "cs.CV", "cs.LG"], "comment": null, "summary": "When a plasma disrupts in a tokamak, significant heat and electromagnetic\nloads are deposited onto the surrounding device components. These forces scale\nwith plasma current and magnetic field strength, making disruptions one of the\nkey challenges for future devices. Unfortunately, disruptions are not fully\nunderstood, with many different underlying causes that are difficult to\nanticipate. Data-driven models have shown success in predicting them, but they\nonly provide limited interpretability. On the other hand, large-scale\nstatistical analyses have been a great asset to understanding disruptive\npatterns. In this paper, we leverage data-driven methods to find an\ninterpretable representation of the plasma state for disruption\ncharacterization. Specifically, we use a latent variable model to represent\ndiagnostic measurements as a low-dimensional, latent representation. We build\nupon the Variational Autoencoder (VAE) framework, and extend it for (1)\ncontinuous projections of plasma trajectories; (2) a multimodal structure to\nseparate operating regimes; and (3) separation with respect to disruptive\nregimes. Subsequently, we can identify continuous indicators for the disruption\nrate and the disruptivity based on statistical properties of measurement data.\nThe proposed method is demonstrated using a dataset of approximately 1600 TCV\ndischarges, selecting for flat-top disruptions or regular terminations. We\nevaluate the method with respect to (1) the identified disruption risk and its\ncorrelation with other plasma properties; (2) the ability to distinguish\ndifferent types of disruptions; and (3) downstream analyses. For the latter, we\nconduct a demonstrative study on identifying parameters connected to\ndisruptions using counterfactual-like analysis. Overall, the method can\nadequately identify distinct operating regimes characterized by varying\nproximity to disruptions in an interpretable manner."}
{"id": "2504.17719", "pdf": "https://arxiv.org/pdf/2504.17719", "abs": "https://arxiv.org/abs/2504.17719", "authors": ["Matthijs van der Lende", "Jeremias Lino Ferrao", "Niclas MÃ¼ller-Hof"], "title": "Evaluating Uncertainty in Deep Gaussian Processes", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Reliable uncertainty estimates are crucial in modern machine learning. Deep\nGaussian Processes (DGPs) and Deep Sigma Point Processes (DSPPs) extend GPs\nhierarchically, offering promising methods for uncertainty quantification\ngrounded in Bayesian principles. However, their empirical calibration and\nrobustness under distribution shift relative to baselines like Deep Ensembles\nremain understudied. This work evaluates these models on regression (CASP\ndataset) and classification (ESR dataset) tasks, assessing predictive\nperformance (MAE, Accu- racy), calibration using Negative Log-Likelihood (NLL)\nand Expected Calibration Error (ECE), alongside robustness under various\nsynthetic feature-level distribution shifts. Results indicate DSPPs provide\nstrong in-distribution calibration leveraging their sigma point approximations.\nHowever, compared to Deep Ensembles, which demonstrated superior robustness in\nboth per- formance and calibration under the tested shifts, the GP-based\nmethods showed vulnerabilities, exhibiting particular sensitivity in the\nobserved metrics. Our findings underscore ensembles as a robust baseline,\nsuggesting that while deep GP methods offer good in-distribution calibration,\ntheir practical robustness under distribution shift requires careful\nevaluation. To facilitate reproducibility, we make our code available at\nhttps://github.com/matthjs/xai-gp."}
{"id": "2504.17735", "pdf": "https://arxiv.org/pdf/2504.17735", "abs": "https://arxiv.org/abs/2504.17735", "authors": ["Akhil Padmanabha", "Saravanan Govindarajan", "Hwanmun Kim", "Sergio Ortiz", "Rahul Rajan", "Doruk Senkal", "Sneha Kadetotad"], "title": "EgoCHARM: Resource-Efficient Hierarchical Activity Recognition using an Egocentric IMU Sensor", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Human activity recognition (HAR) on smartglasses has various use cases,\nincluding health/fitness tracking and input for context-aware AI assistants.\nHowever, current approaches for egocentric activity recognition suffer from low\nperformance or are resource-intensive. In this work, we introduce a resource\n(memory, compute, power, sample) efficient machine learning algorithm,\nEgoCHARM, for recognizing both high level and low level activities using a\nsingle egocentric (head-mounted) Inertial Measurement Unit (IMU). Our\nhierarchical algorithm employs a semi-supervised learning strategy, requiring\nprimarily high level activity labels for training, to learn generalizable low\nlevel motion embeddings that can be effectively utilized for low level activity\nrecognition. We evaluate our method on 9 high level and 3 low level activities\nachieving 0.826 and 0.855 F1 scores on high level and low level activity\nrecognition respectively, with just 63k high level and 22k low level model\nparameters, allowing the low level encoder to be deployed directly on current\nIMU chips with compute. Lastly, we present results and insights from a\nsensitivity analysis and highlight the opportunities and limitations of\nactivity recognition using egocentric IMUs."}
{"id": "2504.17768", "pdf": "https://arxiv.org/pdf/2504.17768", "abs": "https://arxiv.org/abs/2504.17768", "authors": ["Piotr Nawrot", "Robert Li", "Renjie Huang", "Sebastian Ruder", "Kelly Marchisio", "Edoardo M. Ponti"], "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications."}
{"id": "2504.17771", "pdf": "https://arxiv.org/pdf/2504.17771", "abs": "https://arxiv.org/abs/2504.17771", "authors": ["Haochen Wang", "Zhiwei Shi", "Chengxi Zhu", "Yafei Qiao", "Cheng Zhang", "Fan Yang", "Pengjie Ren", "Lan Lu", "Dong Xuan"], "title": "Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted to ICRA 2025. Project page:\n  https://dreamstarring.github.io/HAMLET/", "summary": "Learning-based methods, such as imitation learning (IL) and reinforcement\nlearning (RL), can produce excel control policies over challenging agile robot\ntasks, such as sports robot. However, no existing work has harmonized\nlearning-based policy with model-based methods to reduce training complexity\nand ensure the safety and stability for agile badminton robot control. In this\npaper, we introduce \\ourmethod, a novel hybrid control system for agile\nbadminton robots. Specifically, we propose a model-based strategy for chassis\nlocomotion which provides a base for arm policy. We introduce a\nphysics-informed ``IL+RL'' training framework for learning-based arm policy. In\nthis train framework, a model-based strategy with privileged information is\nused to guide arm policy training during both IL and RL phases. In addition, we\ntrain the critic model during IL phase to alleviate the performance drop issue\nwhen transitioning from IL to RL. We present results on our self-engineered\nbadminton robot, achieving 94.5% success rate against the serving machine and\n90.7% success rate against human players. Our system can be easily generalized\nto other agile mobile manipulation tasks such as agile catching and table\ntennis. Our project website: https://dreamstarring.github.io/HAMLET/."}
{"id": "2504.17782", "pdf": "https://arxiv.org/pdf/2504.17782", "abs": "https://arxiv.org/abs/2504.17782", "authors": ["Xize Cheng", "Slytherin Wang", "Zehan Wang", "Rongjie Huang", "Tao Jin", "Zhou Zhao"], "title": "Unleashing the Power of Natural Audio Featuring Multiple Sound Sources", "categories": ["cs.SD", "cs.LG"], "comment": "Work in Progress", "summary": "Universal sound separation aims to extract clean audio tracks corresponding\nto distinct events from mixed audio, which is critical for artificial auditory\nperception. However, current methods heavily rely on artificially mixed audio\nfor training, which limits their ability to generalize to naturally mixed audio\ncollected in real-world environments. To overcome this limitation, we propose\nClearSep, an innovative framework that employs a data engine to decompose\ncomplex naturally mixed audio into multiple independent tracks, thereby\nallowing effective sound separation in real-world scenarios. We introduce two\nremix-based evaluation metrics to quantitatively assess separation quality and\nuse these metrics as thresholds to iteratively apply the data engine alongside\nmodel training, progressively optimizing separation performance. In addition,\nwe propose a series of training strategies tailored to these separated\nindependent tracks to make the best use of them. Extensive experiments\ndemonstrate that ClearSep achieves state-of-the-art performance across multiple\nsound separation tasks, highlighting its potential for advancing sound\nseparation in natural audio scenarios. For more examples and detailed results,\nplease visit our demo page at https://clearsep.github.io."}
